- en: 9 Node embeddings and classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 节点嵌入和分类
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节涵盖
- en: Introducing node embedding models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍节点嵌入模型
- en: Presenting the difference between transductive and inductive models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示归纳模型和归纳模型之间的区别
- en: Examining the difference between structural roles and homophily-based embeddings
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查结构角色和基于同质性的嵌入之间的差异
- en: Introducing the node2vec algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍node2vec算法
- en: Using node2vec embeddings in a downstream machine learning task
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在下游机器学习任务中使用node2vec嵌入
- en: In the previous chapter, you used a vector to represent each node in the network.
    The vectors were handcrafted based on the features you deemed essential. In this
    chapter, you will learn how to automatically generate node representation vectors
    using a *node embedding model*. Node embedding models fall under the dimensionality
    reduction category.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你使用向量来表示网络中的每个节点。这些向量是基于你认为重要的特征手工制作的。在本章中，你将学习如何使用*节点嵌入模型*自动生成节点表示向量。节点嵌入模型属于降维类别。
- en: An example of feature engineering and dimensionality reduction is the body mass
    index (BMI). BMI is commonly used to define obesity. To precisely characterize
    obesity, you could look at a person’s height and weight, and measure their fat
    percentage, muscle content, and waist circumference. In this case, you would be
    dealing with five input features to predict obesity. Instead of having to measure
    all five features before an observation can be made, the doctors came up with
    a BMI.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程和降维的一个例子是体质指数（BMI）。BMI通常用于定义肥胖。为了精确地描述肥胖，你可以查看一个人的身高和体重，并测量他们的体脂百分比、肌肉含量和腰围。在这种情况下，你需要处理五个输入特征来预测肥胖。而不是在做出观察之前必须测量所有五个特征，医生们提出了BMI。
- en: Figure 9.1 visualizes a BMI scale used to evaluate a person’s body type. For
    example, if the BMI is 35 or greater, the BMI scale would regard that person as
    extremely obese. BMI is calculated by dividing a person’s weight in kilograms
    by their height in square meters and is a rough estimate of body fat. Instead
    of using five input features, a single embedded feature is a good representation
    of the expected output. It is a good approximation but by no means a perfect descriptor
    of obesity. For example, a rugby player would be considered obese given the BMI,
    but they probably have more muscle than fat. An embedding model reduces the dimensionality
    of input features while retaining a strong correlation to a given problem. An
    added bonus of using an embedding model is that you can collect less data for
    training and validating the model. In the case of BMI, you can avoid potentially
    costly measurements by only comparing the height and weight ratios.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1可视化了一个用于评估人体类型的BMI秤。例如，如果BMI为35或更高，BMI秤会将该人视为极度肥胖。BMI是通过将一个人的体重（千克）除以他们的身高（平方米）来计算的，并且是对体脂的粗略估计。与使用五个输入特征相比，单个嵌入特征是期望输出的良好表示。这是一个良好的近似，但绝不是肥胖的完美描述符。例如，一个橄榄球运动员根据BMI会被认为是肥胖的，但他们可能肌肉比脂肪多。嵌入模型在保持与给定问题强相关性的同时降低了输入特征的维度。使用嵌入模型的额外好处是，你可以收集更少的数据来训练和验证模型。在BMI的情况下，你可以通过仅比较身高和体重比来避免可能昂贵的测量。
- en: '![09-01](../../OEBPS/Images/09-01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![09-01](../../OEBPS/Images/09-01.png)'
- en: Figure 9.1 Body mass index chart
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 体质指数图表
- en: Every graph can be represented as an *adjacency matrix*. An adjacency matrix
    is a square matrix in which the elements indicate whether pairs of nodes are connected.
    Such a matrix can be regarded as a *high-dimensional representation* of the network.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图都可以表示为一个*邻接矩阵*。邻接矩阵是一个方阵，其中的元素表示节点对是否连接。这样的矩阵可以被视为网络的*高维表示*。
- en: Figure 9.2 visualizes an adjacency matrix representing a graph with four nodes,
    A, B, C, and D. Each element in the adjacency matrix indicates whether the pair
    of nodes is connected. For example, the element in column C and row D has a value
    of 1, which indicates that a relationship between nodes C and D is present in
    the graph. If the value of the element in the matrix is 0, then a relationship
    between the pair of nodes does not exist.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2可视化了一个包含四个节点A、B、C和D的邻接矩阵。邻接矩阵中的每个元素表示节点对是否连接。例如，C列和D行的元素值为1，表示图中存在节点C和D之间的关系。如果矩阵中元素的值为0，则表示节点对之间不存在关系。
- en: '![09-02](../../OEBPS/Images/09-02.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![09-02](../../OEBPS/Images/09-02.png)'
- en: Figure 9.2 Adjacency matrix
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 邻接矩阵
- en: Now, imagine you have a graph with a million nodes. In an adjacency matrix,
    each node would be represented with a row in the matrix that has a million elements.
    In other words, each node can be described with a vector that has a million elements.
    Therefore, an adjacency matrix is regarded as a high-dimensional network representation,
    as it grows with the number of nodes in the graph.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下你有一个包含一百万个节点的图。在一个邻接矩阵中，每个节点都会在矩阵中以一行表示，该行包含一百万个元素。换句话说，每个节点可以用一个包含一百万个元素的向量来描述。因此，邻接矩阵被视为一个高维网络表示，因为它随着图中节点数量的增加而增长。
- en: 'Suppose you want to train a machine learning model and, somehow, use the network
    structure information as an input feature. Let’s say you use an adjacency matrix
    as an input. There are a couple of problems with this approach:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想训练一个机器学习模型，并且以某种方式使用网络结构信息作为输入特征。比如说，你使用邻接矩阵作为输入。这种方法有几个问题：
- en: There are too many input features.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入特征太多。
- en: Machine learning models are dependent on the size of the graph.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型依赖于图的大小。
- en: Overfitting can become a problem.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合可能成为一个问题。
- en: If you add or remove a single node from the graph, the size of the adjacency
    matrix changes and your model is no longer functional, as there is a different
    number of input features. Using the adjacency matrix as an input to your model
    could also cause overfitting. In practice, you often want to embed a node’s local
    representation to compare nodes with similar neighborhood topology, instead of
    using all relationships between nodes as a feature input. Node embedding techniques
    try to solve these issues by learning lower-dimensional node representation for
    any given network. The learned node representations or embeddings should automatically
    encode the network structure so that the similarity in the embedding space approximates
    the similarity in the network. A key message is that the node representations
    are learned instead of manually engineered. The doctors reduced the dimensionality
    in the BMI example by a manual formula. The node embedding techniques aim to remove
    painstaking manual feature engineering and provide the best possible node representations
    by treating the embedding process as a separate machine learning step. The node
    embedding step is an unsupervised process because it learns to represent nodes
    without using labeled training data. Node embedding models use techniques based
    on deep learning and nonlinear dimensionality reduction to achieve this (Hamilton
    et al., 2018).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从图中添加或删除一个节点，邻接矩阵的大小会改变，你的模型就不再有效，因为输入特征的数量不同。使用邻接矩阵作为模型输入也可能导致过拟合。在实践中，你通常希望嵌入节点的局部表示来比较具有相似邻域拓扑结构的节点，而不是使用节点之间所有的关系作为特征输入。节点嵌入技术试图通过学习任何给定网络的低维节点表示来解决这些问题。学习到的节点表示或嵌入应该自动编码网络结构，以便嵌入空间中的相似性近似网络中的相似性。一个关键信息是，节点表示是通过学习得到的，而不是手动工程化的。在BMI示例中，医生通过手动公式降低了维度。节点嵌入技术旨在通过将嵌入过程视为一个独立的机器学习步骤来消除繁琐的手动特征工程，并提供最佳的节点表示。节点嵌入步骤是一个无监督过程，因为它学习表示节点而不使用标记的训练数据。节点嵌入模型使用基于深度学习和非线性降维的技术来实现这一点（Hamilton等，2018）。
- en: Figure 9.3 visualizes the node embedding process. The node embedding model takes
    the high-dimensional representation of a graph as an input and outputs a lower-dimensional
    representation. In the example of a graph with a million nodes, each node can
    be represented with a vector of a million elements. Suppose you execute a node
    embedding model on this graph. With most node embedding models, you can define
    the embedding dimension. The *embedding dimension* is the number of elements in
    the embedding matrix that describe a node. For example, you could set the embedding
    dimension to be 256\. In that case, each node would be described with a vector
    that contains 256 elements. Reducing the number of elements from a million to
    256 is incredibly beneficial, as it allows you to efficiently describe the network
    topology or position of a node in a graph with a lower-dimensional vector. Lower-dimensional
    vectors can be used in a downstream machine learning workflow, or they can be
    used to infer a similarity network using the nearest neighbor graph algorithm.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3可视化了节点嵌入过程。节点嵌入模型将图的高维表示作为输入，并输出低维表示。在具有一百万个节点的图示例中，每个节点可以用包含一百万个元素的向量表示。假设你在该图上执行节点嵌入模型。对于大多数节点嵌入模型，你可以定义嵌入维度。**嵌入维度**是描述节点的嵌入矩阵中的元素数量。例如，你可以将嵌入维度设置为256。在这种情况下，每个节点将用包含256个元素的向量来描述。将元素数量从一百万减少到256非常有益，因为它允许你使用低维向量有效地描述网络拓扑或图中节点的位置。低维向量可以用于下游机器学习工作流程，或者它们可以用于使用最近邻图算法推断相似性网络。
- en: '![09-03](../../OEBPS/Images/09-03.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![09-03](../../OEBPS/Images/09-03.png)'
- en: Figure 9.3 Adjacency matrix
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 邻接矩阵
- en: 9.1 Node embedding models
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 节点嵌入模型
- en: Node embedding models aim to produce lower-dimensional representations of nodes,
    while preserving network structure information. These lower-dimensional representations
    can then be used as feature inputs for various machine learning tasks, such as
    node classification, link prediction, and community detection, thereby simplifying
    the computational complexity and potentially improving the performance of models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 节点嵌入模型旨在生成节点的低维表示，同时保留网络结构信息。这些低维表示可以随后用作各种机器学习任务的输入特征，例如节点分类、链接预测和社区检测，从而简化计算复杂度并可能提高模型的性能。
- en: 9.1.1 Homophily vs. structural roles approach
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 同质化与结构角色方法比较
- en: What does *network structure information* mean exactly? A common approach is
    to represent nodes in the embedding space so that neighboring nodes in the graph
    are close in the embedding space.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: “网络结构信息”究竟是什么意思？一种常见的方法是在嵌入空间中表示节点，使得图中相邻的节点在嵌入空间中彼此靠近。
- en: Figure 9.4 shows the so-called community-based approach to node embeddings.
    Neighboring nodes in the graph are also close in the embedding space. Therefore,
    nodes that belong to the same community should be close in the embedding space.
    This approach is designed under the node *homophily* assumption that connected
    nodes tend to be similar or have similar labels in a downstream machine learning
    workflow.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4展示了所谓的基于社区的节点嵌入方法。图中相邻的节点在嵌入空间中也彼此靠近。因此，属于同一社区的节点在嵌入空间中应该彼此靠近。这种方法是在节点**同质化**假设下设计的，即连接的节点在下游机器学习工作流程中往往相似或有相似的标签。
- en: '![09-04](../../OEBPS/Images/09-04.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![09-04](../../OEBPS/Images/09-04.png)'
- en: Figure 9.4 Homophily approach to node embedding
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 同质化方法进行节点嵌入
- en: For example, you and your friends probably have similar interests. Suppose you
    wanted to predict a friend’s interest. In that case, you could encode their position
    in the friendship network with a homophily-based node embedding model and train
    a supervised model based on training examples to predict or recommend their interests.
    If the hypothesis that a person and their friends have similar interests is valid,
    the trained model should perform relatively well. One node embedding algorithm
    you could use in this example is the fast random projection (FastRP) algorithm
    (Chen et al., 2019). Another approach is to encode nodes in the embedding space
    so that nodes with a similar network role are close in the embedding space.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您和您的朋友可能有着相似的兴趣。假设您想要预测一个朋友的新兴趣。在这种情况下，您可以使用基于同质性的节点嵌入模型来编码他们在友谊网络中的位置，并基于训练示例训练一个监督模型来预测或推荐他们的兴趣。如果一个人及其朋友具有相似兴趣的假设是有效的，那么训练好的模型应该表现相对较好。在这个例子中，您可以使用的节点嵌入算法之一是快速随机投影（FastRP）算法（Chen等人，2019）。另一种方法是编码节点在嵌入空间中，使得具有相似网络角色的节点在嵌入空间中靠近。
- en: You were briefly introduced to node roles in the previous chapter. Figure 9.5
    visualizes the node embedding process, where the nodes are encoded close in the
    embedding space based on their network *structural roles*. In figure 9.5, both
    nodes D and F act as bridges between the two communities. One can assume they
    have similar network roles, and therefore, they are encoded close in the embedding
    space. You could use the structural role embedding approach to analyze roles of
    researchers in coauthorship. For example, you could use the structural role approach
    to node embedding to analyze roles of researchers in a coauthorship network or,
    perhaps, determine roles of routers on the internet network. For instance, the
    role eXtraction (RolX) algorithm (Henderson et al., 2012) is a node embedding
    algorithm that encodes nodes with a network structural role close in the embedding
    space.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您简要地介绍了节点角色。图9.5展示了节点嵌入过程，其中节点根据其网络**结构角色**在嵌入空间中编码得较为接近。在图9.5中，节点D和F都充当两个社区之间的桥梁。可以假设它们具有相似的网络角色，因此它们在嵌入空间中编码得较为接近。您可以使用结构角色嵌入方法来分析合作作者中研究者的角色。例如，您可以使用结构角色嵌入方法来分析合作作者网络中研究者的角色，或者，也许可以确定互联网网络中路由器的角色。例如，角色提取（RolX）算法（Henderson等人，2012）是一种节点嵌入算法，它将具有网络结构角色的节点在嵌入空间中编码得较为接近。
- en: '![09-05](../../OEBPS/Images/09-05.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![09-05](../../OEBPS/Images/09-05.png)'
- en: Figure 9.5 Structural roles approach to node embedding
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 节点嵌入的结构角色方法
- en: Which design of the node embedding models you want to use depends on the downstream
    task you need to complete. Some algorithms, like node2vec (Grover & Leskovec,
    2016), can also produce a combination of the two embedding designs as the output.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 您想使用哪种节点嵌入模型的设计取决于您需要完成的下游任务。一些算法，如node2vec（Grover & Leskovec，2016），也可以产生两种嵌入设计的组合作为输出。
- en: 9.1.2 Inductive vs. transductive embedding models
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 归纳与演绎嵌入模型
- en: Some node embedding models have a significant limitation. A typical process
    of using a node embedding model in a machine learning workflow involves calculating
    the embeddings and feeding them, for example, into a classification machine learning
    model. However, the difference between *inductive* and *transductive* node embedding
    models is in their ability to encode new unseen nodes during training.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一些节点嵌入模型存在一个显著的局限性。在机器学习工作流程中使用节点嵌入模型的典型过程包括计算嵌入并将它们输入到分类机器学习模型中，例如。然而，**归纳**和**演绎**节点嵌入模型之间的区别在于它们在训练过程中编码新未见节点的能力。
- en: When dealing with a *transductive* node embedding algorithm, you cannot calculate
    embeddings for nodes not seen during the initial embedding calculation. You can
    consider transductive models as creating a vocabulary during initial computation,
    where the key of the vocabulary represents a node and its value represents the
    embedding. If a node was not seen during the initial computation, it is not present
    in the vocabulary; hence, you cannot simply retrieve the embeddings for the new
    unseen nodes. If you want to calculate the embeddings for the new nodes, you must
    calculate the embeddings for the whole graph, meaning all the previously observed
    nodes as well as the new nodes. Since the embeddings might change for existing
    nodes, you must also retrain the classification model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理一个**归纳**节点嵌入算法时，你无法计算在初始嵌入计算期间未看到的节点的嵌入。你可以将归纳模型视为在初始计算期间创建一个词汇表，其中词汇表的关键字代表一个节点，其值代表嵌入。如果一个节点在初始计算期间未被看到，它就不会出现在词汇表中；因此，你不能简单地检索新未见节点的嵌入。如果你想计算新节点的嵌入，你必须计算整个图的嵌入，这意味着所有先前观察到的节点以及新节点。由于现有节点的嵌入可能会发生变化，你还必须重新训练分类模型。
- en: On the other hand, *inductive* node embedding models can calculate embeddings
    for unseen nodes during the initial computation. For example, you can train a
    model based on the initial computation of node embeddings. When a new node is
    introduced, you can calculate the embedding for the new node without recalculating
    embeddings for the whole graph. Likewise, you don’t need to retrain the classification
    model for every new node. Encoding previously unseen nodes is a great advantage
    when dealing with growing or multiple separate graphs. For instance, you could
    train a classification model on a single graph and then use it to predict node
    labels for nodes of different separate graphs. To learn more about inductive models,
    I recommend reading up on the GraphSAGE model (Hamilton et al., 2017).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**归纳**节点嵌入模型可以在初始计算期间计算未见节点的嵌入。例如，你可以基于节点嵌入的初始计算训练一个模型。当引入新节点时，你可以计算新节点的嵌入，而无需重新计算整个图的嵌入。同样，你也不必为每个新节点重新训练分类模型。在处理增长或多个分离的图时，对先前未见节点进行编码是一个巨大的优势。例如，你可以在单个图上训练一个分类模型，然后使用它来预测不同分离图的节点标签。要了解更多关于归纳模型的信息，我建议阅读GraphSAGE模型（Hamilton等人，2017年）。
- en: 9.2 Node classification task
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 节点分类任务
- en: Now, it is time to start with a practical example. Imagine you are working at
    Twitch as a data scientist. Twitch is a streaming platform that makes it possible
    for anyone to start streaming their content to the world. In addition, other users
    can interact with streamers through the chat interface.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候开始一个实际例子了。想象一下，你正在Twitch作为数据科学家工作。Twitch是一个直播平台，它使任何人都能向全世界直播他们的内容。此外，其他用户可以通过聊天界面与直播者互动。
- en: Every day, new users join the platform who decide they want to start streaming.
    Your manager wants you to identify the language of the new streams. Since the
    platform is worldwide, streamers likely use around 30 to 50 languages. Let’s assume
    converting audio to text and running language-detection algorithms is not feasible
    for whatever reason. One of the reasons could be that many streamers on Twitch
    play video games, and therefore, audio from video games could distort language
    detection.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每天都有新用户加入平台，他们决定开始直播。你的经理希望你识别新流的语言。由于该平台是全球性的，直播者可能使用大约30到50种语言。假设由于某种原因，将音频转换为文本并运行语言检测算法是不可行的。其中一个原因可能是，许多Twitch上的直播者玩电子游戏，因此，电子游戏的声音可能会扭曲语言检测。
- en: What other way could you predict the languages of new streamers? You have information
    about users who chat in particular streams. One could hypothesize that users mostly
    chat in a single language. Therefore, if a user chats in two streams, it is likely
    that both streams are in the same language. For example, if a user is chatting
    in a Japanese stream and then switches a stream and interacts with the new streamer
    through chat, the new stream is likely in Japanese. There might be some exceptions
    with the English language, as for the most part, many people on the internet have
    at least a basic understanding of English. Remember, this is only an assumption
    that still needs to be validated.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你还能用其他什么方法来预测新主播的语言呢？你拥有关于在特定流中聊天的用户的信息。可以假设用户主要使用单一语言聊天。因此，如果一个用户在两个流中聊天，那么这两个流很可能使用相同的语言。例如，如果一个用户在日语文流中聊天，然后切换流并与新主播通过聊天互动，那么新的流很可能也是日语。对于英语来说，可能会有一些例外，因为大部分情况下，互联网上的人至少对英语有基本的了解。记住，这只是一个需要验证的假设。
- en: 'Figure 9.6 visualizes the process of extracting network information to predict
    the new streamers’ languages. Raw data has the structure of a bipartite `(:User)-[:CHATTED]→
    (:Stream)` graph. The first step in the process is to project a monopartite graph
    where the nodes represent streams, and the relationships represent their shared
    audience. The schema of the projected monopartite graph can be represented with
    the following Cypher statement: `(:Stream)-[:SHARED_AUDIENCE]-(:Stream)`. The
    monopartite graph is undirected, so if stream A shares the audience with stream
    B, it is automatically implied that stream B also shares the audience with stream
    A. In addition, you can add the count of shared audiences between streamers as
    a relationship weight. Suppose that extracting raw data and transforming it into
    a monopartite graph can be done by a data engineer on your team. The data engineer
    can take an approach similar to what you learned in chapter 7 to project a monopartite
    graph. Your job is now to train a prediction model and evaluate its results.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6可视化了解析网络信息以预测新主播语言的过程。原始数据具有`(:User)-[:CHATTED]→ (:Stream)`二分图的结构。过程的第一步是将单分图投影出来，其中节点代表流，关系代表它们共享的观众。投影的单分图模式可以用以下Cypher语句表示：`(:Stream)-[:SHARED_AUDIENCE]-(:Stream)`。单分图是无向的，所以如果流A与流B共享观众，那么自动意味着流B也共享观众与流A。此外，你还可以将主播之间共享观众的数量作为关系权重。假设提取原始数据并将其转换为单分图可以由你团队中的数据工程师完成。数据工程师可以采用与你在第7章中学到的方法类似的方法来投影单分图。你现在的工作是训练一个预测模型并评估其结果。
- en: '![09-06](../../OEBPS/Images/09-06.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![09-06](../../OEBPS/Images/09-06.png)'
- en: Figure 9.6 The process of predicting the language for new streams
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 预测新流语言的过程
- en: The idea is to prepare a Jupyter notebook that can be used once a day to predict
    the languages of new streamers. Remember, if streams are close in the shared audience
    network, they likely have the same language. Therefore, you will use a node embedding
    model that uses a homophily-based approach to encoding nodes in the embedding
    space. One of the simplest and most broadly used node embedding models is node2vec,
    which you will use in this example. Once the node embeddings are calculated, you
    will use them for training a random forest classification model based on training
    examples of streams for which you already know the language. In the last step
    of the process, you will evaluate the predictions with a standard classification
    report and confusion matrix.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是准备一个Jupyter笔记本，每天使用一次来预测新主播的语言。记住，如果流在共享观众网络中很接近，它们很可能使用相同的语言。因此，你将使用一个基于同质性的方法在嵌入空间中对节点进行编码的节点嵌入模型。最简单且最广泛使用的节点嵌入模型之一是node2vec，你将在本例中使用它。一旦计算出节点嵌入，你将使用它们来训练一个基于已知语言训练示例的随机森林分类模型。在过程的最后一步，你将使用标准的分类报告和混淆矩阵来评估预测结果。
- en: To follow the examples, you need to have a Jupyter notebook environment ready
    and access to a Neo4j database. The database should be empty before starting this
    chapter. You will use the scikit-learn Python library to split the data, train
    the model, and evaluate the results, so make sure you have it installed. The notebook
    with all the code in this chapter is also available on GitHub ([https://github.com/tomasonjo/graphs-network-science](https://github.com/tomasonjo/graphs-network-science)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随示例，您需要一个准备好的Jupyter笔记本环境和访问Neo4j数据库的权限。在开始本章之前，数据库应该是空的。您将使用scikit-learn
    Python库来分割数据、训练模型并评估结果，所以请确保您已经安装了它。本章中所有代码的笔记本也可在GitHub上找到（[https://github.com/tomasonjo/graphs-network-science](https://github.com/tomasonjo/graphs-network-science)）。
- en: 9.2.1 Defining a connection to a Neo4j database
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 定义与Neo4j数据库的连接
- en: 'Start by opening a new Jupyter notebook, or download the filled-in notebook
    from the GitHub link in the previous paragraph. You will need to have the following
    three Python libraries installed to follow the code examples:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，打开一个新的Jupyter笔记本，或者从上一段中的GitHub链接下载已填充的笔记本。为了跟随代码示例，您需要安装以下三个Python库：pandas、scikit-learn和matplotlib。您可以使用pip或Conda包管理器安装所有这三个库。
- en: Neo4j
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neo4j
- en: pandas
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas
- en: scikit-learn
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn
- en: You can install all three libraries with pip or the Conda package manager.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用pip或Conda包管理器安装所有三个库。
- en: First, you need to define the connection to the Neo4j database.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要定义与Neo4j数据库的连接。
- en: Listing 9.1 Defining the connection to Neo4j
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.1 定义与Neo4j的连接
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Listing 9.1 imports the `GraphDatabase` object from the `neo4j` library. To
    establish the connection with the Neo4j database, you need to fill in and optionally
    change the credentials. Once the credentials are defined, pass them to the `driver`
    method of the `GraphDatabase` object. The driver allows you to spawn sessions
    in which you can execute arbitrary Cypher statements.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.1 从`neo4j`库中导入`GraphDatabase`对象。为了与Neo4j数据库建立连接，您需要填写并可选地更改凭据。一旦定义了凭据，就将它们传递给`GraphDatabase`对象的`driver`方法。驱动程序允许您在会话中执行任意的Cypher语句。
- en: Next, you will define a function that takes a Cypher statement as parameter
    and returns the results as a pandas dataframe, as shown in the following listing.
    The pandas dataframe is a convenient data structure that can be used to filter,
    transform, or easily integrate with other Python libraries.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将定义一个函数，该函数接受一个Cypher语句作为参数，并将结果作为pandas数据框返回，如下所示。pandas数据框是一个方便的数据结构，可以用于过滤、转换或轻松与其他Python库集成。
- en: Listing 9.2 Defining a function that executes an arbitrary Cypher statement
    and returns a pandas dataframe
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.2 定义一个执行任意Cypher语句并返回pandas数据框的函数
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 9.2.2 Importing a Twitch dataset
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 导入Twitch数据集
- en: Now that the environment is ready, you can circle back to the specified task.
    Remember, the data engineer on your team was kind enough to extract the information
    about the streams and chatters as well as perform the monopartite projection.
    They prepared two CSV files with relevant information. The first CSV file contains
    information about nodes in the network, as shown in table 9.1.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 环境准备就绪后，您可以回到指定的任务。记住，您的团队中的数据工程师足够友好，提取了有关流和聊天者的信息，并执行了单部分投影。他们准备了包含相关信息的两个CSV文件。第一个CSV文件包含网络中节点信息，如表9.1所示。
- en: Table 9.1 Node CSV structure
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.1 节点CSV结构
- en: '| `id` | `language` |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| `id` | `language` |'
- en: '| 129004176 | en |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 129004176 | en |'
- en: '| 50597026 | fr |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 50597026 | fr |'
- en: '| 102845970 | ko |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 102845970 | ko |'
- en: The node CSV contains information about stream IDs and their language. In this
    example, you have the language information for all the streams so that you can
    evaluate the classification model accuracy of the test data. It is good practice
    to define unique constraints on the unique properties of nodes to speed up the
    import. You will start by defining the unique constraint on the `id` property
    of the `Stream` nodes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 节点CSV包含有关流ID及其语言的信息。在本例中，您拥有所有流的语言信息，以便评估测试数据的分类模型准确率。定义节点唯一属性上的唯一约束以加快导入速度是良好的实践。您将首先定义`Stream`节点`id`属性上的唯一约束。
- en: Listing 9.3 Defining a constraint on `Stream` nodes
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.3 在`Stream`节点上定义约束
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Since you are working in a Python environment, you need to execute Cypher statements
    via the `run_query` function, which is defined in listing 9.2\. The function returns
    the output in a pandas dataframe format. Here, however, you are not interested
    in the result of the Cypher statement, so you don’t have to assign the output
    to a new variable.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你在一个 Python 环境中工作，你需要通过定义在列表 9.2 中的`run_query`函数执行 Cypher 语句。该函数以 pandas dataframe
    格式返回输出。然而，你对此 Cypher 语句的结果不感兴趣，因此不需要将输出分配给新变量。
- en: Now, you can go ahead and import the information about the Twitch streams and
    their languages. The CSV is available on GitHub, so you can utilize the `LOAD`
    `CSV` clause to retrieve and import the CSV information into the database, as
    shown in the following listing.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以继续导入有关 Twitch 流及其语言的信息。CSV 文件可在 GitHub 上找到，因此你可以使用`LOAD` `CSV`子句检索并导入
    CSV 信息到数据库，如下所示。
- en: Listing 9.4 Importing nodes
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.4 导入节点
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The relationship CSV file contains information about shared audiences between
    streams and their count, as shown in table 9.2.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 关系 CSV 文件包含有关流之间共享受众及其数量的信息，如表 9.2 所示。
- en: Table 9.2 Relationship CSV structure
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.2 关系 CSV 结构
- en: '| `source` | `target` | `weight` |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| `source` | `target` | `weight` |'
- en: '| 129004176 | 26490481 | 524 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 129004176 | 26490481 | 524 |'
- en: '| 26490481 | 213749122 | 54 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 26490481 | 213749122 | 54 |'
- en: '| 129004176 | 125387632 | 4591 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 129004176 | 125387632 | 4591 |'
- en: The relationship CSV contains three columns. The `source` and `target` columns
    contain the stream IDs that have a shared audience, while the `weight` column
    indicates how many shared users chatted in both streams. You can import the relationship
    information with the following Cypher statement.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 关系 CSV 包含三个列。`source`和`target`列包含具有共享受众的流 ID，而`weight`列表示在两个流中聊天的共享用户数量。你可以使用以下
    Cypher 语句导入关系信息。
- en: Listing 9.5 Importing relationships
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.5 导入关系
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Exercise 9.1
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 9.1
- en: Inspect how many, if any, `Stream` nodes have no incoming or outgoing relationships.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 检查有多少（如果有）`Stream`节点没有进入或出去关系。
- en: Luckily, there are no isolated nodes in the dataset. An *isolated node* is a
    node that has no incoming or outgoing relationships. When extracting node features
    from a dataset, always pay special attention to isolated nodes. For example, if
    there were some `Stream` nodes without any relationships, that would be a case
    of missing data. If you waited a few days, hopefully, someone would chat in their
    stream, and you would create new relationships for that stream so that it would
    not be isolated anymore. On the other hand, isolated `Stream` nodes can have any
    language. Since most node embedding algorithms encode isolated nodes identically,
    you would introduce noise to your classification model by including isolated nodes.
    Therefore, you would want to exclude all isolated nodes from the training and
    test datasets.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，数据集中没有孤立节点。一个**孤立节点**是指没有进入或出去关系的节点。在从数据集中提取节点特征时，始终要特别注意孤立节点。例如，如果有一些没有关系的`Stream`节点，那将是一个缺失数据的情况。如果你等待几天，希望有人会在他们的流中聊天，你就可以为该流创建新的关系，这样它就不会再是孤立的。另一方面，孤立的`Stream`节点可以是任何语言。由于大多数节点嵌入算法对孤立节点进行相同的编码，包含孤立节点会向你的分类模型引入噪声。因此，你希望从训练和测试数据集中排除所有孤立节点。
- en: On the other hand, if you are dealing with isolated nodes and the relationships
    are not missing, meaning that no new relationships will be formed in the future,
    you can include isolated nodes in your workflow. For example, imagine you were
    to predict a person’s net worth based on their network role and position. Suppose
    a person has no relationships and, therefore, no network influence. In that case,
    encoding isolated nodes could provide a vital signal to the machine learning model
    that predicts net worth. Always remember that most node embedding models encode
    isolated nodes identically. So if isolated nodes all belong to a single class,
    then considering them would make sense. However, if isolated nodes belong to various
    classes, then it would make sense to remove them from the model to remove noise.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你正在处理孤立节点，并且关系没有缺失，这意味着未来不会形成新的关系，你可以将孤立节点包含在你的工作流程中。例如，想象一下，如果你要根据一个人的网络角色和位置预测其净资产。假设一个人没有关系，因此没有网络影响力。在这种情况下，对孤立节点进行编码可以为预测净资产的机器学习模型提供关键信号。始终记住，大多数节点嵌入模型对孤立节点进行相同的编码。所以如果孤立节点都属于单个类别，那么考虑它们是有意义的。然而，如果孤立节点属于各种类别，那么从模型中移除它们以消除噪声是有意义的。
- en: 9.3 The node2vec algorithm
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 节点2vec算法
- en: Now that the graph is constructed, it is your job to encode nodes in the embedding
    space to be able to train the language prediction model based on the network position
    of the nodes. As mentioned, you will use the node2vec algorithm (Grover & Leskovec,
    2016) for this task. The node2vec algorithm is transductive and can be fine-tuned
    to capture either homophily- or role-based embeddings.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在图已经构建好了，你的任务是编码节点到嵌入空间中，以便能够根据节点的网络位置训练基于语言的预测模型。正如之前提到的，你将使用节点2vec算法（Grover
    & Leskovec, 2016）来完成这个任务。节点2vec算法是归纳性的，并且可以被微调以捕捉同质化或基于角色的嵌入。
- en: 9.3.1 The word2vec algorithm
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 word2vec算法
- en: 'The node2vec algorithm is heavily inspired by the word2vec (Mikolov et al.,
    2013) skip-gram model. Therefore, to properly understand node2vec, you must first
    understand how the word2vec algorithm works. Word2vec is a shallow, two-layer
    neural network that is trained to reconstruct linguistic contexts of words. The
    objective of the word2vec model is to produce word representation (vectors) given
    a text corpus. Word representations are positioned in the embedding space such
    that words that share common contexts in the text corpus are located close to
    one another in the embedding space. There are two main models used within the
    context of word2vec:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 节点2vec算法深受word2vec（Mikolov et al., 2013）skip-gram模型的影响。因此，为了正确理解节点2vec，你必须首先了解word2vec算法是如何工作的。Word2vec是一个浅层、两层的神经网络，经过训练以重建词的语用上下文。word2vec模型的目标是给定一个文本语料库产生词表示（向量）。词表示在嵌入空间中定位，使得在文本语料库中共享相同上下文的词在嵌入空间中彼此靠近。在word2vec的背景下使用了两种主要模型：
- en: Continuous bag-of-words (CBOW)
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续词袋（CBOW）
- en: Skip-gram model
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skip-gram模型
- en: Node2vec is inspired by the skip-gram model, so you will skip the CBOW implementation
    explanation. The skip-gram model predicts the context for a given word. The context
    is defined as the words adjacent to the input term.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Node2vec受到skip-gram模型的影响，因此你将跳过CBOW实现说明。skip-gram模型预测给定词的上下文。上下文定义为与输入项相邻的词。
- en: Figure 9.7 visualizes how training pairs of words are collected in a skip-gram
    model. Remember, the objective of the skip-gram model is to predict context words
    or words that frequently co-appear with a target word. The algorithm creates training
    pairs for every word in the text corpus by combining the particular word with
    its adjacent words. For example, in the third row of figure 9.7, you can observe
    that the word *grey* is highlighted and defined as the target word. The algorithm
    collects training samples by observing its adjacent or neighboring words, representing
    the context in which the word appears. In this example, two words to the left
    and the right of the highlighted word are considered when constructing the training
    pair samples. The maximum distance between words in the context window with the
    target word in the center is defined as the *window size*. The training pairs
    are then fed into a shallow, two-layer neural network.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7展示了在skip-gram模型中如何收集训练词对。记住，skip-gram模型的目标是预测上下文词或与目标词经常共现的词。算法通过将特定词与其相邻词结合为训练对，为文本语料库中的每个词创建训练对。例如，在图9.7的第三行中，你可以观察到单词*grey*被突出显示并定义为目标词。算法通过观察其相邻或邻近词来收集训练样本，代表该词出现的上下文。在这个例子中，当构建训练对样本时，考虑了突出词左右两侧的两个词。以目标词为中心的上下文窗口中词的最大距离定义为*窗口大小*。然后，训练对被输入到一个浅层、两层神经网络中。
- en: '![09-07](../../OEBPS/Images/09-07.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![09-07](../../OEBPS/Images/09-07.png)'
- en: Figure 9.7 The process of predicting the language for new streams
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 预测新流语言的过程
- en: Figure 9.8 visualizes the word2vec neural network architecture. Don’t worry
    if you have never seen or worked with neural networks. You simply need to know
    that during the training of this neural network, the input is a *one-hot-encoded*
    vector representing the input word, and the output is also a one-hot-encoded vector
    representing the context word.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8展示了word2vec神经网络的架构。如果你从未见过或使用过神经网络，请不要担心。你只需要知道，在训练这个神经网络的过程中，输入是一个表示输入词的*独热编码*向量，输出也是一个表示上下文词的独热编码向量。
- en: '![09-08](../../OEBPS/Images/09-08.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![09-08](../../OEBPS/Images/09-08.png)'
- en: Figure 9.8 Word2vec shallow neural network architecture
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 Word2vec浅层神经网络架构
- en: Most machine learning models cannot work directly with categorical values. Therefore,
    one-hot encoding is commonly applied to convert categorical values into numerical
    ones. For example, you can see that all the distinct categories in figure 9.9
    transformed into columns through the one-hot-encoding process. There are only
    three distinct categories in figure 9.9, so there are three columns in the one-hot-encoding
    output. Then, you can see that the category `Blue` is encoded as 1 under the `Blue`
    column and 0 under all other columns. Essentially, the numerical representation
    of the category `Blue` is [1,0,0]. Likewise, the numerical representation of `Yellow`
    is [0,0,1]. As you can observe, the one-hot-encoded vectors will have a single
    1 under the column of the category they belong to, while the other elements of
    the vectors are 0\. Consequently, the one-hot-encoding technique assures that
    the Euclidean distance between all classes is identical. While this is a straightforward
    technique, it is quite popular, as it allows the simple transformation of categorical
    values into numerical ones, which can then be fed into machine learning models.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习模型不能直接处理分类值。因此，通常使用一热编码将分类值转换为数值。例如，您可以看到图9.9中所有不同的类别通过一热编码过程转换成了列。图9.9中只有三个不同的类别，所以一热编码输出中有三个列。然后，您可以看到类别“蓝色”在“蓝色”列下编码为1，在其他所有列下编码为0。本质上，类别“蓝色”的数值表示为[1,0,0]。同样，类别“黄色”的数值表示为[0,0,1]。如您所观察到的，一热编码向量将在它们所属类别的列下有一个单独的1，而向量的其他元素都是0。因此，一热编码技术确保了所有类别之间的欧几里得距离是相同的。虽然这是一个简单的技术，但它非常流行，因为它允许将分类值简单地转换为数值，这些数值然后可以被输入到机器学习模型中。
- en: '![09-09](../../OEBPS/Images/09-09.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![09-09](../../OEBPS/Images/09-09.png)'
- en: Figure 9.9 One-hot-encoding technique transforming categorical values into numerical
    values
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9 一热编码技术将分类值转换为数值
- en: After the training step of the skip-gram model is finished, the neurons in the
    output layer represent the probability a word will be associated with the target
    word. Word2vec uses a trick where we aren’t interested in the output vector of
    the neural network but have the goal of learning the weights of the hidden layer.
    The weights of the hidden layer are actually the word embedding we are trying
    to learn. The number of neurons in the hidden layer will determine the *embedding
    dimension* or the size of the vector representing each word in the vocabulary.
    Note that the neural network does not consider the offset of the context word,
    so it does not differentiate between directly adjacent context words to the input
    and those more distant in the context window or even if the context word precedes
    or follows the target term. Consequently, the window size parameter has a significant
    influence on the results of the word embedding. For example, one study (Levy,
    2014) found that larger context window size tends to capture more topic or domain
    information. In contrast, smaller windows tend to capture more information about
    the word itself (e.g., what other words are functionally similar).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 跳字模型训练步骤完成后，输出层的神经元代表一个词与目标词关联的概率。Word2vec使用了一个技巧，我们并不关心神经网络的输出向量，而是希望学习隐藏层的权重。隐藏层的权重实际上是我们要学习的词嵌入。隐藏层中的神经元数量将决定*嵌入维度*或词汇表中每个词所表示的向量的大小。请注意，神经网络不考虑上下文词的偏移，因此它不会区分输入的直接相邻上下文词和上下文窗口中更远的上下文词，甚至不会区分上下文词在目标词之前还是之后。因此，窗口大小参数对词嵌入的结果有重大影响。例如，一项研究（Levy，2014）发现，较大的上下文窗口大小倾向于捕获更多的主题或领域信息。相比之下，较小的窗口倾向于捕获更多关于词本身的信息（例如，哪些其他词在功能上是相似的）。
- en: 9.3.2 Random walks
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 随机游走
- en: So what does word2vec have to do with node embeddings? The node2vec algorithm
    uses the skip-gram model under the hood; however, since you are not working with
    a text corpus in a graph, how do you define the training data? The answer is quite
    clever. Node2vec uses *random walks* to generate a corpus of “sentences” from
    a given network. Metaphorically, a random walk can be thought of as a drunk person
    traversing the graph. Of course, you can never be sure of an intoxicated person’s
    next step, but one thing is certain. A drunk person traversing the graph can only
    hop onto a neighboring node.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，word2vec与节点嵌入有什么关系呢？node2vec算法在底层使用skip-gram模型；然而，由于你在一个图中不是处理文本语料库，你怎么定义训练数据呢？答案是相当巧妙的。Node2vec使用*random
    walks*从给定的网络中生成“句子”语料库。比喻来说，随机游走可以想象为一个醉酒的人遍历图。当然，你永远无法确定一个醉酒的人下一步会去哪里，但有一点是确定的。遍历图的醉酒的人只能跳到相邻的节点。
- en: The node2vec algorithm uses random walks to produce the sentences, which can
    be used as input to the word2vec model. In figure 9.10, the random walk starts
    at node A and traverses to node H via nodes C, B, and F. The random walk length
    is decided arbitrarily and can be changed with the *walk length* parameter. Each
    node in the random walk is treated as a word in the sentence, where the size of
    the sentence is defined with the walk length parameter. Random walks start from
    all the nodes in the graph to make sure to capture all the nodes in the sentences.
    These sentences are then passed to the word2vec skip-gram model as training examples.
    That is the gist of the node2vec algorithm.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: node2vec算法使用随机游走来生成句子，这些句子可以用作word2vec模型的输入。在图9.10中，随机游走从节点A开始，通过节点C、B和F遍历到节点H。随机游走的长度是任意决定的，并且可以用*walk
    length*参数来改变。随机游走中的每个节点被视为句子中的一个单词，句子的长度由游走长度参数定义。随机游走从图中的所有节点开始，以确保捕获句子中的所有节点。然后，这些句子被传递到word2vec
    skip-gram模型作为训练示例。这就是node2vec算法的核心。
- en: '![09-10](../../OEBPS/Images/09-10.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![09-10](../../OEBPS/Images/09-10.png)'
- en: Figure 9.10 Using random walks to produce sentences
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10 使用随机游走来生成句子
- en: However, the node2vec algorithm implements second-order biased random walks.
    A step in the first-order random walk only depends on its current state.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，node2vec算法实现了二阶偏置随机游走。一阶随机游走的一步只依赖于其当前状态。
- en: Imagine you somehow wound up at node A in figure 9.11\. Because the first-order
    random walk only looks at its current state, the algorithm doesn’t know which
    node it was at in the previous step. Therefore, the probability of returning to
    a previous node or any other node is equal. There is no advanced math concept
    behind the calculation of probability. Node A has four neighbors, so the chance
    of traversing to any of them is 25% (1/4).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你 somehow wound up at node A in figure 9.11\. 因为一阶随机游走只看它的当前状态，算法不知道它之前在哪个节点。因此，返回到之前节点或任何其他节点的概率是相等的。计算概率没有复杂的数学概念。节点A有四个邻居，所以遍历到任何一个的概率是25%（1/4）。
- en: '![09-11](../../OEBPS/Images/09-11.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![09-11](../../OEBPS/Images/09-11.png)'
- en: Figure 9.11 First-order random walks
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11 首阶随机游走
- en: Suppose your graph is weighted, meaning each relationship has a property that
    stores its weight. In that case, those weights will be included in the calculation
    of the traversal probability.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的图是带权重的，这意味着每个关系都有一个属性来存储其权重。在这种情况下，这些权重将包含在遍历概率的计算中。
- en: In a weighted graph, the chance of traversing a particular connection is its
    weight divided by the sum of all neighboring weights. For example, the probability
    of traversing from node A to node E in figure 9.12 is 2 divided by 8 (25%) and
    the probability of traversing from node A to node D is 37.5%.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在带权重的图中，遍历特定连接的概率是其权重除以所有相邻权重之和。例如，在图9.12中从节点A遍历到节点E的概率是2除以8（25%），从节点A遍历到节点D的概率是37.5%。
- en: '![09-12](../../OEBPS/Images/09-12.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![09-12](../../OEBPS/Images/09-12.png)'
- en: Figure 9.12 First-order weighted random walks
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12 首阶加权随机游走
- en: On the other hand, second-order walks consider both the current as well as the
    previous state. To put it simply, when the algorithm calculates the traversal
    probabilities, it also considers where it was at the previous step.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，二阶游走考虑了当前状态以及之前的状态。简单来说，当算法计算遍历概率时，它也会考虑之前一步的位置。
- en: In figure 9.13, the walk just traversed from node D to node A in the previous
    step and is now evaluating its next move. The likelihood of backtracking the walk
    and immediately revisiting a node in the walk is controlled by the return parameter
    *p*. If the value of return parameter *p* is low, then the chance of revisiting
    node D is higher, keeping the random walk closer to the starting node of the walk.
    Conversely, setting a high value to parameter *p* ensures lower chances of revisiting
    node D and avoids two-hop redundancy in sampling. A higher value of parameter
    *p* also encourages moderate graph exploration.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 9.13 中，游走刚刚从节点 D 游走到节点 A 的上一步，现在正在评估其下一步移动。回溯游走并立即重新访问游走中节点的可能性由回退参数 *p*
    控制。如果回退参数 *p* 的值较低，那么重新访问节点 D 的可能性更高，使随机游走更接近游走的起始节点。相反，将参数 *p* 设置为高值确保重新访问节点
    D 的可能性较低，并避免在采样中的两跳冗余。参数 *p* 的更高值也鼓励适度的图探索。
- en: '![09-13](../../OEBPS/Images/09-13.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![09-13](../../OEBPS/Images/09-13.png)'
- en: Figure 9.13 Second-order random walks
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13 二阶随机游走
- en: The `inOut` parameter *q* allows the traversal calculation to differentiate
    between inward and outward nodes. Setting a high value for parameter *q* (*q*
    > 1) biases the random walk to move toward nodes closer to the node in the previous
    step. Looking at figure 9.13, if you set a high value for parameter *q*, the random
    walk from node A is more biased toward node B. Such walks obtain a local view
    of the underlying graph with respect to the starting node in the walk and approximate
    breadth-first search. In contrast, if the value of *q* is low (*q* < 1), the walk
    is more inclined to visit nodes further away from node D. In figure 9.13, nodes
    C and E are further away, since they are not neighbors of the node in the previous
    step. This strategy encourages outward exploration and approximates depth-first
    search.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`inOut` 参数 *q* 允许遍历计算区分内向和向外节点。将参数 *q* 设置为高值（*q* > 1）会使随机游走偏向于移动到上一步节点附近的节点。查看图
    9.13，如果你将参数 *q* 设置为高值，从节点 A 出发的随机游走会更偏向于节点 B。这样的游走可以获得相对于游走起始节点的底层图的一个局部视图，并近似广度优先搜索。相反，如果
    *q* 的值较低（*q* < 1），游走更倾向于访问距离节点 D 更远的节点。在图 9.13 中，节点 C 和 E 更远，因为它们不是上一步节点邻居。这种策略鼓励向外探索，并近似深度优先搜索。'
- en: Authors of the node2vec algorithm claim approximating depth-first search will
    produce more community- or homophily-based node embeddings. On the other hand,
    the breadth-first search strategy for random walks encourages structural role
    embeddings.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: node2vec 算法的作者声称，近似深度优先搜索将产生更多基于社区或同质性的节点嵌入。另一方面，随机游走的广度优先搜索策略鼓励结构角色嵌入。
- en: 9.3.3 Calculate node2vec embeddings
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3 计算节点2vec嵌入
- en: Now that you have a theoretical understanding of node embeddings and the node2vec
    algorithm, you will use it in a practical example. As mentioned, your task as
    a data scientist at Twitch is to predict the languages of new streamers based
    on shared audiences or chatters between different streams. The graph is already
    constructed, so you only need to execute the node2vec algorithm and train a classification
    model. As always, you first must project an in-memory graph. Relationships represent
    shared audiences between streams. When stream A shares an audience with stream
    B, that directly implies that stream B also shares an audience with stream A.
    Therefore, you can treat the relationships as undirected. Additionally, you know
    how many users were shared between a pair of streams, which you can represent
    as a relationship weight. Execute the following query to project an undirected
    weighted network of shared audiences between streams.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对节点嵌入和 node2vec 算法有了理论上的理解，你将在一个实际例子中使用它。正如之前提到的，作为 Twitch 的数据科学家，你的任务是根据不同流之间的共享观众或聊天者来预测新主播的语言。图已经构建完成，你只需要执行
    node2vec 算法并训练一个分类模型。一如既往，你首先必须投影一个内存图。关系代表流之间的共享观众。当流 A 与流 B 共享观众时，这直接意味着流 B
    也与流 A 共享观众。因此，你可以将这些关系视为无向的。此外，你知道一对流之间共享了多少用户，你可以将这个关系表示为关系权重。执行以下查询以投影流之间共享观众的无向加权网络。
- en: Listing 9.6 Projecting the in-memory graph of streams and their shared audience
    in-memory
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.6 在内存中投影流及其共享观众关系的内存图
- en: '[PRE5]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The Cypher statement in listing 9.6 projects an in-memory graph named `twitch`.
    To treat relationships as undirected, you must set the `orientation` parameter
    value to `UNDIRECTED`. The `properties` parameter of relationships can be used
    to define the relationship properties to be included in the projection.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.6中的Cypher语句投影了一个名为`twitch`的内存图。为了将关系视为无向的，你必须将`orientation`参数值设置为`UNDIRECTED`。关系`properties`参数可以用来定义要包含在投影中的关系属性。
- en: Finally, you can execute the node2vec algorithm. There are multiple parameters
    you could fine-tune to get the best results. However, hyperparameter optimization
    is not in the scope of this chapter. You will use a `embeddingDimension` parameter
    value of 8, which means that each node will be represented with a vector of eight
    elements. Next, you will define the `inOutFactor` parameter to be 0.5, which encourages
    more depth-first search walks and produces homophily-based embeddings. In this
    example, you are not interested in the structural roles of nodes and only want
    to encode how close they are in the graph. All the other parameters will be left
    at default values. Execute the following Cypher statement to execute the node2vec
    algorithm and store the results to the database.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以执行node2vec算法。你可以调整多个参数以获得最佳结果。然而，超参数优化不在此章的范围内。你将使用8的`embeddingDimension`参数值，这意味着每个节点将用一个包含八个元素的向量来表示。接下来，你将定义`inOutFactor`参数为0.5，这会鼓励更多的深度优先搜索遍历，并产生基于同质性的嵌入。在这个例子中，你对节点的结构角色不感兴趣，只想编码它们在图中的接近程度。所有其他参数都将保留默认值。执行以下Cypher语句以执行node2vec算法并将结果存储到数据库中。
- en: Listing 9.7 Calculating node2vec embeddings and storing them to the database
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.7 计算node2vec嵌入并将它们存储到数据库中
- en: '[PRE6]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 9.3.4 Evaluating node embeddings
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.4 评估节点嵌入
- en: Before you train the language classification model, you will evaluate the embedding
    results. You will start by examining the cosine and Euclidean distance of embeddings
    of pairs of nodes where a relationship is present. The cosine and Euclidean distance
    distribution can be calculated with Cypher and then visualized with the seaborn
    library.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在你训练语言分类模型之前，你将评估嵌入结果。你将首先检查存在关系的节点对的嵌入的余弦和欧几里得距离。余弦和欧几里得距离分布可以用Cypher计算，然后使用seaborn库进行可视化。
- en: Listing 9.8 Evaluating the cosine and Euclidean distance of embeddings of connected
    nodes
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.8 评估连接节点的嵌入的余弦和欧几里得距离
- en: '[PRE7]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The code in listing 9.8 produces the visualization in figure 9.14, which shows
    the distribution of cosine and Euclidean distance of embeddings between pairs
    of nodes where a relationship is present. With Euclidean distance, the lower the
    value is, the more similar or close the nodes are in the embedding space. You
    can observe that the top of the distribution is slightly lower than 1\. Most of
    the nodes are very similar based on Euclidean distance; however, there are some
    pairs of nodes where the distance is slightly larger. On the other hand, with
    cosine similarity, two nodes are very close in the embedding space when the value
    is close to 1\. Similarly, most pairs of nodes where the relationship is present
    have a cosine similarity close to 1\. So what happens when a pair of nodes have
    a relationship but their cosine similarity of embeddings is, for example, less
    than 0.5? Using the following code, you can investigate the dependence of cosine
    similarity between pairs of connected nodes based on their combined degree values.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.8中的代码生成了图9.14中的可视化，它显示了存在关系的节点对之间的嵌入的余弦和欧几里得距离分布。对于欧几里得距离，值越低，节点在嵌入空间中的相似度或接近度越高。你可以观察到分布的顶部略低于1。大多数节点基于欧几里得距离非常相似；然而，也有一些节点对，它们的距离略大。另一方面，对于余弦相似度，当值接近1时，两个节点在嵌入空间中非常接近。同样，大多数存在关系的节点对具有接近1的余弦相似度。那么当一对节点之间存在关系但它们的嵌入的余弦相似度，例如，小于0.5时会发生什么？使用以下代码，你可以调查基于它们的组合度值，连接节点对之间余弦相似度的依赖性。
- en: '![09-14](../../OEBPS/Images/09-14.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![09-14](../../OEBPS/Images/09-14.png)'
- en: Figure 9.14 Distribution of cosine and Euclidean distance of embeddings between
    pairs of nodes where a relationship is present
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14 存在关系的节点对之间嵌入的余弦和欧几里得距离分布
- en: Listing 9.9 Evaluating the dependence of cosine similarity to the combined node
    degree values
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.9 评估余弦相似度对组合节点度值的依赖性
- en: '[PRE8]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The code in listing 9.9 produces the visualization in figure 9.15, where you
    can clearly see that the more connections a node has, on average, the less similar
    it is to its neighbors. That makes sense in a way. Imagine if you only had one
    friend; you might be almost identical to them in several meaningful ways. However,
    when you have 100 friends, you can’t be identical to all of them. You can pick
    some attributes from each friend that you share, but it is practically impossible
    to be identical to all of them, unless they are also all identical.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.9 中的代码生成了图 9.15 中的可视化，你可以清楚地看到，一个节点拥有的连接越多，平均来说，它与其邻居的相似度就越低。这在某种程度上是有道理的。想象一下，如果你只有一个朋友，你可能在几个重要的方面几乎与他们完全相同。然而，当你有
    100 个朋友时，你不可能与他们都完全相同。你可以从每个朋友那里挑选一些你共有的属性，但要做到与所有朋友都完全相同实际上是不可能的，除非他们也都是相同的。
- en: '![09-15](../../OEBPS/Images/09-15.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![09-15](../../OEBPS/Images/09-15.png)'
- en: Figure 9.15 Distribution of average cosine similarity of connected nodes based
    on the combined node degree values
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15 基于组合节点度值的连接节点平均余弦相似度的分布
- en: You have also specified the relationship weight to calculate the node2vec embeddings.
    The higher the relationship weight is, the more biased the random walk is to traverse
    it. You can examine how the cosine similarity of connected nodes is dependent
    on the relationship weight with the following code.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你还指定了关系权重来计算 node2vec 嵌入。关系权重越高，随机游走越倾向于遍历它。你可以通过以下代码检查连接节点的余弦相似度如何依赖于关系权重。
- en: Listing 9.10 Evaluating the dependence of cosine similarity of connected nodes
    to the relationship weight
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.10 评估连接节点余弦相似度对关系权重的依赖
- en: '[PRE9]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The code in listing 9.10 produces the visualization in figure 9.16, which shows
    the distributions of Euclidean and cosine similarities between a connected pair
    of nodes.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.10 中的代码生成了图 9.16 中的可视化，它显示了连接节点对之间的欧几里得和余弦相似度的分布。
- en: '![09-16](../../OEBPS/Images/09-16.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![09-16](../../OEBPS/Images/09-16.png)'
- en: Figure 9.16 The distribution of average cosine similarity of connected nodes
    based on the combined node degree values
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16 基于组合节点度值的连接节点平均余弦相似度的分布
- en: Again, you can distinctly observe the dependence of the cosine similarity of
    connected nodes to the relationship weight. The higher the relationship weight
    is, the more likely the random walk is to traverse it. Consequently, the more
    often a pair of nodes appear closely in the random walk, the more likely their
    embeddings are to be more similar. When a relationship weight is lower, the random
    walk is biased not to traverse it. Therefore, you can observe some examples where
    the embeddings are not similar at all, even when there is a relationship between
    a pair of nodes. One would assume that the pair of nodes are not connected when
    the cosine distance of their embeddings is close to zero. However, it might simply
    be the case that the random walk is biased in a way to avoid traversing the relationship
    between the two nodes.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，你可以清楚地观察到连接节点的余弦相似度与关系权重的依赖关系。关系权重越高，随机游走越有可能遍历它。因此，当一对节点在随机游走中频繁地紧密出现时，它们的嵌入越有可能更相似。当关系权重较低时，随机游走会倾向于不遍历它。因此，你可以观察到一些示例，即使一对节点之间存在关系，它们的嵌入也可能完全不相似。当一对节点的嵌入的余弦距离接近零时，人们可能会假设这对节点没有连接。然而，这可能是随机游走倾向于避免遍历这两个节点之间关系的一种偏见。
- en: 9.3.5 Training a classification model
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.5 训练分类模型
- en: In the final section of this chapter, you will train a classification model
    to predict the languages of new streamers. First, you must retrieve the relevant
    data from the database and make the required preprocessing.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后部分，你将训练一个分类模型来预测新流式的语言。首先，你必须从数据库中检索相关数据并进行必要的预处理。
- en: Listing 9.11 Retrieving and preprocessing relevant data for classification training
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.11 为分类训练检索和预处理相关数据
- en: '[PRE10]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Encodes languages to be represented as integers
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将要表示为整数的语言进行编码
- en: The code in listing 9.11 begins by retrieving the data from the database. A
    simple Cypher statement returns stream ID, language, and node embeddings. Since
    the languages are represented as strings, you need to map or encode them as integers.
    You can easily encode categorical values, such as languages, to integers with
    the `pd.factorize` method. After this step, the dataframe should have the structure
    shown in table 9.3.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.11 中的代码首先从数据库中检索数据。一个简单的 Cypher 语句返回流 ID、语言和节点嵌入。由于语言以字符串形式表示，你需要将它们映射或编码为整数。你可以使用
    `pd.factorize` 方法轻松地将分类值，如语言，编码为整数。在此步骤之后，数据框的结构应如图 9.3 所示。
- en: Table 9.3 Pandas dataframe structure
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.3 Pandas 数据框结构
- en: '| `streamId` | `language` | `embedding` | `output` |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| `streamId` | `language` | `embedding` | `output` |'
- en: '| 129004176 | en | [-0.952458918094635,...] | 0 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 129004176 | en | [-0.952458918094635,...] | 0 |'
- en: '| 50597026 | fr | [-0.25458356738090515,...] | 1 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 50597026 | fr | [-0.25458356738090515,...] | 1 |'
- en: '| 102845970 | ko | [-1.3528306484222412,... ] | 2 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 102845970 | ko | [-1.3528306484222412,... ] | 2 |'
- en: In table 9.3, you can observe that the `pd.factorize` method encoded the English
    language under 0\. The French language is mapped to 1 and so on.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 9.3 中，你可以观察到 `pd.factorize` 方法将英语语言编码为 0。法语语言映射为 1，依此类推。
- en: The `embedding` column contains vectors or lists representing each data point.
    So the input to the classification model will be the `embedding` model, and you
    will train it to predict the integer under the `output` column. In this example,
    you will use the random forest classifier from the scikit-learn library. As with
    all machine learning training, you must split your data into training and test
    sets. You will use the `train_ test_split` to produce the train and test portions
    of the dataset. Execute the following code to train a random forest classification
    model to predict languages of new streams.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`embedding` 列包含表示每个数据点的向量或列表。因此，分类模型的输入将是 `embedding` 模型，你将训练它来预测 `output`
    列下的整数。在这个例子中，你将使用 scikit-learn 库中的随机森林分类器。与所有机器学习训练一样，你必须将你的数据分割成训练集和测试集。你将使用
    `train_test_split` 来生成数据集的训练和测试部分。执行以下代码以训练一个随机森林分类模型来预测新流的语言。'
- en: Listing 9.12 Splitting the dataset and training the random forest model classifier
    based on the training portion of the dataset
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.12 基于数据集的训练部分分割数据集并训练随机森林模型分类器
- en: '[PRE11]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 9.3.6 Evaluating predictions
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.6 评估预测
- en: The last thing you will do in this chapter is evaluate the model on the test
    data. You will begin by examining the *classification report*. A classification
    report is used to measure the quality of predictions from a machine learning model.
    Execute the following code to produce the classification report.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将做的最后一件事是在测试数据上评估模型。你将从检查*分类报告*开始。分类报告用于衡量机器学习模型预测的质量。执行以下代码以生成分类报告。
- en: Listing 9.13 Producing the classification report
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.13 生成分类报告
- en: '[PRE12]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The code in listing 9.13 produces the report in figure 9.17, where you can observe
    that you are dealing with an unbalanced dataset, as there are 384 test data points
    for the English language and only 54 examples of French streams. Additionally,
    the language mapped under number 9 is Italian and has only 19 test data points.
    When dealing with unbalanced datasets, it makes sense to examine the F1 score.
    Both the F1 score and the weighted F1 score are 0.91, which is a great result.
    The hypothesis that chatters usually chat in streams that share the same language
    is valid.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.13 中的代码生成了图 9.17 中的报告，你可以观察到你正在处理一个不平衡的数据集，因为英语有 384 个测试数据点，而法语流只有 54 个示例。此外，编号
    9 下映射的语言是意大利语，只有 19 个测试数据点。在处理不平衡数据集时，检查 F1 分数是有意义的。F1 分数和加权 F1 分数都是 0.91，这是一个很好的结果。假设聊天者通常在共享相同语言的流中进行聊天的假设是有效的。
- en: '![09-17](../../OEBPS/Images/09-17.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![09-17](../../OEBPS/Images/09-17.png)'
- en: Figure 9.17 Classification report
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.17 分类报告
- en: Finally, you will produce the *confusion matrix*. The confusion matrix can help
    you evaluate actual versus predicted classes of data points. Execute the following
    code to visualize the confusion matrix.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将生成*混淆矩阵*。混淆矩阵可以帮助你评估数据点的实际与预测类别。执行以下代码以可视化混淆矩阵。
- en: Listing 9.14 Calculating the tweet count and retweet ratio for each user
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.14 计算每个用户的推文计数和转发率
- en: '[PRE13]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The code in listing 9.14 produces the visualization in figure 9.18\. Remember,
    the English language is mapped to 0\. By examining the confusion matrix in figure
    9.18, you can observe that the model only misclassified between English and other
    languages. For example, the model never incorrectly classified Korean as Portuguese.
    This makes sense, as English is the most common language on the internet, so many
    people can speak at least a bit of English in addition to their native language.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.14 中的代码生成了图 9.18 中的可视化。记住，英语语言映射到 0。通过检查图 9.18 中的混淆矩阵，你可以观察到模型仅在英语和其他语言之间发生错误分类。例如，模型从未将韩语错误分类为葡萄牙语。这很有道理，因为英语是互联网上最常用的语言，所以除了他们的母语外，许多人至少会说一点英语。
- en: '![09-18](../../OEBPS/Images/09-18.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![09-18](../../OEBPS/Images/09-18.png)'
- en: Figure 9.18 Confusion matrix
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.18 混淆矩阵
- en: Exercise 9.2
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 练习题 9.2
- en: Try out various configurations of the node2vec algorithm to observe how it affects
    the cosine distance between embeddings of connected nodes and the accuracy of
    the classification model. You can remove the relationship weight parameter to
    observe how the unweighted variant of the node2vec algorithm behaves or fine-tune
    the `embeddingDimension`, `inOutFactor`, and `returnFactor` parameters. Check
    out the official documentation ([http://mng.bz/lVXo](http://mng.bz/lVXo)) for
    the complete list of node2vec parameters.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试不同的 node2vec 算法配置，以观察它如何影响连接节点嵌入之间的余弦距离和分类模型的准确性。你可以移除关系权重参数来观察 node2vec 算法的无权重版本的行为，或者微调
    `embeddingDimension`、`inOutFactor` 和 `returnFactor` 参数。查看官方文档 ([http://mng.bz/lVXo](http://mng.bz/lVXo))
    以获取 node2vec 参数的完整列表。
- en: Congratulations! You have successfully trained your first node classification
    model based on node2vec embeddings. Remember, since node2vec is a transductive
    model, you should retrain the model when new nodes are added to the graph. Therefore,
    models like node2vec are used in batch processing pipelines that run, for example,
    once a day.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经成功训练了第一个基于 node2vec 嵌入的节点分类模型。记住，由于 node2vec 是一个归纳模型，当图中添加新节点时，你应该重新训练模型。因此，像
    node2vec 这样的模型通常用于每天运行一次的批量处理管道中。
- en: 9.4 Solutions to exercises
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 练习题解答
- en: The solution to exercise 9.1 is as follows.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 练习题 9.1 的解答如下。
- en: Listing 9.15 Counting the `Stream` nodes with no incoming or outgoing relationships
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.15 计算没有传入或传出关系的 `Stream` 节点
- en: '[PRE14]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Summary
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Node embedding models use a dimensionality reduction technique to produce node
    representations of arbitrary sizes.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点嵌入模型使用降维技术来生成任意大小的节点表示。
- en: Node embedding models can encode nodes based on their structural roles in the
    network or can follow a more homophily-based design.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点嵌入模型可以根据网络中的结构角色编码节点，或者可以采用更基于同质性的设计。
- en: Some of the node embedding models are transductive, meaning they cannot produce
    embeddings for nodes not seen during training.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些节点嵌入模型是归纳性的，这意味着它们不能为训练期间未看到的节点生成嵌入。
- en: The node2vec algorithm is inspired by the word2vec skip-gram model.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: node2vec 算法受到 word2vec 跳字模型（skip-gram model）的启发。
- en: The node2vec algorithm uses random walks to produce sentences, which are then
    fed into the skip-gram model.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: node2vec 算法使用随机游走来生成句子，然后将其输入到 skip-gram 模型中。
- en: Second-order random walks consider the previous step of the random walk when
    calculating the next traversal possibilities.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二阶随机游走在计算下一个遍历可能性时会考虑随机游走的上一步。
- en: Node2vec can be fine-tuned to produced embeddings based on node structural roles
    or homophily.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Node2vec 可以微调以生成基于节点结构角色或同质性的嵌入。
- en: The embedding dimension parameter defines the size of the vector that represents
    nodes.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入维度参数定义了表示节点的向量的大小。
- en: Pay special attention to isolated nodes when using node embedding models. Most
    node embedding models encode isolated nodes identically, so ensure that aligns
    with your task requirements.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用节点嵌入模型时，请特别注意孤立节点。大多数节点嵌入模型对孤立节点进行相同的编码，因此请确保它们符合你的任务要求。
- en: Node classification is a task of predicting a property or label of a node based
    on its network features.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点分类是一个基于节点网络特征预测节点属性或标签的任务。
