- en: 7 Processing data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 处理数据
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Accessing large-amounts of cloud-based data quickly
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速访问大量基于云的数据
- en: Using Apache Arrow for efficient, in-memory data processing
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Apache Arrow进行高效的内存数据处理
- en: Leveraging SQL-based query engines to preprocess data for workflows
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用基于SQL的查询引擎对工作流程数据进行预处理
- en: Encoding features for models at scale
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为大规模模型编码特征
- en: 'The past five chapters covered how to take data science projects from prototype
    to production. We have learned how to build workflows, use them to run computationally
    demanding tasks in the cloud, and deploy the workflows to a production scheduler.
    Now that we have a crisp idea of the prototyping loop and interaction with production
    deployments, we can return to the fundamental question: how should the workflows
    consume and produce data?'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 前五章介绍了如何将数据科学项目从原型发展到生产。我们学习了如何构建工作流程，使用它们在云中运行计算密集型任务，并将工作流程部署到生产调度器。现在我们已经对原型循环和生产部署的交互有了清晰的认识，我们可以回到基本问题：工作流程应该如何消费和产生数据？
- en: Interfacing with data is a key concern of all data science applications. Every
    application needs to find and read input data that is stored somewhere. Often,
    the application is required to write its outputs, such as fresh predictions, to
    the same system. Although a huge amount of variation exists among systems for
    managing data, in this context we use a common moniker, *data warehouse*, to refer
    to all of them. Given the foundational nature of data inputs and outputs, it feels
    appropriate to place the concern at the very bottom of the stack, as depicted
    in figure 7.1.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据交互是所有数据科学应用的关键关注点。每个应用都需要找到并读取存储在某处的输入数据。通常，应用还需要将其输出，如新的预测，写入到同一个系统。尽管在管理数据的不同系统中存在大量的差异，但在这种情况下，我们使用一个常见的术语，*数据仓库*，来指代所有这些系统。鉴于数据输入和输出的基础性质，将这一关注点置于堆栈的底部似乎是合适的，如图7.1所示。
- en: '![CH07_F01_Tuulos](../../OEBPS/Images/CH07_F01_Tuulos.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F01_Tuulos](../../OEBPS/Images/CH07_F01_Tuulos.png)'
- en: Figure 7.1 The stack of effective data science infrastructure
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 有效的数据科学基础设施堆栈
- en: 'At the top of the stack resides another question related to data: how should
    the data scientist explore, manipulate, and prepare the data to be fed into models?
    This process is commonly called *feature engineering*. This chapter focuses on
    data at the bottom as well as the top of the stack, although we are biased toward
    the lower-level concerns, which are more clearly in the realm of generalized infrastructure.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在堆栈的顶部存在另一个与数据相关的问题：数据科学家应该如何探索、操作和准备要输入到模型中的数据？这个过程通常被称为*特征工程*。本章关注堆栈的底部和顶部数据，尽管我们更倾向于底层关注点，这些更明显地属于通用基础设施领域。
- en: Notably, this chapter is not about building or setting up data warehouses, which
    is a hugely complex topic of its own, covered by many other books. We assume that
    you have some kind of a data warehouse, that is, some way to store data, already
    in place. Depending on the size of your company, the nature of the data warehouse
    can vary widely, as depicted in figure 7.2.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，本章不是关于构建或设置数据仓库的，这是一个极其复杂的话题，由许多其他书籍涵盖。我们假设你已经有一些形式的数据仓库，也就是说，某种存储数据的方式已经就位。根据你公司的规模，数据仓库的性质可能会有很大的不同，如图7.2所示。
- en: '![CH07_F02_Tuulos](../../OEBPS/Images/CH07_F02_Tuulos.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F02_Tuulos](../../OEBPS/Images/CH07_F02_Tuulos.png)'
- en: Figure 7.2 Various data infrastructures across companies from tiny to large
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 从小型到大型公司各种数据基础设施
- en: If you are just prototyping, you can get going by using local files, for example,
    CSVs loaded using IncludeFile, which was featured in chapter 3\. Most companies
    use a proper database such as Postgres to store their precious data assets. A
    medium-sized company might use multiple databases for different purposes, possibly
    accompanied by a federated query engine like Trino (aka Presto), which provides
    a unified way to query all data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只是进行原型设计，你可以通过使用本地文件开始，例如，使用第3章中介绍过的IncludeFile加载的CSV文件。大多数公司使用像Postgres这样的数据库来存储它们宝贵的数据资产。中等规模的公司可能会使用多个数据库来满足不同的需求，可能还会配备一个联邦查询引擎，如Trino（也称为Presto），它提供了一种统一的方式来查询所有数据。
- en: 'A large company might have a cloud-based data lake with multiple query engines,
    like Apache Flink for real-time data, Apache Iceberg for metadata management,
    and Apache Spark for general data processing. Don’t worry if these systems are
    not familiar to you—we will give a high-level overview of the modern data architectures
    in section 7.2.1\. In all these cases, data scientists face the same key question:
    how to access data in their workflows.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一家大型公司可能有一个基于云的数据湖，拥有多个查询引擎，如Apache Flink用于实时数据，Apache Iceberg用于元数据管理，以及Apache
    Spark用于通用数据处理。如果你对这些系统不熟悉，请不要担心——我们将在第7.2.1节中给出现代数据架构的高级概述。在这些所有情况下，数据科学家面临相同的关键问题：如何在他们的工作流程中访问数据。
- en: 'Besides having to integrate with different technical solutions, often the data
    science infrastructure needs to support different *data modalities* as well. The
    examples in this chapter mainly focus on *structured data*, that is, relational
    or tabular data sources, which are the most common data modality for business
    applications. In addition, the infrastructure may need to support applications
    dealing with *unstructured data*, like text and images. In practice, these days
    many real-world datasets are something in between—they are *semi-structured.*
    They contain some structure, such as columns strictly adhering to a schema, and
    some unstructured fields, for example, JSON or free-form text. This chapter focuses
    on data-related concerns that are common across data warehouses and data modalities,
    namely the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 除了需要与不同的技术解决方案集成外，数据科学基础设施还需要支持不同的 *数据模式*。本章的例子主要关注 *结构化数据*，即关系型或表格数据源，这是商业应用中最常见的数据模式。此外，基础设施可能还需要支持处理
    *非结构化数据* 的应用程序，如文本和图像。在实践中，这些天许多现实世界的数据集介于两者之间——它们是 *半结构化* 的。它们包含一些结构，例如严格遵循模式的列，以及一些非结构化字段，例如JSON或自由格式文本。本章重点关注数据仓库和数据模式中普遍存在的数据相关问题，具体如下：
- en: '*Performance*—Given that data science applications tend to be data-intensive,
    that is, they may need to ingest large amounts of data, loading data can easily
    become a bottleneck in workflows. Having to wait for potentially tens of minutes
    or longer for data can make the prototyping loop quite painful, which we want
    to avoid. We will focus on this question in section 7.1.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*性能*—鉴于数据科学应用程序往往数据密集型，也就是说，它们可能需要摄入大量数据，加载数据很容易成为工作流程中的瓶颈。等待可能长达数十分钟或更长时间的数据可能会使原型设计循环非常痛苦，这是我们想要避免的。我们将在第7.1节中关注这个问题。'
- en: '*Data selection*—How to find and select subsets of data that are relevant for
    the task. SQL is the lingua franca of selecting and filtering data, so we need
    to find ways to interface with query engines like Spark that can execute SQL.
    These solutions can often be applied to semi-structured data, too, or for metadata
    referring to unstructured data. These topics are the theme of section 7.2.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*数据选择*—如何找到和选择与任务相关的数据子集。SQL是选择和过滤数据的通用语言，因此我们需要找到与能够执行SQL的查询引擎（如Spark）接口的方法。这些解决方案通常也适用于半结构化数据，或者用于非结构化数据的元数据。这些主题是第7.2节的主题。'
- en: '*Feature engineering*—How to transform raw data to a format suitable for modeling,
    aka feature transformations. Once we have ingested a chunk of raw data, we need
    to address many concerns before the data can be input to models effectively. We
    will scratch the surface of this deep topic in section 7.3.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*特征工程*—如何将原始数据转换为适合建模的格式，也就是特征转换。一旦我们摄入了一块原始数据，在数据能够有效地输入模型之前，我们需要解决许多问题。我们将在第7.3节中探讨这个深奥的主题。'
- en: These foundational concerns apply to all environments. This chapter gives you
    concrete building blocks that you can use to design data access patterns, and
    maybe helper libraries of your own, which apply to your particular environment.
    Alternatively, you may end up using some higher-level libraries and products,
    such as *feature stores* for feature engineering, that abstract away many of these
    concerns. After learning the fundamentals, you will be able to evaluate and use
    such abstractions more effectively, as we will cover in section 7.3.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基础问题适用于所有环境。本章为你提供了具体的构建块，你可以使用它们来设计数据访问模式，也许还有你自己的辅助库，这些库适用于你的特定环境。或者，你最终可能会使用一些更高级的库和产品，例如用于特征工程的
    *特征存储*，它抽象了许多这些问题。在了解了基础知识之后，你将能够更有效地评估和使用这些抽象，正如我们将在第7.3节中讨论的那样。
- en: Another orthogonal dimension of data access is how frequently the application
    needs to react to changes in data. Similar to earlier chapters, we focus on *batch
    processing*, that is, applications that need to run, say, at most once in every
    15 minutes. The topic of *streaming data* is certainly relevant for many data
    science applications that need more frequent updates, but the infrastructure required
    to do this is more complex. We will briefly touch on the topic in the next chapter.
    As we will discuss in section 7.2, surprisingly many data science applications
    that involve real-time data can be still modeled as batch workflows.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据访问的另一个正交维度是应用程序需要多频繁地响应数据变化。与前面的章节类似，我们关注*批量处理*，即最多每15分钟运行一次的应用程序。对于需要更频繁更新的许多数据科学应用来说，*流数据*的话题当然也很相关，但进行此类操作所需的基础设施更为复杂。我们将在下一章简要介绍这个话题。正如我们将在第7.2节中讨论的，许多涉及实时数据的数据科学应用仍然可以建模为批量工作流程。
- en: 'On top of all these dimensions we have *organizational concerns*: who should
    be responsible for data used by data science applications, and how are responsibilities
    divided between different roles—data engineers and data scientists in particular.
    Although the exact answers are highly company specific, we share some high-level
    ideas in section 7.2.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些维度之上，我们还有*组织上的关注点*：谁应该对数据科学应用中使用的数据负责，以及责任如何在不同的角色之间分配——特别是数据工程师和数据科学家。尽管确切答案高度依赖于公司，但在第7.2节中我们分享了一些高级观点。
- en: 'We will start the chapter with a fundamental technical question: how to load
    data efficiently in workflows. The tools introduced in the next section give a
    solid foundation for the rest of the chapter. You can find all code listings for
    this chapter at [http://mng.bz/95zo](http://mng.bz/95zo).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将本章从一项基本技术问题开始：如何在工作流程中有效地加载数据。下一节中介绍的工具为本章的其余部分提供了坚实的基础。你可以在此处找到本章的所有代码列表：[http://mng.bz/95zo](http://mng.bz/95zo)。
- en: 7.1 Foundations of fast data
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 快速数据的基础
- en: '*Alex developed a workflow that estimates the delivery time of cupcake orders.
    To train the estimator, Alex needs to ingest all historical cupcake orders from
    the company’s main data warehouse. Surprisingly, loading the data from the database
    takes longer than building the model itself! After looking into the problem, Bowie
    realized that machine learning workflows need a faster way to access data than
    the previous path used mainly by dashboards. The new fast data path boosted Alex’s
    productivity massively: it is now possible to train and test at least 10 versions
    of the model daily instead of just two.*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*亚历克斯开发了一个工作流程，用于估算纸杯蛋糕订单的配送时间。为了训练估算器，亚历克斯需要从公司的主要数据仓库中摄取所有历史纸杯蛋糕订单。令人惊讶的是，从数据库加载数据所需的时间比构建模型本身还要长！经过调查问题后，鲍伊意识到机器学习工作流程需要比之前主要由仪表板使用的路径更快的数据访问方式。新的快速数据路径极大地提高了亚历克斯的生产力：现在每天可以训练和测试至少10个模型版本，而不是仅仅两个。*'
- en: '![CH07_F02_UN01_Tuulos](../../OEBPS/Images/CH07_F02_UN01_Tuulos.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F02_UN01_Tuulos](../../OEBPS/Images/CH07_F02_UN01_Tuulos.png)'
- en: 'When the author of this book did an informal survey of data scientists at Netflix,
    asking what the biggest pain point they face on a day-to-day basis is, a majority
    responded: finding suitable data and accessing it in their data science applications.
    We will come back to the question of finding data in the next section. This section
    focuses on the seemingly simple question: how should you load a dataset from a
    data warehouse to your workflow?'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当本书的作者对Netflix的数据科学家进行非正式调查，询问他们在日常工作中面临的最大痛点是什么时，大多数人回答：寻找合适的数据并在他们的数据科学应用中访问它。我们将在下一节回到寻找数据的问题。本节重点探讨一个看似简单的问题：你应该如何将数据集从数据仓库加载到你的工作流程中？
- en: The question may seem quite tactical, but it has far-reaching, strategic implications.
    For the sake of discussion, consider that you couldn’t load a dataset easily and
    quickly (or not at all) from a data warehouse to a separate workflow. By necessity,
    you would have to build models and other application logic *inside* the data warehouse
    system.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可能看起来非常战术性，但它具有深远、战略性的影响。为了讨论方便，假设你无法轻松快速（或根本无法）将数据集从数据仓库加载到单独的工作流程中。出于必要性，你将不得不在数据仓库系统中*内部*构建模型和其他应用程序逻辑。
- en: 'In fact, this has been the traditional way of thinking about data warehouses:
    bulk data is not supposed to be moved out. Instead, applications express their
    data processing needs, in SQL, for example, which the data warehouse executes,
    returning a tiny subset of data as results. Although this approach makes sense
    for traditional business intelligence, it is not a feasible idea to build machine
    learning models in SQL. Even if your data warehouse supports querying data with
    Python, for example, the fundamental issue is that the approach tightly couples
    the *compute layer*, which we discussed in chapter 4, with the data layer. This
    is problematic when workloads are very compute intensive: no mainstream database
    has been designed to run, say, on an autoscaling cluster of GPU-instances.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这一直是关于数据仓库的传统思维方式：大量数据不应该被移动出去。相反，应用程序通过SQL等表达他们的数据处理需求，数据仓库执行这些需求，并返回作为结果的小数据子集。尽管这种方法对于传统的商业智能来说是有道理的，但在SQL中构建机器学习模型并不是一个可行的想法。即使你的数据仓库支持使用Python等查询数据，基本问题仍然是这种方法将*计算层*，我们在第4章中讨论过的，与数据层紧密耦合。当工作负载非常计算密集时，这是一个问题：没有主流数据库是为在自动扩展的GPU实例集群上运行而设计的。
- en: In contrast, if it is feasible to extract bulk data from the warehouse efficiently,
    it becomes possible to *decouple data and compute*. This is great for data science
    applications that tend to be both data- and compute-hungry. As advocated for in
    chapter 4, you can choose the best compute layer for each job, and, most important,
    you can let data scientists iterate and experiment freely without having to fear
    crashing a shared database. A downside is that it becomes harder to control how
    data is used—we will return to this question in the next section.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果能够有效地从仓库中提取大量数据，就可以实现*数据与计算的解耦*。这对于既需要数据又需要计算的数据科学应用来说是个好消息。正如在第4章中提倡的，你可以为每个任务选择最佳的计算层，最重要的是，你可以让数据科学家自由迭代和实验，而无需担心崩溃共享数据库。一个缺点是，控制数据使用变得更加困难——我们将在下一节回到这个问题。
- en: When considering the pros and cons of the coupled versus decoupled approach,
    it is good to remember that data science applications—and machine learning, in
    particular—behave differently than traditional analytics and business intelligence
    use cases. The difference is illustrated in figure 7.3.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑耦合与解耦方法的优缺点时，记住数据科学应用——尤其是机器学习——的行为与传统分析和商业智能用例不同是很重要的。这种差异在图7.3中得到了说明。
- en: '![CH07_F03_Tuulos](../../OEBPS/Images/CH07_F03_Tuulos.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F03_Tuulos](../../OEBPS/Images/CH07_F03_Tuulos.png)'
- en: Figure 7.3 Contrasting data flows between analytics and ML applications
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 对比分析和机器学习应用之间的数据流
- en: 'A traditional business analytics application, say, a Tableau dashboard, typically
    generates a very complex SQL query, which the data warehouse executes to return
    a small, carefully filtered result to the dashboard. In contrast, an ML application
    shows the opposite behavior: it presents a simple query to ingest, for instance,
    a full table of data, select * from table, which it feeds into an ML model.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个传统的商业分析应用，比如一个Tableau仪表板，通常生成一个非常复杂的SQL查询，数据仓库执行这个查询以返回一个小的、仔细过滤的结果给仪表板。相反，一个机器学习应用表现出相反的行为：它呈现一个简单的查询来摄取，例如，一个完整的数据表，select
    * from table，然后将其输入到机器学习模型中。
- en: Consequently, ML applications may hit two issues. First, the data warehouse
    may be surprisingly inefficient at executing simple queries that extract a lot
    of data, because they have been optimized for the opposite query pattern. Second,
    for the same reason, client libraries that are used to interface with the data
    warehouse are often rather inefficient at loading bulk data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，机器学习应用可能会遇到两个问题。首先，数据仓库在执行提取大量数据的简单查询时可能出奇地低效，因为这些查询已经被优化为相反的模式。其次，由于同样的原因，用于与数据仓库接口的客户端库在加载大量数据时通常效率很低。
- en: Although many real-world applications exhibit query patterns that lie somewhere
    between the two extremes, in many applications, loading data is the main performance
    bottleneck. The data scientist may need to wait for tens of minutes for the data
    to load, which seriously hurts their productivity. Removing productivity bottlenecks
    like this is a key goal of effective data science infrastructure, so in the subsequent
    sections, we will explore an alternative, extremely efficient way of accessing
    data. The approach works with many modern data warehouses and allows you to decouple
    data from compute, as well as define a clear division of work between data scientists
    and data engineers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然许多现实世界的应用程序显示的查询模式介于两种极端之间，但在许多应用程序中，加载数据是主要的性能瓶颈。数据科学家可能需要等待数十分钟才能加载数据，这严重影响了他们的生产力。移除这种生产力瓶颈是有效数据科学基础设施的关键目标，因此，在接下来的章节中，我们将探讨一种替代的、极其高效的数据访问方法。这种方法适用于许多现代数据仓库，并允许您将数据与计算解耦，以及明确划分数据科学家和数据工程师之间的工作。 '
- en: 7.1.1 Loading data from S3
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 从S3加载数据
- en: 'If you ask a data scientist how they would prefer accessing a dataset, assuming
    their personal productivity is the only consideration, a typical answer is “a
    local file.” Local files excel at boosting productivity for the following reasons:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果问一个数据科学家他们希望如何访问数据集，假设他们只考虑个人生产力，一个典型的回答是“本地文件”。本地文件在以下方面极大地提高了生产力：
- en: '*They can be loaded very quickly*—Loading data from a local file is faster
    than executing a SQL query.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*它们可以非常快速地加载*—从本地文件加载数据比执行SQL查询要快。'
- en: '*The dataset doesn’t change abruptly*—This is a key to effective prototyping.
    It is impossible to conduct systematic experimentation and iterations if the data
    underneath changes unannounced.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据集不会突然改变*—这是有效原型设计的关键。如果底层数据未宣布改变，就无法进行系统性的实验和迭代。'
- en: '*Ease of use*—Loading data doesn’t require special clients, it doesn’t fail
    randomly or become slow unpredictably when your colleagues are performing their
    experiments, and local files can be used by practically all off-the-shelf libraries.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*易用性*—加载数据不需要特殊的客户端，它不会在同事进行实验时随机失败或变得不可预测地缓慢，并且本地文件几乎可以由所有现成的库使用。'
- en: 'Unfortunately, the downsides of local files are many: they don’t work with
    production deployments or scaled-out experiments running in the cloud, and they
    need to be updated manually. Moreover, they make data warehouse administrators
    cringe because they fall outside the control of data security and governance policies.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，本地文件的缺点很多：它们与云端的实际部署或扩展实验不兼容，并且需要手动更新。此外，它们让数据仓库管理员感到不适，因为它们超出了数据安全和治理策略的控制范围。
- en: 'Using a cloud-based object store like AWS S3 can provide the best of both worlds:
    keeping data in the cloud makes it compatible with cloud-based compute, deployment,
    and data governance policies. With some effort, as demonstrated next, we can make
    the user experience nearly as seamless as accessing local files. In particular,
    many people are surprised about this fact: loading data from a cloud-based object
    store like S3 can be faster than loading it from local files.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于云的对象存储，如AWS S3，可以提供两全其美的解决方案：将数据存储在云端使其与云端的计算、部署和数据治理策略兼容。通过一些努力，正如下面将要展示的，我们可以使用户体验几乎与访问本地文件一样无缝。特别是，许多人对此事实感到惊讶：从基于云的对象存储（如S3）加载数据可能比从本地文件加载数据更快。
- en: 'To showcase S3-based data in action and to see if the previous statement is
    really true, let’s create a simple flow, shown in listing 7.1, to benchmark S3\.
    The listing demonstrates a fundamental operation: loading data from files to memory
    of a Python process and comparing the performance of loading data from local files
    versus files in S3.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示基于S3的数据在实际中的应用，并验证前面的陈述是否真的正确，让我们创建一个简单的流程，如列表7.1所示，以基准测试S3。该列表演示了一个基本操作：从文件到Python进程的内存加载数据，并比较从本地文件和S3中的文件加载数据的性能。
- en: For testing, we use a sample of data from Common Crawl ([commoncrawl.org](https://commoncrawl.org/)),
    which is a public dataset consisting of random web pages. Details of the dataset
    don’t matter. Notably, you can apply the lessons of this section equally well
    to unstructured data like images or videos or structured, tabular data. If you
    want, you can replace the dataset URL in the next listing with any other dataset
    that you can access in S3.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试，我们使用Common Crawl（[commoncrawl.org](https://commoncrawl.org/)）数据集的一个样本数据，这是一个由随机网页组成的公共数据集。数据集的详细信息并不重要。值得注意的是，你可以将本节中的经验同样应用于非结构化数据，如图像或视频，或结构化、表格数据。如果你想，你可以将下一个列表中的数据集URL替换为你可以在S3中访问的任何其他数据集。
- en: Listing 7.1 S3 benchmark
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.1 S3基准测试
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ A public Common Crawl dataset available in S3
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 可在S3中找到的公共Common Crawl数据集
- en: ❷ A helper function that loads data from S3
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从S3加载数据的辅助函数
- en: ❸ Picks the first num files in the given S3 directory
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从给定的S3目录中选择前num个文件
- en: ❹ Collects timing information about the loading operation in stats
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在stats中收集加载操作的计时信息
- en: ❺ Loads files from S3 to temporary local files
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将文件从S3加载到临时本地文件
- en: ❻ Returns paths to temporary files
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 返回临时文件路径
- en: ❼ The S3 scope manages the lifetime of temporary files
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ S3作用域管理临时文件的生命周期
- en: ❽ If a parameter local_dir is specified, loads files from a local directory;
    otherwise, loads from S3
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果指定了参数local_dir，则从本地目录加载文件；否则，从S3加载
- en: ❾ Reads local files in parallel
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 并行读取本地文件
- en: 'Save the code to a file called s3benchmark.py. If you are running it on your
    laptop, you can start by downloading a small amount of data as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到名为s3benchmark.py的文件中。如果你在笔记本电脑上运行它，你可以先下载一小部分数据，如下所示：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This downloads about 1 GB of data and prints statistics about the S3 throughput
    achieved.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这将下载大约1 GB的数据，并打印出达到的S3吞吐量统计信息。
- en: 'The flow operates in two parts: first, if --local_dir is not specified, it
    calls the load_s3 helper function to list the available files at the given URL
    and chooses the first num of them. After creating a list of files, it proceeds
    to download them in parallel using the get_many function of the Metaflow’s built-in
    S3 client, metaflow.S3, which we cover in more detail in section 7.1.3\. The function
    returns a list of paths to local temporary files that contain the downloaded data.
    The with S3 context manager takes care of clearing the temporary files after the
    context exits.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 流程分为两部分：首先，如果没有指定--local_dir，它将调用load_s3辅助函数来列出给定URL上的可用文件，并选择其中的前num个。在创建文件列表后，它将使用Metaflow内置的S3客户端metaflow.S3的get_many函数并行下载文件，我们将在7.1.3节中更详细地介绍这个函数。该函数返回包含下载数据的本地临时文件路径列表。with
    S3上下文管理器负责在上下文退出后清理临时文件。
- en: Second, the flow reads the contents of local files in memory. If --local_dir
    is specified, files are read from the given local directory that should contain
    a local copy of the files in S3\. Otherwise, the downloaded data is read. In either
    case, files are processed in parallel using parallel_map, which is a convenience
    function provided by Metaflow to parallelize a function over multiple CPU cores.
    In this case, we simply count the number of bytes read and discard the file after
    reading it. This benchmark measures only the time spent loading data—we don’t
    need to process the data in any way.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，流程在内存中读取本地文件的正文。如果指定了--local_dir，则从给定的本地目录读取文件，该目录应包含S3中文件的本地副本。否则，读取下载的数据。在两种情况下，文件都使用parallel_map并行处理，这是Metaflow提供的一个便利函数，用于在多个CPU核心上并行化一个函数。在这种情况下，我们只是计算读取的字节数，并在读取文件后丢弃它。这个基准测试只测量加载数据花费的时间——我们不需要以任何方式处理数据。
- en: 'If you are curious to benchmark local disk performance using the --local_dir
    option, you can download files from S3 to a local directory as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你好奇想使用--local_dir选项基准测试本地磁盘性能，你可以按照以下方式将文件从S3下载到本地目录：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that this is going to require 70 GB of disk space. Once the files have
    been downloaded, you can run the flow as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这将需要70 GB的磁盘空间。一旦文件下载完成，你可以按照以下方式运行流程：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If you test the S3 downloading speed on your laptop, you end up mainly benchmarking
    the performance of your local network connection. A better option, as discussed
    in chapter 2, is to use a cloud-based workstation, which speeds up all cloud operations,
    regardless of your local bandwidth.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你测试笔记本电脑上的S3下载速度，你最终主要是在基准测试你的本地网络连接性能。正如第2章所讨论的，更好的选择是使用基于云的工作站，这可以加快所有云操作，无论你的本地带宽如何。
- en: 'To get a better idea of realistic S3 performance, run the flow either on a
    cloud workstation or using a cloud-based compute layer like we discussed in chapter
    4\. You can run it, for instance, using AWS Batch as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解实际的S3性能，可以在云工作站上运行流程，或者使用我们在第4章中讨论的基于云的计算层。例如，你可以使用AWS Batch如下运行：
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When running on a large EC2 instance, you should see results like this:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当在大型EC2实例上运行时，你应该看到如下结果：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note S3 buckets and the data therein are physically located in a certain region.
    It is advisable to run compute in the same region where the bucket is located
    for maximum performance and to avoid having to pay for the data transfer. For
    instance, the commoncrawl bucket used in this example is located in the AWS region
    us-east-1.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 S3存储桶及其中的数据在物理上位于某个特定区域。建议在存储桶所在区域运行计算，以获得最佳性能并避免支付数据传输费用。例如，本例中使用的commoncrawl存储桶位于AWS区域us-east-1。
- en: Figure 7.4 shows the performance as a function of the --num_files option.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4显示了--num_files选项作为性能的函数。
- en: '![CH07_F04_Tuulos](../../OEBPS/Images/CH07_F04_Tuulos.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F04_Tuulos](../../OEBPS/Images/CH07_F04_Tuulos.png)'
- en: Figure 7.4 Loading time from local files vs. S3 as a function of the dataset
    size
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 从本地文件加载时间与数据集大小作为函数的关系
- en: The black line shows the total execution time when loading data from S3 and
    the gray line when loading data from local files. When the dataset is small enough,
    here, less than 12 GB or so, it is slightly faster to use local files. For larger
    datasets, loading data from S3 is indeed faster!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 黑色线表示从S3加载数据时的总执行时间，灰色线表示从本地文件加载数据时的执行时间。当数据集足够小，这里指小于12 GB左右时，使用本地文件稍微快一些。对于更大的数据集，从S3加载数据确实更快！
- en: 'This result comes with an important caveat: the S3 performance is highly dependent
    on the size and type of the instance that executes the task. Figure 7.5 illustrates
    the effect.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果有一个重要的前提：S3性能高度依赖于执行任务的实例的大小和类型。图7.5说明了这种影响。
- en: '![CH07_F05_Tuulos](../../OEBPS/Images/CH07_F05_Tuulos.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F05_Tuulos](../../OEBPS/Images/CH07_F05_Tuulos.png)'
- en: Figure 7.5 Loading data from S3 and from memory as a function of the instance
    size
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 以实例大小为函数从S3和内存中加载数据
- en: 'A very large instance like m5n.24xlarge that comes with 384 GB of RAM and 48
    CPU cores boasts massive throughput when loading data from S3: 20-30 gigabits
    per second, as shown by the gray bars. This is more than the local disk bandwidth
    on a recent Macbook laptop that can read up to 20 Gb/s. Medium-sized instances
    like c4.4xlarge show a fraction of the bandwidth, 1.5 Gbps, although still much
    more than what is achievable over a typical office Wi-Fi. A small instance like
    m4.large exhibits performance that’s much slower than a laptop.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常大的实例，如带有384 GB RAM和48个CPU核心的m5n.24xlarge，在从S3加载数据时具有巨大的吞吐量：20-30 Gb/s，如图中灰色条所示。这比最近Macbook笔记本电脑上的本地磁盘带宽（可读取高达20
    Gb/s）要高。中等大小的实例如c4.4xlarge显示的带宽只有1.5 Gbps，尽管仍然比典型办公室Wi-Fi所能达到的带宽要高得多。小型实例如m4.large的性能比笔记本电脑慢得多。
- en: Recommendation When dealing with data of nontrivial scale, it pays to use a
    large instance type. To control costs, you can use a medium-sized instance as
    a cloud workstation and run data-intensive steps on a compute layer like AWS Batch.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 建议 当处理大规模数据时，使用大型实例类型是有益的。为了控制成本，你可以使用中等大小的实例作为云工作站，并在计算层（如AWS Batch）上运行数据密集型步骤。
- en: File size matters
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 文件大小很重要
- en: If you try to reproduce S3 throughput figures with your own datasets but fail
    to see anything close to the previous numbers, the issue may be file sizes. Looking
    up an object in S3 is a relatively slow operation, which takes somewhere between
    50-100 ms. If you have many small files, a lot of time is spent looking up files,
    which reduces throughput significantly. An optimal file size for S3 is at least
    tens of megabytes or more, depending on the amount of data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试使用自己的数据集重现S3吞吐量图，但未能看到接近之前数字的结果，问题可能是文件大小。在S3中查找对象是一个相对较慢的操作，耗时约50-100毫秒。如果你有很多小文件，查找文件会花费大量时间，这会显著降低吞吐量。S3的最佳文件大小至少是几十兆字节或更多，具体取决于数据量。
- en: Keeping data in memory with the disk cache
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用磁盘缓存将数据保持在内存中
- en: 'To explain the black line in figure 7.5, let’s dive deeper in the example.
    You may wonder if this benchmark makes any sense: the load_s3 function downloads
    data to local temporary files, which we then read in the start step. Hence, we
    seem to be comparing loading data from temporary local files versus local files
    in a directory, which should be equally fast.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释图7.5中的黑色线条，让我们深入到这个例子中。你可能想知道这个基准是否有意义：load_s3函数将数据下载到本地临时文件中，然后我们在启动步骤中读取这些文件。因此，我们似乎是在比较从临时本地文件加载数据和从目录中的本地文件加载数据，这两者的速度应该是相同的。
- en: The trick is that when loading data from S3, the data should stay in memory,
    transparently stored in an in-memory *disk cache* by the operating system, without
    ever hitting the local disk, as long as the dataset is small enough to fit in
    memory. Figure 7.6 illustrates the logic.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 技巧在于，当从S3加载数据时，数据应保持在内存中，由操作系统透明地存储在内存中的磁盘缓存中，而无需触及本地磁盘，只要数据集足够小，可以适合内存。图7.6说明了这个逻辑。
- en: '![CH07_F06_Tuulos](../../OEBPS/Images/CH07_F06_Tuulos.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F06_Tuulos](../../OEBPS/Images/CH07_F06_Tuulos.png)'
- en: Figure 7.6 Loading data through the disk cache is fast compared to disk IO.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 通过磁盘缓存加载数据比磁盘I/O快。
- en: 'When the dataset size fits in memory, loading from S3 happens through the fast
    path depicted by the left arrow. When the dataset is larger than the available
    memory, some data spills onto the local disk, which makes loading data much slower.
    This is why S3 can be faster than local disk: when you run the flow, for example,
    with --with batch:memory=16000, the full 16 GB of memory on the instance is dedicated
    for the task. In contrast, many processes are fighting over the memory on your
    laptop, and as a result, it is often not feasible to keep all data in memory,
    at least when the dataset size grows as depicted by figure 7.4.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集大小适合内存时，从S3的加载通过左侧箭头表示的快速路径进行。当数据集大于可用内存时，一些数据会溢出到本地磁盘，这使得加载数据变得非常慢。这就是为什么S3可以比本地磁盘更快：当你运行带有--with
    batch:memory=16000的流程时，实例上的全部16 GB内存都用于这项任务。相比之下，许多进程都在争夺你的笔记本电脑上的内存，因此，当数据集大小如图7.4所示增长时，通常不可能将所有数据保持在内存中。
- en: The black line in figure 7.5 shows how quickly data is read from either disk
    cache or local disk to the process’s memory. The largest instance, m5n.24xlarge,
    keeps all data in the disk cache, so reading data is very fast, 228 Gbit/s. The
    data is just copied between memory locations in parallel. In contrast, the small
    instance, m4.large, is too small to keep the data in memory, so data spills on
    disk and reading becomes relatively sluggish, just 0.4 Gbit/s.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5中的黑色线条显示了数据从磁盘缓存或本地磁盘读取到进程内存的速度。最大的实例m5n.24xlarge将所有数据保持在磁盘缓存中，因此读取数据非常快，达到228
    Gbit/s。数据只是在内存位置之间并行复制。相比之下，小型实例m4.large太小，无法将数据保持在内存中，因此数据会溢出到磁盘上，读取速度变得相对较慢，仅为0.4
    Gbit/s。
- en: Recommendation Whenever feasible, choose @resources that allow you to keep all
    data in memory. It makes all operations massively faster.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 建议：在可行的情况下，选择允许你将所有数据保持在内存中的资源。这会使所有操作的速度大幅提升。
- en: 'Let’s summarize what we learned in this section:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下本节中学到的内容：
- en: 'It is beneficial to use S3 instead of local files: data is easier to manage,
    it is readily available for tasks running in the cloud, and there can be only
    a minimal performance penalty, or even a performance improvement.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用S3而不是本地文件是有益的：数据更容易管理，它对在云中运行的任务来说随时可用，并且性能损失可以最小化，甚至可能提升性能。
- en: We can load data very fast from S3 to the process’s memory, as long as we use
    a large enough instance.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只要我们使用足够大的实例，我们就可以非常快地从S3将数据加载到进程的内存中。
- en: Metaflow comes with a high-performance S3 client, metaflow.S3, which allows
    data to be loaded directly to memory without hitting the local disk.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Metaflow附带了一个高性能的S3客户端，metaflow.S3，它允许数据直接加载到内存中，而无需触及本地磁盘。
- en: These points form a foundation that we will build upon in the coming sections.
    In the next section, we will take these learnings closer to the everyday life
    of a data scientist and explore how to load large dataframes and other tabular
    data in tasks efficiently.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些点构成了我们将要在接下来的章节中建立的基础。在下一节中，我们将将这些知识应用到数据科学家的日常工作中，并探讨如何在任务中有效地加载数据帧和其他表格数据。
- en: 7.1.2 Working with tabular data
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 与表格数据一起工作
- en: In the previous section, we were merely interested in moving raw bytes. The
    discussion applies to any data modalities from videos to natural language. In
    this section, we focus on a particular kind of data, namely structured or semi-structured
    data, which is often manipulated as dataframes. Data of this kind is extremely
    common in business data science—for instance, all relational databases hold data
    of this nature.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们仅仅对移动原始字节感兴趣。该讨论适用于从视频到自然语言的所有数据模式。在本节中，我们专注于一种特定类型的数据，即结构化或半结构化数据，这类数据通常以数据框的形式进行操作。这种类型的数据在商业数据科学中极为常见——例如，所有关系型数据库都持有这种类型的数据。
- en: Figure 7.7 shows an example of a tabular dataset containing employee information.
    The dataset has three columns, name, age, and role, and three rows, a row for
    each employee.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7展示了包含员工信息的表格数据集的一个示例。该数据集有三个列，分别是姓名、年龄和角色，以及三行，每行代表一个员工。
- en: '![CH07_F07_Tuulos](../../OEBPS/Images/CH07_F07_Tuulos.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F07_Tuulos](../../OEBPS/Images/CH07_F07_Tuulos.png)'
- en: Figure 7.7 Storing tabular data as CSV vs. Parquet format
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 将表格数据存储为CSV与Parquet格式
- en: As depicted in figure 7.7, we can store the dataset in different formats. In
    chapter 3, we talked about the CSV (comma-separated values) format, which is a
    simple text file containing a row of data in each line, with columns separated
    by commas. Alternatively, we can store the same data in the popular *Parquet*
    format, which is a *column-oriented storage format*.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如图7.7所示，我们可以以不同的格式存储数据集。在第3章中，我们讨论了CSV（逗号分隔值）格式，它是一种简单的文本文件，每行包含一行数据，列之间由逗号分隔。或者，我们也可以以流行的*Parquet*格式存储相同的数据，这是一种*列式存储格式*。
- en: In Parquet and other columnar formats, each column of data is stored independently.
    This approach offers a few benefits. First, with structured data, each column
    has a specific type. In this example, Name and Role are strings and Age is an
    integer. Each data type needs to be encoded and compressed in a specific way,
    so grouping data by column, that is, by type, is beneficial. Parquet files store
    an explicit schema and other metadata in the data file itself. In contrast, CSV
    files work around the issue by ignoring the schema altogether—everything becomes
    a string, which is a major downside of CSV.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在Parquet和其他列式格式中，每个数据列都是独立存储的。这种方法提供了一些好处。首先，对于结构化数据，每个列都有一个特定的类型。在这个例子中，姓名和角色是字符串，年龄是整数。每种数据类型都需要以特定的方式进行编码和压缩，因此按列（即按类型）分组数据是有益的。Parquet文件在数据文件本身中存储了显式模式和其它元数据。相比之下，CSV文件通过完全忽略模式来解决这个问题——所有内容都变成了字符串，这是CSV的一个主要缺点。
- en: Second, because each column is stored separately, it is possible to load only
    a subset of columns efficiently—imagine a query like SELECT name, role FROM table.
    Similarly, any operations that need to process a column like SELECT AVG(age) FROM
    table can be processed quickly because all relevant data is laid out contiguously
    in memory. Third, Parquet files are stored in a compressed binary format, so they
    take less space to store and are faster to transfer than plain CSV files.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，由于每个列都是单独存储的，因此可以高效地只加载列的子集——想象一下这样的查询：SELECT name, role FROM table。同样，任何需要处理列的操作，如SELECT
    AVG(age) FROM table，也可以快速处理，因为所有相关数据都在内存中连续排列。第三，Parquet文件以压缩的二进制格式存储，因此占用的存储空间更少，比普通的CSV文件传输更快。
- en: Reading Parquet data in memory with Apache Arrow
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Apache Arrow在内存中读取Parquet数据
- en: A major benefit of CSV files is that Python comes with a built-in module, aptly
    named csv, to read them. To read Parquet files, we will need to use a separate
    open source library called *Apache Arrow*. Besides being a Parquet file decoder,
    Arrow provides an efficient in-memory representation of data, which allows us
    to process data efficiently as well—more examples of this later.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: CSV文件的一个主要优点是Python自带了一个名为csv的内建模块，非常适合读取它们。要读取Parquet文件，我们需要使用一个名为*Apache Arrow*的独立开源库。除了作为Parquet文件解码器之外，Arrow还提供了数据的高效内存表示，这使我们能够高效地处理数据——稍后会有更多这方面的例子。
- en: Let’s compare CSV and Parquet in practice. For testing, we use public trip data
    from New York City’s Taxi Commission ([http://mng.bz/j2rp](http://mng.bz/j2rp)),
    which is already available as public Parquet files in S3\. For our benchmark,
    we use one month of data, which contains about 13 million rows, each representing
    a taxi trip. The dataset has 18 columns providing information about the trip.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实际比较一下CSV和Parquet。为了测试，我们使用了纽约市出租车委员会的公开行程数据([http://mng.bz/j2rp](http://mng.bz/j2rp))，这些数据已经以公开的Parquet文件形式存储在S3上。对于我们的基准测试，我们使用了一个月的数据，包含大约1300万行，每行代表一次出租车行程。该数据集有18列，提供了关于行程的信息。
- en: 'The benchmark, shown in listing 7.2, compares the time spent loading data between
    CSV and two ways of loading Parquet using Arrow or pandas. The code works as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试，如图7.2所示，比较了CSV和两种使用Arrow或pandas加载Parquet文件的方式所花费的时间。代码如下：
- en: The start step loads a Parquet file containing taxi trips from a public S3 bucket
    and makes it available as a local file, taxi.parquet. It uses pandas to convert
    the Parquet file to a CSV file, saving it to taxi.csv. We will use these two files
    to benchmark data loading in the subsequent steps.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始步骤加载一个包含出租车行程的Parquet文件，该文件存储在公共S3桶中，并将其作为本地文件taxi.parquet提供。它使用pandas将Parquet文件转换为CSV文件，并将其保存为taxi.csv。我们将使用这两个文件在后续步骤中基准测试数据加载。
- en: 'After the start step, we split into three separate data-loading steps, each
    of which benchmarks a different way to load the dataset. Each step saves the time
    spent to load data in the stats artifact as follows:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在start步骤之后，我们将其分为三个独立的数据加载步骤，每个步骤基准测试加载数据集的不同方式。每个步骤将加载数据所花费的时间保存到stats工件中如下：
- en: The load_csv step uses Python’s built-in csv module to loop through all rows,
    reading from the CSV file.
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: load_csv步骤使用Python内置的csv模块遍历所有行，从CSV文件中读取。
- en: The load_parquet step loads the dataset in memory using PyArrow.
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: load_parquet步骤使用PyArrow在内存中加载数据集。
- en: The load_pandas step loads the dataset in memory using pandas.
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: load_pandas步骤使用pandas在内存中加载数据集。
- en: Finally, the join step prints the timings measured by the previous steps.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，join步骤打印前一步骤测量的计时。
- en: Listing 7.2 Comparing data formats
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.2 比较数据格式
- en: '[PRE6]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ One month of Taxi data, stored as a Parquet file
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 存储为一个Parquet文件的出租车数据一个月
- en: ❷ Downloads the Parquet file from S3
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从S3下载Parquet文件
- en: ❸ Loads the Parquet file in memory using Arrow
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用Arrow在内存中加载Parquet文件
- en: ❹ Moves the Parquet file to a persistent location, so we can load it later
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将Parquet文件移动到持久位置，以便我们稍后加载
- en: ❺ Writes the dataset in a CSV file, so we can load it later
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将数据集写入CSV文件，以便我们稍后加载
- en: ❻ Stores profiling statistics in this dictionary
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将分析统计信息存储在这个字典中
- en: ❼ Stores timing information in the dictionary
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 将计时信息存储在字典中
- en: ❽ Reads the CSV file using the built-in csv module
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 使用内置的csv模块读取CSV文件
- en: ❾ Discards rows to avoid excessive memory consumption
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 抛弃行以避免过度消耗内存
- en: ❿ Loads the Parquet file using Arrow. This time, we time the operation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 使用Arrow加载Parquet文件。这次，我们计时操作。
- en: ⓫ Loads the Parquet file using pandas
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 使用pandas加载Parquet文件
- en: ⓬ Prints timing statistics from each branch
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 打印每个分支的计时统计信息
- en: 'Save the code in parquet_benchmark.py. For benchmarking purposes, this flow
    stores a Parquet file and a CSV file as local files, so this flow must be run
    on a laptop or a cloud workstation so all steps can access the files. Run the
    flow as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存在parquet_benchmark.py中。出于基准测试的目的，此流程将Parquet文件和CSV文件作为本地文件存储，因此此流程必须在笔记本电脑或云工作站上运行，以便所有步骤都可以访问文件。按照以下方式运行流程：
- en: '[PRE7]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We use --max-workers 1 to force sequential instead of parallel execution of
    branches, which ensures more unbiased timings. Executing the start step takes
    a while, because it downloads a 319 MB compressed Parquet file from S3 and writes
    it to a local CSV file, which expands to 1.6 GB.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用--max-workers 1强制分支按顺序而不是并行执行，这确保了更无偏的计时。执行start步骤需要一段时间，因为它从S3下载一个319MB的压缩Parquet文件并将其写入本地CSV文件，该文件展开为1.6GB。
- en: 'Tip When iterating on a flow like ParquetBenchmarkFlow that has an expensive
    step in the beginning, such as start in this case, remember the resume command:
    instead of using run, which takes a while, you can use, for example, resume load_csv
    and keep iterating on later steps while skipping the slow beginning.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：当迭代像ParquetBenchmarkFlow这样的流程时，该流程在开始处有一个昂贵的步骤，例如本例中的start，记得使用resume命令：而不是使用run，这需要一段时间，你可以使用，例如，resume
    load_csv并继续迭代后续步骤，同时跳过缓慢的开始。
- en: 'You should see an output like this, which shows the timings as milliseconds:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到如下输出，显示计时以毫秒为单位：
- en: '[PRE8]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The results are also visualized in figure 7.8.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 结果也显示在图7.8中。
- en: '![CH07_F08_Tuulos](../../OEBPS/Images/CH07_F08_Tuulos.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F08_Tuulos](../../OEBPS/Images/CH07_F08_Tuulos.png)'
- en: Figure 7.8 Comparing data loading times between Arrow, pandas, and CSV
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 比较Arrow、pandas和CSV之间的数据加载时间
- en: The load_parquet step is by far the fastest, taking less than a second to load
    13 million rows of data! The load_pandas step takes twice as much time to read
    the Parquet file into a pandas DataFrame. We cheated a bit with the load_csv step,
    which takes almost 20 seconds, because it doesn’t keep the data in memory like
    the other steps do. It simply iterates over the rows once. If we keep the data
    in memory—you can try it with list(csv.reader(csvfile))—the step takes 70 seconds
    and consumes nearly 20 GB of memory.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: load_parquet 步骤是目前最快的，加载 1300 万行数据只需不到一秒钟！load_pandas 步骤需要两倍的时间来将 Parquet 文件读入
    pandas DataFrame。我们在 load_csv 步骤上有点作弊，因为它几乎需要 20 秒，因为它不像其他步骤那样将数据保留在内存中。它只是迭代一次行。如果我们保留数据在内存中——你可以通过
    list(csv.reader(csvfile)) 尝试——该步骤需要 70 秒，并消耗近 20 GB 的内存。
- en: Recommendation Whenever feasible, use the Parquet format to store and transfer
    tabular data instead of CSV files.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 建议：在可行的情况下，使用 Parquet 格式存储和传输表格数据，而不是 CSV 文件。
- en: Hopefully these results convince you that using Parquet instead of CSV is almost
    always a win, the main exception being sharing small amounts of data with other
    systems and people who may not be able to handle Parquet. Also, Parquet files
    aren’t as easy to inspect using standard command-line tools as simple textual
    CSV files, although tools exist that allow you to dump the contents of a Parquet
    file to a text file. Now that we know how to load tabular data stored as Parquet
    files, the next step is to consider the components on top of it.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这些结果能让你相信，使用 Parquet 而不是 CSV 几乎总是更胜一筹，主要例外是与其他系统和可能无法处理 Parquet 的人共享少量数据。此外，与简单的文本
    CSV 文件相比，使用标准命令行工具检查 Parquet 文件并不那么容易，尽管存在可以将 Parquet 文件内容导出到文本文件的工具。现在我们知道了如何加载存储为
    Parquet 文件的表格数据，下一步就是考虑其上的组件。
- en: 7.1.3 The in-memory data stack
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 内存数据堆栈
- en: 'Using Parquet instead of CSV is a no-brainer, but how should one choose between
    Arrow and pandas? Luckily, you don’t have to choose: the tools are often quite
    complementary. Figure 7.9 clarifies how the libraries forming the in-memory data
    stack fit together.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Parquet 而不是 CSV 是不言而喻的，但一个人应该如何在 Arrow 和 pandas 之间做出选择？幸运的是，你不必做出选择：这些工具通常是相当互补的。图
    7.9 清晰地说明了构成内存数据堆栈的库是如何相互配合的。
- en: '![CH07_F09_Tuulos](../../OEBPS/Images/CH07_F09_Tuulos.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F09_Tuulos](../../OEBPS/Images/CH07_F09_Tuulos.png)'
- en: Figure 7.9 A modern stack for handling data in-memory
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 处理内存中数据的现代堆栈
- en: 'Parquet is a *data storage format*: it is a way to store and transfer data
    more efficiently than using CSV files. For instance, you can load Parquet files
    from S3 to your workflows very quickly using the metafow.S3 library, as we have
    seen earlier. To use the data, we need to load and decode it from the Parquet
    files, which is the job of the Arrow library. Arrow supports multiple languages—its
    Python binding is called PyArrow. Arrow decodes data from Parquet files into an
    efficient *in-memory representation* of data, which can be used either directly
    or through another library, like pandas.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 是一种 *数据存储格式*：它是一种比使用 CSV 文件更有效地存储和传输数据的方式。例如，你可以使用 metafow.S3 库非常快速地将
    Parquet 文件从 S3 加载到你的工作流程中，正如我们之前所看到的。为了使用这些数据，我们需要从 Parquet 文件中加载和解析它，这是 Arrow
    库的工作。Arrow 支持多种语言——其 Python 绑定称为 PyArrow。Arrow 将数据从 Parquet 文件解码成数据的高效 *内存表示*，可以直接使用或通过另一个库，如
    pandas。
- en: 'This is a superpower of Arrow: its in-memory representation has been designed
    in such a way that it can be leveraged by other *data processing libraries* like
    pandas or NumPy so that they don’t have to make another copy of the data, which
    is a huge win when dealing with large datasets. This means that your *user code*,
    for example, a model-training step using an ML library, can read data managed
    by Arrow, possibly through pandas, in a very memory- and time-efficient manner.
    Notably, all data management is performed by efficient low-level code, not by
    Python directly, which makes it possible to develop extremely high-performance
    code in Python.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Arrow 的一个超级功能：其内存表示已经被设计成可以被其他 *数据处理库*，如 pandas 或 NumPy 利用，这样它们就不必对数据进行另一次复制，这在处理大型数据集时是一个巨大的优势。这意味着你的
    *用户代码*，例如使用 ML 库进行模型训练的步骤，可以以非常内存和时间高效的方式读取由 Arrow 管理的数据，可能通过 pandas。值得注意的是，所有数据管理都是由高效的底层代码执行的，而不是直接由
    Python 执行，这使得在 Python 中开发极高性能的代码成为可能。
- en: Whether to use pandas or NumPy or use the PyArrow library directly depends on
    the exact use case. A major benefit of pandas is that it provides many easy-to-use
    primitives for data manipulation, so if your task needs such functionality, converting
    from Arrow to pandas or using pd.read_parquet is a good option.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 是否使用 pandas 或 NumPy 或直接使用 PyArrow 库取决于具体的使用情况。pandas 的一个主要优点是它提供了许多易于使用的数据操作原语，因此如果你的任务需要此类功能，将
    Arrow 转换为 pandas 或使用 pd.read_parquet 是一个好的选择。
- en: A major downside of pandas is that it can be quite memory-hungry, as we will
    see later, and it is not as performant as pure-Arrow operations. Hence, if you
    use an ML library that can accept Arrow data or NumPy arrays directly, avoiding
    conversion to pandas can save a lot of time and memory. We will see a practical
    example of this in section 7.3.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 的一个主要缺点是它可能非常占用内存，正如我们稍后将会看到的，并且它的性能不如纯 Arrow 操作。因此，如果你使用一个可以接受 Arrow
    数据或 NumPy 数组的 ML 库，避免转换为 pandas 可以节省大量时间和内存。我们将在 7.3 节中看到一个实际例子。
- en: Why metaflow.S3?
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用 metaflow.S3？
- en: 'If you have used Arrow or pandas previously, you may know that they support
    loading data from s3:// URLs directly. Why does figure 7.9 mention metaflow.S3*,*
    then? Currently, loading datasets consisting of multiple files can be much faster
    with metaflow.S3 than using the S3 interface built in Arrow and pandas. The reason
    is simple: metaflow.S3 aggressively parallelizes downloading over multiple network
    connections, which is required for maximum throughput.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前使用过 Arrow 或 pandas，你可能知道它们支持直接从 s3:// URLs 加载数据。那么为什么图 7.9 提到了 metaflow.S3*？目前，使用
    metaflow.S3 加载数据集（由多个文件组成）比使用内置在 Arrow 和 pandas 中的 S3 接口要快得多。原因很简单：metaflow.S3
    在多个网络连接上积极并行下载，这对于最大吞吐量是必需的。
- en: It is likely that the libraries will implement a similar approach in the future.
    Once this happens, you can replace the metaflow.S3 part in the diagram and code
    examples with a library-native approach. Everything else in the picture remains
    the same.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能库将来会实现类似的方法。一旦发生这种情况，你就可以用库原生方法替换图和代码示例中的 metaflow.S3 部分。图片中的其他所有内容保持不变。
- en: Profiling memory consumption
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 分析内存消耗
- en: When dealing with large amounts of in-memory data, memory consumption is often
    a bigger concern than execution time. In the previous examples, we have used the
    with profile context manager to time various operations, but what if we wanted
    to measure memory consumption similarly?
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理大量内存中的数据时，内存消耗通常比执行时间更令人关注。在先前的例子中，我们使用了 with profile 上下文管理器来计时各种操作，但如果我们想以类似的方式测量内存消耗怎么办？
- en: Measuring how memory consumption evolves over time is not as straightforward
    as looking at a timer. However, by leveraging an off-the-shelf library called
    memory_ profiler, we can create a utility function, in fact, a custom decorator,
    that you can use to measure peak memory consumption of any Metaflow step, as shown
    in the next listing.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 测量内存消耗随时间的变化不如查看计时器那么直接。然而，通过利用一个现成的库 memory_profiler，我们可以创建一个实用函数，实际上是一个自定义装饰器，你可以使用它来测量任何
    Metaflow 步骤的峰值内存消耗，如下一列表所示。
- en: Listing 7.3 Memory profiling decorator
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.3 内存分析装饰器
- en: '[PRE9]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Defines a Python decorator—a function that returns a function
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义了一个 Python 装饰器——一个返回函数的函数
- en: ❷ The @wraps decorator helps to make a well-behaving decorator.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ @wraps 装饰器有助于创建一个表现良好的装饰器
- en: ❸ Uses the memory_ profile library to measure memory consumption
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用 memory_profile 库来测量内存消耗
- en: ❹ Stores the peak memory usage in an artifact, mem_usage
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将峰值内存使用存储在名为 mem_usage 的工件中
- en: If you haven’t created decorators in Python before, this example may look a
    bit strange. It defines a function, profile_memory, which takes an argument, mf_step,
    that is the Metaflow step being decorated. It wraps the step in a new function,
    func, which calls the library memory_profiler to execute the step and measure
    its memory_ usage in the background. The profiler returns the peak memory usage,
    which is assigned to an artifact, self.mem_usage.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前没有在 Python 中创建过装饰器，这个例子可能看起来有点奇怪。它定义了一个函数，profile_memory，它接受一个参数 mf_step，这是被装饰的
    Metaflow 步骤。它将步骤包装在一个新的函数 func 中，该函数调用库 memory_profiler 来执行步骤并在后台测量其内存使用情况。分析器返回峰值内存使用量，并将其分配给一个工件，self.mem_usage。
- en: 'Save the code to a file, metaflow_memory.py. Now, in any flow, you can import
    the new decorator by writing from metaflow_memory import profile_memory at the
    top of the file. You must also make sure that the memory_profiler library is available,
    which you can do by adding ''memory_profiler'': ''0.58.0'' to the libraries dictionary
    in @conda_base. Now you can decorate any step to be profiled with @profile_memory.
    For instance, you can enhance listing 7.3 by writing the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '将代码保存到文件中，名为metaflow_memory.py。现在，在任何流程中，你都可以通过在文件顶部写入from metaflow_memory
    import profile_memory来导入新的装饰器。你还必须确保内存分析库可用，这可以通过在@conda_base字典中添加''memory_profiler'':
    ''0.58.0''来实现。现在你可以使用@profile_memory装饰任何要分析步骤。例如，你可以通过编写以下内容来增强列表7.3：'
- en: '[PRE10]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Add the decorator to every branch. To print the memory consumption, you can
    use the following join step:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 将装饰器添加到每个分支。要打印内存消耗，可以使用以下join步骤：
- en: '[PRE11]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: To get a realistic read of the memory consumption of the CSV in the load_csv
    step, you should keep all rows in memory by using list(csv.reader(csvfile)) instead
    of the for loop that discards rows. Note that this will require a workstation
    with more than 16 GB of RAM.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在load_csv步骤中获得CSV内存消耗的实际情况，你应该通过使用list(csv.reader(csvfile))而不是丢弃行的for循环来在内存中保留所有行。请注意，这将需要一个超过16
    GB RAM的工作站。
- en: You can run parquet_benchmark.py as usual. In addition to timings, you will
    see peak memory consumption printed, which is illustrated in figure 7.10.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像往常一样运行parquet_benchmark.py。除了计时外，你还会看到打印出的峰值内存消耗，如图7.10所示。
- en: '![CH07_F10_Tuulos](../../OEBPS/Images/CH07_F10_Tuulos.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F10_Tuulos](../../OEBPS/Images/CH07_F10_Tuulos.png)'
- en: Figure 7.10 Comparing memory overhead between Arrow, pandas, and CSV
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 比较Arrow、pandas和CSV之间的内存开销
- en: As expected, keeping all CSV data in memory as memory-inefficient Python objects
    is very costly—the load_csv step consumes nearly 17 GB of RAM, which is over 10×
    more than Arrow’s efficient memory representation of the same data. pandas consumes
    a gigabyte more than Arrow, because it needs to maintain a Python-friendly representation
    of some objects, strings in particular.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，将所有CSV数据作为内存效率低下的Python对象保存在内存中是非常昂贵的——load_csv步骤消耗了近17 GB的RAM，这比Arrow对相同数据的有效内存表示多出10倍以上。pandas比Arrow多消耗一个GB，因为它需要维护一些Python友好的对象表示，尤其是字符串。
- en: Recommendation If memory consumption is a concern, avoid storing individual
    rows as Python objects. Converting to pandas can be costly as well. The most efficient
    option is to use Arrow and NumPy, if possible.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 建议：如果内存消耗是一个问题，请避免将单个行作为Python对象存储。转换为pandas也可能很昂贵。如果可能的话，最有效的方法是使用Arrow和NumPy。
- en: Thus far, we have developed building blocks that go from S3 to an efficient
    in-memory representation of data. Combined with high-memory instances (think @resources
    (memory=256000)), you can handle massive datasets efficiently in a single task.
    However, what if your dataset is larger than what can be handled on any reasonable
    instance? Or what if a suitable dataset doesn’t exist but must be created by filtering
    and joining multiple tables together? In cases like this, it is best to rely on
    the rest of the data infrastructure, battle-hardened query engines in particular,
    to create suitable datasets for data science workflows from arbitrary amounts
    of raw data.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经开发了从S3到数据高效内存表示的构建块。结合高内存实例（例如@resources (memory=256000)），你可以在单个任务中高效地处理大量数据集。然而，如果你的数据集比任何合理实例能处理的大，或者合适的数据集不存在但必须通过过滤和连接多个表来创建呢？在这种情况下，最好依赖剩余的数据基础设施，特别是经过实战考验的查询引擎，从任意数量的原始数据中创建适合数据科学工作流程的数据集。
- en: 7.2 Interfacing with data infrastructure
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 与数据基础设施接口
- en: '*Alex’s delivery-time estimator turns out to be a success. As a result, the
    product team requests Alex build more fine-grained models for specific product
    categories. This requires more data preprocessing: Alex needs to extract the right
    subsets of data for each category as well as experiment with various combinations
    of columns that produce the best estimates for each category. Bowie suggests that
    Alex can do all the data preprocessing in SQL because databases should be good
    at crunching data. Alex counters the idea by pointing out that it is much faster
    to iterate on models and their input data using Python. Finally, they reach a
    happy compromise: Alex will use SQL to extract a suitable dataset and Python to
    define inputs for the model.*'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*亚历克斯的交货时间估算器证明是成功的。因此，产品团队要求亚历克斯为特定产品类别构建更多细粒度的模型。这需要更多的数据预处理：亚历克斯需要为每个类别提取正确的数据子集，并尝试各种列的组合，以产生每个类别的最佳估计。鲍伊建议亚历克斯可以在SQL中完成所有数据预处理，因为数据库应该擅长处理数据。亚历克斯通过指出使用Python迭代模型及其输入数据要快得多来反驳这一观点。最后，他们达成了一项愉快的妥协：亚历克斯将使用SQL提取合适的数据集，并使用Python定义模型的输入*。'
- en: '![CH07_F10_UN02_Tuulos](../../OEBPS/Images/CH07_F10_UN02_Tuulos.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F10_UN02_Tuulos](../../OEBPS/Images/CH07_F10_UN02_Tuulos.png)'
- en: Data science workflows don’t exist in a vacuum. Most companies have existing
    data infrastructure that has been set up to support various use cases from analytics
    to product features. It is beneficial that all applications rely on a consistent
    set of data, managed by a centralized data infrastructure. Data science and machine
    learning are not an exception.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学工作流程并非孤立存在。大多数公司都有现有的数据基础设施，这些基础设施已经建立起来以支持从分析到产品功能的各种用例。所有应用程序都依赖于一组一致的数据，由集中的数据基础设施管理是有益的。数据科学和机器学习也不例外。
- en: Although data science workflows rely on the same input data, the same *facts*
    as discussed in section 7.3, the way workflows access and use data is often different
    from other applications. First, they tend to access much larger extracts of data—tens
    or hundreds of gigabytes—to, say, train a model, whereas a dashboard might show
    only a few kilobytes of carefully selected data at a time. Second, data science
    workflows tend to be much more compute-intensive than other applications, necessitating
    a separate compute layer, as discussed in chapter 4.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据科学工作流程依赖于相同的数据输入，即与第7.3节中讨论的相同*事实*，但工作流程访问和使用数据的方式通常与其他应用程序不同。首先，它们倾向于访问大量数据提取——数十或数百吉字节的数据——用于训练模型，而仪表板可能一次只显示几千字节精心挑选的数据。其次，数据科学工作流程通常比其他应用程序计算密集得多，需要单独的计算层，正如第4章所讨论的。
- en: This section shows how you can integrate data science workflows into existing
    data infrastructure by leveraging techniques we learned in the previous section.
    Besides the technical concern of moving data around, we touch on an organizational
    question of how work can be divided between data engineers, who mainly work on
    the data infrastructure, and ML engineers and data scientists, who work on the
    data science infrastructure.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了如何通过利用我们在上一节中学到的技术，将数据科学工作流程集成到现有的数据基础设施中。除了移动数据的技术问题外，我们还触及了一个组织问题，即如何在主要工作在数据基础设施上的数据工程师、主要工作在数据科学基础设施上的机器学习工程师和数据科学家之间分配工作。
- en: 7.2.1 Modern data infrastructure
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 现代数据基础设施
- en: 'This book is about *data science* infrastructure, namely infrastructure required
    to prototype and deploy data-intensive applications that leverage optimization—or
    training—techniques of various kinds to build models to serve a diverse set of
    use cases. At many companies, the data science infrastructure has a sibling stack:
    the *data infrastructure* stack.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 本书是关于*数据科学*基础设施的，即用于原型设计和部署利用各种优化或训练技术构建模型以服务于各种用例的数据密集型应用程序所需的基础设施。在许多公司中，数据科学基础设施有一个兄弟堆栈：*数据基础设施*堆栈。
- en: Because both stacks deal with data, and often both use DAGs to express workflows
    that transform input data to output data, one might wonder if there’s actually
    any difference between the two stacks. Couldn’t we just use the data infrastructure
    for data science, too? This book argues that activities related to data science
    are qualitatively different from data engineering, which justifies a parallel
    stack. Model-building requires special libraries, typically more code, and definitely
    more computation than data engineering. However, it is beneficial to keep the
    two stacks closely aligned to avoid redundant solutions and unnecessary operational
    overhead.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两个堆栈都处理数据，并且通常都使用DAG来表示将输入数据转换为输出数据的流程，人们可能会想知道这两个堆栈之间实际上是否有任何区别。我们难道不能也使用数据基础设施来进行数据科学吗？本书认为，与数据工程相关的活动在质量上与数据工程不同，这为并行堆栈提供了合理性。模型构建需要特殊的库，通常需要更多的代码，并且肯定比数据工程需要更多的计算。然而，保持两个堆栈紧密一致是有益的，以避免冗余解决方案和不必要的运营开销。
- en: '![CH07_F11_Tuulos](../../OEBPS/Images/CH07_F11_Tuulos.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F11_Tuulos](../../OEBPS/Images/CH07_F11_Tuulos.png)'
- en: Figure 7.11 Components of the modern data infrastructure
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 现代数据基础设施组件
- en: To better understand how to integrate the stacks, let’s start by considering
    the components of modern data infrastructure, as illustrated in figure 7.11\.
    The figure is structured so that the most foundational components are at the center
    and more advanced, optional components in the outer layers.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解如何集成堆栈，让我们首先考虑现代数据基础设施的组件，如图7.11所示。该图的结构是，最基础的组件位于中心，而更高级、可选的组件位于外层。
- en: '*Data*—At the very core, you have the data asset itself. This diagram doesn’t
    illustrate how the data is acquired, which is a complex topic of its own, but
    we assume that you have some data that is stored, say, as CSV files, Parquet files,
    or as tables in a database.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据*—在最核心的位置，是数据资产本身。此图并未说明数据是如何获取的，这是一个复杂的话题，但我们假设你有一些数据，比如存储为CSV文件、Parquet文件，或者作为数据库中的表。'
- en: '*Durable storage*—Although you could use a USB thumb drive for storage, it
    is preferable to rely on a more durable storage system like AWS S3 or a replicated
    database. One option is a modern *data lake*, that is, storing (Parquet) files
    on a generic storage system like S3 accompanied by a metadata layer like Apache
    Hive or Iceberg to facilitate access to data through query engines.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*持久化存储*—虽然你可以使用USB闪存驱动器进行存储，但更可取的是依赖于更持久的存储系统，如AWS S3或复制的数据库。一个选择是现代*数据湖*，即在S3这样的通用存储系统上存储（Parquet）文件，并辅以Apache
    Hive或Iceberg这样的元数据层，以方便通过查询引擎访问数据。'
- en: '*Query engine*—A query engine takes a query, such as a SQL statement, that
    expresses a subset of data through selects, filters, and joins. Traditional databases,
    such as Postgres, and *data warehouses*, like Teradata, couple the first three
    layers tightly together, whereas newer systems like Trino (formerly Presto) or
    Apache Spark are query engines that are more loosely coupled with the underlying
    storage system. For *streaming data*, systems like Apache Druid or Pinot can be
    used.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*查询引擎*—查询引擎接收一个查询，例如一个SQL语句，它通过选择、过滤和连接来表示数据的一个子集。传统的数据库，如Postgres，以及*数据仓库*，如Teradata，将前三层紧密耦合在一起，而像Trino（原名Presto）或Apache
    Spark这样的新系统则是与底层存储系统松散耦合的查询引擎。对于*流数据*，可以使用Apache Druid或Pinot等系统。'
- en: '*Data loading and transformations*—Extracting, transforming, and loading (ETL)
    data is a core activity of data engineering. Traditionally, data was transformed
    before it was loaded to a data warehouse, but newer systems, like Snowflake or
    Spark, support the extract-load-transform (ELT) paradigm where raw data is loaded
    to the system first and then transformed and refined as blessed datasets. Nowadays,
    tools like DBT ([getdbt.com](https://www.getdbt.com/)) are available to make it
    easier to express and manage data transformations. Data quality can be ensured
    using tools such as Great Expectations ([greatexpectations.io](https://greatexpectations.io/)).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据加载和转换*—提取、转换和加载（ETL）数据是数据工程的核心活动。传统上，数据在加载到数据仓库之前会被转换，但像Snowflake或Spark这样的新系统支持提取-加载-转换（ELT）范式，其中原始数据首先被加载到系统中，然后作为受祝福的数据集进行转换和精炼。如今，像DBT
    ([getdbt.com](https://www.getdbt.com/))这样的工具可以用来更容易地表达和管理数据转换。可以使用Great Expectations
    ([greatexpectations.io](https://greatexpectations.io/))等工具来确保数据质量。'
- en: '*Workflow orchestrator*—ETL pipelines are often expressed as DAGs, similar
    to the data science workflows we have discussed earlier in this book. Correspondingly,
    these DAGs need to be executed by a workflow orchestrator like AWS Step Functions
    or Apache Airflow, or Apache Flink for streaming data. From a workflow orchestrator’s
    point of view, there is no difference between a data science workflow and a data
    workflow. In fact, it is often beneficial to use one centralized orchestrator
    to orchestrate all workflows.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*工作流程编排器*——ETL管道通常表示为DAG，类似于我们在本书前面讨论的数据科学工作流程。相应地，这些DAG需要由工作流程编排器如AWS Step
    Functions或Apache Airflow，或Apache Flink（用于流数据）执行。从工作流程编排器的角度来看，数据科学工作流程和数据工作流程之间没有区别。实际上，通常有益于使用一个集中的编排器来编排所有工作流程。'
- en: '*Data management*—As the volume, variety, and demands for validity of data
    increases, yet another layer of data management components is often required.
    *Data catalogues*, such as Amundsen by Lyft, can make it easier to discover and
    organize datasets. *Data governance* systems can be used to enforce security,
    data lifetime, audits and lineage, and data access policies. *Data monitoring*
    systems help to observe the overall state of all data systems, data quality, and
    ETL pipelines.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据管理*——随着数据量、种类和对有效性的需求增加，通常还需要另一层数据管理组件。*数据目录*，如Lyft的Amundsen，可以更容易地发现和组织数据集。*数据治理*系统可用于执行安全性、数据生命周期、审计和血缘以及数据访问策略。*数据监控*系统有助于观察所有数据系统的整体状态、数据质量和ETL管道。'
- en: It makes sense to start building the data infrastructure from the core outward.
    For instance, a grad student might care only about a dataset, stored in a CSV
    file on their laptop. A startup benefits from durable storage and a basic query
    engine, like Amazon Athena, which is featured in the next section. An established
    company with dedicated data engineers needs a solid setup for ETL pipelines as
    well. As the company grows to become a large multinational enterprise, they will
    add a robust set of data management tools.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 从核心向外构建数据基础设施是有意义的。例如，一个研究生可能只关心存储在他们笔记本电脑上的CSV文件中的数据集。初创公司可以从耐用的存储和基本的查询引擎中受益，如下一节中介绍的Amazon
    Athena。拥有专门数据工程师的成熟公司还需要一个稳固的ETL管道设置。随着公司成长为大型跨国企业，他们还将添加一套强大的数据管理工具。
- en: 'Correspondingly, integrations between data and data science infrastructure
    grow over time. Figure 7.12 highlights the relationships. The layers that operate
    independently from data infrastructure are depicted with dotted lines. The dotted-line
    boxes highlight what makes data science special: we need a dedicated compute layer
    that is capable of executing demanding data science applications and models.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 相应地，数据与数据科学基础设施之间的集成随着时间的推移而增长。图7.12突出了这些关系。独立于数据基础设施运行的层用虚线表示。虚线框突出了数据科学的特点：我们需要一个专门的计算层，能够执行要求高的数据科学应用程序和模型。
- en: '![CH07_F12_Tuulos](../../OEBPS/Images/CH07_F12_Tuulos.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F12_Tuulos](../../OEBPS/Images/CH07_F12_Tuulos.png)'
- en: Figure 7.12 Interfacing the data science stack with data infrastructure
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 将数据科学堆栈与数据基础设施接口
- en: 'In contrast, the other layers often benefit from interfacing with data infrastructure
    as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，其他层通常从与数据基础设施的接口中受益如下：
- en: '*Data warehouse*—In the previous section, we learned effective patterns for
    interacting with raw data, stored as Parquet files, and a durable storage system,
    S3\. In the next subsection, we will learn how to interface with a query engine.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据仓库*——在前一节中，我们学习了与存储为Parquet文件的原始数据以及持久存储系统S3交互的有效模式。在下一小节中，我们将学习如何与查询引擎接口。'
- en: '*Job scheduler*—The workflow orchestration systems we covered in chapter 2
    work equally well for data pipeline DAGs as well as data science DAGs. Because
    these DAGs are often connected—for instance, you may want to start a model training
    workflow whenever an upstream data updates—it is beneficial to execute them on
    the same system. For instance, you can use AWS Step Functions to schedule both
    data science as well as data workflows.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*作业调度器*——我们在第二章中介绍的工作流程编排系统同样适用于数据管道DAG以及数据科学DAG。因为这些DAG通常相互连接——例如，你可能希望在上游数据更新时启动模型训练工作流程——因此，在同一个系统上执行它们是有益的。例如，你可以使用AWS
    Step Functions来安排数据科学以及数据工作流程。'
- en: '*Versioning*—Assuming your data catalogue supports versioning of datasets,
    it can be beneficial to maintain data lineage all the way from upstream datasets
    to models built using the data. For instance, you can achieve this by storing
    a data version identifier, pointing to the data catalogue, as a Metaflow artifact.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*版本控制*——假设您的数据目录支持数据集的版本控制，维护从上游数据集到使用这些数据构建的模型的数据血缘是有益的。例如，您可以通过存储一个数据版本标识符，指向数据目录，作为Metaflow工件来实现这一点。'
- en: '*Model operations*—Changes in data are a common cause for failures in data
    science workflows. Besides monitoring models and workflows, it can be beneficial
    to be able to monitor source data as well.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型运维*——数据的变化是数据科学工作流程中失败的一个常见原因。除了监控模型和工作流程外，能够监控源数据也是有利的。'
- en: '*Feature engineering*—As we will discuss in section 7.3, when designing new
    features for a model, it is convenient to know what data is available, which is
    where a data catalogue can come in handy. Some data catalogues can double as feature
    stores as well.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征工程*——正如我们将在7.3节中讨论的，在设计模型的新特征时，了解可用的数据是很方便的，这正是数据目录可以派上用场的地方。一些数据目录也可以作为特征存储使用。'
- en: Concretely, the integrations can take the form of Python libraries that encode
    the normative access patterns to data. Many modern data tools and services come
    with Python client libraries, such as *AWS Data Wrangler*, featured in the next
    section, which can be used for this purpose. Similar to data infrastructure in
    general, there is no need to implement all components and integrations on day
    one. You can add integrations over time as your needs grow.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，集成可以采取Python库的形式，这些库编码了数据的标准访问模式。许多现代数据工具和服务都附带Python客户端库，例如下一节中介绍的*AWS
    Data Wrangler*，它可以用于此目的。与一般的数据基础设施一样，没有必要在第一天就实现所有组件和集成。随着需求的增长，您可以逐步添加集成。
- en: Dividing work between data scientists and data engineers
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学家和数据工程师之间划分工作
- en: The larger the company, the longer the path from raw data to models. As the
    volume, variety, and requirements of validity of data grows, it is not reasonable
    to ask one person to take care of all of it. Many companies address the issue
    by hiring dedicated data engineers, who focus on all-things-data, and data scientists
    who focus on modeling.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 公司规模越大，从原始数据到模型的路程就越长。随着数据量、种类和有效性的要求增长，要求一个人负责所有这些是不合理的。许多公司通过雇佣专门的数据工程师来解决这一问题，他们专注于所有与数据相关的工作，以及专注于建模的数据科学家。
- en: However, the boundary between data engineers and data scientists is not clear.
    For instance, if three tables contain information that a model needs, who is responsible
    for creating a joined table or a view that can be fed into a data science workflow?
    From the technical point of view, the data engineer is an expert in developing
    even complex SQL statements and optimizing joins, so maybe they should do it.
    On the other hand, the data scientist knows the model and its needs most accurately.
    Also, if the data scientist wants to iterate on the model and feature engineering,
    they shouldn’t have to bother the data engineer every time they need to make even
    a small change in the dataset.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据工程师和数据科学家之间的界限并不清晰。例如，如果三个表包含模型所需的信息，谁负责创建一个可以输入数据科学工作流程的联合表或视图？从技术角度来看，数据工程师是开发甚至复杂的SQL语句和优化连接的专家，所以他们可能应该做这件事。另一方面，数据科学家对模型及其需求了解得最准确。此外，如果数据科学家想要迭代模型和特征工程，他们不需要每次在数据集中进行哪怕微小的更改时都去打扰数据工程师。
- en: The right answer depends on the exact needs of the organization, resourcing,
    and the skill set of data engineers and data scientists involved. Figure 7.13
    suggests one proven way of dividing work, which demarcates responsibilities quite
    clearly between the two roles.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案取决于组织、资源以及涉及的数据工程师和数据科学家的技能组合的具体需求。图7.13提出了一种经过验证的工作划分方法，这种方法在两个角色之间非常清晰地划分了责任。
- en: '![CH07_F13_Tuulos](../../OEBPS/Images/CH07_F13_Tuulos.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F13_Tuulos](../../OEBPS/Images/CH07_F13_Tuulos.png)'
- en: Figure 7.13 Defining an interface between the data engineer and the data scientist
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 定义数据工程师和数据科学家之间的接口
- en: '*The data engineer* is responsible for data acquisition, that is, gathering
    raw data, data quality, and any data transformations that are needed to make the
    data available as widely consumable, authoritative, carefully curated datasets.
    In the case of structured data, the datasets are often tables with a fixed, stable
    schema. Notably, these upstream datasets should focus on *facts*—data that corresponds
    as closely as possible to directly observable raw data—leaving interpretations
    of data to downstream projects. The organization shouldn’t underestimate the demands
    of this role or undervalue it. This role is directly exposed to the chaos of the
    real world through raw data, so they play a critical role in insulating the rest
    of the organization from it. Correspondingly, the validity of all downstream projects
    depends on the quality of upstream data.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据工程师*负责数据获取，即收集原始数据、数据质量和任何必要的转换，以便将数据作为广泛可消费、权威、精心管理的数据集提供。在结构化数据的情况下，数据集通常是具有固定、稳定模式的表格。值得注意的是，这些上游数据集应专注于*事实*——尽可能接近直接可观察的原始数据的数据——将数据的解释留给下游项目。组织不应低估这个角色的需求或低估其价值。这个角色直接通过原始数据暴露于现实世界的混乱之中，因此他们在隔离组织其余部分免受其影响方面发挥着关键作用。相应地，所有下游项目的有效性都取决于上游数据的质量。'
- en: '*The data scientist* focuses on building, deploying, and operating data science
    applications. They are intimately familiar with the specific needs of each project.
    They are responsible for creating project-specific tables based on the upstream
    tables. Because they are responsible for creating these tables, they can iterate
    on them independently as often as needed.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据科学家*专注于构建、部署和运营数据科学应用。他们对每个项目的具体需求有深入了解。他们负责根据上游表创建特定项目的表格。由于他们负责创建这些表格，因此可以根据需要独立迭代它们。'
- en: The requirements for validity and stability can be looser for project-specific
    tables, depending on the needs of each project, because these tables are tightly
    coupled with a specific workflow that is also managed by the same data scientist
    or a small team of scientists. When dealing with large datasets, it is also beneficial
    that the data scientist can affect the layout and partitioning of the table, which
    can have massive performance implications when it comes to ingesting the table
    to the workflow, as discussed in the previous section.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定项目的表格，对有效性和稳定性的要求可以更宽松，这取决于每个项目的需求，因为这些表格与由同一数据科学家或小型科学家团队管理的特定工作流程紧密耦合。在处理大型数据集时，如果数据科学家可以影响表格的布局和分区，这也是有益的，这将在前一段中讨论的将表格摄入工作流程时产生巨大的性能影响。
- en: Crucially, this arrangement works only if the data infrastructure, and the query
    engine in particular, is robust enough so to manage suboptimal queries. We can’t
    assume that every data scientist is also a world-class data engineer, but it is
    still convenient to allow them to execute and schedule queries independently.
    Historically, many data warehouses broke too easily under suboptimal queries,
    so it wouldn’t have been feasible to let non-experts run arbitrary queries. The
    modern data infrastructure should be able to isolate queries so that this isn’t
    a problem anymore.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的是，这种安排只有在数据基础设施，尤其是查询引擎足够健壮，能够管理次优查询的情况下才能有效。我们不能假设每个数据科学家也都是世界级的数据工程师，但仍然方便让他们独立执行和安排查询。从历史上看，许多数据仓库在次优查询下很容易崩溃，因此让非专家运行任意查询是不可行的。现代数据基础设施应该能够隔离查询，这样这个问题就不再是问题。
- en: Details of data engineering are out of scope of this book. However, questions
    pertaining to the work of a data scientist are in scope, so in the next section,
    we will investigate how one can author project-specific ETL, interfacing with
    a query engine, as a part of a data science workflow.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程的具体细节不在此书的讨论范围之内。然而，与数据科学家工作相关的问题则属于讨论范围，因此在下一段落中，我们将探讨一个人如何编写特定项目的ETL，与查询引擎交互，作为数据科学工作流程的一部分。
- en: 7.2.2 Preparing datasets in SQL
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 在SQL中准备数据集
- en: In this section, we will learn how to use a query engine such as Trino or Apache
    Spark, or a data warehouse like Redshift or Snowflake, to prepare datasets that
    can be efficiently loaded in a data science workflow using the patterns we learned
    in the previous section. We will demonstrate the concept, illustrated in figure
    7.14, using a managed cloud-based query engine, Athena, but you can use the same
    approach with other systems, too.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用查询引擎，如 Trino 或 Apache Spark，或数据仓库如 Redshift 或 Snowflake，来准备可以使用我们在上一节中学到的模式高效加载到数据科学工作流程中的数据集。我们将使用图
    7.14 中展示的概念，通过一个基于云的托管查询引擎 Athena 来演示，但您也可以使用相同的方法来使用其他系统。
- en: '![CH07_F14_Tuulos](../../OEBPS/Images/CH07_F14_Tuulos.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F14_Tuulos](../../OEBPS/Images/CH07_F14_Tuulos.png)'
- en: Figure 7.14 Using a query engine with an S3-based data lake
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14 使用基于 S3 的数据湖的查询引擎
- en: First, we will need to load data files to S3 and register them as a table with
    a suitable schema, stored in table metadata. Athena uses the popular Apache Hive
    format for its metadata.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要将数据文件加载到 S3 并将其注册为具有合适模式的表，存储在表元数据中。Athena 使用流行的 Apache Hive 格式来存储其元数据。
- en: After this, we can start querying the table. We will create a workflow that
    sends a SQL query to Athena, selecting a subset of the original fact table and
    writing the results as a new table. This type of query is called *Create Table
    as Select* (CTAS). CTAS queries work well for our needs, because they make it
    possible to download results from S3 using the fast data patterns we learned in
    the previous section.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以开始查询表。我们将创建一个工作流程，将 SQL 查询发送到 Athena，选择原始事实表的子集，并将结果写入新表。这种查询称为 *创建表选择*（CTAS）。CTAS
    查询非常适合我们的需求，因为它们使我们能够使用我们在上一节中学到的快速数据模式从 S3 下载结果。
- en: We will use the open source library *AWS Data Wrangler* ([https://github.com/awslabs/aws-data-wrangler](https://github.com/awslabs/aws-data-wrangler))
    to interface with Athena. AWS Data Wrangler makes it easy to read and write data
    from various databases and data warehouses that AWS provides. However, it is not
    hard to adapt the examples to use other client libraries that provide similar
    functionality.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用开源库 *AWS Data Wrangler* ([https://github.com/awslabs/aws-data-wrangler](https://github.com/awslabs/aws-data-wrangler))
    来与 Athena 进行接口。AWS Data Wrangler 使得从 AWS 提供的各种数据库和数据仓库中读取和写入数据变得容易。然而，将示例修改为使用提供类似功能的其他客户端库并不困难。
- en: If you don’t want to test a query engine at this moment, you can skip this subsection
    and move on to the next one. In the next subsection, we will see how we can postprocess
    data in a workflow.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您此时不想测试查询引擎，可以跳过本小节，继续下一节。在下一节中，我们将看到如何在工作流程中后处理数据。
- en: Setting up a table on Athena
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Athena 上设置表
- en: Amazon Athena is a serverless query engine, based on Trino, requiring no upfront
    setup. Just make sure that your IAM user or Batch role (METAFLOW_ECS_S3_ACCESS_
    IAM_ROLE in the Metaflow config) has a policy called AmazonAthenaFullAccess attached
    to it, which allows Athena queries to be executed. Also, make sure the AWS region
    is set in your configuration, for example, by setting the environment variable
    AWS_DEFAULT_REGION=us-east-1.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Athena 是一个基于 Trino 的无服务器查询引擎，无需预先设置。只需确保您的 IAM 用户或 Batch 角色（在 Metaflow
    配置中的 METAFLOW_ECS_S3_ACCESS_IAM_ROLE）已附加名为 AmazonAthenaFullAccess 的策略，这允许执行 Athena
    查询。此外，请确保在您的配置中设置了 AWS 区域，例如，通过设置环境变量 AWS_DEFAULT_REGION=us-east-1。
- en: 'As a test dataset, we will use a subset of NYC Taxi Trip data, which we first
    used in listing 7.2\. We will initialize a table with a year’s worth of data,
    some 160 million rows, *partitioning* the table by month. In this case, partitioning
    simply organizes files as directories, allowing queries that read only a subset
    of months to finish much faster, because the query engine can skip whole directories
    of files. Partitioning requires a certain naming scheme, with directories prefixed
    with month=, which is why we copy files from their original location to a new
    S3 location that follows the desired naming scheme. The desired path structure
    looks like this:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 作为测试数据集，我们将使用纽约市出租车行程数据的子集，这是我们首次在列表 7.2 中使用的数据。我们将初始化一个包含一年数据的表，大约有 1.6 亿行，通过月份对表进行
    *分区*。在这种情况下，分区只是将文件组织成目录，允许只读取部分月份的查询完成得更快，因为查询引擎可以跳过整个文件目录。分区需要特定的命名方案，目录以 month=
    前缀开头，这就是为什么我们将文件从原始位置复制到遵循所需命名方案的新的 S3 位置。所需的路径结构如下所示：
- en: '[PRE12]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The prefix until /nyc_taxi/ will look different in your case because it depends
    on your Metaflow config. The key part is the suffix after nyc_taxi, in particular
    month=11, which is used for partitioning.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 直到 /nyc_taxi/ 的前缀在你的情况下可能会有所不同，因为它取决于你的 Metaflow 配置。关键部分是 nyc_taxi 后的后缀，特别是
    month=11，它用于分区。
- en: 'To create a table, we need a predefined schema. We will create a schema specification
    by introspecting schema in the Parquet files. The table is registered with a data
    catalogue service called *AWS Glue**,* which is tightly integrated with Athena.
    Listing 7.4 packs all these actions, downloading and uploading data in a desired
    hierarchy, schema definition, and table creation in a single Metaflow workflow.
    Here’s how the code works:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个表，我们需要一个预定义的模式。我们将通过检查 Parquet 文件中的模式来创建一个模式规范。该表通过一个称为 *AWS Glue* 的数据目录服务进行注册，该服务与
    Athena 紧密集成。列表 7.4 将所有这些操作打包在一个 Metaflow 工作流中，包括下载和上传数据到所需的层次结构、模式定义和表创建。以下是代码的工作方式：
- en: 'The start step takes care of two things:'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始步骤处理两件事：
- en: It copies Parquet files to a partitioned directory hierarchy in S3\. This requires
    the creation of new path names, which is accomplished with the utility function
    make_key. Note that by initializing the S3 client with S3(run=self), Metaflow
    chooses a suitable S3 root for the files that is versioned with the run ID, allowing
    us to test different versions of the code safely without having to fear overwriting
    previous results. The root path of the resulting hierarchy is stored in an artifact
    called s3_prefix.
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将 Parquet 文件复制到 S3 中的分区目录层次结构。这需要创建新的路径名，通过 make_key 工具函数实现。请注意，通过使用 S3(run=self)
    初始化 S3 客户端，Metaflow 为具有版本号的文件选择了一个合适的 S3 根目录，该版本号与运行 ID 相关联，这使得我们可以在不担心覆盖先前结果的情况下安全地测试代码的不同版本。结果层次结构的根路径存储在一个名为
    s3_prefix 的工件中。
- en: It inspects the schema of the Parquet files. We assume that all files have the
    same schema, so it suffices to look at the schema of the first file. The schema
    of the Parquet files uses slightly different names than the Hive format, which
    Athena uses, so we rename the fields based on the TYPES mapping using the hive_field
    utility function. The resulting schema is stored in an artifact, schema.
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它检查 Parquet 文件的模式。我们假设所有文件都有相同的模式，因此只需查看第一个文件的模式即可。Parquet 文件的模式使用与 Athena 使用的
    Hive 格式略有不同的名称，因此我们使用 hive_field 工具函数根据 TYPES 映射重命名字段。结果模式存储在一个名为 schema 的工件中。
- en: Equipped with suitably laid-out Parquet files and a schema, we can set up a
    table in the end step. As an initialization step, we create a database in Glue,
    called dsinfra_test by default. The call will raise an exception if the database
    already exists, which we can safely ignore. After this, we can create a bucket
    for Athena to store its results and register a new table. The repair_table call
    makes sure that the newly created partitions are included in the table.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置了适当布局的 Parquet 文件和模式后，我们可以在最终步骤中设置一个表。作为一个初始化步骤，我们创建一个默认名为 dsinfra_test 的数据库。如果数据库已存在，调用将引发异常，我们可以安全地忽略它。之后，我们可以为
    Athena 创建一个存储其结果的存储桶并注册一个新表。repair_table 调用确保新创建的分区包含在表中。
- en: After these steps, the table is ready for querying!
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些步骤之后，表就准备好查询了！
- en: Listing 7.4 Loading taxi data in Athena
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.4 在 Athena 中加载出租车数据
- en: '[PRE13]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Database name—you can choose any name
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 数据库名称——你可以选择任何名称
- en: ❷ NYC taxi data at a public bucket
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在公共存储桶中的纽约出租车数据
- en: ❸ Maps some types in Parquet schema to Hive format used by Glue
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将 Parquet 模式中的某些类型映射到 Glue 使用的 Hive 格式
- en: ❹ Defines the table name, optionally
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义表名，可选
- en: ❺ S3 object key (path) that conforms with our partitioning schema
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 符合我们分区方案的 S3 对象密钥（路径）
- en: ❻ Maps Parquet types to Hive types, used by Glue
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将 Parquet 类型映射到 Glue 使用的 Hive 类型
- en: ❼ Downloads data and generates new keys
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 下载数据并生成新的密钥
- en: ❽ Introspects schema from the first Parquet file and maps it to Hive
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 从第一个 Parquet 文件中检查模式并将其映射到 Hive
- en: ❾ Uploads data to a Metaflow run-specific location
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 将数据上传到 Metaflow 运行特定的位置
- en: ❿ Saves the new S3 location in an artifact
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 将新的 S3 位置保存到工件中
- en: ⓫ Creates a new database in Glue and ignores failures caused by the database
    already existing
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 在 Glue 中创建一个新的数据库并忽略由数据库已存在引起的失败
- en: ⓬ Initializes a bucket for CTAS results
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 初始化一个用于 CTAS 结果的存储桶
- en: ⓭ Registers a new table with the new location and schema
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ⓭ 使用新的位置和模式注册一个新的表
- en: ⓮ Requests Athena to discover newly added partitions
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ⓮ 请求 Athena 发现新添加的分区
- en: 'Save the code in taxi_loader.py. Running the flow will upload and download
    about 4.2 GB of data, so it is advisable to run it on Batch or on a cloud workstation.
    You can run the flow as usual:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到 taxi_loader.py 文件中。运行流程将上传和下载大约 4.2 GB 的数据，因此建议在批处理或云工作站上运行。你可以像平常一样运行流程：
- en: '[PRE14]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: On a large cloud workstation, the flow should take less than 30 seconds to execute.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型云工作站上，该流程的执行时间应少于 30 秒。
- en: Versioned data with metaflow.S3
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 metaflow.S3 进行版本化数据
- en: Listing 7.4 uploads data to S3 without specifying a bucket or an explicit S3
    URL. This is possible because the S3 client was initialized as S3(run=self)*,*
    which tells Metaflow to refer to a *run-specific location* by default. Metaflow
    creates an S3 URL based on its datastore location, prefixing keys with a run ID.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.4 上传数据到 S3 而不指定存储桶或显式的 S3 URL。这是可能的，因为 S3 客户端被初始化为 S3(run=self)*，这告诉 Metaflow
    默认引用 *运行特定位置*。Metaflow 根据其数据存储位置创建一个 S3 URL，并在键前加上运行 ID。
- en: This mode is useful when storing data in S3 that needs to be accessible by other
    systems (data that is internal to workflows can be stored as artifacts). Because
    data is stored relative to the run ID, any data you upload is automatically versioned,
    making sure that each run writes an independent version or copy of data without
    accidentally overwriting unrelated results. Afterward, if you need to track what
    data was produced by a run, you can find the data based on the run ID, allowing
    *data lineage* to be maintained.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 此模式在将数据存储在 S3 且需要其他系统可访问的情况下很有用（工作流程内部的数据可以存储为工件）。因为数据是相对于运行 ID 存储的，所以上传的任何数据都会自动进行版本控制，确保每次运行都独立写入或复制数据，从而避免意外覆盖相关结果。之后，如果你需要跟踪某个运行产生的数据，可以根据运行
    ID 查找数据，从而维护 *数据血缘*。
- en: After the run has completed successfully, you can open the Athena console and
    confirm that a new table, nyc_taxi, is discoverable under the database dsinfra_test.
    The console includes a convenient query editor that allows you to query any table
    in SQL. For instance, you can see a small preview of data by executing SELECT
    * FROM nyc_taxi LIMIT 10. Figure 7.15 shows what the console should look like.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行成功完成后，你可以打开 Athena 控制台，确认在数据库 dsinfra_test 下可以找到新的表 nyc_taxi。控制台包括一个方便的查询编辑器，允许你使用
    SQL 查询任何表。例如，你可以通过执行 SELECT * FROM nyc_taxi LIMIT 10 来查看数据的小型预览。图 7.15 展示了控制台应有的样子。
- en: '![CH07_F15_Tuulos](../../OEBPS/Images/CH07_F15_Tuulos.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F15_Tuulos](../../OEBPS/Images/CH07_F15_Tuulos.png)'
- en: Figure 7.15 Querying Taxi data on the Athena console
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.15 在 Athena 控制台上查询出租车数据
- en: If you can see the table and its columns in the console, and the test query
    returns rows with values, the table is ready for use! Next, we will create a flow
    that executes CTAS queries against the table, which allows us to create arbitrary
    subsets of data to be consumed by workflows.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你可以在控制台中看到表及其列，并且测试查询返回了带有值的行，则该表已准备好使用！接下来，我们将创建一个执行针对该表的 CTAS 查询的流程，这允许我们创建任意数据子集以供工作流程使用。
- en: Running CTAS queries
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 CTAS 查询
- en: How should a data scientist execute project-specific SQL queries? One option
    is to use tools provided by data infrastructure, following best practices used
    by data engineers. A downside of this approach is that the query is decoupled
    from the workflow that relies on it. A better approach might be to execute the
    query as a part of the workflow, as demonstrated next.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家应该如何执行特定项目的 SQL 查询？一个选项是使用数据基础设施提供的数据工具，遵循数据工程师使用的最佳实践。这种方法的一个缺点是查询与依赖它的工作流程解耦。可能更好的方法是作为工作流程的一部分执行查询，如下所示。
- en: You could embed SQL statements as strings in your Python code, but to benefit
    from proper syntax highlighting and checking in IDEs, and to make the code more
    readable overall, we can store them as separate files. To test the idea, let’s
    create a query that selects a subset of the newly created nyc_taxi table. The
    next code listing shows a SQL statement that selects taxi rides that start during
    business hours between 9 a.m. and 5 p.m.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 Python 代码中将 SQL 语句作为字符串嵌入，但为了从 IDE 中的正确语法高亮和检查中受益，以及使代码整体更易于阅读，我们可以将它们存储为单独的文件。为了测试这个想法，让我们创建一个查询，该查询选择新创建的
    nyc_taxi 表的子集。下面的代码列表显示了一个选择在上午 9 点到下午 5 点之间工作时间内开始的出租车行程的 SQL 语句。
- en: Listing 7.5 SQL query to extract data for business hours
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.5 提取商业时段数据的 SQL 查询
- en: '[PRE15]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Save this SQL statement in a new subdirectory, sql, in a file sql/taxi_etl.sql.
    If you wonder about the time logic, pickup_at / 1000 is required because timestamps
    in the dataset are expressed as milliseconds but from_unixtime expects seconds.
    Now we can write a flow that executes the query, shown next.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 将此 SQL 语句保存到一个新的子目录 sql 中，文件为 sql/taxi_etl.sql。如果您对时间逻辑感到好奇，pickup_at / 1000
    是必需的，因为数据集中的时间戳以毫秒表示，但从_unixtime 需要秒。现在我们可以编写一个执行查询的流程，如下所示。
- en: Listing 7.6 A flow with parameters
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.6 带参数的流程
- en: '[PRE16]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Attaches the flow to a project. This will come in handy in the next section.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将流程附加到项目。在下一节中这将很有用。
- en: ❷ Creates a result table name based on the ID of the current task
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 根据当前任务的 ID 创建一个结果表名称
- en: ❸ Formats a CTAS SQL query and stores it as an artifact
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 格式化 CTAS SQL 查询并将其存储为自述文件
- en: ❹ Submits the query to Athena
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将查询提交到 Athena
- en: ❺ Waits for the query to complete
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 等待查询完成
- en: ❻ Lists all Parquet files in the result set
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 列出结果集中所有的 Parquet 文件
- en: ❼ Formats and submits a query, and stores the URLs of the resulting Parquet
    files
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 格式化并提交查询，并存储生成的 Parquet 文件的 URL
- en: TaxiETLFlow is a general-purpose flow that implements the pattern depicted in
    the beginning of this section in figure 7.14\. It reads an arbitrary SELECT statement
    from a file, sql/taxi_etl.sql, converts it into a CTAS query by prefixing it with
    CREATE TABLE, submits the query, waits for it to finish, and stores the paths
    to the resulting Parquet files as an artifact, so they can be easily consumed
    by other downstream flows, an example of which we will see in the next section.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: TaxiETLFlow 是一个通用流程，它实现了本节开头图 7.14 中描述的模式。它从一个文件中读取任意 SELECT 语句，sql/taxi_etl.sql，通过在其前面添加
    CREATE TABLE 转换为 CTAS 查询，提交查询，等待其完成，并将结果 Parquet 文件的路径作为自述文件存储，以便它们可以轻松地被其他下游流程消费，下一节我们将看到一个例子。
- en: Tip You can use the current object to introspect the currently executing run,
    as shown in listing 7.6\. It can come in handy if you want to know the current
    run ID, task ID, or the user who is executing the run.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：您可以使用当前对象来检查当前正在执行的运行，如图 7.6 所示。如果您想了解当前的运行 ID、任务 ID 或执行运行的用户，这会很有用。
- en: 'Save the code to taxi_etl.py. The flow needs an extra SQL file, so we use the
    --package- suffixes option, discussed in the previous chapter, to include all
    .sql files in the code package. Run the flow as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到 taxi_etl.py。该流程需要一个额外的 SQL 文件，因此我们使用上一章中讨论的 --package- 后缀选项，将所有 .sql
    文件包含在代码包中。按照以下方式运行流程：
- en: '[PRE17]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: After the run has completed, you can log in to the Athena console and click
    the History tab to confirm the status of the query, as shown in figure 7.16.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 运行完成后，您可以登录到 Athena 控制台并点击“历史”选项卡以确认查询状态，如图 7.16 所示。
- en: '![CH07_F16_Tuulos](../../OEBPS/Images/CH07_F16_Tuulos.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F16_Tuulos](../../OEBPS/Images/CH07_F16_Tuulos.png)'
- en: Figure 7.16 Query status on the Athena console
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16 Athena 控制台上的查询状态
- en: Because the run and task IDs are embedded in the table name, in this case, mf_ctas_TaxiETLFlow_1631494834745839_start_1
    in figure 7.16, it is easy to draw the connection between queries and runs. Vice
    versa, by storing the query to be executed as an artifact, self.ctas, and its
    results in another artifact, self.paths, we can form a full data lineage from
    the source data, the queries that process it, to the final output, for example,
    models, produced by workflows. Having this lineage can become extremely useful
    when debugging any issues related to the quality of predictions or input data.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 由于运行和任务 ID 嵌入在表名称中，例如图 7.16 中的 mf_ctas_TaxiETLFlow_1631494834745839_start_1，因此很容易在查询和运行之间建立联系。反之，通过将待执行的查询作为自述文件
    self.ctas 存储并其结果存储在另一个自述文件 self.paths 中，我们可以从源数据、处理它的查询到最终输出（例如，工作流程产生的模型）形成一个完整的数据血缘。在调试与预测质量或输入数据相关的问题时，这种血缘关系可能非常有用。
- en: Recommendation Maintain data lineage between data infrastructure and data science
    workflows by including run IDs in the queries executed and by logging queries
    as artifacts.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 建议：通过在执行的查询中包含运行 ID 并将查询作为自述文件记录，在数据基础设施和数据科学工作流程之间维护数据血缘。
- en: As an exercise, you can adapt the example to work with another modern query
    engine. The same CTAS pattern works, for instance, with Spark, Redshift, or Snowflake.
    You can also schedule the ETL workflow to run periodically on a production-grade
    scheduler, possibly the same one used by the data engineering team, by using the
    techniques we learned in the previous chapter.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，你可以将示例修改为与另一个现代查询引擎一起工作。相同的CTAS模式适用于Spark、Redshift或Snowflake。你还可以使用我们在上一章中学到的技术，通过生产级调度器定期调度ETL工作流程，可能就是数据工程团队使用的同一个调度器。
- en: Regardless of what query engine you use, the general capability is what matters.
    Using this approach, a data scientist can include a project-specific data preprocessing
    step in their workflow and offload large-scale data crunching to a scalable query
    engine. In the next section, we will see how to access the results of even a large
    CTAS query by leveraging horizontal scalability and the fast data approach we
    learned earlier.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你使用什么查询引擎，重要的是通用能力。使用这种方法，数据科学家可以在他们的工作流程中包含一个特定项目的数据处理步骤，并将大规模数据处理任务卸载到可扩展的查询引擎。在下一节中，我们将看到如何通过利用我们之前学到的水平扩展和快速数据方法来访问即使是大型CTAS查询的结果。
- en: Cleaning old results
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 清理旧结果
- en: By default, the result tables produced by CTAS queries are persisted forever.
    This is beneficial for reproducibility and auditability, because you can examine
    any old results. However, it is a reasonable practice to delete old results after
    some period of time.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，CTAS查询生成的结果表将永久保留。这对可重复性和可审计性有益，因为你可以检查任何旧结果。然而，在一段时间后删除旧结果是合理的做法。
- en: You can delete old data in S3 by using *S3 lifecycle policies*. Because results
    are just locations in S3, you can set a policy that deletes all objects at the
    location where CTAS results are written after a predetermined period, such as
    after 30 days. In addition, you want to delete table metadata from Glue.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用*S3生命周期策略*来删除S3中的旧数据。因为结果只是S3中的位置，你可以设置一个策略，在预定时间后（例如30天后）删除CTAS结果写入位置的所有对象。此外，你还需要从Glue中删除表元数据。
- en: 'You can delete Glue tables by executing DROP TABLE SQL statements with AWS
    Data Wrangler, or by using the following AWS CLI command:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过执行带有AWS Data Wrangler的DROP TABLE SQL语句或使用以下AWS CLI命令来删除Glue表：
- en: '[PRE18]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: with a list of tables to be deleted as arguments.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 作为要删除的表的参数列表。
- en: 7.2.3 Distributed data processing
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 分布式数据处理
- en: In section 7.1, we learned how to load Parquet files quickly from S3 to a single
    instance. This is a simple and robust pattern for datasets that can fit in memory.
    When combined with large instances with hundreds of gigabytes of RAM and efficient
    in-memory representations provided by Apache Arrow and NumPy, you can handle massive
    datasets, in some cases billions of data points, without having to resort to distributed
    computing and the overhead that comes with it.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在7.1节中，我们学习了如何快速将Parquet文件从S3加载到单个实例。这是一个简单且健壮的模式，适用于可以放入内存的数据集。当与具有数百GB RAM的大实例以及Apache
    Arrow和NumPy提供的有效内存表示相结合时，你可以处理大规模数据集，在某些情况下，可以处理数十亿个数据点，而无需求助于分布式计算及其带来的开销。
- en: 'Naturally this approach has its limits. Obviously, not all datasets fit in
    memory at once. Or, maybe doing complex processing of hundreds of gigabytes of
    data on a single instance is simply too slow. In cases like this, it is a good
    idea to fan out computation to multiple instances. In this section, we learn how
    to load and process data, such as tables produced by CTAS queries, in a distributed
    manner. As before, we will use Metaflow’s foreach construct for distribution,
    but you can apply the same pattern with other frameworks that support distributed
    computation. The lessons learned in this chapter come in handy in the following
    two scenarios that are common in data science workflows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 自然，这种方法有其局限性。显然，并非所有数据集都能一次性放入内存。或者，可能在单个实例上对数百GB的数据进行复杂处理可能太慢。在这种情况下，将计算扇出到多个实例是一个好主意。在本节中，我们将学习如何以分布式方式加载数据和处理数据，例如CTAS查询生成的表。像之前一样，我们将使用Metaflow的foreach构造来分布，但你也可以使用支持分布式计算的其他框架应用相同的模式。本章学到的经验在以下两个常见于数据科学工作流程的场景中很有用：
- en: '*Loading and processing separate subsets of data in each branch*—For instance,
    in contrast to the K-means example in chapter 5, which used the same dataset to
    train all models, you can load a different subset, say, data specific to a country,
    in each foreach task.'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*在每个分支中加载和处理数据的不同子集*——例如，与第5章中的K-means示例不同，该示例使用相同的数据集来训练所有模型，您可以在每个foreach任务中加载不同的子集，例如，针对特定国家的数据。'
- en: '*Efficient data preprocessing*—It is often convenient to do basic data extraction
    in SQL, followed by more advanced preprocessing in Python. We will use this pattern
    for feature engineering later in this chapter and in chapter 9.'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*高效的数据预处理*——通常在SQL中进行基本数据提取后，再在Python中进行更高级的预处理，这样做很方便。我们将在本章和第9章的后续部分使用这种模式进行特征工程。'
- en: A common characteristic in both of these cases is that we want to process *sharded,*
    or chunked, data, instead of all data at once. Figure 7.17 illustrates the high-level
    pattern.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种情况的一个共同特点是，我们希望处理*分片*或分块的数据，而不是一次性处理所有数据。图7.17说明了这种高级模式。
- en: '![CH07_F17_Tuulos](../../OEBPS/Images/CH07_F17_Tuulos.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F17_Tuulos](../../OEBPS/Images/CH07_F17_Tuulos.png)'
- en: Figure 7.17 Processing sharded data in a workflow
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17 工作流中处理分片数据
- en: 'A CTAS query produces a set of Parquet files that we can either partition by
    a column (e.g., by country) or we can divide them to roughly equal-sized shards.
    A workflow fans out a foreach over the shards, distributing data processing over
    multiple parallel steps. We can then combine the results in the join step. To
    highlight the similarity of this pattern to *the MapReduce paradigm* of distributed
    computing, figure 7.17 calls the foreach steps *Mappers* and the join step *Reduce*.
    To see how this pattern works in practice, let’s work through a fun example: visualization
    of the taxi dataset.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: CTAS查询生成一组Parquet文件，我们可以通过一列（例如，按国家）对其进行分区，或者将它们分成大小大致相等的分片。工作流将foreach操作扩展到分片上，将数据处理分布在多个并行步骤中。然后我们可以在连接步骤中合并结果。为了突出这种模式与分布式计算中的*MapReduce范式*的相似性，图7.17将foreach步骤称为*Mappers*，将连接步骤称为*Reduce*。为了了解这种模式在实际中的工作原理，让我们通过一个有趣的例子来探讨：出租车数据集的可视化。
- en: 'Example: Visualizing a large dataset'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：可视化大型数据集
- en: 'The following example works in two modes: it can either ingest an arbitrary
    subset of the taxi dataset produced by TaxiETLFlow (listing 7.6), or it can load
    raw data as-is. You can use the raw data mode if you didn’t set up Athena in the
    previous section. In both cases, we will extract the latitude and longitude of
    the pickup location of each taxi trip and plot it on an image.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例有两种工作模式：它可以读取由TaxiETLFlow生成的出租车数据集的任意子集（列表7.6），或者它可以以原始数据的形式加载数据。如果您在上一节中没有设置Athena，则可以使用原始数据模式。在两种情况下，我们将提取每辆出租车行程的接车位置的纬度和经度，并在图像上绘制它。
- en: The task is made nontrivial by the fact that there are *many* trips in the dataset.
    In the raw mode, we process all trips in the year 2014, which amounts to 48 million
    data points. To demonstrate how large datasets like this can be processed efficiently,
    we perform preprocessing in parallel, as depicted in figure 7.17.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集中有*很多*行程，这个任务变得非同寻常。在原始模式下，我们处理2014年的所有行程，这相当于4800万个数据点。为了展示如何高效地处理这类大型数据集，我们像图7.17所示的那样并行进行预处理。
- en: Let’s start by writing a helper function that plots points on an image. Due
    to the scale of data, we’ll use a specialized open source library, *Datashader*
    ([datashader.org](https://datashader.org/)), that is tuned to handle millions
    of points efficiently. The helper function is shown in the next listing.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先编写一个辅助函数，该函数在图像上绘制点。由于数据量很大，我们将使用一个专门的开源库*Datashader* ([datashader.org](https://datashader.org/))，该库经过调整，可以高效地处理数百万个点。辅助函数在下一列表中显示。
- en: Listing 7.7 Plotting taxi trip coordinates
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.7 绘制出租车行程坐标
- en: '[PRE19]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Defines a bounding box for lower Manhattan
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义下曼哈顿的边界框
- en: '❷ Accepts points as two arrays: latitude and longitude'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 接受两个数组作为点：纬度和经度
- en: ❸ Plots points, shading each pixel logarithmically
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 以对数方式着色每个像素绘制点
- en: ❹ Saves the visualization as an artifact
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将可视化保存为工件
- en: Save the code to taxiviz.py, which we will import in the flow that follows.
    Note that the number of data points (up to 48 million) is much higher than the
    number of pixels (1 million) in the image. Hence, we will shade each pixel according
    to how many points hit it. We use a logarithmic color range to make sure that
    the faintest pixels don’t wash away. Similar to the forecast plots in the previous
    chapter, we store the resulting image as an artifact, so it is versioned and stored
    in a run where it can be retrieved using the Client API and shown, for example,
    in a notebook.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到taxiviz.py中，我们将在后续的流程中导入。请注意，数据点的数量（高达4800万）比图像中的像素数量（100万）多得多。因此，我们将根据击中每个像素的点数对其进行着色。我们使用对数颜色范围以确保最淡的像素不会被冲淡。类似于前一章中的预测图，我们将结果图像存储为工件，因此它具有版本控制并存储在运行中，可以使用客户端API检索并显示，例如在笔记本中。
- en: 'Next, let’s implement the flow itself—see the following listing. The flow implements
    the pattern depicted in figure 7.17\. In the start step, we choose what input
    data to use: either the results of a CTAS query from a previously executed TaxiETLFlow
    or raw data. We divide data into shards, each of which is processed by an independent
    preprocess_data task. The join step merges together data, arrays of coordinates,
    produced by each shard and plots the coordinates on an image.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们实现流程本身——请参见以下列表。该流程实现了图7.17中描述的模式。在起始步骤中，我们选择要使用的数据输入：要么是之前执行的TaxiETLFlow的CTAS查询结果，要么是原始数据。我们将数据划分为分片，每个分片由一个独立的前处理数据任务进行处理。连接步骤将每个分片产生的坐标数组合并在一起，并在图像上绘制坐标。
- en: Listing 7.8 A flow with parameters
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.8 带参数的流程
- en: '[PRE20]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Imports the helper function that we created earlier
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入我们之前创建的辅助函数
- en: ❷ In the raw data mode, uses data from 2014
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在原始数据模式下，使用2014年的数据
- en: ❸ A mapper function that preprocesses data for each shard
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 一个用于为每个分片预处理数据的mapper函数
- en: ❹ As an example, we include only trips with more than one passenger.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 例如，我们只包括有超过一名乘客的行程。
- en: ❺ Uses the same project as TaxiETLFlow
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用与TaxiETLFlow相同的工程
- en: '❻ Chooses between the two modes: CTAS or raw data'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 在两种模式之间进行选择：CTAS或原始数据
- en: ❼ In the CTAS mode, retrieves paths to CTAS results
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 在CTAS模式下，检索到CTAS结果的路径
- en: ❽ In the raw data mode, lists paths to raw data
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 在原始数据模式下，列出原始数据的路径
- en: ❾ Groups all paths into four roughly equal-sized shards
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 将所有路径分组为四个大致相等大小的分片
- en: ❿ Mapper step, processing each shard
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ Mapper步骤，处理每个分片
- en: ⓫ Downloads data for this shard and decodes it as a table
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 下载此分片的数据并将其解码为表
- en: ⓬ Processes the table
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 处理表
- en: ⓭ Stores coordi-nates from the processed table
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ⓭ 存储处理表中的坐标
- en: ⓮ Concatenates coordinates from all shards
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: ⓮ 连接所有分片的坐标
- en: ⓯ Visualizes and stores results as an artifact
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ⓯ 可视化并将结果存储为工件
- en: 'Save the code to taxi_plotter.py. If you ran TaxiETLFlow previously, you can
    run the flow like this:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到taxi_plotter.py。如果您之前运行了TaxiETLFlow，您可以像这样运行流程：
- en: '[PRE21]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Otherwise, leave the option out to use the raw data directly. The flow runs
    for about a minute on a large instance. After the run has completed, you can open
    a notebook and type the following lines in a cell to see the results:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，省略该选项以直接使用原始数据。该流程在大实例上运行大约一分钟。运行完成后，您可以在笔记本中打开一个单元格并输入以下行以查看结果：
- en: '[PRE22]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The result should look something akin to one of the visualizations in figure
    7.18\. On the left, figure 7.18 shows the image of a full dataset. You can see
    the Midtown area being very popular (light shade). The image on the right was
    produced by a CTAS query that shows trips only during one hour between midnight
    and 1 a.m.—many areas outside the Midtown area are sparsely trafficked.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应该类似于图7.18中的某个可视化。图7.18左侧显示了完整数据集的图像。您可以看到Midtown地区非常受欢迎（浅色）。右侧的图像是由一个CTAS查询生成的，该查询显示了午夜到凌晨1点之间的一小时的行程——Midtown地区以外的许多地区交通稀疏。
- en: '![CH07_F18_Tuulos](../../OEBPS/Images/CH07_F18_Tuulos.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F18_Tuulos](../../OEBPS/Images/CH07_F18_Tuulos.png)'
- en: Figure 7.18 Visualizing pickup locations in two different subsets of the taxi
    dataset
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18 在出租车数据集的两个不同子集中可视化接车位置
- en: 'The code in listing 7.8 demonstrates the following three important concepts:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.8中的代码演示了以下三个重要概念：
- en: Thanks to @project, we can safely separate an upstream data processing flow,
    TaxiETLFlow, from other downstream business logic like TaxiPlotterFlow. Crucially,
    Flow('TaxiETLFlow').latest_run points not at any random latest run of TaxiETLFlow
    but to the latest run that exists in the flow’s own namespace, as discussed in
    chapter 6\. This allows multiple data scientists to work on their own versions
    of the TaxiETLFlow→TaxiPlotterFlow sequence without interfering with each other’s
    work.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多亏了 @project，我们可以安全地将上游数据处理流程 TaxiETLFlow 与其他下游业务逻辑（如 TaxiPlotterFlow）分离。关键的是，Flow('TaxiETLFlow').latest_run
    指的不是 TaxiETLFlow 的任何随机最新运行，而是存在于流程自身命名空间中的最新运行，正如第 6 章所讨论的那样。这允许多个数据科学家在自己的 TaxiETLFlow→TaxiPlotterFlow
    序列版本上工作，而不会相互干扰。
- en: The process_data function demonstrates the mapper concept. After a raw dataset
    has been extracted with SQL in a CTAS query, the data scientist can further process
    data in Python, instead of having to pack all project-specific logic in the SQL
    query. Also, we avoid potentially memory-inefficient conversion to pandas.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: process_data 函数展示了映射器概念。在 CTAS 查询中使用 SQL 提取原始数据集之后，数据科学家可以在 Python 中进一步处理数据，而不是必须将所有项目特定的逻辑打包在
    SQL 查询中。此外，我们还避免了可能内存效率低下的 pandas 转换。
- en: Think of process_data as an infinitely versatile user-defined function (UDF)
    that many query engines provide as an escape hatch to work around the limitations
    of SQL. Depending on the computational cost of process_data, the number of shards
    can be increased to speed up processing.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 将 process_data 视为一个无限通用的用户定义函数（UDF），许多查询引擎提供它作为绕过 SQL 限制的逃生门。根据 process_data
    的计算成本，可以增加分片数量以加快处理速度。
- en: We avoid storing the full dataset as an artifact, which is often inconveniently
    slow. Instead, we extract only the data we need—in this case, latitudes and longitudes—as
    space-efficient NumPy arrays and store them. Merging and manipulating NumPy arrays
    is fast.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们避免将完整的数据集作为工件存储，因为这通常会导致不便的缓慢速度。相反，我们只提取所需的数据——在本例中为纬度和经度——作为空间高效的 NumPy 数组并存储它们。合并和操作
    NumPy 数组是快速的。
- en: 'This flow demonstrates that it is possible to handle even large datasets in
    Python using off-the-shelf libraries and a scalable compute layer. A major benefit
    of this approach is operational simplicity: you can leverage existing data infrastructure
    for the initial heavy lifting and SQL queries, and the rest can be handled by
    the data scientist autonomously in Python.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 此流程表明，使用现成的库和可扩展的计算层，在 Python 中处理大型数据集是可能的。这种方法的主要好处是操作简单：您可以使用现有的数据基础设施进行初始的重型工作以及
    SQL 查询，其余部分可以由数据科学家在 Python 中自主处理。
- en: 'In the next section, we add the last step in the data path: feeding data into
    models. We will leverage the pattern discussed in this section to perform feature
    transformations efficiently.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们添加数据路径中的最后一步：将数据输入到模型中。我们将利用本节中讨论的模式来有效地执行特征转换。
- en: 'An alternative approach: Dask or PySpark'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法：Dask 或 PySpark
- en: As an alternative approach for distributed data processing, you could use a
    specialized compute layer like Dask ([dask.org](https://dask.org/)), which provides
    a higher-level interface for performing similar operations in Python. A benefit
    of Dask (or PySpark) is that the data scientist can operate on dataframe-like
    objects, which are automatically sharded and parallelized under the hood.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 作为分布式数据处理的一种替代方法，您可以使用专门的计算层，如 Dask ([dask.org](https://dask.org/))，它为在 Python
    中执行类似操作提供了一个高级接口。Dask（或 PySpark）的一个好处是数据科学家可以操作类似 dataframe 的对象，这些对象在底层自动分片和并行化。
- en: A downside is the introduction of another, operationally nontrivial compute
    layer in your infrastructure stack. When a system like Dask or Spark works, it
    can boost productivity massively. When it doesn’t work, either because of engineering
    issues or due to incompatibility with libraries that a data scientist wants to
    use, it can become a hard-to-debug headache.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 一个缺点是在您的基础设施堆栈中引入了另一个操作上非平凡的计算层。当像 Dask 或 Spark 这样的系统运行良好时，它可以大幅提高生产力。当它不起作用时，无论是由于工程问题还是由于与数据科学家想要使用的库不兼容，它可能会成为一个难以调试的头痛问题。
- en: If you have a Dask cluster already available, you can easily offload data processing
    on it simply by calling it from your Metaflow steps. Equipped with this information
    about various approaches, you can make the right choice for your organization.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经有一个可用的 Dask 集群，您可以通过在 Metaflow 步骤中调用它来轻松地将数据处理任务卸载到该集群上。配备了关于各种方法的信息，您可以为您的组织做出正确的选择。
- en: 7.3 From data to features
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 从数据到特征
- en: '*Equipped with a powerful way to load data from the data warehouse to workflows,
    Alex can start getting more organized about how raw data is converted into matrices
    and tensors consumed by models. Alex would like to load various subsets of data
    from the data warehouse rapidly and define a set of custom Python functions—feature
    encoders—that convert the raw data to model inputs. The exact shape and size of
    the input matrices depends on the use case, so the system should be flexible enough
    to handle a wide variety of needs.*'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '*拥有一种从数据仓库到工作流加载数据的强大方式，亚历克斯可以开始更系统地考虑如何将原始数据转换为模型所消耗的矩阵和张量。亚历克斯希望快速从数据仓库中加载各种数据子集，并定义一组自定义Python函数——特征编码器，将原始数据转换为模型输入。输入矩阵的确切形状和大小取决于用例，因此系统应该足够灵活，能够处理各种需求。*'
- en: '![CH07_F18_UN03_Tuulos](../../OEBPS/Images/CH07_F18_UN03_Tuulos.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F18_UN03_Tuulos](../../OEBPS/Images/CH07_F18_UN03_Tuulos.png)'
- en: 'Thus far, we have discussed the low levels of data processing: how to access
    and process raw data efficiently. The discussion hasn’t had anything to do with
    data science or machine learning specifically. Arguably, data science often involves
    generic data processing, so the focus is justified. This is also why we place
    *data* as the most foundational layer in our infrastructure stack.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了数据处理的基础层次：如何高效地访问和处理原始数据。这次讨论并没有特别涉及数据科学或机器学习。可以说，数据科学通常涉及通用的数据处理，因此这种关注是有道理的。这也是为什么我们将*数据*作为我们基础设施堆栈中最基础的层次。
- en: In this section, we scratch the surface of the multifaceted question of how
    to think about the interface between data and models. This is a much higher-level
    concern with fewer universally applicable answers. For instance, a feature engineering
    approach that works for tabular data doesn’t necessarily work for audio or time
    series.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了如何思考数据与模型之间接口的多方面问题。这是一个更高层次的问题，具有更少的普遍适用答案。例如，适用于表格数据的特征工程方法不一定适用于音频或时间序列。
- en: A major part of the data scientist’s domain expertise relates to feature engineering,
    so it is beneficial to let them experiment and adopt various approaches relatively
    freely. In this sense, feature engineering is a different kind of a concern than,
    say, the foundational topics of data discussed earlier in this chapter or the
    compute layer covered in chapter 4\. The distinction is illustrated in figure
    7.19, which was first presented in chapter 1.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家领域专业知识的一个重要部分与特征工程相关，因此让他们相对自由地实验和采用各种方法是有益的。在这方面，特征工程与本章前面讨论的基础数据主题或第四章中涵盖的计算层不同。这种区别在图7.19中得到了说明，该图最初在第一章中首次提出。
- en: '![CH07_F19_Tuulos](../../OEBPS/Images/CH07_F19_Tuulos.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F19_Tuulos](../../OEBPS/Images/CH07_F19_Tuulos.png)'
- en: Figure 7.19 How much the data scientist cares about various layers of the stack
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.19 数据科学家对堆栈各层关注的程度
- en: Instead of prescribing a one-size-fits-all solution to feature engineering,
    each domain can benefit from domain-specific libraries and services.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是规定一个适合所有情况的特征工程解决方案，每个领域都可以从特定领域的库和服务中受益。
- en: Recommendation It is often a good idea to build, adopt, or buy domain-specific
    solutions for the feature layer, which can be tailored for the use case at hand.
    These solutions can sit on top of the foundational infrastructure, so they are
    complementary, not competing, with the rest of the stack.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 建议：构建、采用或购买针对特征层的特定领域解决方案通常是一个好主意，这些解决方案可以根据手头的用例进行定制。这些解决方案可以建立在基础基础设施之上，因此它们是补充性的，而不是与其他堆栈部分竞争。
- en: We begin by defining the amorphous concept of *a feature*. Then we will provide
    a basic example that takes raw data, converts it into features, and feeds it into
    a model. Later, in chapter 9, we will expand the example to make it ever more
    featureful. This should give you a solid starting point in case you want to dig
    deeper, for instance, by reading a more modeling-oriented book that covers feature
    engineering in detail, and applying the lessons to your own infrastructure stack.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义了模糊的概念*特征*。然后我们将提供一个基本示例，该示例将原始数据转换为特征，并将其输入到模型中。在第九章中，我们将扩展这个示例，使其具有更多的特征。这应该为你提供一个坚实的起点，以防你想要深入研究，例如，通过阅读一本更侧重于建模的书籍，详细介绍了特征工程，并将这些经验应用到你的基础设施堆栈中。
- en: 7.3.1 Distinguishing facts and features
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 区分事实和特征
- en: It is useful to distinguish between *facts* and *features**.* Besides providing
    conceptual clarity, this practice helps in dividing work between data engineers
    and data scientists. Let’s use figure 7.20 to structure the discussion.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 区分*事实*和*特征*是有用的。除了提供概念上的清晰度外，这种做法还有助于在数据工程师和数据科学家之间分配工作。让我们使用图7.20来组织讨论。
- en: The figure is framed on a philosophical assumption that everything exists in
    an objective physical *reality* that we can partially observe to collect *facts*.
    Due to their observational nature, we can expect facts to be biased, inaccurate,
    or even incorrect occasionally. However, crucially, facts are as close to the
    objective reality as we can get.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 该图基于一个哲学假设，即一切存在于一个客观的物理*现实*中，我们可以部分观察以收集*事实*。由于它们的观察性质，我们预计事实可能会存在偏见、不准确，甚至偶尔错误。然而，关键的是，事实是我们能够接近客观现实的最接近的方式。
- en: '![CH07_F20_Tuulos](../../OEBPS/Images/CH07_F20_Tuulos.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F20_Tuulos](../../OEBPS/Images/CH07_F20_Tuulos.png)'
- en: Figure 7.20 The nature of facts and features
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.20 事实与特征的本质
- en: 'From the engineering point of view, facts can be events collected from the
    product or data acquired from a third party. There shouldn’t be much interpretation
    or ambiguity in them: for instance, someone clicking Play to watch *The Lion King*
    is a direct observation—a fact. Later, we may decide to interpret the Play event
    as a signal that the user prefers to watch this type of content, which might be
    a useful label—a *feature*—for a recommendation model. Countless other alternative
    interpretations exist for the Play event (maybe they fat-fingered a large banner
    on their mobile device) and, hence, many opportunities for feature engineering,
    although the underlying fact is unambiguous. The distinction between facts and
    features has important practical implications, some of which are outlined in table
    7.1.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 从工程角度来看，事实可以是来自产品的事件或从第三方获取的数据。其中不应有太多的解释或歧义：例如，有人点击播放来观看《狮子王》是一个直接的观察——事实。后来，我们可能会决定将播放事件解释为用户更喜欢观看此类内容的信号，这可能是推荐模型的一个有用的标签——一个*特征*。对于播放事件存在无数其他可能的解释（也许他们在移动设备上误点了大型横幅）和，因此，许多特征工程的机会，尽管基本事实是明确的。事实与特征之间的区别具有重要的实际意义，其中一些在表7.1中概述。
- en: Table 7.1 Comparing and contrasting facts vs. features
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1 比较和对比事实与特征
- en: '|  | Features | Facts | Reality |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '|  | 特征 | 事实 | 现实 |'
- en: '| **Role** | Data scientist | Data engineer |  |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| **角色** | 数据科学家 | 数据工程师 |  |'
- en: '| **Key activity** | Define new features and challenge existing ones | Collect
    and persist reliable observations |  |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| **关键活动** | 定义新特征并挑战现有特征 | 收集并持久化可靠的观察 |  |'
- en: '| **Speed of iteration** | Fast—coming up with new interpretations is easy
    | Slow—starting to collect new data takes a lot of effort |  |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| **迭代速度** | 快速——提出新的解释很容易 | 慢——开始收集新数据需要大量努力 |  |'
- en: '| **Can we control it?** | Fully—we know and control all the inputs and outputs
    | Partially—we don’t control the input, the reality that behaves in unpredictable
    ways | No, but we can make small interventions, such as A/B experiments |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| **我们能控制它吗？** | 完全可以——我们知道并控制所有输入和输出 | 部分可以——我们无法控制输入，现实以不可预测的方式行为 | 不可以，但我们可以进行小的干预，例如A/B测试
    |'
- en: '| **Trustworthiness** | Varies, low by default | Aims to be high | The objective
    truth |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| **可信度** | 变化，默认情况下较低 | 旨在很高 | 客观的真相 |'
- en: As highlighted by the table, it is useful to have a role, often a data engineer,
    who is responsible for collecting and storing facts reliably. This is a complex
    undertaking by itself, because they need to interface directly with the constantly
    changing outside reality. After a reliable set of facts is available, another
    person, a data scientist, can take the facts, interpret and transform them into
    features, test them in a model, and iterate again with a new set of features to
    improve the model.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 如表格所示，拥有一个角色，通常是一个数据工程师，负责可靠地收集和存储事实，这是非常有用的。这项任务本身就很复杂，因为他们需要直接与不断变化的现实世界进行交互。一旦获得了一组可靠的事实，另一个人，即数据科学家，就可以使用这些事实，进行解释和转换，将它们转化为特征，在模型中进行测试，并使用一组新的特征进行迭代，以改进模型。
- en: 'The key activities between the two roles are distinct. Optimally, the data
    scientist can iterate quickly because they have an infinite number of possible
    features to test. The more accurate and comprehensive the facts are, the better
    the models, which motivates data engineers to collect a wide variety of high-quality
    data. We can project these activities to the following patterns covered in this
    chapter:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 两个角色之间的关键活动是不同的。理想情况下，数据科学家可以快速迭代，因为他们有无限多的可能特征进行测试。事实越准确、越全面，模型就越好，这激励数据工程师收集大量高质量的数据。我们可以将这些活动投影到本章中涵盖的以下模式：
- en: Data engineers maintain reliable *fact tables**,* which are available for all
    projects.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据工程师维护可靠的**事实表**，这些表对所有项目都是可用的。
- en: Data scientists can query the fact tables and extract interesting project-specific
    views of facts for their workflows, for instance, using the CTAS pattern.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学家可以使用CTAS模式查询事实表，并从他们的工作流程中提取有趣的项目特定事实视图，例如。
- en: Data scientists can quickly iterate on features inside their workflow, in Python,
    using the MapReduce-style pattern we covered in the previous section.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学家可以使用我们在上一节中介绍过的MapReduce风格的模式，在Python中快速迭代他们工作流程内的特征。
- en: In some cases, it can be useful to leverage off-the-shelf libraries and services
    like *feature stores* or *data labeling services* to aid with the last two steps,
    or you can create a custom library that answers to the specific needs of your
    company. In any case, it is prudent to start by considering a simple baseline
    solution that doesn’t involve specialized tooling.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，利用现成的库和服务，如**特征存储**或**数据标注服务**来帮助完成最后两个步骤可能是有用的，或者您可以创建一个满足您公司特定需求的自定义库。在任何情况下，考虑一个不涉及专用工具的简单基线解决方案都是谨慎的。
- en: You can either use the baseline approach presented in the next section as-is,
    or you can use it as a foundation for your own domain-specific libraries. No matter
    what approach you take, it should be possible for data scientists to access facts
    easily and iterate on features quickly. It is highly beneficial to use a system
    like Metaflow to take care of versioning while doing this—otherwise, it is easy
    to lose track of what data and features yielded the best results.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择直接使用下一节中介绍的基线方法，或者将其作为您自己领域特定库的基础。无论您采取哪种方法，数据科学家都应该能够轻松访问事实并快速迭代特征。使用Metaflow这样的系统来处理版本控制在这个过程中非常有好处——否则，很容易失去对哪些数据和特征产生了最佳结果的追踪。
- en: 7.3.2 Encoding features
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 特征编码
- en: The process that converts facts to features is called *feature encoding*, or
    *featurization*. A model takes a set of features, produced by sometimes tens or
    even hundreds of individual feature encoding functions, or *feature encoders*.
    Although you could interleave your featurization code with modeling code, especially
    in larger projects, it is very useful to have a consistent way of defining and
    executing feature encoders as a separate step.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 将事实转换为特征的过程称为**特征编码**或**特征化**。模型接受一组特征，这些特征有时由数十甚至数百个单独的特征编码函数或**特征编码器**产生。虽然您可以将特征化代码与建模代码交织在一起，尤其是在较大的项目中，但将特征编码作为单独步骤进行定义和执行的一致方式非常有用。
- en: Besides helping to make the overall architecture manageable, it is crucial that
    the same feature encoders are used both during training and inference to guarantee
    correctness of results. This requirement is often called *offline-online consistency*,
    where *offline* refers to periodic training of models as a batch process and *online*
    refers to on-demand predictions.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 除了帮助使整体架构可管理之外，在训练和推理过程中使用相同的特征编码器来保证结果正确性是至关重要的。这一要求通常被称为**离线-在线一致性**，其中**离线**指的是以批量过程定期训练模型，而**在线**指的是按需预测。
- en: Another central requirement to feature pipelines is the management of *accurate
    train and test splits*. In many cases, like in our weather forecasting example
    in the previous chapter, historical data is used to predict the future. Models
    like this can be *backtested* by taking a past point in time as a reference and
    treating data prior to it as history, which can be used for training, and data
    after it as a pretend future, which can be used for testing. To guarantee the
    validity of results, the training data must not contain any information past the
    reference point, often called *leakage*, which would count as obtaining information
    from the future.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特征管道的另一个核心要求是管理*准确的训练和测试分割*。在许多情况下，例如在前一章中我们的天气预报示例中，历史数据用于预测未来。这样的模型可以通过将过去某个时间点作为参考，将此之前的作为历史数据用于训练，将此之后的作为模拟未来用于测试来进行*回测*。为了保证结果的可靠性，训练数据必须不包含任何超过参考点的信息，通常称为*泄露*，这会被视为从未来获取信息。
- en: A well-designed feature encoding pipeline can treat time as the primary dimension,
    which makes it easy to conduct backtesting while making sure that the time horizons
    are respected by feature encoders, preventing any kind of leakage. A feature encoding
    pipeline can also help to monitor *concept drift*, that is, the statistics of
    the target variable of the model changing over time. Some *feature stores*, which
    help with all these concerns, also present a UI that allows facts and features
    to be shared and discovered easily.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 一个设计良好的特征编码管道可以将时间视为主要维度，这使得在确保特征编码器尊重时间范围的同时进行回测变得容易，从而防止任何类型的泄露。特征编码管道还可以帮助监控*概念漂移*，即模型的目标变量统计随时间变化。一些*特征存储*，这些存储帮助解决所有这些问题，还提供了一个用户界面，允许轻松共享和发现事实和特征。
- en: There isn’t a single right or universal way to implement these concerns. Time
    is handled differently in different datasets, and maintaining online-offline consistency
    can be done in many different ways, depending on the use case—not all applications
    need “online” predictions. A tightly coupled data science team working on a project
    together can quickly communicate and iterate on feature encoders without a heavyweight
    technical solution.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 实施这些关注点并没有唯一正确或普遍适用的方法。在不同数据集中，时间的处理方式不同，而保持在线和离线的一致性可以通过许多不同的方式来实现，这取决于具体的应用场景——并非所有应用程序都需要“在线”预测。一个紧密耦合的数据科学团队在共同开展项目时，可以快速沟通并迭代特征编码器，而无需复杂的解决方案。
- en: 'A key contention is about flexibility and speed of iteration versus guaranteed
    correctness: you can devise (or acquire) a featurization solution that solves
    all the previous concerns, guaranteeing correctness, but makes it hard to define
    new features. For a sensitive, mature project, this might be the right tradeoff
    to make. On the other hand, imposing an overly rigid solution on a new project
    might make it hard to develop it rapidly, limiting its usefulness overall—the
    project may be 100% correct but also quite useless. A good compromise might be
    to start with a flexible approach and make it more rigid as the project matures.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键争议在于灵活性和迭代速度与保证正确性之间的权衡：你可以设计（或获取）一个特征化解决方案来解决所有之前的问题，保证正确性，但会使定义新特征变得困难。对于一个敏感、成熟的项目，这可能是一个正确的权衡。另一方面，将过于僵化的解决方案强加于新项目可能会使其难以快速开发，从而限制其整体效用——项目可能100%正确，但也很无用。一个良好的折衷方案可能是从灵活的方法开始，随着项目的成熟逐渐变得更加严格。
- en: 'Next, we will present an extremely simple feature encoding workflow, based
    on the TaxiPlotter workflow from the previous section. The example is on the flexible
    extreme of the spectrum: it doesn’t address any of the previous concerns, but
    it lays the foundation that we will build on in chapter 9 that presents a more
    full-fledged featurization pipeline.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将展示一个非常简单的特征编码工作流程，基于上一节中的TaxiPlotter工作流程。这个例子位于灵活性的极端：它没有解决任何之前的问题，但为我们将在第9章中介绍的更完整的特征化管道奠定了基础。
- en: 'Example: Predicting the cost of a taxi trip'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：预测出租车行程费用
- en: To demonstrate feature encoding—not our modeling skills—we build a simple predictor
    to estimate the cost of a taxi trip. We know that the price is directly correlated
    with the length of the trip, both in terms of time and space. To keep things simple,
    this example focuses only on the distance. We use simple linear regression to
    predict the amount paid given the distance traveled. We will make the example
    more interesting in chapter 9.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示特征编码——而不是我们的建模技能——我们构建了一个简单的预测器来估算出租车行程的成本。我们知道价格与行程长度直接相关，无论是时间还是空间。为了简化问题，这个例子只关注距离。我们使用简单的线性回归来预测根据行程距离支付的金额。我们将在第9章使这个例子更有趣。
- en: 'If this were a real task given to a data scientist, they would surely start
    by exploring the data, probably in a notebook. You can do it, too, as an exercise.
    You will find out that, as with any real-life dataset containing empirical observations,
    there is noise in data: a good number of trips have a $0 cost or no distance traveled.
    Also, a small number of outliers have a very high cost or distance traveled.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是一个数据科学家实际接到的任务，他们肯定会首先探索数据，可能是在笔记本中。您也可以作为练习来做这件事。您会发现，与任何包含经验观察的真实生活数据集一样，数据中存在噪声：许多行程的成本为0或没有行程距离。此外，少数异常行程的成本或行程距离非常高。
- en: We start by making an interpretation, starting to turn facts into features,
    by assuming that these outlier trips don’t matter. We use the function filter_outliers
    in the following code listing to get rid of trips that have values in the top
    or bottom 2% of the value distribution. The utility module also contains a function,
    sample, that we can use to uniformly sample rows from the dataset.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先进行解释，开始将事实转化为特征，假设这些异常行程并不重要。我们使用以下代码列表中的filter_outliers函数来去除值分布在最顶部或最底部2%的行程。实用模块还包含一个名为sample的函数，我们可以使用它从数据集中均匀地采样行。
- en: Listing 7.9 Removing outlier rows from an Arrow table
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.9 从Arrow表中移除异常行
- en: '[PRE23]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Accepts a pyarrow.Table and a list of columns to be cleaned
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 接受一个pyarrow.Table和一个要清理的列列表
- en: ❷ Starts with a filter that accepts all rows
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 以接受所有行的过滤器开始
- en: ❸ Processes all columns one by one
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 逐个处理所有列
- en: ❹ Finds the top and bottom 2% of the value distribution
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 找到值分布的最顶部和最底部2%
- en: ❺ Includes only rows that fall between 2-98% of the value distribution
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 仅包括位于值分布2-98%之间的行
- en: ❻ Returns a subset of the rows that match the filter
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 返回与过滤器匹配的行子集
- en: ❼ Samples a random p% of rows of the given table
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 从给定的表中采样随机p%的行
- en: ❽ Flips a biased coin on each row and returns the matching rows
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 在每一行上抛一个有偏见的硬币，并返回匹配的行
- en: Save the code in table_utils.py. It is worth noting that both the functions
    in listing 7.9, filter_outlier and sample, could be implemented in SQL as well.
    You could bake these operations in the SQL behind the CTAS query. Consider the
    following benefits of performing these operations in Python instead. First, expressing
    filter_outliers in SQL is somewhat nontrivial, especially when it is done over
    multiple columns. The resulting SQL would likely take more (convoluted) lines
    of code than the Python implementation.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存在table_utils.py中。值得注意的是，列表7.9中的filter_outlier和sample函数也可以在SQL中实现。您可以在CTAS查询背后的SQL中实现这些操作。考虑一下在Python中执行这些操作的好处。首先，在SQL中表达filter_outliers有些复杂，尤其是在多列上执行时。生成的SQL可能比Python实现需要更多的（复杂的）代码行。
- en: 'Second, we are making major assumptions here: is 2% the right number? Should
    it be the same for all columns? A data scientist may want to iterate on these
    choices. We can iterate and test the code in Python much faster than what it takes
    to execute a complex SQL query.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们在这里做出了重大假设：2%是正确的数字吗？它应该对所有列都相同吗？数据科学家可能希望对这些选择进行迭代。我们可以在Python中迭代和测试代码，速度比执行复杂的SQL查询快得多。
- en: Also note that both the functions are implemented without a conversion to pandas,
    which guarantees that the operations are both time- and space-efficient because
    they rely only on Apache Arrow and NumPy, both of which are backed by high-performance
    C and C++ code. It is quite likely that these operations are more performant in
    Python than they would be on any query engine. Listing 7.10 defines the functions
    that build a linear regression model and visualize it.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，这两个函数都是在不转换为pandas的情况下实现的，这保证了操作既节省时间又节省空间，因为它们只依赖于Apache Arrow和NumPy，这两者都由高性能的C和C++代码支持。这些操作在Python中的性能很可能比在任何查询引擎中都要好。列表7.10定义了构建线性回归模型并可视化的函数。
- en: Listing 7.10 Training and visualizing a regression model
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.10 训练和可视化回归模型
- en: '[PRE24]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Accepts features as a dictionary of NumPy arrays
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 接受作为NumPy数组字典的特征
- en: ❷ Builds a linear regression model using Scikit-Learn
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用Scikit-Learn构建线性回归模型
- en: ❸ Visualizes the model
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 可视化模型
- en: ❹ Plots a regression line
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 绘制回归线
- en: ❺ Saves the image as an artifact
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将图像保存为工件
- en: Save the code to taxi_model.py. The fit function builds a simple linear regression
    model using Scikit-Learn. For details, you can see the documentation for Scikit-Learn
    ([http://mng.bz/Wxew](http://mng.bz/Wxew)). The visualize function plots the features
    on a scatterplot and overlays the regression line on top of it.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到taxi_model.py。fit函数使用Scikit-Learn构建一个简单的线性回归模型。有关详细信息，请参阅Scikit-Learn的文档（[http://mng.bz/Wxew](http://mng.bz/Wxew)）。visualize函数在散点图上绘制特征，并在其上叠加回归线。
- en: 'The next code snippet shows the actual workflow, TaxiRegressionFlow, which
    is closely based on TaxiPlotterFlow from the previous section. It has the same
    two modes: you can either use preprocessed data from a CTAS query produced by
    TaxiETLFlow, or you can use the raw mode that accesses two months of unfiltered
    data.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段显示了实际的流程，TaxiRegressionFlow，它紧密基于上一节中的TaxiPlotterFlow。它具有相同的两种模式：你可以使用由TaxiETLFlow生成的CTAS查询的预处理数据，或者你可以使用原始模式，该模式访问两个月未过滤的数据。
- en: Listing 7.11 From facts to features to a model
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.11 从事实到特征再到模型
- en: '[PRE25]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Uses two months of data in the raw mode
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用原始模式中的两个月数据
- en: ❷ Samples the given percentage of data, 0.0-1.0.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 样本给定百分比的数据，0.0-1.0。
- en: ❸ Chooses between raw or CTAS mode, similar to TaxiPlotterFlow
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在原始或CTAS模式之间进行选择，类似于TaxiPlotterFlow
- en: ❹ Cleans and samples each shard independently
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 独立地清理和样本每个碎片
- en: ❺ Extracts clean columns as features
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 提取干净的列作为特征
- en: ❻ Merges feature shards by concatenating the arrays
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 通过连接数组合并特征碎片
- en: ❼ Fits a model
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 拟合一个模型
- en: ❽ Visualizes the model and stores the image as an artifact
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 可视化模型并将图像保存为工件
- en: 'Save the code in taxi_regression.py. If you ran TaxiETLFlow previously, you
    can run the flow like this:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存在taxi_regression.py中。如果你之前运行了TaxiETLFlow，你可以这样运行流程：
- en: '[PRE26]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Otherwise, leave the option out to use the raw data directly. It takes less
    than a minute to process a 10% sample of data on a large instance and about two
    minutes to process the full dataset without sampling. Figure 7.21 visualizes the
    regression line produced by the model as well as a scatterplot of raw data, as
    shown in a Jupyter notebook.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，省略该选项以直接使用原始数据。在大实例上处理10%的数据样本不到一分钟，处理完整数据集（不采样）大约需要两分钟。图7.21可视化了模型生成的回归线以及原始数据的散点图，如图所示在Jupyter笔记本中。
- en: '![CH07_F21_Tuulos](../../OEBPS/Images/CH07_F21_Tuulos.png)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F21_Tuulos](../../OEBPS/Images/CH07_F21_Tuulos.png)'
- en: Figure 7.21 Regression between distance and the cost of a taxi trip, overlaid
    on data
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.21 在数据上叠加距离与出租车行程成本的回归，如图7.21所示
- en: 'This example expands the previous TaxiPlotterFlow example by demonstrating
    the following:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例扩展了之前的TaxiPlotterFlow示例，展示了以下内容：
- en: The data scientist can use the MapReduce-style pattern to encode features.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学家可以使用MapReduce风格的模式来编码特征。
- en: We can combine the best of both worlds by using SQL for data extraction and
    Python for feature engineering.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过使用SQL进行数据提取和Python进行特征工程来结合两者的优点。
- en: It is possible to perform feature engineering in a performance-conscious manner
    in Python, thanks to libraries like Apache Arrow and NumPy, backed by C and C++
    implementations.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于有Apache Arrow和NumPy等库的支持，这些库由C和C++实现，因此可以在Python中以性能意识的方式执行特征工程。
- en: In chapter 9, we will expand the example to include proper testing of the model,
    more sophisticated regression models using deep learning, and an extensible feature
    encoding pipeline.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在第9章中，我们将扩展示例以包括对模型的适当测试，使用深度学习的更复杂的回归模型，以及可扩展的特征编码管道。
- en: This chapter covered a lot of ground, all the way from foundational data access
    patterns to feature engineering pipelines and model training, as summarized in
    figure 7.22 .
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了大量的内容，从基础的数据访问模式到特征工程管道和模型训练，如图7.22所示进行了总结。
- en: '![CH07_F22_Tuulos](../../OEBPS/Images/CH07_F22_Tuulos.png)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F22_Tuulos](../../OEBPS/Images/CH07_F22_Tuulos.png)'
- en: Figure 7.22 Summarizing the concepts covered in this chapter
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.22 总结本章涵盖的概念
- en: We started with the fundamentals of moving data quickly from S3 to instances.
    We discussed efficient in-memory representation of data with Apache Arrow. After
    this, we showed how these techniques can be used to interface with query engines
    like Spark, Snowflake, Trino, or Amazon Athena, which are a core part of the modern
    data infrastructure.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从快速将数据从S3移动到实例的基本原理开始。我们讨论了使用Apache Arrow的高效内存数据表示。之后，我们展示了如何使用这些技术来与Spark、Snowflake、Trino或Amazon
    Athena等查询引擎接口，它们是现代数据基础设施的核心部分。
- en: We created a workflow that uses a query engine to process a dataset by executing
    a Create-Table-As-Select SQL query, the results of which can be downloaded quickly
    to a downstream workflow. Finally, we used this functionality to create a feature
    encoding pipeline that trains a model.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个工作流程，使用查询引擎通过执行Create-Table-As-Select SQL查询来处理数据集，其结果可以快速下载到下游工作流程。最后，我们利用这一功能创建了一个特征编码管道来训练模型。
- en: Combined with the lessons from the previous chapter, these tools allow you to
    build production-grade data science applications that ingest large amounts of
    data from a data warehouse, encode features in parallel, and train models at scale.
    For more computationally demanding feature encoders, you can leverage lessons
    from chapter 5 to optimize them, if necessary.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 结合前一章的教训，这些工具允许您构建生产级的数据科学应用程序，这些应用程序可以从数据仓库中摄取大量数据，并行编码特征，并大规模训练模型。对于计算密集型的特征编码器，如果需要，您可以利用第5章的教训来优化它们。
- en: Summary
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Optimize downloading speed between S3 and EC2 instances by ensuring that data
    fits in memory and files are large enough and by using a large instance type.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过确保数据适合内存、文件足够大以及使用大型实例类型来优化S3和EC2实例之间的下载速度。
- en: Use Parquet as an efficient format for storing tabular data and Apache Arrow
    to read and process it in memory.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Parquet作为存储表格数据的有效格式，并使用Apache Arrow在内存中读取和处理它。
- en: If memory consumption is a concern, avoid converting data to pandas. Instead,
    operate with Arrow and NumPy data structures.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果内存消耗是一个问题，避免将数据转换为pandas。相反，使用Arrow和NumPy数据结构进行操作。
- en: Leverage existing data infrastructure to extract and preprocess data for data
    science workflows and to connect them to ETL workflows.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用现有的数据基础设施从数据科学工作流程中提取和预处理数据，并将它们连接到ETL工作流程。
- en: Use modern query engines like Spark, Trino, Snowflake, or Athena to execute
    SQL queries to produce arbitrary data extracts, stored in Parquet, for data science
    workflows.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用现代查询引擎，如Spark、Trino、Snowflake或Athena，执行SQL查询以生成任意数据提取，存储在Parquet中，供数据科学工作流程使用。
- en: Organizationally, data engineers can focus on producing high-quality, reli-
    able facts, whereas data scientists can iterate on project-specific datasets autonomously.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在组织上，数据工程师可以专注于生产高质量、可靠的事实，而数据科学家可以自主迭代项目特定的数据集。
- en: Use the MapReduce pattern to process large datasets in Python in parallel. Libraries
    like Arrow and NumPy are backed by high-performance C/C++ code, making it possible
    to process data quickly.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MapReduce模式在Python中并行处理大型数据集。像Arrow和NumPy这样的库由高性能的C/C++代码支持，使得快速处理数据成为可能。
- en: Leverage the foundational tools and patterns to build a solution that works
    for your particular use cases—feature engineering and feature pipelines tend to
    be quite domain-specific.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用基础工具和模式构建一个适用于您特定用例的解决方案——特征工程和特征管道往往具有相当强的领域特定性。
- en: When designing a feature pipeline, consider using time as the primary dimension
    to make it easy to backtest with historical data and to prevent leakage.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在设计特征管道时，考虑使用时间作为主要维度，以便于使用历史数据进行回测，并防止信息泄露。
- en: In feature engineering pipelines, ensure that data scientists can access facts
    easily and iterate on features quickly.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在特征工程管道中，确保数据科学家可以轻松访问事实并快速迭代特征。
