- en: Chapter 4\. Automating Data Quality Monitoring with Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章。使用机器学习自动化数据质量监控
- en: 'Machine learning is a statistical approach that, compared to rule-based testing
    and metrics monitoring, has many advantages: it’s scalable, can detect unknown-unknown
    changes, and, at the risk of anthropomorphizing, it’s smart. It can learn from
    prior inputs, use contextual information to minimize false positives, and actually
    understand your data better and better over time.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一种统计方法，与基于规则的测试和指标监控相比，具有许多优势：可扩展性强，可以检测未知的变化，而且，在将风险人性化的前提下，它还很聪明。它可以从先前的输入中学习，使用上下文信息来减少误报，实际上可以越来越好地理解您的数据。
- en: 'In the previous chapters, we’ve explored when and how automation with ML makes
    sense for your data quality monitoring strategy. Now it’s time to explore the
    core mechanism: how you can train, develop, and use a model to detect data quality
    issues—and even explain aspects like their severity and where they occur in your
    data.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们探讨了自动化ML何时以及如何对您的数据质量监控策略有意义。现在是探讨核心机制的时候：您如何训练、开发和使用模型来检测数据质量问题，甚至解释其严重性以及发生在数据中的位置。
- en: In this chapter, we’ll explain which machine learning approach works best for
    data quality monitoring and show you the algorithm (series of steps) you can follow
    to implement this approach. We’ll answer questions like how much data you should
    sample, and how to make the model’s outputs explainable. It’s important to caveat
    that following the steps here won’t result in a model that’s ready to monitor
    real-world data. In [Chapter 5](ch05.html#building_a_model_that_works_on_real_wor),
    we’ll turn to the practical aspects of tuning and testing your system so that
    it functions reliably in an enterprise setting.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将解释哪种机器学习方法最适合数据质量监控，并展示您可以遵循的算法（一系列步骤）来实施这种方法。我们将回答诸如应该采样多少数据以及如何使模型的输出可解释等问题。需要注意的是，遵循这里的步骤不会导致一个准备好监控实际数据的模型。在[第五章](ch05.html#building_a_model_that_works_on_real_wor)中，我们将转向调整和测试系统的实际方面，以使其在企业环境中可靠运行。
- en: Requirements
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要求
- en: 'There are many ML techniques you could potentially apply to a given problem.
    To figure out the right approach for your use case, it’s essential to define the
    requirements upfront. We believe a model for data quality monitoring should have
    four characteristics: sensitivity, specificity, transparency, and scalability.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定问题，有许多ML技术可以潜在应用。要找出适合您使用案例的正确方法，需要在前期定义要求至关重要。我们认为用于数据质量监控的模型应具备四个特征：灵敏度、特异性、透明性和可扩展性。
- en: Sensitivity
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 灵敏度
- en: Sensitivity is a measure of how well an ML model can detect true positives.
    To be effective, an algorithm should be able to detect a wide variety of data
    quality issues in real-world tabular data. A good benchmark is being able to detect
    changes that affect 1% or more of records.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 灵敏度是衡量ML模型能够检测真正阳性的能力的指标。为了有效，算法应能够在实际表格数据中检测到各种数据质量问题。一个良好的基准是能够检测到影响超过1%记录的变化。
- en: In practice, we find that trying to detect changes that affect less than 1%
    of records produces a system that is simply too noisy. Even if the changes detected
    are statistically significant, there will be too many of them to triage and understand,
    especially when scaled to a large number of complex tables. Our experience has
    suggested that changes that affect 1% or more of records are significant *structural*
    changes in the data generation or transformation processes that could be major
    new shocks and scars.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们发现试图检测影响不到1%记录的变化会导致一个系统产生太多噪音。即使检测到的变化在统计上是显著的，也会有太多这样的变化需要分类和理解，特别是当规模扩展到大量复杂表格时。我们的经验表明，影响超过1%记录的变化是数据生成或转换过程中的重大*结构性*变化，可能是重大的新冲击和创伤。
- en: To find changes smaller than 1%, you can either use deterministic approaches
    (like validation rules), or you can focus the ML on a subset of data by running
    the model on a view that queries just the most important records.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要查找小于1%的变化，您可以使用确定性方法（如验证规则），或者可以通过在仅查询最重要记录的视图上运行模型来聚焦ML在数据子集上。
- en: For example, a social media platform might track hundreds of different types
    of events in a single large event processing table. Running ML on the entirety
    of the table would catch gross issues with the format and structure of the most
    common types of events that are collected. But you could instead run the model
    on each of hundreds of event-specific subsets if you wanted to pay close attention
    to them all.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，社交媒体平台可能在单个大事件处理表中跟踪数百种不同类型的事件。在整个表上运行机器学习将捕捉格式和结构最常见的事件类型的严重问题。但如果您想要密切关注每个事件特定子集，您可以选择在每个事件特定的子集上运行模型。
- en: Specificity
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特异性
- en: The counterpart to sensitivity, specificity tells you how good the model is
    at *not triggering false positive alerts*. This is especially important in data
    quality monitoring, where alert fatigue can threaten the adoption and efficacy
    of the entire approach.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与灵敏度相对应，特异性告诉您模型在*不触发假阳性警报*方面的表现有多好。这在数据质量监控中尤为重要，因为警报疲劳可能会威胁整个方法的采纳和功效。
- en: Typically, a monitoring system tends to over-alert for a few reasons. One reason
    can be seasonality—if there are patterns in the data that repeat daily, weekly,
    or annually, the data might look like it’s changing, but it’s not really changing
    in an unusual or unexpected way. The monitor will also be noisy if it isn’t able
    to cluster correlated columns that are affected by the same data change. Or it
    could send false-positive alerts if it reviews too small a sample of data or evaluates
    the data over too small a time window. Additionally, there are some datasets that
    are much more “chaotic” than other datasets, and so the threshold for how sensitive
    a check should be needs to be calibrated to each dataset (and may need to evolve
    over time).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，监控系统倾向于因为几个原因而过度报警。一个原因可能是季节性——如果数据中有每天、每周或每年重复的模式，数据看起来可能正在变化，但实际上并没有以异常或意外的方式变化。如果监视器不能聚合受同一数据变化影响的相关列，它也会变得嘈杂。或者如果它对数据的样本或时间窗口评估太小，也可能发送假阳性警报。此外，有些数据集比其他数据集更加“混乱”，因此需要校准每个数据集的检查灵敏度阈值（并可能随时间演变）。
- en: We’ll explore how a model can learn and account for seasonality, correlations,
    and other challenges of real-world data in [Chapter 5](ch05.html#building_a_model_that_works_on_real_wor).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨模型如何在[第五章](ch05.html#building_a_model_that_works_on_real_wor)中学习和考虑季节性、相关性和其他现实世界数据的挑战。
- en: Transparency
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 透明度
- en: When issues arise, the model should be transparent and help users understand
    and root cause issues. You might think that this doesn’t have to do with the model
    itself—after all, any fancy visualizations and root-cause analysis will happen
    *after* a data issue is detected. But your options really do depend on the ML
    approach you use. Your model’s architecture and implementation will dictate how
    much you’ll be able to explain and attribute its predictions. For instance, some
    ML features might help improve accuracy but will be difficult to explain to users
    in the context of data quality.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当出现问题时，模型应该是透明的，并帮助用户理解和找出问题的根本原因。你可能会认为这与模型本身无关——毕竟，任何复杂的可视化和问题根本原因分析都会在检测到数据问题之后发生。但你的选择确实取决于你使用的机器学习方法。你模型的架构和实现将决定你能够解释和归因其预测的程度。例如，某些机器学习特性可能有助于提高准确性，但在数据质量的背景下解释给用户可能会很困难。
- en: Scalability
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可扩展性
- en: To run daily on potentially billions of rows in a data warehouse, your system
    must also scale—in human, storage, and computational cost. It should require no
    up-front configuration or retuning by administrators to run, as this would just
    create another form of handwritten rules, which we’ve already shown in [Chapter 2](ch02.html#data_quality_monitoring_strategies_and)
    not to be a scalable solution. It should have a minimal query footprint on the
    data warehouse and be capable of executing quickly on inexpensive hardware outside
    of the warehouse. These constraints will affect many aspects of our modeling decisions,
    and we’ll address ways of making the solution more scalable throughout this chapter.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要在数据仓库中每天运行可能涉及数十亿行数据的任务，您的系统必须具备人力、存储和计算成本的可扩展性。它不应该需要管理员进行任何预先配置或重新调整，因为这只会创造另一种手工编写的规则形式，我们已经在[第二章](ch02.html#data_quality_monitoring_strategies_and)中显示这不是一个可扩展的解决方案。它应该对数据仓库的查询占用尽可能小的资源，并且能够在仓库之外的廉价硬件上快速执行。这些约束条件将影响我们建模决策的许多方面，我们将在本章中探讨使解决方案更具可扩展性的方法。
- en: Nonrequirements
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非要求
- en: Defining what a system does *not* need to do can be just as useful as defining
    what it should do. You may recall from [Chapter 2](ch02.html#data_quality_monitoring_strategies_and)
    that an unsupervised ML model should be one part of a three-pillar data quality
    approach that also includes rule-based testing and metrics monitoring. That’s
    because it’s simply infeasible to expect automation to solve every data quality
    problem.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 定义系统*不*需要做的事情与定义应该做的事情一样有用。你可能还记得[第2章](ch02.html#data_quality_monitoring_strategies_and)中讨论的一个无监督机器学习模型应该是三支柱数据质量方法的一部分，另外还包括基于规则的测试和指标监控。这是因为期望自动化解决所有数据质量问题是不可行的。
- en: 'Here’s a list of nonrequirements for our model:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们模型的非要求清单：
- en: 'It doesn’t need to identify individual records that are bad (that’s what rule-based
    testing is for: when you need the data to be perfect). Instead, we expect it to
    look for structural changes in meaningful percentages of records.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不需要识别个别的坏记录（这是基于规则的测试的作用：当需要数据完美时）。相反，我们期望它寻找有意义百分比记录中的结构性变化。
- en: It’s not required to process data in real time. Not only would real-time evaluation
    of an ML model for data quality detection be difficult to scale, but it could
    also be forced to evaluate individual records, which is not within our scope.
    Instead, we expect it to evaluate data in daily or hourly batches.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并非需要实时处理数据。实时评估数据质量检测的机器学习模型不仅难以扩展，而且可能被迫评估单个记录，这超出了我们的范围。相反，我们期望它能够以每日或每小时批次评估数据。
- en: We can’t expect it to be able to tell if data was always corrupted —that’s not
    how ML works, as the model must be trained on historical data. If that historical
    data is wrong, there’s nothing we can do about it! That’s why an ML approach should
    only be relied on to identify *new* changes in the data.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不能期望它能够告诉我们数据一直都是损坏的 — 这不是机器学习的工作方式，因为模型必须基于历史数据训练。如果历史数据是错误的，我们无能为力！这就是为什么机器学习方法仅应依赖于识别数据中的*新*变化。
- en: We can’t expect it to analyze data without some notion of time. The model will
    be tracking data over time to detect changes. If there’s no timestamp built into
    the data itself, we’ll need to develop other ways of identifying when the data
    was generated (more on this later).
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不能指望它在没有时间概念的情况下分析数据。模型将会随时间追踪数据以检测变化。如果数据本身没有时间戳，我们将需要开发其他方法来识别数据生成的时间（稍后详述）。
- en: Data Quality Monitoring Is Not Outlier Detection
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据质量监控不等同于异常检测。
- en: 'As we wrap up our discussion of requirements for a data quality monitoring
    model, it’s worth taking a moment to address a common confusion: the difference
    between outlier detection and data quality monitoring.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束对数据质量监控模型要求的讨论时，值得花一点时间解决一个常见的困惑：异常检测与数据质量监控的区别。
- en: Outlier detection can be a useful way of understanding complex datasets. There
    are many ways to identify outliers, but one of the most scalable and flexible
    approaches is to use a variant of [random forest](https://oreil.ly/oAZd2) called
    [Isolation Forest](https://oreil.ly/teNNd) to identify rows of data that are far
    from the “center” of a multivariate distribution, as shown in [Figure 4-1](#an_example_of_using_isolation_forest_in).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测可以是理解复杂数据集的一种有用方式。有许多方法可以识别异常值，但其中一种最可扩展和灵活的方法是使用一种变体的随机森林，称为[孤立森林](https://oreil.ly/teNNd)，来识别远离多变量分布“中心”的数据行，如图[4-1](#an_example_of_using_isolation_forest_in)所示。
- en: '![An example of using Isolation Forest in scikit-learn to find outlier observations
    (those points outside of the delineated clusters) (scikit-learn at https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_anomaly_comparison.html#sphx-glr-auto-examples-miscellaneous-plot-anomaly-comparison-py)](assets/adqm_0401.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![使用 scikit-learn 中的孤立森林查找异常观测的示例（那些在界定的簇外的点）（scikit-learn 地址 https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_anomaly_comparison.html#sphx-glr-auto-examples-miscellaneous-plot-anomaly-comparison-py）](assets/adqm_0401.png)'
- en: Figure 4-1\. An example of using Isolation Forest in scikit-learn to find outlier
    observations (those points outside of the delineated clusters); from “Comparing
    Anomaly Detection Algorithms for Outlier Detection on Toy Datasets,” [*Scikit-learn*](https://oreil.ly/rhJ9o).
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 使用 scikit-learn 中的孤立森林（Isolation Forest）查找异常观测的示例（那些在界定的簇外的点）；来源于《对玩具数据集上的异常检测算法进行比较》，[*Scikit-learn*](https://oreil.ly/rhJ9o)。
- en: Outlier detection can be accomplished with ML, and it seeks to find out about
    unusual aspects of the data. But the similarities with data quality monitoring
    stop there. After all, every dataset will have unusual observations—even a normal
    distribution has extreme values! These outliers may be interesting (they could
    be fraudulent records or just very rare events or data combinations), but they
    aren’t necessarily going to be data quality issues, which can affect common or
    rare records with equal probability.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过机器学习可以实现异常检测，它旨在发现数据的不寻常方面。但与数据质量监控的相似之处就到此为止。毕竟，每个数据集都会有异常观察结果——即使正态分布也会有极端值！这些异常值可能很有趣（它们可能是欺诈记录或者仅仅是非常罕见的事件或数据组合），但它们未必会是数据质量问题，这可能会影响常见或罕见记录的概率是一样的。
- en: To identify data quality issues, we need to know when there is a sudden structural
    change in the distribution of data arriving into the table. We need to know if,
    in the past, records always appeared with a certain distribution, pattern, or
    relationship, and now, all of a sudden, that has changed in a significant way.
    On the other hand, every dataset has outliers. Outlier detection is solving a
    fundamentally different problem.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要识别数据质量问题，我们需要知道数据到达表中的分布是否出现了突然的结构变化。我们需要知道过去记录是否总是以某种分布、模式或关系出现，而现在突然之间，这种情况以显著的方式发生了变化。另一方面，每个数据集都有异常值。异常检测解决的是一个根本不同的问题。
- en: ML Approach and Algorithm
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习方法和算法
- en: Now that we’ve covered the requirements, we’ll share the approach that we recommend
    and the steps you can follow to implement it. We hesitate to claim that this is
    the only way to use ML to detect data quality issues, but we have yet to encounter
    an approach that more effectively meets the requirements in practice. As always,
    the devil is in the details. Things like feature engineering and parameter tuning/dampening
    make all the difference between an effective implementation and one that over-
    or under-alerts on real-world data, as we’ll discuss further in [Chapter 5](ch05.html#building_a_model_that_works_on_real_wor).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了需求，我们将分享我们推荐的方法以及您可以采用的步骤来实施它。我们不愿声称这是使用机器学习检测数据质量问题的唯一方法，但我们尚未遇到比实践中更有效地满足要求的方法。正如常言道，细节决定成败。诸如特征工程和参数调整/阻尼在实现有效的实施与在真实世界数据上过度或不足报警之间起着至关重要的作用，我们将在第5章进一步讨论。
- en: Recall that we want to develop an ML model to detect unexpected changes in our
    data, without any humans labeling the data and telling us what constitutes a data
    quality issue. This makes this type of ML problem an *unsupervised learning* task.
    However, there does happen to be a feature of the data that we can use as if it
    were a human label, and that’s the *time when the data arrived into the table.*
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 请回忆，我们希望开发一个机器学习模型来检测数据中的意外变化，而无需任何人员标记数据并告诉我们什么构成了数据质量问题。这使得这类机器学习问题成为*无监督学习*任务。然而，数据中确实有一个我们可以像人类标签一样使用的特征，那就是*数据到达表中的时间*。
- en: Herein lies the key insight in this approach. Every day, we take a snapshot
    of the data. Then, every day, we try to train a classifier to *predict whether
    the data is from today or not.*
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是这种方法的关键洞察力所在。每天，我们都会对数据进行快照。然后，每天，我们都试图训练一个分类器来*预测数据是否来自今天*。
- en: '![Image](assets/adqm_04in01.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图像](assets/adqm_04in01.png)'
- en: If there’s nothing statistically remarkable about the data from today, then
    our attempt to train a classifier should fail—predicting whether the data is from
    today or not should be an impossible task, basically a flip of a coin!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果今天的数据在统计上没有什么显著的特点，那么我们尝试训练分类器的努力将会失败——预测数据是否来自今天应该是一个不可能的任务，基本上是扔硬币！
- en: On the other hand, if we *can* build a classifier that predicts with some accuracy
    whether a piece of data came from today, then we can be pretty sure that something
    is unusual about the data from today. And it’s unusual in a meaningful way—because
    a few random changes in a couple of records aren’t going to be enough to train
    a model to make a prediction one way or another. In fact, we’ll even be able to
    use this method to say *how* significant the change is and set appropriate thresholds
    to avoid alert fatigue. By explaining the model’s predictions, we can explain
    what’s most likely going on inside the data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们*能够*构建一个分类器，能够相当准确地预测数据是否来自今天，那么我们可以非常确信今天的数据有些不同寻常。而且这种不寻常是有意义的——因为几条记录中的少量随机变化是不足以训练模型做出预测的。事实上，我们甚至能够使用这种方法来说出变化的*程度*，并设置适当的阈值以避免警报过度疲劳。通过解释模型的预测，我们可以解释数据内部最有可能发生的情况。
- en: A model can detect a change that’s significant even if that change is not interesting.
    The most obvious example of this is when there is a `date` column. That column
    is going to change every day, and so it will always represent a dramatic change
    in the data! We’ll cover how to handle cases like this in [Chapter 5](ch05.html#building_a_model_that_works_on_real_wor).
    The other kind of change that might not be meaningful is one that the end user
    simply doesn’t care about. We’ll talk about how to deal with alerts like these
    in [Chapter 6](ch06.html#implementing_notifications_while_avoidi).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 即使变化并不有趣，模型也能检测到显著的变化。最明显的例子是日期列。该列每天都会变化，因此它总是代表数据的显著变化！我们将在[第五章](ch05.html#building_a_model_that_works_on_real_wor)讨论如何处理这类情况。另一种可能不具有意义的变化是最终用户根本不关心的变化。我们将讨论如何在[第六章](ch06.html#implementing_notifications_while_avoidi)处理这类警报。
- en: 'Now that you have the main idea, let’s explore each step in more detail:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了主要思路，让我们详细探讨每一步：
- en: Data sampling
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据抽样
- en: How do you build a dataset to train your model and what is an appropriate sample
    size?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如何构建用于训练模型的数据集，以及合适的样本大小是多少？
- en: Feature encoding
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 特征编码
- en: How do you go from a row in one of your tables to a set of features that your
    model can use to make predictions?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如何从表中的一行转换为模型可以用来进行预测的特征集？
- en: Model development
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 模型开发
- en: What is the right model architecture for this algorithm and how should you train
    the model?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为这种算法选择正确的模型架构，以及如何训练模型？
- en: Model explainability
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可解释性
- en: Once you’ve trained a model, how do you use it to explain a data quality issue?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你训练好一个模型，如何用它来解释数据质量问题？
- en: Data Sampling
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据抽样
- en: 'The starting point for building any model is to create a training dataset by
    sampling from your overall pool of data. For the algorithm we’ve just described,
    you’ll need a robust set of randomly sampled data from both “today” (label = 1
    for the class we are trying to predict) and “not today” (label = 0). The “not
    today” data should be a mix of prior time comparison periods: yesterday (or the
    last time you got a data update) for sudden changes, as well as other times of
    the week or year to control for seasonality (see the section [“Seasonality”](ch05.html#seasonality)).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 构建任何模型的起点是从总体数据池中抽样创建一个训练数据集。对于我们刚刚描述的算法，您需要从“今天”（标签为1表示我们试图预测的类）和“不是今天”（标签为0）中随机抽取一组健壮的数据。
    “不是今天”的数据应该是之前的时间比较周期的混合物：昨天（或上次获得数据更新的时间）用于突然变化，以及其他周或年的其他时间用于控制季节性（参见章节[“季节性”](ch05.html#seasonality)）。
- en: 'For instance, in [Figure 4-2](#graph_showing_sample_size_compared_to_t) we
    see an example dataset with 150k to 250k rows of data per day. A robust sample
    might include 10,000 rows from each of the following dates:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[图 4-2](#graph_showing_sample_size_compared_to_t)中，我们看到一个示例数据集，每天的数据量在15万到25万行之间。一个健壮的样本可能包括来自以下日期的1万条记录：
- en: '2021-01-16: the date you want to assess for data quality issues'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2021-01-16：您希望评估数据质量问题的日期
- en: '2021-01-15: yesterday, which will help identify any sudden changes'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2021-01-15：昨天，有助于识别任何突然变化
- en: '2021-01-09: one week ago, to control for day-of-week seasonality'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2021-01-09：一周前，为了控制星期几季节性因素
- en: '2021-01-02: two weeks ago, in case last week had anomalies'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2021-01-02：两周前，以防上周出现异常
- en: '![Graph showing sample size compared to the entire size of the dataset](assets/adqm_0402.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![展示样本大小与数据集整体大小的对比图](assets/adqm_0402.png)'
- en: Figure 4-2\. Graph showing sample size compared to the entire size of the dataset.
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. 展示样本大小与数据集整体大小的对比图。
- en: Sample size
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 样本大小
- en: In practice, we find that this algorithm needs at least 100 records per day
    to have a chance of finding meaningful changes in reasonably complex data. But
    that begs the question—what is the upper limit of the number of records that is
    useful for the algorithm?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们发现这种算法每天至少需要 100 条记录才能有机会在相对复杂的数据中找到有意义的变化。但这引出了一个问题——对于算法来说，记录数的上限是多少才是有效的？
- en: Your sampling rate can be chosen to balance computational cost versus accuracy.
    We’ve run this algorithm against datasets that have as many as tens of billions
    of rows added per day. In practice, and based on rigorous testing, we’ve found
    that 10,000 records per day (if randomly sampled) provides enough data to capture
    most data quality issues, even those affecting as little as 1–5% of records. Quality
    improvement decays as sample sizes exceed 100,000.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 可以选择抽样率来平衡计算成本与精度。我们已经针对每天新增数十亿行的数据集运行了这种算法。在实践中，并基于严格的测试，我们发现每天抽样 10,000 条记录（如果随机抽样）提供了足够的数据来捕获大多数数据质量问题，即使这些问题只影响
    1–5% 的记录。随着样本量超过 100,000，质量改善逐渐减弱。
- en: Large sample sizes (say, 1,000,000 records per day) can be used, but the computational
    cost has not proven to be worth the value. A dataset would need to be very, very
    stable (little background chaos), and the change would need to be in a very small
    percentage of records (say, 0.1% of records), for this increase in sample size
    to be worthwhile.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用大样本量（比如，每天 1,000,000 条记录），但计算成本尚未证明其价值。数据集需要非常稳定（背景混乱较少），变化必须在非常小的记录百分比内（比如，0.1%
    的记录）才能使增加抽样大小变得值得。
- en: It can seem like a mistake to sample a fixed sample size (10,000 records), rather
    than to sample, say, 10% of the data. After all, if I have 1 billion records,
    how can 10,000 still be representative of that huge population?
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 将抽样固定为一个固定的样本大小（10,000 条记录）似乎是一个错误，而不是抽样，比如，数据的 10%。毕竟，如果我有 10 亿条记录，10,000 条如何代表那么庞大的人口？
- en: Perhaps counterintuitively, because the sample is chosen entirely at random,
    its accuracy doesn’t depend on the total size of the data, just on the absolute
    sample size. For example, consider estimating the average income of a country.
    Just because China’s population is 1.4 billion, and Luxembourg’s population is
    600k, does that mean that we would need to sample more people in China to get
    an estimate of the average income? No. In both cases, we could take 1,000 people
    and get a very good estimate of the average income.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 或许出乎意料的是，因为样本完全是随机选择的，其精度并不依赖于数据的总大小，而只依赖于绝对样本大小。例如，考虑估计一个国家的平均收入。仅因为中国人口为 14
    亿，卢森堡人口为 60 万，这是否意味着我们需要在中国抽样更多的人来估计平均收入？不是的。在这两种情况下，我们可以抽样 1,000 人，并得到一个非常好的平均收入估计。
- en: Bias and efficiency
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 偏差和效率
- en: It is essential that the sample be taken *at random* from the table. If there
    is any bias in the sampling, then the algorithm will be able to find that bias
    and will represent it as a false positive change in the data that will confuse
    users.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 很重要的一点是从表中*随机抽样*样本。如果抽样中存在任何偏差，算法将能够发现这种偏差，并将其表示为数据中的虚假正变化，这会使用户感到困惑。
- en: It’s also critical to ensure that the sampling is as efficient as possible.
    In practice, getting random records out of the data warehouse for the machine
    learning model is often the most expensive operation in this kind of a system.
    That is because the table may have billions or even trillions of records in it,
    and hundreds or thousands of columns. If a query naively required reading every
    record into memory or sending it over a network in order to sample, this would
    be disastrous for performance and incur a great deal of data warehouse costs.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 确保抽样尽可能高效也至关重要。在实践中，从数据仓库中随机抽取记录对于机器学习模型通常是这类系统中最昂贵的操作。这是因为表中可能包含数十亿甚至数万亿的记录，以及数百或数千列。如果一个查询天真地要求将每条记录读入内存或通过网络发送以进行抽样，这将对性能造成灾难性影响并产生大量的数据仓库成本。
- en: Bias and efficiency can sometimes have a seesaw relationship. For example, one
    way of scaling random sampling efficiently in modern data warehouses is to use
    `TABLESAMPLE` operators rather than `random()` calls. The `TABLESAMPLE` operator
    is implemented in a way that allows the warehouse to efficiently sample random
    records during the query execution, without having to read the records into memory—but
    in some cases it can have negative trade-offs for bias.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 偏倚和效率有时可能存在相互制衡的关系。例如，在现代数据仓库中有效地扩展随机采样的一种方式是使用 `TABLESAMPLE` 操作符，而不是 `random()`
    调用。`TABLESAMPLE` 操作符是以一种允许仓库在查询执行期间有效地对随机记录进行采样的方式实现的，而无需将记录读入内存——但在某些情况下，它可能会对偏倚产生负面的权衡。
- en: In BigQuery, the `TABLESAMPLE` operator [implementation](https://oreil.ly/xVPqO)
    works “by randomly selecting a percentage of data blocks from the table and reading
    all of the rows in the selected blocks.” The documentation continues by explaining
    that, typically, “BigQuery splits tables or table partitions into blocks if they
    are larger than about 1 GB.” This means that, in practice, the results returned
    by the `TABLESAMPLE` operator will often not be random in BigQuery and may instead
    be entirely in a single partition. If you’ve partitioned your data on an identifier
    that you use frequently for joins—say, a customer ID—then you’ll have specific
    subsets of customers that will be far more likely to appear in your random sample
    than others will. This could meaningfully bias the results of your ML, causing
    you to continually see a shift in the user population over time that is entirely
    due to the sampling implementation rather than any real drift in your data itself.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BigQuery 中，`TABLESAMPLE` 操作符的[实现](https://oreil.ly/xVPqO)是通过“随机选择表中一定百分比的数据块，并读取所选块中的所有行”来工作的。文档继续解释说，通常情况下，“如果
    BigQuery 将表或表分区分割成大约 1 GB 的块，则...” 这意味着，在实践中，`TABLESAMPLE` 操作符返回的结果通常在 BigQuery
    中不会是随机的，而可能完全在一个分区中。如果您对标识符（例如客户 ID）进行了数据分区，并且经常用于连接，则将具有特定子集的客户远比其他人更有可能出现在您的随机样本中。这可能会显著偏倚您的
    ML 结果，导致您随着时间的推移不断看到用户群体的变化，而这完全是由于抽样实现而不是数据本身的任何真实漂移所导致的。
- en: 'So, how do you sample efficiently and avoid bias? Here are our recommendations:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何高效抽样并避免偏倚呢？以下是我们的建议：
- en: Ensure that you are only using a small number of days’ worth of data each time
    you run the algorithm. These days can be stored as snapshots so that they don’t
    have to be queried again (though it may be worthwhile to query them again, as
    the historical data in the table could have changed).
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保每次运行算法时仅使用少量天数的数据。这些天数可以存储为快照，因此无需再次查询它们（尽管重新查询它们可能是值得的，因为表中的历史数据可能已更改）。
- en: Ensure that the table is partitioned on the date column that you are using to
    select the data. This allows the data warehouse to efficiently navigate to just
    the files on disk that represent these days and read and process only those dates
    of data without having to access other, irrelevant dates.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保表格在您用于选择数据的日期列上进行了分区。这允许数据仓库有效地导航到仅代表这些日期的磁盘文件，并读取和处理仅有的这些数据日期，而无需访问其他无关的日期。
- en: Use the `TABLESAMPLE` operator to efficiently sample an approximate random sample
    that is larger than the one you need (e.g., if you need 0.3%, then sample 1%).
    Often the lower bound percentage that can be sampled with this operator is 1%,
    though the implementation varies by warehouse. Note that not all databases or
    data warehouses support `TABLESAMPLE` in a robust way—see the previous paragraph
    on BigQuery.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `TABLESAMPLE` 操作符可以有效地对比所需的较大的近似随机样本进行采样（例如，如果您需要 0.3%，则采样 1%）。通常情况下，可以使用该操作符进行采样的最低百分比是
    1%，尽管实现因仓库而异。请注意，并非所有数据库或数据仓库都以健壮的方式支持 `TABLESAMPLE` ——请参阅前文有关 BigQuery 的段落。
- en: Count the total number of records that you will have on the dates you are querying,
    in order to understand the exact sample percentage that you will need to query
    for.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计您在查询日期上将拥有的总记录数，以便了解您需要查询的确切样本百分比。
- en: 'Take the final random sample using code that looks like: `random() <= X` for
    some `X` that gives you approximately the correct number of rows on each date
    after the `TABLESAMPLE` operation. This is far more efficient than something that
    looks like `order by random() limit 10,000`, which would require loading all of
    the data into memory in a master node in the warehouse and sorting it by a random
    number before applying a limit. The benefit of the `random() <= X` approach is
    that it can be applied in a distributed fashion in the warehouse in each of the
    worker compute nodes. Note that the minor downside is that your random sample
    is unlikely to be exactly 10,000 rows but will instead be a number that is quite
    close.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用类似以下代码获取最终随机样本：`random() <= X`，其中 `X` 可以给出 `TABLESAMPLE` 操作后每个日期大约正确行数的近似值。
    这比看起来像 `order by random() limit 10,000` 的方法更有效，后者需要在仓库的主节点中将所有数据加载到内存中，并按随机数进行排序，然后应用限制。
    `random() <= X` 方法的好处是可以在仓库中的每个工作节点上以分布式方式应用。 请注意，其轻微缺点是您的随机样本不太可能恰好是 10,000 行，而是会接近某个数。
- en: 'Another important consideration when querying for data is to ensure that the
    `WHERE` SQL filter is implemented efficiently. For example, for a table with the
    date column `created_date`, specified as a string in YYYY-MM-DD format, this would
    be very inefficient:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 查询数据时的另一个重要考虑因素是确保 `WHERE` SQL 过滤器的高效实施。 例如，对于具有日期列 `created_date`（以 YYYY-MM-DD
    格式指定为字符串）的表，这样做非常低效：
- en: '[PRE0]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code would require the database to read every partition and convert the
    `created_date` column in memory in order to decide if the record should be included.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码需要数据库在内存中读取每个分区，并转换 `created_date` 列以决定是否应包含记录。
- en: 'Instead, try:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 而是，请尝试：
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now the data warehouse can use metadata about each partition to decide which
    to exclude entirely from being considered by the query. This can be quite challenging
    for tables that are formatted with unusual date or time partitions. At Anomalo,
    we have had to add support for all of the types in [Figure 4-3](#example_of_types_of_datesolidustime_par).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数据仓库可以使用关于每个分区的元数据来决定从查询中完全排除哪些分区。 对于格式化有不寻常日期或时间分区的表格，这可能非常具有挑战性。 在 Anomalo，我们不得不为
    [图 4-3](#example_of_types_of_datesolidustime_par) 中的所有类型添加支持。
- en: '![Example of types of date/time partitions](assets/adqm_0403.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![日期/时间分区类型示例](assets/adqm_0403.png)'
- en: Figure 4-3\. Example of types of date/time partitions.
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. 日期/时间分区类型示例。
- en: Jeremy has been known, at times, to joke that we expect to support time in “number
    of days since Kevin Bacon was born” format soon.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，杰里米开玩笑说我们很快将支持“自凯文·培根诞生以来的天数”格式的时间。
- en: Feature Encoding
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征编码
- en: ML models aren’t typically trained on raw data, but rather they learn using
    numerical features, which are transformations of the raw data into signals the
    model can use. How the raw data is transformed can have a significant impact on
    the performance of the model and typically requires both expertise in ML and subject
    matter expertise in the data and problem at hand. This process, called feature
    engineering, must be fully automated in our anomaly detection algorithm.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ML 模型通常不是在原始数据上训练的，而是使用数值特征进行学习，这些特征是将原始数据转换为模型可以使用的信号。 如何转换原始数据对模型性能有重要影响，通常需要
    ML 和数据领域专业知识。 这个过程称为特征工程，在我们的异常检测算法中必须完全自动化。
- en: 'How this works is as follows: each record in the sample will have a number
    of columns, and each column could be an integer, float, string, Boolean, date
    or timestamp, or complex type like JSON or an array. You’ll need an automated
    process that walks through each column (expanding complex types like JSON into
    subcolumns if necessary—see [“Semistructured data”](ch03.html#semistructured_data)
    for more on this), extracts information that could be interesting to your model,
    and encodes this information into a floating point matrix of ML features.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的工作原理：样本中的每条记录都有多列，每列可以是整数、浮点数、字符串、布尔值、日期或时间戳，或者像 JSON 或数组这样的复杂类型。 您需要一个自动化过程，遍历每一列（必要时展开复杂类型，如
    JSON 为子列——详见 [“半结构化数据”](ch03.html#semistructured_data) 以获取更多信息），提取可能对模型有趣的信息，并将此信息编码为
    ML 特征的浮点数矩阵。
- en: '![Encoding data as features. Note that the response variable (a.k.a. label)
    corresponds to the date: 0 for yesterday and 1 for today.](assets/adqm_0404.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![将数据编码为特征。 请注意，响应变量（也称为标签）对应于日期：昨天为 0，今天为 1。](assets/adqm_0404.png)'
- en: 'Figure 4-4\. Encoding data as features. Note that the response variable (a.k.a
    label) corresponds to the date: 0 for yesterday and 1 for today. See a full-sized
    version of this image at [*https://oreil.ly/adqm_4_4*](https://oreil.ly/adqm_4_4).'
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4\. 将数据编码为特征。请注意，响应变量（又名标签）对应于日期：昨天为0，今天为1。在[*https://oreil.ly/adqm_4_4*](https://oreil.ly/adqm_4_4)上查看此图的完整版本。
- en: 'You’ll want to develop a library of candidate encoder types to apply, based
    on the features that you believe could tell you whether the data has changed in
    a meaningful way (see [Figure 4-4](#encoding_data_as_featuresdot_note_that)).
    Here are some encoders we recommend:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要开发一组候选编码器类型，以应用于您认为可以告诉您数据是否以有意义的方式发生变化的特征（见[图 4-4](#encoding_data_as_featuresdot_note_that)）。以下是我们推荐的一些编码器：
- en: '`Numeric`'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`Numeric`'
- en: Converts Boolean, integer, and floating values into floats
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 将布尔值、整数和浮点值转换为浮点数
- en: '`Frequency`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`Frequency`'
- en: How often each value appears in the sample of data
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 样本数据中每个值出现的频率
- en: '`IsNull`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`IsNull`'
- en: A binary indicator for whether the column is NULL
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 用于判断列是否为NULL的二进制指示器
- en: '`TimeDelta`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`TimeDelta`'
- en: Seconds between a time and when the record was created
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 记录创建与时间之间的秒数
- en: '`SecondOfDay`'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`SecondOfDay`'
- en: The time of day the record was created
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 记录创建的时间点
- en: '`OneHot`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`OneHot`'
- en: A one-hot encoder, which lets you map feature values (like categories or frequent
    integer values) to a binary yes or no indicator variable for each unique value
    in the column
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一种独热编码器，允许你将特征值（如类别或频繁整数值）映射到列中每个唯一值的二进制是或否指示变量
- en: Data scientists may wonder about the applicability of common encoders like term
    frequency-inverse document frequency (TF-IDF), mean encoding, or Laplace smoothing.
    Many standard encoders aren’t very relevant for tree-based models (log transformation,
    mean encoding, principal component analysis [PCA]). Others would require a lot
    of subject matter expertise about the specific data to use well (Laplace smoothing),
    and still others might be useful but would be very hard to interpret (TF-IDF,
    word/vector embeddings).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家可能会关心常见编码器如词项频率-逆文档频率（TF-IDF）、均值编码或拉普拉斯平滑的适用性。许多标准编码器对于基于树的模型（对数转换、均值编码、主成分分析[PCA]）并不十分相关。其他编码器可能需要大量关于具体数据的专业知识才能很好地使用（拉普拉斯平滑），还有一些可能是有用的，但解释起来非常困难（TF-IDF、词/向量嵌入）。
- en: You have to be careful with how complex you make your encoders, because in the
    end, you’ll need to use these encodings to explain the data quality issue to the
    user. For example, we tested a “gap” encoder for time, integer, and numeric fields,
    which took each observation and computed the gap between it and the next largest
    value in that column. In practice, this was able to detect some kinds of data
    quality issues, but it would also detect many other changes in the data that would
    be both hard to understand and/or irrelevant for our purposes—like changes in
    the grain of how data is being logged or unrelated changes in the volume (and
    therefore density) of observations.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须小心您的编码器设计得多复杂，因为最终，您将需要使用这些编码器来向用户解释数据质量问题。例如，我们测试了一个用于时间、整数和数值字段的“间隔”编码器，它将每个观察值与该列中下一个较大值之间的间隔进行计算。在实践中，这能够检测到某些数据质量问题，但它也会检测到许多其他数据的变化，这些变化对于我们的目的来说既难以理解又/或不相关，例如数据记录方式的粒度变化或观察值的密度无关的变化。
- en: Model Development
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型开发
- en: To meet the scalability requirements and work in a practical setting, you need
    a model architecture that’s fast at inference and training, can be trained on
    relatively small samples, and will generalize to any kind of tabular data (when
    properly feature-encoded). Gradient-boosted decision trees work well for this
    use case, and you’ll find libraries like [XGBoost](https://oreil.ly/_KZPk) readily
    available for model development.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足可伸缩性要求并在实际环境中工作，您需要一个快速推断和训练的模型架构，可以在相对小的样本上进行训练，并且将泛化到任何类型的表格数据（在适当的特征编码下）。梯度增强决策树非常适合这种用例，您会发现像[XGBoost](https://oreil.ly/_KZPk)这样的库非常适合模型开发。
- en: Gradient-boosted decision trees work in an iterative fashion by building a sequence
    of decision trees on the dataset, where each tree (or “step”) is designed to correct
    the mistakes of all of the trees that came before it. Ultimately, the model’s
    prediction takes into account the results from all the trees that were trained
    at each step (this is known as an ensemble model). See [Figure 4-5](#a_gradient_boosted_decision_tree_left_p).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升决策树通过在数据集上构建一系列决策树（或“步骤”）的迭代方式工作，其中每棵树都旨在纠正之前所有树的错误。最终，模型的预测考虑了在每个步骤训练的所有树的结果（这被称为集成模型）。参见[图4-5](#a_gradient_boosted_decision_tree_left_p)。
- en: '![A gradient-boosted decision tree (adapted from Haowen Deng et al., “Ensemble
    Learning for the Prediction of Neonatal Jaundice with Genetic Features,” BMC Medical
    Informatics and Decision Making, 2011)](assets/adqm_0405.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![梯度提升决策树（改编自邓浩文等人，“基于遗传特征的新生儿黄疸预测的集成学习”，BMC医学信息学和决策制定，2011年）](assets/adqm_0405.png)'
- en: Figure 4-5\. A gradient-boosted decision tree (adapted from Haowen Deng et al.,
    “Ensemble Learning for the Early Prediction of Neonatal Jaundice with Genetic
    Features,” *BMC Medical Informatics and Decision Making* 21, no. 338 [2021]).
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-5. 梯度提升决策树（改编自邓浩文等人，“集成学习早期预测基因特征新生儿黄疸”，BMC医学信息学和决策制定21卷338号[2021年]）。
- en: Helpfully, gradient-boosted decision trees have a very small number of parameters
    that really matter for tuning (mainly, the learning rate and the complexity of
    each tree, though there are others) and can be trained on datasets with thousands
    or even millions of records very quickly.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，梯度提升决策树仅有少量的参数对调整非常重要（主要是学习率和每棵树的复杂度，尽管还有其他因素），并且可以在包含数千甚至数百万条记录的数据集上快速训练。
- en: Some alternate approaches, such as linear models, are too simple to learn the
    complex patterns in most structured datasets. Other approaches, like neural networks,
    are often too complex for problems like this and require extremely large volumes
    of heterogeneous data to become very powerful (as in image and language models).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一些替代方法，比如线性模型，过于简单，无法学习大多数结构化数据集中的复杂模式。其他方法，如神经网络，通常对于这类问题过于复杂，并且需要极大量的异构数据才能发挥出很强的效力（比如在图像和语言模型中）。
- en: 'The downside of gradient-boosted decision trees, like any structured ML technique,
    is that they require feature engineering: human experts have to tell the model
    what aspects of the data it should consider when making its predictions, and this
    can take a lot of time and energy.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何结构化机器学习技术一样，梯度提升决策树的缺点是需要进行特征工程：人类专家必须告诉模型在进行预测时应该考虑数据的哪些方面，这可能需要大量时间和精力。
- en: Training and evaluation
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练与评估
- en: In theory, gradient-boosted decision trees could continue iterating and iterating
    endlessly, so it’s essential to cap the number of steps at some limit. To do this,
    you’ll typically want to evaluate the model’s performance after each step. Select
    a random portion of your data to use as a holdout set for evaluation (and not
    training) and test the model after each iteration. Your model’s performance will
    be an indication of whether there is something anomalous about today’s data.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，梯度提升决策树可以无限迭代下去，因此非常重要的是设定步数的上限。为了做到这一点，通常需要在每一步之后评估模型的性能。选择数据的随机部分作为保留集用于评估（而非训练），并在每次迭代后测试模型。您的模型性能将表明今天的数据是否存在异常。
- en: Specifically, there are three model performance patterns that we tend to see
    in practice shown in [Figure 4-6](#the_three_most_common_scenarios_encount). On
    these charts, the x-axis represents the number of trees added to the model (the
    number of iterations), while the y-axis plots a measure of the model’s accuracy
    (the log of the loss function). Note that here, the y-axis is technically plotting
    how much “error” there is in the model’s predictions, so a lower value indicates
    higher accuracy.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，在实践中我们通常看到三种模型性能模式，如[图4-6](#the_three_most_common_scenarios_encount)所示。在这些图表中，x轴表示添加到模型中的树的数量（迭代次数），而y轴则绘制了模型准确性的一种度量（损失函数的对数）。请注意，这里y轴技术上是绘制模型预测中的“错误”量，因此较低的值表示更高的准确性。
- en: The first scenario, “No anomaly,” is when there is little progress made on the
    training data and the performance on the test data begins getting worse very quickly.
    This means that there is unlikely to be any anomaly in the new data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种情况，“无异常”，是指在训练数据上几乎没有进展，而测试数据的表现很快变得更糟。这意味着新数据中不太可能存在任何异常。
- en: The second scenario, “Incomplete,” happens when the model doesn’t have enough
    time to converge. You reach a maximum number of trees (set to prevent the model
    from running indefinitely) and yet still find that the training error and test
    error are declining. You’ll either need to add more trees or, perhaps more prudently,
    increase the learning rate, which causes the gradient boosting algorithm to take
    larger “steps” in the direction of each tree that it evaluates.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种情况，“不完整”，发生在模型没有足够时间收敛时。您达到了树的最大数量（设置为防止模型无限运行），但仍然发现训练误差和测试误差在下降。您需要增加更多的树，或者更谨慎地增加学习率，这会导致梯度提升算法在评估每棵树时采取更大的“步骤”。
- en: The third scenario, “Optimal,” occurs when the model makes good progress on
    training and test, until a point where the test loss begins increasing. This indicates
    that you can stop where the test loss was at its minimum. At that point, the model
    will have learned as much about what differentiates these two datasets as it can,
    given the other parameters of the learning algorithm.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种情况，“最优”，发生在模型在训练和测试上取得良好进展，直到测试损失开始增加的某一点。这表明您可以在测试损失达到最小值时停止。在那时，模型将尽可能多地了解区分这两个数据集的内容，考虑到学习算法的其他参数。
- en: '![The three most common scenarios encountered when plotting the model’s performance
    on training and test data as the number of trees increases. Performance is measured
    using a log loss error function (a lower value on the y-axis indicates better
    performance).](assets/adqm_0406.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![当树的数量增加时，绘制模型在训练和测试数据上的表现时遇到的三种最常见情景。使用对数损失函数来衡量性能（y 轴上较低的值表示更好的性能）。](assets/adqm_0406.png)'
- en: Figure 4-6\. The three most common scenarios encountered when plotting the model’s
    performance on training and test data as the number of trees increases. Performance
    is measured using a log loss error function (a lower value on the y-axis indicates
    better performance).
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-6。当树的数量增加时，绘制模型在训练和测试数据上的表现时遇到的三种最常见情景。使用对数损失函数来衡量性能（y 轴上较低的值表示更好的性能）。
- en: In practice, in order to provide consistently interpretable model statistics
    and explainability results, you’ll need to strike a balance between optimizing
    your model for a single dataset and building a model that generalizes to many
    datasets across different periods of time.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，为了提供一致可解释的模型统计和可解释性结果，您需要在优化单一数据集的模型和构建能够泛化到不同时间段多个数据集的模型之间取得平衡。
- en: Computational efficiency
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算效率
- en: 'Many organizations have important tables that can include billions of records.
    Examples include:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 许多组织拥有重要的数据表，可能包含数十亿条记录。例如：
- en: Transactional data from financial services industries
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自金融服务行业的交易数据
- en: Raw event data from high-traffic applications or websites
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自高流量应用程序或网站的原始事件数据
- en: Digital advertising impression and event-level data
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字广告印象和事件级数据
- en: Physical sensor data
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物理传感器数据
- en: Messaging information from social platforms
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社交平台的消息信息
- en: With data at this scale, it’s easy to create a monitoring strategy that will
    be cost-prohibitive, or simply fail to successfully run even with modern data
    warehouses.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种规模的数据情况下，很容易创建一个监控策略，这可能会成本过高，或者简单地无法成功运行，即使使用现代数据仓库也是如此。
- en: Because we’ve placed a limit on the number of records we are sampling per day,
    most of the computation and memory usage in the model will scale linearly with
    the number of columns added. For example, searching for the best split while expanding
    a decision tree at each node will increase linearly with the number of columns
    you need to search over. Although typical tables have 10–50 columns, it’s common
    for tables to have 200 columns, and some tables have thousands. Furthermore, tables
    may have JSON data that you’ll need to automatically expand into new synthetic
    columns, which can lead to tables with 10,000 columns in some situations.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们对每天采样的记录数量设定了限制，模型中的大部分计算和内存使用量将随着增加的列数呈线性增长。例如，在扩展决策树的每个节点时搜索最佳分割点将随着需要搜索的列数线性增加。尽管典型表格通常有10至50列，但表格通常会有200列，有些表格甚至有数千列。此外，表格可能包含JSON数据，您需要自动展开为新的合成列，在某些情况下可能会导致表格拥有10,000列。
- en: 'The following optimizations can make your algorithm more computationally efficient:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 下列优化措施可以使您的算法更具计算效率：
- en: Ensuring that you are only querying for one day of data at a time and snapshotting
    results as much as possible to build history. Note that this comes at a cost,
    as algorithms will have less history to work with on day one and won’t be as effective
    in a “cold start” scenario.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保每次只查询一天的数据，并尽可能快照结果以建立历史。请注意，这将带来成本，因为算法在第一天将有更少的历史数据可供使用，并且在“冷启动”场景中不会像在其他情况下那样有效。
- en: Randomly sampling records from the table using the data warehouse (using the
    efficient techniques noted earlier in [“Bias and efficiency”](#bias_and_efficiency))
    and computing more complex profiling or ML results on the random samples.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据仓库从表格中随机抽样记录（使用之前提到的有效技术在[“偏差和效率”](#bias_and_efficiency)中），并在随机样本上计算更复杂的分析或机器学习结果。
- en: If using gradient boosting decision trees, limit the depth and total number
    of trees, as we are not typically looking for very complex interactions, and early
    stop if your test error increases significantly during the training process.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果使用梯度提升决策树，请限制深度和总树数，因为我们通常不寻求非常复杂的交互作用，并在训练过程中如果测试误差显著增加则提前停止。
- en: Optimize the learning process itself, which, depending on your computing platform
    and learning algorithm, could include steps like using sparse encodings, distributing
    learning via multiprocessing, or utilizing GPUs.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化学习过程本身，这可能包括使用稀疏编码、通过多进程分布学习或利用GPU等步骤，具体取决于您的计算平台和学习算法。
- en: Model Explainability
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型可解释性
- en: If you have a model that performs well on the test set, this indicates that
    you’ve found a potential data quality issue. The next step is to explain what
    the model has found.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的模型在测试集上表现良好，这表明您可能发现了潜在的数据质量问题。下一步是解释模型发现了什么。
- en: Explainability is key for several reasons. First, it tells you *how* anomalous
    the data from today is. This lets you perform many kinds of tuning to avoid alert
    fatigue (more on this in Chapters [5](ch05.html#building_a_model_that_works_on_real_wor)
    and [6](ch06.html#implementing_notifications_while_avoidi)). For those issues
    where you do fire an alert, knowing the severity will help end users prioritize
    their response.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性是关键，因为它告诉您今天的数据异常程度。这使您可以执行各种调整以避免警报疲劳（更多详情请参见第[5](ch05.html#building_a_model_that_works_on_real_wor)章和第[6](ch06.html#implementing_notifications_while_avoidi)章）。对于那些确实发出警报的问题，了解严重程度将有助于最终用户优先处理其响应。
- en: Second, explainability tells you *where* in the data that anomaly is located.
    This lets you point investigators to the right segments of the data and create
    all kinds of interesting root cause analysis aids, like samples of bad data (more
    details in [Chapter 6](ch06.html#implementing_notifications_while_avoidi)).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，可解释性告诉您数据中异常发生的*位置*。这使您可以将调查员指向数据的正确部分，并创建各种有趣的根本原因分析辅助工具，例如坏数据样本（更多细节请参见第[6](ch06.html#implementing_notifications_while_avoidi)章）。
- en: So how does model explainability work? The idea is to derive a score that credits
    how much each individual {row, column} cell in the dataset contributed to the
    model’s prediction. While there are several approaches, we use [SHAP](https://oreil.ly/TdiVx),
    which essentially approximate a local linear estimation of what the algorithm
    is doing for each cell in the dataset.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如何解释模型可解释性？其核心思想是衍生一个分数，以表明数据集中每个个体{行，列}单元对模型预测的贡献。虽然有多种方法，我们使用的是[SHAP](https://oreil.ly/TdiVx)，其本质上是对数据集中每个单元进行局部线性估计来近似算法的工作方式。
- en: 'To see how this works in practice, suppose that we are trying to detect data
    quality issues in a table of credit card transaction data and have sampled 10,000
    records from yesterday and today, encoded our features, and built our model predicting
    on which day each record arrived. Then let’s follow the following four records
    through the SHAP explainability process:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解这在实践中如何运作，假设我们试图检测信用卡交易数据表中的数据质量问题，并从昨天和今天中抽取了 10,000 条记录，对我们的特征进行编码，并建立了预测每条记录到达的哪一天的模型。然后让我们通过
    SHAP 解释过程跟随以下四条记录：
- en: '| Amount | Type | FICO score | Brand | Type | Credit limit |   | Source |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 金额 | 类型 | FICO 评分 | 品牌 | 类型 | 信用额度 |   | 来源 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| $18 | Swipe | 684 | Discover | Debit | $12,564 |   | Today |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| $18 | 刷卡 | 684 | 发现 | 借记 | $12,564 |   | 今天 |'
- en: '| $59 | Chip | 578 | Mastercard | Credit | $7,600 |   | Today |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| $59 | 芯片 | 578 | 万事达卡 | 信用 | $7,600 |   | 今天 |'
- en: '| –$445 | Chip | 689 | Visa | Credit | $6,700 |   | Yesterday |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| –$445 | 芯片 | 689 | Visa | 信用 | $6,700 |   | 昨天 |'
- en: '| $137 | Chip | 734 | Mastercard | Credit | $7,100 |   | Yesterday |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| $137 | 芯片 | 734 | 万事达卡 | 信用 | $7,100 |   | 昨天 |'
- en: In this case, we have two records from yesterday and two records from today.
    (Recall that the source column isn’t used to make predictions about which day
    the data arrived on; rather, it is the response that we are training the model
    to predict.)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有两条来自昨天和两条来自今天的记录。（回想一下，源列并不用于预测数据到达的日期；相反，它是我们训练模型预测的响应。）
- en: 'Then, suppose that we take our model and make predictions for each row for
    which day we think it is likely to have arrived on:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们拿出模型，并为每一行做出预测，我们认为它可能在哪一天到达：
- en: '| Amount | Type | FICO score | Brand | Type | Credit limit |   | Source | Predicted
    Pr( Today ) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 金额 | 类型 | FICO 评分 | 品牌 | 类型 | 信用额度 |   | 来源 | 预测的 Pr( 今天 ) |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| $18 | Swipe | 684 | Discover | Debit | $12,564 |   | Today | 51% |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| $18 | 刷卡 | 684 | 发现 | 借记 | $12,564 |   | 今天 | 51% |'
- en: '| $59 | Chip | 578 | Mastercard | Credit | $7,600 |   | Today | 78% |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| $59 | 芯片 | 578 | 万事达卡 | 信用 | $7,600 |   | 今天 | 78% |'
- en: '| –$445 | Chip | 689 | Visa | Credit | $6,700 |   | Yesterday | 45% |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| –$445 | 芯片 | 689 | Visa | 信用 | $6,700 |   | 昨天 | 45% |'
- en: '| $137 | Chip | 734 | Mastercard | Credit | $7,100 |   | Yesterday | 52% |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| $137 | 芯片 | 734 | 万事达卡 | 信用 | $7,100 |   | 昨天 | 52% |'
- en: In this case, we find that our model thinks there is a 78% chance that the second
    record is from today, whereas the other three records are within ±5% of the 50%
    average prediction that would indicate the model has no strong bias for which
    day the data came from.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们发现我们的模型认为第二条记录是今天的概率为 78%，而其他三条记录的预测都在 50% 的平均预测值的 ±5% 范围内，这表明模型对数据来自哪一天没有明显的偏见。
- en: 'Rather than working directly with the predicted probability (which is hard
    to express as a linear relationship, given probabilities are naturally bounded
    between 0% and 100%), we convert the probabilities into their log odds, using
    the formula `log odds = ln [ probability / (1 - probability) ]`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 与直接使用预测概率（由于概率自然地限制在 0% 到 100% 之间，很难表达为线性关系），我们将概率转换为它们的对数几率，使用公式 `对数几率 = ln
    [ 概率 / (1 - 概率) ]`：
- en: '| Amount | Type | FICO score | Brand | Type | Credit limit |   | Source | Predicted
    Pr( Today ) | Log odds |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 金额 | 类型 | FICO 评分 | 品牌 | 类型 | 信用额度 |   | 来源 | 预测的 Pr( 今天 ) | 对数几率 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| $18 | Swipe | 684 | Discover | Debit | $12,564 |   | Today | 51% | 0.02 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| $18 | 刷卡 | 684 | 发现 | 借记 | $12,564 |   | 今天 | 51% | 0.02 |'
- en: '| $59 | Chip | 578 | Mastercard | Credit | $7,600 |   | Today | 78% | 0.55
    |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| $59 | 芯片 | 578 | 万事达卡 | 信用 | $7,600 |   | 今天 | 78% | 0.55 |'
- en: '| –$445 | Chip | 689 | Visa | Credit | $6,700 |   | Yesterday | 45% | –0.09
    |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| –$445 | 芯片 | 689 | Visa | 信用 | $6,700 |   | 昨天 | 45% | –0.09 |'
- en: '| $137 | Chip | 734 | Mastercard | Credit | $7,100 |   | Yesterday | 52% |
    0.03 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| $137 | 芯片 | 734 | 万事达卡 | 信用 | $7,100 |   | 昨天 | 52% | 0.03 |'
- en: 'Then, we can run the SHAP algorithm, which will decompose these log odds statistics
    into a linear combination of contributions from each of the columns as used in
    the ML model (in reality, we would need to get the SHAP values at the feature
    level, and then aggregate those, but you get the point):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以运行 SHAP 算法，将这些对数几率统计分解为每个列的贡献的线性组合，如在 ML 模型中使用的那样（实际上，我们需要在特征级别获取 SHAP
    值，然后聚合这些值，但你明白我的意思）：
- en: '| Amount | Type | FICO score | Brand | Type | Credit limit |   | Predicted
    Pr( Today ) | Predicted Pr( Today ) | Log odds |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 金额 | 类型 | FICO 分数 | 品牌 | 类型 | 信用额度 |   | 预测概率(今天) | 预测概率(今天) | 对数几率 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0.01 | 0.02 | 0.02 | –0.01 | 0.00 | –0.01 |   | Today | 51% | 0.02 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 0.01 | 0.02 | 0.02 | –0.01 | 0.00 | –0.01 |   | 今天 | 51% | 0.02 |'
- en: '| –0.03 | 0.01 | 0.41 | 0.19 | –0.01 | –0.03 |   | Today | 78% | 0.55 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| –0.03 | 0.01 | 0.41 | 0.19 | –0.01 | –0.03 |   | 今天 | 78% | 0.55 |'
- en: '| 0.02 | –0.03 | –0.05 | 0.02 | –0.03 | –0.02 |   | Yesterday | 45% | –0.09
    |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 0.02 | –0.03 | –0.05 | 0.02 | –0.03 | –0.02 |   | 昨天 | 45% | –0.09 |'
- en: '| 0.01 | –0.02 | –0.01 | 0.01 | 0.02 | 0.02 |   | Yesterday | 52% | 0.03 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 0.01 | –0.02 | –0.01 | 0.01 | 0.02 | 0.02 |   | 昨天 | 52% | 0.03 |'
- en: 'In this case, we find that the `FICO SCORE` and `BRAND` column values are contributing
    significantly to the model’s prediction that the second record is from today.
    Examining the data values above, we see that this corresponds to:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们发现 `FICO SCORE` 和 `BRAND` 列的值对于模型预测第二条记录来自今天有显著贡献。查看上述数据值，我们可以看到这对应于：
- en: '`FICO SCORE = 578`'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FICO SCORE = 578`'
- en: '`BRAND = ''Mastercard''`'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BRAND = ''Mastercard''`'
- en: This suggests that there may be something anomalous happening with the distribution
    of low credit scores for Mastercard transactions (though we are examining only
    a few records here—in practice, we would look at the SHAP values distribution
    summarized across all 10,000 records per day).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，在 Mastercard 交易的低信用分布中可能发生了异常情况（尽管在这里我们只检查了几条记录—实际上，我们将查看每天所有 10,000 条记录的
    SHAP 值分布）。
- en: After normalizing and appropriately tuning the SHAP values following techniques
    in [Chapter 5](ch05.html#building_a_model_that_works_on_real_wor), the end result
    is what we call the “anomaly score.” Importantly, this score can be aggregated
    and/or sliced to provide many different levels of granularity.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在对 SHAP 值进行归一化和适当调整之后（如[第 5 章](ch05.html#building_a_model_that_works_on_real_wor)中的技术），最终结果就是我们所称的“异常分数”。重要的是，这个分数可以聚合和/或切片，以提供多种不同粒度的细节。
- en: At the lowest level, you can look at the anomaly score for *each individual
    {row, column} cell in the sampled data*. From here, you can aggregate the anomaly
    scores for a row to find the most anomalous entries, or by sets of rows to find
    anomalous segments. You can take the average anomaly score by column to find the
    most anomalous columns. Or you can calculate the anomaly score for the entire
    table. You can also cluster anomaly scores to find correlations across columns
    (more on this in [Chapter 5](ch05.html#building_a_model_that_works_on_real_wor)).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在最低层次上，您可以查看采样数据中每个个体{行，列}单元格的异常分数。从这里，您可以对一行的异常分数进行聚合，找出最异常的条目，或者按行集合找到异常段落。您可以按列计算平均异常分数，找出最异常的列。或者您可以计算整个表的异常分数。您还可以对异常分数进行聚类，以找出跨列的相关性（有关详细信息，请参见[第 5
    章](ch05.html#building_a_model_that_works_on_real_wor)）。
- en: Knowing the anomaly score isn’t only important for data where there’s been a
    significant change. By calculating the score for every record in the table, you
    can create visualizations that put the anomaly in context, as in [Figure 4-7](#in_this_sample_merchants_datasetcomma_t).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 知道异常分数对于数据发生重大变化的情况至关重要。通过计算表中每条记录的分数，您可以创建将异常放入背景的可视化，如[图 4-7](#in_this_sample_merchants_datasetcomma_t)所示。
- en: '![In this sample merchants dataset, the data quality monitoring platform has
    detected an issue where the value fastfood has decreased significantly in the
    column merchantcategorycode. The anomaly score is compared to the anomaly score
    for other values in this table. You can see that there was simultaneously a significant
    increase in the value food. These two changes may be related, as suddenly fastfood
    records were misclassified as simply food.](assets/adqm_0407.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![在这个样本商户数据集中，数据质量监控平台检测到一个问题，在商户类别代码列中，fastfood 的值显著下降。异常分数与表中其他值的异常分数进行了比较。您可以看到，food
    的值同时显著增加。这两个变化可能相关，因为突然之间，fastfood 记录被错误地分类为 food。](assets/adqm_0407.png)'
- en: Figure 4-7\. In this sample merchants dataset, the data quality monitoring platform
    has detected an issue where the value `fastfood` has decreased significantly in
    the column `merchantcategorycode`. The anomaly score is compared to the anomaly
    score for other values in this table. You can see that there was simultaneously
    a significant increase in the value `food`. These two changes may be related,
    as suddenly `fastfood` records were misclassified as simply `food`. See a full-sized
    version of this image at [*https://oreil.ly/adqm_4_7*](https://oreil.ly/adqm_4_7).
  id: totrans-179
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 在这个样本商户数据集中，数据质量监控平台检测到一个问题：在`merchantcategorycode`列中，值为`fastfood`的数据显著减少。与该表中其他值的异常分数相比，这个异常分数是可以看到`food`值同时显著增加。这两个变化可能相关，因为`fastfood`记录突然被错误分类为简单的`food`。请在[*https://oreil.ly/adqm_4_7*](https://oreil.ly/adqm_4_7)上查看这幅图片的完整版本。
- en: 'As you can see in [Figure 4-7](#in_this_sample_merchants_datasetcomma_t), you
    can assign human-readable categories to anomaly scores to help with interpretation.
    Based on our experience working with a wide variety of datasets, we group anomaly
    scores into six different buckets, from minimal to extreme. These categories are
    based on the log of the overall anomaly score—every two buckets represents an
    order of magnitude increase in the score:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在[Figure 4-7](#in_this_sample_merchants_datasetcomma_t)中所见，您可以为异常分数分配人类可读的类别，以帮助解释。根据我们处理各种数据集的经验，我们将异常分数分为六个不同的桶，从最小到极端。这些类别基于整体异常分数的对数——每两个桶代表分数增加一个数量级：
- en: Minimal
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Minimal
- en: There is little to no significant change in the data.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 数据几乎没有显著变化。
- en: Weak
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Weak
- en: A small percentage of the data is affected by a change that requires explanation
    and careful study to detect.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一小部分数据受到需要解释和仔细研究以检测的变化的影响。
- en: Moderate
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Moderate
- en: A small percentage of the data is affected by an obvious change, or a moderate
    percentage is affected by a change that requires simple explanation.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的一小部分受到明显变化的影响，或者受到需要简单解释的变化的中等百分比影响。
- en: Strong
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Strong
- en: A significant percentage of the data is affected by an obvious change, or a
    majority of the data is affected by a change that is easily explained (though
    it may not be obvious at first glance).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的一大部分受到明显变化的影响，或者大多数数据受到容易解释的变化的影响（尽管一开始可能不明显）。
- en: Severe
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Severe
- en: A majority of the data is subject to a change that is obvious.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据受到明显变化的影响。
- en: Extreme
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Extreme
- en: There is a change that is obviously affecting almost the entirety of the data
    from today.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的数据明显受到影响的变化。
- en: You may notice a threshold in [Figure 4-7](#in_this_sample_merchants_datasetcomma_t)—it’s
    important to use the anomaly score for each table to learn a custom threshold
    for when to trigger an alert, as the data in some tables changes more frequently
    than in others. We’ll discuss this in [Chapter 5](ch05.html#building_a_model_that_works_on_real_wor).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能注意到[Figure 4-7](#in_this_sample_merchants_datasetcomma_t)中的一个阈值——使用每个表的异常分数来学习何时触发警报的自定义阈值非常重要，因为某些表中的数据变化比其他表频繁。我们将在[Chapter 5](ch05.html#building_a_model_that_works_on_real_wor)讨论这一点。
- en: Putting It Together with Pseudocode
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用伪代码综合一下
- en: The following Python pseudocode gives an example of how you could apply the
    approach described in this chapter to find anomalies between two days of data
    and summarize them by column. Don’t take this code too literally though; it is
    meant to simply illustrate the concepts and how they fit together at a high level.
    In particular, note that this ignores more complex issues like seasonality, multiple
    lookbacks, and correlated features and doesn’t implement the sampling, feature
    engineering, or anomaly score computation pieces in any detail.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的Python伪代码示例说明了如何应用本章描述的方法来找到两天数据之间的异常，并按列汇总它们。但不要太过字面地理解这段代码；它只是简单地说明了概念及其高层次的整合方式。特别地，请注意，它忽略了更复杂的问题，如季节性、多个回溯和相关特征，并且没有详细实现抽样、特征工程或异常分数计算部分。
- en: '[PRE2]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here we are defining a method in pseudocode that will detect anomalies in data
    by comparing samples from two different dates, training a model, and computing
    anomaly scores.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们定义了一个伪代码中的方法，通过比较两个不同日期的样本，训练模型并计算异常分数来检测数据中的异常。
- en: 'It accepts the following as parameters:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 它接受以下参数：
- en: '`table`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`table`'
- en: the name of the table to query
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 用来查询表的名称
- en: '`time_column`'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`time_column`'
- en: the name of the time column used to filter the data
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 用于过滤数据的时间列的名称
- en: '`current_date`'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`current_date`'
- en: the current date for which to sample data
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 用于采样数据的当前日期
- en: '`prior_date`'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`prior_date`'
- en: the prior date for which to sample data
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 用于采样数据的先前日期
- en: '`sample_size`'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`sample_size`'
- en: the number of rows to randomly sample for each date
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 每个日期随机抽样的行数
- en: It returns a dictionary where each key is a column name and each value is the
    column’s anomaly score.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回一个字典，其中每个键是列名，每个值是该列的异常分数。
- en: 'The next piece of the pseudocode implements the body of the method and takes
    us from the data sampling step all the way through to explaining the model’s predictions:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 伪代码的下一部分实现了方法的主体，并将我们从数据抽样步骤一直到解释模型预测的过程：
- en: '[PRE3]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Other Applications
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他应用
- en: We’ve focused on how unsupervised ML can help you detect sudden structural changes
    in your data on an ongoing basis. However, the ML approach outlined in this chapter
    has two additional use cases that are worth mentioning.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们专注于无监督机器学习如何帮助您在数据持续变化的情况下检测突发的结构性变化。然而，本章中概述的机器学习方法还有两个值得一提的用例。
- en: The first is finding *legacy* data quality issues, which will appear as shocks
    and scars in the history of your data. This can be done by running the algorithm
    outlined in this chapter on a sequence of historical dates and investigating the
    anomalies you find. In fact, in [Chapter 5](ch05.html#building_a_model_that_works_on_real_wor),
    we’ll outline how we use this process, which we call *backtesting*, to measure
    how effective our models are.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是发现*传统*数据质量问题，这些问题将出现在您数据历史中的震荡和疤痕中。可以通过在历史日期序列上运行本章概述的算法并调查发现的异常来完成此操作。事实上，在[第五章](ch05.html#building_a_model_that_works_on_real_wor)中，我们将概述我们如何使用这一过程（我们称之为*回测*）来衡量我们的模型的有效性。
- en: But beware, as this approach can come with some complications. The first is
    that you may find issues that are very difficult to explain. There are often changes
    that no one in the organization remembers, and validating whether they are concerning
    or not would require expensive and tedious detective work. The second complication
    is that you may be expecting to find some issues that simply aren’t there. This
    often happens when known data quality issues are fixed and a team backfills the
    data to repair the scar. Once that happens, you should no longer be able to detect
    the issue in the history of your data.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 但请注意，这种方法可能会带来一些复杂性。首先是您可能会发现非常难以解释的问题。组织中经常会发生没有人记得的变更，验证这些变更是否令人担忧将需要昂贵且繁琐的调查工作。第二个复杂性是，您可能期望找到一些根本不存在的问题。当已知的数据质量问题得到修复并且团队补充数据以修复缺陷后，您就不应该再在数据历史记录中检测到该问题。
- en: The second use case is more significant, and we will touch on it only briefly
    here. Instead of using unsupervised ML to compare data in the same table over
    time, you can instead compare two samples of data from the same table (or from
    different tables with the same column schema) to find meaningful differences between
    them.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个用例更为重要，我们将在这里简要提及。与其使用无监督机器学习来比较同一表中不同时间的数据，您可以比较同一表中（或具有相同列模式的不同表中）的两个数据样本，以找到它们之间的有意义差异。
- en: In this case, the unsupervised ML algorithm is going to detect, and help explain,
    any significant distribution or relationship differences between the two sets
    of data. Because it uses sampling, this approach can be applied to massive tables,
    and even to tables that reside in different source warehouses or databases!
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，无监督机器学习算法将检测并帮助解释两组数据之间的任何显著分布或关系差异。由于使用了抽样，这种方法可以应用于庞大的表格，甚至是位于不同源仓库或数据库中的表格！
- en: 'This allows for the following kinds of applications:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许进行以下类型的应用程序：
- en: Comparing raw data from a source database to cleansed and transformed data in
    the destination warehouse
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将源数据库中的原始数据与目标数据仓库中的清洗和转换数据进行比较
- en: Comparing the data from the current version of your ETL pipeline with the data
    produced by a new proposed version
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较当前ETL流水线版本的数据与新提议版本生成的数据
- en: Comparing a current sample of data to a sample from the distant past
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将当前数据样本与遥远过去的样本进行比较
- en: Comparing data from different business segments, geographies, product categories,
    or marketing campaigns
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较来自不同业务部门、地理位置、产品类别或营销活动的数据
- en: '[Figure 4-8](#using_unsupervised_mlcomma_a_data_quali) shows how a monitoring
    platform can expose this feature as a custom check that users can configure and
    run on demand to compare and contrast datasets of interest.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-8](#using_unsupervised_mlcomma_a_data_quali)展示了监控平台如何将此功能呈现为用户可以配置并按需运行的自定义检查，以比较和对比感兴趣的数据集。'
- en: Conclusion
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Whether you’re an experienced data scientist or new to machine learning, we
    hope this chapter has been a useful primer on how to build a model to detect sudden
    changes in the distribution of data from one day to the next. We’ve covered the
    overall concept, which relies on trying to build a classifier to predict whether
    a given row in the table is from today’s data. If you can do this, something has
    clearly changed about today’s data. You can use SHAP values to give individual
    rows in the table a score as to how much they helped the model make its determination.
    For the purposes of data quality monitoring, these scores can become indicators
    of how unusual those pieces of data are, and in what ways. This approach can even
    be extended to explain historical changes in your data or compare two SQL query
    result distributions.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您是经验丰富的数据科学家还是新手机器学习者，我们希望本章对构建模型以检测数据分布在一天之内的突变变得更加有用。我们已经涵盖了整体概念，该概念依赖于尝试构建分类器，以预测表中给定行是否来自今天的数据。如果您能够做到这一点，显然今天的数据发生了明显的变化。您可以使用
    SHAP 值为表中的个别行打分，以表明它们在多大程度上帮助模型做出决策。对于数据质量监控而言，这些分数可以成为数据异常性的指标，以及异常的方式。这种方法甚至可以扩展到解释数据的历史变化或比较两个
    SQL 查询结果的分布。
- en: '![Using unsupervised ML, a data quality monitoring platform can expose a check
    that allows users to compare two datasets.](assets/adqm_0408.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![使用无监督机器学习，数据质量监控平台可以揭示一项检查，允许用户比较两个数据集。](assets/adqm_0408.png)'
- en: Figure 4-8\. Using unsupervised ML, a data quality monitoring platform can expose
    a check that allows users to compare two datasets.
  id: totrans-227
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-8\. 使用无监督机器学习，数据质量监控平台可以揭示一项检查，允许用户比较两个数据集。
- en: The steps we’ve just described may sound simple in practice, but everything
    changes when you get into the details of working with real data. Real data has
    seasonal trends, contains correlations that you don’t want to treat as separate
    issues, and often gets updated in place without any indication—to name just a
    few hurdles. We’ll discuss these challenges, and how you can overcome them, in
    [Chapter 5](ch05.html#building_a_model_that_works_on_real_wor).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚描述的步骤在实践中可能听起来很简单，但一旦涉及到处理真实数据的细节，一切都会发生变化。真实数据具有季节性趋势，包含不希望将其视为独立问题的相关性，并且经常在原地更新而没有任何指示——这仅仅是其中的一些障碍。我们将讨论这些挑战，以及您如何克服它们，在[第五章](ch05.html#building_a_model_that_works_on_real_wor)中。
