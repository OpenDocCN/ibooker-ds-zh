- en: Chapter 64\. The Ethical Dilemma of Model Interpretability
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第64章  模型可解释性的伦理困境
- en: Grant Fleming
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Grant Fleming
- en: '![](Images/Grant_Fleming.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/Grant_Fleming.png)'
- en: Data Scientist, Elder Research Inc.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家，Elder Research Inc.
- en: Progress in data science is largely driven by the ever-improving predictive
    performance of increasingly complex “black-box” models. However, these predictive
    gains have come at the expense of losing the ability to interpret the relationships
    derived between the predictors and target(s) of a model, leading to misapplication
    and public controversy. These drawbacks reveal that *interpretability is actually
    an ethical issue*; data scientists should strive to implement additional interpretability
    methods that maintain predictive performance (model complexity) while also minimizing
    its harms.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学的进展在很大程度上是由越来越复杂的“黑盒”模型的预测性能不断改进推动的。然而，这些预测性能的提升是以失去解释模型预测器和目标之间关系的能力为代价的，这导致了误用和公众争议。这些缺点表明*可解释性实际上是一个伦理问题*；数据科学家应努力实施额外的可解释性方法，以维持预测性能（模型复杂性），同时最小化其危害。
- en: Any examination of the scholarly or popular literature on “AI” or “data science”
    makes apparent the profound importance placed on maximizing predictive performance.
    After all, recent breakthroughs in model design and the resulting improvements
    to predictive performance have led to models [exceeding doctors’ performance at
    detecting multiple medical issues](https://oreil.ly/yK67h) and [surpassing human
    reading comprehension](https://oreil.ly/PynnL). These breakthroughs have been
    made possible by transitioning from linear models to black-box models like Deep
    Neural Networks (DNN) and gradient-boosted trees (e.g., XGBoost). Instead of using
    linear transformations of features to generate predictions, these black-box models
    employ complex, nonlinear feature transformations to produce higher-fidelity predictions.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 任何对“AI”或“数据科学”的学术或流行文献的检视都明显显示出对最大化预测性能的深刻重视。毕竟，最近在模型设计方面的突破以及由此带来的预测性能改进已经使模型超过了医生在检测多种医学问题方面的表现，并且超过了人类的阅读理解能力。这些突破得益于从线性模型向深度神经网络（DNN）和梯度提升树（例如XGBoost）等黑盒模型的过渡。这些黑盒模型不再使用特征的线性变换来生成预测，而是采用复杂的非线性特征变换来产生更高保真度的预测结果。
- en: Because of the complex mathematics underlying them, these black-box models assume
    the role of oracle, producing predictions without providing human-interpretable
    explanations for their outputs. While these predictions are often more accurate
    than linear models, moving away from the built-in interpretability of linear models
    can pose challenges. For example, the inability to interpret the decision rules
    of the model can make it harder to gain the trust of users, clients, and regulators,
    even for models that are otherwise well designed and effective.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其背后复杂的数学原理，这些黑盒模型扮演了神谕的角色，能够在不提供可解释的人类解释的情况下产生预测结果。尽管这些预测通常比线性模型更准确，但远离线性模型内置的可解释性可能会带来挑战。例如，无法解释模型的决策规则可能会使得用户、客户和监管机构难以建立信任，即使这些模型在其他方面设计良好且有效。
- en: Forgoing model interpretability also presents an ethical dilemma for the sciences.
    In improving our ability to predict the state of the world, black-box models have
    traded away part of their ability to help us *understand* the reasoning motivating
    those predictions. Entire subfields of economics, medicine, and psychology have
    predicated their existence on successfully translating linear model interpretations
    into policy prescriptions. For these tasks, predictive performance is often secondary
    to exploring the relationships created by the model between its predictors and
    targets. Focusing solely on predictive performance would have neutered our understanding
    in these fields and may prevent future discoveries that would have otherwise been
    drawn out of more transparent models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 放弃模型可解释性也为科学界带来了伦理困境。通过提高我们预测世界状态的能力，黑盒模型牺牲了一部分帮助我们*理解*这些预测背后推理的能力。经济学、医学和心理学的整个子领域依赖于成功将线性模型解释转化为政策建议。对于这些任务来说，预测性能通常次于探索模型在其预测器和目标之间创建的关系。仅关注预测性能可能会削弱我们在这些领域的理解，甚至可能阻止本应从更透明的模型中获得的未来发现。
- en: Outside of public policy and science, forgoing model interpretability has posed
    more direct challenges. Misapplied black-box models within [health care](https://oreil.ly/OLcSU),
    [the legal system](https://oreil.ly/Mi7MW), and corporate [hiring processes](https://oreil.ly/HFLet)
    have unintentionally harmed both the people and the organizations that they were
    built to serve. In these cases, the predictions of the black boxes were clearly
    inaccurate; however, debugging and detecting potential issues prior to deployment
    was either difficult or impossible given the nature of the models. Such cases
    have understandably led to public controversy about the ethics of data science
    as well as [calls for stronger regulation](https://oreil.ly/OzS46) around algorithmic
    data collection, transparency, and fairness.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在公共政策和科学领域之外，放弃模型可解释性带来了更直接的挑战。在[健康保健](https://oreil.ly/OLcSU)、[法律系统](https://oreil.ly/Mi7MW)和企业[招聘流程](https://oreil.ly/HFLet)中，错误应用的黑盒模型无意中伤害了它们本应服务的人群和组织。在这些案例中，黑盒的预测显然是不准确的；然而，鉴于模型的本质，难以在部署前调试和检测潜在问题。这些情况理所当然地引发了公众对数据科学伦理的争议，同时也引发了对算法数据收集、透明度和公平性的[更强监管要求](https://oreil.ly/OzS46)。
- en: Balancing complexity and model interpretability is clearly a challenge. Fortunately,
    there are several *interpretability methods* that allow data scientists to understand,
    to an extent, the inner workings of complex black-box models that are otherwise
    unknowable. Applying these methods can allow for maintaining the improved predictive
    performance of arbitrary black-box models while gaining back much of the interpretability
    lost by moving away from linear models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡复杂性和模型可解释性显然是一个挑战。幸运的是，有几种*可解释性方法*允许数据科学家在一定程度上理解复杂黑盒模型的内部运作，否则这些是无法知晓的。应用这些方法可以在保持任意黑盒模型改进的预测性能的同时，恢复部分线性模型移开时失去的可解释性。
- en: Individual interpretability methods can serve a wide variety of functions. For
    example, global interpretability methods such as partial dependence plots (PDPs)
    can provide diagnostic visualizations for the average impact of features on predictions.
    The plots depict quantitative relationships between the input and output features
    of black-box models and allow for human interpretations similar to how coefficients
    from a linear model might be used. Local methods like Shapley values can produce
    explanations for the impacts of specific feature values on individual predictions,
    increasing user trust by showing how the model relies on specific features. Model
    debugging efforts are also made simpler by the increased insight that these methods
    allow, indicating opportunities for increasing the performance even of black-box
    models that may already perform well.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 个体解释性方法可以服务于各种功能。例如，全局解释性方法如部分依赖图（PDPs）可以提供对特征对预测影响的诊断可视化。这些图表描述了黑盒模型输入和输出特征之间的数量关系，并允许类似于线性模型系数如何使用的人类解释。像Shapley值这样的局部方法可以为个体预测的特定特征值的影响提供解释，通过展示模型依赖于特定特征来增加用户信任。这些方法提供的增强的洞察力也使模型调试工作变得更加简单，表明即使已经表现良好的黑盒模型也存在提升性能的机会。
- en: Ethical data science surely encompasses more than just being able to interpret
    the inner functioning and outputs of a model. However, the case for why model
    interpretability should be a part of ethical best practices is compelling. Data
    scientists integrating interpretability methods into their black-box models are
    improving the ethical due diligence of their work; it is how one can maintain
    model interpretability while still leveraging the great potential of black-box
    models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 道德数据科学当然不仅仅是能够解释模型的内部功能和输出。然而，为何模型可解释性应成为道德最佳实践的一部分的论据是有说服力的。将解释性方法整合到黑盒模型中的数据科学家正在提升其工作的道德尽职调查；这是保持模型可解释性的同时利用黑盒模型巨大潜力的方法。
