- en: 2 Why the Pod?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 为什么需要 Pod？
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: What is a Pod?
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 Pod？
- en: An example web app and why we need the Pod
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个示例 Web 应用程序以及为什么我们需要 Pod
- en: How Kubernetes is built for Pods
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 是如何为 Pods 构建的
- en: The Kubernetes control plane
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 控制平面
- en: In the previous chapter, we provided a high-level overview of Kubernetes and
    an introduction to its features, core components, and architecture. We also showcased
    a couple of business use cases and outlined some container definitions. The Kubernetes
    Pod abstraction for running thousands of containers in a flexible manner has been
    a fundamental part of the transition to containers in enterprises. In this chapter,
    we will cover the Pod and how Kubernetes was built to support it as a basic application
    building block.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们提供了一个关于 Kubernetes 的高级概述，以及对其特性、核心组件和架构的介绍。我们还展示了几个商业用例，并概述了一些容器定义。Kubernetes
    Pod 抽象在灵活运行数千个容器方面已经成为企业向容器化过渡的基本部分。在本章中，我们将介绍 Pod 以及 Kubernetes 如何作为基本应用程序构建块来支持它。
- en: As briefly mentioned in chapter 1, a *Pod* is an object that is defined within
    the Kubernetes API, as are the majority of things in Kubernetes. The Pod is the
    smallest atomic unit that can be deployed to a Kubernetes cluster, and Kubernetes
    is built around the Pod definition. The Pod (figure 2.1) allows us to define an
    object that can include multiple containers, which allows Kubernetes to create
    one or more containers hosted on a node.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如第 1 章简要提到的，*Pod* 是在 Kubernetes API 中定义的对象，在 Kubernetes 中大多数事物也是如此。Pod 是可以部署到
    Kubernetes 集群中的最小原子单元，Kubernetes 是围绕 Pod 定义构建的。Pod（图 2.1）允许我们定义一个可以包含多个容器的对象，这使得
    Kubernetes 能够在节点上创建一个或多个容器。
- en: '![](../Images/CH02_F01_Love.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1 Pod](../Images/CH02_F01_Love.png)'
- en: Figure 2.1 A Pod
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 Pod
- en: Many other Kubernetes API objects either use Pods directly or are API objects
    that support Pods. A Deployment object, for example, uses Pods, as well as StatefulSets
    and DaemonSets. Several different higher-level Kubernetes controllers create and
    manage Pod life cycles. Controllers are software components that run on the control
    plane. Examples of built-in controllers include the controller manager, the cloud
    manager, and the scheduler. But first, let’s digress by laying out a web application
    and then loop that back to Kubernetes, the Pod, and the control plane.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 许多其他 Kubernetes API 对象要么直接使用 Pod，要么是支持 Pod 的 API 对象。例如，Deployment 对象就使用了 Pod，以及
    StatefulSets 和 DaemonSets。几个不同的高级 Kubernetes 控制器创建和管理 Pod 生命周期。控制器是运行在控制平面上的软件组件。内置控制器的例子包括控制器管理器、云管理器和调度器。但首先，让我们先偏离一下主题，先描述一个
    Web 应用程序，然后再将其与 Kubernetes、Pod 和控制平面联系起来。
- en: note You may notice that we use the control plane to define the group of nodes
    that run the controller, the controller manager, and the scheduler. They are also
    referred to as *masters*, but in this book, we will use *control plane* when talking
    about these components.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: note 你可能会注意到，我们使用控制平面来定义运行控制器、控制器管理器和调度器的节点组。它们也被称为 *masters*，但在本书中，当我们谈论这些组件时，我们将使用
    *control plane*。
- en: 2.1 An example web application
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 一个示例 Web 应用程序
- en: Let’s walk through an example web application to understand why we need a Pod
    and how Kubernetes is built to support Pods and containerized applications. In
    order to get a better understanding of why the Pod, we will use the following
    example throughout much of this chapter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个示例 Web 应用程序来了解为什么我们需要 Pod 以及 Kubernetes 是如何构建来支持 Pod 和容器化应用程序的。为了更好地理解为什么需要
    Pod，我们将在本章的大部分内容中使用以下示例。
- en: 'The Zeus Zap energy drink company has an online website that allows consumers
    to purchase their different lines of carbonated beverages. This website consists
    of three different layers: a user interface (UI), a middle tier (various microservices),
    and a backend database. They also have messaging and queuing protocols. A company
    like Zeus Zap usually has various web frontends that include consumer-facing as
    well as administrative ones, different microservices that compose the middle tier,
    and one or more backend databases. Here is a breakdown of one slice of Zeus Zap’s
    web application (figure 2.2):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Zeus Zap 能量饮料公司有一个在线网站，允许消费者购买他们不同系列的碳酸饮料。该网站由三个不同的层组成：用户界面（UI）、中间层（各种微服务）和后端数据库。他们还有消息和排队协议。像
    Zeus Zap 这样的公司通常有各种面向消费者的前端以及管理端，中间层的不同微服务，以及一个或多个后端数据库。以下是 Zeus Zap 的 Web 应用程序的一个切片分解（图
    2.2）：
- en: A JavaScript frontend served up by NGINX
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由 NGINX 提供的 JavaScript 前端
- en: Two web-controller layers that are Python microservices hosted with Django
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个以 Django 托管的 Python 微服务为主控层的网络控制器
- en: A backend CockroachDB on port 6379, backed by storage
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在端口 6379 上运行的后端 CockroachDB，由存储支持
- en: 'Now, let’s imagine that they run these applications in four distinct containers
    in a production setting. Then they can start the app using these `docker run`
    commands:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们想象他们在生产环境中以四个不同的容器运行这些应用程序。然后他们可以使用以下 `docker run` 命令启动应用程序：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once these services are up and running, the company quickly comes to a few
    realizations:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这些服务启动并运行，公司很快就会意识到以下几点：
- en: They cannot run multiple copies of the UI container unless they load balance
    in front of port 80 because there is only one port 80 on the host machine their
    image is running.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除非他们在端口 80 前进行负载均衡，否则他们不能运行 UI 容器的多个副本，因为他们的镜像运行的主机机器上只有一个端口 80。
- en: They cannot migrate the CockroachDB container to a different server unless the
    IP address is modified and injected into the web app (or they add a DNS server
    that is dynamically updated when the CockroachDB container moves).
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除非修改 IP 地址并将其注入到网络应用程序中（或者他们添加一个在 CockroachDB 容器移动时动态更新的 DNS 服务器），否则他们不能将 CockroachDB
    容器迁移到不同的服务器。
- en: They need to run each CockroachDB instance on a separate server to result in
    high availability.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们需要在单独的服务器上运行每个 CockroachDB 实例，以实现高可用性。
- en: If a CockroachDB instance dies on one server, they need a way to move its data
    to a new node and reclaim the unused storage space.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个 CockroachDB 实例在某个服务器上死亡，他们需要一种方法将数据移动到新的节点，并回收未使用的存储空间。
- en: Zeus Zap also realizes that a few requirements for a container orchestration
    platform exist. These include
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Zeus Zap 也意识到一个容器编排平台存在一些需求。这些包括
- en: Shared networking between hundreds of processes all binding to the same port
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数百个进程共享网络，所有进程都绑定到相同的端口
- en: Migration and decoupling of storage volumes from binaries while avoiding dirtying
    up local disks
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在避免污染本地磁盘的同时，将存储卷从二进制文件迁移和解耦
- en: Optimizing the utilization of available CPU and memory resources to achieve
    cost savings
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化可用 CPU 和内存资源的使用，以实现成本节约
- en: 'Note Running more processes on a server often results in the *noisy neighbor*
    phenomenon: crowding applications leads to over-competition for scarce resources
    (CPU, memory). A system must mitigate noisy neighbors.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在服务器上运行更多进程通常会导致 *嘈杂邻居* 现象：拥挤的应用程序会导致对稀缺资源（CPU、内存）的过度竞争。系统必须减轻嘈杂邻居的影响。
- en: 'Containerized applications running at large scale (or even small scale) require
    a higher level of awareness when it comes to scheduling services and managing
    load balancers. Therefore, the following items are also required:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在调度服务和管理工作负载均衡器时，大规模（或甚至小规模）运行的容器化应用程序需要更高层次的认识。因此，还需要以下项目：
- en: '*Storage-aware scheduling*—To schedule a process in concert with making its
    data available'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*存储感知调度*——在使数据可用时与调度进程协同'
- en: '*Service-aware network load balancing*—To send traffic to different IP addresses
    as containers move from one machine to another'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*服务感知的网络负载均衡*——当容器从一个机器移动到另一个机器时，将流量发送到不同的 IP 地址'
- en: '![](../Images/CH02_F02_Love.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F02_Love.png)'
- en: Figure 2.2 The Zeus Zap web architecture
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 Zeus Zap 网络架构
- en: The revelations just shared in our application resounded equally with the founders
    of the distributed scheduling and orchestration tools of the 2000s, including
    Mesos and Borg. Borg is Google’s internal container orchestration system, and
    Mesos is an open source application, both of which provide cluster management
    and predate Kubernetes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在应用程序中刚刚分享的启示与 2000 年代分布式调度和编排工具的创始人产生了共鸣，包括 Mesos 和 Borg。Borg 是谷歌的内部容器编排系统，Mesos
    是一个开源应用程序，两者都提供集群管理，并且早于 Kubernetes。
- en: 2.1.1 Infrastructure for our web application
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 我们网络应用程序的基础设施
- en: Without container orchestration software such as Kubernetes, organizations need
    many components in their infrastructure. In order to run an application, you need
    various virtual machines (VMs) on the cloud or physical computers that act as
    your servers, and as mentioned before, you need stable identifiers to locate services.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 没有像 Kubernetes 这样的容器编排软件，组织在其基础设施中需要许多组件。为了运行一个应用程序，您需要在云上或物理计算机上使用各种虚拟机（VM），这些计算机充当您的服务器，正如之前提到的，您需要稳定的标识符来定位服务。
- en: Server workloads can vary. For instance, you may need servers with more memory
    to run the database, or you may need a system with lower memory but more CPUs
    for the microservices. Also, you might need low-latency storage for a database
    like MySQL or Postgres, but slower storage for backups and other applications
    that usually load data into memory and then never touch the disk again. Additionally,
    your continuous integration servers like Jenkins or CircleCI require full access
    to your servers, but your monitoring system requires read-only access to some
    of your applications. Now, add in human authorization and authentication as well.
    To summarize, you will need
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器负载可能有所不同。例如，你可能需要更多内存的服务器来运行数据库，或者你可能需要一个内存较低但 CPU 更多的系统来运行微服务。此外，你可能需要一个低延迟的存储系统，如
    MySQL 或 Postgres，但对于备份和其他通常将数据加载到内存中然后不再接触磁盘的应用程序，你可能需要一个更慢的存储系统。此外，你的持续集成服务器，如
    Jenkins 或 CircleCI，需要对你的服务器有完全访问权限，但你的监控系统需要对你的一些应用程序有只读访问权限。现在，再加上人类授权和身份验证。总之，你需要
- en: A VM or physical server as a deployment platform
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为部署平台的虚拟机或物理服务器
- en: Load balancing
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡
- en: Application discovery
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序发现
- en: Storage
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储
- en: A security system
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个安全系统
- en: 'In order to sustain the system, your DevOps staff would need to maintain the
    following (in addition to many more subsystems):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了维持系统，你的 DevOps 团队需要维护以下内容（除了许多其他子系统）：
- en: Centralized logging
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集中日志
- en: Monitoring, alerting, and metrics
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控、警报和指标
- en: Continuous integration/continuous delivery (CI/CD) system
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续集成/持续交付 (CI/CD) 系统
- en: Backups
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 备份
- en: Secrets management
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密钥管理
- en: 'In contrast to most home-grown application delivery platforms, Kubernetes comes
    with built-in log rotation, inspection, and management tooling out of the box.
    Next come the business challenges: the operational requirements.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数自建的应用程序交付平台不同，Kubernetes 出厂就自带日志轮转、检查和管理工具。接下来是业务挑战：运营需求。
- en: 2.1.2 Operational requirements
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 运营需求
- en: The Zeus Zap energy drink company does not have a typical seasonal growth period
    like most online retailers, but they do sponsor various e-sporting events that
    drive in a lot of traffic. This is because the marketing department and various
    online gaming streamers run contests that are promoted during these events. These
    online user traffic patterns give the DevOps team one of the most challenging
    patterns to manage—burst traffic. Scaling online applications is a difficult problem
    to maintain and solve, and now the team has to schedule for burst patterns! Also,
    due to the online social media campaigns created around the e-sporting events,
    the business is concerned about outages. The costs for downtimes are ugly and
    huge.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Zeus Zap 能量饮料公司没有像大多数在线零售商那样的典型季节性增长期，但他们确实赞助了各种电子竞技活动，吸引了大量流量。这是因为市场营销部门和各种在线游戏直播员在这些活动中举办了比赛。这些在线用户流量模式给
    DevOps 团队带来了最具挑战性的流量模式之一——突发流量。维护和解决在线应用程序的扩展问题是一个困难的问题，现在团队必须为突发模式进行调度！此外，由于围绕电子竞技活动创建的在线社交媒体活动，公司担心停机。停机成本既难看又庞大。
- en: According to Gartner from a study in 2018 ([http://mng.bz/PWNn](http://mng.bz/PWNn)),
    the average cost of IT downtime is $5,600 per minute, taking into account the
    differences between businesses. It is not uncommon for an application to have
    two hours of downtime, resulting in an average cost of $672,000\. Money is one
    thing, but what about the human cost? DevOps engineers face outages; it is a part
    of life, but it wears on the staff, as well, and can lead to burn out. Employee
    burnout in the U.S. costs industries approximately $125 to $190 billion dollars
    annually ([http://mng.bz/4j6j](http://mng.bz/4j6j)).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Gartner 在 2018 年的一项研究 ([http://mng.bz/PWNn](http://mng.bz/PWNn))，考虑到不同企业的差异，平均每分钟的
    IT 停机成本为 5,600 美元。一个应用程序出现两小时的停机时间并不罕见，导致平均成本为 672,000 美元。钱是一回事，但人的成本呢？DevOps
    工程师面临停机，这是生活的一部分，但它也会消耗员工，并可能导致燃尽。美国员工的燃尽每年给行业造成约 1250 亿美元至 1900 亿美元的成本 ([http://mng.bz/4j6j](http://mng.bz/4j6j))。
- en: Many companies need some level of high availability and rollback in their production
    systems. These requirements go hand in hand with the need for redundancy of applications
    and hardware. However, to save on costs, these same companies may want to scale
    application availability up and down during less demanding time periods. Thus,
    cost management is often antagonistic to broader business requirements around
    uptime. To recap, a simple web application needs
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司在他们的生产系统中需要一定级别的高可用性和回滚功能。这些需求与对应用程序和硬件冗余的需求相辅相成。然而，为了节省成本，这些公司可能希望在需求较低的时间段内上下调整应用程序的可用性。因此，成本管理通常与关于正常运行时间的更广泛业务需求相矛盾。总结一下，一个简单的Web应用程序需要
- en: Scaling
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩缩
- en: High availability
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高可用性
- en: Versioning applications to allow rollbacks
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对应用程序进行版本控制以允许回滚
- en: Cost management
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本管理
- en: 2.2 What is a Pod?
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 什么是Pod？
- en: 'Roughly, a *Pod* is one or more OCI images that run as containers on a Kubernetes
    cluster node. The Kubernetes *node* is a single piece of computing power (a server)
    that runs a kubelet. Like everything else in Kubernetes, a Node is also an API
    object. Deploying a Pod is as simple as issuing the following:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 大概来说，一个*Pod*是在Kubernetes集群节点上作为容器运行的一个或多个OCI镜像。Kubernetes的*节点*是单个计算能力（一个服务器），它运行kubelet。像Kubernetes中的其他一切一样，节点也是一个API对象。部署Pod就像发出以下命令一样简单：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ The API version ID that matches a version on the API server
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 与API服务器上的版本匹配的API版本ID
- en: ❷ kind declares the type of API object (in this case, a Pod) for the API server.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ kind声明API对象的类型（在这种情况下，一个Pod）以供API服务器使用。
- en: ❸ Names the image in the registry
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在注册表中命名镜像
- en: ❹ The kubectl command
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ kubectl命令
- en: The previous syntax runs using the Linux Bash shell and the `kubectl` command.
    The `kubectl` command is the binary that provides a command-line interface to
    work with the Kubernetes API server.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的语法使用Linux Bash shell和`kubectl`命令运行。`kubectl`命令是提供命令行界面以与Kubernetes API服务器一起工作的二进制文件。
- en: 'Pods aren’t deployed directly in most cases. Instead, they are automatically
    created for us by other API objects such as Deployments, Jobs, StatefulSets, and
    DaemonSets that we define:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，Pod不是直接部署的。相反，它们由我们定义的其他API对象（如Deployments、Jobs、StatefulSets和DaemonSets）自动为我们创建：
- en: '*Deployments*—The most commonly used API object in a Kubernetes cluster. They
    are the typical API object that, say, deploys a microservice.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*部署*——在Kubernetes集群中最常用的API对象。它们是典型的API对象，比如部署一个微服务。'
- en: '*Jobs*—Run a Pod as a batch process.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*作业*——以批处理方式运行Pod。'
- en: '*StatefulSets*—Host applications that require specific needs and that are often
    stateful applications like databases.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*StatefulSets*——托管需要特定需求且通常是具有状态的应用程序（如数据库）。'
- en: '*DaemonSets*—Used when we want to run a single Pod as an “agent” on every node
    of a cluster (commonly used for system services involving networking, storage,
    or logging).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DaemonSets*——当我们希望在集群的每个节点上运行单个Pod作为“代理”时使用（通常用于涉及网络、存储或日志的系统服务）。'
- en: 'The following is a list of StatefulSet features:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个StatefulSet功能的列表：
- en: Ordinal Pod naming to get unique network identifiers
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用序号Pod命名以获取唯一的网络标识符
- en: Persistent storage that is always mounted to the same Pod
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总是挂载到相同Pod的持久存储
- en: Ordered starting, scaling, and updating
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有序启动、扩缩和更新
- en: tip Docker image names support using a label called *latest*. Do not use the
    image name mycontainerregistry.io/foo in production because this pulls the `latest`
    tag from the registry, which is the latest version of the image. Always use a
    versioned tag name, not latest, or even better, an SHA to install an image. Image
    tag names are not immutable but an image SHA is. Many production systems fail
    because a newer version of a container is inadvertently installed. Friends don’t
    let friends run *latest*!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：Docker镜像名称支持使用一个名为*latest*的标签。在生产环境中不要使用mycontainerregistry.io/foo这样的镜像名称，因为这会从注册表中拉取`latest`标签，这是镜像的最新版本。始终使用版本化的标签名称，而不是latest，甚至更好的是使用SHA来安装镜像。镜像标签名称不是不可变的，但镜像SHA是。许多生产系统失败是因为意外安装了容器的新版本。朋友们不要让朋友们运行*latest*！
- en: When the Pod is started, you can view the Pod running in the default Namespace
    with a simple `kubectl get po` command. Now that we’ve created a running container,
    it is a simple matter to deploy the components in the Zeus Zap web application
    (figure 2.3). Simply use a favorite image tool like Docker or CRI-O to bundle
    the various binaries and their dependencies into different images, which are just
    tarballs with some file definitions. In the next chapter, we will cover how to
    make your own images and Pods manually.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当Pod启动时，可以使用简单的`kubectl get po`命令查看在默认命名空间中运行的Pod。现在我们已经创建了一个运行的容器，部署宙斯Zap Web应用程序的组件（图2.3）就变得简单了。只需使用Docker或CRI-O等喜欢的镜像工具将各种二进制文件及其依赖项捆绑到不同的镜像中，这些镜像只是带有一些文件定义的tar包。在下一章中，我们将介绍如何手动制作自己的镜像和Pod。
- en: Instead of having the system schedule various `docker run` commands when a server
    starts, we defined four higher-level API objects that create Pods and that call
    the Kubernetes API server. As we mentioned, Pods are rarely used to install applications
    on Kubernetes. Users typically use higher-level abstractions like Deployments
    and StatefulSets. But we still loop back to the Pod because Deployments and StatefulSets
    create replica objects, which then create Pods.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器启动时，系统不会调度各种`docker run`命令，我们定义了四个高级API对象，用于创建Pod并调用Kubernetes API服务器。正如我们提到的，Pod很少用于在Kubernetes上安装应用程序。用户通常使用更高层次的抽象，如Deployments和StatefulSets。但我们仍然回到Pod，因为Deployments和StatefulSets创建副本对象，然后创建Pod。
- en: '![](../Images/CH02_F03_Love.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F03_Love.png)'
- en: Figure 2.3 The Zeus Zap application running on Kubernetes
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 在Kubernetes上运行的宙斯Zap应用
- en: 2.2.1 A bunch of Linux namespaces
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 一系列Linux命名空间
- en: 'Kubernetes Namespaces (the ones you make with `kubectl` `create` `ns`) are
    not the same as Linux namespaces. Linux namespaces are a Linux kernel feature
    that allows for process separation inside the kernel. Pods, at a base level, are
    a bunch of namespaces in a specific configuration. A Pod has the following Linux
    namespaces:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes命名空间（使用`kubectl create ns`创建的）与Linux命名空间不同。Linux命名空间是Linux内核的一个特性，允许在内核内部进行进程分离。在基本层面上，Pod是一系列特定配置的命名空间。Pod具有以下Linux命名空间：
- en: One or more PID namespaces
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个或多个PID命名空间
- en: A single networking namespace
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个网络命名空间
- en: '`IPC` namespace'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IPC` 命名空间'
- en: '`cgroup` (control group) namespace'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cgroup`（控制组）命名空间'
- en: '`mnt` (mount) namespace'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mnt`（挂载）命名空间'
- en: '`user` (user ID) namespace'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`user`（用户ID）命名空间'
- en: Linux namespaces are Linux kernel filesystem components that provide the base
    functionality to take an image and create a running container. Why is this important?
    Let’s loop back to a couple of the requirements to run the example web app.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Linux命名空间是Linux内核文件系统组件，提供从镜像创建运行容器的基本功能。为什么这很重要？让我们回到运行示例Web应用的一些要求。
- en: What is essential is the capability to scale. The Pod does not just give us
    and Kubernetes the ability to deploy a container, but it allows us to also scale
    the capability to handle more traffic. What is required to cut costs and to scale
    vertically is the ability to adjust resource settings for a container. In order
    for the Zeus Zap microservices to communicate with the CockroachDB server, a definitive
    networking and service lookup needs to be deployed.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是具备扩展的能力。Pod不仅为我们和Kubernetes提供了部署容器的能力，而且还允许我们扩展处理更多流量的能力。为了降低成本和垂直扩展，需要调整容器资源设置的能力。为了使宙斯Zap微服务能够与CockroachDB服务器通信，需要部署明确的网络和服务查找。
- en: The Pod and the basis of the Pod, Linux namespaces, provide the support for
    all of these features. Within the networking namespace, a virtual networking stack
    exists that is connected into a software-defined networking (SDN) system that
    spans a Kubernetes cluster. The need to scale is often facilitated via load balancing
    multiple Pods for the application. The SDN within a Kubernetes cluster is the
    networking framework that supports load balancing.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Pod及其基础，Linux命名空间，为所有这些功能提供支持。在网络命名空间内，存在一个虚拟网络栈，该栈连接到一个跨越Kubernetes集群的软件定义网络（SDN）系统。通常通过为应用程序的多个Pod进行负载均衡来促进扩展需求。Kubernetes集群内的SDN是支持负载均衡的网络框架。
- en: 2.2.2 Kubernetes, infrastructure, and the Pod
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 Kubernetes、基础设施和Pod
- en: 'Servers are dependent on running Kubernetes and Pods. As a unit of compute,
    a unit of CPU power is represented by an API object (the Node) within Kubernetes.
    A node can run on a multitude of platforms, but it is simply a server with defined
    components. Node requirements include the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器依赖于运行 Kubernetes 和 Pods。作为一个计算单元，一个 CPU 功率单元在 Kubernetes 中由一个 API 对象（节点）表示。节点可以在多种平台上运行，但它只是一个具有定义组件的服务器。节点要求包括以下内容：
- en: A server
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器
- en: An installed operating system (OS) with a variety of Linux and Windows-supported
    requirements
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个安装了操作系统的服务器，具有各种 Linux 和 Windows 支持的要求
- en: systemd (a Linux system and service manager)
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: systemd（一个 Linux 系统和服务管理器）
- en: The kubelet (a node agent)
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kubelet（一个节点代理）
- en: The container runtime (such as a Docker engine)
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器运行时（如 Docker 引擎）
- en: A network proxy (`kube-proxy`) that handles Kubernetes Services
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个网络代理（`kube-proxy`），用于处理 Kubernetes 服务
- en: A CNI (Container Network Interface) provider
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 CNI（容器网络接口）提供者
- en: A node can run on a Raspberry Pi, a VM in the cloud, or a multitude of other
    platforms. Figure 2.4 displays what components form a node running on Linux.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 节点可以在树莓派、云中的虚拟机或多种其他平台上运行。图 2.4 显示了在 Linux 上运行的节点由哪些组件组成。
- en: '![](../Images/CH02_F04_Love.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F04_Love.png)'
- en: Figure 2.4 A node
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 一个节点
- en: The *kubelet* is a binary program that runs as an agent, communicating with
    the Kubernetes API server via multiple control loops. It runs on every node; without
    it, a Kubernetes node is not schedulable or considered to be a part of a cluster.
    Understanding the kubelet helps us to diagnose low-level problems with such things
    as a node not joining a cluster and a Pod not deploying. The kubelet ensures that
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*kubelet* 是一个作为代理运行的二进制程序，通过多个控制循环与 Kubernetes API 服务器通信。它在每个节点上运行；没有它，Kubernetes
    节点就无法调度，也不被视为集群的一部分。理解 kubelet 有助于我们诊断节点无法加入集群和 Pod 无法部署等低级问题。kubelet 确保以下内容：'
- en: Any Pod scheduled to a kubelet’s host is running via a control loop that watches
    which Pods are scheduled to which nodes.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何调度到 kubelet 主机的 Pod 都是通过一个控制循环运行的，该循环监视哪些 Pods 被调度到哪些节点。
- en: The API server is continually made aware that the kubelet on a node is healthy
    via a heartbeat in Kubernetes 1.17+. (The heartbeat is maintained by looking at
    the `kube-node-lease` namespace of a running cluster.)
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API 服务器通过 Kubernetes 1.17+ 中的心跳不断了解节点上的 kubelet 是否健康。（心跳是通过查看运行集群的 `kube-node-lease`
    命名空间来维护的。）
- en: Garbage is collected as needed for Pods, which includes ephemeral storage or
    network devices.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 需要时进行垃圾回收，包括临时存储或网络设备。
- en: However, the kubelet is incapable of doing any real work without both a CNI
    provider and a container runtime accessible via a Container Runtime Interface
    (CRI). CNI ultimately serves the needs of the CRI, which then starts and stops
    containers. The kubelet utilizes CRI and CNI to reconcile the state of a node
    with the state of the control plane. For example, when the control plane decides
    that NGINX will run on nodes two, three, and four of a five-node cluster, it is
    the kubelet’s job to make sure the CRI provider pulls down this container from
    an image registry and runs with an IP address in the `podCIDR` range. This book
    will cover how those decisions are made in chapter 9\. When mentioning CRI, it
    is essential to have a container engine to start the containers. Common container
    engines include Docker engine, CRI-O, and LXC.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，kubelet 没有CNI提供者和通过容器运行时接口（CRI）可访问的容器运行时，就无法进行任何实际工作。CNI 最终服务于 CRI 的需求，然后启动和停止容器。kubelet
    利用 CRI 和 CNI 来协调节点状态和控制平面状态。例如，当控制平面决定 NGINX 将在五节点集群的第二个、第三个和第四个节点上运行时，kubelet
    的任务是确保 CRI 提供者从镜像仓库拉取此容器，并在 `podCIDR` 范围内的 IP 地址上运行。本书将在第 9 章中介绍这些决策是如何做出的。当提到
    CRI 时，必须有一个容器引擎来启动容器。常见的容器引擎包括 Docker 引擎、CRI-O 和 LXC。
- en: '*Service* is an API object defined by Kubernetes. The Kubernetes network proxy
    binary (`kube-proxy`) handles the creation of the ClusterIP and NodePort Services
    on every node. The different types of Services include:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*服务* 是 Kubernetes 定义的一个 API 对象。Kubernetes 网络代理二进制文件（`kube-proxy`）负责在每个节点上创建
    ClusterIP 和 NodePort 服务。不同类型的 Service 包括：'
- en: '*ClusterIP*—An internal Service that load balances Kubernetes Pods'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ClusterIP*—一个内部服务，用于负载均衡 Kubernetes Pods'
- en: '*NodePort*—An open port on a Kubernetes node that load balances multiple Pods'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*NodePort*—Kubernetes 节点上用于负载均衡多个 Pods 的开放端口'
- en: '*LoadBalancer*—An external Service that creates a load balancer external to
    the cluster'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LoadBalancer*—一个外部服务，在集群外部创建负载均衡器'
- en: The Kubernetes network proxy may or may not be installed as some network providers
    replace it with their own network component that handles service management. Kubernetes
    permits multiple Pods of the same type to be proxied by a Service. For this to
    work, every node in a cluster has to be aware of every Service and every Pod.
    The Kubernetes network proxy facilitates and manages each service on each node
    as defined for a specific cluster. It supports TCP, UDP, and STCP network protocols,
    as well as forwarding and load balancing.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes网络代理可能已安装或未安装，因为一些网络提供商用他们自己的网络组件替换了它，该组件处理服务管理。Kubernetes允许多个相同类型的Pod通过一个服务进行代理。为了使这成为可能，集群中的每个节点都必须知道每个服务和每个Pod。Kubernetes网络代理简化并管理每个节点上每个服务，正如为特定集群定义的那样。它支持TCP、UDP和STCP网络协议，以及转发和负载均衡。
- en: Note Kubernetes networking is provided via a software solution called a CNI
    provider. Some CNI providers are building components that replace the Kubernetes
    network proxy with their own software infrastructure. That way they can undergo
    different networking without the use of iptables.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Kubernetes的网络是通过一个名为CNI提供者的软件解决方案提供的。一些CNI提供者正在构建组件，用他们自己的软件基础设施替换Kubernetes网络代理。这样，他们可以在不使用iptables的情况下进行不同的网络操作。
- en: 2.2.3 The Node API object
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 节点API对象
- en: 'As mentioned, nodes support Pods, and the control plane defines the group of
    nodes that run the controller, the controller manager, and the scheduler. We can
    view a cluster’s node(s) with this simple `kubectl` command:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 正如提到的，节点支持Pods，控制平面定义了运行控制器、控制器管理器和调度器的节点组。我们可以使用这个简单的`kubectl`命令查看集群的节点（s）：
- en: '[PRE2]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ The full command is kubectl get nodes, which retrieves the Node object(s)
    for a Kubernetes cluster.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 完整的命令是kubectl get nodes，它检索Kubernetes集群的节点对象（s）。
- en: ❷ The output from a kind cluster. Note that this is v1.17.0, which is a little
    older then what you’re probably running locally.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 来自kind集群的输出。注意，这是v1.17.0，可能比您本地运行的版本要旧一些。
- en: 'Now, let’s take a look at the Node API object that describes the node that
    hosts the Kubernetes control plane:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看描述托管Kubernetes控制平面的节点的Node API对象：
- en: '[PRE3]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The following example provides the entire API Node object value. (In the example,
    we will break up the YAML into multiple sections because it is long.)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例提供了整个API Node对象的值。（在示例中，我们将YAML拆分为多个部分，因为它很长。）
- en: '[PRE4]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ The CRI socket used. With kind (and most clusters), this is the containerd
    socket.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用的CRI套接字。在kind（以及大多数集群）中，这是containerd套接字。
- en: ❷ Standard labels, including the node name
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 标准标签，包括节点名称
- en: ❸ CNI IP address, which is CIDR for the Pod network
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ CNI IP地址，这是Pod网络的CIDR
- en: Let’s now move on to the status section. It provides information about the node
    and what the node is composed of.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转到状态部分。它提供了有关节点及其组成的详细信息。
- en: '[PRE5]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Updates for the API server for various status fields of the kubelet running
    on the node
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 对节点上运行的kubelet的各种状态字段的API服务器的更新
- en: 'Next, let’s look at all of the images running on the node:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看节点上运行的所有镜像：
- en: '[PRE6]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ The different images running on the node
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 节点上运行的不同镜像
- en: ❷ The etcd server that serves as the database for Kubernetes
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 作为Kubernetes数据库的etcd服务器
- en: ❸ API server and other controllers (like the kube-controller-manager)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ API服务器和其他控制器（如kube-controller-manager）
- en: ❹ The CNI provider. We need a software-defined network, and this container provides
    that functionality.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ CNI提供者。我们需要一个软件定义的网络，这个容器提供了这个功能。
- en: 'Finally, we’ll add the `nodeInfo` block. This includes versioning for the Kubernetes
    system:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将添加`nodeInfo`块。这包括Kubernetes系统的版本控制：
- en: '[PRE7]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Specifies information about the node including OS, kube-proxy, and kubelet
    versions
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指定有关节点的信息，包括操作系统、kube-proxy和kubelet版本
- en: What is needed now is a container engine, the network proxy (`kube-proxy`),
    and the kubelet to run a node. We’ll get to this in a bit.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在需要的是一个容器引擎、网络代理（`kube-proxy`）和kubelet来运行节点。我们稍后会讨论这个问题。
- en: Controllers and control loops
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器和控制循环
- en: The word *control* is an overloaded term in the context of Kubernetes; the meanings
    are related, but it is a bit confusing. There are control loops, controllers,
    and the control plane. A Kubernetes installation consists of multiple executable
    binaries called *controllers*. You know them as the kubelet, the Kubernetes network
    proxy, the scheduler, and more. Controllers are written with computer programming
    patterns called *control loops*. The control plane houses specific controllers.
    These nodes and controllers are the mother ship or brain of Kubernetes. More to
    come on that subject throughout this chapter.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes的上下文中，“控制”这个词是一个多义词；其含义相关，但有点令人困惑。有控制循环、控制器和控制平面。一个Kubernetes安装由多个称为“控制器”的可执行二进制文件组成。您可能知道它们是kubelet、Kubernetes网络代理、调度器等等。控制器是用称为“控制循环”的计算机编程模式编写的。控制平面容纳特定的控制器。这些节点和控制器是Kubernetes的母舰或大脑。关于这个主题的更多内容将在本章的后续部分进行讨论。
- en: 'The nodes that comprise the control plane are sometimes referred to as *masters*,
    but we use control planes throughout the book. At its heart, Kubernetes is a state-reconciliation
    machine with various control loops, like an air-conditioner. Rather than regulating
    temperature, however, Kubernetes controls the following (as well as regulating
    many other aspects of distributed application management):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 构成控制平面的节点有时被称为“主节点”，但我们在整本书中都会使用控制平面。从本质上讲，Kubernetes是一个状态协调机器，具有各种控制循环，就像空调一样。然而，与调节温度不同，Kubernetes控制以下内容（以及调节分布式应用程序管理的许多其他方面）：
- en: Binding storage to processes
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将存储绑定到进程
- en: Creating running containers and scaling the number of containers
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建运行中的容器并扩展容器数量
- en: Killing and migrating containers when they are unhealthy
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当容器不健康时终止和迁移容器
- en: Creating IP routes to ports
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建IP路由到端口
- en: Dynamically updating load-balanced endpoints
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态更新负载均衡端点
- en: 'Let’s loop back to the requirements we have covered thus far: Pods provide
    an instrument to deploy images. The images are deployed to a node, and their life
    cycle is managed by the kubelet. Service objects are managed by the Kubernetes
    network proxy. A DNS system like CoreDNS provides application lookup, allowing
    microservices in one Pod to look up and communicate with another Pod runnning
    CockroachDB, for example.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下到目前为止我们已经讨论过的需求：Pod提供了一个部署镜像的工具。这些镜像被部署到节点上，其生命周期由kubelet管理。服务对象由Kubernetes网络代理管理。像CoreDNS这样的DNS系统提供了应用程序查找功能，允许一个Pod中的微服务查找并与其他运行CockroachDB等服务的Pod进行通信。
- en: The Kubernetes network proxy also provides the capability of having internal
    load balancing within a cluster, thus helping with failover, upgrades, availability,
    and scaling. To address the need for persistent storage, the combination of the
    `mnt` Linux namespace, the kubelet, and the node allows for mounting a drive to
    a Pod. When the kubelet creates the Pod, that storage is then mounted to the Pod.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes网络代理还提供了在集群内部进行内部负载均衡的能力，从而帮助实现故障转移、升级、可用性和扩展。为了解决持久存储的需求，`mnt` Linux命名空间、kubelet和节点的组合允许将驱动器挂载到Pod上。当kubelet创建Pod时，该存储就会被挂载到Pod上。
- en: It seems that we are still missing a few parts. What if a node dies—what happens
    then? How do we even get a Pod on a node? Enter the control plane.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们仍然缺少一些部分。如果某个节点失效了——那会怎样？我们如何将Pod部署到节点上？这就引入了控制平面。
- en: 2.2.4 Our web application and the control plane
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.4 我们的Web应用程序和控制平面
- en: After establishing the Pods and nodes, the next step is to figure out how to
    get complex requirements, such as high availability. High availability (often
    referred to as HA) is not just a simple failover, but it meets the requirements
    of a service-level agreement (SLA). System availability is often measured in the
    number of 9s of uptime. This is a measurement of how much downtime you can have
    with an application or set of applications. Four 9s give us 52 minutes and 36
    seconds of downtime a year; five 9s (99.999% uptime) give us 5 minutes and 15
    seconds of possible downtime. 99.999% uptime gives us 26.25 seconds of downtime
    a month. Having five 9s means we only have less than half a minute a month where
    an application hosted on Kubernetes is not available. That is incredibly hard!
    All of the other requirements we have are not trivial as well. These include
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立 Pod 和节点之后，下一步是要弄清楚如何满足复杂的需求，例如高可用性。高可用性（通常称为 HA）不仅仅是简单的故障转移，它还满足服务级别协议（SLA）的要求。系统可用性通常用连续运行时间的
    9 个数量级来衡量。这是衡量应用程序或应用程序集可以有多长时间停机的一种度量。四个 9 个数量级给我们每年 52 分钟和 36 秒的停机时间；五个 9 个数量级（99.999%
    的正常运行时间）给我们 5 分钟和 15 秒的可能停机时间。99.999% 的正常运行时间给我们每月 26.25 秒的停机时间。拥有五个 9 个数量级意味着我们每月只有不到半分钟的时间，Kubernetes
    上托管的应用程序不可用。这非常困难！我们所有的其他要求也同样不简单。这些包括
- en: Scaling
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规模化
- en: Cost savings
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本节约
- en: Container version control
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器版本控制
- en: User and application security
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户和应用程序安全
- en: Note Yes, Kubernetes provides all of these capabilities, but applications have
    to support the way Kubernetes works as well. We will address the caveats about
    application design in the last chapter of this book.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：是的，Kubernetes 提供了所有这些功能，但应用程序也必须支持 Kubernetes 的工作方式。我们将在本书的最后一章讨论应用程序设计的注意事项。
- en: The first step is the provisioning of Pods. However, beyond that, we have a
    system that provides us not only with fault tolerance and scalability (and in
    the same command line) but also the capability to save money and thus control
    costs. Figure 2.5 shows us the makeup of a typical control plane.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是部署 Pod。然而，除此之外，我们有一个系统不仅为我们提供容错性和可伸缩性（并且在同一命令行中），而且还具有节省金钱和控制成本的能力。图 2.5
    展示了典型控制平面的组成。
- en: '![](../Images/CH02_F05_Love.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F05_Love.png)'
- en: Figure 2.5 The control plane
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 控制平面
- en: 2.3 Creating a web application with kubectl
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 使用 kubectl 创建 Web 应用程序
- en: 'In order to understand how the control plane facilitates complexity, such as
    scaling and fault tolerance, let’s walk through the simple command: `kubectl`
    `apply` `-f deployment.yaml`. The following is the YAML for the deployment:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解控制平面如何促进复杂性，例如规模化和容错性，让我们通过简单的命令：`kubectl` `apply` `-f deployment.yaml`。以下是为部署提供的
    YAML：
- en: '[PRE8]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: When you execute `kubectl apply`, `kubectl` communicates to the first component
    in the control plane. That is the API server.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当你执行 `kubectl apply` 时，`kubectl` 会与控制平面的第一个组件通信。那就是 API 服务器。
- en: '2.3.1 The Kubernetes API server: kube-apiserver'
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 Kubernetes API 服务器：kube-apiserver
- en: The Kubernetes API server, `kube-apiserver`, is an HTTP-based REST server that
    exposes the various API objects for a Kubernetes cluster; these objects range
    from Pods to nodes to the Horizontal Pod Autoscaler. The API server validates
    and provides the web frontend to perform CRUD services on the cluster’s shared
    state. Production control planes for Kubernetes typically provide a robust Kubernetes
    API server, which means running it on every node that comprises the control plane,
    or running it behind a cloud service that uses some other mechanism for failover
    and high availability. In practice, this means that access to the API server is
    done via an HAProxy or cloud load-balancer endpoint.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes API 服务器 `kube-apiserver` 是一个基于 HTTP 的 REST 服务器，它公开了 Kubernetes 集群的各种
    API 对象；这些对象从 Pod 到节点再到水平 Pod 自动扩展器。API 服务器验证并提供网络前端，以在集群的共享状态上执行 CRUD 服务。Kubernetes
    的生产控制平面通常提供强大的 Kubernetes API 服务器，这意味着在每个构成控制平面的节点上运行它，或者运行在背后使用某种其他机制进行故障转移和高可用的云服务中。在实践中，这意味着通过
    HAProxy 或云负载均衡器端点访问 API 服务器。
- en: The components on the control plane communicate with the API server as well.
    When a node starts, the kubelet ensures that the node registers with the cluster
    by communicating with the API server. All of the components in the control plane
    have a control loop that has a watch functionality to monitor the API server for
    changes in objects.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面上的组件也会与 API 服务器进行通信。当一个节点启动时，kubelet 确保节点通过与 API 服务器通信来注册到集群。控制平面中的所有组件都有一个控制循环，该循环具有监视功能，用于监视
    API 服务器中对象的变化。
- en: The API server is the only component on the control plane that communicates
    with etcd, the database for Kubernetes. In the past, some other components like
    CNI network providers have communicated with etcd, but currently, most do not.
    In essence, the API server provides a stateful interface for all operations modifying
    a Kubernetes cluster. Security for all control plane components is essential,
    but securing the API server and its HTTPS endpoints is crucial.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: API 服务器是控制平面上唯一与 Kubernetes 数据库 etcd 通信的组件。在过去，一些其他组件（如 CNI 网络提供者）曾与 etcd 通信，但当前大多数组件都没有这样做。本质上，API
    服务器为所有修改 Kubernetes 集群的操作提供了一个有状态的接口。所有控制平面组件的安全性都是必要的，但保护 API 服务器及其 HTTPS 端点是至关重要的。
- en: When running on a high availability control plane, every Kubernetes API server
    is active and receives traffic. The API server is primarily stateless and can
    run on multiple nodes at the same time. An HTTPS load balancer fronts the API
    servers in a control plane consisting of multiple nodes. But we do not want anyone
    to have the permissions to communicate with the API server. *Admission controllers*
    that run as part of the API server provide both authentication and authorization
    when a client communicates with the API server.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当在高可用性控制平面上运行时，每个 Kubernetes API 服务器都是活动的，并接收流量。API 服务器主要是无状态的，可以同时运行在多个节点上。HTTPS
    负载均衡器在由多个节点组成的管理平面中作为 API 服务器的代理。但我们不希望任何人都有权限与 API 服务器通信。作为 API 服务器一部分运行的 *Admission
    controllers* 在客户端与 API 服务器通信时提供身份验证和授权。
- en: Often, external authentication and authorization systems are integrated into
    the API server in the form of webhooks. A *webhook* is an HTTP PUSH API that allows
    for a callback. The `kubectl` call is authenticated, and then the API server persists
    the new deployment object to etcd. Our next step is to enact the scheduler to
    get the Pod on the node. New Pods need a new home, so the scheduler assigns Pods
    to a specific Node.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，外部身份验证和授权系统以 webhook 的形式集成到 API 服务器中。*webhook* 是一个允许回调的 HTTP PUSH API。`kubectl`
    调用经过身份验证后，API 服务器将新的部署对象持久化到 etcd。我们的下一步是执行调度器，以便将 Pod 部署到节点上。新的 Pod 需要一个新的家，因此调度器将
    Pod 分配到特定的节点。
- en: '2.3.2 The Kubernetes scheduler: kube-scheduler'
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 Kubernetes 调度器：kube-scheduler
- en: Distributed scheduling is not trivial. The Kubernetes scheduler (`kube-scheduler`)
    provides a clean, simple implementation of scheduling—perfect for a complex system
    like Kubernetes. The scheduler considers multiple factors in Pod scheduling. These
    include hardware components on a node, available CPU and memory resources, policy
    scheduling constraints, and other weighting factors.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式调度并非易事。Kubernetes 调度器（`kube-scheduler`）提供了一个干净、简单的调度实现——非常适合像 Kubernetes
    这样的复杂系统。调度器在 Pod 调度时会考虑多个因素。这些因素包括节点上的硬件组件、可用的 CPU 和内存资源、策略调度约束以及其他加权因素。
- en: The scheduler also follows Pod affinity and anti-affinity rules that specify
    Pod scheduling and placement behavior. In essence, Pod affinity rules attract
    Pods to a Node that match the rules, whereas Pod anti-affinity rules repel Pods
    from nodes. Taints also allow a node to repel a set of Pods, meaning the scheduler
    can determine which Pods should not live on which nodes. In the Zeus Zap example
    (section 2.1.2), three replicas of the Pod are defined. The replicas provide both
    fault tolerance and the capability to scale, the scheduler determines which node
    or nodes the replicas can run on, and then the Pods are scheduled for deployment
    on each of the nodes.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器还遵循 Pod 亲和性和反亲和性规则，这些规则指定了 Pod 的调度和放置行为。本质上，Pod 亲和性规则将 Pod 吸引到符合规则的节点，而 Pod
    反亲和性规则则将 Pod 从节点排斥出去。污点（Taints）也允许节点排斥一组 Pod，这意味着调度器可以确定哪些 Pod 不应该运行在哪些节点上。在宙斯
    Zap 示例（第 2.1.2 节）中，定义了三个 Pod 副本。这些副本提供了容错能力和扩展能力，调度器确定副本可以运行在哪些节点上，然后 Pod 被调度到每个节点上进行部署。
- en: The kubelet controls the Pod life cycle, acting like a mini-scheduler for the
    node. Once the Kubernetes scheduler updates the Pod with `NodeName`, the kubelet
    deploys that Pod to its node. The control plane is completely separated from the
    nodes that do not run the control plane components. Even with a control plane
    outage, Zeus Zap will not lose any application information if a node goes down.
    In a control plane outage, nothing new can be deployed, but the website is still
    up and running.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: kubelet控制Pod的生命周期，类似于节点的迷你调度器。一旦Kubernetes调度器通过`NodeName`更新Pod，kubelet就会将该Pod部署到其节点上。控制平面与不运行控制平面组件的节点完全分离。即使在控制平面故障的情况下，如果节点发生故障，Zeus
    Zap也不会丢失任何应用程序信息。在控制平面故障期间，无法部署任何新内容，但网站仍然运行。
- en: What if a persistent disk attached to the applications is required? In this
    case, it’s likely that the storage continues to work for these applications, until
    and unless the nodes running these applications have problems. Even in this event,
    if the control plane is back online, we can generally expect a safe migration
    of both data and application to a new home, thanks to the commensurate functionality
    of the Kubernetes control plane.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要连接到应用程序的持久磁盘怎么办？在这种情况下，存储可能继续为这些应用程序工作，直到并且除非运行这些应用程序的节点出现问题。即使在发生这种情况时，如果控制平面恢复在线，我们通常可以期待数据和应用安全迁移到新位置，这得益于Kubernetes控制平面的相应功能。
- en: 2.3.3 Infrastructure controllers
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 基础设施控制器
- en: 'One of the requirements for Zeus Zap’s infrastructure is a CockroachDB cluster.
    CockroachDB is a Postgres-compliant distributed database that runs in a cloud
    native environment. Applications that are stateful, like databases, often have
    specific operational requirements. This leads to requiring a controller or an
    Operator to manage the application. Because the Operator pattern is quickly becoming
    the standard mechanism to deploy complex applications on Kubernetes, it is our
    advice to diverge from the use of plain YAML to installing and using an Operator
    instead. The following example installs an Operator for the CockroachDB:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Zeus Zap基础设施的一个要求是CockroachDB集群。CockroachDB是一个符合Postgres规范的分布式数据库，在云原生环境中运行。像数据库这样的有状态应用程序通常有特定的操作要求。这导致需要控制器或Operator来管理应用程序。由于Operator模式正迅速成为在Kubernetes上部署复杂应用程序的标准机制，我们建议避免使用纯YAML，而是安装并使用Operator。以下示例安装了CockroachDB的Operator：
- en: '[PRE9]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Installs the custom resource definition utilized by the Operator
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 安装Operator使用的自定义资源定义
- en: ❷ Installs the Operator in the default namespace
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在默认命名空间中安装Operator
- en: Custom resource definitions
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义资源定义
- en: '*Custom resource definitions* (CRDs) are API objects that define new API objects.
    A user creates a CRD by defining it, which is often in YAML. The CRD is then applied
    to the existing Kubernetes cluster and actually allows the API to, in fact, create
    another API object. We can actually use a CRD to define and allow for new custom
    resource API objects.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '*自定义资源定义*（CRDs）是定义新API对象的API对象。用户通过定义CRD来创建它，通常使用YAML格式。然后，CRD应用于现有的Kubernetes集群，实际上允许API创建另一个API对象。我们实际上可以使用CRD来定义并允许创建新的自定义资源API对象。'
- en: 'After installing the CockroachDB Operator, we can then download the example.yaml.
    The following shows the `curl` command for this:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 安装CockroachDB Operator后，我们可以下载example.yaml。以下显示了此操作的`curl`命令：
- en: '[PRE10]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The YAML snippet looks like this:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: YAML片段看起来像这样：
- en: '[PRE11]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ The container that is used to start the database
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 用于启动数据库的容器
- en: 'This custom resource uses the Operator pattern to create and manage the following
    resources (note that these items are easily hundreds of lines of YAML):'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 此自定义资源使用Operator模式创建和管理以下资源（请注意，这些项目可能是数百行YAML）：
- en: Transport Layer Security (TLS) keys stored in secrets for the database
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储在数据库秘密中的传输层安全性（TLS）密钥
- en: A StatefulSet that houses CockroachDB, including PersistentVolume and PersistentVolumeClaim
    storage
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含持久卷和持久卷声明的CockroachDB StatefulSet
- en: Services
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务
- en: A Pod disruption budget (PodDisruptionBudget or PDB)
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod中断预算（PodDisruptionBudget 或 PDB）
- en: Considering the example just given, let’s dive into the infrastructure controller,
    called the Kubernetes controller manager (KCM) or the `kube-controller-manager`
    component, and the cloud controller manager (CCM). With a deployed StatefulSet
    built on Pods, we now need storage for the StatefulSet.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到刚才给出的示例，让我们深入了解名为Kubernetes控制器管理器（KCM）或`kube-controller-manager`组件的基础设施控制器，以及云控制器管理器（CCM）。在基于Pod构建的已部署StatefulSet中，我们现在需要为StatefulSet提供存储。
- en: 'The API objects PersistentVolume (PV) and PersistentVolumeClaim (PVC) create
    the storage definitions and are brought to life by the KCM and the CCM. One of
    the key features for Kubernetes is the capability to run on top of a plethora
    of platforms: the cloud, bare metal, or a laptop. However, storage and other components
    are different across platforms: enter KCM and CCM. The KCM is a set of control
    loops that run various components, called *controllers*, on a node that is on
    a control plane. It is a single binary, but operates multiple control loops and,
    thus, controllers.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: API 对象 PersistentVolume (PV) 和 PersistentVolumeClaim (PVC) 创建了存储定义，并由 KCM 和
    CCM 使其生效。对于 Kubernetes 来说，一个关键特性是能够在众多平台上运行：云平台、裸金属或笔记本电脑。然而，存储和其他组件在不同平台之间是不同的：这就是
    KCM 和 CCM 的作用。KCM 是一组在控制平面上的节点上运行的组件，称为 *controllers* 的控制循环。它是一个单一的二进制文件，但运行多个控制循环，因此有多个控制器。
- en: The birth of the cloud controller manager (CCM)
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 云控制器管理器（CCM）的诞生
- en: The Kubernetes development team is comprised of engineers from all around the
    world, united under the umbrella of the CNCF (Cloud Native Computing Foundation),
    which includes hundreds of corporate members. Supporting the needs of such a vast
    array of businesses is impossible without the decomposition of vendor-specific
    functionality into well-defined, pluggable components.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 开发团队由来自世界各地的工程师组成，他们统一在 CNCF（云原生计算基金会）的旗下，该基金会包括数百个企业成员。没有将特定供应商的功能分解成定义良好、可插拔的组件，就无法支持如此众多的企业需求。
- en: The KCM has historically been a tightly coupled and difficult-to-maintain codebase,
    primarily due to the accidental complexity of different vendor technologies. For
    example, provisioning new IP addresses or storage volumes requires completely
    different code paths, depending on whether you are in Google Kubernetes Engine
    (GKE) or Amazon Web Services (AWS). Given that there are also several bespoke,
    on-premise offerings of Kubernetes (vSphere, Openstack, and so on), a proliferation
    of cloud provider-specific code has persisted since the first days of Kubernetes.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: KCM 一直是一个紧密耦合且难以维护的代码库，这主要是由于不同供应商技术的意外复杂性。例如，分配新的 IP 地址或存储卷需要完全不同的代码路径，这取决于你是在
    Google Kubernetes Engine (GKE) 还是 Amazon Web Services (AWS)。鉴于还有几个定制的本地 Kubernetes
    提供方案（vSphere、Openstack 等），自 Kubernetes 诞生以来，云提供商特定的代码一直呈增长态势。
- en: The KCM codebase is located in the github.com repository kubernetes/kubernetes,
    which is often referred to as *kk*. There’s nothing wrong with having a vast monorepo.
    Google has just one repo for the company, but the Kubernetes codebase monorepo
    outgrew GitHub and the use case of one company. At some point, the engineers working
    on Kubernetes collectively realized that they would need to decompose vendor-specific
    functionality as previously mentioned. One emerging component in this crusade
    was the creation of a CCM that generically utilized functionality from any vendor
    implementing the cloud provider interface ([http://mng.bz/QWRv](http://mng.bz/QWRv)).
    Moreover, the same pattern is now used with the Kubernetes scheduler and scheduler
    plugins.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: KCM 代码库位于 github.com 仓库中的 kubernetes/kubernetes，通常被称为 *kk*。拥有一个庞大的单代码库并没有什么问题。谷歌公司只有一个代码库，但
    Kubernetes 代码库的单代码库已经超越了 GitHub 和一个公司的使用场景。在某个时刻，从事 Kubernetes 开发的工程师们集体意识到，他们需要像之前提到的那样分解特定供应商的功能。在这场斗争中，一个新兴的组件是创建一个通用的
    CCM，它可以从实现云提供商接口的任何供应商那里利用功能([http://mng.bz/QWRv](http://mng.bz/QWRv))。此外，现在这个模式也被用于
    Kubernetes 调度器和调度器插件。
- en: The CCM was designed to allow faster cloud provider development and creation
    of cloud providers. The CCM creates an interface that allows a cloud provider
    such as DigitalOcean to be developed and maintained outside of the primary Kubernetes
    GitHub repository. This restructuring of the repository allows the owners of the
    cloud provider to manage the code and enables the providers to move at a higher
    velocity. Each of the cloud providers now lives in their repositories outside
    of the Kubernetes host.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: CCM 的设计是为了允许更快的云提供商开发和创建云提供商。CCM 创建了一个接口，允许像 DigitalOcean 这样的云提供商在主要的 Kubernetes
    GitHub 仓库之外开发和维护。这种仓库的重构允许云提供商的所有者管理代码，并使提供商能够以更高的速度移动。每个云提供商现在都生活在 Kubernetes
    主仓库之外的自己的仓库中。
- en: Since the Kubernetes v1.6 release, work started on moving functionality out
    of the KCM and into the CCM. CCM promises to make Kubernetes completely cloud-agnostic.
    This design represents a general trend in how Kubernetes architecture is evolving,
    being wholly decoupled from the implementations of any vendor-pluggable technologies.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 自从Kubernetes v1.6版本发布以来，开始着手将功能从KCM移至CCM。CCM承诺使Kubernetes完全云无关。这种设计代表了Kubernetes架构演变的一般趋势，完全解耦于任何供应商可插拔技术的实现。
- en: 'When running Kubernetes on a cloud platform, Kubernetes interacts directly
    with the public or private cloud APIs, and the CCM executes the majority of those
    API calls. This component’s purpose is to run cloud-specific controller loops
    and to execute cloud-based API calls. Here’s a list of that functionality:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 当在云平台上运行Kubernetes时，Kubernetes直接与公共或私有云API交互，CCM执行大多数这些API调用。该组件的目的是运行云特定的控制器循环和执行基于云的API调用。以下是该功能列表：
- en: '*Node controller*—Runs the same code as the KCM'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*节点控制器*—运行与KCM相同的代码'
- en: '*Route controller*—Sets up routes in the underlying cloud infrastructure'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*路由控制器*—在底层云基础设施中设置路由'
- en: '*Service controller*—Creates, updates, and deletes cloud provider load balancers'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*服务控制器*—创建、更新和删除云提供商负载均衡器'
- en: '*Volume controller*—Creates, attaches, and mounts volumes and interacts with
    the cloud provider to orchestrate volumes'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*卷控制器*—创建、附加和挂载卷，并与云提供商交互以编排卷'
- en: These controllers are transitioning to operating against the cloud provider
    interface, and this trend is pervasive throughout Kubernetes. Other interfaces
    that are evolving to support a more modular, vendor-neutral future for Kubernetes
    are
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这些控制器正在过渡到针对云提供商接口进行操作，这一趋势在Kubernetes中普遍存在。其他正在演变以支持Kubernetes更模块化、供应商中立的未来的接口包括
- en: '*Container Networking Interface (CNI)*—Supplies Pods with IP addresses'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*容器网络接口 (CNI)*—为Pod提供IP地址'
- en: '*Container Runtime Interface (CRI)*—Defines and plugs in different container
    execution engines'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*容器运行时接口 (CRI)*—定义和插入不同的容器执行引擎'
- en: '*Container Storage Interface (CSI)*—A modular way for vendors to support new
    storage types without having to modify the Kubernetes codebase'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*容器存储接口 (CSI)*—供应商以模块化方式支持新存储类型，而无需修改Kubernetes代码库'
- en: Now, looping back to our example, storage is needed to attach to the CockroachDB
    Pods. When the Pod is scheduled on a node kubelet, KCM (or CCM) detects that new
    storage is required and creates the storage depending on what platform it is running
    on. It then mounts the storage on the node. When a kubelet creates the Pod, it
    determines which storage to attach, and the storage is attached to the container
    via the `mnt` Linux namespace. Now there is storage for our app.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回到我们的例子，我们需要存储来附加到CockroachDB Pods。当Pod被调度到节点上的kubelet时，KCM（或CCM）会检测到需要新的存储，并根据其运行的平台创建存储。然后它在节点上挂载存储。当kubelet创建Pod时，它会确定要附加哪种存储，并通过`mnt`
    Linux命名空间将存储附加到容器。现在我们的应用程序有了存储。
- en: 'Back to our user case: Zeus Zap also needs a load balancer for their public
    website. Creating a LoadBalancer Service instead of the ClusterIP Service involves
    a Kubernetes cloud provider “watching” for a user load-balancer request and then
    fulfilling it (for example, by making calls to a cloud API to provision an external
    IP address and binding it to internal Kubernetes service endpoints). From an end-user
    perspective, however, the request for this is quite simple:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的用户案例：宙斯Zap也需要一个负载均衡器来为他们的公共网站提供服务。创建负载均衡器服务而不是集群IP服务涉及Kubernetes云提供商“监视”用户负载均衡器请求，然后满足它（例如，通过调用云API来分配外部IP地址并将其绑定到内部Kubernetes服务端点）。然而，从最终用户的角度来看，这个请求相当简单：
- en: '[PRE12]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The KCM watch loop detects that a new load balancer is required and makes the
    API calls needed to create a load balancer in the cloud or calls a hardware load
    balancer external to the Kubernetes cluster. Within those API calls, the underlying
    infrastructure understands which nodes are part of the cluster and then routes
    traffic to the nodes. Once a call reaches a node, the software-defined network
    that is provided by the CNI provider then routes the traffic to the correct Pod.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: KCM监视循环检测到需要新的负载均衡器，并执行在云中创建负载均衡器的API调用所需的API调用，或者调用Kubernetes集群外部的硬件负载均衡器。在这些API调用中，底层基础设施了解哪些节点是集群的一部分，然后路由流量到节点。一旦调用到达节点，由CNI提供商提供的软件定义网络就会将流量路由到正确的Pod。
- en: 2.4 Scaling, highly available applications, and the control plane
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 扩展、高可用应用程序和控制平面
- en: 'The scaling up and down of applications is the underlying mechanism for enabling
    highly available (HA) applications in the cloud, and especially in Kubernetes.
    Either you need more Pods to scale, or you need to redeploy Pods because you do
    not have enough Pods when a Pod or node experiences a failure. Executing `kubectl`
    `scale` can increase and decrease the amount of Pods that are running in a cluster.
    It operates directly on the ReplicaSets, StatefulSets, or other API objects that
    use Pods, depending on the input you provide to the command. For example:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序的扩缩容是使云中的高可用（HA）应用程序，尤其是在 Kubernetes 中成为可能的基本机制。您可能需要更多的 Pods 来进行缩放，或者因为
    Pod 或节点发生故障而没有足够的 Pods，您需要重新部署 Pods。执行 `kubectl scale` 可以增加和减少集群中运行的 Pods 数量。它根据您提供的命令输入直接操作
    ReplicaSets、StatefulSets 或其他使用 Pods 的 API 对象。例如：
- en: '[PRE13]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This command doesn’t apply to DaemonSets. Although DaemonSet objects create
    Pods, they aren’t scalable because, by definition, they run a single Pod on every
    node of a cluster: their scale is determined by the number of nodes in your cluster.
    In the Zeus scenario, this command increases or decreases the number of Pods backing
    the Zeus frontend UI deployment, following the same pattern that the scheduler,
    KCM, and kubelet followed for the previous example. Figure 2.6 shows the typical
    sequence for the `kubectl` `scale` command.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令不适用于 DaemonSets。尽管 DaemonSet 对象创建了 Pods，但它们不可扩展，因为根据定义，它们在每个集群的每个节点上运行单个
    Pod：它们的规模由集群中的节点数量决定。在宙斯场景中，此命令根据调度器、KCM 和 kubelet 对前一个示例遵循的相同模式增加或减少支持宙斯前端 UI
    部署的 Pods 数量。图 2.6 显示了 `kubectl scale` 命令的典型顺序。
- en: '![](../Images/CH02_F06_Love.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.6](../Images/CH02_F06_Love.png)'
- en: Figure 2.6 The sequence of operations for the `kubectl scale` command
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 `kubectl scale` 命令的操作顺序
- en: 'Now, what happens when things go thud, which they always do? We can break up
    basic failures or actions into three levels: Pod, node, and software updates.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当事情发生噗噗声时，这总是会发生？我们可以将基本故障或操作分解为三个级别：Pod、节点和软件更新。
- en: First, Pod outages. The kubelet is in charge of the Pod’s life cycle, which
    includes starting, stopping, and restarting Pods. When a Pod fails, the kubelet
    attempts to restart it, and it knows that the Pod has failed either via the defined
    liveliness probe, or the Pod’s process stops. We cover the kubelet in greater
    detail in chapter 9.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Pod 故障。kubelet 负责 Pod 的生命周期，包括启动、停止和重启 Pods。当一个 Pod 失败时，kubelet 尝试重启它，并且它知道
    Pod 失败是通过定义的存活探针，或者 Pod 的进程停止。我们在第 9 章中对 kubelet 进行了更详细的介绍。
- en: Second, node outages. One of the kubelet’s control loops is constantly updating
    the API server, reporting that a node is healthy (via a heartbeat). In Kubernetes
    1.17+, you can see how this heartbeat is maintained by looking at the `kube-node-lease`
    namespace of a running cluster. If a node does not update its heartbeat often
    enough, its status is changed to offline by the KCM’s controller, and Pods are
    not scheduled for the node anymore. The Pods that did exist on the node are scheduled
    for deletion and then rescheduled to other nodes.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，节点故障。kubelet 的一个控制循环不断更新 API 服务器，报告节点健康（通过心跳）。在 Kubernetes 1.17+ 中，您可以通过查看运行集群的
    `kube-node-lease` 命名空间来了解此心跳是如何维护的。如果一个节点没有足够频繁地更新其心跳，KCM 的控制器会将该节点的状态更改为离线，并且不再为该节点调度
    Pods。已经存在于节点上的 Pods 将被调度删除，然后重新调度到其他节点。
- en: 'You can watch this process by manually running `kubectl` `cordon` `node-name`
    then `kubectl` `drain` `node-name`. A node has various conditions that are monitored:
    network unavailable, no frequent docker restarts, kubelet ready, and more. Any
    of these heartbeats that fail stop the scheduling of new Pods on the node.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过手动运行 `kubectl cordon node-name` 然后运行 `kubectl drain node-name` 来观察这个过程。节点有多种条件被监控：网络不可用、没有频繁的
    Docker 重启、kubelet 已就绪，等等。任何这些心跳失败都会停止在节点上调度新的 Pods。
- en: 'Lastly, with software update outages, many websites and other services have
    scheduled downtime, but the big players on the web, like Facebook and Google,
    never have scheduled downtimes. Both of those companies use custom software that
    predates Kubernetes. Kubernetes is built to roll out both Kubernetes updates and
    new Pod updates without downtime. There is a huge caveat, however: the software
    that is running on a Kubernetes platform has to be durable in a manner that supports
    how Kubernetes restarts applications. If they are not durable enough for outages,
    data loss can occur.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于软件更新中断，许多网站和其他服务已经安排了停机时间，但像Facebook和Google这样的网络大玩家从未安排过停机时间。这两家公司都使用早于Kubernetes定制的软件。Kubernetes旨在在不中断服务的情况下推出Kubernetes更新和新Pod更新。然而，有一个巨大的前提条件：在Kubernetes平台上运行的软件必须以支持Kubernetes重启应用程序的方式具有耐用性。如果它们不足以应对中断，可能会发生数据丢失。
- en: 'Applications that are hosted on a Kubernetes platform must support graceful
    shutdown and then startup. For instance, if you have a transaction running inside
    an application, it needs to support either redoing the transaction in another
    replica of the application or restarting the transaction once the application
    is restarted. Upgrading a deployment is as easy as changing the image version
    in your YAML definition, which is usually just done in one of three ways:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes平台上托管的应用程序必须支持优雅关闭和启动。例如，如果您在应用程序内部运行一个事务，它需要支持在应用程序的另一个副本中重做该事务或一旦应用程序重新启动就重新启动事务。升级部署就像更改YAML定义中的镜像版本一样简单，这通常只是以三种方式之一完成：
- en: '`kubectl edit`—Takes a Kubernetes API object as input and opens a local terminal
    to edit the API object in place'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubectl edit`——接受一个Kubernetes API对象作为输入，并打开一个本地终端以就地编辑API对象'
- en: '`kubectl apply`—Takes a file as input and finds the API object corresponding
    to this file, replacing it automatically'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubectl apply`——接受一个文件作为输入，并找到与该文件对应的API对象，自动替换它'
- en: '`kubectl patch`—Applies a small “patch” file that defines the differences for
    an object'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubectl patch`——应用一个小的“补丁”文件，该文件定义了对象的差异'
- en: In chapter 15, we’ll look at full-blown YAML patching and application life cycle
    tooling. There, we’ll approach this broad subject in a more holistic manner.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在第15章中，我们将探讨完整的YAML补丁和应用程序生命周期工具。在那里，我们将以更全面的方式处理这个广泛的主题。
- en: Upgrading a Kubernetes cluster is not trivial, but Kubernetes supports various
    patterns for upgrades. We’ll discuss this more in the last chapter of the book,
    as this chapter is all about the control plane and not operational tasks.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 升级Kubernetes集群不是一件简单的事情，但Kubernetes支持各种升级模式。我们将在本书的最后一章中更详细地讨论这个问题，因为本章全部关于控制平面而不是操作任务。
- en: 2.4.1 Autoscaling
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 自动扩展
- en: 'Manually scaling deployments is great, but what if you suddenly get 10,000
    new web requests per minute on a cluster? Autoscaling comes to the rescue. You
    can allow three different forms of autoscaling:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 手动扩展部署很棒，但如果你突然在一个集群上每分钟收到10,000个新的Web请求怎么办？自动扩展就来拯救了。您可以允许三种不同的自动扩展形式：
- en: Make more Pods (horizontal Pod autoscaling with the HorizontalPodAutoscaler)
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建更多Pod（使用水平Pod自动扩展器进行水平Pod自动扩展）
- en: Give Pods more resources (vertical Pod autoscaling with the VerticalPodAutoscaler)
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给Pod分配更多资源（使用垂直Pod自动扩展器进行垂直Pod自动扩展）
- en: Create more nodes (with the Custer Autoscaler)
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建更多节点（使用集群自动扩展器）
- en: Note The autoscalers may or may not be available on some bare metal platforms.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：自动扩展器在某些裸金属平台上可能可用也可能不可用。
- en: 2.4.2 Cost management
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 成本管理
- en: When a cluster autoscales, it automatically adds more nodes to the cluster,
    which means that more nodes equal a higher cloud usage cost. More nodes allow
    your applications to have more replicas and to handle more load, but then your
    bosses get the bill and want a solution to save more money. Herein enters Pod
    density—densely packed nodes.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 当集群自动扩展时，它会自动向集群中添加更多节点，这意味着更多节点意味着更高的云使用成本。更多节点允许您的应用程序拥有更多副本并处理更多负载，但随后您的老板会收到账单，并希望找到节省更多资金的方法。这时就出现了Pod密度——密集排列的节点。
- en: 'Pods are the smallest, most basic unit of any Kubernetes application. They
    are a group of one or more containers sharing the same network. Nodes that host
    the Pods are either a VM or a physical server. The more Pods assigned to one node,
    the less spent on additional servers. Kubernetes allows for *higher pod density*,
    which is the ability to run over-provisioned and densely packed nodes. The Pod
    density is controlled via the following steps:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Pods 是任何 Kubernetes 应用程序中最小、最基本的部分。它们是一组共享相同网络的容器。托管 Pods 的节点可以是虚拟机或物理服务器。分配给一个节点的
    Pod 越多，在额外服务器上的花费就越少。Kubernetes 允许 *更高的 Pod 密度*，这是在过度配置和密集打包的节点上运行的能力。Pod 密度通过以下步骤进行控制：
- en: '*Size and profile your applications*—Applications need to be tested and profiled
    for both memory and CPU usage. Once they are profiled, the resource limits in
    Kubernetes must be set properly and appropriately for that application.'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*对应用程序进行大小和配置分析*—应用程序需要针对内存和 CPU 使用进行测试和分析。一旦分析完成，Kubernetes 中必须为该应用程序设置适当的资源限制。'
- en: '*Pick a node size*—This allows you to pack multiple applications on the same
    node. Running different VM sizes or bare metal servers with different capacity
    allows you to save money and deploy more Pods on them. You still need to ensure
    a high enough node count to allow for high availability, meeting your SLA requirements.'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*选择节点大小*—这允许你在同一节点上打包多个应用程序。运行不同大小的虚拟机或具有不同容量的裸金属服务器可以节省金钱，并在其上部署更多的 Pod。你仍然需要确保有足够的节点数量，以允许高可用性，满足你的服务级别协议（SLA）要求。'
- en: '*Group certain applications together on certain nodes*—This gives you the best
    density. If you have a bunch of marbles in a jar, you have a lot of space left
    in the jar. Adding some sand, or smaller applications, allows you to fill in some
    of the gaps. Taints and tolerations allow for Operator patterns to group and control
    Pod deployment.'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*在特定节点上组合某些应用程序*—这提供了最佳密度。如果你在一个罐子里放了一堆弹珠，罐子里还有很多空间。加入一些沙子或更小的应用程序，可以填补一些空隙。污点（Taints）和容忍度（Tolerations）允许操作员模式对
    Pod 部署进行分组和控制。'
- en: Another factor you need to consider in all of this is *noisy neighbors*. Depending
    on your workload, some of this tuning may not be appropriate. Again, you can spread
    the noisy apps more evenly across your Kubernetes clusters using Pod affinity
    and anti-affinity definitions. We can explore even further cost savings using
    autoscaling and cloud-ephemeral VMs. Also, just hitting the off switch helps.
    Many companies have separate clusters for their development and QA environments.
    If you do not need your development environment running over the weekend, then
    why is it up? Simply reduce the number of worker nodes in the control plane to
    zero, and when you need the cluster back up, increase the worker node count.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些因素中，你还需要考虑的是 *嘈杂的邻居*。根据你的工作负载，一些调整可能并不合适。再次强调，你可以通过 Pod 亲和力和反亲和力定义在 Kubernetes
    集群中更均匀地分散嘈杂的应用程序。我们可以进一步探讨使用自动扩展和云短暂虚拟机来节省成本。此外，简单地关闭开关也有帮助。许多公司都有用于开发和 QA 环境的独立集群。如果你周末不需要运行开发环境，那么为什么它还在运行？简单地将控制平面的工作节点数量减少到零，当你需要集群恢复时，增加工作节点数量。
- en: Summary
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The Pod is the basic Kubernetes API object that uses Linux namespaces to create
    an environment to run one or more containers.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 是 Kubernetes 基本的 API 对象，它使用 Linux 命名空间创建一个运行一个或多个容器的环境。
- en: 'Kubernetes was built to run the Pod in different patterns, which are API objects:
    Deployments, StatefulSets, and so forth.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 是为了以不同的模式运行 Pod 而构建的，这些模式是 API 对象：部署（Deployments）、有状态集（StatefulSets）等等。
- en: Controllers are software components that create and manage Pod life cycles.
    These include the kubelet, the cloud controller manager (CCM), and the scheduler.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制器是创建和管理 Pod 生命周期的软件组件。这些包括 kubelet、云控制器管理器（CCM）和调度器。
- en: The control plane is the brain of Kubernetes. Through it, Kubernetes can bind
    storage to processes, create running containers, scale the number of containers,
    kill and migrate containers when they are unhealthy, create IP routes to ports,
    update load balanced endpoints, and regulate many other aspects of distributed
    application management.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制平面是 Kubernetes 的核心。通过它，Kubernetes 可以将存储绑定到进程，创建运行中的容器，调整容器的数量，在容器不健康时终止和迁移容器，创建
    IP 路由到端口，更新负载均衡端点，并管理分布式应用程序的许多其他方面。
- en: The API server (the `kube-apiserver` component) validates and provides the web
    frontend to perform CRUD operations on the cluster’s shared state. Most control
    planes have the API server running on every node that comprises the control plane,
    providing the highly available (HA) cluster for the API server.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API服务器（即`kube-apiserver`组件）负责验证并提供网络前端，以在集群的共享状态上执行CRUD操作。大多数控制平面都将API服务器部署在构成控制平面的每个节点上，为API服务器提供高度可用的（HA）集群。
