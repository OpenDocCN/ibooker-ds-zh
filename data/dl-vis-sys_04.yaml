- en: 3 Convolutional neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 卷积神经网络
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Classifying images using MLP
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MLP对图像进行分类
- en: Working with the CNN architecture to classify images
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CNN架构对图像进行分类
- en: Understanding convolution on color images
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解在彩色图像上的卷积
- en: Previously, we talked about artificial neural networks (ANNs), also known as
    multilayer perceptrons (MLPs), which are basically layers of neurons stacked on
    top of each other that have learnable weights and biases. Each neuron receives
    some inputs, which are multiplied by their weights, with nonlinearity applied
    via activation functions. In this chapter, we will talk about convolutional neural
    networks (CNNs), which are considered an evolution of the MLP architecture that
    performs a lot better with images.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前，我们讨论了人工神经网络（ANNs），也称为多层感知器（MLPs），它们基本上是由具有可学习权重和偏差的神经元层堆叠而成的。每个神经元接收一些输入，这些输入通过它们的权重相乘，并通过激活函数应用非线性。在本章中，我们将讨论卷积神经网络（CNNs），它们被认为是MLP架构的一种演变，在图像处理方面表现更好。
- en: 'The high-level layout of this chapter is as follows:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的高级布局如下：
- en: '*Image classification with MLP* --We will start with a mini project to classify
    images using MLP topology and examine how a regular neural network architecture
    processes images. You will learn about the MLP architecture’s drawbacks when processing
    images and why we need a new, creative neural network architecture for this task.'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*使用MLP进行图像分类* —— 我们将从一个使用MLP拓扑进行图像分类的小型项目开始，检查常规神经网络架构如何处理图像。你将了解MLP架构在处理图像时的缺点以及为什么我们需要一个新的、创新的神经网络架构来完成这项任务。'
- en: '*Understanding CNNs* --We will explore convolutional networks to see how they
    extract features from images and classify objects. You will learn about the three
    main components of CNNs: the convolutional layer, the pooling layer, and the fully
    connected layer. Then we will apply this knowledge in another mini project to
    classify images with CNNs.'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*理解CNN* —— 我们将探索卷积网络，了解它们如何从图像中提取特征并对对象进行分类。你将了解CNN的三个主要组成部分：卷积层、池化层和全连接层。然后我们将应用这些知识在另一个小型项目中使用CNN对图像进行分类。'
- en: '*Color images* --We will compare how computers see color images versus grayscale
    images, and how convolution is implemented over color images.'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*彩色图像* —— 我们将比较计算机如何看到彩色图像与灰度图像，以及卷积如何在彩色图像上实现。'
- en: '*Image classification project* --We will apply all that you learn in this chapter
    in an end-to-end image classification project to classify color images with CNNs.'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*图像分类项目* —— 我们将应用本章所学的一切，在一个端到端的图像分类项目中使用CNN对彩色图像进行分类。'
- en: 'The basic concepts of how the network learns and optimizes parameters are the
    same with both MLPs and CNNs:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 网络如何学习和优化参数的基本概念在MLP和CNN中是相同的：
- en: '*Architecture* --MLPs and CNNs are composed of layers of neurons that are stacked
    on top of each other. CNNs have different structures (convolutional versus fully
    connected layers), as we are going to see in the coming sections.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*架构* —— MLP和CNN由堆叠在一起的神经元层组成。CNN具有不同的结构（卷积层与全连接层），正如我们将在接下来的章节中看到的。'
- en: Weights and biases --In convolutional and fully connected layers, inference
    works the same way. Both have weights and biases that are initially randomly generated,
    and their values are learned by the network. The main difference between them
    is that the weights in MLPs are in a vector form, whereas in convolutional layers,
    weights take the form of convolutional filters or kernels.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重和偏差 —— 在卷积层和全连接层中，推理工作方式相同。两者都有初始随机生成的权重和偏差，其值由网络学习。它们之间的主要区别在于，MLP中的权重以向量形式存在，而在卷积层中，权重以卷积滤波器或核的形式存在。
- en: '*Hyperparameters* --As with MLPs, when we design CNNs we will always specify
    the error function, activation function, and optimizer. All hyperparameters explained
    in the previous chapters remain the same; we will add some new ones that are specific
    to CNNs.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*超参数* —— 与MLP一样，当我们设计CNN时，我们总会指定误差函数、激活函数和优化器。前几章中解释的所有超参数保持不变；我们将添加一些特定于CNN的新超参数。'
- en: '*Training* --Both networks learn the same way. First they perform a forward
    pass to get predictions; second, they compare the prediction with the true label
    to get the loss function (*y − ŷ*); and finally, they optimize parameters using
    gradient descent, backpropagate the error to all the weights, and update their
    values to minimize the loss function.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练* -- 两个网络以相同的方式进行学习。首先，它们执行前向传播以获得预测；其次，它们将预测与真实标签进行比较以获得损失函数（*y − ŷ*）；最后，它们使用梯度下降优化参数，将错误反向传播到所有权重，并更新它们的值以最小化损失函数。'
- en: Ready? Let’s get started!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好了吗？让我们开始吧！
- en: 3.1 Image classification using MLP
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 使用MLP进行图像分类
- en: Let’s recall the MLP architecture from chapter 2\. Neurons are stacked in layers
    on top of each other, with weight connections. The MLP architecture consists of
    an input layer, one or more hidden layers, and an output layer (figure 3.1).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下第2章中的MLP架构。神经元堆叠在彼此之上，通过权重连接。MLP架构由一个输入层、一个或多个隐藏层和一个输出层组成（图3.1）。
- en: '![](../Images/3-1.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-1.png)'
- en: Figure 3.1 The MLP architecture consists of layers of neurons connected by weight
    connections.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 MLP架构由通过权重连接的神经元层组成。
- en: This section uses what you know about MLPs from chapter 2 to solve an image
    classification problem using the MNIST dataset. The goal of this classifier will
    be to classify images of digits from 0 to 9 (10 classes). To begin, let’s look
    at the three main components of our MLP architecture (input layer, hidden layers,
    and output layer).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将利用您从第2章了解的关于MLP的知识，使用MNIST数据集解决图像分类问题。这个分类器的目标将是将0到9的数字图像（10个类别）进行分类。首先，让我们看看我们MLP架构的三个主要组件（输入层、隐藏层和输出层）。
- en: 3.1.1 Input layer
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 输入层
- en: When we work with 2D images, we need to preprocess them into something the network
    can understand before feeding them to the network. First, let’s see how computers
    perceive images. In figure 3.2, we have an image 28 pixels wide × 28 pixels high.
    This image is seen by the computer as a 28 × 28 matrix, with pixel values ranging
    from 0 to 255 (0 for black, 255 for white, and the range in between for grayscale).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理2D图像时，在将它们输入网络之前，我们需要对它们进行预处理，使其成为网络可以理解的形式。首先，让我们看看计算机是如何感知图像的。在图3.2中，我们有一个宽度为28像素、高度为28像素的图像。计算机将这个图像视为一个28×28的矩阵，像素值范围从0到255（0为黑色，255为白色，介于两者之间的为灰度）。
- en: '![](../Images/3-2.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-2.png)'
- en: Figure 3.2 The computer sees this image as a 28 × 28 matrix of pixel values
    ranging from 0 to 255.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 计算机将这个图像视为一个像素值范围在0到255之间的28×28矩阵。
- en: 'Since MLPs only take as input 1D vectors with dimensions (1, *p*), they cannot
    take a raw 2D image matrix with dimensions (*x, y*). To fit the image in the input
    layer, we first need to transform our image into one large vector with the dimensions
    (1, *p*) that contains all the pixel values in the image. This process is called
    image flattening. In this example, the total number (*n*) of pixels in this image
    is 28 × 28 = 784\. Then, in order to feed this image to our network, we need to
    flatten the (28 × 28) matrix into one long vector with dimensions (1, 784). The
    input vector looks like this:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于MLP只接受维度为（1，*p*）的1D向量作为输入，因此它们不能接受维度为（*x, y*）的原始2D图像矩阵。为了将图像放入输入层，我们首先需要将我们的图像转换成一个包含图像中所有像素值的大向量，其维度为（1，*p*）。这个过程称为图像展平。在这个例子中，这个图像的总像素数（*n*）为28×28=784。然后，为了将这个图像输入到我们的网络中，我们需要将（28×28）矩阵展平成一个长向量，其维度为（1，784）。输入向量看起来如下：
- en: '*x* = [*row*1, *row*2, *row*3, ..., *row* 28]'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*x* = [*行*1, *行*2, *行*3, ..., *行* 28]'
- en: 'That said, the input layer in this example will have a total of 784 nodes:
    *x*[1], *x*[2], ..., *x*[784].'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，在这个例子中，输入层将包含总共784个节点：*x*[1]，*x*[2]，...，*x*[784]。
- en: Visualizing input vectors
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化输入向量
- en: 'To help visualize the flattened input vector, let’s look at a much smaller
    matrix (4, 4):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助可视化扁平化的输入向量，让我们看看一个更小的矩阵（4，4）：
- en: '![](../Images/3-unnumb-1.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-unnumb-1.png)'
- en: 'The input (*x*) is a flattened vector with the dimensions (1, 16):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 输入（*x*）是一个维度为（1，16）的扁平向量：
- en: '![](../Images/3-unnumb-2KEY.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-unnumb-2KEY.png)'
- en: 'So, if we have pixel values of 0 for black and 255 for white, the input vector
    will be as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们用0表示黑色，用255表示白色，输入向量将如下所示：
- en: Input = [0, 255, 255, 255, 0, 0, 0, 255, 0, 0, 255, 0, 0, 255, 0, 0]
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 输入 = [0, 255, 255, 255, 0, 0, 0, 255, 0, 0, 255, 0, 0, 255, 0, 0]
- en: 'Here is how we flatten an input image in Keras:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在Keras中展平输入图像的方法：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ As before, imports the Keras library
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如前所述，导入Keras库
- en: ❷ Imports a layer called Flatten to convert the image matrix into a vector
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 导入一个名为Flatten的层，将图像矩阵转换为向量
- en: ❸ Defines the model
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义模型
- en: ❹ Adds the Flatten layer, also known as the input layer
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 添加Flatten层，也称为输入层
- en: The `Flatten` layer in Keras handles this process for us. It takes the 2D image
    matrix input and converts it into a 1D vector. Note that the `Flatten` layer must
    be supplied a parameter value of the shape of the input image. Now the image is
    ready to be fed to the neural network.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Keras中的`Flatten`层为我们处理这个过程。它将2D图像矩阵输入转换为1D向量。请注意，`Flatten`层必须提供一个参数值，即输入图像的形状。现在图像已经准备好被输入到神经网络中。
- en: What’s next? Hidden layers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是什么？隐藏层。
- en: 3.1.2 Hidden layers
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 隐藏层
- en: As discussed in the previous chapter, the neural network can have one or more
    hidden layers (technically, as many as you want). Each layer has one or more neurons
    (again, as many as you want). Your main job as a neural network engineer is to
    design these layers. For the sake of this example, let’s say you decided to arbitrarily
    design the network to have two hidden layers, each having 512 nodes--and don’t
    forget to add the ReLU activation function for each hidden layer.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所述，神经网络可以有一个或多个隐藏层（技术上，可以有任意多个）。每一层有一个或多个神经元（同样，可以有任意多个）。作为神经网络工程师，你的主要任务是设计这些层。为了这个例子，让我们假设你决定任意设计网络，使其有两个隐藏层，每个隐藏层有512个节点--并且别忘了为每个隐藏层添加ReLU激活函数。
- en: Choosing an activation function
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 选择激活函数
- en: In chapter 2, we discussed the different types of activation functions in detail.
    As a DL engineer, you will often have a lot of different choices when you are
    building your network. Choosing the activation function that is the most suitable
    for the problem you are solving is one of these choices. While there is no single
    best answer that fits all problems, in most cases, the ReLU function performs
    best in the hidden layers; and for most classification problems where classes
    are mutually exclusive, softmax is generally a good choice in the output layer.
    The softmax function gives us the probability that the input image depicts one
    of the (*n*) classes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章中，我们详细讨论了不同类型的激活函数。作为一名深度学习工程师，当你构建你的网络时，你将经常面临许多不同的选择。选择最适合你正在解决的问题的激活函数是这些选择之一。虽然没有一种单一的答案适合所有问题，但在大多数情况下，ReLU函数在隐藏层中表现最佳；对于大多数类别互斥的分类问题，softmax函数通常在输出层是一个好的选择。softmax函数为我们提供了输入图像描述(*n*)个类别之一的概率。
- en: 'As in the previous chapter, let’s add two fully connected (also known as dense)
    layers, using Keras:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所述，让我们添加两个全连接层（也称为密集层），使用Keras：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Imports the Dense layer
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入Dense层
- en: ❷ Adds two Dense layers with 512 nodes each
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 添加两个每个有512个节点的Dense层
- en: 3.1.3 Output layer
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.3 输出层
- en: 'The output layer is pretty straightforward. In classification problems, the
    number of nodes in the output layer should be equal to the number of classes that
    you are trying to detect. In this problem, we are classifying 10 digits (0, 1,
    2, 3, 4, 5, 6, 7, 8, 9). Then we need to add one last `Dense` layer that contains
    10 nodes:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层相当直接。在分类问题中，输出层的节点数应该等于你试图检测的类别数。在这个问题中，我们正在对10个数字（0，1，2，3，4，5，6，7，8，9）进行分类。然后我们需要添加一个包含10个节点的最后一个`Dense`层：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 3.1.4 Putting it all together
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.4 整合所有内容
- en: When we put all these layers together, we get a neural network like the one
    in figure 3.3.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将所有这些层组合在一起时，我们得到一个如图3.3所示的神经网络。
- en: '![](../Images/3-3.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-3.png)'
- en: Figure 3.3 The neural network we create by combining the input, hidden, and
    output layers
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 通过组合输入、隐藏和输出层我们创建的神经网络
- en: 'Here is how it looks in Keras:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是它在Keras中的样子：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Imports the Keras library
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入Keras库
- en: ❷ Imports a Flatten layer to convert the image matrix into a vector
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 导入Flatten层以将图像矩阵转换为向量
- en: ❸ Defines the neural network architecture
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义神经网络架构
- en: ❹ Adds the Flatten layer
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 添加Flatten层
- en: ❺ Adds 2 hidden layers with 512 nodes each. Using the ReLU activation function
    is recommended in hidden layers.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 添加两个每个有512个节点的隐藏层。建议在隐藏层中使用ReLU激活函数。
- en: ❻ Adds 1 output Dense layer with 10 nodes. Using the softmax activation function
    is recommended in the output layer for multiclass classification problems.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 添加一个包含10个节点的输出Dense层。对于多类分类问题，建议在输出层使用softmax激活函数。
- en: ❼ Prints a summary of the model architecture
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 打印模型架构摘要
- en: When you run this code, you will see the model summary printed as shown in figure
    3.4.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这段代码时，你会看到如图3.4所示的模型摘要。
- en: You can see that the output of the flatten layer is a vector with 784 nodes,
    as discussed before, since we have 784 pixels in each 28 × 28 images. As designed,
    the hidden layers produce 512 nodes each; and, finally, the output layer (`dense_3`)
    produces a layer with 10 nodes.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，正如之前讨论的那样，flatten层的输出是一个包含784个节点的向量，因为每个28 × 28的图像中都有784个像素。按照设计，隐藏层每个都产生512个节点；最后，输出层（`dense_3`）产生一个包含10个节点的层。
- en: '![](../Images/3-4.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-4.png)'
- en: Figure 3.4 The model summary
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 模型摘要
- en: 'The Param # field represents the number of parameters (weights) produced at
    each layer. These are the weights that will be adjusted and learned during the
    training process. They are calculated as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 参数#字段表示每个层产生的参数（权重）的数量。这些是在训练过程中将被调整和学习的权重。它们的计算如下：
- en: Params after the flatten layer = 0, because this layer only flattens the image
    to a vector for feeding into the input layer. The weights haven’t been added yet.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: flatten层之后的参数数 = 0，因为这个层只是将图像展平成向量以供输入层使用。权重尚未添加。
- en: Params after layer 1 = (784 nodes in input layer) × (512 in hidden layer 1)
    + (512 connections to biases) = 401,920.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 层1之后的参数数 = （输入层中的784个节点）×（隐藏层1中的512个节点）+（到偏置的512个连接）= 401,920。
- en: Params after layer 2 = (512 nodes in hidden layer 1) × (512 in hidden layer
    2) + (512 connections to biases) = 262,656.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 层2之后的参数数 = （隐藏层1中的512个节点）×（隐藏层2中的512个节点）+（到偏置的512个连接）= 262,656。
- en: Params after layer 3= (512 nodes in hidden layer 2) × (10 in output layer) +
    (10 connections to biases) = 5,130.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 层3之后的参数数=（隐藏层2中的512个节点）×（输出层中的10个节点）+（到偏置的10个连接）= 5,130。
- en: Total params in the network = 401,920 + 262,656 + 5,130 = 669,706.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络中的总参数数 = 401,920 + 262,656 + 5,130 = 669,706。
- en: This means that in this tiny network, we have a total of 669,706 parameters
    (weights and biases) that the network needs to learn and whose values it needs
    to tune to optimize the error function. This is a huge number for such a small
    network. You can see how this number would grow out of control if we added more
    nodes and layers or used bigger images. This is one of the two major drawbacks
    of MLPs that we will discuss next.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在这个小小的网络中，我们总共有669,706个参数（权重和偏置）需要网络学习，并调整其值以优化误差函数。对于这样一个小的网络来说，这是一个巨大的数字。你可以看到，如果我们添加更多的节点和层或使用更大的图像，这个数字会如何失控。这是我们将在下面讨论的MLPs的两个主要缺点之一。
- en: MLPs vs. CNNs
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: MLPs与CNNs
- en: If you train the example MLP on the MNIST dataset, you will get pretty good
    results (close to 96% accuracy compared to 99% with CNNs). But MLPs and CNNs do
    not usually yield comparable results. The MNIST dataset is special because it
    is very clean and perfectly preprocessed. For example, all images have the same
    size and are centered in a 28 × 28 pixel grid. Also, the MNIST dataset contains
    only grayscale images. It would be a much harder task if the images had color
    or the digits were skewed or not centered.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将示例MLP在MNIST数据集上训练，你会得到相当好的结果（与CNNs的99%相比，接近96%的准确率）。但MLPs和CNNs通常不会产生可比较的结果。MNIST数据集是特殊的，因为它非常干净且预处理得很好。例如，所有图像都有相同的大小，并且位于一个28
    × 28像素网格的中心。此外，MNIST数据集只包含灰度图像。如果图像有颜色或数字倾斜或未居中，这将是一个更困难的任务。
- en: If you try the example MLP architecture with a slightly more complex dataset
    like CIFAR-10, as we will do in the project at the end of this chapter, the network
    will perform very poorly (around 30-40% accuracy). It performs even worse with
    more complex datasets. In messy real-world image data, CNNs truly outshine MLPs.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试使用稍微复杂一些的数据集，比如CIFAR-10，正如我们在本章末尾的项目中将要做的，网络的表现将非常差（大约30-40%的准确率）。在更复杂的数据集上表现会更差。在混乱的真实世界图像数据中，CNNs确实优于MLPs。
- en: 3.1.5 Drawbacks of MLPs for processing images
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.5 MLPs处理图像的缺点
- en: 'We are nearly ready to talk about the topic of this chapter: CNNs. But first,
    let’s discuss the two major problems in MLPs that convolutional networks are designed
    to fix.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎准备好讨论本章的主题：CNNs。但首先，让我们讨论MLPs中的两个主要问题，卷积网络旨在解决这些问题。
- en: Spatial feature loss
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 空间特征损失
- en: Flattening a 2D image to a 1D vector input results in losing the spatial features
    of the image. As we saw in the mini project earlier, before feeding an image to
    the hidden layers of an MLP, we must flatten the image matrix to a 1D vector.
    This means throwing away all the 2D information contained in the image. Treating
    an input as a simple vector of numbers with no special structure might work well
    for 1D signals; but in 2D images, it will lead to information loss because the
    network doesn’t relate the pixel values to each other when trying to find patterns.
    MLPs have no knowledge of the fact that these pixel numbers were originally spatially
    arranged in a grid and that they are connected to each other. CNNs, on the other
    hand, do not require a flattened image. We can feed the raw image matrix of pixels
    to a CNN network, and the CNN will understand that pixels that are close to each
    other are more heavily related than pixels that are far apart.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 将二维图像展平为1D向量输入会导致丢失图像的空间特征。正如我们在前面的迷你项目中看到的那样，在将图像输入到MLP的隐藏层之前，我们必须将图像矩阵展平为1D向量。这意味着丢弃图像中包含的所有2D信息。将输入视为没有特殊结构的简单数字向量可能对1D信号有效；但在二维图像中，这会导致信息丢失，因为当网络试图寻找模式时，它不会将像素值相互关联。MLP没有意识到这些像素数字最初是按网格空间排列的，并且它们彼此相连。另一方面，CNN不需要展平的图像。我们可以将原始像素图像矩阵输入到CNN网络中，CNN将理解彼此靠近的像素比彼此远离的像素关系更紧密。
- en: Let’s oversimplify things to learn more about the importance of spatial features
    in an image. Suppose we are trying to teach a neural network to identify the shape
    of a square, and suppose the pixel value 1 is white and 0 is black. When we draw
    a white square on a black background, the matrix will look like figure 3.5.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简化一下，以便更多地了解图像中空间特征的重要性。假设我们正在尝试教一个神经网络识别正方形的形状，假设像素值1是白色，0是黑色。当我们在一个黑色背景上画一个白色正方形时，矩阵将看起来像图3.5。
- en: '![](../Images/3-5.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-5.png)'
- en: Figure 3.5 If the pixel value 1 is white and 0 is black, this is what our matrix
    looks like for identifying a square.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 如果像素值1是白色，0是黑色，这是我们识别正方形时矩阵的样子。
- en: 'Since MLPs take a 1D vector as an input, we have to flatten the 2D image to
    a 1D vector. The input vector of figure 3.5 looks like this:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于MLP以1D向量作为输入，我们必须将2D图像展平为1D向量。图3.5的输入向量看起来是这样的：
- en: Input vector = [1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 输入向量 = [1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
- en: When the training is complete, the network will learn to identify a square only
    when the input nodes x1, x2, x5, and x6 are fired. But what happens when we have
    new images with square shapes located in different areas in the image, as shown
    in figure 3.6?
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练完成时，网络将学会仅在输入节点x1、x2、x5和x6被激活时识别正方形。但是，当我们有如图3.6所示的新图像，其中正方形形状位于图像的不同区域时，会发生什么？
- en: '![](../Images/3-6.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-6.png)'
- en: Figure 3.6 Square shapes in different areas of the image
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 图像不同区域的正方形形状
- en: The MLP will have no idea that these are the shapes of squares because the network
    didn’t learn the square shape as a feature. Instead, it learned the input nodes
    that, when fired, might lead to a square shape. If we want our network to learn
    squares, we need a lot of square shapes located everywhere in the image. You can
    see how this solution won’t scale for complex problems.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: MLP将无法知道这些是正方形的形状，因为网络没有将正方形形状作为特征学习。相反，它学习了当被激活时可能导致正方形形状的输入节点。如果我们想让我们的网络学习正方形，我们需要在图像的各个位置放置大量的正方形形状。你可以看到这种解决方案对于复杂问题来说不会扩展。
- en: 'Another example of feature learning is this: if we want to teach a neural network
    to recognize cats, then ideally, we want the network to learn all the shapes of
    cat features regardless of where they appear on the image (ears, nose, eyes, and
    so on). This only happens when the network looks at the image as a set of pixels
    that, when close to each other, are heavily related.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 特征学习的另一个例子是：如果我们想教一个神经网络识别猫，那么理想情况下，我们希望网络学习猫的所有形状特征，无论它们出现在图像的哪个位置（耳朵、鼻子、眼睛等）。这只有在网络将图像视为一组像素时才会发生，当这些像素彼此靠近时，它们关系密切。
- en: The mechanism of how CNNs learn will be explained in detail in this chapter.
    But figure 3.7 shows how the network learns features throughout its layers.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的学习机制将在本章中详细解释。但图3.7显示了网络如何在其层中学习特征。
- en: '![](../Images/3-7.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-7.png)'
- en: Figure 3.7 CNNs learn the image features through its layers.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 CNN通过其层学习图像特征。
- en: Fully connected (dense) layers
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 完全连接（密集）层
- en: MLPs are composed of dense layers that are fully connected to each other. Fully
    connected means every node in one layer is connected to all nodes of the previous
    layer and all nodes in the next layer. In this scenario, each neuron has parameters
    (weights) to train per neuron from the previous layer. While this is not a big
    problem for the MNIST dataset because the images are really small in size (28
    × 28), what happens when we try to process larger images? For example, if we have
    an image with dimensions 1,000 × 1,000, it will yield 1 million parameters for
    each node in the first hidden layer. So if the first hidden layer has 1,000 neurons,
    this will yield 1 billion parameters even in such a small network. You can imagine
    the computational complexity of optimizing 1 billion parameters after only the
    first layer. This number will increase drastically when we have tens or hundreds
    of layers. This can get out of control pretty fast and will not scale.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器（MLPs）由密集层组成，这些层之间是完全连接的。完全连接意味着一个层中的每个节点都与前一层的所有节点以及下一层的所有节点相连。在这种情况下，每个神经元都有参数（权重）需要从前一层的每个神经元进行训练。虽然这对MNIST数据集来说不是大问题，因为图像尺寸真的很小（28
    × 28），但当我们尝试处理更大的图像时会发生什么？例如，如果我们有一个1,000 × 1,000像素的图像，它将为第一隐藏层中的每个节点产生一百万个参数。所以如果第一隐藏层有1,000个神经元，那么即使在如此小的网络中，这也会产生十亿个参数。你可以想象在只有第一层之后优化十亿个参数的计算复杂性。当我们有数十或数百层时，这个数字将急剧增加。这可能会很快失控，并且不会按比例增长。
- en: 'CNNs, on the other hand, are locally connected layers, as figure 3.8 shows:
    nodes are connected to only a small subset of the previous layers’ nodes. Locally
    connected layers use far fewer parameters than densely connected layers, as you
    will see.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如图3.8所示，CNN是局部连接层：节点仅与前一层的节点的小子集相连。局部连接层使用的参数比密集连接层少得多，正如你将看到的。
- en: '![](../Images/3-8.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图3-8](../Images/3-8.png)'
- en: Figure 3.8 (Left) Fully connected neural network where all neurons are connected
    to all pixels of the image. (Right) Locally connected network where only a subset
    of pixels is connected to each neuron. These subsets are called sliding windows.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8（左）所有神经元都与图像的所有像素相连的完全连接神经网络。（右）局部连接网络，其中只有一小部分像素与每个神经元相连。这些子集被称为滑动窗口。
- en: What does it all mean?
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 所有这些意味着什么？
- en: The loss of information caused by flattening a 2D image matrix to a 1D vector
    and the computational complexity of fully connected layers with larger images
    suggest that we need an entirely new way of processing image input, one where
    2D information is not entirely lost. This is where convolutional networks come
    in. CNNs accept the full image matrix as input, which significantly helps the
    network understand the patterns contained in the pixel values.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 将二维图像矩阵展平为1D向量所造成的信息损失以及完全连接层在处理更大图像时的计算复杂性表明，我们需要一种全新的图像输入处理方式，其中二维信息不会完全丢失。这就是卷积网络发挥作用的地方。CNN接受完整的图像矩阵作为输入，这显著帮助网络理解像素值中包含的图案。
- en: 3.2 CNN architecture
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 CNN架构
- en: 'Regular neural networks contain multiple layers that allow each layer to find
    successively complex features, and this is the way CNNs work. The first layer
    of convolutions learns some basic features (edges and lines), the next layer learns
    features that are a little more complex (circles, squares, and so on), the following
    layer finds even more complex features (like parts of the face, a car wheel, dog
    whiskers, and the like), and so on. You will see this demonstrated shortly. For
    now, know that the CNN architecture follows the same pattern as neural networks:
    we stack neurons in hidden layers on top of each other; weights are randomly initiated
    and learned during network training; and we apply activation functions, calculate
    the error (*y − ŷ*), and backpropagate the error to update the weights. This process
    is the same. The difference is that we use convolutional layers instead of regular
    fully connected layers for the feature-learning part.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正规神经网络包含多个层，允许每一层依次找到更复杂的特征，这正是卷积神经网络（CNNs）的工作方式。卷积的第一层学习一些基本特征（边缘和线条），下一层学习稍微复杂一些的特征（如圆形、正方形等），接下来的层找到更复杂的特征（如面部的一部分、汽车轮子、狗的胡须等），依此类推。你很快就会看到这个演示。现在，要知道CNN架构遵循与神经网络相同的模式：我们在隐藏层中堆叠神经元；权重在网络训练期间随机初始化并学习；我们应用激活函数，计算误差（*y
    − ŷ*），并将误差反向传播以更新权重。这个过程是相同的。不同之处在于，我们在特征学习部分使用卷积层而不是常规的完全连接层。
- en: 3.2.1 The big picture
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 整体概念
- en: Before we look in detail at the CNN architecture, let’s back up for a moment
    to see the big picture (figure 3.9). Remember the image classification pipeline
    we discussed in chapter 1?
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们详细探讨CNN架构之前，让我们先退一步，看看整体情况（图3.9）。还记得我们在第一章中讨论过的图像分类流程吗？
- en: '![](../Images/3-9.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-9.png)'
- en: 'Figure 3.9 The image classification pipeline consists of four components: data
    input, data preprocessing, feature extraction, and the ML algorithm.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 图像分类流程包括四个组件：数据输入、数据预处理、特征提取和机器学习算法。
- en: Before deep learning (DL), we used to manually extract features from images
    and then feed the resulting feature vector to a classifier (a regular ML algorithm
    like SVM). With the magic that neural networks provide, we can replace the manual
    work of step 3 in figure 3.9 with a neural network (MLP or CNN) that does both
    feature learning and classification (steps 3 and 4).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习（DL）之前，我们通常手动从图像中提取特征，然后将得到的特征向量输入到分类器（如SVM等常规机器学习算法）中。有了神经网络提供的魔力，我们可以用神经网络（MLP或CNN）来替换图3.9中步骤3的手动工作，该神经网络既能进行特征学习又能进行分类（步骤3和4）。
- en: 'We saw earlier, in the digit-classification project, how to use MLP to learn
    features and classify an image (steps 3 and 4 together). It turned out that our
    issue with fully connected layers was not the classification part--fully connected
    layers do that very well. Our issue was in the way fully connected layers process
    the image to learn features. Let’s get a little creative: we’ll keep what’s working
    and make modifications to what’s not working. The fully connected layers aren’t
    doing a good job of feature extraction (step 3), so let’s replace that with locally
    connected layers (convolutional layers). On the other hand, fully connected layers
    do a great job of classifying the extracted features (step 4), so let’s keep them
    for the classification part.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在数字分类项目中看到，如何使用MLP来学习特征并对图像进行分类（步骤3和4合并）。结果证明，我们与全连接层的问题并不在于分类部分——全连接层在这方面做得很好。我们的问题是全连接层处理图像以学习特征的方式。让我们有点创意：我们将保留有效部分，并对无效部分进行修改。全连接层在特征提取（步骤3）方面做得不好，所以让我们用局部连接层（卷积层）来替换它。另一方面，全连接层在分类提取的特征（步骤4）方面做得很好，所以让我们保留它们用于分类部分。
- en: 'The high-level architecture of CNNs looks like figure 3.10:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的高级架构看起来像图3.10：
- en: Input layer
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层
- en: Convolutional layers for feature extraction
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于特征提取的卷积层
- en: Fully connected layers for classification
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于分类的全连接层
- en: Output prediction
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出预测
- en: '![](../Images/3-10.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-10.png)'
- en: 'Figure 3.10 The CNN architecture consists of the following: input layer, convolutional
    layers, fully connected layers, and output prediction.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 CNN架构包括以下部分：输入层、卷积层、全连接层和输出预测。
- en: 'Remember, we are still talking about the big picture. We will dive into each
    of these components soon. In figure 3.10, suppose we are building a CNN to classify
    images into two classes: the numbers 3 and 7\. Look at the figure, and follow
    along with these steps:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们还在讨论整体情况。我们很快就会深入到每个组件。在图3.10中，假设我们正在构建一个CNN来将图像分类为两个类别：数字3和7。看看图中的内容，并按照以下步骤进行：
- en: Feed the raw image to the convolutional layers.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始图像输入到卷积层中。
- en: The image passes through the CNN layers to detect patterns and extract features
    called feature maps. The output of this step is then flattened to a vector of
    the learned features of the image. Notice that the image dimensions shrink after
    each layer, and the number of feature maps (the layer depth) increases until we
    have a long array of small features in the last layer of the feature-extraction
    part. Conceptually, you can think of this step as the neural network learning
    to represent more abstract features of the original image.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像通过CNN层来检测模式和提取称为特征图的特征。这一步骤的输出随后被展平成一个包含图像学习特征的向量。请注意，图像的维度在每一层之后都会缩小，特征图的数量（层深度）增加，直到我们在特征提取部分的最后一层得到一个长数组的小特征。从概念上讲，你可以将这一步骤视为神经网络学习表示原始图像的更抽象特征。
- en: The flattened feature vector is fed to the fully connected layers to classify
    the extracted features of the image.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将展平的特征向量输入到全连接层中，以对图像提取的特征进行分类。
- en: 'The neural network fires the node that represents the correct prediction of
    the image. Note that in this example, we are classifying two classes (3 and 7).
    Thus the output layer will have two nodes: one to represent the digit 3, and one
    for the digit 7.'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络激活代表图像正确预测的节点。注意，在这个例子中，我们正在对两个类别（3 和 7）进行分类。因此，输出层将有两个节点：一个代表数字 3，另一个代表数字
    7。
- en: 'DEFINITION The basic idea of neural networks is that neurons learn features
    from the input. In CNNs, a feature map is the output of one filter applied to
    the previous layer. It is called a feature map because it is a mapping of where
    a certain kind of feature is found in the image. CNNs look for features such as
    straight lines, edges, or even objects. Whenever they spot these features, they
    report them to the feature map. Each feature map is looking for something specific:
    one could be looking for straight lines and another for curves.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义** 神经网络的基本思想是神经元从输入中学习特征。在 CNN 中，特征图是应用于前一层的某个滤波器的输出。它被称为特征图，因为它表示了图像中某种特征的位置。CNN
    寻找的特征包括直线、边缘，甚至是物体。每当它们发现这些特征时，它们就会将它们报告给特征图。每个特征图都在寻找特定的事物：一个可能是在寻找直线，另一个可能是在寻找曲线。'
- en: 3.2.2 A closer look at feature extraction
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 深入了解特征提取
- en: You can think of the feature-extraction step as breaking large images into smaller
    pieces of features and stacking them into a vector. For example, an image of the
    digit 3 is one image (depth = 1) and is broken into smaller images that contain
    specific features of the digit 3 (figure 3.11). If it is broken into four features,
    then the depth equals 4\. As the image passes through the CNN layers, it shrinks
    in dimensions, and the layer gets deeper because it contains more images of smaller
    features.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将特征提取步骤想象成将大图像分割成包含特征的小块，并将它们堆叠成一个向量。例如，数字 3 的图像是一个图像（深度 = 1），它被分割成包含数字 3
    的特定特征的小图像（图 3.11）。如果它被分割成四个特征，那么深度等于 4。随着图像通过 CNN 层，它在维度上缩小，层变得更深，因为它包含了更多的小特征图像。
- en: '![](../Images/3-11.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-11.png)'
- en: Figure 3.11 An image is broken into smaller images that contain distinctive
    features.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11 图像被分割成包含独特特征的小图像。
- en: Note that this is just a metaphor to help visualize the feature-extraction process.
    CNNs don’t literally break an image into pieces. Instead, they extract meaningful
    features that separate this object from other images in the training set, and
    stack them in an array of features.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这只是一个比喻，用来帮助可视化特征提取过程。CNN 并非字面意义上将图像分割成碎片。相反，它们提取有意义的特征，这些特征将这个对象与其他训练集中的图像区分开来，并将它们堆叠在一个特征数组中。
- en: 3.2.3 A closer look at classification
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 深入了解分类
- en: 'After feature extraction is complete, we add fully connected layers (a regular
    MLP) to look at the features vector and say, “The first feature (top) has what
    looks like an edge: this could be 3, or 7, or maybe an ugly 2\. I’m not sure;
    let’s look at the second feature. Hmm, this is definitely not a 7 because it has
    a curve,” and so on until the MLP is confident that the image is the digit 3.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征提取完成后，我们添加全连接层（一个常规的 MLP）来查看特征向量，并说，“第一个特征（顶部）看起来像一条边缘：这可能是一个 3，或者 7，或者可能是一个难看的
    2。我不确定；让我们看看第二个特征。嗯，这肯定不是一个 7，因为它有一个曲线，”等等，直到 MLP 确信图像是数字 3。
- en: How CNNs learn patterns
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 学习模式的方式
- en: It is important to note that a CNN doesn’t go from the image input to the features
    vector directly in one layer. This usually happens in tens or hundreds of layers,
    as you will see later in this chapter. The feature-learning process happens step
    by step after each hidden layer. So the first layer usually learns very basic
    features like lines and edges, and the second assembles those lines into recognizable
    shapes, corners, and circles. Then, in the deeper layers, the network learns more
    complex shapes such as human hands, eyes, ears, and so on. For example, here is
    a simplified version of how CNNs learn faces.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，CNN 并不是直接在一层中将图像输入转换为特征向量。这通常发生在数十或数百层中，正如你将在本章后面看到的那样。特征学习过程在每个隐藏层之后逐步发生。因此，第一层通常学习非常基本的特征，如线条和边缘，第二层将这些线条组装成可识别的形状、角落和圆形。然后，在更深的层中，网络学习更复杂的形状，如人手、眼睛、耳朵等。例如，这里是一个
    CNN 学习人脸的简化版本。
- en: '![](../Images/3-unnumb-3.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-unnumb-3.png)'
- en: A simplified version of how CNNs learn faces
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 学习人脸的简化版本
- en: 'You can see that the early layers detect patterns in the image to learn low-level
    features like edges, and the later layers detect patterns within patterns to learn
    more complex features like parts of the face, then patterns within patterns within
    patterns, and so on:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，早期层检测图像中的模式以学习低级特征，如边缘，而后期层检测模式中的模式以学习更复杂的特征，如面部的一部分，然后是模式中的模式中的模式，等等：
- en: Input image
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像
- en: + Layer 1 ⇒ patterns
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: + 层1 ⇒ 模式
- en: + Layer 2 ⇒ patterns within patterns
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: + 层2 ⇒ 模式中的模式
- en: + Layer 3 ⇒ patterns within patterns within patterns
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: + 层3 ⇒ 模式中的模式中的模式
- en: '... and so on'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '...等等'
- en: This concept will come in handy when we discuss more advanced CNN architectures
    in later chapters. For now, know that in neural networks, we stack hidden layers
    to learn patterns from each other until we have an array of meaningful features
    to identify the image.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在后面的章节中讨论更高级的CNN架构时，这个概念将非常有用。现在，你知道在神经网络中，我们堆叠隐藏层来相互学习模式，直到我们有一个包含识别图像的具有意义的特征数组。
- en: 3.3 Basic components of a CNN
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 CNN的基本组件
- en: 'Without further ado, let’s discuss the main components of a CNN architecture.
    There are three main types of layers that you will see in almost every convolutional
    network (figure 3.12):'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 不再拖延，让我们讨论CNN架构的主要组件。你几乎在每一个卷积网络（图3.12）中都会看到三种主要类型的层：
- en: Convolutional layer (CONV)
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积层（CONV）
- en: Pooling layer (POOL)
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 池化层（POOL）
- en: Fully connected layer (FC)
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 全连接层（FC）
- en: CNN text representation
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: CNN文本表示
- en: 'The text representation of the architecture in figure 3.12 goes like this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12中架构的文本表示如下：
- en: 'CNN architecture: INPUT ⇒ CONV ⇒ RELU ⇒ POOL ⇒ CONV ⇒ RELU ⇒ POOL ⇒ FC ⇒ SOFTMAX'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: CNN架构：输入 ⇒ CONV ⇒ RELU ⇒ POOL ⇒ CONV ⇒ RELU ⇒ POOL ⇒ FC ⇒ SOFTMAX
- en: Note that the ReLU and softmax activation functions are not really standalone
    layers--they are the activation functions used in the previous layer. The reason
    they are shown this way in the text representation is to call out that the CNN
    designer is using the ReLU activation function in the convolutional layers and
    softmax activation in the fully connected layer. So this represents a CNN architecture
    that contains two convolutional layers plus one fully connected layer. You can
    add as many convolutional and fully connected layers as you see fit. The convolutional
    layers are for feature learning or extraction, and the fully connected layers
    are for classification.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，ReLU和softmax激活函数并不是真正的独立层——它们是前一层中使用的激活函数。它们在文本表示中这样显示的原因是为了指出CNN设计者正在卷积层中使用ReLU激活函数，在全连接层中使用softmax激活函数。因此，这代表了一个包含两个卷积层和一个全连接层的CNN架构。你可以添加你认为合适的任意数量的卷积层和全连接层。卷积层用于特征学习或提取，而全连接层用于分类。
- en: '![](../Images/3-12.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图3-12](../Images/3-12.png)'
- en: Figure 3.12 The basic components of convolutional networks are convolutional
    layers and pooling layers to perform feature extraction, and fully connected layers
    for classification.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 卷积网络的基本组件包括卷积层和池化层用于特征提取，以及全连接层用于分类。
- en: Now that we’ve seen the full architecture of a convolutional network, let’s
    dive deeper into each of the layer types to get a deeper understanding of how
    they work. Then at the end of this section, we will put them all back together.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了卷积网络的完整架构，让我们深入探讨每种层类型的工作原理。然后在本节的最后，我们将它们全部组合起来。
- en: 3.3.1 Convolutional layers
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 卷积层
- en: A convolutional layer is the core building block of a convolutional neural network.
    Convolutional layers act like a feature finder window that slides over the image
    pixel by pixel to extract meaningful features that identify the objects in the
    image.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层是卷积神经网络的核心构建块。卷积层就像一个特征查找窗口，逐像素地在图像上滑动以提取识别图像中对象的具有意义的特征。
- en: What is convolution?
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 什么是卷积？
- en: In mathematics, convolution is the operation of two functions to produce a third
    modified function. In the context of CNNs, the first function is the input image,
    and the second function is the convolutional filter. We will perform some mathematical
    operations to produce a modified image with new pixel values.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学中，卷积是两个函数的操作，以产生第三个修改后的函数。在CNN的上下文中，第一个函数是输入图像，第二个函数是卷积滤波器。我们将执行一些数学运算以产生具有新像素值的修改后的图像。
- en: Let’s zoom in on the first convolutional layer to see how it processes an image
    (figure 3.13). By sliding the convolutional filter over the input image, the network
    breaks the image into little chunks and processes those chunks individually to
    assemble the modified image, a feature map.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们放大查看第一个卷积层，看看它是如何处理图像的（图3.13）。通过在输入图像上滑动卷积滤波器，网络将图像分解成小块，并单独处理这些块以组装修改后的图像，即特征图。
- en: '![](../Images/3-13.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图像3-13](../Images/3-13.png)'
- en: Figure 3.13 A 3 × 3 convolutional filter is sliding over the input image.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13 一个3 × 3的卷积滤波器在输入图像上滑动。
- en: 'Keeping this diagram in mind, here are some facts about convolution filters:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这个图，以下是一些关于卷积滤波器的事实：
- en: The small 3 × 3 matrix in the middle is the convolution filter, also called
    a kernel.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中间的3 × 3小矩阵是卷积滤波器，也称为核。
- en: The kernel slides over the original image pixel by pixel and does some math
    calculations to get the values of the new “convolved” image on the next layer.
    The area of the image that the filter convolves is called the receptive field
    (see figure 3.14).
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核在原始图像上逐像素滑动，并进行一些数学计算，以获取下一层“卷积”的新图像的值。滤波器卷积的图像区域称为感受野（见图3.14）。
- en: '![](../Images/3-14.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图像3-14](../Images/3-14.png)'
- en: Figure 3.14 The kernel slides over the original image pixel by pixel and calculates
    the convolved image on the next layer. The convolved area is called the receptive
    field.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14 核在原始图像上逐像素滑动，并在下一层计算卷积图像。卷积区域称为感受野。
- en: What are the kernel values? In CNNs, the convolution matrix is the weights.
    This means they are also randomly initialized and the values are learned by the
    network (so you will not have to worry about assigning its values).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是核值？在卷积神经网络（CNNs）中，卷积矩阵是权重。这意味着它们也是随机初始化的，值是由网络学习的（所以你不必担心为其分配值）。
- en: Convolutional operations
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 卷积操作
- en: The math should look familiar from our discussion of MLPs. Remember how we multiplied
    the input by the weights and summed them all together to get the weighted sum?
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们对多层感知器（MLPs）的讨论中，数学应该看起来很熟悉。记得我们是如何将输入乘以权重并将它们全部相加以得到加权总和的吗？
- en: weighted sum = *x*[1] · *w*[1] + *x*[2] · *w*[2] + *x*[3] · *w*[3] + ... + *x[n]*
    · *w[n]* + *b*
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 加权总和 = *x*[1] · *w*[1] + *x*[2] · *w*[2] + *x*[3] · *w*[3] + ... + *x[n]* ·
    *w[n]* + *b*
- en: 'We do the same thing here, except that in CNNs, the neurons and weights are
    structured in a matrix shape. So we multiply each pixel in the receptive field
    by the corresponding pixel in the convolution filter and sum them all together
    to get the value of the center pixel in the new image (figure 3.15). This is the
    same matrix dot product we saw in chapter 2:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里做同样的事情，只是在CNN中，神经元和权重以矩阵形状结构化。因此，我们将感受野中的每个像素与卷积滤波器中的对应像素相乘，并将它们全部相加，以得到新图像中中心像素的值（图3.15）。这与我们在第二章中看到的相同的矩阵点积：
- en: (93 × -1) + (139 × 0) + (101 × 1) + (26 × -2) + (252 × 0) + (196 × 2) + (135
    × -1) + (240 × 0) + (48 × 1) = 243
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: (93 × -1) + (139 × 0) + (101 × 1) + (26 × -2) + (252 × 0) + (196 × 2) + (135
    × -1) + (240 × 0) + (48 × 1) = 243
- en: The filter (or kernel) slides over the whole image. Each time, we multiply every
    corresponding pixel element-wise and then add them all together to create a new
    image with new pixel values. This convolved image is called a feature map or activation
    map.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 滤波器（或核）在整个图像上滑动。每次，我们逐个元素地乘以每个对应的像素，然后将它们全部相加，以创建一个具有新像素值的新图像。这个卷积后的图像称为特征图或激活图。
- en: '![](../Images/3-15.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图像3-15](../Images/3-15.png)'
- en: Figure 3.15 Multiplying each pixel in the receptive field by the corresponding
    pixel in the convolution filter and summing them gives the value of the center
    pixel in the new image.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.15 将感受野中的每个像素与卷积滤波器中的对应像素相乘，并将它们相加，得到新图像中中心像素的值。
- en: Applying filters to learn features
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 应用滤波器以学习特征
- en: 'Let’s not lose focus of the initial goal. We are doing all this so the network
    extracts features from the image. How does applying filters lead toward this goal?
    In image processing, filters are used to filter out unwanted information or amplify
    features in an image. These filters are matrices of numbers that convolve with
    the input image to modify it. Look at this edge-detection filter:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们不要偏离最初的目标。我们做所有这些是为了让网络从图像中提取特征。应用滤波器是如何达到这个目标的？在图像处理中，滤波器用于过滤掉不需要的信息或放大图像中的特征。这些滤波器是数字矩阵，它们与输入图像卷积以修改它。看看这个边缘检测滤波器：
- en: '![](../Images/3-unnumb-4.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图像3-unnumb-4](../Images/3-unnumb-4.png)'
- en: When this kernel (K) is convolved with the input image F(x,y), it creates a
    new convolved image (a feature map) that amplifies the edges.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个核（K）与输入图像F(x,y)进行卷积时，它创建了一个新的卷积图像（特征图），放大了边缘。
- en: '![](../Images/3-unnumb-5.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-unnumb-5.png)'
- en: Applying an edge detection kernel on an image
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像上应用边缘检测核
- en: To understand how the convolution happens, let’s zoom in on a small piece of
    the image.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解卷积是如何发生的，让我们放大图像的一小部分。
- en: '![](../Images/3-unnumb-6KEY.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-unnumb-6KEY.png)'
- en: Calculations for applying an edge kernel on an input image
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入图像上应用边缘核的计算
- en: This image shows the convolution calculations in one area of the image to compute
    the value of one pixel. We compute the values of all the pixels by sliding the
    kernel over the input image pixel by pixel and applying the same convolution process.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图像显示了图像某一区域的卷积计算，以计算一个像素的值。我们通过将核在输入图像上逐像素滑动并应用相同的卷积过程来计算所有像素的值。
- en: These kernels are often called weights because they determine how important
    a pixel is in forming a new output image. Similar to what we discussed about MLP
    and weights, these weights represent the importance of the feature on the output.
    In images, the input features are the pixel values.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这些核通常被称为权重，因为它们决定了像素在形成新输出图像中的重要性。类似于我们在关于MLP和权重的讨论中提到的，这些权重代表了特征在输出中的重要性。在图像中，输入特征是像素值。
- en: 'Other filters can be applied to detect different types of features. For example,
    some filters detect horizontal edges, others detect vertical edges, still others
    detect more complex shapes like corners, and so on. The point is that these filters,
    when applied in the convolutional layers, yield the feature-learning behavior
    we discussed earlier: first they learn simple features like edges and straight
    lines, and later layers learn more complex features.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 可以应用其他过滤器来检测不同类型的特征。例如，一些过滤器检测水平边缘，其他检测垂直边缘，还有一些检测更复杂的形状，如角，等等。关键是这些过滤器在卷积层中的应用会产生我们之前讨论过的特征学习行为：首先学习简单的特征，如边缘和直线，然后更深的层学习更复杂的特征。
- en: We are basically done with the concept of filter. That is all there is to it!
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基本上完成了滤波器的概念。这就是全部内容！
- en: 'Now, let’s take a look at the convolutional layer as a whole: Each convolutional
    layer contains one or more convolutional filters. The number of filters in each
    convolutional layer determines the depth of the next layer, because each filter
    produces its own feature map (convolved image). Let’s look at the convolutional
    layers in Keras to see how they work:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们整体看看卷积层：每个卷积层包含一个或多个卷积滤波器。每个卷积层的滤波器数量决定了下一层的深度，因为每个滤波器都会产生自己的特征图（卷积图像）。让我们看看Keras中的卷积层，看看它们是如何工作的：
- en: '[PRE4]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And there you have it. One line of code creates the convolutional layer. We
    will see where this line fits in the full code later in this chapter. Let’s stay
    focused on the convolutional layer. As you can see from the code, the convolutional
    layer takes five main arguments. As mentioned in chapter 2, it is recommended
    that we use the ReLU activation function in the neural networks’ hidden layers.
    That’s one argument out of the way. Now, let’s explain the remaining four hyperparameters
    that control the size and depth of the output volume:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。一行代码就创建了一个卷积层。我们将在本章后面看到这条线在完整代码中的位置。让我们专注于卷积层。从代码中可以看出，卷积层有五个主要参数。正如第2章所述，我们建议在神经网络的隐藏层中使用ReLU激活函数。这样，一个参数就解决了。现在，让我们解释剩下的四个超参数，它们控制输出体积的大小和深度：
- en: 'Filters: the number of convolutional filters in each layer. This represents
    the depth of its output.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤器：每层的卷积滤波器数量。这代表了其输出的深度。
- en: 'Kernel size: the size of the convolutional filter matrix. Sizes vary: 2 × 2,
    3 × 3, 5 × 5.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核大小：卷积滤波器矩阵的大小。大小各异：2 × 2，3 × 3，5 × 5。
- en: Stride.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步长。
- en: Padding.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充。
- en: We will discuss strides and padding in the next section. But now, let’s look
    at each of these four hyperparameters.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节讨论步长和填充。但现在，让我们看看这四个超参数中的每一个。
- en: NOTE As you learned in chapter 2 on deep learning, hyperparameters are the knobs
    you tune (increase and decrease) when configuring your neural network to improve
    performance.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：正如你在第2章关于深度学习的学习中了解到的，超参数是在配置神经网络以改进性能时调整（增加和减少）的旋钮。
- en: Number of filters in the convolutional layer
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 卷积层中的滤波器数量
- en: Each convolutional layer has one or more filters. To understand this, let’s
    review MLPs from chapter 2\. Remember how we stacked neurons in hidden layers,
    and each hidden layer has n number of neurons (also called hidden units)? Figure
    3.16 shows the MLP diagram from chapter 2.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 每个卷积层有一个或多个滤波器。为了理解这一点，让我们回顾第2章中的MLP。记得我们是如何在隐藏层中堆叠神经元的，每个隐藏层有n个神经元（也称为隐藏单元）？图3.16展示了第2章中的MLP图。
- en: '![](../Images/3-16.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-16.png)'
- en: Figure 3.16 Neurons are stacked in hidden layers, and each hidden layer has
    n neurons (hidden units).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16 神经元在隐藏层中堆叠，每个隐藏层有n个神经元（隐藏单元）。
- en: Similarly, with CNNs, the convolutional layers are the hidden layers. And to
    increase the number of neurons in hidden layers, we increase the number of kernels
    in convolutional layers. Each kernel unit is considered a neuron. For example,
    if we have a 3 × 3 kernel in the convolutional layer, this means we have 9 hidden
    units in this layer. When we add another 3 × 3 kernel, we have 18 hidden units.
    Add another one, and we have 27, and so on. So, by increasing the number of kernels
    in a convolutional layer, we increase the number of hidden units, which makes
    our network more complex and able to detect more complex patterns. The same was
    true when we added more neurons (hidden units) to the hidden layers in the MLP.
    Figure 3.17 provides a representation of the CNN layers that shows the number-of-kernels
    idea.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在CNN中，卷积层是隐藏层。为了增加隐藏层中的神经元数量，我们增加卷积层中的核数量。每个核单元被视为一个神经元。例如，如果我们卷积层中有3 ×
    3的核，这意味着在这一层我们有9个隐藏单元。当我们添加另一个3 × 3核时，我们就有18个隐藏单元。再添加一个，我们就有27个，以此类推。因此，通过增加卷积层中的核数量，我们增加了隐藏单元的数量，这使得我们的网络更加复杂，能够检测更复杂的模式。当我们向MLP的隐藏层添加更多的神经元（隐藏单元）时，情况也是如此。图3.17展示了CNN层，显示了核数量的概念。
- en: '![](../Images/3-17.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-17.png)'
- en: Figure 3.17 Representation of the CNN layers that shows the number-of-kernels
    idea
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17 展示CNN层核数量概念的表示
- en: Kernel size
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 核大小
- en: Remember that a convolution filter is also known as a kernel. It is a matrix
    of weights that slides over the image to extract features. The kernel size refers
    to the dimensions of the convolution filter (width times height; figure 3.18).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，卷积滤波器也称为核。它是一个权重矩阵，在图像上滑动以提取特征。核大小指的是卷积滤波器的维度（宽度乘以高度；图3.18）。
- en: '![](../Images/3-18.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-18.png)'
- en: Figure 3.18 The kernel size refers to the dimensions of the convolution filter.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18 核大小指的是卷积滤波器的维度。
- en: '`kernel_size` is one of the hyperparameters that you will be setting when building
    a convolutional layer. Like most neural network hyperparameters, no single best
    answer fits all problems. The intuition is that smaller filters will capture very
    fine details of the image, and bigger filters will miss minute details in the
    image.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`kernel_size` 是你在构建卷积层时将要设置的超参数之一。像大多数神经网络超参数一样，没有单一的最好答案适用于所有问题。直观地说，较小的滤波器会捕捉到图像的非常精细的细节，而较大的滤波器会错过图像中的微小细节。'
- en: Remember that filters contain the weights that will be learned by the network.
    So, theoretically, the bigger the `kernel_size`, the deeper the network, which
    means the better it learns. However, this comes with higher computational complexity
    and might lead to overfitting.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，滤波器包含网络将要学习的权重。因此，从理论上讲，`kernel_size` 越大，网络越深，这意味着它学得越好。然而，这伴随着更高的计算复杂度，可能会导致过拟合。
- en: Kernel filters are almost always square and range from the smallest at 2 × 2
    to the largest at 5 × 5\. Theoretically, you can use bigger filters, but this
    is not preferred because it results in losing important image details.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 内核滤波器几乎总是正方形，大小从最小的2 × 2到最大的5 × 5不等。从理论上讲，你可以使用更大的滤波器，但这并不推荐，因为它会导致丢失重要的图像细节。
- en: Tuning
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 调整
- en: 'I don’t want you to get overwhelmed with all the hyperparameter tuning. Deep
    learning is really an art as well as a science. I can’t emphasize this enough:
    most of your work as a DL engineer will be spent not building the actual algorithms,
    but rather building your network architecture and setting, experimenting, and
    tuning your hyperparameters. A great deal of research today is focused on trying
    to find the optimal topologies and parameters for a CNN, given a type of problem.
    Fortunately, the problem of tuning hyperparameters doesn’t have to be as hard
    as it might seem. Throughout the book, I will indicate good starting points for
    using hyperparameters and help you develop an instinct for evaluating your model
    and analyzing its results to know which knob (hyperparameter) you need to tune
    (increase or decrease).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我不希望你被所有的超参数调整所压倒。深度学习实际上是一门艺术，也是一门科学。我无法强调这一点：作为深度学习工程师，你大部分的工作将不是构建实际的算法，而是构建你的网络架构和设置，进行实验，调整你的超参数。今天，大量的研究都集中在尝试为给定类型的问题找到
    CNN 的最佳拓扑结构和参数。幸运的是，调整超参数的问题不必像看起来那么困难。在整个书中，我将指出使用超参数的良好起点，并帮助你培养评估你的模型和分析其结果的本能，以了解你需要调整哪个旋钮（超参数）（增加或减少）。
- en: Strides and padding
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步长和填充
- en: 'You will usually think of these two hyperparameters together, because they
    both control the shape of the output of a convolutional layer. Let’s see how:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你通常会一起考虑这两个超参数，因为它们都控制卷积层输出的形状。让我们看看如何：
- en: '*Strides* --The amount by which the filter slides over the image. For example,
    to slide the convolution filter one pixel at a time, the strides value is 1\.
    If we want to jump two pixels at a time, the strides value is 2\. Strides of 3
    or more are uncommon and rare in practice. Jumping pixels produces smaller output
    volumes spatially.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步长* -- 滤波器在图像上滑动的量。例如，每次滑动一个像素，步长值是 1。如果我们想每次跳过两个像素，步长值就是 2。步长为 3 或更大的情况在实践中很少见。跳过像素会产生较小的空间输出体积。'
- en: Strides of 1 will make the output image roughly the same height and width of
    the input image, while strides of 2 will make the output image roughly half of
    the input image size. I say “roughly” because it depends on what you set the padding
    parameter to do with the edge of the image.
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步长为 1 将使输出图像大致与输入图像的宽度和高度相同，而步长为 2 将使输出图像大致是输入图像大小的一半。我说“大致”是因为这取决于你如何设置填充参数来处理图像的边缘。
- en: '*Padding* --Often called zero-padding because we add zeros around the border
    of an image (figure 3.19). Padding is most commonly used to allow us to preserve
    the spatial size of the input volume so the input and output width and height
    are the same. This way, we can use convolutional layers without necessarily shrinking
    the height and width of the volumes. This is important for building deeper networks,
    since, otherwise, the height/width would shrink as we went to deeper layers.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*填充* -- 通常称为零填充，因为我们会在图像的边缘添加零（如图 3.19 所示）。填充最常用于允许我们保留输入体积的空间大小，以便输入和输出的宽度和高度相同。这样，我们可以在不必要缩小体积的高度和宽度的情况下使用卷积层。这对于构建更深的网络很重要，因为否则，高度/宽度会随着我们进入更深的层而缩小。'
- en: '![](../Images/3-19.png)'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图像](../Images/3-19.png)'
- en: Figure 3.19 Zero-padding adds zeros around the border of the image. Padding
    = 2 adds two layers of zeros around the border.
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.19 零填充在图像的边缘添加零。填充 = 2 在边缘添加两层零。
- en: 'NOTE The goal when using strides and padding hyperparameters is one of two
    things: keep all the important details of the image and transfer them to the next
    layer (when the `strides` value is `1` and the `padding` value is `same`); or
    ignore some of the spatial information of the image to make the processing computationally
    more affordable. Note that we will be adding the pooling layer (discussed next)
    to reduce the size of the image to focus on the extracted features. For now, know
    that strides and padding hyperparameters are meant to control the behavior of
    the convolutional layer and the size of its output: whether to pass on all image
    details or ignore some of them.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：使用步长和填充超参数的目标是以下两个之一：保留图像的所有重要细节并将它们传递到下一层（当步长值为 `1` 且填充值为 `same` 时）；或者忽略图像的一些空间信息，以使处理在计算上更经济。请注意，我们将添加池化层（将在下一节讨论）以减小图像的大小，以便关注提取的特征。目前，请了解步长和填充超参数的目的是控制卷积层的行为及其输出的大小：是传递所有图像细节还是忽略其中的一些。
- en: 3.3.2 Pooling layers or subsampling
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 池化层或子采样
- en: Adding more convolutional layers increases the depth of the output layer, which
    leads to increasing the number of parameters that the network needs to optimize
    (learn). You can see that adding several convolutional layers (usually tens or
    even hundreds) will produce a huge number of parameters (weights). This increase
    in network dimensionality increases the time and space complexity of the mathematical
    operations that take place in the learning process. This is when pooling layers
    come in handy. Subsampling or pooling helps reduce the size of the network by
    reducing the number of parameters passed to the next layer. The pooling operation
    resizes its input by applying a summary statistical function, such as a maximum
    or average, to reduce the overall number of parameters passed on to the next layer.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 添加更多的卷积层会增加输出层的深度，这会导致网络需要优化的参数（学习）数量增加。你可以看到，添加几个卷积层（通常是几十甚至几百）会产生大量的参数（权重）。这种网络维度的增加会增加学习过程中发生的数学运算的时间和空间复杂度。这就是池化层派上用场的时候。子采样或池化通过减少传递给下一层的参数数量来帮助减小网络的大小。池化操作通过应用总结统计函数（如最大值或平均值）来调整其输入的大小，从而减少传递给下一层的参数总数。
- en: The goal of the pooling layer is to downsample the feature maps produced by
    the convolutional layer into a smaller number of parameters, thus reducing computational
    complexity. It is a common practice to add pooling layers after every one or two
    convolutional layers in the CNN architecture (figure 3.20).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层的目标是将卷积层产生的特征图下采样到更少的参数数量，从而降低计算复杂度。在CNN架构中，在每层卷积层之后或之后每两层卷积层之后添加池化层是一种常见的做法（图3.20）。
- en: '![](../Images/3-20.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-20.png)'
- en: Figure 3.20 Pooling layers are commonly added after every one or two convolutional
    layers.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.20 池化层通常在每个卷积层之后或之后每两个卷积层之后添加。
- en: Max pooling vs. average pooling
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 最大池化与平均池化比较
- en: 'There are two main types of pooling layers: max pooling and average pooling.
    We will discuss max pooling first.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层主要有两种类型：最大池化和平均池化。我们首先讨论最大池化。
- en: Similar to convolutional kernels, max pooling kernels are windows of a certain
    size and strides value that slide over the image. The difference with max pooling
    is that the windows don’t have weights or any values. All they do is slide over
    the feature map created by the previous convolutional layer and select the max
    pixel value to pass along to the next layer, ignoring the remaining values. In
    figure 3.21, you see a pooling filter with a size of 2 × 2 and strides of 2 (the
    filter jumps 2 pixels when sliding over the image). This pooling layer reduces
    the feature map size from 4 × 4 down to 2 × 2.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 与卷积核类似，最大池化核是具有一定大小和步长值的窗口，在图像上滑动。与最大池化的不同之处在于，窗口没有权重或任何值。它们所做的只是滑动到由前一个卷积层创建的特征图上，并选择最大像素值传递到下一层，忽略其他值。在图3.21中，你可以看到一个大小为2×2、步长为2的池化滤波器（滤波器在滑动图像时跳过2个像素）。这个池化层将特征图的大小从4×4减少到2×2。
- en: '![](../Images/3-21.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-21.png)'
- en: Figure 3.21 A 2 × 2 pooling filter and strides of 2, reducing the feature map
    from 4 × 4 to 2 × 2
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.21 一个2×2的池化滤波器和步长为2，将特征图从4×4减少到2×2
- en: When we do that to all the feature maps in the convolutional layer, we get maps
    of smaller dimensions (width times height), but the depth of the layer is kept
    the same because we apply the pooling filter to each of the feature maps from
    the previous filter. So if the convolutional layer has three feature maps, the
    output of the pooling layer will also have three feature maps, but of smaller
    size (figure 3.22).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将此操作应用于卷积层中的所有特征图时，我们得到维度更小的图（宽度乘以高度），但层的深度保持不变，因为我们将对每个来自前一个滤波器的特征图应用池化滤波器。因此，如果卷积层有三个特征图，池化层的输出也将有三个特征图，但尺寸更小（图3.22）。
- en: '![](../Images/3-22.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-22.png)'
- en: Figure 3.22 If the convolutional layer has three feature maps, the pooling layer’s
    output will have three smaller feature maps.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.22 如果卷积层有三个特征图，池化层的输出将包含三个更小的特征图。
- en: Global average pooling is a more extreme type of dimensionality reduction. Instead
    of setting a window size and strides, global average pooling calculates the average
    values of all pixels in the feature map (figure 3.23). You can see in figure 3.24
    that the global average pooling layer takes a 3D array and turns it into a vector.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 全局平均池化是一种更极端的降维类型。与设置窗口大小和步长不同，全局平均池化计算特征图中所有像素的平均值（图3.23）。在图3.24中，你可以看到全局平均池化层将一个3D数组转换成一个向量。
- en: '![](../Images/3-23.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-23.png)'
- en: Figure 3.23 Global average pooling calculates the average values of all the
    pixels in a feature map.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.23 全局平均池化计算特征图中所有像素的平均值。
- en: '![](../Images/3-24.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-24.png)'
- en: Figure 3.24 The global average pooling layer turns a 3D array into a vector.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.24 全局平均池化层将3D数组转换为向量。
- en: Why use a pooling layer?
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么使用池化层？
- en: As you can see from the examples we have discussed, pooling layers reduce the
    dimensionality of our convolutional layers. The reason it is important to reduce
    dimensionality is that in complex projects, CNNs contain many convolutional layers,
    and each has tens or hundreds of convolutional filters (kernels). Since the kernel
    contains the parameters (weights) that the network learns, this can get out of
    control very quickly, and the dimensionality of our convolutional layers can get
    very large. So adding pooling layers helps keep the important features and pass
    them along to the next layer, while shrinking image dimensionality. Think of pooling
    layers as image-compressing programs. They reduce the image resolution while keeping
    its important features (figure 3.25)..
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们从所讨论的示例中可以看到，池化层降低了卷积层的维度。降低维度之所以重要，是因为在复杂的项目中，CNN包含许多卷积层，每个都有数十或数百个卷积滤波器（核）。由于核包含网络学习的参数（权重），这可能会迅速失控，我们的卷积层维度可能会变得非常大。因此，添加池化层有助于保持重要特征并将它们传递到下一层，同时缩小图像维度。将池化层想象成图像压缩程序。它们在保持图像重要特征的同时降低图像分辨率（图3.25）。
- en: '![](../Images/3-25.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-25.png)'
- en: Figure 3.25 Pooling layers reduce image resolution and keep the image’s important
    features.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.25 池化层降低图像分辨率并保留图像的重要特征。
- en: Pooling vs. strides and padding
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 池化与步长和填充
- en: 'The main purpose of pooling and strides is to reduce the number of parameters
    in the neural network. The more parameters we have, the more computationally expensive
    the training process will be. Many people dislike the pooling operation and think
    that we can get away without it in favor of tuning strides and padding the convolutional
    layer. For example, “Striving for Simplicity: The All Convolutional Net”a proposes
    discarding the pooling layer in favor of architecture that only consists of repeated
    convolutional layers. To reduce the size of the representation, the authors suggest
    occasionally using larger strides in the convolutional layer. Discarding pooling
    layers has also been found helpful in training good generative models, such as
    generative adversarial networks (GANs), which we will discuss in chapter 10\.
    It seems likely that future architectures will feature very few to no pooling
    layers. But for now, pooling layers are still widely used to downsample images
    from one layer to the next.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 池化和步长的主要目的是减少神经网络中的参数数量。我们拥有的参数越多，训练过程就越昂贵。许多人不喜欢池化操作，认为我们可以通过调整步长和填充卷积层来避免它。例如，“追求简单：全卷积网络”a
    提出丢弃池化层，转而采用仅由重复卷积层组成的架构。为了减少表示的大小，作者建议在卷积层中偶尔使用较大的步长。丢弃池化层也被发现有助于训练良好的生成模型，例如生成对抗网络（GANs），我们将在第10章中讨论。似乎未来的架构将具有非常少的池化层。但到目前为止，池化层仍然被广泛用于将图像从一层下采样到下一层。
- en: 'a Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller,
    “Striving for Simplicity: The All Convolutional Net,” [https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806).'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: a Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, 和 Martin Riedmiller,
    “追求简单：全卷积网络”，[https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806)。
- en: Convolutional and pooling layers recap
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 卷积和池化层回顾
- en: 'Let’s review what we have done so far. Up until this point, we used a series
    of convolutional and pooling layers to process an image and extract meaningful
    features that are specific to the images in the training dataset. To summarize
    how we got here:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下到目前为止我们所做的工作。到目前为止，我们使用一系列卷积和池化层来处理图像并提取训练数据集中图像的特定有意义特征。为了总结我们是如何到达这里的：
- en: The raw image is fed to the convolutional layer, which is a set of kernel filters
    that slide over the image to extract features.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 原始图像被输入到卷积层，这是一个在图像上滑动以提取特征的核滤波器集合。
- en: 'The convolutional layer has the following attributes that we need to configure:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积层具有以下我们需要配置的属性：
- en: '[PRE5]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`filters` is the number of kernel filters in each layer (the depth of the hidden
    layer).'
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filters` 是每层中核滤波器的数量（隐藏层的深度）。'
- en: '`kernel_size` is the size of the filter (aka kernel). Usually 2, or 3, or 5.'
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kernel_size` 是过滤器的尺寸（也称为核）。通常为2、3或5。'
- en: '`strides` is the amount by which the filter slides over the image. A `strides`
    value of 1 or 2 is usually recommended as a good start.'
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strides` 表示过滤器在图像上滑动的量。通常建议从1或2开始作为良好的起点。'
- en: '`padding` adds columns and rows of zero values around the border of the image
    to reserve the image size in the next layer.'
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` 在图像的边缘添加零值的列和行，以保留下一层的图像大小。'
- en: '`activation` of `relu` is strongly recommended in the hidden layers.'
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在隐藏层中强烈推荐使用`relu`的`activation`。
- en: 'The pooling layer has the following attributes that we need to configure:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 池化层具有以下属性，我们需要进行配置：
- en: '[PRE6]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: And we keep adding pairs of convolutional and pooling layers to achieve the
    required depth for our “deep” neural network.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续添加成对的卷积和池化层，以达到我们“深度”神经网络所需的深度。
- en: Visualize what happens after each layer
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化每层之后发生的情况
- en: After the convolutional layers, the image keeps its width and height dimensions
    (usually), but it gets deeper and deeper after each layer. Why? Remember the cutting-the-image-into-pieces-of-features
    analogy we mentioned earlier? That is what’s happening after the convolutional
    layer.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积层之后，图像保持其宽度和高度维度（通常是），但每经过一层都会变得更深。为什么？还记得我们之前提到的将图像切割成特征块的类比吗？这就是卷积层之后发生的事情。
- en: 'For example, suppose the input image is 28 × 28 (like in the MNIST dataset).
    When we add a CONV_1 layer (with `filters` of 4, `strides` of 1, and `padding`
    of `same`), the output will be the same width and height dimensions but with `depth`
    of 4 (28 × 28 × 4). Now we add a CONV_2 layer with the same hyperparameters but
    more filters (12), and we get deeper output: 28 × 28 × 12.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设输入图像是28 × 28（如MNIST数据集所示）。当我们添加一个CONV_1层（具有4个`filters`、1个`strides`和`same`的`padding`）时，输出将具有相同的宽度和高度维度，但`depth`为4（28
    × 28 × 4）。现在我们添加一个具有相同超参数但更多过滤器的CONV_2层，我们得到更深的输出：28 × 28 × 12。
- en: '![](../Images/3-unnumb-7key.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-unnumb-7key.png)'
- en: 'After the pooling layers, the image keeps its depth but shrinks in width and
    height:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在池化层之后，图像保持其深度，但宽度和高度会缩小：
- en: '![](../Images/3-unnumb-8KEY.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-unnumb-8KEY.png)'
- en: 'Putting the convolutional and pooling together, we get something like this:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 将卷积和池化结合在一起，我们得到如下内容：
- en: '![](../Images/3-unnumb-9key.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-unnumb-9key.png)'
- en: This keeps happening until we have, at the end, a long tube of small shaped
    images that contain all the features in the original image.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这会一直发生，直到最后我们得到一个包含原始图像所有特征的细长图像管。
- en: The output of the convolutional and pooling layers produces a feature tube (5
    × 5 × 40) that is almost ready to be classified. We use 40 here as an example
    for the depth of the feature tube, as in 40 feature maps. The last step is to
    flatten this tube before feeding it to the fully connected layer for classification.
    As discussed earlier, the flattened layer will have the dimensions of (1, m) where
    *m* = 5 × 5 × 40 = 1,000 neurons.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积和池化层的输出产生一个特征管（5 × 5 × 40），几乎可以用于分类。在这里我们使用40作为特征管深度的示例，即40个特征图。最后一步是在将其输入到全连接层进行分类之前将这个管子展平。如前所述，展平层将具有（1，m）的维度，其中
    *m* = 5 × 5 × 40 = 1,000个神经元。
- en: 3.3.3 Fully connected layers
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.3 全连接层
- en: After passing the image through the feature-learning process using convolutional
    and pooling layers, we have extracted all the features and put them in a long
    tube. Now it is time to use these extracted features to classify images. We will
    use the regular neural network architecture, MLP, that we discussed in chapter
    2.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过卷积和池化层对图像进行特征学习过程之后，我们已经提取了所有特征并将它们放入一个长管中。现在是我们使用这些提取的特征来对图像进行分类的时候了。我们将使用第2章中讨论的常规神经网络架构，MLP。
- en: Why use fully connected layers?
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么使用全连接层？
- en: MLPs work great in classification problems. The reason we used convolutional
    layers in this chapter is that MLPs lose a lot of valuable information when extracting
    features from an image--we have to flatten the image before feeding it to the
    network--whereas convolutional layers can process raw images. Now we have the
    features extracted, and after we flatten them, we can use regular MLPs to classify
    them.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: MLP在分类问题中表现良好。我们在这章中使用卷积层的原因是，当从图像中提取特征时，MLP会丢失大量有价值的信息——我们必须在将图像输入网络之前将其展平——而卷积层可以处理原始图像。现在我们已经提取了特征，并在将它们展平后，我们可以使用常规的MLP对它们进行分类。
- en: 'We discussed the MLP architecture thoroughly in chapter 2: nothing new here.
    To reiterate, here are the fully connected layers (figure 3.26):'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第2章中详细讨论了MLP架构：这里没有新的内容。为了重申，以下是全连接层（图3.26）：
- en: Input flattened vector --As illustrated in figure 3.26, to feed the features
    tube to the MLP for classification, we flatten it to a vector with the dimensions
    (1, *p*). For example, if the features tube has the dimensions of 5 × 5 × 40,
    the flattened vector will be (1, 1000).
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入展平向量——如图3.26所示，为了将特征管输入到MLP进行分类，我们需要将其展平成一个维度为（1，*p*）的向量。例如，如果特征管的维度为5 × 5
    × 40，则展平后的向量将是（1，1000）。
- en: Hidden layer --We add one or more fully connected layers, and each layer has
    one or more neurons (similar to what we did when we built regular MLPs).
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层——我们添加一个或多个全连接层，每个层有一个或多个神经元（类似于我们在构建常规MLPs时所做的）。
- en: 'Output layer --Chapter 2 recommended using the softmax activation function
    for classification problems involving more than two classes. In this example,
    we are classifying digits from 0 to 9: 10 classes. The number of neurons in the
    output layer is equal to the number of classes; thus, the output layer will have
    10 nodes.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层——第2章建议对于涉及两个以上类别的分类问题使用softmax激活函数。在这个例子中，我们正在对0到9的数字进行分类：10个类别。输出层中的神经元数量等于类别的数量；因此，输出层将有10个节点。
- en: '![](../Images/3-26.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-26.png)'
- en: Figure 3.26 Fully connected layers for an MLP
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.26 MLP的全连接层
- en: MLPs and fully connected layers
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: MLPs和全连接层
- en: Remember from chapter 2 that multilayer perceptrons (MLPs) are also called fully
    connected layers, because all the nodes from one layer are connected to all the
    nodes in the previous and next layers. They are also called dense layers. The
    terms MLP, fully connected, dense, and sometimes feedforward are used interchangeably
    to refer to the regular neural network architecture.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 记住第2章的内容，多层感知器（MLPs）也被称为全连接层，因为一层的所有节点都与前一层和后一层的所有节点相连。它们也被称为密集层。MLP、全连接、密集以及有时正向传播这些术语可以互换使用，以指代常规神经网络架构。
- en: '![](../Images/3-unnumb-10key.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-unnumb-10key.png)'
- en: 3.4 Image classification using CNNs
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 使用CNN进行图像分类
- en: Okay, you are now fully equipped to build your own CNN model to classify images.
    For this mini project, which is a simple problem but which will help build the
    foundation to more complex problems in the following chapters, we will use the
    MNIST dataset. (The MNIST dataset is like “Hello World” for deep learning.)
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，你现在已经完全准备好构建自己的CNN模型来对图像进行分类。对于这个小型项目，这是一个简单的问题，但将有助于为后续章节中更复杂的问题打下基础，我们将使用MNIST数据集。（MNIST数据集就像是深度学习的“Hello
    World”。）
- en: NOTE Regardless of which DL library you decide to use, the concepts are pretty
    much the same. You start with designing the CNN architecture in your mind or on
    a piece of paper, and then you begin stacking layers on top of each other and
    setting their parameters. Both Keras and MXNet (along with TensorFlow, PyTorch,
    and other DL libraries) have pros and cons that we will discuss later, but the
    concepts are similar. So for the rest of this book, we will be working mostly
    with Keras with a little overview of other libraries here and there.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：无论你决定使用哪个深度学习库，概念基本上是相同的。你首先在心中或在一张纸上设计CNN架构，然后开始堆叠层并设置它们的参数。Keras和MXNet（以及TensorFlow、PyTorch和其他深度学习库）都有其优缺点，我们将在后面讨论，但概念是相似的。因此，在本书的剩余部分，我们将主要使用Keras，并在适当的地方简要介绍其他库。
- en: 3.4.1 Building the model architecture
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 构建模型架构
- en: 'This is the part in your project where you define and build the CNN model architecture.
    To look at the full code of the project that includes image preprocessing, training,
    and evaluating the model, go to the book’s GitHub repo at [https://github.com/
    moelgendy/deep_learning_for_vision_systems](https://github.com/moelgendy/deep_learning_for_vision_systems)
    and open the mnist_cnn notebook or go to the book’s website: [www.manning.com/books/deep-learning-for-vision-systems](http://www.manning.com/books/deep-learning-for-vision-systems)
    or [www.computerVisionBook.com](http://www.computerVisionBook.com). At this point,
    we are concerned with the code that builds the model architecture. At the end
    of this chapter, we will build an end-to-end image classifier and dive deeper
    into the other pieces:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您项目中定义和构建 CNN 模型架构的部分。要查看包含图像预处理、训练和评估模型的完整项目代码，请访问本书的 GitHub 仓库 [https://github.com/moelgendy/deep_learning_for_vision_systems](https://github.com/moelgendy/deep_learning_for_vision_systems)，打开
    mnist_cnn 笔记本，或访问本书的网站：[www.manning.com/books/deep-learning-for-vision-systems](http://www.manning.com/books/deep-learning-for-vision-systems)
    或 [www.computerVisionBook.com](http://www.computerVisionBook.com)。在此阶段，我们关注的是构建模型架构的代码。在本章末尾，我们将构建一个端到端图像分类器，并深入探讨其他部分：
- en: '[PRE7]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Builds the model object
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建 model 对象
- en: '❷ CONV_1: adds a convolutional layer with ReLU activation and depth = 32 kernels'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '❷ CONV_1: 添加一个具有 ReLU 激活和深度 = 32 个核的卷积层'
- en: '❸ POOL_1: downsamples the image to choose the best features'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '❸ POOL_1: 对图像进行下采样以选择最佳特征'
- en: '❹ CONV_2: increases the depth to 64'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '❹ CONV_2: 增加深度到 64'
- en: '❺ POOL_2: more downsampling'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '❺ POOL_2: 更多的下采样'
- en: ❻ Flatten, since there are too many dimensions; we only want a classification
    output
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ Flatten，因为维度太多；我们只想得到分类输出
- en: '❼ FC_1: Fully connected to get all relevant data'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '❼ FC_1: 完全连接以获取所有相关数据'
- en: '❽ FC_2: Outputs a softmax to squash the matrix into output probabilities for
    the 10 classes'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '❽ FC_2: 输出 softmax 以将矩阵压缩为 10 个类别的输出概率'
- en: ❾ Prints the model architecture summary
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 打印模型架构摘要
- en: When you run this code, you will see the model summary printed as in figure
    3.27.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行此代码时，您将看到如图 3.27 所示的模型摘要被打印出来。
- en: '![](../Images/3-27.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-27.png)'
- en: Figure 3.27 The printed model summary
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.27 打印的模型摘要
- en: 'Following are some general observations before we look at the model summary:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看模型摘要之前，有一些一般性的观察：
- en: We need to pass the `input_shape` argument to the first convolutional layer
    only. Then we don’t need to declare the input shape to the model, since the output
    of the previous layer is the input of the current layer--it is already known to
    the model.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们只需要将 `input_shape` 参数传递给第一个卷积层。然后我们不需要向模型声明输入形状，因为前一个层的输出是当前层的输入——它已经为模型所知。
- en: 'You can see that the output of every convolutional and pooling layer is a 3D
    tensor of shape (`None`, `height`, `width`, `channels`). The `height` and `width`
    `values` are pretty straightforward: they are the dimensions of the image at this
    layer. The `channels` value represents the depth of the layer. This is the number
    of feature maps in each layer. The first value in this tuple, set to `None`, is
    the number of images that are processed in this layer. Keras sets this to `None`,
    which means this dimension is variable and accepts any number of `batch_size`.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以看到每个卷积层和池化层的输出都是一个形状为 (`None`, `height`, `width`, `channels`) 的3D张量。`height`
    和 `width` 的值非常直观：它们是图像在这一层的维度。`channels` 值表示层的深度。这代表每个层中的特征图数量。这个元组中的第一个值设置为 `None`，表示在这一层中处理图像的数量。Keras
    将其设置为 `None`，这意味着这个维度是可变的，可以接受任何数量的 `batch_size`。
- en: As you can see in the Output Shape columns, as you go deeper through the network,
    the image dimensions shrink and the depth increases, as we discussed earlier in
    this chapter.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如您在输出形状列中看到的，随着您在网络中深入，图像维度缩小，深度增加，正如我们在本章前面讨论的那样。
- en: 'Notice the number of total params (weights) that the network needs to optimize:
    220,234, compared to the number of params from the MLP network we created earlier
    in this chapter (669,706). We were able to cut it down to almost a third.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意网络需要优化的总参数（权重）数量：220,234，与我们本章早期创建的 MLP 网络的参数数量（669,706）相比。我们将其减少了近三分之一。
- en: 'Let’s take a look at the model summary line by line:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行查看模型摘要：
- en: '`CONV_1`--We know the input shape: (28 × 28 × 1). Look at the output shape
    of `conv2d`: (28 × 28 × 32). Since we set the `strides` parameter to `1` and `padding`
    to `same`, the dimensions of the input image did not change. But `depth` increased
    to 32\. Why? Because we added 32 filters in this layer. Each filter produces one
    feature map.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CONV_1`--我们知道输入形状：(28 × 28 × 1)。看看`conv2d`的输出形状：(28 × 28 × 32)。由于我们将`strides`参数设置为`1`，`padding`设置为`same`，输入图像的尺寸没有改变。但`深度`增加到32。为什么？因为我们在本层添加了32个过滤器。每个过滤器产生一个特征图。'
- en: '`POOL_1`--The input of this layer is the output of its previous layer: (28
    × 28 × 32). After the pooling layer, the image dimensions shrink, and `depth`
    stays the same. Since we used a 2 × 2 pool, the output shape is (14 × 14 × 32).'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`POOL_1`--本层的输入是其前一层的输出：(28 × 28 × 32)。经过池化层后，图像尺寸缩小，但`深度`保持不变。由于我们使用了2 × 2的池化，输出形状为(14
    × 14 × 32)。'
- en: '`CONV_2`-- Same as before, convolutional layers increase depth and keep dimensions.
    The input from the previous layer is (14 × 14 × 32). Since the filters in this
    layer are set to 64, the output is (14 × 14 × 64).'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CONV_2`--与之前相同，卷积层增加深度并保持尺寸。前一层输入为(14 × 14 × 32)。由于本层的过滤器设置为64，输出为(14 × 14
    × 64)。'
- en: '`POOL_2`--Same 2 × 2 pool, keeping the depth and shrinking the dimensions.
    The output is (7 × 7 × 64).'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`POOL_2`--与之前相同，2 × 2池化保持深度并缩小尺寸。输出为(7 × 7 × 64)。'
- en: '`Flatten`--Flattening a features tube that has dimensions of (7 × 7 × 64) converts
    it into a flat vector of dimensions (1, 3136).'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Flatten`--将具有(7 × 7 × 64)维度的特征管扁平化，将其转换为(1, 3136)维度的平坦向量。'
- en: '`Dense_1`--We set this fully connected layer to have 64 neurons, so the output
    is 64.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dense_1`--我们将这个全连接层设置为64个神经元，因此输出为64。'
- en: '`Dense_2`--This is the output layer that we set to 10 neurons, since we have
    10 classes.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dense_2`--这是输出层，我们将其设置为10个神经元，因为我们有10个类别。'
- en: 3.4.2 Number of parameters (weights)
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 参数数量（权重）
- en: 'Okay, now we know how to build the model and read the summary line by line
    to see how the image shape changes as it passes through the network layers. One
    important thing remains: the Param # column on the right in the model summary.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '好的，现在我们知道如何构建模型，并逐行阅读摘要以查看图像形状如何随着通过网络层而变化。还有一个重要的事情需要注意：模型摘要中右侧的`Param #`列。'
- en: What are the parameters?
  id: totrans-318
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 参数是什么？
- en: Parameters is just another name for weights. These are the things that your
    network learns. As we discussed in chapter 2, the network’s goal is to update
    the weight values during the gradient descent and backpropagation processes until
    it finds the optimal parameter values that minimize the error function.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 参数只是权重的一个名称。这些是网络学习的东西。正如我们在第2章中讨论的，网络的目的是在梯度下降和反向传播过程中更新权重值，直到找到最小化误差函数的最优参数值。
- en: How are these parameters calculated?
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这些参数是如何计算的？
- en: 'In MLP, we know that the layers are fully connected to each other, so the weight
    connections or edges are simply calculated by multiplying the number of neurons
    in each layer. In CNNs, weight calculations are not as straightforward. Fortunately,
    there is an equation for this:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在MLP中，我们知道层之间是完全连接的，因此权重连接或边简单地通过乘以每层的神经元数量来计算。在CNN中，权重计算并不那么直接。幸运的是，有一个方程可以用来计算：
- en: number of params = filters × kernel size × depth of the previous layer + number
    of filters (for biases)
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 参数数量 = 过滤器数量 × 核大小 × 前一层深度 + 过滤器数量（对于偏差）
- en: 'Let’s apply this equation in an example. Suppose we want to calculate the parameters
    at the second layer of the previous mini project. Here is the code for `CONV_2`
    again:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来应用这个方程。假设我们想要计算之前的小项目第二层的参数。这是`CONV_2`的代码再次：
- en: '[PRE8]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Since we know that the depth of the previous layer is 32, then
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道前一层深度为32，那么
- en: ⇒ Params = 64 × 3 × 3 × 32 + 64 = 18,496
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ⇒ 参数 = 64 × 3 × 3 × 32 + 64 = 18,496
- en: 'Note that the pooling layers do not add any parameters. Hence, you will see
    the Param # value is 0 after the pooling layers in the model summary. The same
    is true for the flatten layer: no extra weights are added (figure 3.28).'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '注意，池化层不添加任何参数。因此，在模型摘要中，您将看到池化层后的`Param #`值为0。对于扁平化层也是如此：没有添加额外的权重（见图3.28）。'
- en: '![](../Images/3-28.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![图3.28](../Images/3-28.png)'
- en: 'Figure 3.28 Pooling and flatten layers don’t add parameters, so Param # is
    0 after pooling and flattening layers in the model summary.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '图3.28 池化和扁平化层不添加参数，因此在模型摘要中池化和扁平化层后的`Param #`值为0。'
- en: 'When we add all the parameters in the Param # column, we get the total number
    of parameters that this network needs to optimize: 220,234.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将参数编号列中的所有参数相加时，我们得到这个网络需要优化的参数总数：220,234。
- en: Trainable and non-trainable params
  id: totrans-331
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可训练和不可训练参数
- en: In the model summary, you will see the total number of params and, below it,
    the number of trainable and non-trainable params. The trainable params are the
    weights that this neural network needs to optimize during the training process.
    In this example, all our params are trainable (figure 3.29).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型摘要中，您将看到参数总数，以及其下方可训练和不可训练参数的数量。可训练参数是神经网络在训练过程中需要优化的权重。在本例中，我们所有的参数都是可训练的（图3.29）。
- en: '![](../Images/3-29.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-29.png)'
- en: Figure 3.29 All of our params are trainable and need to be optimized during
    training.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.29 我们所有的参数都是可训练的，需要在训练过程中进行优化。
- en: 'In later chapters, we will talk about using a pretrained network and combining
    it with your own network for faster and more accurate results: in such a case,
    you may decide to freeze some layers because they are pretrained. So, not all
    of the network params will be trained. This is useful for understanding the memory
    and space complexity of your model before starting the training process; but more
    on that later. As far as we know now, all our params are trainable.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在后面的章节中，我们将讨论使用预训练网络并将其与您自己的网络结合以获得更快、更准确的结果：在这种情况下，您可能决定冻结一些层，因为它们是预训练的。因此，并非所有网络参数都将进行训练。这在开始训练过程之前了解您模型的内存和空间复杂度很有用；但更多内容将在后面讨论。据我们所知，我们所有的参数都是可训练的。
- en: 3.5 Adding dropout layers to avoid overfitting
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 添加dropout层以避免过拟合
- en: 'So far, you have been introduced to the three main layers of CNNs: convolution,
    pooling, and fully connected. You will find these three layer types in almost
    every CNN architecture. But that’s not all of them--there are additional layers
    that you can add to avoid overfitting.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经介绍了CNN的三个主要层：卷积、池化和全连接。您几乎可以在每个CNN架构中找到这三种层类型。但这还不是全部——还有额外的层可以添加以避免过拟合。
- en: 3.5.1 What is overfitting?
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.1 什么是过拟合？
- en: 'The main cause of poor performance in machine learning is either overfitting
    or underfitting the data. Underfitting is as the name implies: the model fails
    to fit the training data. This happens when the model is too simple to fit the
    data: for example, using one perceptron to classify a nonlinear dataset.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习性能不佳的主要原因要么是过拟合，要么是欠拟合数据。欠拟合正如其名：模型无法拟合训练数据。这种情况发生在模型过于简单，无法拟合数据：例如，使用一个感知器对一个非线性数据集进行分类。
- en: 'Overfitting, on the other hand, means fitting the data too much: memorizing
    the training data and not really learning the features. This happens when we build
    a super network that fits the training dataset perfectly (very low error while
    training) but fails to generalize to other data samples that it hasn’t seen before.
    You will see that, in overfitting, the network performs very well in the training
    dataset but performs poorly in the test dataset (figure 3.30).'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，过拟合意味着过度拟合数据：记住训练数据而没有真正学习特征。这种情况发生在我们构建一个超级网络，可以完美地拟合训练数据集（训练时的错误率非常低），但无法推广到它之前未见过的新数据样本。您将看到，在过拟合的情况下，网络在训练数据集上表现良好，但在测试数据集上表现不佳（图3.30）。
- en: '![](../Images/3-30.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-30.png)'
- en: 'Figure 3.30 Underfitting (left): the model doesn’t represent the data very
    well. Just right (middle): the model fits the data very well. Overfitting (right):
    the model fits the data too much, so it won’t be able to generalize for unseen
    examples.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.30 欠拟合（左）：模型无法很好地表示数据。恰到好处（中）：模型很好地拟合了数据。过拟合（右）：模型过度拟合数据，因此它无法推广到未见过的新例子。
- en: In machine learning, we don’t want to build models that are too simple and so
    underfit the data or are too complex and overfit it. We want to use other techniques
    to build a neural network that is just right for our problem. To address that,
    we will discuss dropout layers next.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们不希望构建过于简单而欠拟合数据或过于复杂而过拟合数据的模型。我们希望使用其他技术构建一个适合我们问题的神经网络。为了解决这个问题，我们将在下一节讨论dropout层。
- en: 3.5.2 What is a dropout layer?
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.2 什么是dropout层？
- en: A dropout layer is one of the most commonly used layers to prevent overfitting.
    Dropout turns off a percentage of neurons (nodes) that make up a layer of your
    network (figure 3.31). This percentage is identified as a hyperparameter that
    you tune when you build your network. By “turns off,” I mean these neurons are
    not included in a particular forward or backward pass. It may seem counterintuitive
    to throw away a connection in your network, but as a network trains, some nodes
    can dominate others or end up making large mistakes. Dropout gives you a way to
    balance your network so that every node works equally toward the same goal, and
    if one makes a mistake, it won’t dominate the behavior of your model. You can
    think of dropout as a technique that makes a network resilient; it makes all the
    nodes work well as a team by making sure no node is too weak or too strong.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout层是防止过拟合最常用的层之一。Dropout关闭了构成您网络一层的一定比例的神经元（节点）（如图3.31所示）。这个比例被识别为一个超参数，当您构建网络时对其进行调整。通过“关闭”，我的意思是这些神经元不包括在特定的正向或反向传播中。在网络中丢弃连接可能看起来有些反直觉，但随着网络的训练，一些节点可能会支配其他节点或最终犯下大错误。Dropout为您提供了一种平衡网络的方法，使每个节点都能平等地朝着同一个目标努力，如果其中一个节点犯了错误，它不会支配您模型的行为。您可以将dropout视为一种使网络具有弹性的技术；通过确保没有节点太弱或太强，它使所有节点作为一个团队良好地工作。
- en: '![](../Images/3-31.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-31.png)'
- en: Figure 3.31 Dropout turns off a percentage of the neurons that make up a network
    layer.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.31 Dropout关闭了构成网络层的一定比例的神经元。
- en: 3.5.3 Why do we need dropout layers?
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.3 为什么我们需要dropout层？
- en: Neurons develop codependency among each other during training, which controls
    the individual power of each neuron, leading to overfitting of training data.
    To really understand why dropouts are effective, let’s take a closer look at the
    MLP in figure 3.31 and think about what the nodes in each layer really represent.
    The first layer (far left) is the input layer that contains the input features.
    The second layer contains the features learned from the patterns of the previous
    layer when multiplied by the weights. Then the following layer is patterns learned
    within patterns, and so on. Each neuron represents a certain feature that, when
    multiplied by a weight, is transformed into another feature. When we randomly
    turn off some of these nodes, we force the other nodes to learn patterns without
    relying on only one or two features, because any feature can be randomly dropped
    out at any point. This results in spreading out the weights among all the features,
    leading to more trained neurons.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，神经元之间会形成相互依赖，这控制了每个神经元的个体能力，导致训练数据的过拟合。为了真正理解为什么dropout是有效的，让我们更仔细地看看图3.31中的MLP，并思考每一层的节点真正代表什么。第一层（最左边）是输入层，包含输入特征。第二层包含从上一层的模式中学习到的特征，当乘以权重时。然后是下一层，它学习的是模式中的模式，依此类推。每个神经元代表一个特定的特征，当乘以权重时，会转换成另一个特征。当我们随机关闭一些这些节点时，我们迫使其他节点在没有仅依赖一个或两个特征的情况下学习模式，因为任何特征在任何时候都可能被随机丢弃。这导致权重在所有特征之间分散，导致更多训练神经元。
- en: Dropout helps reduce interdependent learning among the neurons. In that sense,
    it helps to view dropout as a form of ensemble learning. In ensemble learning,
    we train a number of weaker classifiers separately, and then we use them at test
    time by averaging the responses of all ensemble members. Since each classifier
    has been trained separately, it has learned different aspects of the data, and
    their mistakes (errors) are different. Combining them helps to produce a stronger
    classifier, which is less prone to overfitting.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout有助于减少神经元之间的相互依赖学习。从这个意义上讲，它有助于将dropout视为一种集成学习方法。在集成学习中，我们分别训练多个较弱的分类器，然后在测试时通过平均所有集成成员的响应来使用它们。由于每个分类器都是分别训练的，因此它们已经学习了数据的不同方面，它们的错误（误差）也不同。将它们结合起来有助于产生一个更强的分类器，这种分类器不太可能过拟合。
- en: Intuition
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 直觉
- en: 'An analogy that helps me understand dropout is training your biceps with a
    bar. When lifting a bar with both arms, we tend to rely on our stronger arm to
    lift a little more weight than our weaker arm. Our stronger arm will end up getting
    more training than the other and develop a larger muscle:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有助于我理解dropout的类比是使用杠铃训练二头肌。当我们用双臂举起杠铃时，我们倾向于依赖我们的更强臂举起比我们的弱臂更多的重量。我们的更强臂最终会得到比其他部位更多的训练并发展出更大的肌肉：
- en: '![](../Images/3-unnumb-11K.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-unnumb-11K.png)'
- en: 'Dropout means mixing up our workout (training) a little. We tie our right arm
    and train our left arm only. Then we tie the left arm and train the right arm
    only. Then we mix it up and go back to the bar with both arms, and so on. After
    some time, you will see that you have developed both of your biceps:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout意味着稍微打乱我们的训练（训练）过程。我们绑住我们的右手，只训练左手。然后我们绑住左手，只训练右手。然后我们打乱它，带着两只手臂回到杠铃，以此类推。过了一段时间，你会发现你的两个二头肌都得到了锻炼：
- en: '![](../Images/3-unnumb-12K.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-unnumb-12K.png)'
- en: This is exactly what happens when we train neural networks. Sometimes part of
    the network has very large weights and dominates all the training, while another
    part of the network doesn’t get much training. What dropout does is turn off some
    neurons and let the rest of the neurons train. Then, in the next epoch, it turns
    off other neurons, and the process continues.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们在训练神经网络时发生的情况。有时网络的一部分具有非常大的权重并主导所有训练，而网络的另一部分则没有得到很多训练。dropout的作用是关闭一些神经元，让其余的神经元进行训练。然后，在下一个epoch中，它关闭其他神经元，这个过程持续进行。
- en: 3.5.4 Where does the dropout layer go in the CNN architecture?
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.4 dropout层在CNN架构中的位置在哪里？
- en: 'As you have learned in this chapter, a standard CNN consists of alternating
    convolutional and pooling layers, ending with fully connected layers. To prevent
    overfitting, it’s become standard practice after you flatten the image to inject
    a few dropout layers between the fully connected layers at the end of the architecture.
    Why? Because dropout is known to work well in the fully connected layers of convolutional
    neural nets. Its effect in convolutional and pooling layers is, however, not well
    studied yet:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在本章中学到的，一个标准的CNN由交替的卷积层和池化层组成，以全连接层结束。为了防止过拟合，在将图像展平后，在架构末尾的全连接层之间注入几个dropout层已成为标准做法。为什么？因为dropout在卷积神经网络的完全连接层中已知效果良好。然而，它在卷积和池化层中的效果尚未得到充分研究：
- en: 'CNN architecture: ... CONV ⇒ POOL ⇒ Flatten ⇒ DO ⇒ FC ⇒ DO ⇒ FC'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: CNN架构：... 卷积 ⇒ 池化 ⇒ 展平 ⇒ DO ⇒ FC ⇒ DO ⇒ FC
- en: 'Let’s see how we use Keras to add a dropout layer to our previous model:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何使用Keras将dropout层添加到我们之前的模型中：
- en: '[PRE9]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Flatten layer
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 展平层
- en: ❷ Dropout layer with 30% probability
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 30%概率的dropout层
- en: '❸ FC_1: fully connected to get all relevant data'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ FC_1：完全连接以获取所有相关数据
- en: ❹ Dropout layer with 50% probability
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 50%概率的dropout层
- en: '❺ FC_2: outputs a softmax to squash the matrix into output probabilities for
    the 10 classes'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ FC_2：输出softmax将矩阵压缩成10个类别的输出概率
- en: ❻ Prints the model architecture summary
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 打印模型架构摘要
- en: As you can see, the dropout layer takes `rate` as an argument. The rate represents
    the fraction of the input units to drop. For example, if we set `rate` to 0.3,
    it means 30% of the neurons in this layer will be randomly dropped in each epoch.
    So if we have 10 nodes in a layer, 3 of these neurons will be turned off, and
    7 will be trained. The three neurons are randomly selected, and in the next epoch
    other randomly selected neurons are turned off, and so on. Since we do this randomly,
    some neurons may be turned off more than others, and some may never be turned
    off. This is okay, because we do this many times so that, on average, each neuron
    will get almost the same treatment. Note that this rate is another hyperparameter
    that we tune when building our CNN.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，dropout层以`rate`作为参数。该比率表示要丢弃的输入单元的比例。例如，如果我们把`rate`设置为0.3，这意味着该层中30%的神经元将在每个epoch中被随机丢弃。所以如果我们有一个层中有10个节点，那么其中的3个神经元将被关闭，而7个将被训练。这三个神经元是随机选择的，在下一个epoch中，其他随机选择的神经元将被关闭，以此类推。由于我们是随机进行的，一些神经元可能被关闭的次数比其他的多，有些可能永远不会被关闭。这是可以的，因为我们这样做很多次，所以平均来看，每个神经元将得到几乎相同的治疗。请注意，这个比率是我们构建CNN时调整的另一个超参数。
- en: 3.6 Convolution over color images (3D images)
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6 对彩色图像的卷积（3D图像）
- en: Remember from chapter 1 that computers see grayscale images as 2D matrices of
    pixels (figure 3.32). To a computer, the image looks like a 2D matrix of the pixels’
    values, which represent intensities across the color spectrum. There is no context
    here, just a massive pile of data.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在第一章中，计算机将灰度图像视为像素的二维矩阵（图3.32）。对于计算机来说，图像看起来像是一个像素值的二维矩阵，这些值代表颜色光谱中的强度。这里没有上下文，只有大量数据。
- en: '![](../Images/3-32.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-32.png)'
- en: Figure 3.32 To a computer, an image looks like a 2D matrix of pixel values.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.32 对于计算机来说，图像看起来像是一个像素值的二维矩阵。
- en: 'Color images, on the other hand, are interpreted by the computer as 3D matrices
    with height, width, and depth. In the case of RGB images (red, green, and blue)
    the depth is three: one channel for each color. For example, a color 28 × 28 image
    will be seen by the computer as a 28 × 28 × 3 matrix. Think of this as a stack
    of three 2D matrices--one each for the red, green, and blue channels of the image.
    Each of the three matrices represents the value of intensity of its color. When
    they are stacked, they create a complete color image (figure 3.33).'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，计算机将彩色图像解释为具有高度、宽度和深度的3D矩阵。在RGB图像（红色、绿色和蓝色）的情况下，深度为3：每个颜色一个通道。例如，一个28 ×
    28的彩色图像将被计算机视为一个28 × 28 × 3的矩阵。想象一下，这是一个由三个2D矩阵堆叠而成的——每个矩阵分别对应图像的红色、绿色和蓝色通道。每个矩阵代表其颜色的强度值。当它们堆叠在一起时，就构成了一个完整的彩色图像（图3.33）。
- en: '![](../Images/3-33.png)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-33.png)'
- en: Figure 3.33 Color images are represented by three matrices. Each matrix represents
    the value of its color’s intensity. Stacking them creates a complete color image.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.33 彩色图像由三个矩阵表示。每个矩阵代表其颜色的强度值。将它们堆叠起来就构成了一个完整的彩色图像。
- en: 'NOTE For generalization, we represent images as a 3D array: height × width
    × depth. For grayscale images, depth is 1; and for color images, depth is 3.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了泛化，我们用三维数组表示图像：高度 × 宽度 × 深度。对于灰度图像，深度为1；对于彩色图像，深度为3。
- en: 3.6.1 How do we perform a convolution on a color image?
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.1 我们如何在彩色图像上执行卷积？
- en: 'Similar to what we did with grayscale images, we slide the convolutional kernel
    over the image and compute the feature maps. Now the kernel is itself three-dimensional:
    one dimension for each color channel (figure 3.34).'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们对灰度图像所做的方法类似，我们将卷积核在图像上滑动并计算特征图。现在核本身是三维的：每个维度对应一个颜色通道（图3.34）。
- en: '![](../Images/3-34.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-34.png)'
- en: Figure 3.34 We slide the convolutional kernel over the image and compute the
    feature maps, resulting in a 3D kernel.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.34 我们将卷积核在图像上滑动并计算特征图，从而得到一个3D核。
- en: 'To perform convolution, we will do the same thing we did before, except that
    now, our sum is three times as many terms. Let’s see how (figure 3.35):'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行卷积，我们将做与之前相同的事情，只是现在我们的求和项是之前的3倍。让我们看看如何（图3.35）：
- en: Each of the color channels has its own corresponding filter.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个颜色通道都有自己的对应过滤器。
- en: Each filter will slide over its image, multiply every corresponding pixel element-wise,
    and then add them all together to compute the convolved pixel value of each filter.
    This is similar to what we did previously.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个过滤器都会在其图像上滑动，逐元素相乘对应的像素元素，然后将它们全部相加以计算每个过滤器的卷积像素值。这与我们之前所做的方法类似。
- en: We then add the three values to get the value of a single node in the convolved
    image or feature map. And don’t forget to add the bias value of 1\. Then we slide
    the filters over by one or more pixels (based on the strides value) and do the
    same thing. We continue this process until we compute the pixel values of all
    nodes in the feature map.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们将这三个值相加得到卷积图像或特征图中单个节点的值。别忘了加上1的偏置值。然后我们将过滤器滑动一个或多个像素（基于步长值）并执行相同操作。我们继续这个过程，直到计算完特征图中所有节点的像素值。
- en: '![](../Images/3-35.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-35.png)'
- en: Figure 3.35 Performing convolution
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.35 执行卷积
- en: 3.6.2 What happens to the computational complexity?
  id: totrans-387
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.2 计算复杂度会发生什么变化？
- en: 'Note that if we pass a 3 × 3 filter over a grayscale image, we will have a
    total of 9 parameters (weights) for each filter (as already demonstrated). In
    color images, every filter is itself a 3D filter. This means every filter has
    a number of parameters: (height × width × depth) = (3 × 3 × 3) = 27\. You can
    see how the network complexity increases when processing color images because
    it has to optimize more parameters; color images also take up more memory space.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果我们用一个3 × 3的过滤器在灰度图像上滑动，我们将为每个过滤器有总共9个参数（权重）（如前所述）。在彩色图像中，每个过滤器本身就是一个3D过滤器。这意味着每个过滤器都有一个参数数量：（高度
    × 宽度 × 深度）=（3 × 3 × 3）= 27。你可以看到，当处理彩色图像时，网络复杂性如何增加，因为它必须优化更多的参数；彩色图像也占用更多的内存空间。
- en: 'Color images contain more information than grayscale images. This can add unnecessary
    computational complexity and take up memory space. However, color images are also
    really useful for certain classification tasks. That’s why in some use cases,
    you, as a computer vision engineer, will use your judgement as to whether to convert
    your color images to grayscale where color doesn’t really matter. This is because
    for many objects, color is not needed to recognize and interpret an image: grayscale
    could be enough to recognize objects.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 彩色图像包含比灰度图像更多的信息。这可能会增加不必要的计算复杂性和占用内存空间。然而，彩色图像对于某些分类任务也非常有用。这就是为什么在某些用例中，作为计算机视觉工程师的你将根据自己的判断来决定是否将彩色图像转换为灰度图像，因为在很多情况下，颜色并不是识别和解释图像所必需的：灰度图像可能就足够识别物体了。
- en: 'In figure 3.36, you can see how patterns of light and dark in an object (intensity)
    can be used to define its shape and characteristics. However, in other applications,
    color is important to define certain objects: for example, skin cancer detection
    relies heavily on skin color (red rashes). In general, when it comes to CV applications
    like identifying cars, people, or skin cancer, you can decide whether color information
    is important or not by thinking about your own vision. If the identification problem
    is easier in color for us humans, it’s likely easier for an algorithm to see color
    images, too.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3.36中，你可以看到一个物体（强度）中明暗模式的如何被用来定义其形状和特征。然而，在其他应用中，颜色对于定义某些物体很重要：例如，皮肤癌检测很大程度上依赖于皮肤颜色（红色皮疹）。一般来说，当涉及到CV应用，如识别汽车、人或皮肤癌时，你可以通过思考自己的视觉来决定颜色信息是否重要。如果我们人类在颜色上更容易识别问题，那么算法看到彩色图像可能也更容易。
- en: '![](../Images/3-36.png)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-36.png)'
- en: Figure 3.36 Patterns of light and dark in an object (intensity) can be used
    to define its shape and characteristics in a grayscale image.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.36 在灰度图像中，一个物体（强度）的明暗模式可以用来定义其形状和特征。
- en: Note that in figure 3.36, we added only one filter (that contains 3 channels),
    which produced one feature map. Similarly to grayscale images, each filter we
    add will produce its own feature map. In the CNN in figure 3.37, we have an input
    image of dimensions (7 × 7 × 3). We add two convolution filters of dimensions
    (3 × 3). The output feature map has a depth of 2, since we added two filters,
    similar to what we did with grayscale images.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在图3.36中，我们只添加了一个过滤器（包含3个通道），它产生了一个特征图。与灰度图像类似，我们添加的每个过滤器都会产生它自己的特征图。在图3.37中的CNN中，我们有一个尺寸为（7
    × 7 × 3）的输入图像。我们添加了两个尺寸为（3 × 3）的卷积过滤器。输出特征图的深度为2，因为我们添加了两个过滤器，这与我们在灰度图像中所做的一样。
- en: An important closing note on CNN architecture
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 关于CNN架构的重要注意事项
- en: I strongly recommend looking at existing architectures, since many people have
    already done the work of throwing things together and seeing what works. Practically
    speaking, unless you are working on research problems, you should start with a
    CNN architecture that has already been built by other people to solve problems
    similar to yours. Then tune it further to fit your data.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议查看现有的架构，因为很多人已经做了将事物组合在一起并看看什么有效的工作。从实际的角度来说，除非你正在研究问题，否则你应该从一个已经由其他人构建的用于解决类似你问题的CNN架构开始。然后进一步调整以适应你的数据。
- en: In chapter 4, we will explain how to diagnose your network’s performance and
    discuss tuning strategies to improve it. In chapter 5, we will discuss the most
    popular CNN architectures and examine how other researchers built them. What I
    want you to take from this section is, first, a conceptual understanding of how
    a CNN is built; and, second, that more layers lead to more neurons, which lead
    to more learning behavior. But this comes with computational cost. So you should
    always consider the size and complexity of your training data (many layers may
    not be necessary for a simple task).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章中，我们将解释如何诊断你网络的性能，并讨论调整策略以改进它。在第5章中，我们将讨论最流行的CNN架构，并检查其他研究人员是如何构建它们的。我希望你从这个部分得到的是，首先，对CNN构建的概念理解；其次，更多的层导致更多的神经元，这导致更多的学习行为。但这伴随着计算成本。因此，你应该始终考虑你的训练数据的大小和复杂性（对于简单任务，可能不需要很多层）。
- en: '![](../Images/3-37.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-37.png)'
- en: Figure 3.37 Our input image has dimensions (7 × 7 × 3), and we add two convolution
    filters of dimensions (3 × 3). The output feature map has a depth of 2.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.37 我们的输入图像尺寸为（7 × 7 × 3），我们添加了两个尺寸为（3 × 3）的卷积过滤器。输出特征图的深度为2。
- en: '3.7 Project: Image classification for color images'
  id: totrans-399
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.7 项目：彩色图像的分类
- en: Let’s take a look at an end-to-end image classification project. In this project,
    we will train a CNN to classify images from the CIFAR-10 dataset ([www.cs.toronto.edu/
    ~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html)). CIFAR-10 is an
    established CV dataset used for object recognition. It is a subset of the 80 Million
    Tiny Images dataset[1](#pgfId-1131864) and consists of 60,000 (32 × 32) color
    images containing 1 of 10 object classes, with 6,000 images per class. Now, fire
    up your notebook and let’s get started.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个端到端图像分类项目。在这个项目中，我们将训练一个CNN来对CIFAR-10数据集（[www.cs.toronto.edu/ ~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html)）中的图像进行分类。CIFAR-10是一个用于物体识别的成熟CV数据集，它是8000万小图像数据集的一个子集[1](#pgfId-1131864)，包含60000张（32
    × 32）彩色图像，每类有6000张图像。现在，启动你的笔记本，让我们开始吧。
- en: 'Step 1: Load the dataset'
  id: totrans-401
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第1步：加载数据集
- en: 'The first step is to load the dataset into our train and test objects. Luckily,
    Keras provides the CIFAR dataset for us to load using the `load_data()` method.
    All we have to do is import `keras.datasets` and then load the data:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将数据集加载到我们的训练和测试对象中。幸运的是，Keras为我们提供了`load_data()`方法来加载CIFAR数据集。我们只需要导入`keras.datasets`然后加载数据：
- en: '[PRE10]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Loads the preshuffled train and tests the data
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载预洗牌的训练和测试数据
- en: 'Step 2: Image preprocessing'
  id: totrans-405
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第2步：图像预处理
- en: Based on your dataset and the problem you are solving, you will need to do some
    data cleanup and preprocessing to get it ready for your learning model. A cost
    function has the shape of a bowl, but it can be an elongated bowl if the features
    have very different scales. Figure 3.38 shows gradient descent on a training set
    where features 1 and 2 have the same scale (on the left), and on a training set
    where feature 1 has much smaller values than feature 2 (on the right).
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的数据集和你要解决的问题，你需要进行一些数据清理和预处理，以便为你的学习模型做好准备。成本函数的形状像一个碗，但如果特征具有非常不同的尺度，它可能是一个拉长的碗。图3.38显示了在特征1和2具有相同尺度（左侧）的训练集上的梯度下降，以及在特征1的值比特征2小得多的训练集（右侧）。
- en: TIP When using gradient descent, you should ensure that all features have a
    similar scale; otherwise, it will take much longer to converge.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：在使用梯度下降时，你应该确保所有特征具有相似的尺度；否则，收敛将需要更长的时间。
- en: '![](../Images/3-38.png)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-38.png)'
- en: Figure 3.38 Normalized features are on the same scale represented by a uniform
    bowl (left). Non-normalized features are not on the same scale and are represented
    by an elongated bowl (right). Gradient descent on a training set with features
    that have the same scale (left) and on a training set where feature 1’s values
    are much smaller than feature 2’s (right).
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.38显示了标准化特征具有相同尺度，用一个均匀的碗表示（左侧）。未标准化特征尺度不同，用一个拉长的碗表示（右侧）。在具有相同尺度的特征上的训练集（左侧）和在特征1的值比特征2小得多的训练集（右侧）上的梯度下降。
- en: Rescale the images
  id: totrans-410
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 调整图像尺度
- en: 'Rescale the input images as follows:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 按以下方式调整输入图像的尺度：
- en: '[PRE11]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '❶ Rescales the images by dividing the pixel values by 255: [0,255] ⇒ [0,1]'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过除以255来调整图像的像素值：[0,255] ⇒ [0,1]
- en: Prepare the labels (one-hot encoding)
  id: totrans-414
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 准备标签（独热编码）
- en: 'In this chapter and throughout the book, we will discuss how computers process
    input data (images) by converting it into numeric values in the form of matrices
    of pixel intensities. But what about the labels? How are the labels understood
    by computers? Every image in our dataset has a specific label that explains (in
    text) how this image is categorized. In this particular dataset, for example,
    the labels are categorized by the following 10 classes: `[''airplane'',` `''automobile'',`
    `''bird'',` `''cat'',` `''deer'',` `''dog'',` `''frog'',` `''horse'',` `''ship'',`
    `''truck'']`. We need to convert these text labels into a form that can be processed
    by computers. Computers are good with numbers, so we will do something called
    one-hot encoding. One-hot encoding is a process by which categorical variables
    are converted into a numeric form.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章以及整本书中，我们将讨论计算机如何通过将其转换为像素强度的矩阵形式来处理输入数据（图像）。那么标签呢？计算机是如何理解标签的？我们数据集中的每一张图像都有一个特定的标签，用文本解释了这张图像是如何被分类的。在这个特定的数据集中，例如，标签被分为以下10个类别：`['飞机',`
    `'汽车',` `'鸟',` `'猫',` `'鹿',` `'狗',` `'青蛙',` `'马',` `'船',` `'卡车']`。我们需要将这些文本标签转换为计算机可以处理的形式。计算机擅长处理数字，所以我们将进行一种称为独热编码的过程。独热编码是一种将分类变量转换为数值形式的过程。
- en: 'Suppose the dataset looks like the following:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数据集看起来如下：
- en: '| Image | Label |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| 图像 | 标签 |'
- en: '| image_1 | dog |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| image_1 | 狗 |'
- en: '| image_2 | automobile |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| image_2 | 汽车 |'
- en: '| image_3 | airplane |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| image_3 | 飞机 |'
- en: '| image_4 | truck |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| image_4 | truck |'
- en: '| image_5 | bird |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| image_5 | bird |'
- en: 'After one-hot encoding, we have the following:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码后，我们得到以下内容：
- en: '|  | airplane | bird | cat | deer | dog | frog | horse | ship | truck | automobile
    |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|  | airplane | bird | cat | deer | dog | frog | horse | ship | truck | automobile
    |'
- en: '| image_1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| image_1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 |'
- en: '| image_2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| image_2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 |'
- en: '| image_3 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| image_3 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |'
- en: '| image_4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| image_4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |'
- en: '| image_5 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| image_5 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |'
- en: 'Luckily, Keras has a method that does just that for us:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Keras 有一个方法可以为我们做到这一点：
- en: '[PRE12]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ One-hot encodes the labels
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 对标签进行独热编码
- en: Split the dataset for training and validation
  id: totrans-433
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将数据集分为训练集和验证集
- en: 'In addition to splitting our data into train and test datasets, it is a standard
    practice to further split the training data into training and validation datasets
    (figure 3.39). Why? Because each split is used for a different purpose:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将我们的数据分为训练集和测试集之外，将训练数据进一步分为训练集和验证集是一种标准做法（图 3.39）。为什么？因为每个拆分都有不同的用途：
- en: Training dataset --The sample of data used to train the model.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据集 -- 用于训练模型的样本数据。
- en: Validation dataset --The sample of data used to provide an unbiased evaluation
    of model fit on the training dataset while tuning model hyperparameters. The evaluation
    becomes more biased as skill on the validation dataset is incorporated into the
    model configuration.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证数据集 -- 使用的数据样本，用于在调整模型超参数时对训练数据集上的模型拟合进行无偏评估。当将验证数据集上的技能纳入模型配置时，评估变得更具偏差。
- en: Test dataset --The sample of data used to provide an unbiased evaluation of
    final model fit on the training dataset.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据集 -- 使用的数据样本，用于在训练数据集上对最终模型拟合进行无偏评估。
- en: '![](../Images/3-39.png)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-39.png)'
- en: Figure 3.39 Splitting the data into training, validation, and test subsets
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.39 将数据拆分为训练、验证和测试子集
- en: 'Here is the Keras code:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 Keras 代码：
- en: '[PRE13]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Breaks the training set into training and validation sets
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将训练集拆分为训练集和验证集
- en: ❷ Prints the shape of the training set
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打印训练集的形状
- en: ❸ Prints the number of training, validation, and test images
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打印训练集、验证集和测试集的图像数量
- en: The label matrix
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 标签矩阵
- en: One-hot encoding converts the (1 × n) label vector to a label matrix of dimensions
    (10 × n), where n is the number of sample images. So, if we have 1,000 images
    in our dataset, the label vector will have the dimensions (1 × 1000). After one-hot
    encoding, the label matrix dimensions will be (1000 × 10). That’s why, when we
    define our network architecture in the next step, we will make the output softmax
    layer contain 10 nodes, where each node represents the probability of each class
    we have.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码将 (1 × n) 标签向量转换为维度为 (10 × n) 的标签矩阵，其中 n 是样本图像的数量。所以，如果我们数据集中有 1,000 张图像，标签向量将具有
    (1 × 1000) 的维度。独热编码后，标签矩阵的维度将是 (1000 × 10)。这就是为什么，在下一步定义我们的网络架构时，我们将输出 softmax
    层包含 10 个节点，每个节点代表我们拥有的每个类别的概率。
- en: '![](../Images/3-unnumb-13K.png)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-unnumb-13K.png)'
- en: 'Step 3: Define the model architecture'
  id: totrans-448
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 3 步：定义模型架构
- en: You learned that the core building block of CNNs (and neural networks in general)
    is the layer. Most DL projects consist of stacking together simple layers that
    implement a form of data distillation. As you learned in this chapter, the main
    CNN layers are convolution, pooling, fully connected, and activation functions.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解到 CNN（以及神经网络）的核心构建块是层。大多数深度学习项目都由堆叠简单层组成，这些层实现了数据蒸馏的一种形式。正如你在本章所学，主要的 CNN
    层包括卷积、池化、全连接和激活函数。
- en: How do you decide on the network architecture?
  id: totrans-450
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 你是如何决定网络架构的？
- en: 'How many convolutional layers should you create? How many pooling layers? In
    my opinion, it is very helpful to read about some of the most popular architectures
    (AlexNet, ResNet, Inception) and extract the key ideas leading to the design decisions.
    Looking at how these state-of-the-art architectures are built and playing with
    your own projects will help you build an intuition about the CNN architecture
    that most suits the problem you are solving. We will discuss the most popular
    CNN architectures in chapter 5\. Until then, here is what you need to know:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 应该创建多少个卷积层？多少个池化层？在我看来，了解一些最流行的架构（AlexNet、ResNet、Inception）以及提取导致设计决策的关键思想是非常有帮助的。观察这些最先进的架构是如何构建的，并在你自己的项目中尝试，将帮助你建立对最适合你解决问题的CNN架构的直觉。我们将在第5章讨论最流行的CNN架构。在此之前，你需要了解以下内容：
- en: The more layers you add, the better (at least theoretically) your network will
    learn; but this will come at the cost of increasing the computational and memory
    space complexity, because it increases the number of parameters to optimize. You
    will also face the risk of the network overfitting your training set.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你添加的层越多，理论上你的网络学习效果越好；但这也将带来计算和内存空间复杂度增加的代价，因为它增加了需要优化的参数数量。你还将面临网络过拟合训练集的风险。
- en: As the input image goes through the network layers, its depth increases, and
    the dimensions (width, height) shrink, layer by layer.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当输入图像通过网络层时，其深度会增加，而维度（宽度，高度）会逐层缩小。
- en: In general, two or three layers of 3 × 3 convolutional layers followed by a
    2 × 2 pooling can be a good start for smaller datasets. Add more convolutional
    and pooling layers until your image is a reasonable size (say, 4 × 4 or 5 × 5),
    and then add a couple of fully connected layers for classification.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常情况下，对于较小的数据集，可以从两到三层3 × 3卷积层开始，然后跟一个2 × 2池化层，这可以是一个不错的起点。添加更多的卷积和池化层，直到你的图像达到合理的尺寸（比如4
    × 4或5 × 5），然后添加几个全连接层进行分类。
- en: 'You need to set up several hyperparameters (like `filter`, `kernel_size`, and
    `padding`). Remember that you do not need to reinvent the wheel: instead, look
    in the literature to see what hyperparameters usually work for others. Choose
    an architecture that worked well for someone else as a starting point, and then
    tune these hyperparameters to fit your situation. The next chapter is dedicated
    to looking at what has worked well for others.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要设置几个超参数（如`filter`、`kernel_size`和`padding`）。记住，你不需要重新发明轮子：相反，查阅文献看看通常对其他人有效的超参数。以对其他人有效的架构作为起点，然后调整这些超参数以适应你的情况。下一章将专门探讨对其他人有效的方法。
- en: Learning to work with layers and hyperparameters
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 学习如何与层和超参数一起工作
- en: 'I don’t want you to get hung up on setting hyperparameters when building your
    first CNNs. One of the best ways to gain an instinct for how to put layers and
    hyperparameters together is to actually see concrete examples of how others have
    done it. Most of your work as a DL engineer will involve building your architecture
    and tuning the parameters. The main takeaways from this chapter are these:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 我不希望你在一开始构建CNN时过于纠结于超参数的设置。获得如何组合层和超参数直觉的最好方法之一是实际看到其他人如何具体操作。作为深度学习工程师，你大部分的工作将涉及构建架构和调整参数。本章的主要收获如下：
- en: Understand how the main CNN layers work (convolution, pooling, fully connected,
    dropout) and why they exist.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解主要CNN层（卷积、池化、全连接、dropout）的工作原理以及它们存在的原因。
- en: Understand what each hyperparameter does (number of filters in the convolutional
    layer, kernel size, strides, and padding).
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解每个超参数的作用（卷积层中的滤波器数量、内核大小、步长和填充）。
- en: Understand, in the end, how to implement any given architecture in Keras. If
    you are able to replicate this project on your own dataset, you are good to go.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，理解如何在Keras中实现任何给定的架构。如果你能够在你自己的数据集上复制这个项目，那么你就准备好了。
- en: In chapter 5, we will review several state-of-the-art architectures and see
    what worked for them.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章中，我们将回顾几个最先进的架构，并看看它们是如何工作的。
- en: 'The architecture shown in figure 3.40 is called AlexNet: it’s a popular CNN
    architecture that won the ImageNet challenges in 2011 (more details on AlexNet
    in chapter 5). The AlexNet CNN architecture is composed of five convolutional
    and pooling layers, and three fully connected layers.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.40所示的架构被称为AlexNet：它是一个流行的CNN架构，在2011年赢得了ImageNet挑战赛（关于AlexNet的更多细节请见第5章）。AlexNet
    CNN架构由五个卷积和池化层以及三个全连接层组成。
- en: '![](../Images/3-40.png)'
  id: totrans-463
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-40.png)'
- en: Figure 3.40 AlexNet architecture
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.40 AlexNet架构
- en: 'Let’s try a smaller version of AlexNet and see how it performs with our dataset
    (figure 3.41). Based on the results, we might add more layers. Our architecture
    will stack three convolutional layers and two fully connected (dense) layers as
    follows:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个更小的AlexNet版本，看看它在我们的数据集上的表现如何（图3.41）。根据结果，我们可能会添加更多层。我们的架构将堆叠三个卷积层和两个全连接（密集）层，如下所示：
- en: 'CNN: INPUT ⇒ CONV_1 ⇒ POOL_1 ⇒ CONV_2 ⇒ POOL_2 ⇒ CONV_3 ⇒ POOL_3 ⇒ DO ⇒ FC
    ⇒ DO ⇒ FC (softmax)'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: CNN：输入 ⇒ CONV_1 ⇒ POOL_1 ⇒ CONV_2 ⇒ POOL_2 ⇒ CONV_3 ⇒ POOL_3 ⇒ DO ⇒ FC ⇒ DO
    ⇒ FC (softmax)
- en: '![](../Images/3-41.png)'
  id: totrans-467
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-41.png)'
- en: Figure 3.41 We will build a small CNN consisting of three convolutional layers
    and two dense layers.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.41 我们将构建一个包含三个卷积层和两个密集层的简单CNN。
- en: 'Note that we will use the ReLU activation function for all the hidden layers.
    In the last dense layer, we will use a softmax activation function with 10 nodes
    to return an array of 10 probability scores (summing to 1). Each score will be
    the probability that the current image belongs to our 10 image classes:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将对所有隐藏层使用ReLU激活函数。在最后一个密集层，我们将使用具有10个节点的softmax激活函数，以返回一个包含10个概率分数的数组（总和为1）。每个分数将是当前图像属于我们10个图像类别的概率：
- en: '[PRE14]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ First convolutional and pooling layers. Note that we need to define input_shape
    in the first convolutional layer only.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第一个卷积和池化层。注意，我们只需要在第一个卷积层中定义input_shape。
- en: ❷ Second convolutional and pooling layers with a ReLU activation function
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 第二个卷积和池化层，带有ReLU激活函数
- en: ❸ Third convolutional and pooling layers
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 第三个卷积和池化层
- en: ❹ Dropout layer to avoid overfitting with a 30% rate
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 30%率的dropout层以避免过拟合
- en: ❺ Flattens the last feature map into a vector of features
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将最后一个特征图展平成一个特征向量
- en: ❻ Adds the first fully connected layer
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 添加第一个全连接层
- en: ❼ Another dropout layer with a 40% rate
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 另一个40%率的dropout层
- en: ❽ The output layer is a fully connected layer with 10 nodes and softmax activation
    to give probabilities to the 10 classes.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 输出层是一个包含10个节点的全连接层，并使用softmax激活函数为10个类别提供概率。
- en: ❾ Prints a summary of the model architecture
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 打印模型架构摘要
- en: When you run this cell, you will see the model architecture and how the dimensions
    of the feature maps change with every successive layer, as illustrated in figure
    3.42.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这个单元格时，你会看到模型架构以及特征图维度如何随着每一层的连续变化，如图3.42所示。
- en: '![](../Images/3-42.png)'
  id: totrans-481
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-42.png)'
- en: Figure 3.42 Model summary
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.42 模型摘要
- en: We discussed previously how to understand this summary. As you can see, our
    model has 528,054 parameters (weights and biases) to train. We also discussed
    previously how this number was calculated.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论了如何理解这个摘要。正如你所见，我们的模型有528,054个参数（权重和偏差）需要训练。我们之前也讨论了如何计算这个数字。
- en: 'Step 4: Compile the model'
  id: totrans-484
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第4步：编译模型
- en: 'The last step before training our model is to define three more hyperparameters--a
    loss function, an optimizer, and metrics to monitor during training and testing:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练我们的模型之前，最后一步是定义三个额外的超参数--损失函数、优化器以及在训练和测试期间监控的指标：
- en: Loss function --How the network will be able to measure its performance on the
    training data.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数 --网络将如何衡量其在训练数据上的性能。
- en: Optimizer --The mechanism that the network will use to optimize its parameters
    (weights and biases) to yield the minimum loss value. It is usually one of the
    variants of stochastic gradient descent, explained in chapter 2.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器 --网络将使用的机制来优化其参数（权重和偏差）以产生最小损失值。它通常是第2章中解释的随机梯度下降的变体之一。
- en: Metrics --List of metrics to be evaluated by the model during training and testing.
    Typically we use `metrics=['accuracy']`.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标 --模型在训练和测试期间要评估的指标列表。通常我们使用`metrics=['accuracy']`。
- en: Feel free to revisit chapter 2 for more details on the exact purpose and different
    types of loss functions and optimizers.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 随意回顾第2章以获取关于确切目的和不同类型的损失函数及优化器的更多细节。
- en: 'Here is the code to compile the model:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 这是编译模型的代码：
- en: '[PRE15]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Step 5: Train the model'
  id: totrans-492
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第5步：训练模型
- en: 'We are now ready to train the network. In Keras, this is done via a call to
    the network’s .fit() method (as in fitting the model to the training data):'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好训练网络了。在Keras中，这是通过调用网络的.fit()方法（如将模型拟合到训练数据）来完成的：
- en: '[PRE16]'
  id: totrans-494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: When you run this cell, the training will start, and the verbose output shown
    in figure 3.43 will show one epoch at a time. Since 100 epochs of display do not
    fit on one page, the screenshot shows the first 13 epochs. But when you run this
    on your notebook, the display will keep going for 100 epochs.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这个单元格时，训练将开始，图3.43中显示的详细输出将每次显示一个时代。由于100个时代的显示不适合一页纸，截图显示了前13个时代。但当你在这个笔记本上运行时，显示将一直持续到100个时代。
- en: '![](../Images/3-43.png)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-43.png)'
- en: Figure 3.43 The first 13 epochs of training
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.43 训练的前13个时代
- en: 'Looking at the verbose output in figure 3.43 will help you analyze how your
    network is performing and suggest which knobs (hyperparameter) to tune. We will
    discuss this in detail in chapter 4\. For now, let’s look at the most important
    takeaways:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 查看图3.43中的详细输出将帮助你分析你的网络性能，并提出哪些旋钮（超参数）需要调整。我们将在第4章中详细讨论这一点。现在，让我们看看最重要的要点：
- en: '`loss` and `acc` are the error and accuracy values for the training data. `val_loss`
    and `val_acc` are the error and accuracy values for the validation data.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`和`acc`是训练数据的错误和准确率值。`val_loss`和`val_acc`是验证数据的错误和准确率值。'
- en: Look at the `val_loss` and `val_acc` values after each epoch. Ideally, we want
    `val_loss` to be decreasing and `val_acc` to be increasing, indicating that the
    network is actually learning after each epoch.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看每个时代后的`val_loss`和`val_acc`值。理想情况下，我们希望`val_loss`下降，`val_acc`上升，这表明网络在每个时代之后实际上是在学习的。
- en: From epochs 1 through 6, you can see that the model is saving the weights after
    each epoch, because the validation loss value is improving. So at the end of each
    epoch, we save the weights that are considered the best weights so far.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从第1个时代到第6个时代，你可以看到模型在每个时代之后都会保存权重，因为验证损失值在提高。所以每个时代结束时，我们保存被认为是迄今为止最佳权重的权重。
- en: At epoch 7, `val_loss` went up to 1.1300 from 0.9445, which means that it did
    not improve. So the network did not save the weights at this epoch. If you stop
    the training now and load the weights from epoch 6, you will get the best results
    that you achieved during the training.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第7个时代，`val_loss`从0.9445上升到1.1300，这意味着它没有改进。所以网络在这个时代没有保存权重。如果你现在停止训练并从第6个时代加载权重，你将得到训练期间达到的最佳结果。
- en: 'The same is true for epoch 8: `val_loss` decreases, so the network saves the
    weights as best values. And at epoch 9, there is no improvement, and so forth.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于第8个时代，`val_loss`下降，因此网络将权重保存为最佳值。在第9个时代，没有改进，以此类推。
- en: If you stop your training after 12 epochs and load the best weights, the network
    will load the weights saved after epoch 10 at (`val_loss` = 0.9157) and (`val_acc`
    = 0.6936). This means you can expect to get accuracy on the test data close to
    69%.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你训练12个时代后停止并加载最佳权重，网络将加载在第10个时代保存的权重（`val_loss` = 0.9157）和（`val_acc` = 0.6936）。这意味着你可以期望在测试数据上获得接近69%的准确率。
- en: Keep your eye on these common phenomena
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 关注这些常见现象
- en: '`val_loss` is oscillating. If `val_loss` is oscillating up and down, you might
    want to decrease the learning-rate hyperparameter. For example, if you see `val_loss`
    going from 0.8 to 0.9, to 0.7, to 1.0, and so on, this might mean that your learning
    rate is too high to descend the error mountain. Try decreasing the learning rate
    and letting the network train for a longer time.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`val_loss`在波动。如果`val_loss`上下波动，你可能想要降低学习率超参数。例如，如果你看到`val_loss`从0.8到0.9，到0.7，到1.0，以此类推，这可能意味着你的学习率太高，无法下降错误山。尝试降低学习率，并让网络训练更长的时间。'
- en: '![](../Images/3-unnumb-14K.png)'
  id: totrans-507
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/3-unnumb-14K.png)'
- en: If `val_loss` oscillates, the learning rate may be too high.
  id: totrans-508
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果`val_loss`波动，学习率可能太高。
- en: '`val_loss` is not improving (underfitting). If `val_loss` is not decreasing,
    this might mean your model is too simple to fit the data (underfitting). Then
    you may want to build a more complex model by adding more hidden layers to help
    the network fit the data.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`val_loss`没有改进（欠拟合）。如果`val_loss`没有下降，这可能意味着你的模型太简单，无法拟合数据（欠拟合）。那么你可能需要通过添加更多隐藏层来构建一个更复杂的模型，以帮助网络拟合数据。'
- en: '`loss` is decreasing and `val_loss` stopped improving. This means your network
    started to overfit the training data and failed to decrease the error for the
    validation data. In this case, consider using a technique to prevent overfitting,
    like dropout layers. There are other techniques to avoid overfitting, as we will
    discuss in the next chapter.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`正在下降，而`val_loss`停止了提升。这意味着你的网络开始对训练数据进行过拟合，并且未能减少验证数据的错误。在这种情况下，考虑使用防止过拟合的技术，如丢弃层。我们将在下一章讨论其他避免过拟合的技术。'
- en: 'Step 6: Load the model with the best val_acc'
  id: totrans-511
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第6步：加载具有最佳val_acc的模型
- en: 'Now that the training is complete, we use the Keras method `load_weights``()`
    to load into our model the weights that yielded the best validation accuracy score:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 现在训练完成，我们使用Keras方法`load_weights()`将产生最佳验证准确率分数的权重加载到我们的模型中：
- en: '[PRE17]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Step 7: Evaluate the model'
  id: totrans-514
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第7步：评估模型
- en: 'The last step is to evaluate our model and calculate the accuracy value as
    a percentage indicating how often our model correctly predicts the image classification:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是评估我们的模型，并计算准确率值作为百分比，表示我们的模型正确预测图像分类的频率：
- en: '[PRE18]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: When you run this cell, you will get an accuracy of about 70%. That is not bad.
    But we can do a lot better. Try playing with the CNN architecture by adding more
    convolutional and pooling layers, and see if you can improve your model.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这个单元格时，你将得到大约70%的准确率。这还不错。但我们可以做得更好。尝试通过添加更多的卷积和池化层来调整CNN架构，看看你是否能提高你的模型。
- en: In the next chapter, we will discuss strategies to set up your DL project and
    hyperparameter tuning to improve the model’s performance. At the end of chapter
    4, we will revisit this project to apply these strategies and improve the accuracy
    to above 90%.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论设置深度学习（DL）项目策略和超参数调整以改进模型性能的方法。在第4章结束时，我们将重新审视这个项目，应用这些策略并将准确率提高到90%以上。
- en: Summary
  id: totrans-519
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: MLPs, ANNs, dense, and feedforward all refer to the regular fully connected
    neural network architecture that we discussed in chapter 2.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLPs、ANNs、密集和前馈都指的是我们在第2章中讨论的常规全连接神经网络架构。
- en: MLPs usually work well for 1D inputs, but they perform poorly with images for
    two main reasons. First, they only accept feature inputs in a vector form with
    dimensions (1 × n). This requires flattening the image, which will lead to losing
    its spatial information. Second, MLPs are composed of fully connected layers that
    will yield millions and billions of parameters when processing bigger images.
    This will increase the computational complexity and will not scale for many image
    problems.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLPs通常适用于一维输入，但它们在处理图像时表现不佳，主要有两个原因。首先，它们只接受以向量形式具有维度（1 × n）的特征输入。这需要展平图像，这将导致丢失其空间信息。其次，MLPs由全连接层组成，在处理较大图像时会产生数百万甚至数十亿个参数。这将增加计算复杂度，并且对于许多图像问题无法扩展。
- en: CNNs really shine in image processing because they take the raw image matrix
    as an input without having to flatten the image. They are composed of locally
    connected layers called convolution filters, as opposed to the MLPs’ dense layers.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNNs在图像处理方面表现尤为出色，因为它们可以直接将原始图像矩阵作为输入，而不需要将图像展平。它们由称为卷积滤波器的局部连接层组成，这与MLPs的密集层形成对比。
- en: 'CNNs are composed of three main layers: the convolutional layer for feature
    extraction, the pooling layer to reduce network dimensionality, and the fully
    connected layer for classification.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）由三个主要层组成：用于特征提取的卷积层、用于降低网络维度的池化层，以及用于分类的全连接层。
- en: The main cause of poor prediction performance in machine learning is either
    overfitting or underfitting the data. Underfitting means that the model is too
    simple and fails to fit (learn) the training data. Overfitting means that the
    model is so complex that it memorizes the training data and fails to generalize
    for test data that it hasn’t seen before.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习预测性能不佳的主要原因要么是过拟合，要么是欠拟合数据。欠拟合意味着模型过于简单，无法拟合（学习）训练数据。过拟合意味着模型过于复杂，它记住了训练数据，并且无法对之前未见过的测试数据进行泛化。
- en: A dropout layer is added to prevent overfitting. Dropout turns off a percentage
    of neurons (nodes) that make up a layer of our network.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加一个丢弃层以防止过拟合。丢弃层会关闭网络层中一定比例的神经元（节点）。
- en: '* * *'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '***'
- en: '1.Antonio Torralba, Rob Fergus, and William T. Freeman, “80 Million Tiny Images:
    A Large Data Set for Nonparametric Object and Scene Recognition,” IEEE Transactions
    on Pattern Analysis and Machine Intelligence (November 2008), [https://doi.org/10.1109/TPAMI.2008.128](https://doi.org/10.1109/TPAMI.2008.128).'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 安东尼奥·托拉尔巴、罗布·弗格森和威廉·T·弗里曼，“8000万个小图像：用于非参数对象和场景识别的大数据集”，IEEE 信号处理与机器智能杂志（2008年11月），[https://doi.org/10.1109/TPAMI.2008.128](https://doi.org/10.1109/TPAMI.2008.128).
