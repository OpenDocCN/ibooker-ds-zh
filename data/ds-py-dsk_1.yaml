- en: Part 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2部分
- en: Working with structured data using Dask DataFrames
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Dask DataFrames处理结构化数据
- en: Now that you have a basic understanding of how Dask makes it possible to both
    work with large datasets and take advantage of parallelism, you’re ready to get
    some hands-on experience working with a real dataset to learn how to solve common
    data science challenges with Dask. Part 2 focuses on Dask DataFrames—a parallelized
    implementation of the ever-popular Pandas DataFrame—and how to use them to clean,
    analyze, and visualize large structured datasets.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对Dask如何使你既能处理大型数据集又能利用并行性有了基本的理解，你就可以通过实际数据集的动手实践来学习如何使用Dask解决常见的数据科学挑战。第2部分专注于Dask
    DataFrames——这是广受欢迎的Pandas DataFrame的并行化实现——以及如何使用它们来清理、分析和可视化大型结构化数据集。
- en: Chapter 3 opens the part by explaining how Dask parallelizes Pandas DataFrames
    and describing why some parts of the Dask DataFrame API are different from its
    Pandas counterpart.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 第3章通过解释Dask如何并行化Pandas DataFrames，并描述为什么Dask DataFrame API的一些部分与其Pandas对应部分不同，来开启这一部分的内容。
- en: Chapter 4 jumps into the first part of the data science workflow by addressing
    how to read data into DataFrames from various data sources.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 第4章通过解决如何从各种数据源将数据读入DataFrames的问题，跳入了数据科学工作流程的第一部分。
- en: Chapter 5 continues the workflow by diving into common data manipulation and
    cleaning tasks, such as sorting, filtering, recoding, and filling in missing data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 第5章通过深入常见的数据操作和清理任务，如排序、过滤、重新编码和填充缺失数据，继续工作流程。
- en: Chapter 6 demonstrates how to generate descriptive statistics using some built-in
    functions, as well as how to build your own custom aggregate and window functions.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 第6章展示了如何使用一些内置函数生成描述性统计，以及如何构建自己的自定义聚合和窗口函数。
- en: Chapters 7 and 8 close out part 2 by taking you from basic visualizations through
    advanced, interactive visualizations, even plotting location-based data on a map.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 第7章和第8章通过从基本可视化到高级交互式可视化，甚至是在地图上绘制基于位置的数据，结束第2部分的内容。
- en: After completing part 2, you’ll be well-versed in how to handle many data prep
    and analysis tasks common to data science projects, and you’ll be in a good position
    to move into more advanced topics to come!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 完成第2部分后，你将熟练掌握如何处理数据科学项目中常见的许多数据准备和分析任务，并且你将处于一个很好的位置，可以进入更高级的主题！
- en: '3'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Introducing Dask DataFrames
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍Dask DataFrames
- en: '**This chapter covers**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章涵盖**'
- en: Defining structured data and determining when to use Dask DataFrames
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义结构化数据并确定何时使用Dask DataFrames
- en: Exploring how Dask DataFrames are organized
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索Dask DataFrames的组织方式
- en: Inspecting DataFrames to see how they are partitioned
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查DataFrames以查看它们是如何分区的
- en: Dealing with some limitations of DataFrames
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理DataFrame的一些局限性
- en: In the previous chapter, we started exploring how Dask uses DAGs to coordinate
    and manage complex tasks across many machines. However, we only looked at some
    simple examples using the Delayed API to help illustrate how Dask code relates
    to elements of a DAG. In this chapter, we’ll begin to take a closer look at the
    DataFrame API. We’ll also start working through the NYC Parking Ticket data following
    a fairly typical data science workflow. This workflow and their corresponding
    chapters can be seen in [figure 3.1](#figure3.1).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们开始探索Dask如何使用DAGs在多台机器上协调和管理复杂任务。然而，我们只看了使用延迟API的一些简单示例，以帮助说明Dask代码与DAG元素之间的关系。在本章中，我们将开始更仔细地查看DataFrame
    API。我们还将遵循一个相当典型的数据科学工作流程来处理纽约市停车罚单数据。这个工作流程及其相应的章节可以在[图3.1](#figure3.1)中看到。
- en: '![c03_01.eps](Images/c03_01.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![c03_01.eps](Images/c03_01.png)'
- en: '[Figure 3.1](#figureanchor3.1) The *Data Science with Python and Dask* workflow'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.1](#figureanchor3.1) 使用Python和Dask进行数据科学的流程'
- en: Dask DataFrames wrap Delayed objects around Pandas DataFrames to allow you to
    operate on more sophisticated data structures. Rather than writing your own complex
    web of functions, the DataFrame API contains a whole host of complex transformation
    methods such as Cartesian products, joins, grouping operations, and so on, that
    are useful for common data manipulation tasks. Before we cover those operations
    in depth, which we will do in chapter 5, we’ll start our exploration of Dask by
    addressing some necessary background knowledge for data gathering. More specifically,
    we’ll look at how Dask DataFrames are well suited to manipulate *structured data*,
    which is data that consists of rows and columns. We’ll also look at how Dask can
    support parallel processing and handle large datasets by chunking data into smaller
    pieces called *partitions*. Plus, we’ll look at some performance-maximizing best
    practices throughout the chapter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Dask DataFrame围绕Pandas DataFrame包装延迟对象，以便您可以对更复杂的数据结构进行操作。而不是编写自己的复杂函数网络，DataFrame
    API包含一系列复杂的转换方法，如笛卡尔积、连接、分组操作等，这些方法对于常见的数据处理任务非常有用。在我们深入探讨这些操作之前，我们将在第5章中这样做，我们将从解决数据收集的必要背景知识开始探索Dask。更具体地说，我们将研究Dask
    DataFrame如何非常适合操作*结构化数据*，即由行和列组成的数据。我们还将研究Dask如何通过将数据分成称为*分区*的小块来支持并行处理和处理大型数据集。此外，我们将在本章中探讨一些性能优化的最佳实践。
- en: 3.1 Why use DataFrames?
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 为什么使用DataFrame？
- en: 'The shape of data found “in the wild” is usually described one of two ways:
    structured or unstructured. Structured data is made up of rows and columns: from
    the humble spreadsheet to complex relational database systems, structured data
    is an intuitive way to store information. [Figure 3.2](#figure3.2) shows an example
    of a structured dataset with rows and columns.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然界中找到的数据形状通常以两种方式之一来描述：结构化或非结构化。结构化数据由行和列组成：从简单的电子表格到复杂的数据库系统，结构化数据是存储信息的直观方式。[图3.2](#figure3.2)
    展示了一个具有行和列的结构化数据集示例。
- en: '![c03_02.eps](Images/c03_02.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![c03_02.eps](Images/c03_02.png)'
- en: '[Figure 3.2](#figureanchor3.2) An example of structured data'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.2](#figureanchor3.2) 结构化数据示例'
- en: 'It’s natural to gravitate toward this format when thinking about data because
    the structure helps keep related bits of information together in the same visual
    space. A row represents a logical entity: in the spreadsheet, each row represents
    a person. Rows are made up of one or more columns, which represent things we know
    about each entity. In the spreadsheet, we’ve captured each person’s last name,
    first name, date of birth, and a unique identifier. Many kinds of data can be
    fit into this shape: transactional data from point-of-sale systems, results from
    a marketing survey, clickstream data, and even image data once it’s been specially
    encoded.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当思考数据时，自然倾向于这种格式，因为这种结构有助于将相关的信息片段保持在同一视觉空间中。一行代表一个逻辑实体：在电子表格中，每一行代表一个人。行由一个或多个列组成，这些列代表我们对每个实体的了解。在电子表格中，我们记录了每个人的姓氏、名字、出生日期和唯一标识符。许多类型的数据都可以适应这种形状：来自销售点的交易数据、市场调查结果、点击流数据，甚至经过特殊编码后的图像数据。
- en: Because of the way that structured data is organized and stored, it’s easy to
    think of many different ways to manipulate the data. For example, we could find
    the earliest date of birth in the dataset, filter people out that don’t match
    a certain pattern, group people together by their last name, or sort people by
    their first name. Compare that with how the data might look if we stored it in
    several list objects.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于结构化数据的组织方式，我们很容易想到许多不同的数据操作方法。例如，我们可以找到数据集中最早的出生日期，过滤掉不符合特定模式的人，按姓氏将人分组，或按名字对人进行排序。将此与如果我们将其存储在几个列表对象中的数据可能看起来如何进行比较。
- en: Listing 3.1 A list representation of [figure 3.2](#figure3.2)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.1 [图3.2](#figure3.2) 的列表表示
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In [listing 3.1](#listing3.1), the columns are stored as separate lists. Although
    it’s still possible to do all the transformations previously suggested, it’s not
    immediately evident that the four lists are related to each other and form a complete
    dataset. Furthermore, the code required for operations like grouping and sorting
    on this data would be quite complex and require a substantial understanding of
    data structures and algorithms to write code that performs efficiently. Python
    offers many different data structures that we could use to represent this data,
    but none are as intuitive for storing structured data as the DataFrame.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 3.1](#listing3.1) 中，列被存储为单独的列表。尽管仍然可以执行之前建议的所有转换，但并不立即明显这四个列表是相互关联的，并形成一个完整的数据集。此外，对数据进行分组和排序等操作所需的代码将相当复杂，并且需要深入了解数据结构和算法来编写高效执行的代码。Python
    提供了许多不同的数据结构，我们可以使用它们来表示这些数据，但没有一个像 DataFrame 那样直观地用于存储结构化数据。
- en: 'Like a spreadsheet or a database table, DataFrames are organized into rows
    and columns. However, we have a few additional terms to be aware of when working
    with DataFrames: indexes and axes. [Figure 3.3](#figure3.3) displays the anatomy
    of a DataFrame.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于电子表格或数据库表，DataFrame 是按行和列组织的。然而，当与 DataFrame 一起工作时，我们需要注意一些额外的术语：索引和轴。[图
    3.3](#figure3.3) 展示了 DataFrame 的结构。
- en: '![c03_03.eps](Images/c03_03.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![c03_03.eps](Images/c03_03.png)'
- en: '[Figure 3.3](#figureanchor3.3) A Dask representation of the structured data
    example from [figure 3.2](#figure3.2)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.3](#figureanchor3.3) 从 [图 3.2](#figure2.2) 的结构化数据示例的 Dask 表示'
- en: 'The example in [figure 3.3](#figure3.3) shows a DataFrame representation of
    the structured data from [figure 3.2](#figure3.2). Notice the additional labels
    on the diagram: rows are referred to as “axis 0” and columns are referred to as
    “axis 1.” This is important to remember when working with DataFrame operations
    that reshape the data. DataFrame operations default to working along axis 0, so
    unless you explicitly specify otherwise, Dask will perform operations row-wise.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.3](#figure3.3) 中的示例显示了从 [图 3.2](#figure2.2) 的结构化数据到 DataFrame 的表示。注意图中的附加标签：行被称为“轴
    0”，列被称为“轴 1”。当处理会改变数据形状的 DataFrame 操作时，这一点很重要。DataFrame 操作默认在轴 0 上工作，所以除非您明确指定，否则
    Dask 将按行执行操作。'
- en: 'The other area highlighted in [figure 3.3](#figure3.3) is the index. The index
    provides an identifier for each row. Ideally, these identifiers should be unique,
    especially if you plan to use the index as a key to join with another DataFrame.
    However, Dask does not enforce uniqueness, so you can have duplicate indices if
    necessary. By default, DataFrames are created with a sequential integer index
    like the one seen in [figure 3.3](#figure3.3). If you want to specify your own
    index, you can set one of the columns in the DataFrame to be used as an index,
    or you can derive your own Index object and assign it to be the index of the DataFrame.
    We cover common indexing functions in-depth in chapter 5, but the importance of
    indices in Dask cannot be overstated: they hold the key to distributing DataFrame
    workloads across clusters of machines. With that in mind, we’ll now take a look
    at how indices are used to form partitions.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.3](#figure3.3) 中突出显示的另一个区域是索引。索引为每一行提供了一个标识符。理想情况下，这些标识符应该是唯一的，特别是如果您计划将索引用作与其他
    DataFrame 连接的键。然而，Dask 并不强制唯一性，因此如果需要，您可以有重复的索引。默认情况下，DataFrame 是以类似于 [图 3.3](#figure3.3)
    中看到的顺序整数索引创建的。如果您想指定自己的索引，可以将 DataFrame 中的某一列设置为索引，或者您可以创建自己的索引对象并将其分配给 DataFrame
    的索引。我们将在第 5 章中深入探讨常见的索引函数，但 Dask 中索引的重要性不容忽视：它们是跨机器集群分配 DataFrame 工作负载的关键。考虑到这一点，我们现在将探讨如何使用索引来形成分区。'
- en: 3.2 Dask and Pandas
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 Dask 和 Pandas
- en: 'As mentioned a few times, Pandas is a very popular and powerful framework for
    analyzing structured data, but its biggest limitation is that it was not designed
    with scalability in mind. Pandas is exceptionally well suited for handling small
    structured datasets and is highly optimized to perform fast and efficient operations
    on data stored in memory. However, as we saw in our hypothetical kitchen scenario
    in chapter 1, as the volume of work increases substantially it can be a better
    choice to hire additional help and spread the tasks across many workers. This
    is where Dask’s DataFrame API comes in: by providing a wrapper around Pandas that
    intelligently splits huge data frames into smaller pieces and spreads them across
    a cluster of workers, operations on huge datasets can be completed much more quickly
    and robustly.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几次提到的，Pandas是一个非常流行且强大的用于分析结构化数据的框架，但它的最大局限性在于它没有考虑到可扩展性。Pandas非常适合处理小型结构化数据集，并且高度优化以在内存中存储的数据上执行快速和高效的操作。然而，正如我们在第1章的假设厨房场景中看到的，随着工作量的显著增加，聘请额外的人手并将任务分散到多个工作者上可能是一个更好的选择。这就是Dask的DataFrame
    API发挥作用的地方：通过围绕Pandas提供智能包装，将大型数据帧分割成更小的部分，并将它们分散到一组工作者中，可以更快、更稳健地完成大型数据集的操作。
- en: The different pieces of the DataFrame that Dask oversees are called *partitions*.
    Each partition is a relatively small DataFrame that can be dispatched to any worker
    and maintains its full lineage in case it must be reproduced. [Figure 3.4](#figure3.4)
    demonstrates how Dask uses partitioning for parallel processing.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Dask管理的DataFrame的不同部分被称为*分区*。每个分区都是一个相对较小的DataFrame，可以被分配给任何工作者，并在必要时保持其完整的血缘关系以供重新生成。[图3.4](#figure3.4)展示了Dask如何使用分区进行并行处理。
- en: In [figure 3.4](#figure3.4), you can see the difference between how Pandas would
    handle the dataset and how Dask would handle the dataset. Using Pandas, the dataset
    would be loaded into memory and worked on sequentially one row at a time. Dask,
    on the other hand, can split the data into multiple partitions, allowing the workload
    to be *parallelized*. This means if we had a long-running function to apply over
    the DataFrame, Dask could complete the work more efficiently by spreading the
    work out over multiple machines. However, it should be noted that the DataFrame
    in [figure 3.4](#figure3.4) is used only for the sake of example. As mentioned
    previously, the task scheduler does introduce some overhead into the process,
    so using Dask to process a DataFrame with only 10 rows would likely not be the
    fastest solution. [Figure 3.5](#figure3.5) shows an example of how two hosts might
    coordinate work on this partitioned dataset in more detail.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图3.4](#figure3.4)中，你可以看到Pandas和Dask处理数据集的方式有何不同。使用Pandas，数据集将被加载到内存中，并逐行顺序处理。另一方面，Dask可以将数据分割成多个分区，允许工作负载被*并行化*。这意味着如果我们有一个要应用于DataFrame的长时间运行函数，Dask可以通过在多台机器上分散工作来更有效地完成工作。然而，需要注意的是，[图3.4](#figure3.4)中的DataFrame仅用于示例。如前所述，任务调度器确实会在过程中引入一些开销，因此使用Dask处理只有10行数据的DataFrame可能不是最快的解决方案。[图3.5](#figure3.5)展示了两个主机如何更详细地协调在这个分割数据集上的工作。
- en: As node 1 is driving the computation and telling node 2 what to do, it is currently
    taking on the role of the task scheduler. Node 1 tells node 2 to work on partition
    2 while node 1 works on partition 1\. Each node finishes its processing tasks
    and send its part of the result back to the client. The client then assembles
    the pieces of the results and displays the output.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于节点1正在驱动计算并告诉节点2做什么，它目前承担着任务调度器的角色。节点1告诉节点2处理分区2，而节点1处理分区1。每个节点完成其处理任务，并将其部分结果发送回客户端。然后客户端组装结果的部分，并显示输出。
- en: '![c03_04.eps](Images/c03_04.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![c03_04.eps](Images/c03_04.png)'
- en: '[Figure 3.4](#figureanchor3.4) Dask allows a single Pandas DataFrame to be
    worked on in parallel by multiple hosts.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.4](#figureanchor3.4) Dask允许多个主机并行处理单个Pandas DataFrame。'
- en: 3.2.1 Managing DataFrame partitioning
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 管理DataFrame分区
- en: 'Since partitioning can have such a significant impact on performance, you might
    be worried that managing partitioning will be a difficult and tedious part of
    constructing Dask workloads. However, fear not: Dask tries to help you get as
    much performance as possible without manual tuning by including some sensible
    defaults and heuristics for creating and managing partitions. For example, when
    reading in data using the `read_csv` method of Dask DataFrames, the default partition
    size is 64 MB each (this is also known as the default blocksize). While 64 MB
    might seem quite small given that modern servers tend to have tens of gigabytes
    of RAM, it is an amount of data that is small enough that it can be quickly transported
    over the network if necessary, but large enough to minimize the likelihood that
    a machine will run out of things to do while waiting for the next partition to
    arrive. Using either the default or a user-specified blocksize, the data will
    be split into as many partitions as necessary so that each partition is no larger
    than the blocksize. If you desire to create a DataFrame with a specific number
    of partitions instead, you can specify that when creating the DataFrame by passing
    in the `npartitions` argument.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分区可能会对性能产生如此重大的影响，你可能会担心管理分区将是构建 Dask 工作负载中困难且繁琐的部分。然而，无需担忧：Dask 尝试通过包含一些合理的默认值和启发式方法来创建和管理分区，以帮助你在不进行手动调优的情况下获得尽可能多的性能。例如，当使用
    Dask DataFrames 的 `read_csv` 方法读取数据时，默认分区大小为每个 64 MB（这也称为默认块大小）。考虑到现代服务器通常有数十吉字节
    RAM，64 MB 可能看起来相当小，但这是一种足够小的数据量，如果需要，它可以快速通过网络传输，但足够大，以最小化机器在等待下一个分区到来时耗尽可执行任务的可能性。使用默认的或用户指定的块大小，数据将被分割成必要的分区数量，以确保每个分区的大小不超过块大小。如果你希望创建具有特定分区数的
    DataFrame，你可以在创建 DataFrame 时通过传递 `npartitions` 参数来指定这一点。
- en: '![c03_05.eps](Images/c03_05.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![c03_05.eps](Images/c03_05.png)'
- en: '[Figure 3.5](#figureanchor3.5) Processing data in parallel across several machines'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.5](#figureanchor3.5) 在多台机器上并行处理数据'
- en: Listing 3.2 Creating a DataFrame with a specific number of partitions
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.2 使用特定分区数创建 DataFrame
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In [listing 3.2](#listing3.2), we created a Dask DataFrame and explicitly split
    it into two partitions using the `npartitions` argument. Normally, Dask would
    have put this dataset into a single partition because it is quite small.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 3.2](#listing3.2) 中，我们创建了一个 Dask DataFrame，并使用 `npartitions` 参数显式地将它分割成两个分区。通常，Dask
    会将这个数据集放入一个分区中，因为它相当小。
- en: Listing 3.3 Inspecting partitioning of a Dask DataFrame
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.3 检查 Dask DataFrame 的分区
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Listing 3.3](#listing3.3) shows a couple useful attributes of Dask DataFrames
    that can be used to inspect how a DataFrame is partitioned. The first attribute,
    `divisions`, (0, 5, 9), shows the boundaries of the partitioning scheme (remember
    that partitions are created on the index). This might look strange since there
    are two partitions but three boundaries. Each partition’s boundary consists of
    pairs of numbers from the list of divisions. The boundary for the first partition
    is “from 0 up to (but not including) 5,” meaning it will contain rows 0, 1, 2,
    3, and 4\. The boundary for the second partition is “from 5 through (and including)
    9,” meaning it will contain rows 5, 6, 7, 8, and 9\. The last partition always
    includes the upper boundary, whereas the other partitions go up to but don’t include
    their upper boundary.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 3.3](#listing3.3) 展示了 Dask DataFrames 的几个有用属性，这些属性可以用来检查 DataFrame 的分区情况。第一个属性，`divisions`（0,
    5, 9），显示了分区方案的边界（记住分区是在索引上创建的）。这可能会看起来有些奇怪，因为有两个分区但三个边界。每个分区的边界由来自 `divisions`
    列表的一对数字组成。第一个分区的边界是“从 0 到（但不包括）5”，这意味着它将包含行 0、1、2、3 和 4。第二个分区的边界是“从 5 通过（包括）9”，这意味着它将包含行
    5、6、7、8 和 9。最后一个分区始终包括上边界，而其他分区则达到但不包括它们的上边界。'
- en: The second attribute, `npartitions`, simply returns the number of partitions
    that exist in the DataFrame.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个属性，`npartitions`，简单地返回 DataFrame 中存在的分区数。
- en: Listing 3.4 Inspecting the rows in a DataFrame
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.4 检查 DataFrame 中的行
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Listing 3.4](#listing3.4) shows how to use the `map_partitions` method to
    count the number of rows in each partition. `map_partitions` generally applies
    a given function to each partition. This means that the result of the `map_partitions`
    call will return a Series equal in size to the number of partitions the DataFrame
    currently has. Since we have two partitions in this DataFrame, we get two items
    back in the result of the call. The output shows that each partition contains
    five rows, meaning Dask split the DataFrame into two equal pieces.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 3.4](#listing3.4) 展示了如何使用 `map_partitions` 方法来计算每个分区的行数。`map_partitions`
    通常将给定的函数应用于每个分区。这意味着 `map_partitions` 调用的结果将返回一个与 DataFrame 当前分区数量相等的 Series。由于在这个
    DataFrame 中我们有两个分区，所以调用结果返回了两个项目。输出显示每个分区包含五行，这意味着 Dask 将 DataFrame 分割成了两个相等的部分。'
- en: Sometimes it may be necessary to change the number of partitions in a Dask DataFrame.
    Particularly when your computations include a substantial amount of filtering,
    the size of each partition can become imbalanced, which can have negative performance
    consequences on subsequent computations. The reason for this is because if one
    partition suddenly contains a majority of the data, all the advantages of parallelism
    are effectively lost. Let’s look at an example of this. First, we’ll derive a
    new DataFrame by applying a filter to our original DataFrame that removes all
    people with the last name Williams. We’ll then inspect the makeup of the new DataFrame
    by using the same `map_partitions` call to count the rows per partition.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候可能需要更改 Dask DataFrame 中的分区数量。尤其是在你的计算包括大量过滤操作时，每个分区的尺寸可能会变得不平衡，这可能会对后续计算的性能产生负面影响。原因在于，如果一个分区突然包含了大部分数据，所有并行化的优势实际上都丧失了。让我们来看一个这样的例子。首先，我们将通过在我们的原始
    DataFrame 上应用一个过滤器来创建一个新的 DataFrame，该过滤器移除了所有姓威廉姆斯的人。然后，我们将使用相同的 `map_partitions`
    调用来计算每个分区的行数，以检查新 DataFrame 的构成。
- en: Listing 3.5 Repartitioning a DataFrame
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.5 DataFrame 重新分区
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Notice what happened: the first partition now only contains three rows, and
    the second partition has the original five. People with the last name of Williams
    happened to be in the first partition, so our new DataFrame has become rather
    unbalanced.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意发生了什么：第一个分区现在只包含三行，第二个分区有原来的五行。姓威廉姆斯的人恰好在前一个分区中，所以我们的新 DataFrame 变得相当不平衡。
- en: The second two lines of code in the listing aim to fix the imbalance by using
    the `repartition` method on the filtered DataFrame. The `npartitions` argument
    here works the same way as the `npartitions` argument used earlier when we created
    the initial DataFrame. Simply specify the number of partitions you want and Dask
    will figure out what needs to be done to make it so. If you specify a lower number
    than the current number of partitions, Dask will combine existing partitions by
    concatenation. If you specify a higher number than the current number of partitions,
    Dask will split existing partitions into smaller pieces. You can call `repartition`
    at any time in your program to initiate this process. However, like all other
    Dask operations, it’s a lazy computation. No data will actually get moved around
    until you make a call such as `compute`, `head`, and so on. Calling the `map_partitions`
    function again on the new DataFrame, we can see that the number of partitions
    has been reduced to one, and it contains all eight of the rows. Note that if you
    repartition again, this time increasing the number or partitions, the old divisions
    (0, 5, 9) will be retained. If you want to split the partitions evenly, you will
    need to manually update the divisions to match your data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中的第二行和第三行代码旨在通过在过滤后的 DataFrame 上使用 `repartition` 方法来修复不平衡。这里的 `npartitions`
    参数与我们在创建初始 DataFrame 时使用的 `npartitions` 参数的作用相同。只需指定你想要的分区数量，Dask 就会确定需要做什么来实现这一点。如果你指定了一个比当前分区数量低的数字，Dask
    将通过连接现有分区来合并它们。如果你指定了一个比当前分区数量高的数字，Dask 将将现有分区分割成更小的部分。你可以在程序的任何时间点调用 `repartition`
    来启动这个过程。然而，像所有其他 Dask 操作一样，它是一个惰性计算。实际上，直到你调用 `compute`、`head` 等操作之前，数据实际上是不会移动的。再次在新的
    DataFrame 上调用 `map_partitions` 函数，我们可以看到分区数量已减少到一，并且它包含了所有八行。请注意，如果你再次重新分区，这次增加分区数量，旧的分隔（0、5、9）将被保留。如果你想均匀分割分区，你需要手动更新分隔以匹配你的数据。
- en: 3.2.2 What is the shuffle?
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 什么是洗牌？
- en: 'Now that we’ve learned that partitioning is important, explored how Dask handles
    partitioning, and learned what you can do to influence it, we’ll round out this
    discussion by learning about a frequent challenge that arises in distributed computing:
    dealing with the *shuffle*. No, I’m not talking about the dance move—frankly,
    I wouldn’t be the best source of dance advice! In distributed computing, the shuffle
    is the process of broadcasting all partitions to all workers. Shuffling the data
    is necessary when performing sorting, grouping, and indexing operations, because
    each row needs to be compared to every other row in the entire DataFrame to determine
    its correct relative position. This is a time-expensive operation, because it
    necessitates transferring large amounts of data over the network. Let’s see what
    this might look like.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解到分区的重要性，探讨了 Dask 如何处理分区，以及学习了如何影响它，我们将通过了解分布式计算中经常遇到的挑战来结束这次讨论：处理 *洗牌*。不，我并不是在谈论舞蹈动作——坦白说，我并不是提供舞蹈建议的最佳人选！在分布式计算中，洗牌是将所有分区广播到所有工作者的过程。当执行排序、分组和索引操作时，洗牌数据是必要的，因为每一行都需要与整个
    DataFrame 中的每一行进行比较，以确定其正确的相对位置。这是一个耗时的操作，因为它需要在网络上传输大量数据。让我们看看这可能会是什么样子。
- en: '![c03_06.eps](Images/c03_06.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![c03_06.eps](Images/c03_06.png)'
- en: '[Figure 3.6](#figureanchor3.6) A `GroupBy` operation that requires a shuffle'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.6](#figureanchor3.6) 需要洗牌的 `GroupBy` 操作'
- en: 'In [figure 3.6](#figure3.6), we’re seeing what would happen with our DataFrame
    if we want to group our data by Last Name. For example, we might want to find
    the eldest person by last name. For the majority of the data, it’s no problem.
    Most of the last names in this dataset are unique. As you can see in the data
    in [figure 3.6](#figure3.6), there are only two cases in which we have multiple
    people with the same last name: Williams and Smith. For the two people named Williams,
    they are in the same partition, so server 1 has all the information it needs locally
    to determine that the oldest Williams was born in 1989\. However, for the people
    named Smith, there’s one Smith in partition 1 and one Smith in partition 2\. Either
    server 1 will have to send its Smith to server 2 to make the comparison, or server
    2 will have to send server 1 its Smith. In both cases, for Dask to be able to
    compare the birthdates of each Smith, one of them will have to be shipped over
    the network.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 3.6](#figure3.6) 中，我们看到如果我们想按姓氏对数据进行分组会发生什么。例如，我们可能想找到按姓氏最年长的人。对于大多数数据，这没有问题。这个数据集中大多数姓氏都是唯一的。正如你在
    [图 3.6](#figure3.6) 中的数据中可以看到，只有两种情况有多个同姓的人：威廉姆斯和史密斯。对于两个名叫威廉姆斯的人，他们处于同一个分区，所以服务器
    1 可以在本地获取所有必要的信息来确定最老的威廉姆斯出生于 1989 年。然而，对于史密斯这个名字，有一个史密斯在分区 1，另一个在分区 2。在这两种情况下，为了使
    Dask 能够比较每个史密斯的出生日期，其中一个人必须通过网络传输。
- en: Depending on what needs to be done with the data, completely avoiding shuffle
    operations might not be feasible. However, you can do a few things to minimize
    the need for shuffling the data. First, ensuring that the data is stored in a
    presorted order will eliminate the need to sort the data with Dask. If possible,
    sorting the data in a source system, such as a relational database, can be faster
    and more efficient than sorting the data in a distributed system. Second, using
    a sorted column as the DataFrame’s index will enable greater efficiency with joins.
    When the data is presorted, lookup operations are very fast because the partition
    where a certain row is kept can be easily determined by using the divisions defined
    on the DataFrame. Finally, if you must use an operation that triggers a shuffle,
    persist the result if you have the resources to do so. This will prevent having
    to repeat shuffling the data again if the DataFrame needs to be recomputed.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 根据需要处理的数据，完全避免洗牌操作可能不可行。然而，你可以做一些事情来最小化洗牌数据的需求。首先，确保数据以预排序的顺序存储将消除使用 Dask 对数据进行排序的需求。如果可能的话，在源系统（如关系数据库）中对数据进行排序可能比在分布式系统中排序更快、更高效。其次，使用排序列作为
    DataFrame 的索引将提高连接操作的效率。当数据预排序时，查找操作非常快，因为可以使用 DataFrame 上定义的分区来确定某个特定行所在的分区。最后，如果你必须使用触发洗牌的操作，如果你有资源，请持久化结果。这将防止在
    DataFrame 需要重新计算时再次洗牌数据。
- en: 3.3 Limitations of Dask DataFrames
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 Dask DataFrames 的局限性
- en: Now that you have a good idea of what the DataFrame API is useful for, it will
    be helpful to close the chapter by covering a few limitations that the DataFrame
    API has.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对 DataFrame API 的用途有了很好的了解，通过介绍 DataFrame API 的一些局限性来结束本章将是有帮助的。
- en: First and foremost, Dask DataFrames do not expose the entire Pandas API. Even
    though Dask DataFrames are made up of smaller Pandas DataFrames, some functions
    that Pandas does well are simply not conducive to a distributed environment. For
    example, functions that would alter the structure of the DataFrame, such as insert
    and pop, are not supported because Dask DataFrames are immutable. Some of the
    more complex window operations are also not supported, such as expanding and EWM
    methods, as well as complex transposition methods like stack/unstack and melt,
    because of their tendency to cause a lot of data shuffling. Oftentimes, these
    expensive operations don’t really need to be performed on the full, raw dataset.
    In those cases, you should use Dask to do all your normal data prep, filtering,
    and transformation, then dump the final dataset into Pandas. You will then be
    able to perform the expensive operations on the reduced dataset. Dask’s DataFrame
    API makes it very easy to interoperate with Pandas DataFrames, so this pattern
    can be very useful when analyzing data using Dask DataFrames.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先且最重要的是，Dask DataFrame 并不暴露整个 Pandas API。尽管 Dask DataFrame 由较小的 Pandas DataFrame
    组成，但 Pandas 在某些方面做得很好的函数并不适合分布式环境。例如，会改变 DataFrame 结构的函数，如插入和弹出，由于 Dask DataFrame
    是不可变的，因此不支持这些函数。一些更复杂的窗口操作也不支持，例如扩展和 EWM 方法，以及像堆叠/取消堆叠和熔合这样的复杂转置方法，因为它们倾向于导致大量数据洗牌。通常，这些昂贵的操作并不需要在完整的原始数据集上真正执行。在这些情况下，你应该使用
    Dask 来完成所有正常的数据准备、过滤和转换，然后将最终数据集导入 Pandas。这样，你就可以在减少的数据集上执行这些昂贵的操作。Dask 的 DataFrame
    API 使得与 Pandas DataFrame 交互变得非常容易，因此当使用 Dask DataFrame 分析数据时，这种模式非常有用。
- en: The second limitation is with relational-type operations, such as `join`/`merge`,
    `groupby`, and `rolling`. Although these operations are supported, they are likely
    to involve a lot of shuffling, making them performance bottlenecks. This can be
    minimized, again, either by using Dask to prepare a smaller dataset that can be
    dumped into Pandas, or by limiting these operations to only use the index. For
    example, if we wanted to join a DataFrame of people to a DataFrame of transactions,
    that computation would be significantly faster if both datasets were sorted and
    indexed by the Person ID. This would minimize the likelihood that each person’s
    records are spread out across many partitions, in turn making shuffles more efficient.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个局限性是与关系型操作相关，例如 `join`/`merge`、`groupby` 和 `rolling`。尽管这些操作是支持的，但它们很可能会涉及大量洗牌，从而成为性能瓶颈。这可以通过使用
    Dask 准备一个较小的数据集并将其导入 Pandas 来最小化，或者通过限制这些操作仅使用索引来使用。例如，如果我们想将一个人的 DataFrame 与交易的
    DataFrame 进行连接，如果这两个数据集都按 Person ID 排序并索引，那么这个计算将显著更快。这将最小化每个记录分散在许多分区中的可能性，从而使得洗牌更加高效。
- en: Third, indexing has a few challenges due to the distributed nature of Dask.
    If you wish to use a column in a DataFrame as an index in lieu of the default
    numeric index, it will need to be sorted. If the data is stored presorted, this
    becomes no problem at all. If the data is not presorted, it can be very slow to
    sort the entire DataFrame because it requires a lot of shuffling. Effectively,
    each partition first needs to be sorted, then needs to be merged and sorted again
    with every other partition. Sometimes it may be necessary to do this, but if you
    can proactively store your data presorted for the computations you need, it will
    save you a lot of time.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，由于 Dask 的分布式特性，索引有一些挑战。如果你希望将 DataFrame 中的某一列用作索引而不是默认的数字索引，则需要对其进行排序。如果数据是预先排序的，这根本不是问题。如果数据没有预先排序，则对整个
    DataFrame 进行排序可能非常缓慢，因为它需要大量洗牌。实际上，每个分区首先需要排序，然后需要与其他每个分区合并并再次排序。有时可能需要这样做，但如果你能主动将你的数据预先排序以供所需的计算，这将为你节省大量时间。
- en: The other significant difference you may notice with indexing is how Dask handles
    the `reset_index` method. Unlike Pandas, where this will recalculate a new sequential
    index across the entire DataFrame, the method in Dask DataFrames behaves like
    a `map_partitions` call. This means that each partition will be given its own
    sequential index that starts at 0, so the whole DataFrame will no longer have
    a unique sequential index. In [figure 3.7](#figure3.7), you can see the effect
    of this.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到的另一个与索引相关的显著差异是 Dask 处理 `reset_index` 方法的方式。与 Pandas 不同，在 Pandas 中这将重新计算整个
    DataFrame 的新顺序索引，而 Dask DataFrame 中的该方法表现得像是一个 `map_partitions` 调用。这意味着每个分区将获得自己的从
    0 开始的顺序索引，因此整个 DataFrame 将不再具有唯一的顺序索引。在 [图 3.7](#figure3.7) 中，你可以看到这种效果。
- en: '![c03_07.eps](Images/c03_07.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![c03_07.eps](Images/c03_07.png)'
- en: '[Figure 3.7](#figureanchor3.7) The result of calling `reset_index` on a Dask
    DataFrame'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.7](#figureanchor3.7) 调用 `reset_index` 在 Dask DataFrame 上的结果'
- en: Each partition contained five rows, so once we called `reset_index`, the index
    of the first five rows remains the same, but the next five rows, which are contained
    in the next partition, start over at 0\. Unfortunately, there’s no easy way to
    reset the index in a partition-aware way. Therefore, use the `reset_index` method
    carefully and only if you don’t plan to use the resulting sequential index to
    join, group, or sort the DataFrame.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分区包含五行，因此当我们调用 `reset_index` 时，前五行的索引保持不变，但下一五行（位于下一个分区中）从 0 开始。不幸的是，没有简单的方法可以在分区感知方式下重置索引。因此，请谨慎使用
    `reset_index` 方法，并且仅在你不打算使用生成的顺序索引进行连接、分组或排序 DataFrame 时使用。
- en: Finally, since a Dask DataFrame is made up of many Pandas DataFrames, operations
    that are inefficient in Pandas will also be inefficient in Dask. For example,
    iterating over rows by using the `apply` and `iterrows` methods is notoriously
    inefficient in Pandas. Therefore, following Pandas best practices will give you
    the best performance possible when using Dask DataFrames. If you’re not well on
    your way to mastering Pandas yet, continuing to sharpen your skills will not only
    benefit you as you get more familiar with Dask and distributed workloads, but
    it will help you in general as a data scientist!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于 Dask DataFrame 由多个 Pandas DataFrame 组成，因此在 Pandas 中效率低下的操作在 Dask 中也会效率低下。例如，使用
    `apply` 和 `iterrows` 方法按行迭代在 Pandas 中是出了名的低效。因此，遵循 Pandas 的最佳实践将在使用 Dask DataFrame
    时提供最佳性能。如果你还没有很好地掌握 Pandas，继续磨练你的技能不仅会在你更熟悉 Dask 和分布式工作负载时对你有所帮助，而且对作为数据科学家的一般工作也会有所帮助！
- en: Summary
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Dask DataFrames consist of rows (axis 0), columns (axis 1), and an index.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask DataFrame 由行（轴 0）、列（轴 1）和索引组成。
- en: DataFrame methods tend to operate row-wise by default.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame 方法默认情况下倾向于按行操作。
- en: Inspecting how a DataFrame is partitioned can be done by accessing the `divisions`
    attribute of a DataFrame.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过访问 DataFrame 的 `divisions` 属性可以检查 DataFrame 的分区情况。
- en: Filtering a DataFrame can cause an imbalance in the size of each partition.
    For best performance, partitions should be roughly equal in size. It’s a good
    practice to repartition a DataFrame using the `repartition` method after filtering
    a large amount of data.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤 DataFrame 可能会导致每个分区的尺寸不平衡。为了获得最佳性能，分区的大小应该大致相等。在过滤大量数据后，使用 `repartition`
    方法重新分区 DataFrame 是一个好习惯。
- en: For best performance, DataFrames should be indexed by a logical column, partitioned
    by their index, and the index should be presorted.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了获得最佳性能，DataFrame 应该通过逻辑列进行索引，通过其索引进行分区，并且索引应该是预排序的。
- en: '4'
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Loading data into DataFrames
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据加载到 DataFrame 中
- en: '**This chapter covers**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章涵盖**'
- en: Creating DataFrames from delimited text files and defining data schemas
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从分隔符文本文件创建 DataFrame 并定义数据模式
- en: Extracting data from a SQL relational database and manipulating it using Dask
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 SQL 关系型数据库中提取数据并使用 Dask 进行操作
- en: Reading data from distributed filesystems (S3 and HDFS)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从分布式文件系统（S3 和 HDFS）中读取数据
- en: Working with data stored in Parquet format
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理存储在 Parquet 格式的数据
- en: I’ve given you a lot of concepts to chew on over the course of the previous
    three chapters—all of which will serve you well along your journey to becoming
    a Dask expert. But, we’re now ready to roll up our sleeves and get into working
    with some data. As a reminder, [figure 4.1](#figure4.1) shows the data science
    workflow we’ll be following as we work through the functionality of Dask.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在前三章中，我已经给你介绍了很多概念——所有这些都将在你成为 Dask 专家的旅程中为你提供帮助。但是，我们现在准备卷起袖子开始处理一些数据。作为提醒，[图
    4.1](#figure4.1) 展示了我们在处理 Dask 功能时将遵循的数据科学工作流程。
- en: '![c04_01.eps](Images/c04_01.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![c04_01.eps](Images/c04_01.png)'
- en: '[Figure 4.1](#figureanchor4.1) The *Data Science with Python and Dask* workflow'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.1](#figureanchor4.1) 使用 Python 和 Dask 进行数据科学的工作流程'
- en: 'In this chapter, we remain at the very first steps of our workflow: Problem
    Definition and Data Gathering. Over the next few chapters, we’ll be working with
    the NYC Parking Ticket data to answer the following question:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们停留在工作流程的最初步骤：问题定义和数据收集。在接下来的几章中，我们将使用纽约市停车罚单数据来回答以下问题：
- en: What patterns can we find in the data that are correlated with increases or
    decreases in the number of parking tickets issued by the New York City parking
    authority?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能在数据中找到哪些与纽约市停车管理部门发放的罚单数量增加或减少相关的模式？
- en: Perhaps we might find that older vehicles are more likely to receive tickets,
    or perhaps a particular color attracts more attention from the parking authority
    than other colors. Using this guiding question, we’ll gather, clean, and explore
    the relevant data with Dask DataFrames. With that in mind, we’ll begin by learning
    how to read data into Dask DataFrames.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 或许我们会发现老旧车辆更容易收到罚单，或者可能是某种特定颜色比其他颜色更容易吸引停车管理部门的注意。使用这个指导问题，我们将使用 Dask DataFrames
    收集、清理和探索相关数据。考虑到这一点，我们将首先学习如何将数据读入 Dask DataFrames。
- en: One of the unique challenges that data scientists face is our tendency to study
    *data at rest*, or data that wasn’t specifically collected for the purpose of
    predictive modeling and analysis. This is quite different from a traditional academic
    study in which data is carefully and thoughtfully collected. Consequentially,
    you’re likely to come across a wide variety of storage media and data formats
    throughout your career. We will cover reading data in some of the most popular
    formats and storage systems in this chapter, but by no means does this chapter
    cover the full extent of Dask’s abilities. Dask is very flexible in many ways,
    and the DataFrame API’s ability to interface with a very large number of data
    collection and storage systems is a shining example of that.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家面临的一个独特挑战是我们倾向于研究“静止的数据”，或者那些并非专门为预测建模和分析目的而收集的数据。这与传统学术研究中精心且深思熟虑地收集数据的方式大相径庭。因此，你可能会在整个职业生涯中遇到各种存储介质和数据格式。在本章中，我们将介绍如何读取一些最流行的格式和存储系统中的数据，但绝不意味着本章涵盖了
    Dask 能力的全部范围。Dask 在许多方面都非常灵活，DataFrame API 与大量数据收集和存储系统的接口能力就是其光辉例证。
- en: 'As we work through reading data into DataFrames, keep what you learned in previous
    chapters about Dask’s components in mind: the Dask DataFrames we will create are
    made up of many small Pandas DataFrames that have been logically divided into
    partitions. All operations performed on the Dask DataFrame result in the generation
    of a DAG (directed acyclic graph) of Delayed objects which can be distributed
    to many processes or physical machines. And the task scheduler controls the distribution
    and execution of the task graph. Now on to the data!'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理将数据读入 DataFrames 的过程时，要牢记之前章节中关于 Dask 组件的知识：我们将创建的 Dask DataFrames 由许多逻辑上划分成分区的
    Pandas DataFrames 组成。在 Dask DataFrame 上执行的所有操作都会生成一个 DAG（有向无环图）的 Delayed 对象，这些对象可以被分发到多个进程或物理机器上。任务调度器控制任务图的分发和执行。现在，让我们转向数据！
- en: 4.1 Reading data from text files
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 从文本文件中读取数据
- en: 'We’ll start with the simplest and most common format you’re likely to come
    across: delimited text files. Delimited text files come in many flavors, but all
    share the common concept of using special characters called *delimiters* that
    are used to divide data up into logical rows and columns.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从最简单、最常见的数据格式开始：定界文本文件。定界文本文件有多种风味，但所有文件都共享使用称为*定界符*的特殊字符来将数据划分为逻辑行和列的通用概念。
- en: 'Every delimited text file format has two types of delimiters: row delimiters
    and column delimiters. A row delimiter is a special character that indicates that
    you’ve reached the end of a row, and any additional data to the right of it should
    be considered part of the next row. The most common row delimiter is simply a
    newline character (`\n`) or a carriage return followed by a newline character
    (`\r\n`). Delimiting rows by line is a standard choice because it provides the
    additional benefit of breaking up the raw data visually and reflects the layout
    of a spreadsheet.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 每种分隔文本文件格式都有两种类型的分隔符：行分隔符和列分隔符。行分隔符是一个特殊字符，表示你已到达行的末尾，其右侧的任何额外数据都应被视为下一行的一部分。最常见的行分隔符只是一个换行符（`\n`）或一个回车符后跟一个换行符（`\r\n`）。按行分隔是一种标准选择，因为它提供了额外的视觉上分割原始数据的好处，并反映了电子表格的布局。
- en: 'Likewise, a column delimiter indicates the end of a column, and any data to
    the right of it should be treated as part of the next column. Of all the popular
    column delimiters out there, the comma (`,`) is the most frequently used. In fact,
    delimited text files that use comma column delimiters have a special file format
    named for it: *comma-separated values* or CSV for short. Among other common options
    are pipe (`|`), tab, space, and semicolon.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，列分隔符表示列的结束，其右侧的任何数据都应被视为下一列的一部分。在所有流行的列分隔符中，逗号（`,`）是最常用的。实际上，使用逗号作为列分隔符的分隔文本文件有一个特殊的文件格式，名为*逗号分隔值*或简称为CSV。其他常见选项包括管道（`|`）、制表符、空格和分号。
- en: In [figure 4.2](#figure4.2), you can see the general structure of a delimited
    text file. This one in particular is a CSV file because we’re using commas as
    the column delimiter. Also, since we’re using the newline as the row delimiter,
    you can see that each row is on its own line.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图4.2](#figure4.2)中，你可以看到分隔文本文件的一般结构。这个特定的文件是一个CSV文件，因为我们使用逗号作为列分隔符。此外，由于我们使用换行符作为行分隔符，你可以看到每一行都在自己的行上。
- en: '![c04_02.eps](Images/c04_02.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![c04_02.eps](Images/c04_02.png)'
- en: '[Figure 4.2](#figureanchor4.2) The structure of a delimited text file'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.2](#figureanchor4.2) 分隔文本文件的结构'
- en: Two additional attributes of a delimited text file that we haven’t discussed
    yet include an optional header row and text qualifiers. A header row is simply
    the use of the first row to specify names of columns. Here, Person ID, Last Name,
    and First Name aren’t descriptions of a person; they are *metadata**that describe
    the data structure. While not required, a header row can be helpful for communicating
    what your data structure is supposed to hold.*
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 文件分隔文本的另外两个尚未讨论的属性包括可选的标题行和文本限定符。标题行简单来说就是使用第一行来指定列名。在这里，人员ID、姓氏和名字并不是对一个人的描述；它们是**元数据**，用于描述数据结构。虽然不是必需的，但标题行对于传达数据结构应该包含的内容是有帮助的。
- en: '*Text qualifiers are yet another type of special character used to denote that
    the contents of the column is a text string. They can be very useful in instances
    where the actual data is allowed to contain characters that are also being used
    as row or column delimiters. This is a fairly common issue when working with CSV
    files that contain text data, because commas normally show up in text. Surrounding
    these columns with text qualifiers indicates that any instances of the column
    or row delimiters inside the text qualifiers should be ignored.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*文本限定符是另一种特殊字符，用于表示列的内容是文本字符串。在允许实际数据包含用作行或列分隔符的字符的情况下，它们非常有用。这在处理包含文本数据的CSV文件时是一个相当常见的问题，因为逗号通常出现在文本中。用文本限定符包围这些列表示，在文本限定符内部出现的任何列或行分隔符实例都应该被忽略。'
- en: Now that you’ve had a look at the structure of delimited text files, let’s have
    a look at how to apply this knowledge by importing some delimited text files into
    Dask. The NYC Parking Ticket data we briefly looked at in chapter 2 comes as a
    set of CSV files, so this will be a perfect dataset to work with for this example.
    If you haven’t downloaded the data already, you can do so by visiting www.kaggle.com/new-york-city/nyc-parking-tickets.
    As I mentioned before, I’ve unzipped the data into the same folder as the Jupyter
    notebook I’m working in for convenience’s sake. If you’ve put your data elsewhere,
    you’ll need to change the file path to match the location where you saved the
    data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了分隔文本文件的结构，让我们看看如何通过将一些分隔文本文件导入 Dask 来应用这些知识。在第 2 章中我们简要查看的纽约市停车罚单数据是以一组
    CSV 文件的形式提供的，因此这将是一个非常适合本例的数据集。如果你还没有下载数据，你可以通过访问 www.kaggle.com/new-york-city/nyc-parking-tickets
    来下载。正如我之前提到的，为了方便起见，我已经将数据解压到了我正在使用的 Jupyter 笔记本所在的同一个文件夹中。如果你将数据放在了其他地方，你需要更改文件路径以匹配你保存数据的位置。
- en: Listing 4.1 Importing CSV files using Dask defaults
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.1 使用 Dask 默认设置导入 CSV 文件
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In [listing 4.1](#listing4.1), the first three lines should look familiar:
    we’re simply importing the DataFrame library and the ProgressBar context. In the
    next four lines of code, we’re reading in the four CSV files that come with the
    NYC Parking Ticket dataset. For now, we’ll read each file into its own separate
    DataFrame. Let’s have a look at what happened by inspecting the `fy17` DataFrame.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 4.1](#listing4.1) 中，前三行应该看起来很熟悉：我们只是导入了 DataFrame 库和 ProgressBar 上下文。在接下来的四行代码中，我们正在读取
    NYC 停车罚单数据集附带的所有四个 CSV 文件。目前，我们将每个文件读取到它自己的单独 DataFrame 中。让我们通过检查 `fy17` DataFrame
    来看看发生了什么。
- en: '![c04_03.eps](Images/c04_03.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![c04_03.eps](Images/c04_03.png)'
- en: '[Figure 4.3](#figureanchor4.3) The metadata of the `fy17` DataFrame'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.3](#figureanchor4.3) `fy17` DataFrame 的元数据'
- en: In [figure 4.3](#figure4.3), we see the metadata of the `fy17` DataFrame. Using
    the default 64 MB blocksize, the data was split into 33 partitions. You might
    recall this from chapter 3\. You can also see the column names at the top, but
    where did those come from? By default, Dask assumes that your CSV files will have
    a header row, and our file indeed has a header row. If you look at the raw CSV
    file in your favorite text editor, you will see the column names on the first
    line of the file. If you want to see all the column names, you can inspect the
    `columns` attribute of the DataFrame.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 4.3](#figure4.3) 中，我们看到了 `fy17` DataFrame 的元数据。使用默认的 64 MB 块大小，数据被分割成了
    33 个分区。你可能还记得这一点来自第 3 章。你还可以看到顶部的列名，但它们是从哪里来的？默认情况下，Dask 假设你的 CSV 文件将有一个标题行，而我们的文件确实有一个标题行。如果你用你最喜欢的文本编辑器查看原始
    CSV 文件，你将看到文件的第一行上的列名。如果你想查看所有列名，你可以检查 DataFrame 的 `columns` 属性。
- en: Listing 4.2 Inspecting the columns of a DataFrame
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.2 检查 DataFrame 的列
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If you happen to take a look at the columns of any other DataFrame, such as
    `fy14 (Parking Tickets for 2014)`, you’ll notice that the columns are different
    from the `fy17 (Parking Tickets for 2017)` DataFrame. It looks as though the NYC
    government changed what data it collects about parking violations in 2017\. For
    example, the latitude and longitude of the violation was not recorded prior to
    2017, so these columns won’t be useful for analyzing year-over-year trends (such
    as how parking violation “hotspots” migrate throughout the city). If we simply
    concatenated the datasets together as is, we would get a resulting DataFrame with
    an awful lot of missing values. Before we combine the datasets, we should find
    the columns that all four of the DataFrames have in common. Then we should be
    able to simply union the DataFrames together to produce a new DataFrame that contains
    all four years of data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你偶然查看其他 DataFrame 的列，比如 `fy14（2014年停车罚单）`，你会注意到列与 `fy17（2017年停车罚单）` DataFrame
    的列不同。看起来纽约市政府在 2017 年改变了它收集关于停车违规的数据。例如，违规的纬度和经度在 2017 年之前没有记录，因此这些列对于分析年度趋势（例如停车违规“热点”如何在整个城市中迁移）将没有用。如果我们简单地像这样连接数据集，我们会得到一个包含大量缺失值的
    DataFrame。在我们合并数据集之前，我们应该找到所有四个 DataFrame 共有的列。然后我们应该能够简单地联合 DataFrame 来生成一个新的
    DataFrame，其中包含所有四年的数据。
- en: We could manually look at each DataFrame’s columns and deduce which columns
    overlap, but that would be terribly inefficient. Instead, we’ll automate the process
    by taking advantage of the DataFrames’ `columns` attribute and Python’s set operations.
    The following listing shows you how to do this.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以手动查看每个 DataFrame 的列并推断哪些列重叠，但这将非常低效。相反，我们将通过利用 DataFrame 的 `columns` 属性和
    Python 的集合操作来自动化这个过程。以下列表显示了如何进行此操作。
- en: Listing 4.3 Finding the common columns between the four DataFrames
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.3 查找四个 DataFrame 之间的公共列
- en: '[PRE7]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'On the first line, we create a list that contains four set objects, respectively
    representing each DataFrame’s columns. On the next line, we take advantage of
    the `intersection` method of set objects that returns a set containing the items
    that exist in both of the sets it’s comparing. Wrapping this in a `reduce` function,
    we’re able to walk through each DataFrame’s metadata, pull out the columns that
    are common to all four DataFrames, and discard any columns that aren’t found in
    all four DataFrames. What we’re left with is the following abbreviated list of
    columns:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一行，我们创建了一个包含四个集合对象的列表，分别代表每个 DataFrame 的列。在下一行，我们利用集合对象的 `intersection` 方法，该方法返回一个包含两个集合中存在的项的集合。通过将其包装在
    `reduce` 函数中，我们能够遍历每个 DataFrame 的元数据，提取所有四个 DataFrame 共同的列，并丢弃在所有四个 DataFrame
    中未找到的列。我们最终得到以下简化的列列表：
- en: '[PRE8]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now that we have a set of common columns shared by all four of the DataFrames,
    let’s take a look at the first couple of rows of the `fy17` DataFrame.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一组所有四个 DataFrame 共享的公共列，让我们看一下 `fy17` DataFrame 的前几行。
- en: Listing 4.4 Looking at the head of the `fy17` DataFrame
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.4 查看 `fy17` DataFrame 的头部
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![c04_04.eps](Images/c04_04.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![c04_04.eps](Images/c04_04.png)'
- en: '[Figure 4.4](#figureanchor4.4) The first five rows of the `fy17` DataFrame
    using the common column set'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.4](#figureanchor4.4) 使用公共列集查看 `fy17` DataFrame 的前五行'
- en: 'Two important things are happening in [listing 4.4](#listing4.4): the column
    filtering operation and the top collecting operation. Specifying one or more columns
    in square brackets to the right of the DataFrame name is the primary way you can
    select/filter columns in the DataFrame. Since `common_columns` is a list of column
    names, we can pass that in to the column selector and get a result containing
    the columns contained in the list. We’ve also chained a call to the `head` method,
    which allows you to view the top *n* rows of a DataFrame. As shown in [figure
    4.4](#figure4.4), by default, it will return the first five rows of the DataFrame,
    but you can specify the number of rows you wish to retrieve as an argument. For
    example, `fy17.head(10)` will return the first 10 rows of the DataFrame. Keep
    in mind that when you get rows back from Dask, they’re being loaded into your
    computer’s RAM. So, if you try to return too many rows of data, you will receive
    an out-of-memory error. Now let’s try the same call on the `fy14` DataFrame.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 4.4](#listing4.4) 中发生了两件重要的事情：列过滤操作和顶部收集操作。在 DataFrame 名称右侧的方括号中指定一个或多个列是您在
    DataFrame 中选择/过滤列的主要方式。由于 `common_columns` 是一个列名列表，我们可以将其传递给列选择器，以获取包含列表中列的结果。我们还链式调用了
    `head` 方法，这允许您查看 DataFrame 的顶部 *n* 行。如图 [图 4.4](#figure4.4) 所示，默认情况下，它将返回 DataFrame
    的前五行，但您可以将要检索的行数作为参数指定。例如，`fy17.head(10)` 将返回 DataFrame 的前 10 行。请注意，当您从 Dask 获取行时，它们正在被加载到您的计算机
    RAM 中。因此，如果您尝试返回过多的数据行，您将收到内存不足错误。现在让我们在 `fy14` DataFrame 上尝试相同的调用。
- en: Listing 4.5 Looking at the head of the `fy14` DataFrame
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.5 查看 `fy14` DataFrame 的头部
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Looks like Dask ran into trouble when trying to read the `fy14` data! Thankfully,
    the Dask development team has given us some pretty detailed information in this
    error message about what happened. Five columns—Issuer Squad, Unregistered Vehicle?,
    Violation Description, Violation Legal Code, and Violation Post Code—failed to
    be read correctly because their datatypes were not what Dask expected. As we learned
    in chapter 2, Dask uses random sampling to infer datatypes to avoid scanning the
    entire (potentially massive) DataFrame. Although this usually works well, it can
    break down when a large number of values are missing in a column or the vast majority
    of data can be classified as one datatype (such as an integer), but a small number
    of edge cases break that assumption (such as a random string or two). When that
    happens, Dask will throw an exception once it begins to work on a computation.
    In order to help Dask read our dataset correctly, we’ll need to manually define
    a schema for our data instead of relying on type inference. Before we get around
    to doing that, let’s review what datatypes are available in Dask so we can create
    an appropriate schema for our data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来 Dask 在尝试读取 `fy14` 数据时遇到了麻烦！幸运的是，Dask 开发团队在这个错误消息中给了我们一些相当详细的信息，关于发生了什么。五个列——发行方小队、未注册车辆？、违规描述、违规法律代码和违规邮政代码——未能正确读取，因为它们的数据类型不是
    Dask 预期的。正如我们在第 2 章中学到的，Dask 使用随机抽样来推断数据类型，以避免扫描整个（可能巨大的）DataFrame。尽管这通常效果很好，但当列中缺少大量值或绝大多数数据可以归类为一种数据类型（如整数）时，它可能会失败，但少数边缘情况会打破这个假设（如一个或两个随机字符串）。当这种情况发生时，Dask
    会在开始处理计算时抛出异常。为了帮助 Dask 正确读取我们的数据集，我们需要手动为我们的数据定义一个模式，而不是依赖于类型推断。在我们着手做那之前，让我们回顾一下
    Dask 中可用的数据类型，这样我们就可以为我们的数据创建一个适当的模式。
- en: 4.1.1 Using Dask datatypes
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 使用 Dask 数据类型
- en: Similar to relational database systems, column datatypes play an important role
    in Dask DataFrames. They control what kind of operations can be performed on a
    column, how overloaded operators (+, -, and so on) behave, and how memory is allocated
    to store and access the column’s values. Unlike most collections and objects in
    Python, Dask DataFrames use explicit typing rather than duck typing. This means
    that all values contained in a column must conform to the same datatype. As we
    saw already, Dask will throw errors if values in a column are found that violate
    the column’s datatype.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 与关系型数据库系统类似，列数据类型在 Dask DataFrame 中起着重要作用。它们控制可以在列上执行的操作类型，过载运算符（+、- 等）的行为，以及如何分配内存来存储和访问列的值。与
    Python 中的大多数集合和对象不同，Dask DataFrame 使用显式类型而不是鸭子类型。这意味着列中包含的所有值都必须符合相同的类型。正如我们之前所看到的，如果发现列中的值违反了列的数据类型，Dask
    将会抛出错误。
- en: Since Dask DataFrames consist of partitions made up of Pandas DataFrames, which
    in turn are complex collections of NumPy arrays, Dask sources its datatypes from
    NumPy. The NumPy library is a powerful and important mathematics library for Python.
    It enables users to perform advanced operations from linear algebra, calculus,
    and trigonometry. This library is important for the needs of data science because
    it provides the cornerstone mathematics for many statistical analysis methods
    and machine learning algorithms in Python. Let’s take a look at NumPy’s datatypes,
    which can be seen in [figure 4.5](#figure4.5).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Dask DataFrame 由 Pandas DataFrame 组成的分区构成，而 Pandas DataFrame 又是由 NumPy 数组组成的复杂集合，因此
    Dask 从 NumPy 中获取其数据类型。NumPy 库是 Python 中一个强大且重要的数学库。它使用户能够执行线性代数、微积分和三角学的先进操作。这个库对于数据科学的需求非常重要，因为它为
    Python 中许多统计分析方法和机器学习算法提供了基石数学。让我们看看 NumPy 的数据类型，这些类型可以在[图 4.5](#figure4.5)中看到。
- en: '![c04_05.eps](Images/c04_05.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![c04_05.eps](Images/c04_05.png)'
- en: '[Figure 4.5](#figureanchor4.5) NumPy datatypes used by Dask'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.5](#figureanchor4.5) Dask 使用的 NumPy 数据类型'
- en: As you can see, many of these reflect the primitive types in Python. The biggest
    difference is that NumPy datatypes can be explicitly sized with a specified bit-width.
    For example, the `int32` datatype is a 32-bit integer that allows any integer
    between −2,147,483,648 and 2,147,483,647\. Python, by comparison, always uses
    the maximum bit-width based on your operating system and hardware’s support. So,
    if you’re working on a computer with a 64-bit CPU and running a 64-bit OS, Python
    will always allocate 64 bits of memory to store an integer. The advantage of using
    smaller datatypes where appropriate is that you can hold more data in RAM and
    the CPU’s cache at one time, leading to faster, more efficient computations. This
    means that when creating a schema for your data, you should always choose the
    smallest possible datatype to hold your data. The risk, however, is that if a
    value exceeds the maximum size allowed by the particular datatype, you will experience
    overflow errors, so you should think carefully about the range and domain of your
    data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，许多这些类型反映了Python中的原始类型。最大的区别是NumPy数据类型可以显式地使用指定的位宽来定义。例如，`int32`数据类型是一个32位整数，允许任何介于-2,147,483,648和2,147,483,647之间的整数。相比之下，Python总是根据您的操作系统和硬件支持使用最大位宽。因此，如果您在64位CPU上运行64位操作系统，Python将始终分配64位内存来存储一个整数。在适当的情况下使用较小的数据类型的好处是您可以在一次内存和CPU缓存中保存更多的数据，从而实现更快、更高效的计算。这意味着在创建数据模式时，您应该始终选择可以存储数据的可能最小的数据类型。然而，风险是如果某个值超过了特定数据类型允许的最大大小，您将遇到溢出错误，因此您应该仔细考虑数据的范围和域。
- en: 'For example, consider house prices in the United States: home prices are typically
    above $32,767 and are unlikely to exceed $2,147,483,647 for quite some time if
    historical inflation rates prevail. Therefore, if you were to store house prices
    rounded to the nearest whole dollar, the `int32` datatype would be most appropriate.
    While the `int64` and `int128` types are wide enough to hold this range of numbers,
    it would be inefficient to use more than 32 bits of memory to store each value.
    Likewise, using `int8` or `int16` would not be large enough to hold the data,
    resulting in an overflow error.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑美国的房价：房价通常高于$32,767，如果历史通货膨胀率持续存在，在相当长的一段时间内不太可能超过$2,147,483,647。因此，如果您要将房价四舍五入到最接近的整数，`int32`数据类型将是最合适的。虽然`int64`和`int128`类型足够宽，可以容纳这个数字范围，但使用超过32位的内存来存储每个值将是不高效的。同样，使用`int8`或`int16`将不足以容纳数据，从而导致溢出错误。
- en: If none of the NumPy datatypes are appropriate for the kind of data you have,
    a column can be stored as an `object` type, which represents any Python object.
    This is also the datatype that Dask will default to when its type inference comes
    across a column that has a mix of numbers and strings, or when type inference
    cannot determine an appropriate datatype to use. However, one common exception
    to this rule happens when you have a column with a high percentage of missing
    data. Take a look at [figure 4.6](#figure4.6), which shows part of the output
    of that last error message again.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有适合您数据的NumPy数据类型，可以将列存储为`object`类型，这代表任何Python对象。这也是当Dask的类型推断遇到包含数字和字符串混合的列，或者无法确定使用适当的数据类型时，Dask默认使用的数据类型。然而，这个规则的一个常见例外是当您有一个具有高比例缺失数据的列。请看[图4.6](#figure4.6)，它再次显示了最后一个错误消息的部分输出。
- en: '![c04_06.eps](Images/c04_06.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![c04_06.eps](Images/c04_06.png)'
- en: '[Figure 4.6](#figureanchor4.6) A Dask error showing mismatched datatypes'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.6](#figureanchor4.6) 一个显示不匹配数据类型的Dask错误'
- en: Would you really believe that a column called Violation Description should be
    a floating-point number? Probably not! Typically, we can expect description columns
    to be text, and therefore Dask should use an object datatype. Then why did Dask’s
    type inference think the column holds 64-bit floating-point numbers? It turns
    out that a large majority of records in this DataFrame have missing violation
    descriptions. In the raw data, they are simply blank. Dask treats blank records
    as null values when parsing files, and by default fills in missing values with
    NumPy’s NaN (not a number) object called `np.nan`. If you use Python’s built-in
    type function to inspect the datatype of an object, it reports that `np.nan` is
    a float type. So, since Dask’s type inference randomly selected a bunch of `np.nan`
    objects when trying to infer the type of the Violation Description column, it
    assumed that the column must contain floating-point numbers. Now let’s fix the
    problem so we can read in our DataFrame with the appropriate datatypes.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 你真的相信一个名为“违规描述”的列应该是浮点数吗？可能不会！通常，我们可以预期描述列是文本类型，因此 Dask 应该使用对象数据类型。那么为什么 Dask
    的类型推断认为该列包含 64 位浮点数呢？实际上，这个 DataFrame 中的大多数记录都有缺失的违规描述。在原始数据中，它们只是空白。Dask 在解析文件时将空白记录视为空值，并默认使用
    NumPy 的 NaN（不是一个数字）对象 `np.nan` 来填充缺失值。如果你使用 Python 内置的类型函数来检查对象的类型，它会报告 `np.nan`
    是浮点类型。因此，由于 Dask 的类型推断在尝试推断“违规描述”列的类型时随机选择了一组 `np.nan` 对象，它假设该列必须包含浮点数。现在让我们修复这个问题，以便我们可以使用适当的数据类型读取我们的
    DataFrame。
- en: 4.1.2 Creating schemas for Dask DataFrames
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 为 Dask DataFrame 创建模式
- en: 'Oftentimes when working with a dataset, you’ll know each column’s datatype,
    whether it can contain missing values, and its valid range of values ahead of
    time. This information is collectively known as the data’s *schema*. You’re especially
    likely to know the schema for a dataset if it came from a relational database.
    Each column in a database table must have a well-known datatype. If you have this
    information ahead of time, using with Dask is as easy as writing up the schema
    and applying it to the `read_csv` method. You’ll see how to do that at the end
    of this section. However, sometimes you might not know what the schema is ahead
    of time, and you’ll need to figure it out on your own. Perhaps you’re pulling
    data from a web API which hasn’t been properly documented or you’re analyzing
    a data extract and you don’t have access to the data source. Neither of these
    approaches is ideal because they can be tedious and time consuming, but sometimes
    you may really have no other option. Here are two methods you can try:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理数据集时，通常你会在事先知道每列的数据类型、它是否可以包含缺失值以及其有效值范围。这些信息统称为数据的“模式”。如果你从关系型数据库中获取数据集，你很可能知道该数据集的模式。数据库表中的每一列都必须有一个已知的良好数据类型。如果你提前有这些信息，使用
    Dask 就像编写模式并将其应用于 `read_csv` 方法一样简单。你将在本节的末尾看到如何做到这一点。然而，有时你可能不知道模式是什么，你需要自己找出它。也许你正在从没有适当文档的
    Web API 中提取数据，或者你正在分析数据提取，并且没有访问数据源。这两种方法都不是理想的，因为它们可能会很繁琐且耗时，但有时你可能真的没有其他选择。这里有两种你可以尝试的方法：
- en: Guess-and-check
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 猜测并检查
- en: Manually sample the data
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动采样数据
- en: The guess-and-check method isn’t complicated. If you have well-named columns,
    such as Product Description, Sales Amount, and so on, you can try to infer what
    kind of data each column contains using the names. If you run into a datatype
    error while running a computation like the ones we’ve seen, simply update the
    schema and start over again. The advantage of this method is that you can quickly
    and easily try different schemas, but the downside is that it may become tedious
    to constantly restart your computations if they continue to fail due to datatype
    issues.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 猜测并检查的方法并不复杂。如果你有良好命名的列，例如产品描述、销售额等，你可以尝试使用名称来推断每个列包含的数据类型。如果你在运行我们看到的计算时遇到数据类型错误，只需更新模式并重新开始。这种方法的优势在于你可以快速轻松地尝试不同的模式，但缺点是如果由于数据类型问题而不断失败，可能需要不断地重新启动你的计算，这可能会变得繁琐。
- en: The manual sampling method aims to be a bit more sophisticated but can take
    more time up front since it involves scanning through some of the data to profile
    it. However, if you’re planning to analyze the dataset anyways, it’s not “wasted”
    time in the sense that you will be familiarizing yourself with the data while
    creating the schema. Let’s look at how we can do this.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 手动采样方法旨在更加复杂，但可能需要更多的时间来扫描一些数据以进行配置。然而，如果你计划无论如何都要分析数据集，那么这并不是“浪费”的时间，因为在创建模式的同时，你将熟悉数据。让我们看看我们如何做到这一点。
- en: Listing 4.6 Building a generic schema
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.6 构建通用模式
- en: '[PRE11]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: First we need to build a dictionary that maps column names to datatypes. This
    must be done because the `dtype` argument that we’ll feed this object into later
    expects a dictionary type. To do that, in [listing 4.6](#listing4.6), we first
    walk through the `common_columns` list that we made earlier to hold all of the
    column names that can be found in all four DataFrames. We transform each column
    name into a tuple containing the column name and the `np.str` datatype, which
    represents strings. On the second line, we take the list of tuples and convert
    them into a dict, the partial contents of which are displayed. Now that we’ve
    constructed a generic schema, we can apply it to the `read_csv` function to use
    the schema to load the `fy14` data into a DataFrame.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要构建一个将列名映射到数据类型的字典。这是因为我们稍后要传递给此对象的 `dtype` 参数期望一个字典类型。为此，在 [列表 4.6](#listing4.6)
    中，我们首先遍历我们之前制作的 `common_columns` 列表，该列表包含所有四个 DataFrame 中可以找到的所有列名。我们将每个列名转换成一个包含列名和
    `np.str` 数据类型的元组，其中 `np.str` 代表字符串。在第二行，我们将元组列表转换为字典，部分内容如下所示。现在我们已经构建了一个通用模式，我们可以将其应用于
    `read_csv` 函数，以使用该模式将 `fy14` 数据加载到 DataFrame 中。
- en: Listing 4.7 Creating a DataFrame with an explicit schema
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.7 使用显式模式创建 DataFrame
- en: '[PRE12]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[Listing 4.7](#listing4.7) looks largely the same as the first time we read
    in the 2014 data file. However, this time we specified the `dtype` argument and
    passed in our schema dictionary. What happens under the hood is Dask will disable
    type inference for the columns that have matching keys in the `dtype` dictionary
    and use the explicitly specified types instead. While it’s perfectly reasonable
    to include only the columns you want to change, it’s best to not rely on Dask’s
    type inference at all whenever possible. Here I’ve shown you how to create an
    explicit schema for all columns in a DataFrame, and I encourage you to make this
    a regular practice when working with big datasets. With this particular schema,
    we’re telling Dask to just assume that all of the columns are strings. Now if
    we try to view the first five rows of the DataFrame again, using `fy14[common_columns].head()`,
    Dask doesn’t throw an error message! But we’re not done yet. We now need to have
    a look at each column and pick a more appropriate datatype (if possible) to maximize
    efficiency. Let’s have a look at the Vehicle Year column.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.7](#listing4.7) 在第一次读取 2014 年数据文件时看起来大致相同。然而，这次我们指定了 `dtype` 参数，并传递了我们的模式字典。在底层发生的事情是
    Dask 将禁用具有匹配键的 `dtype` 字典中的列的类型推断，并使用显式指定的类型。虽然只包括你想要更改的列是完全合理的，但最好在可能的情况下根本不依赖
    Dask 的类型推断。在这里，我向你展示了如何为 DataFrame 中的所有列创建显式模式，并鼓励你在处理大型数据集时将其作为常规做法。使用这个特定的模式，我们告诉
    Dask 假设所有列都是字符串。现在，如果我们再次尝试使用 `fy14[common_columns].head()` 查看DataFrame的前五行，Dask
    不会抛出错误信息！但我们的工作还没有完成。我们现在需要查看每个列，并选择一个更合适的（如果可能的话）数据类型以最大化效率。让我们看看“车辆年份”列。'
- en: Listing 4.8 Inspecting the Vehicle Year column
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.8 检查车辆年份列
- en: '[PRE13]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In [listing 4.8](#listing4.8), we’re simply looking at 10 of the unique values
    contained in the Vehicle Year column. It looks like they are all integers that
    would fit comfortably in the `uint16` datatype. `uint16` is the most appropriate
    because years can’t be negative values, and these years are too large to be stored
    in `uint8` (which has a maximum size of 255). If we had seen any letters or special
    characters, we would not need to proceed any further with analyzing this column.
    The string datatype we had already selected would be the only datatype suitable
    for the column.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 4.8](#listing4.8) 中，我们只是查看“车辆年份”列中包含的 10 个唯一值。看起来它们都是可以舒适地存储在 `uint16`
    数据类型中的整数。`uint16` 是最合适的，因为年份不能是负值，而且这些年份太大，不能存储在 `uint8`（最大大小为 255）中。如果我们看到任何字母或特殊字符，我们就不需要进一步分析这个列。我们之前选择的字符串数据类型将是该列唯一适合的数据类型。
- en: One thing to be careful about is that a sample of 10 unique values might not
    be a sufficiently large enough sample size to determine that there aren’t any
    edge cases you need to consider. You could use `.compute``()` instead of `.head``()`
    to bring back all the unique values, but this might not be a good idea if the
    particular column you’re looking at has a high degree of uniqueness to it (such
    as a primary key or a high-dimensional category). The range of 10–50 unique samples
    has served me well in most cases, but sometimes you will still run into edge cases
    where you will need to go back and tweak your datatypes.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一件事是，10 个唯一值的样本可能不足以确定没有需要考虑的边缘情况。您可以使用 `.compute()` 而不是 `.head()` 来获取所有唯一值，但如果您查看的特定列具有高度的唯一性（例如主键或高维类别），这可能不是一个好主意。10-50
    个唯一样本的范围在大多数情况下都对我很有帮助，但有时您仍然会遇到需要返回并调整数据类型的边缘情况。
- en: 'Since we’re thinking an integer datatype might be appropriate for this column,
    we need to check one more thing: Are there any missing values in this column?
    As you learned earlier, Dask represents missing values with `np.nan`, which is
    considered to be a float type object. Unfortunately, `np.nan` cannot be cast or
    coerced to an integer `uint16` datatype. In the next chapter we will learn how
    to deal with missing values, but for now if we come across a column with missing
    values, we will need to ensure that the column will use a datatype that can support
    the `np.nan` object. This means that if the Vehicle Year column contains any missing
    values, we’ll be required to use a float32 datatype and not the `uint16` datatype
    we originally thought appropriate because `uint16` is unable to store `np.nan`.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们考虑整数数据类型可能适用于此列，我们需要检查一件事：此列中是否有任何缺失值？如您之前所学，Dask 使用 `np.nan` 来表示缺失值，这被视为一个浮点类型对象。不幸的是，`np.nan`
    不能被转换为或强制转换为整数 `uint16` 数据类型。在下一章中，我们将学习如何处理缺失值，但到目前为止，如果我们遇到有缺失值的列，我们需要确保该列将使用可以支持
    `np.nan` 对象的数据类型。这意味着如果车辆年份列包含任何缺失值，我们将需要使用 `float32` 数据类型，而不是我们最初认为合适的 `uint16`
    数据类型，因为 `uint16` 无法存储 `np.nan`。
- en: Listing 4.9 Checking the Vehicle Year column for missing values
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.9 检查车辆年份列是否存在缺失值
- en: '[PRE14]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In [listing 4.9](#listing4.9), we’re using the `isnull` method, which checks
    each value in the specified column for existence of `np.nan`. It returns `True`
    if `np.nan` is found and False if it’s not, and then aggregates the checks for
    all rows into a Boolean Series. Chaining with `.values.any``()` reduces the Boolean
    Series to a single `True` if at least one row is True, and `False` if no rows
    are True. This means that if the code in [listing 4.9](#listing4.9) returns `True`,
    at least one row in the Vehicle Year column is missing. If it returned `False`,
    it would indicate that no rows in the Vehicle Year column are missing data. Since
    we have missing values in the Vehicle Year column, we must use the `float32` datatype
    for the column instead of `uint16`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 4.9](#listing4.9) 中，我们使用了 `isnull` 方法，该方法检查指定列中每个值的 `np.nan` 存在情况。如果找到
    `np.nan`，则返回 `True`，如果没有找到，则返回 `False`，然后将所有行的检查汇总为一个布尔 Series。通过 `.values.any()`
    连接，布尔 Series 被简化为单个 `True`（如果至少有一行是 `True`），或者 `False`（如果没有一行是 `True`）。这意味着如果
    [列表 4.9](#listing4.9) 中的代码返回 `True`，则车辆年份列中至少有一行缺失。如果它返回 `False`，则表示车辆年份列中没有缺失数据。由于车辆年份列中存在缺失值，我们必须使用
    `float32` 数据类型而不是 `uint16` 数据类型。
- en: Now, we should repeat the process for the remaining 42 columns. For brevity’s
    sake, I’ve gone ahead and done this for you. In this particular instance, we could
    also use the data dictionary posted on the Kaggle webpage (at [https://www.kaggle.com/new-york-city/nyc-parking-tickets/data](https://www.kaggle.com/new-york-city/nyc-parking-tickets/data))
    to help speed along this process.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们应该对剩余的 42 个列重复此过程。为了简洁起见，我已经为您完成了这项工作。在这种情况下，我们也可以使用 Kaggle 网页上发布的数据字典（在
    [https://www.kaggle.com/new-york-city/nyc-parking-tickets/data](https://www.kaggle.com/new-york-city/nyc-parking-tickets/data)）来帮助加快这一过程。
- en: Listing 4.10 The final schema for the NYC Parking Ticket Data
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.10 纽约市停车罚单数据的最终模式
- en: '[PRE15]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[Listing 4.10](#listing4.10) contains the final schema for the NYC Parking
    Ticket data. Let’s use it to reload all four of the DataFrames, then union all
    four years of data together into a final DataFrame.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.10](#listing4.10) 包含了纽约市停车罚单数据的最终模式。让我们使用它来重新加载所有四个 DataFrame，然后将所有四年的数据合并到一个最终的
    DataFrame 中。'
- en: Listing 4.11 Applying the schema to all four DataFrames
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.11 将模式应用于所有四个 DataFrame
- en: '[PRE16]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In [listing 4.11](#listing4.11), we reload the data and apply the schema we
    created. Notice that instead of loading four separate files into four separate
    DataFrames, we’re now loading all CSV files contained in the nyc-parking-tickets
    folder into a single DataFrame by using the `*` wildcard. Dask provides this for
    convenience since it’s common to split large datasets into multiple files, especially
    on distributed filesystems. As before, we’re passing the final schema into the
    `dtype` argument, and we’re now also passing the list of columns we want to keep
    into the `usecols` argument. `usecols` takes a list of column names and drops
    any columns from the resulting DataFrame that aren’t specified in the list. Since
    we only care about analyzing the data we have available for all four years, we’ll
    choose to simply ignore the columns that aren’t shared across all four years.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 4.11](#listing4.11)中，我们重新加载数据并应用我们创建的模式。请注意，我们不再将四个单独的文件加载到四个单独的数据框中，而是现在通过使用`*`通配符将nyc-parking-tickets文件夹中包含的所有CSV文件加载到一个单独的数据框中。Dask提供这种便利，因为将大型数据集拆分成多个文件是很常见的，尤其是在分布式文件系统中。和之前一样，我们将最终的架构传递给`dtype`参数，现在我们还把我们要保留的列的列表传递给`usecols`参数。`usecols`接受一个列名列表，并从结果数据框中删除任何未在列表中指定的列。由于我们只关心分析我们拥有的所有四年数据，我们将选择简单地忽略所有四年都不共有的列。
- en: '`usecols` is an interesting argument because if you look at the Dask API documentation,
    it’s not listed. It might not be immediately obvious why this is, but it’s because
    the argument comes from Pandas. Since each partition of a Dask DataFrame is a
    Pandas DataFrame, you can pass along any Pandas arguments through the `*args`
    and `**kwargs` interfaces and they will control the underlying Pandas DataFrames
    that make up each partition. This interface is also how you can control things
    like which column delimiter should be used, whether the data has a header or not,
    and so on. The Pandas API documentation for `read_csv` and its many arguments
    can be found at [http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`usecols`是一个有趣的参数，因为如果你查看Dask API文档，它并没有列出。这也许不是立即显而易见的，但这是因为该参数来自Pandas。由于Dask数据框的每个分区都是一个Pandas数据框，你可以通过`*args`和`**kwargs`接口传递任何Pandas参数，它们将控制构成每个分区的底层Pandas数据框。这个接口也是你可以控制诸如应该使用哪种列分隔符、数据是否有标题等事情的方式。`read_csv`的Pandas
    API文档及其许多参数可以在[http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)找到。'
- en: We’ve now read in the data and we are ready to clean and analyze this DataFrame.
    If you count the rows, we have over 42.3 million parking violations to explore!
    However, before we get into that, we will look at interfacing with a few other
    storage systems as well as writing data. We’ll now look at reading data from relational
    database systems.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经读取了数据，我们准备清理和分析这个数据框。如果你数一数行数，我们就有超过4230万起停车违规行为要探索！然而，在我们深入之前，我们还将查看与其他存储系统的接口以及数据写入。现在我们将查看从关系型数据库系统中读取数据。
- en: 4.2 Reading data from relational databases
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 从关系型数据库读取数据
- en: Reading data from a relational database system (RDBMS) into Dask is fairly easy.
    In fact, you’re likely to find that the most tedious part of interfacing with
    RDBMSs is setting up and configuring your Dask environment to do so. Because of
    the wide variety of RDBMSs used in production environments, we can’t cover the
    specifics for each one here. But, a substantial amount of documentation and support
    is available online for the specific RDBMS you’re working with. The most important
    thing to be aware of is that when using Dask in a multi-node cluster, your client
    machine is not the only machine that will need access to the database. Each worker
    node needs to be able to access the database server, so it’s important to install
    the correct software and configure each node in the cluster to be able to do so.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 从关系型数据库系统（RDBMS）读取数据到Dask相当简单。实际上，你可能会发现与RDBMS接口的最繁琐部分是设置和配置你的Dask环境以实现这一点。由于生产环境中使用的RDBMS种类繁多，我们无法在这里涵盖每个系统的具体细节。但是，对于你正在工作的特定RDBMS，网上有大量的文档和支持。重要的是要注意，当在多节点集群中使用Dask时，你的客户端机器不是唯一需要访问数据库的机器。每个工作节点都需要能够访问数据库服务器，因此安装正确的软件并配置集群中的每个节点以实现这一点非常重要。
- en: Dask uses the SQL Alchemy library to interface with RDBMSs, and I recommend
    using the pyodbc library to manage your ODBC drivers. This means you will need
    to install and configure SQL Alchemy, pyodbc, and the ODBC drivers for your specific
    RDBMS on each machine in your cluster for Dask to work correctly. To learn more
    about SQL Alchemy, you can check out [www.sqlalchemy.org/library.html](http://www.sqlalchemy.org/library.html).
    Likewise, you can learn more about pyodbc at [https://github.com/mkleehammer/pyodbc/wiki](https://github.com/mkleehammer/pyodbc/wiki).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 使用 SQL Alchemy 库与 RDBMS 交互，我建议使用 pyodbc 库来管理你的 ODBC 驱动程序。这意味着你需要为集群中的每台机器安装和配置
    SQL Alchemy、pyodbc 以及特定 RDBMS 的 ODBC 驱动程序，以便 Dask 能够正确工作。要了解更多关于 SQL Alchemy 的信息，你可以查看
    [www.sqlalchemy.org/library.html](http://www.sqlalchemy.org/library.html)。同样，你可以在
    [https://github.com/mkleehammer/pyodbc/wiki](https://github.com/mkleehammer/pyodbc/wiki)
    上了解更多关于 pyodbc 的信息。
- en: Listing 4.12 Reading a SQL table into a Dask DataFrame
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.12 将 SQL 表读入 Dask DataFrame
- en: '[PRE17]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In [listing 4.12](#listing4.12), we first set up a connection to our database
    server by building a connection string. For this particular example, I’m using
    SQL Server on Linux from the official SQL Server Docker container on a Mac. Your
    connection string might look different based on the database server and operating
    system you’re running on. The last line demonstrates how to use the `read_sql_table`
    function to connect to the database and create the DataFrame. The first argument
    is the name of the database table you want to query, the second argument is the
    connection string, and the third argument is the column to use as the DataFrame’s
    index. These are the three required arguments for this function to work. However,
    you should be aware of a few important assumptions.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 4.12](#listing4.12) 中，我们首先通过构建连接字符串来设置与数据库服务器的连接。对于这个特定的例子，我正在使用 Mac 上的官方
    SQL Server Docker 容器中的 SQL Server。你的连接字符串可能根据你运行的数据库服务器和操作系统而有所不同。最后一行演示了如何使用
    `read_sql_table` 函数连接到数据库并创建 DataFrame。第一个参数是你想要查询的数据库表名，第二个参数是连接字符串，第三个参数是作为
    DataFrame 索引使用的列。这些是这个函数正常工作的三个必需参数。然而，你应该注意几个重要的假设。
- en: First, concerning datatypes, you might think that Dask gets datatype information
    directly from the database server since the database has a defined schema already.
    Instead, Dask samples the data and infers datatypes just like it does when reading
    a delimited text file. However, Dask sequentially reads the first five rows from
    the table instead of randomly sampling data across the dataset. Because databases
    indeed have a well-defined schema, Dask’s type inference is much more reliable
    when reading data from an RDBMS versus a delimited text file. However, it’s still
    not perfect. Because of the way data might be sorted, edge cases can come up that
    cause Dask to choose incorrect datatypes. For example, a string column might have
    some rows where the strings contain only numbers (“1456,” “2986,” and so on.)
    If the data is sorted in such a way that only these numeric-like strings appear
    in the sample Dask takes when inferring datatypes, it may incorrectly assume the
    column should be an integer datatype instead of a string datatype. In these situations,
    you may still have to do some manual schema tweaking as you learned in the previous
    section.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，关于数据类型，你可能认为 Dask 直接从数据库服务器获取数据类型信息，因为数据库已经有一个定义好的模式。相反，Dask 会采样数据并推断数据类型，就像它在读取带有分隔符的文本文件时做的那样。然而，Dask
    会顺序读取表中的前五行，而不是在数据集上随机采样数据。由于数据库确实有一个定义良好的模式，因此当从关系型数据库管理系统（RDBMS）读取数据时，Dask 的类型推断比从带有分隔符的文本文件读取数据时更为可靠。然而，它仍然不是完美的。由于数据可能是有序的，可能会出现边缘情况，导致
    Dask 选择错误的数据类型。例如，一个字符串列可能有一些行，其中的字符串只包含数字（例如，“1456”，“2986”，等等）。如果数据以这种方式排序，只有这些类似数字的字符串出现在
    Dask 推断数据类型时采样的样本中，它可能会错误地假设该列应该是整数数据类型而不是字符串数据类型。在这些情况下，你可能仍然需要像上一节中学到的那样进行一些手动模式调整。
- en: The second assumption is how the data should be partitioned. If the `index_col`
    (currently set to `'Summons Number'`) is a numeric or date/time datatype, Dask
    will automatically infer boundaries and partition the data based on a 256 MB block
    size (which is larger than `read_csv`’s 64 MB block size). However, if the `index_col`
    is not a numeric or date/time datatype, you must either specify the number of
    partitions or the boundaries to partition the data by.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个假设是数据应该如何分区。如果`index_col`（当前设置为`'Summons Number'`）是数值或日期/时间数据类型，Dask将自动推断边界并基于256
    MB的块大小（大于`read_csv`的64 MB块大小）对数据进行分区。然而，如果`index_col`不是数值或日期/时间数据类型，你必须指定分区数量或通过边界来分区数据。
- en: Listing 4.13 Even partitioning on a non-numeric or date/time index
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.13 在非数值或日期/时间索引上进行均匀分区
- en: '[PRE18]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In [listing 4.13](#listing4.13), we chose to index the DataFrame by the Vehicle
    Color column, which is a string column. Therefore, we have to specify how the
    DataFrame should be partitioned. Here, using the npartitions argument, we are
    telling Dask to split the DataFrame into 200 evenly sized pieces. Alternatively,
    we can manually specify boundaries for the partitions.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表4.13](#listing4.13)中，我们选择通过车辆颜色列对DataFrame进行索引，这是一个字符串列。因此，我们必须指定DataFrame应该如何分区。在这里，使用npartitions参数，我们告诉Dask将DataFrame分割成200个大小均匀的部分。或者，我们也可以手动指定分区的边界。
- en: Listing 4.14 Custom partitioning on a non-numeric or date/time index
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.14 在非数值或日期/时间索引上进行自定义分区
- en: '[PRE19]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[Listing 4.14](#listing4.14) shows how to manually define partition boundaries.
    The important thing to note about this is Dask uses these boundaries as an alphabetically
    sorted half-closed interval. This means that you won’t have partitions that *only*
    contain the color defined by their boundary. For example, because green is alphabetically
    between blue and red, green cars will fall into the red partition. The “red partition”
    is actually all colors that are alphabetically greater than blue and alphabetically
    less than or equal to red. This isn’t really intuitive at first and can take some
    getting used to.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表4.14](#listing4.14) 展示了如何手动定义分区边界。需要注意的是，Dask将这些边界用作按字母顺序排序的半闭区间。这意味着你不会只有包含其边界定义的颜色分区的数据。例如，因为绿色在蓝色和红色之间按字母顺序排列，绿色汽车将落入红色分区。所谓的“红色分区”实际上是所有按字母顺序大于蓝色且小于或等于红色的颜色。这最初可能并不直观，可能需要一些时间来适应。'
- en: The third assumption that Dask makes when you pass only the minimum required
    parameters is that you want to select all columns from the table. You can limit
    the columns you get back using the `columns` argument, which behaves similarly
    to the `usecols` argument in `read_csv`. While you are allowed to use SQL Alchemy
    expressions in the argument, I recommend that you avoid offloading any computations
    to the database server, since you lose the advantages of parallelizing that computation
    that Dask gives you.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 当你只传递所需的最小参数时，Dask做出的第三个假设是你想要选择表中的所有列。你可以使用`columns`参数来限制你获取的列，该参数的行为类似于`read_csv`中的`usecols`参数。虽然你可以在参数中使用SQLAlchemy表达式，但我建议你避免将任何计算卸载到数据库服务器，因为这样你会失去Dask为你提供的并行化计算的优势。
- en: Listing 4.15 Selecting a subset of columns
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.15 选择列子集
- en: '[PRE20]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[Listing 4.15](#listing4.15) shows how to add a column filter to the connection
    query. Here we’ve created a list of column names that exist in the table; then
    we pass them to the columns argument. You can use the column filter even if you
    are querying a view instead of a table.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表4.15](#listing4.15) 展示了如何向连接查询添加列过滤器。在这里，我们创建了一个存在于表中的列名列表；然后我们将它们传递给columns参数。即使你在查询视图而不是表，你也可以使用列过滤器。'
- en: The fourth and final assumption made by providing the minimum arguments is the
    schema selection. When I say “schema” here, I’m not referring to the datatypes
    used by the DataFrame; I’m referring to the database schema object that RDBMSs
    use to group tables into logical clusters (such as `dim/fact` in a data warehouse
    or `sales`, `hr`, and so on, in a transactional database). If you don’t provide
    a schema, the database driver will use the default for the platform. For SQL Server,
    this results in Dask looking for the violations table in the `dbo` schema. If
    we had put the table in a different schema, perhaps one called `chapterFour`,
    we would receive a “table not found” error.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 提供最小参数所做的第四个也是最后一个假设是模式选择。当我说“模式”时，我并不是指 DataFrame 使用的数据类型；我指的是 RDBMS 用来将表分组到逻辑集群（例如数据仓库中的
    `dim/fact` 或事务数据库中的 `sales`、`hr` 等）的数据库模式对象。如果您不提供模式，数据库驱动程序将使用平台的默认模式。对于 SQL
    Server，这会导致 Dask 在 `dbo` 模式中查找违规表。如果我们把表放在不同的模式中，比如叫做 `chapterFour` 的模式，我们会收到“找不到表”的错误。
- en: Listing 4.16 Specifying a database schema
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.16 指定数据库模式
- en: '[PRE21]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[Listing 4.16](#listing4.16) shows you how to select a specific schema from
    Dask. Passing the schema name into the `schema` argument will cause Dask to use
    the provided database schema rather than the default.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.16](#listing4.16) 展示了如何在 Dask 中选择特定的模式。将模式名称传递给 `schema` 参数将导致 Dask 使用提供的数据库模式而不是默认模式。'
- en: Like `read_csv`, Dask allows you to forward along arguments to the underlying
    calls to the Pandas `read_sql` function being used at the partition level to create
    the Pandas DataFrames. We’ve covered all the most important functions here, but
    if you need an extra degree of customization, have a look at the API documentation
    for the Pandas `read_sql` function. All its arguments can be manipulated using
    the `*args` and `**kwargs` interfaces provided by Dask DataFrames. Now we’ll look
    at how Dask deals with distributed filesystems.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `read_csv` 类似，Dask 允许您将参数传递给底层调用 Pandas `read_sql` 函数，该函数在分区级别用于创建 Pandas
    DataFrame。我们已经在这里涵盖了所有最重要的函数，但如果您需要额外的定制程度，请查看 Pandas `read_sql` 函数的 API 文档。所有参数都可以使用
    Dask DataFrame 提供的 `*args` 和 `**kwargs` 接口进行操作。现在我们将看看 Dask 如何处理分布式文件系统。
- en: 4.3 Reading data from HDFS and S3
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 从 HDFS 和 S3 读取数据
- en: 'While it’s very likely that many datasets you’ll come across throughout your
    work will be stored in relational databases, powerful alternatives are rapidly
    growing in popularity. Most notable are the developments in distributed filesystem
    technologies from 2006 onward. Powered by technologies like Apache Hadoop and
    Amazon’s Simple Storage System (or S3 for short), distributed filesystems bring
    the same benefits to file storage that distributed computing brings to data processing:
    increased throughput, scalability, and robustness. Using a distributed computing
    framework alongside a distributed filesystem technology is a harmonious combination:
    in the most advanced distributed filesystems, such as the Hadoop Distributed File
    System (HDFS), nodes are aware of data locality, allowing computations to be shipped
    to the data rather than the data shipped to the compute resources. This saves
    a lot of time and back-and-forth communication over the network. [Figure 4.7](#figure4.7)
    demonstrates why keeping data isolated so a single node can have some performance
    consequences.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在您的工作中遇到的大多数数据集很可能存储在关系数据库中，但强大的替代品正迅速增长其受欢迎程度。最值得注意的是从 2006 年开始的分布式文件系统技术的发展。由
    Apache Hadoop 和亚马逊的简单存储系统（简称 S3）等技术驱动，分布式文件系统为文件存储带来了与分布式计算为数据处理带来的相同好处：提高吞吐量、可扩展性和健壮性。将分布式计算框架与分布式文件系统技术结合使用是一种和谐的组合：在最先进的分布式文件系统中，例如
    Hadoop 分布式文件系统（HDFS），节点了解数据本地性，允许计算被发送到数据而不是数据被发送到计算资源。这节省了大量时间和网络上的往返通信。[图 4.7](#figure4.7)
    展示了为什么保持数据隔离，以便单个节点可以有一些性能影响。
- en: '![c04_07.eps](Images/c04_07.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![c04_07.eps](Images/c04_07.png)'
- en: '[Figure 4.7](#figureanchor4.7) Running a distributed computation without a
    distributed filesystem'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.7](#figureanchor4.7) 在没有分布式文件系统的情况下运行分布式计算'
- en: A significant bottleneck is caused by the need to chunk up and ship data to
    the other nodes in the cluster. Under this configuration, when Dask reads in the
    data, it will partition the DataFrame as usual, but the other worker nodes can’t
    do any work until a partition of data is sent to them. Because it takes some time
    to transfer these 64 MB chunks over the network, the total computation time will
    be increased by the time it takes to ship data back and forth between the node
    that has the data and the other workers. This becomes even more problematic if
    the size of the cluster grows by any significant amount. If we had several hundred
    (or more) worker nodes vying for chunks of data all at once, the networking stack
    on the data node could easily get saturated with requests and slow to a crawl.
    Both of these problems can be mitigated by using a distributed filesystem. [Figure
    4.8](#figure4.8) shows how distributing the data across worker nodes makes the
    process more efficient.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 需要将数据分块并传输到集群中的其他节点，这导致了显著的瓶颈。在这种配置下，当 Dask 读取数据时，它将像往常一样对 DataFrame 进行分区，但其他工作节点无法进行任何工作，直到数据分区被发送到它们那里。由于这些
    64 MB 的大小块通过网络传输需要一些时间，因此总计算时间将因数据在拥有数据的节点和其他工作节点之间来回传输所需的时间而增加。如果集群的规模有任何显著的增长，这个问题将变得更加严重。如果我们有数百（或更多）个工作节点同时争夺数据块，数据节点的网络堆栈可能会很容易因为请求而饱和，速度变得极慢。这两个问题都可以通过使用分布式文件系统来缓解。[图
    4.8](#figure4.8) 展示了如何通过将数据分布到工作节点来使过程更加高效。
- en: '![c04_08.eps](Images/c04_08.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![c04_08.eps](Images/c04_08.png)'
- en: '[Figure 4.8](#figureanchor4.8) Running a distributed computation on a distributed
    filesystem'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.8](#figureanchor4.8) 在分布式文件系统上运行分布式计算'
- en: Instead of creating a bottleneck by holding data on only one node, the distributed
    filesystem chunks up data ahead of time and spreads it across multiple machines.
    It’s standard practice in many distributed filesystems to store redundant copies
    of chunks/partitions both for reliability and performance. From the perspective
    of reliability, storing each partition in triplicate (which is a common default
    configuration) means that two separate machines would have to fail before any
    data loss occurs. The probability of two machines failing in a short amount of
    time is much lower than the probability of one machine failing, so it adds an
    extra layer of safety at a nominal cost of additional storage.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 与仅在单个节点上保留数据以造成瓶颈不同，分布式文件系统会提前将数据分块并分散到多台机器上。在许多分布式文件系统中，存储数据块/分区的冗余副本是标准做法，既为了可靠性也为了性能。从可靠性的角度来看，将每个分区存储三份（这是一个常见的默认配置）意味着必须有两台不同的机器同时出现故障，才会发生数据丢失。在短时间内两台机器同时出现故障的概率远低于一台机器出现故障的概率，因此这增加了额外的安全层，而额外存储的成本却很小。
- en: From the performance perspective, spreading the data out across the cluster
    makes it more likely that a node containing the data will be available to run
    a computation when requested. Or, in the event that all worker nodes that hold
    that partition are already busy, one of them can ship the data to another worker
    node. In this case, spreading out the data avoids any single node getting saturated
    by requests for data. If one node is busy serving up a bunch of data, it can offload
    some of those requests to other nodes that hold the requested data. [Figure 4.9](#figure4.9)
    demonstrates why data-local distributed filesystems are even more advantageous.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 从性能的角度来看，将数据分散到整个集群中，使得包含数据的节点更有可能在请求时可用以运行计算。或者，如果持有该分区的所有工作节点都已经忙碌，其中一个可以将数据发送到另一个工作节点。在这种情况下，分散数据可以避免任何单个节点因为数据请求而饱和。如果一个节点正忙于提供大量数据，它可以将一些请求卸载到其他持有所需数据的节点。
    [图 4.9](#figure4.9) 阐述了为什么数据本地分布式文件系统具有更大的优势。
- en: '![c04_09.eps](Images/c04_09.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![c04_09.eps](Images/c04_09.png)'
- en: '[Figure 4.9](#figureanchor4.9) Shipping computations to the data'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.9](#figureanchor4.9) 将计算任务发送到数据'
- en: 'The node controlling the orchestration of the distributed computation (called
    the *driver*) knows that the data it wants to process is available in a few locations
    because the distributed filesystem maintains a catalogue of the data held within
    the system. It will first ask the machines that have the data locally whether
    they’re busy or not. If one of the nodes is not busy, the driver will instruct
    the worker node to perform the computation. If all the nodes are busy, the driver
    can either choose to wait until one of the worker nodes is free, or instruct another
    free worker node to get the data remotely and run the computation. HDFS and S3
    are two of the most popular distributed filesystems, but they have one key difference
    for our purposes: HDFS is designed to allow computations to run on the same nodes
    that serve up data, and S3 is not. Amazon designed S3 as a web service dedicated
    solely to file storage and retrieval. There’s absolutely no way to execute application
    code on S3 servers. This means that when you work with data stored in S3, you
    will always have to transmit partitions from S3 to a Dask worker node in order
    to process it. Let’s now take a look at how we can use Dask to read data from
    these systems.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 控制分布式计算编排的节点（称为 *driver*）知道它想要处理的数据位于几个位置，因为分布式文件系统维护着系统内数据的目录。它首先会询问拥有数据的本地机器是否忙碌。如果某个节点不忙碌，驱动器将指示工作节点执行计算。如果所有节点都忙碌，驱动器可以选择等待直到某个工作节点空闲，或者指示另一个空闲的工作节点远程获取数据并运行计算。HDFS
    和 S3 是最受欢迎的分布式文件系统之一，但它们在我们的用途上有一个关键的区别：HDFS 是设计为允许计算在提供数据的同一节点上运行，而 S3 则不是。亚马逊设计了
    S3 作为一种仅专注于文件存储和检索的专用网络服务。在 S3 服务器上执行应用程序代码是完全不可能的。这意味着当你处理存储在 S3 中的数据时，你将始终需要将分区从
    S3 传输到 Dask 工作节点以进行处理。现在让我们看看我们如何使用 Dask 从这些系统中读取数据。
- en: Listing 4.17 Reading data from HDFS
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.17 从 HDFS 读取数据
- en: '[PRE22]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In [listing 4.17](#listing4.17), we have a `read_csv` call that should look
    very familiar by now. In fact, the only thing that’s changed is the file path.
    Prefixing the file path with `hdfs://` tells Dask to look for the files on an
    HDFS cluster instead of the local filesystem, and `localhost` indicates that Dask
    should query the local HDFS NameNode for information on the whereabouts of the
    file.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 4.17](#listing4.17) 中，我们有一个 `read_csv` 调用，现在应该看起来非常熟悉了。实际上，唯一改变的是文件路径。在文件路径前加上
    `hdfs://` 告诉 Dask 在 HDFS 集群上查找文件，而不是在本地文件系统中查找，而 `localhost` 指示 Dask 应该查询本地 HDFS
    NameNode 以获取文件位置信息。
- en: All the arguments for `read_csv` that you learned before can still be used here.
    In this way, Dask makes it extremely easy to work with HDFS. The only additional
    requirement is that you install the hdfs3 library on each of your Dask workers.
    This library allows Dask to communicate with HDFS; therefore, this functionality
    won’t work if you haven’t installed the package. You can simply install the package
    with pip or conda (hdfs3 is on the conda-forge channel).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 所有之前你学到的关于 `read_csv` 的参数都可以在这里使用。这样，Dask 使得与 HDFS 的工作变得极其简单。唯一的额外要求是，你需要在每个
    Dask 工作节点上安装 hdfs3 库。这个库允许 Dask 与 HDFS 进行通信；因此，如果你没有安装这个包，这个功能将无法工作。你可以简单地使用 pip
    或 conda 安装这个包（hdfs3 位于 conda-forge 频道）。
- en: Listing 4.18 Reading data from S3
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.18 从 S3 读取数据
- en: '[PRE23]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In [listing 4.18](#listing4.18), our `read_csv` call is (again) almost exactly
    the same as [listing 4.17](#listing4.17). This time, however, we’ve prefixed the
    file path with `s3://` to tell Dask that the data is located on an S3 filesystem,
    and `my-bucket` lets Dask know to look for the files in the S3 bucket associated
    with your AWS account named “my-bucket”.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 4.18](#listing4.18) 中，我们的 `read_csv` 调用（再次）几乎与 [列表 4.17](#listing4.17)
    完全相同。然而，这次我们给文件路径加上了 `s3://` 前缀，以告诉 Dask 数据位于 S3 文件系统中，而 `my-bucket` 让 Dask 知道在名为
    “my-bucket” 的 AWS 账户关联的 S3 桶中查找文件。
- en: In order to use the S3 functionality, you must have the s3fs library installed
    on each Dask worker. Like hdfs3, this library can be installed simply through
    pip or conda (from the conda-forge channel). The final requirement is that each
    Dask worker is properly configured for authenticating with S3\. s3fs uses the
    boto library to communicate with S3\. You can learn more about configuring boto
    at [http://boto.cloudhackers.com/en/latest/getting_started.html](http://boto.cloudhackers.com/en/latest/getting_started.html).
    The most common S3 authentication configuration consists of using the AWS Access
    Key and AWS Secret Access Key. Rather than injecting these keys in your code,
    it’s a better idea to set these values using environment variables or a configuration
    file. Boto will check both the environment variables and the default configuration
    paths automatically, so there’s no need to pass authentication credentials directly
    to Dask. Otherwise, as with using HDFS, the call to `read_csv` allows you to do
    all the same things as if you were operating on a local filesystem. Dask really
    makes it easy to work with distributed filesystems!
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用 S3 功能，您必须在每个 Dask 工作节点上安装 s3fs 库。类似于 hdfs3，这个库可以通过 pip 或 conda（来自 conda-forge
    频道）简单地安装。最终要求是每个 Dask 工作节点都正确配置以与 S3 进行身份验证。s3fs 使用 boto 库与 S3 进行通信。您可以在 [http://boto.cloudhackers.com/en/latest/getting_started.html](http://boto.cloudhackers.com/en/latest/getting_started.html)
    上了解更多有关配置 boto 的信息。最常见的 S3 身份验证配置是使用 AWS 访问密钥和 AWS 秘密访问密钥。与其将这些密钥注入到您的代码中，不如使用环境变量或配置文件设置这些值。Boto
    会自动检查环境变量和默认配置路径，因此无需直接将身份验证凭据传递给 Dask。否则，与使用 HDFS 一样，对 `read_csv` 的调用允许您执行所有与在本地文件系统上操作相同的事情。Dask
    真正简化了与分布式文件系统的交互！
- en: Now that you have some experience working with a few different storage systems,
    we’ll round out the “reading data” part of this chapter by talking about a special
    file format that is very useful for fast computations.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经有一些使用不同存储系统的经验，我们将通过讨论一种对快速计算非常有用的特殊文件格式来结束本章的“读取数据”部分。
- en: 4.4 Reading data in Parquet format
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 以 Parquet 格式读取数据
- en: 'CSV and other delimited text files are great for their simplicity and portability,
    but they aren’t really optimized for the best performance, especially when performing
    complex data operations such as sorts, merges, and aggregations. While a wide
    variety of file formats attempt to increase efficiency in many different ways,
    with mixed results, one of the more recent high-profile file formats is Apache
    Parquet. Parquet is a high-performance columnar storage format jointly developed
    by Twitter and Cloudera that was designed with use on distributed filesystems
    in mind. Its design brings several key advantages to the table over text-based
    formats: more efficient use of IO, better compression, and strict typing. [Figure
    4.10](#figure4.10) shows the difference in how data is stored in Parquet format
    versus a row-oriented storage scheme like CSV.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: CSV 和其他定界文本文件因其简单性和可移植性而很受欢迎，但它们并不是真正针对最佳性能进行优化的，尤其是在执行复杂的操作，如排序、合并和聚合时。虽然各种文件格式试图以多种方式提高效率，但结果参差不齐，其中较新的、知名度较高的文件格式之一是
    Apache Parquet。Parquet 是由 Twitter 和 Cloudera 联合开发的高性能列式存储格式，其设计考虑到了在分布式文件系统上的使用。其设计在文本格式之上带来了几个关键优势：更高效的
    IO 使用、更好的压缩和严格的类型。[图 4.10](#figure4.10) 展示了 Parquet 格式与类似 CSV 的行导向存储方案在数据存储方面的差异。
- en: '![c04_10.eps](Images/c04_10.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![c04_10.eps](Images/c04_10.png)'
- en: '[Figure 4.10](#figureanchor4.10) The structure of Parquet compared with delimited
    text files'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.10](#figureanchor4.10) Parquet 结构与定界文本文件的比较'
- en: 'With row-oriented formats, values are stored on disk and in memory sequentially
    based on the row position of the data. Consider what we’d have to do if we wanted
    to perform an aggregate function over *x*, such as finding the mean. To collect
    all the values of *x*, we’d have to scan over 10 values in order to get the 4
    values we want. This means we spend more time waiting for IO completion just to
    throw away over half of the values read from disk. Compare that with the columnar
    format: in that format, we’d simply grab the sequential chunk of *x* values and
    have all four values we want. This seeking operation is much faster and more efficient.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 使用行格式时，值根据数据的行位置在磁盘和内存中顺序存储。考虑一下如果我们想要对 *x* 执行聚合函数，比如求平均值，我们会做什么。为了收集 *x* 的所有值，我们需要扫描10个值才能得到我们想要的4个值。这意味着我们花费更多的时间等待I/O完成，只是为了丢弃从磁盘读取的超过一半的值。与此相比，列格式：在那个格式中，我们只需简单地获取
    *x* 值的连续块，就能得到我们想要的四个值。这种查找操作要快得多，也更有效率。
- en: Another significant advantage of applying column-oriented chunking of the data
    is that the data can now be partitioned and distributed by column. This leads
    to much faster and more efficient shuffle operations, since only the columns that
    are necessary for an operation can be transmitted over the network instead of
    entire rows.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 应用数据列向分块技术的另一个显著优势是，数据现在可以按列进行分区和分布。这导致shuffle操作更快、更高效，因为只需要传输操作所需的列，而不是整个行。
- en: Finally, efficient compression is also a major advantage of Parquet. With column-oriented
    data, it’s possible to apply different compression schemes to individual columns
    so the data becomes compressed in the most efficient way possible. Python’s Parquet
    library supports many of the popular compression algorithms such as gzip, lzo,
    and snappy.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，高效的压缩也是 Parquet 的一大优势。在列向数据中，可以对单个列应用不同的压缩方案，从而使数据以最有效的方式压缩。Python 的 Parquet
    库支持许多流行的压缩算法，如 gzip、lzo 和 snappy。
- en: To use Parquet with Dask, you need to make sure you have the fastparquet or
    pyarrow library installed, both of which can be installed either via pip or conda
    (conda-forge). I would generally recommend using pyarrow over fastparquet, because
    it has better support for serializing complex nested data structures. You can
    also install the compression libraries you want to use, such as python-snappy
    or python-lzo, which are also available via pip or conda (conda-forge). Now let’s
    take a look at reading the NYC Parking Ticket dataset one more time in Parquet
    format. As a side note, we will be using Parquet format extensively through the
    book, and in the next chapter you will write some of the NYC Parking Ticket dataset
    to Parquet format. Therefore, you will see the `read_parquet` method many more
    times! This discussion is here to simply give you a first look at how to use the
    method. Now, without further ado, here’s how to use the `read_parquet` method.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Dask 中的 Parquet，你需要确保你已经安装了 fastparquet 或 pyarrow 库，这两个库都可以通过 pip 或 conda（conda-forge）安装。我通常会推荐使用
    pyarrow 而不是 fastparquet，因为它对序列化复杂嵌套数据结构的支持更好。你还可以安装你想要的压缩库，例如 python-snappy 或
    python-lzo，这些库也通过 pip 或 conda（conda-forge）提供。现在让我们再次查看 NYC 停车罚单数据集的读取操作，以 Parquet
    格式。作为旁注，我们将在整本书中广泛使用 Parquet 格式，在下一章中，你将把一些 NYC 停车罚单数据集写入 Parquet 格式。因此，你将多次看到
    `read_parquet` 方法！这次讨论只是为了简单地让你看看如何使用该方法。现在，不再拖延，这就是如何使用 `read_parquet` 方法。
- en: Listing 4.19 Reading in Parquet data
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.19 读取 Parquet 数据
- en: '[PRE24]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[Listing 4.19](#listing4.19) is about as simple as it gets! The `read_parquet`
    method is used to create a Dask DataFrame from one or more Parquet files, and
    the only required argument is the path. One thing to notice about this call that
    might look strange: `nyc-parking-tickets-prq` is a directory, not a file. That’s
    because datasets stored as Parquet are typically written to disk pre-partitioned,
    resulting in potentially hundreds or thousands of individual files. Dask provides
    this method for convenience so you don’t have to manually create a long list of
    filenames to pass in. You can specify a single Parquet file in the path if you
    want to, but it’s much more typical to see Parquet datasets referenced as a directory
    of files rather than individual files.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.19](#listing4.19) 简单到极致！`read_parquet` 方法用于从一个或多个 Parquet 文件中创建 Dask
    DataFrame，唯一必需的参数是路径。注意这个调用可能看起来有些奇怪的地方：`nyc-parking-tickets-prq` 是一个目录，而不是一个文件。这是因为以
    Parquet 格式存储的数据集通常在磁盘上预先分区，从而产生可能成百上千个单独的文件。Dask 提供了这个方法以方便您不必手动创建一个长的文件名列表来传递。如果您想指定单个
    Parquet 文件，也可以在路径中指定，但更常见的是将 Parquet 数据集作为文件目录而不是单个文件来引用。'
- en: Listing 4.20 Reading Parquet files from distributed filesystems
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.20 从分布式文件系统中读取 Parquet 文件
- en: '[PRE25]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[Listing 4.20](#listing4.20) shows how to read Parquet from distributed filesystems.
    Just as with delimited text files, the only difference is specifying a distributed
    filesystem protocol, such as `hdfs` or `s3`, and specifying the relevant path
    to the data.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.20](#listing4.20) 展示了如何从分布式文件系统中读取 Parquet 文件。与分隔文本文件类似，唯一的区别是指定一个分布式文件系统协议，例如
    `hdfs` 或 `s3`，并指定数据的相关路径。'
- en: Parquet is stored with a predefined schema, so there are no options to mess
    with datatypes. The only real relevant options that Dask gives you to control
    importing Parquet data are column filters and index selection. These work the
    same way as with the other file formats. By default, they will be inferred from
    the schema stored alongside the data, but you can override that selection by manually
    passing in values to the relevant arguments.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 以预定义的模式存储，因此没有选项可以更改数据类型。Dask 给您提供的唯一真正相关的选项来控制导入 Parquet 数据是列过滤器索引选择。它们的工作方式与其他文件格式相同。默认情况下，它们将从与数据一起存储的模式中推断出来，但您可以通过手动传递相关参数的值来覆盖该选择。
- en: Listing 4.21 Specifying Parquet read options
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.21 指定 Parquet 读取选项
- en: '[PRE26]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In [listing 4.21](#listing4.21), we pick a few columns that we want to read
    from the dataset and put them in a list called `columns`. We then pass in the
    list to the `columns` argument, and we specify Plate ID to be used as the index
    by passing it in to the `index` argument. The result of this will be a Dask DataFrame
    containing only the three columns shown here and sorted/indexed by the Plate ID
    column.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 4.21](#listing4.21) 中，我们选择了一些想要从数据集中读取的列，并将它们放入一个名为 `columns` 的列表中。然后我们将列表传递给
    `columns` 参数，并通过将 `Plate ID` 传递给 `index` 参数来指定用作索引的列。这将产生一个只包含这里显示的三个列，并按 `Plate
    ID` 列排序/索引的 Dask DataFrame。
- en: We’ve now covered a number of ways to get data into Dask from a myriad array
    of systems and formats. As you can see, the DataFrame API offers a great deal
    of flexibility to ingest structured data in fairly simple ways. In the next chapter,
    we’ll cover fundamental data transformations and, naturally, writing data back
    out in a number of different ways.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了几种将数据从各种系统和格式导入 Dask 的方法。正如您所看到的，DataFrame API 提供了多种灵活的方式来以相对简单的方式摄取结构化数据。在下一章中，我们将介绍基本的数据转换，并自然地以多种不同的方式将数据写回。
- en: Summary
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: Inspecting the columns of a DataFrame can be done with the `columns` attribute.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `columns` 属性可以检查 DataFrame 的列。
- en: Dask’s datatype inference shouldn’t be relied on for large datasets. Instead,
    you should define your own schemas based on common NumPy datatypes.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask 的数据类型推断不应依赖于大型数据集。相反，您应该根据常见的 NumPy 数据类型定义自己的模式。
- en: Parquet format offers good performance because it’s a column-oriented format
    and highly compressible. Whenever possible, try to get your dataset in Parquet
    format.*  *# 5
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet 格式因其列式格式和高度可压缩性而提供良好的性能。尽可能地将您的数据集转换为 Parquet 格式。*  *# 5
- en: Cleaning and transforming DataFrames
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 清理和转换 DataFrame
- en: '**This chapter covers**'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章涵盖**'
- en: Selecting and filtering data
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择和过滤数据
- en: Creating and dropping columns
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建和删除列
- en: Finding and fixing columns with missing values
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找和修复缺失值的列
- en: Indexing and sorting DataFrames
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 索引和排序 DataFrame
- en: Combining DataFrames using join and union operations
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用连接和并操作组合 DataFrame
- en: Writing DataFrames to delimited text files and Parquet
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 DataFrame 写入分隔文本文件和 Parquet
- en: In the previous chapter, we created a schema for the NYC Parking Ticket dataset
    and successfully loaded the data into Dask. Now we’re ready to get the data cleaned
    up so we can begin analyzing and visualizing it! As a friendly reminder, [figure
    5.1](#figure5.1) shows what we’ve done so far and where we’re going next within
    our data science workflow.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们为纽约市停车罚单数据集创建了一个模式，并成功地将数据加载到 Dask 中。现在我们准备对数据进行清洗，以便我们可以开始分析和可视化它！作为一个友好的提醒，[图
    5.1](#figure5.1) 展示了我们迄今为止所做的工作以及我们数据科学工作流程中的下一步。
- en: '![c05_01.eps](Images/c05_01.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![c05_01.eps](Images/c05_01.png)'
- en: '[Figure 5.1](#figureanchor5.1) The *Data Science with Python and Dask* workflow'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.1](#figureanchor5.1) 使用 Python 和 Dask 进行数据科学的流程'
- en: Data cleaning is an important part of any data science project because anomalies
    and outliers in the data can negatively influence many statistical analyses. This
    could lead us to make bad conclusions about the data and build machine learning
    models that don’t stand up over time. Therefore, it’s important that we get the
    data cleaned up as much as possible before moving on to exploratory analysis.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗是任何数据科学项目的重要组成部分，因为数据中的异常值和离群值可能会对许多统计分析产生负面影响。这可能导致我们对数据的错误结论，并构建出随时间推移无法站立的机器学习模型。因此，在继续进行探索性分析之前，我们尽可能地对数据进行清洗是很重要的。
- en: As we work on cleaning and prepping the data for analysis, you’ll also learn
    a lot of methods that Dask gives you to manipulate DataFrames. Given the syntactic
    similarity of many methods in the Dask DataFrame API, it should become very evident
    in this chapter that Dask DataFrames are made up of Pandas DataFrames. Some of
    the operations look exactly the same, but we will also see how some operations
    differ due to the distributed nature of Dask and how to cope with these differences.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们致力于清洗和准备数据以进行分析时，你也会学到许多 Dask 提供的用于操作 DataFrame 的方法。鉴于 Dask DataFrame API
    中许多方法的语法相似性，在本章中应该非常明显地看出 Dask DataFrame 由 Pandas DataFrame 组成。一些操作看起来完全相同，但我们也会看到一些操作由于
    Dask 的分布式特性而有所不同，以及如何应对这些差异。
- en: Before we get to work, here’s a recap of the code we’ve run so far to import
    the data into Dask. You’ll want to run this code if you’re working along with
    the chapter.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始工作之前，这里是对我们迄今为止导入数据到 Dask 的代码的回顾。如果你正在与本章一起工作，你需要运行此代码。
- en: Listing 5.1 Importing the NYC Parking Ticket data
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.1 导入纽约市停车罚单数据
- en: '[PRE27]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[Listing 5.1](#listing5.1) should look very familiar. With the first couple
    of lines we’re importing the modules we’ll need for the chapter. Next, we’re loading
    the schema dictionary we created in chapter 4\. Finally, we create a DataFrame
    called `nyc_data_raw` by reading the four CSV files, applying the schema, and
    selecting the columns that we defined in the schema (`usecols=dtypes.keys()`).
    Now we’re ready to go!'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.1](#listing5.1) 应该看起来非常熟悉。在前几行中，我们导入了本章所需的模块。接下来，我们加载了在第 4 章中创建的模式字典。最后，我们通过读取四个
    CSV 文件、应用模式和选择我们在模式中定义的列（`usecols=dtypes.keys()`）来创建一个名为 `nyc_data_raw` 的 DataFrame。现在我们准备出发了！'
- en: 5.1 Working with indexes and axes
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 与索引和轴一起工作
- en: 'In chapter 3 you learned that Dask DataFrames have three structural elements:
    an index and two axes (rows and columns). To refresh your memory, [figure 5.2](#figure5.2)
    shows a visual guide to the structure of a DataFrame.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3 章中，你学习了 Dask DataFrame 有三个结构元素：一个索引和两个轴（行和列）。为了刷新你的记忆，[图 5.2](#figure5.2)
    展示了 DataFrame 结构的视觉指南。
- en: '![c05_02.eps](Images/c05_02.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![c05_02.eps](Images/c05_02.png)'
- en: '[Figure 5.2](#figureanchor5.2) The structure of a DataFrame'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.2](#figureanchor5.2) DataFrame 的结构'
- en: 5.1.1 Selecting columns from a DataFrame
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 从 DataFrame 中选择列
- en: 'So far, we haven’t done much with the NYC Parking Ticket dataset beyond choosing
    appropriate datatypes for each column and reading the data into Dask. Now that
    the data is loaded and ready for us to start exploring, a good place for us to
    ease into our exploration is by learning how to navigate the DataFrame’s index
    and axes. Let’s start with something simple: selecting and filtering columns.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们对纽约市停车罚单数据集的处理并不多，只是为每个列选择了合适的数据类型并将数据读入 Dask。现在数据已加载并准备好供我们开始探索，一个让我们轻松进入探索的好地方是学习如何导航
    DataFrame 的索引和轴。让我们从简单的事情开始：选择和过滤列。
- en: Listing 5.2 Selecting a single column from a DataFrame
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.2 从 DataFrame 中选择单个列
- en: '[PRE28]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You’ve already seen a few times that the `head` method will retrieve the first
    *n* rows of a DataFrame, but in those examples we’ve retrieved the entire DataFrame’s
    first *n* rows. In [listing 5.2](#listing5.2) you can see that we’ve put a pair
    of square brackets ([…]) to the right of `nyc_data_raw` and inside those square
    brackets we specified the name of one of the DataFrame’s columns (Plate ID). The
    column selector accepts either a string or a list of strings and applies a filter
    to the DataFrame that returns only the requested columns. In this particular case,
    since we specified only one column, what we get back is not another DataFrame.
    Instead, we get back a Series object, which is like a DataFrame that doesn’t have
    a column axis. You can see that, like a DataFrame, a Series object has an index,
    which is actually copied over from the DataFrame. Oftentimes when selecting columns,
    however, you’ll want to bring back more than one. [Listing 5.3](#listing5.3) demonstrates
    how to select more than one column from a DataFrame, and [figure 5.3](#figure5.3)
    shows the output of the listing.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经多次看到 `head` 方法会检索 DataFrame 的前 *n* 行，但在那些例子中，我们检索了整个 DataFrame 的前 *n* 行。在
    [列表 5.2](#listing5.2) 中，你可以看到我们在 `nyc_data_raw` 的右侧放置了一对方括号 ([…])，并在那些方括号内指定了
    DataFrame 的一个列名（车牌 ID）。列选择器接受字符串或字符串列表，并应用一个过滤器到 DataFrame 上，只返回请求的列。在这种情况下，因为我们只指定了一个列，所以返回的不是另一个
    DataFrame。相反，我们得到的是一个 Series 对象，它类似于没有列轴的 DataFrame。你可以看到，就像 DataFrame 一样，Series
    对象有一个索引，实际上是从 DataFrame 复制的。然而，在选择列时，你通常会想要返回多个列。[列表 5.3](#listing5.3) 展示了如何从
    DataFrame 中选择多个列，而 [图 5.3](#figure5.3) 显示了列表的输出。
- en: Listing 5.3 Selecting multiple columns from a DataFrame using an inline list
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.3 使用内联列表从 DataFrame 中选择多个列
- en: '[PRE29]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![c05_03.png](Images/c05_03.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![c05_03.png](Images/c05_03.png)'
- en: '[Figure 5.3](#figureanchor5.3) The output of [listing 5.3](#listing5.3)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.3](#figureanchor5.3) 列表 5.3 的输出'
- en: Here we’ve used the `head` method to ask Dask to bring back the first five rows
    of the Plate ID column and the Registration State column. The column selector
    might look a little strange—why did we double up on the square brackets? That’s
    because we’re creating an inline list of strings. To get multiple columns back,
    you need to pass a list of column names (as strings) to the column selector. The
    outer pair of square brackets denotes that we’re using the column selector, and
    the inner pair of square brackets is the inline constructor for the list of column
    names. You can also pass a list of column names that’s stored as a variable. [Listing
    5.4](#listing5.4) should make the differentiation between the column selector
    and the list constructor more apparent—and notice that the output shown in [figure
    5.4](#figure5.4) is exactly the same as [figure 5.3](#figure5.3).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了 `head` 方法请求 Dask 返回“车牌 ID”列和“注册状态”列的前五行。列选择器可能看起来有点奇怪——为什么我们重复使用了方括号？那是因为我们正在创建一个字符串的内联列表。要返回多个列，你需要将一个列名（作为字符串）的列表传递给列选择器。外层的方括号对表示我们正在使用列选择器，而内层的方括号对是列名列表的内联构造器。你也可以传递一个存储为变量的列名列表。[列表
    5.4](#listing5.4) 应该会使列选择器和列表构造器的区别更加明显——注意，[图 5.4](#figure5.4) 中显示的输出与 [图 5.3](#figure5.3)
    完全相同。
- en: Listing 5.4 Selecting multiple columns from a DataFrame using a declared list
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.4 使用声明列表从 DataFrame 中选择多个列
- en: '[PRE30]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![c05_04.png](Images/c05_04.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![c05_04.png](Images/c05_04.png)'
- en: '[Figure 5.4](#figureanchor5.4) The output of [listing 5.4](#listing5.4)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.4](#figureanchor5.4) 列表 5.4 的输出'
- en: 'Since we first create a list of column names and store it to a variable called
    `columns_to_select`, we can pass that previously declared list of column names
    to the column selector. An important thing to note about the column selector:
    every column name you reference must exist in the DataFrame. This runs counter
    to the behavior we saw before with the `dtype` and `usecols` arguments of the
    DataFrame constructor. With those arguments, we can pass a list of column names
    where some column names don’t exist in the data and Dask will simply ignore those
    columns. On the other hand, if we pass a column name to the column selector that
    doesn’t exist in the DataFrame, Dask will return a Key Error.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们首先创建了一个列名列表并将其存储到名为 `columns_to_select` 的变量中，我们可以将之前声明的列名列表传递给列选择器。关于列选择器的一个重要注意事项是：您引用的每个列名都必须存在于
    DataFrame 中。这与我们在 DataFrame 构造函数的 `dtype` 和 `usecols` 参数中看到的先前行为相反。使用这些参数，我们可以传递一个列名列表，其中一些列名在数据中不存在，Dask
    将简单地忽略这些列。另一方面，如果我们传递给列选择器的列名在 DataFrame 中不存在，Dask 将返回一个键错误。
- en: 5.1.2 Dropping columns from a DataFrame
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 从 DataFrame 中删除列
- en: Quite often, instead of selecting a small subset of columns, you might instead
    want to keep all but a few columns. You could do this using the column selector
    method you just learned, but that would be an awful lot of typing, especially
    if your DataFrame has a lot of columns like this one! Fortunately, Dask gives
    you a way to selectively drop columns from your DataFrame, keeping all but the
    columns you specify. [Listing 5.5](#listing5.5) demonstrates how to use the `drop`
    method to get rid of the Violation Code column in the DataFrame, and the output
    of the new DataFrame can be seen in [figure 5.5](#figure5.5).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 很常见的情况是，您可能不想选择一小部分列，而是想保留除了少数列之外的所有列。您可以使用您刚刚学到的列选择器方法来做这件事，但这会涉及大量的输入，尤其是如果您的
    DataFrame 有很多像这样的列！幸运的是，Dask 提供了一种方法，可以为您选择性地从 DataFrame 中删除列，保留您指定的列之外的所有列。[列表
    5.5](#listing5.5) 展示了如何使用 `drop` 方法从 DataFrame 中删除违规代码列，新的 DataFrame 的输出可以在 [图
    5.5](#figure5.5) 中看到。
- en: Listing 5.5 Dropping a single column from a DataFrame
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.5 从 DataFrame 中删除单个列
- en: '[PRE31]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![c05_05.png](Images/c05_05.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![c05_05.png](Images/c05_05.png)'
- en: '[Figure 5.5](#figureanchor5.5) The output of [listing 5.5](#listing5.5)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.5](#figureanchor5.5) 列表 5.5 的输出'
- en: Similar to the column selector, the `drop` method accepts either a single string
    or a list of strings representing column names that you wish to drop. Also, notice
    that we had to specify that the drop operation should be performed on axis 1 (columns)
    since we want to drop a column from the DataFrame.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 与列选择器类似，`drop` 方法接受单个字符串或字符串列表，表示您希望删除的列名。注意，我们必须指定删除操作应在轴 1（列）上执行，因为我们想从 DataFrame
    中删除一列。
- en: 'Since Dask operations default to axis 0 (rows), the expected behavior of the
    drop operation, had we not specified `axis=1`, would be to try to locate and drop
    a row with an index of “Violation Code.” This is exactly how Pandas behaves. But,
    this behavior hasn’t been implemented in Dask. Instead, if you forget to specify
    `axis=1`, you receive an error message that states `NotImplementedError: Drop
    currently only works for axis=1`.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '由于 Dask 操作默认为轴 0（行），如果我们没有指定 `axis=1`，则删除操作的预期行为将是尝试找到并删除索引为“违规代码”的行。这正是 Pandas
    的行为。但是，这种行为尚未在 Dask 中实现。相反，如果您忘记指定 `axis=1`，您将收到一个错误消息，表明 `NotImplementedError:
    Drop currently only works for axis=1`。'
- en: Similar to specifying multiple columns in the column selector, you can specify
    multiple columns to drop as well. The effect of this operation on the DataFrame
    can be seen in [figure 5.6](#figure5.6).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 与在列选择器中指定多个列类似，您也可以指定要删除的多个列。此操作对 DataFrame 的影响可以在 [图 5.6](#figure5.6) 中看到。
- en: Listing 5.6 Dropping multiple columns from a DataFrame
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.6 从 DataFrame 中删除多个列
- en: '[PRE32]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![c05_06.png](Images/c05_06.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![c05_06.png](Images/c05_06.png)'
- en: '[Figure 5.6](#figureanchor5.6) The output of [listing 5.6](#listing5.6)'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.6](#figureanchor5.6) 列表 5.6 的输出'
- en: In [listing 5.6](#listing5.6), we’re getting a bit fancier with our column list
    generation. We’ve decided to drop any columns that contain the word “Violation”
    in the column name. On the first line, we’ve defined an anonymous function to
    check each column name for the presence of “Violation” and applied that to the
    `nyc_data_raw.columns` list (which contains a list of all of the `nyc_data_raw`
    DataFrame’s column names) using the `filter` function. We then take this list
    of column names that matched our filter criteria and passed it into the `drop`
    method of the `nyc_data_raw` DataFrame. All told, that operation will drop 13
    columns from the resulting DataFrame.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 5.6](#listing5.6) 中，我们在列列表生成方面变得有些复杂。我们决定删除任何列名中包含“Violation”一词的列。在第一行，我们定义了一个匿名函数来检查每个列名中是否存在“Violation”，并将其应用于
    `nyc_data_raw.columns` 列表（该列表包含 `nyc_data_raw` DataFrame 的所有列名）使用 `filter` 函数。然后，我们将匹配我们的筛选条件的列名列表传递给
    `nyc_data_raw` DataFrame 的 `drop` 方法。总的来说，该操作将从结果 DataFrame 中删除 13 列。
- en: Now that you’ve seen two ways to select subsets of columns from Dask DataFrames,
    you may wonder, when should I use `drop` and when should I use the column selector?
    They are equivalent from a performance standpoint, so it really comes down to
    the question of whether you intend to drop more columns than you want to keep
    or keep more columns than you want to drop. If you intend to drop more columns
    than you want to keep (for example, you want to keep 2 columns and drop 42 columns),
    it will be more convenient to use the column selector. Conversely, if you intend
    to keep more columns than you want to drop (for example, you want to keep 42 columns
    and drop 2 columns), it will be more convenient to use the `drop` method.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了两种从 Dask DataFrame 中选择子集列的方法，你可能想知道，我应该何时使用 `drop` 和何时使用列选择器？从性能角度来看，它们是等效的，所以这实际上归结为你是否打算删除比想要保留的列更多的列。如果你打算删除比想要保留的列更多的列（例如，你想要保留
    2 列并删除 42 列），使用列选择器将更方便。相反，如果你打算保留比想要删除的列更多的列（例如，你想要保留 42 列并删除 2 列），使用 `drop`
    方法将更方便。
- en: 5.1.3 Renaming columns in a DataFrame
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.3 在 DataFrame 中重命名列
- en: The last thing to cover on navigating columns, for now, is renaming columns.
    Sometimes you might be working with data that doesn’t have very descriptive/friendly
    column names in the header and you want to clean them up. While we fortunately
    do have pretty good column names in the NYC Parking Ticket dataset, we’ll look
    at an example of how to rename columns in the event you need to do so in the future.
    In the following listing, we use the `rename` method to change the name of the
    Plate ID column to License Plate, and the result of this can be seen in [figure
    5.7](#figure5.7).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，关于列导航的最后一件事是重命名列。有时你可能正在处理数据，其标题中的列名描述性/友好性不强，而你想要对其进行清理。幸运的是，在纽约市停车罚单数据集中，我们确实有相当不错的列名，但我们将查看一个示例，说明如何在将来需要时重命名列。在下面的列表中，我们使用
    `rename` 方法将车牌 ID 列的名称更改为车牌，并且可以看到这一结果在 [图 5.7](#figureanchor5.7) 中。
- en: Listing 5.7 Renaming a column
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.7 重命名列
- en: '[PRE33]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '![c05_07.eps](Images/c05_07.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![c05_07.eps](Images/c05_07.png)'
- en: '[Figure 5.7](#figureanchor5.7) The output of [listing 5.7](#listing5.7)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.7](#figureanchor5.7) [列表 5.7](#listing5.7) 的输出'
- en: The `columns` argument simply takes a dictionary where the key is the old column
    name and the value is the new column name. Dask will make a one-for-one swap,
    returning a DataFrame with the new column names. Columns that aren’t specified
    in the dictionary will not be renamed or dropped. Note that these operations are
    not altering the source data on disk, only the data that Dask holds in memory.
    Later in the chapter, we’ll cover how to write the modified data back to disk.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '`columns` 参数简单地接受一个字典，其中键是旧列名，值是新列名。Dask 将进行一对一的交换，返回具有新列名的 DataFrame。未在字典中指定的列将不会被重命名或删除。请注意，这些操作不会更改磁盘上的源数据，只会更改
    Dask 在内存中持有的数据。在本章的后面部分，我们将介绍如何将修改后的数据写回磁盘。'
- en: 5.1.4 Selecting rows from a DataFrame
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.4 从 DataFrame 中选择行
- en: Next, we’ll have a look at how to select data across the rows axis. Later in
    the chapter we’ll talk about searching and filtering rows, which is a more common
    way to navigate the rows axis. But you are likely to run across situations where
    you know the range of rows you want to retrieve ahead of time, in which case selecting
    by index is the appropriate way to get the data. This occurs most frequently when
    your data is indexed by a date or time. Keep in mind that indexes do not need
    to be unique, so you can use an index to select a chunk of data. For example,
    you may want to get all rows that occurred between April 2015 and November 2015\.
    Just like with a clustered index in a relational database, selecting data by index
    offers a performance boost over search and filter methods. This is largely due
    to the fact that Dask stores and partitions data sorted in order of the index.
    Therefore, when seeking specific information, Dask doesn’t have to scan through
    the entire dataset to make sure it brings back all the data you ask for. In the
    following listing, we use the `loc` method to specify the index of the row we
    want to retrieve, and the output is displayed in [figure 5.8](#figure5.8).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看如何跨行轴选择数据。在本章的后面部分，我们将讨论搜索和过滤行，这是在行轴上导航的更常见方式。但你也可能遇到你知道要检索的行范围的情况，在这种情况下，通过索引选择数据是获取数据的一种适当方式。这通常发生在你的数据按日期或时间索引时。请注意，索引不需要是唯一的，因此你可以使用索引来选择数据块。例如，你可能想获取
    2015 年 4 月到 2015 年 11 月之间发生的所有行。就像在关系型数据库中的聚集索引一样，通过索引选择数据比搜索和过滤方法提供了性能提升。这主要是因为
    Dask 按索引顺序存储和划分数据。因此，当寻求特定信息时，Dask 不必扫描整个数据集以确保返回你请求的所有数据。在下面的列表中，我们使用 `loc` 方法指定要检索的行索引，输出显示在
    [图 5.8](#figure5.8) 中。
- en: Listing 5.8 Getting a single row by index
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.8 通过索引获取单行
- en: '[PRE34]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Since the DataFrame hasn’t been indexed by a specific column, the index we’re
    selecting is the default sequential numeric index of the DataFrame (which starts
    at 0). This means we’ve retrieved the 56th row in the DataFrame if we counted
    up from the first row. `loc` is syntactically similar to the column selector in
    that it uses square brackets to accept arguments. Unlike the column selector,
    however, it will not accept a list of values. You can either pass it a single
    value or a range of values using Python’s standard slice notation.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 DataFrame 还未按特定列进行索引，因此我们选择的索引是 DataFrame 的默认顺序数字索引（从 0 开始）。这意味着如果我们从第一行开始计数，我们已经检索了
    DataFrame 中的第 56 行。`loc` 在语法上与列选择器相似，因为它使用方括号来接受参数。然而，与列选择器不同的是，它不接受值列表。你可以传递单个值或使用
    Python 的标准切片符号传递值范围。
- en: '![c05_08.png](Images/c05_08.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![c05_08.png](Images/c05_08.png)'
- en: '[Figure 5.8](#figureanchor5.8) The output of [listing 5.8](#listing5.8)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.8](#figureanchor5.8) [列表 5.8](#listing5.8) 的输出'
- en: Listing 5.9 Getting a sequential slice of rows by index
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.9 通过索引获取行的连续切片
- en: '[PRE35]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[Listing 5.9](#listing5.9) demonstrates using a slice to return rows between
    100 and 200\. This is the same notation as slicing a list or an array in plain-old
    Python. One effect of accessing rows using this slicing notation is that the rows
    that are returned will be sequential. As we’ve already seen, the `drop` function
    does not work across the rows axis, so there’s no way to select just rows 1, 3,
    and 5, for example, without using filtering. You can, however, retrieve the slice
    you want to reduce further from Dask and use Pandas to do the final filtering.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.9](#listing5.9) 展示了使用切片返回 100 到 200 之间的行。这与在纯 Python 中切片列表或数组时的表示法相同。使用这种切片符号访问行的一个影响是返回的行将是连续的。正如我们已经看到的，`drop`
    函数在行轴上不起作用，因此没有方法可以仅选择例如第 1、3 和 5 行，而不使用过滤。然而，你可以从 Dask 中检索你想要的切片，并使用 Pandas 进行最终的过滤。'
- en: Listing 5.10 Filtering a slice of rows using Dask and Pandas
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.10 使用 Dask 和 Pandas 过滤行的切片
- en: '[PRE36]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In [listing 5.10](#listing5.10), we’re taking our Dask DataFrame (`nyc_data_raw`)
    and getting a slice of rows between index 100 and 200\. Using the `head` method
    triggers the computation in Dask and returns the result as a Pandas DataFrame.
    You could also use the `collect` method, since you’re selecting a small range
    of data, but using the `head` method is a good habit to get into in order to avoid
    accidentally retrieving too much data. We store this result to a variable called
    `some_rows`. We’re then using the `drop` method on `some_rows` (which is a *Pandas*
    DataFrame) to drop every other row and display the result. The `drop` method is
    implemented for the rows axis in Pandas, so if you need to drop rows from a DataFrame,
    bringing a subset of your Dask data down to Pandas is a good idea. However, be
    aware that if the slice you’re trying to pull down is too big to fit into your
    computer’s memory, the operation will fail with an out-of-memory error. Therefore,
    this method is only suitable if you’re working on a fairly small slice of the
    Dask DataFrame. Otherwise, you’ll need to rely on more advanced filtering methods
    that we’ll cover a bit later in the chapter.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 5.10](#listing5.10)中，我们正在使用我们的 Dask DataFrame (`nyc_data_raw`) 获取索引 100
    到 200 之间的行切片。使用 `head` 方法触发了 Dask 的计算，并将结果作为 Pandas DataFrame 返回。由于您正在选择一小部分数据，您也可以使用
    `collect` 方法，但使用 `head` 方法是一个好习惯，以避免意外检索到太多数据。我们将此结果存储到一个名为 `some_rows` 的变量中。然后，我们使用
    `some_rows`（它是一个 *Pandas* DataFrame）上的 `drop` 方法来删除每一行并显示结果。`drop` 方法在 Pandas
    中实现了对行轴的操作，因此如果您需要从 DataFrame 中删除行，将您的 Dask 数据子集降低到 Pandas 是一个好主意。然而，请注意，如果您试图拉下的切片太大，无法适应您的计算机内存，操作将因内存不足错误而失败。因此，此方法仅适用于您正在处理相当小的
    Dask DataFrame 的情况。否则，您将需要依赖我们将在本章稍后部分介绍的更高级的过滤方法。
- en: 'Now that you’re getting more comfortable with navigating the data, we’ll jump
    into an important step in the data-cleaning process: finding and fixing missing
    values in our dataset.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您对数据的导航越来越得心应手，我们将进入数据清理过程中的一个重要步骤：在我们的数据集中查找和修复缺失值。
- en: 5.2 Dealing with missing values
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 处理缺失值
- en: 'Oftentimes you’ll come across datasets that have missing values due to deficiencies
    in the data collection process, evolving needs over time, or data processing and
    storage issues. Whatever the cause, you’ll need to decide what to do to eliminate
    these data-quality issues. When fixing missing values, you have three options
    to choose from:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 经常会遇到由于数据收集过程中的缺陷、随着时间的推移而变化的需求或数据处理和存储问题而导致缺失值的数据库集。无论原因如何，您都需要决定如何消除这些数据质量问题。在修复缺失值时，您有三个选项可供选择：
- en: Remove the rows/columns with missing data from your dataset.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从您的数据集中删除具有缺失数据的行/列。
- en: Assign the missing values a default value.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将缺失值分配一个默认值。
- en: Impute the missing values.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 补充缺失值。
- en: For example, imagine that you have a dataset containing height measurements
    of various people, and a few of those height measurements are missing. Depending
    on the objectives of your analysis, you could decide to either throw out the records
    with missing height measurements or assume that those people are about average
    height by finding the arithmetic mean of the measurements you have. Unfortunately,
    there isn’t a “silver bullet” approach to selecting the best method to deal with
    missing values. It largely depends on the context and domain of the missing data.
    A good rule of thumb is to work with the stakeholders who will be interpreting
    and using your analyses to come up with an agreed-upon approach that makes the
    most sense in the context of the problem you’re trying to solve. However, to give
    you some options, we’ll cover how to do all three in this section.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您有一个包含各种人身高测量的数据集，其中一些身高测量值缺失。根据您分析的目标，您可以选择丢弃缺失身高测量的记录，或者通过找到您拥有的测量值的算术平均值来假设这些人的身高约为平均水平。不幸的是，没有一种“万能药”的方法来选择处理缺失值最佳方法。这很大程度上取决于缺失数据的上下文和领域。一个好的经验法则是与将解释和使用您分析的利益相关者合作，共同制定一个在您试图解决的问题的上下文中最有意义的共识方法。然而，为了给您一些选择，我们将在本节中介绍如何执行所有三种方法。
- en: 5.2.1 Counting missing values in a DataFrame
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 在 DataFrame 中计算缺失值数量
- en: We’ll start by having a look at what columns in the NYC Parking Ticket data
    have missing values.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先来看看纽约停车罚单数据中哪些列有缺失值。
- en: Listing 5.11 Calculating the percentage of missing values by column
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.11 按列计算缺失值的百分比
- en: '[PRE37]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[Listing 5.11](#listing5.11) should look a bit familiar—we did the same thing
    on just the 2017 data in chapter 2\. To recap what’s going on here, the first
    line creates a new Series that contains a count of missing values by column. The
    `isnull` method scans each row and returns `True` if a missing value is found
    and `False` if a missing value isn’t found. The `sum` method counts up all the
    `True` values to give us a total count of missing rows per column. We then take
    that Series of counts and divide it by the number of rows in the DataFrame using
    `nyc_data_raw.index.size`, and multiply each value by 100\. Calling the compute
    method triggers the calculation and stores the result as a Pandas Series named
    `percent_missing`.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.11](#listing5.11)看起来应该有点熟悉——我们在第二章的2017年数据上做了同样的事情。为了回顾这里发生的事情，第一行创建了一个新的Series，其中包含每列缺失值的计数。`isnull`方法扫描每一行，如果找到缺失值则返回`True`，如果没有找到则返回`False`。`sum`方法计算所有`True`值的总数，从而给出每列缺失行的总数。然后，我们用`nyc_data_raw.index.size`除以DataFrame中的行数，并将每个值乘以100。调用`compute`方法触发计算，并将结果存储为名为`percent_missing`的Pandas
    Series。'
- en: 5.2.2 Dropping columns with missing values
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 删除包含缺失值的列
- en: Now that we know what we have to work with, we’ll start by dropping any columns
    that have more than 50% of their values missing.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了我们要处理的内容，我们将开始删除任何超过50%值缺失的列。
- en: Listing 5.12 Dropping columns that have more than 50% missing values
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.12 删除超过50%缺失值的列
- en: '[PRE38]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'In [listing 5.12](#listing5.12), we start by filtering the `percent_missing`
    Series to find the names of the columns that have 50% or more missing values.
    That produces a list that looks like this:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 5.12](#listing5.12)中，我们首先过滤`percent_missing`序列，以找到具有50%或更多缺失值的列的名称。这产生了一个看起来像这样的列表：
- en: '[PRE39]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We then use the `drop` method you learned in the last section to drop the columns
    from the DataFrame and save the result as the DataFrame called `nyc_data_clean_stage1`.
    We picked 50% arbitrarily here, but it’s quite typical to drop columns with a
    very high amount of missing data. Take the Double Parking Violation column, for
    example: it’s missing 99.9% of its values. We don’t likely stand to gain much
    information by keeping such a sparse column around, so we’ll remove it from our
    dataset.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用上一节中学到的`drop`方法从DataFrame中删除列，并将结果保存为名为`nyc_data_clean_stage1`的DataFrame。在这里，我们任意选择了50%，但删除具有非常大量缺失数据的列是非常典型的。以“双停车违规”列为例：它的值有99.9%是缺失的。保留这样的稀疏列不太可能给我们带来很多信息，因此我们将它从数据集中删除。
- en: 5.2.3 Imputing missing values
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 插补缺失值
- en: When you have columns that only have a small amount of missing data, it’s more
    appropriate to discard the rows that have missing data. Before we do that, though,
    we’ll *impute* a value for the Vehicle Color column. *Imputing* means we will
    use the data that we do have to make a reasonable guess at what the missing data
    might be. In this instance, we’ll find the most frequently occurring color in
    the dataset. While this assumption might not always hold true, using the most
    frequently occurring value in a dataset maximizes the probability that you chose
    correctly.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的列中只有少量缺失数据时，删除包含缺失数据的行更为合适。不过，在我们这样做之前，我们将对“车辆颜色”列进行*插补*。*插补*意味着我们将使用我们已有的数据来合理猜测缺失数据可能是什么。在这种情况下，我们将找到数据集中出现频率最高的颜色。虽然这个假设可能并不总是成立，但使用数据集中出现频率最高的值可以最大化你选择正确的概率。
- en: Listing 5.13 Imputing missing values
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.13 插补缺失值
- en: '[PRE40]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[Listing 5.13](#listing5.13) aims to fill in missing values in the Vehicle
    Color column by assuming that they were the most common color in the dataset.
    Filling in missing values by using the most common element of a categorical variable
    or the arithmetic mean of a continuous variable is a common way to deal with missing
    values in a way that minimizes the effect on the statistical distribution of the
    data. On the first line of [listing 5.13](#listing5.13), we select the Vehicle
    Color column using the column selector and use the `value_counts` method, which
    counts the unique occurrences of data. Here’s what the contents of `count_of_vehicle_colors`
    look like:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.13](#listing5.13)旨在通过假设它们是数据集中最常见的颜色来填充“车辆颜色”列中的缺失值。使用分类变量的最常见元素或连续变量的算术平均值来填充缺失值是一种常见的处理缺失值的方法，可以最小化对数据统计分布的影响。在[列表
    5.13](#listing5.13)的第一行，我们使用列选择器选择“车辆颜色”列，并使用`value_counts`方法，该方法计算数据的唯一出现次数。以下是`count_of_vehicle_colors`的内容：'
- en: '[PRE41]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As you can see, the results of `value_counts` give us a Series containing each
    color on the index and how many times the color appeared in the data as values.
    In the second line of [listing 5.13](#listing5.13), we sort all the vehicle colors
    from most occurrences to fewest and grab the name of the most common occurring
    color. As you can see, GY (gray) is the most commonly occurring color code with
    over 6.2 million occurrences. On the last line of [listing 5.13](#listing5.13),
    we use the `fillna` method to replace the missing colors with GY. `fillna` takes
    a dictionary of key-value pairs where the name of each column you want to fill
    is used as a key and what you want to fill the occurrences of missing values with
    is used as a value. Columns that you don’t specify in the dictionary will not
    be modified.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`value_counts` 的结果给我们一个包含每个颜色在索引中以及该颜色在数据中出现的次数的 Series。在 [列表 5.13](#listing5.13)
    的第二行中，我们将所有车辆颜色按出现频率从高到低排序，并获取出现频率最高的颜色的名称。如您所见，GY（灰色）是最常见的颜色代码，出现次数超过 620 万次。在
    [列表 5.13](#listing5.13) 的最后一行中，我们使用 `fillna` 方法用 GY 替换缺失的颜色。`fillna` 接受一个键值对字典，其中每个你想要填充的列的名称用作键，而你想要填充缺失值出现次数的内容用作值。在字典中未指定的列将不会被修改。
- en: 5.2.4 Dropping rows with missing data
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.4 删除具有缺失数据的行
- en: Now that we’ve filled in missing values in the Vehicle Color column, we’ll take
    care of the other low missing value columns by dropping the rows that are missing
    values in those columns.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经填充了车辆颜色列中的缺失值，我们将通过删除那些列中缺失值的行来处理其他低缺失值列。
- en: Listing 5.14 Dropping rows with missing data
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.14 删除具有缺失数据的行
- en: '[PRE42]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[Listing 5.14](#listing5.14) starts by finding all the columns that have missing
    values but no more than 5% missing values. We put this result in a list called
    `rows_to_drop`. The contents of the list look like this:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.14](#listing5.14) 从查找所有具有缺失值但缺失值不超过 5% 的列开始。我们将这个结果放入一个名为 `rows_to_drop`
    的列表中。列表的内容如下：'
- en: '[PRE43]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note that we’re not going to drop these columns! We’re just going to drop any
    rows from our DataFrame that have missing values in these columns. Also notice
    that Vehicle Color shows up. However, because we’re going to apply our dropping
    function to `nyc_data_clean_stage2`, no rows will be dropped on account of missing
    vehicle colors because they’ve already been filled in. To do the actual dropping,
    we use the `dropna` method on the DataFrame. If we don’t specify any arguments,
    `dropna` will drop all rows with any missing values, so use with caution! The
    `subset` argument allows us to specify the columns Dask will check for missing
    values. If a row has missing values in columns that aren’t specified, Dask won’t
    drop them.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不会删除这些列！我们只是会删除 DataFrame 中具有这些列缺失值的任何行。另外请注意，车辆颜色出现了。然而，因为我们将要应用我们的删除函数到
    `nyc_data_clean_stage2`，所以不会有任何行因为缺失车辆颜色而被删除，因为它们已经被填充了。为了执行实际的删除，我们在 DataFrame
    上使用 `dropna` 方法。如果我们不指定任何参数，`dropna` 将删除所有具有任何缺失值的行，所以请谨慎使用！`subset` 参数允许我们指定
    Dask 将检查缺失值的列。如果一行在未指定的列中有缺失值，Dask 不会删除它们。
- en: 5.2.5 Imputing multiple columns with missing values
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.5 用默认值填充多个具有缺失值的列
- en: We’re almost done now. The last thing we’ll do is fill in our remaining columns
    that have missing data with default values. One thing we’ll need to make sure
    of is that the default value we set for a column is appropriate for that column’s
    datatype. Let’s check what columns we have left to clean up and what their datatypes
    are.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们几乎完成了。我们最后要做的就是用默认值填充我们剩余的具有缺失数据的列。我们需要确保的是，我们为列设置的默认值适合该列的数据类型。让我们检查我们还有哪些列需要清理以及它们的类型。
- en: Listing 5.15 Finding the datatypes of the remaining columns
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.15 查找剩余列的数据类型
- en: '[PRE44]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The first thing we’ll do in [listing 5.15](#listing5.15), like some of the
    listings before it, is find the columns that we still need to clean up. For any
    columns that have more than 5% missing values and less than 50% missing values,
    we’ll fill the missing values with a default value. That list of columns is stored
    in the `remaining_columns_to_clean` variable, and we use that with the `dtypes`
    parameter of the `nyc_data_raw` DataFrame to find the datatype of each column.
    Here’s what the output looks like:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [列表 5.15](#listing5.15) 中要做的第一件事，就像之前的某些列表一样，是找到我们仍然需要清理的列。对于任何具有超过 5% 缺失值且少于
    50% 缺失值的列，我们将用默认值填充缺失值。这些列的列表存储在 `remaining_columns_to_clean` 变量中，我们使用这个变量与 `nyc_data_raw`
    DataFrame 的 `dtypes` 参数一起找到每列的数据类型。以下是输出结果：
- en: '[PRE45]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: As you can see, all the columns we have left to clean up are strings. You may
    be wondering why they are displayed as `object` instead of `np.str`. This is just
    cosmetic—Dask only explicitly shows numeric (`int`, `float`, and so on) datatypes.
    Any non-numeric datatypes will show as `object`. We’ll fill in each column with
    the string “Unknown” to signify that the value was missing. We’ll use `fillna`
    again to fill in the values, so we need to prepare a dictionary with the values
    for each column.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们剩下要清理的所有列都是字符串类型。你可能想知道为什么它们显示为 `object` 而不是 `np.str`。这只是外观上的问题——Dask
    只会明确显示数值数据类型（`int`、`float` 等）。任何非数值数据类型都会显示为 `object`。我们将用字符串 “Unknown” 填充每个列，以表示该值缺失。我们将再次使用
    `fillna` 来填充值，因此我们需要为每个列准备一个包含值的字典。
- en: Listing 5.16 Making a dictionary of values for `fillna`
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.16 构建 `fillna` 的值字典
- en: '[PRE46]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[Listing 5.16](#listing5.16) show how to build this dictionary. We’re simply
    taking each value in the `remaining_columns_to_clean` list and spitting out a
    tuple of the column name and the string “Unknown.” Finally, we convert the list
    of tuples to a dictionary, which yields an object that looks like this:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.16](#listing5.16) 展示了如何构建这个字典。我们只是简单地从 `remaining_columns_to_clean` 列表中取出每个值，并输出一个包含列名和字符串
    “Unknown.” 的元组。最后，我们将元组列表转换为字典，得到一个看起来像这样的对象：'
- en: '[PRE47]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Now that we have a dictionary representing each column we want to fill in and
    the value to fill with, we can pass it to `fillna`.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了代表每个要填充的列及其填充值的字典，我们可以将其传递给 `fillna`。
- en: Listing 5.17 Filling the DataFrame with default values
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.17 使用默认值填充 DataFrame
- en: '[PRE48]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Nice and simple. We’ve now built up our final DataFrame, `nyc_data_clean_stage4`,
    which has been sequentially built up by starting from `nyc_data_raw` and lazily
    applying each of the four missing value techniques on various columns. Now it’s
    time to check our work.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 简单明了。我们现在已经构建了最终的 DataFrame，`nyc_data_clean_stage4`，它是从 `nyc_data_raw` 开始，逐个应用四个缺失值处理技术到各个列中构建起来的。现在是我们检查工作的时候了。
- en: Listing 5.18 Checking the success of the filling/dropping operations
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.18 检查填充/删除操作的成功情况
- en: '[PRE49]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: In [listing 5.18](#listing5.18), we kick off the computation and get back a
    count of how many missing values remain after applying all our transformations.
    Looks like we got everything! If you’re running the code as you’re reading, you
    might have noticed that the computation took some time to complete. Now would
    be an opportune time to persist the DataFrame. Remember that persisting the DataFrame
    will precompute the work you’ve done up to this point and store it in a processed
    state in memory. This will make sure that we don’t have to recompute all those
    transformations as we continue our analysis. The last line of [listing 5.18](#listing5.18)
    reviews how to do that. Now that we’ve taken care of all the missing values, we’ll
    look at a few methods for cleaning up erroneous-looking values.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 5.18](#listing5.18) 中，我们启动了计算过程，并得到了在应用所有转换后剩余缺失值的数量。看起来我们一切都搞定了！如果你在阅读代码的同时运行它，你可能已经注意到计算花费了一些时间。现在是一个保存
    DataFrame 的好时机。记住，保存 DataFrame 将会预先计算你到目前为止所做的工作，并将其存储在内存中的处理状态中。这将确保我们在继续分析时不需要重新计算所有这些转换。列表
    5.18 的最后一行回顾了如何做到这一点。现在我们已经处理完所有缺失值，我们将探讨一些清理看起来有误的值的方法。
- en: 5.3 Recoding data
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 数据重编码
- en: Like missing values, it’s not unusual to also have some instances in your dataset
    where the data is not missing but its validity is suspect. For instance, if we
    came across a vehicle in the NYC Parking Ticket dataset whose color was purportedly
    Rocky Road, that might raise a few eyebrows. It’s more likely that the parking
    enforcement officer had the local scoop shop’s flavor of the day in mind when
    writing the ticket rather than the job at hand! We need to have a way to clean
    up those kinds of data anomalies, and one way to do that is by either recoding
    those values to a more likely choice (such as the most frequent value or arithmetic
    mean) or placing the anomalous data in an Other category. Just like methods for
    filling missing data, it’s worth it to discuss this with the consumers of your
    analyses and agree upon a plan for identifying and dealing with anomalous data.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 与缺失值一样，在您的数据集中也可能存在一些实例，数据不是缺失的，但其有效性值得怀疑。例如，如果我们遇到了纽约市停车罚单数据集中一辆声称是Rocky Road颜色的车辆，这可能会引起一些怀疑。更有可能的是，停车执法官员在写罚单时心里想的是当地冰淇淋店的当日口味，而不是手头的工作！我们需要有一种方法来清理这些类型的数据异常，一种方法是将这些值重新编码为更可能的选择（例如最频繁的值或算术平均值），或者将异常数据放入其他类别。就像填充缺失数据的方法一样，与您分析的使用者讨论这一点并就识别和处理异常数据达成一致的计划是值得的。
- en: Dask gives you two methods to recode values.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 提供了两种方法来重新编码值。
- en: Listing 5.19 Getting value counts of the Plate Type column
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.19 获取“车牌类型”列的值计数
- en: '[PRE50]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[Listing 5.19](#listing5.19) uses the `value_counts` method again that you
    learned about in the last section. Here we’re using it to take a distinct count
    of all the license plate types that have been recorded over the past four years.
    The Plate Type column records whether the vehicle in question was a passenger
    vehicle, commercial vehicle, and so forth. Here’s an abbreviated output of the
    computation:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.19](#listing5.19) 再次使用了您在上一个章节中学到的`value_counts`方法。这里我们使用它来获取过去四年中记录的所有车牌类型的唯一计数。车牌类型列记录了所讨论的车辆是否为乘用车、商用车辆等。以下是计算结果的简略输出：'
- en: '[PRE51]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'As you can see, the vast majority of license plate types are PAS (passenger
    vehicles). Combined with COM (commercial vehicles), these two plate types make
    up over 92% of the entire DataFrame (~38M out of ~41M rows). However, we can also
    see that there are 90 distinct license plate types (Length: 90)! Let’s collapse
    the Plate Type column so we only have three types: PAS, COM, and Other.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，绝大多数车牌类型是 PAS（乘用车）。与 COM（商用车辆）相结合，这两种车牌类型占整个 DataFrame 的超过 92%（大约 38M 行中的
    41M 行）。然而，我们也可以看到有 90 种不同的车牌类型（长度：90）！让我们折叠“车牌类型”列，这样我们只有三种类型：PAS、COM 和其他。
- en: Listing 5.20 Recoding the Plate Type column
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.20 重新编码“车牌类型”列
- en: '[PRE52]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: We have a lot of things going on in [listing 5.20](#listing5.20). First, we
    need to build a Boolean condition that we’ll use to compare with each row. To
    build the condition, we use the `isin` method. This method will return `True`
    if the value it’s checking is contained in the list of objects passed in as the
    argument. Otherwise, it will return `False`. When applied over the whole Plate
    Type column, it will return a Series of `True` and `False` values. In the next
    line, we pass the Series of `True`/`False` values to the `where` method and apply
    it to the Plate Type column. The `where` method keeps the existing value for all
    rows that are `True` and replaces any rows that are `False` with the value passed
    in the second argument. This means that any row that doesn’t have a Plate Type
    of PAS or COM will have its Plate Type replaced with Other. This results in a
    new Series that we store in the `plate_type_masked` variable.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 5.20](#listing5.20)中，我们有很多事情要做。首先，我们需要构建一个布尔条件，我们将用它来与每一行进行比较。为了构建条件，我们使用`isin`方法。如果它检查的值包含在作为参数传递的对象列表中，则此方法将返回`True`。否则，它将返回`False`。当应用于整个“车牌类型”列时，它将返回一个包含`True`和`False`值的
    Series。在下一行中，我们将`True`/`False`值的 Series 传递给`where`方法，并将其应用于“车牌类型”列。`where`方法保留所有`True`行的现有值，并用第二个参数中传递的值替换任何`False`行。这意味着任何没有
    PAS 或 COM 车牌类型的行，其车牌类型将被替换为其他。这导致了一个新的 Series，我们将其存储在`plate_type_masked`变量中。
- en: Now that we have the new Series, we need to put it back in the DataFrame. To
    do that, we’ll first drop the old Plate Type column using the `drop` method that
    you’ve seen a few times now. Then we use the `assign` method to add the Series
    to the DataFrame as a new column. Because the `assign` method uses `**kwargs`
    for passing column names instead of a dictionary, like many of the other column-based
    methods do, we can’t add a column that has spaces in the column name. Therefore,
    we create the column as “PlateType” and use the `rename` method you learned earlier
    in the chapter to give the column the name we want.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了新的 Series，我们需要将其放回 DataFrame 中。为此，我们首先使用你之前见过几次的 `drop` 方法删除旧的 Plate Type
    列。然后我们使用 `assign` 方法将 Series 添加到 DataFrame 中作为新列。因为 `assign` 方法使用 `**kwargs` 而不是字典来传递列名，就像许多其他基于列的方法一样，所以我们不能添加列名中包含空格的列。因此，我们创建名为“PlateType”的列，并使用本章
    earlier 学习的 `rename` 方法将该列重命名为我们想要的名称。
- en: If we take a look at the value counts now, you can see we successfully collapsed
    the column.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们查看值计数，你可以看到我们成功地将列合并了。
- en: Listing 5.21 Looking at value counts after recoding
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.21 重新编码后的值计数查看
- en: '[PRE53]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Here’s what the output looks like:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果的样子：
- en: '[PRE54]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: This looks much better now! We’ve successfully reduced the number of distinct
    license plate classes to three.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看起来好多了！我们已经成功地将独特的车牌类别数量减少到三个。
- en: 'The other method for recoding that we have at our disposal is the `mask` method.
    It works largely the same as the `where` method, but with one key difference:
    the `where` method replaces values when the condition passed to it evaluates `False`,
    and the `mask` method replaces values when the condition passed to it evaluates
    `True`. To give an example of how this is used, let’s now look at the Vehicle
    Color column again, starting with examining the value counts of the column:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可用的其他重新编码方法是 `mask` 方法。它的工作方式基本上与 `where` 方法相同，但有一个关键的区别：`where` 方法在传递给它的条件评估为
    `False` 时替换值，而 `mask` 方法在传递给它的条件评估为 `True` 时替换值。为了举例说明如何使用这种方法，我们现在再次查看车辆颜色列，从检查该列的值计数开始：
- en: '[PRE55]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: This dataset contains more than 5,744 unique colors, but it looks like some
    colors are quite strange. Just over 50% of the colors in this dataset have only
    a single entry like the many you see here. Let’s reduce the number of unique colors
    by putting all the single-color entries in a category called Other.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含超过 5,744 种独特颜色，但看起来有些颜色相当奇怪。在这个数据集中，超过 50% 的颜色只有单个条目，就像你在这里看到的许多条目一样。让我们通过将所有单色条目放入一个名为“其他”的类别中来减少独特颜色的数量。
- en: Listing 5.22 Using `mask` to put unique colors in an “Other” category
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.22 使用 `mask` 将独特颜色放入“其他”类别
- en: '[PRE56]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'In [listing 5.22](#listing5.22), we first get a list of all of the colors that
    appear only once in our dataset by filtering down the value counts of Vehicle
    Colors. Then, as before, we use the `isin` method to build a Series of `True`/`False`
    values. This will result in `True` for any rows that have one of the unique colors
    and `False` for rows that don’t. We pass this condition into the `mask` method
    along with the alternative value of `Other`. This will return a Series where all
    rows that have one of the unique colors will be replaced with `Other`, and the
    rows that don’t will retain their original value. We then simply follow the same
    process we did before to put the new column back in the DataFrame: drop the old
    column, add the new column, and rename it to what we want.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 5.22](#listing5.22) 中，我们首先通过过滤车辆颜色的值计数来获取我们数据集中只出现一次的所有颜色列表。然后，就像之前一样，我们使用
    `isin` 方法构建一个 `True`/`False` 值的 Series。这将导致具有任何独特颜色的行返回 `True`，而没有独特颜色的行返回 `False`。我们将这个条件传递给
    `mask` 方法，并附带 `Other` 作为替代值。这将返回一个 Series，其中具有任何独特颜色的所有行都将被替换为 `Other`，而没有独特颜色的行将保留其原始值。然后我们简单地遵循之前的过程将新列放回
    DataFrame 中：删除旧列，添加新列，并将其重命名为我们想要的名称。
- en: You’re probably wondering when you should use one method over the other. They
    both fundamentally do the same thing and share the same performance characteristics,
    but sometimes it’s more convenient to use one over the other. If you have many
    unique values, but you want to keep only a few around, using the `where` method
    is more convenient. Conversely, if you have many unique values but you want to
    get rid of only a few of them, using the `mask` method is more convenient.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道何时应该使用一种方法而不是另一种。它们在本质上都做同样的事情，并且具有相同的性能特征，但有时使用一种方法比另一种更方便。如果你有很多独特的值，但只想保留其中几个，使用`where`方法会更方便。相反，如果你有很多独特的值，但只想去除其中几个，使用`mask`方法会更方便。
- en: Now that you’ve learned some methods for replacing one value with another static
    value, we’ll look at some more sophisticated methods to create derived columns
    using functions.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经学习了一些用另一个静态值替换一个值的方法，我们将探讨一些更复杂的方法来使用函数创建派生列。
- en: 5.4 Elementwise operations
  id: totrans-385
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 元素级操作
- en: While the methods for recoding values that you learned in the previous section
    are very useful, and you’re likely to use them often, it’s also good to know how
    to create new columns that are derived from other existing columns in the DataFrame.
    One scenario that comes up often with structured data, like our NYC Parking Ticket
    dataset, is the need to parse and work with date/time dimensions. Back in chapter
    4, when we constructed our schema for the dataset, we opted to import the date
    columns as strings. However, to properly use dates for our analyses, we need to
    change those strings to datetime objects. Dask gives you the ability to automatically
    parse dates when reading data, but it can be finicky with formatting. An alternative
    approach that gives you more control over how the dates are parsed is to import
    the date columns as strings and manually parse them as part of your data prep
    workflow. In this section, we’ll learn how to use the `apply` method on DataFrames
    to apply generic functions to our data and create derived columns. More specifically,
    we’ll parse the Issue Date column, which represents the date that the parking
    citation was issued and converts that column to a datetime datatype. We’ll then
    create a new column containing the month and year that the citation was issued,
    which we’ll use again later in the chapter. With that in mind, let’s get to it!
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管你在上一节中学到的值重编码方法非常有用，你可能会经常使用它们，但了解如何创建由 DataFrame 中其他现有列派生的新列也是很好的。在结构化数据中，例如我们的纽约市停车罚单数据集，经常出现的一个场景是需要解析和处理日期/时间维度。在第四章中，当我们为数据集构建架构时，我们选择将日期列作为字符串导入。然而，为了正确地使用日期进行分析，我们需要将这些字符串转换为日期时间对象。Dask
    允许你在读取数据时自动解析日期，但它对格式有些挑剔。另一种提供更多控制如何解析日期的方法是将日期列作为字符串导入，并在你的数据准备工作流程中手动解析它们。在本节中，我们将学习如何使用
    DataFrame 上的 `apply` 方法来应用通用函数到我们的数据并创建派生列。更具体地说，我们将解析“发行日期”列，该列代表停车罚单的发行日期，并将该列转换为日期时间数据类型。然后我们将创建一个包含罚单发行月份和年份的新列，我们将在本章的后面再次使用它。考虑到这一点，让我们开始吧！
- en: Listing 5.23 Parsing the Issue Date column
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.23 解析“发行日期”列
- en: '[PRE57]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'In [listing 5.23](#listing5.23), we first need to import the datetime object
    from Python’s standard library. Then, as you’ve seen in a few previous examples,
    we create a new Series object by selecting the Issue Date series from our DataFrame
    (`nyc_data_recode_stage6`) and use the `apply` method to perform the transformation.
    In this particular call to `apply`, we create an anonymous (lambda) function that
    takes a value from the input Series, runs it through the `datetime.strptime` function,
    and returns a parsed datetime object. The `datetime.strptime` function simply
    takes a string as input and parses it into a datetime object using the specified
    format. The format we specified was `"%m/%d/%Y"`, which is equivalent to an mm/dd/yyyy
    date. The last thing to note about the `apply` method is the `meta` argument we
    had to specify. Dask tries to infer the output type of the function passed into
    it, but it’s better to explicitly specify what the datatype is. In this case,
    datatype inference will fail so we’re required to pass an explicit datetime datatype.
    The next three lines of code should be very familiar by now: drop, assign, rename—the
    pattern we learned before to add a column to our DataFrame. Let’s take a look
    at what happened.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 5.23](#listing5.23)中，我们首先需要从 Python 的标准库中导入日期时间对象。然后，正如你在几个先前的例子中看到的，我们通过从我们的
    DataFrame (`nyc_data_recode_stage6`) 中选择发行日期系列来创建一个新的 Series 对象，并使用 `apply` 方法执行转换。在这个特定的
    `apply` 调用中，我们创建了一个匿名（lambda）函数，它从输入 Series 中取一个值，通过 `datetime.strptime` 函数运行它，并返回一个解析后的日期时间对象。`datetime.strptime`
    函数简单地接受一个字符串作为输入，并使用指定的格式将其解析为日期时间对象。我们指定的格式是 `"%m/%d/%Y"`，相当于 mm/dd/yyyy 日期格式。关于
    `apply` 方法的最后一点要注意的是我们必须指定的 `meta` 参数。Dask 尝试推断传递给它的函数的输出类型，但最好明确指定数据类型。在这种情况下，类型推断将失败，因此我们需要传递一个明确的日期时间数据类型。接下来的三行代码现在应该非常熟悉：drop、assign、rename——我们之前学到用来向
    DataFrame 添加列的模式。让我们看看发生了什么。
- en: Listing 5.24 Inspecting the result of date parsing
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.24 检查日期解析的结果
- en: '[PRE58]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Looking at the column, we get the following output:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 查看该列，我们得到以下输出：
- en: '[PRE59]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The column is no longer a string type—just what we wanted! Now let’s use our
    new datetime column to extract just the month and year.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 列不再是字符串类型——这正是我们想要的！现在让我们使用我们新的日期时间列来提取月份和年份。
- en: Listing 5.25 Extracting the month and year
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.25 提取月份和年份
- en: '[PRE60]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: This time, in [listing 5.25](#listing5.25), we again create a new Series based
    on the Issue Date column in the DataFrame. However, the function we pass through
    apply now uses the `strftime` method of Python’s datetime objects to extract the
    month and year from the datetime and return a formatted string. We’ve opted to
    format our month/year strings as “yyyyMM,” as specified in the `strftime` argument.
    We also specify that the output type of this function is an integer, as denoted
    by the `meta=int` argument. Finally, we follow the familiar assign-rename pattern
    as usual to add the column to the DataFrame. However, we didn’t need to drop any
    columns because we don’t want to replace an existing column with this new column.
    We’ll simply add it alongside our other columns in the DataFrame. Let’s take a
    look at the contents of this new column now.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，在[列表 5.25](#listing5.25)中，我们再次基于 DataFrame 中的发行日期列创建一个新的 Series。然而，我们通过 apply
    传递的函数现在使用 Python 日期时间对象的 `strftime` 方法从日期时间中提取月份和年份，并返回一个格式化的字符串。我们选择将月份/年份字符串格式化为“yyyyMM”，正如
    `strftime` 参数所指定的。我们还指定该函数的输出类型为整数，如 `meta=int` 参数所示。最后，我们像往常一样遵循熟悉的 assign-rename
    模式将列添加到 DataFrame 中。然而，我们不需要删除任何列，因为我们不想用这个新列替换现有的列。我们只需将其添加到 DataFrame 中其他列的旁边。现在让我们看看这个新列的内容。
- en: Listing 5.26 Inspecting the new derived column
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.26 检查新派生列
- en: '[PRE61]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Looking at the column, we get the following output:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 查看该列，我们得到以下输出：
- en: '[PRE62]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Perfect! Just what we wanted: a nice string representation of the month/year
    that the citation was issued. Now that we’ve created this column, we’re going
    to finally replace our sequential numeric index with the month/year of the citation!
    That will allow us to easily look up citations by month/year and do other neat
    things in coming chapters like look at the ebb and flow of ticket citations month-over-month.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 完美！这正是我们想要的：一个漂亮的月份/年份字符串表示，表示引用被发行。现在我们已经创建了此列，我们最终将用月份/年份替换我们的顺序数字索引！这将使我们能够轻松地按月份/年份查找引用，并在接下来的章节中做其他一些很酷的事情，比如查看月度票据引用的起伏。
- en: 5.5 Filtering and reindexing DataFrames
  id: totrans-403
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 过滤和重新索引 DataFrame
- en: Earlier in the chapter, you learned how to look up values by index slicing using
    the `loc` method. However, we have a few more-sophisticated ways to search and
    filter data using Boolean expressions. Let’s have a look at finding all citations
    for the month of October.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期，你学习了如何使用`loc`方法通过索引切片查找值。然而，我们还有一些更复杂的方法可以使用布尔表达式来搜索和过滤数据。让我们看看如何找到10月份的所有引用。
- en: Listing 5.27 Finding all citations that occurred in October
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.27 查找10月份发生的全部引用
- en: '[PRE63]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: In [listing 5.27](#listing5.27), we first create a list of month-year combinations
    we want to search for (October for years 2013–2017). We then use the familiar
    `isin` method to create a Boolean series that returns `True` for each row that
    matches one of the month-year combinations in the `months` list, and `False` for
    each row that doesn’t match. This Boolean series is then passed into the selector.
    When the result is computed, you’ll get a DataFrame back with only the citations
    that happened in October.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表5.27](#listing5.27)中，我们首先创建了一个我们想要搜索的月份-年份组合列表（2013年至2017年的10月）。然后我们使用熟悉的`isin`方法创建一个布尔序列，对于匹配`months`列表中月份-年份组合的每一行返回`True`，对于不匹配的每一行返回`False`。然后这个布尔序列被传递到选择器中。当结果被计算出来时，你会得到一个只包含10月份发生的引用的DataFrame。
- en: Any kind of Boolean expression that creates a Boolean series can be used this
    way. For example, instead of picking certain months, perhaps we want to find all
    citations that happened after a given date. We can use Python’s built-in inequality
    operators to do this.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 任何创建布尔序列的布尔表达式都可以这样使用。例如，如果我们不想选择某些月份，也许我们想要找到所有在给定日期之后发生的引用。我们可以使用Python内置的不等式运算符来完成这个任务。
- en: Listing 5.28 Finding all citations after 4/25/2016
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.28 查找2016年4月25日之后的全部引用
- en: '[PRE64]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: In [listing 5.28](#listing5.28), we use the greater-than operator to find all
    records with an Issue Date greater than 4-25-2016\. These Boolean filter expressions
    can also be chained together using the AND (`&`) and OR (`|`) operators to create
    quite complex filters! We’ll take a look at how to do that in the next code listing,
    in which we’ll also create a custom index for our DataFrame.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表5.28](#listing5.28)中，我们使用大于运算符来查找所有发行日期大于2016-04-25的记录。这些布尔过滤表达式也可以通过AND(`&`)和OR(`|`)运算符连接起来，创建相当复杂的过滤器！我们将在下一个代码列表中查看如何做到这一点，其中我们还将为我们的DataFrame创建一个自定义索引。
- en: Up to this point, we’ve relied solely on Dask’s default numeric index for our
    dataset. This has served us fine so far, but we’ve reached a point where ignoring
    the benefits of using a more suitable index could cause some serious performance
    issues. This becomes especially important when we want to combine multiple DataFrames,
    which is precisely what we’ll talk about in the next section of the chapter. While
    it’s possible to combine DataFrames that aren’t index aligned, Dask must scan
    both DataFrames for every possible unique combination of keys used to join the
    two DataFrames together, making it quite a slow process. When joining two DataFrames
    that have the same index, and are both sorted and partitioned in index order,
    the join operation is much faster. Therefore, to prepare our data for joining
    to another dataset, we’ll adjust the index and partitions to align with the other
    dataset.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直依赖Dask默认的数值索引来处理我们的数据集。到目前为止，这已经很好地为我们服务了，但我们已经到达了一个阶段，忽视使用更合适的索引的好处可能会引起一些严重的性能问题。这一点在我们想要合并多个DataFrame时尤为重要，这正是本章下一节我们将要讨论的内容。虽然可以合并索引不匹配的DataFrame，但Dask必须扫描两个DataFrame中用于连接两个DataFrame的每个可能的唯一键的组合，这使得整个过程相当缓慢。当我们连接具有相同索引的、按索引顺序排序和分区的两个DataFrame时，连接操作要快得多。因此，为了准备我们的数据以便与其他数据集合并，我们将调整索引和分区以与另一个数据集对齐。
- en: Setting an index on a DataFrame will sort the entire dataset by the specified
    column. While the sorting process can be quite slow, you can persist the result
    of the sorted DataFrame and even write your data back to disk in a sorted Parquet
    file so you only have to sort the data once. To set an index on a DataFrame, we
    use the `set_index` method.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在DataFrame上设置索引将按指定列对整个数据集进行排序。虽然排序过程可能相当慢，但你可以将排序DataFrame的结果持久化，甚至可以将你的数据以排序的Parquet文件的形式写回磁盘，这样你只需要排序一次。要设置DataFrame上的索引，我们使用`set_index`方法。
- en: Listing 5.29 Setting an index on a DataFrame
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.29 在DataFrame上设置索引
- en: '[PRE65]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: In [listing 5.29](#listing5.29), we’re taking the month-year column we created
    in the previous section of the chapter and sorting the DataFrame on that value.
    This will return a new DataFrame that’s sorted by that column, enabling us to
    use it for searching, filtering, and joining much more quickly. If you’re working
    with a dataset that was sorted before it was stored, you can pass the optional
    argument, `sorted=True`, to tell Dask that the data is already sorted. Also, you
    have an opportunity to adjust the partitioning similar to the repartition options
    you learned previously. You can specify a number of partitions to evenly split
    the data into using the `npartitions` argument, or you can manually specify the
    partition boundaries using the `divisions` argument. Since we’ve sorted the data
    by month/year, let’s repartition the data so each partition contains one month
    of data. The following listing demonstrates how to do this.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在[代码列表 5.29](#listing5.29)中，我们正在使用本章前一部分创建的月份-年份列对 DataFrame 进行排序。这将返回一个新的按该列排序的
    DataFrame，使我们能够更快地用于搜索、过滤和连接。如果你正在处理在存储之前已经排序的数据集，你可以传递可选参数`sorted=True`来告诉 Dask
    数据已经排序。此外，你还有机会调整分区，类似于你之前学到的`repartition`选项。你可以使用`npartitions`参数指定一个分区数，以均匀分割数据，或者你可以使用`divisions`参数手动指定分区边界。由于我们已经按月份/年份排序了数据，让我们重新分区数据，以便每个分区包含一个月的数据。以下列表演示了如何进行此操作。
- en: Listing 5.30 Repartitioning the data by month/year
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.30 按月份/年份重新分区数据
- en: '[PRE66]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: In [listing 5.30](#listing5.30), we first produce a list of month/year keys,
    which is used to define our partition scheme (201401, 201402, 201403, and so forth).
    Next, we pass the list of partitions into the `repartition` method to apply it
    to our newly reindexed DataFrame. Finally, we write the results out to a Parquet
    file to avoid needing to repeatedly sort the data every time subsequent computations
    are needed and read the sorted data into a new DataFrame called `nyc_data_new_index`.
    Now that we’ve set an index on our DataFrame, let’s round out the chapter by talking
    about using the index to combine DataFrames.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在[代码列表 5.30](#listing5.30)中，我们首先生成一个月份/年份键的列表，该列表用于定义我们的分区方案（201401、201402、201403
    等等）。接下来，我们将分区列表传递给`repartition`方法，将其应用于我们新索引的 DataFrame。最后，我们将结果写入 Parquet 文件，以避免在后续计算需要时重复排序数据，并将排序后的数据读入一个新的
    DataFrame，称为`nyc_data_new_index`。现在我们已经为我们的 DataFrame 设置了索引，让我们通过讨论如何使用索引来合并 DataFrame
    来结束本章。
- en: 5.6 Joining and concatenating DataFrames
  id: totrans-420
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.6 连接和拼接 DataFrame
- en: If you’ve worked with a relational database management system (RDBMS) before,
    such as SQL Server, you likely already have an appreciation for the power of join
    and union operations. Regardless of whether you’re an expert DBA or just getting
    your first taste of data engineering, it’s important to cover these operations
    in depth because they offer a whole different host of potential performance pitfalls
    in a distributed environment. First, let’s briefly review how join operations
    work. [Figure 5.9](#figure5.9) displays the visual result of a join operation.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前使用过关系型数据库管理系统（RDBMS），例如 SQL Server，你很可能已经对连接和并集操作的力量有了认识。无论你是经验丰富的数据库管理员（DBA）还是刚开始接触数据工程，深入理解这些操作都是非常重要的，因为它们在分布式环境中提供了许多潜在的性能陷阱。首先，让我们简要回顾一下连接操作是如何工作的。[图
    5.9](#figure5.9) 展示了连接操作的视觉结果。
- en: In a join operation, two data objects (such as tables and DataFrames) are combined
    into a single object by adding the columns from the left object to the columns
    of the right object. When we joined the Person table with the Pet table, the resulting
    object added the columns from the Pet table to the right of the columns from the
    Person table. Using the combined table, we can determine relationships between
    the objects, such as Jack is my family’s ever-hungry brown tiger tabby. These
    two objects are logically connected by keys, or a column in one table that’s used
    to look up values in another table.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 在连接操作中，两个数据对象（例如表和 DataFrame）通过将左侧对象中的列添加到右侧对象的列中合并成一个单一的对象。当我们把 Person 表和 Pet
    表连接起来时，结果对象将 Pet 表的列添加到 Person 表列的右侧。使用合并后的表，我们可以确定对象之间的关系，例如杰克是我家那个永远饿着的棕色虎斑猫。这两个对象通过键逻辑上连接，或者是一个表中的列，用于在另一个表中查找值。
- en: '![c05_09.png](Images/c05_09.png)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![c05_09.png](Images/c05_09.png)'
- en: '[Figure 5.9](#figureanchor5.9) A join operation combines two datasets by adding
    the columns of the right table to the columns of the left table.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.9](#figureanchor5.9) 连接操作通过将右侧表的列添加到左侧表的列中来合并两个数据集。'
- en: '![c05_10.png](Images/c05_10.png)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![c05_10.png](Images/c05_10.png)'
- en: '[Figure 5.10](#figureanchor5.10) We can tell that Jack is my cat because his
    Owner ID is a key that points to my Person ID.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.10](#figureanchor5.10) 我们可以判断杰克是我的猫，因为他的所有者ID是一个指向我的Person ID的关键。'
- en: In [figure 5.10](#figure5.10), you can see the key relationship between these
    two tables. Jack has an Owner ID of 1000, which corresponds to my Person ID of
    1000\. Therefore, if you wanted additional information about Jack, like who his
    owner is, you could use this relationship to look up my information. This kind
    of relational model is the primary way complex structured datasets are stored
    in the real world. Since people, places, things, and events typically have some
    degree of relationship to one another, this relational model is an intuitive way
    to structure and organize interrelated datasets. Let’s take a closer look at that
    combined table again.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图5.10](#figure5.10)中，你可以看到这两个表之间的关键关系。杰克有一个所有者ID为1000，这对应于我的Person ID为1000。因此，如果你想获取关于杰克的额外信息，比如他的所有者是谁，你可以使用这种关系来查找我的信息。这种关系模型是复杂结构化数据集在现实世界中存储的主要方式。由于人、地点、事物和事件通常彼此之间有一定程度的关系，这种关系模型是结构化和组织相关数据集的直观方式。让我们再次仔细看看那个合并的表。
- en: '![c05_11.eps](Images/c05_11.png)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![c05_11.eps](Images/c05_11.png)'
- en: '[Figure 5.11](#figureanchor5.11) Showing all the key relationships between
    the Person and Pet tables'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.11](#figureanchor5.11) 显示Person和Pet表之间的所有关键关系'
- en: '![c05_12.png](Images/c05_12.png)'
  id: totrans-430
  prefs: []
  type: TYPE_IMG
  zh: '![c05_12.png](Images/c05_12.png)'
- en: '[Figure 5.12](#figureanchor5.12) New York City average monthly temperature'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.12](#figureanchor5.12) 纽约市平均月度温度'
- en: Notice in [figure 5.11](#figure5.11) that Sarah Robinson doesn’t appear in the
    joined table. It also happens that she doesn’t have a pet. What we’re seeing here
    is called an *inner join*. This means that only records between the two objects
    that have relationships with one another are put in the combined table. Records
    that have no relationships are discarded. If the purpose of combining these two
    tables is to learn more about each pet’s owner, it wouldn’t make sense to include
    people that don’t have any pets. To perform an inner join, you must specify `how=inner`
    as an argument of the `join` method. Let’s see an example of this in action.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在[图5.11](#figure5.11)中，莎拉·罗宾逊没有出现在合并的表中。碰巧的是，她也没有宠物。我们在这里看到的是所谓的*内连接*。这意味着只有两个对象之间有关系的记录会被放入合并的表中。没有关系的记录会被丢弃。如果合并这两个表的目的是为了更多地了解每只宠物的所有者，那么包括没有宠物的那些人就没有意义了。要执行内连接，你必须将`how=inner`作为`join`方法的参数。让我们看看这个操作的例子。
- en: 5.6.1 Joining two DataFrames
  id: totrans-433
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.1 连接两个DataFrame
- en: Returning to our NYC Parking Ticket Data example, I collected some average monthly
    temperature data for New York City from the National Oceanic and Atmospheric Administration
    (NOAA), and have included this data along with the code notebooks. Since we’ve
    indexed our parking citation data by month/year, let’s add on the average monthly
    temperature for the month in which the citation was given. Perhaps we’ll see a
    trend that parking citations happen more in warm weather months when parking enforcement
    can hit the streets. [Figure 5.12](#figure5.12) displays a sample of the temperature
    data.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到我们的纽约市停车罚单数据示例，我从美国国家海洋和大气管理局（NOAA）收集了一些纽约市的平均月度温度数据，并将这些数据以及代码笔记本一起包含在内。由于我们已经按月份/年份对停车罚单数据进行了索引，因此让我们添加在罚单发放月份的平均月度温度。也许我们会看到停车罚单在温暖的月份发生的趋势，因为那时停车执法可以上街。[图5.12](#figure5.12)显示了温度数据的一个样本。
- en: Since the average temperature data and the parking citation data are indexed
    by the same value (a string representation of month/year), the two datasets are
    index aligned and joining them will be a rather quick operation! The next listing
    shows what that looks like.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 由于平均温度数据和停车罚单数据按相同的值（月份/年份的字符串表示）索引，这两个数据集是索引对齐的，因此将它们连接起来将是一个相当快速的操作！下一个列表显示了这看起来是什么样子。
- en: Listing 5.31 Combining the NYC Parking Ticket data with NOAA weather data
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.31 将纽约市停车罚单数据与NOAA天气数据合并
- en: '[PRE67]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: In [listing 5.31](#listing5.31), we first read in the other dataset using Pandas.
    I’ve opted to read this file with Pandas because it’s very small (only a few KBs).
    It’s also worth demonstrating that Pandas DataFrames can be joined to Dask DataFrames.
    Of course, Dask DataFrames can be joined to other Dask DataFrames the exact same
    way, so you have a degree of flexibility here. I’ve next set the index on the
    `nyc_temps` DataFrame to make it index aligned with the Dask DataFrame. Finally,
    we call the join method on the `nyc_data_new_index` DataFrame and pass in the
    temperature DataFrame as the first argument. We also specify `how=inner` to denote
    that this is an inner join. [Figure 5.13](#figure5.13) displays the output of
    [listing 5.31](#listing5.31).
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 5.31](#listing5.31) 中，我们首先使用 Pandas 读取其他数据集。我选择使用 Pandas 读取此文件，因为它非常小（只有几
    KB）。还值得演示 Pandas DataFrame 可以与 Dask DataFrame 连接。当然，Dask DataFrame 也可以以完全相同的方式与其他
    Dask DataFrame 连接，因此您在这里有一定的灵活性。接下来，我将 `nyc_temps` DataFrame 的索引设置为使其与 Dask DataFrame
    索引对齐。最后，我们在 `nyc_data_new_index` DataFrame 上调用 join 方法，并将温度 DataFrame 作为第一个参数传入。我们还指定
    `how=inner` 来表示这是一个内连接。[图 5.13](#figure5.13) 显示了 [列表 5.31](#listing5.31) 的输出。
- en: '![c05_13.png](Images/c05_13.png)'
  id: totrans-439
  prefs: []
  type: TYPE_IMG
  zh: '![c05_13.png](Images/c05_13.png)'
- en: '[Figure 5.13](#figureanchor5.13) The output of [listing 5.31](#listing5.31)'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.13](#figureanchor5.13) [列表 5.31](#listing5.31) 的输出'
- en: As you can see, the Temp column was added to the right of the original DataFrame.
    We’ll keep our eye on that one as we move into the next chapter. Because the weather
    data overlaps the entire timeframe of the parking citation data, we didn’t lose
    any rows in the join process. And, because the DataFrames were index aligned,
    it was a very fast operation. It’s possible to join DataFrames that aren’t index
    aligned, but it can be so detrimental to performance that it’s not really worth
    covering in this book. I’d strongly recommend against it.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，Temp 列被添加到原始 DataFrame 的右侧。当我们进入下一章时，我们将关注这一点。因为天气数据覆盖了整个停车罚单数据的时段，我们在连接过程中没有丢失任何行。而且，因为
    DataFrame 是索引对齐的，所以这是一个非常快速的操作。虽然可以连接索引不对齐的 DataFrame，但这可能会对性能产生严重影响，因此在本书中不值得详细讨论。我强烈建议不要这样做。
- en: If you don’t want to discard records that are unrelated, then you will want
    to perform an *outer join*.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不想丢弃无关的记录，那么您将想要执行一个 *外连接*。
- en: '![c05_14.png](Images/c05_14.png)'
  id: totrans-443
  prefs: []
  type: TYPE_IMG
  zh: '![c05_14.png](Images/c05_14.png)'
- en: '[Figure 5.14](#figureanchor5.14) The result of an outer join preserves records
    that don’t have any relationships.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.14](#figureanchor5.14) 外连接的结果保留了没有任何关系的记录。'
- en: In [figure 5.14](#figure5.14), you can see that as a result of the outer join,
    the pets that have owners are connected as before, but now Sarah shows up in our
    joined table. That’s because an outer join does not discard unrelated records.
    What happens instead is columns that come from the unrelated table will contain
    missing values. You can see in [figure 5.14](#figure5.14) that, since Sarah doesn’t
    have any pets, information about her pets is NULL, which represents missing/unknown
    data. Dask’s default behavior is to perform an outer join, so unless you specify
    otherwise, joining two tables will yield a result like this.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 5.14](#figure5.14) 中，您可以看到，由于外连接，有主人的宠物仍然保持连接，但现在 Sarah 出现在我们的连接表中。这是因为外连接不会丢弃无关记录。相反，来自无关表的列将包含缺失值。您可以在
    [图 5.14](#figure5.14) 中看到，由于 Sarah 没有任何宠物，关于她的宠物信息是 NULL，这代表缺失/未知数据。Dask 的默认行为是执行外连接，所以除非您明确指定，否则连接两个表将产生类似的结果。
- en: 5.6.2 Unioning two DataFrames
  id: totrans-446
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.2 连接两个 DataFrame
- en: The other way to combine datasets is along the row axis. In RDBMSs, this is
    called a union operation, but in Dask it’s called *concatenating* DataFrames.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集组合的另一种方式是沿着行轴。在关系型数据库管理系统（RDBMS）中，这被称为并操作，但在 Dask 中被称为 *连接* DataFrame。
- en: '[Figure 5.15](#figure5.15) shows the result of concatenating the Person and
    More People tables. Whereas joins add more data by increasing the number of columns,
    you can see that concatenation adds more data by increasing the number of rows.
    Columns that share the same column name across both tables are aligned to each
    other and the rows are merged. You can also see the result of what happens when
    the two tables don’t have the exact same columns. In this case, the Favorite Food
    column didn’t overlap between both tables, so the values for the people originating
    from the Person table are assigned a missing value for Favorite Food. Let’s see
    at what this looks like in Dask.'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.15](#figure5.15) 展示了拼接 Person 和 More People 表的结果。与通过增加列数来添加更多数据的不同，你可以看到拼接是通过增加行数来添加更多数据的。在两个表中具有相同列名的列是对齐的，并且行被合并。你还可以看到当两个表没有完全相同的列时会发生什么。在这种情况下，Favorite
    Food 列在两个表中没有重叠，所以来自 Person 表的人的 Favorite Food 值被分配了一个缺失值。让我们看看在 Dask 中这看起来像什么。'
- en: '![c05_15.eps](Images/c05_15.png)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
  zh: '![c05_15.eps](Images/c05_15.png)'
- en: '[Figure 5.15](#figureanchor5.15) The result of concatenating the Person and
    More People tables'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.15](#figureanchor5.15) 拼接 Person 和 More People 表的结果'
- en: Listing 5.32 Concatenating two DataFrames
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.32 拼接两个 DataFrame
- en: '[PRE68]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: In [listing 5.32](#listing5.32), we’re momentarily going all the way back to
    our raw data. Because we don’t need to have a common index we will concatenate
    the raw data from the two sources. An alternative to the schema building and combined
    loading (using *.csv in the file path) we did in chapter 4 would be to load each
    of the files individually and concatenate them with the `append` method. Syntactically,
    this is very simple, and no additional arguments exist for the `append` method.
    You can see that `fy16` contains 10,626,899 rows, `fy17` contains 10,803,028 rows,
    and together `fy1617` contains 21,429,927 rows.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 5.32](#listing5.32)中，我们暂时回到原始数据。因为我们不需要一个共同的索引，所以我们将从两个来源拼接原始数据。作为第四章中我们使用的方案构建和组合加载（使用文件路径中的*.csv）的替代方案，我们可以单独加载每个文件，并使用`append`方法将它们拼接起来。在语法上，这非常简单，`append`方法没有额外的参数。你可以看到`fy16`包含10,626,899行，`fy17`包含10,803,028行，而`fy1617`总共包含21,429,927行。
- en: 5.7 Writing data to text files and Parquet files
  id: totrans-454
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.7 将数据写入文本文件和 Parquet 文件
- en: Now that we’ve put in a considerable amount of work to clean the dataset, it
    would be an opportune time to save your progress. While using the `persist` method
    on your DataFrames from time to time is a good idea to maximize performance, its
    persistence is only temporary. If you shut down your notebook server and end your
    Python session, the persisted DataFrames will be cleared from memory, meaning
    that you’ll have to re-run all the computations when you’re ready to resume working
    with the data.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经投入了大量工作来清理数据集，这是一个保存进度的好时机。虽然不时使用`persist`方法对你的 DataFrame 是一个好的主意，以最大化性能，但其持久性只是临时的。如果你关闭笔记本服务器并结束你的
    Python 会话，持久化的 DataFrame 将从内存中清除，这意味着当你准备好重新开始与数据一起工作时，你必须重新运行所有计算。
- en: 'Writing data out of Dask is pretty simple but has one caveat: since Dask divides
    data into partitions when working on computations, its default behavior is to
    write one file per partition. This isn’t really an issue if you’re writing to
    a distributed filesystem or you’re going to consume the data with another distributed
    system such as Spark or Hive, but if you want to save a single file that you can
    import into another data analysis tool like Tableau or Excel, you’ll have to fold
    all the data into a single partition using the `repartition` method before saving
    it.'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据从 Dask 写出相当简单，但有一个注意事项：由于 Dask 在进行计算时将数据划分为分区，其默认行为是每个分区写一个文件。如果你将数据写入分布式文件系统，或者你打算使用另一个分布式系统（如
    Spark 或 Hive）来消费数据，这并不是一个问题，但如果你想要保存一个可以导入到其他数据分析工具（如 Tableau 或 Excel）的单个文件，你必须在保存之前使用`repartition`方法将所有数据折叠到一个单独的分区中。
- en: 'In this section we’ll look at writing data in both of the formats you learned
    to read in the previous chapter: delimited text files and Parquet.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨如何以你上一章学习到的两种格式写入数据：分隔符文本文件和 Parquet。
- en: 5.7.1 Writing to delimited text files
  id: totrans-458
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7.1 将数据写入分隔符文本文件
- en: First, we’ll look at how to write data back to delimited text files.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将看看如何将数据写回到分隔符文本文件中。
- en: Listing 5.33 Writing a CSV file
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.33 写入 CSV 文件
- en: '[PRE69]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[Listing 5.33](#listing5.33) shows how to save the combined dataset we created
    in the previous sections of the chapter to a single CSV file. One thing to notice
    is the filename we’ve given the data: `part*.csv`. The `*` wildcard will be filled
    in automatically by Dask, indicating the partition number that corresponds to
    that file. Since we’ve collapsed all the data together into a single partition,
    only one CSV file will be written, and it will be called part0.csv. Producing
    a single CSV file may be useful for exporting data to be used in other applications,
    but Dask is a distributed library. It makes much more sense from a performance
    standpoint to keep the data broken into multiple files, which can be read in parallel.
    In fact, Dask’s default behavior is to save each partition to a separate file.
    Next, we’ll look at a few other important options that can be set in the `to_csv`
    method, and we’ll write out the data into multiple CSV files.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.33](#listing5.33) 展示了如何将本章前几节中创建的合并数据集保存到单个 CSV 文件中。要注意的一件事是数据的文件名：`part*.csv`。通配符
    `*` 将由 Dask 自动填充，表示与该文件对应的分区号。由于我们将所有数据合并到一个单独的分区中，因此只会写入一个 CSV 文件，其名称为 part0.csv。生成单个
    CSV 文件可能对导出数据以在其他应用程序中使用很有用，但 Dask 是一个分布式库。从性能角度来看，保持数据拆分为多个文件，这些文件可以并行读取，更有意义。事实上，Dask
    的默认行为是将每个分区保存到单独的文件中。接下来，我们将查看可以在 `to_csv` 方法中设置的几个其他重要选项，并将数据写入多个 CSV 文件。'
- en: Using the default settings in both the methods makes a few assumptions about
    the shape of the output file. Namely, by default, the `to_csv` method will create
    files that
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种方法中默认设置都做了一些关于输出文件形状的假设。具体来说，默认情况下，`to_csv` 方法将创建文件，这些文件
- en: Use a comma (`,`) as a column delimiter
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用逗号（`,`）作为列分隔符
- en: Save missing (`np.nan`) values as an empty string (`""`)
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将缺失值（`np.nan`）保存为空字符串（`""`）
- en: Include a header row
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含标题行
- en: Include the index as a column
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将索引作为列包含
- en: Do not use compression
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不使用压缩
- en: Listing 5.34 Writing a delimited text file with custom options
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.34](#listing5.34) 写入具有自定义选项的分隔文本文件'
- en: '[PRE70]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[Listing 5.34](#listing5.34) demonstrates how to change and customize these
    assumptions. This particular line of code will write the `data` DataFrame to 48
    files, which will be compressed using gzip, will use the pipe (|) as a column
    delimiter instead of the comma, will write any missing values as `NULL`, and will
    not write a header row or an index column. You can adjust any of these options
    to suit your needs.'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.34](#listing5.34) 展示了如何更改和自定义这些假设。这段特定的代码将 `data` DataFrame 写入 48 个文件，这些文件将使用
    gzip 进行压缩，将管道（|）用作列分隔符而不是逗号，将任何缺失值作为 `NULL` 写入，并且不会写入标题行或索引列。你可以调整这些选项中的任何一项以满足你的需求。'
- en: 5.7.2 Writing to Parquet files
  id: totrans-472
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7.2 写入 Parquet 文件
- en: Writing to Parquet is very similar to writing to delimited text files. The key
    difference is that instead of specifying a scheme for individual filenames, Parquet
    is simply saved to a directory. Since Parquet is best used by distributed systems,
    it’s not really worth it to adjust partitioning like we did when saving delimited
    text files. Parquet’s option set is very simple.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 写入 Parquet 与写入分隔文本文件非常相似。关键区别在于，我们不是为单个文件名指定方案，而是将 Parquet 简单地保存到目录中。由于 Parquet
    最好由分布式系统使用，因此调整分区（就像我们在保存分隔文本文件时做的那样）并不值得。Parquet 的选项集非常简单。
- en: Listing 5.35 Writing a DataFrame to Parquet
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.35](#listing5.35) 写入 DataFrame 到 Parquet'
- en: '[PRE71]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[Listing 5.35](#listing5.35) demonstrates writing data to Parquet on a local
    filesystem using the snappy compression codec. It’s also possible to save to HDFS
    or S3 simply by following the path mechanics you’ve already learned in the previous
    chapter. As with non-repartitioned text files, Dask will write one Parquet file
    per partition.'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.35](#listing5.35) 展示了使用 snappy 压缩编解码器在本地文件系统上写入 Parquet 数据的方法。也可以简单地通过遵循上一章中已经学到的路径机制将数据保存到
    HDFS 或 S3。与非分区文本文件一样，Dask 将为每个分区写入一个 Parquet 文件。'
- en: 'There you have it. In this chapter we''ve covered many techniques for manipulating
    data, as well as a very large part of the Dask DataFrame API. I hope you feel
    much more confident in your ability to manipulate DataFrames now. We have our
    data cleaned up and are ready to begin analyzing it. Since you’ve saved your DataFrame,
    feel free to take a break, grab a coffee, and get ready for the fun part: data
    analysis!'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样了。在本章中，我们介绍了许多数据操作技术，以及 Dask DataFrame API 的很大一部分。我希望你现在对操作 DataFrame 的能力更有信心。我们已经清理了数据，准备开始分析它。由于你已经保存了
    DataFrame，你可以休息一下，喝杯咖啡，准备进入有趣的部分：数据分析！
- en: Summary
  id: totrans-478
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Selecting columns from a DataFrame uses square bracket ([…]) notation. You can
    select more than one column by passing a list of column names into the column
    selector brackets.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 DataFrame 中选择列使用方括号（[…]）表示法。您可以通过将列名列表传递到列选择器括号中来选择多个列。
- en: The `head` method shows the first 10 rows of a DataFrame by default. You can
    also specify a number of rows you want to view.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，`head` 方法显示 DataFrame 的前 10 行。您也可以指定要查看的行数。
- en: Columns can be dropped from a DataFrame by using the `drop` method. However,
    since DataFrames are immutable, the column is not dropped from the original DataFrame.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用 `drop` 方法从 DataFrame 中删除列。然而，由于 DataFrame 是不可变的，列不会被从原始 DataFrame 中删除。
- en: Null values can be removed from DataFrames using the `dropna` method.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用 `dropna` 方法从 DataFrame 中删除空值。
- en: Use the drop-assign-rename pattern to replace columns in a DataFrame, such as
    when parsing or recoding values in a column.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 drop-assign-rename 模式在 DataFrame 中替换列，例如在解析或重新编码列中的值时。
- en: Transformation functions can be performed elementwise over a DataFrame using
    the `apply` method.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用 `apply` 方法在 DataFrame 上执行元素级转换函数。
- en: Boolean operators (such as >, <, =) are supported for filtering DataFrames.
    If your filter condition requires more than one input value, you can use NumPy-style
    Boolean functions such as `isin`.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持布尔运算符（如 >, <, =）用于过滤 DataFrame。如果您的过滤条件需要多个输入值，您可以使用类似 NumPy 风格的布尔函数，如 `isin`。
- en: Two DataFrames can be relationally joined using the `merge` method. You can
    even merge a Pandas DataFrame to a Dask DataFrame!
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用 `merge` 方法通过关系连接两个 DataFrame。您甚至可以将 Pandas DataFrame 与 Dask DataFrame 进行合并！
- en: DataFrames can be concatenated (unioned) using the `append` method.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用 `append` 方法将 DataFrame 连接（合并）。
- en: '6'
  id: totrans-488
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Summarizing and analyzing DataFrames
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 总结和分析 DataFrame
- en: '**This chapter covers**'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章内容涵盖**'
- en: Producing descriptive statistics for a Dask Series
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 Dask Series 生成描述性统计
- en: Aggregating/grouping data using Dask’s built-in aggregate functions
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Dask 内置的聚合函数聚合/分组数据
- en: Creating your own custom aggregation functions
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建您自己的自定义聚合函数
- en: Analyzing time series data with rolling window functions
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用滚动窗口函数分析时间序列数据
- en: At the end of the previous chapter we arrived at a dataset ready for us to start
    digging in and analyzing. However, we didn’t perform an exhaustive search for
    every possible issue with the data. In reality, the data cleaning and preparation
    process can take a far longer time to complete. It’s a common adage among data
    scientists that data cleaning can take 80% or more of the total time spent on
    a project. With the skills you learned in the previous chapter, you have a good
    foundation to address all the most common data-quality issues you’ll come across
    in the wild. As a friendly reminder, [figure 6.1](#figure6.1) shows how we’re
    progressing through our workflow—we’re almost at the halfway point!
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章的结尾，我们得到了一个可以开始深入分析和研究的数据集。然而，我们没有对数据中可能出现的每一个问题进行彻底搜索。实际上，数据清洗和准备过程可能需要更长的时间来完成。数据科学家中有一个常见的说法，数据清洗可能占用项目总时间的
    80% 或更多。通过您在上一章中学到的技能，您已经具备了应对在野外遇到的所有最常见数据质量问题的良好基础。作为一个友好的提醒，[图 6.1](#figure6.1)
    展示了我们的工作流程进展——我们几乎已经到达了中点！
- en: '![c06_01.eps](Images/c06_01.png)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
  zh: '![c06_01.eps](Images/c06_01.png)'
- en: '[Figure 6.1](#figureanchor6.1) The *Data Science with Python and Dask* workflow'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6.1](#figureanchor6.1) 使用 Python 和 Dask 进行数据科学的流程'
- en: We’ll now turn our attention to my favorite part of any data science project—exploratory
    data analysis. The goals of exploratory data analysis are to understand the “shape”
    of your data, find interesting patterns and correlations in your dataset, and
    identify significant relationships in your dataset that could be useful for predicting
    your target variable. As with the previous chapter, we’ll highlight the differences
    and special considerations necessary to perform data analysis in the distributed
    paradigm of Dask.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将把注意力转向任何数据科学项目中最喜欢的部分——探索性数据分析。探索性数据分析的目标是了解您数据的“形状”，在您的数据集中找到有趣的模式和相关性，并识别出对预测目标变量可能有用的数据集中显著关系。与上一章一样，我们将强调在
    Dask 分布式范式下进行数据分析时必要的差异和特殊考虑。
- en: 6.1 Descriptive statistics
  id: totrans-499
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 描述性统计
- en: 'In the final dataset we created at the end of chapter 5, we have somewhere
    in the neighborhood of 41 million parking citations—that’s a lot of observations!
    One thing that might be interesting to know is the average age of cars parked
    (illegally) on New York City streets. Are there more newer cars than older cars?
    How old was the oldest illegally parked car—are we talking Model T or Thunderbird?
    Using descriptive statistics, we’ll answer the following questions:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章末尾创建的最终数据集中，我们大约有4100万张停车罚单——这是一个大量的观测值！可能有趣的是知道纽约市街道上停放的车辆的平均年龄。是新车比旧车多吗？最老的非法停放的车辆有多老——我们是谈论T型车还是雷鸟？使用描述性统计，我们将回答以下问题：
- en: Using the NYC Parking Ticket data, what is the average age of vehicles parked
    illegally on the streets of New York City? What can we infer about the age of
    the vehicles?
  id: totrans-501
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用纽约市停车罚单数据，纽约市街道上非法停放的车辆的平均年龄是多少？我们可以从车辆的年龄中推断出什么？
- en: 6.1.1 What are descriptive statistics?
  id: totrans-502
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 描述性统计是什么？
- en: 'Before jumping into the code, we’ll start with a brief overview of how to understand
    the shape of our data. This is typically defined by seven mathematical properties
    used to describe it:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始编写代码之前，我们将简要概述如何理解我们数据的形状。这通常由七个用于描述它的数学属性定义：
- en: The smallest value (also called the *minimum*)
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小的值（也称为*最小值*）
- en: The largest value (also called the *maximum*)
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大的值（也称为*最大值*）
- en: The average of all data points (also called the *mean*)
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有数据点的平均值（也称为*均值*）
- en: The middle point between the minimum and maximum value (also called the *median*)
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小值和最大值之间的中点（也称为*中位数*）
- en: How spread out the data is from the mean (also called the *standard**deviation*)
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据从平均值（也称为*标准差*）的偏差程度
- en: The most commonly occurring observation (also called the *mode*)
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最常出现的观测值（也称为*众数*）
- en: How balanced the number of data points to the left and right of the central
    point is (also called the *skewness*)
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据点在中心点的左右两侧的平衡程度（也称为*偏度*）
- en: You’ve undoubtedly heard some of these terms before, because these concepts
    are typically taught as the foundation of any basic statistics course. These *descriptive
    statistics*, while simple, are very powerful ways to describe all kinds of data
    and tell us important things about our data. As a refresher, here’s a visual guide
    to descriptive statistics.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 你无疑之前听说过这些术语中的一些，因为这些概念通常作为任何基础统计学课程的基石来教授。这些*描述性统计*虽然简单，但是非常强大的描述各种数据的方式，并告诉我们关于我们数据的重要信息。作为一个复习，这里有一个描述性统计的视觉指南。
- en: '![c06_02.eps](Images/c06_02.png)'
  id: totrans-512
  prefs: []
  type: TYPE_IMG
  zh: '![c06_02.eps](Images/c06_02.png)'
- en: '[Figure 6.2](#figureanchor6.2) A visual guide to descriptive statistics'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.2](#figureanchor6.2) 描述性统计的视觉指南'
- en: '[Figure 6.2](#figure6.2) shows a histogram of a hypothetical variable in which
    100,000 observations were made. The values that were observed are along the X
    axis, and the relative *frequency* (how often each value was observed as a percent
    of all observations) is plotted along the Y axis. What you can take away from
    this is that we observed a range of values. Sometimes we observed a value of 0,
    sometimes 5.2, sometimes –3.48, and many more in between. Since we know that the
    observed value of this hypothetical variable doesn’t always stay the same, we
    call this a *randomly distributed variable*. In order to cope with this variable’s
    randomness, it would be useful to set some expectations about the range of possibilities
    that we could observe and the value that we’ll most likely observe. This is precisely
    what descriptive statistics aim to achieve!'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.2](#figure6.2) 展示了一个假设变量的直方图，其中进行了100,000次观测。观测到的值沿着X轴分布，相对频率（每个值被观测到的频率占所有观测值的百分比）沿着Y轴绘制。你可以从中得到的信息是，我们观测到了一系列的值。有时我们观测到0的值，有时是5.2，有时是-3.48，以及许多介于它们之间的值。由于我们知道这个假设变量的观测值并不总是保持不变，所以我们称它为*随机分布变量*。为了应对这个变量的随机性，设定一些关于我们可能观测到的范围和最可能观测到的值的期望将是有用的。这正是描述性统计试图实现的目标！'
- en: Back to [figure 6.2](#figure6.2), take a look at the minimum and maximum. As
    they are intuitively named, they serve as boundary points for the range of observations
    that were made. There were no observations made that fell below the minimum (of
    –10), and likewise, there were no observations that fell above the maximum (of
    10). This tells us that it would be very unlikely to see any future observations
    outside this range. Next, have a look at the mean. This is the “center of mass”
    of the distribution, meaning if we make a random observation, the value is most
    likely to be near this point. You can see that the probability is 0.16, meaning
    that roughly 16% of the time we can expect to observe a value of 0\. But what’s
    likely to occur the other 80% of the time? This is where the standard deviation
    comes into play. The higher the standard deviation, the greater the likelihood
    that we’ll observe values that are far away from the mean.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 回到[图6.2](#figure6.2)，看看最小值和最大值。正如它们的名称所暗示的，它们是所做观察范围边界点。没有观察到低于最小值（-10）的观察值，同样，也没有观察到高于最大值（10）的观察值。这告诉我们，未来观察到这个范围之外的任何观察值的可能性非常小。接下来，看看平均值。这是分布的“质心”，这意味着如果我们进行随机观察，值最有可能接近这个点。你可以看到概率是0.16，这意味着大约16%的时间我们可以期望观察到值为0。但其他80%的时间可能发生什么呢？这就是标准差发挥作用的地方。标准差越高，我们观察到远离平均值的值的可能性就越大。
- en: '![c06_03.eps](Images/c06_03.png)'
  id: totrans-516
  prefs: []
  type: TYPE_IMG
  zh: '![c06_03.eps](Images/c06_03.png)'
- en: '[Figure 6.3](#figureanchor6.3) A comparison of standard deviation'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.3](#figureanchor6.3) 标准差的比较'
- en: You can see this behavior in [figure 6.3](#figure6.3). With a small standard
    deviation, the probability drops off sharply as you move away from the mean, meaning
    values far away from the mean are very unlikely to be observed. Conversely, with
    a large standard deviation, the drop off is much smoother, indicating that values
    far away from the mean are more likely to be observed. In the extreme case, a
    standard deviation of 0 would indicate that the value is constant and is not a
    randomly distributed variable. If we observed a small standard deviation of vehicle
    ages, we could conclude that many vehicles parked illegally in New York are roughly
    the same age—or, put another way, little variation exists in the observed age
    of the vehicles. If we observed a high standard deviation, it would mean that
    a highly diverse mix of new and old vehicles exists. In [figure 6.3](#figure6.3),
    notice that both the distributions are symmetrical. This means that the probability
    of observing a value of 1 is equal to observing a value of –1 and so on. The rate
    of drop-off in probability does not differ based on the direction we move away
    from the highest point on the curve (which represents the most commonly observed
    value, or the *mode*). This symmetry (or potential asymmetry) is what skewness
    describes.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图6.3](#figure6.3)中看到这种行为。当标准差较小时，随着你远离平均值，概率会急剧下降，这意味着远离平均值的值不太可能被观察到。相反，当标准差较大时，下降趋势会更加平滑，这表明远离平均值的值更有可能被观察到。在极端情况下，标准差为0表示该值是恒定的，并且不是一个随机分布的变量。如果我们观察到车辆年龄的小标准差，我们可以得出结论，许多非法停放在纽约的车辆年龄大致相同——或者说，另一种说法是，观察到的车辆年龄变化很小。如果我们观察到大的标准差，这意味着存在高度多样化的新旧车辆混合。在[图6.3](#figure6.3)中，请注意，这两个分布都是对称的。这意味着观察到值为1的概率与观察到值为-1的概率相等，依此类推。概率下降的速度不会因我们远离曲线最高点（代表最常观察到的值，或*众数*）的方向而有所不同。这种对称性（或潜在的不对称性）就是偏度所描述的。
- en: In [figure 6.4](#figure6.4), you can see what the difference in skewness does
    to the shape of the distribution. With a skewness of 0, as shown in the upper
    center of [figure 6.4](#figure6.4), the distribution is symmetrical. Movement
    away from the mode in either direction causes the same drop-off in probability.
    This also makes the mean and mode of this distribution equal. Conversely, when
    skewness is negative, as seen in the lower left of [figure 6.4](#figure6.4), the
    drop-off in probability is extremely steep for values greater than the mode and
    more gradual for values less than the mode. This means that values less than the
    mode are more likely to be observed than values above the mode (but the mode is
    still the most likely value to be observed). Also, notice that with this skewness,
    the mean sits to the left of its original value. Instead of a mean of 0, as before,
    it’s now somewhere around –2.5\. Positive skewness, as seen in the lower right
    of [figure 6.4](#figure6.4), is the mirror opposite of negative skewness. Values
    greater than the mode are more likely to be observed than values less than the
    mode. The mean also sits to the right of 0\. Put in terms of analyzing vehicle
    ages, if we observed a negative skewness, it would indicate that more vehicles
    are newer than the average age. Conversely, a positive skewness would indicate
    that more vehicles are older than the average age. Typically, when skewness is
    greater than 1 or less than –1, we would determine that the distribution is substantially
    skewed and far from symmetrical.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 6.4](#figure6.4)中，你可以看到偏度差异对分布形状的影响。如图 6.4 上部中央所示，当偏度为 0 时，分布是对称的。从模式向任一方向的移动都会导致概率的相同下降。这也使得这个分布的均值和模式相等。相反，当偏度为负，如图
    6.4 下左所示，大于模式的值的概率下降非常陡峭，而小于模式的值的概率下降则更为平缓。这意味着小于模式的值比大于模式的值更有可能被观察到（但模式仍然是观察到的最可能值）。此外，请注意，在这种情况下，均值位于其原始值的左侧。与之前的均值为
    0 不同，现在它大约在 -2.5 左右。正偏度，如图 6.4 下右所示，是负偏度的镜像。大于模式的值比小于模式的值更有可能被观察到。均值也位于 0 的右侧。用分析车辆年龄的话来说，如果我们观察到负偏度，这表明比平均年龄更多的车辆是较新的。相反，正偏度表明比平均年龄更多的车辆是较老的。通常，当偏度大于
    1 或小于 -1 时，我们会确定分布是显著偏斜的，并且远离对称。
- en: '![c06_04.eps](Images/c06_04.png)'
  id: totrans-520
  prefs: []
  type: TYPE_IMG
  zh: '![c06_04.eps](Images/c06_04.png)'
- en: '[Figure 6.4](#figureanchor6.4) A visual comparison of skewness'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.4](#figureanchor6.4) 偏度视觉比较'
- en: 6.1.2 Calculating descriptive statistics with Dask
  id: totrans-522
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 使用 Dask 计算描述性统计
- en: Now that you have a good idea of how to understand and interpret these descriptive
    statistics, let’s have a look at how to calculate these values using Dask. To
    do this, we will first need to calculate the age of each vehicle when it was issued
    a citation. We have the citation date and the car’s model year in the data, so
    we will use that to create a derived column. As always, we’ll start with loading
    the data we produced in the previous chapter.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对如何理解和解释这些描述性统计有了很好的了解，让我们来看看如何使用 Dask 来计算这些值。为此，我们首先需要计算每辆车在收到传票时的年龄。数据中包含了传票日期和汽车的型号年份，因此我们将使用这些信息来创建一个派生列。和往常一样，我们将从加载上一章中产生的数据开始。
- en: Listing 6.1 Loading the data for analysis
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.1 加载分析数据
- en: '[PRE72]'
  id: totrans-525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Everything in [listing 6.1](#listing6.1) should look familiar; we’re simply
    importing the libraries we need and then reading the Parquet file we generated
    at the end of chapter 5\. Since we didn’t previously look at the Vehicle Year
    column to make sure there aren’t any weird values, let’s do that first.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 6.1](#listing6.1) 中的所有内容都应该看起来很熟悉；我们只是导入所需的库，然后读取在第 5 章末生成的 Parquet 文件。由于我们之前没有查看车辆年份列以确保没有奇怪的数据，让我们先做这件事。'
- en: Listing 6.2 Checking the Vehicle Year column for anomalies
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.2 检查车辆年份列的异常
- en: '[PRE73]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: As you can see in [listing 6.2](#listing6.2), our value counts shows some vehicles
    that were allegedly made in year 0 as well as long into the future. Barring any
    likelihood of time travel or other aberrations in the space-time continuum, these
    are likely bad data. We’ll filter them out to avoid introducing bad data into
    our statistical analysis.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 如[列表 6.2](#listing6.2)所示，我们的值计数显示了一些据说在公元 0 年制造的车辆以及远至未来的车辆。除非有时光旅行的可能性或其他时空连续体中的异常，否则这些很可能是错误数据。我们将过滤掉它们，以避免将这些错误数据引入我们的统计分析中。
- en: Listing 6.3 Filtering out the bad data
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.3 过滤掉错误数据
- en: '[PRE74]'
  id: totrans-531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: In [listing 6.3](#listing6.3), we use the same Boolean filtering you learned
    how to use in chapter 5 to filter out any vehicles made in the year 0 or beyond
    2018\. I’ve selected 2018 as my upper bound rather than 2017, because it’s common
    practice for auto manufacturers to be ahead one model year. Since this dataset
    spans 2017, it’s likely that most (if not all) of the observations for 2018 model
    year vehicles are legitimate. The output now looks much better!
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 6.3](#listing6.3) 中，我们使用你在第 5 章中学到的相同布尔过滤来过滤掉在年份 0 或 2018 年之后制造的任何车辆。我选择
    2018 年作为我的上限而不是 2017 年，因为汽车制造商通常领先一个车型年。由于这个数据集跨越 2017 年，因此很可能（如果不是所有的话）2018 款车型的观测值都是合法的。现在的输出看起来好多了！
- en: Now let’s create the derived column in the filtered data. To do that, we’ll
    apply a custom function that subtracts the Vehicle Year column from the date column
    to the filtered data, and then add the result to the DataFrame. We’ll perform
    that in the four steps listed in [figure 6.5](#figure6.5).
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在过滤后的数据中创建派生列。为此，我们将应用一个自定义函数，该函数从日期列中减去车辆年份列，并将结果添加到 DataFrame 中。我们将在
    [图 6.5](#figure6.5) 中列出的四个步骤中执行此操作。
- en: '![c06_05.eps](Images/c06_05.png)'
  id: totrans-534
  prefs: []
  type: TYPE_IMG
  zh: '![c06_05.eps](Images/c06_05.png)'
- en: '[Figure 6.5](#figureanchor6.5) Calculating the age of each vehicle at the time
    a citation was issued'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6.5](#figureanchor6.5) 在罚单发行时计算每辆车的年龄'
- en: Now we’ll implement those four steps in code.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将用代码实现这四个步骤。
- en: Listing 6.4 Calculating the vehicle age at the date of citation
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.4 在罚单日期计算车辆年龄
- en: '[PRE75]'
  id: totrans-538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[Listing 6.4](#listing6.4) should look very familiar as well. On the first
    line, we apply the filter condition to our data, thereby getting rid of observations
    with an invalid Vehicle Year. Next, we create our age calculation function. This
    function takes each DataFrame row as input, gets the year from the Issue Date
    column, and finds the difference between the year the ticket was issued and the
    year the car was built. Because `row[''Issue Date'']` represents a datetime object,
    we can access just its year value using its `year` attribute. The function is
    applied to each row of the DataFrame in the third line, which returns a Series
    containing the age of each vehicle. As a reminder, the `meta` parameter in the
    `apply` method takes a tuple with a name for the new Series as the first element
    and the datatype as the second element. The next two lines use the assign-rename
    pattern you learned in chapter 5 to add a column to a DataFrame and rename it
    to a friendly name. On the last line, we apply one more filter to get rid of any
    rows that result in an invalid age calculation. For example, if the citation was
    written in 2014 and the vehicle year was recorded as 2018, that would result in
    an invalid vehicle age of –4.'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 6.4](#listing6.4) 也应该看起来非常熟悉。在第一行，我们将过滤条件应用于我们的数据，从而去除具有无效车辆年份的观测值。接下来，我们创建年龄计算函数。这个函数将每个
    DataFrame 行作为输入，从“发行日期”列中获取年份，并找出罚单发行年份与车辆制造年份之间的差异。因为 `row[''Issue Date'']` 代表一个
    datetime 对象，我们可以通过其 `year` 属性来访问其年份值。该函数在第三行应用于 DataFrame 的每一行，返回包含每辆车年龄的 Series。提醒一下，`apply`
    方法中的 `meta` 参数接受一个元组，其中第一个元素是新的 Series 的名称，第二个元素是数据类型。接下来的两行使用你在第 5 章中学到的 assign-rename
    模式向 DataFrame 添加一个列并将其重命名为友好的名称。在最后一行，我们应用另一个过滤条件以去除任何导致无效年龄计算的行。例如，如果罚单是在 2014
    年撰写的，而车辆年份被记录为 2018 年，这将导致无效的车辆年龄为 -4。'
- en: We’re now ready to calculate the descriptive statistics! However, we should
    address one thing before we run the calculations. Each of these calculations (such
    as mean and standard deviation) requires fully scanning over the entire dataset,
    so they can take a long time to complete. For example, the mean requires summing
    all values in the DataFrame, then dividing the sum by the number of rows in the
    DataFrame. The calculation to produce the vehicle’s age is also reasonably complex
    because of the object manipulation we need to do with the datetime column (datetime
    operations are typically slow). This would be a good opportunity to use the `persist`
    method to hold the results of this expensive computation in memory. However, we
    will instead save the intermediate result as a Parquet file, because we will use
    this data again in a later chapter. By saving the data to disk, you can come back
    to the data later without needing to recalculate it, and you won’t need to keep
    your Jupyter notebook server up indefinitely until you need the data again. As
    a brief reminder, the two parameters we need to pass to the `to_parquet` method
    are the filename and the Parquet library we want to use to write the data. As
    with other examples, we’ll stick to using PyArrow.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好计算描述性统计了！然而，在运行计算之前，我们应该解决一个问题。这些计算（如平均值和标准差）都需要完全扫描整个数据集，因此它们可能需要很长时间才能完成。例如，计算平均值需要将
    DataFrame 中的所有值相加，然后将总和除以 DataFrame 中的行数。由于需要对日期时间列（通常日期时间操作较慢）进行对象操作，计算车辆的年龄也是相当复杂的。这是一个使用
    `persist` 方法保存昂贵计算结果的内存中的好机会。然而，我们将把中间结果保存为 Parquet 文件，因为我们在后面的章节中还会使用这些数据。通过将数据保存到磁盘，你可以在以后再次访问数据时无需重新计算，而且你不需要无限期地保持
    Jupyter notebook 服务器运行，直到再次需要数据。作为一个简短的提醒，我们需要传递给 `to_parquet` 方法的两个参数是文件名和我们要用来写入数据的
    Parquet 库。与其他示例一样，我们将坚持使用 PyArrow。
- en: Listing 6.5 Saving the intermediate results to Parquet
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.5 将中间结果保存到 Parquet
- en: '[PRE76]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Once these two lines finish executing (which took about 45 minutes on my system),
    we’ll be in good shape to calculate the descriptive statistics more quickly and
    efficiently. For convenience, Dask provides built-in descriptive statistics functions
    so you don’t have to write your own algorithms. We’ll take a look at the five
    descriptive statistics we covered earlier in the section: mean, standard deviation,
    minimum, maximum, and skewness.'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这两行代码执行完成（在我的系统上大约花费了45分钟），我们就能更快速、更高效地计算描述性统计。为了方便起见，Dask 提供了内置的描述性统计函数，这样你就不必编写自己的算法。我们将回顾本节中提到的五种描述性统计：平均值、标准差、最小值、最大值和偏度。
- en: Listing 6.6 Calculating descriptive statistics
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.6 计算描述性统计
- en: '[PRE77]'
  id: totrans-545
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: As you can see in [listing 6.6](#listing6.6), for the mean, standard deviation,
    minimum, and maximum, you can simply call them as built-in methods of the Vehicle
    Age series. The exception to the set is calculating the skewness. There is no
    `skew` method as you might expect. However, Dask contains a myriad of statistical
    tests in the `dask.array` package, which we haven’t yet explored (chapter 9 is
    a deep dive into Dask Array functions). To calculate the skewness for this example,
    we must convert Vehicle Age from a Dask Series object to a Dask Array object,
    since the skew function from `dask.array` requires a Dask Array as input. To do
    this, we can simply use the `values` attribute of the Vehicle Age series. We can
    then feed that into the `skew` function to calculate the skewness. Inspecting
    the results of our calculations, we find the values listed in [table 6.1](#table6.1).
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在[列表 6.6](#listing6.6)中看到的，对于平均值、标准差、最小值和最大值，你可以简单地作为车辆年龄序列的内置方法来调用它们。这个集合的例外是计算偏度。正如你可能期望的那样，没有
    `skew` 方法。然而，Dask 在 `dask.array` 包中包含了许多统计测试，我们还没有探讨过（第 9 章将深入探讨 Dask Array 函数）。为了计算这个示例的偏度，我们必须将车辆年龄从
    Dask 序列对象转换为 Dask 数组对象，因为 `dask.array` 中的偏度函数需要一个 Dask 数组作为输入。为此，我们可以简单地使用车辆年龄序列的
    `values` 属性。然后我们可以将其输入到 `skew` 函数中，以计算偏度。检查我们计算的结果，我们发现列出的值在[表 6.1](#table6.1)中。
- en: Table 6.1 Descriptive statistics of the Vehicle Age column
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.1 车辆年龄列的描述性统计
- en: '| **Statistic** | **Vehicle age** |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| **统计量** | **车辆年龄** |'
- en: '| --- | --- |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Mean | 6.74 |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 6.74 |'
- en: '| Standard deviation | 5.66 |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| 标准差 | 5.66 |'
- en: '| Minimum | 0 |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| 最小值 | 0 |'
- en: '| Maximum | 47 |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| 最大值 | 47 |'
- en: '| Skewness | 1.01 |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| 偏度 | 1.01 |'
- en: How interesting! Ticketed vehicles, on average, are about seven years old. There
    are some brand-new vehicles (denoted by a minimum age of 0 years), and the oldest
    vehicle was 47 years old. The standard deviation of 5.66 indicates that, on average,
    vehicles in this dataset tend to be +/– 5.66 years from the average age of 6.74
    years. Finally, the data has a positive skew, meaning vehicles that were newer
    that 6.74 years were more common than vehicles that were older than 6.74 years.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 多么有趣！平均而言，被罚款的车辆大约是七年车龄。有一些全新的车辆（用0年的最小年龄表示），最老的车辆是47岁。标准差为5.66表明，平均而言，这个数据集中的车辆年龄与6.74年的平均年龄相差+/-
    5.66年。最后，数据呈正偏态，这意味着6.74年以下的车辆比6.74年以上的车辆更常见。
- en: Given some basic intuition about cars, all these numbers should make sense.
    When you consider that many vehicles older than 12 years are starting to face
    the end of their reliable lifespan, it’s expected that there would be a significant
    drop-off in the number of vehicles on the road that are older than this as they
    become more likely to break down and get junked. Given that new vehicles are expensive
    and depreciate rapidly for the first several years of their lifespan, it’s more
    economical to purchase lightly used vehicles that are three to five years old.
    This helps explain the average vehicle age of 6.74 years, as buyers would face
    the least-severe depreciation by purchasing vehicles of this age, but still have
    a reliable vehicle for the next five or more years. Likewise, although some vehicles
    were observed that are extremely old, we can see that the maximum age is many
    times the standard deviation away from the mean, indicating that seeing a 47-year-old
    vehicle on the streets of New York City is exceedingly rare.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 基于对汽车的一些基本直觉，所有这些数字都应该是有意义的。当你考虑到许多超过12年的车辆开始面临其可靠寿命的终结时，预计随着这些车辆更有可能发生故障和报废，道路上超过这个年龄的车辆数量将会有显著下降。鉴于新车价格昂贵且在其寿命的前几年内迅速贬值，购买三到五年车龄的二手车辆在经济上更为合理。这有助于解释平均车辆年龄为6.74年的现象，因为买家通过购买这个年龄段的车辆，可以面临最轻微的折旧，同时还能拥有一个可靠车辆长达五年或更长时间。同样，尽管观察到一些极老的车辆，但我们可以看出最大年龄远远高于平均值的标准差，这表明在纽约市街道上看到一辆47岁的车辆是非常罕见的。
- en: 6.1.3 Using the describe method for descriptive statistics
  id: totrans-557
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 使用describe方法进行描述性统计
- en: Dask also gives you another shortcut for calculating descriptive statistics
    if you don’t want to write out the code for each statistic.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不想为每个统计量编写代码，Dask还为你提供了一个计算描述性统计的快捷方式。
- en: Listing 6.7 Calculating a bunch of descriptive statistics
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.7 计算一系列描述性统计量
- en: '[PRE78]'
  id: totrans-560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: In [listing 6.7](#listing6.7), you can see that the `describe` method produces
    the following Series containing a variety of common descriptive statistics. You
    get the count of non-null values, as well as the mean, standard deviation, minimum,
    and maximum. You also get the 25th percentile, 75th percentile, and median, which
    are also useful for understanding the spread of the data. One advantage of using
    the `describe` method is that it’s actually more efficient than making four separate
    `compute` calls to get the mean, standard deviation, and so on. This is because
    Dask can apply a limited amount of code optimization when you request multiple
    aggregate functions all in one go.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表6.7](#listing6.7)中，你可以看到`describe`方法生成了一个包含各种常见描述性统计量的Series。你得到了非空值的计数，以及平均值、标准差、最小值和最大值。你还得到了第25百分位数、第75百分位数和中位数，这些对于理解数据的分布也很有用。使用`describe`方法的一个优点是，它实际上比分别调用四个`compute`方法来获取平均值、标准差等更有效率。这是因为当你一次性请求多个聚合函数时，Dask可以应用一定程度的代码优化。
- en: 'Now that you’ve learned how to produce descriptive statistics, you can use
    these methods for understanding numeric variables in any dataset. Being able to
    quantify and describe the random behavior of variables is a good start, but another
    important angle of exploratory data analysis is understanding if any of that perceived
    randomness can actually be explained. To do this, we’ll need to look at the relationships
    between variables in our dataset. This is our second goal of exploratory data
    analysis: finding interesting patterns and correlations. We’ll head there next.'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经学会了如何生成描述性统计，你可以使用这些方法来理解任何数据集中的数值变量。能够量化并描述变量的随机行为是一个良好的开端，但探索性数据分析的另一个重要角度是理解那些感知到的随机性是否可以真正解释。为此，我们需要查看我们数据集中变量之间的关系。这是我们探索性数据分析的第二个目标：寻找有趣的模式和相关性。我们将在下一部分进行探讨。
- en: 6.2 Built-In aggregate functions
  id: totrans-563
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 内置聚合函数
- en: 'You may recall from the chapter 5 that we joined some temperature data to the
    NYC Parking Violation dataset, and to do that, we created a column that holds
    the month and year in which each citation was issued. Let’s use that data to answer
    the following questions:'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得在第5章中，我们将一些温度数据与纽约市停车违章数据集相结合，为此，我们创建了一个包含每个传票签发月份和年份的列。让我们使用这些数据来回答以下问题：
- en: Using the NYC Parking Ticket data, how many parking citations were issued each
    month? Does the average temperature correlate with the number of citations issued?
  id: totrans-565
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用纽约市停车违章数据，每个月签发了多少停车传票？平均温度与签发传票的数量是否相关？
- en: 6.2.1 What is correlation?
  id: totrans-566
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 什么是相关性？
- en: 'When we talk about patterns and relationships in data, we’re usually talking
    more specifically about the *correlation* of two variables. Correlation quantifies
    how variables move with respect to one another. It can help us answer questions
    like, “Are the number of issued citations higher when the weather is warmer and
    lower when it’s colder outside?” That might be interesting to know: perhaps the
    NYC Parking Authority doesn’t put as many officers out on the beat in inclement
    weather. Correlation will tell us about both the strength and direction of the
    relationship between temperature and number of tickets issued.'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论数据中的模式和关系时，我们通常更具体地谈论两个变量的**相关性**。相关性量化了变量相对于彼此的移动情况。它可以帮助我们回答像“当天气变暖时，签发的传票数量是否更高，而在天气变冷时是否更低？”这样的问题。这可能是有趣的：也许纽约市停车管理局在恶劣天气时不会派出那么多官员巡逻。相关性将告诉我们温度和签发传票数量之间关系的强度和方向。
- en: '![c06_06.eps](Images/c06_06.png)'
  id: totrans-568
  prefs: []
  type: TYPE_IMG
  zh: '![c06_06.eps](Images/c06_06.png)'
- en: '[Figure 6.6](#figureanchor6.6) A visual guide to correlation'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.6](#figureanchor6.6) 相关性的视觉指南'
- en: '[Figure 6.6](#figure6.6) shows what we mean by strength and direction of the
    relationship. Scatterplot A demonstrates positive correlation: as the variable
    on the X axis increases (moves to the right), the variable on the Y axis also
    tends to increase (moves up). This is a positive correlation—as one variable increases,
    the other variable also increases. This is also a *strong* correlation because
    the points are all relatively close to the red line. A very definitive pattern
    is easy to spot. Scatterplot B shows uncorrelated variables. As the X variable
    increases, the value of Y sometimes increases, sometimes decreases, and sometimes
    stays the same. We find no discernable pattern here, so this data is uncorrelated.
    Finally, scatterplot C shows a strong negative correlation. As the X variable
    increases, the Y variable decreases. Put in terms of the correlation between citations
    and temperature that we want to investigate, if the two are positively correlated,
    that would mean we would typically observe more citations issued in warmer months
    and fewer citations issued in colder months. If the two were negatively correlated,
    we would observe the opposite: more citations in colder months and fewer citations
    in warmer months. And if the two were uncorrelated, then we wouldn’t see any discernable
    pattern. We would sometimes see a large number of citations in warm months and
    other times see very few citations issued when it’s hot outside. We would also
    sometimes see a large number of citations issued in cold months and other times
    see very few citations issued in cold months.'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6.6](#figure6.6) 展示了我们所说的关系的强度和方向。散点图 A 展示了正相关：当 X 轴上的变量增加（向右移动）时，Y 轴上的变量也倾向于增加（向上移动）。这是一个正相关——当一个变量增加时，另一个变量也增加。这也是一个*强*相关，因为点都相对接近红线。一个非常明确的模式很容易被发现。散点图
    B 展示了不相关的变量。当 X 变量增加时，Y 的值有时增加，有时减少，有时保持不变。在这里我们没有发现可辨别的模式，因此这些数据是不相关的。最后，散点图
    C 展示了强烈的负相关。当 X 变量增加时，Y 变量减少。用我们想要研究的引用与温度之间的相关性来表述，如果两者呈正相关，这意味着我们通常会在较暖的月份观察到更多的引用发布，而在较冷的月份发布的引用较少。如果两者呈负相关，我们会观察到相反的情况：在较冷的月份发布更多引用，而在较暖的月份发布较少引用。如果两者不相关，那么我们不会看到任何可辨别的模式。有时我们会看到在温暖的月份有大量的引用，而有时在外界很热的时候看到很少的引用发布。我们也会有时看到在寒冷的月份有大量的引用发布，而有时在寒冷的月份发布的引用很少。'
- en: 6.2.2 Calculating correlations with Dask DataFrames
  id: totrans-571
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 使用 Dask DataFrames 计算相关性
- en: Now let’s have a look at how to perform these calculations in Dask. As mentioned
    earlier, we’ll first need to calculate how many citations were issued per month.
    Before we do that, we’ll create a custom sorting function to help display the
    results in chronological order. Because the month-year column we created in chapter
    5 are strings, simply sorting by that column won’t return the results in chronological
    order. To fix that issue, our custom sort function will map a sequential integer
    to each month in the correct order and sort the data by the numeric column before
    dropping it.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看如何在 Dask 中执行这些计算。如前所述，我们首先需要计算每个月份的引用数量。在这样做之前，我们将创建一个自定义排序函数来帮助按时间顺序显示结果。因为我们创建的第
    5 章中的月份-年份列是字符串，仅按该列排序不会按时间顺序返回结果。为了解决这个问题，我们的自定义排序函数将按正确的顺序将一个连续的整数映射到每个月份，并在删除之前按数字列对数据进行排序。
- en: Listing 6.8 Custom sorting for the month-year column
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.8 对月份-年份列的定制排序
- en: '[PRE79]'
  id: totrans-574
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[Listing 6.8](#listing6.8) has quite a bit going on in it. Let’s unpack it
    line by line. First, we’re going to build a list of month-year values for all
    months in 2014 through 2017\. To do this, we’ve constructed two lists, one containing
    the months and one containing the years. In line 5, we use a list comprehension
    to calculate the Cartesian product of the `months` list and the `years` list.
    This will create every possible combination of months and years. Because of the
    way we set up the list comprehension, the list of month-year values will be in
    the correct chronological order. Next, we turn the list into a Pandas Series.
    We use the month-year values as the index so we can join it to other DataFrames
    that have the same index and create a sequential integer value using the `range`
    function so we can sort the joined data correctly. Finally, we define a quick
    function called `sortByMonths`, which will take any index-aligned DataFrame as
    input, join our sorting map to it, sort chronologically using the integer value
    we’ve mapped to each month-year, and drop the numeric column. This will result
    in a DataFrame sorted chronologically by month-year.'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 6.8](#listing6.8) 中包含了很多内容。让我们逐行分析它。首先，我们将为 2014 年至 2017 年的所有月份构建一个月份-年份值列表。为此，我们构建了两个列表，一个包含月份，另一个包含年份。在第
    5 行，我们使用列表推导来计算 `months` 列表和 `years` 列表的笛卡尔积。这将创建月份和年份的每一种可能的组合。由于我们设置列表推导的方式，月份-年份值的列表将按照正确的时序顺序排列。接下来，我们将列表转换为
    Pandas Series。我们使用月份-年份值作为索引，以便将其与其他具有相同索引的 DataFrames 连接，并使用 `range` 函数创建一个连续的整数值，以便正确排序连接的数据。最后，我们定义了一个名为
    `sortByMonths` 的快速函数，它将接受任何索引对齐的 DataFrame 作为输入，将其排序映射连接到它，使用我们为每个月份-年份映射的整数值按时间顺序排序，并删除数字列。这将导致一个按月份-年份时间顺序排序的
    DataFrame。'
- en: Using aggregate functions in Dask
  id: totrans-576
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 Dask 中使用聚合函数
- en: 'Now that we have our sorting logic fleshed out, let’s look at how to count
    the number of citations by month and year. To do this, we’ll use an *aggregate
    function*. Aggregate functions, as you may expect given the name, combine (or
    aggregate) raw data into some kind of grouping and apply a function over that
    group. You may already be familiar with aggregate functions if you’ve worked with
    `GROUP BY` statements in SQL. Many of the same functions are available in Dask:
    counting, summing, finding the minimum/maximum, and so on, by group. In fact,
    the operations we used in the previous section for descriptive statistics (`min`,
    `max`, `mean`, and so on) are technically aggregate functions! The only difference
    is that we applied those functions over the entire, ungrouped dataset. For example,
    we looked at the average vehicle age overall—but we could have looked at the average
    vehicle age *by vehicle type* or *by license plate state*. Similarly, we can use
    the `count` function to count all the citations issued in the whole dataset, or
    we could count by some sort of grouping like month-year. Unsurprisingly, the function
    to define the groupings for aggregate functions is `groupby`. From a code perspective,
    it’s quite simple and concise to use, but it’s important to understand what goes
    on behind the scenes. What happens when you call an aggregate function with a
    defined grouping is an algorithm known as split-apply-combine. Let’s take a look
    at how this works.'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经详细阐述了排序逻辑，让我们看看如何按月份和年份计算引用次数。为此，我们将使用一个聚合函数。根据其名称，聚合函数会将原始数据组合（或聚合）成某种分组，并对该组应用一个函数。如果您在
    SQL 中使用过 `GROUP BY` 语句，您可能已经熟悉了聚合函数。Dask 中提供了许多相同的函数：计数、求和、查找最小/最大值等，按组进行。实际上，我们在上一节中用于描述性统计的操作（`min`、`max`、`mean`
    等）在技术上也是聚合函数！唯一的区别是我们将这些函数应用于整个未分组的数据集。例如，我们查看整体车辆的平均年龄——但我们也可以按车辆类型或车牌州查看平均车辆年龄。同样，我们可以使用
    `count` 函数来计算整个数据集中发出的所有引用次数，或者我们可以按某种分组（如月份-年份）进行计数。不出所料，定义聚合函数分组的函数是 `groupby`。从代码的角度来看，它的使用非常简单和简洁，但了解幕后发生的事情很重要。当您使用定义的分组调用聚合函数时，会发生一个称为拆分-应用-组合的算法。让我们看看它是如何工作的。
- en: '![c06_07.eps](Images/c06_07.png)'
  id: totrans-578
  prefs: []
  type: TYPE_IMG
  zh: '![c06_07.eps](Images/c06_07.png)'
- en: '[Figure 6.7](#figureanchor6.7) An example of the split-apply-combine algorithm
    of an aggregate function'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6.7](#figureanchor6.7) 聚合函数的拆分-应用-组合算法示例'
- en: For the sake of a simple example, we have four rows of data in the table at
    the top of [figure 6.7](#figure6.7) showing a list of pets and their owners’ IDs.
    If we wanted to count the number of pets owned by each owner, we would group by
    the Owner ID and apply the `count` function to each group. What happens in the
    background is the original table is *split* into partitions, where each partition
    contains only pets owned by a single owner. The original table is split into three
    partitions. Next, we *apply* the aggregate function to each partition. Count will
    simply find how many rows are in each partition. We’re left with a count of 1
    for the left partition, a count of 2 for the center partition, and a count of
    1 for the right partition. To reassemble the result, we’ll need to *combine* the
    results from each partition. Here, the result from each partition is simply concatenated
    to produce the final output.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们在[图 6.7](#figure6.7)顶部表格中有四行数据，展示了宠物及其所有者 ID 的列表。如果我们想统计每个所有者拥有的宠物数量，我们将按所有者
    ID 进行分组，并对每个组应用`count`函数。在后台发生的情况是原始表被*分割*成分区，其中每个分区只包含单个所有者拥有的宠物。原始表被分割成三个分区。接下来，我们对每个分区应用聚合函数。`count`函数将简单地找出每个分区中有多少行。我们得到了左分区1个计数，中间分区2个计数，右分区1个计数。为了重新组装结果，我们需要将每个分区的结果*合并*起来。在这里，每个分区的结果简单地连接起来生成最终输出。
- en: Based on what you’ve learned so far about shuffle performance and partitioning,
    you might be wondering how efficient these split-apply-combine operations can
    be. Since we have to split the data into unique partitions over the grouping column,
    you’d be correct to be concerned that this operation will cause a lot of shuffling
    if you don’t choose a group by column that’s also used as a partitioning column.
    These operations can be slightly more efficient if you’re working on data stored
    in compressed Parquet format, but ultimately, it’s best practice to only group
    by the column used to partition your data. Fortunately, we’ve saved our prepared
    NYC Parking Ticket data partitioned by month-year, so using that column to group
    on should be quite fast!
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你到目前为止关于洗牌性能和分区所学的知识，你可能想知道这些分割-应用-合并操作可以有多高效。由于我们必须将数据分割成具有唯一分区的分组列，如果你没有选择也用作分区列的分组列，那么你正确地担心这个操作将导致大量的洗牌。如果你在存储为压缩
    Parquet 格式的数据上工作，这些操作可以稍微高效一些，但最终，最佳实践是只按用于分区数据的列进行分组。幸运的是，我们已经保存了按月-年分区的准备好的纽约市停车罚单数据，所以使用该列进行分组应该相当快！
- en: Listing 6.9 Counting citations by month-year
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.9 按月-年统计引用次数
- en: '[PRE80]'
  id: totrans-583
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: In [listing 6.9](#listing6.9), we use the `groupby` method to define the column
    we want to group the data on. Next, we choose one column to apply our `count`
    function to and compute it. With the count function, it typically doesn’t matter
    which column you specify. Just be aware that the Dask `count` function will count
    only non-null values, so if you apply count to a column that has null values,
    you will not get a true row count. If you don’t specify a column, you’ll get a
    DataFrame that has a count for each column, which is not really what we want here.
    Finally, we’ll take the `citationsPerMonth` result, which is a Pandas Series,
    convert it to a DataFrame using the `to_frame` method, and apply our custom sort
    function to it. An abbreviated output can be seen in [figure 6.8](#figure6.8).
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 6.9](#listing6.9)中，我们使用`groupby`方法定义我们想要按其分组数据的列。接下来，我们选择一个列应用我们的`count`函数并计算它。使用`count`函数时，通常不需要指定哪个列。只需注意，Dask
    的`count`函数将只计算非空值，所以如果你对一个包含空值的列应用计数，你将不会得到真正的行数。如果你没有指定列，你将得到一个对每个列都有计数的 DataFrame，这并不是我们想要的。最后，我们将`citationsPerMonth`结果（一个
    Pandas Series）转换为 DataFrame，使用`to_frame`方法，并对其应用我们的自定义排序函数。简化的输出可以在[图 6.8](#figure6.8)中看到。
- en: '![c06_08.png](Images/c06_08.png)'
  id: totrans-585
  prefs: []
  type: TYPE_IMG
  zh: '![c06_08.png](Images/c06_08.png)'
- en: '[Figure 6.8](#figureanchor6.8) An abbreviated count of citations by month-year'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6.8](#figureanchor6.8) 按月-年引用的简略计数'
- en: If you run the code and look at the full output, you’ll notice that the count
    of citations past June 2017 is far lower than previous months. At the time of
    writing, this dataset wasn’t complete for 2017\. In the following code we will
    filter it out from our correlation calculation; otherwise, it might negatively
    influence our results. We’ll also need to get the average monthly temperature
    back in our resulting DataFrame, since we want to compare the count of citations
    to the average temperature.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行代码并查看完整输出，你会注意到2017年6月之后的引用次数远低于之前的月份。在撰写本文时，这个数据集2017年的数据并不完整。在下面的代码中，我们将从我们的相关性计算中过滤掉它；否则，它可能会对我们的结果产生负面影响。我们还需要将平均月温度返回到我们的结果DataFrame中，因为我们想将引用次数与平均温度进行比较。
- en: Listing 6.10 Calculating the correlation between citations and temperature
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.10 计算引用次数与温度的相关性
- en: '[PRE81]'
  id: totrans-589
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[Listing 6.10](#listing6.10) shows how to calculate the correlation between
    the temperature and count of citations. First, we build the filter condition to
    get rid of the months with missing data. To do that, we pass a list of the months
    we don’t want to the `isin` method. This Boolean expression would normally filter
    the data so we *only* get the rows back that are contained in the `isin` list.
    However, since we prefaced the expression with the negation operator (`~`), this
    filter will return all months that are not contained in the `isin` list. After
    building the expression, we apply it to the data in the same way you’ve seen many
    times now.'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表6.10](#listing6.10) 展示了如何计算温度和引用次数之间的相关性。首先，我们构建过滤条件以去除缺失数据的月份。为此，我们将我们不想要的月份列表传递给`isin`方法。这个布尔表达式通常会过滤数据，使我们*仅*获取包含在`isin`列表中的行。然而，由于我们在表达式中加上了否定运算符（`~`），这个过滤器将返回所有不包含在`isin`列表中的月份。在构建表达式后，我们以同样的方式将其应用于数据，就像你现在已经看到很多次的那样。'
- en: On line 3, we group up the data by `monthYear` as before, but this time we’ll
    use the `agg` method on the grouped data. The `agg` method allows you to apply
    more than one aggregate operation at once to the same data grouping. To use it,
    you simply pass in a dictionary containing keys that equate to column names and
    values that equate to the name of an aggregate function. Here, we’re applying
    the `count` function to the `Summons Number` column and the `mean` function to
    the `Temp` column.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3行，我们按照之前的做法，通过`monthYear`对数据进行分组，但这次我们将对分组后的数据使用`agg`方法。`agg`方法允许你一次性对相同的数据分组应用多个聚合操作。要使用它，你只需传入一个包含键对应列名和值对应聚合函数名称的字典。在这里，我们正在将`count`函数应用于`Summons
    Number`列，将`mean`函数应用于`Temp`列。
- en: You may be wondering why we apply the `mean` function to the `Temp` column when
    the `Temp` column already contains the average temperature for the month. This
    is because we stamped the temperature on each raw record, but in our result we
    want only one temperature value per month. Since the mean of a series of constant
    numbers is just the constant number, we use `mean` simply to pass through the
    average temperature for the month to the result.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道为什么我们在`Temp`列上应用`mean`函数，而`Temp`列已经包含了该月的平均温度。这是因为我们在每个原始记录上标记了温度，但在我们的结果中，我们只想为每个月提供一个温度值。由于一系列常数数的平均值只是常数本身，我们使用`mean`只是简单地通过平均温度传递给结果。
- en: Finally, we use the `corr` method to calculate the correlation between the variables.
    The output of the correlation matrix can be seen in [figure 6.9](#figure6.9).
    It shows us that the correlation between Count of Summons and Temp is 0.14051\.
    This indicates a positive correlation because the correlation coefficient is positive,
    and it is a *weak* correlation because the correlation coefficient is less than
    0.5\. We can interpret this to mean that in months where the temperature is warmer
    on average, more citations are typically issued than in months where the temperature
    is colder on average. However, a weak correlation indicates that a large amount
    of variation still isn’t neatly explained by these two variables. Put another
    way, if we observed two different months that had the exact same average temperature,
    they would likely still have very different counts of citations issued. This means
    that there could be other variables in the dataset that we can use to help further
    explain some of this variation.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用 `corr` 方法计算变量之间的相关性。相关矩阵的输出可以在[图 6.9](#figure6.9)中看到。它显示计数和温度之间的相关性为
    0.14051。这表明存在正相关，因为相关系数是正的，而且它是一个 *弱* 相关，因为相关系数小于 0.5。我们可以解释这意味着在平均温度较高的月份，通常比平均温度较低的月份发出更多的罚单。然而，弱相关性表明，大量的变化仍然不能被这两个变量很好地解释。换句话说，如果我们观察到两个平均温度完全相同的不同月份，它们很可能仍然会有非常不同的罚单数量。这意味着数据集中可能还有其他变量，我们可以使用这些变量来进一步解释一些这种变化。
- en: '![c06_09.png](Images/c06_09.png)'
  id: totrans-594
  prefs: []
  type: TYPE_IMG
  zh: '![c06_09.png](Images/c06_09.png)'
- en: '[Figure 6.9](#figureanchor6.9) The correlation matrix for Count of Summons
    and Temp'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6.9](#figureanchor6.9) 计数和温度的相关矩阵'
- en: 6.3 Custom aggregate functions
  id: totrans-596
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 自定义聚合函数
- en: 'While correlation is useful for understanding the relationship between two
    continuous numeric variables, you’re likely to also encounter categorical variables
    that you want to analyze. For example, in section 1, we looked at the average
    age of the vehicle when a citation was issued and found that the average vehicle
    was 6.74 years old. But are all kinds of vehicles the same? Let’s add another
    dimension to this analysis by answering the following question:'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然相关性有助于理解两个连续数值变量之间的关系，但你可能还会遇到你想要分析的分类变量。例如，在第1节中，我们研究了发出罚单时车辆的平均年龄，发现平均车辆年龄为6.74年。但所有类型的车辆都一样吗？让我们通过回答以下问题来为这项分析增加另一个维度：
- en: Given the NYC Parking Ticket data, is the average age of privately owned vehicles
    the same as it is for commercially owned vehicles?
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 在纽约市停车罚单数据中，私人拥有车辆的平均年龄是否与商业拥有车辆的相同？
- en: 6.3.1 Testing categorical variables with the t-test
  id: totrans-599
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 使用 t 检验测试分类变量
- en: 'To answer this question, we’ll be looking at two different variables in the
    dataset: average vehicle age and vehicle type. Even though we’re interested in
    how the average age moves as we change our focus from one type of vehicle to another,
    correlation is not appropriate here. Correlation can only be used to describe
    how two continuous variables “move” with respect to one another. Vehicle type
    is not a continuous variable—it’s either PAS for a privately owned passenger vehicle
    or COM for a commercially owned vehicle. It would be strange to say that the average
    age increases or decreases as the vehicle type increases or decreases. We might
    simply answer this question by grouping the data by vehicle type and calculating
    the means, but that poses its own issue: If the means are different, how can you
    be sure that the difference isn’t simply due to random chance? We can turn to
    a different statistical test, known as the two-sample t-test, to help answer this
    question.'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，我们将查看数据集中的两个不同变量：平均车辆年龄和车辆类型。尽管我们对如何随着关注点的改变从一种车型转移到另一种车型时平均年龄的变化感兴趣，但相关性在这里并不适用。相关性只能用来描述两个连续变量相对于彼此“移动”的方式。车辆类型不是一个连续变量——它要么是
    PAS（私人拥有乘客车辆），要么是 COM（商业拥有车辆）。说随着车辆类型的增加或减少，平均年龄也会增加或减少是很奇怪的。我们可能简单地通过按车辆类型分组数据并计算平均值来回答这个问题，但这会带来它自己的问题：如果平均值不同，你怎么能确定这种差异不是仅仅由于随机机会造成的呢？我们可以转向另一种统计检验，称为双样本
    t 检验，以帮助回答这个问题。
- en: Statistical hypothesis testing 101
  id: totrans-601
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 统计假设检验 101
- en: The two-sample t-test is in a family of statistical tests known as *statistical
    hypothesis tests*. Statistical hypothesis tests help answer predefined hypotheses
    about certain aspects of data. In every statistical hypothesis test, we start
    by making a declaration about the data. That declaration, known as the *null hypothesis*,
    is accepted to be a true statement by default. The test attempts to provide sufficient
    evidence to the contrary. If the evidence is convincing enough, you may reject
    the null hypothesis as a true statement. This is measured by a probability that
    the significance of the evidence is due to random chance. If this probability,
    called the *p-value*, is sufficiently low enough, the evidence against the null
    hypothesis is strong enough that it can be rejected. [Figure 6.10](#figure6.10)
    shows a flowchart representing the decision processes around hypothesis testing.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 双样本t检验属于统计假设检验这一类统计检验。统计假设检验有助于回答关于数据某些方面的预定义假设。在每一次统计假设检验中，我们首先对数据进行一个声明。这个声明，被称为**零假设**，默认被认为是真实陈述。检验试图提供足够的证据来反驳这一点。如果证据足够有说服力，你可能会拒绝零假设作为真实陈述。这通过一个概率来衡量，即证据的重要性是否由随机机会引起。如果这个概率，称为**p值**，足够低，那么反对零假设的证据就足够强，可以予以拒绝。[图6.10](#figure6.10)展示了一个表示假设检验决策过程的流程图。
- en: '![c06_10.eps](Images/c06_10.png)'
  id: totrans-603
  prefs: []
  type: TYPE_IMG
  zh: '![c06_10.eps](Images/c06_10.png)'
- en: '[Figure 6.10](#figureanchor6.10) The process of statistical hypothesis tests'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.10](#figureanchor6.10) 统计假设检验的过程'
- en: The null hypothesis of the two-sample t-test is that “no difference in means
    exists between the two categories.” The test determines if convincing-enough evidence
    exists to reject that statement. If we find convincing-enough evidence to reject
    the null hypothesis, we can confidently say that it’s likely that there is a difference
    in the average vehicle age based on its type. Otherwise, we can say that the two
    types of vehicles do not truly have a different average age.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 双样本t检验的零假设是“两个类别之间不存在均值差异。”该检验旨在确定是否存在足够的证据来拒绝这一陈述。如果我们找到足够的证据来拒绝零假设，我们就可以自信地说，基于车辆类型，平均车辆年龄很可能存在差异。否则，我们可以认为这两种类型的车辆在平均年龄上并没有真正的差异。
- en: Assumptions of statistical hypothesis tests
  id: totrans-606
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 统计假设检验的假设
- en: As with many statistical hypothesis tests, two-sample t-tests typically make
    a few assumptions about the underlying data we will be testing. These assumptions
    depend on which two-sample t-test we will use. The two most common kinds of two-sample
    t-tests are Student’s T-Test and Welch’s T-Test, named respectively for the statisticians
    who developed each method (although “Student” was actually a pseudonym adopted
    by statistician and Guinness Brewery employee William Sealy Gosset).
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多统计假设检验一样，双样本t检验通常对我们将要测试的潜在数据做出一些假设。这些假设取决于我们将使用哪种双样本t检验。两种最常见的双样本t检验是Student的t检验和Welch的t检验，分别以开发每种方法的统计学家命名（尽管“Student”实际上是统计学家、Guinness酿酒厂员工威廉·塞利·高塞特采用的化名）。
- en: 'An important assumption of Student’s T-Test is that the variance of each group
    being tested is equal. Variance, like standard deviation, has to do with how spread-out
    observations are from the mean. Higher variance means the typical observation
    tends to lie far away from the mean, and a small variance means the typical observations
    tends to lie close to the mean. This also means that a distribution with a larger
    variance has a higher probability of containing observations close to the minimum/maximum
    points of the distribution. Think about what this means in the context of vehicle
    ages: if we take our two groups, private and commercial vehicles, and they both
    have the same mean vehicle age, but commercial vehicles have a much higher variance,
    that would mean that we’re more likely to encounter much newer and much older
    commercial vehicles compared to private vehicles. It could be that we just got
    lucky and commercial vehicles averaged out to the same mean as private vehicles
    in our sample despite the typical age of commercial vehicles being wildly different.
    If we use Student’s T-Test to compare means between groups that have very different
    variance, the value we calculate to help us decide whether to reject or fail to
    reject the null hypothesis becomes unreliable. This means we will be more likely
    to calculate a value that will lead us to reject the null hypothesis when we actually
    shouldn’t reject it, thereby coming to an erroneous conclusion.'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: Student的T-Test的一个重要假设是每个被测试组的方差相等。方差，就像标准差一样，与观测值从平均值偏离的程度有关。方差越高，典型的观测值往往离平均值越远，方差越小，典型的观测值往往离平均值越近。这也意味着方差较大的分布有更高的概率包含接近分布最小/最大点的观测值。想想这在车辆年龄的背景下意味着什么：如果我们考虑我们的两组，私有车辆和商用车辆，并且它们都有相同的平均车辆年龄，但商用车辆的方差要高得多，这意味着我们更有可能遇到比私有车辆更新和更旧的商用车辆。这可能只是我们运气好，尽管商用车辆的典型年龄差异很大，但商用车辆的平均值在我们的样本中与私有车辆的平均值相同。如果我们使用Student的T-Test来比较具有非常不同方差的组之间的均值，我们计算出的值将帮助我们决定是否拒绝或未能拒绝零假设，这个值将变得不可靠。这意味着我们更有可能计算出会导致我们错误地拒绝零假设的值，从而得出错误的结论。
- en: For cases where the group variances are not the same, we can use Welch’s T-Test
    instead. Welch’s T-Test is formulated slightly differently to help us avoid making
    the wrong conclusion. So, before we can decide to use Welch’s T-Test or Student’s
    T-Test to answer our question, we should check to see if the variance of private
    and commercial vehicles are the same or not. Fortunately, statistical hypothesis
    tests can help us check this—we just have to pick the right one!
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 对于组方差不同的情况，我们可以使用Welch的T-Test。Welch的T-Test的公式略有不同，有助于我们避免得出错误的结论。因此，在我们决定使用Welch的T-Test还是Student的T-Test来回答我们的问题之前，我们应该检查私有车辆和商用车辆的方差是否相同。幸运的是，统计假设检验可以帮助我们检查这一点——我们只需要选择正确的一个！
- en: 6.3.2 Using custom aggregates to implement the Brown-Forsythe test
  id: totrans-610
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 使用自定义聚合实现布朗-福斯泰测试
- en: The family of tests that help us check for equal variances also carry some assumptions
    with them. We can use a test called Bartlett’s Test for Equal Variances if the
    data is normally distributed—that is, symmetrical and roughly “bell shaped.” However,
    in section 6.1 we found that the skewness of vehicle ages is 1.012, meaning it’s
    not symmetrically distributed, so we can’t use Bartlett’s Test without running
    the risk of making an incorrect conclusion. A good alternative test that doesn’t
    have this assumption is called the Brown-Forsythe Test for Equal Variances. Since
    we can’t use Bartlett’s Test reliably on this data, we’ll use the Brown-Forsythe
    test to help us decide whether to use Student’s T-Test or Welch’s T-Test. The
    whole testing process we will go through from start to finish can be seen in [figure
    6.11](#figure6.11).
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 帮助我们检查方差相等的测试系列也附带一些假设。如果数据是正态分布的，即对称且大致呈“钟形”，我们可以使用称为Bartlett方差相等的测试。然而，在第6.1节中我们发现车辆年龄的偏度为1.012，这意味着它不是对称分布的，因此我们不能使用Bartlett测试，否则有得出错误结论的风险。一个没有这种假设的良好替代测试称为布朗-福斯泰方差相等的测试。由于我们无法可靠地使用Bartlett测试来处理这些数据，我们将使用布朗-福斯泰测试来帮助我们决定是否使用Student的T-Test或Welch的T-Test。从开始到结束的整个测试过程可以在[图6.11](#figure6.11)中看到。
- en: '![c06_11.eps](Images/c06_11.png)'
  id: totrans-612
  prefs: []
  type: TYPE_IMG
  zh: '![c06_11.eps](Images/c06_11.png)'
- en: '[Figure 6.11](#figureanchor6.11) The process we will use to determine if mean
    vehicle age has any relationship with vehicle type'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.11](#figureanchor6.11) 我们将使用的过程来确定平均车辆年龄是否与车辆类型有任何关系'
- en: First, we’ll start by declaring the null hypothesis and alternative hypothesis.
    The null hypothesis of the Brown-Forsythe test is that the variances among groups
    are equal. The alternative hypothesis is that the variances among groups are not
    equal. The test will help us decide if enough evidence exists to say that group
    variances are different, in which case we will need to use Welch’s T-Test, or
    if not enough evidence exists, we will use Student’s T-Test.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将开始声明零假设和备择假设。布朗-福斯泰测试的零假设是各组方差相等。备择假设是各组方差不相等。这个测试将帮助我们决定是否有足够的证据来说明组方差不同，在这种情况下，我们需要使用Welch的T检验，或者如果没有足够的证据，我们将使用Student的T检验。
- en: '![c06_12.png](Images/c06_12.png)'
  id: totrans-615
  prefs: []
  type: TYPE_IMG
  zh: '![c06_12.png](Images/c06_12.png)'
- en: '[Figure 6.12](#figureanchor6.12) The Brown-Forsythe Test for Equal Variances'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.12](#figureanchor6.12) 布朗-福斯泰方差齐性检验'
- en: '[Figure 6.12](#figure6.12) displays the Brown-Forsythe equation. While it may
    look complicated, we’ll break the equation down into smaller, more manageable
    pieces; then we’ll assemble the final result. Because the Brown-Forsythe test
    involves a lot of grouping and aggregate operations, this is a great opportunity
    to learn about Dask’s custom aggregate functions! The overall approach we’re going
    to take to calculate the Brown-Forsythe equation will be performed in five steps:'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.12](#figure6.12) 展示了布朗-福斯泰方程。虽然它看起来可能很复杂，但我们将把这个方程分解成更小、更易于管理的部分；然后我们将组装最终结果。由于布朗-福斯泰测试涉及大量的分组和聚合操作，这是学习Dask自定义聚合函数的绝佳机会！我们将采取以下五个步骤来计算布朗-福斯泰方程：'
- en: Calculate the left fraction.
  id: totrans-618
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算左侧分数。
- en: Calculate the denominator of the right fraction.
  id: totrans-619
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算右侧分数的分母。
- en: Calculate the numerator of the right fraction.
  id: totrans-620
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算右侧分数的分子。
- en: Divide the numerator of the right fraction by the denominator of the right fraction
    to calculate the value of the right fraction.
  id: totrans-621
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将右侧分数的分子除以分母来计算右侧分数的值。
- en: Multiply the left fraction by the right fraction.
  id: totrans-622
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将左侧分数乘以右侧分数。
- en: In section 6.1, you created a Parquet file that includes an additional column
    containing the calculated vehicle age. We’ll start by reading this file in again.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6.1节中，你创建了一个包含计算出的车辆年龄额外列的Parquet文件。我们将首先再次读取这个文件。
- en: Listing 6.11 Setting up the Vehicle Age dataset
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.11 设置车辆年龄数据集
- en: '[PRE82]'
  id: totrans-625
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: The reason we want to filter the data to contain only vehicles with PAS-type
    plates or COM-type plates is because we previously recoded that column to also
    contain an Other value for vehicles that were neither PAS type nor COM type. Two-sample
    t-tests can only be used to test the difference in means between two groups, so
    we will filter out the Other category before proceeding. After applying the filter,
    we’ll calculate the first part of the equation using simple aggregate functions
    you’ve learned before. [Figure 6.13](#figure6.13) shows what we’ll be doing in
    the calculation.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要过滤数据，只包含带有PAS型车牌或COM型车牌的车辆的原因是，我们之前将该列重新编码，以包含既不是PAS型也不是COM型的其他车辆。双样本t检验只能用于测试两组之间的均值差异，因此我们在继续之前将过滤掉其他类别。在应用过滤器后，我们将使用你之前学过的简单聚合函数计算方程的第一部分。[图6.13](#figure6.13)
    展示了我们在计算中将要做什么。
- en: '![c06_13.png](Images/c06_13.png)'
  id: totrans-627
  prefs: []
  type: TYPE_IMG
  zh: '![c06_13.png](Images/c06_13.png)'
- en: '[Figure 6.13](#figureanchor6.13) The first part of the Brown-Forsythe test'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.13](#figureanchor6.13) 布朗-福斯泰测试的第一部分'
- en: The first part of the equation, called the *degrees of freedom*, is very simple
    to calculate. We will need to count the total number of citations that are in
    our filtered dataset, and also count the distinct groups we’re testing (which
    for a two-sample t-test should always be 2!). We’ll stash this value and multiply
    it by another value later to get the final result of the Brown-Forsythe test.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 方程的第一部分，称为“自由度”，计算起来非常简单。我们需要计算我们过滤后的数据集中引用的总数，以及我们正在测试的不同组数（对于双样本t检验，这应该是始终为2的！）。我们将保存这个值，稍后乘以另一个值以获得布朗-福斯泰测试的最终结果。
- en: Listing 6.12 Calculating the left fraction of the Brown-Forsythe equation
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.12 计算布朗-福斯泰方程的左侧分数
- en: '[PRE83]'
  id: totrans-631
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Everything in [listing 6.12](#listing6.12) should look familiar. The variable
    `N` represents the total number of observations in the dataset, and the variable
    `p` represents the number of groups. To find the values of `N` and `p`, we simply
    need to count the total number of observations and also count the number of unique
    groups. Then we’ll use the values of `N` and `p` to calculate the left fraction
    of the equation.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 6.12](#listing6.12) 中的所有内容都应该看起来很熟悉。变量 `N` 代表数据集中的观测总数，变量 `p` 代表组数。要找到
    `N` 和 `p` 的值，我们只需计算观测总数，并计算唯一组数。然后我们将使用 `N` 和 `p` 的值来计算方程的左侧分数。'
- en: Calculating a median using the quantile method
  id: totrans-633
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用分位数方法计算中位数
- en: Now we’ll begin working on the right fraction by calculating the denominator
    (see [figure 6.14](#figure6.14)). As you can see, we’ll be calculating the same
    set of values for each group—private vehicles and commercial vehicles—in parallel,
    then summing the results. We’ll start by calculating the median age for each vehicle
    type.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将开始通过计算分母（参见 [图 6.14](#figure6.14)）来处理方程的右侧分数。如您所见，我们将并行计算每个组——私家车和商用车辆——的相同值集，然后汇总结果。我们将从计算每种车辆类型的平均年龄开始。
- en: '![c06_14.eps](Images/c06_14.png)'
  id: totrans-635
  prefs: []
  type: TYPE_IMG
  zh: '![c06_14.eps](Images/c06_14.png)'
- en: '[Figure 6.14](#figureanchor6.14) The process for calculating the right denominator'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6.14](#figureanchor6.14) 计算右侧分母的过程'
- en: Listing 6.13 Calculating the median age for each vehicle type
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.13 计算每种车辆类型的平均年龄
- en: '[PRE84]'
  id: totrans-638
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: Unlike Pandas and NumPy, Dask does not have an explicit median method on DataFrame
    or Series objects. Instead, you must use the `quantile` method to calculate the
    0.5 quantile for the Vehicle Age column, which is equivalent to the 50th percentile,
    or median. Next, we’ll create a new column that uses these medians to subtract
    from the age of each vehicle its corresponding group median. For privately owned
    (PAS) vehicles, we will subtract the median age of all PAS vehicles from each
    vehicle’s age. Similarly, for commercial (COM) vehicles, we will subtract the
    median age of all COM vehicles from each vehicle's age. To do this, we’ll define
    a function to apply the conditional subtraction logic.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Pandas 和 NumPy 不同，Dask 在 DataFrame 或 Series 对象上没有显式的中位数方法。相反，您必须使用 `quantile`
    方法来计算“车辆年龄”列的 0.5 分位数，这相当于第 50 个百分位数，或中位数。接下来，我们将创建一个新的列，使用这些中位数从每辆车的年龄中减去其对应组的平均年龄。对于私人拥有（PAS）车辆，我们将从每辆车的年龄中减去所有
    PAS 车辆的平均年龄。同样，对于商用（COM）车辆，我们将从每辆车的年龄中减去所有 COM 车辆的平均年龄。为此，我们将定义一个函数来应用条件减法逻辑。
- en: Listing 6.14 A function to calculate the absolute median deviation
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.14 一个用于计算绝对中位数偏差的函数
- en: '[PRE85]'
  id: totrans-641
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'This function in [listing 6.14](#listing6.14) is very simple: if a vehicle
    is a PAS type, we subtract the median age of PAS vehicles from the vehicle''s
    age. Otherwise, we subtract the the median of COM vehicles from the vehicle''s
    age. We’ll use this function with the same `apply` method you’ve used several
    times before, which will result in a column containing the absolute difference
    between the vehicle’s age and its corresponding group median.'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 6.14](#listing6.14) 中的这个函数非常简单：如果一辆车是 PAS 类型，我们从车辆的年龄中减去 PAS 车辆的平均年龄。否则，我们从车辆的年龄中减去
    COM 车辆的平均年龄。我们将使用这个函数与之前多次使用过的相同 `apply` 方法，这将导致一个包含车辆年龄与其对应组平均年龄之间绝对差异的列。'
- en: Listing 6.15 Creating a column to calculate the absolute median differences
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.15 创建一个列来计算绝对中位数差异
- en: '[PRE86]'
  id: totrans-644
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: In [listing 6.15](#listing6.15), the `apply` function is used to create a new
    Series containing the result of the calculation; then we assign the column to
    our existing DataFrame and rename it. We’re about halfway done with calculating
    the right denominator now. Let’s check in with how we’re doing. [Figure 6.15](#figure6.15)
    shows our progress so far.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 6.15](#listing6.15) 中，`apply` 函数用于创建一个包含计算结果的新 Series；然后我们将该列分配给现有的 DataFrame
    并重命名它。现在我们在计算右侧分母的过程中已经完成了一半。让我们检查一下我们的进度。[图 6.15](#figure6.15) 显示了我们到目前为止的进展。
- en: '![c06_15.eps](Images/c06_15.png)'
  id: totrans-646
  prefs: []
  type: TYPE_IMG
  zh: '![c06_15.eps](Images/c06_15.png)'
- en: '[Figure 6.15](#figureanchor6.15) Our progress so far calculating the right
    denominator of the Brown-Forsythe equation'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6.15](#figureanchor6.15) 计算布朗-福赛斯方程右侧分母的进展'
- en: All right! The next thing we’ll need to do is to calculate the mean of the median
    differences for each group. We can do that simply with a `groupby`/`mean` call
    that we’ve seen a few times already.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 好了！接下来，我们需要计算每个组的平均中位数差异。我们可以简单地使用我们之前已经看到几次的 `groupby`/`mean` 调用来完成这个操作。
- en: Listing 6.16 Calculating the group means of the median differences
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.16 计算中值差异的组均值
- en: '[PRE87]'
  id: totrans-650
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: The result of this computation, `group_means`, is a Series containing the means
    of the Median Difference column grouped by Plate Type. We can access the mean
    of either group by using normal filter expressions. We’ll use this in another
    conditional function that subtracts each Median Difference amount by the observation’s
    corresponding Plate Type. This will result in a Group Mean Variance for each observation
    in the dataset.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算的结果，`group_means`，是一个包含按板型分组的中值差异列均值的 Series。我们可以通过使用正常的过滤器表达式来访问任何组的均值。我们将在另一个条件函数中使用这个结果，该函数将每个中值差异金额减去观测值的对应板型。这将导致数据集中每个观测值的组均值方差。
- en: Listing 6.17 Calculating the Group Mean Variance
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.17 计算组均值方差
- en: '[PRE88]'
  id: totrans-653
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Lastly, to finish calculating the right denominator of the Brown-Forsythe equation,
    all we need to do is sum up the Group Mean Variance column. We can do this with
    a simple call to the `sum` method.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了完成布朗-福塞方程右侧分母的计算，我们只需要将组均值方差列求和。我们可以通过简单的调用 `sum` 方法来完成这个操作。
- en: Listing 6.18 Finishing calculating the right denominator
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.18 完成计算右侧分母
- en: '[PRE89]'
  id: totrans-656
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Now that we’ve finished calculating the denominator, we’ll finish by calculating
    the numerator. To do that, we’ll follow the process outlined in [figure 6.16](#figure6.16).
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了分母的计算，接下来我们将计算分子。为此，我们将遵循[图 6.16](#figure6.16)中概述的过程。
- en: '![c06_16.png](Images/c06_16.png)'
  id: totrans-658
  prefs: []
  type: TYPE_IMG
  zh: '![c06_16.png](Images/c06_16.png)'
- en: '[Figure 6.16](#figureanchor6.16) The process to calculate the right numerator
    of the Brown-Forsythe equation'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6.16](#figureanchor6.16) 计算布朗-福塞方程正确分子的过程'
- en: We’ll begin by calculating the *grand mean* of the Median Differences column.
    A grand mean is another way of saying “the mean of a column without any grouping.”
    This is opposite of a *group mean*, which, as you may guess, is the mean of a
    group—for example, the mean vehicle age of PAS vehicles is a group mean, and the
    mean vehicle age of all vehicles is a grand mean.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先计算中值差异列的*总体均值*。总体均值是“没有分组的列的均值”的另一种说法。这与*组均值*相反，正如你可能猜到的，组均值是组的均值——例如，PAS
    车辆的平均车辆年龄是一个组均值，所有车辆的平均车辆年龄是一个总体均值。
- en: Listing 6.19 Calculating the grand mean of the Median Difference column
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.19 计算中值差异列的总体均值
- en: '[PRE90]'
  id: totrans-662
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Creating a Custom Aggregation Object
  id: totrans-663
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建自定义聚合对象
- en: Now that we have the grand mean calculated, we’ll handle the next three steps
    in the process using a custom aggregation. As you can see in [figure 6.16](#figure6.16),
    we need both the group means and the count of observations in each group. Rather
    than calculating them separately, we can make use of the Aggregation object in
    the Dask DataFrame API to get both these values as part of the same computation.
    Let’s see what this looks like.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算出了总体均值，接下来我们将使用自定义聚合来处理过程中的下三个步骤。正如你在[图 6.16](#figure6.16)中可以看到的，我们需要每个组中的组均值和观测值的数量。而不是分别计算它们，我们可以利用
    Dask DataFrame API 中的聚合对象来在相同的计算中获取这两个值。让我们看看这会是什么样子。
- en: Listing 6.20 A custom aggregation for calculating the right numerator
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.20 用于计算正确分子的自定义聚合
- en: '[PRE91]'
  id: totrans-666
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Now things are starting to get interesting! In [listing 6.20](#listing6.20),
    we see an example of a custom aggregate function. Until this point, we’ve relied
    on Dask’s built-in aggregate functions such as `sum`, `count`, `mean`, and so
    forth to perform aggregate calculations. But, it’s necessary to define your own
    aggregate function if you have a more complex calculation that needs to be performed
    over a grouping.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 现在事情开始变得有趣了！在[列表 6.20](#listing6.20)中，我们看到一个自定义聚合函数的例子。到目前为止，我们一直依赖于 Dask 的内置聚合函数，如
    `sum`、`count`、`mean` 等，来执行聚合计算。但是，如果你需要执行一个更复杂的计算，并且这个计算需要在分组上执行，那么定义你自己的聚合函数是必要的。
- en: 'Dask’s facility for defining custom aggregate functions is the `Aggregation`
    class found in the `dask.dataframe` package. It has a minimum of three arguments
    with an optional fourth argument:'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 定义自定义聚合函数的设施是在 `dask.dataframe` 包中找到的 `Aggregation` 类。它有三个最小参数，以及一个可选的第四个参数：
- en: An internal name for the aggregation
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合的内部名称
- en: A function to be applied to each partition
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个应用于每个分区的函数
- en: A function to aggregate the results from each partition
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于聚合每个分区结果的函数
- en: (Optional) A function to perform a final transformation on the aggregated values
    before outputting them
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （可选）一个在输出之前对聚合值执行最终转换的函数
- en: The first argument is simply an internal name for the aggregation. The second
    argument takes a function (which can either be a defined function or an anonymous
    lambda function) and applies it to each partition. This is called the *chunk step*.
    In [listing 6.20](#listing6.20), we’re counting the number of observations in
    each chunk as well as summing up the values of each chunk, and returning a tuple
    containing these calculated values. Next, Dask will collect all the results of
    each chunk step and apply the function defined in the third argument to the collected
    chunk results. This is called the *aggregation step*. In [listing 6.20](#listing6.20),
    we’re summing the values we calculated for each chunk, resulting in a grand total
    of the count of observations contained in the entire DataFrame and a grand total
    of the sum of Vehicle Ages in the entire DataFrame. But we’re not quite done with
    this calculation. The fourth and final argument is called the *finalization step*,
    which gives us one last chance to transform the data before returning it to the
    user. In [listing 6.20](#listing6.20), we take the aggregated sum and divide by
    the aggregated count to get the group mean, subtract it from the grand mean, square
    the difference, and multiply by the count. This will yield the results we want
    to then sum up to get the final value for the right-hand numerator. Now that we’ve
    defined the aggregation, let’s apply it to the data to calculate the value.
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数仅仅是聚合的一个内部名称。第二个参数接受一个函数（这可以是一个定义好的函数或匿名lambda函数），并将其应用于每个分区。这被称为*块步骤*。在[列表
    6.20](#listing6.20)中，我们计算了每个块中的观测数量以及每个块值的总和，并返回一个包含这些计算值的元组。接下来，Dask将收集每个块步骤的结果，并将定义在第三个参数中的函数应用于收集到的块结果。这被称为*聚合步骤*。在[列表
    6.20](#listing6.20)中，我们计算了每个块的总和，从而得到了整个DataFrame中观测数量的总和以及整个DataFrame中车辆年龄的总和。但我们还没有完成这个计算。第四个也是最后的参数被称为*最终化步骤*，它在我们将数据返回给用户之前给我们最后一次转换数据的机会。在[列表
    6.20](#listing6.20)中，我们取聚合的总和并除以聚合的计数以得到组均值，从总体均值中减去它，然后平方差值并乘以计数。这将得到我们想要求和以得到右边分子最终值的那些结果。现在我们已经定义了聚合，让我们将其应用于数据以计算值。
- en: Listing 6.21 Using the custom aggregate function
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.21 使用自定义聚合函数
- en: '[PRE92]'
  id: totrans-675
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: As you can see in [listing 6.21](#listing6.21), using custom aggregate functions
    is very similar to using any of the built-in aggregate functions. You can map
    custom aggregate functions to columns in a DataFrame using the `agg` method you
    learned previously, and you can also use them with the `groupby` method. Here
    we’re using the custom aggregate function we defined to calculate the group variances
    for each Plate Type. The last thing we need to do to get the final value for the
    numerator is to sum the group variances. Since our custom aggregate function results
    in a Series object, we can simply use the `sum` method over it that you’ve seen
    several times before.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在[列表 6.21](#listing6.21)中看到的，使用自定义聚合函数与使用任何内置聚合函数非常相似。你可以使用之前学到的`agg`方法将自定义聚合函数映射到DataFrame的列上，你也可以使用`groupby`方法来使用它们。在这里，我们使用我们定义的自定义聚合函数来计算每个板型的组方差。为了得到分子最终值，我们最后需要做的就是求和组方差。由于我们的自定义聚合函数产生一个Series对象，我们可以简单地使用之前多次见过的`sum`方法来操作它。
- en: Listing 6.22 Finishing the right numerator calculation
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.22 完成右边分子的计算
- en: '[PRE93]'
  id: totrans-678
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: Great! We now have all the pieces to finish calculating the Brown-Forsythe equation.
    All we need to do is divide the right numerator by the right denominator, and
    multiply by the left fraction we calculated first. This will yield a result called
    the *F statistic*. The F statistic will help guide us to a conclusion of whether
    or not to reject the null hypothesis. Let’s calculate that now.
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们现在已经拥有了完成布朗-福赛斯方程计算的各个部分。我们只需要将右边的分子除以右边的分母，然后乘以我们首先计算出的左边分数。这将得到一个称为*F统计量*的结果。F统计量将帮助我们得出是否拒绝零假设的结论。现在让我们来计算一下。
- en: Listing 6.23 Calculating the F statistic
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.23 计算F统计量
- en: '[PRE94]'
  id: totrans-681
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: Interpreting the results of the Brown-Forsythe Test
  id: totrans-682
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解读布朗-福赛斯检验的结果
- en: 'Since we’ve done all the hard work, calculating the F statistic is very simple
    and straightforward. If all goes well, you should receive an F statistic value
    of 27644.7165804\. However, we’re not quite done. Is this number good? Bad? The
    statistic by itself isn’t really interpretable. In order for us to make a conclusion
    about our findings and either reject or fail to reject the null hypothesis, we
    have to compare this value with the *critical value* of the test’s underlying
    distribution. The critical value provides a threshold that helps us interpret
    the meaning of the test statistic. If the test statistic is greater than the critical
    value, we can reject the null hypothesis. Otherwise, we fail to reject the null
    hypothesis. As the Brown-Forsythe test produces an F statistic, we must use the
    *F distribution**to find the critical value. To find the critical value from the
    F distribution, we need three parameters: two measures of the data’s degrees of
    freedom and the confidence level we’d like to use.*'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经完成了所有艰苦的工作，计算 F 统计量非常简单直接。如果一切顺利，你应该收到一个 F 统计量值为 27644.7165804。然而，我们还没有完成。这个数字是好是坏？仅凭这个统计量本身并不容易解释。为了我们对我们的发现得出结论，并决定是否拒绝或未能拒绝零假设，我们必须将这个值与测试潜在分布的
    *临界值* 进行比较。临界值提供了一个阈值，帮助我们解释测试统计量的意义。如果测试统计量大于临界值，我们可以拒绝零假设。否则，我们未能拒绝零假设。由于布朗-福斯伊特测试产生一个
    F 统计量，我们必须使用 *F 分布**来找到临界值。为了从 F 分布中找到临界值，我们需要三个参数：数据自由度的两个度量以及我们希望使用的置信水平。*
- en: '*The degrees of freedom for the Brown-Forsythe test are simply the number of
    groups we’re testing minus one and the total number of observations minus the
    number of groups we’re testing. This should look familiar—these were the two parts
    of the left fraction we calculated a while back. We can reuse the values we saved
    to the variables `N` and `p` to find the critical value.'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: '*布朗-福斯伊特测试的自由度简单来说就是我们要测试的组数减一，以及总观测数减去我们要测试的组数。这应该看起来很熟悉——这些是我们之前计算左分数的两个部分。我们可以重用我们保存到变量
    `N` 和 `p` 中的值来找到临界值。*'
- en: The confidence level can be any value between 0 and 1 that you are free to choose.
    It essentially represents a probability that the result of the test will result
    in a correct conclusion. The higher the value, the stricter and more robust the
    result of the test becomes. For example, if we pick a confidence level of 0.95,
    there will be only about a 5% chance that the test might erroneously signify that
    you should reject the null hypothesis. You’ve likely heard of a *p-value**before;
    this is simply one minus the confidence level. A commonly accepted p-value in
    scientific research is 0.05, so to follow suit, we’ll use a confidence level of
    0.95 here. To calculate the F critical value, we’ll use the F distribution from
    SciPy.*
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 置信水平可以是 0 到 1 之间的任何值，你可以自由选择。它本质上代表了一个概率，即测试的结果将导致正确的结论。值越高，测试的结果就越严格、越稳健。例如，如果我们选择置信水平为
    0.95，那么测试错误地表示你应该拒绝零假设的可能性只有大约 5%。你可能之前听说过 *p值**；这实际上是置信水平的倒数。科学研究中普遍接受的 p 值是
    0.05，因此为了保持一致，我们在这里将使用置信水平 0.95。为了计算 F 临界值，我们将使用 SciPy 中的 F 分布。*
- en: '*Listing 6.24 Calculating the F critical value'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 6.24 计算F临界值'
- en: '[PRE95]'
  id: totrans-687
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[Listing 6.24](#listing6.24) shows how to calculate the F critical value for
    our test. The `stats.f` class contains an implementation of the F distribution,
    and the `ppf` method calculates the value of an F distribution with the degrees
    of freedom `dfn` and `dfd` at point `q`. As you can see, point `q` is simply the
    confidence value we selected, and `dfn` and `dfd` use the two variables we computed
    at the beginning of the section. This calculation should result in an F critical
    value of 3.8414591786\. Finally, we can report our findings and make a conclusion.
    The next listing will print out a nice statement summarizing our findings and
    highlighting the relevant numbers we used to come to our conclusion.'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 6.24](#listing6.24) 展示了如何计算测试的 F 临界值。`stats.f` 类包含 F 分布的实现，而 `ppf` 方法计算具有自由度
    `dfn` 和 `dfd` 的 F 分布在点 `q` 的值。正如你所见，点 `q` 简单来说就是我们所选择的置信度值，而 `dfn` 和 `dfd` 使用我们在本节开头计算的两个变量。这个计算应该得到一个
    F 临界值为 3.8414591786。最后，我们可以报告我们的发现并得出结论。下一个列表将打印出一个总结我们的发现并突出我们用来得出结论的相关数字的漂亮声明。'
- en: Listing 6.25 Reporting our findings of the Brown-Forsythe test
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.25 报告布朗-福斯伊特测试的发现
- en: '[PRE96]'
  id: totrans-690
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: In this particular case, we’re told to reject the null hypothesis because the
    F statistic is larger than the F critical value. Therefore, when running a two-sample
    t-test to answer our original question, we should *not* assume equal variances
    among vehicle types. Now we can finally run the appropriate t-test and see if
    the average age of vehicles that received a parking ticket in New York City is
    significantly different based on the vehicle type! Before we move on, let’s recap
    where we came from and what we’re going to do next.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定情况下，我们被告知拒绝零假设，因为 F 统计量大于 F 临界值。因此，当我们运行双样本 t 检验来回答我们的原始问题时，我们不应该假设不同类型的车辆具有相同的方差。现在我们终于可以运行适当的
    t 检验，看看在纽约市收到停车罚单的车辆的平均年龄是否根据车辆类型有显著差异！在我们继续之前，让我们回顾一下我们是从哪里来的以及我们接下来要做什么。
- en: '![c06_17.eps](Images/c06_17.png)'
  id: totrans-692
  prefs: []
  type: TYPE_IMG
  zh: '![c06_17.eps](Images/c06_17.png)'
- en: '[Figure 6.17](#figureanchor6.17) We’ve rejected the null hypothesis of the
    Brown-Forsythe test, so we will next run Welch’s T-Test.'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6.17](#figureanchor6.17) 我们已经拒绝了 Brown-Forsythe 测试的零假设，所以接下来我们将运行 Welch
    的 T-Test。'
- en: 'As you can see in [figure 6.17](#figure6.17), now that we’ve rejected the null
    hypothesis of the Brown-Forsythe test, we will want to run Welch’s T-Test on the
    data to answer our original question of “Is the average age of privately owned
    vehicles the same as it is for commercially owned vehicles?” We’re also presented
    with an important decision here: while Dask does have a handful of statistical
    hypothesis tests (including two-sample t-tests) built into the `dask.array.stats`
    package, you may recall from chapter 1 that when the data you’re working with
    can comfortably fit into memory, it can be faster to pull the data out of Dask
    and work with it in memory. However, we will take a closer look at the statistical
    functions in the Dask Array library in chapter 9\. For the two-sample t-test,
    we need only two one-dimensional numeric arrays: one containing all observations
    of PAS-type vehicle ages and one containing all observations of COM-type vehicle
    ages. Some quick “back-of-the-napkin” math suggests we should expect to have about
    40,000,000 64-bit floating-point numbers, which equates to somewhere in the realm
    of 300 MB of data in memory. This should be easy to fit in memory, so we will
    opt to collect the arrays and perform our t-test computation locally.'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在[图 6.17](#figure6.17)中看到的，现在我们已经拒绝了 Brown-Forsythe 测试的零假设，我们将想要在数据上运行 Welch
    的 T-Test 来回答我们的原始问题：“私有车辆的平均年龄是否与商用车辆的平均年龄相同？” 我们还面临一个重要的决定：虽然 Dask 确实内置了一些统计假设检验（包括双样本
    t 检验），但您可能还记得在第 1 章中提到，当您处理的数据可以舒适地适应内存时，从 Dask 中提取数据并在内存中处理它可能更快。然而，我们将在第 9 章中更详细地探讨
    Dask Array 库中的统计函数。对于双样本 t 检验，我们只需要两个一维数值数组：一个包含 PAS 类型车辆年龄的所有观测值，另一个包含 COM 类型车辆年龄的所有观测值。一些快速的“纸背”数学表明我们预计将有大约
    4,000,000,000 个 64 位浮点数，这相当于大约 300 MB 的内存数据。这应该很容易适应内存，因此我们将选择收集数组并在本地执行我们的 t
    检验计算。
- en: Listing 6.26 Collecting an array of values
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.26 收集值数组
- en: '[PRE97]'
  id: totrans-696
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: As you can see in [listing 6.26](#listing6.26), it’s very easy to collect the
    values locally. The `values` attribute of a Dask Series will expose the underlying
    Dask Array and calling compute on a Dask Array will return a NumPy array containing
    the result.
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在[列表 6.26](#listing6.26)中看到的，在本地收集值非常简单。Dask Series 的 `values` 属性将暴露底层的 Dask
    Array，对 Dask Array 调用 compute 将返回包含结果的 NumPy 数组。
- en: Now that we have the data down locally in NumPy arrays, we can run the t-test.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将数据本地化到 NumPy 数组中，我们可以运行 t 检验。
- en: Listing 6.27 Running the two-sample t-test
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.27 运行双样本 t 检验
- en: '[PRE98]'
  id: totrans-700
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: Very simple—SciPy does all the heavy lifting for us here. Notice that in [listing
    6.27](#listing6.27), we set the `equal_var` argument to False. This lets SciPy
    know that we’ve run a test of equal variances and found that the group variances
    are not equal. With the parameter set this way, SciPy will run Welch’s T-Test
    instead of Student’s T-Test, avoiding the potential problems you learned about
    earlier in this section. SciPy also makes interpreting the result easy for us,
    because in addition to the test statistic, it’s gone ahead and calculated the
    p-value as well. With p-values, we want it to be smaller than one minus the confidence
    level we’ve chosen. So if we choose a confidence level of 0.95 again, we’re looking
    for a p-value of less than 0.05 to reject the null hypothesis. As a reminder,
    the null hypothesis of the t-test is that means are equal among groups. Since
    we see that the p-value of this test is less than 0.05, we can reject the null
    hypothesis and conclude that sufficient evidence exists to show that the average
    vehicle age is different based on vehicle plate type.
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 非常简单——SciPy 在这里为我们做了所有繁重的工作。注意，在[列表 6.27](#listing6.27)中，我们将`equal_var`参数设置为False。这样SciPy就知道我们已经进行了一个方差相等的测试，并发现组方差不相等。以这种方式设置参数后，SciPy将运行Welch的T-Test而不是Student的T-Test，避免了在本节之前学到的潜在问题。SciPy还使我们更容易解释结果，因为它不仅计算了测试统计量，还计算了p值。对于p值，我们希望它小于我们选择的置信水平减一。所以如果我们再次选择置信水平为0.95，我们正在寻找小于0.05的p值来拒绝零假设。作为提醒，t检验的零假设是各组均值相等。由于我们看到这个测试的p值小于0.05，我们可以拒绝零假设，并得出结论，有足够的证据表明根据车牌类型，平均车辆年龄是不同的。
- en: '| **Null hypothesis** | **Condition** | **p-value** | **Reject?** | **Conclusion**
    |'
  id: totrans-702
  prefs: []
  type: TYPE_TB
  zh: '| **零假设** | **条件** | **p值** | **拒绝？** | **结论** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-703
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| PAS and COM vehicles have the same mean age. | p < 0.05 | 0.0 | Yes | PAS
    and COM vehicles *do not* have the same mean age. |'
  id: totrans-704
  prefs: []
  type: TYPE_TB
  zh: '| PAS和COM车辆的平均年龄相同。 | p < 0.05 | 0.0 | 是 | PAS和COM车辆*并不*有相同的平均年龄。 |'
- en: Now that we’ve walked through an example together, I hope you have a better
    idea of how to use your own custom aggregate functions in Dask, and you got some
    extra practice with other common Dask operations along the way. I also hope that
    you are starting to appreciate the power, simplicity, and flexibility Dask has
    to offer when it comes to implementing custom algorithms. We were able to implement
    a reasonably complex statistical calculation with a fairly small amount of code,
    and didn’t even need to dive into the low-level guts of the framework to do what
    we needed to do! Pretty neat.
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经一起走过了这个例子，我希望你对自己的自定义聚合函数在 Dask 中的使用有了更好的理解，并且在过程中也练习了其他常见的 Dask 操作。我还希望，你开始欣赏
    Dask 在实现自定义算法时所能提供的强大功能、简洁性和灵活性。我们能够用相当少的代码实现一个相当复杂的统计计算，甚至不需要深入框架的低级内部结构来完成我们需要做的事情！真是太棒了。
- en: 6.4 Rolling (window) functions
  id: totrans-706
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 滚动（窗口）函数
- en: We’ll close out this chapter on summarizing and analyzing DataFrames with something
    a bit less involved than the previous section, but equally important for many
    classes of analyses. No discussion on data analysis would be complete without
    talking about rolling functions. If you’ve worked with SQL, you may be familiar
    with window functions—rolling functions are just the name given to window functions
    in Dask.
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用比上一节稍微简单一点的内容来结束本章关于总结和分析 DataFrame 的内容，但对于许多分析类别来说同样重要。在数据分析中不讨论滚动函数是不完整的。如果你使用过
    SQL，你可能熟悉窗口函数——滚动函数只是 Dask 中对窗口函数的称呼。
- en: 'If you’re not familiar with the concept of windowing, it allows you to define
    calculations across a set of sequential data that incorporate variables which
    are positionally relative to another value. The most common application of windowing
    is analyzing data that has a time dimension, such as days or hours. For example,
    if we were analyzing sales revenue for an online store, we may want to know how
    many more (or fewer) items were sold today compared with yesterday. This could
    be mathematically expressed as *sales*[t] – *sales*[t–1], where the subscript
    *t* indicates the time period that the sales were measured for. Since the difference
    equation refers to two time periods, we would say that it has a *two-period window*.
    If we applied this calculation across a sequence of sales observations, we would
    end up with a transformed sequence of differences between each day and the day
    prior. Hence, that simple equation is a window function! Of course, window functions
    can be far more complex and can span a much larger window as well. A 50-day simple
    moving average, which is commonly computed for describing the volatility and momentum
    of a publicly traded financial asset, is a good example of a more complicated
    window function with a larger window. In this section, we will use a rolling function
    to answer the following question:'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不太熟悉窗口的概念，它允许你定义跨越一组连续数据集的计算，这些数据集包含相对于另一个值位置相关的变量。窗口最常见的应用是分析具有时间维度的数据，如日或小时。例如，如果我们正在分析在线商店的销售收入，我们可能想知道今天比昨天多卖（或少卖）了多少商品。这可以用数学表达式
    *sales*[t] – *sales*[t–1] 来表示，其中下标 *t* 表示销售被测量的时间周期。由于差分方程涉及两个时间周期，我们可以说它有一个 *两期窗口*。如果我们将此计算应用于一系列销售观察结果，我们最终会得到一个转换后的序列，该序列表示每天与前一天之间的差异。因此，这个简单的方程就是一个窗口函数！当然，窗口函数可以更加复杂，也可以跨越更大的窗口。一个50天的简单移动平均，通常用于描述公开交易的金融资产的波动性和动量，是一个更复杂的窗口函数的例子，具有更大的窗口。在本节中，我们将使用滚动函数来回答以下问题：
- en: Do the number of citations issued over time show any trends or cyclical patterns?
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，发表引用的数量是否显示出任何趋势或周期性模式？
- en: 6.4.1 Preparing data for a rolling function
  id: totrans-710
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 为滚动函数准备数据
- en: 'Rolling functions in Dask are fairly simple to use but take a bit of intelligent
    foresight to use properly due to the distributed nature of Dask DataFrames. Most
    importantly, Dask has some limitations around the size of the window that you
    can use: the size of the window can’t be large enough to span more than one adjacent
    partition. As an example, if your data is partitioned by month, you could not
    specify a window size larger than two months (the month of data in focus and the
    month before/after the month). This makes sense when you consider that these operations
    can induce a lot of shuffling. Therefore, you should ensure that the partition
    sizes you choose are large enough to avoid this boundary issue, but keep in mind
    that larger partitions can begin to slow down your computations, especially if
    a lot of shuffling is necessary. Some common sense and experimentation are in
    order to find the best balance for each problem you want to solve. The data should
    also be index-aligned to ensure that it’s sorted in the correct order. Dask uses
    the index to determine which rows are adjacent to one another, so ensuring proper
    sort order is critical for the correct execution of any calculations on the data.
    Let’s have a look at an example of using a rolling function now!'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: Dask中的滚动函数使用起来相当简单，但由于Dask DataFrames的分布式特性，需要一点智能的前瞻性才能正确使用。最重要的是，Dask在窗口大小方面有一些限制：窗口的大小不能足够大，以至于跨越多个相邻分区。例如，如果你的数据按月份分区，你无法指定超过两个月的窗口大小（即数据关注的月份及其前后的月份）。当你考虑到这些操作可能会引起大量的数据移动时，这就有意义了。因此，你应该确保你选择的分区大小足够大，以避免这种边界问题，但请注意，更大的分区可能会开始减慢你的计算速度，尤其是在需要大量数据移动的情况下。对于每个你想解决的问题，一些常识和实验是必要的，以找到最佳平衡。数据还应进行索引对齐，以确保其按正确顺序排序。Dask使用索引来确定哪些行彼此相邻，因此确保正确的排序对于正确执行任何数据计算至关重要。现在让我们看看使用滚动函数的一个例子！
- en: Listing 6.28 Prepping data for a rolling function
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.28 为滚动函数准备数据
- en: '[PRE99]'
  id: totrans-713
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: To start, in [listing 6.28](#listing6.28), we’ll prepare some data. We’re going
    to go back to the NYC Parking Ticket dataset and look at a moving average of citations
    issued per month. What we’re aiming to find out is if we can spot any discernable
    trend in the data after smoothing out some of the volatility. By averaging each
    month with a certain number of prior months, individual dips and spikes in a given
    month will be less prominent, which may reveal an underlying trend that would
    be difficult to see in the raw data.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在[列表 6.28](#listing6.28)中，我们将准备一些数据。我们将回到纽约市停车罚单数据集，查看每月发布的罚单的移动平均值。我们想要找出的是，在平滑掉一些波动性之后，我们是否能在数据中找到任何可辨别的趋势。通过将每个月与一定数量的前几个月平均，给定月份中的个别低谷和高峰将不那么明显，这可能会揭示在原始数据中难以看到的潜在趋势。
- en: In section 6.1, we noticed that the observations in our dataset tended to drop
    off dramatically after June 2017, and we opted to discard any observations after
    that month. We’ll filter them out again here. Then, we’ll count the number of
    citations per month.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 在 6.1 节中，我们注意到我们的数据集在 2017 年 6 月之后观察值急剧下降，我们选择丢弃该月之后的任何观察值。我们在这里再次过滤掉它们。然后，我们将按月计算引用次数。
- en: 6.4.2 Using the rolling method to apply a window function
  id: totrans-716
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 使用滚动方法应用窗口函数
- en: With the `citationsByMonth` object representing a count of citations per month,
    we can apply the rolling function transformation before computing the result.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 使用代表每月引用次数的`citationsByMonth`对象，我们可以在计算结果之前应用滚动函数转换。
- en: Listing 6.29 Computing a rolling mean of Citations per Month
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.29 计算每月引用的滚动平均值
- en: '[PRE100]'
  id: totrans-719
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '![c06_18.eps](Images/c06_18.png)'
  id: totrans-720
  prefs: []
  type: TYPE_IMG
  zh: '![c06_18.eps](Images/c06_18.png)'
- en: '[Figure 6.18](#figureanchor6.18) Abbreviated output of the window function'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6.18](#figureanchor6.18) 窗口函数的简略输出'
- en: In [listing 6.29](#listing6.29), you can see just how simple the built-in rolling
    functions are! Before applying what looks to be an aggregate function, we’ve chained
    a `rolling` method to tell Dask that we want to compute means in a three-period
    rolling window. Since the periods in this example are months, Dask will average
    together three-month rolling windows of monthly citation counts. For example,
    for the month of March 2017, Dask will compute the mean of the counts for March
    2017, February 2017, and January 2017\. This means that, by default, the number
    of periods you specify, *n,* will represent a window that includes the current
    period (March) and *n – 1* periods before it (February and January). Let’s take
    a look at what effect this has on the output, which can be seen in [figure 6.18](#figure6.18).
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 6.29](#listing6.29)中，你可以看到内置的滚动函数是多么简单！在应用看似的聚合函数之前，我们已链式调用了一个`rolling`方法来告诉
    Dask 我们想要在一个三期的滚动窗口中计算平均值。由于本例中的周期是月份，Dask 将将三个月的滚动窗口的月度引用次数平均在一起。例如，对于 2017 年
    3 月，Dask 将计算 2017 年 3 月、2017 年 2 月和 2017 年 1 月的计数平均值。这意味着，默认情况下，你指定的周期数 *n* 将代表一个包括当前周期（3
    月）及其之前的 *n – 1* 个周期（2 月和 1 月）的窗口。让我们看看这会对输出产生什么影响，这可以在[图 6.18](#figure6.18)中看到。
- en: Notice that the first two periods are `NaN` (missing) values. This is because
    the calculation for the month of February 2014 should include both January 2014
    and December 2013, but our dataset doesn’t have December 2013 in it. Instead of
    computing a partial value for periods with missing data, Dask will instead return
    a `NaN` value to signify that the true value is currently unknown. When using
    a rolling function, the result will always be *n – 1* number of rows smaller than
    the input dataset because of the nature of missing values in early windows.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，前两个周期是`NaN`（缺失）值。这是因为 2014 年 2 月的计算应包括 2014 年 1 月和 2013 年 12 月，但我们的数据集没有
    2013 年 12 月的数据。对于有缺失数据的时间段，Dask 不会计算部分值，而是返回一个`NaN`值来表示真实值目前未知。当使用滚动函数时，由于早期窗口中缺失值的性质，结果将始终比输入数据集少
    *n – 1* 行。
- en: If you would like to include both trailing and leading periods in your calculation,
    you can do that by setting the `center` parameter of the rolling method. This
    will cause Dask to compute a window that includes *n/2* periods before the current
    value and *n/2* periods after the current value. For example, if we used a three-period
    centered window, for the month of March 2017 our mean would include the count
    for February 2017, March 2017, and April 2017—one period in the past and one period
    in the future.
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在计算中包含尾随和领先期，可以通过设置滚动方法的`center`参数来实现。这将导致Dask计算一个窗口，该窗口包括当前值之前和之后的*n/2*个周期。例如，如果我们使用了一个三周期的中心窗口，对于2017年3月，我们的平均值将包括2017年2月、3月和4月的计数——一个过去周期和一个未来周期。
- en: Listing 6.30 Using a centered window
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.30 使用中心窗口
- en: '[PRE101]'
  id: totrans-726
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[Listing 6.30](#listing6.30) demonstrates centering, which will produce the
    output shown in [figure 6.19](#figure6.19).'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表6.30](#listing6.30) 展示了中心化，这将产生[图6.19](#figure6.19)中所示的输出。'
- en: '![c06_19.eps](Images/c06_19.png)'
  id: totrans-728
  prefs: []
  type: TYPE_IMG
  zh: '![c06_19.eps](Images/c06_19.png)'
- en: '[Figure 6.19](#figureanchor6.19) Output of [listing 6.30](#listing6.30) showing
    MonthYear and the mean number of parking citations'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.19](#figureanchor6.19) [列表6.30](#listing6.30)的输出，显示月度和停车罚单的平均数量'
- en: As you can see in the figure, with centering we only lose the first row instead
    of the first two rows. Whether this is appropriate or not again depends on what
    problem you’re trying to solve. You can also do much more than rolling means.
    In fact, every built-in aggregate function is also available as a rolling function,
    such as `sum` and `count`. You may also implement your own custom rolling functions
    by using either `apply` or `map_overlap` in the same way you would with a normal
    DataFrame or Series.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 如图中所示，使用中心化我们只丢失第一行而不是前两行。这是否合适再次取决于您试图解决的问题。您还可以做很多比滚动平均值更多的事情。实际上，每个内置的聚合函数都可以作为滚动函数使用，例如`sum`和`count`。您还可以通过使用`apply`或`map_overlap`以与正常DataFrame或Series相同的方式实现自己的自定义滚动函数。
- en: 'Now that you have some tools for numerically describing and analyzing data
    in Dask, it’s a good opportunity to turn our attention to another aspect of data
    analysis that’s just as important: visualization. If you looked at the unabbreviated
    results of the code in [listing 6.30](#listing6.30), you could see that trends
    look numerically inconclusive. With some high points and some low points, the
    number of citations in June 2017 ends up not far off from the number of citations
    in June 2014\. At times like these, it’s much easier to spot trends and patterns
    visually than just staring at the numbers. It can also be more intuitive to understand
    descriptive statistics and correlations through visualization. So, in the next
    chapter, we’ll pick up right where we left off by looking for trends in citations
    issued per month. However, we’ll use the power of visualization to try to make
    our job easier!'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经有一些工具可以用来在Dask中数值描述和分析数据，这是一个很好的机会将我们的注意力转向数据分析的另一个同样重要的方面：可视化。如果您查看[列表6.30](#listing6.30)代码的不缩写结果，您会看到趋势在数值上并不明确。在某些高点和一些低点之间，2017年6月的罚单数量最终并没有远低于2014年6月的罚单数量。在这些时候，通过视觉识别趋势和模式比仅仅盯着数字要容易得多。通过可视化理解描述性统计和相关性也可能更加直观。因此，在下一章中，我们将继续寻找每月发放的罚单趋势，但我们将利用可视化的力量来尝试使我们的工作更加容易！
- en: Summary
  id: totrans-732
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Dask DataFrames have a number of useful statistical methods such as `mean`,
    `min`, `max`, and so on. Even more statistical methods can be found in the dask.array.stats
    package.
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dask DataFrame拥有许多有用的统计方法，例如`mean`、`min`、`max`等。更多统计方法可以在dask.array.stats包中找到。
- en: Basic descriptive statistics can be produced for a DataFrame or Series by using
    the `describe` method.
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用`describe`方法，可以为DataFrame或Series生成基本描述性统计。
- en: Aggregate functions use the split-apply-combine algorithm to process data in
    parallel. Aggregating on a sorted column of a DataFrame will yield the best performance.
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合函数使用分割-应用-组合算法并行处理数据。在DataFrame的排序列上聚合将产生最佳性能。
- en: Correlation analysis compares two continuous variables, whereas the t-test compares
    a continuous variable across a categorical variable.
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关性分析比较两个连续变量，而t检验比较一个连续变量与一个分类变量之间的关系。
- en: You can use the `Aggregate` object to define your own aggregate functions.
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用`Aggregate`对象来定义自己的聚合函数。
- en: You can use rolling functions to analyze trends across a time index, such as
    moving averages. You should partition the data by time period for the best performance.**  **#
    7
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用滚动函数来分析时间索引上的趋势，例如移动平均。为了获得最佳性能，您应该按时间周期对数据进行分区。**  **# 7
- en: Visualizing DataFrames with Seaborn
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Seaborn 可视化 DataFrame
- en: '**This chapter covers**'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章涵盖**'
- en: Using the prepare-collect-plot-reduce pattern to overcome the challenges of
    visualizing large datasets
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用准备-收集-绘图-减少模式来克服可视化大型数据集的挑战
- en: Visualizing continuous relationships using `seaborn.scatterplot` and `seaborn.regplot`
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `seaborn.scatterplot` 和 `seaborn.regplot` 可视化连续关系
- en: Visualizing groups of continuous data using Seaborn `seaborn.violinplot`
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Seaborn 可视化连续数据组 `seaborn.violinplot`
- en: Visualizing patterns in categorical data using `seaborn.heatmap`
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `seaborn.heatmap` 可视化分类数据中的模式
- en: In the previous chapter, we performed some basic analyses of the NYC Parking
    Ticket data by looking at descriptive statistics and some other numerical properties
    of the dataset. While describing data numerically is precise, the results can
    be somewhat difficult to interpret and are generally not intuitive. On the other
    hand, we humans are very good at detecting and understanding patterns in visual
    information. Incorporating visualization into our analyses can help us better
    understand the general makeup of our dataset as well as how different variables
    interact with one another.
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们通过查看描述性统计和一些其他数值属性对纽约市停车罚单数据进行了基本分析。虽然用数字描述数据是精确的，但结果可能有些难以解释，并且通常不是直观的。另一方面，我们人类在检测和理解视觉信息中的模式方面非常擅长。将可视化纳入我们的分析中可以帮助我们更好地理解数据集的整体构成以及不同变量之间是如何相互作用的。
- en: 'For example, consider the relationship between average temperature and the
    number of citations issued that we explored in the previous chapter. We calculated
    a Pearson correlation coefficient of about 0.14\. We concluded that the two variables
    have a weak positive correlation, meaning we should expect the number of citations
    issued to increase slightly as the average temperature rises. Given our findings,
    should we expect global climate change to become a lucrative phenomenon for the
    city of New York? Or does the nature of the relationship change depending on which
    range of values we’re looking at? A simple correlation coefficient can’t convey
    all that information. We’ll come back to that question momentarily. For now, to
    really highlight why visualization is such an important part of data analysis,
    let’s look at a classic problem in statistics known as *Anscombe’s quartet*. Anscombe’s
    quartet is a hypothetical dataset presented in 1973 by an English statistician
    named Francis Anscombe, who was frustrated by his field’s lack of appreciation
    for visualization. He wanted to demonstrate that numerical methods alone don’t
    always tell the full story: each of the four datasets that make up his quartet
    share the same mean, variance, correlation, regression line, and coefficient of
    determination. [Figure 7.1](#figure7.1) displays the four datasets that make up
    Anscombe’s quartet.'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑我们在上一章中探讨的平均温度和发出的罚单数量之间的关系。我们计算了大约 0.14 的皮尔逊相关系数。我们得出结论，这两个变量之间存在弱正相关，这意味着我们应该预计随着平均温度的上升，发出的罚单数量会略有增加。鉴于我们的发现，我们应该预计全球气候变化将成为纽约市的盈利现象吗？或者关系的性质取决于我们正在查看的值范围？简单的相关系数无法传达所有这些信息。我们稍后会回到这个问题。现在，为了真正强调可视化在数据分析中的重要性，让我们看看统计学中的一个经典问题，即*安斯康姆四重奏*。安斯康姆四重奏是1973年由一位名叫弗朗西斯·安斯康姆的英国统计学家提出的假设数据集，他对他的领域对可视化的缺乏欣赏感到沮丧。他想证明仅用数值方法并不总是能讲全故事：组成他的四重奏的四个数据集具有相同的均值、方差、相关系数、回归线和确定系数。[图
    7.1](#figure7.1) 显示了组成安斯康姆四重奏的四个数据集。
- en: '![c07_01.png](Images/c07_01.png)'
  id: totrans-747
  prefs: []
  type: TYPE_IMG
  zh: '![c07_01.png](Images/c07_01.png)'
- en: '[Figure 7.1](#figureanchor7.1) Anscombe’s quartet highlights the importance
    of using visualization for effective data analysis.'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7.1](#figureanchor7.1) 安斯康姆四重奏强调了使用可视化进行有效数据分析的重要性。'
- en: If we relied on numerical methods alone, we might conclude that each of the
    four datasets is completely identical. However, graphically, they tell a different
    story. Two of the datasets, X[3] and X[4], have extreme outliers. Dataset X[4][appears
    to have an undefined slope, and dataset X[2] appears to be a nonlinear, perhaps
    parabolic function. Consequently, the appropriate methods we might use to numerically
    describe and analyze each of the four datasets would be completely different after
    learning more about the data through visualization.]
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仅依赖数值方法，我们可能会得出结论，这四个数据集完全相同。然而，从图形上看，它们讲述了一个不同的故事。其中两个数据集，X[3]和X[4]，有极端的异常值。数据集X[4]似乎有一个未定义的斜率，而数据集X[2]似乎是一个非线性，可能是抛物线函数。因此，在通过可视化了解更多关于数据的信息后，我们可能使用的适当方法来数值描述和分析这四个数据集将完全不同。
- en: Notice that Anscombe’s quartet is a very small dataset, so visualizing it as
    a set of scatterplots is simple and straightforward. However, visualizing large
    datasets can be tricky due to both the volume and variety of data. Because there
    are many kinds of visualizations to choose from, for all different kinds of data,
    we can’t possibly cover all kinds of visualizations in this book. Therefore, we
    will cover some general patterns and tactics that you can extend to produce many
    kinds of visualizations, and we will cover a few of the more common visualizations
    that are useful for analyzing structured data. As always, [figure 7.2](#figure7.2)
    shows what we’ve accomplished so far and where we’ll turn our focus next.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到安斯康姆四重奏是一个非常小的数据集，因此将其作为一组散点图进行可视化既简单又直接。然而，由于数据量和种类的多样性，可视化大型数据集可能会很棘手。由于有各种各样的可视化可供选择，针对不同类型的数据，我们不可能在这本书中涵盖所有类型的可视化。因此，我们将介绍一些通用的模式和策略，您可以将它们扩展以生成许多种类的可视化，并且我们将介绍一些对分析结构化数据有用的常见可视化。一如既往地，[图7.2](#figure7.2)展示了我们迄今为止所取得的成果以及我们接下来将关注的重点。
- en: '![c07_02.eps](Images/c07_02.png)'
  id: totrans-751
  prefs: []
  type: TYPE_IMG
  zh: '![c07_02.eps](Images/c07_02.png)'
- en: '[Figure 7.2](#figureanchor7.2) The *Data Science with Python and Dask* workflow'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.2](#figureanchor7.2) *使用Python和Dask进行数据科学*的工作流程'
- en: In this chapter, we’ll continue working on exploratory analysis and hypothesis
    formulation and testing, but with a focus on using visualizations to dig deeper
    into the analyses. We’ll also blend some data-manipulation techniques you learned
    in previous chapters, such as aggregations, with new techniques like sampling.
    All the computations to prepare the data for visualization will be performed with
    Dask using the DataFrame API.
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续进行探索性分析和假设的制定与测试，但重点是使用可视化深入分析。我们还将结合您在前面章节中学到的数据处理技术，例如聚合，以及新的技术如抽样。所有用于准备数据以进行可视化的计算都将使用Dask的DataFrame
    API执行。
- en: 7.1 The prepare-reduce-collect-plot pattern
  id: totrans-754
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 准备-减少-收集-绘图模式
- en: We face a few challenges when visualizing large datasets. From a technical perspective,
    plotting a large amount of data is, unsurprisingly, computationally intensive
    and can require a lot of memory. We’ve been able to cope with compute- and memory-hungry
    operations so far by distributing work across multiple workers using Dask, but
    we still need to eventually collect all the data we want to plot on the screen
    back to a single thread to render it on the screen. This means that if the size
    of the dataset we want to plot is bigger than the memory we have on the client
    machine, we won’t be able to plot it. In the next chapter, we’ll look at a library
    called Datashader that overcomes these challenges in a novel way. However, Datashader
    doesn’t support some of the visualizations we’ll discuss in this chapter, so we’ll
    have to work around the technical issues.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 在可视化大型数据集时，我们面临一些挑战。从技术角度来看，绘制大量数据是意料之中的计算密集型操作，可能需要大量的内存。到目前为止，我们已经能够通过使用Dask在多个工作者之间分配工作来应对计算和内存密集型操作。但是，我们最终需要将所有我们想要在屏幕上绘制的想要的数据收集回单个线程以在屏幕上渲染。这意味着，如果我们想要绘制的数据集的大小超过了客户端机器上的内存，我们就无法绘制它。在下一章中，我们将探讨一个名为Datashader的库，它以新颖的方式克服了这些挑战。然而，Datashader不支持我们将在本章讨论的一些可视化，因此我们需要解决技术问题。
- en: The other thing we have to keep in mind when plotting large datasets is that
    the value of visualization comes from the ability to quickly and intuitively discern
    insights from the data. However, it can be quite easy to become overwhelmed when
    faced with a large amount of data. Take a look at the scatterplot in [figure 7.3](#figure7.3).
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: 在绘制大型数据集时，我们还需要注意的另一件事是，可视化的价值来自于快速直观地从数据中获取洞察力的能力。然而，面对大量数据时，可能会很容易感到不知所措。看看[图7.3](#figure7.3)中的散点图。
- en: '![c07_03.png](Images/c07_03.png)'
  id: totrans-757
  prefs: []
  type: TYPE_IMG
  zh: '![c07_03.png](Images/c07_03.png)'
- en: '[Figure 7.3](#figureanchor7.3) An example of a very dense scatterplot'
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.3](#figureanchor7.3) 非常密集的散点图示例'
- en: 'It’s really hard to tell what’s going on at all in the scatterplot because
    so many individual points have been plotted on it. It simply looks like random
    noise! Applying some color coding could make it possible to see a few distinct
    regions in the data, but it would still be difficult to see where those regions
    begin and end and where they overlap with one another. This is a hallmark issue
    with visualizing large datasets: with so much data, individual data points stop
    being useful to analyze individually. Rather, we need to distill broad patterns
    and behaviors out of the data. We could do this in a number of ways, including
    clustering, aggregating, or sampling. Any of these three techniques can be used
    to make the data much easier to understand.'
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 在散点图中几乎无法判断发生了什么，因为图上已经绘制了如此多的单独点。它看起来就像随机噪声！应用一些颜色编码可以使看到数据中的几个不同区域成为可能，但仍然很难看到这些区域开始和结束的位置，以及它们相互重叠的地方。这是可视化大型数据集的一个典型问题：在如此多的数据中，单个数据点不再对单独分析有用。相反，我们需要从数据中提炼出广泛的模式和行为。我们可以通过多种方式做到这一点，包括聚类、聚合或抽样。这三种技术中的任何一种都可以用来使数据更容易理解。
- en: '![c07_04.eps](Images/c07_04.png)'
  id: totrans-760
  prefs: []
  type: TYPE_IMG
  zh: '![c07_04.eps](Images/c07_04.png)'
- en: '[Figure 7.4](#figureanchor7.4) Applying a reduction technique like clustering
    can make data easier to interpret.'
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.4](#figureanchor7.4) 应用聚类等降维技术可以使数据更容易解释。'
- en: '[Figure 7.4](#figure7.4) displays a different dataset, which still has a fairly
    large number of points, but it has also had a clustering technique applied to
    it. This results in a clear division of the data into three distinct regions.
    Even though a large number of points are still on this plot, it can be easier
    to interpret the data because it’s possible to conceptually subdivide our analyses
    into explaining the behavior of three separate groups.'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.4](#figure7.4) 显示了不同的数据集，该数据集仍然有相当多的点，但它也应用了聚类技术。这导致数据被清晰地分为三个不同的区域。尽管图上仍然有大量点，但由于可以概念上将我们的分析细分为解释三个不同组的行为，因此解释数据可能更容易。'
- en: To overcome the technical and conceptual challenges with plotting large datasets,
    we’ll use the prepare-reduce-collect-plot pattern for the remainder of the chapter
    to produce visualizations from Dask DataFrames. Ultimately, the objective of the
    pattern is to transform the raw, large dataset down to a smaller subset specific
    to the needs of the visualization we want to produce, and to do as much of the
    work as possible with Dask.
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服绘制大型数据集的技术和概念挑战，我们将在本章的剩余部分使用准备-降维-收集-绘图模式来从Dask DataFrame生成可视化。最终，该模式的目的是将原始的大型数据集转换为更小的子集，该子集针对我们想要生成的可视化需求，并且尽可能多地使用Dask来完成工作。
- en: '![c07_05.eps](Images/c07_05.png)'
  id: totrans-764
  prefs: []
  type: TYPE_IMG
  zh: '![c07_05.eps](Images/c07_05.png)'
- en: '[Figure 7.5](#figureanchor7.5) The prepare-reduce-collect-plot pattern for
    visualizing large datasets with Dask'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.5](#figureanchor7.5) 使用Dask可视化大型数据集的准备-降维-收集-绘图模式'
- en: '[Figure 7.5](#figure7.5) shows the process step by step. We start the first
    step, Prepare, by identifying what kind of visualization would be appropriate
    for the question we’d like to answer. Are we interested in the relationship between
    two continuous variables? Choose a scatterplot. Interested in seeing counts of
    items by category? A bar chart works well. Once we’ve identified the type of visualization
    to produce, think about what you need to display on the axes. This will dictate
    the columns we need to select out of the DataFrame. Also we need to consider if
    it’s necessary to filter the data at all. Perhaps we want to focus our analysis
    on a certain category. To do that, we will need to write a filter condition, which
    was covered in chapter 5, to select data only from that category.'
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.5](#figure7.5) 展示了逐步的过程。我们首先从“准备”步骤开始，确定针对我们想要回答的问题，哪种可视化方式是合适的。我们是否对两个连续变量之间的关系感兴趣？选择散点图。如果我们想查看按类别划分的项目计数，条形图会是一个不错的选择。一旦我们确定了要生成的可视化类型，就要考虑需要在坐标轴上显示什么。这将决定我们需要从DataFrame中选择的列。此外，我们还需要考虑是否需要过滤数据。也许我们想要将分析集中在某个特定类别上。为此，我们需要编写一个过滤条件，这在第5章中已有介绍，以仅从该类别选择数据。'
- en: 'The Reduce step is all about choosing an appropriate reduction method. Generally,
    you have two choices: aggregate the data into groups that make sense for the question
    we’re trying to answer, such as summing or averaging all observations by month,
    or take a random sample of the data. Choosing to aggregate the data is normally
    a natural result of the question we’re trying to answer. For example, in the NYC
    Parking Ticket dataset, we started with approximately 15 million citations. Since
    we wanted to know how many citations were issued per month, we were able to reduce
    15 million data points down to less than 50 data points by grouping the data by
    month, then counting the number of citations. If we wanted to look at this data
    at the daily or hourly level instead, we would still be able to reduce the original
    15 million citations to a much smaller number of data points. If it’s not possible
    to aggregate the data in any meaningful way, or it’s not appropriate for the question
    we’re trying to answer, sampling can be used to focus in on a predetermined number
    of data points. However, since random sampling relies on random chance, it’s possible
    that the randomly sampled data points won’t form a realistic picture of the underlying
    behaviors in the data. This is especially likely to happen if we take a very small
    sample, so random sampling should be used with caution!'
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: “减少”步骤完全是关于选择一个合适的方法。通常，你有两个选择：将数据聚合到对我们试图回答的问题有意义的组中，例如按月份对所有的观察值求和或平均，或者对数据进行随机抽样。选择聚合数据通常是试图回答的问题的自然结果。例如，在纽约市停车罚单数据集中，我们开始时有大约1500万个罚单。由于我们想知道每月发出了多少罚单，我们能够通过按月份分组将1500万个数据点减少到不到50个数据点，然后计算罚单的数量。如果我们想以每日或每小时的水平查看这些数据，我们仍然可以将原始的1500万个罚单减少到更少的数据点。如果无法以任何有意义的方式聚合数据，或者这不适合我们试图回答的问题，可以使用抽样来集中在预定的数据点上。然而，由于随机抽样依赖于随机机会，随机抽样的数据点可能无法真实反映数据中的潜在行为。这种情况在抽取样本非常小的时候尤其可能发生，因此在使用随机抽样时应该谨慎！
- en: The third step in the process, Collect, is where the computation is executed
    and the result is turned into a single Pandas DataFrame. From there, we can use
    the reduced data with any plotting package. The final step, Plot, is where the
    visualization plotting methods are called and display options (such as plot titles,
    colors, size, and so on) are set. This step is not distributed since we’ve gathered
    all the data into one place during the Collect step.
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: 过程中的第三步“收集”是执行计算并将结果转换为一个单一的Pandas DataFrame的地方。从那里，我们可以使用减少后的数据与任何绘图包一起使用。最后一步“绘图”是调用可视化绘图方法并设置显示选项（如标题、颜色、大小等）的地方。这一步没有分布，因为我们已经在“收集”步骤中将所有数据集中到了一个地方。
- en: Now let’s take a look at a few examples of using this pattern to visualize some
    of the variables in the NYC Parking Ticket dataset. In these examples, we’ll use
    Seaborn to produce the visualizations. Seaborn is part of the Python Open Data
    Science Stack and is a data visualization library based on Matplotlib, another
    popular visualization library. Seaborn provides a nice wrapper around Matplotlib
    that lets you easily produce common statistical data visualizations like regression
    plots, box plots, and scatterplots. If you haven’t already installed Seaborn,
    you can find installation instructions in the appendix.
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看使用此模式来可视化纽约市停车罚单数据集中的一些变量的几个示例。在这些示例中，我们将使用 Seaborn 来生成可视化。Seaborn 是
    Python 开放数据科学栈的一部分，是一个基于 Matplotlib 的数据可视化库，Matplotlib 是另一个流行的可视化库。Seaborn 提供了一个很好的
    Matplotlib 包装器，让你可以轻松地生成常见的统计数据可视化，如回归图、箱线图和散点图。如果你还没有安装 Seaborn，你可以在附录中找到安装说明。
- en: 7.2 Visualizing continuous relationships with scatterplot and regplot
  id: totrans-770
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 使用散点图和 regplot 可视化连续关系
- en: We’ll now take a look at using the prepare-reduce-collect-plot pattern by returning
    to the correlation analysis between monthly average temperature and monthly citations
    issued that we looked at in the previous chapter. Given a Pearson correlation
    of 0.14, we wouldn’t expect there to be much of a relationship, but let’s see
    if Pearson correlation actually tells the whole story or not.
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过回到我们在上一章中查看的月平均温度和月度引用数量之间的相关性分析，来查看使用 prepare-reduce-collect-plot 模式的示例。给定
    0.14 的皮尔逊相关系数，我们不会期望两者之间有太大的关系，但让我们看看皮尔逊相关系数是否真的讲述了整个故事。
- en: 7.2.1 Creating a scatterplot with Dask and Seaborn
  id: totrans-772
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 使用 Dask 和 Seaborn 创建散点图
- en: First, we’ll begin as always by importing the relevant modules and loading in
    the data we saved at the end of chapter 5.
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将像往常一样开始导入相关模块，并加载我们在第 5 章末保存的数据。
- en: Listing 7.1 Importing modules and data
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.1 导入模块和数据
- en: '[PRE102]'
  id: totrans-775
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'We’ll go ahead and import Seaborn and Matplotlib as well here, since we will
    use it momentarily to set some display options for the final plot. A brief side
    note if this is your first time using Seaborn: you will frequently see calls to
    Matplotlib together with Seaborn code. Since Seaborn relies on Matplotlib’s plotting
    engine (pyplot), controlling aspects of the rendered visualizationlike figure
    size and axis limits is done directly through the pyplot API.'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里继续导入 Seaborn 和 Matplotlib，因为我们很快将使用它们来设置最终图表的一些显示选项。如果这是你第一次使用 Seaborn，请注意：你经常会看到
    Matplotlib 和 Seaborn 代码一起调用。由于 Seaborn 依赖于 Matplotlib 的绘图引擎（pyplot），控制渲染的可视化方面（如图大小和坐标轴限制）是通过
    pyplot API 直接完成的。
- en: To look at the relationship between monthly average temperature and number of
    citations, we’ll need to get both the average monthly temperature and a count
    of citations grouped by month/year. Since the number of citations is likely to
    be several orders of magnitude larger than the average temperature, the temperature
    will be put on the x-axis and the number of citations will be put on the y-axis.
    We’ll also filter out any data after June 2017, because those months were not
    completely reported in this dataset. Now that we’ve identified the necessary data,
    we’ll produce the code to prepare and aggregate the data.
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看月平均温度和引用数量之间的关系，我们需要获取按月份/年份分组的平均月温度和引用次数的计数。由于引用次数可能比平均温度大几个数量级，温度将被放在 x
    轴上，而引用次数将被放在 y 轴上。我们还将过滤掉 2017 年 6 月之后的数据，因为这些月份在此数据集中并未完全报告。现在我们已经确定了必要的数据，我们将生成准备和聚合数据的代码。
- en: Listing 7.2 Preparing the reducing the data
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.2 准备数据
- en: '[PRE103]'
  id: totrans-779
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[Listing 7.2](#listing7.2) should look familiar, because we produced this data
    before to calculate the correlation coefficient. As before, we simply filter the
    data and apply the `agg` method (aggregate) over it to calculate the count of
    citations and the mean of the temperature grouped by month. Now we’re ready to
    collect and plot the data.'
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 7.2](#listing7.2) 应该看起来很熟悉，因为我们之前已经生成过这些数据来计算相关系数。和之前一样，我们只是过滤数据并对其应用 `agg`
    方法（聚合）来计算按月份分组的引用次数和温度平均值。现在我们已准备好收集和绘制数据。'
- en: Listing 7.3 Collecting and plotting the data
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.3 收集和绘制数据
- en: '[PRE104]'
  id: totrans-782
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'In [listing 7.3](#listing7.3), we start by setting the style settings for the
    scatterplot we’re about to produce. The `whitegrid` style produces a clean-looking
    figure that has a white background with gray x and y grid lines. Next, we create
    a blank figure using the `plt.subplots` function and specify the size we want.
    This is done to override the default pyplot figure size, which can be a bit small
    when displayed on high resolution screens. The next call to `seaborn.despine`
    is another aesthetic modification to the figure we’re about to produce. This simply
    removes the border box around the figure so all we can see are the gridlines in
    the plot area. Next, within the ProgressBar context (since we will be moving data
    from Dask to the local Python process), we call the `seaborn.scatterplot` function
    to plot the graph. Its required parameters are fairly simple: pass in a variable
    name each for the x and y axis as strings, and pass in a DataFrame to plot. In
    this example, we’ve also passed in our custom axes object we created to ensure
    the scatterplot will look the way it was configured to look. This parameter, however,
    is optional, and if you don’t pass an axes object, it will plot using defaults.
    In the last two lines, we’re setting the minimum of the y and x axes to 0 respectively.
    The reason these methods are called after the `seaborn.scatterplot` call is because
    Matplotlib will be able to calculate the maximum x and y size automatically. If
    you call these methods before plotting, Matplotlib won’t have had a chance to
    see the data and calculate the maxima. Therefore, you would need to pass an explicit
    maximum or the plot wouldn’t display correctly. After a bit of crunching, you
    should get a scatterplot that looks like [figure 7.6](#figure7.6).'
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 7.3](#listing7.3) 中，我们首先设置了即将生成的散点图样式设置。`whitegrid` 样式生成一个看起来干净的图形，具有白色背景和灰色
    x 和 y 网格线。接下来，我们使用 `plt.subplots` 函数创建一个空白图形并指定我们想要的尺寸。这样做是为了覆盖默认的 pyplot 图形大小，当在高分辨率屏幕上显示时可能会有些小。接下来调用
    `seaborn.despine` 是对我们即将生成的图形的另一种美学修改。这仅仅移除了图形周围的边框框，所以我们只能看到绘图区域中的网格线。接下来，在 ProgressBar
    上下文中（因为我们将从 Dask 移动数据到本地 Python 进程），我们调用 `seaborn.scatterplot` 函数来绘制图形。它所需参数相当简单：分别传递给
    x 和 y 轴的变量名作为字符串，并传递一个 DataFrame 来绘制。在这个例子中，我们还传递了我们创建的自定义坐标轴对象，以确保散点图看起来是按照配置的方式。然而，此参数是可选的，如果你不传递坐标轴对象，它将使用默认值进行绘制。在最后两行中，我们将
    y 和 x 轴的最小值分别设置为 0。这些方法在 `seaborn.scatterplot` 调用之后被调用，是因为 Matplotlib 将能够自动计算
    x 和 y 的最大值。如果你在绘图之前调用这些方法，Matplotlib 还没有机会看到数据并计算最大值。因此，你需要传递一个显式的最大值，否则图形将无法正确显示。经过一些计算后，你应该会得到一个看起来像
    [图 7.6](#figure7.6) 的散点图。
- en: '![c07_06.png](Images/c07_06.png)'
  id: totrans-784
  prefs: []
  type: TYPE_IMG
  zh: '![c07_06.png](Images/c07_06.png)'
- en: '[Figure 7.6](#figureanchor7.6) A scatterplot of average temperature vs. number
    of citations'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7.6](#figureanchor7.6) 平均温度与引用次数的散点图'
- en: '[Figure 7.6](#figure7.6) indeed shows that a positive relationship exists between
    the two variables. As we move from 30 degrees to 60 degrees, the number of citations
    generally increases. The points are scattered fairly far apart from each other,
    hence the indication of a weak correlation.'
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7.6](#figure7.6) 确实显示了两个变量之间存在正相关关系。当我们从 30 度移动到 60 度时，引用次数通常会增加。点彼此之间相当分散，因此表明相关性较弱。'
- en: 7.2.2 Adding a linear regression line to the scatterplot
  id: totrans-787
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 在散点图中添加线性回归线
- en: We can have Seaborn try to help us see the gradual pattern in the data better
    by plotting a regression plot instead of a scatterplot. To do that, we’ll use
    the `regplot` function. Its required parameters are the same as the `scatterplot`
    function, so it’s very easy to interchange the two.
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过绘制回归图而不是散点图来让 Seaborn 尝试帮助我们更好地看到数据的渐变模式。为此，我们将使用 `regplot` 函数。它所需参数与
    `scatterplot` 函数相同，因此两者很容易互换。
- en: Listing 7.4 Adding a regression line to a scatterplot using `regplot`
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.4 使用 `regplot` 在散点图中添加回归线
- en: '[PRE105]'
  id: totrans-790
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: In [listing 7.4](#listing7.4)., you can see that the only thing that changed
    is the call to `regplot` instead of `scatterplot`. We also added the optional
    parameter, `robust`. This parameter tells Seaborn to produce a robust regression.
    A robust regression minimizes the influence of outliers on the regression equation.
    This means that points on the y axis that sit far away from other points won’t
    pull the line we’re trying to draw through the points up (or down) by very much.
    This is a good thing since these points are unlikely to occur on a regular basis,
    so we should treat them as anomalies instead of observations that are likely to
    happen again. For example, take a look at the point in [figure 7.6](#figure7.6)
    that represents roughly 1.4 million citations. This happened during a very cold
    month when the average temperature was only about 30 degrees Fahrenheit. We can
    see that all the other months that were just as cold accrued only around 700,000
    citations. Some special circumstance must have happened during the month where
    1.4 million citations were issued, since this seems to be unusual. If we allowed
    the anomalous data point to influence our regression line, it would cause us to
    overestimate the number of citations we can expect to be issued in cold months.
    There seem to be a few outliers in this dataset, so using a robust regression
    would be a good idea. When running the code, you will get a regression plot that
    looks like [figure 7.7](#figure7.7).
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表7.4](#listing7.4)中，你可以看到唯一改变的是将`regplot`的调用替换为`scatterplot`。我们还添加了可选参数`robust`。这个参数告诉Seaborn生成一个稳健的回归。稳健回归最小化了异常值对回归方程的影响。这意味着y轴上远离其他点的点不会很大程度地拉动我们试图通过这些点绘制的线（向上或向下）。这是好事，因为这些点不太可能经常出现，所以我们应将其视为异常而不是可能再次发生的观察结果。例如，看看[图7.6](#figure7.6)中代表大约140万次引用的点。这发生在平均气温仅为约30华氏度的非常冷的月份。我们可以看到，其他同样寒冷的月份累积的引用量只有大约70万。在一个月内发放140万次引用，肯定发生了某些特殊情况，因为这看起来很不寻常。如果我们允许异常数据点影响我们的回归线，它会导致我们高估在寒冷月份可以发放的引用数量。这个数据集中似乎有几个异常值，所以使用稳健回归是个好主意。运行代码时，你会得到一个看起来像[图7.7](#figure7.7)的回归图。
- en: As you can see, a line has been plotted roughly through the center mass of the
    points. Keep in mind, however, that regression is a statistical estimation. Where
    the line was drawn could have been influenced by unusual observations (called
    *outliers*). The shaded area around the line represents the confidence interval,
    meaning that there is a 95% chance that the best-fitting line is somewhere inside
    that area. The line itself is the best “guess” given the data that was provided
    here.
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，一条线大致通过点的中心质量绘制。然而，请记住，回归是一种统计估计。线的绘制可能受到了异常观察（称为*异常值*）的影响。围绕线的阴影区域表示置信区间，这意味着有95%的几率最佳拟合线位于该区域内部。线本身是根据这里提供的数据的最佳“猜测”。
- en: '![c07_07.png](Images/c07_07.png)'
  id: totrans-793
  prefs: []
  type: TYPE_IMG
  zh: '![c07_07.png](Images/c07_07.png)'
- en: '[Figure 7.7](#figureanchor7.7) The scatterplot of average temperature vs. count
    of citations with an added regression line'
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.7](#figureanchor7.7) 平均温度与引用计数散点图，并添加了回归线'
- en: 7.2.3 Adding a nonlinear regression line to a scatterplot
  id: totrans-795
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 在散点图上添加非线性回归线
- en: 'However, it doesn’t look like it fits particularly well. On closer examination
    of the scatterplot, it appears that the number of citations actually starts gradually
    decreasing past about 60 degrees. This makes intuitive sense: perhaps fewer enforcement
    officers are sent on patrol in more extreme weather. This would be a great opportunity
    to follow up with management to see if our hypothesis is true, or if another explanation
    exists for the drop off in citations. Regardless, the relationship does not appear
    to be linear. Instead, a nonlinear equation, such as a parabola, might fit the
    data better.'
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它看起来并不特别适合。在更仔细地检查散点图后，似乎引用的数量实际上在60度左右开始逐渐减少。这从直观上是有道理的：也许在更极端的天气条件下，派出的执法官员巡逻次数更少。这将是一个很好的机会跟进管理层，看看我们的假设是否正确，或者是否存在其他解释来解释引用数量的下降。无论如何，这种关系似乎并不呈线性。相反，一个非线性方程，如抛物线，可能更适合数据。
- en: '![c07_08.eps](Images/c07_08.png)'
  id: totrans-797
  prefs: []
  type: TYPE_IMG
  zh: '![c07_08.eps](Images/c07_08.png)'
- en: '[Figure 7.8](#figureanchor7.8) The relationship appears to be non-linear; the
    relationship changes direction around 60 degrees.'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.8](#figureanchor7.8) 关系似乎是非线性的；关系在60度左右改变方向。'
- en: '[Figure 7.8](#figure7.8) shows why fitting a parabola to the data might result
    in a more accurate fit. As you can see, moving from 20 degrees to about 60 degrees,
    the relationship seems to be positive: as the temperature increases, the number
    of citations written also increases. However, around the 60 degrees mark, the
    relationship seems to reverse direction. As temperature increases from 60 degrees,
    the number of citations seems to decrease overall. Seaborn supports nonlinear
    curve fitting in regression plots with a few parameter adjustments.'
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.8](#figure7.8) 展示了为什么将抛物线拟合到数据可能会得到更准确的拟合。正如你所看到的，从20度到大约60度，关系似乎呈正相关：随着温度的升高，撰写的引用数量也增加。然而，在60度左右，关系似乎发生了方向上的反转。当温度从60度升高时，引用的数量似乎总体上有所减少。Seaborn通过一些参数调整支持回归图中的非线性曲线拟合。'
- en: Listing 7.5 Fitting a nonlinear curve to a dataset
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.5 将非线性曲线拟合到数据集
- en: '[PRE106]'
  id: totrans-801
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'In [listing 7.5](#listing7.5)., the `robust` parameter was replaced with the
    `order` parameter. The `order` parameter determines how many terms to use in order
    to fit a nonlinear curve. Since the data looks roughly parabolic, an order of
    2 is used (you might recall from high school algebra that a parabola has two terms:
    an *x²* term and an *x* term). This produces a regression plot that looks like
    [figure 7.9](#figure7.9).'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表7.5](#listing7.5)中，`robust`参数被替换为`order`参数。`order`参数决定了拟合非线性曲线时使用多少项。由于数据看起来大致呈抛物线形状，因此使用2阶（你可能还记得高中代数中抛物线有两个项：一个*x²*项和一个*x*项）。这产生了一个看起来像[图7.9](#figure7.9)的回归图。
- en: '![c07_09.png](Images/c07_09.png)'
  id: totrans-803
  prefs: []
  type: TYPE_IMG
  zh: '![c07_09.png](Images/c07_09.png)'
- en: '[Figure 7.9](#figureanchor7.9) The scatterplot of average temperature vs. count
    of citations fit with a nonlinear curve'
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.9](#figureanchor7.9) 平均温度与引用计数拟合的非线性曲线散点图'
- en: '[Figure 7.9](#figure7.9) appears to fit the data better than the previous linear
    regression. We’ve also learned something that we would have missed if we only
    looked at the Pearson correlation coefficient! Next, we’ll have a look at visualizing
    relationships in categorical data.'
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.9](#figure7.9) 看起来比之前的线性回归更适合数据。我们还学到了一些，如果我们只看皮尔逊相关系数，我们可能会错过的东西！接下来，我们将看看如何可视化分类数据中的关系。'
- en: 7.3 Visualizing categorical relationships with violinplot
  id: totrans-806
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 使用提琴图可视化分类关系
- en: 'The NYC Parking Ticket data is full of many categorical variables, which presents
    an excellent opportunity to demonstrate a very useful visualization for analyzing
    categorical relationships: the violinplot. An example of a violinplot can be seen
    in [figure 7.10](#figure7.10).'
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: 纽约市停车罚单数据充满了许多分类变量，这为展示一个非常有用的可视化分析分类关系的工具提供了极好的机会：提琴图。一个提琴图的例子可以在[图7.10](#figure7.10)中看到。
- en: '![c07_10.eps](Images/c07_10.png)'
  id: totrans-808
  prefs: []
  type: TYPE_IMG
  zh: '![c07_10.eps](Images/c07_10.png)'
- en: '[Figure 7.10](#figureanchor7.10) A violinplot showing the relative distributions
    of categorical data'
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.10](#figureanchor7.10) 展示了分类数据的相对分布的提琴图'
- en: Violinplots are similar to boxplots in that they are a visual representation
    of several statistical properties of a variable including the mean, median, 25th
    percentile, 75th percentile, minimum, and maximum. But violionplots also incorporate
    a histogram into the plot, so you can determine what the distribution of the data
    looks like and where the most frequently occurring points are. Like boxplots,
    violinplots are used to compare behaviors of a continuous variable across groups.
    For example, we might want to know how the age of a vehicle relates to the color
    of a vehicle. Do black cars tend to be newer or older? Would you be more likely
    to receive a ticket in a new red car or an old green car? A violinplot will help
    us investigate these questions.
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 提琴图与箱线图类似，因为它们都是变量几个统计属性的视觉表示，包括均值、中位数、25百分位数、75百分位数、最小值和最大值。但提琴图还把直方图纳入图中，因此你可以确定数据的分布情况以及最频繁出现的点在哪里。像箱线图一样，提琴图用于比较组间连续变量的行为。例如，我们可能想知道车辆年龄与车辆颜色之间的关系。黑色汽车倾向于较新还是较旧？你更有可能在新的红色汽车还是旧的绿色汽车中收到罚单？提琴图将帮助我们调查这些问题。
- en: 7.3.1 Creating a violinplot with Dask and Seaborn
  id: totrans-811
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 使用Dask和Seaborn创建提琴图
- en: As we did in the previous example, we’ll follow the prepare-reduce-collect-plot
    pattern to produce the violinplots. The data we need are the vehicle ages and
    vehicle colors recorded on each citation. However, in this case, there aren’t
    any logical ways to pre-aggregate the data into smaller groups. To produce the
    descriptive statistics and histogram, we need the raw observations. Therefore,
    we will turn to sampling to help us reduce our dataset.
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的例子中所做的那样，我们将遵循准备-减少-收集-绘图的模式来生成violinplot。我们需要的数据是每个违章记录上记录的车辆年龄和车辆颜色。然而，在这种情况下，没有逻辑方法可以将数据预聚合到更小的组中。为了生成描述性统计和直方图，我们需要原始观测值。因此，我们将转向抽样来帮助我们减少数据集。
- en: 'For the sake of example, we’ll narrow our analysis to the top six most common
    vehicle colors: black, white, grey, red, blue, and green. This will allow us to
    produce a violinplot without using sampling first, so we can compare what the
    violinplot looks like using the entire dataset and what the violinplot looks like
    using a random sample of the data.'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: 为了举例说明，我们将我们的分析缩小到最常见的六种车辆颜色：黑色、白色、灰色、红色、蓝色和绿色。这将允许我们不需要先进行抽样就生成violinplot，这样我们就可以比较使用整个数据集和使用数据随机样本的violinplot看起来像什么。
- en: Listing 7.6 Reading in and filtering the data
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.6 读取和筛选数据
- en: '[PRE107]'
  id: totrans-815
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: For this example, we’re also reusing some data we produced in chapter 6\. This
    was the example where we calculated the difference between the Vehicle Year and
    the Citation Date for each citation to determine how old the vehicle was when
    it received a citation. In [listing 7.6](#listing7.6), we read in the data, select
    the relevant columns, and filter for the top vehicle colors. Next, we’ll take
    a quick count to determine how many observations we have.
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们也在重用我们在第6章中生成的一些数据。这是那个我们计算每个违章记录的车辆年份和违章日期之间的差异的例子，以确定车辆在收到违章通知时的年龄。在[列表7.6](#listing7.6)中，我们读取数据，选择相关列，并筛选出最常见车辆颜色。接下来，我们将快速计数以确定我们有多少个观测值。
- en: Listing 7.7 Counting the observations
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.7 计数观测值
- en: '[PRE108]'
  id: totrans-818
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: With 4,972,085 observations, we should be able to get a decently representative
    sample by randomly sampling 1% of the observations without replacement. First,
    we’ll see what the violinplot for all 4.97 million points looks like.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: 有4,972,085个观测值，我们应该能够通过随机抽取1%的观测值（不重复抽取）来获得一个相当有代表性的样本。首先，我们将看到所有4.97百万个点的violinplot是什么样的。
- en: Listing 7.8 Creating the violinplot
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.8 创建violinplot
- en: '[PRE109]'
  id: totrans-821
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: Once again, we start by setting up the figure and axes as before. We then put
    the colors in a list so we can tell Seaborn how to arrange the groups on the violinplot.
    Then, inside the `ProgressBar` context, we call the `seaborn.violinplot` function
    to produce the violinplot. The parameters should look familiar, because they are
    the same as both `scatterplot` and `regplot`. You can also see where we pass in
    the list of colors we defined. The `order` parameter allows you to specify a custom
    order to display the groups in from left to right. Otherwise, the choice is random.
    It’s good to use a consistent sort order if you plan to compare multiple instances
    of violinplots over the same groups. We also use the same list in the `palette`
    parameter to ensure the colors of the violinplot match the vehicle colors they
    represent (the red violinplot will be colored red, and so forth) After a bit of
    crunching, you will get a plot that looks like [figure 7.11](#figure7.11).
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们首先设置图和坐标轴，就像之前一样。然后我们将颜色放入一个列表中，这样我们就可以告诉Seaborn如何在violinplot上排列组。然后，在`ProgressBar`上下文中，我们调用`seaborn.violinplot`函数来生成violinplot。参数看起来应该很熟悉，因为它们与`scatterplot`和`regplot`相同。您还可以看到我们传递了定义的颜色列表。`order`参数允许您指定从左到右显示组的自定义顺序。否则，选择是随机的。如果您计划比较同一组中多个violinplot的实例，使用一致的排序顺序是很好的。我们还在`palette`参数中使用相同的列表，以确保violinplot的颜色与它们代表的车辆颜色相匹配（红色violinplot将是红色的，依此类推）。经过一些计算，您将得到一个看起来像[图7.11](#figure7.11)的图表。
- en: '![c07_11.png](Images/c07_11.png)'
  id: totrans-823
  prefs: []
  type: TYPE_IMG
  zh: '![c07_11.png](Images/c07_11.png)'
- en: '[Figure 7.11](#figureanchor7.11) A violinplot of vehicle color vs. vehicle
    age'
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.11](#figureanchor7.11) 车辆颜色与车辆年龄的violinplot'
- en: By looking at the violinplot in [figure 7.11](#figure7.11), we can see that
    red, blue, white, and grey vehicles have roughly the same median age (denoted
    by the white dot), whereas black vehicles tend to be newer and green vehicles
    tend to be older. All vehicle colors have roughly the same maximum age, but there
    are more instances of old red and green vehicles than other colors, which is denoted
    by a thicker line in the upper regions of the red and green plots. Wider areas
    indicate a higher number of observations, and narrower regions indicate fewer
    observations. The spikiness of the white plot looks particularly interesting,
    because it looks like white vehicles with an odd age are less common that vehicles
    with an even age. This might be worth looking into further to understand why.
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看[图 7.11](#figure7.11)中的小提琴图，我们可以看到红色、蓝色、白色和灰色车辆的年龄中位数（用白色点表示）大致相同，而黑色车辆倾向于较新，绿色车辆倾向于较老。所有车辆颜色的最大年龄大致相同，但旧红色和绿色车辆比其他颜色的实例更多，这在红色和绿色图表的上部区域用较粗的线表示。较宽的区域表示观察到的数量更多，较窄的区域表示观察到的数量更少。白色图表的尖锐度看起来特别有趣，因为它看起来像奇数年龄的白色车辆比偶数年龄的车辆更不常见。这可能值得进一步研究以了解原因。
- en: 7.3.2 Randomly sampling data from a Dask DataFrame
  id: totrans-826
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 从 Dask DataFrame 中随机采样数据
- en: Now let’s compare this plot to a plot of a random sample of the data. We’ll
    keep the plotting code the same, but we’ll grab a 1% random sample from our filtered
    DataFrame.
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将这个图表与数据的一个随机样本的图表进行比较。我们将保持绘图代码不变，但我们将从我们的过滤 DataFrame 中获取 1% 的随机样本。
- en: Listing 7.9 Sampling the filtered DataFrame
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.9 采样过滤 DataFrame
- en: '[PRE110]'
  id: totrans-829
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: Sampling from a Dask DataFrame is quite simple; use the `sample` method on any
    DataFrame and specify the percentage of the data you want to sample, and you will
    get a filtered DataFrame roughly the size of the percentage you specify. By default,
    sampling is performed *without* replacement. This means once a vehicle record
    is selected out of the DataFrame, that same vehicle record can’t be selected again
    in the same sample. This ensures that all the observations in your sample are
    unique. If you want to sample *with* replacement, you can specify that using the
    Boolean `replace` parameter. Take note, you can only specify the sample size in
    percentage terms—it’s not possible to specify the exact number of items in the
    returned sample. Therefore, you’ll need to count the population size and calculate
    the percentage of the population that will give you the sample size you want.
    49,000 is plenty large for our purposes in this example, so we’ve taken a 1% sample
    without replacement. This will result in a plot that looks like [figure 7.12](#figure7.12).
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Dask DataFrame 中采样相当简单；在任意 DataFrame 上使用 `sample` 方法，并指定你想要采样的数据百分比，你将得到一个大小大约为指定百分比的过滤
    DataFrame。默认情况下，采样是不带替换的。这意味着一旦从 DataFrame 中选择出一个车辆记录，该车辆记录就不能在同一个样本中再次被选中。这确保了你的样本中的所有观察都是唯一的。如果你想带替换地采样，你可以使用布尔
    `replace` 参数来指定。请注意，你只能以百分比的形式指定样本大小——无法指定返回样本中的确切项目数。因此，你需要计算总体大小，并计算给出你想要样本大小的总体百分比。在这个例子中，49,000
    已经足够大，所以我们已经进行了不带替换的 1% 样本。这将导致一个看起来像[图 7.12](#figure7.12)的图表。
- en: '![c07_12.png](Images/c07_12.png)'
  id: totrans-831
  prefs: []
  type: TYPE_IMG
  zh: '![c07_12.png](Images/c07_12.png)'
- en: '[Figure 7.12](#figureanchor7.12) A violinplot of the random sample'
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7.12](#figureanchor7.12) 随机样本的小提琴图'
- en: 'Compared with [figure 7.11](#figure7.11), [figure 7.12](#figure7.12) looks
    very similar. We see the general patterns persist: black vehicles tend to be newer,
    green vehicles tend to be older, and more red and green older vehicles are on
    the road than other colors. The distributions are roughly the same shape, but
    they’ve lost some of their detail. The shape of the white and red distributions
    is much less jagged than they were in the population. However, overall, sampling
    has given us the ability to draw similar insights from the data without working
    with the entire set.'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: 与[图 7.11](#figure7.11)相比，[图 7.12](#figure7.12)看起来非常相似。我们看到一般模式持续存在：黑色车辆倾向于较新，绿色车辆倾向于较老，并且比其他颜色的车辆，道路上红绿色较老的车辆更多。分布的形状大致相同，但它们失去了一些细节。白色和红色分布的形状比总体中的形状要少锯齿状。然而，总的来说，采样使我们能够从数据中得出类似的见解，而无需处理整个集合。
- en: 7.4 Visualizing two categorical relationships with heatmap
  id: totrans-834
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 使用热图可视化两个分类关系
- en: 'As you can see, violinplots are useful for understanding the behavior of your
    data when you have one categorical variable. However, it’s not uncommon to have
    many categorical variables as we do in the NYC Parking Ticket dataset. If we want
    to see how categorical variables interact with each other, heatmaps are a very
    useful way to do so. While you can use a heatmap to visualize the relationship
    between any two categorical variables, it’s quite common to use heatmaps across
    dimensions of time. For example, we looked at the trend in citations issued by
    month in the previous chapter and found that more citations tend to be issued
    in warmer months than colder months, but perhaps there’s another time dimension
    that has a pattern as well. Day of the week could be interesting to explore: perhaps
    more citations are issued on weekdays than weekends (or vice versa).'
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，小提琴图在您有一个分类变量时，有助于理解您数据的行为。然而，在我们所使用的纽约市停车罚单数据集中，拥有许多分类变量并不罕见。如果我们想看到分类变量如何相互作用，热图是一个非常实用的工具。虽然您可以使用热图来可视化任意两个分类变量之间的关系，但通常在时间维度的不同维度上使用热图更为常见。例如，我们在上一章中查看按月发布的引用趋势，并发现较暖和的月份比较冷的月份发布的引用更多，但也许还有另一个时间维度也有类似的模式。周可能是值得探索的：也许工作日比周末（或反之）发布的引用更多。
- en: Let’s see if the day of week effect interacts with the month-of-year effect.
    To do this, we’ll want to get the day of the week and month of the year each citation
    was written in. Then, we’ll want to aggregate the citations by both month of year
    and day of week. That should naturally reduce the number of data points for the
    visualization to 84 (12 months by 7 days).
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看周效应是否与年月效应相互作用。为此，我们需要获取每个引用所写的周和年月。然后，我们将按年月和周对引用进行聚合。这应该会自然地减少用于可视化的数据点数量到
    84（12个月乘以7天）。
- en: Listing 7.10 Extracting the day of week and month of year
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.10 提取周和年月
- en: '[PRE111]'
  id: totrans-838
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: First, we’ll apply the `strftime` function over the Issue Date column to extract
    the day of week and month of year, respectively, in the same way we’ve applied
    functions over the data before in chapter 5.
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将对“发布日期”列应用 `strftime` 函数，以分别提取周和年月，就像我们在第 5 章中之前对数据进行函数应用一样。
- en: Listing 7.11 Adding the columns back to the DataFrame
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.11 将列重新添加到 DataFrame 中
- en: '[PRE112]'
  id: totrans-841
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: Next, we add the columns back into the DataFrame using the assign-rename part
    of the drop-assign-rename pattern you learned previously.
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用之前学到的 drop-assign-rename 模式的 assign-rename 部分，将列重新添加到 DataFrame 中。
- en: Listing 7.12 Counting the citations by Month of Year and Day of Week
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.12 按年月和周统计引用次数
- en: '[PRE113]'
  id: totrans-844
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: Now we use the `groupby` method to count the citations by Day of Week and Month
    of Year.
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用 `groupby` 方法按周和月来统计引用次数。
- en: Listing 7.13 Transforming the result into a pivot table
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.13 将结果转换为交叉表
- en: '[PRE114]'
  id: totrans-847
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: After Dask finishes computing the aggregation, we need to pivot the data so
    we have 12 rows (one for each month) and 7 columns (one for each day of the week)
    in the DataFrame. To do that, we’ll use the `pivot` method. The index must be
    reset first because Month of Year and Day of Week will initially be the index
    of the resulting DataFrame, so they need to be moved back into separate columns,
    so we can reference them in the call to `pivot`. Finally, we can produce the heatmap.
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Dask 完成计算聚合后，我们需要将数据转换成交叉表，以便在 DataFrame 中有 12 行（每个月一行）和 7 列（每周一天），我们将使用 `pivot`
    方法。首先必须重置索引，因为年月和周将最初是结果 DataFrame 的索引，因此需要将它们移回单独的列，以便在 `pivot` 调用中引用它们。最后，我们可以生成热图。
- en: Listing 7.14 Creating the heatmap
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.14 创建热图
- en: '[PRE115]'
  id: totrans-850
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: Before calling the `heatmap` function, we’ll stick the months and weekdays into
    separate lists in the appropriate order. As we’ve seen in previous examples, any
    sorting based on named time dimensions will result in alphabetical sorting as
    Pandas isn’t aware that months or weekdays have any special meaning. In the call
    to `heatmap`, we use the `loc` method on the DataFrame to select the rows and
    columns in the correct order. Alternatively, we could sort the DataFrame using
    one of the previously demonstrated date-sorting methods. The `annot` parameter
    tells Seaborn to put the actual values in each cell, so we can see exactly how
    many citations were issued on Wednesdays in July. The `fmt` parameter tells Seaborn
    to format the contents as numbers rather than strings, and the `linewidths` parameter
    adjusts how much spacing should be placed between each cell in the heatmap. This
    function call should produce a heatmap that looks like [figure 7.13](#figure7.13).
    Heatmaps are very simple to read. The light areas indicate few citations, and
    the dark areas indicate many citations. We’ve also annotated the heatmap with
    the actual counts of citations by month/weekday.
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用`heatmap`函数之前，我们将月份和工作日按适当的顺序放入单独的列表中。正如我们在前面的例子中所看到的，任何基于命名时间维度的排序都会导致字母排序，因为Pandas不知道月份或工作日有任何特殊含义。在调用`heatmap`时，我们使用DataFrame上的`loc`方法来选择正确的行和列。或者，我们也可以使用之前演示的日期排序方法对DataFrame进行排序。`annot`参数告诉Seaborn在每个单元格中放置实际值，这样我们就可以确切地看到7月星期三发布的引用数量。`fmt`参数告诉Seaborn将内容格式化为数字而不是字符串，而`linewidths`参数调整了热图中每个单元格之间的间距。这个函数调用应该生成一个类似于[图7.13](#figure7.13)的热图。热图非常容易阅读。浅色区域表示引用数量少，深色区域表示引用数量多。我们还用月份/工作日的实际引用数量注释了热图。
- en: '![c07_13.png](Images/c07_13.png)'
  id: totrans-852
  prefs: []
  type: TYPE_IMG
  zh: '![c07_13.png](Images/c07_13.png)'
- en: '[Figure 7.13](#figureanchor7.13) A heatmap of citations issued by day of week
    and month of year'
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.13](#figureanchor7.13) 按周几和年月发布的引用热图'
- en: The heatmap in [figure 7.13](#figure7.13) immediately shows that weekends tend
    to see fewer citations issued than weekdays, with Sundays especially lower. We’re
    likely seeing the effect of fewer enforcement officers working weekend shifts.
    It appears that Sundays in December sees the fewest citations issued of any month/weekday
    combination, and Thursdays in January seem to have the highest number of citations
    issued. These might be outliers that would be worth exploring further.
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.13](#figure7.13)中的热图立即显示，周末发布的引用数量往往比工作日少，尤其是周日特别低。我们可能看到了周末值班执法官员较少的影响。12月的周日似乎在任何月份/工作日组合中发布的引用数量最少，而1月的周四似乎发布的引用数量最多。这些可能是值得进一步探索的异常值。'
- en: Hopefully you now have a good understanding of how to apply the prepare-reduce-collect-plot
    pattern to produce visualizations for data analysis. As stated earlier in the
    chapter, this pattern can be extended to other libraries as well. While Seaborn
    can create a wide variety of useful and attractive visualizations, any plotting
    library that can accept Pandas DataFrames or NumPy arrays as input can be easily
    interchanged within this pattern.
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你现在已经很好地理解了如何应用准备-减少-收集-绘图模式来为数据分析生成可视化。正如本章前面所述，这个模式也可以扩展到其他库。虽然Seaborn可以创建各种有用且吸引人的可视化，但任何可以接受Pandas
    DataFrame或NumPy数组作为输入的绘图库都可以在这个模式中轻松互换。
- en: In the next chapter, we’ll explore how to produce interactive visualizations
    and dashboards that can be useful both for data exploration and reporting to end
    users.
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何生成交互式可视化仪表板，这些仪表板对于数据探索和向最终用户报告都很有用。
- en: Summary
  id: totrans-857
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: There may be “more to the story” than numerical analysis can explain—it’s always
    worth it to visualize data.
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故事可能“不止于”数值分析所能解释的内容——可视化数据总是值得的。
- en: The prepare-reduce-collect-plot pattern can be used to create visualizations
    from large datasets. You can use any data visualization library that supports
    Pandas with this method.
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备-减少-收集-绘图模式可以用来从大型数据集中创建可视化。你可以使用任何支持Pandas的数据可视化库来应用这种方法。
- en: If it makes sense for the question you’re trying to answer, your data can be
    reduced using aggregation (e.g. citations per month).
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的问题有意义，你可以通过聚合（例如，每月引用数）来减少数据。
- en: Random sampling can also be a good method to visually approximate the shape
    of your data, given a sufficiently large sample size.
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在样本量足够大的情况下，随机抽样也可以是一种很好的方法来直观地近似数据的形状。
- en: The relationship between two continuous variables can be visualized using a
    scatterplot.
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用散点图来可视化两个连续变量之间的关系。
- en: Regplots can be used to plot linear regression as well as nonlinear regression.
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Regplots可以用来绘制线性回归以及非线性回归。
- en: The distribution of a continuous variable across a categorical variable can
    be visualized using a violinplot.
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用小提琴图来可视化连续变量在分类变量上的分布。
- en: Two categorical variables can be visualized using a heatmap.
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用热图来可视化两个分类变量。
- en: '8'
  id: totrans-866
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Visualizing location data with Datashader
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Datashader可视化位置数据
- en: '**This chapter covers**'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章涵盖**'
- en: Using Datashader to visualize many datapoints when downsampling isn’t appropriate
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在下采样不适用的情况下，使用Datashader可视化大量数据点
- en: Plotting interactive heatmaps using Datashader and Bokeh
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Datashader和Bokeh绘制交互式热图
- en: In the previous chapter, we looked at a few ways to gain insights from data
    using visualization. However, every method we looked at relied on finding workarounds
    to reduce the size of the data we used for plotting. Whether by randomly sampling,
    filtering, or aggregating the data, we used these *downsampling* techniques to
    overcome the inherent limitations of Seaborn and Matplotlib. Although we’ve shown
    that these techniques can be useful, downsampling can cause us to miss patterns
    in the data because we’re throwing data away. This issue becomes the most evident
    when dealing with high-dimensional data, such as location.
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了使用可视化从数据中获取洞察力的几种方法。然而，我们查看的每一种方法都依赖于找到解决方案来减少用于绘图的所用数据的大小。无论是通过随机抽样、过滤还是聚合数据，我们都使用了这些*下采样*技术来克服Seaborn和Matplotlib固有的限制。虽然我们已经表明这些技术是有用的，但下采样可能会使我们错过数据中的模式，因为我们正在丢弃数据。当处理高维数据，如位置数据时，这个问题最为明显。
- en: 'Imagine for a moment that we wanted to use the NYC Parking Ticket dataset to
    find the areas of New York City where drivers are most likely to get parking tickets.
    One way we could do this is by finding the mean location by latitude and longitude
    of all issued citations. However, this would only tell us the “average” location
    of a parking ticket, which might not even be located on a city street! There may
    be many hotspots throughout the city, but we wouldn’t be able to tell that by
    using only the mean. We could try to use some sort of clustering algorithm, such
    as k-Means, to identify multiple hotspots, but this would still be troublesome
    for a few reasons: the center points might again not be on any city streets, and
    we would have to manually choose the number of clusters to feed into the clustering
    algorithm. How would we know how many clusters to use if we don’t have a good
    understanding of the data? The only way to get a true and accurate understanding
    of the data in this case is to use all of it. But if plotting libraries like Seaborn
    and Matplotlib don’t work well with datasets stretching into the millions and
    billions of datapoints, how could we visualize datasets of this size without using
    any kind of downsampling? This is precisely where Datashader shines. Before we
    start, take a brief look at [figure 8.1](#figure8.1), which shows the progress
    we’ve made through our workflow.'
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果我们想使用纽约市停车罚单数据集来找出纽约市司机最有可能被罚款的地点。我们可以通过找到所有发出的罚单的经纬度平均值来实现这一点。然而，这只会告诉我们停车罚单的“平均”位置，而这个位置甚至可能不在城市街道上！整个城市可能有多个热点区域，但仅使用平均值我们无法得知这一点。我们可以尝试使用某种聚类算法，例如k-Means，来识别多个热点区域，但这仍然有几个问题：中心点可能仍然不在任何城市街道上，而且我们必须手动选择要输入聚类算法的簇数量。如果我们对数据没有良好的理解，我们如何知道应该使用多少个簇？在这种情况下，唯一获得真正准确理解数据的方法是使用所有数据。但是，如果Seaborn和Matplotlib等绘图库无法很好地处理数百万甚至数十亿数据点的数据集，我们如何在不进行任何类型下采样的情况下可视化这些规模的数据集？这正是Datashader大放异彩的地方。在我们开始之前，简要看一下[图8.1](#figure8.1)，它展示了我们通过工作流程所取得的进展。
- en: '![c08_01.eps](Images/c08_01.png)'
  id: totrans-873
  prefs: []
  type: TYPE_IMG
  zh: '![c08_01.eps](Images/c08_01.png)'
- en: '[Figure 8.1](#figureanchor8.1) The *Data Science with Python and Dask* workflow'
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.1](#figureanchor8.1) 使用Python和Dask进行数据科学的工作流程'
- en: We’ll round out the Exploratory Analysis and Hypothesis Formulation & Testing
    steps of our workflow in this chapter with a look at another way to analyze data
    visually. Unlike in the previous chapter where we used the prepare-reduce-collect-plot
    pattern to downsample the data before plotting it with Seaborn, we’ll look at
    how Datashader can be used to plot data stored in Dask DataFrames directly. Specifically,
    we’ll look at how Datashader can be used to plot geographic-based data on a map.
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过查看另一种分析数据的方法来完善我们的工作流程中的探索性分析和假设形成与测试步骤。与上一章不同，我们在那里使用 prepare-reduce-collect-plot
    模式在绘图之前对数据进行下采样，然后使用 Seaborn 绘制，我们将探讨如何直接使用 Datashader 绘制存储在 Dask DataFrames 中的数据。具体来说，我们将探讨如何使用
    Datashader 在地图上绘制基于地理的数据。
- en: 8.1 What is Datashader and how does it work?
  id: totrans-876
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 什么是 Datashader 以及它是如何工作的？
- en: 'Datashader is a relatively new library in the Python Open Data Science Stack
    that was created to produce meaningful visualizations of very large datasets.
    Unlike our work with Seaborn where we needed Dask to materialize a downsampled
    Pandas DataFrame before plotting, Datashader’s plotting methods accept Dask objects
    directly and can take full advantage of distributed computing. Datashader can
    produce any grid-based visualization: scatterplots, line charts, heatmaps, and
    so on. Let’s walk through the five-step pipeline that Datashader uses to render
    images to understand how it works.'
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: Datashader 是 Python 开放数据科学栈中的一个相对较新的库，它被创建出来用于生成非常大数据集的有意义可视化。与我们在 Seaborn 中的工作不同，我们当时需要使用
    Dask 在绘图之前将 Pandas DataFrame 下采样并具体化，而 Datashader 的绘图方法可以直接接受 Dask 对象，并充分利用分布式计算。Datashader
    可以生成任何基于网格的可视化：散点图、折线图、热图等等。让我们一步步了解 Datashader 使用五步流程渲染图像的过程，以理解它是如何工作的。
- en: 'Before we jump in, let’s get some data to work with. Unfortunately, NYC OpenData
    does not publish the exact latitude/longitude coordinates of where each parking
    citation is issued in the NYC Parking Ticket dataset. Therefore, we’ll turn to
    another medium-sized dataset available on NYC OpenData that does have detailed
    location data: New York City’s database of 311 service calls. A 311 service call
    is an issue that a citizen reports to one of New York City’s non-emergency services,
    such as when a streetlight is out or a pothole has formed on a road. The dataset
    contains a record of all reported issues from the beginning of 2010 to the present
    time and is updated regularly. As a motivating scenario for this chapter, we’ll
    use this data to answer the following question:'
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入之前，让我们获取一些可以处理的数据。不幸的是，纽约市开放数据（NYC OpenData）没有发布纽约市停车罚单数据集中每个停车罚单发出的确切纬度/经度坐标。因此，我们将转向纽约市开放数据上可用的另一个中等规模的数据集，该数据集确实具有详细的位置数据：纽约市311服务调用数据库。311服务调用是指市民向纽约市非紧急服务报告的问题，例如当街灯熄灭或道路上形成坑洼时。该数据集包含从2010年初至今所有报告问题的记录，并定期更新。作为本章的激励场景，我们将使用这些数据来回答以下问题：
- en: Using the NYC 311 Service Call dataset, how can we show the frequency of service
    calls based on location and plot this data on a map to find common problem areas?
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: 使用纽约市311服务调用数据集，我们如何根据位置显示服务调用的频率，并在地图上绘制这些数据以找到常见问题区域？
- en: 'The link to download the data can be found here: [https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9](https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9).
    To export the data in CSV format, click the Export button in the top-right corner
    and select CSV. Also, please make sure you’ve installed the datashader, holoviews,
    and geoviews packages before continuing. Holoviews and Geoviews are dependencies
    of Datashader and must be installed for the code in this chapter to work correctly.
    Both libraries are used by Datashader to make interactive map-type visualizations.
    Installation instructions can be found in the appendix. After you’ve downloaded
    the data, import the necessary packages and load the data.'
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: 下载数据的链接可以在以下位置找到：[https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9](https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9)。要导出
    CSV 格式的数据，请点击右上角的导出按钮并选择 CSV。此外，请确保在继续之前已安装 datashader、holoviews 和 geoviews 包。Holoviews
    和 Geoviews 是 Datashader 的依赖项，必须安装才能使本章中的代码正确运行。这两个库都由 Datashader 用于创建交互式地图类型可视化。安装说明可以在附录中找到。在您下载数据后，导入必要的包并加载数据。
- en: Listing 8.1 Loading in the data and imports
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.1 加载数据和导入
- en: '[PRE116]'
  id: totrans-882
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[Listing 8.1](#listing8.1) contains all the standard steps to get started:
    importing the packages we’ll use in this chapter, setting up the working directory,
    and reading in the data. The only thing particularly noteworthy is that we’ll
    only bring in the Latitude and Longitude columns for now. To do this, we’ll use
    the `usecols` parameter that you learned about in chapter 4\. We don’t need any
    of the other columns right now.'
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表8.1](#listing8.1) 包含了所有启动的标准步骤：导入本章中我们将使用的包，设置工作目录，并读取数据。唯一值得注意的事情是我们现在只引入纬度和经度列。为此，我们将使用第4章中你学到的`usecols`参数。目前我们不需要其他任何列。'
- en: 8.1.1 The five stages of the Datashader rendering pipeline
  id: totrans-884
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 Datashader渲染管道的五个阶段
- en: 'With the data and packages in place, let’s continue with walking through how
    Datashader works. The five stages that Datashader uses to render images are:'
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: 数据和包准备就绪后，让我们继续了解Datashader的工作原理。Datashader用于渲染图像的五个阶段是：
- en: Projection
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 投影
- en: Aggregation
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合
- en: Transformation
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换
- en: Colormapping
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 颜色映射
- en: Embedding
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入
- en: The first step, *projection*, deals with establishing the *Canvas* that Datashader
    will plot the image on. This includes choosing the size of the image (for example,
    800 pixels wide by 600 pixels high), the variables that will be plotted on the
    x and y axes, and the range of the variables, which is used to center the visualization
    on the Canvas. The anatomy of a Canvas object can be seen in [figure 8.2](#figure8.2).
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步，*投影*，处理在Datashader上绘制图像的*Canvas*的建立。这包括选择图像的大小（例如，800像素宽，600像素高），将在x和y轴上绘制的变量，以及变量的范围，这用于在Canvas上定位可视化。Canvas对象的解剖结构可以在[图8.2](#figure8.2)中看到。
- en: '![c08_02.png](Images/c08_02.png)'
  id: totrans-892
  prefs: []
  type: TYPE_IMG
  zh: '![c08_02.png](Images/c08_02.png)'
- en: '[Figure 8.2](#figureanchor8.2) A visual representation of a Canvas object'
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.2](#figureanchor8.2) Canvas对象的视觉表示'
- en: To create the Canvas object, we’ll call the Canvas constructor and pass in the
    relevant arguments, as is shown next. This listing does not have any output, because
    the Canvas object we’re creating is simply a container to hold our visualization.
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建Canvas对象，我们将调用Canvas构造函数并传入相关参数，如下所示。这个列表没有输出，因为我们创建的Canvas对象只是一个用于存放我们的可视化的容器。
- en: Listing 8.2 Creating a Canvas object
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.2 创建Canvas对象
- en: '[PRE117]'
  id: totrans-896
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'The parameters should be all self-explanatory for the constructor: the plot
    width and height dictate the size of the image that will be produced (in pixels),
    and the x and y range parameters set the bounds for the grid. Here we’ve picked
    some map coordinates that roughly correspond to the area around New York City.
    Keep in mind that longitude is normally plotted along the x axis and latitude
    is normally plotted along the y axis. If you ever work with a dataset and you’re
    unsure which range of coordinates to use, you can compute the min/max of each
    column using the aggregate functions you learned in chapter 6.'
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: 参数对于构造函数来说应该是全部自解释的：图像的宽度和高度决定了将要生成的图像大小（以像素为单位），而x和y范围参数设置了网格的界限。在这里，我们选择了大致对应纽约市周边区域的地图坐标。请记住，经度通常沿x轴绘制，而纬度通常沿y轴绘制。如果你在处理数据集时不确定使用哪个坐标范围，你可以使用第6章中学到的聚合函数计算每一列的最小值/最大值。
- en: The second step in Datashader’s plotting pipeline is *aggregation*. But wait
    a moment—didn’t we just establish a few pages ago that Datashader is supposed
    to use all the data without downsampling? Then why would Datashader aggregate
    the data? Datashader uses the term *aggregation* a bit differently than how we’ve
    talked about aggregation in the past. When we talked about aggregation, we were
    referring to domain-specific aggregation, such as grouping parking citations by
    vehicle year. In all cases, the aggregations we performed were along a specific
    dimension contained in the data. On the other hand, Datashader aggregates data
    into buckets that represent pixels on your screen. The coordinate system used
    by your data is mapped to the pixel area of the image and all datapoints residing
    within one of those buckets will have an aggregate function (such as sum or mean)
    applied to them. For example, if each pixel happened to represent 1/100th of a
    degree latitude/longitude, a 100 x 100 image would cover the area of 1 square
    degree. At 40 degrees north, 1 degree of longitude is equivalent to 53 miles,
    and a degree of latitude is equivalent to 69 miles. This means that if we produced
    a 100 x 100 image of the area around New York City, each pixel on your screen
    would represent approximately 36.5 square miles. This is a pretty low resolution,
    since all 311 service calls in a 36.5 square mile area would be aggregated together.
    [Figure 8.3](#figure8.3) demonstrates how Datashader performs aggregation.
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: Datashader 绘图管道的第二步是*聚合*。但是等等——我们不是在几页之前就建立了 Datashader 应该使用所有数据而不进行下采样的原则吗？那么为什么
    Datashader 会对数据进行聚合呢？Datashader 对*聚合*这个术语的使用方式与我们过去所讨论的略有不同。当我们谈论聚合时，我们指的是特定领域的聚合，例如按车辆年份分组停车罚单。在所有情况下，我们执行的聚合都是沿着数据中包含的特定维度进行的。另一方面，Datashader
    将数据聚合到代表屏幕上像素的桶中。你的数据所使用的坐标系被映射到图像的像素区域，所有位于这些桶中的数据点都将应用聚合函数（如求和或平均值）。例如，如果每个像素恰好代表
    1/100 度的纬度/经度，一个 100 x 100 的图像将覆盖 1 平方度的区域。在北纬 40 度，1 度的经度相当于 53 英里，1 度的纬度相当于
    69 英里。这意味着如果我们生成一个覆盖纽约市周边区域的 100 x 100 图像，屏幕上的每个像素将代表大约 36.5 平方英里。这是一个相当低的分辨率，因为
    36.5 平方英里区域内的所有 311 服务调用都将被聚合在一起。[图 8.3](#figure8.3) 展示了 Datashader 如何执行聚合操作。
- en: '![c08_03.eps](Images/c08_03.png)'
  id: totrans-899
  prefs: []
  type: TYPE_IMG
  zh: '![c08_03.eps](Images/c08_03.png)'
- en: '[Figure 8.3](#figureanchor8.3) Aggregation draws areas around the raw points
    and performs an aggregate operation on the areas; each of these areas resembles
    a single pixel in the final visualization.'
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8.3](#figureanchor8.3) 聚合在原始点周围绘制区域，并对这些区域执行聚合操作；这些区域中的每一个都类似于最终可视化中的单个像素。'
- en: Fortunately, Datashader performs all this mapping and aggregation for you based
    on the width/height and range options you specified in the Projection step. A
    small range and a large image will result in a high-resolution image of the space,
    whereas a large range and a small image will result in a low-resolution image
    of the space. Given the range and size we specified in [listing 8.2](#listing8.2),
    we can expect each pixel of our image to represent about 120,000 square feet,
    which is about half the area of a standard city block in New York City. So, all
    we need to do now is tell Datashader which data we want it to use.
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Datashader 会根据你在投影步骤中指定的宽/高和范围选项为你执行所有这些映射和聚合操作。小范围和大图像将导致空间的高分辨率图像，而大范围和小图像将导致空间的低分辨率图像。根据我们在[列表
    8.2](#listing8.2)中指定的范围和大小，我们可以预期我们图像中的每个像素代表大约 120,000 平方英尺，这大约是纽约市标准街区面积的一半。因此，我们现在需要做的就是告诉
    Datashader 我们希望它使用哪些数据。
- en: Listing 8.3 Defining the aggregation
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.3 定义聚合
- en: '[PRE118]'
  id: totrans-903
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: This call is pretty simple since we don’t need to apply any further transformation
    to the data. We’re simply telling Datashader to take the data in the `nyc311_geo_data`
    DataFrame, plot the Longitude column on the x axis, and plot the Latitude column
    on the y axis. Since we haven’t specified another method of aggregation, such
    as sum, mean, and so forth, Datashader will simply count the number of points
    that fall into each pixel. So, for our example here, Datashader will be counting
    the number of 311 service calls that happen within each 120,000 square foot chunk
    of New York City.
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: 这个调用非常简单，因为我们不需要对数据进行任何进一步的转换。我们只是告诉 Datashader 从 `nyc311_geo_data` DataFrame
    中获取数据，在 x 轴上绘制经度列，在 y 轴上绘制纬度列。由于我们没有指定其他聚合方法，如求和、平均值等，Datashader 将简单地计算落入每个像素中的点的数量。因此，在我们的例子中，Datashader
    将计算纽约市每个 120,000 平方英尺区域内的 311 服务调用数量。
- en: The third step of the Datashader pipeline is *transformation*. We don’t have
    any need to apply any transformations to our data in this example, since we’re
    taking a simple count, but it’s important to note that the `aggregation` object
    we just created in the previous listing is a simple xarray object that represents
    the pixel space defined in the Canvas object. This means we could perform any
    kind of array transformation we’d like on it, such as filtering out pixels that
    are in a certain percentile, multiplying the array by another array, or performing
    any sort of linear algebra transformation. In this particular example, the array
    is 600 x 600 and contains both a value related to the service calls that occurred
    in that specific area as well as the mapping back to the original latitude/longitude
    coordinates.
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: Datashader管道的第三步是**转换**。在这个例子中，我们没有必要对我们的数据进行任何转换，因为我们只是进行了一个简单的计数，但重要的是要注意，我们刚才创建的`aggregation`对象是一个简单的xarray对象，它代表了Canvas对象中定义的像素空间。这意味着我们可以对它执行任何我们想要的数组转换，例如过滤掉位于某个百分位数的像素，将数组乘以另一个数组，或者执行任何类型的线性代数转换。在这个特定的例子中，数组是600
    x 600，包含与该特定区域发生的服务调用相关的值以及映射回原始纬度/经度坐标。
- en: '![c08_04.eps](Images/c08_04.png)'
  id: totrans-906
  prefs: []
  type: TYPE_IMG
  zh: '![c08_04.eps](Images/c08_04.png)'
- en: '[Figure 8.4](#figureanchor8.4) The contents of the pixel located at 300, 300'
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.4](#figureanchor8.4) 位于300, 300像素处的像素内容'
- en: In [figure 8.4](#figure8.4), you can see the value at pixel 300, 300\. The value
    of 140 is not a count of the number of service calls in that area. Instead, the
    value represents a relative rank of how many service calls occurred in that area
    compared to all other areas on the map. A value of 0 signifies no service calls
    in the area, and the value increases in areas where service calls occurred more
    frequently. This number is very important in the next step.
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图8.4](#figure8.4)中，你可以看到像素300, 300处的值。140的值不是该区域服务调用数量的计数。相反，该值表示与地图上所有其他区域相比，该区域发生服务调用的相对排名。0的值表示该区域没有服务调用，该值在服务调用发生更频繁的区域增加。这个数字在下一步非常重要。
- en: The fourth step is *colormapping*. In this step, Datashader will take the relative
    values computed in the Aggregation step and map them to a given color palette.
    In this example, we’re using the default color palette, which ranges from white
    to dark blue. The higher the value in the underlying array, the darker the shade
    of blue. This colormapping is what ultimately conveys the information that we
    want to know. [Figure 8.5](#figure8.5) demonstrates how Datashader performs the
    colormapping step.
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: 第四步是**颜色映射**。在这一步中，Datashader将聚合步骤中计算的相对值映射到给定的颜色图。在这个例子中，我们使用默认的颜色图，从白色到深蓝色。底层数组中的值越高，蓝色的阴影就越深。这种颜色映射最终传达了我们想要知道的信息。[图8.5](#figure8.5)演示了Datashader如何执行颜色映射步骤。
- en: '![c08_05.eps](Images/c08_05.png)'
  id: totrans-910
  prefs: []
  type: TYPE_IMG
  zh: '![c08_05.eps](Images/c08_05.png)'
- en: '[Figure 8.5](#figureanchor8.5) Colormapping translates ranked values into colors.'
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.5](#figureanchor8.5) 颜色映射将排名值转换为颜色。'
- en: The fifth and final step is *embedding*. This is where Datashader renders the
    final image using the information computed in the aggregation step along with
    the colormap. To trigger the final rendering, we use the `shade` method on the
    aggregation object. If we want to use a different colormap (for example, red to
    blue), you can specify that with the `cmap` parameter. Embedding is demonstrated
    in [figure 8.6](#figure8.6).
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: 第五步也是最后一步是**嵌入**。这是Datashader使用聚合步骤中计算的信息以及颜色图来渲染最终图像的地方。为了触发最终渲染，我们在聚合对象上使用`shade`方法。如果我们想使用不同的颜色图（例如，从红色到蓝色），可以通过`cmap`参数指定。嵌入在[图8.6](#figure8.6)中演示。
- en: '![c08_06.png](Images/c08_06.png)'
  id: totrans-913
  prefs: []
  type: TYPE_IMG
  zh: '![c08_06.png](Images/c08_06.png)'
- en: '[Figure 8.6](#figureanchor8.6) Embedding renders the final image on the screen.'
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.6](#figureanchor8.6) 嵌入将最终图像渲染到屏幕上。'
- en: Listing 8.4 Rendering the final image
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.4 渲染最终图像
- en: '[PRE119]'
  id: totrans-916
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: The call to the `shade` method in [listing 8.4](#listing8.4) transforms the
    data into an image, which can then be displayed using IPython.
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表8.4](#listing8.4)中调用`shade`方法将数据转换为图像，然后可以使用IPython显示。
- en: 8.1.2 Creating a Datashader Visualization
  id: totrans-918
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 创建Datashader可视化
- en: To recap, here’s the complete code.
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，以下是完整的代码。
- en: Listing 8.5 Complete code to produce our first Datashader visualization
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.5 生成第一个Datashader可视化的完整代码
- en: '[PRE120]'
  id: totrans-921
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: Notice that we’ve wrapped the code in the `ProgressBar` context as per usual
    with Dask-related code. Datashader is actually using Dask to produce the aggregation
    for us, so we can watch the progress of that aggregation! If you inspect the `image`
    object we’re left with, you should see something similar to [figure 8.7](#figure8.7).
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们像往常一样用`ProgressBar`上下文包裹了代码。实际上，Datashader正在使用Dask为我们生成聚合，因此我们可以查看聚合的进度！如果你检查我们留下的`image`对象，你应该会看到类似于[图8.7](#figure8.7)的内容。
- en: '![c08_07.png](Images/c08_07.png)'
  id: totrans-923
  prefs: []
  type: TYPE_IMG
  zh: '![c08_07.png](Images/c08_07.png)'
- en: '[Figure 8.7](#figureanchor8.7) The output of the Datashader image'
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.7](#figureanchor8.7) Datashader图像的输出'
- en: 'This is pretty exciting! We just plotted about 16 million datapoints in a matter
    of a few seconds. If you’re familiar with the geography of New York City, you
    should immediately recognize the shape of the city. If you’re not, that’s all
    right too—since we have location data in a coordinate system, we can overlay this
    visualization on a map to help figure out where we are, which we’ll be doing in
    the next section. You might also be curious as to how we can focus in on a specific
    part of the city. The top-leftmost island in the image is the island of Manhattan,
    one of the most densely populated parts of New York City. It makes sense that
    just about the entire island is darkly shaded: more people correlate with more
    requests for city services. In fact, the only area that isn’t darkly shaded is
    the white rectangle in the middle of the island, which is New York City’s famous
    Central Park. Perhaps we want to only focus on Lower Manhattan and find where
    the problem areas are in that specific part of the city. In the next section,
    we’ll cover how to make our visualization interactive, allowing us to pan and
    zoom around the city. We’ll also add some map tiles so we have a better idea of
    where we’re looking. Before we move on, let’s recap the five plotting stages of
    Datashader. A summary of each stage can be seen in [table 8.1](#table8.1).'
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常令人兴奋！我们只用了几秒钟就绘制了大约1600万个数据点。如果你熟悉纽约市的地理，你应该能立即认出城市的形状。如果你不熟悉，也没关系——因为我们有坐标系统中的位置数据，我们可以将这个可视化叠加到地图上，帮助我们确定位置，我们将在下一节中这样做。你也可能好奇我们如何聚焦于城市的特定部分。图像中最左上角的岛屿是曼哈顿岛，纽约市人口最稠密的地区之一。整个岛屿被深色阴影覆盖是有道理的：更多的人与对城市服务的更多请求相关。事实上，唯一没有被深色阴影覆盖的区域是岛屿中间的白色矩形，这是纽约市著名的中央公园。也许我们只想关注下曼哈顿，并找出该城市特定部分的问题区域。在下一节中，我们将介绍如何使我们的可视化变得交互式，允许我们在城市周围平移和缩放。我们还将添加一些地图瓦片，以便我们更好地了解我们正在看的地方。在我们继续之前，让我们回顾一下Datashader的五步绘图阶段。每个阶段的总结可以在[表8.1](#table8.1)中看到。
- en: Table 8.1 The five plotting stages of Datashader
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.1 Datashader的五步绘图阶段
- en: '| **Stage** | **Explanation** |'
  id: totrans-927
  prefs: []
  type: TYPE_TB
  zh: '| **阶段** | **说明** |'
- en: '| --- | --- |'
  id: totrans-928
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Projection Aggregation'
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: '| 投影聚合'
- en: Transformation
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: 变换
- en: Colormapping
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: 颜色映射
- en: Embedding | Creating a container (Canvas) to plot the visualization onto Grouping
    data within the same “area” (coordinates/location)
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一“区域”（坐标/位置）内对数据进行分组
- en: Applying mathematical transformation(s) on the aggregated values
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: 对聚合值应用数学变换
- en: Converting raw values into shades of colors to plot on the screen
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始值转换为屏幕上要绘制的颜色阴影
- en: Rendering the final image on the Canvas |
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: 在画布上渲染最终图像 |
- en: 8.2 Plotting location data as an interactive heatmap
  id: totrans-936
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 以交互式热图形式绘制位置数据
- en: Before we make our visualization interactive, we should consider the time it
    will take to render each image. Any panning or zooming will dynamically change
    the x range and y range values that you set manually in the previous section.
    This will require the entire image to be re-rendered. The longer it takes to render
    a new image, the less useful the interactive feature becomes, so we’ll want to
    do everything we can to minimize processing time. It’s recommended that data used
    by Datashader is stored in Parquet format with Snappy compression—both of which
    we already covered in chapter 5! We’ll convert the data from CSV to Parquet before
    we continue.
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将可视化变为交互式之前，我们应该考虑渲染每张图像所需的时间。任何平移或缩放都会动态地改变你在上一节中手动设置的x范围和y范围值。这将需要重新渲染整个图像。渲染新图像所需的时间越长，交互式功能就越不实用，因此我们希望尽可能减少处理时间。建议使用Parquet格式存储Datashader使用的数据，并使用Snappy压缩——这两者我们已经在第5章中介绍过了！在我们继续之前，我们将数据从CSV转换为Parquet格式。
- en: However, we have one more thing to consider. We also want to overlay our original
    heatmap on top of a map, so we can tell which parts of the city we’re looking
    at. Another library called Geoviews allows you to do this. Geoviews uses the coordinate
    data to fetch *map tiles* from a third-party mapping service.
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们还有一件事要考虑。我们还想在地图上叠加我们的原始热图，这样我们就可以知道我们正在查看城市的哪些部分。另一个名为 Geoviews 的库允许你这样做。Geoviews
    使用坐标数据从第三方地图服务获取 *地图瓦片*。
- en: 8.2.1 Preparing geographic data for map tiling
  id: totrans-939
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 准备地理数据以进行地图瓦片化
- en: Map tiles are chunks of a map that are projected on a grid. For example, a tile
    might represent one square mile of Manhattan and contain all the roads and terrain
    features in that square mile. Just like with our data in Datashader, the size
    and area of the tile is based on the range and size of the Canvas. These tiles
    are provided by mapping services that use web APIs to deliver the necessary tiles.
    However, most mapping services don’t index map tiles by latitude/longitude coordinates.
    Instead, they use a different coordinate system called Web Mercator. Web Mercator
    is just another grid coordinate system, but to produce the correct images, we’ll
    need to convert our latitude/longitude coordinates into Web Mercator coordinates.
    Fortunately, Geoviews has a utility method that will do this conversion for us.
    We’ll run the conversion on our coordinates, then save the converted coordinates
    to Parquet.
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: 地图瓦片是投影在网格上的地图块。例如，一个瓦片可能代表曼哈顿的一个平方英里，并包含该平方英里内的所有道路和地形特征。就像我们在 Datashader 中的数据一样，瓦片的大小和面积基于画布的范围和大小。这些瓦片由使用
    Web API 交付必要瓦片的地图服务提供。然而，大多数地图服务不会按经纬度坐标索引地图瓦片。相反，它们使用一个称为 Web Mercator 的不同坐标系。Web
    Mercator 只是一个另一种网格坐标系，但为了生成正确的图像，我们需要将我们的经纬度坐标转换为 Web Mercator 坐标。幸运的是，Geoviews
    有一个实用方法可以为我们完成这个转换。我们将运行转换，然后将转换后的坐标保存到 Parquet。
- en: Listing 8.6 Preparing the data for map tiling
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.6 准备地图瓦片化的数据
- en: '[PRE121]'
  id: totrans-942
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: In [listing 8.6](#listing8.6), we use the `lnglat_to_meters` method to convert
    the latitude/longitude coordinates into Web Mercator coordinates. This particular
    method takes two input objects (an X and a Y) and spits out the transformed X
    and Y as two separate objects. It can accept Dask Series objects without having
    to first collect and materialize it into a Pandas Series, so we simply have to
    pass in the Longitude column of our original DataFrame for the X value, and the
    Latitude column of our original DataFrame for the Y value.
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 8.6](#listing8.6) 中，我们使用 `lnglat_to_meters` 方法将经纬度坐标转换为 Web Mercator 坐标。这个特定的方法接受两个输入对象（X
    和 Y）并输出转换后的 X 和 Y 作为两个单独的对象。它可以接受 Dask Series 对象，而无需首先收集并实体化为 Pandas Series，所以我们只需将原始
    DataFrame 的经度列传递给 X 值，将原始 DataFrame 的纬度列传递给 Y 值。
- en: We want to keep these values together in a single DataFrame, so we’ll use the
    `concat` method that you learned about in chapter 5\. However, this time, instead
    of using it to union two DataFrames, we’ll use it to concatenate along the columns
    axis (axis 1). You’ll receive a warning telling you that Dask is assuming the
    indexes are aligned between both Series, but you can ignore it in this case because
    `web_mercator_x` and `web_mercator_y` were created in the same order, making their
    indexes naturally aligned. We’ll also use the `dropna` method that we’ve used
    before in chapter 4 to drop any rows that do not have valid coordinates. Finally,
    we’ll rename the columns for convenience’s sake to `x` and `y` appropriately and
    save the result to a Parquet file.
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将这些值保存在一个单独的 DataFrame 中，所以我们将使用在第 5 章中学到的 `concat` 方法。然而，这次，我们不会用它来合并两个
    DataFrame，而是用它来沿着列轴（axis 1）进行连接。你会收到一个警告，告诉你 Dask 假设两个 Series 之间的索引是对齐的，但在这个情况下你可以忽略它，因为
    `web_mercator_x` 和 `web_mercator_y` 是按照相同的顺序创建的，使得它们的索引自然对齐。我们还将使用在第 4 章中用过的 `dropna`
    方法来删除任何没有有效坐标的行。最后，我们将为了方便起见将列重命名为 `x` 和 `y`，并将结果保存到 Parquet 文件中。
- en: 8.2.2 Creating the interactive heatmap
  id: totrans-945
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 创建交互式热图
- en: Now we’ll read the Parquet file in and create the interactive visualization.
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将读取 Parquet 文件并创建交互式可视化。
- en: Listing 8.7 Creating the interactive visualization
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.7 创建交互式可视化
- en: '[PRE122]'
  id: totrans-948
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: In [listing 8.7](#listing8.7), we begin by reading the Parquet data that we
    just saved back into our session. Next, we need to activate the Bokeh extension
    in Holoviews. These two packages manage the interactive part of the visualization,
    but we don’t need to change anything to get everything to work out of the box.
    The map tile provider we’ll be using is from a company called Stamen that maintains
    a repository of open source street map data created under the OpenStreetMap project.
    We’ll store the URL for the API in a variable so the map tile provider object
    can use it later. Next, we’ll define some display parameters for the visualization,
    specifying the width and height of the image area. Then, we create the tile provider
    object using the `geoviews.WMTS` constructor. This object is used to call the
    API URL and get the correct map tiles whenever the image needs to be updated.
    All we need to do is pass in the URL variable. We’ve chained this call with the
    `opts` method to set the display options. We then create the heatmap by using
    the `holoviews.Points` function, which is very similar to the `scene.points` method
    you used in the previous section. Also, instead of using `datashader.transfer_functions.shade`
    to produce the image, the `datashade` function from Holoviews is used instead.
    This allows Holoviews to continuously update the image as we pan/zoom with the
    Bokeh widget. The final line ties everything together. While the multiplication
    operator seems a bit odd, this is how the two layers are folded together to produce
    the final image. This will also fire up the Bokeh widget and render the first
    image. You should see something similar to [figure 8.8](#figure8.8).
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表8.7](#listing8.7)中，我们首先将我们刚刚保存的Parquet数据读回到我们的会话中。接下来，我们需要在Holoviews中激活Bokeh扩展。这两个包管理可视化的交互部分，但我们不需要做任何更改即可让一切正常工作。我们将使用的地图瓦片提供者是来自一家名为Stamen的公司，该公司维护着一个由OpenStreetMap项目创建的开源街道地图数据仓库。我们将API的URL存储在一个变量中，以便地图瓦片提供者对象稍后可以使用它。接下来，我们将定义一些可视化参数，指定图像区域的宽度和高度。然后，我们使用`geoviews.WMTS`构造函数创建瓦片提供者对象。该对象用于在图像需要更新时调用API
    URL并获取正确的地图瓦片。我们只需要传递URL变量。我们使用`opts`方法将此调用与显示选项链在一起。然后，我们使用`holoviews.Points`函数创建热图，该函数与您在上一节中使用的`scene.points`方法非常相似。此外，我们不是使用`datashader.transfer_functions.shade`来生成图像，而是使用Holoviews的`datashade`函数。这允许Holoviews在我们使用Bokeh小部件平移/缩放时持续更新图像。最后一行将所有内容结合起来。虽然乘法运算符看起来有点奇怪，但这正是两个层折叠在一起以生成最终图像的方式。这也会启动Bokeh小部件并渲染第一张图像。您应该看到类似于[图8.8](#figure8.8)的内容。
- en: '![c08_08.png](Images/c08_08.png)'
  id: totrans-950
  prefs: []
  type: TYPE_IMG
  zh: '![c08_08.png](Images/c08_08.png)'
- en: '[Figure 8.8](#figureanchor8.8) The interactive heatmap'
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.8](#figureanchor8.8) 交互式热图'
- en: As you can see, we’ve overlaid the heatmap on top of a map of New York City,
    and everything lines up perfectly! Also notice we have the Latitude and Longitude
    along the outside edge of the map, and controls in the upper right to enable panning
    and zooming. [Figure 8.9](#figure8.9) shows a zoomed-in image of the southernmost
    tip of Manhattan.
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们在纽约市的地图上叠加了热图，一切完美对齐！请注意，我们在地图的外边缘有纬度和经度，以及在上右角的控制按钮以启用平移和缩放。[图8.9](#figureanchor8.9)显示了曼哈顿最南端的放大图像。
- en: '![c08_09.png](Images/c08_09.png)'
  id: totrans-953
  prefs: []
  type: TYPE_IMG
  zh: '![c08_09.png](Images/c08_09.png)'
- en: '[Figure 8.9](#figureanchor8.9) Zoomed into the southern tip of Manhattan'
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.9](#figureanchor8.9) 放大曼哈顿南端'
- en: You can see that as we zoom in, the image updates along with the map tiles.
    It should take less than one second to re-render the image at the new zoom level.
    You can also see that relative to the area we’ve zoomed in on, there are some
    areas where more service calls happen than in other areas. For example, many service
    calls are occurring along Broadway, but fewer have occurred along some of the
    side streets, such as around the National September 11th Memorial. You can pan
    and zoom all around the city to explore problem areas.
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，当我们放大时，图像会随着地图瓦片更新。在新缩放级别重新渲染图像应该不到一秒钟。您还可以看到，相对于我们缩放的区域，有些区域的服务调用比其他区域多。例如，许多服务调用发生在百老汇，但沿一些侧街，如国家9/11纪念地周围，发生的调用较少。您可以在整个城市中平移和缩放以探索问题区域。
- en: Summary
  id: totrans-956
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Datashader can be used to produce accurate images of large datasets without
    downsampling.
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Datashader可用于生成大型数据集的精确图像，而无需降采样。
- en: Every DataShader object consists of a Canvas, an aggregation, and a transfer
    function.
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个DataShader对象由一个画布、一个聚合和一个传输函数组成。
- en: DataShader visualizations aggregate based on the number of pixels in the canvas
    area, and their resolution is dynamic, allowing you to “zoom in” to a particular
    area on a graph.
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataShader 可视化基于画布区域中的像素数量进行聚合，它们的分辨率是动态的，允许你“放大”到图表上的特定区域。
- en: Holoviews, Geoviews, and Bokeh can be used with Datashader to produce interactive
    visualizations with map tiles.
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holoviews、Geoviews 和 Bokeh 可以与 Datashader 一起使用，以地图瓦片的形式生成交互式可视化。
- en: Map tiles are overlaid on a grid using a tile provider. If your data has latitude/longitude
    coordinates, they should be translated to Web Mercator coordinates first.***
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 地图瓦片通过瓦片提供者叠加在网格上。如果你的数据有经纬度坐标，它们应首先转换为 Web Mercator 坐标。***
