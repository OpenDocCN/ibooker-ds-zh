- en: 'Chapter 10\. StatefulSets: deploying replicated stateful applications'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第 10 章。StatefulSets：部署复制的有状态应用
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Deploying stateful clustered applications
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署有状态集群应用
- en: Providing separate storage for each instance of a replicated pod
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为复制的 pod 的每个实例提供单独的存储
- en: Guaranteeing a stable name and hostname for pod replicas
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保证 pod 副本具有稳定的名称和主机名
- en: Starting and stopping pod replicas in a predictable order
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以可预测的顺序启动和停止 pod 副本
- en: Discovering peers through DNS SRV records
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 DNS SRV 记录发现对等节点
- en: You now know how to run both single-instance and replicated stateless pods,
    and even stateful pods utilizing persistent storage. You can run several replicated
    web-server pod instances and you can run a single database pod instance that uses
    persistent storage, provided either through plain pod volumes or through Persistent-Volumes
    bound by a PersistentVolumeClaim. But can you employ a ReplicaSet to replicate
    the database pod?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在知道如何运行单实例和复制的无状态 pod，甚至利用持久存储的状态 pod。你可以运行多个复制的 web-server pod 实例，你也可以运行一个使用持久存储的单一数据库
    pod 实例，无论是通过普通的 pod 卷还是通过由 PersistentVolumeClaim 绑定的 Persistent-Volumes。但是，你能使用
    ReplicaSet 来复制数据库 pod 吗？
- en: 10.1\. Replicating stateful pods
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 10.1\. 复制有状态 pod
- en: ReplicaSets create multiple pod replicas from a single pod template. These replicas
    don’t differ from each other, apart from their name and IP address. If the pod
    template includes a volume, which refers to a specific PersistentVolumeClaim,
    all replicas of the ReplicaSet will use the exact same PersistentVolumeClaim and
    therefore the same PersistentVolume bound by the claim (shown in [figure 10.1](#filepos955119)).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSets 从单个 pod 模板创建多个 pod 副本。这些副本之间除了名称和 IP 地址外没有区别。如果 pod 模板包含一个卷，该卷引用特定的
    PersistentVolumeClaim，则 ReplicaSet 的所有副本都将使用完全相同的 PersistentVolumeClaim 和因此相同的
    PersistentVolume（如[图 10.1](#filepos955119)所示）。
- en: Figure 10.1\. All pods from the same ReplicaSet always use the same PersistentVolumeClaim
    and PersistentVolume.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1\. 同一个 ReplicaSet 的所有 pod 总是使用相同的 PersistentVolumeClaim 和 PersistentVolume。
- en: '![](images/00165.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00165.jpg)'
- en: Because the reference to the claim is in the pod template, which is used to
    stamp out multiple pod replicas, you can’t make each replica use its own separate
    Persistent-VolumeClaim. You can’t use a ReplicaSet to run a distributed data store,
    where each instance needs its own separate storage—at least not by using a single
    ReplicaSet. To be honest, none of the API objects you’ve seen so far make running
    such a data store possible. You need something else.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因为引用的声明在 pod 模板中，该模板用于打印多个 pod 副本，所以你不能让每个副本使用它自己的单独的 Persistent-VolumeClaim。你不能使用
    ReplicaSet 来运行分布式数据存储，其中每个实例都需要其自己的单独存储——至少不能通过使用单个 ReplicaSet 来实现。说实话，你迄今为止看到的任何
    API 对象都无法运行此类数据存储。你需要其他东西。
- en: 10.1.1\. Running multiple replicas with separate storage for each
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 10.1.1\. 使用每个副本单独存储运行多个副本
- en: How does one run multiple replicas of a pod and have each pod use its own storage
    volume? ReplicaSets create exact copies (replicas) of a pod; therefore you can’t
    use them for these types of pods. What can you use?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如何运行 pod 的多个副本，并让每个 pod 使用其自己的存储卷？ReplicaSets 创建 pod 的精确副本（副本）；因此，你不能使用它们来处理这些类型的
    pod。你可以使用什么？
- en: Creating pods manually
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 手动创建 pod
- en: You could create pods manually and have each of them use its own PersistentVolumeClaim,
    but because no ReplicaSet looks after them, you’d need to manage them manually
    and recreate them when they disappear (as in the event of a node failure). Therefore,
    this isn’t a viable option.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以手动创建 pod，并让每个 pod 使用它自己的 PersistentVolumeClaim，但由于没有 ReplicaSet 管理它们，你需要手动管理它们，并在它们消失时（例如节点故障事件）重新创建它们。因此，这不是一个可行的选项。
- en: Using one ReplicaSet per pod instance
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个 pod 实例使用一个 ReplicaSet
- en: Instead of creating pods directly, you could create multiple ReplicaSets—one
    for each pod with each ReplicaSet’s desired replica count set to one, and each
    ReplicaSet’s pod template referencing a dedicated PersistentVolumeClaim (as shown
    in [figure 10.2](#filepos957078)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是直接创建 pod，你可以创建多个 ReplicaSets——每个 pod 一个，每个 ReplicaSet 的期望副本数设置为 1，每个 ReplicaSet
    的 pod 模板引用一个专用的 PersistentVolumeClaim（如[图 10.2](#filepos957078)所示）。
- en: Figure 10.2\. Using one ReplicaSet for each pod instance
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2\. 为每个 pod 实例使用一个 ReplicaSet
- en: '![](images/00183.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00183.jpg)'
- en: Although this takes care of the automatic rescheduling in case of node failures
    or accidental pod deletions, it’s much more cumbersome compared to having a single
    ReplicaSet. For example, think about how you’d scale the pods in that case. You
    couldn’t change the desired replica count—you’d have to create additional ReplicaSets
    instead.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这可以处理节点故障或意外删除Pod的情况下的自动重新调度，但与单个ReplicaSet相比，操作要繁琐得多。例如，考虑一下在这种情况下如何扩展Pod。你无法更改期望的副本数量——你将不得不创建额外的ReplicaSet。
- en: Using multiple ReplicaSets is therefore not the best solution. But could you
    maybe use a single ReplicaSet and have each pod instance keep its own persistent
    state, even though they’re all using the same storage volume?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用多个ReplicaSet并不是最佳解决方案。但你是否可以使用单个ReplicaSet，并且让每个Pod实例保持其自己的持久状态，尽管它们都在使用相同的存储卷？
- en: Using multiple directories in the same volume
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一卷中使用多个目录
- en: A trick you can use is to have all pods use the same PersistentVolume, but then
    have a separate file directory inside that volume for each pod (this is shown
    in [figure 10.3](#filepos958285)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用的一个技巧是让所有Pod使用相同的PersistentVolume，然后在卷内为每个Pod创建一个单独的文件目录（如图10.3所示）。
- en: Figure 10.3\. Working around the shared storage problem by having the app in
    each pod use a different file directory
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3. 通过让每个Pod使用不同的文件目录来解决共享存储问题
- en: '![](images/00003.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00003.jpg)'
- en: Because you can’t configure pod replicas differently from a single pod template,
    you can’t tell each instance what directory it should use, but you can make each
    instance automatically select (and possibly also create) a data directory that
    isn’t being used by any other instance at that time. This solution does require
    coordination between the instances, and isn’t easy to do correctly. It also makes
    the shared storage volume the bottleneck.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你无法将Pod副本配置与单个Pod模板不同，因此你无法告诉每个实例它应该使用哪个目录，但你可以让每个实例自动选择（并可能创建）一个当时未被任何其他实例使用的数据目录。这种解决方案确实需要实例之间的协调，并且不容易正确执行。这也使得共享存储卷成为瓶颈。
- en: 10.1.2\. Providing a stable identity for each pod
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 10.1.2. 为每个Pod提供稳定的身份
- en: In addition to storage, certain clustered applications also require that each
    instance has a long-lived stable identity. Pods can be killed from time to time
    and replaced with new ones. When a ReplicaSet replaces a pod, the new pod is a
    completely new pod with a new hostname and IP, although the data in its storage
    volume may be that of the killed pod. For certain apps, starting up with the old
    instance’s data but with a completely new network identity may cause problems.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 除了存储之外，某些集群应用程序还要求每个实例具有长期稳定的身份。Pod有时会被杀死并替换为新的Pod。当ReplicaSet替换Pod时，新的Pod是一个全新的Pod，具有新的主机名和IP地址，尽管其存储卷中的数据可能是被杀死的Pod的数据。对于某些应用程序，使用旧实例的数据但具有完全新的网络身份启动可能会引起问题。
- en: Why do certain apps mandate a stable network identity? This requirement is fairly
    common in distributed stateful applications. Certain apps require the administrator
    to list all the other cluster members and their IP addresses (or hostnames) in
    each member’s configuration file. But in Kubernetes, every time a pod is rescheduled,
    the new pod gets both a new hostname and a new IP address, so the whole application
    cluster would have to be reconfigured every time one of its members is rescheduled.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么某些应用程序强制要求稳定的网络身份？这种要求在分布式有状态应用程序中相当常见。某些应用程序要求管理员在成员的配置文件中列出所有其他集群成员及其IP地址（或主机名）。但在Kubernetes中，每次Pod重新调度时，新的Pod都会获得一个新的主机名和新的IP地址，因此每次成员重新调度时，整个应用程序集群都需要重新配置。
- en: Using a dedicated service for each pod instance
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用为每个Pod实例提供专用服务
- en: A trick you can use to work around this problem is to provide a stable network
    address for cluster members by creating a dedicated Kubernetes Service for each
    individual member. Because service IPs are stable, you can then point to each
    member through its service IP (rather than the pod IP) in the configuration.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的另一个技巧是为集群成员提供一个稳定的网络地址，通过为每个成员创建一个专门的Kubernetes Service。因为服务IP是稳定的，所以你可以在配置中将每个成员指向其服务IP（而不是Pod
    IP）。
- en: This is similar to creating a ReplicaSet for each member to provide them with
    individual storage, as described previously. Combining these two techniques results
    in the setup shown in [figure 10.4](#filepos961040) (an additional service covering
    all the cluster members is also shown, because you usually need one for clients
    of the cluster).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于为每个成员创建一个ReplicaSet以提供它们各自的存储，如前所述。结合这两种技术，结果就是[图10.4](#filepos961040)中所示（还显示了覆盖所有集群成员的额外服务，因为通常你需要一个用于集群客户端）。
- en: Figure 10.4\. Using one Service and ReplicaSet per pod to provide a stable network
    address and an individual volume for each pod, respectively
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4. 使用每个Pod一个Service和一个ReplicaSet来提供每个Pod的稳定网络地址和各自的存储
- en: '![](images/00021.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00021.jpg)'
- en: The solution is not only ugly, but it still doesn’t solve everything. The individual
    pods can’t know which Service they are exposed through (and thus can’t know their
    stable IP), so they can’t self-register in other pods using that IP.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案不仅丑陋，而且仍然没有解决所有问题。单个Pod无法知道它们通过哪个Service暴露（因此无法知道它们的稳定IP），所以它们无法使用该IP在其它Pod中进行自我注册。
- en: Luckily, Kubernetes saves us from resorting to such complex solutions. The proper
    clean and simple way of running these special types of applications in Kubernetes
    is through a StatefulSet.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Kubernetes帮助我们避免了求助于如此复杂的解决方案。在Kubernetes中运行这些特殊类型应用程序的正确、简单方式是通过StatefulSet。
- en: 10.2\. Understanding StatefulSets
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 10.2. 理解StatefulSet
- en: Instead of using a ReplicaSet to run these types of pods, you create a StatefulSet
    resource, which is specifically tailored to applications where instances of the
    application must be treated as non-fungible individuals, with each one having
    a stable name and state.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是使用ReplicaSet来运行这些类型的Pod，你创建一个StatefulSet资源，该资源专门针对那些必须将应用程序的实例视为不可互换的个体，每个实例都有一个稳定的名称和状态。
- en: 10.2.1\. Comparing StatefulSets with ReplicaSets
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 10.2.1. 比较StatefulSet与ReplicaSet
- en: To understand the purpose of StatefulSets, it’s best to compare them to ReplicaSets
    or ReplicationControllers. But first let me explain them with a little analogy
    that’s widely used in the field.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解StatefulSet的目的，最好是将它们与ReplicaSet或ReplicationController进行比较。但首先让我用这个在领域内广泛使用的类比来解释它们。
- en: Understanding stateful pods with the pets vs. cattle analogy
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用宠物与牛的类比理解有状态Pod
- en: You may have already heard of the pets vs. cattle analogy. If not, let me explain
    it. We can treat our apps either as pets or as cattle.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经听说过宠物与牛的类比。如果没有，让我来解释一下。我们可以将我们的应用程序视为宠物或牛。
- en: '|  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: StatefulSets were initially called PetSets. That name comes from the pets vs.
    cattle analogy explained here.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet最初被称为PetSet。这个名字来源于这里解释的宠物与牛的类比。
- en: '|  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: We tend to treat our app instances as pets, where we give each instance a name
    and take care of each instance individually. But it’s usually better to treat
    instances as cattle and not pay special attention to each individual instance.
    This makes it easy to replace unhealthy instances without giving it a second thought,
    similar to the way a farmer replaces unhealthy cattle.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们倾向于将应用程序实例视为宠物，为每个实例命名并单独照顾每个实例。但通常将实例视为牛，不对每个个体实例给予特别注意会更好。这使得替换不健康的实例变得容易，无需多加思考，类似于农民替换不健康的牛。
- en: Instances of a stateless app, for example, behave much like heads of cattle.
    It doesn’t matter if an instance dies—you can create a new instance and people
    won’t notice the difference.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态应用程序的实例，例如，表现得非常像牛头。一个实例死亡并不重要——你可以创建一个新的实例，人们不会注意到差异。
- en: On the other hand, with stateful apps, an app instance is more like a pet. When
    a pet dies, you can’t go buy a new one and expect people not to notice. To replace
    a lost pet, you need to find a new one that looks and behaves exactly like the
    old one. In the case of apps, this means the new instance needs to have the same
    state and identity as the old one.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，对于有状态的应用程序，一个应用程序实例更像是一个宠物。当宠物死亡时，你不能去买一个新的，并期望人们不会注意到。要替换丢失的宠物，你需要找到一个看起来和表现完全像旧宠物的新宠物。在应用程序的情况下，这意味着新实例需要与旧实例具有相同的状态和身份。
- en: Comparing StatefulSets with ReplicaSets or ReplicationControllers
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 比较StatefulSet与ReplicaSet或ReplicationController
- en: Pod replicas managed by a ReplicaSet or ReplicationController are much like
    cattle. Because they’re mostly stateless, they can be replaced with a completely
    new pod replica at any time. Stateful pods require a different approach. When
    a stateful pod instance dies (or the node it’s running on fails), the pod instance
    needs to be resurrected on another node, but the new instance needs to get the
    same name, network identity, and state as the one it’s replacing. This is what
    happens when the pods are managed through a StatefulSet.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由ReplicaSet或ReplicationController管理的Pod副本就像牛群一样。因为它们大多数是无状态的，所以可以在任何时候用完全新的Pod副本替换它们。有状态的Pod需要不同的方法。当一个有状态的Pod实例死亡（或运行它的节点失败）时，Pod实例需要在另一个节点上复活，但新的实例需要获得与它所替代的实例相同的名称、网络标识和状态。这就是当Pod通过StatefulSet管理时发生的情况。
- en: A StatefulSet makes sure pods are rescheduled in such a way that they retain
    their identity and state. It also allows you to easily scale the number of pets
    up and down. A StatefulSet, like a ReplicaSet, has a desired replica count field
    that determines how many pets you want running at that time. Similar to ReplicaSets,
    pods are created from a pod template specified as part of the StatefulSet (remember
    the cookie-cutter analogy?). But unlike pods created by ReplicaSets, pods created
    by the StatefulSet aren’t exact replicas of each other. Each can have its own
    set of volumes—in other words, storage (and thus persistent state)—which differentiates
    it from its peers. Pet pods also have a predictable (and stable) identity instead
    of each new pod instance getting a completely random one.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet确保Pod以保留其标识和状态的方式进行重新调度。它还允许你轻松地增加或减少宠物的数量。StatefulSet，就像ReplicaSet一样，有一个期望的副本计数字段，它决定了你希望在该时间运行多少个宠物。类似于ReplicaSets，Pod是从作为StatefulSet一部分指定的Pod模板创建的（还记得切片机的类比吗？）。但是，与由ReplicaSet创建的Pod不同，由StatefulSet创建的Pod不是彼此的精确副本。每个Pod都可以有自己的卷集——换句话说，存储（以及持久状态），这使其与其同伴区分开来。宠物Pod也有可预测的（和稳定的）标识，而不是每个新的Pod实例都获得一个完全随机的标识。
- en: 10.2.2\. Providing a stable network identity
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 10.2.2. 提供稳定的网络标识
- en: Each pod created by a StatefulSet is assigned an ordinal index (zero-based),
    which is then used to derive the pod’s name and hostname, and to attach stable
    storage to the pod. The names of the pods are thus predictable, because each pod’s
    name is derived from the StatefulSet’s name and the ordinal index of the instance.
    Rather than the pods having random names, they’re nicely organized, as shown in
    the next figure.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由StatefulSet创建的每个Pod都被分配一个序号索引（从零开始），然后用于推导Pod的名称和主机名，并将稳定的存储附加到Pod上。因此，Pod的名称是可预测的，因为每个Pod的名称都是根据StatefulSet的名称和实例的序号索引推导出来的。与Pod具有随机名称不同，它们被很好地组织起来，如图所示。
- en: Figure 10.5\. Pods created by a StatefulSet have predictable names (and hostnames),
    unlike those created by a ReplicaSet
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5. 由StatefulSet创建的Pod具有可预测的名称（和主机名），这与由ReplicaSet创建的Pod不同
- en: '![](images/00042.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00042.jpg)'
- en: Introducing the governing Service
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍管理服务
- en: But it’s not all about the pods having a predictable name and hostname. Unlike
    regular pods, stateful pods sometimes need to be addressable by their hostname,
    whereas stateless pods usually don’t. After all, each stateless pod is like any
    other. When you need one, you pick any one of them. But with stateful pods, you
    usually want to operate on a specific pod from the group, because they differ
    from each other (they hold different state, for example).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 但这不仅仅是Pod具有可预测的名称和主机名。与常规Pod不同，有状态的Pod有时需要通过其主机名来寻址，而无状态的Pod通常不需要。毕竟，每个无状态Pod就像其他任何Pod一样。当你需要时，你可以选择任何一个。但是，对于有状态的Pod，你通常希望对组中的特定Pod进行操作，因为它们彼此不同（例如，它们持有不同的状态）。
- en: For this reason, a StatefulSet requires you to create a corresponding governing
    headless Service that’s used to provide the actual network identity to each pod.
    Through this Service, each pod gets its own DNS entry, so its peers and possibly
    other clients in the cluster can address the pod by its hostname. For example,
    if the governing Service belongs to the `default` namespace and is called `foo`,
    and one of the pods is called `A-0`, you can reach the pod through its fully qualified
    domain name, which is `a-0.foo.default.svc.cluster.local`. You can’t do that with
    pods managed by a ReplicaSet.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，StatefulSet 需要你创建一个相应的管理无头 Service，该 Service 用于为每个 pod 提供实际的网络身份。通过这个 Service，每个
    pod 都会获得自己的 DNS 条目，因此它的对等节点和集群中的其他客户端可以通过主机名来定位 pod。例如，如果管理 Service 属于 `default`
    命名空间，并且被命名为 `foo`，其中一个 pod 被命名为 `A-0`，你可以通过其完全限定域名 `a-0.foo.default.svc.cluster.local`
    来访问该 pod。你不能用由 ReplicaSet 管理的 pod 做到这一点。
- en: Additionally, you can also use DNS to look up all the StatefulSet’s pods’ names
    by looking up SRV records for the `foo.default.svc.cluster.local` domain. We’ll
    explain SRV records in [section 10.4](index_split_083.html#filepos1009336) and
    learn how they’re used to discover members of a StatefulSet.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还可以使用 DNS 通过查找 `foo.default.svc.cluster.local` 域的 SRV 记录来查找所有 StatefulSet
    的 pod 名称。我们将在[第 10.4 节](index_split_083.html#filepos1009336)中解释 SRV 记录，并了解它们是如何用于发现
    StatefulSet 的成员的。
- en: Replacing lost pets
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 替换丢失的宠物
- en: When a pod instance managed by a StatefulSet disappears (because the node the
    pod was running on has failed, it was evicted from the node, or someone deleted
    the pod object manually), the StatefulSet makes sure it’s replaced with a new
    instance—similar to how ReplicaSets do it. But in contrast to ReplicaSets, the
    replacement pod gets the same name and hostname as the pod that has disappeared
    (this distinction between ReplicaSets and StatefulSets is illustrated in [figure
    10.6](#filepos968872)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当由 StatefulSet 管理的 pod 实例消失（因为 pod 运行的节点失败，被从节点驱逐，或有人手动删除 pod 对象）时，StatefulSet
    确保用新的实例替换它——类似于 ReplicaSet 的做法。但与 ReplicaSet 相比，替换的 pod 将获得与消失的 pod 相同的名称和主机名（ReplicaSet
    和 StatefulSet 之间的这种区别在[图 10.6](#filepos968872) 中展示）。
- en: Figure 10.6\. A StatefulSet replaces a lost pod with a new one with the same
    identity, whereas a ReplicaSet replaces it with a completely new unrelated pod.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6\. StatefulSet 用具有相同身份的新 pod 替换丢失的 pod，而 ReplicaSet 则用完全无关的新 pod 替换它。
- en: '![](images/00060.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00060.jpg)'
- en: The new pod isn’t necessarily scheduled to the same node, but as you learned
    early on, what node a pod runs on shouldn’t matter. This holds true even for stateful
    pods. Even if the pod is scheduled to a different node, it will still be available
    and reachable under the same hostname as before.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 新 pod 不一定会被调度到同一个节点，但正如你早期所学的，pod 运行的节点不应该很重要。这对于有状态的 pod 也是成立的。即使 pod 被调度到不同的节点，它仍然会在之前的主机名下可用和可访问。
- en: Scaling a StatefulSet
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展 StatefulSet
- en: Scaling the StatefulSet creates a new pod instance with the next unused ordinal
    index. If you scale up from two to three instances, the new instance will get
    index 2 (the existing instances obviously have indexes 0 and 1).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展 StatefulSet 会创建一个新的 pod 实例，并使用下一个未使用的序号索引。如果你从两个实例扩展到三个实例，新实例将获得索引 2（现有的实例显然有索引
    0 和 1）。
- en: The nice thing about scaling down a StatefulSet is the fact that you always
    know what pod will be removed. Again, this is also in contrast to scaling down
    a ReplicaSet, where you have no idea what instance will be deleted, and you can’t
    even specify which one you want removed first (but this feature may be introduced
    in the future). Scaling down a StatefulSet always removes the instances with the
    highest ordinal index first (shown in [figure 10.7](#filepos970431)). This makes
    the effects of a scale-down predictable.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 缩小 StatefulSet 的好处是，你总是知道哪个 pod 将被移除。再次强调，这与缩小 ReplicaSet 相比形成对比，在缩小 ReplicaSet
    时，你不知道哪个实例将被删除，甚至无法指定首先删除哪个（但这个功能可能在未来被引入）。缩小 StatefulSet 总是首先删除具有最高序号索引的实例（如图
    10.7 所示）。这使得缩小的效果可预测。
- en: Figure 10.7\. Scaling down a StatefulSet always removes the pod with the highest
    ordinal index first.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7\. 缩小 StatefulSet 总是首先删除具有最高序号索引的 pod。
- en: '![](images/00080.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00080.jpg)'
- en: Because certain stateful applications don’t handle rapid scale-downs nicely,
    StatefulSets scale down only one pod instance at a time. A distributed data store,
    for example, may lose data if multiple nodes go down at the same time. For example,
    if a replicated data store is configured to store two copies of each data entry,
    in cases where two nodes go down at the same time, a data entry would be lost
    if it was stored on exactly those two nodes. If the scale-down was sequential,
    the distributed data store has time to create an additional replica of the data
    entry somewhere else to replace the (single) lost copy.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于某些有状态的应用程序不能很好地处理快速的缩放操作，StatefulSets一次只缩放一个Pod实例。例如，一个分布式数据存储，如果多个节点同时宕机，可能会丢失数据。例如，如果一个复制数据存储被配置为存储每个数据条目的两份副本，在两个节点同时宕机的情况下，如果数据条目正好存储在这两个节点上，那么数据条目就会丢失。如果缩放是顺序的，分布式数据存储就有时间在其他地方创建数据条目的额外副本来替换丢失的（单个）副本。
- en: For this exact reason, StatefulSets also never permit scale-down operations
    if any of the instances are unhealthy. If an instance is unhealthy, and you scale
    down by one at the same time, you’ve effectively lost two cluster members at once.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 正因为如此，如果任何实例不健康，StatefulSets也永远不会允许缩放操作。如果一个实例不健康，同时你减少一个实例，你实际上一次就失去了两个集群成员。
- en: 10.2.3\. Providing stable dedicated storage to each stateful instance
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 10.2.3. 为每个有状态实例提供稳定的专用存储
- en: You’ve seen how StatefulSets ensure stateful pods have a stable identity, but
    what about storage? Each stateful pod instance needs to use its own storage, plus
    if a stateful pod is rescheduled (replaced with a new instance but with the same
    identity as before), the new instance must have the same storage attached to it.
    How do Stateful-Sets achieve this?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到了StatefulSets如何确保有状态的Pod拥有稳定的身份，但关于存储呢？每个有状态的Pod实例都需要使用自己的存储，而且如果有一个有状态的Pod被重新调度（用新的实例替换，但保持与之前相同的身份），新的实例也必须附加相同的存储。StatefulSets是如何实现这一点的呢？
- en: Obviously, storage for stateful pods needs to be persistent and decoupled from
    the pods. In [chapter 6](index_split_055.html#filepos588298) you learned about
    PersistentVolumes and PersistentVolumeClaims, which allow persistent storage to
    be attached to a pod by referencing the Persistent-VolumeClaim in the pod by name.
    Because PersistentVolumeClaims map to PersistentVolumes one-to-one, each pod of
    a StatefulSet needs to reference a different PersistentVolumeClaim to have its
    own separate PersistentVolume. Because all pod instances are stamped from the
    same pod template, how can they each refer to a different PersistentVolumeClaim?
    And who creates these claims? Surely you’re not expected to create as many PersistentVolumeClaims
    as the number of pods you plan to have in the StatefulSet upfront? Of course not.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，有状态Pod的存储需要是持久的，并且与Pod解耦。在第6章[章节6](index_split_055.html#filepos588298)中，你学习了持久卷和持久卷声明，它们允许通过在Pod中按名称引用持久卷声明来将持久存储附加到Pod。因为持久卷声明与持久卷是一对一映射的，所以StatefulSet中的每个Pod都需要引用不同的持久卷声明，以便拥有自己的独立持久卷。因为所有Pod实例都是从相同的Pod模板生成的，它们如何各自引用不同的持久卷声明呢？而且谁会创建这些声明？当然，你不应该期望在StatefulSet中预先创建与计划拥有的Pod数量一样多的持久卷声明，对吧？
- en: Teaming up pod templates with volume claim templates
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 将Pod模板与卷声明模板配对
- en: The StatefulSet has to create the PersistentVolumeClaims as well, the same way
    it’s creating the pods. For this reason, a StatefulSet can also have one or more
    volume claim templates, which enable it to stamp out PersistentVolumeClaims along
    with each pod instance (see [figure 10.8](#filepos973434)).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet还必须创建持久卷声明，就像它创建Pod一样。因此，StatefulSet也可以有一个或多个卷声明模板，这使它能够与每个Pod实例一起打印出持久卷声明（参见[图10.8](#filepos973434)）。
- en: Figure 10.8\. A StatefulSet creates both pods and PersistentVolumeClaims.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8. StatefulSet创建Pod和持久卷声明。
- en: '![](images/00098.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00098.jpg)'
- en: The PersistentVolumes for the claims can either be provisioned up-front by an
    administrator or just in time through dynamic provisioning of PersistentVolumes,
    as explained at the end of [chapter 6](index_split_055.html#filepos588298).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于持久卷声明，管理员可以提前配置持久卷，或者通过持久卷的动态配置来即时配置，如第6章末所述[章节6](index_split_055.html#filepos588298)。
- en: Understanding the creation and deletion of PersistentVolumeClaims
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 理解持久卷声明的创建和删除
- en: Scaling up a StatefulSet by one creates two or more API objects (the pod and
    one or more PersistentVolumeClaims referenced by the pod). Scaling down, however,
    deletes only the pod, leaving the claims alone. The reason for this is obvious,
    if you consider what happens when a claim is deleted. After a claim is deleted,
    the PersistentVolume it was bound to gets recycled or deleted and its contents
    are lost.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 通过增加一个 StatefulSet 来进行缩放会创建两个或更多的 API 对象（pod 和 pod 引用的一个或多个 PersistentVolumeClaims）。然而，缩放回时，只会删除
    pod，而不会删除声明。这个原因很明显，如果你考虑一下删除声明时会发生什么。在删除声明后，与之绑定的 PersistentVolume 会被回收或删除，其内容也会丢失。
- en: Because stateful pods are meant to run stateful applications, which implies
    that the data they store in the volume is important, deleting the claim on scale-down
    of a Stateful-Set could be catastrophic—especially since triggering a scale-down
    is as simple as decreasing the `replicas` field of the StatefulSet. For this reason,
    you’re required to delete PersistentVolumeClaims manually to release the underlying
    PersistentVolume.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因为状态化 pod 是为了运行状态化应用程序，这意味着它们在卷中存储的数据很重要，所以在 Stateful-Set 缩小时删除声明可能是灾难性的——特别是触发缩放只需像减小
    StatefulSet 的 `replicas` 字段一样简单。因此，你必须手动删除 PersistentVolumeClaims 来释放底层的 PersistentVolume。
- en: Reattaching the PersistentVolumeClaim to the new instance of the same pod
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 将 PersistentVolumeClaim 重新附加到同一 pod 的新实例
- en: The fact that the PersistentVolumeClaim remains after a scale-down means a subsequent
    scale-up can reattach the same claim along with the bound PersistentVolume and
    its contents to the new pod instance (shown in [figure 10.9](#filepos975566)).
    If you accidentally scale down a StatefulSet, you can undo the mistake by scaling
    up again and the new pod will get the same persisted state again (as well as the
    same name).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放回后 PersistentVolumeClaim 仍然存在，这意味着后续的缩放可以将相同的声明以及与之绑定的 PersistentVolume 和其内容重新附加到新的
    pod 实例（如图 10.9 所示[figure 10.9](#filepos975566)）。如果你意外地缩小了 StatefulSet，你可以通过再次缩放来撤销错误，新的
    pod 将再次获得相同的持久状态（以及相同的名称）。
- en: Figure 10.9\. StatefulSets don’t delete PersistentVolumeClaims when scaling
    down; then they reattach them when scaling back up.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9\. StatefulSets 在缩放时不会删除 PersistentVolumeClaims；然后在缩放回时重新附加它们。
- en: '![](images/00114.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00114.jpg)'
- en: 10.2.4\. Understanding StatefulSet guarantees
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 10.2.4\. 理解 StatefulSet 保证
- en: As you’ve seen so far, StatefulSets behave differently from ReplicaSets or ReplicationControllers.
    But this doesn’t end with the pods having a stable identity and storage. StatefulSets
    also have different guarantees regarding their pods.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，StatefulSets 的行为与 ReplicaSets 或 ReplicationControllers 不同。但这并不仅限于 pod 具有稳定的身份和存储。StatefulSets
    还对其 pod 有不同的保证。
- en: Understanding the implications of stable identity and storage
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 理解稳定身份和存储的影响
- en: While regular, stateless pods are fungible, stateful pods aren’t. We’ve already
    seen how a stateful pod is always replaced with an identical pod (one having the
    same name and hostname, using the same persistent storage, and so on). This happens
    when Kubernetes sees that the old pod is no longer there (for example, when you
    delete the pod manually).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然，常规的无状态 pod 是可互换的，但状态化 pod 不是。我们已经看到状态化 pod 总是会被一个相同的 pod 替换（一个具有相同名称和主机名，使用相同的持久存储等）。当
    Kubernetes 发现旧 pod 已不再存在时（例如，当你手动删除 pod 时），这种情况就会发生。
- en: But what if Kubernetes can’t be sure about the state of the pod? If it creates
    a replacement pod with the same identity, two instances of the app with the same
    identity might be running in the system. The two would also be bound to the same
    storage, so two processes with the same identity would be writing over the same
    files. With pods managed by a ReplicaSet, this isn’t a problem, because the apps
    are obviously made to work on the same files. Also, ReplicaSets create pods with
    a randomly generated identity, so there’s no way for two processes to run with
    the same identity.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果 Kubernetes 无法确定 pod 的状态怎么办？如果它创建了一个具有相同身份的替换 pod，系统中可能会运行两个具有相同身份的应用实例。这两个实例也会绑定到相同的存储，因此具有相同身份的两个进程会覆盖相同的文件。由
    ReplicaSet 管理的 pod，这并不是一个问题，因为应用显然是为了在相同的文件上工作而设计的。此外，ReplicaSets 会创建具有随机生成身份的
    pod，因此两个进程以相同的身份运行是不可能的。
- en: Introducing StatefulSet’s at-most-one semantics
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍 StatefulSet 的最多一个语义
- en: Kubernetes must thus take great care to ensure two stateful pod instances are
    never running with the same identity and are bound to the same PersistentVolumeClaim.
    A StatefulSet must guarantee at-most-one semantics for stateful pod instances.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Kubernetes 必须非常小心，确保两个有状态 Pod 实例永远不会以相同的身份运行，并且绑定到相同的 PersistentVolumeClaim。StatefulSet
    必须保证有状态 Pod 实例最多只有一个语义。
- en: This means a StatefulSet must be absolutely certain that a pod is no longer
    running before it can create a replacement pod. This has a big effect on how node
    failures are handled. We’ll demonstrate this later in the chapter. Before we can
    do that, however, you need to create a StatefulSet and see how it behaves. You’ll
    also learn a few more things about them along the way.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 StatefulSet 必须在创建替换 Pod 之前绝对确定 Pod 已经不再运行。这对处理节点故障有很大影响。我们将在本章后面演示这一点。然而，在我们这样做之前，你需要创建一个
    StatefulSet 并观察它的行为。你还将在这个过程中了解它们的一些其他特性。
- en: 10.3\. Using a StatefulSet
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 10.3\. 使用 StatefulSet
- en: To properly show StatefulSets in action, you’ll build your own little clustered
    data store. Nothing fancy—more like a data store from the Stone Age.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确展示 StatefulSet 的作用，你将构建自己的小型集群化数据存储。没有什么花哨的——更像是石器时代的数据存储。
- en: 10.3.1\. Creating the app and container image
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 10.3.1\. 创建应用和容器镜像
- en: You’ll use the kubia app you’ve used throughout the book as your starting point.
    You’ll expand it so it allows you to store and retrieve a single data entry on
    each pod instance.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用本书中一直使用的 kubia 应用作为起点。你将扩展它，使其能够存储和检索每个 Pod 实例的单个数据条目。
- en: The important parts of the source code of your data store are shown in the following
    listing.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你的数据存储源代码的重要部分如下所示。
- en: 'Listing 10.1\. A simple stateful app: kubia-pet-image/app.js'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.1\. 一个简单的有状态应用：kubia-pet-image/app.js
- en: '`... const dataFile = "/var/data/kubia.txt"; ... var handler = function(request,
    response) {   if (request.method == ''POST'') {     var file = fs.createWriteStream(dataFile);`
    `1` `file.on(''open'', function (fd) {` `1` `request.pipe(file);` `1` `console.log("New
    data has been received and stored.");` `1` `response.writeHead(200);` `1` `response.end("Data
    stored on pod " + os.hostname() + "\n");` `1` `});   } else {     var data = fileExists(dataFile)`
    `2` `? fs.readFileSync(dataFile, ''utf8'')` `2` `: "No data posted yet";` `2`
    `response.writeHead(200);` `2` `response.write("You''ve hit " + os.hostname()
    + "\n");` `2` `response.end("Data stored on this pod: " + data + "\n");` `2` `}
    };  var www = http.createServer(handler); www.listen(8080);`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`... const dataFile = "/var/data/kubia.txt"; ... var handler = function(request,
    response) {   if (request.method == ''POST'') {     var file = fs.createWriteStream(dataFile);`
    `1` `file.on(''open'', function (fd) {` `1` `request.pipe(file);` `1` `console.log("New
    data has been received and stored.");` `1` `response.writeHead(200);` `1` `response.end("Data
    stored on pod " + os.hostname() + "\n");` `1` `});   } else {     var data = fileExists(dataFile)`
    `2` `? fs.readFileSync(dataFile, ''utf8'')` `2` `: "No data posted yet";` `2`
    `response.writeHead(200);` `2` `response.write("You''ve hit " + os.hostname()
    + "\n");` `2` `response.end("Data stored on this pod: " + data + "\n");` `2` `}
    };  var www = http.createServer(handler); www.listen(8080);`'
- en: 1 On POST requests, store the request’s body into a data file.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 在 POST 请求中，将请求体存储到数据文件中。
- en: 2 On GET (and all other types of) requests, return your hostname and the contents
    of the data file.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 在 GET（以及所有其他类型的）请求中，返回你的主机名和数据文件的内容。
- en: Whenever the app receives a POST request, it writes the data it receives in
    the body of the request to the file `/`var/data/kubia.txt. Upon a GET request,
    it returns the hostname and the stored data (contents of the file). Simple enough,
    right? This is the first version of your app. It’s not clustered yet, but it’s
    enough to get you started. You’ll expand the app later in the chapter.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用收到 POST 请求时，它会将请求体中的数据写入到文件 `/var/data/kubia.txt`。在 GET 请求中，它返回主机名和存储的数据（文件内容）。很简单，对吧？这是你应用的第一个版本。它还没有集群化，但足以让你开始。你将在本章后面扩展应用。
- en: The Dockerfile for building the container image is shown in the following listing
    and hasn’t changed from before.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 构建容器镜像的 Dockerfile 如下所示，与之前没有变化。
- en: 'Listing 10.2\. Dockerfile for the stateful app: kubia-pet-image/Dockerfile'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.2\. 有状态应用的 Dockerfile：kubia-pet-image/Dockerfile
- en: '`FROM node:7 ADD app.js /app.js ENTRYPOINT ["node", "app.js"]`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`FROM node:7 ADD app.js /app.js ENTRYPOINT ["node", "app.js"]`'
- en: Go ahead and build the image now, or use the one I pushed to docker.io/luksa/kubia-pet.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在开始构建镜像，或者使用我推送到 docker.io/luksa/kubia-pet 的镜像。
- en: 10.3.2\. Deploying the app through a StatefulSet
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 10.3.2\. 通过 StatefulSet 部署应用
- en: 'To deploy your app, you’ll need to create two (or three) different types of
    objects:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署你的应用，你需要创建两种（或三种）不同类型的对象：
- en: PersistentVolumes for storing your data files (you’ll need to create these only
    if the cluster doesn’t support dynamic provisioning of PersistentVolumes).
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于存储你的数据文件的PersistentVolumes（只有当集群不支持PersistentVolumes的动态供应时，你才需要创建这些卷）。
- en: A governing Service required by the StatefulSet.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态集所需的管理服务。
- en: The StatefulSet itself.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态集本身。
- en: For each pod instance, the StatefulSet will create a PersistentVolumeClaim that
    will bind to a PersistentVolume. If your cluster supports dynamic provisioning,
    you don’t need to create any PersistentVolumes manually (you can skip the next
    section). If it doesn’t, you’ll need to create them as explained in the next section.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个Pod实例，状态集将创建一个PersistentVolumeClaim，并将其绑定到一个PersistentVolume。如果你的集群支持动态供应，你不需要手动创建任何PersistentVolumes（你可以跳过下一节）。如果不支持，你将需要按照下一节所述创建它们。
- en: Creating the persistent volumes
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 创建持久卷
- en: You’ll need three PersistentVolumes, because you’ll be scaling the StatefulSet
    up to three replicas. You must create more if you plan on scaling the StatefulSet
    up more than that.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要三个PersistentVolumes，因为你将扩展状态集到三个副本。如果你计划将状态集扩展得更多，你必须创建更多的PersistentVolumes。
- en: If you’re using Minikube, deploy the PersistentVolumes defined in the Chapter06/
    persistent-volumes-hostpath.yaml file in the book’s code archive.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用Minikube，请在书籍代码存档中的Chapter06/persistent-volumes-hostpath.yaml文件中部署定义的PersistentVolumes。
- en: 'If you’re using Google Kubernetes Engine, you’ll first need to create the actual
    GCE Persistent Disks like this:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用Google Kubernetes Engine，你首先需要创建实际的GCE Persistent Disks，如下所示：
- en: '`$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-a $ gcloud
    compute disks create --size=1GiB --zone=europe-west1-b pv-b $ gcloud compute disks
    create --size=1GiB --zone=europe-west1-b pv-c`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-a $ gcloud
    compute disks create --size=1GiB --zone=europe-west1-b pv-b $ gcloud compute disks
    create --size=1GiB --zone=europe-west1-b pv-c`'
- en: '|  |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Make sure to create the disks in the same zone that your nodes are running in.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在节点运行的同一区域创建磁盘。
- en: '|  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Then create the PersistentVolumes from the persistent-volumes-gcepd.yaml file,
    which is shown in the following listing.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然后从persistent-volumes-gcepd.yaml文件创建PersistentVolumes，该文件如下所示。
- en: 'Listing 10.3\. Three PersistentVolumes: persistent-volumes-gcepd.yaml'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.3\. 三个PersistentVolumes：persistent-volumes-gcepd.yaml
- en: '`kind: List` `1` `apiVersion: v1 items: - apiVersion: v1   kind: PersistentVolume`
    `1` `metadata:     name: pv-a` `2` `spec:     capacity:       storage: 1Mi` `3`
    `accessModes:       - ReadWriteOnce     persistentVolumeReclaimPolicy: Recycle`
    `4` `gcePersistentDisk:` `5` `pdName: pv-a` `5` `fsType: nfs4` `5` `- apiVersion:
    v1   kind: PersistentVolume   metadata:     name: pv-b ...`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`kind: List` `1` `apiVersion: v1 items: - apiVersion: v1   kind: PersistentVolume`
    `1` `metadata:     name: pv-a` `2` `spec:     capacity:       storage: 1Mi` `3`
    `accessModes:       - ReadWriteOnce     persistentVolumeReclaimPolicy: Recycle`
    `4` `gcePersistentDisk:` `5` `pdName: pv-a` `5` `fsType: nfs4` `5` `- apiVersion:
    v1   kind: PersistentVolume   metadata:     name: pv-b ...`'
- en: 1 File describes a list of three persistent volumes
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 文件描述了一个包含三个持久卷的列表
- en: 2 Persistent volumes’ names are pv-a, pv-b, and pv-c
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 持久卷的名称是pv-a、pv-b和pv-c
- en: 3 Capacity of each persistent volume is 1 Mebibyte
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 每个持久卷的容量是1梅吉字节
- en: 4 When the volume is released by the claim, it’s recycled to be used again.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4 当卷被声明释放时，它将被回收以再次使用。
- en: 5 The volume uses a GCE Persistent Disk as the underlying storage mechanism.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 5 该卷使用GCE Persistent Disk作为底层存储机制。
- en: '|  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In the previous chapter you specified multiple resources in the same YAML by
    delimiting them with a three-dash line. Here you’re using a different approach
    by defining a `List` object and listing the resources as items of the object.
    Both methods are equivalent.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你通过使用三横线分隔来在同一个YAML文件中指定多个资源。在这里，你使用了一种不同的方法，通过定义一个`List`对象并将资源作为对象的项目列出。这两种方法都是等效的。
- en: '|  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: This manifest creates PersistentVolumes called `pv-a`, `pv-b`, and `pv-c`. They
    use GCE Persistent Disks as the underlying storage mechanism, so they’re not appropriate
    for clusters that aren’t running on Google Kubernetes Engine or Google Compute
    Engine. If you’re running the cluster elsewhere, you must modify the PersistentVolume
    definition and use an appropriate volume type, such as NFS (Network File System),
    or similar.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此清单创建了名为`pv-a`、`pv-b`和`pv-c`的PersistentVolumes。它们使用GCE Persistent Disks作为底层存储机制，因此它们不适用于不在Google
    Kubernetes Engine或Google Compute Engine上运行的集群。如果你在其他地方运行集群，你必须修改PersistentVolume定义并使用适当的卷类型，例如NFS（网络文件系统），或类似类型。
- en: Creating the governing Service
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 创建管理服务
- en: As explained earlier, before deploying a StatefulSet, you first need to create
    a headless Service, which will be used to provide the network identity for your
    stateful pods. The following listing shows the Service manifest.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在部署状态集之前，您首先需要创建一个无头服务，该服务将用于为您的有状态Pod提供网络标识。以下列表显示了服务清单。
- en: 'Listing 10.4\. Headless service to be used in the StatefulSet: kubia-service-headless.yaml'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.4\. 状态集将使用的无头服务：kubia-service-headless.yaml
- en: '`apiVersion: v1 kind: Service metadata:   name: kubia` `1` `spec:   clusterIP:
    None` `2` `selector:` `3` `app: kubia` `3` `ports:   - name: http     port: 80`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Service metadata:   name: kubia` `1` `spec:   clusterIP:
    None` `2` `selector:` `3` `app: kubia` `3` `ports:   - name: http     port: 80`'
- en: 1 Name of the Service
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 服务的名称
- en: 2 The StatefulSet’s governing Service must be headless.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 状态集的主控服务必须是无头服务。
- en: 3 All pods with the app=kubia label belong to this service.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 所有带有app=kubia标签的Pod都属于此服务。
- en: You’re setting the `clusterIP` field to `None`, which makes this a headless
    Service. It will enable peer discovery between your pods (you’ll need this later).
    Once you create the Service, you can move on to creating the actual StatefulSet.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 您将`clusterIP`字段设置为`None`，这使得这是一个无头服务。它将启用Pod之间的对等发现（您稍后会需要）。一旦创建服务，您就可以继续创建实际的状态集。
- en: Creating the StatefulSet manifest
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 创建状态集清单
- en: Now you can finally create the StatefulSet. The following listing shows the
    manifest.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您终于可以创建状态集了。以下列表显示了清单。
- en: 'Listing 10.5\. StatefulSet manifest: kubia-statefulset.yaml'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.5\. 状态集清单：kubia-statefulset.yaml
- en: '`apiVersion: apps/v1beta1 kind: StatefulSet metadata:   name: kubia spec:  
    serviceName: kubia   replicas: 2   template:     metadata:       labels:` `1`
    `app: kubia` `1` `spec:       containers:       - name: kubia         image: luksa/kubia-pet
            ports:         - name: http           containerPort: 8080         volumeMounts:
            - name: data` `2` `mountPath: /var/data` `2` `volumeClaimTemplates:  
    - metadata:` `3` `name: data` `3` `spec:` `3` `resources:` `3` `requests:` `3`
    `storage: 1Mi` `3` `accessModes:` `3` `- ReadWriteOnce` `3`'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: apps/v1beta1 kind: StatefulSet metadata:   name: kubia spec:  
    serviceName: kubia   replicas: 2   template:     metadata:       labels:` `1`
    `app: kubia` `1` `spec:       containers:       - name: kubia         image: luksa/kubia-pet
            ports:         - name: http           containerPort: 8080         volumeMounts:
            - name: data` `2` `mountPath: /var/data` `2` `volumeClaimTemplates:  
    - metadata:` `3` `name: data` `3` `spec:` `3` `resources:` `3` `requests:` `3`
    `storage: 1Mi` `3` `accessModes:` `3` `- ReadWriteOnce` `3`'
- en: 1 Pods created by the StatefulSet will have the app=kubia label.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 状态集创建的Pod将具有app=kubia标签。
- en: 2 The container inside the pod will mount the pvc volume at this path.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 Pod内部的容器将在此路径上挂载pvc卷。
- en: 3 The PersistentVolumeClaims will be created from this template.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 将从该模板创建持久卷声明。
- en: The StatefulSet manifest isn’t that different from ReplicaSet or Deployment
    manifests you’ve created so far. What’s new is the `volumeClaimTemplates` list.
    In it, you’re defining one volume claim template called `data`, which will be
    used to create a PersistentVolumeClaim for each pod. As you may remember from
    [chapter 6](index_split_055.html#filepos588298), a pod references a claim by including
    a `persistentVolumeClaim` volume in the manifest. In the previous pod template,
    you’ll find no such volume. The StatefulSet adds it to the pod specification automatically
    and configures the volume to be bound to the claim the StatefulSet created for
    the specific pod.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 状态集清单与您迄今为止创建的副本集或部署清单没有太大区别。新的内容是`volumeClaimTemplates`列表。在其中，您定义了一个名为`data`的卷声明模板，该模板将用于为每个Pod创建一个持久卷声明。如您从[第6章](index_split_055.html#filepos588298)中可能记得的那样，Pod通过在清单中包含一个`persistentVolumeClaim`卷来引用声明。在前一个Pod模板中，您找不到这样的卷。状态集会自动将其添加到Pod规范中，并将卷配置为绑定到状态集为特定Pod创建的声明。
- en: Creating the StatefulSet
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 创建状态集
- en: 'You’ll create the StatefulSet now:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您将创建状态集：
- en: '`$ kubectl create -f kubia-statefulset.yaml` `statefulset "kubia" created`'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl create -f kubia-statefulset.yaml` `statefulset "kubia" created`'
- en: 'Now, list your pods:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，列出您的Pod：
- en: '`$ kubectl get po` `NAME      READY     STATUS              RESTARTS   AGE
    kubia-0   0/1       ContainerCreating   0          1s`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po` `NAME      READY     STATUS              RESTARTS   AGE
    kubia-0   0/1       ContainerCreating   0          1s`'
- en: Notice anything strange? Remember how a ReplicationController or a ReplicaSet
    creates all the pod instances at the same time? Your StatefulSet is configured
    to create two replicas, but it created a single pod.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到有什么奇怪的地方吗？还记得副本控制器或副本集是如何同时创建所有Pod实例的吗？您的状态集配置为创建两个副本，但它只创建了一个Pod。
- en: Don’t worry, nothing is wrong. The second pod will be created only after the
    first one is up and ready. StatefulSets behave this way because certain clustered
    stateful apps are sensitive to race conditions if two or more cluster members
    come up at the same time, so it’s safer to bring each member up fully before continuing
    to bring up the rest.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 别担心，没有问题。第二个pod将在第一个pod启动并就绪后创建。StatefulSets以这种方式运行，因为某些集群状态应用程序对同时启动的两个或多个集群成员的竞争条件敏感，所以在继续启动其他成员之前，先完全启动每个成员更安全。
- en: 'List the pods again to see how the pod creation is progressing:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 再次列出pods以查看pod创建的进度：
- en: '`$ kubectl get po` `NAME      READY     STATUS              RESTARTS   AGE
    kubia-0   1/1       Running             0          8s kubia-1   0/1       ContainerCreating  
    0          2s`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po` `NAME      READY     STATUS              RESTARTS   AGE
    kubia-0   1/1       Running             0          8s kubia-1   0/1       ContainerCreating  
    0          2s`'
- en: See, the first pod is now running, and the second one has been created and is
    being started.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 看看，第一个pod现在正在运行，第二个pod已经被创建并正在启动。
- en: Examining the generated stateful pod
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 检查生成的有状态pod
- en: Let’s take a closer look at the first pod’s spec in the following listing to
    see how the StatefulSet has constructed the pod from the pod template and the
    PersistentVolumeClaim template.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看以下列表中第一个pod的spec，以了解StatefulSet是如何从pod模板和PersistentVolumeClaim模板构建pod的。
- en: Listing 10.6\. A stateful pod created by the StatefulSet
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.6\. 由StatefulSet创建的有状态pod
- en: '`$ kubectl get po kubia-0 -o yaml` `apiVersion: v1 kind: Pod metadata:   ...
    spec:   containers:   - image: luksa/kubia-pet     ...     volumeMounts:     -
    mountPath: /var/data` `1` `name: data` `1` `- mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: default-token-r2m41       readOnly: true   ...   volumes:   - name:
    data` `2` `persistentVolumeClaim:` `2` `claimName: data-kubia-0` `3` `- name:
    default-token-r2m41     secret:       secretName: default-token-r2m41`'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po kubia-0 -o yaml` `apiVersion: v1 kind: Pod metadata:   ...
    spec:   containers:   - image: luksa/kubia-pet     ...     volumeMounts:     -
    mountPath: /var/data` `1` `name: data` `1` `- mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: default-token-r2m41       readOnly: true   ...   volumes:   - name:
    data` `2` `persistentVolumeClaim:` `2` `claimName: data-kubia-0` `3` `- name:
    default-token-r2m41     secret:       secretName: default-token-r2m41`'
- en: 1 The volume mount, as specified in the manifest
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 如清单中指定的卷挂载
- en: 2 The volume created by the StatefulSet
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 由StatefulSet创建的卷
- en: 3 The claim referenced by this volume
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 由这个卷引用的声明
- en: The PersistentVolumeClaim template was used to create the PersistentVolumeClaim
    and the volume inside the pod, which refers to the created PersistentVolumeClaim.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PersistentVolumeClaim模板创建了PersistentVolumeClaim以及pod内部的卷，这指的是创建的PersistentVolumeClaim。
- en: Examining the generated PersistentVolumeClaims
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 检查生成的PersistentVolumeClaims
- en: 'Now list the generated PersistentVolumeClaims to confirm they were created:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在列出生成的PersistentVolumeClaims以确认它们已被创建：
- en: '`$ kubectl get pvc` `NAME           STATUS    VOLUME    CAPACITY   ACCESSMODES  
    AGE data-kubia-0   Bound     pv-c      0                        37s data-kubia-1  
    Bound     pv-a      0                        37s`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get pvc` `NAME           STATUS    VOLUME    CAPACITY   ACCESSMODES  
    AGE data-kubia-0   Bound     pv-c      0                        37s data-kubia-1  
    Bound     pv-a      0                        37s`'
- en: The names of the generated PersistentVolumeClaims are composed of the name defined
    in the `volumeClaimTemplate` and the name of each pod. You can examine the claims’
    YAML to see that they match the template.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的PersistentVolumeClaims的名称由`volumeClaimTemplate`中定义的名称和每个pod的名称组成。你可以检查声明的YAML以查看它们是否与模板匹配。
- en: 10.3.3\. Playing with your pods
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 10.3.3\. 玩转你的pods
- en: With the nodes of your data store cluster now running, you can start exploring
    it. You can’t communicate with your pods through the Service you created because
    it’s headless. You’ll need to connect to individual pods directly (or create a
    regular Service, but that wouldn’t allow you to talk to a specific pod).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你的数据存储集群的节点现在正在运行，你可以开始探索它。你不能通过你创建的服务与你的pods通信，因为它是无头的。你需要直接连接到单个pods（或者创建一个常规服务，但这不会允许你与特定的pod通信）。
- en: 'You’ve already seen ways to connect to a pod directly: by piggybacking on another
    pod and running `curl` inside it, by using port-forwarding, and so on. This time,
    you’ll try another option. You’ll use the API server as a proxy to the pods.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到了直接连接到pod的方法：通过在其他pod上附加并运行`curl`，通过端口转发，等等。这次，你将尝试另一个选项。你将使用API服务器作为pods的代理。
- en: Communicating with pods through the API server
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 通过API服务器与pods通信
- en: 'One useful feature of the API server is the ability to proxy connections directly
    to individual pods. If you want to perform requests against your `kubia-0` pod,
    you hit the following URL:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: API 服务器的一个有用功能是能够直接代理连接到单个 pod。如果你想对你的 `kubia-0` pod 执行请求，你将访问以下 URL：
- en: '`<apiServerHost>:<port>/api/v1/namespaces/default/pods/kubia-0/proxy/<path>`'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`<apiServerHost>:<port>/api/v1/namespaces/default/pods/kubia-0/proxy/<path>`'
- en: 'Because the API server is secured, sending requests to pods through the API
    server is cumbersome (among other things, you need to pass the authorization token
    in each request). Luckily, in [chapter 8](index_split_070.html#filepos790863)
    you learned how to use `kubectl proxy` to talk to the API server without having
    to deal with authentication and SSL certificates. Run the proxy again:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 API 服务器是受保护的，通过 API 服务器向 pods 发送请求是繁琐的（例如，你需要在每个请求中传递授权令牌）。幸运的是，在 [第 8 章](index_split_070.html#filepos790863)
    中你学习了如何使用 `kubectl proxy` 来与 API 服务器通信，而无需处理身份验证和 SSL 证书。再次运行代理：
- en: '`$ kubectl proxy` `Starting to serve on 127.0.0.1:8001`'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl proxy` `开始服务在 127.0.0.1:8001`'
- en: 'Now, because you’ll be talking to the API server through the `kubectl` proxy,
    you’ll use localhost:8001 rather than the actual API server host and port. You’ll
    send a request to the `kubia-0` pod like this:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，因为你将通过 `kubectl` 代理与 API 服务器通信，所以你会使用 localhost:8001 而不是实际的 API 服务器主机和端口。你将像这样向
    `kubia-0` pod 发送请求：
- en: '`$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/` `You''ve
    hit kubia-0 Data stored on this pod: No data posted yet`'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/` `你已访问
    kubia-0 存储在此 pod 上的数据：No data posted yet`'
- en: The response shows that the request was indeed received and handled by the app
    running in your pod `kubia-0`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 响应显示，请求确实已被运行在你的 pod `kubia-0` 中的应用程序接收和处理。
- en: '|  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you receive an empty response, make sure you haven’t left out that last slash
    character at the end of the URL (or make sure `curl` follows redirects by using
    its `-L` option).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你收到一个空响应，请确保你没有在 URL 的末尾遗漏最后一个斜杠字符（或者确保 `curl` 通过使用其 `-L` 选项跟随重定向）。
- en: '|  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Because you’re communicating with the pod through the API server, which you’re
    connecting to through the `kubectl` proxy, the request went through two different
    proxies (the first was the `kubectl` proxy and the other was the API server, which
    proxied the request to the pod). For a clearer picture, examine [figure 10.10](#filepos1000671).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 因为你是通过连接到 `kubectl` 代理的 API 服务器与 pod 通信的，所以请求通过了两个不同的代理（第一个是 `kubectl` 代理，另一个是代理请求到
    pod 的 API 服务器）。为了更清晰地了解，请查看 [图 10.10](#filepos1000671)。
- en: Figure 10.10\. Connecting to a pod through both the `kubectl` proxy and API
    server proxy
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10\. 通过 `kubectl` 代理和 API 服务器代理连接到 pod
- en: '![](images/00134.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00134.jpg)'
- en: The request you sent to the pod was a GET request, but you can also send POST
    requests through the API server. This is done by sending a POST request to the
    same proxy URL as the one you sent the GET request to.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 你发送到 pod 的请求是一个 GET 请求，但你也可以通过 API 服务器发送 POST 请求。这是通过向发送 GET 请求的相同代理 URL 发送
    POST 请求来完成的。
- en: 'When your app receives a POST request, it stores whatever’s in the request
    body into a local file. Send a POST request to the `kubia-0` pod:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的应用程序收到一个 POST 请求时，它会将请求体中的内容存储到一个本地文件中。向 `kubia-0` pod 发送一个 POST 请求：
- en: '`$ curl -X POST -d "Hey there! This greeting was submitted to kubia-0."`![](images/00006.jpg)`localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/`
    `Data stored on pod kubia-0`'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ curl -X POST -d "Hey there! This greeting was submitted to kubia-0."`![](images/00006.jpg)`localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/`
    `存储在 pod kubia-0 上的数据`'
- en: 'The data you sent should now be stored in that pod. Let’s see if it returns
    the stored data when you perform a GET request again:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 你发送的数据现在应该存储在那个 pod 中。让我们看看当你再次执行 GET 请求时，它是否会返回存储的数据：
- en: '`$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/` `You''ve
    hit kubia-0 Data stored on this pod: Hey there! This greeting was submitted to
    kubia-0.`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/` `你已访问
    kubia-0 存储在此 pod 上的数据：Hey there! This greeting was submitted to kubia-0.`'
- en: 'Okay, so far so good. Now let’s see what the other cluster node (the `kubia-1`
    pod) says:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，到目前为止一切顺利。现在让我们看看其他集群节点（`kubia-1` pod）会说什么：
- en: '`$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-1/proxy/` `You''ve
    hit kubia-1 Data stored on this pod: No data posted yet`'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-1/proxy/` `你已访问
    kubia-1 存储在此 pod 上的数据：No data posted yet`'
- en: As expected, each node has its own state. But is that state persisted? Let’s
    find out.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，每个节点都有自己的状态。但这个状态是否持久化？让我们来查明。
- en: Deleting a stateful pod to see if the rescheduled pod is reattach- hed to the
    same storage
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 删除有状态的Pod以查看重新调度的Pod是否连接到相同的存储
- en: 'You’re going to delete the `kubia-0` pod and wait for it to be rescheduled.
    Then you’ll see if it’s still serving the same data as before:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 你将要删除`kubia-0` Pod并等待它被重新调度。然后你会看到它是否仍然像之前一样提供相同的数据：
- en: '`$ kubectl delete po kubia-0` `pod "kubia-0" deleted`'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl delete po kubia-0` `pod "kubia-0" deleted`'
- en: 'If you list the pods, you’ll see that the pod is terminating:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你列出Pod，你会看到Pod正在终止：
- en: '`$ kubectl get po` `NAME      READY     STATUS        RESTARTS   AGE kubia-0  
    1/1       Terminating   0          3m kubia-1   1/1       Running       0         
    3m`'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po` `NAME      READY     STATUS        RESTARTS   AGE kubia-0  
    1/1       Terminating   0          3m kubia-1   1/1       Running       0         
    3m`'
- en: 'As soon as it terminates successfully, a new pod with the same name is created
    by the StatefulSet:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦成功终止，StatefulSet就会创建一个具有相同名称的新Pod：
- en: '`$ kubectl get po` `NAME      READY     STATUS              RESTARTS   AGE
    kubia-0   0/1       ContainerCreating   0          6s kubia-1   1/1       Running            
    0          4m` `$ kubectl get po` `NAME      READY     STATUS    RESTARTS   AGE
    kubia-0   1/1       Running   0          9s kubia-1   1/1       Running   0         
    4m`'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po` `NAME      READY     STATUS              RESTARTS   AGE
    kubia-0   0/1       ContainerCreating   0          6s kubia-1   1/1       Running            
    0          4m` `$ kubectl get po` `NAME      READY     STATUS    RESTARTS   AGE
    kubia-0   1/1       Running   0          9s kubia-1   1/1       Running   0         
    4m`'
- en: Let me remind you again that this new pod may be scheduled to any node in the
    cluster, not necessarily the same node that the old pod was scheduled to. The
    old pod’s whole identity (the name, hostname, and the storage) is effectively
    moved to the new node (as shown in [figure 10.11](#filepos1004697)). If you’re
    using Minikube, you can’t see this because it only runs a single node, but in
    a multi-node cluster, you may see the pod scheduled to a different node than before.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我再提醒你一次，这个新Pod可能被调度到集群中的任何节点，而不一定是旧Pod被调度到的相同节点。旧Pod的整个身份（名称、主机名和存储）实际上被移动到了新节点（如图10.11所示）。如果你使用Minikube，你无法看到这一点，因为它只运行一个节点，但在多节点集群中，你可能看到Pod被调度到了与之前不同的节点。
- en: Figure 10.11\. A stateful pod may be rescheduled to a different node, but it
    retains the name, hostname, and storage.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11\. 有状态的Pod可能被重新调度到不同的节点，但它保留了名称、主机名和存储。
- en: '![](images/00152.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00152.jpg)'
- en: 'With the new pod now running, let’s check to see if it has the exact same identity
    as in its previous incarnation. The pod’s name is the same, but what about the
    hostname and persistent data? You can ask the pod itself to confirm:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 新Pod现在正在运行，让我们检查它是否与其之前的形态具有完全相同的身份。Pod的名称是相同的，但主机名和持久数据呢？你可以要求Pod本身来确认：
- en: '`$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/` `You''ve
    hit kubia-0 Data stored on this pod: Hey there! This greeting was submitted to
    kubia-0.`'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/` `You''ve
    hit kubia-0 Data stored on this pod: Hey there! This greeting was submitted to
    kubia-0.`'
- en: The pod’s response shows that both the hostname and the data are the same as
    before, confirming that a StatefulSet always replaces a deleted pod with what’s
    effectively the exact same pod.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Pod的响应显示，主机名和数据与之前相同，这证实了StatefulSet总是用实际上完全相同的Pod替换被删除的Pod。
- en: Scaling a StatefulSet
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放StatefulSet
- en: Scaling down a StatefulSet and scaling it back up after an extended time period
    should be no different than deleting a pod and having the StatefulSet recreate
    it immediately. Remember that scaling down a StatefulSet only deletes the pods,
    but leaves the PersistentVolumeClaims untouched. I’ll let you try scaling down
    the StatefulSet yourself and confirm this behavior.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 缩小StatefulSet并在较长时间后将其扩大，应该与删除Pod并让StatefulSet立即重新创建它没有区别。记住，缩小StatefulSet只会删除Pod，但会保留PersistentVolumeClaims不变。我会让你自己尝试缩小StatefulSet并确认这种行为。
- en: The key thing to remember is that scaling down (and up) is performed gradually—similar
    to how individual pods are created when the StatefulSet is created initially.
    When scaling down by more than one instance, the pod with the highest ordinal
    number is deleted first. Only after the pod terminates completely is the pod with
    the second highest ordinal number deleted.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的关键点是缩放（缩小和扩大）是逐步进行的——类似于当StatefulSet最初创建时创建单个Pod的方式。当缩放超过一个实例时，首先删除具有最高序号的Pod。只有当Pod完全终止后，才会删除具有第二高序号的Pod。
- en: Exposing stateful pods through a regular, non-headless Service
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 通过常规的非无头Service暴露有状态的Pod
- en: Before you move on to the last part of this chapter, you’re going to add a proper,
    non-headless Service in front of your pods, because clients usually connect to
    the pods through a Service rather than connecting directly.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在您继续本章的最后部分之前，您将在Pod前面添加一个适当的、非无头服务，因为客户端通常通过服务而不是直接连接来连接到Pod。
- en: You know how to create the Service by now, but in case you don’t, the following
    listing shows the manifest.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在知道如何创建服务，但以防万一您不知道，以下列表显示了清单。
- en: 'Listing 10.7\. A regular Service for accessing the stateful pods: kubia-service-public.yaml'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.7\. 用于访问有状态Pod的常规服务：kubia-service-public.yaml
- en: '`apiVersion: v1 kind: Service metadata:   name: kubia-public spec:   selector:
        app: kubia   ports:   - port: 80     targetPort: 8080`'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Service metadata:   name: kubia-public spec:   selector:
        app: kubia   ports:   - port: 80     targetPort: 8080`'
- en: Because this isn’t an externally exposed Service (it’s a regular `ClusterIP`
    Service, not a `NodePort` or a `LoadBalancer`-type Service), you can only access
    it from inside the cluster. You’ll need a pod to access it from, right? Not necessarily.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这不是一个公开暴露的服务（它是一个常规的`ClusterIP`服务，而不是`NodePort`或`LoadBalancer`类型的服务），您只能从集群内部访问它。您需要一个Pod来访问它，对吗？不一定。
- en: Connecting to cluster-internal services through the API server
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 通过API服务器连接到集群内部服务
- en: Instead of using a piggyback pod to access the service from inside the cluster,
    you can use the same proxy feature provided by the API server to access the service
    the way you’ve accessed individual pods.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用背负式舱室从集群内部访问服务相比，您可以使用API服务器提供的相同代理功能以您访问单个Pod的方式访问服务。
- en: 'The URI path for proxy-ing requests to Services is formed like this:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 代理请求到服务的URI路径如下所示：
- en: '`/api/v1/namespaces/<namespace>/services/<service name>/proxy/<path>`'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '`/api/v1/namespaces/<namespace>/services/<service name>/proxy/<path>`'
- en: 'Therefore, you can run `curl` on your local machine and access the service
    through the `kubectl` proxy like this (you ran `kubectl proxy` earlier and it
    should still be running):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您可以在本地机器上运行`curl`并通过`kubectl`代理访问服务，如下所示（您之前已运行`kubectl proxy`，并且它应该仍在运行）：
- en: '`$ curl localhost:8001/api/v1/namespaces/default/services/kubia-`![](images/00006.jpg)`public/proxy/`
    `You''ve hit kubia-1 Data stored on this pod: No data posted yet`'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ curl localhost:8001/api/v1/namespaces/default/services/kubia-`![图片](images/00006.jpg)`public/proxy/`
    `您已访问kubia-1 数据存储在此Pod上：尚未发布数据`'
- en: Likewise, clients (inside the cluster) can use the `kubia-public` service for
    storing to and reading data from your clustered data store. Of course, each request
    lands on a random cluster node, so you’ll get the data from a random node each
    time. You’ll improve this next.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，客户端（在集群内部）可以使用`kubia-public`服务将数据存储到您的集群数据存储中，并从中读取数据。当然，每个请求都落在随机集群节点上，因此您每次都会从随机节点获取数据。您将在下一部分改进这一点。
- en: 10.4\. Discovering peers in a StatefulSet
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 10.4\. 在StatefulSet中发现同伴
- en: We still need to cover one more important thing. An important requirement of
    clustered apps is peer discovery—the ability to find other members of the cluster.
    Each member of a StatefulSet needs to easily find all the other members. Sure,
    it could do that by talking to the API server, but one of Kubernetes’ aims is
    to expose features that help keep applications completely Kubernetes-agnostic.
    Having apps talk to the Kubernetes API is therefore undesirable.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要讨论一个更重要的事情。集群应用程序的一个重要要求是节点发现——找到集群中其他成员的能力。StatefulSet的每个成员都需要轻松地找到所有其他成员。当然，它可以通过与API服务器通信来实现这一点，但Kubernetes的目标之一是提供帮助保持应用程序完全Kubernetes无关的特性。因此，应用程序与Kubernetes
    API通信是不希望的。
- en: How can a pod discover its peers without talking to the API? Is there an existing,
    well-known technology you can use that makes this possible? How about the Domain
    Name System (DNS)? Depending on how much you know about DNS, you probably understand
    what an A, CNAME, or MX record is used for. Other lesser-known types of DNS records
    also exist. One of them is the SRV record.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Pod如何在不与API通信的情况下发现其同伴？是否存在一种现有、众所周知的您可以使用的技术来实现这一点？域名系统（DNS）怎么样？根据您对DNS了解的程度，您可能理解A、CNAME或MX记录的用途。其他不太为人所知的DNS记录类型也存在。其中之一是SRV记录。
- en: Introducing SRV records
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍SRV记录
- en: SRV records are used to point to hostnames and ports of servers providing a
    specific service. Kubernetes creates SRV records to point to the hostnames of
    the pods backing a headless service.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: SRV记录用于指向提供特定服务的服务器的主机名和端口号。Kubernetes创建SRV记录以指向无头服务后端的Pod的主机名。
- en: 'You’re going to list the SRV records for your stateful pods by running the
    `dig` DNS lookup tool inside a new temporary pod. This is the command you’ll use:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 你将通过在新的临时 Pod 中运行 `dig` DNS 查询工具来列出你的有状态 Pod 的 SRV 记录。这是你将使用的命令：
- en: '`$ kubectl run -it srvlookup --image=tutum/dnsutils --rm` ![](images/00006.jpg)
    `--restart=Never -- dig SRV kubia.default.svc.cluster.local`'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl run -it srvlookup --image=tutum/dnsutils --rm` ![图片](images/00006.jpg)
    `--restart=Never -- dig SRV kubia.default.svc.cluster.local`'
- en: 'The command runs a one-off pod (`--restart=Never`) called `srvlookup`, which
    is attached to the console (`-it`) and is deleted as soon as it terminates (`--rm`).
    The pod runs a single container from the `tutum/dnsutils` image and runs the following
    command:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令运行一个一次性 Pod (`--restart=Never`)，名为 `srvlookup`，它连接到控制台 (`-it`)，并在终止后立即删除
    (`--rm`)。该 Pod 运行来自 `tutum/dnsutils` 镜像的单个容器，并执行以下命令：
- en: '`dig SRV kubia.default.svc.cluster.local`'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`dig SRV kubia.default.svc.cluster.local`'
- en: The following listing shows what the command prints out.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了该命令打印的内容。
- en: Listing 10.8\. Listing DNS SRV records of your headless Service
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.8\. 列出无头服务的 DNS SRV 记录
- en: '`... ;; ANSWER SECTION: k.d.s.c.l. 30 IN  SRV     10 33 0 kubia-0.kubia.default.svc.cluster.local.
    k.d.s.c.l. 30 IN  SRV     10 33 0 kubia-1.kubia.default.svc.cluster.local.  ;;
    ADDITIONAL SECTION: kubia-0.kubia.default.svc.cluster.local. 30 IN A 172.17.0.4
    kubia-1.kubia.default.svc.cluster.local. 30 IN A 172.17.0.6 ...`'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`... ;; ANSWER SECTION: k.d.s.c.l. 30 IN  SRV     10 33 0 kubia-0.kubia.default.svc.cluster.local.
    k.d.s.c.l. 30 IN  SRV     10 33 0 kubia-1.kubia.default.svc.cluster.local.  ;;
    ADDITIONAL SECTION: kubia-0.kubia.default.svc.cluster.local. 30 IN A 172.17.0.4
    kubia-1.kubia.default.svc.cluster.local. 30 IN A 172.17.0.6 ...`'
- en: '|  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: I’ve had to shorten the actual name to get records to fit into a single line,
    so `kubia.d.s.c.l` is actually `kubia.default.svc.cluster.local`.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我不得不缩短实际名称以便将记录放入单行，所以 `kubia.d.s.c.l` 实际上是 `kubia.default.svc.cluster.local`。
- en: '|  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The `ANSWER SECTION` shows two `SRV` records pointing to the two pods backing
    your headless service. Each pod also gets its own `A` record, as shown in `ADDITIONAL
    SECTION`.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`ANSWER SECTION` 显示了两个指向支持你的无头服务的 Pod 的 `SRV` 记录。每个 Pod 也都有自己的 `A` 记录，如 `ADDITIONAL
    SECTION` 所示。'
- en: 'For a pod to get a list of all the other pods of a StatefulSet, all you need
    to do is perform an SRV DNS lookup. In Node.js, for example, the lookup is performed
    like this:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让 Pod 获取 StatefulSet 中所有其他 Pod 的列表，你只需要执行一个 SRV DNS 查询。例如，在 Node.js 中，查询是这样执行的：
- en: '`dns.resolveSrv("kubia.default.svc.cluster.local", callBackFunction);`'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`dns.resolveSrv("kubia.default.svc.cluster.local", callBackFunction);`'
- en: You’ll use this command in your app to enable each pod to discover its peers.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在你的应用中使用这个命令来使每个Pod能够发现其 peers。
- en: '|  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The order of the returned SRV records is random, because they all have the same
    priority. Don’t expect to always see `kubia-0` listed before `kubia-1`.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的 SRV 记录的顺序是随机的，因为它们都具有相同的优先级。不要期望总是看到 `kubia-0` 在 `kubia-1` 之前列出。
- en: '|  |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 10.4.1\. Implementing peer discovery through DNS
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 10.4.1\. 通过 DNS 实现对等发现
- en: Your Stone Age data store isn’t clustered yet. Each data store node runs completely
    independently of all the others—no communication exists between them. You’ll get
    them talking to each other next.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 你的石器时代数据存储还没有集群化。每个数据存储节点完全独立于所有其他节点运行——它们之间不存在通信。你将在下一部分让它们相互交谈。
- en: Data posted by clients connecting to your data store cluster through the `kubia-public`
    Service lands on a random cluster node. The cluster can store multiple data entries,
    but clients currently have no good way to see all those entries. Because services
    forward requests to pods randomly, a client would need to perform many requests
    until it hit all the pods if it wanted to get the data from all the pods.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `kubia-public` 服务连接到你的数据存储集群的客户发布的数据会落在随机集群节点上。集群可以存储多个数据条目，但客户目前没有很好的方法来查看所有这些条目。因为服务随机转发请求到
    Pod，如果客户想要从所有 Pod 获取数据，就需要执行许多请求，直到它击中所有 Pod。
- en: You can improve this by having the node respond with data from all the cluster
    nodes. To do this, the node needs to find all its peers. You’re going to use what
    you learned about StatefulSets and SRV records to do this.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过让节点响应来自所有集群节点的数据来改进这一点。为此，节点需要找到所有它的 peers。你将使用你关于 StatefulSets 和 SRV 记录的知识来完成这个任务。
- en: You’ll modify your app’s source code as shown in the following listing (the
    full source is available in the book’s code archive; the listing shows only the
    important parts).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 你将按照以下列表修改你的应用程序的源代码（完整的源代码可在本书的代码存档中找到；列表只显示了重要的部分）。
- en: 'Listing 10.9\. Discovering peers in a sample app: kubia-pet-peers-image/app.js'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.9\. 在示例应用中查找 peers：kubia-pet-peers-image/app.js
- en: '`... const dns = require(''dns'');  const dataFile = "/var/data/kubia.txt";
    const serviceName = "kubia.default.svc.cluster.local"; const port = 8080; ...  var
    handler = function(request, response) {   if (request.method == ''POST'') {    
    ...   } else {     response.writeHead(200);     if (request.url == ''/data'')
    {       var data = fileExists(dataFile)         ? fs.readFileSync(dataFile, ''utf8'')
            : "No data posted yet";       response.end(data);     } else {       response.write("You''ve
    hit " + os.hostname() + "\n");       response.write("Data stored in the cluster:\n");
          dns.resolveSrv(serviceName, function (err, addresses) {` `1` `if (err) {
              response.end("Could not look up DNS SRV records: " + err);          
    return;         }         var numResponses = 0;         if (addresses.length ==
    0) {           response.end("No peers discovered.");         } else {          
    addresses.forEach(function (item) {` `2` `var requestOptions = {              
    host: item.name,               port: port,               path: ''/data''            
    };             httpGet(requestOptions, function (returnedData) {` `2` `numResponses++;
                  response.write("- " + item.name + ": " + returnedData);              
    response.write("\n");               if (numResponses == addresses.length) {                
    response.end();               }             });           });         }      
    });     }   } }; ...`'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`... const dns = require(''dns'');  const dataFile = "/var/data/kubia.txt";
    const serviceName = "kubia.default.svc.cluster.local"; const port = 8080; ...  var
    handler = function(request, response) {   if (request.method == ''POST'') {    
    ...   } else {     response.writeHead(200);     if (request.url == ''/data'')
    {       var data = fileExists(dataFile)         ? fs.readFileSync(dataFile, ''utf8'')
            : "No data posted yet";       response.end(data);     } else {       response.write("您已访问
    " + os.hostname() + "\n");       response.write("存储在集群中的数据：\n");       dns.resolveSrv(serviceName,
    function (err, addresses) {` `1` `if (err) {           response.end("无法查找 DNS
    SRV 记录： " + err);           return;         }         var numResponses = 0;        
    if (addresses.length == 0) {           response.end("未发现对等节点.");         } else
    {           addresses.forEach(function (item) {` `2` `var requestOptions = {              
    host: item.name,               port: port,               path: ''/data''            
    };             httpGet(requestOptions, function (returnedData) {` `2` `numResponses++;
                  response.write("- " + item.name + ": " + returnedData);              
    response.write("\n");               if (numResponses == addresses.length) {                
    response.end();               }             });           });         }      
    });     }   } }; ...`'
- en: 1 The app performs a DNS lookup to obtain SRV records.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 应用执行 DNS 查询以获取 SRV 记录。
- en: 2 Each pod pointed to by an SRV record is then contacted to get its data.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 然后，会联系由 SRV 记录指向的每个 pod 以获取其数据。
- en: '[Figure 10.12](#filepos1018219) shows what happens when a GET request is received
    by your app. The server that receives the request first performs a lookup of SRV
    records for the headless `kubia` service and then sends a GET request to each
    of the pods backing the service (even to itself, which obviously isn’t necessary,
    but I wanted to keep the code as simple as possible). It then returns a list of
    all the nodes along with the data stored on each of them.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.12](#filepos1018219) 展示了当你的应用收到一个 GET 请求时会发生什么。接收请求的服务器首先查找无头 `kubia`
    服务的 SRV 记录，然后向服务后端的每个 pod 发送 GET 请求（甚至包括它自己，显然这是不必要的，但我希望保持代码尽可能简单）。然后，它返回所有节点及其上存储的数据列表。'
- en: Figure 10.12\. The operation of your simplistic distributed data store
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12\. 你简单分布式数据存储的操作
- en: '![](images/00169.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00169.jpg)'
- en: The container image containing this new version of the app is available at docker.io/
    luksa/kubia-pet-peers.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 包含此应用新版本的容器镜像可在 docker.io/ luksa/kubia-pet-peers 上找到。
- en: 10.4.2\. Updating a StatefulSet
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 10.4.2\. 更新 StatefulSet
- en: 'Your StatefulSet is already running, so let’s see how to update its pod template
    so the pods use the new image. You’ll also set the replica count to 3 at the same
    time. To update the StatefulSet, use the `kubectl edit` command (the `patch` command
    would be another option):'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 你的 StatefulSet 已经在运行，让我们看看如何更新其 pod 模板，以便 pod 使用新的镜像。同时，你也会将副本数设置为 3。要更新 StatefulSet，请使用
    `kubectl edit` 命令（`patch` 命令也是另一种选择）：
- en: '`$ kubectl edit statefulset kubia`'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl edit statefulset kubia`'
- en: 'This opens the StatefulSet definition in your default editor. In the definition,
    change `spec.replicas` to `3` and modify the `spec.template.spec.containers.image`
    attribute so it points to the new image (`luksa/kubia-pet-peers` instead of `luksa/kubia-pet`).
    Save the file and exit the editor to update the StatefulSet. Two replicas were
    running previously, so you should now see an additional replica called `kubia-2`
    starting. List the pods to confirm:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在你的默认编辑器中打开StatefulSet定义。在定义中，将`spec.replicas`更改为`3`，并修改`spec.template.spec.containers.image`属性，使其指向新的镜像（`luksa/kubia-pet-peers`而不是`luksa/kubia-pet`）。保存文件并退出编辑器以更新StatefulSet。之前有运行了两个副本，所以你现在应该看到一个新的副本`kubia-2`正在启动。列出pod以确认：
- en: '`$ kubectl get po` `NAME      READY     STATUS              RESTARTS   AGE
    kubia-0   1/1       Running             0          25m kubia-1   1/1       Running            
    0          26m kubia-2   0/1       ContainerCreating   0          4s`'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po` `名称      就绪     状态              重启次数   年龄 kubia-0   1/1      
    运行             0          25m kubia-1   1/1       运行             0          26m
    kubia-2   0/1       容器创建中   0          4s`'
- en: 'The new pod instance is running the new image. But what about the existing
    two replicas? Judging from their age, they don’t seem to have been updated. This
    is expected, because initially, StatefulSets were more like ReplicaSets and not
    like Deployments, so they don’t perform a rollout when the template is modified.
    You need to delete the replicas manually and the StatefulSet will bring them up
    again based on the new template:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 新的pod实例正在运行新的镜像。但是现有的两个副本呢？从它们的年龄来看，它们似乎没有被更新。这是预期的，因为最初，StatefulSets更像是ReplicaSets而不是Deployments，所以当模板被修改时，它们不会执行滚动更新。你需要手动删除副本，然后StatefulSet将根据新的模板重新启动它们：
- en: '`$ kubectl delete po kubia-0 kubia-1` `pod "kubia-0" deleted pod "kubia-1"
    deleted`'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl delete po kubia-0 kubia-1` `pod "kubia-0" 已删除 pod "kubia-1" 已删除`'
- en: '|  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Starting from Kubernetes version 1.7, StatefulSets support rolling updates the
    same way Deployments and DaemonSets do. See the StatefulSet’s `spec.updateStrategy`
    field documentation using `kubectl explain` for more information.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 从Kubernetes版本1.7开始，StatefulSets支持与Deployments和DaemonSets相同的滚动更新。使用`kubectl explain`查看StatefulSet的`spec.updateStrategy`字段文档以获取更多信息。
- en: '|  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 10.4.3\. Trying out your clustered data store
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 10.4.3\. 尝试你的集群数据存储
- en: Once the two pods are up, you can see if your shiny new Stone Age data store
    works as expected. Post a few requests to the cluster, as shown in the following
    listing.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 两个pod启动后，你可以查看你闪亮的新石器时代数据存储是否按预期工作。向集群发送几个请求，如下所示。
- en: Listing 10.10\. Writing to the clustered data store through the service
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.10\. 通过服务写入集群数据存储
- en: '`$ curl -X POST -d "The sun is shining" \`![](images/00006.jpg)`localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/`
    `Data stored on pod kubia-1` `$ curl -X POST -d "The weather is sweet" \`![](images/00006.jpg)`localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/`
    `Data stored on pod kubia-0`'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ curl -X POST -d "The sun is shining" \`![](images/00006.jpg)`localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/`
    `存储在pod kubia-1上的数据` `$ curl -X POST -d "The weather is sweet" \`![](images/00006.jpg)`localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/`
    `存储在pod kubia-0上的数据`'
- en: Now, read the stored data, as shown in the following listing.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，读取存储的数据，如下所示。
- en: Listing 10.11\. Reading from the data store
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.11\. 从数据存储读取
- en: '`$ curl localhost:8001/api/v1/namespaces/default/services`![](images/00006.jpg)`/kubia-public/proxy/`
    `You''ve hit kubia-2 Data stored on each cluster node: - kubia-0.kubia.default.svc.cluster.local:
    The weather is sweet - kubia-1.kubia.default.svc.cluster.local: The sun is shining
    - kubia-2.kubia.default.svc.cluster.local: No data posted yet`'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ curl localhost:8001/api/v1/namespaces/default/services`![](images/00006.jpg)`/kubia-public/proxy/`
    `你已访问kubia-2 数据存储在每个集群节点上：- kubia-0.kubia.default.svc.cluster.local: The weather
    is sweet - kubia-1.kubia.default.svc.cluster.local: The sun is shining - kubia-2.kubia.default.svc.cluster.local:
    尚未发布数据`'
- en: Nice! When a client request reaches one of your cluster nodes, it discovers
    all its peers, gathers data from them, and sends all the data back to the client.
    Even if you scale the StatefulSet up or down, the pod servicing the client’s request
    can always find all the peers running at that time.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！当客户端请求到达你的集群节点之一时，它会发现所有对等节点，从它们那里收集数据，并将所有数据发送回客户端。即使你扩展或缩减StatefulSet，服务客户端请求的pod也总能找到当时运行的所有对等节点。
- en: The app itself isn’t that useful, but I hope you found it a fun way to show
    how instances of a replicated stateful app can discover their peers and handle
    horizontal scaling with ease.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 应用本身可能并不那么有用，但我希望你能找到一种有趣的方式来展示复制的有状态应用实例如何发现它们的对等节点，并且可以轻松地处理水平扩展。
- en: 10.5\. Understanding how StatefulSets deal with node failures
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 10.5\. 理解有状态集如何处理节点故障
- en: In [section 10.2.4](index_split_081.html#filepos975865) we stated that Kubernetes
    must be absolutely sure that a stateful pod is no longer running before creating
    its replacement. When a node fails abruptly, Kubernetes can’t know the state of
    the node or its pods. It can’t know whether the pods are no longer running, or
    if they still are and are possibly even still reachable, and it’s only the Kubelet
    that has stopped reporting the node’s state to the master.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 10.2.4 节](index_split_081.html#filepos975865) 中，我们指出 Kubernetes 必须在创建替换
    pod 之前绝对确定有状态 pod 已经不再运行。当一个节点突然失败时，Kubernetes 无法知道节点或其 pod 的状态。它无法知道 pod 是否已经不再运行，或者它们是否仍在运行，甚至可能仍然可访问，而且只有
    Kubelet 停止向主节点报告节点的状态。
- en: Because a StatefulSet guarantees that there will never be two pods running with
    the same identity and storage, when a node appears to have failed, the StatefulSet
    cannot and should not create a replacement pod until it knows for certain that
    the pod is no longer running.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有状态集保证不会有两个具有相同身份和存储的 pod 同时运行，当节点看起来已经失败时，有状态集不能也不应该在没有确定 pod 已经不再运行之前创建替换
    pod。
- en: It can only know that when the cluster administrator tells it so. To do that,
    the admin needs to either delete the pod or delete the whole node (doing so then
    deletes all the pods scheduled to the node).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 它只能知道这一点，当集群管理员告诉它时。为此，管理员需要删除 pod 或删除整个节点（这样做会删除所有调度到该节点的 pod）。
- en: As your final exercise in this chapter, you’ll look at what happens to StatefulSets
    and their pods when one of the cluster nodes gets disconnected from the network.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章的最后一个练习，你将查看当集群中的一个节点从网络断开连接时，有状态集及其 pod 会发生什么。
- en: 10.5.1\. Simulating a node’s disconnection from the network
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 10.5.1\. 模拟节点从网络断开
- en: As in [chapter 4](index_split_038.html#filepos358794), you’ll simulate the node
    disconnecting from the network by shutting down the node’s `eth0` network interface.
    Because this example requires multiple nodes, you can’t run it on Minikube. You’ll
    use Google Kubernetes Engine instead.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [第 4 章](index_split_038.html#filepos358794) 中所述，你将通过关闭节点的 `eth0` 网络接口来模拟节点从网络断开。由于此示例需要多个节点，你无法在
    Minikube 上运行它。你将使用 Google Kubernetes Engine。
- en: Shutting down the node’s network adapter
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 关闭节点的网络适配器
- en: 'To shut down a node’s `eth0` interface, you need to `ssh` into one of the nodes
    like this:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 要关闭节点的 `eth0` 接口，你需要像这样通过 `ssh` 登录到节点之一：
- en: '`$ gcloud compute ssh gke-kubia-default-pool-32a2cac8-m0g1`'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ gcloud compute ssh gke-kubia-default-pool-32a2cac8-m0g1`'
- en: 'Then, inside the node, run the following command:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在节点内部运行以下命令：
- en: '`$ sudo ifconfig eth0 down`'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ sudo ifconfig eth0 down`'
- en: Your `ssh` session will stop working, so you’ll need to open another terminal
    to continue.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 你的 `ssh` 会话将停止工作，因此你需要打开另一个终端以继续。
- en: Checking the node’s status as seen by the Kubernetes master
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 检查 Kubernetes 主节点看到的节点状态
- en: With the node’s network interface down, the Kubelet running on the node can
    no longer contact the Kubernetes API server and let it know that the node and
    all its pods are still running.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 当节点的网络接口关闭时，运行在节点上的 Kubelet 将无法联系 Kubernetes API 服务器并通知它节点及其所有 pod 仍在运行。
- en: After a while, the control plane will mark the node as `NotReady`. You can see
    this when listing nodes, as the following listing shows.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 一段时间后，控制平面会将节点标记为 `NotReady`。你可以通过列出节点时看到这一点，如下所示。
- en: Listing 10.12\. Observing a failed node’s status change to `NotReady`
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.12\. 观察失败节点的状态变为 `NotReady`
- en: '`$ kubectl get node` `NAME                                   STATUS     AGE      
    VERSION gke-kubia-default-pool-32a2cac8-596v   Ready      16m       v1.6.2 gke-kubia-default-pool-32a2cac8-m0g1`
    `NotReady``   16m       v1.6.2 gke-kubia-default-pool-32a2cac8-sgl7   Ready     
    16m       v1.6.2`'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get node` `NAME                                   STATUS     AGE      
    VERSION gke-kubia-default-pool-32a2cac8-596v   Ready      16m       v1.6.2 gke-kubia-default-pool-32a2cac8-m0g1`
    `NotReady``   16m       v1.6.2 gke-kubia-default-pool-32a2cac8-sgl7   Ready     
    16m       v1.6.2`'
- en: Because the control plane is no longer getting status updates from the node,
    the status of all pods on that node is `Unknown`. This is shown in the pod list
    in the following listing.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 由于控制平面不再从节点接收状态更新，该节点上所有 pod 的状态都是 `Unknown`。这在下述 pod 列表中显示。
- en: Listing 10.13\. Observing the pod’s status change after its node becomes `NotReady`
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.13\. 观察节点变为 `NotReady` 状态后 pod 的状态变化
- en: '`$ kubectl get po` `NAME      READY     STATUS    RESTARTS   AGE kubia-0  
    1/1       Unknown   0          15m kubia-1   1/1       Running   0          14m
    kubia-2   1/1       Running   0          13m`'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po` `NAME      READY     STATUS    RESTARTS   AGE kubia-0  
    1/1       Unknown   0          15m kubia-1   1/1       Running   0          14m
    kubia-2   1/1       Running   0          13m`'
- en: As you can see, the `kubia-0` pod’s status is no longer known because the pod
    was (and still is) running on the node whose network interface you shut down.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`kubia-0` Pod的状态不再已知，因为Pod（仍然）运行在您关闭了网络接口的节点上。
- en: Understanding what happens to pods whose status is unknown
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 理解状态未知的Pod会发生什么
- en: If the node were to come back online and report its and its pod statuses again,
    the pod would again be marked as `Running`. But if the pod’s status remains unknown
    for more than a few minutes (this time is configurable), the pod is automatically
    evicted from the node. This is done by the master (the Kubernetes control plane).
    It evicts the pod by deleting the pod resource.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 如果节点重新上线并再次报告其及其Pod的状态，Pod将被再次标记为`Running`。但如果Pod的状态未知超过几分钟（这个时间是可以配置的），Pod将被自动从节点中移除。这是由主节点（Kubernetes控制平面）完成的。它通过删除Pod资源来移除Pod。
- en: When the Kubelet sees that the pod has been marked for deletion, it starts terminating
    the pod. In your case, the Kubelet can no longer reach the master (because you
    disconnected the node from the network), which means the pod will keep running.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 当Kubelet看到Pod被标记为删除时，它开始终止Pod。在这种情况下，Kubelet无法再连接到主节点（因为您已将节点从网络中断开），这意味着Pod将继续运行。
- en: Let’s examine the current situation. Use `kubectl describe` to display details
    about the `kubia-0` pod, as shown in the following listing.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查当前的情况。使用`kubectl describe`来显示`kubia-0` Pod的详细信息，如下所示。
- en: Listing 10.14\. Displaying details of the pod with the unknown status
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.14\. 显示状态未知的Pod的详细信息
- en: '`$ kubectl describe po kubia-0` `Name:        kubia-0 Namespace:   default
    Node:        gke-kubia-default-pool-32a2cac8-m0g1/10.132.0.2 ... Status:     
    Terminating (expires Tue, 23 May 2017 15:06:09 +0200) Reason:      NodeLost Message:    
    Node gke-kubia-default-pool-32a2cac8-m0g1 which was              running pod kubia-0
    is unresponsive`'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl describe po kubia-0` `Name:        kubia-0 Namespace:   default
    Node:        gke-kubia-default-pool-32a2cac8-m0g1/10.132.0.2 ... Status:     
    Terminating (expires Tue, 23 May 2017 15:06:09 +0200) Reason:      NodeLost Message:    
    Node gke-kubia-default-pool-32a2cac8-m0g1 which was running pod kubia-0 is unresponsive`'
- en: The pod is shown as `Terminating`, with `NodeLost` listed as the reason for
    the termination. The message says the node is considered lost because it’s unresponsive.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: Pod显示为`Terminating`，终止原因列为`NodeLost`。信息表明节点被认为已丢失，因为它无响应。
- en: '|  |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: What’s shown here is the control plane’s view of the world. In reality, the
    pod’s container is still running perfectly fine. It isn’t terminating at all.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这里显示的是控制平面的世界视图。实际上，Pod的容器仍在正常运行，根本就没有终止。
- en: '|  |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 10.5.2\. Deleting the pod manually
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 10.5.2\. 手动删除Pod
- en: You know the node isn’t coming back, but you need all three pods running to
    handle clients properly. You need to get the `kubia-0` pod rescheduled to a healthy
    node. As mentioned earlier, you need to delete the node or the pod manually.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 您知道节点不会回来，但您需要所有三个Pod运行以正确处理客户端。您需要将`kubia-0` Pod重新调度到健康的节点。如前所述，您需要手动删除节点或Pod。
- en: Deleting the pod in the usual way
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 以常规方式删除Pod
- en: 'Delete the pod the way you’ve always deleted pods:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 按照您通常删除Pod的方式删除Pod：
- en: '`$ kubectl delete po kubia-0` `pod "kubia-0" deleted`'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl delete po kubia-0` `pod "kubia-0" deleted`'
- en: 'All done, right? By deleting the pod, the StatefulSet should immediately create
    a replacement pod, which will get scheduled to one of the remaining nodes. List
    the pods again to confirm:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了，对吧？通过删除Pod，StatefulSet应该立即创建一个替换Pod，该Pod将被调度到剩余的节点之一。再次列出Pod以确认：
- en: '`$ kubectl get po` `NAME      READY     STATUS    RESTARTS   AGE kubia-0  
    1/1       Unknown   0          15m kubia-1   1/1       Running   0          14m
    kubia-2   1/1       Running   0          13m`'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po` `NAME      READY     STATUS    RESTARTS   AGE kubia-0  
    1/1       Unknown   0          15m kubia-1   1/1       Running   0          14m
    kubia-2   1/1       Running   0          13m`'
- en: That’s strange. You deleted the pod a moment ago and `kubectl` said it had deleted
    it. Why is the same pod still there?
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这很奇怪。您刚才删除了Pod，`kubectl`说它已经删除了。为什么同一个Pod还在那里？
- en: '|  |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The `kubia-0` pod in the listing isn’t a new pod with the same name—this is
    clear by looking at the `AGE` column. If it were new, its age would be merely
    a few seconds.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中的 `kubia-0` Pod 并不是一个具有相同名称的新 Pod——通过查看 `AGE` 列可以清楚地看出这一点。如果是新的，它的年龄将仅仅是几秒钟。
- en: '|  |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Understanding why the pod isn’t deleted
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 理解为什么 Pod 没有被删除
- en: The pod was marked for deletion even before you deleted it. That’s because the
    control plane itself already deleted it (in order to evict it from the node).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你在删除之前，Pod 已经被标记为删除。这是因为控制平面本身已经将其删除（为了将其从节点中驱逐出去）。
- en: If you look at [listing 10.14](#filepos1029071) again, you’ll see that the pod’s
    status is `Terminating`. The pod was already marked for deletion earlier and will
    be removed as soon as the Kubelet on its node notifies the API server that the
    pod’s containers have terminated. Because the node’s network is down, this will
    never happen.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如果再次查看[列表 10.14](#filepos1029071)，你会看到 Pod 的状态是 `Terminating`。Pod 之前已经被标记为删除，并且一旦其节点上的
    Kubelet 通知 API 服务器 Pod 的容器已经终止，它就会被移除。因为节点的网络已断开，所以这种情况永远不会发生。
- en: Forcibly deleting the pod
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 强制删除 Pod
- en: 'The only thing you can do is tell the API server to delete the pod without
    waiting for the Kubelet to confirm that the pod is no longer running. You do that
    like this:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 你唯一能做的就是告诉 API 服务器删除 Pod，而无需等待 Kubelet 确认 Pod 已不再运行。你可以这样做：
- en: '`$ kubectl delete po kubia-0 --force --grace-period 0` `warning: Immediate
    deletion does not wait for confirmation that the running      resource has been
    terminated. The resource may continue to run on the      cluster indefinitely.
    pod "kubia-0" deleted`'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl delete po kubia-0 --force --grace-period 0` `警告：立即删除不会等待确认正在运行的资源已被终止。资源可能会在集群中无限期地继续运行。pod
    "kubia-0" 已删除`'
- en: 'You need to use both the `--force` and `--grace-period 0` options. The warning
    displayed by `kubectl` notifies you of what you did. If you list the pods again,
    you’ll finally see a new `kubia-0` pod created:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要同时使用 `--force` 和 `--grace-period 0` 选项。`kubectl` 显示的警告通知你做了什么。如果你再次列出 Pod，你最终会看到一个新的
    `kubia-0` Pod 被创建：
- en: '`$ kubectl get po` `NAME          READY     STATUS              RESTARTS  
    AGE kubia-0       0/1       ContainerCreating   0          8s kubia-1       1/1      
    Running             0          20m kubia-2       1/1       Running            
    0          19m`'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po` `NAME          READY     STATUS              RESTARTS  
    AGE kubia-0       0/1       ContainerCreating   0          8s kubia-1       1/1      
    Running             0          20m kubia-2       1/1       Running            
    0          19m`'
- en: '|  |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Warning
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Don’t delete stateful pods forcibly unless you know the node is no longer running
    or is unreachable (and will remain so forever).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你知道节点不再运行或无法访问（并且将永远如此），否则不要强制删除有状态 Pod。
- en: '|  |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Before continuing, you may want to bring the node you disconnected back online.
    You can do that by restarting the node through the GCE web console or in a terminal
    by issuing the following command:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，你可能想将你断开连接的节点重新上线。你可以通过 GCE 网络控制台或通过终端执行以下命令来实现：
- en: '`$ gcloud compute instances reset <node name>`'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ gcloud compute instances reset <节点名称>`'
- en: 10.6\. Summary
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 10.6. 摘要
- en: This concludes the chapter on using StatefulSets to deploy stateful apps. This
    chapter has shown you how to
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了关于使用 StatefulSets 部署有状态应用程序的章节。本章向你展示了如何
- en: Give replicated pods individual storage
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为复制的 Pod 提供单独的存储
- en: Provide a stable identity to a pod
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 Pod 提供稳定的标识
- en: Create a StatefulSet and a corresponding headless governing Service
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个 StatefulSet 和相应的无头管理服务
- en: Scale and update a StatefulSet
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展和更新 StatefulSet
- en: Discover other members of the StatefulSet through DNS
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 DNS 发现 StatefulSet 的其他成员
- en: Connect to other members through their host names
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过主机名连接到其他成员
- en: Forcibly delete stateful pods
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强制删除有状态 Pod
- en: Now that you know the major building blocks you can use to have Kubernetes run
    and manage your apps, we can look more closely at how it does that. In the next
    chapter, you’ll learn about the individual components that control the Kubernetes
    cluster and keep your apps running.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了可以用来让 Kubernetes 运行和管理你的应用程序的主要构建块，我们可以更详细地了解它是如何做到这一点的。在下一章中，你将学习控制
    Kubernetes 集群并保持你的应用程序运行的单个组件。
