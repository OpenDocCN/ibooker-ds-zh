- en: Chapter 9\. Fairness in the Age of Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 机器学习算法时代的公平性
- en: Anna Jacobson
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安娜·雅各布森
- en: '![](Images/Anna_Jacobson.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/Anna_Jacobson.png)'
- en: Candidate for Masters in Data Science, UC Berkeley
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 加州大学伯克利分校数据科学硕士候选人
- en: Of all the exciting work taking place in the field of data science, the machine
    learning algorithm (MLA) is one of the advancements that has garnered the most
    popular attention—and to many, it is the area of data science that holds the most
    promise for the future. However, as with all powerful technologies, MLAs also
    carry the risk of becoming destructive forces in the world.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学领域进行的所有激动人心的工作中，机器学习算法（MLA）是吸引最广泛关注的进步之一，对许多人来说，它是数据科学领域未来最有前途的领域。然而，就像所有强大的技术一样，机器学习算法也存在成为世界破坏性力量的风险。
- en: Early applications of MLAs included email spam filtering, image recognition,
    and recommender systems for entertainment. In these low-stakes settings, the cost
    of any errors is low, usually a minor inconvenience at worst. However, the cost
    of errors in MLAs has dramatically increased as they have begun to be applied
    to human beings, such as in predictive policing. Despite the apparently objective
    process of training MLAs, it sometimes results in algorithms that, while computationally
    correct, produce outputs that are biased and unjust from a human perspective.
    And in high-stakes settings, MLAs that produce unfair results can do enormous
    damage.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 早期机器学习算法的应用包括电子邮件垃圾邮件过滤、图像识别以及娱乐推荐系统。在这些低风险的环境中，任何错误的代价都很低，通常最多只是轻微的不便。然而，随着它们开始应用于人类，如在预测性警务中，机器学习算法的错误成本显著增加。尽管机器学习算法训练过程表面上看起来是客观的，但有时会导致从人类角度看来偏见和不公正的算法输出。在高风险环境中，产生不公平结果的机器学习算法可能会造成巨大的伤害。
- en: Fairness is an elusive concept. In the practice of machine learning, the quality
    of an algorithm is judged based on its accuracy (the percentage of correct results),
    its precision (the ability not to label as positive a sample that is negative),
    or its recall (the ability to find all the positive samples). Deciding which of
    these three measures is the best proxy for fairness is not always straightforward,
    and improvements in one metric can cause decreases in others.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的实践中，算法的质量是根据其准确率（正确结果的百分比）、精确度（不将负样本误标为正的能力）或召回率（找到所有正样本的能力）来判断的，公平性是一个难以捉摸的概念。决定这三种度量中哪一种最能代表公平性并不总是直截了当的，而且在一个度量指标上的改进可能会导致其他指标的下降。
- en: Fundamentally, MLAs are only as fair as the data itself. If the underlying data
    is biased in any way, its structural inequalities may not only be replicated but
    may even be amplified in the algorithm. ML engineers must be aware of their own
    blind spots; all the small decisions they make about their training data can be
    as impactful as their engineering techniques. Even more problematic, however,
    is that societal problems such as discrimination and exclusion are deeply rooted
    in the world around us—and consequently, they are inherent in the data that we
    extract from the world.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上说，机器学习算法的公平性取决于数据本身。如果底层数据在任何方面都存在偏见，那么这些结构性的不平等可能不仅会被复制，甚至可能在算法中被放大。机器学习工程师必须意识到自己的盲点；他们对训练数据所做的所有小决定可能对其工程技术一样有重大影响。然而，更为问题的是，社会问题如歧视和排斥深深扎根于我们周围的世界之中，因此它们也存在于我们从世界中提取的数据之中。
- en: 'Achieving algorithmic fairness, it seems, is as difficult as achieving fairness
    in human-led systems. Human systems are biased in all of the ways that algorithmic
    systems are biased—and humans are additionally biased in ways that machines cannot
    be. However, algorithmic systems can be both less visible and less transparent:
    often, people are unaware that an algorithm is being used to make a decision that
    affects them—and even if they are aware, the algorithm is presented as a complex,
    unknowable “black box” that is impossible to see, much less understand.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 实现算法公平性似乎与实现人类主导系统的公平性一样困难。人类系统在所有算法系统存在偏见的方式中都存在偏见，而人类还以机器无法做到的方式存在偏见。然而，算法系统可能既不那么显眼也不那么透明：通常，人们不知道正在使用算法来做出影响他们的决定—即使他们意识到了，算法也呈现为一个复杂、不可知的“黑箱”，根本无法看到，更别说理解了。
- en: 'Three clear steps must be taken to improve algorithmic fairness:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高算法的公平性，必须采取三个明确的步骤：
- en: First, we must do better to ensure the quality of the data being used to train
    the algorithms. For instance, all subjects should have an equal chance of being
    represented in the data, which means that additional effort may be required to
    obtain data from traditionally underrepresented groups. Models also must be retrained
    periodically with new data to start to root out historical biases, despite the
    added expense that this incurs.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须做得更好，以确保用于训练算法的数据质量。例如，所有受试者应有平等的机会在数据中被代表，这意味着可能需要额外的努力来获取传统上未被充分代表的群体的数据。此外，模型还必须定期用新数据重新训练，以开始消除历史偏见，尽管这会增加额外的费用。
- en: Second, within the field of machine learning, processes must be standardized
    across the industry to eradicate as much bias as possible from the engineering
    process. This should include a variety of approaches, including unconscious bias
    training for engineers similar to the training that intelligence analysts routinely
    undergo; engineering protocols akin to the protocols of scientific research, such
    as rigorous peer review; and independent post-implementation auditing of algorithmic
    fairness in which the quality of an algorithm is judged not only by standard engineering
    metrics but also by how it impacts the most vulnerable people affected by it.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其次，在机器学习领域内，必须在整个行业标准化流程，以尽可能消除工程过程中的偏见。这应包括一系列方法，包括为工程师提供类似情报分析员定期接受的无意识偏见培训；类似于科学研究严格同行评审的工程协议；以及独立的后实施审计，评估算法公平性的质量，评估算法的标准不仅限于标准工程指标，还包括其对受其影响最脆弱的人群的影响。
- en: Third, MLAs must be brought into the light in our society, so that we are all
    aware of when they are being used in ways that impact our lives; a well-informed
    citizenry is essential to holding the groups that create and use these algorithms
    accountable to ensure their fairness. We are constitutionally guaranteed the right
    to due process and the right to equal protection; we should interpret these rights
    to include the right to know what data about ourselves is being used as input
    and the right to access any output that is generated about ourselves when MLAs
    are used in constitutionally protected contexts.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三，MLA必须在我们社会中被揭示，以便我们都知道它们在影响我们生活的方式上的使用；一个信息充足的公民群体对于保持创建和使用这些算法的团体的责任性至关重要，以确保它们的公平性。我们在宪法上有权获得正当程序和平等保护；我们应将这些权利解释为包括了解用于输入的数据以及在使用MLA时生成的任何输出的权利。
- en: Taking these steps will require profound changes throughout our society, by
    many stakeholders and across many domains. In a world governed by laws and conventions
    that never envisioned the power of the MLA, the responsibility to strive for fairness
    in machine learning systems belongs to everyone who works in or with them. As
    MLAs become more prevalent in our society, it will become increasingly critical
    that the humans in the loop address this issue to ensure that this technology
    fulfills its promise to do good rather than its potential to do harm.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 采取这些步骤将需要社会各界的深刻变革，涵盖多个利益相关者和多个领域。在一个法律和传统从未预见到MLA力量的世界中，确保机器学习系统公平性的责任属于所有在其中工作或与之合作的人。随着MLA在我们社会中的普及，人类在处理这一问题上的角色变得愈发关键，以确保这项技术兑现其造福社会的承诺，而不是潜在的危害。
- en: References
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'Vyacheslav Polonski, “Mitigating Algorithmic Bias in Predictive Justice: 4
    Design Principles for AI Fairness,” Towards Data Science, November 23, 2018, [*https://oreil.ly/TIKHr*](https://oreil.ly/TIKHr).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[维亚切斯拉夫·波隆斯基](https://oreil.ly/TIKHr)，“缓解预测正义中的算法偏见：AI公平性的四个设计原则”，Towards Data
    Science，2018年11月23日，[*https://oreil.ly/TIKHr*](https://oreil.ly/TIKHr)。'
- en: Gal Yona, “A Gentle Introduction to the Discussion on Algorithmic Fairness,”
    Towards Data Science, October 5, 2017, [*https://oreil.ly/NbVOD*](https://oreil.ly/NbVOD).
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[加尔·约纳](https://oreil.ly/NbVOD)，“算法公平性讨论的初步介绍”，Towards Data Science，2017年10月5日，[*https://oreil.ly/NbVOD*](https://oreil.ly/NbVOD)。'
- en: Moritz Hardt, “How Big Data Is Unfair,” Medium, September 26, 2014, [*https://oreil.ly/kNrtx*](https://oreil.ly/kNrtx).
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[莫里茨·哈特](https://oreil.ly/kNrtx)，“大数据的不公平性”，Medium，2014年9月26日，[*https://oreil.ly/kNrtx*](https://oreil.ly/kNrtx)。'
- en: Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner, “Machine Bias,”
    ProPublica, May 23, 2016, [*https://oreil.ly/b41AW*](https://oreil.ly/b41AW).
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[朱莉娅·安格温](https://oreil.ly/b41AW)，[杰夫·拉尔森](https://oreil.ly/b41AW)，[苏里亚·马图](https://oreil.ly/b41AW)和[劳伦·柯克纳](https://oreil.ly/b41AW)，“机器偏见”，ProPublica，2016年5月23日，[*https://oreil.ly/b41AW*](https://oreil.ly/b41AW)。'
- en: Hugo-Bowne Anderson, “Weapons of Math Destruction (with Cathy O’Neil),” November
    26, 2018, in *DataFramed*, podcast, 55:53, [*https://oreil.ly/5ScpO*](https://oreil.ly/5ScpO).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugo-Bowne Anderson，“数学毁灭的武器（与凯西·奥尼尔）”，2018年11月26日，在*DataFramed*，播客，55:53，[*https://oreil.ly/5ScpO*](https://oreil.ly/5ScpO)。
- en: 'Kate Crawford and Jason Schultz, “Big Data and Due Process: Toward a Framework
    to Redress Predictive Privacy Harms,” *Boston College Law Review* 55, no. 1 (January
    29, 2014), [*https://oreil.ly/X_W8h*](https://oreil.ly/X_W8h).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kate Crawford和Jason Schultz，“大数据与正当程序：走向补偿预测性隐私伤害的框架”，*波士顿学院法律评论*，第55卷，第1期（2014年1月29日），[*https://oreil.ly/X_W8h*](https://oreil.ly/X_W8h)。
- en: “Statement of Concern About Predictive Policing by ACLU and 16 Civil Rights
    Privacy, Racial Justice, and Technology Organizations,” American Civil Liberties
    Union, August 31, 2016, [*https://oreil.ly/_hZHO*](https://oreil.ly/_hZHO).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “关于预测性警务的关注声明由ACLU和16个民权、隐私、种族正义和技术组织共同发表”，美国公民自由联盟，2016年8月31日，[*https://oreil.ly/_hZHO*](https://oreil.ly/_hZHO)。
- en: Sam Corbett-Davies, Emma Pierson, Avi Feller, and Sharad Goel, “A Computer Program
    Used for Bail and Sentencing Decisions Was Labeled Biased Against Blacks. It’s
    Actually Not That Clear,” *Washington Post*, October 17, 2016, [*https://oreil.ly/cQMJz*](https://oreil.ly/cQMJz).
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sam Corbett-Davies, Emma Pierson, Avi Feller, and Sharad Goel，“一个计算机程序用于保释和判决决策，被标记为对黑人有偏见。事实上并不那么清楚”，*华盛顿邮报*，2016年10月17日，[*https://oreil.ly/cQMJz*](https://oreil.ly/cQMJz)。
- en: Mark Puente, “LAPD to Scrap Some Crime Data Programs After Criticism,” *Los
    Angeles Times*, April 5, 2019, [*https://oreil.ly/JI7NA*](https://oreil.ly/JI7NA).
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mark Puente，“洛杉矶时报在批评后放弃一些犯罪数据程序”，*洛杉矶时报*，2019年4月5日，[*https://oreil.ly/JI7NA*](https://oreil.ly/JI7NA)。
