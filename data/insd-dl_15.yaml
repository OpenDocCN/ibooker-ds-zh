- en: 13 Transfer learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13 迁移学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Transferring a pretrained network to a new problem
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将预训练网络转移到新的问题上
- en: Understanding the difference between frozen and warm weights
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解冻结权重和热权重之间的区别
- en: Learning with less data via transfer learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过迁移学习用更少的数据进行学习
- en: Transfer learning for text problems with transformer-based models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于transformer的模型进行文本问题的迁移学习
- en: You now know a range of techniques for training models from scratch on new data.
    But what if you do not have time to wait for a big model to train? Or what if
    you do not have a lot of data to begin with? Ideally, we could use information
    from a bigger, well-curated dataset to help us learn a more accurate model in
    fewer epochs for our new, smaller dataset.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经了解了一系列从头开始在新数据上训练模型的技术。但是，如果你没有时间等待大模型训练怎么办？或者，如果你一开始就没有很多数据怎么办？理想情况下，我们可以使用来自更大、精心整理的数据集的信息来帮助我们更快地在更少的迭代中学习一个更准确的模型，针对我们新的、较小的数据集。
- en: That is where *transfer learning* comes into play. The idea behind transfer
    learning is that if someone has gone through the effort of training a big model
    on a bunch of data, you can probably use that *already trained* model as a starting
    point for your problem. In essence, you want to *transfer* to your problem all
    the information that the model has extracted from some related problem. When that’s
    possible, transfer learning can save you weeks of time, improve your accuracy,
    and just generally work better. This is especially valuable because you can get
    better results with less labeled data, which is a big time- and moneysaver. This
    makes transfer learning one of the most practical tools you should know for on-the-job
    work.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 正是在这里，*迁移学习*发挥了作用。迁移学习背后的理念是，如果有人已经花费了精力在大量数据上训练了一个大模型，那么你很可能可以使用这个*已经训练好*的模型作为你问题的起点。本质上，你希望将模型从相关问题中提取的所有信息*迁移*到你的问题上。当这成为可能时，迁移学习可以为你节省数周的时间，提高准确性，并且总体上工作得更好。这尤其有价值，因为你可以用更少的标记数据获得更好的结果，这可以节省大量的时间和金钱。这使得迁移学习成为你应该知道的、在实际工作中最实用的工具之一。
- en: 'Transfer learning works best when there are intrinsic similarities between
    the original larger set of data and the smaller target data you want to apply
    it to. This is particularly true for CNNs because of how intrinsically similar
    images are. Even if you have a source dataset of landscape photographs and a target
    dataset of cats and dogs, the structural priors we talked about in chapter 3 are
    still true: pixels near each other are related to each other, and far-off pixels
    have little bearing.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当原始较大的数据集和你要应用迁移学习的小型目标数据之间存在内在相似性时，迁移学习效果最佳。这对于CNN尤其如此，因为图像在本质上是非常相似的。即使你有一个风景照片的源数据集和一个包含猫和狗的目标数据集，我们在第3章中讨论的结构先验仍然成立：相邻的像素彼此相关，而远离像素的影响很小。
- en: This chapter focuses on one particular type of transfer learning that reuses
    the literal weights and architecture from a previously trained network for a new
    problem. After we show how this works with CNNs for images, we see how to do transfer
    learning for text classification models using transformer-based models. Just a
    few years ago, transfer learning for text was not nearly as easy or successful.
    But transfer learning allows us to sidestep the enormous training costs of transformers,
    getting their benefits at little cost.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章专注于一种特定的迁移学习类型，它重新使用了先前训练网络的实际权重和架构来解决新问题。在我们展示了如何使用CNN进行图像迁移学习之后，我们将看到如何使用基于transformer的模型进行文本分类模型的迁移学习。就在几年前，文本迁移学习还远没有这么容易或成功。但迁移学习允许我们绕过transformer巨大的训练成本，以极小的代价获得其好处。
- en: 13.1 Transferring model parameters
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.1 转移模型参数
- en: 'The key to success in any new machine learning application is access to representative
    data that has been labeled accurately. But getting lots of labeled data requires
    time, effort, and money. Entire companies exist just to help people label their
    data using services like Amazon’s Mechanical Turk. At the same time, we want evidence
    that our approach will work before we invest a large amount of time collecting
    and labeling a huge corpus. This puts us in a conundrum: we want to build a good
    initial model to see if a task is viable, but getting enough data to build a good
    first model is expensive.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何新的机器学习应用中取得成功的关键是能够访问到经过准确标记的代表性数据。但是，获取大量标记数据需要时间、精力和金钱。整个公司都存在只是为了帮助人们使用像亚马逊的
    Mechanical Turk 这样的服务来标记他们的数据。同时，我们在投入大量时间收集和标记大量语料库之前，希望有证据表明我们的方法将有效。这使我们陷入了两难：我们希望构建一个良好的初始模型来查看任务是否可行，但获取足够的数据来构建一个良好的初始模型是昂贵的。
- en: We want to make an accurate model with less data and compute time by using *related*
    data to help us build the model. Essentially, we want to *transfer* things we
    have *learned* about one domain to a different, but related, domain. This is the
    idea behind *transfer learning*. Practically, transfer learning is one of the
    most useful approaches you should know about in your arsenal of deep learning
    tools. Especially if you are doing any work in a computer vision- or text-based
    application area, transfer learning can be very powerful.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望通过使用 *相关* 数据来帮助我们构建模型，以更少的数据和计算时间来制作一个准确的模型。本质上，我们希望将我们在一个领域 *学习* 到的东西转移到另一个但相关的领域。这就是
    *迁移学习* 的理念。实际上，迁移学习是你在深度学习工具库中应该知道的最有用的方法之一。特别是如果你在计算机视觉或基于文本的应用领域做任何工作，迁移学习可以非常强大。
- en: One of the most successful approaches to transfer learning that we learn in
    this chapter is to transfer the *weights* θ from one model to another. The original
    model is trained on a large dataset of high-quality data, which shares some structural
    similarities with the smaller set of data that we really care about. For example,
    images have a lot of structural similarities—so we could take a model trained
    on almost any large image-classification task and use it to help us with a more
    nuanced task. To do this, we need to make the smallest possible modification or
    addition to the original model f to make it fit the new problem. This high-level
    approach is depicted in figure 13.1; we see in a moment how to perform the mechanical
    details.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中我们学习到的最成功的迁移学习方法之一是将一个模型的 *权重* θ 转移到另一个模型。原始模型是在一个包含高质量数据的大型数据集上训练的，这个数据集与我们真正关心的较小数据集有一些结构上的相似性。例如，图像有很多结构上的相似性——因此我们可以使用在几乎任何大型图像分类任务上训练的模型来帮助我们完成更细致的任务。为了做到这一点，我们需要对原始模型
    f 进行最小的修改或添加，使其适应新的问题。这种高级方法在图 13.1 中有所描述；我们很快就会看到如何执行这些机械细节。
- en: '![](../Images/CH13_F01_Raff.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.1](../Images/CH13_F01_Raff.png)'
- en: Figure 13.1 Training a big model on a big dataset. It is a one-time cost, because
    we can reuse the big model by transferring it to many different smaller tasks.
    This is done by editing the model but keeping almost all of the original architecture
    and weights θ intact. Then we train the modified model on the new dataset.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1 在大型数据集上训练大模型。这是一个一次性成本，因为我们可以通过将其转移到许多不同的较小任务中来重复使用大模型。这是通过编辑模型但几乎保持所有原始架构和权重
    θ 完整来实现的。然后我们在新的数据集上训练修改后的模型。
- en: 13.1.1  Preparing an image dataset
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.1 准备图像数据集
- en: 'To get started with transfer learning, we will download a Cats-vs-Dogs dataset
    that was organized as part of a Microsoft Kaggle competition ([https://www.kaggle.com/shaunthesheep/microsoft-catsvsdogs-dataset](https://www.kaggle.com/shaunthesheep/microsoft-catsvsdogs-dataset)),
    so it will be a binary classification problem. This is our first time creating
    a new image-classification dataset in PyTorch, so we walk through the steps as
    a useful exercise. The following code snippet downloads the zip file containing
    the dataset and extracts it to a folder called PetImages. Note that two of the
    files in this zip file are unfortunately corrupted. We need to delete these two
    files for our data loader to work properly, which is why we have a `bad_files`
    list indicating the corrupted images and removing them. Here’s the code:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始迁移学习，我们将下载一个作为微软 Kaggle 竞赛一部分组织的 Cats-vs-Dogs 数据集（[https://www.kaggle.com/shaunthesheep/microsoft-catsvsdogs-dataset](https://www.kaggle.com/shaunthesheep/microsoft-catsvsdogs-dataset)），因此它将是一个二元分类问题。这是我们第一次在
    PyTorch 中创建新的图像分类数据集，因此我们将作为有用的练习逐步说明这些步骤。以下代码片段下载包含数据集的 zip 文件并将其提取到名为 PetImages
    的文件夹中。请注意，这个 zip 文件中的两个文件不幸损坏。我们需要删除这两个文件，以便数据加载器正常工作，这就是为什么我们有一个 `bad_files`
    列表来指示损坏的图像并删除它们。以下是代码：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Download this dataset if we have not already done so!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果我们还没有这样做，请下载此数据集！
- en: ❷ This file is bad and will screw up the data loader!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 此文件有问题，将会搞乱数据加载器！
- en: 'Some of the images have corrupted EXIF data as well. The EXIF data is metadata
    about the image (such as where the photo was taken) and is not important for what
    we want to do. For that reason, we disable any warnings about this issue:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一些图像也有损坏的 EXIF 数据。EXIF 数据是关于图像的元数据（例如照片拍摄地点），对于我们想要做的事情并不重要。因此，我们禁用了关于此问题的任何警告：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Don’t bother us about these bad files, thank you.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 请不要打扰我们处理这些坏文件，谢谢。
- en: Now we use the `ImageFolder` class provided by PyTorch to create a `Dataset`
    for this class. The `ImageFolder` expects a root directory with one child folder
    for each class. The name of the folder is the name of the class, and every image
    in that folder is loaded as an example for the dataset with that specific class
    label; see figure 13.2.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用 PyTorch 提供的 `ImageFolder` 类为这个类别创建一个 `Dataset`。`ImageFolder` 期望一个根目录，每个类别有一个子文件夹。文件夹的名称是类别的名称，该文件夹中的每个图像都作为具有特定类别标签的数据集的示例加载；参见图
    13.2。
- en: '![](../Images/CH13_F02_Raff.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH13_F02_Raff.png)'
- en: Figure 13.2 The `ImageFolder` class of PyTorch takes in a path to a root folder.
    It assumes every subfolder represents a class, and each subfolder should be filled
    with images of that class. This example shows two classes, cats and dogs, with
    very artistic drawings.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2 PyTorch 的 `ImageFolder` 类接受一个根文件夹的路径。它假设每个子文件夹代表一个类别，并且每个子文件夹应该填充该类别的图像。本例展示了两个类别，猫和狗，以及非常艺术化的插图。
- en: 'The `ImageFolder` class takes an optional `transform` object that we use to
    simplify the loading process. This `transform` is the same transformation class
    we used for data augmentation in chapter 3\. Since the images are all of varying
    sizes, we can use a `Compose` transform to resize, crop, and standardize them
    to all have the same shape. This way, we can train on batches of data the same
    way we did on MNIST (all 28 × 28) and CIFAR-10 (32 × 32). Our `Compose` transform
    builds a pipeline of sub-transforms to run in order, which in this case will do
    the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`ImageFolder` 类接受一个可选的 `transform` 对象，我们使用它来简化加载过程。这个 `transform` 是我们在第 3 章中使用的数据增强相同的转换类。由于图像大小各不相同，我们可以使用一个
    `Compose` 转换来调整大小、裁剪并将它们标准化为相同的形状。这样，我们可以像在 MNIST（所有 28 × 28）和 CIFAR-10（32 × 32）上一样在数据批次上训练。我们的
    `Compose` 转换构建了一个子转换的管道，按顺序运行，在这种情况下将执行以下操作：'
- en: Resize the image so the smallest dimension is 130 pixels. For example, a 260
    × 390 image would become 130 × 195 (maintaining the same aspect ratio).
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整图像大小，使最小维度为 130 像素。例如，260 × 390 的图像将变为 130 × 195（保持相同的宽高比）。
- en: Crop out the center 128 × 128 pixels.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 裁剪出中心 128 × 128 像素。
- en: Convert the image to a PyTorch tensor, which includes normalizing the pixel
    values from [0,255] to [0,1].
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像转换为 PyTorch 张量，这包括将像素值从 [0,255] 标准化到 [0,1]。
- en: 'In addition to the transforms, we make a train/test split with 80% of the data
    for training and the remaining 20% for our test set. The following code sets all
    this up with our dataset of cats and dogs:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 除了转换之外，我们还对数据进行 80% 用于训练和剩余 20% 用于测试集的划分。以下代码使用我们的猫和狗数据集设置所有这些：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ The smallest of width/height becomes 130 pixels.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 最小的宽/高变为 130 像素。
- en: ❷ Takes the center 128 × 128 image
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 取中心128 × 128的图像
- en: ❸ Converts it to a PyTorch tensor
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将其转换为PyTorch张量
- en: ❹ Picks 80% for training
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 选择80%用于训练
- en: ❺ 20% remainder for testing
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 20%的剩余部分用于测试
- en: ❻ Creates the random splits of the specified sizes
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 创建指定大小的随机分割
- en: 'With the dataset in place, we can now create our `DataLoader`s for training
    and testing (using a batch size *B* = 128). This dataset has a little over 20,000
    samples in the training set, making it smaller in the total number of images than
    MNIST, but the images are much larger than what we’ve previously worked with (128
    × 128 instead of 32 × 32 or smaller):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集就绪后，我们现在可以创建用于训练和测试的`DataLoader`（使用批大小 *B* = 128）。该数据集的训练集中有超过20,000个样本，在图像总数上比MNIST小，但图像的大小比我们之前处理的大得多（128
    × 128而不是32 × 32或更小）：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We have a dataset and loader objects; let’s take a look at the data. Class
    0 is the cat class, and 1 is the dog class. The next block of code visualizes
    some of the data with the class number marked in the corner. This gives us an
    idea about the complexity of this dataset:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个数据集和加载器对象；让我们看看数据。类别0是猫类，类别1是狗类。下一块代码使用角落中的类别编号可视化了一些数据。这让我们对这个数据集的复杂性有了了解：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Creates a grid of eight images (2 × 4)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个包含八个图像的网格（2 × 4）
- en: ❷ Rows
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 行
- en: ❸ Columns
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 列
- en: ❹ Grabs an image from the test corpus
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从测试语料库中抓取一个图像
- en: ❺ Plots the image
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 绘制图像
- en: ❻ Draws the label in the top-left corner
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 在左上角绘制标签
- en: '![](../Images/CH13_UN01_Raff.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH13_UN01_Raff.png)'
- en: While there are only two classes, the content of the images is more diverse
    and complicated than other toy datasets we have used, like MNIST and CIFAR. The
    animals are in a variety of poses, cameras have over/underexposure, there may
    be more than one animal in the image with different backgrounds, and humans may
    appear in the photos as well. This complexity needs to be learned, but it will
    be a challenge to learn how to classify cats versus dogs using only 20,000 samples.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然只有两个类别，但图像的内容比我们使用的其他玩具数据集（如MNIST和CIFAR）更为多样和复杂。动物有多种姿势，相机可能曝光过度或不足，图像中可能有多个不同的背景下的动物，人类也可能出现在照片中。这种复杂性需要学习，但仅使用20,000个样本来学习如何分类猫与狗将是一个挑战。
- en: 13.2 Transfer learning and training with CNNs
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.2 使用CNN进行迁移学习和训练
- en: Let’s train a model. In particular, we use the smaller ResNet architecture as
    our starting point. ResNet is the architecture that invented the residual connections
    we learned about in chapter 6, and ResNet-X usually refers to one of a few specific
    neural networks that use residual layers (e.g., ResNet-50 or ResNet-101). We use
    ResNet-18, which is the smallest of the (common) ResNets. We can grab an instance
    of the model using the `torchvision.models` class, which has a number of popular
    prebuilt architectures for various computer vision tasks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练一个模型。特别是，我们使用较小的ResNet架构作为起点。ResNet是我们在第6章中学习的残差连接的架构，ResNet-X通常指的是几个使用残差层的特定神经网络之一（例如，ResNet-50或ResNet-101）。我们使用ResNet-18，这是（常见）ResNet中最小的。我们可以使用`torchvision.models`类获取模型的实例，该类为各种计算机视觉任务提供了许多流行的预构建架构。
- en: ResNet uses adaptive pooling at the end of the network, which means we can reuse
    the ResNet architectures for classification problems with essentially any arbitrarily
    sized input image. The problem is that ResNet was designed for a dataset called
    ImageNet, which has 1,000 output classes. Since ImageNet and its 1,000 classes
    are what the ResNet models have been pretrained on, that makes ImageNet our source
    domain. So, the architecture will end with a `nn.Linear` layer with 1,000 outputs.
    Our target domain of cats versus dogs has two classes, so we want it to have only
    one or two outputs (for training with binary cross entropy and softmax, respectively).
    Figure 13.3 shows the situation and how we achieve transfer learning.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet在网络的末端使用自适应池化，这意味着我们可以重用ResNet架构来解决具有任意大小输入图像的分类问题。问题是ResNet是为一个名为ImageNet的数据集设计的，该数据集有1,000个输出类别。由于ImageNet及其1,000个类别是ResNet模型预训练的基础，这使得ImageNet成为我们的源域。因此，该架构将以一个有1,000个输出的`nn.Linear`层结束。我们的目标域（猫与狗）有两个类别，因此我们希望它只有一或两个输出（分别用于具有二元交叉熵和softmax的训练）。图13.3显示了这种情况以及我们如何实现迁移学习。
- en: '![](../Images/CH13_F03_Raff.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH13_F03_Raff.png)'
- en: Figure 13.3 The left side shows a summary of ResNet. We want to alter it to
    look like the right side, where only the last `nn.Linear` layer has been changed.
    The input to each `nn.Linear` layer is 512 because the final convolutional layer
    has *C* = 512 channels, and using adaptive average pooling down to just 1 × 1
    means there are only 512 values in the output, regardless of the size of the original
    image.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3 左侧显示了 ResNet 的总结。我们想要将其修改为右侧的样子，其中只有最后一个 `nn.Linear` 层被改变。每个 `nn.Linear`
    层的输入是 512，因为最终的卷积层有 *C* = 512 个通道，使用自适应平均池化到 1 × 1 意味着输出中只有 512 个值，无论原始图像的大小如何。
- en: 'We can use the object-oriented nature of PyTorch to adapt the preexisting models
    to our new problem with relative ease. The finally fully connected layer in PyTorch’s
    ResNet is named “fc,” so we can reach into the network and replace the `fc` object!
    Since the `nn.Linear` layer keeps the number of inputs in an object called `in_features`,
    we can replace this last layer in a generic fashion that will work without us
    having to hardcode the number of inputs. The following code shows this process,
    which is often called *surgery* because we are chopping off part of the model
    and replacing it with a new part.[¹](#fn49) It takes two lines of code and is
    very easy to accomplish:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用 PyTorch 的面向对象特性，相对容易地将现有模型适应到我们新的问题上。PyTorch 的 ResNet 中的最后一层全连接层被命名为“fc”，因此我们可以深入到网络中并替换掉
    `fc` 对象！由于 `nn.Linear` 层在名为 `in_features` 的对象中保持输入数量，我们可以以通用的方式替换这最后一层，而无需我们硬编码输入数量。下面的代码展示了这个过程，这个过程通常被称为
    *手术*，因为我们正在从模型中移除一部分并替换成新的部分。[¹](#fn49) 这只需要两行代码，并且非常容易完成：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Performs some “surgery"
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 执行一些“手术”
- en: The two lines of code are summarized in figure 13.4\. By default, the ResNet
    model has random weights, so this is basically giving us a new ResNet to train
    from scratch for our problem.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4 中总结了这两行代码。默认情况下，ResNet 模型具有随机权重，所以这基本上是给我们一个新的 ResNet，以便从头开始训练以解决我们的问题。
- en: '![](../Images/CH13_F04_Raff.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH13_F04_Raff.png)'
- en: Figure 13.4 Demonstration of the mechanics of transferring a model via its weights
    in PyTorch. The original model has been initialized with a set of weights (random
    by default; potentially pretrained). We replace the top part of the model with
    a new version that fits our purpose.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4 通过 PyTorch 中的权重转移模型机制的演示。原始模型使用一组权重初始化（默认为随机；可能为预训练）。我们用适合我们目的的新版本替换了模型的顶部部分。
- en: 'This gives us a model we can train to predict whether an image is of a cat
    or a dog. The following code uses the standard `CrossEntropyLoss` that we have
    used many times now, and our regular `train_network` function. The network is
    larger than most of what we have built in this book, and the images are larger,
    too, so training this example takes a bit longer to run:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们一个可以训练来预测图像是猫还是狗的模型。下面的代码使用了我们多次使用过的标准 `CrossEntropyLoss` 和我们的常规 `train_network`
    函数。网络比本书中我们构建的大多数网络都要大，图像也更大，因此训练这个示例需要更长的时间来运行：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now that we have trained the model, we can do our very well-trodden process
    of plotting the result of this regular approach to training a model. We are getting
    some decent results without having to do much thinking. We just took ResNet-18
    and ran with it, which is a common approach that many people use to solve practical
    problems:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了模型，我们可以进行我们非常熟悉的模型训练常规方法的结果绘图过程。我们得到了一些相当不错的结果，而无需进行太多思考。我们只是使用了 ResNet-18
    并运行它，这是许多人用来解决实际问题的常见方法：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/CH13_UN02_Raff.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH13_UN02_Raff.png)'
- en: Note Architectures like ResNet-18 and the others in the `torchvision.models`
    package have gone through a lot of testing and been found to work well across
    a variety of problems. So if you need to do any kind of image classification,
    this is almost always a good place to start, using the hard work others have done
    to design good architectures. But what we’ve done so far just creates a new version
    of ResNet with random initial weights. Transfer learning involves using a set
    of weights Θ that have already been trained on another dataset, which can significantly
    improve our results.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意像 ResNet-18 这样的架构以及 `torchvision.models` 包中的其他架构已经经过大量测试，并在各种问题上表现出良好的效果。所以如果你需要进行任何类型的图像分类，这几乎总是一个好的起点，利用他人已经完成的设计良好的架构的辛勤工作。但到目前为止我们所做的是创建了一个具有随机初始权重的
    ResNet 的新版本。迁移学习涉及使用一组已经在另一个数据集上训练过的权重 Θ，这可以显著提高我们的结果。
- en: 13.2.1  Adjusting pretrained networks
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.1 调整预训练网络
- en: We have initialized a default ResNet module, performed surgery on it to adapt
    it to our classification task, and trained it using the same gradient descent
    tools we have used throughout this book. The only thing missing to turn this into
    real transfer learning is pretraining the ResNet model on a source domain (e.g.,
    ImageNet) instead of randomly initializing it (the default).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经初始化了一个默认的ResNet模块，对其进行了手术以适应我们的分类任务，并使用本书中一直使用的梯度下降工具对其进行训练。要将这变成真正的迁移学习，唯一缺少的是在源域（例如，ImageNet）上预训练ResNet模型，而不是随机初始化（默认设置）。
- en: 'Fortunately, that pretraining has already been done. All of the models provided
    by PyTorch under `torchvision.models` have an option to set a flag `pretrained=True`,
    which returns a version of the model that has already been trained on a designated
    original dataset. For ResNet, that original dataset is ImageNet. So let’s quickly
    grab our new model. We have essentially the same two lines of code for our initial
    surgery on ResNet-18, except that we add the `pretrained=True` flag:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，预训练已经完成。PyTorch在`torchvision.models`下提供的所有模型都有一个选项来设置标志`pretrained=True`，这将返回一个已经在指定原始数据集上训练过的模型版本。对于ResNet，原始数据集是ImageNet。所以让我们快速获取我们的新模型。对于我们最初的ResNet-18手术，我们基本上有相同的两行代码，只是我们添加了`pretrained=True`标志：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Model that has been trained on a dataset
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在数据集上训练过的模型
- en: ❶ Performs some surgery
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 进行一些手术
- en: As before, we replaced the fully connected layer of this network with a new
    one. The original convolutional filters are pretrained, but the fully connected
    layer at the end is randomly initialized because we replaced it with a new `nn.Linear`
    layer—and by default, all modules in PyTorch start with random weights. The hope
    is that because ImageNet is such a large dataset, with 1 million training images,
    we can learn a better model more quickly. We may have to learn this `nn.Linear`
    layer from scratch, but all the preceding convolutional layers should be at a
    better starting place because they were trained on so much data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将这个网络的完全连接层替换成了一个新层。原始的卷积滤波器是预训练的，但最后的完全连接层是随机初始化的，因为我们用一个新的`nn.Linear`层替换了它——默认情况下，PyTorch中的所有模块都以随机权重开始。希望由于ImageNet是一个如此庞大的数据集，有100万张训练图像，我们可以更快地学习到一个更好的模型。我们可能需要从头开始学习这个`nn.Linear`层，但所有前面的卷积层应该有一个更好的起点，因为它们是在大量数据上训练的。
- en: Why pretraining works
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练为什么有效
- en: Before we train `model_pretrained` on our cat/dog problem, we should ask, “what
    is the intuition behind how this works?” *Why* does starting from the weights
    of an already-trained model help us on a new problem? In chapter 3, when we first
    talked about convolutional networks, we saw how convolutions can learn to find
    edges at different angles. Convolutions can also learn to find colors or changes
    in color, sharpen or blur an image, etc. All of these things are broadly useful
    to *any* image-based problem. So the crux of why pretraining works is that the
    things convolutions learn to detect on a large dataset are *probably* the same
    kinds of things we want to detect in any other image-based problem. If these are
    generally useful things for a CNN to learn how to detect, the CNN will probably
    learn them better from more data rather than less. Thus, the pretrained network
    from a larger dataset should hopefully have already learned to look for the kinds
    of patterns we care about, and the final `nn.Linear` layer simply needs to learn
    how to assemble these patterns into a decision. Training from scratch would require
    learning both the patterns and how to make a classification decision from them.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们在猫/狗问题上训练`model_pretrained`之前，我们应该问，“这种工作的直觉是什么？”*为什么*从已经训练过的模型的权重开始可以帮助我们在新的问题上？在第3章，当我们第一次讨论卷积网络时，我们看到了卷积如何学会在不同角度找到边缘。卷积还可以学会找到颜色或颜色变化，锐化或模糊图像等。所有这些对于任何基于图像的问题都是广泛有用的。所以预训练之所以有效，其核心在于卷积在大数据集上学会检测到的东西*可能*是我们想要在任何其他基于图像的问题中检测到的东西。如果这些是一般有用的东西，CNN学会如何检测它们，那么CNN可能从更多的数据中而不是更少的数据中学习得更好。因此，来自更大数据集的预训练网络应该已经学会寻找我们关心的模式，而最后的`nn.Linear`层只需要学会如何将这些模式组装成决策。从头开始训练将需要学习模式和如何从它们中做出分类决策。
- en: Transfer learning generally works best when the source dataset you want to transfer
    from is larger than the target dataset, because you need enough data to have an
    advantage over just training from scratch on the data you care about. There is
    also a factor of how relevant the source data is compared to the target domain
    you want to work on. If the data is relevant enough, the things the source model
    learns well (because it has more data) are more likely to be reusable on the target
    data. This balancing act is summarized in figure 13.5.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习通常在源数据集大于目标数据集时效果最佳，因为您需要足够的数据才能在仅从您关心的数据从头开始训练时获得优势。此外，源数据与您想要工作的目标领域的相关性也是一个因素。如果数据的相关性足够高，源模型学习得好的东西（因为它有更多的数据）更有可能在目标数据上重用。这种平衡行为在图13.5中得到了总结。
- en: '![](../Images/CH13_F05_Raff.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH13_F05_Raff.png)'
- en: Figure 13.5 The tradeoff between source and target dataset size and relevance.
    The top-right corner is the best place to be, and top-left is sub-optimal and
    won’t always work. The bottom half is a uniformly difficult place to do effective
    transfer learning.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5 源数据集和目标数据集大小及相关性的权衡。右上角是最好的位置，左上角次优且不一定总是有效。下半部分是进行有效迁移学习的一个统一困难的地方。
- en: 'Luckily for us, almost any large image dataset tends to be relevant enough
    to put us in the top-right corner of figure 13.5: the structure of pixel correlations
    is very strong and generalizable, which is something easiest to visualize with
    the first layer of the network. Because the first convolutional layer takes in
    the image’s red, green, and blue channels, we can treat each filter as an image
    and plot it to see what it is looking for. Let’s do that for our pretrained model
    first to see what it has as a starting point. First we need to grab the weights
    for the filters from the first convolutional layer. For ResNet, this is defined
    as the `conv1` layer:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，对于几乎任何大型图像数据集，它们的相关性通常足够高，使我们处于图13.5的右上角：像素相关性的结构和可推广性非常强，这是用网络的第一层最容易可视化的。因为第一层卷积层接受图像的红色、绿色和蓝色通道，我们可以将每个滤波器视为一个图像，并绘制它以查看它在寻找什么。让我们首先为我们的预训练模型做这件事，看看它作为起点有什么。首先，我们需要从第一层卷积层获取滤波器的权重。对于ResNet，这被定义为`conv1`层：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Grabs the first convolutional filters weights, moves them to the CPU, and
    turns them into a NumPy tensor
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取第一组卷积滤波器的权重，将它们移动到CPU，并将它们转换为NumPy张量
- en: 'The `filters_pretrained` object now has a copy of the weights used by the model.
    It has a shape of (64,3,7,7) because ResNet-18’s first layer has 64 filters and
    expects an input with 3 channels (red, green, and blue), and the 64 filters are
    each 7 × 7 in width and height. Since we want to plot these, let’s first normalize
    the filters to the range [0,1], since that is what Matplotlib expects for color
    images:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`filters_pretrained`对象现在有了模型使用的权重副本。它的形状为(64,3,7,7)，因为ResNet-18的第一层有64个滤波器，期望输入有3个通道（红色、绿色和蓝色），并且64个滤波器在宽度和高度上都是7
    × 7。由于我们想要绘制这些，让我们首先将滤波器归一化到[0,1]的范围内，因为这是Matplotlib对彩色图像的期望：'
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Shift so everything is in the range [0, Max value]
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将所有值移至[0, 最大值]范围内
- en: ❷ Re-scale so everything is [0, 1]
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 重新缩放，使所有值都在[0, 1]范围内
- en: 'Matplotlib also expects images to be formatted as (*W*,*H*,*C*), but PyTorch
    uses (*C*,*W*,*H*). To fix this, we move the channel dimension (1, because dimension
    0 has the number of filters) to the last position (-1) to match Matplotlib’s expectations:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Matplotlib还期望图像格式为(*W*,*H*,*C*)，但PyTorch使用(*C*,*W*,*H*)。为了解决这个问题，我们将通道维度（1，因为维度0是滤波器的数量）移动到最后的位置（-1），以匹配Matplotlib的期望：
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ The weights are shaped (#Filters, C, W, H), but Matplotlib expects (W, H,
    C), so we move the channel dimension.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 权重的形状为(#Filters, C, W, H)，但Matplotlib期望(W, H, C)，因此我们移动了通道维度。
- en: Look to the filters
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 查看滤波器
- en: 'Next we can plot the filters. You should see a number of common patterns among
    them, like white/black edges at different angles and frequencies (one white and
    one black line, versus several). These black-and-white filters act as edge detectors,
    detecting edges at different angles and patterns with different rates of repetition.
    You also see a few filters that have a single color like blue, red, purple, or
    green. These filters detect specific color patterns. If you have a large, diverse
    enough training set, you’ll tend to see results like these in your first convolutional
    layer, even if it is a completely different problem:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们可以绘制过滤器。你应该会看到它们之间有许多共同的模式，比如不同角度和频率的黑白边缘（一条白色和一条黑色线，与几条线）。这些黑白过滤器作为边缘检测器，以不同的重复率检测不同角度和模式的边缘。你还看到一些具有单一颜色如蓝色、红色、紫色或绿色的过滤器。这些过滤器检测特定的颜色模式。如果你有一个足够大、多样化的训练集，即使是一个完全不同的问题，你也会在第一个卷积层中看到类似的结果：
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Take sqrt(# items) to make a square grid of images
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将项目数开平方以形成一个图像的正方形网格
- en: '❷ Divides by the # of rows'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 除以行数
- en: ❸ Makes the grid in which to plot the images
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建用于绘制图像的网格
- en: ❹ Each row
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 每一行
- en: ❺ Each column
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 每一列
- en: ❻ Indexes into the filters
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 对过滤器进行索引
- en: ❼ Plots the specific filter
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 绘制特定的过滤器
- en: ❽ Turns off the numbered axis to avoid clutter
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 关闭编号的坐标轴以避免杂乱
- en: '![](../Images/CH13_UN03_Raff.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH13_UN03_Raff.png)'
- en: 'But we got that with ImageNet’s 1 million training images. We have around 20,000,
    which is far fewer. What happens when we learn filters from scratch on this data?
    Let’s take a look at the `model` we trained and find out. This uses the same code
    as before, but we wrap it in a function called `visualizeFilters` that takes in
    the tensor to visualize. We pass in the first `conv1` filters from the original
    `model` we trained from scratch, and we can see the filters that result:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们是通过ImageNet的100万张训练图像得到这个结果的。我们大约有20,000张，这要少得多。当我们从这些数据中从头开始学习过滤器时会发生什么？让我们看看我们训练的`model`，并找出答案。这使用与之前相同的代码，但我们将其包装在一个名为`visualizeFilters`的函数中，该函数接受要可视化的张量。我们传入原始`model`中训练的`conv1`过滤器的第一个，我们可以看到产生的过滤器：
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Filters from the model we trained at the start of this chapter
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 本章开始时我们训练的模型的过滤器
- en: ❷ Plots the results
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 绘制结果
- en: '![](../Images/CH13_UN04_Raff.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH13_UN04_Raff.png)'
- en: Pretrained ResNet-18 had nice, crisp filters, and it was easy to see what each
    had learned to detect. Here, the filters look like they contain noise. We see
    some evidence of black/white edge-detection filters forming, but they are also
    tainted with color information. This means a filter that is working to detect
    edges will partially fire for items without edges but with the right color—which
    could cause issues down the line.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的ResNet-18有清晰、锐利的过滤器，很容易看出每个过滤器学到了什么来检测。在这里，过滤器看起来像包含了噪声。我们看到一些形成黑白边缘检测过滤器的证据，但它们也受到了颜色信息的影响。这意味着一个用于检测边缘的过滤器在边缘不明显的物品上也会部分激活，但颜色是正确的——这可能会在后续造成问题。
- en: In general, you should not judge the quality of a model by what the filters
    look like but instead by the behavior of the model on realistic, diverse test
    data. In this case, we know what good filters *usually* look like in the first
    layer of an RGB image model and can make reasonable comparisons. These filters
    don’t look good.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，你不应该根据过滤器的样子来判断模型的质量，而应该根据模型在真实、多样化的测试数据上的行为来判断。在这种情况下，我们知道在RGB图像模型的第一层中，好的过滤器通常是什么样的，并且可以做出合理的比较。这些过滤器看起来并不好。
- en: But just because the filters are different does not mean they are always worse.
    To judge that, we would need to train the pretrained network and compare the accuracy
    on the test data. If the pretrained model had better accuracy, the quality of
    the initial convolutional filters could be one valid explanation for why the performance
    was worse.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 但仅仅因为过滤器不同并不意味着它们总是更差。要判断这一点，我们需要训练预训练网络并在测试数据上比较准确率。如果预训练模型有更高的准确率，那么初始卷积过滤器的质量可能是性能较差的一个有效解释。
- en: 13.2.2  Preprocessing for pretrained ResNet
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.2 预训练ResNet的预处理
- en: Now that we have some intuition as to *why* we want to use a pretrained network,
    let’s train a new network and find out if it is any better. It is critically important
    that we make our input data match the pretrained model’s expectations. In particular,
    the ImageNet models in `torchvision.models` use a standardization of zero mean
    and unit variance (*μ* = 0, *σ* = 1) for each input color channel. The particular
    coefficients used are from ImageNet, so we quickly define a new `Module` to normalize
    the input before it’s passed to the pretrained network.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对为什么要使用预训练网络有了些直觉，让我们训练一个新的网络，看看它是否有所改进。确保我们的输入数据与预训练模型期望的相匹配是至关重要的。特别是`torchvision.models`中的ImageNet模型对每个输入颜色通道使用零均值和单位方差（*μ*
    = 0，*σ* = 1）进行标准化。所使用的特定系数来自ImageNet，所以我们快速定义一个新的`Module`来在输入传递给预训练网络之前对其进行归一化。
- en: Warning If you use transfer learning via a pretrained network, you *must* make
    sure your preprocessing of the data matches how the model was originally trained.
    Otherwise, the model will not get what it initially expects, and the weights become
    meaningless. The preprocessing used isn’t always well documented, which can be
    annoying, and it’s a critical gotcha detail to look out for.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：如果你通过预训练网络进行迁移学习，你*必须*确保你的数据预处理与模型最初训练的方式相匹配。否则，模型将无法获得它最初期望的内容，权重将变得毫无意义。使用的预处理并不总是有很好的文档记录，这可能会让人烦恼，而且这是一个需要特别注意的关键细节。
- en: 'The next block of code does this normalization. The entire model that does
    all the work is passed in as the `baseModel`, and we wrap it with a `Module` that
    pre-normalizes the input. This normalization is specified by how ResNet was originally
    trained, and we use `requires_grad=False` so the normalization isn’t altered during
    training:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 下一段代码执行了这个归一化。执行所有工作的整个模型作为`baseModel`传入，我们用`Module`包装它以预先归一化输入。这种归一化是由ResNet最初训练的方式指定的，我们使用`requires_grad=False`以确保归一化在训练过程中不被改变：
- en: '[PRE14]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ The model that we want to use. We need to normalize its input first.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们想要使用的模型。我们首先需要对其进行输入归一化。
- en: ❷ The mean and standard deviation used for ImageNet normalization. We just have
    to accept these “magic” numbers that everyone uses.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 用于ImageNet归一化的均值和标准差。我们只能接受这些大家普遍使用的“魔法”数字。
- en: '❸ requires_grad=False: we don’t want these values to change during training!'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ requires_grad=False：我们不想在训练过程中改变这些值！
- en: ❹ Normalizes the input and then feeds it into the model we want to use
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 归一化输入并将其输入到我们想要使用的模型中
- en: Warning A lot of code you see online hardcodes this normalization step or hardcodes
    it into the data loader. I don’t like either of these approaches. The normalization
    is *specific* to networks pretrained on ImageNet. That means it’s not part of
    the data, so it shouldn’t be part of the transforms used in the `Dataset` class.
    If you want to switch to something else, you may not want to use the same normalization.
    I prefer making the normalization part of the model because that’s intrinsically
    where these normalization values came from! I appear to be in the minority on
    this, so be on the lookout when you read other people’s code.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：你在线上看到的很多代码都硬编码了这个归一化步骤，或者将其硬编码到数据加载器中。我不喜欢这两种方法。归一化是*特定*于在ImageNet上预训练的网络。这意味着它不是数据的一部分，因此它不应该成为`Dataset`类中使用的转换的一部分。如果你想要切换到其他东西，你可能不想使用相同的归一化。我更喜欢将归一化部分作为模型的一部分，因为这些都是归一化值来源的地方！我似乎在这个问题上属于少数，所以当你阅读其他人的代码时要留心。
- en: 'This `NormalizeInput` class performs the normalization used for the pretrained
    ResNet models in PyTorch. Now we can wrap our pretrained model with this normalizing
    `Module` to get the correct behavior. This way, there is no mismatch between how
    we have formatted our data and what the pretrained weights were expecting. I like
    this approach because it encapsulates the peculiarity of the preprocessing that
    is *specific to this circumstance* into its own class. If we want to change to
    a different model or augment the data loader with our own transformations, we
    can do so without worrying about the model-specific preprocessing that must occur,
    because the model-specific processing is part of the model. This happens here
    in a single line of code that wraps the pretrained model with this specific normalizer:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`NormalizeInput`类执行了用于PyTorch中预训练ResNet模型的归一化。现在我们可以用这个归一化`Module`包装我们的预训练模型，以获得正确的行为。这样，我们数据格式化和预训练权重期望之间就没有不匹配。我喜欢这种方法，因为它将特定于这种情况的预处理特殊性封装到其自己的类中。如果我们想更换不同的模型或用我们自己的转换增强数据加载器，我们可以这样做，而不用担心必须发生的模型特定预处理，因为模型特定处理是模型的一部分。这在这里通过一行代码实现，即用这个特定的归一化器包装预训练模型：
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Our model preprocessing matches what the pretrained model expects, and we can
    finally move on to training the network. There are two primary approaches to doing
    so, which we will discuss.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的预处理与预训练模型所期望的相匹配，我们最终可以继续训练网络。有两大主要方法可以这样做，我们将讨论。
- en: 13.2.3  Training with warm starts
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.3 使用warm start进行训练
- en: 'We have a pretrained model that is set to preprocess the data correctly, and
    we have our new data. The simplest way to go forward is to call `train_network`
    with `model_``pretrained` and see what happens. The following line trains this
    model, and we can check the results and see how it did. We called these the `warmstart_results`
    because this approach to transfer learning is called a *warm start*:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个预训练模型，它被设置为正确地预处理数据，而我们也有我们的新数据。前进的最简单方式是调用`train_network`函数，并传入`model_`pretrained`参数，看看会发生什么。以下行训练了这个模型，我们可以检查结果并看看它的表现如何。我们称之为`warmstart_results`，因为这个迁移学习的方法被称为*warm
    start*：
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: A nuanced point worth making is that warm starting and transfer learning are
    *not* synonyms. Training *any* model with a warm start is when you use *any* set
    of initial weight values *Θ*[*i**n**i**t*] that you expect to be closer to your
    desired solution than using the default random values. Warm starts are a common
    optimization approach, and transfer learning is not the only situation where warm
    starts happen. So if someone tells you they are using a warm start, that does
    not necessarily mean they are doing any kind of transfer learning. In short, a
    warm start simply means you have an initial set of weights that you believe are
    better than random weights. It so happens that one approach to transfer learning
    is via this warm-start strategy.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，warm start和迁移学习并不是同义词。用warm start训练*任何*模型是指你使用*任何*一组你期望比使用默认随机值更接近你期望解的初始权重值*Θ*[*i**n**i**t*]。Warm
    start是一种常见的优化方法，迁移学习并不是唯一发生warm start的情况。所以如果有人告诉你他们正在使用warm start，这并不一定意味着他们正在进行任何类型的迁移学习。简而言之，warm
    start仅仅意味着你有一个你相信比随机权重更好的初始权重集。恰好有一种迁移学习的方法是通过这种warm-start策略。
- en: Warm starts outside transfer learning
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习之外的warm start
- en: A common application of warm starts outside of transfer learning is hyperparameter
    optimization with linear models. Because linear models are often trained with
    exact solvers (you converge to the one true answer), their runtime depends in
    part on the values used at the start of the process. When you are training 10+
    models, each with a different regularization penalty λ, the solution to the model
    with one value of λ is probably similar to the solution with a slightly different
    value *λ* + *ϵ*. Since you will arrive at the correct answer regardless of starting
    point, you can warm-start the solution to *λ* + *ϵ* using the previous solution
    found for λ.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习之外，warm start的一个常见应用是线性模型的超参数优化。因为线性模型通常使用精确求解器（你收敛到唯一正确答案），它们的运行时间部分取决于过程开始时使用的值。当你训练10+个模型，每个模型都有一个不同的正则化惩罚λ时，具有一个λ值的模型解可能与具有略微不同值*λ*
    + *ϵ*的解相似。由于你将到达正确答案，无论起始点如何，你可以使用之前为λ找到的解来warm start *λ* + *ϵ*的解。
- en: This technique is very popular with Lasso-regularized models and support vector
    machines because of their higher training costs and the need to do a hyperparameter
    search to get good results. This is what happens with tools like scikit’s `LassoCV`
    class ([http://mng.bz/nrB8](http://mng.bz/nrB8)).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术在 Lasso 正则化模型和支持向量机中非常受欢迎，因为它们的训练成本较高，并且需要执行超参数搜索以获得良好的结果。这就是 scikit 的 `LassoCV`
    类等工具所发生的情况 ([http://mng.bz/nrB8](http://mng.bz/nrB8))。
- en: 'Since our warm weights come from a model trained on another problem, the weights
    are how we *transfer* knowledge from the original domain to the new domain. Another
    name for this approach is *fine tuning*: we have something that is generically
    good, and we want to adjust it slightly to our specific problem.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的预训练权重来自训练另一个问题的模型，因此权重是我们如何从原始域*迁移*知识到新域的方式。这种方法的另一个名称是*微调*：我们有一些通用的好东西，我们想要对其进行轻微调整以适应我们的特定问题。
- en: 'By calling the `train_network` function with the pretrained weights, we are
    performing that slight adjustment, because gradient descent will alter every weight
    of the network to try to minimize the loss. When we plot the results to see if
    this was a good idea, a dramatic difference in accuracy occurs. The warm start
    didn’t just reach a higher accuracy: it reached a higher accuracy after a *single*
    epoch. That means we could have made this whole process 10× faster by not training
    for 10 epochs. We obviously didn’t know that in advance, but this illustrates
    the kinds of advantages you see when using pretraining. You converge faster, often
    to better solutions:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用预训练权重调用 `train_network` 函数，我们正在执行这种轻微的调整，因为梯度下降会改变网络中的每个权重以尝试最小化损失。当我们绘制结果以查看这是否是一个好主意时，准确性出现了显著差异。预启动不仅达到了更高的准确性：它在一个*单个*周期后达到了更高的准确性。这意味着我们可以通过不训练
    10 个周期来使整个过程快 10 倍。我们显然事先不知道这一点，但这说明了使用预训练时可以看到的优势：你收敛得更快，通常到更好的解决方案：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](../Images/CH13_UN05_Raff.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH13_UN05_Raff.png)'
- en: 'This double win of better accuracy in less time is part of why transfer learning
    is one of the most useful tools to tackle new problems. We can also get some intuition
    about how useful the warm start was by comparing the weights before and after
    fine-tuning. The following code again calls `visualizeFilters` to look at the
    convolutional filters after the fine-tuning of the ResNet-18 model. The filters
    are essentially identical to what we started with, which is a good indicator that
    they are indeed generically good filters for many problems. If they weren’t, SGD
    would have altered them more to improve its accuracy:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这种在更短时间内获得更高精度的双重胜利是迁移学习成为解决新问题最有用的工具之一的部分原因。我们还可以通过比较微调前后的权重来了解预启动有多有用。以下代码再次调用
    `visualizeFilters` 来查看 ResNet-18 模型微调后的卷积过滤器。这些过滤器基本上与我们开始时的一样，这是一个很好的指标，表明它们确实是许多问题的通用良好过滤器。如果不是这样，SGD
    会更多地改变它们以提高其准确性：
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Grabs the filters after fine-tuning the warm-started model
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在微调预启动模型后获取过滤器
- en: ❷ Plots the filters, which look very similar to the pretrained model’s initial
    filters
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 绘制过滤器，它们看起来与预训练模型的初始过滤器非常相似
- en: '![](../Images/CH13_UN06_Raff.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH13_UN06_Raff.png)'
- en: 13.2.4  Training with frozen weights
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.4 使用冻结权重进行训练
- en: We also have another option for transfer learning in this situation called *weight
    freezing* or using *frozen* weights. Weight freezing is when we decide not to
    alter the parameters/coefficients of a layer. Gradients are still calculated and
    backpropagated through the layer, but when we perform gradient updates, we make
    no changes—as if we set the learning rate *η* = 0.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们还有另一种迁移学习的选项，称为*权重冻结*或使用*冻结*权重。权重冻结是指我们决定不改变层的参数/系数。梯度仍然会在层中计算并反向传播，但当我们执行梯度更新时，我们不做任何改变——就像我们将学习率
    *η* 设置为 0 一样。
- en: It is not possible to use weight freezing for *all* layers of the network; that
    would mean there is nothing to train! Training is only meaningful if we adjust
    at least some of the parameters of the model. A common approach is to freeze the
    weights for all the convolutional and normalizing layers and only change the weights
    of the fully connected layers. This implicitly assumes that the filters learned
    from the original domain are *as good as or better* than what we could learn on
    this new domain.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 无法使用权重冻结来冻结网络的*所有*层；这意味着没有东西可以训练！只有当我们调整模型的一些参数时，训练才有意义。一种常见的方法是冻结所有卷积和归一化层的权重，只更改全连接层的权重。这隐含地假设从原始域学习到的过滤器*与或优于*在这个新域上学习到的过滤器。
- en: 'To do this, we start by setting the `requires_grad` flag for every parameter
    in our model to `False`. This way, no parameter saves a gradient after backpropagation,
    and thus no changes occur when the optimizer performs the update step. After freezing
    the entire model, we replace the model’s fully connected layer, which by default
    has `requires_grad=True`. We want this since the new fully connected layer is
    the only layer we want to adjust. Then we can build and train the model just as
    we did with the warm-start approach. The following code does the freezing process
    and then trains the model:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们首先将模型中每个参数的 `requires_grad` 标志设置为 `False`。这样，在反向传播后，没有任何参数保存梯度，因此当优化器执行更新步骤时，不会发生任何变化。在冻结整个模型后，我们替换模型的完全连接层，该层默认具有
    `requires_grad=True`。我们希望这样做，因为新的完全连接层是我们唯一想要调整的层。然后我们可以像使用预热方法一样构建和训练模型。以下代码执行冻结过程，然后训练模型：
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Turn off gradient updating for all parameters!
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 关闭所有参数的梯度更新！
- en: ❷ Our new fc layer has requires_grad = True by default.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们的新全连接层默认 requires_grad = True。
- en: 'Next we plot the results. The frozen model is *very* stable in its results.
    That makes sense because it isn’t adjusting nearly as many parameters. It performs
    just a smidge worse than the warm model but still far better than the naive approach
    of training from scratch:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们绘制结果。冻结模型的结果非常稳定。这很合理，因为它调整的参数数量要少得多。它的表现略逊于预热模型，但仍然远优于从头开始训练的朴素方法：
- en: '[PRE20]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/CH13_UN07_Raff.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH13_UN07_Raff.png)'
- en: 'So which is better, warm or frozen weights? This single result seems to indicate
    a minor tradeoff: warm weights are more accurate, but frozen weights are more
    consistent. This is *true* but does not tell the full story. To hear that story,
    keep reading: the next section talks about the primary factor that can tip the
    scales in this tradeoff—*dataset size*.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，预热权重和冻结权重哪个更好？这个单一的结果似乎表明了一个小的权衡：预热权重更准确，但冻结权重更一致。这是真的，但它并没有讲述整个故事。要了解这个故事，请继续阅读：下一节将讨论在这个权衡中可能起到决定性作用的因素——*数据集大小*。
- en: 13.3 Learning with fewer labels
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.3 使用较少标签进行学习
- en: So far, we have warm starting and frozen weights as our two primary methods
    of performing transfer learning. These are the most common and successful ways
    of doing transfer learning in practice. But when should you do which? You can
    always try both and see which works best, but frozen weights have a particular
    advantage when you have extremely small amounts of training data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经有了预热和冻结权重作为我们执行迁移学习的两种主要方法。这些是在实践中进行迁移学习最常见和最成功的方法。但你应该在什么时候使用哪一种？你总是可以尝试两者，看看哪个效果最好，但当你有极少的训练数据时，冻结权重具有特定的优势。
- en: 'Why would that be? Imagine that a single ideal set of parameters *Θ*^* will
    give the best performance for a problem.[²](#fn50) Your ability to find *any*
    set of parameters Θ depends on how much data (and compute resources) you can get.
    One *simplified* way to think about this is that your estimate of the true parameters
    is a noisy view of them:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会这样？想象一下，一个理想的参数集 Θ 将为问题提供最佳性能。[²](#fn50) 您找到任何一组参数 Θ 的能力取决于您可以获得多少数据（以及计算资源）。一种简化的思考方式是，您对真实参数的估计是对它们的噪声视图：
- en: '![](../Images/ch13-eqs-to-illustrator0x.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ch13-eqs-to-illustrator0x.png)'
- en: The more data you have, the better the model you can build, and *ϵ* → 0. If
    you have *no* data, you can only pick answers randomly because you can only pick
    Θ randomly, so *ϵ* → ∞. Clearly, the size of your training data N impacts how
    well you can estimate your parameters.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 你拥有的数据越多，你能够构建的模型就越好，*ϵ* → 0。如果你没有数据，你只能随机选择答案，因为你只能随机选择 Θ，所以 *ϵ* → ∞。显然，你的训练数据大小
    N 影响你估计参数的能力。
- en: The other factor is how many parameters you have. Imagine if you had 1,000 data
    points about the height of random people. You could probably get a very accurate
    estimate about the average height and standard deviation of heights for people
    in general from this 1,000-person sample. But what if you want to document *1
    trillion* different things, such as every possible interaction of DNA with height,
    weight, hair color, health, diseases, left-/right-handedness, propensity for making
    bad puns, and so on? There are too many interactions, and your ability to get
    an accurate answer for *every* one of those things from just 1,000 people will
    be astronomically low. The number of parameters D is a factor in how well you
    can estimate your model. Crudely, we could say
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个因素是你有多少个参数。想象一下，如果你有关于随机人群身高的1,000个数据点。你可能会从这个1,000人的样本中非常准确地估计出一般人群的平均身高和身高标准差。但如果你想要记录*1万亿*种不同的事物，比如DNA与身高、体重、发色、健康、疾病、左撇子/右撇子、恶搞倾向等所有可能的相互作用？有太多的相互作用，而你从1,000个人中得到关于所有这些事物的准确答案的能力将极其低。参数数量D是你估计模型好坏的一个因素。粗略地说，我们可以这样讲
- en: '![](../Images/CH13_UN08_Raff.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH13_UN08_Raff.png)'
- en: This is a very rough intuitive form. There is no truly linear relationship between
    solution quality, the number of features D, and the number of data points N.[³](#fn51)
    The point is to illustrate that if you do not have enough data N, and you have
    too many parameters D, you will not learn a good model.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常粗略的直观形式。解决方案质量、特征数量D和数据点数量N之间没有真正的线性关系。[³](#fn51) 重点是要说明，如果你没有足够的数据N，而你有很多参数D，你将无法学习一个好的模型。
- en: 'This gives us more understanding of when to use warm versus frozen weights.
    When we freeze weights, they are no longer parameters we can modify, which effectively
    reduces the D term of the equation and better estimates the remaining parameters.
    This is also why the frozen approach is more stable than the warm approach: the
    noise factor is dampened by reducing the number of parameters D.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我们更了解何时使用预热与冻结权重。当我们冻结权重时，它们就不再是我们可以修改的参数，这实际上减少了方程中的D项，并更好地估计剩余的参数。这也是为什么冻结方法比预热方法更稳定的原因：通过减少参数数量D，噪声因素被减弱。
- en: 'This is particularly valuable when we have less labeled data. To show how,
    we can simulate the situation with our cats versus dogs classifier by randomly
    sampling a small portion to use for training: twice our batch size, for a total
    of 256 training images. This is normally *far* too little data to learn any kind
    of CNN from scratch:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我们拥有较少标记数据时尤其有价值。为了展示这一点，我们可以通过随机采样一小部分用于训练来模拟我们的猫狗分类器的情况：批量大小的两倍，总共256个训练图像。这通常*远远*不够的数据来从头开始学习任何类型的CNN：
- en: '[PRE21]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Makes the small dataset = 2* the batch size
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将小数据集设置为批量大小的2倍
- en: ❷ Makes the loader for this tiny dataset
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ ❶ 为这个微小的数据集制作加载器
- en: 'Now we have a *much* smaller dataset. We can train a model using all three
    approaches: from scratch, using a warm start, and using frozen weights. Our first
    results showed a warm start performing slightly better than freezing. If our understanding
    is correct, freezing the weights should do better than a warm start in this scenario.
    To test this, let’s train up each of these options:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个*小得多*的数据集。我们可以使用所有三种方法来训练一个模型：从头开始、使用预热启动和使用冻结权重。我们的初步结果显示，预热启动的表现略好于冻结。如果我们理解正确，在这种情况下冻结权重应该比预热启动表现更好。为了测试这一点，让我们训练每个选项：
- en: '[PRE22]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ 1\. Training from scratch
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 1. 从头开始训练
- en: ❷ 2\. Training the warm model
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ ❷ 2. 预热模型的训练
- en: ❸ Performs some surgery
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 执行一些手术
- en: ❹ 3\. Training with frozen weights
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 3. 使用冻结权重进行训练
- en: ❺ Turns off gradient updating for all parameters
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 关闭所有参数的梯度更新
- en: ❻ Our new fc layer has requires_grad = True by default.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 我们的新全连接层默认requires_grad = True。
- en: 'Notice that we didn’t change *any* of the code for each of these three options.
    They all operate mechanically the same way as before; the only difference is how
    little data we are giving each model. The results are plotted next, and we can
    see a huge impact that matches our understanding of how parameter count D and
    dataset size N affect learning with warm and frozen weights:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们没有改变这三个选项中的任何代码。它们都以与之前相同的方式机械地操作；唯一的区别是我们给每个模型提供的数据量有多小。结果将在下面绘制，我们可以看到巨大的影响，这与我们对参数数量D和数据集大小N如何影响使用预热和冻结权重进行学习的理解相匹配：
- en: '[PRE23]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../Images/CH13_UN09_Raff.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH13_UN09_Raff.png)'
- en: We see a *dramatic* difference in the results. Training from scratch is still
    the worst, barely getting above 50% accuracy on the test set. Using a warm start
    is better, with ≈ 80% accuracy; but using frozen convolutional layers does best
    at ≈ 91%, almost as good as the results training on 20,000 samples. So the frozen
    approach is the best when we have a very limited amount of training data, but
    its ability for further improvement is also limited. That is where warm starts
    come in; warm models can start coming out ahead of frozen models if you have enough
    labeled data (but still not too terribly much).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在结果上看到了*显著*的差异。从头开始训练仍然是效果最差的，测试集上的准确率勉强超过50%。使用预热启动更好，准确率约为80%；但使用冻结的卷积层效果最佳，约为91%，几乎与在20,000个样本上训练的结果一样好。因此，当我们的训练数据非常有限时，冻结方法是最好的，但其进一步改进的能力也是有限的。这就是预热启动发挥作用的地方；如果您有足够的标记数据（但仍然不是太多），预热模型可以开始领先于冻结模型。
- en: Note If you are doing on-the-job work that is computer-vision-based, I would
    almost always recommend starting with pretrained models. The approach is so effective
    that there is limited reason to train a model from scratch if you want to use
    current tools and build a product. It’s worth training one model from scratch
    to see if you have a rare problem where pretraining does not work, but otherwise,
    pretrained models will make your life easier. If you can build a viable solution
    with pretrained models, you can eventually build a data collection and labeling
    process to make your own large corpus that can help improve things; but that is
    a sizable investment. Pretraining helps you reach a viable first solution without
    having to pay that high cost.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果您正在进行基于计算机视觉的在职工作，我几乎总是建议从预训练模型开始。这种方法非常有效，如果您想使用当前的工具并构建一个产品，就没有必要从头开始训练模型。值得从头训练一个模型来看看您是否有一个罕见的问题，预训练不起作用，但除此之外，预训练模型会使您的生活更轻松。如果您可以使用预训练模型构建一个可行的解决方案，您最终可以构建一个数据收集和标注过程，以创建自己的大型语料库，这有助于改进事物；但这是一项重大的投资。预训练帮助您在不支付高昂成本的情况下达到一个可行的初步解决方案。
- en: While we have not shown it here, you can also balance between a warm start and
    a frozen start. As we mentioned earlier, the first convolutional layers of a computer
    vision task tend to learn filters that are generically useful across a wide array
    of problems. As you get deeper into the network, the filters become more specialized.
    In very deep models, the last convolutional filters are often only useful for
    the specific task at hand.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们没有在这里展示，您也可以在预热启动和冻结启动之间取得平衡。正如我们之前提到的，计算机视觉任务的第一层卷积通常学习到在广泛问题中通用的过滤器。随着您深入网络，过滤器变得更加专业化。在非常深的模型中，最后的卷积过滤器通常只对当前任务有用。
- en: You can exploit that by freezing the initial layers of the network but allowing
    the later layers to be altered as a warm start. This hybrid approach can require
    a bit of trial and error to select the depth at which to stop freezing the weights,
    depending on your original model (i.e., which version of ResNet or some other
    architecture), how much data it was trained on, and your new target domain data.
    The adjustment of freezing versus not-freezing different layers essentially becomes
    your new way to modify the model when using a pretrained network since you can’t
    add more neurons or layers to something that already exists. This won’t always
    make a huge difference, so many people skip it to focus on designing more/better
    data-augmentation pipelines as a better return on their time.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过冻结网络的前几层，但允许后续层作为预热启动来利用这一点。这种混合方法可能需要一些试错来选择停止冻结权重的深度，这取决于您的原始模型（例如，ResNet的哪个版本或其他架构），它在多少数据上进行了训练，以及您的新目标领域数据。调整冻结与不冻结不同层实际上成为您在使用预训练网络时修改模型的新方法，因为您不能向已经存在的东西添加更多神经元或层。这并不总是会产生巨大差异，所以许多人跳过这一步，专注于设计更多/更好的数据增强管道，以获得更好的时间回报。
- en: 13.4 Pretraining with text
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.4 使用文本进行预训练
- en: The success of using pretrained networks to perform transfer learning depends
    on learning *robust* features/patterns that are widely applicable. Until recently,
    this approach had not been successful for natural language processing (NLP) related
    tasks. Thanks to new models like the *transformer* that we learned about in chapter
    12, this situation is finally starting to change.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练网络进行迁移学习成功的关键在于学习*鲁棒*的特征/模式，这些模式具有广泛的应用性。直到最近，这种方法在自然语言处理（NLP）相关任务中还没有成功。感谢我们在第12章中了解到的新模型*transformer*，这种情况终于开始改变。
- en: In particular, a family of transformer-based algorithms has significantly improved
    the quality of results we can achieve on text problems. The first of these pretrained
    models is called BERT[⁴](#fn52) (yes, named after the character Bert on *Sesame
    Street*). To get started adjusting a pretrained BERT model, we will reuse the
    AG News dataset from the last chapter (loading `torchtext`, the `tokenizer` and
    `Vocab` objects, and the `text_transform` and associated `label_transform`), and
    call the training and test set for AG News `train_dataset_text` and `test_dataset_text`,
    respectively).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是基于转换器的一组算法显著提高了我们在文本问题上的结果质量。这些预训练模型中的第一个被称为BERT[⁴](#fn52)（是的，是以《芝麻街》上的角色Bert的名字命名的）。为了开始调整预训练的BERT模型，我们将重用上一章的AG
    News数据集（加载`torchtext`、`tokenizer`和`Vocab`对象，以及`text_transform`和相关的`label_transform`），并将AG
    News的训练集和测试集分别命名为`train_dataset_text`和`test_dataset_text`）。
- en: 'The only real change is that we create a small version of the corpus with 256
    labeled items. Learning with limited training data is where transfer learning
    gets the greatest return on results and also helps these examples run quickly:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一真正的变化是我们创建了一个包含256个标记项的小型语料库版本。在有限训练数据上进行学习是迁移学习获得最大结果回报的地方，同时也帮助这些示例快速运行：
- en: '[PRE24]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Slices off a tiny dataset
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 切割出一个小型数据集
- en: 'Now we train the same GRU model from chapter 12 as a baseline. On the full
    dataset, a GRU was able to obtain 92% accuracy. With this smaller labeled set,
    we expect the accuracy to decrease. The following block of code reuses the same
    `pad_batch` function from the last chapter to train the same GRU model, but we
    have only 256 labeled examples:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们训练第12章中相同的GRU模型作为基线。在完整数据集上，GRU能够达到92%的准确率。使用这个较小的标记集，我们预计准确率会下降。以下代码块重用了上一章的相同`pad_batch`函数来训练相同的GRU模型，但我们只有256个标记示例：
- en: '[PRE25]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ (B, T) -> (B, T, D)
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ (B, T) -> (B, T, D)
- en: ❷ (B, T, D) -> ( (B,T,D) , (S, B, D) )
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ (B, T, D) -> ( (B,T,D) , (S, B, D) )
- en: ❸ Reduces the RNN output to one item, (B, 2*D)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将RNN输出减少到一个项目，(B, 2*D)
- en: ❹ (B, D) -> (B, classes)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ (B, D) -> (B, classes)
- en: ❺ Creates train and test loaders using collate_fn
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用collate_fn创建训练和测试加载器
- en: ❻ Trains our baseline GRU model
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 训练我们的基线GRU模型
- en: 13.4.1  Transformers with the Hugging Face library
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.4.1 使用Hugging Face库的转换器
- en: 'Our baseline GRU is trained and represents the typical approach we would use.
    For the transfer learning version, we create a frozen BERT model to gain the benefits
    of this pretrained text model. The first thing we need is an implementation that
    contains some pretrained architectures. Lucky for us, the Hugging Face ([https://huggingface.co/transformers](https://huggingface.co/transformers))
    library has quickly become a de facto tool and repository for researchers to place
    the latest and greatest extensions to BERT. To install it, run the following command:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的基线GRU已经训练好，代表了我们会使用的典型方法。对于迁移学习版本，我们创建一个冻结的BERT模型来获得这个预训练文本模型的好处。我们首先需要的是一个包含一些预训练架构的实现。幸运的是，Hugging
    Face ([https://huggingface.co/transformers](https://huggingface.co/transformers))
    库已经迅速成为研究人员放置BERT最新和最伟大扩展的事实上工具和仓库。要安装它，请运行以下命令：
- en: '[PRE26]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We will use a model called *DistilBERT*,[⁵](#fn53) which is a version of the
    BERT model that has been distilled down into a smaller network with fewer parameters.
    This is just to make the example run faster, because transformer models in general
    are computationally expensive. Part of making BERT-type models successful is training
    large models using dozens of GPUs on hundreds of gigabytes of data. The fact that
    transformers continue to benefit from more layers and larger datasets, and parallelize
    well across many GPUs, to a much greater degree than RNNs is part of what makes
    transformers so powerful. But training a transformer/BERT model from scratch is
    simply too much investment for many people and teams today. The ability to use
    transformers for transfer learning is part of what makes them so relevant to the
    rest of us who don’t have a dozen GPUs available at all times.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个名为 *DistilBERT* 的模型[⁵](#fn53)，这是一个经过蒸馏的BERT模型版本，已经被压缩成一个具有更少参数的小型网络。这样做只是为了使示例运行得更快，因为一般来说，转换器模型在计算上都很昂贵。BERT类型模型成功的一部分是使用数十个GPU在数百GB的数据上训练大型模型。转换器继续从更多层和更大的数据集中受益，并且能够在许多GPU上很好地并行化，这比RNN要大得多，这也是转换器之所以如此强大的原因之一。但是，对于许多人来说，从头开始训练转换器/BERT模型的投资太大。能够使用转换器进行迁移学习是它们对我们这些没有数十个GPU可用的人如此相关的部分。
- en: 'Because using pretrained BERT models is quickly becoming popular, the models
    also come with a handy `from_pretrained` function, which can take different strings
    specifying BERT models trained in different settings. For example, one may have
    been trained on case-sensitive inputs and another trained on case-insensitive
    input. The official documentation([https://huggingface.co/transformers/model_doc/distilbert.html](https://huggingface.co/transformers/model_doc/distilbert.html))
    describes which options are available. We use the insensitive one since we have
    less data (fewer cases means fewer parameters and better performance in small
    datasets):'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用预训练的BERT模型正迅速变得流行，这些模型还附带了一个方便的`from_pretrained`函数，它可以接受不同的字符串，指定在不同设置下训练的BERT模型。例如，一个可能在区分大小写的输入上训练，另一个可能在忽略大小写的输入上训练。官方文档([https://huggingface.co/transformers/model_doc/distilbert.html](https://huggingface.co/transformers/model_doc/distilbert.html))描述了哪些选项可用。我们使用不区分大小写的选项，因为我们有较少的数据（更少的案例意味着更少的参数和在小数据集上的更好性能）：
- en: '[PRE27]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ Loads the DistilBert classes
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载DistilBert类
- en: ❷ Initializes the tokenizer (converts strings to input tensors) and the model
    (input tensors to output tensors)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化tokenizer（将字符串转换为输入张量）和模型（输入张量到输出张量）
- en: Notice that we have not just the `bert_model` but also a *new* `tokenizer`.
    This is so we can use the same encoding process used by the original BERT training
    to convert new strings into inputs for BERT. *We can’t mix and match tokenizers
    between different models*. This is similar to how we used a specific normalizing
    mean and standard deviation when using the pretrained ResNet-18 model. We need
    the initial input to the model for our new target domain to be processed the same
    way as in the original domain. The `tokenizer` object takes raw strings as input
    and performs all the preprocessing that was used when the original model was trained
    in the same manner, making our lives easier.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不仅有`bert_model`，还有一个*新的* `tokenizer`。这样做是为了我们可以使用原始BERT训练中使用的相同编码过程，将新的字符串转换为BERT的输入。*我们不能在不同模型之间混合匹配tokenizer*。这类似于我们使用特定的归一化均值和标准差来使用预训练的ResNet-18模型。我们需要对新目标域的初始输入以与原始域相同的方式进行处理。`tokenizer`对象接受原始字符串作为输入，并执行与原始模型以相同方式训练时使用的所有预处理操作，使我们的生活更加简单。
- en: 'The strategy for implementing our `collate_fn` for the BERT model looks very
    similar to our GRU model. Instead of calling `text_transform`, we call the `tokenizer`
    that Hugging Face provides on the original strings. In particular, there is a
    `batch_encode_plus` function that takes a list of strings and converts it into
    a batch of data ready for processing (with masks, if you so desire). We simply
    add the arguments `return_tensors=pt` to let Hugging Face know we want PyTorch
    tensors (it supports TensorFlow, too) and the `padding=True` flag so the shorter
    sentences are padded to equal length:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 实现BERT模型的`collate_fn`策略看起来与我们的GRU模型非常相似。我们不是调用`text_transform`，而是在原始字符串上调用Hugging
    Face提供的`tokenizer`。特别是有一个`batch_encode_plus`函数，它接受字符串列表并将其转换为处理（如果有需要，带有掩码）的数据批次。我们简单地添加`return_tensors=pt`参数来让Hugging
    Face知道我们想要PyTorch张量（它也支持TensorFlow）和`padding=True`标志，以便较短的句子被填充到相同长度：
- en: '[PRE28]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ The first three lines are the same as before.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 前三条与之前相同。
- en: '❷ Changed: Don’t use the old text_transform; get the raw texts.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 修改：不要使用旧的text_transform；获取原始文本。
- en: '❸ New: Hugging Face encodes a batch of strings for us.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 新增：Hugging Face为我们编码一批字符串。
- en: '❹ Back to old code: stack them up and return the tensors.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 回到旧代码：将它们堆叠起来并返回张量。
- en: ❺ Makes our data loaders with the new collage_fn
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用新的collage_fn创建我们的数据加载器
- en: 13.4.2  Freezing weights with no-grad
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.4.2 使用no-grad冻结权重
- en: The last thing we need is our BERT model with frozen weights. Since the output
    contains padding, we define a `Module` class to figure out the mask for the padding
    and use it as appropriate. BERT gives us an output tensor of shape (*B*,*T*,*D*),
    which we need to reduce to (*B*,*D*) to make a classification prediction. The
    `getMaskByFill` function from chapter 12 gives us the padding mask so that we
    can reuse the attention layers to average over only the valid (not padded) tokens.
    We can access the number of hidden neurons D that BERT is using with the `bert_model.config.dim`
    variable. Each model in Hugging Face has a `.config` variable with various information
    about how the model is configured.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个带有冻结权重的BERT模型。由于输出包含填充，我们定义了一个`Module`类来找出填充的掩码，并适当地使用它。BERT给我们一个形状为
    (*B*,*T*,*D*) 的输出张量，我们需要将其减少到 (*B*,*D*) 以进行分类预测。第12章中的`getMaskByFill`函数给我们填充掩码，这样我们就可以重用注意力层，只对有效的（非填充）标记进行平均。我们可以使用`bert_model.config.dim`变量访问BERT使用的隐藏神经元数D。Hugging
    Face中的每个模型都有一个`.config`变量，其中包含有关模型配置的各种信息。
- en: We also use this as an opportunity to show a different approach to frozen weights.
    Instead of manually setting `requires_grad=False` for each parameter, we can use
    the `with torch.no_grad():` context instead. It has the same effect, computing
    gradients for any needed backpropagation but forgetting them immediately so they
    are not used during the gradient update. This is convenient if we want to make
    the freezing adaptive or make the code more explicit that gradients will not be
    used for a portion of it. The downside to this approach is that it’s difficult
    to implement models with a mix of warm and frozen layers.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也利用这个机会展示一种不同的冻结权重的方法。我们不需要手动为每个参数设置 `requires_grad=False`，而是可以使用 `with torch.no_grad():`
    上下文。它具有相同的效果，为任何需要的反向传播计算梯度，但立即忘记它们，因此在梯度更新期间不会使用。如果我们想使冻结自适应或使代码更明确地表明梯度将不会用于其中的一部分，这将很方便。这种方法的缺点是，实现具有混合热和冻结层的模型比较困难。
- en: 'Here’s the code:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码：
- en: '[PRE29]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ Our new class for frozen training of BERT models
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们为BERT模型的冻结训练创建的新类
- en: ❷ We get a tensor of shape (B, T, D) from BERT, so we define a few of our own
    layers to get from (B, T, D) to a prediction of shape (B, classes)
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们从BERT得到一个形状为 (B, T, D) 的张量，因此我们定义了一些自己的层，从 (B, T, D) 到形状为 (B, classes) 的预测
- en: ❸ Attention to get down to (B, D) shape
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 注意力降低到 (B, D) 形状
- en: ❹ Does a little feature extraction
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 进行一些特征提取
- en: ❺ Makes a prediction about the class
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 对类别做出预测
- en: ❻ Input is (B, T).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 输入是 (B, T)。
- en: ❼ This with no_grad() does the freezing.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用no_grad()进行冻结。
- en: ❽ Hugging Face returns a tuple, so unpack it! (B, T, D)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ Hugging Face 返回一个元组，所以解包它！ (B, T, D)
- en: ❾ Computes the average embedding
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 计算平均嵌入
- en: ❿ Applies the attention
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 应用注意力
- en: ⓫ Makes predictions and returns
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 进行预测并返回
- en: ⓬ Builds the classifier
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 构建分类器
- en: 'As before, we can train this BERT-based classifier using our handy `train_network`
    function:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们可以使用我们方便的`train_network`函数来训练这个基于BERT的分类器：
- en: '[PRE30]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![](../Images/CH13_UN10_Raff.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH13_UN10_Raff.png)'
- en: Looking at the results, we can see that the GRU is learning, but very slowly.
    The GRU tops out at ≈ 40% accuracy, less than half of the 92% it can achieve on
    the full training set. Our frozen BERT model gets ≈ 84%, which is a significant
    improvement. There is a higher price in this case, though. As we mentioned earlier,
    BERT-style models tend to be very large and thus computationally expensive. Training
    and applying our BERT classifier was about 10× slower than the GRU.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 查看结果，我们可以看到GRU正在学习，但速度非常慢。GRU的最高准确率约为40%，不到它在完整训练集上可以达到的92%的一半。我们的冻结BERT模型达到了约84%，这是一个显著的提升。然而，这种情况的代价也更高。正如我们之前提到的，BERT风格的模型通常非常大，因此计算成本很高。训练和应用我们的BERT分类器比GRU慢了大约10倍。
- en: From a model accuracy perspective, it is clearly a net win since the GRU will
    never get to 84% accuracy on its own. The BERT-based model may be too slow to
    apply in practice, though. It depends on the resources available and the specifics
    of the problem at hand. This is an important tradeoff to be aware of, and it is
    not uncommon when using the pretraining approach. Intrinsically, we want to use
    the pretrained model because it was already trained on a larger dataset, but that
    also means the models become larger to maximize accuracy.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型准确率的角度来看，这显然是一个净胜，因为 GRU 永远无法单独达到 84% 的准确率。然而，基于 BERT 的模型可能在实际应用中太慢了。这取决于可用的资源和问题的具体细节。这是一个需要了解的重要权衡，在使用预训练方法时并不罕见。本质上，我们想要使用预训练模型，因为它已经在更大的数据集上进行了训练，但这也意味着模型变得更大，以最大化准确率。
- en: In my personal experience, I’ve found that pretrained transformers often start
    to lose their advantage as you build up the labeled dataset for your problem.
    Whereas pretrained CNNs almost always outperform those trained from scratch, I
    often find that with transformers, there isn’t such a clear-cut, consistent result.
    Sometimes they are better, sometimes not. This is especially true compared to
    non-deep approaches like Lasso-penalized logistic regression, which performs very
    competitively on many text datasets.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在我个人的经验中，我发现随着你为你的问题建立标记数据集，预训练的转换器通常会开始失去它们的优势。而预训练的 CNN 几乎总是优于从头开始训练的 CNN，我经常发现，对于转换器来说，并没有如此明确、一致的结果。有时它们更好，有时则不然。这与非深度方法（如
    Lasso 惩罚逻辑回归）相比尤其如此，它在许多文本数据集上表现出很强的竞争力。
- en: Still, using a pretrained model for textual data is now a possibility. It will
    likely improve with time and is a powerful tool when you have very few training
    samples. Keep it in your toolbox, but explore the alternatives if you have a text-based
    problem.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，现在使用预训练模型进行文本数据是可能的。它可能会随着时间的推移而改进，并且当训练样本非常少时是一个强大的工具。将其保留在你的工具箱中，但如果你有一个基于文本的问题，探索替代方案。
- en: Exercises
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Share and discuss your solutions on the Manning online platform at Inside Deep
    Learning Exercises ([https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945)).
    Once you submit your own answers, you will be able to see the solutions submitted
    by other readers, and see which ones the author judges to be the best.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Manning 在线平台 Inside Deep Learning Exercises ([https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945))
    上分享和讨论你的解决方案。一旦你提交了自己的答案，你将能够看到其他读者提交的解决方案，并看到作者认为哪些是最好的。
- en: Use the `dir` command to explore the subcomponents of the PyTorch `resnet18`
    model, and write your own function `def warmFrozenResnet18(num_frozen)` that freezes
    only the first `num_frozen` convolutional layers in the network.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `dir` 命令来探索 PyTorch `resnet18` 模型的子组件，并编写你自己的函数 `def warmFrozenResnet18(num_frozen)`，该函数只冻结网络中的前
    `num_frozen` 个卷积层。
- en: Using your `warmFrozenResnet18` function, explore the tradeoffs between the
    degree of warm versus frozen layers when using *N* = 256, *N* = 1, 024, and *N*
    = 8, 192 labeled training samples.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你的 `warmFrozenResnet18` 函数，在 *N* = 256，*N* = 1,024 和 *N* = 8,192 个标记的训练样本中，探索暖层与冻结层程度之间的权衡。
- en: Repeat the previous two exercises, but use the `MobileNet` class instead.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复前两个练习，但使用 `MobileNet` 类。
- en: 'Go back to chapter 8, where we trained Faster R-CNN to detect the location
    of MNIST digits. Do your own pretraining of the backbone network as an MNIST classifier,
    and then use that pretrained backbone to train Faster R-CNN. Test the results
    on a number of images, and describe any differences you see in the results. *Note:*
    This will be easier if you make the `backbone` have two parts: one that is the
    larger feature processing subnetwork containing only convolutional layers and
    pooling, and a second prediction subnetwork that does any final pooling (optional),
    flattening, and prediction of the class label. Then you can use just the feature
    processing subnetwork for the Faster R-CNN.'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回到第 8 章，在那里我们训练了 Faster R-CNN 来检测 MNIST 数字的位置。作为 MNIST 分类器进行自己的预训练，然后使用该预训练的主干网络来训练
    Faster R-CNN。在多张图像上测试结果，并描述你看到的结果中的任何差异。*注意:* 如果你使 `backbone` 有两个部分：一个只包含卷积层和池化的较大特征处理子网络，另一个进行任何最终池化（可选）、展平和类别标签预测的第二个预测子网络，这将更容易。然后你可以只为
    Faster R-CNN 使用特征处理子网络。
- en: You can use autoencoders to do unsupervised pretraining when you have a lot
    of data but most of it is unlabeled. Write a denoising autoencoder for the cats
    versus dogs problem that is trained on the entire dataset, and then use the encoder
    portion as the warm start for a classifier. You can think of the encoder feature
    as the processing backbone, and you need to add a prediction subnetwork on top.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你拥有大量数据但其中大部分未标记时，可以使用自动编码器进行无监督预训练。为猫狗问题编写一个去噪自动编码器，该编码器在全部数据集上训练，然后将编码器部分作为分类器的预热启动。你可以将编码器特征视为处理骨干，并需要在上面添加一个预测子网络。
- en: Hugging Face has special classes to make it easier to use pretrained models.
    Look up the documentation for the `DistilBertForSequenceClassification` class,
    and replace our approach for the AG News dataset with the Hugging Face API’s built-in
    approach. How do they compare?
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hugging Face有特殊的类来简化使用预训练模型的过程。查阅`DistilBertForSequenceClassification`类的文档，并用Hugging
    Face API的内置方法替换我们的AG News数据集的方法。它们是如何比较的？
- en: Summary
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: You can transfer knowledge by reusing the weights of a network that has already
    been trained on one dataset, applying them to a new dataset, and performing surgery
    on the final layer of the network to make it match your problem.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过重用已经在某个数据集上训练好的网络的权重，将它们应用于新的数据集，并对网络的最后一层进行手术以使其匹配你的问题来迁移知识。
- en: The weights of a pretrained model can be warm (allowed to change) or frozen
    (kept constant) during training, providing different benefits when you have more
    or less data, respectively.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练模型的权重在训练过程中可以是温暖的（允许改变）或冻结的（保持不变），在数据量多或少时分别提供不同的好处。
- en: The benefits of transfer learning are greatest when we have very little labeled
    data.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们拥有的标记数据非常少时，迁移学习的优势最为显著。
- en: Pretraining for convolutional networks and computer vision problems is extremely
    effective, whereas pretraining for text problems is still useful but has more
    tradeoffs due to differences in model size and compute time.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于卷积网络和计算机视觉问题进行预训练非常有效，而对于文本问题进行预训练仍然有用，但由于模型大小和计算时间的差异，存在更多的权衡。
- en: Pretraining for text is best done with transformer-type models trained on large
    corpora.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于文本的预训练，最好使用在大语料库上训练的Transformer类型模型。
- en: '* * *'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '***'
- en: ¹ I’ve never had a surgery that quite worked that way, and I think I’m OK with
    that. Please note I am a PhD, not an MD, so my knowledge of proper surgery etiquette
    and how to chop things off safely is limited.[↩](#fnref49)
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 我从未有过完全成功的手术，我认为我对此很满意。请注意，我是一个博士，而不是医学博士，因此我对适当的手术礼仪和如何安全地切除东西的了解有限。[↩](#fnref49)
- en: ² This is often described as having an oracle that can magically deliver to
    you the *perfect* solution.[↩](#fnref50)
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ² 这通常被描述为拥有一个可以神奇地为你提供*完美*解决方案的先知。[↩](#fnref50)
- en: '³ My oversimplification is very oversimplified, but the intuition is a good
    one to have. It is an intuition that will hold for most machine learning but not
    always for deep learning. It is very hard to prove things about deep learning:
    much progress has been made, but when D becomes large, you should always second-guess
    your intuitions. It is difficult to reason about such high-dimensional spaces.
    For more discussion of the weirdness of learning the parameters of neural networks,
    I encourage you to look at C. Zhang et al., “Understanding deep learning requires
    rethinking generalization,” International Conference on Learning Representations,
    2017.[↩](#fnref51)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ³ 我过于简化的说法非常简化，但直觉是很好的。这是一个对于大多数机器学习都适用的直觉，但并不总是适用于深度学习。证明深度学习中的事情非常困难：已经取得了很大进展，但当D变得很大时，你应该总是对直觉进行二次怀疑。对这些高维空间进行推理是困难的。关于学习神经网络参数的奇怪之处，我鼓励你阅读C.
    Zhang等人，“理解深度学习需要重新思考泛化”，国际学习表示会议，2017。[↩](#fnref51)
- en: '⁴ J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of
    deep bidirectional transformers for language understanding,” *Proceedings of the
    2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pp.
    4171–4186, 2019.[↩](#fnref52)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴ J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova，“BERT：用于语言理解的深度双向Transformer的预训练”，*2019年北美计算语言学协会分会会议：人机语言技术会议论文集（长篇和短篇论文）*，第4171-4186页，2019。[↩](#fnref52)
- en: '⁵ V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT, a distilled version
    of BERT : smaller, faster, cheaper and lighter,” ArXiv e-prints, pp. 2–6, 2019,
    [https://arxiv.org/abs/1910.01108](https://arxiv.org/abs/1910.01108).[↩](#fnref53)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵ V. Sanh, L. Debut, J. Chaumond, 和 T. Wolf, “DistilBERT，BERT的精简版：更小、更快、更便宜、更轻便，”
    ArXiv e-prints，第 2-6 页，2019年，[https://arxiv.org/abs/1910.01108](https://arxiv.org/abs/1910.01108)。[↩](#fnref53)
