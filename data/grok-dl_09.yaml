- en: 'Chapter 10\. Neural learning about edges and corners: intro to convolutional
    neural networks'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第10章。关于边缘和角落的神经网络学习：卷积神经网络简介
- en: '**In this chapter**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章内容**'
- en: Reusing weights in multiple places
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个地方重用权重
- en: The convolutional layer
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层
- en: “The pooling operation used in convolutional neural networks is a big mistake,
    and the fact that it works so well is a disaster.”
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “卷积神经网络中使用的池化操作是一个大错误，而它之所以能如此有效，简直是一场灾难。”
- en: ''
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Geoffrey Hinton, from “Ask Me Anything” on Reddit*'
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*来自Reddit的“问我任何问题”活动，作者：杰弗里·辛顿*'
- en: Reusing weights in multiple places
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在多个地方重用权重
- en: If you need to detect the same feature in multiple places, use the same weights!
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如果你需要在多个地方检测到相同的特征，请使用相同的权重！
- en: The greatest challenge in neural networks is that of overfitting, when a neural
    network memorizes a dataset instead of learning useful abstractions that generalize
    to unseen data. In other words, the neural network learns to predict based on
    noise in the dataset as opposed to relying on the fundamental signal (remember
    the analogy about a fork embedded in clay?).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中最大的挑战是过拟合，当神经网络记住数据集而不是学习有用的抽象，这些抽象可以推广到未见过的数据。换句话说，神经网络学习根据数据集中的噪声进行预测，而不是依赖于基本信号（还记得关于泥土中嵌入叉子的类比吗？）。
- en: '![](Images/f0178-01.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0178-01.jpg)'
- en: 'Overfitting is often caused by having more parameters than necessary to learn
    a specific dataset. In this case, the network has so many parameters that it can
    memorize every fine-grained detail in the training dataset (neural network: “Ah.
    I see we have image number 363 again. This was the number 2.”) instead of learning
    high-level abstractions (neural network: “Hmm, it’s got a swooping top, a swirl
    at the bottom left, and a tail on the right; it must be a 2.”). When neural networks
    have lots of parameters but not very many training examples, overfitting is difficult
    to avoid.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合通常是由于学习特定数据集所需的参数过多。在这种情况下，网络有如此多的参数，以至于它可以记住训练数据集中每个细微的细节（神经网络：“啊，我看到我们又有图像编号363了。这是编号2。”），而不是学习高级抽象（神经网络：“嗯，它有一个弯曲的顶部，左下角有一个漩涡，右边有一个尾巴；它一定是2。”）。当神经网络有很多参数但训练示例不多时，过拟合很难避免。
- en: We covered this topic extensively in [chapter 8](kindle_split_016.xhtml#ch08),
    when we looked at regularization as a means of countering overfitting. But regularization
    isn’t the only technique (or even the most ideal technique) to prevent overfitting.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第8章](kindle_split_016.xhtml#ch08)中详细介绍了这个主题，当时我们探讨了正则化作为对抗过拟合的手段。但正则化并不是唯一的技术（甚至不是最理想的技术）来防止过拟合。
- en: As I mentioned, overfitting is concerned with the ratio between the number of
    weights in the model and the number of datapoints it has to learn those weights.
    Thus, there’s a better method to counter overfitting. When possible, it’s preferable
    to use something loosely defined as *structure*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我提到的，过拟合关注的是模型中权重数量与学习这些权重所需的数据点数量之间的比率。因此，有一种更好的方法来对抗过拟合。当可能时，最好使用某种松散定义的*结构*。
- en: Structure is when you selectively choose to reuse weights for multiple purposes
    in a neural network because we believe the same pattern needs to be detected in
    multiple places. As you’ll see, this can significantly reduce overfitting and
    lead to much more accurate models, because it reduces the weight-to-data ratio.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 结构是指你选择性地在神经网络中选择重用权重以实现多个目的，因为我们相信相同的模式需要在多个地方被检测到。正如你将看到的，这可以显著减少过拟合，并导致更精确的模型，因为它减少了权重与数据的比率。
- en: '![](Images/f0178-02.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0178-02.jpg)'
- en: But whereas normally removing parameters makes the model less expressive (less
    able to learn patterns), if you’re clever in where you reuse weights, the model
    can be equally expressive but more robust to overfitting. Perhaps surprisingly,
    this technique also tends to make the model smaller (because there are fewer actual
    parameters to store). The most famous and widely used structure in neural networks
    is called a *convolution*, and when used as a layer it’s called a *convolutional
    layer*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，通常移除参数会使模型的表达能力降低（更难学习模式），如果你在重用权重的地方足够聪明，模型可以同样具有表达能力，但更能抵抗过拟合。也许令人惊讶的是，这种技术也往往使模型变得更小（因为需要存储的实际参数更少）。神经网络中最著名和最广泛使用的结构称为*卷积*，当用作层时，称为*卷积层*。
- en: The convolutional layer
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积层
- en: Lots of very small linear layers are reused in every position, in- nstead of
    a single big one
  id: totrans-18
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在每个位置都重复使用很多非常小的线性层，而不是一个单一的大的层
- en: The core idea behind a convolutional layer is that instead of having a large,
    dense linear layer with a connection from every input to every output, you instead
    have lots of very small linear layers, usually with fewer than 25 inputs and a
    single output, which you use in every input position. Each mini-layer is called
    a convolutional *kernel*, but it’s really nothing more than a baby linear layer
    with a small number of inputs and a single output.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层背后的核心思想是，而不是有一个大型的、密集的线性层，其中每个输入都连接到每个输出，你反而有很多非常小的线性层，通常少于25个输入和一个输出，你可以在每个输入位置使用这些层。每个微型层被称为卷积核，但实际上它不过是一个具有少量输入和一个输出的婴儿线性层。
- en: '![](Images/f0179-01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0179-01.jpg)'
- en: Shown here is a single 3 × 3 convolutional kernel. It will predict in its current
    location, move one pixel to the right, then predict again, move another pixel
    to the right, and so on. Once it has scanned all the way across the image, it
    will move down a single pixel and scan back to the left, repeating until it has
    made a prediction in every possible position within the image. The result will
    be a smaller square of kernel predictions, which are used as input to the next
    layer. Convolutional layers usually have many kernels.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了一个单一的3 × 3卷积核。它将在当前位置预测，然后向右移动一个像素，再次预测，然后再次向右移动，以此类推。一旦它扫描了整个图像，它将向下移动一个像素，然后向左扫描，重复直到它在图像中的每个可能位置都进行了预测。结果将是一个较小的核预测正方形，这些预测被用作下一层的输入。卷积层通常有很多核。
- en: At bottom-right are four different convolutional kernels processing the same
    8 × 8 image of a 2\. Each kernel results in a 6 × 6 prediction matrix. The result
    of the convolutional layer with four 3 × 3 kernels is four 6 × 6 prediction matrices.
    You can either sum these matrices elementwise (sum pooling), take the mean elementwise
    (mean pooling), or compute the elementwise maximum value (max pooling).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在右下角有四个不同的卷积核处理同一个2的8 × 8图像。每个核产生一个6 × 6预测矩阵。使用四个3 × 3核的卷积层的结果是四个6 × 6预测矩阵。你可以将这些矩阵逐元素相加（逐元素求和池化），逐元素取平均值（平均池化），或者计算逐元素的最大值（最大池化）。
- en: 'The last version turns out to be the most popular: for each position, look
    into each of the four kernel’s outputs, find the max, and copy it into a final
    6 × 6 matrix as pictured at upper-right of this page. This final matrix (and only
    this matrix) is then forward propagated into the next layers.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个版本证明是最受欢迎的：对于每个位置，查看每个核的输出，找到最大值，并将其复制到页面右上角的最终6 × 6矩阵中。这个最终矩阵（只有这个矩阵）然后被前向传播到下一层。
- en: There are a few things to notice in these figures. First, the bottom-right kernel
    forward propagates a 1 only if it’s focused on a horizontal line segment. The
    bottom-left kernel forward propagates a 1 only if it’s focused on a diagonal line
    pointing upward and to the right. Finally, the bottom-right kernel didn’t identify
    any patterns that it was trained to predict.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些图中需要注意几个方面。首先，右下角的核只在前向传播1，前提是它专注于一条水平线段。左下角的核只在前向传播1，前提是它专注于一个向上向右的对角线。最后，右下角的核没有识别出它被训练去预测的任何模式。
- en: It’s important to realize that this technique allows each kernel to learn a
    particular pattern and then search for the existence of that pattern somewhere
    in the image. A single, small set of weights can train over a much larger set
    of training examples, because even though the dataset hasn’t changed, each mini-kernel
    is forward propagated multiple times on multiple segments of data, thus changing
    the ratio of weights to datapoints on which those weights are being trained. This
    has a powerful impact on the network, drastically reducing its ability to overfit
    to training data and increasing its ability to generalize.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要认识到，这项技术允许每个核学习特定的模式，然后在图像的某个地方搜索该模式的存在。一个小的权重集可以训练一个更大的训练示例集，因为尽管数据集没有变化，每个微型核在多个数据段上多次前向传播，从而改变了权重与训练数据点的比率。这对网络有强大的影响，极大地减少了它对训练数据的过度拟合能力，并增加了它的一般化能力。
- en: '![](Images/f0180-01.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0180-01.jpg)'
- en: A simple implementation in NumPy
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NumPy中的简单实现
- en: Just think mini-linear layers, and you already know what you need to know
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 只需想想微型线性层，你就已经知道你需要知道的内容了
- en: 'Let’s start with forward propagation. This method shows how to select a subregion
    in a batch of images in NumPy. Note that it selects the same subregion for the
    entire batch:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从前向传播开始。这个方法展示了如何在NumPy中从一批图像中选择一个子区域。请注意，它为整个批次选择了相同的子区域：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, let’s see how this method is used. Because it selects a subsection of
    a batch of input images, you need to call it multiple times (on every location
    within the image). Such a `for` loop looks something like this:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这个方法是如何使用的。因为它选择了一批输入图像的一个子集，所以你需要多次调用它（在图像的每个位置上）。这样的`for`循环看起来可能像这样：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this code, `layer_0` is a batch of images 28 × 28 in shape. The `for` loop
    iterates through every (`kernel_rows` × `kernel_cols`) subregion in the images
    and puts them into a list called `sects`. This list of sections is then concatenated
    and reshaped in a peculiar way.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，`layer_0`是一个形状为28 × 28的图像批次。`for`循环遍历图像中的每个（`kernel_rows` × `kernel_cols`）子区域，并将它们放入一个名为`sects`的列表中。然后，这个部分列表以独特的方式连接和重塑。
- en: Pretend (for now) that each individual subregion is its own image. Thus, if
    you had a batch size of 8 images, and 100 subregions per image, you’d pretend
    it was a batch size of 800 smaller images. Forward propagating them through a
    linear layer with one output neuron is the same as predicting that linear layer
    over every subregion in every batch (pause and make sure you get this).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: （暂时）假设每个单独的子区域都是它自己的图像。因此，如果你有一个包含8张图像的批次，并且每张图像有100个子区域，那么你就可以假装这是一个包含800张更小图像的批次。通过一个只有一个输出神经元的线性层前向传播它们，就相当于预测每个批次中每个子区域的线性层（暂停并确保你理解了这一点）。
- en: 'If you instead forward propagate using a linear layer with *n* output neurons,
    it will generate the outputs that are the same as predicting *n* linear layers
    (kernels) in every input position of the image. You do it this way because it
    makes the code both simpler and faster:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用具有*n*个输出神经元的线性层进行前向传播，它将生成与在每个输入位置的图像中预测*n*个线性层（核）相同的输出。你这样做是因为这样做可以使代码更简单、更快：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following listing shows the entire NumPy implementation:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了整个NumPy实现：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see, swapping out the first layer from the network in [chapter 9](kindle_split_017.xhtml#ch09)
    with a convolutional layer gives another few percentage points in error reduction.
    The output of the convolutional layer (`kernel_output`) is itself also a series
    of two-dimensional images (the output of each kernel in each input position).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，将[第9章](kindle_split_017.xhtml#ch09)中的网络的第一层替换为卷积层，可以进一步减少几个百分点的错误。卷积层的输出（`kernel_output`）本身也是一系列二维图像（每个输入位置中每个核的输出）。
- en: Most uses of convolutional layers stack multiple layers on top of each other,
    such that each convolutional layer treats the previous as an input image. (Feel
    free to do this as a personal project; it will increase accuracy further.)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数卷积层的应用都是将多层堆叠在一起，使得每个卷积层将前一层作为输入图像处理。（你可以自由地将这作为一个个人项目来做；这将进一步提高准确性。）
- en: Stacked convolutional layers are one of the main developments that allowed for
    very deep neural networks (and, by extension, the popularization of the phrase
    *deep learning*). It can’t be overstressed that this invention was a landmark
    moment for the field; without it, we might still be in the previous AI winter
    even at the time of writing.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠卷积层是允许非常深的神经网络（以及由此产生的“深度学习”这一术语的普及）的主要发展之一。不能过分强调这一发明是该领域的里程碑时刻；没有它，我们可能仍然处于写作时的上一个AI冬天。
- en: Summary
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Reusing weights is one of the most important innovations in deep learning
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重复使用权重是深度学习中最重要的一项创新。
- en: Convolutional neural networks are a more general development than you might
    realize. The notion of reusing weights to increase accuracy is hugely important
    and has an intuitive basis. Consider what you need to understand in order to detect
    that a cat is in an image. You first need to understand colors, then lines and
    edges, corners and small shapes, and eventually the combination of such lower-level
    features that correspond to a cat. Presumably, neural networks also need to learn
    about these lower-level features (like lines and edges), and the intelligence
    for detecting lines and edges is learned in the weights.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络是一种比你所意识到的更一般的发展。重用权重以提高准确性的概念非常重要，并且有直观的基础。考虑一下你为了检测图像中是否有猫需要理解什么。首先，你需要理解颜色，然后是线条和边缘，角落和小的形状，最终是这些低级特征的组合，这些特征对应于猫。假设神经网络也需要学习这些低级特征（如线条和边缘），检测线条和边缘的智能就体现在权重中。
- en: 'But if you use different weights to analyze different parts of an image, each
    section of weights has to independently learn what a line is. Why? Well, if one
    set of weights looking at one part of an image learns what a line is, there’s
    no reason to think that another section of weights would somehow have the ability
    to use that information: it’s in a different part of the network.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你使用不同的权重来分析图像的不同部分，每个权重的部分都必须独立地学习线条是什么。为什么？好吧，如果一组权重观察图像的一部分学会了线条是什么，就没有理由认为另一部分的权重会以某种方式拥有使用该信息的能力：它位于网络的另一部分。
- en: 'Convolutions are about taking advantage of a property of learning. Occasionally,
    you need to use the same idea or piece of intelligence in multiple places; and
    if that’s the case, you should attempt to use the same weights in those locations.
    This brings us to one of the most important ideas in this book. If you don’t learn
    anything else, learn this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是利用学习的一个特性。偶尔，，你需要多次使用相同的思想或智能；如果是这样，你应该尝试在这些位置使用相同的权重。这带我们到了本书中最重要的思想之一。如果你不学习其他任何东西，请学习这一点：
- en: '|  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**The structure trick**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**结构技巧**'
- en: When a neural network needs to use the same idea in multiple places, endeavor
    to use the same weights in both places. This will make those weights more intelligent
    by giving them more samples to learn from, increasing generalization.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当神经网络需要在多个地方使用相同的思想时，应努力在这两个地方使用相同的权重。这将通过提供更多样本供学习，使这些权重变得更加智能，从而提高泛化能力。
- en: '|  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Many of the biggest developments in deep learning over the past five years (some
    before) are iterations of this idea. Convolutions, recurrent neural networks (RNNs),
    word embeddings, and the recently published capsule networks can all be viewed
    through this lens. When you know a network will need the same idea in multiple
    places, force it to use the same weights in those places. I fully expect that
    more deep learning discoveries will continue to be based on this idea, because
    it’s challenging to discover new, higher-level abstract ideas that neural networks
    could use repeatedly throughout their architecture.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 过去五年（有些在之前）中深度学习领域最大的发展（其中一些在之前）都是这一想法的迭代。卷积、循环神经网络（RNNs）、词嵌入以及最近发布的胶囊网络都可以通过这个角度来理解。当你知道一个网络将在多个地方需要相同的思想时，强迫它在那些地方使用相同的权重。我完全预期更多的深度学习发现将继续基于这一想法，因为发现新的、更高层次的抽象思想，神经网络可以在其架构的各个部分重复使用，是具有挑战性的。
