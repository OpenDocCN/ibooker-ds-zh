- en: 13 Measuring text similarities
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13 测量文本相似度
- en: This section covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖
- en: What is natural language processing?
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是自然语言处理？
- en: Comparing texts based on word overlap
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据词重叠比较文本
- en: Comparing texts using one-dimensional arrays called vectors
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用称为向量的单维数组比较文本
- en: Comparing texts using two-dimensional arrays called matrices
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用称为矩阵的二维数组比较文本
- en: Efficient matrix computation using NumPy
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用NumPy进行高效的矩阵计算
- en: Rapid text analysis can save lives. Let’s consider a real-world incident when
    US soldiers stormed a terrorist compound. In the compound, they discovered a computer
    containing terabytes of archived data. The data included documents, text messages,
    and emails pertaining to terrorist activities. The documents were too numerous
    to be read by any single human being. Fortunately, the soldiers were equipped
    with special software that could perform very fast text analysis. The software
    allowed the soldiers to process all of the text data without even having to leave
    the compound. The onsite analysis immediately revealed an active terrorist plot
    in a nearby neighborhood. The soldiers instantly responded to the plot and prevented
    a terrorist attack.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 快速文本分析可以挽救生命。让我们考虑一个现实世界的案例，当美国士兵突袭恐怖分子营地时。在营地中，他们发现了一台包含数太字节存档数据的电脑。这些数据包括与恐怖活动相关的文件、短信和电子邮件。文件太多，任何一个人都无法阅读。幸运的是，士兵们配备了能够进行非常快速文本分析的特殊软件。该软件允许士兵处理所有文本数据，甚至无需离开营地。现场分析立即揭示了一个附近社区中的活跃恐怖分子阴谋。士兵们立即对阴谋做出反应，防止了恐怖袭击。
- en: 'This swift defensive response would not have been possible without *natural
    language processing* (NLP) techniques. NLP is a branch of data science that focuses
    on speedy text analysis. Typically, NLP is applied to very large text datasets.
    NLP use cases are numerous and diverse and include the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 没有自然语言处理（NLP）技术，这种迅速的防御性反应是不可能的。NLP是数据科学的一个分支，专注于快速文本分析。通常，NLP应用于非常大的文本数据集。NLP用例众多且多样化，包括以下内容：
- en: Corporate monitoring of social media posts to measure the public’s sentiment
    toward a company’s brand
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对社交媒体帖子进行公司监控以衡量公众对公司品牌的情感
- en: Analyzing transcribed call center conversations to monitor common customer complaints
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析转录的客户服务中心对话以监控常见的客户投诉
- en: Matching people on dating sites based on written descriptions of shared interests
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据共享兴趣的书面描述在约会网站上匹配人员
- en: Processing written doctors’ notes to ensure proper patient diagnosis
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理书面医生笔记以确保正确的患者诊断
- en: These use cases depend on fast analysis. Delayed signal extraction could be
    costly. Unfortunately, the direct handling of text is an inherently slow process.
    Most computational techniques are optimized for numbers, not text. Consequently,
    NLP methods depend on a conversion from pure text to a numeric representation.
    Once all words and sentences have been replaced with numbers, the data can be
    analyzed very rapidly.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些用例依赖于快速分析。延迟信号提取可能会造成损失。不幸的是，直接处理文本是一个本质上缓慢的过程。大多数计算技术都是针对数字而不是文本进行优化的。因此，NLP方法依赖于将纯文本转换为数值表示的转换。一旦所有单词和句子都被替换成数字，数据就可以非常快速地进行分析。
- en: 'In this section, we focus on a basic NLP problem: measuring the similarity
    between two texts. We will quickly discover a feasible solution that is not computationally
    efficient. We will then explore a series of numerical techniques for rapidly computing
    text similarities. These computations will require us to transform our input texts
    into 2D numeric tables for full efficiency.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们关注一个基本的NLP问题：测量两个文本之间的相似度。我们将很快发现一个可行的解决方案，但它不是计算效率高的。然后我们将探索一系列用于快速计算文本相似性的数值技术。这些计算需要我们将输入文本转换成二维数值表以实现完全效率。
- en: 13.1 Simple text comparison
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.1 简单文本比较
- en: 'Many NLP tasks depend on the analysis of similarities and differences between
    texts. Suppose we want to compare three simple texts:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 许多NLP任务依赖于分析文本之间的相似性和差异性。假设我们想要比较三个简单的文本：
- en: '`text1`—*She sells seashells by the seashore.*'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text1`—*她在海边卖贝壳。*'
- en: '`text2`—*“Seashells! The seashells are on sale! By the seashore.”*'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text2`—*“贝壳！贝壳正在打折！在海边。”*'
- en: '`text3`—*She sells 3 seashells to John, who lives by the lake.*'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text3`—*她向住在湖边的约翰卖3个贝壳。*'
- en: Our goal is to determine whether `text1` is more similar to `text2` or to `text3`.
    We start by assigning the texts to three variables.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是确定`text1`是否比`text2`或`text3`更相似。我们首先将文本分配给三个变量。
- en: Listing 13.1 Assigning texts to variables
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.1 将文本分配给变量
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we need to quantify the differences between texts. One basic approach is
    to simply count the words shared between each pair of texts. This requires us
    to split each text into a list of words. Text splitting in Python can be carried
    out using the built-in string `split` method.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要量化文本之间的差异。一种基本的方法是简单地计算每对文本之间共享的单词数量。这需要我们将每个文本分割成单词列表。在Python中，可以使用内置的字符串`split`方法进行文本分割。
- en: Note The process of splitting text into individual words is commonly called
    *tokenization*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：将文本分割成单个单词的过程通常称为*分词*。
- en: Listing 13.2 Splitting texts into words
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.2 将文本分割成单词
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Even though we’ve split the texts, an accurate word comparison is not immediately
    possible for the following reasons:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经分割了文本，但由于以下原因，准确地进行单词比较并不立即可行：
- en: '*Capitalization inconsistency*—The words *she* and *seashells* are capitalized
    in some texts but not others, making a direct comparison difficult.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*大小写不一致性*——某些文本中的*she*和*seashells*单词被大写，而其他文本中没有，这使得直接比较变得困难。'
- en: '*Punctuation inconsistency*—For example, an exclamation point and a quotation
    mark are attached to *seashells* in `text2` but not in the other texts.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*标点不一致性*——例如，在`text2`中，感叹号和引号附着在*seashells*上，但其他文本中没有。'
- en: We can eliminate the capitalization inconsistency by calling the built-in `lower`
    string method, which converts a string to lowercase. Furthermore, we can strip
    out punctuation from a `word` string by calling `word.replace(punctuation, ' ')`,
    where `punctuation` is set to `'!'` or `'"'`. Let’s use these built-in string
    methods to eliminate all inconsistencies. We define a `simplify_text` function,
    which converts text to lowercase and removes all common punctuation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调用内置的`lower`字符串方法来消除大小写不一致性，该方法将字符串转换为小写。此外，我们可以通过调用`word.replace(punctuation,
    ' ')`来从`word`字符串中删除标点符号，其中`punctuation`设置为`'!'`或`'"'`。让我们使用这些内置的字符串方法来消除所有不一致性。我们定义了一个`simplify_text`函数，该函数将文本转换为小写并删除所有常见标点符号。
- en: Listing 13.3 Removing case sensitivity and punctuation
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.3 移除大小写敏感性和标点符号
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Strips out common punctuation from a string and converts the string to lowercase
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从字符串中删除常见的标点符号并将其转换为小写
- en: As a reminder, our immediate goal is to
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，我们的直接目标是
- en: Count all unique words in `text1` that are also present in `text2`.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算在`text1`中出现的所有唯一单词，这些单词也存在于`text2`中。
- en: Count all unique words in `text1` that are also present in `text3`.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算在`text1`中出现的所有唯一单词，这些单词也存在于`text3`中。
- en: Use the counts to determine whether `text2` or `text3` is more similar to `text1`.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用计数来确定`text2`或`text3`与`text1`的相似度更高。
- en: Currently, we’re just interested in comparing unique words. Therefore, duplicate
    words (like *seashore*, which appears twice in `text2`) will only be counted once.
    Thus, we can eliminate all duplicate words by converting each word list into a
    set.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们只对比较唯一单词感兴趣。因此，重复的单词（如`text2`中出现的两次的*seashore*）只会计算一次。因此，我们可以通过将每个单词列表转换为集合来消除所有重复的单词。
- en: Listing 13.4 Converting word lists to sets
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.4 将单词列表转换为集合
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Given two Python sets `set_a` and `set_b`, we can extract all overlapping elements
    by running `set_a & set_b`. Let’s use the `&` operator to count overlapping words
    between text pairs `(text1, text2)` and `(text1, text3)`.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 给定两个Python集合`set_a`和`set_b`，我们可以通过运行`set_a & set_b`来提取所有重叠元素。让我们使用`&`运算符来计算文本对`(text1,
    text2)`和`(text1, text3)`之间的重叠单词。
- en: Note Formally, the set of overlapping elements is called the *intersection*
    of two sets.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：形式上，重叠元素集称为两个集合的*交集*。
- en: Listing 13.5 Extracting overlapping words between two texts
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.5 提取两个文本之间的重叠单词
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Texts 1 and 2 share four words, while texts 1 and 3 share five words. Does this
    mean `text1` is more similar to `text3` than to `text2`? Not necessarily. While
    texts 1 and 3 share five overlapping words, they also contain diverging words
    that appear in one text but not the other. Let’s count all the diverging words
    between text pairs `(text1, text2)` and `(text1, text3)`. We use the `^` operator
    to extract diverging elements between each pair of word sets.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 文本1和2共享四个单词，而文本1和3共享五个单词。这意味着`text1`比`text2`更接近`text3`吗？不一定。虽然文本1和3共享五个重叠单词，但它们还包含差异单词，这些单词在一个文本中出现，但在另一个文本中没有。让我们计算文本对`(text1,
    text2)`和`(text1, text3)`之间的所有差异单词。我们使用`^`运算符来提取单词集合之间的差异元素。
- en: Listing 13.6 Extracting diverging words between two texts
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.6 提取两个文本之间的差异单词
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Texts 1 and 3 contain two more diverging words than texts 1 and 2\. Thus, texts
    1 and 3 show significant word overlap and also significant divergence. To combine
    their overlap and divergence into a single similarity score, we must first combine
    all overlapping and diverging words between the texts. This aggregation, which
    is called a *union*, will contain all the unique words across the two texts. Given
    two Python sets `set_a` and `set_b`, we can compute their union by running `set_a
    | set_b`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 文本1和3比文本1和2多两个差异单词。因此，文本1和3显示出显著的单词重叠和差异。为了将它们的重叠和差异合并成一个相似度分数，我们必须首先合并文本之间的所有重叠和差异单词。这个聚合，称为*并集*，将包含两个文本中的所有独特单词。给定两个
    Python 集合`set_a`和`set_b`，我们可以通过运行`set_a | set_b`来计算它们的并集。
- en: The differences between word divergence, intersection, and union are illustrated
    in figure 13.1\. Here, the unique words in texts 1 and 2 are displayed in three
    rectangular boxes. The leftmost box and the rightmost box represent the diverging
    words between texts 1 and 2, respectively. Meanwhile, the middle box contains
    all the shared words at the intersection of texts 1 and 2\. Together, the three
    boxes represent the union of all words in the two texts.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 单词差异、交集和并集之间的差异在图13.1中得到了说明。在这里，文本1和2的独特单词显示在三个矩形框中。最左边的框和最右边的框分别代表文本1和2之间的差异单词。同时，中间的框包含文本1和2交集处的所有共享单词。这三个框共同代表了两个文本中所有单词的并集。
- en: '![](../Images/13-01.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13-01.png)'
- en: Figure 13.1 A visualized representation of the union, intersection, and divergence
    between two texts.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1 两个文本之间并集、交集和差异的可视化表示。
- en: Common Python set operations
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的 Python 集合操作
- en: '`set_a & set_b`—Returns all overlapping elements between `set_a` and `set_b`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`set_a & set_b`—返回`set_a`和`set_b`之间的所有重叠元素'
- en: '`set_a ^ set_b`—Returns all diverging elements between `set_a` and `set_b`'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`set_a ^ set_b`—返回`set_a`和`set_b`之间的所有差异元素'
- en: '`set_a | set_b`—Returns the union of all elements between `set_a` and `set_b`'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`set_a | set_b`—返回`set_a`和`set_b`之间所有元素的并集'
- en: '`set_a - set_b`—Returns all elements in `set_a` that are not in `set_b`'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`set_a - set_b`—返回`set_a`中不在`set_b`中的所有元素'
- en: Let’s utilize the `|` operator to count the total unique words across text pairs
    `(text1, text2)` and `(text1, text3)`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`|`运算符来计算文本对`(text1, text2)`和`(text1, text3)`中所有独特单词的总数。
- en: Listing 13.7 Extracting the union of words between two texts
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.7 提取两个文本之间的单词并集
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Together, `text1` and `text3` contain 12 unique words. Five of these words overlap,
    and seven diverge. Accordingly, both overlap and divergence represent complementary
    percentages of the total unique word count across texts. Let’s output these percentages
    for text pairs `(text1, text2)` and `(text1, text3)`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，`text1`和`text3`包含12个独特单词。其中5个单词重叠，7个单词不同。因此，重叠和差异都代表了文本中总独特单词计数的一个补充百分比。让我们为文本对`(text1,
    text2)`和`(text1, text3)`输出这些百分比。
- en: Listing 13.8 Extracting the percentage of shared words between two texts
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.8 提取两个文本之间共享单词的百分比
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Percent of total words shared with text 1
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 与文本1共享的总单词百分比
- en: ❷ Percent of total words that diverge from text 1
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 与文本1差异的总单词百分比
- en: Texts 1 and 3 share 41.67% of total words. The remaining 58.33% of words diverge.
    Meanwhile, texts 1 and 2 share 44.44% of total words. That percentage is higher,
    and thus we can infer that `text1` is more similar to `text2` than to `text3`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 文本1和3共享41.67%的总单词。剩余的58.33%的单词有差异。同时，文本1和2共享44.44%的总单词。这个百分比更高，因此我们可以推断`text1`比`text3`更相似于`text2`。
- en: 'We’ve essentially developed a simple metric for assessing similarities between
    texts. The metric works as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上已经开发了一个简单的指标来评估文本之间的相似性。该指标的工作方式如下：
- en: Given two texts, extract a list of words from each text.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定两个文本，从每个文本中提取单词列表。
- en: Count the unique words that are shared between the texts.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算文本之间共享的独特单词。
- en: Divide the shared word count by the total unique words across both texts. Our
    output is a fraction of the total words shared between texts.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将共享单词计数除以两个文本中的总独特单词数。我们的输出是文本之间共享单词总数的分数。
- en: This similarity metric is referred to as the *Jaccard similarity*, or the *Jaccard
    index*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个相似度指标被称为*Jaccard相似度*，或*Jaccard指数*。
- en: The Jaccard similarity between texts 1 and 2 is illustrated in figure 13.2,
    where the texts are represented as two circles. The left circle corresponds to
    text 1, and the right circle corresponds to text 2\. Each circle contains the
    words in its corresponding text. The two circles intersect, and their intersection
    contains all words that are shared between the texts. The Jaccard similarity equals
    the fraction of total words that are present in the intersection. Four of the
    nine words in the diagram appear in the intersection. Therefore, the Jaccard similarity
    is equal to 4 / 9.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 文本 1 和 2 之间的 Jaccard 相似度如图 13.2 所示，其中文本被表示为两个圆圈。左边的圆圈对应于文本 1，右边的圆圈对应于文本 2。每个圆圈包含其对应文本中的单词。两个圆圈相交，它们的交集包含两个文本之间共享的所有单词。Jaccard
    相似度等于交集中总单词的比例。图中九个单词中有四个出现在交集中。因此，Jaccard 相似度等于 4/9。
- en: '![](../Images/13-02.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/13-02.png)'
- en: Figure 13.2 A visualized representation of the Jaccard similarity between two
    texts
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2 两个文本之间 Jaccard 相似度的可视化表示
- en: 13.1.1 Exploring the Jaccard similarity
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.1 探索 Jaccard 相似度
- en: 'The Jaccard similarity is a reasonable measure of text resemblance for the
    following reasons:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Jaccard 相似度是衡量文本相似度的合理度量，原因如下：
- en: The similarity takes into account both text overlap and text divergence.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相似度同时考虑了文本重叠和文本差异。
- en: 'The fractional similarity is always between 0 and 1\. The fraction is easy
    to interpret: 0 indicates that no words are shared, 0.5 indicates that half the
    words are shared, and 1 indicates that all the words are shared.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分数相似度总是在 0 和 1 之间。这个分数很容易理解：0 表示没有共享单词，0.5 表示一半的单词共享，1 表示所有单词都共享。
- en: The similarity is simple to implement.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相似度易于实现。
- en: Let’s define a function to compute the Jaccard similarity.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个函数来计算 Jaccard 相似度。
- en: Listing 13.9 Computing the Jaccard similarity
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.9 计算 Jaccard 相似度
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Our implementation of the Jaccard similarity is functional but not very efficient.
    The function executes two set-comparison operations: `word_set_a & word_set_b`
    and `word_set_a | word_set_b`. These operations compare and contrast all words
    between two sets. In Python, such comparisons are computationally costlier than
    streamlined numerical analysis.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现的 Jaccard 相似度是功能性的，但效率不高。该函数执行两个集合比较操作：`word_set_a & word_set_b` 和 `word_set_a
    | word_set_b`。这些操作比较和对比两个集合之间的所有单词。在 Python 中，这种比较的计算成本比简化的数值分析要高。
- en: 'How do we make the function more efficient? Well, we can start by eliminating
    the union computation `word_set_a | word_set_b`. We take the union in order to
    count the unique words between the sets, but there’s a simpler way to obtain that
    count. Consider the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何使函数更高效？嗯，我们可以从消除并集计算 `word_set_a | word_set_b` 开始。我们取并集是为了计算集合之间的唯一单词数，但有一个更简单的方法可以得到这个计数。考虑以下：
- en: Adding `len(word_set_a)` and `len(word_set_b)` yields a word count where the
    shared words are counted twice.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `len(word_set_a)` 和 `len(word_set_b)` 相加得到一个单词计数，其中共享的单词被计算了两次。
- en: Subtracting `len(word_set_a & word_set_b)` from that sum eliminates the double
    count. The final result equals `len(word_set_a | word_set_b)`.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从那个和中减去 `len(word_set_a & word_set_b)` 消除了重复计数。最终结果等于 `len(word_set_a | word_set_b)`。
- en: We can replace the union computation with `len(word_set_a) + len(word_set_b)
    - num_shared`, thus making our function more efficient. Let’s modify the function
    while ensuring that our Jaccard output remains the same.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用 `len(word_set_a) + len(word_set_b) - num_shared` 来替换并集计算，从而使我们的函数更高效。让我们修改函数，同时确保我们的
    Jaccard 输出保持不变。
- en: Listing 13.10 Efficiently computing the Jaccard similarity
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.10 高效计算 Jaccard 相似度
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Unlike our previous jaccard_similarity function, here we compute num_total
    without executing any set-comparison operations.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 与我们之前的 jaccard_similarity 函数不同，这里我们计算 num_total 而不执行任何集合比较操作。
- en: 'We’ve improved our Jaccard function. Unfortunately, the function still won’t
    scale: it might run efficiently on hundreds of sentences but not on thousands
    of multisentence documents. The inefficiency is caused by our remaining set comparison,
    `word_set_a & word_set_b`. The operation is too slow to execute across thousands
    of complicated texts. Perhaps we can speed up the computation by somehow running
    it using NumPy. However, NumPy is intended to process numbers, not words, so we
    cannot use the library unless we replace all words with numeric values.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们改进了我们的Jaccard函数。不幸的是，该函数仍然无法扩展：它可能在数百个句子上运行得很好，但在数千个多句子文档上则不行。这种低效是由我们剩余的集合比较`word_set_a
    & word_set_b`引起的。该操作在数千个复杂文本上执行得太慢。也许我们可以通过某种方式使用NumPy来加速计算。然而，NumPy旨在处理数字，而不是单词，因此除非我们将所有单词替换为数值，否则我们不能使用该库。
- en: 13.1.2 Replacing words with numeric values
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.2 用数值替换单词
- en: Can we swap out words for numbers? Yes! We simply need to iterate over all words
    in all texts and assign each unique *i*th word a value of `i`. The mapping between
    words and their numeric values can be stored in a Python dictionary. We’ll refer
    to this dictionary as our *vocabulary*. Let’s build a vocabulary that covers all
    the words in our three texts. We’ll also create a complementary `value_to_word`
    dictionary, which maps the numeric values back to words.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否用数字替换单词？是的！我们只需要遍历所有文本中的所有单词，并将每个唯一的*i*个单词分配一个值为`i`。单词与其数值之间的映射可以存储在Python字典中。我们将把这个字典称为我们的*词汇表*。让我们构建一个涵盖我们三篇文本中所有单词的词汇表。我们还将创建一个互补的`value_to_word`字典，它将数值映射回单词。
- en: Note Essentially, we’re numbering all the words in the union of the texts. We
    iteratively choose a word and assign it a number, starting with zero. However,
    the order in which we choose the words is not important—we might as well reach
    blindly into a bag of words and pull the words out by random. That is why this
    technique is commonly referred to as the *bag-of-words* technique.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本质上，我们在文本的并集中对所有单词进行编号。我们迭代地选择一个单词并为其分配一个数字，从零开始。然而，我们选择单词的顺序并不重要——我们不妨像从单词袋中盲目抽取单词一样随机抽取单词。这就是为什么这种技术通常被称为*词袋*技术。
- en: Listing 13.11 Assigning words to numbers in a vocabulary
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.11 在词汇表中分配单词到数字
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note The order of the words in the `total_words` variable in listing 13.11 may
    vary based on the installed version of Python. That order change will slightly
    alter certain figures used to display the texts later in this section. Setting
    `total_words` to equal `['sells', 'seashells', 'to', 'lake', 'who', 'by', 'on',
    'lives', 'are', 'sale', 'seashore', 'john', '3', 'the', 'she']` will ensure consistency
    in the outputs.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：列表13.11中`total_words`变量的单词顺序可能取决于Python的安装版本。这种顺序变化将略微改变本节后面用于显示文本的某些图形。将`total_words`设置为`['sells',
    'seashells', 'to', 'lake', 'who', 'by', 'on', 'lives', 'are', 'sale', 'seashore',
    'john', '3', 'the', 'she']`将确保输出的一致性。
- en: Given our vocabulary, we can convert any text into a one-dimensional array of
    numbers. Mathematically, a 1D numeric array is called a *vector*. Hence, the process
    of converting text into a vector is called *text vectorization*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的词汇量，我们可以将任何文本转换为一个一维数字数组。从数学的角度来看，一维数字数组被称为*向量*。因此，将文本转换为向量的过程被称为*文本向量化*。
- en: Note Array dimensionality is different from data dimensionality. A data point
    has `d` dimensions if `d` coordinates are required to spatially represent that
    point. Meanwhile, an array has `d` dimensions if `d` values are required to describe
    the array’s shape. Imagine that we have recorded five data points, each with three
    coordinates. Our data is three-dimensional because it can be plotted in 3D space.
    Additionally, we can store the data in a table containing five rows and three
    columns. That table has a two-element shape of (5, 3) and is therefore two-dimensional.
    Thus, we store our 3D data in a 2D array.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：数组维度与数据维度不同。如果需要`d`个坐标来在空间上表示一个数据点，则该数据点具有`d`个维度。同时，如果需要`d`个值来描述数组的形状，则数组具有`d`个维度。想象一下，我们记录了五个数据点，每个数据点有三个坐标。我们的数据是三维的，因为它可以在3D空间中绘制。此外，我们可以在一个包含五行三列的表中存储数据。该表具有(5,
    3)的二维形状，因此它是二维的。因此，我们将3D数据存储在二维数组中。
- en: The simplest way to vectorize text is to create a vector of binary elements.
    Each index of that vector corresponds to a word in the vocabulary. Hence, the
    vector size equals the vocabulary size, even if some vocabulary words are missing
    from the associated text. If the word at index `i` is missing from the text, the
    *i*th vector element is set to 0\. Otherwise, it is set to 1\. Consequently, each
    vocabulary index in the vector maps to either 0 or 1.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 向量化文本的最简单方法是通过创建一个由二进制元素组成的向量。该向量的每个索引对应于词汇表中的一个单词。因此，向量的大小等于词汇表的大小，即使某些词汇表单词在相关的文本中缺失。如果索引
    `i` 的单词在文本中缺失，则 *i* 个向量元素被设置为 0。否则，它被设置为 1。因此，向量中的每个词汇表索引映射到 0 或 1。
- en: For example, in `vocabulary`, the word *john* maps to a value of 11\. Also,
    the word *john* is not present in `text1`. Thus, the vectorized representation
    of `text1` has a 0 at index 11\. Meanwhile, the word *john* is present in `text3`.
    Consequently, the vectorized representation of `text3` has a 1 at index 11 (figure
    13.3). In this manner, we can convert any text into a binary vector of 0s and
    1s.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在 `vocabulary` 中，单词 *john* 映射到值为 11。此外，单词 *john* 不在 `text1` 中。因此，`text1`
    的向量化表示在索引 11 处有一个 0。同时，单词 *john* 在 `text3` 中存在。因此，`text3` 的向量化表示在索引 11 处有一个 1（图
    13.3）。以这种方式，我们可以将任何文本转换为 0 和 1 的二进制向量。
- en: '![](../Images/13-03.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13-03.png)'
- en: Figure 13.3 Text3 is converted into a binary vector. Each index in the vector
    corresponds to a word in the vocabulary. For example, index 0 corresponds to *sells*.
    This word is present in our text, so the first element of the vector is set to
    1\. Meanwhile, the words *on*, *are*, *sale*, and *seashore* are not present in
    the text. Their corresponding elements are thus set to 0 in the vector.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3 将 Text3 转换为二进制向量。向量的每个索引对应于词汇表中的一个单词。例如，索引 0 对应于 *sells*。这个单词在我们的文本中存在，所以向量的第一个元素被设置为
    1。同时，单词 *on*、*are*、*sale* 和 *seashore* 在文本中不存在。因此，向量中相应的元素被设置为 0。
- en: Let’s use binary vectorization to convert all texts into NumPy arrays. We’ll
    store the computed vectors in a 2D `vectors` list, which can be treated like a
    table. The table’s rows will map to texts, and its columns will map to the vocabulary.
    Figure 13.4 visualizes the table as a heatmap using techniques discussed in section
    8.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用二进制向量化将所有文本转换为 NumPy 数组。我们将计算出的向量存储在一个 2D `vectors` 列表中，它可以像表格一样处理。表格的行将映射到文本，列将映射到词汇表。图
    13.4 使用第 8 节中讨论的技术将表格可视化为一个热图。
- en: '![](../Images/13-04.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13-04.png)'
- en: Figure 13.4 A table of vectorized texts. Rows correspond to labeled texts. Columns
    correspond to labeled words. Binary table elements are either 0 or 1\. A nonzero
    value indicates the presence of a specified word in the specified text. Glancing
    at the table, we can immediately tell which words are shared across which texts.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4 向量化文本的表格。行对应于标记的文本。列对应于标记的单词。二进制表格元素为 0 或 1。非零值表示在指定的文本中存在指定的单词。通过查看表格，我们可以立即知道哪些单词在哪些文本中是共享的。
- en: Note As discussed in section 8, heatmaps are best visualized using the Seaborn
    library.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如第 8 节所述，热图最好使用 Seaborn 库来可视化。
- en: Listing 13.12 Transforming words into binary vectors
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.12 将单词转换为二进制向量
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Generates an array of 0s. We can also generate this array by running np.zeros(len(vocabulary)).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生成一个全为 0 的数组。我们也可以通过运行 np.zeros(len(vocabulary)) 来生成这个数组。
- en: ❷ As of Python 3.6, the dictionary keys method returns dictionary keys based
    on their order of insertion. In vocabulary, the order of insertion is equivalent
    to the word index.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 截至 Python 3.6，字典键方法根据其插入顺序返回字典键。在词汇表中，插入顺序等同于单词索引。
- en: Using our table, we can easily tell which words are shared between which texts.
    Take, for example, the word *sells*, which is tracked in the first column of the
    table. In that column, *sells* is assigned a 1 in the first and third rows of
    the table. These rows correspond to `text1` and `text3`. Hence, we know that *sells*
    is shared between `text1` and `text3`. More formally, the word is shared between
    the texts because `vectors[0][0] == 1` and `vectors[2][0] == 1`. Furthermore,
    since both elements equal 1, their product must also equal 1\. Consequently, the
    texts share a word in column `i` if the product of `vectors[0][i]` and `vectors[2][i]`
    is equal to 1.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的表格，我们可以轻松地判断哪些单词在哪些文本之间是共享的。以单词 *sells* 为例，它在表格的第一列中被跟踪。在该列中，*sells* 在表格的第一行和第三行被分配了
    1。这些行对应于 `text1` 和 `text3`。因此，我们知道 *sells* 在 `text1` 和 `text3` 之间是共享的。更正式地说，单词在文本之间是共享的，因为
    `vectors[0][0] == 1` 和 `vectors[2][0] == 1`。此外，由于这两个元素都等于 1，它们的乘积也必须等于 1。因此，如果
    `vectors[0][i]` 和 `vectors[2][i]` 的乘积等于 1，则文本在列 `i` 中共享一个单词。
- en: Our binary vector representation allows us to extract shared words numerically.
    Suppose we wish to know whether the word in column `i` is present both in `text1`
    and `text2`. If the associated vectors are labeled `vector1` and `vector2`, then
    the word is present in both texts if `vector1[i] * vector2[i] == 1`. Here, we
    use pairwise vector multiplication to find all words shared by `text1` and `text2`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的二进制向量表示法使我们能够从数值上提取共享单词。假设我们想知道列 `i` 中的单词是否同时存在于 `text1` 和 `text2` 中。如果相关的向量被标记为
    `vector1` 和 `vector2`，那么如果 `vector1[i] * vector2[i] == 1`，则该单词在两个文本中都存在。在这里，我们使用成对向量乘法来找到
    `text1` 和 `text2` 共享的所有单词。
- en: Listing 13.13 Finding shared words using vector arithmetic
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.13 使用向量算术查找共享单词
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We’ve outputted all four words shared between `text1` and `text2`. That shared
    word count is equal to the sum of every nonzero instance of `vector1[i] * vector2[i]`.
    Meanwhile, the sum of every zero instance equals 0\. Therefore, we can compute
    the shared word count merely by summing the pairwise product of `vector1[i]` and
    `vector2[i]` across every possible `i`. In other words, `sum(vector1[i] * vector2[i]
    for i in range(len(vocabulary)))` equals `len(words_set1 & words_set2)`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经输出了 `text1` 和 `text2` 之间共享的所有四个单词。这个共享单词数等于 `vector1[i] * vector2[i]` 的每个非零实例的总和。同时，每个零实例的总和等于
    0。因此，我们可以通过将 `vector1[i]` 和 `vector2[i]` 的成对乘积在所有可能的 `i` 上求和来计算共享单词数。换句话说，`sum(vector1[i]
    * vector2[i] for i in range(len(vocabulary)))` 等于 `len(words_set1 & words_set2)`。
- en: Listing 13.14 Counting shared words using vector arithmetic
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.14 使用向量算术计算共享单词数
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The sum of the pairwise products across all vector indices is called the *dot
    product*. Given two NumPy arrays `vector_a` and `vector_b`, we can compute their
    dot product by running `vector_a.dot(vector_b)`. We can also compute the dot product
    using the `@` operator by running `vector_a @ vector_b`. In our example, that
    dot product equals the number of shared words between texts 1 and 2, which of
    course also equals their intersection size. Thus, running `vector1 @ vector2`
    produces a value that is equal to `len(words_set1 & words_set2)`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 所有向量索引的成对乘积之和被称为 *点积*。给定两个 NumPy 数组 `vector_a` 和 `vector_b`，我们可以通过运行 `vector_a.dot(vector_b)`
    来计算它们的点积。我们也可以通过运行 `vector_a @ vector_b` 使用 `@` 运算符来计算点积。在我们的例子中，这个点积等于文本 1 和
    2 之间共享的单词数，这当然也等于它们的交集大小。因此，运行 `vector1 @ vector2` 产生一个值，该值等于 `len(words_set1
    & words_set2)`。
- en: Listing 13.15 Computing a vector dot product using NumPy
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.15 使用 NumPy 计算向量点积
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The dot product of `vector1` and `vector2` equals the shared word count between
    `text1` and `text2`. Suppose that, instead, we take the dot product of `vector1`
    with itself. That output should equal the number of words that `text1` shares
    with `text1`. Stated more concisely, `vector1 @ vector1` should equal the number
    of unique words in `text1`, which is also equal to `len(words_set1)`. Let’s confirm.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`vector1` 和 `vector2` 的点积等于 `text1` 和 `text2` 之间的共享单词数。假设我们取 `vector1` 与自身的点积。该输出应等于
    `text1` 与 `text1` 共享的单词数。更简洁地说，`vector1 @ vector1` 应等于 `text1` 中独特单词的数量，这也等于 `len(words_set1)`。让我们来确认一下。'
- en: Listing 13.16 Counting total words using vector arithmetic
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.16 使用向量算术计算总词数
- en: '[PRE15]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We are able to compute both shared word count and total unique word count using
    vector dot products. Essentially, we can compute the Jaccard similarity using
    only vector operations. This vectorized implementation of Jaccard is called the
    *Tanimoto similarity*.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够使用向量点积来计算共享单词数和总唯一单词数。本质上，我们可以仅使用向量操作来计算 Jaccard 相似度。这种向量化的 Jaccard 实现被称为
    *Tanimoto 相似度*。
- en: Useful NumPy vector operations
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 有用的NumPy向量操作
- en: '`vector_a.dot(vector_b)`—Returns the dot product between `vector_a` and `vector_b`.
    Equivalent to running `sum(vector_a[i] * vector_b[i] for i in range(vector_a.size))`.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vector_a.dot(vector_b)`—返回`vector_a`和`vector_b`之间的点积。相当于运行`sum(vector_a[i]
    * vector_b[i] for i in range(vector_a.size))`。'
- en: '`vector_b @ vector_b`—Returns the dot product between `vector_a` and `vector_b`
    using the `@` operator.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vector_b @ vector_b`—使用`@`运算符返回`vector_a`和`vector_b`之间的点积。'
- en: '`binary_text_vector_a @ binary_text_vector_b`—Returns the number of shared
    words between `text_a` and `text_b`.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_text_vector_a @ binary_text_vector_b`—返回`text_a`和`text_b`之间共享的单词数量。'
- en: '`binary_text_vector_a @ binary_text_vector_a`—Returns the number of unique
    words in `text_a`.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_text_vector_a @ binary_text_vector_a`—返回`text_a`中独特单词的数量。'
- en: Let’s define a `tanimoto_similarity` function. The function takes as input two
    vectors, `vector_a` and `vector_b`. Its output is equal to `jaccard_similarity
    (text_a, text_b)`.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个`tanimoto_similarity`函数。该函数接受两个向量`vector_a`和`vector_b`作为输入。它的输出等于`jaccard_similarity
    (text_a, text_b)`。
- en: Listing 13.17 Computing text similarity using vector arithmetic
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.17 使用向量算术计算文本相似度
- en: '[PRE16]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Our `tanimoto_similarity` function was intended to compare binary vectors. What
    would happen if we inputted two arrays with values other than 0 or 1? Technically,
    the function should return a similarity, but would that similarity make sense?
    For instance, vectors `[5, 3]` and `[5, 2]` are nearly identical. We expect their
    similarity to be nearly equal to 1\. Let’s test our expectations by inputting
    the vectors into `tanimoto_similarity`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`tanimoto_similarity`函数原本是用来比较二进制向量的。如果我们输入了除了0或1之外的值，会发生什么？技术上，该函数应该返回一个相似度，但这个相似度有意义吗？例如，向量`[5,
    3]`和`[5, 2]`几乎相同。我们预计它们的相似度几乎等于1。让我们通过将向量输入到`tanimoto_similarity`来测试我们的预期。
- en: Listing 13.18 Computing the similarity of non-binary vectors
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.18 计算非二进制向量的相似度
- en: '[PRE17]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The outputted value is nearly equal to 1\. Thus, `tanimoto_similarity` has successfully
    measured the similarity between two nearly identical vectors. The function can
    analyze non-binary inputs. This means we can use non-binary techniques to vectorize
    our texts before comparing their contents.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '输出的值几乎等于1。因此，`tanimoto_similarity`成功测量了两个几乎相同的向量之间的相似度。该函数可以分析非二进制输入。这意味着我们可以在比较内容之前使用非二进制技术向量化我们的文本。 '
- en: There are benefits to vectorizing texts in a non-binary way. Let’s discuss these
    benefits in more detail.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 以非二进制方式向量化文本有好处。让我们更详细地讨论这些好处。
- en: 13.2 Vectorizing texts using word counts
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.2 使用单词计数向量化文本
- en: 'Binary vectorization captures the presence and absence of words in a text,
    but it doesn’t capture word counts. This is unfortunate since word counts can
    provide a differentiating signal between texts. For example, suppose we’re contrasting
    two texts: A and B. Text A mentions *Duck* 61 times and *Goose* twice. Text B
    mentions *Goose* 71 times and *Duck* only once. Based on the counts, we can infer
    that the two texts are rather different relative to the discussion of ducks and
    geese. That difference is not captured by binary vectorization, which assigns
    a 1 to the *Duck* index and *Goose* index of both texts. What if we replace all
    binary values with actual word counts? For instance, we can assign values of 61
    and 2 to the *Duck* and *Goose* indices of vector A, while assigning 1 and 71
    to the corresponding indices of vector B.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制向量化捕捉了文本中单词的存在和缺失，但它不捕捉单词计数。这是不幸的，因为单词计数可以在文本之间提供区分信号。例如，假设我们正在对比两个文本：A和B。文本A提到*鸭*61次和*鹅*两次。文本B提到*鹅*71次和*鸭*只有一次。根据计数，我们可以推断这两个文本在讨论鸭和鹅方面相当不同。这种差异没有被二进制向量化捕捉到，它将两个文本的*鸭*索引和*鹅*索引都分配为1。如果我们用实际的单词计数来替换所有二进制值会怎样呢？例如，我们可以将向量A的*鸭*和*鹅*索引的值分别赋为61和2，而将向量B的相应索引赋为1和71。
- en: 'These assignments will produce vectors of word counts. A vector of word counts
    is commonly referred to as a *term-frequency vector*, or a *TF vector* for short.
    Let’s compute the TF vectors of A and B using a two-element vocabulary `{''duck'':
    0, ''goose'': 1}`. As a reminder, each word in the vocabulary maps to a vector
    index. Given the vocabulary, we can convert the texts into TF vectors `[61, 2]`
    and `[1, 71]`. Then we print the Tanimoto similarity of the two vectors.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '这些赋值将生成单词计数的向量。单词计数的向量通常被称为 *词频向量*，或简称为 *TF 向量*。让我们使用一个包含两个元素的词汇表 `{''duck'':
    0, ''goose'': 1}` 来计算 A 和 B 的 TF 向量。作为提醒，词汇表中的每个单词都映射到一个向量索引。给定词汇表，我们可以将文本转换为
    TF 向量 `[61, 2]` 和 `[1, 71]`。然后我们打印这两个向量的 Tanimoto 相似度。'
- en: Listing 13.19 Computing TF vector similarity
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.19 计算TF向量相似度
- en: '[PRE18]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The TF vector similarity between the texts is very low. Let’s compare it to
    the binary-vector similarity of the two texts. Each text has a binary-vector representation
    of `[1, 1]`, and thus the binary similarity should equal 1.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 文本之间的 TF 向量相似度非常低。让我们将其与两个文本的二进制向量相似度进行比较。每个文本都有一个 `[1, 1]` 的二进制向量表示，因此二进制相似度应该等于
    1。
- en: Listing 13.20 Assessing identical vector similarity
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.20 评估相同向量的相似度
- en: '[PRE19]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Replacing binary values with word counts can greatly impact our similarity output.
    What will happen if we vectorize `text1`, `text2`, and `text3` based on their
    word counts? Let’s find out. We start by computing TF vectors for each of the
    three texts using the word lists stored in `words_lists`. These vectors are visualized
    in figure 13.5 using a heatmap.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 用单词计数替换二进制值可以极大地影响我们的相似度输出。如果我们根据 `text1`、`text2` 和 `text3` 的单词计数来向量化这些文本，会发生什么？让我们来看看。我们首先使用存储在
    `words_lists` 中的单词列表计算这三个文本的 TF 向量。这些向量在图 13.5 中使用热图进行可视化。
- en: '![](../Images/13-05.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13-05.png)'
- en: Figure 13.5 A table of TF vectors. Rows correspond to labeled texts. Columns
    correspond to labeled words. Each value indicates the count of a specified word
    in the specified text. Two words in the table are mentioned twice; all other words
    are mentioned no more than once.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5 一个 TF 向量表。行对应于标记的文本。列对应于标记的单词。每个值表示在指定文本中指定单词的计数。表中提到了两次的两个单词；所有其他单词都没有被提及超过一次。
- en: Listing 13.21 Computing TF vectors from word lists
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.21 从单词列表计算 TF 向量
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Updates the word count using the word’s vocabulary index
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用单词的词汇索引更新单词计数
- en: The TF vectors of texts 1 and 3 are identical to previously seen binary vector
    outputs. However, the TF vector of text 2 is no longer binary since two words
    are mentioned more than once. How will this affect the similarity between `text1`
    and `text2`? Let’s find out. The following code computes the TF vector similarity
    between `text1` and the other two texts. It also outputs the original binary vector
    similarity for comparison. Based on our observations, the similarity between `text1`
    and `text2` should shift, while the similarity between `text1` and `text3` should
    remain the same.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 文本 1 和 3 的 TF 向量与之前看到的二进制向量输出相同。然而，文本 2 的 TF 向量不再是二进制，因为有两个单词被提及了不止一次。这将对 `text1`
    和 `text2` 之间的相似度产生什么影响？让我们来看看。以下代码计算了 `text1` 与其他两个文本之间的 TF 向量相似度。它还输出了原始的二进制向量相似度以供比较。根据我们的观察，`text1`
    和 `text2` 之间的相似度应该会发生变化，而 `text1` 和 `text3` 之间的相似度应该保持不变。
- en: Listing 13.22 Comparing metrics of vector similarity
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.22 比较向量相似度的度量
- en: '[PRE21]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As expected, the similarity between `text1` and `text3` has stayed the same,
    while the similarity between `text1` and `text2` has increased. Thus, TF vectorization
    has made the affinity of the two texts more pronounced.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，`text1` 和 `text3` 之间的相似度保持不变，而 `text1` 和 `text2` 之间的相似度有所增加。因此，TF 向量化使得这两篇文本的亲和度更加明显。
- en: TF vectors yield improved comparisons because they’re sensitive to count differences
    between texts. This sensitivity is useful. However, it can also be detrimental
    when comparing texts of different lengths. In the following subsection, we examine
    a flaw associated with TF vector comparison. Then we apply a technique called
    *normalization* to eliminate this flaw.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: TF 向量提供了改进的比较，因为它们对文本之间的计数差异敏感。这种敏感性是有用的。然而，当比较不同长度的文本时，它也可能是有害的。在接下来的小节中，我们将检查与
    TF 向量比较相关的一个缺陷。然后我们应用一种称为 *归一化* 的技术来消除这个缺陷。
- en: 13.2.1 Using normalization to improve TF vector similarity
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.1 使用归一化来提高 TF 向量相似度
- en: Imagine that you are testing a very simple search engine. The search engine
    takes a query and compares it to document titles stored in a database. The query’s
    TF vector is compared to every vectorized title. Titles with a nonzero Tanimoto
    similarity are returned and ranked based on their similarity score.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在测试一个非常简单的搜索引擎。该搜索引擎接收一个查询并将其与存储在数据库中的文档标题进行比较。查询的 TF 向量与每个向量化的标题进行比较。具有非零
    Tanimoto 相似度的标题将被返回，并根据它们的相似度得分进行排序。
- en: 'Suppose you run a query for “Pepperoni Pizza” and the following two titles
    are returned:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你运行了一个针对“Pepperoni Pizza”的查询，并返回以下两个标题：
- en: '*Title A*—“Pepperoni Pizza! Pepperoni Pizza! Pepperoni Pizza!”'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*标题 A*—“Pepperoni Pizza! Pepperoni Pizza! Pepperoni Pizza!”'
- en: '*Title B*—“Pepperoni”'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*标题 B*—“Pepperoni”'
- en: Note These titles are purposefully oversimplified for easier visualization.
    Most real document titles are more complicated.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这些标题故意简化，以便更容易可视化。大多数真实文档的标题都更复杂。
- en: Which of our two titles best matches the query? Most data scientists would agree
    that title A is a better match than title B. Both title A and the query mention
    *pepperoni pizza*. Meanwhile, title B mentions only *pepperoni*. There is no indication
    that the associated document actually discusses pizza in any context.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的两个标题中哪一个最能匹配查询？大多数数据科学家都会同意标题 A 比标题 B 更匹配。标题 A 和查询都提到了 *Pepperoni Pizza*。而标题
    B 只提到了 *Pepperoni*。没有任何迹象表明相关的文档在任何上下文中实际讨论了披萨。
- en: 'Let’s check whether title A ranks higher than title B relative to the query.
    We start by constructing TF vectors from a two-element vocabulary `{pepperoni:
    0, pizza: 1}`.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们检查标题 A 相对于查询是否比标题 B 排名更高。我们首先从包含两个元素词汇表 `{pepperoni: 0, pizza: 1}` 中构建 TF
    向量。'
- en: Listing 13.23 Simple search engine vectorization
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.23 简单搜索引擎向量化
- en: '[PRE22]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We now compare the query to the titles and sort the titles based on the Tanimoto
    similarity.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将查询与标题进行比较，并根据 Tanimoto 相似度对标题进行排序。
- en: Listing 13.24 Ranking titles by query similarity
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.24 通过查询相似度对标题进行排序
- en: '[PRE23]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Unfortunately, title A outranks title B. This discrepancy in rankings is caused
    by text size. Title A has three times as many words as the query, while title
    B and the query differ by just a single word. Superficially, this difference can
    be used to distinguish texts by size. However, in our search engine, the size
    signal leads to faulty rankings. We need to subdue the influence of text size
    on ranked results. One naive approach is to just divide `title_a_vector` by 3\.
    The division yields an output that is equal to `query_ vector`. Therefore, running
    `tanimoto_similarity(query_vector, title_a_vector / 3)` should return a similarity
    of 1.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，标题 A 在排名上超过了标题 B。这种排名差异是由文本大小引起的。标题 A 的单词数量是查询的三倍，而标题 B 和查询只相差一个单词。表面上，这种差异可以用来通过大小区分文本。然而，在我们的搜索引擎中，大小信号导致了错误的排名。我们需要削弱文本大小对排序结果的影响。一种简单的方法是将
    `title_a_vector` 除以 3。除法得到的结果等于 `query_vector`。因此，运行 `tanimoto_similarity(query_vector,
    title_a_vector / 3)` 应该返回相似度为 1。
- en: Listing 13.25 Eliminating size differences through division
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.25 通过除法消除大小差异
- en: '[PRE24]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Using simple division, we can manipulate `title_a_vector` to equal `query_vector`.
    Such manipulation is not possible for `title_b_vector`. Why is this the case?
    To illustrate the answer, we need to plot all three vectors in 2D space.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 使用简单的除法，我们可以操作 `title_a_vector` 以等于 `query_vector`。对于 `title_b_vector`，这种操作是不可能的。为什么是这样？为了说明答案，我们需要在
    2D 空间中绘制所有三个向量。
- en: How can we visualize our vectors? Well, from a mathematical standpoint, all
    vectors are geometric objects. Mathematicians treat every vector `v` as a line
    stretching from the origin to the numerical coordinates in `v`. Essentially, our
    three vectors are merely 2D line segments rising from the origin. We can visualize
    the segments in a 2D plot where the x-axis represents mentions of *pepperoni*
    and the y-axis represents mentions of *pizza* (figure 13.6).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何可视化我们的向量呢？从数学的角度来看，所有向量都是几何对象。数学家将每个向量 `v` 视为从原点到 `v` 中数值坐标的线段。本质上，我们的三个向量仅仅是起源于原点的
    2D 线段。我们可以在一个 2D 图中可视化这些线段，其中 x 轴代表 *Pepperoni* 的提及，y 轴代表 *pizza* 的提及（图 13.6）。
- en: '![](../Images/13-06.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13-06.png)'
- en: Figure 13.6 Three TF vectors have been plotted as lines in 2D space. Each vector
    stretches from the origin to its two-dimensional coordinates. The query vector
    and the title A vector both face in the same direction. The angle between these
    vectors is zero. However, one of the lines is three times as long as the other.
    Adjusting the segment lengths will force the two vectors to be identical.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.6 在二维空间中将三个 TF 向量绘制为线条。每个向量从原点延伸到其二维坐标。查询向量和标题向量都朝同一方向。这两个向量之间的角度为零。然而，其中一条线是另一条线的三倍长。调整线段长度将迫使两个向量相同。
- en: Listing 13.26 Plotting TF vectors in 2D space
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.26 在二维空间中绘制 TF 向量
- en: '[PRE25]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In our plot, `title_a_vector` and `query_vector` point in the same direction.
    The only difference between the two lines is that `title_a_vector` is three times
    as long. Shrinking `title_a_vector` will force the two lines to be identical.
    Meanwhile, `title_b_vector` and `query_vector` point in different directions.
    We cannot make these vectors overlap. Shrinking or lengthening `title_b_vector`
    will not yield alignment with the other two line segments.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的图中，`title_a_vector` 和 `query_vector` 指向同一方向。这两条线之间的唯一区别是 `title_a_vector`
    的长度是三倍。缩小 `title_a_vector` 将迫使两条线相同。同时，`title_b_vector` 和 `query_vector` 指向不同方向。我们无法使这些向量重叠。缩小或延长
    `title_b_vector` 不会使其他两个线段对齐。
- en: We’ve gained some insight by representing our vectors as line segments. These
    segments have geometric lengths. Hence, every vector has a geometric length, which
    is called the *magnitude*. The magnitude is also called the *Euclidean norm* or
    the *L2 norm*. All vectors have a magnitude, even those that can’t be plotted
    in two dimensions. For instance, in figure 13.7, we illustrate the magnitude of
    a 3D vector associated with *Pepperoni Pizza Pie*.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将向量表示为线段，我们已经获得了一些洞察。这些线段具有几何长度。因此，每个向量都有一个几何长度，称为 *模*。模长也称为 *欧几里得范数* 或 *L2
    范数*。所有向量都有模长，即使那些不能在二维中绘制的向量也是如此。例如，在图 13.7 中，我们展示了与 *Pepperoni Pizza Pie* 相关的
    3D 向量的模长。
- en: '![](../Images/13-07.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13-07.png)'
- en: Figure 13.7 The plotted TF vector representation of the three-word title *Pepperoni
    Pizza Pie*. This 3D vector stretches from the origin to its coordinates of (1,
    1, 1). According to the Pythagorean theorem, the length of the plotted 3D segment
    is equal to `(1` `+` `1` `+` `1)` `**` `0.5`. That length is referred to as the
    *magnitude* of the vector.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.7 展示了三个单词标题 *Pepperoni Pizza Pie* 的 TF 向量表示。这个 3D 向量从原点延伸到其坐标 (1, 1, 1)。根据勾股定理，绘制的
    3D 段的长度等于 `(1` `+` `1` `+` `1)` `**` `0.5`。这个长度被称为向量的 *模*。
- en: Measuring the magnitude allows us to account for differences in geometric lengths.
    There are several ways to compute the magnitude in Python. Given vector `v`, we
    can measure the magnitude naively by measuring the Euclidean distance between
    `v` and the origin. We can also find the magnitude using NumPy by running `np.linalg.norm(v)`.
    Finally, we can compute the magnitude using the Pythagorean theorem (figure 13.8).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 测量模长使我们能够考虑几何长度的差异。在 Python 中有多种计算模长的方法。给定向量 `v`，我们可以通过测量 `v` 与原点之间的欧几里得距离来天真地测量模长。我们也可以使用
    NumPy 通过运行 `np.linalg.norm(v)` 来找到模长。最后，我们可以使用勾股定理（图 13.8）来计算模长。
- en: '![](../Images/13-08.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13-08.png)'
- en: Figure 13.8 Using the Pythagorean theorem to compute the magnitude of a vector.
    Generally, a two-dimensional vector `[a,` `b]` can be represented by a right triangle.
    The perpendicular segments of the triangle have lengths *a* and *b*. Meanwhile,
    the length of the triangle’s hypotenuse is equal to *c*. According to the Pythagorean
    theorem, *c* * *c* == *a* * *a* + *b* * *b*. Hence, the vector’s magnitude is
    equal to `sum([value` `*` `value` `for` `value` `in` `vector])` `**` `0.5`. This
    formula extends beyond 2D to any arbitrary number of dimensions.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.8 使用勾股定理计算向量的模。一般来说，二维向量 `[a,` `b]` 可以用一个直角三角形来表示。三角形的垂直边长分别为 *a* 和 *b*。同时，三角形的斜边长度等于
    *c*。根据勾股定理，*c* * *c* == *a* * *a* + *b* * *b*。因此，向量的模等于 `sum([value` `*` `value`
    `for` `value` `in` `vector])` `**` `0.5`。这个公式不仅适用于二维，也适用于任意维数。
- en: According to the Pythagorean theorem, the squared distance of coordinate `v`
    to the origin is equal to `sum([value * value for value in v])`. This dovetails
    nicely with our earlier definition of the dot product. As a reminder, the dot
    product of two vectors `v1` and `v2` equals `sum([value1 * value2 for value1,
    value2 in zip(v1, v2)])`. Consequently, the dot product of `v` with itself equals
    `sum([value * value for value in v])`. Hence, the magnitude of `v` equals `(v
    @ v) ** 0.5`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 根据勾股定理，坐标 `v` 到原点的平方距离等于 `sum([value * value for value in v])`。这与我们之前对点积的定义完美契合。提醒一下，两个向量
    `v1` 和 `v2` 的点积等于 `sum([value1 * value2 for value1, value2 in zip(v1, v2)])`。因此，向量
    `v` 与自身的点积等于 `sum([value * value for value in v])`。因此，`v` 的幅度等于 `(v @ v) ** 0.5`。
- en: Let’s output the magnitudes of our search engine vectors. Based on our observations,
    the magnitude of `title_a_vector` should equal three times the magnitude of `query_vector`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们输出搜索引擎向量的幅度。根据我们的观察，`title_a_vector` 的幅度应该等于 `query_vector` 的三倍。
- en: Listing 13.27 Computing vector magnitude
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.27 计算向量幅度
- en: '[PRE26]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ The magnitude equals the Euclidean distance between the vector and the origin.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这个向量的幅度等于该向量与原点之间的欧几里得距离。
- en: ❷ NumPy’s norm function returns the magnitude.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ NumPy 的 `norm` 函数返回幅度。
- en: ❸ We can also compute the magnitude using the dot product.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们也可以使用点积来计算幅度。
- en: 'As expected, there is a threefold difference between the magnitudes of `query_vector`
    and `title_a_vector`. Furthermore, the magnitudes of both vectors are greater
    than 1\. Meanwhile, the magnitude of `title_vector_b` is equal to exactly 1\.
    A vector with a magnitude of 1 is referred to as a *unit vector*. Unit vectors
    have many useful properties, which we discuss shortly. One benefit of unit vectors
    is that they are easy to compare: since unit vectors share an equal magnitude,
    it doesn’t play a role in their similarity. Fundamentally, the difference between
    unit vectors is determined solely by direction.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，`query_vector` 和 `title_a_vector` 的幅度之间存在三倍差异。此外，这两个向量的幅度都大于 1。同时，`title_vector_b`
    的幅度正好等于 1。幅度为 1 的向量被称为**单位向量**。单位向量具有许多有用的特性，我们将在稍后讨论。单位向量的一个好处是它们很容易比较：由于单位向量具有相同的幅度，这并不影响它们的相似性。从根本上说，单位向量之间的差异完全由方向决定。
- en: Imagine if `title_a_vector` and `query_vector` both had a magnitude of 1\. As
    a consequence, they’d share an equal length while also pointing in the same direction.
    In essence, the two vectors would be identical. The word count differences between
    our query and title A would no longer matter.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 `title_a_vector` 和 `query_vector` 都具有幅度为 1。结果，它们将具有相同的长度，并且指向相同的方向。本质上，这两个向量将是相同的。我们查询和标题
    A 之间的词数差异将不再重要。
- en: To illustrate this point, let’s convert our TF vectors into unit vectors. Dividing
    any vector by its magnitude transforms that magnitude to 1\. That division by
    the magnitude is called *normalization*, since the magnitude is also referenced
    as the L2 norm. Running `v / norm(v)` returns a *normalized vector* with a magnitude
    of 1.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，让我们将我们的 TF 向量转换为单位向量。将任何向量除以其幅度会将该幅度转换为 1。这种除以幅度的操作称为**归一化**，因为幅度也被称为
    L2 范数。运行 `v / norm(v)` 返回一个幅度为 1 的**归一化向量**。
- en: We now normalize our vectors and generate a unit vector plot (figure 13.9).
    In the plot, two of the vectors should be identical.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在归一化我们的向量并生成一个单位向量图（图 13.9）。在图中，应该有两个向量是相同的。
- en: Listing 13.28 Plotting normalized vectors
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.28 绘制归一化向量
- en: '[PRE27]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ The two normalized unit vectors are now identical. We use np.allclose to confirm,
    rather than np.array_equal, to compensate for minuscule floating-point errors
    that may occur during normalization.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这两个归一化的单位向量现在是相同的。我们使用 `np.allclose` 来确认，而不是 `np.array_equal`，以补偿归一化过程中可能出现的微小浮点误差。
- en: ❷ This vector is already a unit vector. There is no need to normalize.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 这个向量已经是单位向量了。没有必要进行归一化。
- en: '![](../Images/13-09.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图 13-09](../Images/13-09.png)'
- en: Figure 13.9 Our vectors have been normalized. All plotted vectors now have a
    magnitude of 1\. The normalized query vector and normalized title A vector are
    identical in the plot.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.9 我们已经将向量归一化。现在所有绘制的向量幅度都为 1。在图中，归一化的查询向量和归一化的标题 A 向量是相同的。
- en: The normalized query vector and the normalized title A vector are now indistinguishable.
    All differences arising from text size have been eliminated. Meanwhile, the location
    of the title B vector diverges from the query vector because the two segments
    point in different directions. If we rank our unit vectors based on their similarity
    to `unit_query_vector`, then `unit_title_a_vector` outranks `unit_title_b_vector`.
    As a consequence, title A outranks title B relative to the query.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化查询向量和归一化标题向量现在无法区分。所有由文本大小引起的差异都已消除。同时，标题 B 向量的位置与查询向量不同，因为这两个片段指向不同的方向。如果我们根据它们与
    `unit_query_vector` 的相似度对单位向量进行排序，那么 `unit_title_a_vector` 的排名高于 `unit_title_b_vector`。因此，标题
    A 相对于查询的排名高于标题 B。
- en: Listing 13.29 Ranking titles by unit vector similarity
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.29 通过单位向量相似度对标题进行排序
- en: '[PRE28]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Common vector magnitude operations
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的向量长度运算
- en: '`euclidean(vector, vector.size * [0])`—Returns the vector’s magnitude, which
    equals the Euclidean distance between `vector` and the origin'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`euclidean(vector, vector.size * [0])`—返回向量的长度，等于 `vector` 与原点之间的欧几里得距离'
- en: '`norm(vector)`—Returns the vector’s magnitude using NumPy’s `norm` function'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm(vector)`—使用 NumPy 的 `norm` 函数返回向量的长度'
- en: '`(vector @ vector) ** 0.5`—Computes the vector’s magnitude using the Pythagorean
    theorem'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(vector @ vector) ** 0.5`—使用勾股定理计算向量的长度'
- en: '`vector / norm(vector)`—Normalizes the vector so that its magnitude equals
    1.0'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vector / norm(vector)`—将向量归一化，使其长度等于 1.0'
- en: 'Vector normalization has fixed a flaw in our search engine: the search engine
    is no longer overly sensitive to title length. In the process, we have inadvertently
    made our Tanimoto computation more efficient. Let’s discuss why.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 向量归一化修复了我们搜索引擎中的一个缺陷：搜索引擎不再对标题长度过度敏感。在这个过程中，我们不经意间使我们的 Tanimoto 计算更加高效。让我们讨论一下原因。
- en: 'Suppose we measure the Tanimoto similarity of two unit vectors, `u1` and `u2`.
    Logically, we can infer the following:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们测量两个单位向量 `u1` 和 `u2` 的 Tanimoto 相似度。从逻辑上讲，我们可以推断出以下内容：
- en: The Tanimoto similarity equals `u1 @ u2 / (u1 @ u1 + u2 @ u2 - u1 @ u2)`.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanimoto 相似度等于 `u1 @ u2 / (u1 @ u1 + u2 @ u2 - u1 @ u2)`。
- en: '`u1 @ u1` equals `norm(u1) ** 2`. Based on our previous discussions, we know
    that `u1 @ u1` equals the squared magnitude of `u`.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`u1 @ u1` 等于 `norm(u1) ** 2`。根据我们之前的讨论，我们知道 `u1 @ u1` 等于 `u` 的长度的平方。'
- en: '`u1` is a unit vector, so `norm(u1)` equals 1\. Therefore, `norm(u1) ** 2`
    equals 1\. Thus `u1 @ u1` equals 1.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`u1` 是一个单位向量，所以 `norm(u1)` 等于 1。因此，`norm(u1) ** 2` 等于 1。因此 `u1 @ u1` 等于 1。'
- en: By that same logic, `u2 @ u2` also equals 1.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照同样的逻辑，`u2 @ u2` 也等于 1。
- en: Hence, the Tanimoto similarity reduces to `u1 @ u2 / (2 - u1 @ u2)`.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，Tanimoto 相似度简化为 `u1 @ u2 / (2 - u1 @ u2)`。
- en: Taking the dot product of each vector with itself is no longer necessary. The
    only required vector computation is `u1 @ u2`.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 与每个向量与其自身进行点积不再必要。唯一需要的向量计算是 `u1 @ u2`。
- en: Let’s define a `normalized_tanimoto` function. The function takes as input two
    normalized vectors, `u1` and `u2`, and computes their Tanimoto similarity directly
    from `u1 @ u2`. That result equals `tanimoto_similarity(u1, u2)`.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个 `normalized_tanimoto` 函数。该函数接受两个归一化向量 `u1` 和 `u2` 作为输入，并直接从 `u1 @ u2`
    计算它们的 Tanimoto 相似度。该结果等于 `tanimoto_similarity(u1, u2)`。
- en: Listing 13.30 Computing a unit vector Tanimoto similarity
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.30 计算单位向量 Tanimoto 相似度
- en: '[PRE29]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The dot product of two unit vectors is a very special value. It can easily
    be converted into the angle between the vectors and also into the spatial distance
    between them. Why is this important? Well, common geometric metrics like vector
    angle and distance appear in all vector analysis libraries. Meanwhile, the Tanimoto
    similarity is used less frequently outside of NLP. It usually needs to be implemented
    from scratch, which can have serious real-world consequences. Imagine the following
    scenario. You’re hired by a search engine company to improve all its pizza-related
    queries. You propose to use the normalized Tanimoto similarity as a metric of
    query relevance. However, your manager objects: they insist that, based on company
    policy, employees can only use relevance metrics that are already included in
    scikit-learn.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 两个单位向量的点积是一个非常特殊的值。它可以很容易地转换为向量之间的角度，也可以转换为它们之间的空间距离。这为什么很重要呢？嗯，常见的几何度量，如向量角度和距离，出现在所有的向量分析库中。同时，Tanimoto
    相似度在 NLP 之外的使用频率较低。它通常需要从头开始实现，这可能会产生严重的现实世界后果。想象以下场景。你被一家搜索引擎公司雇佣来改进所有与披萨相关的查询。你提议使用归一化的
    Tanimoto 相似度作为查询相关性的度量。然而，你的经理反对：他们坚持认为，根据公司政策，员工只能使用已经包含在 scikit-learn 中的相关性度量。
- en: Note Sadly, this scenario is entirely realistic. Most organizations tend to
    validate their core metrics for both speed and quality. In large organizations,
    the validation process can take months. Thus, it’s usually easier to rely on a
    prevalidated library than validate a brand-new metric.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 很遗憾，这种场景是完全现实的。大多数组织倾向于验证其核心指标的速度和质量。在大组织中，验证过程可能需要数月。因此，通常更容易依赖于预先验证的库，而不是验证全新的指标。
- en: The manager points you to the scikit-learn documentation that outlines the acceptable
    metric functions ([http://mng.bz/9aM1](http://mng.bz/9aM1)). You see the scikit-learn
    metric names and functions displayed in the two-column table shown in figure 13.10\.
    Multiple versions of a name can map to the same function. Four of the eight metrics
    refer to the Euclidean distance, and three refer to the Manhattan and Haversine
    (aka great circle) distances, which we introduced in section 11\. Also, there’s
    a reference to a metric called `'cosine'`, which we haven’t yet discussed. There
    is no mention of a Tanimoto metric, so you can’t use it to evaluate query relevance.
    What should you do?
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 经理人指向了概述可接受度量函数的 scikit-learn 文档（[http://mng.bz/9aM1](http://mng.bz/9aM1)）。你看到
    scikit-learn 度量名称和函数显示在图 13.10 所示的两个列表中。多个名称可以映射到同一个函数。其中四种度量指的是欧几里得距离，三种指的是曼哈顿和哈弗辛（也称为大圆）距离，我们在第
    11 节中介绍了这些距离。还有一个关于名为 `'cosine'` 的度量的引用，我们还没有讨论过。没有提到 Tanimoto 度量，因此你不能用它来评估查询相关性。你应该怎么办？
- en: '![](../Images/13-10.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13-10.png)'
- en: Figure 13.10 A screenshot of the scikit-learn document for valid distance metric
    implementations
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.10 显示 scikit-learn 文档中有效距离度量实现的屏幕截图
- en: Fortunately, math gives you a way out. If your vectors are normalized, their
    Tanimoto similarity can be substituted with the Euclidean and cosine metrics.
    This is because all three measures are very closely related to the normalized
    dot product. Let’s examine why this is the case.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，数学为你提供了一条出路。如果你的向量已经归一化，它们的Tanimoto相似度可以用欧几里得和余弦度量来代替。这是因为这三个度量都与归一化点积非常密切相关。让我们来看看为什么是这样的情况。
- en: 13.2.2 Using unit vector dot products to convert between relevance metrics
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.2 使用单位向量点积在相关性指标之间进行转换
- en: The unit vector dot product unites multiple types of comparison metrics. We’ve
    just seen how `tanimoto_similarity(u1, u2)` is a direct function of `u1 @ u2`.
    As it turns out, the Euclidean distance between unit vectors is also a function
    of `u1 @ u2`. It’s not difficult to prove that `euclidean(u1, u2)` equals `(2
    - 2* u1 @ u2) ** 0.5`. Additionally, the angle between linear unit vectors is
    likewise dependent on `u1 @ u2`. These relationships are illustrated in figure
    13.11.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 单位向量点积将多种比较度量类型统一起来。我们刚刚看到 `tanimoto_similarity(u1, u2)` 是 `u1 @ u2` 的直接函数。实际上，单位向量之间的欧几里得距离也是
    `u1 @ u2` 的函数。证明 `euclidean(u1, u2)` 等于 `(2 - 2* u1 @ u2) ** 0.5` 并不难。此外，线性单位向量之间的角度同样依赖于
    `u1 @ u2`。这些关系在图 13.11 中得到了说明。
- en: '![](../Images/13-11.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13-11.png)'
- en: Figure 13.11 Two unit vectors, A and B. The angle between the vectors equals
    θ. The dot product of the vectors equals cosine(θ). C represents the Euclidean
    distance between the vectors, which equals (2 – 2 * cosine(θ))0.5.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.11 两个单位向量 A 和 B。向量之间的角度等于 θ。向量的点积等于余弦(θ)。C 代表向量之间的欧几里得距离，等于 (2 – 2 * 余弦(θ))0.5。
- en: Geometrically, the dot product of two unit vectors equals the cosine of the
    angle between them. Due to its equivalence with the cosine, the dot product of
    two unit vectors is commonly referred to as the *cosine similarity*. Given the
    cosine similarity `cs`, we can convert it to either Euclidean distance or the
    Tanimoto similarity by running `(2 – 2 * cs) ** 0.5` or `cs / (2 – cs)`, respectively.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 几何上，两个单位向量的点积等于它们之间角度的余弦值。由于其与余弦值的等价性，两个单位向量的点积通常被称为 *余弦相似度*。给定余弦相似度 `cs`，我们可以通过运行
    `(2 – 2 * cs) ** 0.5` 或 `cs / (2 – cs)` 分别将其转换为欧几里得距离或 Tanimoto 相似度。
- en: Note The cosine is a very important function in trigonometry. It maps an angle
    between lines to a value ranging from –1 to 1\. If two lines point in an identical
    direction, then the angle between them is 0, and the cosine of the angle equals
    1\. If two lines point in opposite directions, then the angle between them is
    180 degrees, and the cosine of that angle equals –1\. Given a pair of vectors
    `v1` and `v2`, we can compute their cosine similarity by running `(v1 / norm(v1))
    @ (v2 / norm(v2))`. Then we can input that result into the inverse cosine function
    `np.arccos` to measure the angle between the vectors.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：余弦函数是三角学中一个非常重要的函数。它将两条线之间的角度映射到-1到1之间的值。如果两条线指向相同方向，则它们之间的角度为0，该角度的余弦值为1。如果两条线指向相反方向，则它们之间的角度为180度，该角度的余弦值为-1。给定一对向量`v1`和`v2`，我们可以通过运行`(v1
    / norm(v1)) @ (v2 / norm(v2))`来计算它们的余弦相似度。然后我们可以将那个结果输入到反余弦函数`np.arccos`中，以测量向量之间的角度。
- en: Listing 13.31 illustrates how easy it is to convert between the Tanimoto similarity,
    the cosine similarity, and the Euclidean distance. We compute the Tanimoto similarity
    between the query vector and each of our unit title vectors. The Tanimoto similarity
    is subsequently converted into the cosine similarity, and then the cosine similarity
    is converted into the Euclidean distance.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.31说明了在Tanimoto相似度、余弦相似度和欧几里得距离之间进行转换是多么容易。我们计算查询向量和我们的每个单位标题向量之间的Tanimoto相似度。随后将Tanimoto相似度转换为余弦相似度，然后将余弦相似度转换为欧几里得距离。
- en: Note Additionally, we utilize the cosine similarity to compute the angle between
    the vectors. We do this to emphasize how the cosine metric reflects the angle
    between line segments.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：此外，我们还利用余弦相似度来计算向量之间的角度。我们这样做是为了强调余弦度量如何反映线段之间的角度。
- en: Listing 13.31 Converting between unit vector metrics
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.31 在单位向量度量之间进行转换
- en: '[PRE30]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ normalized_tanimoto is a function of cosine_similarity. Using basic algebra,
    we can invert the function to solve for cosine_similarity.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `normalized_tanimoto`是`cosine_similarity`的一个函数。使用基本的代数，我们可以反转该函数以求解`cosine_similarity`。
- en: 'The Tanimoto similarity between normalized vectors can be transformed into
    other metrics of similarity or distance. This is useful for the following reasons:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化向量的Tanimoto相似度可以转换为其他相似度或距离度量。这有以下好处：
- en: Swapping the Tanimoto similarity for Euclidean distance allows us to carry out
    K-means clustering on text data. We discuss K-means clustering of texts in section
    15.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将Tanimoto相似度替换为欧几里得距离允许我们在文本数据上执行K-means聚类。我们将在第15节中讨论文本的K-means聚类。
- en: Swapping the Tanimoto similarity for cosine similarity simplifies our computational
    requirements. All our computations are reduced to basic dot product operations.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将Tanimoto相似度替换为余弦相似度简化了我们的计算需求。所有计算都简化为基本的点积运算。
- en: Note NLP practitioners commonly use the cosine similarity instead of the Tanimoto
    similarity. Research shows that in the long term, the Tanimoto similarity is more
    accurate than the cosine similarity. However, in many practical applications,
    the two similarities are interchangeable.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：NLP从业者通常使用余弦相似度而不是Tanimoto相似度。研究表明，从长远来看，Tanimoto相似度比余弦相似度更准确。然而，在许多实际应用中，这两种相似度是可以互换的。
- en: Common unit vector comparison metrics
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的单位向量比较度量
- en: '`u1 @ u2`—The cosine of the angle between unit vectors `u1` and `u2`'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`u1 @ u2`—单位向量`u1`和`u2`之间角度的余弦值'
- en: '`(u1 @ u2) / (2 - u1 @ u2)`—The Tanimoto similarity between unit vectors `u1`
    and `u2`'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(u1 @ u2) / (2 - u1 @ u2)`—单位向量`u1`和`u2`之间的Tanimoto相似度'
- en: '`(2 - 2 * u1 @ u2) ** 0.5`—The Euclidean distance between unit vectors `u1`
    and `u2`'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(2 - 2 * u1 @ u2) ** 0.5`—单位向量`u1`和`u2`之间的欧几里得距离'
- en: 'Vector normalization allows us to swap between multiple comparison metrics.
    Other benefits of normalization include these:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 向量归一化允许我们在多个比较度量之间进行切换。归一化的其他好处包括：
- en: '*Elimination of text length as a differentiating signal*—This lets us compare
    long and short texts with similar contents.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*消除文本长度作为区分信号*—这使我们能够比较内容相似的长文本和短文本。'
- en: '*More efficient Tanimoto similarity computation*—Only a single dot product
    operation is required.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*更有效地计算Tanimoto相似度*—只需要一个点积运算。'
- en: '*More efficient computation of the similarity between every pair of vectors*—This
    is called the *all-by-all* similarity.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*更有效地计算每对向量之间的相似度*—这被称为*全对全*相似度。'
- en: The last benefit has not yet been discussed. However, we will shortly learn
    that a table of cross-text similarities can be elegantly computed using *matrix
    multiplication*. In mathematics, matrix multiplication generalizes the dot product
    from one-dimensional vectors to two-dimensional arrays. The generalized dot product
    leads to the efficient computation of similarities across all pairs of texts.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个好处尚未讨论。然而，我们将很快了解到，可以使用 *矩阵乘法* 精巧地计算跨文本相似度表。在数学中，矩阵乘法将点积从一维向量推广到二维数组。推广的点积导致高效地计算所有文本对之间的相似度。
- en: 13.3 Matrix multiplication for efficient similarity calculation
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.3 矩阵乘法用于高效相似度计算
- en: When analyzing our *seashell*-centric texts, we compared each text pair individually.
    What if, instead, we visualized all pairwise similarities in a table? The rows
    and columns would correspond to individual texts, while the elements would correspond
    to Tanimoto similarities. The table would provide us with a bird’s-eye view of
    all the relationships between texts. We would finally learn whether `text2` is
    more similar to `text1` or `text3`.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 当分析以 *海螺* 为中心的文本时，我们逐个比较每个文本对。如果我们不是逐个比较，而是将所有成对相似度可视化在一个表中会怎样？行和列将对应于单个文本，而元素将对应于
    Tanimoto 相似度。这个表将为我们提供所有文本之间关系的鸟瞰图。我们最终将了解 `text2` 是否比 `text1` 或 `text3` 更相似。
- en: Let’s generate a table of normalized Tanimoto similarities, using the process
    outlined in figure 13.12\. We start by normalizing the TF vectors in our previously
    precomputed `tf_vectors` list. Then we iterate over every pair of vectors and
    compute their Tanimoto similarity. We store the similarities in a 2D `similarities`
    array, where `similarities[i][j]` equals the similarities between the *i*th text
    and the *j*th text. Finally, we visualize the `similarities` array using a heatmap
    (figure 13.13).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成一个归一化 Tanimoto 相似度表，使用图 13.12 中概述的过程。我们首先将先前预计算的 `tf_vectors` 列表中的 TF 向量进行归一化。然后我们遍历每一对向量并计算它们的
    Tanimoto 相似度。我们将相似度存储在一个二维 `similarities` 数组中，其中 `similarities[i][j]` 等于第 *i*
    个文本和第 *j* 个文本之间的相似度。最后，我们使用热图（图 13.13）可视化 `similarities` 数组。
- en: '![](../Images/13-12.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.12](../Images/13-12.png)'
- en: Figure 13.12 Transforming our three texts into a normalized matrix. The initial
    texts appear in the upper-left corner. These texts share a vocabulary of 15 unique
    words. We use the vocabulary to transform the texts into a matrix of word counts,
    which is in the upper-right corner. Its three rows correspond to the three texts,
    and its 15 columns track the occurrence count of every word in each text. We normalize
    these counts by dividing each row by its magnitude. The normalization produces
    the matrix in the lower-right corner. The dot product between any two rows in
    the normalized matrix equals the cosine similarity between the corresponding texts.
    Subsequently, running `cos` `/` `(2` `-` `cos)` transforms the cosine similarity
    into the Tanimoto similarity.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.12 将我们的三个文本转换为归一化矩阵。初始文本出现在左上角。这些文本共享 15 个独特的词汇。我们使用词汇将文本转换为词频矩阵，该矩阵位于右上角。其三行对应于三个文本，其
    15 列跟踪每个文本中每个词的出现次数。我们通过将每行除以其幅度来归一化这些计数。归一化产生了位于右下角的矩阵。归一化矩阵中任意两行的点积等于对应文本之间的余弦相似度。随后，运行
    `cos` `/` `(2` `-` `cos`) 将余弦相似度转换为 Tanimoto 相似度。
- en: Listing 13.32 Computing a table of normalized Tanimoto similarities
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.32 计算归一化 Tanimoto 相似度表
- en: '[PRE31]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ Creates a 2D array containing just zeros. We can more efficiently create this
    array by running np.zeros((num_texts, num_texts)). We fill this empty array with
    similarities between texts.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个只包含零的二维数组。我们可以通过运行 `np.zeros((num_texts, num_texts))` 来更高效地创建这个数组。我们将这个空数组填充为文本之间的相似度。
- en: '![](../Images/13-13.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.13](../Images/13-13.png)'
- en: Figure 13.13 A table of normalized Tanimoto similarities across text pairs.
    The table’s diagonal represents the similarity between each text and itself. Not
    surprisingly, that similarity is 1\. Ignoring the diagonal, we see that texts
    1 and 2 share the highest similarity. Meanwhile, texts 2 and 3 share the lowest
    similarity.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.13 显示了文本对之间的归一化 Tanimoto 相似度表。表的对角线表示每个文本与其自身之间的相似度。不出所料，这个相似度为 1。忽略对角线，我们看到文本
    1 和 2 之间的相似度最高。同时，文本 2 和 3 之间的相似度最低。
- en: Looking at the table is informative. We can immediately tell which text pairs
    share the highest similarity. However, our table computation relied on inefficient
    code. The following computations are redundant and can be eliminated.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 看表格是有信息的。我们可以立即判断哪些文本对具有最高的相似度。然而，我们的表格计算依赖于低效的代码。以下计算是冗余的，可以被消除。
- en: The creation of an empty three-by-three array
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个空的3x3数组
- en: The nested `for` loop iteration across all pairwise vector combinations
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有成对向量组合上嵌套的 `for` 循环迭代
- en: The individual computation of each pairwise vector similarity
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个成对向量相似度的单独计算
- en: We can purge our code of these operations using matrix multiplication. However,
    first we need to introduce basic matrix operations.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用矩阵乘法来清除这些操作。然而，首先我们需要介绍基本的矩阵运算。
- en: 13.3.1 Basic matrix operations
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3.1 基本矩阵运算
- en: Matrix operations power many subfields of data science, including NLP, network
    analysis, and machine learning. Therefore, knowing the basics of matrix manipulation
    is crucial for our data science careers. A *matrix* is the extension of a one-dimensional
    vector to two dimensions. In other words, a matrix is just a table of numbers.
    By that definition, `similarities` is a matrix, and so is `unit_vectors`. Most
    numeric tables discussed in this book are also naturally matrices.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵运算推动了数据科学许多子领域的发展，包括NLP、网络分析和机器学习。因此，了解矩阵操作的基本知识对我们数据科学职业生涯至关重要。*矩阵*是一维向量的二维扩展。换句话说，矩阵只是一个数字表。根据这个定义，`similarities`
    是一个矩阵，`unit_vectors` 也是一个矩阵。本书中讨论的大多数数值表也都是自然矩阵。
- en: Note Every matrix is a numeric table, but not every numeric table is a matrix.
    All matrix rows must share an equal length. The same is true of all matrix columns.
    Thus, if a table contains both a five-element column and a seven-element column,
    it is not a matrix.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：每个矩阵都是一个数值表，但并非每个数值表都是矩阵。所有矩阵的行必须具有相等的长度。所有矩阵的列也是如此。因此，如果一个表格包含一个五元素列和一个七元素列，它就不是矩阵。
- en: Since matrices are tables, they can be analyzed using Pandas. Conversely, numeric
    tables can be handled using 2D NumPy arrays. Both matrix representations are valid.
    In fact, Pandas DataFrames and NumPy arrays can sometimes be used interchangeably,
    because they share certain attributes. For instance, `matrix.shape` returns a
    count of rows and columns, regardless of whether `matrix` is a DataFrame or an
    array. Likewise, `matrix.T` transposes rows and columns, regardless of `matrix`
    type. Let’s confirm.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 由于矩阵是表格，因此可以使用Pandas进行分析。相反，数值表可以使用2D NumPy数组处理。这两种矩阵表示都是有效的。事实上，Pandas DataFrames和NumPy数组有时可以互换使用，因为它们共享某些属性。例如，`matrix.shape`
    返回行和列的数量，无论 `matrix` 是DataFrame还是数组。同样，`matrix.T` 也会根据 `matrix` 类型转置行和列。让我们来确认一下。
- en: Listing 13.33 Comparing Pandas and NumPy matrix attributes
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.33 比较Pandas和NumPy矩阵属性
- en: '[PRE32]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ Transposing the matrix flips the rows and columns.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 转置矩阵会翻转行和列。
- en: 'Pandas and NumPy table structures are similar. Nonetheless, there are certain
    benefits to storing matrices in 2D NumPy arrays. One immediate benefit is NumPy’s
    integration of Python’s built-in arithmetic operators: we can run basic arithmetic
    operations directly on NumPy arrays.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas和NumPy表结构相似。尽管如此，将矩阵存储在2D NumPy数组中有某些好处。一个直接的好处是NumPy集成了Python的内置算术运算符：我们可以在NumPy数组上直接运行基本的算术运算。
- en: NumPy matrix arithmetic operations
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy矩阵算术运算
- en: Occasionally in NLP we need to modify a matrix using basic arithmetic. Suppose,
    for instance, that we wish to compare a collection of documents based on both
    their bodies and titles. We hypothesize that title similarity is twice as important
    as body similarity since documents with similar titles are very likely to be thematically
    related. We thus decide to double the title similarity matrix to better weigh
    it relative to the body.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，我们有时需要使用基本的算术运算修改矩阵。例如，如果我们希望根据文档的正文和标题来比较文档集合，我们假设标题相似度是正文相似度的一半，因为具有相似标题的文档很可能在主题上是相关的。因此，我们决定将标题相似度矩阵加倍，以便更好地权衡它与正文的相关性。
- en: Note This relative importance of title versus body is particularly true in news
    articles. Two articles that share a similar title are very likely to refer to
    the same news story, even if their bodies offer different perspectives on that
    story. A good heuristic for measuring news article similarity is to compute `2
    * title_similarity + body_similarity`.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：标题与正文之间的这种相对重要性在新闻文章中尤其如此。两个标题相似的文章很可能指的是同一个新闻故事，即使它们的正文提供了对该故事的不同观点。衡量新闻文章相似度的一个良好启发式方法是计算
    `2 * title_similarity + body_similarity`。
- en: Doubling the values of a matrix is very easy to do in NumPy. For example, we
    can double our `similarities` matrix by running `2 * similarities`. We can also
    add `similarities` directly to itself by running `similarities + similarities`.
    Of course, the two arithmetic outputs will be equal. Meanwhile, running `similarities
    - similarities` will return a matrix of 0s. Furthermore, running `similarities
    - similarities - 1` will subtract 1 from each of the zeros.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NumPy 中加倍矩阵的值非常容易。例如，我们可以通过运行 `2 * similarities` 来加倍我们的 `similarities` 矩阵。我们也可以通过运行
    `similarities + similarities` 直接将 `similarities` 加到自身上。当然，这两个算术输出将是相等的。同时，运行 `similarities
    - similarities` 将返回一个全为 0 的矩阵。此外，运行 `similarities - similarities - 1` 将从每个零中减去
    1。
- en: Note We are subtracting `similarities + 1` from `similarities` simply to show
    the arithmetic flexibility of NumPy. Normally, there’s no valid reason to return
    this operation unless we really need a matrix of negative 1s.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们只是从 `similarities` 中减去 `similarities + 1` 来展示 NumPy 的算术灵活性。通常，除非我们真的需要一个全为
    -1 的矩阵，否则没有有效的理由返回这个操作。
- en: Listing 13.34 NumPy array addition and subtraction
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.34 NumPy 数组的加法和减法
- en: '[PRE33]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In the same manner, we can multiply and divide NumPy arrays. Running `similarities
    / similarities` will divide each similarity by itself, thus returning a matrix
    of 1s. Meanwhile, running `similarities * similarities` will return a matrix of
    squared similarity values.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 以同样的方式，我们可以对 NumPy 数组进行乘法和除法。运行 `similarities / similarities` 将将每个相似度除以自身，从而返回一个全为
    1 的矩阵。同时，运行 `similarities * similarities` 将返回一个平方相似度值的矩阵。
- en: Listing 13.35 NumPy array multiplication and division
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.35 NumPy 数组的乘法和除法
- en: '[PRE34]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Matrix arithmetic lets us conveniently transition between similarity matrix
    types. For instance, we can convert our Tanimoto matrix into a cosine similarity
    matrix simply by running `2 * similarities / (1 + similarities)`. Thus, if we
    wish to compare the Tanimoto similarity with the more popular cosine similarity,
    we can compute the second cosine matrix in just a single line of code.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵算术使我们能够方便地在相似矩阵类型之间进行转换。例如，我们可以通过运行 `2 * similarities / (1 + similarities)`
    将我们的 Tanimoto 矩阵转换为余弦相似度矩阵。因此，如果我们想比较 Tanimoto 相似度与更流行的余弦相似度，我们只需一行代码就能计算出第二个余弦矩阵。
- en: Listing 13.36 Converting between matrix similarity-types
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.36 在矩阵相似度类型之间转换
- en: '[PRE35]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ❶ Confirms that the cosine similarity equals the actual vector dot product
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 确认余弦相似度等于实际的向量点积
- en: ❷ Rounds the results because of floating-point errors
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 由于浮点数错误而四舍五入结果
- en: NumPy 2D arrays confer additional benefits over Pandas. Accessing rows and columns
    by index is much more straightforward in NumPy.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 2D 数组比 Pandas 提供了额外的优势。在 NumPy 中通过索引访问行和列要直接得多。
- en: NumPy matrix row and column operations
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 矩阵的行和列操作
- en: Given any 2D `matrix` array, we can access the row at index `i` by running `matrix[i]`.
    Likewise, we can access the column at index `j` by running `matrix[:,j]`. Let’s
    use NumPy indexing to print the first row and column of both `unit_vectors` and
    `similarities`.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何二维 `matrix` 数组，我们可以通过运行 `matrix[i]` 来访问索引为 `i` 的行。同样，我们可以通过运行 `matrix[:,j]`
    来访问索引为 `j` 的列。让我们使用 NumPy 索引来打印 `unit_vectors` 和 `similarities` 的第一行和第一列。
- en: Listing 13.37 Accessing NumPy matrix rows and columns
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.37 访问 NumPy 矩阵的行和列
- en: '[PRE36]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: All printed rows and columns are one-dimensional NumPy arrays. Given two arrays,
    we can compute their dot product, but only if the array lengths are the same.
    In our output, both `similarities[0].size` and `unit_vectors[:,0].size` are equal
    to 3\. Hence, we can take the dot product between the first row of `similarities`
    and the first column of `unit_vectors`. This particular row-to-column dot product
    is not useful in our text analysis, but it serves to illustrate our ability to
    easily compute the dot product between matrix rows and matrix columns. A little
    later, we will use that ability to compute text vector similarities with great
    efficiency.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 所有打印的行和列都是一维 NumPy 数组。给定两个数组，我们可以计算它们的点积，但只有当数组长度相同时。在我们的输出中，`similarities[0].size`
    和 `unit_vectors[:,0].size` 都等于 3。因此，我们可以计算 `similarities` 的第一行和 `unit_vectors`
    的第一列之间的点积。这种特定的行到列点积在我们的文本分析中并不有用，但它有助于说明我们轻松计算矩阵行和矩阵列之间点积的能力。稍后，我们将利用这种能力以极高的效率计算文本向量的相似度。
- en: Listing 13.38 Computing the dot product between a row and column
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.38 计算行和列之间的点积
- en: '[PRE37]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In that same vein, we can take the dot product between every row in `similarities`
    and every column in `unit_vectors`. Let’s print all possible dot product results.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们可以在 `similarities` 的每一行和 `unit_vectors` 的每一列之间计算点积。让我们打印出所有可能点积的结果。
- en: Listing 13.39 Computing dot products between all rows and columns
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.39 计算所有行和列之间的点积
- en: '[PRE38]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We’ve generated 45 dot products: one for each row, column combination. Our
    printed outputs are excessive. These outputs can be stored more concisely in a
    table called `dot_products`, where `dot_products[i][j]` is equal to `similarities[i]
    @ unit_vectors[:,j]`. Of course, by definition, that table is a matrix.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经生成了 45 个点积：每个行和列组合一个。我们的打印输出过多。这些输出可以更简洁地存储在一个名为 `dot_products` 的表中，其中 `dot_products[i][j]`
    等于 `similarities[i] @ unit_vectors[:,j]`。当然，根据定义，该表是一个矩阵。
- en: Listing 13.40 Storing all-by-all dot products in a matrix
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.40 将所有行和列之间的点积存储在矩阵中
- en: '[PRE39]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ❶ Returns an empty array of zeros
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 返回一个全为零的空数组
- en: The operation we’ve just executed is called a *matrix product*. It’s a generalization
    of the vector dot product to two dimensions. Given two matrices, `matrix_a` and
    `matrix_b`, we can compute their product by calculating `matrix_c`, where `matrix_c[i][j]`
    is equal to `matrix_a[i] @ matrix_b[:,j]` (figure 13.14). Matrix products are
    crucial to many modern technological advancements. They power the ranking algorithms
    in massive search engines like Google, serve as the foundation for techniques
    used to train self-driving cars, and underlie much of modern NLP. The usefulness
    of matrix products will become apparent shortly, but first we must discuss matrix
    product operations in more detail.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才执行的操作称为 *矩阵乘积*。它是向量点积到二维的推广。给定两个矩阵 `matrix_a` 和 `matrix_b`，我们可以通过计算 `matrix_c`
    来计算它们的乘积，其中 `matrix_c[i][j]` 等于 `matrix_a[i] @ matrix_b[:,j]`（图 13.14）。矩阵乘积对于许多现代技术进步至关重要。它们为像
    Google 这样的搜索引擎的排名算法提供动力，是训练自动驾驶汽车所使用技术的基石，也是现代自然语言处理的基础。矩阵乘积的实用性将很快显现出来，但首先我们必须更详细地讨论矩阵乘积操作。
- en: '![](../Images/13-14.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13-14.png)'
- en: Figure 13.14 Computing the matrix product of matrices A and B. The operation
    outputs a new matrix. Each element in the *i*th row and *j*th column of the output
    equals the dot product between the *i*th row of A and the *j*th column of B. For
    instance, the element in the first row and second column of the output equals
    a[1,1] * b[1,2] + a[1,2] * b[2,2]. Meanwhile, the element in the third row and
    third column of the output equals a[3,1] * b[1,3] + a[3,2] * b[2,3].
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.14 计算矩阵 A 和 B 的矩阵乘积。该操作输出一个新的矩阵。输出矩阵的第 *i* 行和第 *j* 列的每个元素等于 A 的第 *i* 行与
    B 的第 *j* 列之间的点积。例如，输出矩阵的第一行第二列的元素等于 a[1,1] * b[1,2] + a[1,2] * b[2,2]。同时，输出矩阵的第三行第三列的元素等于
    a[3,1] * b[1,3] + a[3,2] * b[2,3]。
- en: NumPy matrix products
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 矩阵乘积
- en: Naively, we can calculate `matrix_c` by running nested `for` loops across `matrix_a`
    and `matrix_b`. This technique is not efficient. Conveniently, NumPy’s product
    operator `@` can be applied to 2D matrices as well as 1D arrays. If `matrix_a`
    and `matrix_b` are both NumPy arrays, then `matrix_c` equals `matrix_a @ matrix_b`.
    Thus, the matrix product of `similarities` and `unit_vectors` equals `similarities
    @ unit_vectors`. Let’s confirm.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 天真地，我们可以通过在`matrix_a`和`matrix_b`上运行嵌套的`for`循环来计算`matrix_c`。这种技术效率不高。方便的是，NumPy的产品运算符`@`可以应用于二维矩阵以及一维数组。如果`matrix_a`和`matrix_b`都是NumPy数组，那么`matrix_c`等于`matrix_a
    @ matrix_b`。因此，`similarities`和`unit_vectors`的矩阵乘积等于`similarities @ unit_vectors`。让我们来验证一下。
- en: Listing 13.41 Computing a matrix product using NumPy
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.41 使用NumPy计算矩阵乘法
- en: '[PRE40]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: ❶ Asserts that all the elements of matrix_product are nearly identical to all
    the elements of dot_products. There are tiny differences in results due to floating-point
    errors.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 断言矩阵乘积的所有元素几乎与点积的所有元素完全相同。由于浮点数误差，结果存在微小差异。
- en: What will happen if we flip our input matrices and run `unit_vectors @ similarities`?
    NumPy will throw an error! The computation takes the vector dot product between
    rows in `unit_vectors` and columns in `similarities`, but these rows and columns
    have different lengths. Therefore, the computation is not possible (figure 13.15).
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们翻转输入矩阵并运行`unit_vectors @ similarities`会发生什么？NumPy将抛出一个错误！计算将取`unit_vectors`中的行和`similarities`中的列之间的向量点积，但这些行和列的长度不同。因此，计算是不可能的（图13.15）。
- en: '![](../Images/13-15.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13-15.png)'
- en: Figure 13.15 An erroneous effort to compute the matrix product of matrices A
    and B. Matrix A has three columns in each row, and matrix B has four rows in each
    column. We cannot take the dot product between a three-element row and a four-element
    column. Thus, running `A` `@` `B` causes an error.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.15 计算矩阵A和B的矩阵乘法的一个错误尝试。矩阵A的每一行有三个列，矩阵B的每一列有四个行。我们不能在三个元素的行和四个元素的列之间取点积。因此，运行`A`
    `@` `B`会导致错误。
- en: Listing 13.42 Computing an erroneous matrix product
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.42 计算错误的矩阵乘法
- en: '[PRE41]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The matrix product is order dependent. The output of `matrix_a @ matrix_b`
    is not necessarily the same as `matrix_b @ matrix_a`. In words, we can distinguish
    between `matrix_a @ matrix_b` and `matrix_b @ matrix_a` as follows:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法是顺序相关的。`matrix_a @ matrix_b`的输出不一定与`matrix_b @ matrix_a`相同。换句话说，我们可以这样区分`matrix_a
    @ matrix_b`和`matrix_b @ matrix_a`：
- en: '`matrix_a @ matrix_b` is the product of `matrix_a` and `matrix_b`.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matrix_a @ matrix_b`是`matrix_a`和`matrix_b`的乘积。'
- en: '`matrix_b @ matrix_a` is the product of `matrix_b` and `matrix_a`.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matrix_b @ matrix_a`是`matrix_b`和`matrix_a`的乘积。'
- en: In mathematics, the words *product* and *multiplication* are often interchangeable.
    Thus, computing the matrix product is commonly called *matrix multiplication*.
    That name is so ubiquitous that NumPy includes an `np.matmul` function. The output
    of `np.matmul (matrix_a, matrix_b)` is identical to `matrix_a @ matrix_b`.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学中，*乘积*和*乘法*这两个词经常可以互换。因此，计算矩阵乘法通常被称为*矩阵乘法*。这个名字如此普遍，以至于NumPy包含了一个`np.matmul`函数。`np.matmul
    (matrix_a, matrix_b)`的输出与`matrix_a @ matrix_b`相同。
- en: Listing 13.43 Running matrix multiplication using `matmul`
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.43 使用`matmul`运行矩阵乘法
- en: '[PRE42]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Common NumPy matrix operations
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的NumPy矩阵操作
- en: '`matrix.shape`—Returns a tuple containing the row count and column count in
    the matrix.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matrix.shape`—返回一个包含矩阵行数和列数的元组。'
- en: '`matrix.T`—Returns a transposed matrix where rows and columns are swapped.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matrix.T`—返回一个行列互换的转置矩阵。'
- en: '`matrix[i]`—Returns the *i*th row in the matrix.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matrix[i]`—返回矩阵中的第*i*行。'
- en: '`matrix[:,j]`—Returns the *j*th column in the matrix.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matrix[:,j]`—返回矩阵中的第*j*列。'
- en: '`k * matrix`—Multiplies each element of the matrix by a constant `k`.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`k * matrix`—将矩阵的每个元素乘以常数`k`。'
- en: '`matrix + k`—Adds a constant `k` to each element of the matrix.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matrix + k`—将常数`k`加到矩阵的每个元素上。'
- en: '`matrix_a + matrix_b`—Adds each element of `matrix_a` to `matrix_b`. Equivalent
    to running `matrix_c[i][j] = matrix_a[i][j] + matrix_b[i][j]` for every possible
    `i` and `j`.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matrix_a + matrix_b`—将`matrix_a`的每个元素加到`matrix_b`上。相当于对每个可能的`i`和`j`执行`matrix_c[i][j]
    = matrix_a[i][j] + matrix_b[i][j]`。'
- en: '`matrix_a * matrix_b`—Multiplies each element of `matrix_a` with an element
    of `matrix_b`. Equivalent to running `matrix_c[i][j] = matrix_a[i][j] * matrix_b[i][j]`
    for every possible `i` and `j`.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matrix_a * matrix_b`—将`matrix_a`的每个元素与`matrix_b`的元素相乘。相当于对每个可能的`i`和`j`执行`matrix_c[i][j]
    = matrix_a[i][j] * matrix_b[i][j]`。'
- en: '`matrix_a @ matrix_b`—Returns the matrix product of `matrix_a` and `matrix_b`.
    Equivalent to running `matrix_c[i][j] = matrix_a[i] @ matrix_b[;,j]` for every
    possible `i` and `j`.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matrix_a @ matrix_b`—返回`matrix_a`和`matrix_b`的矩阵乘积。相当于为每个可能的*i*和*j*执行`matrix_c[i][j]
    = matrix_a[i] @ matrix_b[;,j]`。'
- en: '`np.matmult(matrix_a, matrix_b)`—Returns the matrix product of `matrix_a` and
    `matrix_b`, without relying on the `@` operator.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`np.matmult(matrix_a, matrix_b)`—返回`matrix_a`和`matrix_b`的矩阵乘积，不依赖于`@`运算符。'
- en: NumPy lets us execute matrix multiplication without relying on nested `for`
    loops. This improvement is more than just cosmetic. Standard Python `for` loops
    are designed to run on generalized data lists; they are not optimized for numbers.
    Meanwhile, NumPy cleverly optimizes its array iterations. Consequently, matrix
    multiplication is noticeably faster when run in NumPy.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy允许我们执行矩阵乘法而不依赖于嵌套的`for`循环。这种改进不仅仅是外观上的。标准的Python `for`循环设计用于运行通用数据列表；它们不是针对数字优化的。与此同时，NumPy巧妙地优化了其数组迭代。因此，在NumPy中运行矩阵乘法时，速度会明显提高。
- en: Let’s compare the matrix product speed between NumPy and regular Python (figure
    13.16). Listing 13.44 plots the product speed across matrices of variable sizes
    using both NumPy and Python `for` loops. Python’s built-in `time` module is employed
    to time the matrix multiplications.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较NumPy和常规Python（图13.16）之间的矩阵乘积速度。列表13.44使用NumPy和Python的`for`循环绘制了不同大小矩阵的乘积速度。Python内置的`time`模块被用来计时矩阵乘法。
- en: '![](../Images/13-16.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/13-16.png)'
- en: Figure 13.16 Matrix size plotted against product running times for NumPy and
    regular Python. NumPy is vastly faster than regular Python.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.16 NumPy和常规Python的矩阵大小与乘积运行时间的对比。NumPy比常规Python快得多。
- en: Note Running times will fluctuate depending on the local state of the machine
    that is executing the code.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 运行时间将根据执行代码的机器的本地状态而波动。
- en: Listing 13.44 Comparing matrix product running times
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.44 比较矩阵乘积运行时间
- en: '[PRE43]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: ❶ Creates a size-by-size matrix of ones, where size ranges from 1 to 100
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个大小为1到100的矩阵，每个元素都是1
- en: ❷ Returns the current time in seconds
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 返回当前时间（以秒为单位）
- en: ❸ Stores the matrix product speed in NumPy
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将矩阵乘积速度存储在NumPy中
- en: ❹ Stores the matrix product speed of a Python for loop
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 存储Python循环的矩阵乘积速度
- en: When it comes to matrix multiplication, NumPy greatly outperforms basic Python.
    NumPy matrix product code is more efficient to run and also to write. We will
    now use NumPy to compute our all-by-all text similarities with maximum efficiency.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到矩阵乘法时，NumPy比基本的Python要高效得多。NumPy的矩阵乘积代码运行效率更高，编写起来也更方便。我们现在将使用NumPy以最高效的方式计算所有文本之间的相似度。
- en: 13.3.2 Computing all-by-all matrix similarities
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3.2 计算所有向量的矩阵相似度
- en: We’ve previously computed our text similarities by iterating over a `unit_vectors`
    matrix. This matrix holds the normalized TF vectors for our three seashell texts.
    What will happen if we multiply `unit_vectors` by `unit_vectors.T`? Well, `unit_vectors.T`
    is a transpose of `unit_vectors.` Therefore, each column `i` in the transpose
    equals row `i` in `unit_vectors`. Taking the dot product of `unit_vectors[i]`
    and `unit_vectors.T[:,i]` will return the cosine similarity between a unit vector
    and itself, as shown in figure 13.17\. That similarity, of course, will equal
    1\. By this logic, `unit_vectors[i] @ unit_vectors[j].T` equals the cosine similarity
    between the *i*th and *j*th vectors. Consequently, `unit_vectors @ unit_vectors.T`
    returns a matrix of all-by-cosine similarities. The matrix should equal our previously
    computed `cosine_similarities` array. Let’s confirm.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前通过遍历`unit_vectors`矩阵来计算文本相似度。这个矩阵存储了我们三个贝壳文本的归一化TF向量。如果我们用`unit_vectors`乘以`unit_vectors.T`会发生什么？嗯，`unit_vectors.T`是`unit_vectors`的转置。因此，转置中的每一列`i`等于`unit_vectors`中的行`i`。计算`unit_vectors[i]`和`unit_vectors.T[:,i]`的点积将返回一个单位向量与其自身的余弦相似度，如图13.17所示。当然，这个相似度将等于1。按照这个逻辑，`unit_vectors[i]
    @ unit_vectors[j].T`等于第*i*个和第*j*个向量之间的余弦相似度。因此，`unit_vectors @ unit_vectors.T`返回一个所有向量之间的余弦相似度矩阵。这个矩阵应该等于我们之前计算的`cosine_similarities`数组。让我们来验证一下。
- en: '![](../Images/13-17.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/13-17.png)'
- en: Figure 13.17 Computing the dot product between A and the transpose of A. The
    operation outputs a new matrix. Each element in the *i*th row and *j*th column
    of the output equals the dot product between the *i*th row of A and the *j*th
    column of A. Thus, the element in the third row and third column of the output
    equals the dot product of A[2] with itself. If matrix A is normalized, then that
    dot product will equal 1.0.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.17 计算矩阵 A 与其转置 A 的点积。该操作输出一个新的矩阵。输出矩阵的第 *i* 行和第 *j* 列的每个元素等于 A 的第 *i* 行与
    A 的第 *j* 列的点积。因此，输出矩阵的第三行和第三列的元素等于 A[2] 与其自身的点积。如果矩阵 A 是归一化的，那么这个点积将等于 1.0。
- en: Listing 13.45 Obtaining cosines from a matrix product
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.45 从矩阵乘积中获取余弦值
- en: '[PRE44]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Each element in `cosine_matrix` equals the cosine of the angle between two vectorized
    texts. That cosine can be transformed into a Tanimoto value, which generally reflects
    word overlap and divergence between texts. Using NumPy arithmetic, we can convert
    `cosine_matrix` into a Tanimoto similarity matrix by running `cosine_matrix /
    (2 - cosine_matrix)`.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '`cosine_matrix` 中的每个元素等于两个向量文本之间角度的余弦值。这个余弦值可以转换成 Tanimoto 值，这通常反映了文本之间的词重叠和差异。使用
    NumPy 算术运算，我们可以通过运行 `cosine_matrix / (2 - cosine_matrix)` 将 `cosine_matrix` 转换成一个
    Tanimoto 相似度矩阵。'
- en: Listing 13.46 Converting cosines to a Tanimoto matrix
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.46 将余弦值转换为 Tanimoto 矩阵
- en: '[PRE45]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We’ve computed all the Tanimoto similarities in just two lines of code. We
    can also compute these similarities by inputting `unit_vectors` and `unit_vectors.T`
    directly into our `normalized_tanimoto` function. As a reminder, the function
    does the following:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅用两行代码就计算了所有的 Tanimoto 相似度。我们也可以通过直接将 `unit_vectors` 和 `unit_vectors.T` 输入到我们的
    `normalized_tanimoto` 函数中来计算这些相似度。提醒一下，该函数执行以下操作：
- en: Takes as input two NumPy arrays. Their dimensionality is not constrained.
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入两个 NumPy 数组。它们的维度没有限制。
- en: Applies the `@` operator to the NumPy arrays. If the arrays are matrices, the
    operation returns a matrix product.
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `@` 运算符应用于 NumPy 数组。如果数组是矩阵，则操作返回矩阵乘积。
- en: Uses arithmetic to modify the product. The arithmetic operations can be equally
    applied to both numbers and matrices.
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用算术运算来修改乘积。算术运算可以同样应用于数字和矩阵。
- en: Hence it’s not surprising that `normalized_tanimoto(unit_vectors, unit_vectors.T)`
    returns an output equal to `tanimoto_matrix`.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`normalized_tanimoto(unit_vectors, unit_vectors.T)` 返回的输出等于 `tanimoto_matrix`。
- en: Listing 13.47 Inputting matrices into `normalized_tanimoto`
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.47 将矩阵输入到 `normalized_tanimoto`
- en: '[PRE46]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Given a matrix of normalized TF vectors, we can compute their all-by-all similarities
    in a single, efficient line of code.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个归一化 TF 向量矩阵，我们可以通过一行高效的代码计算它们的所有元素之间的相似度。
- en: Common normalized matrix comparisons
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的归一化矩阵比较
- en: '`norm_matrix @ norm_matrix.T`—Returns a matrix of all-by-all cosine similarities'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm_matrix @ norm_matrix.T` — 返回所有元素之间的余弦相似度矩阵'
- en: '`norm_matrix @ norm_matrix.T / ( 2 - norm_matrix @ norm_matrix.T)`—Returns
    a matrix of all-by-all Tanimoto similarities'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm_matrix @ norm_matrix.T / (2 - norm_matrix @ norm_matrix.T)` — 返回所有元素之间的
    Tanimoto 相似度矩阵'
- en: 13.4 Computational limits of matrix multiplication
  id: totrans-367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.4 矩阵乘法的计算限制
- en: Matrix multiplication speed is determined by the matrix size. NumPy may optimize
    for speed, but even NumPy has its limits. These limits become obvious when we
    compute real-world text matrix products. Issues arise from the matrix column count,
    which is dependent on vocabulary size. The total words in a vocabulary can spiral
    out of control when we begin to compare nontrivial texts.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法的速度由矩阵大小决定。NumPy 可能会优化速度，但即使是 NumPy 也有其限制。当我们计算实际的文本矩阵乘积时，这些限制变得明显。问题源于矩阵的列数，这取决于词汇表的大小。当开始比较非平凡文本时，词汇表中的总词数可能会失控。
- en: Consider, for instance, the analysis of novels. The average novel contains roughly
    5,000 to 10,000 unique words. *The Hobbit*, for example, contains 6,175 unique
    words. Meanwhile, *A Tale of Two Cities* contains 9,699 unique words. Some of
    the words overlap between the two novels; others do not. Together, the novels
    share a vocabulary size of 12,138 words. We can also throw a third novel into
    the mix. Adding *The Adventures of Tom Sawyer* expands the vocabulary size to
    13,935 words. At this rate, adding 27 more novels will expand the vocabulary size
    to approximately 50,000 words.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 以小说分析为例。平均小说包含大约 5,000 到 10,000 个独特的单词。例如，*《霍比特人》*包含 6,175 个独特的单词。而*《双城记》*包含
    9,699 个独特的单词。两个小说之间的一些单词重叠；而另一些则不重叠。这两个小说共同拥有 12,138 个单词的词汇量。我们还可以加入第三部小说。加入*《汤姆·索亚历险记》*将词汇量扩大到
    13,935 个单词。按照这个速度，再加入 27 部小说将词汇量扩大到大约 50,000 个单词。
- en: Let’s assume that 30 novels require a shared vocabulary containing 50,000 words.
    Furthermore, let’s assume we take an all-by-all similarity across the 30 books.
    How much time is required to compute these similarities? Let’s find out! We’ll
    create a 30-book by 50,000-word `book_matrix`. All rows in the matrix will be
    normalized. Then we’ll measure the running time of `normalized_tanimoto(book_matrix,
    book_ matrix.T)`.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 30 部小说需要一个包含 50,000 个单词的共享词汇表。此外，假设我们对 30 本书进行全对全相似度比较。计算这些相似度需要多少时间？让我们来看看！我们将创建一个
    30 本书乘以 50,000 个单词的`book_matrix`。矩阵中的所有行都将进行归一化。然后我们将测量`normalized_tanimoto(book_matrix,
    book_matrix.T)`的运行时间。
- en: Note The purpose of our experiment is to test the impact of the matrix column
    count on running time. Here, the actual matrix contents don’t matter. We thus
    oversimplify the situation by setting all word counts to 1\. Consequently, the
    normalized values in each row equal `1 / 5000`. In a real-world setting, this
    would not be the case. Also, note that it’s possible to optimize running time
    by tracking all zero-value matrix elements. Here, we don’t consider the impact
    of zero values on matrix multiplication speed.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们实验的目的是测试矩阵列数对运行时间的影响。在这里，实际的矩阵内容并不重要。因此，我们通过将所有单词计数设置为1来简化情况。因此，每行的归一化值等于`1
    / 5000`。在现实世界的设置中，情况并非如此。另外，请注意，通过跟踪所有零值矩阵元素，可以优化运行时间。在这里，我们不考虑零值对矩阵乘法速度的影响。
- en: Listing 13.48 Timing an all-by-all comparison of 30 novels
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.48 测量 30 部小说的全对全比较时间
- en: '[PRE47]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: ❶ The function computes the running time on a book_count-by-50,000 matrix. The
    function is reused in the next two code listings.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 该函数在 book_count-by-50,000 矩阵上计算运行时间。该函数在接下来的两个代码列表中重复使用。
- en: The similarity matrix took approximately 5 milliseconds to compute. This is
    a reasonable running time. Will it stay reasonable as the number of analyzed books
    continues to increase? Let’s check. We’ll plot the running times across multiple
    book counts ranging from 30 to nearly 1,000 (figure 13.18). For consistency’s
    sake, we’ll keep the vocabulary size at 50,000.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 相似度矩阵的计算大约需要 5 毫秒。这是一个合理的运行时间。随着分析书籍数量的增加，它是否会保持合理？让我们检查一下。我们将绘制从 30 到近 1,000（图
    13.18）的多个书籍数量的运行时间。为了保持一致性，我们将词汇量保持在 50,000。
- en: '![](../Images/13-18.png)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13-18.png)'
- en: Figure 13.18 Book counts plotted against the running time of text comparisons.
    The running times increase quadratically.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.18 将书籍数量与文本比较的运行时间进行对比。运行时间呈二次增长。
- en: Listing 13.49 Plotting book counts vs. running times
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.49 绘制书籍数量与运行时间的关系图
- en: '[PRE48]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: ❶ We do not sample every single book count—the accumulated running time would
    be too slow.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们不会对每个单个书籍数量进行采样——累积的运行时间会太慢。
- en: ❷ Generates a scatter plot instead of a curve. Figure 13.18 fits the discrete
    points to a continuous parabolic curve.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 生成散点图而不是曲线。图 13.18 将离散点拟合到连续的抛物线曲线。
- en: The similarity running time rises quadratically with book count. At 1,000 books,
    the running time increases to approximately 0.27 seconds. This delay is tolerable.
    However, if we increase the book count even further, the delay is no longer acceptable.
    We can show this using simple math. Our plotted curve takes on a parabolic shape
    defined by `y = n * (x ** 2)`. When `x` is approximately 1,000, `y` equals approximately
    `0.27`. Thus, we can model our running times using the equation `y = (0.27 / (1000
    ** 2)) * (x ** 2)`. Let’s confirm by plotting equation outputs together with our
    precomputed measurements (figure 13.19). The two plots should mostly overlap.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 相似度运行时间随着书籍数量的增加呈二次方增长。当书籍数量达到1,000本时，运行时间增加到大约0.27秒。这种延迟是可以容忍的。然而，如果我们进一步增加书籍数量，延迟就不再可以接受了。我们可以用简单的数学来证明这一点。我们绘制的曲线呈现出由
    `y = n * (x ** 2)` 定义的抛物线形状。当 `x` 大约是1,000时，`y` 大约等于 `0.27`。因此，我们可以用方程 `y = (0.27
    / (1000 ** 2)) * (x ** 2)` 来模拟我们的运行时间。让我们通过绘制方程输出和我们的预计算测量结果（图13.19）来验证。这两个图表应该大部分重叠。
- en: Listing 13.50 Modeling running times using a quadratic curve
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.50 使用二次曲线模拟运行时间
- en: '[PRE49]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![](../Images/13-19.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/13-19.png)'
- en: Figure 13.19 A quadratic curve plotted together with running times. The shape
    of the curve overlaps with the running time scatter plot.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.19 抛物线与运行时间的绘图。曲线的形状与运行时间散点图重叠。
- en: Our plotted equation overlaps with the measured times. Thus, we can use the
    equation to predict the speed of larger book comparisons. Let’s see how long it
    will take to measure the similarity across 300,000 books.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 我们绘制的方程与测量时间重叠。因此，我们可以使用这个方程来预测更大书籍比较的速度。让我们看看测量300,000本书之间的相似度需要多长时间。
- en: Note 300,000 may seem like an unusually large number. However, it reflects the
    200,000 to 300,000 English-language novels that are published every year. If we
    wish to compare all the novels published in a year, we need to multiply matrices
    containing more than 200,000 rows.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，300,000可能看起来是一个异常大的数字。然而，它反映了每年出版的200,000到300,000本英语小说。如果我们希望比较一年内出版的所有小说，我们需要乘以包含超过200,000行的矩阵。
- en: Listing 13.51 Predicting the running time for 300,000 books
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.51 预测300,000本书的运行时间
- en: '[PRE50]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: ❶ Divides by 3600 to convert seconds into hours
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过除以3600将秒转换为小时
- en: It will take nearly 7 hours to compare 300,000 books. This delay in time is
    not acceptable, especially in industrial NLP systems, which are designed to process
    millions of texts in mere seconds. We need to somehow reduce the running time.
    One approach is to reduce the matrix size.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 比较300,000本书需要近7小时。这种时间延迟是不可接受的，尤其是在工业NLP系统中，这些系统旨在在几秒钟内处理数百万文本。我们需要以某种方式减少运行时间。一种方法是通过减少矩阵大小。
- en: 'Our matrix is too large, partially because of column size. Each row contains
    50,000 columns corresponding to 50,000 words. However, in a real-world setting,
    not all words are distributed equally. While some words are common across novels,
    other words may appear only once. For example, take the lengthy novel *Moby Dick*
    : 44% of the words in it are mentioned once and never used again. Some of the
    words are rarely mentioned in other novels. Removing them will lower the column
    size.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的矩阵太大，部分原因是列的大小。每一行包含50,000列，对应于50,000个单词。然而，在现实世界的设置中，并非所有单词的分布都是均匀的。虽然有些单词在小说中很常见，但其他单词可能只出现一次。例如，以长篇小说《白鲸》为例：其中44%的单词只出现一次，之后再也没有使用过。有些单词在其他小说中很少出现。移除它们将降低列的大小。
- en: On the opposite end of the spectrum are common words that appear in every novel.
    Words like *the* do not provide a differentiating signal between texts. Removing
    common words will also reduce the column size.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在光谱的另一端是每个小说中都出现的常见单词。像 *the* 这样的单词在文本之间不提供区分信号。移除常见单词也会减少列的大小。
- en: It’s possible to systematically reduce the dimensions of each matrix row from
    50,000 to a more reasonable value. In the next section, we introduce a series
    of dimension-reduction techniques to shrink the shape of any inputted matrix.
    Reducing the dimensions of text matrices greatly lowers the running times of common
    NLP computations.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能系统地减少每个矩阵行的维度，从50,000减少到一个更合理的值。在下一节中，我们将介绍一系列降维技术，以缩小任何输入矩阵的形状。降低文本矩阵的维度可以大大降低常见NLP计算的运行时间。
- en: Summary
  id: totrans-396
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: We can compare texts using the *Jaccard similarity*. This similarity metric
    equals the fraction of total unique words that are shared between two texts.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用 *Jaccard相似度* 来比较文本。这个相似度指标等于两个文本之间共享的总独特单词的分数。
- en: We can compute the Jaccard similarity by transforming our texts into binary
    vectors of 1s and 0s. Taking the *dot product* of two binary text vectors returns
    the shared word count between the texts. Meanwhile, the dot product of a text
    vector with itself returns the total count of words in the text. These values
    are sufficient to compute the Jaccard similarity.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过将我们的文本转换为1和0的二进制向量来计算Jaccard相似度。两个二进制文本向量的点积返回文本之间的共享词数。同时，文本向量与其自身的点积返回文本中的总词数。这些值足以计算Jaccard相似度。
- en: The *Tanimoto similarity* generalizes Jaccard to include non-binary vectors.
    This allows us to compare vectors of word counts, which are referred to as *TF
    vectors*.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tanimoto相似度**将Jaccard推广到包括非二进制向量。这使得我们能够比较词频向量，这些向量被称为**TF向量**。'
- en: '*TF vector similarity* is overly dependent on text size. We can eliminate this
    dependence using *normalization*. A vector can be normalized by first computing
    its *magnitude*, which is the vector’s distance to the origin. Dividing a vector
    by its magnitude produces a normalized unit vector.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TF向量相似度**过度依赖于文本大小。我们可以通过**归一化**来消除这种依赖。通过首先计算向量的**大小**，即向量到原点的距离，来归一化一个向量。将向量除以其大小产生一个归一化的单位向量。'
- en: The magnitude of a *unit vector* is 1\. Furthermore, the Tanimoto similarity
    is partially dependent on vector magnitude. Thus, we can simplify the similarity
    function if it’s run exclusively on unit vectors. Moreover, that unit vector similarity
    can be converted into other common metrics, such as *cosine similarity* and distance.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单位向量的**大小**为1。此外，Tanimoto相似度部分依赖于向量大小。因此，如果仅对单位向量运行相似度函数，我们可以简化相似度函数。此外，单位向量相似度可以转换为其他常见指标，例如**余弦相似度**和距离。
- en: We can efficiently compute all-by-all similarities using *matrix multiplication*.
    A *matrix* is just a 2D table of numbers. We can multiply two matrices by taking
    pairwise dot products between every matrix row and matrix column. If we multiply
    a normalized matrix by its transpose, we produce a matrix of all-by-all cosine
    similarities. Using NumPy’s matrix arithmetic, we transform these cosine similarities
    into Tanimoto similarities.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用**矩阵乘法**有效地计算所有对之间的相似度。矩阵只是一个数字的二维表。我们可以通过将每个矩阵行与矩阵列之间的成对点积相乘来相乘两个矩阵。如果我们用一个归一化矩阵乘以其转置，我们就会得到一个包含所有对余弦相似度的矩阵。使用NumPy的矩阵算术，我们将这些余弦相似度转换为Tanimoto相似度。
- en: Matrix multiplication is much faster in NumPy than in pure Python. However,
    even NumPy has its limits. Once a matrix gets too large, we need to find a way
    to reduce its size.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在NumPy中矩阵乘法比纯Python要快得多。然而，即使是NumPy也有其局限性。一旦矩阵变得过大，我们需要找到一种方法来减小其大小。
