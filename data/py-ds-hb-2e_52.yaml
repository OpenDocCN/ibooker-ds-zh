- en: 'Chapter 47\. In Depth: k-Means Clustering'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 47 章：深入理解 k-Means 聚类
- en: 'In the previous chapters we explored unsupervised machine learning models for
    dimensionality reduction. Now we will move on to another class of unsupervised
    machine learning models: clustering algorithms. Clustering algorithms seek to
    learn, from the properties of the data, an optimal division or discrete labeling
    of groups of points.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们探索了用于降维的无监督机器学习模型。现在我们将转向另一类无监督机器学习模型：聚类算法。聚类算法试图从数据的属性中学习出最优的分割或离散标记的群组点。
- en: Many clustering algorithms are available in Scikit-Learn and elsewhere, but
    perhaps the simplest to understand is an algorithm known as *k-means clustering*,
    which is implemented in `sklearn.cluster.KMeans`.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 和其他地方提供了许多聚类算法，但可能最容易理解的算法是称为 *k-means 聚类* 的算法，它在 `sklearn.cluster.KMeans`
    中实现。
- en: 'We begin with the standard imports:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从标准导入开始：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Introducing k-Means
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 k-Means
- en: 'The *k*-means algorithm searches for a predetermined number of clusters within
    an unlabeled multidimensional dataset. It accomplishes this using a simple conception
    of what the optimal clustering looks like:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-means 算法在未标记的多维数据集中搜索预定数量的簇。它使用简单的概念来定义最优的聚类：'
- en: The *cluster center* is the arithmetic mean of all the points belonging to the
    cluster.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*簇中心* 是属于该簇的所有点的算术平均值。'
- en: Each point is closer to its own cluster center than to other cluster centers.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个点更接近它自己的簇中心，而不是其他簇中心。
- en: Those two assumptions are the basis of the *k*-means model. We will soon dive
    into exactly *how* the algorithm reaches this solution, but for now let’s take
    a look at a simple dataset and see the *k*-means result.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个假设是 *k*-means 模型的基础。我们很快将深入了解算法如何达到这个解决方案，但现在让我们看一看一个简单数据集，并查看 *k*-means
    的结果。
- en: First, let’s generate a two-dimensional dataset containing four distinct blobs.
    To emphasize that this is an unsupervised algorithm, we will leave the labels
    out of the visualization (see [Figure 47-1](#fig_0511-k-means_files_in_output_5_0)).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们生成一个包含四个不同斑点的二维数据集。为了突出这是一个无监督算法，我们将在可视化中省略标签（见[图 47-1](#fig_0511-k-means_files_in_output_5_0)）。
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![output 5 0](assets/output_5_0.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![output 5 0](assets/output_5_0.png)'
- en: Figure 47-1\. Data for demonstration of clustering
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 47-1：用于演示聚类的数据
- en: 'By eye, it is relatively easy to pick out the four clusters. The *k*-means
    algorithm does this automatically, and in Scikit-Learn uses the typical estimator
    API:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 肉眼看来，相对容易选出这四个簇。*k*-means 算法自动执行此操作，并在 Scikit-Learn 中使用典型的估计器 API：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s visualize the results by plotting the data colored by these labels ([Figure 47-2](#fig_0511-k-means_files_in_output_9_0)).
    We will also plot the cluster centers as determined by the *k*-means estimator:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过按照这些标签对数据进行着色来可视化结果（[图 47-2](#fig_0511-k-means_files_in_output_9_0)）。我们还将绘制由
    *k*-means 估计器确定的簇中心：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The good news is that the *k*-means algorithm (at least in this simple case)
    assigns the points to clusters very similarly to how we might assign them by eye.
    But you might wonder how this algorithm finds these clusters so quickly: after
    all, the number of possible combinations of cluster assignments is exponential
    in the number of data points—an exhaustive search would be very, very costly.
    Fortunately for us, such an exhaustive search is not necessary: instead, the typical
    approach to *k*-means involves an intuitive iterative approach known as *expectation–maximization*.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 令人高兴的是，*k*-means 算法（至少在这个简单案例中）将点分配到簇中的方式与我们通过肉眼观察的方式非常相似。但你可能会想知道这个算法是如何如此快速地找到这些簇的：毕竟，簇分配的可能组合数随数据点数量呈指数增长——全面搜索将非常、非常昂贵。对我们来说幸运的是，这样的全面搜索并不是必需的：相反，*k*-means
    的典型方法涉及一种直观的迭代方法，称为期望最大化。
- en: '![output 9 0](assets/output_9_0.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![output 9 0](assets/output_9_0.png)'
- en: Figure 47-2\. k-means cluster centers with clusters indicated by color
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 47-2：带有颜色指示簇的 k-means 簇中心
- en: Expectation–Maximization
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 期望最大化
- en: 'Expectation–maximization (E–M) is a powerful algorithm that comes up in a variety
    of contexts within data science. *k*-means is a particularly simple and easy-to-understand
    application of the algorithm; we’ll walk through it briefly here. In short, the
    expectation–maximization approach here consists of the following procedure:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 期望最大化（E–M）是数据科学中多种情境中的一个强大算法。*k*-means 是该算法的一个特别简单且易于理解的应用；我们将在这里简要介绍它。简而言之，在这里期望最大化方法包括以下步骤：
- en: Guess some cluster centers.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 猜测一些聚类中心。
- en: 'Repeat until converged:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直到收敛重复：
- en: '*E-step*: Assign points to the nearest cluster center.'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*E 步*：将点分配给最近的聚类中心。'
- en: '*M-step*: Set the cluster centers to the mean of their assigned points.'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*M 步*：将聚类中心设置为其分配点的平均值。'
- en: Here the *E-step* or *expectation step* is so named because it involves updating
    our expectation of which cluster each point belongs to. The *M-step* or *maximization
    step* is so named because it involves maximizing some fitness function that defines
    the locations of the cluster centers—in this case, that maximization is accomplished
    by taking a simple mean of the data in each cluster.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 *E 步* 或 *期望步骤* 之所以这样命名，是因为它涉及更新我们对每个点属于哪个聚类的期望。*M 步* 或 *最大化步骤* 之所以这样命名，是因为它涉及最大化某些定义聚类中心位置的适应函数——在本例中，通过简单地取每个聚类中数据的平均值来实现该最大化。
- en: 'The literature about this algorithm is vast, but can be summarized as follows:
    under typical circumstances, each repetition of the E-step and M-step will always
    result in a better estimate of the cluster characteristics.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这一算法的文献非常丰富，但可以总结如下：在典型情况下，每次 E 步和 M 步的重复都会导致对聚类特征的更好估计。
- en: We can visualize the algorithm as shown in [Figure 47-3](#fig_images_in_0511-expectation-maximization).
    For the particular initialization shown here, the clusters converge in just three
    iterations. (For an interactive version of this figure, refer to the code in the
    online [appendix](https://oreil.ly/wFnok).)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将算法可视化如图 [Figure 47-3](#fig_images_in_0511-expectation-maximization) 所示。对于此处显示的特定初始化，聚类在仅三次迭代中收敛。（有关此图的交互版本，请参阅在线
    [附录](https://oreil.ly/wFnok) 中的代码。）
- en: '![05.11 expectation maximization](assets/05.11-expectation-maximization.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![05.11 expectation maximization](assets/05.11-expectation-maximization.png)'
- en: Figure 47-3\. Visualization of the E–M algorithm for k-means^([1](ch47.xhtml#idm45858724046864))
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 47-3\. k-means 的 E-M 算法可视化^([1](ch47.xhtml#idm45858724046864))
- en: The *k*-means algorithm is simple enough that we can write it in a few lines
    of code. The following is a very basic implementation (see [Figure 47-4](#fig_0511-k-means_files_in_output_15_0)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-means 算法简单到我们可以用几行代码来编写它。以下是一个非常基本的实现（参见图 [Figure 47-4](#fig_0511-k-means_files_in_output_15_0)）。'
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![output 15 0](assets/output_15_0.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![output 15 0](assets/output_15_0.png)'
- en: Figure 47-4\. Data labeled with k-means
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 47-4\. 使用 k-means 标记的数据
- en: 'Most well-tested implementations will do a bit more than this under the hood,
    but the preceding function gives the gist of the expectation–maximization approach.
    There are a few caveats to be aware of when using the expectation–maximization
    algorithm:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数经过良好测试的实现在底层会做更多事情，但上述函数传达了期望-最大化方法的主旨。在使用期望-最大化算法时，有几个需要注意的事项：
- en: The globally optimal result may not be achieved
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 可能无法达到全局最优结果
- en: First, although the E–M procedure is guaranteed to improve the result in each
    step, there is no assurance that it will lead to the *global* best solution. For
    example, if we use a different random seed in our simple procedure, the particular
    starting guesses lead to poor results (see [Figure 47-5](#fig_0511-k-means_files_in_output_19_0)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，尽管 E-M 过程保证在每个步骤中改善结果，但不能保证它会导致 *全局* 最佳解。例如，如果在我们的简单过程中使用不同的随机种子，特定的起始猜测会导致糟糕的结果（参见图
    [Figure 47-5](#fig_0511-k-means_files_in_output_19_0)）。
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![output 19 0](assets/output_19_0.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![output 19 0](assets/output_19_0.png)'
- en: Figure 47-5\. An example of poor convergence in k-means
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 47-5\. k-means 算法收敛不良的示例
- en: Here the E–M approach has converged, but has not converged to a globally optimal
    configuration. For this reason, it is common for the algorithm to be run for multiple
    starting guesses, as indeed Scikit-Learn does by default (the number is set by
    the `n_init` parameter, which defaults to 10).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，E-M 方法已经收敛，但未收敛到全局最优配置。因此，通常会对算法使用多个起始猜测进行多次运行，默认情况下 Scikit-Learn 就是如此（该数字由
    `n_init` 参数设置，默认为 10）。
- en: The number of clusters must be selected beforehand
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 必须事先选择聚类数
- en: 'Another common challenge with *k*-means is that you must tell it how many clusters
    you expect: it cannot learn the number of clusters from the data. For example,
    if we ask the algorithm to identify six clusters, it will happily proceed and
    find the best six clusters, as shown in [Figure 40-1](ch40.xhtml#fig_0504-feature-engineering_files_in_output_24_0):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-means 的另一个常见挑战是您必须告诉它您期望的聚类数：它无法从数据中学习到聚类数。例如，如果我们要求算法识别六个聚类，它将愉快地继续并找到最佳的六个聚类，如图
    [Figure 40-1](ch40.xhtml#fig_0504-feature-engineering_files_in_output_24_0) 所示：'
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![output 22 0](assets/output_22_0.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![output 22 0](assets/output_22_0.png)'
- en: Figure 47-6\. An example where the number of clusters is chosen poorly
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图47-6\. 簇数量选择不当的示例
- en: Whether the result is meaningful is a question that is difficult to answer definitively;
    one approach that is rather intuitive, but that we won’t discuss further here,
    is called [silhouette analysis](https://oreil.ly/xybmq).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是否有意义是一个很难明确回答的问题；一个相当直观的方法是使用[轮廓分析](https://oreil.ly/xybmq)，但我们这里不再进一步讨论。
- en: Alternatively, you might use a more complicated clustering algorithm that has
    a better quantitative measure of the fitness per number of clusters (e.g., Gaussian
    mixture models; see [Chapter 48](ch48.xhtml#section-0512-gaussian-mixtures)) or
    which *can* choose a suitable number of clusters (e.g., DBSCAN, mean-shift, or
    affinity propagation, all available in the `sklearn.cluster` submodule).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用更复杂的聚类算法，该算法对于每个簇的适应度有更好的定量衡量（例如，高斯混合模型；参见[第48章](ch48.xhtml#section-0512-gaussian-mixtures)），或者可以选择合适的簇数量（例如，DBSCAN、均值漂移或亲和力传播，这些都在`sklearn.cluster`子模块中提供）。
- en: k-means is limited to linear cluster boundaries
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: k-means仅限于线性簇边界
- en: The fundamental model assumptions of *k*-means (points will be closer to their
    own cluster center than to others) means that the algorithm will often be ineffective
    if the clusters have complicated geometries.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-means的基本模型假设（点会更靠近自己的簇中心而不是其他簇）意味着如果簇具有复杂的几何结构，则该算法通常会失效。'
- en: In particular, the boundaries between *k*-means clusters will always be linear,
    which means that it will fail for more complicated boundaries. Consider the following
    data, along with the cluster labels found by the typical *k*-means approach (see
    [Figure 47-7](#fig_0511-k-means_files_in_output_26_0)).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，*k*-means簇之间的边界始终是线性的，这意味着对于更复杂的边界，它将失败。 考虑以下数据，以及典型*k*-means方法找到的簇标签（见[图47-7](#fig_0511-k-means_files_in_output_26_0)）。
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![output 26 0](assets/output_26_0.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![output 26 0](assets/output_26_0.png)'
- en: Figure 47-7\. Failure of k-means with nonlinear boundaries
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图47-7\. k-means在非线性边界下的失败
- en: This situation is reminiscent of the discussion in [Chapter 43](ch43.xhtml#section-0507-support-vector-machines),
    where we used a kernel transformation to project the data into a higher dimension
    where a linear separation is possible. We might imagine using the same trick to
    allow *k*-means to discover non-linear boundaries.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况让人想起了[第43章](ch43.xhtml#section-0507-support-vector-machines)中的讨论，在那里我们使用核变换将数据投影到更高的维度，从而可能实现线性分离。
    我们可以想象使用相同的技巧来允许*k*-means发现非线性边界。
- en: One version of this kernelized *k*-means is implemented in Scikit-Learn within
    the `SpectralClustering` estimator. It uses the graph of nearest neighbors to
    compute a higher-dimensional representation of the data, and then assigns labels
    using a *k*-means algorithm (see [Figure 47-8](#fig_0511-k-means_files_in_output_28_0)).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基于核的*k*-means的一个版本在Scikit-Learn中通过`SpectralClustering`估计器实现。 它使用最近邻图来计算数据的更高维表示，然后使用*k*-means算法分配标签（参见[图47-8](#fig_0511-k-means_files_in_output_28_0)）。
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![output 28 0](assets/output_28_0.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![output 28 0](assets/output_28_0.png)'
- en: Figure 47-8\. Nonlinear boundaries learned by SpectralClustering
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图47-8\. SpectralClustering学习的非线性边界
- en: We see that with this kernel transform approach, the kernelized *k*-means is
    able to find the more complicated nonlinear boundaries between clusters.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，通过这种核变换方法，基于核的*k*-means能够找到更复杂的簇之间的非线性边界。
- en: k-means can be slow for large numbers of samples
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大量样本，k-means可能会运行缓慢。
- en: Because each iteration of *k*-means must access every point in the dataset,
    the algorithm can be relatively slow as the number of samples grows. You might
    wonder if this requirement to use all data at each iteration can be relaxed; for
    example, you might just use a subset of the data to update the cluster centers
    at each step. This is the idea behind batch-based *k*-means algorithms, one form
    of which is implemented in `sklearn.cluster.MiniBatchKMeans`. The interface for
    this is the same as for standard `KMeans`; we will see an example of its use as
    we continue our discussion.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 因为*k*-means的每次迭代都必须访问数据集中的每个点，所以随着样本数量的增长，该算法可能相对缓慢。 您可能会想知道是否可以放宽每次迭代使用所有数据的要求；例如，您可能只使用数据的子集来更新每个步骤的簇中心。
    这就是批量式*k*-means算法背后的思想，其中一种形式在`sklearn.cluster.MiniBatchKMeans`中实现。 其接口与标准的`KMeans`相同；我们将在继续讨论时看到其使用示例。
- en: Examples
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 例子
- en: Being careful about these limitations of the algorithm, we can use *k*-means
    to our advantage in a variety of situations. We’ll now take a look at a couple
    of examples.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们要注意算法的这些限制，但我们可以在各种情况下利用*k*-均值来获益。现在我们来看几个例子。
- en: 'Example 1: k-Means on Digits'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 1：数字上的*k*-均值
- en: To start, let’s take a look at applying *k*-means on the same simple digits
    data that we saw in Chapters [44](ch44.xhtml#section-0508-random-forests) and
    [45](ch45.xhtml#section-0509-principal-component-analysis). Here we will attempt
    to use *k*-means to try to identify similar digits *without using the original
    label information*; this might be similar to a first step in extracting meaning
    from a new dataset about which you don’t have any *a priori* label information.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看在我们在第 [44 章](ch44.xhtml#section-0508-random-forests) 和第 [45 章](ch45.xhtml#section-0509-principal-component-analysis)
    中看到的相同简单数字数据上应用*k*-均值。在这里，我们将尝试使用*k*-均值来尝试识别类似的数字，*而不使用原始标签信息*；这可能类似于从一个没有任何先验标签信息的新数据集中提取含义的第一步。
- en: 'We will start by loading the dataset, then find the clusters. Recall that the
    digits dataset consists of 1,797 samples with 64 features, where each of the 64
    features is the brightness of one pixel in an 8 × 8 image:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从加载数据集开始，然后找到聚类。回想一下，数字数据集包含 1,797 个样本，每个样本有 64 个特征，其中每个特征是 8 × 8 图像中一个像素的亮度。
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The clustering can be performed as we did before:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像之前一样执行聚类：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The result is 10 clusters in 64 dimensions. Notice that the cluster centers
    themselves are 64-dimensional points, and can be interpreted as representing the
    “typical” digit within the cluster. Let’s see what these cluster centers look
    like (see [Figure 47-9](#fig_0511-k-means_files_in_output_37_0)).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是 64 维空间中的 10 个聚类。请注意，聚类中心本身是 64 维点，可以解释为聚类内“典型”的数字。让我们看看这些聚类中心是什么样子的（见 [图 47-9](#fig_0511-k-means_files_in_output_37_0)）。
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![output 37 0](assets/output_37_0.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![output 37 0](assets/output_37_0.png)'
- en: Figure 47-9\. Cluster centers learned by k-means
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 47-9\. *k*-均值学习到的聚类中心
- en: We see that *even without the labels*, `KMeans` is able to find clusters whose
    centers are recognizable digits, with perhaps the exception of 1 and 8.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，即使没有标签的情况下，`KMeans` 也能够找到其聚类中心可识别的数字，也许除了“1”和“8”之外。
- en: 'Because *k*-means knows nothing about the identities of the clusters, the 0–9
    labels may be permuted. We can fix this by matching each learned cluster label
    with the true labels found in the clusters:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 因为*k*-均值对聚类的身份一无所知，0–9 标签可能会被排列。我们可以通过将每个学习到的聚类标签与聚类中找到的真实标签匹配来解决这个问题：
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now we can check how accurate our unsupervised clustering was in finding similar
    digits within the data:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以检查我们的无监督聚类在找到数据中相似数字方面的准确性：
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: With just a simple *k*-means algorithm, we discovered the correct grouping for
    80% of the input digits! Let’s check the confusion matrix for this, visualized
    in [Figure 47-10](#fig_0511-k-means_files_in_output_43_0).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅使用简单的*k*-均值算法，我们就为80%的输入数字找到了正确的分组！让我们来查看这个混淆矩阵，它在 [图 47-10](#fig_0511-k-means_files_in_output_43_0)
    中可视化。
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![output 43 0](assets/output_43_0.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![output 43 0](assets/output_43_0.png)'
- en: Figure 47-10\. Confusion matrix for the k-means classifier
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 47-10\. *k*-均值分类器的混淆矩阵
- en: As we might expect from the cluster centers we visualized before, the main point
    of confusion is between the eights and ones. But this still shows that using *k*-means,
    we can essentially build a digit classifier *without reference to any known labels*!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前可视化的聚类中心所示，混淆的主要点在于数字“8”和“1”。但这仍然表明，使用*k*-均值，我们基本上可以建立一个数字分类器，*无需参考任何已知标签*！
- en: 'Just for fun, let’s try to push this even farther. We can use the t-distributed
    stochastic neighbor embedding algorithm (mentioned in [Chapter 46](ch46.xhtml#section-0510-manifold-learning))
    to preprocess the data before performing *k*-means. t-SNE is a nonlinear embedding
    algorithm that is particularly adept at preserving points within clusters. Let’s
    see how it does:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅是为了好玩，让我们尝试推动这个进展更远。我们可以在执行*k*-均值之前使用 t-分布随机邻居嵌入算法（在[第 46 章](ch46.xhtml#section-0510-manifold-learning)中提到）对数据进行预处理。t-SNE
    是一种非线性嵌入算法，特别擅长保留簇内的点。我们来看看它的表现：
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'That’s a 94% classification accuracy *without using the labels*. This is the
    power of unsupervised learning when used carefully: it can extract information
    from the dataset that it might be difficult to extract by hand or by eye.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种*不使用标签*的 94% 分类准确率。这展示了无监督学习在谨慎使用时的强大能力：它可以从数据集中提取信息，这可能难以手工或肉眼提取。
- en: 'Example 2: k-Means for Color Compression'
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 2：颜色压缩的*k*-均值
- en: One interesting application of clustering is in color compression within images
    (this example is adapted from Scikit-Learn’s [“Color Quantization Using K-Means”](https://oreil.ly/TwsxU)).
    For example, imagine you have an image with millions of colors. In most images,
    a large number of the colors will be unused, and many of the pixels in the image
    will have similar or even identical colors.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的一个有趣应用是图像内的颜色压缩（此示例改编自Scikit-Learn的[“使用K-Means进行颜色量化”](https://oreil.ly/TwsxU)）。例如，想象一下你有一幅包含数百万种颜色的图像。在大多数图像中，许多颜色将未被使用，并且图像中的许多像素将具有相似或甚至相同的颜色。
- en: For example, consider the image shown in [Figure 47-11](#fig_0511-k-means_files_in_output_48_0),
    which is from the Scikit-Learn `datasets` module (for this to work, you’ll have
    to have the `PIL` Python package installed):^([2](ch47.xhtml#idm45858722773584))
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑图像显示在[图47-11](#fig_0511-k-means_files_in_output_48_0)中，这是来自Scikit-Learn
    `datasets`模块的（为了使其工作，您必须安装`PIL` Python包）：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![output 48 0](assets/output_48_0.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![output 48 0](assets/output_48_0.png)'
- en: Figure 47-11\. The input image
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图47-11\. 输入图像
- en: 'The image itself is stored in a three-dimensional array of size `(height, width,
    RGB)`, containing red/blue/green contributions as integers from 0 to 255:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图像本身存储在一个大小为`(height, width, RGB)`的三维数组中，包含从0到255的整数表示的红/蓝/绿分量：
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'One way we can view this set of pixels is as a cloud of points in a three-dimensional
    color space. We will reshape the data to `[n_samples, n_features]` and rescale
    the colors so that they lie between 0 and 1:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这组像素视为三维色彩空间中的一组点云。我们将重新调整数据为`[n_samples, n_features]`的形状，并重新缩放颜色，使其介于0到1之间：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We can visualize these pixels in this color space, using a subset of 10,000
    pixels for efficiency (see [Figure 47-12](#fig_0511-k-means_files_in_output_55_0)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用10000个像素的子集在此色彩空间中可视化这些像素（见[图47-12](#fig_0511-k-means_files_in_output_55_0)）。
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![output 55 0](assets/output_55_0.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![output 55 0](assets/output_55_0.png)'
- en: Figure 47-12\. The distribution of the pixels in RGB color space^([3](ch47.xhtml#idm45858722326976))
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图47-12\. 在RGB色彩空间中的像素分布^([3](ch47.xhtml#idm45858722326976))
- en: 'Now let’s reduce these 16 million colors to just 16 colors, using a *k*-means
    clustering across the pixel space. Because we are dealing with a very large dataset,
    we will use the mini-batch *k*-means, which operates on subsets of the data to
    compute the result (shown in [Figure 47-13](#fig_0511-k-means_files_in_output_57_0))
    much more quickly than the standard *k*-means algorithm:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将这1600万种颜色减少到只有16种颜色，使用像素空间的* k *-means聚类。由于我们正在处理一个非常大的数据集，我们将使用小批量* k
    *-means，它在数据子集上计算结果（显示在[图47-13](#fig_0511-k-means_files_in_output_57_0)中）比标准*
    k *-means算法更快：
- en: '[PRE22]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![output 57 0](assets/output_57_0.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![output 57 0](assets/output_57_0.png)'
- en: Figure 47-13\. 16 clusters in RGB color space^([4](ch47.xhtml#idm45858722264912))
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图47-13\. RGB色彩空间中的16个聚类^([4](ch47.xhtml#idm45858722264912))
- en: The result is a recoloring of the original pixels, where each pixel is assigned
    the color of its closest cluster center. Plotting these new colors in the image
    space rather than the pixel space shows us the effect of this (see [Figure 47-14](#fig_0511-k-means_files_in_output_59_0)).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是原始像素的重新着色，其中每个像素分配到最接近的聚类中心的颜色。将这些新颜色在图像空间而不是像素空间中绘制，显示了这种效果（见[图47-14](#fig_0511-k-means_files_in_output_59_0)）。
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![output 59 0](assets/output_59_0.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![output 59 0](assets/output_59_0.png)'
- en: Figure 47-14\. A comparison of the full-color image (left) and the 16-color
    image (right)
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图47-14\. 全彩图像（左）和16色图像（右）的比较
- en: Some detail is certainly lost in the rightmost panel, but the overall image
    is still easily recognizable. In terms of the bytes required to store the raw
    data, the image on the right achieves a compression factor of around 1 million!
    Now, this kind of approach is not going to match the fidelity of purpose-built
    image compression schemes like JPEG, but the example shows the power of thinking
    outside of the box with unsupervised methods like *k*-means.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧面板中确实丢失了一些细节，但整体图像仍然很容易识别。在存储原始数据所需的字节方面，右侧的图像实现了约100万的压缩比！现在，这种方法不会与像JPEG这样的专用图像压缩方案匹敌，但这个例子展示了通过*
    k *-means等无监督方法进行创新思维的威力。
- en: ^([1](ch47.xhtml#idm45858724046864-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/yo6GV).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch47.xhtml#idm45858724046864-marker)) 生成此图的代码可以在[在线附录](https://oreil.ly/yo6GV)中找到。
- en: ^([2](ch47.xhtml#idm45858722773584-marker)) For a color version of this and
    following images, see the [online version of this book](https://oreil.ly/PDSH_GitHub).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch47.xhtml#idm45858722773584-marker)) 欲查看彩色版本以及后续图像，请参阅[本书的在线版本](https://oreil.ly/PDSH_GitHub)。
- en: ^([3](ch47.xhtml#idm45858722326976-marker)) A full-size version of this figure
    can be found on [GitHub](https://oreil.ly/PDSH_GitHub).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch47.xhtml#idm45858722326976-marker)) 这幅图的全尺寸版本可以在[GitHub](https://oreil.ly/PDSH_GitHub)上找到。
- en: ^([4](ch47.xhtml#idm45858722264912-marker)) A full-size version of this figure
    can be found on [GitHub](https://oreil.ly/PDSH_GitHub).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch47.xhtml#idm45858722264912-marker)) 这幅图的全尺寸版本可以在[GitHub](https://oreil.ly/PDSH_GitHub)上找到。
