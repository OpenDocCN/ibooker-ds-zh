- en: Appendix B. The math behind gradient descent: Coming down a mountain using derivatives
    and slopes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录B. 梯度下降背后的数学：使用导数和斜率下山
- en: In this appendix, we’ll go over the mathematical details of gradient descent.
    This appendix is fairly technical, and understanding it is not required to follow
    the rest of the book. However, it is here to provide a sense of completeness for
    the readers who wish to understand the inner workings of some of the core machine
    learning algorithms. The mathematics knowledge required for this appendix is higher
    than for the rest of the book. More specifically, knowledge of vectors, derivatives,
    and the chain rule is required.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个附录中，我们将介绍梯度下降的数学细节。这个附录相当技术性，理解它不是阅读本书其余部分所必需的。然而，它在这里是为了给那些希望了解一些核心机器学习算法内部工作原理的读者提供一个完整的感观。这个附录所需的数学知识比本书的其他部分要高。更具体地说，需要了解向量、导数和链式法则。
- en: 'In chapters 3, 5, 6, 10, and 11, we used gradient descent to minimize the error
    functions in our models. More specifically, we used gradient descent to minimize
    the following error functions:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3、5、6、10和11章中，我们使用了梯度下降来最小化我们模型中的误差函数。更具体地说，我们使用了梯度下降来最小化以下误差函数：
- en: 'Chapter 3: the absolute and square error functions in a linear regression model'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第3章：线性回归模型中的绝对值和平方误差函数
- en: 'Chapter 5: the perceptron error function in a perceptron model'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第5章：感知器模型中的感知器误差函数
- en: 'Chapter 6: the log loss in a logistic classifier'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第6章：逻辑分类器中的对数损失
- en: 'Chapter 10: the log loss in a neural network'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第10章：神经网络中的对数损失
- en: 'Chapter 11: the classification (perceptron) error and the distance (regularization)
    error in an SVM'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第11章：SVM中的分类（感知器）误差和距离（正则化）误差
- en: As we learned in chapters 3, 5, 6, 10, and 11, the error function measures how
    poorly the model is doing. Thus, finding the minimum value for this error function—or
    at least a really small value, even if it’s not the minimum—will be instrumental
    in finding a good model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第3、5、6、10和11章中学到的，误差函数衡量模型做得有多糟糕。因此，找到这个误差函数的最小值——或者至少是一个非常小的值，即使它不是最小值——对于找到一个好的模型将是至关重要的。
- en: 'The analogy we used was that of descending a mountain—Mount Errorest, shown
    in figure B.1\. The scenario is the following: You are somewhere on top of a mountain,
    and you’d like to get to the bottom of this mountain. It is very cloudy, so you
    can’t see far around you. The best bet you can have is to descend from the mountain
    one step at a time. You ask yourself , “If I were to take only one step, in which
    direction should I take it to descend the most?” You find that direction and take
    that step. Then you ask the same question again, and take another step, and you
    repeat the process many times. It is imaginable that if you always take the one
    step that helps you descend the most, that you must get to a low place. You may
    need a bit of luck to actually get to the bottom of the mountain, as opposed to
    getting stuck in a valley, but we’ll deal with this later in the section “Getting
    stuck on local minima.”'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的类比是下山——如图B.1所示的埃罗斯特山。场景如下：你正站在山顶的某个地方，你想到达这座山的底部。天气非常多云，所以你看不见你周围很远的地方。你能做的最好的事情是一步一步地从山上下来。你问自己，“如果我只迈出一步，我应该朝哪个方向迈出，才能下得最快？”你找到那个方向，迈出那一步。然后你再次提出相同的问题，再迈出另一步，你重复这个过程很多次。可以想象，如果你总是迈出一步，那一步能让你下得最快，那么你肯定能到达一个低洼的地方。你可能需要一点运气才能真正到达山底，而不是陷入山谷，但我们在“陷入局部最小值”这一节中会处理这个问题。
- en: '![](../Images/B-1.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B-1.png)'
- en: Figure B.1 In the gradient descent step, we want to descend from a mountain
    called Mount Errorest.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图B.1 在梯度下降步骤中，我们想要从被称为埃罗斯特山的山上下来。
- en: Throughout the following sections we’ll describe the mathematics behind gradient
    descent and use it to help us train several machine learning algorithms by decreasing
    their error functions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将描述梯度下降背后的数学，并使用它来帮助我们通过减少它们的误差函数来训练几个机器学习算法。
- en: Using gradient descent to decrease functions
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用梯度下降来减少函数
- en: 'The mathematical formalism of gradient descent follows: say you want to minimize
    the function *f*(*x*[1], *x*[2], …, *x*[n]) on the *n* variables *x*[1], *x*[2],
    …, *x*[n]. We assume the function is continuous and differentiable over each of
    the *n* variables.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的数学形式如下：假设你想要最小化函数 *f*(*x*[1], *x*[2], …, *x*[n]) 在 *n* 个变量 *x*[1], *x*[2],
    …, *x*[n] 上的值。我们假设这个函数在每个 *n* 个变量上都是连续且可导的。
- en: 'We are currently standing at the point *p* with coordinates (*p*[1], *p*[2],
    …, *p*[n]), and we wish to find the direction in which the function decreases
    the most, in order to take that step. This is illustrated in figure B.2\. To find
    the direction in which the function decreases the most, we use the *gradient*
    of the function. The gradient is the *n*-dimensional vector formed by the partial
    derivatives of *f* with respect to each of the variables *x*[1], *x*[2], …, *x*[n].
    This gradient is denoted as ∇*f*, as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前位于点*p*，坐标为(*p*[1], *p*[2], …, *p*[n])，我们希望找到函数减少最多的方向，以便采取该步骤。这如图B.2所示。为了找到函数减少最多的方向，我们使用函数的*梯度*。梯度是由函数*f*相对于每个变量*x*[1],
    *x*[2], …, *x*[n]的偏导数组成的*n*-维向量。这个梯度表示为∇*f*，如下所示：
- en: '![](../Images/AppB_01_E01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![AppB_01_E01.png]'
- en: The gradient is a vector that points in the direction of greatest growth, namely,
    the direction in which the function *increases* the most. Thus, the negative of
    the gradient is the direction in which the function *decreases* the most. This
    is the step we want to take. We determine the size of the step using the *learning
    rate* we learned in chapter 3 and which we denote with *η*. The gradient descent
    step consists of taking a step of length *η*|∇*f|* in the direction of the negative
    of the gradient ∇*f*. Thus, if our original point was *p*, after applying the
    gradient descent step, we obtain the point *p* – *η*∇*f*. Figure B.2 illustrates
    the step that we’ve taken to decrease the function *f*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度是一个指向最大增长方向的向量，即函数*增加*最多的方向。因此，梯度的负值是函数*减少*最多的方向。这是我们想要采取的步骤。我们使用在第3章中学到的*学习率*来确定步长的大小，我们用*η*表示它。梯度下降步骤包括在负梯度的方向上迈出长度为*η*|∇*f|*的步长。因此，如果我们的原始点是*p*，在应用梯度下降步骤后，我们得到点*p*
    – *η*∇*f*。图B.2说明了我们为减少函数*f*所采取的步骤。
- en: '![](../Images/B-2.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![B-2](../Images/B-2.png)'
- en: 'Figure B.2 We were originally at the point *p*. We take a step in the direction
    of the negative of the gradient and end up at a new point. This is the direction
    in which the function decreases the most. (Source: Image created with the assistance
    of Grapher™ from Golden Software, LLC; [https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图B.2 我们最初位于点*p*。我们朝着负梯度的方向迈出一步，最终到达一个新的点。这是函数减少最多的方向。（来源：使用Golden Software,
    LLC的Grapher™辅助创建的图像；[https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)）。
- en: 'Now that we know how to take one step to slightly decrease the function, we
    can simply repeat this process many times to minimize our function. Thus, the
    pseudocode of the gradient descent algorithm is the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何迈出一小步来略微减少函数，我们可以简单地重复这个过程多次来最小化我们的函数。因此，梯度下降算法的伪代码如下：
- en: Pseudocode for the gradient descent algorithm
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法的伪代码
- en: '**Goal**: To minimize the function *f*.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标**：最小化函数*f*。'
- en: 'Hyperparameters:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数：
- en: Number of epochs (repetitions) *N*
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代次数（重复次数）*N*
- en: Learning rate *η*
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率*η*
- en: 'Process:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 流程：
- en: Pick a random point *p*[0].
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个随机点*p*[0]。
- en: 'For *i* = 0, …, *N* – 1:'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于*i* = 0, …, *N* – 1：
- en: – Calculate the gradient ∇*f*(*p*[i]).
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: – 计算梯度∇*f*(*p*[i])。
- en: – Pick the point *p*[i][+1] = *p*[i] – *η*∇*f*(*p*[i]).
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: – 选择点*p*[i][+1] = *p*[i] – *η*∇*f*(*p*[i])。
- en: End with the point *p*[n].
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以点*p*[n]结束。
- en: '![](../Images/B-3.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![B-3](../Images/B-3.png)'
- en: 'Figure B.3 If we repeat the gradient descent step many times, we have a high
    chance of finding the minimum value of the function. In this figure, *p*[1] represents
    the starting point and *p*[n] the point we have obtained using gradient descent.
    (Source: Image created with the assistance of Grapher™ from Golden Software, LLC;
    [https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图B.3 如果我们多次重复梯度下降步骤，我们有很大的机会找到函数的最小值。在这个图中，*p*[1]代表起点，*p*[n]代表使用梯度下降获得的点。（来源：使用Golden
    Software, LLC的Grapher™辅助创建的图像；[https://www.goldensoftware.com/products/grapher](https://www.goldensoftware.com/products/grapher)）。
- en: Does this process *always* find the minimum of the function? Unfortunately,
    no. Several problems may occur when trying to minimize a function using gradient
    descent, such as getting stuck at a local minimum (a valley). We’ll learn a very
    useful technique to deal with this problem in the section “Getting stuck on local
    minima.”
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程**总是**找到函数的最小值吗？不幸的是，不是的。在尝试使用梯度下降最小化函数时可能会出现几个问题，例如陷入局部最小值（一个山谷）。我们将在“陷入局部最小值”这一节中学习一种非常有用的技术来处理这个问题。
- en: Using gradient descent to train models
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用梯度下降训练模型
- en: 'Now that we know how gradient descent helps us minimize (or at least, find
    small values for) a function, in this section we see how to use it to train some
    machine learning models. The models we’ll train follow:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了梯度下降如何帮助我们最小化（或者至少，找到函数的小值），在本节中我们将看到如何使用它来训练一些机器学习模型。我们将训练的模型如下：
- en: Linear regression (from chapter 3).
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归（来自第3章）。
- en: Perceptron (from chapter 5).
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知器（来自第5章）。
- en: Logistic classifier (from chapter 6).
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑分类器（来自第6章）。
- en: Neural network (from chapter 10).
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络（来自第10章）。
- en: Regularization (from chapters 4 and 11). This one is not a model, but we can
    still see the effects that a gradient descent step has on a model that uses regularization.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化（来自第4章和第11章）。这不是一个模型，但我们仍然可以看到梯度下降步骤对使用正则化的模型产生的影响。
- en: The way we use gradient descent to train a model is by letting *f* be the corresponding
    error function of the model and using gradient descent to minimize *f*. The value
    of the error function is calculated over the dataset. However, as we saw in the
    sections “Do we train using one point at a time or many” in chapter 3, “Stochastic,
    mini-batch, and batch gradient descent” in chapter 6, and “Hyperparameters” in
    chapter 10, if the dataset is too large, we can speed up training by splitting
    the dataset into mini-batches of (roughly) the same size and, at each step, picking
    a different mini-batch on which to calculate the error function.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用梯度下降训练模型的方式是让 *f* 成为模型的对应误差函数，并使用梯度下降来最小化 *f*。误差函数的值在数据集上计算。然而，正如我们在第3章的“我们是一次训练一个点还是多个点”部分、第6章的“随机、小批量和大批量梯度下降”部分以及第10章的“超参数”部分所看到的，如果数据集太大，我们可以通过将数据集分成大小大致相同的小批量来加速训练，并在每一步中选取不同的一个小批量来计算误差函数。
- en: 'Here is some notation we’ll use in this appendix. Most of the terms have been
    introduced in chapters 1 and 2:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本附录中，我们将使用以下记号。大多数术语已在第1章和第2章中介绍：
- en: The **size** of the dataset, or the number of rows, is *m*.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的**大小**，或行数，是 *m*。
- en: The **dimension** of the dataset, or number of columns, is *n*.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的**维度**，或列数，是 *n*。
- en: The dataset is formed by **features** and **labels**.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集由**特征**和**标签**组成。
- en: The **features** are the *m* vectors *x*[i] = (*x*[1]^(^i^), *x*[2]^(^i^), …,
    *x*[n]^(^i^)) for *i* = 1, 2, …, *m*.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征**是 *m* 个向量 *x*[i] = (*x*[1]^(^i^)，*x*[2]^(^i^)，…，*x*[n]^(^i^))，对于 *i*
    = 1，2，…，*m*。'
- en: The **labels** *y*[i], for *i* = 1, 2, …, *m*.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签** *y*[i]，对于 *i* = 1，2，…，*m*。'
- en: The **model** is given by the vector of *n* weights *w* = (*w*[1], *w*[2], …,
    *w*[n]) and the bias *b* (a scalar) (except when the model is a neural network,
    which will have more weights and biases).
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**由 *n* 个权重向量 *w* = (*w*[1]，*w*[2]，…，*w*[n]) 和偏置 *b*（一个标量）给出（除非模型是神经网络，它将具有更多的权重和偏置）。'
- en: The **predictions** *ŷ*, for *i* = 1, 2, …, *m*.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测** *ŷ*，对于 *i* = 1，2，…，*m*。'
- en: The **learning rate** of the model is *η*.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的**学习率**是 *η*。
- en: The **mini-batches** of data are *B*[1], *B*[2], …, *B*[l], for some number
    *l*. Each mini-batch has length *q*. The points in one mini-batch (for notational
    convenience) are denoted *x*^((1)), …, *x*^(^q^), and the labels are *y*[1], …,
    *y*[q].
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的**小批量**是 *B*[1]，*B*[2]，…，*B*[l]，其中 *l* 是某个数字。每个小批量具有长度 *q*。一个小批量中的点（为了方便记号）表示为
    *x*^((1))，…，*x*^(^q^)，标签为 *y*[1]，…，*y*[q]。
- en: 'The gradient descent algorithm that we’ll use for training models follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于训练模型的梯度下降算法如下：
- en: Gradient descent algorithm for training machine learning models
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型训练的梯度下降算法
- en: 'Hyperparameters:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数：
- en: Number of epochs (repetitions) *N*
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轮数（重复次数）*N*
- en: Learning rate *η*
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率 *η*
- en: 'Process:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 流程：
- en: Pick random weights *w*[1], *w*[2], …, *w*[n] and a random bias *b*.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机选择权重 *w*[1]，*w*[2]，…，*w*[n] 和随机偏置 *b*。
- en: 'For *i* = 0, …, *N* – 1:'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 *i* = 0，…，*N* – 1：
- en: For each of the mini-batches *B*[1], *B*[2], …, *B*[l].
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个小批量 *B*[1]，*B*[2]，…，*B*[l]。
- en: Calculate the error function *f*(*w, b*) on that particular mini-batch.
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算特定小批量的误差函数 *f*(*w, b*)。
- en: Calculate the gradient ![](../Images/AppB_03_E01.png)
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算梯度 ![梯度计算图](../Images/AppB_03_E01.png)
- en: 'Replace the weights and bias as follows:'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将权重和偏差替换如下：
- en: '*w*[1] gets replaced by ![](../Images/AppB_03_E02.png)'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*[1] 被替换为 ![w*[1] 替换图](../Images/AppB_03_E02.png)'
- en: '*b* gets replaced by ![](../Images/AppB_03_E03.png)'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b* 被替换为 ![b 替换图](../Images/AppB_03_E03.png)'
- en: 'Throughout the following subsections, we will perform this process in detail
    for each of the following models and error functions:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将详细说明以下每个模型和误差函数的此过程：
- en: Linear regression model with the mean absolute error function (the following
    section)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有平均绝对误差函数的线性回归模型（下一节）
- en: Linear regression model with the mean square error function (the following section)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有平均平方误差函数的线性回归模型（下一节）
- en: Perceptron model with the perceptron error function (the section “Using gradient
    descent to train classification models”)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知器模型带有感知器误差函数（“使用梯度下降训练分类模型”这一节）
- en: Logistic regression model with the log loss function (the section “Using gradient
    descent to train classification models”)
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有对数损失函数的逻辑回归模型（“使用梯度下降训练分类模型”这一节）
- en: Neural network with the log loss function (the section “Using gradient descent
    to train neural networks”)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有对数损失函数的神经网络（“使用梯度下降训练神经网络”这一节）
- en: Models with regularization (the section “Using gradient descent for regularization”)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有正则化的模型（“使用梯度下降进行正则化”这一节）
- en: Using gradient descent to train linear regression models
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度下降来训练线性回归模型
- en: 'In this section, we use gradient descent to train a linear regression model,
    using both of the error functions we’ve learned previously: the mean absolute
    error and the mean square error. Recall from chapter 3 that in linear regression,
    the predictions *ŷ*[1],*ŷ*[1], … , *ŷ*[q] are given by the following formula:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用梯度下降来训练一个线性回归模型，使用我们之前学过的两个误差函数：平均绝对误差和平均平方误差。回顾第3章，在线性回归中，预测值 *ŷ*[1]，*ŷ*[1]，…，*ŷ*[q]
    由以下公式给出：
- en: '![](../Images/AppB_03_E04.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![感知器误差函数图](../Images/AppB_03_E04.png)'
- en: The goal of our regression model is to find the weights *w*[1], …, *w*[n], which
    produce predictions that are really close to the labels. Thus, the error function
    helps by measuring how far *ŷ* is from *y* for a particular set of weights. As
    we’ve seen in sections “The absolute error” and “The square error” in chapter
    3, we have two different ways to calculate this distance. The first is the absolute
    value |*ŷ* – *y*|, and the second one is the square of the difference (*y* – *ŷ*)².
    The first one gives rise to the mean absolute error, and the second one to the
    mean square error. Let’s study them separately.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回归模型的目标是找到权重 *w*[1]，…，*w*[n]，它们产生的预测值与标签非常接近。因此，误差函数通过测量特定权重集下 *ŷ* 与 *y* 之间的距离来帮助。正如我们在第3章的“绝对误差”和“平方误差”一节中看到的，我们有两种不同的方式来计算这个距离。第一种是绝对值
    |*ŷ* – *y*|，第二种是差异的平方 (*y* – *ŷ*)²。第一种产生了平均绝对误差，第二种产生了平均平方误差。让我们分别研究它们。
- en: Training a linear regression model using gradient descent to reduce the mean
    absolute error
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度下降来训练线性回归模型以减少平均绝对误差
- en: 'In this subsection, we’ll calculate the gradient of the mean absolute error
    function and use it to apply gradient descent and train a linear regression model.
    The mean absolute error is a way to tell how far apart *ŷ* and *y* are. It was
    first defined in the section “The absolute error” in chapter 3, and its formula
    follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将计算平均绝对误差函数的梯度，并使用它来应用梯度下降并训练一个线性回归模型。平均绝对误差是一种衡量 *ŷ* 和 *y* 之间距离的方法。它首次在第3章的“绝对误差”一节中定义，其公式如下：
- en: '![](../Images/AppB_03_E05.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![平均绝对误差公式图](../Images/AppB_03_E05.png)'
- en: For convenience, we’ll abbreviate *MAE*(*w, b, x*, *y*) as *MAE*. To use gradient
    descent to reduce *MAE*, we need to calculate the gradient ∇*MAE*, which is the
    vector containing the *n* + 1 partial derivatives of *MAE* with respect to *w*[1],
    …, *w*[n], *b*,
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们将 *MAE*(*w, b, x*, *y*) 简写为 *MAE*。为了使用梯度下降来减少 *MAE*，我们需要计算梯度 ∇*MAE*，它是一个包含
    *n* + 1 个关于 *w*[1]，…，*w*[n]，*b* 的偏导数的向量，
- en: '![](../Images/AppB_03_E06.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![偏导数计算图](../Images/AppB_03_E06.png)'
- en: We’ll calculate these partial derivatives using the chain rule. First, notice
    that
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用链式法则来计算这些偏导数。首先，注意
- en: '![](../Images/AppB_03_E07.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![权重和偏差替换图](../Images/AppB_03_E07.png)'
- en: The derivative of *f*(*x*) = |*x*| is the sign function ![](../Images/AppB_03_E08.png),
    which is +1 when *x* is positive and –1 when x is negative (it is undefined at
    0, but for convenience we can define it to be 0). Thus, we can rewrite the previous
    equation as
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 *f*(*x*) = |*x*| 的导数是符号函数 ![](../Images/AppB_03_E08.png)，当 *x* 为正时为 +1，当
    *x* 为负时为 -1（在 0 处未定义，但为了方便，我们可以将其定义为 0）。因此，我们可以将前面的方程重写为
- en: '![](../Images/AppB_03_E09.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_03_E09.png)'
- en: To calculate this value, let’s focus on the final part of the equation, namely,
    the ![](../Images/AppB_03_E10.png). Since ![](../Images/AppB_03_E11.png), then
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算这个值，让我们关注方程的最后一部分，即 ![](../Images/AppB_03_E10.png)。由于 ![](../Images/AppB_03_E11.png)，因此
- en: '![](../Images/AppB_03_E12.png).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/AppB_03_E12.png).'
- en: 'This is because the derivative of *w*[j] with respect to *w*[i], is 1 if *j*
    = *i* and 0 otherwise. Thus, replacing on the derivative, we get the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为 *w*[j] 对 *w*[i] 的导数，当 *j* = *i* 时为 1，否则为 0。因此，在导数上替换，我们得到以下结果：
- en: '![](../Images/AppB_03_E13.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_03_E13.png)'
- en: Using a similar analysis, we can calculate the derivative of *MAE*(*w*, *b*)
    with respect to *b* to be
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用类似的分析，我们可以计算出 *MAE*(*w*, *b*) 对 *b* 的导数是
- en: '![](../Images/AppB_03_E14.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_03_E14.png)'
- en: 'The gradient descent step is the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降步骤如下：
- en: 'Gradient descent step:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降步骤：
- en: Replace (*w*, *b*) by (*w**'*, *b**'*), where
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 将 (*w*, *b*) 替换为 (*w**'*, *b**'*)，其中
- en: '![](../Images/AppB_03_E15.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_03_E15.png)'
- en: 'Notice something interesting: if the mini-batch has size *q* = 1 and consists
    only of the point *x* = (*x*[1], *x*[2], …, *x*[n]) with label *y* and prediction
    *ŷ*, then the step is defined as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到有趣的一点：如果小批量的大小 *q* = 1，并且只包含点 *x* = (*x*[1], *x*[2], …, *x*[n])，带有标签 *y*
    和预测 *ŷ*，则步骤定义如下：
- en: Replace (*w*, *b*) by (*w**'*, *b**'*), where
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 将 (*w*, *b*) 替换为 (*w**'*, *b**'*)，其中
- en: '*w*[j]'' = *w*[j] + *η* sgn(*y* – *ŷ*)*x*[j]'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*[j]'' = *w*[j] + *η* sgn(*y* – *ŷ*)*x*[j]'
- en: '*b*'' = *b* + *η* sgn(*y* – *ŷ*)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b*'' = *b* + *η* sgn(*y* – *ŷ*)'
- en: This is precisely the simple trick we used in the section “The simple trick”
    in chapter 3 to train our linear regression algorithm.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们在第三章“简单技巧”一节中使用的简单技巧来训练我们的线性回归算法。
- en: Training a linear regression model using gradient descent to reduce the mean
    square error
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度下降法通过减少均方误差来训练线性回归模型
- en: In this subsection, we’ll calculate the gradient of the mean square error function
    and use it to apply gradient descent and train a linear regression model. The
    mean square error is another way to tell how far apart *ŷ* and *y* are. It was
    first defined in the section “The square error” in chapter 3 and its formula is
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将计算均方误差函数的梯度，并利用它来应用梯度下降法并训练线性回归模型。均方误差是衡量 *ŷ* 和 *y* 之间距离的另一种方式。它首次在第三章“平方误差”一节中定义，其公式为
- en: '![](../Images/AppB_03_E16.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_03_E16.png)'
- en: For convenience, we’ll abbreviate *MSE*(*w, b, x*, *y*) as *MSE*. To calculate
    the gradient ∇*MSE*, we can follow the same procedure as we did for the mean absolute
    error described earlier, with the exception that the derivative of *f*(*x*) =
    *x*² is 2*x*. Therefore, the derivative of *MSE* with respect to *w*[j] is
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，我们将 *MSE*(*w, b, x*, *y*) 简写为 *MSE*。为了计算梯度 ∇*MSE*，我们可以遵循与之前描述的均方绝对误差相同的程序，只是
    *f*(*x*) = *x*² 的导数是 2*x*。因此，*MSE* 对 *w*[j] 的导数是
- en: '![](../Images/AppB_03_E17.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_03_E17.png)'
- en: Similarly, the derivative of *MSE*(*w*, *b*) with respect to *b* is
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，*MSE*(*w*, *b*) 对 *b* 的导数是
- en: '![](../Images/AppB_03_E18.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_03_E18.png)'
- en: 'Gradient descent step:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降步骤：
- en: Replace (*w*, *b*) by (*w**'*, *b**'*), where
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 将 (*w*, *b*) 替换为 (*w**'*, *b**'*)，其中
- en: '![](../Images/AppB_03_E19.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_03_E19.png)'
- en: 'Notice again that if the mini-batch has size *q =* 1 and consists only of the
    point *x* = (*x*[1], *x*[2], …, *x*[n]) with label *y* and prediction *ŷ*, then
    the step is defined as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 注意再次，如果小批量的大小 *q =* 1，并且只包含点 *x* = (*x*[1], *x*[2], …, *x*[n])，带有标签 *y* 和预测
    *ŷ*，则步骤定义如下：
- en: Replace (*w*, *b*) by (*w**'*, *b**'*), where
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 将 (*w*, *b*) 替换为 (*w**'*, *b**'*)，其中
- en: '*w*[j]'' = *w*[j] + *η*(*y* – *ŷ*)*x*[j]'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*[j]'' = *w*[j] + *η*(*y* – *ŷ*)*x*[j]'
- en: '*b**''* = *b* + *η*(*y* – *ŷ*)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b**''* = *b* + *η*(*y* – *ŷ*)'
- en: This is precisely the square trick we used in the section “The square trick”
    in chapter 3 to train our linear regression algorithm.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们在第三章“平方技巧”一节中使用的平方技巧来训练我们的线性回归算法。
- en: Using gradient descent to train classification models
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度下降法训练分类模型
- en: In this section we learn how to use gradient descent to train classification
    models. The two models that we’ll train are the perceptron model (chapter 5) and
    the logistic regression model (chapter 6). Each one of them has its own error
    function, so we will develop them separately.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习如何使用梯度下降来训练分类模型。我们将训练的两个模型是感知器模型（第五章）和对数回归模型（第六章）。每个模型都有自己的误差函数，因此我们将分别开发它们。
- en: Training a perceptron model using gradient descent to reduce the perceptron
    error
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度下降来训练感知器模型以减少感知器误差
- en: In this subsection, we’ll calculate the gradient of the perceptron error function
    and use it to apply gradient descent and train a perceptron model. In the perceptron
    model, the predictions are *ŷ*[1]*,* *ŷ*[2]*, …,* *ŷ*[q] where each *ŷ*[i] is
    0 or 1\. To calculate the predictions, we first need to remember the step function
    *step*(*x*), introduced in chapter 5\. This function takes as an input any real
    number *x* and outputs 0 if *x* < 0 and 1 if *x* ≥ 0\. Its graph is shown in figure
    B.4.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将计算感知器误差函数的梯度，并使用它来应用梯度下降并训练感知器模型。在感知器模型中，预测是 *ŷ*[1]*,* *ŷ*[2]*, …,*
    *ŷ*[q]，其中每个 *ŷ*[i] 是 0 或 1。为了计算预测，我们首先需要记住第五章中引入的步骤函数 *step*(*x*)。这个函数将任何实数 *x*
    作为输入，如果 *x* < 0 则输出 0，如果 *x* ≥ 0 则输出 1。它的图示在图 B.4 中。
- en: '![](../Images/B-41.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B-41.png)'
- en: Figure B.4 The step function. For negative numbers it outputs 0, and for non-negative
    numbers it outputs 1.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.4 步骤函数。对于负数，它输出 0，对于非负数，它输出 1。
- en: The model gives each point a *score*. The score that the model with weights
    (*w*[1], *w*[2], …, *w*[n]) and bias *b* gives to the point *x*^(^i^) = (*x*[1]^(^i^),
    *x*[n]^(^i^), …, *x*[n]^(^i^)) is
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 模型给每个点一个 *分数*。具有权重 (*w*[1], *w*[2], …, *w*[n]) 和偏置 *b* 的模型给点 *x*^(^i^) = (*x*[1]^(^i^),
    *x*[n]^(^i^), …, *x*[n]^(^i^)) 的分数是
- en: '![](../Images/AppB_04_E01.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_04_E01.png)'
- en: 'The predictions *ŷ*[i] are given by the following formula:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 预测 *ŷ*[i] 由以下公式给出：
- en: '![](../Images/AppB_04_E02.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_04_E02.png)'
- en: In other words, the prediction is 1 if the score is positive, and 0 otherwise.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果分数为正，预测为 1，否则为 0。
- en: The perceptron error function is called *PE*(*w, b, x*, *y*), which we’ll abbreviate
    as *PE*. It was first defined in the section “How to compare classifiers? The
    error function” in chapter 5\. By construction, it is a large number if the model
    made a bad prediction, and a small number (in this case, actually 0) if the model
    made a good prediction. The error function is defined as follows.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器误差函数被称为 *PE*(*w, b, x*, *y*)，我们将它简称为 *PE*。它首次在第五章的“如何比较分类器？误差函数”一节中定义。根据构造，如果模型做出了错误的预测，它就是一个大数，如果模型做出了正确的预测，它就是一个小数（在这种情况下，实际上为
    0）。误差函数的定义如下。
- en: '*PE*(*w, b, x, y*) = 0 if *ŷ* = *y*'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*PE*(*w, b, x, y*) = 0 如果 *ŷ* = *y*'
- en: '*PE*(*w, b, x*, *y*) = |*score*(*w, b, x*)| if *ŷ* *≠* *y*'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*PE*(*w, b, x*, *y*) = |*score*(*w, b, x*)| 如果 *ŷ* *≠* *y*'
- en: In other words, if the point is correctly classified, the error is zero. If
    the point is incorrectly classified, the error is the absolute value of the score.
    Thus, misclassified points with scores of low absolute value produce a low error,
    and misclassified points with scores of high absolute value produce a high error.
    This is because the absolute value of the score of a point is proportional to
    the distance between that point and the boundary. Thus, the points with low error
    are points that are close to the boundary, whereas points with high error are
    far from the boundary.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果点被正确分类，误差为零。如果点被错误分类，误差是分数的绝对值。因此，分数绝对值低的错误分类点产生低误差，而分数绝对值高的错误分类点产生高误差。这是因为点的分数的绝对值与该点与边界的距离成正比。因此，误差低的点是靠近边界的点，而误差高的点是远离边界的点。
- en: To calculate the gradient ∇*PE*, we can use the same rule as before. One thing
    we should notice is that the derivative of the absolute value function |*x*| is
    1 when *x* ≥ 0 and 0 when *x* < 0\. This derivative is undefined at 0, which is
    a problem with our calculations, but in practice, we can arbitrarily define it
    as 1 without any problems.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算梯度 ∇*PE*，我们可以使用之前的相同规则。我们应该注意的一件事是，绝对值函数 |*x*| 的导数在 *x* ≥ 0 时为 1，在 *x* <
    0 时为 0。这个导数在 0 处未定义，这是我们计算中的一个问题，但在实践中，我们可以任意将其定义为 1 而不会出现任何问题。
- en: 'In chapter 10, we introduced the *ReLU*(*x*) (rectified linear unit) function,
    which is 0 when *x* < 0 and *x* when *x* ≥ 0\. Notice that there are two ways
    in which a point can be misclassified:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在第10章中，我们介绍了 *ReLU*(*x*)（修正线性单元）函数，当 *x* < 0 时为 0，当 *x* ≥ 0 时为 *x*。请注意，有两种方式可能导致点被错误分类：
- en: If *y* = 0 and *ŷ* = 1\. This means *score*(*w, b, x*) ≥ 0.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *y* = 0 且 *ŷ* = 1。这意味着 *score*(*w, b, x*) ≥ 0。
- en: If *y* = 1 and *ŷ* = 0\. This means *score*(*w, b, x*) < 0.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *y* = 1 且 *ŷ* = 0。这意味着 *score*(*w, b, x*) < 0。
- en: Thus, we can conveniently rewrite the perceptron error as
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以方便地将感知器误差重写为
- en: '![](../Images/AppB_04_E03.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_04_E03.png)'
- en: or in more detail, as
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 或者更详细地说，作为
- en: '![](../Images/AppB_04_E04.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_04_E04.png)'
- en: Now we can proceed to calculate the gradient ∇*PE* using the chain rule. An
    important observation that we’ll use and that the reader can verify is that the
    derivative of *ReLU*(*x*) is the step function *step*(*x*). This gradient is
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用链式法则来计算梯度 ∇*PE*。一个重要的观察结果是，我们将使用，并且读者可以验证的是，*ReLU*(*x*)的导数是步函数 *step*(*x*)。这个梯度是
- en: '![](../Images/AppB_04_E05.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_04_E05.png)'
- en: which we can rewrite as
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以重写为
- en: '![](../Images/AppB_04_E06.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_04_E06.png)'
- en: This looks complicated, but it’s actually not that hard. Let’s analyze each
    summand from the right-hand side of the previous expression. Notice that *step*(*score*(*w,
    b, x*)) is 1 if and only if *score*(*w, b, x*) > 0, and otherwise, it is 0\. This
    is precisely when *ŷ* = 1\. Similarly, *step*(–*score*(*w, b, x*)) is 1 if and
    only if *score*(*w, b, x*) < 0, and otherwise, it is 0\. This is precisely when
    *ŷ* = 0\. Therefore
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很复杂，但实际上并不难。让我们分析上一个表达式的右侧的每一项。请注意，*step*(*score*(*w, b, x*)) 当且仅当 *score*(*w,
    b, x*) > 0 时为 1，否则为 0。这正是 *ŷ* = 1 的时候。同样，*step*(–*score*(*w, b, x*)) 当且仅当 *score*(*w,
    b, x*) < 0 时为 1，否则为 0。这正是 *ŷ* = 0 的时候。因此
- en: 'If *ŷ*[i] = 0 and *y*[i] = 0:'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *ŷ*[i] = 0 且 *y*[i] = 0：
- en: −*y*[i] *x*[j]^(^i^)) *step*(−*score*(*w, b, x*)) + (1 − *y*[i]) *x*[j]^(^i^) *step*(*score*(*w,
    b, x*)) = 0
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: −*y*[i] *x*[j]^(^i^)) *step*(−*score*(*w, b, x*)) + (1 − *y*[i]) *x*[j]^(^i^) *step*(*score*(*w,
    b, x*)) = 0
- en: 'If *ŷ*[i] = 1 and *y*[i] = 1:'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *ŷ*[i] = 1 且 *y*[i] = 1：
- en: −*y*[i] *x*[j]^(^i^)) *step*(−*score*(*w, b, x*)) + (1 − *y*[i]) *x*[j]^(^i^) *step*(*score*(*w,
    b, x*)) = 0
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: −*y*[i] *x*[j]^(^i^)) *step*(−*score*(*w, b, x*)) + (1 − *y*[i]) *x*[j]^(^i^) *step*(*score*(*w,
    b, x*)) = 0
- en: 'If *ŷ*[i] = 0 and *y*[i] = 1:'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *ŷ*[i] = 0 且 *y*[i] = 1：
- en: −*y*[i] *x*[j]^(^i^)) *step*(−*score*(*w, b, x*)) + (1 − *y*[i]) *x*[j]^(^i^) *step*(*score*(*w,
    b, x*)) = −*x*[j]^(^i^))
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: −*y*[i] *x*[j]^(^i^)) *step*(−*score*(*w, b, x*)) + (1 − *y*[i]) *x*[j]^(^i^) *step*(*score*(*w,
    b, x*)) = −*x*[j]^(^i^))
- en: 'If *ŷ*[i] = 1 and *y*[i] = 0:'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *ŷ*[i] = 1 且 *y*[i] = 0：
- en: −*y*[i] *x*[j]^(^i^)) *step*(−*score*(*w, b, x*)) + (1 − *y*[i]) *x*[j]^(^i^) *step*(*score*(*w,
    b, x*)) = *x*[j]^(^i^))
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: −*y*[i] *x*[j]^(^i^)) *step*(−*score*(*w, b, x*)) + (1 − *y*[i]) *x*[j]^(^i^) *step*(*score*(*w,
    b, x*)) = *x*[j]^(^i^))
- en: This means that when calculating ∂*PE*/∂*x*[j], only the summands coming from
    misclassified points will add value.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在计算 ∂*PE*/∂*x*[j] 时，只有来自错误分类点的项会添加值。
- en: In a similar manner,
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，
- en: '![](../Images/AppB_04_E07.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_04_E07.png)'
- en: 'Thus, the gradient descent step is defined as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，梯度下降步骤定义为以下：
- en: 'Gradient descent step:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降步骤：
- en: Replace (*w*, *b*) by (*w**'*, *b**'*), where
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 将 (*w*, *b*) 替换为 (*w**'*, *b**'*)，其中
- en: '*w*[j]'' = *w*[j] + *η* Σ[i]^q[=1] −*y*[i] *x*[j]^(^i^)*step*(−*score*(*w,
    b, x*)) + (1 − *y*[i]) *x*[j]^(^i^) *step*(*score*(*w, b, x*)), and'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*[j]'' = *w*[j] + *η* Σ[i]^q[=1] −*y*[i] *x*[j]^(^i^)*step*(−*score*(*w,
    b, x*)) + (1 − *y*[i]) *x*[j]^(^i^) *step*(*score*(*w, b, x*)), 并且'
- en: '*b*'' = *b* + *η* Σ[i]^q[=1] −*y*[i]*step*(−*score*(*w, b, x*)) + (1− *y*[i])*step*(*score*(*w,
    b, x*))'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b*'' = *b* + *η* Σ[i]^q[=1] −*y*[i]*step*(−*score*(*w, b, x*)) + (1− *y*[i])*step*(*score*(*w,
    b, x*))'
- en: And again, looking at the right-hand side of the previous expression
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，查看上一个表达式的右侧
- en: 'If *ŷ*[i] = 0 and *y*[i] = 0:'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *ŷ*[i] = 0 且 *y*[i] = 0：
- en: −*y*[i]*step*(−*score*(*w, b, x*)) + (1− *y*[i])*step*(*score*(*w, b, x*)) =
    0
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: −*y*[i]*step*(−*score*(*w, b, x*)) + (1− *y*[i])*step*(*score*(*w, b, x*)) =
    0
- en: 'If *ŷ*[i] = 1 and *y*[i] = 1:'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *ŷ*[i] = 1 且 *y*[i] = 1：
- en: −*y*[i]*step*(−*score*(*w, b, x*)) + (1− *y*[i])*step*(*score*(*w, b, x*)) =
    0
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: −*y*[i]*step*(−*score*(*w, b, x*)) + (1− *y*[i])*step*(*score*(*w, b, x*)) =
    0
- en: 'If *ŷ*[i] = 0 and *y*[i] = 1:'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *ŷ*[i] = 0 且 *y*[i] = 1：
- en: −*y*[i]*step*(−*score*(*w, b, x*)) + (1− *y*[i])*step*(*score*(*w, b, x*)) =
    −*1*
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: −*y*[i]*step*(−*score*(*w, b, x*)) + (1− *y*[i])*step*(*score*(*w, b, x*)) =
    −*1*
- en: 'If *ŷ*[i] = 1 and *y*[i] = 0:'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *ŷ*[i] = 1 且 *y*[i] = 0：
- en: −*y*[i]*step*(−*score*(*w, b, x*)) + (1− *y*[i])*step*(*score*(*w, b, x*)) =
    *1*
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: −*y*[i]*step*(−*score*(*w, b, x*)) + (1− *y*[i])*step*(*score*(*w, b, x*)) =
    *1*
- en: 'This all may not mean very much, but one can code this to calculate all the
    entries of the gradient. Notice again that if the mini-batch has size *q* = 1
    and consists only of the point *x* = (*x*[1], *x*[2], …, *x*[n]) with label *y*
    and prediction *ŷ*, then the step is defined as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些可能并不意味太多，但可以编写代码来计算梯度的所有条目。再次注意，如果小批量的大小 *q* = 1，并且只包含点 *x* = (*x*[1]，*x*[2]，…，*x*[n])，具有标签
    *y* 和预测 *ŷ*，则步骤定义如下：
- en: 'Gradient descent step:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降步骤：
- en: If the point is correctly classified, don’t change *w* and *b*.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果点被正确分类，则不要改变 *w* 和 *b*。
- en: 'If the point has label *y* = 0 and is classified as *ŷ* = 1:'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果点具有标签 *y* = 0 并被分类为 *ŷ* = 1：
- en: Replace *w* by *w*' = *w* – *η**x*.
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 *w* 替换为 *w*' = *w* – *η**x*。
- en: Replace *b* by *b*' = *w* – *η*.
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 *b* 替换为 *b*' = *w* – *η*。
- en: 'If the point has label *y* = 1 and is classified as *ŷ* = 0:'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果点具有标签 *y* = 1 并被分类为 *ŷ* = 0：
- en: Replace *w* by *w*' = *w* + *η**x*.
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 *w* 替换为 *w*' = *w* + *η**x*。
- en: Replace *b* by *b*' = *w* + *η*.
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 *b* 替换为 *b*' = *w* + *η*。
- en: Note that this is precisely the perceptron trick described in the section “The
    perceptron trick” in chapter 5.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这正是第 5 章中“感知器技巧”部分描述的感知器技巧。
- en: Training a logistic regression model using gradient descent to reduce the log
    loss
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度下降法训练逻辑回归模型以减少对数损失
- en: In this subsection, we’ll calculate the gradient of the log loss function and
    use it to apply gradient descent and train a logistic regression model. In the
    logistic regression model, the predictions are *ŷ*[1], *ŷ*[2], …, *ŷ*[q] where
    each *ŷ*[i] is some real number in between 0 and 1\. To calculate the predictions,
    we first need to remember the sigmoid function *σ*(*x*), introduced in chapter
    6\. This function takes as an input any real number *x* and outputs some number
    between 0 and 1\. If *x* is a large positive number, then *σ*(*x*) is close to
    1\. If *x* is a large negative number, then *σ*(*x*) is close to 0\. The formula
    for the sigmoid function is
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将计算对数损失函数的梯度，并使用它来应用梯度下降并训练逻辑回归模型。在逻辑回归模型中，预测值是 *ŷ*[1]，*ŷ*[2]，…，*ŷ*[q]，其中每个
    *ŷ*[i] 是介于 0 和 1 之间的某个实数。为了计算预测值，我们首先需要记住第 6 章中引入的 sigmoid 函数 *σ*(*x*)。该函数将任何实数
    *x* 作为输入，并输出介于 0 和 1 之间的某个数字。如果 *x* 是一个很大的正数，那么 *σ*(*x*) 接近 1。如果 *x* 是一个很大的负数，那么
    *σ*(*x*) 接近 0。sigmoid 函数的公式是
- en: '![](../Images/AppB_04_E08.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图片 AppB_04_E08.png](../Images/AppB_04_E08.png)'
- en: The graph of *σ*(*x*) is illustrated in figure B.5.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*σ*(*x*) 的图像如图 B.5 所示。'
- en: 'The predictions of the logistic regression model are precisely the output of
    the sigmoid function, namely, for *i* = 1, 2, …, *q* they are defined as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型的预测值正是 sigmoid 函数的输出，即对于 *i* = 1, 2, …, *q*，它们被定义为以下：
- en: '![](../Images/AppB_04_E09.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图片 AppB_04_E09.png](../Images/AppB_04_E09.png)'
- en: '![](../Images/B-51.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图片 B-51](../Images/B-51.png)'
- en: Figure B.5 The sigmoid function always outputs a number between 0 and 1\. The
    output for negative numbers is close to 0, and the output for positive numbers
    is close to 1.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.5 Sigmoid 函数始终输出介于 0 和 1 之间的数字。对于负数，输出接近 0，对于正数，输出接近 1。
- en: The log loss is denoted as *LL*(*w, b, x*, *y*), which we’ll abbreviate as *LL*.
    This error function was first defined in the section “The dataset and the predictions”
    in chapter 6\. It is similar to the perceptron error function because by construction,
    it is a large number if the model made a bad prediction and a small number if
    the model made a good prediction. The log loss function is defined as
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失表示为 *LL*(*w, b, x*, *y*)，我们将简写为 *LL*。这个错误函数最初在第 6 章的“数据集和预测”部分中定义。它类似于感知器错误函数，因为按照构造，如果模型做出了错误的预测，它就是一个很大的数字；如果模型做出了好的预测，它就是一个小的数字。对数损失函数定义为
- en: '![](../Images/AppB_05_E01.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图片 AppB_05_E01.png](../Images/AppB_05_E01.png)'
- en: We can proceed to calculate the gradient ∇*LL* using the chain rule. Before
    this, let’s note that the derivative of the sigmoid function can be written as
    *σ**'*(*x*) = *σ*(*x*)|1 – *σ*(*x*)|. The details on the last calculation can
    be worked out using the quotient rule for differentiation, and they are left to
    the reader. Using this, we can calculate the derivative of *ŷ*[i] with respect
    to *w*[j]. Because *ŷ*[i] = *σ*(Σ[i]^n[=1]*w*[j]*x*[j]^(^i^) + *b*)), then by
    the chain rule,
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以继续使用链式法则计算梯度 ∇*LL*。在此之前，请注意，sigmoid 函数的导数可以写成 *σ**'*(*x*) = *σ*(*x*)|1 –
    *σ*(*x*)|。关于最后计算的详细情况，可以使用微分的有理数法则来计算，具体留给读者。使用这个，我们可以计算 *ŷ*[i] 对 *w*[j] 的导数。因为
    *ŷ*[i] = *σ*(Σ[i]^n[=1]*w*[j]*x*[j]^(^i^) + *b*))，所以根据链式法则，
- en: '![](../Images/AppB_05_E02.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图片 AppB_05_E02.png](../Images/AppB_05_E02.png)'
- en: Now, on to develop the log loss. Using the chain rule again, we get
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续开发对数损失。再次使用链式法则，我们得到
- en: '![](../Images/AppB_05_E03.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_05_E03.png)'
- en: And by the previous calculation for ∂*ŷ*[i]/∂*w*[j],
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 并且通过之前对 ∂*ŷ*[i]/∂*w*[j] 的计算，
- en: '![](../Images/AppB_05_E04.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_05_E04.png)'
- en: Simplifying, we get
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 简化后得到
- en: '![](../Images/AppB_05_E05.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_05_E05.png)'
- en: which simplifies even more as
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这进一步简化为
- en: '![](../Images/AppB_05_E06.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_05_E06.png)'
- en: Similarly, taking the derivative with respect to *b*, we get
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对 *b* 求导，我们得到
- en: '![](../Images/AppB_05_E07.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_05_E07.png)'
- en: 'Therefore, the gradient descent step becomes the following:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，梯度下降步骤变为以下：
- en: 'Gradient descent step:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降步骤：
- en: Replace (*w*, *b*) by (*w**'*, *b**'*), where
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 将 (*w*, *b*) 替换为 (*w**'*, *b**'*), 其中
- en: '*w*'' = *w* + *η*Σ[i]^q[=1](*y*[i] – *ŷ*[i])*x*^(^i^)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w**'' = *w* + *η*Σ[i]^q[=1](*y*[i] – *ŷ*[i])*x*^(^i^)'
- en: '*b*'' = *b* + *η*Σ[i]^q[=1](*y*[i] – *ŷ*[i])'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b**'' = *b* + *η*Σ[i]^q[=1](*y*[i] – *ŷ*[i])'
- en: 'Notice that when the mini-batches are of size 1, the gradient descent step
    becomes the following:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 注意当小批量的大小为1时，梯度下降步骤变为以下：
- en: Replace (*w*, *b*) by (*w**'*, *b**'*), where
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 将 (*w*, *b*) 替换为 (*w**'*, *b**'*), 其中
- en: '*w*'' = *w* + *η*(*y*[i] – *ŷ*[i])*x*^(^i^)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w**'' = *w* + *η*(*y*[i] – *ŷ*[i])*x*^(^i^)'
- en: '*b*'' = *b* + *η*(*y*[i] – *ŷ*[i])'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b**'' = *b* + *η*(*y*[i] – *ŷ*[i])'
- en: This is precisely the logistic regression trick we learned in the section “How
    to find a good logistic classifier?” in chapter 6.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们在第6章“如何找到一个好的逻辑分类器？”中学习到的逻辑回归技巧。
- en: Using gradient descent to train neural networks
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度下降训练神经网络
- en: In the section “Backpropagation” in chapter 10, we went over backpropagation—the
    process of training a neural network. This process consists of repeating a gradient
    descent step to minimize the log loss. In this subsection, we see how to actually
    calculate the derivatives to perform this gradient descent step. We’ll perform
    this process in a neural network of depth 2 (one input layer, one hidden layer,
    and one output layer), but the example is big enough to show how these derivatives
    are calculated in general. Furthermore, we’ll apply gradient descent on the error
    for only one point (in other words, we’ll do stochastic gradient descent). However,
    I encourage you to work out the derivatives for a neural network of more layers,
    and using mini-batches of points (mini-batch gradient descent).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在第10章的“反向传播”一节中，我们介绍了反向传播——训练神经网络的步骤。这个过程包括重复梯度下降步骤以最小化log损失。在本节中，我们将看到如何实际计算导数以执行这个梯度下降步骤。我们将在这个深度为2的神经网络（一个输入层、一个隐藏层和一个输出层）中执行此过程，但示例足够大，可以展示这些导数是如何在一般情况下计算的。此外，我们将只对单个点的误差应用梯度下降（换句话说，我们将进行随机梯度下降）。然而，我鼓励你计算出具有更多层的神经网络的导数，并使用点的小批量（小批量梯度下降）。
- en: In our neural network, the input layer consists of *m* input nodes, the hidden
    layer consists of *n* hidden nodes, and the output layer consists of one output
    node. The notation in this
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的神经网络中，输入层由 *m* 个输入节点组成，隐藏层由 *n* 个隐藏节点组成，输出层由一个输出节点组成。本节中的符号如下
- en: 'subsection is different than in the other ones, for the sake of simplicity,
    as follows (and illustrated in figure B.6):'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 子节不同，为了简化，如下（并在图B.6中说明）：
- en: The input is the point with coordinates *x*[1], *x*[2], …, *x*[m].
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入是坐标为 *x*[1], *x*[2], …, *x*[m] 的点。
- en: The first hidden layer has weights *V*[ij] and biases *b*[j], for *i* = 1, 2,
    …, *m* and *j* = 1, 2, …, *n*.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一隐藏层具有权重 *V*[ij] 和偏置 *b*[j]，其中 *i* = 1, 2, …, *m* 和 *j* = 1, 2, …, *n*。
- en: The second hidden layer has weights *W*[j] for *j* = 1, 2, …, *n*, and bias
    *c*.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二隐藏层具有权重 *W*[j]，其中 *j* = 1, 2, …, *n*，以及偏置 *c*。
- en: '![](../Images/B-64.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B-64.png)'
- en: Figure B.6 The process of calculating a prediction using a neural network with
    one hidden layer and sigmoid activation functions
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图B.6 使用具有一个隐藏层和sigmoid激活函数的神经网络计算预测的过程
- en: 'The way the output is calculated is via the following two equations:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的计算方式如下两个方程：
- en: '![](../Images/AppB_06_Ea01.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_06_Ea01.png)'
- en: 'To ease the calculation of the derivatives, we use the following helper variables
    *r*[j] and *s*:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化导数的计算，我们使用以下辅助变量 *r*[j] 和 *s*：
- en: '![](../Images/AppB_06_Ea02.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/AppB_06_Ea02.png)'
- en: 'In that way, we can calculate the following partial derivatives (recalling
    that the derivative of the sigmoid function is *σ**''*(*x*) = *σ**''*(*x*)[1 –
    *σ*(*x*)] and that the log loss is *L*(*y*, *ŷ*) = – *y ln*(*ŷ*) – (1 – *y*)*ln*(1
    – *ŷ*)—we’ll call it *L* for convenience):'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们可以计算以下偏导数（回忆一下sigmoid函数的导数是 *σ**'*(*x*) = *σ**'*(*x*)[1 – *σ*(*x*)]，以及log损失是
    *L*(*y*, *ŷ*) = – *y ln*(*ŷ*) – (1 – *y*)*ln*(1 – *ŷ*)——为了方便起见，我们将其称为 *L*）：
- en: '![](../Images/AppB_06_Eb01.png)'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_IMG
  zh: '![](../Images/AppB_06_Eb01.png)'
- en: '![](../Images/AppB_06_Eb02.png)'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_IMG
  zh: '![图片](../Images/AppB_06_Eb02.png)'
- en: '![](../Images/AppB_06_Eb03.png)'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_IMG
  zh: '![图片](../Images/AppB_06_Eb03.png)'
- en: '![](../Images/AppB_06_Eb04.png)'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_IMG
  zh: '![图片](../Images/AppB_06_Eb04.png)'
- en: '![](../Images/AppB_06_Eb05.png)'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_IMG
  zh: '![图片](../Images/AppB_06_Eb05.png)'
- en: '![](../Images/AppB_06_Eb06.png)'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_IMG
  zh: '![图片](../Images/AppB_06_Eb06.png)'
- en: To simplify our calculations, notice that if we multiply equations 1 and 2 and
    use the chain rule, we get
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了简化我们的计算，请注意，如果我们乘以方程 1 和 2 并使用链式法则，我们得到
- en: '![](../Images/AppB_06_Eb07.png)'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_IMG
  zh: '![图片](../Images/AppB_06_Eb07.png)'
- en: 'Now, we can use the chain rule and equations 3–7 to calculate the derivatives
    of the log loss with respect to the weights and biases as follows:'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们可以使用链式法则和方程 3–7 来计算关于权重和偏差的 log 损失的导数，如下所示：
- en: '![](../Images/AppB_06_Eb08.png)'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_IMG
  zh: '![图片](../Images/AppB_06_Eb08.png)'
- en: '![](../Images/AppB_06_Eb09.png)'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_IMG
  zh: '![图片](../Images/AppB_06_Eb09.png)'
- en: '![](../Images/AppB_06_Eb10.png)'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_IMG
  zh: '![图片](../Images/AppB_06_Eb10.png)'
- en: '![](../Images/AppB_06_Eb11.png)'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_IMG
  zh: '![图片](../Images/AppB_06_Eb11.png)'
- en: 'Using the previous equations, the gradient descent step is the following:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的方程，梯度下降步骤如下：
- en: 'Gradient descent step for neural networks:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的梯度下降步骤：
- en: '![](../Images/AppB_06_Ec01a.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/AppB_06_Ec01a.png)'
- en: The previous equations are quite complicated, and so are the equations of backpropagation
    for neural networks of even more layers. Thankfully, we can use PyTorch, TensorFlow,
    and Keras to train neural networks without having to calculate all the derivatives.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的方程相当复杂，即使是更多层的神经网络的反向传播方程也是如此。幸运的是，我们可以使用 PyTorch、TensorFlow 和 Keras 来训练神经网络，而无需计算所有导数。
- en: Using gradient descent for regularization
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用梯度下降进行正则化
- en: In the section “Modifying the error function to solve our problem” in chapter
    4, we learned regularization as a way to decrease overfitting in machine learning
    models. Regularization consists of adding a regularization term to the error function,
    which helps reduce overfitting. This term could be the L1 or the L2 norm of the
    polynomial used in the model. In the section “Techniques for training neural networks”
    in chapter 10, we learned how to apply regularization to train neural networks
    by adding a similar regularization term. Later, in the section “Distance error
    function” in chapter 11, we learned the distance error function for SVMs, which
    ensured that the two lines in the classifier stayed close to each other. The distance
    error function had the same form of the L2 regularization term.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 4 章的“修改误差函数以解决我们的问题”部分，我们学习了正则化作为降低机器学习模型过拟合的一种方法。正则化包括向误差函数中添加一个正则化项，这有助于减少过拟合。这个项可以是模型中使用的多项式的
    L1 或 L2 范数。在第 10 章的“训练神经网络的技巧”部分，我们学习了如何通过添加类似的正则化项来应用正则化以训练神经网络。后来，在第 11 章的“距离误差函数”部分，我们学习了
    SVM 的距离误差函数，这确保了分类器中的两条线彼此靠近。距离误差函数与 L2 正则化项具有相同的形式。
- en: However, in the section “An intuitive way to see regularization” in chapter
    4, we learned a more intuitive way to see regularization. In short, every gradient
    descent step that uses regularization is decreasing the values of the coefficients
    of the model by a slight amount. Let’s look at the math behind this phenomenon.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在第 4 章的“直观地看待正则化”部分，我们学习了一种更直观的方式来理解正则化。简而言之，每个使用正则化的梯度下降步骤都会略微减少模型系数的值。让我们看看这一现象背后的数学原理。
- en: 'For a model with weights *w*[1], *w*[2], …, *w*[n], the regularization terms
    were the following:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有权重 *w*[1]、*w*[2]、…、*w*[n] 的模型，正则化项如下：
- en: 'L1 regularization: *W*[1] = |*w*[1]| + |*w*[2]| + … + |*w*[n]|'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1 正则化：*W*[1] = |*w*[1]| + |*w*[2]| + … + |*w*[n]|
- en: 'L2 regularization: *W*[2] = *w*[1]² + *w*[2]² + … + *w*[n]²'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2 正则化：*W*[2] = *w*[1]² + *w*[2]² + … + *w*[n]²
- en: 'Recall that in order to not alter the coefficients too drastically, the regularization
    term is multiplied by a regularization parameter *l*. Thus, when we apply gradient
    descent, the coefficients are modified as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，为了不剧烈改变系数，正则化项乘以一个正则化参数 *l*。因此，当我们应用梯度下降时，系数的修改如下：
- en: 'L1 regularization: *w*[i] is replaced by *w*[i] – ∇ *W*[1]'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1 正则化：*w*[i] 被替换为 *w*[i] – ∇ *W*[1]
- en: 'L2 regularization: *w*[i] is replaced by *w*[i] – ∇ *W*[2]'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2 正则化：*w*[i] 被替换为 *w*[i] – ∇ *W*[2]
- en: 'Where ∇ denotes the gradient of the regularization term. In other words, ![](../Images/AppB_06_Ed01.png),
    . Since ![](../Images/AppB_06_Ed02.png), and ![](../Images/AppB_06_Ed03.png) then
    the gradient descent step is the following:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ∇ 表示正则化项的梯度。换句话说，![](../Images/AppB_06_Ed01.png)，. 由于 ![](../Images/AppB_06_Ed02.png)，和
    ![](../Images/AppB_06_Ed03.png)，那么梯度下降步骤如下：
- en: 'Gradient descent step for regularization:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化的梯度下降步骤：
- en: 'L1 regularization: Replace *a*[i] by ![](../Images/AppB_06_Ed04.png)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1 正则化：将 *a*[i] 替换为 ![](../Images/AppB_06_Ed04.png)
- en: 'L2 regularization: Replace *a*[i] by ![](../Images/AppB_06_Ed05.png)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2 正则化：将 *a*[i] 替换为 ![](../Images/AppB_06_Ed05.png)
- en: Notice that this gradient descent step always reduces the absolute value of
    the coefficient *a*[i]. In L1 regularization, we are subtracting a small value
    from *a*[i] if it is *a*[i] positive and adding a small value if it is negative.
    In L2 regularization, we are multiplying *a*[i] by a number that is slightly smaller
    than 1.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到这个梯度下降步骤总是减小系数 *a*[i] 的绝对值。在 L1 正则化中，如果 *a*[i] 是正的，我们从 *a*[i] 中减去一个小的值；如果它是负的，我们则加上一个小的值。在
    L2 正则化中，我们将 *a*[i] 乘以一个略小于 1 的数。
- en: 'Getting stuck on local minima: How it happens, and how we solve it'
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卡在局部最小值上：它是如何发生的，以及我们如何解决它
- en: As was mentioned at the beginning of this appendix, the gradient descent algorithm
    doesn’t necessarily find the minimum value of the function. As an example, look
    at figure B.7\. Let’s say that we want to find the minimum of the function in
    this figure using gradient descent. Because the first step in gradient descent
    is to start on a random point, we’ll start at the point labeled “Starting point.”
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如附录开头所述，梯度下降算法不一定能找到函数的最小值。例如，看看图 B.7。假设我们想使用梯度下降法找到图中函数的最小值。由于梯度下降的第一步是从一个随机点开始，我们将从标记为“起点”的点开始。
- en: '![](../Images/B-710.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B-710.png)'
- en: Figure B.7 We’re standing at the point labeled “Starting point.” The minimum
    value of the function is the point labeled “Minimum.” Will we be able to reach
    this minimum using gradient descent?
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.7 我们站在标记为“起点”的点。函数的最小值是标记为“最小值”的点。我们能否通过梯度下降法达到这个最小值？
- en: Figure B.8 shows the path that the gradient descent algorithm will take to find
    the minimum. Note that it succeeds in finding the closest local minimum to that
    point, but it completely misses the global minimum on the right.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.8 展示了梯度下降算法寻找最小值将采取的路径。请注意，它成功地找到了离那个点最近的局部最小值，但它完全错过了右边的全局最小值。
- en: '![](../Images/B-810.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B-810.png)'
- en: Figure B.8 Unfortunately, gradient descent didn’t help us find the minimum value
    of this function. We did manage to go down, but we got stuck at a local minimum
    (valley). How can we solve this problem?
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.8 很遗憾，梯度下降法并没有帮助我们找到这个函数的最小值。我们确实有所下降，但我们在局部最小值（山谷）处卡住了。我们该如何解决这个问题？
- en: How do we solve this problem? We can use many techniques to fix this, and in
    this section, we learn a common one called *random restart*. The solution is to
    simply run the algorithm several times, always starting at a different random
    point, and picking the minimum value that was found overall. In figure B.9, we
    use random restart to find the global minimum on a function (note that this function
    is defined only on the interval pictured, and thus, the lowest value in the interval
    is indeed the global minimum). We have chosen three random starting points, one
    illustrated by a circle, one by a square, and one by a triangle. Notice that if
    we use gradient descent on each of these three points, the square manages to find
    the global minimum of the function.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们该如何解决这个问题？我们可以使用许多技术来解决这个问题，在本节中，我们学习一个常见的称为 *随机重启动* 的方法。解决方案是简单地多次运行算法，每次从一个不同的随机点开始，并选择找到的最小值。在图
    B.9 中，我们使用随机重启动找到一个函数的全局最小值（请注意，这个函数仅在图中所示区间内定义，因此该区间内的最低值确实是全局最小值）。我们选择了三个随机起点，一个用圆圈表示，一个用正方形表示，一个用三角形表示。请注意，如果我们在这三个点上的每个点上使用梯度下降法，正方形设法找到了函数的全局最小值。
- en: '![](../Images/B-91.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B-91.png)'
- en: 'Figure B.9 The random restart technique illustrated. The function is defined
    only on this interval, with three valleys, and the global minimum located in the
    second valley. Here we run the gradient descent algorithm with three different
    starting points: the circle, the square, and the triangle. Notice that the square
    managed to find the global minimum of the function.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.9 随机重启动技术的说明。该函数仅在这一点上定义，有三个山谷，全局最小值位于第二个山谷中。在这里，我们使用三个不同的起点运行梯度下降算法：圆圈、正方形和三角形。请注意，正方形设法找到了函数的全局最小值。
- en: This method is still not guaranteed to find the global minimum, because we may
    be unlucky and pick only points that get stuck in valleys. However, with enough
    random starting points, we have a much higher chance of finding the global minimum.
    And even if we can’t find the global minimum, we may still be able to find a good
    enough local minimum that will help us train a good model.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法仍然不能保证找到全局最小值，因为我们可能运气不佳，只选取了那些陷入山谷中的点。然而，如果我们有足够的随机起始点，我们找到全局最小值的几率就会大大提高。即使我们无法找到全局最小值，我们也可能仍然能够找到一个足够好的局部最小值，这将有助于我们训练出一个好的模型。
