- en: Chapter 5\. Partitioning Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 5 章。数据分区
- en: Partitioning is defined as “the act of dividing; separation by the creation
    of a boundary that divides or keeps apart.” Data partitioning is used in tools
    like Spark, Amazon Athena, and Google BigQuery to improve query execution performance.
    To scale out big data solutions, data is divided into partitions that can be managed,
    accessed, and executed separately and in parallel.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分区被定义为“划分的行为；通过创建分隔或保持分开的边界而进行的分离”。数据分区在像 Spark、Amazon Athena 和 Google BigQuery
    这样的工具中用于改善查询执行性能。为了扩展大数据解决方案，数据被分割成可以单独管理、访问和并行执行的分区。
- en: 'As discussed in previous chapters of this book, Spark splits data into smaller
    chunks, called *partitions*, and then processes these partitions in a parallel
    fashion (many partitions can be processed concurrently) using executors on the
    worker nodes. For example, if your input has 100 billion records, then Spark might
    split it into 10,000 partitions, where each partition will have about 10 million
    elements:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本书前几章所讨论的，Spark 将数据分割成称为*分区*的较小块，然后并行处理这些分区（可以同时处理多个分区），使用工作节点上的执行器。例如，如果您的输入有
    1000 亿条记录，那么Spark可能将其分成 10,000 个分区，每个分区大约有 1,000 万个元素：
- en: 'Total records: 100,000,000,000'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总记录数：100,000,000,000
- en: 'Number of partitions: 10,000'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区数量：10,000
- en: 'Number of elements per partition: 10,000,000'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个分区的元素数量：10,000,000
- en: 'Maximum possible parallelism: 10,000'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大可能的并行性：10,000
- en: Note
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: By default, Spark implements hash-based partitioning with a `HashPartitioner`,
    which uses Java’s `Object.hashCode()` function.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Spark 使用 `HashPartitioner` 实现基于哈希的分区，该分区使用 Java 的 `Object.hashCode()`
    函数。
- en: Partitioning data can improve manageability and scalability, reduce contention,
    and optimize performance. Suppose you have hourly temperature data for cities
    in all the countries in the world (7 continents and 195 countries), and the goal
    is to query and analyze data for a given continent, country, or or set of countries.
    If you do not partition your data accordingly, for each query you’ll have to load,
    read, and apply your mapper and reducer to the entire dataset to get the result
    you’re looking for. This is not very efficient, since for most queries you only
    actually need a subset of the data. A much faster approach is to just load the
    data that you need.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分区可以改善可管理性和可扩展性，减少争用，并优化性能。假设您有全球所有国家（7大洲和195个国家）的城市的每小时温度数据，并且目标是查询和分析给定大洲、国家或一组国家的数据。如果您不按相应的方式对数据进行分区，对于每个查询，您将不得不加载、读取并应用您的映射器和减速器到整个数据集，以获得您所需要的结果。这不是非常高效的，因为对于大多数查询，您实际上只需要数据的子集。一个更快的方法是只加载您需要的数据。
- en: Data partitioning in Spark is primarily done for the purpose of parallelism
    to allow tasks to execute independently, but in query tools such as Amazon Athena
    and Google BigQuery, its purpose is to allow you to analyze a slice of the data
    rather than the whole dataset. PySpark make it very easy to physically partition
    DataFrames by column name so that these tools can perform queries efficiently.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 中的数据分区主要是为了并行处理任务而进行的，但在Amazon Athena和Google BigQuery等查询工具中，其目的是允许您分析数据的一部分而不是整个数据集。PySpark
    使按列名物理分区 DataFrame 变得非常容易，以便这些工具可以高效执行查询。
- en: Introduction to Partitions
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分区介绍
- en: By partitioning your data, you can restrict the amount of data scanned by each
    query, thus improving performance and reducing cost. For example, Amazon Athena,
    which leverages Spark and Hive for partitioning, lets you partition your data
    by any key (BigQuery provides the same functionality). Therefore, for our earlier
    example of weather data, you can just select and use specific folders for your
    query rather than using the entire data set for all countries.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对数据进行分区，您可以限制每个查询扫描的数据量，从而提高性能并降低成本。例如，Amazon Athena 利用 Spark 和 Hive 进行分区，让您可以按任意键对数据进行分区（BigQuery提供了相同的功能）。因此，对于我们之前的天气数据示例，您只需选择并使用特定的文件夹进行查询，而不是对所有国家的整个数据集进行使用。
- en: If your data is represented in a table, such as a Spark DataFrame, partitioning
    is a way of dividing that table into related parts based on the values of particular
    columns. Partitioning can be based on one or more columns (these columns are called
    partition keys). The values in these partitioned columns are used to determine
    which partition each row should be stored in. Using partitions makes it easy to
    execute queries on slices of the data rather than loading the entire dataset for
    analysis. For example, genomics data records include a total of 25 chromosomes,
    which are labeled as `{chr1, chr2, ..., chr22, chrX, chrY, chrMT}`. Since in most
    genomics analyses, you do not mix chromosomes, it makes sense to partition this
    data by chromosome ID. This can reduce the analysis time by enabling you to load
    just the data for the desired chromosome.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的数据以表格形式表示，比如 Spark DataFrame，分区是根据特定列的值将该表格划分为相关部分的一种方式。分区可以基于一个或多个列（这些列称为分区键）。分区列中的值用于确定每行应存储在哪个分区中。使用分区可以轻松地在数据片段上执行查询，而不是加载整个数据集进行分析。例如，基因组数据记录包括总共
    25 条染色体，这些染色体标记为 `{chr1, chr2, ..., chr22, chrX, chrY, chrMT}`。由于在大多数基因组分析中，不会混合使用染色体数据，因此按染色体
    ID 分区数据是有意义的。这可以通过仅加载所需染色体的数据来减少分析时间。
- en: Partitions in Spark
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 中的分区
- en: Suppose you’re using a distributed storage system like HDFS or Amazon S3, where
    your data is distributed among many cluster nodes. How do your Spark partitions
    work? As your physical data is distributed in partitions across the physical cluster,
    Spark treats each partition as a high-level logical data abstraction (RDD or DataFrame)
    in memory (and on disk if there is not sufficient memory), as illustrated in [Figure 5-1](#spark_logical_partitioning).
    The Spark cluster will optimize partition access and will read the partition closest
    to it in the network, observing data locality.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您正在使用像 HDFS 或 Amazon S3 这样的分布式存储系统，其中您的数据分布在许多集群节点之间。您的 Spark 分区如何工作？当物理数据在物理集群中的分区中分布时，Spark
    将每个分区视为内存中（如果内存不足则在磁盘上）的高级逻辑数据抽象（RDD 或 DataFrame），如 [图 5-1](#spark_logical_partitioning)
    所示。Spark 集群将优化分区访问，并读取网络中最接近它的分区，观察数据的局部性。
- en: '![partitioned data](Images/daws_0501.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![分区数据](Images/daws_0501.png)'
- en: Figure 5-1\. Logical model of partitioning in Spark
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-1\. Spark 分区的逻辑模型
- en: In Spark, the main purpose of partitioning data is to achieve maximum parallelism,
    by having executors on cluster nodes execute many tasks at the same time. Spark
    executors are launched at the start of a Spark application in coordination with
    the Spark cluster manager. They are worker node processes responsible for running
    individual tasks in a given Spark job/application. Breaking up data into partitions
    allows executors to process those partitions in parallel and independently, with
    each executor assigned its own data partition to work on (see [Figure 5-2](#spark_partitioning_in_action)).
    No synchronization is required.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中，分区数据的主要目的是通过使集群节点上的执行器同时执行多个任务来实现最大并行性。Spark 执行器在 Spark 应用程序启动时与 Spark
    集群管理器协调启动。它们是负责在给定 Spark 作业/应用程序中运行单个任务的工作节点进程。将数据分割为分区允许执行器并行和独立地处理这些分区，每个执行器分配其自己的数据分区来处理（参见
    [图 5-2](#spark_partitioning_in_action)）。不需要同步。
- en: '![partitioned data](Images/daws_0502.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![分区数据](Images/daws_0502.png)'
- en: Figure 5-2\. Spark partitioning in action
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-2\. Spark 分区的实际应用
- en: To understand how partitions enable us to achieve maximum performance and throughput
    in Spark, imagine that we have an RDD of 10 billion elements with 10,000 partitions
    (each partition will have about 1 million elements) and we want to execute a `map()`
    transformation on this RDD. Further imagine that we have a cluster of 51 nodes
    (1 master and 50 worker nodes), where the master acts as a cluster manager and
    has no executors, and each worker node can execute 5 mapper functions at the same
    time. This means that at any time 5 × 50 = 250 mappers are executing in parallel
    and independently, until we exhaust all 10,000 partitions. As each mapper finishes,
    a new one will be assigned by the cluster manager. Therefore, on average, each
    worker node will handle 10,000 / 250 = 40 partitions. This scenario guaranties
    that all worker nodes are utilized, which should be your goal when partitioning
    to achieve maximum optimization. In this scenario, if there had been 100 partitions
    (instead of 10,000), then each partition would have had about 100 million elements
    and only 100 / 5 = 20 worker nodes would have been utilized. The remaining 30
    worker nodes might be idle (underutilization indicates a waste of resources).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解分区如何使我们在Spark中实现最大性能和吞吐量，可以想象我们有一个包含100亿元素的RDD，有10,000个分区（每个分区大约有100万个元素），我们希望在这个RDD上执行`map()`转换。进一步想象我们有一个由51个节点组成的集群（1个主节点和50个工作节点），其中主节点充当集群管理器且没有执行器，每个工作节点可以同时执行5个mapper函数。这意味着任何时候会有5
    × 50 = 250个mapper并行和独立地执行，直到所有10,000个分区被用完。每个mapper完成时，集群管理器将分配一个新的mapper。因此，平均每个工作节点将处理10,000
    / 250 = 40个分区。这种情况保证了所有工作节点都得到利用，这应该是分区以实现最大优化时的目标。在这种情况下，如果分区数为100（而不是10,000），则每个分区将大约有1亿个元素，并且只有100
    / 5 = 20个工作节点会被利用。其余的30个工作节点可能处于空闲状态（低效利用表明资源浪费）。
- en: '[Figure 5-3](#example_of_partitioning_data_in_spark) shows how Spark executors
    process partitions.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-3](#example_of_partitioning_data_in_spark) 展示了Spark执行器如何处理分区。'
- en: '![partitioned data](Images/daws_0503.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![分区数据](Images/daws_0503.png)'
- en: Figure 5-3\. Example of partitioning data in Spark
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-3\. Spark中数据分区的示例
- en: In this figure, the input data is partitioned into 16 chunks. Given two executors,
    `Executor-1` and `Executor-2`, that can each process at most three partitions
    at a time, three iterations are required to process (such as through a mapper
    transformation) all of the partitions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，输入数据被分成了16个块。假设有两个执行器，`Executor-1` 和 `Executor-2`，每个执行器最多可以同时处理三个分区，因此需要三次迭代来处理（例如通过映射转换）所有分区。
- en: Another reason for partitioning in Spark is that the datasets are often so large
    that they cannot be stored in a single node. As the earlier example showed, how
    the partitioning is done is important, as it determines how the cluster’s hardware
    resources are utillized when executing any job. The optimal partitioning should
    maximize utilization of hardware resources by maximizing parallelism for data
    transformations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中分区的另一个原因是数据集通常非常大，无法存储在单个节点上。正如前面的例子所示，分区的方式很重要，因为它决定了在执行任何作业时如何利用集群的硬件资源。最优的分区应通过数据转换的最大并行性来最大化硬件资源的利用。
- en: 'The following factors affect data partitioning choices:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下因素影响数据分区选择：
- en: Available resources
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 可用资源
- en: The number of cores on which a task can run
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 任务可以运行的核心数
- en: External data sources
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 外部数据源
- en: Size of local collections, input filesystem used (such as HDFS, S3, etc.)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本地集合的大小，使用的输入文件系统（如HDFS，S3等）。
- en: Transformations used to derive RDDs and DataFrames
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 用于派生RDD和DataFrame的转换。
- en: Rules affecting the use of partitions when an RDD/DataFrame is derived from
    another RDD/DataFrame
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当RDD/DataFrame从另一个RDD/DataFrame派生时影响分区使用的规则。
- en: Let’s see how partitioning works in a Spark computing environment. When Spark
    reads a datafile into an RDD (or DataFrame), it automatically partitions that
    RDD into multiple smaller chunks, regardless of the RDD’s size. Then, when we
    apply a transformation (such as `map()`, `reduceByKey()`, etc.) on an RDD, the
    transformation is applied to each of its partitions. Spark spawns a single task
    per partition, which will run inside the executor’s JVM (each worker can process
    one task at a time). Each stage contains as many tasks as there are partitions
    of the RDD and will perform the transformations requested in that stage on all
    of the partitions in parallel. This process is illustrated by [Figure 5-4](#partitioning_data_in_spark).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看分区在 Spark 计算环境中是如何工作的。当 Spark 将数据文件读入 RDD（或 DataFrame）时，它会自动将该 RDD 分成多个较小的块，而不管
    RDD 的大小如何。然后，在我们对 RDD 应用转换（如 `map()`、`reduceByKey()` 等）时，该转换将应用于每个分区。Spark 为每个分区生成一个任务，在执行器的
    JVM 中运行（每个工作节点一次只能处理一个任务）。每个阶段包含与 RDD 分区数相同的任务，并将在所有分区上并行执行请求的转换。这个过程由 [图 5-4](#partitioning_data_in_spark)
    所示。
- en: '![partitioned data](Images/daws_0504.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![partitioned data](Images/daws_0504.png)'
- en: Figure 5-4\. Operating on partitioned data in Spark
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-4\. 在 Spark 中操作分区数据
- en: Note
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Partitions in Spark do not span multiple machines. This means that each partition
    is sent to a single worker machine, and tuples in the same partition are guaranteed
    to be on the same machine.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 中的分区不跨多台机器。这意味着每个分区被发送到单个工作机器，并且同一分区中的元组保证位于同一台机器上。
- en: Just as proper partitioning can improve the performance of your data analysis,
    improper partitioning can harm performance of your data analysis. For example,
    suppose you have a Spark cluster with 501 nodes (1 master and 500 worker nodes).
    For an RDD with 10 billion elements the proper number of partitions would be over
    500 (say, 1,000), to ensure that all cluster nodes are utilized at the same time.
    If you had 100 partitions and each worker could accept at most 2 tasks, then most
    of your worker nodes (about 400 of them) would be idle and useless. The more fully
    you utilize the worker nodes, the faster your query will run.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 就像适当的分区可以提高数据分析的性能一样，不恰当的分区可能会损害数据分析的性能。例如，假设您有一个拥有 501 个节点（1 个主节点和 500 个工作节点）的
    Spark 集群。对于一个包含 100 亿个元素的 RDD，适当的分区数量应该超过 500（比如说，1,000），以确保所有集群节点同时被利用。如果您只有
    100 个分区，并且每个工作节点最多只能接受 2 个任务，那么大多数工作节点（大约 400 个）将处于空闲和无用状态。充分利用工作节点，您的查询将运行得更快。
- en: Next, we’ll dig more deeply into how partitioning is done in Spark.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将更深入地探讨 Spark 中的分区方式。
- en: Managing Partitions
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理分区
- en: Spark has both a default and a custom partitioner. That means when you create
    an RDD, you can let Spark set the number of partitions, or you can set it explicitly.
    The number of partitions in the default case depends on the data source, the cluster
    size, and the available resources. Most of the time, the default partitioning
    will work just fine, but if you are an experienced Spark programmer, you may prefer
    to set the number of partitions explicitly using the `RDD.repartition`, `RDD.coalesce()`,
    or `DataFrame.coalesce()` function.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 同时具有默认和自定义的分区器。这意味着当您创建一个 RDD 时，您可以让 Spark 设置分区数，也可以显式地设置它。在默认情况下，分区数取决于数据源、集群大小和可用资源。大多数情况下，默认分区将工作得很好，但如果您是经验丰富的
    Spark 程序员，您可能更喜欢使用 `RDD.repartition`、`RDD.coalesce()` 或 `DataFrame.coalesce()`
    函数显式设置分区数。
- en: 'Spark offers several functions to manage partitioning. You can use `RDD.repartition(numPartitions)`
    to return a new RDD that has exactly `numPartitions` partitions. This function
    can increase or decrease the level of parallelism in the RDD, as the following
    example shows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供了几个函数来管理分区。您可以使用 `RDD.repartition(numPartitions)` 返回一个具有确切 `numPartitions`
    分区数的新 RDD。该函数可以增加或减少 RDD 中的并行级别，如下例所示：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](Images/1.png)](#co_partitioning_data_CO1-1)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_partitioning_data_CO1-1)'
- en: '`RDD.glom()` returns an RDD created by coalescing all the elements in each
    partition into a list.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`RDD.glom()` 返回一个由将每个分区中的所有元素合并成列表而创建的 RDD。'
- en: 'Internally, the `RDD.repartition()` function uses a shuffle to redistribute
    the data. If you are decreasing the number of partitions in the RDD, consider
    using `RDD.coalesce()` instead, which can avoid performing a shuffle. `RDD.coalesce(numPartitions,
    shuffle=False)` returns a new RDD that is reduced into `numPartitions` partitions
    (you don’t need to provide the second parameter, as by default the shuffle is
    avoided). This concept is demonstrated by the following example:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 内部，`RDD.repartition()`函数使用洗牌来重新分发数据。如果您减少RDD中的分区数，请考虑使用`RDD.coalesce()`，它可以避免执行洗牌。`RDD.coalesce(numPartitions,
    shuffle=False)` 返回一个新的RDD，将其减少到`numPartitions`个分区（默认情况下避免洗牌，您不需要提供第二个参数）。以下示例演示了这个概念：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Default Partitioning
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 默认分区
- en: The default partitioning of an RDD or DataFrame happens when the programmer
    does not set the number of partitions explicitly. In this case, the number of
    partitions depends on the data and resources available in the cluster.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当程序员未显式设置分区数时，默认情况下RDD或DataFrame的分区。在这种情况下，分区的数量取决于集群中可用的数据和资源。
- en: Default Number of Partitions
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 默认分区数
- en: For production environments, most of the time, the default partitioner will
    work well. It ensures that all cluster nodes are utilized and that no cluster
    nodes/executors are idle.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生产环境，大多数情况下默认的分区器效果良好。它确保所有集群节点都被利用，没有集群节点/执行器处于空闲状态。
- en: 'When you create an RDD or a DataFrame, there is an option for setting the number
    of partitions. For example, when creating an RDD from a Python collection, you
    may set the number of partitions using the following API (where `numSlices` represents
    the number of partitions, or slices, to create):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当您创建RDD或DataFrame时，可以选择设置分区数。例如，当从Python集合创建RDD时，可以使用以下API设置分区数（其中`numSlices`表示要创建的分区或切片数）：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Similarly, when you use `textfile()` to read a text file from a filesystem
    (such as HDFS or S3) and return it as an `RDD[String]`, you can set the `minPartitions`
    parameter:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，当您使用`textfile()`从文件系统（如HDFS或S3）中读取文本文件并将其作为`RDD[String]`返回时，可以设置`minPartitions`参数：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In both cases, if you do not set the optional parameter, Spark will set it
    to the default number of partitions (based on data size and available resources
    in the cluster). Here, I’ll demonstrate creating an RDD from a collection without
    setting the number of partitions. First, I’ll introduce a simple debugger function
    to display the elements of each partition:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，如果未设置可选参数，则Spark将其设置为默认的分区数（基于数据大小和集群中可用的资源）。在这里，我将演示从集合创建RDD而不设置分区数。首先，我将介绍一个简单的调试器函数来显示每个分区的元素：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'I can then create an RDD and use this to display the contents of the partitions:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我可以创建一个RDD，并使用它来显示分区的内容：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Warning
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Note that this function is intended for testing and teaching purposes only and
    should not be used in a production environment, where each partition may contain
    millions of elements.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，此函数仅用于测试和教学目的，并且不应在生产环境中使用，因为每个分区可能包含数百万个元素。
- en: Explicit Partitioning
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 显式分区
- en: As mentioned in the previous section, the programmer can also explicitly set
    the number of partitions when creating an RDD.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，程序员在创建RDD时还可以显式设置分区数。
- en: Setting the Number of Partitions
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置分区数
- en: Before you set the number of partitions explicitly in a production environment,
    you need to understand your data and your cluster. Make sure that no cluster nodes/executors
    are idle.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中显式设置分区数之前，您需要了解您的数据和集群。确保没有集群节点/执行器处于空闲状态。
- en: 'Here, I create an RDD from the same collection but specify the number of partitions
    at the time of creation:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我从相同的集合创建一个RDD，但在创建时指定了分区数：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](Images/1.png)](#co_partitioning_data_CO2-1)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_partitioning_data_CO2-1)'
- en: The number of partitions is 3.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 分区数量为3。
- en: 'Next, let’s debug the created RDD and view the contents of the partitions:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们调试创建的RDD并查看分区的内容：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can then apply the `mapPartitions()` transformation on this RDD:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以在这个RDD上应用`mapPartitions()`转换：
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Physical Partitioning for SQL Queries
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SQL查询的物理分区
- en: In this section, our focus is on the physical partitioning of data rather than
    RDD and DataFrame partitioning. Physical partitioning is a technique to improve
    the performance of queries on data utilized by query tools like Hive, Amazon Athena,
    and Google BigQuery. Athena and BigQuery are serverless services for querying
    data using SQL. Given a SQL query, proper physical data partitioning at the field
    level enablles us to read, scan, and query one or more slices of a dataset rather
    than reading and analyzing the whole dataset, greatly improving query performance.
    Spark also allows us to implement physical data partitioning on disk, as you’ll
    see in the next section.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们关注的是数据的物理分区，而不是RDD和DataFrame的分区。物理分区是一种技术，用于提高类似Hive、Amazon Athena和Google
    BigQuery等查询工具对数据执行查询的性能。Athena和BigQuery是使用SQL查询数据的无服务器服务。通过适当的字段级物理数据分区，可以使我们能够读取、扫描和查询数据集的一个或多个片段，而不是读取和分析整个数据集，极大地提高了查询性能。Spark也允许我们在磁盘上实现物理数据分区，您将在下一节中看到。
- en: Note
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Partitioning data by specific fields (which are used in SQL’s `WHERE` clause)
    plays a crucial role when querying data with Athena or BigQuery. By limiting the
    volume of data scanned, it dramatically speeds up query execution and reducing
    costs, since cost is based on the amount of data scanned.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据按特定字段（在SQL的`WHERE`子句中使用的字段）分区在使用Athena或BigQuery查询数据时起着关键作用。通过限制扫描的数据量，显著加快查询执行速度并降低成本，因为成本是基于扫描的数据量计算的。
- en: 'Consider our earlier example of temperature data for cities around the world.
    By looking at the data, you can see that each continent has a list of countries,
    and each country has a set of cities. If you are going to query this data by continent,
    country, and city, then it makes a lot of sense to partition your data by these
    three fields: `(continent, country, city)`. The simple partitioning solution will
    be to create one folder per continent, then partition each continent by country,
    and finally partition each country by city. Then, instead of scanning the entire
    directory structure under *<root-dir>/*, the following query:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们早期关于世界各地城市温度数据的例子。通过查看数据，您可以看到每个大洲都有一组国家，每个国家都有一组城市。如果您要按大洲、国家和城市查询此数据，那么按这三个字段`(continent,
    country, city)`分区数据就非常有意义。简单的分区解决方案是为每个大洲创建一个文件夹，然后按国家分区每个大洲，最后按城市分区每个国家。然后，而不是扫描*<root-dir>/*下的整个目录结构，以下查询：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'will only scale this:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 只会扩展这个：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As this example shows, partitioning can enable us to scan a very limited portion
    of our data, rather than the whole dataset. For example, if you have a query that
    involves the United States, you’ll only need to scan one folder rather than scanning
    all 195 folders. In big data analysis, partitioning data by directories is very
    effective since we do not have an indexing mechanism like with relational tables.
    In fact, you can think of partitioning as a very simple indexing mechanism. Partitioning
    allows you to limit the amount of data scanned by each query, thus improving performance
    and reducing costs.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这个例子所示，分区可以使我们扫描数据的一个非常有限的部分，而不是整个数据集。例如，如果您有一个涉及美国的查询，您只需要扫描一个文件夹，而不是扫描所有195个文件夹。在大数据分析中，按目录分区数据非常有效，因为我们没有像关系表那样的索引机制。事实上，您可以将分区视为非常简单的索引机制。分区允许您限制每个查询扫描的数据量，从而提高性能并降低成本。
- en: 'Let’s look at another example. Given a world temperature dataset, you could
    create this partitioned table as follows in Amazon Athena:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看另一个例子。给定一个世界温度数据集，您可以在Amazon Athena中按以下方式创建这个分区表：
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](Images/1.png)](#co_partitioning_data_CO3-1)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_partitioning_data_CO3-1)'
- en: First partition by `continent`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 首先按`continent`分区。
- en: '[![2](Images/2.png)](#co_partitioning_data_CO3-2)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_partitioning_data_CO3-2)'
- en: Then partition by `country`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然后按`country`分区。
- en: '[![3](Images/3.png)](#co_partitioning_data_CO3-3)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_partitioning_data_CO3-3)'
- en: Finally, partition by `city`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，按`city`分区。
- en: If you then query this table and specify a partition in the `WHERE` clause,
    Amazon Athena will scan the data only from that partition.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您然后查询此表并在`WHERE`子句中指定一个分区，Amazon Athena将只从该分区扫描数据。
- en: '![partitioned data](Images/daws_0505.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![partitioned data](Images/daws_0505.png)'
- en: Figure 5-5\. Querying partitioned data
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-5\. 查询分区数据
- en: 'Note that if you were qoing to query this data by `year`, `month`, and `day`,
    you could partition the same data into another form, where the partition fields
    are `year`, `month`, and `day`. In this case your schema will change to the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您打算按 `year`、`month` 和 `day` 查询此数据，可以将同一数据分区到另一种形式，其中分区字段为 `year`、`month`
    和 `day`。在这种情况下，您的模式将变成以下形式：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](Images/1.png)](#co_partitioning_data_CO4-1)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_partitioning_data_CO4-1)'
- en: First partition by `year`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先按 `year` 进行分区。
- en: '[![2](Images/2.png)](#co_partitioning_data_CO4-2)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_partitioning_data_CO4-2)'
- en: Then partition by `month`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后按 `month` 进行分区。
- en: '[![3](Images/3.png)](#co_partitioning_data_CO4-3)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_partitioning_data_CO4-3)'
- en: Finally, partition by `day`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，按 `day` 进行分区。
- en: 'With this new schema, you can issue SQL queries like this one:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种新模式，您可以像下面这样发出 SQL 查询：
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As this example illustrates, to partition your data effectively you need to
    understand the queries that you will execute against your table (i.e., your data
    expressed as a table).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这个例子所说明的，为了有效地对数据进行分区，您需要理解将针对表执行的查询（即将数据表达为表）。
- en: 'As another example, suppose you have customer data, where each record has the
    following format:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设您有客户数据，每条记录格式如下：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Further, assume that your goal is to analyze data by a given year, or by a combination
    of year and month. Partitioning the data is a good idea, as it will allow you
    to limit the amount of data scanned by selecting specific folders (by year or
    by year and month). [Figure 5-6](#partitioned_data_by_year_month_day) shows what
    this might look like.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，假设您的目标是按年分析数据，或者按年和月的组合分析数据。分区数据是一个好主意，因为它允许您通过选择特定文件夹（按年或按年和月）来限制扫描的数据量。[Figure 5-6](#partitioned_data_by_year_month_day)
    展示了可能的效果。
- en: '![partitioned data](Images/daws_0506.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![partitioned data](Images/daws_0506.png)'
- en: Figure 5-6\. Querying data partitioned by year/month/day
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 5-6\. 按年/月/日分区查询数据
- en: Now, lets dig into how to partition data in Spark.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解如何在 Spark 中对数据进行分区。
- en: Physical Partitioning of Data in Spark
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 中数据的物理分区
- en: 'Spark offers a simple DataFrame API for physical partitioning of your data.
    Let `df` denote a DataFrame for our example data, with records of the form:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供了一个简单的 DataFrame API 用于数据的物理分区。让 `df` 表示我们示例数据的 DataFrame，其记录形式如下：
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can physically partition our data using the `DataFrameWriter.partitionBy()`
    method, either into a text format (row-based) or a binary format such as Parquet
    (column-based). The following subsections show how.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `DataFrameWriter.partitionBy()` 方法对数据进行物理分区，可以是文本格式（基于行）或 Parquet 等二进制格式（基于列）。以下各小节展示了具体方法。
- en: Partition as Text Format
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 按文本格式分区
- en: 'The following code snippet shows how to partition data (represented as a DataFrame)
    by year and month into a text format. First, we create a DataFrame with four columns:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段显示了如何将数据（表示为 DataFrame）按年和月分区到文本格式中。首先，我们创建一个具有四列的 DataFrame：
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we add two new columns (`year` and `month`):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们添加两个新列（`year` 和 `month`）：
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[![1](Images/1.png)](#co_partitioning_data_CO5-1)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_partitioning_data_CO5-1)'
- en: Add a `year` column.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 添加一个 `year` 列。
- en: '[![2](Images/2.png)](#co_partitioning_data_CO5-2)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_partitioning_data_CO5-2)'
- en: Add a `month` column.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 添加一个 `month` 列。
- en: 'Finally, we partition by `year` and `month` and then write and save our DataFrame:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们按 `year` 和 `month` 进行分区，然后写入并保存我们的 DataFrame：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](Images/1.png)](#co_partitioning_data_CO6-1)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_partitioning_data_CO6-1)'
- en: Get a `DataFrameWriter` object.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 获取一个 `DataFrameWriter` 对象。
- en: '[![2](Images/2.png)](#co_partitioning_data_CO6-2)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_partitioning_data_CO6-2)'
- en: Partition the data by the desired columns.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 按所需列对数据进行分区。
- en: '[![3](Images/3.png)](#co_partitioning_data_CO6-3)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_partitioning_data_CO6-3)'
- en: Save each partition in a text file.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个分区保存为文本文件。
- en: A complete solution for partitioning data is available in the book’s GitHub
    repository, in the file *partition_data_as_text_by_year_month.py*. A sample run
    with detailed output is also provided, in the file *partition_data_as_text_by_year_month.log*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 数据按年和月分区的完整解决方案可在该书的 GitHub 存储库中找到，文件名为 *partition_data_as_text_by_year_month.py*。还提供了包含详细输出的示例运行日志，文件名为
    *partition_data_as_text_by_year_month.log*。
- en: Partition as Parquet Format
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 按 Parquet 格式分区
- en: 'Partitioning data into [Parquet format](http://parquet.apache.org) has a few
    advantages: data aggregation can be done faster than with text data since Parquet
    stores data in columnar format, and Parquet stores metadata as well. The process
    is the same, except instead of using the `text()` function of the `DataFrameWriter`
    class, you use the `parquet()` function:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据分区为[Parquet格式](http://parquet.apache.org)具有几个优点：与文本数据相比，数据聚合可以更快完成，因为Parquet以列格式存储数据，并且Parquet还存储元数据。该过程相同，只是不再使用`DataFrameWriter`类的`text()`函数，而是使用`parquet()`函数：
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If desired, you may partition your data by other columnar formats too, such
    as [ORC](http://orc.apache.org) or [CarbonData](http://carbondata.apache.org).
    If you want to just create a single partitioned file per partition, you can repartition
    the data before partitioning. Spark’s `repartition(numPartitions, *cols)` function
    returns a new DataFrame partitioned by the given partitioning expressions. The
    resulting DataFrame is hash-partitioned. For example, this creates a single output
    file per partition `(''year'', ''month'')`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，您可以按其他列格式分区数据，例如[ORC](http://orc.apache.org)或[CarbonData](http://carbondata.apache.org)。如果您只想为每个分区创建单个分区文件，您可以在分区之前重新分区数据。Spark的`repartition(numPartitions,
    *cols)`函数返回一个新的DataFrame，其分区由给定的分区表达式进行哈希分区。例如，这将为每个`('year', 'month')`分区创建一个单独的输出文件：
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can view the physical partitioning of data by examining the output path:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过检查输出路径来查看数据的物理分区：
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: How to Query Partitioned Data
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何查询分区数据
- en: 'To optimize query performance, you should include the physically partitioned
    column(s) in your SQL `WHERE` clauses. For example, if you have partitioned your
    data by `("year", "month", "day")`, then the following will be optimized queries:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化查询性能，您应该在SQL `WHERE`子句中包含物理分区列。例如，如果您按`("year", "month", "day")`分区数据，则以下查询将被优化：
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `WHERE` clause will guide the query engine to analyze slices of the data
    rather than the whole dataset, which is what it will do if you query non-partitioned
    columns. Let’s take a look at an example using Amazon Athena.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`WHERE`子句将指导查询引擎分析数据的片段，而不是整个数据集，如果查询非分区列，则会这样做。让我们看一个使用Amazon Athena的示例。'
- en: Amazon Athena Example
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon Athena示例
- en: 'To access and query your data in Athena using SQL, you need to implement the
    following simple steps:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用SQL在Athena中访问和查询您的数据，您需要执行以下简单步骤：
- en: 'Consider the types of queries you will issue, then partition your data accordingly.
    For example, if you’re working with genome data and your SQL queries will look
    like this:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑您将发出的查询类型，然后相应地分区数据。例如，如果您正在处理基因组数据，并且您的SQL查询看起来像这样：
- en: '[PRE23]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then you should partition your data by the `chromosome` column. Load your data
    into a DataFrame (which includes a `chromosome` column), then partition it by
    chromosome and save it in S3 in Parquet format:'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，您应该按`chromosome`列分区数据。将数据加载到DataFrame中（其中包括`chromosome`列），然后按染色体分区并以Parquet格式保存到S3中：
- en: '[PRE24]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, define your schema, specifying the same S3 location you defined in the
    previous step:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义您的架构，指定与前面步骤中定义的相同的S3位置：
- en: '[PRE25]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note that the `chromosome` column is a data field defined in the `PARTITIONED
    BY` section.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，`chromosome`列是在`PARTITIONED BY`部分中定义的数据字段。
- en: Now that your schema is ready, you can execute/run it (this will create metadata
    used by Amazon Athena).
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在您的架构准备就绪，您可以执行/运行它（这将创建Amazon Athena使用的元数据）。
- en: 'Load your partitions:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载您的分区：
- en: '[PRE26]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Once your partitions are ready, you can start executing SQL queries like this
    one:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦您的分区准备就绪，您可以开始执行像这样的SQL查询：
- en: '[PRE27]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Since you’ve partitioned your data by the `chromosome` column, only one directory,
    `chromosome=chr7`, will be read/scanned for this SQL query.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您已按`chromosome`列分区数据，因此仅会读取/扫描一个名为`chromosome=chr7`的目录，用于此SQL查询。
- en: Summary
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'Partitioning in Spark is the process of splitting data (expressed as an RDD
    or DataFrame) into multiple partitions on which you can execute transformations
    in parallel, allowing for faster completion of data analysis tasks. You can also
    write partitioned data into multiple subdirectories in a filesystem for faster
    reads by downstream systems. To recap:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中的分区是将数据（表示为RDD或DataFrame）分割为多个分区的过程，您可以并行执行转换，从而更快地完成数据分析任务。您还可以将分区数据写入文件系统中的多个子目录，以便下游系统更快地读取。回顾一下：
- en: Physical data partitioning involves partitioning data (expressed as an RDD or
    DataFrame) by data fields/columns into smaller pieces (chunks) in order to manage
    and access the data at a more fine-grained level.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物理数据分区涉及按数据字段/列将数据（表达为RDD或DataFrame）分割成较小的片段（块），以便以更精细的级别管理和访问数据。
- en: Data partitioning enables us to reduce the cost of storing a large amount of
    data as well as speeding up the processing of big datasets.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分区使我们能够减少存储大量数据的成本，同时加快大数据集的处理速度。
- en: When using serverless services such as Amazon Athena and Google BigQuery you
    need to partition your data by fields/columns, mainly used in the `WHERE` clause
    of SQL queries. It’s important to understand the kinds of queries you’ll be making
    and partition the data accordingly.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当使用像Amazon Athena和Google BigQuery这样的无服务器服务时，需要按字段/列对数据进行分区，主要用于SQL查询的`WHERE`子句。了解将要执行的查询类型并相应地分区数据非常重要。
- en: 'In a nutshell, data partitioning gives us the following advantages:'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简言之，数据分区给我们带来以下优势：
- en: It improves query performance and manageability. For a given query, you just
    analyze the relevant slice(s) of data based on the query clause.
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提高了查询性能和可管理性。对于给定的查询，你只需分析基于查询子句的相关数据片段。
- en: It reduces the cost of querying the data, which is based on the amount of data
    scanned.
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它减少了查询数据的成本，这是基于扫描的数据量。
- en: It simplifies common ETL tasks, as you can browse and view data based on the
    partitions.
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它简化了常见的ETL任务，因为你可以基于分区浏览和查看数据。
- en: It makes ad hoc querying easier and faster, since you can analyze slices of
    the data instead of the whole dataset.
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使得临时查询更加简单和快速，因为你可以分析数据的片段而不是整个数据集。
- en: It enables us to simulate partial indexing of relational database tables.
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使我们能够模拟关系数据库表的部分索引。
- en: Next, we’ll look at graph algorithms.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一下图算法。
