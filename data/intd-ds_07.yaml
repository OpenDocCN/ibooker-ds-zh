- en: Chapter 8\. Text mining and text analytics
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第8章。文本挖掘和文本分析
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Understanding the importance of text mining
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解文本挖掘的重要性
- en: Introducing the most important concepts in text mining
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍文本挖掘中的最重要的概念
- en: Working through a text mining project
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完成文本挖掘项目
- en: Most of the human recorded information in the world is in the form of written
    text. We all learn to read and write from infancy so we can express ourselves
    through writing and learn what others know, think, and feel. We use this skill
    all the time when reading or writing an email, a blog, text messages, or this
    book, so it’s no wonder written language comes naturally to most of us. Businesses
    are convinced that much value can be found in the texts that people produce, and
    rightly so because they contain information on what those people like, dislike,
    what they know or would like to know, crave and desire, their current health or
    mood, and so much more. Many of these things can be relevant for companies or
    researchers, but no single person can read and interpret this tsunami of written
    material by themself. Once again, we need to turn to computers to do the job for
    us.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 世界上大部分人类记录的信息都是以书面文字的形式存在的。我们从婴儿时期就开始学习阅读和写作，以便通过写作表达自己，并学习他人的知识、想法和感受。我们在阅读或撰写电子邮件、博客、短信或这本书时都会使用这项技能，因此对大多数人来说，书面语言是自然而然的。企业相信，人们产生的文本中可以找到很多价值，这是正确的，因为这些文本包含了关于这些人喜欢什么、不喜欢什么、知道什么或想了解什么、渴望和需求、他们的当前健康状况或情绪等等的信息。其中许多内容对公司或研究人员来说都可能是相关的，但没有人能够独自阅读和解释这些海量的书面材料。再次，我们需要求助于计算机来完成这项工作。
- en: Sadly, however, the natural language doesn’t come as “natural” to computers
    as it does to humans. Deriving meaning and filtering out the unimportant from
    the important is still something a human is better at than any machine. Luckily,
    data scientists can apply specific text mining and text analytics techniques to
    find the relevant information in heaps of text that would otherwise take them
    centuries to read themselves.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，不幸的是，自然语言对计算机来说并不像对人类那样“自然”。从意义中提取并过滤掉不重要的信息仍然是人类比任何机器都擅长的事情。幸运的是，数据科学家可以应用特定的文本挖掘和文本分析技术，从他们自己可能需要数百年时间才能阅读的文本堆中找到相关信息。
- en: '*Text mining* or *text analytics* is a discipline that combines language science
    and computer science with statistical and machine learning techniques. Text mining
    is used for analyzing texts and turning them into a more structured form. Then
    it takes this structured form and tries to derive insights from it. When analyzing
    crime from police reports, for example, text mining helps you recognize persons,
    places, and types of crimes from the reports. Then this new structure is used
    to gain insight into the evolution of crimes. See [figure 8.1](#ch08fig01).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*文本挖掘*或*文本分析*是一门结合语言科学和计算机科学、统计和机器学习技术的学科。文本挖掘用于分析文本并将它们转化为更结构化的形式。然后，它尝试从这个结构化形式中提取见解。例如，在分析警察报告中的犯罪时，文本挖掘可以帮助你从报告中识别人员、地点和犯罪类型。然后，这种新的结构被用来深入了解犯罪的演变。参见[图8.1](#ch08fig01)。'
- en: Figure 8.1\. In text analytics, (usually) the first challenge is to structure
    the input text; then it can be thoroughly analyzed.
  id: totrans-8
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.1. 在文本分析中，（通常）第一个挑战是结构化输入文本；然后它可以被彻底分析。
- en: '![](Images/08fig01_alt.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig01_alt.jpg)'
- en: While language isn’t limited to the natural language, the focus of this chapter
    will be on *Natural Language Processing (NLP)*. Examples of non-natural languages
    would be machine logs, mathematics, and Morse code. Technically even Esperanto,
    Klingon, and Dragon language aren’t in the field of natural languages because
    they were invented deliberately instead of evolving over time; they didn’t come
    “natural” to us. These last languages are nevertheless fit for natural communication
    (speech, writing); they have a grammar and a vocabulary as all natural languages
    do, and the same text mining techniques could apply to them.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然语言并不仅限于自然语言，但本章的重点将放在*自然语言处理（NLP）*上。非自然语言的例子包括机器日志、数学和莫尔斯电码。技术上讲，甚至世界语、克林贡语和龙语也不属于自然语言的范畴，因为它们是人为发明而非随着时间的推移演变而来的；它们对我们来说并非“自然”。然而，这些最后提到的语言适合自然交流（口语、写作）；它们拥有像所有自然语言一样的语法和词汇，并且可以使用相同的文本挖掘技术。
- en: 8.1\. Text mining in the real world
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1. 真实世界中的文本挖掘
- en: In your day-to-day life you’ve already come across text mining and natural language
    applications. Autocomplete and spelling correctors are constantly analyzing the
    text you type before sending an email or text message. When Facebook autocompletes
    your status with the name of a friend, it does this with the help of a technique
    called *named entity recognition*, although this would be only one component of
    their repertoire. The goal isn’t only to detect that you’re typing a noun, but
    also to guess you’re referring to a person and recognize who it might be. Another
    example of named entity recognition is shown in [figure 8.2](#ch08fig02). Google
    knows Chelsea is a football club but responds differently when asked for a person.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的日常生活中，你已经遇到了文本挖掘和自然语言应用。自动补全和拼写检查器在发送电子邮件或短信之前会不断分析你输入的文本。当Facebook使用名为“命名实体识别”的技术自动补全你的状态时，它会帮助你完成这个过程，尽管这将是他们技能库中的一个组成部分。目标不仅仅是检测你正在输入一个名词，而是猜测你指的是一个人并识别可能是谁。命名实体识别的另一个例子在[图8.2](#ch08fig02)中展示。谷歌知道切尔西是一个足球俱乐部，但在被问及一个人时，它的回应则不同。
- en: Figure 8.2\. The different answers to the queries “Who is Chelsea?” and “What
    is Chelsea?” imply that Google uses text mining techniques to answer these queries.
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.2。对查询“切尔西是谁？”和“切尔西是什么？”的不同回答表明，谷歌使用文本挖掘技术来回答这些问题。
- en: '![](Images/08fig02_alt.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig02_alt.jpg)'
- en: 'Google uses many types of text mining when presenting you with the results
    of a query. What pops up in your own mind when someone says “Chelsea”? Chelsea
    could be many things: a person; a soccer club; a neighborhood in Manhattan, New
    York or London; a food market; a flower show; and so on. Google knows this and
    returns different answers to the question “Who is Chelsea?” versus “What is Chelsea?”
    To provide the most relevant answer, Google must do (among other things) all of
    the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌在呈现查询结果时使用了多种类型的文本挖掘。当有人说“切尔西”时，你脑海中首先想到的是什么？切尔西可能指很多事物：一个人；一个足球俱乐部；纽约或伦敦的一个社区；一个食品市场；一个花展；等等。谷歌知道这一点，并在回答“切尔西是谁？”和“切尔西是什么？”这两个问题时给出不同的答案。为了提供最相关的答案，谷歌必须做（包括其他事情）以下所有事情：
- en: Preprocess all the documents it collects for named entities
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对它收集的所有文档进行命名实体预处理
- en: Perform language identification
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行语言识别
- en: Detect what type of entity you’re referring to
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测你指的是哪种实体
- en: Match a query to a result
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将查询与结果进行匹配
- en: Detect the type of content to return (PDF, adult-sensitive)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测要返回的内容类型（PDF，成人敏感）
- en: This example shows that text mining isn’t only about the direct meaning of text
    itself but also involves meta-attributes such as language and document type.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子表明，文本挖掘不仅仅是关于文本的直接含义，还涉及到元属性，如语言和文档类型。
- en: Google uses text mining for much more than answering queries. Next to shielding
    its Gmail users from spam, it also divides the emails into different categories
    such as social, updates, and forums, as shown in [figure 8.3](#ch08fig03).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌在回答查询时不仅使用文本挖掘，还用于其他许多方面。除了保护其Gmail用户免受垃圾邮件的侵扰外，它还将电子邮件分为不同的类别，如社交、更新和论坛，如图8.3所示。
- en: Figure 8.3\. Emails can be automatically divided by category based on content
    and origin.
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.3。根据内容和来源，可以自动根据内容将电子邮件分类。
- en: '![](Images/08fig03_alt.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig03_alt.jpg)'
- en: It’s possible to go much further than answering simple questions when you combine
    text with other logic and mathematics.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当你结合文本与其他逻辑和数学时，你可以在回答简单问题之外走得更远。
- en: This allows for the creation of *automatic reasoning engines* driven by natural
    language queries. [Figure 8.4](#ch08fig04) shows how “Wolfram Alpha,” a computational
    knowledge engine, uses text mining and automatic reasoning to answer the question
    “Is the USA population bigger than China?”
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得基于自然语言查询的*自动推理引擎*的创建成为可能。[图8.4](#ch08fig04)展示了计算知识引擎“Wolfram Alpha”如何使用文本挖掘和自动推理来回答问题“美国人口是否比中国多？”
- en: Figure 8.4\. The Wolfram Alpha engine uses text mining and logical reasoning
    to answer a question.
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.4。Wolfram Alpha引擎使用文本挖掘和逻辑推理来回答问题。
- en: '![](Images/08fig04_alt.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig04_alt.jpg)'
- en: If this isn’t impressive enough, the IBM Watson astonished many in 2011 when
    the machine was set up against two human players in a game of *Jeopardy*. *Jeopardy*
    is an American quiz show where people receive the answer to a question and points
    are scored for guessing the correct question for that answer. See [figure 8.5](#ch08fig05).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这还不够令人印象深刻，IBM Watson在2011年让许多人惊讶，当时机器在一场*Jeopardy*游戏中对抗两名人类玩家。*Jeopardy*是一档美国问答节目，人们收到问题的答案，并因猜测出正确的问题而得分。参见[图8.5](#ch08fig05)。
- en: Figure 8.5\. IBM Watson wins *Jeopardy* against human players.
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.5。IBM Watson在*Jeopardy*游戏中战胜人类玩家。
- en: '![](Images/08fig05_alt.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig05_alt.jpg)'
- en: It’s safe to say this round goes to artificial intelligence. IBM Watson is a
    cognitive engine that can interpret natural language and answer questions based
    on an extensive knowledge base.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 可以肯定地说，这一轮人工智能获胜。IBM Watson是一个认知引擎，它可以解释自然语言，并基于广泛的知识库回答问题。
- en: 'Text mining has many applications, including, but not limited to, the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 文本挖掘有许多应用，包括但不限于以下内容：
- en: Entity identification
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实体识别
- en: Plagiarism detection
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抄袭检测
- en: Topic identification
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题识别
- en: Text clustering
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本聚类
- en: Translation
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 翻译
- en: Automatic text summarization
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动文本摘要
- en: Fraud detection
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欺诈检测
- en: Spam filtering
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邮件过滤
- en: Sentiment analysis
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析
- en: 'Text mining is useful, but is it difficult? Sorry to disappoint: Yes, it is.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 文本挖掘很有用，但它困难吗？很抱歉让你失望：是的，它很困难。
- en: When looking at the examples of Wolfram Alpha and IBM Watson, you might have
    gotten the impression that text mining is easy. Sadly, no. In reality text mining
    is a complicated task and even many seemingly simple things can’t be done satisfactorily.
    For instance, take the task of guessing the correct address. [Figure 8.6](#ch08fig06)
    shows how difficult it is to return the exact result with certitude and how Google
    Maps prompts you for more information when looking for “Springfield.” In this
    case a human wouldn’t have done any better without additional context, but this
    ambiguity is one of the many problems you face in a text mining application.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当查看Wolfram Alpha和IBM Watson的示例时，你可能会得到文本挖掘很容易的印象。遗憾的是，并非如此。实际上，文本挖掘是一项复杂的任务，甚至许多看似简单的事情也无法令人满意地完成。例如，考虑猜测正确地址的任务。[图8.6](#ch08fig06)显示了如何难以确信地返回精确的结果，以及当搜索“Springfield”时，Google
    Maps如何提示你提供更多信息。在这种情况下，如果没有额外的上下文，人类也不会做得更好，但这个歧义是你在文本挖掘应用中面临的问题之一。
- en: Figure 8.6\. Google Maps asks you for more context due to the ambiguity of the
    query “Springfield.”
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.6。由于查询“Springfield”的歧义，Google Maps要求你提供更多信息。
- en: '![](Images/08fig06.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig06.jpg)'
- en: 'Another problem is *spelling mistakes* and *different (correct) spelling* forms
    of a word. Take the following three references to New York: “NY,” “Neww York,”
    and “New York.” For a human, it’s easy to see they all refer to the city of New
    York. Because of the way our brain interprets text, understanding text with spelling
    mistakes comes naturally to us; people may not even notice them. But for a computer
    these are unrelated strings unless we use algorithms to tell it that they’re referring
    to the same entity. Related problems are synonyms and the use of pronouns. Try
    assigning the right person to the pronoun “she” in the next sentences: “John gave
    flowers to Marleen’s parents when he met her parents for the first time. She was
    so happy with this gesture.” Easy enough, right? Not for a computer.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是对一个词的*拼写错误*和*不同的（正确的）拼写形式*。以下是对纽约的三个引用：“NY”，“Neww York”，和“New York”。对人类来说，很容易看出它们都指的是纽约市。由于我们的大脑解释文本的方式，理解带有拼写错误的文本对我们来说很自然；人们甚至可能没有注意到它们。但对于计算机来说，除非我们使用算法告诉它它们指的是同一个实体，否则这些是无关的字符串。相关问题是同义词和代词的使用。尝试在以下句子中为代词“她”分配正确的人：“John在他第一次见到Marleen的父母时给了Marleen的父母花。她对此举感到非常高兴。”这很容易，对吗？不对，对计算机来说不是这样。
- en: We can solve many similar problems with ease, but they often prove hard for
    a machine. We can train algorithms that work well on a specific problem in a well-defined
    scope, but more general algorithms that work in all cases are another beast altogether.
    For instance, we can teach a computer to recognize and retrieve US account numbers
    from text, but this doesn’t generalize well to account numbers from other countries.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松解决许多类似的问题，但它们通常对机器来说很难。我们可以训练算法在定义良好的范围内解决特定问题，但更通用的算法在所有情况下都是另一回事。例如，我们可以教会计算机从文本中识别和检索美国账户号码，但这并不能很好地推广到其他国家的账户号码。
- en: 'Language algorithms are also sensitive to the context the language is used
    in, even if the language itself remains the same. English models won’t work for
    Arabic and vice versa, but even if we keep to English—an algorithm trained for
    Twitter data isn’t likely to perform well on legal texts. Let’s keep this in mind
    when we move on to the chapter case study: there’s no perfect, one-size-fits-all
    solution in text mining.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 语言算法对语言使用的上下文也很敏感，即使语言本身保持不变。英语模型对阿拉伯语不起作用，反之亦然，但即使我们坚持使用英语——为Twitter数据训练的算法在法律文本上可能表现不佳。当我们继续进行章节案例研究时，请记住这一点：在文本挖掘中没有完美的一劳永逸的解决方案。
- en: 8.2\. Text mining techniques
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2. 文本挖掘技术
- en: 'During our upcoming case study we’ll tackle the problem of *text classification*:
    automatically classifying uncategorized texts into specific categories. To get
    from raw textual data to our final destination we’ll need a few data mining techniques
    that require background information for us to use them effectively. The first
    important concept in text mining is the “bag of words.”'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们即将进行的案例研究中，我们将解决*文本分类*问题：自动将未分类的文本分类到特定的类别中。要从原始文本数据到达我们的最终目的地，我们需要一些数据挖掘技术，这些技术需要背景信息以便我们有效地使用它们。文本挖掘中的第一个重要概念是“词袋”。
- en: 8.2.1\. Bag of words
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.1. 词袋
- en: 'To build our classification model we’ll go with the bag of words approach.
    *Bag of words* is the simplest way of structuring textual data: every document
    is turned into a word vector. If a certain word is present in the vector it’s
    labeled “True”; the others are labeled “False”. [Figure 8.7](#ch08fig07) shows
    a simplified example of this, in case there are only two documents: one about
    the television show *Game of Thrones* and one about data science. The two word
    vectors together form the *document-term matrix*. The document-term matrix holds
    a column for every term and a row for every document. The values are yours to
    decide upon. In this chapter we’ll use binary: term is present? True or False.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的分类模型，我们将采用词袋方法。*词袋*是结构化文本数据最简单的方式：每个文档都被转换成一个词向量。如果一个特定的词出现在向量中，它被标记为“True”；其他的是“False”。[图8.7](#ch08fig07)展示了这种方法的简化示例，假设只有两个文档：一个是关于电视剧*权力的游戏*的，另一个是关于数据科学的。这两个词向量一起形成了*文档-词矩阵*。文档-词矩阵为每个术语都有一个列，为每个文档都有一个行。值由你决定。在本章中，我们将使用二进制：术语是否存在？是或否。
- en: Figure 8.7\. A text is transformed into a bag of words by labeling each word
    (term) with “True” if it is present in the document and “False” if not.
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.7. 文本通过标记每个词（术语）在文档中存在时为“True”，不存在时为“False”来转换为词袋。
- en: '![](Images/08fig07_alt.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig07_alt.jpg)'
- en: 'The example from [figure 8.7](#ch08fig07) does give you an idea of the structured
    data we’ll need to start text analysis, but it’s severely simplified: not a single
    word was filtered out and no stemming (we’ll go into this later) was applied.
    A big corpus can have thousands of unique words. If all have to be labeled like
    this without any filtering, it’s easy to see we might end up with a large volume
    of data. *Binary coded bag of words* as shown in [figure 8.7](#ch08fig07) is but
    one way to structure the data; other techniques exist.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.7](#ch08fig07)中的示例确实给你一个关于我们将需要开始文本分析的结构化数据的概念，但它严重简化了：没有过滤掉任何单词，也没有应用词干提取（我们稍后会讨论）。一个大型的语料库可以包含成千上万的独特单词。如果所有这些单词都必须这样标记而没有过滤，很容易看出我们可能会得到大量数据。[图8.7](#ch08fig07)中显示的*二进制编码词袋*只是结构化数据的一种方式；其他技术也存在。'
- en: '|  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Term Frequency—Inverse Document Frequency (TF-IDF)**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**词频-逆文档频率（TF-IDF**）'
- en: 'A well-known formula to fill up the document-term matrix is *TF-IDF* or Term
    Frequency multiplied by Inverse Document Frequency. *Binary bag of words* assigns
    True or False (term is there or not), while *simple frequencies* count the number
    of times the term occurred. TF-IDF is a bit more complicated and takes into account
    how many times a term occurred in the document (TF). TF can be a simple term count,
    a binary count (True or False), or a logarithmically scaled term count. It depends
    on what works best for you. In case TF is a term frequency, the formula of TF
    is the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 填充文档-词矩阵的一个著名公式是*TF-IDF*，即词频乘以逆文档频率。*二进制词袋*分配True或False（术语是否存在），而*简单频率*计算术语出现的次数。TF-IDF稍微复杂一些，考虑了术语在文档中出现的次数（TF）。TF可以是简单的术语计数，二进制计数（True或False），或对数缩放的术语计数。这取决于对你来说什么最有效。如果TF是词频，TF的公式如下：
- en: '*TF* = *f[t,d]*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*TF* = *f[t,d]*'
- en: TF is the frequency (f) of the term (t) in the document (d).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: TF是术语（t）在文档（d）中的频率（f）。
- en: 'But TF-IDF also takes into account all the other documents because of the Inverse
    Document Frequency. IDF gives an idea of how common the word is in the entire
    corpus: the higher the document frequency the more common, and more common words
    are less informative. For example the words “a” or “the” aren’t likely to provide
    specific information on a text. The formula of IDF with logarithmic scaling is
    the most commonly used form of IDF:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 但TF-IDF也考虑了所有其他文档，这是由于逆文档频率（Inverse Document Frequency）的影响。IDF给出了一个词在整个语料库中普遍程度的概念：文档频率越高，词越常见，而常见的词提供的信息越少。例如，“a”或“the”这样的词不太可能提供关于文本的具体信息。具有对数缩放的IDF公式是最常用的IDF形式：
- en: '*IDF* = log(*N*/|{*d* ε *D*:*t* ε *d*}|)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*IDF* = log(*N*/|{*d* ε *D*:*t* ε *d*}|)'
- en: with *N* being the total number of documents in the corpus, and the |{*d* ε
    *D*:*t* ε *d*}| being the number of documents (d) in which the term (t) appears.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*N*是语料库中文档的总数，而|{*d* ε *D*:*t* ε *d*}|是包含术语（t）的文档（d）的数量。
- en: 'The TF-IDF score says this about a term: how important is this word to distinguish
    this document from the others in the corpus? The formula of TF-IDF is thus'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF分数对某个词的描述是：这个词在区分语料库中的文档与其他文档时有多重要？因此，TF-IDF的公式是
- en: '![](Images/226equ01.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/226equ01.jpg)'
- en: We won’t use TF-IDF, but when setting your next steps in text mining, this should
    be one of the first things you’ll encounter. TF-IDF is also what was used by Elasticsearch
    behind the scenes in [chapter 6](kindle_split_014.xhtml#ch06). It’s a good way
    to go if you want to use TF-IDF for text analytics; leave the text mining to specialized
    software such as SOLR or Elasticsearch and take the document/term matrix for text
    analytics from there.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会使用TF-IDF，但在设置您在文本挖掘中的下一步时，这应该是您将遇到的第一件事之一。TF-IDF也是Elasticsearch在[第6章](kindle_split_014.xhtml#ch06)幕后所使用的。如果您想使用TF-IDF进行文本分析，这是一个好方法；将文本挖掘留给专门的软件，如SOLR或Elasticsearch，并从那里获取文本分析的文档/术语矩阵。
- en: '|  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Before getting to the actual bag of words, many other data manipulation steps
    take place:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在到达实际的词袋模型之前，许多其他数据操作步骤都会发生：
- en: '***Tokenization*** —The text is cut into pieces called “tokens” or “terms.”
    These tokens are the most basic unit of information you’ll use for your model.
    The terms are often words but this isn’t a necessity. Entire sentences can be
    used for analysis. We’ll use *unigrams*: terms consisting of one word. Often,
    however, it’s useful to include *bigrams* (two words per token) or *trigrams*
    (three words per token) to capture extra meaning and increase the performance
    of your models. This does come at a cost, though, because you’re building bigger
    term-vectors by including bigrams and/or trigrams in the equation.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词** —文本被切分成称为“标记”或“术语”的片段。这些标记是您在模型中将使用的最基本的信息单元。术语通常是单词，但这不是必需的。整个句子也可以用于分析。我们将使用**单词**：只包含一个单词的术语。然而，通常包括**二元组**（每个标记两个单词）或**三元组**（每个标记三个单词）以捕捉额外的意义并提高模型的表现力。但这也会带来成本，因为您通过在方程中包含二元组和/或三元组来构建更大的术语向量。'
- en: '***Stop word filtering*** —Every language comes with words that have little
    value in text analytics because they’re used so often. NLTK comes with a short
    list of English stop words we can filter. If the text is tokenized into words,
    it often makes sense to rid the word vector of these low-information stop words.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**停用词过滤** —每种语言都有一些词在文本分析中价值不大，因为它们被使用得非常频繁。NLTK附带了一个简短的英语停用词列表，我们可以过滤掉这些低信息量的停用词。如果文本被切分成单词，通常有道理从单词向量中去除这些低信息的停用词。'
- en: '***Lowercasing*** —Words with capital letters appear at the beginning of a
    sentence, others because they’re proper nouns or adjectives. We gain no added
    value making that distinction in our term matrix, so all terms will be set to
    lowercase.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小写化** —大写字母开头的单词出现在句子的开头，其他单词因为它们是专有名词或形容词。在我们的术语矩阵中，我们不会从这种区分中获得额外的价值，因此所有术语都将设置为小写。'
- en: Another data preparation technique is *stemming*. This one requires more elaboration.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种数据准备技术是**词干提取**。这一点需要更详细的解释。
- en: 8.2.2\. Stemming and lemmatization
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.2. 词干提取和词形还原
- en: '*Stemming* is the process of bringing words back to their root form; this way
    you end up with less variance in the data. This makes sense if words have similar
    meanings but are written differently because, for example, one is in its plural
    form. Stemming attempts to unify by cutting off parts of the word. For example
    “planes” and “plane” both become “plane.”'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*词干提取* 是将单词还原到其词根形式的过程；这样你最终得到的数据变异性更小。如果单词具有相似的含义但书写不同，例如一个单词是复数形式，这就有意义了。词干提取试图通过截断单词的一部分来实现统一。例如，“planes”和“plane”都变成了“plane”。'
- en: Another technique, called *lemmatization*, has this same goal but does so in
    a more grammatically sensitive way. For example, while both stemming and lemmatization
    would reduce “cars” to “car,” lemmatization can also bring back conjugated verbs
    to their unconjugated forms such as “are” to “be.” Which one you use depends on
    your case, and lemmatization profits heavily from POS Tagging (Part of Speech
    Tagging). *POS Tagging* is the process of attributing a grammatical label to every
    part of a sentence. You probably did this manually in school as a language exercise.
    Take the sentence “*Game of Thrones* is a television series.” If we apply POS
    Tagging on it we get
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种技术，称为 *词干提取*，具有相同的目标，但以更语法敏感的方式进行。例如，虽然词干提取和词干提取都会将“cars”还原为“car”，但词干提取还可以将动词的变形还原为非变形形式，例如将“are”还原为“be”。你使用哪一种取决于你的情况，词干提取在很大程度上受益于词性标注（词性标注）。*词性标注*
    是将语法标签分配给句子中每个部分的过程。你可能在学校作为语言练习手动执行过这个操作。以句子“*Game of Thrones* is a television
    series.”为例。如果我们对它进行词性标注，我们得到
- en: ({“game”:”NN”},{“of”:”IN},{“thrones”:”NNS},{“is”:”VBZ},{“a”:”DT},{“television”:”NN},{“series”:”NN})
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '| VBZ | 动词，第三人称单数现在时 | WDT | 疑问限定词 |'
- en: NN is a noun, IN is a preposition, NNS is a noun in its plural form, VBZ is
    a third-person singular verb, and DT is a determiner. [Table 8.1](#ch08table01)
    has the full list.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: NN 是名词，IN 是介词，NNS 是名词的复数形式，VBZ 是第三人称单数动词，而 DT 是限定词。[表8.1](#ch08table01) 包含了完整的列表。
- en: Table 8.1\. A list of all POS tags
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表8.1\. 所有词性标注的列表
- en: '| Tag | Meaning | Tag | Meaning |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Tag | 标签含义 | Tag | 标签含义 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| CC | Coordinating conjunction | CD | Cardinal number |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| CC | 并列连词 | CD | 基数词 |'
- en: '| DT | Determiner | EX | Existential |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| DT | 限定词 | EX | 存在 |'
- en: '| FW | Foreign word | IN | Preposition or subordinating conjunction |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: ({“game”:”NN”},{“of”:”IN”},{“thrones”:”NNS”},{“is”:”VBZ”},{“a”:”DT”},{“television”:”NN”},{“series”:”NN”})
- en: '| JJ | Adjective | JJR | Adjective, comparative |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| JJ | 形容词 | JJR | 形容词，比较级 |'
- en: '| JJS | Adjective, superlative | LS | List item marker |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| JJS | 形容词，最高级 | LS | 列表项标记 |'
- en: '| MD | Modal | NN | Noun, singular or mass |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| MD | 情态动词 | NN | 名词，单数或复数 |'
- en: '| NNS | Noun, plural | NNP | Proper noun, singular |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| NNS | 名词，复数 | NNP | 名词，单数 |'
- en: '| NNPS | Proper noun, plural | PDT | Predeterminer |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| NNPS | 名词，复数 | PDT | 预定冠词 |'
- en: '| POS | Possessive ending | PRP | Personal pronoun |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| POS | 物主后缀 | PRP | 人称代词 |'
- en: '| PRP$ | Possessive pronoun | RB | Adverb |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| PRP$ | 物主代词 | RB | 副词 |'
- en: '| RBR | Adverb, comparative | RBS | Adverb, superlative |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| RBR | 副词，比较级 | RBS | 副词，最高级 |'
- en: '| RP | Particle | SYM | Symbol |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| VBN | 动词，过去分词 | VBP | 动词，非第三人称单数现在时 |'
- en: '| UH | Interjection | VB | Verb, base form |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| UH | 呼语 | VB | 动词，基本形式 |'
- en: '| VBD | Verb, past tense | VBG | Verb, gerund or present participle |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| VBD | 动词，过去式 | VBG | 动词，动名词或现在分词 |'
- en: '| VBN | Verb, past participle | VBP | Verb, non-3rd person singular present
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| FW | 外来词 | IN | 介词或从属连词 |'
- en: '| VBZ | Verb, 3rd person singular present | WDT | Wh-determiner |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: 我们现在知道我们将使用来做数据清洗和操作（文本挖掘）的最重要的事情。对于我们的文本分析，让我们将决策树分类器添加到我们的工具箱中。
- en: '| WP | Wh-pronoun | WP$ | Possessive wh-pronoun |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| WP | 疑问代词 | WP$ | 物主疑问代词 |'
- en: '| WRB | Wh-adverb |   |   |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| RP | 动词小品词 | SYM | 符号 |'
- en: POS Tagging is a use case of sentence-tokenization rather than word-tokenization.
    After the POS Tagging is complete you can still proceed to word tokenization,
    but a POS Tagger requires whole sentences. Combining POS Tagging and lemmatization
    is likely to give cleaner data than using only a stemmer. For the sake of simplicity
    we’ll stick to stemming in the case study, but consider this an opportunity to
    elaborate on the exercise.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注是句子分词而不是单词分词的一个用例。在完成词性标注后，你仍然可以继续进行单词分词，但词性标注器需要整个句子。将词性标注和词干提取结合起来，可能比仅使用词干提取提供更干净的数据。为了简化，我们在案例研究中将坚持使用词干提取，但请将此视为详细阐述练习的机会。
- en: We now know the most important things we’ll use to do the data cleansing and
    manipulation (text mining). For our text analytics, let’s add the decision tree
    classifier to our repertoire.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '| WRB | 疑问副词 |   |   |'
- en: 8.2.3\. Decision tree classifier
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.3. 决策树分类器
- en: 'The data analysis part of our case study will be kept simple as well. We’ll
    test a Naïve Bayes classifier and a decision tree classifier. As seen in [chapter
    3](kindle_split_011.xhtml#ch03) the Naïve Bayes classifier is called that because
    it considers each input variable to be independent of all the others, which is
    naïve, especially in text mining. Take the simple examples of “data science,”
    “data analysis,” or “game of thrones.” If we cut our data in unigrams we get the
    following separate variables (if we ignore stemming and such): “data,” “science,”
    “analysis,” “game,” “of,” and “thrones.” Obviously links will be lost. This can,
    in turn, be overcome by creating bigrams (data science, data analysis) and trigrams
    (game of thrones).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们案例研究的数据分析部分也将保持简单。我们将测试一个朴素贝叶斯分类器和决策树分类器。如[第3章](kindle_split_011.xhtml#ch03)中所述，朴素贝叶斯分类器之所以被称为朴素贝叶斯，是因为它认为每个输入变量都是独立于所有其他变量的，这在文本挖掘中尤其天真。以“数据科学”、“数据分析”或“权力的游戏”的简单例子来说，如果我们对我们的数据使用单语元进行切割，我们会得到以下单独的变量（如果我们忽略词干提取等）：“数据”、“科学”、“分析”、“游戏”、“of”和“thrones”。显然，链接将会丢失。这可以通过创建双语元（数据科学、数据分析）和三元组（权力的游戏）来克服。
- en: The decision tree classifier, however, doesn’t consider the variables to be
    independent of one another and actively creates *interaction variables* and *buckets*.
    An *interaction variable* is a variable that combines other variables. For instance
    “data” and “science” might be good predictors in their own right but probably
    the two of them co-occurring in the same text might have its own value. A bucket
    is somewhat the opposite. Instead of combining two variables, a variable is split
    into multiple new ones. This makes sense for numerical variables. [Figure 8.8](#ch08fig08)
    shows what a decision tree might look like and where you can find interaction
    and bucketing.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，决策树分类器并不认为变量之间是相互独立的，并且会积极创建*交互变量*和*桶*。一个*交互变量*是结合其他变量的变量。例如，“数据”和“科学”本身可能是很好的预测因子，但这两者在同一文本中同时出现可能具有其自身的价值。桶则相反。不是将两个变量合并，而是将一个变量分割成多个新的变量。这对于数值变量来说是有意义的。[图8.8](#ch08fig08)展示了决策树可能的样子以及你可以找到交互和桶化的地方。
- en: Figure 8.8\. Fictitious decision tree model. A decision tree automatically creates
    buckets and supposes interactions between input variables.
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.8. 虚拟决策树模型。决策树自动创建桶并假设输入变量之间存在交互。
- en: '![](Images/08fig08.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图8.8](Images/08fig08.jpg)'
- en: Whereas Naïve Bayes supposes independence of all the input variables, a decision
    tree is built upon the assumption of interdependence. But how does it build this
    structure? A decision tree has a few possible criteria it can use to split into
    branches and decide which variables are more important (are closer to the root
    of the tree) than others. The one we’ll use in the NLTK decision tree classifier
    is “information gain.” To understand information gain, we first need to look at
    entropy. *Entropy* is a measure of unpredictability or chaos. A simple example
    would be the gender of a baby. When a woman is pregnant, the gender of the fetus
    can be male or female, but we don’t know which one it is. If you were to guess,
    you have a 50% chance to guess correctly (give or take, because gender distribution
    isn’t 100% uniform). However, during the pregnancy you have the opportunity to
    do an ultrasound to determine the gender of the fetus. An ultrasound is never
    100% conclusive, but the farther along in fetal development, the more accurate
    it becomes. This accuracy gain, or *information gain*, is there because uncertainty
    or entropy drops. Let’s say an ultrasound at 12 weeks pregnancy has a 90% accuracy
    in determining the gender of the baby. A 10% uncertainty still exists, but the
    ultrasound did reduce the uncertainty from 50% to 10%. That’s a pretty good discriminator.
    A decision tree follows this same principle, as shown in [figure 8.9](#ch08fig09).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 与朴素贝叶斯假设所有输入变量相互独立不同，决策树是基于相互依赖的假设构建的。但它是如何构建这个结构的呢？决策树有几个可能的准则可以用来分割成分支并决定哪些变量比其他变量更重要（更接近树的根部）。我们在NLTK决策树分类器中使用的准则是“信息增益”。为了理解信息增益，我们首先需要看看熵。*熵*是衡量不可预测性或混沌的度量。一个简单的例子就是婴儿的性别。当女性怀孕时，胎儿的性别可能是男性或女性，但我们不知道具体是哪一个。如果你要猜测，你有50%的机会猜对（考虑到性别分布并不完全均匀）。然而，在怀孕期间，你有机会进行超声波检查以确定胎儿的性别。超声波检查永远不会100%确定，但随着胎儿发育的深入，其准确性会越来越高。这种准确性的提升，或称*信息增益*，是因为不确定性或熵降低了。假设怀孕12周时进行超声波检查，在确定婴儿性别方面有90%的准确性。仍然存在10%的不确定性，但超声波确实将不确定性从50%降低到了10%。这是一个相当好的判别器。决策树遵循同样的原则，如[图8.9](#ch08fig09)所示。
- en: 'Figure 8.9\. Decision tree with one variable: the doctor’s conclusion from
    watching an ultrasound during a pregnancy. What is the probability of the fetus
    being female?'
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.9\. 具有一个变量的决策树：医生通过观察怀孕期间的超声波检查得出的结论。胎儿为女性的概率是多少？
- en: '![](Images/08fig09.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig09.jpg)'
- en: 'If another gender test has more predictive power, it could become the root
    of the tree with the ultrasound test being in the branches, and this can go on
    until we run out of variables or observations. We can run out of observations,
    because at every branch split we also split the input data. This is a big weakness
    of the decision tree, because at the leaf level of the tree robustness breaks
    down if too few observations are left; the decision trees starts to overfit the
    data. *Overfitting* allows the model to mistake randomness for real correlations.
    To counteract this, a decision tree is *pruned*: its meaningless branches are
    left out of the final model.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果另一个性别测试具有更强的预测能力，它可能成为树的根，而超声波测试则位于树枝上，这个过程可以一直持续到我们用尽变量或观察数据。我们可能会用尽观察数据，因为在每个分支分叉时，我们也会分割输入数据。这是决策树的一个大弱点，因为在树的叶级，如果剩余的观察数据太少，鲁棒性就会崩溃；决策树开始过度拟合数据。*过度拟合*使得模型会将随机性误认为是真实的关联。为了对抗这一点，决策树会被*剪枝*：其无意义的分支被排除在最终模型之外。
- en: Now that we’ve looked at the most important new techniques, let’s dive into
    the case study.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了最重要的新技巧，让我们深入到案例研究中。
- en: '8.3\. Case study: Classifying Reddit posts'
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3\. 案例研究：分类Reddit帖子
- en: 'While text mining has many applications, in this chapter’s case study we focus
    on *document classification*. As pointed out earlier in this chapter, this is
    exactly what Google does when it arranges your emails in categories or attempts
    to distinguish spam from regular emails. It’s also extensively used by contact
    centers that process incoming customer questions or complaints: written complaints
    first pass through a topic detection filter so they can be assigned to the correct
    people for handling. Document classification is also one of the mandatory features
    of social media monitoring systems. The monitored tweets, forum or Facebook posts,
    newspaper articles, and many other internet resources are assigned topic labels.
    This way they can be reused in reports. *Sentiment analysis* is a specific type
    of text classification: is the author of a post negative, positive, or neutral
    on something? That “something” can be recognized with entity recognition.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然文本挖掘有许多应用，但在本章的案例研究中，我们专注于**文档分类**。正如本章前面所指出的，这正是谷歌在将您的电子邮件分类或尝试区分垃圾邮件和常规邮件时所做的事情。它也被客户服务中心广泛使用，以处理客户提出的问题或投诉：书面投诉首先通过主题检测过滤器，以便可以分配给正确的人员处理。文档分类也是社交媒体监控系统的一项必备功能。被监控的推文、论坛或
    Facebook 帖子、报纸文章以及许多其他互联网资源都被分配了主题标签。这样，它们可以在报告中重复使用。**情感分析**是文本分类的一种特定类型：帖子的作者对某事是消极的、积极的还是中性的？这个“某事”可以通过实体识别来识别。
- en: In this case study we’ll draw on posts from Reddit, a website also known as
    the self-proclaimed “front page of the internet,” and attempt to train a model
    capable of distinguishing whether someone is talking about “data science” or “game
    of thrones.”
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们将利用 Reddit 的帖子，Reddit 也被称为自称为“互联网首页”的网站，并尝试训练一个能够区分某人是否在谈论“数据科学”或“权力的游戏”的模型。
- en: The end result can be a presentation of our model or a full-blown interactive
    application. In [chapter 9](kindle_split_017.xhtml#ch09) we’ll focus on application
    building for the end user, so for now we’ll stick to presenting our classification
    model.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果可以是我们的模型演示或一个完整的交互式应用程序。在第 9 章（[kindle_split_017.xhtml#ch09](https://kindle_split_017.xhtml#ch09)）中，我们将专注于为最终用户构建应用程序，所以现在我们将坚持展示我们的分类模型。
- en: To achieve our goal we’ll need all the help and tools we can get, and it happens
    Python is once again ready to provide them.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现我们的目标，我们需要尽可能多的帮助和工具，而 Python 一次又一次地准备好提供这些。
- en: 8.3.1\. Meet the Natural Language Toolkit
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.1. 认识自然语言工具包
- en: 'Python might not be the most execution efficient language on earth, but it
    has a mature package for text mining and language processing: the *Natural Language
    Toolkit **(NLTK)***. NLTK is a collection of algorithms, functions, and annotated
    works that will guide you in taking your first steps in text mining and natural
    language processing. NLTK is also excellently documented on nltk.org. NLTK is,
    however, not often used for production-grade work, like other libraries such as
    scikit-learn.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Python 可能不是地球上执行效率最高的语言，但它有一个成熟的文本挖掘和语言处理包：**自然语言工具包 (NLTK)**。NLTK 是一组算法、函数和标注作品，将引导您在文本挖掘和自然语言处理的第一步。NLTK
    在 nltk.org 上也有出色的文档。然而，NLTK 并不常用于生产级工作，如 scikit-learn 等其他库。
- en: '|  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Installing NLTK and its corpora**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装 NLTK 及其语料库**'
- en: 'Install NLTK with your favorite package installer. In case you’re using Anaconda,
    it comes installed with the default Anaconda setup. Otherwise you can go for “pip”
    or “easy_install”. When this is done you still need to install the models and
    corpora included to have it be fully functional. For this, run the following Python
    code:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用您喜欢的包安装程序安装 NLTK。如果您使用 Anaconda，它将默认与 Anaconda 设置一起安装。否则，您可以选择“pip”或“easy_install”。完成此操作后，您仍然需要安装包含的模型和语料库，以便使其完全功能。为此，请运行以下
    Python 代码：
- en: import nltk
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: import nltk
- en: nltk.download()
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: nltk.download()
- en: Depending on your installation this will give you a pop-up or more command-line
    options.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的安装，这将为您提供弹出窗口或更多命令行选项。
- en: '[Figure 8.10](#ch08fig10) shows the pop-up box you get when issuing the `nltk.download()`
    command.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.10](#ch08fig10) 显示了执行 `nltk.download()` 命令时出现的弹出窗口。'
- en: Figure 8.10\. Choose All Packages to fully complete the NLTK installation.
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.10. 选择所有包以完全完成 NLTK 的安装。
- en: '![](Images/08fig10_alt.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig10_alt.jpg)'
- en: You can download all the corpora if you like, but for this chapter we’ll only
    make use of “punkt” and “stopwords”. This download will be explicitly mentioned
    in the code that comes with this book.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您喜欢，可以下载所有语料库，但本章我们只会使用“punkt”和“stopwords”。此下载将在本书附带代码中明确提及。
- en: '|  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Two IPython notebook files are available for this chapter:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了两个IPython笔记本文件：
- en: '***Data collection*** —Will contain the data collection part of this chapter’s
    case study.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***数据收集*** —包含本章案例研究的数据收集部分。'
- en: '***Data preparation and analysis*** —The stored data is put through data preparation
    and then subjected to analysis.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***数据准备和分析*** —存储的数据经过数据准备，然后进行数据分析。'
- en: 'All code in the upcoming case study can be found in these two files in the
    same sequence and can also be run as such. In addition, two interactive graphs
    are available for download:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 所有即将到来的案例研究中的代码都可以在这两个文件中按相同顺序找到，也可以这样运行。此外，还提供了两个可下载的交互式图形：
- en: '***forceGraph.html*** —Represents the top 20 features of our Naïve Bayes model'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***forceGraph.html*** —表示我们的朴素贝叶斯模型的前20个特征'
- en: '***Sunburst.html*** —Represents the top four branches of our decision tree
    model'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Sunburst.html*** —表示我们的决策树模型的前四个分支'
- en: 'To open these two HTML pages, an HTTP server is necessary, which you can get
    using Python and a command window:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 要打开这两个HTML页面，需要一个HTTP服务器，您可以使用Python和命令窗口来获取：
- en: Open a command window (Linux, Windows, whatever you fancy).
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打开一个命令窗口（Linux、Windows，随便您喜欢）。
- en: 'Move to the folder containing the HTML files and their JSON data files: decisionTreeData.json
    for the sunburst diagram and NaiveBayesData.json for the force graph. It’s important
    the HTML files remain in the same location as their data files or you’ll have
    to change the JavaScript in the HTML file.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移动到包含HTML文件及其JSON数据文件的文件夹：decisionTreeData.json用于sunburst图和NaiveBayesData.json用于force图。重要的是HTML文件必须与它们的数据文件位于同一位置，否则您将不得不更改HTML文件中的JavaScript。
- en: 'Create a Python HTTP server with the following command: `python –m Simple-HTTPServer
    8000`'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下命令创建一个Python HTTP服务器：`python –m Simple-HTTPServer 8000`
- en: Open a browser and go to localhost:8000; here you can select the HTML files,
    as shown in [figure 8.11](#ch08fig11).
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打开浏览器并转到localhost:8000；在这里，您可以像[图8.11](#ch08fig11)所示选择HTML文件。
- en: Figure 8.11\. Python HTTP server serving this chapter’s output
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.11\. Python HTTP服务器提供本章的输出
- en: '![](Images/08fig11.jpg)'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](Images/08fig11.jpg)'
- en: 'The Python packages we’ll use in this chapter:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将使用的Python包：
- en: '***NLTK*** —For text mining'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***NLTK*** —用于文本挖掘'
- en: '***PRAW*** —Allows downloading posts from Reddit'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***PRAW*** —允许从Reddit下载帖子'
- en: '***SQLite3*** —Enables us to store data in the SQLite format'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***SQLite3*** —使我们能够以SQLite格式存储数据'
- en: '***Matplotlib*** —A plotting library for visualizing data'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Matplotlib*** —用于数据可视化的绘图库'
- en: Make sure to install all the necessary libraries and corpora before moving on.
    Before we dive into the action, however, let’s look at the steps we’ll take to
    get to our goal of creating a topic classification model.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请确保安装所有必要的库和语料库。然而，在我们深入行动之前，让我们看看我们将采取的步骤来实现我们的目标：创建一个主题分类模型。
- en: '8.3.2\. Data science process overview and step 1: The research goal'
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.2\. 数据科学流程概述和第1步：研究目标
- en: To solve this text mining exercise, we’ll once again make use of the data science
    process. [Figure 8.12](#ch08fig12) shows the data science process applied to our
    Reddit classification case.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个文本挖掘练习，我们再次利用数据科学流程。[图8.12](#ch08fig12)显示了将数据科学流程应用于我们的Reddit分类案例。
- en: Figure 8.12\. Data science process overview applied to Reddit topic classification
    case study
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.12\. 将数据科学流程概述应用于Reddit主题分类案例研究
- en: '![](Images/08fig12_alt.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig12_alt.jpg)'
- en: 'Not all the elements depicted in [figure 8.12](#ch08fig12) might make sense
    at this point, and the rest of the chapter is dedicated to working this out in
    practice as we work toward our research goal: creating a classification model
    capable of distinguishing posts about “data science” from posts about “Game of
    Thrones.” Without further ado, let’s go get our data.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，[图8.12](#ch08fig12)中描绘的所有元素可能并不都很有意义，本章的其余部分将致力于在实践中解决这个问题，我们朝着我们的研究目标前进：创建一个能够区分关于“数据科学”的帖子与关于“权力的游戏”的帖子的分类模型。无需多言，让我们去获取我们的数据。
- en: '8.3.3\. Step 2: Data retrieval'
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.3\. 第2步：数据检索
- en: We’ll use Reddit data for this case, and for those unfamiliar with Reddit, take
    the time to familiarize yourself with its concepts at [www.reddit.com](http://www.reddit.com).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Reddit数据来处理这个案例，对于那些不熟悉Reddit的人，请花时间熟悉其概念，请访问[www.reddit.com](http://www.reddit.com)。
- en: Reddit calls itself “the front page of the internet” because users can post
    things they find interesting and/or found somewhere on the internet, and only
    those things deemed interesting by many people are featured as “popular” on its
    homepage. You could say Reddit gives an overview of the trending things on the
    internet. Any user can post within a predefined category called a “subreddit.”
    When a post is made, other users get to comment on it and can up-vote it if they
    like the content or down-vote it if they dislike it. Because a post is always
    part of a subreddit, we have this metadata at our disposal when we hook up to
    the Reddit API to get our data. We’re effectively fetching labeled data because
    we’ll assume that a post in the subreddit “gameofthrones” has something to do
    with “gameofthrones.”
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Reddit 自称为“互联网首页”，因为用户可以发布他们觉得有趣或在网上找到的内容，只有那些被许多人认为有趣的内容才会被展示在其首页上作为“热门”。可以说
    Reddit 给出了互联网上趋势事物的概述。任何用户都可以在预定义的分类中发布，这个分类被称为“subreddit”。当一个帖子发布后，其他用户可以对其发表评论，如果他们喜欢内容，可以对其进行点赞；如果不喜欢，可以对其进行踩。因为帖子总是属于某个
    subreddit，所以当我们连接到 Reddit API 获取数据时，我们就有这样的元数据可以利用。我们实际上是在获取标记数据，因为我们假设 subreddit
    “gameofthrones” 中的帖子与“gameofthrones”有关。
- en: To get to our data we make use of the official Reddit Python API library called
    PRAW. Once we get the data we need, we’ll store it in a lightweight database-like
    file called SQLite. SQLite is ideal for storing small amounts of data because
    it doesn’t require any setup to use and will respond to SQL queries like any regular
    relational database does. Any other data storage medium will do; if you prefer
    Oracle or Postgres databases, Python has an excellent library to interact with
    these without the need to write SQL. SQLAlchemy will work for SQLite files as
    well. [Figure 8.13](#ch08fig13) shows the data retrieval step within the data
    science process.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取我们的数据，我们使用了官方的 Reddit Python API 库，称为 PRAW。一旦我们获取了所需的数据，我们就会将其存储在一个轻量级的类似数据库的文件中，称为
    SQLite。SQLite 对于存储少量数据非常理想，因为它不需要任何设置即可使用，并且可以像任何常规的关系数据库一样响应 SQL 查询。任何其他数据存储介质都可以；如果您更喜欢
    Oracle 或 Postgres 数据库，Python 有一个出色的库可以与之交互，而无需编写 SQL。SQLAlchemy 也可以用于 SQLite 文件。[图
    8.13](#ch08fig13) 显示了数据科学过程中的数据检索步骤。
- en: Figure 8.13\. The data science process data retrieval step for a Reddit topic
    classification case
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.13\. Reddit 主题分类案例的数据科学过程数据检索步骤
- en: '![](Images/08fig13_alt.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig13_alt.jpg)'
- en: Open your favorite Python interpreter; it’s time for action, as shown in [listing
    8.1](#ch08ex01). First we need to collect our data from the Reddit website. If
    you haven’t already, use `pip install praw` or `conda install praw` (Anaconda)
    before running the following script.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 打开您喜欢的 Python 解释器；现在是行动的时候了，如[列表 8.1](#ch08ex01)所示。首先我们需要从 Reddit 网站收集我们的数据。如果您还没有安装，请在运行以下脚本之前使用
    `pip install praw` 或 `conda install praw`（Anaconda）。
- en: '|  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The code for step 2 can also be found in the IPython file “[Chapter 8](#ch08)
    data collection.” It’s available in this book’s download section.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 2 的代码也可以在 IPython 文件中找到，位于“[第 8 章](#ch08) 数据收集”部分。它可在本书的下载区获取。
- en: '|  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Listing 8.1\. Setting up SQLLite database and Reddit API client
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.1\. 设置 SQLite 数据库和 Reddit API 客户端
- en: '![](Images/235fig01_alt.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/235fig01_alt.jpg)'
- en: Let’s first import the necessary libraries.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先导入必要的库。
- en: 'Now that we have access to the SQLite3 and PRAW capabilities, we need to prepare
    our little local database for the data it’s about to receive. By defining a connection
    to a SQLite file we automatically create it if it doesn’t already exist. We then
    define a data cursor that’s capable of executing any SQL statement, so we use
    it to predefine the structure of our database. The database will contain two tables:
    the topics table contains Reddit topics, which is similar to someone starting
    a new post on a forum, and the second table contains the comments and is linked
    to the topic table via the “topicID” column. The two tables have a one (topic
    table) to many (comment table) relationship. For the case study, we’ll limit ourselves
    to using the topics table, but the data collection will incorporate both because
    this allows you to experiment with this extra data if you feel like it. To hone
    your text-mining skills you could perform sentiment analysis on the topic comments
    and find out what topics receive negative or positive comments. You could then
    correlate this to the model features we’ll produce by the end of this chapter.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经可以访问SQLite3和PRAW功能，我们需要为即将接收的数据准备我们的本地数据库。通过定义一个到SQLite文件的连接，如果它还不存在，我们会自动创建它。然后我们定义一个数据游标，它可以执行任何SQL语句，因此我们使用它来预定义数据库的结构。数据库将包含两个表：主题表包含Reddit主题，类似于某人在论坛上发起一个新的帖子，第二个表包含评论，并通过“topicID”列与主题表相关联。这两个表之间存在一对一（主题表）到多对一（评论表）的关系。对于案例研究，我们将限制自己只使用主题表，但数据收集将包括两者，因为这允许你在愿意的情况下实验这些额外数据。为了磨练你的文本挖掘技能，你可以对主题评论进行情感分析，找出哪些主题收到了负面或正面的评论。然后你可以将此与本章末尾我们将产生的模型特征相关联。
- en: We need to create a PRAW client to get access to the data. Every subreddit can
    be identified by its name, and we’re interested in “datascience” and “gameofthrones.”
    The limit represents the maximum number of topics (posts, not comments) we’ll
    draw in from Reddit. A thousand is also the maximum number the API allows us to
    fetch at any given request, though we could request more later on when people
    have posted new things. In fact we can run the API request periodically and gather
    data over time. While at any given time you’re limited to a thousand posts, nothing
    stops you from growing your own database over the course of months. It’s worth
    noting the following script might take about an hour to complete. If you don’t
    feel like waiting, feel free to proceed and use the downloadable SQLite file.
    Also, if you run it now you are not likely to get the exact same output as when
    it was first run to create the output shown in this chapter.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建一个PRAW客户端来获取数据。每个subreddit都可以通过其名称来识别，我们感兴趣的是“datascience”和“gameofthrones”。限制表示我们从Reddit中抽取的最大主题数量（帖子，不是评论）。一千也是API在任何给定请求中允许我们获取的最大数量，尽管我们可以在人们发布新内容后稍后请求更多。实际上，我们可以定期运行API请求并随着时间的推移收集数据。虽然在任何给定时间你只能限制为一千篇帖子，但没有任何东西阻止你在几个月的时间里扩大你自己的数据库。值得注意的是，以下脚本可能需要大约一个小时才能完成。如果你不想等待，请随意继续并使用可下载的SQLite文件。此外，如果你现在运行它，你不太可能得到与最初运行以创建本章中显示的输出完全相同的输出。
- en: Let’s look at our data retrieval function, as shown in the following listing.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的数据检索函数，如下所示。
- en: Listing 8.2\. Reddit data retrieval and storage in SQLite
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.2\. Reddit数据在SQLite中的检索和存储
- en: '![](Images/ch08ex02-0.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/ch08ex02-0.jpg)'
- en: '![](Images/ch08ex02-1.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/ch08ex02-1.jpg)'
- en: The `prawGetData()` function retrieves the “hottest” topics in its subreddit,
    appends this to an array, and then gets all its related comments. This goes on
    until a thousand topics are reached or no more topics exist to fetch and everything
    is stored in the SQLite database. The print statements are there to inform you
    on its progress toward gathering a thousand topics. All that’s left for us to
    do is execute the function for each subreddit.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`prawGetData()`函数检索其subreddit中的“热门”主题，将其追加到数组中，然后获取所有相关评论。这个过程会一直进行，直到达到一千个主题或没有更多主题可以检索，并且所有内容都存储在SQLite数据库中。打印语句用于告知你收集一千个主题的进度。我们剩下的工作就是为每个subreddit执行该函数。'
- en: If you’d like this analysis to incorporate more than two subreddits, this is
    a matter of adding an extra category to the subreddits array.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望这个分析包含超过两个subreddits，这只是一个在subreddits数组中添加额外类别的问题。
- en: With the data collected, we’re ready to move on to data preparation.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 收集数据后，我们就可以继续进行数据准备了。
- en: '8.3.4\. Step 3: Data preparation'
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.4\. 第3步：数据准备
- en: As always, data preparation is the most crucial step to get correct results.
    For text mining this is even truer since we don’t even start off with structured
    data.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 总是来说，数据准备是获取正确结果的最关键步骤。对于文本挖掘来说，这一点尤为重要，因为我们甚至没有从结构化数据开始。
- en: The upcoming code is available online as IPython file “[Chapter 8](#ch08) data
    preparation and analysis.” Let’s start by importing the required libraries and
    preparing the SQLite database, as shown in the following listing.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的代码作为 IPython 文件在线提供，文件名为“[第 8 章](#ch08) 数据准备和分析。”让我们首先导入所需的库并准备 SQLite 数据库，如下面的列表所示。
- en: Listing 8.3\. Text mining, libraries, corpora dependencies, and SQLite database
    connection
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.3\. 文本挖掘、库、语料库依赖项和 SQLite 数据库连接
- en: '![](Images/237fig01_alt.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/237fig01_alt.jpg)'
- en: In case you haven’t already downloaded the full NLTK corpus, we’ll now download
    the part of it we’ll use. Don’t worry if you already downloaded it, the script
    will detect if your corpora is up to date.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有下载完整的 NLTK 语料库，我们现在将下载我们将要使用的那部分。如果你已经下载了它，脚本将检测你的语料库是否是最新的。
- en: Our data is still stored in the Reddit SQLite file so let’s create a connection
    to it.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据仍然存储在 Reddit SQLite 文件中，所以让我们创建一个连接到它的连接。
- en: 'Even before exploring our data we know of at least two things we have to do
    to clean the data: stop word filtering and lowercasing.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索我们的数据之前，我们就知道至少有两件事要做来清理数据：停用词过滤和转换为小写。
- en: A general word filter function will help us filter out the unclean parts. Let’s
    create one in the following listing.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通用的词过滤函数将帮助我们过滤掉不干净的部分。让我们在下面的列表中创建一个。
- en: Listing 8.4\. Word filtering and lowercasing functions
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.4\. 单词过滤和转换为小写的函数
- en: '![](Images/238fig01_alt.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/238fig01_alt.jpg)'
- en: 'The English stop words will be the first to leave our data. The following code
    will provide us these stop words:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 英语停用词将首先离开我们的数据。下面的代码将提供这些停用词：
- en: '[PRE0]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Figure 8.14](#ch08fig14) shows the list of English stop words in NLTK.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.14](#ch08fig14) 显示了 NLTK 中的英语停用词列表。'
- en: Figure 8.14\. English stop words list in NLTK
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.14\. NLTK 中的英语停用词列表
- en: '![](Images/08fig14_alt.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig14_alt.jpg)'
- en: With all the necessary components in place, let’s have a look at our first data
    processing function in the following listing.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有必要的组件都到位之后，让我们看看下面的列表中我们的第一个数据处理函数。
- en: Listing 8.5\. First data preparation function and execution
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.5\. 第一个数据准备函数和执行
- en: '![](Images/ch08ex05-0.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/ch08ex05-0.jpg)'
- en: '![](Images/ch08ex05-1.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/ch08ex05-1.jpg)'
- en: 'Our `data_processing()` function takes in a SQL statement and returns the document-term
    matrix. It does this by looping through the data one entry (Reddit topic) at a
    time and combines the topic title and topic body text into a single word vector
    with the use of word tokenization. A *tokenizer* is a text handling script that
    cuts the text into pieces. You have many different ways to tokenize a text: you
    can divide it into sentences or words, you can split by space and punctuations,
    or you can take other characters into account, and so on. Here we opted for the
    standard NLTK word tokenizer. This word tokenizer is simple; all it does is split
    the text into terms if there’s a space between the words. We then lowercase the
    vector and filter out the stop words. Note how the order is important here; a
    stop word in the beginning of a sentence wouldn’t be filtered if we first filter
    the stop words before lowercasing. For instance in “I like Game of Thrones,” the
    “I” would not be lowercased and thus would not be filtered out. We then create
    a word matrix (term-document matrix) and a list containing all the words. Notice
    how we extend the list without filtering for doubles; this way we can create a
    histogram on word occurrences during data exploration. Let’s execute the function
    for our two topic categories.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `data_processing()` 函数接收一个 SQL 语句并返回文档-词矩阵。它是通过逐个遍历数据（Reddit 主题）并使用词标记化将主题标题和主题正文文本合并成一个单词向量来做到这一点的。一个
    *标记化器* 是一个文本处理脚本，它将文本分割成片段。你有多种不同的方式来标记化文本：你可以将其分割成句子或单词，可以按空格和标点符号分割，或者可以考虑到其他字符，等等。在这里，我们选择了标准的
    NLTK 单词标记化器。这个单词标记化器很简单；它所做的只是如果单词之间有空格，就将文本分割成术语。然后我们将向量转换为小写并过滤掉停用词。注意这里的顺序很重要；如果我们在转换为小写之前先过滤停用词，那么句子开头的停用词就不会被过滤掉。例如，在“我喜欢权力的游戏”中，“我”就不会被转换为小写，因此不会被过滤掉。然后我们创建一个单词矩阵（术语-文档矩阵）和一个包含所有单词的列表。注意我们在这里没有过滤重复项；这样我们就可以在数据探索期间创建单词出现频率的直方图。让我们为我们的两个主题类别执行这个函数。
- en: '[Figure 8.15](#ch08fig15) shows the first word vector of the “datascience”
    category.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.15](#ch08fig15)显示了“datascience”类别的第一个词向量。'
- en: Figure 8.15\. The first word vector of the “datascience” category after first
    data processing attempt
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.15\. 首次数据处理尝试后“datascience”类别的第一个词向量
- en: '![](Images/08fig15_alt.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig15_alt.jpg)'
- en: '[PRE1]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This sure looks polluted: punctuations are kept as separate terms and several
    words haven’t even been split. Further data exploration should clarify a few things
    for us.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实看起来很杂乱：标点符号被当作单独的术语，而且一些单词甚至没有被分割。进一步的数据探索应该会为我们澄清一些事情。
- en: '8.3.5\. Step 4: Data exploration'
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.5\. 步骤4：数据探索
- en: 'We now have all our terms separated, but the sheer size of the data hinders
    us from getting a good grip on whether it’s clean enough for actual use. By looking
    at a single vector, we already spot a few problems though: several words haven’t
    been split correctly and the vector contains many single-character terms. Single
    character terms might be good topic differentiators in certain cases. For example,
    an economic text will contain more $, £, and € signs than a medical text. But
    in most cases these one-character terms are useless. First, let’s have a look
    at the frequency distribution of our terms.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将所有术语分开，但数据的巨大规模阻碍了我们判断它是否足够干净以供实际使用。通过查看单个向量，我们已经发现了几个问题：一些单词没有被正确分割，向量中包含许多单字符术语。在某些情况下，单字符术语可能是好的主题区分器。例如，经济文本将包含比医学文本更多的$、£和€符号。但在大多数情况下，这些单字符术语是无用的。首先，让我们看看我们术语的频率分布。
- en: '[PRE2]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: By drawing a histogram of the frequency distribution ([figure 8.16](#ch08fig16))
    we quickly notice that the bulk of our terms only occur in a single document.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 通过绘制频率分布直方图（[图8.16](#ch08fig16)），我们很快注意到我们的大部分术语只出现在单个文档中。
- en: Figure 8.16\. This histogram of term frequencies shows both the “data science”
    and “game of thrones” term matrices have more than 3,000 terms that occur once.
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.16\. 这个词频直方图显示了“数据科学”和“权力的游戏”术语矩阵中都有超过3,000个只出现一次的术语。
- en: '![](Images/08fig16.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig16.jpg)'
- en: Single-occurrence terms such as these are called *hapaxes*, and model-wise they’re
    useless because a single occurrence of a feature is never enough to build a reliable
    model. This is good news for us; cutting these hapaxes out will significantly
    shrink our data without harming our eventual model. Let’s look at a few of these
    single-occurrence terms.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 像这样的单次出现术语被称为*单现词*，从模型的角度来看它们是无用的，因为一个特征的单一出现永远不足以构建一个可靠的模型。这对我们来说是个好消息；去掉这些单现词将显著减少我们的数据量，而不会损害我们最终构建的模型。让我们看看这些单次出现术语中的几个。
- en: '[PRE3]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Terms we see in [figure 8.17](#ch08fig17) make sense, and if we had more data
    they’d likely occur more often.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.17](#ch08fig17)中看到的术语是有意义的，如果我们有更多的数据，它们可能会更频繁地出现。'
- en: Figure 8.17\. “Data science” and “game of thrones” single occurrence terms (hapaxes)
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.17\. “数据科学”和“权力的游戏”单次出现术语（单现词）
- en: '![](Images/08fig17_alt.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig17_alt.jpg)'
- en: '[PRE4]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Many of these terms are incorrect spellings of otherwise useful ones, such
    as: Jaimie is Jaime (Lannister), Milisandre would be Melisandre, and so on. A
    decent *Game of Thrones*-specific thesaurus could help us find and replace these
    misspellings with a fuzzy search algorithm. This proves data cleaning in text
    mining can go on indefinitely if you so desire; keeping effort and payoff in balance
    is crucial here.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 许多这些术语是其他有用的单词的错误拼写，例如：Jaimie是Jaime（兰尼斯特家族），Milisandre应该是Melisandre，等等。一个不错的*《权力的游戏》*专用同义词典可以帮助我们使用模糊搜索算法找到并替换这些错误拼写。这证明，如果你愿意，文本挖掘中的数据清洗可以无限期地进行；保持努力和回报的平衡在这里至关重要。
- en: Let’s now have a look at the most frequent words.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看最常见的单词。
- en: '[PRE5]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[Figure 8.18](#ch08fig18) shows the output of asking for the top 20 most common
    words for each category.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.18](#ch08fig18)显示了请求每个类别中最常见的20个单词的输出。'
- en: Figure 8.18\. Top 20 most frequent words for the “data science” and “game of
    thrones” posts
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.18\. “数据科学”和“权力的游戏”帖子中最常见的20个单词
- en: '![](Images/08fig18_alt.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig18_alt.jpg)'
- en: 'Now this looks encouraging: several common words do seem specific to their
    topics. Words such as “data,” “science,” and “season” are likely to become good
    differentiators. Another important thing to notice is the abundance of the single
    character terms such as “.” and “,”; we’ll get rid of these.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看起来很有希望：一些常见的单词似乎与它们的主题相关。像“数据”、“科学”和“季节”这样的单词很可能是好的区分器。另一个需要注意的重要事情是单字符术语（如“。”和“，”）的丰富性；我们将去掉这些。
- en: With this extra knowledge, let’s revise our data preparation script.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 带着这些额外的知识，让我们修改我们的数据准备脚本。
- en: '8.3.6\. Step 3 revisited: Data preparation adapted'
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.6。步骤3回顾：数据准备调整
- en: This short data exploration has already drawn our attention to a few obvious
    tweaks we can make to improve our text. Another important one is stemming the
    terms.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这短暂的探索已经引起了我们对几个可以改进文本的明显调整的注意。另一个重要的一点是对术语进行词干提取。
- en: The following listing shows a simple stemming algorithm called “snowball stemming.”
    These snowball stemmers can be language-specific, so we’ll use the English one;
    however, it does support many languages.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表显示了一个简单的称为“snowball词干提取”的词干提取算法。这些snowball词干提取器可以是语言特定的，所以我们将使用英语版本；然而，它也支持许多语言。
- en: Listing 8.6\. The Reddit data processing revised after data exploration
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.6。数据探索后修订的Reddit数据处理
- en: '![](Images/ch08ex06-0.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/ch08ex06-0.jpg)'
- en: '![](Images/ch08ex06-1.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/ch08ex06-1.jpg)'
- en: Notice the changes since the last `data_processing()` function. Our tokenizer
    is now a regular expression tokenizer. Regular expressions are not part of this
    book and are often considered challenging to master, but all this simple one does
    is cut the text into words. For words, any alphanumeric combination is allowed
    (\w), so there are no more special characters or punctuations. We also applied
    the word stemmer and removed a list of extra stop words. And, all the hapaxes
    are removed at the end because everything needs to be stemmed first. Let’s run
    our data preparation again.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 注意自上次`data_processing()`函数以来的变化。我们的分词器现在是一个正则表达式分词器。正则表达式不是本书的内容，通常被认为很难掌握，但这个简单的正则表达式所做的只是将文本切分成单词。对于单词，任何字母数字组合都是允许的（\w），因此不再有特殊字符或标点符号。我们还应用了词干提取器并移除了一个额外的停用词列表。最后，所有单次出现的词都被移除了，因为一切都需要先进行词干提取。让我们再次运行我们的数据准备。
- en: If we did the same exploratory analysis as before, we’d see it makes more sense,
    and we have no more hapaxes.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们像以前一样进行相同的探索性分析，我们会发现它更有意义，而且我们没有更多的单次出现词。
- en: '[PRE6]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s take the top 20 words of each category again (see [figure 8.19](#ch08fig19)).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次取每个类别的前20个单词（参见[图8.19](#ch08fig19)）。
- en: Figure 8.19\. Top 20 most frequent words in “data science” and “game of thrones”
    Reddit posts after data preparation
  id: totrans-233
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.19。数据准备后的“数据科学”和“权力的游戏”Reddit帖子中最常见的20个单词
- en: '![](Images/08fig19_alt.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig19_alt.jpg)'
- en: We can see in [figure 8.19](#ch08fig19) how the data quality has improved remarkably.
    Also, notice how certain words are shortened because of the stemming we applied.
    For instance, “science” and “sciences” have become “scienc;” “courses” and “course”
    have become “cours,” and so on. The resulting terms are not actual words but still
    interpretable. If you insist on your terms remaining actual words, lemmatization
    would be the way to go.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在[图8.19](#ch08fig19)中看到数据质量已经显著提高。同时，请注意由于我们应用的词干提取，某些单词被缩短了。例如，“science”和“sciences”变成了“scienc;”，“courses”和“course”变成了“cours,”等等。结果术语不是实际单词，但仍然可以解释。如果您坚持让术语保持为实际单词，那么词形还原可能是正确的选择。
- en: 'With the data cleaning process “completed” (remark: a text mining cleansing
    exercise can almost never be fully completed), all that remains is a few data
    transformations to get the data in the bag of words format.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据清理过程“完成”（备注：文本挖掘清理练习几乎永远无法完全完成）后，剩下的只是进行一些数据转换，以将数据转换为词袋格式。
- en: First, let’s label all our data and also create a holdout sample of 100 observations
    per category, as shown in the following listing.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们给所有数据贴上标签，并为每个类别创建一个包含100个观察值的保留样本，如下所示。
- en: Listing 8.7\. Final data transformation and data splitting before modeling
  id: totrans-238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.7。建模前的最终数据转换和数据拆分
- en: '![](Images/ch08ex07-0.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/ch08ex07-0.jpg)'
- en: '![](Images/ch08ex07-1.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/ch08ex07-1.jpg)'
- en: The holdout sample will be used for our final test of the model and the creation
    of a confusion matrix. A *confusion matrix* is a way of checking how well a model
    did on previously unseen data. The matrix shows how many observations were correctly
    and incorrectly classified.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 保留样本将被用于我们模型的最终测试和混淆矩阵的创建。*混淆矩阵*是一种检查模型在之前未见过的数据上表现如何的方法。矩阵显示了有多少观察值被正确和错误地分类。
- en: 'Before creating or training and testing data we need to take one last step:
    pouring the data into a bag of words format where every term is given either a
    “True” or “False” label depending on its presence in that particular post. We
    also need to do this for the unlabeled holdout sample.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建或训练和测试数据之前，我们需要采取最后一步：将数据倒入一个词袋格式，其中每个术语根据其在特定帖子中的存在与否被赋予“True”或“False”标签。我们还需要对未标记的保留样本做同样的事情。
- en: Our prepared data now contains every term for each vector, as shown in [figure
    8.20](#ch08fig20).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备的数据现在包含了每个向量的每个术语，如图8.20所示。
- en: Figure 8.20\. A binary bag of words ready for modeling is very sparse data.
  id: totrans-244
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.20。一个准备建模的二进制词袋模型是非常稀疏的数据。
- en: '![](Images/08fig20_alt.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig20_alt.jpg)'
- en: '[PRE7]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We created a big but sparse matrix, allowing us to apply techniques from [chapter
    5](kindle_split_013.xhtml#ch05) if it was too big to handle on our machine. With
    such a small table, however, there’s no need for that now and we can proceed to
    shuffle and split the data into a training and test set.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个大但稀疏的矩阵，允许我们在机器上处理不过大时应用第5章中的技术。然而，对于如此小的表格，现在没有必要这样做，我们可以继续将数据随机打乱并分割成训练集和测试集。
- en: 'While the biggest part of your data should always go to the model training,
    an optimal split ratio exists. Here we opted for a 3-1 split, but feel free to
    play with this. The more observations you have, the more freedom you have here.
    If you have few observations you’ll need to allocate relatively more to training
    the model. We’re now ready to move on to the most rewarding part: data analysis.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你的数据的大部分应该总是用于模型训练，但存在一个最佳分割比例。在这里，我们选择了3-1的分割，但你可以自由地尝试不同的比例。你拥有的观察值越多，你在这里的自由度就越大。如果你观察值很少，你需要相对更多地分配给模型训练。我们现在可以继续到最有回报的部分：数据分析。
- en: '8.3.7\. Step 5: Data analysis'
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.7. 步骤5：数据分析
- en: 'For our analysis we’ll fit two classification algorithms to our data: Naïve
    Bayes and decision trees. Naïve Bayes was explained in [chapter 3](kindle_split_011.xhtml#ch03)
    and decision tree earlier in this chapter.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的分析中，我们将拟合两个分类算法到我们的数据：朴素贝叶斯和决策树。朴素贝叶斯在第3章中解释过，决策树在本章的早期也提到过。
- en: Let’s first test the performance of our Naïve Bayes classifier. NLTK comes with
    a classifier, but feel free to use algorithms from other packages such as SciPy.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先测试我们的朴素贝叶斯分类器的性能。NLTK自带了一个分类器，但你可以自由使用来自其他包（如SciPy）的算法。
- en: '[PRE8]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: With the classifier trained we can use the test data to get a measure on overall
    accuracy.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器训练完成后，我们可以使用测试数据来衡量整体准确率。
- en: '[PRE9]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The accuracy on the test data is estimated to be greater than 90%, as seen in
    [figure 8.21](#ch08fig21). *Classification accuracy* is the number of correctly
    classified observations as a percentage of the total number of observations. Be
    advised, though, that this can be different in your case if you used different
    data.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据上的准确率估计超过90%，如图8.21所示。[分类准确率]是指正确分类的观察值占总观察值的百分比。不过，请注意，如果你使用了不同的数据，这个结果可能会有所不同。
- en: Figure 8.21\. Classification accuracy is a measure representing what percentage
    of observations was correctly classified on the test data.
  id: totrans-256
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.21。分类准确率是表示在测试数据上正确分类的观察值所占百分比的度量。
- en: '![](Images/08fig21.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig21.jpg)'
- en: '[PRE10]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: That’s a good number. We can now lean back and relax, right? No, not really.
    Let’s test it again on the 200 observations holdout sample and this time create
    a confusion matrix.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的数字。现在我们可以放松一下，对吧？不，实际上并不是。让我们再次在200个观察值的保留样本上测试它，这次创建一个混淆矩阵。
- en: '[PRE11]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The confusion matrix in [figure 8.22](#ch08fig22) shows us the 97% is probably
    over the top because we have 28 (23 + 5) misclassified cases. Again, this can
    be different with your data if you filled the SQLite file yourself.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.22中的混淆矩阵显示，97%可能过高，因为我们有28个（23 + 5）个错误分类的案例。再次提醒，如果你自己填充了SQLite文件，这个结果可能会有所不同。
- en: Figure 8.22\. Naïve Bayes model confusion matrix shows 28 (23 + 5) observations
    out of 200 were misclassified
  id: totrans-262
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.22。朴素贝叶斯模型混淆矩阵显示，在200个观察值中有28个（23 + 5）个被错误分类。
- en: '![](Images/08fig22.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig22.jpg)'
- en: Twenty-eight misclassifications means we have an 86% accuracy on the holdout
    sample. This needs to be compared to randomly assigning a new post to either the
    “datascience” or “gameofthrones” group. If we’d randomly assigned them, we could
    expect an accuracy of 50%, and our model seems to perform better than that. Let’s
    look at what it uses to determine the categories by digging into the most informative
    model features.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 二十八次错误分类意味着我们在保留样本上的准确率为86%。这需要与随机分配一个新帖子到“datascience”或“gameofthrones”组进行比较。如果我们随机分配，我们可能会期望准确率为50%，而我们的模型似乎表现得更好。让我们深入挖掘最有信息量的模型特征，看看它如何确定类别。
- en: '[PRE12]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[Figure 8.23](#ch08fig23) shows the top 20 terms capable of distinguishing
    between the two categories.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.23](#ch08fig23)显示了能够区分两个类别的顶级20个术语。'
- en: Figure 8.23\. The most important terms in the Naïve Bayes classification model
  id: totrans-267
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.23\. 朴素贝叶斯分类模型中最重要的术语
- en: '![](Images/08fig23_alt.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig23_alt.jpg)'
- en: The term “data” is given heavy weight and seems to be the most important indicator
    of whether a topic belongs in the data science category. Terms such as “scene,”
    “season,” “king,” “tv,” and “kill” are good indications the topic is *Game of
    Thrones* rather than data science. All these things make perfect sense, so the
    model passed both the accuracy and the sanity check.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 术语“数据”被赋予了很高的权重，似乎是最重要的指标，用以判断一个主题是否属于数据科学类别。诸如“场景”、“季节”、“国王”、“电视”和“杀戮”等术语是判断主题是*权力的游戏*而不是数据科学的良好指标。所有这些事情都完全合理，因此模型通过了准确性和合理性检查。
- en: The Naïve Bayes does well, so let’s have a look at the decision tree in the
    following listing.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯表现良好，因此让我们看看以下列表中的决策树。
- en: Listing 8.8\. Decision tree model training and evaluation
  id: totrans-271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.8\. 决策树模型训练和评估
- en: '![](Images/248fig01_alt.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/248fig01_alt.jpg)'
- en: As shown in [figure 8.24](#ch08fig24), the promised accuracy is 93%.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图8.24](#ch08fig24)所示，承诺的准确率是93%。
- en: Figure 8.24\. Decision tree model accuracy
  id: totrans-274
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.24\. 决策树模型准确率
- en: '![](Images/08fig24.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig24.jpg)'
- en: We now know better than to rely solely on this single test, so once again we
    turn to a confusion matrix on a second set of data, as shown in [figure 8.25](#ch08fig25).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道不能仅仅依赖这个单一的测试，因此我们再次转向第二组数据的混淆矩阵，如图[图8.25](#ch08fig25)所示。
- en: Figure 8.25\. Confusion matrix on decision tree model
  id: totrans-277
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.25\. 决策树模型的混淆矩阵
- en: '![](Images/08fig25.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig25.jpg)'
- en: '[Figure 8.25](#ch08fig25) shows a different story. On these 200 observations
    of the holdout sample the decision tree model tends to classify well when the
    post is about *Game of Thrones* but fails miserably when confronted with the data
    science posts. It seems the model has a preference for *Game of Thrones*, and
    can you blame it? Let’s have a look at the actual model, even though in this case
    we’ll use the Naïve Bayes as our final model.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.25](#ch08fig25)展示了不同的故事。在这些200个保留样本的观察中，当帖子是关于*权力的游戏*时，决策树模型倾向于很好地分类，但当面对数据科学帖子时却表现糟糕。似乎模型更喜欢*权力的游戏*，你能责怪它吗？让我们看看实际的模型，尽管在这种情况下我们将使用朴素贝叶斯作为我们的最终模型。'
- en: '[PRE13]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The decision tree has, as the name suggests, a tree-like model, as shown in
    [figure 8.26](#ch08fig26).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树，正如其名所示，具有树状模型，如图[图8.26](#ch08fig26)所示。
- en: Figure 8.26\. Decision tree model tree structure representation
  id: totrans-282
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.26\. 决策树模型树结构表示
- en: '![](Images/08fig26.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig26.jpg)'
- en: The Naïve Bayes considers all the terms and has weights attributed, but the
    decision tree model goes through them sequentially, following the path from the
    root to the outer branches and leaves. [Figure 8.26](#ch08fig26) only shows the
    top four layers, starting with the term “data.” If “data” is present in the post,
    it’s always data science. If “data” can’t be found, it checks for the term “learn,”
    and so it continues. A possible reason why this decision tree isn’t performing
    well is the lack of pruning. When a decision tree is built it has many leaves,
    often too many. A tree is then pruned to a certain level to minimize overfitting.
    A big advantage of decision trees is the implicit interaction effects between
    words it takes into account when constructing the branches. When multiple terms
    together create a stronger classification than single terms, the decision tree
    will actually outperform the Naïve Bayes. We won’t go into the details of that
    here, but consider this one of the next steps you could take to improve the model.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯考虑了所有术语并赋予了权重，但决策树模型按顺序通过它们，从根到外部的分支和叶子。 [图8.26](#ch08fig26)仅显示了前四层，从“数据”这个术语开始。如果帖子中存在“数据”，它总是指数据科学。如果找不到“数据”，它会检查“学习”这个术语，然后继续。这个决策树表现不佳的可能原因是缺乏剪枝。当构建决策树时，它有许多叶子，通常太多。然后，将树剪枝到一定水平以最小化过拟合。决策树的一个大优点是它在构建分支时考虑到的单词之间的隐式交互效应。当多个术语一起创建比单个术语更强的分类时，决策树实际上会优于朴素贝叶斯。我们不会深入探讨这一点，但请考虑这是您可以采取的下一步来改进模型。
- en: We now have two classification models that give us insight into how the two
    contents of the subreddits differ. The last step would be to share this newfound
    information with other people.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了两个分类模型，它们让我们了解了两个subreddits的内容有何不同。最后一步就是将这个新发现的信息与其他人分享。
- en: '8.3.8\. Step 6: Presentation and automation'
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.8\. 步骤6：展示和自动化
- en: As a last step we need to use what we learned and either turn it into a useful
    application or present our results to others. The last chapter of this book discusses
    building an interactive application, as this is a project in itself. For now we’ll
    content ourselves with a nice way to convey our findings. A nice graph or, better
    yet, an interactive graph, can catch the eye; it’s the icing on the presentation
    cake. While it’s easy and tempting to represent the numbers as such or a bar chart
    at most, it could be nice to go one step further.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一步，我们需要运用我们所学到的知识，要么将其转化为有用的应用，要么向他人展示我们的成果。本书的最后一章讨论了构建交互式应用，因为这本身就是一个项目。现在，我们将满足于以一种优雅的方式传达我们的发现。一张好的图表，或者更好的是，一张交互式图表，可以吸引人的注意力；这是演示文稿的点睛之笔。虽然将数字以这样的方式或最多以条形图的形式表示很容易且诱人，但更进一步可能更好。
- en: For instance, to represent the Naïve Bayes model, we could use a force graph
    ([figure 8.27](#ch08fig27)), where the bubble and link size represent how strongly
    related a word is to the “game of thrones” or “data science” subreddits. Notice
    how the words on the bubbles are often cut off; remember this is because of the
    stemming we applied.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了表示朴素贝叶斯模型，我们可以使用力图([图8.27](#ch08fig27))，其中气泡和链接的大小表示一个单词与“权力的游戏”或“数据科学”subreddits的关联强度。注意气泡上的单词通常被截断；记住这是由于我们应用的词干提取造成的。
- en: Figure 8.27\. Interactive force graph with the top 20 Naïve Bayes significant
    terms and their weights
  id: totrans-289
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.27\. 包含前20个朴素贝叶斯显著术语及其权重的交互式力图
- en: '![](Images/08fig27_alt.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig27_alt.jpg)'
- en: While [figure 8.27](#ch08fig27) in itself is static, you can open the HTML file
    “forceGraph.html” to enjoy the d3.js force graph effect as explained earlier in
    this chapter. d3.js is outside of this book’s scope but you don’t need an elaborate
    knowledge of d3.js to use it. An extensive set of examples can be used with minimal
    adjustments to the code provided at [https://github.com/mbostock/d3/wiki/Gallery](https://github.com/mbostock/d3/wiki/Gallery).
    All you need is common sense and a minor knowledge of JavaScript. The code for
    the force graph example can found at [http://bl.ocks.org/mbostock/4062045](http://bl.ocks.org/mbostock/4062045).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然[图8.27](#ch08fig27) 本身是静态的，但你可以通过打开“forceGraph.html”这个HTML文件来享受本章前面解释过的d3.js力图效果。d3.js超出了本书的范围，但你不需要对d3.js有深入的了解就可以使用它。可以通过对提供的代码进行最小调整来使用一套广泛的示例，这些示例可以在
    [https://github.com/mbostock/d3/wiki/Gallery](https://github.com/mbostock/d3/wiki/Gallery)
    找到。你所需要的只是常识和一点JavaScript知识。力图示例的代码可以在 [http://bl.ocks.org/mbostock/4062045](http://bl.ocks.org/mbostock/4062045)
    找到。
- en: We can also represent our decision tree in a rather original way. We could go
    for a fancy version of an actual tree diagram, but the following sunburst diagram
    is more original and equally fun to use.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以以一种相当独特的方式表示我们的决策树。我们可以选择一个实际的树图的华丽版本，但下面的太阳图更加独特，同样有趣。
- en: '[Figure 8.28](#ch08fig28) shows the top layer of the sunburst diagram. It’s
    possible to zoom in by clicking a circle segment. You can zoom back out by clicking
    the center circle. The code for this example can be found at [http://bl.ocks.org/metmajer/5480307](http://bl.ocks.org/metmajer/5480307).'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.28](#ch08fig28) 展示了太阳图的最顶层。可以通过点击一个圆环段来放大视图。点击中心圆可以缩小视图。本例的代码可以在 [http://bl.ocks.org/metmajer/5480307](http://bl.ocks.org/metmajer/5480307)
    找到。'
- en: Figure 8.28\. Sunburst diagram created from the top four branches of the decision
    tree model
  id: totrans-294
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.28\. 从决策树模型的前四个分支创建的太阳图
- en: '![](Images/08fig28_alt.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig28_alt.jpg)'
- en: Showing your results in an original way can be key to a successful project.
    People never appreciate the effort you’ve put into achieving your results if you
    can’t communicate them and they’re meaningful to them. An original data visualization
    here and there certainly helps with this.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 以一种独特的方式展示你的结果可能是成功项目的关键。如果你无法传达你的成果，而它们对他们来说又是有意义的，人们永远不会欣赏你为达到这些成果所付出的努力。在这里，偶尔使用一些原创的数据可视化肯定有助于这一点。
- en: 8.4\. Summary
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4\. 摘要
- en: Text mining is widely used for things such as entity identification, plagiarism
    detection, topic identification, translation, fraud detection, spam filtering,
    and more.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本挖掘被广泛用于实体识别、抄袭检测、主题识别、翻译、欺诈检测、垃圾邮件过滤等领域。
- en: Python has a mature toolkit for text mining called NLTK, or the natural language
    toolkit. NLTK is good for playing around and learning the ropes; for real-life
    applications, however, Scikit-learn is usually considered more “production-ready.”
    Scikit-learn is extensively used in previous chapters.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 拥有一个成熟的文本挖掘工具包，称为 NLTK，或自然语言工具包。NLTK 适合玩耍和学习基本操作；然而，对于实际应用，Scikit-learn
    通常被认为更“适用于生产”。Scikit-learn 在前几章中被广泛使用。
- en: The data preparation of textual data is more intensive than numerical data preparation
    and involves extra techniques, such as
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据准备比数值数据准备更复杂，涉及额外的技术，例如
- en: '***Stemming*** —Cutting the end of a word in a smart way so it can be matched
    with some conjugated or plural versions of this word.'
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词干提取** —— 以一种智能的方式截取单词的末尾，以便它可以与这个单词的某些变形或复数形式相匹配。'
- en: '***Lemmatization*** —Like stemming, it’s meant to remove doubles, but unlike
    stemming, it looks at the meaning of the word.'
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词形还原** —— 与词干提取类似，它的目的是去除重复的部分，但与词干提取不同的是，它关注单词的意义。'
- en: '***Stop word filtering*** —Certain words occur too often to be useful and filtering
    them out can significantly improve models. Stop words are often corpus-specific.'
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**停用词过滤** —— 某些单词出现得太频繁，以至于没有用处，过滤掉它们可以显著提高模型。停用词通常是语料库特定的。'
- en: '***Tokenization*** —Cutting text into pieces. Tokens can be single words, combinations
    of words (n-grams), or even whole sentences.'
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词** —— 将文本分割成片段。标记可以是单个单词，单词的组合（n-gram），甚至是整个句子。'
- en: '***POS Tagging*** —Part-of-speech tagging. Sometimes it can be useful to know
    what the function of a certain word within a sentence is to understand it better.'
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词性标注** —— 词性标注。有时了解句子中某个单词的功能对于更好地理解它是有用的。'
- en: In our case study we attempted to distinguish Reddit posts on “Game of Thrones”
    versus posts on “data science.” In this endeavor we tried both the Naïve Bayes
    and decision tree classifiers. Naïve Bayes assumes all features to be independent
    of one another; the decision tree classifier assumes dependency, allowing for
    different models.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的案例研究中，我们试图区分关于“权力的游戏”的 Reddit 帖子和关于“数据科学”的帖子。在这个努力中，我们尝试了朴素贝叶斯和决策树分类器。朴素贝叶斯假设所有特征都是相互独立的；决策树分类器假设存在依赖关系，允许使用不同的模型。
- en: In our example, Naïve Bayes yielded the better model, but very often the decision
    tree classifier does a better job, usually when more data is available.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的例子中，朴素贝叶斯产生了更好的模型，但非常经常决策树分类器做得更好，通常当有更多数据时。
- en: We determined the performance difference using a confusion matrix we calculated
    after applying both models on new (but labeled) data.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过在新的（但已标记的）数据上应用这两种模型后计算出的混淆矩阵来确定性能差异。
- en: When presenting findings to other people, it can help to include an interesting
    data visualization capable of conveying your results in a memorable way.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当向其他人展示研究结果时，包括一个有趣的数据可视化可以帮助以难忘的方式传达你的结果。
