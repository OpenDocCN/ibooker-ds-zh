- en: 3 Introducing Metaflow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 介绍Metaflow
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Defining a workflow in Metaflow that accepts input data and produces useful
    outputs
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Metaflow中定义一个接受输入数据并产生有用输出的工作流程
- en: Optimizing the performance of workflows with parallel computation on a single
    instance
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单个实例上使用并行计算优化工作流程的性能
- en: Analyzing the results of workflows in notebooks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在笔记本中分析工作流程的结果
- en: Developing a simple end-to-end application in Metaflow
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Metaflow中开发一个简单的端到端应用程序
- en: You are probably anxious to roll up your sleeves and start hacking actual code,
    now that we have a development environment set up. In this chapter, you will learn
    the basics of developing data science applications using Metaflow, a framework
    that shows how different layers of the infrastructure stack can work together
    seamlessly.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了开发环境，您可能已经迫不及待地想要动手编写实际的代码了。在本章中，您将学习使用Metaflow开发数据科学应用程序的基础知识，Metaflow是一个框架，展示了基础设施堆栈的不同层如何无缝协作。
- en: 'The development environment, which we discussed in the previous chapter, determines
    *how* the data scientist develops applications: by writing code in an editor,
    evaluating it in a terminal, and analyzing results in a notebook. On top of this
    toolchain, the data scientist uses Metaflow to determine *what* code gets written
    and *why*, which is the topic of this chapter. The next chapters will then cover
    the infrastructure that determines *where* and *when* the workflows are executed.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一章中讨论的开发环境决定了数据科学家如何开发应用程序：通过在编辑器中编写代码，在终端中评估它，并在笔记本中分析结果。在这个工具链之上，数据科学家使用Metaflow来确定编写什么代码以及为什么编写这些代码，这正是本章的主题。接下来的章节将涵盖确定工作流程在哪里以及何时执行的基础设施。
- en: We will introduce Metaflow from the ground up. You will first learn the syntax
    and the basic concepts that allow you to define basic workflows in Metaflow. After
    this, we will introduce branches in workflows. Branches are a straightforward
    way to embed concurrency in workflows, which often leads to higher performance
    through parallel computation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从零开始介绍Metaflow。您将首先学习语法和基本概念，这些概念允许您在Metaflow中定义基本的工作流程。之后，我们将介绍工作流程中的分支。分支是嵌入并发到工作流程中的直接方法，这通常通过并行计算带来更高的性能。
- en: Finally, we put all these concepts into action by building a realistic classifier
    application. By going through an end-to-end project, you will learn how Metaflow
    powers the prototyping loop by providing tools for local code evaluation, debugging,
    and result inspection in notebooks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过构建一个实际的分类器应用程序将这些概念付诸实践。通过完成一个端到端的项目，您将学习Metaflow如何通过提供在笔记本中进行本地代码评估、调试和结果检查的工具来驱动原型设计循环。
- en: After reading this chapter, you, or the data scientists you support, will be
    able to develop fully functional data science applications by combining Metaflow
    with other off-the-shelf libraries. The subsequent chapters will build on this
    foundation and show how you can make applications more scalable, highly available,
    and amenable to collaboration by utilizing the full infrastructure stack. You
    can find all code listings for this chapter at [http://mng.bz/xnB6](http://mng.bz/xnB6).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本章后，您或您支持的数据科学家将能够通过结合Metaflow与其他现成库来开发功能齐全的数据科学应用程序。随后的章节将在此基础上构建，并展示您如何通过利用完整的基础设施堆栈来使应用程序更具可扩展性、高度可用性和易于协作。您可以在[http://mng.bz/xnB6](http://mng.bz/xnB6)找到本章的所有代码列表。
- en: 3.1 The basics of Metaflow
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 Metaflow的基础知识
- en: '*Alex realizes that the job of a data scientist entails much more than just
    building models. As the first data scientist at Caveman Cupcakes, Alex has a great
    opportunity to help the company by building complete data science solutions independently.
    Alex finds the situation both exhilarating and terrifying. Alex is a marine biologist
    by training, not a software engineer—hopefully, building the necessary software
    around models won’t be too daunting. Bowie recommends that they take a look at
    Metaflow, a framework that is supposed to make building end-to-end data science
    applications easy.*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*亚历克斯意识到数据科学家的工作远不止构建模型。作为Caveman Cupcakes的第一位数据科学家，亚历克斯有机会通过独立构建完整的数据科学解决方案来帮助公司。亚历克斯既感到兴奋又感到害怕。亚历克斯是一位海洋生物学家，而不是软件工程师——希望围绕模型构建必要的软件不会太令人畏惧。鲍伊建议他们看看Metaflow，这是一个据说可以简化构建端到端数据科学应用程序的框架。*'
- en: '![CH03_00_UN01_Tuulos](../../OEBPS/Images/CH03_00_UN01_Tuulos.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_00_UN01_Tuulos](../../OEBPS/Images/CH03_00_UN01_Tuulos.png)'
- en: 'Metaflow was started at Netflix in 2017 to help data scientists build, deliver,
    and operate complete data science applications independently. The framework was
    designed to address a practical business need: a large company like Netflix has
    tens if not hundreds of potential use cases for data science, similar to those
    of Caveman Cupcakes. The company wants to test new ideas quickly in a realistic
    setup, preferably without having to allocate a large team to work on an experimental
    idea, and then promote the most promising experiments to production without too
    much overhead.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow 于 2017 年在 Netflix 启动，旨在帮助数据科学家独立构建、交付和运营完整的数据科学应用。该框架旨在解决一个实际业务需求：像
    Netflix 这样的大型公司可能有数十个甚至数百个潜在的数据科学用例，类似于洞穴人蛋糕店。公司希望在现实设置中快速测试新想法，最好是不需要分配大量团队来研究一个实验性想法，然后将最有前途的实验推广到生产中，而无需过多的开销。
- en: 'The ideas introduced in chapter 1 serve as a motivation for Metaflow: we need
    to account for the full stack of data science, we want to cover the whole life
    cycle of projects from prototype to production, and we want to do this by focusing
    on data scientist productivity. We can use the four Vs introduced in chapter 1
    to answer the question, “Why Metaflow?” as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第一章中介绍的思想是 Metaflow 的动力：我们需要考虑数据科学的整个堆栈，我们希望涵盖项目从原型到生产的整个生命周期，我们希望通过关注数据科学家的生产力来实现这一点。我们可以使用第一章中引入的四个“V”来回答“为什么是
    Metaflow？”这个问题，如下所示：
- en: '*Volume*—Metaflow helps to deliver more data science applications with fewer
    human resources involved. It reduces accidental complexity in the humming factory
    of data science applications by providing a uniform way to build them, leveraging
    the common language of workflows.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*体积*—Metaflow 通过提供一种统一的方式来构建它们，利用工作流程的通用语言，帮助以更少的人力资源交付更多的数据科学应用。它通过减少数据科学应用嗡嗡声中的偶然复杂性来实现这一点。'
- en: '*Variety*—Metaflow is not optimized for any particular type of data science
    problems. It helps deliver a diverse set of applications by being more opinionated
    at the lower layers of the stack and less opinionated about the top, domain-specific
    layers.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多样性*—Metaflow 并未针对任何特定类型的数据科学问题进行优化。它通过在堆栈的低层提供更多观点，而在顶层、特定领域的层提供较少观点，帮助交付多样化的应用。'
- en: '*Velocity*—Metaflow speeds up the prototyping loop, as well as interaction
    with production deployments. It does this by prioritizing human productivity in
    all parts of the framework, for example, by allowing data scientists to use idiomatic
    Python.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*速度*—Metaflow 加快了原型设计循环以及与生产部署的交互。它通过在整个框架的所有部分优先考虑人类生产力来实现这一点，例如，允许数据科学家使用惯用的
    Python。'
- en: '*Validity*—Metaflow makes applications more robust by enforcing best practices
    that make building and operating production-grade applications feasible, even
    by data scientists without a DevOps background.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*有效性*—Metaflow 通过强制执行最佳实践，使构建和运营生产级应用成为可能，即使对于没有 DevOps 背景的数据科学家也是如此，从而使得应用更加健壮。'
- en: In the data scientist’s point of view, Metaflow is all about making the prototyping
    loop and interaction with production deployments, which we introduced in section
    2.1, as smooth as possible. Doing this well requires that all layers of the infrastructure
    stack are integrated seamlessly. Whereas some frameworks tackle only workflows,
    compute resources, or model operations, Metaflow aims to address the full stack
    of data science, as depicted in figure 3.1.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据科学家的角度来看，Metaflow 的全部内容都是使原型设计循环和生产部署的交互尽可能平滑，正如我们在第 2.1 节中介绍的那样。要做好这一点，需要确保基础设施堆栈的所有层都能够无缝集成。而一些框架只处理工作流程、计算资源或模型操作，而
    Metaflow 旨在解决数据科学的全堆栈问题，如图 3.1 所示。
- en: '![CH03_F01_Tuulos](../../OEBPS/Images/CH03_F01_Tuulos.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F01_Tuulos](../../OEBPS/Images/CH03_F01_Tuulos.png)'
- en: Figure 3.1 Metaflow binds together the layers of the data science stack.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 Metaflow 将数据科学堆栈的各个层绑定在一起。
- en: In the engineering point of view, Metaflow acts as a substrate for integrations
    rather than as an attempt to reinvent individual layers of the stack. Companies
    have built or bought great solutions for data warehousing, data engineering, compute
    platforms, and job scheduling, not to mention the vibrant ecosystem of open source
    machine learning libraries. It would be unnecessary and unproductive to try to
    replace the existing established systems to accommodate the needs of data scientists.
    We should want to integrate data science applications into the surrounding business
    systems, not isolate them on an island.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从工程角度来看，Metaflow充当集成的基础，而不是试图重新发明堆栈的各个层。公司已经为数据仓库、数据工程、计算平台和作业调度构建或购买了优秀的解决方案，更不用说开源机器学习库的繁荣生态系统了。试图用现有的成熟系统来满足数据科学家的需求既不必要也不高效。我们应该希望将数据科学应用集成到周围的业务系统中，而不是将它们孤立在一个孤岛上。
- en: Metaflow is based on a plugin architecture that allows different backends to
    be used for different layers of the stack, as long as the layers can support a
    set of basic operations. In particular, Metaflow is designed to be a cloud-native
    framework, relying on basic compute and storage abstractions provided by all major
    cloud providers.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow基于插件架构，允许在不同的堆栈层使用不同的后端，只要这些层能够支持一组基本操作。特别是，Metaflow被设计成一个云原生框架，依赖于所有主要云提供商提供的基本计算和存储抽象。
- en: Metaflow has a gentle adoption curve. You can get started with the “single-player
    mode” on a laptop and gradually scale the infrastructure out to the cloud as your
    needs grow. In the remaining sections of this chapter, we will introduce the basics
    of Metaflow. In the chapters to follow, we will expand its footprint and show
    how to address increasingly complex data science applications, spanning all the
    layers of the stack, and enhance collaboration among multiple data scientists.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow有一个平缓的采用曲线。你可以在笔记本电脑上以“单玩家模式”开始，随着需求的增长，逐渐将基础设施扩展到云上。在本章的剩余部分，我们将介绍Metaflow的基础知识。在接下来的章节中，我们将扩大其影响力，展示如何解决越来越复杂的数据科学应用，涵盖堆栈的所有层，并增强多个数据科学家之间的协作。
- en: 'If you want to build your infrastructure using other frameworks instead of
    Metaflow, you can read the next sections for inspiration—the concepts are applicable
    to many other frameworks, too—or you can jump straight in to chapter 4, which
    focuses on a foundational layer of the stack: compute resources.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想使用其他框架而不是Metaflow来构建你的基础设施，你可以阅读下一节以获得灵感——这些概念也适用于许多其他框架，或者你可以直接跳到第4章，该章节专注于堆栈的基础层：计算资源。
- en: 3.1.1 Installing Metaflow
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 安装Metaflow
- en: Metaflow greatly benefits from a cloud-based development environment, introduced
    in section 2.1, notebooks included. However, you can get started with just a laptop.
    As of the writing of this book, Metaflow supports OS X and Linux but not Windows.
    If you want to test Metaflow on Windows, you can use either the Windows Subsystem
    for Linux, a local Linux-based Docker container, or a cloud-based editor as discussed
    in the previous chapter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow从第2.1节中引入的基于云的开发环境中受益匪浅，包括笔记本在内。然而，你只需一台笔记本电脑就可以开始使用。截至本书编写时，Metaflow支持OS
    X和Linux，但不支持Windows。如果你想在Windows上测试Metaflow，可以使用Windows Subsystem for Linux、基于Linux的本地Docker容器或前一章中讨论的基于云的编辑器。
- en: 'Metaflow supports any Python version later than Python 3.5\. After installing
    a Python interpreter, you can install Metaflow as any other Python package using
    pip as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow支持所有高于Python 3.5版本的Python。安装Python解释器后，你可以像安装其他Python包一样使用pip安装Metaflow，如下所示：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In this book, lines prefixed with #, like the previous one, are meant to be
    executed in a terminal window without the hash mark.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，带有#前缀的行，如上一行，意味着需要在没有井号标记的终端窗口中执行。
- en: Note In all examples, we assume that the pip and python commands refer to the
    latest version of Python, which should be later than Python 3.5\. In some systems,
    the correct commands are called pip3 and python3. In this case, substitute the
    commands in examples accordingly.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在所有示例中，我们假设pip和python命令指的是Python的最新版本，该版本应高于Python 3.5。在某些系统中，正确的命令被称为pip3和python3。在这种情况下，相应地替换示例中的命令。
- en: 'You can confirm that Metaflow works simply by executing the following code:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过执行以下代码来确认Metaflow是否工作：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If Metaflow was installed correctly, this should print a top-level help with
    a header like so:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Metaflow安装正确，它应该会打印出一个带有如下标题的顶级帮助信息：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can follow the examples in this chapter without a cloud (AWS) account, but
    if you want to try out all examples in the coming chapters, you will need one.
    You can sign up for a free account at [https://aws.amazon.com/free](https://aws.amazon.com/free).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以不使用云（AWS）账户就跟随本章中的示例，但如果您想尝试下一章中的所有示例，您将需要一个账户。您可以在[https://aws.amazon.com/free](https://aws.amazon.com/free)注册一个免费账户。
- en: 3.1.2 Writing a basic workflow
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 编写基本工作流
- en: As described in the previous chapter, the concept of a workflow helps to structure
    a data science application. It is considerably easier to think of your application
    in terms of steps of a workflow rather than as a set of arbitrary Python modules,
    especially if you are not a software engineer by training.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所述，工作流的概念有助于结构化数据科学应用。如果您不是软件工程师出身，将您的应用视为工作流步骤而不是一组任意的Python模块要容易得多。
- en: Imagine Alex, our protagonist, writing a Metaflow workflow. Alex is already
    familiar with notebooks, so the concept of writing small snippets of Python as
    steps sounds doable. Steps are like notebook cells on steroids. It would require
    much more cognitive effort to piece together an application using arbitrary Python
    classes, functions, and modules.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下我们的主角Alex编写一个Metaflow工作流。Alex已经熟悉笔记本，所以将Python的小片段作为步骤编写看起来是可行的。步骤就像强化版的笔记本单元格。使用任意的Python类、函数和模块来拼凑一个应用将需要更多的认知努力。
- en: Let’s start with a classic Hello World example. Everything in Metaflow is centered
    on the concept of a workflow, or simply a flow, which is a directed acyclic graph
    (DAG), as discussed in section 2.2\. Our HelloWorldFlow, defined in listing 3.1,
    corresponds to the DAG depicted in figure 3.2.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从经典的Hello World示例开始。在Metaflow中，一切都是以工作流的概念为中心的，或者简单地说，是一个流程，它是一个有向无环图（DAG），如2.2节中讨论的那样。我们定义在列表3.1中的HelloWorldFlow对应于图3.2中展示的DAG。
- en: '![CH03_F02_Tuulos](../../OEBPS/Images/CH03_F02_Tuulos.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F02_Tuulos](../../OEBPS/Images/CH03_F02_Tuulos.png)'
- en: Figure 3.2 HelloWorldFlow
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 HelloWorldFlow
- en: 'To define a workflow in Metaflow, you must follow these six simple rules:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Metaflow中定义工作流，您必须遵循以下六个简单规则：
- en: '*A flow* is defined as a Python class that is derived from the FlowSpec class.
    You can name your flows freely. In this book, by convention the flow class names
    end with a Flow suffix, as in HelloWorldFlow. You can include any methods (functions)
    in this class, but methods annotated with @step are treated specially.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*流程*定义为从FlowSpec类派生的Python类。您可以自由命名您的流程。在本书中，按照惯例，流程类的名称以Flow后缀结尾，如HelloWorldFlow。您可以在该类中包含任何方法（函数），但用@step注解的方法会被特别处理。'
- en: '*A step* (node) of the flow is a method of the class, annotated with the @step
    decorator. You can write arbitrary Python in the method body, but the last line
    is special, as described next. You can include an optional docstring in the method,
    explaining the purpose of the step. After the first example, we will omit docstrings
    to keep listings concise in the book, but it is advisable to use them in real-life
    code.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*步骤*（节点）是类的成员方法，用@step装饰器进行注解。您可以在方法体中编写任意的Python代码，但最后一行是特殊的，如下所述。您可以在方法中包含一个可选的文档字符串，解释步骤的目的。在第一个示例之后，我们将省略文档字符串以使书中的列表简洁，但在实际代码中建议使用它们。'
- en: Metaflow executes the method bodies as an atomic unit of computation called
    *a task*. In a simple flow like this, there is a one-to-one correspondence between
    a step and a task, but that’s not always the case, as we will see later in section
    3.2.3.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Metaflow将方法体作为称为*任务*的原子计算单元执行。在这样一个简单的流程中，步骤与任务之间是一对一的对应关系，但情况并不总是如此，我们将在3.2.3节中稍后看到。
- en: The first step must be called start, so the flow has an unambiguous starting
    point.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步必须命名为start，这样流程就有了一个明确的起点。
- en: The edges (arrows) between steps are defined by calling self.next (step_name)
    on the last line of the method, where step_name is the name of the next step to
    be executed.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤（节点）之间的边（箭头）是通过在方法的最后一行调用self.next（step_name）来定义的，其中step_name是要执行的下一步的名称。
- en: The last step must be called end. Because the end step finishes the flow, it
    doesn’t need a self.next transition on the last line.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步必须命名为end。因为结束步骤完成了流程，所以最后一行不需要self.next转换。
- en: One Python file (module) must contain only a single flow. You should instantiate
    the flow class at the bottom of the file inside an if __name__ == '__main__' conditional,
    which causes the class to be evaluated only if the file is called as a script.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个Python文件（模块）必须只包含一个流程。你应该在文件的底部，在if __name__ == '__main__'条件语句内实例化流程类，这会导致类仅在文件作为脚本被调用时被评估。
- en: The corresponding source code is listed in the next code listing.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的源代码列在下一代码列表中。
- en: Listing 3.1 Hello World
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.1 Hello World
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ A workflow is defined by subclassing from FlowSpec.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 工作流是通过从FlowSpec派生来定义的。
- en: ❷ The @step decorator denotes a step in the workflow.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ @step装饰器表示工作流中的一个步骤。
- en: ❸ The first step must be called start.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 第一步必须命名为start。
- en: ❹ A call to self.next() denotes an edge in the workflow.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 对self.next()的调用表示工作流中的一个边。
- en: ❺ The last step must be called end.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 最后一步必须命名为end。
- en: ❻ Instantiating the workflow allows it to be executed.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 实例化工作流允许它被执行。
- en: 'Here’s how to read and comprehend code that corresponds to a Metaflow flow:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何阅读和理解与Metaflow流程相对应的代码：
- en: First, find the start method. You know that this is where the execution starts.
    You can read the method to understand what it is doing.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，找到启动方法。你知道这是执行开始的地方。你可以阅读这个方法来了解它在做什么。
- en: See what the next step is by looking at the last line of start. In this case,
    it is self.hello, that is, the hello method.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过查看start的最后一条来查看下一个步骤。在这种情况下，它是self.hello，即hello方法。
- en: Read the code for the next step, and identify the step after that. Keep doing
    this until you reach the end step.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读下一步的代码，并确定那之后的步骤。继续这样做，直到你到达结束步骤。
- en: 'Doing this is simpler than trying to understand an arbitrary set of Python
    functions and modules that don’t even have a clear beginning and end. Save the
    code in a file, helloworld.py. You can execute the Python as any Python script.
    First, try running the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 做这件事比试图理解一组任意的Python函数和模块要简单得多，这些函数和模块甚至没有明确的开始和结束。将代码保存在文件中，命名为helloworld.py。你可以像任何Python脚本一样执行Python。首先，尝试运行以下代码：
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This will validate the flow structure without executing any steps. Metaflow
    has a number of rules for what is considered a valid DAG. For instance, all steps
    must be connected to each other, and there must not be any cycles in the graph.
    If Metaflow detects any issues with your DAG, a helpful error message is shown.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这将验证流程结构而不执行任何步骤。Metaflow有一系列关于被认为是有效DAG的规则。例如，所有步骤都必须相互连接，图中不能有任何循环。如果Metaflow检测到你的DAG有任何问题，将显示有用的错误信息。
- en: 'Metaflow also runs a basic code check, a *linter*, every time you execute the
    script, which can detect typos, missing functions, and other such syntactic errors.
    If any issues are found, an error is shown and nothing else is run. This can be
    a huge time-saver, because issues can be detected before any time is spent on
    running the code. However, sometimes the linter can produce false positives. In
    this case, you can disable it by specifying the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow在每次执行脚本时都会运行一个基本的代码检查，即一个*代码检查器*，它可以检测拼写错误、缺失的函数和其他类似的语法错误。如果发现任何问题，将显示错误信息，并且不会执行其他操作。这可以节省大量时间，因为问题可以在代码执行之前被发现。然而，有时代码检查器可能会产生误报。在这种情况下，你可以通过指定以下内容来禁用它：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now try running the next code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试运行以下代码：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This should print out a textual representation of the DAG, which for HelloWorldFlow
    corresponds to figure 3.2\. You can see that docstrings are included in the output,
    so you can use the show command to get a quick idea of what an unfamiliar flow
    does.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会打印出DAG的文本表示，对于HelloWorldFlow来说，对应于图3.2。你可以看到输出中包含了文档字符串，因此你可以使用show命令来快速了解一个不熟悉的流程做了什么。
- en: 'Now, the moment of truth: let’s execute the flow, as shown next! We call an
    execution of a flow *a run*:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候检验真伪了：让我们执行流程，如下所示！我们称流程的一次执行为*运行*：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This command executes the start, hello, and end methods in order. If all goes
    well, you should see a bunch of lines printed out that look like this:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令按顺序执行start、hello和end方法。如果一切顺利，你应该会看到一系列看起来像这样的输出行：
- en: '![CH03_F02_UN02_Tuulos](../../OEBPS/Images/CH03_F02_UN02_Tuulos.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F02_UN02_Tuulos](../../OEBPS/Images/CH03_F02_UN02_Tuulos.png)'
- en: 'Every line printed from your flow to the standard output (aka *stdout*) or
    the standard error (aka *stderr*) streams will get a header like shown previously.
    Let’s parse the header as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从你的流程打印到标准输出（也称为*stdout*）或标准错误（也称为*stderr*）的每一行都会得到一个类似于之前显示的标题。让我们按照以下方式解析标题：
- en: '*Timestamp* denotes when the line was output. You can take a look at consecutive
    timestamps to get a rough idea of how long different segments of the code take
    to execute. A short delay may occur between a line being output and the minting
    of a timestamp, so don’t rely on the timestamps for anything that requires accurate
    timekeeping.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时间戳*表示该行输出的时间。您可以通过查看连续的时间戳来大致了解代码的不同部分执行所需的时间。在输出一行和生成时间戳之间可能会出现短暂的延迟，因此不要依赖于时间戳进行任何需要精确时间记录的操作。'
- en: 'The following information inside the square brackets identifies a task:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方括号内的以下信息用于标识一个任务：
- en: Every Metaflow run gets a unique ID, a *run ID*.
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个Metaflow运行都会获得一个唯一的ID，即*运行ID*。
- en: A run executes the steps in order. The step that is currently being executed
    is denoted by *step name*.
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行按照顺序执行步骤。当前正在执行的步骤用*步骤名称*表示。
- en: A step may spawn multiple tasks using the foreach construct (see section 3.2.3),
    which are identified by a *task ID*.
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤可以使用foreach构造（参见第3.2.3节）产生多个任务，这些任务由*任务ID*标识。
- en: The combination of a flow name, run ID, step name, and a task ID uniquely identifies
    a task in your Metaflow environment, among all runs of any flow. Here, the flow
    name is omitted because it is the same for all lines. We call this globally unique
    identifier a *pathspec*.
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流名称、运行ID、步骤名称和任务ID的组合在您的Metaflow环境中唯一地标识了任何流程运行中的任务。在这里，流名称被省略，因为对于所有行都是相同的。我们称这个全局唯一标识符为*路径规范*。
- en: Each task is executed by a separate process in your operating system, identified
    by a *process ID*, aka *pid*. You can use any operating system-level monitoring
    tools, such as *top,* to monitor resource consumption of a task based on its process
    ID.
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个任务由操作系统中的单独进程执行，该进程由*进程ID*，即*pid*标识。您可以使用任何操作系统级别的监控工具，如*top*，根据进程ID监控任务的资源消耗。
- en: After the square bracket comes a *log message***,** which may be a message output
    by Metaflow itself, like “Task is starting” in this example, or a line output
    by your code.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方括号之后是*日志消息*，这可能是Metaflow本身输出的消息，如本例中的“任务开始”，或者是由您的代码输出的行。
- en: 'What’s the big deal about the IDs? Running a countless number of quick experiments
    is a core activity in data science—remember the prototyping loop we discussed
    earlier. Imagine hacking many different variations of the code, running them,
    and seeing slightly different results every time. After a while, it is easy to
    lose track of results: was it the third version that produced promising results
    or the sixth one?'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 关于ID的特别之处是什么？运行无数快速实验是数据科学的核心活动——记得我们之前讨论过的原型设计循环。想象一下，编写许多不同的代码变体，运行它们，每次都会看到略有不同的结果。过了一段时间，很容易失去对结果的跟踪：是第三个版本产生了有希望的结果，还是第六个版本？
- en: In the old days, a diligent scientist might have recorded all their experiments
    and their results in a lab notebook. A decade ago, a spreadsheet might have served
    the same role, but keeping track of experiments was still a manual, error-prone
    process. Today, a modern data science infrastructure keeps track of experiments
    automatically through *an experiment tracking system*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去，勤奋的科学家可能会在实验笔记本中记录所有他们的实验及其结果。十年前，电子表格可能起到了相同的作用，但跟踪实验仍然是一个手动且容易出错的流程。今天，现代数据科学基础设施通过*实验跟踪系统*自动跟踪实验。
- en: An effective experiment tracking system allows a data science team to inspect
    what has been run, identify each run or experiment unambiguously, access any past
    results, visualize them, and compare experiments against each other. Moreover,
    it is desirable to be able to rerun a past experiment and reproduce their results.
    Doing this accurately is much harder than it sounds, so we have dedicated many
    pages for the topic of *reproducibility* in chapter 6.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有效的实验跟踪系统允许数据科学团队能够检查已运行的内容，明确识别每个运行或实验，访问任何过去的结果，可视化它们，并将实验相互比较。此外，能够重新运行过去的实验并重现其结果是非常受欢迎的。这比听起来要困难得多，因此我们在第6章中专门用许多页面来讨论*可重现性*这一主题。
- en: Standalone experiment tracking products can work with any piece of code, as
    long as the code is instrumented appropriately to send metadata to the tracking
    system. If you use Metaflow to build data science applications, you get experiment
    tracking for free—Metaflow tracks all executions automatically. The IDs shown
    earlier are a part of this system. They allow you to identify and access results
    immediately after a task has completed.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 独立的实验跟踪产品可以与任何代码一起工作，只要代码被适当配置以向跟踪系统发送元数据。如果您使用Metaflow构建数据科学应用程序，您将免费获得实验跟踪——Metaflow自动跟踪所有执行。之前显示的ID是这个系统的一部分。它们允许您在任务完成后立即识别和访问结果。
- en: 'We will talk more about accessing past results in section 3.3.2, but you can
    get a taste by using the logs command, which allows you to inspect the output
    of any past run. Use the logs command with a pathspec corresponding to the task
    you want to inspect. For instance, you can copy and paste a pathspec from the
    output your run produces and execute the next command:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在3.3.2节中更详细地讨论访问过去的结果，但您可以通过使用日志命令来提前体验，该命令允许您检查任何过去运行的输出。使用与您想要检查的任务对应的路径规范执行日志命令。例如，您可以从运行产生的输出中复制并粘贴路径规范，然后执行下一个命令：
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You should see a line of output that corresponds to the print statement in the
    step you inspected. The logs subcommand has a few options, which you can see by
    executing logs --help.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到一行输出，它与您检查的步骤中的print语句相对应。日志子命令有几个选项，您可以通过执行logs --help来查看。
- en: Finally, notice how Metaflow turns a single Python file into a command-line
    application without any boilerplate code. You don’t have to worry about parsing
    command-line arguments or capturing logs manually. Every step is executed as a
    separate operating system-level subprocess, so they can be monitored independently.
    This is also a key feature enabling fault tolerance and scalability, as we will
    learn in chapter 4.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意Metaflow如何将单个Python文件转换为一个无需任何模板代码的命令行应用程序。您无需担心解析命令行参数或手动捕获日志。每个步骤都作为一个独立的操作系统级子进程执行，因此可以独立监控。这也是实现容错性和可扩展性的关键特性，我们将在第4章中学习。
- en: 3.1.3 Managing data flow in workflows
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.3 管理工作流程中的数据流
- en: 'Data science applications are all about processing data. A prototypical application
    ingests raw data from a data warehouse, transforms it in various ways, turns it
    into features, and maybe trains a model or does inference with an existing model.
    A trained model or predictions are then the output data of the workflow. To be
    able to build a workflow like this, you need to answer the following three questions:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学应用程序都是关于处理数据的。一个典型的应用程序从数据仓库中摄取原始数据，以各种方式对其进行转换，将其转换为特征，并可能训练一个模型或使用现有模型进行推理。训练好的模型或预测是工作流程的输出数据。要能够构建这样的工作流程，您需要回答以下三个问题：
- en: How should the workflow access the input data?
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工作流程应该如何访问输入数据？
- en: How should the workflow move transformed data, that is, the workflow’s internal
    state, across steps?
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工作流程应该如何移动转换后的数据，即工作流程的内部状态，跨步骤？
- en: How should the workflow make its outputs accessible by outside systems?
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工作流程应该如何使其输出对外部系统可用？
- en: By answering the three questions, you determine the *data flow* of the application,
    that is, the mechanisms for transporting data through the workflow. Figure 3.3
    depicts data flow in the context of a workflow that consists of three steps, A,
    B, and C.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 通过回答三个问题，您可以确定应用程序的*数据流*，即通过工作流程传输数据的方式。图3.3展示了由三个步骤A、B和C组成的工作流程中的数据流。
- en: '![CH03_F03_Tuulos](../../OEBPS/Images/CH03_F03_Tuulos.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F03_Tuulos](../../OEBPS/Images/CH03_F03_Tuulos.png)'
- en: Figure 3.3 Data flow from input to output
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 从输入到输出的数据流
- en: In the workflow depicted in figure 3.3, step A comes before step B. Because
    steps are executed in order, any data processed by step A could be made available
    to step B, but not vice versa. This way, the workflow order determines how data
    can flow through the graph. Using Metaflow terminology, data flows from the start
    step toward the end step, like water flowing in a river from upstream to downstream
    but never the reverse.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3.3所示的工作流程中，步骤A在步骤B之前。因为步骤是按顺序执行的，所以步骤A处理的数据可以提供给步骤B，但反之则不行。这样，工作流程的顺序决定了数据如何通过图流动。使用Metaflow术语，数据从起始步骤流向结束步骤，就像河水从上游流向下游，但不会反向流动。
- en: To illustrate why being explicit about the data flow and state is useful, consider
    the example shown in figure 3.4 that shows a simple Jupyter notebook.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明明确数据流和状态的好处，考虑图3.4所示的示例，它展示了一个简单的Jupyter笔记本。
- en: '![CH03_F04_Tuulos](../../OEBPS/Images/CH03_F04_Tuulos.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F04_Tuulos](../../OEBPS/Images/CH03_F04_Tuulos.png)'
- en: Figure 3.4 Hidden state and undefined data flow in a notebook
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 笔记本中隐藏的状态和未定义的数据流
- en: The output may seem surprising. Why is the value of x printed out as 2, even
    though it was just assigned to be 1 in the previous cell? In this case, the user
    first evaluated all cells from the top to the bottom and then decided to reevaluate
    the middle cell. Evaluating cells out of order is a common practice in notebooks.
    It is a part of their appeal as an unconstrained scratchpad.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 输出可能看起来令人惊讶。为什么x的值打印为2，尽管它在上一个单元格中只被分配为1？在这种情况下，用户首先从顶部到底部评估了所有单元格，然后决定重新评估中间的单元格。在笔记本中按顺序评估单元格是一种常见的做法。它是它们作为不受约束的草稿本吸引力的一部分。
- en: The Jupyter kernel maintains the state of all variables under the hood. It lets
    the user evaluate the cells in whichever order, based on its hidden state. Like
    in this case, the results can be very surprising and practically impossible to
    reproduce. In contrast, workflows solve the problem by making the evaluation order
    and the corresponding data flow explicit.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter内核在幕后维护所有变量的状态。它允许用户根据其隐藏状态以任意顺序评估单元格。就像在这个例子中，结果可能非常令人惊讶，实际上无法重现。相比之下，工作流程通过使评估顺序和相应的数据流明确来解决这个问题。
- en: Instead of using a notebook, the three cells could be organized as a workflow
    like the one depicted in figure 3.3, which would make it impossible to produce
    inconsistent results. Notebooks have an important role in the data science stack—they
    are convenient for quick explorations and analysis. However, as exemplified earlier,
    it is better to structure any serious application or modeling code as a workflow
    with an unambiguous data flow.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用笔记本不同，这三个单元格可以组织成如图3.3所示的工作流程，这将使得产生不一致的结果变得不可能。笔记本在数据科学堆栈中扮演着重要的角色——它们便于快速探索和分析。然而，如前所述，将任何严肃的应用或建模代码作为具有明确数据流的工作流程结构会更好。
- en: Transferring and persisting state in a workflow
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在工作流程中转移和持久化状态
- en: What does the data flow look like in practice? If all steps were executed within
    a single process on a single computer, we could keep the state in memory, which
    is the ordinary way of building software. A challenge for data science workflows
    is that we may want to execute steps on different computers in parallel or to
    access special hardware like GPUs. Hence, we need to be able to *transfer state*
    between steps, which may execute on physically separate computers.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 实际中数据流看起来是怎样的呢？如果所有步骤都在单台计算机上的单个进程中执行，我们就可以在内存中保持状态，这是构建软件的常规方式。数据科学工作流程的挑战在于，我们可能希望在不同计算机上并行执行步骤，或者访问特殊硬件，如GPU。因此，我们需要能够在步骤之间*转移状态*，这些步骤可能在不同物理计算机上执行。
- en: We can do this by *persisting state*, that is, by storing all data that is relevant
    for subsequent steps, after a step completes. Then, when a new step starts, even
    on another computer, we can load the state back and continue execution. Figure
    3.5 illustrates this idea.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过*持久化状态*来实现这一点，即在步骤完成后存储所有与后续步骤相关的数据。然后，当新的步骤开始时，即使在另一台计算机上，我们也可以重新加载状态并继续执行。图3.5说明了这个想法。
- en: '![CH03_F05_Tuulos](../../OEBPS/Images/CH03_F05_Tuulos.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F05_Tuulos](../../OEBPS/Images/CH03_F05_Tuulos.png)'
- en: Figure 3.5 Transferring state between steps through a common datastore
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 通过公共数据存储在步骤之间转移状态
- en: In figure 3.5, the state consists of a single variable, x. The variable is first
    initialized in step A and then incremented by steps B and C. When a step finishes,
    the value of x is persisted. Before a step starts, its value is loaded back. Naturally,
    the process needs to be repeated for every piece of state, for every variable,
    that needs to be accessed across steps.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3.5中，状态由一个变量x组成。变量首先在步骤A中初始化，然后在步骤B和C中增加。当步骤完成时，x的值被持久化。在步骤开始之前，其值被重新加载。自然地，对于需要跨步骤访问的每一块状态、每一个变量，都需要重复这个过程。
- en: You can implement the idea of a persisted state in many different ways. Many
    workflow frameworks are not particularly opinionated about it. It is up to the
    user to decide how to load and store the state, maybe using a database as the
    persistence layer. The resulting workflow code can resemble that shown in figure
    3.6\. Each step includes code for loading and storing data. Although this approach
    is flexible, it adds quite a bit of boilerplate code. Worse, it adds cognitive
    overhead to the data scientist, because they have to decide explicitly what data
    to persist and how.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过许多不同的方式实现持久化状态的想法。许多工作流程框架对此并不特别有意见。如何加载和存储状态取决于用户，可能使用数据库作为持久化层。生成的流程代码可能类似于图3.6中所示。每个步骤都包括加载和存储数据的代码。虽然这种方法很灵活，但它增加了大量的样板代码。更糟糕的是，它给数据科学家增加了认知负担，因为他们必须明确决定要持久化哪些数据以及如何持久化。
- en: '![CH03_F06_Tuulos](../../OEBPS/Images/CH03_F06_Tuulos.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F06_Tuulos](../../OEBPS/Images/CH03_F06_Tuulos.png)'
- en: Figure 3.6 Persisting state manually
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 手动持久化状态
- en: In our experience, data scientists can be quite conservative when storing data
    that may feel superfluous, in particular the workflow’s internal state, if they
    need to make an explicit choice about it, like in figure 3.6\. If the workflow
    framework makes it tedious to move state between steps, the user may be tempted
    to pack many unrelated operations in a single step, to avoid having to add boilerplate
    code for loading and storing data. Or, they may choose to persist only the outputs
    that are absolutely required by downstream consumers.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的经验，数据科学家在存储可能感觉多余的数据时可能会相当保守，特别是当需要对其做出明确选择时，比如图3.6中的情况。如果工作流程框架使得在步骤之间移动状态变得繁琐，用户可能会倾向于将许多无关的操作打包在一个步骤中，以避免添加加载和存储数据的样板代码。或者，他们可能会选择只持久化下游消费者绝对需要输出的数据。
- en: Although technically such a parsimonious approach can work, being too frugal
    with data is not great for the long-term health of the application. First, the
    workflow structure should primarily optimize for the logical structure of the
    application, so other human readers can easily understand it. For instance, it
    makes sense to have separate steps for data preprocessing and model training—you
    shouldn’t merge the steps just to avoid having to transfer state. Second, imagine
    the workflow failing in production. You want the maximum, not minimal, amount
    of information to understand what went wrong.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然从技术上讲，这种节俭的方法可以工作，但过于节俭地处理数据并不利于应用程序的长期健康。首先，工作流程结构应该主要优化应用程序的逻辑结构，以便其他读者可以轻松理解它。例如，对于数据预处理和模型训练有单独的步骤是有意义的——你不应该合并步骤只是为了避免转移状态。其次，想象一下工作流程在生产中失败的情况。你希望获得最大量的信息来了解出了什么问题。
- en: To summarize, it is beneficial to have a mechanism for state transfer that makes
    it nearly transparent to the user. We don’t want the user to have to worry about
    the technical detail that the steps may execute on physically distinct computers.
    Also, we don’t want them to compromise the readability of the workflow to avoid
    having to use boilerplate code.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，拥有一种使状态转移对用户几乎透明的机制是有益的。我们不希望用户必须担心步骤可能在实际不同的计算机上执行的技术细节。同样，我们也不希望他们为了避免使用样板代码而牺牲工作流程的可读性。
- en: Most important, we want to encourage the user to persist data liberally, even
    if persisting it is not strictly necessary to make the workflow functional. The
    more data is persisted for every run, the more *observable* the workflow becomes,
    complementing metadata stored by the experiment tracking system. If enough data
    is persisted after each step, we can get a comprehensive idea of the workflow’s
    status during and after execution, as illustrated by figure 3.7.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，我们希望鼓励用户自由地持久化数据，即使持久化数据并不是使工作流程功能性的严格必要条件。每次运行持久化的数据越多，工作流程就越*可观察*，这补充了实验跟踪系统存储的元数据。如果在每个步骤之后都足够持久化数据，我们就可以全面了解工作流程在执行期间和执行后的状态，如图3.7所示。
- en: '![CH03_F07_Tuulos](../../OEBPS/Images/CH03_F07_Tuulos.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F07_Tuulos](../../OEBPS/Images/CH03_F07_Tuulos.png)'
- en: Figure 3.7 Persisted state allows you to observe workflow execution.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 持久化状态允许你观察工作流程执行。
- en: This approach pays great dividends in the long term. You will be able to monitor
    and debug workflows more effectively and reproduce, reuse, and share their results
    without extra work. On the flip side, storing data costs money, but thanks to
    the cloud, storage costs are becoming quite insignificant compared to the data
    scientist’s time. Besides, we are not advocating for storing copies of the *input
    data* over and over again—more about this in chapter 7.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在长期来看会带来巨大的回报。您将能够更有效地监控和调试工作流程，并无需额外工作即可重现、重用和共享其结果。另一方面，存储数据需要花费金钱，但多亏了云，存储成本与数据科学家的时间相比变得相当微不足道。此外，我们并不提倡反复存储
    *输入数据* 的副本——关于这一点，请参阅第 7 章。
- en: Metaflow Artifacts
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow 工件
- en: 'To give an example of how you can make the data flow nearly transparent to
    the user, let’s consider how Metaflow does it. Metaflow automatically persists
    all *instance variables**,* that is, anything assigned to self in the step code.
    We call these persisted instance variables *artifacts*. Artifacts can be any data:
    scalar variables, models, data frames, or any other Python object that can be
    serialized using Python’s pickle library. Artifacts are stored in a common data
    repository called a *datastore*, which is a layer of persisted state managed by
    Metaflow. You can learn more about the datastore later in this chapter in the
    sidebar box, “How Metaflow’s datastore works.”'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了举例说明如何使数据流对用户几乎透明，让我们考虑 Metaflow 是如何做到的。Metaflow 自动持久化所有 *实例变量**，也就是说，在步骤代码中分配给
    self 的任何内容。我们称这些持久化的实例变量为 *工件*。工件可以是任何数据：标量变量、模型、数据框或任何可以使用 Python 的 pickle 库序列化的
    Python 对象。工件存储在称为 *数据存储库* 的通用数据仓库中，这是由 Metaflow 管理的持久化状态层。您可以在本章后面的侧边栏框“Metaflow
    的数据存储库如何工作”中了解更多关于数据存储库的信息。
- en: 'Each step may produce any number of artifacts. After a step has completed,
    its artifacts are persisted as immutable units of data in the datastore. Those
    artifacts are permanently bound to the step, identified by the pathspec that produced
    them. This is crucial for experiment tracking: we want to produce an accurate
    and unmodifiable audit trail of what was produced during a run. However, subsequent
    steps may read the artifact and produce their own version of it.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 每个步骤可以产生任意数量的工件。步骤完成后，其工件作为不可变的数据单元持久化到数据存储库中。这些工件永久绑定到步骤上，由产生它们的路径规范标识。这对于实验跟踪至关重要：我们希望产生一个准确且不可修改的审计跟踪，记录运行期间产生的所有内容。然而，后续步骤可能会读取工件并产生自己的版本。
- en: To make this concept more concrete, let’s start with a simple example that adds
    state and a counter variable, count, in a slightly modified version of HelloWorldFlow,
    which we introduced in listing 3.1\. For clarity, let’s rename the flow to CounterFlow.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个概念更加具体，让我们从一个简单的例子开始，该例子在 HelloWorldFlow 的略微修改版本中添加了状态和计数器变量 count，我们在列表
    3.1 中介绍了这个例子。为了清晰起见，让我们将流程重命名为 CounterFlow。
- en: 'As illustrated in figure 3.8, we initialize a counter variable, count, to zero
    in the start step. We do this simply by creating an instance variable in Python
    as usual, self.count = 0. In the following add step we increment count by one:
    self.count += 1. We increment count once more in the end step before printing
    out the final value, which is 2.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 3.8 所示，我们在起始步骤中初始化计数器变量 count 为零。我们这样做是通过在 Python 中创建一个实例变量，就像通常那样，self.count
    = 0。在接下来的添加步骤中，我们将 count 增加 1：self.count += 1。在打印最终值之前，我们在结束步骤中再次将 count 增加 1，最终值是
    2。
- en: '![CH03_F08_Tuulos](../../OEBPS/Images/CH03_F08_Tuulos.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F08_Tuulos](../../OEBPS/Images/CH03_F08_Tuulos.png)'
- en: Figure 3.8 CounterFlow
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 CounterFlow
- en: The next listing shows the corresponding code.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表显示了相应的代码。
- en: Listing 3.2 A simple flow that maintains state
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.2 维护状态的简单流程
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Initializes the count to zero
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化计数器为零
- en: ❷ Increments the count by one
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将计数器增加一
- en: ❸ Shows the final count
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 显示最终计数
- en: 'Save the flow code to a file called counter.py and execute it as before, like
    so:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 将流程代码保存到名为 counter.py 的文件中，并像之前一样执行，如下所示：
- en: '[PRE10]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In addition to the usual messages output by Metaflow, you should see a line
    saying, “the count is 0 before incrementing,” and “The final count is 2.” Assuming
    you are already familiar with the basics of Python, you will notice that the flow
    behaves like any Python object when self.start(), self.add(), and self.end() are
    called in a sequence. For a refresher of how instance variables (data attributes)
    work in Python, take a look at the Python tutorial section about instance variables
    at [http://mng.bz/AyDQ](http://mng.bz/AyDQ).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Metaflow输出的常规消息外，你还应该看到一行说“在增加之前计数为0”和“最终计数为2”。假设你已经熟悉Python的基础知识，你会注意到当self.start()、self.add()和self.end()按顺序调用时，流程的行为就像任何Python对象一样。为了复习Python中实例变量（数据属性）的工作方式，请查看Python教程中关于实例变量的部分，链接为[http://mng.bz/AyDQ](http://mng.bz/AyDQ)。
- en: 'By design, the syntax of managing state in Metaflow looks like idiomatic, straightforward
    Python: just create instance variables with self as usual. It is equally easy
    to exclude temporary values that are not worth saving: just create ordinary, noninstance
    variables that will be cleaned up after the step function exits.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 按照设计，在Metaflow中管理状态的语言看起来像自然的、直接的Python语法：只需像往常一样使用self创建实例变量。排除那些不值得保存的临时值同样简单：只需创建普通、非实例变量，这些变量将在步骤函数退出后清理。
- en: Rule of thumb Use instance variables, such as self, to store any data and objects
    that may have value outside the step. Use local variables only for intermediary,
    temporary data. When in doubt, use instance variables because they make debugging
    easier.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 经验法则：使用实例变量（如self）来存储任何可能具有步骤之外价值的任何数据和对象。仅使用局部变量来存储中间、临时数据。如果有疑问，请使用实例变量，因为它们使调试更容易。
- en: 'By design, the state management looks almost trivially simple in Metaflow but
    a lot is happening under the hood. Metaflow must address the following two key
    challenges related to data flow:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 按照设计，在Metaflow中状态管理看起来几乎是简单得令人难以置信，但在幕后发生了很多事情。Metaflow必须解决与数据流相关的以下两个关键挑战：
- en: Each task is executed as a separate process, possibly on a separate physical
    computer. We must concretely move state across processes and instances.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个任务都作为一个独立的过程执行，可能是在一个独立的物理计算机上。我们必须具体地在进程和实例之间移动状态。
- en: Runs may fail. We want to understand why they failed, which requires understanding
    of the state of the flow prior to the failure. Also, we may want to restart failed
    steps without having to restart the whole flow from the beginning. All these features
    require us to persist state.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行可能会失败。我们想要了解它们失败的原因，这需要了解失败之前的状态。此外，我们可能想要重新启动失败步骤，而无需从头开始重新启动整个流程。所有这些功能都需要我们持久化状态。
- en: To address these challenges, Metaflow snapshots and stores the state of the
    workflow, as stored in self, after every task. Snapshotting is one of the key
    features of Metaflow that enable many other features, such as resuming workflows
    and executing tasks on disparate compute environments and, in particular, easy
    observability of workflows.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些挑战，Metaflow在每次任务之后都会快照并存储工作流程的状态，存储在self中。快照是Metaflow的关键特性之一，它使许多其他特性成为可能，例如恢复工作流程、在不同的计算环境中执行任务，特别是易于观察工作流程。
- en: 'You can observe instance variables as soon as a task completes, even when a
    run is still executing. You can do this in multiple ways of doing, but an easy
    way is to use the dump command, which works similar to the logs command that we
    used earlier. Just copy and paste a pathspec of a task that you want to observe,
    such as the following example:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在任务完成时立即观察到实例变量，即使运行仍在执行中。你可以通过多种方式做到这一点，但一种简单的方法是使用dump命令，它的工作方式与我们之前使用的logs命令类似。只需复制并粘贴你想要观察的任务的路径规范，例如以下示例：
- en: '[PRE11]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If you used a pathspec corresponding to the end task, like in the previous example,
    you should see a line printed out that says count has a value of 2\. Expectedly,
    the value will be lower for earlier steps. Besides the dump command, you can access
    artifacts programmatically, for example, in a notebook using the Metaflow Client
    API, which we will cover in section 3.3.2.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用了对应于最终任务的路径规范，就像上一个例子中那样，你应该会看到一行输出，显示计数器的值为2。预期地，对于更早的步骤，值会低一些。除了dump命令外，你还可以通过编程方式访问工件，例如，在笔记本中使用Metaflow客户端API，我们将在第3.3.2节中介绍。
- en: This discussion touched on only the basics of artifacts. The next section about
    *parameters* shows how artifacts can be passed into a run from outside the flow.
    The next chapter will go into detail about how to deal with large datasets, which
    sometimes require special treatment. Later, in chapter 6, we will discuss how
    to handle complex objects, such as machine learning models, as artifacts.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这段讨论仅涉及了工件的基础知识。下一节关于*参数*将展示如何将工件从流程外部传递到运行中。下一章将详细介绍如何处理大型数据集，有时可能需要特殊处理。稍后，在第6章中，我们将讨论如何处理复杂对象，例如机器学习模型，作为工件。
- en: How Metaflow’s datastore works
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow的数据存储是如何工作的
- en: You don’t need these technical details to use or operate Metaflow successfully,
    but in case you’re curious, here is how Metaflow’s datastore works under the hood.
    After Metaflow finishes evaluating a task, it inspects what instance variables
    have been created by the user code. All variables are serialized, that is, converted
    to bytes, and stored in a datastore managed by Metaflow. These serialized objects,
    called artifacts, are a key concept in the Metaflow universe.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要这些技术细节就能成功使用或操作Metaflow，但如果你好奇，以下是Metaflow的数据存储在底层是如何工作的。在Metaflow完成评估任务后，它会检查用户代码创建的实例变量。所有变量都被序列化，即转换为字节，并存储在Metaflow管理的数据存储中。这些序列化对象，称为工件，是Metaflow宇宙中的关键概念。
- en: The following figure illustrates how data is moved and persisted in the datastore
    in the case of CounterFlow. After the start step finishes, Metaflow detects the
    count variable. Its value of 0 is serialized to bytes, currently using the Python’s
    built-in pickle library, but this is considered an internal implementation detail
    that is subject to change. Let’s assume the byte sequence corresponding to 0 is
    ab0ef2. These bytes are stored in the datastore (unless they exist there already)
    as an immutable blob, an artifact. After this, internal metadata is updated so
    that the count variable refers to the artifact ab0ef21 at the start step.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了在CounterFlow的情况下，数据如何在数据存储中移动和持久化。在开始步骤完成后，Metaflow检测到计数变量。其值为0被序列化为字节，目前使用Python内置的pickle库，但这被视为一个内部实现细节，可能会更改。假设与0对应的字节序列为ab0ef2。这些字节作为不可变blob（工件）存储在数据存储中（除非它们已经存在）。之后，内部元数据被更新，以便计数变量引用开始步骤的工件ab0ef21。
- en: '![CH03_F08_UN03_Tuulos](../../OEBPS/Images/CH03_F08_UN03_Tuulos.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F08_UN03_Tuulos](../../OEBPS/Images/CH03_F08_UN03_Tuulos.png)'
- en: How a datastore handles artifacts internally
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储如何内部处理工件
- en: When the add step accesses count for the first time, Metaflow fetches it from
    the datastore based on metadata. We know that add gets its values from start because
    of the flow order. The add step increments the value of count, which causes a
    new artifact to be created. It is important to note that we don’t change the previously
    stored value of count, because its historical value at start hasn’t changed. Each
    step has its own set of artifacts. The process is repeated for the end step.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当添加步骤第一次访问计数时，Metaflow根据元数据从数据存储中获取它。我们知道添加步骤从开始处获取其值，因为流程的顺序。添加步骤增加计数的值，这会导致创建一个新的工件。需要注意的是，我们没有更改之前存储的计数值，因为其在开始时的历史值没有改变。每个步骤都有自己的工件集。这个过程会重复进行到结束步骤。
- en: Metaflow’s datastore is organized as a content-addressed storage, bearing conceptual
    similarity to the Git version-control system*.* Internally, artifacts are named
    using the hash of their contents, and hence, only one copy of its unique value
    needs to be stored. In other words, the datastore de-duplicates artifacts automatically.
    This means that disk space is used efficiently, and in most cases, you don’t have
    to worry about creating too many artifacts to conserve space.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow的数据存储组织成一个内容寻址存储，在概念上类似于Git版本控制系统*.* 内部，工件使用其内容的哈希值命名，因此只需要存储其唯一值的单个副本。换句话说，数据存储自动去重工件。这意味着磁盘空间被高效使用，在大多数情况下，你不必担心创建太多工件以节省空间。
- en: What should be a step?
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 应该是什么步骤？
- en: When you are developing a new flow, you may wonder what operations should belong
    to the same step and when it makes sense to split a large step into multiple separate
    steps. Although there isn’t a single right answer, it may be helpful to think
    of steps as *checkpoints*. As discussed previously, artifacts are persisted when
    a step—a task launched by the step, to be precise—finishes. After artifacts have
    been persisted successfully, they become available for inspection, as described
    in section 3.3.2\. Also, you will be available to resume execution at an arbitrary
    step, as described in section 3.3.3\. Therefore, it makes sense to keep steps
    reasonably small in terms of execution time, so if failures happen, you won’t
    lose too much work. Or, if you want to monitor the state of a run in near real
    time, you will need small steps, too.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开发一个新的流程时，你可能想知道哪些操作应该属于同一个步骤，以及何时将大步骤拆分成多个单独的步骤是有意义的。虽然没有唯一的正确答案，但将步骤视为*检查点*可能会有所帮助。如前所述，当步骤（精确地说，是步骤启动的任务）完成时，工件会被持久化。工件成功持久化后，它们将可用于检查，如3.3.2节所述。此外，你将能够在任意步骤恢复执行，如3.3.3节所述。因此，从执行时间方面来看，保持步骤合理小是有意义的，这样如果发生故障，你不会丢失太多工作。或者，如果你想几乎实时地监控运行的状况，你也需要小步骤。
- en: 'On the other hand, persisting artifacts and launching tasks creates some overhead.
    If your steps are way too small, the overhead starts dominating the total execution
    time. This overhead is quite easy to notice, though: you can always merge tiny
    steps back together if it becomes an issue.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，持久化工件和启动任务会产生一些开销。如果你的步骤太小，开销开始主导总执行时间。不过，这种开销很容易注意到：如果你发现它成为问题，你总是可以将微小的步骤合并在一起。
- en: Another consideration is the readability of the code. If you execute
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个考虑因素是代码的可读性。如果你执行
- en: '[PRE12]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: do you find the graph meaningful? It is likely that steps that are too large
    hurt comprehensibility more than steps that are too small.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 你觉得这个图有意义吗？很可能是步骤太大比步骤太小更损害可理解性。
- en: Rule of thumb Structure your workflow in logical steps that are easily explainable
    and understandable. When in doubt, err on the side of small steps. They tend to
    be more easily understandable and debuggable than large steps.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 经验法则：以逻辑步骤结构化你的工作流程，这些步骤应易于解释和理解。如有疑问，应偏向于小步骤。它们通常比大步骤更容易理解和调试。
- en: 3.1.4 Parameters
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.4 参数
- en: In the previous section, we learned how we can pass data downstream in a flow
    using artifacts, that is, by assigning variables to self. But what if you want
    to pass in data to start, that is, set parameters for the flow?
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习了如何使用工件在流程中传递数据，即通过将变量分配给self。但如果你想在开始时传递数据，即设置流程的参数呢？
- en: For instance, imagine that you are experimenting with a new model, and you train
    it with various parameterizations. When you analyze the results of your experiments
    afterward, you should know what parameters were used to train a particular version
    of the model. As a solution, Metaflow provides a special artifact called Parameter,
    which you can use to pass data into a run. Parameter artifacts are tracked like
    any other artifacts, so you can check the parameters assigned to any past run.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你正在尝试一个新的模型，并且用各种参数化对其进行训练。在你分析实验结果之后，你应该知道用于训练模型特定版本的参数是什么。作为解决方案，Metaflow提供了一个名为参数的特殊工件，你可以使用它将数据传递到运行中。参数工件与其他工件一样被跟踪，因此你可以检查分配给任何过去运行的参数。
- en: 'Parameters are a flow-level, that is, a class-level, construct. They are not
    bound to any particular step and are automatically made available to all steps,
    including start. To define a Parameter, you have to specify the following four
    elements:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 参数是一个流程级（即类级）构造。它们不绑定到任何特定的步骤，并且自动对所有步骤（包括开始步骤）可用。要定义一个参数，你必须指定以下四个元素：
- en: Create a Parameter instance at the class level.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在类级别创建一个参数实例。
- en: Assign the parameter to an artifact, for example, animal and count in listing
    3.3.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将参数分配给一个工件，例如，列表3.3中的动物和计数。
- en: Specify the name of the parameter that is shown to the user, for example, creature
    and count, as shown next. The artifact name and the parameter name can be the
    same, but they don’t have to be, as illustrated in listing 3.3.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定用户看到的参数名称，例如，生物和计数，如下所示。工件名称和参数名称可以相同，也可以不同，如列表3.3所示。
- en: Decide on the type of the parameter. By default, parameters are strings. You
    can change the type either by specifying a default value for the parameter, like
    for count as shown in the next code listing, or by explicitly setting the type
    to one of the basic scalar types of Python—str, float, int, or bool—as for ratio
    in listing 3.3.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定参数的类型。默认情况下，参数是字符串。您可以通过指定参数的默认值来更改类型，例如在下一个代码示例中为 count 所示，或者通过显式设置类型为 Python
    的一种基本标量类型——str、float、int 或 bool，如列表 3.3 中的 ratio。
- en: Besides these required elements, Parameter supports a set of optional arguments.
    Typical options include help, which specifies a user-visible help text, and required=True,
    which indicates that the user must supply a value for the parameter. By default,
    all parameters are optional. They receive a value of None if no default is specified
    and the user didn’t provide a value. The following listing shows an example.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些必需元素之外，参数支持一组可选参数。典型的选项包括 help，它指定了用户可见的帮助文本，以及 required=True，它表示用户必须为参数提供一个值。默认情况下，所有参数都是可选的。如果没有指定默认值且用户没有提供值，它们将接收
    None 值。以下列表显示了一个示例。
- en: Listing 3.3 A flow with parameters
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.3 带有参数的工作流
- en: '[PRE13]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Parameters are defined at the class level, outside steps.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 参数在类级别定义，位于步骤之外。
- en: 'Save the code to a file called parameters.py and try to run it as usual:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到名为 parameters.py 的文件中，并尝试像往常一样运行它：
- en: '[PRE14]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This fails with the error, Missing option ''--creature'', because the creature
    parameter has required=True. If this was a real flow, this error would be a good
    reason to check the help for the flow, as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这会因错误“缺少选项 '--creature'”而失败，因为生物参数的 required=True。如果这是一个真实的工作流，这个错误将是一个很好的理由去检查工作流的帮助，如下所示：
- en: '[PRE15]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'A bunch of options are listed. The user-defined parameters are at the top of
    the option list with their help texts shown. Try setting a value for -creature
    as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 列出了许多选项。用户定义的参数位于选项列表的顶部，其帮助文本显示。尝试按以下方式设置 -creature 的值：
- en: '[PRE16]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The flow should run, and you see an output corresponding to the assigned parameter
    values. Note that ratio doesn’t have a default, so it is set to None. Let’s try
    specifying all the values, as shown here:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流应该运行，您会看到与分配的参数值相对应的输出。请注意，比例没有默认值，因此它被设置为 None。让我们尝试指定所有值，如下所示：
- en: '[PRE17]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Notice how count and ratio are converted to the correct Python types automatically.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 注意观察计数和比例是如何自动转换为正确的 Python 类型的。
- en: Note Parameters are constant, immutable values. You can’t change them in your
    code. If you want to mutate a parameter, create a copy of the parameter value,
    and assign it to another artifact.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：参数是常量，不可变值。您不能在代码中更改它们。如果您想修改参数，请创建参数值的副本，并将其分配给另一个工件。
- en: Specifying parameters as environment variables
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 将参数指定为环境变量
- en: If you execute the same run command line frequently, maybe with slight modifications,
    it may get frustrating to specify the same parameters over and over again. For
    convenience, you can specify any options as environment variables, too.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您经常执行相同的运行命令行，可能略有修改，可能需要反复指定相同的参数，这可能会令人沮丧。为了方便，您还可以将任何选项指定为环境变量。
- en: 'To do this, set an environment variable whose name matches the option name,
    prefixed with METAFLOW_RUN_. For instance, we can fix the value of creature for
    parameters.py as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 要这样做，请设置一个与选项名称匹配的环境变量，前面加上 METAFLOW_RUN_ 前缀。例如，我们可以将 creature 的值固定为 parameters.py，如下所示：
- en: '[PRE18]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now you can run ParameterFlow without specifying—creature, because its value
    is specified via an environment variable, as shown next:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以在不指定 -creature 的情况下运行 ParameterFlow，因为它的值是通过环境变量指定的，如下所示：
- en: '[PRE19]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If both an environment variable and a command-line option are set, the latter
    takes precedence, as you can see by executing the following:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如果同时设置了环境变量和命令行选项，则后者具有优先级，正如您可以通过执行以下操作看到的那样：
- en: '[PRE20]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The creature should be set to otter, not dinosaur.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 生物应该设置为水獭，而不是恐龙。
- en: Complex parameters
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂参数
- en: The previous mechanism works for basic scalar parameters that are strings, integers,
    floating-point numbers, or Boolean values. Most basic flows need nothing besides
    these basic types as parameters.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到的机制适用于基本标量参数，这些参数是字符串、整数、浮点数或布尔值。大多数基本工作流除了这些基本类型之外不需要其他参数。
- en: But sometimes, you might need a parameter that is a list or a mapping of some
    sort, or a complex combination of these. A challenge is that because parameters
    are typically defined on the command line as a string, we need a way to define
    nonscalar values as strings so they can be passed in as a Parameter. This is where
    JSON-encoded parameters come in handy.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 但有时，您可能需要一个列表或某种映射的参数，或者这些参数的复杂组合。挑战在于，因为参数通常在命令行上定义为字符串，我们需要一种方法来定义非标量值作为字符串，以便它们可以作为参数传递。这就是JSON编码参数派上用场的地方。
- en: The next listing shows a simple example that accepts a dictionary as a parameter.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表展示了接受字典作为参数的简单示例。
- en: Listing 3.4 A flow with a JSON-typed parameter
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.4 带有JSON类型参数的流程
- en: '[PRE21]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Defines a JSON-typed parameter, imports JSONType, and specifies it as the
    parameter type
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义了一个JSON类型的参数，导入了JSONType，并将其指定为参数类型
- en: 'Save the snippet to a file called json_parameter.py. You can pass in a mapping,
    a dictionary, on the command line as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码片段保存到名为json_parameter.py的文件中。您可以在命令行中传递一个映射，一个字典，如下所示：
- en: '[PRE22]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note that the dictionary is enclosed in single quotes to avoid the special characters
    from confusing the shell.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，字典被单引号包围，以避免特殊字符混淆shell。
- en: 'It is not convenient to specify a large JSON object inline on the command line.
    For large JSON objects, a better approach is to read the value from a file using
    a standard shell expression. If you don’t have a large JSON file for testing handy,
    you can create one—let’s call it myconfig.json—as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令行上内联指定大型JSON对象并不方便。对于大型JSON对象，更好的方法是使用标准的shell表达式从文件中读取值。如果您没有现成的用于测试的大型JSON文件，您可以创建一个——让我们称它为myconfig.json——如下所示：
- en: '[PRE23]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now you can provide this file as a parameter like so:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以像这样提供一个文件作为参数：
- en: '[PRE24]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The shell expression, $(cat myconfig.json), substitutes the value on the command
    line with the contents of a file, myconfig.json. In this case, we must enclose
    the shell expression in double quotes.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: shell表达式，$(cat myconfig.json)，将命令行上的值替换为文件myconfig.json的内容。在这种情况下，我们必须将shell表达式用双引号括起来。
- en: Files as parameters
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 文件作为参数
- en: The mechanisms shown previously allow you to parameterize a run with small values,
    or maybe configuration files, passed on the command line. They are not intended
    to be a mechanism for passing in large amounts of input data.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 之前展示的机制允许您使用命令行传递的小数值或配置文件来参数化一个运行，它们并不是用来传递大量输入数据的机制。
- en: In real-life data science applications, however, it is not always easy to draw
    the line between parameters and input data. Large configuration files may be larger
    than the smallest datasets. Or you may have a medium-size auxiliary dataset that
    feels like a parameter, although the actual input data is provided through a separate
    channel.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实际的数据科学应用中，区分参数和输入数据并不总是容易。大型配置文件可能比最小的数据集还要大。或者，您可能有一个中型辅助数据集，它感觉像是一个参数，尽管实际输入数据是通过另一个通道提供的。
- en: Metaflow provides a special parameter called IncludeFile, which you can use
    to include small or medium-size datasets in a run as an artifact. A typical example
    would be a CSV (comma-separated value) file. There isn’t an exact limit for the
    size of files that can be handled by IncludeFile, but its performance isn’t optimized
    for big data—say, files larger than a gigabyte. Consider it a supersized Parameter,
    as illustrated in figure 3.9, rather than a mechanism for large-scale data processing,
    which will be covered in chapter 7.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow提供了一个特殊的参数名为IncludeFile，您可以使用它将小型或中型数据集作为一个工件包含在运行中。一个典型的例子是一个CSV（逗号分隔值）文件。IncludeFile可以处理文件的大小没有确切限制，但它的性能并没有针对大数据进行优化——比如说，大于一个GB的文件。将其视为一个超大的参数，如图3.9所示，而不是大规模数据处理机制，这将在第7章中介绍。
- en: '![CH03_F09_Tuulos](../../OEBPS/Images/CH03_F09_Tuulos.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F09_Tuulos](../../OEBPS/Images/CH03_F09_Tuulos.png)'
- en: Figure 3.9 Parameters are meant only for small and medium-size datasets.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 参数仅适用于小型和中型数据集。
- en: Let’s take a look at an example in listing 3.5\. *I*t accepts a CSV file as
    a parameter and parses it. The example uses Python’s built-in CSV parser from
    the csv module, so it can handle quoted values and configurable field delimiters.
    You can change the default delimiter, a comma, by specifying the --delimiter option.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看列表3.5中的示例。它接受一个CSV文件作为参数并解析它。该示例使用Python内置的csv模块中的CSV解析器，因此它可以处理引号值和可配置的字段分隔符。您可以通过指定--delimiter选项来更改默认的分隔符，即逗号。
- en: 'To test the flow, you can create a simple CSV file, test.csv, which contains
    any comma-separated values, like here:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试流程，你可以创建一个简单的CSV文件，test.csv，其中包含任何以逗号分隔的值，如下所示：
- en: '[PRE25]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The csv.reader function will take the CSV data as a file object, so we wrap
    our string-valued self.data artifact in a StringIO, which makes it an in-memory
    file object. IncludeFile, is_text=True indicates that the corresponding artifact
    should be returned as a Unicode string instead of a bytes object.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: csv.reader函数将把CSV数据作为文件对象，因此我们将我们的字符串值self.data工件包装在StringIO中，使其成为一个内存中的文件对象。IncludeFile,
    is_text=True表示相应的工件应以Unicode字符串的形式返回，而不是字节对象。
- en: Listing 3.5 A flow that includes a CSV file as a parameter
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.5 包含CSV文件作为参数的流程
- en: '[PRE26]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Save the code to csv_file.py. You can run it as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到csv_file.py。你可以按以下方式运行它：
- en: '[PRE27]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: You should see parsed fields of the CSV file printed out. You might wonder how
    this simple example is different from just opening the CSV file directly in the
    code, for example, by using csv.reader(open('test.csv')). The key difference is
    that IncludeFile reads the file and persists it as an immutable Metaflow artifact,
    attached to the run. Consequently, the input file is snapshotted and versioned
    together with the run, so you can access the original data, even if test.csv is
    changed or lost. This can be very useful for reproducibility, as we will learn
    in chapter 6.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能看到打印出来的CSV文件的解析字段。你可能会想知道这个简单的例子与直接在代码中打开CSV文件有什么不同，例如，使用csv.reader(open('test.csv'))。关键区别在于IncludeFile读取文件并将其持久化为不可变的Metaflow工件，附加到运行中。因此，输入文件与运行一起快照和版本化，这样你就可以访问原始数据，即使test.csv被更改或丢失。这可以非常有用，我们将在第6章中学习。
- en: Now you know how to define sequential workflows that may receive data from the
    outside world through parameters and process it in multiple steps that share state
    via artifacts. In the next section, we will learn how to run many such sequences
    of steps concurrently.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了如何定义可能通过参数从外部世界接收数据并在多个步骤中处理这些数据的顺序工作流程，这些步骤通过工件共享状态。在下一节中，我们将学习如何并行运行这些步骤序列。
- en: 3.2 Branching and merging
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 分支和合并
- en: '*Alex is positively surprised by the fact that defining basic workflows in
    Metaflow isn’t much harder than writing code in notebooks. But is there any benefit
    to writing code this way? At this point, purported benefits of workflows seem
    quite abstract. Alex chats with Bowie over coffee, reminiscing about a project
    that took nine minutes to execute in a notebook. Bowie points out that workflows
    make it easy to execute operations in parallel, which can make processing much
    faster. The idea of getting stuff done faster resonates with Alex—maybe this is
    a killer feature of workflows!*'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '*亚历克斯对在Metaflow中定义基本工作流程并不比在笔记本中编写代码更困难的事实感到非常惊讶。但这样编写代码有什么好处吗？到目前为止，工作流程的所谓好处似乎相当抽象。亚历克斯和鲍伊一边喝咖啡一边聊天，回忆起一个在笔记本中执行需要九分钟的项目。鲍伊指出，工作流程使得并行执行操作变得容易，这可以使处理速度大大加快。快速完成工作的想法与亚历克斯产生了共鸣——这可能是工作流程的一个杀手级特性！*'
- en: '![CH03_F09_UN04_Tuulos](../../OEBPS/Images/CH03_F09_UN04_Tuulos.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F09_UN04_Tuulos](../../OEBPS/Images/CH03_F09_UN04_Tuulos.png)'
- en: Workflows provide an abstraction of concurrency—branching—that allow efficient
    use of parallel compute resources, such as multicore CPUs and distributed compute
    clusters. Although several other paradigms enable parallel computing, many of
    them are notoriously hard to get right, multithreaded programming being a well-known
    example. Workflows are uniquely powerful because they make parallelism accessible
    to nonexpert software developers, including data scientists.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程提供了一个并发（分支）的抽象，允许高效地使用并行计算资源，如多核CPU和分布式计算集群。尽管有几个其他范例能够实现并行计算，但其中许多都因其难以正确实现而闻名，多线程编程就是一个众所周知的例子。工作流程的独特之处在于，它们使得并行性对非专家软件开发者，包括数据科学家，变得可访问。
- en: 'When should one use branches? Let’s start by considering a linear workflow
    that doesn’t have branches. Designing a linear workflow isn’t too hard, typically.
    It is often quite evident that we must do A before B and only after B can C happen.
    The A → B → C order is imposed by the data flow: C needs some data from B that
    needs data from A.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 应该在何时使用分支？让我们先考虑一个没有分支的线性工作流程。设计一个线性工作流程通常并不太难。通常很明显，我们必须先做A，然后才能做B，只有B完成后C才能发生。A
    → B → C的顺序是由数据流强制的：C需要从B那里获取一些数据，而B又需要从A那里获取数据。
- en: Correspondingly, you should use branches whenever the data flow allows it. If
    A produces data that can be used by both B and C, and there’s no other data being
    shared between B and C, then B and C should branch off from A, so they can be
    run concurrently. Figure 3.10 depicts a concrete example of this. To train any
    models, we need to fetch a dataset that is performed by step A. We want to train
    two separate versions of the model using the data produced by step A. There’s
    nothing that step B requires from step C or vice versa, so we should specify them
    as independent branches. After steps B and C complete, we want to choose the best
    model in step D, which obviously requires input from both steps B and C.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 相应地，当数据流允许时，你应该使用分支。如果A产生可以被B和C使用的数据，并且B和C之间没有其他共享数据，那么B和C应该从A分支出来，这样它们可以并行运行。图3.10展示了这个例子。为了训练任何模型，我们需要获取由步骤A执行的数据集。我们希望使用步骤A产生的数据训练两个独立的模型版本。步骤B不需要从步骤C获取任何东西，反之亦然，因此我们应该将它们指定为独立的分支。在步骤B和C完成后，我们希望在步骤D中选择最佳模型，这显然需要来自步骤B和C的输入。
- en: '![CH03_F10_Tuulos](../../OEBPS/Images/CH03_F10_Tuulos.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F10_Tuulos](../../OEBPS/Images/CH03_F10_Tuulos.png)'
- en: Figure 3.10 A basic workflow with two branches
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 基本工作流程，包含两个分支
- en: We could express the DAG of figure 3.10 as a linear DAG, A → B → C → D or A
    → C → B → D, and get exactly the same results. These DAGs would be slower to execute
    because Metaflow wouldn’t be able to run steps B and C in parallel. Besides performance
    benefits, branches can make workflows more readable by highlighting the actual
    data flow and interdependencies between steps. Hence, we recommend the following
    best practice.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将图3.10的DAG表示为线性DAG，A → B → C → D或A → C → B → D，并得到完全相同的结果。这些DAG的执行速度会慢一些，因为Metaflow无法并行运行步骤B和C。除了性能优势外，分支可以通过突出实际的数据流和步骤之间的相互依赖性来使工作流程更易于阅读。因此，我们建议以下最佳实践。
- en: Rule of thumb Whenever you have two or more steps that can be executed independently,
    make them parallel branches. It will make your workflow easier to understand,
    because the reader can see what steps don’t share data just by looking at the
    workflow structure. It will make your workflow faster, too.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 经验法则：每当你有两个或更多可以独立执行的步骤时，将它们做成并行分支。这将使你的工作流程更容易理解，因为读者只需查看工作流程结构就能看到哪些步骤不共享数据。这也会使你的工作流程更快。
- en: You may wonder whether the system could figure out an optimal DAG structure
    automatically. *Automatic parallelization* has been an active research topic in
    computer science for decades, but, alas, it is practically impossible to do this
    using arbitrary, idiomatic Python code. The main obstacle is that often the flow
    code itself does not contain enough information about what can be parallelized,
    because the steps interact with other third-party libraries and services. We have
    found that letting the user stay in control is less confusing than relying on
    a half-baked, error-prone automation. Also, ultimately, workflows are a medium
    of human communication. No automated system can decide the most understandable
    way to describe a business problem to a human being.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道系统是否能够自动确定最优的DAG结构。*自动并行化*在计算机科学领域已经是一个活跃的研究课题几十年了，但遗憾的是，使用任意、惯用的Python代码来做这件事实际上是不可能的。主要障碍是，通常流程代码本身并不包含足够的信息来表明什么可以并行化，因为步骤与其他第三方库和服务交互。我们发现，让用户保持控制比依赖半成品、容易出错的自动化更不容易造成混淆。此外，最终，工作流程是人类沟通的媒介。没有自动化的系统可以决定向人类描述业务问题的最易懂方式。
- en: 3.2.1 Valid DAG structures
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 有效的DAG结构
- en: We call a step that fans out branches, like step A in figure 3.10, a *split
    step*. Correspondingly, we call a step that fans in branches, like step D in figure
    3.10, a *join step*. To keep the data flow easy to understand, Metaflow requires
    that every split step have a corresponding join step. You can think of split as
    the left parenthesis, (, and join as the right one, ). A properly parenthesized
    expression (like this one) needs parentheses on both sides. You can nest splits
    and joins as deeply as needed.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称像图3.10中的步骤A那样分支扩散的步骤为*split步骤*。相应地，我们称像图3.10中的步骤D那样合并分支的步骤为*join步骤*。为了使数据流易于理解，Metaflow要求每个split步骤都有一个相应的join步骤。你可以把split看作左括号(，而join看作右括号)。一个正确括号的表达式（就像这个）需要在两侧都有括号。你可以根据需要将split和join嵌套到任意深度。
- en: Figure 3.11 shows an example of a valid DAG with nested branches. The graph
    has three split steps, shaded light gray. Each split step is matched by a join
    step, shaded dark gray.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11 展示了一个有效 DAG 的示例，其中包含嵌套分支。图中有三个分割步骤，浅灰色阴影。每个分割步骤都有一个对应的连接步骤，深灰色阴影。
- en: '![CH03_F11_Tuulos](../../OEBPS/Images/CH03_F11_Tuulos.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F11_Tuulos](../../OEBPS/Images/CH03_F11_Tuulos.png)'
- en: Figure 3.11 A valid DAG with two levels of nested branches
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11 一个有效 DAG，具有两层嵌套分支
- en: Note that just having the same number of splits and joins isn’t sufficient—there’s
    another rule to follow.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，仅仅有相同数量的分割和连接是不够的——还有另一条规则需要遵循。
- en: Rule A join step can join only steps that have a common split parent.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 规则 A 连接步骤只能连接具有公共分割父步骤的步骤。
- en: Figure 3.12 shows a DAG with two invalid splits, shown in dark and light gray.
    The dark gray split should have a corresponding dark gray join, but here the dark
    gray join tries to join a light gray step, which is not allowed—a join step can
    join only steps from a common split parent. When you plot a valid Metaflow DAG,
    edges (arrows) never have to cross.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.12 展示了一个 DAG，其中包含两个无效的分割，分别用深灰和浅灰色表示。深灰色分割应该有一个对应的深灰色连接，但在这里，深灰色连接试图连接一个浅灰色步骤，这是不允许的——连接步骤只能连接来自公共分割父步骤的步骤。当你绘制一个有效的
    Metaflow DAG 时，边（箭头）永远不需要交叉。
- en: '![CH03_F12_Tuulos](../../OEBPS/Images/CH03_F12_Tuulos.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F12_Tuulos](../../OEBPS/Images/CH03_F12_Tuulos.png)'
- en: Figure 3.12 An invalid DAG, where splits and joins don’t match
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.12 一个无效的 DAG，其中分割和连接不匹配
- en: 'The reason for these rules goes back to the data flow: we need to keep track
    of the lineage of artifacts, which could get very confusing in a graph with crossing
    edges.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这些规则的原因可以追溯到数据流：我们需要跟踪艺术品的血统，这在有交叉边的图中可能会变得非常混乱。
- en: 3.2.2 Static branches
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 静态分支
- en: Earlier, in chapter 2, we introduced the concept of *a static DAG*, that is,
    a DAG whose structure is fully known before execution begins. All examples depicted
    earlier, for instance, the one in figure 3.10, have been static DAGs with static
    branches. In this section, we will show how static branches are defined in Metaflow.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的第 2 章中，我们介绍了 *静态 DAG* 的概念，即执行开始前结构完全已知的 DAG。所有之前展示的示例，例如图 3.10 中的示例，都是具有静态分支的静态
    DAG。在本节中，我们将展示如何在 Metaflow 中定义静态分支。
- en: 'Before we get to the syntax of splits and joins, we need to cover the following
    important topic: in branches, the data flow, that is, the artifacts, will diverge
    by design. When we reach a join step, we must decide what to do with divergent
    values. In other words, we must *merge* artifacts. The question of merging often
    trips up new users of Metaflow, so let’s start with a simple example.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们到达分割和连接的语法之前，我们需要涵盖以下重要主题：在分支中，数据流，即艺术品，将按设计发散。当我们到达连接步骤时，我们必须决定如何处理发散的值。换句话说，我们必须
    *合并* 艺术品。合并的问题经常困扰 Metaflow 的新用户，所以让我们从一个简单的例子开始。
- en: Let’s expand our original CounterFlow example from listing 3.2 by adding another
    branch, as depicted in figure 3.13\. Here, start is our split step. We have one
    branch, add_one, which increments count by one, and another branch, add_two, which
    increments it by two. Now, going in the join step, we have two possible values
    for count, 1 and 2\. We must decide which one is the right value going forward.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过添加另一个分支来扩展 3.2 列表中的原始 CounterFlow 示例，如图 3.13 所示。在这里，start 是我们的分割步骤。我们有一个分支，add_one，它将计数增加一，另一个分支，add_two，它将计数增加两。现在，在连接步骤中，我们有
    count 的两个可能值，1 和 2。我们必须决定哪个是正确的值。
- en: '![CH03_F13_Tuulos](../../OEBPS/Images/CH03_F13_Tuulos.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F13_Tuulos](../../OEBPS/Images/CH03_F13_Tuulos.png)'
- en: Figure 3.13 CounterBranchFlow
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13 CounterBranchFlow
- en: 'In this example, similar to real-life flows, there isn’t an unambiguous right
    or wrong answer for what value count should get in the join step. The right choice
    depends on the application: maybe it should be the maximum of the two, like in
    listing 3.6; maybe it should be the average; or maybe it should be the sum. It
    is up to the user to define how the values should be merged. For instance, considering
    the “choose the best model” step in figure 3.10, the step would iterate through
    models X and Y and choose the one with the highest score.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，与现实生活中流程类似，对于连接步骤中 count 应该得到什么值并没有明确的正确或错误答案。正确的选择取决于应用：可能是两个中的最大值，就像在列表
    3.6 中那样；可能是平均值；或者可能是总和。定义如何合并值取决于用户。例如，考虑图 3.10 中的“选择最佳模型”步骤，该步骤将遍历模型 X 和 Y 并选择得分最高的模型。
- en: 'Although it is easy to see that the question of count needs to be resolved
    in figure 3.13, there’s an additional challenge: Metaflow can’t detect reliably
    which artifacts have been modified, so a join step needs to *decide what to do
    with all artifacts coming upstream*. If you don’t do anything in a join step,
    downstream steps won’t have access to any data prior to the join, except the parameters
    that are constant and, hence, guaranteed to be always available.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在图3.13中很容易看出计数问题需要解决，但还有一个额外的挑战：Metaflow无法可靠地检测哪些工件已被修改，因此需要*决定如何处理所有上游来的工件*。如果你在合并步骤中不做任何事情，下游步骤将无法访问合并之前的数据，除非是常量参数，因此可以保证始终可用。
- en: Rule join steps act as barriers in the data flow. You must explicitly merge
    all artifacts except parameters to let data flow through downstream.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 规则合并步骤在数据流中充当屏障。你必须明确合并所有工件（除了参数），以便数据可以通过下游流动。
- en: In listing 3.6, which corresponds to figure 3.13, we added another artifact,
    creature, to demonstrate this. If the join step doesn’t do anything with creature,
    it won’t be available in the end step, although the branches didn’t modify it
    at all.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表3.6中，对应于图3.13，我们添加了另一个工件，creature，以演示这一点。如果合并步骤不对creature做任何处理，最终步骤中将无法访问它，尽管分支根本未对其进行修改。
- en: 'The syntax for defining a split for a static branch is simple: just list all
    branches as arguments to self.next. join steps take an extra argument that is
    by convention called inputs, which gives you access to artifacts from each inbound
    branch. A join step doesn’t have to be called join—it is recognized as a join
    step solely based on the extra argument.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 定义静态分支的语法很简单：只需将所有分支作为self.next的参数列出。合并步骤接受一个额外的参数，按照惯例称为inputs，它为你提供了访问每个入站分支的工件。合并步骤不必称为join——它仅基于额外的参数被识别为合并步骤。
- en: 'The inputs object allows you to access the branches in the following three
    ways:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 输入对象允许你以下三种方式访问分支：
- en: You can iterate over inputs. It is common to merge artifacts using Python’s
    built-in functions, like min, max, or sum, with a generator expression that loops
    over the inputs. This is how we pick the maximum of counts in the next listing.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以遍历输入。通常使用Python内置函数，如min、max或sum，结合一个遍历输入的生成器表达式来合并工件。这就是我们在下一个列表中选取计数最大值的方式。
- en: With static branches, you can refer to branches by their names, like in the
    print statements in the next listing.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用静态分支，你可以通过名称引用分支，就像在下一个列表中的打印语句中那样。
- en: You can refer to branches by index. It is common to use the first branch, inputs[0],
    to reassign artifacts that are known to be constant across all branches. This
    is how we reassign the creature artifact in the following listing.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以通过索引引用分支。通常使用第一个分支，inputs[0]，来重新分配所有分支中已知为常量的工件。这就是我们在以下列表中重新分配creature工件的方式。
- en: Listing 3.6 A flow with static branches
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.6 带有静态分支的流程
- en: '[PRE28]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ A static branch is defined by giving all outbound steps as arguments to self.next.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过将所有输出步骤作为self.next的参数来定义静态分支。
- en: ❷ A join step is defined by an extra inputs argument to step.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过向步骤添加额外的inputs参数来定义合并步骤。
- en: ❸ We take the maximum of two counts by iterating over inputs.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过遍历输入来取两个计数的最大值。
- en: ❹ We can also print values from specific named branches.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们还可以从特定的命名分支中打印值。
- en: ❺ To reassign unmodified artifacts, we can just refer to the first branch by
    index.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 要重新分配未修改的工件，我们只需通过索引引用第一个分支。
- en: 'Save the code to counter_branch.py. You can run it as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到counter_branch.py。你可以按以下方式运行它：
- en: '[PRE29]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The final count printed should be 2, that is, the maximum of the two branches.
    You can try commenting out the self.creature line in the join step to see what
    happens when not all artifacts required by the downstream steps—end in this case—are
    handled by join. It will crash because self.creature is not found.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 打印的最终计数应该是2，即两个分支中的最大值。你可以尝试在合并步骤中注释掉self.creature行，看看当不是所有下游步骤所需的工件——在本例中是end——都由合并处理时会发生什么。它将崩溃，因为找不到self.creature。
- en: In the logs, notice how the *pid*, the process identifier, is different for
    add_one and add_two. Metaflow executes the two branches as separate processes.
    If your computer has multiple CPU cores, which is almost certain on any modern
    system, the operating system is likely to execute the processes on separate CPU
    cores, so the computation is physically happening in parallel. This means that
    you can get results up to twice as fast compared to running them sequentially.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在日志中，注意 *pid*（进程标识符）在 add_one 和 add_two 中是不同的。Metaflow 将这两个分支作为单独的进程执行。如果你的计算机有多个
    CPU 核心，这在任何现代系统中几乎是肯定的，操作系统可能会在单独的 CPU 核心上执行这些进程，这意味着计算实际上是在并行进行的。这意味着你可以将结果的速度提高两倍，与顺序运行相比。
- en: Merge helper
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 合并辅助函数
- en: 'You may wonder what happens if you have many artifacts. Is it really necessary
    to reassign all of them explicitly? All of them do need to be reassigned, but
    to avoid boilerplate code, Metaflow provides a helper function, merge_artifacts,
    which does most of the grunt work for you. To see it in action, you can replace
    the line that reassigns the constant artifact:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道如果你有很多 artifacts 会发生什么。真的有必要显式地重新分配所有这些 artifacts 吗？它们确实都需要被重新分配，但为了避免样板代码，Metaflow
    提供了一个辅助函数 merge_artifacts，它为你做了大部分的体力活。要看到它的实际效果，你可以替换重新分配常量 artifact 的那一行：
- en: '[PRE30]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'with the following line:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下行：
- en: '[PRE31]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: If you run the flow again, you see that it works equally well with merge_artifacts.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你再次运行流程，你会看到它同样适用于 merge_artifacts。
- en: As you can imagine, merge_artifacts can’t do all the merging for you. It doesn’t
    know that you want to use the maximum of counts, for instance. It relies on you
    first merging all diverged artifacts explicitly, like we did with count in listing
    3.6\. When you call merge_artifacts after all diverged artifacts have been reassigned,
    it will reassign all the remaining, nondivergent artifacts—that is, artifacts
    that have the same value in all branches—for you automatically. It will fail loudly
    if any divergent artifacts remain.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所想，merge_artifacts 不能为你完成所有的合并。它不知道你想要使用计数中的最大值，例如。它依赖于你首先显式地合并所有发散的 artifacts，就像我们在列表
    3.6 中对 count 所做的那样。当你对所有发散的 artifacts 进行重新分配后调用 merge_artifacts，它将自动为你重新分配所有剩余的非发散
    artifacts——即所有分支中具有相同值的 artifacts。如果任何发散的 artifacts 仍然存在，它将大声失败。
- en: Sometimes you may have artifacts that don’t have to be visible downstream, so
    you don’t want to merge them, but they would confuse merge_artifacts. Listing
    3.7 demonstrates such a case. We define an artifact, increment, in the two branches
    with two different values. We consider it as an internal detail to the step, so
    we don’t want to merge it. However, we want to save it in an artifact in case
    we need to debug code later. We can use the exclude option in merge_artifacts
    to list all artifacts that can be safely ignored.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候你可能有一些不需要在下游可见的 artifacts，因此你不想合并它们，但它们可能会让 merge_artifacts 迷惑。列表 3.7 展示了这样一个案例。我们在两个分支中定义了一个名为
    increment 的 artifact，它具有两个不同的值。我们将其视为步骤的内部细节，因此我们不想合并它。然而，我们希望将其保存为 artifact，以防我们以后需要调试代码。我们可以在
    merge_artifacts 中使用 exclude 选项列出所有可以安全忽略的 artifacts。
- en: Listing 3.7 Merging branching using the merge helper
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.7 使用 merge 辅助函数合并分支
- en: '[PRE32]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ The value of increment diverges between the two branches.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ increment 的值在两个分支之间发散。
- en: ❷ We must explicitly ignore the diverged artifact, because we are not handling
    it explicitly.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们必须显式地忽略发散的 artifact，因为我们没有显式地处理它。
- en: 'Save the code to counter_branch_helper.py. You can run it as follows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到 counter_branch_helper.py。你可以按以下方式运行它：
- en: '[PRE33]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The output is the same as in listing 3.6\. You can remove the exclude option
    to see the error that merge_artifacts raises when it faces artifacts with diverged
    values. Besides exclude, merge_artifacts has a few more convenient options that
    you can peruse in the online documentation of Metaflow.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 输出与列表 3.6 中的相同。你可以移除 exclude 选项来查看 merge_artifacts 面对具有发散值的 artifacts 时抛出的错误。除了
    exclude 之外，merge_artifacts 还有一些更方便的选项，你可以在 Metaflow 的在线文档中查阅。
- en: 3.2.3 Dynamic branches
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 动态分支
- en: 'In the previous section, we showed how you can fan out to a predefined list
    of named steps, each performing a different operation. Concurrent operations like
    this are sometimes called *task parallelism*. In contrast, what if you want to
    perform essentially the same operation but with different input data? *Data parallelism*
    like this is extremely common in data science applications. An article written
    by Intel director James Reinders about task parallelism on ZDNet ([http://mng.bz/j2Dz](http://mng.bz/j2Dz))
    describes the two types of parallelism as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们展示了如何将操作扩展到预定义的命名步骤列表中，每个步骤执行不同的操作。这种类型的并发操作有时被称为*任务并行性*。相反，如果你想要执行本质上相同的操作，但使用不同的输入数据呢？这种*数据并行性*在数据科学应用中非常常见。英特尔总监詹姆斯·雷inders在ZDNet上撰写的一篇文章([http://mng.bz/j2Dz](http://mng.bz/j2Dz))描述了这两种类型的并行性如下：
- en: Data parallelism involves running the same task on different components of data,
    whereas task parallelism is distinguished by running many different tasks at the
    same time on the same data.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行性涉及在不同的数据组件上运行相同的任务，而任务并行性则通过在同一数据上同时运行许多不同的任务来区分。
- en: In the context of data science, data parallelism occurs in many contexts, for
    instance, when you train or score models in parallel, process shards of data in
    parallel, or do hyperparameter searches in parallel. In Metaflow, data parallelism
    is expressed via the foreach construct. We call foreach branches *dynamic branches*,
    because the width, or the cardinality, of the branch is determined dynamically
    at the runtime based on data, not in the code as with static branches.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学的背景下，数据并行性出现在许多场景中，例如，当你并行训练或评分模型时，并行处理数据分片，或者并行进行超参数搜索时。在Metaflow中，数据并行性通过foreach构造来表示。我们称foreach分支为*动态分支*，因为分支的宽度或基数是在运行时根据数据动态确定的，而不是像静态分支那样在代码中确定。
- en: Note Whereas static branches are suitable for expressing concurrency in code,
    that is, operations that are always concurrent no matter what data is being processed,
    dynamic branches are suitable for expressing concurrency in data.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：静态分支适合在代码中表达并发性，即无论处理什么数据，操作总是并发的，而动态分支适合在数据中表达并发性。
- en: In figure 3.10, we sketched how a static DAG could be used to build two models,
    X and Y, in parallel. The structure makes sense if training X and Y require substantially
    different code—maybe X is a decision tree and Y is a deep neural network. However,
    this approach wouldn’t make sense if the code in various branches is equal but
    only the data is different. For instance, consider training a decision tree model
    for each country in the world, as depicted in figure 3.14.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3.10中，我们概述了如何使用静态DAG来并行构建两个模型，X和Y。如果训练X和Y需要大量不同的代码——比如X是一个决策树，Y是一个深度神经网络，那么这种结构是有意义的。然而，如果各个分支中的代码相同，但只有数据不同，这种方法就不合理了。例如，考虑为世界上的每个国家训练一个决策树模型，如图3.14所示。
- en: '![CH03_F14_Tuulos](../../OEBPS/Images/CH03_F14_Tuulos.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F14_Tuulos](../../OEBPS/Images/CH03_F14_Tuulos.png)'
- en: Figure 3.14 A workflow with dynamic, data-driven branches
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14 带有动态、数据驱动分支的工作流程
- en: A construct called foreach in Metaflow allows you to run a copy of a step for
    each value of a given list, hence the name *for-each*. Many programming languages,
    including Python, provide a similar function called map. Like map, foreach takes
    a user-defined function (a step in Metaflow) and applies it to each item of the
    given list, saving and returning the result (artifacts in Metaflow).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow中一个称为foreach的构造允许你为给定列表中的每个值运行步骤的一个副本，因此得名*for-each*。许多编程语言，包括Python，都提供了一个类似的功能，称为map。像map一样，foreach接受一个用户定义的函数（Metaflow中的一个步骤）并将其应用于给定列表的每个项目，保存并返回结果（Metaflow中的工件）。
- en: The logistics of splitting and joining work exactly the same way as for static
    branches, except for a slightly different syntax used in the split step. In particular,
    you need to merge artifacts for foreach similarly as with static branches. This
    next listing demonstrates the syntax for foreach.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 分裂和合并的工作方式与静态分支完全相同，只是在分裂步骤中使用了略微不同的语法。特别是，你需要像静态分支一样合并foreach的工件。下面的列表演示了foreach的语法。
- en: Listing 3.8 A flow with a foreach branch
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.8 带有foreach分支的流程
- en: '[PRE34]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ❶ A foreach branch is defined with the foreach keyword that refers to a list.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用foreach关键字定义foreach分支，该关键字引用一个列表。
- en: ❷ self.input points to an item of the foreach list.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ self.input指向foreach列表中的一个项。
- en: A foreach split is defined by calling self.next with a reference to a step as
    usual and a keyword argument foreach, which takes an artifact name, a string,
    as its value. The artifact referred to by foreach should be a Python list. In
    this example, the foreach artifact is called creatures.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: foreach拆分是通过调用self.next并传递一个步骤引用以及关键字参数foreach来定义的，foreach的值是一个字符串，表示工件名称。foreach引用的工件应该是一个Python列表。在这个例子中，foreach工件被称为creatures。
- en: The analyze_creatures step will be called for each item of the list, in this
    case, three times. In the foreach step, you have access to a special attribute
    called self .input, which contains an item from the foreach list that is assigned
    for the currently executing branch. Note that self.input is not available outside
    foreach, so if you want to keep the value, you should assign it to another artifact,
    like we did with self.creature later.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: analyze_creatures步骤将为列表中的每个项目调用，在这种情况下，三次。在foreach步骤中，你可以访问一个特殊属性self.input，它包含分配给当前执行分支的foreach列表中的项目。请注意，self.input在foreach外部不可用，因此如果你想保留该值，你应该将其分配给另一个工件，就像我们后来对self.creature所做的那样。
- en: This example also demonstrates a common pattern of picking a branch that maximizes
    some artifact value, in this case, score. Python’s built-in max function accepts
    an optional key argument, which defines a function that produces a sort key that
    is used to define the maximum. In effect, this is an implementation of arg max
    in Python, which is a very common operation in data science, especially in the
    context of foreaches.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子也演示了选择一个最大化某些工件值的分支的常见模式，在这种情况下，是分数。Python内置的max函数接受一个可选的key参数，它定义了一个产生排序键的函数，该键用于定义最大值。实际上，这是Python中arg
    max的实现，这在数据科学中非常常见，尤其是在foreach的上下文中。
- en: 'Save the code to foreach.py, and run it as follows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到foreach.py，并按以下方式运行：
- en: '[PRE35]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: You can see that three instances of analyze_creatures were running concurrently,
    each getting a different value from the creatures list. Each creature was scored
    based on the length of their name and mouse won.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到有三个analyze_creatures实例正在并发运行，每个实例从creatures列表中获取不同的值。每个生物根据其名称的长度进行评分，老鼠获胜。
- en: This is the first example that shows how a single step spawns multiple tasks.
    In the logs, you can see each task having a unique ID, like analyze_creatures/2
    and analyze _creatures/3, which are used to uniquely identify branches of the
    foreach.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第一个展示单个步骤如何产生多个任务的例子。在日志中，你可以看到每个任务都有一个唯一的ID，如analyze_creatures/2和analyze_creatures/3，这些ID用于唯一标识foreach的分支。
- en: Numerical computing loves dynamic branching
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 数值计算喜欢动态分支
- en: 'The pattern of executing a piece of code for different parts of data in parallel
    and then collecting the results is universal in numerical computing. In the literature,
    the pattern goes by names such as the following:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在数值计算中，并行执行代码的不同数据部分，然后收集结果的模式是通用的。在文献中，这种模式有如下名称：
- en: Bulk synchronous parallel (a concept first introduced in the 1980s)
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量同步并行（一个在20世纪80年代首次提出的概念）
- en: MapReduce (popularized by an open source data processing framework, Hadoop)
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MapReduce（由开源数据处理框架Hadoop普及）
- en: Fork-Join model (e.g., java.util.concurrent.ForkJoinPool in Java)
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分叉-合并模型（例如，Java中的java.util.concurrent.ForkJoinPool）
- en: Parallel map (e.g., in Python’s multiprocessing module)
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行映射（例如，Python的multiprocessing模块中的）
- en: If you are curious, you can use Google to find more details about these concepts.
    They are all similar to the foreach construct in Metaflow.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你好奇，可以使用Google查找这些概念的更多详细信息。它们都与Metaflow中的foreach构造类似。
- en: In a typical data science application, parallelism takes place at many levels.
    For instance, at the application level, you can use Metaflow’s foreach to define
    a workflow that trains a separate model for each country, like in figure 3.14\.
    Then at the low level, close to the metal, models are trained using an ML library
    like TensorFlow that parallelizes matrix calculations over multiple CPU cores
    using a similar pattern internally.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型数据科学应用中，并行化发生在多个级别。例如，在应用级别，你可以使用Metaflow的foreach定义一个工作流程，为每个国家训练一个单独的模型，如图3.14所示。然后在低级别，接近硬件级别，使用类似模式的ML库（如TensorFlow）训练模型，该库在多个CPU核心上并行化矩阵计算。
- en: The philosophy of Metaflow is to focus on the high-level, human-centric concerns
    regarding the overall structure of the application and let off-the-shelf ML libraries
    handle machine-centric optimizations.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow的哲学是专注于应用整体结构的高层次、以人为本的关注点，并让现成的机器学习库处理以机器为中心的优化。
- en: 3.2.4 Controlling concurrency
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.4 控制并发
- en: A single foreach can be used to fan out even tens of thousands of tasks. In
    fact, foreach is a key element of the scalability story of Metaflow, as described
    in chapter 4\. As a side effect, a foreach branch can accidentally launch so many
    concurrent tasks on your laptop that it starts glowing red hot. To make life easier
    for your laptop (or data center), Metaflow provides a mechanism for controlling
    the number of concurrent tasks.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 单个 foreach 可以扩展成数以万计的任务。实际上，foreach 是 Metaflow 可伸缩性故事的关键元素，如第 4 章所述。作为副作用，foreach
    分支可能会意外地在您的笔记本电脑上启动如此多的并发任务，以至于它开始变得非常热。为了使笔记本电脑（或数据中心）的生活更轻松，Metaflow 提供了一种控制并发任务数量的机制。
- en: The concurrency limit doesn’t change the workflow structure in any way—the code
    stays intact. By default, the limit is enforced during execution by Metaflow’s
    built-in *local scheduler*, which takes care of executing the workflow when you
    type run. As discussed in chapter 6, Metaflow supports other schedulers for use
    cases that require higher availability and scalability.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 并发限制不会以任何方式改变工作流程结构——代码保持完整。默认情况下，限制由 Metaflow 内置的 *本地调度器* 在执行时强制执行，该调度器负责在您输入
    run 命令时执行工作流程。如第 6 章所述，Metaflow 支持其他调度器，用于需要更高可用性和可伸缩性的用例。
- en: To see how the concurrency limit works in practice, let’s use the code in listing
    3.9 as an example. It shows a flow that includes a foreach over a list with a
    thousand items.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解并发限制在实际中的工作方式，让我们以列表 3.9 中的代码为例。它显示了一个包含一千个项目的列表的 foreach 流。
- en: Listing 3.9 A flow with a 1000-way foreach
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.9 包含 1000 方 foreach 的流程
- en: '[PRE36]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Save the code to wide_foreach.py. Try running it as follows:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到 wide_foreach.py。尝试以下方式运行它：
- en: '[PRE37]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This should fail with an error message about start spawning too many child tasks.
    Because defining foreaches is so easy in Metaflow, you can inadvertently use a
    very large list in foreach, possibly including millions of items. Running a flow
    like this would take a good while to execute.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会因错误信息“启动子任务过多”而失败。由于在 Metaflow 中定义 foreach 非常容易，您可能会无意中使用非常大的列表，可能包括数百万个项目。运行这样的流程将需要很长时间才能执行。
- en: 'To prevent silly and possibly expensive mistakes from happening, Metaflow has
    a safeguard for the maximum size of a foreach, by default 100\. You can raise
    the limit with the --max-num-splits option, like here:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止发生愚蠢且可能昂贵的错误，Metaflow 默认对 foreach 的最大大小进行了保护，为 100。您可以使用 --max-num-splits
    选项提高限制，如下所示：
- en: '[PRE38]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'If you run wide foreaches all the time, it might be easier to set an environment
    variable, like so:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您总是运行宽 foreach，设置一个环境变量可能更容易，如下所示：
- en: '[PRE39]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Theoretically, all 1,000 tasks of the foreach in listing 3.9 could be run concurrently.
    However, remember that each task becomes a process of its own, so it is likely
    that your operating system wouldn’t be too happy managing a thousand active processes
    running concurrently. Besides, it wouldn’t make things any faster because your
    computer doesn’t have 1,000 CPU cores, so most of the time tasks would just idle
    in the operating system’s execution queue.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论上讲，列表 3.9 中的 foreach 的所有 1,000 个任务都可以并发运行。然而，请记住，每个任务都成为它自己的进程，因此您的操作系统可能不会太高兴管理成千上万的并发活动进程。此外，这也不会使事情变得更快，因为您的计算机没有
    1,000 个 CPU 核心，所以大多数时候任务都会在操作系统的执行队列中闲置。
- en: 'To power the prototyping loop, the run command in particular, Metaflow includes
    a built-in workflow scheduler, similar to those listed in section 2.2.3\. Metaflow
    refers to this scheduler as *the local scheduler*, distinguishing it from other
    schedulers that can orchestrate Metaflow flows, as we will learn in chapter 6\.
    The local scheduler behaves like a proper workflow scheduler: it processes steps
    in the flow order, translating steps to tasks and executing tasks as processes.
    Importantly, it can control how many tasks are executed concurrently.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持原型设计循环，特别是 run 命令，Metaflow 包含一个内置的工作流程调度器，类似于第 2.2.3 节中列出的那些。Metaflow 将此调度器称为
    *本地调度器*，以区别于我们将在第 6 章中学习的其他可以编排 Metaflow 流的调度器。本地调度器的行为就像一个合适的工作流程调度器：它按流程顺序处理步骤，将步骤转换为任务，并将任务作为进程执行。重要的是，它可以控制同时执行的任务数量。
- en: You can use the --max-workers option to control the maximum number of concurrent
    processes launched by the scheduler. By default, the maximum is 16\. For local
    runs, like the ones we have been executing thus far, there isn’t much benefit
    in setting the value higher than the number of CPU cores in your development environment.
    Sometimes you may want to lower the value to conserve resources on your computer.
    For instance, if a task requires 4 GB of memory, you can’t run 16 of them concurrently
    unless your computer has at least 16*4=64 GB of memory available.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`--max-workers`选项来控制调度器启动的最大并发进程数。默认情况下，最大值为16。对于本地运行，例如我们迄今为止所执行的那些，将值设置得高于您开发环境中的CPU核心数并不会带来太多好处。有时您可能希望降低该值以节省计算机资源。例如，如果一项任务需要4GB的内存，除非您的计算机至少有16*4=64GB的可用内存，否则您无法同时运行16个这样的任务。
- en: You can experiment with different values of max-workers. For instance, compare
    the execution times
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以尝试不同的`max-workers`值。例如，比较执行时间
- en: '[PRE40]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: versus
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 与之相比
- en: '[PRE41]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We will learn more about the effects of max-workers in chapter 5\. Figure 3.15
    summarizes the two options.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第5章中了解更多关于`max-workers`的影响。图3.15总结了这两个选项。
- en: '![CH03_F15_Tuulos](../../OEBPS/Images/CH03_F15_Tuulos.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F15_Tuulos](../../OEBPS/Images/CH03_F15_Tuulos.png)'
- en: Figure 3.15 The effect of max-num-splits and max-workers
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.15 max-num-splits和max-workers的影响
- en: The step A is a foreach split that spawns eight tasks of step B. In this case,
    if you specified any value lower than 8 for --max-num-splits, the run would crash,
    because the option controls the maximum width of the foreach branch. All eight
    tasks will be run regardless of the value of --max-workers, because it controls
    only concurrency. Here, setting --max-workers=2 informs the local scheduler to
    run at most two tasks concurrently, so the eight tasks will be executed as four
    mini-batches.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤A是一个`foreach`拆分，它生成步骤B的八个任务。在这种情况下，如果您为`--max-num-splits`指定了低于8的任何值，运行将崩溃，因为该选项控制了`foreach`分支的最大宽度。无论`--max-workers`的值是多少，都会运行所有八个任务，因为它只控制并发性。在这里，设置`--max-workers=2`通知本地调度器最多同时运行两个任务，因此八个任务将作为四个小批量执行。
- en: Congratulations! You are now able to define arbitrary workflows in Metaflow,
    manage data flow through branches, and execute even large-scale test cases on
    your laptop without melting it! With this foundation, we can proceed to building
    our first real-life data science application.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您现在能够在Metaflow中定义任意工作流程，通过分支管理数据流，并在您的笔记本电脑上执行甚至大规模的测试案例而不会过热！有了这个基础，我们可以继续构建我们的第一个真实数据科学应用。
- en: 3.3 Metaflow in Action
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 Metaflow实践
- en: '*As the first actual data science project, Harper suggests that Alex could
    build an application that predicts the type of a promotional cupcake that a new
    customer is most likely to enjoy, given a set of known attributes about the customer.
    Luckily, thus far customers have manually picked their favorites, so their past
    choices can be used as labels in the training set. Alex recognizes that this is
    a simple classification task, which shouldn’t be too hard to implement using an
    off-the-shelf machine learning library. Alex starts developing a prototype using
    Metaflow.*'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '*作为第一个实际数据科学项目，Harper建议Alex可以构建一个应用，预测新客户最可能喜欢的促销纸杯蛋糕的类型，给定客户的一些已知属性。幸运的是，到目前为止，客户已经手动选择了他们最喜欢的，因此他们的过去选择可以用作训练集中的标签。Alex认识到这是一个简单的分类任务，使用现成的机器学习库实现应该不会太难。Alex开始使用Metaflow开发原型。*'
- en: '![CH03_F15_UN05_Tuulos](../../OEBPS/Images/CH03_F15_UN05_Tuulos.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F15_UN05_Tuulos](../../OEBPS/Images/CH03_F15_UN05_Tuulos.png)'
- en: Now that we have covered the basics of Metaflow, we can build a simple but functional
    data science application. The application creates a dataset for training and testing,
    trains two different classifier models, and chooses the best performing one. The
    workflow resembles the one shown in figure 3.10.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了Metaflow的基础知识，我们可以构建一个简单但功能性的数据科学应用。该应用创建用于训练和测试的数据集，训练两个不同的分类器模型，并选择表现最好的一个。工作流程类似于图3.10中所示。
- en: Don’t worry if you are not an expert in machine learning. The application, like
    all the other examples in this book, demonstrates the development experience and
    infrastructure for data science, not modeling techniques. If you are curious about
    the modeling side of things, you can learn more in the tutorial for *Scikit-Learn*
    ([https://scikit-learn.org/stable/tutorial/](https://scikit-learn.org/stable/tutorial/index.xhtml)),
    which this example is based on.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不是机器学习专家，请不要担心。这个应用，就像这本书中的所有其他示例一样，展示了数据科学的发展经验和基础设施，而不是建模技术。如果你对建模方面感兴趣，可以在*Scikit-Learn*教程中了解更多信息（[https://scikit-learn.org/stable/tutorial/](https://scikit-learn.org/stable/tutorial/index.xhtml)），这个示例就是基于这个教程的。
- en: We will build the example incrementally through multiple iterations, as we would
    if we were prototyping a real-life application. This is a good opportunity to
    put the workstation set up with an IDE, notebooks, and a cloud instance outlined
    in chapter 2 into action, too.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过多次迭代逐步构建示例，就像我们原型化一个真实应用一样。这是一个将第2章中概述的工作站设置（IDE、笔记本和云实例）付诸实践的好机会。
- en: 3.3.1 Starting a new project
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 启动新项目
- en: 'Starting a new project from scratch can feel overwhelming: You are staring
    at a blank file in the editor, not knowing how and where to begin. Thinking about
    the project in terms of a workflow can help. We might have only a faint idea of
    what needs to be done, but at least we know that the workflow will have a start
    step in the beginning and an end step in the end.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 从零开始启动一个新项目可能会感到不知所措：你在编辑器中面对一个空白的文件，不知道从何开始。从工作流程的角度思考项目可能会有所帮助。我们可能只有模糊的想法需要做什么，但至少我们知道工作流程在开始时会有一个起始步骤，在结束时会有一个结束步骤。
- en: We know that the workflow will need to get some input data, and it will need
    to write the results somewhere. We know that some processing will need to happen
    between start and end, which we can figure out iteratively. Figure 3.16 depicts
    the approach as a spiral.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道工作流程将需要一些输入数据，并且它需要将结果写入某个地方。我们知道在开始和结束之间需要发生一些处理，我们可以迭代地找出这些处理。图3.16将这种方法描绘为一个螺旋。
- en: '![CH03_F16_Tuulos](../../OEBPS/Images/CH03_F16_Tuulos.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F16_Tuulos](../../OEBPS/Images/CH03_F16_Tuulos.png)'
- en: Figure 3.16 The spiral recipe for starting a new project
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16 新项目启动的螺旋食谱
- en: 'Follow the path indicated by the black arrows when starting a new project:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动新项目时，遵循黑色箭头指示的路径：
- en: What is the business problem we are trying to solve?
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们试图解决的业务问题是什么？
- en: What input data can we use? How and where can we read it?
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用哪些输入数据？我们如何以及在哪里读取它？
- en: What should be the output data? How and where should we write it?
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出数据应该是什么？我们应该如何以及在哪里写入它？
- en: What techniques can we use to produce a better output based on the input?
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用哪些技术来根据输入产生更好的输出？
- en: 'The arrows show the workflow order. You can see that we build the workflow
    from the outside in. This spiral approach is useful for the following reasons,
    which have been proven true over many data science projects:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 箭头显示了工作流程的顺序。你可以看到我们是从外向内构建工作流程的。这种螺旋方法对以下原因很有用，这些原因在许多数据科学项目中已被证明是正确的：
- en: It is easy to overlook details of the actual problem we are trying to solve,
    especially when dealing with a new and exciting model. The problem should dictate
    the solution, not the other way around.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 容易忽视我们试图解决的实际问题的细节，尤其是在处理一个新而令人兴奋的模型时。问题应该决定解决方案，而不是反过来。
- en: Discovering, verifying, cleaning, and transforming suitable input data is often
    harder than expected. Better to start the process as early as possible.
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发现、验证、清理和转换合适的输入数据通常比预期的更困难。最好尽早开始这个过程。
- en: Integrating the results to the surrounding business systems can also be harder
    than expected—better to start early. Also, there may be surprising requirements
    for the outputs, which may inform the modeling approach.
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果集成到周围业务系统中可能比预期的更困难——最好尽早开始。此外，输出可能存在令人惊讶的要求，这可能会影响建模方法。
- en: Start with the simplest possible modeling approach and get an end-to-end workflow
    working with it. After a basic application works, we can measure results in the
    real business context using real data, which makes it possible to start improving
    the model rigorously. If the project is successful, this step never ends—there
    are always ways to improve the model.
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从最简单的建模方法开始，并使其与端到端工作流程一起工作。当基本应用工作后，我们可以使用真实数据在真实业务环境中测量结果，这使得我们可以开始严格地改进模型。如果项目成功，这一步永远不会结束——总有方法可以改进模型。
- en: Infrastructure should make it easy to follow the spiral recipe. *I*t should
    support iterative development out of the box. Let’s see how the recipe works in
    practice with Metaflow.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施应使遵循螺旋食谱变得容易。*I* 应支持开箱即用的迭代开发。让我们看看Metaflow在实际中的工作方式。
- en: Project skeleton
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 项目骨架
- en: Our toy business problem is to classify wine—not exactly cupcakes, but close
    enough. Conveniently, a suitable dataset is packaged with Scikit-Learn. The example
    will contain nothing specific to this dataset, so you can use the same template
    to test other datasets, too.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的玩具业务问题是对葡萄酒进行分类——并不完全是纸杯蛋糕，但足够接近。方便的是，Scikit-Learn附带了一个合适的数据集。示例将不包含特定于该数据集的内容，因此您也可以使用相同的模板来测试其他数据集。
- en: 'We start by installing the Scikit-Learn package as follows:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先按照以下方式安装Scikit-Learn包：
- en: '[PRE42]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: We will learn more sophisticated ways of handling dependencies in chapter 6,
    but installing the package system-wide works for now. Following the spiral, we
    start with a simple skeleton version of the flow that loads only the input data.
    We will add more functionality in this flow later, as depicted in figure 3.17\.
    The corresponding code is shown in listing 3.10.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第6章学习更复杂的处理依赖关系的方法，但目前在系统范围内安装包是可行的。遵循螺旋食谱，我们首先从只加载输入数据的简单骨架版本开始。我们将在该流程中添加更多功能，如图3.17所示。相应的代码在3.10列表中给出。
- en: '![CH03_F17_Tuulos](../../OEBPS/Images/CH03_F17_Tuulos.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F17_Tuulos](../../OEBPS/Images/CH03_F17_Tuulos.png)'
- en: Figure 3.17 The first iteration of ClassifierTrainFlow
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17 ClassifierTrainFlow的第一迭代
- en: Listing 3.10 First iteration of ClassifierTrainFlow
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.10 ClassifierTrainFlow的第一迭代
- en: '[PRE43]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: ❶ Does imports inside the step code, not at the top of the file
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在步骤代码内部进行导入，而不是在文件顶部
- en: ❷ Loads the dataset
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 加载数据集
- en: ❸ Splits the dataset into a test set containing 20% of the rows and a training
    set with the rest
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将数据集分为包含20%行的测试集和包含其余行的训练集
- en: ❹ Dummy placeholder for the actual model
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 实际模型的占位符
- en: We use functions from Scikit-Learn to load the dataset (load_wine) and split
    it into a train and test sets (train_test_split). You can look up the functions
    online for more information, but it is not essential for this example.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Scikit-Learn中的函数来加载数据集（load_wine）并将其拆分为训练集和测试集（train_test_split）。您可以在网上查找这些函数以获取更多信息，但这对于本例不是必需的。
- en: Note It is considered a good practice in Metaflow to have import statements
    inside the steps that use the modules and not at the top of the file. This way
    imports are executed only when needed.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在Metaflow中，将导入语句放在使用模块的步骤内部，而不是在文件顶部，被认为是一种良好的实践。这样，导入仅在需要时执行。
- en: 'Save the code to classifier_train_v1.py and run it, as shown here:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到classifier_train_v1.py，并按以下所示运行：
- en: '[PRE44]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The code should execute successfully. To confirm that some data has been loaded,
    you can execute a command like the following:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 代码应成功执行。为了确认已加载数据，您可以执行以下类似命令：
- en: '[PRE45]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Replace the pathspec with a real one that you can copy and paste from the output
    of the previous command. The command should show that some artifacts have been
    created, but they are too large to be shown. This is promising—some data was fetched—but
    it would be nice to actually see the data. We will learn how to do this next.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 将路径规范替换为可以从上一个命令的输出中复制和粘贴的实际路径。该命令应显示已创建一些工件，但它们太大而无法显示。这是一个好兆头——已经获取了一些数据，但实际看到数据会更好。我们将在下一节学习如何做到这一点。
- en: 3.3.2 Accessing results with the Client API
  id: totrans-387
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 使用Client API访问结果
- en: In section 3.1.2, we learned that Metaflow persists all instance variables,
    such as train_data and test_data in listing 3.10, as artifacts in its own datastore.
    After artifacts have been stored, you can read them programmatically using the
    Metaflow Client, or the Client API. The Client API is the key mechanism that allows
    you to inspect results and use them across flows.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在3.1.2节中，我们了解到Metaflow将所有实例变量，如3.10列表中的train_data和test_data，作为其自己的数据存储中的工件持久化。工件存储后，您可以使用Metaflow
    Client或Client API以编程方式读取它们。Client API是允许您检查结果并在流程中使用它们的键机制。
- en: Note You can use the Client API to read artifacts. Artifacts can’t be mutated
    after they have been created by a run.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您可以使用Client API读取工件。工件创建后不能被修改。
- en: The Client API exposes a hierarchy of containers, which can be used to refer
    to different parts of a run, that is, an execution of a flow. Besides a single
    run, the containers allow you to navigate the whole Metaflow universe, including
    all the runs by you and your colleagues, assuming you use a shared metadata server
    (more about this in chapter 6). The container hierarchy is depicted in figure
    3.18.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端API公开了一个容器层次结构，可以用来引用运行的不同部分，即流程的执行。除了单个运行之外，容器还允许你导航整个Metaflow宇宙，包括你和你同事的所有运行，前提是你使用了一个共享元数据服务器（关于这一点，请参阅第6章）。容器层次结构在图3.18中展示。
- en: '![CH03_F18_Tuulos](../../OEBPS/Images/CH03_F18_Tuulos.png)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F18_Tuulos](../../OEBPS/Images/CH03_F18_Tuulos.png)'
- en: Figure 3.18 The container hierarchy of the Client API
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18 客户端API的容器层次结构
- en: 'What you can find inside each container follows:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 每个容器内部可以找到的内容如下：
- en: '*Metaflow*—Contains all flows. You can use it to discover flows created by
    you and your colleagues.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Metaflow*—包含所有流程。你可以用它来发现你和你同事创建的流程。'
- en: '*Flow*—Contains all runs that have been executed with a FlowSpec class.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*流程*—包含使用FlowSpec类执行的所有运行。'
- en: '*Run*—Contains all steps of a flow whose execution was started during this
    run. Run is the core concept of the hierarchy, because all other objects are produced
    through runs.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*运行*—包含在此运行期间启动执行的流程的所有步骤。运行是层次结构中的核心概念，因为所有其他对象都是通过运行产生的。'
- en: '*Step*—Contains all tasks that were started by this step. Only foreach steps
    contain more than one task.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤*—包含由此步骤启动的所有任务。只有foreach步骤包含多个任务。'
- en: '*Task*—Contains all data artifacts produced by this task.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*任务*—包含此任务产生的所有数据工件。'
- en: '*Data artifact*—Contains a piece of data produced by task.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据工件*—包含由任务产生的一份数据。'
- en: Besides acting as containers, the objects contain other metadata, such as the
    time of creation and tags. Notably, you can also access logs through the Task
    object.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 除了作为容器外，这些对象还包含其他元数据，例如创建时间和标签。值得注意的是，你还可以通过Task对象访问日志。
- en: 'You can instantiate a Client API object in the following three ways:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下三种方式实例化客户端API对象：
- en: You can instantiate any object directly with a pathspec that uniquely identifies
    the object in the hierarchy. For instance, you can access the data of a partic-ular
    run with Run(pathspec), for example, Run("ClassifierTrainFlow/1611541088765447").
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以直接使用唯一标识对象层次结构中的对象的路径规范来实例化任何对象。例如，你可以使用Run(pathspec)访问特定运行的 数据，例如，Run("ClassifierTrainFlow/1611541088765447")。
- en: You can access a child object with the bracket notation. For example, Run ("ClassifierTrainFlow/1611541088765447")['start']
    returns the start step.
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用括号符号来访问子对象。例如，Run ("ClassifierTrainFlow/1611541088765447")['start'] 返回起始步骤。
- en: You can iterate over any container to access its children. For instance, list
    (Run("ClassifierTrainFlow/1611541088765447")) returns a list of all Step objects
    that correspond to the given Run.
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以迭代任何容器来访问其子对象。例如，list (Run("ClassifierTrainFlow/1611541088765447")) 返回与给定运行对应的所有Step对象的列表。
- en: In addition, the Client API contains a number of handy shortcuts for navigating
    the hierarchy, which we will cover in the coming examples.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，客户端API包含了一些方便的快捷方式来导航层次结构，我们将在接下来的示例中介绍。
- en: Inspecting results in a notebook
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本中检查结果
- en: 'You can use the Client API anywhere Python is supported: in a script, with
    an interactive Python interpreter (just execute python to open one), or in a notebook.
    Notebooks are a particularly handy environment for inspecting results because
    they support rich visualizations.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在Python支持的所有地方使用客户端API：在脚本中、使用交互式Python解释器（只需执行python即可打开一个），或者在笔记本中。笔记本是一个特别方便的环境，用于检查结果，因为它们支持丰富的可视化。
- en: Let’s inspect the results of ClassifierTrainFlow from listing 3.10 in a notebook.
    First, open a notebook either in your editor or by executing jupyter-notebook
    on the command line in the same working directory where you executed your Metaflow
    runs.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在笔记本中检查列表3.10中ClassifierTrainFlow的结果。首先，在你的编辑器中打开一个笔记本，或者通过在执行Metaflow运行的同个工作目录中执行命令行上的jupyter-notebook来打开一个笔记本。
- en: We can use the notebook to inspect the data we loaded earlier. Specifically,
    we want to inspect an artifact called train_data that was created at the start
    step. To do this, copy the lines from the next listing in a notebook cell.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用笔记本来检查我们之前加载的数据。具体来说，我们想要检查一个名为train_data的工件，它在起始步骤处创建。为此，将下一列表中的行复制到笔记本中的一个单元格中。
- en: Listing 3.11 Inspecting data in a notebook
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.11 在笔记本中检查数据
- en: '[PRE46]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Using the Client API is all about navigating the object hierarchy shown in figure
    3.18\. Flow.latest_run is a shortcut that gives the latest Run of the given Flow.
    We use ['start'] to access the desired Step, then the .task shortcut to get the
    corresponding Task object, and .data to take a peek at the given artifact. The
    result should look something like figure 3.19.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Client API 实际上是关于导航图 3.18 所示的对象层次结构。Flow.latest_run 是一个快捷方式，它给出了给定 Flow 的最新运行。我们使用
    ['start'] 来访问所需的步骤，然后使用 .task 快捷方式来获取相应的任务对象，并使用 .data 来查看给定的工件。结果应该类似于图 3.19。
- en: '![CH03_F19_Tuulos](../../OEBPS/Images/CH03_F19_Tuulos.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F19_Tuulos](../../OEBPS/Images/CH03_F19_Tuulos.png)'
- en: Figure 3.19 Using the Client API in a notebook
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.19 在笔记本中使用 Client API
- en: 'The Client API is meant for easy exploration of data. Here are some exercises
    you can try by yourself:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: Client API 旨在方便地探索数据。以下是一些你可以自己尝试的练习：
- en: Try inspecting other artifacts created by the run, for example, train_labels
    or model.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试检查由运行创建的其他工件，例如，train_labels 或 model。
- en: Run the flow again. Note how .latest_run returns a different Run ID. Now try
    to inspect a previous run.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 再次运行流程。注意 `.latest_run` 返回了一个不同的运行 ID。现在尝试检查之前的运行。
- en: 'Try exploring other attributes of the objects, for example, .created_at. Hint:
    You can use help() to see documentation—try help(run).'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试探索对象的其它属性，例如，.created_at。提示：你可以使用 help() 来查看文档——尝试 help(run)。
- en: Accessing data across flows
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在流程间访问数据
- en: Figure 3.20 shows how our project has progressed through the first two steps,
    from the business problem definition to setting up the input data in ClassifierTrainFlow.
    Now that we have confirmed that the input data has been loaded correctly, we can
    proceed to the next step in our spiral, namely, outputs of the project. We would
    like to use a model, to be trained by ClassifierTrainFlow, to classify unseen
    data points, in this case, wines.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.20 展示了我们的项目如何通过前两个步骤进行进展，从业务问题定义到在 ClassifierTrainFlow 中设置输入数据。现在我们已经确认输入数据已正确加载，我们可以继续螺旋式流程的下一步，即项目的输出。我们希望使用一个由
    ClassifierTrainFlow 训练的模型来分类未见过的数据点，在这种情况下，是葡萄酒。
- en: '![CH03_F20_Tuulos](../../OEBPS/Images/CH03_F20_Tuulos.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F20_Tuulos](../../OEBPS/Images/CH03_F20_Tuulos.png)'
- en: Figure 3.20 Focusing on the outputs of the project next
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.20 接下来关注项目的输出
- en: 'It is common to split a predictive application like this into two flows: a
    flow that trains a model and another one that uses the model to provide predictions
    for unseen data. The split is useful because the prediction or inference flow
    is often run independently and more frequently than the training flow. For instance,
    we may train a new model once a day but predict new data once an hour.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 将预测应用如此类分成两个流程是很常见的：一个用于训练模型，另一个用于使用模型为未见过的数据提供预测。这种分割是有用的，因为预测或推理流程通常独立运行，并且比训练流程运行得更频繁。例如，我们可能每天训练一个新的模型，但每小时预测新的数据一次。
- en: Let’s prototype a prediction flow, ClassifierPredictFlow, to accompany our training
    flow ClassifierTrainFlow. The key idea is to access a previously trained model,
    which we can do using the Client API. For this example, we accept a data point
    to be classified as a numerical vector specified as a JSON-typed Parameter (see
    listing 3.4 for a reminder of how it works). As an exercise, you can replace this
    with a CSV file of data points (see listing 3.5 for an example), which would be
    a more realistic approach. The first iteration of the flow is shown in the following
    code listing.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们原型化一个预测流程，ClassifierPredictFlow，以配合我们的训练流程 ClassifierTrainFlow。关键思想是访问之前训练好的模型，我们可以通过
    Client API 来实现。在这个例子中，我们接受一个要分类的数据点，它被指定为一个 JSON 类型的参数（有关其工作方式的提醒，请参阅 3.4 节的列表）。作为一个练习，你可以用数据点的
    CSV 文件替换它（请参阅 3.5 节的示例），这将是一个更现实的方法。流程的第一个迭代如下所示。
- en: Listing 3.12 First iteration of ClassifierPredictFlow
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.12 ClassifierPredictFlow 的第一次迭代
- en: '[PRE47]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: ❶ Finds the latest training run using the Client API
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 Client API 查找最新的训练运行
- en: ❷ Saves the pathspec of the training run for lineage tracking
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 保存训练运行的路径规范以进行血缘跟踪
- en: ❸ Obtains the actual model object
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取实际的模型对象
- en: 'Save the code to classifier_predict_v1.py, and run it as follows:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到 classifier_predict_v1.py，并按以下方式运行：
- en: '[PRE48]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The run should report the model as “nothingburger” as specified in our project
    skeleton in listing 3.10\. This is the spiral approach in action: we establish
    and verify connections between all parts of the end-to-end application before
    worrying about the actual model.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 运行应该报告模型为“nothingburger”，正如我们在列表3.10中指定的项目骨架所示。这是螺旋方法的实际应用：我们在担心实际模型之前，先建立和验证端到端应用所有部分的连接。
- en: 'Note how we persist an artifact, train_run_id, that includes the pathspec of
    the training run. We can use this artifact to keep track of *model lineage*: if
    predictions contain surprises, we can track down the exact training run that produced
    the model that produced the results.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们持久化了一个包含训练运行路径spec的artifact，train_run_id。我们可以使用这个artifact来跟踪*模型血缘*：如果预测结果有意外，我们可以追踪到产生结果的模型的精确训练运行。
- en: 3.3.3 Debugging failures
  id: totrans-434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.3 调试失败
- en: 'Now that we have a skeleton flow both for the input and the output of the project,
    we get to the fun part: defining a machine learning model. As it commonly happens
    in real-world projects, the first version of the model won’t work. We will get
    to practice how to debug failures with Metaflow.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了项目的输入和输出的骨架流程，我们来到了有趣的部分：定义机器学习模型。正如在现实世界的项目中常见的那样，模型的第一个版本可能不会工作。我们将通过Metaflow练习如何调试失败。
- en: Another common characteristic of a data science project is that initially we
    are not sure what kind of model would work best for the given data. Maybe we should
    train two different types of models and choose the one that performs the best,
    as we discussed in the context of figure 3.10.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学项目的另一个常见特征是，最初我们不确定哪种类型的模型最适合给定的数据。也许我们应该训练两种不同类型的模型，并选择表现最好的那个，正如我们在图3.10的上下文中讨论的那样。
- en: Inspired by the Scikit-Learn tutorial, we train a K-nearest neighbor (KNN) classifier
    and a support vector machine (SVM). Don’t worry if these techniques are not familiar
    to you—knowing them is not essential for this example. You can refer to the Scikit-Learn
    tutorial for more information about the models.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 受Scikit-Learn教程的启发，我们训练了一个K最近邻（KNN）分类器和支持向量机（SVM）。如果你对这些技术不熟悉，不用担心——了解它们对这个例子不是必需的。你可以参考Scikit-Learn教程了解更多关于模型的信息。
- en: 'Training a model is often the most time-consuming part of a flow, so it makes
    sense to train models in parallel steps to speed up execution. The next listing
    expands the earlier ClassifierTrainFlow skeleton from listing 3.10 by adding three
    new steps in the middle: train_knn and train_svm, which are parallel branches,
    and choose_ model, which chooses the best performing model of the two.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型通常是流程中最耗时的部分，因此并行步骤训练模型以加快执行速度是有意义的。接下来的列表通过在列表3.10中的ClassifierTrainFlow骨架中间添加三个新步骤来扩展：train_knn和train_svm，它们是并行分支，以及choose_model，它选择两个模型中表现最好的模型。
- en: Listing 3.13 Almost working ClassifierTrainFlow
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.13 几乎完成的ClassifierTrainFlow
- en: '[PRE49]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: ❶ No changes in the start step besides updated self.next().
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 除了更新self.next()之外，起始步骤没有其他更改。
- en: ❷ New training steps are added in the middle of the flow.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在流程中间添加了新的训练步骤。
- en: '❸ This line will cause an error: the argument should be ''poly''.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 这行代码将导致错误：参数应该是'poly'。
- en: ❹ The end step is modified to print out information about the models.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 末步骤被修改为打印出有关模型的信息。
- en: The two train_ steps fit a model using training data from the artifacts train_data
    and train_labels that we initialized in the start step. This straightforward approach
    works well for small and medium-size datasets. Training larger models using larger
    amounts of data sometimes requires different techniques, which we will discuss
    in chapter 7.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 两个train_步骤使用我们在起始步骤中初始化的artifacts train_data和train_labels中的训练数据来拟合模型。这种方法对于小型和中型数据集来说效果很好。使用大量数据训练大型模型有时需要不同的技术，我们将在第7章中讨论。
- en: The choose_model step uses Scikit-Learn’s score method to score each model using
    test data. The models are sorted in the descending order (thanks to -x[1], which
    negates the score in the sort key) by their score. We store the best model in
    the model artifact, which will be used later by ClassifierPredictFlow. Note that
    all models and their scores are stored in the results artifact, allowing us to
    inspect the results later in a notebook.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: choose_model步骤使用Scikit-Learn的score方法使用测试数据对每个模型进行评分。根据分数（多亏了-x[1]，它将排序键中的分数取反）按降序排序模型。我们将最佳模型存储在model
    artifact中，它将被ClassifierPredictFlow稍后使用。请注意，所有模型及其评分都存储在results artifact中，允许我们稍后在笔记本中检查结果。
- en: 'Save the code again to classifier_train.py, and run it as shown here:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码再次保存到classifier_train.py中，并按以下所示运行：
- en: '[PRE50]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Ouch! The code in listing 3.13 fails with an error like ValueError: ''polynomial''
    is not in list.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '哎呀！列表3.13中的代码在出现类似ValueError: ''polynomial''不在列表中的错误时失败。'
- en: Errors such as this are an expected part of the prototyping loop. In fact, many
    features of Metaflow and other similar frameworks are specifically designed to
    make debugging failures easier. Whenever something fails, you can triage and fix
    the issue by following the steps suggested in figure 3.21.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 这种错误是原型设计循环中预期的一部分。事实上，Metaflow和其他类似框架的许多功能都是专门设计来使调试失败更容易的。每当有东西失败时，您可以通过遵循图3.21中建议的步骤对问题进行分类和修复。
- en: '![CH03_F21_Tuulos](../../OEBPS/Images/CH03_F21_Tuulos.png)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F21_Tuulos](../../OEBPS/Images/CH03_F21_Tuulos.png)'
- en: Figure 3.21 The debugging loop
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.21 调试循环
- en: Let’s go through the steps one by one.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地来。
- en: 1\. Finding error messages in logs
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 在日志中查找错误信息
- en: The first step is to try to understand what failed exactly, in particular, which
    step failed and with what error message. If you are running flows manually, you
    should see a stack trace on the terminal, prefixed with the step name (train_svm
    in the case of ClassifierTrainFlow from earlier).
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是尝试理解确切失败的原因，特别是哪个步骤失败了，以及错误信息是什么。如果您手动运行流程，您应该在终端上看到带有步骤名称的前缀的堆栈跟踪（例如，在先前的ClassifierTrainFlow中的train_svm）。
- en: Especially with wide foreaches, it is possible that so many error messages appear
    on the terminal that reading them becomes hard. In this situation, the logs command
    (see section 3.1.1), which can be used to show the output of an individual task,
    can come in handy. However, the command is useful only if you know what step or
    task might have failed. It is not helpful for finding a failed needle in a haystack.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是在宽泛的foreach中，终端上可能会出现如此多的错误信息，以至于阅读它们变得困难。在这种情况下，日志命令（见3.1.1节），可以用来显示单个任务的输出，可能会很有用。然而，该命令只有在您知道可能失败的步骤或任务时才有用。它对于在干草堆中寻找失败的针并没有帮助。
- en: Alternatively, you can use the Client API, for example, in a notebook to comb
    through all tasks automatically. You can copy and paste the next snippet in a
    notebook.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用客户端API，例如在笔记本中自动遍历所有任务。您可以将以下片段复制并粘贴到笔记本中。
- en: Listing 3.14 Accessing logs with the Client API
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.14 使用客户端API访问日志
- en: '[PRE51]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: You can replace Flow().latest_run with a Run object referring to a specific
    run, such as Run("ClassifierTrainFlow/1611603034239532"), to analyze the logs
    of any past run. A benefit of using the Client API is that you can use the full
    power of Python to find what you need. For instance, you can see only logs containing
    a specific term by adding a conditional like
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将Flow().latest_run替换为指向特定运行的Run对象，例如Run("ClassifierTrainFlow/1611603034239532")，以分析任何过去运行的日志。使用客户端API的好处是您可以使用Python的全部功能来找到您需要的内容。例如，您可以通过添加一个条件来仅查看包含特定术语的日志
- en: '[PRE52]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: in the code.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中。
- en: 2\. Understanding why the code failed
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 理解代码失败的原因
- en: Once you have figured out what failed, you can start analyzing why it failed.
    Often, this step involves double-checking the documentation (and googling!) of
    the API that failed. Metaflow supports debugging as usual with a few extra tools.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您弄清楚什么失败了，您就可以开始分析为什么它失败了。通常，这一步涉及到仔细检查失败API的文档（以及谷歌搜索！）。Metaflow支持使用一些额外的工具进行常规调试。
- en: Use the Client API to inspect artifacts that represent the state of the execution
    before the failure. A major motivation for storing as much information as possible
    as artifacts is to help reconstruct the state of the flow before a failure. You
    can load artifacts in a notebook, inspect them, and use them to test hypotheses
    related to the failure. Learn to love the artifacts!
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 使用客户端API检查代表执行前状态的工件。尽可能多地存储信息作为工件的主要动机是帮助重建失败前的流程状态。您可以在笔记本中加载工件，检查它们，并使用它们来测试与失败相关的假设。学会热爱工件吧！
- en: Metaflow is compatible with debuggers, like the ones embedded in Visual Studio
    Code and PyCharm. Because Metaflow executes tasks as separate processes, debuggers
    need a bit of extra configuration to work correctly. You can find instructions
    for configuring debuggers in popular editors in the online documentation of Metaflow.
    Once you have a debugger configured, you can use it to inspect the live code as
    usual.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow与调试器兼容，如Visual Studio Code和PyCharm中嵌入的调试器。由于Metaflow将任务作为单独的进程执行，调试器需要一些额外的配置才能正确工作。您可以在Metaflow的在线文档中找到配置流行编辑器中调试器的说明。一旦您配置了调试器，您就可以像往常一样使用它来检查实时代码。
- en: Commonly, computationally-intensive code involving large amounts of data fails
    because of resource exhaustion, such as running out of memory. We will learn more
    about how to deal with these issues in chapter 4.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，涉及大量数据的计算密集型代码会因资源耗尽而失败，例如内存不足。我们将在第 4 章中学习如何处理这些问题。
- en: 3\. Testing fixes
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 测试修复
- en: Finally, you can attempt to fix the code. A hugely beneficial feature of Metaflow
    is that you don’t have to restart the whole run from the beginning to test the
    fix. Imagine having a flow that first spends 30 minutes processing input data,
    then three hours training a model, and then fails at the end step due to a misspelled
    name. You can fix the typo in a minute, but it would be frustrating to have to
    wait for 3.5 hours to confirm that the fix works.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以尝试修复代码。Metaflow 的一个巨大优势是您不需要从头开始重新启动整个运行来测试修复。想象一下，有一个流程首先花费 30 分钟处理输入数据，然后训练模型花费
    3 小时，最后由于拼写错误在最后一步失败。您可以在一分钟内修复这个错误，但不得不等待 3.5 小时来确认修复是否有效，这会让人感到沮丧。
- en: 'Instead, you can use the resume command. Let’s use it to fix the error in listing
    3.13\. Instead of "polynomial" as an argument to the model in the train_svm step,
    the argument should be ''poly''. Replace the incorrect line with this:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，您可以使用 `resume` 命令。让我们用它来修复列表 3.13 中的错误。在 `train_svm` 步骤中，将 "polynomial" 作为模型的参数，参数应该是
    'poly'。将错误的行替换为以下内容：
- en: '[PRE53]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'You could run the code again with the run command, but instead of doing that,
    try:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用运行命令再次运行代码，但与其这样做，不如尝试以下方法：
- en: '[PRE54]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: This command will find the previous run, clone the results of all of the steps
    that succeeded, and resume execution from the step that failed. In other words,
    it won’t spend time re-executing already successful steps, which in the previous
    example would have saved 3.5 hours of execution time!
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将找到之前的运行，克隆所有成功步骤的结果，并从失败的步骤开始恢复执行。换句话说，它不会花费时间重新执行已经成功的步骤，这在前面的例子中可以节省 3.5
    小时的执行时间！
- en: If your attempt to fix the code wasn’t successful, you can try another idea
    and resume again. You can keep iterating on fixes for as long as needed, as depicted
    by the back arrow in figure 3.21.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您尝试修复代码不成功，您可以尝试另一个想法并再次恢复。您可以持续迭代修复，直到需要为止，如图 3.21 中的反向箭头所示。
- en: 'In the previous example, resume reuses the results of the train_knn step that
    succeeded. However, in some cases, fixing one step might necessitate changes in
    successful steps, too, which then you may want to resume as well. You can do this
    by instructing resume to resume execution from any step that precedes the failed
    step, such as:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，`resume` 重新使用了 `train_knn` 步骤的成功结果。然而，在某些情况下，修复一个步骤可能需要更改成功的步骤，这时您可能也想恢复。您可以通过指示
    `resume` 从任何先于失败步骤的步骤开始恢复执行，例如：
- en: '[PRE55]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: This will force resume to rerun both the train_knn and train_svm steps, as well
    as any subsequent steps. Failed steps and steps that follow them are always rerun.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 这将强制 `resume` 重新运行 `train_knn` 和 `train_svm` 步骤，以及任何后续步骤。失败的步骤及其后续步骤总是会被重新运行。
- en: 'By default, resume finds the latest run ID that was executed in the current
    working directory and uses it as the *origin run*, that is, the run whose results
    are cloned for the resumed run. You can change the origin run to any other run
    of the same flow using the --origin-run-id option as follows:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`resume` 命令会在当前工作目录中找到最新执行的运行 ID，并将其用作 *起源运行*，即克隆其结果用于恢复运行的运行。您可以使用 `--origin-run-id`
    选项将起源运行更改为同一流程中的任何其他运行，如下所示：
- en: '[PRE56]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: This will resume the execution of a past run 1611609148294496 starting from
    the train_knn step using the latest version of the code in classifier_train.py.
    The origin run doesn’t have to be a failed run, nor does it have to be a run executed
    by you! In chapter 5, we will use this feature to resume failed production runs
    locally.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从 `train_knn` 步骤开始，使用 `classifier_train.py` 中的最新代码恢复运行 1611609148294496 的执行。起源运行不必是失败的运行，也不必是您执行的运行！在第
    5 章中，我们将使用此功能在本地恢复失败的生产运行。
- en: Resumed runs are registered as normal runs. They will get their own unique Run
    ID, so you can access their results using the Client API. However, you won’t be
    able to change the parameters of the run that was resumed, because changing them
    might affect the results of tasks to be cloned, which could lead to inconsistent
    results overall.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 恢复的运行被注册为正常运行。它们将获得自己的唯一运行 ID，因此您可以使用客户端 API 访问它们的结果。但是，您无法更改已恢复运行的参数，因为更改它们可能会影响要克隆的任务的结果，这可能导致整体结果不一致。
- en: 3.3.4 Finishing touches
  id: totrans-483
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.4 完成细节
- en: 'After fixing ClassifierTrainFlow, it should complete successfully and produce
    a valid model. To complete ClassifierPredictFlow (listing 3.12), add the following
    line to its end step:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 在修复ClassifierTrainFlow后，它应该成功完成并生成一个有效的模型。要完成ClassifierPredictFlow（列表3.12），请将其末尾步骤添加以下行：
- en: '[PRE57]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'To test predictions, you have to supply a vector on the command line. The wine
    dataset contains 13 attributes for each wine. You can find their definitions in
    the Scikit-Learn’s dataset page ([http://mng.bz/ZAw9](http://mng.bz/ZAw9)). For
    instance, here’s an example that uses a vector from the training set:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试预测，您必须在命令行上提供一个向量。葡萄酒数据集包含每个葡萄酒的13个属性。您可以在Scikit-Learn的数据集页面（[http://mng.bz/ZAw9](http://mng.bz/ZAw9)）中找到它们的定义。例如，以下是一个使用训练集向量的示例：
- en: '[PRE58]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The predicted class should be 0\. Congratulations—we have a working classifier!
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的类别应该是0。恭喜——我们有一个工作的分类器！
- en: What if the classifier produces incorrect results? You can use the combination
    of Scikit-Learn’s model insight tools and the Client API to inspect the models
    as shown in figure 3.22.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 如果分类器产生错误的结果怎么办？您可以使用Scikit-Learn的模型洞察工具和Client API来检查模型，如图3.22所示。
- en: '![CH03_F22_Tuulos](../../OEBPS/Images/CH03_F22_Tuulos.png)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F22_Tuulos](../../OEBPS/Images/CH03_F22_Tuulos.png)'
- en: Figure 3.22 A notebook for inspecting a classifier model
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.22 检查分类器模型的笔记本
- en: Now that the application seems to work end-to-end, let’s summarize what we built.
    The application demonstrates many key concepts of Metaflow and data science applications
    in general. Figure 3.23 illustrates the overall architecture of our final application.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，应用程序似乎可以端到端工作，让我们总结一下我们构建的内容。该应用程序展示了Metaflow和一般数据科学应用程序的许多关键概念。图3.23说明了我们最终应用程序的整体架构。
- en: '![CH03_F23_Tuulos](../../OEBPS/Images/CH03_F23_Tuulos.png)'
  id: totrans-493
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F23_Tuulos](../../OEBPS/Images/CH03_F23_Tuulos.png)'
- en: Figure 3.23 The architecture of classifier application
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.23 分类器应用程序的架构
- en: 'Reading the figure from the top down:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 从上到下阅读图示：
- en: We obtained input data and split it into a train and test sets, which were stored
    as artifacts.
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们获得了输入数据，并将其分为训练集和测试集，这些集被存储为工件。
- en: We trained two alternative models as parallel branches . . .
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们训练了两个替代模型作为并行分支……
- en: . . . and chose the best performing one based on the accuracy with test data.
  id: totrans-498
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ……并基于测试数据的准确性选择表现最好的一个。
- en: The chosen model was stored as an artifact, which we can inspect together with
    other artifacts in a notebook using the Client API.
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所选模型被存储为一个工件，我们可以使用Client API与笔记本中的其他工件一起检查它。
- en: A separate prediction flow can be called as often as needed to classify new
    vectors using the trained model.
  id: totrans-500
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以根据需要多次调用单独的预测流程来对新向量进行分类，使用训练好的模型。
- en: Although this section featured a minimal toy example (can you imagine that we
    implemented the application depicted in figure 3.23 in less than 100 lines of
    code!), the architecture is perfectly valid for production-grade applications.
    You can replace the input data with your own dataset, improve the modeling steps
    according to your actual needs, add more details in the notebook to make it more
    informative, and replace the single --vector input for predictions, such as with
    a CSV file.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本节介绍了一个最小化的玩具示例（你能想象我们用不到100行代码实现了图3.23所示的应用程序吗！），但该架构对于生产级应用程序是有效的。您可以替换输入数据为您的数据集，根据您的实际需求改进建模步骤，在笔记本中添加更多细节以使其更具信息性，并替换用于预测的单个--vector输入，例如使用CSV文件。
- en: 'The rest of this book answers the following questions (as well as many others)
    that you are likely to face when you adapt this application, and other data science
    applications of similar nature, to real-world use cases:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 本书剩余部分回答了以下问题（以及许多其他问题），这些问题可能是您在将此应用程序以及其他类似性质的数据科学应用程序应用于实际用例时可能遇到的问题：
- en: What if I have to handle a terabyte of input data?
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我必须处理一个TB的输入数据怎么办？
- en: What if I want to train 2,000 models instead of two and training each model
    takes an hour?
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我想训练2000个模型而不是两个，并且每个模型的训练需要一个小时怎么办？
- en: After adding the actual modeling and data processing code in the flow, the file
    is getting quite long. Can we split the code into multiple files?
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在流程中添加实际的建模和数据预处理代码后，文件变得相当长。我们可以将代码拆分为多个文件吗？
- en: Am I supposed to keep running flows manually in production? Can we schedule
    them to run automatically?
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我是否应该在生产中手动运行流程？我们可以安排它们自动运行吗？
- en: When I call Flow().latest_run, I want to be sure that the latest run refers
    to my latest run, not to my colleague’s latest run. Can we isolate our runs somehow?
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我调用Flow().latest_run时，我想确保最新的运行指的是我的最新运行，而不是我同事的最新运行。我们能否以某种方式隔离我们的运行？
- en: Our production flows run with an older version of Scikit-Learn, but I want to
    prototype a new model using the latest experimental version of Scikit-Learn—any
    ideas?
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的生产流程使用的是Scikit-Learn的旧版本，但我想要使用Scikit-Learn的最新实验版本来原型化一个新模型——有什么想法吗？
- en: Fear not—the rest of the infrastructure will build seamlessly on the foundation
    that we laid out in this chapter. If you made it this far, you have a good grasp
    of the essentials, and you can skip any sections in subsequent chapters that are
    not relevant for your use cases.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 不要担心——其余的基础设施将无缝地建立在我们在本章中建立的基础上。如果你已经走到这一步，你对基本概念已经有了很好的掌握，你可以跳过后续章节中与你的用例不相关的任何部分。
- en: Summary
  id: totrans-510
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'How to define workflows with Metaflow:'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用Metaflow定义工作流：
- en: You can define basic workflows in Metaflow and test them on your laptop or a
    cloud workstation.
  id: totrans-512
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在Metaflow中定义基本的工作流并在你的笔记本电脑或云工作站上测试它们。
- en: Metaflow tracks all executions automatically, giving them unique IDs, so your
    project stays organized throughout iterations without extra effort. Unique IDs
    allow you to find logs and data related to any task easily.
  id: totrans-513
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Metaflow会自动跟踪所有执行，为它们分配唯一的ID，这样你的项目在整个迭代过程中都能保持组织有序，无需额外努力。唯一的ID允许你轻松找到与任何任务相关的日志和数据。
- en: Use artifacts to store and move data within workflows.
  id: totrans-514
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用工件在工作流中存储和移动数据。
- en: Parameterize workflows using special artifacts called Parameter.
  id: totrans-515
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用称为Parameter的特殊工件来参数化工作流。
- en: Use notebooks with the Client API to analyze, visualize, and compare metadata
    and artifacts from any past runs.
  id: totrans-516
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用客户端API的笔记本来分析、可视化和比较任何过去运行中的元数据和工件。
- en: 'How to do parallel computation with Metaflow:'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用Metaflow进行并行计算：
- en: Use branches to make your application more understandable by making data dependencies
    explicit as well as to achieve higher performance.
  id: totrans-518
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分支可以使你的应用程序通过明确数据依赖性来提高可理解性，同时也能实现更高的性能。
- en: You can run either one operation on multiple pieces of data using dynamic branches,
    or you can run many distinct operations in parallel using static branches.
  id: totrans-519
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用动态分支在多份数据上执行一个操作，或者使用静态分支并行执行许多不同的操作。
- en: 'How to develop a simple end-to-end application:'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何开发一个简单的端到端应用程序：
- en: It is best to develop applications iteratively.
  id: totrans-521
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最好迭代地开发应用程序。
- en: Use resume to continue execution quickly after failures.
  id: totrans-522
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用恢复功能在失败后快速继续执行。
- en: Metaflow is designed to be used with off-the-shelf data science libraries like
    Scikit-Learn.
  id: totrans-523
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Metaflow旨在与现成的数据科学库（如Scikit-Learn）一起使用。
