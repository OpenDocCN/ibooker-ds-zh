- en: 4 Deploying applications in Kubernetes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 在Kubernetes中部署应用程序
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Scheduling Pods to nodes
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将Pod调度到节点
- en: Creating multiple containers within a Pod
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Pod内创建多个容器
- en: Using the Helm templating engine
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Helm模板引擎
- en: Requesting and limiting the resources a Pod can take
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求和限制Pod可以使用的资源
- en: Passing configuration data to Pods
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将配置数据传递给Pod
- en: In this chapter, we’ll shift our focus to the workloads and scheduling part
    of the exam objectives and focus on the many different aspects of scheduling and
    their intricacies. You will realize that one size doesn’t fit all, which creates
    freedom to run an application in the environment that’s best for containerized
    applications. This includes the ability to reserve resources from the underlying
    infrastructure as well as decouple components such as configuration information
    and sensitive data within.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将把我们的重点从考试目标的工作负载和调度部分转移到许多不同的调度方面及其复杂性。你会意识到没有一种方法适合所有人，这为在最适合容器化应用程序的环境中运行应用程序提供了自由。这包括从底层基础设施中预留资源以及解耦配置信息等敏感数据的能力。
- en: The workloads and scheduling domain
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 工作负载和调度领域
- en: This chapter covers part of the workloads and scheduling domain of the CKA curriculum.
    This domain covers how we run applications on Kubernetes. It encompasses the following
    competencies.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了CKA课程的工作负载和调度领域的一部分。这个领域涵盖了我们在Kubernetes上运行应用程序的方式。它包括以下能力。
- en: '| Competency | Chapter section |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 能力 | 章节部分 |'
- en: '| --- | --- |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Use ConfigMaps and Secrets to configure applications. | 4.3 |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 使用ConfigMaps和Secrets配置应用程序。 | 4.3 |'
- en: '| Understand how resource limits can affect Pod scheduling. | 4.3 |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 理解资源限制如何影响Pod调度。 | 4.3 |'
- en: '| Awareness of manifest management and common templating tools. | 4.2 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 对清单管理和常见模板工具的了解。 | 4.2 |'
- en: '| Understand the primitives used to create robust, self-healing application
    Deployments. | 4.1 |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 理解用于创建健壮、自我修复的应用程序部署的原语。 | 4.1 |'
- en: 4.1 Scheduling applications
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 调度应用程序
- en: Deploying applications to run on Kubernetes is called *scheduling*, which makes
    sense when you think of it from the perspective of running a Pod on a node. There
    are many ways to deploy applications (schedule Pods) to Kubernetes. As we’ve discovered
    in previous chapters, the Kubernetes component that controls the assignment of
    a Pod to a node is called the *scheduler*. The scheduler will not only tell the
    Pod to run on a specific node, but it will also ensure that the resources are
    available on the node so that the Pod can run successfully, as depicted in figure
    4.1.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 将应用程序部署到Kubernetes上运行称为*调度*，当你从在节点上运行Pod的角度考虑时，这是有意义的。有许多方法可以将应用程序（调度Pod）部署到Kubernetes。正如我们在前面的章节中所发现的，控制Pod分配到节点的Kubernetes组件称为*调度器*。调度器不仅会告诉Pod在特定节点上运行，还会确保节点上有可用的资源，以便Pod可以成功运行，如图4.1所示。
- en: '![](../../OEBPS/Images/04-01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/04-01.png)'
- en: Figure 4.1 The scheduler allows the Pod to run on the worker node after verifying
    resource availability.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 调度器在验证资源可用性后允许Pod在工作节点上运行。
- en: 'The CKA exam will test your knowledge of creating and updating application
    Deployments. This includes scheduling to certain nodes, according to their labels.
    For example, an exam prompt might say, “Apply the label ‘disk=ssd’ to the worker
    node named ‘kind-worker’ and schedule a Pod named ‘ssd-pod’ to the ‘kind-worker’
    node by using node selector.” To better approach a task like this when we sit
    for the exam, let’s create a new Pod from YAML with the command `k run ssd-pod
    --image=nginx --dry-run=client -o yaml > ssd-pod.yaml`. This command will save
    the Pod YAML in a file named `ssd-pod.yaml`. Before we start modifying the YAML,
    we have to show the existing labels on each node in our cluster with the command
    `k get no --show-labels`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: CKA考试将测试你创建和更新应用程序部署的知识。这包括根据它们的标签将Pod调度到特定的节点。例如，考试提示可能说，“将标签‘disk=ssd’应用到名为‘kind-worker’的工作节点上，并使用节点选择器调度名为‘ssd-pod’的Pod到‘kind-worker’节点。”为了在考试中更好地处理这类任务，让我们使用命令`k
    run ssd-pod --image=nginx --dry-run=client -o yaml > ssd-pod.yaml`创建一个新的Pod。这个命令将Pod
    YAML保存到名为`ssd-pod.yaml`的文件中。在我们开始修改YAML之前，我们必须使用命令`k get no --show-labels`显示我们集群中每个节点的现有标签：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: EXAM TIP On the exam, there may not be a label pre-applied to the node. You
    can check the labels on a node with the command `k get no --show-labels``.`
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 考试技巧 在考试中，可能不会预先在节点上应用标签。你可以使用命令`k get no --show-labels`来检查节点的标签。`
- en: Let’s apply a new label to the node named `kind-worker` that will signify that
    this node has an SSD disk as opposed to HDD. This way, the Pod can be scheduled
    to the `kind-worker` node by using a node selector. To apply a label to a node,
    we use the syntax `key=value` so we can select that label at a later time. To
    apply a label with the key `disktype` and value `ssd`, we can use the command
    `k label no kind-worker disktype=ssd`. Then we can run the `show labels` command
    again to see that the label was applied successfully.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们给名为 `kind-worker` 的节点应用一个新的标签，以表明该节点具有 SSD 硬盘而不是 HDD。这样，Pod 就可以通过节点选择器调度到
    `kind-worker` 节点。要给节点应用标签，我们使用 `key=value` 语法，这样我们可以在以后选择该标签。要应用具有键 `disktype`
    和值 `ssd` 的标签，我们可以使用命令 `k label no kind-worker disktype=ssd`。然后我们可以再次运行 `show labels`
    命令，以查看标签是否成功应用。
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now that the worker node is labeled, we can add the node selector to our Pod
    YAML to ensure that the Pod gets scheduled to this newly labeled node. Go ahead
    and open the file `ssd-pod.yaml` in a text editor like Vim (any text editor will
    do). Once we have it open, we can add the line `nodeSelector:` in line with the
    word containers, and just below that add `disktype: ssd` indented two spaces.
    We indent two spaces to indicate that the specified parameters are grouped with
    the previous block above it (e.g., `disktype` is contained within the block),
    as shown in figure 4.2.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '现在工节点已经标记，我们可以在 Pod YAML 中添加节点选择器，以确保 Pod 被调度到这个新标记的节点。请打开文本编辑器（如 Vim）中的文件
    `ssd-pod.yaml`。一旦打开，我们可以在与容器一词平行的行中添加 `nodeSelector:`，然后在下面缩进两个空格添加 `disktype:
    ssd`。我们缩进两个空格是为了表示指定的参数与上面的块组合在一起（例如，`disktype` 包含在块中），如图 4.2 所示。'
- en: '![](../../OEBPS/Images/04-02.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/04-02.png)'
- en: Figure 4.2 Selecting the type of node to schedule a Pod to, based on the node
    label `disk=ssd`
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 根据节点标签 `disk=ssd` 选择调度 Pod 的节点类型
- en: 'After you’ve added this, you can save the file. We can use the command `k apply
    -f ssd-pod.yaml` to create the Pod and schedule it to the node, which we specified
    in the Pod YAML by using the node selector. Let’s now verify that the Pod was
    scheduled to the intended node by running the command `k get po -o wide`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加此内容后，您可以保存文件。我们可以使用命令 `k apply -f ssd-pod.yaml` 来创建 Pod 并将其调度到指定的节点，这是我们在
    Pod YAML 中通过节点选择器指定的。现在，让我们通过运行命令 `k get po -o wide` 来验证 Pod 是否被调度到预期的节点：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Looks like the Pod was scheduled to the correct node, and the rules for the
    node selector were applied successfully. Great job!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来 Pod 被调度到了正确的节点，节点选择器的规则也成功应用。做得好！
- en: There are other scenarios in which you would want to change how Pods are scheduled.
    In most cases, however, you want to limit your alterations to scheduling in Kubernetes.
    Keep in mind that changing the default rules for scheduling changes the primary
    functionality of Kubernetes, which is to promote high availability and fault tolerance.
    You may have a specific need to schedule Pods to certain nodes (as in the previous
    task), but normally the scheduler is going to distribute workloads properly and
    efficiently. In figure 4.3, you will see the default labels for both the control
    plane node and the worker nodes, with the addition of the `disktype` label we
    applied to `kind-worker` earlier in this chapter.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，您可能希望更改 Pod 的调度方式。然而，在大多数情况下，您希望将您的更改限制在 Kubernetes 的调度中。请注意，更改调度默认规则会改变
    Kubernetes 的主要功能，即提高可用性和容错性。您可能需要将 Pod 调度到特定的节点（如前一个任务中所示），但通常调度器会正确且高效地分配工作负载。在图
    4.3 中，您将看到控制平面节点和工作节点默认的标签，包括我们在本章早期为 `kind-worker` 应用到的 `disktype` 标签。
- en: '![](../../OEBPS/Images/04-03.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/04-03.png)'
- en: Figure 4.3 All nodes can have different labels that you can select and schedule
    Pods to by selecting those labels.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 所有节点都可以有不同的标签，您可以通过选择这些标签来调度 Pod。
- en: 4.1.1 Node selectors
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 节点选择器
- en: As we discussed in the previous section, there may come a time when you’d like
    to schedule a Pod to a specific node. Perhaps that node has a certain type of
    CPU or memory that is optimized for your workload. There are many things we can
    use inside of our Pod YAML (and Deployment YAML) that will help solve this. Let’s
    talk about a couple of them now—the node selector and the node name attribute
    inside of the manifest. We can add either one (not both) to the YAML manifest
    and be able to schedule the Pod on a specific node. What’s the difference? Well,
    the node selector is a little bit more flexible, as it identifies a node by its
    labels. This way, the labels can be applied to many nodes, not just one.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中讨论的，可能会有这样的时候，你希望将 Pod 安排到特定的节点。也许那个节点具有某种类型的 CPU 或内存，这对你的工作负载进行了优化。我们可以在
    Pod YAML（以及 Deployment YAML）内部使用许多东西来帮助我们解决这个问题。现在让我们讨论其中的一些——节点选择器和清单中的节点名称属性。我们可以在
    YAML 清单中添加其中一个（而不是两个），以便能够将 Pod 安排在特定的节点上。它们之间有什么区别呢？嗯，节点选择器稍微灵活一些，因为它通过标签来识别节点。这样，标签可以应用于多个节点，而不仅仅是单个节点。
- en: Adding a node name instead of a node selector is just the opposite. You can
    specify the specific node by name, but as we know, we can’t have two nodes with
    the same name, so that limits our options a bit. What if we have node failure?
    What if the node runs out of resources? Creating a single point of failure by
    scheduling a Pod to only a single node is not the best idea. As you’ll see in
    figure 4.4, you can force a Pod to be scheduled to a specific node by label or
    by name.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用节点名称而不是节点选择器正好相反。你可以通过名称指定特定的节点，但正如我们所知，我们不能有两个具有相同名称的节点，这限制了我们的选择。如果我们遇到节点故障怎么办？如果节点资源耗尽怎么办？通过将
    Pod 安排在单个节点上创建单点故障并不是一个好主意。正如你在图 4.4 中所看到的，你可以通过标签或名称强制将 Pod 安排在特定的节点上。
- en: '![](../../OEBPS/Images/04-04.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/04-04.png)'
- en: Figure 4.4 When scheduling a Pod, you can force the node to which it is scheduled
    based on the node’s name or label.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 在安排 Pod 时，你可以根据节点的名称或标签强制指定其安排的节点。
- en: A DaemonSet is a type of Kubernetes object that ensures that one Pod replica
    is always running on every single node in the cluster at all times. Even if you
    delete the Pod, the DaemonSet will respawn the Pod on any node that’s missing
    that DaemonSet’s Pod. DaemonSets use the node name attribute for all Pods within
    a DaemonSet to schedule Pods to specific nodes by their name. This ensures that
    one and only one Pod is assigned to a node, and by using the node name, they ensure
    that there aren’t accidentally two Pods running on a single node, as depicted
    in figure 4.5 with the kube-proxy Pod being an example of one DaemonSet that runs
    in every Kubernetes cluster.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: DaemonSet 是一种 Kubernetes 对象，确保集群中的每个节点始终运行一个 Pod 副本。即使你删除了 Pod，DaemonSet 也会在任何缺少该
    DaemonSet Pod 的节点上重新启动 Pod。DaemonSet 使用节点名称属性为 DaemonSet 内部的所有 Pod 安排 Pod 到特定的节点。这确保了只有一个
    Pod 被分配给一个节点，并且通过使用节点名称，它们确保不会意外地在单个节点上运行两个 Pod，如图 4.5 中的 kube-proxy Pod 就是 Kubernetes
    集群中运行的示例之一。
- en: '![](../../OEBPS/Images/04-05.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/04-05.png)'
- en: Figure 4.5 The kube-proxy Pods are a part of a DaemonSet and reside on each
    node in the cluster.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 kube-proxy Pods 是 DaemonSet 的一部分，并位于集群中的每个节点上。
- en: Let’s look at one of the DaemonSet Pods running in our cluster with the command
    `k -n kube-system get po kindnet-2tlqh -o yaml | grep nodeName -a3`, and you’ll
    see output similar to figure 4.6.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用命令 `k -n kube-system get po kindnet-2tlqh -o yaml | grep nodeName -a3`
    来查看我们集群中运行的其中一个 DaemonSet Pod，你将看到类似于图 4.6 的输出。
- en: '![](../../OEBPS/Images/04-06.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/04-06.png)'
- en: Figure 4.6 Scheduling a Pod by specifying the node name in the Pod YAML
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 通过在 Pod YAML 中指定节点名称来安排 Pod
- en: In this command, we are getting the YAML output of the kindnet DaemonSet Pod,
    which resides in the `kube-system` namespace. Then, we can search through the
    YAML and find the instance of the node name with the grep tool. As you can see,
    the node name is added to the Pod YAML for this DaemonSet Pod. Remember, the Pod
    name will be different for your cluster, so replace the name `kindnet-2tlqh` with
    the Pod name in your cluster. The command to retrieve the Pod names is `k get
    po -A`. Also, to look at the DaemonSets running in your cluster, type the command
    `k get ds`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个命令中，我们正在获取位于`kube-system`命名空间中的kindnet守护进程Pod的YAML输出。然后，我们可以通过grep工具搜索YAML文件并找到节点名称的实例。如您所见，节点名称被添加到这个守护进程Pod的YAML中。请记住，Pod名称对于您的集群可能会有所不同，所以将名称`kindnet-2tlqh`替换为您集群中的Pod名称。获取Pod名称的命令是`k
    get po -A`。此外，要查看您集群中运行的守护进程集，请输入命令`k get ds`。
- en: Hopefully, you can better understand where the node name is used and why it’s
    used. In most cases, you will use a node selector, and you can probably see the
    benefits much more clearly. The node selector is used in many more scenarios,
    as it can apply to many nodes, selecting the node labels. For example, you can
    label nodes with `disk=ssd` to indicate that they have solid-state drives, which
    may make a difference for applications that require high disk I/O.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 希望您能更好地理解节点名称的使用位置及其使用原因。在大多数情况下，您将使用节点选择器，您可能更清楚地看到其好处。节点选择器在更多场景中使用，因为它可以应用于多个节点，选择节点标签。例如，您可以使用`disk=ssd`来标记节点，表示它们具有固态硬盘，这可能对需要高磁盘I/O的应用程序产生影响。
- en: Exam Tip You may see a node selector or node name on the exam, where the question
    might ask you to add the necessary values to an existing Pod YAML. So, become
    familiar with the placement and proper syntax used here.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 考试技巧：您可能在考试中看到节点选择器或节点名称，问题可能要求您向现有的Pod YAML中添加必要的值。因此，熟悉这里的放置和正确语法。
- en: The same goes for CPU or memory-intensive applications, as the node’s resources
    have a direct effect on how the applications will run inside of Pods running containers.
    To take a look at how node selectors are used in our cluster, look no further
    than the core DNS Pods that are already running in the `kube-system` namespace.
    To see the Pod YAML for a CoreDNS Pod, we’ll use the same method we did with the
    DaemonSet Pods with the command `k -n kube-system get po coredns-64897985d-4th9h
    -o yaml | grep nodeSelector -a4`, and you’ll see output similar to figure 4.7.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CPU或内存密集型应用程序也是如此，因为节点的资源会直接影响容器内运行的Pod中的应用程序如何运行。要查看节点选择器在我们集群中的使用情况，无需进一步寻找，只需查看已经在`kube-system`命名空间中运行的core
    DNS Pods。要查看CoreDNS Pod的YAML，我们将使用与守护进程Pod相同的方法，使用命令`k -n kube-system get po coredns-64897985d-4th9h
    -o yaml | grep nodeSelector -a4`，您将看到类似于图4.7的输出。
- en: '![](../../OEBPS/Images/04-07.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图4.7](../../OEBPS/Images/04-07.png)'
- en: Figure 4.7 Selecting which node this Pod will run on by its label, specified
    in the Pod YAML
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 通过Pod YAML中指定的标签选择此Pod将在哪个节点上运行
- en: NOTE The name of your CoreDNS Pod will be different, so use the command `k -n
    kube-system get po` to find your unique name.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您的CoreDNS Pod名称可能不同，因此请使用命令`k -n kube-system get po`来查找您独特的名称。
- en: In this command, we are getting the Pod YAML output for the CoreDNS Pod, which
    resides in the `kube-system` namespace, and searching for the word `nodeSelector`.
    Remember, the Pod name will be different for your cluster, so replace the name
    `coredns-64897985d-4th9h` with the Pod name in your cluster. The command to retrieve
    the Pod names is `k get po -A`. Now that we’ve taken a look at node names and
    node selectors in this section, as well as a useful way to edit Kubernetes objects
    inline, let’s move on to another scheduling technique, called *affinity*, to favor
    certain nodes over others.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个命令中，我们正在获取位于`kube-system`命名空间中的CoreDNS Pod的YAML输出，并搜索单词`nodeSelector`。请记住，Pod名称对于您的集群可能会有所不同，所以将名称`coredns-64897985d-4th9h`替换为您集群中的Pod名称。获取Pod名称的命令是`k
    get po -A`。现在我们已经在本节中查看过节点名称和节点选择器，以及编辑Kubernetes对象的内联方法，让我们继续探讨另一种调度技术，称为*亲和性*，以优先选择某些节点。
- en: 4.1.2 Node and Pod affinity
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 节点和Pod亲和性
- en: As we know, labels can be applied to both Pods and nodes. We can use these labels
    to apply affinity. Affinity is a lot like a node selector, but its rules are more
    expressive. For example, a node selector rule will select a node with a label,
    whereas an affinity rule will prefer a node with a label but also accept scheduling
    to another node if that node label is not present.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，标签可以应用于 Pod 和节点。我们可以使用这些标签来应用亲和力。亲和力很像节点选择器，但其规则更为灵活。例如，节点选择器规则会选择带有标签的节点，而亲和力规则则会偏好带有标签的节点，但如果该节点标签不存在，也会接受调度到其他节点。
- en: To better understand affinity, we have to understand the problem it solves.
    There is an inherent problem in larger Kubernetes clusters where not only Pods
    come and go but nodes do as well. So, setting hard rules for the Pod to be scheduled
    to a node based on its name or label doesn’t suffice. Instead of alienating a
    Pod if a node is not present, affinity provides a more flexible ruleset, so applications
    can be scheduled to a cluster environment that is constantly changing. As you
    see in figure 4.8, you can set rules that prefer one node, but if that node is
    unavailable, it will still have the option to be scheduled to a second node.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解亲和力，我们必须了解它解决的问题。在较大的 Kubernetes 集群中存在一个固有的问题，不仅 Pod 会来去，节点也是如此。因此，仅根据节点的名称或标签为
    Pod 设置硬规则是不够的。亲和力提供了一种更灵活的规则集，以便应用程序可以调度到一个不断变化的集群环境中。如图 4.8 所示，您可以设置偏好一个节点的规则，但如果该节点不可用，它仍然有选项被调度到第二个节点。
- en: '![](../../OEBPS/Images/04-08.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片 4-08](../../OEBPS/Images/04-08.png)'
- en: Figure 4.8 Affinity prefers the Pod be scheduled to a node with the label `SSD`
    but will schedule to node `LINUX` if none exists.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 亲和力偏好将 Pod 调度到带有标签 `SSD` 的节点，如果不存在则调度到节点 `LINUX`。
- en: Let’s go through an example of setting an affinity rule. We’ll start with a
    brand-new Pod, and we’ll use the dry run to create a template for our Pod YAML,
    as we did previously with the command `k run affinity --image nginx --dry-run=client
    -o yaml > affinity.yaml`. Once we have a file named `affinity.yaml` in our current
    directory, let’s open it up in a text editor and add our node affinity rules.
    Just below the spec, in line with `containers`, add `affinity:` to the line. Indent
    two spaces and add `nodeAffinity:`, followed by `requiredDuringSchedulingIgnoredDuringExecution:`.
    See figure 4.9 for the remaining YAML syntax, as it is quite extensive.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过设置亲和力规则的示例来了解。我们将从一个全新的 Pod 开始，并使用 dry run 来创建 Pod YAML 的模板，就像我们之前使用命令
    `k run affinity --image nginx --dry-run=client -o yaml > affinity.yaml` 所做的那样。一旦我们在当前目录中有一个名为
    `affinity.yaml` 的文件，让我们在文本编辑器中打开它并添加我们的节点亲和力规则。在 `spec` 下方，与 `containers` 行对齐，添加
    `affinity:` 到该行。缩进两个空格，然后添加 `nodeAffinity:`，后面跟着 `requiredDuringSchedulingIgnoredDuringExecution:`。参见图
    4.9 了解剩余的 YAML 语法，因为它相当广泛。
- en: '![](../../OEBPS/Images/04-09.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片 4-09](../../OEBPS/Images/04-09.png)'
- en: Figure 4.9 Variations of affinity that prefer to schedule a Pod to a node based
    on certain expressions
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 基于特定表达式偏好将 Pod 调度到节点的亲和力变化
- en: 'Below node affinity, where we set the required attribute, the scheduler can''t
    schedule the Pod unless this rule is met. This functions like `nodeSelector`,
    but with a more expressive syntax. The preferred attribute, however, is optional
    for the scheduler. If there isn’t a node that matches this preference, then the
    Pod will still be scheduled. Also, you can specify a weight between 1 and 100
    for each preferred rule, which the scheduler evaluates based on the total weight,
    based on the score of other priority functions for the node. Nodes with the highest
    total score will be prioritized. Now that we’ve added our affinity rules, let’s
    schedule our Pod with the command `k apply -f affinity.yaml`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置所需属性的下级节点亲和力中，调度器只有在满足此规则的情况下才能调度 Pod。这类似于 `nodeSelector`，但具有更灵活的语法。然而，对于调度器来说，首选属性是可选的。如果没有节点匹配此偏好，Pod
    仍然会被调度。此外，您可以为每个首选规则指定介于 1 和 100 之间的权重，调度器将根据节点的其他优先函数的得分，基于总权重进行评估。总得分最高的节点将被优先考虑。现在我们已经添加了亲和力规则，让我们使用命令
    `k apply -f affinity.yaml` 来调度我们的 Pod：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: From the output of the command `k get po -o wide`, we see that the Pod was scheduled
    to a node that did not have the `disktype=ssd` label (`kind-worker2`). That’s
    because the weight that we set for this preferred rule was too low, and because
    there was another Pod already running on `kind-worker`, the node `kind-worker2`
    had a higher total weight overall being that it did not already have a Pod running
    on it (higher weight equals greater probability that a Pod will be scheduled).
    If you didn’t quite follow, or if you’d like to check your work, apply this YAML
    to create your Pod with the command `k apply -f https://raw.githubusercontent.com/chadmcrowell/k8s/
    main/manifests/pod-node-affinity.yaml`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 从命令 `k get po -o wide` 的输出中，我们看到 Pod 被调度到了没有 `disktype=ssd` 标签的节点 (`kind-worker2`)。这是因为我们为这个首选规则设置的权重太低，并且因为
    `kind-worker` 上已经有一个 Pod 在运行，所以 `kind-worker2` 节点的总权重更高（权重越高，Pod 被调度的概率越大）。如果您没有完全理解，或者想检查您的作业，可以使用以下命令应用此
    YAML 创建您的 Pod：`k apply -f https://raw.githubusercontent.com/chadmcrowell/k8s/main/manifests/pod-node-affinity.yaml`。
- en: In the same vein as node affinity is inter-Pod affinity, which is the preference
    for a node based on other Pods that are already on that node. To put it in simpler
    terms, if a Pod already exists on the node (e.g., `nginx`), then go ahead and
    schedule a new Pod to that same node (the node with the `nginx` Pod on it), as
    depicted in figure 4.10\. The way Kubernetes checks if the Pod exists is by its
    label. This works well with Pods that need to run on the same node because of
    increased performance or latency requirements.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与节点亲和力类似，Pod 亲和力是节点亲和力，即基于该节点上已存在的其他 Pod 的偏好。用更简单的话来说，如果一个 Pod 已经存在于节点上（例如，`nginx`），那么请继续将新的
    Pod 调度到该节点（即具有 `nginx` Pod 的节点），如图 4.10 所示。Kubernetes 通过标签检查 Pod 是否存在。这对于需要运行在相同节点上以提高性能或延迟要求的
    Pod 来说是有效的。
- en: '![](../../OEBPS/Images/04-10.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10](../../OEBPS/Images/04-10.png)'
- en: Figure 4.10 The Pod will only be scheduled to a node that already has an `nginx`
    Pod running.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 Pod 只会被调度到已经运行了 `nginx` Pod 的节点。
- en: To demonstrate inter-Pod affinity, let’s first create a Pod that has the label
    `run=nginx` with the command `k run nginx --image=nginx`. Now, let’s go ahead
    and use the same YAML file that we were using for node affinity but change only
    a few lines, because, as you will see, setting the inter-Pod affinity is very
    similar to how we set node affinity. If you don’t already have the previous YAML
    file, you can download it using curl and the `-O` flag to save the file with the
    same name using the command `curl -O https://raw.githubusercontent.com/chadmcrowell/k8s/main/manifests/
    pod-node-affinity.yaml``.`
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示 Pod 亲和力，我们首先使用命令 `k run nginx --image=nginx` 创建一个具有标签 `run=nginx` 的 Pod。现在，让我们继续使用之前用于节点亲和力的相同
    YAML 文件，但只更改几行，因为您将看到，设置 Pod 亲和力与设置节点亲和力非常相似。如果您还没有之前的 YAML 文件，可以使用 curl 和 `-O`
    标志下载文件，并使用以下命令保存文件：`curl -O https://raw.githubusercontent.com/chadmcrowell/k8s/main/manifests/pod-node-affinity.yaml`。
- en: Go ahead and open the file in your text editor and change the name of the Pod
    to `pod-affinity` to prevent a naming conflict if you still have the Pod running
    from the previous demonstration. In addition, change `nodeAffinity` to `podAffinity`;
    change the line `nodeSelectorTerms:` to `- labelSelector:`; remove the hyphen
    (-) from `matchExpressions`; and change the key to `run` and value to `nginx`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请继续在您的文本编辑器中打开文件，将 Pod 的名称更改为 `pod-affinity`，以防止与之前演示中仍在运行的 Pod 发生命名冲突。此外，将
    `nodeAffinity` 更改为 `podAffinity`；将 `nodeSelectorTerms:` 行更改为 `- labelSelector:`；从
    `matchExpressions` 中移除连字符 (-)；并将键更改为 `run`，值更改为 `nginx`。
- en: Finally, add the `topologyKey` below the values to be in line with the word
    `labelSelector`. The topology key is required for inter-Pod affinity and simply
    indicates the node label key. In this case, all nodes in our cluster have the
    `kubernetes.io/hostname` key, so this will match all nodes. See figure 4.11 for
    a comparison between node affinity and inter-Pod affinity. You can remove everything
    below the topology key, as in this example we’re just focusing on the inter-Pod
    affinity required rule.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在 `labelSelector` 下方添加 `topologyKey` 以保持与单词 `labelSelector` 的一致性。拓扑键是 Pod
    亲和力所必需的，它简单地表示节点标签键。在这种情况下，我们集群中的所有节点都有 `kubernetes.io/hostname` 键，因此这将匹配所有节点。参见图
    4.11 比较节点亲和力和 Pod 亲和力。您可以删除拓扑键以下的所有内容，因为在这个例子中我们只关注所需的 Pod 亲和力规则。
- en: '![](../../OEBPS/Images/04-11.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.11](../../OEBPS/Images/04-11.png)'
- en: Figure 4.11 How inter-Pod affinity is different than node affinity in its structure
    within the Pod YAML
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 Pod YAML 中 Pod 间亲和性与节点亲和性在结构上的不同
- en: Now that our `nginx` Pod is running and we’ve typed out the YAML for our new
    Pod with inter-Pod affinity rules, we can go ahead and schedule the Pod with the
    command `k apply -f pod-node-affinity.yaml`. Just after you schedule the Pod,
    you should see that the Pod has been scheduled to the same node as the Pod named
    `nginx` by running the command `k get po -o wide`, and you’ll see an output similar
    to figure 4.12.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的 `nginx` Pod 正在运行，我们已经输出了具有 Pod 间亲和性规则的新 Pod 的 YAML，我们可以继续使用命令 `k apply
    -f pod-node-affinity.yaml` 来调度 Pod。在调度 Pod 后，你应该会看到 Pod 已经通过运行命令 `k get po -o
    wide` 被调度到与名为 `nginx` 的 Pod 相同的节点，输出类似于图 4.12。
- en: '![](../../OEBPS/Images/04-12.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.12](../../OEBPS/Images/04-12.png)'
- en: Figure 4.12 The Pods have been scheduled to the same node based on their affinity
    rules.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12 根据亲和性规则，Pod 已被调度到相同的节点。
- en: You can see how, under normal circumstances, the Pod would have been scheduled
    to the node named `kind-worker`, but because we told the scheduler to find all
    Pods with the `run=nginx` label and schedule a Pod named `pod-affinity` to that
    very same node, it overruled the default scheduler settings. If you are getting
    different results, or maybe you can’t get your Pod to be scheduled at all, please
    compare what you have to the YAML in this file, which you can download using the
    command `curl -O https://raw.githubusercontent.com/chadmcrowell/k8s/main/manifests/pod-with-pod-affinty.yaml``.`
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在正常情况下，你可以看到 Pod 将会被调度到名为 `kind-worker` 的节点，但由于我们告诉调度器找到所有带有 `run=nginx` 标签的
    Pod，并将名为 `pod-affinity` 的 Pod 调度到该节点，它覆盖了默认的调度设置。如果你得到不同的结果，或者可能根本无法调度你的 Pod，请将你的内容与该文件中的
    YAML 进行比较，你可以使用以下命令下载该文件：`curl -O https://raw.githubusercontent.com/chadmcrowell/k8s/main/manifests/pod-with-pod-affinty.yaml`。
- en: Exam exercises
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 练习考试
- en: Apply the label `disk=ssd` to a node. Create a Pod named `fast` using the `nginx`
    image and make sure that it selects a node based on the label `disk=ssd`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 将标签 `disk=ssd` 应用到一个节点上。创建一个名为 `fast` 的 Pod，使用 `nginx` 镜像，并确保它根据标签 `disk=ssd`
    选择节点。
- en: Edit the `fast` Pod using `k edit po fast` and change the node selector to `disk=slow`.
    Notice that the Pod cannot be changed and the YAML was saved to a temporary location.
    Take the YAML in `/tmp/` and apply it by force to delete and recreate the Pod
    using a single imperative command.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `k edit po fast` 编辑 `fast` Pod，并将节点选择器更改为 `disk=slow`。注意，Pod 无法更改，YAML 已保存到临时位置。将
    `/tmp/` 中的 YAML 应用到强制删除并重新创建 Pod，使用单个强制命令。
- en: Create a new Pod named `ssd-pod` using the `nginx` image, and use node affinity
    to select nodes based on a weight of 1 to nodes that have a label `disk=ssd`.
    If the selection criteria don’t match, it can also choose nodes that have the
    label `kubernetes .io/os=linux`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `nginx` 镜像创建一个新的 Pod，名为 `ssd-pod`，并使用节点亲和性根据标签 `disk=ssd` 的权重 1 选择节点。如果选择标准不匹配，它还可以选择带有标签
    `kubernetes .io/os=linux` 的节点。
- en: 4.2 Using Helm
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 使用 Helm
- en: With all the different methods to configure applications for Deployment in Kubernetes,
    therein lies a problem of misconfiguration and also developing redeployable components.
    For example, when creating Deployments with Services, Ingress, ConfigMaps, Roles,
    and Role bindings, it can be complex, and it’s not always intuitive how the application
    developer intended for the application to run in order to operate according to
    best practices. Helm is a package manager for Kubernetes that supports templating
    to solve this problem.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中配置应用程序以用于 Deployment 的所有不同方法中，存在一个配置错误和开发可重部署组件的问题。例如，当创建带有服务、入口、配置映射、角色和角色绑定的部署时，可能会很复杂，并且并不总是直观地了解应用程序开发者希望应用程序如何运行以根据最佳实践操作。Helm
    是 Kubernetes 的包管理器，支持模板化以解决这个问题。
- en: Testing your knowledge of using Helm will be a part of the exam; therefore,
    it’s important to understand what problem it solves and how to use it. Instead
    of having to rebuild each YAML file from scratch, they can be packaged up and
    stored in a repository for others to use and share. In addition to a template
    engine, much like a package manager in your favorite Linux distro (i.e., `apt`
    in Ubuntu), Helm allows you to pull down Helm charts from a repository (public
    or private) and install those Helm charts locally.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 测试您使用 Helm 的知识将是考试的一部分；因此，了解它解决的问题以及如何使用它是很重要的。我们不必从头开始重新构建每个 YAML 文件，它们可以被打包并存储在存储库中供他人使用和分享。除了模板引擎之外，它类似于您最喜欢的
    Linux 发行版中的包管理器（例如，Ubuntu 中的 `apt`），Helm 允许您从存储库（公共或私有）中拉取 Helm 图表并本地安装这些 Helm
    图表。
- en: 'To install Helm on macOS, simply perform the command `brew install helm``.`
    To install Helm on Windows, simply perform the command `choco install kubernetes-helm.`
    To install Helm on Ubuntu or Debian, perform the following commands to add Helm
    to your package repository and install using `apt`:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 macOS 上安装 Helm，只需执行命令 `brew install helm`。要在 Windows 上安装 Helm，只需执行命令 `choco
    install kubernetes-helm`。要在 Ubuntu 或 Debian 上安装 Helm，执行以下命令将 Helm 添加到您的包存储库，并使用
    `apt` 安装：
- en: '[PRE4]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Once you get Helm installed, you can run the command `helm version --short`,
    and you’ll see output similar to figure 4.13, which will be the version of Helm
    that is currently installed.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了 Helm，您就可以运行命令 `helm version --short`，您将看到类似于图 4.13 的输出，这将显示当前安装的 Helm
    版本。
- en: '![](../../OEBPS/Images/04-13.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.13](../../OEBPS/Images/04-13.png)'
- en: Figure 4.13 Show the version of Helm via the CLI.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.13 通过 CLI 显示 Helm 的版本。
- en: Next, we can install a Helm chart, which is nothing more than a group of YAML
    manifests packaged up into an application that runs in Kubernetes. For Helm to
    connect to the cluster, it uses the same kubeconfig that `kubectl` uses. Being
    that Helm is using the same kubeconfig, it can perform the operations, which could
    be a good thing or a bad thing. For us, it’s okay for now, as we’ll need admin
    permissions to create the resources associated with our Helm chart.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以安装一个 Helm 图表，它不过是一组打包成应用程序的 YAML 清单，该应用程序在 Kubernetes 中运行。为了连接到集群，Helm
    使用与 `kubectl` 相同的 kubeconfig。由于 Helm 使用相同的 kubeconfig，它可以执行操作，这可能是好事也可能是坏事。对我们来说，现在没问题，因为我们需要管理员权限来创建与我们的
    Helm 图表相关的资源。
- en: 'We can really use any Helm chart from the public repository located at [https://artifacthub.io/](https://artifacthub.io/).
    To use a Helm chart, we first must add the repository where the Helm chart exists
    or else Helm won’t know where to pull the chart from. We can run the command `helm
    repo add hashicorp https://helm.releases.hashicorp.com` to add the repository
    to Helm, and then run the command `helm repo list` to see that it’s been added:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用位于 [https://artifacthub.io/](https://artifacthub.io/) 的公共存储库中的任何 Helm
    图表。要使用 Helm 图表，我们首先必须添加包含 Helm 图表的存储库，否则 Helm 将不知道从哪里拉取图表。我们可以运行命令 `helm repo
    add hashicorp https://helm.releases.hashicorp.com` 将存储库添加到 Helm，然后运行命令 `helm repo
    list` 以查看它已被添加：
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that a repository has been added, we can search that repository for all
    available charts with the command `helm search repo vault`. You’ll see that, within
    the repo HashiCorp, there’s a Helm chart named `hashicorp/vault`, which we can
    use:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经添加了存储库，我们可以使用命令 `helm search repo vault` 在该存储库中搜索所有可用的图表。您会看到，在 HashiCorp
    存储库中有一个名为 `hashicorp/vault` 的 Helm 图表，我们可以使用它：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s add another repo! I thought it would be helpful to install a load balancer
    in our kind cluster. This will be useful later on when we talk about communication,
    in chapter 6\. A load balancer in Kubernetes allows access to our application
    from outside the cluster. For example, if we were hosting a web application as
    a part of a Deployment in Kubernetes, we’d be able to expose it via the LoadBalancer
    Service. This way, visitors to your application could reach the frontend of the
    application via its IP address. For now, though, let’s focus on adding the repo
    and installing the Helm chart.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再添加一个存储库！我认为在 kind 集群中安装一个负载均衡器会有所帮助。这将在我们讨论第 6 章中的通信时很有用。Kubernetes 中的负载均衡器允许从集群外部访问我们的应用程序。例如，如果我们将
    Web 应用程序作为 Kubernetes 部署的一部分托管，我们将能够通过 LoadBalancer 服务公开它。这样，访问您应用程序的访客可以通过其 IP
    地址访问应用程序的前端。不过，现在让我们专注于添加存储库和安装 Helm 图表。
- en: We can add the repository just as we did with the HashiCorp repository using
    the command `helm repo add metallb https://metallb.github.io/metallb.`
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用与添加 HashiCorp 存储库相同的命令 `helm repo add metallb https://metallb.github.io/metallb`
    添加存储库。
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that we have the repo added, we can search for `metallb` with the command
    `helm search repo metallb:`
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经添加了仓库，我们可以使用命令 `helm search repo metallb:` 来搜索 `metallb`
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Sure enough, there it is! Now, before we install it, let’s talk about something
    called the *values file*. Now, we know that Helm is a templating engine. The templating
    engine takes a file that has values to plug into the template, that changes the
    configuration of our Helm chart. This values file is written in YAML and will
    look similar to figure 4.14 for the configuration of a Pod.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 确实如此！现在，在我们安装它之前，让我们谈谈一个叫做 *值文件* 的东西。现在，我们知道 Helm 是一个模板引擎。模板引擎会读取一个包含值的文件，并将其插入到模板中，从而改变
    Helm 图表的配置。这个值文件是用 YAML 编写的，其外观将类似于图 4.14 中 Pod 的配置。
- en: '![](../../OEBPS/Images/04-14.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.14](../../OEBPS/Images/04-14.png)'
- en: Figure 4.14 How to create a values file and apply values to Helm for a Pod manifest
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14 如何创建一个值文件并将值应用到 Pod 清单的 Helm 中
- en: Then, once we establish these values, the templating in the actual Helm chart
    looks similar to a regular Pod YAML file that we’ve created before, but instead
    of the values filled in, they use the templating syntax `{{ .values.name }}`,
    as shown in figure 4.15.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一旦我们确定了这些值，实际 Helm 图表中的模板化看起来类似于我们之前创建的常规 Pod YAML 文件，但它们使用模板语法 `{{ .values.name
    }}` 而不是填充的值，如图 4.15 所示。
- en: '![](../../OEBPS/Images/04-15.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.15](../../OEBPS/Images/04-15.png)'
- en: Figure 4.15 How a normal Pod YAML would look compared to a Helm chart with templated
    values
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.15 与我们之前创建的常规 Pod YAML 文件相比，具有模板化值的 Helm 图表
- en: 'Now that we know what a template file does (it templatizes your YAML files),
    you may be asking how we apply the values file to our Helm chart. Well, we simply
    run the command `helm install` and use the `--values` flag. So, to install the
    MetalLB Helm chart, perform the command `helm install metallb metallb/metallb
    --values values.yaml`. But wait! We haven’t created the values file yet. Let’s
    do this now using the following YAML and save it to a file named `values.yaml`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了模板文件的作用（它将您的 YAML 文件模板化），您可能想知道我们如何将值文件应用到我们的 Helm 图表中。好吧，我们只需运行 `helm
    install` 命令并使用 `--values` 标志。所以，要安装 MetalLB Helm 图表，请执行命令 `helm install metallb
    metallb/metallb --values values.yaml`。但是等等！我们还没有创建值文件。现在让我们使用以下 YAML 并将其保存到名为
    `values.yaml` 的文件中：
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The address range (172.18.255.200-250) is within the address range of your
    Docker bridge network (the range may be different for you). To find the `kind`
    Docker bridge CIDR, use the command `docker network inspect -f ''{{.IPAM.Config}}''
    kind` (type `exit` to get out of `kind-control-plane` shell). Once you’ve created
    the `values.yaml` file, you can run the command `helm install metallb metallb/metallb
    --values values.yaml` (type `docker exec -it kind-control-plane bash` to get back
    into the `kind-control-plane` shell):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 地址范围（172.18.255.200-250）位于您的 Docker 网桥网络地址范围内（您的范围可能不同）。要找到 `kind` Docker 网桥
    CIDR，请使用命令 `docker network inspect -f '{{.IPAM.Config}}' kind`（输入 `exit` 退出 `kind-control-plane`
    命令行）。一旦创建了 `values.yaml` 文件，您就可以运行命令 `helm install metallb metallb/metallb --values
    values.yaml`（输入 `docker exec -it kind-control-plane bash` 返回到 `kind-control-plane`
    命令行）：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that we’ve deployed the MetalLB load balancer via Helm, we can see that
    it was more than just a single resource. It created a Deployment, a DaemonSet,
    four Secrets, two Service Accounts, and a ConfigMap, as shown in figure 4.16 (obtained
    by running `k get po,deploy,ds,secret,sa,cm`). All we had to do was run `helm
    install`, and it installed all of that! Pretty cool, huh? It reminds me of when
    I’m at a restaurant and order a delicious meal. I have no idea how it’s made or
    what all of the ingredients are; it just comes out of the kitchen piping hot and
    tastes so delicious!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经通过 Helm 部署了 MetalLB 负载均衡器，我们可以看到它不仅仅是一个单一的资源。它创建了一个 Deployment、一个 DaemonSet、四个
    Secrets、两个 Service Accounts 和一个 ConfigMap，如图 4.16 所示（通过运行 `k get po,deploy,ds,secret,sa,cm`
    获取）（我们只需运行 `helm install`，它就安装了所有这些！真酷，不是吗？这让我想起了我坐在餐厅里点了一道美味的菜。我不知道它是如何制作的，也不知道所有的配料是什么；它只是从厨房里热腾腾地端出来，味道如此美味！）
- en: '![](../../OEBPS/Images/04-16.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.14](../../OEBPS/Images/04-16.png)'
- en: Figure 4.16 Simplifying Kubernetes scheduling and creating multiple Kubernetes
    objects via a Helm chart
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.16 通过 Helm 图表简化 Kubernetes 调度并创建多个 Kubernetes 对象
- en: 'To list the Helm charts we have installed, we can run the command `helm ls`:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要列出我们已安装的 Helm 图表，我们可以运行命令 `helm ls`：
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We’ll come back to using MetalLB in chapter 6\. For now, this was good practice
    using Helm and installing a Helm chart using custom values.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第 6 章中回到使用 MetalLB。现在，这只是一个使用 Helm 和使用自定义值安装 Helm 图表的好习惯。
- en: 4.3 Pod metadata
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 Pod 元数据
- en: After discussing scheduling at great length, let’s talk about some of the customizations
    you can make before scheduling your applications in Kubernetes. There are several
    options, which we’ll cover in this section, but as you can imagine, there’s no
    one size that fits all. Not all Kubernetes objects (Deployments, Pods, Secrets,
    ConfigMaps, etc.) are created in the same way. Sometimes you need to consider
    the underlying hardware, not only in terms of selecting nodes to schedule Pods
    to, as we discussed in sections 4.1.2 and 4.1.3, but also in terms of resource
    constraints, additional features, and unique characteristics that each application
    may need. Let’s start with resource requests and limits, which will help us with
    scheduling a Pod to a node with adequate resources (CPU and RAM).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细讨论了调度之后，让我们谈谈在Kubernetes中调度应用程序之前你可以进行的一些自定义设置。本节将介绍几个选项，但正如你所想象的那样，没有一种大小适合所有情况。并非所有Kubernetes对象（部署、Pod、机密、配置映射等）都是用相同的方式创建的。有时你需要考虑底层硬件，不仅是在选择将Pod调度到哪个节点方面，正如我们在第4.1.2节和第4.1.3节中讨论的那样，而且还要考虑资源限制、附加功能和每个应用程序可能需要的独特特性。让我们从资源请求和限制开始，这将帮助我们调度到具有充足资源（CPU和RAM）的节点上的Pod。
- en: 4.3.1 Resource requests and limits
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 资源请求和限制
- en: Resource requests are much like labels in that they provide information to the
    scheduler that determines the node on which a Pod is scheduled to or placed on.
    The request comes in the form of a CPU and RAM minimum that the Pod requires to
    run optimally. The limit also comes in the form of a value of CPU or RAM but determines
    the maximum that the Pod should consume, versus the minimum. These two parameters
    don’t have to be defined at the same time—meaning you can apply requests without
    limits and vice versa. In the same vein, you can specify a request for only CPU
    without specifying a request for RAM. So, these fields are not dependent on each
    other and are totally optional.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 资源请求与标签类似，它们向调度器提供信息，以确定Pod将被调度到哪个节点或放置在哪个节点上。请求以Pod运行最优所需的CPU和RAM的最小值的形式出现。限制也以CPU或RAM的值的形式出现，但确定Pod应该消耗的最大值，与最小值相对。这两个参数不必同时定义——这意味着你可以应用请求而不设置限制，反之亦然。同样，你可以在不指定RAM请求的情况下仅指定CPU请求。因此，这些字段相互独立，完全是可选的。
- en: On the CKA exam, you will be tested on your knowledge about resource limits
    and how they can affect how Pods are scheduled. An example of an exam prompt might
    be “Node ‘worker-1’ has 500MB of memory and .5 CPU available. Schedule a Pod that
    would allow for this memory and CPU limitation, and still schedule to the node.”
    Knowing how to modify the Pod YAML to change resource limits will enable you to
    successfully complete questions like this on exam day.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在CKA考试中，你将接受关于资源限制及其如何影响Pod调度知识的测试。一个可能的考试提示可能是：“节点‘worker-1’有500MB的内存和0.5个CPU可用。调度一个Pod，允许这种内存和CPU限制，并且仍然调度到该节点。”知道如何修改Pod
    YAML以更改资源限制将使你能够在考试当天成功完成此类问题。
- en: You can apply resource requests and limits for a Pod by adding it into the container
    spec via YAML. In line with the name and image of the container, you can add requests
    and limits as values below the resources. To best accomplish this (without having
    to know the exact placement within YAML), you can create a dry run of a Pod again
    by performing the command `k run nginx2 --image=nginx --dry-run=client -o yaml
    > nginx2.yaml.`
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过将资源请求和限制添加到容器规范中，通过YAML为Pod应用资源请求和限制。与容器的名称和镜像一起，你可以在资源下方添加请求和限制作为值。为了最好地完成这项任务（而无需知道YAML中的确切位置），你可以再次通过执行命令`k
    run nginx2 --image=nginx --dry-run=client -o yaml > nginx2.yaml`来对Pod进行干运行。
- en: When you open the file in your favorite text editor, you’ll already see a resources
    section. As we know from the YAML language, we can simply indent two spaces below
    the word `resources` and begin defining our requests and limits. The word `resources`
    is used to insert resource limits and requests. Resource requests are CPU and
    memory minimums that you can specify for Pods. Resource limits are the limits
    you can specify for the same. We’ll cover resource limits and requests later in
    this book. For now, let’s delete the curly braces after the word `resources`,
    as the curly braces indicate a blank entry, so we’re not specifying any resource
    limits or requests for our Pods. Deleting them will have the same effect. For
    this container, we’ll request 100 milicores of CPU and 128 mebibytes, and we’ll
    limit the container to using no more than 2 CPUs and 1 gigabyte of memory. You
    can see where to insert the requests and limits in the YAML spec in figure 4.17.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在你喜欢的文本编辑器中打开文件时，你将已经看到资源部分。正如我们从 YAML 语言中了解到的，我们可以在 `resources` 单词下方简单地缩进两个空格，并开始定义我们的请求和限制。单词
    `resources` 用于插入资源限制和请求。资源请求是你可以为 Pod 指定的 CPU 和内存的最小值。资源限制是你可以为相同内容指定的限制。我们将在本书的后面部分介绍资源限制和请求。现在，让我们删除
    `resources` 单词后面的花括号，因为花括号表示一个空条目，所以我们没有为我们的 Pod 指定任何资源限制或请求。删除它们将产生相同的效果。对于这个容器，我们将请求
    100 milicores 的 CPU 和 128 mebibytes，并将容器限制在不超过 2 个 CPU 和 1 个千兆字节的内存。你可以在图 4.17
    中看到在 YAML 规范中插入请求和限制的位置。
- en: '![](../../OEBPS/Images/04-17.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/04-17.png)'
- en: Figure 4.17 Where to insert the resource requests for Pods within the Pod YAML
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.17 在 Pod YAML 中何处插入 Pod 的资源请求
- en: Now, you may be asking why there are different measurements of CPU and memory.
    In the Kubernetes ecosystem, limits and requests for CPU are measured in CPU units.
    One CPU unit is equivalent to 1 physical CPU core, or 1 virtual core, depending
    on whether the node is a physical host or a virtual machine running inside a physical
    machine. You can write the value in milicores, whereas the value 1000m would be
    the same as 1 CPU. Fractional requests are also allowed, so instead of 1 CPU,
    we can enter a value of 0.5 CPU (equal to 500m). Memory is measured in mebibytes,
    which is a measurement based on powers of 2 (a mebibyte equals 2^(20) or 1,048,576
    bytes).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能想知道为什么 CPU 和内存有不同的度量。在 Kubernetes 生态系统中，CPU 的限制和请求是以 CPU 单位来衡量的。一个 CPU
    单位相当于 1 个物理 CPU 核心，或者 1 个在物理机器内部运行的虚拟核心，具体取决于节点是物理主机还是虚拟机。你可以用 milicores 来写这个值，而
    1000m 的值就等同于 1 个 CPU。也可以允许分数请求，所以我们可以输入 0.5 CPU 的值（等于 500m）。内存是以 mebibytes 为单位来衡量的，这是一个基于
    2 的幂的度量（1 个 mebibyte 等于 2^20 或 1,048,576 字节）。
- en: The reason we specify the requests and limits is that, if the node where a Pod
    is running has enough of a resource available, it's possible (and allowed) for
    a container to use more of a resource than its request for that resource specifies.
    However, a container is not allowed to use more than its resource limit.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定请求和限制的原因是，如果一个 Pod 运行的节点有足够的资源可用，那么容器使用比其请求的资源更多的资源是可能的（也是允许的）。然而，容器不允许使用超过其资源限制的资源。
- en: 4.3.2 Multicontainer Pods
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 多容器 Pod
- en: Sometimes in Kubernetes, it makes sense to have more than one container running
    inside of the same Pod. This can be useful in some cases because each container
    inside of a Pod shares the name network namespace and storage, so communication
    is faster and you don’t have to establish additional Services and network policies
    to communicate between Pods. There are a couple of reasons for creating multiple
    containers inside of the same Pod. One is for log collection, where all application
    logs would go to a separate container specifically for logging. Another reason
    is to ensure the initialization of another container or Service—in other words,
    running a command from one container would verify that the MySQL Service was up
    and running, for example. Containers in the same Pod also can share storage. So,
    you could have a volume attached to both Pods and stream that data to the volume
    simultaneously. This could be useful if one container were to output the logs
    to a file, then another container was to read them. In this case, the second container
    is sometimes referred to as a “sidecar container.” This is because it’s not acting
    as the main application; it’s just assisting with the collection of logs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，有时在同一个Pod中运行多个容器是有意义的。在某些情况下，这可能很有用，因为Pod内的每个容器都共享名称网络命名空间和存储，因此通信更快，你不必建立额外的服务和网络策略来在Pod之间通信。在同一个Pod内创建多个容器有几个原因。一个是用于日志收集，所有应用程序日志都会发送到一个专门用于日志的单独容器。另一个原因是确保另一个容器或服务的初始化——换句话说，从一个容器中运行命令将验证MySQL服务是否正在运行，例如。同一个Pod中的容器还可以共享存储。因此，你可以将卷附加到两个Pod上，并将数据同时流式传输到该卷。如果其中一个容器将日志输出到文件，然后另一个容器读取它们，这可能很有用。在这种情况下，第二个容器有时被称为“sidecar容器”。这是因为它不是作为主应用程序运行；它只是协助收集日志。
- en: 'Let’s go ahead and see this in action by running the following command to create
    a sidecar container: `k run sidecar --image=busybox --command "sh" "c" "while
    true; do cat /var/log/nginx/error.log; sleep 20; done" --dry-run=client -o yaml
    > sidecar.yaml`. Now, open the file `sidecar.yaml` in your favorite text editor.
    Change the `c` to a `-c` and add another container that will serve as the main
    container to the current sidecar container. In addition, to allow the sidecar
    container to read the log data, we’ll go ahead and add a volume mount to an `emptyDir`
    volume type, as shown in figure 4.18.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行以下命令来创建一个sidecar容器，看看这个操作的实际效果：`k run sidecar --image=busybox --command
    "sh" "c" "while true; do cat /var/log/nginx/error.log; sleep 20; done" --dry-run=client
    -o yaml > sidecar.yaml`。现在，用你喜欢的文本编辑器打开文件`sidecar.yaml`。将`c`改为`-c`，并添加另一个容器作为当前sidecar容器的主体容器。此外，为了允许sidecar容器读取日志数据，我们将添加一个挂载到`emptyDir`卷类型的卷挂载，如图4.18所示。
- en: '![](../../OEBPS/Images/04-18.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图4.18 向Pod添加额外的容器，用于从主应用程序读取日志](../../OEBPS/Images/04-18.png)'
- en: Figure 4.18 Adding an additional container to a Pod, which will serve to read
    logs from the main application
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.18 向Pod添加额外的容器，用于从主应用程序读取日志
- en: An `emptyDir` volume is one of many types of volumes that live and die with
    the container. This is ephemeral storage, so as soon as the Pod is deleted, the
    data is also deleted and gone forever.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`emptyDir`卷是与容器同生共死的多种卷类型之一。这是一种临时存储，因此一旦Pod被删除，数据也会被删除并永远消失。'
- en: Another use case for creating multiple containers in the same Pod is to ensure
    the main application container is initialized. This is called an *init container*,
    being that its only job is to check on the main container or perform some task,
    and once it has done its job, the job is complete and doesn’t continue to run.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一个Pod中创建多个容器的另一个用例是确保主应用程序容器初始化。这被称为*初始化容器*，因为它的唯一任务是检查主容器或执行某些任务，一旦完成其工作，任务就完成了，不会继续运行。
- en: An init container can be added to any Pod manifest, in line with the container’s
    specification. To quickly create our starter YAML, let’s run the command `k run
    init --image=busybox:1.35 --command "sh" "c" "echo The app is running! && sleep
    3600" --dry-run=client -o yaml > init.yaml`. Then, we’ll enter `initContainers:`
    inline with the word `container`. For everything else, we’ll mimic the existing
    container except for the command. We’ll run the `until` loop `until nslookup init-svc;
    do echo waiting for svc; sleep 2; done`. The complete YAML should look similar
    to what you see in figure 4.19.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将初始化容器添加到任何 Pod 清单中，与容器的规范一致。为了快速创建我们的起始 YAML，让我们运行命令 `k run init --image=busybox:1.35
    --command "sh" "c" "echo The app is running! && sleep 3600" --dry-run=client -o
    yaml > init.yaml`。然后，我们将按照单词 `container` 的方式输入 `initContainers:`。对于其他所有内容，我们将模仿现有的容器，除了命令。我们将运行
    `until` 循环 `until nslookup init-svc; do echo waiting for svc; sleep 2; done`。完整的
    YAML 应该看起来与图 4.19 中看到的内容相似。
- en: '![](../../OEBPS/Images/04-19.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.19](../../OEBPS/Images/04-19.png)'
- en: Figure 4.19 Init containers will not allow the main application to start before
    the Service `init-svc` is created.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.19 初始化容器不会允许主应用程序在创建服务 `init-svc` 之前启动。
- en: 'Specifying an init container is much like a regular container, but you’ll notice
    the difference more clearly as you begin to create the Pod. Upon creation, you
    see the Pod go into a pending state. It will stay pending until you create the
    Service named `init-svc`. When you create the Service with the command `k create
    svc clusterip init-svc --tcp 80`, the Pod will change from `Init:0/1` to `running`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 指定一个初始化容器与普通容器非常相似，但当你开始创建 Pod 时，你会更清楚地注意到它们之间的区别。在创建时，你会看到 Pod 进入挂起状态。它将保持挂起状态，直到你创建名为
    `init-svc` 的服务。当你使用命令 `k create svc clusterip init-svc --tcp 80` 创建服务时，Pod 将从
    `Init:0/1` 变为 `running`：
- en: '[PRE12]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Notice, the Pod doesn’t start running until approximately 7 minutes later. So
    if you’re getting impatient, just wait a few more minutes for the Pod to come
    back up.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Pod 直到大约 7 分钟后才开始运行。所以如果你感到不耐烦，只需再等几分钟，直到 Pod 重新启动。
- en: 4.3.3 ConfigMaps and Secrets
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 ConfigMaps 和 Secrets
- en: So now that we’ve talked about Pods requesting CPU and memory, let’s see what
    else Pods can request by talking about a ConfigMap in Kubernetes. Many times,
    you’ll need to pass configuration data to your application, whether it be to change
    the log level or to change the background color of your web app. The most common
    way to do this is by using the ConfigMap object in Kubernetes.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，既然我们已经讨论了 Pods 请求 CPU 和内存，那么让我们通过讨论 Kubernetes 中的 ConfigMap 来看看 Pods 还可以请求什么。很多时候，你需要将配置数据传递给你的应用程序，无论是更改日志级别还是更改你的
    Web 应用的背景颜色。最常见的方法是在 Kubernetes 中使用 ConfigMap 对象。
- en: Once you create the ConfigMap object, you can inject it into a Pod using various
    methods. You can inject it into a Pod via an environment variable or a volume
    mounted to the container. You can create a ConfigMap from literal values (i.e.,
    username and password), from a file (i.e., `ca.crt`), or from multiple files within
    a directory.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了 ConfigMap 对象，你可以使用各种方法将其注入到 Pod 中。你可以通过环境变量或挂载到容器的卷将 ConfigMap 注入到 Pod
    中。你可以从字面值（即用户名和密码）创建 ConfigMap，从文件（即 `ca.crt`）创建，或从目录中的多个文件创建。
- en: You can create a ConfigMap by using an imperative command or by creating YAML
    and checking it into your source code. Because we prefer the latter, let’s create
    the YAML and save it to a file with the command `k create configmap redis-config
    --from-literal=key1=config1 --from-literal=key2=config2 --dry-run=client -o yaml
    > redis-config.yaml`. This command will create a ConfigMap named `redis-config`,
    with literal key `key1` with value `config1`, as well as literal key `key2` with
    value `config2`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用强制命令或创建 YAML 并将其检查到源代码中来创建 ConfigMap。因为我们更喜欢后者，让我们使用命令 `k create configmap
    redis-config --from-literal=key1=config1 --from-literal=key2=config2 --dry-run=client
    -o yaml > redis-config.yaml` 创建 YAML 并将其保存到文件中。此命令将创建一个名为 `redis-config` 的 ConfigMap，其中包含字面量键
    `key1` 的值为 `config1`，以及字面量键 `key2` 的值为 `config2`。
- en: We’ll do a dry run—which means to simulate creating the resource—and finally
    output that to a file named `redis-config.yaml`. When we open this file in our
    text editor, we notice that the key and value pairs can be established line by
    line. Alternatively, we could perform multiline arguments by specifying the key
    and then a pipe symbol (`|`), which we have done in previous chapters. Let’s replace
    the existing key-value pairs in our file to store the ConfigMap data in a Redis
    cache for later use. To do this, delete the lines starting with `key1` and `key2`
    and add a key named `redis-config` by adding it under `data` for a multiline argument
    in our YAML file, similar to figure 4.20.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进行 dry run（即模拟创建资源）并最终将输出保存到名为 `redis-config.yaml` 的文件中。当我们用文本编辑器打开这个文件时，我们会注意到键值对可以逐行建立。或者，我们也可以通过指定键然后一个管道符号（`|`）来执行多行参数，就像我们在前面的章节中所做的那样。让我们替换文件中现有的键值对，以便将
    ConfigMap 数据存储在 Redis 缓存中供以后使用。为此，删除以 `key1` 和 `key2` 开头的行，并在我们的 YAML 文件中的 `data`
    下添加一个名为 `redis-config` 的键，通过在 YAML 文件中的多行参数下添加它，类似于图 4.20。
- en: '![](../../OEBPS/Images/04-20.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/04-20.png)'
- en: Figure 4.20 Creating a ConfigMap in Kubernetes automatically Base64 encodes
    the data within.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.20 在 Kubernetes 中自动创建 ConfigMap 会将数据 Base64 编码。
- en: Now that we’ve created the YAML for our ConfigMap, save and close the file.
    Create the ConfigMap with the command `k apply -f redis-config.yaml`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为 ConfigMap 创建了 YAML 文件，保存并关闭文件。使用命令 `k apply -f redis-config.yaml` 创建
    ConfigMap。
- en: Because the ConfigMap is now available for our application to use, go ahead
    and create a YAML file for a Redis Pod with the command `k run init --image=redis:7
    --port 6379 --command 'redis-server' '/redis-master/redis.conf' --dry-run=client
    -o yaml > redis.yaml`. Let’s add a volume mount to this Pod for the Redis configuration
    data, as well as an `emptyDir`-type volume for any ephemeral data. We can insert
    the volumes and mount paths to the YAML spec, as shown in figure 4.21.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 ConfigMap 现在可供我们的应用程序使用，请继续创建一个 Redis Pod 的 YAML 文件，使用命令 `k run init --image=redis:7
    --port 6379 --command 'redis-server' '/redis-master/redis.conf' --dry-run=client
    -o yaml > redis.yaml`。让我们为这个 Pod 添加一个用于 Redis 配置数据的卷挂载，以及一个 `emptyDir` 类型的卷用于任何临时数据。我们可以将卷和挂载路径插入到
    YAML 规范中，如图 4.21 所示。
- en: '![](../../OEBPS/Images/04-21.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/04-21.png)'
- en: Figure 4.21 Attaching ConfigMap data to the container via an `emptyDir` volume,
    ensuring the volume name and the `mountPath` name are the same
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.21 通过 `emptyDir` 卷将 ConfigMap 数据附加到容器中，确保卷名和 `mountPath` 名相同
- en: We’ll go ahead and create that Pod, and once it’s running, we’ll put a shell
    inside of the container with the Redis CLI prompt using the command `k exec -it
    redis --redis-cli`. Once we’ve done that, we can issue the Redis-specific commands,
    which are `CONFIG GET maxmemory` and `CONFIG GET maxmemory-policy`, and you will
    get an output similar to figure 4.22.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续创建该 Pod，一旦它运行起来，我们将使用命令 `k exec -it redis --redis-cli` 在容器内放置一个带有 Redis
    CLI 提示的 shell。完成这些后，我们可以发出 Redis 特定的命令，这些命令是 `CONFIG GET maxmemory` 和 `CONFIG
    GET maxmemory-policy`，您将得到类似于图 4.22 的输出。
- en: '![](../../OEBPS/Images/04-22.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/04-22.png)'
- en: Figure 4.22 We can view the ConfigMap data via the Redis CLI and relate that
    to the ConfigMap original data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.22 我们可以通过 Redis CLI 查看 ConfigMap 数据，并将其与原始 ConfigMap 数据相关联。
- en: We can match these up directly with our ConfigMap data, and they match! This
    is a great way to insert configuration data into containers and let that configuration
    data remain decoupled from the main application.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接将这些与我们的 ConfigMap 数据匹配，并且它们匹配！这是将配置数据插入容器并使配置数据与主应用程序解耦的绝佳方式。
- en: Secrets are similar to ConfigMaps but different in that they store secret data
    instead of application configuration data. Whether it be a database password or
    certificate data, you can store it in Kubernetes with a Base64 encryption. However,
    don’t believe me if I ever say that it’s secure, because it’s not. Anyone who
    has access to your cluster can view and read the secret data, so never use this
    as your only method of security.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Secrets 与 ConfigMaps 类似，但不同之处在于它们存储的是密钥数据而不是应用程序配置数据。无论是数据库密码还是证书数据，您都可以使用 Base64
    加密在 Kubernetes 中存储。然而，如果我说它是安全的，请不要相信我，因为它并不安全。任何有权访问您的集群的人都可以查看和读取密钥数据，因此永远不要将其作为您唯一的保护方法。
- en: You can create a Secret in the familiar way with the command `k create secret
    generic dev-login --from-literal=username=dev --from-literal=password= 'S!B\*d$zDsb='
    --dry-run=client -o yaml > dev-login.yaml`. Now open the file `dev-login.yaml`
    in your text editor and you’ll notice that both the username and password are
    Base64 encoded. Secrets values are encoded as Base64 strings and are stored unencrypted
    by default, as show in figure 4.23.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用熟悉的命令 `k create secret generic dev-login --from-literal=username=dev --from-literal=password=
    'S!B\*d$zDsb=' --dry-run=client -o yaml > dev-login.yaml` 创建秘密。现在在您的文本编辑器中打开文件
    `dev-login.yaml`，您会注意到用户名和密码都是Base64编码的。秘密值以Base64字符串的形式编码，并且默认情况下以未加密的方式存储，如图4.23所示。
- en: '![](../../OEBPS/Images/04-23.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/04-23.png)'
- en: Figure 4.23 All of the data inside of our Secret named `dev-login` is automatically
    Base64 encoded.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.23](../../OEBPS/Images/04-23.png) 我们名为 `dev-login` 的秘密中的所有数据都会自动进行Base64编码。'
- en: Now, go ahead and create the Secret with the command `k apply -f dev-login.yaml`.
    We can take this Secret and make it available to a Pod by mounting it to a directory
    inside of the container. First, create a Pod YAML with the command `k run secret-pod
    --image=busybox --command "sh" "c" "echo The app is running! && sleep 3600" --dry-run=client
    -o yaml > secret-pod.yaml`. Let’s open the file `secret-pod.yaml` inside of a
    text editor and make some modifications. We’ll add a volume mount to the container
    inside of the Pod, and add a volume that’s just a type of Secret, which Kubernetes
    knows how to handle. You can specify this in the Pod YAML, as seen in figure 4.24.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用命令 `k apply -f dev-login.yaml` 来创建秘密。我们可以通过将秘密挂载到容器内部的目录中来使其对Pod可用。首先，使用命令
    `k run secret-pod --image=busybox --command "sh" "c" "echo The app is running!
    && sleep 3600" --dry-run=client -o yaml > secret-pod.yaml` 创建一个Pod YAML文件。让我们在文本编辑器中打开文件
    `secret-pod.yaml` 并进行一些修改。我们将在Pod内部的容器中添加一个卷挂载，并添加一个类型为Secret的卷，Kubernetes知道如何处理它。您可以在Pod
    YAML中指定此内容，如图4.24所示。
- en: '![](../../OEBPS/Images/04-24.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/04-24.png)'
- en: Figure 4.24 Mount the secret data for the Pod to access, ensuring the data can
    be reached from `/etc/secret-vol` inside the container.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.24](../../OEBPS/Images/04-24.png) 将Pod要访问的秘密数据挂载，确保数据可以从容器内的 `/etc/secret-vol`
    访达。'
- en: At this point, we can view our secret data from within our Pod by running the
    command `k exec secret-pod --cat /etc/secret-vol/username && echo` and `k exec
    secret-pod --cat /etc/secret-vol/password && echo`. You’ll see output similar
    to figure 4.25.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以通过运行命令 `k exec secret-pod --cat /etc/secret-vol/username && echo`
    和 `k exec secret-pod --cat /etc/secret-vol/password && echo` 在Pod内部查看我们的秘密数据。您将看到类似于图4.25的输出。
- en: '![](../../OEBPS/Images/04-25.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/04-25.png)'
- en: Figure 4.25 Secret data can be obtained by running a command inside of the container.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.25](../../OEBPS/Images/04-25.png) 通过在容器内运行命令可以获取秘密数据。'
- en: This is not the only way to give secret data to the container; we can also use
    the Secret inside of the Pod via an environment variable. You can pass the `--env`
    flag after the `kubectl run` command with `k run secret-env --image=busybox --command
    "sh" "c" "printenv DEV_USER DEV_PASS; sleep 8200" --env=DEV_PASS=password --dry-run=client
    -o yaml > secret-env.yaml`. When you open up the `secret-env.yaml` file, you can
    replace `value:` with `valueFrom:` and remove the value of `password`. Below `valueFrom:`,
    you can indent two spaces and type `secretKeyRef:`. Below that, you can indent
    two spaces and type `name:` followed by the name of the Secret that we created
    earlier, which was `dev-login`. Then, in line with the name, you can type `key:`
    followed by `username`, as seen in figure 4.26\. You can repeat this for however
    many Secrets or secret values you have in Kubernetes. Also, multiple Pods can
    reference the same Secret, so we don’t have to delete the previous Pod that we
    created (named `secret-pod`) to use the Secret again inside of a new Pod.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是向容器提供秘密数据的唯一方法；我们还可以通过Pod内的环境变量使用秘密。您可以在 `kubectl run` 命令后使用 `--env` 标志，例如
    `k run secret-env --image=busybox --command "sh" "c" "printenv DEV_USER DEV_PASS;
    sleep 8200" --env=DEV_PASS=password --dry-run=client -o yaml > secret-env.yaml`。当您打开
    `secret-env.yaml` 文件时，可以将 `value:` 替换为 `valueFrom:` 并删除 `password` 的值。在 `valueFrom:`
    下方，您可以缩进两个空格并输入 `secretKeyRef:`。在其下方，您可以缩进两个空格并输入 `name:` 后跟之前创建的秘密名称，即 `dev-login`。然后，与名称对齐，您可以输入
    `key:` 后跟 `username`，如图4.26所示。您可以为Kubernetes中的任何秘密或秘密值重复此操作。此外，多个Pod可以引用同一个秘密，因此我们不需要删除之前创建的Pod（命名为
    `secret-pod`）来在新的Pod中使用秘密。
- en: '![](../../OEBPS/Images/04-26.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/04-26.png)'
- en: Figure 4.26 You can pass the secret data to a Pod via environment variables.
    Set the command inside the container to print the environment variables (`printenv`).
    Don’t forget to change the `c` to `-c`!
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.26 您可以通过环境变量将秘密数据传递给Pod。在容器内设置命令以打印环境变量（`printenv`）。别忘了将`c`改为`-c`！
- en: Exam exercises
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 考试练习
- en: Create a Pod named `limited` with the image `httpd` and set the resource requests
    to 1Gi for CPU and 100Mi for memory.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为`limited`的Pod，使用镜像`httpd`，并将CPU的资源请求设置为1Gi，内存设置为100Mi。
- en: 'Create a ConfigMap named `ui-data` with the key and value pairs as follows.
    Apply a ConfigMap to a Pod named `frontend` with the image `busybox:1.28` and
    pass it to the Pod via the following environment variables:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为`ui-data`的ConfigMap，其键值对如下。将ConfigMap应用到名为`frontend`的Pod上，使用镜像`busybox:1.28`，并通过以下环境变量传递给它：
- en: '[PRE13]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Summary
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The term *scheduling* refers to creating a Pod and assigning it to a node. You
    can change which node the Pod is assigned to with a node selector, node name,
    or affinity rules. Make sure you know the correct YAML syntax for these scheduling
    changes.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 术语*调度*指的是创建一个Pod并将其分配给一个节点。您可以通过节点选择器、节点名称或亲和规则来更改Pod分配到的节点。确保您知道这些调度更改的正确YAML语法。
- en: You can also apply labels to control Pod scheduling. Like a node selector, you
    can use a label selector to schedule a Pod to a specific node based on its label.
    For the exam, know how to apply labels to nodes and Pods to affect scheduling.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您还可以应用标签来控制Pod的调度。就像节点选择器一样，您可以使用标签选择器根据Pod的标签将Pod调度到特定的节点。对于考试，了解如何将标签应用到节点和Pod上以影响调度。
- en: Much like a package manager in Linux, Helm is a package manager and templating
    engine for Kubernetes. Know how to use Helm for the exam and how to use Helm templates
    to deploy applications to Kubernetes.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 就像Linux中的包管理器一样，Helm是Kubernetes的包管理器和模板引擎。了解如何使用Helm进行考试以及如何使用Helm模板将应用程序部署到Kubernetes。
- en: Reserve CPU and memory for Pods using resource requests and limits. This is
    important for the exam, as the YAML syntax for a Pod or Deployment is slightly
    harder than most.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用资源请求和限制为Pod保留CPU和内存。这对于考试很重要，因为Pod或Deployment的YAML语法比大多数都要复杂一些。
- en: There are specific reasons to create more than one container inside of a Pod.
    Sharing the same network namespace and storage allows for direct access and communication
    back and forth.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Pod内部创建多个容器有特定的原因。共享相同的网络命名空间和存储允许直接访问和双向通信。
- en: Injecting configuration data and sensitive information into Pods is simplified
    in Kubernetes using ConfigMaps and Secrets. Make sure you know how to use ConfigMaps
    and Secrets mounted as volumes to a Pod, as well as environment variables.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes中使用ConfigMaps和Secrets将配置数据和敏感信息注入Pod中变得简单。确保您知道如何使用作为卷挂载到Pod中的ConfigMaps和Secrets，以及环境变量。
