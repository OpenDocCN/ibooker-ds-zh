- en: 9 Workflow orchestration
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 工作流编排
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Defining workflow and workflow orchestration
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义工作流和工作流编排
- en: Why deep learning systems need to support workflows
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么深度学习系统需要支持工作流
- en: Designing a general workflow orchestration system
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计通用工作流编排系统
- en: 'Introducing three open source orchestration systems: Airflow, Argo Workflows,
    and Metaflow'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍三个开源编排系统：Airflow、Argo Workflows 和 Metaflow
- en: 'In this chapter, we will discuss the last but critical piece of a deep learning
    system: workflow orchestration—a service that manages, executes, and monitors
    workflow automation. Workflow is an abstract and broad concept; it is essentially
    a sequence of operations that are part of some larger task. If you can devise
    a plan with a set of tasks to complete a work, this plan is a workflow. For example,
    we can define a sequential workflow for training a machine learning (ML) model.
    This workflow can be composed of the following tasks: fetching raw data, rebuilding
    the training dataset, training the model, evaluating the model, and deploying
    the model.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论深度学习系统最后但至关重要的部分：工作流编排——一种管理、执行和监控工作流自动化的服务。工作流是一个抽象和广泛的概念；它本质上是一系列属于某个更大任务的操作。如果你能制定一个包含一系列任务的计划来完成一项工作，那么这个计划就是一个工作流。例如，我们可以定义一个用于训练机器学习（ML）模型的顺序工作流。这个工作流可以由以下任务组成：获取原始数据、重建训练数据集、训练模型、评估模型和部署模型。
- en: Because a workflow is an execution plan, it can be performed manually. For instance,
    a data scientist can manually complete the tasks of the model training workflow
    we just described. For example, to complete the “fetching raw data” task, the
    data scientist can craft web requests and send them to the dataset management
    (DM) service to fetch a dataset—all with no help from the engineers.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 因为工作流是一个执行计划，它可以手动执行。例如，数据科学家可以手动完成我们刚才描述的模型训练工作流中的任务。例如，为了完成“获取原始数据”的任务，数据科学家可以构建网络请求并将它们发送到数据集管理（DM）服务以获取数据集——这一切都不需要工程师的帮助。
- en: However, executing a workflow manually is not ideal. We want to automate the
    workflow execution. When there are numerous workflows developed for different
    purposes, we need a dedicated system to handle the complexity of workflow executions.
    We call this kind of system a *workflow orchestration system*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，手动执行工作流并不是理想的选择。我们希望自动化工作流执行。当有大量针对不同目的开发的工作流时，我们需要一个专门的系统来处理工作流执行的复杂性。我们称这种系统为“工作流编排系统”。
- en: A workflow orchestration system is built to manage workflow life cycles, including
    workflow creation, execution, and troubleshooting. It provides not only the pulse
    to keep all the scheduled code running but also a control plane for data scientists
    to manage all the automation in a deep learning system.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流编排系统是为了管理工作流生命周期而构建的，包括工作流创建、执行和故障排除。它不仅提供了保持所有预定代码运行的脉搏，还为数据科学家提供了一个控制平面，以便管理深度学习系统中的所有自动化。
- en: In this chapter, we will discuss workflow orchestration system design and the
    most popular open source orchestration systems used in the deep learning field.
    By reading this chapter, you will not only gain a solid understanding of the system
    requirements and the design options, but you will also learn how to choose the
    right open source orchestration system that works best for your own situation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论工作流编排系统设计和在深度学习领域使用的最流行的开源编排系统。通过阅读本章，你不仅将获得对系统需求和设计选项的扎实理解，还将学习如何选择最适合你自身情况的正确开源编排系统。
- en: 9.1 Introducing workflow orchestration
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 工作流编排简介
- en: Before we dive into the details of designing workflow orchestration systems,
    let’s have a quick discussion on the basic concept of workflow orchestration,
    especially about the special workflow challenges from a deep learning/ML perspective.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入讨论设计工作流编排系统的细节之前，让我们快速讨论一下工作流编排的基本概念，特别是从深度学习/机器学习的角度来看的特殊工作流挑战。
- en: Note Because the requirements of using workflow orchestration for deep learning
    projects and ML projects are almost identical, we will use the word *deep learning*
    and *machine learning* interchangeably in this chapter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：由于使用工作流编排进行深度学习项目和机器学习项目的要求几乎相同，因此在本章中我们将交替使用“深度学习”和“机器学习”这两个词。
- en: 9.1.1 What is workflow?
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 什么是工作流？
- en: In general, a workflow is a sequence of operations that are part of some larger
    task. A workflow can be viewed as a directed acyclic graph (DAG) of steps.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，工作流程是某些更大任务的一部分的操作序列。工作流程可以被视为步骤的有向无环图（DAG）。
- en: A step is the smallest resumable unit of computation that describes an action;
    this task could be fetching data or triggering a service, for example. A step
    either succeeds or fails as a whole. In this chapter, we use the word *task* and
    *step* interchangeably.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤是描述一个动作的最小可恢复的计算单元；这个任务可以是获取数据或触发一个服务，例如。步骤要么成功要么失败作为一个整体。在本章中，我们使用"任务"和"步骤"这两个词可以互换使用。
- en: A DAG specifies the dependencies between steps and the order in which to execute
    them. Figure 9.1 shows a sample workflow for training natural language processing
    (NLP) models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: DAG（有向无环图）指定了步骤之间的依赖关系以及执行它们的顺序。图9.1展示了用于训练自然语言处理（NLP）模型的示例工作流程。
- en: '![](../Images/09-01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/09-01.png)'
- en: Figure 9.1 A DAG of a sample model training workflow with multiple steps. Both
    ovals and diamonds are steps, but different types. The solid arrows indicate the
    dependencies between steps, and the dotted-line arrows represent the external
    web requests sent from steps.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1展示了具有多个步骤的示例模型训练工作流程的DAG。圆圈和菱形都是步骤，但类型不同。实线箭头指示步骤之间的依赖关系，而虚线箭头表示从步骤发送的外部网络请求。
- en: From the sample DAG in figure 9.1, we see a workflow that consists of many steps.
    Every step depends on another, and the solid arrows show the dependencies between
    steps. These arrows and steps form a workflow DAG with no loops.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从图9.1中的示例DAG中，我们看到一个由许多步骤组成的工作流程。每个步骤都依赖于另一个步骤，实线箭头显示了步骤之间的依赖关系。这些箭头和步骤形成一个无环的工作流程DAG。
- en: If you follow the arrows in the DAG (from left to right) and complete the tasks,
    you can train and release an NLP model to production. For example, when an incoming
    request triggers the workflow, the auth (authorization) step will be executed
    first, and then the dataset-building step and the embedding fetching step will
    both be executed simultaneously. The steps on the other side of the arrows will
    be executed after these two steps have been completed.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你按照DAG中的箭头（从左到右）完成任务，你就可以训练并发布一个NLP模型到生产环境中。例如，当传入的请求触发工作流程时，auth（授权）步骤将首先执行，然后数据集构建步骤和嵌入获取步骤将同时执行。箭头另一侧的步骤将在这两个步骤完成后执行。
- en: Workflows are used everywhere in the IT industry. As long as you can define
    a process as a DAG of single tasks/steps, this process can be considered a workflow.
    Workflows are critical to deep learning model development. In fact, in production
    environments, most of the deep learning model–building activities are presented
    and executed as workflows.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程在IT行业的各个地方都被使用。只要你能将一个过程定义为单个任务/步骤的DAG，这个过程就可以被视为工作流程。工作流程对于深度学习模型开发至关重要。实际上，在生产环境中，大多数深度学习模型构建活动都是以工作流程的形式呈现和执行的。
- en: Note A workflow should not have a loop. To guarantee a workflow can be completed
    under any condition, its execution graph needs to be a DAG, which prevents the
    workflow execution from falling into a dead loop.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：工作流程不应该有循环。为了保证工作流程在任何条件下都能完成，其执行图需要是一个DAG，这可以防止工作流程执行陷入死循环。
- en: 9.1.2 What is workflow orchestration?
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 什么是工作流程编排？
- en: Once we define a workflow, the next step is to run the workflow. Running a workflow
    means executing the workflow steps based on the sequence defined in the workflow’s
    DAG. *Workflow orchestration* is the term we use to describe the execution and
    monitoring of the workflow.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了一个工作流程，下一步就是运行这个工作流程。运行一个工作流程意味着根据工作流程中定义的DAG的顺序执行工作流程步骤。"工作流程编排"是我们用来描述工作流程执行和监控的术语。
- en: The goal of workflow orchestration is to automate the execution of tasks defined
    in workflows. In practice, the concept of workflow orchestration often extends
    to workflow management as a whole—that is, creating, scheduling, executing, and
    monitoring multiple workflows in an automated manner.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程编排的目标是自动化执行工作流程中定义的任务。在实践中，工作流程编排的概念通常扩展到整个工作流程管理——也就是说，以自动化的方式创建、调度、执行和监控多个工作流程。
- en: 'Why do deep learning systems need workflow orchestration? Ideally, we should
    be able to code an entire deep learning project as one piece. And that’s exactly
    what we do in the prototyping phase of a project, putting all the code in a Jupyter
    notebook. So, why do we need to transform the prototyping code into a workflow
    and run it in a workflow orchestration system? The answer is twofold: automation
    and work sharing. To understand these reasons, let’s look at three sample training
    workflows in figure 9.2.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么深度学习系统需要工作流程编排？理想情况下，我们应该能够将整个深度学习项目编码为一个整体。这正是我们在项目原型阶段所做的事情，将所有代码放入Jupyter笔记本中。那么，为什么我们需要将原型代码转换为工作流程并在工作流程编排系统中运行它呢？答案是双重的：自动化和工作共享。为了理解这些原因，让我们看看图9.2中的三个示例训练工作流程。
- en: '![](../Images/09-02.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2](../Images/09-02.png)'
- en: Figure 9.2 Deep learning workflows are composed of many reusable tasks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 深度学习工作流程由许多可重用任务组成。
- en: A great benefit of using a workflow is that it turns a large chunk of code into
    a group of sharable and reusable components. In figure 9.2, we imagined three
    data scientists working on three model training projects (A, B, and C). Because
    each project’s training logic is different, data scientists developed three different
    workflows (A, B, and C) to automate their model training processes. Although each
    workflow has different DAGs, the steps in each DAG are highly overlapped. The
    total six steps are sharable and reusable. For example, the auth step (step 1)
    is the first step for all three workflows.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用工作流程的一个巨大好处是将一大块代码转换为一组可共享和可重用的组件。在图9.2中，我们想象了三位数据科学家正在从事三个模型训练项目（A、B和C）。由于每个项目的训练逻辑都不同，数据科学家开发了三个不同的工作流程（A、B和C）来自动化他们的模型训练过程。尽管每个工作流程有不同的DAG，但每个DAG中的步骤高度重叠。总共六个步骤是可共享和可重用的。例如，认证步骤（步骤1）是所有三个工作流程的第一步。
- en: Having reusable steps can greatly improve data scientists’ productivity. For
    example, to pull data from a DM service (step 2 in figure 9.2), data scientists
    need to learn how the DM web API works. But if someone already built a DM data
    pull method as a step function, scientists can just reuse this step in their workflow
    without learning how to interact with the DM service. If everyone writes their
    project in the form of a workflow, we will have lots of reusable steps, which
    will save lots of duplicate effort at an organizational level!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有可重用步骤可以大大提高数据科学家的生产力。例如，要从DM服务（图9.2中的步骤2）中提取数据，数据科学家需要学习DM网络API的工作原理。但如果有人已经将DM数据提取方法作为一个步骤函数构建，科学家们就可以在他们的工作流程中重用这个步骤，而无需学习如何与DM服务交互。如果每个人都以工作流程的形式编写他们的项目，我们将拥有大量的可重用步骤，这将在一个组织层面上节省大量的重复工作！
- en: Another reason that a workflow is well adapted to deep learning development
    is that it facilitates collaboration. Model development requires teamwork; a dedicated
    team might work on data while another team works on the training algorithm. By
    defining a complex model-building process in the workflow, we can dispatch a big
    complex project in pieces (or steps) and assign them to different teams while
    still keeping the project organized and the components in proper order. The workflow
    DAG shows the task dependencies clearly for all the project participants to see.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个原因是工作流程非常适合深度学习开发，因为它促进了协作。模型开发需要团队合作；一个专门的团队可能负责数据，而另一个团队可能负责训练算法。通过在工作流程中定义复杂的模型构建过程，我们可以将一个大型的复杂项目分解成小块（或步骤），并将它们分配给不同的团队，同时仍然保持项目的组织性和组件的正确顺序。工作流程DAG清楚地显示了所有项目参与者的任务依赖关系。
- en: In short, a good workflow orchestration system encourages work sharing, facilitates
    team collaboration, and automates complicated development scenarios. All these
    merits make workflow orchestration a crucial component of deep learning project
    development.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，一个好的工作流程编排系统鼓励工作共享，促进团队合作，并自动化复杂的发展场景。所有这些优点使工作流程编排成为深度学习项目开发的一个关键组成部分。
- en: 9.1.3 The challenges for using workflow orchestration in deep learning
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.3 在深度学习中使用工作流程编排的挑战
- en: 'In the previous section, we saw how a workflow system can provide a lot of
    benefits to deep learning project development. But there is one caveat: using
    workflows to prototype deep learning algorithm ideas is cumbersome.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了工作流程系统如何为深度学习项目开发提供很多好处。但有一个注意事项：使用工作流程来原型化深度学习算法想法是繁琐的。
- en: To understand how and why it is cumbersome, let’s look at a deep learning development
    process diagram (figure 9.3). This diagram should set the foundation for you to
    understand the challenges that workflow presents in the deep learning context.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解为什么这个过程如此繁琐，让我们看看一个深度学习开发过程图（图9.3）。这个图应该为你理解工作流程在深度学习环境中提出的挑战奠定基础。
- en: '![](../Images/09-03.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3](../Images/09-03.png)'
- en: Figure 9.3 A data scientist’s view of deep learning project development
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 数据科学家对深度学习项目开发的看法
- en: 'In figure 9.3, we see a typical deep learning project development process from
    a data scientist’s perspective. The process can be divided into two phases: the
    local incubation phase and the production phase.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在图9.3中，我们从数据科学家的角度看到了一个典型的深度学习项目开发过程。这个过程可以分为两个阶段：本地孵化阶段和生产阶段。
- en: 'In the local incubation phase, data scientists work on data exploration and
    model training prototyping at their local/dev environment. When the prototyping
    is done and the project looks promising, data scientists start to work on production
    onboarding: moving the prototyping code to the production system.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地孵化阶段，数据科学家在他们本地/开发环境中进行数据探索和模型训练原型设计。当原型完成且项目看起来很有希望时，数据科学家开始着手生产上线：将原型代码移动到生产系统。
- en: In the production phase, data scientists convert the prototyping code to a workflow.
    They break the code down into multiple steps and define a workflow DAG and then
    submit the workflow to the workflow orchestration system. After that, the orchestration
    system takes over and runs the workflow based on its schedule.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产阶段，数据科学家将原型代码转换为工作流程。他们将代码分解成多个步骤，定义一个工作流程DAG，然后将工作流程提交给工作流程编排系统。之后，编排系统接管并根据其计划运行工作流程。
- en: Gaps between prototyping and production
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 原型与生产之间的差距
- en: 'If you ask an engineer who works on workflow orchestration systems how they
    feel about the development process in figure 9.3, the answer most likely is: It’s
    pretty good! But in practice, this process is problematic for data scientists.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你问一个从事工作流程编排系统工作的工程师他们对图9.3中的开发过程有何感受，最可能的回答是：相当不错！但在实践中，这个过程对数据科学家来说是有问题的。
- en: 'From the data scientists’ point of view, once an algorithm is tested locally,
    its prototyping code should be shipped to production right away. But in figure
    9.3, we see the prototyping phase and production phase are *not* smoothly connected.
    Shipping incubation code to production is not straightforward; data scientists
    have to do extra work to construct a workflow to run their code in production.
    The gap between prototyping code and production workflow jeopardizes development
    velocity for two reasons:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据科学家的角度来看，一旦算法在本地进行了测试，其原型代码应立即部署到生产环境中。但在图9.3中，我们看到原型阶段和生产阶段并没有**顺利**连接。将孵化代码部署到生产环境并不简单；数据科学家必须额外工作来构建一个工作流程，以便在生产环境中运行他们的代码。原型代码与生产工作流程之间的差距会因以下两个原因而危及开发速度：
- en: '*Workflow building and debugging aren’t straightforward*—Data scientists normally
    face a huge learning curve when authoring model training workflows in orchestration
    systems. Learning the workflow DAG syntax, workflow libraries, coding paradigms,
    and troubleshooting is a huge burden to data scientists. The workflow troubleshooting
    is the most painful part. The majority of the orchestration system doesn’t support
    local execution, which means data scientists have to test their workflow in the
    remote orchestration system. This is hard because both the workflow environment
    and workflow execution logs are remote, so data scientists cannot easily figure
    out the root cause when a workflow execution goes wrong.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*工作流程构建和调试并不简单*——数据科学家在编排系统中编写模型训练工作流程时通常面临巨大的学习曲线。学习工作流程DAG语法、工作流程库、编码范式和故障排除对数据科学家来说是一大负担。工作流程故障排除是最痛苦的部分。大多数编排系统不支持本地执行，这意味着数据科学家必须在他们远程编排系统中测试他们的工作流程。这是困难的，因为工作流程环境和工作流程执行日志都是远程的，所以当工作流程执行出错时，数据科学家很难找出根本原因。'
- en: '*Workflow construction happens not once but frequently*—The common misperception
    is that because workflow construction only happens once, it’s fine if it is time-consuming
    and cumbersome. But the fact is, workflow construction happens continuously because
    deep learning development is an iterative process. As figure 9.3 shows, data scientists
    work on prototyping and production experimentation iteratively, so the workflow
    needs to be updated frequently to test new improvements from local to production.
    Therefore, the unpleasant and time-consuming workflow construction happens repeatedly,
    which hinders the development velocity.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*工作流程构建不是一次性的，而是频繁发生的*——常见的误解是，因为工作流程构建只发生一次，如果它耗时且繁琐，那也无所谓。但事实是，由于深度学习开发是一个迭代过程，工作流程构建是持续发生的。如图9.3所示，数据科学家会迭代地进行原型设计和生产实验，因此工作流程需要频繁更新，以从本地到生产测试新的改进。因此，不愉快且耗时的工作流程构建会反复发生，这阻碍了开发速度。'
- en: Smoothing the transition from prototyping to production
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑从原型到生产的过渡
- en: Although there are gaps, the process in figure 9.3 is good. Data scientists
    start prototyping locally with a straightforward script, and then they keep working
    on it. If the results after each iteration seem promising enough, the “straightforward
    local script” is converted to a workflow and runs in the orchestration system
    in production.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然存在差距，但图9.3中的过程是好的。数据科学家从本地使用简单的脚本开始原型设计，然后继续工作。如果每次迭代后的结果看起来足够有希望，那么“简单的本地脚本”就会被转换成工作流程，并在生产环境的编排系统中运行。
- en: The key improvement is to make the transition step from prototyping code to
    a production workflow seamless. If an orchestration system is designed for deep
    learning use cases, it should provide tools to help data scientists build workflows
    from their code with minimum effort. For example, Metaflow, an open source library
    that will be discussed in section 9.3.3, allows data scientists to authorize workflow
    by writing Python code with Python annotations. Data scientists can obtain a workflow
    from their prototyping code directly without making any changes. Metaflow also
    provides a unified user experience on model execution between local and cloud
    production environments. This eliminates the friction in workflow testing because
    Metaflow operates workflows the same way in both local and production environments.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的改进是使从原型代码到生产工作流程的过渡步骤无缝。如果一个编排系统是为深度学习用例设计的，它应该提供工具来帮助数据科学家以最小的努力从他们的代码中构建工作流程。例如，将在第9.3.3节中讨论的开源库Metaflow允许数据科学家通过编写带有Python注解的Python代码来授权工作流程。数据科学家可以直接从他们的原型代码中获得工作流程，而无需进行任何更改。Metaflow还在本地和云生产环境之间提供了统一的模型执行用户体验。这消除了工作流程测试中的摩擦，因为Metaflow在本地和生产环境中以相同的方式运行工作流程。
- en: A deep learning system should be humancentric
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习系统应以人为中心
- en: When we introduce a general-purpose tool—like workflow orchestration—to deep
    learning systems, don’t be satisfied with only enabling the functionality. Try
    to reduce human time in the system. Customization work is always possible to help
    our users be more productive.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将通用工具——如工作流程编排——引入深度学习系统时，不要满足于仅仅启用其功能。尝试减少系统中的手动时间。定制工作总是可能的，以帮助我们的用户提高生产力。
- en: Metaflow (section 9.3.3) is a good example of what happens when engineers aren't
    satisfied with just building an orchestration system to automate deep learning
    workflows. Instead, they went a step further to optimize the workflow construction
    and management to address the way data scientists work.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow（第9.3.3节）是工程师们不满足于仅仅构建一个自动化深度学习工作流程的编排系统时的一个很好的例子。相反，他们更进一步，优化了工作流程的构建和管理，以适应数据科学家的工作方式。
- en: 9.2 Designing a workflow orchestration system
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 设计工作流程编排系统
- en: In this section, we will approach the design of workflow orchestration systems
    in three steps. First, we use a typical data scientist user scenario to show how
    an orchestration system works from a user perspective. Second, we learn a generic
    orchestration system design. Third, we summarize the key design principles for
    building or evaluating an orchestration system. By reading this section, you will
    understand how orchestration systems work, in general, so you can be confident
    evaluating or working on any orchestration system.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将分三步来探讨工作流程编排系统的设计。首先，我们使用一个典型的数据科学家用户场景来展示编排系统从用户角度是如何工作的。其次，我们学习一个通用的编排系统设计。第三，我们总结构建或评估编排系统的关键设计原则。通过阅读本节，您将了解编排系统在一般情况下是如何工作的，这样您就可以自信地评估或处理任何编排系统。
- en: 9.2.1 User scenarios
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 用户场景
- en: 'Although the process of workflows varies a lot from one scenario to another,
    the user scenarios for data scientists are quite standard. Most workflow usage
    can be divided into two phases: the development phase and the execution phase.
    See figure 9.4 for a data scientist’s (Vena’s) workflow user experience. Let’s
    follow through with Vena’s user scenario in figure 9.4 step by step.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管工作流程的过程因场景而异，数据科学家的用户场景相当标准。大多数工作流程的使用可以划分为两个阶段：开发阶段和执行阶段。请参阅图9.4，了解数据科学家（Vena）的工作流程用户体验。让我们逐步跟随图9.4中Vena的用户场景。
- en: '![](../Images/09-04.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4](../Images/09-04.png)'
- en: Figure 9.4 A general deep learning user scenario of a workflow orchestration
    system
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 工作流程编排系统的一般深度学习用户场景
- en: Development phase
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 开发阶段
- en: 'During the development phase, data scientists convert their training code into
    a workflow. See Vena’s example as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发阶段，数据科学家将他们的训练代码转换为工作流程。以下为Vena的示例：
- en: Vena, a data scientist, prototypes her model training algorithm in a Jupyter
    notebook or pure Python in her local environment. After local testing and evaluation,
    Vena thinks it’s time to deploy the code to production for online experiments
    with real customer data.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据科学家Vena在她的本地环境中使用Jupyter笔记本或纯Python原型她的模型训练算法。经过本地测试和评估后，Vena认为现在是时候将代码部署到生产环境中，以使用真实客户数据进行在线实验了。
- en: Because everything running in production is a workflow, Vena needs to convert
    her prototype code to a workflow. So Vena uses the syntax provided by the orchestration
    system to rebuild her work into a DAG of tasks in a YAML (a text configuration)
    file. For example, data parsing -> data augmentation -> dataset building -> training
    -> [online evaluation, offline evaluation] -> model release.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于生产环境中运行的一切都是工作流程，Vena需要将她的原型代码转换为工作流程。因此，Vena使用编排系统提供的语法，将她的工作重新构建为YAML（一种文本配置）文件中的任务DAG。例如，数据解析
    -> 数据增强 -> 数据集构建 -> 训练 -> [在线评估，离线评估] -> 模型发布。
- en: Vena then sets the input/output parameters and actions for each step in the
    DAG. Using the training step as an example, Vena sets the step action as a RESTful
    HTTP request. This step will send a RESTful request to the model training service
    to start a training job. The payload and parameters of this request come from
    the step input parameters.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，Vena为DAG中的每个步骤设置输入/输出参数和动作。以训练步骤为例，Vena将步骤动作设置为RESTful HTTP请求。这一步骤将向模型训练服务发送RESTful请求以启动训练作业。此请求的有效负载和参数来自步骤输入参数。
- en: Once the workflow is defined, Vena sets the workflow’s execution schedule in
    the DAG YAML file. For example, Vena can schedule the workflow to run on the first
    day of every month, and she also sets the workflow to be triggered by an external
    event.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦定义了工作流程，Vena就在DAG YAML文件中设置工作流程的执行计划。例如，Vena可以安排工作流程在每月的第一天运行，她还设置了工作流程由外部事件触发。
- en: Vena runs the workflow local validation and submits the workflow to the orchestration
    service.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vena运行工作流程的本地验证，并将工作流程提交给编排服务。
- en: 'To give you an idea of what a workflow means in reality, the following code
    shows a pseudo workflow for Vena (in section 9.3, we will discuss the actual workflow
    systems):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您了解工作流程在现实中的含义，以下代码展示了Vena的伪工作流程（在第9.3节中，我们将讨论实际的工作流程系统）：
- en: '[PRE0]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ DAG definition; defines the body of the workflow, including steps and dependencies
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ DAG定义；定义工作流程的主体，包括步骤和依赖关系
- en: ❷ Executes a bash command for data augmentation
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 执行数据增强的bash命令
- en: ❸ A sequential execution flow
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 顺序执行流程
- en: Execution phase
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 执行阶段
- en: 'In the execution phase, the orchestration service executes the model training
    workflow, as illustrated by Vena’s example:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行阶段，编排服务执行模型训练工作流，正如Vena的例子所示：
- en: Once Vena’s workflow is submitted, the orchestration service saves the workflow
    DAG into a database.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦Vena的工作流提交，编排服务将工作流DAG保存到数据库中。
- en: The orchestration service’s scheduler component detects Vena’s workflow and
    dispatches the tasks of the workflow to backend workers. The scheduler will make
    sure the tasks are executed in the sequence that is defined in the workflow DAG.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编排服务的调度器组件检测Vena的工作流并将工作流任务调度到后端工作节点。调度器将确保任务按照工作流DAG中定义的顺序执行。
- en: Vena uses the orchestration service’s web UI to check the workflow’s execution
    progress and results in real time.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vena使用编排服务的Web UI实时检查工作流的执行进度和结果。
- en: If the workflow produces a good model, Vena can promote it to the staging and
    production environments. If not, Vena starts another iteration of prototyping.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果工作流产生了一个好的模型，Vena可以将其提升到预发布和生产环境。如果不是，Vena开始另一个原型化的迭代。
- en: A critical indicator of whether an orchestration system is a good fit for deep
    learning is how easy it is to convert the prototyping code into a workflow. In
    figure 9.4, we see that Vena needs to transform her training code into a workflow
    every time she prototypes a new idea. We can imagine how much human time it would
    save if we eased the friction of converting the deep learning code to a workflow.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一个编排系统是否适合深度学习的关键指标是将其原型代码转换为工作流的难易程度。在图9.4中，我们看到Vena每次原型化一个新想法时都需要将她的训练代码转换为工作流。我们可以想象，如果我们简化将深度学习代码转换为工作流的摩擦，将节省多少人力。
- en: Note A workflow should always be lightweight. The workflow is used to automate
    a process; its goal is to group and connect a series of tasks and execute them
    in a defined sequence. The great benefit of using a workflow is that people can
    share and reuse the tasks, so they can automate their process faster. Therefore,
    the workflow itself shouldn’t do any heavy computation, the real work should be
    done by the tasks of the workflow.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：工作流应该始终轻量。工作流用于自动化一个过程；其目标是分组和连接一系列任务，并按定义的顺序执行它们。使用工作流的一个巨大好处是人们可以共享和重用任务，因此他们可以更快地自动化他们的流程。因此，工作流本身不应该进行任何重量级的计算，真正的工作应该由工作流的任务来完成。
- en: 9.2.2 A general orchestration system design
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 通用编排系统设计
- en: Let’s now turn to a generic workflow orchestration system. To help you understand
    how an orchestration system works and how to research open source orchestration
    systems, we prepared a high-level system design. By zooming out of the detailed
    implementation and only keeping the core components, this design is applicable
    to most orchestration systems, including open source systems, which will be discussed
    in section 9.3\. See figure 9.5 for the design proposal.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转向一个通用的工作流编排系统。为了帮助您理解编排系统是如何工作的以及如何研究开源编排系统，我们准备了一个高级系统设计。通过从详细的实现中抽离出来，只保留核心组件，这个设计适用于大多数编排系统，包括将在第9.3节中讨论的开源系统。请参阅图9.5以了解设计提案。
- en: '![](../Images/09-05.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-05.png)'
- en: Figure 9.5 A design overview for a generic workflow orchestration service
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 通用工作流编排服务的设计概述
- en: 'A workflow orchestration system generally consists of the following five components:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一个工作流编排系统通常由以下五个组件组成：
- en: '*Web server*—The web server presents a web user interface and a set of web
    APIs for users to create, inspect, trigger, and debug the behavior of a workflow.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Web服务器*——Web服务器提供了一个Web用户界面和一组Web API，供用户创建、检查、触发和调试工作流的行为。'
- en: '*Scheduler and controller*—The scheduler and controller component does two
    things. First, the scheduler watches every active workflow in the system, and
    it schedules the workflow to run when the time is right. Second, the controller
    dispatches the workflow tasks to workers. Although the scheduler and controller
    are two different function units, they usually are implemented together because
    they are all related to workflow execution.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*调度器和控制器*——调度器和控制器组件做两件事。首先，调度器监视系统中的每个活动工作流，并在适当的时间安排工作流运行。其次，控制器将工作流任务调度到工作节点。尽管调度器和控制器是两个不同的功能单元，但它们通常一起实现，因为它们都与工作流执行相关。'
- en: '*Metadata database*—The metadata database stores the workflows’ configuration,
    DAG, editing and execution history, and the tasks’ execution state.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*元数据数据库*—元数据数据库存储工作流的配置、DAG、编辑和执行历史，以及任务的执行状态。'
- en: '*Worker group*—The worker group provides the compute resource to run workflow
    tasks. The worker abstracts the infrastructure and is agnostic to the task that’s
    running. For example, we might have different types of workers, such as a Kubernetes
    worker and an Amazon Elastic Compute Cloud (EC2) worker, but they can all execute
    the same task, albeit on different infrastructures.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*工作组*—工作组为运行工作流任务提供计算资源。工作组抽象了基础设施，对正在运行的任务不敏感。例如，我们可能有不同类型的工人，如Kubernetes工作组和Amazon弹性计算云（EC2）工作组，但它们都可以执行相同的任务，尽管是在不同的基础设施上。'
- en: '*Object store*—The object store is shared file storage for all other components;
    it’s normally built on top of cloud object storage, such as Amazon Simple Storage
    Service (S3). One usage of an object store is task output sharing. When a worker
    runs a task, it reads the output value of the previous task from the object store
    as the task input; the worker also saves the task output to the object store for
    its successor tasks.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对象存储*—对象存储是所有其他组件的共享文件存储；它通常建立在云对象存储之上，如Amazon Simple Storage Service (S3)。对象存储的一个用途是任务输出共享。当工作组运行一个任务时，它从对象存储中读取前一个任务的输出值作为任务输入；工作组还将任务输出保存到对象存储中，供后续任务使用。'
- en: Both the object store and the metadata database are accessible to all the components
    of the orchestration system, including the scheduler, web server, and workers’
    components. Having centralized data storage decouples the core components, so
    the web server, scheduler, and workers can work independently.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对象存储和元数据数据库对所有编排系统的组件都是可访问的，包括调度器、Web服务器和工作组组件。集中式数据存储解耦了核心组件，因此Web服务器、调度器和工人可以独立工作。
- en: How is a workflow executed?
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流是如何执行的？
- en: First, Vena defines the DAG for the workflow. Inside the DAG, Vena declares
    a set of tasks and defines the control flow of the task execution sequence. For
    each task, Vena either uses the system’s default operator, such as a Shell command
    operator or Python operator, or builds her own operator to execute tasks.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Vena定义工作流的DAG。在DAG内部，Vena声明一组任务并定义任务执行序列的控制流。对于每个任务，Vena要么使用系统的默认操作符，如Shell命令操作符或Python操作符，要么构建自己的操作符来执行任务。
- en: Second, Vena submits the workflow—DAG with dependent code—to the web server
    through the web UI or command line. The workflow is saved in the metadata database.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，Vena通过Web UI或命令行将工作流提交给Web服务器—包含依赖代码的DAG。工作流保存在元数据数据库中。
- en: Third, the scheduler periodically (every few seconds or minutes) scans the metadata
    database and detects the new workflow; it then kicks off the workflow at the scheduled
    time. To execute a workflow, the scheduler calls the controller component to dispatch
    the workflow’s tasks to the worker queue based on the task sequence defined in
    DAG.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，调度器定期（每几秒或几分钟）扫描元数据数据库并检测新的工作流；然后它在预定时间启动工作流。为了执行工作流，调度器调用控制器组件，根据DAG中定义的任务序列将工作流的任务调度到工作队列。
- en: Fourth, a worker picks up a task from the shared job queue; it reads the task
    definition from the metadata database and executes the task by running the task’s
    operator. During the execution, the worker saves the task’s output value to the
    object store and reports the task’s execution status back to the metadata database.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 第四，一个工作组从共享作业队列中提取一个任务；它从元数据数据库中读取任务定义并通过运行任务的操作符来执行任务。在执行过程中，工作组将任务的输出值保存到对象存储中，并将任务的执行状态报告回元数据数据库。
- en: Last but not least, Vena uses the web UI hosted on the web server component
    to monitor the workflow execution. Because both the scheduler/controller components
    and the workers report the status to the metadata database in real time, the web
    UI always displays the latest workflow status.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，Vena使用托管在Web服务器组件上的Web UI来监控工作流的执行。因为调度器/控制器组件和工作组都实时向元数据数据库报告状态，所以Web
    UI始终显示最新的工作流状态。
- en: 9.2.3 Workflow orchestration design principles
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.3 工作流编排设计原则
- en: Because we have seen how a workflow orchestration system works internally and
    externally, now it’s time to examine the design principles that make an orchestration
    system outstanding for deep learning scenarios. We hope you can use the principles
    here as a guide to evolving your system or for evaluating open source approaches.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经了解了工作流编排系统内部和外部的运作方式，现在是时候检查那些使编排系统在深度学习场景中表现出色的设计原则了。我们希望您可以将这里的原则作为指导您系统演变或评估开源方法的指南。
- en: Note The workflow orchestration system is one of the most complicated components
    in a deep learning system in terms of engineering effort, so don't worry too much
    about making your system match perfectly with these principles in the first few
    versions.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在工作流编排系统中，从工程角度来看，它是深度学习系统中最复杂的组件之一，因此在最初几个版本中，不必过于担心您的系统与这些原则完全匹配。
- en: 'Principle 1: Criticality'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 1：关键性
- en: Workflow orchestration is essentially a job scheduling challenge, so the bottom
    line for any orchestration system is to provide a solid workflow execution experience.
    A valid workflow should always be executed correctly, repeatedly, and on schedule.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流编排本质上是一个作业调度挑战，因此任何编排系统的底线是提供良好的工作流执行体验。一个有效的工作流应该始终被正确、重复且按时执行。
- en: 'Principle 2: Usability'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 2：可用性
- en: The usability measurement of an orchestration system in a deep learning context
    is whether it optimizes data scientists’ productivity. Most data scientist interactions
    in an orchestration system are workflow creation, testing, and monitoring. So
    a user-friendly orchestration system should let users create, monitor, and troubleshoot
    workflows easily.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习环境中，编排系统的可用性衡量标准是它是否优化了数据科学家的生产力。在编排系统中，大多数数据科学家的交互都是工作流创建、测试和监控。因此，一个用户友好的编排系统应该让用户能够轻松地创建、监控和调试工作流。
- en: 'Principle 3: Extensibility'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 3：可扩展性
- en: To cater to the wide variety of deep learning infrastructures, people should
    easily define their own task operators and executors without worrying about where
    they are deployed to. The orchestration system should provide the level of abstraction
    that suits your environment, whether if it’s Amazon EC2 or Kubernetes.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应各种深度学习基础设施，人们应该能够轻松定义自己的任务操作符和执行器，而无需担心它们部署的位置。编排系统应该提供适合您环境的抽象级别，无论是 Amazon
    EC2 还是 Kubernetes。
- en: 'Principle 4: Isolation'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 4：隔离
- en: 'Two types of isolations can occur that are critical: workflow creation isolation
    and workflow execution isolation. Workflow creation isolation means people can
    not interfere with each other when creating workflows. For example, if Vena submits
    an invalid workflow DAG or releases a new version of a common shared library that’s
    referenced in other workflows, the existing workflows shouldn’t be affected.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 可能发生两种关键性的隔离：工作流创建隔离和工作流执行隔离。工作流创建隔离意味着人们在创建工作流时不能相互干扰。例如，如果 Vena 提交了一个无效的工作流
    DAG 或发布了一个被其他工作流引用的公共共享库的新版本，现有工作流不应受到影响。
- en: Workflow execution isolation means that each workflow is running in an isolated
    environment. There should be no resource competition between workflows, and failure
    of a workflow won’t affect other workflow’s executions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流执行隔离意味着每个工作流都在一个隔离的环境中运行。工作流之间不应存在资源竞争，一个工作流的失败不应影响其他工作流的执行。
- en: 'Principle 5: Scaling'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 5：可扩展性
- en: 'A good orchestration system should address the following two scaling problems:
    handling large numbers of concurrent workflows and handling large expansive workflows.
    Concurrent workflow scaling generally means that given enough compute resources—for
    example, adding more workers to the worker group—the orchestration system can
    cater to an infinite concurrent number of workflow executions. Also, the system
    should always keep the service-level agreement (SLA) for every workflow. For example,
    a workflow should be executed at its scheduled time and no later than 2 seconds,
    regardless of how many other workflows are executing.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的编排系统应该解决以下两个可扩展性问题：处理大量并发工作流和处理大型扩展工作流。并发工作流的可扩展性通常意味着，在提供足够的计算资源的情况下——例如，向工作组添加更多工作者——编排系统可以满足无限数量的并发工作流执行。此外，系统应始终为每个工作流保持服务级别协议（SLA）。例如，一个工作流应在预定时间执行，且不超过
    2 秒，无论有多少其他工作流正在执行。
- en: For single, large workflow scaling, the system should encourage users not to
    worry about performance, so they can focus on readable, straightforward code,
    and easy operations. When the workflow execution hits a limit—for example, the
    training operators take too long to execute—the orchestration system should provide
    some horizontal parallelism operators, such as distributed training operators,
    to address single workflow performance problems.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个、大型工作流程的扩展，系统应鼓励用户不要担心性能，这样他们可以专注于可读性高、直接易懂的代码，以及易于操作。当工作流程执行达到极限时——例如，训练操作执行时间过长——编排系统应提供一些横向并行操作符，例如分布式训练操作符，以解决单个工作流程的性能问题。
- en: The main scaling idea for deep learning orchestration is that we should solve
    the performance problem at the system level and avoid asking users to write code
    with scalability in mind. This can lead to worse readability, harder debuggability,
    and increased operational burden.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习编排的主要扩展思想是我们应该在系统级别解决性能问题，避免要求用户编写考虑可扩展性的代码。这可能导致可读性更差、调试更困难，以及增加运营负担。
- en: 'Principle 6: Human-centric support for both prototyping and production'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 6：支持原型设计和生产的以人为中心
- en: The capability of connecting the data scientist’s local prototyping code to
    the production workflow is a requirement specific to deep learning. It’s a key
    indicator that we use to evaluate whether an orchestration system is a good fit
    for deep learning systems.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据科学家的本地原型代码连接到生产工作流程的能力是深度学习特有的要求。这是我们用来评估编排系统是否适合深度学习系统的一个关键指标。
- en: An orchestration system designed for deep learning will respect that deep learning
    project development is an iterative, ongoing effort from prototyping to production.
    Therefore, it will make a dedicated effort to help data scientists convert their
    local prototype code to production workflow in a seamless fashion.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为深度学习设计的编排系统会尊重深度学习项目开发是一个从原型设计到生产的迭代、持续努力的过程。因此，它将专门努力帮助数据科学家以无缝的方式将他们的本地原型代码转换为生产工作流程。
- en: 9.3 Touring open source workflow orchestration systems
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 探索开源工作流程编排系统
- en: 'In this section, we will introduce three battle-tested workflow orchestration
    systems: Airflow, Argo Workflows, and Metaflow. These three open source systems
    are widely adopted in the IT industry and backed by active communities. In addition
    to introducing them generally, we also evaluate these workflow systems from the
    perspective of deep learning project development.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍三个经过实战考验的工作流程编排系统：Airflow、Argo Workflows 和 Metaflow。这三个开源系统在 IT 行业中得到广泛应用，并得到活跃社区的支撑。除了一般介绍它们之外，我们还从深度学习项目开发的角度评估了这些工作流程系统。
- en: To make a fair comparison, we implement pseudocode for the same workflow in
    Airflow, Argo Workflows, and Metaflow. Basically, if there is new data, we initially
    transform the data and save it to a new table in the database, and then we notify
    the data science team. Also, we expect the workflow to run daily.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行公平的比较，我们在 Airflow、Argo Workflows 和 Metaflow 中实现了相同工作流程的伪代码。基本上，如果有新数据，我们最初将数据转换并保存到数据库中的新表中，然后通知数据科学团队。我们还期望工作流程每天运行。
- en: 9.3.1 Airflow
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 Airflow
- en: Airflow ([https://airflow.apache.org/docs/apache-airflow/stable/index.html](https://airflow.apache.org/docs/apache-airflow/stable/index.html))
    was created in 2014 at Airbnb and is now a part of the Apache Foundation. Airflow
    is a platform to programmatically author, schedule, and monitor workflows. Airflow
    is not designed for deep learning use cases; it was originally built to orchestrate
    the increasingly complex ETL (extract, transform, load) pipelines (or data pipelines).
    But because of Airflow’s good extensibility, production quality, and GUI support,
    it’s widely used in many other domains, including deep learning. As this book
    is written, Airflow is the most adopted orchestration system.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow ([https://airflow.apache.org/docs/apache-airflow/stable/index.html](https://airflow.apache.org/docs/apache-airflow/stable/index.html))
    是在 2014 年由 Airbnb 创建的，现在是 Apache 基金会的一部分。Airflow 是一个用于程序化创建、调度和监控工作流程的平台。Airflow
    并非为深度学习用例而设计；它最初是为了编排日益复杂的 ETL（提取、转换、加载）管道（或数据管道）而构建的。但由于 Airflow 良好的可扩展性、生产质量和
    GUI 支持，它在许多其他领域得到了广泛应用，包括深度学习。在本书撰写时，Airflow 是最广泛采用的编排系统。
- en: A typical use case
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的用例
- en: Building a workflow in Airflow takes two steps. First, define the workflow DAG
    and tasks. Second, declare the task dependencies in the DAG. An Airflow DAG is
    essentially Python code. See the following listing for how our sample workflow
    is implemented in Airflow.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在Airflow中构建工作流需要两个步骤。首先，定义工作流DAG和任务。其次，在DAG中声明任务依赖关系。Airflow DAG本质上是一种Python代码。以下列出我们的示例工作流在Airflow中的实现方式。
- en: Listing 9.1 A sample Airflow workflow definition
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.1 一个示例Airflow工作流定义
- en: '[PRE1]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Checks whether a new file arrives
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 检查是否有新文件到达
- en: ❷ The actual logic is implemented in the "transform_data" function.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 实际逻辑是在“transform_data”函数中实现的。
- en: ❸ The PostgresOperator is a predefined airflow operator for interacting with
    postgres db.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ PostgresOperator是一个用于与PostgreSQL数据库交互的预定义Airflow操作员。
- en: In code listing 9.1, we see the sample workflow DAG consists of multiple tasks,
    such as `create_table` and `save_into_db`. A task in Airflow is implemented as
    an operator. There are lots of predefined and community-managed operators, such
    as MySqlOperator, SimpleHttpOperator, and Docker operator.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码列表9.1中，我们看到示例工作流DAG由多个任务组成，例如`create_table`和`save_into_db`。Airflow中的任务实现为一个操作员。有许多预定义和社区管理的操作员，例如MySqlOperator、SimpleHttpOperator和Docker操作员。
- en: 'Airflow’s predefined operators help users implement tasks without coding. You
    can also use the PythonOperator to run your customized Python functions. Once
    the workflow DAG is constructed and all the code is deployed to Airflow, we can
    use the UI or the following CLI command to check workflow execution status; see
    some sample shell commands as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow的预定义操作员帮助用户在不编写代码的情况下实现任务。您还可以使用PythonOperator运行您自定义的Python函数。一旦工作流DAG构建完成，并将所有代码部署到Airflow，我们就可以使用UI或以下CLI命令来检查工作流执行状态；以下是一些示例shell命令：
- en: '[PRE2]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Prints all active DAG
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 打印所有活动DAG
- en: ❷ Prints the list of tasks in the "data_process_dag" DAG
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打印“data_process_dag”DAG中的任务列表
- en: ❸ Prints the hierarchy of tasks in the "data_process_dag" DAG
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打印“data_process_dag”DAG中任务的层次结构
- en: If you want to learn more about Airflow, you can check out its architecture
    overview doc and tutorials ([http://mng.bz/Blpw](http://mng.bz/Blpw)).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于Airflow的信息，你可以查看其架构概述文档和教程（[http://mng.bz/Blpw](http://mng.bz/Blpw)）。
- en: Key Features
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 关键特性
- en: 'Airflow offers the following key features:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow提供的以下关键特性：
- en: '*DAGs*—Airflow abstracts complex workflow using DAGs, and the workflow DAG
    is implemented through a Python library.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DAGs*—Airflow使用DAG抽象复杂的流程，工作流DAG通过Python库实现。'
- en: '*Programmatic workflow management*—Airflow supports creating tasks on the fly
    and allows the creation of complex dynamic workflows.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*程序化工作流管理*—Airflow支持动态创建任务，并允许创建复杂的动态工作流。'
- en: '*Great built-in operators to help build automation*—Airflow offers lots of
    predefined operators that help users achieve tasks without coding.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*强大的内置操作员以帮助构建自动化*—Airflow提供了许多预定义的操作员，帮助用户在不编写代码的情况下完成任务。'
- en: '*Solid task dependency and execution management*—Airflow has an auto-retry
    policy built into every task, and it provides different types of sensors to handle
    run-time dependencies, such as detecting task completion, workflow run status
    change, and file presence.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*坚固的任务依赖和执行管理*—Airflow在每个任务中都内置了自动重试策略，并提供不同类型的传感器来处理运行时依赖关系，例如检测任务完成、工作流运行状态更改和文件存在。'
- en: '*Extensibility*—Airflow makes its sensors, hooks, and operators fully extendable,
    which allows it to benefit from a large amount of community-contributed operators.
    Airflow can also be easily integrated into different systems by adding customized
    operators.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可扩展性*—Airflow使其传感器、钩子和操作员完全可扩展，这使其能够从大量社区贡献的操作员中受益。Airflow还可以通过添加自定义操作员轻松集成到不同的系统中。'
- en: '*Monitoring and management interface*—Airflow provides a powerful UI so users
    can get a quick overview of workflow/task execution status and history. Users
    can also trigger and clear tasks or workflow runs from the UI.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*监控和管理界面*—Airflow提供了一个强大的UI，用户可以快速了解工作流/任务执行状态和历史记录。用户还可以从UI中触发和清除任务或工作流运行。'
- en: '*Production quality*—Airflow provides many useful tools for maintaining the
    service in production environments, such as task log searching, scaling, alerting,
    and restful APIs.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生产质量*—Airflow提供了许多用于在生产环境中维护服务的有用工具，例如任务日志搜索、扩展、警报和RESTful API。'
- en: Limitations
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 局限性
- en: 'Although Airflow is a great workflow orchestration, we still see several disadvantages
    when using it for deep learning scenarios:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Airflow是一个出色的工作流编排工具，但在用于深度学习场景时，我们仍然发现了一些缺点：
- en: '*High upfront cost for data scientists to onboard*—Airflow has a steep learning
    curve to achieve tasks that are not supported by the built-in operators. Also,
    there is no easy way to do workflow local testing.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据科学家上手的初始成本高*——Airflow有一个陡峭的学习曲线才能完成内置操作符不支持的任务。此外，没有简单的方法来进行工作流程本地测试。'
- en: '*High friction when moving deep learning prototyping code to production*—When
    we apply Airflow to deep learning, data scientists have to convert their local
    model training code into Airflow DAG. This is extra work, and it’s an unpleasant
    experience for data scientists, especially when considering that this is avoidable
    if we build workflow DAG from the model training code directly.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将深度学习原型代码迁移到生产环境时的摩擦力高*——当我们应用Airflow到深度学习时，数据科学家必须将他们的本地模型训练代码转换为Airflow
    DAG。这是一项额外的工作，对于数据科学家来说是一种不愉快的体验，尤其是考虑到如果我们直接从模型训练代码构建工作流程DAG，这是可以避免的。'
- en: '*High complexity when operating on Kubernetes* —Deploying and operating Airflow
    on Kubernetes is not straightforward. If you are looking to adopt an orchestration
    system to run on Kubernetes, Argo Workflows is a better choice.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在Kubernetes上操作时的复杂性高*——在Kubernetes上部署和操作Airflow并不简单。如果你正在寻找一个在Kubernetes上运行的编排系统，Argo
    Workflows是一个更好的选择。'
- en: 9.3.2 Argo Workflows
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 Argo Workflows
- en: Argo Workflows is an open source, container-native workflow engine for orchestrating
    parallel workflows/tasks on Kubernetes. Argo Workflows solves the same problem
    that Airflow addresses but in a different way; it takes a Kubernetes-native approach.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Argo Workflows是一个开源的、容器本地的用于在Kubernetes上编排并行工作流程/任务的流程引擎。Argo Workflows解决了Airflow所解决的问题，但以不同的方式；它采用Kubernetes原生方法。
- en: The biggest difference between Argo Workflows and Airflow is that Argo Workflows
    is built natively on Kubernetes. More specifically, the workflows and tasks in
    Argo Workflows are implemented as Kubernetes custom resource definition (CRD)
    objects, and each task (step) is executed as a Kubernetes pod. See figure 9.6
    for a high-level system overview.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Argo Workflows与Airflow之间最大的区别在于Argo Workflows是原生构建在Kubernetes之上的。更具体地说，Argo
    Workflows中的工作流程和任务被实现为Kubernetes自定义资源定义（CRD）对象，每个任务（步骤）都作为一个Kubernetes pod执行。参见图9.6以获取高级系统概述。
- en: '![](../Images/09-06.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/09-06.png)'
- en: Figure 9.6 The workflow and its steps in Argo Workflows are executed as Kubernetes
    pods.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 Argo Workflows的工作流程及其步骤作为Kubernetes pod执行。
- en: In figure 9.6, Vena (the data scientist) first defines a workflow and its steps/tasks
    as a Kubernetes CRD object, which is usually presented as a YAML file. Then she
    submits the workflow to Argo Workflows, and its controller creates CRD objects
    inside the Kubernetes cluster. Next, Kubernetes pods are launched dynamically
    to run the workflow steps/tasks in the workflow sequence.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在图9.6中，Vena（数据科学家）首先将工作流程及其步骤/任务定义为Kubernetes CRD对象，这通常以YAML文件的形式呈现。然后她将工作流程提交给Argo
    Workflows，其控制器在Kubernetes集群内部创建CRD对象。接下来，Kubernetes pod会动态启动以按工作流程顺序运行工作流程步骤/任务。
- en: You may also notice that each step’s execution is completely isolated by container
    and pod; each step uses files to present its input and output values. Argo Workflows
    will magically mount the dependent file into the step’s container.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还会注意到，每个步骤的执行完全由容器和pod隔离；每个步骤使用文件来展示其输入和输出值。Argo Workflows会神奇地将依赖文件挂载到步骤的容器中。
- en: The task isolation created by the Kubernetes pod is a great advantage of Argo
    Workflows. Simplicity is also another reason people choose Argo Workflows. If
    you understand Kubernetes, Argo’s installation and troubleshooting are straightforward.
    We can use either Argo Workflows commands or the standard Kubernetes CLI commands
    to debug the system.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes pod创建的任务隔离是Argo Workflows的一个巨大优势。简单性也是人们选择Argo Workflows的另一个原因。如果你理解Kubernetes，Argo的安装和故障排除都是直接的。我们可以使用Argo
    Workflows命令或标准的Kubernetes CLI命令来调试系统。
- en: A typical use case
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 典型用例
- en: For a better understanding, let’s look at an Argo Workflows example. In this
    section, we use Argo Workflows to automate the same data processing work we saw
    in the previous Airflow section. The workflow includes checking new data first,
    transforming the data, saving it to a new table in the database, and then notifying
    the data scientist team by Slack. See the following code listing for the Argo
    Workflows definition.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，让我们看看一个Argo Workflows的例子。在本节中，我们使用Argo Workflows来自动化我们在之前Airflow部分中看到的数据处理工作。该工作流程包括首先检查新数据，转换数据，将其保存到数据库中的新表中，然后通过Slack通知数据科学家团队。请参见以下代码列表以获取Argo
    Workflows的定义。
- en: Listing 9.2 A sample workflow for Argo Workflows with a series of steps
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.2 Argo Workflows 的一个示例工作流，包含一系列步骤
- en: '[PRE3]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Claims the CRD object type as workflow
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 声明 CRD 对象类型为工作流
- en: ❷ Declares the steps of the workflow
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 声明工作流的步骤
- en: ❸ The step body is defined as another template, similar to a function.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 步骤体被定义为另一个模板，类似于函数。
- en: ❹ Declares the data-paths artifact is from the new-data-paths artifact generated
    by the check-new-data step
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 声明 data-paths 工件来自由 check-new-data 步骤生成的 new-data-paths 工件
- en: ❺ This is how steps pass parameters.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 这就是步骤如何传递参数。
- en: ❻ The actual step definition, similar to a function implementation
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 实际步骤定义，类似于函数实现
- en: ❼ Declares an output artifact (generates new-data-paths) for this step; the
    artifact is from /tmp/data_paths.txt, which can also be a directory.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 声明此步骤的输出工件（生成 new-data-paths），该工件来自 /tmp/data_paths.txt，它也可以是一个目录。
- en: ❽ Unpacks the data_paths input artifact and puts it at /tmp/raw_data/data_paths.txt
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 解包 data_paths 输入工件并将其放置在 /tmp/raw_data/data_paths.txt
- en: The most fundamental concepts in Argo Workflows are the workflow and template.
    A workflow object represents a single instance of a workflow; it contains the
    workflow’s definition and execution state. We should treat a workflow as a “live”
    object. A template can be thought of as *functions*; they define instructions
    to be executed. The `entrypoint` field defines what the main function will be,
    meaning the template that will be executed first.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Argo Workflows 中最基本的概念是工作流和模板。工作流对象代表工作流的单个实例；它包含工作流的定义和执行状态。我们应该将工作流视为一个“活”对象。模板可以被视为*函数*；它们定义要执行的指令。`entrypoint`
    字段定义了主函数将是什么，即首先执行的那个模板。
- en: 'In code listing 9.2, we see a four-step sequential workflow: `check-new-data`
    -> `transform_data` -> `save-into-db` -> `notify-data-science-team`. Each step
    can reference a template, and steps pass parameters via artifacts (files). For
    example, the `check-new-data` references the `data-checker` template, which defines
    the Docker image for checking whether there is new data. The `data-checker` template
    also declares that the step output—the newly arrived data file path—will be saved
    to `/tmp/data_paths.txt` as its output value.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码列表 9.2 中，我们看到一个四步骤的顺序工作流：`check-new-data` -> `transform_data` -> `save-into-db`
    -> `notify-data-science-team`。每个步骤都可以引用一个模板，并且步骤通过工件（文件）传递参数。例如，`check-new-data`
    引用了 `data-checker` 模板，该模板定义了检查是否有新数据的 Docker 镜像。`data-checker` 模板还声明步骤输出——新到达的数据文件路径——将作为其输出值保存到
    `/tmp/data_paths.txt`。
- en: 'Next, the step `transform_data` binds the output of the `check-new-data` to
    the input of the data-converter template. This is how variables move around between
    steps and templates. Once you submit the workflow—for example, `argo` `submit`
    `-n` `argo` `sample_workflow.yaml`—you can either use the Argo Workflows UI or
    the following commands to review the details of the workflow run:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，步骤 `transform_data` 将 `check-new-data` 的输出绑定到数据转换模板的输入。这就是变量如何在步骤和模板之间移动的方式。一旦提交工作流——例如，`argo
    submit -n argo sample_workflow.yaml`——您可以使用 Argo Workflows UI 或以下命令来查看工作流运行的详细信息：
- en: '[PRE4]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Besides using the `argo` command, we can also use the Kubernetes CLI command
    to check the workflow execution because Argo Workflows runs natively on Kubernetes;
    see the following example:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用 `argo` 命令外，我们还可以使用 Kubernetes CLI 命令来检查工作流执行，因为 Argo Workflows 在 Kubernetes
    上是原生运行的；请参阅以下示例：
- en: '[PRE5]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To learn more about Argo Workflows, you can check out Argo Workflows user guide
    ([http://mng.bz/WAG0](http://mng.bz/WAG0)) and Argo Workflows architecture graph
    ([https://argoproj.github.io/argo-workflows/architecture](https://argoproj.github.io/argo-workflows/architecture)).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 Argo Workflows 的信息，您可以查看 Argo Workflows 用户指南([http://mng.bz/WAG0](http://mng.bz/WAG0))
    和 Argo Workflows 架构图([https://argoproj.github.io/argo-workflows/architecture](https://argoproj.github.io/argo-workflows/architecture))。
- en: 'Code Dockerization: Easy production deployment'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 代码 Docker 化：易于生产部署
- en: Argo Workflows is essentially a Kubernetes pod (Docker images) scheduling system.
    Although it forces people to write their code into a series of Docker images,
    it creates great flexibility and isolation inside the orchestration system. Because
    the code is in Docker form, it can be executed by any worker without worrying
    about configuring the worker environments.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Argo Workflows 实质上是一个 Kubernetes pod（Docker 镜像）调度系统。尽管它强制人们将代码写入一系列 Docker 镜像中，但在编排系统中它创造了极大的灵活性和隔离性。因为代码是以
    Docker 形式存在的，所以它可以由任何工作节点执行，无需担心配置工作节点环境。
- en: Another advantage to Argo Workflows is its low cost of production deployment.
    When you test your code locally in Docker, the Docker image (prototyping code)
    can be used directly in Argo Workflows. Unlike Airflow, Argo Workflows has almost
    no conversion effort from prototyping code to production workflow.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Argo Workflows 的另一个优点是其生产部署的低成本。当你在本地的 Docker 中测试代码时，可以直接在 Argo Workflows 中使用
    Docker 镜像（原型代码）。与 Airflow 不同，Argo Workflows 从原型代码到生产工作流的转换工作几乎为零。
- en: Key Features
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 关键特性
- en: 'Argo Workflows offers the following key features:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Argo Workflows 提供以下关键特性：
- en: '*Low cost of installation and maintenance*—Argo Workflows runs natively on
    Kubernetes, so you can just use the Kubernetes process to troubleshoot any problems;
    no need to learn other tools. Also, its installation is very straightforward;
    with a few `kubectl` commands, you can get Argo Workflows running in a Kubernetes
    environment.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*安装和维护成本低*—Argo Workflows 在 Kubernetes 上原生运行，因此你可以仅使用 Kubernetes 进程来排查任何问题；无需学习其他工具。此外，其安装非常简单；只需几个
    `kubectl` 命令，你就可以在 Kubernetes 环境中运行 Argo Workflows。'
- en: '*Robust workflow execution*—The Kubernetes pod creates great isolation for
    Argo Workflows’ task execution. Argo Workflows also supports cron workflow and
    task retry.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*鲁棒的流程执行*—Kubernetes Pod 为 Argo Workflows 的任务执行提供了良好的隔离。Argo Workflows 还支持
    cron 工作流和任务重试。'
- en: '*Templating and composability*—Argo Workflows templates are like functions.
    When building a workflow, Argo Workflows supports composing different templates
    (step functions). This composability encourages sharing the common work across
    teams, thus greatly improving productivity.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模板化和可组合性*—Argo Workflows 模板类似于函数。在构建工作流时，Argo Workflows 支持组合不同的模板（步骤函数）。这种可组合性鼓励团队间共享常见工作，从而大大提高生产力。'
- en: '*Fully featured UI*—Argo Workflows offers a convenient UI to manage the entire
    life cycle of a workflow, such as submitting/stopping a workflow, listing all
    workflows, and viewing workflow definitions.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*功能齐全的 UI*—Argo Workflows 提供了一个方便的 UI 来管理工作流的整个生命周期，例如提交/停止工作流、列出所有工作流以及查看工作流定义。'
- en: '*Highly flexible and applicable*—Argo Workflows defines REST APIs to manage
    the system and add new capabilities (plugins), and workflow tasks are defined
    as Docker images. These features make Argo Workflows highly customizable and used
    widely in many domains, such as ML, ETL, batch/data processing, and CI/CD (continuous
    integration and continuous delivery/continuous deployment).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*高度灵活和适用性*—Argo Workflows 定义了 REST API 来管理系统并添加新功能（插件），工作流任务被定义为 Docker 镜像。这些特性使得
    Argo Workflows 非常可定制，并在许多领域得到广泛应用，例如机器学习（ML）、ETL、批量/数据处理以及 CI/CD（持续集成和持续交付/持续部署）。'
- en: '*Production quality*—Argo Workflows is designed to run in a serious production
    environment. Kubeflow pipeline and Argo CD are great examples of productionizing
    Argo Workflows.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*生产质量*—Argo Workflows 是为在严肃的生产环境中运行而设计的。Kubeflow 管道和 Argo CD 是将 Argo Workflows
    商业化的优秀例子。'
- en: Limitations
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 局限性
- en: 'The downsides of using Argo Workflows in deep learning systems are as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习系统中使用 Argo Workflows 的缺点如下：
- en: '*Everyone will write and maintain YAML files*—Argo Workflows demands that the
    workflow is defined as a Kubernetes CRD in a YAML file. A short YAML file for
    a single project is manageable, but once the number of workflows starts increasing
    and workflow logic becomes more complex, the YAML file can become long and confusing.
    Argo Workflows offers templates to keep the workflow definition simple, but it’s
    still not very intuitive unless you are accustomed to working with Kubernetes
    YAML configurations.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*每个人都需要编写和维护 YAML 文件*—Argo Workflows 要求将工作流定义为 YAML 文件中的 Kubernetes CRD。对于单个项目来说，一个简短的
    YAML 文件是可管理的，但一旦工作流数量开始增加且工作流逻辑变得更加复杂，YAML 文件可能会变得很长且难以理解。Argo Workflows 提供模板以保持工作流定义简单，但除非你习惯于使用
    Kubernetes YAML 配置工作，否则它仍然不太直观。'
- en: '*Must be an expert on Kubernetes*—You will feel like it’s second nature if
    you are an expert on Kubernetes. But a novice user may need to spend quite some
    time learning Kubernetes concepts and practices.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*必须是 Kubernetes 专家*—如果你是 Kubernetes 专家，这会感觉像是一种本能。但对于新手用户来说，可能需要花费相当多的时间来学习
    Kubernetes 概念和实践。'
- en: '*Task execution latency*—In Argo Workflows, for every new task, Argo will launch
    a new Kubernetes pod to execute it. The pod launching can introduce seconds or
    minutes to every single task execution, which limits Argo when supporting time-sensitive
    workflows. For example, Argoflow is not a good fit for real-time model prediction
    workflow, which runs model prediction requests with millisecond SLAs.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*任务执行延迟*——在Argo Workflows中，对于每个新任务，Argo都会启动一个新的Kubernetes pod来执行它。pod的启动可能会为每个任务执行引入几秒或几分钟，这限制了Argo在支持对时间敏感的工作流程时的能力。例如，Argoflow不适合实时模型预测工作流程，该工作流程以毫秒级的服务水平协议运行模型预测请求。'
- en: 9.3.3 Metaflow
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3 Metaflow
- en: Metaflow is a human-friendly Python library that focuses on MLOps. It was originally
    developed at Netflix and open-sourced in 2019\. Metaflow is special in that it
    follows a humancentric design; it’s not only built for automating workflows but
    also aims to reduce the human time (operational cost) spent in deep learning project
    development.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow是一个面向人类的Python库，专注于MLOps。它最初在Netflix开发，并于2019年开源。Metaflow的独特之处在于它遵循以人为本的设计理念；它不仅旨在自动化工作流程，还旨在减少深度学习项目开发中花费的人类时间（运营成本）。
- en: 'In section 9.1.3, we pointed out that the conversion from prototyping code
    to production workflow generates a lot of friction in ML development. Data scientists
    have to construct and test a new version of the workflow for each model development
    iteration. To bridge the gap between prototyping and production, Metaflow made
    two improvements: first, it simplifies workflow construction, and second, it unifies
    the workflow execution experience between the local and production environments
    (see figure 9.7).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在9.1.3节中，我们指出，从原型代码到生产工作流程的转换在机器学习开发中产生了大量摩擦。数据科学家必须为每个模型开发迭代构建和测试工作流程的新版本。为了弥合原型和生产的差距，Metaflow做出了两项改进：首先，它简化了工作流程构建；其次，它统一了本地和生产环境之间的工作流程执行体验（见图9.7）。
- en: '![](../Images/09-07.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-07.png)'
- en: Figure 9.7 Metaflow offers a unified development experience between prototyping
    and production.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 Metaflow在原型和生产之间提供了一个统一的发展体验。
- en: In figure 9.7, we can see that Metaflow treats both prototyping and production
    environments as first-class execution environments. Because the Metaflow library
    offers a set of unified APIs to abstract the actual infrastructure, a workflow
    can be executed in the same way regardless of which environment it runs on. For
    example, a workflow can be run by both a local scheduler and a production scheduler
    without any change. The local scheduler executes workflows locally whereas the
    production scheduler integrates into other production orchestration systems, such
    as AWS Step Functions or Argo Workflows.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在图9.7中，我们可以看到Metaflow将原型和生产环境都视为一等执行环境。因为Metaflow库提供了一套统一的API来抽象实际的基础设施，所以无论工作流程在哪个环境中运行，都可以以相同的方式进行执行。例如，工作流程可以由本地调度器和生产调度器运行，而无需任何更改。本地调度器在本地执行工作流程，而生产调度器集成到其他生产编排系统中，例如AWS
    Step Functions或Argo Workflows。
- en: Metaflow lets users annotate a Python code—a DAG Python class—to define the
    workflow. The Metaflow library then creates/packages the workflow automatically
    from the Python annotations. With Metaflow Python annotation, Vena can build a
    workflow without changing any of her prototyping code.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow允许用户注释Python代码——一个DAG Python类——来定义工作流程。然后，Metaflow库会自动从Python注释中创建/打包工作流程。使用Metaflow
    Python注释，Vena可以在不更改任何原型代码的情况下构建工作流程。
- en: Besides seamless workflow creation and testing, Metaflow offers other useful
    features that are key to model reproducibility, such as workflow/steps versioning
    and step input/output saving. To learn more about Metaflow, you can check out
    Metaflow’s official website ([https://docs.metaflow.org/](https://docs.metaflow.org/))
    and a great Metaflow book, *Effective Data Science Infrastructure*, written by
    Ville Tuulos (Manning, 2022; [https://www.manning.com/books/effective-data-science-infrastructure](https://www.manning.com/books/effective-data-science-infrastructure)).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 除了无缝的工作流程创建和测试之外，Metaflow还提供了一些其他有用的功能，这些功能对于模型的可重复性至关重要，例如工作流程/步骤版本控制和步骤输入/输出保存。要了解更多关于Metaflow的信息，您可以查看Metaflow的官方网站([https://docs.metaflow.org/](https://docs.metaflow.org/))和一本优秀的Metaflow书籍，《有效的数据科学基础设施》，由Ville
    Tuulos（Manning，2022年；[https://www.manning.com/books/effective-data-science-infrastructure](https://www.manning.com/books/effective-data-science-infrastructure)）编写。
- en: A typical use case
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型用例
- en: Let’s use Metaflow to automate the same data process work we saw in sections
    9.3.1 and 9.3.2\. See the following listing for the pseudocode.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Metaflow来自动化我们在9.3.1和9.3.2节中看到的相同数据流程工作。请参阅以下列表以获取伪代码。
- en: Listing 9.3 A sample Metaflow workflow
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.3 一个示例Metaflow工作流程
- en: '[PRE6]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In code listing 9.3, we see Metaflow takes a novel approach to building a workflow
    by using code annotations. By annotating `@step` on the functions and using `self.next`
    function to connect steps, we can easily construct a workflow DAG (figure 9.8)
    from our prototyping code.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码列表9.3中，我们看到Metaflow通过使用代码注释来采取一种新颖的方法构建工作流程。通过在函数上注释`@step`并使用`self.next`函数连接步骤，我们可以轻松地从我们的原型代码中构建一个工作流程DAG（图9.8）。
- en: '![](../Images/09-08.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图9.8](../Images/09-08.png)'
- en: Figure 9.8 Workflow DAG constructed from listing 9.3
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 从列表9.3构建的工作流程DAG
- en: One of the beauties here is that we don’t have to define the workflow DAG in
    a separate system and repackage code to a different format, such as a Docker image.
    The Metaflow workflow is immersed in our code. Workflow development and prototyping
    code development happen at the same place and can be tested together from the
    very beginning to the end of the entire ML development cycle.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这里美丽的一面之一是我们不必在单独的系统中定义工作流程DAG并重新打包代码到不同的格式，如Docker镜像。Metaflow工作流程沉浸在我们的代码中。工作流程开发和原型代码开发发生在同一个地方，并且可以从整个机器学习开发周期的开始到结束一起进行测试。
- en: 'Once the code is ready, we can validate and run the workflow locally. See the
    following sample commands:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 代码准备好后，我们可以在本地验证和运行工作流程。请参阅以下示例命令：
- en: '[PRE7]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once we finish local development and testing, it’s time to push the workflow
    to production, which can be achieved by the following two commands:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成本地开发和测试，就是时候将工作流程推送到生产环境了，这可以通过以下两个命令实现：
- en: '[PRE8]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: These commands will export our data process workflow defined in code listing
    9.3 to AWS Step Functions and Argo Workflows. You can then also search for the
    flow by name within the AWS Step Functions UI or Argo Workflows UI and hence see
    the exported flow.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令将我们的数据流程工作流程（代码列表9.3中定义）导出到AWS Step Functions和Argo Workflows。然后您也可以在AWS
    Step Functions UI或Argo Workflows UI中通过名称搜索流程，从而查看导出的流程。
- en: Note Metaflow offers a unified development experience between local and production
    environments. Thanks to the unified API provided by Metaflow, we have a seamless
    experience when testing our code and workflow locally and in production. Regardless
    of the backend workflow orchestration system used, whether Metaflow local scheduler,
    Argo Workflows, or AWS Step Functions, the Metaflow user experience on the workflow
    development remains the same!
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Metaflow在本地和生产环境之间提供统一的开发体验。多亏了Metaflow提供的统一API，我们在本地和生产环境中测试代码和工作流程时拥有无缝的体验。无论使用的后台工作流程编排系统是什么，无论是Metaflow本地调度器、Argo
    Workflows还是AWS Step Functions，Metaflow在流程开发上的用户体验保持一致！
- en: Key Features
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 关键特性
- en: 'Metaflow offers the following key features:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow提供了以下关键特性：
- en: '*Structures code as workflow*—Metaflow lets users create a workflow by annotating
    Python code, which greatly simplifies workflow construction.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将代码结构化为工作流程*—Metaflow允许用户通过注释Python代码来创建工作流程，这极大地简化了工作流程构建。'
- en: '*Reproducibility*—Metaflow preserves an immutable snapshot of the data, code,
    and external dependencies required to execute each workflow step. Metaflow also
    records the metadata of each workflow execution.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可重现性*—Metaflow保留每个工作流程步骤执行所需的数据、代码和外部依赖项的不变快照。Metaflow还记录每个工作流程执行的元数据。'
- en: '*Versioning*—Metaflow addresses the version control requirement of an ML project
    by hashing all the code and data in a workflow.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*版本控制*—Metaflow通过哈希工作流程中的所有代码和数据来解决机器学习项目的版本控制需求。'
- en: '*Robust workflow execution*—Metadata provides dependency management mechanisms
    at both the workflow level and step level by using the @conda decorator. It also
    offers task retries.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*健壮的工作流程执行*—通过使用`@conda`装饰器，元数据在工作和步骤级别提供依赖关系管理机制。它还提供任务重试功能。'
- en: '*Usability design for ML*—Metaflow treats prototyping and production as equally
    important. It provides a set of unified APIs to abstract the infrastructure, so
    the same code can run in both the prototyping environment and the production environment
    without any changes.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*面向机器学习的可用性设计*—Metaflow将原型设计和生产视为同等重要。它提供了一套统一的API来抽象基础设施，因此相同的代码可以在原型环境和生产环境中运行，无需任何更改。'
- en: '*Seamless scalability*—Metaflow integrates with Kubernetes and AWS Batch, which
    allows users to define the required computing resource easily, and can parallel
    the workflow steps over an arbitrarily large number of instances. For example,
    by applying an annotation like `@batch(cpu=1,` `memory=500)` to a step function,
    Metaflow will work with AWS Batch to allocate the required resource to compute
    this step.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*无缝可扩展性*—Metaflow与Kubernetes和AWS Batch集成，使用户可以轻松定义所需的计算资源，并且可以在任意数量的实例上并行工作流程步骤。例如，通过将类似于`@batch(cpu=1,
    memory=500)`的注解应用于步骤函数，Metaflow将与AWS Batch合作分配所需的资源来计算此步骤。'
- en: Limitations
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 局限性
- en: 'The downsides of using Metaflow in deep learning systems are as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习系统中使用Metaflow的缺点如下：
- en: '*No conditional branching support*—Metaflow step annotation doesn’t support
    conditional branching (only executing a step when a condition is met). This is
    not a red flag, but it’s a nice feature to have.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不支持条件分支*—Metaflow步骤注解不支持条件分支（只有当条件满足时才执行步骤）。这并不是一个红旗，但这是一个很好的特性。'
- en: '*No job scheduler*—Metaflow itself doesn’t come with a job scheduler, so you
    can’t use a cron workflow. This is not a big problem because Metaflow can integrate
    with other orchestration systems that support job scheduling, such as AWS Step
    Functions and Argo Workflows.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*没有作业调度器*—Metaflow本身不包含作业调度器，因此你不能使用cron工作流程。这不是一个大问题，因为Metaflow可以与其他支持作业调度的编排系统集成，例如AWS
    Step Functions和Argo Workflows。'
- en: '*Tightly coupled to AWS*—the most important features of Metaflow are tightly
    coupled to AWS—for example, Amazon S3 and AWS Batch. Luckily, Metaflow is an open
    source project, so it’s possible to extend it to non-AWS alternatives.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*紧密耦合到AWS*—Metaflow最重要的特性与AWS紧密耦合——例如，Amazon S3和AWS Batch。幸运的是，Metaflow是一个开源项目，因此可以将其扩展到非AWS替代方案。'
- en: 9.3.4 When to use
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.4 何时使用
- en: If you are looking for an orchestration system to automate workflow execution
    for non-ML projects, both Airflow and Argo Workflows are great choices. They have
    excellent community support and have been used widely in the IT industry. If your
    system runs on Kubernetes and your team feels comfortable working with Docker,
    then Argo Workflows would be a good fit; otherwise, Airflow won’t disappoint you.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在寻找一个编排系统来自动化非ML项目的流程执行，Airflow和Argo Workflows都是不错的选择。它们拥有出色的社区支持，并且在IT行业得到了广泛应用。如果你的系统运行在Kubernetes上，并且你的团队对使用Docker感到舒适，那么Argo
    Workflows将是一个很好的选择；否则，Airflow也不会让你失望。
- en: If you are looking for a system to streamline your ML project development, Metaflow
    is highly recommended. Metaflow is not just an orchestration tool; it’s an MLOps
    tool that focuses on saving data scientists’ time in the ML development cycle.
    Because Metaflow abstracts the backend infrastructure part of a ML project, data
    scientists can focus on model development without worrying about production conversion
    and deployment.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在寻找一个系统来简化你的ML项目开发，Metaflow强烈推荐。Metaflow不仅仅是一个编排工具；它是一个MLOps工具，专注于在ML开发周期中节省数据科学家的宝贵时间。因为Metaflow抽象了ML项目的后端基础设施部分，数据科学家可以专注于模型开发，而无需担心生产转换和部署。
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A workflow is a sequence of operations that are part of some larger task. A
    workflow can be viewed as a DAG of steps. A step is the smallest resumable unit
    of computation that describes what to do; a step either succeeds or fails as a
    whole. A DAG specifies the dependencies between steps and the order in which to
    execute them.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作流程是一系列属于某个更大任务的运算操作。工作流程可以被视为步骤的DAG。步骤是描述要做什么的最小可恢复的计算单元；步骤要么成功要么失败。DAG指定了步骤之间的依赖关系以及它们的执行顺序。
- en: Workflow orchestration means executing the workflow steps based on the sequence
    defined in the workflow’s DAG.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作流程编排意味着根据工作流程中定义的DAG序列执行工作流程步骤。
- en: Adopting a workflow encourages work sharing, team collaboration, and automation.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用工作流程可以鼓励资源共享、团队合作和自动化。
- en: The main challenge of applying a workflow on deep learning projects is to reduce
    workflow construction costs and simplify workflow testing and debugging.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在深度学习项目中应用工作流程的主要挑战是降低工作流程构建成本并简化工作流程测试和调试。
- en: The six recommended design principles for building/evaluating workflow orchestration
    systems are criticality, usability, extensibility, task isolation, scalability,
    and human centricity.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建或评估工作流程编排系统的六个推荐设计原则是关键性、可用性、可扩展性、任务隔离、可扩展性和以人为中心。
- en: When choosing an orchestration system for non-ML projects, both Airflow and
    Argo Workflows are great choices. Argo Workflows is a better option if the project
    runs on Kubernetes and Docker.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当选择用于非机器学习项目的编排系统时，Airflow 和 Argo Workflows 都是不错的选择。如果项目运行在 Kubernetes 和 Docker
    上，Argo Workflows 是更好的选择。
- en: When selecting an orchestration system for ML projects, Metaflow is so far the
    best option.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当选择用于机器学习项目的编排系统时，Metaflow 目前是最佳选择。
