- en: 11 Sequence-to-sequence
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 序列到序列
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Preparing a sequence-to-sequence dataset and loader
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备序列到序列数据集和加载器
- en: Combining RNNs with attention mechanisms
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 RNN 与注意力机制结合
- en: Building a machine translation model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建机器翻译模型
- en: Interpreting attentionscores to understand a model’s decisions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释注意力分数以理解模型的决定
- en: Now that we have learned about attention mechanisms, we can wield them to build
    something new and powerful. In particular, we will develop an algorithm known
    as *sequence-to-sequence* (Seq2Seq for short) that can perform machine translation.
    As the name implies, this is an approach for getting neural networks to take one
    sequence as input and produce a different sequence as the output. Seq2Seq has
    been used to get computers to perform symbolic calculus,[¹](#fn40) summarize long
    documents,[²](#fn41) and even translate from one language to another. I’ll show
    you step by step how we can translate from English to French. In fact, Google
    used essentially the same approach as its production machine-translation tool,
    and you can read about it at [https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html).
    If you can imagine your inputs/outputs as sequences of things, there is a good
    chance Seq2Seq can help you solve the task.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了注意力机制，我们可以利用它们来构建一些新颖而强大的东西。特别是，我们将开发一个名为 *序列到序列*（简称 Seq2Seq）的算法，它可以执行机器翻译。正如其名所示，这是一种让神经网络将一个序列作为输入并产生不同序列作为输出的方法。Seq2Seq
    已被用于让计算机执行符号计算[¹](#fn40)，总结长文档[²](#fn41)，甚至将一种语言翻译成另一种语言。我将一步步地展示我们如何将英语翻译成法语。实际上，谷歌使用的生产机器翻译工具基本上采用了同样的方法，你可以在[https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html)上了解相关信息。如果你能将你的输入/输出想象成一系列事物，那么
    Seq2Seq 有很大机会帮助你解决这个任务。
- en: The astute reader may think, an RNN takes in a sequence and produces an output
    for every input. So it takes in a sequence and outputs a sequence. Isn’t that
    “Seq-to-Seq”? You are a wise reader, but this is part of why I called Seq2Seq
    a design approach. You could hypothetically get an RNN to do anything Seq2Seq
    could do, but it would be difficult to get it to work. One problem is that an
    RNN alone implies that the output sequence is the same length as the input, which
    is rarely true. Seq2Seq decouples the input and output into two separate stages
    and parts and thus works much better. We get to how that happens shortly.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 聪明的读者可能会想，RNN 接收一个序列并为每个输入产生一个输出。所以它接收一个序列并输出一个序列。这不是“序列到序列”吗？你是一个明智的读者，但这也是我为什么称
    Seq2Seq 为一种设计方法的部分原因。理论上，你可以让 RNN 做任何 Seq2Seq 能做的事情，但让它工作起来会很困难。一个问题是一个单独的 RNN
    假设输出序列的长度与输入相同，这很少是真实的。Seq2Seq 将输入和输出解耦成两个独立阶段和部分，因此工作得更好。我们很快就会看到这是如何发生的。
- en: Seq2Seq is the most complex algorithm we implement in the book, but I’ll show
    you how we can break it into smaller, manageable subcomponents. Using what we
    have learned about organizing PyTorch modules, this will be only a little painful.
    We walk through the task-specific setup with our dataset and `DataLoader` to prepare
    for training, describe the submodules of a Seq2Seq implementation, and end by
    showing how the attention mechanism lets us peer into the black box of neural
    networks to understand *how* Seq2Seq is performing the translations.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Seq2Seq 是我们在本书中实现的最复杂的算法，但我会向你展示如何将其分解成更小、更易于管理的子组件。利用我们关于组织 PyTorch 模块的知识，这将只会带来一点痛苦。我们通过我们的数据集和
    `DataLoader` 的任务特定设置来准备训练，描述 Seq2Seq 实现的子模块，并以展示注意力机制如何让我们窥视神经网络的黑盒，理解 Seq2Seq
    是如何进行翻译的作为结束。
- en: 11.1 Sequence-to-sequence as a kind of denoising autoencoder
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 将序列到序列视为一种去噪自编码器
- en: In a moment, we will go into the details of Seq2Seq, how to implement it, and
    how to apply it to translate English into French. But first, I want to show you
    an example of how well it can work and the explainability[³](#fn42) that attention
    provides. A result of a large Seq2Seq translation model is shown in figure 11.1,
    translating from French to English. Each word in the output has a different attention
    applied to the original sentence, where black values indicate 0 (not important)
    and white values indicate 1 (very important).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将详细介绍Seq2Seq，如何实现它，以及如何将其应用于将英语翻译成法语。但首先，我想展示一个例子，看看它如何工作得很好，以及注意力提供的可解释性[³](#fn42)。图11.1展示了大型Seq2Seq翻译模型的结果，从法语翻译成英语。输出中的每个单词都对原始句子应用了不同的注意力，其中黑色值表示0（不重要）和白色值表示1（非常重要）。
- en: '![](../Images/CH11_F01_Raff.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH11_F01_Raff.png)'
- en: Figure 11.1 Results from the paper “Neural machine translation by jointly learning
    to align and translate” by Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
    The input is on the left, and the output on the top. The attention result when
    trained on a larger corpus for more epochs results in a nice, crisp attention
    map.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1展示了Dzmitry Bahdanau、Kyunghyun Cho和Yoshua Bengio发表的论文“通过联合学习对齐和翻译进行神经机器翻译”的结果。输入在左侧，输出在顶部。在更大语料库上训练更多轮次时，注意力结果产生了一个清晰、紧凑的注意力图。
- en: For most items in the output, very few items from the input are relevant to
    getting the correct translation. But some cases require multiple words from the
    input to translate properly, and others need to be reordered. Classical machine
    translation approaches, which worked on a more word-by-word level, often had complex
    heuristic code to deal with situations that required multiple words of context
    and possibly unusual order. But now we can let the neural network learn the details
    for us and get an idea about how the model is learning to perform the translation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输出中的大多数项目，来自输入的相关项非常少，才能得到正确的翻译。但有些情况需要从输入中提取多个单词才能正确翻译，而另一些则需要重新排序。传统的机器翻译方法，在更单词级别的层面上工作，通常需要复杂的启发式代码来处理需要多个上下文单词和可能不寻常顺序的情况。但现在我们可以让神经网络为我们学习细节，并了解模型是如何学习执行翻译的。
- en: How does Seq2Seq learn to do this? At a high level, the Seq2Seq algorithm trains
    a denoising autoencoder over sequences rather than static images. You can think
    of the original English as the noisy input and French as the clean output, and
    we ask the Seq2Seq model to learn how to remove the noise. Since we are dealing
    with sequences, this usually involves an RNN. A simple diagram is presented in
    figure 11.2\. We have an encoder and decoder to convert inputs to outputs just
    like a denoising autoencoder; the difference is that a Seq2Seq model works over
    sequences instead of images or fully connected inputs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Seq2Seq是如何学习做到这一点的呢？从高层次来看，Seq2Seq算法在序列上训练去噪自编码器，而不是在静态图像上。你可以把原始英语看作是带噪声的输入，法语看作是干净的输出，并要求Seq2Seq模型学习如何去除噪声。由于我们处理的是序列，这通常涉及到RNN。图11.2展示了一个简单的图。我们有一个编码器和解码器，将输入转换为输出，就像去噪自编码器一样；区别在于Seq2Seq模型在序列上工作，而不是在图像或全连接输入上。
- en: '![](../Images/CH11_F02_Raff.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH11_F02_Raff.png)'
- en: Figure 11.2 High-level depiction of the sequence-to-sequence approach. An input
    sequence **x**[1], **x**[2], …, **x**[T] goes into an encoder. The encoder produces
    a representation of the sequence **h**[encoded]. The decoder takes this representation
    and outputs a new sequence **y**[1], **y**[2], …, **y**[*T*’].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2展示了序列到序列方法的概述。输入序列 **x**[1]，**x**[2]，…，**x**[T] 进入编码器。编码器生成序列的表示 **h**[encoded]。解码器接收这个表示并输出一个新的序列
    **y**[1]，**y**[2]，…，**y**[*T*’]。
- en: Here we have some original input sequence **X** = **x**[1], **x**[2], …, **x**[T],
    and the goal is to output a *new* sequence **Y** = **y**[1], **y**[2], …, **y**[*T*′].
    These sequences do not have to be the same. **x**[j] ≠ **y**[j], and they can
    even be different lengths so *T* ≠ *T*′ is also possible. We describe this as
    a denoising approach because the input sequence X gets mapped to a related sequence
    Y, almost as if X were a noisy version of Y.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有一些原始输入序列 **X** = **x**[1]，**x**[2]，…，**x**[T]，目标是输出一个 *新* 的序列 **Y** =
    **y**[1]，**y**[2]，…，**y**[*T*′]。这些序列不必相同。**x**[j] ≠ **y**[j]，它们甚至可以是不同长度的，所以 *T*
    ≠ *T*′ 也是可能的。我们把这描述为去噪方法，因为输入序列X被映射到一个相关的序列Y，几乎就像X是Y的噪声版本一样。
- en: Training a Seq2Seq model is no easy task. Like anything that uses RNNs, it is
    computationally challenging. It is also a difficult learning problem; the encoder
    RNN takes in X and produces a final hidden state activation **h**[T], which the
    decoder RNN must take as input to produce a new sequence Y. That requires packing
    a lot of information into a single vector, and we have already seen how RNNs can
    be difficult to train.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 训练Seq2Seq模型并不容易。像任何使用RNN的东西一样，它计算上具有挑战性。它也是一个困难的学习问题；编码器RNN接收X并产生一个最终的隐藏状态激活**h**[T]，解码器RNN必须将其作为输入来产生新的序列Y。这需要将大量信息压缩到一个单一的向量中，我们已经看到RNN训练的困难性。
- en: 11.1.1  Adding attention creates Seq2Seq
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.1 添加注意力创建Seq2Seq
- en: The secret to getting a Seq2Seq model working well is adding an attention mechanism.
    Instead of forcing the model to learn one representation **h**[encoded] to represent
    the *entire* input, we learn a representation **h**[i] for each item **x**[i]
    of the input sequence. Then we use an attention mechanism at each step of the
    output to look at all of the input. This is shown in more detail in figure 11.3,
    and we continue to expand the details of different parts of Seq2Seq until we can
    implement the entire approach. Notice that the last hidden state from the encoder,
    **h**[T], becomes the *initial* hidden state of the decoder, but we haven’t indicted
    what the inputs to the decoder are at each time step. We get to that later in
    the chapter; there are too many parts to discuss all at once.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让Seq2Seq模型工作良好的秘诀是添加一个注意力机制。我们不是强迫模型学习一个表示**h**[编码]来表示整个输入，而是为输入序列中的每个项目**x**[i]学习一个表示**h**[i]。然后我们在输出的每个步骤使用注意力机制来查看所有输入。这更详细地展示在图11.3中，我们继续扩展Seq2Seq不同部分的细节，直到我们可以实现整个方法。注意，编码器的最后一个隐藏状态**h**[T]成为解码器的**初始**隐藏状态，但我们还没有指出解码器在每个时间步的输入是什么。我们将在本章的后面讨论这个问题；因为部分太多，不能一次性全部讨论。
- en: '![](../Images/CH11_F03_Raff.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F03_Raff.png)'
- en: Figure 11.3 Deeper high-level diagram of how Seq2Seq operates. The first input
    to the decoder is the last output of the encoder. The decoder produces a sequence
    of outputs that are used as the contexts for an attention mechanism. By changing
    the context, we change what the model is looking for in the input.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3 Seq2Seq操作的高级图。解码器的第一个输入是编码器的最后一个输出。解码器产生一系列输出，这些输出被用作注意力机制的环境。通过改变环境，我们改变了模型在输入中寻找的内容。
- en: Now let’s talk about figure 11.1 and figure 11.3 together. The T inputs/hidden
    states from the encoder network in figure 11.3 would become the T rows of the
    original French in figure 11.1\. Each attention block in figure 11.3 becomes one
    of the columns of figure 11.1\. Because the attention outputs a score from 0 to
    1 for every item in the input (filling a row), we can plot that in a heat map
    with dark values for low scores and white for high scores, showing which parts
    of the input were most important for producing each part of the output. Hence,
    we get figure 11.1 in its entirety. This also exemplifies how the attention captures
    the idea that importance is a *relative* concept. The importance of each word
    in the input changes depending on what we are trying to produce or what other
    words are present.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们一起来讨论图11.1和图11.3。图11.3中的编码器网络的T个输入/隐藏状态将变成图11.1中原始法语的T行。图11.3中的每个注意力块变成图11.1中的一列。因为注意力为输入中的每个项目（填充一行）输出一个从0到1的分数，我们可以用热图来展示这些分数，低分用深色表示，高分用白色表示，从而显示输入中哪些部分对于产生输出中的每个部分最为重要。因此，我们得到了完整的图11.1。这也说明了注意力如何捕捉到重要性是一个**相对**概念。输入中每个词的重要性会根据我们试图产生的结果或其他存在的词而变化。
- en: 'You may be wondering why the attention improves the RNN-based encoder and decoder:
    doesn’t the gating mechanism from the long short-term memory (LSTM) do the same
    thing, selectively allowing/disallowing (gate on/off) information based on the
    current context? At a high level, yes, there is similarity to the approach. The
    key difference is the availability of information. If you did this as figure 11.2
    depicts using just two RNNs for the encoder and decoder, the hidden state of the
    decoder RNN must learn how to represent (1), how far along it is with its creation
    of the output, (2) the entire original input sequence, and (3) how to not corrupt
    one with the other. By using an attention mechanism on the outputs of the decoder
    RNN, the RNN now only has to learn #1, as all of the original input items are
    available later for the attention, which alleviates #2 and also means #3 is a
    non-issue. While Seq2Seq is a complex algorithm, you have already learned and
    used *every* step that goes into implementing it. This is really an exercise in
    putting a lot of the building blocks of deep learning together into one powerful
    result.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道为什么注意力机制能提高基于RNN的编码器和解码器的性能：长短期记忆（LSTM）中的门控机制不是也在做同样的事情，根据当前上下文有选择地允许/不允许（门控开启/关闭）信息吗？从高层次来看，是的，这种方法有相似之处。关键的区别在于信息的可用性。如果你像图11.2所示的那样，只用两个RNN作为编码器和解码器，解码器RNN的隐藏状态必须学会如何表示（1），它在创建输出过程中的进度有多远，（2）整个原始输入序列，以及（3）如何不使两者相互干扰。通过在解码器RNN的输出上使用注意力机制，RNN现在只需要学习#1，因为所有原始输入项都将在后续的注意力过程中可用，这缓解了#2，也意味着#3不再是问题。虽然Seq2Seq是一个复杂的算法，但你已经学习和使用了实现它的每一个步骤。这实际上是将深度学习的许多构建块组合在一起以产生一个强大结果的练习。
- en: 11.2 Machine translation and the data loader
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 机器翻译和数据加载器
- en: With the big-picture goal in mind—building a Seq2Seq translation model—we will
    work from the bottom up. The very bottom starts with defining what translation
    is and getting the data loaded. Then we can move on to how the input to a Seq2Seq
    model is handled, and finally add the attention mechanism to produce the outputs
    one at a time.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑大局目标——构建Seq2Seq翻译模型的情况下——我们将从下往上工作。最底层开始于定义什么是翻译以及加载数据。然后我们可以继续处理Seq2Seq模型的输入，最后添加注意力机制以逐个产生输出。
- en: Broadly, machine translation is the name researchers use for studying how to
    get computers to translate from one language (e.g., English) to another (e.g.,
    French). Machine translation as a problem will also help solidify how the inputs
    and outputs to the Seq2Seq model are different sequences of possibly different
    lengths.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 广义上，机器翻译是研究人员用来研究如何让计算机将一种语言（例如，英语）翻译成另一种语言（例如，法语）的术语。机器翻译作为一个问题也将有助于巩固Seq2Seq模型输入和输出的不同序列以及可能的不同长度。
- en: 'The language that the input X is in—in our case, English—is called the *source*
    language. The destination language, French, is the *target* language. Our input
    sequence X might be the string “what a nice day,” with a target string Y of “quelle
    belle journée.” One difficult aspect of translation is that these sequences are
    not the same length. If we use words as our tokens (also called our alphabet Σ),
    our source sequence X and target sequence Y are:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 输入X所使用的语言——在我们的案例中，是英语——被称为*源语言*。目标语言，法语，被称为*目标语言*。我们的输入序列X可能是字符串“what a nice
    day”，目标字符串Y为“quelle belle journée”。翻译的一个困难方面是这些序列的长度不同。如果我们使用单词作为我们的标记（也称为我们的字母表Σ），我们的源序列X和目标序列Y是：
- en: '![](../Images/CH11_F03_EQ01.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F03_EQ01.png)'
- en: 'If we can successfully convert the sequence X into Y, we will accomplish the
    task of machine translation. Several nuances can make this difficult:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能成功将序列X转换为Y，我们就完成了机器翻译的任务。几个细微之处可能会使这变得困难：
- en: As already mentioned, the sequences may not be the same length.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前所述，序列的长度可能不同。
- en: There may be complex relationships between sequences. For example, one language
    may put adjectives before nouns, and another may put nouns beforeadjectives.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列之间可能存在复杂的关系。例如，一种语言可能将形容词放在名词之前，而另一种语言可能将名词放在形容词之前。
- en: There may not be a one-to-one relationship. For example, “what a nice day” and
    “what a lovely day” can both have the same translation from English to French.
    Translation is usually a many-to-many task, where multiple valid inputs map to
    multiple valid outputs.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能不存在一对一的关系。例如，“what a nice day”和“what a lovely day”都可以有相同的从英语到法语的翻译。翻译通常是一个多对多任务，其中多个有效输入映射到多个有效输出。
- en: If you asked a natural language processing (NLP) researcher, they would give
    you an even longer list of why and how machine translation is challenging. But
    it’s also a great opportunity to use Seq2Seq models because you do not need to
    look at the *entire* input to make decisions about each word of the output. For
    example, “journée” translates to “day” or “daytime”; it is not a homonym. Thus
    we can almost translate that word on its own without any other context. The word
    “amende” requires more context because it is a homonym for a “fine” and for “almond”;
    you can’t translate it without knowing if someone is talking about food or money.
    Our attention mechanism can help us ignore the inputs that do not provide any
    helpful context for translation. This is part of why a Seq2Seq model can do very
    well at this task, even without us being able to enumerate all the reasons translation
    is difficult.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你问一个自然语言处理（NLP）研究人员，他们会给你一个更长的列表，解释为什么以及机器翻译具有挑战性。但这也是使用Seq2Seq模型的一个绝佳机会，因为你不需要查看整个输入来决定输出中每个单词的决策。例如，“journée”可以翻译为“day”或“daytime”；它不是一个同音异义词。因此，我们可以几乎独立于任何其他上下文来翻译这个单词。单词“amende”需要更多的上下文，因为它是一个“fine”和“almond”的同音异义词；如果你不知道某人是在谈论食物还是金钱，你就无法翻译它。我们的注意力机制可以帮助我们忽略那些对翻译没有提供任何有用上下文的输入。这也是为什么Seq2Seq模型能够在这一任务上做得很好，即使我们无法列举出所有翻译困难的理由。
- en: 11.2.1  Loading a small English-French dataset
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.1  加载一个小型的英语-法语数据集
- en: 'To build a machine translation dataset, we need some data. We’ll reuse a small
    corpus of English-French translations. The following code quickly downloads it
    and does some minor preprocessing: removing punctuation and converting everything
    to lowercase. While it is possible to learn these things, doing so takes more
    data, and we want this example to run quickly with a limited amount of data:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个机器翻译数据集，我们需要一些数据。我们将重用一小部分英语-法语翻译语料库。以下代码快速下载它并进行一些轻微的预处理：移除标点符号并将所有内容转换为小写。虽然有可能学习这些事情，但这样做需要更多的数据，而我们希望这个例子在有限的数据量下快速运行：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Lowercase only, please.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 只需小写即可。
- en: ❷ No punctuation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 不需要标点符号。
- en: ❸ (English, French)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ （英语，法语）
- en: 'To help give us some intuition, the following code prints the first few lines
    of the corpus to show the data. We already see a number of difficulties in the
    data. Words like “run” have more than one correct translation on their own, and
    some single English words can become one or more French words. This is before
    we even look at the longer sentences in the corpus:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们获得一些直观感受，以下代码打印了语料库的前几行，以展示数据。我们已经在数据中看到了许多困难。像“run”这样的单词本身就有多个正确的翻译，而且一些单个的英语单词可以变成一个或多个法语单词。这是在我们甚至查看语料库中更长的句子之前：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To make training faster, let’s limit ourselves to sentences that contain six
    or fewer words. You can try increasing this limit to see how the model does, but
    I want to make these examples train quickly. A real-world translation task could
    use the same code but would need more time and data to learn something more robust—but
    that could take days of training, so let’s keep it short:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使训练更快，让我们限制自己只使用包含六个或更少单词的句子。你可以尝试增加这个限制，看看模型的表现如何，但我想让这些例子快速训练。一个现实世界的翻译任务可以使用相同的代码，但需要更多的时间和数据进行学习，但这可能需要数天的训练，所以让我们保持简短：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ The subset we use
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们使用的子集
- en: Building the alphabet
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 构建字母表
- en: Now the `short_subset` list contains all of the English-French translation pairs
    we will use, and we can build a vocabulary or alphabet for our model. As before,
    a vocabulary gives every unique string we use a unique ID as a token, starting
    from 0 and counting up. But we also add some special tokens to give our model
    helpful hints. First, because not all sentences are the same length, we use a
    `PAD_token` to denote padding, indicating that the value in a tensor is not used
    because the underlying sentence has already ended.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的`short_subset`列表包含了我们将用于模型的全部英语-法语翻译对，我们可以为我们的模型构建一个词汇表或字母表。和之前一样，一个词汇表会给每个唯一的字符串分配一个唯一的ID作为标记，从0开始计数。但我们还添加了一些特殊的标记，以给我们的模型提供有用的提示。首先，因为不是所有句子长度都相同，我们使用一个`PAD_token`来表示填充，表示张量中的值没有被使用，因为底层句子已经结束。
- en: The two new things we introduce are *start of sentence* (SOS) and *end of sentence*
    (EOS) tokens. These are commonly found in machine translation, as well as many
    other NLP tasks. The `SOS_token` token is often placed at the beginning of the
    source sequence X to indicate to the algorithm that translation has started. The
    `EOS_token` is more useful, as it is appended to the end of the target sequence
    Y to indicate that the sentence is complete. This is very useful later so the
    model learns how to end a translation. When the model is done, it outputs an `EOS_token`,
    and we can stop the process. You may think that punctuation is a good stopping
    point, but that approach would prevent us from extending our model to translate
    sentences or paragraphs at a time. Once every item in the batch has generated
    an `EOS_token`, we know it is safe to stop. This also helps because the outputs
    may have differing lengths, and the EOS marker helps us figure out how long each
    different generated sequence should be.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入的两个新事物是 *句子开始*（SOS）和 *句子结束*（EOS）标记。这些在机器翻译以及许多其他 NLP 任务中都很常见。`SOS_token`
    标记通常放置在源序列 X 的开头，以指示算法翻译已经开始。`EOS_token` 更有用，因为它附加到目标序列 Y 的末尾，表示句子已完成。这非常有用，因为模型可以学习如何结束翻译。当模型完成时，它输出一个
    `EOS_token`，我们可以停止这个过程。你可能认为标点符号是一个好的停止点，但这种方法会阻止我们将模型扩展到一次翻译句子或段落。一旦批次中的每个项目都生成了一个
    `EOS_token`，我们就知道可以安全地停止。这也很有帮助，因为输出可能具有不同的长度，EOS 标记帮助我们确定每个不同生成的序列应该有多长。
- en: 'The following code defines our SOS, EOS, and padding markers and creates a
    dictionary `word2indx` to create the mapping. We are also defining `PAD_token`
    as mapping to index 0 so that we can use our `getMaskByFill` function easily.
    Similar to our auto-regressive model, an inverted dictionary `indx2word` is created
    so that we can look at our results more easily when we are done (it is easier
    to read words than a sequence of integers):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '以下代码定义了我们的 SOS、EOS 和填充标记，并创建了一个字典 `word2indx` 来创建映射。我们还将 `PAD_token` 定义为映射到索引
    0，这样我们就可以轻松地使用我们的 `getMaskByFill` 函数。类似于我们的自回归模型，创建了一个逆字典 `indx2word`，这样我们可以在完成后更容易地查看我们的结果（阅读单词比阅读整数序列更容易）： '
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ “START_OF_SENTENCE_TOKEN"
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ “句子开始标记"
- en: ❷ “END_OF_SENTENCE_TOKEN"
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ “句子结束标记"
- en: ❸ Builds the inverted dictionary for looking at the outputs later
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为后续查看构建逆字典
- en: Implementing a translation dataset
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 实现翻译数据集
- en: 'Now we create a `TranslationDataset` object that represents our translation
    task. It takes in the `short_subset` as the underlying input data and returns
    PyTorch `int64` tensors by splitting on spaces and using the vocabulary `word2indx`
    that we just created:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建一个 `TranslationDataset` 对象，它代表我们的翻译任务。它接受 `short_subset` 作为底层输入数据，并通过使用我们刚刚创建的词汇表
    `word2indx` 来分割空间，返回 PyTorch `int64` 张量：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Converts to lists of integers
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 转换为整数列表
- en: Seq2Seq tasks generally require a lot of training data, which we do not have
    right now because we want this example to run in 10 minutes. We will use more
    of the data (90%) for training and just 10% for testing.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Seq2Seq 任务通常需要大量的训练数据，但我们目前没有这么多，因为我们希望这个示例能在 10 分钟内运行。我们将使用 90% 的数据用于训练，仅 10%
    用于测试。
- en: Implementing a collate function for translation data
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 实现翻译数据的 collate 函数
- en: We also need to define a `collate_fn` function to create one larger batch from
    the inputs that are different lengths. Each item already ends with an `EOS_token`,
    so we just pad the shorter inputs with our `PAD_token` value to make everything
    the same length.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要定义一个 `collate_fn` 函数，用于将不同长度的输入合并成一个更大的批次。每个项目已经以 `EOS_token` 结尾，所以我们只需用我们的
    `PAD_token` 值填充较短的输入，使所有内容长度相同。
- en: To make this compatible with our `train_network` function, we also need to be
    a little clever in how we return the results of `collate_fn`. We will return the
    data as a set of nested tuples, ((**X**,**Y**),**Y**) because the `train_network`
    function expects tuples with two items, (*i**n**p**u**t*,*o**u**t**p**u**t*),
    and our Seq2Seq model requires both X and Y at training time. The reason is explained
    soon. This way, the `train_network` function will break up the tuple as
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使此与我们的 `train_network` 函数兼容，我们还需要在返回 `collate_fn` 的结果时稍微聪明一点。我们将以一组嵌套元组的形式返回数据，((**X**,**Y**),**Y**)，因为
    `train_network` 函数期望包含两个元素的元组（*i**n**p**u**t*,*o**u**t**p**u**t*），而我们的 Seq2Seq
    模型在训练时需要 X 和 Y。原因将在下面解释。这样，`train_network` 函数将分解元组为
- en: '![](../Images/CH11_F03_EQ02.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH11_F03_EQ02.png)'
- en: 'The following code does all the work. `pad_batch` is our collation function,
    which begins by finding the longest input sequence length `max_x` and the longest
    output sequence length `max_y`. Since we are padding (not *packing*, which is
    supported only by RNNs), we can use the `F.pad` function to do that. It takes
    the sequence to pad as the first input and a tuple telling it how much to pad
    to the left and how much to pad to the right. We only want to pad to the right
    side (end) of a sequence, so our tuples look like `(0, pad_amount)`:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码完成了所有工作。`pad_batch` 是我们的收集函数，它首先找到最长的输入序列长度 `max_x` 和最长的输出序列长度 `max_y`。由于我们是在填充（而不是
    *打包*，这仅由 RNNs 支持），我们可以使用 `F.pad` 函数来完成这项工作。它接受要填充的序列作为第一个输入，以及一个元组告诉它需要向左和向右填充多少。我们只想在序列的右侧（末尾）进行填充，所以我们的元组看起来像
    `(0, pad_amount)`：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '❶ We have two different maximum lengths: the max length of the input sequences
    and the max length of the output sequences. We determine each separately and only
    pad the inputs/outputs by the exact amount we need.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们有两个不同的最大长度：输入序列的最大长度和输出序列的最大长度。我们分别确定每个长度，并且只填充所需的精确量。
- en: ❷ We use the F.pad function to pad each tensor to the right.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们使用 `F.pad` 函数将每个张量向右填充。
- en: 11.3 Inputs to Seq2Seq
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 Seq2Seq 的输入
- en: 'When we are talking about a Seq2Seq model, there are two sets of inputs: the
    input to the encoder and the input to the decoder. To specify what goes into each
    part, we need to define what the encoder and decoder blocks of Seq2Seq are in
    figure 11.3\. It should not surprise you that we use an RNN for each, and it is
    your choice if you want to use a gated recurrent unit (GRU) or LSTM. When we code
    this later, we’ll use a GRU because it makes the code a little easier to read.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论 Seq2Seq 模型时，有两个输入集：编码器的输入和解码器的输入。为了指定每个部分的输入内容，我们需要定义 Seq2Seq 的编码器和解码器块在图
    11.3 中的样子。您可能不会感到惊讶，我们为每个都使用了 RNN，您可以选择是否要使用门控循环单元（GRU）或 LSTM。当我们稍后编码时，我们将使用 GRU，因为它使代码更容易阅读。
- en: 'The inputs to the encoder are easy: we are just feeding in the input sequence
    **X** = **x**[1], **x**[1], …, **x**[T]. The decoder produces predictions of what
    it thinks the output sequence is: **Ŷ** = **ŷ**[1], **ŷ**[1], …, **ŷ**[*T*’].
    We take the cross-entropy loss between Ŷ and Y as the learning signal for training
    this network.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的输入很简单：我们只是输入输入序列 **X** = **x**[1], **x**[1], …, **x**[T]。解码器产生它认为的输出序列的预测：**Ŷ**
    = **ŷ**[1], **ŷ**[1], …, **ŷ**[*T*’]。我们将 Ŷ 和 Y 之间的交叉熵损失作为训练这个网络的信号。
- en: 'But we are missing a big detail: the inputs to the decoder. RNNs usually take
    in a previous hidden state (that would be *h*[encoded] for the decoder) plus an
    input for the current time step. There are two options for the decoder’s inputs:
    autoregressive style and teacher forcing. We will learn about both options because
    using both works better than using either one alone.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们遗漏了一个重要细节：解码器的输入。循环神经网络（RNNs）通常接受一个先前的隐藏状态（对于解码器来说，这将是对应的 *h*[encoded]）以及当前时间步的输入。解码器的输入有两种选择：自回归风格和教师强制。我们将学习这两种选择，因为同时使用两者比单独使用任何一个都要好。
- en: For both options, we use the last hidden state of the encoder (**h**[T]) as
    the initial hidden state of the decoder (**h**[encoded] = **h**[T]). We do this
    instead of using the zero vector so that the gradient flows through the decoder
    and into the encoder, connecting them. More important, the last hidden state **h**[T]
    is a summary of the entire input sequence, and that context of “what the input
    was” will help the decoder decide what the first part of the output should be.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两种选择，我们使用编码器的最后一个隐藏状态（**h**[T]）作为解码器的初始隐藏状态（**h**[encoded] = **h**[T]）。我们这样做而不是使用零向量，是为了让梯度通过解码器流向编码器，将它们连接起来。更重要的是，最后一个隐藏状态
    **h**[T] 是整个输入序列的摘要，这种“输入是什么”的上下文将帮助解码器决定输出序列的第一部分应该是什么。
- en: 11.3.1  Autoregressive approach
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.1 自回归方法
- en: Figure 11.4 shows our first option for implementing the decoder’s inputs, which
    we call the *autoregressive* option. For the autoregressive approach, we use the
    predicted token at time step t as the input for the next time step *t* + 1 (the
    dashed grey lines).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 展示了我们实现解码器输入的第一个选项，我们称之为 *自回归* 选项。对于自回归方法，我们使用时间步 t 的预测标记作为下一个时间步 *t*
    + 1 的输入（虚线灰色线条）。
- en: '![](../Images/CH11_F04_Raff.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH11_F04_Raff.png)'
- en: Figure 11.4 Example of an autoregressive approach to the decoding step. The
    first input to the decoder is the last input to the encoder. Every subsequent
    input to the decoder is the prediction from the previous step.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4展示了自回归方法在解码步骤中的应用示例。解码器的第一个输入是编码器的最后一个输入。解码器的后续每个输入都是前一步的预测。
- en: The other detail is what the *first* input should be. We have not made any predictions
    yet, so we can’t use the previous prediction as the input. There are two sub-options,
    and both have very similar performance. The first option is to *always* use the
    SOS marker as the input, which is a very reasonable idea. Semantically, it makes
    sense as well; the first input says, “the sentence is starting,” and the RNN must
    use the context to predict the first word. The second option is to use the last
    token of the input, which should be an EOS marker or a padding token. This ends
    up with the decoder RNN learning that the EOS and padding have the same semantic
    meaning as “sentence start.” Either option is acceptable, and in practice the
    choice does not tend to make a noticeable difference. We will implement it so
    the last item from the encoder becomes the first input to the decoder, as I think
    it is a slightly more general approach.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个细节是第一个输入应该是什么。我们还没有做出任何预测，所以不能使用之前的预测作为输入。有两种子选项，它们的表现非常相似。第一种选项是**始终**使用SOS标记作为输入，这是一个非常合理的想法。从语义上讲，这也很有意义；第一个输入表示“句子开始”，RNN必须使用上下文来预测第一个单词。第二种选项是使用输入的最后一个标记，这应该是EOS标记或填充标记。这导致解码器RNN学习到EOS和填充具有与“句子开始”相同的语义意义。任何一种选项都是可接受的，并且在实践中选择通常不会造成明显的差异。我们将实现它，使编码器的最后一个项目成为解码器的第一个输入，因为我认为这是一个稍微更通用的方法。
- en: 'Since the output of our model is a probability of the next word, there are
    two ways to select the next input *t* + 1: take the most likely token, or sample
    the next token based on the probabilities given. In chapter 6, when we trained
    an autoregressive model, selecting the most likely next word caused unrealistic
    output. So when we implement this, we will go for the sampling approach.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们模型的输出是下一个单词的概率，因此有两种选择下一个输入*t* + 1的方式：选择最可能的标记，或者根据给定的概率采样下一个标记。在第六章中，当我们训练自回归模型时，选择最可能的下一个单词导致了不切实际的结果。因此，当我们实现这一点时，我们将采用采样方法。
- en: 11.3.2  Teacher-forcing approach
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.2 教师强制方法
- en: The second option is called *teacher forcing*. The first input to the decoder
    is handled the exact same way as in the autoregressive approach, but the subsequent
    inputs are different. Rather than using the prediction **ŷ**[t] as the input
    for **ŷ**[*t* + 1], we use the true correct token **y**[t], as shown in figure
    11.5.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法被称为**教师强制**。解码器的第一个输入处理方式与自回归方法完全相同，但后续输入不同。我们不是使用预测**ŷ**[t]作为**ŷ**[*t*
    + 1]的输入，而是使用真实的正确标记**y**[t]，如图11.5所示。
- en: '![](../Images/CH11_F05_Raff.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F05_Raff.png)'
- en: Figure 11.5 Example of teacher forcing. For the decoder, we ignore the predictions
    and at step t, we feed in the correct previous output **y**[*t* − 1]. The predictions
    are still used when the loss is computed.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5展示了教师强制示例。对于解码器，我们忽略预测，并在步骤t时输入正确的先前输出**y**[*t* - 1]。当计算损失时，预测仍然被使用。
- en: 'This makes teacher forcing a little easier to implement because we don’t have
    to guess what comes next: we have the answer. This is also why our code needs
    the true labels Y as input during training so that we can compute the teacher-forcing
    result.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得教师强制更容易实现，因为我们不需要猜测接下来会发生什么：我们已经有答案了。这也是为什么我们的代码在训练期间需要真实的标签Y作为输入，以便我们可以计算教师强制结果。
- en: 11.3.3  Teacher forcing vs. an autoregressive approach
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.3 教师强制与自回归方法的比较
- en: 'The teacher-forcing approach has the benefit of giving the network the *correct*
    answer from which to continue its predictions. This makes it easier to predict
    all subsequent tokens correctly. The intuition is simple: it is harder to be correct
    on the tth prediction if you have been wrong for all of the *t* − 1 previous predictions.
    Teacher forcing helps the network by allowing it to learn simultaneously all the
    words it needs to predict.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 教师强制方法的好处是给网络提供正确的答案，以便继续其预测。这使得正确预测所有后续标记变得更容易。直觉很简单：如果你在前`t` - 1次预测中都错了，那么在`t`次预测中保持正确就更加困难。教师强制通过允许网络同时学习它需要预测的所有单词来帮助网络。
- en: 'The autoregressive approach can be slower to learn because the network has
    to learn to predict the first word correctly before it can focus on the second,
    and then the second before the third, and so on. But when we want to make predictions
    on new data, where we *do not know the answer*, teacher forcing is impossible:
    we have to make predictions in an autoregressive fashion, so it’s good for the
    model to learn to make predictions this way.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归方法可能学习速度较慢，因为网络必须先正确预测第一个单词，然后才能专注于第二个单词，然后是第三个，依此类推。但是当我们想要对新数据进行预测，而我们*不知道答案*时，教师强制（teacher
    forcing）是不可能的：我们必须以自回归的方式做出预测，因此对于模型学习以这种方式进行预测是有好处的。
- en: The practical solution we use is to combine the autoregressive and teacher-forcing
    approaches. For each input, we randomly decide which approach we want to take,
    so we will train with both. But at prediction time, we only do the autoregressive
    option since teacher forcing requires knowing the answer. A fancier approach is
    to switch between teacher forcing and autoregressive in a single batch, but that’s
    painful to implement. Picking one option per batch will make our code easier to
    write and read.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的实际解决方案是将自回归和教师强制方法结合起来。对于每个输入，我们随机决定我们想要采取哪种方法，因此我们将同时进行训练。但是在预测时间，我们只执行自回归选项，因为教师强制需要知道答案。一个更复杂的方法是在单个批次中在教师强制和自回归之间切换，但这很难实现。每个批次选择一个选项将使我们的代码更容易编写和阅读。
- en: The use of teacher forcing is also why we made the target sequence Y part of
    the input to our network. We use the `self.training` flag to switch between different
    behaviors when training and predicting with our model.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 教师强制的使用也是我们为什么将目标序列Y作为网络输入的一部分的原因。我们使用`self.training`标志在训练和预测我们的模型时切换不同的行为。
- en: 11.4 Seq2Seq with attention
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.4 带有注意力的Seq2Seq
- en: 'What we have shown and described so far is technically enough to implement
    a sequence-to-sequence-style model. However, it will not learn very well and will
    have poor performance. Including an attention mechanism is the key to making Seq2Seq
    work. The attention mechanism can work with both teacher forcing and the autoregressive
    approach and will change how we make predictions about the current word at the
    tth step of the RNN. So rather than have the decoder RNN predict **ŷ**[t], we
    have it produce a latent value **ẑ**[t]. The value **ẑ**[t] is our *context*
    for the attention mechanism. Figure 11.6 shows the process for predicting the
    tth word in four main components:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止所展示和描述的技术上足够实现一个序列到序列风格的模型。然而，它不会很好地学习，并且性能会很差。引入注意力机制是使Seq2Seq工作起来的关键。注意力机制可以与教师强制和自回归方法一起工作，并将改变我们在RNN的第t步预测当前单词的方式。因此，我们不是让解码RNN预测**ŷ**[t]，而是让它产生一个潜在值**ẑ**[t]。值**ẑ**[t]是注意力机制中的*上下文*。图11.6显示了预测第t个单词的四个主要组件的过程：
- en: The encoder step learns a useful representation of the input.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码步骤学习输入的有用表示。
- en: The decoder step predicts a context for each item of the output.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码步骤为输出中的每个项目预测一个上下文。
- en: The attention step uses the context to produce an output **x̄**[t], which is
    combined with the context **ẑ**[t].
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意力步骤使用上下文产生一个输出**x̄**[t]，并将其与上下文**ẑ**[t]结合。
- en: The prediction step takes the combined attention/context result and predicts
    the next token of the sequence.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测步骤将结合的注意力/上下文结果预测序列的下一个标记。
- en: '![](../Images/CH11_F06_Raff.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F06_Raff.png)'
- en: Figure 11.6 The process of applying attention to predict one item at a time
    of the output. Each highlighted region shows one of the four steps of encoding,
    decoding the context, attention, and prediction. This is repeated until an EOS
    token is found or a maximum limit is reached.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6 应用注意力预测输出中每个项目的流程。每个突出显示的区域显示了四个步骤之一：编码、解码上下文、注意力和预测。这会重复进行，直到找到EOS标记或达到最大限制。
- en: You can see that the attention block (which could use any of the three score
    functions we defined) takes the context **ẑ**[t] and the hidden states **h**[1],
    **h**[2], …, **h**[T] from the encoder RNN as its inputs.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，注意力块（可以使用我们定义的三个评分函数中的任何一个）将上下文**ẑ**[t]和来自编码RNN的隐藏状态**h**[1]，**h**[2]，…，**h**[T]作为其输入。
- en: The attention mechanism, combined with the `apply_attn` module from before,
    produces a weight *α*[1], *α*[2], …, *α*[T] for each hidden state. We then use
    the hidden states and weights to compute a final context for the current time
    step t, ***x̄**[t]* = Σ*[i]^T*[=1] *α[i]* **h**[i]. Since each **h**[i] is most
    influenced by the ith input, this gives the Seq2Seq model a way to look at just
    a subset of the input sequence as being relevant to predicting the tth item of
    the output.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制，结合之前使用的 `apply_attn` 模块，为每个隐藏状态生成一个权重 *α*[1]，*α*[2]，…，*α*[T]。然后我们使用隐藏状态和权重来计算当前时间步
    t 的最终上下文，***x̄**[t]* = Σ*[i]^T*[=1] *α[i]* **h**[i]。由于每个 **h**[i] 最受第 i 个输入的影响，这为
    Seq2Seq 模型提供了一种只查看输入序列的子集作为预测输出第 t 项相关的方法。
- en: To complete this, we concatenate the attention output **x̄**[t] with the local
    context **ẑ**[t], giving us a new vector
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个，我们将注意力输出 **x̄**[t] 与局部上下文 **ẑ**[t] 连接起来，得到一个新的向量
- en: '![](../Images/CH11_F06_EQ01.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F06_EQ01.png)'
- en: which we feed into a final fully connected network to transform into our final
    prediction, **ŷ**[t].
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其输入到一个最终的完全连接网络中，以将其转换为我们的最终预测，**ŷ**[t]。
- en: This may seem daunting, but you have already written or used code for all of
    these steps. The encoder uses a normal `nn.GRU` layer because it can return a
    tensor of shape (*B*,*T*,*H*) giving us all the outputs, which you learned to
    do in chapter 4\. We use a `nn.GRUCell` for the context predictions **ẑ**[t] because
    it has to happen step by step; you used this in chapter 6 for the autoregressive
    model. In the same chapter, you used sampling to select the next token and trained
    the model using a teacher-forcing-like approach. We just learned and used the
    attention mechanism, and you learned about the concatenated output feed into another
    layer in chapter 7 for U-Net.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来有些令人畏惧，但你已经为所有这些步骤编写或使用过代码。编码器使用一个普通的 `nn.GRU` 层，因为它可以返回形状为 (*B*,*T*,*H*)
    的张量，提供所有输出，这是你在第4章中学到的。我们使用 `nn.GRUCell` 来进行上下文预测 **ẑ**[t]，因为它必须一步一步地进行；你在第6章中用它来自回归模型。在同一章中，你使用采样来选择下一个标记，并使用类似教师强制的方法训练模型。我们刚刚学习和使用了注意力机制，你在第7章中学习了将连接输出输入到另一个层中用于U-Net。
- en: 11.4.1  Implementing Seq2Seq
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.1 实现Seq2Seq
- en: We’re ready to implement the Seq2Seq model. To set up our constructor, let’s
    talk about what components we need in the model itself.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好实现 Seq2Seq 模型。为了设置我们的构造函数，让我们谈谈模型本身需要哪些组件。
- en: First we need an `nn.Embedding` layer to convert tokens into feature vectors,
    similar to the char-RNN model from chapter 7\. We use the `padding_idx` option
    to state which token value is used to indicate padding since there are multiple
    different sequence lengths.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要一个 `nn.Embedding` 层来将标记转换为特征向量，类似于第7章中的 char-RNN 模型。我们使用 `padding_idx`
    选项来指定哪个标记值用于表示填充，因为存在多个不同的序列长度。
- en: Next we need an encoding RNN and a decoding RNN. We use the GRU because it’s
    a little simpler to write code for compared to the LSTM. In particular, we will
    code each RNN differently. For the encoder, we use the normal `nn.GRU` module,
    which takes in a tensor of shape (*B*,*T*,*D*), since it expects all T items at
    once. This is easier to code and also lets us use the bidirectional option with
    ease. Since we have the entire input, the bidirectional option is a good choice.[⁴](#fn43)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个编码器 RNN 和一个解码器 RNN。我们使用 GRU，因为它相对于 LSTM 来说编写代码要简单一些。特别是，我们将分别对每个 RNN
    进行编码。对于编码器，我们使用正常的 `nn.GRU` 模块，它接受形状为 (*B*,*T*,*D*) 的张量，因为它期望一次性接受所有 T 项。这更容易编写代码，也让我们能够轻松地使用双向选项。由于我们有整个输入，双向选项是一个不错的选择。[⁴](#fn43)
- en: The decoder RNN can’t be bidirectional because we are generating the output
    one item at a time. This also means we can’t use the normal `nn.GRU` module, because
    it expects all *T*′ items of the output to be ready at once, but we do not know
    how long each output is until we have encountered all the EOS tokens. To fix this,
    we use the `nn.GRUCell`. This will require us to keep track of the hidden states
    and multiple layers of the decoder manually, and we have to write a `for` loop
    to keep iterating over the predictions until we have a complete result.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器 RNN 不能是双向的，因为我们一次生成一个输出项。这也意味着我们不能使用正常的 `nn.GRU` 模块，因为它期望所有 *T*′ 个输出项同时准备好，但我们不知道每个输出有多长，直到我们遇到所有的
    EOS 标记。为了解决这个问题，我们使用 `nn.GRUCell`。这将需要我们手动跟踪解码器的隐藏状态和多层，我们必须编写一个 `for` 循环来持续迭代预测，直到我们得到完整的结果。
- en: 'To make sure we do not end up in an infinite loop in the case of a bad prediction,
    we include a `max_decode_length` to enforce a maximum number of decoding steps
    through the decoder RNN that we are willing to attempt. Finally, we need our `ApplyAttention`
    module with some `score_net` for computing the scores (we use the `DotScore`)
    and a small network `predict_word` to predict the next word. The following code
    snippet covers all the items we have discussed and creates the constructor for
    our Seq2Seq model:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保在出现不良预测的情况下不会陷入无限循环，我们包含一个 `max_decode_length` 来强制执行通过解码器 RNN 的最大解码步骤数。最后，我们需要我们的
    `ApplyAttention` 模块和一些 `score_net` 来计算分数（我们使用 `DotScore`），以及一个小型网络 `predict_word`
    来预测下一个单词。以下代码片段涵盖了我们所讨论的所有内容，并为我们的 Seq2Seq 模型创建了一个构造函数：
- en: '[PRE6]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ We set the hidden size to half the intended length because we make the encoder
    bidirectional. That means we get two hidden state representations, which we concatenate
    together, giving us the desired size.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们将隐藏大小设置为预期长度的一半，因为我们使编码器双向。这意味着我们得到两个隐藏状态表示，我们将它们连接起来，得到所需的大小。
- en: ❷ The decoder is uni-directional, and we need to use GRUCells so that we can
    do the decoding one step at a time.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 解码器是单向的，我们需要使用 GRUCells 以便我们可以逐个步骤地进行解码。
- en: ❸ predict_word is a small, fully connected network that converts the result
    of the attention mechanism and the local context into a prediction of the next
    word.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ `predict_word` 是一个小型全连接网络，它将注意力机制的结果和局部上下文转换为对下一个单词的预测。
- en: Now we can talk about how to implement the `forward` function of the Seq2Seq
    algorithm. Figure 11.7 outlines the process.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以讨论如何实现 Seq2Seq 算法的 `forward` 函数。图 11.7 概述了该过程。
- en: '![](../Images/CH11_F07_Raff.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F07_Raff.png)'
- en: Figure 11.7 Outline of the forward function and the seven steps in its implementation.
    Each block shows one unit of work, and arrows show sequential tasks.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 `forward` 函数的概述及其实现中的七个步骤。每个块显示一个工作单元，箭头显示顺序任务。
- en: 'We will walk through these blocks in the order they would run according to
    our diagram, explain what is happening, and show the code to make it happen. Note
    that in the diagram, we have separated out two lists: `all_attentions` and `all_predictions`.
    These collect the attention and prediction scores so that we can make both available
    from the model to look at the attention scores and examine the predictions or
    pass them to any subsequent module we may want to use.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照我们的图中的顺序遍历这些块，解释正在发生的事情，并展示实现它们的代码。请注意，在图中，我们已经将两个列表分开：`all_attentions`
    和 `all_predictions`。这些列表收集注意力和预测分数，以便我们可以从模型中获取并查看注意力分数，或者将它们传递给任何我们可能想要使用的后续模块。
- en: Prep, embedding, and mask block
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 准备、嵌入和掩码块
- en: 'The first thing we need to do in our function is some prep and organization.
    The input can be a tensor of shape (*B*,*T*) or a tuple of two tensors ((*B*,*T*),(*B*,*T*′)),
    depending on whether we are in testing or training mode, respectively. We check
    what our input is and extract the `input` and `target` values appropriately. We
    `embd` all of the input values and compute useful things like our `mask`; and
    from the `mask`, we can determine how long each sequence is. The length is the
    number of `True` values, so we can get the `seq_lengths` with a simple `sum` call.
    We also grab the compute device being used, for later when we need to sample the
    next input for the decoder:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的函数中，我们首先需要做一些准备和组织工作。输入可以是形状为 (*B*,*T*) 的张量，或者是一个包含两个张量的元组 ((*B*,*T*),(*B*,*T*′))，这取决于我们是在测试模式还是训练模式。我们检查输入的内容，并适当地提取
    `input` 和 `target` 值。我们 `embd` 所有的输入值，并计算一些有用的东西，比如我们的 `mask`；从 `mask` 中，我们可以确定每个序列的长度。长度是
    `True` 值的数量，因此我们可以通过简单的 `sum` 调用得到 `seq_lengths`。我们还获取正在使用的计算设备，以便稍后当我们需要为解码器采样下一个输入时使用：
- en: '[PRE7]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Input should be (B, T) or ((B, T), (B, T’))
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入应为 (B, T) 或 ((B, T), (B, T’))
- en: ❷ What is the batch size?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 批处理大小是多少？
- en: ❸ What is the max number of input time steps?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 最大输入时间步数是多少？
- en: ❹ (B, T, D)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ (B, T, D)
- en: ❺ Grabs the device that the model currently resides on. We need this later.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 获取模型当前所在的设备。我们稍后会需要这个信息。
- en: ❻ Shape (B, T)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 形状 (B, T)
- en: ❼ Shape (B), containing the number of nonzero values
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 形状（B），包含非零值的数量
- en: Encoding block
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 编码块
- en: Now that our data and masks are ready, we need to push the data through our
    encoder network. To maximize throughput, we pack the input data before feeding
    it to the RNN, which is easy and fast to do since we computed the `seq_lengths`
    from the `mask`. In addition, `h_last` contains the last activation even with
    variable-length items, simplifying our code. We do need to unpack `h_encoded`
    for our attention mechanism later and reshape it to (*B*,*T*,*D*), since we are
    using a bidirectional model. Some similar shape manipulation will make sure `h_last`
    is of shape (*B*,*D*) instead of the default (2,*B*,*D*/2) from our bidirectional
    approach.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们数据和掩码已经准备好了，我们需要将数据推送到我们的编码器网络。为了最大化吞吐量，我们在将其输入到RNN之前对输入数据进行打包，由于我们已经从`mask`中计算了`seq_lengths`，所以这既简单又快速。此外，`h_last`即使在变长项中也包含最后一个激活，简化了我们的代码。我们确实需要为后续的注意力机制解包`h_encoded`，并将其重塑为(*B*,*T*,*D*)，因为我们使用的是双向模型。一些类似的形状操作将确保`h_last`的形状为(*B*,*D*)，而不是我们双向方法默认的(2,*B*,*D*/2)。
- en: 'Here’s the code:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是代码：
- en: '[PRE8]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ The sequence lengths are used to create a packed input for the encoder RNN.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用序列长度为编码器RNN创建打包的输入。
- en: ❷ (B, T, 2, D//2) because it is bidirectional
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ (B, T, 2, D//2)，因为它是双向的
- en: ❸ (B, T, D). Now h_encoded is the result of running the encoder RNN on the input!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ (B, T, D)。现在`h_encoded`是编码器RNN在输入上运行的结果！
- en: ❹ Getting the last hidden state is a little trickier. First the output is reshaped
    as (num_layers, directions, batch_size, hidden_size); then we grab the last index
    in the first dimension because we want the last layer’s output.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 获取最后一个隐藏状态有点复杂。首先，输出被重塑为(num_layers, directions, batch_size, hidden_size)；然后我们抓取第一个维度中的最后一个索引，因为我们想要最后一层的输出。
- en: ❺ The shape is now (2, B, D/2).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 现在的形状是(2, B, D/2)。
- en: ❻ Reorders to (B, 2, D/2) and flattens the last two dimensions down to (B, D)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 重排为(B, 2, D/2)并将最后两个维度展平到(B, D)
- en: Decode Prep Block
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 解码准备块
- en: Before we start the decoding block, we need to do a little prep. First, we store
    a list of the previous hidden state activations of the decoder RNN. We do this
    because we are using `GRUCell`s, which need us to keep track of hidden state activations
    so we can make sequential steps of the RNN run efficiently.[⁵](#fn44)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始解码块之前，我们需要做一些准备工作。首先，我们存储一个列表，其中包含解码器RNN的前一个隐藏状态激活。我们这样做是因为我们使用的是`GRUCell`，它需要我们跟踪隐藏状态激活，以便我们可以使RNN的顺序步骤运行得更加高效。[⁵](#fn44)
- en: To make our code simpler, we reuse the `embd` layer for encoding the inputs
    to the decoder. This is OK because the `embd` layer does very little work; most
    of it is done by the decoder layer. Since we will make the last input to the encoder
    be the first input to the decoder, we need to grab that. Doing `input[:,seq_lengths-1]`
    looks like it should work, but it returns a tensor that is of shape (*B*,*B*,*D*)
    instead of (*B*,*D*). To make this work the way we want, we need to use the `gather`
    function instead, which gathers the specified indices along the specified axis
    (`1`).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的代码更简单，我们重用`embd`层来编码解码器的输入。这是可以的，因为`embd`层做的工作非常少；大部分工作都是由解码器层完成的。由于我们将使编码器的最后一个输入成为解码器的第一个输入，我们需要获取这个输入。执行`input[:,seq_lengths-1]`看起来应该可以工作，但它返回一个形状为(*B*,*B*,*D*)的张量，而不是(*B*,*D*)。为了使它按我们的意愿工作，我们需要使用`gather`函数，它沿着指定的轴(`1`)收集指定的索引。
- en: 'All of this is in the following code block for the decoder prep, which ends
    with figuring out the number of steps for which we need to run the decoder:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都在以下解码器准备代码块中，它以确定我们需要运行解码器的步骤数结束：
- en: '[PRE9]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ New hidden states for decoders
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为解码器生成新的隐藏状态
- en: ❷ Grabs the last item from the input (which should be an EOS marker) as the
    first input for the decoder. We could also hardcode the SOS marker. (B, D).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从输入中获取最后一个项目（应该是EOS标记）作为解码器的第一个输入。我们也可以硬编码SOS标记。(B, D)。
- en: ❸ How many decoding steps should we do?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们应该进行多少解码步骤？
- en: ❹ If we are training, the target value tells us exactly how many steps to take.
    We know the exact decode length.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 如果我们在训练，目标值会告诉我们确切需要走多少步。我们知道确切的解码长度。
- en: Compute context and attention block
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 计算上下文和注意力块
- en: 'Now we need to compute the context and attention results. This is running inside
    a `for` loop over `t` `steps`. The variable `decodr_input` has our current input
    to workon: the value selected from the previous prep step or the value we compute
    in the next two steps.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要计算上下文和注意力结果。这是在`t`步骤的`for`循环中运行的。变量`decodr_input`是我们的当前输入：从上一个准备步骤中选择的值或我们在接下来的两个步骤中计算的值。
- en: 'We have our `GRUCell`s in a list of `decode_layers` that we iterate through
    to push the batch through the layers ourselves, just as we did in chapter 6 with
    the autoregressive model. Once done, our result **ẑ**[t] is stored in a variable
    called `h_decoder`. A call to our `score_net` gets the normalized scores, and
    then `apply_attn` returns the **x̄**[t] and the softmax weights α in the variables
    `context` and `weights`, respectively. This is shown in the following chunk of
    code:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`GRUCell`放在`decode_layers`列表中，我们遍历这个列表，自己将批量推过层，就像我们在第6章中用自回归模型做的那样。一旦完成，我们的结果**ẑ**[t]存储在一个名为`h_decoder`的变量中。对`score_net`的调用获取归一化分数，然后`apply_attn`返回变量`context`和`weights`中的**x̄**[t]和softmax权重α。这在上面的代码块中显示：
- en: '[PRE10]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ (B, D)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ (B, D)
- en: ❷ (B, D). We now have the hidden state for the decoder at this time step.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ (B, D)。我们现在有了解码器在这个时间步的隐藏状态。
- en: ❸ This is the attention mechanism. Let’s look at all the previous encoded states
    and see which ones look relevant. (B, T, 1) ![](../Images/tilde_alpha.png).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 这就是注意力机制。让我们看看所有之前的编码状态，看看哪些看起来相关。(B, T, 1) ![../Images/tilde_alpha.png]。
- en: ❹ (B, D) for x̄ and (B, T) for α
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ (B, D)用于x̄和(B, T)用于α
- en: ❺ Saves the attention weights for visualization later. We are detaching the
    weights because we do not want to compute anything else with them; we just want
    to save their values to make visualizations.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 保存注意力权重以供后续可视化。我们正在断开连接权重，因为我们不想用它们计算任何其他东西；我们只想保存它们的值以进行可视化。
- en: Compute prediction block
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 计算预测块
- en: 'With the previous task done, we can use `torch.cat` to combine **x̄**[t] and
    **ẑ**[t] and feed the result to `predict_word` to get our final prediction **ŷ**[t]
    for the `t`th output:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成前面的任务后，我们可以使用`torch.cat`将**x̄**[t]和**ẑ**[t]组合起来，并将结果输入到`predict_word`中以获取我们的最终预测**ŷ**[t]对于`t`th输出：
- en: '[PRE11]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '❶ Compute the final representation by concatenating the attention result and
    the initial context: (B, D) + (B, D) -> (B, 2*D).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过连接注意力结果和初始上下文来计算最终表示：(B, D) + (B, D) -> (B, 2*D)。
- en: '❷ Get a prediction about what the next token is by pushing it through a small
    fully connected network: (B, 2*D) -> (B, V).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过将下一个标记推入一个小型全连接网络来获取关于下一个标记的预测：(B, 2*D) -> (B, V)。
- en: This is another situation where `nn.Sequential` helps make the code clean, since
    `predict_word` is an entire neural network that we do not need to think about.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这又是`nn.Sequential`帮助使代码整洁的情况，因为`predict_word`是一个完整的神经网络，我们不需要考虑它。
- en: Select the decoder’s next input block
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 选择解码器的下一个输入块
- en: 'We are almost done with Seq2Seq, with one last part to implement: selecting
    the next value for `decoder_input` at step *t* + 1. This is done in the `with torch.no_grad()`
    context so that we can do the work we need. First, we check whether the model
    is in `self.training` mode, because if it is not, we can bail out and simply select
    the most likely word. If we are in training mode, we check whether we should use
    `teacher_forcing` and defensively check that our `target` values are available.
    If both are true, we make the input for *t* + 1 be the true output that should
    have occurred for the current time step t. Otherwise, we sample the next work
    in the autoregressive manner. Here’s the code:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎完成了Seq2Seq的实现，只剩下一部分需要实现：在步骤*t* + 1选择`decoder_input`的下一个值。这是在`with torch.no_grad()`上下文中完成的，这样我们就可以做我们需要的工作。首先，我们检查模型是否处于`self.training`模式，因为如果不是，我们可以退出并简单地选择最可能的词。如果我们处于训练模式，我们检查是否应该使用`teacher_forcing`，并防御性地检查我们的`target`值是否可用。如果两者都为真，我们将*t*
    + 1的输入设置为当前时间步t应该发生的真实输出。否则，我们以自回归的方式采样下一个词。以下是代码：
- en: '[PRE12]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ We have the target and selected teacher forcing, so use the correct next answer.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们有目标和选定的教师强制，所以使用正确的下一个答案。
- en: ❷ Samples the next token based on the predictions made
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 根据预测采样下一个标记
- en: ❸ We are trying to make an actual prediction, so we take the most likely word.
    We could improve this by using temperature and sampling, as we did for the char-RNN
    model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们正在尝试做出实际预测，所以我们取最可能的词。我们可以通过使用温度和采样来改进这一点，就像我们在char-RNN模型中做的那样。
- en: There are two things worth noting that could be improved in this code. First,
    instead of taking the most likely next word at test time, we could do sampling
    as we did with the autoregressive model. Second, we could add a temperature option,
    as we used before, to change how often we chose the more likely words. I haven’t
    made these changes to keep the code a little simpler.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，有两点值得注意，可以改进。首先，在测试时间，我们不是取最可能的下一个词，而是可以像自回归模型那样进行采样。其次，我们可以添加一个温度选项，就像我们之前使用的那样，以改变我们选择更可能词的频率。我没有做出这些更改，以使代码更简单。
- en: Once the `next_words` are selected and we exit the `with torch.no_grad()` block,
    we can set `decoder_input = self.embd(next_words.to(device))`. We need to wait
    until the `no_grad()` context is gone so gradients are tracked for this step.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选择了`next_words`并退出`with torch.no_grad()`块，我们就可以设置`decoder_input = self.embd(next_words.to(device))`。我们需要等待`no_grad()`上下文消失，以便跟踪这一步的梯度。
- en: Return the result block
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 返回结果块
- en: 'Finally we are at the end of our Seq2Seq implementation. The last step is to
    return our results. If we are in training mode, we just need to give back the
    predictions by stacking the predictions for all *T*′ words together, giving us
    `torch.stack(all_predictions, dim=1)`. If we are in evaluation mode, we also want
    to get the attention scores so that we can examine them. Similar to the predictions,
    they are stacked together:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们到达了Seq2Seq实现的终点。最后一步是返回我们的结果。如果我们处于训练模式，我们只需要通过堆叠所有*T*′个单词的预测来返回预测，得到`torch.stack(all_predictions,
    dim=1)`。如果我们处于评估模式，我们还想获取注意力分数，以便我们可以检查它们。与预测类似，它们也被堆叠在一起：
- en: '[PRE13]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ When training, only the predictions are important.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在训练时，只有预测是重要的。
- en: ❶ When evaluating, we also want to look at the attention weights.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在评估时，我们还想查看注意力权重。
- en: 11.4.2  Training and evaluation
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.2 训练和评估
- en: We have defined a Seq2Seq model, and now we can try it. We use 20 epochs of
    training and a few layers. Since this is an RNN, we also use gradient clipping
    as part of our training. An embedding dimension of 64 and hidden neuron size of
    256 is on the smaller side to make this run faster; I would prefer to set these
    values to 128 and 512 if I was willing to wait longer and had more data.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了一个Seq2Seq模型，现在我们可以尝试使用了。我们使用20个训练周期和几个层。由于这是一个RNN，我们在训练中也使用了梯度裁剪。64维的嵌入维度和256个隐藏神经元的大小属于较小的一侧，以便使运行更快；如果我有更多的时间和数据，我更愿意将这些值设置为128和512。
- en: 'Here’s the code:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码：
- en: '[PRE14]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Defining a loss function
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 定义损失函数
- en: The last thing we need is a loss function to train our network. The standard`nn.CrossEntropyLoss`
    does not handle this situation where our output has a shape of (*B*,*T*,*V*),
    with V as the size of the vocabulary. Instead, we iterate through all T time steps
    of the output and slice off the correct piece of the input and labels that we
    can call `nn.CrossEntropyLoss` without any errors. This is the same kind of approach
    we used for training the autoregressive model in chapter 6.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后需要的是一个损失函数来训练我们的网络。标准的`nn.CrossEntropyLoss`无法处理我们的输出形状为(*B*,*T*,*V*)的情况，其中V是词汇表的大小。相反，我们遍历输出的所有T个时间步，并从输入和标签中切下正确的部分，这样我们就可以调用`nn.CrossEntropyLoss`而不会出现任何错误。这与我们在第6章中用于训练自回归模型的方法相同。
- en: 'The only change we make is the use of the `ignore_index` value. If the label`y=ignore_index`,
    the `nn.CrossEntropyLoss` does not calculate any loss for that value. We can use
    this to handle the padding tokens, as we do not want the network to learn to predict
    padding; we want it to predict the EOS token when appropriate and then be finished.
    This allows us to make our loss function understand that the output has padding,
    too:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所做的唯一改变是使用`ignore_index`值。如果标签`y=ignore_index`，`nn.CrossEntropyLoss`不会为该值计算任何损失。我们可以使用这一点来处理填充标记，因为我们不希望网络学习预测填充；我们希望它在适当的时候预测EOS标记，然后完成。这允许我们的损失函数理解输出也有填充：
- en: '[PRE15]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ We do not want to compute a loss for items that have been padded!
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们不希望为填充过的项目计算损失！
- en: 'With that, we can call the `train_network` function with our Seq2Seq model
    and our new loss function. We are not using the validation loss right now for
    a few reasons. We would need to add more code to our `train_network` function
    to support it, since our Seq2Seq model changes its output at evaluation time.
    A bigger issue is that the loss function we are using is not very intuitive to
    look at. Still, we plot the training loss afterward to make sure it’s going down,
    to confirm that learning is happening:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们可以使用我们的Seq2Seq模型和新的损失函数调用`train_network`函数。目前我们不使用验证损失，有几点原因。我们需要在我们的`train_network`函数中添加更多代码来支持它，因为我们的Seq2Seq模型在评估时会改变其输出。更大的问题是，我们使用的损失函数看起来并不直观。尽管如此，我们之后绘制训练损失图，以确保它在下降，以确认学习正在发生：
- en: '[PRE16]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../Images/CH11_UN01_Raff.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_UN01_Raff.png)'
- en: BLEU about validation loss?
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 关于验证损失的BLEU？
- en: Evaluating Seq2Seq models can be particularly challenging. How do you know if
    a translation is bad when there are multiple valid translations? This is something
    we are ignoring during training, yet somehow the model still works surprisingly
    well.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 评估Seq2Seq模型可能特别具有挑战性。当存在多个有效翻译时，你怎么知道一个翻译是错误的？这是我们训练过程中忽略的东西，但不知何故，模型仍然表现得相当出色。
- en: The loss function we used for training is what people use to train Seq2Seq models
    in general, but people tend to use different evaluation metrics to determine how
    good their model is. These evaluation metrics are fairly complicated, so we stick
    with a more subjective evaluation to avoid adding too much to this chapter. If
    you want to learn more, machine translation is normally evaluated against BLEU
    ([https://en.wikipedia.org/wiki/BLEU](https://en.wikipedia.org/wiki/BLEU)) scores.
    But BLEU may not be the best metric if you are not doing a translation task, as
    BLEU is specifically designed for translation.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于训练的损失函数是人们通常用来训练Seq2Seq模型的东西，但人们倾向于使用不同的评估指标来确定他们的模型有多好。这些评估指标相当复杂，所以我们坚持使用更主观的评估，以避免在本章中添加太多内容。如果你想了解更多，机器翻译通常与BLEU([https://en.wikipedia.org/wiki/BLEU](https://en.wikipedia.org/wiki/BLEU))分数进行比较。但如果你不是在进行翻译任务，BLEU可能不是最好的指标，因为BLEU是专门为翻译设计的。
- en: Visualizing attention score maps
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化注意力分数图
- en: Looking at the loss of the network, we can clearly see that it has decreased
    during training. To help us evaluate the results, we can make some translations
    and look at the attention mechanism’s results. This will be a subjective analysis,
    but I encourage performing a subjective evaluation whenever working with Seq2Seq
    models. Objective evaluations tend to be difficult when you have many-to-many
    mappings, so getting dirty with the data helps build an understanding of what
    is happening. At each time step t, the attention mechanism tells us which input
    words were considered important for predicting each output word. This can help
    us understand whether the model is learning something reasonable.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 观察网络的损失，我们可以清楚地看到它在训练过程中有所下降。为了帮助我们评估结果，我们可以进行一些翻译并查看注意力机制的结果。这将是一种主观分析，但我鼓励在处理Seq2Seq模型时进行主观评估。在存在多对多映射的情况下，客观评估往往很困难，因此与数据打交道有助于理解正在发生的事情。在每一个时间步t，注意力机制会告诉我们哪些输入单词对于预测每个输出单词是重要的。这有助于我们了解模型是否在学习合理的东西。
- en: 'We define the following `plot_heatmap` function to quickly plot the attention
    results. The `results` function will use it to take in an input, translate it
    to French, and show the prediction, attention, and true translation:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了以下`plot_heatmap`函数来快速绘制注意力结果。`results`函数将使用它来接收输入，将其翻译成法语，并显示预测、注意力和真实翻译：
- en: '[PRE17]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Puts the major ticks at the middle of each cell and the x-ticks on top
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将主要刻度放在每个单元格的中间，并将x刻度放在顶部
- en: 'Before defining the results function, let’s quickly put our `seq2seq` model
    into evaluation mode. This way, we get the attention maps. Then the function can
    use our inverse map `indx2word` to see what the original data and predictions
    should have been. We print out the input and target, along with `seq2seq`’s prediction.
    At the end, we show the heat map of the attention scores that will help us subjectively
    evaluate the results. The input to this function is simply the index into the
    test set that we want to consider:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义结果函数之前，让我们快速将我们的`seq2seq`模型置于评估模式。这样，我们就能得到注意力图。然后函数可以使用我们的逆映射`indx2word`来查看原始数据和预测应该是什么。我们打印出输入和目标，以及`seq2seq`的预测。最后，我们展示注意力分数的热图，这将帮助我们主观地评估结果。这个函数的输入仅仅是我们要考虑的测试集索引：
- en: '[PRE18]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now we can look at some results. You may get slightly different results, depending
    on your model’s training run. For the first sentence, I get the translation “les
    animaux ont peur du feu,” which Google Translate says translates back into the
    English sentence “some animals are afraid of fire.” In this case, the model had
    a different result that was *mostly* correct, but it used “les,” which translates
    as “the” instead of the more appropriate “certain.”
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看看一些结果。根据你的模型训练运行的不同，你可能会得到略微不同的结果。对于第一句话，我得到的翻译是“les animaux ont peur
    du feu”，谷歌翻译说这翻译回英文句子是“some animals are afraid of fire”。在这种情况下，模型得到了一个*基本上*正确的结果，但它使用了“les”，这翻译成“the”而不是更合适的“certain”。
- en: The attention map shows that “les” was indeed looking at the correct part of
    the input (“some”) but maybe focusing too much on the start of the sentence. If
    I had to guess, “the” is just a more common start of a sentence than “some,” and
    the network made the error based on that. But we can also see that “du feu” correctly
    attends to the “of fire” portion of the input to produce the correct output. While
    the math is not perfect, the model picked a word with a similar meaning for understandable
    reasons.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力图显示，“les”确实在查看输入的正确部分（“一些”），但可能过于关注句子的开头。如果我必须猜测，“the”可能比“some”更常见于句首，网络基于这一点犯了错误。但我们也可以看到，“du
    feu”正确地关注了输入中的“of fire”部分，从而产生了正确的输出。虽然数学并不完美，但模型出于可理解的原因选择了具有相似意义的单词。
- en: 'We can also see how we can use the action mechanism to better understand our
    model’s results. This is something very powerful about attention mechanisms: they
    give us a degree of *interpretability* in our otherwise opaque neural networks.
    In this case, maybe we need to get more diverse sentences with a variety of starting
    tokens/phrases.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到如何使用动作机制来更好地理解我们的模型结果。这是注意力机制的一个非常强大的特点：它们为我们提供了对通常不透明的神经网络的一定程度的*可解释性*。在这种情况下，可能需要获取更多具有各种起始标记/短语的多样化句子。
- en: 'Here’s the code:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码：
- en: '[PRE19]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/CH11_UN02_Raff.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_UN02_Raff.png)'
- en: The next translation shows the result for a sentence for which I often see translations
    with greater differences. Here we get “quel temps fait il aujourd’hui” as the
    translation, changing the first four words from the original target of “comment
    est le temps.” While different than the target, the result is still a reasonably
    correct translation. This shows both that Seq2Seq has some very difficult things
    to learn around and the power of this approach to successfully learn despite these
    issues. This is also why evaluating the equality of machine translation tasks
    can be very difficult. The attention mechanism is not quite as crisp in this example
    since no individual word used means “weather” in this case.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个翻译展示了对于我经常看到翻译差异较大的句子，结果如下：“今天天气怎么样”，将原文的前四个词从“天气如何”进行了改变。虽然与目标不同，但结果仍然是一个相当正确的翻译。这表明Seq2Seq在学习和处理这些困难问题时具有一定的能力，尽管存在这些问题，这种方法仍然能够成功学习。这也是为什么评估机器翻译任务的平等性可以非常困难。在这个例子中，注意力机制并不那么清晰，因为在这种情况下，没有单个单词意味着“天气”。
- en: 'Here’s the code:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码：
- en: '[PRE20]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/CH11_UN03_Raff.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_UN03_Raff.png)'
- en: 'The next example shows another reasonable translation—Google Translate returns
    the same result for me. It looks like the model may have swapped out some synonyms
    or changed a gendered word, but I would need to know French to be sure. This is
    part of why I love machine learning—I can get computers to do the many things
    I’m incapable of on my own:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个例子显示了另一个合理的翻译——谷歌翻译为我返回了相同的结果。看起来模型可能替换了一些同义词或改变了性别化的单词，但为了确定这一点，我需要知道法语。这也是为什么我喜欢机器学习的原因之一——我可以让计算机做许多我自己无法做到的事情：
- en: '[PRE21]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](../Images/CH11_UN04_Raff.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_UN04_Raff.png)'
- en: If you had the computational resources to do a larger training set and also
    had more time, you could get results like the translation we showed at the start
    of the chapter in figure 11.1\. Not only does this show a longer sentence being
    converted from French to English, but a more refined output from the attention
    mechanism. It becomes obvious which exact word is being translated, and you can
    see the model correctly changes the order of “zone économique européenne” to “European
    Economic Area,” adapting to the linguistic nuances of the language.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你拥有进行更大训练集的计算资源，并且有更多时间，你就可以得到如图11.1所示的翻译结果。这不仅显示了从法语到英语的较长句子的转换，而且还显示了注意力机制的更精细输出。可以明显看出正在翻译的确切单词，并且可以看到模型正确地改变了“zone
    économique européenne”到“European Economic Area”的顺序，适应了语言的细微差别。
- en: While the previous code is still missing one major trick to maximize performance,
    this is not a toy implementation. This same approach has been used in real-world
    machine translation systems and was only supplanted by new approaches in 2019.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然之前的代码仍然缺少一个最大化性能的主要技巧，但这并不是一个玩具实现。这种方法已经在现实世界的机器翻译系统中使用，并且在2019年被新的方法所取代。
- en: Exercises
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Share and discuss your solutions on the Manning online platform at Inside Deep
    Learning Exercises ([https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945)).
    Once you submit your own answers, you will be able to see the solutions submitted
    by other readers, and see which ones the author judges to be the best.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Inside Deep Learning Exercises 的 Manning 在线平台上分享和讨论您的解决方案（[https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945)）。一旦您提交了自己的答案，您将能够看到其他读者提交的解决方案，并看到作者认为哪些是最好的。
- en: By default, the decoder users **h**[T] as its initial hidden state. Try changing
    it to use an average of the decoder’s outputs, 1/*T* Σ*[t]^T*[=1] ***h**[t]*.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认情况下，解码器用户 **h**[T] 作为其初始隐藏状态。尝试将其更改为使用解码器输出的平均值，即 1/*T* Σ*[t]^T*[=1] ***h**[t]*。
- en: Our Seq2Seq was hardcoded to `DotScore`. Try using `GeneralScore` and your new
    initialization of `GeneralScore` on the translation task. Does your evaluation
    show one performing better or worse than the others?
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的 Seq2Seq 模型是硬编码为 `DotScore` 的。尝试使用 `GeneralScore` 以及您对 `GeneralScore` 的新初始化在翻译任务上。您的评估结果显示哪个表现更好或更差？
- en: '**Challenging:** Create a new dataset for an image addition task. Make the
    input a random number of MNIST images and the output value a sequence of characters
    denoting the sum of the digits. So the input may be a sequence of the images for
    6, 1, and 9, then the output should be the string “16.” Then modify the Seq2Seq
    model to train and run on this problem. *Hint:* Modify the encoder and `collate_fn`.'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**挑战性任务：** 为图像加法任务创建一个新的数据集。将输入设为随机数量的 MNIST 图像，输出值设为表示数字之和的字符序列。因此，输入可能是一系列表示
    6、1 和 9 的图像，然后输出应该是字符串“16。”然后修改 Seq2Seq 模型以训练和运行此问题。*提示：* 修改编码器和 `collate_fn`。'
- en: Write out the reasons why the task in exercise 3 is challenging. What things
    will the network need to learn to solve the problem? Try to articulate how many
    different mental concepts and tasks exist to solve the problem and how they are
    explicitly or implicitly defined by the loss of the network.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写出为什么练习 3 中的任务具有挑战性的原因。网络需要学习哪些内容来解决这个问题？尝试阐述解决该问题需要多少不同的心理概念和任务，以及它们是如何通过网络的损失函数显式或隐式定义的。
- en: Modify the Seq2Seq model to use LSTM layers instead of GRU ones. This will require
    some extra tracking for the LSTM’s own context results.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Seq2Seq 模型修改为使用 LSTM 层而不是 GRU 层。这将需要对 LSTM 的自身上下文结果进行一些额外的跟踪。
- en: Try training models that do not use *any* teacher forcing—only the autoregressive
    approach. How do they perform comparatively?
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试训练不使用任何教师强制——只使用自回归方法的模型。它们的性能如何比较？
- en: Summary
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: We can train a denoising autoencoding-like model called sequence-to-sequence
    for solving problems with a many-to-many mapping.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以训练一个类似于去噪自编码的模型，称为序列到序列模型，用于解决具有多对多映射的问题。
- en: Machine translation is a many-to-many problem that we can implement and do well
    with a Seq2Seq model.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译是一个多对多的问题，我们可以通过 Seq2Seq 模型来实现并做得很好。
- en: Attention scores can be used as an interpretable output of the model, allowing
    us to better understand how the model makes decisions.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力分数可以用作模型的可解释输出，使我们更好地理解模型是如何做出决策的。
- en: The autoregressive approach can be combined with teacher forcing to help the
    model learn faster and better.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自回归方法可以与教师强制结合使用，以帮助模型更快、更好地学习。
- en: '* * *'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ¹ G. Lample and F. Charton, F. “Deep learning for symbolic mathematics,” in
    Proceeds of ICLR 2020.[↩](#fnref40)
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ G. Lample 和 F. Charton, F. “深度学习在符号数学中的应用”，载于 ICLR 2020 会议论文集。[↩](#fnref40)
- en: '² A. See, P. J. Liu, and C. D. Manning, “Get to the point: summarization with
    pointer-generator networks,” Association for Computational Linguistics, 2017.[↩](#fnref41)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '² A. See, P. J. Liu, 和 C. D. Manning, “Get to the point: summarization with
    pointer-generator networks,” 计算机语言学协会，2017。[↩](#fnref41)'
- en: ³ What “explainability” is, or if it is even a real thing, is actually a fiery
    topic in many ML circles. For agreat opinion piece, check out Zachary Lipton’s
    “The mythos of model interpretability" at [https://arxiv.org/abs/1606.03490](https://arxiv.org/abs/1606.03490).[↩](#fnref42)
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ³ “可解释性”是什么，或者它是否真的存在，在许多机器学习圈子中实际上是一个热烈讨论的话题。为了获得一篇优秀的观点文章，请参阅 Zachary Lipton
    的“模型可解释性的神话”一文，见 [https://arxiv.org/abs/1606.03490](https://arxiv.org/abs/1606.03490)。[↩](#fnref42)
- en: ⁴ The biggest choice of a bidirectional RNN is usually “Do I have access to
    the future/whole sequence at test time?” For our translation task, this is true,
    so it’s worth doing. But it’s not always true! If we wanted to make a Seq2Seq
    model that took in *live speech* in real time, we would need to use a non-bidirectional
    model, because we would not know the speaker’s future words.[↩](#fnref43)
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴ 双向 RNN 的最大选择通常是“在测试时我能否访问到未来的/整个序列？”对于我们的翻译任务来说，这是成立的，所以这是值得做的。但并非总是如此！如果我们想制作一个实时接收
    *现场语音* 的 Seq2Seq 模型，我们就需要使用一个非双向模型，因为我们不会知道说话者的未来词汇。[↩](#fnref43)
- en: ⁵ If we wanted to switch this out with an `LSTMCell`, we would need another
    list for the LSTM’s context states.[↩](#fnref44)
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵ 如果我们想用 `LSTMCell` 来替换它，我们需要为 LSTM 的上下文状态再准备另一个列表。[↩](#fnref44)
