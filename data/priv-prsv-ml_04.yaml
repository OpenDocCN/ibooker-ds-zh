- en: 3 Advanced concepts of differential privacy for machine learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 机器学习差分隐私的高级概念
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Design principles of differentially private machine learning algorithms
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 差分隐私机器学习算法的设计原则
- en: Designing and implementing differentially private supervised learning algorithms
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计和实现差分隐私监督学习算法
- en: Designing and implementing differentially private unsupervised learning algorithms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计和实现差分隐私无监督学习算法
- en: Walking through designing and analyzing a differentially private machine learning
    algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计和分析差分隐私机器学习算法的步骤
- en: In the previous chapter we investigated the definition and general use of differential
    privacy (DP) and the properties of differential privacy that work under different
    scenarios (the postprocessing property, group property, and composition properties).
    We also looked into common and widely adopted DP mechanisms that have served as
    essential building blocks in various privacy-preserving algorithms and applications.
    This chapter will walk through how you can use those building blocks to design
    and implement multiple differentially private ML algorithms and how you can apply
    such algorithms in real-world scenarios.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们研究了差分隐私（DP）的定义和一般用途，以及在不同场景下差分隐私的性质（后处理属性、分组属性和组合属性）。我们还探讨了常见且广泛采用的差分隐私机制，这些机制在各种隐私保护算法和应用中作为基本构建块。本章将介绍如何使用这些构建块来设计和实现多个差分隐私机器学习算法，以及如何在现实场景中应用这些算法。
- en: 3.1 Applying differential privacy in machine learning
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 在机器学习中应用差分隐私
- en: In chapter 2 we investigated different DP mechanisms and their properties. This
    chapter will showcase how you can use those DP mechanisms to design and implement
    various differentially private ML algorithms.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章中，我们研究了不同的差分隐私机制及其性质。本章将展示如何使用这些差分隐私机制来设计和实现各种差分隐私机器学习算法。
- en: As shown in figure 3.1, we are considering a two-party ML training scenario
    with a data owner and a data user. The data owner provides the private data, which
    is the input or the training data. Usually, the training data will go through
    a data preprocessing stage to clean the data and remove any noise. The data user
    will then perform a specific ML algorithm (regression, classification, clustering,
    etc.) on this personal data to train and produce an ML model, which is the output.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如图3.1所示，我们考虑的是一个两方机器学习训练场景，包括数据所有者和数据用户。数据所有者提供私有数据，即输入或训练数据。通常，训练数据将经过数据预处理阶段以清理数据并去除任何噪声。然后，数据用户将对这些个人数据执行特定的机器学习算法（回归、分类、聚类等），以训练并生成机器学习模型，即输出。
- en: '![CH03_F01_Zhuang](../../OEBPS/Images/CH03_F01_Zhuang.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F01_Zhuang](../../OEBPS/Images/CH03_F01_Zhuang.png)'
- en: Figure 3.1 The design principles of differentially private ML
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 差分隐私机器学习的设计原则
- en: As we all know, DP has received growing attention over the last decade. As a
    result, various differentially private ML algorithms have been proposed, designed,
    and implemented by both industrial and academic researchers. DP can be applied
    to prevent data users from inferencing the private data by analyzing the ML model.
    As depicted in figure 3.1, the perturbation can be applied at different steps
    in the ML process to provide DP guarantees. For instance, the input perturbation
    approach adds noise directly to the clean private data or during the data preprocessing
    stage. The algorithm perturbation approach adds noise during the training of ML
    models. The objective perturbation approach adds noise to the objective loss functions
    of the ML model. Finally, output perturbation adds noise directly to the trained
    ML model that is the output of the ML algorithms.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知，差分隐私（DP）在过去十年中受到了越来越多的关注。因此，工业和学术研究人员都提出了各种不同的差分隐私机器学习算法，并进行了设计、实现。差分隐私可以应用于防止数据用户通过分析机器学习模型来推断私有数据。如图3.1所示，扰动可以在机器学习过程的各个步骤中应用，以提供差分隐私保证。例如，输入扰动方法直接向干净私有数据或数据预处理阶段添加噪声。算法扰动方法在训练机器学习模型期间添加噪声。目标扰动方法向机器学习模型的目标损失函数添加噪声。最后，输出扰动直接向训练好的机器学习模型（即机器学习算法的输出）添加噪声。
- en: Figure 3.2 identifies a set of popular perturbation strategies applied in ML
    algorithms as examples presented in this chapter. Of course, there are other possible
    examples out there, but we will stick to these methods, and we will discuss these
    perturbation approaches in detail in the next section.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 识别了在本章中作为示例展示的机器学习算法中应用的几种流行扰动策略。当然，还有其他可能的例子，但我们将坚持这些方法，并在下一节详细讨论这些扰动方法。
- en: '![CH03_F02_Zhuang](../../OEBPS/Images/CH03_F02_Zhuang.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F02_Zhuang](../../OEBPS/Images/CH03_F02_Zhuang.png)'
- en: Figure 3.2 DP perturbation strategies and the sections they’re discussed in
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 DP扰动策略及其讨论的章节
- en: Let’s walk through these four common design principles for differentially private
    ML algorithms, starting with input perturbation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一探讨这些针对差分隐私机器学习算法的四个常见设计原则，从输入扰动开始。
- en: 3.1.1 Input perturbation
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 输入扰动
- en: In input perturbation, noise is added directly to the input data (the training
    data), as shown in figure 3.3\. After the desired non-private ML algorithm computation
    (the ML training procedure) is performed on the sanitized data, the output (the
    ML model) will be differentially private. For example, consider the principal
    component analysis (PCA) ML algorithm. Its input is the covariance matrix of the
    private data, and its output is a projection matrix (the output model of PCA).
    Before performing the Eigen-decomposition on the covariance matrix (the input),
    we can add a symmetric Gaussian noise matrix to the covariance matrix [1]. Now
    the output will be a differentially private projection matrix (remember, the goal
    here is not releasing the projected data but the DP-projection matrix).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入扰动中，噪声直接添加到输入数据（训练数据）中，如图3.3所示。在清洗数据上执行所需的非隐私机器学习算法计算（机器学习训练过程）后，输出（机器学习模型）将是差分隐私的。例如，考虑主成分分析（PCA）机器学习算法。它的输入是私有数据的协方差矩阵，它的输出是一个投影矩阵（PCA的输出模型）。在对方差矩阵（输入）进行特征分解之前，我们可以在协方差矩阵中添加一个对称的高斯噪声矩阵[1]。现在输出将是一个差分隐私投影矩阵（记住，这里的目的是不发布投影数据，而是DP投影矩阵）。
- en: '![CH03_F03_Zhuang](../../OEBPS/Images/CH03_F03_Zhuang.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F03_Zhuang](../../OEBPS/Images/CH03_F03_Zhuang.png)'
- en: Figure 3.3 How input perturbation works
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 输入扰动的原理
- en: Input perturbation is easy to implement and can be used to produce a sanitized
    dataset that can be applied to different kinds of ML algorithms. Since this approach
    focuses on perturbing the input data to ML models, the same procedure can be generalized
    to many different ML algorithms. For instance, the perturbed covariance matrix
    can also be the input for many different component analysis algorithms, such as
    PCA, linear discriminant analysis (LDA), and multiple discriminant analysis (MDA).
    In addition, most DP mechanisms can be utilized in input perturbation approaches,
    depending on the properties of the input data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 输入扰动易于实现，并且可以用来生成适用于不同类型机器学习（ML）算法的清洗数据集。由于这种方法侧重于扰动输入数据以应用于机器学习模型，因此相同的程序可以推广到许多不同的机器学习算法。例如，扰动协方差矩阵也可以作为许多不同的成分分析算法的输入，例如主成分分析（PCA）、线性判别分析（LDA）和多重判别分析（MDA）。此外，大多数差分隐私（DP）机制都可以在输入扰动方法中利用，具体取决于输入数据的属性。
- en: However, input perturbation usually requires adding more noise to the ML input
    data, since the raw input data usually has a higher sensitivity. As discussed
    in chapter 2, sensitivity in DP is the largest possible difference that one individual’s
    private information could make. Data with a higher sensitivity requires us to
    add more noise to provide the same level of privacy guarantee. In section 3.2.3,
    we’ll discuss differentially private linear regression to show you how input perturbation
    can be utilized when designing DP ML algorithms.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，输入扰动通常需要向机器学习输入数据添加更多的噪声，因为原始输入数据通常具有更高的敏感性。正如第2章所讨论的，差分隐私中的敏感性是一个个体私人信息可能造成的最大可能差异。具有更高敏感性的数据需要我们添加更多的噪声以提供相同级别的隐私保证。在第3.2.3节中，我们将讨论差分隐私线性回归，以展示在设计差分隐私机器学习算法时如何利用输入扰动。
- en: 3.1.2 Algorithm perturbation
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 算法扰动
- en: With algorithm perturbation, the private data is input to the ML algorithm (possibly
    after non-private data preprocessing procedures), and then DP mechanisms are employed
    to generate the corresponding sanitized models (see figure 3.4). For ML algorithms
    that need several iterations or multiple steps, DP mechanisms can be used to perturb
    the intermediate values (i.e., the model parameters) in each iteration or step.
    For instance, Eigen-decomposition for PCA can be performed using the power iteration
    method, which is an iterative algorithm. In the noisy power method, Gaussian noise
    can be added in each iteration of the algorithm, which operates on the non-perturbed
    covariance matrix (i.e., the input), leading to DP PCA [2]. Similarly, Abadi proposed
    a DP deep learning system that modifies the stochastic gradient descent algorithm
    to have Gaussian noise added in each of its iterations [3].
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在算法扰动中，私有数据被输入到机器学习算法中（可能经过非私有数据预处理程序），然后采用差分隐私机制生成相应的净化模型（见图3.4）。对于需要多次迭代或多个步骤的机器学习算法，差分隐私机制可以用来扰动每个迭代或步骤中的中间值（即模型参数）。例如，PCA的特征分解可以使用幂迭代方法执行，这是一个迭代算法。在带噪声的幂方法中，可以在算法的每个迭代中添加高斯噪声，该算法作用于未扰动的协方差矩阵（即输入），从而实现差分隐私PCA
    [2]。同样，Abadi提出了一种差分隐私深度学习系统，该系统修改了随机梯度下降算法，使其在每个迭代中添加高斯噪声 [3]。
- en: '![CH03_F04_Zhuang](../../OEBPS/Images/CH03_F04_Zhuang.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F04_Zhuang](../../OEBPS/Images/CH03_F04_Zhuang.png)'
- en: 'Figure 3.4 How algorithm perturbation works: Input is private data. Then we
    preprocess the data (non-private). Next, we perturb the intermediate values during
    training iterations inside the ML algorithm. Finally, we have a DP ML model.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 算法扰动的原理：输入是私有数据。然后我们预处理数据（非私有）。接下来，我们在机器学习算法的训练迭代过程中扰动中间值。最后，我们得到一个差分隐私机器学习模型。
- en: As you can see, algorithm perturbation approaches are usually applied to ML
    models that need several iterations or multiple steps, such as linear regression,
    logistic regression, and deep neural networks. Compared with input perturbation,
    algorithm perturbation requires a specific design for different ML algorithms.
    However, it usually introduces less noise while using the same DP privacy budget,
    since the intermediate values in the training ML models generally have less sensitivity
    than the raw input data. Less noise usually leads to better utility of the DP
    ML models. In section 3.3.1 we’ll introduce differentially private *k*-means clustering
    to discuss further how to utilize algorithm perturbation in the design of DP ML.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，算法扰动方法通常应用于需要多次迭代或多个步骤的机器学习模型，例如线性回归、逻辑回归和深度神经网络。与输入扰动相比，算法扰动需要对不同的机器学习算法进行特定设计。然而，由于训练机器学习模型中的中间值通常比原始输入数据具有更少的敏感性，因此它通常在相同的差分隐私预算下引入更少的噪声。更少的噪声通常会导致差分隐私机器学习模型具有更好的效用。在3.3.1节中，我们将介绍差分隐私*k*-均值聚类，进一步讨论如何在差分隐私机器学习的设计中利用算法扰动。
- en: 3.1.3 Output perturbation
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.3 输出扰动
- en: With output perturbation we use a non-private learning algorithm and then add
    noise to the generated model, as depicted in figure 3.5\. For instance, we could
    achieve DP PCA by sanitizing the projection matrix produced by the PCA algorithm
    by using an exponential mechanism (i.e., by sampling a random *k*-dimensional
    subspace that approximates the top *k* PCA subspace).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用输出扰动时，我们采用非私有学习算法，然后对生成的模型添加噪声，如图3.5所示。例如，我们可以通过使用指数机制（即采样一个随机*k*-维子空间，该子空间近似于前*k*个PCA子空间）来净化PCA算法生成的投影矩阵，从而实现差分隐私PCA。
- en: '![CH03_F05_Zhuang](../../OEBPS/Images/CH03_F05_Zhuang.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F05_Zhuang](../../OEBPS/Images/CH03_F05_Zhuang.png)'
- en: 'Figure 3.5 How output perturbation works: Input is private data. Then we preprocess
    the data (non-private). Next, we apply DP perturbation to the non-private ML algorithm.
    Finally, we have a DP ML model.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 输出扰动的原理：输入是私有数据。然后我们预处理数据（非私有）。接下来，我们对非私有机器学习算法应用差分隐私扰动。最后，我们得到一个差分隐私机器学习模型。
- en: In general, output perturbation approaches are usually applied to ML algorithms
    that produce complex statistics as their ML models. For example, feature extraction
    and dimensionality reduction algorithms typically publish the extracted features.
    Thus, using a projection matrix for dimensionality reduction is a suitable scenario
    for using output perturbation. However, many supervised ML algorithms that require
    releasing the model and interacting with the testing data multiple times, such
    as linear regression, logistic regression, and support vector machines, are not
    suitable for output perturbation. In section 3.2.1 we’ll walk through differentially
    private naive Bayes classification to discuss further how we can utilize output
    perturbation in DP ML.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，输出扰动方法适用于产生复杂统计数据的机器学习算法。例如，特征提取和降维算法通常发布提取的特征。因此，使用投影矩阵进行降维是使用输出扰动的合适场景。然而，许多需要发布模型并与测试数据多次交互的监督机器学习算法，如线性回归、逻辑回归和支持向量机，不适合输出扰动。在3.2.1节中，我们将通过差分隐私朴素贝叶斯分类来讨论如何利用输出扰动在差分隐私机器学习中。
- en: 3.1.4 Objective perturbation
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.4 目标扰动
- en: As shown in figure 3.6, objective perturbation entails adding noise to the objective
    function for learning algorithms, such as empirical risk minimization, that use
    the minimum/maximum of the noisy function as the output model. The core idea of
    empirical risk minimization is that we cannot know exactly how well an algorithm
    will work with actual data because we don’t know the true distribution of the
    data that the algorithm will work on. However, we can measure its performance
    on a known set of training data, and we call this measurement the *empirical risk*.
    As such, in objective perturbation, a vector mechanism can be designed to accommodate
    noise addition. To learn more about exactly how to do that, refer to section A.2.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如图3.6所示，目标扰动涉及向学习算法（如经验风险最小化）的目标函数添加噪声，这些算法使用噪声函数的最小/最大值作为输出模型。经验风险最小化的核心思想是，由于我们不知道算法在实际数据上的表现如何，因为我们不知道算法将要处理的数据的真实分布。然而，我们可以测量它在已知训练数据集上的性能，我们称这种测量为*经验风险*。因此，在目标扰动中，可以设计一个向量机制来适应噪声的添加。要了解更多关于如何具体操作的细节，请参阅附录A.2。
- en: '![CH03_F06_Zhuang](../../OEBPS/Images/CH03_F06_Zhuang.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F06_Zhuang](../../OEBPS/Images/CH03_F06_Zhuang.png)'
- en: 'Figure 3.6 How objective perturbation works: The input is private data. Then
    we preprocess the data (non-private). Next, we perturb the objective function
    of the ML model during the training. Finally, we have a DP ML model.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 目标扰动的工作原理：输入是私有数据。然后我们对数据进行预处理（非私有）。接下来，我们在训练过程中扰动机器学习模型的目标函数。最后，我们得到一个差分隐私机器学习模型。
- en: What is an objective function?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是目标函数？
- en: In mathematical optimization, an objective function attempts to maximize the
    cost (or minimize the losses) based on a set of constraints and the relationship
    between one or more decision variables. Typically, a loss function (or cost function)
    maps values of one or more variables to a real number (a numeric value) that then
    can be represented as the “cost” associated with the event. In practice, it could
    be the cost of a project, the profit margin, or even the quantity of a production
    line. With the objective function, we are trying to arrive at a target for output,
    profit, resource use, and the like.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学优化中，目标函数试图根据一组约束条件和一或多个决策变量之间的关系，最大化成本（或最小化损失）。通常，损失函数（或成本函数）将一个或多个变量的值映射到一个实数（一个数值），然后可以表示为与事件相关的“成本”。在实践中，它可能是项目的成本、利润率，甚至是生产线上的数量。有了目标函数，我们试图达到输出、利润、资源使用等目标。
- en: In mathematical terms, the objective function can be represented like this.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 用数学术语来说，目标函数可以表示如下。
- en: '![CH03_F06_zhuang-ch3-eqs-0x](../../OEBPS/Images/CH03_F06_zhuang-ch3-eqs-0x.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F06_zhuang-ch3-eqs-0x](../../OEBPS/Images/CH03_F06_zhuang-ch3-eqs-0x.png)'
- en: Consider the example of maximizing the profit of a product where we have n number
    of variables that could directly affect the profit. In this formula, X[i] is the
    *i*th variable among those variables, and c[i] is the coefficient of the *i*th
    variable. What we want to achieve here is to determine the best setting for these
    variables in attempting to arrive at maximum profit.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个最大化产品利润的例子，其中我们有一些变量可以直接影响利润。在这个公式中，X[i]是这些变量中的第*i*个变量，c[i]是第*i*个变量的系数。我们在这里想要实现的是，在试图达到最大利润的过程中，确定这些变量的最佳设置。
- en: As you’ll know, a *sample space* is the set of all possible outcomes of an event
    or experiment. A real-world sample space can sometimes include both bounded values
    (values that lie within a specific range) and unbounded values covering an infinite
    set of possible outcomes. However, most perturbation mechanisms assume a bounded
    sample space. When the sample space is unbounded, it leads to unbounded sensitivity,
    resulting in unbounded noise addition. Hence, if the sample space is unbounded,
    we can assume that the value of each sample will be truncated in the preprocessing
    stage, and the truncation rule is independent of the private data. For instance,
    we could use common knowledge or extra domain knowledge to decide on the truncation
    rule. In section 3.2.2 we’ll discuss differentially private logistic regression
    and objective perturbation in DP ML.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所知，*样本空间*是事件或实验所有可能结果的集合。现实世界的样本空间有时包括有界值（位于特定范围内的值）和无界值，覆盖无限多的可能结果。然而，大多数扰动机制假设样本空间是有界的。当样本空间是无界的，它会导致无界敏感性，从而导致无界噪声的增加。因此，如果样本空间是无界的，我们可以假设每个样本的值在预处理阶段将被截断，并且截断规则与私有数据无关。例如，我们可以使用常识或额外的领域知识来决定截断规则。在第3.2.2节中，我们将讨论差分隐私逻辑回归和DP
    ML中的目标扰动。
- en: 3.2 Differentially private supervised learning algorithms
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 差分隐私监督学习算法
- en: '*Supervised learning* employs labeled data where each feature vector is associated
    with an output value that might be a class label (classification) or a continuous
    value (regression). The labeled data is used to build models that can predict
    the labels of new feature vectors (during the testing phase). With classification,
    the samples usually belong to two or more classes, and the objective of the ML
    algorithm is to determine which class the new sample belongs to. Some algorithms
    might achieve that by finding a separating hyperplane between the different classes.
    An example application is face recognition, where a face image can be tested to
    ascertain that it belongs to a particular person.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*监督学习*使用标记数据，其中每个特征向量都与一个输出值相关联，该输出值可能是一个类别标签（分类）或连续值（回归）。标记数据用于构建模型，这些模型可以预测新特征向量的标签（在测试阶段）。在分类中，样本通常属于两个或多个类别，机器学习算法的目标是确定新样本属于哪个类别。一些算法可能通过在不同类别之间找到分离的超平面来实现这一点。一个示例应用是面部识别，其中可以测试面部图像以确定它属于特定的人。'
- en: Multiple classification algorithms can be used for each of the previously mentioned
    applications, such as support vector machines (SVMs), neural networks, or logistic
    regression. When a sample label is a continuous value (also referred to as a dependent
    or response variable) rather than a discrete one, the task is called regression.
    The samples are formed of features that are also called independent variables.
    The target of regression is fitting a predictive model (such as a line) to an
    observed dataset such that the distances between the observed data points and
    the line are minimized. An example of this would be estimating the price of a
    house based on its location, zip code, and number of rooms.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 多种分类算法可以用于之前提到的每个应用，例如支持向量机（SVMs）、神经网络或逻辑回归。当样本标签是一个连续值（也称为因变量或响应变量）而不是离散值时，该任务被称为回归。样本由也称为自变量的特征组成。回归的目标是将预测模型（如直线）拟合到观察数据集，使得观察数据点与直线之间的距离最小化。例如，可以根据房屋的位置、邮编和房间数量来估计房屋价格。
- en: 'In the following subsections, we’ll formulate the DP design of the three most
    common supervised learning algorithms: naive Bayes classification, logistic regression,
    and linear regression.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下小节中，我们将制定三种最常见监督学习算法的DP设计：朴素贝叶斯分类、逻辑回归和线性回归。
- en: 3.2.1 Differentially private naive Bayes classification
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 差分隐私朴素贝叶斯分类
- en: First, let’s look into how differentially private naive Bayes classification
    works, along with some mathematical explanations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解一下差分隐私朴素贝叶斯分类是如何工作的，以及一些数学解释。
- en: Naive Bayes classification
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类
- en: 'In probability theory, Bayes’ theorem describes the probability of an event
    based on prior knowledge of conditions that might be related to the event. It
    is stated as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率论中，贝叶斯定理描述了基于先前对可能相关事件的条件的知识来描述事件的概率。它表述如下：
- en: '![CH03_F06_zhuang-ch3-eqs-1x](../../OEBPS/Images/CH03_F06_zhuang-ch3-eqs-1x.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F06_zhuang-ch3-eqs-1x](../../OEBPS/Images/CH03_F06_zhuang-ch3-eqs-1x.png)'
- en: A and B are the events.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A和B是事件。
- en: P(A|B) is the probability of A, given B is true.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(A|B)是在B为真的条件下A的概率。
- en: P(B|A) is the probability of B, given A is true.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(B|A)是在A为真的条件下B的概率。
- en: P(A) and P(B) are the independent probabilities of A and B.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(A)和P(B)是A和B的独立概率。
- en: The naive Bayes classification technique uses Bayes’ theorem and the assumption
    of independence between every pair of features.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类技术使用贝叶斯定理以及每对特征之间相互独立的假设。
- en: First, let the instance to be classified be the n -dimensional vector X = [x[1],
    x[2],...,x[n]], the names of the features be [F[1], F[2],...,F[n]], and the possible
    classes that can be assigned to the instance be C = [c[1], c[2],...,c[n]].
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让待分类的实例为n维向量X = [x[1], x[2],...,x[n]]，特征的名称为[F[1], F[2],...,F[n]]，可以分配给实例的可能类别为C
    = [c[1], c[2],...,c[n]]。
- en: The naive Bayes classifier assigns the instance *X* to the class F[S] if and
    only if P(C[s]|X) P(C[j]|X) for 1 ≤ j ≤ k and j ≠ s. Hence, the classifier needs
    to compute P(C[j]│X) for all classes and compare these probabilities.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器将实例*X*分配到类别F[S]，当且仅当P(C[s]|X) P(C[j]|X)对于1 ≤ j ≤ k且j ≠ s。因此，分类器需要计算所有类别的P(C[j]│X)并比较这些概率。
- en: We know that when using Bayes’ theorem, the probability P(C[j]│X) can be calculated
    as
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道在使用贝叶斯定理时，概率P(C[j]│X)可以计算为
- en: '![CH03_F06_zhuang-ch3-eqs-2x](../../OEBPS/Images/CH03_F06_zhuang-ch3-eqs-2x.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F06_zhuang-ch3-eqs-2x](../../OEBPS/Images/CH03_F06_zhuang-ch3-eqs-2x.png)'
- en: Since P(X) is the same for all classes, it is sufficient to find the class with
    the maximum P(X|C[j]) ∙ P(C[j]). Assuming the independence of features, that class
    is equal to P(C[j]) ⋅ Π^n[i=1] P(F[i] = x[i]|C[j]). Hence, the probability of
    assigning C[j] to the given instance X is proportional to P(C[1]) ⋅ Π³[i=1] P(F[i]=
    x[i]|C[j]).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于P(X)对所有类别都是相同的，因此找到具有最大P(X|C[j]) ∙ P(C[j])的类别就足够了。假设特征是独立的，那么这个类别等于P(C[j])
    ⋅ Π^n[i=1] P(F[i] = x[i]|C[j])。因此，将C[j]分配给给定实例X的概率与P(C[1]) ⋅ Π³[i=1] P(F[i]= x[i]|C[j])成正比。
- en: That’s the mathematical background of naive Bayes classification for now. Next,
    we’ll demonstrate how to apply naive Bayes for discrete and continuous data with
    examples.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是目前朴素贝叶斯分类的数学背景。接下来，我们将通过示例演示如何应用朴素贝叶斯对离散和连续数据进行分类。
- en: Discrete naive Bayes
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 离散朴素贝叶斯
- en: To demonstrate the concept of the naive Bayes classifier for discrete (categorical)
    data, let’s use the dataset in table 3.1.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示朴素贝叶斯分类器在离散（分类）数据上的概念，让我们使用表3.1中的数据集。
- en: Table 3.1 This is an extract from a dataset of mortgage payments. Age, income,
    and gender are the independent variables, whereas missed payment represents the
    dependent variable for the prediction task.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1 这是从一个抵押贷款支付数据集中提取的部分。年龄、收入和性别是独立变量，而未支付款项代表预测任务的因变量。
- en: '| Number | Age | Income | Gender | Missed payment (yes or no) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 数量 | 年龄 | 收入 | 性别 | 未支付款项（是或否） |'
- en: '| 1 | Young | Low | Male | Yes |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 年轻 | 低 | 男 | 是 |'
- en: '| 2 | Young | High | Female | Yes |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 年轻 | 高 | 女 | 是 |'
- en: '| 3 | Medium | High | Male | No |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 中等 | 高 | 男 | 否 |'
- en: '| 4 | Old | Medium | Male | No |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 老年 | 中等 | 男 | 否 |'
- en: '| 5 | Old | High | Male | No |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 老年 | 高 | 男 | 否 |'
- en: '| 6 | Old | Low | Female | Yes |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 老年 | 低 | 女 | 是 |'
- en: '| 7 | Medium | Low | Female | No |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 中等 | 低 | 女 | 否 |'
- en: '| 8 | Medium | Medium | Male | Yes |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 中等 | 中等 | 男 | 是 |'
- en: '| 9 | Young | Low | Male | No |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 年轻 | 低 | 男 | 否 |'
- en: '| 10 | Old | High | Female | No |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 老年 | 高 | 女 | 否 |'
- en: In this example, the classification task is predicting whether a customer will
    miss a mortgage payment or not. Hence, there are two classes, C[1] and C[2], representing
    missing a payment or not, respectively. P(C[1]) = 4⁄10 and P(C[2]) = 6⁄10. In
    addition, conditional probabilities for the age feature are shown in figure 3.7\.
    We can similarly calculate conditional probabilities for the other features.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，分类任务是预测客户是否会错过抵押贷款支付。因此，有两个类别，C[1]和C[2]，分别代表是否错过支付。P(C[1]) = 4⁄10和P(C[2])
    = 6⁄10。此外，年龄特征的条件下概率如图3.7所示。我们可以类似地计算其他特征的条件下概率。
- en: '![CH03_F07_Zhuang](../../OEBPS/Images/CH03_F07_Zhuang.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F07_Zhuang](../../OEBPS/Images/CH03_F07_Zhuang.png)'
- en: Figure 3.7 Conditional probabilities for F[1] (i.e., age) in the example dataset
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 示例数据集中F[1]（即年龄）的条件概率
- en: To predict whether a young female with medium income will miss a payment or
    not, we can set X = (Age = Young, Income = Medium, Gender = Female). Then, using
    the calculated results from the raw data in table 3.1, we will have P(Age = Young|C[1])
    = 2/4, P(Income = Medium|C[1]) = 1/4, P(Gender = Female|C[1]) = 2/4, P(Age = Young|C[2])
    = 1/6, P(Income = Medium|C[2]) = 1/6, P(Gender = Female|C[2]) = 2/6, P(C[1]) =
    4/10, and P(C[2]) = 6/10.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预测一个中等收入的年轻女性是否会错过付款，我们可以设置 X = (Age = Young, Income = Medium, Gender = Female)。然后，使用表
    3.1 中的原始数据计算结果，我们将得到 P(Age = Young|C[1]) = 2/4, P(Income = Medium|C[1]) = 1/4,
    P(Gender = Female|C[1]) = 2/4, P(Age = Young|C[2]) = 1/6, P(Income = Medium|C[2])
    = 1/6, P(Gender = Female|C[2]) = 2/6, P(C[1]) = 4/10, 和 P(C[2]) = 6/10。
- en: To use the naive Bayes classifier, we need to compare P(C[1]) ⋅ Π³[i=1] P(F[i]
    = x[i]|C[1]) and P(C[2]) ⋅ Π³[i=1] P(F[i] = x[i]|C[2]). Since the first is equal
    to 0.025 (i.e., 4/10 × 2/4 × 1/4 × 2/4 = 0.025) and the second is equal to 0.0056
    (i.e., 6/10 × 1/6 × 1/6 × 2/6 = 0.0056), it can be determined that is assigned
    for the instance X by the naive Bayes classifier. In other words, it can be predicted
    that a young female with medium income will miss her payments.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用朴素贝叶斯分类器，我们需要比较 P(C[1]) ⋅ Π³[i=1] P(F[i] = x[i]|C[1]) 和 P(C[2]) ⋅ Π³[i=1]
    P(F[i] = x[i]|C[2])。由于前者等于 0.025（即，4/10 × 2/4 × 1/4 × 2/4 = 0.025）而后者等于 0.0056（即，6/10
    × 1/6 × 1/6 × 2/6 = 0.0056），可以确定朴素贝叶斯分类器将实例 X 分配给 C[1]。换句话说，可以预测一个中等收入的年轻女性会错过她的付款。
- en: Gaussian naive Bayes
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯朴素贝叶斯
- en: When it comes to continuous data (with infinite possible values between any
    two numerical data points), a common approach is to assume that the values are
    distributed according to Gaussian distribution. Then we can compute the conditional
    probabilities using the *mean* and the *variance* of the values.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到连续数据（任何两个数值数据点之间有无限可能的值）时，一种常见的方法是假设这些值是按照高斯分布分布的。然后我们可以使用这些值的**均值**和**方差**来计算条件概率。
- en: 'Let’s assume a feature F[i] has a continuous domain. For each class C[j] ∈
    C, the mean μ[i,j] and the variance σ[i,j]² of the values of F[i] in the training
    set are computed. Then, for the given instance X, the conditional probability
    P(F[i] = x[i]│C[j]) is computed using Gaussian distribution as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个特征 F[i] 具有连续域。对于每个类别 C[j] ∈ C，计算训练集中 F[i] 的值的均值 μ[i,j] 和方差 σ[i,j]²。然后，对于给定的实例
    X，使用高斯分布计算条件概率 P(F[i] = x[i]│C[j])，如下所示：
- en: '![CH03_F07_zhuang-ch3-eqs-8x](../../OEBPS/Images/CH03_F07_zhuang-ch3-eqs-8x.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F07_zhuang-ch3-eqs-8x](../../OEBPS/Images/CH03_F07_zhuang-ch3-eqs-8x.png)'
- en: You might have noticed that with discrete naive Bayes, the accuracy may reduce
    in large discrete domains because of the high number of values not seen in the
    training set. However, Gaussian naive Bayes can be used for features with large
    discrete domains as well.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在离散朴素贝叶斯中，由于训练集中未看到的值的数量很高，在大离散域中准确性可能会降低。然而，高斯朴素贝叶斯也可以用于具有大离散域的特征。
- en: Implementing differentially private naive Bayes classification
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 实现差分隐私朴素贝叶斯分类
- en: Now, let’s look into how we can make naive Bayes classification differentially
    private. This design follows an output perturbation strategy [4], where the sensitivity
    of the naive Bayes model parameters is derived and the Laplace mechanism (i.e.,
    adding Laplacian noise) is then directly applied to the model parameters (as described
    in section 2.2.2).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们如何使朴素贝叶斯分类具有差分隐私。这种设计遵循输出扰动策略 [4]，其中推导出朴素贝叶斯模型参数的敏感性，然后直接将拉普拉斯机制（即，添加拉普拉斯噪声）应用于模型参数（如第
    2.2.2 节所述）。
- en: First, we need to formulate the sensitivity of the model parameters. Discrete
    and Gaussian naive Bayes model parameters have different sensitivities. In discrete
    naive Bayes, the model parameters are the probabilities
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要确定模型参数的敏感性。离散和高斯朴素贝叶斯模型参数有不同的敏感性。在离散朴素贝叶斯中，模型参数是概率
- en: '![CH03_F07_zhuang-ch3-eqs-9x](../../OEBPS/Images/CH03_F07_zhuang-ch3-eqs-9x.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F07_zhuang-ch3-eqs-9x](../../OEBPS/Images/CH03_F07_zhuang-ch3-eqs-9x.png)'
- en: where n is the total number of training samples where C = C[j], and n[i,j] is
    the number of such training samples that also have F[i] = x[i].
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 n 是总训练样本数，其中 C = C[j]，而 n[i,j] 是具有 F[i] = x[i] 的此类训练样本的数量。
- en: Thus, the DP noise could be added to the number of training samples (i.e., n[i,j]).
    We can see that whether we add or delete a new record, the difference of n[i,j]
    is always 1\. Therefore, for discrete naive Bayes, the sensitivity of each model
    parameter n[i,j] is 1 (for all feature values F[i] = x[i] and class values C[j]).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，DP 噪声可以添加到训练样本数量（即 n[i,j]）。我们可以看到，无论我们添加还是删除一条新记录，n[i,j] 的差异总是 1。因此，对于离散朴素贝叶斯，每个模型参数
    n[i,j] 的敏感性为 1（对于所有特征值 F[i] = x[i] 和类别值 C[j]）。
- en: For Gaussian naive Bayes, the model parameter P(F[i] = x[i]│C[j]) depends on
    the mean μ[i,j] and the variance σ[i,j]², so we need to figure out the sensitivity
    of these means and variances. Suppose the values of feature F[i] are bounded by
    the range [l[i], u[i]]. Then, as suggested by Vaidya et al. [4], the sensitivity
    of the mean μ[i,j] is (μ[i] − l[i] ) /(n + 1), and the sensitivity of the variance
    σ[i,j]² is n ∙ (μ[i] − l[i]) / (n + 1), where n is the number of training samples
    where C = C[j].
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高斯朴素贝叶斯，模型参数 P(F[i] = x[i]│C[j]) 依赖于均值 μ[i,j] 和方差 σ[i,j]²，因此我们需要找出这些均值的敏感性。假设特征
    F[i] 的值被范围 [l[i], u[i]] 所限制。那么，正如 Vaidya 等人 [4] 所建议的，均值 μ[i,j] 的敏感性是 (μ[i] − l[i]
    ) /(n + 1)，方差 σ[i,j]² 的敏感性是 n ∙ (μ[i] − l[i]) / (n + 1)，其中 n 是 C = C[j] 的训练样本数量。
- en: To design our differentially private naive Bayes classification algorithm, we’ll
    use the output perturbation strategy where the sensitivity of each feature is
    calculated according to whether it is discrete or numerical. Laplacian noise of
    the appropriate scale (as described in our discussion of the Laplace mechanism
    in section 2.2.2) is added to the parameters (the number of samples for discrete
    features and the means and variances for numerical features). Figure 3.8 illustrates
    the pseudocode of our algorithm, which is mostly self-explanatory.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设计我们的差分隐私朴素贝叶斯分类算法，我们将使用输出扰动策略，其中每个特征的敏感性根据其是否为离散或数值来计算。根据第 2.2.2 节中关于拉普拉斯机制的讨论，添加适当尺度的拉普拉斯噪声（参数为离散特征的样本数量和数值特征的均值与方差）。图
    3.8 展示了我们的算法伪代码，大部分内容都是自我解释的。
- en: '![CH03_F08_Zhuang](../../OEBPS/Images/CH03_F08_Zhuang.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F08_Zhuang](../../OEBPS/Images/CH03_F08_Zhuang.png)'
- en: Figure 3.8 How differentially private naive Bayes classification works
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 差分隐私朴素贝叶斯分类的工作原理
- en: Now we’ll implement some of these concepts to get a little hands-on experience.
    Let’s consider a scenario where we are training a naive Bayes classification model
    to predict whether a person makes over $50,000 a year based on the census dataset.
    You can find more details about the dataset at [https://archive.ics.uci.edu/ml/datasets/adult](https://archive.ics.uci.edu/ml/datasets/adult).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将实现一些这些概念，以获得一些实际操作经验。让我们考虑一个场景，其中我们正在训练一个朴素贝叶斯分类模型，根据人口普查数据集预测一个人是否年收入超过
    50,000 美元。您可以在 [https://archive.ics.uci.edu/ml/datasets/adult](https://archive.ics.uci.edu/ml/datasets/adult)
    找到更多关于数据集的详细信息。
- en: First, we need to load the training and testing data from the adult dataset.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要从 adult 数据集中加载训练和测试数据。
- en: Listing 3.1 Loading the dataset
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.1 加载数据集
- en: '[PRE0]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we’ll train a regular (non-private) naive Bayes classifier and test its
    accuracy, as shown in the following listing.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将训练一个常规（非隐私）的朴素贝叶斯分类器，并测试其准确性，如下所示。
- en: Listing 3.2 Naive Bayes with no privacy
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.2 无隐私的朴素贝叶斯
- en: '[PRE1]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output will look like the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '[PRE2]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To apply differentially private naive Bayes, we’ll use the IBM Differential
    Privacy Library, diffprivlib:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用差分隐私朴素贝叶斯，我们将使用 IBM 差分隐私库 diffprivlib：
- en: '[PRE3]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Using the models.GaussianNB module of diffprivlib, we can train a naive Bayes
    classifier while satisfying DP. If we don’t specify any parameters, the model
    defaults to epsilon = 1.00.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 diffprivlib 的 models.GaussianNB 模块，我们可以在满足 DP 的同时训练朴素贝叶斯分类器。如果我们不指定任何参数，模型默认为
    epsilon = 1.00。
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You will get something like the following as the output:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see from the preceding output accuracies, the regular (non-private)
    naive Bayes classifier produces an accuracy of 79.64%; by setting epsilon=1.00,
    the differentially private naive Bayes classifier achieves an accuracy of 78.59%.
    It is noteworthy that the training process of the (non-private) naive Bayes classifier
    and differentially private naive Bayes classifier is nondeterministic. Hence,
    you may obtain slightly different numbers than the accuracies we listed. Nevertheless,
    the result of DP-naive Bayes is slightly lower than its non-private version.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从前面的输出准确率中可以看到，常规（非隐私）朴素贝叶斯分类器产生了79.64%的准确率；通过设置epsilon=1.00，差分隐私朴素贝叶斯分类器达到了78.59%的准确率。值得注意的是，（非隐私）朴素贝叶斯分类器和差分隐私朴素贝叶斯分类器的训练过程是非确定性的。因此，您可能获得的数字与我们所列的准确率略有不同。尽管如此，DP-朴素贝叶斯的结果略低于其非隐私版本。
- en: 'Using a smaller epsilon usually leads to better privacy protection with less
    accuracy. For instance, let’s set epsilon=0.01:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用较小的epsilon通常会导致更好的隐私保护，但准确性较低。例如，让我们设置epsilon=0.01：
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now the output would look like this:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的输出将看起来像这样：
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 3.2.2 Differentially private logistic regression
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 差分隐私逻辑回归
- en: The previous subsection looked into the naive Bayes approach to differentially
    private supervised learning algorithms. Now let us look at how logistic regression
    can be applied under the deferential privacy setup.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 上一小节探讨了差分隐私监督学习算法的朴素贝叶斯方法。现在让我们看看在差分隐私设置下如何应用逻辑回归。
- en: Logistic regression
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Logistic regression (LR) is a model for binary classification. LR is usually
    formulated as training the parameters w by minimizing the negative log-likelihood
    over the training set (X, Y),
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归（LR）是二元分类的模型。LR通常被表述为通过最小化训练集（X, Y）上的负对数似然来训练参数w。
- en: '![CH03_F08_zhuang-ch3-eqs-12x](../../OEBPS/Images/CH03_F08_zhuang-ch3-eqs-12x.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F08_zhuang-ch3-eqs-12x](../../OEBPS/Images/CH03_F08_zhuang-ch3-eqs-12x.png)'
- en: where X = [x[1], x[2], ...,x[n]] and Y = [y[1], y[2], ...,y[n]].
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 其中X = [x[1], x[2], ...,x[n]] 和 Y = [y[1], y[2], ...,y[n]]。
- en: Compared with standard LR, regularized LR has a regularization term in its loss
    function. It is thus formulated as training the parameters w by minimizing
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准LR相比，正则化LR在其损失函数中有一个正则化项。因此，它被表述为通过最小化来训练参数w。
- en: '![CH03_F08_zhuang-ch3-eqs-13x](../../OEBPS/Images/CH03_F08_zhuang-ch3-eqs-13x.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F08_zhuang-ch3-eqs-13x](../../OEBPS/Images/CH03_F08_zhuang-ch3-eqs-13x.png)'
- en: over the training set (X, Y), where X = [x[1], x[2],...,x[n]], Y = [y[1], y[2],...,y[n]],
    and λ is a hyperparameter that sets the strength of regularization.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练集（X, Y）上，其中X = [x[1], x[2],...,x[n]], Y = [y[1], y[2],...,y[n]], λ是一个超参数，用于设置正则化的强度。
- en: Why do we need regularization in logistic regression?
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么逻辑回归中需要正则化？
- en: Overfitting is a common problem in ML tasks. Often, we’ll train a model on one
    set of data, and it appears to work well on that data, but when we test it on
    a new set of unseen data, the performance deteriorates. One of the reasons for
    this problem is overfitting, where the model too closely conforms to the training
    set and hence misses the more generalizable trends.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 过度拟合是机器学习任务中常见的问题。通常，我们会在一组数据上训练一个模型，它似乎在那组数据上表现良好，但当我们用一组未见过的数据对其进行测试时，性能会下降。导致这个问题的原因之一是过度拟合，即模型过于紧密地符合训练集，从而错过了更通用的趋势。
- en: Regularizations are known as shrinkage methods because they “shrink” the coefficients
    in the resulting regression. This shrinkage of coefficients reduces the variance
    in the model, which helps to avoid overfitting. In simpler terms, with regularization,
    the prediction of the model changes less than it would have without regularization
    when the input variables are changed.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化被称为收缩方法，因为它们“收缩”了结果回归中的系数。这种系数的收缩减少了模型中的方差，有助于避免过度拟合。用更简单的话说，有了正则化，当输入变量改变时，模型的预测变化小于没有正则化时的变化。
- en: Implementing differentially private logistic regression
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 实现差分隐私逻辑回归
- en: We can adopt the objective perturbation strategy in designing differentially
    private logistic regression, where noise is added to the objective function for
    learning algorithms. We will use the empirical risk minimization-based vector
    mechanism to decide the minimum and maximum of the noisy function to produce the
    DP noisy output model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在设计差分隐私逻辑回归时采用目标扰动策略，其中向学习算法的目标函数中添加噪声。我们将使用基于经验风险最小化的向量机制来决定噪声函数的最小值和最大值，以产生DP噪声输出模型。
- en: Theorem 3.1 proposed by Chaudhuri et al. formulates the sensitivity for the
    regularized logistic regression [5]. The training data input {(x[i], y[i]) ∈ X
    × Y:i = 1,2,...,n} consists of n data-label pairs. In addition, we will use the
    notation ‖A‖[2] to denote L[2] norm of A. We are training the parameters w, and
    λ is a hyperparameter that sets the strength of regularization.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Chaudhuri 等人提出的定理 3.1 为正则化逻辑回归公理化了敏感性 [5]。训练数据输入 {(x[i], y[i]) ∈ X × Y:i = 1,2,...,n}
    由 n 个数据标签对组成。此外，我们将使用符号 ‖A‖[2] 来表示 A 的 L[2] 范数。我们正在训练参数 w，λ 是一个超参数，用于设置正则化的强度。
- en: '![03_theorem3.1](../../OEBPS/Images/03_theorem3.1.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![03_theorem3.1](../../OEBPS/Images/03_theorem3.1.png)'
- en: Theorem 3.1
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 3.1
- en: You can refer to the original article for the mathematical proof and the details
    of getting there. For now, what we are looking at here is the calculation of sensitivity,
    which is the difference of ‖f(X) − f(X')‖[2], and it is less than or equal to
    2/λ ∙ n.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考原文以获取数学证明和细节。现在我们关注的是敏感度的计算，即 ‖f(X) − f(X')‖[2] 的差，它小于或等于 2/λ ∙ n。
- en: Now we’ll use the objective perturbation strategy to design our differentially
    private logistic regression algorithm, where the sensitivity is calculated according
    to theorem 3.1\. Figure 3.9 illustrates the pseudocode. For more details regarding
    the empirical risk minimization-based vector mechanism, refer to the original
    paper [5].
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用目标扰动策略来设计我们的差分隐私逻辑回归算法，其中敏感性是根据定理 3.1 计算的。图 3.9 展示了伪代码。有关基于经验风险最小化的向量机制的更多细节，请参阅原文
    [5]。
- en: '![CH03_F09_Zhuang](../../OEBPS/Images/CH03_F09_Zhuang.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F09_Zhuang](../../OEBPS/Images/CH03_F09_Zhuang.png)'
- en: Figure 3.9 How differentially private logistic regression works
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 差分隐私逻辑回归的工作原理
- en: Based on the adult dataset we used earlier, let’s continue with our previous
    scenario by training a logistic regression classification model to predict whether
    a person makes over $50,000 a year. First, let’s load the training and testing
    data from the adult dataset.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们之前使用的 adult 数据集，让我们继续之前的场景，通过训练逻辑回归分类模型来预测一个人是否每年收入超过 50,000 美元。首先，让我们从
    adult 数据集中加载训练和测试数据。
- en: Listing 3.3 Loading testing and training data
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.3 加载测试和训练数据
- en: '[PRE8]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For diffprivlib, LogisticRegression works best when the features are scaled
    to control the norm of the data. To streamline this process, we’ll create a Pipeline
    in sklearn:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 diffprivlib，当特征缩放到控制数据范数时，逻辑回归表现最佳。为了简化此过程，我们将在 sklearn 中创建一个 Pipeline：
- en: '[PRE9]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now let’s first train a regular (non-private) logistic regression classifier
    and test its accuracy:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们首先训练一个常规（非隐私）逻辑回归分类器并测试其准确性：
- en: '[PRE10]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You will have output like the following:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 您将得到如下输出：
- en: '[PRE11]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To apply differentially private logistic regression, we’ll start by installing
    the IBM Differential Privacy Library:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用差分隐私逻辑回归，我们首先安装 IBM 差分隐私库：
- en: '[PRE12]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Using the diffprivlib.models.LogisticRegression module of diffprivlib, we can
    train a logistic regression classifier while satisfying DP.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 diffprivlib.models.LogisticRegression 模块，我们可以在满足 DP 的同时训练逻辑回归分类器。
- en: If we don’t specify any parameters, the model defaults to epsilon = 1 and data_norm
    = None. If the norm of the data is not specified at initialization (as in this
    case), the norm will be calculated on the data when .fit() is first called, and
    a warning will be thrown because it causes a privacy leak. To ensure no additional
    privacy leakage, we should specify the data norm explicitly as an argument and
    choose the bounds independent of the data. For instance, we can use domain knowledge
    to do that.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有指定任何参数，模型默认为 epsilon = 1 和 data_norm = None。如果在初始化时没有指定数据的范数（如本例所示），则在第一次调用
    .fit() 时会在数据上计算范数，并会抛出一个警告，因为这会导致隐私泄露。为了确保没有额外的隐私泄露，我们应该显式地指定数据范数作为参数，并选择与数据无关的界限。例如，我们可以使用领域知识来做到这一点。
- en: Listing 3.4 Training a logistic regression classifier
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.4 训练逻辑回归分类器
- en: '[PRE13]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output will look something like the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '[PRE14]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As you can see from the preceding output accuracies, the regular (non-private)
    logistic regression classifier produces an accuracy of 81.04%; by setting epsilon=1.00,
    the differentially private logistic regression could achieve an accuracy of 80.93%.
    Using a smaller epsilon usually leads to better privacy protection but lesser
    accuracy. For instance, suppose we set epsilon=0.01:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述的输出准确率所示，常规（非隐私）逻辑回归分类器产生了81.04%的准确率；通过设置 epsilon=1.00，差分隐私逻辑回归可以达到80.93%的准确率。使用较小的
    epsilon 通常会导致更好的隐私保护，但准确性会降低。例如，如果我们设置 epsilon=0.01：
- en: '[PRE15]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As expected, the result will look something like this:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，结果将看起来像这样：
- en: '[PRE16]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 3.2.3 Differentially private linear regression
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 差分隐私线性回归
- en: Unlike logistic regression, a linear regression model defines a linear relationship
    between an observed target variable and multiple explanatory variables in a dataset.
    It is often used in regression analysis for trend prediction. The most common
    way to compute such a model is to minimize the residual sum of squares between
    the observed targets (of explanatory variables) in the dataset and the targets
    (of explanatory variables) predicted by linear approximation.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 与逻辑回归不同，线性回归模型在数据集中定义了观察到的目标变量与多个解释变量之间的线性关系。它通常用于回归分析中的趋势预测。计算此类模型的最常见方法是使数据集中观察到的目标（解释变量）与线性近似预测的目标（解释变量）之间的残差平方和最小化。
- en: Let’s dig into the theoretical foundations. We can formulate the standard problem
    of liner regression as finding β = arg min[β]‖Xβ − y‖², where X is the matrix
    of explanatory variables, y is the vector of the explained variable, and β is
    the vector of the unknown coefficients to be estimated.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入研究理论基础。我们可以将线性回归的标准问题表述为寻找 β = arg min[β]‖Xβ − y‖²，其中 X 是解释变量矩阵，y 是解释变量向量，β
    是待估计的未知系数向量。
- en: 'Ridge regression, a regularized version of linear regression, can be formulated
    as β^R= arg min[β]‖Xβ − y‖² + w²‖β‖², which has a closed form solution: β^R =
    (X^TX + w²I[p×p])X^Ty, where w is set to minimize the risk of β^R.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归是线性回归的正则化版本，可以表示为 β^R= arg min[β]‖Xβ − y‖² + w²‖β‖²，它有一个封闭形式的解：β^R = (X^TX
    + w²I[p×p])X^Ty，其中 w 被设置为最小化 β^R 的风险。
- en: The problem of designing a differentially private linear regression becomes
    designing a differentially private approximation of the second moment matrix.
    To achieve this, Sheffet [6] proposed an algorithm that adds noise to the second
    moment matrix using the Wishart mechanism. For more details, you can refer to
    the original paper, but this is enough for us to proceed.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 设计差分隐私线性回归的问题变成了设计第二矩矩阵的差分隐私近似。为了实现这一点，Sheffet [6] 提出了一种算法，使用 Wishart 机制向第二矩矩阵添加噪声。有关更多详细信息，您可以参考原始论文，但这对我们继续前进已经足够了。
- en: 'Let’s consider the scenario of training a linear regression on a diabetes dataset.
    This is another popular dataset among ML researchers, and you can find more information
    about it here: [https://archive.ics.uci.edu/ml/datasets/diabetes](https://archive.ics.uci.edu/ml/datasets/diabetes).
    We will work with the example proposed by scikit-learn ([https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.xhtml](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.xhtml)),
    and we’ll use the diabetes dataset to train and test a linear regressor.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑在糖尿病数据集上训练线性回归的场景。这是机器学习研究人员中另一个流行的数据集，您可以在以下链接中找到更多关于它的信息：[https://archive.ics.uci.edu/ml/datasets/diabetes](https://archive.ics.uci.edu/ml/datasets/diabetes)。我们将使用
    scikit-learn 提出的示例（[https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.xhtml](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.xhtml)），并使用糖尿病数据集来训练和测试线性回归器。
- en: 'We’ll begin by loading the dataset and splitting it into training and testing
    samples (an 80/20 split):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载数据集，并将其分为训练样本和测试样本（80/20的分割）：
- en: '[PRE17]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You will get a result something like the following, showing the number of samples
    in the train and test sets:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到类似以下的结果，显示训练集和测试集中的样本数量：
- en: '[PRE18]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We’ll now use scikit-learn’s native LinearRegression function to establish a
    non-private baseline for our experiments. We’ll use the r-squared score to evaluate
    the goodness-of-fit of the model. The r-squared score is a statistical measure
    that represents the proportion of the variance of a dependent variable that’s
    explained by an independent variable or variables in a regression model. A higher
    r-squared score indicates a better linear regression model.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用 scikit-learn 的原生 LinearRegression 函数为我们的实验建立一个非隐私基线。我们将使用 r 平方分数来评估模型的拟合优度。r
    平方分数是一个统计量，表示回归模型中由独立变量或变量解释的因变量的方差比例。r 平方分数越高，线性回归模型越好。
- en: '[PRE19]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The result will look like the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将类似于以下内容：
- en: '[PRE20]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To apply differentially private linear regression, let’s start by installing
    the IBM Differential Privacy Library, if you have not done so already:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用差分隐私线性回归，让我们首先安装 IBM Differential Privacy Library，如果您还没有这样做的话：
- en: '[PRE21]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now let’s train a differentially private linear regressor (epsilon=1.00), where
    the trained model is differentially private with respect to the training data:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们训练一个差分隐私线性回归器（epsilon=1.00），其中训练模型对训练数据是差分隐私的：
- en: '[PRE22]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You will get an R2 score similar to the following:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到一个类似于以下 R2 分数：
- en: '[PRE23]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 3.3 Differentially private unsupervised learning algorithms
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 差分隐私无监督学习算法
- en: Unsupervised learning is a type of algorithm that learns patterns from unlabeled
    data. The feature vectors do not come with class labels or response variables
    in this type of learning. The target, in this case, is to find the structure of
    the data.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是一种从未标记数据中学习模式的算法。在这种学习类型中，特征向量不包含类标签或响应变量。在这种情况下，目标是找到数据结构。
- en: Clustering is probably the most common unsupervised learning technique, and
    it aims to group a set of samples into different clusters. Samples in the same
    cluster are supposed to be relatively similar and different from samples in other
    clusters (the similarity measure could be the Euclidean distance). *k*-means clustering
    is one of the most popular clustering methods, and it’s used in many applications.
    This section will introduce the differentially private design of *k*-means clustering,
    and we’ll walk you through the design process.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类可能是最常见的无监督学习技术，它的目的是将一组样本分组到不同的聚类中。同一聚类中的样本应该相对相似，并且与其他聚类中的样本不同（相似度度量可以是欧几里得距离）。*k*-means
    聚类是最受欢迎的聚类方法之一，它被用于许多应用。本节将介绍 *k*-means 聚类的差分隐私设计，并带您了解设计过程。
- en: 3.3.1 Differentially private k-means clustering
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 差分隐私 k-means 聚类
- en: We will now moving on to differentially private unsupervised learning algorithms.
    We’ll start by checking out *k*-means clustering and how it works.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将转向差分隐私无监督学习算法。我们将首先检查 *k*-means 聚类及其工作原理。
- en: What is *k*-means clustering?
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是 *k*-means 聚类？
- en: At a high level, *k*-means clustering tries to group similar items in clusters
    or groups. Suppose we have a set of data points that we want to assign to groups
    (or clusters) based on their similarity; the number of groups is represented by
    *k*.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，*k*-means 聚类尝试将相似的项目分组或分组。假设我们有一组数据点，我们希望根据它们的相似性将它们分配到组（或聚类）中；组数用 *k*
    表示。
- en: There are multiple different implementations of *k*-means, including Lloyd’s,
    MacQueen’s, and Hartigan-Wong’s *k*-means. We’ll look at Lloyd’s *k*-mean algorithm
    [7], as it is the most widely known implementation of *k*-means.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-means 有多种不同的实现，包括 Lloyd 的、MacQueen 的和 Hartigan-Wong 的 *k*-means。我们将查看 Lloyd
    的 *k*-mean 算法 [7]，因为它是 *k*-means 最广为人知的实现。'
- en: In the process of training a *k*-means model, the algorithm starts with *k*
    randomly selected centroid points that represent the *k* clusters. Then the algorithm
    iteratively clusters samples to the nearest centroid point and updates the centroid
    points by calculating the means of the samples that are clustered to the centroid
    points.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练 *k*-means 模型过程中，算法从 *k* 个随机选择的质心点开始，这些点代表 *k* 个聚类。然后算法迭代地将样本聚类到最近的质心点，并通过计算聚类到质心点的样本的平均值来更新质心点。
- en: 'Let’s look at an example. In the fresh produce section of your supermarket
    you’ll see different kinds of fruits and vegetables. These items are arranged
    in groups by their types: all the apples are in one place, the oranges are kept
    together, and so on. You will quickly find that they form groups or clusters,
    where each of the items is within a group of their kind, forming the clusters.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。在你的超市新鲜食品区，你会看到不同种类的水果和蔬菜。这些商品按其类型分组排列：所有的苹果都在一个地方，橙子放在一起，等等。你很快会发现它们形成组或聚类，其中每个项目都在其类型的组内，形成聚类。
- en: Implementing differentially private *k*-means clustering
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 实现差分隐私**k**-均值聚类
- en: Now that we’ve outlined *k*-means clustering, let’s start walking through differentially
    private *k*-means clustering. This design follows an algorithm perturbation strategy
    called DPLloyd [8] (an extension of Lloyd’s *k*-means), where the Laplace mechanism
    (i.e., adding Laplacian noise) is applied to the iterative update step in the
    Lloyd algorithm. In essence, it adds Laplace noise to the intermediate centroids
    and cluster sizes and produces differentially private *k*-means models.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经概述了**k**-均值聚类，接下来让我们开始探讨差分隐私**k**-均值聚类。这种设计遵循一种称为DPLloyd [8]（Lloyd的**k**-均值的扩展）的算法扰动策略，其中在Lloyd算法的迭代更新步骤中应用拉普拉斯机制（即添加拉普拉斯噪声）。本质上，它向中间质心和聚类大小添加拉普拉斯噪声，从而产生差分隐私**k**-均值模型。
- en: 'Suppose each sample of the *k*-means clustering is a *d*-dimensional point,
    and assume the *k*-means algorithm has a predetermined number of running iterations,
    denoted as t. In each iteration of the *k*-means algorithm, two values are calculated:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 假设**k**-均值聚类的每个样本是一个**d**-维点，并假设**k**-均值算法有一个预定的运行迭代次数，表示为t。在**k**-均值算法的每次迭代中，计算两个值：
- en: The total number of samples of each cluster C[i], denoted as n[i] (i.e., the
    count queries)
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个聚类C[i]的样本总数，表示为n[i]（即计数查询）
- en: The sum of the samples of each cluster C[i] (to recalculate the centroids),
    denoted as s[i] (i.e., the sum queries)
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个聚类C[i]的样本总和（用于重新计算质心），表示为s[i]（即求和查询）
- en: Then, in *k*-means, each sample will involve d ⋅ t sum queries and t count queries.
    Adding or deleting a new sample will change n[i] by 1, and this operation could
    happen in every iteration, so the sensitivity of n[i] is t. Suppose the size of
    each dimension (i.e., feature) of each sample is bounded in the range [−r,r].
    Then, by adding or deleting a new sample, the difference of x[i] will be d ⋅ r
    ⋅ t.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在**k**-均值中，每个样本将涉及d ⋅ t次求和查询和t次计数查询。添加或删除一个新样本将使n[i]增加1，这种操作可能在每次迭代中发生，因此n[i]的敏感性是t。假设每个样本的每个维度（即特征）的大小被限制在范围[−r,r]内。那么，通过添加或删除一个新样本，x[i]的变化将是d
    ⋅ r ⋅ t。
- en: As mentioned, we are going to use the algorithm perturbation strategy to design
    our differentially private *k*-means clustering algorithm, where the sensitivities
    of the count queries and the sum queries are calculated. The Laplace mechanism
    (i.e., adding Laplacian noise) is applied to the iterative update step in the
    Lloyd algorithm by adding noise to the intermediate centroids and cluster sizes.
    Figure 3.10 illustrates the pseudocode of the algorithm, which is mostly self-explanatory.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将使用算法扰动策略来设计我们的差分隐私**k**-均值聚类算法，其中计算计数查询和求和查询的敏感性。拉普拉斯机制（即添加拉普拉斯噪声）通过向中间质心和聚类大小添加噪声，应用于Lloyd算法的迭代更新步骤。图3.10展示了算法的伪代码，大部分都是自我解释的。
- en: '![CH03_F10_Zhuang](../../OEBPS/Images/CH03_F10_Zhuang.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F10_Zhuang](../../OEBPS/Images/CH03_F10_Zhuang.png)'
- en: Figure 3.10 How differentially private *k*-means clustering works
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 差分隐私**k**-均值聚类的工作原理
- en: 'Let’s train a *k*-means clustering model on the scikit-learn load_digits dataset.
    We’ll follow the example given by scikit-learn and use the load_digits dataset
    to train and test a *k*-means model. As you can see in listing 3.5, we’ll use
    several different metrics for evaluating the clustering performance. Evaluating
    the performance of a clustering algorithm is not as trivial as counting the number
    of errors or the precision and recall of a supervised classification algorithm.
    As such, we will be looking at homogeneity, completeness, and V-measure scores,
    as well as adjusted rand index (ARI) and adjusted mutual information (AMI) score.
    Please refer to the scikit-learn documentation for detailed steps and the mathematical
    formulation: [https://scikit-learn.org/stable/modules/clustering.xhtml#clustering-evaluation](https://scikit-learn.org/stable/modules/clustering.xhtml#clustering-evaluation).'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在scikit-learn的load_digits数据集上训练一个*k*-means聚类模型。我们将遵循scikit-learn给出的示例，并使用load_digits数据集来训练和测试一个*k*-means模型。如图3.5所示，我们将使用几个不同的指标来评估聚类性能。评估聚类算法的性能并不像计算错误数量或监督分类算法的精确率和召回率那样简单。因此，我们将查看同质性、完整性和V-measure评分，以及调整后的兰德指数（ARI）和调整后的互信息（AMI）评分。请参阅scikit-learn文档以获取详细步骤和数学公式：[https://scikit-learn.org/stable/modules/clustering.xhtml#clustering-evaluation](https://scikit-learn.org/stable/modules/clustering.xhtml#clustering-evaluation)。
- en: Listing 3.5 Training a *k*-means clustering model
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.5 训练一个*k*-means聚类模型
- en: '[PRE24]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We’ll now use scikit-learn’s native KMeans function to establish a non-private
    baseline for our experiments. We will use the k-means++ and random initializations:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用scikit-learn的本地KMeans函数来为我们的实验建立一个非私有的基线。我们将使用k-means++和随机初始化：
- en: '[PRE25]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The results may look like figure 3.11\. As you can see, the preceding code covers
    different resulting score metrics such as homogeneity, completeness, and so on.
    Homogeneity, completeness, and V-measure scores are bounded below by 0.0 and above
    by 1.0, and the higher the value, the better. Intuitively, clustering with a bad
    V-measure can be qualitatively analyzed by using homogeneity and completeness
    scores to better determine what kinds of mistakes are being made by the assignment.
    For the ARI, AMI, and silhouette coefficient scores, the range is -1 to 1\. Again,
    the higher the number, the better. On a final note, lower values generally indicate
    two largely independent labels, whereas values close to 1 indicate significant
    agreement.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可能看起来像图3.11。如图所示，前面的代码涵盖了不同的结果评分指标，如同质性、完整性等。同质性、完整性和V-measure评分的下限是0.0，上限是1.0，数值越高越好。直观上，可以通过使用同质性和完整性评分来定性分析具有较差V-measure的聚类，以更好地确定分配过程中所犯的错误类型。对于ARI、AMI和轮廓系数评分，范围是-1到1。同样，数值越高越好。最后，较低的值通常表示两个几乎独立的标签，而接近1的值表示有显著的一致性。
- en: '![CH03_F11_Zhuang](../../OEBPS/Images/CH03_F11_Zhuang.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F11_Zhuang](../../OEBPS/Images/CH03_F11_Zhuang.png)'
- en: 'Figure 3.11 Comparing results: k-means++ vs. random initializations'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 比较结果：k-means++与随机初始化
- en: 'Now let’s apply differential privacy for k-means clustering:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们为k-means聚类应用差分隐私：
- en: '[PRE26]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Once the differentially private *k*-means clustering is applied, you’ll see
    results like those in figure 3.12\. As you can see, dp_k-means provides much more
    varied results than k-means++ and random initializations, providing better privacy
    assurance. When you compare the numbers of the different score metrics, you will
    see how DP affects the final result.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦应用了差分隐私*k*-means聚类，你将看到如图3.12所示的结果。如图所示，dp_k-means提供了比k-means++和随机初始化更多样化的结果，提供了更好的隐私保障。当你比较不同评分指标的数值时，你会看到差分隐私如何影响最终结果。
- en: '![CH03_F12_Zhuang](../../OEBPS/Images/CH03_F12_Zhuang.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F12_Zhuang](../../OEBPS/Images/CH03_F12_Zhuang.png)'
- en: Figure 3.12 Comparing results once the differentially private *k*-means clustering
    is applied
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 应用差分隐私*k*-means聚类后的结果比较
- en: We’ve now investigated the design and use of several differentially private
    ML algorithms, and we’ve gone through some hands-on experiments. In the next section
    we’ll use differentially private principal component analysis (PCA) as a case
    study application, and we’ll walk you through the process of designing a differentially
    private ML algorithm.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经研究了几个不同差分隐私机器学习算法的设计和使用，并进行了一些实际实验。在下一节中，我们将使用差分隐私主成分分析（PCA）作为案例研究应用，并引导你了解设计差分隐私机器学习算法的过程。
- en: '3.4 Case study: Differentially private principal component analysis'
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 案例研究：差分隐私主成分分析
- en: 'In previous sections we discussed the DP mechanisms commonly used in today’s
    applications and the design and usage of various differentially private ML algorithms.
    In this section we’ll discuss how to design differentially private principal component
    analysis (PCA) as a case study, to walk you through the process of designing a
    differentially private ML algorithm. The content in this section is partially
    published in our paper [9], which you can refer to for further details. The implementation
    of this case study can be found in the book’s GitHub repository: [https://github.com/nogrady/PPML/blob/main/Ch3/distr_dp_pca-master.zip](https://github.com/nogrady/PPML/blob/main/Ch3/distr_dp_pca-master.zip).'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了当今应用中常用的DP机制以及各种差分隐私机器学习算法的设计和用法。在本节中，我们将讨论如何设计作为案例研究的差分隐私主成分分析（PCA），以向您介绍设计差分隐私机器学习算法的过程。本节的内容部分发表在我们的论文[9]中，您可以参考以获取更多详细信息。本案例研究的实现可以在本书的GitHub仓库中找到：[https://github.com/nogrady/PPML/blob/main/Ch3/distr_dp_pca-master.zip](https://github.com/nogrady/PPML/blob/main/Ch3/distr_dp_pca-master.zip)。
- en: NOTE This section aims to walk you through all the mathematical formulations
    and empirical evaluations of our case study so that you can understand how to
    develop a differentially private application from scratch. If you do not need
    to know all these implementation details right now, you can skip to the next chapter
    and come back to this section later.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本节旨在向您介绍我们案例研究的所有数学公式和实证评估，以便您了解如何从头开始开发一个差分隐私应用。如果您目前不需要了解所有这些实现细节，您可以跳到下一章，稍后再回到这一节。
- en: 3.4.1 The privacy of PCA over horizontally partitioned data
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 PCA在水平分区数据中的隐私性
- en: PCA is a statistical procedure that computes a low-dimension subspace from the
    underlying data and generates a new set of variables that are linear combinations
    of the original ones. It is widely used in various data mining and ML applications,
    such as network intrusion detection, recommendation systems, text and image processing,
    and so on.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种统计过程，它从底层数据中计算出一个低维子空间，并生成一组新的变量，这些变量是原始变量的线性组合。它在各种数据挖掘和机器学习应用中得到了广泛应用，例如网络入侵检测、推荐系统、文本和图像处理等。
- en: In 2016, Imtiaz et al. presented a first-of-its-kind privacy-preserving distributed
    PCA protocol [10]. Their main idea was to approximate the global PCA by aggregating
    the local PCA from each data owner, in which the data owner holds horizontally
    partitioned data. However, their work suffers from excessive running time and
    utility degradation, while the local principal components fail to provide a good
    representation of the data. More specifically, their solution requires all data
    owners to be online and to transfer the local PCA data one by one. This serialized
    computation made their protocol dependent on the number of data owners, which
    seriously degraded efficiency and scalability. Also, the local principal components
    cannot provide a good representation of the utility of the principal components
    when the amount of data is far less than its number of features.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在2016年，Imtiaz等人提出了一种前所未有的隐私保护分布式PCA协议[10]。他们的主要想法是通过聚合每个数据所有者的本地PCA来近似全局PCA，其中数据所有者持有水平分区数据。然而，他们的工作存在运行时间过长和效用退化的缺点，而局部主成分无法很好地表示数据。更具体地说，他们的解决方案要求所有数据所有者都在线，并且逐个传输本地PCA数据。这种序列化计算使得他们的协议依赖于数据所有者的数量，这严重降低了效率和可扩展性。此外，当数据量远小于其特征数量时，局部主成分无法很好地表示主成分的效用。
- en: The difference between horizontal and vertical partitioning of data
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 数据水平分区和垂直分区的区别
- en: In many practical large-scale solutions, data is usually divided into partitions
    that can be managed and accessed separately. Partitioning can improve scalability
    and performance while reducing contention. Horizontal partitioning (often called
    sharding) partitions rows into multiple datastores with the same schema and columns.
    Vertical partitioning segments columns into multiple datastores containing the
    same rows.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实际的大规模解决方案中，数据通常被划分为可以单独管理和访问的分区。分区可以提高可扩展性和性能，同时减少竞争。水平分区（通常称为分片）将行分区到具有相同模式和列的多个数据存储中。垂直分区将列分割到包含相同行的多个数据存储中。
- en: In our case study, we will assume that the data is horizontally partitioned,
    which means all the data shares the same features. The number of data owners will
    be more than hundreds. We’ll presume an untrustworthy data user would like to
    learn the principal components of the distributed data. An honest but curious
    intermediary party, named *proxy*, works between the data user and data owners.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例研究中，我们将假设数据是水平划分的，这意味着所有数据共享相同的特征。数据所有者的数量将超过数百。我们将假设一个不可信的数据用户想要学习分布式数据的主成分。一个诚实但好奇的中间方，称为
    *proxy*，在数据用户和数据所有者之间工作。
- en: Data owners simultaneously encrypt their own data share and send it to the proxy.
    The proxy runs a differentially private aggregation algorithm over the encrypted
    data and sends the output to the data user. The data user then computes the principal
    components from the output without learning the content of the underlying data.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 数据所有者同时加密自己的数据份额并将其发送给代理。代理在加密数据上运行差分隐私聚合算法，并将输出发送给数据用户。然后，数据用户从输出中计算主成分，而不了解底层数据的内容。
- en: In our experiments, we will study the running time, utility, and privacy tradeoff
    of the proposed protocol and compare it with previous work.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们将研究所提出协议的运行时间、效用和隐私权衡，并将其与先前的工作进行比较。
- en: What does “honest but curious” mean?
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: “诚实但好奇”是什么意思？
- en: Typically, in communication protocols, an honest-but-curious adversary is a
    legitimate participant or a user who will not deviate from the defined limits
    of the protocol but will attempt to learn all possible information from legitimately
    received messages.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在通信协议中，一个诚实但好奇的对手是一个合法的参与者或用户，他不会偏离协议定义的限制，但会尝试从合法接收的消息中学习所有可能的信息。
- en: Before moving forward with our protocol design, let’s briefly go over the concepts
    of PCA and homomorphic encryption.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续我们的协议设计之前，让我们简要回顾一下主成分分析（PCA）和同态加密的概念。
- en: How principal component analysis works
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析是如何工作的
- en: Let’s quickly walk through the mathematical formulation of PCA. Given a square
    matrix A, an eigenvector v of A is a nonzero vector that does not change the direction
    when A is applied to it, such that
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速浏览一下主成分分析（PCA）的数学公式。给定一个方阵 A，A 的一个特征向量 v 是一个非零向量，当 A 作用于它时不会改变方向，即
- en: Av = λv
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Av = λv
- en: where λ is a real number scalar, referred to as the eigenvalue. Suppose A ∈
    ℝ^((n×n)), then it has at most n eigenvectors, and each eigenvector associates
    with a distinct eigenvalue.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 λ 是一个实数标量，称为特征值。假设 A ∈ ℝ^((n×n))，那么它最多有 n 个特征向量，每个特征向量都与一个不同的特征值相关联。
- en: Consider a dataset with *N* samples x[1], x[2],...,x[N], where each sample has
    M features (x^i ∈ ). A center-adjusted scatter matrix ![CH03_F12_zhuang-ch3-eqs-27x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-27x.png) is
    computed as follows,
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个包含 *N* 个样本 x[1], x[2],...,x[N] 的数据集，其中每个样本有 M 个特征（x^i ∈ ）。一个中心调整的散点矩阵 ![CH03_F12_zhuang-ch3-eqs-27x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-27x.png)
    计算如下，
- en: '![CH03_F12_zhuang-ch3-eqs-28x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-28x.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F12_zhuang-ch3-eqs-28x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-28x.png)'
- en: where μ is the mean vector, μ = 1/N ∑^N[(i=1)]x[i]. By using eigenvalue decomposition
    (EVD) on ![CH03_F12_zhuang-ch3-eqs-30x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-30x.png),
    we will have *Λ* and *U*, where *Λ* = diag(*λ*[1], *λ*[2],...,*λ*[M]) is a diagonal
    matrix of eigenvalues.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 μ 是均值向量，μ = 1/N ∑^N[(i=1)]x[i]。通过在 ![CH03_F12_zhuang-ch3-eqs-30x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-30x.png)
    上进行特征值分解（EVD），我们将得到 *Λ* 和 *U*，其中 *Λ* = diag(*λ*[1], *λ*[2],...,*λ*[M]) 是一个特征值对角矩阵。
- en: This can be arranged to a non-increasing order in absolute value; in other words,
    ‖λ[1]‖ ≥ ‖λ[2]‖ ≥ ⋯ ≥ ‖λ[M]‖, U = [u[1] u[2] ... u[M] ] is an M × M matrix where
    u[j] denotes the j^(th) eigenvector of ![CH03_F12_zhuang-ch3-eqs-30x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-30x.png).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以按绝对值非递增的顺序排列；换句话说，‖λ[1]‖ ≥ ‖λ[2]‖ ≥ ⋯ ≥ ‖λ[M]‖，U = [u[1] u[2] ... u[M] ]
    是一个 M × M 矩阵，其中 u[j] 表示 ![CH03_F12_zhuang-ch3-eqs-30x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-30x.png)
    的第 j 个特征向量。
- en: In PCA, each eigenvector represents a principal component.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在主成分分析（PCA）中，每个特征向量代表一个主成分。
- en: What is homomorphic encryption?
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 同态加密是什么？
- en: Homomorphic encryption is an essential building block of this work. In simple
    terms, it allows computations to be performed over the encrypted data, in which
    the decryption of the generated result matches the result of operations performed
    on the plaintext. In this section we will use the Paillier cryptosystem to implement
    the protocol.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 同态加密是这个工作的一个基本构建块。简单来说，它允许在加密数据上执行计算，其中生成的结果的解密与在明文上执行的操作的结果相匹配。在本节中，我们将使用Paillier密码系统来实现协议。
- en: To refresh your memory, the Paillier cryptosystem is a probabilistic asymmetric
    algorithm for public-key cryptography (a partially homomorphic encryption scheme)
    introduced by Pascal Paillier.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 为了刷新你的记忆，Paillier密码系统是由Pascal Paillier提出的一种用于公钥密码学的概率非对称算法（一种部分同态加密方案）。
- en: Let’s consider the function ε[pk][⋅] to be the encryption scheme with public
    key pk, the function D[sk][⋅] to be the decryption scheme with private key sk.
    The additive homomorphic encryption can be defined as
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑函数 ε[pk][⋅]，它是一个具有公钥pk的加密方案，函数 D[sk][⋅]是一个具有私钥sk的解密方案。加法同态加密可以定义为
- en: '![CH03_F12_zhuang-ch3-eqs-32x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-32x.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F12_zhuang-ch3-eqs-32x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-32x.png)'
- en: where ⊗ denotes the modulo multiplication operator in the encrypted domain and
    a and b are the plaintext messages. The multiplicative homomorphic encryption
    is defined as
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ⊗ 表示加密域中的模乘法运算符，a和b是明文消息。乘法同态加密定义为
- en: '![CH03_F12_zhuang-ch3-eqs-33x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-33x.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F12_zhuang-ch3-eqs-33x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-33x.png)'
- en: Since the cryptosystem only accepts integers as input, real numbers should be
    discretized. In this example, we’ll adopt the following formulation,
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 由于密码系统只接受整数作为输入，实数应该被离散化。在这个例子中，我们将采用以下公式，
- en: '![CH03_F12_zhuang-ch3-eqs-34x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-34x.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F12_zhuang-ch3-eqs-34x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-34x.png)'
- en: where e is the number of bits, and min[F], max[F] are the minimal and maximal
    value of feature F, x is the real number to be discretized, and Discretize[(e,F)]
    (x) takes a value in [0,2^((e-1))].
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 e是位数，min[F]，max[F]是特征F的最小值和最大值，x是要离散化的实数，Discretize[(e,F)](x)取值在[0,2^((e-1))].
- en: 3.4.2 Designing differentially private PCA over horizontally partitioned data
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 在水平划分的数据上设计差分隐私PCA
- en: Let’s first revisit what we want to achieve here (see figure 3.13). Suppose
    there are L data owners, and each data owner l has a dataset ![CH03_F12_zhuang-ch3-eqs-35x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-35x.png),
    where *M* is the number of dimensions and *N*^l is the number of samples held
    by *l*. The horizontal aggregation of *X*^l, *i* *= 1,2,...,**l* generates a data
    matrix *X* ∈ ℝ^((N×M)), where *N* *= ∑*^l[(i=1)]*N*^i. Now assume a data user
    wants to perform PCA on *X*. To protect the privacy of the original data, data
    owners would not share the original data with the data user in cleartext form.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先回顾一下我们在这里想要实现的目标（见图3.13）。假设有L个数据所有者，每个数据所有者l有一个数据集![CH03_F12_zhuang-ch3-eqs-35x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-35x.png)，其中
    *M* 是维数数量，*N*^l是数据所有者l持有的样本数量。*X*^l的横向聚合生成一个数据矩阵 *X* ∈ ℝ^((N×M))，其中 *N* *= ∑*^l[(i=1)]*N*^i。现在假设一个数据用户想要对
    *X* 执行PCA。为了保护原始数据的隐私，数据所有者不会以明文形式与数据用户共享原始数据。
- en: '![CH03_F13_Zhuang](../../OEBPS/Images/CH03_F13_Zhuang.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F13_Zhuang](../../OEBPS/Images/CH03_F13_Zhuang.png)'
- en: Figure 3.13 The high-level overview of distributed PCA
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13 分布式PCA的高级概述
- en: To accommodate this, we need to design a differentially private distributed
    PCA protocol that allows data users to perform PCA but to learn nothing except
    the principal components. Figure 3.14 illustrates one design for a differentially
    private distributed PCA protocol. In this scenario, data owners are assumed to
    be honest and not to collude with each other, but the data user is assumed to
    be untrustworthy and will want to learn more information than the principal components.
    The proxy works as an honest-but-curious intermediary party who does not collude
    with the data user or owners.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应这一点，我们需要设计一个差分隐私分布式PCA协议，允许数据用户执行PCA，但除了主成分之外，不学习任何信息。图3.14说明了差分隐私分布式PCA协议的一个设计方案。在这种情况下，假设数据所有者是诚实的，并且不会相互勾结，但假设数据用户是不可信的，并且会想要学习比主成分更多的信息。代理作为一个诚实但好奇的中间方，不会与数据用户或所有者勾结。
- en: '![CH03_F14_Zhuang](../../OEBPS/Images/CH03_F14_Zhuang.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F14_Zhuang](../../OEBPS/Images/CH03_F14_Zhuang.png)'
- en: Figure 3.14 The design of the protocol and how it works
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14 协议的设计和工作原理
- en: In order to learn the principal components of X, the scatter matrix of X needs
    to be computed. In the proposed protocol, each data owner l computes a data share
    of X^l. To prevent the proxy learning the data, each data share is encrypted before
    it is sent to the proxy. Once the proxy receives the encrypted data share from
    each data owner, the proxy runs the differentially private aggregation algorithm
    and sends the aggregated result to the data user. Then the data user constructs
    the scatter matrix from the result and computes the principal components. Figure
    3.14 outlines these steps.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习X的主成分，需要计算X的散布矩阵。在提出的协议中，每个数据所有者l计算X^l的数据份额。为了防止代理学习数据，每个数据份额在发送到代理之前都会被加密。一旦代理从每个数据所有者那里收到加密的数据份额，代理就会运行差分隐私聚合算法，并将聚合结果发送给数据用户。然后数据用户从结果中构建散布矩阵并计算主成分。图3.14概述了这些步骤。
- en: Computing the scatter matrix
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 计算散布矩阵
- en: Let’s look at the computation of the distributed scatter matrix.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看分布式散布矩阵的计算。
- en: Suppose there are L data owners, and each data owner l has a dataset, ![CH03_F12_zhuang-ch3-eqs-35x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-35x.png),
    where M is the number of dimensions, and N^l is the number of samples held by
    l. Each data owner locally computes a data share that contains
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有L个数据所有者，每个数据所有者l有一个数据集，![CH03_F12_zhuang-ch3-eqs-35x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-35x.png)，其中M是维度数，N^l是l持有的样本数。每个数据所有者本地计算一个包含
- en: '![CH03_F14_zhuang-ch3-eqs-38x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-38x.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F14_zhuang-ch3-eqs-38x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-38x.png)'
- en: 'where x[i] = [x[i1] x[i2]... x[iM]]^Te. The scatter matrix ![CH03_F12_zhuang-ch3-eqs-30x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-30x.png) can
    be computed by summing the data share from each data owner:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 x[i] = [x[i1] x[i2]... x[iM]]^Te。散布矩阵 ![CH03_F12_zhuang-ch3-eqs-30x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-30x.png)可以通过对每个数据所有者的数据份额求和来计算：
- en: '![CH03_F14_zhuang-ch3-eqs-39x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-39x.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F14_zhuang-ch3-eqs-39x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-39x.png)'
- en: where
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![CH03_F14_zhuang-ch3-eqs-40x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-40x.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F14_zhuang-ch3-eqs-40x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-40x.png)'
- en: The distributed scatter matrix computation allows each data owner to compute
    a partial result simultaneously. Unlike other methods [10], this approach reduces
    the dependence between data owners and allows them to send the data shares simultaneously.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式散布矩阵的计算允许每个数据所有者同时计算部分结果。与其它方法[10]不同，这种方法减少了数据所有者之间的依赖性，并允许它们同时发送数据份额。
- en: Designing the protocol
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 设计协议
- en: It is crucial to prevent any possible data leakage through the proxy, so each
    individual data share should be encrypted by the data owners. Then the proxy aggregates
    the received encrypted shares. To prevent the inference from PCA, the proxy adds
    a noise matrix to the aggregated result, which makes the approximation of the
    scatter matrix satisfy (ϵ, δ)-DP. The aggregated result is then sent to the data
    user.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 防止通过代理泄露任何可能的数据泄露至关重要，因此每个数据份额应由数据所有者加密。然后代理对收到的加密份额进行聚合。为了防止从PCA中进行推断，代理向聚合结果添加一个噪声矩阵，使得散布矩阵的近似满足(ϵ,
    δ)-DP。然后，聚合结果被发送给数据用户。
- en: What is (ϵ, δ)-DP? δ is another privacy budget parameter, where, when δ = 0,
    the algorithm satisfies ϵ-DP, which is a stronger privacy guarantee than (ϵ, δ)-DP
    with δ > 0. You can find more details about δ in section A.1
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是(ϵ, δ)-DP？δ是另一个隐私预算参数，其中，当δ = 0时，算法满足ϵ-DP，这比δ > 0的(ϵ, δ)-DP提供了更强的隐私保证。你可以在附录A.1中找到关于δ的更多详细信息。
- en: This can be seen as the most general relaxation of ϵ-DP, which weakens the definition
    by allowing an additional small δ density of probability on which the upper bound
    ε does not hold. If you think of the practicality of DP, this leads to the dilemma
    of data release. You cannot always be true to the data and protect the privacy
    of all individuals. There is a tradeoff between the utility of the output and
    privacy. Especially in the case of outliers, an ϵ private release will have very
    little utility (as the sensitivity is very large, leading to a lot of noise being
    added).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以看作是ϵ-DP的最一般松弛，它通过允许一个额外的微小δ概率密度来放宽定义，这个概率密度上的上界ε不成立。如果你考虑DP的实际应用，这会导致数据发布的困境。你不可能总是忠实于数据并保护所有个人的隐私。输出效用和隐私之间存在权衡。特别是在异常值的情况下，ε隐私发布将几乎没有效用（因为敏感性很大，导致添加了很多噪声）。
- en: Alternatively, you can remove the outliers or clip their values to achieve a
    more sensible sensitivity. This way the output will have better utility, but it
    is no longer a true representation of the dataset.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以移除异常值或修剪它们的值以实现更合理的敏感性。这样，输出将具有更好的效用，但它不再是数据集的真实表示。
- en: 'This all leads to (ϵ, δ) privacy. The data user decrypts the result, constructs
    an approximation of the scatter matrix, and then calculates PCA, as you saw earlier.
    Let’s walk through the steps:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些最终导致 (ϵ, δ) 隐私。数据用户解密结果，构建散点矩阵的近似，然后计算PCA，正如你之前看到的。让我们一步一步来：
- en: The data user generates a public key pair (pk and sk) for the Paillier cryptosystem
    and sends pk to the proxy and data owners. In practice, secure distribution of
    keys is important, but we are not emphasizing it here.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据用户为Paillier密码系统生成一个公钥对（pk和sk），并将pk发送给代理和数据所有者。在实践中，安全地分发密钥很重要，但在这里我们不强调这一点。
- en: Thereafter, the data owners compute the share R^l, v^l, l = 1,2,..., L, and
    send ε[pk][R^l], ε[pk][v^l], ε[pk][N^l] to the proxy.
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此后，数据所有者计算份额 R^l，v^l，l = 1,2,..., L，并将 ε[pk][R^l]，ε[pk][v^l]，ε[pk][N^l] 发送到代理。
- en: After receiving the encrypted data share from each data owner, the proxy aggregates
    the shares and applies the symmetric matrix noise to satisfy DP. This process
    is shown in algorithm 1 in figure 3.15.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在收到来自每个数据所有者的加密数据份额后，代理聚合份额并应用对称矩阵噪声以满足DP。这个过程如图3.15中的算法1所示。
- en: 'Let’s look at algorithm 1 to understand what’s happening here:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看算法1来了解这里发生了什么：
- en: 'Lines 2-4: Aggregate the data shares from each data owner.'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第2-4行：聚合来自每个数据所有者的数据份额。
- en: '![CH03_F14_zhuang-ch3-eqs-41x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-41x.png)'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH03_F14_zhuang-ch3-eqs-41x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-41x.png)'
- en: 'Lines 5-7: Construct the noisy ε[pk][v '']. To prevent the data user learning
    information from v, the proxy generates a noisy ε[pk][v''] by summing a random
    vector ε[pk][b] with ε[pk][v], such that ε[pk][v''] = ε[pk][v] ⊗ ε[pk][b]. It
    can be shown that the element v''[ij] of v''v''^T is'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第5-7行：构建噪声 ε[pk][v ']。为了防止数据用户从v中学习信息，代理通过将随机向量 ε[pk][b] 与 ε[pk][v] 相加来生成噪声
    ε[pk][v']，使得 ε[pk][v'] = ε[pk][v] ⊗ ε[pk][b]。可以证明，v'v'^T的元素 v'[ij] 是
- en: '![CH03_F14_zhuang-ch3-eqs-43x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-43x.png)'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH03_F14_zhuang-ch3-eqs-43x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-43x.png)'
- en: Both sides of the equation are divided by N, which yields
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方程式的两边都除以N，得到
- en: '![CH03_F14_zhuang-ch3-eqs-44x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-44x.png)'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH03_F14_zhuang-ch3-eqs-44x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-44x.png)'
- en: '![CH03_F14_zhuang-ch3-eqs-45x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-45x.png)'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH03_F14_zhuang-ch3-eqs-45x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-45x.png)'
- en: Thus, we have
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，我们有
- en: '![CH03_F14_zhuang-ch3-eqs-46x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-46x.png)'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH03_F14_zhuang-ch3-eqs-46x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-46x.png)'
- en: Recall that, in the Paillier cryptosystem, the multiplicative homomorphic property
    is defined as
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回想一下，在Paillier密码系统中，乘法同态性质定义为
- en: '![CH03_F14_zhuang-ch3-eqs-47x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-47x.png)'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH03_F14_zhuang-ch3-eqs-47x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-47x.png)'
- en: Then, ε[pk][G[ij]] is
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，ε[pk][G[ij]] 是
- en: '![CH03_F14_zhuang-ch3-eqs-48x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-48x.png)'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH03_F14_zhuang-ch3-eqs-48x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-48x.png)'
- en: At this point we can make the exponent be an integer by multiplying b with N.
    It should be noted that during the encryption, the proxy has to learn N. To achieve
    this, the proxy sends ε[pk][N] to the data user, and the data user returns N in
    cleartext once it’s decrypted.
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以通过将b与N相乘来使指数成为整数。需要注意的是，在加密过程中，代理必须学习N。为了实现这一点，代理将 ℇ[pk][N] 发送到数据用户，数据用户在解密后以明文形式返回N。
- en: 'Lines 8-10: Apply symmetric matrix to satisfy (ϵ, δ)-DP. The proxy generates
    G’ ∈ ℝ^(M×M), based on the DP parameter (ϵ, δ), and gets ℇ[pk][R’], ℇ[pk][v’],
    where'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第8-10行：应用对称矩阵以满足 (ϵ, δ)-DP。代理根据DP参数 (ϵ, δ) 生成 G’ ∈ ℝ^(M×M)，并获取 ℇ[pk][R’]，ℇ[pk][v’]，其中
- en: '![CH03_F14_zhuang-ch3-eqs-50x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-50x.png)'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH03_F14_zhuang-ch3-eqs-50x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-50x.png)'
- en: '![CH03_F14_zhuang-ch3-eqs-51x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-51x.png)'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH03_F14_zhuang-ch3-eqs-51x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-51x.png)'
- en: Then, ℇ[pk][R’], ℇ[pk][v’] are sent to the data user.
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，将 ℇ[pk][R’]，ℇ[pk][v’] 发送到数据用户。
- en: After receiving the aggregated result from the proxy, ε[pk][N], ε[pk][R’], ε[pk][v’],
    the data user decrypts each and computes ![CH03_F14_zhuang-ch3-eqs-52x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-52x.png).
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在从代理收到聚合结果 ε[pk][N]，ε[pk][R’]，ε[pk][v’] 后，数据用户对每个结果进行解密并计算 ![CH03_F14_zhuang-ch3-eqs-52x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-52x.png)。
- en: '![CH03_F14_zhuang-ch3-eqs-53x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-53x.png)'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH03_F14_zhuang-ch3-eqs-53x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-53x.png)'
- en: With ![CH03_F14_zhuang-ch3-eqs-52x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-52x.png),
    the data user can proceed to compute the eigenvector and get the principal components.
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用![CH03_F14_zhuang-ch3-eqs-52x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-52x.png)，数据用户可以继续计算特征向量并得到主成分。
- en: '![CH03_F15_Zhuang](../../OEBPS/Images/CH03_F15_Zhuang.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F15_Zhuang](../../OEBPS/Images/CH03_F15_Zhuang.png)'
- en: 'Figure 3.15 Algorithm 1: DPAggregate'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.15 算法1：DPAggregate
- en: Analyzing the security and privacy
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 分析安全性和隐私性
- en: Now let’s determine whether the protocol is secure enough. In our example, the
    data user is assumed to be untrustworthy, and the proxy is assumed to be honest
    but curious. Furthermore, we are assuming that the proxy will not collude with
    the data user or data owners.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来确定协议是否足够安全。在我们的例子中，假设数据用户是不可信的，代理假设是诚实但好奇的。此外，我们假设代理不会与数据用户或数据所有者勾结。
- en: To protect the data against the proxy, R^l, v^l, and N^l are encrypted by the
    data owner. During the protocol execution, the proxy only learns N in plaintext,
    and it will not disclose the privacy of a single data owner. Without colluding
    with the data user, the proxy cannot learn the values of R^l, v^l, and N^l. On
    the other side, the proxy mixes R and v with random noise, to prevent the data
    user from gaining information other than the principal components.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保护数据免受代理的侵害，数据所有者加密了R^l、v^l和N^l。在协议执行过程中，代理只学习到明文中的N，它不会泄露单个数据所有者的隐私。在没有与数据用户勾结的情况下，代理不能学习到R^l、v^l和N^l的值。另一方面，代理将R和v与随机噪声混合，以防止数据用户获取除了主成分之外的信息。
- en: Of the data received from the proxy, the data user decrypts ε[pk][N], ε[pk][R’],
    ε[pk][v’] and then proceeds to construct an approximation of the scatter matrix, ![CH03_F15_zhuang-ch3-eqs-54x](../../OEBPS/Images/CH03_F15_zhuang-ch3-eqs-54x.png),
    in which G’ is the Gaussian symmetric matrix carried by R’. The (ϵ, δ)-DP is closed
    for the postprocessing algorithm of ![CH03_F14_zhuang-ch3-eqs-52x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-52x.png).
    Since the proxy is not colluding with the data user, the data user cannot learn
    the value of R and v. Therefore, the data user learns nothing but the computed
    principal components.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 从代理接收到的数据中，数据用户解密了ε[pk][N]、ε[pk][R’]、ε[pk][v’]，然后继续构建散点矩阵的近似值，![CH03_F15_zhuang-ch3-eqs-54x](../../OEBPS/Images/CH03_F15_zhuang-ch3-eqs-54x.png)，其中G’是R’携带的高斯对称矩阵。对于![CH03_F14_zhuang-ch3-eqs-52x](../../OEBPS/Images/CH03_F14_zhuang-ch3-eqs-52x.png)的后处理算法，(ϵ,
    δ)-DP是封闭的。由于代理没有与数据用户勾结，数据用户不能学习R和v的值。因此，数据用户只能学习到计算出的主成分。
- en: As a flexible design, this approach can cooperate with different symmetric noise
    matrices to satisfy (ϵ, δ)-DP. To demonstrate the protocol to you, we implemented
    the Gaussian mechanism as shown in algorithm 2 (figure 3.16).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种灵活的设计，这种方法可以与不同的对称噪声矩阵合作，以满足(ϵ, δ)-DP。为了向您展示协议，我们实现了算法2（图3.16）中的高斯机制。
- en: '![CH03_F16_Zhuang](../../OEBPS/Images/CH03_F16_Zhuang.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F16_Zhuang](../../OEBPS/Images/CH03_F16_Zhuang.png)'
- en: 'Figure 3.16 Algorithm 2: the Gaussian mechanism'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16 算法2：高斯机制
- en: It is noteworthy that once the data user learns the private principal components
    from the protocol, they could release the principal components to the public for
    further use, which would allow the proxy to access the components. In that case,
    the proxy would still not have enough information to recover the covariance matrix
    from a full set of principal components, which implies that the proxy cannot recover
    the approximation of the covariance matrix with the released private principal
    components. Moreover, the data user might release a subset (top K ) of principal
    components rather than the full set of components, which would make it even harder
    for the proxy to recover the covariance matrix. Without knowing the approximation
    of the covariance matrix, the proxy could not infer the plain data by removing
    the added noise.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，一旦数据用户从协议中学习到私有主成分，他们可能会将这些主成分公开以供进一步使用，这将允许代理访问这些组件。在这种情况下，代理仍然没有足够的信息从完整的主成分集中恢复协方差矩阵，这意味着代理不能从发布的数据中恢复协方差矩阵的近似值。此外，数据用户可能会发布主成分的子集（前K个），而不是完整的组件集，这将使代理恢复协方差矩阵变得更加困难。不知道协方差矩阵的近似值，代理不能通过去除添加的噪声来推断原始数据。
- en: 3.4.3 Experimentally evaluating the performance of the protocol
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.3 实验评估协议的性能
- en: We’ve discussed the theoretical background of the proposed protocol, so now
    let’s implement the differentially private distributed PCA (DPDPCA) protocol and
    evaluate it in terms of efficiency, utility, and privacy. For efficiency, we will
    measure the run-time efficiency of DPDPCA and compare it to similar work in the
    literature [10]; this will show that DPDPCA outperforms the previous work.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了所提议协议的理论背景，现在让我们实现差分隐私分布式主成分分析（DPDPCA）协议，并从效率、效用和隐私等方面对其进行评估。为了评估效率，我们将测量DPDPCA的运行时间效率，并将其与文献[10]中的类似工作进行比较；这将表明DPDPCA优于先前的工作。
- en: This experiment will be developed using Python and the Python Paillier homomorphic
    cryptosystem library published in [https://github.com/mikeivanov/paillier](https://github.com/mikeivanov/paillier).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 此实验将使用Python和发布在[https://github.com/mikeivanov/paillier](https://github.com/mikeivanov/paillier)的Python
    Paillier同态密码系统库进行开发。
- en: The dataset and the evaluation methodology
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集和评估方法
- en: We will use six datasets for the experiments, as shown in table 3.2\. The Aloi
    dataset is a collection of color images of small objects, the Facebook comment
    volume dataset contains features extracted from Facebook posts, and the Million
    Song dataset consists of audio features. The cardinalities of Aloi, Facebook,
    and Million Song are more than 100,000, and the dimensionality of each is less
    than 100\. The CNAE dataset is a text dataset extracted from business documents,
    and the attributes are the term frequency. The GISETTE dataset contains grayscale
    images of the highly confusable digits 4 and 9, used in the NIPS 2003 feature
    selection challenge. ISOLET is a dataset of spoken letters, which records the
    26 English letters from 150 subjects, and it has a combination of features like
    spectral coefficients and contour features. All the datasets, excluding Aloi,
    are from the UCI ML repository, whereas Aloi is from LIBSVM dataset repository.
    We’ll evaluate the performance of DPDPCA in terms of SVM classification over the
    CNAE, GISETTE, and ISOLET datasets.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用六个数据集进行实验，如表3.2所示。Aloi数据集是小型物体彩色图像的集合，Facebook评论量数据集包含从Facebook帖子中提取的特征，而百万歌曲数据集由音频特征组成。Aloi、Facebook和百万歌曲的基数超过100,000，每个的维度小于100。CNAE数据集是从商业文档中提取的文本数据集，属性是词频。GISETTE数据集包含用于NIPS
    2003特征选择挑战的高度混淆的数字4和9的灰度图像。ISOLET是一个语音字母数据集，记录了150个受试者的26个英语字母，并具有如频谱系数和轮廓特征等特征组合。所有数据集（除Aloi外）均来自UCI
    ML存储库，而Aloi来自LIBSVM数据集存储库。我们将通过CNAE、GISETTE和ISOLET数据集上的SVM分类来评估DPDPCA的性能。
- en: Table 3.2 A summary of the experimental datasets
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2 实验数据集摘要
- en: '| Dataset | Feature | Cardinality |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 特征 | 基数 |'
- en: '| Aloi | 29 | 108,000 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| Aloi | 29 | 108,000 |'
- en: '| Facebook | 54 | 199,030 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| Facebook | 54 | 199,030 |'
- en: '| Million Song | 90 | 515,345 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 百万歌曲 | 90 | 515,345 |'
- en: '| CNAE | 857 | 1,080 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| CNAE | 857 | 1,080 |'
- en: '| ISOLET | 617 | 7,797 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| ISOLET | 617 | 7,797 |'
- en: '| GISETTE | 5,000 | 13,500 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| GISETTE | 5,000 | 13,500 |'
- en: For the classification result, we’ll measure the precision, recall, and F1 scores
    because the datasets are unbalanced. You can refer to the following mathematical
    formulations for the details of these measurements. In addition, all experiments
    will be run 10 times, and the mean and standard deviation of the result will be
    drawn in figures.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类结果，我们将测量精确度、召回率和F1分数，因为数据集是不平衡的。您可以参考以下数学公式以了解这些测量的详细信息。此外，所有实验将运行10次，并将结果的平均值和标准差绘制在图中。
- en: '![CH03_F16_zhuang-ch3-eqs-55x](../../OEBPS/Images/CH03_F16_zhuang-ch3-eqs-55x.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F16_zhuang-ch3-eqs-55x](../../OEBPS/Images/CH03_F16_zhuang-ch3-eqs-55x.png)'
- en: '![CH03_F16_zhuang-ch3-eqs-56x](../../OEBPS/Images/CH03_F16_zhuang-ch3-eqs-56x.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F16_zhuang-ch3-eqs-56x](../../OEBPS/Images/CH03_F16_zhuang-ch3-eqs-56x.png)'
- en: '![CH03_F16_zhuang-ch3-eqs-57x](../../OEBPS/Images/CH03_F16_zhuang-ch3-eqs-57x.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F16_zhuang-ch3-eqs-57x](../../OEBPS/Images/CH03_F16_zhuang-ch3-eqs-57x.png)'
- en: The efficiency of the proposed approach
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 提出方法的高效性
- en: 'As we’ve mentioned, the previous work suffered from two main problems: the
    excessive protocol running time and the utility degradation when the local principal
    components failed to provide good data representation. In this section we’ll compare
    both protocols in these two aspects. For brevity, we’ll refer to the proposed
    protocol as “DPDPCA” and the work done by Imtiaz and Sarwate [11] as “PrivateLocalPCA.”'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所提到的，先前的工作存在两个主要问题：协议运行时间过长以及当局部主成分无法提供良好的数据表示时，效用下降。在本节中，我们将从这两个方面比较这两种协议。为了简洁起见，我们将所提议的协议称为“DPDPCA”，而Imtiaz和Sarwate[11]所做的工作称为“PrivateLocalPCA”。
- en: 'First, we’ll look at the results of the running times of DPDPCA and PrivateLocalPCA.
    The total running time of DPDPCA included the following:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看DPDPCA和PrivateLocalPCA的运行时间结果。DPDPCA的总运行时间包括以下部分：
- en: The average local computation time of the data owner
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据所有者的平均局部计算时间
- en: The time of the private aggregation algorithm in the proxy
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理中私有聚合算法的时间
- en: The time of performing PCA by the data user
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据用户执行PCA的时间
- en: The data transmission time among parties
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各方之间的数据传输时间
- en: For PrivateLocalPCA, the running time started with the first data owner and
    ended with the last data owner, including the local PCA computation and transmission
    time. We simulated the data transmission using I/O operations rather than the
    local network to make the communication consistent and stable. We measured the
    protocol running time for the different number of data owners, and all samples
    were distributed evenly to each data owner. The experiment ran on a desktop machine
    (i7-5820k, 64 GB memory).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 对于PrivateLocalPCA，运行时间从第一个数据所有者开始，到最后一个数据所有者结束，包括局部PCA计算和传输时间。我们使用I/O操作而不是本地网络来模拟数据传输，以使通信一致且稳定。我们测量了不同数量数据所有者的协议运行时间，并且所有样本都被均匀分配到每个数据所有者。实验在一台台式机上运行（i7-5820k，64
    GB内存）。
- en: The results are shown in figure 3.17\. The horizontal axis specifies the number
    of data owners, and the vertical axis specifies the running time in seconds. You
    can see that PrivateLocalPCA runs almost linearly upon the number of data owners.
    That’s because PrivateLocalPCA requires that the local principal components are
    transmitted through data owners one by one, and the next data owner has to wait
    for the result from the previous one. Thus, it has a time complexity of O(*n*),
    where *n* is the number of data owners. In contrast, DPDPCA costs far less time
    than PrivateLocalPCA, given the same number of data owners. The reason is, first,
    that the distributed scatter matrix computation allows each data owner to compute
    their local share simultaneously, and second, that the proxy can implement the
    aggregation of the local shares in parallel, which runs loglinearly upon the number
    of data owners. Overall, DPDPCA has better scalability than PrivateLocalPCA regarding
    the number of data owners.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如图3.17所示。横坐标指定数据所有者的数量，纵坐标指定运行时间（秒）。您可以看到PrivateLocalPCA的运行时间几乎与数据所有者的数量呈线性关系。这是因为PrivateLocalPCA要求将局部主成分逐个通过数据所有者传输，下一个数据所有者必须等待前一个数据所有者的结果。因此，它的时间复杂度为O(*n*)，其中*n*是数据所有者的数量。相比之下，DPDPCA在相同数量的数据所有者下所需时间远少于PrivateLocalPCA。原因是，首先，分布式散度矩阵计算允许每个数据所有者同时计算他们的局部份额，其次，代理可以并行实现局部份额的聚合，其运行时间与数据所有者的数量呈对数线性关系。总的来说，DPDPCA在数据所有者数量方面比PrivateLocalPCA具有更好的可扩展性。
- en: '![CH03_F17_Zhuang](../../OEBPS/Images/CH03_F17_Zhuang.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F17_Zhuang](../../OEBPS/Images/CH03_F17_Zhuang.png)'
- en: Figure 3.17 Running-time comparison between DPDPCA and PrivateLocalPCA, **∈**
    = 0.3
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17 DPDPCA与PrivateLocalPCA的运行时间比较，**∈** = 0.3
- en: The effect on the utility of the application
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 对应用程序效用的影响
- en: Next, we’ll explore the utility degradation of PrivateLocalPCA and DPDPCA when
    the amount of data is far less than the number of features. We are considering
    a scenario where each data owner holds a dataset where the cardinality may be
    far smaller than the number of features, such as the images, ratings of music
    and movies, and personal activity data. To simulate this scenario, we distributed
    different sized samples to each data owner in the experiment. For PrivateLocalPCA,
    the variance is not fully preserved because only the first few principal components
    are used to represent the data.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨当数据量远小于特征数量时，PrivateLocalPCA和DPDPCA的效用退化。我们考虑一个场景，即每个数据所有者持有包含基数可能远小于特征数量的数据集，例如图像、音乐和电影的评分以及个人活动数据。为了模拟这种情况，我们在实验中将不同大小的样本分配给每个数据所有者。对于PrivateLocalPCA，由于只使用了前几个主成分来表示数据，因此方差并未完全保留。
- en: In contrast, DPDPCA is not affected by the number of samples that each data
    owner holds, and the local descriptive statistics are aggregated to build the
    scatter matrix. Thus, the total variance is not lost. In this experiment, we measured
    the F1 score of the transformed data regarding the different number of private
    principal components. The number of principal components was determined by each
    data owner’s rank of the data. Training and testing data were projected to a lower-dimensional
    space with components from each protocol. Then we used the transformed training
    data to train an SVM classifier with the RBF kernel and test the classifier with
    unseen data. To provide ground truth, the noiseless PCA was performed over the
    training data as well. Also, the same symmetric matrix noise mechanism [10] was
    applied to DPDPCA to make a fair comparison.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 与之相反，DPDPCA不受每个数据拥有者持有的样本数量的影响，并且局部描述性统计被汇总以构建散点矩阵。因此，总方差不会丢失。在本实验中，我们测量了关于不同数量的私有主成分的转换数据的F1分数。主成分的数量由每个数据拥有者对数据的排名决定。训练和测试数据被投影到一个低维空间，其中包含来自每个协议的组件。然后我们使用转换后的训练数据训练一个具有RBF核的SVM分类器，并使用未见过的数据测试分类器。为了提供真实情况，还对训练数据执行了无噪声PCA。此外，相同的对称矩阵噪声机制[10]也被应用于DPDPCA，以进行公平的比较。
- en: Figure 3.18 shows the results. The horizontal axis specifies the number of samples
    held by each data owner, and the vertical axis shows the F1 score. You can see
    that the F1 score of DPDPCA is invariant to the number of samples for the data
    owner, and the result is compatible with the noiseless PCA, which implies that
    high utility is maintained. In contrast, the F1 score of PrivateLocalPCA is heavily
    affected by the number of samples for each data owner, and it cannot maintain
    the utility with only a few samples. Overall, for the CNAE and GISETTE datasets,
    the F1 score of DPDPCA outperforms PrivateLocalPCA under all settings.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18显示了结果。水平轴指定每个数据拥有者持有的样本数量，垂直轴显示F1分数。你可以看到DPDPCA的F1分数对数据拥有者的样本数量不变，结果与无噪声PCA兼容，这意味着保持了高效用。相比之下，PrivateLocalPCA的F1分数受到每个数据拥有者样本数量的严重影响，并且它不能仅通过少量样本来保持效用。总的来说，对于CNAE和GISETTE数据集，在所有设置下，DPDPCA的F1分数都优于PrivateLocalPCA。
- en: '![CH03_F18_Zhuang](../../OEBPS/Images/CH03_F18_Zhuang.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F18_Zhuang](../../OEBPS/Images/CH03_F18_Zhuang.png)'
- en: Figure 3.18 Principal components utility comparison between DPDPCA and PrivateLocalPCA,
    **∈** = 0.5
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18 DPDPCA与PrivateLocalPCA之间的主成分效用比较，**∈** = 0.5
- en: The tradeoff between utility and privacy
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 效用与隐私之间的权衡
- en: The other important concern is the tradeoff between utility and privacy. Let’s
    investigate the tradeoff for DPDPCA by measuring the captured variance of the
    private principal components using the Gaussian mechanism, where the standard
    deviation of the additive noise is inversely proportional to ∈. The smaller ∈
    is, the more noise is added and the more privacy is gained. The result is shown
    in figure 3.19, where the horizontal axis specifies ∈, and the vertical axis shows
    the ratio of the captured variance. In the figure you can see that the variance
    captured by the Gaussian mechanism almost maintained the same level for the given
    ∈ range. Moreover, the value of the ratio implies that the Gaussian mechanism
    captures a large proportion of the variance.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的关注点是效用与隐私之间的权衡。让我们通过使用高斯机制来测量私有主成分的捕获方差来研究DPDPCA的权衡，其中加性噪声的标准差与∈成反比。∈越小，添加的噪声越多，获得的隐私越多。结果如图3.19所示，其中水平轴指定∈，垂直轴显示捕获方差的比率。在图中，你可以看到高斯机制捕获的方差在给定的∈范围内几乎保持相同的水平。此外，比率的值表明高斯机制捕获了大部分方差。
- en: '![CH03_F19_Zhuang](../../OEBPS/Images/CH03_F19_Zhuang.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F19_Zhuang](../../OEBPS/Images/CH03_F19_Zhuang.png)'
- en: Figure 3.19 Captured variance, δ = 1/N
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.19捕获的方差，δ = 1/N
- en: In conclusion, this case study presents a highly efficient and largely scalable
    (ϵ, δ)-DP distributed PCA protocol, DPDPCA. As you can see, we considered the
    scenario where the data is horizontally partitioned and an untrustworthy data
    user wants to learn the principal components of the distributed data in a short
    time. We can think of practical applications, such as disaster management and
    emergency response. Compared to previous work, DPDPCA offers higher efficiency
    and better utility. Additionally, it can incorporate different symmetric matrix
    schemes to achieve (ϵ, δ)-DP.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，本案例研究提出了一种高度高效且可大规模扩展的 (ϵ, δ)-DP 分布式 PCA 协议，DPDPCA。正如你所见，我们考虑了数据水平分区且不可信的数据用户希望在短时间内学习分布式数据的主成分的场景。我们可以想到实际应用，例如灾害管理和应急响应。与先前的工作相比，DPDPCA
    提供了更高的效率和更好的效用。此外，它还可以结合不同的对称矩阵方案来实现 (ϵ, δ)-DP。
- en: Summary
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: DP techniques resist membership inference attacks by adding random noise to
    the input data, to iterations in the algorithm, or to algorithm output.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DP 技术通过向输入数据、算法迭代或算法输出添加随机噪声来抵抗成员推理攻击。
- en: For input perturbation-based DP approaches, noise is added to the data itself,
    and after the desired non-private computation has been performed on the noisy
    input, the output will be differentially private.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于基于输入扰动的 DP 方法，噪声被添加到数据本身，在执行所需的非隐私计算后，输出将是差分隐私的。
- en: For algorithm perturbation-based DP approaches, noise is added to the intermediate
    values in iterative ML algorithms.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于基于算法扰动的 DP 方法，噪声被添加到迭代机器学习算法的中间值中。
- en: Output perturbation-based DP approaches involve running the non-private-learning
    algorithm and adding noise to the generated model.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于输出扰动的 DP 方法涉及运行非隐私学习算法并向生成的模型添加噪声。
- en: Objective perturbation DP approaches entail adding noise to the objective function
    for learning algorithms, such as empirical risk minimization.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标扰动 DP 方法包括向学习算法（如经验风险最小化）的目标函数添加噪声。
- en: Differentially private naive Bayes classification is based on Bayes’ theorem
    and the assumption of independence between every pair of features.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 差分隐私朴素贝叶斯分类基于贝叶斯定理以及每对特征之间独立性的假设。
- en: We can adopt the objective perturbation strategy in designing differentially
    private logistic regression, where noise is added to the objective function for
    learning algorithms.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以在设计差分隐私逻辑回归时采用目标扰动策略，其中向学习算法的目标函数添加噪声。
- en: In differentially private *k*-means clustering, the Laplace noise is added to
    the intermediate centroids and cluster sizes and finally produces differentially
    private *k*-means models.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在差分隐私的 *k*-means 聚类中，拉普拉斯噪声被添加到中间质心和聚类大小中，最终产生差分隐私的 *k*-means 模型。
- en: The concept of DP can be used in many distributed ML scenarios, such as PCA,
    to design highly efficient and largely scalable (ϵ, δ)-DP distributed protocols.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DP 的概念可以应用于许多分布式机器学习场景，例如 PCA，以设计高度高效且可大规模扩展的 (ϵ, δ)-DP 分布式协议。
