- en: 20 Extending Kubernetes with custom resources and Operators
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 20 通过自定义资源和 Operators 扩展 Kubernetes
- en: At the heart of Kubernetes is a highly available database and a REST API with
    a consistent way of working with objects. When you create a Pod through the API,
    the definition is stored in the database, and a controller is notified and knows
    that it needs to allocate the Pod to a node to get it running. It’s a generic
    pattern where different controllers work on different types of objects, and it’s
    extensible, so you can add your own resource definitions and your own custom controllers
    to act on those resources. This may sound like an obscure topic, but it’s very
    common for products to extend Kubernetes to make the product itself easier to
    use. It’s also a straightforward way to customize Kubernetes to make it work better
    in your organization.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 的核心是一个高可用数据库和一个具有一致工作方式的 REST API。当你通过 API 创建一个 Pod 时，其定义会被存储在数据库中，并且控制器会收到通知，知道它需要将
    Pod 分配给一个节点以使其运行。这是一个通用的模式，其中不同的控制器处理不同类型的对象，并且它是可扩展的，因此你可以添加自己的资源定义和自定义控制器来作用于这些资源。这可能听起来像是一个晦涩难懂的话题，但许多产品都会扩展
    Kubernetes 以使产品本身更容易使用。这也是一种直接的方式来定制 Kubernetes，使其在你的组织中工作得更好。
- en: Custom resources and controllers can hide a lot of the complexity in an application,
    and in this chapter, you’ll see how to define and work with them. The definition
    part is simple, but controllers need custom code. We won’t focus on the code,
    but we’ll have some examples of customizations so you can see what they can do.
    We’ll also cover the Operator pattern in this chapter, which is a way of using
    custom resources and controllers to automate the deployment and the ongoing operational
    tasks of an application.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义资源和控制器可以隐藏应用程序中的许多复杂性，在本章中，你将了解如何定义和操作它们。定义部分很简单，但控制器需要自定义代码。我们不会专注于代码，但我们会提供一些自定义化的示例，以便你可以看到它们能做什么。我们还将在本章中介绍
    Operator 模式，这是一种使用自定义资源和控制器来自动化应用程序部署和持续操作任务的方法。
- en: 20.1 How to extend Kubernetes with custom resources
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.1 如何通过自定义资源扩展 Kubernetes
- en: 'Kubectl commands map closely to the Kubernetes REST API. When you run `kubectl`
    `get`, it makes a request to the API to get a resource or a list of resources.
    A standard set of actions are available for all resources, and you know from the
    RBAC rules covered in chapter 17 that they’re defined as verbs: `create`, `get`,
    `list`, `watch`, and `delete`. When you define a custom resource in Kubernetes,
    it receives automatic support for all of those actions in the API; Kubernetes
    clients understand custom resources, too, so you can work with them using kubectl
    just like any other object. Figure 20.1 shows how the cluster supports custom
    resources.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl 命令与 Kubernetes REST API 密切对应。当你运行 `kubectl get` 时，它会向 API 发送请求以获取资源或资源列表。所有资源都有一组标准的操作可用，并且根据第
    17 章中介绍的 RBAC 规则，它们被定义为动词：`create`（创建）、`get`（获取）、`list`（列出）、`watch`（监视）和 `delete`（删除）。当你定义
    Kubernetes 中的自定义资源时，它会自动支持 API 中的所有这些操作；Kubernetes 客户端也理解自定义资源，因此你可以像处理任何其他对象一样使用
    kubectl 与它们一起工作。图 20.1 展示了集群如何支持自定义资源。
- en: '![](../Images/20-1.jpg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/20-1.jpg)'
- en: Figure 20.1 Kubernetes is extensible with custom resources, which work just
    like standard resources.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 20.1 Kubernetes 通过自定义资源进行扩展，它们的工作方式与标准资源相同。](../Images/20-1.jpg)'
- en: You define standard Kubernetes objects in YAML by specifying the kind of resource
    and all the fields in the spec—Pod specs have a container list, and container
    specs have an image name and a set of ports. Kubernetes stores those fields in
    a schema, so it knows the structure of the resource and can validate new objects.
    This is where the API version field comes in—version 1 of the HorizontalPodAutoscaler
    resource has a different structure from v2beta2\. Custom resources have a known
    structure, too, and you create your own schema in a CustomResourceDefinition (CRD)
    object. Listing 20.1 shows a simple CRD for recording to-do items as Kubernetes
    objects.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你通过在 YAML 中指定资源的类型和在规范中所有字段来定义标准的 Kubernetes 对象——Pod 规范有一个容器列表，容器规范有一个镜像名称和一组端口。Kubernetes
    将这些字段存储在模式中，因此它知道资源的结构并可以验证新对象。这就是 API 版本字段发挥作用的地方——HorizontalPodAutoscaler 资源的第
    1 版结构与 v2beta2 不同。自定义资源也有一个已知的结构，你可以在自定义资源定义（CRD）对象中创建自己的模式。列表 20.1 展示了一个简单的 CRD，用于将待办事项记录为
    Kubernetes 对象。
- en: Listing 20.1 todo-crd.yaml, a CRD for storing to-do items in Kubernetes
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.1 todo-crd.yaml，一个用于在 Kubernetes 中存储待办事项的 CRD
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The CRD structure itself is verbose, and the schema section is particularly
    awkward to read, but it uses the standard JSONSchema project to define the structure
    of the resource, and your definitions can be as complex or as simple as you need.
    The CRD in listing 20.1 is hard on the eyes but makes for a simple custom resource.
    Listing 20.2 shows a ToDo item that uses this structure.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: CRD 结构本身很冗长，尤其是模式部分读起来特别别扭，但它使用标准的 JSONSchema 项目来定义资源的结构，你的定义可以像你需要的那样复杂或简单。列表
    20.1 中的 CRD 在视觉上可能有些刺眼，但它构成了一个简单的自定义资源。列表 20.2 展示了一个使用这种结构的 ToDo 项目。
- en: Listing 20.2 todo-ch20.yaml, a ToDo custom resource
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.2 todo-ch20.yaml，一个 ToDo 自定义资源
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This looks just like a normal Kubernetes YAML, which is the whole point of CRDs—to
    store your own resources in Kubernetes and make them feel like standard objects.
    The API version identifies that as a custom resource from this chapter of the
    book, and it’s version 1\. The metadata is standard metadata, so it can include
    labels and annotations, and the spec is the custom structure defined in the CRD.
    That’s enough YAML now—let’s put this into practice and use Kubernetes as our
    to-do list app.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来就像一个正常的 Kubernetes YAML，这正是 CRD 的全部意义——在 Kubernetes 中存储你自己的资源，并使它们感觉像标准对象。API
    版本标识它为本书这一章节中的自定义资源，版本为 1。元数据是标准的元数据，因此它可以包括标签和注解，而 spec 是在 CRD 中定义的自定义结构。现在我们已经有了足够的
    YAML，让我们将其付诸实践，并使用 Kubernetes 作为我们的待办事项列表应用程序。
- en: Try it now Deploy a custom resource definition and some resources to see how
    you work with them using kubectl.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 部署一个自定义资源定义和一些资源，看看你是如何使用 kubectl 与它们一起工作的。
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can see in figure 20.2 that you work with custom resources just like any
    other resource. Once the CRD is deployed, the API now supports the ToDo object,
    and you can create items by applying YAML. Kubectl is the tool for managing custom
    resources, and now the `get`, `describe`, and `delete` commands work in the same
    way for ToDo objects as for Pods.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图 20.2 中看到，你与自定义资源的工作方式就像与其他任何资源一样。一旦 CRD 部署完成，API 现在支持 ToDo 对象，你可以通过应用
    YAML 来创建项目。Kubectl 是管理自定义资源的工具，现在 `get`、`describe` 和 `delete` 命令对 ToDo 对象和 Pods
    的工作方式相同。
- en: '![](../Images/20-2.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/20-2.jpg)'
- en: Figure 20.2 CRDs and custom resources are described in YAML and deployed with
    kubectl.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.2 中的 CRDs 和自定义资源是用 YAML 描述并通过 kubectl 部署的。
- en: The CRD specification is rich and lets you build lots of logic around your resources,
    including validation rules, subresources, and multiple versions. We won’t get
    into that level of detail, but you can be confident that custom resources are
    a mature feature in Kubernetes with all the functionality you need to manage an
    evolving set of object definitions. The purpose of CRDs is to provide a simplified
    user experience, and we can make a small change to the ToDo CRD to make it more
    usable.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: CRD 规范丰富，允许你在资源周围构建大量逻辑，包括验证规则、子资源和多个版本。我们不会深入到那个层面的细节，但你可以确信自定义资源是 Kubernetes
    中的一个成熟特性，它提供了你管理不断发展的对象定义集所需的所有功能。CRD 的目的是提供简化的用户体验，我们可以对 ToDo CRD 进行一些小的修改，使其更易于使用。
- en: Try it now An update to the CRD spec makes the kubectl output more useful. This
    update doesn’t affect the custom resources; it just adds to the columns that are
    printed.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 CRD 规范的更新使得 kubectl 输出更加有用。这次更新不会影响自定义资源；它只是增加了打印的列。
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This exercise updated the CRD with additional printer columns, so the API returns
    extra information in the `get` request. You can see in figure 20.3 that this is
    now a fully featured to-do list application. It even lets you delete items, so
    it’s better than the to-do web app we’ve been running in this book.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习通过添加额外的打印列更新了 CRD，因此 API 在 `get` 请求中返回了额外的信息。你可以在图 20.3 中看到，这现在是一个功能齐全的待办事项列表应用程序。它甚至允许你删除项目，所以它比我们在本书中运行的待办事项
    Web 应用程序更好。
- en: '![](../Images/20-3.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/20-3.jpg)'
- en: Figure 20.3 A fully functional to-do app powered by Kubernetes with no custom
    code!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.3 是一个由 Kubernetes 驱动的完全功能的待办事项应用程序，没有任何自定义代码！
- en: This is fine for a simple demo, and we could get distracted writing a custom
    controller that watches these resources, adds items to your Google calendar, and
    sends email reminders when they’re due, but we’re not going to do that. It’s not
    a good use of custom resources because the objects we’re storing and the actions
    they trigger have nothing to do with Kubernetes; we’re not integrating with other
    objects or extending what the cluster can do—we’re just using Kubernetes as a
    massively overspecified content management system. We can do better than that,
    and we’ll start by clearing up the ToDo resources.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于简单的演示来说是可以的，我们可能会分心编写一个自定义控制器，该控制器监视这些资源，将项目添加到你的Google日历中，并在到期时发送电子邮件提醒，但我们不会这样做。这不是自定义资源的良好用途，因为我们存储的对象和它们触发的操作与Kubernetes无关；我们不是与其他对象集成或扩展集群的功能——我们只是在将Kubernetes作为一个过度指定的内容管理系统使用。我们可以做得更好，我们将从清理ToDo资源开始。
- en: Try it now Remove the to-do CRD, and confirm the custom resources are deleted,
    too.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看：移除待办CRD，并确认自定义资源也被删除了。
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can see in this exercise and in figure 20.4 that custom resources can’t
    exist without a CRD—Kubernetes won’t store any unknown objects. Deleting the CRD
    deletes all of its resources, so if you make use of custom resources, you need
    to make sure your RBAC permissions around the CRD itself are tight.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个练习和图20.4中看到，没有CRD（Custom Resource Definition）自定义资源就无法存在——Kubernetes不会存储任何未知对象。删除CRD会删除所有其资源，所以如果你使用了自定义资源，你需要确保围绕CRD本身的RBAC（Role-Based
    Access Control）权限是严格的。
- en: '![](../Images/20-4.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/20-4.jpg)'
- en: Figure 20.4 Custom resources are removed when the CRD that defines them is removed.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.4 当定义自定义资源的CRD被移除时，自定义资源也会被移除。
- en: We’ll go on to add a different CRD and pair it with a custom controller to add
    a user authentication system to Kubernetes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续添加不同的CRD，并将其与自定义控制器配对，以向Kubernetes添加用户认证系统。
- en: 20.2 Triggering workflows with custom controllers
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.2 使用自定义控制器触发工作流程
- en: You know from chapter 17 that production Kubernetes clusters are usually integrated
    with an external identity provider to authenticate users. Smaller organizations
    often use Service Accounts as end user accounts, which means you don’t need an
    external system. You do, however, need to manage namespaces for groups and deal
    with creating accounts and tokens. It’s a situation where Kubernetes has all the
    pieces for you, but you’ll need to do a fair amount of work to put them together.
    That’s exactly when you should think about custom resources and custom controllers.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你从第17章知道，生产Kubernetes集群通常与外部身份提供者集成以认证用户。较小的组织通常使用服务账户作为最终用户账户，这意味着你不需要外部系统。然而，你需要管理组别的命名空间，并处理创建账户和令牌。这是一个Kubernetes为你提供所有部件的情况，但你将需要做相当多的工作来将它们组合起来。这正是你应该考虑自定义资源和自定义控制器的时候。
- en: 'The custom resource here is a *user*, and the main workflows are adding and
    deleting users. A simple user CRD needs to store just a name and a group, maybe
    with contact details, too. You can add and remove users with kubectl. When that
    happens, the workflow is processed by a custom controller. The controller is just
    an application that runs in a Pod and connects to the Kubernetes API. It watches
    for changes to user objects, and then it creates or deletes the necessary resources:
    namespaces, service accounts, and tokens. Figure 20.5 shows the add workflow,
    and the delete workflow is effectively the reverse.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的自定义资源是*用户*，主要的工作流程是添加和删除用户。一个简单的用户CRD只需要存储一个名称和一个组，也许还有联系详情。你可以使用kubectl添加和删除用户。当这样做时，工作流程将由自定义控制器处理。控制器只是一个运行在Pod中的应用程序，并连接到Kubernetes
    API。它监视用户对象的变化，然后创建或删除必要的资源：命名空间、服务账户和令牌。图20.5显示了添加工作流程，而删除工作流程实际上是相反的。
- en: '![](../Images/20-5.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/20-5.jpg)'
- en: Figure 20.5 Adding a user in the custom authentication system creates all of
    the Kubernetes resources.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.5 在自定义认证系统中添加用户会创建所有的Kubernetes资源。
- en: We’ll start by deploying the user CRD and some users. The CRD is difficult to
    read because of the schema, but there’s nothing new in it, so we’ll skip the listing
    (it’s the file `user-crd.yaml`, if you want to walk through it). The user resources
    themselves are simple. Listing 20.3 shows a user in the SRE team.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先部署用户CRD和一些用户。由于schema（模式），CRD难以阅读，但其中没有新的内容，所以我们将跳过列表（如果你想要查看，它是文件`user-crd.yaml`）。用户资源本身很简单。列表20.3显示了SRE团队中的一个用户。
- en: Listing 20.3 user-crd.yaml, spec for a user resource
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.3 user-crd.yaml，用户资源的spec
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You need to be aware that CRDs take a few seconds to register in the API server,
    so you can’t usually deploy a folder of CRDs and custom resources in one go because
    the CRD often isn’t ready in time. You need to deploy the CRD first, and then
    you can deploy the resources. We’ll do that now with the SRE user and a test user.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要意识到 CRD 需要几秒钟的时间在 API 服务器上注册，所以你通常不能一次性部署一个 CRD 和自定义资源的文件夹，因为 CRD 往往没有及时准备好。你需要先部署
    CRD，然后才能部署资源。我们现在将使用 SRE 用户和一个测试用户来做这件事。
- en: Try it now Create the CRD for users and some user resources.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 创建用户的 CRD 和一些用户资源。
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: My output in figure 20.6 shows the user experience works well. The CRD needs
    to be deployed only once, and then you can add as many users as you like using
    simple YAML like that in listing 20.3\. Now custom user objects are stored in
    Kubernetes, but there’s no controller running, so nothing will happen yet.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.6 的输出显示用户体验良好。CRD 只需部署一次，然后你可以使用像列表 20.3 中的简单 YAML 那样添加尽可能多的用户。现在自定义用户对象存储在
    Kubernetes 中，但没有控制器运行，所以目前还没有任何事情发生。
- en: '![](../Images/20-6.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/20-6.jpg)'
- en: Figure 20.6 Creating the CRD and some users—this doesn’t trigger anything without
    a controller.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.6 创建 CRD 和一些用户——没有控制器，这不会触发任何事情。
- en: Custom controllers are usually written in Go. A few packages take care of the
    boilerplate wiring up that you need to do. Kubernetes API clients exist in all
    the major languages, though, and my user controller is written in .NET. I don’t
    want to throw a pile of source code at you, but you should realize a couple of
    things about building custom controllers. Listing 20.4 is some C# code that is
    part of the add-user workflow (the full file is in the source code for this chapter).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义控制器通常用 Go 编写。一些包负责处理你需要做的样板连接。不过，Kubernetes API 客户端存在于所有主要语言中，我的用户控制器是用 .NET
    编写的。我不想给你一堆源代码，但你应该意识到构建自定义控制器的一些事情。列表 20.4 是一些 C# 代码，它是添加用户工作流程的一部分（完整文件在本章的源代码中）。
- en: Listing 20.4 UserAddedHandler.cs, using the Kubernetes API with a client library
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.4 UserAddedHandler.cs，使用客户端库调用 Kubernetes API
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The first thing you’ll notice is that working with the Kubernetes API feels
    natural because all of the operations are effectively the same thing you do in
    kubectl but with different syntax, so writing a controller is not very difficult.
    The second thing is that you typically build Kubernetes resources in code, so
    you need to translate the YAML in your head into a series of objects—so writing
    a controller is cumbersome. Luckily, you’ve got me to do it for you, and when
    you deploy the user controller, it will be notified about the new users straight
    away, and it will run the add-user workflow.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先会注意到，使用 Kubernetes API 感觉很自然，因为所有的操作实际上都是你在 kubectl 中所做的相同事情，只是语法不同，所以编写控制器并不困难。第二点是，你通常在代码中构建
    Kubernetes 资源，所以你需要将你心中的 YAML 转换为一系列对象——因此编写控制器比较繁琐。幸运的是，我有我来为你做这件事，当你部署用户控制器时，它会立即通知新用户，并运行添加用户的工作流程。
- en: Try it now Deploy the custom controller, and verify that the add-user process
    is triggered and creates all the authentication resources.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 部署自定义控制器，并验证添加用户过程是否被触发并创建了所有认证资源。
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You see in that exercise that the controller automatically performs all the
    actions we did manually in section 17.3—creating a namespace for the group, creating
    a service account in the namespace, and requesting a new token. Actually, it does
    more than that, because it checks if any of those resources exist first and creates
    them only if it needs to. Figure 20.7 shows the outcome is a service account,
    which can be secured by applying RBAC rules to the group (which is the namespace),
    and a token, which can be distributed to the user for them to store in their kubectl
    context.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到在这个练习中，控制器自动执行了我们手动在 17.3 节中做的所有操作——为组创建命名空间、在命名空间中创建服务账户，并请求新的令牌。实际上，它做的不仅仅是这些，因为它首先检查这些资源是否存在，只有在需要时才创建它们。图
    20.7 显示结果是服务账户，可以通过对组（即命名空间）应用 RBAC 规则来保护，以及令牌，可以分发给用户，让他们存储在他们的 kubectl 上下文中。
- en: '![](../Images/20-7.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/20-7.jpg)'
- en: Figure 20.7 The controller automates the onboarding process, except for distributing
    credentials.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.7 控制器自动化了入职流程，除了分发凭证。
- en: Please don’t use this controller as your production authentication system; it’s
    just a quick sample of how CRDs work with custom controllers to extend your Kubernetes
    experience. The code doesn’t deal with updates to objects, and the design allows
    only one group per user, but you can see that the basis for managing authentication
    within Kubernetes is all there. You could have a source code repository with all
    of your user YAMLs and group RBAC rules and deploy that as part of provisioning
    any new cluster.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 请不要将此控制器用作您的生产认证系统；它只是快速展示如何使用自定义控制器与CRDs一起扩展您的Kubernetes体验。代码不处理对象的更新，设计允许每个用户只有一个组，但您可以看到管理Kubernetes内认证的基础都在那里。您可以将所有用户YAML和组RBAC规则存储在一个源代码库中，并将其作为部署任何新集群的一部分。
- en: The basic role of any controller is to implement a control loop, constantly
    watching for changes to objects and performing whatever tasks are needed to get
    the actual state of the system into the desired state specified in the object.
    They do that by watching resources for changes—just like using the `watch` parameter
    in kubectl. The `watch` is an endless loop, and it is notified when objects are
    created, updated, or deleted. The user controller added the users it found when
    it started up, and it’s still running in the background, waiting for you to add
    another user.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 任何控制器的基本作用是实现控制循环，持续监视对象的变化，并执行将系统的实际状态转换为对象中指定的所需状态的任何任务。它们通过监视资源的变化来实现这一点——就像使用kubectl中的`watch`参数一样。`watch`是一个无限循环，当对象被创建、更新或删除时，它会收到通知。用户控制器在启动时添加了它找到的用户，并且它仍在后台运行，等待您添加另一个用户。
- en: Try it now The controller is watching for new users. Create another user resource,
    and confirm that the controller does its stuff.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下。控制器正在监视新用户。创建另一个用户资源，并确认控制器执行了其操作。
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'My output in figure 20.8 shows that the controller is behaving as it should;
    the desired state is that the user should be created as a service account with
    a namespace for the group. The namespace already exists, so the controller doesn’t
    need to do anything there; it just creates the service account and token. Custom
    controllers need to work on the same principle as standard controllers: they get
    to the desired state no matter what the initial state is.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我在图20.8中的输出显示，控制器表现正常；期望状态是用户应该作为一个服务账户被创建，并为组创建一个命名空间。命名空间已经存在，因此控制器在那里不需要做任何事情；它只是创建服务账户和令牌。自定义控制器需要按照与标准控制器相同的原则工作：无论初始状态如何，它们都能达到所需状态。
- en: '![](../Images/20-8.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图20-8](../Images/20-8.jpg)'
- en: Figure 20.8 The declarative desired-state approach should be used in custom
    controllers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.8 在自定义控制器中应使用声明性期望状态方法。
- en: Custom controllers also need to own the cleanup logic, because there’s a disconnect
    between creating a custom resource and seeing a whole bunch of extra resources
    being created. This is one concern with extending Kubernetes—your controller code
    needs to be solid to make sure any failures don’t leave objects lying around that
    the cluster admin isn’t expecting. That’s particularly important for any sensitive
    data stored in Secrets, like the tokens created for users.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义控制器也需要拥有清理逻辑，因为创建自定义资源和看到大量额外资源被创建之间存在脱节。这是扩展Kubernetes的一个问题——您的控制器代码需要稳固，以确保任何故障都不会留下集群管理员不期望的对象。这对于存储在Secrets中的敏感数据尤其重要，例如为用户创建的令牌。
- en: Try it now Delete the test user. Now there are no users in the group, so the
    namespace should be removed, too.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下。删除测试用户。现在组中没有用户了，因此命名空间也应该被删除。
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You can see in figure 20.9 that the controller doesn’t explicitly delete the
    Secret, but this type of Secret is deleted by Kubernetes anyway when the service
    account is deleted. The controller does check to see if there are any more users
    in the group, and if not, it deletes the namespace. Woe betide you if you added
    any other resources in that namespace—they’ll be gone now.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在图20.9中看到，控制器没有明确删除Secret，但此类型的Secret在服务账户被删除时，Kubernetes会自动删除。控制器会检查组中是否还有其他用户，如果没有，它会删除命名空间。如果您在那个命名空间中添加了任何其他资源，那么它们现在都将消失。
- en: '![](../Images/20-9.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图20-9](../Images/20-9.jpg)'
- en: Figure 20.9 Controllers are notified when objects are removed so they can clean
    up the resources they created.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当对象被移除时，控制器会收到通知，以便它们可以清理它们创建的资源。
- en: Custom resources are a powerful way to extend Kubernetes, especially for use
    cases like this where you want to provide a higher-level abstraction over stock
    Kubernetes objects. But those objects are just normal resources in the cluster,
    and your controller code needs to allow for admins coming along and deleting them,
    without realizing they’re managed by a controller. The user controller should
    also watch Secrets, service accounts, and namespaces to recreate anything that
    is deleted outside of the controller process.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义资源是扩展Kubernetes的强大方式，特别是对于像这样的用例，你希望提供比标准Kubernetes对象更高层次的概念抽象。但那些对象只是集群中的普通资源，你的控制器代码需要允许管理员在不知道它们由控制器管理的情况下删除它们。用户控制器还应监视机密、服务帐户和命名空间，以重新创建控制器过程外删除的任何内容。
- en: More sophisticated controllers might deploy their own RBAC rules to limit interference
    and would support running across multiple Pods for high availability. If you want
    to explore a production-grade example of CRDs and custom controllers, the cert-manager
    project ([https://cert-manager.io](https://cert-manager.io)) is a great example.
    It’s a CNCF project that adds TLS certificate management to Kubernetes and can
    request certificates and apply them to your web apps. The next level of sophistication
    comes with the Operator pattern.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的控制器可能会部署自己的RBAC规则以限制干扰，并支持跨多个Pod运行以实现高可用性。如果你想探索CRDs和自定义控制器的生产级示例，cert-manager项目([https://cert-manager.io](https://cert-manager.io))是一个很好的例子。它是一个CNCF项目，为Kubernetes添加了TLS证书管理功能，可以请求证书并将它们应用到你的Web应用程序中。更高级的复杂性来自于操作符模式。
- en: 20.3 Using Operators to manage third-party components
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.3 使用操作符管理第三方组件
- en: Operators use custom resources and controllers to provide full life cycle management
    for an application. They’re used for complex apps where a lot of operational tasks
    are beyond the standard Kubernetes feature set. Stateful apps are a good example—if
    you decide to run a database in Kubernetes, then upgrading the database server
    might mean putting the database into read-only mode and making a backup before
    the upgrade.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 操作符使用自定义资源和控制器来提供应用程序的全生命周期管理。它们用于需要大量操作任务且超出标准Kubernetes功能集的复杂应用程序。有状态应用程序是一个很好的例子——如果你决定在Kubernetes中运行数据库，那么升级数据库服务器可能意味着将数据库置于只读模式并在升级前进行备份。
- en: You can’t express requirements like that with standard Kubernetes resources;
    you can achieve something like it with Helm install hooks, but often the logic
    is quite involved, and you need more control. The goal of the Operator is to implement
    all of those operational requirements with controllers and custom resources, abstracting
    the complexity with simple resources like a database object and a backup object.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你不能用标准Kubernetes资源表达这样的要求；你可以通过Helm安装钩子实现类似的功能，但通常逻辑相当复杂，你需要更多的控制。操作符的目标是使用控制器和自定义资源实现所有这些操作要求，通过简单的资源如数据库对象和备份对象来抽象复杂性。
- en: 'Third-party components that your app relies on are much easier to work with
    if they can be deployed with an Operator, because it gives you an as-a-service
    experience, where you can focus on your app and leave the dependencies to manage
    themselves. In this section, we’ll deploy a modified version of the to-do list
    web app, using Operators to manage the dependencies: a database and a message
    queue for asynchronous communication between components.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果第三方组件可以通过操作符部署，那么与你的应用程序依赖的组件一起工作会容易得多，因为它为你提供了一个as-a-service体验，你可以专注于你的应用程序，而将依赖项的管理留给它们自己。在本节中，我们将部署待办事项Web应用程序的修改版本，使用操作符来管理依赖项：一个数据库和一个消息队列，用于组件之间的异步通信。
- en: Try it now This version of the to-do app uses a message queue server called
    NATS. The NATS team publish an Operator that runs highly available clusters of
    queue servers.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试这个待办事项应用版本使用了一个名为NATS的消息队列服务器。NATS团队发布了一个操作符，该操作符运行高度可用的队列服务器集群。
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: NATS is a message queue that acts as a go-between for application components,
    so they communicate by passing messages instead of connecting to each other directly.
    It’s a powerful and very capable technology (another CNCF project), but in a production
    environment, it needs to be set up carefully for high availability to make sure
    messages don’t get lost. No one knows how to do that better than the NATS team,
    and they provide the Operator you’ve just deployed. As you see in figure 20.10,
    it adds a CRD for a NatsCluster object, which you can use to deploy a distributed
    message queue.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: NATS是一个充当应用程序组件之间的中介的消息队列，因此它们通过传递消息而不是直接连接来通信。它是一个强大且非常能够的技术（另一个CNCF项目），但在生产环境中，为了确保消息不会丢失，需要仔细设置以实现高可用性。没有比NATS团队更好的团队来做这件事了，他们提供了你刚刚部署的操作员。如图20.10所示，它为NatsCluster对象添加了一个CRD，你可以使用它来部署分布式消息队列。
- en: '![](../Images/20-10.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/20-10.jpg)'
- en: Figure 20.10 Operators should be simple to deploy and create all the resources
    they need.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.10 操作员应该简单易部署，并创建它们需要的所有资源。
- en: The updated to-do app uses a message queue to improve performance and scalability.
    When users save messages in the new version, the web app sends a message to the
    queue and then returns to the user. Another component listens for those messages
    and adds the item to the database. You can scale up to hundreds of web Pods without
    needing to scale up the database, because the queue acts as a buffer, smoothing
    out any peaks in traffic. The queue becomes a critical component in the app, and
    listing 20.5 shows just how simple it is to deploy a production-grade queue using
    the NATS Operator.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的待办事项应用使用消息队列来提高性能和可伸缩性。当用户在新版本中保存消息时，Web应用会将消息发送到队列，然后返回给用户。另一个组件监听这些消息并将项目添加到数据库中。你可以扩展到数百个Web
    Pod，而无需扩展数据库，因为队列充当缓冲区，平滑任何流量峰值。队列成为应用中的关键组件，列表20.5展示了使用NATS操作员部署生产级队列是多么简单。
- en: Listing 20.5 todo-list-queue.yaml, a custom resource for a message queue
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.5 todo-list-queue.yaml，一个消息队列的自定义资源
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The NatsCluster resource contains two fields: the number of Pods to run as
    queue servers you want in your highly available queue cluster and the version
    of NATS to use. When you deploy it, the Operator creates a Service for apps to
    use the queue, a Service for the instances of NATS to coordinate with each other,
    and a set of Pods, each running NATS and configured with a Secret to run as a
    highly available, distributed queue.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: NatsCluster资源包含两个字段：你希望在高度可用的队列集群中运行的队列服务器Pod数量以及要使用的NATS版本。当你部署它时，操作员会为应用使用队列创建一个服务，为NATS实例创建一个服务以相互协调，以及一组Pod，每个Pod运行NATS并配置了一个Secret以作为高度可用、分布式的队列运行。
- en: Try it now Create the NATS cluster resource, and confirm that the Operator creates
    all the queue resources.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 创建NATS集群资源，并确认操作员是否创建了所有队列资源。
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Figure 20.11 shows my NATS cluster is already up and running. The container
    image is just a few megabytes in size, so Pods will start quickly, even on nodes
    that need to pull the image. If you describe one of the Pods, you’ll see the spec
    uses some of the best practices you’ve learned from this book, like container
    probes and Pod priority. But the Pods are not managed by a Deployment or StatefulSet;
    the NATS Operator is the Pod controller, which means it can use its own approach
    for maintaining availability.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.11显示我的NATS集群已经启动并运行。容器镜像的大小只有几兆字节，所以Pod会很快启动，即使在需要拉取镜像的节点上也是如此。如果你描述一个Pod，你会看到规范使用了你从这本书中学到的一些最佳实践，比如容器探测和Pod优先级。但Pod不是由Deployment或StatefulSet管理的；NATS操作员是Pod控制器，这意味着它可以使用自己的方法来维护可用性。
- en: '![](../Images/20-11.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/20-11.jpg)'
- en: Figure 20.11 Two lines of YAML in a custom resource gets you a distributed message
    queue.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.11 在自定义资源中使用两行YAML，即可获得一个分布式消息队列。
- en: The Operator pattern is a loose definition; there’s no Operator object in Kubernetes,
    and it’s up to the project team to decide how to design, build, and distribute
    their Operator. The NATS Operator is deployed from YAML manifests, which are released
    on GitHub; other projects might use Helm or a tool called the Operator Lifecycle
    Manager (OLM). OLM adds some consistency around Operators with a catalog to publish
    and distribute them, but it’s one of those technologies at the fringe of the Kubernetes
    ecosystem that hasn’t taken off so far.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Operator模式是一个宽泛的定义；Kubernetes中没有Operator对象，设计、构建和分发它们的决定权在项目团队。NATS Operator是从GitHub上发布的YAML清单部署的；其他项目可能使用Helm或称为Operator
    Lifecycle Manager（OLM）的工具。OLM通过一个目录来发布和分发Operator，增加了一些一致性，但它是在Kubernetes生态系统边缘的技术之一，到目前为止还没有得到广泛的应用。
- en: You can visit the OperatorHub site ([https://operatorhub.io](https://operatorhub.io))
    to see the kind of projects available through OLM. A few of them are maintained
    by product teams; others are published by third parties or individuals. At the
    time of writing, three Operators exist for the Postgres database-none of them
    backed by the Postgres project—and they vary wildly in capabilities and ease of
    use. There are no Operators for MySQL, and although there is one for MariaDB (a
    fork of MySQL), it’s maintained by one person on GitHub—that might not be the
    kind of support structure you’re happy with for a core component.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以访问OperatorHub网站([https://operatorhub.io](https://operatorhub.io))，查看通过OLM可用的项目类型。其中一些由产品团队维护；其他由第三方或个人发布。在撰写本文时，有三个Postgres数据库的Operator——它们都没有得到Postgres项目的支持——它们在功能和易用性方面差异很大。没有MySQL的Operator，尽管有一个MariaDB（MySQL的分支）的Operator，但它由GitHub上的一位个人维护——这可能不是您对核心组件所期望的支持结构。
- en: This is not to say that Operators are not a viable technology; it’s just that
    the pattern isn’t restricted to OLM. If you’re looking for an Operator for a product,
    you need to search more widely than the OperatorHub site and investigate the maturity
    of the options. The to-do list app can use MySQL as a data store—a very good MySQL
    Operator is available from the team at Presslabs who operate MySQL at scale in
    Kubernetes for their WordPress platform. The Operator is easy to use, well documented,
    and well maintained, and it’s simple to install with Helm.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是说Operator不是一种可行的技术；只是这种模式并不局限于OLM。如果您正在寻找某个产品的Operator，您需要比OperatorHub网站更广泛地搜索，并调查选项的成熟度。待办事项应用可以使用MySQL作为数据存储——来自Presslabs团队的一个非常好的MySQL
    Operator可用于在Kubernetes中大规模运行MySQL，为他们的WordPress平台提供服务。该Operator易于使用，文档齐全，维护良好，并且使用Helm安装简单。
- en: Try it now Deploy the MySQL Operator using Helm, which can deploy and manage
    replicated MySQL databases in the cluster.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 使用Helm部署MySQL Operator，它可以在集群中部署和管理复制的MySQL数据库。
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The MySQL Operator gives you a database-as-a-service experience: the Helm release
    creates CRDs for database and database backup objects and an Operator that runs
    controllers for those objects. My output in figure 20.12 is snipped, but the Helm
    release notes also show you how to create a database—you just need a Secret for
    the MySQL password and a MysqlCluster object.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: MySQL Operator为您提供了数据库即服务（DBaaS）的体验：Helm发布创建了数据库和数据库备份对象的CRD以及运行这些对象的Operator。我在图20.12中的输出被截断，但Helm发布说明还显示了如何创建数据库——您只需要一个MySQL密码的Secret和一个MysqlCluster对象。
- en: '![](../Images/20-12.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20-12.jpg)'
- en: Figure 20.12 Complex software is easy to deploy and manage if you find a good
    Operator.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您找到一个好的Operator，复杂软件的部署和管理就变得容易了。
- en: You can now deploy highly available databases using a simple resource spec.
    Listing 20.6 shows the manifest for the to-do list database, and it also illustrates
    some of the limitations of custom resources. The CRD schema lets you set MySQL
    configuration and also customize the Pod definition the Operator generates for
    the database server, so you can set resource requests and limits, affinity rules,
    and a priority class. These Kubernetes details leak into the database object spec,
    so it’s not purely a description of the database you need, but it’s far simpler
    than the replicated Postgres database we set up from scratch in chapter 8.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以使用简单的资源规范部署高度可用的数据库。列表20.6显示了待办事项数据库的清单，同时也说明了自定义资源的某些局限性。CRD模式允许您设置MySQL配置，并自定义Operator为数据库服务器生成的Pod定义，因此您可以设置资源请求和限制、亲和规则和优先级类别。这些Kubernetes细节会泄露到数据库对象规范中，所以它不仅仅是您需要的数据库的描述，但它比我们在第8章从头开始设置的复制的Postgres数据库要简单得多。
- en: Listing 20.6 todo-list-db.yaml, a replicated MySQL database using the Operator
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.6 todo-list-db.yaml，一个使用操作员的复制的MySQL数据库
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: When you deploy the MysqlCluster object, the Operator creates a StatefulSet
    to run a replicated MySQL database and a set of Services for consumers to connect
    to the database. There are separate Services for the cluster as a whole and for
    the manager and replica nodes, so you can choose how you want your client applications
    to connect.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当你部署MysqlCluster对象时，操作员创建一个StatefulSet来运行一个复制的MySQL数据库，并为消费者连接到数据库创建一组服务。有针对整个集群以及管理器和副本节点的一组单独的服务，所以你可以选择你的客户端应用程序如何连接。
- en: Try it now Deploy the database, and confirm the expected resources are created
    by the Operator.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试它 部署数据库，并确认操作员创建了预期的资源。
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You’ll see when you look at the StatefulSet that the Pod runs a MySQL container
    and a set of sidecar containers, including the Prometheus exporter for MySQL,
    shown in figure 20.13\. This is one of the big advantages of Operators: they model
    applications with best practices so you don’t need to dig into the finer details
    yourself. If you look at the spec of one of the Pods, you’ll see it has the standard
    Prometheus annotations we used in chapter 14, so if you have Prometheus running
    in your cluster, the Operator will pick up the new database Pods without any extra
    configuration, and you can add MySQL metrics to your dashboard.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当你查看StatefulSet时，你会看到Pod运行一个MySQL容器和一组边车容器，包括MySQL的Prometheus导出器，如图20.13所示。这是操作员的一个大优点：它们使用最佳实践来建模应用程序，这样你就不需要自己深入研究细节。如果你查看Pod的规范，你会看到它有我们在第14章中使用的标准Prometheus注释，所以如果你在集群中运行Prometheus，操作员将自动检测新的数据库Pod，无需任何额外配置，你还可以将MySQL度量添加到你的仪表板中。
- en: '![](../Images/20-13.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图20-13](../Images/20-13.jpg)'
- en: Figure 20.13 The Operator sets up an opinionated MySQL database with common
    best practices.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.13 操作员使用常见的最佳实践设置了一个有偏见的MySQL数据库。
- en: Now we have a production-grade database and message queue running, defined in
    just 20 lines of YAML. We could standardize on NATS and MySQL for all our apps,
    and the Operators would take care of multiple databases and queues. Operators
    are usually clusterwide, so you can still isolate application workloads in different
    namespaces. That’s all the dependencies for the new to-do app, so we can deploy
    the rest of the components, the website, and the message handler, which saves
    data to the database.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个生产级别的数据库和消息队列正在运行，仅用20行YAML定义。我们可以将NATS和MySQL标准化为所有我们的应用程序，操作员将负责多个数据库和队列。操作员通常是集群范围的，所以你仍然可以在不同的命名空间中隔离应用程序工作负载。这就是新待办事项应用程序的所有依赖项，因此我们可以部署其余的组件，即网站和消息处理器，它将数据保存到数据库中。
- en: Try it now Deploy the application components, a website, and a message handler,
    which use both the queue and the database.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试它 部署应用程序组件，一个网站和一个消息处理器，它们都使用队列和数据库。
- en: '[PRE17]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In this exercise, you’ll see the app works just the way it always has, and we’ve
    significantly reduced the amount of YAML we need to maintain, compared to our
    custom Postgres version. My output, shown in figure 20.14, actually hides the
    fact that the app doesn’t quite work the way it did—now the to-do data is saved
    in a separate process. You’ll see there’s a lag between adding an item and seeing
    it in the list, so you’ll need to refresh. Welcome to *eventual consistency*,
    which is a side effect of the new messaging architecture; it has nothing to do
    with Operators, so I’ll leave you to research it, if it’s a new concept for you.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，你会看到应用程序工作得就像它一直以来的那样，与我们定制的Postgres版本相比，我们显著减少了需要维护的YAML数量。我的输出，如图20.14所示，实际上隐藏了这样一个事实：应用程序并不完全按原来的方式工作——现在待办数据被保存在一个单独的进程中。你会发现添加一个条目和看到它在列表中之间有一个延迟，所以你需要刷新。欢迎来到*最终一致性*，这是新消息架构的副作用；这与操作员无关，所以如果你对此概念是新手，我会让你去研究。
- en: '![](../Images/20-14.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图20-14](../Images/20-14.jpg)'
- en: Figure 20.14 Message queues and databases are critical components, and the Operators
    run them with high availability.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.14 消息队列和数据库是关键组件，操作员以高可用性运行它们。
- en: It’s not just ease of deployment and high availability we get from the Operators;
    they’ll also take care of safe upgrades to the core components, and the MySQL
    database can be backed up to cloud storage by creating a MysqlBackup object. We
    won’t go any further with that because we’re not really running a production-grade
    to-do list application. In fact, the setup we have running now is probably consuming
    quite a lot of resources on your lab machine, so we’ll clear it down before we
    move on.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从Operator中获得的不只是部署的便捷性和高可用性；它们还会负责核心组件的安全升级，并且可以通过创建MysqlBackup对象将MySQL数据库备份到云存储。我们不会进一步探讨这一点，因为我们实际上并没有运行一个生产级别的待办事项列表应用。事实上，我们现在运行的设置可能正在消耗你实验室机器上相当多的资源，所以在我们继续之前，我们将将其清除。
- en: Try it now Remove the application, the custom resources, and the Operators.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试吧！移除应用、自定义资源和Operator。
- en: '[PRE18]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You can see my output in figure 20.15, where uninstalling everything is just
    the reverse of deployment. Operators don’t necessarily remove every resource they
    create because they might contain data you don’t want to lose. Expect to see ConfigMaps,
    Secrets, PersistentVolumeClaims, and even CRDs hang around, even after you remove
    an Operator—another good reason to use separate namespaces for your apps so you
    can remove everything cleanly.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图20.15中看到我的输出，其中卸载所有内容只是部署的反向操作。Operator并不一定删除它们创建的所有资源，因为它们可能包含你不希望丢失的数据。预期在移除Operator后，ConfigMaps、Secrets、PersistentVolumeClaims甚至CRDs仍然存在，这又是使用单独的命名空间为你的app提供的一个好理由，这样你可以干净利落地移除所有内容。
- en: '![](../Images/20-15.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图20.15](../Images/20-15.jpg)'
- en: Figure 20.15 Operators don’t necessarily clean up when they get deleted, so
    you’ll need to manually check for leftover resources.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.15 Operator在删除时并不一定会清理，所以你需要手动检查剩余的资源。
- en: Operators are a neat way to manage third-party dependencies. You need to put
    some effort into finding an Operator that works for you, and bear in mind, many
    of these are open source projects that may not have a lot of momentum. Compare
    the Prometheus Operator (one of the best examples on OperatorHub) which has 350
    contributors and new updates pretty much daily, and the MySQL Operator from Oracle,
    which has 18 contributors and, at the time of writing, hasn’t been worked on for
    two years. Lots of Operators are flagged as alpha or beta software, and these
    are typically for critical components, so you need to be comfortable with the
    maturity level of anything you bring into your cluster.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Operator是一种管理第三方依赖的整洁方式。你需要投入一些精力去找到一个适合你的Operator，并且记住，这些中的许多都是开源项目，可能没有太多的动力。比较Prometheus
    Operator（OperatorHub上最好的例子之一），它有350位贡献者，几乎每天都有新的更新，以及来自Oracle的MySQL Operator，它有18位贡献者，在撰写本文时，已经两年没有更新了。许多Operator被标记为alpha或beta软件，这些通常是关键组件，所以你需要对你集群中引入的任何东西的成熟度水平感到舒适。
- en: But Operators are not just for third-party software; you can build your own
    Operator to simplify the deployment and ongoing maintenance of your own applications.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 但Operator不仅限于第三方软件；你可以构建自己的Operator来简化你自己的应用的部署和持续维护。
- en: 20.4 Building Operators for your own applications
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.4 为自己的应用构建Operator
- en: There are two main reasons for building your own Operator. The first is for
    apps that have complex operational requirements, and the second is for common
    components that are installed as services for many projects. An Operator for the
    to-do app might have custom upgrade logic—updating the Service to direct traffic
    to an “under maintenance” page, or waiting for the message queue to be empty and
    then backing up the database. Any app with routine operations tasks that can be
    automated is a potential candidate for a custom Operator.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 构建自己的Operator有两个主要原因。第一个原因是针对那些具有复杂操作需求的app，第二个原因是针对那些作为许多项目服务安装的通用组件。为待办事项app设计的Operator可能包含自定义升级逻辑——例如更新服务以将流量导向“维护中”页面，或者等待消息队列为空然后备份数据库。任何具有可自动化常规操作任务的app都可能是定制Operator的潜在候选者。
- en: Building your own Operator is not a trivial task because it involves multiple
    custom resource types and multiple custom controllers. The complexity comes in
    mapping out all the scenarios, not just the workflows the Operator owns but also
    any additional work it needs to do to put right any interference from human operators.
    There’s a custom Operator in this chapter’s resources, but I won’t focus on the
    code—it’s an example of an as-a-service Operator for the web-ping application
    we used back in chapter 10\. That’s an app that makes `GET` requests to a web
    address on a schedule and logs the response time. At a push, I might be able to
    convince you that’s a service many teams would use to monitor the uptime for their
    apps.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 构建自己的操作员不是一项简单的工作，因为它涉及到多个自定义资源类型和多个自定义控制器。复杂性在于规划所有场景，不仅包括操作员拥有的工作流程，还包括它需要执行的其他任何工作以纠正人为操作员的干扰。本章的资源中有一个自定义操作员，但我不打算关注代码——它是一个用于我们在第10章中使用的web-ping应用程序的as-a-service操作员的示例。这是一个按计划对Web地址执行`GET`请求并记录响应时间的应用程序。如果可能，我可能会说服你这是一个许多团队都会使用的用于监控应用程序正常运行时间的服务。
- en: Try it now The web-ping Operator is deployed with YAML manifests like the NATS
    Operator. Install it to see how it runs and the CRDs it deploys.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 The web-ping操作员使用与NATS操作员相同的YAML清单进行部署。安装它以查看其运行方式和部署的CRD。
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You’ll see that the Operator Pod has several roles: it installs two CRDs and
    runs two containers, a custom controller for each custom resource. Figure 20.16
    shows that the CRDs are for a WebPinger resource, which defines the address and
    schedule to use, and a WebPingerArchive resource, which is for archiving the results
    of a WebPinger resource.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到操作员Pod有几个角色：它安装了两个CRD并运行了两个容器，每个自定义资源都有一个自定义控制器。图20.16显示了CRD是用于WebPinger资源，它定义了要使用的地址和计划，以及WebPingerArchive资源，用于归档WebPinger资源的结果。
- en: '![](../Images/20-16.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/20-16.jpg)'
- en: Figure 20.16 The web-ping Operator has a minimal manifest and deploys other
    resources when it runs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.16 web-ping操作员具有最小的清单，并在运行时部署其他资源。
- en: One of the goals of the Operator pattern is to keep the user experience simple,
    so the installation is handled inside the Operator as much as possible. That keeps
    the deployment spec simple and removes any potential for errors—the Operator doesn’t
    rely on a complex manifest (with the exception of RBAC rules, which are needed
    in advance). The Operator spec you just deployed is shown in listing 20.7; there’s
    an init container, which creates the CRDs, and two containers, which run the controllers.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 操作员模式的一个目标是将用户体验保持简单，因此尽可能在操作员内部处理安装。这保持了部署规范简单，并消除了任何潜在的错误——操作员不依赖于复杂的清单（除了RBAC规则，这些规则需要提前准备）。你刚刚部署的操作员规范在列表20.7中显示；有一个初始化容器，用于创建CRD，以及两个容器，用于运行控制器。
- en: Listing 20.7 02-wpo-deployment.yaml, the Pod spec for the Operator
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.7 02-wpo-deployment.yaml，操作员的Pod规范
- en: '[PRE20]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Not much can go wrong there. If the Operator needed ConfigMaps, Secrets, Services,
    and PersistentVolumeClaims, it would own the creation of them all, keeping the
    complexity away from the administrator. The web-ping application has a few parameters
    to specify the address to test, the type of HTTP request, and the interval between
    requests. The CRD lets users state those fields, and the custom controller running
    in the Operator creates a correctly configured Deployment for each instance of
    the app. Listing 20.8 shows a WebPinger resource configured to test my blog.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里不太可能出错。如果操作员需要ConfigMaps、Secrets、Services和PersistentVolumeClaims，它将拥有创建它们的全部权限，从而将复杂性从管理员那里移开。web-ping应用程序有一些参数可以指定要测试的地址、HTTP请求类型和请求之间的间隔。CRD允许用户声明这些字段，并且运行在操作员中的自定义控制器为应用程序的每个实例创建一个正确配置的Deployment。列表20.8显示了一个配置为测试我的博客的WebPinger资源。
- en: Listing 20.8 webpinger-blog.yaml, a custom resource to test a web address
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.8 webpinger-blog.yaml，一个用于测试Web地址的自定义资源
- en: '[PRE21]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: When you deploy this, the Operator creates an instance of the web-ping app with
    a special configuration, logging responses to a file in JSON format for analysis.
    The Pod also includes a sidecar container, which provides an HTTP API for clearing
    down the log file, and that powers the archive functionality.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当你部署这个操作员时，它会创建一个具有特殊配置的web-ping应用程序实例，将响应记录到JSON格式的文件中进行分析。Pod还包括一个侧边容器，它提供了一个HTTP
    API来清除日志文件，并支持归档功能。
- en: Try it now Create a web ping resource, and confirm the Operator creates an instance
    of the app, which sends HTTP requests to my blog.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 创建一个web ping资源，并确认操作员创建了一个应用程序实例，该实例向我的博客发送HTTP请求。
- en: '[PRE22]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This is a nice and easy way to deploy simple blackbox observation of a site,
    and every team can include a WebPinger spec with their production deployments
    to keep an eye on the uptime of their app. If teams are familiar with the web-ping
    app, it behaves in the same way as a manual deployment with human-readable logs
    printing to the standard output stream. As you see in figure 20.17, the logs are
    also written as JSON, which is where the archive requirement comes in to protect
    disk space.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单且易于部署对网站进行简单黑盒观察的好方法，每个团队都可以在他们的生产部署中包含一个 WebPinger 规范，以监控他们应用程序的运行时间。如果团队熟悉
    web-ping 应用，它将以与手动部署相同的方式运行，将可读日志打印到标准输出流。正如你在图 20.17 中看到的，日志也被写入为 JSON，这就是存档需求出现以保护磁盘空间的原因。
- en: '![](../Images/20-17.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图 20.17](../Images/20-17.jpg)'
- en: Figure 20.17 The web-ping app is now easy to deploy and manage across multiple
    instances.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.17 网络ping应用现在可以轻松地在多个实例中部署和管理。
- en: 'Archiving is the only operational feature provided by the web-ping Operator,
    and it’s simple to use: create a WebPingerArchive resource that specifies the
    target domain name. The custom controller for that resource looks for a web-ping
    Pod that matches the domain name and uses the API in the sidecar container to
    grab a snapshot of the current log file and then clear the file down. This archive
    function is a good example of the work you need to do to automate operational
    tasks. It’s not just the CRD and the controller; the app itself needs a sidecar
    to provide additional admin features.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 存档是 web-ping Operator 提供的唯一操作功能，使用起来很简单：创建一个指定目标域名名的 WebPingerArchive 资源。该资源的自定义控制器会寻找与域名匹配的
    web-ping Pod，并使用边车容器中的 API 抓取当前日志文件的快照，然后清除文件。这个存档功能是自动化操作任务所需工作的一个好例子。这不仅仅是 CRD
    和控制器；应用程序本身需要一个边车来提供额外的管理功能。
- en: Try it now Test out the operational side of the web-ping app—creating an archive
    of the logs for the blog requests.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看——测试 web-ping 应用的操作方面——创建博客请求的日志存档。
- en: '[PRE23]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: My output appears in figure 20.18\. This is a contrived example, but it’s a
    good way to see how Operators solve complex problems without getting lost in the
    actual problem. After the archive runs, the ping results are available in the
    Pod logs for the Job, while the web-ping Pod is still happily chewing up my bandwidth
    and has an empty log file to start filling again.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我的输出显示在图 20.18 中。这是一个人为设计的例子，但它是观察 Operators 如何解决复杂问题而不迷失在实际问题中的好方法。在归档运行后，ping
    结果将在作业的 Pod 日志中可用，而 web-ping Pod 仍在愉快地消耗我的带宽，并且有一个空日志文件开始再次填充。
- en: '![](../Images/20-18.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 20.18](../Images/20-18.jpg)'
- en: Figure 20.18 The archive workflow is managed by the Operator and triggered by
    creating another custom resource.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.18 存档工作流程由 Operator 管理，并通过创建另一个自定义资源来触发。
- en: 'Kubernetes Operators are usually written in Go, and if you have Go, two tools
    take care of a lot of the boilerplate code for you: Kubebuilder from Google and
    the Operator SDK, which is part of the same toolset as Operator Lifecycle Manager.
    My Go isn’t really up to scratch, so my Operator is written in .NET, and it took
    me about a day’s worth of coding to build the Operator for this section. It’s
    good fun digging into the Kubernetes API and writing code that creates and manages
    resources, and building up objects in code certainly makes you appreciate YAML
    a whole lot more.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes Operators 通常是用 Go 编写的，如果你有 Go，两个工具会为你处理很多样板代码：来自 Google 的 Kubebuilder
    和 Operator SDK，后者是 Operator Lifecycle Manager 工具集的一部分。我的 Go 并非真正精通，所以我用 .NET 编写了
    Operator，构建这个部分的 Operator 大约花费了我一天的时间进行编码。深入挖掘 Kubernetes API 并编写创建和管理资源的代码，以及在代码中构建对象确实让你更加欣赏
    YAML。
- en: But it’s time to stop pinging my blog. This Operator doesn’t have any admission
    controllers to stop you from removing its CRDs, so you can delete them. That will
    trigger the deletion of the custom resources, and then the controllers will clean
    up the resources they created.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在是停止ping我的博客的时候了。这个 Operator 没有任何准入控制器来阻止你删除它的 CRDs，所以你可以删除它们。这将触发自定义资源的删除，然后控制器将清理它们创建的资源。
- en: Try it now Delete the Operator’s CRDs. The custom resources will be deleted,
    triggering the removal workflows in the Operator.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看——删除 Operator 的 CRDs。自定义资源将被删除，这将触发 Operator 中的删除工作流程。
- en: '[PRE24]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You see in figure 20.19 that in this Operator, it’s controllers all the way
    down. The WebPing custom controller deletes the Deployment resource, and then
    the system controllers delete the ReplicaSet and the Pod. The Operator doesn’t
    try to replace or replicate Kubernetes functionality—it builds on it, using standard
    resources that have been used in production all around the world for many years,
    abstracting them to provide a simple user experience.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在图20.19中，你可以看到在这个Operator中，控制器贯穿始终。WebPing自定义控制器删除Deployment资源，然后系统控制器删除ReplicaSet和Pod。Operator并不试图取代或复制Kubernetes的功能——它是基于它的，使用已经在全球范围内使用多年的标准资源，将它们抽象化以提供简单的用户体验。
- en: '![](../Images/20-19.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/20-19.jpg)'
- en: Figure 20.19 The custom controllers in this Operator manage standard Kubernetes
    resources.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.19 这个Operator中的自定义控制器管理标准的Kubernetes资源。
- en: You need to understand how the Operator pattern works because you’re sure to
    come across it in your Kubernetes journey, although you’re more likely to use
    someone else’s Operator than build your own. The key thing to understand is that
    it’s a loose classification for a way of making apps simple to use and maintain,
    taking advantage of the extensibility of Kubernetes and making good use of the
    core system resources.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要理解Operator模式是如何工作的，因为你肯定会在你的Kubernetes之旅中遇到它，尽管你更有可能使用他人的Operator而不是自己构建。关键是要理解，它是一种松散的分类，用于使应用程序易于使用和维护，利用Kubernetes的可扩展性，并充分利用核心系统资源。
- en: 20.5 Understanding when to extend Kubernetes
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.5 理解何时扩展Kubernetes
- en: We’ve covered a lot of ground in this chapter without digging too much into
    the detail. Extending Kubernetes is about getting the cluster to run your own
    code, and what happens in that code depends on the problem you’re trying to solve.
    The patterns are generic, though, and figure 20.20 shows how all the pieces fit
    together with the Operator, custom resources, and custom controllers for the web-ping
    application.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们没有深入细节，但已经覆盖了很多内容。扩展Kubernetes是让集群运行你自己的代码，而在代码中发生什么取决于你试图解决的问题。尽管你更有可能使用他人的Operator而不是自己构建，但这些模式是通用的，图20.20显示了Operator、自定义资源和自定义控制器如何与web-ping应用程序的所有部分协同工作。
- en: '![](../Images/20-20.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/20-20.jpg)'
- en: Figure 20.20 Operators and custom controllers make apps easy to manage by abstracting
    the complexity.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.20 Operator和自定义控制器通过抽象复杂性使应用程序易于管理。
- en: There are a few guidelines for extending Kubernetes. The first is to make sure
    that you really need to. Writing an Operator to save on the YAML for an app that
    runs a Deployment, a ConfigMap, and a Service is overkill. If your goal is to
    ensure the app is deployed with the proper specification, it would be better to
    use admission controllers. Writing and maintaining custom controllers and Operators
    is a chunk of work, and if you don’t map out all the workflows, your app can get
    into an inconsistent state, and then the Operator makes maintenance harder. Admins
    won’t enjoy manually building the spec and deploying resources that custom controllers
    should own.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展Kubernetes有一些指导原则。首先，确保你真的需要这样做。为运行Deployment、ConfigMap和Service的应用程序编写Operator以节省YAML是过度的。如果你的目标是确保应用程序以适当的规范部署，那么使用admission
    controllers会更好。编写和维护自定义控制器和Operator是一项大量工作，如果你没有规划好所有工作流程，你的应用程序可能会进入不一致的状态，然后Operator会使维护变得更加困难。管理员不会喜欢手动构建规范和部署自定义控制器应该拥有的资源。
- en: If you do have a clear need, then start simple with CRDs and controllers, and
    focus on the user experience; the whole point of custom resources is to simplify
    complex problems. Make use of the development toolkits if you’re writing in Go,
    and design your controllers to work with Kubernetes, building on the standard
    resources rather than reinventing them. It’s always better to build a generic
    system when you have a couple of concrete examples to work from, so you know the
    generic approach will cover all the requirements. When you’ve done some complex
    upgrades and you know the workflows, or when you’ve deployed a common component
    multiple times and you know the variations, then it’s time to design your Operator.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你确实有明确的需求，那么就从CRDs和控制器开始，并专注于用户体验；自定义资源的全部意义在于简化复杂问题。如果你用Go编写，请使用开发工具包，并设计你的控制器以与Kubernetes协同工作，基于标准资源而不是重新发明它们。当你有几个具体的例子可以参考时，构建一个通用系统总是更好的，这样你就知道通用方法将涵盖所有需求。当你完成了一些复杂的升级并且了解了工作流程，或者当你多次部署了通用组件并且了解了变体，那么就是时候设计你的Operator了。
- en: Third-party Operators are a great way to use somebody else’s production experience
    to improve the reliability of your own applications. The key here is to find a
    good one, and that will take some investigation and experimentation with different
    options. Using an Operator to manage third-party components is a big dependency.
    You don’t want to find that the project stalls and you need to reverse-engineer
    the Operator and take ownership yourself. The Operator Framework is the umbrella
    project that owns OLM and the Operator SDK, and it was added as a new CNCF project
    a few weeks before I wrote this chapter, so that might bring some new energy to
    OperatorHub.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 第三方操作符是使用他人的生产经验来提高你自己的应用程序可靠性的好方法。关键在于找到一个好的，这需要一些调查和尝试不同的选项。使用操作符来管理第三方组件是一个大的依赖。你不想发现项目停滞不前，你需要逆向工程操作符并自己承担所有权。操作符框架是拥有OLM和操作符SDK的母项目，它在我写这一章的前几周被添加为新的CNCF项目，这可能会给操作符Hub带来一些新的活力。
- en: That’s all for extending Kubernetes, so we can clean up before we get to the
    lab.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是扩展Kubernetes的全部内容，所以我们可以在进入实验室之前进行清理。
- en: Try it now Clear down any remaining resources.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 清理掉任何剩余的资源。
- en: '[PRE25]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 20.6 Lab
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.6 实验室
- en: 'This lab will give you some experience writing a CRD and managing a custom
    controller. Don’t worry; the controller is already written for you. In the lab
    folder, there’s a custom resource spec for a timecheck app but no CRD, so you
    can’t deploy it. The task is to build the CRD, deploy the controller and the resource,
    and verify it all works as expected. Just a couple of hints:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 本实验室将为你提供一些编写CRD和管理自定义控制器的一些经验。别担心；控制器已经为你准备好了。在实验室文件夹中，有一个为timecheck应用定制的资源规范，但没有CRD，所以你不能部署它。任务是构建CRD，部署控制器和资源，并验证它们是否按预期工作。这里有一些提示：
- en: The custom controller is all ready to go in the `timecheck-controller` folder.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义控制器已经在`timecheck-controller`文件夹中准备好了。
- en: Your CRD names need to match the ones in the resource.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的CRD名称需要与资源中的名称匹配。
- en: You’ll need to have a look at the logs when you deploy the controller; depending
    on the order in which you approach the lab, it might not work as expected.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署控制器时，你需要查看日志；根据你接近实验室的顺序，它可能不会按预期工作。
- en: 'You can check my solution on GitHub as always: [https://github.com/sixeyed/kiamol/blob/master/ch20/lab/README.md](https://github.com/sixeyed/kiamol/blob/master/ch20/lab/README.md).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像往常一样在GitHub上查看我的解决方案：[https://github.com/sixeyed/kiamol/blob/master/ch20/lab/README.md](https://github.com/sixeyed/kiamol/blob/master/ch20/lab/README.md).
