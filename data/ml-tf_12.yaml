- en: 10 Part-of-speech tagging and word-sense disambiguation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10部分：词性标注和词义消歧
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Disambiguating language by predicting nouns, verbs, and adjectives from past
    data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过从过去的数据中预测名词、动词和形容词来消歧语言
- en: Making decisions and explaining them using hidden Markov models (HMMs)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用隐马尔可夫模型（HMMs）做出决策并解释它们
- en: Using TensorFlow to model explainable problems and collect evidence
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow来建模可解释的问题并收集证据
- en: Computing HMM initial, transition, and emission probabilities from existing
    data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从现有数据计算HMM的初始、转移和发射概率
- en: Creating a part-of-speech (PoS) tagger from your own data and larger corpora
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从你自己的数据和更大的语料库中创建词性（PoS）标注器
- en: You use language every day to communicate with others, and if you are like me,
    sometimes you scratch your head, especially if you are using the English language.
    English is known to have a ton of exceptions that make it difficult to teach non-native
    speakers, along with your little ones who are growing up trying to learn it themselves.
    Context matters. Conversationally, you can use tools such as hand motions, facial
    expressions, and long pauses to convey additional context or meaning, but when
    you are reading language as written text, much of that context is missing, and
    there is a lot of ambiguity. Parts of speech (PoS) can help fill that missing
    context to disambiguate words and make sense of them in text. PoS tells you whether
    the word is being used is an action word (verb), whether it refers to an object
    (noun), whether it describes a noun (adjective), and so on.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你每天都在使用语言与他人沟通，如果你像我一样，有时你会挠头，尤其是如果你在使用英语。众所周知，英语有很多例外，这使得非母语者难以教授，包括那些正在努力自学的小家伙们。上下文很重要。在对话中，你可以使用诸如手势、面部表情和长暂停等工具来传达额外的上下文或意义，但当你阅读书面语言时，大部分上下文都缺失了，而且有很多歧义。词性（PoS）可以帮助填补缺失的上下文，以消歧义并使文本中的词语有意义。词性（PoS）告诉你一个词是否被用作动作词（动词），是否指代一个对象（名词），是否描述一个名词（形容词），等等。
- en: Consider the two sentences in figure 10.1\. In the first sentence—“I am hoping
    to engineer a future Mars rover vehicle!”—someone says they want to help engineer
    (build) the next Mars rover. The person who said it in real life was most definitely
    not interested in anything other than Mars and planetary science. The second sentence—“I
    love being an engineer and working at NASA JPL on Earth Science!”—uttered by a
    different person, is about enjoying working at the Jet Propulsion Laboratory,
    part of the National Aeronautics and Space Administration (NASA), where I work.
    The second sentence was said by someone who enjoys being an engineer at NASA working
    on earth-science projects.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑图10.1中的两个句子。在第一个句子——“我希望能设计一个未来的火星漫游车！”中——有人表示他们想要帮助设计（建造）下一个火星漫游车。现实生活中说这句话的人肯定对火星和行星科学之外的事情不感兴趣。第二个句子——“我喜欢成为一名工程师，并在NASA
    JPL的地球科学部门工作！”是由另一个人说的，关于享受在喷气推进实验室（NASA的一部分）工作，我在那里工作。第二个句子是由一个喜欢在NASA工作并参与地球科学项目的研究工程师说的。
- en: '![CH10_F01_Mattmann2](../Images/CH10_F01_Mattmann2.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F01_Mattmann2](../Images/CH10_F01_Mattmann2.png)'
- en: Figure 10.1 Two sentences that require disambiguation
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 两个需要消歧的句子
- en: 'Here’s the rub: both sentences use the word engineer in different ways. The
    first sentence uses *engineer* as a verb—an action—and the second sentence uses
    *engineer* as a noun to refer to his role. Verbs and nouns are different PoS in
    the English language, and deciding between a noun or a verb is not unique to English;
    PoS exist in many languages and delineate meaning among the words and characters
    in the text. But as you can see, the problem is that words and their (sense of)
    meaning—word sense, for short—frequently require disambiguation to provide the
    contextual cues you normally understood through spoken conversation or visual
    cues. Remember that when you are reading text, you don’t have the benefit of hearing
    the inflection of someone’s voice. You don’t see visual cues to decide whether
    the person is saying that they want to build something that goes to Mars (using
    hand motions like hammering) or whether they’re saying that enjoy working on earth-science
    projects (using hand motions to refer to the world around them).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题是：两个句子都以不同的方式使用了单词**engineer**。第一个句子将**engineer**用作动词——一个动作——而第二个句子将**engineer**用作名词来指代他的角色。动词和名词在英语中是不同的词性，而决定是名词还是动词并不局限于英语；词性存在于许多语言中，并在文本中的单词和字符之间划分意义。但正如你所看到的，问题是单词及其（意义）——简称为词义——通常需要去歧义以提供你通常通过口头交谈或视觉线索所理解的上下文线索。记住，当你阅读文本时，你无法听到说话者声音的语调。你看不到视觉线索来判断这个人是在说他们想要建造一个去火星的东西（使用锤子的手势）还是说他们喜欢从事地球科学项目（使用手势指向他们周围的世界）。
- en: The good news is that language scholars have long studied PoS across the humanities,
    mainly by reading literature and producing helpful guidance and rules that convey
    the particular PoS class a word can take on. Over the years, these scholars have
    examined many texts and recorded what they saw. One example of these efforts is
    Project Gutenberg ([http://mng.bz/5pBB](http://mng.bz/5pBB)), which contains more
    than 200,000 words and dozens of PoS classes that those words could take on across
    a variety of written texts in the English language. You can think of the Gutenberg
    corpora as being a table of 200,000 English words; each word is a row and serves
    as the primary key, and the column values are the different PoS classes the word
    could take on (such as adjective or noun).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，语言学者们长期以来一直在人文科学领域研究词性，主要通过阅读文学作品并产生有用的指导和规则，这些规则传达了单词可以采取的特定词性。多年来，这些学者们检查了许多文本并记录了他们所看到的内容。这些努力的例子之一是古腾堡计划([http://mng.bz/5pBB](http://mng.bz/5pBB))，它包含超过200,000个单词和数十种可能的词性，这些词性可以在英语的多种书面文本中出现。你可以把古腾堡语料库想象成一个包含200,000个英语单词的表格；每个单词是一行，并作为主键，列值是该单词可能采取的不同词性（如形容词或名词）。
- en: 'Try applying Gutenberg to the engineer sentences in figure 10.1, and the ambiguity
    will pop out at you. I’ve annotated the sentences with the PoS tag from Gutenberg
    for illustration (omitting the earth-science part from the first sentence for
    brevity):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试将古腾堡计划应用于图10.1中的工程师句子，歧义就会跃然纸上。我已经用古腾堡的词性标注来注释这些句子以供说明（为了简洁，省略了第一句中的地球科学部分）：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As you can clearly see, Gutenberg has told us that *engineer* can be either
    a noun or a verb. How can you look at the text and figure out automatically, using
    the surrounding context or by reading lots of similar text, what PoS class a particular
    word is with confidence, rather than guessing?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所清晰看到的，古腾堡告诉我们**engineer**可以是一个名词或动词。你如何通过观察文本并利用周围语境或阅读大量类似文本，自动地有信心地判断一个特定单词的词性，而不是猜测呢？
- en: Machine learning and probabilistic models such as hidden Markov models (HMMs),
    which you learned about in chapter 9, can help you fill in some of that context
    by modeling the process of PoS tagging as an HMM problem. As an extremely early
    sketch, the initial probabilities are the probabilities that a particular PoS
    class will occur based on studying a set of input sentences. The transition probabilities
    are the probabilities of seeing a particular PoS class occur after another or
    set of classes in a particular order. Finally, the emission or observable properties
    are the probabilities of an ambiguous class like `Noun/Verb` being a noun or `Noun/Verb`
    being a verb, based on all other sentences your program has seen.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习和概率模型，如第9章中你所学到的隐藏马尔可夫模型（HMMs），可以通过将词性标注（PoS）的过程建模为HMM问题来帮助你填补一些上下文。作为一个极其初步的草图，初始概率是基于研究一组输入句子而得到的，一个特定PoS类发生的概率。转移概率是在特定顺序中看到某个PoS类在另一个或一系列类之后发生的概率。最后，发射或可观察属性是基于所有其他句子程序所看到的，一个模糊类如`名词/动词`是名词还是`名词/动词`是动词的概率。
- en: The great news is that all your code from chapter 9 is reusable here. The grand
    effort you spend in this chapter will prepare the data for machine learning, compute
    the model, and set up TensorFlow to do its thing! Let’s get started. Before we
    jump into PoS tagging, though, I’ll quickly review what the HMM is doing, using
    the Rainy or Sunny example from chapter 9\. Follow me!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，第9章中所有的代码都可以在这里重用。你在这章中付出的巨大努力将为机器学习准备数据，计算模型，并设置TensorFlow以执行其任务！让我们开始吧。在我们跳入词性标注之前，我会快速回顾一下HMM正在做什么，使用第9章中的Rainy或Sunny示例。跟我来！
- en: '10.1 Review of HMM example: Rainy or Sunny'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 HMM示例回顾：雨天或晴天
- en: The Rainy/Sunny example from chapter 9 uses HMMs to model the hidden states
    of the weather if you could only ever observe indirect activities such as walking,
    shopping, or cleaning. To model the code as an HMM, you need the initial probabilities
    of the weather’s being rainy or sunny—a 2 × 1 matrix. You need the transition
    probabilities of the weather’s being rainy and then it being sunny (and vice versa)—a
    2 × 2 matrix. And you need the probabilities of seeing the indirect actions, or
    emission probabilities, of walking, shopping, or cleaning for each of the hidden
    states Rainy and Sunny—a 3 × 2 matrix. The left side of figure 10.2 shows the
    construction of this problem.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 第9章中的Rainy/Sunny示例使用HMM来模拟天气的隐藏状态，如果你只能观察到间接活动，如走路、购物或清洁。要将代码作为HMM建模，你需要天气是雨天或晴天的初始概率——一个2×1矩阵。你需要天气是雨天然后是晴天的转移概率（反之亦然）——一个2×2矩阵。你还需要看到间接动作或发射概率的概率，即走路、购物或清洁对于隐藏状态Rainy和Sunny的概率——一个3×2矩阵。图10.2的左侧展示了这个问题的构建。
- en: '![CH10_F02_Mattmann2](../Images/CH10_F02_Mattmann2.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F02_Mattmann2](../Images/CH10_F02_Mattmann2.png)'
- en: Figure 10.2 The left side is the HMM construction of the Rainy/Sunny example.
    The right side demonstrates how TensorFlow and our HMM class take in a set of
    observed states and accumulate the probabilities of those observed states occurring.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2的左侧是Rainy/Sunny示例的HMM构建。右侧展示了TensorFlow和我们的HMM类如何接收一组观察到的状态并累积这些观察状态发生的概率。
- en: 'Also in chapter 9, I showed you how to create an HMM class in Python and TensorFlow
    to represent the initialization of the initial, transition, and emission probabilities
    and then run the forward model to compute the overall probability of a sequence
    of events occurring (such as walk, shop, shop, clean) and then the next event
    as shop. This computation is shown on the right side of figure 10.2\. TensorFlow
    operations shine in the development of the HMM class, in particular TensorFlow’s
    lazy evaluation and graph construction process. Here’s how:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在第9章中，我向你展示了如何在Python和TensorFlow中创建一个HMM类来表示初始、转移和发射概率的初始化，然后运行前向模型来计算事件序列发生的整体概率（例如走路、购物、购物、清洁），然后下一个事件是购物。这个计算在图10.2的右侧展示。TensorFlow操作在HMM类的开发中表现出色，特别是TensorFlow的延迟评估和图构建过程。以下是方法：
- en: The HMM class acts as a proxy for constructing a graph of `tf.slice` operations
    that pull out specific emission probabilities based on the observation provided
    and `tf.matmul` operations that accumulate the forward model probabilities (steps
    1-2, 2-3, and 3-4 from figure 10.2).
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HMM类充当构建`tf.slice`操作图的代理，这些操作根据提供的观察结果提取特定的发射概率，以及`tf.matmul`操作，这些操作累积前向模型概率（如图10.2中的步骤1-2、2-3和3-4）。
- en: The algorithm begins with initial probabilities and multiplies them by the emission
    probabilities based on the first observation index for the walk event on the right
    side of figure 10.2 (step 2).
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法从初始概率开始，根据图10.2右侧的行事件右侧的第一个观测索引乘以发射概率。
- en: Next, the algorithm accumulates the forward model probabilities and prepares
    to iterate through the remaining observations (steps 4-6 in figure 10.2).
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，算法累积前向模型概率并准备迭代剩余的观测（图10.2中的步骤4-6）。
- en: 'For each observation, the HMM class runs the forward model to execute the following
    steps repeatedly:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个观测，HMM类运行前向模型以重复执行以下步骤：
- en: Use `tf.slice` to slice out the particular emission probability set for Rainy
    or Sunny based on the observation index and multiple those probabilities by the
    forward model (steps 1-3 in figure 10.2).
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tf.slice`根据观测索引切片出特定的发射概率集，并将其与前向模型相乘（图10.2中的步骤1-3）。
- en: Accumulate (through matrix multiplication or `tf.matmul`) the forward model
    probabilities for each hidden state, based on multiplying the forward model by
    the transition probabilities (step 4 in figure 10.2).
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据乘以前向模型和转换概率，累积每个隐藏状态的前向模型概率（图10.2中的步骤4）。
- en: Use `tf.reduce_sum` to accumulate for each choice, Rainy and Sunny, the agglomerative
    probability of being in those states based on the observations and `tf.transpose`
    to return the Rainy/Sunny matrix to the original forward model structure (steps
    5-6 in figure 10.2).
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tf.reduce_sum`累积每个选择（雨天和晴天）基于观测和`tf.transpose`将Rainy/Sunny矩阵返回到原始前向模型结构（图10.2中的步骤5-6）。
- en: Listing 10.1 creates the HMM class and then runs the forward model. This listing
    maps to the visual steps 1-6 (figure 10.2), and you can see the first iteration
    unfolding for rainy and sunny weather as a series of transformations on the right
    side of figure 10.2.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.1创建了HMM类并运行前向模型。此列表对应于视觉步骤1-6（图10.2），你可以看到图10.2右侧的一系列变换展示了雨天和晴天的第一次迭代。
- en: Listing 10.1 The HMM class and forward model
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.1 HMM类和前向模型
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Initializes the model with initial probabilities, emission probabilities,
    and transition probabilities
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用初始概率、发射概率和转换概率初始化模型
- en: ❷ Uses tf.slice to extract the emission probabilities corresponding to the observation
    index
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用tf.slice提取与观测索引对应的发射概率
- en: ❸ Runs the first forward operation on the first observation and accumulates
    forward model
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个观测上运行第一个前向操作并累积前向模型
- en: ❹ Accumulates through multiplication the emission probabilities and transition
    probabilities for an observation
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在观测上累积发射概率和转换概率
- en: ❺ Runs the forward model on observation 2 and beyond, accumulating probabilities
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在观测2及以后运行前向模型并累积概率
- en: To use the forward model, you create the initial probabilities, transition probabilities,
    and emission probabilities and then feed them to the HMM class, as shown in listing
    10.2\. The output of the program is the accumulated probability of seeing those
    specific indirect observations (or emissions) in that particular order.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用前向模型，你创建初始概率、转换概率和发射概率，然后将它们输入到HMM类中，如列表10.2所示。程序输出是看到那些特定间接观测（或发射）以特定顺序出现的累积概率。
- en: Listing 10.2 Running the HMM class with the Rainy/Sunny example
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.2使用Rainy/Sunny示例运行HMM类
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Creates the initial NumPy arrays representing initial, transition, and emission
    probabilities
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 创建表示初始、转换和发射概率的初始NumPy数组
- en: ❷ Walk, shop, shop, clean, and shop observations
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 行走、购物、购物、清洁和购物观测
- en: ❸ Runs the forward model and prints the accumulated probability of those events
    occurring
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 运行前向模型并打印这些事件发生的累积概率
- en: Recall from chapter 9 that the Viterbi algorithm is a simple specialization
    of the HMM model and class that keeps track of the transitions between states
    through back pointers and accumulates the probable transition between states,
    given a set of observations. Instead of providing the cumulative probability of
    a set of states occurring, the Viterbi algorithm provides the most probable hidden
    state for each indirect observed emission state. In other words, it provides the
    most probable state (Rainy or Sunny) for each indirect observation of walk, shop,
    shop, clean, and shop. You will make direct use the Viterbi algorithm later in
    the chapter because you will want to predict the PoS for a given ambiguity class.
    Specifically, you will want to know whether the PoS is a noun or a verb, if given
    an ambiguity class `Noun/Verb`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 回想第9章，维特比算法是HMM模型和类的一个简单特化，它通过回溯指针跟踪状态之间的转换，并累积给定一组观察到的状态之间的概率转换。维特比算法不是提供一组状态发生的累积概率，而是提供每个间接观察到的隐藏状态的最可能状态。换句话说，它为每个间接观察到的行走、购物、购物、清洁和购物的状态提供最可能的状态（雨天或晴天）。您将在本章后面直接使用维特比算法，因为您将想要预测给定歧义类的PoS。具体来说，您将想知道如果给定歧义类`名词/动词`，PoS是名词还是动词。
- en: Now that our HMM review is done, it’s time to put together our initial ambiguous
    PoS tagger.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了HMM的复习，是时候组装我们的初始歧义PoS标签器了。
- en: 10.2 PoS tagging
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 PoS标注
- en: 'PoS tagging can be generalized to the problem of taking input sentences and
    then trying to disambiguate the ambiguous parts of speech. The Moby project is
    one of the largest free phonetic databases and is mirrored as part of Project
    Gutenberg. The Moby effort contains the file mobypos.txt, which is a database
    of words in English and the PoSes they can take. The file (available at [http://mng.bz/6Ado](http://mng.bz/6Ado))
    needs to be parsed because it’s a set of lines in which each line begins with
    the word that the database knows about and is followed by a set of tags (such
    as `\A\N`) that correspond to parts of speech—in this case adjective (`\A`) and
    noun `(\N`). The mapping of PoS tags to mobypos.txt annotations is shown in listing
    10.3, and you will reuse this mapping in the remainder of the chapter. Words appear
    in mobypos.txt in the following form:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: PoS标注可以推广到将输入句子作为输入，然后尝试区分词性的歧义问题。Moby项目是最大的免费音标数据库之一，它是作为Project Gutenberg的一部分进行镜像的。Moby项目包含文件mobypos.txt，这是一个包含英语单词及其可以接受的PoS的数据库。该文件（可在[http://mng.bz/6Ado](http://mng.bz/6Ado)获取）需要解析，因为它是一组以数据库中已知的单词开头的行，后面跟着一组标签（如`\A\N`），这些标签对应于词性——在这种情况下是形容词（`\A`）和名词（`\N`）。PoS标签到mobypos.txt注释的映射在列表10.3中显示，您将在本章的其余部分重用此映射。单词在mobypos.txt中的形式如下：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A concrete example follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个具体的例子：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: According to mobypos.txt, the English word *abdominous* has a single PoS use
    as an adjective.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 根据mobypos.txt，英语单词*abdominous*作为形容词有一个单一的PoS用法。
- en: Listing 10.3 PoS tag mapping for mobypos.txt
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.3 mobypos.txt的PoS标签映射
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ The PoS tags defined in Project Gutenberg as part of mobypos.txt
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Project Gutenberg中定义的PoS标签作为mobypos.txt的一部分
- en: ❷ The PoS tag keys
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ PoS标签键
- en: ❸ The PoS tag values
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ PoS标签值
- en: ❹ Add end-of-sentence tag because you will use it later for computing initial
    probabilities.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 添加句尾标记，因为您稍后会用它来计算初始概率。
- en: You can find the full documentation on Project Gutenberg’s PoS database below
    the heading Database Legend at [http://mng.bz/oROd](http://mng.bz/oROd).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Project Gutenberg的PoS数据库的“数据库图例”标题下方找到完整的文档，链接为[http://mng.bz/oROd](http://mng.bz/oROd)。
- en: 'To get started disambiguating word sense with uing TensorFlow and HMMs, you
    are going to need some words. Luckily for you, I’ve got a set of thriving youngsters
    and a wife who are replete with sayings and phrases that beg for disambiguation.
    While compiling this book, I listened to some of the things they said and jotted
    down a few for your perusal. I hope that those of you who have preteen children
    should get a kick out of these:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用TensorFlow和HMM来区分词义，您将需要一些单词。幸运的是，我有一群充满活力的小伙子和一位妻子，他们有很多需要区分的谚语和短语。在编写这本书的过程中，我听了他们说的某些话，并记下了一些供您参考。我希望那些有青少年孩子的读者会喜欢这些：
- en: “There is tissue in the bathroom!”
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “浴室里有纸巾！”
- en: “What do you want it for?”
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “你用它来做什么？”
- en: “I really enjoy playing Fortnite, it’s an amazing game!”
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “我真的喜欢玩《堡垒之夜》，这是一款令人惊叹的游戏！”
- en: “The tissue is coming out mommy, what should I use it for?”
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “纸巾出来了，妈妈，我应该用它来做什么？”
- en: “We are really interested in the Keto diet, do you know the best way to start
    it?”
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “我们真的很感兴趣于生酮饮食，你知道如何最好地开始它吗？”
- en: Now I’ll show you how to start using Project Gutenberg and mobypos.txt to label
    the PoS. The first step is creating a function to parse the full mobypos.txt file
    and load it into a Python variable. Because parsing the file amounts to reading
    line by line, building a table of words and their PoS classes, the simple function
    in listing 10.4 does the trick. The function returns `pos_words`, which is the
    database of English words and their PoS classes along with `pos_tag_counts`—a
    summary of PoS tags and their overall occurrences.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我将向您展示如何开始使用 Project Gutenberg 和 mobypos.txt 来标记词性。第一步是创建一个函数来解析完整的 mobypos.txt
    文件并将其加载到 Python 变量中。因为解析文件相当于逐行读取，构建单词及其词性类别的表格，所以列表 10.4 中的简单函数就足够了。该函数返回 `pos_words`，这是英语单词及其词性类别的数据库，以及
    `pos_tag_counts`——词性标签及其总出现次数的摘要。
- en: Listing 10.4 Parsing the mobypos.txt file into a PoS database
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.4 将 mobypos.txt 文件解析到词性数据库中
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Reads mobypos.txt line by line
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 逐行读取 mobypos.txt
- en: ❷ Uses TQDM to print progress while parsing each line
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用 TQDM 在解析每一行时打印进度
- en: ❸ Splits the line by the word and its remaining classes after the ‘\’ symbol
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 按照单词及其后面的类别之后的 '\ ' 符号分割行
- en: ❹ Occasionally, there are PoS classes that you will want to ignore or are unknown,
    so skip them.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 有时，您可能希望忽略某些词性类别或它们是未知的，因此跳过它们。
- en: ❺ Joins the English word to its set of PoS classes
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将英语单词与其词性类别集合相连接
- en: ❻ Accumulates totals of PoS tags in the corpora
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 累积语料库中词性标签的总数
- en: When you execute the `parse_pos_file` function and obtain `pos_words` and `pos_tag_counts`,
    you can analyze the distribution of PoS tags across the project Gutenberg corpora
    to see whether anything jumps out at you. In particular, you see a few frequently
    occurring PoS tags and many that do not occur frequently and likely will not appear
    when you’re tagging sentence text. The simple Matplotlib listing in listing 10.5
    reveals the PoS tag distribution (figure 10.3).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当您执行 `parse_pos_file` 函数并获得 `pos_words` 和 `pos_tag_counts` 时，您可以分析 Project Gutenberg
    语料库中词性标签的分布，看看是否有任何让您印象深刻的内容。特别是，您会看到一些频繁出现的词性标签和许多不太频繁出现的词性标签，这些标签在标记句子文本时可能不会出现。列表
    10.5 中的简单 Matplotlib 列表揭示了词性标签的分布（图 10.3）。
- en: Listing 10.5 Distribution of PoS tags across Project Gutenberg
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.5 Project Gutenberg 中词性标签的分布
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Sorted by key, returns a list
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 按键排序，返回一个列表
- en: ❷ Creates a list of tuples of (tag, count)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个包含（标签，计数）元组的列表
- en: ❸ Unpacks a list of pairs into two tuples
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将一对列表解包成两个元组
- en: ❹ Visualizes the plot
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 可视化图表
- en: Note Adjective, noun, noun phrase, verb (usu.—short for *usually* —participle),
    plural, verb (transitive) and adverb are the most frequently occurring PoS tags
    in the corpora.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：形容词、名词、名词短语、动词（通常简称为 *usually* —分词）、复数、动词（及物）和副词是语料库中最频繁出现的词性标签。
- en: '![CH10_F03_Mattmann2](../Images/CH10_F03_Mattmann2.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F03_Mattmann2](../Images/CH10_F03_Mattmann2.png)'
- en: Figure 10.3 The distribution of PoS tags in the Project Gutenberg corpus
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 Project Gutenberg 语料库中词性标签的分布
- en: 'You may also wonder what percentage of English words have multiple PoS classes
    assigned to them, so that you can get a feel in general for what requires disambiguation,
    at least according to Project Gutenberg. Listing 10.6 provides an answer that
    may surprise you: about 6.5%.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还想知道有多少百分比英语单词被分配了多个词性类别，这样您就可以大致了解需要消歧义的内容，至少根据 Project Gutenberg 的数据。列表
    10.6 提供了一个可能让您感到惊讶的答案：大约 6.5%。
- en: Listing 10.6 Computing the percentage of English words with multiple PoS classes
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.6 计算具有多个词性类别的英语单词的百分比
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Counts the number of words that have more than one PoS class
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算具有多个词性类别的单词数量
- en: ❷ Divides words with more than one PoS class by the total number of words
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将具有多个词性类别的单词除以总单词数
- en: ❸ Prints the resulting percentage
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打印结果百分比
- en: It’s important to note that of those 6.5% of words with multiple PoS classes
    assigned to them, many classes include the most frequently occurring words in
    the language. Overall, word-sense disambiguation is needed. You’ll see soon how
    much it’s needed. Before we get to that topic, though, I want you to see the big
    picture.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，在这 6.5% 被分配了多个词性类别的单词中，许多类别包括语言中最频繁出现的单词。总的来说，需要词义消歧。您很快就会看到需要多少。不过，在我们到达这个主题之前，我想让您先看到整体情况。
- en: '10.2.1 The big picture: Training and predicting PoS with HMMs'
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1 整体情况：使用 HMM 训练和预测词性
- en: It’s worth taking a step back to consider what the HMM and TensorFlow are providing
    to the PoS tagger. As I mentioned earlier, PoS tagging can be generalized to the
    problem of trying to disambiguate ambiguous parts of speech when the PoS tagger
    can’t figure out whether a word should have a noun, verb, or other PoS tag.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 值得退一步考虑HMM和TensorFlow为PoS标签器提供了什么。如我之前提到的，PoS标签可以推广到试图在PoS标签器无法确定一个单词应该有名词、动词或其他PoS标签时，尝试消除歧义性词性的问题。
- en: 'Similar to the other machine-learning processes that you have encountered in
    this book, the process of creating an HMM to predict the unambiguous PoS tag involves
    a set of training steps. In those steps, you need to create a TensorFlow-based
    HMM model; in turn, you need a way of learning the initial probabilities, transition
    probabilities, and emission probabilities. You provide the HMM some text as input
    (such as a set of sentences). The first part of training involves running a PoS
    tagger that will tag the words in those sentences in an ambiguous way, as I showed
    you with Project Gutenberg earlier. After running the PoS tagger and obtaining
    annotated ambiguous text, you can ask for human feedback to disambiguate the output
    of the tagger. Doing so yields a training set of three parallel corpora of data
    that you can use to construct your HMM model:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与你在本书中遇到的其它机器学习过程类似，创建HMM以预测无歧义PoS标签的过程涉及一系列训练步骤。在这些步骤中，你需要创建一个基于TensorFlow的HMM模型；反过来，你需要一种学习初始概率、转移概率和发射概率的方法。你为HMM提供一些文本作为输入（例如一组句子）。训练的第一部分涉及运行一个PoS标签器，该标签器将以模糊的方式对句子中的单词进行标记，就像我之前在Project
    Gutenberg中展示的那样。在运行PoS标签器并获得标注的模糊文本后，你可以请求人类反馈以消除标签器的输出歧义。这样做会产生一个包含三个并行语料库数据的训练集，你可以用它来构建你的HMM模型：
- en: Input text such as sentences
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入文本，如句子
- en: Annotated sentences with ambiguous PoS tags
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有模糊PoS标签的标注句子
- en: Annotated sentences with disambiguated PoS tags based on human feedback
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于人类反馈消除歧义的标注句子
- en: 'Given those corpora, the HMM model probabilities can be computed as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这些语料库，HMM模型的概率可以按以下方式计算：
- en: Construct a bigram matrix of transition counts.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建转移计数的二元矩阵。
- en: Calculate the transition probabilities for each tag.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个标签的转移概率。
- en: Compute the emission probabilities.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算发射概率。
- en: You construct a bigram matrix of transition counts per PoS tags or a matrix
    counting how many times the pairwise sets of PoS tags follow one another, and
    vice versa. The rows of the matrix are the list of PoS tags, and the columns are
    the list of PoS tags—hence the term *bigram* (a pair of consecutive units, words,
    or phrases). The table may look something like figure 10.4.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你构建一个按PoS标签构建的转移计数的二元矩阵，或者一个计数成对PoS标签相互跟随次数的矩阵，反之亦然。矩阵的行是PoS标签列表，列也是PoS标签列表——因此称为*二元*（连续单元、单词或短语的成对）。表格可能看起来像图10.4。
- en: '![CH10_F04_Mattmann2](../Images/CH10_F04_Mattmann2.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F04_Mattmann2](../Images/CH10_F04_Mattmann2.png)'
- en: Figure 10.4 An example bigram matrix of transition counts for PoS
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 PoS的转移计数示例二元矩阵
- en: Then you take the counts of one PoS tag occurring after another and sum them.
    The transition probabilities are the cell values divided by the sum of the values
    in that particular row. Then, for the initial probabilities, you take the computed
    transition probabilities in the row corresponding to the end-of-sentence tag (`sent`)
    in figure 10.4\. That row of probabilities is the perfect set of initial probabilities
    because the probability of a PoS tag occurring in that row is the probability
    of it occurring at the beginning of a sentence or after the end-of-sentence tag,
    such as the final row highlighted in figure 10.5.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你计算一个PoS标签在另一个标签之后出现的次数并将它们相加。转移概率是单元格值除以该特定行的值之和。然后，对于初始概率，你取图10.4中对应于句子结束标签（`sent`）的行的计算转移概率。这行的概率是完美的初始概率集合，因为该行中PoS标签出现的概率是它在句子开头或句子结束标签之后出现的概率，例如图10.5中突出显示的最后一行。
- en: '![CH10_F05_Mattmann2](../Images/CH10_F05_Mattmann2.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F05_Mattmann2](../Images/CH10_F05_Mattmann2.png)'
- en: Figure 10.5 The initial probabilities are those computed transition probabilities
    for PoS tags in the end-of-sentence/beginning-of-sentence tag (sent).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 初始概率是句子结束/句子开头标签（sent）中PoS标签的计算转移概率。
- en: Finally, you compute the emission probabilities by computing the ratio of ambiguous
    PoS tags and their count of occurrences in the ambiguous corpus to the user-provided
    tags in the disambiguated corpus. If, in the user-provided disambiguated corpus,
    a word should be tagged as a verb seven times, and the PoS ambiguous corpus tagged
    it as `Noun/Verb` three times, `Adjective` two times, and `Noun` two times, the
    emission proba- bility of `Noun/Verb` if the hidden state is truly `Noun` is 3
    divided by 7 or 43%, and so on.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您通过计算歧义词性标签及其在歧义语料库中的出现次数与用户提供的消除歧义语料库中的标签之间的比率来计算发射概率。如果，在用户提供的消除歧义语料库中，一个单词应该被标注为动词七次，而词性歧义语料库将其标注为`Noun/Verb`三次，`Adjective`两次，`Noun`两次，那么当隐藏状态真正是`Noun`时，`Noun/Verb`的发射概率是3除以7，即43%，依此类推。
- en: Given this construction, you can build your initial, transition, and emission
    matrices and then run your HMM, using the TensorFlow code in listing 10.1 and
    the Viterbi algorithm, which can tell you the actual hidden states given observed
    states. In this case, the hidden states are the true PoS tags, and the observed
    tags are the ambiguity-class PoS tags. Figure 10.6 summarizes this discussion
    and pinpoints how TensorFlow and HMMs can help you disambiguate text.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在此结构下，您可以构建您的初始、转换和发射矩阵，然后运行您的隐马尔可夫模型（HMM），使用列表10.1中的TensorFlow代码和维特比算法，该算法可以告诉您给定观察状态的实际隐藏状态。在这种情况下，隐藏状态是真实的词性标注（PoS），而观察到的标签是歧义类词性标注。图10.6总结了这次讨论，并指出了TensorFlow和HMM如何帮助您消除文本歧义。
- en: '![CH10_F06_Mattmann2](../Images/CH10_F06_Mattmann2.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F06_Mattmann2](../Images/CH10_F06_Mattmann2.png)'
- en: Figure 10.6 The training and prediction steps for the HMM-based PoS tagger.
    Both steps require input text (sentences), and the PoS tagger annotates them ambiguously.
    In the training portion, humans provide unambiguous PoS for the input sentences
    to train the HMM on. In the prediction stage, the HMM predicts the unambiguous
    PoS. The input text, annotated text with ambiguous PoS, and annotated text with
    disambiguated PoS form the three corpora needed to build an HMM.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6 基于HMM的词性标注器的训练和预测步骤。这两个步骤都需要输入文本（句子），词性标注器对它们进行歧义标注。在训练部分，人类为输入句子提供无歧义的词性以训练HMM。在预测阶段，HMM预测无歧义的词性。输入文本、带有歧义词性的标注文本和带有消除歧义词性的标注文本构成了构建HMM所需的三个语料库。
- en: Now that I’ve gone over the machine-learning problem setup, you can get your
    coding fingers ready to write the function to perform initial PoS tagging and
    generate the ambiguous corpora.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我已经介绍了机器学习问题的设置，您可以准备好编写函数来执行初始词性标注并生成歧义语料库。
- en: 10.2.2 Generating the ambiguity PoS tagged dataset
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.2 生成歧义词性标注数据集
- en: In the field of natural language processing, the process of taking in text and
    then analyzing it for its PoS is called morphological analysis. If you consider
    the sentences that my children said earlier and then perform morphological analysis
    on them, you’ll have the first two of three corpora needed to start PoS tagging.
    Remember that the output of the morphological analysis is the ambiguous PoS text.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理领域，将文本输入并对其词性进行分析的过程称为形态分析。如果您考虑我孩子们之前说的句子，并对它们进行形态分析，您将得到开始词性标注所需的三个语料库中的前两个。请记住，形态分析的结果是歧义词性文本。
- en: To perform the analysis, you can use the Python Natural Language ToolKit (NLTK)
    library, which provides handy text analysis code for reuse. The `word_tokenize`
    function breaks a sentence into individual words. Then you can check each word
    against the `pos_words` dictionary you computed in listing 10.4\. For each ambiguity,
    you get a word with multiple PoS tags, so you’ll need to collect and output them
    as part of the input word. In listing 10.7, you’ll create an `analyse` function
    that performs the morphological analysis step and returns the ambiguously tagged
    sentences.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行此分析，您可以使用Python自然语言工具包（NLTK）库，它提供了方便的文本分析代码以供重用。`word_tokenize`函数将句子分解成单个单词。然后您可以将每个单词与列表10.4中计算的`pos_words`字典进行对比。对于每个歧义，您会得到一个具有多个词性标注的单词，因此您需要收集并输出它们作为输入单词的一部分。在列表10.7中，您将创建一个`analyse`函数，该函数执行形态分析步骤并返回歧义标注的句子。
- en: Listing 10.7 Performing morphological analysis on input sentences
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.7 对输入句子进行形态分析
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Imports NLTK and tokenizes the input sentence
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入NLTK并对输入句子进行分词
- en: ❷ Iterates through each word
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历每个单词
- en: ❸ Creates a combined string of the word and its PoS tag
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建单词及其词性标签的组合字符串
- en: ❹ If the word is known to the PoS tags from Project Gutenberg, check its possible
    PoS.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 如果单词已知来自Project Gutenberg的词性标签，检查其可能的词性。
- en: ❺ If the word is the end-of-sentence tag, add that special PoS.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 如果单词是句子结束标签，添加那个特殊的词性。
- en: ❻ Returns the analyzed words and their PoSes joined by whitespace
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 返回由空格连接的分析单词及其词性
- en: Running the code in listing 10.7 produces a list of Python strings. You can
    run `analyse` on each of those strings to generate the first two parts of the
    parallel corpora. For the third part of the corpora, you need an expert to disambiguate
    the PoS tags for you and tell you the right answer. For a handful of sentences,
    it’s almost trivial to identify whether the person was referring to the word as
    a noun or verb, or other PoS class. Because I collected the data for you, I also
    provided the disambiguated PoS tags that you can teach TensorFlow (in the `tagged_sample_sentences`
    list). Later, you will see that having more knowledge and tagged data helps, but
    for now, you’ll be fine. Lo and behold, listing 10.8 combines the three parallel
    corpora and prepares them for you. Following the construction of the three corpora,
    you can use the Python Pandas library to create a dataframe, which in turn you
    will use to extract a NumPy array of probabilities. Then, after the probabilities
    are extracted, the code in listing 10.9 will help subset out the initial, transition,
    and emission probabilities for each corpus and store the results in a dataframe.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 运行列表10.7中的代码会产生一个Python字符串列表。您可以对每个字符串运行`analyse`来生成平行语料库的前两部分。对于语料库的第三部分，您需要一个专家来为您消歧词性标签并告诉您正确答案。对于少数句子，几乎可以轻易地识别出某人是否将单词作为名词、动词或其他词性类别。因为我为您收集了数据，所以我提供了您可以教授TensorFlow的消歧词性标签（在`tagged_sample_sentences`列表中）。稍后您将看到，拥有更多知识和标记数据是有帮助的，但现在您会做得很好。瞧，列表10.8将三个平行语料库合并在一起，并为您准备好了。在构建三个语料库之后，您可以使用Python
    Pandas库创建一个数据框，然后您将使用它来提取一个概率的NumPy数组。然后，在提取概率之后，列表10.9中的代码将帮助从每个语料库中提取初始、转移和发射概率，并将结果存储在数据框中。
- en: Listing 10.8 Creating the parallel corpora of sentences, analyzed and tagged
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.8 创建句子、分析和标记的平行语料库
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ The input sentences I collected
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我收集的输入句子
- en: ❷ Runs analyse on the sentences and stores the analyzed data in a list
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在句子上运行analyse，并将分析数据存储在列表中
- en: ❸ The disambiguated PoS tags I provided
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我提供的消歧词性（PoS）标签
- en: Listing 10.9 Building a Pandas dataframe of the parallel corpora
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.9 构建平行语料库的Pandas数据框
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Creates a function for building the dataframe
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个用于构建数据框的函数
- en: ❷ Iterates through each of the five sentences and adds each element from the
    three parallel corpora
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历五个句子中的每一个，并将三个平行语料库中的每个元素添加进去
- en: ❸ Returns the dataframe
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回数据框
- en: Using the Pandas dataframe construct allows you to call `df.head` and interactively
    inspect the sentences, their morphological analyzed tags, and human-provided tags
    in a Jupyter notebook to keep track of the data you are working with. You can
    easily slice out a column or row to work with; even better for your purposes,
    you can get a NumPy array back. You see an example of calling `df.head` in figure
    10.7.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Pandas数据框结构，您可以调用`df.head`并交互式地检查句子、它们的形态学分析标签和人类提供的标签，以跟踪您正在处理的数据。您可以轻松地切片出列或行来处理；对于您的目的来说，更好的是，您可以获取一个NumPy数组。您可以在图10.7中看到一个调用`df.head`的示例。
- en: '![CH10_F07_Mattmann2](../Images/CH10_F07_Mattmann2.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F07_Mattmann2](../Images/CH10_F07_Mattmann2.png)'
- en: 'Figure 10.7 Easily view your three parallel corpora with Pandas: the untagged
    original sentences, the (morphologically) analyzed ambiguous corpora, and the
    user-provided tagged corpora without ambiguity.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7 使用Pandas轻松查看您的三个平行语料库：未标记的原始句子、（形态学）分析的模糊语料库，以及用户提供的无歧义的标记语料库。
- en: Next, I’ll show you how to process the dataframe and create the initial probabilities,
    transition probabilities, and emission probabilities. Then you will apply the
    HMM class to predict the disambiguated PoS classes.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将向您展示如何处理数据框并创建初始概率、转移概率和发射概率。然后您将应用HMM类来预测消歧的词性（PoS）类别。
- en: 10.3 Algorithms for building the HMM for PoS disambiguation
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 用于构建PoS消歧的HMM算法
- en: Computing the emission, transition, and initial probability matrices proves
    to be a fairly straightforward activity, given your Pandas dataframe. You can
    slice out a column (such as Analyzed in figure 10.7) corresponding to the morphologically
    analyzed sentences and then extract all the tags from them. As it turns out, taking
    the analyzed and tagged sentences and extracting out the PoS tags is a needed
    utility function. The function `compute_tags` in listing 10.10 enumerates all
    the sentences in one of your dataframe’s analyzed or tagged columns and extracts
    the PoS tags positionally.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在Pandas数据框的条件下，计算发射、转换和初始概率矩阵证明是一项相当直接的活动。你可以从图10.7中的“分析”列等对应的列中切出，然后从中提取所有标签。实际上，从分析和标注的句子中提取词性标注是一个需要的实用函数。列表10.10中的`compute_tags`函数枚举了数据框分析或标注列中的所有句子，并按位置提取词性标注。
- en: Listing 10.10 Extracting tags from a sentence dataframe
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.10 从句子数据框中提取标签
- en: '[PRE12]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Initializes the list of tags to return—a list of tags in the order in which
    they were encountered
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化要返回的标签列表——按遇到顺序排列的标签列表
- en: ❷ Enumerates the sentences in the dataframe and for each sentence, enumerates
    its characters
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 枚举数据框中的句子，并对每个句子枚举其字符
- en: ❸ Catches the end-of-tag value and decides whether there is another tag or whether
    to add this tag
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 捕获标签结束值并决定是否存在另一个标签或是否添加此标签
- en: ❹ If there are two <Noun><Verb> tags (ambiguous), add both tags.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 如果存在两个 <名词><动词> 标签（模糊），则添加两个标签。
- en: ❺ Catches the end-of-tag value and decides whether there is another tag or whether
    to add this tag
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 捕获标签结束值并决定是否存在另一个标签或是否添加此标签
- en: ❻ Collects the tag value
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 收集标签值
- en: ❼ Extracts the analyzed tags (a_all_tags), and human-tagged tags (t_all_tags)
    for all parallel sentences
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 提取所有平行句子中的分析标签（`a_all_tags`）和人工标注标签（`t_all_tags`）
- en: 'Armed with `compute_tags` and the extracted analyzed tags (`a_all_tags`) and
    extracted human-tagged tags (`t_all_tags`), you are ready to begin computing the
    transition probability matrix. You will use `t_all_tags` to build a matrix of
    bigram PoS sentence occurrence counts, as shown in figure 10.4\. Pandas can help
    you construct such an occurrence matrix. You already have a set of the valid PoS
    tags from Project Gutenberg (listing 10.3): the `pt_vals` variable. If you were
    to print the values from that variable, you’d see something like this:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有 `compute_tags` 以及提取的分析标签（`a_all_tags`）和提取的人工标注标签（`t_all_tags`），你就可以开始计算转换概率矩阵了。你将使用
    `t_all_tags` 来构建一个双词性标注句子出现次数的矩阵，如图10.4所示。Pandas可以帮助你构建这样的出现矩阵。你已经从Project Gutenberg（列表10.3）获得了一组有效的词性标注标签（`pt_vals`变量）。如果你要打印该变量的值，你会看到类似以下的内容：
- en: '[PRE13]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Your matrix of PoS tag occurrence counts is a bigram matrix in which the columns
    are a tag. The column `FirstPOS`, followed by PoS tags from `pt_vals`, makes up
    the columns, and the rows are the same values as shown in figure 10.4\. If you
    make the dataframe indexed by the value of `FirstPoS`, you can easily use the
    PoS tag as a key for slicing the dataframe, such as dividing a column by its summed
    values. Listing 10.11 builds the uninitialized (all cells’ value `0`) transition
    probability matrix.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你的词性标注出现次数矩阵是一个二元矩阵，其中列是一个标签。列`FirstPOS`，后跟`pt_vals`中的词性标注，构成了列，行与图10.4中显示的相同值。如果你使数据框按`FirstPoS`的值索引，你可以轻松地使用词性标注作为切片数据框的键，例如通过除以列的总和值。列表10.11构建了一个未初始化的（所有单元格的值为`0`）转换概率矩阵。
- en: Listing 10.11 Computing the transition probability matrix
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.11 计算转换概率矩阵
- en: '[PRE14]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Constructs the dataframe with column FirstPoS representing the first PoS tag
    of the two
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建包含“FirstPoS”列的数据框，代表两个词性标注中的第一个
- en: ❷ Creates the dataframe index on PoS tag
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在词性标注（PoS）标签上创建数据框索引
- en: ❸ Initializes the count to zero
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 初始化计数为零
- en: ❹ Returns the transition probability matrix dataframe
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 返回转换概率矩阵数据框
- en: When you have the uninitialized dataframe, you need to count the occurrence
    of PoS tags in your tagged sentence corpora. Luckily for you, this data was captured
    in `t_all_tags``,` which you computed in listing 10.10\. A simple algorithm can
    capture the counts of the PoS tags for the transition count matrix from the tagged
    corpus. You need to iterate for all sentences the captured PoS tags, capture the
    tags that are co-occurring, and then sum their occurrences. The two special cases
    are the beginning of the sentences after the first sentence (preceded by as end-of-sentence
    tag) and the final sentence (which ends with a `sent` PoS tag). Because the dataframe
    is a pass by reference, you can update its count values (the `cnt` variable at
    the specified array `[row_idx, col_idx]`) to fill the cell counts. The function
    `compute_trans_matrix` in listing 10.12 does the heavy lifting for you.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当你拥有未初始化的数据框时，你需要统计你标记的句子语料库中PoS标签的出现次数。幸运的是，这些数据已经被捕获在`t_all_tags`中，这是你在第10.10列表中计算的。一个简单的算法可以从标记语料库中捕获PoS标签的计数，用于转换计数矩阵。你需要遍历所有句子中捕获的PoS标签，捕获共现的标签，然后计算它们的出现次数。两个特殊情况是第一个句子之后的句子开头（由一个句子结束标签
    precede）和最后一个句子（以`snt` PoS标签结束）。因为数据框是通过引用传递的，所以你可以更新其计数值（在指定数组 `[row_idx, col_idx]`
    中的`cnt`变量）来填充单元格计数。第10.12列表中的`compute_trans_matrix`函数为你做了繁重的工作。
- en: Listing 10.12 Counting the occurrence of PoS tags in the tagged corpus
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.12 计算标记语料库中PoS标签的出现次数
- en: '[PRE15]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Iterates each sentence in the PoS tagged corpus dataframe
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历PoS标记语料库数据框中的每个句子
- en: ❷ Initializes the row, column index (tt_idx) to a blank list. The column index
    will have only two PoS tags.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将行和列索引（tt_idx）初始化为空列表。列索引将只有两个PoS标签。
- en: ❸ Iterates the tags for this sentence from the tagged corpus
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 遍历这个句子从标记语料库中的标签
- en: ❹ Adds the first element of the row, column index as the current PoS tag for
    the sentence
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将行和列索引的第一元素作为句子的当前PoS标签
- en: ❺ For any sentence after the first and the first tag, the first element is always
    the end-of-sentence tag from the preceding sentence.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 对于第一个句子之后的任何句子和第一个标签，第一个元素总是前一个句子的句子结束标签。
- en: ❻ Increments count at row index and column index
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 在行索引和列索引处增加计数
- en: ❼ If we have two elements for row, column index, we are ready to increment the
    count there.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 如果我们有行和列索引的两个元素，我们就准备好在那里增加计数。
- en: ❽ If we’ve reached the end-of-sentence PoS tag for the column, we grab the preceding
    tag for our row index and increment the count.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 如果我们达到了列的句子结束PoS标签，我们就获取我们行索引的前一个标签并增加计数。
- en: When you have the counts of the co-occurring PoS tags in the matrix, you need
    to do a little bit of postprocessing to turn them into probabilities. You sum
    the counts for each row in the PoS bigram count matrix and then divide each cell
    count in that row by the sum. This process computes the transition probabilities
    and the initial probabilities. Because quite a few steps are involved, I captured
    the key parts in figure 10.8, which is a handy reference for looking at listing
    10.13.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在矩阵中有共现PoS标签的计数时，你需要进行一些后处理，将它们转换为概率。你首先对PoS二元组计数矩阵中的每一行的计数进行求和，然后在该行的每个单元格计数上除以总和。这个过程计算了转换概率和初始概率。因为涉及很多步骤，我在图10.8中捕捉了关键部分，这是一个查看列表10.13的有用参考。
- en: Listing 10.13 Postprocessing the transition probability matrix
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.13 后处理转换概率矩阵
- en: '[PRE16]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Computes the initial zero count transition probability matrix
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算初始零计数转换概率矩阵
- en: ❷ Drops the FirstPoS column as it is not needed (corresponds to figure 10.8
    step 3)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 删除第一个PoS列，因为它不再需要（对应于图10.8步骤3）
- en: ❸ Adds the sum column—figure 10.8 step 2—and sums counts horizontally for each
    row
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 添加总和列——图10.8步骤2——并对每行的计数进行水平求和
- en: ❹ Avoid divide by zero if there are zero counts and divide by a really small
    number instead.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 如果有零计数，避免除以零，而是除以一个非常小的数。
- en: ❺ Create a new dataframe without the sum column and perform the division in
    place for each cell for all valid PoS tags between Noun and sent by the values
    in the sum column.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 创建一个新的数据框，不包含总和列，并对所有有效的PoS标签（在名词和`snt`之间）在总和列的值上进行就地除法。
- en: Phew! That was a lot of work. Now you will work on the emission probabilities.
    Aside from a couple of more algorithms, the process is fairly straightforward,
    and it’s the last thing that you need to do before running the HMM.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 呼吁！这真是做了很多工作。现在你将处理发射概率。除了几个更多的算法之外，这个过程相当直接，这是你在运行HMM之前需要做的最后一件事。
- en: '![CH10_F08_Mattmann2](../Images/CH10_F08_Mattmann2.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F08_Mattmann2](../Images/CH10_F08_Mattmann2.png)'
- en: Figure 10.8 The operational steps 1-4 to compute on your Pandas dataframe and
    turn transition counts into transition probabilities and initial probabilities
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8 计算操作步骤1-4，在Pandas数据框上操作并将转换计数转换为转换概率和初始概率。
- en: 10.3.1 Generating the emission probabilities
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.1 生成发射概率。
- en: 'You can generate the emission probabilities by creating a bigram matrix of
    emission probabilities from construction, similar to the one you created in section
    10.3\. The major difference is that instead of listing only the valid PoS tags
    for each row, you also list the ambiguity classes—classes such as `Noun/Verb`,
    `Noun/Adjective/Verb`, and `Verb/Adverb`. See, the whole point of the HMM is to
    deal with ambiguity. You can’t observe the hidden state of `Noun` or `Verb` directly,
    so instead you observe the ambiguous class with some emission probability, which
    you can learn from construction. The matrix of emission probability counts is
    constructed as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过创建一个发射概率的大矩阵来生成发射概率，类似于你在第10.3节中创建的矩阵。主要区别在于，你不仅列出每行的有效词性标记，还列出歧义类别——例如“名词/动词”、“名词/形容词/动词”和“动词/副词”这样的类别。看，HMM的全部目的就是处理歧义。你不能直接观察到“名词”或“动词”的隐藏状态，所以你通过一些发射概率观察到模糊的类别，这些概率可以从构造中学习。发射概率计数的矩阵如下构建：
- en: The rows are the ambiguity classes followed by the valid PoS tags from the tagged
    corpus. These rows are the emission variables that you observe instead of the
    hidden states. The columns are the valid PoS tags from the tagged corpus, corresponding
    to what the hidden unambiguous PoS class would be if you could observe it directly.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 行是歧义类别，后面跟着来自标记语料库的有效词性标记。这些行是你观察到的发射变量，而不是隐藏状态。列是来自标记语料库的有效词性标记，对应于如果你能直接观察到它，隐藏的无歧义词性类别会是什么。
- en: The cell values for first and second tag are the number of times that an ambiguity
    class was seen (for the first tag) in the morphological analyzed sentence corpus
    divided by the total number of times the actual PoS tag appeared in the tagged
    corpus.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 第一和第二标记的单元格值是歧义类别在形态分析句子语料库中出现的次数（对于第一标记）除以实际词性标记在标记语料库中出现的总次数。
- en: So if `Noun/Verb` appeared four times in the morphologically analyzed corpus
    and was definitively tagged as `Verb` in the tagged corpus, you take the total
    number of times `Verb` appeared in the tagged corpus, such as six, and you get
    4/6, or .66\. Of the six times that a PoS tag was a `Verb` in the tagged corpus,
    four times the PoS tagger ambiguously thought that the word was a `Noun/Verb`
    during morphological analysis.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果“名词/动词”在形态分析语料库中出现了四次，并在标记语料库中被明确标记为“动词”，那么你将标记语料库中“动词”出现的总次数，例如六次，得到4/6，或0.66。在标记语料库中，有六个“动词”的词性标记，其中四次在形态分析中词性标注器模糊地认为该词是“名词/动词”。
- en: The remainder of the ambiguity classes can be computed the same way. You’ll
    whip up a `build_emission` function in listing 10.14 to create the initial emission
    matrix with placeholders of `0`; then, as before, you’ll create a function to
    process the analyzed and tagged corpora and fill in the emission probabilities.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的歧义类别可以以相同的方式进行计算。你将在列表10.14中创建一个`build_emission`函数，用`0`作为占位符创建初始发射矩阵；然后，像之前一样，你将创建一个函数来处理分析和标记语料库，并填写发射概率。
- en: Listing 10.14 Constructing the initial emission count matrix
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.14 构建初始发射计数矩阵。
- en: '[PRE17]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Adds the First part of speech (PoS) index for the data frame and the columnar
    values corresponding to the tagged corpus valid PoS
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为数据框添加第一个词性（PoS）索引和对应于标记语料库有效词性标记的列值。
- en: ❷ After extracting all tags, collect them as keys in a dictionary.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 提取所有标记后，将它们收集为字典的键。
- en: ❸ Sorts the collected ambiguity classes, which are the keys in the dictionary
    amb_classes
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 对收集到的歧义类别进行排序，这些是字典amb_classes中的键。
- en: ❹ Adds the ambiguity classes as the rows in the emission probability matrix
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将歧义类别作为行添加到发射概率矩阵中。
- en: ❺ Adds the remaining PoS classes as rows, though you will flag a 1 because you
    need only the ambiguity classes
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 添加剩余的词性类别作为行，尽管你只需要标记歧义类别，但你需要标记为1。
- en: Given the output of `build_emission,` which is the `emission_df` or emission
    dataframe, along with the identified ambiguity classes, you can perform the simple
    algorithm discussed earlier to fill the emission probability matrix that you see
    in listing 10.15 as a start. To recap, you will compute the times that a tag in
    the tagged corpus—such as `Verb`—had ambiguity in the morphological analyzed corpora
    and identified the PoS tag as `Noun/Verb` because the model was unsure. The ratio
    of the amount of times that happens, given all the occurrences of `Verb` in the
    tagged corpus, is the emission probability to see the ambiguity class `Noun/Verb`.
    Listing 10.15 creates the initial counts in the matrix of PoS ambiguity classes.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 给定`build_emission`的输出，即`emission_df`或发射数据框，以及确定的歧义类别，你可以执行前面讨论的简单算法，以填充列表10.15中看到的发射概率矩阵作为开始。为了回顾，你将计算在标记语料库中某个标签（如`Verb`）在形态分析语料库中的歧义次数，并将词性标签识别为`Noun/Verb`，因为模型不确定。这种情况发生的次数与标记语料库中`Verb`出现的所有次数的比例是看到歧义类别`Noun/Verb`的发射概率。列表10.15在词性歧义类别的矩阵中创建初始计数。
- en: Listing 10.15 Constructing the emission count matrix
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.15 构建发射计数矩阵
- en: '[PRE18]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ The analyzed ambiguous PoS tag
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 分析的歧义词性标签
- en: ❷ The human-tagged corpora PoS tag
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 人工标记语料库的词性标签
- en: ❸ The ambiguity classes will be of type list because there will be multiple
    possible tags for them. Convert to a string, and use it to index the row_idx/col_idx
    to update its count.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 歧义类别将是列表类型，因为它们将具有多个可能的标签。将其转换为字符串，并使用它来索引row_idx/col_idx以更新其计数。
- en: ❹ Should never happen; tags don’t agree from analyzed and tagged corpora.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 永远不会发生；标签在分析和标记语料库中不一致。
- en: ❺ Update the count of ambiguous PoS class to 1 only one time; otherwise, skip
    updating it.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 仅更新一次歧义词性类别的计数；否则，跳过更新。
- en: To convert the emission count matrix to the emission probability matrix, you
    again do some minimal postprocessing. Because you need to figure out the total
    count of tags in the tagged corpora, it makes sense to write a helper function
    to compute it and save the result as a dictionary of `tag name:count`. The `count_tagged`
    function in listing 10.16 performs this task. The function is generic and can
    count from either the tagged corpora or the analyzed corpora (which will be useful
    later).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 将发射计数矩阵转换为发射概率矩阵，你再次进行一些最小化后处理。因为你需要确定标记语料库中标签的总数，所以编写一个辅助函数来计算它并将结果保存为`标签名称:计数`的字典是有意义的。列表10.16中的`count_tagged`函数执行此任务。该函数是通用的，可以从标记语料库或分析的语料库（稍后会有用）中进行计数。
- en: Listing 10.16 Computing the count of tags in the tagged corpora
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.16 计算标记语料库中标签的计数
- en: '[PRE19]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Gets all the tags for a sentence
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取一个句子的所有标签
- en: ❷ Iterates the tags, if a list; then it’s an ambiguity class and captures its
    subtag counts
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历标签，如果是一个列表；那么它是一个歧义类别，并捕获其子标签计数
- en: ❸ Otherwise, sums the counts
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 否则，汇总计数
- en: After one other bit of postprocessing, you can finish computing the emission
    probabilities in the dataframe. This processing (listing 10.17) performs the divide
    step in each cell with the computed tag counts.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 经过另一项后处理，你就可以完成在数据框中计算发射概率。此处理（列表10.17）在每个单元格中执行除法步骤，使用计算出的标签计数。
- en: Listing 10.17 Dividing each emission ambiguity class count by the tagged corpora
    tag count
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.17 将每个发射歧义类计数除以标记语料库的词性标签计数
- en: '[PRE20]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Collects the row index (the ambiguity class) and the column index (the PoS
    tag)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 收集行索引（歧义类别）和列索引（词性标签）
- en: ❷ Grabs the count of the particular PoS tag
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取特定词性标签的计数
- en: ❸ Divides the count of the ambiguity class by the PoS-tagged corpora tag count
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将歧义类计数除以词性标记语料库的词性标签计数
- en: Now you are ready to compute the emission probabilities dataframe fully. The
    process is fairly trivial at this point; you chain your processing steps, as shown
    in listing 10.18\. Build the initial zero’ed emission dataframe with PoS tags
    from the tagged corpus as columns and ambiguity classes as rows from the analyzed
    corpus. Then you count the occurrence of ambiguity classes compared with the tagged
    corpora analog, and compute the ratio of those counts to the total number of times
    the unambiguous PoS class appeared in the tagged corpora. The code is shown in
    listing 10.18.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经准备好完全计算发射概率数据框。在这个阶段，这个过程相当简单；你将你的处理步骤链接起来，如列表10.18所示。使用标记语料库中的词性标签作为列和从分析语料库中作为行的歧义类别构建初始零值发射数据框。然后你计算与标记语料库中类似歧义类别的发生次数，并计算这些计数的比例与在标记语料库中出现的无歧义词性类别的总次数。代码显示在列表10.18中。
- en: Listing 10.18 Computing the emission probability dataframe
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.18 计算发射概率数据框
- en: '[PRE21]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Builds the zero’ed-out emission count dataframe
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建零值发射计数数据框
- en: ❷ Computes the counts of ambiguity classes
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算模糊类计数
- en: ❸ Computes the total counts of PoS tags in the tagged corpora
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算标记语料库中PoS标签的总计数
- en: ❹ Divides the ambiguity class counts by unambiguous PoS class total count and
    removes the sum column
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将模糊类计数除以无歧义PoS类总计数并删除求和列
- en: ❺ Drops the FirstPoS index column
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 删除FirstPoS索引列
- en: Now that you have all three probabilities computed, you can finally run the
    HMM! I’ll show you how in section 10.4.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经计算了所有三个概率，你最终可以运行HMM了！我将在第10.4节中展示如何操作。
- en: 10.4 Running the HMM and evaluating its output
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4 运行HMM并评估其输出
- en: 'You’ve written all the necessary code (which turned out to be quite a bit)
    to prepare your three parallel corpora and to use those corpora as training input
    for your HMM in TensorFlow. It’s time to see the fruits of your labor. Recall
    that you have two Pandas dataframes at this point: `just_trans_df`, and `just_emission_df`.
    The row in `just_trans_df` corresponding to the sentence (`sent`) tag is the initial
    probabilities, as already mentioned, so you have all three pieces of data for
    your HMM model.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经编写了所有必要的代码（结果证明相当多）来准备你的三个并行语料库，并使用这些语料库作为TensorFlow中HMM的训练输入。现在是时候看到你劳动的成果了。回想一下，你现在有两个Pandas数据框：`just_trans_df`和`just_emission_df`。`just_trans_df`中与句子(`sent`)标签对应的行是初始概率，如前所述，因此你有了HMM模型所需的所有三个数据片段。
- en: But as you remember, you need NumPy arrays for TensorFlow to work its magic.
    The good news is that these arrays are crazy simple to get out of Pandas dataframes
    with the handy helper function `.values``,` which returns a NumPy array of values
    for the matrix inside. Coupled with the `.astype('float64')` function, you have
    an easy way to grab all three of the NumPy arrays you need. The code in listing
    10.19 handles this task for you. The only tricky part is transposing the values
    from the emission probabilities to ensure that it is indexed by PoS tag and not
    ambiguity class. (In short, you flip the rows and columns.)
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 但正如你所记得的，TensorFlow要发挥其魔力，你需要NumPy数组。好消息是，你可以通过方便的辅助函数`.values`轻松地从Pandas数据框中获取这些数组，该函数返回矩阵内部的值构成的NumPy数组。结合`.astype('float64')`函数，你就有了一个简单的方法来获取所需的三个NumPy数组。列表10.19中的代码为你处理了这个任务。唯一棘手的部分是将发射概率的值转置，以确保它是按PoS标签而不是模糊类索引的。（简而言之，你需要交换行和列。）
- en: Listing 10.19 Getting the NumPy arrays for your HMM
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.19 获取HMM的NumPy数组
- en: '[PRE22]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Gets the sent row values and returns a NumPy array (16, 1)
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取句子行值并返回一个(16, 1)大小的NumPy数组
- en: ❷ Gets the transition probabilities as a (16, 16) NumPy array
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取转移概率作为一个(16, 16)大小的NumPy数组
- en: ❸ Gets the emission probabilities by transposing the (36, 16) array of PoS ambiguity
    classes into a (16, 36) array indexed by PoS class
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过将(36, 16)的PoS模糊类数组转置为(16, 36)的数组，并按PoS类索引来获取发射概率
- en: The code in listing 10.19 gives you three NumPy arrays of size (16, 1), (16,
    16), and (16, 36) for initial probabilities, transition probabilities, and emission
    probabilities, respectively. With these arrays, you can use your TensorFlow HMM
    class and run the Viterbi algorithm to reveal the hidden states, which are the
    actual PoS tags, given the ambiguities.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.19中的代码为你提供了三个大小为(16, 1)、(16, 16)和(16, 36)的NumPy数组，分别对应初始概率、转移概率和发射概率。有了这些数组，你可以使用你的TensorFlow
    HMM类并运行Viterbi算法来揭示隐藏状态，即实际的PoS标签，考虑到模糊性。
- en: One helper function you need to write is a simple way to convent a sentence
    to its PoS ambiguity class observations. You can use the original emission dataframe
    you made to look up the index of a particular observed PoS tag/ambiguity class
    from the morphologically analyzed corpus and then collect those indexes in a list.
    You also need the reverse function that converts predicted indices to their corresponding
    PoS tag. Listing 10.20 handles these tasks for you.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要编写的一个辅助函数是将句子简单地转换为它的PoS模糊类观察值的一种方法。你可以使用你制作的原始发射数据框来查找从形态学分析的语料库中特定观察到的PoS标签/模糊类索引，然后收集这些索引到一个列表中。你还需要一个反向函数，将预测的索引转换为相应的PoS标签。列表10.20为你处理这些任务。
- en: Listing 10.20 Converting a sentence to its PoS ambiguity class observations
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.20 将句子转换为其PoS模糊类观察值
- en: '[PRE23]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Gets the sentence and its PoS tags/ambiguity classes
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取句子及其PoS标签/模糊类
- en: ❷ Gets the index of observation from the emission dataframe
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从发射数据框中获取观察值的索引
- en: ❸ Takes a predicted set of PoS indices and returns the PoS tag name
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 接受一组预测的词性索引，并返回词性标签名称
- en: ❹ Computes the PoS tag names and returns them as a list for all predicted observations
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算词性标签名称，并将它们作为列表返回所有预测的观察结果
- en: 'Now you can get your HMM and run it on a random sentence from the initial five
    that I curated for you. I picked sentence index 3, shown in its ambiguous analyzed
    form first and in its tagged disambiguated form second:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以得到你的HMM并在你为我精心挑选的前五个随机句子上运行它。我选择了句子索引3，首先以它的歧义分析形式展示，然后以它的标记消除歧义形式展示：
- en: '[PRE24]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Without further ado, you can run your TensorFlow HMM class. Give it a try in
    listing 10.21.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 不再拖延，你可以运行你的TensorFlow HMM类。在列表10.21中试一试。
- en: Listing 10.21 Running the HMM on your parallel corpora
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.21 在你的平行语料库上运行HMM
- en: '[PRE25]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ The index of the third sentence in the morphologically analyzed parallel corpora
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 形态分析平行语料库中第三句话的索引
- en: ❷ Converts the sentence’s PoS ambiguity classes to observation indices from
    the emission matrix, or [9, 23, 1, 3, 20, 4, 23, 31, 18, 14, 13, 35]
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将句子的词性歧义类别转换为从发射矩阵到观察索引，或[9, 23, 1, 3, 20, 4, 23, 31, 18, 14, 13, 35]
- en: ❸ Initializes the TensorFlow HMM model with the computed initial, emission,
    and transition probabilities
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用计算出的初始、发射和转移概率初始化TensorFlow HMM模型
- en: ❹ Runs the Viterbi algorithm and predicts the most likely hidden states
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 运行维特比算法并预测最可能的隐藏状态
- en: ❺ Converts the predicted internal state indices to PoS tags
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将预测的内部状态索引转换为词性标签
- en: 'When you run listing 10.21, you get some interesting results:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行列表10.21时，你会得到一些有趣的结果：
- en: '[PRE26]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'If you compare the predicted output, you see that the first two tags were predicted
    correctly, but after that, every other hidden state was predicted to be `Noun`,
    which is clearly wrong. Why did the code make those incorrect predictions? The
    answer boils down to lack of data and the algorithm’s ability to make confident
    predictions. It’s the age-old machine-learning issue: without enough data, the
    model isn’t finely tuned. The model can make predictions but has not seen enough
    examples to delineate the necessary PoS tag classes properly. How can you address
    this problem?'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你比较预测的输出，你会发现前两个标签被正确预测了，但之后，每个其他的隐藏状态都被预测为`名词`，这显然是错误的。代码为什么做出了那些错误的预测？答案归结为数据不足和算法做出自信预测的能力。这是机器学习的一个老问题：没有足够的数据，模型无法进行精细调整。模型可以做出预测，但还没有看到足够的例子来正确划分必要的词性标签类别。你该如何解决这个问题？
- en: One way would be to jot down a whole bunch more sentences and then go through
    each sentence with a PoS tagger like Gutenberg. Afterward, I could disambiguate
    the ambiguity classes myself.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是将更多句子写下来，然后用像Gutenberg这样的词性标注器逐句处理。之后，我可以自己消除歧义类别。
- en: Incentivizing data collection
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 激励数据收集
- en: The process of incentivizing collection of annotations has taken off in the
    past decade. Predicted by Tim Berners-Lee in his famous 2001 Scientific American
    article on the semantic web ([https://www.scientificamerican.com/article/the-semantic-web](https://www.scientificamerican.com/article/the-semantic-web/)),
    organizations have been trying to crowdsource valuable annotations from users
    forever. Berners-Lee thought that the benefit of having an intelligent agent handle
    your calendar, much as Siri does today, would be enough to get regular web users
    to write well-curated XML annotations for web pages, an expectation that failed
    miserably. Later, social media companies persuaded users to provide annotations
    for web content by giving them a cool service to keep in touch with their relatives,
    family members, and social connections. They overperformed and collected an amazing
    social corpus by providing the right incentive. In this case, as much as I love
    you guys, I didn’t have the time to collect more than a handful of PoS annotations.
    Luckily for you, many other people have already done that. Read on to find out
    how to use their work.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 过去十年中，激励收集注释的过程已经展开。蒂姆·伯纳斯-李在他的著名2001年《科学美国人》关于语义网的科学文章中预测了这一点([https://www.scientificamerican.com/article/the-semantic-web](https://www.scientificamerican.com/article/the-semantic-web/))，组织机构一直在尝试从用户那里众包有价值的注释。伯纳斯-李认为，拥有一个智能代理来处理你的日历，就像今天的Siri一样，将足以让普通网络用户为网页编写精心制作的XML注释，这个期望彻底失败了。后来，社交媒体公司通过提供一种让用户与亲戚、家庭成员和社会联系保持联系的服务来说服用户为网络内容提供注释。他们做得过火了，通过提供正确的激励，收集了一个惊人的社会语料库。在这种情况下，尽管我非常喜欢你们，但我没有时间收集超过几个词性标注。幸运的是，许多其他人已经做了这件事。继续阅读，了解如何使用他们的工作。
- en: This solution is possible, especially given the current times, when the little
    ones are around the house a great deal more often. But why invest human labor
    when there are plenty of other sources of tagged corpora? One such source is the
    Brown Corpus that’s part of the PNLTK.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这种解决方案是可能的，尤其是在当前时代，孩子们在家的时间比以往任何时候都要多。但是，当有大量其他标记语料库来源时，为什么还要投入人力呢？其中一个来源是布朗语料库，它是
    PNLTK 的一部分。
- en: 10.5 Getting more training data from the Brown Corpus
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.5 从布朗语料库获取更多训练数据
- en: The Brown Corpus is the first million-word electronic corpus of English words
    collected from more than 500 sources of information, such as news and editorials,
    created in 1961 at Brown University. The corpus is organized by genre, annotated
    with PoS tags and other structures. You can read more about the corpus at [https://www.nltk.org/
    book/ch02.html](https://www.nltk.org/book/ch02.html).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 布朗语料库是 1961 年在布朗大学创建的第一个包含来自 500 多个信息来源（如新闻和社论）的百万词英语词汇电子语料库。语料库按体裁组织，并标注了词性标记和其他结构。您可以在[https://www.nltk.org/book/ch02.html](https://www.nltk.org/book/ch02.html)了解更多关于语料库的信息。
- en: 'The Brown Corpus has various text articles, organized by genre or chapter,
    that contain annotated sentences. You can pull out 100 sentences and their corresponding
    PoS tags from chapter 7 of the corpus, for example (listing 10.22). One caveat
    is that not all corpora are tagged with the same PoS tag set. Instead of using
    the Project Gutenberg set of PoS tags—the 16 that you have seen thus far in this
    chapter—the Brown Corpus is tagged with the Universal tag set, a set of 14 PoS
    tags defined by Slav Petrov, Dipanjan Das, and Ryan McDonald in a 2011 paper ([https://arxiv.org/
    abs/1104.2086](https://arxiv.org/abs/1104.2086)). You see an example of the Universal
    tag set output classes, which instead of short codes and descriptions such as
    `"A" : "Adjective"`, as you see in Project Gutenberg, you get `"ADJ" : "Adjective"`.
    I’ve done the heavy lifting for you, though, mapping a subset of the overlap between
    the tag sets and recorded it in listing 10.23\. In the future, you can decide
    to map more of the overlap, but the listing gives you an idea of the process.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 布朗语料库包含各种按体裁或章节组织的文本文章，其中包含标注的句子。例如，您可以从中提取第 7 章的 100 个句子及其相应的词性标记（列表 10.22）。一个注意事项是，并非所有语料库都使用相同的词性标记集。而不是使用您在本章中迄今为止看到的
    Project Gutenberg 的词性标记集——16 个标记——布朗语料库使用的是通用标记集，这是一个由 Slav Petrov、Dipanjan Das
    和 Ryan McDonald 在 2011 年的一篇论文中定义的 14 个词性标记集（[https://arxiv.org/abs/1104.2086](https://arxiv.org/abs/1104.2086)）。不过，我已经为您做了大量工作，将标记集之间的子集重叠映射并记录在列表
    10.23 中。将来，您可以决定映射更多的重叠，但列表为您提供了这个过程的一个想法。
- en: Listing 10.22 Exploring PoS tags from 100 sentences in the Brown Corpus
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.22 从布朗语料库的 100 个句子中探索词性标记
- en: '[PRE27]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ Imports the Brown Corpus and its PoS tags in the Universal tag set
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入布朗语料库及其在通用标记集中的词性标记
- en: ❷ Prints the PoS tagged sentences from chapter 7 in the Brown Corpus to discern
    the format
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打印布朗语料库第 7 章的词性标记句子，以识别格式
- en: ❸ Prints the number of sentences from chapter 7 (122)
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打印第 7 章的句子数量（122）
- en: The output from listing 10.22 is worth a look to get a feel for how the Brown
    Corpus is recorded, because you are going to process it and prepare it in a dataframe
    as you did for my small set of example sentences. The output is a set of lists;
    each list contains a tuple corresponding to the word and its associated PoS tag
    from the Universal tag set. Because these assignments are unambiguous, you can
    treat them as your user-provided tagged corpora from the set of three parallel
    corpora that you need to train the disambiguated PoS tagger. The mapping of the
    Universal tag set to the Gutenberg tags is provided in listing 10.23.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.22 的输出值得一看，以了解布朗语料库的记录方式，因为您将像处理我的小例句集一样处理它并准备它在一个数据框中。输出是一系列列表；每个列表包含一个与单词及其关联的通用标记集中的词性标记相对应的元组。因为这些分配是无歧义的，您可以将其视为您从三个平行语料库集中提供的用户提供的标记语料库。通用标记集到
    Gutenberg 标记集的映射在列表 10.23 中提供。
- en: '[PRE28]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Listing 10.23 Mapping of Universal tag set to Project Gutenberg tag set
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.23 通用标记集到 Project Gutenberg 标记集的映射
- en: '[PRE29]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ The set of tag identifiers and full names for PoS tags from the Universal
    tag set
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通用标记集中词性标记的标识符和全名集合
- en: ❷ The mapping of overlapping tags from Project Gutenberg to consider in the
    Universal tag set keyed by the Universal tag set identifier
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 需要考虑从Project Gutenberg到通用标签集的重复标签映射，该标签集以通用标签集标识符为键
- en: ❸ Creates a reverse index by Project Gutenberg short PoS tag identifier
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过Project Gutenberg简短PoS标签标识符创建反向索引
- en: With the mapping of overlap between the Project Gutenberg and Universal tag
    sets, you have the tagged corpus. But you need a way to remove the tags and get
    back to the original sentences so you can run them through Project Gutenberg and
    get the ambiguous sentences for use in training your parallel corpora. NLTK provides
    a handy `untag` function that takes care of this task. Run `untag` on a tagged
    sentence; it returns the original sentence (without the tuples). So you have the
    original sentences and the annotated tagged corpora, but you need to make a simple
    update to your morphological analyzer to handle the mapping between Project Gutenberg
    and the Universal tag sets.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Project Gutenberg和通用标签集之间的重叠映射，您有了标记语料库。但是，您需要一种方法来删除标签并返回原始句子，以便您可以在Project
    Gutenberg中运行它们并获取用于训练平行语料库的模糊句子。NLTK提供了一个方便的`untag`函数，可以处理这个任务。对标记句子运行`untag`；它返回原始句子（不带元组）。因此，您有了原始句子和标记的注释语料库，但您需要更新您的形态分析器以处理Project
    Gutenberg和通用标签集之间的映射。
- en: 'The handy `analyse` function that you wrote in listing 10.7 needs some updating
    in a few areas:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 您在列表10.7中编写的方便的`analyse`函数需要在几个方面进行更新：
- en: '`white_list` *variable* —The ability to take in the Gutenberg to universal
    tag-set mapping and use it to map from the Gutenberg PoS ambiguous tagger so that
    your `pos_words` mapping represents to the corresponding universal tag set.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`white_list` *变量* — 能够接收Gutenberg到通用标签集映射，并使用它将Gutenberg PoS模糊标签器映射到您的`pos_words`映射，以便您的`pos_words`映射表示相应的通用标签集。'
- en: '`tagged_sent` *variable* —The tagged sentences with existing PoS tags from
    the NLTK annotations, used to ensure that you consider only the universal tag-set
    ground-truth tags for which there are corresponding Gutenberg tags.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tagged_sent` *变量* — 包含现有PoS标签的标记句子，来自NLTK的注释，用于确保您只考虑有对应Gutenberg标签的通用标签集真实标签。'
- en: '`map_tags` *variable* —Some valid universal tag-set PoS tags don’t have a Gutenberg
    corollary, so I took the liberty of mapping them for you. `DET` (for *determinant*),
    for example, doesn’t have a great mapping in Gutenberg, so I mapped it to `CONJ`
    (for *conjunction*) for you. This example could use improving, but for illustrative
    purposes, it works fine.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map_tags` *变量* — 一些有效的通用标签集PoS标签在Gutenberg中没有对应项，因此我擅自为您进行了映射。例如，`DET`（代表*限定词*）在Gutenberg中没有很好的映射，所以我将其映射到`CONJ`（代表*连词*）。这个例子可能需要改进，但为了说明目的，它工作得很好。'
- en: Listing 10.24 has the updated `analyse` function that will handle and create
    all three parallel corpora from the Brown Corpus.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.24包含更新的`analyse`函数，该函数将处理并从Brown语料库创建所有三个平行语料库。
- en: Listing 10.24 Updating the analyse function to learn the three parallel corpora
    from Brown
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.24 更新analyse函数以从Brown语料库学习三个平行语料库
- en: '[PRE30]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ Remaps some tags from the Universal tag set to Project Gutenberg tags that
    aren’t equivalent
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将一些通用标签集中的标签重新映射到Project Gutenberg标签，这些标签不相等
- en: ❷ Tokenizes the sentence into words by using NLTK
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用NLTK将句子分词成单词
- en: ❸ Whitelist is the allowable set of tags that Project Gutenberg and Universal
    tag set both recognize.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 白名单是Project Gutenberg和通用标签集都认可的允许标签集。
- en: ❹ Creates a reverse index of the whitelist by identifier
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 通过标识符创建白名单的反向索引
- en: ❺ The PoS tag isn’t in the whitelist, so skip it.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ PoS标签不在白名单中，因此跳过它。
- en: ❻ Tagged_sent is the set of actual tags from a truth set for this sentence.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ Tagged_sent是来自此句子的真实集的实际标签集合。
- en: ❼ If the tag is in the whitelist, consider it.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 如果标签在白名单中，则考虑它。
- en: ❽ The tagged corpus tag disagrees with label set, so pick the PoS tag for its
    corresponding identifier in Project Gutenberg.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 标记语料库的标签与标签集不一致，因此选择其在Project Gutenberg中对应标识符的PoS标签。
- en: ❾ Tagged annotation doesn’t correspond to Gutenberg.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 标记的注释与Gutenberg不对应。
- en: Now you can create the three parallel corpora. I grabbed the first 100 of 132
    sentences from the Brown Corpus to perform the training. This process can be computationally
    intensive because these sentences are 20 times the data you were using before,
    and there are many more tags to use to update your HMM model. In practice, this
    approach would scale even better if you fed it all of the Brown Corpus from various
    chapters, but this way, you’ll get a real-world example and won’t have to wait
    hours for it to run. Listing 10.25 sets up the parallel corpora and creates the
    new PoS dataframe for training.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以创建三个并行语料库。我从132个句子中抓取了前100个句子来自布朗语料库进行训练。这个过程可能计算量很大，因为这些句子是之前使用数据的20倍，并且有更多的标签用于更新你的HMM模型。在实践中，如果你将布朗语料库中各个章节的所有内容都输入进去，这种方法会扩展得更好，但这样你将得到一个真实世界的例子，而且不需要等待数小时才能运行。列表10.25设置了并行语料库并创建了新的用于训练的词性标注数据框。
- en: Listing 10.25 Preparing the Brown Corpus for training
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.25 准备布朗语料库以进行训练
- en: '[PRE31]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: To remind you of where you are in the process of making better PoS disambiguation
    predictions, it’s worthwhile to recap. You saw that my limited sentences didn’t
    have enough PoS tags and tagged corpora to learn from. By using NLTK and datasets
    like the Brown Corpus, which has thousands of tagged sentences and corpora, you
    embarked down a path of learning the parallel corpora from 100 Brown sentences.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提醒你在制作更好的词性消歧预测过程中的位置，回顾一下是值得的。你看到我的句子有限，没有足够的词性标签和标记语料库来学习。通过使用NLTK和像布朗语料库这样的数据集，它有数千个标记的句子和语料库，你开始从100个布朗句子学习并行语料库。
- en: You couldn’t use the sentences directly; the `analyse` function required updating
    to take into account the fact that Brown and other corpora use a different PoS
    tag set from the Project Gutenberg corpus tag set. I showed you how to create
    a mapping to take that fact into account and to make sure that the Gutenberg PoS
    tagger morphologically analyzed only the sentence and output tags that were present
    in the Brown tags. In listing 10.25, you used this information to get back to
    your PoS dataframe of three parallel corpora and to extract the analyzed and tagged
    corpora PoS tags. You’ve completed steps 1 and 2 of figure 10.9.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 你不能直接使用句子；`analyse`函数需要更新以考虑布朗和其他语料库使用与Project Gutenberg语料库标签集不同的词性标注集的事实。我向你展示了如何创建一个映射来考虑这一事实，并确保Gutenberg词性标注器仅对句子进行形态分析，并输出存在于布朗标签中的标签。在列表10.25中，你使用这些信息回到你的三个并行语料库的词性标注数据框，并提取了分析和标记的语料库词性标签。你已经完成了图10.9的步骤1和2。
- en: '![CH10_F09_Mattmann2](../Images/CH10_F09_Mattmann2.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F09_Mattmann2](../Images/CH10_F09_Mattmann2.png)'
- en: Figure 10.9 Using real-world datasets and PoS tags to create and train your
    TensorFlow HMM for PoS disambiguation tagging
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9 使用真实世界数据集和词性标注（PoS）标签来创建和训练你的TensorFlow HMM进行词性消歧标注
- en: To get to step 3 in figure 10.9, you need to run the algorithms on the Pandas
    dataframe representing the three parallel corpora and generate the transition,
    initial, and emission matrices. You did this on the small five-sentence dataset,
    so you can use those existing functions and run them again on the dataframe to
    get ready for the HMM (listing 10.26).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 要到达图10.9的步骤3，你需要在对代表三个并行语料库的Pandas数据框上运行算法，生成转移、初始和发射矩阵。你在小的五句数据集上已经这样做过了，所以你可以使用那些现有的函数，再次在数据框上运行它们，为HMM做准备（列表10.26）。
- en: Listing 10.26 Generating the transition and emission count matrices
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.26 生成转移和发射计数矩阵
- en: '[PRE32]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ Builds the transition matrix
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建转移矩阵
- en: ❷ Avoids dividing by zero
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 避免除以零
- en: ❸ Builds the emission matrix
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 构建发射矩阵
- en: With the transition and emission matrices built, you extract the NumPy arrays
    and load the HMM model. Again, you grab the location of the last PoS tag. Because
    there are 16 columns and the dataframe is indexed by 0, it is index 15 for the
    initial probabilities row. Then extract the values of the transition and emission
    probabilities from their respective dataframes (listing 10.27).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 构建了转移和发射矩阵后，你提取NumPy数组并加载HMM模型。再次，你获取最后一个词性标签的位置。因为有16列，而数据框是通过0索引的，所以初始概率行是索引15。然后从它们各自的数据框（列表10.27）中提取转移和发射概率的值。
- en: Listing 10.27 Generating the NumPy arrays
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.27 生成NumPy数组
- en: '[PRE33]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ❶ Extracts the initial probabilities from the sent PoS row in the transition
    probabilities of shape (16, 1)
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从形状为（16，1）的转移概率中的句子词性行中提取初始概率
- en: ❷ Extracts the transition probabilities of shape (16, 16).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 提取形状为（16，16）的转移概率。
- en: ❸ Extracts the emission probabilities of shape (16, 50).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 提取形状为(16, 50)的发射概率
- en: 'Next, in listing 10.28, I picked a sentence randomly again. I picked the sentence
    at index 3 for illustration but could have picked any from the Brown Corpus. The
    sentence is shown in its original form, ambiguous analyzed form, and tagged form
    for your perusal:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在列表10.28中，我再次随机选择了一个句子。为了说明，我选择了索引为3的句子，但也可以从布朗语料库中任意选择。句子以原始形式、模糊分析形式和标记形式展示供您参考：
- en: '[PRE34]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The code in listing 10.28 runs the sentence through the TensorFlow HMM PoS disambiguation
    step. It should look familiar because it is basically the same as listing 10.21.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.28中的代码将句子通过TensorFlow HMM词性歧义消除步骤运行。它看起来很熟悉，因为它基本上与列表10.21相同。
- en: Listing 10.28 Picking a sentence and running the HMM
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.28 选择一个句子并运行HMM
- en: '[PRE35]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ❶ Selects the sentence at index 3
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 选择索引为3的句子
- en: ❷ Converts the ambiguous PoS tags to evidence observations from their indices
    [33, 23, 7, 34, 11, 34, 34, 34, 40, 34, 34, 34, 21, 41, 34, 49]
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将模糊的词性标签转换为从索引[33, 23, 7, 34, 11, 34, 34, 34, 40, 34, 34, 34, 21, 41, 34,
    49]的证据观察值
- en: ❸ Creates the HMM with the learned initial, transition, and emission probabilities
    from the Brown Corpus
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用从布朗语料库学习到的初始、转移和发射概率创建HMM
- en: ❹ Runs TensorFlow and makes predictions to disambiguate the PoS tags
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 运行TensorFlow并做出预测以消除词性标签的歧义
- en: ❺ Outputs the predicted hidden states
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 输出预测的隐藏状态
- en: 'Running the HMM this time generates a few valid predictions of elements besides
    `Noun`s, in particular two more PoS predictions:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这次运行HMM除了`名词`之外还生成了一些有效预测的元素，特别是两个更多的词性预测：
- en: '[PRE36]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The challenge with HMMs is the same as that in any machine-learning model:
    the more examples you show, the better the representational variables represent
    even unforeseen cases. Because `Noun`s tend to surround many of the other PoS
    tags in sentence structure, they will be the most selected or highly probable
    guess. That said, our HMM from the Brown Corpus seems to perform better than our
    five-sentence example, which represented a handful of PoS tags and their co-occurrences.
    There were 50 PoS tags and ambiguity classes this time compared with 36 in the
    first example, so you can deduce that you’ve seen more ambiguity examples and
    trained your TensorFlow model to recognize them.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: HMM的挑战与任何机器学习模型中的挑战相同：你展示的例子越多，表示变量就能更好地代表甚至未预见的情况。因为`名词`往往围绕句子结构中的许多其他词性，它们将是选择最多或最可能猜测的词性。话虽如此，从布朗语料库中得到的我们的HMM似乎比我们的五句示例表现更好，后者代表了一小部分词性及其共现。这次有50个词性和歧义类别，而第一个例子中只有36个，所以你可以推断你已经看到了更多的歧义示例，并且训练了你的TensorFlow模型来识别它们。
- en: You can do better in terms of showing that your model improved with more data,
    however. Let’s measure it!
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在展示你的模型随着更多数据而改进方面，你可以做得更好。让我们来衡量一下！
- en: 10.6 Defining error bars and metrics for PoS tagging
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.6 定义词性标注的错误条和度量标准
- en: A simple way to compute how well your PoS tagger is doing is to define a couple
    of simple metrics. The first metric is the per-line or per-sentence error rate,
    which boils down to how many tags your PoS tagger predicted correctly. This metric
    is measured as
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 计算你的词性标注器表现如何的一个简单方法是定义几个简单的度量标准。第一个度量标准是每行或每句的错误率，这归结为你标注器预测正确的标签数量。这个度量标准测量如下：
- en: '![CH10_F09EQ01_Mattmann2](../Images/CH10_F09EQ01_Mattmann2.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F09EQ01_Mattmann2](../Images/CH10_F09EQ01_Mattmann2.png)'
- en: where TLp is the predicted PoS tags for the sentence or line L and TLt is the
    actual tagged PoS tags for the sentence or line L. The equation takes the number
    of tags predicted correctly out of the total possible tags. Then it divides by
    the total amount of valid tags to predict. This type of equation is commonly known
    as a containment equation because the numerator represents how much the prediction
    captured what was contained in the denominator out of the valid tags to predict.
    The per-sentence error rate is useful for measuring the accuracy of your algorithm,
    and you can add a hyperparameter that is the acceptable threshold per sentence
    that you will allow. After experimenting with the Brown Corpus, I’ve determined
    that a threshold value of .4, or 40%, is an acceptable rate for each sentence.
    The algorithm considers a sentence to be correct if it correctly predicted at
    least 60% of the PoS tags.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 TLp 是句子或行 L 的预测 PoS 标记，TLt 是句子或行 L 的实际标记 PoS 标记。该方程从可能的总标记数中取出预测正确的标记数。然后除以要预测的有效标记总数。这种类型的方程通常被称为包含方程，因为分子表示预测捕获了分母中有效标记的多少。每句错误率对于衡量算法的准确度很有用，并且你可以添加一个超参数，即每句可接受的阈值。在实验了布朗语料库之后，我确定阈值值为
    .4，即 40%，是每句可接受的比率。算法认为，如果至少正确预测了 60% 的 PoS 标记，则句子是正确的。
- en: 'Another metric you can use to evaluate your PoS tagger captures all diffs for
    all sentences and then computes the total number of diffs compared with the total
    number of PoS tags to predict for all sentences. This technique can provide an
    overall accuracy assessment for your algorithm. Running listing 10.29 tells you
    that your HMM predicted 73 of the total 100 sentences correctly (73%) with a 40%
    threshold error rate, and that overall, your algorithm incorrectly identified
    only 254 of a possible 1,219 PoS tags it could predict, yielding overall accuracy
    of 79%. Not bad! You were right: your model performed better.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用另一个指标来评估你的 PoS 标记器，该指标捕获所有句子的所有差异，然后计算与所有句子要预测的总 PoS 标记数相比的总差异数。这种技术可以为你的算法提供一个整体准确度评估。运行列表
    10.29 会告诉你，你的 HMM 正确预测了总 100 个句子中的 73 个（73%），错误率阈值为 40%，并且总体而言，你的算法错误地识别了可能预测的
    1,219 个 PoS 标记中的 254 个，从而得到总体准确度为 79%。还不错！你是对的：你的模型表现更好。
- en: Try running TensorFlow and your HMM again, this time capturing these metrics
    as shown in listing 10.29.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 再次运行 TensorFlow 和你的 HMM，这次按照列表 10.29 中所示捕获这些指标。
- en: Listing 10.29 Capturing and printing per-sentence error rate, diffs, and overall
    accuracy
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.29 捕获和打印每句错误率、差异和总体准确度
- en: '[PRE37]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ❶ Reinitializes TensorFlow
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 重新初始化 TensorFlow
- en: ❷ Defines the metrics you want to capture
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义你想要捕获的指标
- en: ❸ Records the per-line/per-sentence error rate results for each line
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 记录每行/每句错误率结果
- en: ❹ Runs TensorFlow and the HMM for 100 sentences in the Brown Corpus
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在布朗语料库中对 100 个句子运行 TensorFlow 和 HMM
- en: ❺ Computes per-line error rate and adds it to line_diffs
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算每行错误率并将其添加到 line_diffs
- en: ❻ Saves the number of PoS tag diffs
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 保存 PoS 标记差异的数量
- en: ❼ Saves the number of PoS tags possible
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 保存可能的 PoS 标记数量
- en: ❽ Defines the per-line error_rate threshold
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 定义每行的错误率阈值
- en: ❾ Counts how many sentences were predicted correctly based on the error_rate
    threshold
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 根据错误率阈值计算正确预测的句子数量
- en: ❿ Prints the number of total diffs, total possible PoS tags, and overall accuracy
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 打印总差异数、总可能的 PoS 标记数和总体准确度
- en: One last thing you can do is visualize the fruits of your labor, taking a look
    at the per-line error rate graphically with Matplotlib. After you develop a good
    PoS tagger, it’s worth looking for any particular trends on certain sentences.
    Listing 10.30 and figure 10.10 show the Matplotlib Python code and the output
    of the visualized error rate.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以做的一件事是可视化你的劳动成果，使用 Matplotlib 图形化地查看每行的错误率。在你开发了一个好的 PoS 标记器之后，查找某些句子上的任何特定趋势是值得的。列表
    10.30 和图 10.10 显示了 Matplotlib Python 代码和可视化错误率的输出。
- en: Evaluating the graph about every 20 sentences or so, the PoS tagger performs
    horribly. But overall, the performance is strong. This situation could warrant
    further evaluation. You could shuffle your sentences in the Brown Corpus, for
    example, or inspect the underperforming sentences to determine whether they are
    representative of PoS tags that you haven’t learned well. But I will leave that
    analysis for a later date. You’ve done a great job sticking with me in this chapter
    and learning a real-world application of HMMs. Onward!
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 大约每20个句子评估一次图表，PoS标签器的表现非常糟糕。但总体而言，性能很强。这种情况可能需要进一步评估。例如，您可以随机打乱布朗语料库中的句子，或者检查表现不佳的句子，以确定它们是否代表了您尚未很好地学习的PoS标签。但我将把那项分析留到以后。您在这章中一直跟着我，学习HMMs的实际应用，做得很好。继续前进！
- en: Listing 10.30 Using Matplotlib to visualize the per-line error rate
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.30 使用Matplotlib可视化每行错误率
- en: '[PRE38]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ❶ Sorted by key, returns a list of tuples of the form (line num, error rate)
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 按键排序，返回形式为（行号，错误率）的元组列表
- en: ❷ Unpacks a list of pairs into two tuples
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将一对列表拆分为两个元组
- en: ❸ Shows the plot
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 显示图表
- en: '![CH10_F10_Mattmann2](../Images/CH10_F10_Mattmann2.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![CH10_F10_Mattmann2](../Images/CH10_F10_Mattmann2.png)'
- en: Figure 10.10 The per-line error rate for your PoS disambiguation tagger
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10 您的PoS歧义消解标签器的每行错误率
- en: Summary
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Word-sense disambiguation occurs in everyday life, and you can use machine learning
    and TensorFlow to perform it.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词义消歧在日常生活中发生，您可以使用机器学习和TensorFlow来执行它。
- en: Labeled data is available; you need methods of setting it up for a machine-learning
    problem by using HMMs.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签数据可用；您需要使用HMMs来设置机器学习问题的方法。
- en: HMMs are explainable models that accumulate probabilistic evidence and guide
    decisions based on possible states that the evidence represents.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HMMs是可解释的模型，它们累积概率证据，并根据证据所表示的可能状态来引导决策。
