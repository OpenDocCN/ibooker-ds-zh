- en: '4 Sequential ensembles: Adaptive boosting'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 序列集成：自适应提升
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Training sequential ensembles of weak learners
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练弱学习者的序列集成
- en: Implementing and understanding how AdaBoost works
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现并理解AdaBoost的工作原理
- en: Using AdaBoost in practice
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际应用中的AdaBoost
- en: Implementing and understanding how LogitBoost works
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现并理解LogitBoost的工作原理
- en: The ensembling strategies we’ve seen thus far have been parallel ensembles.
    These include homogeneous ensembles such as bagging and random forests (where
    the same base-learning algorithm is used to train base estimators) and heterogeneous
    ensemble methods such as stacking (where different base-learning algorithms are
    used to train base estimators).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止看到的集成策略都是并行集成。这包括同质集成，如Bagging和随机森林（使用相同的基学习算法来训练基估计器），以及异质集成方法，如Stacking（使用不同的基学习算法来训练基估计器）。
- en: 'Now, we’ll explore a new family of ensemble methods: sequential ensembles.
    Unlike parallel ensembles, which exploit the *independence* of each base estimator,
    sequential ensembles exploit the *dependence* of base estimators. More specifically,
    during learning, sequential ensembles train a new base estimator in such a manner
    that it minimizes mistakes made by the base estimator trained in the previous
    step.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探索一种新的集成方法族：序列集成。与利用每个基础估计器独立性的并行集成不同，序列集成利用基础估计器之间的依赖性。更具体地说，在学习的阶段，序列集成以这种方式训练新的基础估计器，即最小化前一步训练的基础估计器所犯的错误。
- en: The first sequential ensemble method we’ll investigate is *boosting*. Boosting
    aims to combine *weak learners*, or simple base estimators. Put another way, boosting
    literally aims to boost the performance of a collection of weak learners.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要研究的第一个序列集成方法是*提升*。提升的目标是结合*弱学习器*，或简单的基估计器。换句话说，提升实际上旨在提升一组弱学习器的性能。
- en: This is in contrast to algorithms such as bagging, which combine complex base
    estimators, also known as *strong learners*. Boosting commonly refers to AdaBoost,
    or *adaptive boosting*. This approach was introduced by Freund and Schapire in
    1995,[¹](#pgfId-1163290) for which they eventually won the prestigious Gödel Prize
    for outstanding papers in theoretical computer science.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这与诸如Bagging之类的算法形成对比，这些算法结合了复杂的基估计器，也称为*强学习器*。提升通常指的是AdaBoost，或*自适应提升*。这种方法由Freund和Schapire于1995年提出，[¹](#pgfId-1163290)，他们最终因其卓越的理论计算机科学论文而获得了声望极高的哥德尔奖。
- en: Since 1995, boosting has emerged as a core machine-learning method. Boosting
    is surprisingly simple to implement, computationally efficient, and can be used
    with a wide variety of base-learning algorithms. Prior to the reemergence of deep
    learning in the mid-2010s, boosting was widely applied to computer vision tasks
    such as object classification and natural language processing tasks such as text
    filtering.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 自1995年以来，提升（Boosting）已成为机器学习的一个核心方法。提升方法实现起来非常简单，计算效率高，并且可以与多种基础学习算法结合使用。在2010年代中期深度学习重新兴起之前，提升方法被广泛应用于计算机视觉任务，如目标分类，以及自然语言处理任务，如文本过滤。
- en: 'For most of this chapter, we focus on AdaBoost, a popular boosting algorithm
    that is also quite illustrative of the general framework of sequential ensemble
    methods. Other boosting algorithms can be derived by changing aspects of this
    framework, such as the loss function. Such variants are usually not available
    in packages and must be implemented. We also implement one such variant: LogitBoost.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的大部分内容中，我们关注AdaBoost，这是一种流行的提升算法，也是序列集成方法一般框架的很好的说明。通过改变这个框架的某些方面，如损失函数，可以推导出其他提升算法。这些变体通常不在包中提供，必须实现。我们还实现了一个这样的变体：LogitBoost。
- en: 4.1 Sequential ensembles of weak learners
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 弱学习者的序列集成
- en: 'There are two key differences between parallel and sequential ensembles:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 并行和序列集成之间有两个关键区别：
- en: Base estimators in parallel ensembles can usually be trained independently,
    while in sequential ensembles, the base estimator in the current iteration depends
    on the base estimator in the previous iteration. This is shown in figure 4.1,
    where (in iteration *t*) the behavior of base estimator *M*[t-1] influences the
    sample S[t], and the next model *M*[t].
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在并行集成中，基估计器通常可以独立训练，而在序列集成中，当前迭代的基估计器依赖于前一个迭代的基估计器。这如图4.1所示，其中（在迭代*t*）基估计器*M*[t-1]的行为影响了样本S[t]，以及下一个模型*M*[t]。
- en: Base estimators in parallel ensembles are typically strong learners, while in
    sequential ensembles, they are weak learners. Sequential ensembles aim to combine
    several weak learners into one strong learner.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行集成中的基估计器通常是强学习器，而在顺序集成中，它们是弱学习器。顺序集成旨在将多个弱学习器组合成一个强学习器。
- en: '![CH04_F01_Kunapuli](../Images/CH04_F01_Kunapuli.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F01_Kunapuli](../Images/CH04_F01_Kunapuli.png)'
- en: 'Figure 4.1 Differences between parallel and sequential ensembles: (1) base
    estimators in parallel ensembles are trained independently of each other, while
    in sequential ensembles, they are trained to improve on the predictions of the
    previous base estimator; (2) sequential ensembles typically use weak learners
    as base estimators.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 并行和顺序集成的区别：（1）并行集成中的基估计器是相互独立训练的，而在顺序集成中，它们被训练以改进前一个基估计器的预测；（2）顺序集成通常使用弱学习器作为基估计器。
- en: 'Intuitively, we can think of strong learners as professionals: highly confident
    people who are independent and sure about their answers. Weak learners, on the
    other hand, are like amateurs: not so confident and unsure about their answers.
    How can we get a bunch of not-so-confident amateurs to come together? By boosting
    them, of course. Before we see how exactly, let’s characterize what weak learners
    are.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，我们可以将强学习器视为专业人士：高度自信且独立，对自己的答案确信无疑。另一方面，弱学习器则像业余爱好者：不太自信，对自己的答案不确定。我们如何能让一群不太自信的业余爱好者团结起来呢？当然是通过提升他们。在我们具体了解之前，先让我们来描述一下弱学习器的特点。
- en: Weak learners
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 弱学习器
- en: While the precise definition of the strength of learners is rooted in machine-learning
    theory, for our purposes, a strong learner is a good model (or estimator). In
    contrast, a weak learner is a very simple model that doesn’t perform that well.
    The only requirement of a weak learner (for binary classification) is that it
    performs better than random guessing. Put another way, its accuracy needs to be
    only slightly better than 50%. Decision trees are often used as base estimators
    for sequential ensembles. Boosting algorithms typically use decision stumps, or
    decision trees of depth 1 (see figure 4.2).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然学习器强度的精确定义根植于机器学习理论，但就我们的目的而言，强学习器是一个好的模型（或估计器）。相比之下，弱学习器是一个非常简单的模型，表现并不好。弱学习器（对于二元分类）的唯一要求是它必须比随机猜测表现得更好。换句话说，它的准确率只需要略高于50%。决策树通常用作顺序集成的基估计器。提升算法通常使用决策桩，或深度为1的决策树（见图4.2）。
- en: '![CH04_F02_Kunapuli](../Images/CH04_F02_Kunapuli.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F02_Kunapuli](../Images/CH04_F02_Kunapuli.png)'
- en: Figure 4.2 Decision stumps (trees of depth 1, left) are commonly used as weak
    learners in sequential ensemble methods such as boosting. As tree depth increases,
    a decision stump grows into a decision tree, becoming a stronger classifier, and
    its performance improves. However, it isn’t possible to arbitrarily increase the
    strength of classifiers as they will begin to overfit during training, which decreases
    their prediction performance when deployed.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 决策桩（深度为1的树，左侧）在顺序集成方法，如提升法中常用作弱学习器。随着树深度的增加，决策桩会成长为一个决策树，成为一个更强的分类器，其性能也会提高。然而，不能随意增加分类器的强度，因为它们在训练过程中会开始过拟合，这会降低它们部署时的预测性能。
- en: Sequential ensemble methods such as boosting aim to combine several weak learners
    into a single strong learner. These methods literally “boost” weak learners into
    a strong learner.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序集成方法，如提升法，旨在将多个弱学习器组合成一个单一强学习器。这些方法实际上是将弱学习器“提升”为强学习器。
- en: TIP A weak learner is a simple classifier that is easy and efficient to train,
    but generally performs much worse than a strong learner (though better than random
    guessing). Sequential ensembles are generally agnostic to the underlying base-learning
    algorithms, meaning that you can use any classification algorithm as a weak learner.
    In practice, weak learners, such as shallow decision trees and shallow neural
    networks, are common.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示**：弱学习器是一个简单且易于训练的分类器，但通常表现远不如强学习器（尽管比随机猜测要好）。顺序集成通常对底层基学习算法不敏感，这意味着你可以使用任何分类算法作为弱学习器。在实践中，弱学习器，如浅层决策树和浅层神经网络，很常见。'
- en: Recall Dr. Randy Forrest’s ensemble of interns from chapters 1 and 2\. In a
    parallel ensemble of knowledgeable medical personnel, each intern can be considered
    a strong learner. To understand how different the philosophy of sequential ensembles
    is, we turn to Freund and Schapire, who describe boosting as “a committee of blockheads
    that can somehow arrive at highly reasoned decisions.”[²](#pgfId-1163317)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下第一章和第二章中提到的兰迪·福雷斯特博士的实习生团队。在一个由知识渊博的医疗人员组成的并行团队中，每个实习生都可以被视为一个强大的学习者。为了理解顺序集成团队的哲学有多么不同，我们转向弗里德曼和沙皮雷，他们把提升描述为“一个由傻瓜组成的委员会，但不知何故能做出高度合理的决策。”[²](#pgfId-1163317)
- en: This would be akin to Dr. Randy Forrest sending away his interns and deciding
    to crowdsource medical diagnoses instead. While this is certainly a far-fetched
    (and unreliable) strategy for diagnosing a patient, it turns out that “garnering
    wisdom from a council of fools”[³](#pgfId-1163321) works surprisingly well in
    machine learning. This is the underlying motivation for sequential ensembles of
    weak learners.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像是兰迪·福雷斯特博士让他的实习生离开，决定采用众包医疗诊断的策略。虽然这当然是一种（且不可靠的）诊断患者的策略，但“从一群傻瓜那里汲取智慧”[³](#pgfId-1163321)在机器学习中表现得出奇地好。这是弱学习者顺序集成的潜在动机。
- en: '4.2 AdaBoost: Adaptive boosting'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 AdaBoost：自适应提升
- en: 'In this section, we begin with an important sequential ensemble: AdaBoost.
    AdaBoost is simple to implement and computationally efficient to use. As long
    as the performance of each weak learner in AdaBoost is slightly better than random
    guessing, the final model converges to a strong learner. However, beyond applications,
    understanding how AdaBoost works is also key to understanding two state-of-the-art
    sequential ensemble methods we’ll look at in the next couple of chapters: gradient
    boosting and Newton boosting.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍一个重要的顺序集成：AdaBoost。AdaBoost易于实现，使用起来计算效率高。只要AdaBoost中每个弱学习者的性能略好于随机猜测，最终模型就会收敛到一个强学习者。然而，除了应用之外，理解AdaBoost的工作原理也是理解我们将在下一章中探讨的两个最先进的顺序集成方法——梯度提升和牛顿提升——的关键。
- en: A brief history of boosting
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 提升的简要历史
- en: 'The origins of boosting lie in computational learning theory, when learning
    theorists Leslie Valiant and Michael Kearns posed the following question in 1988:
    Can one boost a weak learner to a strong learner? This question was answered affirmatively
    two years later by Rob Schapire in his now landmark paper, “The Strength of Weak
    Learnability.”'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 提升的起源在于计算学习理论，当学习理论家莱斯利·瓦利亚恩特和迈克尔·基恩斯在1988年提出了以下问题：能否将弱学习者提升为强学习者？两年后，罗布·沙皮雷在他的现在已成为里程碑式的论文《弱学习能力的强度》中肯定地回答了这个问题。
- en: The earliest boosting algorithms were limited because weak learners didn’t adapt
    to fix the mistakes made by weak learners trained in previous iterations. Freund
    and
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最早的提升算法受到限制，因为弱学习者没有适应来纠正先前迭代中训练的弱学习者犯的错误。弗里德曼和
- en: Schapire’s AdaBoost, or *adaptive boosting* algorithm, proposed in 1994, ultimately
    addressed these limitations. Their original algorithm endures to this day and
    has been widely applied in several application domains, including text mining,
    computer vision, and medical informatics.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 沙皮雷的AdaBoost，或称*自适应提升*算法，于1994年提出，最终解决了这些限制。他们的原始算法至今仍在使用，并在多个应用领域得到广泛应用，包括文本挖掘、计算机视觉和医学信息学。
- en: '4.2.1 Intuition: Learning with weighted examples'
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 直觉：使用加权示例进行学习
- en: 'AdaBoost is an adaptive algorithm: at every iteration, it trains a new base
    estimator that fixes the mistakes made by the previous base estimator. Thus, it
    needs some way to ensure that the base-learning algorithm prioritizes misclassified
    training examples. AdaBoost does this by maintaining *weights over individual
    training examples*. Intuitively, weights reflect the relative importance of training
    examples. Misclassified examples have higher weights, while correctly classified
    examples have lower weights.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost是一种自适应算法：在每次迭代中，它训练一个新的基估计器来纠正前一个基估计器犯的错误。因此，它需要某种方式来确保基学习算法优先考虑被错误分类的训练示例。AdaBoost通过维持*单个训练示例的权重*来实现这一点。直观地说，权重反映了训练示例的相对重要性。被错误分类的示例具有更高的权重，而正确分类的示例具有较低的权重。
- en: When we train the next base estimator sequentially, the weights will allow the
    learning algorithm to prioritize (and hopefully fix) mistakes from the previous
    iteration. This is the adaptive component of AdaBoost, which ultimately leads
    to a powerful ensemble.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们按顺序训练下一个基估计器时，权重将允许学习算法优先考虑（并希望修复）前一次迭代的错误。这是AdaBoost的自适应组件，最终导致一个强大的集成。
- en: NOTE All machine-learning frameworks use *loss functions* (and, in some cases,
    *likelihood functions*) to characterize performance, and training is essentially
    the process of finding the best-fitting model according to the loss function.
    Loss functions can either treat all training examples equally (by weighting them
    all exactly the same) or focus on some specific examples (by assigning higher
    weights to specific examples to reflect their increased priority). When implementing
    ensemble methods that use weights on training examples, care must be taken to
    ensure that the base-learning algorithm can actually use these weights. Most weighted
    classification algorithms use modified loss functions to prioritize correct classification
    of examples with higher weights.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：所有机器学习框架都使用 *损失函数*（在某些情况下，也使用 *似然函数*）来描述性能，而训练本质上是根据损失函数找到最佳拟合模型的过程。损失函数可以平等地对待所有训练示例（通过将它们都赋予相同的权重）或关注一些特定的示例（通过将特定示例赋予更高的权重以反映其增加的优先级）。当实现使用训练示例权重的集成方法时，必须注意确保基学习算法实际上可以使用这些权重。大多数加权分类算法使用修改后的损失函数来优先考虑具有更高权重的示例的正确分类。
- en: 'Let’s visualize the first few iterations of boosting. Each iteration performs
    the same steps:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化提升的前几次迭代。每次迭代执行相同的步骤：
- en: Train a weak learner (here, a decision stump) that learns a model to ensure
    that training examples with higher weights are prioritized.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个弱学习器（这里是一个决策树），以确保具有更高权重的训练示例被优先考虑。
- en: Update the weights of the training examples such that misclassified examples
    are assigned higher weights; the worse the error, the higher the weight.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新训练示例的权重，使得错误分类的示例被赋予更高的权重；错误越严重，权重越高。
- en: Initially (iteration *t* - 1), all examples are initialized with *equal weights*.
    The decision stump trained in iteration 1 (figure 4.3) is a simple, axis-parallel
    classifier with an error rate of 15%. The misclassified points are plotted larger
    than the correctly classified points.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时（迭代 *t* - 1），所有示例都使用 *相等权重* 初始化。第1次迭代中训练的决策树（如图4.3所示）是一个简单、轴平行的分类器，错误率为15%。被错误分类的点比正确分类的点画得更大。
- en: The next decision stump (in iteration 2, as shown in figure 4.4) to be trained
    must correctly classify the examples misclassified by the previous decision stump
    (in iteration 1). Thus, mistakes are weighted higher, which enables the decision-tree
    algorithm to prioritize them during learning.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个要训练的决策树（在第2次迭代中，如图4.4所示）必须正确分类前一个决策树（第1次迭代中）错误分类的示例。因此，错误被赋予更高的权重，这使得决策树算法能够在学习过程中优先考虑它们。
- en: '![CH04_F03_Kunapuli](../Images/CH04_F03_Kunapuli.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F03_Kunapuli](../Images/CH04_F03_Kunapuli.png)'
- en: Figure 4.3 Initially (iteration 1), all the training examples are weighted equally
    (and hence plotted with the same size on the left). The decision stump learned
    on this data set is shown on the right. The correctly classified examples are
    plotted with smaller markers compared to the misclassified examples, which are
    plotted with larger markers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 初始时（第1次迭代），所有训练示例都赋予相等的权重（因此左图中以相同大小绘制）。在此数据集上学习到的决策树显示在右侧。与错误分类的示例相比，正确分类的示例用较小的标记绘制，而错误分类的示例用较大的标记绘制。
- en: '![CH04_F04_Kunapuli](../Images/CH04_F04_Kunapuli.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F04_Kunapuli](../Images/CH04_F04_Kunapuli.png)'
- en: Figure 4.4 At the start of iteration 2, training examples misclassified in iteration
    1 (shown with larger markers in figure 4.3, right) are assigned higher weights.
    This is visualized on the left, where each example’s size is proportional to its
    weight. Since weighted examples have higher priority, the new decision stump in
    the sequence (right) ensures that these are now correctly classified. Observe
    that the new decision stump on the right correctly classifies most of the misclassified
    examples (shown larger) on the left.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 在第2次迭代的开始时，第1次迭代中错误分类的训练示例（如图4.3右侧所示，用较大的标记表示）被赋予更高的权重。这在上图中可视化，其中每个示例的大小与其权重成比例。由于加权示例具有更高的优先级，序列中的新决策树（右侧）确保这些现在被正确分类。观察右侧的新决策树正确分类了左侧大多数错误分类的示例（用较大的标记表示）。
- en: The decision stump trained in the second iteration does indeed correctly classify
    the training examples with higher weights, though it makes mistakes of its own.
    In iteration 3, a third decision stump can be trained that aims to rectify these
    mistakes (see figure 4.5).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次迭代中训练的决策树确实正确分类了具有更高权重的训练示例，尽管它也有自己的错误。在迭代3中，可以训练第三个决策树，旨在纠正这些错误（见图4.5）。
- en: '![CH04_F05_Kunapuli](../Images/CH04_F05_Kunapuli.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F05_Kunapuli](../Images/CH04_F05_Kunapuli.png)'
- en: Figure 4.5 At the start of iteration 3, training examples misclassified in iteration
    2 (shown with larger markers in figure 4.4, right) are assigned higher weights.
    Note that misclassified points also have different weights. The new decision stump
    in the sequence trained in this iteration (right) ensures that these are now correctly
    classified.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 在迭代3的开始时，迭代2中错误分类的训练示例（在图4.4的右侧以较大的标记显示）被分配了更高的权重。请注意，错误分类的点也有不同的权重。在这个迭代中训练的新决策树（右侧）确保这些现在被正确分类。
- en: 'After three iterations, we can combine the three individual weak learners into
    a strong learner, shown in figure 4.6\. Following are some useful points to note:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 经过三次迭代后，我们可以将三个单独的弱学习器组合成一个强学习器，如图4.6所示。以下是一些需要注意的有用点：
- en: Observe the weak estimators trained in the three iterations. They are all different
    from each other and classify the problem in diversely different ways. Recall that
    at each iteration, base estimators are trained on the same training set but with
    *different weights*. Reweighting allows AdaBoost to train a different base estimator
    at each iteration, one that is often different from an estimator trained at the
    previous iterations. Thus, *adaptive reweighting*, or updating adaptively, promotes
    ensemble diversity.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察在三个迭代中训练的弱估计器。它们彼此不同，并以多种不同的方式对问题进行分类。回想一下，在每次迭代中，基估计器都在相同的训练集上训练，但具有*不同的权重*。重新加权允许AdaBoost在每次迭代中训练不同的基估计器，通常与之前迭代中训练的估计器不同。因此，*自适应重新加权*或自适应更新，促进了集成多样性。
- en: The resulting ensemble of weak (and linear) decision stumps is stronger (and
    nonlinear). More precisely, each base estimator had training error rates of 15%,
    20%, and 25%, respectively, while their ensemble has an error rate of 9%.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果的弱（和线性）决策树集成更强（和非线性）。更确切地说，每个基估计器的训练错误率分别为15%、20%和25%，而它们的集成错误率为9%。
- en: '![CH04_F06_Kunapuli](../Images/CH04_F06_Kunapuli.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F06_Kunapuli](../Images/CH04_F06_Kunapuli.png)'
- en: Figure 4.6 The three weak decision stumps shown in the previous figures can
    be boosted into a stronger ensemble.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 前几个图中显示的三个弱决策树可以通过提升变成一个更强的集成。
- en: As noted earlier, this boosting algorithm earns its name from boosting the performance
    of weak learners into a more powerful and complex ensemble, a strong learner.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这个提升算法得名于提升弱学习者的性能，使其成为一个更强大、更复杂的集成，即强学习器。
- en: 4.2.2 Implementing AdaBoost
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 实现AdaBoost
- en: 'First, we’ll implement our own version of AdaBoost. As we do so, we’ll keep
    the following key properties of AdaBoost in mind:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将实现自己的AdaBoost版本。在这个过程中，我们将牢记AdaBoost的以下关键特性：
- en: AdaBoost uses decision stumps as base estimators, which can be trained extremely
    quickly, even with a large number of features. Decision stumps are *weak learners*.
    Contrast this to bagging, which uses deeper decision trees, that is, strong learners.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaBoost使用决策树作为基估计器，即使有大量特征，也可以非常快速地进行训练。决策树是*弱学习器*。这与使用更深决策树的bagging方法形成对比，后者是*强学习器*。
- en: AdaBoost keeps track of *weights on individual training examples.* This allows
    AdaBoost to ensure ensemble diversity by *reweighting* training examples. We saw
    how reweighting helped AdaBoost learn different base estimators in the visualizations
    in the previous subsection. Contrast this to bagging and random forests, which
    use resampling of training examples.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaBoost跟踪*各个训练示例的权重*。这允许AdaBoost通过*重新加权*训练示例来确保集成多样性。我们在前一小节的可视化中看到了重新加权如何帮助AdaBoost学习不同的基估计器。这与使用训练示例重采样的bagging和随机森林形成对比。
- en: AdaBoost keeps track of *weights on individual base estimators*. This is similar
    to combination methods, which weight each classifier differently.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaBoost跟踪*各个基估计器的权重*。这类似于组合方法，它们对每个分类器进行不同的加权。
- en: 'AdaBoost is fairly straightforward to implement. The basic algorithmic outline
    at the *t*th iteration can be described by the following steps:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost的实现相当直接。第*t*次迭代的算法基本框架可以描述为以下步骤：
- en: Train a weak learner *h*[t](*x*) using the weighted training examples, (*x*[i],*y*[i],*D*[i]).
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用加权训练示例(*x*[i],*y*[i],*D*[i])训练弱学习器*h*[t](*x*)。
- en: Compute the training error *ϵ*[t] of the weak learner *h*[t](*x*).
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算弱学习器*h*[t](*x*)的训练错误*ϵ*[t]。
- en: Compute the weight of the weak learner *α*[t] that depends on *ϵ*[t].
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算依赖于*ϵ*[t]的弱学习器*α*[t]的权重。
- en: 'Update the weights of the training examples, as follows:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式更新训练示例的权重：
- en: Increase the weight of misclassified examples by *D*[i]*e*^(α[t]).
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过*D*[i]*e*^(α[t])增加被错误分类的示例的权重。
- en: Decrease the weight of misclassified examples by *D*[i]/*e*^(α[i]).
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过*D*[i]/*e*^(α[i])减少被错误分类的示例的权重。
- en: 'At the end of *T* iterations, we have weak learners *h*[t] along with the corresponding
    weak learner weight *α*[t]. The overall classifier after *t* iterations is just
    a weighted ensemble:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在*T*次迭代结束时，我们有了弱学习器*h*[t]以及相应的弱学习器权重*α*[t]。经过*t*次迭代的整体分类器只是一个加权集成：
- en: '![CH04_F06_Kunapuli-eqs-2x](../Images/CH04_F06_Kunapuli-eqs-2x.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F06_Kunapuli-eqs-2x](../Images/CH04_F06_Kunapuli-eqs-2x.png)'
- en: 'This form is a *weighted linear combination of base estimators*, similar to
    the linear combinations used by parallel ensembles we’ve seen previously, such
    as bagging, combination methods, or stacking. The main difference from those methods
    is that the base estimators used by AdaBoost are weak learners. Now, we need to
    answer two key questions:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这种形式是**基础估计器的加权线性组合**，类似于我们之前看到的并行集成中使用的线性组合，例如bagging、组合方法或stacking。与这些方法的主要区别在于，AdaBoost使用的基础估计器是弱学习器。现在，我们需要回答两个关键问题：
- en: How do we update the weights on the training examples, *D*[i]?
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何更新训练示例的权重，*D*[i]？
- en: How do we compute the weight of each base estimator, *α*[t]?
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何计算每个基础估计器的权重，*α*[t]？
- en: 'AdaBoost uses the same intuition as the combination methods we’ve seen previously
    in chapter 3\. Recall that weights are computed to reflect base estimator *performance*:
    base estimators with better performance (say, accuracy) should have higher weights
    than those with worse performance.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost使用与我们在第3章中看到的组合方法相同的直觉。回想一下，权重是计算来反映基础估计器*性能*的：表现更好的基础估计器（例如，准确率）应该比表现较差的估计器具有更高的权重。
- en: Weak learner weights
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 弱学习器权重
- en: 'At each iteration *t*, we train a base estimator *h*[t](*x*). Each base estimator
    (which is also a weak learner) has a corresponding weight *α*[t] that depends
    on its training error. The training error *ϵ*[t] of  *h*[t](*x*) is a simple and
    immediate measure of its performance. AdaBoost computes the weight of estimator
    *h*[t](*x*) as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个迭代*t*中，我们训练一个基础估计器*h*[t](*x*)。每个基础估计器（也是弱学习器）都有一个相应的权重*α*[t]，它取决于其训练错误。*h*[t](*x*)的训练错误*ϵ*[t]是其性能的一个简单直接的度量。AdaBoost按照以下方式计算估计器*h*[t](*x*)的权重：
- en: '![CH04_F06_Kunapuli-eqs-3x](../Images/CH04_F06_Kunapuli-eqs-3x.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F06_Kunapuli-eqs-3x](../Images/CH04_F06_Kunapuli-eqs-3x.png)'
- en: 'Why this particular formulation? Let’s look at the relationship between *α*[t]
    and the error *ϵ*[t] by visualizing how *α*[t] changes with increasing error *ϵ*[t]
    (figure 4.7). Recall our intuition: better-performing base estimators (those with
    lower errors) must be weighted higher so that their contribution to the ensemble
    prediction is higher.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么是这个特定的公式？让我们通过可视化*α*[t]如何随着错误*ϵ*[t]的增加而变化来观察*α*[t]与错误*ϵ*[t]之间的关系（图4.7）。回想一下我们的直觉：表现更好的基础估计器（那些错误率更低的）必须被赋予更高的权重，以便它们对集成预测的贡献更高。
- en: Conversely, the weakest learners perform the worst. Sometimes, they are barely
    better than random guessing. Put another way, in a binary classification problem,
    the weakest learners are only slightly better than flipping a coin to decide the
    answer.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，最弱的学习器表现最差。有时，它们几乎与随机猜测一样好。换句话说，在二元分类问题中，最弱的学习器仅略好于掷硬币来决定答案。
- en: '![CH04_F07_Kunapuli](../Images/CH04_F07_Kunapuli.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F07_Kunapuli](../Images/CH04_F07_Kunapuli.png)'
- en: Figure 4.7 AdaBoost assigns stronger learners (which have lower training errors)
    higher weights, and assigns weaker learners (which have higher training errors)
    lower weights.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 AdaBoost将更强的学习器（具有更低的训练错误）赋予更高的权重，并将较弱的学习器（具有更高的训练错误）赋予较低的权重。
- en: Concretely, the weakest learners have error rates only slightly better than
    0.5 (or 50%). These weakest learners have the lowest weights, *α*[t] ≈ 0\. The
    strongest learners achieve training errors close to 0.0 (or 0%). These learners
    have the highest weights.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，最弱的学习器的错误率仅略好于0.5（或50%）。这些最弱的学习器具有最低的权重，*α*[t] ≈ 0\. 最强的学习器达到的训练错误接近0.0（或0%）。这些学习器具有最高的权重。
- en: Training example weights
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 训练示例权重
- en: The base estimator weight (*α*[t]) can also be used to update the weights of
    each training example. AdaBoost updates example weights as
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 基础估计器权重（*α*[t]）也可以用来更新每个训练样本的权重。AdaBoost按照以下方式更新样本权重：
- en: '![CH04_F07_Kunapuli-eqs-4x](../Images/CH04_F07_Kunapuli-eqs-4x.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F07_Kunapuli-eqs-4x](../Images/CH04_F07_Kunapuli-eqs-4x.png)'
- en: 'When examples are correctly classified, the new weight is decreased by *e*^(α[t])
    : *D*[i]^(*t*+1) = *D*[i]^(*t*)/*e*^(α[t]). Stronger base estimators will decrease
    the weight more because they are more confident in their correct classification.
    Similarly, when examples are misclassified, the new weight is increased by *e*^(α[t])
    : *D*[i]^(*t*+1) = *D*[i]^(*t*) ⋅ *e*^(α[t]).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '当样本被正确分类时，新的权重会减少*e*^(α[t]) : *D*[i]^(*t*+1) = *D*[i]^(*t*)/*e*^(α[t])。更强的基估计器会减少更多的权重，因为它们对自己的正确分类更有信心。同样，当样本被错误分类时，新的权重会增加*e*^(α[t])
    : *D*[i]^(*t*+1) = *D*[i]^(*t*) ⋅ *e*^(α[t])。'
- en: In this manner, AdaBoost ensures that misclassified training examples receive
    higher weights, which will then be better classified in the next iteration, *t*+1\.
    For example, let’s say we have two training examples *x*[1] and *x*[2], both with
    weights *D^t*[1]= *D^t*[2] = 0.75\. The current weak learner *h*[t] has weight
    *α*[t] = 1.5\. Let’s say *x*[1] is correctly classified by *h*[t]; hence, its
    weight should decrease by a factor of *e*^(*α*[t]). The new weight for the next
    iteration *t*+1 will be *D*[i]^(*t*+1) = *D*[1]/*e*^(*α*[t]) = 0.75/*e*^(1.5)
    - 0.17.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，AdaBoost确保错误分类的训练样本获得更高的权重，这将使它们在下一个迭代*t*+1中更好地被分类。例如，假设我们有两个训练样本*x*[1]和*x*[2]，它们的权重都是*D^t*[1]=
    *D^t*[2] = 0.75。当前的弱学习器*h*[t]的权重是*α*[t] = 1.5。假设*x*[1]被*h*[t]正确分类；因此，其权重应减少一个因子*e*^(*α*[t])。下一个迭代*t*+1的新权重将是*D*[i]^(*t*+1) = *D*[1]/*e*^(*α*[t])
    = 0.75/*e*^(1.5) - 0.17。
- en: Conversely, if *x*[1] is misclassified by *h*[t], its weight should increase
    by a factor of *e*^(*α*[t]). The new weight will be *D*[i]^(*t*+1) = *D*[2] ⋅
    *e*^(*α*[t]) = 0.75 ⋅ *e*^(1.5) = 3.36\. This is illustrated in figure 4.8.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果*x*[1]被*h*[t]错误分类，其权重应增加一个因子*e*^(*α*[t])。新的权重将是 *D*[i]^(*t*+1) = *D*[2]
    ⋅ *e*^(*α*[t]) = 0.75 ⋅ *e*^(1.5) = 3.36。这如图4.8所示。
- en: '![CH04_F08_Kunapuli](../Images/CH04_F08_Kunapuli.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F08_Kunapuli](../Images/CH04_F08_Kunapuli.png)'
- en: Figure 4.8 In iteration *t*, two training examples, *x*[1] and *x*[2], have
    the same weights. *x*[1] is correctly classified, while *x*[2] is misclassified
    by the current base estimator *h*[t]. As the goal in the next iteration is to
    learn a classifier *h*[t+1] that fixes the mistakes of *h*[t], AdaBoost increases
    the weight of the misclassified example *x*[2], while decreasing the weight of
    the correctly classified example *x*[1].This allows the base-learning algorithm
    to prioritize *x*[2] during training in iteration *t*+1.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 在迭代*t*中，两个训练样本*x*[1]和*x*[2]具有相同的权重。*x*[1]被正确分类，而*x*[2]被当前的基础估计器*h*[t]错误分类。由于下一个迭代的目标是学习一个分类器*h*[t+1]，它可以纠正*h*[t]的错误，AdaBoost增加了错误分类样本*x*[2]的权重，同时减少了正确分类样本*x*[1]的权重。这允许基础学习算法在迭代*t*+1中优先考虑*x*[2]。
- en: Training with AdaBoost
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost训练
- en: The AdaBoost algorithm is easy to implement. The following listing shows training
    for boosting.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost算法易于实现。以下列表展示了提升的训练过程。
- en: Listing 4.1 Training an ensemble of weak learners using AdaBoost
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.1 使用AdaBoost训练弱学习器集成
- en: '[PRE0]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Nonnegative weights, initialized to 1
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 非负权重，初始化为1
- en: ❷ Initializes an empty ensemble
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化一个空的集成
- en: ❸ Normalizes the weights so they sum to 1
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将权重归一化，使它们的总和为1
- en: ❹ Trains a weak learner (*h*[t]) with weighted examples
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用加权样本训练弱学习器(*h*[t])
- en: ❺ Computes the training error (*ε*[t]) and the weight (*α*[t]) of the weak learner
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算训练误差(*ε*[t])和弱学习器的权重(*α*[t])
- en: '❻ Updates the example weights: increase for misclassified examples, decrease
    for correctly classified examples'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 更新样本权重：错误分类的样本增加，正确分类的样本减少
- en: ❼ Saves the weak learner and its weight
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 保存弱学习器和其权重
- en: Once we have a trained ensemble, we can use it to make predictions. Listing
    4.2 shows how to predict new test examples using the boosted ensemble. Observe
    that this is identical to making predictions with other weighted ensemble methods
    such as stacking.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有一个训练好的集成，我们就可以用它来进行预测。列表4.2展示了如何使用提升集成来预测新的测试样本。观察发现，这与使用其他加权集成方法（如堆叠）进行预测是相同的。
- en: Listing 4.2 Making predictions with AdaBoost
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.2 使用AdaBoost进行预测
- en: '[PRE1]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Initializes all the predictions to 0
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将所有预测初始化为0
- en: ❷ Makes weighted prediction for each example
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对每个样本进行加权预测
- en: ❸ Converts weighted predictions to -1/1 labels
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将加权预测转换为-1/1标签
- en: 'We can use these functions to fit and predict on a data set:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些函数来拟合和预测数据集：
- en: '[PRE2]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Generates a synthetic classification data set of 200 points
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生成一个包含200个点的合成分类数据集
- en: ❷ Converts 0/1 labels to -1/1 labels
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将0/1标签转换为-1/1标签
- en: ❸ Splits into training and test sets
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将数据集分为训练集和测试集
- en: ❹ Trains an AdaBoost model using listing 4.1
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用列表4.1训练AdaBoost模型
- en: ❺ Makes predictions with this AdaBoost using listing 4.2
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用列表4.2中的AdaBoost进行预测
- en: 'How did we do? We can compute the overall test set accuracy of our model:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做得怎么样？我们可以计算我们模型的总体测试集准确率：
- en: '[PRE3]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This produces the following output:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE4]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The test error of the ensemble learned by our implementation using 10 weak stumps
    is 2%.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用10个弱树桩通过实现学习得到的集成测试错误率为2%。
- en: 'Training labels for binary classification: 0/1 or -1/1?'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类的训练标签：0/1还是-1/1？
- en: The boosting algorithm we’ve implemented requires negative examples and positive
    examples to be labeled -1 and 1, respectively. The function make_moons creates
    labels *y* with negative examples labeled 0 and positive examples labeled 1, respectively.
    We manually convert them from 0 and 1 to -1 and 1, respectively, with *y*[converted]
    = 2 ⋅ *y*[original] - 1.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现的提升算法要求负样本和正样本分别标记为-1和1。函数make_moons创建带有负样本标记为0和正样本标记为1的标签*y*。我们手动将它们从0和1转换为-1和1，即*y*[converted]
    = 2 ⋅ *y*[original] - 1。
- en: Abstractly, labels for each class in a binary classification task can be anything
    we like, as long as the labels are helpful in clearly distinguishing between the
    two classes. Mathematically, this choice depends on the loss function. If using
    the cross-entropy loss, for example, the classes need to be 0 and 1 for the loss
    function to work correctly. In contrast, if using the hinge loss in SVMs, the
    classes need to be -1 and 1.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象地说，二元分类任务中每个类的标签可以是任何我们喜欢的，只要标签有助于清楚地区分两个类别。从数学上讲，这个选择取决于损失函数。例如，如果使用交叉熵损失，则类别需要是0和1，以便损失函数能够正确工作。相比之下，如果使用SVM中的hinge损失，则类别需要是-1和1。
- en: AdaBoost uses the exponential loss (more on this in section 4.5), and requires
    class labels to be -1 and 1 for the subsequent training to be mathematically sound
    and convergent.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost使用指数损失（更多内容请见第4.5节），并且要求类标签为-1和1，以便后续训练在数学上合理且收敛。
- en: Luckily for us, we don’t have to worry about this when using most machine-learning
    packages such as scikit-learn as they automatically preprocess a variety of training
    labels to what the underlying training algorithm needs.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，当我们使用scikit-learn等大多数机器学习包时，我们不必担心这个问题，因为它们会自动预处理各种训练标签，以满足底层训练算法的需求。
- en: We visualize the performance of AdaBoost as the number of base estimators increases
    in figure 4.9\. As we add more and more weak learners into the mix, the overall
    ensemble is increasingly boosted into a stronger, more complex, and more nonlinear
    classifier.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图4.9中可视化AdaBoost的性能，随着基学习器的数量增加。随着我们添加越来越多的弱学习器，整体集成不断增强，成为一个更强大、更复杂、非线性更强的分类器。
- en: '![CH04_F09_Kunapuli](../Images/CH04_F09_Kunapuli.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F09_Kunapuli](../Images/CH04_F09_Kunapuli.png)'
- en: Figure 4.9 As the number of weak learners increases, the overall classifier
    is boosted into a strong model, which becomes increasingly nonlinear and is able
    to fit (and possibly overfit) the training data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 随着弱学习器的数量增加，整体分类器被提升为强模型，该模型变得越来越非线性，能够拟合（并可能过度拟合）训练数据。
- en: While AdaBoost is generally more resistant to overfitting, like many other classifiers,
    overtraining a boosting algorithm can also result in overfitting, especially in
    the presence of noise. We’ll see how do deal with such situations in section 4.3.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然AdaBoost通常对过拟合有更强的抵抗力，但像许多其他分类器一样，过度训练提升算法也可能导致过拟合，尤其是在存在噪声的情况下。我们将在第4.3节中看到如何处理这种情况。
- en: 4.2.3 AdaBoost with scikit-learn
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.3 使用scikit-learn的AdaBoost
- en: Now that we understand the intuition of how the AdaBoost classification algorithm
    works, we can look at how to use scikit-learn’s AdaBoostClassifier package. scikit-learn’s
    implementation provides additional functionality, including support for multiclass
    classification, as well as other base-learning algorithms beyond decision trees.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了AdaBoost分类算法的直觉，我们可以看看如何使用scikit-learn的AdaBoostClassifier包。scikit-learn的实现提供了额外的功能，包括对多类分类的支持，以及决策树以外的其他基学习算法。
- en: 'The AdaBoostClassifier package takes the following three important arguments
    for binary and multiclass classification tasks:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoostClassifier 包针对二分类和多分类任务接受以下三个重要参数：
- en: base_estimator—The base-learning algorithm AdaBoost uses to train weak learners.
    In our implementation, we used decision stumps. However, it’s possible to use
    other weak learners such as shallow decision trees, shallow artificial neural
    networks, and stochastic gradient descent-based classifiers.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: base_estimator—AdaBoost 用于训练弱学习者的基础学习算法。在我们的实现中，我们使用了决策树桩。然而，也可以使用其他弱学习者，例如浅层决策树、浅层人工神经网络和基于随机梯度下降的分类器。
- en: n_estimators—The number of weak learners that will be trained sequentially by
    AdaBoost.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: n_estimators—AdaBoost 将按顺序训练的弱学习者的数量。
- en: learning_rate—An additional parameter that progressively shrinks the contribution
    of each successive weak learner trained for the ensemble.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: learning_rate—一个额外的参数，它逐步减少每个连续训练的弱学习者在集成中的贡献。
- en: Smaller values of learning_rate make the weak learner weights *α*[t] smaller.
    Smaller *α*[t] means the variation in the example weights *D*[i] decreases, and
    there are less-diverse weak learners. Larger values of learning_rate have the
    opposite effect and increase diversity in weak learners.
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率（learning_rate）的较小值会使弱学习者的权重 *α*[t] 较小。较小的 *α*[t] 意味着示例权重 *D*[i] 的变化减小，并且弱学习者更加单一。学习率的较大值则产生相反的效果，并增加弱学习者的多样性。
- en: The learning_rate parameter has a natural interplay and tradeoff with n_estimators.
    Increasing n_estimators (essentially, the number of iterations since we train
    one estimator per iteration) can lead to the training example weights *D*[i] to
    keep growing. The unconstrained growth of example weights can be controlled by
    the learning_rate.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率参数与 n_estimators（本质上，每次迭代训练一个估计器的迭代次数）之间存在自然的相互作用和权衡。增加 n_estimators（即迭代次数）可能导致训练示例权重
    *D*[i] 持续增长。可以通过学习率来控制示例权重的无约束增长。
- en: 'The following example illustrates AdaBoostClassifier in action on a binary
    classification data set. First, we load the breast cancer data and split into
    training and test sets:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了 AdaBoostClassifier 在二元分类数据集上的实际应用。首先，我们加载乳腺癌数据并将其分为训练集和测试集：
- en: '[PRE5]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We’ll use shallow decision trees of depth 2 as base estimators for training:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用深度为2的浅层决策树作为训练的基础估计器：
- en: '[PRE6]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After training, we can use the boosted ensemble to make predictions on the
    test set:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们可以使用增强集成在测试集上进行预测：
- en: '[PRE7]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'AdaBoost achieves a test error rate of 5.59% on the breast cancer data set:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 在乳腺癌数据集上实现了 5.59% 的测试错误率：
- en: '[PRE8]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Multiclass classification
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 多分类
- en: scikit-learn’s AdaBoostClassifier also supports multiclass classification, where
    data belongs to more than two classes. This is because scikit-learn contains the
    multiclass implementation of AdaBoost called Stagewise Additive Modeling using
    Multiclass Exponential loss, or SAMME. SAMME is a generalization of Freund and
    Schapire’s adaptive boosting algorithm (implemented in section 4.2.2) from two
    to multiple classes. In addition to SAMME, AdaBoostClassifier also provides a
    variant called SAMME.R. The key difference between these two algorithms is that
    SAMME.R handles real-valued predictions from base-estimator algorithms (i.e.,
    class probabilities), whereas vanilla SAMME handles discrete predictions (i.e.,
    class labels).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 的 AdaBoostClassifier 也支持多分类，其中数据属于两个以上的类别。这是因为 scikit-learn 包含了多分类
    AdaBoost 的实现，称为使用多类指数损失的阶跃式添加建模（Stagewise Additive Modeling using Multiclass Exponential
    loss，或 SAMME）。SAMME 是 Freund 和 Schapire 的自适应提升算法（在第4.2.2节中实现）从二类推广到多类的泛化。除了 SAMME
    之外，AdaBoostClassifier 还提供了一个名为 SAMME.R 的变体。这两种算法之间的关键区别在于，SAMME.R 可以处理来自基础估计器算法的实值预测（即类概率），而原始的
    SAMME 处理离散预测（即类标签）。
- en: 'Does this sound familiar? Recall from chapter 3 that there are two types of
    combination functions: those that use the predicted class labels directly, and
    those that can use predicted class probabilities. This is precisely the difference
    between SAMME and SAMME.R as well.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来熟悉吗？回想第3章，存在两种类型的组合函数：那些直接使用预测类标签的，以及那些可以使用预测类概率的。这正是 SAMME 和 SAMME.R 之间的区别。
- en: 'The following example illustrates AdaBoostClassifier in action on a multiclass
    classification data set called iris, where the classification task is to distinguish
    between three species of iris based on the sizes of their petals and sepals. First,
    we load the iris data and split that data into training and test sets:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了AdaBoostClassifier在名为iris的多类分类数据集上的实际应用，其中分类任务是区分三种鸢尾花物种，基于它们花瓣和萼片的尺寸。首先，我们加载鸢尾花数据，并将数据分为训练集和测试集：
- en: '[PRE9]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We check that this data set has three different labels with unique_labels(y),
    which produces array([0, 1, 2]), meaning that this is a three-class classification
    problem. As before, we can train and evaluate AdaBoost on this multiclass data
    set:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查这个数据集有三个不同的标签，具有唯一的_labels(y)，这产生数组([0, 1, 2])，这意味着这是一个三分类问题。与之前一样，我们可以在这个多类数据集上训练和评估AdaBoost：
- en: '[PRE10]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'AdaBoost achieves a test error of 7.89% on the three-class iris data set:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost在三个类别的鸢尾花数据集上实现了7.89%的测试错误率：
- en: '[PRE11]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 4.3 AdaBoost in practice
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 AdaBoost的实际应用
- en: In this chapter, we look at some practical challenges we can expect to encounter
    when using AdaBoost and strategies to ensure that we train robust models. AdaBoost’s
    adaptive procedure makes it susceptible to *outliers*, or data points that are
    extremely noisy. In this section, we’ll see examples of how this problem can affect
    the robustness of AdaBoost and what we can do to mitigate it.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨在使用AdaBoost时可能会遇到的一些实际挑战，以及确保我们训练鲁棒模型的策略。AdaBoost的适应性程序使其容易受到*异常值*的影响，即极其嘈杂的数据点。在本节中，我们将看到这个问题的例子，以及我们可以采取哪些措施来减轻它。
- en: At the core of AdaBoost is its ability to adapt to mistakes made by previous
    weak learners. This adaptive property, however, can also be a disadvantage when
    outliers are present.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost的核心是其适应先前弱学习器所犯错误的能力。然而，当存在异常值时，这种适应性也可能是一个缺点。
- en: Outliers
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值
- en: Outliers are extremely noisy data points that are often the result of measurement
    or input errors and are prevalent in real data to varying degrees. Standard preprocessing
    techniques such as normalization often simply rescale the data and don’t remove
    outliers, which allows them to continue to affect algorithm performance. This
    can be addressed by preprocessing the data to specifically detect and remove outliers.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值是极其嘈杂的数据点，通常是测量或输入错误的结果，在真实数据中普遍存在，程度不同。标准预处理技术，如归一化，通常只是重新缩放数据，并不能去除异常值，这允许它们继续影响算法性能。这可以通过预处理数据来专门检测和去除异常值来解决。
- en: For some tasks (e.g., detecting network cyberattacks), the very thing we need
    to detect and classify (a cyberattack) will be an outlier, also called an anomaly,
    and extremely rare. In such situations, the goal of our learning task will itself
    be anomaly detection.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些任务（例如，检测网络网络攻击），我们需要检测和分类（网络攻击）的东西本身就是一个异常值，也称为异常，并且极其罕见。在这种情况下，我们学习任务的目标本身将是异常检测。
- en: 'AdaBoost is especially susceptible to outliers. Outliers are often misclassified
    by weak learners. Recall that AdaBoost increases the weight of misclassified examples,
    so the weight assigned to outliers continues to increase. When the next weak learner
    is trained, it does one of the following:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost特别容易受到异常值的影响。异常值通常会被弱学习器错误分类。回想一下，AdaBoost会增加错误分类示例的权重，因此分配给异常值的权重会持续增加。当训练下一个弱学习器时，它会执行以下操作之一：
- en: Continues to misclassify the outlier, in which case AdaBoost will increase its
    weight further, which, in turn, causes succeeding weak learners to misclassify,
    fail, and keep growing its weight.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 继续错误地分类异常值，在这种情况下，AdaBoost将进一步增加其权重，这反过来又导致后续的弱学习器错误分类、失败并继续增加其权重。
- en: Correctly classifies the outlier, in which case AdaBoost has just overfit the
    data, as illustrated in figure 4.10.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确地分类异常值，在这种情况下，AdaBoost刚刚过度拟合了数据，如图4.10所示。
- en: '![CH04_F10_Kunapuli](../Images/CH04_F10_Kunapuli.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F10_Kunapuli](../Images/CH04_F10_Kunapuli.png)'
- en: Figure 4.10 Consider a data set with an outlier (circled, top left). In iteration
    1, it has the same weight as all the examples. As AdaBoost continues to sequentially
    train new weak learners, the weights of other data points eventually decrease
    as they are correctly classified. The weight of the outlier continues to increase,
    ultimately resulting in overfitting.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 考虑一个包含异常值（圆圈标注，左上角）的数据集。在迭代1中，它与所有示例具有相同的权重。随着AdaBoost继续依次训练新的弱学习器，其他数据点的权重最终会随着它们被正确分类而降低。异常值的权重持续增加，最终导致过拟合。
- en: Outliers force AdaBoost to spend a disproportionate amount of effort on training
    examples that are noisy. Put another way, outliers tend to confound AdaBoost and
    make it less robust.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值迫使AdaBoost在训练示例上投入不成比例的努力。换句话说，异常值往往会混淆AdaBoost，使其变得不那么鲁棒。
- en: 4.3.1 Learning rate
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 学习率
- en: Now, let’s look at ways to train robust models with AdaBoost. The first aspect
    we can control is *learning rate*, which adjusts the contribution of each estimator
    to the ensemble. For example, a learning rate of 0.75 tells AdaBoost to decrease
    the overall contribution of each base estimator by a factor of 0.75\. When there
    are outliers, a high learning rate will cause their influence to grow proportionally
    quickly, which can absolutely kill the performance of your model. Therefore, one
    way to mitigate the effect of outliners is to lower the learning rate.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用AdaBoost训练鲁棒模型。我们可以控制的第一方面是*学习率*，它调整每个估计器对集成模型的贡献。例如，学习率为0.75表示AdaBoost将每个基础估计器的整体贡献减少到0.75倍。当存在异常值时，高学习率会导致它们的影响成比例地迅速增长，这绝对会损害你模型的性能。因此，减轻异常值影响的一种方法就是降低学习率。
- en: As lowering the learning rate shrinks the contribution of each base estimator,
    controlling the learning rate is also known as *shrinkage* and is a form of model
    regularization to minimize overfitting. Concretely, at iteration *t*, the ensemble
    *F*[t] is updated to *F*[t+1] as
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 降低学习率会缩小每个基础估计器的贡献，因此控制学习率也被称为*收缩*，这是一种模型正则化的形式，用于最小化过拟合。具体来说，在迭代*t*时，集成模型*F*[t]更新为*F*[t+1]。
- en: '![CH04_F10_Kunapuli-eqs-10x](../Images/CH04_F10_Kunapuli-eqs-10x.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F10_Kunapuli-eqs-10x](../Images/CH04_F10_Kunapuli-eqs-10x.png)'
- en: Here, *α*[t] is the weight of weak learner *h*[t] computed by AdaBoost, and
    *η* is the learning rate. The learning rate is a user-defined learning parameter
    that lies in the range 0 < *η* ≤ 1.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*α*[t]是AdaBoost计算出的弱学习器*h*[t]的权重，*η*是学习率。学习率是一个用户定义的学习参数，其范围在0 < *η* ≤ 1之间。
- en: A slower learning rate means that it will often take more iterations (and consequently,
    more base estimators) to build an effective ensemble. More iterations also mean
    more computational effort and longer training times. Often, however, slower learning
    rates may produce a robust model that generalizes better and may well be worth
    the effort.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 较慢的学习率意味着构建一个有效的集成模型通常需要更多的迭代（因此，更多的基础估计器）。更多的迭代也意味着更多的计算努力和更长的训练时间。然而，较慢的学习率可能会产生一个鲁棒性更好的模型，并且可能值得付出努力。
- en: 'An effective way to select the best learning rate is with a validation set
    or cross validation (CV). Listing 4.3 uses 10-fold CV to identify the best learning
    rate in the range [0.1, 0.2, ..., 1.0]. We can observe the effectiveness of shrinkage
    on the breast cancer data:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最佳学习率的一个有效方法是使用验证集或交叉验证（CV）。列表4.3使用10折交叉验证来识别范围[0.1, 0.2, ..., 1.0]内的最佳学习率。我们可以观察到收缩在乳腺癌数据上的有效性：
- en: '[PRE12]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We use stratified k-fold CV, as we did with stacking. Recall that *stratified*
    means the folds are created in such a way that the class distribution is preserved
    across the folds. This also helps with imbalanced data sets, as stratification
    ensures that data from all classes is represented.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用分层k折交叉验证，就像我们在堆叠中做的那样。回想一下，“分层”意味着折叠是以一种方式创建的，使得类分布在整个折叠中保持不变。这也帮助处理不平衡的数据集，因为分层确保了所有类别的数据都得到代表。
- en: Listing 4.3 Cross validation to select the best learning rate
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.3交叉验证选择最佳学习率
- en: '[PRE13]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Sets up stratified 10-fold CV and initializes the search space
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置分层10折交叉验证并初始化搜索空间
- en: ❷ Uses decision stumps as weak learners
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用决策树桩作为弱学习器
- en: ❸ For all choices of learning rates
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 对于所有学习率的选取
- en: ❹ For training, validation sets
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 对于训练和验证集
- en: ❺ Fits a model to training data in this fold
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在这个折叠中拟合训练数据模型
- en: ❻ Computes training and validation errors for this fold
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 计算这个折叠的训练和验证误差
- en: ❼ Averages training and validation errors across the folds
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 在折叠间平均训练和验证误差
- en: We plot the results of this parameter search in figure 4.11, which shows how
    the training and validation errors change as the learning rate increases. The
    number of base learners is fixed to 10\. While the average training error continues
    to decrease with increasing learning rate, the best average validation error is
    achieved for learning _rate=0.8.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图4.11中绘制了此参数搜索的结果，该图显示了随着学习率的增加，训练和验证误差如何变化。基础学习器的数量固定为10。虽然平均训练误差随着学习率的增加而继续下降，但最佳平均验证误差是在学习率率为_rate=0.8时达到的。
- en: '![CH04_F11_Kunapuli](../Images/CH04_F11_Kunapuli.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F11_Kunapuli](../Images/CH04_F11_Kunapuli.png)'
- en: Figure 4.11 Average training and validation errors for different learning rates.
    The validation error for learning_rate=0.6 is lowest, and, in fact, lower than
    the default learning_rate= 1.0.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11：不同学习率下的平均训练和验证误差。学习率=0.6的验证误差最低，实际上低于默认的学习率=1.0。
- en: 4.3.2 Early stopping and pruning
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 提前停止和剪枝
- en: Besides the learning_rate, the other important consideration for practical boosting
    is the number of base learners, n_estimators. It might be tempting to try to build
    an ensemble with a very large number of weak learners, but this doesn’t always
    translate to the best generalization performance. In fact, we often can achieve
    roughly the same performance with fewer base estimators than we think we might
    need. Identifying the least number of base estimators to build an effective ensemble
    is known as *early stopping*. Maintaining fewer base estimators can help control
    overfitting. Additionally, early stopping can also decrease training time as we
    end up having to train fewer base estimators. Listing 4.4 uses a CV procedure
    identical to the one in listing 4.3 to identify the best number of estimators.
    The learning rate here is fixed to 1.0.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 除了学习率之外，对于实际的提升来说，另一个重要的考虑因素是基学习器的数量，即n_estimators。尝试构建一个包含大量弱学习器的集成可能很有吸引力，但这并不总是转化为最佳泛化性能。实际上，我们通常可以用比我们想象的更少的基估计器实现大致相同的性能。确定构建有效集成所需的最少基估计器数量被称为*提前停止*。保持较少的基估计器可以帮助控制过拟合。此外，提前停止还可以减少训练时间，因为我们最终需要训练的基估计器更少。列表4.4使用与列表4.3中相同的CV过程来识别最佳估计器数量。这里的学习率固定为1.0。
- en: Listing 4.4 Cross validation to select the best number of weak learners
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.4：交叉验证以选择最佳弱学习器数量
- en: '[PRE14]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Sets up stratified 10-fold CV and initializes the search space
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置分层10折交叉验证并初始化搜索空间
- en: ❷ Uses decision stumps as weak earners
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用决策树桩作为弱学习器
- en: ❸ For all estimator sizes
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 对于所有估计器大小
- en: ❹ For training, validation sets
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 对于训练和验证集
- en: ❺ Fits a model to training data in this fold
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将模型拟合到本折叠的训练数据
- en: ❻ Computes the training and validation errors for this fold
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 计算此折叠的训练和验证误差
- en: ❼ Averages the errors across the folds
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 对各折叠的平均误差
- en: The results of this search for the best number of estimators are shown in figure
    4.12\. The average validation error suggests that it’s sufficient to use as few
    as 30 decision trees to achieve good predictive performance on this data set.
    In practice, we can stop training early once the performance on the validation
    set reaches an acceptable level.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索最佳估计器数量的结果如图4.12所示。平均验证误差表明，使用多达30个决策树就足以在这组数据集上实现良好的预测性能。在实践中，一旦验证集的性能达到可接受的水平，我们就可以提前停止训练。
- en: '![CH04_F12_Kunapuli](../Images/CH04_F12_Kunapuli.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F12_Kunapuli](../Images/CH04_F12_Kunapuli.png)'
- en: Figure 4.12 Average training and validation errors for different numbers of
    base estimators (decision stumps, in this case). The validation error for n_estimators=20
    is lowest.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12：不同数量基估计器（在这种情况下为决策树桩）的平均训练和验证误差。n_estimators=20的验证误差最低。
- en: Early stopping is also known as *pre-pruning*, as we terminate training before
    fitting a large number of base estimators and often leads to faster training times.
    If we aren’t concerned about training time but want to be more judicious in selecting
    the number of base estimators, we can also consider *post-pruning*. Post-pruning
    means that we train a very large ensemble and then drop the worst base estimators.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 提前停止也称为*预剪枝*，因为我们在大规模拟合基估计器之前终止训练，这通常会导致更快的训练时间。如果我们不关心训练时间，但想更谨慎地选择基估计器的数量，我们也可以考虑*后剪枝*。后剪枝意味着我们训练一个非常大的集成，然后移除最差的基估计器。
- en: 'For AdaBoost, post-pruning drops all weak learners whose weights (*α*[t]) are
    below a certain threshold. We can access the individual weak learners as well
    as their weights after training an AdaBoostClassifier through the fields model.estimators_
    and model.estimator_weights_. To prune the contribution of the least-significant
    weak learners (those whose weight is below a certain threshold), we can simply
    set their weights to zero:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 对于AdaBoost，后剪枝会移除所有权重（*α*[t]）低于某个阈值的弱学习器。我们可以在训练了AdaBoostClassifier之后，通过model.estimators_和model.estimator_weights_字段访问单个弱学习器及其权重。为了剪枝最不显著的弱学习器的贡献（那些权重低于某个阈值的），我们可以简单地将它们的权重设置为0：
- en: '[PRE15]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As before, CV can be used to select a good threshold. Always remember that there
    is typically a tradeoff between AdaBoost’s learning_rate and n_estimators parameters.
    Lower learning rates typically require more iterations (hence, more weak learners),
    while higher learning rates require fewer iterations (and fewer weak learners).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，交叉验证可以用来选择一个好的阈值。始终记住，AdaBoost 的学习率（learning_rate）和 n_estimators 参数之间通常存在权衡。较低的学习率通常需要更多的迭代（因此，更多的弱学习器），而较高的学习率则需要较少的迭代（和较少的弱学习器）。
- en: To be most effective, the best values of these parameters should be identified
    using grid search combined with CV. An example of this is shown in the case study,
    which we look at in the next section.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最有效地进行，应使用网格搜索与交叉验证相结合来识别这些参数的最佳值。案例研究中展示了这一示例，我们将在下一节中讨论。
- en: Outlier detection and removal
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值检测和移除
- en: While the procedures described here are generally effective on noisy data sets,
    training examples with high amounts of noise (i.e., outliers) can still cause
    significant problems. In such cases, it’s often advisable to preprocess the data
    set to remove these outliers entirely.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这里描述的程序在处理噪声数据集时通常有效，但含有大量噪声（即异常值）的训练示例仍然可能引起重大问题。在这种情况下，通常建议预处理数据集以完全删除这些异常值。
- en: '4.4 Case study: Handwritten digit classification'
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 案例研究：手写数字分类
- en: One of the earliest machine-learning applications is on handwritten digit classification.
    In fact, this task has been studied so extensively since the early 1990s that
    we might consider it the “Hello World!” of object recognition.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习最早的几个应用之一是手写数字分类。实际上，自 1990 年代初以来，这项任务已经被广泛研究，我们可能会将其视为对象识别的“Hello World！”
- en: This task originated with the US Postal Service’s attempts to automate digit
    recognition to accelerate mail processing by rapidly identifying ZIP codes. Since
    then, several different handwritten data sets have been created and are widely
    used to benchmark and evaluate various machine-learning algorithms.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务起源于美国邮政服务尝试自动化数字识别，以通过快速识别 ZIP 码来加速邮件处理。从那时起，已经创建了几个不同的手写数据集，并被广泛用于基准测试和评估各种机器学习算法。
- en: In this case study, we’ll use scikit-learn’s digits data set to illustrate the
    effectiveness of AdaBoost. The data set consists of 1,797 scanned images of handwritten
    digits from
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们将使用 scikit-learn 的数字数据集来说明 AdaBoost 的有效性。该数据集包含来自 1,797 张扫描的手写数字图像。
- en: '0 to 9\. Each digit is associated with a unique label, which makes this a 10-class
    classification problem. There are roughly 180 digits per class. We can load the
    data set directly from scikit-learn:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 0 到 9。每个数字都与一个唯一的标签相关联，这使得这是一个 10 类分类问题。每个类别大约有 180 个数字。我们可以直接从 scikit-learn
    加载数据集：
- en: '[PRE16]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The digits themselves are represented as 16 x 16 normalized grayscale bitmaps
    (see figure 4.13), which, when flattened, results in a 64-dimensional (64D) vector
    for each handwritten digit. The training set comprises 1,797 examples × 64 features.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字本身被表示为 16 x 16 的归一化灰度位图（见图 4.13），当展开时，每个手写数字将形成一个 64 维（64D）向量。训练集包含 1,797
    个示例 × 64 个特征。
- en: '![CH04_F13_Kunapuli](../Images/CH04_F13_Kunapuli.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F13_Kunapuli](../Images/CH04_F13_Kunapuli.png)'
- en: Figure 4.13 A snapshot of the digits data set used in this case study
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.13 本案例研究中使用的数字数据集快照
- en: 4.4.1 Dimensionality reduction with t-SNE
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.1 使用 t-SNE 进行降维
- en: While AdaBoost can effectively handle the dimensionality of the digits data
    set (64 features), we’ll (rather aggressively) look to reduce the dimensionality
    to 2\. The main reason for this is to be able to visualize the data as well as
    the models learned by AdaBoost.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 AdaBoost 可以有效地处理数字数据集的维度（64 个特征），但我们将（相当激进地）将其维度降低到 2。这样做的主要原因是为了能够可视化数据以及
    AdaBoost 学习到的模型。
- en: We’ll use a nonlinear dimensionality reduction technique known as t-distributed
    stochastic neighbor embedding (t-SNE). t-SNE is a highly effective preprocessing
    technique for the digits data set and extracts an embedding in a 2D space.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一种称为 t 分布随机邻域嵌入（t-SNE）的非线性降维技术。t-SNE 是数字数据集的一种非常有效的预处理技术，并在二维空间中提取嵌入。
- en: t-SNE
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE
- en: 'Stochastic neighbor embedding, as its name suggests, uses neighborhood information
    to construct a lower dimensional embedding. Specifically, it exploits the similarity
    between two examples: *x*[i] and *x*[j]. In our case, *x*[i] and *x*[j] are two
    example digits from the data set and are 64D. The similarity between two digits
    can be measured as'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 随机邻域嵌入，正如其名所示，使用邻域信息来构建低维嵌入。具体来说，它利用两个示例之间的相似性：*x*[i]和*x*[j]。在我们的案例中，*x*[i]和*x*[j]是从数据集中提取的两个示例数字，它们是64D。两个数字之间的相似度可以测量为
- en: '![CH04_F13_Kunapuli-eqs-11x](../Images/CH04_F13_Kunapuli-eqs-11x.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F13_Kunapuli-eqs-11x](../Images/CH04_F13_Kunapuli-eqs-11x.png)'
- en: where ||*x*[i] - *x*[j]||² is the squared distance between *x*[i] and *x*[j],
    and *σ*²[i] is a similarity parameter. You may have seen this form of a similarity
    function in other areas of machine learning, especially in the context of support
    vector machines, where it’s known as the radial basis function (RBF) kernel or
    the Gaussian kernel.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 其中||*x*[i] - *x*[j]||²是*x*[i]和*x*[j]之间的平方距离，*σ*²[i]是一个相似度参数。你可能在其他机器学习的领域中见过这种相似度函数的形式，特别是在支持向量机的上下文中，它被称为径向基函数（RBF）核或高斯核。
- en: 'The similarity between *x*[i] and *x*[j] can be converted to a probability
    *p*[j|i] that *x*[j] is a neighbor of *x*[i]. The probability is just a normalized
    similarity measure, where we normalize by the sum of similarities of all points
    in the data set *x*[k] with *x*[i]:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '*x*[i]和*x*[j]之间的相似度可以转换为*x*[j]是*x*[i]邻居的概率*p*[j|i]。这个概率只是一个归一化的相似度度量，其中我们通过数据集*x*[k]中所有点与*x*[i]的相似度之和进行归一化：'
- en: '![CH04_F13_Kunapuli-eqs-12x](../Images/CH04_F13_Kunapuli-eqs-11xa.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F13_Kunapuli-eqs-12x](../Images/CH04_F13_Kunapuli-eqs-11xa.png)'
- en: 'Let’s say that the 2D embedding of these two digits is given by *z*[i] and
    *z*[j]. Then, it’s natural to expect that two similar digits *x*[i] and *x*[j]
    will continue to be neighbors even embedding into *z*[i] and *z*[j], respectively.
    The probability of *z*[j] being a neighbor of *z*[i] can be measured similarly:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这两个数字的2D嵌入由*z*[i]和*z*[j]给出。那么，自然地预期两个相似的数字*x*[i]和*x*[j]在嵌入到*z*[i]和*z*[j]后仍然会是邻居。测量*z*[j]是*z*[i]邻居的概率的方法是类似的：
- en: '![CH04_F13_Kunapuli-eqs-14x](../Images/CH04_F13_Kunapuli-eqs-14x.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F13_Kunapuli-eqs-14x](../Images/CH04_F13_Kunapuli-eqs-14x.png)'
- en: 'Here, we assume that the variance in the exponential distribution in the 2D
    (z-space) is 1/2\. Then, we can identify the embeddings of all the points by ensuring
    that *q*[j|i], the probabilities in the 2D embedding space (*z*-space) are well-aligned
    with *p*[j|i] in the 64D original digit space (*x*-space). Mathematically, this
    is achieved by minimizing the KL-divergence (a statistical measure of the difference
    or distance) between the distributions *q*[j|i] and *p*[j|i]. With scikit-learn,
    the embeddings can be computed very easily:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们假设2D（z空间）指数分布的方差是1/2。然后，我们可以通过确保2D嵌入空间（z空间）中的概率*q*[j|i]与64D原始数字空间（x空间）中的*p*[j|i]良好对齐来识别所有点的嵌入。从数学上讲，这是通过最小化分布*q*[j|i]和*p*[j|i]之间的KL散度（一个差异或距离的统计度量）来实现的。使用scikit-learn，嵌入可以非常容易地计算：
- en: '[PRE17]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Figure 4.14 shows what this data set looks like when embedded into a 2D space.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14显示了当数据集嵌入到2D空间时的样子。
- en: '![CH04_F14_Kunapuli](../Images/CH04_F14_Kunapuli.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F14_Kunapuli](../Images/CH04_F14_Kunapuli.png)'
- en: Figure 4.14 Visualization of the 2D embedding of the digits data set produced
    by t-SNE, which is able to embed and separate the digits, effectively clustering
    them
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14 t-SNE生成的数字数据集的2D嵌入可视化，它能够嵌入并分离数字，有效地将它们聚类
- en: Train-test split
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 训练-测试分割
- en: 'As always, it’s important to hold aside a part of the training data for evaluation
    and to quantify the predictive performance of our models on future data. We split
    the lower-dimensional data Xemb and the labels into training and test sets:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 和以往一样，保留一部分训练数据用于评估，并量化我们的模型在未来的数据上的预测性能是很重要的。我们将低维数据Xemb和标签分为训练集和测试集：
- en: '[PRE18]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Observe the use of stratify=y to ensure that the ratios of the different digits
    in train and test sets are identical.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到使用stratify=y确保训练集和测试集中不同数字的比例相同。
- en: 4.4.2 Boosting
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.2 提升法
- en: 'We’ll now train an AdaBoost model for this digit classification task. Recall
    from our earlier discussion that AdaBoost requires us to first choose the type
    of base estimator. We continue to use decision stumps, as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将为这个数字分类任务训练一个AdaBoost模型。回想一下我们之前的讨论，AdaBoost要求我们首先选择基估计器的类型。我们继续使用决策树桩，如下所示：
- en: '[PRE19]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the previous section, we saw how to use CV for selecting the best value of
    learning_rate and n_estimators individually. In practice, we have to identify
    the *best combination* of learning_rate and n_estimators. For this, we’ll employ
    a combination of k-fold CV and grid search.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了如何使用交叉验证（CV）来分别选择学习率（learning_rate）和n_estimators的最佳值。在实践中，我们必须确定学习率和n_estimators的*最佳组合*。为此，我们将结合使用k折交叉验证和网格搜索。
- en: 'The basic idea is to consider different combinations of learning_rate and n_estimators
    and evaluate what their performance would be like via CV. First, we select various
    parameter values we want to explore:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思路是考虑学习率和n_estimators的不同组合，并通过交叉验证（CV）评估它们的性能。首先，我们选择想要探索的各种参数值：
- en: '[PRE20]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we make a scoring function to evaluate the performance of each parameter
    combination. For this task, we use the *balanced accuracy score*, which is essentially
    just the accuracy score weighted by each class. This scoring criterion is effective
    for multiclass classification problems such as this one, and also for imbalanced
    data sets:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个评分函数来评估每个参数组合的性能。对于这个任务，我们使用*平衡准确率*，这本质上就是每个类别的准确率加权。这个评分标准对于像这样的多类分类问题以及不平衡数据集都是有效的：
- en: '[PRE21]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we set up and run the grid search to identify the best parameter combination
    with the GridSearchCV class. Several arguments to GridSearchCV are of interest
    to us. Parameter cv=5 specifies 5-fold CV, and n_jobs=-1 specifies that the job
    should use all available cores for parallel processing (see chapter 2):'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们设置并运行网格搜索，使用GridSearchCV类来识别最佳参数组合。GridSearchCV的几个参数对我们来说很有兴趣。参数cv=5指定了5折交叉验证，n_jobs=-1指定了该作业应使用所有可用的核心进行并行处理（见第2章）：
- en: '[PRE22]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The final parameter in GridSearchCV is set to refit=True. This tells GridSearchCV
    to train a final model using all the available training data, using the best parameter
    combination it has identified.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: GridSearchCV的最后一个参数设置为refit=True。这告诉GridSearchCV使用所有可用的训练数据，使用它已识别的最佳参数组合来训练一个最终模型。
- en: TIP For many data sets, it may not be computationally efficient to exhaustively
    explore and validate all possible hyperparameter choices with GridSearchCV. For
    such cases, it may be more computationally efficient to use RandomizedSearchCV,
    which samples a much smaller subset of hyperparameter combinations to validate.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: TIP 对于许多数据集，使用GridSearchCV穷尽地探索和验证所有可能的超参数选择可能计算效率不高。对于这种情况，使用RandomizedSearchCV可能更有效，它只采样一个更小的超参数组合子集进行验证。
- en: 'After training, we can look up the scores for every parameter combination and
    even pull out the best results:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以查看每个参数组合的分数，甚至提取最佳结果：
- en: '[PRE23]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'These results print the following:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果打印出以下内容：
- en: '[PRE24]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The best model is also available (because we set refit=True). Note that this
    model is trained using the best_combo parameters, using the entire training data
    (Xtrn, ytrn) by GridSearchCV. This model is available in search.best_estimator_
    and can be used for making predictions on the test data:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳模型也可用（因为我们设置了refit=True）。请注意，这个模型是使用最佳_combo参数，通过GridSearchCV使用全部训练数据（Xtrn,
    ytrn）训练的。这个模型在search.best_estimator_中可用，可用于对测试数据进行预测：
- en: '[PRE25]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'How well did this model do? We can first look at the classification report:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型做得怎么样？我们可以首先查看分类报告：
- en: '[PRE26]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The classification report contains class-wise performance metrics, including
    precision and recall for each digit. Precision is the fraction of true positives
    among anything that was predicted as positive, including false positives. It’s
    computed as *TP / (TP + FP)*, where *TP* is the number of true positives and *FP*
    is the number of false positives.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 分类报告包含类别的性能指标，包括每个数字的精确率和召回率。精确率是预测为正的任何事物中真正正例的比例，包括假正例。它计算为*TP / (TP + FP)*，其中*TP*是真正例的数量，*FP*是假正例的数量。
- en: 'Recall is the fraction of true positives among everything that was supposed
    to be predicted as positive, including false negatives. It’s computed as *TP*
    / (*TP* + *FN*), where *FN* is the number of false negatives. The classification
    report is as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率是应该预测为正的所有事物中真正正例的比例，包括假负例。它计算为*TP / (TP + FN)*，其中*FN*是假负例的数量。分类报告如下：
- en: '[PRE27]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'AdaBoost does quite well on most digits. It seems that it struggles a bit with
    5s and 9s, which both have lower F1 scores. We can also look at the *confusion
    matrix*, which will give us a good idea which letters are being confounded with
    others:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 在大多数数字上表现相当好。它似乎在与 5 和 9 这两个数字上有点吃力，它们的 F1 分数较低。我们还可以查看 *混淆矩阵*，这将给我们一个很好的想法，哪些字母被混淆了：
- en: '[PRE28]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The confusion matrix allows us to visualize how the model performed on each
    class:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵使我们能够可视化模型在每个类别上的表现：
- en: '[PRE29]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Each row of the confusion matrix corresponds to the true labels (digits to 0
    to 9), and each column corresponds to the predicted labels. The (9, 5) entry in
    the confusion matrix (tenth row, sixth column, as we begin indexing from 0) indicates
    that several 9s are misclassified as 5s by AdaBoost. Finally, we can plot the
    decision boundaries of the trained AdaBoost model, shown in figure 4.15.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵的每一行对应于真实标签（数字从 0 到 9），每一列对应于预测标签。混淆矩阵中 (9, 5) 的条目（第 10 行，第 6 列，因为我们从 0
    开始索引）表示 AdaBoost 将几个 9 错误地分类为 5。最后，我们可以绘制训练好的 AdaBoost 模型的决策边界，如图 4.15 所示。
- en: '![CH04_F15_Kunapuli](../Images/CH04_F15_Kunapuli.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F15_Kunapuli](../Images/CH04_F15_Kunapuli.png)'
- en: Figure 4.15 The decision boundaries learned by AdaBoost on the embeddings of
    the digits data set
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.15 AdaBoost 在数字数据集嵌入上学习到的决策边界
- en: This case study illustrates how AdaBoost can boost the performance of weak learners
    into a powerful strong learner that can achieve good performance on a complex
    task. Before we end the chapter, let’s look at another adaptive boosting algorithm,
    LogitBoost.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 本案例研究说明了 AdaBoost 如何将弱学习者的性能提升到强大的强学习者，从而在复杂任务上实现良好的性能。在我们结束本章之前，让我们看看另一种自适应提升算法，即
    LogitBoost。
- en: '4.5 LogitBoost: Boosting with the logistic loss'
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 LogitBoost：使用逻辑损失进行提升
- en: We now move on to a second boosting algorithm called logistic boosting (LogitBoost).
    The development of LogitBoost was motivated by the desire to bring loss functions
    from established classification models (e.g., logistic regression) into the AdaBoost
    framework. In this manner, the general boosting framework can be applied to specific
    classification settings to train boosted ensembles with properties similar to
    those classifiers.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在转向第二种提升算法，称为逻辑提升（LogitBoost）。LogitBoost 的发展是由将损失函数从已建立的分类模型（例如，逻辑回归）引入 AdaBoost
    框架的愿望所激发的。以这种方式，通用的提升框架可以应用于特定的分类设置，以训练具有类似这些分类器特性的提升集成。
- en: 4.5.1 Logistic vs. exponential loss functions
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.1 逻辑损失与指数损失函数
- en: 'Recall from section 4.2.2 that AdaBoost updates weights *α*[t] of weak learners
    with the following:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 回想第 4.2.2 节，AdaBoost 使用以下方式更新弱学习者的权重 *α*[t]：
- en: '![CH04_F15_Kunapuli-eqs-15xa](../Images/CH04_F15_Kunapuli-eqs-15xa.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F15_Kunapuli-eqs-15xa](../Images/CH04_F15_Kunapuli-eqs-15xa.png)'
- en: Where does this weighting scheme come from? This expression is a consequence
    of the fact that AdaBoost optimizes the exponential loss. In particular, AdaBoost
    optimizes the exponential loss of an example (*x*,*y*) with respect to a weak
    learner *h*[t](*x*) as given by
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这种加权方案从何而来？这个表达式是 AdaBoost 优化指数损失的事实结果。特别是，AdaBoost 优化了示例 (*x*,*y*) 相对于弱学习器
    *h*[t](*x*) 的指数损失，如下所示：
- en: '![CH04_F15_Kunapuli-eqs-15x](../Images/CH04_F15_Kunapuli-eqs-15x.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F15_Kunapuli-eqs-15x](../Images/CH04_F15_Kunapuli-eqs-15x.png)'
- en: where *y* is the true label, and *h*[t](*x*) is the prediction made by the weak
    learner *h*[t].
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *y* 是真实标签，*h*[t](*x*) 是弱学习器 *h*[t] 的预测。
- en: Can we use other loss functions to derive variants of AdaBoost? We absolutely
    can! LogitBoost is essentially an AdaBoost-like ensemble method whose weighting
    scheme uses a different loss function. It’s just that when we change the underlying
    loss function, we also need to make some other small tweaks to get the overall
    approach to work.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否使用其他损失函数来推导 AdaBoost 的变体？我们绝对可以！LogitBoost 实质上是一种类似于 AdaBoost 的集成方法，其加权方案使用不同的损失函数。只是当我们改变底层损失函数时，我们也需要做一些小的调整，以使整体方法能够工作。
- en: 'LogitBoost differs from AdaBoost in three important ways. First, LogitBoost
    optimizes the logistic loss:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: LogitBoost 与 AdaBoost 在三个方面有所不同。首先，LogitBoost 优化的是逻辑损失：
- en: '![CH04_F15_Kunapuli-eqs-16x](../Images/CH04_F15_Kunapuli-eqs-16x.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F15_Kunapuli-eqs-16x](../Images/CH04_F15_Kunapuli-eqs-16x.png)'
- en: You may have seen the logistic loss in other machine-learning formulations,
    most notably logistic regression. The logistic loss penalizes mistakes differently
    than the exponential loss (see figure 4.16).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在其他机器学习公式中见过逻辑损失，最著名的是逻辑回归。逻辑损失对错误的惩罚方式与指数损失不同（见图 4.16）。
- en: '![CH04_F16_Kunapuli](../Images/CH04_F16_Kunapuli.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F16_Kunapuli](../Images/CH04_F16_Kunapuli.png)'
- en: Figure 4.16 Comparing the exponential loss and the logistic loss functions
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16 比较指数损失函数和对数损失函数
- en: The exact 0-1 loss (also known as the misclassification loss) is an idealized
    loss function that returns 0 for correctly classified examples and 1 for misclassified
    examples. However, this loss is difficult to optimize as it’s not continuous.
    To build feasible machine-learning algorithms, different methods use different
    surrogates, such as the exponential and logistic losses.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 精确的0-1损失（也称为误分类损失）是一个理想化的损失函数，对于正确分类的示例返回0，对于错误分类的示例返回1。然而，由于它不连续，这种损失函数难以优化。为了构建可行的机器学习算法，不同的方法使用不同的替代品，例如指数和对数损失。
- en: The exponential loss function and the logistic loss function both penalize correctly
    classified examples similarly. Training examples that are correctly classified
    with greater confidence have corresponding losses close to zero. The exponential
    loss penalizes misclassified examples far more harshly than the logistic loss,
    which makes it more susceptible to outliers and noise. The logistic loss is more
    measured.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 指数损失函数和对数损失函数都对正确分类的示例进行类似的惩罚。以更高置信度正确分类的训练示例对应的损失接近零。指数损失函数对错误分类的示例的惩罚比对数损失函数更为严厉，这使得它更容易受到异常值和噪声的影响。对数损失函数更为稳健。
- en: 4.5.2 Regression as a weak learning algorithm for classification
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.2 回归作为分类问题的弱学习算法
- en: The second key difference is that AdaBoost works with predictions, while LogitBoost
    works with prediction probabilities. More precisely, AdaBoost works with the predictions
    of the overall ensemble *F*(*x*), while LogitBoost works with prediction probabilities,
    *P*(*x*).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个关键区别在于，AdaBoost使用预测，而LogitBoost使用预测概率。更确切地说，AdaBoost使用整体集成*F*(*x*)的预测，而LogitBoost使用预测概率*P*(*x*)。
- en: The probability of predicting a training example x as a positive example is
    given by
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 预测训练示例x为正例的概率为
- en: '![CH04_F16_Kunapuli-eqs-17x](../Images/CH04_F16_Kunapuli-eqs-17x.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F16_Kunapuli-eqs-17x](../Images/CH04_F16_Kunapuli-eqs-17x.png)'
- en: while the probability of predicting x as a negative example is given by *P*(*y*
    = 0 | *x*) = 1 - *P*(*y* = 1 | *x*). This fact directly influences our choice
    of base estimator.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 而预测x为负例的概率由*P*(*y* = 0 | *x*) = 1 - *P*(*y* = 1 | *x*)给出。这一事实直接影响我们选择基估计器的选择。
- en: The third key difference is that because AdaBoost works directly with discrete
    predictions (-1 or 1, for negative and positive examples), it uses any classification
    algorithm as the base-learning algorithm. LogitBoost, instead, works with continuous
    prediction probabilities. Consequently, it uses *any* regression algorithm as
    the base-learning algorithm.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个关键区别在于，因为AdaBoost直接与离散预测（-1或1，用于负例和正例）工作，它可以使用任何分类算法作为基学习算法。相反，LogitBoost使用连续预测概率。因此，它使用任何回归算法作为基学习算法。
- en: 4.5.3 Implementing LogitBoost
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.3 实现LogitBoost
- en: 'Putting all of these together, the LogitBoost algorithm performs the following
    steps within each iteration. The probability *P*(*y*[i] = 1 | *x*[i]) is abbreviated
    *P*[i] in the following:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起，LogitBoost算法在每个迭代中执行以下步骤。以下简写为*P*[i]的概率*P*(*y*[i] = 1 | *x*[i])：
- en: '[PRE30]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![CH04_F16_Kunapuli-eqs-18x](../Images/CH04_F16_Kunapuli-eqs-18x.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F16_Kunapuli-eqs-18x](../Images/CH04_F16_Kunapuli-eqs-18x.png)'
- en: '[PRE31]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![CH04_F16_Kunapuli-eqs-19x](../Images/CH04_F16_Kunapuli-eqs-19x.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F16_Kunapuli-eqs-19x](../Images/CH04_F16_Kunapuli-eqs-19x.png)'
- en: As we can see from step 4, LogitBoost, like AdaBoost, is an *additive ensemble*.
    This means that LogitBoost ensembles base estimators and combines their predictions
    additively. Furthermore, any weak regressor can be used in step 3, where we use
    regression stumps, which are shallow regression trees. The LogitBoost algorithm
    is also easy to implement, as shown in the following listing.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如第4步所示，LogitBoost与AdaBoost一样，是一种**加性集成**。这意味着LogitBoost集成基估计器并加性组合它们的预测。此外，任何弱回归器都可以在第3步中使用，在那里我们使用回归树桩，即浅层回归树。LogitBoost算法也易于实现，如下面的列表所示。
- en: Listing 4.5 LogitBoost for classification
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.5 LogitBoost用于分类
- en: '[PRE32]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ Initializes example weights, “pred” probabilities
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化示例权重，“pred”概率
- en: ❷ Computes working responses
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算工作响应
- en: ❸ Computes new example weights
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算新示例权重
- en: ❹ Uses decision-tree regression as base estimators for classification problems
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用决策树回归作为分类问题的基估计器
- en: ❺ Appends weak learners to ensemble *F*[t+1](*x*) = *F*[t](*x*) + *h*[t](*x*)
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将弱学习器附加到集成 *F*[t+1](*x*) = *F*[t](*x*) + *h*[t](*x*)
- en: ❻ Updates prediction probabilities
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 更新预测概率
- en: The predict_boosting function described in listing 4.2 can be used to make predictions
    with the LogitBoost ensembles as well and is implemented in listing 4.6.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.2 中描述的 predict_boosting 函数也可以用于使用 LogitBoost 集成进行预测，并在列表 4.6 中实现。
- en: However, LogitBoost requires training labels to be in 0/1 form while AdaBoost
    requires them to be in -1/1 form. Thus, we modify that function slightly to return
    0/1 labels.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LogitBoost 需要训练标签以 0/1 形式存在，而 AdaBoost 需要以 -1/1 形式存在。因此，我们稍微修改了该函数以返回 0/1
    标签。
- en: Listing 4.6 LogitBoost for prediction
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.6 LogitBoost 预测
- en: '[PRE33]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ❶ Converts -1/1 predictions to 0/1
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将 -1/1 预测转换为 0/1
- en: As with AdaBoost, we can visualize how the ensemble trained by LogitBoost evolves
    over several iterations in figure 4.17\. Contrast this figure with the earlier
    figure 4.9, which shows the evolution of the ensemble trained by AdaBoost over
    several iterations.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 与 AdaBoost 一样，我们可以通过图 4.17 视觉化 LogitBoost 在多次迭代中训练的集成如何演变。将此图与早期图 4.9 进行对比，该图显示了
    AdaBoost 在多次迭代中训练的集成演变。
- en: '![CH04_F17_Kunapuli](../Images/CH04_F17_Kunapuli.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F17_Kunapuli](../Images/CH04_F17_Kunapuli.png)'
- en: Figure 4.17 LogitBoost uses decision-tree regression to train regression stumps
    as weak learners to sequentially optimize the logistic loss.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.17 LogitBoost 使用决策树回归来训练回归树桩作为弱学习器，以顺序优化逻辑损失。
- en: We’ve now seen two boosting algorithms that handle two different loss functions.
    Is there a way to generalize boosting to different loss functions and for different
    tasks such as regression?
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了两种处理不同损失函数的 boosting 算法。有没有一种方法可以将 boosting 推广到不同的损失函数和不同的任务，如回归？
- en: The answer to this question is an emphatic yes, as long as the loss function
    is differentiable (and you can compute its gradient). This is the intuition behind
    *gradient boosting*, which we’ll look into in the next two chapters.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的答案是肯定的，只要损失函数是可微分的（并且你可以计算其梯度）。这就是 *梯度提升* 的直觉，我们将在接下来的两章中探讨。
- en: Summary
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Adaptive boosting (AdaBoost) is a sequential ensemble algorithm that uses weak
    learners as base estimators.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自适应提升（AdaBoost）是一种使用弱学习器作为基估计器的顺序集成算法。
- en: In classification, a weak learner is a simple model that performs only slightly
    better than random guessing, that is, 50% accuracy. Decision stumps and shallow
    decision trees are examples of weak learners.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分类中，弱学习器是一个简单的模型，其表现仅略好于随机猜测，即 50% 的准确率。决策树桩和浅层决策树是弱学习器的例子。
- en: AdaBoost maintains and updates weights over training examples. It uses reweighting
    both to prioritize misclassified examples and to promote ensemble diversity.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaBoost 在训练示例上维护和更新权重。它使用重新加权来优先考虑误分类示例并促进集成多样性。
- en: AdaBoost is also an additive ensemble in that it makes final predictions through
    weighted additive (linear) combinations of the predictions of its base estimators.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaBoost 也是一个加性集成，因为它通过其基估计器的预测的加性（线性）组合来做出最终预测。
- en: AdaBoost is generally robust to overfitting as it ensembles several weak learners.
    However, AdaBoost is sensitive to outliers owing to its adaptive reweighting strategy,
    which repeatedly increases the weight of outliers over iterations.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaBoost 通常对过拟合具有鲁棒性，因为它集成了多个弱学习器。然而，由于自适应重新加权策略，AdaBoost 对异常值敏感，该策略在迭代过程中反复增加异常值的权重。
- en: The performance of AdaBoost can be improved by finding a good tradeoff between
    the learning rate and number of base estimators.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaBoost 的性能可以通过在学习率和基估计器的数量之间找到一个良好的权衡来提高。
- en: Cross validation with grid search is commonly deployed to identify the best
    parameter tradeoff between learning rate and number of estimators.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用网格搜索进行交叉验证通常用于确定学习率和估计器数量之间的最佳参数权衡。
- en: Under the hood, AdaBoost optimizes the exponential loss function.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在底层，AdaBoost 优化指数损失函数。
- en: 'LogitBoost is another boosting algorithm that optimizes the logistic loss function.
    It differs from AdaBoost in two other ways: (1) by working with prediction probabilities,
    and (2) by using any classification algorithm as the base-learning algorithm.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LogitBoost 是另一种优化逻辑损失函数的 boosting 算法。它在两个方面与 AdaBoost 不同：(1) 通过处理预测概率，以及(2)
    使用任何分类算法作为基学习算法。
- en: '* * *'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.) Yoav Freund and Robert E. Schapire. “A decision-theoretic generalization
    of on-line learning and an application to boosting,” *Journal of Computer and
    System Sciences*, 55(1):119-139, 1997.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: (1.) Yoav Freund 和 Robert E. Schapire. “在线学习的决策理论推广及其在提升中的应用”，*《计算机与系统科学杂志》*，第
    55 卷第 1 期，第 119-139 页，1997 年。
- en: ^(2.) Ibid.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: (2.) 同上。
- en: ^(3.) Ibid.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: (3.) 同上。
