- en: 10 Hyperparameter tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10 超参数调整
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Initializing the weights in a model prior to warm-up training
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预热训练之前初始化模型中的权重
- en: Doing hyperparameter search manually and automatically
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动和自动进行超参数搜索
- en: Constructing a learning rate scheduler for training a model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为训练模型构建学习率调度器
- en: Regularizing a model during training
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中正则化模型
- en: Hyperparameter tuning is the process of finding the optimal settings of the
    training hyperparameters, so that we *minimize the training time* and *maximize
    the test accuracy*. Usually, these two objectives can’t be fully optimized. If
    we minimize the training time, we likely will not achieve the best accuracy. Likewise,
    if we maximize the test accuracy, we likely will need longer to train.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整是寻找训练超参数最优设置的过程，以便我们*最小化训练时间*和*最大化测试准确率*。通常，这两个目标无法完全优化。如果我们最小化训练时间，我们可能无法达到最佳准确率。同样，如果我们最大化测试准确率，我们可能需要更长时间进行训练。
- en: '*Tuning* is finding the combination of hyperparameter settings that meet your
    targets for the objectives. For example, if your target is the highest possible
    accuracy, you may not concern yourself with minimizing the training time. In another
    situation, if you need only good (but not the best) accuracy, and you are continuously
    retraining, you may want to find settings that get this good accuracy while minimizing
    the training time.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*调整*是找到满足你目标的最优超参数设置组合。例如，如果你的目标是尽可能高的准确率，你可能不会关心最小化训练时间。在另一种情况下，如果你只需要良好的（但不是最好的）准确率，并且你持续进行重新训练，你可能希望找到在最小化训练时间的同时获得这种良好准确率的设置。'
- en: Generally, an objective has no specific set of settings. More likely, within
    the search space various sets of settings will achieve your objective. You need
    to find only one of those sets—and that’s what tuning is.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一个目标没有特定的设置。更有可能的是，在搜索空间内，各种设置组合都能实现你的目标。你需要找到其中之一——这就是调整的目的。
- en: Now, what are the hyperparameters that we are tuning? We’ll go into these in
    detail in this chapter, but basically they are the parameters that guide the training
    of the model to maximize achieving the objective. The parameters we will tune
    in this chapter, for instance, are batch size, learning rate, and learning rate
    schedulers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们调整的超参数有哪些？我们将在本章中详细探讨这些内容，但基本上它们是指导模型训练以最大化实现目标的参数。本章我们将调整的参数，例如，包括批量大小、学习率和学习率调度器。
- en: In this chapter, we will look at several commonly used hyperparameter search
    (tuning) techniques. Figure 10.1 depicts the overall hyperparameter process in
    a conventional production environment. Don’t worry about the details yet; we will
    cover them step-by-step.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨几种常用的超参数搜索（调整）技术。图10.1展示了传统生产环境中整体超参数过程的概览。目前不必担心细节，我们将一步步进行讲解。
- en: '![](Images/CH10_F01_Ferlitsch.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH10_F01_Ferlitsch.png)'
- en: Figure 10.1 Hyperparameter process in a conventional production training environment
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 传统生产训练环境中的超参数过程
- en: 'I’ll briefly walk through this diagram to orient you to the process we’ll be
    following in the rest of this chapter. The first step is to select the best initialization
    of the weights for a model, and we’ll spend some time understanding why this choice
    can significantly affect the outcome when training. We’ll start with predetermined
    distributions based on research and progress to an alternative way of selecting
    a draw from the distribution: the lottery ticket principle.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我将简要地浏览这个图表，以便你了解本章余下部分我们将遵循的过程。第一步是选择模型权重最佳初始化，我们将花些时间了解为什么这个选择可以显著影响训练结果。我们将从基于研究和进展的预定分布开始，进而探讨选择分布中抽取的一种替代方法：彩票原则。
- en: Next, with the weights initialized, we move to the warm-up pretraining. This
    process numerically stabilizes the weights, which will increase the likelihood
    of a more optimal outcome, both in training time and model accuracy.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在权重初始化后，我们转向预热预训练。这个过程从数值上稳定了权重，这将增加在训练时间和模型准确率方面获得更优结果的可能性。
- en: Once the weights are numerically stable, we will look at techniques to search
    and tune the hyperparameters.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦权重数值稳定，我们将探讨搜索和调整超参数的技术。
- en: After we have good initialized and numerical stable weights and hyperparameters
    tuned, we move to the actual training, starting with techniques to further increase
    the likelihood of a more optimal outcome. One of those techniques, which we’ll
    use here, is varying the learning rate in the later parts of training. This can
    significantly improve the chances of converging on a global or near-optimal optimum.
    In other words, these techniques increase the likelihood of producing more-accurate
    models at a lower overall economic cost.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们初始化良好且数值稳定的权重和超参数调整完毕后，我们进入实际训练阶段，首先采用一些技术来进一步提高获得更优结果的可能性。其中一种我们将在此使用的技术是在训练的后期部分调整学习率。这可以显著提高收敛到全局或近似最优解的机会。换句话说，这些技术增加了在较低总体经济成本下产生更精确模型的概率。
- en: 'We will wrap up the chapter by covering common practices of regularization
    techniques that are implemented in training during weight updates. Regularization
    helps reduce memorization (overfitting) while also increasing generalization to
    examples the model will see when deployed in production. We will go over the two
    most commonly used techniques in production: weight decay (also referred to as
    *kernel regularization* or *layer regularization*) and label smoothing.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过介绍在训练过程中权重更新时实施的常见正则化技术来结束本章。正则化有助于减少记忆（过拟合），同时增加模型在生产部署时对示例的泛化能力。我们将讨论生产中最常用的两种技术：权重衰减（也称为*核正则化*或*层正则化*）和标签平滑。
- en: 10.1 Weight initialization
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 权重初始化
- en: When we first start training a model from scratch, we need to give the weights
    an initial value. This process is called *initialization*. For simplicity, we
    could start by setting all the weights to the same value—say, 0 or 1\. That won’t
    work, however, because the way that gradient descent works in backward propagation
    means each weight would have identical updates.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从零开始训练一个模型时，我们需要给权重一个初始值。这个过程称为*初始化*。为了简单起见，我们可以先设置所有权重为相同的值——比如说，0或1。然而，这样做是不行的，因为反向传播中梯度下降的工作方式意味着每个权重都会进行相同的更新。
- en: That neural network would be symmetrical and equivalent to just a single node.
    A single node can make only a single binary decision, and can solve only a problem
    with linear separation, like a logical AND or OR. A logical XOR problem cannot
    be solved with a single node, since it requires a nonlinear separation. The failure
    of the early perceptron model to solve the XOR problem is attributed to the reduced
    funding and research in artificial intelligence, from 1984 to 2012, which is referred
    to as the *AI winter*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 那个神经网络将是对称的，相当于一个单独的节点。一个单独的节点只能做出单一的二元决策，并且只能解决具有线性分离的问题，如逻辑与或或。逻辑异或问题不能通过单个节点解决，因为它需要一个非线性分离。早期感知器模型无法解决异或问题，这归因于从1984年到2012年人工智能研究减少和资金减少，这被称为*人工智能冬天*。
- en: So, we need to set the weights in the model to a random distribution of values.
    Ideally, the distribution should be a small range (between –1 and 1) and be centered
    at 0\. Several ranges for random distributions have been used over the last several
    years for initializing weights. Why should the weights be within a small distribution
    range? Well, if our range is large, the larger initialized weights will dominate
    the updating of the model over the smaller weights, leading to sparsity, less
    accuracy, and possibly lack of converging.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要将模型中的权重设置为随机值分布。理想情况下，分布范围应较小（在-1和1之间），且以0为中心。在过去几年中，为了初始化权重，已经使用了几个随机分布的范围。为什么权重应该在一个小的分布范围内？好吧，如果我们的范围很大，较大的初始化权重将主导模型更新中的较小权重，导致稀疏性、准确性降低，并可能无法收敛。
- en: 10.1.1 Weight distributions
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1 权重分布
- en: Let’s start by clarifying the difference between weight initialization and weight
    distribution. *Weight initialization* is the initial set of values for the weights,
    the starting point, before training the model. *Weight distribution* is the source
    from which we select those initial weights.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先澄清权重初始化和权重分布之间的区别。*权重初始化*是在训练模型之前为权重设置的初始值，是起点。*权重分布*是我们选择那些初始权重的来源。
- en: 'Three weight distributions have proved to be the most popular with researchers.
    *Uniform distribution* is uniformly distributed across a range. This is no longer
    used. *Xavier,* or *Glorot,* *distribution*, which improved upon uniform distribution,
    is a random, normal distribution centered at zero. It has a standard deviation
    set to the following formula, where *fan_in* is the number of inputs to the layer:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 三种权重分布已被证明是最受研究人员欢迎的。**均匀分布**在整个范围内均匀分布。这不再使用。**Xavier**，或**Glorot**分布，是对均匀分布的改进，是一种以零为中心的随机正态分布。其标准差设置为以下公式，其中*fan_in*是层的输入数量：
- en: sqrt(1 / *fan_in*)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: sqrt(1 / *fan_in*)
- en: This was a popular method in early SOTA models, and was best suited when activations
    were a tanh (hyperbolic tangent). It is now seldom used.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在早期SOTA模型中流行的一种方法，最适合激活函数为tanh（双曲正切）时使用。现在很少使用。
- en: 'Finally, we have the *He-normal distribution*, which improved upon Xavier distribution.
    These days, almost all weight initialization is done with the He-normal distribution;
    it is the current mainstream distribution and is best suited for ReLU activations.
    This random distribution is a normal distribution centered at zero and with a
    standard deviation set to the following formula, where *fan_in* is the number
    of inputs to the layer:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有**He-normal分布**，它是对Xavier分布的改进。如今，几乎所有权重初始化都是使用He-normal分布进行的；它是当前的主流分布，最适合ReLU激活函数。这种随机分布是以零为中心的正态分布，其标准差设置为以下公式，其中*fan_in*是层的输入数量：
- en: sqrt(2 / *fan_in*)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: sqrt(2 / *fan_in*)
- en: 'Now let’s see how to implement this. In TF.Keras, by default, weights are initialized
    to an Xavier distribution (referred to as `glorot_uniform`). To initialize the
    weights to a He-normal distribution, you must explicitly set the keyword parameter
    `kernel_ initializer` to `he_normal`. Here’s how that looks:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看如何实现这一点。在TF.Keras中，默认情况下，权重初始化为Xavier分布（称为`glorot_uniform`）。要将权重初始化为He-normal分布，必须显式设置关键字参数`kernel_initializer`为`he_normal`。以下是实现方式：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Initializes the weights to a He-normal distribution
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将权重初始化为He-normal分布
- en: 10.1.2 Lottery hypothesis
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2 彩票假设
- en: Once researchers had established a consensus on a distribution to draw weights
    from for initializing a neural network, the next question was, what was the best
    method to draw from the distribution? We will start by discussing the lottery
    hypothesis, which kicked off a sequence of rapid advances in drawing from a distribution,
    which then led to the concept of numerical stability (covered in section 10.1.3).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦研究人员就用于初始化神经网络的权重分布达成共识，下一个问题是，从分布中抽取的最佳方法是什么？我们将从讨论彩票假设开始，它引发了一系列从分布中抽取的快速进展，这进而导致了数值稳定性概念（在第10.1.3节中介绍）。
- en: 'The *lottery hypothesis* for weight initialization was proposed in 2019\. The
    hypothesis makes two presumptions:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年提出了用于权重初始化的**彩票假设**。该假设包含两个假设：
- en: No two draws from a random distribution are equal. Some draws from the random
    distribution for weight initialization produce better results than others.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从随机分布中抽取的两个值不会相等。对于权重初始化的随机分布抽取中，有些抽取结果比其他抽取结果更好。
- en: Large models have high accuracy because they are really a collection of small
    models. Each has a different draw from the random distribution, and one of the
    draws is the winning ticket.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型模型具有高精度，因为它们实际上是一系列小型模型的集合。每个模型从随机分布中抽取不同的值，其中一个抽取值是“中奖彩票”。
- en: Subsequent attempts to identify and extract the submodel with the winning ticket
    into a compact model from a trained large model never panned out. As a result,
    the method as it was proposed in “The Lottery Ticket Hypothesis” by Jonathan Frankle
    and Michael Carbin ([https://arxiv.org/abs/1803.03635](https://arxiv.org/abs/1803.03635))
    is not used today, but subsequent research led to other variations. In this subsection,
    we explore one of the variations that is commonly used.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 随后尝试从训练的大型模型中识别和提取具有“中奖彩票”的子模型到一个紧凑模型，但从未成功。因此，由Jonathan Frankle和Michael Carbin在“彩票假设”（[https://arxiv.org/abs/1803.03635](https://arxiv.org/abs/1803.03635)）中提出的方法现在不再使用，但后续研究导致了其他变体。在本节中，我们将探讨其中一种常用的变体。
- en: The question, though, about the “winning ticket” has not yet been resolved.
    Another camp of ML practitioners uses the methodology of pretraining multiple
    instances of the model, each with a separate draw. Typically, when using this
    approach, we run a small number of epochs with a very small learning rate (for
    example, 0.0001). For each epoch, the number of steps is substantially fewer than
    the size of the training data. By doing so, we can pretrain a large number of
    instances in a short period of time. Once completed, the model instance with the
    best objective metric, such as training loss, is selected. The assumption is that
    this draw is a better winning ticket than the others.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，关于“中奖彩票”的问题尚未解决。另一群机器学习实践者使用预训练多个模型实例的方法，每个实例都有单独的抽取。通常，当使用这种方法时，我们使用非常小的学习率（例如，0.0001）运行少量epoch。对于每个epoch，步数远少于训练数据的大小。通过这样做，我们可以在短时间内预训练大量实例。一旦完成，选择具有最佳目标指标（如训练损失）的模型实例。假设这种抽取的中奖彩票比其他抽取更好。
- en: Figure 10.2 illustrates pretraining model instances by using the lottery hypothesis
    method. Multiple copies of the reference model architecture to train are instantiated,
    each with a different draw from a random distribution. Each instance is then pretrained
    with the same tiny learning rate for a small number of epochs/reduced steps. If
    the compute resources are available, the pretraining is distributed. Once completed,
    the training loss of each pretrained model is inspected. The instance with the
    lowest training loss is the instance with the best draw—the winning ticket.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2通过使用彩票假设方法展示了预训练模型实例。创建了多个参考模型架构的副本以进行训练，每个副本从随机分布中抽取不同的样本。然后，每个实例使用相同的小学习率进行少量epoch/减少的步数进行预训练。如果计算资源可用，预训练是分布式的。一旦完成，检查每个预训练模型的训练损失。具有最低训练损失的实例是具有最佳抽取的实例——即中奖彩票。
- en: '![](Images/CH10_F02_Ferlitsch.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH10_F02_Ferlitsch.png)'
- en: Figure 10.2 Pretraining with the lottery hypothesis method
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 使用彩票假设方法进行预训练
- en: 'We can implement this process with the following code. The major steps, shown
    in this sample, are as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码实现此过程。样本中显示的主要步骤如下：
- en: Create 10 instances of the model, each with a separate draw for weight initialization.
    We do this to emulate the principle that no two draws are the same. The choice
    of 10 is just arbitrary for this example. The larger the number of instances,
    each with a separate draw, the more likely one of those draws is the winning ticket.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建10个模型实例，每个实例都有单独的抽取进行权重初始化。我们这样做是为了模拟这样一个原则：没有两个抽取是相同的。在这个例子中，选择10只是一个任意数。实例数量越多，每个实例都有单独的抽取，那么其中某个抽取是中奖彩票的可能性就越大。
- en: Train each instance for a small number of epochs and steps.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个实例进行少量epoch和步数的训练。
- en: Select the model instance (`best`) with the lowest training loss.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择具有最低训练损失的模型实例（`best`）。
- en: 'Here is the code:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是代码：
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Creates 10 model instances, each with a separate draw for initialization
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建10个模型实例，每个实例都有单独的抽取进行初始化
- en: ❷ Does pretraining and selects the instance with the lowest training loss
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 预训练并选择具有最低训练损失的实例
- en: Next, we look at another approach to weight initialization, using warm-up for
    numeric stabilization of the weights.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们看看另一种权重初始化的方法，即使用预热来对权重进行数值稳定性。
- en: 10.1.3 Warm-up (numerical stability)
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.3 预热（数值稳定性）
- en: The *numerical stability method*, which takes a different approach from the
    lottery hypothesis for weight initialization, is currently the prevailing technique
    for initializing the weights before full training. In the lottery hypothesis,
    a large model is viewed as a collection of submodels, and one submodel has the
    winning ticket. In the numerical stability method, a large model is divided into
    a higher (bottom) and lower (top) layers.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与彩票假设方法在权重初始化方面采取不同方法的*数值稳定性方法*，是目前在完整训练之前初始化权重的流行技术。在彩票假设中，大模型被视为子模型的集合，其中一个子模型拥有中奖彩票。在数值稳定性方法中，大模型被分为上层（底部）和下层（顶部）。
- en: While we previously discussed *bottom* versus *top*, that terminology still
    may seem backward to some readers—it sure did to me. In a neural network, the
    input layer is the bottom, and the output layer is the top. Input is fed from
    the bottom of the model, and the predictions are outputted from the top.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们之前讨论了*底部*与*顶部*的区别，但这种术语可能对一些读者来说仍然显得有些倒退——对我来说确实如此。在神经网络中，输入层是底部，输出层是顶部。输入从模型的底部馈入，预测从顶部输出。
- en: The presumption is that the lower (top) layers provide numerical stability to
    the higher (bottom) layers during training. Or specifically, the lower layers
    provide the numerical stability for the higher layers to *learn* the winning ticket
    (initialization draw). Figure 10.3 depicts this process.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 假设较低（顶部）层在训练期间为较高（底部）层提供数值稳定性。或者更具体地说，较低层为较高层提供数值稳定性，以便它们可以*学习*获胜的彩票（初始化抽签）。图10.3描述了此过程。
- en: '![](Images/CH10_F03_Ferlitsch.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH10_F03_Ferlitsch.png)'
- en: Figure 10.3 Pretraining for numerical stability of lower layers, so higher layers
    learn the winning lottery ticket initialization
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 预训练以实现底层数值稳定性，以便高层学习获胜的彩票初始化
- en: This method is typically implemented as a warm-up training cycle before full
    training of the model. For warm-up training, we start with a very tiny learning
    rate to avoid causing large swings in weights and to get the weights to shift
    toward the winning ticket. Typical initial values for the warm-up learning rate
    are in range of 1e-5 to 1e-4.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法通常在模型完整训练之前作为一个预热训练周期来实现。对于预热训练，我们从一个非常小的学习率开始，以避免引起权重的大幅波动，并使权重向获胜彩票移动。预热学习率的典型初始值在1e-5到1e-4的范围内。
- en: We train the model for a small number of epochs, typically four or five, and
    gradually step up the learning rate after each epoch to the initial learning rate
    selected for training.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对模型进行少量周期（通常是四到五个）的训练，并在每个周期后逐步提高学习率到为训练所选的初始学习率。
- en: Figure 10.4 illustrates the warm-up training method, depicted previously as
    steps 1, 2, and 3 in figure 10.1\. Unlike the lottery hypothesis, we start with
    a single instance of the reference model to train. Starting at a very low learning
    rate, where weights are adjusted by minute amounts, the model is trained with
    full epochs. Each time the learning rate is progressively proportional to the
    initial learning rate for full training. Upon reaching the final epoch, the weights
    in the model instance are deemed to be numerically stable.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4说明了预热训练方法，如图10.1中的步骤1、2和3所示。与彩票假设不同，我们从一个参考模型的单个实例开始训练。从非常低的学习率开始，其中权重通过微小的调整，模型以完整周期进行训练。每次学习率逐渐与完整训练的初始学习率成比例。达到最终周期后，模型实例中的权重被认为是数值稳定的。
- en: '![](Images/CH10_F04_Ferlitsch.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH10_F04_Ferlitsch.png)'
- en: Figure 10.4 Warm-up pretraining for numerical stability
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 预热预训练以实现数值稳定性
- en: 'In the following code sample, you can see the five key steps implemented:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码示例中，你可以看到实现了以下五个关键步骤：
- en: Instantiate a single weight-initialized instance of the model.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个模型的单个权重初始化实例。
- en: Define the learning rate scheduler `warmup_scheduler()` to step up the learning
    rate after each epoch. Section 10.3 describes learning rate schedulers in detail.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义学习率调度器`warmup_scheduler()`，在每个周期后提高学习率。第10.3节详细介绍了学习率调度器。
- en: Add the warm-up scheduler as a callback for the `fit()` method.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将预热调度器作为`fit()`方法的回调。
- en: Train for a small number of epochs (for example, four).
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练少量周期（例如，四个）。
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Sets warm-up learning rate and learning rate step
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置预热学习率和学习率步长
- en: ❷ Creates the model and sets the initial learning rate to the warm-up rate
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建模型并设置初始学习率为预热率
- en: ❸ Incrementally increases from warm-up rate to initial learning rate for full
    training
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从预热率逐步增加到完整训练的初始学习率
- en: ❹ Creates the callback to the learning rate scheduler
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 创建回调到学习率调度器
- en: Now that we’ve covered the pretraining, let’s take a look at the fundamentals
    behind hyperparameter search. Then we’ll put everything you’ve learned here into
    practice and do a full training of a model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了预训练，让我们来看看超参数搜索背后的基础。然后我们将把在这里学到的所有内容付诸实践，并对模型进行完整训练。
- en: 10.2 Hyperparameter search fundamentals
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 超参数搜索基础
- en: Once your model’s weight initialization has numeric stability (whether by lottery
    or warm-up), we do *hyperparameter search*, also referred to as *hyperparameter
    tuning* or *hyperparameter optimization*.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的模型权重初始化具有数值稳定性（无论是由抽签还是预热），我们进行**超参数搜索**，也称为**超参数调整**或**超参数优化**。
- en: Remember, the goal of hyperparameter search is to find the (near) optimal hyperparameter
    setting to maximize training of your model for your objective—which might be,
    for instance, speed of training or evaluation accuracy. And, as we’ve also discussed,
    we distinguish between parameters for model configuration, known as *metaparameters*,
    and those for training, known as *hyperparameters*. In this section, we focus
    only on tuning hyperparameters.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，超参数搜索的目的是找到（近似）最佳超参数设置，以最大化您模型针对目标（例如，训练速度或评估准确性）的训练。而且，正如我们之前讨论的，我们区分模型配置的参数，称为*元参数*，以及训练的参数，称为*超参数*。在本节中，我们只关注调整超参数。
- en: 'Typically, when training a preconfigured model, the only hyperparameters we
    attempt to tune are as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在训练预配置模型时，我们尝试调整的超参数如下：
- en: Learning rate
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率
- en: Batch size
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大小
- en: Learning rate scheduler
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率调度器
- en: Regularization
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化
- en: NOTE Do not perform a hyperparameter search on a model whose weights have not
    been numerically stabilized. Without numerical stabilization of the weights, the
    practitioner may inadvertently discard combinations with poor performance, which
    may otherwise be a good combination.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：不要在权重未进行数值稳定的模型上进行超参数搜索。如果没有权重的数值稳定，实践者可能会无意中丢弃性能较差的组合，而这些组合可能原本是好的组合。
- en: Let’s start with a visual. Figure 10.5 depicts the search space. The black area
    represents combinations of hyperparameters that produce optimal results. Multiple
    areas of optimal combinations may exist in the search space; in this case, we
    have three black dots. Typically, within the vicinity of each optimal area is
    a larger area of near-optimal results, represented as gray. The vast majority
    of the search space, represented by white space, produces nonoptimal (and non-near)
    results.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从视觉开始。图10.5展示了搜索空间。黑色区域代表产生最佳结果的超参数组合。搜索空间中可能存在多个最佳组合区域；在这种情况下，我们有三个黑色点。通常，在每个最佳区域附近都有一个较大的近似最佳结果区域，用灰色表示。搜索空间的大部分，用白色空间表示，产生非最佳（和非近似）的结果。
- en: '![](Images/CH10_F05_Ferlitsch.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH10_F05_Ferlitsch.png)'
- en: Figure 10.5 Hyperparameter search space
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 超参数搜索空间
- en: As you can see by the preponderance of white space to black space, if you were
    to pick a random handful of hyperparameter combinations, it’s unlikely that you
    would find an optimal or near-optimal result. So, you need a strategy. A good
    strategy is one that has a high likelihood of landing in a near-optimal area(s);
    being within the near-optimal area narrows the search space to find the optimal
    area within the vicinity.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，白色区域相对于黑色区域的数量，如果您随机挑选一些超参数组合，您不太可能找到一个最佳或近似最佳结果。因此，您需要一个策略。一个好的策略是具有高概率落在近似最佳区域（s）的策略；处于近似最佳区域内可以缩小搜索空间，以找到附近的最佳区域。
- en: 10.2.1 Manual method for hyperparameter search
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1 超参数搜索的手动方法
- en: 'Let’s start by walking through a manual method, before we get into an automated
    search. I have a lot of experience training computer vision models and have a
    strong intuition when it comes to selecting hyperparameters. I am able to use
    this learned intuition to do a manually guided search. I generally follow these
    initial four steps:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入自动化搜索之前，让我们先通过一个手动方法来了解一下。我在训练计算机视觉模型方面有很多经验，并且在选择超参数方面有很强的直觉。我能够利用这种学到的直觉来进行手动引导的搜索。我通常遵循以下这四个初始步骤：
- en: Coarse-tune the initial learning rate.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 粗调初始学习率。
- en: Tune the batch rate.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整批量率。
- en: Fine-tune the initial learning rate.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 精调初始学习率。
- en: Tune the learning rate scheduler.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整学习率调度器。
- en: Coarse-tune initial learning rate
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 粗调初始学习率
- en: I start by using a fixed batch size and fixed learning rate. If it is a small
    dataset, typically 50,000 examples or fewer, I use a batch size of 32; otherwise,
    I use 256\. I pick a center point for the learning rate—typically, 0.001\. I then
    run experiments at the center point (for example, 0.001) and one magnitude greater
    (for example, 0.01) and less (0.0001). I look at the validation loss and accuracy
    between the three runs and decide which direction leads to better convergence.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我首先使用固定的批量大小和固定的学习率。如果是一个小数据集，通常少于50,000个示例，我使用32个批量大小；否则，我使用256个。我选择一个学习率的中心点——通常为0.001。然后我在中心点（例如，0.001）及其一个数量级更大（例如，0.01）和更小（0.0001）的位置进行实验。我查看三个运行之间的验证损失和准确性，并决定哪个方向会导致更好的收敛。
- en: If I have a run with the lowest validation loss and highest validation accuracy,
    I will go with that one. Sometimes a lower validation loss on one run doesn’t
    lead to a higher accuracy. In those cases, I follow more of a gut feeling, but
    tend to lean toward making the decision on the lowest validation loss.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我有一个具有最低验证损失和最高验证准确率的运行，我将选择那个。有时一个运行上的较低验证损失不会导致更高的准确率。在这些情况下，我更多地依赖直觉，但倾向于倾向于基于最低验证损失做出决定。
- en: I then pick a new center point halfway between the existing and the better convergence
    point. For example, if the center and convergent points are 0.001 and 0.01, I
    pick 0.005 as the center and use one magnitude greater (0.05) and lesser (0.0005),
    and repeat the experiment. I repeat this divide-and-conquer strategy until the
    center point gives me the best convergence, which becomes the coarse-tuned initial
    learning rate. There is a high likelihood I am in a near-optimal area (gray).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我在现有和更好的收敛点之间选择一个新的中心点。例如，如果中心和收敛点是 0.001 和 0.01，我选择 0.005 作为中心，并使用一个数量级更大（0.05）和更小（0.0005），然后重复实验。我重复这种分而治之的策略，直到中心点给我最佳的收敛，这成为粗调的初始学习率。我很有可能接近最优区域（灰色）。
- en: Tune the batch rate
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 调整批量大小
- en: Next, I tune the batch size. In general, using 32 for small and 256 for large
    datasets represents the lowest levels. So I am going to try higher ones. I use
    a 2× factor. For example, if my batch size is 32, I try 64 with the coarse learning
    rate. If the convergence improves, I would then try 128, and so forth. When it
    does not improve, I select the previous good value.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我调整批量大小。一般来说，对于小型数据集使用 32，对于大型数据集使用 256 代表最低水平。所以我将尝试更高的值。我使用 2 倍因子。例如，如果我的批量大小是
    32，我将尝试使用粗略学习率的 64。如果收敛性有所改善，我将尝试 128，依此类推。当它没有改善时，我选择之前的好值。
- en: Fine-tune the initial learning rate
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 微调初始学习率
- en: At this point, there is a high likelihood that I have gotten closer to an optimal
    area (black). The larger the batch size, the less variance we will have on the
    loss per batch. As a result, we typically can raise the learning rate if we have
    increased the batch size.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我很有可能已经接近了最优区域（黑色）。批量大小越大，每批损失的变化就越小。因此，如果我们增加了批量大小，我们通常可以提高学习率。
- en: Given the larger batch size, I repeat the tuning experiments for the learning
    rate, using the coarse learning rate as the initial center point.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到较大的批量大小，我重复进行了学习率的调整实验，以粗略学习率作为初始中心点。
- en: Learning rate schedule
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率调度
- en: At this point, I start a full training run with early stopping when the validation
    accuracy stops improving. I generally first try cosine annealing on the learning
    rate (discussed subsequently). If that makes a significant improvement, I generally
    stop there. Otherwise, I look back at the initial full run and find the epoch
    where validation accuracy plateaued or diverged. I then set a learning rate scheduler
    to drop the learning rate by one magnitude at one epoch before that point.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我开始一个完整的训练运行，当验证准确率停止提高时进行早期停止。我通常首先尝试在学习率上使用余弦退火（随后讨论）。如果那有显著的改进，我通常就停在那里。否则，我会回顾最初的完整运行，并找到验证准确率平顶或发散的时期。然后我设置一个学习率调度器，在该点之前的一个时期将学习率降低一个数量级。
- en: This generally gives me a really good starting point, and I can now focus on
    other pretraining steps like augmentation and label smoothing (discussed in section
    10.4).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常给我一个非常好的起点，我现在可以专注于其他预训练步骤，如增强和标签平滑（在第 10.4 节中讨论）。
- en: 10.2.2 Grid search
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.2 网格搜索
- en: '*Grid* *search* is the oldest form of hyperparameter search. It means that
    you search every possible combination in a narrow search space; it’s an innate
    human approach for a new problem to gain insight. This is practical only with
    a few parameters and values. For example, if we have three learning rate values
    and two batch sizes, the number of combinations would be 3 × 2 or 6, which is
    practical. Let’s just slightly increase that to five learning rate values and
    three batch sizes. That’s now 5 × 3 or 15\. Wow, look how fast the combinations
    grow!'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*网格* *搜索* 是超参数搜索的最古老形式。这意味着你在狭窄的搜索空间中搜索每一个可能的组合；这是对于新问题获得洞察力的固有的人类方法。这种方法只有少数参数和值时才是实用的。例如，如果我们有三个学习率值和两个批量大小，组合的数量将是
    3 × 2 或 6，这是实用的。让我们稍微增加一下，增加到五个学习率值和三个批量大小。现在就是 5 × 3 或 15。哇，看看组合是如何快速增长的！'
- en: Since the (near) optimal area is much smaller in comparison to the entire search,
    we are unlikely to find a good combination early.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于与整个搜索空间相比，（近）最优区域要小得多，我们不太可能早期就找到一个好的组合。
- en: This approach is not used anymore because of its computational overhead. The
    following is an example implementation of a grid search. I present it here so
    you can make a comparison to random search in the next subsection.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法不再使用，因为它计算开销大。以下是一个网格搜索的示例实现。我在这里提出它，以便你可以在下一小节中将它与随机搜索进行比较。
- en: 'In this example, we do a grid search on two hyperparameters: learning rate
    (`lr`) and batch size (`bs`). For both, we specify a set of values to try, such
    as `[0.1, 0.01]` for the learning rate. We then use two nested loop iterators
    to generate all combinations of the set of values for learning rate and batch
    size. For each combination, we get a copy of the pretrained instance of the model
    (`get_model()`) and train it for a few epochs. A running tally of the best validation
    score and corresponding hyperparameter combination is kept (`best`). When done,
    the tuple `best` contains the hyperparameter settings that resulted in the lowest
    validation loss.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们在两个超参数上进行网格搜索：学习率（`lr`）和批量大小（`bs`）。对于两者，我们指定要尝试的值集，例如学习率指定为`[0.1, 0.01]`。然后我们使用两个嵌套循环迭代器生成学习率和批量大小值集的所有组合。对于每个组合，我们获取模型预训练实例的副本（`get_model()`）并对其进行几个epoch的训练。保持最佳验证分数及其对应的超参数组合的运行总计（`best`）。完成后，`best`元组包含导致最低验证损失的超参数设置。
- en: '[PRE3]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Grid search for three learning rates and two batch sizes
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 对三个学习率和两个批量大小进行网格搜索
- en: ❷ Sets the learning rate when compiling the model
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在编译模型时设置学习率
- en: ❸ Trains for a few epochs
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 训练几个epoch
- en: ❹ Uses validation accuracy to select the best combination of learning rate and
    batch size
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用验证准确率来选择最佳的学习率和批量大小组合
- en: 10.2.3 Random search
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.3 随机搜索
- en: Let’s turn to a random search method, which is less computationally expensive
    than the grid search for finding good hyperparameters. Your first question might
    be, how could a random search be computationally less expensive than a grid search
    (it’s just random)?
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们转向随机搜索方法，这种方法在寻找好的超参数方面比网格搜索计算成本更低。你可能会问，随机搜索怎么可能比网格搜索计算成本更低（它只是随机的）？
- en: To answer this question, let’s revisit our earlier depiction of the hyperparameter
    search space. We know that only a small portion of it has an optimal combination,
    so we have a very low probability of finding one randomly. But we also know that
    substantially larger areas are near-optimal, so we have a substantially higher
    probability of landing in one of those using a random search.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，让我们回顾一下我们之前对超参数搜索空间的描述。我们知道其中只有一小部分有最优组合，所以我们随机找到它的概率非常低。但我们还知道，大量更大的区域是近最优的，所以我们使用随机搜索落在这些区域之一的概率大大提高。
- en: Once the search finds a near-optimal combination, we know there is a good likelihood
    that an optimal combination exists in the vicinity. At this point, we narrow the
    random search to an area surrounding the near-optimal combination. If a new combination
    improves the result, we may further narrow the random search around the vicinity
    of the new combination.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦搜索找到一个近最优组合，我们就知道在附近很可能存在一个最优组合。在这个时候，我们将随机搜索缩小到围绕近最优组合的区域。如果新的组合提高了结果，我们可能会进一步缩小围绕新组合附近的随机搜索。
- en: 'To summarize these steps:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 总结这些步骤：
- en: Set the boundaries for the search space.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置搜索空间的边界。
- en: Do a random search within the entire search space.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在整个搜索空间内进行随机搜索。
- en: Once a near-optimal combination is found, narrow the search space to the vicinity
    of the new combination.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦找到一个近最优组合，将搜索空间缩小到新组合的附近。
- en: Continuously repeat until a combination meets your objective criteria.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 持续重复，直到找到一个满足你的目标标准的组合。
- en: If a new combination improves the result, further narrow the search space around
    the new combination.
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果新的组合提高了结果，进一步缩小围绕新组合的搜索空间。
- en: If, after a predefined number of trials, the result does not improve, return
    to searching the entire search space (step 2).
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在预定义的试验次数后结果没有改善，则返回到搜索整个搜索空间（步骤2）。
- en: Figure 10.6 illustrates the first three steps.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6说明了前三个步骤。
- en: '![](Images/CH10_F06_Ferlitsch.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH10_F06_Ferlitsch.png)'
- en: Figure 10.6 Hyperparameter random search
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6 超参数随机搜索
- en: 'Here is an example implementation of the first three steps. In this code, we
    do the following:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是前三个步骤的一个示例实现。在这段代码中，我们做了以下操作：
- en: Run five `trials` on the full search space. Because this example has just a
    small number of combinations, generally five trials is sufficient.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在整个搜索空间上运行五个`试验`。因为这个例子只有少数几种组合，通常五个试验就足够了。
- en: Select a random combination of the learning rate (`lr)` and batch size (`bs`).
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个随机组合的学习率（`lr`）和批量大小（`bs`）。
- en: Do a short training run on a pretrained instance of the model.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在模型的一个预训练实例上进行短时间训练。
- en: Maintain a running tally of the best validation accuracy and hyperparameter
    combination (`best`).
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 维护最佳验证准确率和超参数组合（`best`）的累计记录。
- en: Select the best validation accuracy from the five trials as the near-optimal.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从五个试验中选择最佳验证准确率作为接近最优的。
- en: Set a narrow search space around the near-optimal hyperparameters (2*X* and
    1/2X ).
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接近最优的超参数（2*X* 和 1/2X ）周围设置一个狭窄的搜索空间。
- en: Run five more trials within the narrow search space.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在狭窄的搜索空间内再运行五个试验。
- en: '[PRE4]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ If we did grid search, we would have 4 × 3 = 12 combinations.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果我们进行网格搜索，我们将有 4 × 3 = 12 种组合。
- en: '❷ Step 1: First round of trials, finds the best near-optimal combination'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 步骤 1：第一轮试验，找到最佳接近最优组合
- en: '❸ Step 2: Selects random combinations'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 步骤 2：选择随机组合
- en: '❹ Step 3: Does short training run for trial'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 步骤 3：为试验进行短时间训练
- en: '❺ Steps 4 and 5: Maintains tally of current best result'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 步骤 4 和 5：记录当前最佳结果
- en: '❻ Step 6: Narrows the search space to within the vicinity of the best near-optimal'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 步骤 6：将搜索空间缩小到最佳接近最优的附近
- en: '❼ Step 7: Runs another set of trials around narrowed search space'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 步骤 7：在缩小后的搜索空间周围运行另一组试验
- en: I ran this code without numerical stabilization, using the CIFAR-10 dataset.
    After the first five trials on the full search space, the best validation accuracy
    was 0.352\. After narrowing the search space, the best validation accuracy leaped
    to 0.487, with learning rate = 0.0002 and batch size = 64.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我在没有数值稳定化的情况下运行了这段代码，使用了 CIFAR-10 数据集。在完整搜索空间的前五个试验之后，最佳验证准确率为 0.352。在缩小搜索空间后，最佳验证准确率跃升至
    0.487，学习率为 0.0002，批量大小为 64。
- en: I then repeated the process, but this time I first did numerical stabilization
    on the model before doing hyperparameter search. I updated the `get_model()` method
    to fetch a copy of the saved numerical stabilized model. After the first five
    trials on the full search space, the best validation accuracy was 0.569\. After
    narrowing the search space, the best validation accuracy leaped to 0.576, with
    learning rate = 0.1 and batch size = 512\. Wow, that is amazingly better. And
    we haven’t yet tuned for learning rate scheduling, regularization, augmentation,
    and label smoothing!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我然后重复了这个过程，但这次我在进行超参数搜索之前首先对模型进行了数值稳定化。在完整搜索空间的前五个试验之后，最佳验证准确率为 0.569。在缩小搜索空间后，最佳验证准确率跃升至
    0.576，学习率为 0.1，批量大小为 512。哇，这真是太好了。而且我们还没有对学习率调度、正则化、增强和标签平滑进行调整！
- en: Next, we will discuss how to use the automated hyperparameter search tool KerasTuner,
    which is an add-on module to TF.Keras. You might ask, why learn a manual method
    when we can just use an automated method? Even with an automated method, you need
    to guide the search space. Using manual methods helps you gain expertise in guiding
    the search space. For me, and researchers, developing manual methods gives us
    insight into future improvements to automated searches. Finally, you may find
    that the out-of-the-box automated method is not well suited for your proprietary
    dataset and model, and you can improve it with your unique learned approach.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何使用自动超参数搜索工具KerasTuner，它是一个TF.Keras的附加模块。你可能会问，为什么我们要学习手动方法，而不是直接使用自动方法？即使使用自动方法，你也需要引导搜索空间。使用手动方法可以帮助你获得引导搜索空间的专长。对我来说，以及研究人员来说，开发手动方法让我们对未来自动搜索的改进有了洞察。最后，你可能会发现，现成的自动方法并不适合你的专有数据集和模型，你可以通过你独特的学习方法来改进它。
- en: 10.2.4 KerasTuner
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.4 KerasTuner
- en: '*KerasTuner* is an add-on module to TF.Keras for doing automated hyperparameter
    tuning. It has two methods: random search and hyperband search. For brevity, this
    section covers the random search method. An understanding of this method will
    give you insight to the overall approach for searching hyperparameters in what
    otherwise is a sparse space, when few good combinations exist within the larger
    search space.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '*KerasTuner* 是TF.Keras的一个附加模块，用于进行自动化超参数调整。它有两种方法：随机搜索和超参数搜索。为了简洁，本节将介绍随机搜索方法。了解这种方法将使您对在搜索空间稀疏且好的组合较少的情况下搜索超参数的整体方法有所了解。'
- en: Note I refer you to the online documentation ([https://keras-team.github .io/keras-tuner/](https://keras-team.github.io/keras-tuner/))
    for hyperband, a bandit algorithm approach for improving times for random search.
    You can find more information in “Hyberband” by Lisha Li et al. ([https://arxiv.org/abs/1603.06560](https://arxiv.org/abs/1603.06560)).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我建议您参考在线文档（[https://keras-team.github.io/keras-tuner/](https://keras-team.github.io/keras-tuner/)）了解超参数调整，这是一种用于改进随机搜索时间的bandit算法方法。您可以在李莎·李等人的“Hyberband”中找到更多信息（[https://arxiv.org/abs/1603.06560](https://arxiv.org/abs/1603.06560)）。
- en: Like all automated tools, KerasTuner has pros and cons. Being automated and
    fairly straightforward to use is obviously good. For me, the lack of the ability
    to tune the batch size is a big con, as you end up having to tune the batch size
    manually.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 像所有自动化工具一样，KerasTuner既有优点也有缺点。自动化且使用起来相对简单显然是优点。对我来说，无法调整批量大小是一个很大的缺点，因为你最终不得不手动调整批量大小。
- en: 'Here is the `pip` command for installing KerasTuner:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这是安装KerasTuner的`pip`命令：
- en: '[PRE5]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To use KerasTuner, we start by creating an instance of the tuner. In the following
    example, we instantiate an instance of the `RandomSearch` class. This instantiation
    takes three required parameters:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用KerasTuner，我们首先创建一个tuner实例。在以下示例中，我们创建了一个`RandomSearch`类的实例。这个实例化需要三个必需的参数：
- en: Hyperparameter tunable (`hp`) model
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可调整超参数（`hp`）模型
- en: The objective measurement (for example, validation accuracy)
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标测量（例如，验证准确率）
- en: The maximum number of training trials (experiments)
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练试验的最大数量（实验）
- en: '[PRE6]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Gets the hyperparameter tunable model
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取可调整超参数的模型
- en: ❷ Training metric to compare (improve on)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 用于比较（改进）的训练指标
- en: ❸ The number of training trials
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 训练试验次数
- en: In this example, I set the number of trials low (3) for demonstration purposes.
    At most, three random combinations will be tried. Depending on the size of your
    search space, you will generally use a larger number. This is a tradeoff. The
    more trials, the more of the search space is explored, but the more computational
    expense (time) required.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，为了演示目的，我将试验次数设置得较低（3次）。最多尝试三种随机组合。根据你的搜索空间大小，你通常会使用更大的数字。这是一个权衡。试验次数越多，探索的搜索空间就越大，但所需的计算成本（时间）也越高。
- en: Next, we create the function that instantiates a hyperparameter-tunable model.
    The function takes one parameter, denoted by `hp`. This is a hyperparameter control
    variable passed in by KerasTuner.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个函数来实例化一个可调整超参数的模型。该函数接受一个参数，表示为`hp`。这是一个由KerasTuner传入的超参数控制变量。
- en: 'In our example, we will tune just the learning rate. We start with getting
    an instance of a numerically stabilized version of our model, as I previously
    recommended. We then set the learning rate for the instance with the `optimizer`
    parameter in the `compile()` method. In our example, we will specify four choices
    of the learning rate by using the hyperparameter tuner (`hp`) control method `hp.Choice``()`.
    This tells the tuner the set of values for a parameter to search. In this case,
    we set the choices to `[1e-1, 1e-2, 1e-3, 1e-4]`:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们将仅调整学习率。我们首先获取我们模型的一个数值稳定的版本实例，正如我之前推荐的那样。然后，我们使用`compile()`方法中的`optimizer`参数设置实例的学习率。在我们的例子中，我们将使用超参数调整器（`hp`）控制方法`hp.Choice()`指定四个学习率的选择。这告诉调整器要搜索的参数值集合。在这种情况下，我们将选择设置为`[1e-1,
    1e-2, 1e-3, 1e-4]`：
- en: '[PRE7]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Loads the saved (on disk) model
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载已保存（在磁盘上）的模型
- en: ❷ Recompiles the model to reset the learning rate
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 重新编译模型以重置学习率
- en: ❸ Makes the learning rate a tunable parameter
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将学习率设置为可调整的参数
- en: 'Next, we are ready to perform the hyperparameter tuning. We initiate the search
    with the `search()` method of `tuner`. The method takes the same parameters as
    the Keras model `fit()` method. Note, the batch size is explicitly specified in
    `search()` and thus is not automatically tunable. In our example, our training
    data is the CIFAR-10 training data:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们准备进行超参数调整。我们使用`tuner`的`search()`方法开始搜索。该方法接受与Keras模型`fit()`方法相同的参数。注意，在`search()`中明确指定了批大小，因此它不是自动可调的。在我们的例子中，我们的训练数据是CIFAR-10训练数据：
- en: '[PRE8]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And now the results! First, use the `results_summary()` method to view a summary
    of the trials:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是结果！首先，使用`results_summary()`方法查看试验的摘要：
- en: '[PRE9]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here is the output, which shows 0.1 was the best learning rate:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出，它显示0.1是最佳学习率：
- en: '[PRE10]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ The selected best learning rate
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 选定的最佳学习率
- en: You then use the `get_best_models()` method to get the corresponding model.
    This method returns a list of the best models in descending order based on the
    parameter `num_models`. In this case, we want just the best one, so we set it
    to 1.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你使用`get_best_models()`方法来获取相应的模型。此方法根据参数`num_models`按降序返回最佳模型列表。在这种情况下，我们只想得到最佳的一个，所以我们将它设置为1。
- en: '[PRE11]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Finally, your results and models are stored in a folder, which can be specified
    by the parameter `project_name` when instantiating the tuner. When not specified,
    the folder name defaults to `untitled_project.` To clean up after your trials,
    you would delete this folder.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你的结果和模型存储在一个文件夹中，可以在实例化`tuner`时通过参数`project_name`指定。如果没有指定，文件夹名称默认为`untitled_project`。为了清理试验后的文件夹，你会删除这个文件夹。
- en: 10.3 Learning rate scheduler
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 学习率调度器
- en: So far in our examples, we have kept the learning rate constant throughout training.
    You can get good results with a constant learning rate, but it is not as effective
    as adjusting the learning rate during training.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在我们的示例中，我们一直在整个训练过程中保持学习率不变。你可以用恒定的学习率得到好的结果，但它不如在训练过程中调整学习率有效。
- en: Typically, you progress from a larger learning rate to a lower learning rate
    during training. Initially, you want to start with as large a learning rate as
    possible, without causing numerical instability. The larger learning rate allows
    the optimizer to explore different paths (local optima) to convergence and make
    some initial large gains in minimizing the loss to speed up training.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在训练过程中，你会从较大的学习率逐渐降低到较小的学习率。最初，你希望尽可能开始使用较大的学习率，而不引起数值不稳定性。较大的学习率允许优化器探索不同的路径（局部最优解），并在最小化损失方面取得一些初始的大幅收益，从而加快训练速度。
- en: But once we are making good progress toward a good local optimum, if we keep
    using the high learning rate, we may start oscillating back and forth without
    converging or inadvertently jump out of the good local optimum and start converging
    in a less good local optimum.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 但一旦我们朝着良好的局部最优解取得良好进展，如果我们继续使用高学习率，我们可能会开始来回震荡，无法收敛，或者无意中跳出良好的局部最优解，开始向较差的局部最优解收敛。
- en: So as we get closer to convergence, we start to lower the learning rate to make
    smaller and smaller steps, so as to not oscillate and find the best path in the
    local optimum to converge on.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，随着我们接近收敛，我们开始降低学习率，以采取越来越小的步骤，这样就不会震荡，并找到局部最优解中的最佳路径以收敛。
- en: 'So what is meant by the term *learning rate scheduler* ? It means that we will
    have a method that monitors the training process, and based on a certain condition
    makes a change to the learning rate to find and converge within the best or near
    local optimum. In this section, we will cover several common methods: including
    time decay, ramp, constant step, and cosine annealing. We will start with describing
    the time decay method, which is the method built into the TF.Keras set of optimizers
    for progressively lowering the learning rate during training.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，“学习率调度器”这个术语是什么意思呢？这意味着我们将有一个方法来监控训练过程，并根据一定的条件对学习率进行调整，以找到并收敛到最佳或近似的局部最优解。在本节中，我们将介绍几种常见的方法：包括时间衰减、斜坡、常数步长和余弦退火。我们将从描述时间衰减方法开始，这是TF.Keras优化器集内置的方法，用于在训练过程中逐步降低学习率。
- en: 10.3.1 Keras decay parameter
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.1 Keras衰减参数
- en: 'The TF.Keras optimizers support progressively lowering the learning rate with
    the `decay` parameter. The optimizers use the time-decay method. The mathematical
    formula for time decay is as follows, where *lr* is the learning rate, *k* is
    the decay and *t* is the number of iterations (for example, epochs):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: TF.Keras优化器支持使用`decay`参数逐步降低学习率。优化器使用时间衰减方法。时间衰减的数学公式如下，其中*lr*是学习率，*k*是衰减系数，*t*是迭代次数（例如，epochs）：
- en: '*lr* = *lr* 0 / (1 + *kt*)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*lr* = *lr* 0 / (1 + *kt*)'
- en: 'In TF.Keras, the time decay is implemented as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在TF.Keras中，时间衰减的实现方式如下：
- en: '*lr* = *lr* × (1.0 / (1.0 + decay × iterations))'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*lr* = *lr* × (1.0 / (1.0 + decay × iterations))'
- en: 'Here is an example of setting a time decay for the learning rate when specifying
    the optimizer in the `compile()` method:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在`compile()`方法中指定优化器时设置学习率时间衰减的示例：
- en: '[PRE12]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Table 10.1 shows the progression in learning rate for 10 epochs with the preceding
    settings; typical decay values are between 1e-3 and 1e-6.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.1显示了使用先前设置的前10个epochs的学习率进度；典型的衰减值在1e-3和1e-6之间。
- en: Table 10.1 Decay progression of learning rate over epochs
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.1 学习率随epoch的衰减进度
- en: '| Iteration (epoch) | Learning rate |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 迭代（epoch） | 学习率 |'
- en: '| 1 | 0.0999 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.0999 |'
- en: '| 2 | 0.0997 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.0997 |'
- en: '| 3 | 0.0994 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.0994 |'
- en: '| 4 | 0.0990 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.0990 |'
- en: '| 5 | 0.0985 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.0985 |'
- en: '| 6 | 0.0979 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 0.0979 |'
- en: '| 7 | 0.0972 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.0972 |'
- en: '| 8 | 0.0964 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 0.0964 |'
- en: '| 9 | 0.0955 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 0.0955 |'
- en: '| 10 | 0.0945 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.0945 |'
- en: 10.3.2 Keras learning rate scheduler
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.2 Keras学习率调度器
- en: If using time decay does not produce optimal results, you can implement your
    own custom method for progressively lowering the learning rate, using the `LearningRateScheduler`
    callback. It’s not uncommon in a production environment for the ML team over time
    to experiment and find custom tweaks that make the training more time efficient
    and result in better outcomes on the objective, such as accuracy on classification
    when deployed into production.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用时间衰减没有产生最佳结果，你可以使用`LearningRateScheduler`回调函数实现自己的自定义方法来逐步降低学习率。在生产环境中，ML团队随着时间的推移进行实验并找到自定义调整，使训练更加高效，并在目标上产生更好的结果，例如在生产部署时的分类准确率。
- en: 'The following code is an example implementation whose steps are outlined here:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是一个示例实现，其步骤在此概述：
- en: Define a function for our learning rate scheduler callback.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义我们的学习率调度器回调函数。
- en: During training (via the `fit()` method), the parameters passed to the callback
    are the current epoch count (`epoch`) and learning rate (`lr`).
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练过程中（通过`fit()`方法），传递给回调函数的参数是当前epoch计数（`epoch`）和学习率（`lr`）。
- en: For the first epoch, return the current (initial) learning value.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于第一个epoch，返回当前的（初始）学习值。
- en: Otherwise, implement a progressively lowering of the learning rate.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 否则，实现一个逐步降低学习率的方法。
- en: Instantiate a callback for the learning rate scheduler.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个用于学习率调度器的回调函数。
- en: Pass in the callback to the `fit()` method.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将回调函数传递给`fit()`方法。
- en: '[PRE13]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '❶ Step 3: For the first (0) epoch, starts with the initial learning rate'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 步骤3：对于第一个（0）epoch，从初始学习率开始
- en: '❷ Step 4: Adds your implementation for progressively lowering the learning
    rate'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 步骤4：添加你的逐步降低学习率的实现
- en: '❸ Step 1: Sets the initial learning rate'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 步骤1：设置初始学习率
- en: '❹ Step 5: Creates the callback for the learning rate scheduler'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 步骤5：创建学习率调度器的回调函数
- en: '❺ Steps 2 and 6: Enables the learning rate scheduler for training'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 步骤2和6：为训练启用学习率调度器
- en: 10.3.3 Ramp
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.3 渐增
- en: 'So, you’ve done the pretraining step for numerical stability and the hyperparameter
    tuning for batch size and initial learning rate. Now you’re ready to tackle implementing
    your algorithm for the learning rate scheduler. Often you can do that with a ramp
    algorithm, which resets the learning rate at a specified number of epochs. Typically,
    at this point I will do an extended training run. I usually start with 50 epochs
    and set an early stop condition on the valuation loss (with a `patience` of 2).
    Regardless of the dataset, I tend to see one of two things:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你已经完成了数值稳定性的预训练步骤和批量大小以及初始学习率的超参数调整。现在你准备好实现你的学习率调度器算法了。通常，你可以使用一个渐增算法来实现这一点，该算法在指定数量的epochs后重置学习率。通常，在这个阶段，我会进行一次扩展的训练运行。我通常从50个epochs开始，并在评估损失上设置一个提前停止条件（`patience`为2）。无论数据集如何，我通常会看到两种情况之一：
- en: Steady and consistent reduction in validation loss through the last (50) epochs.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最后（50）个epochs中，评估损失保持稳定和一致地减少。
- en: Before the last epoch, a plateauing occurs in the validation loss and the early
    stop has kicked in.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最后一个时期之前，验证损失出现平台期，并且提前停止已经启动。
- en: If I see a steady reduction in the validation loss, I will continue to repeat
    for an additional 50 epochs, until I have an early stop.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我看到验证损失持续减少，我将继续重复额外的50个时期，直到出现提前停止。
- en: Once I have an early stop, I look at what epoch it occurred at. Let’s say it
    was on epoch 40\. I will then subtract a few epochs, typically 5 (in this case,
    resulting in 35). I then hardwire my learning rate scheduler to do a one-magnitude
    drop in learning rate at the epoch. Nearly 100% of the time, my training improves
    to a lower validation loss and higher validation accuracy. Figure 10.7 shows a
    ramped-down learning rate.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我设置了提前停止，我会查看它发生在哪个时期。比如说，它发生在第40个时期。然后我会减去几个时期，通常是5个（在这种情况下，结果是35）。然后我将我的学习率调度器硬编码为在该时期降低一个数量级的学习率。几乎100%的情况下，我的训练会改善到更低的验证损失和更高的验证准确率。图10.7显示了降低的学习率。
- en: '![](Images/CH10_F07_Ferlitsch.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH10_F07_Ferlitsch.png)'
- en: Figure 10.7 Ramped-down learning rate
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7 降低的学习率
- en: 'The following is an example implementation of a ramp learning rate scheduler:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个斜坡学习率调度器的示例实现：
- en: '[PRE14]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Sets the epoch for ramping down a magnitude
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置降低一个数量级的时期
- en: ❷ Lowers the learning rate by a magnitude when it is at the ramp epoch
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在斜坡时期降低学习率一个数量级
- en: This is typically not my last step, but instead I use it to get an idea of what
    the loss landscape likely looks like for this dataset. From that, I plan my full
    training learning rate scheduler. At this level, it would be too challenging to
    explain a loss landscape. I will instead cover a variety of learning rate scheduler
    strategies you can try.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常不是我的最后一步，而是我用它来了解这个数据集的损失地形可能是什么样子。从那以后，我计划我的完整训练学习率调度器。在这个层面上，解释损失地形会太具有挑战性。相反，我将介绍你可以尝试的各种学习率调度器策略。
- en: 10.3.4 Constant step
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.4 恒定步长
- en: In the constant step method, we want to go from the initial learning rate to
    zero on the last epoch in even increments. The method is straightforward. You
    take the initial learning rate and divide it by the number of epochs. Figure 10.8
    illustrates the method.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在恒定步长方法中，我们希望在最后一个时期内以等量递增的方式从初始学习率到零。这个方法很简单。你将初始学习率除以时期数。图10.8展示了这个方法。
- en: '![](Images/CH10_F08_Ferlitsch.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH10_F08_Ferlitsch.png)'
- en: Figure 10.8 Constant step learning rate
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8 恒定步长学习率
- en: 'This is an example implementation of the step method for a learning rate scheduler:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个学习率调度器的步长方法的示例实现：
- en: '[PRE15]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ The number of epochs for training
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练的时期数
- en: ❷ The initial learning rate determined by hyperparameter tuning
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 由超参数调整确定的初始学习率
- en: ❸ The size of the step decay after each epoch
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 每个时期后步长衰减的大小
- en: 10.3.5 Cosine annealing
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.5 余弦退火
- en: The *cosine annealing method* has been popular among researchers and appears
    frequently in research papers about ablation studies. It is also known as a *cyclic
    learning rate*. The concept here is, instead of progressively lowering the learning
    rate across training, to do it in cycles.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '*余弦退火方法*在研究人员中很受欢迎，在关于消融研究的学术论文中经常出现。它也被称为*循环学习率*。这里的理念是，不是在训练过程中逐渐降低学习率，而是在周期中这样做。'
- en: More simply, we start with the initial learning rate and progressively lower
    to a lower learning rate, and then we progressively raise it again. We continuously
    repeat this cycle, but each time the rate that the cycle starts at (high) and
    ends at (low) is lower—so we are still progressing lower across the cycles.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 更简单地来说，我们从初始学习率开始，逐渐降低到一个较低的学习率，然后我们再次逐渐提高它。我们持续重复这个循环，但每次循环开始时的速率（高）和结束时的速率（低）都更低——因此我们在循环中仍然在向更低的方向进步。
- en: 'So what’s the advantage? It provides the opportunity to periodically explore
    other local optima (jump out) and to escape saddle points. For the local optima,
    it’s like doing a beam search. The training will likely jump out of a current
    local optimum and start diving into another. While at first nothing indicates
    that the new local optimum is better, eventually it will. Here’s why. As the training
    progresses, we will dive deeper into better optima than less good ones. With the
    declining upper learning rate, it becomes less and less likely we would jump out
    of a good local optimum. Another way to think of this cyclic behavior is as exploration
    versus exploitation: on the high end of the cycle, the training is exploring new
    paths, and on the low end, it is exploiting good paths. As we progress in training,
    we move to less and less exploration and to more and more exploitation.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，它的优势是什么？它提供了定期探索其他局部最优解（跳出）和逃离鞍点的机会。对于局部最优解，它就像进行一次梁搜索。训练过程可能会跳出当前的局部最优解，并开始深入另一个。虽然一开始没有任何迹象表明新的局部最优解更好，但最终会是这样。原因如下。随着训练的进行，我们将深入到比较差的局部最优解更好的局部最优解。随着学习率的下降，我们跳出好的局部最优解的可能性越来越小。另一种思考这种周期性行为的方式是探索与利用：在周期的较高端，训练正在探索新的路径，而在较低端，它正在利用好的路径。随着训练的进展，我们逐渐减少探索，增加利用。
- en: Another advantage is that after we are diving deep with the lower end of the
    learning rate cycle, we may get stuck on a saddle point. Let’s use the following
    diagram to aid in understanding what a saddle point is.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个优势是，在我们使用学习率周期的低端深入挖掘后，我们可能会卡在鞍点上。让我们使用以下图表来帮助理解鞍点是什么。
- en: If our features (independent variables) have a linear relationship to the labels
    (dependent variables), once we discover the slope of change, we will dive to the
    global optimum, regardless of the learning rate (as depicted in the first curve).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的特征（自变量）与标签（因变量）之间存在线性关系，一旦我们发现了变化率，我们就会深入到全局最优解，无论学习率如何（如图中第一条曲线所示）。
- en: On the other hand, if the relationship is polynomial, we will see something
    more like a convex curve, with the global optimum as the lowest point of the curve.
    In principle, as long as we continuously reduce the learning rate, we will descend
    to the lowest point, avoiding bouncing back and forth between the sides of the
    curve (as depicted in the second curve).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果关系是多项式的，我们将看到更像是凸曲线的东西，全局最优解作为曲线的最低点。原则上，只要我们持续降低学习率，我们就会下降到最低点，避免在曲线两侧来回弹跳（如图中第二条曲线所示）。
- en: But the power of deep learning is with features that have a nonlinear (and nonpolynomial)
    relationship to the labels (as depicted in the third curve). In this case, think
    of the loss space consisting of valleys, peaks, and saddle points, and one valley
    is the global optima. Our goal is, of course, to find that valley, hence the advantage
    of exploring multiple local optima (valleys).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 但深度学习的力量在于特征与标签之间存在非线性（和非多项式）关系（如图中第三条曲线所示）。在这种情况下，考虑损失空间由山谷、山峰和鞍点组成，一个山谷是全局最优解。我们的目标当然是找到这个山谷，这就是探索多个局部最优解（山谷）的优势。
- en: A saddle point is a portion of the loss space in a valley that has a plateau;
    it levels off before continuing down. If our learning rate is very low, we will
    bounce around the plateau endlessly. So, while we want that tiny learning rate
    as we near the end of the training, we want it to occasionally go up to push us
    off saddle points on the way down to the lowest point.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 鞍点是在山谷中具有平台的部分；它在继续下降之前变得平坦。如果我们的学习率非常低，我们将在平台上无休止地弹跳。因此，虽然我们希望在训练接近结束时拥有那个很小的学习率，但我们希望它偶尔上升，以推动我们在下降到最低点时离开鞍点。
- en: Figure 10.9 contrasts the loss surface between linear/polynomial to a nonlinear
    relationship, which shows peaks, valleys, and plateaus—which can become a saddle
    point.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9 对比了线性/多项式与非线性关系之间的损失表面，显示了峰值、谷值和平台——这些可以成为鞍点。
- en: '![](Images/CH10_F09_Ferlitsch.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH10_F09_Ferlitsch.png)'
- en: Figure 10.9 Gradient descent and slope of rate of change
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9 梯度下降和学习率变化率斜率
- en: When using cosine decay in conjunction with early stopping, we must rethink
    the objective (validation accuracy) for stopping. If we are training with a noncyclic
    decay, we would likely use a very small threshold of difference between epochs
    before stopping. But with cyclic behavior, we are likely to see sudden spikes
    in difference (increased validation loss) when exploring (the high end of cycle).
    Thus, we need to use a wider gap for the early stop. The alternative is to use
    a custom early stop that progressively decreases the difference in conjunction
    with the high end of the cycle decreasing.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用余弦衰减与早停结合时，我们必须重新思考停止的目标（验证准确率）。如果我们使用非周期性衰减进行训练，我们可能会在停止前使用非常小的阈值差异。但是，由于周期性行为，我们在探索（周期的末端）时可能会看到差异的突然激增（验证损失增加）。因此，我们需要为早停使用更大的差距。另一种选择是使用自定义早停，随着周期末端的降低，逐渐减小差异。
- en: The following is an example implementation of a learning rate scheduler using
    cosine decay. The function is a bit complex. We are using the cosine function
    `np.cos``()` to generate a sine wave from 0 to 1\. For example, cosine(**π**)
    is –1 and cosine(2**π**) is 1, so the calculation of the value to pass to `np.cos()`
    is a multiple of **π**. So that the value is positive, a 1 is added to the calculation,
    and the result will now be in the range of 0 to 2\. This value is then reduced
    by one-half (0.5 times), so the result will now be in the range of 0 to 1\. The
    decay is then adjusted by `alpha`, which sets a lower bound on the minimum learning
    rate.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个使用余弦衰减的学习率调度器的示例实现。该函数有点复杂。我们使用余弦函数`np.cos()`从0到1生成正弦波。例如，余弦(π)是-1，余弦(2π)是1，所以传递给`np.cos()`的值的计算是π的倍数。这样，值就是正的，计算中加1，结果现在将在0到2的范围内。然后，该值减半（0.5倍），所以结果现在将在0到1的范围内。然后，衰减通过alpha调整，它设置最小学习率的下限。
- en: '[PRE16]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Calculates cosine value between 0 and 2 and reduces by one-half
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算介于0和2之间的余弦值并减半
- en: ❷ Adjusts value by alpha
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过alpha调整值
- en: ❸ Returns the decayed learning rate
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回衰减后的学习率
- en: ❹ Connects learning rate scheduler callback to cosine decay function
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将学习率调度器回调连接到余弦衰减函数
- en: 'In TF 2.x, cosine decay was added as a built-in learning rate scheduler:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在TF 2.x中，余弦衰减被添加为内置的学习率调度器：
- en: '[PRE17]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Imports the CosineDecay built-in learning rate scheduler
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入CosineDecay内置学习率调度器
- en: ❷ Instantiates the CosineDecay learning rate scheduler
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 实例化CosineDecay学习率调度器
- en: ❸ Adds the learning rate scheduler as a callback during training
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在训练期间将学习率调度器作为回调添加
- en: 10.4 Regularization
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4 正则化
- en: The next important hyperparameter is *regularization*. This refers to methods
    of adding noise to the training such that the model doesn’t memorize the training
    data. The longer we can delay memorization, the better the opportunity to gain
    higher accuracy in the model when predicting on data not trained on—such as the
    test (holdout) data.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个重要的超参数是*正则化*。这指的是向训练中添加噪声的方法，使得模型不会记住训练数据。我们可以延迟记忆的时间越长，在预测未训练数据（如测试（保留）数据）时获得更高模型准确率的机会就越好。
- en: Let’s restate this more simply. We want the model to learn the essential features
    (generalization) and not the data (memorization).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更简单地重申这一点。我们希望模型学习基本特征（泛化），而不是数据（记忆）。
- en: 'A note on dropout for regularization: nobody does that anymore; it’s arcane.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 关于正则化中的dropout的说明：现在没有人那样做了；这是古老的。
- en: 10.4.1 Weight regularization
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.1 权重正则化
- en: The most widely used form of regularization currently is *weight regularization*,
    also referred to as *weight decay*. Weight regularization is applied on a per
    layer basis. Its purpose is to add noise to the update of the weights during backward
    propagation that is relative to the size of the weights. This noise is commonly
    referred to as a *penalty*, and layers with larger weights have a larger penalty
    than layers with smaller weights.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 目前最广泛使用的正则化形式是*权重正则化*，也称为*权重衰减*。权重正则化是按层应用。其目的是在反向传播中向权重更新添加与权重大小相关的噪声。这种噪声通常被称为*惩罚*，权重较大的层比权重较小的层有更大的惩罚。
- en: 'Without diving deep into gradient descent and backward propagation, it is sufficient
    to say that the loss calculation is part of the computation for updating the weights
    at each layer. For example, in a regressor model, we typically use a mean square
    error for the loss between the predicted values (*ŷ*) and the actual (ground truth
    – *y*) values, which can be denoted as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 不深入探讨梯度下降和反向传播，可以说损失计算是更新每一层权重计算的一部分。例如，在回归器模型中，我们通常使用均方误差来表示预测值 (*ŷ*) 和实际（真实
    – *y*）值之间的损失，可以表示如下：
- en: loss = MSE(*ŷ*, *y*)
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数 = MSE(*ŷ*, *y*)
- en: 'To add noise per layer, we want to add a tiny bit as a penalty in proportion
    to the size of the weights:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为每一层添加噪声，我们希望按权重大小比例添加一小部分作为惩罚：
- en: loss = MSE(*ŷ*, *y*) + *penalty*
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数 = MSE(*ŷ*, *y*) + *penalty*
- en: penalty = *decay* × *R*(*w*)
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 惩罚 = *decay* × *R*(*w*)
- en: 'Here, *decay* is the weight decay, which is a value << 1\. And *R*(*w*) is
    a regularizer function applied to the weights *w* for that layer. TF.Keras supports
    the following regularizer functions:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*decay* 是权重衰减，其值 << 1。而 *R*(*w*) 是应用于该层权重 *w* 的正则化函数。TF.Keras支持以下正则化函数：
- en: '*L1*—Sum of the absolute weights, also known as *Lasso regularization*'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L1*—绝对权重的总和，也称为 *Lasso 正则化*'
- en: '*L2*—Sum of the squared weights, also known as *Ridge regularization*'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L2*—平方权重的总和，也称为 *Ridge 正则化*'
- en: '*L1L2*—Sum of the absolute and the squared weights, also referred to as *Elastic
    Net regularization*'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L1L2*—绝对和平方权重的总和，也称为 *Elastic Net 正则化*'
- en: Ablation studies cited in modern SOTA research papers use L2 weight regularization
    around the value ranges of 0.0005 to 0.001\. From my own experience, I found values
    above 0.001 to be too aggressive in weight regularization, and the training does
    not converge.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 现代SOTA研究论文中引用的消融研究使用L2权重正则化，其值范围在0.0005到0.001之间。根据我的经验，我发现0.001以上的值在权重正则化上过于激进，并且训练无法收敛。
- en: 'In TF.Keras, the keyword parameter `kernel_regularizer` is used to set weight
    regularization per layer. If you use it, you should specify it on all layers that
    have learned parameters (for example, `Conv2D`, `Dense)`. Here is an example implementation
    of specifying an `L2` weight decay regularization for a convolutional layer (`Conv2D`):'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在TF.Keras中，使用关键字参数 `kernel_regularizer` 来设置每层的权重正则化。如果您使用它，您应该在所有具有学习参数的层上指定它（例如，`Conv2D`，`Dense`）。以下是一个为卷积层（`Conv2D`）指定`L2`权重衰减正则化的示例实现：
- en: '[PRE18]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 10.4.2 Label smoothing
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.2 标签平滑
- en: '*Label* *smoothing* approaches regularization from a different direction. Up
    to now, we discussed techniques of adding noise to prevent memorization, so that
    models will generalize to examples within the same distribution that was not seen
    by the model during training.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '*标签* *平滑*方法从不同的角度进行正则化。到目前为止，我们讨论了添加噪声以防止记忆化的技术，从而使模型能够泛化到模型在训练期间未见过的同一分布内的示例。'
- en: What we find, though, is that even when we penalize these weight updates to
    prevent memorization, these models tend to be overconfident in their prediction
    (high probability value).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们发现，即使我们惩罚这些权重更新以防止记忆化，这些模型在预测上往往过于自信（高概率值）。
- en: When a model is overconfident, the distance between the ground-truth label and
    non-ground-truth labels can vary greatly. When plotted, it would tend to appear
    more of a scatter plot than a cluster; it would be more desirable if the ground-truth
    labels clustered together, even if the confidence is lower. Figure 10.10 depicts
    an overconfident model using hard targets for labels.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型过于自信时，真实标签和非真实标签之间的距离可以有很大差异。当绘制时，它更倾向于看起来像散点图而不是簇；如果真实标签聚集在一起，即使置信度较低，这也是更理想的情况。图10.10展示了使用硬目标标签的过于自信的模型。
- en: '![](Images/CH10_F10_Ferlitsch.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH10_F10_Ferlitsch.png)'
- en: Figure 10.10 Labels as hard targets when training as one-hot-encoded labels
    (0 or 1)
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10 训练时作为独热编码标签（0或1）的标签
- en: Label smoothing aids in generalizing the model by making the predictions less
    confident, which results in the distances between ground truths and non-ground-truths
    to cluster together.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 标签平滑通过使预测不那么自信来帮助模型泛化，这导致真实标签和非真实标签之间的距离聚集在一起。
- en: In label smoothing, we alter the one-hot-encoding labels (ground truths) from
    being absolute certainty (1 and 0) to something less than absolute certainty,
    denoted as *α* (alpha). For example, for the ground-truth label instead of setting
    the value to 1 (100%), we set it to something slightly less, like 0.9 (90%), and
    then change all the non-truths from 0 (0%) to the same amount we lowered the ground-truth
    label (for example, 10%).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在标签平滑中，我们将one-hot编码的标签（真实值）从绝对确定性（1和0）调整为小于绝对确定性，用*α*（阿尔法）表示。例如，对于真实标签，我们不是将其值设置为1（100%），而是将其设置为略低一些的值，比如0.9（90%），然后将所有非真实值从0（0%）调整到降低真实标签的相同数量（例如，10%）。
- en: Figure 10.11 illustrates label smoothing. In this depiction, the predictions
    from the outputting dense layer are compared to the ground-truth labels after
    label smoothing, referred to as *soft targets*. The loss is calculated from the
    soft targets instead of the hard targets, which has been shown in practice to
    make the distance between the ground truth and non-ground truth more consistent.
    These distances then are more likely to form clusters, which aids in the model
    being more generalized.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11说明了标签平滑。在这个描述中，将输出密集层的预测与标签平滑后的真实标签进行比较，称为*软目标*。损失是从软目标而不是硬目标计算出来的，这在实践中已被证明可以使真实值和非真实值之间的距离更加一致。这些距离更有可能形成簇，这有助于模型更加泛化。
- en: '![](Images/CH10_F11_Ferlitsch.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH10_F11_Ferlitsch.png)'
- en: Figure 10.11 Label smoothing as soft targets when labels are less than absolutely
    confident
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11 标签平滑作为当标签小于绝对确定性时的软目标
- en: In TF 2.x, label smoothing is built into the loss functions. To use, explicitly
    instantiate the corresponding loss function and set the keyword parameter `label_smoothing`.
    In practice, the *α* factor is kept small, with 0.1 being the most widely used
    value.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在TF 2.x中，标签平滑内置在损失函数中。要使用它，显式实例化相应的损失函数并设置关键字参数`label_smoothing`。在实践中，*α*因子保持较小，0.1是最常用的值。
- en: '[PRE19]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Sets the label smoothing when compiling the model
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在编译模型时设置标签平滑
- en: Next, we will summarize everything we covered on hyperparameters and how they
    impact achieving the optimal outcome in training time and objective (for example,
    accuracy) during training.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将总结我们在超参数方面所涵盖的内容，以及它们如何影响在训练时间和目标（例如，准确率）方面实现最佳结果。
- en: 10.5 Beyond computer vision
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.5 超越计算机视觉
- en: 'All deep learning model architectures, regardless of the data type or field,
    have tunable hyperparameters. And the strategies for tuning them are the same.
    Regardless of whether you are working with computer vision, natural-language understanding,
    or structured data, the big four hyperparameters exist in all deep learning fields:
    learning rate, learning rate decay, batch size, and regularization.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 所有深度学习模型架构，无论数据类型或领域，都有可调整的超参数。调整它们的策略是相同的。无论你是在处理计算机视觉、自然语言理解还是结构化数据，深度学习领域的四大超参数都存在：学习率、学习率衰减、批量大小和正则化。
- en: The hyperparameters for regularization can vary in type across model architectures
    and across different fields. Many times they do not. For example, weight decay
    can be applied to any layer with learnable weights, regardless of whether it is
    a computer vision, NLU, or structured data model.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化的超参数在模型架构和不同领域之间可能类型不同。很多时候它们并不不同。例如，权重衰减可以应用于任何具有可学习权重的层，无论它是计算机视觉、NLU还是结构化数据模型。
- en: Some model architectures, like deep neural networks and boosted trees, have
    some historically unique hyperparameters. For example, for DNNs, you may see the
    tuning of the number of layers and number of units per layer. For boosted trees,
    you may see tuning the number of trees and leaves. But, since the division into
    hyperparameters (for training the model) and metaparameters (for configuring the
    model architecture), these tunable parameters are now referred to as *metaparameters*.
    So, if you’re tuning the number of layers and units in conjunction with the learning
    rate on a deep neural network, you are, in effect, doing macro-architecture search
    and hyperparameter tuning in parallel.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型架构，如深度神经网络和提升树，有一些历史上独特的超参数。例如，对于DNN，你可能看到调整层数和每层的单元数。对于提升树，你可能看到调整树的数量和叶子数。但是，由于超参数（用于训练模型）和元参数（用于配置模型架构）的划分，这些可调整的参数现在被称为*元参数*。因此，如果你在深度神经网络中同时调整层数和单元数以及学习率，实际上你正在进行宏观架构搜索和超参数调整的并行操作。
- en: Summary
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Different weight distributions and draws affect convergence during training.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的权重分布和抽取会影响训练过程中的收敛性。
- en: The difference between searching for an optimal weight initialization (lottery
    principle) versus learning the optimal weight initialization (warm-up) is that
    the model learns the best initialization instead of empirically finding it.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在搜索最佳权重初始化（抽签原则）与学习最佳权重初始化（预热）之间的区别在于，模型学习最佳初始化而不是通过经验找到它。
- en: A manual approach to hyperparameter search is best used when the dataset is
    small, and has the drawback that you might overlook hyperparameter values that
    achieve a better outcome during training.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据集较小时，使用手动方法进行超参数搜索是最佳选择，但其缺点是您可能会忽略在训练过程中实现更好结果的超参数值。
- en: Grid search is used in a small search space, and random search is substantially
    more efficient in a larger search space for hyperparameter tuning.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网格搜索用于小搜索空间，而在大搜索空间中进行超参数调整时，随机搜索效率更高。
- en: Using KerasTuner for hyperparameter search allows you to automate the search,
    but has the disadvantage that you cannot hand-guide the search.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用KerasTuner进行超参数搜索可以自动化搜索过程，但其缺点是您无法手动引导搜索。
- en: Various algorithms are used for learning rate decay, such as time decay, constant
    step, ramp step, and cosine annealing.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于学习率衰减的各种算法包括时间衰减、恒定步长、斜坡步长和余弦退火。
- en: Setting up a learning rate scheduler involves defining a callback function,
    implementing the custom learning rate algorithm in the callback function, and
    adding the callback function to the `fit()` method.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置学习率调度器涉及定义回调函数，在回调函数中实现自定义学习率算法，并将回调函数添加到`fit()`方法中。
- en: The conventional approaches for regularization are weight decay and label smoothing.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常规的正则化方法包括权重衰减和标签平滑。
