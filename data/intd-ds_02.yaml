- en: Chapter 3\. Machine learning
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3章. 机器学习
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Understanding why data scientists use machine learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解为什么数据科学家使用机器学习
- en: Identifying the most important Python libraries for machine learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定最重要的Python机器学习库
- en: Discussing the process for model building
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论模型构建的过程
- en: Using machine learning techniques
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习技术
- en: Gaining hands-on experience with machine learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得机器学习的实践经验
- en: Do you know how computers learn to protect you from malicious persons? Computers
    filter out more than 60% of your emails and can learn to do an even better job
    at protecting you over time.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道计算机是如何学会保护你免受恶意人员侵害的吗？计算机可以过滤掉你超过60%的电子邮件，并且随着时间的推移，它们可以学会做得更好，以保护你。
- en: Can you explicitly teach a computer to recognize persons in a picture? It’s
    possible but impractical to encode all the possible ways to recognize a person,
    but you’ll soon see that the possibilities are nearly endless. To succeed, you’ll
    need to add a new skill to your toolkit, *machine learning*, which is the topic
    of this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你能明确教计算机在图片中识别人物吗？虽然可能，但将所有可能的识别方法编码进去是不切实际的，但你很快就会看到可能性几乎是无穷无尽的。为了成功，你需要给你的工具箱添加一项新技能，*机器学习*，这正是本章的主题。
- en: 3.1\. What is machine learning and why should you care about it?
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1. 什么是机器学习，为什么你应该关注它？
- en: “Machine learning is a field of study that gives computers the ability to learn
    without being explicitly programmed.”
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “机器学习是一个研究领域，它赋予计算机在没有明确编程的情况下学习的能力。”
- en: ''
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Arthur Samuel, 1959^([[1](#ch03fn01)])*'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*亚瑟·萨缪尔，1959^([[1](#ch03fn01)])*'
- en: ¹
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Although the following paper is often cited as the source of this quote, it’s
    not present in a 1967 reprint of that paper. The authors were unable to verify
    or find the exact source of this quote. See Arthur L. Samuel, “Some Studies in
    Machine Learning Using the Game of Checkers,” *IBM Journal of Research and Development*
    3, no. 3 (1959):210–229.
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 尽管以下论文经常被引用为这句话的来源，但它并未出现在1967年的论文重印版中。作者们无法验证或找到这句话的确切来源。参见Arthur L. Samuel，“使用国际象棋游戏进行机器学习的一些研究”，*IBM研究与发展杂志*
    3，第3期（1959年）：210–229。
- en: The definition of machine learning coined by Arthur Samuel is often quoted and
    is genius in its broadness, but it leaves you with the question of how the computer
    learns. To achieve machine learning, experts develop general-purpose algorithms
    that can be used on large classes of learning problems. When you want to solve
    a *specific task* you only need to feed the algorithm more *specific data*. In
    a way, you’re programming by example. In most cases a computer will use data as
    its source of information and compare its output to a desired output and then
    correct for it. The more data or “experience” the computer gets, the better it
    becomes at its designated job, like a human does.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 亚瑟·萨缪尔提出的机器学习的定义经常被引用，其广泛性堪称天才，但它也留下了你如何让计算机学习的疑问。为了实现机器学习，专家们开发了通用的算法，这些算法可以用于解决大量学习问题。当你想要解决一个*特定任务*时，你只需要向算法提供更多*特定数据*。从某种意义上说，你是在通过示例编程。在大多数情况下，计算机将使用数据作为其信息来源，将其输出与期望输出进行比较，然后对其进行纠正。计算机获得的数据或“经验”越多，它在指定任务上的表现就越好，就像人类一样。
- en: 'When machine learning is seen as a process, the following definition is insightful:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当机器学习被视为一个过程时，以下定义是富有洞察力的：
- en: “Machine learning is the process by which a computer can work more accurately
    as it collects and learns from the data it is given.”
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “机器学习是计算机在收集和学习给定数据的过程中工作更准确的过程。”
- en: ''
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Mike Roberts^([[2](#ch03fn02)])*'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*迈克·罗伯茨^([[2](#ch03fn02)])*'
- en: ²
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mike Roberts is the technical editor of this book. Thank you, Mike.
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 迈克·罗伯茨是本书的技术编辑。感谢，迈克。
- en: For example, as a user writes more text messages on a phone, the phone learns
    more about the messages’ common vocabulary and can predict (autocomplete) their
    words faster and more accurately.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当用户在手机上发送更多短信时，手机就会对短信的常用词汇有更多了解，并能更快、更准确地预测（自动完成）它们的单词。
- en: In the broader field of science, machine learning is a subfield of artificial
    intelligence and is closely related to applied mathematics and statistics. All
    this might sound a bit abstract, but machine learning has many applications in
    everyday life.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在更广泛的科学领域，机器学习是人工智能的一个子领域，与应用数学和统计学密切相关。所有这些都可能听起来有些抽象，但机器学习在日常生活中有许多应用。
- en: 3.1.1\. Applications for machine learning in data science
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1. 机器学习在数据科学中的应用
- en: '*Regression* and *classification* are of primary importance to a data scientist.
    To achieve these goals, one of the main tools a data scientist uses is machine
    learning. The uses for regression and automatic classification are wide ranging,
    such as the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*回归*和*分类*对数据科学家来说至关重要。为了实现这些目标，数据科学家使用的主要工具之一是机器学习。回归和自动分类的用途非常广泛，例如以下内容：'
- en: Finding oil fields, gold mines, or archeological sites based on existing sites
    (classification and regression)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据现有地点找到油田、金矿或考古遗址（分类和回归）
- en: Finding place names or persons in text (classification)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在文本中找到地名或人物（分类）
- en: Identifying people based on pictures or voice recordings (classification)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据图片或声音录音识别人员（分类）
- en: Recognizing birds based on their whistle (classification)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据鸟鸣声识别鸟类（分类）
- en: Identifying profitable customers (regression and classification)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别有利可图的客户（回归和分类）
- en: Proactively identifying car parts that are likely to fail (regression)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主动识别可能失效的汽车部件（回归）
- en: Identifying tumors and diseases (classification)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别肿瘤和疾病（分类）
- en: Predicting the amount of money a person will spend on product X (regression)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测一个人在产品X上花费的金额（回归）
- en: Predicting the number of eruptions of a volcano in a period (regression)
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测一段时间内火山喷发次数（回归）
- en: Predicting your company’s yearly revenue (regression)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测公司年度收入（回归）
- en: Predicting which team will win the Champions League in soccer (classification)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测哪支足球队将赢得欧洲冠军联赛（分类）
- en: 'Occasionally data scientists build a *model* (an abstraction of reality) that
    provides insight to the underlying processes of a phenomenon. When the goal of
    a model isn’t prediction but interpretation, it’s called *root cause analysis*.
    Here are a few examples:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有时数据科学家会构建一个*模型*（现实的抽象），它可以为现象的潜在过程提供洞察。当模型的目标不是预测而是解释时，它被称为*根本原因分析*。以下是一些例子：
- en: Understanding and optimizing a business process, such as determining which products
    add value to a product line
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解和优化业务流程，例如确定哪些产品为产品线增值
- en: Discovering what causes diabetes
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现糖尿病的成因
- en: Determining the causes of traffic jams
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定交通拥堵的原因
- en: This list of machine learning applications can only be seen as an appetizer
    because it’s ubiquitous within data science. Regression and classification are
    two important techniques, but the repertoire and the applications don’t end, with
    clustering as one other example of a valuable technique. Machine learning techniques
    can be used throughout the data science process, as we’ll discuss in the next
    section.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这份机器学习应用列表只能被视为开胃菜，因为它在数据科学中无处不在。回归和分类是两种重要的技术，但技巧和应用的范畴并不止于此，聚类就是一个有价值的技巧的例子。机器学习技术可以在数据科学过程的各个阶段使用，我们将在下一节中讨论。
- en: 3.1.2\. Where machine learning is used in the data science process
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2. 机器学习在数据科学过程中的应用
- en: Although machine learning is mainly linked to the data-modeling step of the
    data science process, it can be used at almost every step. To refresh your memory
    from previous chapters, the data science process is shown in [figure 3.1](#ch03fig01).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管机器学习主要与数据科学过程中的数据建模步骤相关联，但它几乎可以在每个步骤中使用。为了回顾前几章的内容，数据科学过程在[图3.1](#ch03fig01)中展示。
- en: Figure 3.1\. The data science process
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1. 数据科学过程
- en: '![](Images/03fig01.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/03fig01.jpg)'
- en: The data modeling phase can’t start until you have qualitative raw data you
    can understand. But prior to that, the *data preparation* phase can benefit from
    the use of machine learning. An example would be cleansing a list of text strings;
    machine learning can group similar strings together so it becomes easier to correct
    spelling errors.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据建模阶段只能在您拥有可以理解的定性原始数据后开始。但在那之前，*数据准备*阶段可以从机器学习的使用中受益。一个例子就是清洗文本字符串列表；机器学习可以将相似字符串分组在一起，使其更容易纠正拼写错误。
- en: Machine learning is also useful when *exploring data*. Algorithms can root out
    underlying patterns in the data where they’d be difficult to find with only charts.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习在*探索数据*时也非常有用。算法可以在数据中找出难以仅通过图表发现的潜在模式。
- en: Given that machine learning is useful throughout the data science process, it
    shouldn’t come as a surprise that a considerable number of Python libraries were
    developed to make your life a bit easier.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习在整个数据科学过程中都很有用，因此开发了大量Python库来使您的生活更加轻松，这并不令人惊讶。
- en: 3.1.3\. Python tools used in machine learning
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3\. 机器学习中使用的 Python 工具
- en: Python has an overwhelming number of packages that can be used in a machine
    learning setting. The Python machine learning ecosystem can be divided into three
    main types of packages, as shown in [figure 3.2](#ch03fig02).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Python 有大量的包可以在机器学习环境中使用。Python 机器学习生态系统可以分为三种主要类型的包，如图 3.2 所示。
- en: Figure 3.2\. Overview of Python packages used during the machine-learning phase
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.2\. 机器学习阶段使用的 Python 包概述
- en: '![](Images/03fig02_alt.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/03fig02_alt.jpg)'
- en: The first type of package shown in [figure 3.2](#ch03fig02) is mainly used in
    simple tasks and when data fits into memory. The second type is used to optimize
    your code when you’ve finished prototyping and run into speed or memory issues.
    The third type is specific to using Python with big data technologies.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.2](#ch03fig02) 中展示的第一种类型的包主要用于简单任务和数据适合内存时。第二种类型用于在原型设计完成后优化代码，遇到速度或内存问题时。第三种类型专门用于与大数据技术一起使用
    Python。'
- en: Packages for working with data in memory
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 用于在内存中处理数据的包
- en: 'When prototyping, the following packages can get you started by providing advanced
    functionalities with a few lines of code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在原型设计时，以下包可以通过几行代码提供高级功能，帮助你开始：
- en: '*SciPy* is a library that integrates fundamental packages often used in scientific
    computing such as NumPy, matplotlib, Pandas, and SymPy.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SciPy* 是一个库，它集成了在科学计算中经常使用的许多基本包，如 NumPy、matplotlib、Pandas 和 SymPy。'
- en: '*NumPy* gives you access to powerful array functions and linear algebra functions.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*NumPy* 给你提供了强大的数组函数和线性代数函数。'
- en: '*Matplotlib* is a popular 2D plotting package with some 3D functionality.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Matplotlib* 是一个流行的 2D 绘图包，具有一些 3D 功能。'
- en: '*Pandas* is a high-performance, but easy-to-use, data-wrangling package. It
    introduces dataframes to Python, a type of in-memory data table. It’s a concept
    that should sound familiar to regular users of R.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Pandas* 是一个高性能但易于使用的数据处理包。它将数据框引入 Python，这是一种内存中的数据表类型。这个概念应该对 R 的常规用户来说很熟悉。'
- en: '*SymPy* is a package used for symbolic mathematics and computer algebra.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SymPy* 是一个用于符号数学和计算机代数的包。'
- en: '*StatsModels* is a package for statistical methods and algorithms.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*StatsModels* 是一个用于统计方法和算法的包。'
- en: '*Scikit-learn* is a library filled with machine learning algorithms.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Scikit-learn* 是一个充满机器学习算法的库。'
- en: '*RPy2* allows you to call R functions from within Python. R is a popular open
    source statistics program.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*RPy2* 允许你在 Python 中调用 R 函数。R 是一个流行的开源统计程序。'
- en: '*NLTK* (Natural Language Toolkit) is a Python toolkit with a focus on text
    analytics.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*NLTK*（自然语言工具包）是一个专注于文本分析的 Python 工具包。'
- en: These libraries are good to get started with, but once you make the decision
    to run a certain Python program at frequent intervals, performance comes into
    play.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些库是入门的好选择，但一旦你决定频繁运行某个 Python 程序，性能就会变得重要。
- en: Optimizing operations
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优化操作
- en: Once your application moves into production, the libraries listed here can help
    you deliver the speed you need. Sometimes this involves connecting to big data
    infrastructures such as Hadoop and Spark.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的应用程序进入生产阶段，这里列出的库可以帮助你提供所需的性能。有时这涉及到连接到 Hadoop 和 Spark 等大数据基础设施。
- en: '***Numba and NumbaPro*** —These use just-in-time compilation to speed up applications
    written directly in Python and a few annotations. NumbaPro also allows you to
    use the power of your graphics processor unit (GPU).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Numba 和 NumbaPro*** — 这些使用即时编译来加速直接用 Python 编写的应用程序和一些注释。NumbaPro 还允许你使用你的图形处理器单元（GPU）的强大功能。'
- en: '***PyCUDA*** —This allows you to write code that will be executed on the GPU
    instead of your CPU and is therefore ideal for calculation-heavy applications.
    It works best with problems that lend themselves to being parallelized and need
    little input compared to the number of required computing cycles. An example is
    studying the robustness of your predictions by calculating thousands of different
    outcomes based on a single start state.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***PyCUDA*** — 这允许你编写将在 GPU 上而不是 CPU 上执行的代码，因此非常适合计算密集型应用程序。它最适合那些适合并行化并且与所需计算周期相比输入很少的问题。一个例子是通过对单个起始状态进行数千种不同的结果计算来研究你预测的鲁棒性。'
- en: '***Cython, or C for Python*** —This brings the C programming language to Python.
    C is a lower-level language, so the code is closer to what the computer eventually
    uses (bytecode). The closer code is to bits and bytes, the faster it executes.
    A computer is also faster when it knows the type of a variable (called *static
    typing*). Python wasn’t designed to do this, and Cython helps you to overcome
    this shortfall.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Cython，或 C 语言用于 Python*** — 这将 C 编程语言引入 Python。C 是一种低级语言，因此代码更接近计算机最终使用的（字节码）。代码与位和字节越接近，执行速度越快。当计算机知道变量的类型时（称为
    *静态类型*），计算机也会更快。Python 并未设计用于此，Cython 帮助你克服这一不足。'
- en: '***Blaze*** —Blaze gives you data structures that can be bigger than your computer’s
    main memory, enabling you to work with large data sets.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Blaze*** — Blaze 提供了可以大于你计算机主内存的数据结构，使你能够处理大型数据集。'
- en: '***Dispy and IPCluster*** —These packages allow you to write code that can
    be distributed over a cluster of computers.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Dispy 和 IPCluster*** — 这些包允许你编写可以在计算机集群上分布的代码。'
- en: '***PP*** —Python is executed as a single process by default. With the help
    of PP you can parallelize computations on a single machine or over clusters.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***PP*** — Python 默认以单个进程执行。借助 PP，你可以在单个机器或集群上并行化计算。'
- en: '***Pydoop and Hadoopy*** —These connect Python to Hadoop, a common big data
    framework.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Pydoop 和 Hadoopy*** — 这些将 Python 连接到 Hadoop，一个常见的大数据框架。'
- en: '***PySpark*** —This connects Python and Spark, an in-memory big data framework.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***PySpark*** — 这将 Python 和 Spark 连接起来，Spark 是一个内存中的大数据框架。'
- en: Now that you’ve seen an overview of the available libraries, let’s look at the
    modeling process itself.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了可用的库的概述，让我们来看看模型过程本身。
- en: 3.2\. The modeling process
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 模型过程
- en: 'The modeling phase consists of four steps:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 模型阶段包括四个步骤：
- en: '**1**.  Feature engineering and model selection'
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**. 特征工程和模型选择'
- en: '**2**.  Training the model'
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**. 训练模型'
- en: '**3**.  Model validation and selection'
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**. 模型验证和选择'
- en: '**4**.  Applying the trained model to unseen data'
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**4**. 将训练好的模型应用于未见数据'
- en: Before you find a good model, you’ll probably iterate among the first three
    steps.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在你找到一个好的模型之前，你可能会在前三个步骤之间迭代。
- en: The last step isn’t always present because sometimes the goal isn’t prediction
    but explanation (root cause analysis). For instance, you might want to find out
    the causes of species’ extinctions but not necessarily predict which one is next
    in line to leave our planet.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步并不总是存在，因为有时目标不是预测而是解释（根本原因分析）。例如，你可能想找出物种灭绝的原因，但并不一定预测下一个即将离开我们星球的是哪一个。
- en: It’s possible to *chain* or *combine* multiple techniques. When you chain multiple
    models, the output of the first model becomes an input for the second model. When
    you combine multiple models, you train them independently and combine their results.
    This last technique is also known as *ensemble learning*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 可以 *链式* 或 *组合* 多种技术。当你链式连接多个模型时，第一个模型的输出成为第二个模型的输入。当你组合多个模型时，你独立训练它们并组合它们的结果。这种最后的技术也被称为
    *集成学习*。
- en: A model consists of constructs of information called *features* or *predictors*
    and a *target* or *response variable*. Your model’s goal is to predict the target
    variable, for example, tomorrow’s high temperature. The variables that help you
    do this and are (usually) known to you are the features or predictor variables
    such as today’s temperature, cloud movements, current wind speed, and so on. The
    best models are those that accurately represent reality, preferably while staying
    concise and interpretable. To achieve this, feature engineering is the most important
    and arguably most interesting part of modeling. For example, an important feature
    in a model that tried to explain the extinction of large land animals in the last
    60,000 years in Australia turned out to be the population number and spread of
    humans.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型由称为 *特征* 或 *预测器* 的信息构造和 *目标* 或 *响应变量* 组成。你的模型的目标是预测目标变量，例如，明天的最高气温。帮助你做到这一点并且（通常）为你所知的变量是特征或预测变量，例如今天的温度、云的移动、当前风速等。最好的模型是那些能够准确反映现实，同时保持简洁和可解释性的模型。为了实现这一点，特征工程是建模过程中最重要且最具趣味性的部分。例如，一个试图解释过去
    60,000 年澳大利亚大型陆地动物灭绝的重要特征最终被发现是人口数量和人类的扩散。
- en: 3.2.1\. Engineering features and selecting a model
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 特征工程和模型选择
- en: With engineering features, you must come up with and create possible predictors
    for the model. This is one of the most important steps in the process because
    a model recombines these features to achieve its predictions. Often you may need
    to consult an expert or the appropriate literature to come up with meaningful
    features.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在工程特性方面，你必须为模型提出并创建可能的预测因子。这是过程中的一个最重要的步骤，因为模型通过重新组合这些特性来实现其预测。通常，你可能需要咨询专家或查阅适当的文献来提出有意义的特性。
- en: 'Certain features are the variables you get from a data set, as is the case
    with the provided data sets in our exercises and in most school exercises. In
    practice you’ll need to find the features yourself, which may be scattered among
    different data sets. In several projects we had to bring together more than 20
    different data sources before we had the raw data we required. Often you’ll need
    to apply a transformation to an input before it becomes a good predictor or to
    combine multiple inputs. An example of combining multiple inputs would be *interaction
    variables*: the impact of either single variable is low, but if both are present
    their impact becomes immense. This is especially true in chemical and medical
    environments. For example, although vinegar and bleach are fairly harmless common
    household products by themselves, mixing them results in poisonous chlorine gas,
    a gas that killed thousands during World War I.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 某些特性是从数据集中获得的变量，正如我们在练习中提供的数据集和大多数学校练习中的情况一样。在实践中，你需要自己找到这些特性，它们可能分散在不同的数据集中。在几个项目中，我们不得不在获得所需原始数据之前汇集20多个不同的数据源。通常，你需要在输入成为良好的预测因子之前对其进行转换，或者将多个输入组合起来。多个输入组合的一个例子是*交互变量*：单个变量的影响较低，但如果两者都存在，它们的影响就会变得巨大。这在化学和医学环境中尤其如此。例如，虽然醋和漂白剂本身是相对无害的常见家用产品，但混合它们会产生有毒的氯气，这种气体在第一次世界大战中杀死了成千上万的人。
- en: In medicine, clinical pharmacy is a discipline dedicated to researching the
    effect of the interaction of medicines. This is an important job, and it doesn’t
    even have to involve two medicines to produce potentially dangerous results. For
    example, mixing an antifungal medicine such as Sporanox with grapefruit has serious
    side effects.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在医学领域，临床药学是一门致力于研究药物相互作用效果的学科。这是一项重要的工作，甚至不需要两种药物相互作用就能产生潜在的危险结果。例如，将抗真菌药物如斯波拉诺克斯与葡萄柚混合会产生严重的副作用。
- en: 'Sometimes you have to use modeling techniques to derive features: the output
    of a model becomes part of another model. This isn’t uncommon, especially in text
    mining. Documents can first be annotated to classify the content into categories,
    or you can count the number of geographic places or persons in the text. This
    counting is often more difficult than it sounds; models are first applied to recognize
    certain words as a person or a place. All this new information is then poured
    into the model you want to build. One of the biggest mistakes in model construction
    is the *availability bias*: your features are only the ones that you could easily
    get your hands on and your model consequently represents this one-sided “truth.”
    Models suffering from availability bias often fail when they’re validated because
    it becomes clear that they’re not a valid representation of the truth.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有时你必须使用建模技术来推导特性：一个模型的输出成为另一个模型的一部分。这在文本挖掘中并不罕见。文档首先可以被标注以将内容分类到不同的类别中，或者你可以计算文本中的地理地点或人物的数量。这种计数往往比听起来更困难；模型首先被应用于识别某些单词作为人物或地点。然后，所有这些新信息都被倒入你想要构建的模型中。模型构建中的一个最大错误是*可用性偏差*：你的特性只是你能够轻易获得的那些，因此你的模型相应地代表了一种单方面的“真相”。受到可用性偏差影响的模型在验证时往往失败，因为很明显，它们并不是对真相的有效表示。
- en: 'In World War II, after bombing runs on German territory, many of the English
    planes came back with bullet holes in the wings, around the nose, and near the
    tail of the plane. Almost none of them had bullet holes in the cockpit, tail rudder,
    or engine block, so engineering decided extra armor plating should be added to
    the wings. This looked like a sound idea until a mathematician by the name of
    Abraham Wald explained the obviousness of their mistake: they only took into account
    the planes that returned. The bullet holes on the wings were actually the least
    of their concern, because at least a plane with this kind of damage could make
    it back home for repairs. Plane fortification was hence increased on the spots
    that were unscathed on returning planes. The initial reasoning suffered from availability
    bias: the engineers ignored an important part of the data because it was harder
    to obtain. In this case they were lucky, because the reasoning could be reversed
    to get the intended result without getting the data from the crashed planes.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二次世界大战期间，在对德国领土进行轰炸行动后，许多英国飞机在机翼、机头和机尾附近都出现了弹孔，但几乎没有飞机的驾驶舱、尾翼或发动机块上有弹孔。因此，工程师决定在机翼上增加额外的装甲板。这个想法看起来很合理，直到一位名叫亚伯拉罕·瓦尔德的数学家解释了他们错误的明显性：他们只考虑了返回的飞机。机翼上的弹孔实际上是他们最不需要担心的问题，因为至少带有这种损伤的飞机能够返回家中进行维修。因此，飞机的加固被增加到了返回飞机上未受损的部分。最初的推理受到了可用性偏差的影响：工程师忽略了一个重要的数据部分，因为它更难获得。在这种情况下，他们很幸运，因为推理可以被反转，以得到预期的结果，而无需从坠毁的飞机中获得数据。
- en: When the initial features are created, a model can be *trained* to the data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当初始特征被创建时，模型可以被训练到数据上。
- en: 3.2.2\. Training your model
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 训练你的模型
- en: With the right predictors in place and a modeling technique in mind, you can
    progress to model training. In this phase you present to your model data from
    which it can learn.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定了合适的预测因子并考虑了建模技术后，你可以进入模型训练阶段。在这个阶段，你向模型展示数据，使其能够从中学习。
- en: The most common modeling techniques have industry-ready implementations in almost
    every programming language, including Python. These enable you to train your models
    by executing a few lines of code. For more state-of-the art data science techniques,
    you’ll probably end up doing heavy mathematical calculations and implementing
    them with modern computer science techniques.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的建模技术几乎在每种编程语言中都有现成的实现，包括Python。这些技术使你能够通过执行几行代码来训练你的模型。对于更先进的数据科学技术，你可能需要进行大量的数学计算，并使用现代计算机科学技术来实现。
- en: 'Once a model is trained, it’s time to test whether it can be extrapolated to
    reality: model validation.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被训练，就需要测试它是否可以推广到现实中：模型验证。
- en: 3.2.3\. Validating a model
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3\. 验证模型
- en: 'Data science has many modeling techniques, and the question is which one is
    the right one to use. A good model has two properties: it has good predictive
    power and it generalizes well to data it hasn’t seen. To achieve this you define
    an error measure (how wrong the model is) and a validation strategy.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学有许多建模技术，问题是哪一种才是正确的选择。一个好的模型有两个特性：它具有良好的预测能力和良好的泛化能力，能够很好地推广到它尚未见过的数据。为了实现这一点，你需要定义一个误差度量（模型错误程度）和一个验证策略。
- en: 'Two common *error measures* in machine learning are the *classification error
    rate* for classification problems and the *mean squared error* for regression
    problems. The classification error rate is the percentage of observations in the
    test data set that your model mislabeled; lower is better. The mean squared error
    measures how big the average error of your prediction is. Squaring the average
    error has two consequences: you can’t cancel out a wrong prediction in one direction
    with a faulty prediction in the other direction. For example, overestimating future
    turnover for next month by 5,000 doesn’t cancel out underestimating it by 5,000
    for the following month. As a second consequence of squaring, bigger errors get
    even more weight than they otherwise would. Small errors remain small or can even
    shrink (if <1), whereas big errors are enlarged and will definitely draw your
    attention.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中两种常见的*误差度量*是分类问题的*分类错误率*和回归问题的*均方误差*。分类错误率是测试数据集中模型错误标记的观测值的百分比；越低越好。均方误差衡量预测的平均误差有多大。平均误差的平方有两个后果：你不能用一个方向的错误预测抵消另一个方向的错误预测。例如，下个月过度估计周转额5,000并不抵消下下个月低估5,000。作为平方的第二个后果，更大的误差比它们原本的权重还要大。小误差保持不变，甚至可以缩小（如果<1），而大误差会扩大，并肯定会引起你的注意。
- en: 'Many *validation strategies* exist, including the following common ones:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多*验证策略*，包括以下常见的几种：
- en: '*Dividing your data into a training set with X% of the observations and keeping
    the rest as a holdout data set* (a data set that’s never used for model creation)—This
    is the most common technique.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将数据分为包含X%观测值的训练集，并将剩余的作为保留数据集*（一个永远不会用于模型创建的数据集）——这是最常用的技术。'
- en: '***K-folds cross validation*** —This strategy divides the data set into k parts
    and uses each part one time as a test data set while using the others as a training
    data set. This has the advantage that you use all the data available in the data
    set.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***K折交叉验证*** —这种策略将数据集分为k部分，并使用每一部分一次作为测试数据集，而将其他部分作为训练数据集。这种方法的优点是，你使用了数据集中所有可用的数据。'
- en: '***Leave-1 out*** —This approach is the same as k-folds but with k=1\. You
    always leave one observation out and train on the rest of the data. This is used
    only on small data sets, so it’s more valuable to people evaluating laboratory
    experiments than to big data analysts.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***留一法*** —这种方法与k折交叉验证相同，但k=1。你总是留出一个观测值，并在其余数据上训练。这种方法仅用于小型数据集，因此对评估实验室实验的人更有价值，而不是对大数据分析师。'
- en: 'Another popular term in machine learning is *regularization*. When applying
    regularization, you incur a penalty for every extra variable used to construct
    the model. With *L1 regularization* you ask for a model with as few predictors
    as possible. This is important for the model’s robustness: simple solutions tend
    to hold true in more situations. *L2 regularization* aims to keep the variance
    between the coefficients of the predictors as small as possible. Overlapping variance
    between predictors makes it hard to make out the actual impact of each predictor.
    Keeping their variance from overlapping will increase interpretability. To keep
    it simple: regularization is mainly used to stop a model from using too many features
    and thus prevent over-fitting.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中另一个流行的术语是*正则化*。在应用正则化时，你为构建模型使用的每个额外变量都会受到惩罚。使用*L1正则化*时，你要求模型尽可能少地使用预测变量。这对于模型的鲁棒性很重要：简单的解决方案往往在更多情况下是正确的。*L2正则化*旨在使预测变量系数之间的方差尽可能小。预测变量之间的重叠方差使得难以识别每个预测变量的实际影响。保持它们的方差不重叠将提高可解释性。简单来说：正则化主要用于防止模型使用过多的特征，从而防止过拟合。
- en: 'Validation is extremely important because it determines whether your model
    works in real-life conditions. To put it bluntly, it’s whether your model is worth
    a dime. Even so, every now and then people send in papers to respected scientific
    journals (and sometimes even succeed at publishing them) with faulty validation.
    The result of this is they get rejected or need to retract the paper because everything
    is wrong. Situations like this are bad for your mental health so always keep this
    in mind: test your models on data the constructed model has never seen and make
    sure this data is a true representation of what it would encounter when applied
    on fresh observations by other people. For classification models, instruments
    like the confusion matrix (introduced in [chapter 2](kindle_split_010.xhtml#ch02)
    but thoroughly explained later in this chapter) are golden; embrace them.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 验证非常重要，因为它决定了你的模型是否能在实际生活中工作。直白地说，就是你的模型是否值一美元。即便如此，人们时不时地会向受尊敬的科学期刊提交论文（有时甚至成功发表），但这些论文的验证存在缺陷。结果是，这些论文被拒绝或需要撤回，因为一切都错了。这种情况对你的心理健康有害，所以请始终记住：在从未见过的数据上测试你的模型，并确保这些数据是它在应用于其他人新鲜观察时可能遇到的真实表现的准确代表。对于分类模型，混淆矩阵（在第2章中介绍，但将在本章后面详细解释）是黄金工具；拥抱它们。
- en: Once you’ve constructed a good model, you can (optionally) use it to predict
    the future.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你构建了一个好的模型，你可以（可选地）使用它来预测未来。
- en: 3.2.4\. Predicting new observations
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4\. 预测新的观察结果
- en: If you’ve implemented the first three steps successfully, you now have a performant
    model that generalizes to unseen data. The process of applying your model to new
    data is called model scoring. In fact, model scoring is something you implicitly
    did during validation, only now you don’t know the correct outcome. By now you
    should trust your model enough to use it for real.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经成功实施了前三个步骤，你现在有一个性能良好的模型，它可以泛化到未见过的数据。将你的模型应用于新数据的过程称为模型评分。实际上，模型评分是你在验证过程中隐式完成的，但现在你不知道正确的结果。到如今，你应该足够信任你的模型，可以使用它进行实际应用。
- en: Model scoring involves two steps. First, you prepare a data set that has features
    exactly as defined by your model. This boils down to repeating the data preparation
    you did in step one of the modeling process but for a new data set. Then you apply
    the model on this new data set, and this results in a prediction.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 模型评分涉及两个步骤。首先，你准备一个具有与你的模型定义完全一致的特征的数据集。这相当于重复你在建模过程的第一个步骤中进行的数据处理，但针对一个新的数据集。然后，你将模型应用于这个新的数据集，这会产生一个预测结果。
- en: 'Now let’s look at the different types of machine learning techniques: a different
    problem requires a different approach.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看不同的机器学习技术：不同的问题需要不同的方法。
- en: 3.3\. Types of machine learning
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 机器学习的类型
- en: Broadly speaking, we can divide the different approaches to machine learning
    by the amount of human effort that’s required to coordinate them and how they
    use *labeled data—*data with a category or a real-value number assigned to it
    that represents the outcome of previous observations.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 从广义上讲，我们可以根据协调它们所需的人类努力以及它们如何使用*标记数据*（即分配有类别或代表先前观察结果的真实数值的数据）来划分不同的机器学习方法。
- en: '*Supervised learning* techniques attempt to discern results and learn by trying
    to find patterns in a labeled data set. Human interaction is required to label
    the data.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*监督学习*技术试图通过试图在标记数据集中找到模式来辨别结果并学习。需要人类交互来标记数据。'
- en: '*Unsupervised learning* techniques don’t rely on labeled data and attempt to
    find patterns in a data set without human interaction.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*无监督学习*技术不依赖于标记数据，并试图在数据集中找到模式，而不需要人类交互。'
- en: '*Semi-supervised learning* techniques need labeled data, and therefore human
    interaction, to find patterns in the data set, but they can still progress toward
    a result and learn even if passed unlabeled data as well.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*半监督学习*技术需要标记数据，因此需要人类交互来在数据集中找到模式，但即使传递了未标记的数据，它们仍然可以朝着结果前进并学习。'
- en: In this section, we’ll look at all three approaches, see what tasks each is
    more appropriate for, and use one or two of the Python libraries mentioned earlier
    to give you a feel for the code and solve a task. In each of these examples, we’ll
    work with a downloadable data set that has already been cleaned, so we’ll skip
    straight to the data modeling step of the data science process, as discussed earlier
    in this chapter.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨所有三种方法，看看每种方法更适合哪些任务，并使用前面提到的几个Python库中的一到两个来让你对代码有一个直观感受，并解决一个任务。在这些示例中，我们将使用一个已经清洗过的可下载数据集，因此我们将直接跳到数据科学流程中的数据建模步骤，正如本章前面所讨论的。
- en: 3.3.1\. Supervised learning
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 监督学习
- en: As stated before, supervised learning is a learning technique that can only
    be applied on labeled data. An example implementation of this would be discerning
    digits from images. Let’s dive into a case study on number recognition.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，监督学习是一种只能应用于标记数据的机器学习技术。一个例子就是从图像中识别数字。让我们深入研究一个关于数字识别的案例研究。
- en: 'Case study: discerning digits from images'
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 案例研究：从图像中识别数字
- en: One of the many common approaches on the web to stopping computers from hacking
    into user accounts is the Captcha check—a picture of text and numbers that the
    human user must decipher and enter into a form field before sending the form back
    to the web server. Something like [figure 3.3](#ch03fig03) should look familiar.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络上防止计算机黑客攻击用户账户的许多常见方法之一是验证码检查——用户必须解码并输入到表单字段中，然后将表单发送回网络服务器的文本和数字图片。类似于[图3.3](#ch03fig03)的图片应该很熟悉。
- en: Figure 3.3\. A simple Captcha control can be used to prevent automated spam
    being sent through an online web form.
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.3\. 简单的验证码控制可以用来防止通过在线表单发送自动垃圾邮件。
- en: '![](Images/03fig03.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/03fig03.jpg)'
- en: With the help of the *Naïve Bayes classifier*, a simple yet powerful algorithm
    to categorize observations into classes that’s explained in more detail in the
    sidebar, you can recognize digits from textual images. These images aren’t unlike
    the Captcha checks many websites have in place to make sure you’re not a computer
    trying to hack into the user accounts. Let’s see how hard it is to let a computer
    recognize images of numbers.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在侧边栏中详细解释的简单但强大的算法*朴素贝叶斯分类器*的帮助下，你可以从文本图像中识别数字。这些图像与许多网站为了确保你不是试图黑客攻击用户账户的计算机而设置的验证码检查非常相似。让我们看看让计算机识别数字图片有多难。
- en: Our research goal is to let a computer recognize images of numbers (step one
    of the data science process).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究目标是让计算机能够识别数字图片（数据科学流程的第一步）。
- en: The data we’ll be working on is the MNIST data set, which is often used in the
    data science literature for teaching and benchmarking.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要处理的数据是MNIST数据集，它常被用于数据科学文献中的教学和基准测试。
- en: '|  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Introducing Naïve Bayes classifiers in the context of a spam filter**'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**在垃圾邮件过滤器中介绍朴素贝叶斯分类器**'
- en: Not every email you receive has honest intentions. Your inbox can contain unsolicited
    commercial or bulk emails, a.k.a. spam. Not only is spam annoying, it’s often
    used in scams and as a carrier for viruses. Kaspersky^([[3](#ch03fn03)]) estimates
    that more than 60% of the emails in the world are spam. To protect users from
    spam, most email clients run a program in the background that classifies emails
    as either spam or safe.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你收到的每一封电子邮件并不都有诚实的目的。你的收件箱可能包含未经请求的商业邮件或批量邮件，也就是垃圾邮件。垃圾邮件不仅令人烦恼，它还常被用于诈骗和病毒传播。卡巴斯基估计，世界上超过60%的电子邮件都是垃圾邮件。为了保护用户免受垃圾邮件的侵扰，大多数电子邮件客户端都会在后台运行一个程序，将电子邮件分类为垃圾邮件或安全邮件。
- en: ³
  id: totrans-132
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Kaspersky 2014 Quarterly Spam Statistics Report, [http://usa.kaspersky.com/internet-security-center/threats/spam-statistics-report-q1-2014#.VVym9blViko](http://usa.kaspersky.com/internet-security-center/threats/spam-statistics-report-q1-2014#.VVym9blViko).
  id: totrans-134
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 卡巴斯基2014年季度垃圾邮件统计报告，[http://usa.kaspersky.com/internet-security-center/threats/spam-statistics-report-q1-2014#.VVym9blViko](http://usa.kaspersky.com/internet-security-center/threats/spam-statistics-report-q1-2014#.VVym9blViko)。
- en: 'A popular technique in spam filtering is employing a classifier that uses the
    words inside the mail as predictors. It outputs the chance that a specific email
    is spam given the words it’s composed of (in mathematical terms, P(spam | words)
    ). To reach this conclusion it uses three calculations:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在垃圾邮件过滤中，一个流行的技术是使用邮件内部的单词作为预测器的分类器。它输出特定电子邮件由其组成的单词是垃圾邮件的概率（用数学术语，P(垃圾邮件 |
    单词)）。为了得出这个结论，它使用了三个计算：
- en: P(spam)—The average rate of spam without knowledge of the words. According to
    Kaspersky, an email is spam 60% of the time.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(垃圾邮件)—在不知道单词的情况下垃圾邮件的平均比率。根据卡巴斯基的数据，一封邮件有60%的概率是垃圾邮件。
- en: P(words)—How often this word combination is used regardless of spam.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(单词)—无论是否为垃圾邮件，这个单词组合的使用频率。
- en: P(words | spam)—How often these words are seen when a training mail was labeled
    as spam.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(单词 | 垃圾邮件)—在训练邮件被标记为垃圾邮件时，这些单词出现的频率。
- en: 'To determine the chance that a new email is spam, you’d use the following formula:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定一封新邮件是垃圾邮件的概率，你会使用以下公式：
- en: P(spam|words) = P(spam)P(words|spam) / P(words)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: P(垃圾邮件|单词) = P(垃圾邮件)P(单词|垃圾邮件) / P(单词)
- en: This is an application of the rule P(B|A) = P(B) P(A|B) / P(A), which is known
    as Bayes’s rule and which lends its name to this classifier. The “naïve” part
    comes from the classifier’s assumption that the presence of one feature doesn’t
    tell you anything about another feature (feature independence, also called absence
    of multicollinearity). In reality, features are often related, especially in text.
    For example the word “buy” will often be followed by “now.” Despite the unrealistic
    assumption, the naïve classifier works surprisingly well in practice.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对规则 P(B|A) = P(B) P(A|B) / P(A) 的应用，该规则被称为贝叶斯定理，并以该分类器的名字命名。其中“朴素”部分来自分类器假设一个特征的存在不会告诉你关于另一个特征的信息（特征独立性，也称为多重共线性不存在）。在现实中，特征往往是相关的，尤其是在文本中。例如，“购买”这个词通常会被“现在”跟随。尽管这个假设不切实际，但朴素分类器在实践中表现得相当出色。
- en: '|  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: With the bit of theory in the sidebar, you’re ready to perform the modeling
    itself. Make sure to run all the upcoming code in the same scope because each
    piece requires the one before it. An IPython file can be downloaded for this chapter
    from the Manning download page of this book.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在侧边栏中的理论基础上，你已经准备好进行建模本身。确保运行所有即将到来的代码都在相同的范围内，因为每一部分都需要前一部分。可以从这本书的Manning下载页面下载本章的IPython文件。
- en: 'The MNIST images can be found in the data sets package of Scikit-learn and
    are already normalized for you (all scaled to the same size: 64x64 pixels), so
    we won’t need much data preparation (step three of the data science process).
    But let’s first fetch our data as step two of the data science process, with the
    following listing.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST图像可以在Scikit-learn的数据集包中找到，并且已经为你进行了归一化（所有都缩放到相同的大小：64x64像素），因此我们不需要太多的数据准备（数据科学流程的第三步）。但让我们首先按照数据科学流程的第二步，使用以下列表获取我们的数据。
- en: 'Listing 3.1\. Step 2 of the data science process: fetching the digital image
    data'
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.1\. 数据科学流程的第二步：获取数字图像数据
- en: '![](Images/067fig01_alt.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/067fig01_alt.jpg)'
- en: 'Working with images isn’t much different from working with other data sets.
    In the case of a gray image, you put a value in every matrix entry that depicts
    the gray value to be shown. The following code demonstrates this process and is
    step four of the data science process: data exploration.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 与图像一起工作与其他数据集一起工作没有太大区别。在灰度图像的情况下，你将每个矩阵条目中的值设置为要显示的灰度值。以下代码演示了此过程，并且是数据科学流程的第四步：数据探索。
- en: 'Listing 3.2\. Step 4 of the data science process: using Scikit-learn'
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.2\. 数据科学流程的第四步：使用Scikit-learn
- en: '![](Images/068fig01_alt.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/068fig01_alt.jpg)'
- en: '[Figure 3.4](#ch03fig04) shows how a blurry “0” image translates into a data
    matrix.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.4](#ch03fig04) 展示了模糊的“0”图像如何转换为数据矩阵。'
- en: Figure 3.4\. Blurry grayscale representation of the number 0 with its corresponding
    matrix. The higher the number, the closer it is to white; the lower the number,
    the closer it is to black.
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.4\. 数字0的模糊灰度表示及其对应的矩阵。数字越高，越接近白色；数字越低，越接近黑色。
- en: '![](Images/03fig04.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/03fig04.jpg)'
- en: '[Figure 3.4](#ch03fig04) shows the actual code output, but perhaps [figure
    3.5](#ch03fig05) can clarify this slightly, because it shows how each element
    in the vector is a piece of the image.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.4](#ch03fig04) 展示了实际的代码输出，但也许 [图3.5](#ch03fig05) 可以稍微澄清这一点，因为它显示了向量中的每个元素是如何成为图像的一部分。'
- en: Figure 3.5\. We’ll turn an image into something usable by the Naïve Bayes classifier
    by getting the grayscale value for each of its pixels (shown on the right) and
    putting those values in a list.
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.5\. 我们将通过获取图像中每个像素的灰度值（如图右侧所示）并将这些值放入列表中，将图像转换为朴素贝叶斯分类器可用的格式。
- en: '![](Images/03fig05.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/03fig05.jpg)'
- en: 'Easy so far, isn’t it? There is, naturally, a little more work to do. The Naïve
    Bayes classifier is expecting a list of values, but `pl.matshow()` returns a two-dimensional
    array (a matrix) reflecting the shape of the image. To flatten it into a list,
    we need to call `reshape()` on `digits.images`. The net result will be a one-dimensional
    array that looks something like this:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这很简单，不是吗？当然，还有一些额外的工作要做。朴素贝叶斯分类器期望一个值列表，但`pl.matshow()`返回一个反映图像形状的二维数组（一个矩阵）。要将它展平成一个列表，我们需要在`digits.images`上调用`reshape()`。最终结果将是一个类似这样的单维数组：
- en: '[PRE0]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The previous code snippet shows the matrix of [figure 3.5](#ch03fig05) flattened
    (the number of dimensions was reduced from two to one) to a Python list. From
    this point on, it’s a standard classification problem, which brings us to step
    five of the data science process: model building.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码片段显示了[图3.5](#ch03fig05)中的矩阵展平（维度从二维减少到一维）到一个Python列表。从这一点开始，它就是一个标准的分类问题，这把我们带到了数据科学过程的第五步：模型构建。
- en: Now that we have a way to pass the contents of an image into the classifier,
    we need to pass it a training data set so it can start learning how to predict
    the numbers in the images. We mentioned earlier that Scikit-learn contains a subset
    of the MNIST database (1,800 images), so we’ll use that. Each image is also labeled
    with the number it actually shows. This will build a probabilistic model in memory
    of the most likely digit shown in an image given its grayscale values.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了将图像内容传递给分类器的方法，我们需要传递一个训练数据集，这样它就可以开始学习如何预测图像中的数字。我们之前提到Scikit-learn包含MNIST数据库的子集（1,800个图像），因此我们将使用它。每个图像也标有它实际显示的数字。这将构建一个基于图像的灰度值的概率模型，存储在内存中最可能显示的数字。
- en: Once the program has gone through the training set and built the model, we can
    then pass it the test set of data to see how well it has learned to interpret
    the images using the model.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦程序通过了训练集并构建了模型，我们就可以传递测试数据集给它，看看它是否已经学会了如何使用模型来解释图像。
- en: The following listing shows how to implement these steps in code.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表展示了如何在代码中实现这些步骤。
- en: Listing 3.3\. Image data classification problem on images of digits
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.3\. 数字图像数据分类问题
- en: '![](Images/069fig01_alt.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/069fig01_alt.jpg)'
- en: The end result of this code is called a *confusion matrix*, such as the one
    shown in [figure 3.6](#ch03fig06). Returned as a two-dimensional array, it shows
    how often the number predicted was the correct number on the main diagonal and
    also in the matrix entry (i,j), where j was predicted but the image showed i.
    Looking at [figure 3.6](#ch03fig06) we can see that the model predicted the number
    2 correctly 17 times (at coordinates 3,3), but also that the model predicted the
    number 8 15 times when it was actually the number 2 in the image (at 9,3).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码的最终结果被称为*混淆矩阵*，例如[图3.6](#ch03fig06)中所示。作为一个二维数组返回，它显示了预测的数字在主对角线以及矩阵中的(i,j)位置（j被预测，但图像显示的是i）出现的频率。查看[图3.6](#ch03fig06)，我们可以看到模型正确预测数字2共17次（坐标3,3），但模型也错误地预测了数字8共15次，而图像中实际显示的是数字2（坐标9,3）。
- en: Figure 3.6\. Confusion matrix produced by predicting what number is depicted
    by a blurry image
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.6\. 由预测模糊图像所生成的混淆矩阵
- en: '![](Images/03fig06.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/03fig06.jpg)'
- en: '|  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Confusion matrices**'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**混淆矩阵**'
- en: 'A confusion matrix is a matrix showing how wrongly (or correctly) a model predicted,
    how much it got “confused.” In its simplest form it will be a 2x2 table for models
    that try to classify observations as being A or B. Let’s say we have a classification
    model that predicts whether somebody will buy our newest product: deep-fried cherry
    pudding. We can either predict: “Yes, this person will buy” or “No, this customer
    won’t buy.” Once we make our prediction for 100 people we can compare this to
    their actual behavior, showing us how many times we got it right. An example is
    shown in [table 3.1](#ch03table01).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是一个矩阵，显示了模型预测错误（或正确）的情况，以及它有多少次“混淆”。在其最简单的形式中，它将是一个2x2的表格，用于尝试将观测值分类为A或B的模型。假设我们有一个分类模型，它预测某人是否会购买我们最新的产品：油炸樱桃布丁。我们可以预测：“是的，这个人会购买”或“不，这位顾客不会购买”。一旦我们对100人做出预测，我们可以将它们与他们的实际行为进行比较，这会告诉我们我们有多少次预测正确。一个例子在[表3.1](#ch03table01)中显示。
- en: Table 3.1\. Confusion matrix example
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.1\. 混淆矩阵示例
- en: '| Confusion matrix | Predicted “Person will buy” | Predicted “Person will not
    buy” |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 混淆矩阵 | 预测“人会购买” | 预测“人不会购买” |'
- en: '| --- | --- | --- |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Person **bought** the deep-fried cherry pudding | 35 (true positive) | 10
    (false negative) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 人员 **购买** 油炸樱桃布丁 | 35 (真阳性) | 10 (假阴性) |'
- en: '| Person **didn’t** buy the deep-fried cherry pudding | 15 (false positive)
    | 40 (true negative) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 人员 **未购买** 油炸樱桃布丁 | 15 (假阳性) | 40 (真阴性) |'
- en: The model was correct in (35+40) 75 cases and incorrect in (15+10) 25 cases,
    resulting in a (75 correct/100 total observations) 75% accuracy.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在 (35+40) 75 个案例中是正确的，在 (15+10) 25 个案例中是错误的，结果是一个 (75 正确/100 总观察值) 75% 的准确率。
- en: 'All the correctly classified observations are added up on the diagonal (35+40)
    while everything else (15+10) is incorrectly classified. When the model only predicts
    two classes (binary), our correct guesses are two groups: true positives (predicted
    to buy and did so) and true negatives (predicted they wouldn’t buy and they didn’t).
    Our incorrect guesses are divided into two groups: false positives (predicted
    they would buy but they didn’t) and false negatives (predicted not to buy but
    they did). The matrix is useful to see where the model is having the most problems.
    In this case we tend to be overconfident in our product and classify customers
    as future buyers too easily (false positive).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 所有正确分类的观察结果都加在对角线上（35+40），而其他所有内容（15+10）都被错误分类。当模型只预测两个类别（二元）时，我们的正确猜测分为两组：真阳性（预测购买并确实购买）和真阴性（预测不会购买且确实没有购买）。我们的错误猜测分为两组：假阳性（预测他们会购买但实际没有）和假阴性（预测不会购买但实际购买了）。矩阵有助于看到模型最容易出现问题的位置。在这种情况下，我们对自己的产品过于自信，轻易地将客户分类为未来的购买者（假阳性）。
- en: '|  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: From the confusion matrix, we can deduce that for most images the predictions
    are quite accurate. In a good model you’d expect the sum of the numbers on the
    main diagonal of the matrix (also known as the matrix *trace*) to be very high
    compared to the sum of all matrix entries, indicating that the predictions were
    correct for the most part.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 从混淆矩阵中，我们可以推断出对于大多数图像，预测都是相当准确的。在一个好的模型中，你期望矩阵主对角线上的数字之和（也称为矩阵 *迹*）与矩阵所有条目之和相比非常高，这表明大部分预测是正确的。
- en: 'Let’s assume we want to show off our results in a more easily understandable
    way or we want to inspect several of the images and the predictions our program
    has made: we can use the following code to display one next to the other. Then
    we can see where the program has gone wrong and needs a little more training.
    If we’re satisfied with the results, the model building ends here and we arrive
    at step six: presenting the results.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要以更易于理解的方式展示我们的结果，或者我们想要检查几个图像以及我们的程序所做的预测：我们可以使用以下代码来并排显示。然后我们可以看到程序出错的地方，需要更多的训练。如果我们对结果满意，模型构建就结束了，我们到达第六步：展示结果。
- en: Listing 3.4\. Inspecting predictions vs actual numbers
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.4\. 检查预测值与实际数值
- en: '![](Images/071fig01_alt.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/071fig01_alt.jpg)'
- en: '[Figure 3.7](#ch03fig07) shows how all predictions seem to be correct except
    for the digit number 2, which it labels as 8\. We should forgive this mistake
    as this 2 does share visual similarities with 8\. The bottom left number is ambiguous,
    even to humans; is it a 5 or a 3? It’s debatable, but the algorithm thinks it’s
    a 3.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.7](#ch03fig07) 展示了所有预测似乎都是正确的，除了数字 2，它将其标记为 8。我们应该原谅这个错误，因为这个 2 与 8 在视觉上确实有相似之处。左下角的数字是模糊的，即使是人类；它是
    5 还是 3？这值得商榷，但算法认为它是 3。'
- en: Figure 3.7\. For each blurry image a number is predicted; only the number 2
    is misinterpreted as 8\. Then an ambiguous number is predicted to be 3 but it
    could as well be 5; even to human eyes this isn’t clear.
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.7\. 对于每个模糊图像，都会预测一个数字；只有数字 2 被误认为是 8。然后一个模糊的数字被预测为 3，但它也可能是 5；即使是人类眼睛也看不清楚。
- en: '![](Images/03fig07.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/03fig07.jpg)'
- en: By discerning which images were misinterpreted, we can train the model further
    by labeling them with the correct number they display and feeding them back into
    the model as a new training set (step 5 of the data science process). This will
    make the model more accurate, so the cycle of learn, predict, correct continues
    and the predictions become more accurate. This is a controlled data set we’re
    using for the example. All the examples are the same size and they are all in
    16 shades of gray. Expand that up to the variable size images of variable length
    strings of variable shades of alphanumeric characters shown in the Captcha control,
    and you can appreciate why a model accurate enough to predict any Captcha image
    doesn’t exist yet.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 通过辨别哪些图像被误解，我们可以通过将正确的数字标签添加到这些图像上，并将它们作为新的训练集反馈到模型中（数据科学过程的第5步）来进一步训练模型。这将使模型更准确，因此学习、预测、纠正的循环继续，预测变得更加准确。这是一个我们用于示例的受控数据集。所有示例大小相同，它们都是16种灰度的。扩展到Captcha控制中显示的变量大小、变量长度字符串和变量色调的字母数字字符，你就可以理解为什么足够准确以预测任何Captcha图像的模型还不存在。
- en: In this supervised learning example, it’s apparent that without the labels associated
    with each image telling the program what number that image shows, a model cannot
    be built and predictions cannot be made. By contrast, an unsupervised learning
    approach doesn’t need its data to be labeled and can be used to give structure
    to an unstructured data set.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个监督学习示例中，很明显，如果没有与每个图像关联的标签告诉程序该图像显示的数字，就无法构建模型和做出预测。相比之下，无监督学习方法不需要其数据被标记，并且可以用来为无结构化数据集提供结构。
- en: 3.3.2\. Unsupervised learning
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2. 无监督学习
- en: It’s generally true that most large data sets don’t have labels on their data,
    so unless you sort through it all and give it labels, the supervised learning
    approach to data won’t work. Instead, we must take the approach that will work
    with this data because
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，大多数大型数据集的数据上没有标签，所以除非你全部整理并为其添加标签，否则监督学习方法在数据上不会起作用。相反，我们必须采取适合这些数据的方法，因为
- en: We can study the *distribution of the data* and infer truths about the data
    in different parts of the distribution.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以研究数据的*分布*并从分布的不同部分推断关于数据的真相。
- en: We can study the *structure and values in the data* and infer new, more meaningful
    data and structure from it.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以研究数据的*结构和值*并从中推断出新的、更有意义的数据和结构。
- en: Many techniques exist for each of these *unsupervised learning* approaches.
    However, in the real world you’re always working toward the research goal defined
    in the first phase of the data science process, so you may need to combine or
    try different techniques before either a data set can be labeled, enabling supervised
    learning techniques, perhaps, or even the goal itself is achieved.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些*无监督学习*方法中的每一个都存在许多技术。然而，在现实世界中，你总是在数据科学过程的第一阶段定义的研究目标下工作，因此你可能需要在数据集可以标记、启用监督学习技术之前，或者甚至达到目标本身之前，结合或尝试不同的技术。
- en: Discerning a simplified latent structure from your data
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从你的数据中辨别简化的潜在结构
- en: Not everything can be measured. When you meet someone for the first time you
    might try to guess whether they like you based on their behavior and how they
    respond. But what if they’ve had a bad day up until now? Maybe their cat got run
    over or they’re still down from attending a funeral the week before? The point
    is that certain variables can be immediately available while others can only be
    inferred and are therefore missing from your data set. The first type of variables
    are known as *observable variables* and the second type are known as *latent variables*.
    In our example, the emotional state of your new friend is a latent variable. It
    definitely influences their judgment of you but its value isn’t clear.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有事物都可以衡量。当你第一次见到某人时，你可能会根据他们的行为和他们的反应来猜测他们是否喜欢你。但如果是他们今天过得不好呢？也许他们的猫被撞了，或者他们可能还在为上周参加的葬礼感到难过？重点是，某些变量可以立即获得，而其他变量只能推断出来，因此它们缺失在你的数据集中。第一种类型的变量被称为*可观察变量*，第二种类型被称为*潜在变量*。在我们的例子中，你新朋友的情绪状态是一个潜在变量。它肯定会影响他们对你的判断，但它的值并不明确。
- en: Deriving or inferring latent variables and their values based on the actual
    contents of a data set is a valuable skill to have because
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据集的实际内容推导或推断潜在变量及其值是一项非常有用的技能，因为
- en: Latent variables can substitute for several existing variables already in the
    data set.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜在变量可以替代数据集中已经存在的几个变量。
- en: By reducing the number of variables in the data set, the data set becomes more
    manageable, any further algorithms run on it work faster, and predictions may
    become more accurate.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过减少数据集中的变量数量，数据集变得更加易于管理，任何进一步运行的算法都会更快，预测可能更加准确。
- en: Because latent variables are designed or targeted toward the defined research
    goal, you lose little key information by using them.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于潜在变量是为定义的研究目标而设计和针对的，因此使用它们会丢失很少的关键信息。
- en: If we can reduce a data set from 14 observable variables per line to 5 or 6
    latent variables, for example, we have a better chance of reaching our research
    goal because of the data set’s simplified structure. As you’ll see from the example
    below, it’s not a case of reducing the existing data set to as few latent variables
    as possible. You’ll need to find the sweet spot where the number of latent variables
    derived returns the most value. Let’s put this into practice with a small case
    study.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够将数据集从每行14个可观察变量简化到5或6个潜在变量，例如，由于数据集结构的简化，我们将更有可能达到我们的研究目标。从下面的例子中您将看到，这并不是将现有数据集简化到尽可能少的潜在变量的案例。您需要找到潜在变量数量带来的最大价值的“甜点”。让我们通过一个小案例研究来实践一下。
- en: 'Case study: finding latent variables in a wine quality data set'
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 案例研究：在葡萄酒质量数据集中寻找潜在变量
- en: In this short case study, you’ll use a technique known as Principal Component
    Analysis (PCA) to find latent variables in a data set that describes the quality
    of wine. Then you’ll compare how well a set of latent variables works in predicting
    the quality of wine against the original observable set. You’ll learn
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简短的案例研究中，您将使用一种称为主成分分析（PCA）的技术来寻找描述葡萄酒质量的数据集中的潜在变量。然后您将比较一组潜在变量在预测葡萄酒质量方面与原始可观察集相比的效果。您将学习
- en: '**1**.  How to identify and derive those latent variables.'
  id: totrans-201
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**. 如何识别和推导出那些潜在变量。'
- en: '**2**.  How to analyze where the sweet spot is—how many new variables return
    the most utility—by generating and interpreting a *scree plot* generated by PCA.
    (We’ll look at scree plots in a moment.)'
  id: totrans-202
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**. 如何通过生成和解释由PCA生成的*碎石图*来分析“甜点”在哪里——即多少新变量能带来最大的效用。（我们稍后会看到碎石图。）'
- en: Let’s look at the main components of this example.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个例子中的主要组成部分。
- en: '***Data set*** —The University of California, Irvine (UCI) has an online repository
    of 325 data sets for machine learning exercises at [http://archive.ics.uci.edu/ml/](http://archive.ics.uci.edu/ml/).
    We’ll use the Wine Quality Data Set for red wines created by P. Cortez, A. Cerdeira,
    F. Almeida, T. Matos, and J. Reis^([[4](#ch03fn04)]). It’s 1,600 lines long and
    has 11 variables per line, as shown in [table 3.2](#ch03table02).'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***数据集*** —加州大学欧文分校（UCI）有一个在线仓库，包含325个用于机器学习练习的数据集，网址为[http://archive.ics.uci.edu/ml/](http://archive.ics.uci.edu/ml/)。我们将使用由P.
    Cortez、A. Cerdeira、F. Almeida、T. Matos和J. Reis创建的红葡萄酒质量数据集。该数据集有1600行，每行有11个变量，如[表3.2](#ch03table02)所示。'
- en: ⁴
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can find full details of the Wine Quality Data Set at [https://archive.ics.uci.edu/ml/datasets/Wine+Quality](https://archive.ics.uci.edu/ml/datasets/Wine+Quality).
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 您可以在[https://archive.ics.uci.edu/ml/datasets/Wine+Quality](https://archive.ics.uci.edu/ml/datasets/Wine+Quality)找到关于葡萄酒质量数据集的完整详细信息。
- en: Table 3.2\. The first three rows of the Red Wine Quality Data Set
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.2\. 红葡萄酒质量数据集的前三行
- en: '| Fixed acidity | Volatile acidity | Citric acid | Residual sugar | Chlorides
    | Free sulfur dioxide | Total sulfur dioxide | Density | pH | Sulfates | Alcohol
    | Quality |'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 固定酸度 | 挥发性酸度 | 柠檬酸 | 残糖 | 氯化物 | 自由二氧化硫 | 总二氧化硫 | 密度 | pH | 硫酸盐 | 酒精 | 质量
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 7.4 | 0.7 | 0 | 1.9 | 0.076 | 11 | 34 | 0.9978 | 3.51 | 0.56 | 9.4 | 5 |'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 7.4 | 0.7 | 0 | 1.9 | 0.076 | 11 | 34 | 0.9978 | 3.51 | 0.56 | 9.4 | 5 |'
- en: '| 7.8 | 0.88 | 0 | 2.6 | 0.098 | 25 | 67 | 0.9968 | 3.2 | 0.68 | 9.8 | 5 |'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 7.8 | 0.88 | 0 | 2.6 | 0.098 | 25 | 67 | 0.9968 | 3.2 | 0.68 | 9.8 | 5 |'
- en: '| 7.8 | 0.76 | 0.04 | 2.3 | 0.092 | 15 | 54 | 0.997 | 3.26 | 0.65 | 9.8 | 5
    |'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 7.8 | 0.76 | 0.04 | 2.3 | 0.092 | 15 | 54 | 0.997 | 3.26 | 0.65 | 9.8 | 5
    |'
- en: '***Principal Component Analysis*** —A technique to find the latent variables
    in your data set while retaining as much information as possible.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***主成分分析*** —一种在尽可能保留信息的同时，寻找数据集中潜在变量的技术。'
- en: '***Scikit-learn*** —We use this library because it already implements PCA for
    us and is a way to generate the scree plot.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Scikit-learn*** —我们使用这个库是因为它已经为我们实现了PCA，并且是生成碎石图的一种方式。'
- en: 'Part one of the data science process is to set our research goal: We want to
    explain the subjective “wine quality” feedback using the different wine properties.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学过程的第一步是设定我们的研究目标：我们希望利用不同的葡萄酒特性来解释主观的“葡萄酒质量”反馈。
- en: 'Our first job then is to download the data set (step two: acquiring data),
    as shown in the following listing, and prepare it for analysis (step three: data
    preparation). Then we can run the PCA algorithm and view the results to look at
    our options.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的第一个任务是下载数据集（第二步：获取数据），如下面的列表所示，并为其分析做准备（第三步：数据准备）。然后我们可以运行PCA算法并查看结果以查看我们的选项。
- en: Listing 3.5\. Data acquisition and variable standardization
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.5\. 数据获取和变量标准化
- en: '![](Images/074fig01_alt.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/074fig01_alt.jpg)'
- en: 'With the initial data preparation behind you, you can execute the PCA. The
    resulting scree plot (which will be explained shortly) is shown in [figure 3.8](#ch03fig08).
    Because PCA is an explorative technique, we now arrive at step four of the data
    science process: data exploration, as shown in the following listing.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成初步数据准备之后，你可以执行PCA。生成的散点图（将在稍后解释）显示在[图3.8](#ch03fig08)中。因为PCA是一种探索性技术，我们现在到达数据科学过程的第四步：数据探索，如下面的列表所示。
- en: Figure 3.8\. PCA scree plot showing the marginal amount of information of every
    new variable PCA can create. The first variables explain approximately 28% of
    the variance in the data, the second variable accounts for another 17%, the third
    approximately 15%, and so on.
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.8\. PCA散点图显示了PCA可以创建的每个新变量的边际信息量。前几个变量解释了数据中大约28%的方差，第二个变量解释了另外17%，第三个大约15%，以此类推。
- en: '![](Images/03fig08_alt.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/03fig08_alt.jpg)'
- en: Listing 3.6\. Executing the principal component analysis
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.6\. 执行主成分分析
- en: '![](Images/075fig01_alt.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/075fig01_alt.jpg)'
- en: Now let’s look at the scree plot in [figure 3.8](#ch03fig08).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看[图3.8](#ch03fig08)中的散点图。
- en: The plot generated from the wine data set is shown in [figure 3.8](#ch03fig08).
    What you hope to see is an elbow or hockey stick shape in the plot. This indicates
    that a few variables can represent the majority of the information in the data
    set while the rest only add a little more. In our plot, PCA tells us that reducing
    the set down to one variable can capture approximately 28% of the total information
    in the set (the plot is zero-based, so variable one is at position zero on the
    x axis), two variables will capture approximately 17% more or 45% total, and so
    on. [Table 3.3](#ch03table03) shows you the full read-out.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 从葡萄酒数据集生成的图表显示在[图3.8](#ch03fig08)中。你希望看到的是图表中的肘形或曲棍球棒形状。这表明几个变量可以代表数据集中大部分的信息，而其余的只增加了少许。在我们的图表中，PCA告诉我们，将集合减少到一个变量可以捕获集合中大约28%的总信息（图表从零开始，所以变量一在x轴上的位置为零），两个变量将捕获大约17%更多或45%的总信息，以此类推。[表3.3](#ch03table03)显示了完整的读数。
- en: Table 3.3\. The findings of the PCA
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.3\. PCA的结果
- en: '| Number of variables | Extra information captured | Total data captured |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 变量数量 | 捕获的额外信息 | 总数据捕获 |'
- en: '| --- | --- | --- |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 28% | 28% |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 28% | 28% |'
- en: '| 2 | 17% | 45% |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 17% | 45% |'
- en: '| 3 | 14% | 59% |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 14% | 59% |'
- en: '| 4 | 10% | 69% |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 10% | 69% |'
- en: '| 5 | 8% | 77% |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 8% | 77% |'
- en: '| 6 | 7% | 84% |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 7% | 84% |'
- en: '| 7 | 5% | 89% |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 5% | 89% |'
- en: '| 8 - 11 | ... | 100% |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 8 - 11 | ... | 100% |'
- en: An elbow shape in the plot suggests that five variables can hold most of the
    information found inside the data. You could argue for a cut-off at six or seven
    variables instead, but we’re going to opt for a simpler data set versus one with
    less variance in data against the original data set.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图表中出现的肘形表明，五个变量可以包含数据内部的大部分信息。你也可以争论在六个或七个变量处截断，但我们将选择一个比原始数据集方差更小的简单数据集。
- en: At this point, we could go ahead and see if the original data set recoded with
    five latent variables is good enough to predict the quality of the wine accurately,
    but before we do, we’ll see how we might identify what they represent.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以继续查看使用五个潜在变量重新编码的原始数据集是否足够准确预测葡萄酒的质量，但在我们这样做之前，我们将看看我们如何识别它们代表的内容。
- en: Interpreting the new variables
  id: totrans-240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解释新变量
- en: With the initial decision made to reduce the data set from 11 original variables
    to 5 latent variables, we can check to see whether it’s possible to interpret
    or name them based on their relationships with the originals. Actual names are
    easier to work with than codes such as lv1, lv2, and so on. We can add the line
    of code in the following listing to generate a table that shows how the two sets
    of variables correlate.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定将数据集从11个原始变量减少到5个潜在变量后，我们可以检查是否可以根据它们与原始变量的关系来解释或命名它们。实际名称比lv1、lv2等代码更容易使用。我们可以在以下列表中添加一行代码以生成一个表格，显示两组变量之间的相关性。
- en: Listing 3.7\. Showing PCA components in a Pandas data frame
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.7\. 在Pandas数据框中显示PCA组件
- en: '[PRE1]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The rows in the resulting table ([table 3.4](#ch03table04)) show the mathematical
    correlation. Or, in English, the first latent variable lv1, which captures approximately
    28% of the total information in the set, has the following formula.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表([表3.4](#ch03table04))中的行显示了数学相关性。或者用英语来说，第一个潜在变量lv1，它捕捉了大约28%的总信息，具有以下公式。
- en: Table 3.4\. How PCA calculates the 11 original variables’ correlation with 5
    latent variables
  id: totrans-245
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.4\. PCA如何计算11个原始变量与5个潜在变量之间的相关性
- en: '|   | Fixed acidity | Volatile acidity | Citric acid | Residual sugar | Chlorides
    | Free sulfur dioxide | Total sulfur dioxide | Density | pH | Sulphates | Alcohol
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|   | 固定酸度 | 挥发性酸度 | 柠檬酸 | 残余糖 | 氯化物 | 自由二氧化硫 | 总二氧化硫 | 密度 | pH | 硫酸盐 | 酒精
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 0.489314 | -0.238584 | 0.463632 | 0.146107 | 0.212247 | -0.036158 | 0.023575
    | 0.395353 | -0.438520 | 0.242921 | -0.113232 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.489314 | -0.238584 | 0.463632 | 0.146107 | 0.212247 | -0.036158 | 0.023575
    | 0.395353 | -0.438520 | 0.242921 | -0.113232 |'
- en: '| 1 | -0.110503 | 0.274930 | -0.151791 | 0.272080 | 0.148052 | 0.513567 | 0.569487
    | 0.233575 | 0.006711 | -0.037554 | -0.386181 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 1 | -0.110503 | 0.274930 | -0.151791 | 0.272080 | 0.148052 | 0.513567 | 0.569487
    | 0.233575 | 0.006711 | -0.037554 | -0.386181 |'
- en: '| 2 | 0.123302 | 0.449963 | -0.238247 | -0.101283 | 0.092614 | -0.428793 |
    -0.322415 | 0.338871 | -0.057697 | -0.279786 | -0.471673 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.123302 | 0.449963 | -0.238247 | -0.101283 | 0.092614 | -0.428793 |
    -0.322415 | 0.338871 | -0.057697 | -0.279786 | -0.471673 |'
- en: '| 3 | -0.229617 | 0.078960 | -0.079418 | -0.372793 | 0.666195 | -0.043538 |
    -0.034577 | -0.174500 | -0.003788 | 0.550872 | -0.122181 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 3 | -0.229617 | 0.078960 | -0.079418 | -0.372793 | 0.666195 | -0.043538 |
    -0.034577 | -0.174500 | -0.003788 | 0.550872 | -0.122181 |'
- en: '| 4 | 0.082614 | -0.218735 | 0.058573 | -0.732144 | -0.246501 | 0.159152 |
    0.222465 | -0.157077 | -0.267530 | -0.225962 | -0.350681 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.082614 | -0.218735 | 0.058573 | -0.732144 | -0.246501 | 0.159152 |
    0.222465 | -0.157077 | -0.267530 | -0.225962 | -0.350681 |'
- en: '[PRE2]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Giving a useable name to each new variable is a bit trickier and would probably
    require consultation with an actual wine expert for accuracy. However, as we don’t
    have a wine expert on hand, we’ll call them the following ([table 3.5](#ch03table05)).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 给每个新变量起一个可用的名字有点棘手，可能需要咨询实际的葡萄酒专家以确保准确性。然而，由于我们手头没有葡萄酒专家，我们将它们称为以下内容([表3.5](#ch03table05))。
- en: Table 3.5\. Interpretation of the wine quality PCA-created variables
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.5\. 基于PCA创建的葡萄酒质量变量的解释
- en: '| Latent variable | Possible interpretation |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 潜在变量 | 可能的解释 |'
- en: '| --- | --- |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0 | Persistent acidity |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 持续酸度 |'
- en: '| 1 | Sulfides |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 硫化物 |'
- en: '| 2 | Volatile acidity |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 挥发性酸度 |'
- en: '| 3 | Chlorides |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 氯化物 |'
- en: '| 4 | Lack of residual sugar |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 缺乏残余糖 |'
- en: 'We can now recode the original data set with only the five latent variables.
    Doing this is data preparation again, so we revisit step three of the data science
    process: data preparation. As mentioned in [chapter 2](kindle_split_010.xhtml#ch02),
    the data science process is a recursive one and this is especially true between
    step three: data preparation and step 4: data exploration.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以用只有五个潜在变量的原始数据集进行重新编码。这样做是数据准备，因此我们回顾数据科学过程中的第三步：数据准备。如[第2章](kindle_split_010.xhtml#ch02)中所述，数据科学过程是递归的，这尤其是在第三步：数据准备和第四步：数据探索之间。
- en: '[Table 3.6](#ch03table06) shows the first three rows with this done.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[表3.6](#ch03table06)显示了完成此操作的前三行。'
- en: Table 3.6\. The first three rows of the Red Wine Quality Data Set recoded in
    five latent variables
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.6\. 在五个潜在变量中重新编码的红葡萄酒质量数据集的前三行
- en: '|   | Persistent acidity | Sulfides | Volatile acidity | Chlorides | Lack of
    residual sugar |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|   | 持续酸度 | 硫化物 | 挥发性酸度 | 氯化物 | 缺乏残余糖 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | -1.619530 | 0.450950 | **1.774454** | 0.043740 | -0.067014 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 0 | -1.619530 | 0.450950 | **1.774454** | 0.043740 | -0.067014 |'
- en: '| 1 | -0.799170 | 1.856553 | 0.911690 | 0.548066 | 0.018392 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 1 | -0.799170 | 1.856553 | 0.911690 | 0.548066 | 0.018392 |'
- en: '| 2 | **2.357673** | -0.269976 | -0.243489 | -0.928450 | **1.499149** |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 2 | **2.357673** | -0.269976 | -0.243489 | -0.928450 | **1.499149** |'
- en: Already we can see high values for wine 0 in volatile acidity, while wine 2
    is particularly high in persistent acidity. Don’t sound like good wines at all!
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 已经可以看到，葡萄酒0在挥发性酸度上有很高的值，而葡萄酒2在持久性酸度上特别高。听起来一点也不像好酒！
- en: Comparing the accuracy of the original data set with latent variables
  id: totrans-272
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 比较原始数据集与潜在变量的准确性
- en: Now that we’ve decided our data set should be recoded into 5 latent variables
    rather than the 11 originals, it’s time to see how well the new data set works
    for predicting the quality of wine when compared to the original. We’ll use the
    Naïve Bayes Classifier algorithm we saw in the previous example for supervised
    learning to help.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经决定将数据集重新编码为5个潜在变量而不是最初的11个，现在是时候看看新的数据集在预测葡萄酒质量方面与原始数据集相比表现如何了。我们将使用之前示例中看到的朴素贝叶斯分类器算法来进行监督学习以帮助。
- en: Let’s start by seeing how well the original 11 variables could predict the wine
    quality scores. The following listing presents the code to do this.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看原始的11个变量如何预测葡萄酒质量评分。以下列表展示了执行此操作的代码。
- en: Listing 3.8\. Wine score prediction before principal component analysis
  id: totrans-275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.8\. 主成分分析前的葡萄酒评分预测
- en: '![](Images/078fig01_alt.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/078fig01_alt.jpg)'
- en: Now we’ll run the same prediction test, but starting with only 1 latent variable
    instead of the original 11\. Then we’ll add another, see how it did, add another,
    and so on to see how the predictive performance improves. The following listing
    shows how this is done.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将运行相同的预测测试，但开始时只有1个潜在变量而不是原始的11个。然后我们再添加另一个，看看效果如何，再添加一个，以此类推，以查看预测性能如何提高。以下列表展示了如何进行此操作。
- en: Listing 3.9\. Wine score prediction with increasing number of principal components
  id: totrans-278
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.9\. 随着主成分数量增加的葡萄酒评分预测
- en: '![](Images/078fig02_alt.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/078fig02_alt.jpg)'
- en: The resulting plot is shown in [figure 3.9](#ch03fig09).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在[图 3.9](#ch03fig09)中。
- en: Figure 3.9\. The results plot shows that adding more latent variables to a model
    (x-axis) greatly increases predictive power (y-axis) up to a point but then tails
    off. The gain in predictive power from adding variables wears off eventually.
  id: totrans-281
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.9\. 结果图显示，向模型添加更多潜在变量（x轴）可以大大提高预测能力（y轴），但达到一定程度后就会下降。添加变量带来的预测能力提升最终会减弱。
- en: '![](Images/03fig09.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/03fig09.jpg)'
- en: The plot in [figure 3.9](#ch03fig09) shows that with only 3 latent variables,
    the classifier does a better job of predicting wine quality than with the original
    11\. Also, adding more latent variables beyond 5 doesn’t add as much predictive
    power as the first 5\. This shows our choice of cutting off at 5 variables was
    a good one, as we’d hoped.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3.9](#ch03fig09)中的图表显示，只有3个潜在变量时，分类器在预测葡萄酒质量方面比原始的11个变量做得更好。此外，添加超过5个潜在变量并不会像最初的5个那样增加太多的预测能力。这表明我们选择在5个变量处截断是一个很好的选择，正如我们所希望的。'
- en: We looked at how to group similar variables, but it’s also possible to group
    observations.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了如何对相似变量进行分组，但也可以对观测值进行分组。
- en: Grouping similar observations to gain insight from the distribution of your
    data
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通过对相似观测值进行分组来从数据分布中获得洞察
- en: Suppose for a moment you’re building a website that recommends films to users
    based on preferences they’ve entered and films they’ve watched. The chances are
    high that if they watch many horror movies they’re likely to want to know about
    new horror movies and not so much about new teen romance films. By grouping together
    users who’ve watched more or less the same films and set more or less the same
    preferences, you can gain a good deal of insight into what else they might like
    to have recommended.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在构建一个网站，该网站根据用户输入的偏好和观看的电影推荐电影。如果他们看了很多恐怖电影，他们很可能想了解新的恐怖电影，而不是那么想了解新的青少年浪漫电影。通过将观看相似电影并设置相似偏好的用户分组在一起，你可以获得很多关于他们可能还喜欢什么推荐的洞察。
- en: The general technique we’re describing here is known as *clustering.* In this
    process, we attempt to divide our data set into observation subsets, or *clusters*,
    wherein observations should be similar to those in the same cluster but differ
    greatly from the observations in other clusters. [Figure 3.10](#ch03fig10) gives
    you a visual idea of what clustering aims to achieve. The circles in the top left
    of the figure are clearly close to each other while being farther away from the
    others. The same is true of the crosses in the top right.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里描述的一般技术被称为*聚类*。在这个过程中，我们试图将我们的数据集划分为观测子集，或*聚类*，其中观测值应该与同一聚类中的观测值相似，但与其他聚类中的观测值差异很大。[图3.10](#ch03fig10)给你一个聚类试图达到的目标的直观概念。图顶左部的圆圈彼此很近，而与其他圆圈较远。右上角的十字架也是同样的情况。
- en: Figure 3.10\. The goal of clustering is to divide a data set into “sufficiently
    distinct” subsets. In this plot for instance, the observations have been divided
    into three clusters.
  id: totrans-288
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.10。聚类的目标是把数据集划分为“足够不同的”子集。在这个例子中，观测值被划分为三个聚类。
- en: '![](Images/03fig10.jpg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/03fig10.jpg)'
- en: Scikit-learn implements several common algorithms for clustering data in its
    `sklearn.cluster` module, including the k-means algorithm, affinity propagation,
    and spectral clustering. Each has a use case or two for which it’s more suited,^([[5](#ch03fn05)])
    although k-means is a good general-purpose algorithm with which to get started.
    However, like all the clustering algorithms, you need to specify the number of
    desired clusters in advance, which necessarily results in a process of trial and
    error before reaching a decent conclusion. It also presupposes that all the data
    required for analysis is available already. What if it wasn’t?
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn在其`sklearn.cluster`模块中实现了几个常见的聚类数据算法，包括k-means算法、亲和传播和谱聚类。每个算法都有一些用例，更适合某些情况，尽管k-means是一个很好的通用算法，可以用来入门。然而，像所有聚类算法一样，你需要提前指定所需的聚类数量，这必然会导致在得出合理结论之前进行试错的过程。它还假设所有分析所需的数据已经可用。如果数据不可用怎么办？
- en: ⁵
  id: totrans-291
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-292
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can find a comparison of all the clustering algorithms in Scikit-learn at
    [http://scikit-learn.org/stable/modules/clustering.html](http://scikit-learn.org/stable/modules/clustering.html).
  id: totrans-293
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以在[http://scikit-learn.org/stable/modules/clustering.html](http://scikit-learn.org/stable/modules/clustering.html)找到Scikit-learn中所有聚类算法的比较。
- en: Let’s look at the actual case of clustering irises (the flower) by their properties
    (sepal length and width, petal length and width, and so on). In this example we’ll
    use the k-means algorithm. It’s a good algorithm to get an impression of the data
    but it’s sensitive to start values, so you can end up with a different cluster
    every time you run the algorithm unless you manually define the start values by
    specifying a seed (constant for the start value generator). If you need to detect
    a hierarchy, you’re better off using an algorithm from the class of hierarchical
    clustering techniques.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看通过属性（花瓣长度和宽度、花瓣长度和宽度等）对鸢尾花（一种花）进行聚类的实际案例。在这个例子中，我们将使用k-means算法。这是一个很好的算法，可以让你对数据有一个大致的了解，但它对起始值很敏感，所以每次运行算法时，你可能会得到不同的聚类结果，除非你通过指定一个种子（起始值生成器的常量）手动定义起始值。如果你需要检测层次结构，你最好使用层次聚类技术类别的算法。
- en: One other disadvantage is the need to specify the number of desired clusters
    in advance. This often results in a process of trial and error before coming to
    a satisfying conclusion.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个缺点是需要提前指定所需的聚类数量。这通常会导致在得出令人满意的结论之前进行试错的过程。
- en: Executing the code is fairly simple. It follows the same structure as all the
    other analyses except you don’t have to pass a target variable. It’s up to the
    algorithm to learn interesting patterns. The following listing uses an iris data
    set to see if the algorithm can group the different types of irises.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码相当简单。它遵循所有其他分析相同的结构，除了你不需要传递一个目标变量。算法将学习有趣的模式。下面的列表使用鸢尾花数据集来查看算法是否可以将不同类型的鸢尾花分组。
- en: Listing 3.10\. Iris classification example
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.10。鸢尾花分类示例
- en: '![](Images/ch03ex10-0.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/ch03ex10-0.jpg)'
- en: '![](Images/ch03ex10-1.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/ch03ex10-1.jpg)'
- en: '[Figure 3.11](#ch03fig11) shows the output of the iris classification.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.11](#ch03fig11)显示了鸢尾花分类的输出。'
- en: Figure 3.11\. Output of the iris classification
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.11。鸢尾花分类的输出
- en: '![](Images/03fig11.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/03fig11.jpg)'
- en: This figure shows that even without using a label you’d find clusters that are
    similar to the official iris classification with a result of 134 (50+48+36) correct
    classifications out of 150.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 此图显示，即使不使用标签，你也会找到与官方的鸢尾花分类相似的簇，结果是150个正确分类中的134个（50+48+36）。
- en: You don’t always need to choose between supervised and unsupervised; sometimes
    combining them is an option.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 你并不总是需要在监督学习和无监督学习之间做出选择；有时将它们结合起来是一个选项。
- en: 3.4\. Semi-supervised learning
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4. 半监督学习
- en: It shouldn’t surprise you to learn that while we’d like all our data to be labeled
    so we can use the more powerful supervised machine learning techniques, in reality
    we often start with only minimally labeled data, if it’s labeled at all. We can
    use our unsupervised machine learning techniques to analyze what we have and perhaps
    add labels to the data set, but it will be prohibitively costly to label it all.
    Our goal then is to train our predictor models with as little labeled data as
    possible. This is where semi-supervised learning techniques come in—hybrids of
    the two approaches we’ve already seen.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 当你了解到，尽管我们希望所有数据都被标记以便使用更强大的监督式机器学习技术，但在现实中，我们通常只从最少标记的数据开始，如果有的话。我们可以使用我们的无监督机器学习技术来分析我们所拥有的数据，并可能为数据集添加标签，但全部标记的成本将非常高昂。因此，我们的目标是用尽可能少的标记数据来训练我们的预测模型。这就是半监督学习技术发挥作用的地方——它是我们之前看到两种方法的混合体。
- en: Take for example the plot in [figure 3.12](#ch03fig12). In this case, the data
    has only two labeled observations; normally this is too few to make valid predictions.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 以图3.12中的示例来说明。在这种情况下，数据中只有两个标记的观测值；通常情况下，这太少以至于无法做出有效的预测。
- en: Figure 3.12\. This plot has only two labeled observations—too few for supervised
    observations, but enough to start with an unsupervised or semi-supervised approach.
  id: totrans-308
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.12。此图只有两个标记的观测值——对于监督学习来说太少，但足以开始使用无监督或半监督方法。
- en: '![](Images/03fig12_alt.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/03fig12_alt.jpg)'
- en: A common semi-supervised learning technique is *label propagation*. In this
    technique, you start with a labeled data set and give the same label to similar
    data points. This is similar to running a clustering algorithm over the data set
    and labeling each cluster based on the labels they contain. If we were to apply
    this approach to the data set in [figure 3.12](#ch03fig12), we might end up with
    something like [figure 3.13](#ch03fig13).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的半监督学习技术是*标签传播*。在这种技术中，你从一个标记的数据集开始，并将相同的标签赋予相似的数据点。这类似于在数据集上运行聚类算法，并根据它们包含的标签对每个簇进行标记。如果我们将这种方法应用于图3.12中的数据集，我们可能会得到类似于图3.13的结果。
- en: Figure 3.13\. The previous figure shows that the data has only two labeled observations,
    far too few for supervised learning. This figure shows how you can exploit the
    structure of the underlying data set to learn better classifiers than from the
    labeled data only. The data is split into two clusters by the clustering technique;
    we only have two labeled values, but if we’re bold we can assume others within
    that cluster have that same label (buyer or non-buyer), as depicted here. This
    technique isn’t flawless; it’s better to get the actual labels if you can.
  id: totrans-311
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.13。前面的图显示，数据中只有两个标记的观测值，对于监督学习来说远远不够。此图显示了如何利用底层数据集的结构来学习比仅从标记数据更好的分类器。数据被聚类技术分为两个簇；我们只有两个标记的值，但如果我们大胆一些，可以假设该簇内的其他值具有相同的标签（买家或非买家），如图所示。这种技术并不完美；如果能得到实际的标签就更好了。
- en: '![](Images/03fig13_alt.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/03fig13_alt.jpg)'
- en: One special approach to semi-supervised learning worth mentioning here is *active
    learning*. In active learning the program points out the observations it wants
    to see labeled for its next round of learning based on some criteria you have
    specified. For example, you might set it to try and label the observations the
    algorithm is least certain about, or you might use multiple models to make a prediction
    and select the points where the models disagree the most.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的半监督学习特殊方法之一是*主动学习*。在主动学习中，程序根据你指定的某些标准指出它想要标记的观测值，以便进行下一轮学习。例如，你可以设置它尝试标记算法最不确定的观测值，或者你可以使用多个模型进行预测，并选择模型意见分歧最大的点。
- en: With the basics of machine learning at your disposal, the next chapter discusses
    using machine learning within the constraints of a single computer. This tends
    to be challenging when the data set is too big to load entirely into memory.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在你掌握机器学习基础知识之后，下一章将讨论在单个计算机的约束下使用机器学习。当数据集太大而无法完全加载到内存中时，这通常具有挑战性。
- en: 3.5\. Summary
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5\. 摘要
- en: In this chapter, you learned that
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解到
- en: Data scientists rely heavily on techniques from statistics and machine learning
    to perform their modeling. A good number of real-life applications exist for machine
    learning, from classifying bird whistling to predicting volcanic eruptions.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学家高度依赖统计学和机器学习的技术来进行他们的建模。机器学习在现实生活中的应用非常广泛，从分类鸟鸣声到预测火山爆发。
- en: 'The modeling process consists of four phases:'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建模过程包括四个阶段：
- en: '**1**.  *Feature engineering, data preparation, and model parameterization*—We
    define the input parameters and variables for our model.'
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**.  *特征工程、数据准备和模型参数化*—我们定义模型输入的参数和变量。'
- en: '**2**.  *Model training*—The model is fed with data and it learns the patterns
    hidden in the data.'
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**.  *模型训练*—模型被喂入数据，并学习数据中隐藏的模式。'
- en: '**3**.  *Model selection and validation*—A model can perform well or poorly;
    based on its performance we select the model that makes the most sense.'
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**.  *模型选择与验证*—一个模型可能表现良好或糟糕；根据其表现，我们选择最合理的模型。'
- en: '**4**.  *Model scoring*—When our model can be trusted, it’s unleashed on new
    data. If we did our job well, it will provide us with extra insights or give us
    a good prediction of what the future holds.'
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**4**.  *模型评分*—当我们的模型值得信赖时，它将被应用于新数据。如果我们做得好，它将为我们提供额外的见解或对我们对未来情况的预测提供良好的结果。'
- en: The two big types of machine learning techniques
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两种主要的机器学习技术
- en: '**1**.  *Supervised*—Learning that requires labeled data.'
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**.  *监督学习*—需要标记数据的机器学习。'
- en: '**2**.  *Unsupervised*—Learning that doesn’t require labeled data but is usually
    less accurate or reliable than supervised learning.'
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**.  *无监督学习*—不需要标记数据，但通常比监督学习不准确或不可靠。'
- en: Semi-supervised learning is in between those techniques and is used when only
    a small portion of the data is labeled.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半监督学习介于这些技术之间，当只有一小部分数据被标记时使用。
- en: 'Two case studies demonstrated supervised and unsupervised learning, respectively:'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个案例研究分别展示了监督学习和无监督学习：
- en: '**1**.  Our first case study made use of a Naïve Bayes classifier to classify
    images of numbers as the number they represent. We also took a look at the confusion
    matrix as a means to determining how well our classification model is doing.'
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**.  我们的第一个案例研究使用了朴素贝叶斯分类器来对数字图像进行分类，以表示其代表的数字。我们还研究了混淆矩阵，作为确定我们的分类模型表现如何的手段。'
- en: '**2**.  Our case study on unsupervised techniques showed how we could use principal
    component analysis to reduce the input variables for further model building while
    maintaining most of the information.'
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**.  我们对无监督技术的案例研究展示了我们如何使用主成分分析来减少输入变量，以便进一步构建模型，同时保持大部分信息。'
