- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Working with a mountain of data
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 处理大量数据
- en: '**This chapter covers**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章涵盖**'
- en: Using a database for a more efficient data-wrangling process
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据库进行更高效的数据处理过程
- en: Getting a huge data file into MongoDB
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将大量数据文件导入MongoDB
- en: Working effectively with a large database
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效地处理大量数据
- en: Optimizing your code for improved data throughput
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化你的代码以提高数据吞吐量
- en: 'This chapter addresses the question: How can we be more efficient and effective
    when we’re working with a massive data set?'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章解决的问题是：当我们处理大量数据集时，我们如何更高效和有效地工作？
- en: In the last chapter, we worked with several extremely large files that were
    originally downloaded from the National Oceanic and Atmospheric Administration.
    Chapter 7 showed that it’s possible to work with CSV and JSON files that are this
    large! However, files of this magnitude are too big for effective use in data
    analysis. To be productive now, we must move our large data set to a database.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们处理了从国家海洋和大气管理局下载的几个非常大的文件。第7章表明，可以处理这样大的CSV和JSON文件！然而，这样大小的文件对于数据分析来说太大，无法有效使用。为了现在变得高效，我们必须将我们的大数据集移动到数据库中。
- en: In this chapter, we move our data into a MongoDB database, and this is a big
    operation considering the size of the data. With our data in the database, we
    can work more effectively with the help of queries and other features of the database
    API.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将数据移动到MongoDB数据库中，考虑到数据的大小，这是一个大操作。数据在数据库中，我们可以借助查询和其他数据库API功能更有效地工作。
- en: I selected MongoDB for this chapter, and the book generally, because it’s my
    preferred database. That’s a personal (and I believe also a practical) choice,
    but really any database will do, and I encourage you to try out the techniques
    in this chapter on your database of choice. Many of the techniques presented here
    will work with other databases, but you’ll have to figure out how to translate
    the code to work with your technology of choice.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择MongoDB作为本章以及整本书的数据库，因为它是我偏好的数据库。这是一个个人选择（我相信也是一个实用的选择），但实际上任何数据库都可以，我鼓励你尝试在本章中使用你选择的数据库来实践这些技术。这里介绍的大多数技术都可以与其他数据库一起工作，但你必须找出如何将代码转换为与你的技术选择兼容。
- en: 8.1 Expanding our toolkit
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 扩展我们的工具集
- en: In this chapter, we’ll use several MongoDB database tools to work with our large
    data set. We’ll also use Node.js functions to spawn new operating system processes
    to execute data processing operations in parallel on multiple CPU cores.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用几个MongoDB数据库工具来处理我们的大数据集。我们还将使用Node.js函数来创建新的操作系统进程，以便在多个CPU核心上并行执行数据处理操作。
- en: '[Table 8.1](#table8.1) lists the various tools that we cover in chapter 8.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[表8.1](#table8.1) 列出了我们在第8章中介绍的各种工具。'
- en: Table 8.1 Tools used in chapter 8
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.1第8章中使用的工具
- en: '| **API / Library** | **Function** | **Notes** |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| **API / 库** | **函数** | **说明** |'
- en: '| --- | --- | --- |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| MongoDB | `find` | Retrieve a database cursor so that we can visit each record
    in the database incrementally. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| MongoDB | `find` | 获取数据库游标，以便我们可以增量地访问数据库中的每条记录。 |'
- en: '|  | `skip and limit` | Retrieve a data window, or collection of records, so
    that we can visit every record of the database in batches. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|  | `skip and limit` | 获取数据窗口或记录集合，以便我们可以分批访问数据库中的每条记录。 |'
- en: '|  | `createIndex` | Create a database index for efficient query and sorting.
    |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '|  | `createIndex` | 为高效查询和排序创建数据库索引。 |'
- en: '|  | `find(query)` | Find records using a database query. |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '|  | `find(query)` | 使用数据库查询查找记录。 |'
- en: '|  | `find({}, projection)` | Retrieve records but with certain fields discarded.
    |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|  | `find({}, projection)` | 获取记录，但丢弃某些字段。 |'
- en: '|  | `sort` | Sort records retrieved from the database. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|  | `sort` | 对从数据库中检索的记录进行排序。 |'
- en: '| Node.js | `spawn, fork` | Create new operating system processes to operate
    on data in parallel. |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| Node.js | `spawn, fork` | 创建新的操作系统进程以并行处理数据。 |'
- en: '| async-await-parallel | `parallel(sequence, X)` | Execute a sequence of operations
    where X operations are executed in parallel. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| async-await-parallel | `parallel(sequence, X)` | 执行一系列操作，其中X个操作并行执行。 |'
- en: 8.2 Dealing with a mountain of data
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 处理大量数据
- en: We want to analyze the weather stations data set from the previous chapter.
    We can’t do that yet because we have more data than we can deal with effectively.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想分析上一章中的气象站数据集。我们目前还不能这样做，因为我们有比我们能有效处理更多的数据。
- en: We have weather-stations.csv, but at 28 GB, it’s not practical to work with
    this file as it is. Most data science tutorials and courses have you work with
    a CSV file to analyze data, and that’s a great way to work when it’s possible,
    but it’s effective only at a small scale. Using CSV files (and JSON files for
    that matter) doesn’t scale to massive data sets such as we have now. How are we
    going to deal with that?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有weather-stations.csv文件，但28GB的大小使得直接处理这个文件并不实际。大多数数据科学教程和课程都要求你使用CSV文件来分析数据，当可能的时候，这是一种很好的工作方式，但它只适用于小规模。使用CSV文件（以及JSON文件）无法扩展到我们现在拥有的这种大规模数据集。我们该如何处理这个问题呢？
- en: We’re about to move our data to a database, and we’ll then have a number of
    new tools available for working with our data. Before looking at the database,
    though, we’ll explore simpler techniques that will help you manage your large
    data sets. Then we’ll look at the memory limitations of Node.js and how we can
    go beyond them. Finally, we’ll look at code optimization and other ways to increase
    your data throughput.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们即将将数据迁移到数据库，届时我们将有更多新工具可用于处理我们的数据。但在查看数据库之前，我们将探索一些更简单的技术，这些技术将帮助你管理你的大数据集。然后我们将探讨Node.js的内存限制以及我们如何超越它们。最后，我们将探讨代码优化和其他提高数据吞吐量的方法。
- en: 8.3 Getting the code and data
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 获取代码和数据
- en: The code and data for this chapter are available in the Data Wrangling with
    JavaScript Chapter-8 repository in GitHub at [https://github.com/data-wrangling-with-javascript/chapter-8](https://github.com/data-wrangling-with-javascript/chapter-8)[.](http://.)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码和数据可在GitHub上的Data Wrangling with JavaScript Chapter-8存储库中找到，网址为[https://github.com/data-wrangling-with-javascript/chapter-8](https://github.com/data-wrangling-with-javascript/chapter-8)[。](http://.)
- en: The example data is located under the *data* subdirectory in the repository.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 示例数据位于存储库下的*data*子目录中。
- en: The GitHub repo contains two Vagrant scripts that bootstrap virtual machines
    with the MongoDB database for your convenience. The first script boots a VM with
    an empty database that you can use when you run [listing 8.2](#listing8.2) to
    practice importing your data into the database. The second script boots a VM with
    a database that is already prefilled with example data for you to try with [listing
    8.3](#listing8.3) and beyond.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub存储库包含两个Vagrant脚本，这些脚本可以方便地引导虚拟机安装MongoDB数据库。第一个脚本启动一个带有空数据库的虚拟机，当你运行[列表8.2](#listing8.2)以练习将数据导入数据库时可以使用。第二个脚本启动一个带有预先填充示例数据的虚拟机，你可以用它来尝试[列表8.3](#listing8.3)及以后的练习。
- en: Refer to “Getting the code and data” in chapter 2 if you need help getting the
    code and data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在获取代码和数据时需要帮助，请参考第2章的“获取代码和数据”。
- en: 8.4 Techniques for working with big data
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 处理大数据的技术
- en: We need our large data set in a database. However, before we get to that, let’s
    quickly go through several techniques that will help you be more effective in
    any case.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将我们的大数据集存入数据库。然而，在我们做到这一点之前，让我们快速浏览几种技术，这些技术将帮助你在任何情况下都更加高效。
- en: 8.4.1 Start small
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 从小开始
- en: From chapter 5, we already understand that we should start by working with a
    small data set. You should first aggressively cut down your large data set into
    something that you can work with more easily and effectively.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从第5章，我们已经了解到我们应该从处理小数据集开始。你应该首先将你的大数据集削减到可以更容易和更有效地处理的大小。
- en: Working with big data slows you down; you have no way around that, so don’t
    be too eager to dive into big data. Tackle your problems and write your code first
    for a small data set; small problems are easier to solve than big problems! Focus
    on building reliable and well-tested code at a small scale. Then incrementally
    scale up to big data only when you’re confident and ready to deal with it.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 处理大数据会减慢你的速度；你无法绕过这一点，所以不要急于投身于大数据。先解决你的问题，为小数据集编写你的代码；小问题比大问题更容易解决！专注于在小规模上构建可靠且经过充分测试的代码。然后，只有在你自信且准备好处理它时，才逐步扩展到大数据。
- en: 8.4.2 Go back to small
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.2 回到小规模
- en: When you’re working with big data and you hit a problem, cut back your data
    so that you’re again working with a small data set and focusing as closely as
    possible on the problem. Trying to solve a problem in a large data set can be
    like trying to find a needle in a haystack ([figure 8.1](#figure8.1)). This applies
    to troubleshooting any kind of coding problem. You should try to isolate the problem
    by minimizing the space in which it can hide.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当你处理大数据并遇到问题时，减少你的数据量，以便再次处理一个小数据集，尽可能紧密地关注问题。试图在大数据集中解决问题可能就像在干草堆里找针一样（[图8.1](#figure8.1)）。这适用于解决任何类型的编码问题。你应该尝试通过最小化问题可能隐藏的空间来隔离问题。
- en: You can do this by progressively cutting out code and data (where possible)
    until the problem has nowhere left to hide. The problem should then become obvious
    or at least easier to find. To find a problem in a large data set, use a *binary
    search* or the bisection method to progressively cut down the data and home in
    on the problem.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过逐步删除代码和数据（如果可能的话）直到问题无处可藏来做这件事。问题应该变得明显或至少更容易找到。为了在大数据集中找到问题，使用二分搜索或二分法逐步减少数据并聚焦于问题。
- en: '![c08_01.png](Images/c08_01.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![c08_01.png](Images/c08_01.png)'
- en: '[Figure 8.1](#figureanchor8.1) An error in a large data set is like a needle
    in a haystack.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.1](#figureanchor8.1) 大数据集中的错误就像干草堆里的针。'
- en: 8.4.3 Use a more efficient representation
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.3 使用更高效的表现形式
- en: Make sure you’re using an efficient data representation. CSV files are more
    efficient (at least more compact) than JSON files, and using a database is more
    efficient than JSON and CSV ([figure 8.2](#figure8.2)). Using either JSON or CSV
    is effective at a small scale, but when working at a large scale, we need to bring
    out the big guns.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你使用的是高效的数据表示。CSV文件比JSON文件更高效（至少更紧凑），而使用数据库比JSON和CSV更高效（[图8.2](#figure8.2)）。在小规模工作时，使用JSON或CSV是有效的，但在大规模工作时，我们需要拿出“杀手锏”。
- en: '![c08_02.eps](Images/c08_02.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![c08_02.eps](Images/c08_02.png)'
- en: '[Figure 8.2](#figureanchor8.2) The efficiency spectrum of data formats'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.2](#figureanchor8.2) 数据格式的效率谱'
- en: 8.4.4 Prepare your data offline
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.4 离线准备你的数据
- en: Before attempting to scale up, make sure you have adequately prepared your data.
    We should go through a preparation and cleanup phase using the various techniques
    covered in chapter 6 to reduce the amount of data and deal proactively with problems.
    The process of preparing data for production use is summarized in f.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试扩展之前，确保你已经充分准备了你的数据。我们应该通过使用第6章中介绍的各种技术进行准备和清理阶段，以减少数据量并主动处理问题。为生产使用准备数据的过程总结在f中。
- en: '![c08_03.eps](Images/c08_03.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![c08_03.eps](Images/c08_03.png)'
- en: Figure 8.3 Offline preparation of data for use in production
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 离线准备用于生产的数据
- en: How long does such preparation take? It can take an extraordinary amount of
    time depending on the size of your data. For this chapter, I prepared the NOAA
    weather stations data, and I ran a script that executed for more than 40 hours!
    The data was processed in parallel on an 8-core CPU using a technique that I’ll
    cover at the end of this chapter. Don’t worry, though; you won’t have to go through
    40 hours of processing to learn how to do big data processing.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的准备需要多长时间？这取决于你的数据量的大小，可能需要非常多的时间。对于本章，我准备了NOAA气象站数据，并运行了一个执行了超过40小时的脚本！数据是在一个8核心CPU上并行处理的，使用的技术将在本章末尾介绍。不过，不用担心；你不需要经历40小时的处理时间来学习如何进行大数据处理。
- en: How long is too long? I advocate that you let your data processing script run
    for as long as it needs, but there’s obviously an upper limit to this, and it
    depends on the nature of your business. We’d like to get results in a timely fashion.
    For example, if you need to deliver a report this Friday, you can’t run a process
    that goes for longer than that.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 多长时间算太长？我主张让你的数据处理脚本运行尽可能长的时间，但显然有一个上限，这取决于你业务的性质。我们希望及时得到结果。例如，如果你需要在周五提交报告，你不能运行超过那个时间的过程。
- en: Before you get to this stage, you need reliable and robust code (see the section
    “Start small”). You should also use a powerful PC. Keep in mind that if I can
    process 1 billion plus records in 40 hours (a weekend), you can too. This isn’t
    rocket science, but it does require careful preparation and plenty of patience.
    Later in this chapter, we’ll look at methods to optimize our data pipeline and
    achieve greater throughput.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在达到这个阶段之前，你需要可靠且健壮的代码（参见“从小处着手”部分）。你还应该使用功能强大的电脑。记住，如果我能在 40 小时（一个周末）内处理超过 10
    亿条记录，你也可以。这并不是火箭科学，但它确实需要仔细的准备和足够的耐心。在本章的后面部分，我们将探讨优化我们的数据管道和实现更高吞吐量的方法。
- en: 'If you plan to run a long data processing operation, please consider the following
    tips:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划运行长时间的数据处理操作，请考虑以下建议：
- en: Include logging and progress reports so that you can see what’s happening.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含日志和进度报告，这样你可以看到发生了什么。
- en: Report all the errors. You may have to correct for them later.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 报告所有错误。你可能以后需要纠正它们。
- en: Don’t fail the entire process on individual errors. Getting 85% of the way through
    a huge data processing operation is better than having to start from scratch when
    you encounter problems.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要因为个别错误而使整个过程失败。完成一个大型数据处理操作的 85% 比在遇到问题时从头开始要好。
- en: Make sure your process is resumable in case of errors. If you get an error halfway
    through that aborts the process, fix the error; then restart the process and have
    it recover from where it left off.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你的过程在出错时可以恢复。如果你在过程中遇到错误导致进程终止，修复错误；然后重新启动进程，并从上次停止的地方恢复。
- en: 8.5 More Node.js limitations
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 更多 Node.js 限制
- en: In chapter 7, we worked with several extremely large CSV and JSON files, and
    we faced the limitation that we couldn’t load these files entirely into memory.
    We reached this limit because we hit the maximum string size that can be allocated
    in Node.js. At this point we switched to using Node.js streams and incremental
    file processing and that allowed us to deal with these large files. In this chapter,
    we have a new limitation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 7 章中，我们处理了几个非常大的 CSV 和 JSON 文件，并遇到了无法将这些文件完全加载到内存中的限制。我们达到这个限制是因为我们触发了 Node.js
    中可以分配的最大字符串大小。在这个时候，我们转向使用 Node.js 流和增量文件处理，这使得我们能够处理这些大文件。在本章中，我们有一个新的限制。
- en: Using a database means we can load a much bigger data set into memory at the
    one time. Now, though, we’re limited by the maximum amount of memory that can
    be allocated in Node.js before all available memory has been exhausted.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据库意味着我们可以一次性将更大的数据集加载到内存中。然而，现在我们受限于在所有可用内存耗尽之前 Node.js 可以分配的最大内存量。
- en: How much memory exactly? It depends on your version of Node.js and your operating
    system. I’ve tested the limits myself using 64-bit Node.js v7.7.4 running on my
    Windows 10 laptop, which has 8 GB of memory. I tested this by allocating Node.js
    arrays until memory was exhausted; then I estimated the amount of memory that
    had been allocated. This isn’t 100% accurate, but it’s a good way to gauge roughly
    how much memory we can access.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 究竟需要多少内存？这取决于你使用的 Node.js 版本和操作系统。我亲自测试了使用 64 位 Node.js v7.7.4 在我的 Windows 10
    笔记本电脑上运行的极限，该电脑有 8 GB 的内存。我是通过分配 Node.js 数组直到内存耗尽来测试的；然后我估计了已分配的内存量。这并不完全准确，但这是一个很好的方法来大致判断我们可以访问多少内存。
- en: Through testing I can say that I have around 1.4 GB of memory available for
    use. That’s a good amount, and it should handle a pretty hefty data set, but we
    can already see that Node.js can’t load our 28 GB weather stations data set from
    NOAA!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过测试，我可以说我大约有 1.4 GB 的可用内存。这是一个很好的数量，应该可以处理相当大的数据集，但我们已经可以看到 Node.js 无法从 NOAA
    加载我们的 28 GB 气象站数据集！
- en: If you want to know more about how I conducted this test or you want to run
    the test yourself, please see my code in the following GitHub repository at [https://github.com/data-wrangling-with-javascript/nodejs-memory-test](https://github.com/data-wrangling-with-javascript/nodejs-memory-test)[.](http://.)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于我如何进行这项测试或你想自己运行测试，请参阅以下 GitHub 仓库中的我的代码 [https://github.com/data-wrangling-with-javascript/nodejs-memory-test](https://github.com/data-wrangling-with-javascript/nodejs-memory-test)[。](http://.)
- en: 8.6 Divide and conquer
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.6 分而治之
- en: We can’t load the entire weather stations data set into memory, but we can divide
    it up for processing in batches, as shown in [figure 8.4](#figure8.4). Divide
    and conquer is a classic computer science technique. Put simply, we have a big
    problem that’s best solved by dividing it into a number of smaller problems. Smaller
    problems are easier to solve than bigger problems (see the sections “Start small”
    and “Go back to small”). Once we solve each of the smaller problems, we merge
    the result, and we have solved the bigger problem.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法将整个气象站数据集加载到内存中，但我们可以将其分成批次进行处理，如图8.4所示。[figure 8.4](#figure8.4)。分而治之是计算机科学中的一个经典技术。简单来说，我们有一个大问题，最好通过将其分解成多个小问题来解决。小问题比大问题更容易解决（参见“从小处着手”和“回到小处”部分）。一旦我们解决了每个小问题，我们将结果合并，这样我们就解决了大问题。
- en: '![c08_04.eps](Images/c08_04.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![c08_04.eps](Images/c08_04.png)'
- en: '[Figure 8.4](#figureanchor8.4) Splitting data for processing in separate batches'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[Figure 8.4](#figureanchor8.4) 将数据分割成单独的批次进行处理'
- en: When we split our data, we must organize it such that each batch is small enough
    to fit entirely in memory. Not only does this technique allow us to fit our data
    in memory (processed batch by batch), but it can also make processing dramatically
    faster. At the end of this chapter, we’ll see how we can process our data in parallel
    and use multiple CPU cores to massively increase our data throughput.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们分割我们的数据时，我们必须组织它，使得每个批次足够小，可以完全放入内存中。这种技术不仅允许我们将数据放入内存（逐批处理），而且还可以使处理速度大大加快。在本章的结尾，我们将看到如何并行处理我们的数据，并使用多个CPU核心来大幅提高我们的数据吞吐量。
- en: 8.7 Working with large databases
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.7 与大型数据库协同工作
- en: Using a database is the go-to standard for professional data management. All
    databases have features for working with large data sets, and that’s what we’re
    interested in here.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据库是专业数据管理的首选标准。所有数据库都有处理大型数据集的功能，这正是我们感兴趣的。
- en: 'These are the features we’ll look at:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看以下功能：
- en: Incrementally processing one record at a time using a database cursor
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据库游标逐条增量处理记录
- en: Incrementally processing batches of records using data windows
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据窗口逐批增量处理记录
- en: Using queries to filter and discard data
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用查询过滤和丢弃数据
- en: Sorting a large data set
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对大型数据集进行排序
- en: Although most (if not all) databases have the features we need, we’ll focus
    on MongoDB. I had to pick something, and my preference is MongoDB.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数（如果不是所有）数据库都有我们需要的功能，但我们将专注于MongoDB。我必须选择一个，我的首选是MongoDB。
- en: Why MongoDB? It’s convenient, easy to use, and flexible. Most of all, it requires
    no predefined schema. We can express many kinds of schemas and structured data
    with MongoDB, but we don’t have to predefine that structure; we can throw any
    kind of data at MongoDB, and it will handle it. MongoDB and its BSON (binary JSON)
    data format naturally fit well with JavaScript.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么选择MongoDB？它方便、易用且灵活。最重要的是，它不需要预定义的模式。我们可以使用MongoDB表达许多种模式化的结构化数据，但不必预先定义这种结构；我们可以将任何类型的数据扔给MongoDB，它都会处理。MongoDB及其BSON（二进制JSON）数据格式与JavaScript自然地很好地配合。
- en: 8.7.1 Database setup
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1 数据库设置
- en: Before we can start working with our database, we need to have it set up! You
    can download and install MongoDB from [http://www.mongodb.org/](http://www.mongodb.org/).
    Otherwise, you can use one of the Vagrant scripts that I supplied in the GitHub
    repo for chapter 8 (see “Getting the code and data”).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始使用数据库之前，我们需要先设置它！你可以从[http://www.mongodb.org/](http://www.mongodb.org/)下载并安装MongoDB。否则，你可以使用我在GitHub仓库中为第8章提供的Vagrant脚本之一（参见“获取代码和数据”）。
- en: 'To use the scripts, you first need Virtual Box and Vagrant installed. Then
    open a command line and change the directory to the Chapter-8 git repository:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这些脚本，你首先需要安装Virtual Box和Vagrant。然后打开命令行，将目录更改为第8章的git仓库：
- en: '[PRE0]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then you can start a virtual machine with an empty MongoDB database using the
    first vagrant script:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以使用第一个Vagrant脚本启动一个带有空MongoDB数据库的虚拟机：
- en: '[PRE1]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When the VM has finished booting up, you’ll have an empty MongoDB database that’s
    accessible via `mongodb://localhost:6000`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当虚拟机完成启动后，你将拥有一个可通过`mongodb://localhost:6000`访问的空MongoDB数据库。
- en: 'Alternatively, if you want to experiment with a database that already contains
    a sample of the weather stations data (I can’t publish the full data set because
    it’s too large), please use the second Vagrant script:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你想实验一个已经包含气象站数据样本的数据库（由于数据集太大，我无法发布完整的数据集），请使用第二个Vagrant脚本：
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When this VM has finished booting up, you’ll have a MongoDB that contains sample
    data and is accessible on `mongodb://localhost:7000`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个虚拟机完成启动后，你将拥有一个包含示例数据且可通过`mongodb://localhost:7000`访问的MongoDB。
- en: 'After you finish with your virtual machines, please destroy them so they stop
    consuming your system resources. To do this, execute the following command for
    both VMs:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在你完成虚拟机后，请销毁它们，以便停止消耗你的系统资源。为此，为两个虚拟机执行以下命令：
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can recreate the VMs at any time by using Vagrant again. For more information
    on Vagrant, please see appendix C.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过再次使用Vagrant来随时重新创建虚拟机。有关Vagrant的更多信息，请参阅附录C。
- en: 'To access the database from JavaScript, we’ll use the official MongoDB library
    for Node.js. If you install dependencies for the chapter 8 repository, you have
    the MongoDB API installed; otherwise, you can install it in a fresh Node.js project
    as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要从JavaScript访问数据库，我们将使用官方的MongoDB Node.js库。如果你为第8章的存储库安装了依赖项，你已经安装了MongoDB API；否则，你可以在新的Node.js项目中安装它，如下所示：
- en: '[PRE4]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 8.7.2 Opening a connection to the database
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.2 打开数据库连接
- en: 'In all the following code listings, the first thing we must do is connect to
    our database. To keep the listings simple, they all use the following code to
    open the database connection:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有接下来的代码清单中，我们必须做的第一件事是连接到我们的数据库。为了使清单简单，它们都使用以下代码来打开数据库连接：
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The connection string that you pass to `openDatabase` determines which database
    to connect to. For example, the code in [listing 8.2](#listing8.2) connects to
    `mongodb://127.0.0.1:6000`. That’s the empty database in the Vagrant VM that we
    started in the previous section. The other code listings rely on having data to
    work with already, so they connect to `mongodb://localhost:7000`. That’s the database
    from the other VM, the one that’s prefilled with example data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你传递给`openDatabase`的连接字符串决定了要连接到哪个数据库。例如，[代码清单8.2](#listing8.2)连接到`mongodb://127.0.0.1:6000`。这是我们在上一节中启动的Vagrant
    VM中的空数据库。其他代码清单依赖于已经存在的数据，因此它们连接到`mongodb://localhost:7000`。这是来自另一个虚拟机的数据库，其中已经预先填充了示例数据。
- en: If you aren’t using the VMs and have instead installed MongoDB directly on your
    PC, you should set the connection string to `mongodb://127.0.0.1:27017` because
    27017 is the default port number for accessing a local install of MongoDB.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有使用虚拟机，而是直接在你的PC上安装了MongoDB，你应该将连接字符串设置为`mongodb://127.0.0.1:27017`，因为27017是访问本地MongoDB安装的默认端口号。
- en: 8.7.3 Moving large files to your database
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.3 将大文件移动到数据库中
- en: To use a database, we must first transfer our data there. In this case we must
    move our CSV file weather-stations.csv to the database. To do this, we can build
    on the techniques we learned in chapter 7\. We’ll combine a readable CSV data
    input stream with a writable MongoDB output stream to pipe the data into our database,
    as shown in [figure 8.5](#figure8.5).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用数据库，我们必须首先将我们的数据传输到那里。在这种情况下，我们必须将我们的CSV文件weather-stations.csv移动到数据库中。为此，我们可以构建我们在第7章中学到的技术。我们将结合可读的CSV数据输入流和可写的MongoDB输出流，将数据管道输入到我们的数据库中，如图8.5所示。
- en: In chapter 7, we wrote a toolkit function called `openCsvInputStream`. We’ll
    reuse that again here, but we still need a new toolkit function to create a writable
    MongoDB output stream. The code for this is presented in the following listing.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，我们编写了一个名为`openCsvInputStream`的工具包函数。我们在这里将再次使用它，但我们仍然需要一个新工具包函数来创建可写的MongoDB输出流。此代码在以下列表中展示。
- en: Listing 8.1 Toolkit function for opening a MongoDB output stream (toolkit/open-mongodb-output-stream.js)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.1 用于打开MongoDB输出流的工具包函数（toolkit/open-mongodb-output-stream.js）
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This code is similar to the other writable streams that we created in chapter
    7\. Note that we’re opening the stream in *object mode* and inserting each array
    of objects into the database using MongoDB’s `insertMany` function.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码与我们在第7章中创建的其他可写流类似。请注意，我们是以*对象模式*打开流，并使用MongoDB的`insertMany`函数将每个对象数组插入到数据库中。
- en: '[Listing 8.2](#listing8.2) connects both streams into a data pipeline to populate
    our database from the input file weather-stations.csv. You should run this code,
    give it plenty of time to compete, and then inspect your database using Robomongo
    to confirm that the data has indeed been copied to the database.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表8.2](#listing8.2)将两个流连接到数据管道中，从输入文件weather-stations.csv填充我们的数据库。你应该运行此代码，给它足够的时间完成，然后使用Robomongo检查你的数据库，以确认数据确实已复制到数据库中。'
- en: '![c08_05.eps](Images/c08_05.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![c08_05.eps](Images/c08_05.png)'
- en: '[Figure 8.5](#figureanchor8.5) Streaming input CSV data to our MongoDB database'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.5](#figureanchor8.5) 将流式输入CSV数据传输到我们的MongoDB数据库'
- en: Listing 8.2 Moving a huge CSV file to MongoDB (listing-8.2.js)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.2 将大型CSV文件移动到MongoDB（listing-8.2.js）
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Okay, now we have our data in the database! We’re ready to start looking at
    the ways that we now have to efficiently retrieve and work with our data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们的数据已经在数据库中了！我们准备好开始研究我们现在如何高效地检索和使用我们的数据了。
- en: 8.7.4 Incremental processing with a database cursor
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.4 使用数据库游标进行增量处理
- en: With our data in the database, we have multiple ways we can use it to handle
    our large data set! The first of these is using a database cursor to visit each
    and every record in the database, as illustrated in [figure 8.6](#figure8.6).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据库中有了我们的数据后，我们有多种方法可以使用它来处理我们的大量数据！这些方法中的第一个是使用数据库游标访问数据库中的每一条记录，如图8.6所示。
- en: This is another form of incremental data processing, although instead of working
    incrementally with a file as we did in chapter 7, we’re now working incrementally
    with our database. When working this way, we’re not so concerned with exhausting
    our available memory in Node.js—working with one record at a time shouldn’t do
    that—although this also depends on what other work your application is doing at
    the same time.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种增量数据处理的另一种形式，尽管我们不像在第7章中那样以文件的形式进行增量工作，我们现在是以数据库的形式进行增量工作。以这种方式工作时，我们不太担心耗尽Node.js中可用的内存——一次处理一条记录不应该这样做——尽管这也取决于你的应用程序在同一时间正在做什么其他工作。
- en: '[Listing 8.3](#listing8.3) demonstrates how to create a database cursor and
    traverse the entire data set, sequentially visiting each record. You can run this
    script, but make sure you run it on a database that contains data! Out of this
    box, this script connects to the database on port 7000, which is the prefilled
    database created by the second Vagrant script. Please change the port number to
    6000 if you populated the database yourself from the first Vagrant script or to
    27017 if you’re using a local database that you installed yourself.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表8.3](#listing8.3) 展示了如何创建数据库游标并遍历整个数据集，顺序访问每条记录。你可以运行这个脚本，但请确保你在包含数据的数据库上运行它！默认情况下，这个脚本连接到7000端口的数据库，这是由第二个Vagrant脚本创建的预填充数据库。如果你是从第一个Vagrant脚本中自己填充数据库，请将端口号更改为6000；如果你在使用你自己安装的本地数据库，请将其更改为27017。'
- en: Listing 8.3 Using the database cursor to incrementally traverse the database
    one record at a time (listing-8.3.js)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.3 使用数据库游标逐条记录地增量遍历数据库（listing-8.3.js）
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The database cursor is created with the `find` function. Then by repeatedly
    calling the cursor’s `next` function, we traverse each record in the database.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库游标是通过`find`函数创建的。然后通过反复调用游标的`next`函数，我们可以遍历数据库中的每条记录。
- en: This might seem a little like streaming database access—and indeed it’s a fairly
    simple task to create a Node.js readable stream that reads from MongoDB—however,
    I’ll leave that as a reader exercise. Please feel free to base your code on one
    of the readable streams (CSV or JSON) in chapter 7 and combine it with [listing
    8.3](#listing8.3) to create your own readable MongoDB stream.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来有点像流式数据库访问——实际上，创建一个从MongoDB读取的Node.js可读流是一个相当简单的任务——然而，我将把这个任务留给读者去练习。请随意基于第7章中的一个可读流（CSV或JSON）来编写你的代码，并将其与[列表8.3](#listing8.3)结合，以创建你自己的可读MongoDB流。
- en: '![c08_06.eps](Images/c08_06.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![c08_06.eps](Images/c08_06.png)'
- en: '[Figure 8.6](#figureanchor8.6) A database cursor allows us to visit each record
    in the database one after the other.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.6](#figureanchor8.6) 数据库游标允许我们依次访问数据库中的每条记录。'
- en: 8.7.5 Incremental processing with data windows
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.5 使用数据窗口进行增量处理
- en: Visiting every record in the database one by one is hardly the most efficient
    technique for data access, although at least we can use it to handle a large data
    set. However, we can increase our data throughput by working with multiple records
    at a time instead of a single record at a time. This is still incremental processing,
    but now we’ll use data windows, where each window is a batch of records, rather
    than a single record. After processing each data window, we move the window forward.
    This allows us to sequentially *view* each set of records, as shown in [figure
    8.7](#figure8.7).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 逐个访问数据库中的每条记录并不是数据访问中最有效的方法，尽管至少我们可以用它来处理大量数据。然而，我们可以通过一次处理多条记录而不是单条记录来提高我们的数据吞吐量。这仍然是增量处理，但现在我们将使用数据窗口，其中每个窗口是一批记录，而不是单条记录。处理完每个数据窗口后，我们将窗口向前移动。这使我们能够按顺序**查看**每批记录，如图8.7所示。
- en: '![c08_07.eps](Images/c08_07.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![c08_07.eps](Images/c08_07.png)'
- en: '[Figure 8.7](#figureanchor8.7) Dividing a data set into windows for efficient
    incremental processing'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8.7](#figureanchor8.7) 将数据集划分为窗口以进行高效的增量处理'
- en: We can read a window of data by chaining calls to `skip` and `limit` after calling
    MongoDB’s `find` function. `skip` allows us to skip a number of records; we use
    this to select the starting record in the window. `limit` allows us to retrieve
    only a certain number of records; we can use this to constrain the number of records
    that are in a window. Code for this is shown in [listing 8.4](#listing8.4). You
    can run this code and it will read database records window by window. It doesn’t
    do anything useful though, but it does have a placeholder where you can add your
    own data processing code.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在调用 MongoDB 的 `find` 函数后链式调用 `skip` 和 `limit` 来读取数据窗口。`skip` 允许我们跳过一定数量的记录；我们使用它来选择窗口中的起始记录。`limit`
    允许我们只检索一定数量的记录；我们可以使用它来限制窗口中的记录数量。此代码的示例在 [列表 8.4](#listing8.4) 中。你可以运行此代码，它将逐窗口读取数据库记录。尽管它没有做任何有用的事情，但它有一个占位符，你可以在这里添加自己的数据处理代码。
- en: Listing 8.4 Using data windows to process batches of database records (listing-8.4.js)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.4 使用数据窗口处理数据库记录批量（listing-8.4.js）
- en: '[PRE9]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `readWindow` function in [listing 8.4](#listing8.4) uses the MongoDB API
    to retrieve a window’s worth of data. How many records should we include in each
    window? That’s completely up to you, but you do need to make sure that each window
    fits comfortably in available memory, and that depends on the size of each data
    record and how much memory the other parts of your application are already using.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8.4](#listing8.4) 中的 `readWindow` 函数使用 MongoDB API 来检索一窗口的数据。我们应该在每个窗口中包含多少条记录呢？这完全取决于你，但你确实需要确保每个窗口都能舒适地适应可用内存，而这取决于每条数据记录的大小以及你的应用程序其他部分已经使用的内存量。'
- en: The `readDatabase` function is responsible for traversing the entire database;
    it calls `readWindow` until all data windows have been visited. `readDatabase`
    calls itself repeatedly until the entire database has been divided up into windows
    and processed. This looks like a normal recursive function, but it doesn’t operate
    the same way. That’s because it recurses after the `readWindow` promise has been
    resolved. Due to the way promises work in Node.js, the `then` callback isn’t triggered
    until the next tick of the event loop. The `readDatabase` callstack has exited
    by the time `readDatabase` is called again, and the callstack isn’t growing with
    each new call. Therefore, we’re never in danger of exhausting the stack here as
    we would be if this were a normal recursive function call.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`readDatabase` 函数负责遍历整个数据库；它调用 `readWindow` 直到所有数据窗口都被访问。`readDatabase` 会重复调用自身，直到整个数据库都被划分为窗口并处理。这看起来像是一个正常的递归函数，但它并不以相同的方式操作。这是因为它在
    `readWindow` promise 解决后进行递归。由于 Node.js 中 promises 的工作方式，`then` 回调直到事件循环的下一个 tick
    才被触发。当再次调用 `readDatabase` 时，`readDatabase` 调用栈已经退出，并且调用栈不会随着每个新调用而增长。因此，我们在这里永远不会像在正常递归函数调用中那样面临耗尽栈的危险。'
- en: 'Processing your database using data windows could also be called *pagination*:
    the process of dividing up data for display in multiple pages, typically for display
    across multiple pages of a website. I avoided calling it pagination though, because
    pagination is a different use case even though it would also use MongoDB’s `find`,
    `skip,` and `limit` functions.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据窗口处理数据库也可以称为 *分页*：将数据分割成多个页面以供显示的过程，通常用于在网站的多页面上显示。但我避免将其称为分页，因为尽管它也会使用
    MongoDB 的 `find`、`skip` 和 `limit` 函数，但分页是一个不同的用例。
- en: Here again we could create a readable stream for processing all records in batches,
    and this would be a stream for visiting multiple records at once instead of a
    single record at a time. I’ll leave it as a reader exercise to create such a stream,
    if that sounds useful to you.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们同样可以创建一个可读流来批量处理所有记录，这将是一个一次访问多条记录而不是逐条记录的流。如果这对你来说听起来很有用，我将把它留作读者的练习来创建这样的流。
- en: Processing data in windows allows us to make much more efficient use of the
    data. We can process multiple records at a time, but that’s not the main benefit.
    We now have the fundamentals in place to do parallel processing of our data, an
    idea we’ll return to before the end of the chapter.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在窗口中处理数据使我们能够更有效地利用数据。我们可以一次处理多条记录，但这并不是主要的好处。我们现在已经具备了进行数据并行处理的基础，这个想法我们将在本章结束前再次提及。
- en: 8.7.6 Creating an index
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.6 创建索引
- en: We have yet to look at database queries and sorting. Before we can do that,
    we must create an index for our database. The example query and sort in the following
    sections make use of the *Year* field in the database. To make our queries and
    sorting fast, we should create an index for this field.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尚未查看数据库查询和排序。在我们能够这样做之前，我们必须为我们的数据库创建一个索引。以下部分中的示例查询和排序使用了数据库中的 *Year* 字段。为了使我们的查询和排序快速，我们应该为这个字段创建一个索引。
- en: 'If you’re using the prefilled example database from the second Vagrant script,
    then you already have the index you need. If you started with the empty database
    created by the first Vagrant script or if you have built your own database from
    scratch, you can add the index yourself by opening a MongoDB shell (or Robomongo)
    and entering the following commands:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用第二个 Vagrant 脚本提供的预填充示例数据库，那么您已经拥有了所需的索引。如果您是从第一个 Vagrant 脚本创建的空数据库开始，或者如果您是从头开始构建自己的数据库，您可以通过打开
    MongoDB shell（或 Robomongo）并输入以下命令来自己添加索引：
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: When you’re working with a massive database, it can take significant time to
    create the index, so please be patient and allow it to complete.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当您与大型数据库一起工作时，创建索引可能需要相当长的时间，所以请耐心等待，并允许其完成。
- en: 'To check if an index already exists or if your new index was successfully created,
    you can execute the following commands in the MongoDB shell:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查索引是否已存在或您的新索引是否已成功创建，您可以在 MongoDB shell 中执行以下命令：
- en: '[PRE11]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `getIndexes` function will give you a dump of the indexes that have already
    been created for the collection.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`getIndexes` 函数将为您提供已为集合创建的索引的转储。'
- en: 8.7.7 Filtering using queries
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.7 使用查询进行过滤
- en: When we’re looking for ways to cut down our data so that it can fit in memory,
    one option we have is to use a filter. We can filter our data through a database
    query to significantly cut down the amount of data we are working with. For instance,
    we might only be interested in analyzing more recent data, so in this example
    we request that the database return only those records from the year 2016 or later.
    The result is a set of records where all the records prior to 2016 have been omitted,
    leaving us with only the recent records. This concept is illustrated in [figure
    8.8](#figure8.8).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在寻找方法来减少我们的数据以便它能够适应内存时，我们有一个选项是使用过滤器。我们可以通过数据库查询过滤我们的数据，从而显著减少我们正在处理的数据量。例如，我们可能只对分析更近期的数据感兴趣，因此在这个例子中，我们要求数据库只返回
    2016 年或之后的记录。结果是，所有在 2016 年之前的记录都被省略，只留下最近的记录。这一概念在[图 8.8](#figure8.8) 中得到了说明。
- en: The idea here is that we’re proactively culling data that we don’t need, so
    we can work with a significantly reduced data set. In [listing 8.5](#listing8.5)
    we’re using MongoDB’s `$gte` (greater than or equal to) query operator on the
    Year field to filter out records prior to 2016\. You can run [listing 8.5](#listing8.5)
    and the query should execute quickly (because of the index for the Year field)
    and print records from 2016 and after to the console.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是我们正在积极删除我们不需要的数据，这样我们就可以与一个显著减少的数据集一起工作。在[列表 8.5](#listing8.5) 中，我们正在使用
    MongoDB 的 `$gte`（大于或等于）查询运算符在 Year 字段上过滤掉 2016 年之前的记录。您可以运行[列表 8.5](#listing8.5)，查询应该会快速执行（因为
    Year 字段的索引），并将 2016 年及以后的记录打印到控制台。
- en: Listing 8.5 Filtering data with a database query (listing-8.5.js)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.5 使用数据库查询过滤数据（listing-8.5.js）
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note in [listing 8.5](#listing8.5) how we define a query object and pass that
    to the `find` function. This is one example of how we can build a query in MongoDB
    to retrieve filtered records from the database. MongoDB supports flexible and
    complex queries, but you have more to learn that’s outside the scope of this book.
    Please see the MongoDB documentation to understand what other types of expressions
    you can use in your queries.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在[列表 8.5](#listing8.5) 中我们如何定义查询对象并将其传递给 `find` 函数。这是我们在 MongoDB 中构建查询以从数据库中检索过滤记录的一个示例。MongoDB
    支持灵活和复杂的查询，但您还有更多需要学习的内容，这些内容超出了本书的范围。请参阅 MongoDB 文档以了解您可以在查询中使用哪些其他类型的表达式。
- en: You should also note that any other time we previously used the `find` function—for
    example, in the earlier sections on incremental processing of records and data
    windows—we could also have used a query to filter the data we were looking at.
    Queries also work with projection and sorting, as we’ll see in the next two sections.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 您还应该注意，我们之前在处理记录增量处理和数据窗口的早期部分中使用的任何其他 `find` 函数——例如——我们也可以使用查询来过滤我们正在查看的数据。查询也可以与投影和排序一起使用，正如我们将在下一两个部分中看到的。
- en: '![c08_08.eps](Images/c08_08.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![c08_08.eps](Images/c08_08.png)'
- en: '[Figure 8.8](#figureanchor8.8) Filter data with a database query.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.8](#figureanchor8.8) 使用数据库查询过滤数据。'
- en: 8.7.8 Discarding data with projection
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.8 使用投影删除数据
- en: Another way to cut down the data we’re working with is through a projection.
    A projection allows us to discard fields from the records that are returned for
    our queries. [Figure 8.9](#figure8.9) shows an example of fields being discarded
    and only allowing those fields that we want to retrieve to be returned for a query.
    In this example, we’re choosing to retrieve only the fields *Year*, *Month,* and
    *Precipitation*. This is useful when we require only certain data fields—say we’re
    doing a study of rainfall—and we don’t need to retrieve all the fields of the
    full data set.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种减少我们处理的数据量的方法是使用投影。投影允许我们从返回给查询的记录中删除字段。[图8.9](#figure8.9) 展示了一个示例，其中删除了某些字段，并且只返回我们想要检索的字段。在这个例子中，我们选择只检索*年份*、*月份*和*降水量*字段。当我们只需要某些数据字段时，这非常有用——比如说我们正在进行降雨研究，我们不需要检索整个数据集的所有字段。
- en: As you can see in the following listing, we specify a projection through the
    `find` function, so we can attach our projection onto any other query. If you
    run this code, it will print the retrieved data records to the console, but only
    with the fields that we selected in the projection.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在下面的列表中可以看到的，我们通过`find`函数指定了一个投影，因此我们可以将投影附加到任何其他查询上。如果你运行这段代码，它将打印检索到的数据记录到控制台，但只包含我们在投影中选择的字段。
- en: Listing 8.6 Cutting down on data retrieved using a projection (listing-8.6.js)
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.6 使用投影减少检索数据（listing-8.6.js）
- en: '[PRE13]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Projection allows us to reduce the size of each record and therefore reduce
    the total size of the data set that we retrieve from a query. Not only does this
    increase the number of records we can fit in memory (because each one is smaller),
    but it also reduces the bandwidth required to retrieve a set of records when we’re
    accessing our database over the internet.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 投影允许我们减小每条记录的大小，从而减小我们从查询中检索的数据集的总大小。这不仅增加了我们可以在内存中容纳的记录数量（因为每条记录都更小），而且在我们通过互联网访问数据库时，也减少了检索一组记录所需的带宽。
- en: '![c08_09.eps](Images/c08_09.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![c08_09.eps](Images/c08_09.png)'
- en: '[Figure 8.9](#figureanchor8.9) Using a projection to discard data from each
    database record'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.9](#figureanchor8.9) 使用投影从每个数据库记录中删除数据'
- en: 8.7.9 Sorting large data sets
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.9 对大数据集进行排序
- en: Sorting is a useful and often necessary operation. Most sorting algorithms,
    for example, the built-in JavaScript `sort` function, require that we fit our
    entire data set into memory. When working with a data set that doesn’t fit in
    memory, we can use our database to do the sorting for us ([figure 8.10](#figure8.10)).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 排序是一个有用且通常是必要的操作。例如，大多数排序算法，比如内置的JavaScript `sort`函数，都需要我们将整个数据集放入内存。当我们处理不适合内存的数据集时，我们可以使用数据库来为我们进行排序([图8.10](#figure8.10))。
- en: '![c08_10.eps](Images/c08_10.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![c08_10.eps](Images/c08_10.png)'
- en: '[Figure 8.10](#figureanchor8.10) Normally, when sorting, all data must fit
    in memory.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.10](#figureanchor8.10) 通常，在排序时，所有数据都必须适合内存。'
- en: In the following listing, we’re finding and sorting records by the Year field.
    This will be efficient because we already have an index for the Year field. You
    can run this code, and it will print sorted data records to the console.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的列表中，我们通过年份字段来查找和排序记录。这将非常高效，因为我们已经为年份字段建立了索引。你可以运行这段代码，它将打印排序后的数据记录到控制台。
- en: Listing 8.7 Sorting a large data set with MongoDB (listing-8.7.js)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.7 使用MongoDB对大数据集进行排序（listing-8.7.js）
- en: '[PRE14]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: See how the `sort` function is chained after the `find` function. In this example,
    we haven’t passed any parameters to the `find` function, but we could as easily
    have specified both a query and a projection to cut down the data before sorting
    it.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 看看`sort`函数是如何在`find`函数之后链式调用的。在这个例子中，我们没有向`find`函数传递任何参数，但我们同样可以指定一个查询和一个投影，在排序之前减小数据量。
- en: Note also the use of `toArray` chained after the `sort` function. This returns
    the entire sorted data set, but with a big data set that’s probably not what we
    wanted. We can easily drop the `toArray` function and instead do record-by-record
    processing using a database cursor the way we did earlier. Alternatively, we can
    keep the `toArray` function and combine it with `skip` and `limit` and instead
    do the window-by-window processing from earlier. These techniques all revolve
    around the `find` function, and they fit together to help us work with huge data
    sets.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意在`sort`函数之后链式使用`toArray`。这会返回整个排序后的数据集，但对于大数据集来说，这可能不是我们想要的。我们可以轻松地删除`toArray`函数，而是使用数据库游标按记录逐个处理，就像我们之前做的那样。或者，我们可以保留`toArray`函数，并将其与`skip`和`limit`结合使用，从而从之前的方法中执行窗口处理。所有这些技术都围绕`find`函数展开，它们结合在一起帮助我们处理大量数据集。
- en: One final thought on sorting. I think it’s always a good idea to work on sorted
    data. Why is that? Because when processing a large set of data, it’s best to have
    it in a dependable order. Otherwise, your records are going to be returned in
    whatever order the database wants, which isn’t necessarily the best for you. Having
    sorted data makes debugging easier. It makes reasoning about data problems easier.
    It also makes for a useful progress indicator! For example, when you can see that
    the As, Bs, Cs, and Ds are done, you have a fair idea of what’s left to process
    and how long it might take.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 关于排序的最后一点思考。我认为始终处理排序后的数据是一个好主意。为什么？因为当处理大量数据时，最好有一个可靠的顺序。否则，你的记录将以数据库想要的任何顺序返回，这对你来说可能不是最好的。有排序的数据使得调试更容易。它使得对数据问题的推理更容易。它还提供了一个有用的进度指示器！例如，当你看到A、B、C和D都已完成时，你就有了一个很好的想法，知道还剩下多少要处理，以及可能需要多长时间。
- en: 8.8 Achieving better data throughput
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.8 提高数据吞吐量
- en: We’ve learned how we can use a database to more effectively manage our large
    data set. Now let’s look at techniques we can use to increase our data throughput.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了如何使用数据库更有效地管理我们的大数据集。现在让我们看看我们可以使用的技巧来提高我们的数据吞吐量。
- en: 8.8.1 Optimize your code
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.8.1 优化你的代码
- en: 'The obvious advice for better performance is: *optimize your code*. Mostly
    this is beyond the scope of the book, and plenty of information is out there on
    how to optimize JavaScript code. For example, don’t use the `forEach` function
    in performance-sensitive code; instead, use a regular `for` loop.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 提高性能的明显建议是：*优化你的代码*。大多数情况下，这超出了本书的范围，关于如何优化JavaScript代码的信息有很多。例如，在性能敏感的代码中不要使用`forEach`函数；相反，使用常规的`for`循环。
- en: 'I will, however, give you two important pieces of advice when it comes to code
    optimization that will help you be more productive:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当涉及到代码优化时，我会给你两条重要的建议，这有助于你提高生产力：
- en: Focus on the bottleneck. Time your code and measure the length of time it takes
    to run using a library such as `statman-stopwatch`. Focus on the code that takes
    the most time. If you spend time optimizing code that isn’t a bottleneck, you’re
    wasting your time because it won’t make any difference to your data throughput.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 专注于瓶颈。使用像`statman-stopwatch`这样的库来计时你的代码，并测量运行所需的时间长度。专注于耗时最长的代码。如果你花时间优化不是瓶颈的代码，你就是在浪费时间，因为这不会对你的数据吞吐量产生任何影响。
- en: Don’t focus on the code, focus on the algorithm.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不要专注于代码，而要专注于算法。
- en: 8.8.2 Optimize your algorithm
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.8.2 优化你的算法
- en: Carefully consider the algorithm you’re using. Selecting an algorithm that’s
    more appropriate to the task will give you a much bigger performance boost than
    if you’re focusing on your code. For example, when you need to do a fast lookup,
    make sure you’re using a JavaScript hash and not an array. This is just one simple
    and obvious example.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细考虑你使用的算法。选择一个更适合任务的算法将比专注于你的代码给你带来更大的性能提升。例如，当你需要快速查找时，确保你使用的是JavaScript散列而不是数组。这只是一个简单且明显的例子。
- en: In general, though, algorithms are a large field and a topic of study unto themselves
    (search for *Big O notation* if you want to follow up on this). But before we
    end this chapter, let’s look at one particular method that can pay huge performance
    dividends when dealing with a large data set.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，总的来说，算法是一个很大的领域，也是一个独立的研究主题（如果你想跟进这个话题，可以搜索*大O表示法*）。但在结束这一章之前，让我们看看一种在处理大量数据集时可以带来巨大性能收益的特定方法。
- en: 8.8.3 Processing data in parallel
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.8.3 并行处理数据
- en: Node.js is inherently single-threaded. That can be a good thing because generally
    we can code away without concerns such as thread safety and locking. In terms
    of performance, Node.js normally makes up for its lack of threads by bringing
    asynchronous coding to the front and center. But still, running only a single
    thread can be an inefficient use of your CPU when you have multiple cores that
    you could otherwise throw at the problem.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Node.js本质上是单线程的。这可以是一件好事，因为通常我们可以编写代码而无需担心线程安全或锁定。在性能方面，Node.js通常通过将异步编程置于首位和中心来弥补其线程不足。但是，当你有多个核心可以用来解决问题时，只运行一个线程可能会浪费你的CPU。
- en: In this section, we’ll look at how we can divide up our data and process it
    in parallel using separate operating system processes that make use of multiple
    cores and can process batches of data simultaneously. This an extension of “Divide
    and conquer” from earlier and builds on “Incremental processing with data windows.”
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨如何通过使用多个操作系统能够利用多个核心并能够同时处理数据批次的单独操作系统进程来并行划分我们的数据并处理。这是“分而治之”的扩展，并建立在“使用数据窗口的增量处理”之上。
- en: You can see how this works in [figure 8.11](#figure8.11). We have one *master*
    process that controls two or more *slave* processes. We divide our data set into
    two or more separate data windows. Each slave is responsible for processing a
    single data window, and multiple slaves can operate simultaneously on multiple
    data windows using separate CPU cores.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图8.11](#figure8.11)中看到这是如何工作的。我们有一个*主*进程，它控制两个或更多的*奴隶*进程。我们将数据集划分为两个或更多的单独数据窗口。每个奴隶负责处理单个数据窗口，并且多个奴隶可以使用单独的CPU核心同时操作多个数据窗口。
- en: '![c08_11.eps](Images/c08_11.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![c08_11.eps](Images/c08_11.png)'
- en: '[Figure 8.11](#figureanchor8.11) Using multiple OS processes to work on data
    in parallel'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.11](#figureanchor8.11) 使用多个操作系统进程并行处理数据'
- en: 'Unfortunately, this kind of application structure makes our application much
    more complex, and the complexity rises with the number of slave processes that
    we add. When we add complexity to our applications, we should ensure that it’s
    for a good reason. In this case, we’re doing it for two reasons:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这种应用程序结构使我们的应用程序变得更加复杂，并且随着我们添加的奴隶进程数量的增加，复杂性也会上升。当我们向应用程序添加复杂性时，我们应该确保这是出于一个很好的原因。在这种情况下，我们这样做有两个原因：
- en: We can work on the data in parallel, and this increases our overall data throughput.
    We stand to increase our throughput by the number of slaves we’re running. If
    we have 8 × slaves (on an 8-core CPU), we stand to increase our throughput × 8.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以并行处理数据，这增加了我们的整体数据吞吐量。我们可以通过运行的奴隶数量来增加我们的吞吐量。如果我们有8倍的奴隶（在8核CPU上），我们有望将吞吐量增加8倍。
- en: Slightly less obvious is the fact that each slave process is operating in its
    own memory space. This increases the amount of memory we have to work with by
    the number of slave processes. With 8 slave processes, we’d have 8 × memory.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个稍微不那么明显的事实是，每个奴隶进程都在自己的内存空间中运行。这通过奴隶进程的数量增加了我们可用的内存量。如果有8个奴隶进程，我们就有8倍的内存。
- en: To get more throughput and more memory, we can add more slaves. This has its
    limits, though, because increasing the number of slaves beyond the number of physical
    CPU cores has diminishing returns.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更高的吞吐量和更多的内存，我们可以添加更多的奴隶。然而，这有其局限性，因为当奴隶的数量超过物理CPU核心的数量时，收益递减。
- en: In practice, we can empirically adjust the number of slaves to consume an appropriate
    percentage of our CPU time. We may not want to dedicate 100% of our CPU time to
    this because that can impact the performance of other applications on the computer
    and even make it run hot and become unstable.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们可以通过经验调整奴隶的数量，以消耗我们CPU时间的适当百分比。我们可能不想将100%的CPU时间都用于此，因为这可能会影响计算机上其他应用程序的性能，甚至使其过热并变得不稳定。
- en: Also, you should have enough physical memory to support the number of slaves
    and the amount of memory they’ll consume. Exhausting your physical memory can
    be counterproductive because your application will start *thrashing* as data is
    swapped between working memory and the file system.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你应该有足够的物理内存来支持奴隶的数量以及它们将消耗的内存量。耗尽你的物理内存可能会适得其反，因为当数据在工作内存和文件系统之间交换时，你的应用程序将开始*颠簸*。
- en: How do we implement this? It’s not as difficult as you might think. First, I’m
    going to show you how I tackle this problem by running separate Node.js commands
    in parallel. Then I’ll explain how everyone else does it using the Node.js `fork`
    function.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何实现这个？这不像你想象的那么困难。首先，我将向你展示我是如何通过并行运行单独的Node.js命令来解决这个问题。然后我会解释其他人如何使用Node.js的`fork`函数来做这件事。
- en: Executing separate commands in parallel
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 并行执行单独的命令
- en: Let’s look at how to implement parallel processing with a simplified example.
    This can get complicated, so to keep the example simple, we won’t do any actual
    data processing, and we’ll visit our data windows in parallel. But you’ll see
    a placeholder where you can add your own data processing code. It can be a reader
    exercise to later add data processing to this framework.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简化的例子来看看如何实现并行处理。这可能会变得复杂，所以为了保持例子简单，我们不会进行任何实际的数据处理，而是并行访问我们的数据窗口。但你会看到一个占位符，你可以在这里添加你自己的数据处理代码。这可以作为一个练习，稍后添加数据处理到这个框架中。
- en: 'For this example, we need to have *yargs* installed for reading command-line
    parameters and also a module called *async-await-parallel* that we’ll discuss
    soon. If you installed dependencies for the chapter 8 repository, you’ll have
    these installed already; otherwise, you can install them in a fresh Node.js project
    as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们需要安装*yargs*来读取命令行参数，还需要一个名为*async-await-parallel*的模块，我们很快会讨论它。如果你为第
    8 章的存储库安装了依赖项，那么这些依赖项已经安装好了；否则，你可以在一个新的Node.js项目中安装它们，如下所示：
- en: '[PRE15]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: My approach is presented in listings 8.8 and 8.9\. The first script, [listing
    8.8](#listing8.8), is the slave process. This script operates on a single data
    window similar to what we saw earlier in “Incremental processing with data windows.”
    The position and size of the data window are passed to the script using the `skip`
    and `limit` command-line parameters. Look over this script before we move onto
    [listing 8.9](#listing8.9) and note in the function `processData` a line where
    you can insert your own data processing code (or insert a call to one of your
    reusable data processing code modules from earlier chapters).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我的方法在列表 8.8 和 8.9 中展示。第一个脚本[列表 8.8](#listing8.8)是从脚本。这个脚本在一个单独的数据窗口上操作，类似于我们在“使用数据窗口进行增量处理”中看到的。数据窗口的位置和大小通过`skip`和`limit`命令行参数传递给脚本。在我们转到[列表
    8.9](#listing8.9)之前，查看这个脚本，并在`processData`函数中注意可以插入你自己的数据处理代码的行（或者插入一个调用你之前章节中可重用数据处理代码模块的调用）。
- en: Listing 8.8 The slave process that does the work in parallel (listing-8.8.js)
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.8 并行工作的从进程（listing-8.8.js）
- en: '[PRE16]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now let’s look at the master script in [listing 8.9](#listing8.9). This script
    invokes the slave script in [listing 8.8](#listing8.8) to do the actual work.
    It will fire off two slaves at a time, wait until they finish, and then fire off
    the next two slaves. It continues running slaves in groups of two until the entire
    database has been processed. I set the number of slaves at two to keep things
    simple. When you run this code for yourself, you should try tuning the `maxProcesses`
    variable to fit the number of cores you have available for data processing.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看主脚本[列表 8.9](#listing8.9)。这个脚本调用[列表 8.8](#listing8.8)中的从脚本来完成实际工作。它一次会启动两个从脚本，等待它们完成，然后启动下一组两个从脚本。它会继续以两组两个的方式运行从脚本，直到整个数据库都被处理。我设置从脚本的数量为两个以保持事情简单。当你自己运行这段代码时，你应该尝试调整`maxProcesses`变量以适应你可用于数据处理的核心数量。
- en: Listing 8.9 The master process that coordinates the slave processes (listing-8.9.js)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.9 协调从进程的主进程（listing-8.9.js）
- en: '[PRE17]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[Listing 8.9](#listing8.9) starts by calling `find().count()` on the database
    collection to determine how many records it contains. It then divides the database
    into data windows. For each window, it calls `processBatch`. This has the unusual
    behavior of creating and returning an anonymous function that wraps up a call
    to `runSlave`. I’ll explain that in a moment.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8.9](#listing8.9)首先在数据库集合上调用`find().count()`来确定它包含多少条记录。然后它将数据库划分为数据窗口。对于每个窗口，它调用`processBatch`。这个函数有一个不寻常的行为，就是创建并返回一个匿名函数，该函数封装了对`runSlave`的调用。我稍后会解释这一点。'
- en: '`runSlave` is the function that starts the slave process. Here we use the Node.js
    `spawn` function to create a new process. We’re invoking Node.js to run the slave
    script that we saw in [listing 8.8](#listing8.8). Note the `skip` and `limit`
    command-line parameters that are being passed to the slave. These tell the slave
    which data window it must process.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`runSlave`是启动奴隶进程的函数。在这里，我们使用Node.js的`spawn`函数创建一个新的进程。我们正在调用Node.js来运行我们在[列表8.8](#listing8.8)中看到的奴隶脚本。注意传递给奴隶的`skip`和`limit`命令行参数。这些告诉奴隶必须处理哪个数据窗口。'
- en: After `processBatch` has been called for every window, we now have a list of
    functions that when executed will invoke `runSlave` for each batch of data. We
    need this kind of deferred action to use with the `parallel` function that we’re
    using from the async-await-parallel library.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在为每个窗口调用`processBatch`之后，我们现在有一个函数列表，当执行这些函数时，将为每批数据调用`runSlave`。我们需要这种延迟操作来与我们从async-await-parallel库中使用的`parallel`函数一起使用。
- en: We pass to `parallel` our list of functions and the number of operations to
    execute in parallel. `parallel` does the hard work for us, invoking our deferred
    functions in parallel batches until all have been executed. `parallel` returns
    a promise that’s resolved when the entire sequence has completed or otherwise
    rejected if any of the individual operations have failed.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的函数列表和要并行执行的操作数量传递给`parallel`。`parallel`为我们做艰苦的工作，并行批处理调用我们的延迟函数，直到所有函数都执行完毕。`parallel`返回一个当整个序列完成时解决的承诺，或者在任何单个操作失败时拒绝。
- en: Forking a new process
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建新的进程
- en: We learned one way of doing parallel data processing in Node.js, but we also
    have a simpler way to build a master/slave type application in Node.js, and that’s
    using the `fork` function. This is the technique that you’ll find most often when
    searching the internet on this topic.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了在Node.js中执行并行数据处理的一种方法，但我们还有在Node.js中构建主/从类型应用的一个更简单的方法，那就是使用`fork`函数。这是你在搜索互联网上这个主题时最常找到的技术。
- en: We start our application with a single process and then we call `fork` for as
    many slaves as we need. The `fork` function causes our process to branch into
    two processes, and then our code is running in either the master or the slave.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以单个进程开始我们的应用程序，然后我们为所需的奴隶调用`fork`。`fork`函数使我们的进程分支成两个进程，然后我们的代码在主进程或奴隶进程中运行。
- en: Why not use the `fork` function if it’s simpler than running separate commands?
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果运行单独的命令比运行单独的命令更简单，为什么不使用`fork`函数呢？
- en: 'Here are a few reasons I prefer my own approach to this:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我更喜欢自己的方法的一些原因：
- en: Running separate commands is more explicit, and it’s easier to ensure the slaves
    are operating in parallel.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行单独的命令更加明确，并且更容易确保奴隶进程并行运行。
- en: You have a clean separation between master and slave. You are either in the
    master script or the slave script.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主从进程之间有清晰的分离。你要么在主脚本中，要么在奴隶脚本中。
- en: It makes the slave easy to test. Because you can run the slave from the command
    line, you can easily run it this way for testing and debugging.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这使得奴隶进程易于测试。因为你可以从命令行运行奴隶进程，所以你可以轻松地以这种方式进行测试和调试。
- en: I believe it makes the code more reusable. Decomposing your application into
    multiple scripts means you have a master that can potentially (with a little refactoring)
    be used with different slaves. Also, you have separate slave scripts that can
    potentially be used in other ways and in other circumstances.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我认为这使代码更具可重用性。将你的应用程序分解成多个脚本意味着你有一个主进程，它可以潜在地（经过一些重构）与不同的奴隶一起使用。此外，你还有可以潜在地在其他方式和情况下使用的单独的奴隶脚本。
- en: It works with more than Node.js scripts. You might have other tools you want
    to run, and the master can run those as easily as it can run Node.js and your
    slave script.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以与Node.js脚本以外的脚本一起工作。你可能还有其他想要运行的工具，主进程可以像运行Node.js和你的奴隶脚本一样轻松地运行这些工具。
- en: The end result between the two approaches is much the same; we get to process
    our data in parallel. Using `fork` is the simpler alternative. Running separate
    commands is more difficult, but not by a large amount and has the benefits that
    I outlined. Pick the method that most suits you.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 两种方法最终的结果几乎相同；我们能够并行处理我们的数据。使用`fork`是一个更简单的替代方案。运行单独的命令更困难，但并不太多，并且具有我概述的好处。选择最适合你的方法。
- en: Through this chapter and the last, we wrangled a massive data set. We took it
    from a huge CSV file and imported it into our database. We’re now armed to the
    teeth with an array of techniques for building data pipelines, cleaning, and transforming
    our data—and now our techniques can be scaled to huge data sets. We’re finally
    ready for some data analysis! Bring on chapter 9!
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章和上一章，我们处理了一个庞大的数据集。我们从一个大型的CSV文件中提取数据并将其导入我们的数据库。现在我们武装了各种构建数据管道、清理和转换数据的技术，并且现在我们的技术可以扩展到庞大的数据集。我们终于准备好进行一些数据分析了！第九章，我们来了！
- en: Summary
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: We discussed how the memory limitations of Node.js constrain the amount of data
    you can fit in memory at any one time.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们讨论了Node.js的内存限制如何限制在任何时候可以放入内存中的数据量。
- en: You explored various techniques for working with a large database, including
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你探索了与大型数据库一起工作的各种技术，包括
- en: How to move a large CSV data file to your MongoDB database
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将大型CSV数据文件移动到MongoDB数据库
- en: How to divide your data into batches, where each batch fits in memory and can
    be processed separately
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将数据分成批次，其中每个批次都适合内存并且可以单独处理
- en: Using a cursor or data window to incrementally process the entire database
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用游标或数据窗口来逐步处理整个数据库
- en: Using queries and projections to reduce your data
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用查询和投影来减少数据
- en: Using the database to sort your data—an operation that's otherwise difficult
    when your data doesn't fit into memory
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据库来排序数据——当数据不适合内存时，这是一个通常难以进行的操作
- en: You worked through an example of spawning multiple operating system processes
    to do parallel processing of our data and increase the throughput of our data
    pipeline.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你通过一个例子学习了如何通过生成多个操作系统进程来并行处理我们的数据，从而提高数据管道的吞吐量。
