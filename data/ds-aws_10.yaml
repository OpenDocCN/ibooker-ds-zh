- en: Chapter 10\. Pipelines and MLOps
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章 管道和MLOps
- en: In previous chapters, we demonstrated how to perform each individual step of
    a typical ML pipeline, including data ingestion, analysis, and feature engineering—as
    well as model training, tuning, and deploying.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们演示了如何执行典型ML管道的每个单独步骤，包括数据摄取、分析和特征工程，以及模型训练、调优和部署。
- en: In this chapter, we tie everything together into repeatable and automated pipelines
    using a complete machine learning operations (MLOps) solution with SageMaker Pipelines.
    We also discuss various pipeline-orchestration options, including AWS Step Functions,
    Kubeflow Pipelines, Apache Airflow, MLFlow, and TensorFlow Extended (TFX).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将所有内容结合到可重复和自动化的管道中，使用完整的机器学习运营（MLOps）解决方案与SageMaker管道。我们还讨论了各种管道编排选项，包括AWS
    Step Functions、Kubeflow Pipelines、Apache Airflow、MLFlow和TensorFlow Extended（TFX）。
- en: We will then dive deep into automating our SageMaker Pipelines when new code
    is committed, when new data arrives, or on a fixed schedule. We describe how to
    rerun a pipeline when we detect statistical changes in our deployed model, such
    as data drift or model bias. We will also discuss the concept of human-in-the-loop
    workflows, which can help to improve our model accuracy.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将深入探讨如何在新代码提交时、新数据到达时或按固定时间表自动化我们的SageMaker管道。我们描述了如何在检测到部署模型中的统计变化（如数据漂移或模型偏差）时重新运行管道。我们还将讨论人在环回工作流的概念，这可以帮助提高我们模型的准确性。
- en: Machine Learning Operations
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习运营
- en: The complete model development life cycle typically requires close collaboration
    between the application, data science, and DevOps teams to successfully productionize
    our models, as shown in [Figure 10-1](#productionizing_machine_learning_applic).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的模型开发生命周期通常需要应用程序、数据科学和DevOps团队之间的密切协作，以成功地将我们的模型产品化，如[图10-1](#productionizing_machine_learning_applic)所示。
- en: '![](assets/dsaw_1001.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1001.png)'
- en: Figure 10-1\. Productionizing machine learning applications requires collaboration
    between teams.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1\. 将机器学习应用产品化需要各团队之间的协作。
- en: Typically, the data scientist delivers the trained model, the DevOps engineer
    manages the infrastructure that hosts the model as a REST API, and the application
    developer integrates the REST API into their applications. Each team must understand
    the needs and requirements of the other teams in order to implement an efficient
    workflow and smooth hand-off process.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，数据科学家提供训练好的模型，DevOps工程师管理托管模型的基础设施作为REST API，并且应用开发者将REST API集成到他们的应用程序中。每个团队必须了解其他团队的需求和要求，以实施高效的工作流程和顺畅的交接流程。
- en: 'MLOps has evolved through three stages of maturity:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps已经发展到了三个成熟阶段：
- en: MLOps v1.0
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps v1.0
- en: Manually build, train, tune, and deploy models
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 手动构建、训练、调优和部署模型
- en: MLOps v2.0
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps v2.0
- en: Manually build and orchestrate model pipelines
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 手动构建和编排模型管道
- en: MLOps v3.0
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps v3.0
- en: Automatically run pipelines when new data arrives or code changes from deterministic
    triggers such as GitOps or when models start to degrade in performance based on
    statistical triggers such as drift, bias, and explainability divergence
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当新数据到达或代码发生变化时，自动运行管道，这些变化可以是确定性触发器，如GitOps，或者是基于统计触发器，如漂移、偏差和可解释性差异，当模型性能开始下降时。
- en: In this chapter, we describe how SageMaker supports the complete MLOps strategy,
    including pipeline orchestration, deterministic automation from changes in data
    or code, and statistical automation from changes in drift, bias, or explainability.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们描述了SageMaker如何支持完整的MLOps策略，包括管道编排、从数据或代码变化的确定性自动化，以及从漂移、偏差或可解释性变化的统计自动化。
- en: Software Pipelines
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 软件管道
- en: In the early 2000s, software practitioners started to use continuous integration
    (CI) and continuous delivery (CD) to automatically build, test, and deploy their
    software modules directly and safely to production. CI and CD facilitated a low-friction
    collaboration between the DevOps engineers and software engineers. Prior to CI
    and CD, software engineers would hand their code “over the wall” to the DevOps
    engineer, who pushed the software to production after confirming successful integration
    test results in preproduction staging environments and coordinating with quality
    assurance (QA) teams, etc. An example software pipeline is shown in [Figure 10-2](#simple_application_deployment_pipeline).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 2000年代初期，软件从业者开始使用持续集成（CI）和持续交付（CD）来直接、安全地构建、测试和部署他们的软件模块到生产环境。CI和CD促进了DevOps工程师与软件工程师之间低摩擦的协作。在CI和CD之前，软件工程师会将他们的代码“扔过墙”给DevOps工程师，在预生产环境中确认成功的集成测试结果，并与质量保证（QA）团队协调等后，将软件推送到生产环境。示例软件管道如[Figure 10-2](#simple_application_deployment_pipeline)所示。
- en: '![](assets/dsaw_1002.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1002.png)'
- en: Figure 10-2\. Simple application deployment pipeline.
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-2\. 简单的应用部署管道。
- en: Jenkins is a popular open source tool for managing software pipelines. With
    its rich plug-in architecture, Jenkins can orchestrate complex CI/CD software
    pipelines and provide in-depth reports on the health of the pipeline at any point
    during the pipeline execution. For large code bases, pipeline execution can span
    many days, and components can fail for a variety of reasons. Jenkins provides
    mechanisms to restart any failed components and keep the pipeline running. Human
    intervention is often required, however. Jenkins supports manual, human-in-the-loop
    feedback as well.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Jenkins是一个流行的开源工具，用于管理软件管道。凭借其丰富的插件架构，Jenkins可以编排复杂的CI/CD软件管道，并在管道执行的任何阶段提供深入的报告。对于大型代码库，管道执行可能需要几天时间，并且组件可能因各种原因而失败。Jenkins提供机制来重新启动任何失败的组件并保持管道运行。然而，通常需要人工干预。Jenkins还支持手动、人为反馈环节。
- en: In addition to restarts, sophisticated pipeline orchestration engines such as
    Jenkins support component caching strategies as well to improve pipeline execution
    performance. For example, if our pipeline fails during the integration test step
    because a remote system is unavailable, the orchestration engine can detect which
    pipeline steps have already run, reuse the cached results if no dependencies have
    changed, retry the failed step, and continue the pipeline to completion.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 除了重新启动外，像Jenkins这样的高级管道编排引擎还支持组件缓存策略，以提高管道执行性能。例如，如果我们的管道在集成测试阶段失败，因为远程系统不可用，编排引擎可以检测哪些管道步骤已经运行，如果没有更改依赖项，则重用缓存的结果，重试失败的步骤，并继续执行管道直至完成。
- en: Machine Learning Pipelines
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习管道
- en: While CI and CD pipelines were built primarily to automate the software development
    cycle and improve the quality of application releases, they can also improve machine
    learning releases. ML engineers and data scientists seek to consistently and repeatedly
    train, test, and deploy models into production with little friction. This lets
    us spend more time on building and experimenting with new models versus manually
    retraining and redeploying existing models with the latest datasets.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然CI和CD管道主要用于自动化软件开发周期并提高应用发布的质量，它们也可以改进机器学习的发布过程。机器学习工程师和数据科学家致力于以最小的摩擦度一致地训练、测试和部署模型到生产环境中。这使我们能够花更多时间构建和尝试新模型，而不是手动重新训练和重新部署使用最新数据集的现有模型。
- en: Similar to CI and CD to efficiently update and improve software in production,
    machine learning pipelines automatically perform continuous training and CD for
    machine learning to efficiently update and improve models in production. Automated,
    reproducible, and parameterized pipelines help to maintain and track framework
    versions, container runtimes, and hardware throughout the entire process, from
    feature ingestion and feature engineering to model training and deployment.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于CI和CD以高效地更新和改进生产中的软件，机器学习管道自动执行持续训练和持续交付，以高效地更新和改进模型。自动化、可复制和参数化的管道有助于在整个过程中维护和跟踪框架版本、容器运行时和硬件，从特征摄取和特征工程到模型训练和部署。
- en: Using automated ML pipelines instead of manual, one-off Python scripts will
    help to reduce subtle bugs that may creep into any step of the pipeline. For example,
    small changes to an upstream application may introduce data-quality issues such
    as star ratings outside the bounded and discrete value range between 1 (worst)
    and 5 (best).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自动化 ML 流水线而不是手动的一次性 Python 脚本，有助于减少可能出现在流水线任何步骤中的细微错误。例如，上游应用程序的小改变可能引入数据质量问题，例如星级评分超出了介于
    1（最差）和 5（最佳）之间的边界和离散值范围。
- en: While the model may appear to train successfully with poor-quality data, the
    model could negatively affect our business if pushed to production. By automating
    the data-quality checks before model training, we could raise a pipeline exception,
    notify the application team of the bad data, and save the cost of training a bad
    model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然模型可能在质量低下的数据上看起来训练成功，但如果推送到生产环境，模型可能会对我们的业务产生负面影响。通过在模型训练之前自动化数据质量检查，我们可以引发流水线异常，通知应用团队有关坏数据的情况，并节省训练糟糕模型的成本。
- en: We can also combine ML pipelines with artifact and experiment tracking for model
    reproducibility and auditing. Artifact tracking provides the lineage of deployed
    models all the way back to the original dataset version used during model training.
    Experiment tracking records the hyper-parameters used during training as well
    as the training results, such as model accuracy. The SageMaker Experiments and
    Lineage APIs are integrated throughout SageMaker to handle these scenarios.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将 ML 流水线与模型可复现性和审计的工件和实验跟踪结合起来。工件跟踪提供了部署模型的血统，直到模型训练期间使用的原始数据集版本。实验跟踪记录了训练过程中使用的超参数以及训练结果，如模型准确度。SageMaker
    实验和血统 API 在整个 SageMaker 中集成，以处理这些情况。
- en: Verifiable ML pipelines can help solve the problem of model degradation. Model
    degradation is a relatively common and underengineered scenario due to the complexity
    of monitoring models in production. Degrading model predictions results in poorly
    classified reviews and missed business opportunities.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 可验证的 ML 流水线可以帮助解决模型退化的问题。由于在生产环境中监控模型的复杂性，模型退化是一个相对常见且工程不足的情况。模型预测的退化导致错误分类的评论和错失的业务机会。
- en: By continually monitoring our model predictions with SageMaker Model Monitor
    and Clarify, we can detect shifts in data distributions, model bias, and model
    explainability—triggering a pipeline to retrain and deploy a new review-classifier
    model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通过持续使用 SageMaker Model Monitor 和 Clarify 监控我们的模型预测，我们可以检测到数据分布的变化、模型偏差和模型可解释性，从而触发重新训练和部署新的审阅分类器模型的流水线。
- en: '[Figure 10-3](#machine_learning_pipeline_mapped_to_aws) shows a sample machine
    learning pipeline mapped to AWS services, including S3, Data Wrangler, and SageMaker.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-3](#machine_learning_pipeline_mapped_to_aws) 显示了映射到 AWS 服务的样本机器学习流水线，包括
    S3、Data Wrangler 和 SageMaker。'
- en: '![](assets/dsaw_1003.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1003.png)'
- en: Figure 10-3\. Machine learning pipeline mapped to AWS services.
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. 映射到 AWS 服务的机器学习流水线。
- en: Once the pipeline is running smoothly, we can increase velocity of experimentation
    by adding simultaneous pipelines to deploy multiple versions of the same model
    into production, as shown in [Figure 10-4](#deploying_multiple_versions_of_the_same).
    This could be used for online A/B/C or multiarmed bandit (MAB) tests.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线顺利运行后，我们可以通过添加同时流水线来增加实验速度，将同一模型的多个版本部署到生产环境，如 [图 10-4](#deploying_multiple_versions_of_the_same)
    所示。这可用于在线 A/B/C 或多臂赌博（MAB）测试。
- en: '![](assets/dsaw_1004.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1004.png)'
- en: Figure 10-4\. Training, tuning, and deploying multiple versions of the same
    model to improve experimentation velocity.
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. 训练、调优和部署同一模型的多个版本，以提高实验速度。
- en: Components of Effective Machine Learning Pipelines
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有效机器学习流水线的组成部分
- en: There are still many machine learning pipelines that include a high-friction
    step where the data scientist hands their model “over the wall” to the DevOps
    engineer or ML engineer to deploy. Machine learning pipelines are ripe for the
    type of revolution that stunned the software engineering community in the early
    2000s.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然有许多机器学习流水线包含一个高摩擦的步骤，即数据科学家将他们的模型“扔过墙”给 DevOps 工程师或 ML 工程师来部署。机器学习流水线正好可以进行革新，类似于
    2000 年代初惊艳软件工程社区的革新。
- en: Effective machine learning pipelines hide the details of the pipeline implementation
    and allow data science practitioners to focus on their business-specific, data
    science problem. Machine learning is continuous. The more we automate the process,
    the more we are free to solve additional business problems. Otherwise, we find
    ourselves manually rerunning one-off scripts every time new data arrives. While
    running a script is fairly simple, monitoring or restarting the script requires
    cognitive load that we could likely apply to higher-value tasks.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的机器学习流水线隐藏了流水线实现的细节，并允许数据科学从业者专注于其特定业务的数据科学问题。机器学习是连续的。我们自动化的过程越多，我们就越能自由地解决其他业务问题。否则，每次有新数据到达时我们就需要手动重新运行一次性脚本。虽然运行脚本相对简单，但监控或重新启动脚本则需要我们可能本可以用来解决更高价值任务的认知负荷。
- en: The ability to go from “ad hoc Jupyter notebook” to “repeatable machine learning
    pipeline” to “production cluster” is still a complex, error-prone, and underengineered
    workflow. However, we will provide some options on how to minimize the complexity
    and reduce errors with AWS.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 从“临时的Jupyter笔记本”到“可重复的机器学习流水线”再到“生产集群”的能力仍然是一个复杂、易错且未经充分工程化的工作流程。然而，我们将提供一些选项来如何最小化复杂性并减少AWS中的错误。
- en: 'Effective ML pipelines should include the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的机器学习流水线应包括以下内容：
- en: Data-focused tasks such as data ingestion, data versioning, data-quality checking,
    data preprocessing, and feature engineering
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诸如数据摄取、数据版本控制、数据质量检查、数据预处理和特征工程等以数据为重点的任务
- en: Model-building tasks such as model training, model-quality checking, and model
    versioning
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型构建任务，如模型训练、模型质量检查和模型版本控制
- en: Automated model deployment, model scaling, model explaining, and bias detection
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化模型部署、模型扩展、模型解释和偏差检测
- en: Experiment and lineage tracking to work backward and reproduce any model version
    from scratch
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验和谱系跟踪以反向工作并从头开始重现任何模型版本
- en: Automatic pickup of new data as it arrives (S3 `PutObject` event) and retraining—or
    perhaps automation using a cron-like timer (every night at midnight)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动捕捉新数据到达（S3 `PutObject`事件）并重新训练，或者可能使用类似cron的定时器进行自动化（每晚午夜）
- en: Feedback mechanism to continuously improve the model in accordance with our
    business objectives and key results, such as increasing customer satisfaction
    by 10% in the next 12 months
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反馈机制以根据我们的业务目标和关键结果持续改进模型，例如在接下来的12个月内将客户满意度提高10%
- en: In our experience, data-quality issues are the number-one cause of bad ML pipelines.
    In [Chapter 5](ch05.html#explore_the_dataset), we demonstrated how to use the
    AWS Deequ open source library to perform data-quality checks on our data as “step
    0” of the ML pipeline. Without consistent and expected quality, our ML pipeline
    will, at best, fail quickly and minimize cost. At worst, poor-quality data will
    produce poor-quality models that may include bias and negatively impact our business.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的经验，数据质量问题是糟糕的机器学习流水线的主要原因。在[第5章](ch05.html#explore_the_dataset)中，我们演示了如何使用AWS
    Deequ开源库对我们的数据进行数据质量检查，作为机器学习流水线的“步骤0”。没有一致和预期的数据质量，我们的机器学习流水线最多会迅速失败并最大程度地降低成本。在最坏的情况下，低质量的数据将生成低质量的模型，可能包含偏差并对我们的业务产生负面影响。
- en: In the beginning phases of ML exploration, we may not need a pipeline. The rigidity
    of a pipeline may seem too limiting. Pipelines are often deployed when we are
    ready to start training models regularly. If we are rapidly experimenting with
    many different types of features, models, and hyper-parameters, we may want to
    stay in the research lab until we are ready to automate for the long term and
    gain the benefits of regular pipeline executions, including data-quality checking,
    lineage tracking, and infrastructure scaling. However, even the simplest pipeline
    can help improve our model exploration.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习探索的初期阶段，我们可能不需要流水线。流水线的严格性可能显得过于限制。当我们准备开始定期训练模型时，通常会部署流水线。如果我们正在快速尝试许多不同类型的特征、模型和超参数，我们可能希望保持在研究实验室，直到准备好长期自动化并获得定期流水线执行的好处，包括数据质量检查、谱系跟踪和基础设施扩展。然而，即使是最简单的流水线也可以帮助我们改进模型探索。
- en: Steps of an Effective Machine Learning Pipeline
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有效的机器学习流水线步骤
- en: The following is a collection of steps that make up an effective, modern machine
    learning pipeline. We will demonstrate how to perform each of these steps in AWS
    using SageMaker Pipelines, AWS Step Functions, Airflow, Kubeflow, and other open
    source options in the upcoming sections.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是构成有效现代机器学习流水线的步骤集合。我们将在接下来的章节中演示如何使用SageMaker Pipelines、AWS Step Functions、Airflow、Kubeflow和其他开源选项执行每个步骤。
- en: Data ingestion and versioning
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数据摄入和版本控制
- en: Read the raw dataset from a data source such as a database, S3, or stream. Transform
    the dataset into a format that will be used in the next steps of the pipeline
    (i.e., CSV, Parquet, etc.) and version both the raw and transformed datasets.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据源（例如数据库、S3或流）中读取原始数据集。将数据集转换为下一步流水线中将使用的格式（例如CSV、Parquet等），并对原始和转换后的数据集进行版本控制。
- en: Data analysis and validation
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析和验证
- en: Analyze the quality and bias of the ingested dataset. Validate that the data
    is ready for the next pipeline steps.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 分析所摄入数据集的质量和偏差。验证数据是否准备好进入下一个流水线步骤。
- en: Feature engineering
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程
- en: Transform the dataset into features such as BERT embeddings used by the next
    pipeline steps. Balance and split the dataset into train, validation, and test
    splits. Publish the features to a feature store to be used for both training and
    inference by the entire organization.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集转换为诸如BERT嵌入之类的特征，供下一个流水线步骤使用。平衡和拆分数据集为训练、验证和测试集。发布这些特征到特征存储中，以供整个组织在训练和推断时使用。
- en: Model training and tuning
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练和调优
- en: Train a model using the features created in the previous pipeline step as well
    as a set of hyper-parameters specific to the model’s algorithm, analyze the accuracy
    of the model and hyper-parameters using the known validation dataset split, and
    repeat with different sets of hyper-parameters until the model accuracy is sufficient.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前一流水线步骤中创建的特征以及特定于模型算法的一组超参数来训练模型。使用已知的验证数据集拆分分析模型和超参数的准确性，并使用不同的超参数集重复，直到模型准确性达到要求。
- en: Model evaluation
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 模型评估
- en: Test the trained model using the known test dataset split, calculate additional
    metrics such as a confusion matrix and area under the curve, validate the model
    bias on different segments of the test dataset split (e.g., different product
    categories), and retrain and retune to reduce or remove bias.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用已知的测试数据集拆分测试训练模型，计算额外的指标（例如混淆矩阵和曲线下面积），验证不同测试数据集拆分（例如不同的产品类别）上的模型偏差，并重新训练和调整以减少或消除偏差。
- en: Model version and deployment
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 模型版本和部署
- en: Version the trained model along with the hyper-parameters and dataset splits
    and deploy the model into production as a real-time endpoint or batch prediction
    job.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对训练模型及其超参数和数据集拆分进行版本控制，并将模型部署到生产环境作为实时端点或批量预测作业。
- en: Model feedback and skew detection
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 模型反馈和偏差检测
- en: Analyze the model performance against business metrics (e.g., revenue increases,
    successful fraud detections, etc.), detect training-serving skew by analyzing
    the model inputs and outputs (predictions) relative to the training data baseline,
    and retrain the model if skew is detected.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 分析模型在业务指标（例如收入增长、成功识别欺诈等）上的表现。通过分析模型输入和输出（预测）与训练数据基准之间的差异来检测训练-服务偏差，并在检测到偏差时重新训练模型。
- en: Pipeline Orchestration with SageMaker Pipelines
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SageMaker Pipelines进行流水线编排
- en: SageMaker Pipelines is the most complete way to implement AI and machine learning
    pipelines on AWS. Let’s build a pipeline for our BERT-based review classifier
    and perform many of the steps described in previous chapters, including data ingestion,
    feature engineering, model training, and model deployment, as shown in [Figure 10-5](#using_sagemaker_pipeline_to_traincomma).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Pipelines是在AWS上实现AI和机器学习流水线的最完整方式。让我们为基于BERT的评论分类器构建一个流水线，并执行前几章描述的许多步骤，包括数据摄入、特征工程、模型训练和模型部署，如图10-5所示。
- en: '![](assets/dsaw_1005.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1005.png)'
- en: Figure 10-5\. Using SageMaker Pipeline to train, validate, create, and register
    our trained BERT model.
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-5。使用SageMaker Pipeline来训练、验证、创建和注册我们训练过的BERT模型。
- en: Let’s set up the pipeline programmatically using the SageMaker Python SDK to
    define each of the steps discussed earlier.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用SageMaker Python SDK以编程方式设置流水线，定义前面讨论的每个步骤。
- en: Create an Experiment to Track Our Pipeline Lineage
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个实验来跟踪我们的流水线谱系。
- en: 'First, we create an experiment and trial to track and compare our pipeline
    runs:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个实验和试验来跟踪和比较我们的流水线运行：
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Define Our Pipeline Steps
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义我们的管道步骤
- en: 'The first step of our pipeline is to transform the raw review text into BERT
    features using a SageMaker Processing Job. We will reuse the same `processor`
    from [Chapter 6](ch06.html#prepare_the_dataset_for_model_training) but wrap it
    in a `ProcessingStep` from the SageMaker Pipeline Python SDK:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们管道的第一步是使用SageMaker处理作业将原始评审文本转换为BERT特征。我们将重用来自[第6章](ch06.html#prepare_the_dataset_for_model_training)的相同`processor`，但将其包装在SageMaker
    Pipeline Python SDK的`ProcessingStep`中：
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now let’s train our model using the output from the previous feature-engineering
    processing step. We will use the same `estimator` from [Chapter 7](ch07.html#train_your_first_model)
    but wrap it in a `TrainingStep` from the SageMaker Pipeline Python SDK:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用先前特征工程处理步骤的输出来训练我们的模型。我们将使用来自[第7章](ch07.html#train_your_first_model)的相同`estimator`，但将其包装在SageMaker
    Pipeline Python SDK的`TrainingStep`中：
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, let’s add a step to evaluate our model using a SageMaker Processing Job
    to calculate the model test accuracy with *evaluate_model_metrics.py* and write
    the results to a file called *evaluation.json* in S3\. This file will be used
    by the next steps to conditionally register and prepare the model for deployment:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们添加一步，使用SageMaker处理作业评估我们的模型，计算模型测试准确率，并将结果写入名为*evaluation.json*的文件中存储在S3中。该文件将由下一步骤有条件地注册和准备模型部署使用：
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The *evaluate_model_metrics.py* file downloads the model, runs a set of test
    predictions, and writes the results to *evaluation.json*, as shown in the following
    code:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*evaluate_model_metrics.py*文件下载模型，运行一组测试预测，并将结果写入*evaluation.json*，如下所示：'
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s register our trained model with the SageMaker Model Registry. Once the
    model is registered, our pipeline requires a manual-approval step to deploy the
    model to staging. We first need to capture the evaluation metrics generated from
    the previous evaluation step in a `ModelMetrics` Python object named `model_metrics`,
    as shown in the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在SageMaker模型注册表中注册我们训练过的模型。一旦模型注册成功，我们的管道需要手动批准步骤将模型部署到staging环境。我们首先需要在名为`model_metrics`的Python对象中捕获上一个评估步骤生成的评估指标，如下所示：
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s pass `model_metrics` and create the `RegisterModel` step using the `estimator`
    from the previous `TrainingStep`. We can limit the instance types for both SageMaker
    Endpoints and Batch Transform Jobs by specifying lists for `inference_instances`
    and `transform_instances`, respectively:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们传递`model_metrics`并使用来自先前`TrainingStep`的`estimator`创建`RegisterModel`步骤。我们可以通过分别为`inference_instances`和`transform_instances`指定列表来限制SageMaker
    Endpoint和Batch Transform作业的实例类型：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now we will write the `CreateModelStep` to wrap the SageMaker `Model` used
    by both our SageMaker Endpoint and Batch Transform Jobs:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将编写`CreateModelStep`来封装SageMaker中用于SageMaker Endpoint和Batch Transform作业的`Model`：
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let’s add a `ConditionStep` to compare the evaluation accuracy metrics against
    a threshold. Our pipeline will register, create, and prepare the model for deployment
    only if the model accuracy exceeds the given threshold of 95%, as shown here:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们添加一个`ConditionStep`来比较评估准确性指标与阈值。只有当模型准确率超过95%的给定阈值时，我们的管道才会注册、创建并准备模型部署，如下所示：
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Configure the Pipeline Parameters
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置管道参数
- en: 'Before creating our pipeline, we must define parameter placeholders to use
    across all steps in our pipeline with `ParameterInteger`, `ParameterString`, and
    `ParameterFloat` from the SageMaker Pipelines Python SDK. These are merely placeholders
    for now because we are defining the pipeline. When we start the pipeline, we will
    specify the exact value to use for each parameter—or use the `default_value` if
    a value is not provided:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建管道之前，我们必须使用SageMaker Pipelines Python SDK中的`ParameterInteger`、`ParameterString`和`ParameterFloat`定义参数占位符，以在管道的所有步骤中使用。这些只是现在的占位符，因为我们正在定义管道。当我们启动管道时，我们将为每个参数指定确切的值，或者如果未提供值，则使用`default_value`：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Create the Pipeline
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建管道
- en: 'Next, we create the pipeline using all of the previously defined steps. This
    includes the `processing_step`, `training_step`, `evaluation_step`, as well as
    the `minimum_accuracy_condition_step`, which conditionally calls the `register_step`
    and `create_step` if the model achieves a minimum accuracy of 95% during model
    evaluation:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用所有先前定义的步骤创建管道。这包括`processing_step`、`training_step`、`evaluation_step`以及`minimum_accuracy_condition_step`，后者在模型评估过程中如果模型达到95%的最低准确率条件时有条件地调用`register_step`和`create_step`：
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Start the Pipeline with the Python SDK
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Python SDK启动管道
- en: 'Finally, we start the `Pipeline` by providing the desired parameter values,
    including the S3 location of the reviews dataset, maximum sequence length of the
    BERT tokens, and learning rate of the TensorFlow gradient-descent optimizer:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过提供所需的参数值启动`Pipeline`，包括评论数据集的S3位置、BERT token的最大序列长度以及TensorFlow梯度下降优化器的学习率：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Start the Pipeline with the SageMaker Studio UI
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SageMaker Studio UI启动管道
- en: We can also trigger a SageMaker Pipeline execution through the SageMaker Studio
    UI, as shown in [Figure 10-6](#start_a_pipeline_execution_through_sage). The Studio
    UI presents input fields for each of the parameters defined in our `Pipeline`
    object.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过SageMaker Studio UI启动SageMaker管道执行，如[图 10-6](#start_a_pipeline_execution_through_sage)所示。Studio
    UI为我们的`Pipeline`对象中定义的每个参数提供输入字段。
- en: '![](assets/dsaw_1006.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1006.png)'
- en: Figure 10-6\. Start a pipeline execution through SageMaker Studio UI.
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-6\. 通过SageMaker Studio UI启动管道执行。
- en: Approve the Model for Staging and Production
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批准模型用于暂存和生产环境
- en: 'We can approve models through the SageMaker Model Registry either manually
    through the SageMaker Studio UI or programmatically through our notebook. Approving
    the model will automatically deploy the model to a staging environment for testing.
    Our pipeline then requires a separate approval to move the model from staging
    to production if testing is successful. We can programmatically approve the model
    to staging using the following code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过SageMaker模型注册表手动在SageMaker Studio UI或通过我们的笔记本程序来批准模型。批准模型将自动将模型部署到用于测试的暂存环境。如果测试成功，我们的管道随后需要单独批准将模型从暂存环境移至生产环境。我们可以使用以下代码通过程序来将模型批准至暂存环境：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Review the Pipeline Artifact Lineage
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看管道artifact谱系
- en: 'We can review the artifact lineage directly either through the SageMaker Studio
    UI or programmatically in our notebook with the Python SDK. Following is the code
    to list the artifacts across all steps, including feature engineering, model training,
    evaluation, approval, and deployment:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接通过SageMaker Studio UI或通过我们的笔记本程序使用Python SDK来查看artifact谱系。以下是列出所有步骤的artifact的代码，包括特征工程、模型训练、评估、批准和部署：
- en: '[PRE13]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is similar to the following table:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '输出结果类似于以下表格： '
- en: '|   | Name/source | Direction | Type | Association type | Lineage type |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|   | 名称/来源 | 方向 | 类型 | 关联类型 | 谱系类型 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 0 | preprocess-scikit-text-to-bert-feature-store.py | Input | DataSet | ContributedTo
    | artifact |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 0 | preprocess-scikit-text-to-bert-feature-store.py | 输入 | 数据集 | 贡献至 | artifact
    |'
- en: '| 1 | s3://.../amazon-reviews-pds/tsv/ | Input | DataSet | ContributedTo |
    artifact |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 1 | s3://.../amazon-reviews-pds/tsv/ | 输入 | 数据集 | 贡献至 | artifact |'
- en: '| 2 | 68331...om/sagemaker-scikit-learn:0.23-1-cpu-py3 | Input | Image | ContributedTo
    | artifact |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 68331...om/sagemaker-scikit-learn:0.23-1-cpu-py3 | 输入 | 图像 | 贡献至 | artifact
    |'
- en: '| 3 | s3://.../output/bert-test | Output | DataSet | Produced | artifact |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 3 | s3://.../output/bert-test | 输出 | 数据集 | 产出 | artifact |'
- en: '| 4 | s3://.../output/bert-validation | Output | DataSet | Produced | artifact
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 4 | s3://.../output/bert-validation | 输出 | 数据集 | 产出 | artifact |'
- en: '| 5 | s3://.../output/bert-train | Output | DataSet | Produced | artifact |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 5 | s3://.../output/bert-train | 输出 | 数据集 | 产出 | artifact |'
- en: '| 6 | s3://.../output/bert-test | Input | DataSet | ContributedTo | artifact
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 6 | s3://.../output/bert-test | 输入 | 数据集 | 贡献至 | artifact |'
- en: '| 7 | s3://.../output/bert-validation | Input | DataSet | ContributedTo | artifact
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 7 | s3://.../output/bert-validation | 输入 | 数据集 | 贡献至 | artifact |'
- en: '| 8 | s3://.../output/bert-train | Input | DataSet | ContributedTo | artifact
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 8 | s3://.../output/bert-train | 输入 | 数据集 | 贡献至 | artifact |'
- en: '| 9 | 76310.../tensorflow-training:2.3.1-cpu-py37 | Input | Image | ContributedTo
    | artifact |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 76310.../tensorflow-training:2.3.1-cpu-py37 | 输入 | 图像 | 贡献至 | artifact
    |'
- en: '| 10 | model.tar.gz | Output | Model | Produced | artifact |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 10 | model.tar.gz | 输出 | 模型 | 产出 | artifact |'
- en: '| 11 | model.tar.gz | Input | Model | ContributedTo | artifact |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 11 | model.tar.gz | 输入 | 模型 | 贡献至 | artifact |'
- en: '| 12 | 76310.../tensorflow-inference:2.1.0-cpu | Input | Image | ContributedTo
    | artifact |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 76310.../tensorflow-inference:2.1.0-cpu | 输入 | 图像 | 贡献至 | artifact |'
- en: '| 13 | bert-reviews-1610437484-1-Approved-1610443150-aws-model-group | Input
    | Approval | ContributedTo | action |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 13 | bert-reviews-1610437484-1-Approved-1610443150-aws-model-group | 输入 |
    批准 | 贡献至 | action |'
- en: '| 14 | bert-reviews-1610437484-1-Approved-1610443150-aws-endpoint | Output
    | ModelDeployment | ContributedTo | action |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 14 | bert-reviews-1610437484-1-Approved-1610443150-aws-endpoint | 输出 | 模型部署
    | 贡献至 | action |'
- en: '| 15 | bert-reviews-1610437484-1-aws-model-group | Output | ModelGroup | AssociatedWith
    | context |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 15 | bert-reviews-1610437484-1-aws-model-group | 输出 | ModelGroup | AssociatedWith
    | context |'
- en: Review the Pipeline Experiment Lineage
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看管道实验谱系
- en: 'Using the SageMaker Experiments API, we can show the experiment lineage of
    our pipeline through all steps of the pipeline, including feature engineering,
    model training, evaluation, and deployment, as shown in the following:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SageMaker实验API，我们可以通过管道的所有步骤展示管道的实验谱系，包括特征工程、模型训练、评估和部署，如下所示：
- en: '[PRE14]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '| TrialComponentName | DisplayName | max_seq_​length | learning_​rate | train_​accuracy
    | test_​accuracy | endpoint_​name |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| TrialComponentName | DisplayName | max_seq_​length | learning_​rate | train_​accuracy
    | test_​accuracy | endpoint_​name |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| pipelines-0tsa93mahu8v-​processing-kch2vw03qc-​aws-processing-job | prepare
    | 64.0 | NaN | NaN | NaN |   |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| pipelines-0tsa93mahu8v-​processing-kch2vw03qc-​aws-processing-job | prepare
    | 64.0 | NaN | NaN | NaN |   |'
- en: '| pipelines-0tsa93mahu8v-​Train-tlvC7YdBl9-​aws-training-job | train | 64.0
    | 0.000017 | 0.9416 | NaN |   |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| pipelines-0tsa93mahu8v-​Train-tlvC7YdBl9-​aws-training-job | train | 64.0
    | 0.000017 | 0.9416 | NaN |   |'
- en: '| pipelines-​1daa23hlku3v-​processing-hkc9w0v0q-​aws-processing-job | evaluate
    | 64.0 | NaN | NaN | 0.9591 |   |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| pipelines-​1daa23hlku3v-​processing-hkc9w0v0q-​aws-processing-job | evaluate
    | 64.0 | NaN | NaN | 0.9591 |   |'
- en: '| TrialComponent-2021-01-​09214921-dgtu | deploy | NaN | NaN | NaN | NaN |
    bert-reviews-​1610437484-​endpoint |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| TrialComponent-2021-01-​09214921-dgtu | deploy | NaN | NaN | NaN | NaN |
    bert-reviews-​1610437484-​endpoint |'
- en: Automation with SageMaker Pipelines
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SageMaker管道进行自动化
- en: 'There are two main ways to automatically start a pipeline: event-based triggers
    and time-based triggers. Event-based triggers will start a pipeline when a particular
    event occurs, for example, when a new *train.py* is committed to our Git-based
    code repository. This is often called “GitOps” automation. We can also start a
    new pipeline when new data arrives into S3 from a `PutObject` event. Time-based
    triggers will start the pipeline on a schedule, such as every week, every two
    days, or every four hours. Let’s discuss how to implement GitOps, S3, and time-based
    triggers to automatically start a SageMaker Pipeline.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种主要的自动启动管道的方式：基于事件的触发器和基于时间的触发器。基于事件的触发器将在特定事件发生时启动管道，例如，当一个新的*train.py*提交到我们基于Git的代码仓库时。这通常称为“GitOps”自动化。当新数据通过`PutObject`事件进入S3时，我们也可以启动新的管道。基于时间的触发器将按计划启动管道，例如每周、每两天或每四小时一次。让我们讨论如何实现GitOps、S3和基于时间的触发器，以自动启动SageMaker管道。
- en: GitOps Trigger When Committing Code
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在提交代码时通过GitOps触发
- en: SageMaker implements GitOps pipeline automation through SageMaker Projects.
    SageMaker Projects come with pre-built MLOps templates that automate the model-building
    and deployment pipelines. We can customize the templates, or create our own templates,
    as needed.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker通过SageMaker项目实现GitOps管道自动化。SageMaker项目提供了预构建的MLOps模板，自动化模型构建和部署管道。我们可以根据需要定制这些模板，或者创建自己的模板。
- en: We can create our own project by selecting one of the pre-built MLOps templates
    provided by SageMaker or by using our own custom template that we provide. The
    MLOps templates use AWS CloudFormation to automatically set up all required components
    for our GitOps automation workflow with SageMaker Pipelines. The MLOps template
    also sets up a trigger to run the pipeline each time we commit new code to the
    code repositories.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择使用SageMaker提供的预构建MLOps模板之一，或者使用我们自己提供的自定义模板来创建我们自己的项目。MLOps模板使用AWS CloudFormation自动设置所有必需的组件，以便我们的GitOps自动化工作流可以与SageMaker管道配合使用。MLOps模板还设置了一个触发器，每次我们向代码仓库提交新代码时都会运行管道。
- en: 'There are two main components of our MLOps template for SageMaker Pipelines:
    `modelbuild` and `modeldeploy`. The `modelbuild` component builds and registers
    the model. The `modeldeploy` component deploys the model to staging and production.
    Deploying the model to production requires a second manual approval step, as shown
    in [Figure 10-7](#mlops_pipeline_to_deploy_models_to_both).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们SageMaker管道的MLOps模板主要由两个组件组成：`modelbuild`和`modeldeploy`。`modelbuild`组件用于构建和注册模型。`modeldeploy`组件将模型部署到暂存和生产环境。将模型部署到生产环境需要进行第二次手动批准步骤，如[图 10-7](#mlops_pipeline_to_deploy_models_to_both)所示。
- en: '![](assets/dsaw_1007.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1007.png)'
- en: Figure 10-7\. MLOps pipeline to deploy models to both staging and production
    with manual approvals.
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-7\. MLOps管道，用于将模型部署到暂存和生产环境，带有手动批准。
- en: The separation of `modelbuild` and `modeldeploy` allows for a separation of
    responsibility and access control. For example, the data scientist may be responsible
    for the `modelbuild` phase to push the model into staging, while the DevOps team
    is responsible for the `modeldeploy` phase to push the model into production.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`modelbuild` 和 `modeldeploy` 的分离允许责任和访问控制的分离。例如，数据科学家可能负责 `modelbuild` 阶段将模型推送到暂存区，而
    DevOps 团队负责 `modeldeploy` 阶段将模型推送到生产环境。'
- en: S3 Trigger When New Data Arrives
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: S3 在新数据到达时触发
- en: As new data arrives into the system—directly from an application or through
    data streaming services like Kinesis Streams and Managed Streaming for Apache
    Kafka—we may want to continuously run our pipeline and update our models to include
    the new data. While it’s perfectly acceptable to manually run our pipelines every
    week, day, or even hour, we can easily automate the pipeline as new data lands
    in S3 from an upstream application, as shown in [Figure 10-8](#automatically_start_a_pipeline_when_new).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当新数据直接从应用程序或通过数据流服务（如 Kinesis Streams 和 Managed Streaming for Apache Kafka）进入系统时，我们可能希望持续运行我们的管道并更新我们的模型以包括新数据。虽然每周、每天甚至每小时手动运行管道是完全可以接受的，但我们可以在上游应用程序从
    S3 中新数据着陆时轻松自动化管道，如 [图 10-8](#automatically_start_a_pipeline_when_new) 所示。
- en: '![](assets/dsaw_1008.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1008.png)'
- en: Figure 10-8\. Automatically start a SageMaker Pipeline when new data arrives
    in S3.
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-8\. 当新数据到达 S3 时自动启动 SageMaker 管道。
- en: 'First, we need to be notified when new data arrives in S3 by enabling AWS CloudTrail
    data-event logging on our S3 bucket:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要在我们的 S3 存储桶上启用 AWS CloudTrail 数据事件日志记录，以便在新数据到达 S3 时收到通知：
- en: '[PRE15]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we will create an Amazon EventBridge rule to trigger the SageMaker Pipeline
    every time new files are uploaded to the S3 bucket using an EventBridge rule that
    matches both the S3 `PutObject` and `CompleteMultipartUpload`. Here is the Python
    code to enable this behavior:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个 Amazon EventBridge 规则，使用一个 EventBridge 规则来触发 SageMaker 管道，每当新文件上传到
    S3 存储桶时触发，这个规则匹配 S3 的 `PutObject` 和 `CompleteMultipartUpload`。以下是启用此行为的 Python
    代码：
- en: '[PRE16]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Lastly, we associate the rule with an AWS Lambda function to start our pipeline
    when the rule is matched:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将规则与 AWS Lambda 函数关联，以在匹配规则时启动我们的管道：
- en: '[PRE17]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here is an excerpt of the AWS Lambda function used to trigger our SageMaker
    pipeline:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于触发我们 SageMaker 管道的 AWS Lambda 函数的摘录：
- en: '[PRE18]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Anytime a new file is uploaded to this S3 bucket, EventBridge will trigger the
    rule and start our pipeline execution. We can use the `lambda_handler` function’s
    `event` variable to find out the exact file that was uploaded and, perhaps, incrementally
    train our model on just that new file. Depending on our use case, we may not want
    to start a new pipeline for every file uploaded to S3\. However, this is a good
    starting point to build our own rules and triggers from many AWS services.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 每当有新文件上传到这个 S3 存储桶时，EventBridge 将触发规则并启动我们的管道执行。我们可以使用 `lambda_handler` 函数的
    `event` 变量来确定上传的确切文件，也许可以仅对新文件进行增量训练我们的模型。根据我们的用例，我们可能不希望为每个上传到 S3 的文件启动新管道。然而，这是构建我们自己的规则和触发器的一个良好起点，适用于许多
    AWS 服务。
- en: Note
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: At the time of this writing, there was no native integration between EventBridge
    and SageMaker Pipelines, so we need to use a Lambda function shim. However, there
    will likely be native integration by the time this book is published, so we may
    be able to skip the Lambda function and integrate EventBridge directly with SageMaker
    Pipelines.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，EventBridge 与 SageMaker 管道之间没有原生集成，因此我们需要使用 Lambda 函数的桥接。然而，到本书出版时，可能会有原生集成，因此我们可以跳过
    Lambda 函数直接将 EventBridge 与 SageMaker 管道集成。
- en: Time-Based Schedule Trigger
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于时间的计划触发器
- en: 'We may want to trigger our pipeline on batches of data over a specific period
    of time, such as hourly, daily, monthly, etc. Similar to configuring a cron job,
    we can create an EventBridge rule to run our pipeline on a schedule. We can specify
    the schedule using familiar cron syntax or by defining a fixed rate, such as every
    hour. Or we can programmatically define the schedule using the AWS Python SDK
    for EventBridge. The following code triggers the pipeline to run every hour:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能希望在特定时间段内（例如每小时、每天、每月等）对数据批次触发我们的管道。类似于配置 cron 作业，我们可以创建一个 EventBridge 规则来按计划运行我们的管道。我们可以使用熟悉的
    cron 语法或通过定义固定的频率（例如每小时）来指定计划。或者，我们可以使用 AWS Python SDK 以编程方式定义计划的规则。以下代码每小时触发管道运行：
- en: '[PRE19]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Statistical Drift Trigger
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统计漂移触发器
- en: We can also start a new pipeline if SageMaker Model Monitor detects data-quality
    drift, model-quality drift, bias drift, or explainability drift relative to a
    given baseline or ground truth set of predicted labels. We can create baselines
    for data quality, model quality, model bias, and feature importances and monitor
    our deployed models with SageMaker Model Monitor, as discussed in [Chapter 9](ch09.html#deploy_models_to_production).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 SageMaker Model Monitor 检测到相对于给定基线或预测标签的真实性的数据质量漂移、模型质量漂移、偏差漂移或可解释性漂移，我们也可以启动新的流水线。我们可以为数据质量、模型质量、模型偏差和特征重要性创建基线，并使用
    SageMaker Model Monitor 监控我们部署的模型，如第 9 章所讨论的。
- en: Model Monitor captures the real-time model predictions and analyzes the data
    distributions for model inputs and model outputs in comparison to the baseline
    thresholds learned from the training data. This helps us to detect statistical
    changes such as covariate shift or concept drift that may trigger a new pipeline
    execution to retrain the model.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Model Monitor 捕获实时模型预测，并分析数据分布，比较模型输入和模型输出与从训练数据中学习的基线阈值。这有助于我们检测统计变化，如协变量漂移或概念漂移，这可能触发新的流水线执行来重新训练模型。
- en: Model Monitor integrates with SageMaker Clarify. With Clarify, SageMaker continuously
    monitors the deployed models for changes in model bias and feature importances.
    We define a confidence range of bias metrics for our models based on the offline
    training data. We continually monitor the confidence intervals seen in the model’s
    online predictions. If the observed confidence interval doesn’t overlap with the
    defined confidence range, SageMaker Clarify will trigger a bias-drift alert that
    we can use to start a new pipeline. Similarly, if the changes in feature importances
    cross a defined threshold, SageMaker Clarify will trigger a feature attribution
    drift alert, which we can use to start a new pipeline.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Model Monitor 与 SageMaker Clarify 集成。使用 Clarify，SageMaker 连续监控部署的模型，检测模型偏差和特征重要性的变化。我们基于离线训练数据定义模型偏差指标的置信区间。我们持续监控模型在线预测中观察到的置信区间。如果观察到的置信区间与定义的置信区间不重叠，SageMaker
    Clarify 将触发一个偏差漂移警报，我们可以用来启动新的流水线。类似地，如果特征重要性的变化超过了定义的阈值，SageMaker Clarify 将触发一个特征归因漂移警报，我们可以用来启动新的流水线。
- en: More Pipeline Options
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多的流水线选项
- en: While SageMaker Pipelines is the standard way to implement AI and machine learning
    pipelines on AWS, we also present AWS Step Functions and various open source options
    such as Kubeflow Pipelines, Apache Airflow, TFX, and MLflow. These tools provide
    great support for AWS data stores, including Amazon S3, Athena, EMR, EFS, and
    FSx for Lustre.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 SageMaker Pipelines 是在 AWS 上实现 AI 和机器学习流水线的标准方式，我们还介绍了 AWS Step Functions
    以及各种开源选项，如 Kubeflow Pipelines、Apache Airflow、TFX 和 MLflow。这些工具对包括 Amazon S3、Athena、EMR、EFS
    和 FSx for Lustre 在内的 AWS 数据存储提供了良好的支持。
- en: AWS Step Functions and the Data Science SDK
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS Step Functions 和 Data Science SDK
- en: Step Functions is a great option for building complex workflows without having
    to build and maintain our own infrastructure. While Step Functions were not specifically
    designed for machine learning, they provide great flexibility and deep integration
    with many AWS services, and expose the Step Functions Data Science SDK.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Step Functions 是一个很好的选择，可以构建复杂的工作流，而无需构建和维护自己的基础设施。虽然 Step Functions 并非专门为机器学习而设计，但它们提供了与许多
    AWS 服务深度集成的灵活性，并公开了 Step Functions Data Science SDK。
- en: '[Figure 10-9](#step_function_pipeline_to_orchestrate_o) shows a Step Function
    Pipeline that was built to orchestrate the same BERT-based review-classifier pipeline
    shown in the SageMaker Pipelines section.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-9](#step_function_pipeline_to_orchestrate_o) 显示了一个 Step Function 流水线，用于编排在
    SageMaker Pipelines 部分展示的基于 BERT 的评论分类器流水线。'
- en: '![](assets/dsaw_1009.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1009.png)'
- en: Figure 10-9\. Step Function Pipeline to orchestrate our BERT-based pipeline
    on SageMaker.
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-9\. Step Function 流水线，用于在 SageMaker 上编排我们基于 BERT 的流水线。
- en: 'Here is an excerpt from the Step Function configuration for the training step
    of our pipeline. The complete code is in the GitHub repository associated with
    this book:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们流水线中训练步骤的 Step Function 配置的摘录。完整的代码可以在与本书关联的 GitHub 仓库中找到：
- en: '[PRE20]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Kubeflow Pipelines
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubeflow Pipelines
- en: Kubeflow is a popular machine learning ecosystem built on Kubernetes that includes
    an orchestration subsystem called *Kubeflow Pipelines*. While Kubeflow requires
    us to build and maintain our own Amazon EKS clusters, it is well supported in
    AWS, as shown in [Figure 10-10](#kubeflow_is_well_supported_on_aws_due_t).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 是建立在 Kubernetes 上的流行机器学习生态系统，包括一个称为 *Kubeflow Pipelines* 的编排子系统。尽管
    Kubeflow 要求我们构建和维护自己的 Amazon EKS 集群，但它在 AWS 上得到了很好的支持，如 [图 10-10](#kubeflow_is_well_supported_on_aws_due_t)
    所示。
- en: '![](assets/dsaw_1010.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1010.png)'
- en: Figure 10-10\. Kubeflow is well supported on AWS due to tight integration with
    Amazon EKS.
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-10\. Kubeflow 在 AWS 上得到了很好的支持，因为与 Amazon EKS 的紧密集成。
- en: With Kubeflow, we can run distributed training jobs, analyze training metrics,
    track pipeline lineage, restart failed pipelines, and schedule pipeline runs.
    The conventions used in Kubeflow are well defined and well supported by a large
    community of open source contributors across many organizations. If we are already
    using Kubernetes, Kubeflow may be a good option to manage our pipelines.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Kubeflow，我们可以运行分布式训练作业，分析训练指标，跟踪流水线谱系，重新启动失败的流水线，并安排流水线运行。Kubeflow 中使用的约定是明确定义的，并得到了许多组织中开源贡献者大规模支持的支持。如果我们已经在使用
    Kubernetes，Kubeflow 可能是管理我们的流水线的一个好选择。
- en: While managing Kubernetes is fun to some folks—including the authors of this
    book—it is a distraction from everyday data science and engineering tasks. The
    authors of this book have spent many nights and weekends troubleshooting Kubernetes-level
    issues—time that could have been spent engineering more features and training
    better models.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对于一些人来说管理 Kubernetes 是一件有趣的事情，包括本书的作者们，在日常数据科学和工程任务中却是一种分心。本书的作者们在许多夜晚和周末都在解决
    Kubernetes 层面的问题，这段时间本可以用来工程更多功能和训练更好的模型。
- en: 'Because of Kubeflow’s tight integration with Kubernetes, almost every question
    about managing and scaling a Kubeflow cluster can be answered by reviewing Kubernetes
    and Amazon EKS features. Here are some examples:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Kubeflow 与 Kubernetes 的紧密集成，几乎可以通过查阅 Kubernetes 和 Amazon EKS 的功能来回答关于管理和扩展
    Kubeflow 集群的每一个问题。以下是一些示例：
- en: '*Question:* “How do I monitor the GPUs in my Kubeflow training jobs?”'
  id: totrans-185
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*问题:* “如何监控我的 Kubeflow 训练作业中的 GPU？”'
- en: ''
  id: totrans-186
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Answer:* “The same way you monitor other system resources in Kubernetes on
    AWS: Prometheus, Grafana, and CloudWatch.”'
  id: totrans-187
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*回答:* “在 AWS 上，您监控 Kubernetes 上其他系统资源的方式相同：Prometheus、Grafana 和 CloudWatch。”'
- en: ''
  id: totrans-188
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Question:* “How do I auto-scale my Kubeflow REST endpoints?”'
  id: totrans-189
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*问题:* “如何自动缩放我的 Kubeflow REST 端点？”'
- en: ''
  id: totrans-190
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Answer:* “The same way you auto-scale other Kubernetes resources on AWS: Horizontal
    Pod Autoscaling, Cluster Autoscaling, and CloudWatch.”'
  id: totrans-191
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*回答:* “在 AWS 上，您自动缩放 Kubernetes 资源的方式与其他资源相同：水平 Pod 自动缩放、集群自动缩放和 CloudWatch。”'
- en: ''
  id: totrans-192
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Question:* “Does Kubeflow support Spot Instances?”'
  id: totrans-193
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*问题:* “Kubeflow 支持 Spot Instances 吗？”'
- en: ''
  id: totrans-194
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Answer:* “Yes, because Amazon EKS supports Spot Instances.”'
  id: totrans-195
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*回答:* “是的，因为 Amazon EKS 支持 Spot Instances。”'
- en: Note
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It’s worth noting that when using Spot Instances to train a model with Kubeflow,
    we must use a framework that tolerates the Spot Instances leaving the cluster
    (during a training job) as they are replaced when new Spot Instances become available.
    When the Spot Instances are replaced, they are removed from the cluster and appear
    as failed instances to the training job. Modern frameworks such as TensorFlow,
    PyTorch, and Apache MXNet support instance failures but require extra code and
    configuration to perform the checkpointing needed to efficiently recover from
    the failure and continue training. We demonstrated the TensorFlow code and SageMaker
    configuration for checkpointing in [Chapter 8](ch08.html#train_and_optimize_models_at_scale).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在使用 Spot Instances 训练 Kubeflow 模型时，我们必须使用一个能够容忍 Spot Instances 离开集群（在训练作业期间）的框架，在新的
    Spot Instances 可用时替换它们。当 Spot Instances 被替换时，它们会从集群中移除，并在训练作业中出现为失败的实例。现代框架如 TensorFlow、PyTorch
    和 Apache MXNet 支持实例故障，但需要额外的代码和配置来执行所需的检查点，以有效地从故障中恢复并继续训练。我们在 [第 8 章](ch08.html#train_and_optimize_models_at_scale)
    中展示了 TensorFlow 代码和 SageMaker 配置用于检查点。
- en: Let’s create an open source Kubeflow pipeline that trains a BERT model using
    managed Amazon SageMaker and the same Amazon Customer Reviews Dataset from the
    previous chapters, as shown in [Figure 10-11](#kubeflow_pipeline_orchestrating_our_ber).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个开源的 Kubeflow 流水线，使用托管的 Amazon SageMaker 和前几章节中使用的相同的 Amazon Customer
    Reviews 数据集来训练一个 BERT 模型，如 [图 10-11](#kubeflow_pipeline_orchestrating_our_ber)
    所示。
- en: '![](assets/dsaw_1011.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1011.png)'
- en: Figure 10-11\. Kubeflow pipeline orchestrating our BERT-based pipeline on SageMaker.
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-11\. Kubeflow 流水线在 SageMaker 上编排我们基于 BERT 的流水线。
- en: 'First, we import the SageMaker Components for Kubeflow Pipelines Python library
    and supporting assets to use in our Kubeflow Pipeline. The following YAML can
    be found [on GitHub](https://oreil.ly/Uh4Ls):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入SageMaker Components for Kubeflow Pipelines Python库和支持资产以在我们的Kubeflow
    Pipeline中使用。以下YAML可以在[GitHub上找到](https://oreil.ly/Uh4Ls)：
- en: '[PRE21]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now let’s set up the S3 locations of the raw training data:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们设置原始训练数据的S3位置：
- en: '[PRE22]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let’s define the S3 locations of the transformed features:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义转换后的BERT特征的S3位置：
- en: '[PRE23]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s define the actual Kubeflow Pipeline using the Kubeflow Pipelines Python
    SDK:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Kubeflow Pipelines Python SDK定义实际的Kubeflow Pipeline：
- en: '[PRE24]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let’s transform the raw input data to BERT features:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将原始输入数据转换为BERT特征：
- en: '[PRE25]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let’s train the model:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练模型：
- en: '[PRE26]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Deploy the BERT model as a REST-based SageMaker Endpoint:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 部署BERT模型作为基于REST的SageMaker端点：
- en: '[PRE27]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let’s compile and run the Kubeflow Pipeline, which results in a deployed SageMaker
    Endpoint with our BERT model:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编译并运行Kubeflow Pipeline，结果是部署一个带有我们的BERT模型的SageMaker端点：
- en: '[PRE28]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let’s invoke the SageMaker Endpoint and get a star rating prediction from the
    review text:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调用SageMaker端点并从评论文本中获取星级预测：
- en: '[PRE29]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Apache Airflow
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Airflow
- en: Apache Airflow is a very mature and popular option initially developed to orchestrate
    data engineering and extract-transform-load (ETL) pipelines for analytics workloads.
    However, Airflow has expanded into the machine-learning space as a viable pipeline
    orchestrator. Amazon supports Amazon Managed Workflows for Apache Airflow to reduce
    the operational burden of running Airflow clusters on AWS.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Airflow是一个非常成熟和流行的选项，最初开发用于编排数据工程和ETL管道以支持分析工作负载。然而，Airflow已经扩展到机器学习领域作为一个可行的管道编排工具。Amazon支持Amazon
    Managed Workflows for Apache Airflow以减少在AWS上运行Airflow集群的操作负担。
- en: With a large library of third-party plug-ins and native integration with many
    AWS services, Amazon MWAA is a great option for managing pipelines on AWS with
    Airflow. If we are already using Airflow for our data engineering and ETL pipelines,
    Airflow may be a good option to orchestrate our machine learning pipelines. [Figure 10-12](#managed_workflows_for_apache_airflow_or)
    shows our BERT-based review-classifier pipeline implemented as an Apache Airflow
    directed acyclic graph (DAG) using Amazon MWAA and SageMaker.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 带有大量第三方插件库和与许多AWS服务的原生集成，Amazon MWAA是在AWS上管理使用Airflow的管道的一个很好的选择。如果我们已经在数据工程和ETL管道中使用Airflow，Airflow可能是编排机器学习管道的一个很好的选择。[图 10-12](#managed_workflows_for_apache_airflow_or)
    展示了我们基于BERT的评论分类器管道，通过Amazon MWAA和SageMaker实现为Apache Airflow的有向无环图(DAG)。
- en: '![](assets/dsaw_1012.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1012.png)'
- en: Figure 10-12\. Amazon MWAA orchestrating our BERT-based pipeline on SageMaker.
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-12\. Amazon MWAA在SageMaker上编排我们的基于BERT的管道。
- en: 'Let’s demonstrate how to build an Airflow DAG with SageMaker to orchestrate
    our BERT-based machine learning pipeline. First, we need to define the Airflow
    DAG:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们演示如何使用SageMaker构建一个Airflow DAG来编排我们基于BERT的机器学习管道。首先，我们需要定义Airflow DAG：
- en: '[PRE30]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, let’s transform the raw data into BERT features:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将原始数据转换为BERT特征：
- en: '[PRE31]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let’s train the model:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练模型：
- en: '[PRE32]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now let’s deploy the model:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们部署模型：
- en: '[PRE33]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let’s define the pipeline:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义管道：
- en: '[PRE34]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: MLflow
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLflow
- en: MLflow is an open source project that offers experiment tracking and multiframework
    support including Apache Spark, but limited workflow support. While MLflow has
    some nice features, it requires us to build and maintain our own Amazon EC2 or
    EKS clusters. If we need a lightweight, simple way to track experiments and run
    simple workflows, MLflow may be a good choice.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow是一个开源项目，提供实验跟踪和多框架支持，包括Apache Spark，但工作流支持有限。虽然MLflow具有一些不错的功能，但它要求我们建立和维护自己的Amazon
    EC2或EKS集群。如果我们需要一种轻量级、简单的方法来跟踪实验和运行简单的工作流，MLflow可能是一个不错的选择。
- en: TensorFlow Extended
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow Extended
- en: TFX is an open source collection of Python libraries used within a pipeline
    orchestrator such as Kubeflow Pipelines, Apache Airflow, and MLflow. At a very
    high level, TFX is a collection of Python libraries that addresses every step
    of the machine learning pipeline. Most used within the TensorFlow community, TFX
    does have limited support for other frameworks, such as scikit-learn. If we are
    already using TensorFlow and looking to add some structure to our process, TFX
    may be a good choice for us. However, to scale, tune, and manage TFX beyond a
    single node, we should understand Apache Beam, which powers TFX’s distributed
    data processing. Apache Beam has a bit of a learning curve but is pretty straightforward
    once you dive into it. [Figure 10-13](#tfx_libraries_and_components) shows the
    different libraries and components of TFX.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: TFX 是一组开源的 Python 库，用于在流水线编排器（如 Kubeflow Pipelines、Apache Airflow 和 MLflow）中使用。从非常高的层次来看，TFX
    是一组解决机器学习流水线的每个步骤的 Python 库。虽然大多数在 TensorFlow 社区中使用，TFX 对其他框架（如 scikit-learn）也有有限的支持。如果我们已经在使用
    TensorFlow 并希望为我们的流程添加一些结构，TFX 可能是一个不错的选择。然而，要在单个节点之外扩展、调整和管理 TFX，我们应该理解支持 TFX
    分布式数据处理的 Apache Beam。Apache Beam 学习曲线略高，但一旦深入研究，就会变得非常直观。[图 10-13](#tfx_libraries_and_components)
    展示了 TFX 的不同库和组件。
- en: '![](assets/dsaw_1013.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1013.png)'
- en: Figure 10-13\. TFX libraries and components.
  id: totrans-239
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-13\. TFX 库和组件。
- en: Human-in-the-Loop Workflows
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人在环回路工作流程
- en: While AI and machine learning services make our lives easier, humans are far
    from being obsolete. In fact, the concept of “human-in-the-loop” has emerged as
    an important cornerstone in many AI/ML workflows. Humans provide necessary quality
    assurances before pushing sensitive or regulated models into production. We can
    also leverage human intelligence by “crowdsourcing” data labeling tasks to humans.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 AI 和机器学习服务使我们的生活更加便利，但人类远未过时。事实上，“人在环”概念已经成为许多 AI/ML 工作流的重要基石。在将敏感或受监管的模型推向生产之前，人类提供必要的质量保证。我们还可以通过向人类“众包”数据标注任务来利用人类智慧。
- en: We describe two services, Amazon A2I and SageMaker Ground Truth, that demonstrate
    how humans and AI can work successfully together. Amazon A2I enables machine learning
    practitioners to integrate human review workflows into their applications. SageMaker
    Ground Truth leverages human workforces combined with an active learning approach
    to create accurate training datasets.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述了两个服务，Amazon A2I 和 SageMaker Ground Truth，展示了人类与 AI 成功协作的案例。Amazon A2I 可让机器学习从业者将人工审核工作流整合到他们的应用程序中。SageMaker
    Ground Truth 则利用人力工作人员结合主动学习方法来创建准确的训练数据集。
- en: Improving Model Accuracy with Amazon A2I
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Amazon A2I 改善模型准确性
- en: Amazon A2I is a fully managed service to develop human-in-the-loop workflows,
    which include a user interface, role-based access control with IAM, and data storage
    with S3\. Amazon A2I is integrated with services such as Amazon Rekognition for
    content moderation and Amazon Textract for form-data extraction. [Figure 10-14](#amazon_augmented_ai_workflow_to_review)
    illustrates an Amazon A2I workflow to review model predictions from Amazon Comprehend.
    We can also use Amazon A2I with Amazon SageMaker and custom ML models.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon A2I 是一个完全托管的服务，用于开发人在环回路工作流程，包括用户界面、基于角色的 IAM 访问控制和 S3 数据存储。Amazon A2I
    与 Amazon Rekognition（用于内容审核）和 Amazon Textract（用于表单数据提取）等服务集成。[图 10-14](#amazon_augmented_ai_workflow_to_review)
    展示了一个 Amazon A2I 工作流程，用于审核来自 Amazon Comprehend 的模型预测。我们还可以将 Amazon A2I 与 Amazon
    SageMaker 和自定义 ML 模型一起使用。
- en: '![](assets/dsaw_1014.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1014.png)'
- en: Figure 10-14\. Amazon Augmented AI workflow to review model predictions.
  id: totrans-246
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-14\. Amazon 增强型 AI 工作流程，用于审核模型预测。
- en: In this example, Amazon Comprehend receives input data in a prediction request.
    We set a confidence threshold that defines when to involve the human reviewers.
    If the model’s prediction meets the confidence threshold, Amazon A2I will send
    the prediction result directly to the client application. In case the model is
    unable to make a high-confidence prediction, Amazon A2I sends the task to human
    reviewers.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，Amazon Comprehend 收到预测请求的输入数据。我们设置了一个置信度阈值，用于确定何时需要人工审核。如果模型的预测满足置信度阈值，Amazon
    A2I 将直接将预测结果发送给客户端应用程序。如果模型无法进行高置信度预测，Amazon A2I 将任务发送给人工审核员。
- en: In our example of classifying product reviews, a low-confidence prediction could
    wrongly classify negative reviews as neutral or positive reviews. Our business
    may be negatively affected if we do not have an automated way to fix these low-confidence
    predictions and improve our model.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们分类产品评论的示例中，置信度较低的预测可能会错误地将负面评论分类为中性或正面评论。如果我们没有自动修复这些置信度低的预测并改进我们的模型的方法，我们的业务可能会受到负面影响。
- en: We may also want to randomly audit a sample of all predictions—both low and
    high confidence. This could be important for models that make critical decisions,
    for example in the healthcare/medical sector. In such situations, we probably
    want to have humans review and audit high-confidence predictions as well to make
    sure the model performs correctly.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能也希望随机审核所有预测的样本，无论置信度高低。这对于做出关键决策的模型可能很重要，例如在医疗保健行业。在这种情况下，我们可能也希望让人类审查和审核置信度高的预测，以确保模型表现正确。
- en: Amazon A2I consolidates the human reviewer results and sends the final prediction
    response to the client application. Amazon A2I can also store the human review
    results in S3, which we could use as new training data.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon A2I 整合人工审核者的结果，并将最终的预测响应发送给客户端应用程序。Amazon A2I 还可以将人工审核结果存储在 S3 中，我们可以将其用作新的训练数据。
- en: 'Amazon A2I introduces a few new terms: Worker Task Template, Flow Definition,
    and Human Loop. The Worker Task Template defines the Human Task UI for the worker.
    This UI displays input data and instructions for workers. The Flow Definition
    defines the human review workflow. The definition contains the chosen workforce
    and provides information about how to accomplish the review task. The Human Loop
    represents the actual human review workflow. Once the human loop is triggered,
    Amazon A2I sends the human review tasks to the workers as specified in the flow
    definition.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon A2I 引入了几个新术语：Worker Task Template（工作人员任务模板）、Flow Definition（流程定义）和 Human
    Loop（人工循环）。Worker Task Template 定义了工作人员的人工任务 UI。此 UI 显示工作人员的输入数据和指令。Flow Definition
    定义了人工审核工作流程。该定义包含选择的工作人员，并提供有关如何完成审核任务的信息。Human Loop 表示实际的人工审核工作流程。一旦触发了人工循环，Amazon
    A2I 将按照流程定义将人工审核任务发送给工作人员。
- en: 'Let’s define some sample product reviews that we will send to Amazon Comprehend:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一些产品评论示例，我们将发送给 Amazon Comprehend：
- en: '[PRE35]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We also define a prediction confidence score threshold of 70%, which works
    well for our use case. If our model returns a prediction with a lower confidence
    score, Amazon A2I will trigger the human loop and our workforce team receives
    a task:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个预测置信度阈值为70%，这对我们的使用案例非常有效。如果我们的模型返回一个较低置信度的预测，亚马逊 A2I 将触发人工循环，我们的工作人员团队将收到一个任务：
- en: '[PRE36]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'If we run this code, we will see the following responses:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行此代码，我们将看到以下响应：
- en: '[PRE37]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We see that two predictions didn’t meet our confidence threshold and started
    human loops. When the assigned worker logs into the review system, the worker
    sees the submitted review tasks.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到两个预测未达到我们的置信度阈值，并启动了人工循环。当分配的工作人员登录审核系统时，工作人员可以看到提交的审核任务。
- en: With Augmented AI, we can choose between a public or private workforce. Public
    workforces integrate with the Amazon Mechanical Turk service with hundreds of
    thousands of human labelers that have been pre-screened by Amazon. We can also
    use third-party, pre-screened workforce providers listed on the AWS Marketplace.
    Or we create private workforces with co-workers or employees.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 借助增强型 AI，我们可以选择公共或私人工作人员。公共工作人员与亚马逊 Mechanical Turk 服务集成，拥有经过亚马逊预筛选的成千上万名人类标注者。我们还可以使用列在
    AWS Marketplace 上的第三方经过预筛选的工作人员提供者。或者我们可以创建私人工作人员，与同事或员工一起工作。
- en: The instructions are “Classify Reviews into Star Ratings Between 1 (Worst) and
    5 (Best).” The worker sees the input data “sometimes it works” and might classify
    this as a 3-star rating. Note that we can assign a single task to more than one
    human reviewer to mitigate human bias. Amazon A2I consolidates multiple responses
    per task using weighted reviewer scores. Once all review tasks are completed,
    the UI clears the task from the worker’s UI. We can use this newly labeled data
    in S3 to build a continuous pipeline for training and improving our Comprehend
    Custom model, as shown in [Figure 10-15](#continuous_training_pipeline_to_improve).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 指导指令是“将评价分类为介于1星（最差）和5星（最佳）之间的星级”。工作人员看到输入数据“有时候有效”，可能会将其分类为3星评级。请注意，我们可以将单个任务分配给多个人工审阅者，以减少人为偏见。Amazon
    A2I通过加权审阅者评分来整合每个任务的多个回应。一旦所有审阅任务完成，UI会从工作人员的界面中清除该任务。我们可以在S3中使用这些新标记的数据来构建用于训练和改进我们的Comprehend
    Custom模型的连续流水线，如[图 10-15](#continuous_training_pipeline_to_improve)所示。
- en: '![](assets/dsaw_1015.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1015.png)'
- en: Figure 10-15\. Continuous training pipeline to improve model predictions.
  id: totrans-263
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-15\. 用于改进模型预测的连续训练流水线。
- en: The more accurate our model becomes, the less reviews are sent to our workers.
    This concept is also called “active learning” and is implemented in SageMaker
    Ground Truth.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型变得越来越精确，发送给我们工作人员的评审越少。这个概念也被称为“主动学习”，在SageMaker Ground Truth中实现。
- en: Active-Learning Feedback Loops with SageMaker Ground Truth
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SageMaker Ground Truth的主动学习反馈循环
- en: Active learning starts with a human labeling workflow and then transitions to
    self-labeling after enough samples have been seen. The active learning feedback
    loop is used to continuously retrain the model and improve the confidence of future
    label predictions. Active learning helps to scale the data labeling process by
    handling the high-confidence predictions and free up the workforce to focus on
    the low-confidence predictions that require specialized human intelligence.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习始于人类标记工作流程，然后在看到足够样本后转向自动标记。主动学习反馈循环用于持续重新训练模型并提升未来标签预测的置信度。主动学习有助于通过处理高置信度预测来扩展数据标记过程，并释放工作力量专注于需要专业人工智能处理的低置信度预测。
- en: Amazon SageMaker Ground Truth is an Augmented AI workflow implementation for
    automatic data labeling. With enough data, SageMaker Ground Truth combines human
    review workflows with active learning. As the human workforce labels more and
    more data, SageMaker Ground Truth proactively trains a model to join the workforce
    and perform automated labeling of new data as it arrives. If the model is not
    confident, the data is sent to the human workforce for review. [Figure 10-16](#sagemaker_ground_truth_uses_active_lear)
    illustrates the SageMaker Ground Truth workflow and transition from manual to
    automated labeling.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker Ground Truth是用于自动数据标记的增强型AI工作流实现。随着数据量的增加，SageMaker Ground
    Truth将人类审阅工作流程与主动学习结合起来。随着人类工作力量对越来越多的数据进行标记，SageMaker Ground Truth积极训练模型以加入工作力量，并自动标记到达的新数据。如果模型信心不足，则将数据发送给人类工作力量进行审阅。[图 10-16](#sagemaker_ground_truth_uses_active_lear)说明了SageMaker
    Ground Truth的工作流程以及从手动标记到自动标记的过渡。
- en: '![](assets/dsaw_1016.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1016.png)'
- en: Figure 10-16\. SageMaker Ground Truth uses active learning to augment human
    data labeling.
  id: totrans-269
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-16\. SageMaker Ground Truth使用主动学习增强人类数据标记。
- en: SageMaker Ground Truth offers pre-built labeling workflows and task templates
    to process images, text, and video. We can also define a custom workflow. In the
    following example, we will create an active learning pipeline for images. SageMaker
    Ground Truth will actively create a new object detection model beneath the covers
    as it sees more and more human labels. SageMaker Ground Truth uses this new model
    to automatically detect objects in the images with increasing accuracy. This allows
    humans to focus on labeling images that are more difficult to classify. [Figure 10-17](#sample_worker_ui_in_ground_truth_to_det)
    shows a sample worker UI in SageMaker Ground Truth to detect and label objects
    in each image.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Ground Truth提供预构建的标记工作流和任务模板来处理图像、文本和视频。我们也可以定义一个自定义工作流。在下面的示例中，我们将为图像创建一个主动学习流水线。SageMaker
    Ground Truth在背后积极创建一个新的对象检测模型，随着看到越来越多的人工标签。SageMaker Ground Truth使用这个新模型来自动检测图像中的对象，并且随着精度的增加。这允许人类专注于标记更难分类的图像。[图 10-17](#sample_worker_ui_in_ground_truth_to_det)展示了SageMaker
    Ground Truth中用于检测和标记每个图像中对象的示例工作人员界面。
- en: '![](assets/dsaw_1017.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_1017.png)'
- en: Figure 10-17\. Sample worker UI in SageMaker Ground Truth.
  id: totrans-272
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-17\. SageMaker Ground Truth中的示例工作人员界面。
- en: Reduce Cost and Improve Performance
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少成本，提高性能
- en: Most pipeline orchestration engines support some type of step caching to avoid
    reexecuting steps that have not changed. This is called pipeline “step caching.”
    And because pipelines typically build upon other primitives such as SageMaker
    Training Jobs, we will highlight the Spot Instance cost savings for SageMaker
    Training Jobs used by our SageMaker Pipeline.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数管道编排引擎支持某种类型的步骤缓存，以避免重新执行未更改的步骤。这称为管道的“步骤缓存”。因为管道通常基于其他原语（如SageMaker训练作业），我们将强调Spot实例成本节省对我们的SageMaker管道使用的SageMaker训练作业的影响。
- en: Cache Pipeline Steps
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓存管道步骤
- en: In some cases, we can reuse the results of previously successful pipeline steps
    and avoid running the step again. SageMaker Pipelines supports step caching by
    checking for previously successful step executions for the same input artifacts
    and parameters. Other orchestrators support pipeline step caching as well, including
    Kubeflow Pipelines.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们可以重用先前成功的管道步骤的结果，并避免再次运行该步骤。SageMaker Pipelines通过检查相同输入工件和参数的先前成功步骤执行来支持步骤缓存。其他编排器也支持管道步骤缓存，包括Kubeflow
    Pipelines。
- en: 'To enable step caching in SageMaker Pipelines, we provide a cache configuration
    to each step upon creation, as shown in the following for the feature-engineering
    `ProcessingStep`. If SageMaker Pipelines detects that the raw dataset and processing
    parameters have not changed, SageMaker Pipelines will skip the step execution,
    reuse the generated BERT embeddings, and continue with the pipeline:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 要在SageMaker Pipelines中启用步骤缓存，我们在每个步骤创建时提供缓存配置，如下所示适用于特征工程`ProcessingStep`。如果SageMaker
    Pipelines检测到原始数据集和处理参数未发生变化，它将跳过步骤执行，重用生成的BERT嵌入，并继续管道的执行：
- en: '[PRE39]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Use Less-Expensive Spot Instances
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用成本较低的Spot实例
- en: 'SageMaker Pipelines build upon SageMaker primitives like Training Jobs, which
    support Spot Instances. We demonstrated how to enable Spot Instances for SageMaker
    Training Jobs in [Chapter 7](ch07.html#train_your_first_model). Remember to also
    enable checkpointing when training with Spot Instances, as shown in the following
    when defining our estimator:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Pipelines基于像训练作业这样的SageMaker原语构建，支持Spot实例。我们演示了如何在SageMaker训练作业中启用Spot实例，在[第7章](ch07.html#train_your_first_model)中展示了如何定义我们的估算器时也要启用检查点。
- en: '[PRE40]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Summary
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we described how effective machine learning pipelines help
    improve model quality and free up human resources to focus on higher-level tasks.
    We identified the key components to an effective machine learning pipeline, such
    as data-quality checks upon data ingestion and model validation after model training.
    We demonstrated how to orchestrate pipelines using SageMaker Pipelines and various
    other options, including AWS Step Functions, Kubeflow Pipelines, Apache Airflow,
    MLflow, and TFX.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们描述了如何通过有效的机器学习管道提高模型质量，并释放人力资源专注于更高级别的任务。我们确定了有效机器学习管道的关键组成部分，如数据摄取时的数据质量检查和模型训练后的模型验证。我们演示了如何使用SageMaker
    Pipelines和其他各种选项（包括AWS Step Functions、Kubeflow Pipelines、Apache Airflow、MLflow和TFX）来编排管道。
- en: We showed how to implement pipeline automation with SageMaker Pipelines. We
    discussed event-based triggers such as code commits and new data arriving to S3
    to start a pipeline execution. And we learned how to set up time-based schedules
    and statistical triggers to automatically run a pipeline execution. We showed
    how to use human-in-the-loop workflows to automate data labeling, how to improve
    model accuracy using Amazon Augmented AI, and how to implement active-learning
    feedback loops with SageMaker Ground Truth.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了如何使用SageMaker Pipelines实现管道自动化。我们讨论了基于事件的触发器，如代码提交和新数据到达S3以启动管道执行。我们学习了如何设置基于时间的调度和统计触发器来自动运行管道执行。我们展示了如何使用人在环路工作流自动化数据标记，如何使用Amazon增强AI提高模型准确性，以及如何使用SageMaker
    Ground Truth实现主动学习反馈循环。
- en: With this knowledge on how to create repeatable and automated pipelines, we
    are now fully equipped to move our data science projects from experimentation
    into production. We increase productivity and ensure repeatability by automating
    all steps in the model development and model deployment workflow. We improve reliability
    by implementing GitOps practices to enforce consistency and quality. And we achieve
    auditability by keeping track of all pipeline steps and executions with SageMaker
    Experiments and input/output artifacts with ML Lineage Tracking. We can also maintain
    high-quality models by automatically checking for changes to the statistical properties
    of our datasets, models, predictions, and explanations.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 通过掌握如何创建可重复和自动化的流水线，我们现在已经完全准备好将我们的数据科学项目从实验阶段转移到生产阶段。通过自动化模型开发和模型部署工作流中的所有步骤，我们提高了生产力并确保了可重复性。通过实施
    GitOps 实践来强化一致性和质量，我们提升了可靠性。通过使用 SageMaker 实验追踪所有流水线步骤和执行情况以及 ML Lineage 追踪输入/输出工件，我们实现了可审计性。我们还可以通过自动检测数据集、模型、预测和解释的统计属性变化来维护高质量的模型。
- en: In [Chapter 11](ch11.html#streaming_analytics_and_machine_lear), we extend our
    analytics and machine learning to streaming data. We will calculate real-time
    summary statistics, detect anomalies, and train models on continuous streams of
    product review data.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第11章](ch11.html#streaming_analytics_and_machine_lear)中，我们将我们的分析和机器学习扩展到流数据。我们将计算实时摘要统计信息，检测异常，并在连续的产品评论数据流上训练模型。
