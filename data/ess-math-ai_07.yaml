- en: 'Chapter 7\. Natural Language And Finance AI: Vectorization And Time Series'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。自然语言和金融人工智能：向量化和时间序列
- en: '*They. Can. Read.*'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*他们。能。阅读。*'
- en: 'One of the hallmarks of human intelligence is our mastery of language at a
    very early age: Comprehension of written and spoken language, written and spoken
    expression of thoughts, conversation between two or more people, translation from
    one language to another, and the use of language to express empathy, convey emotions,
    and process visual and audio data perceived from our surroundings. Leaving the
    philosophical question of consciousness aside, if machines acquire the ability
    to perform these language tasks, deciphering the intent of words, at a level similar
    to humans, or above humans, then it is a major propeller towards general artificial
    intelligence. These tasks fall under the umbrellas of *natural language processing*,
    *computational linguistics*, *machine learning* and/or *probabilistic language
    modeling*. These fields are vast and it is easy to find interested people wandering
    aimlessly in a haze of various models with big promises. We should not get lost.
    The aim of this chapter is to lay out the natural processing field all at once
    so we can have a bird’s eye view without getting into the weeds.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 人类智慧的一个标志是我们在很小的时候就掌握了语言：理解书面和口头语言，书面和口头表达思想，两个或更多人之间的对话，从一种语言到另一种语言的翻译，以及使用语言表达同理心、传达情感，并处理从周围环境中感知到的视觉和音频数据。撇开意识的哲学问题不谈，如果机器能够像人类一样或超过人类一样，以类似于人类的水平执行这些语言任务，解读单词的意图，那么这将是通往通用人工智能的重要推动力。这些任务属于*自然语言处理*、*计算语言学*、*机器学习*和/或*概率语言建模*的范畴。这些领域广阔，很容易让人们在各种充满大量承诺的模型中徘徊。我们不应迷失方向。本章的目的是一次性展示自然处理领域，以便我们可以一览全局而不深入细节。
- en: 'The following questions guide us at all times:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 以下问题始终指导我们：
- en: What type of task is at hand? In other words, what is our goal?
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 手头上是什么类型的任务？换句话说，我们的目标是什么？
- en: What type of data is at hand? What type of data do we need to collect?
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 手头上是什么类型的数据？我们需要收集什么类型的数据？
- en: What state of the art models are out there that deal with similar tasks and
    similar types of data? If there are none, then we have to come up with the models
    ourselves.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有哪些最先进的模型处理类似任务和类似类型的数据？如果没有，那么我们必须自己设计模型。
- en: How do we train these models? In what formats do they consume their data? In
    what formats do they produce their outputs? Do they have a training function,
    loss function (or objective function), and optimization structure?
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何训练这些模型？它们以什么格式消耗数据？它们以什么格式产生输出？它们是否有训练函数、损失函数（或目标函数）和优化结构？
- en: What are the advantages and disadvantages of various models versus others?
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 各种模型相对于其他模型的优缺点是什么？
- en: Are there Python packages or libraries available for their implementation? Luckily,
    nowadays, most models come out accompanied with their Python implementations and
    very simple APIs (application programming interfaces). Even better, there are
    many pre-trained models available to download and ready for use in applications.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是否有Python包或库可用于实现它们？幸运的是，如今，大多数模型都伴随着它们的Python实现和非常简单的API（应用程序编程接口）一起发布。更好的是，有许多预训练模型可供下载并准备在应用中使用。
- en: How much computational infrastructure do we need in order to train and/or deploy
    these models?
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了训练和/或部署这些模型，我们需要多少计算基础设施？
- en: Can we do better? There is always room for improvement.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们能做得更好吗？总有改进的空间。
- en: We also need to *extract the math from the best performing models*. Thankfully,
    this is the easy part, since similar mathematics underlies many models, even when
    relating to different types of tasks or from dissimilar application areas, such
    as predicting the next word in a sentence or predicting the stock market behavior.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要*从表现最佳的模型中提取数学*。幸运的是，这是容易的部分，因为许多模型都有相似的数学基础，即使涉及不同类型的任务或来自不同的应用领域，比如预测句子中的下一个单词或预测股市行为。
- en: 'The state of the art models that we intend to cover in this chapter are:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们打算在本章中介绍的最先进模型有：
- en: 'Transformers or attention models (since 2017). The important math here is extremely
    simple: The dot product between two vectors.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变压器或注意力模型（自2017年起）。这里的重要数学非常简单：两个向量之间的点积。
- en: Recurrent long short term memory neural networks (since 1995). The important
    math here is *backpropagation in time*. We covered backpropagation in [Chapter 4](ch04.xhtml#ch04),
    but for recurrent nets, we take the derivatives with respect to time.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归长短期记忆神经网络（自1995年起）。这里的重要数学是*时间反向传播*。我们在[第4章](ch04.xhtml#ch04)中介绍了反向传播，但对于递归网络，我们要对时间进行导数。
- en: Convolutional neural networks (since 1989) for time series data. The important
    math is the *convolution* operation, which we covered in [Chapter 5](ch05.xhtml#ch05).
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络（自1989年起）用于时间序列数据。重要的数学是*卷积*操作，我们在[第5章](ch05.xhtml#ch05)中介绍过。
- en: These models are very well suited for *time series data*, that is, data that
    appears sequentially with time. Examples of time series data are movies, audio
    files such as music and voice recordings, financial markets data, climate data,
    dynamical systems data, documents, and books.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型非常适用于*时间序列数据*，即随时间顺序出现的数据。时间序列数据的例子包括电影、音频文件（如音乐和语音录音）、金融市场数据、气候数据、动态系统数据、文件和书籍。
- en: 'We might wonder why documents and books can be considered as time dependent,
    even though they have already been written and are just *there*. How come an image
    is not time dependent but a book, and in general, reading and writing, are? The
    answer is simple:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会想知道为什么文件和书籍可以被视为时间相关，即使它们已经被写成并且只是*存在*。为什么图像不是时间相关，但书籍，以及一般来说，阅读和写作是？答案很简单：
- en: When we read a book, we comprehend what we read one word at a time, then one
    phrase at a time, then one sentence at a time, then one paragraph at a time, and
    so on. This is how we grasp the concepts and topics of the book.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们阅读一本书时，我们逐个理解我们读到的每个单词，然后逐个理解每个短语，然后逐个理解每个句子，然后逐个理解每个段落，依此类推。这是我们理解书中概念和主题的方式。
- en: The same is true when we write a document, outputting one word at time, even
    though the whole idea we are trying to express is already there, *encoded*, before
    we output the words, *sequentially*, on paper.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们写文档时，即使我们尝试表达的整个想法已经*编码*在那里，我们也是逐个输出一个单词，*顺序地*在纸上输出。
- en: When we caption an image, the image itself is not time dependent, but our captioning
    (the output) is.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们给图像加标题时，图像本身不是时间相关的，但我们的标题（输出）是。
- en: When we summarize an article, answer a question, or translate from one language
    to another, the output text is time dependent. The input text could be time dependent
    if processed using a recurrent neural network, or stationary if processed all
    at once using a transformer or a convolutional model.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们总结一篇文章、回答一个问题或从一种语言翻译到另一种语言时，输出文本是时间相关的。如果使用循环神经网络处理，输入文本可能是时间相关的，如果使用transformer或卷积模型一次处理所有文本，则是静态的。
- en: Until 2017, the most popular machine learning models to process time series
    data were based either on *convolutional neural neworks* or on *recurrent neural
    networks with long short term memory*. In 2017, *transformers* took over, abandoning
    recurrence altogether in certain application areas. The question whether recurrent
    neural networks are obsolete is out there, but with things changing everyday in
    the AI field, who knows which models die and which models survive the test of
    time. Moreover, recurrent neural networks power many AI engines and are still
    subjects of active research.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 直到2017年，处理时间序列数据的最流行的机器学习模型要么基于*卷积神经网络*，要么基于*具有长短期记忆的循环神经网络*。2017年，*transformers*取代了它们，在某些应用领域完全放弃了循环。关于循环神经网络是否过时的问题存在，但在人工智能领域每天都在发生变化，谁知道哪些模型会消亡，哪些模型会经受时间的考验。此外，循环神经网络驱动许多人工智能引擎，仍然是积极研究的对象。
- en: 'In this chapter, we answer the following questions:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回答以下问题：
- en: How do we transform natural language text to numerical quantities that retain
    meaning? Our machines only understand numbers, and we need to process natural
    language using these machines. We must *vectorize* our samples of text data, or
    *embed* them into finite dimensional vector spaces.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何将自然语言文本转换为保留含义的数值量？我们的机器只能理解数字，我们需要使用这些机器处理自然语言。我们必须*向量化*我们的文本数据样本，或者将它们*嵌入*到有限维向量空间中。
- en: How do we lower the dimension of the vectors from the enormous ones initially
    required to represent natural language? For example, the French language has around
    135,000 distinct words, how do we get around having to one-hot code words in a
    French sentence using vectors of 135,000 entries each?
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何将最初需要的巨大向量的维度降低到表示自然语言所需的维度？例如，法语有大约135,000个不同的单词，我们如何避免使用每个包含135,000个条目的向量对法语句子进行独热编码？
- en: Does the model at hand consider (as its input and/or output) our natural language
    data as a time dependent sequence fed into it one term at a time, or a stationary
    vector consumed all at once?
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 手头的模型是否将我们的自然语言数据（作为输入和/或输出）视为一个接一个地输入的时间相关序列，还是一次消耗所有的静态向量？
- en: How exaclty do various models for natural language processing work?
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 各种自然语言处理模型究竟是如何工作的？
- en: Why is there finance in this chapter as well?
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么本章中也涉及金融？
- en: Along the way, we discuss the types of natural language and finance applications
    that our models are well suited for. We keep the focus on the mathematics and
    not the programming, since such models (especially for language applications)
    require substantive computational infrastructures. For example, [DeepL Translator](https://www.deepl.com/en/translator)
    generates its translations using a supercomputer operated with hydropower from
    Iceland, which reaches 5.1 petaflops. We also note that the AI-specialized chip
    industry is booming, led by Nvidia, Google’s Tensor Processing Unit, Amazon’s
    Inferentia, AMD’s Instinct GPU, and startups like Cerebras and Graphcore. While
    conventional chips have struggled to keep pace with Moore’s Law, which predicted
    a doubling of processing power every 18 months, AI-specialized chips have outpaced
    this law by a wide margin.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论过程中，我们将讨论我们的模型适用于哪些类型的自然语言和金融应用。我们将重点放在数学上，而不是编程，因为这些模型（特别是语言应用）需要实质性的计算基础设施。例如，[DeepL
    Translator](https://www.deepl.com/en/translator)使用冰岛水力发电操作的超级计算机生成翻译，达到5.1 petaflops。我们还注意到，AI专用芯片行业正在蓬勃发展，由Nvidia、谷歌的Tensor
    Processing Unit、亚马逊的Inferentia、AMD的Instinct GPU以及Cerebras和Graphcore等初创公司领导。尽管传统芯片难以跟上摩尔定律，即每18个月处理能力翻倍，但AI专用芯片已经远远超过了这一定律。
- en: Even though we do not write code for this chapter, we note that most programming
    can be accomplished using Python’s TensorFlow and Keras libraries.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在本章中没有编写代码，但我们注意到大多数编程可以使用Python的TensorFlow和Keras库完成。
- en: Throughout our discussion below, we have to be mindful of whether we are in
    the *training* phase of a model or in the *prediction* phase (using the pre-trained
    model to do tasks). Moreover, it is important to differentiate whether our model
    needs labeled data to be trained, such as English sentences *along with* their
    French translations as labels, or can learn from unlabeled data, such as computing
    *the meanings* of words from their contexts.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的讨论中，我们必须注意我们是在模型的*训练*阶段还是在*预测*阶段（使用预先训练的模型执行任务）。此外，重要的是区分我们的模型是否需要标记数据进行训练，例如英语句子*以及*它们的法语翻译作为标签，或者可以从未标记的数据中学习，例如从上下文中计算单词的*含义*。
- en: Natural Language AI
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言人工智能
- en: 'Natural language processing applications are ubiquitous. This technology has
    been integrated into many aspects of our lives that we just take it for granted,
    when using apps on our smartphones, digital calendars, digital home assistants,
    Siri, Alexa, and others. The following list is partially adapted from the excellent
    book: *Natural Language Processing In Action* by Lane, Howard, and Hapke. It demonstrates
    how indispencable natural language processing has become:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理应用程序是无处不在的。这项技术已经融入到我们生活的许多方面，当我们在智能手机上使用应用程序、数字日历、数字家庭助手、Siri、Alexa等时，我们往往认为这是理所当然的。以下列表部分改编自优秀书籍《自然语言处理实战》（Lane、Howard和Hapke）。它展示了自然语言处理已经变得不可或缺的程度：
- en: 'Search and information retrieval: Web, documents, autocomplete, chatbots'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索和信息检索：网络，文档，自动完成，聊天机器人
- en: 'Email: Spam filter, email classification, email prioritization'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子邮件：垃圾邮件过滤器，电子邮件分类，电子邮件优先级
- en: 'Editing: Spelling check, grammar check, style recommendation'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编辑：拼写检查，语法检查，风格建议
- en: 'Sentiment Analysis: Product reviews, customer care, monitoring of community
    morale'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析：产品评论，客户关怀，社区士气监测
- en: 'Dialog: Chatbots, digital assistants such as Amazon’s Alexa, scheduling'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对话：聊天机器人，数字助手如亚马逊的Alexa，调度
- en: 'Writing: Indexing, concordance, table of contents'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 写作：索引，一致性，目录
- en: 'Text mining: Summarization, knowledge extraction such as mining election campaigns’
    finance and natural language data (finding connections between political donors),
    resume to job matching, medical diagnosis'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本挖掘：摘要，知识提取，如挖掘选举活动的财务和自然语言数据（找到政治捐赠者之间的联系），简历与工作匹配，医学诊断
- en: 'Law: Legal inference, precedent search, subpoena classification'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 法律：法律推理，先例搜索，传票分类
- en: 'News: Event detection, fact checking, headline composition'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新闻：事件检测，事实核查，标题撰写
- en: 'Attribution: Plagiarism detection, literary forensics, style coaching'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归因：抄袭检测，文学取证，风格指导
- en: 'Behavior prediction: Finance applications, election forecasting, marketing'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行为预测：金融应用，选举预测，营销
- en: 'Creative Writing: Movie scripts, poetry, song lyrics, bot powered financial
    and sports news stories'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创意写作：电影剧本，诗歌，歌词，机器人驱动的金融和体育新闻报道
- en: 'Captioning: Computer vision combined with natural language processing'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字幕：计算机视觉结合自然语言处理
- en: 'Translation: Google Translate and DeepL Translate.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 翻译：谷歌翻译和DeepL翻译。
- en: 'Even though the past decade has brought impressive feats, machines are still
    nowhere close to mastering natural language. The processes involved are tedious,
    requiring attentive statistical bookkeeping, and substansive *memory*, the same
    way humans require memory to master languages. The point here is: There is plenty
    of room for new innovations and contributions to the field.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管过去的十年取得了令人印象深刻的成就，但机器仍然远远没有掌握自然语言。涉及的过程是繁琐的，需要注意的统计记账和实质性的记忆，就像人类需要记忆来掌握语言一样。这里的重点是：在这个领域有很多新的创新和贡献的空间。
- en: Language models have recently shifted from hand coded to data driven. They do
    not implement hard coded logical and grammar rules. Instead, they rely on detecting
    the statistical relationships between words. Even though there is a school of
    thought in linguistics that asserts grammar as an innate property for humans,
    or in other words, *hardcoded* into our brains, humans have a striking ability
    to master new languages without ever encountering any grammatical rules for these
    languages. From personal experience, attempting to learn the grammar of a new
    language seems to impede the learning process, but do not quote me on that.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型最近已经从手工编码转变为数据驱动。它们不实现硬编码的逻辑和语法规则。相反，它们依赖于检测单词之间的统计关系。尽管在语言学中有一种思想认为语法是人类的固有属性，或者换句话说，硬编码到我们的大脑中，人类有一种惊人的能力，可以掌握新语言，而从未遇到过这些语言的任何语法规则。从个人经验来看，尝试学习一门新语言的语法似乎会妨碍学习过程，但请不要引用我说的话。
- en: One major challenge is that data for natural language is extremely high dimensional.
    There are millions of words across thousands of languages. There are huge corpuses
    of documents, such as entire collections of authors’ works, billions of tweets,
    wikipedia articles, news articles, Facebook comments, movie reviews, *etc*. A
    first goal is then to reduce the number of dimensions for efficient storage, processing,
    and computation, while at the same time avoiding the loss of essential information.
    This has been a common theme in the AI field, and one cannot help but wonder how
    many mathematical innovations would have never seen the light of the day had we
    possessed unlimited storage and computational infrastructures.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一个主要挑战是自然语言数据的维度极高。有数以百万计的单词跨越数千种语言。有大量的文档语料库，如整个作者作品集，数十亿条推特，维基百科文章，新闻文章，Facebook评论，电影评论等等。第一个目标是减少维度以实现高效存储、处理和计算，同时避免丢失关键信息。这在人工智能领域是一个常见主题，人们不禁要想，如果我们拥有无限的存储和计算基础设施，多少数学创新将永远不会见天日。
- en: Preparing Natural Language Data For Machine Processing
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为机器处理自然语言数据做准备
- en: 'For a machine to process any natural language task, the first thing it must
    do is to break down text and organize it into building blocks that retain meaning,
    intent, context, topics, information, and sentiments. To this end, it must establish
    a correspondence between words and number tags, using processes called *tokenizing*,
    *stemming* (such as giving singular words and their plural variation the same
    token), *lemmatization* (associating several words of similar meaning together),
    *case normalization* (such as giving capitalized and lower case words of same
    spelling the same tokens) and others. This correspondence is not for individual
    characters that make up words, but for full words, pairs or more of words (*2-grams
    or n-grams*), punctuations, significant capitalizations, *etc.*, that carry meaning.
    This creates a *vocabulary* or a *lexicon* of numerical tokens corresponding to
    a given corpus of natural language documents. A vocabulary or a lexicon in this
    sense is similar to a Python dictionary: Each individual natural language *building
    block object* has a unique token.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器来处理任何自然语言任务，它必须首先对文本进行分解，并将其组织成保留意义、意图、上下文、主题、信息和情感的构建块。为此，它必须建立单词和数字标签之间的对应关系，使用称为*标记化*、*词干提取*（例如给予单词及其复数变体相同的标记）、*词形还原*（将意思相近的几个单词关联在一起）、*大小写规范化*（例如给予拼写相同的大写和小写单词相同的标记）等过程。这种对应关系不是针对构成单词的个别字符，而是针对完整的单词、成对或更多的单词（*2-gram或n-gram*）、标点符号、重要的大写等携带意义的内容。这创建了一个与给定自然语言文档语料库对应的数字标记的*词汇表*或*词典*。在这种意义上，词汇表或词典类似于Python字典：每个单独的自然语言*构建块对象*都有一个唯一的标记。
- en: An *n-gram* is a sequence of *n* words that carry different meaning when kept
    ordered together than when each word is floating on its own. For example, a 2-gram
    is a couple of words that come together and it would change the meaning if we
    unpair them, such as *ice cream* or *was not*, so the whole 2-gram gets one numerical
    token, retaining the meaning of the two words within their correct context. Similarly,
    a 3-gram is a triplet of ordered words, such as *John F. Kennedy*; and so on.
    A *parser* for natural language is the same as a compiler for computers. Do not
    worry if these new terms confuse you. For our mathematical purposes, all we need
    are the numerical tokens associated with unique words, *n-grams* , emojis, punctuations,
    *etc.*, and the resulting *vocabulary* for a corpus of natural language documents.
    These are saved in dictionary like objects, allowing us to flip back and forth
    easily between text and numerical tokens.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*n-gram*是一系列按顺序排列的*n*个单词，当它们一起保持有序时，其含义与每个单词单独浮动时不同。例如，一个2-gram是一对单词，如果我们将它们分开，如*冰淇淋*或*不是*，它们的含义会发生变化，因此整个2-gram会得到一个数字标记，保留两个单词在正确上下文中的含义。类似地，一个3-gram是一组有序的三个单词，例如*约翰·F·肯尼迪*；依此类推。自然语言的*解析器*与计算机的编译器相同。如果这些新术语让您感到困惑，不要担心。对于我们的数学目的，我们只需要与唯一单词、*n-gram*、表情符号、标点符号、*等*相关联的数字标记，以及自然语言文档语料库的结果*词汇表*。这些保存在类似字典的对象中，使我们可以轻松地在文本和数字标记之间来回切换。
- en: We leave the actual details of tokenizing, stemming, lemmatization, parsing,
    and other natural language data preparations for computer scientists and their
    collaborations with linguists. In fact, collaboration with linguists has become
    less important as the models mature in their ability to detect patterns directly
    from the data, thus, the need for coding hand crafted linguistic rules into natural
    language models has diminished. Note also that not all natural language pipelines
    include stemming and lemmatization. They all, however, involve tokenizng. The
    quality of tokenizing text data is crucial for the performance of our natural
    language pipeline. It is the first step containing fundamental building blocks
    representing the data that we feed into our models. The quality of both the data
    and the way it is tokenized affects the outputs of the entire natural language
    processing pipeline. For your production applications, use *spaCy* parser, which
    does sentence segmentation, tokenization, and multiple other things in one pass.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将标记化、词干提取、词形还原、解析以及其他自然语言数据准备的实际细节留给计算机科学家和他们与语言学家的合作。事实上，随着模型在直接从数据中检测模式的能力成熟，与语言学家的合作变得不那么重要，因此，将手工制作的语言规则编码到自然语言模型中的需求已经减少。还要注意，并非所有自然语言流水线都包括词干提取和词形还原。但它们都涉及到标记化。对文本数据进行标记化的质量对我们自然语言流水线的性能至关重要。这是包含我们输入模型的数据的基本构建块的第一步。数据和标记化方式的质量会影响整个自然语言处理流水线的输出。对于生产应用程序，请使用*spaCy*解析器，它可以在一次处理中进行句子分割、标记化和多种其他操作。
- en: After tokenizing and possessing a healthy vocabulary (the collection of numerical
    tokens and the entities they correspond to in the natural language text), we need
    to represent entire natural language documents using vectors of numbers. These
    documents can range from very long, such as a book series, to very short, such
    as a Twitter tweet or a simple search query for Google Search or DuckDuckGo. We
    can then express a corpus of one million documents as a collection of one million
    numerical vectors, or a matrix with one million columns. These columns will be
    as long as our chosen vocabulary, or shorter if we decide to *compress* these
    documents further. In linear algebra language, the length of these vectors is
    the dimension of our *vector space* that our documents are *embedded in*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在对文本进行标记化和拥有丰富的词汇表（数字标记的集合以及它们在自然语言文本中对应的实体）之后，我们需要使用数字向量来表示整个自然语言文档。这些文档可以非常长，比如一本书系列，也可以非常短，比如一条Twitter推文或者一个简单的Google搜索或DuckDuckGo搜索查询。然后，我们可以将一百万个文档的语料库表示为一百万个数字向量的集合，或者一个具有一百万列的矩阵。这些列的长度将与我们选择的词汇表一样长，或者如果我们决定进一步*压缩*这些文档，则会更短。在线性代数语言中，这些向量的长度是我们的文档所*嵌入*的*向量空间*的维度。
- en: 'The whole point of the above process is to obtain numerical vector representations
    of our documents so that we can do math on them: Comes linear algebra with its
    arsenal of linear combinations, projections, dot products, and singular value
    decompositions. There is, however, one caveat: For natural language applications,
    the lengths of the vectors representing our documents, or the size of our vocabulary,
    are prohibitively enormous to do any useful computations with. The curse of dimensionality
    becomes a real thing.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 上述过程的整个目的是获得我们文档的数值向量表示，以便我们可以对其进行数学运算：线性代数带来了一系列线性组合、投影、点积和奇异值分解。然而，有一个警告：对于自然语言应用，表示我们文档的向量的长度，或者我们词汇表的大小，是非常庞大的，无法进行任何有用的计算。维度的诅咒成为了一个真实存在的问题。
- en: 'Note: The Curse Of Dimensionality'
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注：维度的诅咒
- en: Vectors become exponentially farther apart in terms of Euclidean distance as
    the number of dimensions increases. One natural language example is sorting documents
    based on their *distance* from another document, such as a search query. This
    simple operation becomes impractical when we go above 20 dimensions or so, if
    we use the Euclidean distance to measure the *closeness* of documents (see [Wikipedia’s
    Curse Of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)
    for more details). Thus, for natural language applications, we must use another
    measure for distance between documents. We will discuss *cosine similarity* shortly,
    which measures the *angle* between two document vectors, as opposed to their Euclidean
    distance.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 随着维度数量的增加，向量之间的欧几里德距离呈指数增长。一个自然语言的例子是根据它们与另一个文档的*距离*对文档进行排序，比如搜索查询。当我们超过20个维度时，如果使用欧几里德距离来衡量文档的*接近程度*，这种简单操作就变得不切实际了（有关更多详细信息，请参阅[Wikipedia的维度诅咒](https://en.wikipedia.org/wiki/Curse_of_dimensionality)）。因此，对于自然语言应用，我们必须使用另一种衡量文档之间距离的方法。我们将很快讨论*余弦相似度*，它衡量两个文档向量之间的*角度*，而不是它们的欧几里德距离。
- en: Therefore, a main driver for natural language processing models is to represent
    these documents using shorter vectors that convey the main topics and retain meaning.
    Think how many unique tokens or combinations of tokens we have to use in order
    to represent this book while at the same time preserving its most important information.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，自然语言处理模型的主要驱动因素是使用较短的向量来表示这些文档，传达主要主题并保留含义。想想我们必须使用多少个唯一标记或标记组合来表示这本书，同时又保留其最重要的信息。
- en: 'To summarize, our natural language processing pipeline proceeds as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们的自然语言处理流程如下：
- en: From text to numerical tokens, then to an acceptable vocabulary for an entire
    corpus of documents.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从文本到数字标记，然后到整个文档语料库的可接受词汇。
- en: From documents of tokens to high dimensional vectors of numbers.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从标记文档到高维数字向量。
- en: From high dimensional vectors of numbers to lower dimensional vectors of topics
    using techniques like *direct projection onto a smaller subset of the vocabulary
    space* (just dropping part of the vocabulary, making the corresponding entries
    zero), *latent semantic analysis* (projecting onto special vectors determined
    by special linear combinations of the document vectors), *Word2Vec*, *Doc2Vec*,
    *thought vectors*, *Dirichlet allocation*, and others. We dicuss these shortly.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从高维数字向量到使用*直接投影到词汇空间的较小子集*（只是丢弃词汇的一部分，使相应的条目为零）、*潜在语义分析*（投影到由文档向量的特殊线性组合确定的特殊向量）、*Word2Vec*、*Doc2Vec*、*思维向量*、*Dirichlet分配*等技术生成的主题的较低维向量。我们会简要讨论这些。
- en: As it is usually the case in mathematical modeling, there is more than one way
    to represent a given document as a vector of numbers. It is us who decide on the
    vector space which our documents inhabit, or get *embedded in*. Each vector representation
    has advantages and disadvantages, depending on the goal of our natural language
    task. Some are simpler than others too.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在数学建模中，有多种方法可以将给定文档表示为一组数字的向量。我们决定我们的文档所在的向量空间，或者被*嵌入*的向量空间。每种向量表示都有优点和缺点，取决于我们自然语言任务的目标。有些比其他的简单。
- en: Statistical Models And The *log* Function
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 统计模型和*log*函数
- en: When representing a document as a vector of numbers starts with counting the
    number of times certain terms appear in the document, then our document vectorizing
    model is *statistical*, since it is frequency based.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当将文档表示为一组数字的向量时，首先要计算文档中某些术语出现的次数，那么我们的文档向量化模型是*统计*的，因为它是基于频率的。
- en: When we deal with term frequencies, it is better to apply the *log* function
    to our counts as opposed to using raw counts. The *log* function is advantageous
    when we deal with quantities that could get extremely large, extremely small,
    or could have extreme variations in scale. Viewing these extreme counts or variations
    within a logarithmic scale brings them back to the normal realm.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理术语频率时，最好将*log*函数应用于我们的计数，而不是使用原始计数。当处理可能变得极大、极小或在规模上有极端变化的数量时，*log*函数是有利的。在对数尺度内查看这些极端计数或变化将它们带回正常范围。
- en: For example, the number <math alttext="10 Superscript 23"><msup><mn>10</mn>
    <mn>23</mn></msup></math> is huge, but <math alttext="log left-parenthesis 10
    Superscript 23 Baseline right-parenthesis equals 23 log left-parenthesis 10 right-parenthesis"><mrow><mo
    form="prefix">log</mo> <mrow><mo>(</mo> <msup><mn>10</mn> <mn>23</mn></msup> <mo>)</mo></mrow>
    <mo>=</mo> <mn>23</mn> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>10</mn>
    <mo>)</mo></mrow></mrow></math> is not. Similarly, if the term *shark* appears
    in two documents of a corpus of 20 million documents (20 million/2=10 million),
    and the term *whale* appears in twenty documents of this corpus (20 million/20=1
    million), then that is a 9 million difference, which seems excessive for terms
    that appeared in two and twenty documents respectively. Computing the same quantities
    but on a *log* scale, we get *7log(10)* and *6log(10)* respectively (it doesn’t
    matter which *log* base we use), which doesn’t seem excessive anymore, and more
    in line with the terms’ occurence in the corpus.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，数字<math alttext="10 Superscript 23"><msup><mn>10</mn> <mn>23</mn></msup></math>是巨大的，但<math
    alttext="log left-parenthesis 10 Superscript 23 Baseline right-parenthesis equals
    23 log left-parenthesis 10 right-parenthesis"><mrow><mo form="prefix">log</mo>
    <mrow><mo>(</mo> <msup><mn>10</mn> <mn>23</mn></msup> <mo>)</mo></mrow> <mo>=</mo>
    <mn>23</mn> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>10</mn> <mo>)</mo></mrow></mrow></math>不是。同样，如果术语*shark*出现在语料库中的两个文档中（2000万/2=1000万），而术语*whale*出现在这个语料库的二十个文档中（2000万/20=100万），那么这是一个900万的差异，对于分别出现在两个和二十个文档中的术语来说，这似乎过多。在*对数*尺度上计算相同的数量，我们分别得到*7log(10)*和*6log(10)*（无论使用哪种*对数*基数），这看起来不再过多，更符合语料库中术语的出现情况。
- en: The need for using the *log* function when dealing with word counts in particular
    is reinforced by *Zipf law*. This law says that term counts in a corpus of natural
    language naturally follow a power law, so it is best to temper that with a log
    function, transforming differences in term frequencies into a linear scale. We
    discuss this next.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理特定的词频时，使用*对数*函数的必要性特别受到*齐普夫定律*的支持。这个定律说，自然语言语料库中的术语计数自然遵循幂律，因此最好用对数函数来调节，将术语频率的差异转化为线性刻度。接下来我们将讨论这一点。
- en: Zipf’s Law For Term Counts
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语计数的齐普夫定律
- en: '[*Zipf’s law*](https://en.wikipedia.org/wiki/Zipf%27s_law) for natural language
    has to do with word counts. It is very interesting and so surprising that I am
    tempted to try and see if it applies to my own book. It is hard to imagine that
    as I a write each word in this book, my unique word counts are actually following
    some law. Are we, along with the way we word our ideas and thoughts, *that predictable*?
    It turns out that Zipf’s law extends to counting many things around us, not only
    words in documents and corpuses.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 齐普夫定律与自然语言中的词频有关。它非常有趣，令人惊讶，以至于我忍不住想尝试看看它是否适用于我的书。很难想象，当我在这本书中写下每个词时，我的独特词频实际上遵循了某种规律。我们连同我们表达思想和观点的方式，是*那么可预测*吗？事实证明，齐普夫定律适用于我们周围许多事物的计数，不仅仅是文档和语料库中的词语。
- en: '***Zipf’s Law***: *For a corpus of natural language where the terms have been
    ordered according to their frequencies, the frequency of the first item is twice
    as that of the second item, three times as the third item, and so on.* That is,
    the frequency with which an item appears in a corpus is related to its ranking:
    <math alttext="f 1 equals 2 f 2 equals 3 f 3 equals period period period"><mrow><msub><mi>f</mi>
    <mn>1</mn></msub> <mo>=</mo> <mn>2</mn> <msub><mi>f</mi> <mn>2</mn></msub> <mo>=</mo>
    <mn>3</mn> <msub><mi>f</mi> <mn>3</mn></msub> <mo>=</mo> <mo>.</mo> <mo>.</mo>
    <mo>.</mo></mrow></math>'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '***齐普夫定律***：*对于一个自然语言语料库，其中术语根据它们的频率排序，第一项的频率是第二项的两倍，第三项的三倍，依此类推。*也就是说，一个项目在语料库中出现的频率与其排名有关：<math
    alttext="f 1 equals 2 f 2 equals 3 f 3 equals period period period"><mrow><msub><mi>f</mi>
    <mn>1</mn></msub> <mo>=</mo> <mn>2</mn> <msub><mi>f</mi> <mn>2</mn></msub> <mo>=</mo>
    <mn>3</mn> <msub><mi>f</mi> <mn>3</mn></msub> <mo>=</mo> <mo>.</mo> <mo>.</mo>
    <mo>.</mo></mrow></math>'
- en: 'We can verify if Zipf’s law applies by plotting the frequency of the terms
    against their respective ranks and verifying the power law: <math alttext="f Subscript
    r Baseline equals f left-parenthesis r right-parenthesis equals f 1 r Superscript
    negative 1"><mrow><msub><mi>f</mi> <mi>r</mi></msub> <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo>
    <mi>r</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>f</mi> <mn>1</mn></msub> <msup><mi>r</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math> . To verify power laws,
    it is easier to make a *log-log* plot, plotting <math alttext="log left-parenthesis
    f Subscript r Baseline right-parenthesis"><mrow><mo form="prefix">log</mo> <mo>(</mo>
    <msub><mi>f</mi> <mi>r</mi></msub> <mo>)</mo></mrow></math> against <math alttext="log
    left-parenthesis r right-parenthesis"><mrow><mo form="prefix">log</mo> <mo>(</mo>
    <mi>r</mi> <mo>)</mo></mrow></math> . If we obtain a straight line in the *log-log*
    plot, then <math alttext="f Subscript r Baseline equals f left-parenthesis r right-parenthesis
    equals f 1 r Superscript alpha"><mrow><msub><mi>f</mi> <mi>r</mi></msub> <mo>=</mo>
    <mi>f</mi> <mrow><mo>(</mo> <mi>r</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>f</mi>
    <mn>1</mn></msub> <msup><mi>r</mi> <mi>α</mi></msup></mrow></math> where <math
    alttext="alpha"><mi>α</mi></math> is the slope of the straight line.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过绘制术语的频率与它们各自的排名，并验证幂律来验证齐普夫定律：<math alttext="f Subscript r Baseline equals
    f left-parenthesis r right-parenthesis equals f 1 r Superscript negative 1"><mrow><msub><mi>f</mi>
    <mi>r</mi></msub> <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>r</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>f</mi> <mn>1</mn></msub> <msup><mi>r</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math>。要验证幂律，最好制作一个*对数-对数*图，将<math
    alttext="log left-parenthesis f Subscript r Baseline right-parenthesis"><mrow><mo
    form="prefix">log</mo> <mo>(</mo> <msub><mi>f</mi> <mi>r</mi></msub> <mo>)</mo></mrow></math>绘制在<math
    alttext="log left-parenthesis r right-parenthesis"><mrow><mo form="prefix">log</mo>
    <mo>(</mo> <mi>r</mi> <mo>)</mo></mrow></math>上。如果我们在*对数-对数*图中获得一条直线，那么<math alttext="f
    Subscript r Baseline equals f left-parenthesis r right-parenthesis equals f 1
    r Superscript alpha"><mrow><msub><mi>f</mi> <mi>r</mi></msub> <mo>=</mo> <mi>f</mi>
    <mrow><mo>(</mo> <mi>r</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>f</mi> <mn>1</mn></msub>
    <msup><mi>r</mi> <mi>α</mi></msup></mrow></math>，其中<math alttext="alpha"><mi>α</mi></math>是直线的斜率。
- en: Various Vector Representations For Natural Language Documents
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言文档的各种向量表示形式
- en: 'Let’s list the most common document vector representations for state of the
    art natural language processing models. The first two, Term Frequency and Term
    Frequency times Inverse Document Frequency, are statistical representations since
    they are frequency based, relying on counting word appearances in documents. They
    are slightly more involved than a simple binary representation detecting the presence
    or nonpresence of certain words, nevertheless, they are still shallow, merely
    counting words. Even with this shallowness, they are very useful for applications
    such as spam filtering and sentiment analysis:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们列出最常见的自然语言处理模型的文档向量表示。前两种，词项频率和词项频率乘以逆文档频率，是统计表示，因为它们是基于频率的，依赖于计算文档中单词出现的次数。它们比简单的二进制表示稍微复杂一些，可以检测文档中某些单词的存在或不存在，然而，它们仍然是浅层的，仅仅计算单词。即使有这种浅显性，它们对于垃圾邮件过滤和情感分析等应用非常有用：
- en: Term Frequency (TF) vector representation of a document or bag of words
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文档或词袋的词项频率（TF）向量表示
- en: Here, we represent a document using a *bag of words*, discarding the order in
    which words appear in the document. Even though word order encodes important information
    about a document’s content, ignoring it is usually an okay approximation for short
    sentences and phrases.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用*词袋*来表示一个文档，忽略单词在文档中出现的顺序。尽管单词顺序编码了关于文档内容的重要信息，但忽略它通常是一个对于短句和短语的良好近似。
- en: Suppose that we want to embed our given document in a *vocabulary space* of
    10,000 tokens. Then the vector representing this document will have 10,000 entries,
    with each entry counting how many times each particular token appears in the document.
    For the obvious reasons, this is called the *Term Frequency* or *bag of words*
    vector representation of the document, where each entry is a nonnegative integer
    (a whole number).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要将给定的文档嵌入到一个包含10,000个标记的*词汇空间*中。那么代表这个文档的向量将有10,000个条目，每个条目计算每个特定标记在文档中出现的次数。出于明显的原因，这被称为文档的*词项频率*或*词袋*向量表示，其中每个条目是一个非负整数（整数）。
- en: 'For example, a Google search query: *What’s the weather tomorrow?* will be
    vectorized as zeros everywhere except for ones at the tokens representing the
    words *what*, *the* *weather*, and *tomorrow* if they exist in the vocabulary.
    We then *normalize* this vector, dividing each entry by the total number of terms
    in the document, so that the length of the document doesn’t skew our analysis.
    That is, if a document has 50,000 terms and the term *cat* gets mentioned a hundred
    times, and another document has a hundred terms only and the term *cat* gets mentioned
    10 times, then obviously the word cat is more important for the second document
    than for the first, and a mere word count without normalizing would not be able
    to capture that.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个谷歌搜索查询：*明天的天气如何？*将被矢量化为除了代表单词*what*、*the*、*weather*和*tomorrow*的标记外，其他地方都是零，如果它们存在于词汇表中。然后我们*标准化*这个向量，将每个条目除以文档中的总术语数，以便文档的长度不会扭曲我们的分析。也就是说，如果一个文档有50,000个术语，术语*cat*被提及100次，另一个文档只有100个术语，术语*cat*被提及10次，那么显然单词cat对于第二个文档比对于第一个文档更重要，而仅仅计算单词数量而不进行标准化将无法捕捉到这一点。
- en: Finally, some natural language processing classes take the log of each term
    in the document vector for the reasons mentioned in the previous two sections.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一些自然语言处理课程对文档向量中的每个术语取对数，原因在前两节中提到。
- en: Term Frequency times Inverse Document Frequency (TF-IDF) vector representation
    of a document
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文档的词项频率乘以逆文档频率（TF-IDF）向量表示
- en: Here, for each entry of the vector representing the document we still count
    the number of times the token appears in the document, *but then we divide by
    the number of documents in our corpus in which the token occurs*.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，对于代表文档的向量的每个条目，我们仍然计算标记在文档中出现的次数，*但然后我们除以我们语料库中包含该标记的文档的数量*。
- en: The idea is that if a term appears many times in one document and not as much
    in the others, then this term must be important for this one document, getting
    a higher score in the corresponding entry of the vector representing this document.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是，如果一个术语在一个文档中出现很多次，而在其他文档中出现的次数不那么多，那么这个术语对于这个文档必须是重要的，在代表这个文档的向量的相应条目中得到更高的分数。
- en: 'In order to avoid division by zero, in the case a term does not appear in any
    document, it is common practice to add one to the denominator. For example, the
    inverse document frequency of the token *cat* is:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免除以零，在一个术语不出现在任何文档中的情况下，通常的做法是在分母上加一。例如，标记*cat*的逆文档频率是：
- en: <math alttext="dollar-sign IDF for cat equals StartFraction number of documents
    in corpus Over number of documents containing cat plus 1 EndFraction dollar-sign"><mrow><mtext>IDF</mtext>
    <mtext>for</mtext> <mtext>cat</mtext> <mo>=</mo> <mfrac><mrow><mtext>number</mtext><mtext>of</mtext><mtext>documents</mtext><mtext>in</mtext><mtext>corpus</mtext></mrow>
    <mrow><mtext>number</mtext><mtext>of</mtext><mtext>documents</mtext><mtext>containing</mtext><mtext>cat</mtext><mo>+</mo><mn>1</mn></mrow></mfrac></mrow></math>
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign IDF for cat equals StartFraction number of documents
    in corpus Over number of documents containing cat plus 1 EndFraction dollar-sign"><mrow><mtext>IDF</mtext>
    <mtext>for</mtext> <mtext>cat</mtext> <mo>=</mo> <mfrac><mrow><mtext>number</mtext><mtext>of</mtext><mtext>documents</mtext><mtext>in</mtext><mtext>corpus</mtext></mrow>
    <mrow><mtext>number</mtext><mtext>of</mtext><mtext>documents</mtext><mtext>containing</mtext><mtext>cat</mtext><mo>+</mo><mn>1</mn></mrow></mfrac></mrow></math>
- en: Obviously, using TF-IDF representation, the entries of the document vectors
    will be nonnegative rational numbers, each providing a measure of the *importance*
    of that particular token to the document. Finally, we take the log of each entry
    in this vector, for the same reasons stated in the previous section.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，使用TF-IDF表示，文档向量的条目将是非负有理数，每个条目提供了该特定标记对文档的*重要性*的度量。最后，我们对这个向量中的每个条目取对数，原因与前一节中所述相同。
- en: There are many alternative TF-IDF approaches relevant to information retrieval
    systems, such as Okapi BM25\. See Molino 2017.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多与信息检索系统相关的TF-IDF方法，例如Okapi BM25。参见Molino 2017。
- en: Topic vector represenation of a document determined by latent semantic analysis
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 由潜在语义分析确定的文档主题向量表示
- en: TF-IDF vectors are very high dimensional (as many dimensions as tokens in the
    corpus, so it could be in the millions), sparse, and have no special meaning when
    added or subtracted from each other. We need more compact vectors, in the hundreds
    of dimensions or less, which is a big squeeze from millions of dimensions. In
    addition to the dimension reduction advantage, these vectors capture some meaning
    not only word counts and statistics. We call them *topic vectors*. Instead of
    focusing on the *statistics of words in documents*, we focus on the *statistics
    of connections between words in documents and across corpuses*. The topics produced
    here will be linear combinations of word counts.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF向量非常高维（与语料库中的标记数量一样多，可能达到百万级），稀疏，并且在相互相加或相减时没有特殊含义。我们需要更紧凑的向量，维度在数百个或更少，这是从百万维度中的大幅压缩。除了降维的优势外，这些向量捕捉了一些含义，不仅仅是词频和统计数据。我们称它们为*主题向量*。我们不再关注*文档中单词的统计数据*，而是关注*文档内和跨语料库中单词之间的连接统计数据*。这里产生的主题将是单词计数的线性组合。
- en: First we process the whole TF-IDF matrix *X* of our corpus, producing our *topic
    space*. Processing in this case means that we compute the *singular value decomposition*
    of the TF-IDF matrix from linear algebra, namely, <math alttext="upper X equals
    upper U normal upper Sigma upper V Superscript t"><mrow><mi>X</mi> <mo>=</mo>
    <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup></mrow></math> . We have
    a whole chapter on the singular value decomposition in this book, so we will not
    go into its details now, but we will explain how it is used for producing our
    topic space for a corpus. Singular value decomposition from linear algebra is
    called [*latent semantic analysis*](https://en.wikipedia.org/wiki/Latent_semantic_analysis)
    in natural language processing. We will use both terms synonymously.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们处理我们语料库的整个TF-IDF矩阵*X*，产生我们的*主题空间*。在这种情况下，处理意味着我们计算线性代数中TF-IDF矩阵的*奇异值分解*，即，<math
    alttext="upper X equals upper U normal upper Sigma upper V Superscript t"><mrow><mi>X</mi>
    <mo>=</mo> <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup></mrow></math>。我们在本书中有一整章关于奇异值分解，所以我们现在不会详细介绍，但我们会解释如何用它来产生我们语料库的主题空间。线性代数中的奇异值分解在自然语言处理中被称为[*潜在语义分析*](https://en.wikipedia.org/wiki/Latent_semantic_analysis)。我们将两个术语互换使用。
- en: 'We have to pay attention as to whether the columns of the corpus’s TF-IDF matrix
    *X* represent the word tokens or the documents. Different authors and software
    packages use one or the other, so we must be careful and process either the matrix
    or its transpose in order to produce our topic space. In this section we follow
    the representation: The rows are all the words (tokens for words, n-grams, etc.)
    of the entire corpus, and the columns are the TF-IDF vector representations for
    each document in the corpus. This is slightly divergent from the usual representation
    of a data matrix, where the features (the words within each document) are in the
    columns and the instances (the documents) are in the rows. The reason for this
    switch will be apparent shortly. However, this is not divergent from our represetation
    for documents as column vectors.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须注意语料库的TF-IDF矩阵*X*的列是代表单词标记还是文档。不同的作者和软件包使用其中一个，因此我们必须小心处理矩阵或其转置以产生我们的主题空间。在本节中，我们遵循这种表示：行是整个语料库的所有单词（标记为单词、n-gram等），列是语料库中每个文档的TF-IDF向量表示。这与数据矩阵的通常表示略有不同，其中特征（每个文档中的单词）在列中，实例（文档）在行中。这种切换的原因很快就会显而易见。然而，这与我们对文档的列向量表示并不相悖。
- en: 'Next, given a new document with its TF-IDF vector representation, we convert
    it to a much more compact *topic vector* by *projecting it onto* the topic space
    produced by the singular value decomposition of the corpus’s TF-IDF matrix. *Projecting*
    in linear algebra is merely computing the *dot product* between the appropriate
    vectors, and saving the resulting scalar numbers into the entries of a new *projected*
    vector. Here are the steps:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，给定一个新文档及其TF-IDF向量表示，我们通过将其*投影到*由语料库的TF-IDF矩阵的奇异值分解产生的主题空间上，将其转换为一个更紧凑的*主题向量*。在线性代数中，*投影*仅仅是计算适当向量之间的*点积*，并将结果标量数保存到新的*投影*向量的条目中。以下是步骤：
- en: We have a TF-IDF vector of a document that has as many entries as the number
    of tokens in the entire corpus;
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有一个文档的TF-IDF向量，条目数量与整个语料库中的标记数量相同；
- en: We have *topic weight vectors*, which are the columns of the matrix *U* produced
    by the singular value decomposition of the TF-IDF matrix <math alttext="upper
    X equals upper U normal upper Sigma upper V Superscript t"><mrow><mi>X</mi> <mo>=</mo>
    <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup></mrow></math> . Again,
    each topic weight vector has as many entries as tokens in our corpus. Initially,
    we also have as many topic weight vectors as tokens in our entire corpus (columns
    of *U*). The *weights* in the column of *U* tell us how much a certain token contributes
    to the topic, with a big contribution if it a positive number close to one, ambivalent
    contribution if it is close to zero, and even a negative contribution if it is
    a negative number close to -1\. Note that the entries of *U* are always numbers
    between -1 and 1, so we interpret them as weighing factors for our corpus’s tokens.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有*主题权重向量*，它们是通过TF-IDF矩阵的奇异值分解产生的矩阵*U*的列。同样，每个主题权重向量的条目数量与我们的语料库中的标记数量相同。最初，我们的主题权重向量与整个语料库中的标记数量一样多（*U*的列）。*U*中的列中的*权重*告诉我们某个标记对主题的贡献有多大，如果是接近1的正数，则贡献很大，如果接近零，则贡献模棱两可，如果是接近-1的负数，则贡献甚至为负。请注意，*U*的条目始终是-1和1之间的数字，因此我们将其解释为我们语料库标记的权重因子。
- en: 'You might be wondering: If we have as many topic weight vectors as tokens in
    our corpus, each having as many entries as tokens as well, then where are the
    savings, and when will compression or dimension reduction happen? Keep reading.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想：如果我们的语料库中有与标记数量相同的主题权重向量，每个向量的条目数也与标记数量相同，那么节省在哪里，何时会发生压缩或降维？继续阅读。
- en: 'Goal 1: Compute how much of a certain topic our document contains. This is
    simply the dot product between the document’s TF-IDF vector and the column of
    *U* corresponding to the topic that we care for. Record this as the first scalar
    number.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标1：计算我们的文档包含多少特定主题。这只是文档的TF-IDF向量与我们关心的主题对应的*U*列之间的点积。将其记录为第一个标量数字。
- en: 'Goal 2: Compute how much of *another* topic our document contains. This is
    the dot product between the document’s TF-IDF vector and the column of *U* corresponding
    to this other topic that we care for. Record this as the second scalar number.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标2：计算我们的文档包含*另一个*主题的多少。这是文档的TF-IDF向量与与我们关心的另一个主题对应的*U*列之间的点积。将其记录为第二个标量数字。
- en: 'Goal 3: Repeat this for as many (as there are columns of *U*, which is the
    same as the total number of tokens in the corpus) or as little (as one) *topics*
    as we like, recording the scalar number from each dot product that we compute.
    It is clear now that *a topic* in this context means a column vector containing
    weights between -1 and 1 assigned to each token in the corpus.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标3：为我们喜欢的主题（与*U*的列数相同，也就是语料库中的总标记数）或不喜欢的主题（一个）重复这个过程，记录我们计算的每个点积的标量数字。现在很明显，在这种情况下，*一个主题*意味着一个包含在语料库中每个标记之间分配的-1到1之间权重的列向量。
- en: 'Goal 4: *Reduce the dimension* by keeping only the topics that matter. That
    is, if we decide to keep only two topics, then the *compressed vector representation*
    of our document will be the *two dimensional* vector containing the two scalar
    numbers produced using the two dot products between the document’s TF-IDF vector
    and the two topics’ weight vectors. This way, we would have reduced the dimension
    of our document from possibly *millions* to just two. Pretty cool stuff.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标4：通过仅保留重要的主题来*减少维度*。也就是说，如果我们决定只保留两个主题，那么我们文档的*压缩向量表示*将是包含两个标量数字的二维向量，这两个标量数字是使用文档的TF-IDF向量和两个主题权重向量之间的两个点积产生的。这样，我们将把文档的维度从可能的*数百万*减少到只有两个。非常酷。
- en: 'Goal 5: *Choose the right topics* to represent our documents. This is where
    the singular value decomposition works its magic. The columns of *U* are organized
    in order, from the most important topic across the corpus to the least important.
    In the language of statistics, the columns are organized from the topic with the
    *most variance* across the corpus and hence encodes more information, to the one
    with the *least variance* and hence encodes little information. We explain how
    variance and singular value decomposition are related in [Chapter 10](ch10.xhtml#ch10).
    Thus, if we decide to project our high dimensional document onto the first few
    column vectors of *U* only, we are guaranteed that we are not missing much in
    terms of capturing enough variation of possible topics across the corpus, and
    assessing how much of these our document contains.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标5：*选择正确的主题*来代表我们的文档。这就是奇异值分解发挥作用的地方。*U*的列按顺序组织，从整个语料库中最重要的主题到最不重要的主题。从统计学的角度来看，列按照在整个语料库中方差最大的主题到方差最小的主题的顺序组织，因此编码更多信息。我们在[第10章](ch10.xhtml#ch10)中解释了方差和奇异值分解的关系。因此，如果我们决定仅将我们的高维文档投影到*U*的前几个列向量上，我们可以确保在捕捉语料库中可能的主题变化方面不会错过太多，并评估我们的文档包含多少这些内容。
- en: 'Goal 6: Understand that *this is still a statistical method for capturing topics
    in a document*. We started with the TF-IDF matrix of a corpus, simply counting
    token occurences in documents. In this sense, a topic is captured based only on
    the premise that documents that refer to similar things use similar words. This
    is different than capturing topics based on the *meanings* of the words they use.
    That is, if we have two documents discussing the same topic but using entirely
    different vocabulary, they will be far apart in topic space. The remedy to this
    would be to store words together with other words of similar meaning, which is
    the Word2Vec approach, discussed later in this chapter.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标6：了解*这仍然是一种用于捕捉文档中主题的统计方法*。我们从语料库的TF-IDF矩阵开始，简单地计算文档中标记的出现次数。在这种意义上，主题是基于一个前提捕捉的，即涉及相似事物的文档使用相似的词语。这与基于它们使用的词语的*含义*捕捉主题是不同的。也就是说，如果我们有两篇讨论相同主题但使用完全不同词汇的文档，它们在主题空间中会相距很远。解决这个问题的方法是将具有相似含义的单词与其他单词一起存储，这是后面在本章中讨论的Word2Vec方法。
- en: 'Question 1: What happens if we add another document to our corpus? Luckily,
    we do not have to reprocess the whole corpus to produce the document’s topic vector,
    we just project it onto the corpus’s existing topic space. This of course breaks
    down if we add a new document that has nothing in common with our corpus, such
    as an article on pure mathematics added to a corpus on Shakespeare’s love sonnets.
    Our math article in this case will be represented by a bunch of zeros or close
    to zero entries, which does not capture the ideas in the article adequately.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题1：如果我们向我们的语料库添加另一个文档会发生什么？幸运的是，我们不必重新处理整个语料库来生成文档的主题向量，我们只需将其投影到语料库现有的主题空间中。当然，如果我们添加一个与我们的语料库没有共同之处的新文档，比如将一篇关于莎士比亚情诗的语料库添加到一篇关于纯数学的文章中，这种方法就会失败。在这种情况下，我们的数学文章将由一堆零或接近零的条目表示，这并不能充分捕捉文章中的思想。
- en: 'Question 2: What about the matrix <math alttext="upper V Superscript t"><msup><mi>V</mi>
    <mi>t</mi></msup></math> in the singular value decomposition <math alttext="upper
    X equals upper U normal upper Sigma upper V Superscript t"><mrow><mi>X</mi> <mo>=</mo>
    <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup></mrow></math> , what
    does it mean in the context of natural language processing of our corpus? The
    matrix <math alttext="upper V Superscript t"><msup><mi>V</mi> <mi>t</mi></msup></math>
    has the same number of rows and columns as the number of documents in our corpus.
    It is the document-document matrix and gives the shared meaning between documents.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题2：在奇异值分解中的矩阵<math alttext="upper V Superscript t"><msup><mi>V</mi> <mi>t</mi></msup></math>在我们语料库的自然语言处理背景下意味着什么？矩阵<math
    alttext="upper V Superscript t"><msup><mi>V</mi> <mi>t</mi></msup></math>的行数和列数与我们语料库中的文档数量相同。它是文档-文档矩阵，给出了文档之间的共享含义。
- en: 'Question 3: When we move to a lower dimensional topic space using latent semantic
    analysis, are large distances between documents preserved? Yes, since the singular
    value decomposition focuses on *maximizing* the variance across the corpus’s documents.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题3：当我们使用潜在语义分析转移到较低维度的主题空间时，文档之间的大距离会被保留吗？是的，因为奇异值分解侧重于最大化语料库文档之间的方差。
- en: 'Question 4: Are small distances preserved, meaning does latent semantic analysis
    preserve the *fine structure* of a document that separates it from *not so different*
    other documents? No. Latent Dirichlet allocation, discussed soon, does a better
    job here.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题4：小距离被保留吗，即潜在语义分析是否保留了文档的“精细结构”，使其与“并非完全不同”的其他文档区分开来？不。即将讨论的潜在Dirichlet分配在这方面做得更好。
- en: 'Question 5: Can we improve latent semantic analysis inorder to also keep close
    document vectors together in the lower dimensional topic space? Yes, we can *steer*
    the vectors by taking advantage of extra information, or *meta-data*, of the documents,
    such as messages having the same sender, or by penalizing using a cost function
    so that the method spills out topic vectors that preserve *closeness* as well.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题5：我们可以改进潜在语义分析以使较低维度主题空间中的文档向量保持接近吗？是的，我们可以通过利用文档的额外信息或元数据，例如具有相同发件人的消息，或通过惩罚使用成本函数来“引导”向量，以便该方法产生保持“接近性”的主题向量。
- en: '**To summarize**: Latent semantic analysis chooses the topics in an optimal
    way that maximizes the diversity in the topics across the corpus. The matrix *U*
    from the singular value decomposition of the TF-IDF matrix is very important for
    us. It returns the directions along which the variance is maximal. We usually
    get rid of the topics that have the least amount of variance between the documents
    in the corpus, throwing away the last columns of *U*. This is similar to manually
    getting rid of stop words (and, a, the, *etc.*) during text preparation, but latent
    semantic analysis does that for us, in an optimized way. The matrix *U* has the
    same number of rows and columns as our vocabulary. It is the cross correlation
    between words and topics based on word co-occurence in the same document. When
    we multiply a new document by *U* (project it onto the columns of *U*), we would
    get the amount of each topic in the document. We can truncate *U* as we wish and
    throw away less important topics, reducing the dimension to as little topics as
    we want.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结**：潜在语义分析以最大化语料库中主题的多样性的最佳方式选择主题。来自TF-IDF矩阵奇异值分解的矩阵*U*对我们非常重要。它返回方差最大的方向。我们通常会摆脱在语料库中文档之间方差最小的主题，丢弃*U*的最后几列。这类似于在文本准备过程中手动摆脱停用词（和、a、the等），但潜在语义分析以优化的方式为我们完成了这项工作。矩阵*U*的行数和列数与我们的词汇量相同。它是基于单词在同一文档中共现的词和主题之间的交叉相关性。当我们将一个新文档乘以*U*（将其投影到*U*的列上）时，我们将得到文档中每个主题的数量。我们可以根据需要截断*U*并丢弃不太重要的主题，将维度减少到我们想要的主题数量。'
- en: '**Latent semantic analysis has shortcomings**: The topic spaces it produces,
    or the columns of *U*, are mere *linear combinations* of tokens that are thrown
    together in a way that captures as much variance in the usage across the vocabulary’s
    tokens as possible. This doesn’t necessarily translate into word combinations
    that are in any way meaningful to humans. Bummer. Word2Vec, discussed later, addresses
    these shortcomings.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在语义分析存在缺点**：它产生的主题空间或*U*的列只是将标记的线性组合放在一起，以尽可能多地捕捉词汇标记在使用中的方差。这并不一定转化为对人类有意义的词组合。遗憾。稍后将讨论的Word2Vec解决了这些缺点。'
- en: Finally, the topic vectors produced via latent semantic analysis are just linear
    transformations performed on the TF-IDF vectors. They should be the first choice
    for semantic searches, clustering documents, and content based recommendation
    engines. All of this can be accomplished by measuring distances between these
    topic vectors, which we explain later in this chapter.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过潜在语义分析产生的主题向量只是在TF-IDF向量上执行的线性变换。它们应该是语义搜索、文档聚类和基于内容的推荐引擎的首选。所有这些都可以通过测量这些主题向量之间的距离来实现，我们将在本章后面解释。
- en: Topic vector represenation of a document determined by latent Dirichlet allocation
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 由潜在Dirichlet分配确定的文档的主题向量表示
- en: 'Unlike topic vectors using latent semantic analysis, with [*latent Dirichlet
    allocation*](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) we do
    have to reprocess the entire corpus if we add a new document to the corpus in
    order to produce its topic vector. Moreover, we use a nonlinear statistical approach
    to bundle words together into topics: We assume a Dirichlet distribution of word
    frequencies. This makes the method more precise than latent semantic analysis
    in terms of the statistics of allocating words to topics. Thus, the method is
    explainable: The way words are allocated to topics, based on how often they occured
    together in a document, and the way topics are allocated to documents, tend to
    make sense to us as humans.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用潜在语义分析的主题向量不同，使用[潜在狄利克雷分配](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)时，如果我们向语料库添加新文档以生成其主题向量，则必须重新处理整个语料库。此外，我们使用非线性统计方法将单词捆绑到主题中：我们假设单词频率遵循狄利克雷分布。这使得该方法在将单词分配给主题的统计方面比潜在语义分析更精确。因此，该方法是可解释的：单词如何分配到主题中，基于它们在文档中一起出现的频率，以及主题如何分配到文档中，对我们人类来说是有意义的。
- en: This nonlinear method takes longer time to train than the linear latent semantic
    analysis. For this reason it is impractical for applications involving corpuses
    of documents, even though it is explainable. We can use it instead for summarizing
    single documents, where each sentence in the document becomes its own *document*,
    and the mother document becomes the corpus.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这种非线性方法的训练时间比线性潜在语义分析长。因此，尽管它是可解释的，但对于涉及文档语料库的应用来说是不切实际的。我们可以将其用于总结单个文档，其中文档中的每个句子都成为其自己的*文档*，而母文档则成为语料库。
- en: 'Latent Dirichlet allocation was invented in 2000 by geneticists for the purpose
    of infering population structure, and adopted in 2003 for natural language processing.
    The following are the its assumptions:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配是在2000年由遗传学家发明的，用于推断人口结构，并于2003年用于自然语言处理。以下是其假设：
- en: We start with raw word counts (rather than normalized TF-IDF vectors), but there
    is still no sequencing of words in order to make sense of them. Instead, we are
    still relying on modeling the statistics of words for each document, except this
    time we will incorporporate the word distribution explicitly into the model.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从原始单词计数开始（而不是归一化的TF-IDF向量），但仍然没有对单词进行排序以理解它们。相反，我们仍然依靠对每个文档的单词统计进行建模，只是这一次我们将单词分布明确地纳入模型。
- en: A document is a linear combination of an arbitrary number of topics (specify
    this number ahead of time so that the method allocates the document’s tokens to
    this number of topics).
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档是任意数量主题的线性组合（提前指定此数量，以便该方法将文档的标记分配给此数量的主题）。
- en: We can represent each topic by a certain distribution of words based on their
    term frequencies.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以根据单词的词频来表示每个主题的某种分布。
- en: The probability of occurence of a certain topic in a document follows a Dirichlet
    probability distribution.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档中某个主题出现的概率遵循狄利克雷概率分布。
- en: The probability of a certain word being assigned to a topic also follows a Dirichlet
    probability distribution.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某个单词被分配到某个主题的概率也遵循狄利克雷概率分布。
- en: As a result, topic vectors obtained using Dirichlet allocation are sparse, indicating
    clean separation between the topics in the sense of which words they contain,
    which makes them explainable.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用狄利克雷分配获得的主题向量是稀疏的，表明主题在包含哪些单词方面有清晰的分离，这使得它们可以解释。
- en: With Dirichlet allocation, words that occur frequently together are assigned
    to the same topics. So this method keeps tokens that were close together close
    together when we move to the lower dimensional topic space. Latent semantic analysis,
    on the other hand keeps tokens that were spread apart spread apart when we move
    to the lower dimensional topic space, so this is better for classification problems
    where the separation between the classes is maintained even as we move to the
    lower dimensional space.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用狄利克雷分配，频繁一起出现的单词被分配到相同的主题中。因此，当我们转移到较低维度的主题空间时，该方法会保持靠在一起的标记靠在一起。另一方面，潜在语义分析在我们转移到较低维度的主题空间时会保持分散的标记分散，因此在我们转移到较低维度空间时，这对于保持类别之间的分离更好。
- en: Topic vector represenation of a document determined by latent discriminant analysis
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 由潜在判别分析确定的文档的主题向量表示
- en: Unlike latent semantic analysis and latent Dirichlet allocation which break
    down a document into as many topics as we choose, latent discriminant analysis
    breaks down a document into *only one topic* such as spamness, sentiment, *etc.*.
    This is good for binary classification, such as classifying messages as spam or
    nonspam, or classifying reviews as positive or negative. Rather than what latent
    semantic analysis does, maximizing the separation between all the vectors in the
    new topic space, latent discriminant analysis maximizes the separation only between
    the centroids of the vectors belonging to each class.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 与潜在语义分析和潜在狄利克雷分配将文档分解为我们选择的主题数量不同，潜在判别分析将文档分解为*仅一个主题*，例如垃圾邮件、情感等。这对于二元分类非常有用，例如将消息分类为垃圾邮件或非垃圾邮件，或将评论分类为积极或消极。与潜在语义分析最大化新主题空间中所有向量之间的分离不同，潜在判别分析仅最大化属于每个类别的向量的质心之间的分离。
- en: But how do we determine the vector representing this one topic? Given the TF-IDF
    vectors of labeled spam and nonspam documents, we compute the centroid of each
    class, then our vector is along the line connecting the two centroids (see [Figure 7-1](#Fig_LDA)).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们如何确定代表这个主题的向量？给定带标签的垃圾邮件和非垃圾邮件文档的TF-IDF向量，我们计算每个类别的质心，然后我们的向量沿着连接两个质心的线（参见[图7-1](#Fig_LDA)）。
- en: '![300](assets/emai_0701.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![300](assets/emai_0701.png)'
- en: Figure 7-1\. Latent discriminant analysis.
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1. 潜在判别分析。
- en: Each new document can now be projected onto this one dimension. The coordinate
    of our document along that line is the dot product between its TF-IDF and the
    direction vector of the cetroids line. The whole document (with millions of dimensions)
    is now squashed into one number along one dimension (one axis) that carries the
    two centroids along with their midpoint. We can then classify the document as
    belonging to one class or the other depending on its distance from each centroid
    along that one line. Note that the decision boundary for separating classes using
    this method is linear.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在每个新文档都可以投影到这个一维空间中。我们的文档沿着那条线的坐标是其TF-IDF和质心线方向向量之间的点积。整个文档（具有数百万维度）现在被压缩成一个数字沿着一个维度（一个轴），其中包含两个质心及其中点。然后我们可以根据文档沿着那条线到每个质心的距离来将文档分类为属于一个类别或另一个类别。请注意，使用这种方法分离类别的决策边界是线性的。
- en: Meaning vector representations of words and of documents determined by neural
    network embeddings
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 由神经网络嵌入确定的单词和文档的含义向量表示
- en: The previous models for vectorizing documents of natural language text only
    considered linear relationships between words, or, in latent Dirichlet allocation,
    we had to use human judgement to be able to select the model’s parameters and
    extract features. We now know that the power of neural networks lies in their
    ability to capture nonlinear relationships, extract features, and find appropriate
    model parameters automatically. We will now use neural networks to create vectors
    that represent individual words and terms, and we will employ similar methods
    to create vectors representing the meanings of entire paragraphs. Since these
    vectors encode the meaning and the logical and contexual usage of each term, we
    can reason with them simply by doing usual vector additions and subtractions.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以前用于将自然语言文本文档向量化的模型只考虑了单词之间的线性关系，或者在潜在狄利克雷分配中，我们必须使用人类判断力来选择模型的参数并提取特征。我们现在知道神经网络的力量在于其捕捉非线性关系、提取特征和自动找到适当的模型参数的能力。我们现在将使用神经网络来创建代表单个单词和术语的向量，并采用类似的方法来创建代表整个段落含义的向量。由于这些向量编码了每个术语的含义、逻辑和上下文使用，我们可以通过简单地进行常规向量加法和减法来进行推理。
- en: Word2Vec vector representation of individual terms by incorporating continuous
    *ness* attributes
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Word2Vec通过整合连续的*ness*属性来表示单个术语的向量
- en: By using TF vectors or TF-IDF vectors as a starting point for our topic vector
    models, we have ignored the nearby context of words and the effect that has on
    the words’ meanings. Word vectors solve this problem. A word vector is a numerical
    vector representation of a word’s meaning, so every single term in the corpus
    becomes a vector of semantics. This vector representation, with floating point
    number entries, of single words, enables semantic queries and logical reasoning.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用TF向量或TF-IDF向量作为我们主题向量模型的起点，我们忽略了单词附近上下文的影响以及这对单词含义的影响。单词向量解决了这个问题。单词向量是单词含义的数值向量表示，因此语料库中的每个术语都变成了语义向量。这种向量表示，具有浮点数条目，单词的单个词语使得语义查询和逻辑推理成为可能。
- en: Word vector represenations are *learned* using a neural network. They usually
    have 100 to 500 dimensions encoding how much of each meaning dimension a word
    carries within it. During training a word vector model, the text data is unlabeled.
    Once trained, two terms can be determined to be close in meaning or far apart
    by comparing their vectors via some closeness metrics. Cosine similarity, discussed
    next, is the to-go-to method.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 单词向量表示是使用神经网络*学习*的。它们通常具有100到500个维度，编码一个词在其中携带多少意义维度。在训练单词向量模型时，文本数据是无标签的。一旦训练完成，可以通过比较它们的向量来确定两个术语在意义上是接近的还是相距甚远，通过一些接近度指标。余弦相似度，接下来会讨论，是一种常用的方法。
- en: In 2013, Google created this word-to-vector model, Word2Vec, that it trained
    on Google News feed containing 100 billion words. The resulting pre-trained word2vec
    model contains 300 dimensional vectors for 3 million words and phrases. It is
    freely available to download at the word2vec the [Google Code Archive page for
    the word2vec project](https://code.google.com/archive/p/word2vec/).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 2013年，谷歌创建了这个单词到向量模型Word2Vec，它在包含1000亿个单词的谷歌新闻源上进行了训练。结果预训练的word2vec模型包含300维向量，用于3百万个单词和短语。可以在word2vec项目的[谷歌代码存档页面](https://code.google.com/archive/p/word2vec/)上免费下载。
- en: 'The vector that Word2Vec builds up captures much more of a word’s meaning than
    the topic vectors discussed earlier in this chapter. The abstract of the paper,
    [*Efficient Estimation of Word Representations in Vector Space (2013)*](https://arxiv.org/abs/1301.3781),
    is informative: *We propose two novel model architectures for computing continuous
    vector representations of words from very large data sets. The quality of these
    representations is measured in a word similarity task, and the results are compared
    to the previously best performing techniques based on different types of neural
    networks. We observe large improvements in accuracy at much lower computational
    cost, i.e. it takes less than a day to learn high quality word vectors from a
    1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art
    performance on our test set for measuring syntactic and semantic word similarities.*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec构建的向量捕捉了比本章前面讨论的主题向量更多的单词含义。论文摘要[*Efficient Estimation of Word Representations
    in Vector Space (2013)*](https://arxiv.org/abs/1301.3781)很有启发性：*我们提出了两种新颖的模型架构，用于从非常大的数据集计算单词的连续向量表示。这些表示的质量是通过单词相似性任务来衡量的，并且结果与以前基于不同类型的神经网络的表现最佳技术进行了比较。我们观察到在准确性方面取得了很大的改进，而计算成本要低得多，即从一个包含16亿个单词的数据集中学习高质量的单词向量不到一天的时间。此外，我们展示这些向量在我们的测试集上提供了最先进的性能，用于测量句法和语义单词相似性。*
- en: 'The paper published a month later [*Distributed Representations of Words and
    Phrases and their Compositionality*](https://arxiv.org/pdf/1310.4546.pdf) addressed
    the represenation of word phrases that mean something different than their individual
    components, such as *Air Canada*: *The recently introduced continuous Skip-gram
    model is an efficient method for learning high quality distributed vector representations
    that capture a large number of precise syntactic and semantic word relationships.
    In this paper we present several extensions that improve both the quality of the
    vectors and the training speed. By subsampling of the frequent words we obtain
    significant speedup and also learn more regular word representations. We also
    describe a simple alterna- tive to the hierarchical softmax called negative sampling.
    An inherent limitation of word representations is their indifference to word order
    and their inability to represent idiomatic phrases. For example, the meanings
    of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated
    by this example, we present a simple method for finding phrases in text, and show
    that learning good vector representations for millions of phrases is possible*.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一个月后发表的论文[*Distributed Representations of Words and Phrases and their Compositionality*](https://arxiv.org/pdf/1310.4546.pdf)讨论了表示与其各个组成部分含义不同的词组，例如*Air
    Canada*：*最近引入的连续Skip-gram模型是一种学习高质量分布式向量表示的有效方法，可以捕捉大量精确的句法和语义单词关系。在本文中，我们提出了几种扩展，既提高了向量的质量，又提高了训练速度。通过对频繁单词进行子采样，我们获得了显著的加速，并且学习了更规则的单词表示。我们还描述了一种简单的替代分层softmax的负采样。单词表示的固有局限性在于它们对单词顺序的漠不关心以及无法表示习语短语。例如，“Canada”和“Air”的含义不能轻松结合以获得“Air
    Canada”。受到这个例子的启发，我们提出了一种在文本中找到短语的简单方法，并展示了学习数百万短语的良好向量表示是可能的。*
- en: 'The publication that introduced Word2Vec representations [Linguistic Regularities
    in Continuous Space Word Representations (2013)](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rvecs.pdf)
    demonstrates how these meaning vectors for words encode logical regularities and
    how this enables us to answer regular analogy questions: *Continuous space language
    models have recently demonstrated outstanding results across a variety of tasks.
    In this paper, we examine the vector space word representations that are implicitly
    learned by the input layer weights. We find that these representations are surprisingly
    good at capturing syntactic and semantic regularities in language, and that each
    relationship is characterized by a relation specific vector offset. This allows
    vector oriented reasoning based on the offsets between words. For example, the
    male/female relationship is automatically learned, and with the induced vector
    representations, “King - Man + Woman” results in a vector very close to “Queen.”
    We demonstrate that the word vectors capture syntactic regularities by means of
    syntactic analogy questions (provided with this paper), and are able to correctly
    answer almost 40% of the questions. We demonstrate that the word vectors capture
    semantic regularities by using the vector offset method to answer SemEval-2012
    Task 2 questions. Remarkably, this method outperforms the best previous systems.*'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 引入Word2Vec表示的出版物[Linguistic Regularities in Continuous Space Word Representations
    (2013)](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rvecs.pdf)演示了这些单词的含义向量如何编码逻辑规律，以及这如何使我们能够回答常规类比问题：*连续空间语言模型最近在各种任务中表现出色。在本文中，我们研究了隐式学习的输入层权重所学习的向量空间单词表示。我们发现这些表示在捕捉语言中的句法和语义规律方面出奇地好，并且每个关系都由一个特定关系向量偏移所表征。这允许基于单词之间的偏移进行向量定向推理。例如，男性/女性关系是自动学习的，通过诱导的向量表示，“国王
    - 男人 + 女人”得到的向量非常接近“女王”。我们通过句法类比问题（本文提供）演示了单词向量捕捉句法规律的能力，并且能够正确回答近40%的问题。我们通过使用向量偏移方法回答SemEval-2012任务2问题来演示单词向量捕捉语义规律的能力。值得注意的是，这种方法胜过了以前最好的系统。*
- en: The performance of word2vec improved dramatically since 2013, by training it
    on much larger corpuses.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 自2013年以来，word2vec的性能显著提高，通过在更大的语料库上进行训练。
- en: 'Word2Vec takes one word and assigns to it a vector of attributes such as: place-ness,
    animal-ness, city-ness, positivity (sentiment), brightness, gender, *etc.* Each
    attribute is a dimension, capturing how much of the attribute the meaning of the
    word contains.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec将一个单词分配给一个属性向量，例如：地点性，动物性，城市性，积极性（情感），明亮度，性别，*等等*。每个属性都是一个维度，捕捉单词含义中包含多少该属性。
- en: 'These word meaning vectors and the attributes are not endcoded manually, but
    during training, where the model learns the meaning of a word from the company
    it keeps: The five or so nearby words in the same sentence. This is different
    from latent semantic analysis where the topics are learned only from words occuring
    in the same document, not necessarily close to each other. For applications involving
    short documents and statements, Word2Vec embeddings have actually replaced topic
    vectors obtained through latent semantic analysis. We can also use word vectors
    to derive word clusters from huge data sets: Performing K-means clustering on
    top of the word vector represenations. See [Google Code Archive page for the word2vec
    project](https://code.google.com/archive/p/word2vec/) for more information.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这些词义向量和属性并非手动编码，而是在训练过程中学习的，模型从句子中的周围五个左右单词中学习一个词的含义。这与潜在语义分析不同，潜在语义分析仅从出现在同一文档中的单词中学习主题，而不一定是彼此接近的单词。对于涉及短文档和陈述的应用，Word2Vec嵌入实际上已经取代了通过潜在语义分析获得的主题向量。我们还可以使用单词向量从庞大的数据集中推导出单词簇：在单词向量表示之上执行K均值聚类。有关更多信息，请参阅[Google
    Code Archive页面上的word2vec项目](https://code.google.com/archive/p/word2vec/)。
- en: 'The advantage of representing words through vectors that mean something (rather
    than count something) is that we can reason with them. For example, if we subtract
    the vector representing man from the vector representing king and add the vector
    representing woman, then we get a vector very close to the vector representing
    the word queen. Another example is capturing the relationship between singular
    and plural words: If we subtract vectors representing the singular form of words
    from vectors representing their plural forms, we obtain vectors that are roughly
    the same for all words.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 通过用有意义的向量表示单词（而不是计数）的优势是我们可以用它们进行推理。例如，如果我们从代表男人的向量中减去代表国王的向量并加上代表女人的向量，那么我们得到一个非常接近代表女王的向量。另一个例子是捕捉单数和复数单词之间的关系：如果我们从代表单词单数形式的向量中减去代表它们复数形式的向量，我们得到的向量对于所有单词来说大致相同。
- en: 'The next questions are: How do we compute Word2Vec embeddings? That is, how
    do we train a Word2Vec model? What are training data, the neural network’s architecture,
    and its input and output? The neural networks that train Word2Vec models are shallow
    with only one hidden layer. The input is a large corpus of text and the outpute
    are vectors of several hundred dimensions, one for each unique term in the corpus.
    Words that share common liguistic contexts end up with vectors that are *close*
    to each other.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个问题是：我们如何计算Word2Vec嵌入？也就是说，我们如何训练一个Word2Vec模型？训练数据、神经网络的架构以及其输入和输出是什么？训练Word2Vec模型的神经网络是浅层的，只有一个隐藏层。输入是一个大型文本语料库，输出是语料库中每个唯一术语的几百维向量。共享相同语言环境的单词最终具有*接近*的向量。
- en: 'There are two learning algorithms for Word2Vec, which we will not detail here
    [for now, I will detail this in the next round of editing], however, by now, we
    have a very good idea of how neural networks work, especially shallow ones with
    only one hidden layer. The two learning algorithms are:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec有两种学习算法，我们暂时不详细介绍[我将在下一轮编辑中详细介绍]，但是到目前为止，我们对神经网络的工作原理有了很好的了解，特别是只有一个隐藏层的浅层网络。这两种学习算法是：
- en: '*Continuous bag-of-words*: This predicts the current word from a window of
    surrounding context words; the order of the context words does not influence the
    prediction.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*连续词袋*：这从周围上下文词的窗口中预测当前单词；上下文词的顺序不影响预测。'
- en: '*Continuous skip-gram*: This uses the current word to predict the surrounding
    window of context words; the algorithm weighs nearby context words more heavily
    than more distant context words.'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*连续跳字*：这使用当前单词来预测周围上下文词的窗口；该算法更加重视附近的上下文词而不是更远的上下文词。'
- en: Both algorithms learn the vector representation of a term that is useful for
    prediction of other terms in a sentence. Continuous-bag-of-words is apparently
    faster than continuous-skip-gram, while skip-gram is better for infrequent words.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种算法都学习一个术语的向量表示，这对于预测句子中的其他术语很有用。连续词袋比连续跳字似乎更快，而跳字对于不常见的词更好。
- en: 'For more details, we refer to the tutorial [The Amazing Power Of Word Vectors](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/),
    the [Wikipedia page on Word2Vec](https://en.wikipedia.org/wiki/Word2vec) and the
    three original papers on the subject (2013): [*Efficient Estimation of Word Representations
    in Vector Space*](https://arxiv.org/pdf/1301.3781.pdf), [Distributed Representations
    of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf),
    [Linguistic Regularities in Continuous Space Word Representations](https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息，我们参考教程[Word Vectors的惊人力量](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)，[Word2Vec的维基百科页面](https://en.wikipedia.org/wiki/Word2vec)以及关于该主题的三篇原始论文（2013年）：[*在向量空间中高效估计单词表示*](https://arxiv.org/pdf/1301.3781.pdf)，[单词和短语的分布式表示及其组合性](https://arxiv.org/pdf/1310.4546.pdf)，[连续空间单词表示中的语言规律性](https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf)。
- en: The trained Google News Word2vec model has three million words, each represented
    with a vector of 300 dimensions. To download this, you would need 3 GB of available
    memory. There are ways around downloading the whole pre-trained model if we have
    limited memory or if we only care for a fraction of the words.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 经过训练的Google News Word2vec模型有三百万个单词，每个单词用一个300维的向量表示。要下载这个模型，您需要3GB的可用内存。如果我们的内存有限或者我们只关心部分单词，那么可以绕过下载整个预训练模型。
- en: '**How to visualize vectors representing words?**'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**如何可视化代表单词的向量？**'
- en: Word vectors are very high dimensional (100-500 dimensions), but humans can
    only visualize two and three dimensional vectors, so we need to project our high
    dimensional vectors onto these drastically lower dimensional spaces and still
    retain their most essential characteristics. By now we know that the singular
    value decomposition (principal component analysis) accomplishes that for us, giving
    us the vectors along which to project in decreasing order of importance, or the
    directions along which a given collection of word vectors vary the most. That
    is, the singular value decomposition ensures that this projection gives the best
    possible view of the word vectors, keeping these as far apart as possible.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 单词向量是非常高维的（100-500维），但人类只能可视化二维和三维向量，因此我们需要将我们的高维向量投影到这些极低维度的空间上，并仍然保留它们最基本的特征。到目前为止，我们知道奇异值分解（主成分分析）可以为我们完成这项工作，给出我们投影的向量，按重要性递减的顺序，或者给出一组单词向量变化最大的方向。也就是说，奇异值分解确保这种投影给出单词向量的最佳视图，使它们尽可能远离。
- en: 'There are many nice worked examples on the web. In the publication [*https://www.researchgate.net/publication/338405739_WET_Word_Embedding-Topic_distribution_vectors_for_MOOC_video_lectures_dataset*](https://www.researchgate.net/publication/338405739_WET_Word_Embedding-Topic_distribution_vectors_for_MOOC_video_lectures_dataset)
    [*Word Embedding-Topic distribution vectors for MOOC (Massive Open Online Courses)
    video lectures dataset*], the authors generate two things from a data set from
    the education domain, namely, the transcripts of 12,032 video lectures from 200
    courses collected from [Coursera](https://www.coursera.org): Word vectors using
    Word2Vec model, and document topic vectors using Latent Dirichlet allocation.
    The data set has 878 thousand sentences and more than 79 million tokens. The vocabulary
    size is over 68 thousand unique words. The individual video transcripts are of
    different lengths, varying from 228 to 32,767 tokens, with an average of 6622
    tokens per video transcript. The authors use Word2Vec’s and Latent Dirichlet Allocation
    implementations in the Gensim package in Python. [Figure 7-2](#Fig_word2vec_visualization_pca)
    shows the publication’s three dimensional visualization of a subset of the word
    vectors using principal component analysis.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 网络上有许多很好的工作示例。在出版物[*https://www.researchgate.net/publication/338405739_WET_Word_Embedding-Topic_distribution_vectors_for_MOOC_video_lectures_dataset*](https://www.researchgate.net/publication/338405739_WET_Word_Embedding-Topic_distribution_vectors_for_MOOC_video_lectures_dataset)
    [*Word Embedding-Topic distribution vectors for MOOC (Massive Open Online Courses)
    video lectures dataset*]中，作者从教育领域的数据集中生成了两样东西，即来自[Coursera](https://www.coursera.org)收集的200门课程中12,032个视频讲座的转录：使用Word2Vec模型的词向量，以及使用潜在狄利克雷分配的文档主题向量。数据集包含878万个句子和超过7900万个标记。词汇量超过68,000个独特单词。各个视频讲座的转录长度不同，标记数量从228到32,767个不等，平均每个视频讲座有6622个标记。作者在Python的Gensim包中使用了Word2Vec和潜在狄利克雷分配的实现。[图7-2](#Fig_word2vec_visualization_pca)展示了该出版物使用主成分分析的三维可视化的部分词向量。
- en: '![280](assets/emai_0702.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![280](assets/emai_0702.png)'
- en: 'Figure 7-2\. Three dimensional visualization of word vectors using the first
    three principal components. This example highlights the vector representing the
    word ‘studying’ and its neighbours: academic, studies, institution, reading, *etc.*
    ([image source](https://www.researchgate.net/publication/338405739_WET_Word_Embedding-Topic_distribution_vectors_for_MOOC_video_lectures_dataset)).'
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2. 使用前三个主成分的三维词向量可视化。此示例突出显示了代表单词“studying”及其邻居的向量：academic, studies, institution,
    reading, *等*（[图片来源](https://www.researchgate.net/publication/338405739_WET_Word_Embedding-Topic_distribution_vectors_for_MOOC_video_lectures_dataset)）。
- en: 'Note that word vectors and document topic vectors are not an end by themselves;
    instead they are a means to an end, which is usually a natural language processing
    task, such as: Classiﬁcation within specific domains, such as the massive open
    online courses (MOOCS) in the example above; benchmarking and performance analysis
    of existing and new models; transfer learning, recommendation systems, contextual
    analysis, short text enrichment with topics, personalized learning; organizing
    content in a way that is easy to search and for maximum visibility. We visit such
    tasks shortly.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，词向量和文档主题向量本身并不是目的，而是达到目的的手段，通常是自然语言处理任务，例如：特定领域内的分类，例如上面的大规模在线开放课程（MOOCS）；现有和新模型的基准测试和性能分析；迁移学习，推荐系统，上下文分析，用主题丰富短文本，个性化学习；以易于搜索和最大可见性的方式组织内容。我们很快会讨论这些任务。
- en: Facebook’s fastText vector representation of individual n-character grams
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Facebook的fastText对个别n字符gram的向量表示
- en: 'Facebook’s fastText is similar to Word2Vec, but instead of representing full
    words or n-grams as vectors, it is trained to output a vector representation for
    every *n-character* gram. This enables fastText to handle rare, mispelled, and
    even partial words, such as the ones frequently appearing in social media posts.
    During training, Word2Vec’s skip-gram algorithm learns to predict the surrounding
    context of a given word. Similarly, fastText’s n-character gram algorithm learns
    to predict a word’s surrounding n-character grams, hence providing more granularity
    and flexibility. For example, instead of only representing the full word lovely
    as a vector, it will represent the 2 and 3 grams as vectors as well: lo, lov,
    ov, ove, ve, vel, el, ely, ly.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook的fastText类似于Word2Vec，但它不是将完整单词或n-gram表示为向量，而是训练为每个*n字符* gram输出一个向量表示。这使得fastText能够处理罕见、拼写错误甚至部分单词，例如社交媒体帖子中经常出现的单词。在训练过程中，Word2Vec的skip-gram算法学习预测给定单词的周围上下文。类似地，fastText的n字符gram算法学习预测单词周围的n字符gram，因此提供了更多的细粒度和灵活性。例如，它不仅仅将单词lovely表示为一个向量，还会将2个和3个gram也表示为向量：lo,
    lov, ov, ove, ve, vel, el, ely, ly。
- en: Facebook released their pretrained fastText models for 294 languages, trained
    on available Wikipedea corpora for these languages. These range from Abkhazian
    to Zulu, and include rare languages only spoken by a handful of people. Of course,
    the accuracy of the released models vary across languages and depend on the availability
    and quality of the training data.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook发布了他们预训练的fastText模型，涵盖了294种语言，这些模型是在这些语言的可用维基百科语料库上训练的。这些语言从阿布哈兹语到祖鲁语，包括只有少数人口使用的罕见语言。当然，发布的模型的准确性在不同语言之间有所差异，并取决于训练数据的可用性和质量。
- en: Doc2Vec or Par2Vec vector representation of a document
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Doc2Vec或Par2Vec文档的向量表示
- en: How about representing documents semantically? In previous sections we were
    able to represent entire documents as topic vectors, but Word2Vec only represents
    individual words or phrases as vectors. Can we then extend Word2Vec model to represent
    entire documents as vectors carrying meaning? The (2014) paper [*Distributed Representations
    of Sentences and Documents*](https://arxiv.org/abs/1405.4053) does exactly that,
    with an unsupervised algorithm that learns fixed-length dense vectors from variable-length
    pieces of texts, such as sentences, paragraphs, and documents. The tutorial [*Doc2Vec
    tutorial using Gensim*](https://medium.com/@klintcho/doc2vec-tutorial-using-gensim-ab3ac03d3a1)
    walks through the Python implementation process, producing a fixed size vector
    for each full document in a given corpus.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 那么如何在语义上表示文档呢？在前面的部分中，我们能够将整个文档表示为主题向量，但Word2Vec只将单个单词或短语表示为向量。那么我们能否将Word2Vec模型扩展到将整个文档表示为携带含义的向量？2014年的论文[*句子和文档的分布式表示*](https://arxiv.org/abs/1405.4053)正是这样做的，它使用一种无监督算法从可变长度的文本片段（如句子、段落和文档）中学习固定长度的密集向量。教程[*使用Gensim进行Doc2Vec教程*](https://medium.com/@klintcho/doc2vec-tutorial-using-gensim-ab3ac03d3a1)通过Python实现过程，为给定语料库中的每个完整文档生成固定大小的向量。
- en: Global Vector or (GloVe) vector represenation of words
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全局向量或（GloVe）单词向量表示
- en: There are other ways to produce vectors representing meanings of words. [Gobal
    vector GloVe (2014)](https://nlp.stanford.edu/projects/glove/) is a model that
    obtains such vectors using the singular value decomposition. It is trained only
    on the non-zero entries of a global word-word co-occurrence matrix, which tabulates
    how frequently words co-occur with one another across an entire corpus.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他产生表示单词含义的向量的方法。[Gobal vector GloVe (2014)](https://nlp.stanford.edu/projects/glove/)是一个使用奇异值分解获得这些向量的模型。它仅在全局单词共现矩阵的非零条目上进行训练，该矩阵记录了整个语料库中单词彼此共现的频率。
- en: GloVe is essentially a *log-bilinear model* with a weighted least-squares objective.
    The log-bilinear model is perhaps the simplest neural language model. Given the
    preceding *n-1* words, the log-bilinear model computes an initial vector representation
    for the next word simply by linearly combining the vector representations of these
    preceding *n-1* words. Then the probability for the occurence of the next word
    given those *n-1* preceding words is computed based on computing the similarity
    (dot product) between the linear combination vector representation and the representations
    of all words in the vocabulary.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe本质上是一个带有加权最小二乘目标的*对数双线性模型*。对数双线性模型可能是最简单的神经语言模型。给定前*n-1*个单词，对数双线性模型通过简单地线性组合这些前*n-1*个单词的向量表示来计算下一个单词的初始向量表示。然后，基于计算线性组合向量表示与词汇表中所有单词的表示之间的相似性（点积），计算给定这前*n-1*个单词的下一个单词出现的概率。
- en: <math alttext="dollar-sign upper P r o b left-parenthesis w Subscript n Baseline
    equals w vertical-bar w 1 comma w 2 comma ellipsis comma w Subscript n minus 1
    Baseline right-parenthesis equals StartFraction e x p left-parenthesis w Subscript
    v o c a b Sub Subscript i Subscript Superscript t Baseline w right-parenthesis
    Over e x p left-parenthesis w Subscript v o c a b 1 Superscript t Baseline w right-parenthesis
    plus e x p left-parenthesis w Subscript v o c a b 2 Superscript t Baseline w right-parenthesis
    plus ellipsis plus e x p left-parenthesis w Subscript v o c a b Sub Subscript
    v o c a b minus s i z e Subscript Superscript t Baseline w right-parenthesis EndFraction
    dollar-sign"><mrow><mi>P</mi> <mi>r</mi> <mi>o</mi> <mi>b</mi> <mrow><mo>(</mo>
    <msub><mi>w</mi> <mi>n</mi></msub> <mo>=</mo> <mi>w</mi> <mo>|</mo> <msub><mi>w</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo>
    <mo>,</mo> <msub><mi>w</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><msubsup><mi>w</mi>
    <mrow><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><msub><mi>b</mi> <mi>i</mi></msub></mrow>
    <mi>t</mi></msubsup> <mi>w</mi><mo>)</mo></mrow> <mrow><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><msubsup><mi>w</mi>
    <mrow><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><msub><mi>b</mi> <mn>1</mn></msub></mrow>
    <mi>t</mi></msubsup> <mi>w</mi><mo>)</mo></mrow><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><msubsup><mi>w</mi>
    <mrow><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><msub><mi>b</mi> <mn>2</mn></msub></mrow>
    <mi>t</mi></msubsup> <mi>w</mi><mo>)</mo></mrow><mo>+</mo><mo>⋯</mo><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><msubsup><mi>w</mi>
    <mrow><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><msub><mi>b</mi> <mrow><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><mi>b</mi><mo>-</mo><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi></mrow></msub></mrow>
    <mi>t</mi></msubsup> <mi>w</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign upper P r o b left-parenthesis w Subscript n Baseline
    equals w vertical-bar w 1 comma w 2 comma ellipsis comma w Subscript n minus 1
    Baseline right-parenthesis equals StartFraction e x p left-parenthesis w Subscript
    v o c a b Sub Subscript i Subscript Superscript t Baseline w right-parenthesis
    Over e x p left-parenthesis w Subscript v o c a b 1 Superscript t Baseline w right-parenthesis
    plus e x p left-parenthesis w Subscript v o c a b 2 Superscript t Baseline w right-parenthesis
    plus ellipsis plus e x p left-parenthesis w Subscript v o c a b Sub Subscript
    v o c a b minus s i z e Subscript Superscript t Baseline w right-parenthesis EndFraction
    dollar-sign"><mrow><mi>P</mi> <mi>r</mi> <mi>o</mi> <mi>b</mi> <mrow><mo>(</mo>
    <msub><mi>w</mi> <mi>n</mi></msub> <mo>=</mo> <mi>w</mi> <mo>|</mo> <msub><mi>w</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo>
    <mo>,</mo> <msub><mi>w</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><msubsup><mi>w</mi>
    <mrow><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><msub><mi>b</mi> <mi>i</mi></msub></mrow>
    <mi>t</mi></msubsup> <mi>w</mi><mo>)</mo></mrow> <mrow><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><msubsup><mi>w</mi>
    <mrow><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><msub><mi>b</mi> <mn>1</mn></msub></mrow>
    <mi>t</mi></msubsup> <mi>w</mi><mo>)</mo></mrow><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><msubsup><mi>w</mi>
    <mrow><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><msub><mi>b</mi> <mn>2</mn></msub></mrow>
    <mi>t</mi></msubsup> <mi>w</mi><mo>)</mo></mrow><mo>+</mo><mo>⋯</mo><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><msubsup><mi>w</mi>
    <mrow><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><msub><mi>b</mi> <mrow><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><mi>b</mi><mo>-</mo><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi></mrow></msub></mrow>
    <mi>t</mi></msubsup> <mi>w</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
- en: The main intuition underlying the Global Vector model is the simple observation
    that ratios of word-word co-occurrence probabilities potentially encode some form
    of meaning. The example on the GloVe project website considers the co-occurrence
    probabilities for target words ice and steam with various probe words from the
    vocabulary. [Figure 7-3](#Fig_GloVe_probability_table) shows the actual probabilities
    from a 6 billion word corpus.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 全局向量模型的主要直觉是简单观察到单词共现概率的比值可能编码某种含义。GloVe项目网站上的示例考虑了目标单词"ice"和"steam"与词汇表中各种探针词的共现概率。[图7-3](#Fig_GloVe_probability_table)显示了来自60亿字的语料库的实际概率。
- en: '![280](assets/emai_0703.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![280](assets/emai_0703.png)'
- en: Figure 7-3\. Probability table showing the occurence of the words ice and steam
    with the words solid, gas, water and fashion ([image source](https://nlp.stanford.edu/projects/glove/)).
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-3。显示单词"ice"和"steam"与单词"solid"、"gas"、"water"和"fashion"共现的概率表格（[图片来源](https://nlp.stanford.edu/projects/glove/)）。
- en: Observing the table in [Figure 7-3](#Fig_GloVe_probability_table), we notice
    that, as expected, the word ice co-occurs more frequently with the word solid
    than it does with the word gas, whereas the word steam co-occurs more frequently
    with the word gas than it does with the word solid. Both ice and steam co-occur
    with their shared property water frequently, and both co-occur with the unrelated
    word fashion infrequently. Calculating the the ratio of probabilities cancels
    out the noise from non-discriminative words like water, so that large values,
    much greater than 1, correlate well with properties specific to ice, and small
    values, much less than 1, correlate well with properties specific of steam. This
    way, the ratio of probabilities encodes some crude form of meaning associated
    with the abstract concept of thermodynamic phase.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 观察[图7-3](#Fig_GloVe_probability_table)中的表格，我们注意到，如预期，单词"ice"与单词"solid"的共现频率比与单词"gas"的共现频率更高，而单词"steam"与单词"gas"的共现频率比与单词"solid"的共现频率更高。"ice"和"steam"都经常与它们共享的属性"water"共现，而与不相关的单词"fashion"共现的频率较低。计算概率的比值可以消除与非歧视性词语（如"water"）相关的噪音，因此大于1的大值与"ice"特有的属性相关联，而远小于1的小值与"steam"特有的属性相关联。这样，概率的比值编码了与热力学相位这一抽象概念相关的某种粗糙含义。
- en: The training objective of GloVe is to learn word vectors such that their dot
    product equals the logarithm of the probability of co-occurrence of words. Since
    the logarithm of a ratio is equal to the difference of logarithms, this objective
    considers vector differences in the word vector space. Because these ratios can
    encode some form of meaning, this information gets encoded as vector differences
    as well. For this reason, the resulting word vectors perform very well on word
    analogy tasks, such as those discussed in the Word2Vec package.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe的训练目标是学习单词向量，使它们的点积等于单词共现的概率的对数。由于比值的对数等于对数的差，这个目标考虑了单词向量空间中的向量差异。由于这些比值可以编码某种含义，这些信息也被编码为向量差异。因此，由此产生的单词向量在词类比任务上表现非常出色，例如在Word2Vec包中讨论的任务。
- en: Since singular value decomposition algorithms have been optimized for decades,
    GloVe has an advantage in training over Word2Vec, which is a neural network and
    relies on gradient descent and backpropagation to perform its error minimization.
    If training our own word vectors from a certain corpus that we care for, we are
    probably better off using a Gobal Vector model than Word2Vec, even though Word2vec
    is the first to accomplish semantic and logical reasoning with words, since Global
    Vector trains faster, has better RAM and CPU efficiency, and gives more accurate
    results than Word2Vec even on smaller corpora.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 由于奇异值分解算法已经经过数十年的优化，GloVe在训练方面比Word2Vec具有优势，后者是一个神经网络，依赖于梯度下降和反向传播来执行其误差最小化。如果我们要从我们关心的某个语料库中训练自己的词向量，我们可能最好使用全局向量模型而不是Word2Vec，即使Word2Vec是第一个用单词实现语义和逻辑推理的模型，因为全局向量训练速度更快，具有更好的RAM和CPU效率，并且即使在较小的语料库上也比Word2Vec给出更准确的结果。
- en: Cosine Similarity
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 余弦相似度
- en: 'So far in this chapter we have worked towards one goal only: Convert a document
    of natural language text into a vector of numbers. Our document can be one word,
    one sentence, a paragraph, multiple paragraphs, or longer. We discovered multiple
    ways to get our vectors, some are more semantically representative of our documents
    than others.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们只致力于一个目标：将自然语言文本的文档转换为数字向量。我们的文档可以是一个词、一个句子、一个段落、多个段落或更长。我们发现了多种获取向量的方法，其中一些更具语义代表性。
- en: Once we have a document’s vector representation, we can feed it into machine
    learning models, such as classification algorithms, clustering algoritms, or others.
    One example is to cluster the document vectors of a corpus with some clustering
    algorithm such as *k-means* in order to create a document classifier. We can also
    determine how semantically similar our document is to other documents, for search
    engines, information retrieval systems, and other applications.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了一个文档的向量表示，我们可以将其输入到机器学习模型中，例如分类算法、聚类算法或其他算法。一个例子是使用一些聚类算法（如*k-means*）对语料库的文档向量进行聚类，以创建一个文档分类器。我们还可以确定我们的文档在语义上与其他文档有多相似，用于搜索引擎、信息检索系统和其他应用。
- en: 'We have established that due to the curse of dimensionality, measuring the
    Euclidean distance between two very high dimensional document vectors is useless,
    since they would come out extremely far apart, only because of the vastness of
    the space they inhabit. So how do we determine whether vectors representing documents
    are *close* or *far*, or *similar* or *different*? One successful way is to use
    *cosine similarity*, measuring the cosine of the angle between the two document
    vectors. This is given by the dot product of the vectors, each normalized by its
    length (had we normalized the document vectors ahead of time, then their lengths
    would’ve already been one):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定，由于维度诅咒，测量两个非常高维文档向量之间的欧氏距离是没有意义的，因为它们会因为所处空间的广阔而相距甚远。那么我们如何确定代表文档的向量是“接近”还是“远”，或者是“相似”还是“不同”？一个成功的方法是使用“余弦相似度”，测量两个文档向量之间的夹角的余弦值。这由向量的点积给出，每个向量都被其长度归一化（如果我们提前归一化文档向量，那么它们的长度已经是一了）：
- en: <math alttext="dollar-sign cosine left-parenthesis angle between ModifyingAbove
    d o c 1 With right-arrow and ModifyingAbove d o c 2 With right-arrow right-parenthesis
    equals StartFraction ModifyingAbove d o c 1 With right-arrow Superscript t Baseline
    Over l e n g t h left-parenthesis ModifyingAbove d o c 1 With right-arrow right-parenthesis
    EndFraction StartFraction ModifyingAbove d o c 2 With right-arrow Over l e n g
    t h left-parenthesis ModifyingAbove d o c 2 With right-arrow right-parenthesis
    EndFraction dollar-sign"><mrow><mo form="prefix">cos</mo> <mrow><mo>(</mo> <mtext>angle</mtext>
    <mtext>between</mtext> <mover accent="true"><mrow><mi>d</mi><mi>o</mi><msub><mi>c</mi>
    <mn>1</mn></msub></mrow> <mo>→</mo></mover> <mtext>and</mtext> <mover accent="true"><mrow><mi>d</mi><mi>o</mi><msub><mi>c</mi>
    <mn>2</mn></msub></mrow> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msup><mover
    accent="true"><mrow><mi>d</mi><mi>o</mi><msub><mi>c</mi> <mn>1</mn></msub></mrow>
    <mo>→</mo></mover> <mi>t</mi></msup> <mrow><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo>(</mo><mover
    accent="true"><mrow><mi>d</mi><mi>o</mi><msub><mi>c</mi> <mn>1</mn></msub></mrow>
    <mo>→</mo></mover><mo>)</mo></mrow></mfrac> <mfrac><mover accent="true"><mrow><mi>d</mi><mi>o</mi><msub><mi>c</mi>
    <mn>2</mn></msub></mrow> <mo>→</mo></mover> <mrow><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo>(</mo><mover
    accent="true"><mrow><mi>d</mi><mi>o</mi><msub><mi>c</mi> <mn>2</mn></msub></mrow>
    <mo>→</mo></mover><mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign cosine left-parenthesis angle between ModifyingAbove
    d o c 1 With right-arrow and ModifyingAbove d o c 2 With right-arrow right-parenthesis
    equals StartFraction ModifyingAbove d o c 1 With right-arrow Superscript t Baseline
    Over l e n g t h left-parenthesis ModifyingAbove d o c 1 With right-arrow right-parenthesis
    EndFraction StartFraction ModifyingAbove d o c 2 With right-arrow Over l e n g
    t h left-parenthesis ModifyingAbove d o c 2 With right-arrow right-parenthesis
    EndFraction dollar-sign"><mrow><mo form="prefix">cos</mo> <mrow><mo>(</mo> <mtext>angle</mtext>
    <mtext>between</mtext> <mover accent="true"><mrow><mi>d</mi><mi>o</mi><msub><mi>c</mi>
    <mn>1</mn></msub></mrow> <mo>→</mo></mover> <mtext>and</mtext> <mover accent="true"><mrow><mi>d</mi><mi>o</mi><msub><mi>c</mi>
    <mn>2</mn></msub></mrow> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msup><mover
    accent="true"><mrow><mi>d</mi><mi>o</mi><msub><mi>c</mi> <mn>1</mn></msub></mrow>
    <mo>→</mo></mover> <mi>t</mi></msup> <mrow><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo>(</mo><mover
    accent="true"><mrow><mi>d</mi><mi>o</mi><msub><mi>c</mi> <mn>1</mn></msub></mrow>
    <mo>→</mo></mover><mo>)</mo></mrow></mfrac> <mfrac><mover accent="true"><mrow><mi>d</mi><mi>o</mi><msub><mi>c</mi>
    <mn>2</mn></msub></mrow> <mo>→</mo></mover> <mrow><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo>(</mo><mover
    accent="true"><mrow><mi>d</mi><mi>o</mi><msub><mi>c</mi> <mn>2</mn></msub></mrow>
    <mo>→</mo></mover><mo>)</mo></mrow></mfrac></mrow></math>
- en: '[Figure 7-4](#Fig_cosine_similarity) shows three documents represented in a
    two dimensional vector space. We care about the angles between them'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-4](#Fig_cosine_similarity)展示了在二维向量空间中表示的三个文档。我们关心它们之间的夹角。'
- en: '![300](assets/emai_0704.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![300](assets/emai_0704.png)'
- en: Figure 7-4\. three documents represented in a two dimensional vector space.
  id: totrans-176
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-4. 在二维向量空间中表示的三个文档。
- en: The cosine of an angle is always a number between -1 and 1\. When two document
    vectors are perfectly aligned and pointing in the same direction along all the
    dimensions, their cosine similarity is 1; when they are perfect opposites of each
    other regarding every single dimension, their cosine similarity is -1; and when
    they are orthogonal to each other, their cosine similarity is zero.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一个角的余弦值始终在-1和1之间。当两个文档向量完全对齐并沿着所有维度指向相同方向时，它们的余弦相似度为1；当它们在每个维度上都是完全相反的时，它们的余弦相似度为-1；当它们彼此正交时，它们的余弦相似度为零。
- en: Natural Language Processing Applications
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理应用
- en: The bulk of this chapter was about converting a given document of natural language
    text to a vector of numbers. We have established that there are multiple ways
    to get our document vectors, all leading to varying representations (and hence
    conclusions), or emphasizing certain aspects of the given natural language data
    over others. For people entering the natural language processing subfield of AI,
    this is one of the hardest barriers to overcome, especially if they are from a
    quantitative background, where the entities they work with are inherently numerical,
    ripe for mathematical modeling and analysis. Now that we have overcome this barrier,
    equipped with concrete vector representations for natural language data, we can
    think mathematically about popular applications. It is important to be aware that
    there are multiple ways to accomplish each of the following. Traditional approaches
    are hard coded rules, assigning scores to words, punctuations, emojis, etc., then
    relying on the existence of these in a data sample to produce a result. Modern
    approaches rely on various machine learning models, which in turn rely on (mostly)
    labeled training data sets. To excell in this field, we must set time aside and
    try different models on the same task, compare performance, and gain an in depth
    understanding of each model along with its strengths, weaknesses, and mathematical
    justifications of its successes and failures.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的大部分内容是关于将给定的自然语言文本文档转换为数字向量。我们已经确定有多种方法可以获得我们的文档向量，所有这些方法都会导致不同的表示（因此结论也不同），或者强调给定自然语言数据的某些方面而不是其他方面。对于进入人工智能自然语言处理子领域的人来说，这是最难克服的障碍之一，特别是如果他们来自数量背景，那里他们处理的实体本质上是数字，适合数学建模和分析。现在我们已经克服了这个障碍，具备了自然语言数据的具体向量表示，我们可以数学地思考流行的应用。重要的是要意识到每个任务都有多种实现方式。传统方法是硬编码规则，为单词、标点符号、表情符号等分配分数，然后依赖于数据样本中这些元素的存在来产生结果。现代方法依赖于各种机器学习模型，这些模型又依赖于（主要是）标记的训练数据集。要在这个领域取得成功，我们必须抽出时间，尝试在同一个任务上使用不同的模型，比较性能，并深入了解每个模型以及其成功和失败的数学理由。
- en: Sentiment Analysis
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 情感分析
- en: 'Hard coded rules: A successful algorithm is VADER or Valence Aware Dictionary
    for sEntiment Reasoning. A tokenizer here needs to handle punctuation and emojis
    properly, since these convey a lot of sentiment. We also have to manually compile
    thousands of words along with their sentiment score, as opposed to having the
    machine accomplish this automatically.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬编码规则：一个成功的算法是VADER或情感推理的Valence Aware Dictionary。这里的分词器需要正确处理标点符号和表情符号，因为这些传达了很多情感。我们还必须手动编译成千上万个单词以及它们的情感分数，而不是让机器自动完成这个任务。
- en: '[Naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier):
    This is a set of classifying algorithms based in Bayes Theorem from probability.
    The decision rule for classification on maximum likelihood. This will be discussed
    in the probability and measure chapter.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[朴素贝叶斯分类器](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)：这是一组基于贝叶斯定理的分类算法。分类的决策规则是最大似然。这将在概率和测度章节中讨论。'
- en: 'Latent discriminant analysis: In the previous section, we learned how to classify
    documents using latent discriminant analysis.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜在判别分析：在前一节中，我们学习了如何使用潜在判别分析对文档进行分类。
- en: 'Using latent semantic analysis: Clusters of document vectors formed using latent
    semantic analysis can be used for classification. Ideally, positive reviews cluster
    away from negative reviews in latent semantic analysis’s topic spaces. Given a
    bunch of reviews labeled positive or negative, we first compute their topic vectors
    using latent semantic analysis. Now inorder to classify a new review, we can compute
    its topic vector, then that topic vector’s cosine similarity with the positive
    and negative topic vectors. Finally we classify the review as positive if it is
    more similar to positive topic vectors, and negative if more similar to negative
    topic vectors.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用潜在语义分析：使用潜在语义分析形成的文档向量集群可以用于分类。理想情况下，在潜在语义分析的主题空间中，正面评价与负面评价会聚集在一起。给定一堆标记为正面或负面的评价，我们首先使用潜在语义分析计算它们的主题向量。现在，为了对新的评价进行分类，我们可以计算其主题向量，然后将该主题向量与正面和负面主题向量的余弦相似度进行比较。最后，如果评价更类似于正面主题向量，则将其分类为正面，如果更类似于负面主题向量，则将其分类为负面。
- en: 'Transformers, convolutional neural network, recurrent long short term memory
    neural network: All of these modern machine learning methods require passing our
    document in vector form into a neural network with a certain architecture. We
    will spend time on these state of the art methods shortly.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变压器、卷积神经网络、循环长短期记忆神经网络：所有这些现代机器学习方法都需要将我们的文档以向量形式传递到具有特定架构的神经网络中。我们将很快花时间研究这些最先进的方法。
- en: Spam Filter
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 垃圾邮件过滤器
- en: Mathematically, spam filtering is a similar classification problem as sentiment
    analysis discussed above, when the sentiment of a document is either positive
    or negative. Thus, the same methods for sentiment classification apply for spam
    filtering. In all cases, it doesn’t matter how we create our document vectors,
    we can use them to predict whether a social post is spam or not spam, predict
    how likely it is to get *likes*, *etc.*.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，垃圾邮件过滤与上述讨论的情感分析是一个类似的分类问题，当文档的情感为正面或负面时。因此，情感分类的相同方法也适用于垃圾邮件过滤。在所有情况下，我们创建文档向量的方式并不重要，我们可以使用它们来预测社交帖子是否是垃圾邮件，预测它有多大可能获得*赞*等。
- en: Search And Information Retrieval
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 搜索和信息检索
- en: Again, no matter how we create the numerical vectors representing documents,
    we can use them for search and information retrieval tasks. The search can be
    index based or a semantic based, finding documents based on their meaning.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，无论我们如何创建代表文档的数值向量，我们都可以将它们用于搜索和信息检索任务。搜索可以基于索引或基于语义，根据它们的含义找到文档。
- en: 'Full text search: When we search for a document based on a word or a partial
    word that it contains. Search engines break documents into words that can be indexed,
    similar indexes we find at the end of textbooks. Of course spelling errors and
    typos require a lot of tracking and sometimes guessing. Indices, when available,
    work pretty well.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全文搜索：当我们基于文档中包含的单词或部分单词搜索文档时。搜索引擎将文档分解为可以索引的单词，类似于我们在教科书末尾找到的索引。当然，拼写错误和打字错误需要大量的跟踪和有时猜测。索引，一旦可用，工作得相当不错。
- en: 'Semantic search: When our search for documents takes into account the meaning
    of the words in both our query and in the documents among which we are searching.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义搜索：当我们搜索文档时考虑到查询和我们搜索的文档中单词的含义。
- en: Based on cosine similarity between TF-IDF of documents (for corpuses containing
    billions of documents). Any search engine with a milisecond response utilizes
    a TF-IDF matrix.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于文档的TF-IDF之间的余弦相似度（对包含数十亿文档的语料库）。任何具有毫秒响应的搜索引擎都利用TF-IDF矩阵。
- en: 'Based on semantics: Cosine similarity between topic vectors of documents obtained
    through Latent Semantic Analysis (for corpuses containing millions of documents)
    or Latent Dirichlet allocation (for much smaller corpuses). This is similar to
    how we classified whether a message is spammy or not using latent semantic analysis,
    except that now we compute the cosine similarity between the new document’s topic
    vector and *all* the topic vectors of our database, returning the ones that are
    most similar to our document.'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于语义：通过潜在语义分析（对包含数百万文档的语料库）或潜在狄利克雷分配（对规模较小的语料库）获得的文档主题向量之间的余弦相似度。这类似于我们如何使用潜在语义分析来分类一条消息是否是垃圾邮件，只是现在我们计算新文档的主题向量与我们数据库中*所有*主题向量之间的余弦相似度，返回与我们文档最相似的那些。
- en: Based on eigenvector iteration
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于特征向量迭代
- en: 'Semantic search and queries using word vectors (Word2Vec or GloVe): Consider
    a search like this (this example is adopted from the book Natural Language Processing
    In Action): *She invented something to do with physics in Europe in the early
    20th century*. When we enter our search sentence into Google or Bing, we may not
    get the direct answer *Marie Curie*. Google Search will most likely only give
    us links to lists of famous physicists, both men and women. After searching several
    pages we find *Marie Curie*, our answer. Google will take note of that, and refine
    our results net time we search. Now using word vectors, we can do simple arithmetic
    on the word vectors representing: woman+Europe+physics+scientist+famous, then
    we would obtain a new vector, close in cosine similarity to the vector representing
    *Marie Curie*, and Voila! We have our answer. We can even subtract gender bias
    in the natural sciences from word vectors by simply subtracting the vector representing
    the token *man*, *male*, *etc.*, so we can search for the word vector closest
    to: woman+Europe+physics+scientist-male-2*man.'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 语义搜索和使用词向量（Word2Vec或GloVe）的查询：考虑这样一个搜索（这个例子改编自书籍《自然语言处理实战》）：*她在20世纪初在欧洲发明了与物理有关的东西*。当我们将我们的搜索句子输入到Google或Bing时，我们可能不会直接得到答案*玛丽·居里*。Google搜索很可能只会给我们一些著名物理学家的链接，包括男性和女性。在搜索了几页之后，我们找到了*玛丽·居里*，这就是我们的答案。Google会注意到这一点，并在下次搜索时优化我们的结果。现在使用词向量，我们可以对代表的单词向量进行简单的算术运算：女人+欧洲+物理+科学家+著名，然后我们会得到一个新的向量，与代表*玛丽·居里*的向量在余弦相似度上接近，Voila！我们得到了答案。我们甚至可以通过简单地减去代表*男人*、*男性*等的向量来消除自然科学中的性别偏见，这样我们就可以搜索与女人+欧洲+物理+科学家-男性-2*男最接近的词向量。
- en: 'Search based on analogy questions: To compute a search such as *They are to
    music what Marie Curie is to science*, all we have to do is simple vector arithmetic
    of the word vectors representing: Marie Curie-science+music.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于类比问题的搜索：要计算这样一个搜索*他们对音乐的意义就像玛丽·居里对科学的意义一样*，我们只需要对代表的单词向量进行简单的向量算术：玛丽·居里-科学+音乐。
- en: 'This note is paraphrased from the book Natural Language Processing In Action:
    Traditional indexing approaches work with binary word occurrence vectors, discrete
    vectors (bag of word vectors), sparse floating point number vectors (Term Frequency
    times Inverse Document Frequency vectors), and low-dimensional floating point
    number vectors (such as three dimensional Geographic Information Systems data).
    But high-dimensional floating point number vectors, such as topic vectors from
    Latent Semantic Analysis or Latent Dirichlet Allocation, are challenging. Inverted
    indexes work for discrete vectors or binary vectors, because the index only needs
    to maintain an entry for each nonzero discrete dimension. Either that value of
    that dimension is present or not present in the referenced vector or document.
    Because TF-IDF vectors are sparse, mostly zero, we do not need an entry in our
    index for most dimensions for most documents. Now Latent Semantic Analysis and
    Latent Dirichlet Allocation produce topic vectors that are high-dimensional, continuous,
    and dense, where zeros are rare. Moreover, the semantic analysis algorithm does
    not produce an efficient index for scalable search. This is exasperated by the
    curse of dimensionality, which makes an exact index impossible. One solution to
    the challenge of high-dimensional vectors is to index them with a locality sensitive
    hash, like a ZIP code, that designates a region of hyperspace. Such a hash is
    similar to a regular hash: It is discrete and it only depends on the values in
    the vector. But even this doesn’t work perfectly once we exceed about 12 dimensions.
    Exact semantic search wouldn’t work for a large corpus, such as Google Search
    or even Wikipedia semantic search. The key is to settle for *good enough* rather
    than striving for a perfect index or a latent hashing algorithm for our high-dimensional
    vectors. There are now several open source implementations of some efficient and
    accurate approximate nearest neighbors algorithms that use latent semantic hashing
    to efficiently implement semantic search. Technically these indexing or hashing
    solutions cannot guarantee that we will find all the best matches for our semantic
    search query. But they can get a good list of close matches almost as fast as
    with a conventional reverse index on a TF-IDF vector or bag-of-words vector, if
    we are willing to give up a little precision. Neural network models fine tune
    the concepts of topic vectors so that the vectors associated with words are more
    precise and useful, hence enhancing searches.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇笔记是从书籍《自然语言处理实战》中改编的：传统的索引方法使用二进制词出现向量、离散向量（词袋向量）、稀疏浮点数向量（词频乘以逆文档频率向量）和低维浮点数向量（如三维地理信息系统数据）。但是，高维浮点数向量，如潜在语义分析或潜在狄利克雷分配的主题向量，是具有挑战性的。倒排索引适用于离散向量或二进制向量，因为索引只需要维护每个非零离散维度的条目。被引用的向量或文档中是否存在该维度的值。由于TF-IDF向量是稀疏的，大部分为零，我们不需要为大多数文档的大多数维度在我们的索引中添加条目。现在，潜在语义分析和潜在狄利克雷分配产生的主题向量是高维、连续且密集的，零值很少。此外，语义分析算法并不为可扩展搜索生成有效的索引。这受到维度诅咒的影响，使得精确索引变得不可能。解决高维向量挑战的一个方法是使用局部敏感哈希对它们进行索引，类似于ZIP代码，指定超空间的区域。这样的哈希类似于常规哈希：它是离散的，只取决于向量中的值。但是，即使在超过大约12个维度时，这也不完美。精确的语义搜索对于大语料库，如Google搜索或甚至维基百科语义搜索，都不适用。关键是要接受*足够好*，而不是追求完美的索引或高维向量的潜在哈希算法。现在有几个开源实现了一些高效准确的近似最近邻算法，使用潜在语义哈希来有效实现语义搜索。从技术上讲，这些索引或哈希解决方案不能保证我们会找到语义搜索查询的所有最佳匹配。但是，如果我们愿意放弃一点精度，它们可以几乎与TF-IDF向量或词袋向量的常规反向索引一样快地获得一份良好的接近匹配列表。神经网络模型微调主题向量的概念，使与单词相关联的向量更精确和有用，从而增强搜索。
- en: Machine Translation
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器翻译
- en: Translating a sequence of tokens of any length (such as a sentence or a paragraph)
    to sequence of any length in a different language. The Coder-decoder architecture,
    dicussed below in the context of transformers and recurrent neural networks, has
    proven successful for translation tasks. The coder-decoder architecture is different
    than the auto-encoder architecture.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 将任意长度的令牌序列（如句子或段落）翻译成不同语言的任意长度序列。下面讨论的编码器-解码器架构，在变压器和循环神经网络的上下文中，已被证明对翻译任务非常成功。编码器-解码器架构与自动编码器架构不同。
- en: Image Captioning
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像字幕
- en: This combines computer vision with natural language processing.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这将计算机视觉与自然语言处理相结合。
- en: Chatbots
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聊天机器人
- en: 'This is the ultimate application of natural language processing. A chatbot
    requires more than one kind of processing: Parse language, search, analyze, generate
    responses, respond to requests and execute them. Moreover, it requires a database
    to maintain a memory of past statements and responses.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这是自然语言处理的终极应用。聊天机器人需要多种处理：解析语言，搜索，分析，生成响应，响应请求并执行它们。此外，它需要一个数据库来维护过去语句和响应的记忆。
- en: Other Applications
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他应用
- en: Other applications include [*named-entity recognition*](https://medium.com/mysuperai/what-is-named-entity-recognition-ner-and-how-can-i-use-it-2b68cf6f545d),
    [*conceptual focus*](https://www.sciencedirect.com/science/article/pii/S1877042815035193),
    relevant information extraction from text (such as dates), and language generation,
    which we visit in the next chapter.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 其他应用包括[*命名实体识别*](https://medium.com/mysuperai/what-is-named-entity-recognition-ner-and-how-can-i-use-it-2b68cf6f545d)，[*概念焦点*](https://www.sciencedirect.com/science/article/pii/S1877042815035193)，从文本中提取相关信息（如日期），以及语言生成，我们将在下一章中讨论。
- en: Transformers And Attention Models
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变压器和注意力模型
- en: Transformers and attention models are the state of the art for natural language
    processing applications such as machine translation, question answering, language
    generation, named-entity recognition, image captioning, and chatbots (as of 2022).
    Currently, they underlie large language models such as [Google’s BERT](https://en.wikipedia.org/wiki/BERT_(language_model))
    and OpenAI’s [GPT-2](https://en.wikipedia.org/wiki/GPT-2) and [GPT-3](https://en.wikipedia.org/wiki/GPT-3).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers和注意力模型是自然语言处理应用的最新技术，如机器翻译、问答、语言生成、命名实体识别、图像字幕和聊天机器人（截至2022年）。目前，它们是大型语言模型的基础，如[Google的BERT](https://en.wikipedia.org/wiki/BERT_(language_model))和OpenAI的[GPT-2](https://en.wikipedia.org/wiki/GPT-2)和[GPT-3](https://en.wikipedia.org/wiki/GPT-3)。
- en: Transformers by-pass both recurrence and convolution architectures, which were
    the go-to architectures for natural language processing applications up until
    2017, when the paper, [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762),
    introduced the first transformer model.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer绕过了直到2017年之前自然语言处理应用中常用的循环和卷积架构，当时论文[*Attention Is All You Need*](https://arxiv.org/abs/1706.03762)引入了第一个transformer模型。
- en: 'The de-throned recurrent and convolutional neural network architectures are
    still in use (and work well) for certain natural language processing applications,
    as well as other applications such as finance. We elaborate on these models later
    in this chapter. However, the reasons that led to abandoning them for natural
    language are:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 被推翻的循环和卷积神经网络架构仍然在某些自然语言处理应用（以及金融等其他应用）中使用（并且表现良好）。我们稍后在本章中详细介绍这些模型。然而，导致放弃它们用于自然语言处理的原因是：
- en: For short input sequences of natural language tokens, the attention layers that
    are involved in transformer models are faster than recurrent layers. Even for
    long sequences, we can modify attention layers to focus only certain neighbourhoods
    within the input.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于自然语言标记的短输入序列，transformer模型中涉及的注意力层比循环层更快。即使对于长序列，我们也可以修改注意力层，只关注输入中的某些邻域。
- en: The number of sequential operations required by a recurrent layer depends on
    the length of the input sequence. This number stays constant for an attention
    layer.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环层所需的顺序操作次数取决于输入序列的长度。对于注意力层，这个数字保持不变。
- en: In convolutional neural networks, the width of the kernel directly affects the
    long-term dependencies between pairs of inputs and corresponding outputs. Tracking
    long-term dependencies then requires large kernels, or stacks of convolutional
    layers, all increasing the computational cost of the natural language model employing
    them.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在卷积神经网络中，卷积核的宽度直接影响输入和输出对之间的长期依赖关系。跟踪长期依赖关系需要大的卷积核或者卷积层的堆叠，这些都增加了使用它们的自然语言模型的计算成本。
- en: The Transformer Architecture
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: transformer架构
- en: 'Transformers are an integral part of enormous language models such as OpenAI’s
    [GPT-2](https://en.wikipedia.org/wiki/GPT-2) (Generative Pre-trained Transformer),
    [GPT-3](https://en.wikipedia.org/wiki/GPT-3), and Google’s BERT (Bidirectional
    Encoder Representations from Transformers, which trains the language model by
    looking at the sequential text data from both left to right and right to left)
    and [Wu Dao’s transformer](https://en.wikipedia.org/wiki/Wu_Dao). These models
    are massive: GPT-2 has around 1.5 billion parameters trained on millions of documents,
    drawn from 8 million websites from all around the internet. GPT-3 has 175 billion
    parameters trained on an even larger data set. Wu Dao’s transformer has a whopping
    1.75 trillion parameters, consuming even more computational resources for training
    and inference.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers是庞大语言模型的重要组成部分，如OpenAI的[GPT-2](https://en.wikipedia.org/wiki/GPT-2)（生成式预训练transformer）、[GPT-3](https://en.wikipedia.org/wiki/GPT-3)和Google的BERT（双向编码器transformer表示，通过从左到右和从右到左查看顺序文本数据来训练语言模型）以及[Wu
    Dao的transformer](https://en.wikipedia.org/wiki/Wu_Dao)。这些模型非常庞大：GPT-2在数百万文档上训练了大约15亿个参数，这些文档来自互联网上的800万个网站。GPT-3在更大的数据集上训练了1750亿个参数。Wu
    Dao的transformer有1.75万亿个参数，为训练和推理消耗更多的计算资源。
- en: 'Transformers were originally designed for language translation tasks, so they
    have an *encoder-decoder* structure. [Figure 7-5](#Fig_transformer_architecture)
    illustrates the architecture of the transformer model originally introduced by
    the paper [Attention Is All You Need (2017)](https://arxiv.org/pdf/1706.03762.pdf).
    However, each of the encoder and decoder are their own modules so they can be
    used separately to perform various tasks. For example, we can use the encoder
    alone to perform a classification task such as part of speech tagging, meaning
    we input the sentence: *I love cooking in my kitchen*, and the output will be
    a class for each word: *I*: pronoun; *love*: verb, cooking: [my grammar is failing
    me editors help me here].'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers最初是为语言翻译任务设计的，因此具有*编码器-解码器*结构。[图7-5](#Fig_transformer_architecture)说明了transformer模型的架构，最初由论文[Attention
    Is All You Need (2017)](https://arxiv.org/pdf/1706.03762.pdf)引入。然而，编码器和解码器各自是独立的模块，因此可以单独使用来执行各种任务。例如，我们可以仅使用编码器执行分类任务，如词性标注，这意味着我们输入句子：“我喜欢在厨房里做饭”，输出将是每个词的类别：“我”：代词；“喜欢”：动词；“做饭”：[我的语法出问题了，编辑帮帮我]。
- en: '![280](assets/emai_0705.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![280](assets/emai_0705.png)'
- en: Figure 7-5\. The simple encoder-decoder architecture of a transformer model
    ([image source](https://arxiv.org/pdf/1706.03762.pdf)).
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-5. transformer模型的简单编码器-解码器架构（[图片来源](https://arxiv.org/pdf/1706.03762.pdf)）。
- en: The input to the full transformer model (with both the encoder and decoder included)
    is a sequence of natural language tokens, of any length, such as a question to
    a chatbot, a paragraph in English that requires translation to French, or summarization
    into a headline. The output is another sequence of natural language tokens, also
    of any length, such as the chatbot’s answer, the translated paragraph in French,
    or the headline.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 完整Transformer模型（包括编码器和解码器）的输入是任意长度的自然语言标记序列，例如对话机器人的问题，需要将英语段落翻译成法语，或者总结成标题。输出是另一个自然语言标记序列，也是任意长度的，例如对话机器人的回答，法语翻译的段落，或标题。
- en: 'Do not confuse the training phase from the inference phase of a model:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 不要混淆模型的训练阶段和推理阶段：
- en: During training, the model is fed both the data and the labels, such as an English
    sentence (input data sample) along with its French translation (label), and the
    model learns a mapping from the input to the target label that generalizes well
    to hopefully the whole of vocabularies and grammars of both languages.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练期间，模型同时接收数据和标签，例如英语句子（输入数据样本）以及其法语翻译（标签），模型学习从输入到目标标签的映射，以期望很好地泛化到两种语言的整个词汇和语法。
- en: During inference, the model is fed only the English sentence, and outputs its
    French translation. Transformers output the French sentence one new token at a
    time.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推理过程中，模型只接收英语句子，并输出其法语翻译。Transformer逐个新标记输出法语句子。
- en: The encoder, on the left half of the Transformer architecture ([Figure 7-5](#Fig_transformer_architecture))
    receives an input of tokens, such as an English sentence, *How was your day?*,
    and produces multiple numerical vector representations each token of this sentence,
    encoding the token’s contextual information from within the sentence. The decoder
    part of the architecture receives these vectors as its input.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器位于Transformer架构的左半部分（见[图7-5](#Fig_transformer_architecture)），接收诸如英语句子“你今天过得怎么样？”之类的标记输入，并为该句子的每个标记产生多个数值向量表示，编码标记在句子中的上下文信息。架构的解码器部分接收这些向量作为其输入。
- en: The decoder, on the right half of the architecture, receives the vector output
    of the encoder together with the decoder’s output at the previous time step [explain
    this]. Ultimately, it generates an output of tokens, such as the French translation
    of the input sentence, *Comme se passe ta journée* (see [Figure 7-6](#Fig_attention_mechanism)).
    What the decoder actually computes is a *probability* for each word in the French
    vocabulary (say 50,000 tokens) using a softmax function, then produces the token
    with the highest probability. In fact, since computing a softmax for such a high
    dimensional vocabulary is expensive, the decoder uses a *sampled softmax* which
    computes the probability for each token in a random sample of the French vocabulary
    at each step. During training, it has to include the target token in this sample,
    but during inference, there is no target token.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器位于架构的右半部分，接收编码器的向量输出以及前一个时间步的解码器输出[解释这个]。最终，它生成标记的输出，例如输入句子的法语翻译“Comme se
    passe ta journée”（见[图7-6](#Fig_attention_mechanism)）。解码器实际计算的是法语词汇中每个单词的*概率*（假设有50,000个标记），使用softmax函数，然后产生概率最高的标记。实际上，由于计算如此高维度词汇的softmax代价高昂，解码器使用*采样softmax*，在每个步骤中计算法语词汇中每个标记的概率。在训练期间，必须在此样本中包含目标标记，但在推理期间，没有目标标记。
- en: Transformers use a process called *attention* to capture long term dependencies
    in sequences of tokens. The word *sequence* is confusing here, especially for
    mathematicians, who have clear distinctions between the terms *sequence*, *series*,
    *vector*, and *list*. Sequences are usually processed one term at a time, meaning
    one term is processed, then the next, then the next, and so on, until the whole
    input is consumed. Transformers do not process input tokens sequentially. They
    process them altogether, in parallel. This is different than the way recurrent
    neural networks process input tokens, which have to be fed sequentially, in effect
    prohibiting parallel computation. If it was up to us to correct this terminology,
    we should call a natural language sentence a *vector* if we are processing it
    using a transformer model, or a *matrix* since each word in the sentence is represented
    as its own vector, or a *tensor* if we process a batch of sentences at a time,
    which the architecture of the transformer allows. If we want to process the same
    exact sentence using a recurrent neural network model, then we should call it
    a *sequence*, since this model consumes its input data sequentially, one token
    at a time. If we process it using a convolutional neural network, then we would
    call it a vector (or matrix) again, since the network consumes it as a whole,
    not broken down into one token at a time.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer使用一种称为*注意力*的过程来捕捉标记序列中的长期依赖关系。这里的“序列”一词可能会让人感到困惑，特别是对于数学家来说，他们在“序列”、“级数”、“向量”和“列表”这些术语之间有明确的区分。序列通常是逐个项地处理，意味着一个项被处理，然后是下一个，然后是下一个，依此类推，直到整个输入被消耗。Transformer不会按顺序处理输入标记。它们一起并行处理。这与递归神经网络处理输入标记的方式不同，后者必须按顺序馈送，实际上禁止了并行计算。如果由我们来纠正这个术语，我们应该称自然语言句子为*向量*，如果我们使用Transformer模型来处理它，或者称为*矩阵*，因为句子中的每个单词都表示为自己的向量，或者称为*张量*，如果我们一次处理一批句子，这是Transformer的架构允许的。如果我们想使用递归神经网络模型处理完全相同的句子，那么我们应该称之为*序列*，因为这个模型按顺序消耗其输入数据，逐个标记处理。如果我们使用卷积神经网络处理它，那么我们会再次称之为向量（或矩阵），因为网络将其作为整体消耗，而不是逐个标记拆分。
- en: It is an advantage when a model does not need to consume the input sequentially,
    because such architectures allow for parallel processing. That said, even though
    parallelization makes transformers computationally efficient, they cannot take
    full advantage of the inherent sequential nature of the natural language input
    and the information encoded within this sequentiality. Think of how humans process
    text. There are new transformer models that try to leverage this.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型不需要按顺序处理输入时，这是一个优势，因为这样的架构允许并行处理。尽管并行化使变压器在计算上更有效，但它们无法充分利用自然语言输入的固有顺序性以及在这种顺序性中编码的信息。想想人类如何处理文本。有一些新的变压器模型试图利用这一点。
- en: 'The Transformer model runs as follows [explain this better]:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器模型的运行如下[更好地解释这一点]：
- en: Represent each word from the input sequence as a d-dimensional vector.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入序列中的每个单词表示为一个d维向量。
- en: 'Incorporate the order of words into the model by adding to the word vector
    information about its position (*positional encoding*): Introduce positional information
    into the input by accompanying each vector of each word with a positional encoding
    vector of the same length. The positional encoding vectors have the same dimension
    the word vector embeddings (this allows the two vectors to be added together).
    There are many choices of positional encodings, some are learned during training,
    others are fixed. Discretized sine and cosine functions with varying frequencies
    are common.'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将单词的顺序加入模型中，通过向单词向量添加关于其位置的信息（*位置编码*）：通过将每个单词的向量与相同长度的位置编码向量相伴，将位置信息引入输入。位置编码向量具有与单词向量嵌入相同的维度（这允许将这两个向量相加）。有许多选择的位置编码，一些在训练期间学习，另一些是固定的。具有不同频率的离散正弦和余弦函数是常见的。
- en: We next feed the positionally encoded word vectors to the encoder block. The
    encoder attends to all words in the input sequence, irrespective if they precede
    or succeed the word under consideration, thus the transformer encoder is bidirectional.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将位置编码的单词向量馈送到编码器块。编码器关注输入序列中的所有单词，无论它们是在考虑的单词之前还是之后，因此变压器编码器是双向的。
- en: The decoder receives as input its own predicted output word at time step t–1,
    along with the output vectors of the encoder.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器在时间步t-1接收到自己预测的输出单词，以及编码器的输出向量。
- en: The input to the decoder is also augmented by positional encoding.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器的输入也通过位置编码进行增强。
- en: The augmented decoder input is fed into the three sublayers. The decoder cannot
    attend to succeeding words, so we apply masking in its first sublayer. At the
    second sublayer, the decoder also receives the output of the encoder, which now
    allows the decoder to attend to all of the words in the input sequence.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增强的解码器输入被馈送到三个子层。解码器不能关注后续单词，因此我们在其第一个子层中应用掩码。在第二个子层中，解码器还接收编码器的输出，这样解码器就可以关注输入序列中的所有单词。
- en: The output of the decoder finally passes through a fully connected layer, followed
    by a softmax layer, to generate a prediction for the next word of the output sequence.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器的输出最终通过一个全连接层，然后是一个softmax层，以生成输出序列的下一个单词的预测。
- en: The Attention Mechanism
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力机制
- en: 'Transformer’s magic is largely due to their built in *attention mechanisms*.
    An attention mechanism comes with bonuses:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器的魔力主要归功于其内置的*注意力机制*。注意力机制带来了一些好处：
- en: '**Explainability**: Pointing out which parts of the input sentence (or document)
    the model paid attention to when producing a particular output (see [Figure 7-6](#Fig_attention_mechanism)).'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**可解释性**：指出模型在生成特定输出时关注输入句子（或文档）的哪些部分（参见[图7-6](#Fig_attention_mechanism)）。'
- en: '**Leveraging pre-trained attention models**: We can adapt pretrained models
    to domain specific tasks. That is, we can further tweak their parameter values
    with extra training on domain specific data.'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**利用预训练的注意力模型**：我们可以将预训练模型调整到特定领域的任务。也就是说，我们可以通过在特定领域的数据上进行额外训练来进一步调整其参数值。'
- en: '**More accurate modeling of longer sentences**: Another value of attention
    mechanisms is that they allow the modeling of dependencies in sequences of natural
    language tokens without regard to how far apart tokens which are related to each
    other occur in these sequences.'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更准确地对长句建模**：注意力机制的另一个价值在于，它们允许对自然语言标记序列中的依赖关系进行建模，而不考虑相关标记在这些序列中相距多远。'
- en: '[Figure 7-6](#Fig_attention_mechanism) illustrates attention for a translation
    task from English to French.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-6](#Fig_attention_mechanism)展示了从英语到法语的翻译任务中的注意力。'
- en: '![280](assets/emai_0706.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![280](assets/emai_0706.png)'
- en: 'Figure 7-6\. Illustrating attention via a translation task: How weights assigned
    to input tokens show which ones the model paid more attention to in order to produce
    each output token ([image source](https://blog.floydhub.com/attention-mechanism/)).'
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-6. 通过翻译任务展示注意力：指示分配给输入标记的权重，显示模型为了生成每个输出标记而更关注哪些标记（[图片来源](https://blog.floydhub.com/attention-mechanism/)）。
- en: 'There is no hard core mathematics involved in an attention mechanism: We only
    have to compute a scaled dot product. The main goal of attention is to highlight
    the most relevant parts of the input sequence, how strongly they relate to each
    other within the input itself, and how strongly they contribute to certain parts
    of the output.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在注意力机制中没有涉及到硬核数学：我们只需要计算一个缩放后的点积。注意力的主要目标是突出输入序列中最相关的部分，它们在输入本身内部之间的关联程度以及它们对输出的某些部分的贡献程度。
- en: '*Self attention* is when a sequence of vectors computes alignment within its
    own members. We are now familiar with the fact that the dot product measures the
    compatibility between two vectors. We can compute the simplest possible self attention
    weights by finding the dot products between all the members of the sequence of
    vectors. For example, for the sentence *I love cooking in my kitchen*, we would
    compute all the dot products between the word vectors representing the words *I*,
    *love*, *cooking*, *in*, *my*, and *kitchen*. We would expect the dot product
    between *I* and *my* to be high, similarly between *cooking* and *kitchen*. However,
    the dot product will be highest between *I* and *I*, *love* and *love*, *etc.*,
    because these vectors are perfectly aligned with themsleves, but there is no valuable
    information gleaned from there. The transformer’s solution to avoiding this waste
    is multifold:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '*自注意力*是指一系列向量在其成员内部计算对齐。我们现在熟悉点积测量两个向量之间的兼容性。我们可以通过找到序列向量的所有成员之间的点积来计算最简单的自注意力权重。例如，对于句子*I
    love cooking in my kitchen*，我们将计算代表单词*I*、*love*、*cooking*、*in*、*my*和*kitchen*的单词向量之间的所有点积。我们期望*I*和*my*之间的点积很高，类似地，*cooking*和*kitchen*之间也是如此。然而，*I*和*I*、*love*和*love*等之间的点积将最高，因为这些向量与自身完全对齐，但从中无法获得有价值的信息。变压器避免这种浪费的解决方案是多方面的：'
- en: 'First, apply three different tranformations to each vector of the input sequence
    (each word of the sentence), multiplying them by three different weight matrices.
    We then obtain three different sets of vectors corresponding to each input word
    vector <math alttext="ModifyingAbove w With right-arrow"><mover accent="true"><mi>w</mi>
    <mo>→</mo></mover></math> :'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，对输入序列的每个向量（句子中的每个单词）应用三种不同的转换，将它们分别乘以三个不同的权重矩阵。然后，我们获得与每个输入单词向量<math alttext="ModifyingAbove
    w With right-arrow"><mover accent="true"><mi>w</mi> <mo>→</mo></mover></math>对应的三组不同的向量：
- en: The *query* vector <math alttext="ModifyingAbove q u e r y With right-arrow
    equals upper W Subscript q Baseline ModifyingAbove w With right-arrow"><mrow><mover
    accent="true"><mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow>
    <mo>→</mo></mover> <mo>=</mo> <msub><mi>W</mi> <mi>q</mi></msub> <mover accent="true"><mi>w</mi>
    <mo>→</mo></mover></mrow></math> , whose purpose is to be the vector *attended
    from*.
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*查询*向量<math alttext="ModifyingAbove q u e r y With right-arrow equals upper
    W Subscript q Baseline ModifyingAbove w With right-arrow"><mrow><mover accent="true"><mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow>
    <mo>→</mo></mover> <mo>=</mo> <msub><mi>W</mi> <mi>q</mi></msub> <mover accent="true"><mi>w</mi>
    <mo>→</mo></mover></mrow></math>，其目的是成为*被关注的*向量。'
- en: The *key* vector <math alttext="ModifyingAbove k e y With right-arrow equals
    upper W Subscript k Baseline ModifyingAbove w With right-arrow"><mrow><mover accent="true"><mrow><mi>k</mi><mi>e</mi><mi>y</mi></mrow>
    <mo>→</mo></mover> <mo>=</mo> <msub><mi>W</mi> <mi>k</mi></msub> <mover accent="true"><mi>w</mi>
    <mo>→</mo></mover></mrow></math> , whose purpose is to be the vector *attended
    to*.
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*关键*向量<math alttext="ModifyingAbove k e y With right-arrow equals upper W Subscript
    k Baseline ModifyingAbove w With right-arrow"><mrow><mover accent="true"><mrow><mi>k</mi><mi>e</mi><mi>y</mi></mrow>
    <mo>→</mo></mover> <mo>=</mo> <msub><mi>W</mi> <mi>k</mi></msub> <mover accent="true"><mi>w</mi>
    <mo>→</mo></mover></mrow></math>，其目的是成为*被关注的*向量。'
- en: The *value* vector <math alttext="ModifyingAbove v a l u e With right-arrow
    equals upper W Subscript v Baseline ModifyingAbove w With right-arrow"><mrow><mover
    accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mo>=</mo> <msub><mi>W</mi> <mi>v</mi></msub> <mover accent="true"><mi>w</mi>
    <mo>→</mo></mover></mrow></math> , whose purpose is to capture the context that
    is being generated.
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*值*向量<math alttext="ModifyingAbove v a l u e With right-arrow equals upper
    W Subscript v Baseline ModifyingAbove w With right-arrow"><mrow><mover accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mo>=</mo> <msub><mi>W</mi> <mi>v</mi></msub> <mover accent="true"><mi>w</mi>
    <mo>→</mo></mover></mrow></math>，其目的是捕捉正在生成的上下文。'
- en: 'Second, obtain alignment scores between the the query and key vectors for all
    words in the sentence, by computing their dot product scaled by the inverse of
    the square root of the length of these vectors <math alttext="StartRoot l EndRoot"><msqrt><mi>l</mi></msqrt></math>
    . We apply this scaling for numerical stability, in order to keep the dot products
    from becoming large (these dot products will soon be passed into a softmax function.
    Since the softmax function has a very small gradient when its input has a large
    magnitude, we offset this effect by dividing each dot product by <math alttext="s
    q r t l"><mrow><mi>s</mi> <mi>q</mi> <mi>r</mi> <mi>t</mi> <mi>l</mi></mrow></math>
    .). Moreover, alignment of two vectors is independent from the lengths of these
    vectors. Therefore, the alignment score between *cooking* and *kitchen* in our
    sentence will be:'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，通过计算它们的点积乘以这些向量长度的平方根的倒数来获得句子中所有单词的查询向量和关键向量之间的对齐分数。我们应用这种缩放以保持数值稳定性，以防止点积变得很大（这些点积很快将被传递到softmax函数中。由于softmax函数在其输入具有很大幅度时具有非常小的梯度，我们通过将每个点积除以s
    q r t l来抵消这种效应。）。此外，两个向量的对齐与这些向量的长度无关。因此，在我们的句子中，*cooking*和*kitchen*之间的对齐分数将是：
- en: <math alttext="dollar-sign a l i g n m e n t Subscript c o o k i n g comma k
    i t c h e n Baseline equals StartFraction 1 Over StartRoot l EndRoot EndFraction
    ModifyingAbove q u e r y With right-arrow Subscript c o o k i n g Superscript
    t Baseline ModifyingAbove k e y With right-arrow Subscript k i t c h e n dollar-sign"><mrow><mi>a</mi>
    <mi>l</mi> <mi>i</mi> <mi>g</mi> <mi>n</mi> <mi>m</mi> <mi>e</mi> <mi>n</mi> <msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>=</mo> <mfrac><mn>1</mn> <msqrt><mi>l</mi></msqrt></mfrac> <msubsup><mover
    accent="true"><mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow>
    <mi>t</mi></msubsup> <msub><mover accent="true"><mrow><mi>k</mi><mi>e</mi><mi>y</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub></mrow></math>
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign a l i g n m e n t Subscript c o o k i n g comma k
    i t c h e n Baseline equals StartFraction 1 Over StartRoot l EndRoot EndFraction
    ModifyingAbove q u e r y With right-arrow Subscript c o o k i n g Superscript
    t Baseline ModifyingAbove k e y With right-arrow Subscript k i t c h e n dollar-sign"><mrow><mi>a</mi>
    <mi>l</mi> <mi>i</mi> <mi>g</mi> <mi>n</mi> <mi>m</mi> <mi>e</mi> <mi>n</mi> <msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>=</mo> <mfrac><mn>1</mn> <msqrt><mi>l</mi></msqrt></mfrac> <msubsup><mover
    accent="true"><mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow>
    <mi>t</mi></msubsup> <msub><mover accent="true"><mrow><mi>k</mi><mi>e</mi><mi>y</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub></mrow></math>
- en: Note that this will be different than the alignment score between *kitchen*
    and *cooking*, since the query and key vectors for each are different. Thus, the
    resulting alignment matrix is not symmetric.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这与*kitchen*和*cooking*之间的对齐分数不同，因为每个的查询和关键向量都不同。因此，得到的对齐矩阵不是对称的。
- en: 'Third, transform each alignment score between each two words in the sentence
    into a probability, by passing the score into the *softmax* function. For example:'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三，将句子中每两个单词之间的对齐分数转换为概率，通过将分数传递到*softmax*函数中。例如：
- en: <math alttext="dollar-sign omega Subscript c o o k i n g comma k i t c h e n
    Baseline equals s o f t m a x left-parenthesis a l i g n m e n t Subscript c o
    o k i n g comma k i t c h e n Baseline right-parenthesis equals StartFraction
    e x p left-parenthesis a l i g n m e n t Subscript c o o k i n g comma k i t c
    h e n Baseline right-parenthesis Over e x p left-parenthesis a l i g n m e n t
    Subscript c o o k i n g comma upper I Baseline right-parenthesis plus e x p left-parenthesis
    a l i g n m e n t Subscript c o o k i n g comma l o v e Baseline right-parenthesis
    plus e x p left-parenthesis a l i g n m e n t Subscript c o o k i n g comma c
    o o k i n g Baseline right-parenthesis plus e x p left-parenthesis a l i g n m
    e n t Subscript c o o k i n g comma i n Baseline right-parenthesis plus e x p
    left-parenthesis a l i g n m e n t Subscript c o o k i n g comma m y Baseline
    right-parenthesis plus e x p left-parenthesis a l i g n m e n t Subscript c o
    o k i n g comma k i t c h e n Baseline right-parenthesis EndFraction dollar-sign"><mrow><msub><mi>ω</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>=</mo> <mi>s</mi> <mi>o</mi> <mi>f</mi> <mi>t</mi> <mi>m</mi> <mi>a</mi> <mi>x</mi>
    <mrow><mo>(</mo> <mi>a</mi> <mi>l</mi> <mi>i</mi> <mi>g</mi> <mi>n</mi> <mi>m</mi>
    <mi>e</mi> <mi>n</mi> <msub><mi>t</mi> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>)</mo></mrow> <mrow><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>I</mi></mrow></msub>
    <mo>)</mo></mrow><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>l</mi><mi>o</mi><mi>v</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow></msub>
    <mo>)</mo></mrow><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>i</mi><mi>n</mi></mrow></msub>
    <mo>)</mo></mrow><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>m</mi><mi>y</mi></mrow></msub>
    <mo>)</mo></mrow><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign omega Subscript c o o k i n g comma k i t c h e n
    Baseline equals s o f t m a x left-parenthesis a l i g n m e n t Subscript c o
    o k i n g comma k i t c h e n Baseline right-parenthesis equals StartFraction
    e x p left-parenthesis a l i g n m e n t Subscript c o o k i n g comma k i t c
    h e n Baseline right-parenthesis Over e x p left-parenthesis a l i g n m e n t
    Subscript c o o k i n g comma upper I Baseline right-parenthesis plus e x p left-parenthesis
    a l i g n m e n t Subscript c o o k i n g comma l o v e Baseline right-parenthesis
    plus e x p left-parenthesis a l i g n m e n t Subscript c o o k i n g comma c
    o o k i n g Baseline right-parenthesis plus e x p left-parenthesis a l i g n m
    e n t Subscript c o o k i n g comma i n Baseline right-parenthesis plus e x p
    left-parenthesis a l i g n m e n t Subscript c o o k i n g comma m y Baseline
    right-parenthesis plus e x p left-parenthesis a l i g n m e n t Subscript c o
    o k i n g comma k i t c h e n Baseline right-parenthesis EndFraction dollar-sign"><mrow><msub><mi>ω</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>=</mo> <mi>s</mi> <mi>o</mi> <mi>f</mi> <mi>t</mi> <mi>m</mi> <mi>a</mi> <mi>x</mi>
    <mrow><mo>(</mo> <mi>a</mi> <mi>l</mi> <mi>i</mi> <mi>g</mi> <mi>n</mi> <mi>m</mi>
    <mi>e</mi> <mi>n</mi> <msub><mi>t</mi> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>)</mo></mrow> <mrow><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>I</mi></mrow></msub>
    <mo>)</mo></mrow><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>l</mi><mi>o</mi><mi>v</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow></msub>
    <mo>)</mo></mrow><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>i</mi><mi>n</mi></mrow></msub>
    <mo>)</mo></mrow><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>m</mi><mi>y</mi></mrow></msub>
    <mo>)</mo></mrow><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></mfrac></mrow></math>
- en: 'Finally, encode the context of each word, by linearly combining the value vectors
    using the alignment probabilities as weights for the linear combination. For example:'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，通过线性组合值向量，使用对齐概率作为线性组合的权重来编码每个单词的上下文。例如：
- en: <math alttext="dollar-sign c o n t e x t Subscript c o o k i n g Baseline equals
    omega Subscript c o o k i n g comma upper I Baseline ModifyingAbove v a l u e
    With right-arrow Subscript upper I Baseline plus omega Subscript c o o k i n g
    comma l o v e Baseline ModifyingAbove v a l u e With right-arrow Subscript l o
    v e Baseline plus omega Subscript c o o k i n g comma c o o k i n g Baseline ModifyingAbove
    v a l u e With right-arrow Subscript c o o k i n g Baseline plus omega Subscript
    c o o k i n g comma i n Baseline ModifyingAbove v a l u e With right-arrow Subscript
    i n Baseline plus omega Subscript c o o k i n g comma m y Baseline ModifyingAbove
    v a l u e With right-arrow Subscript m y Baseline plus omega Subscript c o o k
    i n g comma k i t c h e n Baseline ModifyingAbove v a l u e With right-arrow Subscript
    k i t c h e n dollar-sign"><mrow><mi>c</mi> <mi>o</mi> <mi>n</mi> <mi>t</mi> <mi>e</mi>
    <mi>x</mi> <msub><mi>t</mi> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow></msub>
    <mo>=</mo> <msub><mi>ω</mi> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>I</mi></mrow></msub>
    <msub><mover accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mi>I</mi></msub> <mo>+</mo> <msub><mi>ω</mi> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>l</mi><mi>o</mi><mi>v</mi><mi>e</mi></mrow></msub>
    <msub><mover accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>l</mi><mi>o</mi><mi>v</mi><mi>e</mi></mrow></msub>
    <mo>+</mo> <msub><mi>ω</mi> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow></msub>
    <msub><mover accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow></msub>
    <mo>+</mo> <msub><mi>ω</mi> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>i</mi><mi>n</mi></mrow></msub>
    <msub><mover accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>i</mi><mi>n</mi></mrow></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>m</mi><mi>y</mi></mrow></msub>
    <msub><mover accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>m</mi><mi>y</mi></mrow></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <msub><mover accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub></mrow></math>
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign c o n t e x t Subscript c o o k i n g Baseline equals
    omega Subscript c o o k i n g comma upper I Baseline ModifyingAbove v a l u e
    With right-arrow Subscript upper I Baseline plus omega Subscript c o o k i n g
    comma l o v e Baseline ModifyingAbove v a l u e With right-arrow Subscript l o
    v e Baseline plus omega Subscript c o o k i n g comma c o o k i n g Baseline ModifyingAbove
    v a l u e With right-arrow Subscript c o o k i n g Baseline plus omega Subscript
    c o o k i n g comma i n Baseline ModifyingAbove v a l u e With right-arrow Subscript
    i n Baseline plus omega Subscript c o o k i n g comma m y Baseline ModifyingAbove
    v a l u e With right-arrow Subscript m y Baseline plus omega Subscript c o o k
    i n g comma k i t c h e n Baseline ModifyingAbove v a l u e With right-arrow Subscript
    k i t c h e n dollar-sign"><mrow><mi>c</mi> <mi>o</mi> <mi>n</mi> <mi>t</mi> <mi>e</mi>
    <mi>x</mi> <msub><mi>t</mi> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow></msub>
    <mo>=</mo> <msub><mi>ω</mi> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>I</mi></mrow></msub>
    <msub><mover accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mi>I</mi></msub> <mo>+</mo> <msub><mi>ω</mi> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>l</mi><mi>o</mi><mi>v</mi><mi>e</mi></mrow></msub>
    <msub><mover accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>l</mi><mi>o</mi><mi>v</mi><mi>e</mi></mrow></msub>
    <mo>+</mo> <msub><mi>ω</mi> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow></msub>
    <msub><mover accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow></msub>
    <mo>+</mo> <msub><mi>ω</mi> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>i</mi><mi>n</mi></mrow></msub>
    <msub><mover accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>i</mi><mi>n</mi></mrow></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>m</mi><mi>y</mi></mrow></msub>
    <msub><mover accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>m</mi><mi>y</mi></mrow></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <msub><mover accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub></mrow></math>
- en: We have thus managed to capture in one vector the context of each word in the
    given sentence, with high worth assigned to those words in the sentence it mostly
    aligns with.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经成功地在一个向量中捕捉了给定句子中每个单词的上下文，对于与句子中大部分对齐的单词赋予了高价值。
- en: The good news here is that we can compute the context vector for all the words
    of a sentence (data sample) simultaneously, since we can pack the vectors above
    in matrices and use efficient and parallelizable matrix computations to get the
    contexts for all the terms at once.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，我们可以同时计算句子（数据样本）中所有单词的上下文向量，因为我们可以将上述向量打包在矩阵中，并使用高效且可并行化的矩阵计算一次性获取所有术语的上下文。
- en: We implement all of the above in one *attention head*. That is, one attention
    head produces one context vector for each token in the data sample. We would benefit
    from producing multiple context vectors for the same token, since some information
    gets lost with all the averaging happening on the way to a context vector. The
    idea here is to be able to extract information using different representations
    of the terms of a sentence (data sample), as opposed to a single representation
    corresponding to a single attention head. So we implement a *multi-head attention*,
    choosing for each head new transformation matrices <math alttext="upper W Subscript
    q"><msub><mi>W</mi> <mi>q</mi></msub></math> , <math alttext="upper W Subscript
    k"><msub><mi>W</mi> <mi>k</mi></msub></math> and <math alttext="upper W Subscript
    v"><msub><mi>W</mi> <mi>v</mi></msub></math> .
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一个*注意力头*中实现了上述所有内容。也就是说，一个注意力头为数据样本中的每个标记产生一个上下文向量。我们会从为同一标记产生多个上下文向量中受益，因为在生成上下文向量的过程中发生的所有平均化会导致一些信息丢失。这里的想法是能够使用句子（数据样本）术语的不同表示来提取信息，而不是对应于单个注意力头的单个表示。因此，我们实现了*多头注意力*，为每个头选择新的变换矩阵<math
    alttext="上标W下标q"><msub><mi>W</mi> <mi>q</mi></msub></math>，<math alttext="上标W下标k"><msub><mi>W</mi>
    <mi>k</mi></msub></math>和<math alttext="上标W下标v"><msub><mi>W</mi> <mi>v</mi></msub></math>。
- en: Note that during the training process, the entries of the transformation matrices
    are model parameters that have to be learned from the training data samples. Imagine
    then how fast the number of the model’s parameters balloons.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在训练过程中，变换矩阵的条目是必须从训练数据样本中学习的模型参数。想象一下，模型参数的数量会迅速增加多快。
- en: '[Figure 7-7](#Fig_multihead_attention) illustrates the *multi-head attention
    mechanism*, implementing *h* heads that receive different linearly transformed
    versions of the queries, keys and values each, to produce *h* context vectors
    for each token, that are then concacenated to produce the output of the *multi-head
    attention* part of the model’s structure.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-7](#Fig_multihead_attention)展示了*多头注意力机制*，实现了*h*个头部，每个头部接收查询、键和值的不同线性变换版本，为每个标记产生*h*个上下文向量，然后将它们连接起来以产生模型结构中*多头注意力*部分的输出。'
- en: '![280](assets/emai_0707.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![280](assets/emai_0707.png)'
- en: Figure 7-7\. Multi-head attention mechanism ([image source](https://arxiv.org/pdf/1706.03762.pdf)).
  id: totrans-261
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-7。多头注意力机制（[图片来源](https://arxiv.org/pdf/1706.03762.pdf)）。
- en: The decoder uses a similar self attention mechanism, but here each word can
    only attend to the words before it, since text is generated from left to right.
    Moreover, the decoder has an extra attention mechanism attending to the outputs
    it receives from the encoder.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器使用类似的自注意机制，但在这里，每个单词只能关注其之前的单词，因为文本是从左到右生成的。此外，解码器还有一个额外的注意机制，关注来自编码器的输出。
- en: Transformers Are Far From Perfect
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变压器远非完美
- en: Even though transformer models have revolutionalized the natural language processing
    field, they are far from perfect. Language models, in general, are *mindless mimics.
    They undertand neither their inputs nor their outputs*. Critical articles, such
    as [this](https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/)
    and [this](https://www.technologyreview.com/2021/02/24/1017797/gpt3-best-worst-ai-openai-natural-language/)
    by the *MIT Technology Review*, among others, detail their shortcomings, such
    as lack of comprehension of language, repitition when used to generate long passages
    of text, and other. That said, the transformer model brought about a tidal wave
    for natural language, and it is making its way to other AI fields such as biomedicine,
    computer vision and image generation.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管变压器模型已经彻底改变了自然语言处理领域，但它们远非完美。总的来说，语言模型是*无意识的模仿者。它们既不理解它们的输入，也不理解它们的输出*。[这篇](https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/)和[这篇](https://www.technologyreview.com/2021/02/24/1017797/gpt3-best-worst-ai-openai-natural-language/)等批评文章详细说明了它们的缺点，比如对语言的理解不足，在生成长篇文本时重复等。尽管如此，变压器模型为自然语言带来了一场巨浪，并且正在向生物医学、计算机视觉和图像生成等其他人工智能领域迈进。
- en: Convolutional Neural Networks For Time Series Data
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络用于时间序列数据
- en: The term *time series* in the natural language processing and finance domains
    should be instead *time sequence*. *Series* in mathematics refers to adding up
    the terms of an infinite *sequence*. So when our data is *not summed*, which is
    the case for all of natural language data and most of finance data, we actually
    have *sequences*, not *series*, of numbers, vectors, *etc.*. Oh well, vocabulary
    collisions are unavoidable even across different fields that heavily rely on one
    another.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理和金融领域中，“时间序列”一词应该改为“时间序列”。在数学中，“序列”指的是无限*序列*的项相加。因此，当我们的数据*没有被求和*时，这适用于所有自然语言数据和大多数金融数据，实际上我们有数字、向量等的*序列*，而不是*系列*。哦，词汇冲突是不可避免的，即使在彼此之间严重依赖的不同领域中也是如此。
- en: Other than the definition of a word in a dictionary, their meanings are mostly
    correlated to the way they occur relative to each other. This is conveyed through
    the way words are *ordered* in sentences, as well words’ context and their proximity
    to other words in sentences.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 除了字典中对单词的定义外，它们的含义大多与它们相对于彼此出现的方式相关。这通过单词在句子中的*顺序*、单词的上下文以及它们在句子中与其他单词的接近程度传达。
- en: 'We first emphasize the two ways which we can explore the meanings behind words
    and terms in documents:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先强调我们可以探索文档中单词和术语背后含义的两种方式：
- en: 'Spatially: Exploring a sentence all at once as one vector of tokens, whiever
    way these tokens are represented mathematically.'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 空间上：将一个句子作为一个标记向量一次性地探索，无论这些标记在数学上如何表示。
- en: 'Temporarily: Exploring a sentence sequentially, one token at a time.'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 临时：逐个标记地探索一个句子。
- en: Convolutional neural networks, discussed in [Chapter 5](ch05.xhtml#ch05), explore
    sentences spatially, via sliding a fixed-width window (kernel or filter) along
    the tokens of the sentence. When using convolutional neural networks to analyze
    text data, the network expects an input of fixed dimensions. On the other hand,
    when using recurrent neural networks (discussed next) to analyze text data, the
    network expects tokens sequentially, hence, the input does not need to be of fixed
    length.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络，讨论在[第5章](ch05.xhtml#ch05)中，通过沿着句子的标记滑动一个固定宽度的窗口（卷积核或滤波器）来空间地探索句子。当使用卷积神经网络来分析文本数据时，网络期望输入具有固定的维度。另一方面，当使用循环神经网络（接下来讨论）来分析文本数据时，网络期望标记按顺序，因此，输入不需要具有固定长度。
- en: In [Chapter 5](ch05.xhtml#ch05), we were sliding two dimensional windows (kernels
    or filters) over images, and this chapter, we will slide one dimensional kernels
    over text tokens. We now know that each token is itself represented as a vector
    of numbers. We can either use one hot encoding or word vectors of the Word2Vec
    model. One hot encoded tokens are represented with a very long vector that has
    a 0 for every possible vocabulary word that we want to include from the corpus,
    and a 1 in the position of the token we are encoding. Alternatively, we can use
    trained word vectors produced via Word2Vec. Thus, an input data sample to the
    convolutional neural network is a matrix, made up of column vectors, one column
    for each token in the data sample. If we use Word2Vec to represent tokens, then
    each column vector would have 100 to 500 entries, depending the particular Word2Vec
    model used. Recall that for a convolutional neural network, each data sample has
    to have the exact same number of tokens.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.xhtml#ch05)中，我们在图像上滑动二维窗口（卷积核或滤波器），而在本章中，我们将在文本标记上滑动一维卷积核。我们现在知道每个标记本身都表示为数字向量。我们可以使用独热编码或Word2Vec模型的单词向量。独热编码的标记用一个非常长的向量表示，其中对于我们想要从语料库中包含的每个可能的词汇词，都有一个0，并且在我们正在编码的标记位置有一个1。或者，我们可以使用通过Word2Vec生成的训练过的单词向量。因此，卷积神经网络的输入数据样本是一个矩阵，由列向量组成，每个列向量对应数据样本中的一个标记。如果我们使用Word2Vec表示标记，那么每个列向量将有100到500个条目，取决于使用的特定Word2Vec模型。请记住，对于卷积神经网络，每个数据样本必须具有完全相同数量的标记。
- en: 'Therefore, one data sample (a sentence or a paragraph) is represented with
    a two dimensional matrix, where the number of rows is is the full length of the
    word vector. In this context, saying that we are sliding a *one-dimensional* kernel
    over our data sample is slightly misleading, but here is the explanation: The
    vector representation of the sample’s tokens extends *downwards*, however, the
    filter covers the whole length of that dimension all at once. That is, if the
    filter is three tokens wides, it would be a matrix of weights with three columns
    and as many rows as the vector representation of our tokens. Thus, one dimensional
    convolution here refers to convolving only *horizontally*. This is different than
    two-dimensional convolution for images, where the two dimensional filter travels
    across the image both horizontally and vertically.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个数据样本（一个句子或一个段落）用一个二维矩阵表示，其中行数是单词向量的完整长度。在这种情况下，说我们正在在数据样本上滑动一个*一维*卷积核有点误导，但这里是解释：样本标记的向量表示向*下*延伸，然而，滤波器一次覆盖该维度的整个长度。也就是说，如果滤波器宽度为三个标记，它将是一个具有三列权重的矩阵，行数与我们标记的向量表示一样多。因此，这里的一维卷积是指仅在*水平*方向上进行卷积。这与图像的二维卷积不同，图像的二维滤波器在图像上水平和垂直方向上移动。
- en: As in the previous chapter, during a forward pass, the weight values in the
    filters are the same for one data sample. This means that we can parallelize the
    process, which is why convolutional neural networks are efficient to train.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一章类似，在前向传播期间，滤波器中的权重值对于一个数据样本是相同的。这意味着我们可以并行化这个过程，这就是为什么卷积神经网络训练高效的原因。
- en: Recall that convolutional neural networks can also process more than one channel
    of input at the same time, that is, three dimensional tensors of input, not only
    two dimensional matrices of numbers. For images this was processing the red, green,
    and blue channels of an input image all at once. For natural language, one input
    sample is a bunch of words represented as columnn vectors lined up next to each
    other. We now know that there are multiple ways to represent the same word as
    a vector of numbers, each perhaps capturing different semantics of the same word.
    These different vector representations of the same word are not necessarily of
    the same length. If we restrict them to be of the same length, then each of these
    representations can be a word’s *channel*, and the convolutional neural network
    can process all the channels of the same data sample at once.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，卷积神经网络也可以同时处理多个通道的输入，也就是说，三维张量的输入，而不仅仅是数字的二维矩阵。对于图像来说，这是同时处理输入图像的红色、绿色和蓝色通道。对于自然语言，一个输入样本是一堆单词，表示为相邻排列的列向量。我们现在知道有多种方式将同一个单词表示为数字向量，每种方式可能捕捉到同一个单词的不同语义。这些同一个单词的不同向量表示不一定具有相同的长度。如果我们限制它们具有相同的长度，那么这些表示中的每一个可以是一个单词的*通道*，卷积神经网络可以同时处理同一个数据样本的所有通道。
- en: As in [Chapter 5](ch05.xhtml#ch05), convolutional neural networks are efficient
    due to weight sharing, pooling layers, dropout, and the small filters sizes. We
    can run the model with multiple size filters then concatenate the output of each
    size filter into a longer thought vector before passing it into the fully connected
    at the last layer. Of course, the last layer of the network is the one that accomplishes
    the desired task, such as sentiment classification, spam filetering, text generation,
    and others. We went over this in the past two chapters.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 就像[第5章](ch05.xhtml#ch05)中所述，卷积神经网络由于权重共享、池化层、dropout和小的过滤器尺寸而高效。我们可以使用多个尺寸的过滤器运行模型，然后将每个尺寸过滤器的输出连接成一个更长的思考向量，然后将其传递到最后一层的全连接层。当然，网络的最后一层是完成所需任务的层，例如情感分类、垃圾邮件过滤、文本生成等。我们在过去的两章中已经讨论过这一点。
- en: Recurrent Neural Networks For Time Series Data
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归神经网络用于时间序列数据
- en: 'Consider the following three sentences:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下三个句子：
- en: She bought tickets to watch the movie.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 她买了电影票去看电影。
- en: She, having free time, bought tickets to watch the movie.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 她有空的时候，买了电影票去看电影。
- en: She, having heard about it nonstop for two weeks in a row, finally bought tickets
    to watch the movie.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 她连续两周听说了这件事，最终买了电影票去看电影。
- en: 'In all three sentences, the predicate *bought tickets to watch the movie* corresponds
    to the sentence’s subject *She*. A natural language model will be able to learn
    this if it is designed to handle long term dependencies:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有三个句子中，谓词“买了电影票去看电影”对应于句子的主语“她”。如果自然语言模型被设计为处理长期依赖关系，它将能够学习到这一点：
- en: A convolutional neural network, with its narrow filtering window ranging 3-5
    tokens scanning the sentence, will be able to learn from the first sentence easily,
    and maybe the second sentence, given that the predicate’s position changed only
    a little bit (pooling layers help with the network’s resistence to small variations),
    but the third sentence will be tough, unless we use larger filters (which increases
    the computation cost and make the network more like a fully connected network
    than a convolutional network), or if we deepen the network, stacking convolutional
    layers on top of each other so that the coverage widens as the sentence makes
    its way deeper into the network.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络，通过其窄的过滤窗口范围为3-5个标记扫描句子，将能够轻松地从第一个句子中学习，并且也许可以从第二个句子中学习，因为谓词的位置只有稍微改变（池化层有助于网络对小变化的抵抗力），但第三个句子将会很困难，除非我们使用更大的过滤器（这会增加计算成本，并使网络更像全连接网络而不是卷积网络），或者我们加深网络，将卷积层堆叠在一起，以便随着句子深入网络，覆盖范围扩大。
- en: A completely different approach is to feed the sentence into the network sequentially,
    one token at a time, and maintain a *state* and a *memory* that hold onto important
    information for a certain amount of time. The network produces an outcome when
    all the tokens in the sentence have passed through it. If this is during training,
    only the outcome produced after the last token has been processed gets compared
    to the sentence’s label, then the error *backpropagates through time*, to adjust
    the weights. Compare this to the way we hold onto information when reading a long
    sentence or paragraph. Recurrent neural networks with long-short-term-memory units
    are designed to this way.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个完全不同的方法是将句子逐个标记地顺序输入到网络中，并保持一个“状态”和一个“记忆”，保存重要信息一段时间。当句子中的所有标记都通过网络时，网络会产生一个结果。如果这是在训练期间，只有在处理完最后一个标记后产生的结果与句子的标签进行比较，然后错误会“通过时间反向传播”来调整权重。将这与我们阅读长句子或段落时保留信息的方式进行比较。带有长短期记忆单元的递归神经网络是按照这种方式设计的。
- en: Transformer models, which we discussed earlier, abolish both convolution and
    recurrence, relying only on attention to capture the relationship between the
    subject of the sentence *she*, and the predicate *bought tickets to watch the
    movie*.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们之前讨论过的Transformer模型废除了卷积和循环，仅依赖于注意力来捕捉句子的主语“她”和谓词“买了电影票去看电影”之间的关系。
- en: 'There is one more thing that differentiates recurrence models from convolutional
    and transformer models: Does the model expect its input to be of the same length
    for all data samples? Meaning can we only input sentences of the same length?
    The answer for transformers and convolutional nets is that they expect only fixed
    length data samples, so we have to preprocess our samples and make them all of
    the same length. On the other hand, recurrent neural networks have variable length
    inputs really well, since after all they only take them one token at a time.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一件事可以区分递归模型和卷积以及Transformer模型：模型是否期望其输入对于所有数据样本都是相同长度？意味着我们只能输入相同长度的句子吗？对于Transformer和卷积网络来说，它们只期望固定长度的数据样本，因此我们必须预处理我们的样本并使它们都具有相同的长度。另一方面，递归神经网络非常适应可变长度的输入，因为它们一次只处理一个标记。
- en: The main idea for a recurrent neural network is that it network holds onto past
    information as it processes new information. How does this holding happen? In
    a feedforward network, the output of a neuron leaves it and never gets back to
    it. In a recurrent network, the output loops back into the neuron, along with
    new input, in essence, creating a *memory*. Such algorithms are great for autocompletion
    and grammar check. They have been integrated into Gmail’s *Smart Compose* since
    2018.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络的主要思想是，网络在处理新信息时保留过去的信息。这种保留是如何发生的？在前馈网络中，神经元的输出离开它并永远不会返回。在递归网络中，输出回路返回到神经元中，同时带有新的输入，实质上创建了一个“记忆”。这样的算法非常适合自动完成和语法检查。自2018年以来，它们已经集成到Gmail的“智能撰写”中。
- en: How do recurrent neural networks work?
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 递归神经网络是如何工作的？
- en: 'Here are the steps for how a recurrent gets trained on a set of labeled data
    samples. Each data sample is made up of a bunch of tokens and a label. As always,
    the goal of the network is to learn the general features and patterns within the
    data that end up producing a certain label (or output) versus others. When tokens
    of each sample are input sequentially, our goal is then to detect features, across
    all the data samples, that emerge when certain tokens appear in patterns relative
    to each other:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是递归在一组带标签的数据样本上进行训练的步骤。每个数据样本由一堆标记和一个标签组成。与往常一样，网络的目标是学习数据中产生某个标签（或输出）与其他标签相比产生的一般特征和模式。当每个样本的标记按顺序输入时，我们的目标是检测特征，跨所有数据样本，当某些标记以相对于彼此的模式出现时产生的特征：
- en: Grab one tokenized and labeled data sample from your data set (such as a movie
    review labeled positive, or a tweet labeled fake news).
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据集中获取一个经过标记和标记的数据样本（例如标记为积极的电影评论，或标记为虚假新闻的推文）。
- en: Pass the first token <math alttext="t o k e n 0"><mrow><mi>t</mi> <mi>o</mi>
    <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>0</mn></msub></mrow></math> of your
    sample into the network. Remember that tokens are vectorized so you are really
    passing a vector of numbers into the network. In mathematical terms, we are evaluating
    a function at that token’s vector and producing another vector. So far, our network
    has calculated <math alttext="f left-parenthesis t o k e n 0 right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>0</mn></msub>
    <mo>)</mo></mrow></math> .
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将样本的第一个标记<math alttext="t o k e n 0"><mrow><mi>t</mi> <mi>o</mi> <mi>k</mi>
    <mi>e</mi> <msub><mi>n</mi> <mn>0</mn></msub></mrow></math>传递到网络中。请记住，标记是矢量化的，因此您实际上是将一组数字传递到网络中。在数学术语中，我们正在评估该标记的矢量处的函数，并产生另一个矢量。到目前为止，我们的网络已经计算了<math
    alttext="f left-parenthesis t o k e n 0 right-parenthesis"><mrow><mi>f</mi> <mo>(</mo>
    <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>0</mn></msub>
    <mo>)</mo></mrow></math>。
- en: Now pass the second token <math alttext="t o k e n 1"><mrow><mi>t</mi> <mi>o</mi>
    <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>1</mn></msub></mrow></math> of your
    sample into the network, *along with the output of the first token*, <math alttext="f
    left-parenthesis t o k e n 0 right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>t</mi>
    <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>0</mn></msub> <mo>)</mo></mrow></math>
    . The network now will evaluate <math alttext="f left-parenthesis t o k e n 1
    plus f left-parenthesis t o k e n 0 right-parenthesis right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>1</mn></msub>
    <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi>
    <msub><mi>n</mi> <mn>0</mn></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></math>
    . This is the recurrence step, and this is how the network does not forget <math
    alttext="t o k e n 0"><mrow><mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi>
    <mn>0</mn></msub></mrow></math> as it processes <math alttext="t o k e n 1"><mrow><mi>t</mi>
    <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>1</mn></msub></mrow></math>
    .
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在将样本的第二个标记<math alttext="t o k e n 1"><mrow><mi>t</mi> <mi>o</mi> <mi>k</mi>
    <mi>e</mi> <msub><mi>n</mi> <mn>1</mn></msub></mrow></math>传递到网络中，*以及第一个标记的输出*，<math
    alttext="f left-parenthesis t o k e n 0 right-parenthesis"><mrow><mi>f</mi> <mo>(</mo>
    <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>0</mn></msub>
    <mo>)</mo></mrow></math>。网络现在将评估<math alttext="f left-parenthesis t o k e n 1
    plus f left-parenthesis t o k e n 0 right-parenthesis right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>1</mn></msub>
    <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi>
    <msub><mi>n</mi> <mn>0</mn></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></math>。这是循环步骤，这就是网络在处理<math
    alttext="t o k e n 1"><mrow><mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi>
    <mn>1</mn></msub></mrow></math>时不会忘记<math alttext="t o k e n 0"><mrow><mi>t</mi>
    <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>0</mn></msub></mrow></math>。
- en: Now pass the third token <math alttext="t o k e n 2"><mrow><mi>t</mi> <mi>o</mi>
    <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>2</mn></msub></mrow></math> of your
    sample into the network, *along with the output of the previous step*, <math alttext="f
    left-parenthesis t o k e n 1 plus f left-parenthesis t o k e n 0 right-parenthesis
    right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi>
    <mi>e</mi> <msub><mi>n</mi> <mn>1</mn></msub> <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo>
    <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>0</mn></msub>
    <mo>)</mo></mrow> <mo>)</mo></mrow></math> . The network now will evaluate <math
    alttext="f left-parenthesis t o k e n 2 plus f left-parenthesis t o k e n 1 plus
    f left-parenthesis t o k e n 0 right-parenthesis right-parenthesis right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>2</mn></msub>
    <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi>
    <msub><mi>n</mi> <mn>1</mn></msub> <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi>
    <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>0</mn></msub> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>)</mo></mrow></math> .
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在将样本的第三个标记<math alttext="t o k e n 2"><mrow><mi>t</mi> <mi>o</mi> <mi>k</mi>
    <mi>e</mi> <msub><mi>n</mi> <mn>2</mn></msub></mrow></math>传递到网络中，*以及上一步的输出*，<math
    alttext="f left-parenthesis t o k e n 1 plus f left-parenthesis t o k e n 0 right-parenthesis
    right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi>
    <mi>e</mi> <msub><mi>n</mi> <mn>1</mn></msub> <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo>
    <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>0</mn></msub>
    <mo>)</mo></mrow> <mo>)</mo></mrow></math>。网络现在将评估<math alttext="f left-parenthesis
    t o k e n 2 plus f left-parenthesis t o k e n 1 plus f left-parenthesis t o k
    e n 0 right-parenthesis right-parenthesis right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>2</mn></msub>
    <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi>
    <msub><mi>n</mi> <mn>1</mn></msub> <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi>
    <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>0</mn></msub> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>)</mo></mrow></math>。
- en: Keep going until you finish all the tokens of your one sample. Suppose this
    sample only had five tokens, then our recurrent network will output <math alttext="f
    left-parenthesis t o k e n 4 plus f left-parenthesis t o k e n 3 plus f left-parenthesis
    t o k e n 2 plus f left-parenthesis t o k e n 1 plus f left-parenthesis t o k
    e n 0 right-parenthesis right-parenthesis right-parenthesis right-parenthesis
    right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi>
    <mi>e</mi> <msub><mi>n</mi> <mn>4</mn></msub> <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo>
    <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>3</mn></msub>
    <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi>
    <msub><mi>n</mi> <mn>2</mn></msub> <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi>
    <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>1</mn></msub> <mo>+</mo>
    <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi>
    <mn>0</mn></msub> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>)</mo></mrow></math> . Note that this output looks very similar to the output
    of the feed forward fully connected networks that we discussed in [Chapter 4](ch04.xhtml#ch04),
    except that this output *unfolds through time* as we input a sample’s tokens one
    at a time *into one recurrent neuron*, while in [Chapter 4](ch04.xhtml#ch04),
    the network’s output *unfolds through space*, as one data sample moves from one
    layer of the neural network to the next. Mathematically, when we write the formulas
    of each, they are the same, so we don’t need more math beyond what we learned
    in the past three chapters. That’s why we love math.
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续进行，直到完成一个样本的所有标记。假设这个样本只有五个标记，那么我们的递归网络将输出<math alttext="f left-parenthesis
    t o k e n 4 plus f left-parenthesis t o k e n 3 plus f left-parenthesis t o k
    e n 2 plus f left-parenthesis t o k e n 1 plus f left-parenthesis t o k e n 0
    right-parenthesis right-parenthesis right-parenthesis right-parenthesis right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>4</mn></msub>
    <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi>
    <msub><mi>n</mi> <mn>3</mn></msub> <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi>
    <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>2</mn></msub> <mo>+</mo>
    <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi>
    <mn>1</mn></msub> <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi>
    <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>0</mn></msub> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow></math>。请注意，这个输出看起来与我们在[第4章](ch04.xhtml#ch04)讨论的前馈全连接网络的输出非常相似，只是这个输出*随着时间展开*，当我们逐个将样本的标记*输入到一个递归神经元*时，而在[第4章](ch04.xhtml#ch04)中，网络的输出*随着空间展开*，当一个数据样本从神经网络的一层移动到下一层。从数学上讲，当我们写出每个公式时，它们是相同的，所以我们不需要更多的数学知识，超出我们在过去三章中学到的。这就是为什么我们喜欢数学。
- en: When training the network to produce the right thing, it is the final output
    of the sample, <math alttext="f left-parenthesis t o k e n 4 plus f left-parenthesis
    t o k e n 3 plus f left-parenthesis t o k e n 2 plus f left-parenthesis t o k
    e n 1 plus f left-parenthesis t o k e n 0 right-parenthesis right-parenthesis
    right-parenthesis right-parenthesis right-parenthesis"><mrow><mi>f</mi> <mo>(</mo>
    <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>4</mn></msub>
    <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi>
    <msub><mi>n</mi> <mn>3</mn></msub> <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi>
    <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>2</mn></msub> <mo>+</mo>
    <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi>
    <mn>1</mn></msub> <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi>
    <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>0</mn></msub> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow></math> , that gets compared
    against the sample’s true label via evaluating a loss function, exaclty as we
    did in Chapters [3](ch03.xhtml#ch03), [4](ch04.xhtml#ch04) and [5](ch05.xhtml#ch05).
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练网络以产生正确结果时，样本的最终输出<math alttext="f left-parenthesis t o k e n 4 plus f left-parenthesis
    t o k e n 3 plus f left-parenthesis t o k e n 2 plus f left-parenthesis t o k
    e n 1 plus f left-parenthesis t o k e n 0 right-parenthesis right-parenthesis
    right-parenthesis right-parenthesis right-parenthesis"><mrow><mi>f</mi> <mo>(</mo>
    <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>4</mn></msub>
    <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi>
    <msub><mi>n</mi> <mn>3</mn></msub> <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi>
    <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>2</mn></msub> <mo>+</mo>
    <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi>
    <mn>1</mn></msub> <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi>
    <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>0</mn></msub> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow></math>，将与样本的真实标签进行比较，通过评估损失函数，就像我们在第[3](ch03.xhtml#ch03)、[4](ch04.xhtml#ch04)和[5](ch05.xhtml#ch05)章中所做的那样。
- en: Now pass the next data sample one token at a time into the network and do the
    same thing again.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在逐个将下一个数据样本的标记传递到网络中，并再次执行相同的操作。
- en: We update the network’s weights in exactly the same way we updated them in [Chapter 4](ch04.xhtml#ch04),
    by minimizing the loss function via a gradient descent based algorithm, where
    we calculate the required gradient (the derivatives with respect to all the network’s
    weights) via backpropagation. As we just said, this is exactly the same backpropagation
    mathematics we learned in [Chapter 4](ch04.xhtml#ch04), except now of course we
    get to say *we’re backpropagating through time*.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过梯度下降算法来最小化损失函数，从而更新网络的权重，计算所需的梯度（相对于网络所有权重的导数）通过反向传播。正如我们刚才所说的，在[第4章](ch04.xhtml#ch04)学到的反向传播数学，现在当然我们可以说*我们正在通过时间进行反向传播*。
- en: In finance, dynamics, and feedback control, the process above is called an [auto-regressive
    moving average (ARMA) model](https://en.wikipedia.org/wiki/Autoregressive_model).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融、动力学和反馈控制中，上述过程被称为[自回归移动平均（ARMA）模型](https://en.wikipedia.org/wiki/Autoregressive_model)。
- en: 'Training a recurrent neural net can be expensive, especially for data samples
    of any significant length, say 10 tokens or more, since the number of weights
    to learn is directly related to the number of tokens in data samples: The more
    tokens, the more *depth in time* the recurrent network has. Other than the computational
    cost, this depth comes with all the troubles encountered by regular feed forward
    networks with many layers: Vanishing or exploding gradients, especially with samples
    of data with hundreds of tokens, which will be the mathematical equivalent of
    a fully connected feed forward neural network with hundreds of layers! The same
    remedies for exploding and vanishing gradients for feed forward networks work
    here.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 训练递归神经网络可能会很昂贵，特别是对于任何长度显著的数据样本，比如10个标记或更多，因为要学习的权重数量与数据样本中的标记数量直接相关：标记越多，递归网络的“时间深度”就越大。除了计算成本之外，这种深度带来了与具有许多层的常规前馈网络相同的所有问题：梯度消失或爆炸，特别是对于具有数百个标记的数据样本，这将是数百层全连接前馈神经网络的数学等价物！对于前馈网络的梯度爆炸和消失问题，这里也适用相同的解决方法。
- en: Gated Recurrent Units And Long Short Term Memory Units
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 门控循环单元和长短期记忆单元
- en: 'Recurrent neurons in recurrent networks are not enough to capture long term
    dependencies in a sentence: A token’s effect gets diluted and stepped on by the
    new information as more tokens pass through the recurrent neuron. In fact, a token’s
    information is almost completely gone only after two tokens have passed. This
    problem can be addressed if we add memory units, called *long short term memory
    units* to the architecture of the network. These help learning dependencies stretching
    across a whole data sample.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 递归网络中的递归神经元不足以捕捉句子中的长期依赖关系：随着更多标记通过递归神经元，一个标记的影响会被新信息稀释和覆盖。事实上，只有在两个标记经过后，一个标记的信息几乎完全消失。如果我们在网络的架构中添加记忆单元，即*长短期记忆单元*，则可以解决这个问题。这有助于学习跨整个数据样本的依赖关系。
- en: Long short term memory units themselves contain neural networks, and they can
    be trained to find only the new information that needs to be retained for the
    upcoming input, and to forget, or reset to zero, information that is no longer
    relevant to learning. Therefore, long short term memory units learn which information
    to hold on to, while the rest of the network learns to predict the target label.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆单元本身包含神经网络，它们可以被训练来找到需要保留以供即将输入的新信息，并忘记或重置为零不再相关于学习的信息。因此，长短期记忆单元学会了保留哪些信息，而网络的其余部分则学会了预测目标标签。
- en: There is not any new mathematics beyond what we learned in [Chapter 4](ch04.xhtml#ch04)
    here, so we will not go into the weeds digging into the specific architecture
    of a long short term memory unit, or a *gated unit*. In summary, the input token
    for each time step passes through the forget and update gates (functions), gets
    multiplied by weights and masks, then gets stored in a memory cell. The network’s
    next output depends on a combination of the input token and the memory unit’s
    current state. Moreover, long short term memory units share the weights they learned
    across samples, so they do not have to relearn basic information about language
    as they go through each sample’s tokens. [I will add couple things here explaining
    the best way to think about these]
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，除了我们在[第4章](ch04.xhtml#ch04)中学到的数学知识外，没有任何新的数学内容，因此我们不会深入研究长短期记忆单元或门控单元的具体架构。总的来说，每个时间步的输入标记通过遗忘和更新门（函数）传递，与权重和掩码相乘，然后存储在记忆单元中。网络的下一个输出取决于输入标记和记忆单元的当前状态的组合。此外，长短期记忆单元共享它们跨样本学习的权重，因此它们不必在通过每个样本的标记时重新学习关于语言的基本信息。
- en: Humans are able to process language on a subconscious level, and long short
    term memory are a step into modeling that. They are able to detect patterns in
    language that allow us to address more complex tasks than mere classification,
    such as language generation. We can generate novel text from learned probability
    distributions. This is the next chapter’s topic.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 人类能够在潜意识水平上处理语言，而长短期记忆是对此进行建模的一步。它们能够检测语言中的模式，使我们能够处理比简单分类更复杂的任务，如语言生成。我们可以根据学习的概率分布生成新的文本。这是下一章的主题。
- en: An Example Of Natural Language Data
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言数据的示例
- en: 'When faced with narratives about different models it always makes things easier
    when we have specific examples in mind with real data, along with the models’
    hyper-paramters. We can find the [IMDB](https://www.imdb.com/chart/top/) movie
    review dataset at [the Stanford AI website](https://ai.stanford.edu/~amaas/data/sentiment/).
    Each data sample is labeled with a 0 (negative review) or a 1 (positive review).
    We can start with the raw text data if we want to practice preprocessing natural
    language text. Then we can tokenize it and vectorize it using one-hot encoding
    over a chosen vocabulary, the Google Word2vec model, or some other model. Do not
    forget to split the data into the training and test sets. Then choose the hyper-paramters,
    for example: length of word vectors around 300, number of tokens per data sample
    around 400, mini-batches of 32, number of epochs 2\. We can play around with these
    to get a feel for the models’ performance.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 当面对关于不同模型的叙述时，如果我们有具体的实例和真实数据以及模型的超参数，事情总是变得更容易。我们可以在[斯坦福人工智能网站](https://ai.stanford.edu/~amaas/data/sentiment/)上找到[IMDB](https://www.imdb.com/chart/top/)电影评论数据集。每个数据样本都标有0（负面评论）或1（正面评论）。如果我们想要练习处理自然语言文本，可以从原始文本数据开始。然后，我们可以使用一个选择的词汇表、Google
    Word2vec模型或其他模型对其进行标记化和向量化。不要忘记将数据分割成训练集和测试集。然后选择超参数，例如：单词向量长度约为300，每个数据样本的标记数约为400，小批量为32，迭代次数为2。我们可以尝试调整这些参数以了解模型的性能。
- en: Finance AI
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 金融人工智能
- en: AI models have wide use for the finance field. By now, we know the underlying
    structure of most of AI models (except for graphs, which have a different mathematical
    structure; we discuss graph networks in [Chapter 9](ch09.xhtml#ch09)). At this
    point, only mentioning an application area from finance is enough for us to have
    a very good idea about how to go about modeling it using AI. Moreover, many finance
    applications are naturally interwined with natural language processing applications,
    such as marketing decisions based on customer reviews, or a natural language processing
    system used to predict economic trends and trigger large financial transactions
    based only on the models’ outputs.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: AI模型在金融领域有广泛应用。到目前为止，我们已经了解了大多数AI模型的基本结构（除了图形，它们具有不同的数学结构；我们在[第9章](ch09.xhtml#ch09)中讨论图形网络）。此时，仅仅提及金融领域的一个应用领域就足以让我们对如何使用AI进行建模有一个很好的想法。此外，许多金融应用与自然语言处理应用自然交织在一起，例如基于客户评论的营销决策，或者使用自然语言处理系统仅基于模型的输出来预测经济趋势并触发大额金融交易。
- en: 'The following only two AI applications in finance, among many. Think of ways
    we can put what we have learned so far to good use modeling these problems:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是金融领域中的两个AI应用，其中之一。想想我们如何能够将迄今学到的知识应用于对这些问题进行建模：
- en: 'Stock Market: Time series prediction. A recurrent neural network can take a
    sequence of inputs and produce a sequence of outputs. This is useful for the time
    series prediction required for stock prices. We input the prices over the past
    *n* days, and the networks outputs the prices from the past *n-1* days *along
    with tomorrow’s price*.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 股票市场：时间序列预测。递归神经网络可以接受一系列输入并产生一系列输出。这对于股票价格所需的时间序列预测非常有用。我们输入过去*n*天的价格，网络输出过去*n-1*天的价格*以及明天的价格*。
- en: '[auto-regressive moving average (ARMA) model](https://en.wikipedia.org/wiki/Autoregressive_model)
    in finance, dynamics, and feedback control.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自回归移动平均（ARMA）模型](https://en.wikipedia.org/wiki/Autoregressive_model)在金融、动力学和反馈控制中。'
- en: The stock market appears multiple times in this book. Keep an eye for it when
    discussing stochastic processes in [Chapter 11](ch11.xhtml#ch11) on probability.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 股票市场在本书中多次出现。在讨论概率中的随机过程时，请留意[第11章](ch11.xhtml#ch11)中的内容。
- en: Summary And Looking Ahead
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结和展望
- en: There was almost no new math in this chapter, however, it was one of the hardest
    to write. The goal was to summarize the most important ideas in the whole natural
    language processing field. Moving from words to relatively low dimensional vectors
    of numbers that carry meaning was the main barrier to overcome. Once we learned
    multiple ways to do this, whether vectorizing one word at a time or the main topics
    in a long document or an entire corpus, feeding those vectors to different machine
    learning models with different architectures and purposes was just business as
    usual.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 本章几乎没有新的数学知识，但却是最难写的之一。目标是总结整个自然语言处理领域中最重要的思想。从单词转移到携带含义的相对低维数字向量是要克服的主要障碍。一旦我们学会了多种方法来做到这一点，无论是一次一个词的向量化，还是一个长文档或整个语料库中的主要主题的向量化，将这些向量馈送到具有不同架构和目的的不同机器学习模型中就变得司空见惯。
- en: Calculus
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 微积分
- en: The *log* scale for Term Frequencies and Inverse Document Frequencies
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 术语频率和逆文档频率的*对数*比例尺
- en: Statistics
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学
- en: Zipf law for word counts
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zipf定律适用于单词计数
- en: The Dirichlet probability distribution for assigning words to topics and topics
    to documents.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dirichlet概率分布用于将单词分配给主题和主题分配给文档。
- en: Linear Algebra
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数
- en: Vectorizing documents of natural language
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将自然语言文档向量化
- en: The dot product of two vectors and how it provides a measure of similarity or
    compatibility between the entities that the vectors represent.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个向量的点积以及它如何提供向量所代表的实体之间的相似性或兼容性度量。
- en: Cosine similarity
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦相似度
- en: Singular value decomposition i.e. latent semantic analysis
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奇异值分解，即潜在语义分析
- en: Probability
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 概率
- en: Conditional probabilities
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件概率
- en: Bilinear log model
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双线性对数模型
- en: Time Series Data
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据
- en: What it means
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 含义
- en: How it is fed into machine learning models (as one bulk, or one item at a time)
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将其输入到机器学习模型中（作为一个整体，或一次一个项目）
- en: AI Model Of The Day
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: AI模型每日一题
- en: The Transformer
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器
