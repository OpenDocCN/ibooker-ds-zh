- en: 13 Data windowing and creating baselines for deep learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13 数据窗口化和为深度学习创建基线
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Creating windows of data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建数据窗口
- en: Implementing baseline models for deep learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现深度学习的基线模型
- en: 'In the last chapter, I introduced deep learning for forecasting by covering
    the situations where deep learning is ideal and by outlining the three main types
    of deep learning models: single-step, multi-step, and multi-output. We then proceeded
    with data exploration and feature engineering to remove useless features and create
    new features that will help us forecast traffic volume. With that setup done,
    we are now ready to implement deep learning to forecast our target variable, which
    is the traffic volume.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我通过介绍深度学习在预测中的理想情况，并概述了三种主要的深度学习模型：单步、多步和多输出，来介绍深度学习用于预测。然后我们进行了数据探索和特征工程，以去除无用的特征并创建有助于我们预测交通量的新特征。完成这个设置后，我们现在准备实现深度学习来预测我们的目标变量，即交通量。
- en: In this chapter, we’ll build a reusable class that will create windows of data.
    This step is probably the most complicated and most useful topic in this part
    of the book on deep learning. Applying deep learning for forecasting relies on
    creating appropriate time windows and specifying the inputs and labels. Once that
    is done, you will see that implementing different models becomes incredibly easy,
    and this framework can be reused for different situations and datasets.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建一个可重用的类，该类将创建数据窗口。这一步可能是本书这一部分最复杂也最有用的主题。应用深度学习进行预测依赖于创建适当的时间窗口并指定输入和标签。一旦完成这些，你就会发现实现不同的模型变得极其简单，这个框架可以用于不同的情况和数据集。
- en: Once you know how to create windows of data, we’ll move on to implement baseline
    models, linear models, and deep neural networks. This will let us measure the
    performance of these models, and we can then move on to more complex architectures
    in the following chapters.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你知道如何创建数据窗口，我们将继续实现基线模型、线性模型和深度神经网络。这将使我们能够衡量这些模型的性能，然后我们可以继续在接下来的章节中转向更复杂的架构。
- en: 13.1 Creating windows of data
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.1 创建数据窗口
- en: We’ll start off by creating the `DataWindow` class, which will allow us to format
    the data appropriately to be fed to our deep learning models. We’ll also add a
    plotting method to this class so that we can visualize the predictions and the
    actual values.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先创建`DataWindow`类，这将使我们能够适当地格式化数据，以便将其提供给我们的深度学习模型。我们还将为此类添加一个绘图方法，这样我们就可以可视化预测值和实际值。
- en: Before diving into the code and building the `DataWindow` class, however, it
    is important to understand why we must perform data windowing for deep learning.
    Deep learning models have a particular way of fitting on data, which we’ll explore
    in the next section. Then we’ll move on and implement the `DataWindow` class.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在深入代码和构建`DataWindow`类之前，理解为什么我们必须为深度学习执行数据窗口化是很重要的。深度学习模型在拟合数据上有一种特别的方式，我们将在下一节中探讨。然后我们将继续前进，并实现`DataWindow`类。
- en: 13.1.1 Exploring how deep learning models are trained for time series forecasting
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.1 探索深度学习模型如何进行时间序列预测的训练
- en: In the first half of this book, we fit statistical models, such as SARIMAX,
    on training sets and made predictions. We were, in reality, fitting a set of predefined
    functions of a certain order (*p*,*d*,*q*)(*P*,*D*,*Q*)[m], and finding out which
    order resulted in the best fit.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前半部分，我们在训练集上拟合统计模型，如SARIMAX，并进行预测。实际上，我们是在拟合一组预定义的函数，这些函数具有特定的阶数（*p*，*d*，*q*）（*P*，*D*，*Q*）[m]，并找出哪个阶数导致了最佳拟合。
- en: For deep learning models, we do not have a set of functions to try. Instead,
    we let the neural network derive its own function such that when it takes the
    inputs, it generates the best predictions possible. To achieve that, we perform
    what is called *data windowing*. This is a process in which we define a sequence
    of data points on our time series and define which are inputs and which are labels.
    That way, the deep learning model can fit on the inputs, generate predictions,
    compare them to the labels, and repeat this process until it cannot improve the
    accuracy of its predictions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度学习模型，我们没有一组函数去尝试。相反，我们让神经网络推导出它自己的函数，这样当它接收输入时，就能生成最佳预测。为了实现这一点，我们执行所谓的*数据窗口化*。这是一个过程，我们在时间序列上定义一系列数据点，并定义哪些是输入，哪些是标签。这样，深度学习模型就可以在输入上拟合，生成预测，将它们与标签进行比较，并重复这个过程，直到它无法提高其预测的准确性。
- en: Let’s walk through an example of data windowing. Our data window will use 24
    hours of data to predict the next 24 hours. You probably wonder why are we using
    just 24 hours of data to generate predictions. After all, deep learning is data
    hungry and is used for large datasets. The key lies in the data window. A single
    window has 24 timesteps as input to generate an output of 24 timesteps. However,
    the entire training set is separated into multiple windows, meaning that we have
    many windows with inputs and labels, as shown in figure 13.1.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个数据窗口的例子来了解一下。我们的数据窗口将使用24小时的数据来预测接下来的24小时。您可能想知道为什么我们只使用24小时的数据来生成预测。毕竟，深度学习是数据密集型的，并且用于大型数据集。关键在于数据窗口。单个窗口有24个时间步作为输入来生成24个时间步的输出。然而，整个训练集被分割成多个窗口，这意味着我们有许多带有输入和标签的窗口，如图13.1所示。
- en: '![](../../OEBPS/Images/13-01.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/13-01.png)'
- en: Figure 13.1 Visualizing the data windows on the training set. The inputs are
    shown with square markers, and the labels are shown with crosses. Each data window
    consists of 24 timesteps with square markers followed by 24 labels with crosses.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1展示了训练集上的数据窗口。输入以方块标记显示，标签以交叉显示。每个数据窗口由24个带有方块标记的时间步组成，随后是24个带有交叉的标签。
- en: In figure 13.1 you can see the first 400 timesteps of our training set for traffic
    volume. Each data window consists of 24 input timesteps and 24 label timesteps
    (as shown in figure 13.2), giving us a total length of 48 timesteps. We can generate
    many data windows with the training set, so we are, in fact, leveraging this large
    quantity of data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在图13.1中，您可以看到我们的训练集交通量的前400个时间步。每个数据窗口由24个输入时间步和24个标签时间步组成（如图13.2所示），总长度为48个时间步。我们可以使用训练集生成许多数据窗口，因此我们实际上正在利用这些大量的数据。
- en: As you can see in figure 13.2, the data window’s total length is the sum of
    the lengths of each sequence. In this case, since we have 24 timesteps as input
    and 24 labels, the total length of the data window is 48 timesteps.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在图13.2中看到的，数据窗口的总长度是每个序列长度的总和。在这种情况下，由于我们有24个时间步作为输入和24个标签，数据窗口的总长度是48个时间步。
- en: '![](../../OEBPS/Images/13-02.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/13-02.png)'
- en: Figure 13.2 An example of a data window. Our data window has 24 timesteps as
    input and 24 timesteps as output. The model will then use 24 hours of input to
    generate 24 hours of predictions. The total length of the data window is the sum
    of the length of inputs and labels. In this case, we have a total length of 48
    timesteps.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2展示了数据窗口的一个例子。我们的数据窗口有24个时间步作为输入和24个时间步作为输出。模型将使用24小时的输入来生成24小时的预测。数据窗口的总长度是输入和标签长度的总和。在这种情况下，我们总共有48个时间步。
- en: You might think that we are wasting a lot of training data, since in figure
    13.2 timesteps 24 to 47 are labels. Are those never going to be used as inputs?
    Of course, they will be. The `DataWindow` class that we’ll implement in the next
    section generates data windows with inputs starting at *t* = 0. Then it will create
    another set of data windows, but this time starting at *t* = 1. Then it will start
    at *t* = 2. This goes on until it cannot have a sequence of 24 consecutive labels
    in the training set, as illustrated in figure 13.3.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会认为我们在浪费大量的训练数据，因为如图13.2所示，时间步24到47是标签。这些标签永远不会作为输入使用吗？当然，它们会。我们将在下一节中实现的`DataWindow`类会生成以*t*
    = 0开始的数据窗口。然后它将创建另一组数据窗口，但这次以*t* = 1开始。然后它将以*t* = 2开始。这个过程一直持续到训练集中不能有24个连续的标签序列，如图13.3所示。
- en: '![](../../OEBPS/Images/13-03.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/13-03.png)'
- en: Figure 13.3 Visualizing the different data windows that are generated by the
    `DataWindow` class. You can see that by repeatedly shifting the starting point
    by one timestep, we use as much of the training data as possible to fit our deep
    learning models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3展示了由`DataWindow`类生成的不同数据窗口。您可以看到，通过每次将起点向后移动一个时间步，我们尽可能多地使用训练数据来拟合我们的深度学习模型。
- en: To make computation more efficient, deep learning models are trained with *batches*.
    A batch is simply a collection of data windows that are fed to the model for training,
    as shown in figure 13.4.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高计算效率，深度学习模型使用**批次**进行训练。批次仅仅是输入到模型进行训练的数据窗口集合，如图13.4所示。
- en: '![](../../OEBPS/Images/13-04.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/13-04.png)'
- en: Figure 13.4 A batch is simply a collection of data windows that are used for
    training the deep learning model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4显示了一个批次仅仅是用于训练深度学习模型的数据窗口集合。
- en: Figure 13.4 shows an example of a batch with a batch size of 32\. That means
    that 32 data windows are grouped together and used to train the model. Of course,
    this is only one batch—the `DataWindow` class generates as many batches as possible
    with the given training set. In our case, we have a training set with 12,285 rows.
    If each batch has 32 data windows, that means that we will have 12285/32 = 384
    batches.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4展示了32个数据窗口大小的批次示例。这意味着32个数据窗口被分组在一起用于训练模型。当然，这只是一个批次——`DataWindow`类会根据给定的训练集尽可能多地生成批次。在我们的例子中，我们有一个包含12,285行的训练集。如果每个批次有32个数据窗口，那么我们将有12285/32
    = 384个批次。
- en: Training the model on all 384 batches once is called one *epoch*. One epoch
    often does not result in an accurate model, so the model will train for as many
    epochs as necessary until it cannot improve the accuracy of its predictions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一次在所有384个批次上训练模型称为一个*epoch*。一个epoch通常不会导致模型准确度提高，因此模型将根据需要训练尽可能多的epoch，直到无法提高其预测的准确度。
- en: The final important concept in data windowing for deep learning is *shuffling*.
    I mentioned in the very first chapter of this book that time series data cannot
    be shuffled. Time series data has an order, and that order must be kept, so why
    are we shuffling the data here?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中数据窗口的最后一个重要概念是*洗牌*。我在本书的第一章中提到，时间序列数据不能进行洗牌。时间序列数据有顺序，这个顺序必须保持，那么为什么我们在这里要洗牌呢？
- en: In this context, shuffling occurs at the batch level, not inside the data window—the
    order of the time series itself is maintained within each data window. Each data
    window is independent of all others. Therefore, in a batch, we can shuffle the
    data windows and still keep the order of our time series, as shown in figure 13.5\.
    Shuffling the data is not essential, but it is recommended as it tends to make
    more robust models.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文中，洗牌发生在批次级别，而不是数据窗口内部——时间序列本身的顺序在每个数据窗口内保持不变。每个数据窗口与其他所有数据窗口独立。因此，在批次中，我们可以洗牌数据窗口，同时仍然保持时间序列的顺序，如图13.5所示。洗牌数据不是必需的，但它是推荐的，因为它往往会产生更鲁棒的模型。
- en: '![](../../OEBPS/Images/13-05.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/13-05.png)'
- en: Figure 13.5 Shuffling the data windows in a batch. Each data window is independent
    of all others, so it is safe to shuffle the data windows within a batch. Note
    that the order of the time series is maintained within each data window.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5展示了批次中数据窗口的洗牌。每个数据窗口与其他所有数据窗口独立，因此在批次内洗牌数据窗口是安全的。注意，时间序列的顺序在每个数据窗口内保持不变。
- en: Now that you understand the inner working of data windowing and how it is used
    for training deep learning models, let’s implement the `DataWindow` class.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了数据窗口的内部工作原理以及它是如何用于训练深度学习模型的，让我们来实现`DataWindow`类。
- en: 13.1.2 Implementing the DataWindow class
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.2 实现DataWindow类
- en: 'We are now ready to implement the `DataWindow` class. This class has the advantage
    of being flexible, meaning that you can use it in a wide variety of scenarios
    to apply deep learning. The full code is available on GitHub: [https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14](https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好实现`DataWindow`类。这个类具有灵活性的优势，意味着你可以在各种场景中使用它来应用深度学习。完整的代码可以在GitHub上找到：[https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14](https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14)。
- en: The class is based on the width of the input, the width of the label, and the
    shift. The width of the input is simply the number of timesteps that are fed into
    the model to make predictions. For example, given that we have hourly data in
    our dataset, if we feed the model with 24 hours of data to make a prediction,
    the input width is 24\. If we feed only 12 hours of data, the input width is 12.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类基于输入宽度、标签宽度和位移。输入宽度简单地说就是输入到模型中以进行预测的时间步数。例如，如果我们数据集中的数据是按小时收集的，如果我们向模型提供24小时的数据进行预测，那么输入宽度是24。如果我们只提供12小时的数据，那么输入宽度是12。
- en: The label width is equivalent to the number of timesteps in the predictions.
    If we predict only one timestep, the label width is 1\. If we predict a full day
    of data (with hourly data), the label width is 24.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 标签宽度等同于预测中的时间步数。如果我们只预测一个时间步，标签宽度是1。如果我们预测一个完整的数据日（按小时数据），标签宽度是24。
- en: Finally, the shift is the number of timesteps separating the input and the predictions.
    If we predict the next timestep, the shift is 1\. If we predict the next 24 hours
    (with hourly data), the shift is 24.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，偏移量是输入和预测之间分离的时间步长数。如果我们预测下一个时间步长，偏移量是 1。如果我们预测接下来的 24 小时（使用每小时数据），偏移量是 24。
- en: Let’s visualize some windows of data to better understand these parameters.
    Figure 13.6 shows a window of data where the model predicts the next data point,
    given a single data point.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化一些数据窗口，以更好地理解这些参数。图 13.6 显示了一个数据窗口，其中模型根据单个数据点预测下一个数据点。
- en: '![](../../OEBPS/Images/13-06.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/13-06.png)'
- en: Figure 13.6 A data window where the model predicts one timestep in the future,
    given a single point of data. The input width is 1, since the model takes only
    1 data point as input. The label width is also only 1, since the model outputs
    the prediction for 1 timestep only. Since the model predicts the next timestep,
    the shift is also 1\. Finally, the total window size is the sum of the shift and
    the input widths, which equals 2.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.6 显示了一个数据窗口，其中模型根据单个数据点预测未来的一个时间步长。输入宽度为 1，因为模型只接受 1 个数据点作为输入。标签宽度也为 1，因为模型只输出
    1 个时间步长的预测。由于模型预测下一个时间步长，偏移量也是 1。最后，总窗口大小是偏移量和输入宽度的总和，等于 2。
- en: Now let’s consider the situation where we feed 24 hours of data to the model
    in order to predict the next 24 hours. The data window in that situation is shown
    in figure 13.7.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑这种情况，即我们向模型提供 24 小时的数据以预测接下来的 24 小时。这种情况下的数据窗口如图 13.7 所示。
- en: '![](../../OEBPS/Images/13-07.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/13-07.png)'
- en: Figure 13.7 Data window where the model predicts the next 24 hours using the
    last 24 hours of data. The input width is 24 and the label width is also 24\.
    Since there are 24 timesteps separating the inputs and the predictions, the shift
    is also 24\. This gives a total window size of 48 timesteps.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.7 显示了数据窗口，其中模型使用最后 24 小时的数据预测接下来的 24 小时。输入宽度为 24，标签宽度也为 24。由于输入和预测之间有 24
    个时间步长，因此偏移量也是 24。这给出了总窗口大小为 48 个时间步长。
- en: Now that you understand the concept of input width, label width, and shift,
    we can create the `DataWindow` class and define its initialization function in
    listing 13.1\. The function will also take in the training, validation, and test
    sets, as the windows of data will come from our dataset. Finally, we’ll allow
    the target column to be specified.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你理解了输入宽度、标签宽度和偏移量的概念后，我们可以创建 `DataWindow` 类并在列表 13.1 中定义其初始化函数。该函数还将接受训练、验证和测试集，因为数据窗口将来自我们的数据集。最后，我们将允许指定目标列。
- en: Listing 13.1 Defining the initialization function of `DataWindow`
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.1 定义 `DataWindow` 的初始化函数
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Name of the column that we wish to predict
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们希望预测的列名称
- en: ❷ Create a dictionary with the name and index of the label column. This will
    be used for plotting.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个包含标签列名称和索引的字典。这将用于绘图。
- en: ❸ Create a dictionary with the name and index of each column. This will be used
    to separate the features from the target variable.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建一个包含每列名称和索引的字典。这将用于将特征与目标变量分开。
- en: ❹ The slice function returns a slice object that specifies how to slice a sequence.
    In this case, it says that the input slice starts at 0 and ends when we reach
    the input_width.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 切片函数返回一个切片对象，指定如何切片一个序列。在这种情况下，它表示输入切片从 0 开始，到我们达到 input_width 时结束。
- en: ❺ Assign indices to the inputs. These are useful for plotting.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 为输入分配索引。这些对于绘图很有用。
- en: ❻ Get the index at which the label starts. In this case, it is the total window
    size minus the width of the label.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 获取标签开始的索引。在这种情况下，它是总窗口大小减去标签宽度。
- en: ❼ The same steps that were applied for the inputs are applied for labels.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 对输入应用相同的步骤也应用于标签。
- en: In listing 13.1 you can see that the initialization function basically assigns
    the variables and manages the indices of the inputs and the labels. Our next step
    is to split our window between inputs and labels, so that our models can make
    predictions based on the inputs and measure an error metric against the labels.
    The following `split_to_ inputs_labels` function is defined within the `DataWindow`
    class.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 13.1 中，你可以看到初始化函数基本上分配变量并管理输入和标签的索引。我们的下一步是将窗口在输入和标签之间分割，以便我们的模型可以根据输入进行预测并测量与标签的错误度量。以下
    `split_to_inputs_labels` 函数是在 `DataWindow` 类中定义的。
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Slice the window to get the inputs using the input_slice defined in __init__.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用在 __init__ 中定义的 input_slice 切片窗口以获取输入。
- en: ❷ Slice the window to get the labels using the labels_slice defined in __init__.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用在__init__中定义的labels_slice对窗口进行切片以获取标签。
- en: ❸ If we have more than one target, we stack the labels.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 如果我们有多个目标，我们将堆叠标签。
- en: ❹ The shape will be [batch, time, features]. At this point, we only specify
    the time dimension and allow the batch and feature dimensions to be defined later.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 形状为[批次，时间，特征]。在此阶段，我们只指定时间维度，允许批次和特征维度稍后定义。
- en: 'The `split_to_inputs_labels` function will separate the big data window into
    two windows: one for the inputs and the other for the labels, as shown in figure
    13.8.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`split_to_inputs_labels`函数将大数据窗口分为两个窗口：一个用于输入，另一个用于标签，如图13.8所示。'
- en: '![](../../OEBPS/Images/13-08.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/13-08.png)'
- en: Figure 13.8 The `split_to_inputs_labels` function simply separates the big data
    window into two windows, where one contains the inputs and the other the labels.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.8 `split_to_inputs_labels`函数简单地将大数据窗口分为两个窗口，其中一个包含输入，另一个包含标签。
- en: Next we’ll define a function to plot the input data, the predictions, and the
    actual values (listing 13.2). Since we will be working with many time windows,
    we’ll show only the plot of three time windows, but this parameter can easily
    be changed. Also, the default label will be traffic volume, but we can change
    that by specifying any column we choose. Again, this function should be included
    in the `DataWindow` class.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个函数来绘制输入数据、预测值和实际值（列表13.2）。由于我们将处理许多时间窗口，我们只显示三个时间窗口的图表，但这个参数可以很容易地更改。此外，默认标签将是交通量，但我们可以通过指定任何列来更改它。再次强调，这个函数应该包含在`DataWindow`类中。
- en: Listing 13.2 Method to plot a sample of data windows
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.2绘制数据窗口样本的方法
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Plot the inputs. They will appear as a continuous blue line with dots.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 绘制输入值。它们将以连续的蓝色线条和点表示。
- en: ❷ Plot the labels or actual values. They will appear as green squares.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 绘制标签或实际值。它们将以绿色方块的形式出现。
- en: ❸ Plot the predictions. They will appear as red crosses.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 绘制预测值。它们将以红色十字形出现。
- en: We are almost done building the `DataWindow` class. The last main piece of logic
    will format our dataset into tensors so that they can be fed to our deep learning
    models. TensorFlow comes with a very handy function called `timeseries_dataset_from_
    array`, which creates a dataset of sliding windows, given an array.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎完成了`DataWindow`类的构建。最后一项主要逻辑是将我们的数据集格式化为张量，以便它们可以被喂给我们的深度学习模型。TensorFlow附带一个非常有用的函数`timeseries_dataset_from_array`，它根据一个数组创建一个滑动窗口数据集。
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Pass in the data. This corresponds to our training set, validation set, or
    test set.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入数据。这对应于我们的训练集、验证集或测试集。
- en: ❷ Targets are set to None, as they are handled by the split_to_input_labels
    function.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 目标设置为None，因为它们由split_to_input_labels函数处理。
- en: ❸ Define the total length of the array, which is equal to the total window length.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义数组的总长度，它等于总窗口长度。
- en: ❹ Define the number of timesteps separating each sequence. In our case, we want
    the sequences to be consecutive, so sequence_stride=1.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义每个序列之间的时间步数。在我们的案例中，我们希望序列是连续的，因此sequence_stride=1。
- en: ❺ Shuffle the sequences. Keep in mind that the data is still in chronological
    order. We are simply shuffling the order of the sequences, which makes the model
    more robust.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 打乱序列。请注意，数据仍然是按时间顺序排列的。我们只是在序列的顺序上进行打乱，这使得模型更加鲁棒。
- en: ❻ Define the number of sequences in a single batch.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义单个批次中的序列数量。
- en: Remember that we are shuffling the sequences in a batch. This means that within
    each sequence, the data is in chronological order. However, in a batch of 32 sequences,
    we can and should shuffle them to make our model more robust and less prone to
    overfitting.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们在批次中打乱序列。这意味着在每个序列内部，数据是按时间顺序排列的。然而，在32个序列的批次中，我们可以也应该打乱它们，以使我们的模型更加鲁棒，并减少过拟合的风险。
- en: We’ll conclude our `DataWindow` class by defining some properties to apply the
    `make_dataset` function on the training, validation, and testing sets. We’ll also
    create a sample batch that we’ll cache within the class for plotting purposes.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过定义一些属性来结束`DataWindow`类的定义，这些属性将应用于训练、验证和测试集上的`make_dataset`函数。我们还将创建一个样本批次，并将其在类内部缓存，用于绘图目的。
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Get a sample batch of data for plotting purposes. If the sample batch does
    not exist, we’ll retrieve a sample batch and cache it.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取用于绘图目的的样本批次数据。如果样本批次不存在，我们将检索一个样本批次并将其缓存。
- en: Our `DataWindow` class is now complete. The full class with all methods and
    properties is shown in listing 13.3.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成了`DataWindow`类。完整的类，包括所有方法和属性，如列表13.3所示。
- en: Listing 13.3 The complete `DataWindow` class
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.3 完整的`DataWindow`类
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: For now, the `DataWindow` class might seem a bit abstract, but we will soon
    use it to apply baseline models. We will be using this class in all the chapters
    in this deep learning part of the book, so you will gradually tame this code and
    appreciate how easy it is to test different deep learning architectures.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，`DataWindow`类可能看起来有点抽象，但我们将很快使用它来应用基线模型。我们将在这本书的深度学习部分的每一章中使用这个类，所以您将逐渐驯服这段代码，并欣赏测试不同深度学习架构的简便性。
- en: 13.2 Applying baseline models
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.2 应用基线模型
- en: With the `DataWindow` class complete, we are ready to use it. We will apply
    baseline models as single-step, multi-step, and multi-output models. You will
    see that their implementation is similar and incredibly simple when we have the
    right data windows.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 随着`DataWindow`类的完成，我们准备好使用它了。我们将应用基线模型作为单步、多步和多输出模型。您将看到，当我们拥有正确的数据窗口时，它们的实现是相似的，并且极其简单。
- en: Recall that a baseline is used as a benchmark to evaluate more complex models.
    A model is performant if it compares favorably to another, so building a baseline
    is an important step in modeling.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，基线被用作基准来评估更复杂的模型。如果一个模型的表现优于另一个模型，那么它就是性能良好的，因此构建基线是建模的重要步骤。
- en: 13.2.1 Single-step baseline model
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.1 单步基线模型
- en: We’ll first implement a single-step model as a baseline. In a single-step model,
    the input is one timestep and the output is the prediction of the next timestep.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将实现一个单步模型作为基线。在单步模型中，输入是一个时间步长，输出是下一个时间步长的预测。
- en: The first step is to generate a window of data. Since we are defining a single-step
    model, the input width is 1, the label width is 1, and the shift is also 1, since
    the model predicts the next timestep. Our target variable is the volume of traffic.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是生成一个数据窗口。由于我们正在定义一个单步模型，输入宽度为1，标签宽度为1，偏移量也为1，因为模型预测下一个时间步长。我们的目标变量是交通量。
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: For plotting purposes, we’ll also define a wider window so we can visualize
    many predictions of our model. Otherwise, we could only visualize one input data
    point and one output prediction, which is not very interesting.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绘图目的，我们还将定义一个更宽的窗口，这样我们就可以可视化我们模型的多项预测。否则，我们只能可视化一个输入数据点和一项输出预测，这并不很有趣。
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this situation, the simplest prediction we can make is the last observed
    value. Basically, the prediction is simply the input data point. This is implemented
    by the class `Baseline`. As you can see in the following listing, the `Baseline`
    class can also be used for a multi-output model. For now, we’ll solely focus on
    a single-step model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以做出的最简单的预测是最后一个观察到的值。基本上，预测仅仅是输入数据点。这是通过`Baseline`类实现的。正如您在下面的列表中可以看到的，`Baseline`类也可以用于多输出模型。现在，我们将仅关注单步模型。
- en: Listing 13.4 Class to return the input data as a prediction
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.4 返回输入数据作为预测的类
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ If no target is specified, we return all columns. This is useful for multi-output
    models where all columns are to be predicted.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果没有指定目标，我们将返回所有列。这对于所有列都需要预测的多输出模型很有用。
- en: ❷ If we specify a list of targets, it will return only the specified columns.
    Again, this is used for multi-output models.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果我们指定了一个目标列表，它将只返回指定的列。同样，这也用于多输出模型。
- en: ❸ Return the input for a given target variable.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回给定目标变量的输入。
- en: With the class defined, we can now initialize the model and compile it to generate
    predictions. To do so, we’ll find the index of our target column, traffic_volume,
    and pass it in to `Baseline`. Note that TensorFlow requires us to provide a loss
    function and a metric of evaluation. In this case, and throughout the deep learning
    chapters, we’ll use the mean squared error (MSE) as a loss function—it penalizes
    large errors, and it generally yields well-fitted models. For the evaluation metric,
    we’ll use the mean absolute error (MAE) for its ease of interpretation.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了类之后，我们现在可以初始化模型并编译它以生成预测。为此，我们将找到我们的目标列`traffic_volume`的索引，并将其传递给`Baseline`。请注意，TensorFlow要求我们提供一个损失函数和评估指标。在这种情况下，以及在整个深度学习章节中，我们将使用均方误差（MSE）作为损失函数——它惩罚大误差，并且通常会产生拟合良好的模型。对于评估指标，我们将使用平均绝对误差（MAE），因为它易于解释。
- en: '[PRE9]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Generate a dictionary with the name and index of each column in the training
    set.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生成一个包含训练集中每个列名称和索引的字典。
- en: ❷ Pass the index of the target column in the Baseline class.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在基线类中传递目标列的索引。
- en: ❸ Compile the model to generate the predictions.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 编译模型以生成预测。
- en: We’ll now evaluate the performance of our baseline on both the validation and
    test sets. Models built with TensorFlow conveniently come with the `evaluate`
    method, which allows us to compare the predictions to the actual values and calculate
    the error metric.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将评估我们的基线在验证集和测试集上的性能。使用TensorFlow构建的模型方便地带有`evaluate`方法，这允许我们将预测与实际值进行比较，并计算误差指标。
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Create a dictionary to hold the MAE of a model on the validation set.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个字典来存储模型在验证集上的MAE。
- en: ❷ Create a dictionary to hold the MAE of a model on the test set.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个字典来存储模型在测试集上的MAE。
- en: ❸ Store the MAE of the baseline on the validation set.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在验证集上存储基线的MAE。
- en: ❹ Store the MAE of the baseline on the test set.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在测试集上存储基线的MAE。
- en: Great, we have successfully built a baseline that predicts the last known value
    and evaluated it. We can visualize the predictions using the `plot` method of
    the `DataWindow` class. Remember to use the `wide_window` to see more than just
    two data points.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，我们已经成功构建了一个预测最后一个已知值的基线，并对其进行了评估。我们可以使用`DataWindow`类的`plot`方法可视化预测结果。请记住使用`wide_window`来查看不仅仅是两个数据点。
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In figure 13.9 the labels are squares and the predictions are crosses. The crosses
    at each timestep are simply the last known value, meaning that we have a baseline
    that functions as expected. Your plot may differ from figure 13.9, as the cached
    sample batch changes every time a data window is initialized.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在图13.9中，标签是正方形，预测是交叉。每个时间步的交叉只是最后一个已知值，这意味着我们有一个按预期工作的基线。你的图表可能与图13.9不同，因为每次初始化数据窗口时缓存的样本批次都会改变。
- en: '![](../../OEBPS/Images/13-09.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图13.9](../../OEBPS/Images/13-09.png)'
- en: Figure 13.9 Predictions of our baseline single-step model on three sequences
    from the sample batch. The prediction at each timestep is the last known value,
    meaning that our baseline works as expected.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.9 我们基线单步模型在样本批次中的三个序列上的预测。每个时间步的预测是最后一个已知值，这意味着我们的基线按预期工作。
- en: We can optionally print the MAE of our baseline on the test set.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择打印基线在测试集上的MAE。
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This returns an MAE of 0.081\. More complex models should perform better than
    the baseline, resulting in a smaller MAE.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回了一个MAE为0.081。更复杂的模型应该比基线表现更好，从而产生更小的MAE。
- en: 13.2.2 Multi-step baseline models
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.2 多步基线模型
- en: In the previous section, we built a single-step baseline model that simply predicted
    the last known value. For multi-step models, we’ll predict more than one timestep
    into the future. In this case, we’ll forecast the traffic volume for the next
    24 hours of data given an input of 24 hours.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们构建了一个单步基线模型，它简单地预测了最后一个已知值。对于多步模型，我们将预测未来超过一个时间步。在这种情况下，我们将根据24小时的输入预测接下来24小时的数据流量。
- en: Again, the first step is to generate the appropriate window of data. Because
    we wish to predict 24 timesteps into the future with an input of 24 hours, the
    input width is 24, the label width is 24, and the shift is also 24.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，第一步是生成适当的数据窗口。因为我们希望使用24小时的输入预测未来24个时间步，所以输入宽度是24，标签宽度是24，偏移量也是24。
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'With the data window generated, we can now focus on implementing the baseline
    models. In this situation, there are two reasonable baselines:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成数据窗口后，我们现在可以专注于实现基线模型。在这种情况下，有两种合理的基线：
- en: Predict the last known value for the next 24 timesteps.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测接下来24个时间步的最后一个已知值。
- en: Predict the last 24 timesteps for the next 24 timesteps.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测接下来的24个时间步的最后一个24个时间步。
- en: With that in mind, let’s implement the first baseline, where we’ll simply repeat
    the last known value over the next 24 timesteps.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，让我们实现第一个基线，我们将简单地重复下一个24个时间步内的最后一个已知值。
- en: Predicting the last known value
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 预测最后一个已知值
- en: To predict the last known value, we’ll define a `MultiStepLastBaseline` class
    that simply takes in the input and repeats the last value of the input sequence
    over 24 timesteps. This acts as the prediction of the model.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预测最后一个已知值，我们将定义一个`MultiStepLastBaseline`类，它简单地接受输入并在24个时间步内重复输入序列的最后一个值。这作为模型的预测。
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ If no target is specified, return the last known value of all columns over
    the next 24 timesteps.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果未指定目标，则返回接下来24个时间步内所有列的最后一个已知值。
- en: ❷ Return the last known value of the target column over the next 24 timesteps.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 返回目标列接下来24个时间步的最后一个已知值。
- en: Next we’ll initialize the class and specify the target column. We’ll then repeat
    the same steps as in the previous section, compiling the model and evaluating
    it on the validation set and test set.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将初始化类并指定目标列。然后我们将重复上一节中的相同步骤，编译模型并在验证集和测试集上评估它。
- en: '[PRE15]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can now visualize the predictions using the `plot` method of `DataWindow`.
    The result is shown in figure 13.10.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 `DataWindow` 的 `plot` 方法可视化预测。结果如图 13.10 所示。
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../../OEBPS/Images/13-10.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/13-10.png)'
- en: Figure 13.10 Predicting the last known value for the next 24 timesteps. We can
    see that the predictions, shown as crosses, correspond to the last value of the
    input sequence, so our baseline behaves as expected.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.10 预测下一个 24 个时间步长的最后一个已知值。我们可以看到，预测值（以十字形表示）对应于输入序列的最后一个值，因此我们的基线表现符合预期。
- en: Again, we can optionally print the baseline’s MAE. From figure 13.10, we can
    expect it to be fairly high, since there is a large discrepancy between the labels
    and the predictions.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以选择性地打印基线的 MAE。从图 13.10 中，我们可以预期它相当高，因为标签和预测之间存在很大的差异。
- en: '[PRE17]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This gives an MAE of 0.347\. Now let’s see if we can build a better baseline
    by simply repeating the input sequence.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了 0.347 的 MAE。现在让我们看看是否可以通过简单地重复输入序列来构建一个更好的基线。
- en: Repeating the input sequence
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 重复输入序列
- en: Let’s implement a second baseline for multi-step models, which simply returns
    the input sequence. This means that the prediction for the next 24 hours will
    simply be the last known 24 hours of data. This is implemented through the `RepeatBaseline`
    class.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现第二个基线，用于多步模型，它简单地返回输入序列。这意味着下一个 24 小时的预测将仅仅是最后一个已知的 24 小时数据。这是通过 `RepeatBaseline`
    类实现的。
- en: '[PRE18]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Return the input sequence for the given target column.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 返回给定目标列的输入序列。
- en: Now we can initialize the baseline model and generate predictions. Note that
    the loss function and evaluation metric remain the same.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以初始化基线模型并生成预测。请注意，损失函数和评估指标保持不变。
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Next we can visualize the predictions. The result is shown in figure 13.11.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以可视化预测结果。结果如图 13.11 所示。
- en: '![](../../OEBPS/Images/13-11.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/13-11.png)'
- en: Figure 13.11 Repeating the input sequence as the predictions. You’ll see that
    the predictions (represented as crosses) match exactly the input sequence. You’ll
    also notice that many predictions overlap the labels, which indicates that this
    baseline performs quite well.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.11 将输入序列作为预测。您将看到预测（以十字形表示）与输入序列完全匹配。您还会注意到许多预测与标签重叠，这表明这个基线表现相当好。
- en: This baseline performs well. This is to be expected, since we identified daily
    seasonality in the previous chapter. This baseline is the equivalent to predicting
    the last known season.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基线表现良好。这是可以预料的，因为我们已经在上一章中识别出了日季节性。这个基线相当于预测最后一个已知的季节。
- en: Again, we can print the MAE on the test set to verify that we indeed have a
    better baseline than simply predicting the last known value.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以打印测试集上的 MAE 以验证我们确实有一个比简单地预测最后一个已知值更好的基线。
- en: '[PRE20]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This gives an MAE of 0.341, which is lower than the MAE obtained by predicting
    the last known value. We have therefore successfully built a better baseline.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了 0.341 的 MAE，低于预测最后一个已知值获得的 MAE。因此，我们已经成功构建了一个更好的基线。
- en: 13.2.3 Multi-output baseline model
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.3 多输出基线模型
- en: The final type of model we’ll cover is the multi-output model. In this situation,
    we wish to predict the traffic volume and the temperature for the next timestep
    using a single input data point. Essentially, we’re applying the single-step model
    on both the traffic volume and temperature, making it a multi-output model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要介绍的最后一类模型是多输出模型。在这种情况下，我们希望使用单个输入数据点预测下一个时间步长的交通量和温度。本质上，我们是在交通量和温度上应用单步模型，使其成为多输出模型。
- en: 'Again, we’ll start off by defining the window of data, but here we’ll define
    two windows: one for training and the other for visualization. Since the model
    takes in one data point and outputs one prediction, we want to initialize a wide
    window of data to visualize many predictions over many timesteps.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将从定义数据窗口开始，但在这里我们将定义两个窗口：一个用于训练，另一个用于可视化。由于模型接收一个数据点并输出一个预测，我们希望初始化一个宽的数据窗口来可视化多个时间步长的多个预测。
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Notice that we pass in both temp and traffic_volume, as those are our two
    targets for the multi-output model.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 注意，我们传递了 temp 和 traffic_volume，因为那是我们的多输出模型的两个目标。
- en: Then we’ll use the `Baseline` class that we defined for the single-step model.
    Recall that this class can output the last known value for a list of targets.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用为单步模型定义的 `Baseline` 类。回想一下，这个类可以输出一系列目标中的最后已知值。
- en: Listing 13.5 Class to return the input data as a prediction
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.5 返回输入数据作为预测的类
- en: '[PRE22]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ If no target is specified, we return all columns. This is useful for multi-output
    models where all columns are to be predicted.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果没有指定目标，我们返回所有列。这对于所有列都需要预测的多输出模型很有用。
- en: ❷ If we specify a list of targets, it will return only these specified columns.
    Again, this is used for multi-output models.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果我们指定了一个目标列表，它将只返回这些指定的列。同样，这用于多输出模型。
- en: ❸ Return the input for a given target variable.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回给定目标变量的输入。
- en: In the case of the multi-output model, we must simply pass the indexes of the
    temp and traffic_volume columns to output the last known value for the respective
    variables as a prediction.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在多输出模型的情况下，我们必须简单地传递 temp 和 traffic_volume 列的索引，以输出相应变量的最后已知值作为预测。
- en: '[PRE23]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Prints out 2
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输出 2
- en: ❷ Prints out 0
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 输出 0
- en: With the baseline initialized with our two target variables, we can now compile
    the model and evaluate it.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使用初始化了我们的两个目标变量的基线，我们现在可以编译模型并评估它。
- en: '[PRE24]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Finally, we can visualize the predictions against the actual values. By default,
    our `plot` method will show the traffic volume on the *y*-axis, allowing us to
    quickly display one of our targets, as shown in figure 13.12.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将预测值与实际值进行可视化。默认情况下，我们的 `plot` 方法将在 *y*-轴上显示交通量，使我们能够快速显示我们的一个目标，如图 13.12
    所示。
- en: '[PRE25]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](../../OEBPS/Images/13-12.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/13-12.png)'
- en: Figure 13.12 Predicting the last known value for traffic volume
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.12 预测交通量的最后已知值
- en: Figure 13.12 does not show anything surprising, as we already saw these results
    when we built a single-step baseline model. The particularity of the multi-output
    model is that we also have predictions for the temperature. Of course, we can
    also visualize the predictions for the temperature by specifying the target in
    the `plot` method. The result is shown in figure 13.13.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.12 没有显示任何令人惊讶的内容，因为我们已经在构建单步基线模型时看到了这些结果。多输出模型的特点是，我们还有温度的预测值。当然，我们也可以通过在
    `plot` 方法中指定目标来可视化温度的预测值。结果如图 13.13 所示。
- en: '[PRE26]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](../../OEBPS/Images/13-13.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/13-13.png)'
- en: Figure 13.13 Predicting the last known value for the temperature. The predictions
    (crosses) are equal to the previous data point, so our baseline model behaves
    as expected.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.13 预测温度的最后已知值。预测值（交叉点）等于前一个数据点，因此我们的基线模型表现如预期。
- en: Again, we can print the MAE of our baseline model.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以打印基线模型的 MAE。
- en: '[PRE27]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We obtain an MAE of 0.047 on the test set. In the next chapter, we’ll start
    building more complex models, and they should result in a lower MAE, as they will
    be trained to fit the data.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在测试集上获得了一个 MAE 为 0.047。在下一章中，我们将开始构建更复杂的模型，它们的 MAE 应该会更低，因为它们将被训练以拟合数据。
- en: 13.3 Next steps
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.3 下一步
- en: In this chapter, we covered the crucial step of creating data windows, which
    will allow us to quickly build any type of model. We then proceeded to build baseline
    models for each type of model, so that we have benchmarks we can compare to when
    we build our more complex models in later chapters.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了创建数据窗口的关键步骤，这将使我们能够快速构建任何类型的模型。然后我们继续为每种类型的模型构建基线模型，这样我们就有基准可以比较，当我们稍后在章节中构建更复杂的模型时。
- en: Of course, building baseline models is not an application of deep learning just
    yet. In the next chapter, we will implement linear models and deep neural networks,
    and see if those models are already more performant than the simple baselines.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，构建基线模型还不是深度学习应用。在下一章中，我们将实现线性模型和深度神经网络，看看这些模型是否已经比简单的基线模型更有效。
- en: 13.4 Exercises
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.4 练习
- en: In the previous chapter, as an exercise, we prepared the air pollution dataset
    for deep learning modeling. Now we’ll use the training set, validation set, and
    test set to build baseline models and evaluate them.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，作为练习，我们为深度学习建模准备了空气污染数据集。现在我们将使用训练集、验证集和测试集来构建基线模型并评估它们。
- en: 'For each type of model, follow the steps outlined. Recall that the target for
    the single-step and multi-step model is the concentration of NO[2], and the targets
    for the multi-output model are the concentration of NO[2] and temperature. The
    complete solution is available on GitHub: [https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14](https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每种类型的模型，遵循概述的步骤。请记住，单步和多步模型的目标是NO[2]的浓度，而多输出模型的目标是NO[2]的浓度和温度。完整的解决方案可在GitHub上找到：[https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14](https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14)。
- en: For the single-step model
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于单步模型
- en: Build a baseline model that predicts the last known value.
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立一个基线模型，预测最后已知值。
- en: Plot it.
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制其图形。
- en: Evaluate its performance using the mean absolute error (MAE) and store it for
    comparison in a dictionary.
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用平均绝对误差（MAE）评估其性能，并将其存储在字典中以供比较。
- en: For the multi-step model
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于多步模型
- en: Build a baseline that predicts the last known value over a horizon of 24 hours.
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立一个基线模型，在24小时的时间范围内预测最后已知值。
- en: Build a baseline model that repeats the last 24 hours.
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立一个基线模型，重复最后24小时的数据。
- en: Plot the predictions of both models.
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制两个模型的预测结果。
- en: Evaluate both models using the MAE and store their performance.
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MAE评估两个模型，并存储它们的性能。
- en: For the multi-output model
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于多输出模型
- en: Build a baseline model that predicts the last known value.
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立一个基线模型，预测最后已知值。
- en: Plot it.
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制其图形。
- en: Evaluate its performance using the MAE and store it for comparison in a dictionary.
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MAE评估其性能，并将其存储在字典中以供比较。
- en: Summary
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Data windowing is essential in deep learning to format the data as inputs and
    labels for the model.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据窗口化在深度学习中至关重要，可以将数据格式化为模型的输入和标签。
- en: The `DataWindow` class can easily be used in any situation and can be extended
    to your liking. Make use of it in your own projects.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataWindow`类可以轻松地用于任何情况，并且可以根据您的喜好进行扩展。在您自己的项目中使用它。'
- en: Deep learning models require a loss function and an evaluation metric. In our
    case, we chose the mean squared error (MSE) as the loss function, because it penalizes
    large errors and tends to yield better-fit models. The evaluation metric is the
    mean absolute error (MAE), chosen for its ease of interpretation.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型需要一个损失函数和评估指标。在我们的案例中，我们选择了均方误差（MSE）作为损失函数，因为它对大误差进行惩罚，并倾向于产生更好的拟合模型。评估指标是平均绝对误差（MAE），因为它易于解释。
