- en: 2 Fully connected networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 全连接网络
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节涵盖
- en: Implementing a training loop in PyTorch
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 PyTorch 中实现训练循环
- en: Changing loss functions for regression and classification problems
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改回归和分类问题的损失函数
- en: Implementing and training a fully connected network
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现和训练一个全连接网络
- en: Training faster using smaller batches of data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更小的数据批次加速训练
- en: 'Now that we understand how PyTorch gives us tensors to represent our data and
    parameters, we can progress to building our first neural networks. This starts
    with showing how *learning* happens in PyTorch. As we described in chapter 1,
    learning is based on the principle of optimization: we can compute a loss for
    how well we are doing and use gradients to minimize that loss. This is how the
    parameters of a network are “learned” from the data and is also the basis of many
    different machine learning (ML) algorithms. For these reasons, optimization of
    loss functions is the foundation PyTorch is built from. So to implement any kind
    of neural network in PyTorch, we must phrase the problem as an optimization problem
    (remember that this is also called *function minimization*).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 PyTorch 如何为我们提供张量来表示数据和参数，我们可以继续构建我们的第一个神经网络。这始于展示 PyTorch 中 *学习*
    的发生。正如我们在第 1 章中描述的，学习基于优化的原则：我们可以计算一个损失来衡量我们做得如何，并使用梯度来最小化这个损失。这就是网络参数从数据中“学习”的方式，也是许多不同的机器学习（ML）算法的基础。因此，损失函数的优化是
    PyTorch 构建的基础。因此，要在 PyTorch 中实现任何类型的神经网络，我们必须将问题表述为一个优化问题（记住这也可以称为 *函数最小化*）。
- en: In this chapter, we first learn how to set up this optimization approach to
    learning. It is a widely applicable concept, and the code we write will be usable
    for almost any neural network. This process is called a *training loop* and even
    works for simple bread-and-butter ML algorithms like linear and logistic regression.
    Since we are focusing on the mechanics of training, we will start with these two
    basic algorithms so we can stay focused on how training loops work in PyTorch
    for classification and regression.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先学习如何设置这种学习优化方法。这是一个广泛适用的概念，我们编写的代码几乎可以用于任何神经网络。这个过程被称为 *训练循环*，甚至适用于简单的线性回归和逻辑回归等机器学习算法。由于我们专注于训练的机制，我们将从这两个基本算法开始，以便我们能够专注于
    PyTorch 中分类和回归的训练循环的工作方式。
- en: The vector of weights that makes a logistic/linear regression is also called
    a *linear layer* or a *fully connected* layer. This means both can be considered
    a single layer model in PyTorch. Since neural networks can be described as a sequence
    of layers, we will modify our original logistic and linear models to become full-fledged
    neural networks. In doing so, you will learn the importance of a nonlinear layer
    and how logistic and linear regression are related to each other and to neural
    networks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使逻辑回归/线性回归成立的权重向量也被称为 *线性层* 或 *全连接层*。这意味着在 PyTorch 中，两者都可以被视为单一层模型。由于神经网络可以描述为一系列层的序列，我们将修改我们原始的逻辑回归和线性模型，使其成为完整的神经网络。在这个过程中，你将了解非线性层的重要性，以及逻辑回归和线性回归如何相互关联以及与神经网络的关系。
- en: After mastering these concepts of a training loop, classification and regression
    loss functions, and defining a fully connected neural network, we will have covered
    the foundational concepts of deep learning that will reoccur in almost every model
    you will ever train. To round out the chapter, we will refactor our code into
    a convenient helper function and learn the practical utility of training on small
    groups of data called *batches* instead of using the entire dataset.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握训练循环、分类和回归损失函数以及定义全连接神经网络的概念之后，我们将涵盖深度学习的基础概念，这些概念几乎会在你训练的每个模型中重复出现。为了完善本章，我们将重构我们的代码到一个方便的辅助函数中，并学习在小型数据组（称为
    *批次*）上训练而不是使用整个数据集的实用价值。
- en: 2.1 Neural networks as optimization
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 神经网络作为优化
- en: 'In chapter 1, we used PyTorch’s *automatic differentiation* capability for
    optimizing (read, minimizing) a function. We defined a loss function to minimize
    and used the `.backward()` function to compute gradients, which told us how to
    alter the parameters to minimize the function. If we *make the input to the loss
    function a neural network*, we can use this exact same approach to train a neural
    network. This creates a process called a *training loop* with three major components:
    the training data (with correct answers), the model and loss function, and the
    update via gradients. These three components are outlined in figure 2.1.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们使用了 PyTorch 的**自动微分**功能来优化（即最小化）一个函数。我们定义了一个损失函数来最小化，并使用 `.backward()`
    函数来计算梯度，这告诉我们如何改变参数以最小化函数。如果我们**将损失函数的输入设为神经网络**，我们可以使用完全相同的方法来训练神经网络。这创建了一个称为**训练循环**的过程，有三个主要组成部分：训练数据（带有正确答案）、模型和损失函数，以及通过梯度进行更新。这三个组成部分在图2.1中概述。
- en: '![](../Images/CH02_F01_Raff.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F01_Raff.png)'
- en: Figure 2.1 The three major steps to training a model in PyTorch. 1\. The input
    data drives learning.. The model is used to make a prediction, and a loss scores
    how much the prediction and true labeldiffer. 3\. PyTorch’s automatic differentiation
    is used to update the parameters of the model, improvingits predictions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 PyTorch中训练模型的主要三个步骤。1. 输入数据驱动学习。模型用于做出预测，损失分数衡量预测和真实标签之间的差异。3. 使用PyTorch的自动微分来更新模型的参数，提高其预测能力。
- en: 2.1.1  Notation of training a neural network
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1  神经网络的训练符号表示
- en: Before we start, let’s introduce some standard notation we reuse throughout
    this book. We use x to denote input features and *f*() to denote a neural network
    model. The label associated with x is denoted with y. Our model takes in x and
    produces a prediction ŷ. Written out, this becomes *ŷ* = *f*(**x**). This notation
    is widely used in deep learning papers, and getting familiar with it will help
    you stay up to date as new approaches are developed.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，让我们介绍一些在本书中重复使用的标准符号。我们用 x 表示输入特征，用 *f*() 表示神经网络模型。与 x 相关的标签用 y 表示。我们的模型接受
    x 并产生一个预测 ŷ。写成这样，就是 *ŷ* = *f*(**x**)。这个符号在深度学习论文中广泛使用，熟悉它将帮助你跟上新方法的开发。
- en: Our model needs parameters to adjust. Changing the parameters allows the network
    to alter its predictions to try to reduce the loss function. We will denote in
    abstract *all* the parameters of our model using Θ. If we want to be explicit,
    we might say *ŷ* = *f*[Θ](**x**) to state that the model’s prediction and behavior
    depend on the value of its parameters Θ. You will also see Θ called the *state*
    of the model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模式需要参数进行调整。改变参数允许网络改变其预测，以尝试减少损失函数。我们将使用 Θ 表示我们模型的**所有**参数。如果我们想明确一点，我们可能会说
    *ŷ* = *f*[Θ](**x**) 来表明模型的预测和行为取决于其参数 Θ 的值。你也会看到 Θ 被称为模型的**状态**。
- en: We have a notation and language for describing our model, but we also need a
    way to frame the goal as function minimization. To do this, we use the concept
    of a *loss function*. The loss function *quantifies* how badly our model is doing
    at the goal of predicting the ground truth y. If y is our goal and ŷ is the prediction,
    we will denote our loss function as ℓ(*ŷ*,*y*).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有描述我们模型的符号和语言，但我们还需要一种方法来将目标表述为函数最小化。为此，我们使用**损失函数**的概念。损失函数**量化**我们的模型在预测真实值
    y 的目标上做得有多糟糕。如果 y 是我们的目标，ŷ 是预测，我们将我们的损失函数表示为 ℓ(*ŷ*,*y*).
- en: 'Now we have all we need to abstractly describe learning as a function minimization
    problem. Say we have a training set with N examples, which is trained by optimizing
    the equation min*Θ* Σ[i]^N[=1] *ℓ*(*f[Θ]*(***x**[i]*), *y[i]*). Let’s write out
    what this equation says in English; we will color-code each English description
    to the math that says the same thing:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了抽象地描述学习为函数最小化问题的所有必要条件。假设我们有一个包含 N 个示例的训练集，通过优化方程 min*Θ* Σ[i]^N[=1]
    *ℓ*(*f[Θ]*(***x**[i]*), *y[i]*). 让我们用英语写出这个方程的含义；我们将为每个英语描述着色，以匹配相同的数学表达：
- en: '![](../Images/CH02_UN01_Raff.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_UN01_Raff.png)'
- en: By looking at each piece of the math, we can see how this is just describing
    our goal and that the math allows us to convey a long sentence in less space.
    As code, it might instead look like this:[¹](#fn1)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看数学的每一部分，我们可以看到这只是在描述我们的目标，而且数学允许我们在更小的空间中传达一个长句子。作为代码，它可能看起来像这样：[¹](#fn1)
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The summation (Σ[i]^N[=1]) goes over all N pairs of input (**x**[i]) and output
    (*y*[i]) and determines how badly (ℓ(⋅,⋅)) we are doing. This gets us the equation
    that computes the loss but doesn’t minimize it. The big question is: how do we
    adjust Θ to do this minimization?'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 求和 (Σ[i]^N[=1]) 遍历所有 N 对输入 (**x**[i]) 和输出 (*y*[i])，并确定我们做得有多糟糕（ℓ(⋅,⋅)）。这为我们提供了计算损失的方程，但并没有最小化它。最大的问题是：我们如何调整
    Θ 来进行这种最小化？
- en: 'We do this by gradient descent, which is why PyTorch provides automatic differentiation.
    Suppose *Θ*[k] is the *current* state of our model, which we want to improve.
    How do we find the next state *Θ*[*k* + 1], which hopefully reduces our model’s
    loss? The equation we want to solve is *Θ[k]*[+1] = *Θ[k]* − *η* · 1/*N* Σ[i]^N[=1]
    ∇*Θ[k]ℓ*(*f[Θ][k]*(***x**[i]*), *y[i]*). Again, let’s write out what this equation
    says in English and map it back to the symbols:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过梯度下降来实现这一点，这也是为什么 PyTorch 提供自动微分的原因。假设 *Θ*[k] 是我们想要改进的模型的 *当前* 状态。我们如何找到下一个状态
    *Θ*[*k* + 1]，希望它能减少我们模型的损失？我们想要解决的方程是 *Θ[k]*[+1] = *Θ[k]* − *η* · 1/*N* Σ[i]^N[=1]
    ∇*Θ[k]ℓ*(*f[Θ][k]*(***x**[i]*), *y[i]*)。再次，让我们用英语写出这个方程的含义，并将其映射回符号：
- en: '![](../Images/CH02_UN02_Raff.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_UN02_Raff.png)'
- en: This equation shows the math for what is known as *gradient descent*. It looks
    almost exactly the same as what we did in chapter 1 to optimize a simple function.
    This biggest difference is the fancy new ∇ symbol. This *nabla* symbol ∇ is used
    to denote the *gradient*. In the last chapter, we used the terms *derivative*
    and *gradient* interchangeably because we had only *one* parameter. Since we have
    a set of parameters now, the gradient is the language we use to refer to the derivative
    with respect to every parameter. If we want to alter only a select subset z of
    the parameters, we write that as ∇[z]. This means ∇ will be a tensor with one
    value for *every* parameter.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程展示了被称为 *梯度下降* 的数学原理。它看起来几乎与我们在第一章中优化简单函数时所做的完全一样。最大的区别是那个花哨的新 ∇ 符号。这个 *nabla*
    符号 ∇ 用于表示 *梯度*。在上一个章节中，我们使用 *导数* 和 *梯度* 互换，因为我们只有一个参数。由于我们现在有一组参数，梯度是我们用来指代每个参数的导数的语言。如果我们只想改变参数的选定子集
    z，我们将其写为 ∇[z]。这意味着 ∇ 将是一个具有每个参数一个值的张量。
- en: The gradient (∇) tells us how to adjust Θ, and just as before, we head in the
    opposite direction of the sign. The important thing to remember is that PyTorch
    provides automatic differentiation. That means if we use the PyTorch API and framework,
    we do not have to worry about how to compute ∇[Θ]. We don’t have to keep track
    of everything in Θ, either.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度 (∇) 告诉我们如何调整 Θ，就像之前一样，我们朝着相反的方向前进。重要的是要记住，PyTorch 提供了自动微分。这意味着如果我们使用 PyTorch
    API 和框架，我们不必担心如何计算 ∇[Θ]。我们也不必跟踪 Θ 中的所有内容。
- en: All we *need* to do is define what our model *f*(⋅) looks like and what our
    loss function ℓ(⋅,⋅) is. That will take care of almost all the work for us. We
    can write a function that performs this whole process.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的只是定义我们的模型 *f*(⋅) 的样子以及我们的损失函数 ℓ(⋅,⋅) 是什么。这将为我们处理几乎所有的工作。我们可以编写一个执行整个过程的函数。
- en: 2.1.2  Building a linear regression model
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2  构建线性回归模型
- en: The framework we have described to train a model *f*(⋅) using gradient descent
    is widely applicable. The process demonstrated by figure 2.1 of iterating over
    the data and performing these gradient updates is the training loop. Using PyTorch
    and this approach, we can re-create many types of ML methods, like linear and
    logistic regression. To do so, we simply need to define *f*(⋅) in the correct
    manner. We will start by re-creating one of the bread-and-butter algorithms, linear
    regression, to introduce the code infrastructure that PyTorch provides for us
    to build into a larger neural network later.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述的框架是使用梯度下降训练模型 *f*(⋅) 的，它具有广泛的应用性。图 2.1 中展示的迭代数据和执行这些梯度更新的过程是训练循环。使用 PyTorch
    和这种方法，我们可以重新创建许多类型的机器学习方法，如线性回归和逻辑回归。要做到这一点，我们只需以正确的方式定义 *f*(⋅)。我们将从重新创建线性回归算法开始，这是面包和黄油算法，以介绍
    PyTorch 为我们提供的代码基础设施，以便我们可以在稍后构建更大的神经网络。
- en: 'The first thing to do is make sure we have all the needed standard imports.
    From PyTorch, this includes `torch.nn` and `torch.nn.functional`, which provide
    common building blocks that we use throughout this book. `torch.utils.data` has
    the tools for working with `Datasets`, and `idlmam` provides code we have written
    in previous chapters as we progress:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要确保我们拥有所有需要的标准导入。从PyTorch，这包括`torch.nn`和`torch.nn.functional`，它们提供了我们在整本书中使用的常见构建块。`torch.utils.data`提供了处理`Datasets`的工具，而`idlmam`提供了我们在前几章中编写的代码，随着我们的进展：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 2.1.3  The training loop
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.3 训练循环
- en: Now that we have those additional imports, let’s start by writing a *training
    loop*. Assume we have a loss function `loss_func` (ℓ(⋅,⋅)) that takes a `prediction`
    (ŷ) and a `target` (y), returning a single score for how well a `model` (*f*(⋅))
    has done. We need an iterator that loads the training data for us to train on.[²](#fn2)
    This `training_loader` will gives us pairs of `input`s with their associated `label`s
    for training.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了那些额外的导入，让我们先写一个*训练循环*。假设我们有一个损失函数`loss_func`（ℓ(⋅,⋅)），它接受一个`prediction`（ŷ）和一个`target`（y），返回一个分数，表示模型（*f*(⋅)）做得有多好。我们需要一个迭代器来为我们加载训练数据。[²](#fn2)
    这个`training_loader`将为我们提供与训练相关的`input`s和`label`s对。
- en: Figure 2.2 shows the steps of a training loop. The yellow Prep section shows
    the object creation that needs to be done before training can start. We have to
    pick the device that will do all the computations (normally a GPU), define our
    model *f*(⋅), and create an optimizer for the model’s parameters θ. The red regions
    indicate the start/repetition of the loop, which provides new data for us to train
    on. The blue region computes the prediction ŷ and loss ℓ(*ŷ*,*y*) for the model
    with its *current* parameters Θ. The green section takes the loss and computes
    the gradients and updates to alter the parameters Θ. Notice that the colors match
    the steps we saw in figure 2.1, but with a little more detail showing the PyTorch
    functions we need to call.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2展示了训练循环的步骤。黄色的准备部分显示了在开始训练之前需要完成的对象创建。我们必须选择将执行所有计算（通常是GPU）的设备，定义我们的模型 *f*(⋅)，并为模型的参数
    θ 创建一个优化器。红色区域表示循环的开始/重复，为我们提供了新的数据以进行训练。蓝色区域计算模型当前参数 Θ 的预测 ŷ 和损失 ℓ(*ŷ*,*y*)。绿色部分使用损失计算梯度和对参数
    Θ 的更新以改变参数 Θ。请注意，颜色与我们在图2.1中看到的步骤相匹配，但提供了更多细节，显示了我们需要调用的PyTorch函数。
- en: '![](../Images/CH02_F02_Raff.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F02_Raff.png)'
- en: Figure 2.2 Diagram of the training loop process. It includes the same three
    major steps of training a model in PyTorch that we originally described, with
    matching color-coding. New is the initialization of objects that are reused in
    every training loop. Solid areas show steps, and dashed arrows show effects.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2展示了训练循环过程图。它包括我们在最初描述的PyTorch中训练模型时的相同三个主要步骤，并使用匹配的颜色编码。新的是初始化在每次训练循环中重复使用的对象。实心区域表示步骤，虚线箭头表示效果。
- en: Using PyTorch, we can write a minimal amount of code that is enough to train
    many different kinds of neural networks. The `train_simple_network` function in
    the next code block follows all the parts of the figure 2.2 process. First we
    create an `optimizer` that takes in the model’s `parameters()` Θ that will be
    altered. We then move the model to the correct compute `device` and repeat the
    optimization process for some number of `epochs`. Each epoch means we used every
    data point **x**[i] *once*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyTorch，我们可以编写足够少的代码来训练许多不同类型的神经网络。下一个代码块中的`train_simple_network`函数遵循图2.2过程中的所有部分。首先，我们创建一个`optimizer`，它接受将被改变的模型的`parameters()`
    Θ。然后，我们将模型移动到正确的计算`device`，并重复优化过程一定数量的`epochs`。每个epoch意味着我们使用了一次每个数据点 **x**[i]。
- en: Each epoch involves putting the model into training mode with `model.train()`.
    The `training_loader` gives us our data in groups of tuples (**x**,*y*), which
    we move to the same compute `device`. The inner loop over these tuples cleans
    up the optimizer state with `zero_grad()` and then passes the `inputs` to the
    `model` to get a prediction `y_hat`. Our `loss_fun` takes in the prediction `y_hat`
    and the true `labels` to calculate a `loss`, describing how badly our network
    has done. Then we compute the gradients with `loss.backward()` and take a `step()`
    with the `optimizer`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 每个epoch包括使用`model.train()`将模型放入训练模式。`training_loader`以元组（**x**,*y*）的形式提供我们的数据，我们将它们移动到相同的计算`device`。这些元组的内部循环使用`zero_grad()`清理优化器状态，然后将`inputs`传递给`model`以获取预测`y_hat`。我们的`loss_fun`接受预测`y_hat`和真实`labels`来计算一个`loss`，描述了我们的网络做得有多糟糕。然后我们使用`loss.backward()`计算梯度，并使用`optimizer`的`step()`进行一步。
- en: 'The code for the simple training loop is as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 简单训练循环的代码如下：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Yellow step is done here. Creates the optimizer and moves the model to the
    compute device. SGD is stochastic gradient descent over the parameters Θ.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这里完成了黄色步骤。创建优化器并将模型移动到计算设备。SGD是对参数Θ的随机梯度下降。
- en: ❷ Places the model on the correct compute resource (CPU or GPU)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将模型放置在正确的计算资源（CPU或GPU）上
- en: ❸ The two for loops handle the red steps, iterating through all the data (batches)
    multiple times (epochs).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 两个for循环处理红色步骤，多次（epochs）迭代所有数据（批次）。
- en: ❹ Puts our model in training mode
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将我们的模型置于训练模式
- en: ❺ Moves the batch of data to the device we are using. This is the last red step.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将数据批次移动到我们正在使用的设备上。这是最后一个红色步骤。
- en: '❻ First a yellow step: prepare the optimizer. Most PyTorch code does this first
    to make sure everything is in a clean and ready state. PyTorch stores gradients
    in a mutable data structure, so we need to set it to a clean state before we use
    it. Otherwise, it will have old information from a previous iteration.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 首先是一个黄色步骤：准备优化器。大多数PyTorch代码首先这样做，以确保一切都在干净和准备就绪的状态。PyTorch将梯度存储在可变的数据结构中，因此在使用之前我们需要将其设置为干净的状态。否则，它将包含来自先前迭代的旧信息。
- en: ❼ This line and the next perform the two blue steps. This line computes *f*[θ](**x**[i])
    .
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 这行代码和下一行执行两个蓝色步骤。这行代码计算*f*[θ](**x**[i])。
- en: ❽ Computes the loss
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 计算损失
- en: ❾ The remaining two yellow steps compute the gradient and “.step()" the optimizer.
    The call on this line computes ∇[Θ].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 剩下的两个黄色步骤计算梯度并对优化器进行“.step()"操作。这行代码计算∇[Θ]。
- en: ❿ Updates all the parameters *Θ*[*k* + 1] = *Θ*[k] − *η* ⋅ ∇[*Θ*[k]]ℓ(*ŷ*,*y*)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 更新所有参数 *Θ*[*k* + 1] = *Θ*[k] − *η* ⋅ ∇[*Θ*[k]]ℓ(*ŷ*,*y*)
- en: ⓫ Grabs information we would like to have
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 获取我们想要的信息
- en: 2.1.4  Defining a dataset
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.4 定义数据集
- en: The code we have just described is sufficient to train almost all of the neural
    networks we design during this book. Now we need some data, a network, and a loss
    function to work with. Let’s start by training a simple *linear regression* model.
    You should recall from a ML class or training that in *regression* problems, we
    want to predict a numeric value. For example, predicting the miles per gallon
    (mpg) of a car based on its features (e.g., weight in pounds, engine size, year
    produced) would be a regression problem because the mpg could be 20, 24, 33.7,
    or hypothetically even 178.1342 or almost any number.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才描述的代码足以训练本书中设计的几乎所有神经网络。现在我们需要一些数据、一个网络和一个损失函数来工作。让我们先训练一个简单的 *线性回归* 模型。你应该还记得，从机器学习课程或培训中，在
    *回归* 问题中，我们想要预测一个数值。例如，根据汽车的特征（例如，重量（磅）、发动机大小、生产年份）预测每加仑英里数（mpg）将是一个回归问题，因为mpg可能是20、24、33.7，或者假设甚至可以是178.1342或几乎任何数字。
- en: 'Here we’ve created a synthetic regression problem with linear and nonlinear
    components, with some noise added to make it interesting. Because of how strong
    the linear component is, a linear model will do OK but won’t be perfect:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个包含线性和非线性成分的合成回归问题，并添加了一些噪声使其更有趣。由于线性成分非常强，线性模型可以做得不错，但不会完美：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Creates one-dimensional input
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一维输入
- en: ❷ Creates output
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建输出
- en: '![](../Images/CH02_UN03_Raff.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_UN03_Raff.png)'
- en: We have created a simple toy problem with a strong linear trend and a smaller
    but consistent oscillation up and down. We use toy problems like this one to do
    experiments so we can see the results and get a more intuitive understanding of
    what is happening. But we need the problem in a form that PyTorch will understand.
    The next code block creates a simple dataset object that knows we have a one-dimensional
    problem. The training data is shaped as an (*n*,1) matrix, where n is the number
    of data points.The labels (`y`) take a similar form. When we get an item, we grab
    the correct row of the dataset and return a PyTorch `tensor` object that is a
    `torch.float32` type, which is the default type for most things in PyTorch.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个简单的玩具问题，具有强烈的线性趋势和上下波动的小但一致的振荡。我们使用这样的玩具问题来进行实验，以便我们可以看到结果，并获得对正在发生的事情的更直观的理解。但是，我们需要以PyTorch能够理解的形式来表示这个问题。下一个代码块创建了一个简单的数据集对象，它知道我们有一个一维问题。训练数据以(*n*,1)矩阵的形式排列，其中n是数据点的数量。标签(`y`)采用类似的形式。当我们获取一个项目时，我们会抓取数据集的正确行，并返回一个PyTorch
    `tensor`对象，它是`torch.float32`类型，这是PyTorch中大多数事物的默认类型。
- en: Along with the `Dataset`, we also need a `DataLoader`, which is already implemented
    for us by PyTorch. Whereas the `Dataset` defines how to get any specific data
    point, the `DataLoader` decides which data points to get. The standard approach
    is to pick the data points at random one at a time, which ensures that your model
    learns the data and not the order of the data.[³](#fn3) There is a lot of built-in
    functionality with PyTorch’s `DataLoader`, and we introduce some of it on an as-needed
    basis, but feel free to learn more in the documentation at [https://pytorch.org/docs/stable/data.html](https://pytorch.org/docs/stable/data.html).
    The most important feature to know about a `DataLoader` is that while your model
    is busy training on the GPU, your `DataLoader` is busy fetching the next data
    so the GPU is as busy as possible. (This type of performance optimization is often
    called *pipelining*.)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `Dataset`，我们还需要一个 `DataLoader`，它已经被 PyTorch 为我们实现了。`Dataset` 定义了如何获取任何特定的数据点，而
    `DataLoader` 决定获取哪些数据点。标准的方法是一次随机选择一个数据点，这确保了你的模型学习的是数据而不是数据的顺序。[³](#fn3) PyTorch
    的 `DataLoader` 有很多内置的功能，我们根据需要介绍其中的一些，但你可以自由地在[https://pytorch.org/docs/stable/data.html](https://pytorch.org/docs/stable/data.html)的文档中了解更多。关于
    `DataLoader` 最重要的是，当你的模型忙于在 GPU 上训练时，你的 `DataLoader` 正在忙于获取下一个数据，这样 GPU 就尽可能忙碌。（这种性能优化通常被称为
    *pipelining*。）
- en: 'Here’s the code:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这是代码：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How reshape works
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用 reshape
- en: The `reshape` function is important to understand because we use its behavior
    throughout this book. Suppose we have a tensor with six total values. That could
    be a vector of length 6, a 2 × 3 matrix, 3 × 2, or a tensor with three dimensions
    where one dimension has a size of “1.” As long as the total number of values stays
    the same, we can *reinterpret* the tensor as having a different shape with the
    values moved around. The following figure shows how this can be done. What is
    special about `reshape` is that it lets us specify all but one of the dimensions,
    and it automatically puts the leftovers into the unspecified dimension. This leftover
    dimension is denoted with –1; as we add more dimensions, there are more ways we
    can ask NumPy or PyTorch to reshape a tensor.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`reshape` 函数的理解非常重要，因为我们在整本书中都会使用它的行为。假设我们有一个包含六个总值的张量。这可能是一个长度为 6 的向量，一个 2
    × 3 矩阵，3 × 2，或者一个具有一个尺寸为“1”的三个维度的张量。只要总值的数量保持不变，我们就可以 *重新解释* 张量，使其具有不同的形状，值被重新排列。以下图显示了如何做到这一点。`reshape`
    的特殊之处在于它允许我们指定除了一个维度之外的所有维度，并且它自动将剩余的值放入未指定的维度中。这个剩余的维度用 –1 表示；随着我们添加更多的维度，我们可以以更多的方式要求
    NumPy 或 PyTorch 重塑张量。'
- en: '![](../Images/CH02_F03_Raff.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F03_Raff.png)'
- en: Four different tensor shapes that can be used to represent the same six values.
    The `reshape` function takes as many arguments as you want axes in the resulting
    tensor. If you know the exact size, you can specify it; or if you don’t know the
    size for one axis, you can use –1 to indicate a leftover spot.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 有四种不同的张量形状可以用来表示相同的六个值。`reshape` 函数接受你想要的任何数量的轴作为结果张量的参数。如果你知道确切的尺寸，你可以指定它；或者如果你不知道一个轴的尺寸，你可以使用
    –1 来表示一个剩余的位置。
- en: The `view` function has the same kind of behavior. Why have two functions with
    the same behavior? Because `view` uses less memory and can be faster but can throw
    errors if used inappropriately. You will learn the details with more advanced
    use of PyTorch; but for the sake of simplicity, you can *always* call `reshape`
    safely.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`view` 函数也有类似的行为。为什么有两个具有相同行为的函数？因为 `view` 使用更少的内存并且可以更快，但如果使用不当可能会抛出错误。你将在
    PyTorch 的更高级使用中学习这些细节；但为了简单起见，你可以 *始终* 安全地调用 `reshape`。'
- en: Note If you have a total of N items in your tensor, you can reshape it into
    any new number of dimensions as long as the final number of items adds to N. So
    we can turn the *N* = 6 item tensor into a shape of (3,2) because 3 × 2 = 6. This
    also means we can have *any* number of total dimensions if we keep inserting dimensions
    of size 1\. For example, we can turn the *N* = 6 values into a tensor with five
    dimensions by doing (3,1,1,2,1) because 3 × 1 × 1 × 2 × 1 = 6.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你在你的张量中有总共 N 个项目，你可以将其重塑为任何新的维度数，只要最终的项目数加起来等于 N。所以我们可以将 *N* = 6 项的张量转换成形状为
    (3,2)，因为 3 × 2 = 6。这也意味着如果我们继续插入尺寸为 1 的维度，我们可以有 *任何* 数量的总维度。例如，我们可以通过执行 (3,1,1,2,1)
    将 *N* = 6 的值转换成一个具有五个维度的张量，因为 3 × 1 × 1 × 2 × 1 = 6。
- en: 2.1.5  Defining the model
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.5 定义模型
- en: At this point, we have successfully created a training loop and a `Dataset`
    object to load our dataset. The last thing we are missing is a PyTorch model that
    implements the linear regression algorithm as a network. This is the Model box
    from figure 2.2, which I’ve isolated in figure 2.3.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经成功创建了一个训练循环和一个用于加载数据集的 `Dataset` 对象。我们最后缺少的是实现线性回归算法作为网络的 PyTorch
    模型。这是图 2.2 中的模型框，我在图 2.3 中将其单独隔离出来。
- en: '![](../Images/CH02_F04_Raff.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F04_Raff.png)'
- en: Figure 2.3 The Model box denotes the neural network *f[Θ]*(∙) that we want to
    train. This could be a small network or a very complex one. It is encapsulated
    into one model object with a single set of parameters Θ. The same process works
    for almost any network definition.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 模型框表示我们想要训练的神经网络 *f[Θ]*(∙)。这可能是一个小型网络或一个非常复杂的网络。它被封装在一个具有单一参数集 Θ 的模型对象中。几乎任何网络定义的过程都是相同的。
- en: 'Defining the network we will use is very easy in this case. We want a simple
    linear function, and a linear function means a weight matrix W. Applying a weight
    matrix W to an input x means taking the matrix vector product. So we want a model
    *f*(⋅) that looks like this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个情况下，定义我们将使用的网络非常简单。我们想要一个简单的线性函数，而线性函数意味着权重矩阵 W。将权重矩阵 W 应用到输入 x 上意味着取矩阵向量积。因此，我们想要一个看起来像这样的模型
    *f*(⋅)：
- en: '![](../Images/ch2-eqs-to-illustrator0x.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/ch2-eqs-to-illustrator0x.png)'
- en: The vector x has all of our d features (in this case, *d* = 1), and the matrix
    W has a row for every feature and a column for every output. We use *W*^(*d*,
    *C*) to be extra explicit that it is a tensor with d rows and C columns. That’s
    a common notation we use in this book to be precise about the shapes of certain
    objects. Since we are predicting a single value, this means *C* = 1.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 x 包含了我们所有的 d 个特征（在这种情况下，*d* = 1），矩阵 W 对于每个特征有一行，对于每个输出有一列。我们使用 *W*^(*d*,
    *C*) 来明确指出它是一个有 d 行和 C 列的张量。这是我们在这本书中用来精确描述某些对象形状的常见符号。由于我们预测的是一个单一值，这意味着 *C*
    = 1。
- en: 'Notice, though, that this linear function is not quite complete. If **x** =
    0, then *f*(**x**) = 0. That’s a very strong constraint to have on a model. Instead,
    we add a *bias term* b that has no interaction with x:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管这个线性函数并不完整。如果 **x** = 0，那么 *f*(**x**) = 0。这对模型来说是一个非常强的约束。相反，我们添加一个没有与
    x 交互的 *偏差项* b：
- en: '![](../Images/ch2-eqs-to-illustrator2x.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/ch2-eqs-to-illustrator2x.png)'
- en: Adding a bias term allows the model to shift its solution to the left or right
    as needed. Luckily for us, PyTorch has a `Module` that implements a linear function
    like this, which we can access using `nn.Linear(d, C)`. This creates a linear
    layer with d inputs and C outputs—exactly what we want.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 添加一个偏差项允许模型根据需要将其解向左或向右移动。幸运的是，PyTorch 有一个实现这种线性函数的 `Module`，我们可以通过 `nn.Linear(d,
    C)` 来访问它。这创建了一个具有 d 个输入和 C 个输出的线性层——这正是我们想要的。
- en: Note The bias term is always a + **b** on the side that does not interact with
    anything else. Because bias terms are almost always used but are annoying and
    cumbersome to write out, they are often dropped and assumed to exist implicitly.
    We do that in this book as well. Unless we state otherwise, assume the bias is
    implicitly there. If you see three weight matrices, assume there are three bias
    terms, one for each.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：偏差项始终是 a + **b** 在不与其他任何东西交互的一侧。由于偏差项几乎总是被使用，但写起来很麻烦，因此它们经常被省略，并假设它们隐含存在。在这本书中我们也这样做。除非我们明确说明，否则请假设偏差是隐含存在的。如果你看到三个权重矩阵，假设有三个偏差项，每个对应一个。
- en: Modules are how PyTorch organizes the building blocks of modern neural network
    design. Modules have a `forward` function that takes inputs and produces an output
    (we need to implement this if we make our own `Module`), and a `backward` function
    (PyTorch takes care of this for us unless we have a reason to intervene). Many
    standard modules are provided in the `torch.nn` package, and we can build new
    ones out of tensors and `Parameter` and `torch.nn.functional` objects. Modules
    may also contain other modules, which is how we build larger networks.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 模块是 PyTorch 组织现代神经网络设计构建块的方式。模块有一个 `forward` 函数，它接受输入并产生输出（如果我们创建自己的 `Module`，我们需要实现这个函数），还有一个
    `backward` 函数（除非我们有干预的理由，否则 PyTorch 会为我们处理这个函数）。`torch.nn` 包中提供了许多标准模块，我们可以使用张量、`Parameter`
    和 `torch.nn.functional` 对象来构建新的模块。模块也可能包含其他模块，这是我们构建更大网络的方式。
- en: 2.1.6  Defining the loss function
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.6 定义损失函数
- en: 'So `nn.Linear` gives us our model *f*(**x**), but we still need to decide on
    a loss function ℓ. Again, PyTorch makes this pretty simple for a standard regression
    problem. Say the ground truth is y, and our prediction is *ŷ* = *f*(**x**). How
    do we quantify the difference between y and ŷ? We can just look at the absolute
    difference between them: ℓ(*y*,*ŷ*) = |*y*−*ŷ*|.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因此 `nn.Linear` 给我们模型 *f*(**x**)，但我们仍然需要决定一个损失函数 ℓ。同样，PyTorch 使得对于标准的回归问题，这变得非常简单。假设真实值是
    y，我们的预测是 *ŷ* = *f*(**x**)。我们如何量化 y 和 ŷ 之间的差异？我们只需查看它们之间的绝对差异：ℓ(*y*,*ŷ*) = |*y*−*ŷ*|。
- en: Why the *absolute* difference? If we did not take the absolute value, *ŷ* <
    *y* would produce a positive loss, encouraging the model *f*(**x**) to make its
    prediction smaller. But if *ŷ* > *y*, then *y* − *ŷ* would produce a *negative*
    loss. If it feels intimidating to reason about this using symbols like y and ŷ,
    try plugging in some actual values. So if *ŷ* = 100 and *y* = 1, and we computed
    the loss as *y* − *ŷ* = 1 − 100 = − 99, we would end up with a loss of –99! A
    negative loss would be confusing (what would that be, profit?) and would encourage
    the network to make its predictions even larger when they were already too big.
    Our goal is *ŷ* = *y*, but since the network will blindly march forward trying
    to *minimize* the loss, it will learn to make ŷ unrealistically large to exploit
    a negative loss. This is why we need our loss function to always return zero or
    a positive value. Otherwise, the loss will not make sense. Remember that the loss
    function is a penalty for errors, and negative penalties would mean encouragement.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么是 *绝对* 差异？如果我们没有取绝对值，*ŷ* < *y* 将产生一个正损失，鼓励模型 *f*(**x**) 的预测更小。但如果是 *ŷ* >
    *y*，那么 *y* − *ŷ* 将产生一个 *负* 损失。如果您觉得用 y 和 ŷ 这样的符号进行推理感到害怕，试着插入一些实际值。所以如果 *ŷ* =
    100 和 *y* = 1，并且我们计算损失为 *y* − *ŷ* = 1 − 100 = − 99，我们最终会得到一个损失为 –99！负损失会令人困惑（那会是什么，利润？）并且会鼓励网络在预测已经太大时进一步增大其预测。我们的目标是
    *ŷ* = *y*，但由于网络会盲目地前进，试图 *最小化* 损失，它将学会使 ŷ 不切实际地大，以利用负损失。这就是为什么我们需要我们的损失函数始终返回零或正值。否则，损失将没有意义。请记住，损失函数是错误的惩罚，负惩罚意味着鼓励。
- en: 'Another option is to take the squared difference between y and ŷ: ℓ(*y*,*ŷ*)
    = (*y*−*ŷ*)². This again results in a function that is zero only if *y* = *ŷ*
    and that grows only as y and ŷ move farther away from each other.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是取 y 和 ŷ 之间的平方差：ℓ(*y*,*ŷ*) = (*y*−*ŷ*)²。这又导致了一个函数，仅在 *y* = *ŷ* 时为零，并且随着
    y 和 ŷ 相互远离而增长。
- en: 'Both of these options are pre-implemented in PyTorch. The former is called
    the *L*1 loss, as it corresponds to taking the 1-norm of the different (i.e.,
    ∥*y* − *ŷ*∥[1]). The latter is popularly known as the mean squared error (MSE,
    also called *L*2) loss, and is the most popular, so we will use it going forward:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种选项都在 PyTorch 中预先实现了。前者被称为 *L*1 损失，因为它对应于取不同（即，∥*y* − *ŷ*∥[1]）的 1-范数。后者通常被称为均方误差（MSE，也称为
    *L*2）损失，是最受欢迎的，因此我们将继续使用它：
- en: '| Loss function ℓ(*y*,*ŷ*) | PyTorch module |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 损失函数 ℓ(*y*,*ŷ*) | PyTorch 模块 |'
- en: '| &#124;*y*−*ŷ*&#124; | `torch.nn.L1Loss` |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| &#124;*y*−*ŷ*&#124; | `torch.nn.L1Loss` |'
- en: '| (*y*−*ŷ*)² | `torch.nn.MSELoss` |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| (*y*−*ŷ*)² | `torch.nn.MSELoss` |'
- en: Choosing between two loss functions
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个损失函数之间进行选择
- en: You have two loss functions, L1 and MSE, both of which are appropriate for a
    regression problem. How do you choose which one to use? We won’t detour into the
    nuanced differences between loss functions because any one built into PyTorch
    will give reasonable results. You just need to know which loss functions are appropriate
    for which type of problem (like regression versus classification).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 您有两个损失函数，L1 和 MSE，两者都适用于回归问题。您如何选择使用哪一个？我们不会深入探讨损失函数之间细微的差异，因为 PyTorch 中内置的任何损失函数都会给出合理的结果。您只需要知道哪些损失函数适用于哪种类型的问题（如回归与分类）。
- en: Getting comfortable with the meaning behind the math will help you to make these
    choices. In this case, the MSE loss has the squared term, which makes large differences
    grow larger (e.g., 100² will become 10,000); and L1 will keep differences the
    same (e.g., |100| = 100). So if the problem you are trying to solve is one where
    small differences are OK but large ones are really bad, the MSE loss might be
    a better choice. If your problem is such that being off by 200 is twice as bad
    as being off by 100, the L1 loss makes more sense. This is not a complete picture
    of how to choose between these two options, but it’s a good way to make an initial
    choice.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 理解数学背后的含义将帮助您做出这些选择。在这种情况下，均方误差（MSE）损失具有平方项，使得大的差异变得更大（例如，100²将变为10,000）；而L1将保持差异不变（例如，|100|
    = 100）。因此，如果您试图解决的问题是小差异可以接受但大差异真的很糟糕，那么MSE损失可能是一个更好的选择。如果您的问题是偏离200比偏离100更糟糕两倍，那么L1损失更有意义。这并不是选择这两种选项的完整图景，但这是一个做出初始选择的好方法。
- en: '2.1.7  Putting it together: Training a linear regression model on the data'
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.7 将其组合：在数据上训练线性回归模型
- en: 'We now have everything we need to create a linear regression: the `Dataset`,
    the`train_simple_network` function, a `loss_func` ℓ, and a `nn.Linear` model.
    The following code shows how we can quickly set it all up and pass it to our function
    to train a model:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在拥有创建线性回归所需的一切：`Dataset`、`train_simple_network`函数、`loss_func` ℓ和`nn.Linear`模型。以下代码显示了我们可以如何快速设置所有这些，并将其传递给我们的函数以训练模型：
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Did it work? Do we have a trained model? That’s easy to find out, especially
    since this is a one-dimensional problem. We can just plot our model’s prediction
    for all the data. We will use the `with torch.no_grad():` context to get those
    predictions: it tells PyTorch that for any computation done within the scope of
    the `no_grad()` block, *do not calculate gradients*. We only want gradients to
    be computed during *training*. The gradient calculations take additional time
    and memory and can cause bugs if we want to train the model more after performing
    a prediction. So, good practice is to make sure we use the `no_grad()` block when
    making predictions. The following code block uses `no_grad()` to get the predictions:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 它是否有效？我们是否有训练好的模型？这很容易找到答案，尤其是这是一个一维问题。我们只需绘制我们模型对所有数据的预测。我们将使用`with torch.no_grad():`上下文来获取这些预测：它告诉PyTorch，在`no_grad()`块的作用域内进行的任何计算，*不要计算梯度*。我们只希望在*训练*期间计算梯度。梯度计算需要额外的时间和内存，如果在预测后想要再次训练模型，可能会导致错误。因此，良好的做法是在进行预测时确保我们使用`no_grad()`块。以下代码块使用`no_grad()`来获取预测：
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ The data
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 数据
- en: ❷ What our model learned
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们模型学到了什么
- en: '![](../Images/CH02_UN04_Raff.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_UN04_Raff.png)'
- en: This code plots the result of our network, and it learned a good linear fit
    to the somewhat nonlinear data. This is exactly what we asked of the model, and
    the results look correct.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码绘制了网络的输出结果，并且它学习到了对非线性数据的好线性拟合。这正是我们要求模型做到的，结果看起来是正确的。
- en: Note Making predictions on new data is also called *inference*. This is common
    jargon among ML practitioners—particularly in the deep learning community—because
    neural networks often require a GPU, so deploying a model is a bigger deal. Companies
    often buy GPUs designed for inference that have less RAM to be more cost-effective.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在新数据上进行预测也称为*推理*。这在机器学习从业者中是常见的术语——尤其是在深度学习社区中——因为神经网络通常需要GPU，所以部署模型是一件大事。公司通常会购买专为推理设计的GPU，以减少内存来提高成本效益。
- en: You have now used all the mechanics and tools for building neural networks.
    Each of the components we have described—a loss function ℓ, a `Module` for specifying
    our network, and the training loop—can be swapped in and out in a piecemeal fashion
    to build more powerful and complex models.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经使用了构建神经网络的所有机制和工具。我们描述的每个组件——损失函数ℓ、指定我们网络的`Module`和训练循环——都可以以零散的方式替换，以构建更强大和复杂的模型。
- en: 2.2 Building our first neural network
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 构建我们的第一个神经网络
- en: Now we have learned how to create a training loop and use gradient descent to
    modify a model so that it learns to solve a problem. This is the foundational
    framework we will use for all learning in this book. To train a neural network,
    we only need to replace the `model` object that we defined. The trick is knowing
    how to define these models. If we do a good job defining a neural network, it
    should be able to capture and model the nonlinear parts of the data. For our toy
    example, that means getting the smaller oscillations and not just the larger linear
    trend.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何创建训练循环并使用梯度下降来修改模型，以便它学会解决问题。这是我们在这本书中所有学习的基础框架。要训练一个神经网络，我们只需要替换我们定义的
    `model` 对象。关键是知道如何定义这些模型。如果我们能很好地定义一个神经网络，它应该能够捕捉和模拟数据的非线性部分。对于我们的玩具示例，这意味着获取较小的振荡，而不仅仅是较大的线性趋势。
- en: When we discuss neural networks and deep learning, we are usually talking about
    *layers*. Layers are the building blocks we use to define our `model`, and most
    of the PyTorch `Module` classes implement different layers that have different
    purposes. The linear regression model we just built could be described as having
    an *input* layer (the input data itself) and a *linear layer* (`nn.Linear`) that
    made the predictions.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论神经网络和深度学习时，我们通常是在谈论 *层*。层是我们用来定义我们的 `model` 的构建块，PyTorch 的 `Module` 类中的大多数类实现了具有不同目的的不同层。我们刚刚构建的线性回归模型可以描述为有一个
    *输入* 层（输入数据本身）和一个 *线性层* (`nn.Linear`) 进行预测。
- en: 2.2.1  Notation for a fully connected network
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1  全连接网络的符号表示
- en: Our first neural network will be a simple *feed-forward* *fully connected* neural
    network. It’s called *feed-forward* because every output from one layer flows
    directly into the next layer. So each layer has one input and one output and progresses
    sequentially. It is called *fully connected* because each network input has a
    connection to everything from the previous layer.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个神经网络将是一个简单的 *前馈* *全连接* 神经网络。它被称为 *前馈*，因为每一层的输出直接流入下一层。所以每一层有一个输入和一个输出，并按顺序进行。它被称为
    *全连接*，因为每个网络输入都与前一层的所有内容相连。
- en: Let’s start with what is called a *hidden layer*. The input x is consider the
    *input layer*, and a ℝ^C (a vector with C outputs for C predictions) dimensional
    output is called the *output layer*. In our linear regression model, we essentially
    had only an input layer and an output layer. You can think of hidden layers as
    anything sandwiched between input and output.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从所谓的 *隐藏层* 开始。输入 x 被认为是 *输入层*，一个具有 C 个输出（C 个预测）的 ℝ^C 维输出被称为 *输出层*。在我们的线性回归模型中，我们实际上只有一个输入层和一个输出层。你可以把隐藏层想象成输入和输出之间的任何东西。
- en: How do we do that? Well, the easiest option is to stick another matrix between
    the input and output. So instead of
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何做到这一点？好吧，最简单的方法是在输入和输出之间再添加一个矩阵。所以，而不是
- en: '![](../Images/CH02_F04_Raff_EQ01.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F04_Raff_EQ01.png)'
- en: 'we add a second layer with a new matrix, giving us something like this:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了一个新的矩阵的第二层，得到类似这样的结构：
- en: '![](../Images/CH02_F04_Raff_EQ02.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F04_Raff_EQ02.png)'
- en: Notice the new value n of the matrix dimension. It’s a new hyperparameter for
    us to tune and deal with. That means we get to decide what the value of n should
    be. It is called the *hidden layer size* or *number of neurons* in the first hidden
    layer. Why neurons?
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意矩阵维度的新的值 n。它是我们需要调整和处理的新的超参数。这意味着我们可以决定 n 的值应该是多少。它被称为 *隐藏层大小* 或 *第一隐藏层的神经元数量*。为什么是神经元？
- en: If you draw every intermediate output as a *node* and draw arrows representing
    weights, you get what could be described as a *network*. This is shown in figure
    2.4\. The lines connecting the input to hidden nodes correspond to a `nn.Linear(3, 4)`
    layer, which is a matrix *W*^(3 × 4). Every column of that matrix corresponds
    to the inputs of one of the *n* = 4 neurons or outputs of that layer. Each row
    is an input’s connection to each output. So if we wanted to know the strength
    of the connection from the second input to the fourth output, we would index `W[1,3]`.
    In the same fashion, the lines from hidden nodes to the output of the figure are
    a `nn.Linear(4, 1)` layer.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将每个中间输出作为一个 *节点* 并绘制表示权重的箭头，你得到的就是可以描述为 *网络* 的东西。这如图2.4所示。连接输入到隐藏节点的线条对应于一个
    `nn.Linear(3, 4)` 层，这是一个 *W*^(3 × 4) 的矩阵。该矩阵的每一列对应于 *n* = 4 个神经元中的一个的输入或该层的输出。每一行是输入与每个输出的连接。因此，如果我们想知道第二个输入到第四个输出的连接强度，我们会索引
    `W[1,3]`。同样地，从隐藏节点到图输出的线条是一个 `nn.Linear(4, 1)` 层。
- en: '![](../Images/CH02_F05_Raff.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F05_Raff.png)'
- en: Figure 2.4 A simple feed-forward fully connected network with one input layer
    of *d* = 3 inputs, a hidden layers with *n* = 4 neurons, and one output in the
    output layer. Connections feed directly into the next layer only, and each node
    in one layer is connected to every neuron in the preceding layer.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 一个简单的具有一个输入层（*d* = 3个输入）、一个隐藏层（*n* = 4个神经元）和一个输出层的输出层的正向全连接网络。连接只直接进入下一层，同一层中的每个节点都与前一层的每个神经元相连。
- en: Notice that all the arrows connecting nodes/neurons to each other only move
    from left to right. That is the feed-forward property. Also note that each node
    in one layer is connected to every other node in the next layer. That is the fully
    connected property.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，连接节点/神经元彼此的所有箭头都只从左到右移动。这就是前馈属性。此外，注意同一层中的每个节点都与下一层的每个节点相连。这就是全连接属性。
- en: This network interpretation was in part inspired by how neurons in the brain
    work. A simple toy model of a neuron and its connections is shown in figure 2.5\.
    On the left is a neuron, which has many *dendrites* that are connected to other
    neurons and act as the inputs. The dendrites get electrical signals when other
    neurons fire and carry those signals to the nucleus (center) of the neuron, which
    sums all the signals together. Finally, the neuron emits a new signal out from
    its *axon*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这种网络解释部分灵感来源于大脑中神经元的工作方式。图2.5展示了神经元及其连接的简单玩具模型。左侧是一个神经元，它有许多与其他神经元相连的树突，充当输入。当其他神经元放电时，树突会接收到电信号并将这些信号携带到神经元的核（中心），该核将所有信号相加。最后，神经元从其轴突发出一个新的信号。
- en: '![](../Images/CH02_F06_Raff.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F06_Raff.png)'
- en: Figure 2.5 Simplified diagram of biological neuron connectivity. By analogy,
    dendrites are connections/weights between neurons, and the axon caries the result
    of the neuron forward. This is a loose inspiration and an oversimplification of
    how real neurons work.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 生物神经元连接的简化图。通过类比，树突是神经元之间的连接/权重，轴突携带神经元的输出结果。这是对真实神经元工作方式的松散启发和过度简化。
- en: Warning Although neural networks were originally inspired by how neurons are
    connected and wired in the brain, do not let the analogy drag you too far in.
    The previous description is a *very* simplified model. A neural network’s functionality
    is very far from what little we know about how the brain works in reality. You
    should take it as only a mild inspiration, not a literal analogy.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：尽管神经网络最初是受大脑中神经元连接和布线方式的启发，但不要让这种类比带你走得太远。前面的描述是一个非常简化的模型。神经网络的功能与我们关于大脑实际工作方式的有限了解相去甚远。你应该将其视为一种轻微的启发，而不是字面上的类比。
- en: 2.2.2  A fully connected network in PyTorch
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 PyTorch中的全连接网络
- en: According to these cool diagrams, and keeping with the idea of adding just one
    small change, we can insert two linear layers one after the other, we will have
    our first neural network. This is where the `nn.Sequential` `Module` comes into
    play. This is a `Module` that takes a list or sequence of `Module`s as its input.
    It then runs that sequence in a feed-forward fashion, using the output of one
    `Module` as the input to the next, until we have no more `Module`s. Figure 2.6
    shows how we can do this for the toy network with three inputs and four hidden
    neurons.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些酷炫的图表，并保持只添加一个小变化的想法，我们可以依次插入两个线性层，我们将拥有我们的第一个神经网络。这就是`nn.Sequential` `Module`发挥作用的地方。这是一个接受模块列表或序列作为其输入的`Module`。然后它以正向方式运行该序列，使用一个`Module`的输出作为下一个`Module`的输入，直到我们没有更多的`Module`。图2.6展示了我们如何为具有三个输入和四个隐藏神经元的玩具网络这样做。
- en: '![](../Images/CH02_F07_Raff.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F07_Raff.png)'
- en: Figure 2.6 Converting the conceptual feed-forward fully connected network into
    a PyTorch `Module`. `nn.Sequential` is a wrapper that takes two `nn.Linear` layers.
    The first `nn.Linear` defines the mapping from input to hidden layer, and the
    second `nn.Linear` defines the mapping from hidden layerto output.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 将概念性的前馈全连接网络转换为PyTorch `Module`。`nn.Sequential`是一个包装器，它接受两个`nn.Linear`层。第一个`nn.Linear`定义了从输入到隐藏层的映射，第二个`nn.Linear`定义了从隐藏层到输出的映射。
- en: Rendering this to practice is easy because all the other code we have written
    will still work. The following code creates a new simple `model` that is a sequence
    of two `nn.Linear` layers. Then we just pass the `model` into the same `train_simple_network`
    function and continue as before. This model has one input, one output, and a single
    hidden layer with 10 neurons:[⁴](#fn4)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 将此应用于实践很容易，因为所有其他我们编写的代码仍然有效。以下代码创建了一个新的简单`model`，它是由两个`nn.Linear`层组成的序列。然后我们只需将`model`传递给相同的`train_simple_network`函数，并继续之前的操作。这个模型有一个输入，一个输出，以及一个包含10个神经元的单个隐藏层：[⁴](#fn4)
- en: '[PRE7]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Input “layer" is implicitly the input.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入“层”隐式地是输入。
- en: ❷ Hidden layer
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 隐藏层
- en: ❸ Output layer
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 输出层
- en: Note The `nn.Sequential` class provides the easiest way to specify neural networks
    in PyTorch, and we use it for *every* network in this book! So, it is worth getting
    familiar with. Eventually we will build more complex networks that can’t be described
    *entirely* as a feed-forward process. Still, we will use the `nn.Sequential` class
    to help us organize the subcomponents of a network that can be organized that
    way. This class is essentially your go-to tool for organizing models in PyTorch.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：`nn.Sequential`类提供了在PyTorch中指定神经网络的最简单方法，我们在这本书的每个网络中都使用它！因此，熟悉它是值得的。最终，我们将构建更复杂的网络，这些网络不能完全描述为前馈过程。尽管如此，我们仍然会使用`nn.Sequential`类来帮助我们组织可以那样组织的网络的子组件。这个类基本上是你组织PyTorch中模型的必备工具。
- en: 'Now we can perform inference with our fancy neural network that has a hidden
    layer and see what we get. We use the exact same inference code as before. The
    only difference is our `model` object, which we’ve redesigned. You may notice
    a new NumPy function`ravel()` that is called to make the plot; using this function
    is the same as calling `reshape(-1)` on a PyTorch tensor, and we call it because
    `Y_pred` has an initial shape of (*N*,1):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用具有隐藏层的花哨神经网络进行推理，看看我们得到什么。我们使用与之前完全相同的推理代码。唯一的区别是我们的`model`对象，我们已经重新设计了它。你可能注意到了一个名为`ravel()`的新NumPy函数，它被用来生成绘图；使用这个函数与在PyTorch张量上调用`reshape(-1)`相同，我们之所以调用它是因为`Y_pred`的初始形状为(*N*,1)：
- en: '[PRE8]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Shape of (N, 1)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 形状为 (N, 1)
- en: ❷ The data
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 数据
- en: ❸ What our model learned
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们模型学到的内容
- en: '![](../Images/CH02_UN05_Raff.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_UN05_Raff.png)'
- en: What gives? We made our `model` *f*(⋅) more sophisticated, training took longer,
    and it did about the same or maybe worse! A little linear algebra will explain
    why this happened. Recall that we defined
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 那是什么情况？我们使我们的`模型` *f*(⋅)变得更加复杂，训练时间更长，但效果可能相同甚至更差！一点线性代数就能解释为什么会这样。回想一下，我们定义了
- en: '![](../Images/CH02_F07_Raff_EQ01.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F07_Raff_EQ01.png)'
- en: Here **W**[(*h*[1])]^(*d* × *n*) is our hidden layer. Since we have one feature
    *d* = 1 and *n* = 10 hidden units, **W**[(out)]^(*n* × *C*) is our output layer,
    where we still have the *n* = 10 hidden units from the previous layer and *C*
    = 1 total outputs. But we can simplify the two weight matrices. If we have a matrix
    with shape (*a*,*b*) and a second matrix with shape (*b*,*c*) and we multiply
    them together, we get a new matrix with shape (*a*,*c*). That means
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 **W**[(*h*[1])]^(*d* × *n*) 是我们的隐藏层。由于我们有一个特征 *d* = 1 和 *n* = 10 个隐藏单元，**W**[(out)]^(*n*
    × *C*) 是我们的输出层，其中我们仍然有来自前一层的 *n* = 10 个隐藏单元和 *C* = 1 个总输出。但我们可以简化这两个权重矩阵。如果我们有一个形状为
    (*a*,*b*) 的矩阵和一个形状为 (*b*,*c*) 的第二个矩阵，我们将它们相乘，我们得到一个形状为 (*a*,*c*) 的新矩阵。这意味着
- en: '![](../Images/CH02_F07_Raff_EQ02.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F07_Raff_EQ02.png)'
- en: and therefore
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: '![](../Images/CH02_F07_Raff_EQ03.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F07_Raff_EQ03.png)'
- en: That’s equivalent to the linear model we started with. This shows that *adding
    any number of sequential linear layers is equivalent to using just one linear
    layer*. Linear operations beget linear operations and are usually redundant. Placing
    multiple linear layers one after the other is a common error I see in code written
    by novice or junior practitioners.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们最初使用的线性模型是等价的。这表明*添加任意数量的顺序线性层等同于使用一个线性层*。线性操作产生线性操作，通常是冗余的。在代码中连续放置多个线性层是我在新手或初级从业者编写的代码中常见的一个错误。
- en: 2.2.3  Adding nonlinearities
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 添加非线性
- en: To get any kind of benefit, we need to introduce *nonlinearity* between every
    step. By inserting a nonlinear function after every linear operation, we allow
    the network to build up more complex functions. We call nonlinear functions that
    are used this way *activation functions*. The analogy from biology is that a neuron
    sums all of its inputs linearly and eventually *fires* or *activates*, sending
    a signal to other neurons in the brain.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得任何形式的益处，我们需要在每一步之间引入 *非线性*。通过在每个线性操作之后插入一个非线性函数，我们允许网络构建更复杂的函数。我们称这种以这种方式使用的非线性函数为
    *激活函数*。从生物学的类比来看，一个神经元线性地求和所有输入，最终 *触发* 或 *激活*，向大脑中的其他神经元发送信号。
- en: What should we use as our activation functions? The first two we will look at
    are the *sigmoid* (*σ*(⋅)) and *hyperbolic tangent* (tanh (⋅)) functions, which
    were two of the original activation functions and are still widely used.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该使用什么作为我们的激活函数？我们将首先查看的两个是 *sigmoid* (*σ*(⋅)) 和 *双曲正切* (tanh (⋅)) 函数，它们是原始激活函数中的两个，并且仍然被广泛使用。
- en: 'The tanh function is an historically popular nonlinearity. It maps everything
    into the range [−1,1]:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: tanh 函数是一个历史上流行的非线性函数。它将所有值映射到范围 [−1,1] 内：
- en: '![](../Images/CH02_F07_Raff_EQ04.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH02_F07_Raff_EQ04.png)'
- en: 'The sigmoid is the historical nonlinearity and is where the notation σ is most
    often used. It maps everything into the range [0,1]:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 是历史悠久的非线性函数，也是 σ 符号最常被使用的场合。它将所有值映射到范围 [0,1] 内：
- en: '![](../Images/CH02_F07_Raff_EQ05.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH02_F07_Raff_EQ05.png)'
- en: 'Let’s quickly plot what these look like. The input is on the x-axis, and the
    activation is on the y-axis:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速绘制一下这些函数的图像。输入在 x 轴上，激活值在 y 轴上：
- en: '[PRE9]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/CH02_UN06_Raff.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH02_UN06_Raff.png)'
- en: 'As promised, the sigmoid activation (*σ*(*x*)) maps everything to a minimum
    of 0 and a maximum of 1\. The tanh function goes from –1 to 1\. Notice how there
    is a range of input, around 0, where the tanh function *looks* linear but then
    diverges away: that’s OK and can even be desirable. The important thing from a
    learning perspective is that neither tanh (⋅) or *σ*(⋅) can be *perfectly* fit
    by a linear function.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 正如承诺的那样，sigmoid 激活 (*σ*(*x*)) 将所有值映射到最小值 0 和最大值 1。tanh 函数的范围是从 -1 到 1。注意，在 0
    附近的输入范围内，tanh 函数 *看起来* 是线性的，但随后会发散：这是可以接受的，甚至可能是期望的。从学习的角度来看，重要的是 tanh (⋅) 或 *σ*(⋅)
    都不能被线性函数 *完美* 地拟合。
- en: 'We talk more about the properties of these functions in future chapters. For
    now, let’s use the tanh function. We will define a new model that matches the
    following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在未来的章节中更多地讨论这些函数的性质。现在，让我们使用 tanh 函数。我们将定义一个新的模型，该模型与以下内容匹配：
- en: '![](../Images/CH02_F07_Raff_EQ06.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH02_F07_Raff_EQ06.png)'
- en: 'Instead of stacking `nn.Linear` layers directly after one another, we call
    the tanh function after the first linear layer. When using PyTorch, we generally
    want to end our network with a `nn.Linear` layer. For this model, we have two
    `nn.Linear` layers, so we use 2 − 1 = 1 activations. This is as simple as adding
    a `nn.Tanh` node to our sequential network specification, which PyTorch has built
    in. Let’s see what happens when we train this new `model`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是直接将 `nn.Linear` 层堆叠在一起，而是在第一个线性层之后调用 tanh 函数。在使用 PyTorch 时，我们通常希望以一个 `nn.Linear`
    层结束我们的网络。对于这个模型，我们有两个 `nn.Linear` 层，所以我们需要 2 - 1 = 1 个激活函数。这就像在我们的顺序网络规范中添加一个
    `nn.Tanh` 节点一样简单，这是 PyTorch 内置的。让我们看看训练这个新 `model` 时会发生什么：
- en: '[PRE10]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Hidden layer
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 隐藏层
- en: ❷ Activation
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 激活函数
- en: ❸ Output layer
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 输出层
- en: ❹ The data
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 数据
- en: ❺ What our model learned
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 我们模型学到的内容
- en: '![](../Images/CH02_UN07_Raff.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH02_UN07_Raff.png)'
- en: Reproducibility of numbers
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 数字的再现性
- en: At this point in the book, you may notice that you don’t always get the *exact*
    same results I do. You may sometimes get *very* different-looking results. This
    is *normal* and *OK*. The initial weights for every neural network are random
    values, and a different set of initial random values may give you a different
    result. Don’t let that scare you off; getting used to the variability in results
    can be valuable.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的这一部分，你可能注意到你并不总是得到与我完全相同的结果。有时你可能会得到 *非常不同* 的结果。这是 *正常* 的，也是 *可以接受的*。每个神经网络的初始权重都是随机值，不同的随机值集合可能会给你不同的结果。不要因此感到害怕；习惯结果的变异性可能是有价值的。
- en: If you know about random seeds, you may be thinking, “If you set a random seed
    so that you know what the random results will be, wouldn’t that solve the problem?”
    This is correct at a high level, but unfortunately, the result of the layers in
    PyTorch are *not* always deterministic. They often exploit hardware-specific optimizations
    that can slightly alter the results and even change from run to run in odd circumstances.
    PyTorch is working on supporting more deterministic behavior, but it doesn’t work
    for everything yet, and it doesn’t work for all versions of CUDA. If I tried to
    enable it, it might make everything stop working for you. So, unfortunately, we
    are stuck with slightly different results every time you run the code.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你了解随机种子，你可能正在想，“如果你设置一个随机种子，知道随机结果会是什么，这难道不能解决问题吗？”这在高层次上是正确的，但不幸的是，PyTorch中层的输出并不总是确定的。它们经常利用硬件特定的优化，这可能会略微改变结果，甚至在某些情况下从一次运行到下一次运行会有所不同。PyTorch正在努力支持更多确定性行为，但还不是所有情况都适用，也不是所有CUDA版本都适用。如果我尝试启用它，可能会使你的所有东西都停止工作。所以，很遗憾，我们每次运行代码时都会得到略有不同的结果。
- en: Another option would be to run each experiment 5 to 15 times and plot the average
    result with error bars to show the variance of what can happen. But then the code
    samples would take 5 to 15 times longer to run. I’ve chosen to run things once
    to keep the time reasonable.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是运行每个实验5到15次，并绘制带有误差线的平均值，以显示可能发生的情况的变异性。但这样，代码示例的运行时间将会是5到15倍更长。我选择只运行一次，以保持时间合理。
- en: 'We can see that the network is now learning a nonlinear function, with bends
    that move and adapt to match the data’s behavior. It’s not perfect, though, especially
    for larger values of the input x on the right side of the plot. We also had to
    train for many more `epochs` than we did previously. That’s also pretty common.
    This is part of why we use GPUs so much in deep learning: we have larger models,
    which means each update requires more computation; and the larger models need
    more updates to converge, resulting in longer training times.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，网络现在正在学习一个非线性函数，其弯曲部分会移动和适应以匹配数据的行为。然而，这并不完美，尤其是在图表右侧输入x的较大值上。我们还需要比之前更多的`epochs`进行训练。这也是很常见的情况。这也是我们为什么在深度学习中大量使用GPU的原因之一：我们有更大的模型，这意味着每次更新都需要更多的计算；而且更大的模型需要更多的更新才能收敛，从而导致更长的训练时间。
- en: In general, the more complex a function that needs to be learned, the more rounds
    of training we have to perform—and we may even get more data. However, there are
    many ways to improve the quality and rate at which our neural networks learn from
    the data, which we review in more detail later in the book. For now, our goal
    is to learn the basics.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，需要学习的函数越复杂，我们就需要进行的训练轮数就越多——甚至可能需要更多的数据。然而，有许多方法可以提高我们的神经网络从数据中学习的质量和速度，这些方法我们将在本书的后续部分更详细地讨论。目前，我们的目标是学习基础知识。
- en: 2.3 Classification problems
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 分类问题
- en: You have now built your first neural network by extending a linear regression
    model, but what about classification problems? In this situation, you have C different
    *classes* that an input might belong to. For example, a car could be an SUV, sedan,
    coupe, or truck. As you may have guessed, you need an output layer that looks
    like `nn.Linear(n, C)`, where again n is the number of hidden units in the previous
    layer and C is the number of classes/outputs. It would be difficult to make C
    predictions if we had fewer than C outputs.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经通过扩展线性回归模型构建了你的第一个神经网络，但关于分类问题呢？在这种情况下，你有一个输入可能属于的C个不同的*类别*。例如，一辆车可能是一辆SUV、轿车、敞篷车或卡车。正如你可能已经猜到的，你需要一个看起来像`nn.Linear(n,
    C)`的输出层，其中n是前一层中的隐藏单元数，C是类别/输出的数量。如果我们有少于C个输出，要做出C个预测将是困难的。
- en: Similar to how we can walk from linear regression to a nonlinear regression
    neural network, we can make the same walk from logistic regression to a nonlinear
    classification network. Recall that logistic regression is a popular algorithm
    for classification problems that finds a linear solution to try to separate C
    classes.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们如何从线性回归走向非线性回归神经网络一样，我们也可以从逻辑回归走向非线性分类网络。回想一下，逻辑回归是分类问题中一个流行的算法，它找到一个线性解决方案来尝试分离C个类别。
- en: 2.3.1  Classification toy problem
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 分类玩具问题
- en: 'Before we can build a logistic model, we need a dataset. Getting our data loaded
    and into a `Dataset` object is always the first and most important step when using
    PyTorch. For this example, we use the `make_moons` class from scikit-learn:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们构建逻辑模型之前，我们需要一个数据集。在 PyTorch 中使用时，将数据加载并放入 `Dataset` 对象中始终是第一步和最重要的步骤。对于这个例子，我们使用
    scikit-learn 的 `make_moons` 类：
- en: '[PRE11]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/CH02_UN08_Raff.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH02_UN08_Raff.png)'
- en: The moons dataset has *d* = 2 input features now, and the scatter plot shows
    the two classes as circles and crosses. This is a good toy problem since a linear
    classification model can do an *OK* job of separating circles from crosses but
    can’t *perfectly* solve the problem.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 月亮数据集现在有 *d* = 2 个输入特征，散点图显示了两个类别作为圆圈和十字。这是一个很好的玩具问题，因为线性分类模型可以很好地将圆圈和十字分开，但不能
    *完美* 解决这个问题。
- en: To make our lives easier, we use the built-in `TensorDataset` object to wrap
    our current data. This only works if we can fit all of our data into RAM. But
    if you can, this is the easiest way to prepare data. You can use your favorite
    pandas or NumPy methods to load in the data and then start modeling.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的工作更简单，我们使用内置的 `TensorDataset` 对象来包装我们当前的数据。这仅在我们能够将所有数据放入 RAM 中时才有效。但如果你可以，这是准备数据的最简单方法。你可以使用你喜欢的
    pandas 或 NumPy 方法来加载数据，然后开始建模。
- en: 'We did make one important change, though. Our vector of labels y is now a `torch.long`
    rather than a `torch.float32`. Why? Because the labels are now classes, which
    start from 0 and go up to *C* − 1 to represent C different classes. There is no
    0.25 class; only integers are allowed! For this reason, we use the long data type
    (a 64-bit integer) rather than a floating-point value since we only need to concern
    ourselves with integers. For example, if our classes were `cat`, `bird`, and `car`,
    we would use `0, 1, 2` to represent the three classes. You may recognize this
    as being very close to a one-hot encoding, where each class is given its own dimension.
    PyTorch does that last step for us under the hood to avoid wastefully representing
    all the non-present classes as one-hot encoding does:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们确实做了一项重要的更改。我们的标签向量 y 现在是 `torch.long` 类型，而不是 `torch.float32` 类型。为什么？因为标签现在代表类别，从
    0 开始，到 *C* − 1 结束，以表示 C 个不同的类别。没有 0.25 类；只允许整数！因此，我们使用长数据类型（64 位整数）而不是浮点值，因为我们只关心整数。例如，如果我们的类别是
    `猫`、`鸟` 和 `车`，我们将使用 `0, 1, 2` 来表示这三个类别。你可能认识这种表示方式与独热编码非常接近，其中每个类别都有自己的维度。PyTorch
    在幕后为我们完成最后一步，以避免像独热编码那样无谓地表示所有非存在的类别：
- en: '[PRE12]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we define a linear classification model just as we did previously. In this
    case, we have two features, and we have two outputs (one for each class), so our
    model is *slightly* bigger. Notice that even though the *target* vector y is represented
    as a single integer, the network has C explicit outputs. This is because the labels
    are *absolute*: there is only one true class per data point. The network, however,
    must always consider all C classes as potential options and make a prediction
    for each class separately:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义一个线性分类模型，就像我们之前做的那样。在这种情况下，我们有两个特征，我们有两个输出（每个类别一个），所以我们的模型稍微大一些。请注意，尽管
    *目标* 向量 y 被表示为一个单独的整数，但网络有 C 个显式输出。这是因为标签是 *绝对* 的：每个数据点只有一个真实类别。然而，网络必须始终考虑所有
    C 个类别作为潜在选项，并为每个类别分别做出预测：
- en: '[PRE13]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 2.3.2  Classification loss function
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2  分类损失函数
- en: The big question is, what do we use as our loss function? This was an easy question
    to answer when we did a regression problem. We had two inputs, and they were both
    floating-point values, so we could just subtract them to determine how far the
    two values were from each other.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的问题是，我们使用什么作为损失函数？当我们做回归问题时，这个问题很容易回答。我们有两个输入，它们都是浮点值，所以我们只需相减就可以确定两个值之间的距离。
- en: 'This situation is different, though. Now our prediction is **ŷ** ∈ ℝ^C because
    we need a prediction for each of our C different classes. But our labels are one
    value of a set of integers, *y* ∈ {0, 1, …, *C* − 1}. If we can define a loss
    function ℓ(**ŷ**, *y*) that takes in a vector of predictions **ŷ** ∈ ℝ^C and compare
    it to a correct class y, we can reuse everything from the training loop in figure
    2.2 and our previously defined neural network. Luckily this function already exists,
    but it’s worth talking about in detail because of how foundational it is to everything
    we do in this book. We need two components: the softmax function and the cross-entropy,
    which combined are often just called the *cross-entropy loss*.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种情况是不同的。现在我们的预测是 **ŷ** ∈ ℝ^C，因为我们需要对我们的 C 个不同类别中的每一个进行预测。但我们的标签是整数集的一个值，*y*
    ∈ {0, 1, …, *C* − 1}。如果我们能定义一个损失函数 ℓ(**ŷ**, *y*)，它接受一个预测向量 **ŷ** ∈ ℝ^C 并将其与正确的类别
    y 进行比较，我们就可以重用图 2.2 中的训练循环以及我们之前定义的神经网络中的所有内容。幸运的是，这个函数已经存在，但由于它在本书的整个过程中都至关重要，因此详细讨论它是值得的。我们需要两个组件：softmax
    函数和交叉熵，这两个函数结合在一起通常被称为 *交叉熵损失*。
- en: While many tutorials are happy to say “use cross-entropy” without explaining
    what it is, we will take a slightly involved detour and walk through the mechanics
    of cross-entropy so you can build a stronger mental foundation. The essence is
    to first convert some set of C scores (the values can be any number) into C probabilities
    (values must be between 0 and 1), and then calculate a loss based on the probability
    of the true class y. There are many statistical arguments for why it is done this
    way, and it’s OK if you need to read this section more than once or come back
    to it later.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然许多教程都乐于说“使用交叉熵”而不解释它是什么，但我们将稍微绕个弯，并解释交叉熵的机制，这样你就可以建立一个更强的心理基础。本质上是首先将一些 C
    个分数（这些值可以是任何数字）转换成 C 个概率（值必须在 0 和 1 之间），然后根据真实类别 y 的概率计算损失。有许多统计论据支持为什么这样做，如果你需要多次阅读这一节或稍后回来，那也是可以的。
- en: Softmax
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax
- en: 'First, we intuitively want the dimension in ŷ that has the largest value to
    correspond to the correct class label y (the same as if we used `np.argmax`).
    If we write this in math, we want:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们直观地希望 ŷ 中具有最大值的维度对应于正确的类别标签 y（这与使用 `np.argmax` 相同）。如果我们用数学表达，我们希望：
- en: '![](../Images/ch2-eqs-to-illustrator4x.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ch2-eqs-to-illustrator4x.png)'
- en: We also want our predictions to be a sensible probability. Why? Suppose the
    correct class is *y* = *k* and we succeed and have *ŷ*[k] as the largest value.
    How right or wrong is this? What if *ŷ*[k] − *ŷ*[j] = 0.00001? The difference
    is very small, and we want a way to tell the model that it should make the difference
    larger.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也希望我们的预测是一个合理的概率。为什么？假设正确的类别是 *y* = *k*，我们成功并且有 *ŷ*[k] 作为最大的值。这是对还是错？如果 *ŷ*[k]
    − *ŷ*[j] = 0.00001 呢？这个差异非常小，我们希望有一种方法告诉模型应该使差异更大。
- en: If we make ŷ into probabilities, they have to sum up to 1\. That means
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将 ŷ 转换为概率，它们必须总和为 1。这意味着
- en: '![](../Images/ch2-eqs-to-illustrator6x.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ch2-eqs-to-illustrator6x.png)'
- en: This way we know a model is confident in its prediction when *ŷ*[k] = 1, and
    all other values of *j* ≠ *k* result in *ŷ*[j] = 0. If the model was less confident,
    we might see *ŷ*[k] = 0.9; and if it was completely wrong, *ŷ*[k] = 0. The constraint
    that ŷ sums to 1 makes it easy for us to interpret the results.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这样我们就可以知道，当 *ŷ*[k] = 1 时，模型对其预测有信心，而所有其他值 *j* ≠ *k* 的结果都是 *ŷ*[j] = 0。如果模型不太自信，我们可能会看到
    *ŷ*[k] = 0.9；如果它完全错误，*ŷ*[k] = 0。ŷ 总和为 1 的约束使得我们很容易解释结果。
- en: 'But how do we ensure this? The values we get from the last `nn.Linear` layer
    could be anything, especially when we first start training and have not yet taught
    the model about what is correct. We will use the *soft maximum* (or *softmax*)
    function: it converts everything to a non-negative number and ensures that the
    values sum up to 1.0\. The index k with the largest value will have the largest
    value afterward as well, *even if it is negative*, as long as every other index
    is an even smaller number. Smaller values also receive smaller but nonzero values.
    So softmax gives every value in 0, 1, …, *C* − 1 a value in the range [0,1], such
    that they all sum to 1.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何确保这一点呢？我们从最后一个 `nn.Linear` 层得到的值可以是任何东西，尤其是在我们刚开始训练并且还没有教会模型什么是正确的时候。我们将使用
    *软最大值*（或 *softmax*）函数：它将所有东西转换成非负数，并确保这些值的总和为 1.0。具有最大值的索引 k 在之后也将具有最大的值，*即使它是负数*，只要其他所有索引的值都更小。较小的值也会得到较小的但非零的值。因此，softmax
    给 0, 1, …, *C* − 1 中的每个值分配一个 [0,1] 范围内的值，这样它们的总和为 1。
- en: 'The following equation defines the softmax function, which I’m abbreviating
    in math as “sm” to make longer equations easier to read:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方程定义了softmax函数，我在数学中将其简称为“sm”，以便使更长的方程更容易阅读：
- en: '![](../Images/CH02_UN09_Raff.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_UN09_Raff.png)'
- en: 'Let’s quickly look at the results of calling softmax on two different vectors:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下对两个不同向量调用softmax的结果：
- en: '![](../Images/ch2-eqs-to-illustrator8x.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/ch2-eqs-to-illustrator8x.png)'
- en: '![](../Images/ch2-eqs-to-illustrator10x.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/ch2-eqs-to-illustrator10x.png)'
- en: In the first case, 4 is the largest value, so it receives the largest normalized
    score of 0.705\. The second case (–1) is the largest value, and it receives a
    score of 0.844\. Why does the second case result in a larger score even though
    –1 is smaller than 4? Because softmax is relative, 4 is only 1 larger than 3;
    the second case (–1) is 3 larger than –4, and because the difference is bigger,
    this case receives a bigger score.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况下，4是最大值，因此它获得了最大的归一化分数0.705。第二种情况（-1）是最大值，它获得了0.844的分数。为什么第二种情况的结果分数更大，尽管-1比4小？因为softmax是相对的，4只比3大1；第二种情况（-1）比-4大3，因为差异更大，所以这种情况获得了更高的分数。
- en: Why is it called softmax?
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么叫softmax？
- en: 'Before we continue, I find it helpful to explain *why* the softmax function
    is called *softmax*. We can use this score to compute a “soft” maximum, where
    every value contributes a portion of the answer. If we take the dot product between
    the softmax scores and the original values, it is approximately equal to the maximum
    value. Let’s look at how that happens:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我觉得解释一下为什么softmax函数被称为softmax是有帮助的。我们可以使用这个分数来计算一个“软”的最大值，其中每个值都贡献了答案的一部分。如果我们取softmax分数和原始值之间的点积，它大约等于最大值。让我们看看这是如何发生的：
- en: '![](../Images/CH02_F07_Raff_EQ07.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F07_Raff_EQ07.png)'
- en: The value of sm (**x**)^⊤**x** is approximately equal (≈) to finding the maximum
    value of x. Because every value contributes to at least a portion of the answer,
    it is also the case that sm (**x**)^⊤**x** ≤ max[i]*x*[i]. So the softmax function
    can get close but only becomes equal to the maximum when *all* values are the
    same.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: sm(x)^⊤x的值大约等于找到x的最大值。因为每个值至少贡献了答案的一部分，所以sm(x)^⊤x ≤ max[i]*x*[i]也成立。所以softmax函数可以接近最大值，但只有在所有值都相同的情况下才等于最大值。
- en: Cross-entropy
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵
- en: With the softmax function in hand, we have one of the two tools we need to define
    a good loss function for classification problems. The second tool we need is called
    the *cross-entropy* loss. If we have two probability distributions p and q, the
    cross-entropy between them is
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在手头有了softmax函数之后，我们拥有了定义分类问题良好损失函数所需的两个工具之一。我们需要的第二个工具被称为*交叉熵*损失。如果我们有两个概率分布p和q，它们之间的交叉熵是
- en: '![](../Images/CH02_UN10_Raff.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_UN10_Raff.png)'
- en: Why cross-entropy? It’s a statistical tool that tells us how much extra information
    it will take for us to encode information if we used the distribution defined
    by q when the data *actually* follows the distribution p. This is measured in
    bits (one-eighth of a byte), and the more bits it takes to encode something, the
    worse the fit between p and q. This explanation glosses over some of the precision
    of what the cross-entropy function is doing but gives you an intuitive idea at
    a high level. Cross-entropy boils down to telling us how different two distributions
    are.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么是交叉熵？这是一个统计工具，它告诉我们如果我们使用q定义的分布来编码信息时，需要多少额外的信息才能编码数据实际上遵循的分布p。这是以比特（一个字节的八分之一）来衡量的，用来编码某物所需的比特越多，p和q之间的拟合就越差。这个解释省略了交叉熵函数所做的一些精确性，但给你一个高层次上的直观理解。交叉熵归结为告诉我们两个分布有多不同。
- en: Think of it as trying to minimize cost. Imagine we are ordering lunch for a
    group of people, and we expect 70% to eat chicken and 30% to eat turkey (see figure
    2.7). That’s the predicted distribution q. In reality, 5% want turkey and 95%
    want chicken. That’s the true distribution p. In this scenario, we *think* we
    want to order more turkey than we actually need. But if we order all that turkey,
    we will be short on chicken and have wasted/unused turkey. If we knew what we
    actually needed, we wouldn’t have so many leftovers. Cross-entropy is just a way
    to quantify how different these distributions are so that we can order the right
    amount of each thing.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 想象这是在尝试最小化成本。想象我们正在为一组人订购午餐，我们预计70%的人会吃鸡肉，30%的人会吃火鸡（见图2.7）。这就是预测分布q。实际上，5%的人想要火鸡，95%的人想要鸡肉。这就是真实分布p。在这种情况下，我们*认为*我们想要订购比实际需要的更多的火鸡。但如果我们订购了所有的火鸡，我们就会缺少鸡肉，并且浪费了/未使用的火鸡。如果我们知道我们实际需要什么，我们就不会有这么多剩余的食物。交叉熵只是量化这些分布差异的一种方式，这样我们就可以订购正确数量的每种东西。
- en: '![](../Images/CH02_F08_Raff.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F08_Raff.png)'
- en: 'Figure 2.7 There are two different distributions: the real distribution on
    the left and our prediction on the right. To learn, we need a loss function to
    tell us precisely how different these distributions are. Cross-entropy solves
    that problem. In most cases, our label defines reality as being 100% for one class
    and 0% for all the others.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7显示了两种不同的分布：左侧的实际情况和右侧的我们的预测。为了学习，我们需要一个损失函数来精确地告诉我们这两个分布有多不同。交叉熵解决了这个问题。在大多数情况下，我们的标签将现实定义为100%属于一个类别，而其他所有类别为0%。
- en: Now, with these two tools combined, we arrive at a simple loss function and
    approach. We first apply the softmax function (sm (*x*)), followed by computing
    the cross-entropy. If ŷ is our vector output from the network and y is the correct
    class index, this simplifies to
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，结合这两个工具，我们得到了一个简单的损失函数和方法。我们首先应用softmax函数（sm(*x*)），然后计算交叉熵。如果ŷ是我们从网络输出的向量，而y是正确的类别索引，这简化为
- en: '![](../Images/CH02_UN11_Raff.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_UN11_Raff.png)'
- en: This may seem a bit mysterious, and that’s OK. The result comes from simplifying
    the equations, and derivations are not the point of this book. We worked through
    the details because the softmax and cross-entropy functions are ubiquitous in
    deep learning research today, and some extra effort to understand what they do
    will make your life easier later in the book. The important thing is to know what
    a softmax function does (normalized inputs into probabilities) and that it can
    be used with cross-entropy to quantify how different two distributions (arrays
    of probabilities) are.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来有点神秘，但没关系。结果是简化方程式得来的，推导不是这本书的重点。我们详细地解释了这些，因为softmax和交叉熵函数在今天的深度学习研究中无处不在，多花点力气了解它们的作用会在书后的学习中使你的生活更容易。重要的是要知道softmax函数的作用（将归一化输入转换为概率）以及它可以与交叉熵一起使用来量化两个分布（概率数组）之间的差异。
- en: We use this loss function because it has a strong statistical grounding and
    interpretation. It ensures that we can interpret the results as being a probability
    distribution. For the case of a linear model, it results in the well-known algorithm
    *logistic regression*.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用这个损失函数，因为它有很强的统计基础和解释性。它确保我们可以将结果解释为概率分布。对于线性模型的情况，它导致众所周知的算法**逻辑回归**。
- en: Using softmax followed by cross-entropy is so standard and well known that PyTorch
    integrates them into a single loss function `CrossEntropyLoss`, which performs
    both steps for us. This is good because implementing the softmax and cross-entropy
    functions manually can lead to tricky numerical stability issues and is not as
    direct as you might think.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 使用softmax后跟交叉熵是如此标准和众所周知，以至于PyTorch将它们集成到一个单一的损失函数`CrossEntropyLoss`中，它为我们执行这两个步骤。这是好的，因为手动实现softmax和交叉熵函数可能会导致棘手的数值稳定性问题，并且不如你想象的那么直接。
- en: 2.3.3  Training a classification network
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 训练分类网络
- en: 'Now we can train a `model` and see how well it performs:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以训练一个`模型`并查看它的表现如何：
- en: '[PRE14]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'With our model trained, let’s visualize the results. Since this is a 2D function,
    it’s a little more complicated than our previous regression case. We use a contour
    plot to show the decision surface of our algorithm: dark blue represents the first
    class, dark red represents the second class, and the color transitions as the
    model’s confidence decreases and increases. The original data points are shown
    as their respective blue and orange markers:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型训练完成后，让我们可视化结果。由于这是一个二维函数，它比我们之前的回归案例要复杂一些。我们使用等高线图来显示算法的决策面：深蓝色代表第一类，深红色代表第二类，颜色过渡表示模型置信度的降低和增加。原始数据点以各自的蓝色和橙色标记显示：
- en: '[PRE15]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/CH02_UN12_Raff.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_UN12_Raff.png)'
- en: Note Notice that we call the PyTorch function `F.softmax` to perform the conversation
    from raw outputs into actual probability distributions. It is common jargon to
    call the value that goes into softmax the *logits* and the outputs ŷ the probabilities.
    We will avoid using the term *logits* too much in this book, but you should be
    familiar with it. It often comes up when people are discussing the nitty-gritty
    details of an implementation or approach.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：请注意，我们调用 PyTorch 函数 `F.softmax` 来将原始输出转换为实际的概率分布。通常将输入 softmax 的值称为 *logits*，而输出
    ŷ 称为概率。在这本书中，我们将尽量避免过多使用 *logits* 这个术语，但你应该熟悉它。它经常出现在人们讨论实现或方法细节的时候。
- en: 'We can now see the results of our model on this data. Overall, it’s a decent
    job: most of the blue circles are in the blue region, and the red crosses are
    in the red region. There is a middle ground where errors are being made because
    our problem cannot be fully solved with a linear model.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以看到我们的模型在这组数据上的结果。总体来说，这是一项不错的工作：大多数蓝色圆圈都在蓝色区域，红色十字都在红色区域。有一个中间地带，错误正在发生，因为我们的问题不能完全用线性模型解决。
- en: 'Now we do the same we did with our regression problem: add a hidden layer to
    increase the complexity of the neural network. In this case, we add two hidden
    layers just to show how easy it is. I’ve arbitrarily selected *n* = 30 hidden
    units for both hidden layers:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对回归问题做同样的处理：添加一个隐藏层来增加神经网络的复杂性。在这种情况下，我们添加了两个隐藏层只是为了展示这是多么容易。我随意选择了 *n*
    = 30 个隐藏单元用于两个隐藏层：
- en: '[PRE16]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You should notice that these models are starting to take some time to train:
    250 `epochs` required 36 seconds when I ran this. The results appear to be worth
    it, though: if we look at a plot of our data, we see that the model has higher
    confidence for the regions that are unambiguously circles or crosses. You can
    also see that the threshold is starting to bend and curve as the neural network
    learns a nonlinear separation between the two classes:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该注意到这些模型开始需要一些时间来训练：当我运行这个模型时，250 个 `epochs` 需要 36 秒。不过，结果似乎值得等待：如果我们查看我们的数据图，我们会看到模型对于明显是圆形或十字的区域有更高的置信度。你还可以看到，随着神经网络学习两个类别之间的非线性分离，阈值开始弯曲和曲线：
- en: '[PRE17]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](../Images/CH02_UN13_Raff.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_UN13_Raff.png)'
- en: 2.4 Better training code
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 更好的训练代码
- en: 'We’ve now successfully trained fully connected networks for regression and
    classification problems. There is still a lot of room to improve our approaches.
    In particular, we have been training and evaluating visually on the same data.
    *This is not OK*: you can never judge how well your model works on *new* data
    by testing it on the *training* data. That gives the model a chance to cheat by
    memorizing the answers for each training datum instead of learning the underlying
    task. We also have another issue when dealing with classification problems: minimizing
    the cross-entropy loss is not really our goal. Our goal is to minimize errors,
    but we can’t define errors in a differentiable way that will work with PyTorch,
    so we used cross-entropy as a proxy metric instead. Reporting the loss after every
    epoch for a classification problem is not as helpful because it is not our true
    goal.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经成功训练了用于回归和分类问题的全连接网络。我们还有很多改进方法的空间。特别是，我们一直在同一数据集上训练和评估。*这并不好*：你永远不能通过在
    *训练* 数据上测试来判断你的模型在 *新* 数据上的表现如何。这给了模型一个机会通过记住每个训练数据点的答案来作弊，而不是学习底层任务。当我们处理分类问题时，还有一个问题：最小化交叉熵损失并不是我们的真正目标。我们的目标是减少错误，但我们不能以
    PyTorch 可以处理的方式对错误进行可微分的定义，所以我们使用了交叉熵作为代理指标。在分类问题中，每经过一个 epoch 就报告损失并不那么有帮助，因为这不是我们的真正目标。
- en: We will talk about a number of changes we can make to our training code to give
    us much more robust tools. Like all good ML practitioners, we are making and using
    a *training* set and a *testing* set. We will also evaluate other metrics that
    we care about so we can track performance as we train.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论我们可以对训练代码进行的一些修改，以获得更强大的工具。像所有优秀的机器学习从业者一样，我们正在创建和使用一个**训练**集和一个**测试**集。我们还将评估我们关心的其他指标，以便在训练过程中跟踪性能。
- en: 2.4.1  Custom metrics
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1  自定义指标
- en: 'As mentioned, the metrics we care about (e.g., accuracy) may not be the same
    as the loss we used to train our model (e.g., cross-entropy). There are many ways
    these may *not* match perfectly, because a loss function must have the property
    of being *differentiable*, and most of the time our true goal does not have this
    property. So we often have two sets of scores: the metrics by which the developers
    and humans understand the problem and the loss function that lets the network
    understand the problem.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们关注的指标（例如，准确率）可能与我们用于训练模型的损失（例如，交叉熵）不同。这些指标可能不完全匹配的许多方式，因为损失函数必须具有**可微分的**属性，而我们的真正目标大多数时候并不具备这个属性。因此，我们通常会有两套评分：开发者和人理解问题的指标以及让网络理解问题的损失函数。
- en: 'To help with this issue, we will modify our code so that we can pass in functions
    to compute different metrics from the labels and predicted values. We also want
    to know how these metrics vary across our training and validation datasets, so
    we will record multiple versions: one for each type of dataset.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助解决这个问题，我们将修改我们的代码，以便我们可以传递函数来从标签和预测值计算不同的指标。我们还想知道这些指标在训练和验证数据集上的变化情况，因此我们将记录多个版本：每个数据集类型一个。
- en: To make our lives easier, we will make our code work well with most of the metrics
    provided by the scikit-learn library ([https://scikit-learn.org/stable/modules/classes.html](https://scikit-learn.org/stable/modules/classes.html)).
    To do this, let’s assume we have an array `y_true` that contains the correct output
    label for every data point. We also need another array `y_pred` that contains
    the prediction of our model. If we are doing regression, each prediction is a
    scalar ŷ. If classification, each prediction is a vector ŷ.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的工作更轻松，我们将使我们的代码与scikit-learn库提供的多数指标兼容（[https://scikit-learn.org/stable/modules/classes.html](https://scikit-learn.org/stable/modules/classes.html)）。为此，让我们假设我们有一个数组`y_true`，它包含每个数据点的正确输出标签。我们还需要另一个数组`y_pred`，它包含我们模型的预测。如果我们进行回归，每个预测都是一个标量ŷ。如果是分类，每个预测都是一个向量ŷ。
- en: We need a way for the user (that’s you) to specify which functions to evaluate
    and a place to store the results. For the score functions, let’s use a dictionary
    `score_funcs` that takes the name of the metric as the key and a function reference
    as the value. That looks like
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一种方式让用户（也就是你）指定要评估的函数以及存储结果的地方。对于评分函数，我们可以使用一个字典`score_funcs`，它以指标的名称作为键，以函数引用作为值。这看起来像
- en: '[PRE18]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'if we use the functions provided by scikit-learn’s `metrics` class (see [https://scikit-learn.org/stable/modules/model_evaluation.html](https://scikit-learn.org/stable/modules/model_evaluation.html)).
    This way, we can specify as many custom metrics as we want, as long as we implement
    a function `score_func(y_ture, y_pred)`. Then we just need a place to store the
    computed scores. After each epoch of the loop, we can use another dictionary `results`
    that maps strings as keys to a `list` of results. We’ll use a list so we have
    one score for each epoch:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用scikit-learn的`metrics`类提供的函数（见[https://scikit-learn.org/stable/modules/model_evaluation.html](https://scikit-learn.org/stable/modules/model_evaluation.html)）。这样，我们可以指定我们想要的任何自定义指标，只要我们实现一个`score_func(y_ture,
    y_pred)`函数。然后我们只需要一个地方来存储计算出的评分。在每个循环的每个epoch之后，我们可以使用另一个字典`results`，它将字符串作为键映射到一个`list`的结果。我们将使用列表，以便每个epoch有一个评分：
- en: '[PRE19]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If we just used the `name` of each score function, we would not be able to differentiate
    between the score on the training set and the testing set. This is important because
    if there is a wide gap, it could indicate overfitting, and a small gap could indicate
    underfitting. So we use a `prefix` to distinguish between `train` and `test` scores.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只使用每个评分函数的`name`，我们就无法区分训练集和测试集上的评分。这很重要，因为如果存在很大的差距，可能表明过拟合，而小的差距可能表明欠拟合。因此，我们使用一个`prefix`来区分`train`和`test`评分。
- en: Note If we are being proper in our evaluations, we should use only the validation/test
    performance to make adjustments and changes to our code, hyper-parameters, network
    architecture, etc. This is another reason we need to make sure we are distinguishing
    between training and validation performance. You should *never* use training performance
    to decide how well a model is doing.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果我们对评估很认真，我们应该只使用验证/测试性能来调整和更改我们的代码、超参数、网络架构等。这也是我们需要确保我们区分训练和验证性能的另一个原因。你不应该*永远*使用训练性能来决定模型的表现如何。
- en: 2.4.2  Training and testing passes
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 训练和测试遍历
- en: We are modifying our training function to better support real-life work. That
    includes supporting a training epoch where we alter the model weights and a testing
    epoch where we only record our performance. It’s essential that we make the testing
    epoch *never* adjust the weights of the model.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在修改我们的训练函数，以更好地支持现实生活中的工作。这包括支持一个训练epoch，其中我们改变模型权重，以及一个测试epoch，其中我们只记录我们的性能。确保测试epoch*永远*不调整模型权重是至关重要的。
- en: 'Performing one epoch of training or evaluation requires a *lot* of different
    inputs:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 执行一次训练或评估需要大量的不同输入：
- en: '`model`—The PyTorch `Module` to run for one epoch, which represents our model
    *f*(⋅)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`—运行一个epoch的PyTorch `Module`，代表我们的模型 *f*(⋅)'
- en: '`optimizer`—The object that updates the weights of the network and should be
    used only if we are performing a training epoch'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer`—更新网络权重的对象，只有在执行训练epoch时才应使用'
- en: '`data_loader`—The `DataLoader` object, which returns tuples of (input, label)
    pairs'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_loader`—`DataLoader`对象，它返回（输入，标签）对的元组'
- en: '`loss_func`—The loss function ℓ(⋅,⋅), which takes two arguments, the `model`
    outputs (**ŷ** = *f*(**x**) and the `labels` (y), and returns a loss to use for
    training'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss_func`—损失函数 ℓ(⋅,⋅)，它接受两个参数，即`model`的输出（**ŷ** = *f*(**x**)）和标签（y），并返回用于训练的损失'
- en: '`device`—The compute location to perform training'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`—执行训练的计算位置'
- en: '`results`—A dictionary of strings to lists for storing results, as described
    previously'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`results`—一个字符串到列表的字典，用于存储结果，如前所述'
- en: '`score_funcs`—A dictionary of scoring functions to use to evaluate the performance
    of the `model`, as described previously'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score_funcs`—一个用于评估`model`性能的评分函数字典，如前所述'
- en: '`prefix`—A string prefix for any scores placed in the `results` dictionary'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prefix`—放置在`results`字典中的任何分数的字符串前缀'
- en: 'Last, because neural networks can take a while to train, let’s include an optional
    argument `desc` to provide a descriptive string for a progress bar. That will
    give us all the inputs we need for a function that processes one epoch, which
    we can give the following signature:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于神经网络训练可能需要一段时间，让我们包括一个可选参数`desc`，为进度条提供一个描述性字符串。这将为我们提供一个函数所需的所有输入，该函数处理一个epoch，我们可以给出以下签名：
- en: '[PRE20]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'At the start of this function, we need to allocate space to store results such
    as the losses, predictions, and the time we start computing:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在此函数的开始，我们需要分配空间来存储结果，如损失、预测和开始计算的时间：
- en: '[PRE21]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The training loop looks almost identical to the one we have used so far. The
    only thing we need to change is whether we use the optimizer. We can check this
    by looking at the `model.training` flag, which is `True` if our model is in training
    mode (`model = model.train()`) or `False` if it’s in evaluation/inference mode
    (`model = model.eval()`). We can wrap the `backward()` call on the loss and `optimizer`
    calls into an `if` statement at the end of each loop:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环看起来几乎与迄今为止我们所使用的循环完全相同。唯一需要改变的是是否使用优化器。我们可以通过查看`model.training`标志来检查这一点，如果我们的模型处于训练模式（`model
    = model.train()`）则该标志为`True`，如果处于评估/推理模式（`model = model.eval()`）则该标志为`False`。我们可以在每个循环的末尾将损失和优化器调用包裹在一个`if`语句中：
- en: '[PRE22]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, we need to store the `labels` and predictions `y_hat` into `y_true`
    and `y_pred`, respectively. This can be done by calling `.detach().cpu().numpy()`
    to convert both from PyTorch tensors into NumPy arrays. Then we simply `extend`
    the lists of all labels by the current labels we are processing:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要将`labels`和预测`y_hat`分别存储到`y_true`和`y_pred`中。这可以通过调用`.detach().cpu().numpy()`来完成，将两者都从PyTorch张量转换为NumPy数组。然后我们只需将当前正在处理的标签列表扩展到所有标签列表中：
- en: '[PRE23]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Moves labels and predictions back to CPU for later
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将标签和预测移回CPU以供后续使用
- en: ❷ Adds to predictions so far
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将预测结果累加到迄今为止的预测中
- en: 2.4.3  Saving checkpoints
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.3 保存检查点
- en: 'The last modification we will make is the ability to save a simple checkpoint
    of the most recently completed epoch. In PyTorch, the `torch.load` and `torch.save`
    functions can be used for this purpose. While there is more than one way to use
    these methods, we recommend using the dictionary-style approach shown here, which
    lets us save the model, the optimizer state, and other information, all in one
    object:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要做的最后一个修改是保存最近完成epoch的简单检查点。在PyTorch中，可以使用`torch.load`和`torch.save`函数来完成此目的。虽然使用这些方法的方式不止一种，但我们建议使用这里所示的字典式方法，它允许我们将模型、优化器状态和其他信息都保存在一个对象中：
- en: '[PRE24]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The second argument `checkpoint_file` is a path to where we should save the
    file. We can put any picklable object into this dictionary to be saved. In our
    case, we denote the number of training epochs, the `model` state (the weights
    /parameters Θ), and any state used by the `optimizer`.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数`checkpoint_file`是我们应该保存文件的路径。我们可以将任何可序列化的对象放入此字典以保存。在我们的情况下，我们表示训练epoch的数量、`model`状态（权重/参数
    Θ）以及`optimizer`使用的任何状态。
- en: We need to be able to save our model so that when we are ready to use it, we
    do not have to retrain it from scratch. Saving after every epoch is a better idea,
    especially when you start to train networks that can take weeks to complete. Sometimes
    our code may fail after many epochs or a power failure may interrupt our job.
    By saving the model after every epoch, we can resume our training from the last
    epoch rather than starting from scratch.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要能够保存我们的模型，这样当我们准备好使用它时，我们就不需要从头开始重新训练。在每个epoch后保存是一个更好的主意，尤其是当你开始训练可能需要几周才能完成的网络时。有时我们的代码可能在许多epoch后失败，或者电力故障可能会中断我们的工作。通过在每个epoch后保存模型，我们可以从最后一个epoch恢复训练，而不是从头开始。
- en: '2.4.4  Putting it all together: A better model training function'
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.4  将所有内容整合：更好的模型训练函数
- en: 'Now we have everything to build a better function for training our neural networks:
    not only the networks we have talked about (e.g., fully connected) but almost
    all the networks we discuss in this book. The signature for this new function
    looks like this:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了构建更好的神经网络训练函数所需的一切：不仅是我们讨论过的网络（例如，全连接）几乎本书中讨论的所有网络。这个新函数的签名看起来是这样的：
- en: '[PRE25]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The arguments are as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 参数如下：
- en: '`model`—The PyTorch `Module` to run for one epoch, which represents our model
    *f*(⋅)'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`—运行一个epoch的PyTorch `Module`，它代表我们的模型 *f*(⋅)'
- en: '`loss_func`—The loss function ℓ(⋅,⋅), which takes two arguments, the `model`
    outputs (**ŷ** = *f*(**x**) and the `labels` (y), and returns a loss to use for
    training'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss_func`—损失函数ℓ(⋅,⋅)，它接受两个参数，即`model`的输出(**ŷ** = *f*(**x**)和`labels` (y)，并返回用于训练的损失'
- en: '`train_loader`—The `DataLoader` object that returns tuples of (input, label)
    pairs used for training the model'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_loader`—返回用于训练模型的(input, label)对的`DataLoader`对象'
- en: '`test_loader`—The `DataLoader` object that returns tuples of (input, label)
    pairs used for evaluating the model'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_loader`—返回用于评估模型的(input, label)对的`DataLoader`对象'
- en: '`score_funcs`—A dictionary of scoring functions to use to evaluate the performance
    of the `model`, as described earlier'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score_funcs`—一个用于评估`model`性能的评分函数字典，如前所述'
- en: '`device`—The compute location to perform training'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`—执行训练的计算位置'
- en: '`checkpoint_file`—A string indicating the location to save model checkpoints
    to disk'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`checkpoint_file`—一个字符串，指示保存模型检查点到磁盘的位置'
- en: 'The gist of this new function is shown next, and you can find the full version
    in the idlmam.py file that comes with the book:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新功能的核心内容如下所示，完整的版本可以在书中提供的idlmam.py文件中找到：
- en: '[PRE26]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Performs bookkeeping and setup; prepares the optimizer
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 执行账务和设置；准备优化器
- en: ❶ Places the model on the correct compute resource (CPU or GPU)
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将模型放置在正确的计算资源（CPU或GPU）上
- en: ❶ Puts our model in training mode
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将我们的模型置于训练模式
- en: ❶ Saves a checkpoint if checkpoint_file is not None
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果checkpoint_file不为空，则保存检查点
- en: We use the `run_epoch` function to perform a training step after putting the
    model into the correct mode, and that function records the results from training.
    Then, if `test_loader` is given, we switch to `model.eval()` mode and enter the
    `with``torch.no_grad()` context so we do not alter the model in any way and can
    examine its performance on the held-out data. We use the prefixes `"train"` and
    `"test"` for the results from the training and testing runs, respectively.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`run_epoch`函数在将模型置于正确模式后执行训练步骤，该函数记录训练结果。然后，如果提供了`test_loader`，我们切换到`model.eval()`模式并进入`with
    torch.no_grad()`上下文，这样我们就不会以任何方式修改模型，并可以检查其在保留数据上的性能。我们分别使用前缀`"train"`和`"test"`来表示训练和测试运行的结果。
- en: 'Finally, we have this new training function convert the results into a pandas
    `DataFrame`, which will make it easy for us to access and view them later:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有一个新的训练函数将结果转换为pandas `DataFrame`，这将使我们以后更容易访问和查看它们：
- en: '[PRE27]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'With this new and improved code, let’s retrain our model on the moons dataset.
    Since accuracy is what we really care about, we import the accuracy metrics from
    scikit-learn. Let’s include the F1 score metrics to demonstrate how the code can
    handle two different metrics at the same time:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个新的改进代码，让我们在月亮数据集上重新训练我们的模型。由于准确率是我们真正关心的，我们导入scikit-learn中的准确度度量。让我们包括F1分数度量来展示代码如何同时处理两个不同的度量：
- en: '[PRE28]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We also want to do a better job of evaluating and including a validation set.
    Since the moons data is synthetic, we can easily create a new dataset for validation.
    Rather than performing 200 epochs of training like before, let’s generate a larger
    training set:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望更好地评估并包括一个验证集。由于月亮数据是合成的，我们可以轻松创建一个新的验证数据集。与其像以前那样进行200个训练周期，让我们生成一个更大的训练集：
- en: '[PRE29]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We have everything we need to train our model again. We will use model.pt as
    the location to save our model’s results. All we need to do is declare a new `model`
    object and call our new `train_simple_network` function:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了重新训练模型所需的一切。我们将使用model.pt作为保存模型结果的存储位置。我们所需做的只是声明一个新的`model`对象并调用我们的新`train_simple_network`函数：
- en: '[PRE30]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Time to look at some results. First, let’s see that we can load our checkpoint
    `model` rather than use the one we already trained. To load a `model`, we first
    need to define a *new* `model` that has all the same sub-modules as the original,
    and they all need to be the same size. This is necessary so the weights all match
    up. If we saved a model with 30 neurons in the second hidden layer, we need to
    have a new model with 30 neurons; otherwise, there will be too few or too many,
    and an error will occur.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候看看一些结果了。首先，让我们看看我们是否可以加载我们的检查点`model`而不是使用我们已训练的模型。要加载一个`model`，我们首先需要定义一个*新*的`model`，它具有与原始模型相同的所有子模块，并且它们都需要相同的大小。这是必要的，以确保所有权重都匹配。如果我们保存了一个具有30个神经元的第二隐藏层的模型，我们需要有一个具有30个神经元的新的模型；否则，将会有太多或太少，并且会发生错误。
- en: 'One reason we use the `torch.load` and `torch.save` functions is the `map_location`
    argument that they provide. This handles loading a model from the data to the
    correct compute device for us. Once we load in the dictionary of results, we can
    use the `load_state_dict` function to restore the states of our original model
    into this new object. Then we can apply the model to the data and see that we
    get the same results:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`torch.load`和`torch.save`函数的一个原因是因为它们提供的`map_location`参数。这为我们处理从数据到正确计算设备加载模型提供了便利。一旦我们加载了结果字典，我们就可以使用`load_state_dict`函数将原始模型的状态恢复到这个新对象中。然后我们可以将模型应用于数据，并看到我们得到了相同的结果：
- en: '[PRE31]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![](../Images/CH02_UN14_Raff.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_UN14_Raff.png)'
- en: You can easily see that the initial model does not give very good predictions
    because its weights are random values and untrained. If you run the code several
    times, you should see many slightly different but equally unhelpful results. But
    after we load the previous `model` state into the `model_new`, we get the nice
    crisp results we expect.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以很容易地看出，初始模型并不给出很好的预测，因为它的权重是随机值且未经过训练。如果你运行几次代码，你应该会看到许多略有不同但同样无用的结果。但当我们把之前的`model`状态加载到`model_new`中时，我们得到了我们预期的清晰结果。
- en: Note In this example, we only load the model’s state because we only made predictions;
    you don’t need the optimizer unless you want to continue training. If you do want
    to continue training, you’ll want to add a `optimizer.load_state_dict (checkpoint[‘optimizer_state_dict)`
    line to your code.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在这个例子中，我们只加载了模型的州因为只做了预测；除非你想继续训练，否则不需要优化器。如果你想继续训练，你需要在你的代码中添加一行`optimizer.load_state_dict(checkpoint['optimizer_state_dict'])`。
- en: 'Our new training function was written to return a pandas `DataFrame` object
    with information about the model after every epoch. This gives us some valuable
    information that we can easily visualize. For example, we can quickly plot the
    training and validation accuracy as a function of the finished epoch:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新训练函数被编写为在每次周期结束后返回一个包含模型信息的pandas `DataFrame`对象。这为我们提供了一些有价值的信息，我们可以轻松地可视化。例如，我们可以快速绘制训练和验证准确率作为完成周期的函数：
- en: '[PRE32]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![](../Images/CH02_UN16_Raff.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_UN16_Raff.png)'
- en: 'It’s now easy to see that by using more data, it took about two epochs for
    our model to top out on the noisier training data. Two score functions were provided,
    so let’s look at the F1 score as a function of the literal amount of training
    time in seconds. This will become more useful in the future if we want to compare
    how quickly two different models can learn:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在很容易看出，通过使用更多的数据，我们的模型在大约两个周期内达到了噪声训练数据的顶峰。提供了两个分数函数，所以让我们看看F1分数作为实际训练时间（秒）的函数。如果我们想要比较两个不同的模型学习速度的快慢，这将在未来更有用：
- en: '[PRE33]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '![](../Images/CH02_UN17_Raff.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_UN17_Raff.png)'
- en: For this toy dataset, F1 and Accuracy give very similar scores because both
    classes have similar behavior and are balanced in size. A more interesting trend
    you should notice is that the training accuracy increases and then stabilizes,
    but the validation accuracy has more asperity as it moves up and down with each
    epoch of training. *This is normal*. The model will start to overfit the training
    data, which makes its performance look stable, and slowly inch upward as it finishes
    learning and starts to memorize the more challenging data points. Because the
    validation data is separate, these small changes that may be good or bad for new
    data are unknown to the model, so it cannot adjust to get them consistently correct.
    It’s essential that we keep a separate validation or test set so that we can see
    this less biased view of how the model will actually perform on new data.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个玩具数据集，F1分数和准确率给出了非常相似的分数，因为这两个类别具有相似的行为，并且在大小上平衡。你应该注意到的更有趣的趋势是，训练准确率在上升后趋于稳定，但验证准确率随着每个训练周期的上下波动而更加崎岖。*这是正常的*。模型将开始过度拟合训练数据，这使得其性能看起来稳定，并在完成学习并开始记忆更具有挑战性的数据点时缓慢上升。由于验证数据是分开的，这些可能对新的数据有益或有害的小变化对模型来说是未知的，因此它无法调整以使它们始终正确。我们保留一个单独的验证或测试集是至关重要的，这样我们就可以看到模型在新的数据上实际表现的较少偏差的视图。
- en: 2.5 Training in batches
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 批量训练
- en: If you look at the x-axis of the previous graph, when we plot the F1 score as
    a function of training time, you may notice that it took almost a minute to train
    a model on just 8,000 data points with only *d* = 2 features. Given this long
    training time, how could we ever hope to scale up to larger datasets?
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看上一张图的x轴，当我们把F1分数作为训练时间的函数来绘制时，你可能注意到，只用*d* = 2个特征，在仅仅8,000个数据点上训练一个模型几乎需要一分钟。鉴于这个漫长的训练时间，我们如何才能扩大到更大的数据集呢？
- en: 'We need to train on *batches* of data. A batch of data is just a larger group
    of data. Let’s say we have the following dataset of *N* = 4 items:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在数据*批次*上进行训练。数据批次只是数据的一个更大的组。假设我们有以下数据集，包含*N* = 4个项目：
- en: '![](../Images/CH02_F07_Raff_EQ09.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F07_Raff_EQ09.png)'
- en: 'Our current code, over one epoch, will perform four updates: one for each item
    in the dataset. This is why it is called *stochastic gradient descent* (SGD).
    The word *stochastic* is jargon that means “random” but usually with some underlying
    purpose or invisible hand driving the randomness. The stochastic part of the name
    SGD comes from us using only a portion of the shuffled data to compute the gradient
    instead of the entire dataset. Because it is shuffled, we get a different result
    every time.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前的代码，在一个周期内，将执行四次更新：针对数据集中的每个项目。这就是为什么它被称为*随机梯度下降*（SGD）。*随机*这个词是行话，意思是“随机的”，但通常有一些潜在的目的或无形的手在驱动这种随机性。SGD名称中的随机部分来自于我们只使用部分洗牌后的数据来计算梯度，而不是整个数据集。由于它是洗牌的，我们每次都会得到不同的结果。
- en: 'If we push all N data points through the model and compute the loss over the
    entire dataset, ∇ Σ*[i]^N*[=1] ℓ(*f*(***x**[i]*), *y[i]*), we get the *true* gradient.
    This can also make our training more *computationally efficient* by processing
    all of the data at once instead of one datum at a time. So instead of passing
    in a vector with a shape of (*d*) as the input to a `model` *f*(⋅), we pass in
    a matrix of shape (*N*,*d*). PyTorch modules are designed for this situation by
    default; we just need a way to tell PyTorch to group our data into a larger batch.
    It turns out the `DataLoader` has this functionality built in with the optional
    `batch_size` argument. If a value is unspecified, it defaults to `batch_size=1`.
    If we set this to `batch_size=len(train_dataset)`, we perform true gradient descent:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将所有N个数据点通过模型，并在整个数据集上计算损失，即 ∇ Σ*[i]^N*[=1] ℓ(*f*(***x**[i]*), *y[i]*)，我们得到的是*真实*梯度。这也可以通过一次性处理所有数据而不是逐个处理数据点来使我们的训练更加*计算效率高*。因此，我们不是将形状为(*d*)的向量作为输入传递给`model`
    *f*(⋅)，而是传递一个形状为(*N*,*d*)的矩阵。PyTorch模块默认为此情况设计；我们只需要一种方法来告诉PyTorch将我们的数据分组到更大的批次中。结果证明，`DataLoader`具有内置的此功能，通过可选的`batch_size`参数。如果没有指定值，则默认为`batch_size=1`。如果我们将其设置为`batch_size=len(train_dataset)`，我们执行真正的梯度下降：
- en: '[PRE34]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Five epochs of training just happened in 0.536 seconds. Clearly, training on
    *more* data at one time has allowed us to benefit from the parallelism available
    in a modern GPU. But if we plot the accuracy, we see that training the gradient
    descent (*B*=*N*) has produced a less accurate model:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 五个训练周期仅用了0.536秒。显然，一次性在*更多*数据上训练使我们能够从现代GPU提供的并行性中受益。但如果我们绘制准确率，我们会看到训练梯度下降(*B*=*N*)产生了一个不太准确的模型：
- en: '[PRE35]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](../Images/CH02_UN19_Raff.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_UN19_Raff.png)'
- en: Let’s look at a toy example to explain why this is happening. Figure 2.8 shows
    a function that we are optimizing, and if we are using gradient descent (which
    looks at *all* the data), we take steps leading us in the correct direction. But
    each step is expensive, so we can only take a few steps. This example shows us
    taking four total updates/steps corresponding to four epochs.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个玩具示例来解释为什么会发生这种情况。图2.8显示了一个我们正在优化的函数，如果我们使用梯度下降（它查看*所有*数据），我们采取的步骤将引导我们走向正确的方向。但是每一步都很昂贵，所以我们只能采取少数几步。这个例子显示我们总共进行了四次更新/步骤，对应于四个周期。
- en: '![](../Images/CH02_F09_Raff.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F09_Raff.png)'
- en: Figure 2.8 The left image shows gradient descent over four epochs of the data.
    That means it can make only four steps of progress, but each step is headed in
    the right direction. On the right, SGD makes multiple updates per epoch by looking
    at just some of the data. This means each step is noisy and not always in the
    correct direction, but it is usually in a useful direction, so SGD can often make
    more progress toward the goal in fewer epochs.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 左边的图像显示了数据四个周期的梯度下降。这意味着它只能前进四步，但每一步都是正确的方向。在右边，SGD通过仅查看一些数据在每个周期进行多次更新。这意味着每一步都是嘈杂的，并不总是指向正确的方向，但它通常是朝着有用的方向，因此SGD通常可以在更少的周期内朝着目标取得更多进展。
- en: When we use SGD, we perform N updates per epoch, so we get more updates or steps
    for a fixed number of epochs. But because of the *stochastic* or random behavior
    of using just one data point for each update, the steps we take are noisy. They
    don’t always head in the correct direction. The larger total number of steps eventually
    gets us closer to the answer, but the cost is an increase in runtime because we
    lose the computational efficiency of processing all the data at once.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用SGD时，我们每个周期执行N次更新，因此我们得到更多更新或步骤，对于固定数量的周期。但是，由于使用每个更新仅一个数据点的*随机*或随机行为，我们采取的步骤是嘈杂的。它们并不总是指向正确的方向。更大的总步骤数最终使我们更接近答案，但代价是运行时间的增加，因为我们失去了同时处理所有数据的计算效率。
- en: The solution we use in practice is to balance between these two extremes. Let’s
    choose a batch size big enough to use the GPU more efficiently but small enough
    that we still get to perform many more updates per epoch. We use B to denote the
    batch size; for most applications, you will find *B* ∈ [32,256] is a good choice.
    Another good rule of thumb is to make the batch size as large as you can fit into
    GPU memory and to add more training epochs until the model converges. This requires
    a bit more work because as you develop your network and make changes, the largest
    batch size you can fit onto your GPU may change.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在实践中使用的解决方案是在这两个极端之间取得平衡。让我们选择一个足够大的批量大小，以便更有效地使用GPU，但又要足够小，以便我们仍然能够在每个epoch中进行更多的更新。我们用B表示批量大小；对于大多数应用，你会发现*B*
    ∈ [32,256]是一个不错的选择。另一个好的经验法则是使批量大小尽可能大，以便可以放入GPU内存，并添加更多的训练epoch，直到模型收敛。这需要做更多的工作，因为随着你开发你的网络并做出改变，你可以放入GPU的最大批量大小可能会改变。
- en: Note Because we only use the validation data to *evaluate* how our model is
    doing and not to update the weights of the model, the batch size used to validate
    the data has no particular tradeoff. We can just increase the batch size to whatever
    runs fastest and go with that. The results will be the same regardless of the
    batch size used for the test data. In practice, most people use the same batch
    size for training and testing data for simplicity.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：因为我们只使用验证数据来*评估*我们的模型表现，而不是更新模型的权重，所以用于验证数据的批量大小没有特定的权衡。我们只需增加批量大小到运行最快的程度即可。无论测试数据使用的批量大小如何，结果都将相同。在实践中，大多数人为了简单起见，会使用相同的批量大小进行训练和测试数据。
- en: 'Here’s the code:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是代码：
- en: '[PRE36]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now, if we plot the results as a function of time, we see the green line giving
    us the best of both worlds. It runs in only 1.044 seconds and gets nearly the
    same accuracy. You will find that using batches of data like this has almost no
    downside and is the preferred approach in modern deep learning.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们把结果作为时间的函数来绘制，我们会看到绿色的线给出了两者之间的最佳结果。它只运行了1.044秒，并且几乎达到了相同的准确度。你会发现使用这样的数据批量几乎没有缺点，并且在现代深度学习中是首选的方法。
- en: '[PRE37]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](../Images/CH02_UN20_Raff.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_UN20_Raff.png)'
- en: Exercises
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Share and discuss your solutions on the Manning online platform at Inside Deep
    Learning Exercises ([https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945)).
    Once you submit your own answers, you will be able to see the solutions submitted
    by other readers, and see which ones the author judges to be the best.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在Manning在线平台Inside Deep Learning Exercises上分享和讨论你的解决方案([https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945))。一旦你提交了自己的答案，你将能够看到其他读者提交的解决方案，并看到哪些是作者认为最好的。
- en: The input range of data can have a large impact on a neural network. This applies
    to inputs *and* outputs, as for regression problems. Try applying scikit-learn’s
    `StandardScaler` to the targets y of the toy regression problem at the start of
    this chapter, and train a new neural network on it. Does changing the scale of
    the outputs help or hurt the model’s predictions?
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据的输入范围可以对神经网络产生重大影响。这适用于输入*和*输出，例如回归问题。尝试将scikit-learn的`StandardScaler`应用于本章开始时的玩具回归问题的目标y，并在其上训练一个新的神经网络。改变输出的尺度是否有助于或损害模型的预测？
- en: The area under the curve (AUC) metric does not follow the standard pattern in
    scikit-learn, as it requires `y_pred` to be a vector of shape (*N*) instead a
    matrix of shape (*N*,2). Write a wrapper function for AUC that makes it compatible
    with our `train_simple_network` function.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 曲线下方的面积（AUC）指标在scikit-learn中并不遵循标准模式，因为它需要`y_pred`是一个形状为(*N*)的向量，而不是形状为(*N*,2)的矩阵。为AUC编写一个包装函数，使其与我们的`train_simple_network`函数兼容。
- en: Write a new function `resume_simple_network` that loads a `checkpoint_file`
    from disk, restores both the `optimizer` and `model` states, and continues training
    to a specified total number of epochs. So if the model is saved after 20 epochs,
    and you specify 30 epochs, it should perform only 10 more epochs of training.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个新的函数`resume_simple_network`，从磁盘加载`checkpoint_file`，恢复`optimizer`和`model`的状态，并继续训练到指定的总epoch数。所以如果模型在20个epoch后保存，而你指定了30个epoch，它应该只进行10个额外的epoch训练。
- en: When performing experiments, we may want to go back and try versions of our
    model from different epochs, especially if we are trying to determine when some
    weird behavior started to occur. Modify the `train_simple_network` function to
    take a new argument `checkpoint_every_x` that saves a version of the model every
    `x` epochs with different filenames. That way, you can go back and load a specific
    version without filling your hard drive with a model for *every* epoch.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在进行实验时，我们可能希望回到之前的不同时代的模型版本，特别是如果我们试图确定某些奇怪行为开始出现的时间。修改`train_simple_network`函数，添加一个新参数`checkpoint_every_x`，该参数在每个`x`个时期保存一个不同文件名的模型版本。这样，你可以回退并加载特定版本，而不会用每个时期的模型填满你的硬盘驱动器。
- en: The *deep* part of deep learning refers to the number of layers in a neural
    network. Try adding more layers (up to 20) to the models we used for the `make_moons`
    classification problem. How do more layers impact performance?
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 深度学习的“深度”部分指的是神经网络中的层数。尝试向用于`make_moons`分类问题的模型中添加更多层（最多20层）。更多的层如何影响性能？
- en: Try changing the number of neurons used in the hidden layers of the `make_moons`
    classification problem. How does it impact performance?
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试更改`make_moons`分类问题中隐藏层中使用的神经元数量。它如何影响性能？
- en: Use scikit-learn to load the breast cancer Wisconsin dataset ([https://scikit-learn.org/
    stable/modules/generated/sklearn.datasets.load_breast_cancer.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html)),
    convert it into a `TensorDataset`, and then split it into 80% for training and
    20% for testing. Try to build your own classification neural network for this
    data.
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用scikit-learn加载威斯康星乳腺癌数据集([https://scikit-learn.org/ stable/modules/generated/sklearn.datasets.load_breast_cancer.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html))，将其转换为`TensorDataset`，然后将其分为80%用于训练和20%用于测试。尝试为这些数据构建自己的分类神经网络。
- en: We saw results on the `make_moons` dataset with a batch size of *B* = {1, 32,
    *N*}. Write a loop to train a new model on that same dataset for every power-of-two
    batch size less than N (i.e., *B* = {2, 4, 8, 16, 32, 64, …}, and plot the results.
    Do you notice any trends in terms of accuracy and/or training time?
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在`make_moons`数据集上，以批大小*B* = {1, 32, *N*}看到了结果。编写一个循环，在相同的数据集上为小于N的每个2的幂次批大小训练一个新的模型（即*B*
    = {2, 4, 8, 16, 32, 64, …}），并绘制结果。你注意到准确性和/或训练时间方面有任何趋势吗？
- en: Summary
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The `train_simple_network` function abstracts away details and can be reused
    for almost any neural network.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_simple_network`函数抽象了细节，可以用于几乎任何神经网络。'
- en: '`CrossEntropyLoss` is for classification problems, and `MSE` and `L1` losses
    are for regression problems.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CrossEntropyLoss`用于分类问题，而`MSE`和`L1`损失用于回归问题。'
- en: The `nn.Linear` layer can be used to implement linear and logistic regression.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nn.Linear`层可以用来实现线性回归和逻辑回归。'
- en: Fully connected networks can be seen as extensions to linear and logistic regression
    by adding more `nn.Linear` layers with *nonlinearities* inserted between each
    layer.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全连接的网络可以通过添加更多带有*非线性*的`nn.Linear`层，被视为线性回归和逻辑回归的扩展。
- en: The `nn.Sequential` `Module` can be used to organize sub-`Module`s to create
    larger networks.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用`nn.Sequential` `Module`来组织子`Module`以创建更大的网络。
- en: We can trade compute efficiency versus number of optimization steps by using
    the `batch_size` option in a `DataLoader`.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在`DataLoader`中使用`batch_size`选项，我们可以通过计算效率与优化步数之间的权衡。
- en: '* * *'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '¹ A common convention in some ML research: when you have a larger function
    to optimize that is composed of the total loss over many smaller items, denote
    the larger function as F and the inner function as f. This is not hard and fast,
    but I like using common notations so you get familiar with them—even if they aren’t
    very informative.[↩](#fnref1)'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 一些机器学习研究中的常见约定：当你有一个较大的函数要优化，该函数由许多较小的项的总损失组成时，将较大的函数表示为F，将内部函数表示为f。这并不是一成不变的，但我喜欢使用常见的符号，这样你就可以熟悉它们——即使它们并不非常具有信息量。[↩](#fnref1)
- en: ² You could have all your data in a giant array, but that is a bad practice
    because you have your entire dataset in memory at all times. Iterators can load
    the data on the fly, which avoids excess memory usage. This is critical when you
    work with datasets larger than your computer memory.[↩](#fnref2)
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ² 你可以将所有数据放在一个巨大的数组中，但这是一种不好的做法，因为你在任何时候都将整个数据集保留在内存中。迭代器可以动态加载数据，这避免了额外的内存使用。当你处理比计算机内存大的数据集时，这是至关重要的。[↩](#fnref2)
- en: ³ There is some nice math that better justifies this, but we are not going to
    go there.[↩](#fnref3)
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ³ 有一些很好的数学可以更好地证明这一点，但我们不会深入探讨。[↩](#fnref3)
- en: ⁴ The size is different from that in the figures because I need 10 neurons to
    get interesting results, but 10 is way too many to draw in a picture. It’s also
    easier to plot functions with one input instead of three inputs. But I want the
    figure to show you that all inputs connect to all items in the next layer. A necessary
    inconsistency, and I apologize.[↩](#fnref4)
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴ 尺寸与图中所示不同，因为为了得到有趣的结果我需要10个神经元，但10个神经元在图中绘制过多。而且，用单个输入而不是三个输入来绘制函数也更容易。但我希望这个图能展示所有输入都连接到下一层的所有项目。这是一个必要的矛盾，对此我表示歉意。[↩](#fnref4)
