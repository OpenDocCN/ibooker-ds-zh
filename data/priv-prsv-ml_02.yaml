- en: 1 Privacy considerations in machine learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 机器学习中的隐私考虑
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The importance of privacy protection in the era of big data artificial intelligence
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大数据人工智能时代隐私保护的重要性
- en: Types of privacy-related threats, vulnerabilities, and attacks in machine learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习中的隐私相关威胁、漏洞和攻击类型
- en: Techniques that can be utilized in machine learning tasks to minimize or evade
    privacy risks and attacks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在机器学习任务中利用的技术，以最小化或规避隐私风险和攻击
- en: Our search queries, browsing history, purchase transactions, watched videos,
    and movie preferences are a few types of information that are collected and stored
    daily. Advances in artificial intelligence have increased the ability to capitalize
    on and benefit from the collection of private data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的搜索查询、浏览历史、购买交易、观看视频和电影偏好是每天收集和存储的一些信息类型。人工智能的进步增加了利用和从收集的私人数据中获益的能力。
- en: This data collection happens within our mobile devices and computers, on the
    streets, and even in our own offices and homes, and the data is used by a variety
    of machine learning (ML) applications in different domains, such as marketing,
    insurance, financial services, mobility, social networks, and healthcare. For
    instance, more and more cloud-based data-driven ML applications are being developed
    by different service providers (who can be classified as the *data users*, such
    as Facebook, LinkedIn, and Google). Most of these applications leverage the vast
    amount of data collected from each individual (the *data owner*) to offer users
    valuable services. These services often give users some commercial or political
    advantage by facilitating various user recommendations, activity recognition,
    health monitoring, targeted advertising, or even election predictions. However,
    on the flip side, the same data could be repurposed to infer sensitive (private)
    information, which would jeopardize the privacy of individuals. Moreover, with
    the increased popularity of Machine Learning as a Service (MLaaS), where cloud-based
    ML and computing resources are bundled together to provide efficient analytical
    platforms (such as Microsoft Azure Machine Learning Studio, AWS Machine Learning,
    and Google Cloud Machine Learning Engine), it is necessary to take measures to
    enforce privacy on those services before they are used with sensitive datasets.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据收集发生在我们的移动设备和计算机上，在街道上，甚至在我们的办公室和家中，这些数据被不同领域的各种机器学习（ML）应用使用，例如市场营销、保险、金融服务、移动性、社交网络和医疗保健。例如，越来越多的基于云的数据驱动ML应用正在由不同的服务提供商（可以归类为*数据用户*，如Facebook、LinkedIn和Google）开发。这些应用中的大多数都利用从每个个人（*数据所有者*）收集的大量数据为用户提供有价值的服务。这些服务通常通过促进各种用户推荐、活动识别、健康监测、定向广告甚至选举预测，使用户获得一些商业或政治优势。然而，另一方面，相同的数据可能被重新用于推断敏感（私人）信息，这可能会危及个人的隐私。此外，随着机器学习即服务（MLaaS）的日益普及，其中基于云的ML和计算资源捆绑在一起，以提供高效的分析平台（如Microsoft
    Azure Machine Learning Studio、AWS Machine Learning和Google Cloud Machine Learning
    Engine），在它们与敏感数据集一起使用之前，有必要采取措施来强制执行这些服务的隐私。
- en: 1.1 Privacy complications in the AI era
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 人工智能时代的隐私问题
- en: Let’s first visit a real-world example of private data leakage to visualize
    the problem. During the Facebook-Cambridge Analytica scandal in April 2018, data
    from about 87 million Facebook users was collected by a Facebook quiz app (a cloud-based
    data-driven application) and then paired with information taken from those users’
    social media profiles (including their gender, age, relationship status, location,
    and “likes”) without any privacy-preserving operations being taken other than
    anonymization. How did this happen?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先来看一个真实世界的私人数据泄露的例子，以便可视化这个问题。在2018年4月的Facebook-Cambridge Analytica丑闻中，大约8700万Facebook用户的资料被一个Facebook测验应用（一个基于云的数据驱动应用）收集，然后与从这些用户的社交媒体资料中获取的信息（包括他们的性别、年龄、关系状况、位置和“点赞”）配对，除了匿名化之外，没有采取任何保护隐私的操作。这是怎么发生的？
- en: The quiz, called “This Is Your Digital Life,” was originally created by Aleksandr
    Kogan, a Russian psychology professor at the University of Cambridge. The quiz
    was designed to collect personality information, and around 270,000 people were
    paid to take part. However, in addition to what the quiz was created to collect,
    it also pulled data from the participants’ friends’ profiles, making a large data
    pile. Later, Kogan shared this information in a commercial partnership with Cambridge
    Analytica, which harvested personal information from this dataset, such as where
    users lived and what pages they liked, eventually helping Cambridge Analytica
    to build psychological profiles and infer certain sensitive information about
    each individual, such as their identity, sexual orientation, and marital status,
    as summarized in figure 1.1.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这个名为“这是你的数字生活”的测验最初由剑桥大学的俄罗斯心理学教授亚历山大·科甘创建。这个测验旨在收集个性信息，大约有27万人为此获得了报酬。然而，除了测验旨在收集的信息之外，它还从参与者的朋友资料中提取了数据，形成了一个庞大的数据集。后来，科甘与剑桥分析公司达成商业合作，剑桥分析公司从这些数据集中提取了个人信息，例如用户的居住地和他们喜欢的页面，最终帮助剑桥分析公司构建心理档案，并推断出每个个体的某些敏感信息，如图1.1所示。
- en: '![CH01_F01_Zhuang](../../OEBPS/Images/CH01_F01_Zhuang.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F01_Zhuang](../../OEBPS/Images/CH01_F01_Zhuang.png)'
- en: Figure 1.1 The Facebook-Cambridge Analytica scandal raised the alarm about privacy
    concerns.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 Facebook-Cambridge Analytica丑闻引起了人们对隐私问题的警觉。
- en: That was just one incident! In 2020, after another privacy scandal, Facebook
    agreed to pay $550 million to settle a class-action lawsuit over its use of ML-based
    facial recognition technology, which again raised questions about the social network’s
    data-mining practices. The suit said the company violated an Illinois biometric
    privacy law by harvesting facial data for tag suggestions from the photos of millions
    of users in the state without their permission and without telling them how long
    the data would be kept. Eventually, Facebook disabled the tag-suggestion feature
    amid privacy concerns.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 那只是一个事件！2020年，在另一起隐私丑闻之后，Facebook同意支付5.5亿美元来解决因公司使用基于机器学习的面部识别技术而引发的集体诉讼。这再次引发了人们对社交网络数据挖掘实践的质疑。诉讼称，该公司未经用户同意，也未告知他们数据将保留多长时间，就收集了该州数百万用户的照片中的面部数据，用于标签建议，违反了伊利诺伊州的生物识别隐私法。最终，Facebook在隐私问题担忧之下禁用了标签建议功能。
- en: These unprecedented data leak scandals raised the alarm about privacy concerns
    in cloud-based data-driven applications. People began to think twice before submitting
    any data to cloud-based services. Thus, data privacy has become a hotter topic
    than ever before among academic researchers and technology companies, which have
    put enormous efforts into developing privacy-preserving techniques to prevent
    private data leaks. For instance, Google developed Randomized Aggregatable Privacy-Preserving
    Ordinal Response (RAPPOR), a technology for crowdsourcing statistics from end-user
    client software. Apple also claimed it first introduced its well-developed privacy
    techniques with iOS 11, for crowdsourcing statistics from iPhone users regarding
    emoji preferences and usage analysis. In 2021 Google introduced its privacy sandbox
    initiative to the Chrome web browser by replacing third-party cookies and putting
    boundaries around how advertising companies could interact with private data used
    in the browser. When people use the internet, publishers and advertisers want
    to provide content, including ads, that is relevant and interesting to them. People’s
    interests are often gauged on today’s web by observing what sites or pages they
    visit, relying on third-party cookies or less transparent and undesirable mechanisms
    like device fingerprinting. With this privacy sandbox initiative, Google introduced
    a new way to provide relevant content and ads by clustering large groups of people
    with similar browsing patterns, hiding individuals in the crowd and keeping their
    web histories on their browsers, without using third-party cookies.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些前所未有的数据泄露丑闻引起了人们对基于云的数据驱动应用程序隐私问题的警觉。人们在提交任何数据到云服务之前开始三思而后行。因此，数据隐私已经成为学术界和技术公司比以往任何时候都更热门的话题，它们投入了巨大的努力来开发隐私保护技术，以防止私人数据泄露。例如，谷歌开发了随机可聚合隐私保护序数响应（RAPPOR）技术，这是一种从终端用户客户端软件中收集统计数据的众包技术。苹果也声称它首先在iOS
    11中引入了其成熟的隐私技术，用于从iPhone用户那里收集表情符号偏好和使用分析统计数据。2021年，谷歌通过替换第三方cookies并在广告公司如何与浏览器中使用的私人数据互动方面设置边界，将其隐私沙盒倡议引入Chrome网络浏览器。当人们使用互联网时，出版商和广告商希望提供与他们相关且有趣的内容，包括广告。人们的兴趣通常通过观察他们访问的网站或页面来衡量，依赖于第三方cookies或更不透明且不受欢迎的机制，如设备指纹识别。通过这一隐私沙盒倡议，谷歌引入了一种新的方式，通过将具有相似浏览模式的大量人群进行聚类，隐藏个体在人群中的身份，并保留他们在浏览器中的网络历史记录，而不使用第三方cookies，来提供相关的内容和广告。
- en: 1.2 The threat of learning beyond the intended purpose
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 超出预期目的的学习威胁
- en: ML can be seen as the capability of an algorithm to mimic intelligent human
    behavior, performing complex tasks by looking at data from different angles and
    analyzing it across domains. This learning process is utilized by various applications
    in our day-to-day life, from product recommendation systems in online web portals
    to sophisticated intrusion detection mechanisms in internet security applications.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可以被视为算法模仿智能人类行为的能力，通过从不同角度观察数据并跨领域分析数据来执行复杂任务。这一学习过程被我们日常生活中的各种应用所利用，从在线门户网站的产品推荐系统到互联网安全应用中的复杂入侵检测机制。
- en: 1.2.1 Use of private data on the fly
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.1 动态使用私人数据
- en: ML applications require vast amounts of data from various sources to produce
    high confidence results. Web search queries, browsing histories, transaction histories
    of online purchases, movie preferences, and individual location check-ins are
    some of the information collected and stored on a daily basis, usually without
    users’ knowledge. Some of this information is private to the individuals, but
    it is uploaded to high-end centralized servers mostly in clear-text format so
    ML algorithms can extract patterns and build ML models from it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习应用需要从各种来源收集大量数据以产生高度可信的结果。网络搜索查询、浏览历史、在线购买交易历史、电影偏好以及个人位置签到是一些每天收集和存储的信息，通常在用户不知情的情况下进行。其中一些信息对个人来说是私密的，但它们通常以明文格式上传到高端集中式服务器，以便机器学习算法从中提取模式和构建机器学习模型。
- en: The problem is not limited to the collection of this private data by different
    ML applications. The data is also exposed to insider attacks because the information
    is available to workers at data-mining companies. For example, database administrators
    or application developers may have access to this data without many restrictions.
    This data may also be exposed to external hacking attacks, with private information
    being revealed to the outside world. For instance, in 2015 Twitter fired an engineer
    after intelligence officials found that he might have been spying on the accounts
    of Saudi dissidents by accessing user details such as phone numbers and IP addresses.
    According to the *New York Times*, the accounts belonged to security and privacy
    researchers, surveillance specialists, policy academics, and journalists. This
    incident is yet another example of how big the problem is. Most importantly, it
    is possible to extract additional information from private data even if it is
    transformed to a different embedding (anonymized), or if datasets and ML models
    are inaccessible and only the testing results are revealed.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 问题不仅限于不同机器学习应用收集这些私有数据。由于信息对数据挖掘公司的员工是可用的，数据还可能受到内部攻击。例如，数据库管理员或应用程序开发者可能在没有太多限制的情况下访问这些数据。这些数据还可能受到外部黑客攻击，私人信息可能泄露给外界。例如，2015年，Twitter解雇了一名工程师，因为情报官员发现他可能通过访问用户详情（如电话号码和IP地址）来监视沙特异议人士的账户。据《纽约时报》报道，这些账户属于安全和隐私研究人员、监控专家、政策学者和记者。这一事件是另一个例子，说明了问题的严重性。最重要的是，即使数据被转换为不同的嵌入（匿名化），或者数据集和机器学习模型不可访问，只透露测试结果，也可能从私有数据中提取更多信息。
- en: 1.2.2 How data is processed inside ML algorithms
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.2 机器学习算法内部如何处理数据
- en: To understand the relationship between data privacy and ML algorithms, knowing
    how ML systems work with the data they process is crucial. Typically, we can represent
    the input data for an ML algorithm (captured from various sources) as a set of
    sample values, and each sample can be a group of features. Let’s take the example
    of a facial recognition algorithm that recognizes people when they upload an image
    to Facebook. Consider an image of 100 × 100 pixels where a single value from 0
    to 255 represents each pixel. These pixels can be concatenated to form a feature
    vector. Each image can be represented to the ML algorithm as a vector of data,
    along with an associated label for that data. The ML algorithm would use multiple
    feature vectors and their associated labels during the training phase to produce
    an ML model. This model would then be used with fresh, unseen data (testing samples)
    to predict the result—in this case, to recognize a person.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解数据隐私与机器学习算法之间的关系，了解机器学习系统如何处理它们处理的数据至关重要。通常，我们可以将机器学习算法的输入数据（从各种来源捕获）表示为一组样本值，每个样本可以是一组特征。以一个面部识别算法为例，该算法在人们上传图像到Facebook时识别人员。考虑一个100
    × 100像素的图像，其中每个像素由0到255之间的单个值表示。这些像素可以连接起来形成一个特征向量。每个图像都可以表示为机器学习算法的数据向量，以及与该数据相关联的标签。在训练阶段，机器学习算法会使用多个特征向量和它们的相关标签来生成机器学习模型。然后，该模型将用于新鲜、未见的数据（测试样本）来预测结果——在这种情况下，识别一个人。
- en: Measuring the performance of ML models
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量机器学习模型的性能
- en: The ability of an ML model to accurately predict the final result is a measure
    of how well the ML model generalizes to unseen data or to data introduced for
    the first time. This accuracy is usually measured empirically, and it varies depending
    on factors such as the number of training samples, the algorithm used to build
    the model, the quality of the training samples, and the selection of hyperparameters
    for the ML algorithm. It is equally important to preprocess the raw data before
    feeding it to the ML models in some applications that use different mechanisms
    to extract the essential features from raw data. These mechanisms may involve
    various techniques (such as principal component analysis) to project data to a
    lower dimension.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型准确预测最终结果的能力是衡量该模型如何将泛化到未见数据或首次引入的数据的程度。这种准确性通常通过经验方法进行衡量，并且会根据训练样本数量、构建模型所使用的算法、训练样本的质量以及为机器学习算法选择的超参数等因素而变化。在某些使用不同机制从原始数据中提取基本特征的应用中，在将原始数据输入到机器学习模型之前进行预处理同样重要。这些机制可能涉及各种技术（如主成分分析），以将数据投影到较低维度。
- en: 1.2.3 Why privacy protection in ML is important
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.3 为什么在机器学习中保护隐私很重要
- en: When personal information is used for a wrong or unintended purpose, it can
    be manipulated to gain a competitive advantage. When massive volumes of personal
    records are coupled with ML algorithms, no one can predict what new results they
    may produce or how much private information those results may reveal. The Facebook-Cambridge
    Analytica scandal discussed earlier is a perfect example of the wrongful use of
    personal data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当个人信息被用于错误或不正当的目的时，它可以被操纵以获得竞争优势。当大量个人记录与机器学习算法相结合时，没有人能够预测它们可能会产生什么新的结果，或者这些结果可能会揭示多少私人信息。之前讨论过的Facebook-Cambridge
    Analytica丑闻就是个人数据被错误使用的完美例子。
- en: Hence, when designing ML algorithms for an application, ensuring privacy protection
    is vital. First, it ensures that other parties (data users) cannot use the personal
    data for their own advantage. Second, everyone has things that they do not want
    others to know. For example, they may not want others to know the details of their
    medical history. But ML applications are data driven, and we need training samples
    to build a model. We want to use the private data to build a model, but we want
    to prevent the ML algorithm from learning anything sensitive. How can we do that?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在设计应用于机器学习的算法时，确保隐私保护至关重要。首先，它确保其他方（数据使用者）不能利用个人数据为自己谋取利益。其次，每个人都有他们不想让别人知道的事情。例如，他们可能不希望别人知道他们医疗历史的细节。但机器学习应用是数据驱动的，我们需要训练样本来构建模型。我们希望使用私有数据来构建模型，但我们也希望防止机器学习算法学习到任何敏感信息。我们该如何做到这一点呢？
- en: 'Let’s consider a scenario where we use two databases: a sanitized medical database
    that lists patients’ histories of medication prescriptions, and another data source
    with user information and pharmacies visited. When these sources are linked together,
    the correlated database can have additional knowledge, such as which patient bought
    their medication from which pharmacy. Suppose we are using this correlated dataset
    with an ML application to extract relationships between the patients, medications,
    and pharmacies. While it will extract the obvious relations between different
    diseases and the medications prescribed, it may also learn roughly where the patient
    resides simply by referring to the zip codes of their most-visited pharmacies,
    even if the data does not contain patient addresses. This is a simple example,
    but you can imagine how severe the consequences could be if privacy is not protected.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个场景，在这个场景中我们使用了两个数据库：一个是列出患者用药历史的清洁医疗数据库，另一个是包含用户信息和访问过的药店的数据库。当这些来源被链接在一起时，相关的数据库可以拥有额外的知识，例如哪些患者从哪些药店购买了他们的药物。假设我们正在使用这个相关数据集与一个机器学习应用来提取患者、药物和药店之间的关系。虽然它将提取不同疾病和所开具药物之间的明显关系，但它也可能仅仅通过参考患者最常访问的药店的邮编，粗略地学习到患者的居住地，即使数据中不包含患者的地址。这是一个简单的例子，但你可以想象如果隐私没有得到保护，后果可能会多么严重。
- en: 1.2.4 Regulatory requirements and the utility vs. privacy tradeoff
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.4 监管要求与效用与隐私权衡
- en: Traditionally, data security and privacy requirements were set by the data owners
    (such as organizations) to safeguard the competitive advantage of the products
    and services they offered. However, in the big data era, data has become the most
    valuable asset in the digital economy, and governments imposed many privacy regulations
    to prevent the use of sensitive information beyond its intended purpose. Privacy
    standards such as HIPAA (Health Insurance Portability and Accountability Act of
    1996), PCI DSS (Payment Card Industry Data Security Standard), FERPA (Family Educational
    Rights and Privacy Act), and the European Union’s GDPR (General Data Protection
    Regulation) are some of the privacy regulations that organizations commonly adhere
    to. For example, regardless of the size of practice, most healthcare providers
    transmit health information electronically, such as for claims, medication records,
    benefit eligibility inquiries, referral authorization requests, and the like.
    However, HIPAA regulations require that these healthcare providers protect sensitive
    patient health information from being disclosed without the patient’s consent
    or knowledge.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，数据安全和隐私要求由数据所有者（如组织）设定，以保护他们提供的产品和服务的竞争优势。然而，在大数据时代，数据已成为数字经济中最有价值的资产，政府实施了众多隐私法规，以防止敏感信息被用于其预期目的之外。例如，像1996年的《健康保险可携带性和问责法案》（HIPAA）、《支付卡行业数据安全标准》（PCI
    DSS）、《家庭教育权利和隐私法案》（FERPA）以及欧盟的《通用数据保护条例》（GDPR）这样的隐私标准，是组织通常遵守的一些隐私法规。例如，无论实践规模大小，大多数医疗保健提供者都会通过电子方式传输健康信息，如索赔、药物记录、福利资格查询、转诊授权请求等。然而，HIPAA法规要求这些医疗保健提供者在没有患者同意或知情的情况下，保护敏感的患者健康信息不被泄露。
- en: Regardless of whether the data is labeled or not, or whether raw data is preprocessed,
    ML models are essentially very sophisticated statistics based on the training
    dataset, and ML algorithms are optimized to squeeze every bit of utility out of
    the data. Therefore, in most conditions, they are capable of learning sensitive
    attributes in the dataset, even when that is not the intended task. When we attempt
    to preserve privacy, we want to prevent these algorithms from learning sensitive
    attributes. Hence, as you can see, utility and privacy are on opposite ends of
    the spectrum. When you tighten privacy, it can affect the performance of the utility.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 无论数据是否标记，或原始数据是否预处理，机器学习模型本质上是基于训练数据集的非常复杂的统计模型，机器学习算法被优化以从数据中榨取每一丝效用。因此，在大多数情况下，它们能够学习数据集中的敏感属性，即使这不是预期的任务。当我们试图保护隐私时，我们希望防止这些算法学习敏感属性。因此，正如你所看到的，效用和隐私处于光谱的两端。当你加强隐私时，可能会影响效用的性能。
- en: The real challenge is balancing privacy and performance in ML applications so
    that we can better utilize the data while ensuring the privacy of the individuals.
    Because of the regulatory and application-specific requirements, we cannot degrade
    privacy protection just to increase the utility of the application. On the other
    hand, privacy has to be implemented systematically without using arbitrary mechanisms,
    as many additional threats must be considered. ML applications are prone to different
    privacy and security attacks. We will explore these potential attacks in detail
    next and look at how we can mitigate them by designing privacy-preserving ML (PPML)
    algorithms.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的挑战是在机器学习应用中平衡隐私和性能，以便我们更好地利用数据，同时确保个人的隐私。由于监管和特定应用的要求，我们不能仅仅为了提高应用的效用而降低隐私保护。另一方面，隐私必须系统地实施，而不使用任意机制，因为必须考虑许多额外的威胁。机器学习应用容易受到不同的隐私和安全攻击。我们将在下一节详细探讨这些潜在攻击，并探讨我们如何通过设计隐私保护机器学习（PPML）算法来减轻它们。
- en: 1.3 Threats and attacks for ML systems
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 机器学习系统的威胁和攻击
- en: We discussed a few privacy leakage incidents in the previous section, but many
    other threats and attacks on ML systems are being proposed and discussed in the
    literature and could potentially be deployed in real-world scenarios. For instance,
    figure 1.2 is a time-line showing a list of threats and attacks for ML systems,
    including de-anonymization (re-identification) attacks, reconstruction attacks,
    parameter inference attacks, model inversion attacks, and membership inference
    attacks. We will briefly explore the details of these threats or attacks in this
    section.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节讨论了一些隐私泄露事件，但文献中提出了许多其他针对机器学习系统的威胁和攻击，这些攻击可能被部署在实际场景中。例如，图1.2是一个显示机器学习系统威胁和攻击列表的时间线，包括去匿名化（重新识别）攻击、重建攻击、参数推断攻击、模型反演攻击和成员推断攻击。我们将简要探讨这些威胁或攻击的细节。
- en: '![CH01_F02_Zhuang](../../OEBPS/Images/CH01_F02_Zhuang.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F02_Zhuang](../../OEBPS/Images/CH01_F02_Zhuang.png)'
- en: Figure 1.2 A timeline of threats and attacks identified for ML systems
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 为机器学习系统识别的威胁和攻击的时间线
- en: Although some leading companies, such as Google and Apple, started designing
    and utilizing their own privacy-preserving methodologies for ML tasks, it is still
    a challenge to improve public awareness of these privacy technologies, mainly
    due to the lack of well-organized tutorials and books that explain the concepts
    methodically and systematically.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些领先的公司，如谷歌和苹果，开始设计和使用他们自己的用于机器学习任务的隐私保护方法，但提高公众对这些隐私技术的认识仍然是一个挑战，这主要是因为缺乏组织良好的教程和书籍，这些教程和书籍系统地解释了概念。
- en: 1.3.1 The problem of private data in the clear
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.1 明文中的私有数据问题
- en: Figure 1.3 illustrates a typical client/server application scenario. As you
    can see, when an application collects private information, that information is
    often transferred, possibly through encrypted channels, to cloud-based servers
    where the learning happens. In the figure, a mobile application connects to a
    cloud server to perform an inference task.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3说明了典型的客户端/服务器应用程序场景。如图所示，当应用程序收集私有信息时，该信息通常通过加密渠道传输，转移到云服务器上进行学习。在该图中，一个移动应用程序连接到云服务器以执行推理任务。
- en: '![CH01_F03_Zhuang](../../OEBPS/Images/CH01_F03_Zhuang.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F03_Zhuang](../../OEBPS/Images/CH01_F03_Zhuang.png)'
- en: Figure 1.3 The problem of storing private data in cleartext format
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 存储私有数据为明文格式的问题
- en: For example, a parking app may send the user’s location data to find a nearby
    available garage. Even though the communication channel is secured, the data most
    likely resides in the cloud in its original unencrypted form or as features extracted
    from the original record. This is one of the biggest challenges to privacy because
    that data is susceptible to various insider and outsider attacks.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个停车应用可能会将用户的位置数据发送以寻找附近的可用停车场。即使通信渠道是安全的，数据很可能会以原始未加密的形式或作为从原始记录中提取的特征存储在云中。这是对隐私的最大挑战之一，因为该数据容易受到各种内部和外部攻击。
- en: 1.3.2 Reconstruction attacks
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.2 重建攻击
- en: 'As you’ve seen, it is essential to store private data on the server in encrypted
    form, and we should not send raw data directly to the server in its original form.
    However, reconstruction attacks pose another possible threat: the attacker could
    reconstruct data even without having access to the complete set of raw data on
    the server. In this case, the adversary gains an advantage by having external
    knowledge of feature vectors (the data used to build the ML model).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，在服务器上以加密形式存储私有数据是至关重要的，我们不应该以原始形式直接将原始数据发送到服务器。然而，重建攻击提出了另一个可能的威胁：攻击者甚至在没有访问服务器上完整原始数据集的情况下重建数据。在这种情况下，对手通过对外部特征向量（用于构建机器学习模型的数据）的了解而获得优势。
- en: The adversary usually requires direct access to the ML models deployed on the
    server, which is referred to as white-box access (see table 1.1). They then try
    to reconstruct the raw private data by using their knowledge of the feature vectors
    in the model. These attacks are possible when the feature vectors used during
    the training phase to build the ML model are not flushed from the server after
    building the model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对手通常需要直接访问服务器上部署的机器学习模型，这被称为白盒访问（见表1.1）。然后他们试图通过使用对模型中特征向量的了解来重建原始的私有数据。当在训练阶段用于构建机器学习模型的特征向量在模型构建后未从服务器清除时，这些攻击是可能的。
- en: Table 1.1 Difference between white-box, black-box, and grey-box access
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 表1.1 白盒、黑盒和灰盒访问的区别
- en: '| White-box access | Black-box access | Grey-box access |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 白盒访问 | 黑盒访问 | 灰盒访问 |'
- en: '| Has full access to the internal details of the ML models, such as parameters
    and loss functions | Has no access to the internal details of the ML models |
    Has partial access to the internal details of the ML models |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 具有完全访问内部细节的ML模型，如参数和损失函数 | 没有访问内部细节的ML模型 | 具有部分访问内部细节的ML模型 |'
- en: 'How reconstruction works: An attacker’s perspective'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 重建工作原理：攻击者的视角
- en: 'Now that you’ve had a high-level overview of how a reconstruction attack works,
    let’s look into the details of how it is possible. The approach taken to reconstruct
    the data depends on what information (background knowledge) the attacker has available
    to reproduce the data accurately. We’ll consider the following two use case examples
    of biometric-based authentication systems:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对重建攻击的工作原理有了高级概述，让我们来看看它是如何可能的细节。重建数据的方法取决于攻击者可用以准确重现数据的背景知识（信息）。我们将考虑以下两个基于生物识别的认证系统的用例示例：
- en: '*Reconstruction of fingerprint images from a minutiae template*—Nowadays, fingerprint-based
    authentication is prevalent in many organizations: users are authenticated by
    comparing a newly acquired fingerprint image with a fingerprint image already
    saved in the user authentication system. In general, these fingerprint-matching
    systems use one of four different representation schemes known as *grayscale*,
    *skeleton*, *phase*, and *minutiae* (figure 1.4). Minutiae-based representation
    is the most widely adopted due to the compactness of the representation. Because
    of this compactness, many people erroneously think that the minutiae template
    does not contain enough information for attackers to reconstruct the original
    fingerprint image.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*指纹图像的细节点模板重建*——如今，基于指纹的认证在许多组织中都很普遍：用户通过将新获取的指纹图像与用户认证系统中已保存的指纹图像进行比较来进行认证。通常，这些指纹匹配系统使用四种不同的表示方案，称为*灰度*、*骨架*、*相位*和*细节点*（图1.4）。基于细节点的表示法由于表示的紧凑性而被广泛采用。正因为这种紧凑性，许多人错误地认为细节点模板不包含足够的信息供攻击者重建原始指纹图像。'
- en: In 2011, a team of researchers successfully demonstrated an attack that could
    reconstruct fingerprint images directly from minutiae templates [1]. They reconstructed
    a phase image from minutiae, which was then converted into the original (grayscale)
    image. Next, they launched an attack against fingerprint recognition systems to
    infer private data.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2011年，一个研究团队成功地演示了一种可以从细节点模板直接重建指纹图像的攻击[1]。他们从细节点重建了一个相位图像，然后将其转换为原始（灰度）图像。接下来，他们针对指纹识别系统发起攻击，以推断私人数据。
- en: '![CH01_F04_Zhuang](../../OEBPS/Images/CH01_F04_Zhuang.png)'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH01_F04_Zhuang](../../OEBPS/Images/CH01_F04_Zhuang.png)'
- en: 'Figure 1.4 The four different types of representation schemes used in fingerprint
    matching systems: (a) grayscale image, (b) skeleton image, (c) phase image, and
    (d) minutiae'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图1.4指纹匹配系统中使用的四种不同表示方案：（a）灰度图像，（b）骨架图像，（c）相位图像和（d）细节点
- en: '*Reconstruction attacks against mobile-based continuous authentication systems*—Al-Rubaie
    et al. investigated the possibility of reconstruction attacks that used gestural
    raw data from users’ authentication profiles in mobile-based continuous authentication
    systems [2]. Continuous authentication is a method of verifying the user not just
    once but continuously throughout an entire session (such as Face ID in iPhones).
    Without continuous authentication, organizations are more vulnerable to many attack
    vectors, such as a system being taken over when it is no longer being used but
    the session remains open. In such a scenario, a reconstruction attack could use
    available private information that is leaked to the adversary. At a high level,
    Al-Rubaie et al. used the feature vectors stored in user profiles to reconstruct
    the raw data and then used that information to hack into other systems.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*针对基于移动的连续认证系统的重建攻击*——Al-Rubaie等人调查了使用基于移动的连续认证系统中用户认证配置文件的手势原始数据进行的重建攻击的可能性[2]。连续认证是一种在整个会话期间（如iPhone中的Face
    ID）不断验证用户的方法，而不仅仅是验证一次。如果没有连续认证，组织更容易受到许多攻击向量的攻击，例如，当系统不再使用但会话仍然打开时，系统可能会被接管。在这种情况下，重建攻击可能会使用泄露给对手的可用私人信息。从高层次来看，Al-Rubaie等人使用存储在用户配置文件中的特征向量来重建原始数据，然后使用这些信息来入侵其他系统。'
- en: In most of these cases, the privacy threat resulted from a security threat to
    the authentication system in which reconstructed raw data misguided the ML system
    by forcing it to think that the raw data belonged to a specific user. For example,
    in the case of mobile-based continuous authentication systems, an attacker gained
    access to the mobile device and its personal records; hence, the authentication
    mechanism failed to protect the user’s privacy. Another class of reconstruction
    attack might reveal private information directly, as you’ll see next.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数这些情况下，隐私威胁源于对认证系统的安全威胁，重建的原始数据误导了机器学习系统，迫使它认为原始数据属于特定用户。例如，在基于移动的持续认证系统案例中，攻击者获得了对移动设备和其个人记录的访问权限；因此，认证机制未能保护用户的隐私。另一类重建攻击可能会直接揭示私人信息，正如你接下来将看到的。
- en: A real-world scenario involving reconstruction attacks
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 涉及重建攻击的真实场景
- en: In 2019, Simson Garfinkel and his team at the US Census Bureau presented a detailed
    example of how a reconstruction attack can be primed by an attacker, just utilizing
    data available to the public [3]. They further explained that publishing the frequency
    count, mean, and median age of a population, broken down by a few demographics,
    allows anyone with access to the statistics and a personal computer to accurately
    reconstruct the personal data of almost the entire survey population. This incident
    raised concerns about the privacy of census data. Based on this finding, the US
    Census Bureau conducted a series of experiments on 2010 census data. Among 8 billion
    statistics, 25 data points per person allowed successful reconstruction of confidential
    records for more than 40% of the US population.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在2019年，西蒙·加芬克尔及其在美国人口普查局的团队展示了一个详细的例子，说明了攻击者如何仅利用公开数据来启动重建攻击[3]。他们进一步解释说，发布按少数人口统计分组的人口频率计数、平均值和中位数年龄，允许任何拥有这些统计数据和个人电脑的人准确重建几乎整个调查人群的个人数据。这一事件引发了人们对人口普查数据隐私的关注。基于这一发现，美国人口普查局对2010年的人口普查数据进行了系列实验。在800亿条统计数据中，每人25个数据点允许成功重建超过40%的美国人口的机密记录。
- en: Even though this is not directly related to ML algorithms, you can probably
    understand how scary the problem is. The vast amount of sensitive data published
    by statistical agencies each year may provide a determined attacker with more
    than enough information to reconstruct some or all of the target database and
    breach the privacy of millions of people. The US Census Bureau has identified
    this risk and implemented the correct measures to protect the 2020 US Census,
    but it is important to note that reconstruction is no longer a theoretical danger.
    It is real.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这与机器学习算法没有直接关系，但你可能已经理解了这个问题有多么可怕。统计机构每年发布的庞大敏感数据量可能为决心攻击者提供足够的信息来重建某些或全部目标数据库，并侵犯数百万人的隐私。美国人口普查局已经识别了这一风险，并实施了正确的措施来保护2020年美国人口普查，但重要的是要注意，重建不再是一个理论上的危险。它是真实的。
- en: Now the question is, how can we prevent such attacks from succeeding? In terms
    of mitigating reconstruction attacks tailored for ML models, the best approach
    is to avoid storing explicit feature vectors inside the ML model. If the feature
    vectors are required to be stored (e.g., SVM requires feature vectors and metadata
    to be stored alongside the model), they should be inaccessible to the users of
    the ML model so that they are hard to reconstruct. Feature names can at least
    be anonymized. To mitigate reconstruction attacks targeting database or data mining
    operations (as in the US Census example), different and well-established data
    sanitization and disclosure-avoidance techniques can be used.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是，我们如何防止此类攻击成功？在减轻针对机器学习模型的重建攻击方面，最佳方法是避免在机器学习模型中存储显式特征向量。如果必须存储特征向量（例如，支持向量机需要存储特征向量和模型元数据），它们应该对机器学习模型的用户不可访问，以便难以重建。特征名称至少可以被匿名化。为了减轻针对数据库或数据挖掘操作（如美国人口普查案例）的重建攻击，可以使用不同的和已建立的数据净化和披露避免技术。
- en: This discussion has just been a summary of how reconstruction attacks work.
    We will discuss these techniques and other mitigation strategies in more detail
    in the forthcoming chapters.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这场讨论只是对重建攻击工作原理的总结。我们将在接下来的章节中更详细地讨论这些技术和其他缓解策略。
- en: 1.3.3 Model inversion attacks
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.3 模型反演攻击
- en: While some ML models store explicit feature vectors, other ML algorithms (such
    as neural networks and ridge regression) do not keep feature vectors inside the
    model. In these circumstances, the adversary’s knowledge is limited, but they
    may still have access to the ML model, as discussed in the white-box access scenario.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一些机器学习模型存储显式特征向量，但其他机器学习算法（如神经网络和岭回归）不保留模型内部的特征向量。在这些情况下，攻击者的知识有限，但他们可能仍然可以访问机器学习模型，正如在白盒访问场景中讨论的那样。
- en: 'In another black-box access scenario, the adversary does not have direct access
    to the ML model: they can listen for incoming requests to an ML model when a user
    submits new testing samples and for responses generated by the model. In model
    inversion attacks, the adversary utilizes the responses generated by the ML model
    in a way that resembles the original feature vectors used to create the ML model
    [4]. Figure 1.5 illustrates how a model inversion attack works.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一种黑盒访问场景中，攻击者没有直接访问机器学习模型：当用户提交新的测试样本时，他们可以监听对机器学习模型的请求，以及模型生成的响应。在模型反演攻击中，攻击者以类似于创建机器学习模型时使用的原始特征向量方式利用机器学习模型生成的响应[4]。图1.5展示了模型反演攻击的工作原理。
- en: '![CH01_F05_Zhuang](../../OEBPS/Images/CH01_F05_Zhuang.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F05_Zhuang](../../OEBPS/Images/CH01_F05_Zhuang.png)'
- en: Figure 1.5 The difference between white-box access and black-box access. White-box
    access requires direct access and permission for the ML model to infer data; black-box
    access usually involves listening to the communication channel.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 白盒访问和黑盒访问的区别。白盒访问需要直接访问和权限来推断数据；黑盒访问通常涉及监听通信通道。
- en: Typically, such attacks utilize the confidence values received from the ML model
    (such as the probability decision score) to generate feature vectors. For example,
    let’s consider a facial recognition algorithm. When you submit a face image to
    the algorithm, the algorithm produces a result vector with the class and the corresponding
    confidence score based on the features it identifies in that image. For now, assume
    the result vector generated by the algorithm is
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，此类攻击利用从机器学习模型收到的置信度值（如概率决策分数）来生成特征向量。例如，让我们考虑一个人脸识别算法。当你向算法提交人脸图像时，算法会根据它在图像中识别的特征生成一个包含类别和相应置信度分数的结果向量。现在，假设算法生成的结果向量是
- en: '[*John*:.99, *Simon*:.73, *Willey*:.65]'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[*John*:.99, *Simon*:.73, *Willey*:.65]'
- en: What is the meaning of this result? The algorithm is 99% confident (the decision
    score) that this image belongs to John (the class) and 73% confident that it belongs
    to Simon, and so on.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果的意义是什么？算法有99%的信心（决策分数）认为这张图像属于John（类别），有73%的信心认为它属于Simon，等等。
- en: What if the adversary can listen to all these communications? Even though they
    do not have the input image or know whose image it is, they can deduce that they
    will get a confidence score in this range if they input a similar image. By accumulating
    results over a certain period, the attacker can produce an average score representing
    a certain class in the ML model. If the class represents a single individual,
    as in a facial recognition algorithm, identifying the class could result in a
    threatening privacy breach. At the beginning of the attack, the adversary does
    not know who the person is, but by accumulating data over time, they will be able
    to identify the person, which is serious.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果攻击者可以监听所有这些通信呢？即使他们没有输入图像或不知道图像属于谁，他们也可以推断，如果输入一个相似的图像，他们将在这个范围内获得一个置信度分数。通过在一定时期内累积结果，攻击者可以生成代表机器学习模型中某个类别的平均分数。如果这个类别代表一个单独的个人，如在人脸识别算法中，识别这个类别可能导致隐私泄露的威胁。在攻击开始时，攻击者不知道这个人是谁，但随着时间的推移累积数据，他们将能够识别这个人，这是非常严重的。
- en: Therefore, the model inversion attack is a severe threat to ML-based systems.
    Note that in some cases, model inversion attacks can be classified as a subclass
    of reconstruction attacks, based on how well the features are arranged in raw
    data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，模型反演攻击对基于机器学习的系统构成了严重威胁。请注意，在某些情况下，模型反演攻击可以根据特征在原始数据中的排列方式被归类为重建攻击的一个子类。
- en: In mitigating model inversion attacks, limiting the adversary to black-box access
    is important, because it limits the adversary’s knowledge. In our example of face
    recognition-based authentication, instead of providing the exact confidence value
    of a certain ML class, we could round it. Alternatively, only the final predicted
    class label may be returned, so that it is harder for an adversary to learn anything
    beyond the specific prediction.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在缓解模型反演攻击时，限制攻击者仅限于黑盒访问很重要，因为它限制了攻击者的知识。在我们的基于人脸识别的认证示例中，我们不是提供某个机器学习类别的确切置信度值，而是将其四舍五入。或者，我们可以只返回最终的预测类别标签，这样攻击者就难以学习到特定预测之外的信息。
- en: 1.3.4 Membership inference attacks
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.4 成员推理攻击
- en: Whereas model inversion attacks do not try to reproduce an actual sample from
    the training dataset, membership inference attacks try to infer a sample based
    on the ML model output to identify whether it was in the original training dataset.
    The idea behind a membership inference attack is that given an ML model, a sample,
    and domain knowledge, the adversary can determine whether the sample is a member
    of the training dataset used to build the ML model, as shown in figure 1.6.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与模型反演攻击不试图从训练数据集中重现实际样本不同，成员推理攻击试图根据机器学习模型的输出推断一个样本，以确定它是否在原始训练数据集中。成员推理攻击背后的想法是，给定一个机器学习模型、一个样本和领域知识，攻击者可以确定该样本是否是构建机器学习模型所使用的训练数据集的成员，如图1.6所示。
- en: '![CH01_F06_Zhuang](../../OEBPS/Images/CH01_F06_Zhuang.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F06_Zhuang](../../OEBPS/Images/CH01_F06_Zhuang.png)'
- en: Figure 1.6 How membership inference attack works
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 成员推理攻击的工作原理
- en: Let’s consider an ML-based disease diagnosis system by analyzing the input medical
    information and conditions. For instance, suppose a patient participates in a
    study that diagnoses the correct difficulty level for a complex game designed
    to identify people who have Alzheimer’s disease. If an attacker succeeds in carrying
    out membership inference, they will know whether this patient was in the original
    dataset used to build the model. Not only that, by knowing the difficulty level
    of the game, the adversary can deduce whether this patient suffers from Alzheimer’s
    disease. This scenario describes is a serious leakage of sensitive information
    that could be used later for targeted action against the patient.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过分析输入的医疗信息和条件来考虑一个基于机器学习的疾病诊断系统。例如，假设一位患者参与了一项研究，该研究诊断了一个旨在识别患有阿尔茨海默病的复杂游戏的正确难度级别。如果攻击者成功执行了成员推理，他们将知道这位患者是否在构建模型所使用的原始数据集中。不仅如此，通过了解游戏的难度级别，攻击者可以推断出这位患者是否患有阿尔茨海默病。这种情况描述的是一种严重的信息泄露，这些信息可能后来被用于针对患者的针对性行动。
- en: How to infer the membership
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如何进行成员推理
- en: In this kind of attack, the adversary intends to learn whether an individual’s
    personal record was used to train the original ML model. To do so, the attacker
    first generates a secondary attack model by utilizing the model’s domain knowledge.
    Typically, these attack models are trained using shadow models previously generated
    based on noisy versions of actual data, data extracted from model inversion attacks,
    or statistics-based synthesis. To train these shadow models, the adversary requires
    black- or white-box access to the original ML model and sample datasets.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种攻击中，攻击者的意图是了解个人的个人记录是否被用于训练原始机器学习模型。为此，攻击者首先通过利用模型的领域知识生成一个二级攻击模型。通常，这些攻击模型是使用基于实际数据噪声版本、从模型反演攻击中提取的数据或基于统计的综合生成的影子模型进行训练的。为了训练这些影子模型，攻击者需要黑盒或白盒访问原始机器学习模型和样本数据集。
- en: With that, the attacker has access to both the ML service and the attack model.
    The attacker observes the dissimilarities between the output data produced by
    ML model predictions used during the training phase and the data not included
    in the training set [5], as depicted in figure 1.6\. Membership inference attempts
    to learn whether a particular record is in the training dataset, not the dataset
    itself.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，攻击者可以访问机器学习服务以及攻击模型。攻击者观察在训练阶段使用的机器学习模型预测产生的输出数据与训练集中未包含的数据之间的差异，如图1.6所示。成员推理尝试学习特定记录是否在训练数据集中，而不是数据集本身。
- en: There are a couple of strategies for mitigating membership inference attacks,
    such as regularization or coarse precision of prediction values. We will discuss
    these regularization strategies in chapter 8\. However, limiting the output of
    the ML model only to class labels is the most effective way of downgrading the
    threat. In addition, differential privacy (DP) is an effective resisting mechanism
    for membership inference attacks, as we’ll discuss in chapter 2.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种缓解成员推理攻击的策略，例如正则化或预测值的粗略精度。我们将在第8章中讨论这些正则化策略。然而，仅将机器学习模型的输出限制为类别标签是降低威胁的最有效方法。此外，差分隐私（DP）是抵抗成员推理攻击的有效机制，我们将在第2章中讨论。
- en: 1.3.5 De-anonymization or re-identification attacks
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.5 反匿名化或重新识别攻击
- en: Anonymizing datasets before releasing them to third-party users is a typical
    approach to protecting user privacy. In simple terms, anonymization protects private
    or sensitive information by erasing or changing identifiers that connect an individual
    to stored data. For example, you can anonymize personally identifiable information
    such as names, addresses, and social security numbers through a data anonymization
    process that retains the data but keeps the source anonymous. Some organizations
    employ various strategies to release only anonymized versions of their datasets
    for the general public’s use (e.g., public voter databases, Netflix prize dataset,
    AOL search data, etc.). For example, Netflix published a large dataset of 500,000
    Netflix subscribers with anonymized movie ratings by inviting different contestants
    to perform data mining and propose new algorithms to build better movie recommender
    systems.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据集发布给第三方用户之前对其进行匿名化是一种典型的保护用户隐私的方法。简单来说，匿名化通过删除或更改将个人与存储数据连接的标识符来保护私人或敏感信息。例如，您可以通过保留数据但保持来源匿名的数据匿名化过程来匿名化个人身份信息，如姓名、地址和社会安全号码。一些组织采用各种策略，只为公众使用发布其数据集的匿名版本（例如，公共选民数据库、Netflix奖项数据集、AOL搜索数据等）。例如，Netflix通过邀请不同的参赛者进行数据挖掘并提出构建更好的电影推荐系统的算法，发布了一个包含50万Netflix订阅者的大型数据集，其中包含了匿名化的电影评分。
- en: However, even when data is cleared of identifiers, attacks are still possible
    through de-anonymization. De-anonymization techniques can easily cross-reference
    publicly available sources and reveal the original information. In 2008, Narayanan
    et al. demonstrated that even with data anonymization techniques such as *k*-anonymity,
    it is possible to infer the private information of individuals [6]. In their attack
    scenario, they utilized the Internet Movie Database (IMDb) as background knowledge
    to identify the Netflix records of known users, apparently uncovering users’ political
    preferences. Thus, simple syntax-based anonymization cannot reliably protect data
    privacy against strong adversaries. We need to rely on something like differential
    privacy. We will discuss re-identification attacks in more detail in chapter 8.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使数据被清除了标识符，攻击仍然可能通过反匿名化进行。反匿名化技术可以轻松地交叉引用公开可用的来源并揭示原始信息。在2008年，Narayanan等人证明了即使在*k*匿名等数据匿名化技术下，也有可能推断出个人的私人信息[6]。在他们攻击场景中，他们利用互联网电影数据库（IMDb）作为背景知识来识别已知用户的Netflix记录，显然揭示了用户的政治偏好。因此，基于简单语法的匿名化不能可靠地保护数据隐私免受强大对手的攻击。我们需要依赖类似差分隐私的东西。我们将在第8章中更详细地讨论重新识别攻击。
- en: 1.3.6 Challenges of privacy protection in big data analytics
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.6 大数据分析中隐私保护面临的挑战
- en: Apart from the threats and attacks tailored explicitly to ML models and frameworks,
    another privacy challenge arises at the opposite end of the ML and privacy spectrum.
    That is, how can we protect *data at rest*, such as data stored in a database
    system before it is fed to an ML task, and *data in transit*, where data flows
    through various elements in the underlying ML framework? The ongoing move toward
    larger and connected data reservoirs makes it more challenging for database systems
    and data analytics tools to protect data against privacy threats.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 除了针对机器学习模型和框架的特定威胁和攻击之外，在机器学习和隐私光谱的另一端也出现了一个隐私挑战。那就是，我们如何保护*静态数据*，例如在输入机器学习任务之前存储在数据库系统中的数据，以及*传输中的数据*，其中数据在底层机器学习框架的各个元素中流动？向更大、更互联的数据存储库的持续转变使得数据库系统和数据分析工具在保护数据免受隐私威胁方面变得更加困难。
- en: 'One of the significant privacy threats in database systems is linking different
    database instances to explore an individual’s unique fingerprint. This type of
    attack can be categorized as a subclass of a re-identification attack and is most
    often an insider attack in terms of database applications. Based on the formation
    and identification of data, these attacks can be further classified into two types:
    correlation attacks and identification attacks.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库系统中一个显著的隐私威胁是将不同的数据库实例链接起来以探索个人的独特指纹。这种攻击可以归类为重识别攻击的一个子类，并且在数据库应用中通常是内部攻击。基于数据的形成和识别，这些攻击可以进一步分为两种类型：相关攻击和识别攻击。
- en: Correlation attacks
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 相关攻击
- en: The ultimate purpose of a correlation attack is to find a correlation between
    two or more data fields in a database or set of database instances to create unique
    and informative data tuples. As you know, in some cases we can bring domain knowledge
    from external data sources into the identification process. For example, let’s
    take a medical records database that lists user information with a history of
    medication prescriptions. Consider another data source with user information along
    with pharmacies visited. Once these sources are linked together, the correlated
    database can include some additional knowledge, such as which patient bought their
    medication from which pharmacy. Moreover, if it is smart enough to explore the
    frequently visited pharmacies, an adversary might obtain a rough estimate of where
    the patient resides. Thus, the final correlated dataset can have more private
    information per user than the original datasets.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 相关攻击的最终目的是在数据库或一组数据库实例中找到两个或更多数据字段之间的关联，以创建独特且信息丰富的数据元组。正如您所知，在某些情况下，我们可以将来自外部数据源的主题知识引入识别过程。例如，让我们以一个列出用户信息以及药物处方历史的医疗记录数据库为例。考虑另一个包含用户信息以及访问过的药房的数据库。一旦这些来源被连接起来，相关的数据库可以包括一些额外的知识，例如哪些患者从哪些药房购买了药物。此外，如果足够智能地探索常去的药房，攻击者可能能够获得患者居住的大致估计。因此，最终的关联数据集可能比原始数据集包含更多关于每个用户的私人信息。
- en: Identification attacks
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 识别攻击
- en: While a correlation attack tries to extract more private information, an identification
    attack tries to identify a targeted individual by linking entries in a database
    instance. The idea is to explore more personal information about a particular
    individual for identification. We can consider this one of the most threatening
    types of data privacy attacks on a dataset as it affects an individual’s privacy
    more. For example, suppose an employer looked into all the occurrences of its
    employees in a medical record or pharmacy customer database. That might reveal
    lots of additional information about the employees’ medication records, medical
    treatments, and illnesses. Thus, this attack is an increasing threat to individual
    privacy.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当相关攻击试图提取更多私人信息时，识别攻击试图通过链接数据库实例中的条目来识别目标个人。其想法是探索有关特定个人的更多个人信息以进行识别。我们可以将这种攻击视为对数据集中最具威胁的数据隐私攻击之一，因为它对个人的隐私影响更大。例如，假设雇主调查了其员工在医疗记录或药房客户数据库中的所有出现情况。这可能会揭示有关员工药物记录、医疗治疗和疾病的大量额外信息。因此，这种攻击对个人隐私构成了日益增长的威胁。
- en: At this point, it should be clear that we need to have sophisticated mechanisms
    in data analytics and ML applications to protect individuals’ privacy from different
    targeted attacks. Using multiple layers of data anonymization and data pseudonymization
    techniques makes it possible to protect privacy in such a way that linking different
    datasets is still possible, but identifying an individual by analyzing the data
    records is challenging. Chapters 7 and 8 provide a comprehensive assessment of
    different privacy-preserving techniques, a detailed analysis of how they can be
    used in modern data-driven applications, and a demonstration of how you can implement
    them in Python.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，应该很清楚，我们需要在数据分析和应用机器学习中有复杂的机制来保护个人免受不同针对性攻击的隐私。使用多层数据匿名化和数据假名化技术使得在链接不同数据集的同时保护隐私成为可能，但通过分析数据记录来识别个人则具有挑战性。第7章和第8章对不同的隐私保护技术进行了全面评估，详细分析了它们如何在现代数据驱动应用中使用，并展示了如何在Python中实现它们。
- en: '1.4 Securing privacy while learning from data: Privacy-preserving machine learning'
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 在从数据中学习的同时保护隐私：隐私保护机器学习
- en: Many privacy-enhancing techniques concentrate on allowing multiple input parties
    to collaboratively train ML models without releasing the private data in its original
    form. This collaboration can be performed by utilizing cryptographic approaches
    (e.g., secure multiparty computation) or differential private data release (perturbation
    techniques). Differential privacy is especially effective in preventing membership
    inference attacks. Finally, as discussed previously, the success of model inversion
    and membership inference attacks can be decreased by limiting the model’s prediction
    output (e.g., class labels only).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 许多增强隐私的技术集中在允许多个输入方协作训练机器学习模型，同时不泄露原始形式的私有数据。这种协作可以通过使用加密方法（例如，安全多方计算）或差分隐私数据发布（扰动技术）来实现。差分隐私在防止成员推理攻击方面特别有效。最后，正如之前讨论的，通过限制模型的预测输出（例如，仅限类别标签）可以降低模型反演和成员推理攻击的成功率。
- en: This section introduces several privacy-enhancing techniques at a high level
    to give you a general understanding of how they work. These techniques include
    differential privacy, local differential privacy (LDP), privacy-preserving synthetic
    data generation, privacy-preserving data mining techniques, and compressive privacy.
    Each will be expanded on later in this book.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 本节从高层次介绍了几种增强隐私的技术，以使您对它们的工作原理有一个一般性的了解。这些技术包括差分隐私、局部差分隐私（LDP）、隐私保护合成数据生成、隐私保护数据挖掘技术以及压缩隐私。这些内容将在本书的后续章节中进一步展开。
- en: 1.4.1 Use of differential privacy
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.1 差分隐私的使用
- en: The data explosion has resulted in greatly increased amounts of data being held
    by individuals and entities, such as personal images, financial records, census
    data, and so on. However, the privacy concern is raised when this data leaves
    the hands of data owners and is used in some computations. The AOL search engine
    log attack [7] and Netflix prize contest attacks [8] demonstrate the existence
    of such threats and emphasize the importance of having privacy-aware ML algorithms.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 数据爆炸导致个人和实体（如个人图像、财务记录、人口普查数据等）持有的数据量大幅增加。然而，当这些数据离开数据所有者的控制并用于某些计算时，就会引发隐私担忧。AOL搜索引擎日志攻击[7]和Netflix奖项竞赛攻击[8]证明了这种威胁的存在，并强调了具有隐私意识机器学习算法的重要性。
- en: 'Differential privacy (DP) is a promising solution for providing privacy protection
    for data. It attempts to protect an individual’s sensitive information from any
    inference attacks targeting the statistics or aggregated data of the individual.
    Publishing only the statistics or aggregated data of multiple people in a dataset
    does not necessarily ensure privacy protection in many cases. Let’s consider a
    simple retail use case with a loyalty card scenario. Suppose we have two aggregate
    statistics: the total amount spent by all customers on a particular day and a
    subgroup of that—the total amount spent by customers using a loyalty card on the
    same day. Suppose there is precisely one customer who purchases without a loyalty
    card. In that case, by simply comparing the difference between two such statistics,
    someone could easily infer this customer’s total amount spent, based on only those
    aggregate statistics.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私（DP）是提供数据隐私保护的一个有希望的解决方案。它试图保护个体的敏感信息，防止针对个体统计数据或聚合数据的任何推理攻击。在许多情况下，仅发布数据集中多个人群的统计数据或聚合数据并不一定能确保隐私保护。让我们考虑一个简单的零售用例，忠诚度卡场景。假设我们有两组聚合统计数据：特定一天所有顾客的总消费额以及其中一部分——当天使用忠诚度卡的顾客的总消费额。假设恰好有一位顾客没有使用忠诚度卡进行购买。在这种情况下，通过简单地比较这两组统计数据之间的差异，某人可以很容易地推断出这位顾客的总消费额，仅基于这些聚合统计数据。
- en: DP is based on the idea that statistics or aggregated data (including ML models)
    should not reveal whether an individual appears in the original dataset (the training
    data for the ML models). For example, given two identical datasets, one including
    an individual’s information and the other without their information, DP ensures
    that the probability of generating specific statistics or aggregated values is
    nearly the same whether conducted on the first or the second dataset.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: DP（差分隐私）基于这样一个理念：统计数据或聚合数据（包括机器学习模型）不应揭示个体是否出现在原始数据集中（机器学习模型的训练数据）。例如，给定两个相同的数据库，一个包含个体的信息，另一个则没有，DP确保在第一个或第二个数据库上生成特定统计数据或聚合值的概率几乎相同。
- en: To be more specific, consider a trusted data curator that gathers data from
    multiple data owners and performs a computation on the data, such as calculating
    the mean value or finding the maximum or minimum value. To ensure no one can reliably
    infer any individual sample from the computation result, DP requires the curator
    to add random noise to the result, such that the released data will not change
    if any sample of the underlying data changes. Since no single sample can significantly
    affect the distribution, adversaries cannot confidently infer the information
    corresponding to any individual sample. Thus, a mechanism is differentially private
    if the computation result of the data is robust to any change in the individual
    samples.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，考虑一个可信的数据管理员，他从多个数据所有者那里收集数据并对数据进行计算，例如计算平均值或找到最大值或最小值。为了确保没有人可以从计算结果中可靠地推断出任何单个样本，DP要求管理员向结果添加随机噪声，使得如果任何底层数据样本发生变化，发布的数据将不会改变。由于没有单个样本可以显著影响分布，对手无法自信地推断出与任何单个样本对应的信息。因此，如果数据的计算结果是针对任何单个样本变化的鲁棒的，那么该机制就是差分隐私的。
- en: Due to its underlying mechanisms, DP techniques resist membership inference
    attacks by adding random noise to the input data, to iterations of a particular
    ML algorithm, or to algorithm output. In chapters 2 and 3, we will thoroughly
    analyze how we can adopt differential privacy in privacy-preserving ML (PPML)
    applications.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其底层机制，差分隐私技术通过向输入数据、特定机器学习算法的迭代或算法输出中添加随机噪声来抵抗成员推断攻击。在第2章和第3章中，我们将详细分析我们如何在隐私保护机器学习（PPML）应用中采用差分隐私。
- en: 1.4.2 Local differential privacy
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.2 本地差分隐私
- en: When the input parties do not have enough information to train an ML model,
    it can be better to utilize approaches that rely on local differential privacy
    (LDP). For instance, multiple cancer research institutions want to build an ML
    model to diagnose skin lesions, but no single party has enough data to train a
    model. LDP is one of the solutions that they can use to train an ML model collaboratively
    without violating individual privacy.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入方没有足够的信息来训练一个机器学习模型时，利用依赖于本地差分隐私（LDP）的方法可能更好。例如，多个癌症研究机构希望构建一个机器学习模型来诊断皮肤病变，但没有一个机构拥有足够的数据来训练模型。LDP是他们可以用来在不侵犯个人隐私的情况下协作训练机器学习模型的一种解决方案。
- en: In LDP, individuals send their data to the data aggregator after privatizing
    data by perturbation. Hence, these techniques provide plausible deniability (an
    adversary cannot prove that the original data exists) for individuals. The data
    aggregator collects all the perturbed values and estimates statistics such as
    the frequency of each value in the population. Compared with DP, LDP shifts the
    perturbation from the central site to the local data owner. It considers a scenario
    where there is no trusted third party and an untrustworthy data curator needs
    to collect data from data owners and perform certain computations. The data owners
    are still willing to contribute their data, but the privacy of the data must be
    enforced.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在LDP中，个人在通过扰动对数据进行隐私化处理后，将他们的数据发送给数据聚合器。因此，这些技术为个人提供了合理的否认可能性（对手无法证明原始数据存在）。数据聚合器收集所有扰动值并估计诸如人口中每个值的频率等统计数据。与DP相比，LDP将扰动从中心站点转移到本地数据所有者。它考虑了一个没有可信第三方且不可信的数据管理员需要从数据所有者那里收集数据并执行某些计算的场景。数据所有者仍然愿意贡献他们的数据，但数据的隐私必须得到强制执行。
- en: 'An old and well-known version of local privacy is a randomized response (RR),
    which provides plausible deniability for respondents to sensitive queries. For
    example, a respondent flips a fair coin:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 本地隐私的一个古老且广为人知的版本是随机响应（RR），它为受访者对敏感查询提供了合理的否认可能性。例如，受访者抛一枚公平的硬币：
- en: If tails, the respondent answers truthfully.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果是反面，受访者将如实回答。
- en: If heads, they flip a second coin and respond “yes” if heads and “no” if tails.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果是正面，他们将抛第二枚硬币，如果正面则回答“是”，如果反面则回答“否”。
- en: An ML-oriented work, AnonML, utilized the ideas of RR to generate histograms
    from multiple input parties [9]. AnonML uses these histograms to create synthetic
    data on which an ML model can be trained. Like other LDP approaches, AnonML is
    a good option when no input party has enough data to build an ML model on their
    own (and when there is no trusted aggregator). In chapters 4 and 5, we will present
    a detailed analysis of how LDP differs from differential privacy and how it can
    be used in different ML applications.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一个面向机器学习的工作，AnonML，利用了RR的思想从多个输入方生成直方图[9]。AnonML使用这些直方图来创建用于训练机器学习模型的合成数据。与其他LDP方法一样，当没有输入方有足够的数据自己构建机器学习模型（且没有可信的聚合器）时，AnonML是一个不错的选择。在第4章和第5章中，我们将详细分析LDP与差分隐私的不同之处以及它在不同的机器学习应用中的使用方法。
- en: 1.4.3 Privacy-preserving synthetic data generation
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.3 隐私保护合成数据生成
- en: Although many privacy-preserving techniques have been proposed and developed
    for all kinds of ML algorithms, sometimes data users may want to execute new ML
    algorithms and analysis procedures. When there is no predefined algorithm for
    the requested operation, data users may request data to utilize it locally. To
    that end, different privacy-preserving data-sharing techniques such as *k*-anonymity,
    *l*-diversity, *t*-closeness, and data perturbation have been proposed in the
    past. These techniques can be fine-tuned to generate a new anonymized dataset
    from the same original dataset. However, in some cases, anonymization alone may
    hurt the utility of the underlying ML algorithms. Thus, a promising solution for
    data sharing is to generate synthetic yet representative data that can be safely
    shared with others.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经提出了许多用于各种机器学习算法的隐私保护技术，但有时数据用户可能想要执行新的机器学习算法和分析程序。当没有预定义的算法用于请求的操作时，数据用户可能要求使用数据本地化。为此，过去已经提出了不同的隐私保护数据共享技术，如*k*-匿名、*l*-多样性、*t*-接近性和数据扰动。这些技术可以被微调，从同一原始数据集中生成一个新的匿名化数据集。然而，在某些情况下，仅匿名化可能会损害底层机器学习算法的效用。因此，数据共享的一个有希望的解决方案是生成既合成又具有代表性的数据，可以安全地与他人共享。
- en: Synthetic data is generated artificially rather than produced through real-world
    events at a high level. It is usually generated algorithmically and is often used
    as a stand-in for training and testing ML models. Nevertheless, in practice, sharing
    a synthetic dataset in the same format (preserving the same statistical characteristics)
    as the original dataset gives data users much more flexibility with minimal privacy
    concerns.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据是通过人工生成而非通过高级别现实世界事件产生的。它通常是通过算法生成的，并且经常被用作训练和测试机器学习模型的替代品。然而，在实践中，以与原始数据集相同的格式（保留相同的统计特征）共享合成数据集，为数据用户提供更大的灵活性，同时隐私担忧最小化。
- en: Different studies have investigated privacy-preserving synthetic data generation
    on different dimensions. For instance, *plausible deniability* is one such approach
    employing a privacy test after generating the synthetic data. In 2012, Hardt et
    al. proposed an algorithm that combines the multiplicative weights approach and
    an exponential mechanism for differentially private data release [10]. On the
    other hand, Bindschaedler et al. proposed a generative model, a probabilistic
    model that captures the joint distribution of features based on correlation-based
    feature selection [11]. In 2017, Domingo-Ferrer et al. proposed a micro-aggregation-based
    differential private data releasing approach, which reduces the noise required
    by differential privacy based on *k*-anonymity [12]. All in all, privacy-preserving
    synthetic data generation is gaining traction within the ML community.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的研究在不同的维度上调查了隐私保护合成数据生成。例如，*合理否认*是这样一个方法，在生成合成数据后进行隐私测试。在2012年，Hardt等人提出了一种结合乘性权重方法和差分隐私数据发布指数机制的算法[10]。另一方面，Bindschaedler等人提出了一种生成模型，这是一种基于相关特征选择的概率模型，它捕捉了特征联合分布[11]。在2017年，Domingo-Ferrer等人提出了一种基于微聚合的差分隐私数据发布方法，它基于*k*-匿名减少了差分隐私所需的噪声[12]。总的来说，隐私保护合成数据生成在机器学习社区中越来越受欢迎。
- en: The benefits of using synthetic data, such as reduced constraints when using
    sensitive data and the capability to tailor the data for certain conditions that
    cannot be obtained with authentic data, have already gained attention in many
    real-world practical use cases. In chapter 6, we will introduce different mechanisms
    for synthetic data generation with the goal of privacy-preserving ML.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用合成数据的好处，例如在使用敏感数据时减少约束以及能够针对某些无法通过真实数据获得的特定条件定制数据的能力，已经在许多现实世界的实际应用案例中引起了关注。在第6章中，我们将介绍用于合成数据生成的不同机制，目标是实现隐私保护机器学习。
- en: 1.4.4 Privacy-preserving data mining techniques
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.4 隐私保护数据挖掘技术
- en: So far in this chapter, we have looked into different privacy protection approaches
    for ML algorithms. Now, let’s focus on protecting privacy while engaging data
    mining operations. The evolving interest in advances in ML algorithms, storage,
    and the flow of sensitive information poses significant privacy concerns. As a
    result, different approaches to handling and publishing sensitive data have been
    proposed over the past decade.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们已经探讨了针对机器学习算法的不同隐私保护方法。现在，让我们专注于在执行数据挖掘操作时保护隐私。对机器学习算法、存储和敏感信息流动的进步不断增长的兴趣引发了重大的隐私问题。因此，在过去十年中，已经提出了不同的处理和发布敏感数据的方法。
- en: Among privacy-preserving data mining (PPDM) techniques, the vast majority rely
    on either modifying data or removing some of the original content to protect privacy.
    The resulting quality degradation from this sanitization or transformation is
    the tradeoff between the quality of data and the level of privacy. Nevertheless,
    the basic idea behind all these PPDM techniques is to efficiently mine data while
    preserving the privacy of individuals. There are three main classes of techniques
    for dealing with PPDM, based on the different stages of data collection, publishing,
    and processing. Let’s briefly look at these approaches.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在隐私保护数据挖掘（PPDM）技术中，绝大多数依赖于修改数据或删除部分原始内容以保护隐私。这种净化或转换导致的质量下降是数据质量与隐私水平之间的权衡。尽管如此，所有这些PPDM技术背后的基本思想是在保护个人隐私的同时高效地挖掘数据。根据数据收集、发布和处理的各个阶段，处理PPDM的技术可以分为三个主要类别。让我们简要地看看这些方法。
- en: Techniques for privacy-preserving data collection
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私保护数据收集技术
- en: The first class of PPDM techniques ensures privacy at the stage of data collection.
    It usually incorporates different randomization techniques at the data collection
    stage and generates privatized values, so original values are never stored. The
    most common randomization approach is to modify data by adding some noise with
    a known distribution. Whenever data mining algorithms are involved, the original
    data distribution, but not the individual values, can be reproduced. Additive
    and multiplicative noise approaches are two of the most common data randomization
    techniques in this category.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 第一类PPDM技术确保在数据收集阶段保护隐私。它通常在数据收集阶段结合不同的随机化技术，生成私有化值，因此原始值从未被存储。最常用的随机化方法是通过添加已知分布的噪声来修改数据。每当涉及数据挖掘算法时，可以重现原始数据分布，但不是个别值。加性和乘性噪声方法是这一类别中最常见的两种数据随机化技术。
- en: Different approaches to data publishing and processing
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 数据发布和处理的不同方法
- en: The second class of PPDM deals with techniques related to when data is released
    to third parties (published) without disclosing the ownership of sensitive information.
    Removing attributes that can explicitly identify an individual from a dataset
    is not sufficient, as users may still be identified by combining nonsensitive
    attributes or records. For example, consider a dataset of patient records from
    a hospital. We can remove the identifiable attributes, such as name and address,
    from this dataset before publishing it, but if someone else knows the age, gender,
    and zip code of a patient, they might be able to combine that information to trace
    the specific patient’s record in the dataset, even without having access to the
    name attribute.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: PPDM的第二类技术处理的是当数据被发布给第三方（公开）而未披露敏感信息所有权的情况。仅从数据集中移除可以明确识别个人的属性是不够的，因为用户可能仍然可以通过组合非敏感属性或记录来被识别。例如，考虑一个来自医院的病人记录数据集。在发布之前，我们可以从这个数据集中移除可识别的属性，如姓名和地址，但如果有人知道一个病人的年龄、性别和邮政编码，他们可能能够结合这些信息来追踪数据集中的特定病人记录，即使没有访问到姓名属性。
- en: Hence, PPDM techniques usually incorporate one or more data sanitization operations,
    such as generalization, suppression, anatomization, and perturbation. Based on
    these sanitization operations, a set of privacy models can be proposed, which
    are now broadly used in different application domains for privacy protection.
    We will discuss these techniques and privacy models in chapter 7.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，PPDM技术通常包含一个或多个数据清洗操作，例如泛化、抑制、解剖和扰动。基于这些清洗操作，可以提出一系列隐私模型，这些模型现在广泛应用于不同的应用领域以实现隐私保护。我们将在第7章中讨论这些技术和隐私模型。
- en: Protecting privacy of data mining algorithm output
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 保护数据挖掘算法输出的隐私
- en: 'Even with only implicit access to the original dataset, outputs of data mining
    algorithms may reveal private information about the underlying dataset. An active
    adversary may access these algorithms and query data to infer some private information.
    Thus, different techniques have been proposed to preserve the privacy of the output
    of data mining algorithms:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 即使只有对原始数据集的隐式访问，数据挖掘算法的输出也可能揭示底层数据集的隐私信息。一个活跃的攻击者可能访问这些算法并查询数据以推断一些隐私信息。因此，已经提出了不同的技术来保护数据挖掘算法输出的隐私：
- en: '*Association rule hiding*—In data mining, association rule mining is a popular
    rule-based data mining approach to discover relationships between different variables
    in datasets. However, these rules sometimes may disclose an individual’s private
    or sensitive information. The idea of association rule hiding is to mine only
    the nonsensitive rules, ensuring that no sensitive rule is discovered. The most
    straightforward approach is to perturb the entries so that all sensitive, but
    not nonsensitive, rules are hidden.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关联规则隐藏*——在数据挖掘中，关联规则挖掘是一种基于规则的流行数据挖掘方法，用于发现数据集中不同变量之间的关系。然而，这些规则有时可能会泄露个人的隐私或敏感信息。关联规则隐藏的想法是仅挖掘非敏感规则，确保不会发现任何敏感规则。最直接的方法是对条目进行扰动，使得所有敏感但非敏感的规则都被隐藏。'
- en: '*Downgrading classifier effectiveness*—As we discussed in the context of membership
    inference attacks, classifier applications may leak information such that an adversary
    can determine whether a particular record is in the training dataset. Going back
    to our example of an ML service for diagnosing diseases, an adversary can devise
    an attack to learn whether a record for a specific individual has been used to
    train the ML model. In such circumstances, downgrading the accuracy of the classifier
    is one way to preserve privacy.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*降低分类器有效性*——正如我们在成员推理攻击的上下文中讨论的那样，分类器应用可能会泄露信息，使得攻击者可以确定特定记录是否在训练数据集中。回到我们用于诊断疾病的ML服务示例，攻击者可以设计攻击来学习特定个人的记录是否被用于训练ML模型。在这种情况下，降低分类器的准确性是保护隐私的一种方法。'
- en: '*Query auditing and restriction*—In some applications, users can query the
    original dataset but with limited query functionality, such as aggregate queries
    (SUM, AVERAGE, etc.). However, an adversary may still infer some private information
    by looking at the sequences of the queries and their corresponding results. Query
    auditing is commonly used to protect privacy in such scenarios by either perturbing
    the query results or denying one or more queries from a sequence of queries. On
    the downside, the computational complexity of this approach is much higher than
    that of the other approaches.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*查询审计和限制*——在某些应用中，用户可以查询原始数据集，但具有有限的查询功能，例如聚合查询（SUM、AVERAGE等）。然而，攻击者仍然可以通过查看查询序列及其对应的结果来推断一些隐私信息。查询审计通常用于此类场景的隐私保护，通过扰动查询结果或拒绝查询序列中的一或多个查询来实现。然而，这种方法计算复杂度远高于其他方法。'
- en: This discussion is just an overview of how PPDM works. In chapters 7 and 8,
    we will walk through a comprehensive analysis of PPDM techniques, as well as privacy-enhanced
    data management techniques in database systems.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个关于PPDM工作原理的概述。在第7章和第8章中，我们将详细分析PPDM技术，以及数据库系统中的隐私增强数据管理技术。
- en: 1.4.5 Compressive privacy
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.5 压缩隐私
- en: Compressive privacy perturbs the data by projecting it to a lower-dimensional
    hyperplane via compression and dimensionality reduction techniques. Most of these
    transformation techniques are lossy. Liu et al. suggested that compressive privacy
    would strengthen the privacy protection of the sensitive data, since recovering
    the exact original data from a transformed version (i.e., compressed or dimensionality
    reduced data) would not be possible [13].
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩隐私通过压缩和降维技术将数据投影到低维超平面来扰动数据。这些转换技术中的大多数都是有损的。刘等人建议，压缩隐私将加强敏感数据的隐私保护，因为从转换版本（即压缩或降维数据）中恢复精确的原始数据是不可能的
    [13]。
- en: In figure 1.7, x^i is the original data, and ![x_tilde_i](../../OEBPS/Images/x_tilde_i.png) is
    the corresponding transformed data—the projection of x^i on dimension U¹. We know
    that ![x_tilde_i](../../OEBPS/Images/x_tilde_i.png) can be mapped back to an infinite
    number of points perpendicular to U¹. In other words, the possible solutions are
    infinite as the number of equations is less than the number of unknowns. Therefore,
    Liu et al. proposed applying a random matrix to reduce the dimensions of the input
    data. Since a random matrix might decrease the utility, other approaches used
    both unsupervised and supervised dimensionality reduction techniques, such as
    principal component analysis, discriminant component analysis, and multidimensional
    scaling. These techniques attempt to find the best projection matrix for utility
    purposes while relying on the reduced dimensionality to enhance privacy.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在图1.7中，x^i是原始数据，而![x_tilde_i](../../OEBPS/Images/x_tilde_i.png)是对应的转换数据——x^i在维度U¹上的投影。我们知道![x_tilde_i](../../OEBPS/Images/x_tilde_i.png)可以被映射回与U¹垂直的无限多个点。换句话说，可能的解是无限的，因为方程的数量少于未知数的数量。因此，刘等人提出了应用随机矩阵来降低输入数据的维度。由于随机矩阵可能会降低效用，其他方法使用了无监督和监督的降维技术，如主成分分析、判别成分分析和多维尺度。这些技术试图找到最佳投影矩阵以实现效用目的，同时依靠降低的维度来增强隐私。
- en: '![CH01_F07_Zhuang](../../OEBPS/Images/CH01_F07_Zhuang.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F07_Zhuang](../../OEBPS/Images/CH01_F07_Zhuang.png)'
- en: Figure 1.7 Compressive privacy works by projecting data to a lower-dimensional
    hyperplane.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7 压缩隐私通过将数据投影到低维超平面来工作。
- en: DEFINITION What is a lower dimensional hyperplane? In general, a hyperplane
    is a subspace whose dimension is one less than that of its original space. For
    example, in figure 1.7 the original ambient space is two-dimensional; hence, its
    hyperplanes are one-dimensional lines, as shown.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 定义：什么是低维超平面？一般来说，超平面是其原始空间维度少一的子空间。例如，在图1.7中，原始环境空间是二维的；因此，其超平面是一维线，如图所示。
- en: Compressive privacy guarantees that the original data can never be fully recovered.
    However, we can still obtain an approximation of the original data from the reduced
    dimensions. Therefore, some approaches, such as Jiang et al. [14], combine compressive
    privacy techniques (dimensionality reduction) with differential privacy techniques
    to publish differentially private data.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩隐私保证原始数据永远无法完全恢复。然而，我们仍然可以从降低的维度中获得原始数据的近似值。因此，一些方法，如江等人[14]，将压缩隐私技术（降维）与差分隐私技术相结合，以发布差分隐私数据。
- en: Although some entities may attempt to totally hide their data, compressive privacy
    has another benefit for privacy. For datasets that have samples with two labels—a
    utility label and a privacy label—Kung [15] proposes a dimensionality reduction
    method that enables the data owner to project their data in a way that maximizes
    the accuracy of learning the utility labels while decreasing the accuracy of learning
    the privacy labels. Although this method does not eliminate all data privacy risks,
    it controls the misuse of the data when the privacy target is known. Chapter 9
    will walk through the different approaches to and applications of compressive
    privacy.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管某些实体可能试图完全隐藏他们的数据，压缩隐私对隐私还有另一个好处。对于具有两个标签的样本集——效用标签和隐私标签——Kung [15] 提出了一种降维方法，允许数据所有者以最大化学习效用标签的准确性的方式投影他们的数据，同时降低学习隐私标签的准确性。尽管这种方法不能消除所有数据隐私风险，但它控制了当隐私目标已知时数据的滥用。第9章将介绍压缩隐私的不同方法和应用。
- en: 1.5 How is this book structured?
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 这本书是如何结构的？
- en: The forthcoming chapters of this book are structured as follows. Chapters 2
    and 3 will discuss how differential privacy can be utilized in PPML, with different
    use case scenarios and applications. If you are interested in finding out how
    to use DP in practical applications, along with a set of real-world examples,
    these chapters have got you covered.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 本书接下来的章节结构如下。第2章和第3章将讨论如何利用差分隐私（DP）在PPML中，包括不同的用例场景和应用。如果你对了解如何在实际应用中使用DP，以及一系列真实世界示例感兴趣，这些章节将为你提供所需的内容。
- en: In chapters 4 and 5, we will walk through methods and applications of applying
    differential privacy in the local setup, with an added restriction so that even
    if an adversary has access to individual responses, they will still be unable
    to learn anything beyond those responses.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章和第5章中，我们将介绍在本地设置中应用差分隐私的方法和应用程序，并增加一个限制条件，即使对手能够访问个别响应，他们仍然无法了解超出这些响应之外的信息。
- en: Chapter 6 will investigate how synthetic data generation techniques can be used
    in the PPML paradigm. As we already discussed, synthetic data generation is gaining
    traction within the ML community, especially as a stand-in for training and testing
    ML models. If you are interested in finding ways and means to generate synthetic
    data with the goal of PPML, this is your chapter.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 第6章将研究合成数据生成技术如何在PPML范式中使用。正如我们之前讨论的，合成数据生成在ML社区中越来越受欢迎，尤其是在作为训练和测试ML模型的替代品。如果你对寻找生成合成数据的方法和手段以实现PPML感兴趣，这是你的章节。
- en: In chapters 7 and 8, we will explore how privacy-enhancing technologies can
    be applied in data mining tasks and used and implemented in database systems.
    We know that, ultimately, everything has to be stored in a database somewhere,
    whether the data model is relational, NoSQL, or NewSQL. What if these databases
    or data mining applications are prone to privacy attacks while accessing or releasing
    data? These two chapters will investigate different techniques, methodologies,
    and well-established industry practices for mitigating such privacy leaks.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章和第8章中，我们将探讨隐私增强技术如何在数据挖掘任务中应用，以及如何在数据库系统中使用和实现。我们知道，最终，所有东西都必须存储在某个地方的数据库中，无论是关系型、NoSQL还是NewSQL数据模型。如果这些数据库或数据挖掘应用在访问或发布数据时容易受到隐私攻击，怎么办？这两章将调查不同的技术、方法和成熟的行业实践，以减轻这种隐私泄露。
- en: Next, we will look at another possible approach to PPML involving compressing
    or reducing the dimension of data by projecting it to another hyperplane. To that
    end, we will be discussing different compressive privacy approaches with their
    applications in chapter 9\. If you are designing or developing privacy applications
    for constrained environments with compressed data, we suggest you invest more
    time in this chapter. We will employ practical examples of data compression techniques
    to achieve privacy preservation for different application scenarios.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨另一种可能的PPML方法，即通过将数据投影到另一个超平面来压缩或降低数据的维度。为此，在第9章中，我们将讨论不同的压缩隐私方法及其应用。如果你正在设计或开发针对压缩数据的受限环境的隐私应用，我们建议你在这章上投入更多时间。我们将使用数据压缩技术的实际例子来实现不同应用场景的隐私保护。
- en: Finally, in chapter 10 we will put everything together and design a platform
    for research data protection and sharing by emphasizing the design challenges
    and implementation considerations.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在第10章中，我们将把所有内容整合起来，通过强调设计挑战和实施考虑，设计一个用于研究数据保护和共享的平台。
- en: '![CH01_UN01_Zhuang](../../OEBPS/Images/CH01_UN01_Zhuang.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_UN01_Zhuang](../../OEBPS/Images/CH01_UN01_Zhuang.png)'
- en: Summary
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In reconstruction attacks, the adversary gains the advantage by having external
    knowledge of feature vectors or the data used to build the ML model.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在重建攻击中，对手通过对外部知识（特征向量或用于构建ML模型的数据）的了解获得优势。
- en: Reconstruction attacks usually require direct access to the ML models deployed
    on the server; we call this white-box access.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重建攻击通常需要直接访问服务器上部署的ML模型；我们称之为白盒访问。
- en: Sometimes an adversary can listen to both the incoming requests to an ML model,
    when a user submits new testing samples, and the responses generated by the model
    for a given sample. This can lead to model inversion attacks.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时，对手可以监听ML模型的新测试样本提交时的传入请求以及模型为给定样本生成的响应。这可能导致模型反演攻击。
- en: A membership inference attack is an extended version of the model inversion
    attack where an adversary tries to infer a sample based on the ML model’s output
    to identify whether the sample is in the training dataset.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成员推理攻击是模型反演攻击的扩展版本，其中对手试图根据机器学习模型的输出推断样本，以确定该样本是否在训练数据集中。
- en: Even when datasets are anonymized, ensuring a system’s ability to reliably protect
    data privacy is challenging because attackers can utilize background knowledge
    to infer data with de-anonymization or re-identification attacks.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使数据集被匿名化，确保系统可靠地保护数据隐私仍然具有挑战性，因为攻击者可以利用背景知识通过去匿名化或重新识别攻击来推断数据。
- en: Linking different database instances together to explore an individual’s unique
    fingerprint is a significant privacy threat to database systems.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将不同的数据库实例链接起来以探索个人的独特指纹，对数据库系统来说是一个重大的隐私威胁。
- en: Differential privacy (DP) attempts to protect sensitive information from inference
    attacks targeting an individual’s statistics or aggregated data by adding random
    noise.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 差分隐私（DP）通过添加随机噪声来尝试保护敏感信息，防止针对个人统计数据或聚合数据的推理攻击。
- en: Local differential privacy (LDP) is the local setting of DP, where individuals
    send their data to the data aggregator after privatizing the data by perturbation,
    thus providing plausible deniability.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地差分隐私（LDP）是DP的本地设置，其中个人在通过扰动对数据进行隐私化处理后，将数据发送给数据聚合器，从而提供合理的否认可能性。
- en: Compressive privacy perturbs data by projecting it to a lower-dimensional hyperplane
    via compression and dimensionality reduction techniques.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 压缩隐私通过压缩和降维技术将数据投影到低维超平面，从而扰动数据。
- en: Synthetic data generation is a promising solution for data sharing that produces
    and shares a synthetic dataset in the same format as the original data, which
    gives much more flexibility in how data users can use the data, with no concerns
    about query-based budgets for privacy.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成合成数据是数据共享的一个有希望的解决方案，它以与原始数据相同的格式生成和共享合成数据集，这为数据用户如何使用数据提供了更大的灵活性，同时无需担心基于查询的隐私预算。
- en: 'Privacy-preserving data mining (PPDM) can be achieved using different techniques,
    which can be categorized into three main classes: privacy-preserving approaches
    to data collection, data publishing, and modifying data mining output.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐私保护数据挖掘（PPDM）可以通过不同的技术实现，这些技术可以分为三个主要类别：数据收集、数据发布和修改数据挖掘输出的隐私保护方法。
