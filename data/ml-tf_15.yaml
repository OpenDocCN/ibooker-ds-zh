- en: '12 Applying autoencoders: The CIFAR-10 image dataset'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 应用自动编码器：CIFAR-10 图像数据集
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Navigating and understanding the structure of the CIFAR-10 image dataset
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导航和理解 CIFAR-10 图像数据集的结构
- en: Building an autoencoder model to represent different CIFAR-10 image classes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建自动编码器模型以表示不同的 CIFAR-10 图像类别
- en: Applying the CIFAR-10 autoencoder as an image classifier
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 CIFAR-10 自动编码器应用于图像分类
- en: Implementing a stacked and denoising autoencoder on CIFAR-10 images
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 CIFAR-10 图像上实现堆叠和去噪自动编码器
- en: 'Autoencoders are powerful tools for learning arbitrary functions that transform
    input into output without having the full set of rules to do so. Autoencoders
    get their names from their function: learning a representation of the input much
    smaller than its size, which means encoding input data using less knowledge and
    then decoding that internal representation to get approximately back to its original
    input. When the input is an image, autoencoders have many useful applications.
    Compression is one, such as using 100 neurons in a hidden layer and formatting
    your 2D image input in row-order format (chapter 11). With averaging for the red,
    green, and blue channels, the autoencoder learns a representation of the image
    and is able to encode a 32 × 32 × 3 height × width × channel image, or 3,072 pixel
    intensities into 100 numbers, which is a reduction of 30x in data. How’s that
    for compression? Though you trained a network to demonstrate this use case in
    chapter 11, you did not explore the resulting learned representation of the images,
    but you will in this chapter.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器是学习任意函数的强大工具，这些函数可以将输入转换为输出，而不需要完整的规则集来完成。自动编码器得名于其功能：学习一个比其大小小得多的输入表示，这意味着使用更少的知识编码输入数据，然后解码内部表示以近似地回到原始输入。当输入是图像时，自动编码器有许多有用的应用。压缩是一个，例如使用隐藏层中的
    100 个神经元，并以行顺序格式格式化你的 2D 图像输入（第 11 章）。通过红、绿、蓝通道的平均值，自动编码器学习图像的表示，并能够将 32 × 32
    × 3 高度 × 宽度 × 通道图像，或 3,072 个像素强度编码为 100 个数字，这是数据量减少了 30 倍。这压缩效果如何？尽管你在第 11 章中训练了一个网络来展示这个用例，但你没有探索图像的最终学习表示，但你在本章中将会。
- en: Classification is also possible by using the autoencoder’s representation. You
    could train the autoencoder on a set of horse images from a labeled training dataset
    like the Canadian Institute for Advanced Research (CIFAR)-10 data and then compare
    the autoencoder’s representation of a horse—those 100 numbers, trained and weighted
    on many samples—with that the autoencoder’s learned representation for another
    image class, such as `frog`. The representation will be different, and you can
    use it as a compact way to classify (and cluster) the image types. Also, if you
    can classify samples with a strong representation, you can detect anomalous samples
    and thus detect automatically when some set of data in the series is different
    or perform anomaly detection. You explore these uses of autoencoders in this chapter,
    which focuses on the CIFAR-10 dataset.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用自动编码器的表示也可以进行分类。你可以在一个标记的训练数据集（如加拿大高级研究研究所的 CIFAR-10 数据）上训练一组马图像的自动编码器，然后比较自动编码器对马的表示——这些经过训练和加权的
    100 个数字，在许多样本上训练——与自动编码器对另一个图像类别（如“青蛙”）的学习表示。表示将不同，你可以用它作为对图像类型进行分类（和聚类）的紧凑方式。此外，如果你可以用强大的表示来分类样本，你可以检测异常样本，从而在数据序列中检测到某些数据集的不同，或者执行异常检测。你将在本章中探索这些自动编码器的用途，本章重点介绍
    CIFAR-10 数据集。
- en: As I hinted at the end of chapter 11, there are different types of autoencoders,
    including a stacked autoencoder, which uses more than one hidden layer and supports
    a deep architecture for classification. Also, denoising autoencoders try to noise
    up the input, such as an image, and see whether the network can still learn a
    more robust representation that’s resilient to image imperfections. You’ll play
    with both of those concepts in this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在第 11 章末尾所暗示的，存在不同类型的自动编码器，包括堆叠自动编码器，它使用多个隐藏层并支持用于分类的深度架构。此外，去噪自动编码器试图对输入（如图像）进行噪声处理，并查看网络是否仍然能够学习一个更鲁棒的表示，该表示对图像不完美具有弹性。你将在本章中探索这两个概念。
- en: 12.1 What is CIFAR-10?
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 什么是 CIFAR-10？
- en: CIFAR-10 gets its name from its 10 image classes, including `automobile`, `plane`,
    and `frog`. CIFAR-10 is a powerful tool of labeled images selected from the 80
    million Tiny Images dataset of size 32 × 32 and in three-channel RGB format. Each
    image represents 3,072 pixels in full color and 1,024 pixels in grayscale. You
    can read more about Tiny Images at [https://people.csail.mit.edu/torralba/publications/80millionImages.pdf](https://people.csail.mit.edu/torralba/publications/80millionImages.pdf).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-10的名字来源于其10个图像类别，包括`automobile`（汽车）、`plane`（飞机）和`frog`（青蛙）。CIFAR-10是从大小为32
    × 32和三通道RGB格式的8000万Tiny Images数据集中选出的标记图像的强大工具。每张图像代表3,072个全色像素和1,024个灰度像素。你可以在[https://people.csail.mit.edu/torralba/publications/80millionImages.pdf](https://people.csail.mit.edu/torralba/publications/80millionImages.pdf)了解更多关于Tiny
    Images的信息。
- en: Before you get too deep into autoencoder representations, it’s worthwhile to
    explore the CIFAR-10 dataset in detail.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究自动编码器表示之前，详细探索CIFAR-10数据集是值得的。
- en: CIFAR-10 is divided into a training and a test dataset, which is a good practice,
    as I have been preaching to you. Following in the vein of reserving approximately
    80% of the data for training and 20% of the data for testing, the dataset consists
    of 60,000 images divided into 50,000 training and 10,000 test images. Each image
    class— `airplane`, `automobile`, `bird`, `cat`, `deer`, `dog`, `frog`, `horse`,
    `ship`, and `truck`—has 6,000 representative samples in the dataset. The training
    set includes 5,000 random samples of each class, and the test set includes 1,000
    random samples of the data for each of the 10 classes. The data is formatted on
    disk in binary file format for Python, Matlab, and C programming users. The 50,000-image
    training dataset is broken into five batches, each with 10,000 images, and the
    training file batch is a single file of 10,000 images in random order. Figure
    12.1 shows a visual representation of CIFAR-10 and and its 10 classes, with randomly
    selected images.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-10被分为训练集和测试集，这是一个好的做法，正如我一直在向你宣扬的那样。遵循保留大约80%的数据用于训练和20%的数据用于测试的做法，该数据集由60,000张图像组成，分为50,000张训练图像和10,000张测试图像。每个图像类别——`airplane`（飞机）、`automobile`（汽车）、`bird`（鸟）、`cat`（猫）、`deer`（鹿）、`dog`（狗）、`frog`（青蛙）、`horse`（马）、`ship`（船）和`truck`（卡车）——在数据集中有6,000个代表性样本。训练集包括每个类别的5,000个随机样本，测试集包括每个10个类别的1,000个随机样本。数据以二进制文件格式存储在磁盘上，适用于Python、Matlab和C编程用户。50,000张图像的训练数据集被分为五个批次，每个批次有10,000张图像，训练文件批次是一个包含10,000张图像的单个文件，图像顺序是随机的。图12.1显示了CIFAR-10及其10个类别的视觉表示，其中显示了随机选择的图像。
- en: To grayscale or not to grayscale
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要转换为灰度还是不转换
- en: You may be wondering why you used grayscale for images in chapters 10 and 11
    instead of the full RGB values. Maybe the color property is something you want
    your neural network to learn, so why remove R(ed), G(reen) and B(lue), and replace
    them with grayscale? The answer lies in dimensionality reduction. Reducing to
    grayscale reduces the number of required learned parameters by a factor of 3,
    which helps in training and learning without sacrificing much. That’s not to say
    that you’ll never want to train a neural network on color as a feature. When you
    try to find a yellow cup in a picture that also has blue and red cups, you have
    to use color. But for your work in this chapter, the autoencoder will be happy
    enough with a third less data, and so will your computer’s CPU fan.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道为什么在第10章和第11章中使用灰度图像而不是完整的RGB值。也许颜色属性是你希望你的神经网络学习的东西，那么为什么去掉R(红色)、G(绿色)和B(蓝色)，并用灰度代替它们？答案在于降维。转换为灰度可以将所需的参数数量减少3倍，这有助于训练和学习，而不会牺牲太多。但这并不是说你永远不会想训练一个以颜色为特征的神经网络。当你试图在一张同时有蓝色和红色杯子的图片中找到黄色杯子时，你必须使用颜色。但在本章的工作中，自动编码器对三分之二的数据量就足够满意了，你的电脑CPU风扇也是如此。
- en: '![CH12_F01_Mattmann2](../Images/CH12_F01_Mattmann2.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![CH12_F01_Mattmann2](../Images/CH12_F01_Mattmann2.png)'
- en: Figure 12.1 Ten randomly picked samples for each of CIFAR-10’s image classes.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 CIFAR-10图像类别的每个类别随机挑选的10个样本。
- en: In chapter 11, you built an `autoencoder` class that took in a CIFAR-10 batch
    of images of the `horse` class; then it learned a representation of the images
    so that it could reduce each 1,024-pixel (32 × 32 in grayscale) image to 100 numbers.
    How well did the encoding perform? You explore that topic in section 12.1.1 as
    you refresh your memory of the `autoencoder` class.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在第11章中，你构建了一个`autoencoder`类，它接受一个CIFAR-10批次的`horse`类图像；然后它学习图像的表示，以便将每个1,024像素（灰度32
    × 32）的图像减少到100个数字。编码性能如何？你将在12.1.1节中探索这个主题，同时刷新你对`autoencoder`类的记忆。
- en: 12.1.1 Evaluating your CIFAR-10 autoencoder
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1.1 评估你的CIFAR-10自动编码器
- en: The CIFAR-10 `autoencoder` class that you built in chapter 11 had a single hidden
    layer of size 100 neurons, took as input a 5000 × 1024 vector of training images,
    encoded the input using `tf.matmul` and a hidden layer of size 1024 × 100, and
    decoded the hidden layer using an output layer of 100 × 1024\. The `autoencoder`
    class trained using 1,000 epochs and a user-specified batch size, and also provided
    methods to test the process by printing the values of the hidden neurons or the
    encoding and then the decoded representation of the original input values. Listing
    12.1 shows the class and its utility methods.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你在第 11 章中构建的 CIFAR-10 `autoencoder` 类有一个大小为 100 个神经元的单个隐藏层，输入是一个 5000 × 1024
    的训练图像向量，使用 `tf.matmul` 和大小为 1024 × 100 的隐藏层对输入进行编码，并使用大小为 100 × 1024 的输出层对隐藏层进行解码。`autoencoder`
    类使用 1,000 个 epoch 和用户指定的批次大小进行训练，并且还提供了方法通过打印隐藏神经元的值或编码以及原始输入值的解码表示来测试这个过程。列表
    12.1 显示了该类及其实用方法。
- en: Listing 12.1 The `autoencoder` class
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.1 `autoencoder` 类
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Gets a randomly selected batch of size from X using indices, selecting only
    unique samples because replace=False
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用索引从 X 中获取随机选择的批次大小，由于 replace=False，只选择唯一样本
- en: ❷ Reuses weights and biases using tf.scope construct for the encoding step
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用 tf.scope 构造重用权重和偏差进行编码步骤
- en: ❸ Creates encoding of size input_dim, hidden_dim
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建大小为 input_dim 和 hidden_dim 的编码
- en: ❹ Decodes encoding from hidden_dim back to input_dim using learned weights
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用学习到的权重将编码从 hidden_dim 解码回 input_dim
- en: ❺ Uses root mean-squared error (RMSE) as the loss function and optimizer
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用均方根误差（RMSE）作为损失函数和优化器
- en: ❻ Reuses a saver to save and restore the model
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 重用保存器来保存和恢复模型
- en: ❼ Iterates by num batches or floor(dataset size/ batch_size) for each epoch
    and train
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 通过 num_batches 或 floor(dataset size/ batch_size) 迭代每个 epoch 并进行训练
- en: ❽ Computes the encoding and decoding layers and output, and prints their values
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 计算编码和解码层以及输出，并打印它们的值
- en: Though you ran this class in chapter 11, you didn’t look at the input to it
    or the output from the decoding step to check out how well it learned a representation
    of the input. To do so, you can use the code from chapter 11, which is recommended
    on the CIFAR-10 website as an easy way to read the Python-formatted data stored
    in the Pickle format. Pickle is a Python library for compact binary representation
    that serializes a Python object into a byte stream and then provides methods to
    deserialize the object back into an active Python dynamic object from that byte
    stream. You can use the `unpickle` function and associated dataset loading code,
    such as the `greyscale` function, which converts the 50,000 training three-channel
    RGB images into one-channel grayscale images by taking the mean of the RGB values
    for each image. The rest of the CIFAR-10 loading code iterates through the downloaded
    five 10,000-image training files. The files are in the Python pickle format; the
    data is stored in a Python dictionary with key `data` and the key `labels` with
    valid values (0-9) for the 10 image classes. The 10 image-class names are populated
    into the `names` variable read from the Python pickle batches.meta file and include
    the values `automobile`, `bird`, and so on. Each of the five sets of 10,000 images
    and labels is vertically and horizontally stacked on two NumPy arrays—`data` of
    size `(50000,1024)` and `labels` `(5000,` `)`, respectively—and made available
    for training. Listing 12.2 gets the CIFAR-10 data ready for the `Autoencoder`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你在第 11 章中运行了这个类，但你没有查看它的输入或解码步骤的输出，以检查它如何学习输入的表示。为此，你可以使用第 11 章中的代码，该代码在 CIFAR-10
    网站上被推荐作为一种读取存储在 Pickle 格式的 Python 格式数据的简单方法。Pickle 是一个用于紧凑二进制表示的 Python 库，它将 Python
    对象序列化为字节流，然后提供方法将对象反序列化回从该字节流中激活的 Python 动态对象。你可以使用 `unpickle` 函数和相关数据集加载代码，例如
    `greyscale` 函数，该函数通过取每个图像的 RGB 值的平均值，将 50,000 张训练的三通道 RGB 图像转换为单通道灰度图像。CIFAR-10
    加载代码的其余部分遍历下载的五个 10,000 张图像的训练文件。这些文件是 Python pickle 格式；数据存储在具有键 `data` 和键 `labels`
    的 Python 字典中，`labels` 的有效值（0-9）对应于 10 个图像类别。10 个图像类别的名称填充到从 Python pickle batches.meta
    文件中读取的 `names` 变量中，包括值 `automobile`、`bird` 等等。每套 10,000 张图像和标签分别垂直和水平堆叠在两个 NumPy
    数组上——`data` 的大小为 `(50000,1024)` 和 `labels` `(5000,` `)`，分别，并可供训练使用。列表 12.2 将 CIFAR-10
    数据准备好用于 `Autoencoder`。
- en: Listing 12.2 Getting the CIFAR-10 data ready for the `Autoencoder`
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.2 为 `Autoencoder` 准备 CIFAR-10 数据
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Loads a CIFAR-10 batch Pickle file. There are five files for training and
    one for test, of size 10,000 images and labels each.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载 CIFAR-10 批量 Pickle 文件。共有五个用于训练的文件和一个用于测试的文件，每个文件包含 10,000 张图像和标签。
- en: ❷ The return from loading by Pickle is a Python dictionary, with keys data and
    labels.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用Pickle加载返回的是Python字典，键为data和labels。
- en: ❸ Converts 3-channel RGB image input into 1-channel grayscale by averaging the
    RGB values
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将3通道RGB图像输入转换为1通道灰度图，通过平均RGB值实现
- en: ❹ The named image classes indexed by each label index from labels
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 根据标签索引的每个标签索引命名的图像类别
- en: ❺ Iterates through the five batches for training and unpickles the data
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 迭代五个训练批次并反序列化数据
- en: ❻ Data is vertically stacked into (50000,1024), and labels are horizontally
    stacked into (50000, ) after the first iteration.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 数据在第一次迭代后垂直堆叠为(50000,1024)，标签在水平堆叠为(50000, )
- en: ❼ Applies the grayscale function on the (50000,1024) array of image data
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 在(50000,1024)的图像数据数组上应用灰度函数
- en: With the image data from CIFAR-10 loaded into the `data` array, the `labels`
    in the associated named array and the `names` array giving you the image classes
    associated with the label values, you can look at a particular class of images,
    such as `horse`, as you did in chapter 11\. After selecting the indices of the
    image data that have the `horse` class by first selecting the indices of all the
    horse labels from the labels array, you can use those indices to index into the
    data array for label `ID=7` (`horse`). Then you can display a few horse images
    from the set of 50,000—5,000 for each class in training—and also another 10,000
    for testing (1,000 for each class), using Matplotlib (listing 12.3).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在将CIFAR-10的图像数据加载到`data`数组中，相关命名数组中的`labels`以及`names`数组提供与标签值关联的图像类别后，你可以查看特定的图像类别，例如`horse`，就像在第11章中做的那样。通过首先从标签数组中选择所有马标签的索引，然后你可以使用这些索引来索引数据数组中的标签`ID=7`（`horse`），然后你可以使用Matplotlib（列表12.3）显示从50,000张图像集中的一小部分马图像——每个类别训练中有5,000张，另外10,000张用于测试（每个类别1,000张）。
- en: Listing 12.3 Selecting the set of 5,000 horse images and displaying them
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.3 选择5,000张马图像并显示它们
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Converts the data to a NumPy matrix for the Autoencoder later
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将数据转换为NumPy矩阵，以便稍后用于Autoencoder
- en: ❷ Converts the labels to a NumPy array through an explicit cast so you can use
    np.where
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过显式转换将标签转换为NumPy数组，以便可以使用np.where
- en: ❸ Uses np.where to select the indices of the horse labels (class ID=7)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用np.where选择马标签的索引（类别ID=7）
- en: ❹ Uses the horse indices to index into the image data of size (5000,1024)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用马索引索引到大小为(5000,1024)的图像数据
- en: ❺ Sets up Matplot lib to print five images of horses of size 10,10
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 设置Matplotlib以打印五张10x10大小的马图像
- en: ❻ Displays the horse image
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 显示马图像
- en: The resulting output is shown in figure 12.2\. Now you can see the CIFAR-10
    images you used your autoencoder to learn a representation of.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 结果输出显示在图12.2中。现在你可以看到使用你的autoencoder学习到的CIFAR-10图像表示。
- en: '![CH12_F02_Mattmann2](../Images/CH12_F02_Mattmann2.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![CH12_F02_Mattmann2](../Images/CH12_F02_Mattmann2.png)'
- en: Figure 12.2 The first five horse images returned from the set of 5,000 samples
    in CIFAR-10 training data
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 从CIFAR-10训练数据中的5,000个样本集中返回的前五张马图像
- en: Now review listing 12.4, which has the small snippet of code required to train
    your 100-neuron autoencoder to learn the representation of those horse images.
    You’ll need to run your TensorFlow-trained autoencoder.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回顾列表12.4，其中包含训练100神经元autoencoder以学习这些马图像表示所需的小段代码。你需要运行你的TensorFlow训练的autoencoder。
- en: Listing 12.4 Training your `Autoencoder`
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.4 训练你的`Autoencoder`
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Input dimensions are sized 1024 (32 × 32) grayscale CIFAR-10 image.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入维度大小为1024（32 × 32）的灰度CIFAR-10图像。
- en: ❷ Use hidden neurons as the encoding size to train up the Autoencoder on the
    horse images.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用隐藏神经元作为编码大小来训练Autoencoder以处理马图像。
- en: Next, I’ll show you how to use your learned representation to evaluate how well
    your encoding process did and how well it captures the `horse` image class.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将向你展示如何使用你学习到的表示来评估你的编码过程做得有多好，以及它如何捕捉`horse`图像类别。
- en: 12.2 Autoencoders as classifiers
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 自编码器作为分类器
- en: 'It’s probably worth reviewing the steps that you completed to build your autoencoder
    and get the CIFAR-10 data ready for loading. I’ll summarize them for you:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 审查你完成构建autoencoder和准备CIFAR-10数据以加载的步骤可能是有益的。我将为你总结：
- en: Load the 50,000 training images for CIFAR-10\. There are 5,000 sample images
    for each of the 10 classes of images. The images and their associated labels are
    read from Python’s Pickle binary representation.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载CIFAR-10的50,000个训练图像。每个图像类别有5,000个样本图像。图像及其相关标签从Python的Pickle二进制表示中读取。
- en: Each CIFAR-10 image is of 32 × 32, with three channels, or one value for red,
    green, and blue pixels. You converted the three channels to one-channel grayscale
    by averaging their three values.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个CIFAR-10图像的大小为32 × 32，具有三个通道，或红色、绿色和蓝色像素的一个值。您通过平均它们的三个值将三个通道转换为单通道灰度。
- en: You created an `autoencoder` class with a single hidden layer with 100 neurons,
    which took as input the set of 5,000 images with 1,024 pixels of grayscale intensity
    and reduced the size in a hidden layer to 100 values during the encoding step.
    The encoding step uses TensorFlow, learns via training, and uses root mean-squared
    error (RMSE) as the loss function and associated optimizer to learn the encoding
    weights (We) and biases (Be) for your hidden layer encoding.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您创建了一个`autoencoder`类，它有一个包含100个神经元的单个隐藏层，该层以5,000个具有1,024个灰度强度的像素的图像集作为输入，在编码步骤中将大小减少到100个值。编码步骤使用TensorFlow，通过训练学习，并使用均方根误差（RMSE）作为损失函数及其相关的优化器来学习隐藏层编码的编码权重（We）和偏置（Be）。
- en: The decoding portion of your autoencoder also learns associated weights (Wd)
    and biases (Bd) and is codified in the `test` function, which you’ll use in this
    section. You’ll also create a `classify` function that will show off the learned
    representation of horse images that your network managed.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自动编码器的解码部分也学习相关的权重（Wd）和偏置（Bd），并在`test`函数中编码，您将在本节中使用它。您还将创建一个`classify`函数，该函数将展示您的网络管理的马图像的学得表示。
- en: The learned representation of your image data is the encoding hidden layer and
    its associated learned weights in your autoencoder. The overall process flow is
    shown in figure 12.3.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 您的图像数据的学得表示是自动编码器中的编码隐藏层及其相关的学得权重。整个过程流程如图12.3所示。
- en: '![CH12_F03_Mattmann2](../Images/CH12_F03_Mattmann2.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![CH12_F03_Mattmann2](../Images/CH12_F03_Mattmann2.png)'
- en: Figure 12.3 The overall CIFAR-10 autoencoding process. Images of size 32 × 32
    with three channels are reshaped to 1,024 grayscale pixels.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3 整个CIFAR-10自动编码过程。大小为32 × 32，具有三个通道的图像被重塑为1,024个灰度像素。
- en: Now let’s see some information about the learned encoding of images. You can
    use the `Autoencoder.test` function to print the values of the hidden layer and
    its values for reconstructing the inputs. You can try it on the 1000 test CIFAR-10
    horse images, but first, you’ll have to load the images (listing 12.5). CIFAR-10’s
    test data comes in Python Pickle format. Loading it and converting it to grayscale
    should be familiar, as should selecting the `horse` class indices (class `ID=7`)
    and selecting the horse images from the 1,000-image test set. Then you will run
    `Autoencoder.test` to get an idea of how well the autoencoder performed. As you
    may remember from listing 12.1, the `test` function loads the hidden encoded layer,
    runs the decoder step, and shows the original input values.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看关于图像学得编码的一些信息。您可以使用`Autoencoder.test`函数打印隐藏层的值及其重建输入的值。您可以在1,000个测试CIFAR-10马图像上尝试它，但首先，您必须加载图像（列表12.5）。CIFAR-10的测试数据以Python
    Pickle格式提供。加载它并将其转换为灰度应该很熟悉，选择`horse`类索引（类`ID=7`）和从1,000个图像的测试集中选择马图像也是如此。然后您将运行`Autoencoder.test`以了解自动编码器的性能如何。如您从列表12.1中可能记得的那样，`test`函数加载隐藏编码层，运行解码步骤，并显示原始输入值。
- en: Listing 12.5 Loading test CIFAR-10 images and evaluating the `Autoencoder`
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.5 加载测试CIFAR-10图像并评估`Autoencoder`
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Loads the pickled 10,000 test images for CIFAR-10 by reading the ‘data’ and
    ‘labels’ keys from the resulting deserialized dictionary
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过读取序列化字典中的‘data’和‘labels’键，加载了CIFAR-10的10,000个测试图像的pickle格式
- en: ❷ Selects the indices of the horse class ID=7 and indexes into the image data
    array for the 1,000 horse images
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 选择马类ID=7的索引，并索引到图像数据数组中的1,000个马图像
- en: ❸ Runs the Autoencoder test method to evaluate the Autoencoder
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 运行自动编码器测试方法来评估自动编码器
- en: 'The output from running listing 12.5 follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 运行列表12.5的输出如下：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Python and NumPy does the lovely summarization:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Python和NumPy进行了可爱的总结：
- en: Each test horse image of the 1,000 includes 1,024 image pixels, so each row
    in the `input` is 1,024 numbers.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1,000个测试马图像中的每个图像包含1,024个图像像素，因此`input`中的每一行是1,024个数字。
- en: The `compressed` layer is 100 neurons, turned on (`1.`) or off (`0.`).
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`compressed`层有100个神经元，开启（`1.`）或关闭（`0.`）。'
- en: The `reconstructed` values are the values restored after the decode step of
    the autoencoder.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reconstructed`值是自动编码器解码步骤之后恢复的值。'
- en: A quick scan of the reconstruction for image values shows quite a variance.
    The first three numbers in the original input image—`34`, `60.66666667`, and `36.33333333`—are
    reconstructed as `88.69392`, `93.134995`, `93.69954` by the autoencoder. Similar
    differences exist in other images and in other pixels, but don’t get too worked
    up about them. The autoencoder was trained by default for only 100 epochs, with
    a learning rate of 0.001\. As I explained in earlier chapters, hyperparameter
    tuning is an active area of research. You can tweak these hyperparameters and
    obtain better values. You could even do other optimizations and tricks that I
    will teach you when convolutional neural networks (CNNs) and data augmentation
    come into the picture, but for now, using the `test` method is a quick-and-dirty
    way to evaluate the autoencoder. A better way may be to visualize the learned
    representation of the images compared with their originals and visually compare
    how well they match. I’ll show you how, but first, I’ll demonstrate another computational
    method to evaluate the autoencoder.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 快速扫描图像值的重建显示出了相当大的差异。原始输入图像中的前三个数字——`34`、`60.66666667`和`36.33333333`——被自动编码器重建为`88.69392`、`93.134995`和`93.69954`。在其他图像和其他像素中存在类似差异，但不要过于担心它们。自动编码器默认训练了仅100个epoch，学习率为0.001。正如我在前面的章节中解释的，超参数调整是一个活跃的研究领域。您可以调整这些超参数并获得更好的值。您甚至可以执行其他优化和技巧，这些技巧我将在卷积神经网络（CNN）和数据增强出现时教给您，但到目前为止，使用`test`方法是一种快速而简单的方法来评估自动编码器。一种更好的方法可能是将图像的学习表示与其原始图像进行可视化比较，并直观地比较它们匹配得有多好。我会向您展示如何做，但首先，我将演示另一种计算方法来评估自动编码器。
- en: 12.2.1 Using the autoencoder as a classifier via loss
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2.1 通过损失使用自动编码器作为分类器
- en: Comparing the values from the reconstructed images with the original input shows
    a variance between the reconstructed pixel values and the original pixel values.
    This variance is easily measured with the loss function set up to guide the autoencoder
    during training. You used RMSE. RMSE is the difference between the model-generated
    data and the input, squared, reduced to a scalar via the mean average and then
    the square root of that. RMSE is a nice distance measure for evaluating loss for
    training. Even better, it’s a good classification indication, because as it turns
    out, images that belong to a particular image class have a similar loss from the
    model value when compared with the original value.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 将重建图像的值与原始输入值进行比较，显示出重建像素值与原始像素值之间的差异。这种差异可以通过在训练过程中引导自动编码器的损失函数轻松测量。您使用了RMSE。RMSE是模型生成数据与输入之间的差异，平方后，通过平均值降低到标量，然后取平方根。RMSE是评估训练损失的一个很好的距离度量。更好的是，它是一个很好的分类指示，因为结果证明，属于特定图像类的图像与原始值相比，模型值产生的损失是相似的。
- en: 'Horse images have a particular loss, airplane images have a different loss,
    and so on. You can test this intuition by adding a simple method to the `autoencoder`
    class: `classify`. This method’s job will be to compute the loss function for
    the `horse` class and then compare the loss generated for those images with that
    of the loss for the other image classes and see whether there is a difference.
    Listing 12.6 adds the `classify` method to the `autoencoder` class and returns
    the hidden and reconstructed layers for later use.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 马的图像有特定的损失，飞机的图像有不同的损失，等等。您可以通过向`autoencoder`类添加一个简单的方法来测试这种直觉：`classify`。这个方法的工作将是计算`horse`类的损失函数，然后比较这些图像产生的损失与其他图像类别的损失，看看是否存在差异。列表12.6向`autoencoder`类添加了`classify`方法，并返回隐藏和重建层以供以后使用。
- en: Listing 12.6 `Autoencoder.classify` comparing loss across classes
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.6 `Autoencoder.classify` 比较不同类别的损失
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Initializes the TensorFlow trained model and loads it
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化并加载TensorFlow训练的模型
- en: ❷ Obtains the hidden (encoded) layer and its reconstruction for use in computing
    loss
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取隐藏（编码）层及其重建，用于计算损失
- en: ❸ Uses NumPy to compute the loss RMSE for all images
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用NumPy计算所有图像的损失RMSE
- en: ❹ Computes the indices of the horse images and the indices of all other classes,
    and computes the mean loss value across all images (horse or not horse)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算马的图像索引和所有其他类别的索引，并计算所有图像（马或非马）的平均损失值
- en: ❺ Prints the horse loss and the not horse loss from the classifier
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 打印出分类器的马损失和非马损失
- en: 'Following is the output from listing 12.6:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出来自列表12.6：
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: And though not staggering, the loss value from image to reconstruction for `horse`
    is clearly different and statistically significant compared with that of the other
    image classes. So although the autoencoder may not have enough experience to reconstruct
    the image with little loss, it learned enough of a representation of horse images
    to be able to distinguish their structure from that of the other nine image classes
    in CIFAR-10\. Cool!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管不是令人震惊的，但与其他图像类别相比，`horse`从图像到重构的损失值明显不同，并且具有统计学意义。所以尽管自动编码器可能没有足够的经验以少量损失重构图像，但它已经学会了足够多的马图像表示，能够从CIFAR-10中的其他九个图像类别中区分其结构。酷！
- en: Now you can look at the difference in the reconstruction of some of the images.
    A little Matplotlib goes a long way. You can take the hidden layer returned from
    the `classify` function and a small `decode` method to your autoencoder, which
    will generate the reconstruction for a particular encoded image and reconvert
    it from 1,024 pixels to a 32 × 32 grayscale image. The code is in listing 12.7.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以查看一些图像重构的差异。一点Matplotlib就能走得很远。你可以将`classify`函数返回的隐藏层和一个小型的`decode`方法应用到你的自动编码器上，这将生成特定编码图像的重构，并将其从1,024像素重新转换为32
    × 32的灰度图像。代码在列表12.7中。
- en: Listing 12.7 The decode method to convert an encoded image back to CIFAR-10
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.7 将编码图像转换回CIFAR-10的解码方法
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Loads TensorFlow and the stored Autoencoder model to generate the image reconstruction
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载TensorFlow和存储的自动编码器模型以生成图像重构
- en: ❷ Reshapes the reconstructed 1,024-pixel array into a 32 × 32 grayscale image
    and returns it
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将重构的1,024像素数组重塑为32 × 32的灰度图像并返回
- en: After you run that code, you can use Matplotlib (listing 12.8) to generate a
    set of reconstructions of the first 20 horse images from the CIFAR-10 test dataset,
    with the original images on the left and what the autoencoder “sees” on the right.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 运行该代码后，你可以使用Matplotlib（列表12.8）生成CIFAR-10测试数据集中前20张马图像的一组重构，左侧是原始图像，右侧是自动编码器“看到”的图像。
- en: Listing 12.8 Evaluating and visualizing CIFAR-10 reconstructed horse images
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.8 评估和可视化CIFAR-10重构马图像
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Sets up the Matplotlib plot to be a set of 100 × 100 figure areas (rows)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置Matplotlib图表为100 × 100的图区域（行）
- en: ❷ Iterates through the first 20 test horse images from CIFAR-10
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历CIFAR-10的前20张测试马图像
- en: ❸ For each CIFAR-10 test horse image, shows it on the left side of the column
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 对于每个CIFAR-10测试马图像，在列的左侧显示
- en: ❹ For each CIFAR-10 test horse image, shows the autoencoder reconstruction on
    the right side of the column
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 对于每个CIFAR-10测试马图像，在列的右侧显示自动编码器重构
- en: ❺ Shows the plot
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 显示图表
- en: Figure 12.4 shows the results.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4显示了结果。
- en: '![CH12_F04_Mattmann2](../Images/CH12_F04_Mattmann2.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![CH12_F04_Mattmann2](../Images/CH12_F04_Mattmann2.png)'
- en: 'Figure 12.4 A subset of the output from listing 12.8 showing 3 of the first
    20 CIFAR-10 test horse images: the original CIFAR-10 test horse image on the left
    and what the autoencoder “sees” on the right'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4 显示列表12.8输出的一部分，展示了前20张CIFAR-10测试马图像中的3张：左侧是原始的CIFAR-10测试马图像，右侧是自动编码器“看到”的图像
- en: 'Now that you have both numerical and visual ways of evaluating your autoencoder,
    I’ll show you another type of autoencoder: the denoising autoencoder.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经有了评估你的自动编码器的数值和视觉方法，我将向你展示另一种类型的自动编码器：去噪自动编码器。
- en: The results of examining the autoencoder representation on the right side of
    figure 12.4 are fascinating if you think about them. Based on a few of the representations
    on the right, the autoencoder clearly has some basic understanding of the horse
    features versus the background and the general shape of the object. I typically
    think of an autoencoder representation as being what you would think about when
    you close your eyes and try to reimagine something you saw in the distant past.
    Sometimes, imagination allows you to remember the recent future fairly well, but
    many of your representations of the distant past look like the autoencoder-generated
    images on the right side of figure 12.4—basic features, shapes, and differentiation
    with background, but not perfect reconstructions by any means. If an autoencoder
    can learn to take imperfection into account when remembering the images, it can
    build a more resilient model. That’s where the denoising autoencoder comes in
    to play.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仔细思考，检查图12.4右侧的自动编码器表示的结果是非常有趣的。基于右侧的一些表示，自动编码器显然对马的特征与背景以及物体的一般形状有一些基本理解。我通常将自动编码器表示想象成当你闭上眼睛尝试重新想象你很久以前看到的东西时你会想到的东西。有时，想象力可以让你相当好地记住最近未来，但你对遥远过去的许多表示看起来就像图12.4右侧的自动编码器生成的图像——基本特征、形状和与背景的区别，但绝不是完美的重建。如果一个自动编码器能够学会在记住图像时考虑不完美，它就能构建一个更健壮的模型。这就是降噪自动编码器发挥作用的地方。
- en: 12.3 Denoising autoencoders
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.3 降噪自动编码器
- en: In real life, we humans use our minds to think about something for a while and
    improve the way that we remember it. Suppose that you are trying to remember playing
    fetch with your family dog, and you struggle to remember what he looked like or
    the black patch of fur under his rosy nose. Seeing more dogs with black patches
    in a similar area or with slightly different patches, perhaps of different colors,
    might trigger a better reconstruction in your memory.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，我们人类用我们的思维去思考一段时间，以改善我们记住东西的方式。假设你正在试图记住和家人狗狗玩接球的样子，你努力回忆起它看起来像什么，或者它玫瑰色鼻子下的黑色毛斑。看到更多在类似区域有黑色毛斑的狗，或者有略微不同毛斑的狗，可能是不同颜色的，可能会触发你记忆中更好的重建。
- en: Part of the reason is that our minds build visual models that get better by
    seeing more examples. I’ll discuss this topic more in chapter 14, which covers
    CNNs, which are a learning approach and network architecture focused on delineating
    lower- and higher-level image features. Additional images that are extremely similar
    to the one you are trying to recall—such as an old pet, perhaps—help you focus
    on the important features of that image. More examples of dissimilar images help
    too. Images with noise in them—such as variations in light, hue, contrast, or
    other repeatable differences—build a more resilient model and memory of the original
    image.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一部分原因是我们的思维构建视觉模型，通过看到更多例子而变得更好。我将在第14章中更详细地讨论这个话题，该章涵盖了CNN，这是一种专注于描绘低级和高级图像特征的学习方法和网络架构。与你要回忆的那张图像极其相似的其他图像——比如一只老宠物——有助于你关注那张图像的重要特征。更多不同图像的例子也有帮助。带有噪声的图像——比如光、色调、对比度或其他可重复差异的变化——构建了一个更健壮的模型和原始图像的记忆。
- en: Drawing from this inspiration, denoising autoencoders introduce some noise by
    passing pixel values through a Gaussian function or randomly masking some pixels
    by turning them off (or on) from the original image to build a more resilient,
    robust representation of the image your network is trying to learn. You need to
    modify the autoencoder original class only slightly to create a denoising autoencoder
    and try it on your CIFAR-10 data. Listing 12.9 sets up the `Denoiser` class with
    a slight modification to the function to obtain batch data. Your `Denoiser` will
    maintain a parallel noised version of the input and as such will need to account
    for it in batch functions and throughout the remainder of the class. Most of the
    other methods you will recall from the `autoencoder` class—`train`, `test`, `classify`,
    and `decode`—work the same way as in the other listings in this chapter except
    for `train`, highlighted in list-ing 12.11.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个灵感出发，去噪自编码器通过将像素值通过高斯函数或随机屏蔽一些像素（通过关闭或打开原始图像中的像素）来引入一些噪声，从而构建一个更健壮、更鲁棒的网络学习图像的表示。你需要稍微修改自动编码器原始类来创建去噪自编码器，并在你的
    CIFAR-10 数据上尝试它。列表 12.9 通过对获取批量数据的函数进行轻微修改来设置 `Denoiser` 类。你的 `Denoiser` 将维护输入的并行噪声版本，因此需要在批量函数和类的其余部分中考虑这一点。你将记住的大多数其他方法——`train`、`test`、`classify`
    和 `decode`——与本章其他列表中的方法工作方式相同，除了 `train`，这在列表 12.11 中突出显示。
- en: Listing 12.9 The denoiser autoencoder
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.9 去噪自编码器
- en: '[PRE10]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Gets a batch of training data input, along with its noised version
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取一批训练数据输入，以及其噪声版本
- en: ❷ Sets up the placeholder for the noised version of the input data
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置输入数据噪声版本的占位符
- en: ❸ Creates the weights and biases and encoded data layer, using the sigmoid function
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用 sigmoid 函数创建权重、偏置和编码数据层
- en: ❹ Creates the weights and biases and decoded data layer for emitting the learned
    representation
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 创建权重、偏置和解码数据层，用于发射学习到的表示
- en: ❺ Sets up the loss and training operation using the AdamOptimizer
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用 AdamOptimizer 设置损失和训练操作
- en: The biggest difference in the setup for the denoiser is the storage of a parallel
    TensorFlow placeholder for the noised-up version of the input data that the encoding
    step uses instead of the original input. You could take a few approaches. You
    could use a Gaussian function and sample random pixels and mask out the input
    pixels according to some frequency, for example. These random pixels are—you guessed
    it—another hyperparameter in the machine learning modeling process. There isn’t
    a best approach, so experiment with methods and pick the best one for your use
    case. Your `Denoiser` will implement two noise approaches—Gaussian and random
    mask—in listing 12.10.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于去噪器的设置中最大的不同之处在于存储一个并行 TensorFlow 占位符，用于存储输入数据的噪声版本，编码步骤使用这个版本而不是原始输入。你可以采取几种方法。例如，你可以使用高斯函数并采样随机像素，然后根据某些频率屏蔽输入像素。这些随机像素——正如你所猜想的——是机器学习建模过程中的另一个超参数。没有最佳方法，所以尝试不同的方法，并选择最适合你用例的方法。你的
    `Denoiser` 将在列表 12.10 中实现两种噪声方法——高斯和随机屏蔽。
- en: Listing 12.10 The `add_noise` method for your `Denoiser`
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.10 为你的 `Denoiser` 添加噪声的方法
- en: '[PRE11]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Adds a value between 0 and 0.1 along a Gaussian random function to each pixel
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在高斯随机函数上为每个像素添加介于 0 和 0.1 之间的值
- en: ❷ Picks some percentage provided by frac of the overall input pixels to set
    to value 0 randomly
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 根据提供的百分比 frac 选择整体输入像素的一部分随机设置为值 0
- en: ❸ Returns the noised data
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回噪声数据
- en: With noised data, the `train` method uses the `get_batch_n` method, which returns
    noised and regular batch input data for training. Additionally, the remaining
    methods for `test`, `classify`, and `decode` are unchanged other than to provide
    the noised data so that the appropriate tensors can be returned. Listing 12.11
    completes the `Denoiser` class with the exception of the `decode` function, which
    is the same as that of listing 12.7.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用噪声数据，`train` 方法使用 `get_batch_n` 方法，该方法返回用于训练的噪声和常规批量输入数据。此外，`test`、`classify`
    和 `decode` 的其余方法没有变化，除了提供噪声数据，以便返回适当的张量。列表 12.11 通过省略 `decode` 函数（与列表 12.7 中的相同）来完成
    `Denoiser` 类。
- en: Listing 12.11 The rest of the `Denoiser` class
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.11 `Denoiser` 类的其余部分
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Creates a noised version of the data with the add_noise function
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 `add_noise` 函数创建数据的噪声版本
- en: ❷ Creates a new TensorFlow session for training
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个新的 TensorFlow 会话用于训练
- en: ❸ Uses the noised data
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用噪声数据
- en: ❹ Gets a noised version of the data for testing and classification
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 获取用于测试和分类的数据的噪声版本
- en: 'One thing that you will notice when looking at the output of the test function
    is that with only 1,000 test samples and noise, the `Denoiser` has lost some of
    its early gains in classification power and ability to distinguish among image
    classes because there is little difference in mean loss between the `horse` and
    `not horse` classes:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当你查看测试函数的输出时，你会注意到，由于只有1,000个测试样本和噪声，`去噪器`在分类能力和区分图像类别方面的早期收益有所下降，因为在`马`和`非马`类别之间平均损失的差异很小：
- en: '[PRE13]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'But not to worry, because over time, the `Denoiser` autoencoder will learn
    a more robust model of the image features, resilient to fluctuations and imperfections
    in training data, to develop a more solid model. You need more data than CIFAR-10
    has for you for the `horse` class at the moment. Figure 12.5 shows the image representation
    of what the `Denoiser` learns for three of the first 20 horse images from the
    CIFAR-10 test set (on the left). There’s one more autoencoder to learn about:
    the stacked or deep autoencoder. Onward to the finish line!'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 但不必担心，因为随着时间的推移，`去噪器`自编码器将学会一个更鲁棒的特征模型，对训练数据中的波动和不完美具有抵抗力，以发展一个更稳固的模型。对于目前你拥有的`马`类别，你需要比CIFAR-10更多的数据。图12.5显示了`去噪器`从CIFAR-10测试集的前20张马图像中学习到的图像表示（左侧）。还有另一个自编码器需要了解：堆叠或深度自编码器。向着终点前进！
- en: '![CH12_F05_Mattmann2](../Images/CH12_F05_Mattmann2.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![CH12_F05_Mattmann2](../Images/CH12_F05_Mattmann2.png)'
- en: Figure 12.5 The noised version of the autoencoder and its representations (right)
    of the original CIFAR-10 test horse images (left)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5 自编码器的噪声版本及其表示（右侧）和原始CIFAR-10测试马图像（左侧）
- en: 12.4 Stacked deep autoencoders
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.4 堆叠深度自编码器
- en: Another class of autoencoders is referred to as stacked or deep autoencoders.
    Instead of having a single hidden layer that handles the encoding step on the
    input end and ends the decoding with a single layer of hidden neurons on the output
    end, stacked autoencoders have several layers of hidden neurons, using the previous
    layer to parameterize and tune the learned representation of the future layers.
    Future layers in these autoencoders have fewer neurons. The goal is to create
    and learn the optimal settings for an even more compressed interpretation of your
    input data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类自编码器被称为堆叠或深度自编码器。与只有一个隐藏层处理输入端的编码步骤并在输出端以一个隐藏神经元的单层结束解码不同，堆叠自编码器有多个隐藏神经元层，使用前一层的参数化和调整来学习未来层的表示。这些自编码器的未来层具有更少的神经元。目标是创建和学会对输入数据进行更压缩解释的最佳设置。
- en: 'In practice, each hidden layer in the stacked autoencoder learns a parameterization
    of a set of optimal features to achieve optimal compressed representation in the
    adjacent layers. You could think of each layer as representing a set of higher-order
    features, and because you don’t know precisely what they are yet, this concept
    is a useful way to simplify the complexity of the problem. In chapter 14, you’ll
    explore some ways to visualize these representations, but for now, the lesson
    from chapter 11 holds: as you add more hidden neurons that are not linear, your
    autoencoder can learn and represent any function. Now you can take input and make
    it even smaller!'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，堆叠自编码器中的每一层隐藏层都学习一组最优特征的参数化，以在相邻层中实现最优的压缩表示。你可以将每一层视为代表一组高阶特征，因为你还不知道它们具体是什么，所以这个概念是简化问题复杂性的有用方法。在第14章中，你将探索一些可视化这些表示的方法，但就目前而言，第11章的教训仍然适用：随着你添加更多非线性的隐藏神经元，你的自编码器可以学习和表示任何函数。现在你可以接受输入并将其变得更小了！
- en: Your `StackedAutoencoder` class is a fairly straightforward adaption of the
    previous autoencoders, as you see in listing 12.12\. You will create hidden layers
    with neurons amounting to half of the provided input. The first hidden layer is
    `input_dim` `/2`, the second is `input_dim` `/4,` and so on. With three hidden
    layers by default, your autoencoder learns an encoding that is a fourth the size
    of the original input, so for CIFAR-10, it learns a representation of the input
    with 256 numbers. The last hidden layer is sized `input_dim` `/2`, and the final
    output layer is size `input_dim`. The class methods are similar to previous autoencoders
    except that they operate on the N-1 encoding layer of the architecture.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 您的 `StackedAutoencoder` 类是对之前自动编码器的相当直接的改编，正如您在列表 12.12 中看到的。您将创建具有与提供的输入数量一半的神经元的隐藏层。第一个隐藏层是
    `input_dim` `/2`，第二个是 `input_dim` `/4`，依此类推。默认情况下，有三个隐藏层，您的自动编码器学习到的编码大小是原始输入的四分之一，因此对于
    CIFAR-10，它学习了一个包含 256 个数字的输入表示。最后一个隐藏层的大小是 `input_dim` `/2`，最终的输出层大小是 `input_dim`。类方法与之前的自动编码器类似，只是它们在架构的
    N-1 编码层上操作。
- en: Listing 12.12 The `StackedAutoencoder` class
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.12 `StackedAutoencoder` 类
- en: '[PRE14]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Creates a StackedAutoencoder with 3 hidden layers, batch size of 250, and
    learning rate 0.01, and trains for 100 epochs
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个具有 3 个隐藏层、批量大小为 250 和学习率为 0.01 的 StackedAutoencoder，并训练 100 个周期
- en: ❷ Input dimensions for each hidden layer
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 每个隐藏层的输入维度
- en: ❸ Number of hidden neurons for each hidden layer
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 每个隐藏层的隐藏神经元数量
- en: ❹ The tensors representing each hidden layer activated by the act function (tf.nn.relu)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 由激活函数（tf.nn.relu）激活的每个隐藏层的张量
- en: ❺ The weights for each hidden layer
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 每个隐藏层的权重
- en: ❻ The biases for each hidden layer
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 每个隐藏层的偏差
- en: ❼ Uses the tf.nn.relu (rectifying linear unit) activation function
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用 tf.nn.relu（整流线性单元）激活函数
- en: ❽ Performs the network construction input to hidden layers
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 执行网络从输入层到隐藏层的构建
- en: ❾ Inputs dimensions, initially the size of the input and then the output from
    the previous hidden layer
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 输入维度，最初是输入大小，然后是前一个隐藏层的输出大小
- en: ❿ Number of hidden neurons (input_dim /2) for each hidden layer
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 每个隐藏层的隐藏神经元数量（input_dim /2）
- en: ⓫ The hidden layer is the matrix multiplication of the input × weights plus
    the biases, or the previous layer encoding × weights plus the biases.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 隐藏层是输入 × 权重加上偏差的矩阵乘积，或者是前一层编码 × 权重加上偏差。
- en: ⓬ Creates the output layer, which is of dimension layer N-1 × output size, or
    original input size.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 创建输出层，其维度为 layer N-1 × 输出大小，或原始输入大小
- en: The `train` method is fairly straightforward, though I introduce you briefly
    to the TensorFlow `Dataset` API, a powerful system for easily manipulating and
    preparing data for training. Instead of rewriting the same batch method over and
    over, you can use `tf.Dataset` and its methods to batch and shuffle the data (in
    a randomized fashion) to prepare it for training. You’ll see other useful methods
    in chapter 14\. The key takeaways for `tf.Dataset` are the ability to automatically
    set `shuffle` and `batch` parameters ahead of time and then to use an iterator
    to get batches of size `batch_size` that are automatically shuffled in random
    order so that your network doesn’t memorize the order of the input data. When
    the iterator is exhausted, you are done with the epoch, and you can catch a `tf.errors.OutOfRangeException`
    to account for this case.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`train` 方法相当直接，尽管我简要介绍了 TensorFlow `Dataset` API，这是一个用于轻松操作和准备训练数据的强大系统。您不必反复重写相同的批量方法，可以使用
    `tf.Dataset` 和其方法来批量处理和打乱数据（以随机方式），以便为训练做准备。您将在第 14 章中看到其他有用的方法。`tf.Dataset` 的关键要点是能够提前自动设置
    `shuffle` 和 `batch` 参数，然后使用迭代器获取大小为 `batch_size` 的批次，这些批次会自动以随机顺序打乱，这样您的网络就不会记住输入数据的顺序。当迭代器耗尽时，您就完成了这个时代，您可以捕获一个
    `tf.errors.OutOfRangeException` 来处理这种情况。'
- en: Listing 12.13 The `train` method for `StackedAutoencoder` and `tf.Dataset`
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.13 `StackedAutoencoder` 和 `tf.Dataset` 的 `train` 方法
- en: '[PRE15]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Creates a new tf.Dataset from tensor slices in this case, the input data a
    NumPy array
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从这个案例中的张量切片创建一个新的 tf.Dataset，即输入数据是一个NumPy数组
- en: ❷ Sets the shuffle size for randomized sample selection from the input
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置从输入中进行随机样本选择的打乱大小
- en: ❸ Sets the size of the batch in each batch training step
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 设置每个批量训练步骤中批量大小的参数
- en: ❹ Uses an iterator that can be initialized with the input data in each batch
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用一个迭代器，该迭代器可以用每个批次的输入数据初始化
- en: ❺ Gets the next batch of size randomly shuffled from the original input set
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 从原始输入集中随机打乱的大小获取下一个批次
- en: ❻ Catches the exception, indicating that the dataset has been exhausted in this
    epoch, and then moves to the save epoch after saving the mode
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 捕获异常，指示在此个epoch中数据集已被耗尽，然后在保存模式后移动到保存的epoch
- en: TensorFlow’s Dataset API is elegant and simplifies common batch-training techniques,
    as you saw with the `train` method in listing 12.13\. The rest of the `StackedAutoencoder`
    is more or less the same as the other autoencoders, except that you use the N-1
    layer encodings for classification and for the decoding step. Listing 12.14 defines
    the rest of the class.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 的 Dataset API 精美且简化了常见的批量训练技术，正如你在列表 12.13 中的 `train` 方法所看到的。`StackedAutoencoder`
    的其余部分与其它自动编码器大致相同，只是你使用 N-1 层编码进行分类和解码步骤。列表 12.14 定义了其余的类。
- en: Listing 12.14 The rest of the `StackedAutoencoder` method
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.14 `StackedAutoencoder` 类的其余方法
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Computes the last layer before output, based on the input, and then decodes
    that layer
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 根据输入计算输出前的最后一层，然后解码该层
- en: ❷ Prints the input data
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打印输入数据
- en: ❸ Prints the activated N-1 layer neuron values
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打印激活的 N-1 层神经元值
- en: ❹ Prints the reconstructed data
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 打印重建的数据
- en: ❺ Uses the last N-1 layer for classification
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用最后一个 N-1 层进行分类
- en: ❻ Prints the loss from the horse test class
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 打印来自马测试类别的损失
- en: ❼ Prints the loss from all other image classes
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 打印来自所有其他图像类别的损失
- en: ❽ Uses the N-1 layer to obtain the output layer and returns the reconstructed
    32 × 32 image
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 使用 N-1 层来获取输出层并返回重建的 32 × 32 图像
- en: The output of the test method shows that the `StackedEncoder` N-1 layer encodings
    are enough to delineate between the horse class and other image classes, though
    the encodings show less variation than the original autoencoder. Interestingly
    enough, though, the reconstructed images show a more compact overall representation
    of features, and some grainy higher-order features are present in the masked-out
    artifacts on the right side of the representations of the original CIFAR-test
    horse images in figure 12.6.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 测试方法的输出显示，`StackedEncoder` 的 N-1 层编码足以区分马类和其他图像类别，尽管编码显示的变异性比原始自动编码器少。然而，有趣的是，重建的图像显示了特征的整体更紧凑的表示，并且在图
    12.6 中原始 CIFAR-test 马图像表示的右侧的遮挡区域中存在一些粗糙的高阶特征。
- en: '![CH12_F06_Mattmann2](../Images/CH12_F06_Mattmann2.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![CH12_F06_Mattmann2](../Images/CH12_F06_Mattmann2.png)'
- en: Figure 12.6 Stacked autoencoder representations (right) of the three CIFAR-10
    test images (left). Higher order features are visible in masked-out grainy areas.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.6 三张 CIFAR-10 测试图像（左）的堆叠自动编码器表示（右）。高阶特征在遮挡的粗糙区域中可见。
- en: '[PRE17]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now that you have explored different autoencoders and their benefits and trade-offs
    for classification and detection related to images, you’re all set to apply them
    to other real datasets!
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经探索了不同的自动编码器以及它们在图像分类和检测方面的优势和权衡，您已经准备好将它们应用于其他真实数据集了！
- en: Summary
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Autoencoders can be used to classify, sort, and cluster images by learning a
    representation of them with neural network hidden layers.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器可以通过使用神经网络隐藏层学习图像的表示来用于分类、排序和聚类图像。
- en: CIFAR-10 is a widely used image dataset with 10 classes of images (including
    `horse`, `bird`, and `automobile`). CIFAR-10 includes 5,000 images per class for
    training (50,000 total) and 10,000 total images for testing (1,000 per class).
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CIFAR-10 是一个广泛使用的图像数据集，包含 10 类图像（包括 `horse`、`bird` 和 `automobile`）。CIFAR-10
    包含每类 5,000 张训练图像（总计 50,000 张）和 10,000 张测试图像（每类 1,000 张）。
- en: Specializations of autoencoders can be used to learn more robust representations
    of input such as images by handling noise in data features or by using denser
    architectures with more layers. These autoencoders are called denoising and stacked
    autoencoders.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器的特殊化可以通过处理数据特征中的噪声或使用具有更多层的更密集架构来学习输入（如图像）的更鲁棒表示。这些自动编码器被称为去噪和堆叠自动编码器。
- en: You can evaluate autoencoders on image data by comparing the learned representation
    values with their original results by hand or by using Matplotlib and other helper
    libraries.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过比较通过手动或使用 Matplotlib 和其他辅助库学习到的表示值与它们的原始结果来评估图像数据上的自动编码器。
