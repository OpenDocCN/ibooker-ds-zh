- en: Chapter 9\. Deploy Models to Production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。将模型部署到生产环境
- en: In previous chapters, we demonstrated how to train and optimize models. In this
    chapter, we shift focus from model development in the research lab to model deployment
    in production. We demonstrate how to deploy, optimize, scale, and monitor models
    to serve our applications and business use cases.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们演示了如何训练和优化模型。在本章中，我们将焦点从研究实验室中的模型开发转移到生产环境中的模型部署。我们展示了如何部署、优化、扩展和监控模型以服务我们的应用程序和业务用例。
- en: We deploy our model to serve online, real-time predictions and show how to run
    offline, batch predictions. For real-time predictions, we deploy our model via
    SageMaker Endpoints. We discuss best practices and deployment strategies, such
    as canary rollouts and blue/green deployments. We show how to test and compare
    new models using A/B tests and how to implement reinforcement learning with multiarmed
    bandit (MAB) tests. We demonstrate how to automatically scale our model hosting
    infrastructure with changes in model-prediction traffic. We show how to continuously
    monitor the deployed model to detect concept drift, drift in model quality or
    bias, and drift in feature importance. We also touch on serving model predictions
    via serverless APIs using Lambda and how to optimize and manage models at the
    edge. We conclude the chapter with tips on how to reduce our model size, reduce
    inference cost, and increase our prediction performance using various hardware,
    services, and tools, such as the AWS Inferentia hardware, SageMaker Neo service,
    and TensorFlow Lite library.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将模型部署以提供在线实时预测，并展示如何进行离线批处理预测。对于实时预测，我们通过SageMaker端点部署模型。我们讨论最佳实践和部署策略，例如金丝雀发布和蓝/绿部署。我们展示如何使用A/B测试测试和比较新模型，以及如何使用多臂老虎机（MAB）测试实现强化学习。我们展示如何根据模型预测流量的变化自动扩展我们的模型托管基础设施。我们展示如何持续监控部署的模型以检测概念漂移、模型质量或偏差漂移以及特征重要性的漂移。我们还涉及通过Lambda提供服务器无API来提供模型预测的内容，以及如何优化和管理边缘模型。我们通过如何减少模型大小、降低推理成本以及使用AWS
    Inferentia硬件、SageMaker Neo服务和TensorFlow Lite库等各种硬件、服务和工具来增加预测性能结束本章。
- en: Choose Real-Time or Batch Predictions
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择实时或批处理预测
- en: We need to understand the application and business context to choose between
    real-time and batch predictions. Are we trying to optimize for latency or throughput?
    Does the application require our models to scale automatically throughout the
    day to handle cyclic traffic requirements? Do we plan to compare models in production
    through A/B tests?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要了解应用程序和业务背景来选择实时和批处理预测。我们是在优化延迟还是吞吐量？应用程序是否需要我们的模型在一天中自动扩展以处理周期性的流量需求？我们是否计划通过A/B测试在生产中比较模型？
- en: If our application requires low latency, then we should deploy the model as
    a real-time API to provide super-fast predictions on single prediction requests
    over HTTPS, for example. We can deploy, scale, and compare our model prediction
    servers with SageMaker Endpoints using the REST API protocol with HTTPS and JSON,
    as shown in [Figure 9-1](#we_can_deploycomma_scalecomma_and_compa).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的应用程序需要低延迟，那么我们应将模型部署为实时API，以便在单个预测请求上通过HTTPS提供超快速预测，例如。我们可以使用SageMaker端点使用REST
    API协议、HTTPS和JSON部署、扩展和比较我们的模型预测服务器，如[图9-1](#we_can_deploycomma_scalecomma_and_compa)所示。
- en: '![](assets/dsaw_0901.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0901.png)'
- en: Figure 9-1\. Deploy model as a real-time REST endpoint.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1。将模型部署为实时REST端点。
- en: For less-latency-sensitive applications that require high throughput, we should
    deploy our model as a batch job to perform batch predictions on large amounts
    of data in S3, for example. We will use SageMaker Batch Transform to perform the
    batch predictions along with a data store like Amazon RDS or DynamoDB to productionize
    the predictions, as shown in [Figure 9-2](#deploying_our_model_as_a_batch_job_to_p).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对延迟敏感度较低但需要高吞吐量的应用程序，我们应将模型部署为批处理作业，在例如S3中的大量数据上执行批处理预测。我们将使用SageMaker批处理转换来执行批处理预测，以及像Amazon
    RDS或DynamoDB这样的数据存储来将预测产品化，如[图9-2](#deploying_our_model_as_a_batch_job_to_p)所示。
- en: '![](assets/dsaw_0902.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0902.png)'
- en: Figure 9-2\. Deploying our model as a batch job to perform batch predictions
    on large amounts of data in S3 using SageMaker Batch Transform.
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2。通过SageMaker批处理转换将我们的模型部署为批处理作业以在S3中执行大量数据的批处理预测。
- en: Real-Time Predictions with SageMaker Endpoints
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SageMaker端点进行实时预测
- en: In 2002, Jeff Bezos, founder of Amazon, wrote a memo to his employees later
    called the “Bezos API Mandate.” The mandate dictated that all teams must expose
    their services through APIs—and communicate with each other through these APIs.
    This mandate addressed the “deadlock” situation that Amazon faced back in the
    early 2000s in which everybody wanted to build and use APIs, but nobody wanted
    to spend the time refactoring their monolithic code to support this idealistic
    best practice. The mandate released the deadlock and required all teams to build
    and use APIs within Amazon.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 2002 年，亚马逊创始人杰夫·贝索斯写了一份备忘录给他的员工，后来被称为“贝索斯 API 法令”。该法令要求所有团队必须通过 API 公开其服务，并通过这些
    API 进行通信。该法令解决了亚马逊在 2000 年初面临的“僵局”情况，即每个人都想构建和使用 API，但没有人愿意花时间重构他们的单块代码以支持这种理想的最佳实践。该法令解决了僵局，并要求所有团队在亚马逊内部构建和使用
    API。
- en: Note
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Seen as the cornerstone of Amazon’s success early on, the Bezos API Mandate
    is the foundation of Amazon Web Services as we know it today. APIs helped Amazon
    reuse their internal ecosystem as scalable managed services for other organizations
    to build upon.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 被视为亚马逊早期成功的基石，贝索斯 API 法令是亚马逊网络服务今日的基础。API 帮助亚马逊将其内部生态系统重复使用为其他组织可构建的可扩展托管服务。
- en: Following the Bezos API Mandate, we will deploy our model as a REST API using
    SageMaker Endpoints. SageMaker Endpoints are, by default, distributed containers.
    Applications invoke our models through a simple RESTful interface, as shown in
    [Figure 9-3](#application_invoking_our_highly_availab), which shows the model
    deployed across multiple cluster instances and Availability Zones for higher availability.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循贝索斯 API 法令，我们将使用 SageMaker 终端节点将我们的模型部署为 REST API。默认情况下，SageMaker 终端节点是分布式容器。应用程序通过简单的
    RESTful 接口调用我们的模型，如 [图 9-3](#application_invoking_our_highly_availab) 所示，该图展示了模型部署在多个集群实例和可用区以提高可用性。
- en: '![](assets/dsaw_0903.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0903.png)'
- en: Figure 9-3\. Application invoking our highly available model hosted on a REST
    endpoint.
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-3\. 应用调用我们托管在 REST 终端点上的高可用模型。
- en: Deploy Model Using SageMaker Python SDK
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SageMaker Python SDK 部署模型
- en: There are two ways to deploy the model using the SageMaker Python SDK. We can
    call `deploy()` on a model object, or we can call `deploy()` on the SageMaker
    estimator object that we used to train the model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SageMaker Python SDK 部署模型有两种方式。我们可以在模型对象上调用 `deploy()`，或者在用于训练模型的 SageMaker
    估算器对象上调用 `deploy()`。
- en: Note
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We can also deploy models to SageMaker that were not trained using SageMaker.
    This is often called “bring your own model.”
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以部署未使用 SageMaker 训练的模型到 SageMaker。这通常被称为“自带模型”。
- en: 'Following is the code for deploying our TensorFlow-and-BERT-based review classifier
    model trained with SageMaker:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是部署我们使用 SageMaker 训练的基于 TensorFlow 和 BERT 的评论分类器模型的代码：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next is the *inference.py* specified earlier. This Python script contains the
    `input_handler()` and `output_handler()` functions that convert raw JSON to and
    from TensorFlow tensors. These functions are critical pieces of the prediction
    request/response process.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是前面指定的 *inference.py*。这个 Python 脚本包含将原始 JSON 转换为 TensorFlow 张量及从中转换的 `input_handler()`
    和 `output_handler()` 函数。这些函数是预测请求/响应过程的关键部分。
- en: 'The `input_handler()` function converts the JSON containing raw review text
    into BERT-embedding tensors using `DistilBertTokenizer`. These embeddings are
    converted to tensors and used as inputs to the TensorFlow model:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`input_handler()` 函数使用 `DistilBertTokenizer` 将包含原始评论文本的 JSON 转换为 BERT 嵌入张量。这些嵌入被转换为张量并用作
    TensorFlow 模型的输入：'
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `output_handler()` converts the TensorFlow response from a tensor into
    a JSON response with the predicted label (`star_rating)` and the prediction confidence:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`output_handler()` 将 TensorFlow 响应从张量转换为包含预测标签（`star_rating`）和预测置信度的 JSON 响应：'
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Track Model Deployment in Our Experiment
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在我们的实验中跟踪模型部署
- en: 'We also want to track the deployment within our experiment for data lineage:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望在我们的实验中跟踪部署以获取数据血统：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Analyze the Experiment Lineage of a Deployed Model
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析已部署模型的实验血统
- en: 'Let’s use the Experiment Analytics API to show us the lineage of our model
    in production, including feature engineering, model training, hyper-parameter
    optimization, and model deployment. We will tie everything together in an end-to-end
    pipeline with full lineage tracking in [Chapter 10](ch10.html#pipelines_and_mlops),
    but let’s analyze the experiment lineage up to this point:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用实验分析API展示我们模型在生产中的血统，包括特征工程、模型训练、超参数优化和模型部署。我们将在[第10章](ch10.html#pipelines_and_mlops)中将所有内容绑定在一起，形成端到端的管道，并进行全面的血统追踪，但让我们分析到目前为止的实验血统：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '| TrialComponentName | DisplayName | max_seq_length | learning_rate | train_accuracy
    | endpoint_name |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| TrialComponentName | DisplayName | max_seq_length | learning_rate | train_accuracy
    | endpoint_name |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| TrialComponent-​2021-01-09-062410-pxuy | prepare | 64.0 | NaN | NaN |   |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| TrialComponent-​2021-01-09-062410-pxuy | prepare | 64.0 | NaN | NaN |   |'
- en: '| tensorflow-training-​2021-01-09-06-24-12-989 | train | 64.0 | 0.00001 | 0.9394
    |   |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| tensorflow-training-​2021-01-09-06-24-12-989 | train | 64.0 | 0.00001 | 0.9394
    |   |'
- en: '| TrialComponent-​2021-01-09-193933-bowu | optimize-1 | 64.0 | 0.000017 | 0.9416
    |   |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| TrialComponent-​2021-01-09-193933-bowu | optimize-1 | 64.0 | 0.000017 | 0.9416
    |   |'
- en: '| TrialComponent-​2021-01-09214921-dgtu | deploy | NaN | NaN | NaN | tensorflow-training-​2021-01-09-06-​24-12-989
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| TrialComponent-​2021-01-09214921-dgtu | deploy | NaN | NaN | NaN | tensorflow-training-​2021-01-09-06-​24-12-989
    |'
- en: Invoke Predictions Using the SageMaker Python SDK
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SageMaker Python SDK调用预测
- en: 'Here is some simple application code to invoke our deployed model endpoint
    and classify raw product reviews into `star_rating` 1–5:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是用于调用我们部署的模型端点并将原始产品评论分类为`star_rating` 1–5的简单应用程序代码：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now let’s predict on a sample batch of raw product reviews using a pandas DataFrame:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用pandas DataFrame对一批原始产品评论进行预测：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The output shows the predicted class for `star_rating` 1–5.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了`star_rating` 1–5的预测类别。
- en: '| review_body | predicted_class |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| review_body | predicted_class |'
- en: '| --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| “This is great!” | 5 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| “这太棒了！” | 5 |'
- en: '| “This is OK.” | 3 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| “这还行。” | 3 |'
- en: '| “This is terrible.” | 1 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| “这太糟糕了。” | 1 |'
- en: Invoke Predictions Using HTTP POST
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用HTTP POST调用预测
- en: When we productionize models as microservices, we need to decide how to make
    our predictions available to client applications. Assuming we have the proper
    authentication credentials and HTTP headers, we can invoke a model as a SageMaker
    Endpoint directly using the following HTTP request/response syntax.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将模型作为微服务投入生产时，我们需要决定如何使我们的预测可供客户端应用程序使用。假设我们具有适当的身份验证凭据和HTTP标头，我们可以直接使用以下HTTP请求/响应语法调用SageMaker端点中的模型。
- en: 'HTTP request syntax:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP请求语法：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'HTTP response syntax:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP响应语法：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this example, we implemented the `input_handler()` and `output_handler()`
    functions using a single *inference.py* script. For more complex request and response
    handling, we can deploy each function in its own container using SageMaker Inference
    Pipelines, as we see in the next section.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们使用一个*inference.py*脚本实现了`input_handler()`和`output_handler()`函数。对于更复杂的请求和响应处理，我们可以使用SageMaker推断流水线将每个函数部署到其自己的容器中，如我们在下一节中所见。
- en: Create Inference Pipelines
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建推断流水线
- en: An inference pipeline is a sequence of steps deployed on a single endpoint.
    Following our example, we could deploy the request handler as its own scikit-learn
    container (`step1`), followed by the TensorFlow/BERT model in its own TensorFlow
    Serving container (`step2`), and succeeded by the response handler as its own
    scikit-learn container (`step3`), as shown in [Figure 9-4](#inference_pipeline_with_three_models).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 推断流水线是部署在单个端点上的一系列步骤。根据我们的例子，我们可以将请求处理程序部署为其自己的scikit-learn容器（`step1`），然后是其自己的TensorFlow
    Serving容器中的TensorFlow/BERT模型（`step2`），最后是其自己的scikit-learn容器中的响应处理程序（`step3`），如[图 9-4](#inference_pipeline_with_three_models)所示。
- en: '![](assets/dsaw_0904.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0904.png)'
- en: Figure 9-4\. Inference pipeline with three steps.
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-4\. 推断流水线，包含三个步骤。
- en: We can also deploy ensembles of models across different AI and machine learning
    frameworks, including TensorFlow, PyTorch, scikit-learn, Apache Spark ML, etc.
    Each step is a sequence of HTTPS requests between the containers controlled by
    SageMaker. One step’s response is used as the prediction request for the next
    step and so on. The last step returns the final response back to the inference
    pipeline, which returns the response back to the calling application. The inference
    pipeline is fully managed by SageMaker and can be used for real-time predictions
    as well as batch transforms.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在不同的AI和机器学习框架（包括TensorFlow、PyTorch、scikit-learn、Apache Spark ML等）之间部署模型集合。每个步骤都是由SageMaker控制的容器之间的一系列HTTPS请求。一个步骤的响应被用作下一个步骤的预测请求，依此类推。最后一个步骤将最终响应返回到推断管道，推断管道将响应返回给调用应用程序。推断管道完全由SageMaker管理，可用于实时预测和批处理转换。
- en: 'To deploy an inference pipeline, we create a `PipelineModel` with a sequence
    of steps, including the request handler, model prediction, and response handler.
    We can then call `deploy()` on the `PipelineModel`, which deploys the inference
    pipeline and returns the endpoint API:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署推断管道，我们创建一个包含请求处理程序、模型预测和响应处理程序的`PipelineModel`序列步骤。然后我们可以在`PipelineModel`上调用`deploy()`，这将部署推断管道并返回端点API：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`pipeline_model.deploy()` returns a predictor, as shown in the single-model
    example. Whenever we make an inference request to this predictor, make sure we
    pass the data that the first container expects. The predictor returns the output
    from the last container.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`pipeline_model.deploy()`返回一个预测器，如单一模型示例所示。每当我们向该预测器发出推断请求时，请确保我们传递第一个容器期望的数据。预测器返回最后一个容器的输出。'
- en: 'If we want to run a batch transform job with the `PipelineModel`, just follow
    the steps of creating a `pipeline_model.transformer()` object and call `transform()`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想使用`PipelineModel`运行批处理转换作业，只需按照创建`pipeline_model.transformer()`对象和调用`transform()`的步骤即可：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preceding example demonstrates how to create steps from a series of Python
    scripts. With SageMaker Inference Pipelines, we can also provide our own Docker
    containers for each step.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例演示了如何从一系列Python脚本创建步骤。使用SageMaker推断管道，我们还可以为每个步骤提供自己的Docker容器。
- en: Invoke SageMaker Models from SQL and Graph-Based Queries
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从SQL和基于图的查询中调用SageMaker模型
- en: AWS provides deep integration between the Amazon AI, machine learning, and analytics
    services. Amazon Redshift, Athena, and Aurora can execute predictive SQL queries
    with models deployed as SageMaker Endpoints. Neptune can execute graph-based queries
    with SageMaker Endpoints, as well.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: AWS在Amazon AI、机器学习和分析服务之间提供深度集成。Amazon Redshift、Athena和Aurora可以执行部署为SageMaker端点的模型的预测SQL查询。Neptune也可以执行基于图形的查询，并与SageMaker端点集成。
- en: Auto-Scale SageMaker Endpoints Using Amazon CloudWatch
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Amazon CloudWatch自动缩放SageMaker端点
- en: While we can manually scale using the `InstanceCount` parameter in `EndpointConfig`,
    we can configure our endpoint to automatically scale out (more instances) or scale
    in (less instances) based on a given metric like requests per second. As more
    requests come in, SageMaker will automatically scale our model cluster to meet
    the demand.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以使用`EndpointConfig`中的`InstanceCount`参数手动缩放，但我们可以根据请求每秒等给定的度量配置我们的端点，使其自动扩展（增加实例）或缩小（减少实例）。随着更多请求的到来，SageMaker将自动扩展我们的模型集群以满足需求。
- en: In the cloud, we talk about “scaling in” and “scaling out” in addition to the
    typical “scaling down” and “scaling up.” Scaling in and out refers to removing
    and adding instances of the same type, respectively. Scaling down and up refers
    to using smaller or bigger instance types, respectively. Larger instances have
    more CPUs, GPUs, memory, and network bandwidth, typically.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在云计算中，除了典型的“缩小”和“放大”外，我们还谈论“扩展内部”和“扩展外部”。扩展内部和扩展外部分别指移除和添加相同类型的实例。缩小和放大则指使用较小或较大的实例类型。通常，较大的实例具有更多的CPU、GPU、内存和网络带宽。
- en: It’s best to use homogenous instance types when defining our cluster. If we
    mix instance types, we may have difficulty tuning the cluster and defining scaling
    policies that apply consistently to every instance in the heterogeneous cluster.
    When trying new instance types, we recommend creating a new cluster with only
    that instance type and comparing each cluster as a single unit.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义集群时最好使用同类实例类型。如果混合使用不同的实例类型，可能会难以调整集群并定义适用于异构集群中每个实例的一致缩放策略。在尝试新实例类型时，建议创建一个仅包含该实例类型的新集群，并将每个集群作为单个单位进行比较。
- en: Define a Scaling Policy with AWS-Provided Metrics
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 AWS 提供的指标定义缩放策略
- en: 'In this example, we use `SageMakerVariantInvocationsPerInstance`, the AWS-provided
    CloudWatch metric, to automatically scale our model endpoint when we reach a certain
    threshold of invocations per instance. In the next section, we will use a custom
    auto-scaling metric:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们使用 `SageMakerVariantInvocationsPerInstance`，AWS 提供的 CloudWatch 指标，当我们达到特定的每实例调用阈值时自动扩展我们的模型端点。在接下来的部分中，我们将使用自定义自动缩放指标：
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can see a spike in the `InvocationsPerInstance` metric in CloudWatch after
    we send a large amount of traffic to our endpoint, as shown in [Figure 9-5](#spike_in_the_invocationsperinstance_met),
    as well as a spike in CPU and memory utilization, as shown in [Figure 9-6](#spike_in_cpuutilizationcomma_diskutiliz).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们向端点发送大量流量后，我们可以在 CloudWatch 中看到 `InvocationsPerInstance` 指标的峰值，如[图 9-5](#spike_in_the_invocationsperinstance_met)所示，以及
    CPU 和内存利用率的峰值，如[图 9-6](#spike_in_cpuutilizationcomma_diskutiliz)所示。
- en: '![](assets/dsaw_0905.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0905.png)'
- en: Figure 9-5\. Spike in the `InvocationsPerInstance` metric.
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-5\. `InvocationsPerInstance` 指标的峰值。
- en: '![](assets/dsaw_0906.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0906.png)'
- en: Figure 9-6\. Spike in `CPUUtilization`, `DiskUtilization`, and `MemoryUtilization`
    from increased prediction traffic.
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-6\. `CPUUtilization`、`DiskUtilization` 和 `MemoryUtilization` 由于预测流量的增加而出现的峰值。
- en: This causes an alarm that triggers a scale-out event from one instance to two
    instances to handle the spike in prediction traffic by sharing the traffic across
    two instances. [Figure 9-7](#the_number_of_invocationsperinstance_de) shows the
    positive effect of adding an additional instance to the endpoint cluster. As the
    number of `InvocationsPerInstance` decreases, so does the CPU and memory utilization.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这将触发警报，从一个实例扩展到两个实例来处理预测流量的激增，通过在两个实例之间共享流量来实现。[图 9-7](#the_number_of_invocationsperinstance_de)
    显示了向端点集群添加额外实例的正面影响。随着 `InvocationsPerInstance` 的减少，CPU 和内存利用率也相应减少。
- en: '![](assets/dsaw_0907.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0907.png)'
- en: Figure 9-7\. The number of `InvocationsPerInstance` decreases when we add a
    second instance to the endpoint cluster.
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-7\. 当我们向端点集群添加第二个实例时，`InvocationsPerInstance` 的数量减少。
- en: Define a Scaling Policy with a Custom Metric
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用自定义指标定义缩放策略
- en: Netflix is known to use a custom auto-scaling metric called “starts per second”
    or SPS. A start is recorded every time a user clicks “play” to watch a movie or
    TV show. This was a key metric for auto-scaling because the more “Starts per Second,”
    the more traffic we would start receiving on our streaming control plane.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Netflix 以“每秒启动次数”或 SPS（Starts per Second）的自定义自动缩放指标而闻名。每当用户点击“播放”观看电影或电视节目时，就记录一个启动。这是自动缩放的关键指标，因为“每秒启动次数”越多，我们的流媒体控制平面将接收到的流量也就越多。
- en: 'Assuming we are publishing the `StartsPerSecond` metric, we can use this custom
    metric to scale out our cluster as more movies are started. This metric is called
    a “target tracking” metric, and we need to define the metric name, target value,
    model name, variant name, and summary statistic. The following scaling policy
    will begin scaling out the cluster if the aggregate `StartsPerSecond` metric exceeds
    an average of 50% across all instances in our model-serving cluster:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在发布 `StartsPerSecond` 指标，我们可以使用这个自定义指标来扩展我们的集群，因为更多的电影开始播放。这个指标被称为“目标跟踪”指标，我们需要定义指标名称、目标值、模型名称、变体名称和汇总统计。以下缩放策略将在我们模型服务集群中的所有实例的平均
    `StartsPerSecond` 指标超过 50% 时开始扩展集群：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: When using custom metrics for our scaling policy, we should pick a metric that
    measures instance utilization, decreases as more instances are added, and increases
    as instances are removed.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用自定义指标进行缩放策略时，我们应选择一个衡量实例利用率的指标，随着添加实例而减少，并随着移除实例而增加。
- en: Tuning Responsiveness Using a Cooldown Period
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用冷却期调整响应性
- en: 'When our endpoint is auto-scaling in or out, we likely want to specify a “cooldown”
    period in seconds. A cooldown period essentially reduces the responsiveness of
    the scaling policy by defining the number of seconds between iterations of the
    scale events. We may want to scale out quickly when a spike of traffic comes in,
    but we should scale in slowly to make sure we handle any temporary dips in traffic
    during rapid scale-out events. The following scaling policy will take twice as
    long to scale in as it does to scale out, as shown in the `ScaleInCooldown` and
    `ScaleOutCooldown` attributes:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的端点在自动扩展时，我们可能希望指定一个“冷却”期（以秒为单位）。冷却期本质上通过定义扩展策略迭代之间的秒数来减少响应性。当流量激增时，我们可能希望快速扩展，但在快速扩展事件期间，应缓慢缩减以确保处理任何临时的流量下降。下面的扩展策略将在缩减和扩展时采用不同的时间，如`ScaleInCooldown`和`ScaleOutCooldown`属性所示：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Auto-Scale Policies
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动扩展策略
- en: 'There are three main types of scaling policies to choose from when setting
    up auto-scaling for our SageMaker Endpoints:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置我们的SageMaker端点的自动扩展时，有三种主要的扩展策略可供选择：
- en: Target tracking
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 目标跟踪
- en: Specify a single metric and AWS auto-scales as needed; for example, “keep `InvocationsPerInstance
    = 1000`.” This strategy requires the least configuration.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 指定一个单一的度量，并根据需要由AWS自动扩展；例如，“保持`InvocationsPerInstance = 1000`”。这种策略需要的配置最少。
- en: Simple
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 简单
- en: Trigger on a metric at a given threshold with a fixed amount of scaling; for
    example, “when `InvocationsPerInstance > 1000`, add 1 instance.” This strategy
    requires a bit of configuration but provides more control over the target-tracking
    strategy.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定阈值上触发度量，并使用固定的扩展量；例如，“当`InvocationsPerInstance > 1000`时，添加1个实例”。这种策略需要一些配置，但提供了更多对目标跟踪策略的控制。
- en: Step scaling
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 步进扩展
- en: Trigger on a metric at various thresholds with configurable amounts of scaling
    at each threshold; for example, “when `InvocationsPerInstance > 1000`, add 1 instance,
    `InvocationsPerInstance > 2000`, add 5 instances,” etc. This strategy requires
    the most amount of configuration but provides the most amount of control for situations
    such as spiky traffic.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种阈值上触发度量，并配置每个阈值的扩展量；例如，“当`InvocationsPerInstance > 1000`时，添加1个实例；`InvocationsPerInstance
    > 2000`时，添加5个实例”等。这种策略需要最多的配置，但在如突发流量等情况下提供了最多的控制。
- en: Strategies to Deploy New and Updated Models
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署新模型和更新模型的策略
- en: We can test and deploy new and updated models behind a single SageMaker Endpoint
    with a concept called “production variants.” These variants can differ by hardware
    (CPU/GPU), by data (comedy/drama movies), or by region (US West or Germany North).
    We can safely shift traffic between the model variants in our endpoint for canary
    rollouts, blue/green deployments, A/B tests, and MAB tests. Using these deployment
    strategies, we can minimize the risks involved when pushing new and updated models
    to production.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在单个SageMaker端点后面测试和部署新的和更新的模型，使用名为“生产变体”的概念。这些变体可以通过硬件（CPU/GPU）、数据（喜剧/戏剧电影）或区域（美国西部或德国北部）的不同而不同。我们可以安全地在端点中的模型变体之间转移流量，用于金丝雀发布、蓝/绿部署、A/B测试和MAB测试。使用这些部署策略，我们可以在推送新的和更新的模型到生产环境时最小化风险。
- en: Split Traffic for Canary Rollouts
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分流金丝雀发布
- en: Since our data is continuously changing, our models need to evolve to capture
    this change. When we update our models, we may choose to do this slowly using
    a “canary rollout,” named after the antiquated and morbid process of using a canary
    to detect whether a human could breathe in a coal mine. If the canary survives
    the coal mine, then the conditions are good and we can proceed. If the canary
    does not survive, then we should make adjustments and try again later with a different
    canary. Similarly, we can point a small percentage of traffic to our “canary”
    model and test if the model services. Perhaps there is a memory leak or other
    production-specific issue that we didn’t catch in the research lab.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据不断变化，我们的模型需要演变以捕捉这种变化。当我们更新模型时，我们可以选择使用“金丝雀发布”来逐步进行，这个名字来自于使用金丝雀来检测煤矿中是否适合人类呼吸的过程。如果金丝雀在煤矿中存活下来，那么条件是良好的，我们可以继续进行。如果金丝雀没有存活下来，那么我们应该进行调整，并稍后使用不同的金丝雀再次尝试。类似地，我们可以将少量流量指向我们的“金丝雀”模型，并测试模型的服务情况。也许在研究实验室中没有发现的内存泄漏或其他生产特定问题。
- en: The combination of the cloud instance providing compute, memory, and storage
    and the model container application is called a “production variant.” The production
    variant defines the instance type, instance count, and model. By default, the
    SageMaker Endpoint is configured with a single production variant, but we can
    add multiple variants as needed.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 云实例提供计算、内存和存储，模型容器应用程序的组合称为“生产变体”。生产变体定义了实例类型、实例数量和模型。默认情况下，SageMaker 端点配置为单个生产变体，但我们可以根据需要添加多个变体。
- en: 'Here is the code to setup a single variant, `VariantA`, at a single endpoint
    receiving 100% of the traffic across 20 instances:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是设置单个变体 `VariantA` 的代码，在一个端点上接收来自 20 个实例的 100% 流量：
- en: '[PRE14]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: After creating a new production variant for our canary, we can create a new
    endpoint and point a small amount of traffic (5%) to the canary and point the
    rest of the traffic (95%) to our existing variant, as shown in [Figure 9-8](#splitting_fivepercent_traffic_to_a_new).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 创建新的金丝雀生产变体后，我们可以创建新的端点，并将少量流量（5%）指向金丝雀，其余流量（95%）指向我们的现有变体，如图 [9-8](#splitting_fivepercent_traffic_to_a_new)
    所示。
- en: '![](assets/dsaw_0908.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0908.png)'
- en: Figure 9-8\. Splitting 5% traffic to a new model for a canary rollout.
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-8\. 将 5% 的流量分配到新模型，用于金丝雀发布。
- en: 'Following is the code to create a new endpoint, including the new canary `VariantB`
    accepting 5% of the traffic. Note that we are specifying `''InitialInstanceCount'':
    1` for the new canary, `VariantB`. Assuming that 20 instances handle 100% of the
    current traffic, then each instance likely handles approximately 5% of the traffic.
    This 5% matches the amount of traffic we wish to send to the new canary instance.
    If we wished to send 10% traffic to the new canary, for example, we would choose
    `''InitialInstanceCount'': 2` to support 10% of the canary traffic. This assumes
    that we are using the same instance type for the new canary. If choosing a different
    instance type, we may need more or less instances to handle the % traffic load:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来是创建新端点的代码，包括接受 5% 流量的新金丝雀 `VariantB`。请注意，我们为新金丝雀 `VariantB` 指定了 `''InitialInstanceCount'':
    1`。假设 20 个实例处理当前流量的 100%，那么每个实例可能处理大约 5% 的流量。这个 5% 与我们希望发送到新金丝雀实例的流量量相匹配。例如，如果我们希望将
    10% 的流量发送到新金丝雀，我们将选择 `''InitialInstanceCount'': 2` 来支持 10% 的金丝雀流量。这假设我们对新金丝雀使用相同的实例类型。如果选择不同的实例类型，可能需要更多或更少的实例来处理
    % 流量负载：'
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Canary rollouts release new models safely to a small percentage of users for
    initial production testing in the wild. They are useful if we want to test in
    live production without affecting the entire user base. Since the majority of
    traffic goes to the existing model, the cluster size of the canary model can be
    relatively small since it’s only receiving 5% of the traffic. In the preceding
    example, we are only using a single instance for the canary variant.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀发布可以安全地向小部分用户发布新模型，进行实际生产环境的初步测试。如果我们希望在生产环境中进行测试而不影响整个用户群体，金丝雀发布将非常有用。由于大部分流量流向现有模型，金丝雀模型的集群规模可以相对较小。在上述示例中，我们仅使用单个实例作为金丝雀变体。
- en: Shift Traffic for Blue/Green Deployments
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝/绿部署的流量转移
- en: If the new model performs well, we can proceed with a blue/green deployment
    to shift all traffic to the new model, as shown in [Figure 9-9](#shift_traffic_for_bluesolidusgreen_depl).
    Blue/green deployments help to reduce downtime in case we need to roll back to
    the old deployment. With a blue/green deployment, we spin up a full clone of the
    existing model-server cluster using the new canary model. We then shift all the
    traffic from the old cluster (blue) over to the new cluster (green), as shown
    in [Figure 9-9](#shift_traffic_for_bluesolidusgreen_depl). Blue/green deployments
    prevent the partial-deployment scenario where some of the instances are running
    the new canary model and some are running the existing model. This partial-deployment
    scenario is very hard to debug and manage at scale.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果新模型表现良好，我们可以进行蓝/绿部署，将所有流量转移到新模型，如图 [9-9](#shift_traffic_for_bluesolidusgreen_depl)
    所示。蓝/绿部署有助于在需要回滚到旧部署时减少停机时间。通过蓝/绿部署，我们使用新的金丝雀模型在现有模型服务器集群的完整克隆上进行运行。然后将所有流量从旧集群（蓝）转移到新集群（绿），如图
    [9-9](#shift_traffic_for_bluesolidusgreen_depl) 所示。蓝/绿部署可以防止部分部署场景，即一些实例运行新金丝雀模型，而另一些实例运行现有模型。在大规模情况下，这种部分部署场景很难调试和管理。
- en: '![](assets/dsaw_0909.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0909.png)'
- en: Figure 9-9\. Shift traffic to model variant B for blue/green deployments.
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-9\. 将流量转移到模型变体 B，用于蓝/绿部署。
- en: 'Following is the code to update our endpoint and shift 100% of the traffic
    to the successful canary model, `VariantB`. Note that we have also increased the
    size of the new cluster to match the existing cluster since the new cluster is
    now handling all of the traffic:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是更新端点并将100%流量转移至成功的金丝雀模型`VariantB`的代码。请注意，我们还增加了新集群的大小，以匹配现有集群的大小，因为新集群现在处理所有流量：
- en: '[PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We will keep the old cluster with `VariantA` idle for 24 hours, let’s say,
    in case our canary fails unexpectedly and we need to roll back quickly to the
    old cluster. After 24 hours, we can remove the old environment and complete the
    blue/green deployment. Here is the code to remove the old model, `VariantA`, by
    removing `VariantA` from the endpoint configuration and updating the endpoint:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保留旧的`VariantA`集群闲置24小时，以防我们的金丝雀测试失败，需要快速回滚到旧集群。24小时后，我们可以移除旧环境并完成蓝/绿部署。以下是移除旧模型`VariantA`的代码，方法是从端点配置中移除`VariantA`并更新端点：
- en: '[PRE17]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: While keeping the old cluster idle for a period of time—24 hours, in our example—may
    seem wasteful, consider the cost of an outage during the time needed to roll back
    and scale out the previous model, `VariantA`. Sometimes the new model cluster
    works fine for the first few hours, then degrades or crashes unexpectedly after
    a nighttime cron job, early morning product catalog refresh, or other untested
    scenario. In these cases, we were able to immediately switch traffic back to the
    old cluster and conduct business as usual.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在我们的示例中，将旧集群闲置一段时间（24小时）可能看起来是浪费的，但请考虑在需要回滚并扩展先前的`VariantA`模型所需时间中的停机成本。有时新模型集群在开始几小时运行正常，然后在夜间cron作业、清晨产品目录刷新或其他未经测试的场景后突然退化或崩溃。在这些情况下，我们能够立即将流量切换回旧集群，并正常运营。
- en: Testing and Comparing New Models
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试和比较新模型
- en: We can test new models behind a single SageMaker Endpoint using the same “production
    variant” concept described in the previous section on model deployment. In this
    section, we will configure our SageMaker Endpoint to shift traffic between the
    models in our endpoint to compare model performance in production using A/B and
    MAB tests.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在单个SageMaker端点后面测试新模型，使用在模型部署的前一节中描述的相同的“生产变体”概念。在本节中，我们将配置我们的SageMaker端点，以在端点内的模型之间转移流量，通过A/B和MAB测试来比较生产中的模型性能。
- en: When testing our models in production, we need to define and track the business
    metrics that we wish to optimize. The business metric is usually tied to revenue
    or user engagement, such as orders purchased, movies watched, or ads clicked.
    We can store the metrics in any database, such as DynamoDB, as shown in [Figure 9-10](#tracking_business_metrics_to_determine).
    Analysts and scientists will use this data to determine the winning model from
    our tests.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中测试我们的模型时，我们需要定义和跟踪希望优化的业务指标。业务指标通常与收入或用户参与度相关，例如购买订单、观看电影或点击广告次数。我们可以将这些指标存储在任何数据库中，比如DynamoDB，如图[9-10](#tracking_business_metrics_to_determine)所示。分析师和科学家将使用这些数据来确定我们测试中的优胜模型。
- en: '![](assets/dsaw_0910.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0910.png)'
- en: Figure 9-10\. Tracking business metrics to determine the best model variant.
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-10\. 跟踪业务指标以确定最佳模型变体。
- en: Continuing with our text-classifier example, we will create a test to maximize
    the number of successfully labeled customer service messages. As customer service
    receives new messages, our application will predict the message’s `star_rating`
    (1–5) and route 1s and 2s to a high-priority customer service queue. If the representative
    agrees with the predicted `star_rating`, they will mark our prediction as successful
    (positive feedback); otherwise they will mark the prediction as unsuccessful (negative
    feedback). Unsuccessful predictions could be routed to a human-in-the-loop workflow
    using Amazon A2I and SageMaker Ground Truth, which we discuss in more detail in
    [Chapter 10](ch10.html#pipelines_and_mlops). We will then choose the model variant
    with the most successful number of `star_rating` predictions and start shifting
    traffic to this winning variant. Let’s dive deeper into managing the experiments
    and shifting the traffic.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的文本分类器示例中，我们将创建一个测试来最大化成功标记的客户服务消息的数量。当客户服务收到新消息时，我们的应用程序将预测消息的 `star_rating`（1-5）并将
    1 和 2 分的消息路由到高优先级的客户服务队列。如果代表同意预测的 `star_rating`，他们将将我们的预测标记为成功（积极反馈）；否则，他们将把预测标记为失败（消极反馈）。不成功的预测可以通过使用
    Amazon A2I 和 SageMaker Ground Truth 进行人为介入的工作流程路由，我们将在 [第 10 章](ch10.html#pipelines_and_mlops)
    中详细讨论这一点。然后，我们将选择成功预测数量最多的模型变体，并开始将流量转移到这个获胜的变体。让我们深入探讨如何管理实验和转移流量。
- en: Perform A/B Tests to Compare Model Variants
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行 A/B 测试以比较模型变体
- en: Similar to canary rollouts, we can use traffic splitting to direct subsets of
    users to different model variants for the purpose of comparing and testing different
    models in live production. The goal is to see which variants perform better. Often,
    these tests need to run for a long period of time (weeks) to be statistically
    significant. [Figure 9-11](#asolidusb_testing_with_two_model_varian) shows two
    different recommendation models deployed using a random 50/50 traffic split between
    the two variants.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于金丝雀发布，我们可以使用流量分割将用户子集引导到不同的模型变体，以比较和测试生产中的不同模型。目标是看看哪些变体表现更好。通常，这些测试需要长时间运行（几周）才能具有统计学意义。[图
    9-11](#asolidusb_testing_with_two_model_varian) 展示了使用随机的 50/50 流量分割部署的两个不同推荐模型。
- en: '![](assets/dsaw_0911.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0911.png)'
- en: Figure 9-11\. A/B testing with two model variants by splitting traffic 50/50.
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-11. 通过 50/50 流量分割进行的两个模型变体的 A/B 测试。
- en: While A/B testing seems similar to canary rollouts, they are focused on gathering
    data about different variants of a model. A/B tests are targeted to larger user
    groups, take more traffic, and run for longer periods of time. Canary rollouts
    are focused more on risk mitigation and smooth upgrades.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 A/B 测试看起来与金丝雀发布类似，但它们专注于收集有关模型不同变体的数据。A/B 测试针对较大的用户群体，需要更多的流量，并且运行时间更长。金丝雀发布更专注于风险缓解和平稳升级。
- en: Note
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For fine-grained traffic routing based on IP address, HTTP headers, query string,
    or payload content, use an Application Load Balancer in front of the SageMaker
    Endpoints.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于 IP 地址、HTTP 头、查询字符串或有效负载内容进行细粒度流量路由，可以在 SageMaker 端点前使用应用程序负载均衡器。
- en: 'One example for a model A/B test could be streaming music recommendations.
    Let’s assume we are recommending a playlist for Sunday mornings. We might want
    to test if we can identify specific user groups that are more likely to listen
    to powerful wake-up beats (model A) or that prefer smooth lounge music (model
    B). Let’s implement this A/B test using Python. We start with creating a SageMaker
    Endpoint configuration that defines a separate production variant for Model A
    and Model B. We initialize both production variants with the identical instance
    types and instance counts:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型 A/B 测试的例子可以是流媒体音乐推荐。假设我们正在推荐一个星期天早晨的播放列表。我们可能想要测试我们是否能够识别更倾向于听力量感强烈的醒目音乐的特定用户群体（模型
    A），或者更喜欢柔和的休闲音乐的用户群体（模型 B）。让我们使用 Python 实现这个 A/B 测试。我们首先创建一个 SageMaker Endpoint
    配置，定义模型 A 和模型 B 的单独生产变体。我们使用相同的实例类型和实例计数初始化这两个生产变体：
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: After we have monitored the performance of both models for a period of time,
    we can shift 100% of the traffic to the better-performing model, Model B in our
    case. Let’s shift our traffic from a 50/50 split to a 0/100 split, as shown in
    [Figure 9-12](#asolidusb_testing_traffic_shift_from_fi).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们监控了一段时间两个模型的性能后，我们可以将 100% 的流量转移到表现更好的模型，我们这里是模型 B。让我们将我们的流量从 50/50 分割转移到
    0/100 分割，如 [图 9-12](#asolidusb_testing_traffic_shift_from_fi) 所示。
- en: '![](assets/dsaw_0912.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0912.png)'
- en: Figure 9-12\. A/B testing traffic shift from 50/50 to 0/100.
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-12\. A/B 测试流量从 50/50 到 0/100 的转移。
- en: 'Following is the code to shift all traffic to `VariantB` and ultimately remove
    `VariantA` when we are confident that `VariantB` is working correctly:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码，用于将所有流量转移到`VariantB`，并在我们确信`VariantB`正常运行时最终移除`VariantA`：
- en: '[PRE19]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Reinforcement Learning with Multiarmed Bandit Testing
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用多臂老虎机测试的强化学习
- en: A/B tests are static and must run for a period of time—sometimes weeks or months—before
    they are considered statistically significant. During this time, we may have deployed
    a bad model variant that is negatively affecting revenue. However, if we stop
    the test early, we ruin the statistical significance of the experiment and cannot
    derive much meaning from the results. In other words, our model may have performed
    poorly initially but may actually have been a better model overall if the experiment
    had run longer. A/B tests are static and do not allow us to dynamically shift
    traffic during an experiment to minimize the “regret” caused by a poor-performing
    model. They also do not allow us to add or remove model variants during the lifetime
    of the experiment.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: A/B 测试是静态的，必须运行一段时间——有时是几周或几个月——才能被认为具有统计显著性。在此期间，我们可能部署了一个影响收入的糟糕模型变体。然而，如果我们提前停止测试，我们将破坏实验的统计显著性，并不能从结果中得出多少意义。换句话说，我们的模型可能最初表现不佳，但如果实验运行时间更长，实际上可能是更好的模型。A/B
    测试是静态的，并不允许我们在实验期间动态地转移流量，以减少由表现不佳的模型引起的“后悔”。它们也不允许我们在实验的生命周期内添加或删除模型变体。
- en: A more dynamic method for testing different model variants is called MABs. Named
    after a slot machine that can quickly take our money, these mischievous bandits
    can actually earn us quite a bit of money by dynamically shifting traffic to the
    winning model variants much sooner than with an A/B test. This is the “exploit”
    part of the MAB. At the same time, the MAB continues to “explore” the nonwinning
    model variants just in case the early winners were not the overall best model
    variants. This dynamic pull between “exploit and explore” is what give MABs their
    power. Based on reinforcement learning (RL), MABs rely on the feedback positive-negative
    mechanism to choose an “action.”
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更动态的测试不同模型变体的方法称为多臂老虎机（MABs）。这种方法得名于一个可以迅速夺走我们钱的老虎机，这些淘气的"土匪"事实上可以通过动态地将流量转移到获胜的模型变体来为我们赚取相当可观的收益，远比
    A/B 测试更快。这就是 MAB 的“利用”部分。与此同时，MAB 仍然“探索”那些未获胜的模型变体，以防早期的获胜者并不是整体最佳模型变体。这种“利用和探索”的动态平衡赋予了
    MAB 他们的力量。基于强化学习（RL），MAB 依赖于正负反馈机制来选择“动作”。
- en: In our case, the MAB chooses the model variant based on the current reward metrics
    and the chosen exploit-explore strategy. The RL-based MAB acts as the primary
    SageMaker Endpoint and dynamically routes prediction traffic to the available
    BERT-based SageMaker Endpoints, as shown in [Figure 9-13](#find_the_best_bert_model_using_reinforc).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，MAB 根据当前奖励指标和选择的利用-探索策略选择模型变体。基于 RL 的 MAB 充当主要的 SageMaker 端点，并动态路由预测流量到可用的基于
    BERT 的 SageMaker 端点，如[图 9-13](#find_the_best_bert_model_using_reinforc)所示。
- en: '![](assets/dsaw_0913.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0913.png)'
- en: Figure 9-13\. Find the best BERT model using RL and MABs.
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-13\. 使用 RL 和 MABs 找到最佳的 BERT 模型。
- en: 'There are various MAB exploration strategies, including epsilon greedy, Thompson
    sampling, bagging, and online cover. Epsilon greedy uses a fixed exploit-explore
    threshold, while Thompson sampling uses a more sophisticated and dynamic threshold
    based on prior information—a technique rooted in Bayesian statistics. Bagging
    uses an ensemble approach by training on random subsets of data to generate a
    set of policies to ensemble. Online cover is, in theory, the optimal exploration
    algorithm based on the [paper “Taming the Monster: A Fast and Simple Algorithm
    for Contextual Bandits”](https://oreil.ly/ZNyKH). Like bagging, online cover trains
    a set of policies on different subsets of the dataset. Unlike bagging, however,
    online cover trains the set of policies to result in a diverse set of predictions
    for a more sophisticated and complete exploration strategy.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多种MAB探索策略，包括ε-greedy、Thompson采样、bagging和在线覆盖。ε-greedy使用固定的利用-探索阈值，而Thompson采样基于先验信息使用更复杂和动态的阈值，这是一种基于贝叶斯统计的技术。Bagging使用集成方法，通过训练数据的随机子集生成一组策略以进行集成。在线覆盖理论上是基于论文《驯服怪物：一种快速简单的情境赌博算法》的最优探索算法。与Bagging不同的是，在线覆盖训练一组策略以生成更复杂和完整的预测，以实现更复杂和全面的探索策略。
- en: SageMaker natively supports popular RL libraries, including Vowpal Wabbit, Ray,
    Coach, Unity, and others. Additionally, we can use any other reinforcement library
    by building our own Docker image for SageMaker to deploy and manage. In our example,
    we will use Vowpal Wabbit and the online-cover exploration strategy. Our Vowpal
    Wabbit–based MAB is continuously trained on the latest reward metrics and will
    adjust the prediction traffic to send more traffic to the winning BERT-based model,
    as shown in [Figure 9-14](#multiarmed_bandit_dynamically_shifting), where Model
    2 starts to receive more traffic as it accumulates more rewards.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker原生支持流行的RL库，包括Vowpal Wabbit、Ray、Coach、Unity等。此外，我们可以通过为SageMaker构建自己的Docker镜像来使用任何其他强化学习库进行部署和管理。在我们的示例中，我们将使用Vowpal
    Wabbit和在线覆盖探索策略。我们基于Vowpal Wabbit的MAB持续根据最新的奖励指标进行训练，并将调整预测流量，向获胜的基于BERT的模型发送更多流量，如[图 9-14](#multiarmed_bandit_dynamically_shifting)所示，其中模型2开始随着累积奖励接收更多流量。
- en: '![](assets/dsaw_0914.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0914.png)'
- en: Figure 9-14\. MAB dynamically shifting traffic to the “winning” model variant.
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-14\. MAB动态转移流量至“获胜”模型变体。
- en: '[Figure 9-15](#completecomma_end_to_end_implementation) shows a complete, end-to-end
    production implementation of MABs in AWS using the Vowpal Wabbit RL framework,
    SageMaker, Amazon Kinesis Firehose, S3 for persistent storage, and Athena for
    application queries.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-15](#completecomma_end_to_end_implementation)展示了在AWS上使用Vowpal Wabbit RL框架、SageMaker、Amazon
    Kinesis Firehose、S3持久存储以及Athena应用查询的完整端到端MAB生产实现。'
- en: '![](assets/dsaw_0915.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0915.png)'
- en: Figure 9-15\. Complete, end-to-end implementation of MABs and RL on AWS.
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-15\. AWS上MAB和RL的完整端到端实现。
- en: We are continuously training our MAB with the Vowpal Wabbit RL framework as
    new reward data flows into the system from our applications. New versions of the
    MAB models are continuously deployed as SageMaker Endpoints. We can dynamically
    add and remove model variants from testing because of the dynamic nature of MABs.
    This is something that we cannot do with traditional A/B tests, where we need
    to keep all model variants fixed for the lifetime of the experiment.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用Vowpal Wabbit RL框架持续训练我们的多臂赌博机（MAB），随着新的奖励数据从我们的应用程序流入系统。新版本的MAB模型持续部署为SageMaker端点。由于MAB的动态特性，我们可以动态地添加和移除测试中的模型变体。这是我们在传统的A/B测试中做不到的，因为在实验的整个生命周期内，我们需要保持所有模型变体不变。
- en: 'Following is a subset of the configuration for our MAB model using the native
    Vowpal Wabbit integration with SageMaker, DynamoDB, and Kinesis. This configuration
    also highlights the hyper-parameters used by the online-cover exploration strategy,
    including the number of subpolicies to train as well as the counterfactual analysis
    (CFA) strategy to use:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们的MAB模型配置的一部分，使用了与SageMaker、DynamoDB和Kinesis原生集成的Vowpal Wabbit，同时也突出了在线覆盖探索策略使用的超参数，包括要训练的子策略数量以及要使用的反事实分析（CFA）策略：
- en: '[PRE20]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We have chosen to train three subpolicies when deciding which action to take
    (which BERT model to invoke)—as well as the doubly robust (DR) CFA method. For
    more information on these hyper-parameters, see the [Vowpal Wabbit documentation](https://oreil.ly/lDikQ)
    and the GitHub repository associated with this book.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择在决定采取哪个操作（调用哪个BERT模型）时训练三个子策略，以及双重稳健（DR）CFA方法。有关这些超参数的更多信息，请参阅[Vowpal Wabbit文档](https://oreil.ly/lDikQ)和与本书相关的GitHub存储库。
- en: 'Following is a snippet from the SageMaker Training Job logs as the bandit model
    is continuously trained on new reward data arriving to the system. In this case,
    six hundred new rewards were picked up:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是来自SageMaker培训作业日志的片段，因为强盗模型不断根据系统中到达的新奖励数据进行训练。在这种情况下，捡起了六百个新奖励：
- en: '[PRE21]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let’s assume we want to compare two BERT models: BERT Model 1 and BERT Model
    2\. We will reuse the model we trained in [Chapter 7](ch07.html#train_your_first_model)
    for BERT Model 1\. This model had a training accuracy of close to 93% and a validation
    accuracy around 50%. Given that random chance to predict the five categories is
    20%, 50% is not that bad. For BERT Model 2, we train a model that achieves slightly
    less training and validation accuracy around 40%.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想比较两个BERT模型：BERT模型1和BERT模型2。我们将重复使用我们在[第7章](ch07.html#train_your_first_model)中训练的BERT模型1。该模型的训练准确率接近93%，验证准确率约为50%。考虑到随机预测五个类别的机会为20%，50%并不算糟糕。对于BERT模型2，我们训练了一个准确率略低于40%的模型。
- en: We deploy the two BERT models and a fresh MAB. After running these models in
    production, we analyze the latest probabilities used by the MAB to choose either
    Model 1 or Model 2\. Action probability is a measurement of the probability that
    selecting either Model 1 or Model 2 is the best choice given the current reward
    information and context. The mean action probability for BERT Model 1 is 0.743
    and for BERT Model 2 is 0.696\. BERT Model 1 is favored in this case as measured
    by the higher action probability. [Figure 9-16](#action_probability_used_by_the_multiarm)
    shows the plot of the action probability used by the MAB for all predictions.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们部署了两个BERT模型和一个新的MAB。在生产中运行这些模型后，我们分析了MAB用于选择Model 1或Model 2的最新概率。动作概率是根据当前奖励信息和背景选择Model
    1或Model 2作为最佳选择的概率测量值。BERT模型1的平均动作概率为0.743，BERT模型2为0.696。在这种情况下，BERT模型1因较高的动作概率而受青睐。[图 9-16](#action_probability_used_by_the_multiarm)显示了MAB用于所有预测的动作概率图表。
- en: '![](assets/dsaw_0916.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0916.png)'
- en: Figure 9-16\. MAB action probability.
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-16\. MAB动作概率。
- en: Sample probability is a measurement of the probability that the bandit will
    choose Model 1 or Model 2 given the exploration policy, current reward information,
    and context. The combination of the action probability and sample probability
    determines which BERT Model the bandit will use to classify the review test. The
    mean sample probability our bandit uses for BERT Model 1 is 0.499, and for BERT
    Model 2 it is 0.477\. BERT Model 1 is favored in this case as measured by the
    higher sample probability.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 样本概率是衡量强盗将根据探索策略、当前奖励信息和背景选择Model 1或Model 2的概率。结合动作概率和样本概率，确定强盗用于分类评论测试的BERT模型。我们的强盗使用BERT模型1的平均样本概率为0.499，BERT模型2为0.477。在这种情况下，BERT模型1因较高的样本概率而受青睐。
- en: '[Figure 9-17](#sample_probability_used_by_multiarmed_b) shows the sample probability
    used by the MAB to choose between BERT Model 1 and BERT Model 2 across all predictions.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-17](#sample_probability_used_by_multiarmed_b)显示了MAB在所有预测中用于选择BERT模型1和BERT模型2之间的样本概率。'
- en: '![](assets/dsaw_0917.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0917.png)'
- en: Figure 9-17\. MAB sample probability.
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-17\. MAB样本概率。
- en: We also notice a shift in traffic between the two variants, as shown in [Figure 9-18](#traffic_split_between_bert_model_one_an).
    Model 2 starts with all of the traffic but slowly receives less traffic as the
    MAB begins to favor Model 1 due to higher rewards, which leads to a higher sample
    probability.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还注意到两个变体之间流量的变化，如[图 9-18](#traffic_split_between_bert_model_one_an)所示。模型2开始时拥有所有流量，但随着MAB开始因较高的奖励而更喜欢模型1，其接收的流量逐渐减少，这导致更高的样本概率。
- en: '![](assets/dsaw_0918.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0918.png)'
- en: Figure 9-18\. Traffic split between BERT Model 1 and BERT Model 2.
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-18\. 流量分割在BERT模型1和BERT模型2之间。
- en: We see that BERT Model 1, the incumbent model, has an advantage over the challenger
    model, BERT Model 2\. In this case, we choose to keep Model 1 in production and
    not replace it with BERT Model 2.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到BERT模型1，即现有模型，在挑战者模型BERT模型2面前具有优势。在这种情况下，我们选择保留模型1并不替换为BERT模型2。
- en: Let’s analyze reward versus regret and make sure our model is exploiting and
    exploring appropriately and not giving up too much during the exploration process.
    We assign a reward of 1 if the model predicts the `star_rating` correctly and
    a reward of 0 if the model predicts incorrectly. Therefore, the reward is tied
    to model accuracy. The mean reward is 0.472, which is, not coincidentally, a blend
    of the validation accuracies of BERT Model 1 and BERT Model 2 that we trained
    in [Chapter 7](ch07.html#train_your_first_model). [Figure 9-19](#rolling_onezerozero_mean_reward_of_our)
    shows a plot of the rolling one hundred mean reward across all predictions.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析奖励与遗憾，确保我们的模型在探索过程中适当地进行利用和探索，并在此过程中不放弃太多。如果模型预测`star_rating`正确，则分配奖励为1；如果预测错误，则分配奖励为0。因此，奖励与模型准确度相关联。平均奖励为0.472，这不巧是我们在[第7章](ch07.html#train_your_first_model)中训练的BERT模型1和BERT模型2的验证精度的混合值。[图9-19](#rolling_onezerozero_mean_reward_of_our)展示了所有预测中滚动一百次的平均奖励的图表。
- en: '![](assets/dsaw_0919.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0919.png)'
- en: Figure 9-19\. Rolling one hundred mean reward of our experiment.
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-19. 实验中滚动一百次的平均奖励。
- en: All of these plots indicate that the bandit initially explores the action space
    by sending traffic to both BERT Model 1 and BERT Model 2, finds an early winner,
    and exploits BERT Model 2 up to around 230 predictions. It then starts exploring
    again until around 330 predictions, when it begins to exploit up BERT Model 2
    again up to the 500th prediction, where it likely would have begun exploring again.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些图表表明，贝叶斯测试初始通过将流量发送到BERT模型1和BERT模型2来探索动作空间，找到早期的优胜者，并利用BERT模型2进行约230次预测。然后它再次开始探索，直到约330次预测时，它再次开始利用BERT模型2，直至第500次预测，那时它可能会再次开始探索。
- en: This trade-off between exploit and explore is controlled by the chosen exploration
    policy and is the key differentiator between A/B and MAB tests. With an aggressive
    exploration policy, we will see the bandit explore the space and reduce the mean
    reward. Here, we are using the self-tuning online-cover exploration policy.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 探索和利用之间的这种权衡由选择的探索策略控制，是A/B测试和多臂老虎机测试之间的关键区别。通过激进的探索策略，我们将看到贝叶斯在空间中进行探索，并降低平均奖励。在这里，我们使用自调整的在线覆盖探索策略。
- en: Bandits help us minimize the regret of deploying a poor-performing model in
    production and give us near-real-time insight into our BERT model performance
    on real-world data. If one of our BERT models is performing poorly, we can remove
    the model from the experiment—or even add new model variants that the bandit will
    begin exploring using the exploration policy that we selected.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯帮助我们最小化在生产中部署性能不佳的模型所带来的遗憾，并让我们在真实数据上快速了解我们的BERT模型性能。如果我们的某个BERT模型表现不佳，我们可以从实验中移除该模型，甚至添加新的模型变体，使用我们选择的探索策略来开始探索。
- en: We can tune our Vowpal Wabbit bandit model using the hyper-parameters described
    in the framework’s documentation. For more information on the Vowpal Wabbit hyper-parameters
    for the online-cover exploration policy, see the [Vowpal Wabbit documentation](https://oreil.ly/lDikQ).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用框架文档中描述的超参数来调整我们的Vowpal Wabbit贝叶斯模型。有关在线覆盖探索策略的Vowpal Wabbit超参数的更多信息，请参阅[Vowpal
    Wabbit文档](https://oreil.ly/lDikQ)。
- en: We can also provide historical data to pre-train the RL model before initially
    deploying to production. This seeds our model with action and sample probabilities
    that may potentially reduce the regret caused by the initial exploration phase
    when the RL model is learning the action and sample space from scratch.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以提供历史数据，以在最初部署到生产环境之前对RL模型进行预训练。这将用我们的模型种子化动作和样本概率，这可能会减少RL模型在从头学习动作和样本空间时由初始探索阶段引起的遗憾。
- en: Remember that this is just an example of predictions over a few minutes. We
    likely want to run the experiment longer to gain more insight into which model
    is better for our application and use case.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这只是在几分钟内的预测示例。我们可能希望延长实验时间，以获得更多关于哪种模型更适合我们的应用程序和用例的见解。
- en: Monitor Model Performance and Detect Drift
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控模型性能并检测漂移
- en: The world continues to change around us. Customer behavior changes relatively
    quickly. The application team is releasing new features. The Netflix catalog is
    swelling with new content. Fraudsters are finding clever ways to hack our credit
    cards. A continuously changing world requires continuous retraining and redeploying
    of our predictive models to adjust for these real-world drift scenarios.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 世界在我们周围继续变化。客户行为相对快速变化。应用团队发布新功能。Netflix 目录正在充实新内容。欺诈者正找到聪明的方法来盗取我们的信用卡信息。一个不断变化的世界需要持续对我们的预测模型进行再培训和重新部署，以调整这些真实世界漂移情况。
- en: In [Chapter 5](ch05.html#explore_the_dataset), we discussed various types of
    drift that may cause model performance to degrade. By automatically recording
    SageMaker Endpoint inputs (features) and outputs (predictions), SageMaker Model
    Monitor automatically detects and measures drift against a provided baseline.
    SageMaker Model Monitor then notifies us when the drift reaches a user-specified
    threshold from a baseline learned on our trained model and specified during model
    deployment.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 5 章](ch05.html#explore_the_dataset)中，我们讨论了可能导致模型性能下降的各种漂移类型。通过自动记录 SageMaker
    端点的输入（特征）和输出（预测），SageMaker Model Monitor 自动检测并测量与提供的基线的漂移。当漂移达到用户指定的基线（在我们训练模型期间学习并在模型部署期间指定）的阈值时，SageMaker
    Model Monitor 将通知我们。
- en: SageMaker Model Monitor calculates drift using statistical methods such as Kullback–Leibler
    divergence and L-infinity norm. For L-infinity norm, for example, SageMaker Model
    Monitor supports `linf_simple` and `linf_robust`. The `linf_simple` method is
    based on the maximum absolute difference between the cumulative distribution functions
    of two distributions. The `linf_robust` method is based on `linf_simple` but is
    used when there are not enough samples. The `linf_robust` formula is based on
    the two-sample Kolmogorov–Smirnov test.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Model Monitor 使用统计方法（如 Kullback–Leibler 散度和 L-infinity 范数）计算漂移。例如，对于
    L-infinity 范数，SageMaker Model Monitor 支持`linf_simple`和`linf_robust`。`linf_simple`方法基于两个分布的累积分布函数之间的最大绝对差异。`linf_robust`方法基于`linf_simple`，但在样本不足时使用。`linf_robust`公式基于两样本
    Kolmogorov–Smirnov 测试。
- en: Enable Data Capture
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用数据捕获
- en: SageMaker Model Monitor analyzes our model predictions (and their inputs) to
    detect drift in data quality, model quality, model bias, or feature attribution.
    In a first step, we need to enable data capture for a given endpoint, as shown
    in [Figure 9-20](#enable_data_capture_for_a_given_endpoin).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Model Monitor 分析我们模型的预测（及其输入），以检测数据质量、模型质量、模型偏差或特征归因的漂移。首先，我们需要为给定的端点启用数据捕获，如[图 9-20](#enable_data_capture_for_a_given_endpoin)所示。
- en: '![](assets/dsaw_0920.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0920.png)'
- en: Figure 9-20\. Enable data capture for a given endpoint.
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-20\. 启用给定端点的数据捕获。
- en: 'Following is the code to enable data capture. We can define all configuration
    options in the `DataCaptureConfig` object. We can choose to capture the request
    payload, the response payload, or both with this configuration. The capture config
    applies to all model production variants of the endpoint:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是启用数据捕获的代码。我们可以在`DataCaptureConfig`对象中定义所有配置选项。使用此配置可以选择捕获请求有效负载、响应有效负载或两者。捕获配置适用于端点的所有模型生产变体：
- en: '[PRE22]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We are now capturing all inference requests and prediction results in the specified
    S3 destination.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们正在捕获指定的 S3 目的地中的所有推断请求和预测结果。
- en: Understand Baselines and Drift
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解基线和漂移
- en: In [Chapter 5](ch05.html#explore_the_dataset), we explored our dataset and visualized
    the distribution of reviews for each `product_category` and `star_rating`. We
    will use this data to create baseline distribution metrics to compare with live
    distributions seen by our SageMaker Model Endpoints. [Figure 9-21](#the_number_of_reviews_per_product_categ)
    shows the number of reviews per product category.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 5 章](ch05.html#explore_the_dataset)中，我们探索了数据集并可视化了每个`product_category`和`star_rating`的评论分布。我们将使用这些数据创建基准分布度量标准，以与我们的
    SageMaker 模型端点观察到的实时分布进行比较。[图 9-21](#the_number_of_reviews_per_product_categ)显示了每个产品类别的评论数量。
- en: '![](assets/dsaw_0921.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0921.png)'
- en: Figure 9-21\. The number of reviews per product category in our data is an example
    baseline for input feature distribution.
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-21\. 我们数据中每个产品类别的评论数量是输入特征分布的基准示例。
- en: This represents the baseline distribution of the `product_category` input features
    used to train our model. SageMaker Model Monitor captures the actual model-input
    distribution seen by our SageMaker Model Endpoints, compares against that baseline
    distribution used during training, and produces a drift metric that measures the
    covariate shift in our model-input distribution.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这代表用于训练我们的模型的`product_category`输入特征的基线分布。SageMaker Model Monitor捕获我们SageMaker模型端点看到的实际模型输入分布，将其与训练期间使用的基线分布进行比较，并生成衡量模型输入分布中协变量漂移的漂移度量。
- en: If the measure drift exceeds a threshold that we specify, SageMaker Model Monitor
    would notify us and potentially retrain and redeploy an updated version of the
    model trained on the latest distribution of input data. [Figure 9-22](#the_distribution_of_star_rating_labels)
    shows the baseline distribution of data for each `product_category` and `star_rating`
    from [Chapter 5](ch05.html#explore_the_dataset).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们指定的测量漂移超过阈值，SageMaker Model Monitor会通知我们，并可能重新训练和部署在最新输入数据分布上训练的模型的更新版本。[图9-22](#the_distribution_of_star_rating_labels)显示每个`product_category`和`star_rating`的数据的基线分布，来自[第5章](ch05.html#explore_the_dataset)。
- en: '![](assets/dsaw_0922.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0922.png)'
- en: Figure 9-22\. The distribution of `star_rating` labels in our training data
    is an example baseline for target distribution.
  id: totrans-203
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-22. 我们训练数据中`star_rating`标签的分布是目标分布的一个示例基线。
- en: We can detect covariate shifts in model-input distribution using SageMaker Model
    Monitor’s data-quality monitoring feature. And we can also detect concept shifts
    using SageMaker Model Monitor’s model-quality monitoring feature that compares
    live predictions against ground truth labels for the same model inputs captured
    by SageMaker Model Monitor on live predictions. These ground truth labels are
    provided by humans in an offline human-in-the-loop workflow using, for example,
    Amazon A2I and SageMaker Ground Truth, as described in [Chapter 3](ch03.html#automated_machine_learnin).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用SageMaker Model Monitor的数据质量监控功能检测模型输入分布中的协变量漂移。我们还可以使用SageMaker Model
    Monitor的模型质量监控功能检测概念漂移，该功能将实时预测与由SageMaker Model Monitor在实时预测中捕获的相同模型输入的地面实况标签进行比较。这些地面实况标签由人类在离线人类环路工作流中提供，例如使用Amazon
    A2I和SageMaker Ground Truth，在[第3章](ch03.html#automated_machine_learnin)中有描述。
- en: In addition, SageMaker Model Monitor’s model-quality feature can monitor, measure,
    and detect drifts in model bias, feature importance, and model explainability.
    Each drift is measured relative to a baseline generated from our trained model.
    These baselines are provided to each SageMaker Endpoint deployed with SageMaker
    Model Monitor enabled.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，SageMaker Model Monitor的模型质量功能可以监控、衡量和检测模型偏差、特征重要性和模型可解释性的漂移。每个漂移相对于我们训练模型生成的基线进行测量。这些基线由启用SageMaker
    Model Monitor的每个SageMaker终端节点提供。
- en: Monitor Data Quality of Deployed SageMaker Endpoints
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控部署的SageMaker终端节点的数据质量
- en: Our model learns and adapts the statistical characteristics of our training
    data. If the statistical characteristics of the data that our online model receives
    drifts from that baseline, the model quality will degrade. We can create a data-quality
    baseline using Deequ, as discussed in [Chapter 5](ch05.html#explore_the_dataset).
    Deequ analyzes the input data and creates schema constraints and statistics for
    each input feature. We can identify missing values and detect covariate shifts
    relative to that baseline. SageMaker Model Monitor uses Deequ to create baselines
    for data-quality monitoring.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型学习并适应我们训练数据的统计特征。如果我们在线模型接收到的数据的统计特征从该基线漂移，模型质量将会下降。我们可以使用Deequ创建数据质量基线，如在[第5章](ch05.html#explore_the_dataset)中讨论的那样。Deequ分析输入数据，并为每个输入特征创建模式约束和统计信息。我们可以识别缺失值并相对于该基线检测协变量漂移。SageMaker
    Model Monitor使用Deequ为数据质量监控创建基线。
- en: Create a Baseline to Measure Data Quality
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建基线以衡量数据质量
- en: A data-quality baseline helps us detect drift in the statistical characteristics
    of online model inputs from the provided baseline data. We typically use our training
    data to create the first baseline, as shown in [Figure 9-23](#a_data_quality_baseline_helps_us_detect).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量基线帮助我们检测在线模型输入的统计特征中的漂移，这些输入来自所提供的基线数据。通常，我们使用我们的训练数据创建第一个基线，如[图9-23](#a_data_quality_baseline_helps_us_detect)所示。
- en: '![](assets/dsaw_0923.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0923.png)'
- en: Figure 9-23\. Create a data-quality baseline from training data.
  id: totrans-211
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-23. 从训练数据创建数据质量基线。
- en: 'The training dataset schema and the inference dataset schema must match exactly,
    including the number of features and the order in which they are passed in for
    inference. We can now start a SageMaker Processing Job to suggest a set of baseline
    constraints and generate statistics of the data as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集模式和推理数据集模式必须完全匹配，包括特征数量和它们传递的顺序。现在我们可以启动 SageMaker 处理作业，建议一组基线约束，并生成数据统计，如下所示：
- en: '[PRE23]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'After the baseline job has finished, we can explore the generated statistics:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 基线作业完成后，我们可以查看生成的统计数据：
- en: '[PRE24]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here is an example set of statistics for our `review_body` prediction inputs:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们`review_body`预测输入的一组统计数据示例：
- en: '[PRE25]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can explore the generated constraints as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按以下方式查看生成的约束：
- en: '[PRE26]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here is an example of the constraints defined for our `review_body` prediction
    inputs:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们`review_body`预测输入的约束定义示例：
- en: '[PRE27]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In this example, the constraint would raise an alarm if there are missing values
    for `review_body`. With the baseline, we can now create and schedule data-quality
    monitoring jobs.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，如果`review_body`中有缺失值，约束将引发警报。通过基线，我们现在可以创建和调度数据质量监控作业。
- en: Schedule Data-Quality Monitoring Jobs
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调度数据质量监控作业
- en: SageMaker Model Monitor gives us the ability to continuously monitor the data
    collected from the endpoints on a schedule. We can create the schedule with the
    `CreateMonitoringSchedule` API defining a periodic interval. Similar to the data-quality
    baseline job, SageMaker Model Monitor starts a SageMaker Processing Job, which
    compares the dataset for the current analysis with the baseline statistics and
    constraints. The result is a violation report. In addition, SageMaker Model Monitor
    sends metrics for each feature to CloudWatch, as shown in [Figure 9-24](#sagemaker_model_monitor_gives_us_the_ab).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 模型监控使我们能够定期监控从端点收集的数据。我们可以使用`CreateMonitoringSchedule` API 创建一个周期性间隔的调度。类似于数据质量基线作业，SageMaker
    模型监控启动一个 SageMaker 处理作业，用于比较当前分析的数据集与基线统计数据和约束条件。结果将生成违规报告。此外，SageMaker 模型监控将每个特征的指标发送到
    CloudWatch，如图 [9-24](#sagemaker_model_monitor_gives_us_the_ab) 所示。
- en: '![](assets/dsaw_0924.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0924.png)'
- en: Figure 9-24\. SageMaker Model Monitor gives us the ability to continuously monitor
    the data collected from the endpoints on a schedule.
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-24\. SageMaker 模型监控使我们能够定期监控从端点收集的数据。
- en: 'We can use `my_default_monitor.create_monitoring_schedule()` to create a model
    monitoring schedule for an endpoint. In the configuration of the monitoring schedule,
    we point to the baseline statistics and constraints and define a cron schedule:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`my_default_monitor.create_monitoring_schedule()`为端点创建模型监控调度。在监控调度的配置中，我们指向基线统计数据和约束，并定义一个
    cron 调度：
- en: '[PRE28]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: SageMaker Model Monitor now runs at the scheduled intervals and analyzes the
    captured data against the baseline. The job creates a violation report and stores
    the report in Amazon S3, along with a statistics report for the collected data.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 模型监控现在按计划间隔运行，并分析捕获的数据与基线的比较。作业生成违规报告，并将报告存储在 Amazon S3 中，同时存储收集数据的统计报告。
- en: 'Once the monitoring job has started its executions, we can use `list_executions()`
    to view all executions:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦监控作业开始执行，我们可以使用`list_executions()`查看所有执行情况：
- en: '[PRE29]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The SageMaker Model Monitor jobs should exit with one of the following statuses:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 模型监控作业应以以下状态之一退出：
- en: Completed
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 完成
- en: The monitoring execution completed and no violations were found.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 监控执行完成且未发现违规。
- en: CompletedWithViolations
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 带违规完成
- en: The monitoring execution completed, but constraint violations were found.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 监控执行完成，但发现了约束违规。
- en: Failed
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 失败
- en: The monitoring execution failed, maybe due to incorrect role permissions or
    infrastructure issues.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 监控执行失败，可能是由于角色权限不正确或基础设施问题。
- en: Stopped
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 停止
- en: The job exceeded the specified maximum runtime or was manually stopped.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 作业超出了指定的最大运行时间或被手动停止。
- en: Note
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We can create our own custom monitoring schedules and procedures using preprocessing
    and postprocessing scripts. We can also build our own analysis container.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用预处理和后处理脚本创建自定义的监控调度和流程。我们也可以构建自己的分析容器。
- en: Inspect Data-Quality Results
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查数据质量结果
- en: With the monitoring data collected and continuously compared against the data-quality
    baseline, we are now in a much better position to make decisions about how to
    improve the model. Depending on the model monitoring results, we might decide
    to retrain and redeploy the model. In this final step, we visualize and interpret
    the data-quality monitoring results, as shown in [Figure 9-25](#visualize_and_interpret_the_data_qualit).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 通过收集的监控数据并持续与数据质量基线进行比较，我们现在能够更好地决定如何改进模型。根据模型监控结果，我们可能决定重新训练和部署模型。在这一最后步骤中，我们展示并解释数据质量监控结果，如[图 9-25](#visualize_and_interpret_the_data_qualit)所示。
- en: '![](assets/dsaw_0925.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0925.png)'
- en: Figure 9-25\. Visualize and interpret the data-quality monitoring results.
  id: totrans-246
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-25\. 可视化和解释数据质量监控结果。
- en: 'Let’s query for the location for the generated reports:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查询生成报告的位置：
- en: '[PRE30]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, we can list the generated reports:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以列出生成的报告：
- en: '[PRE31]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Output:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE32]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We already looked at *constraints.json* and *statistics.json*, so let’s analyze
    the violations:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经查看了*constraints.json*和*statistics.json*，所以让我们分析违规情况：
- en: '[PRE33]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here are example violations for our `review_body` inputs:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们对*review_body*输入的违规示例：
- en: '[PRE34]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: To find the root cause of this data-quality drift, we want to examine the model
    inputs and examine any upstream application bugs (or features) that may have been
    recently introduced. For example, if the application team adds a new set of product
    categories that our model was not trained on, the model may predict poorly for
    those particular product categories. In this case, SageMaker Model Monitor would
    detect the covariate shift in model inputs, notify us, and potentially retrain
    and redeploy the model.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 要找出数据质量漂移的根本原因，我们需要检查模型输入并检查最近引入的任何上游应用程序错误（或功能）。例如，如果应用团队添加了我们的模型未经过训练的新产品类别集，模型可能对这些特定产品类别预测不准确。在这种情况下，SageMaker
    Model Monitor将检测模型输入中的协变量漂移，通知我们，并有可能重新训练和部署模型。
- en: As an extreme example, let’s say that the application team started to feature
    emojis as the primary review mechanism. Given that our review classifier has not
    been trained on a vocabulary that includes emojis, the model may predict poorly
    on reviews that contain emojis. In this case, SageMaker Model Monitor would notify
    us of the change in review-language distribution. We could then retrain and redeploy
    an updated model that understands the emoji language.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 举个极端的例子，假设应用团队开始将表情符号作为主要的评论机制。鉴于我们的评论分类器并未在包含表情符号的词汇表上进行训练，模型可能对包含表情符号的评论预测不准确。在这种情况下，SageMaker
    Model Monitor将通知我们评论语言分布的变化。然后，我们可以重新训练和部署一个理解表情符号语言的更新模型。
- en: Monitor Model Quality of Deployed SageMaker Endpoints
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控部署的SageMaker端点的模型质量
- en: We can also use SageMaker Model Monitor to detect drift in model quality metrics
    such as accuracy. SageMaker Model Monitor compares the online model predictions
    with provided ground truth labels. Model-quality monitoring can be used to detect
    concept drift.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用SageMaker Model Monitor检测模型质量指标如准确性的漂移。SageMaker Model Monitor将在线模型预测与提供的地面实况标签进行比较。模型质量监控可用于检测概念漂移。
- en: Input data is captured by SageMaker Model Monitor using the real-time data capture
    feature. This data is saved into S3 and labeled by humans offline. A Model Quality
    Job then compares the offline data at a schedule that we define. If the model
    quality decays, SageMaker Model Monitor will notify us and potentially retrain
    and redeploy the model, including the ground truth data labeled by humans. Note
    that the availability of the ground truth labels might be delayed because of the
    required human interaction. [Figure 9-26](#comparing_model_predictions_to_ground_t)
    shows the high-level overview of model-quality drift detection using offline,
    ground-truth labels provided by a human workforce.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据由SageMaker Model Monitor使用实时数据捕获功能捕获。这些数据保存到S3，并由人类离线标记。然后，模型质量作业根据我们定义的时间表比较离线数据。如果模型质量下降，SageMaker
    Model Monitor 将通知我们，并有可能重新训练和部署模型，包括由人类标记的地面实况数据。请注意，由于需要人类交互，地面实况标签的可用性可能会延迟。[图 9-26](#comparing_model_predictions_to_ground_t)显示了使用人力工作人员提供的离线地面实况标签进行模型质量漂移检测的高级概述。
- en: '![](assets/dsaw_0926.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0926.png)'
- en: Figure 9-26\. Comparing model predictions to ground-truth data labels generated
    from a human workforce offline.
  id: totrans-263
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-26\. 比较模型预测与由人力离线生成的地面实况数据标签。
- en: 'Here, the Model Quality Job compares the actual, ground truth `star_rating`
    chosen by the human with the predicted `star_rating` from the model endpoint.
    The job calculates a confusion matrix and the standard multiclass classification
    metrics, including accuracy, precision, recall, etc.:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，模型质量任务将实际的人工选择的`star_rating`与模型端点预测的`star_rating`进行比较。该任务计算混淆矩阵和标准多类别分类指标，包括准确率、精确率、召回率等：
- en: '[PRE35]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Before we start monitoring the model quality, we need to create a baseline.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始监控模型质量之前，我们需要创建一个基准。
- en: Create a Baseline to Measure Model Quality
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建基准来衡量模型质量
- en: The model quality baseline job compares the model’s predictions with provided
    ground truth labels we store in S3\. The baseline job then calculates the relevant
    model quality metrics and suggests applicable constraints to identify drift.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 模型质量基准作业将模型的预测与我们在S3中存储的提供的地面实况标签进行比较。然后，基准作业计算相关的模型质量指标，并建议适用的约束条件以识别漂移。
- en: 'We start with the creation of a `ModelQualityMonitor` as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个`ModelQualityMonitor`如下所示：
- en: '[PRE36]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, we can start the baseline job with `suggest_baseline` as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以按以下方式使用`suggest_baseline`启动基准作业：
- en: '[PRE37]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Once the job completes, we can review the suggested constraints in the *constraints.json*
    file in the specified S3 output path. In our example, the file will contain the
    suggested constraints for our multiclass classification model. Make sure to review
    the constraints and adjust them if needed. We will then pass the constraints as
    a parameter when we schedule the model-quality monitoring job:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 任务完成后，我们可以在指定的S3输出路径中的*constraints.json*文件中查看建议的约束条件。在我们的示例中，该文件将包含我们多类别分类模型的建议约束条件。确保审查约束条件并根据需要进行调整。然后，在安排模型质量监控任务时，我们将把约束条件作为参数传递：
- en: '[PRE38]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Schedule Model-Quality Monitoring Jobs
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安排模型质量监控作业
- en: Model-quality monitoring jobs follow the same scheduling steps as data-quality
    monitoring jobs. One difference to keep in mind is that the model-quality monitoring
    jobs assume the availability of ground truth labels for the captured predictions.
    As humans need to provide the ground truth labels, we need to deal with potential
    delays. Therefore, model-quality monitor jobs provide additional `StartOffset`
    and `EndOffset` parameters, which subtract the specified offset from the job’s
    start and end time, respectively.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 模型质量监控作业遵循与数据质量监控作业相同的调度步骤。需要注意的一个区别是，模型质量监控作业假定捕获的预测已经准备好地面实况标签。由于需要人工提供地面实况标签，我们需要处理潜在的延迟。因此，模型质量监控作业提供额外的`StartOffset`和`EndOffset`参数，分别从作业的开始时间和结束时间中减去指定的偏移量。
- en: For example, if we start providing the ground truth labels one day after the
    data capture, we could grant a window of three days for the ground truth data
    to be labeled by specifying a `StartOffset` with -P3D and an `EndOffset` with
    -P1D for the monitoring job. Assuming the ground truth data is labeled in that
    time, the job will analyze data starting three days ago up to one day ago. The
    job then merges the ground truth labels with the captured model predictions and
    calculates the distribution drift.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们开始提供地面实况标签的时间比数据捕获晚一天，我们可以通过指定`StartOffset`为-P3D和`EndOffset`为-P1D为监控作业授予三天的窗口期来标记地面实况数据。假设在那段时间内标记了地面实况数据，则作业将分析从三天前到一天前的数据。然后，作业将地面实况标签与捕获的模型预测合并，并计算分布漂移。
- en: 'We can create the model-quality monitoring job as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按以下方式创建模型质量监控作业：
- en: '[PRE39]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'And we define the monitoring schedule for our `ModelQualityMonitor` as follows:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义我们的`ModelQualityMonitor`的监控计划如下：
- en: '[PRE40]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The `ModelQualityMonitor` now runs at the scheduled intervals and compares the
    model-quality metrics based on the captured data and ground truth labels against
    the baseline. We can inspect the constraint violation reports in Amazon S3.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '`ModelQualityMonitor`现在按预定间隔运行，并基于捕获数据和地面实况标签比较模型质量指标与基线。我们可以在Amazon S3中检查约束违规报告。'
- en: Inspect Model-Quality Monitoring Results
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查模型质量监控结果
- en: '`ModelQualityMonitor` stores the constraint violations in Amazon S3\. We can
    compare the baseline and observed model-quality metrics directly in SageMaker
    Studio, as shown in [Figure 9-27](#sagemaker_studio_endpoint_details_sho), or
    programmatically inspect the constraint violations using the following code. The
    baseline average accuracy is on top, and the current average accuracy is on bottom.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '`ModelQualityMonitor` 将约束违规存储在 Amazon S3 中。我们可以直接在 SageMaker Studio 中比较基线和观察到的模型质量度量，如
    [Figure 9-27](#sagemaker_studio_endpoint_details_sho) 所示，或者通过以下代码程序化地检查约束违规。顶部是基线的平均准确率，底部是当前的平均准确率。'
- en: '![](assets/dsaw_0927.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0927.png)'
- en: Figure 9-27\. SageMaker Studio Endpoint details show charts of model-quality
    metrics such as average accuracy.
  id: totrans-286
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 9-27\. SageMaker Studio 端点详细信息显示模型质量度量图表，如平均准确率。
- en: '[PRE41]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Monitor Bias Drift of Deployed SageMaker Endpoints
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控部署的 SageMaker 端点的偏差漂移
- en: Even though we cleared our training data of bias and took action to mitigate
    bias in our trained models, bias can still be introduced in deployed models. This
    happens if the data that our model sees has a different distribution compared
    to the training data. New data can also cause our model to assign different weights
    to input features. SageMaker Clarify integrates with SageMaker Model Monitor to
    help us to detect bias drift in our deployed models.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们已经清除了训练数据中的偏差并采取了措施来减轻我们训练的模型中的偏差，偏差仍可能会在部署的模型中引入。这种情况发生在我们的模型所见的数据与训练数据的分布不同的情况下。新数据也可能导致我们的模型对输入特征分配不同的权重。SageMaker
    Clarify 与 SageMaker Model Monitor 集成，帮助我们检测部署模型中的偏差漂移。
- en: Create a Baseline to Detect Bias
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建基线以检测偏差
- en: 'SageMaker Clarify continuously monitors the bias metrics of our deployed models
    and raises an alarm if those metrics exceed defined thresholds. We start with
    the creation of a `ModelBiasMonitor` as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Clarify 持续监控我们部署模型的偏差度量，并在这些度量超出定义的阈值时发出警报。我们从创建 `ModelBiasMonitor`
    开始：
- en: '[PRE42]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Similar to detecting post-training model bias with SageMaker Clarify in [Chapter 7](ch07.html#train_your_first_model),
    we need to specify the `DataConfig`, the `BiasConfig`, and the `ModelConfig`,
    which points to the model used for inference. The `ModelPredictedLabelConfig`
    specifies again how to parse the model predictions:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于在 [Chapter 7](ch07.html#train_your_first_model) 中使用 SageMaker Clarify 检测训练后模型偏差，我们需要指定
    `DataConfig`、`BiasConfig` 和 `ModelConfig`，指向用于推理的模型。`ModelPredictedLabelConfig`
    再次指定如何解析模型预测：
- en: '[PRE43]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'With this configuration, we can create and start the model bias baselining
    job:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个配置，我们可以创建并启动模型偏差基线作业：
- en: '[PRE44]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: By calling `suggest_baseline()` we start a SageMaker Clarify Processing Job
    to generate the constraints. Once the job completes and we have our bias baseline,
    we can create a bias-drift monitoring job and schedule.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用 `suggest_baseline()`，我们启动 SageMaker Clarify 处理作业以生成约束条件。一旦作业完成并且我们有了偏差基线，我们可以创建偏差漂移监控作业并进行调度。
- en: Schedule Bias-Drift Monitoring Jobs
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调度偏差漂移监控作业
- en: 'The monitor will automatically pick up the results from the baseline job as
    its model bias analysis configuration. We can also create the analysis configuration
    manually if we haven’t run a baseline job:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 监控将自动从基线作业中获取结果作为其模型偏差分析配置。如果我们还没有运行基线作业，我们也可以手动创建分析配置：
- en: '[PRE45]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Note that the model bias monitor makes use of the provided ground truth label
    data as well. The bias monitoring job merges the ground truth labels with the
    captured model predictions and uses the combined data as its validation dataset.
    The bias drift monitor results are stored in Amazon S3 again.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，模型偏差监视器还使用提供的地面真实标签数据。偏差监控作业将地面真实标签与捕获的模型预测合并，并使用组合数据作为其验证数据集。偏差漂移监控结果再次存储在
    Amazon S3 中。
- en: Inspect Bias-Drift Monitoring Results
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查偏差漂移监控结果
- en: 'We inspect the bias and drift results for each monitored endpoint in SageMaker
    Studio, as shown in [Figure 9-28](#sagemaker_studio_endpoint_details_show), or
    programmatically with the following code:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 SageMaker Studio 中检查每个监控端点的偏差和漂移结果，如 [Figure 9-28](#sagemaker_studio_endpoint_details_show)
    所示，或者通过以下代码进行程序化检查：
- en: '[PRE46]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![](assets/dsaw_0928.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0928.png)'
- en: Figure 9-28\. SageMaker Studio Endpoint details show bias-drift monitoring results.
  id: totrans-306
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 9-28\. SageMaker Studio 端点详细信息显示偏差漂移监控结果。
- en: 'If the bias-drift monitor detected any violations compared to its baseline,
    we can list the violations as follows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 如果偏差漂移监视器检测到与其基线相比的任何违规行为，我们可以列出违规行为如下：
- en: '[PRE47]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Monitor Feature Attribution Drift of Deployed SageMaker Endpoints
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控部署的 SageMaker 端点的特征归因漂移
- en: Similarly to model bias drift, SageMaker Clarify monitors the features contributing
    to the predictions over time. Feature attributions help to explain model predictions.
    If the ranking of feature attributions changes, SageMaker Clarify raises a feature
    attribution drift alarm. SageMaker Clarify implements a model-agnostic method
    called *SHAP* to analyze global and local feature importances. SHAP has been inspired
    by game theory and generates multiple datasets that differ by just one feature.
    SHAP uses the trained model to receive the model predictions for each of the generated
    datasets. The algorithm compares the results against pre-calculated baseline statistics
    to infer the importance of each feature toward the prediction target.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 与模型偏差漂移类似，SageMaker Clarify监控随时间变化的贡献于预测的特征。特征归因有助于解释模型预测。如果特征归因的排名发生变化，SageMaker
    Clarify将提出特征归因漂移警报。SageMaker Clarify实施了一种称为*SHAP*的模型不可知方法来分析全局和局部特征重要性。SHAP受博弈论启发，生成仅通过一个特征不同的多个数据集。SHAP使用训练模型为每个生成的数据集接收模型预测。该算法将结果与预先计算的基线统计数据进行比较，以推断每个特征对预测目标的重要性。
- en: Create a Baseline to Monitor Feature Attribution
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个基线以监控特征归因
- en: 'The feature attribution baseline job can leverage the same dataset used for
    the model bias baseline job:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 特征归因基线作业可以利用用于模型偏差基线作业的相同数据集：
- en: '[PRE48]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'SageMaker Clarify implements SHAP for model explanation. Hence, we need to
    provide a `SHAPConfig` as follows:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Clarify为模型解释实施了SHAP。因此，我们需要按照以下`SHAPConfig`提供：
- en: '[PRE49]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '`shap_baseline` needs to contain a list of rows to be used as the baseline
    dataset, or an S3 object URI to the baseline dataset. The data should only contain
    the feature columns and no label column. `num_samples` specifies the number of
    samples used in the Kernel SHAP algorithm. `agg_method` defines the aggregation
    method for global SHAP values. We can choose between `mean_abs` (mean of absolute
    SHAP values), `median` (median of all SHAP values), and `mean_sq` (mean of squared
    SHAP values).'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`shap_baseline`需要包含用作基线数据集的行列表，或者基线数据集的S3对象URI。数据应只包含特征列，没有标签列。`num_samples`指定在Kernel
    SHAP算法中使用的样本数。`agg_method`定义全局SHAP值的聚合方法。我们可以选择`mean_abs`（绝对SHAP值的平均）、`median`（所有SHAP值的中位数）和`mean_sq`（平方SHAP值的平均）之间。'
- en: 'We can then start the feature attribution baselining job as follows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以按以下方式启动特征归因基线作业：
- en: '[PRE50]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'By calling `suggest_baseline()` we start a SageMaker Clarify Processing Job
    to generate the constraints. Once the baselining job completes, we can view the
    suggested constraints as follows:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用`suggest_baseline()`，我们启动SageMaker Clarify处理作业以生成约束条件。基线作业完成后，我们可以查看建议的约束条件如下：
- en: '[PRE51]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We can now create a feature attribution drift monitoring job and schedule.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建特征归因漂移监控作业并进行调度。
- en: Schedule Feature Attribution Drift Monitoring Jobs
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定时特征归因漂移监控作业
- en: 'The monitor will automatically pick up the results from the baseline job as
    its feature attribution analysis configuration. We can also create the analysis
    configuration manually if we haven’t run a baseline job:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 监视器将自动从基线作业中提取其特征归因分析配置的结果。如果我们尚未运行基线作业，我们也可以手动创建分析配置：
- en: '[PRE52]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Inspect Feature Attribution Drift Monitoring Results
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查特征归因漂移监控结果
- en: 'We can inspect the feature attribution drift monitoring results as follows:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按以下方式检查特征归因漂移监控结果：
- en: '[PRE53]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'If the feature attribution drift monitor detected any violations compared to
    its baseline, we can list the violations as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征归因漂移监视器检测到与其基线相比的任何违规行为，我们可以列出违规行为如下：
- en: '[PRE54]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We can also find the explainability results for each monitored endpoint in SageMaker
    Studio in the endpoint details, as shown in [Figure 9-29](#sagemaker_studio_endpoint_details_sh).
    In addition, we can see a chart that visualizes the change in the top 10 features,
    as shown in [Figure 9-30](#sagemaker_studio_endpoint_details_s).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在SageMaker Studio中，我们还可以在端点详细信息中找到每个监控端点的可解释性结果，如[图 9-29](#sagemaker_studio_endpoint_details_sh)所示。此外，我们可以看到一个图表，可视化了前10个特征的变化，如[图 9-30](#sagemaker_studio_endpoint_details_s)所示。
- en: '![](assets/dsaw_0929.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0929.png)'
- en: Figure 9-29\. SageMaker Studio Endpoint details show model explainability monitoring
    results showing “No Issues” when generating the report.
  id: totrans-332
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-29\. SageMaker Studio端点详细信息显示模型可解释性监控结果，在生成报告时显示“无问题”。
- en: '![](assets/dsaw_0930.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0930.png)'
- en: Figure 9-30\. SageMaker Studio Endpoint details show the change in the top 10
    features, with `review_body`, `review_headline`, `product_category`, `product_title`,
    and `total_votes` as the top 5.
  id: totrans-334
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-30。SageMaker Studio端点详细信息显示前10个特征的变化，其中`review_body`、`review_headline`、`product_category`、`product_title`和`total_votes`是前5个。
- en: Now that we have detailed monitoring of our models in place, we can build additional
    automation. We could leverage the SageMaker Model Monitor integration into CloudWatch
    to trigger actions on baseline drift alarms, such as model updates, training data
    updates, or an automated retraining of our model.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经建立了详细的模型监控体系，我们可以构建额外的自动化。我们可以利用SageMaker Model Monitor与CloudWatch的集成来在基线漂移警报时触发操作，例如模型更新、训练数据更新或自动重新训练我们的模型。
- en: Perform Batch Predictions with SageMaker Batch Transform
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SageMaker批量转换执行批量预测
- en: Amazon SageMaker Batch Transform allows us to make predictions on batches of
    data in S3 without setting up a REST endpoint. Batch predictions are also called
    “offline” predictions since they do not require an online REST endpoint. Typically
    meant for higher-throughput workloads that can tolerate higher latency and lower
    freshness, batch prediction servers typically do not run 24 hours per day like
    real-time prediction servers. They run for a few hours on a batch of data, then
    shut down—hence the term “batch.” SageMaker Batch Transform manages all of the
    resources needed to perform the inferences, including the launch and termination
    of the cluster after the job completes.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker批量转换允许我们在S3上的数据批次上进行预测，而无需设置REST端点。批量预测也称为“离线”预测，因为它们不需要在线REST端点。通常用于可以容忍较高延迟和较低新鲜度的高吞吐量工作负载，批量预测服务器通常不像实时预测服务器那样全天运行。它们在一批数据上运行几个小时，然后关闭，因此称为“批量”。SageMaker批量转换管理执行推断所需的所有资源，包括作业完成后的集群的启动和终止。
- en: For example, if our movie catalog only changes a few times a day, we can likely
    just run one batch prediction job each night that uses a new recommendation model
    trained with the day’s new movies and user activity. Since we are only updating
    the recommendations once in the evening, our recommendations will be a bit stale
    throughout the day. However, our overall cost is minimized and, even more importantly,
    stays predictable.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们的电影目录每天只更改几次，那么我们可能只需每晚运行一次批量预测作业，该作业使用当天新电影和用户活动训练的新推荐模型。由于我们每天晚上只更新一次推荐，因此我们的推荐在一天中的大部分时间可能会有些陈旧。但是，我们能够最小化整体成本，并且更重要的是保持可预测性。
- en: The alternative is to continuously retrain and redeploy new recommendation models
    throughout the day with every new movie that joins or leaves our movie catalog.
    This could lead to excessive model training and deployment costs that are difficult
    to control and predict. These types of continuous updates typically fall under
    the “trending now” category of popular websites like Facebook and Netflix that
    offer real-time content recommendations. We explore these types of continuous
    models when we discuss streaming data analytics.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是在一天中不断地重新训练和部署新的推荐模型，每当新电影加入或离开我们的电影目录时就进行这样的操作。这可能导致过多的模型训练和部署成本，这些成本难以控制和预测。这些类型的持续更新通常属于像Facebook和Netflix等流行网站的“当前热门”类别，它们提供实时内容推荐。我们在讨论流数据分析时探讨了这些类型的连续模型。
- en: Select an Instance Type
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择实例类型
- en: 'Similar to model training, the choice of instance type often involves a balance
    between latency, throughput, and cost. Always start with a small instance type
    and then increase only as needed. Batch predictions may benefit from GPUs more
    than real-time endpoint predictions since GPUs perform much better with large
    batches of data. However, we recommend trying CPU instances first to set the baseline
    for latency, throughput, and cost. Here, we are using a cluster of high-CPU instances:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 与模型训练类似，实例类型的选择通常涉及延迟、吞吐量和成本之间的平衡。始终从较小的实例类型开始，然后根据需要逐步增加。批量预测可能比实时端点预测更适合使用GPU，因为GPU在处理大批量数据时性能更好。然而，我们建议首先尝试CPU实例，以设定延迟、吞吐量和成本的基线。在这里，我们正在使用一组高CPU实例集群：
- en: '[PRE55]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Set Up the Input Data
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置输入数据
- en: 'Let’s specify the input data. In our case, we are using the original TSVs that
    are stored as *gzip* compressed text files:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们指定输入数据。在我们的情况下，我们使用存储为*gzip*压缩文本文件的原始TSV文件：
- en: '[PRE56]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We specify `MultiRecord` for our strategy to take advantage of our multiple
    CPUs. We specify `Gzip` as the compression type since our input data is compressed
    using *gzip*. We’re using TSVs, so `text/csv` is a suitable `accept_type` and
    `content_type`. And since our rows are separated by line breaks, we use `Line`
    for `assemble_with` and `split_type`:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定`MultiRecord`作为我们的策略以利用多个CPU。由于我们的输入数据使用*gzip*压缩，我们指定`Gzip`作为压缩类型。我们使用TSV格式，因此`text/csv`是合适的`accept_type`和`content_type`。由于我们的行是通过换行符分隔的，因此我们使用`Line`作为`assemble_with`和`split_type`：
- en: '[PRE57]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Tune the SageMaker Batch Transform Configuration
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整SageMaker批处理转换配置
- en: When we start the batch transform job, our code runs in an HTTP server inside
    the TensorFlow Serving inference container. Note that TensorFlow Serving natively
    supports batches of data on a single request.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们启动批处理转换作业时，我们的代码在TensorFlow Serving推理容器内部的HTTP服务器上运行。请注意，TensorFlow Serving本地支持单个请求上的数据批处理。
- en: 'Let’s leverage TensorFlow Serving’s built-in batching feature to batch multiple
    records to increase prediction throughput—especially on GPU instances that perform
    well on batches of data. Set the following environment variables to enable batching:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用TensorFlow Serving的内置批处理功能，对多个记录进行批处理，以增加预测吞吐量，特别是在对数据批次性能良好的GPU实例上。设置以下环境变量以启用批处理：
- en: '[PRE58]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Prepare the SageMaker Batch Transform Job
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备SageMaker批处理转换作业
- en: We can inject preprocessing and postprocessing code directly into the Batch
    Transform Container to customize the prediction flow. The preprocessing code is
    specified in *inference.py* and will transform the request from raw data (i.e.,
    `review_body` text) into machine-readable features (i.e., BERT tokens). These
    features are then fed to the model for inference. The model prediction results
    are then passed through the postprocessing code from *inference.py* to convert
    the model prediction into human-readable responses before saving to S3\. [Figure 9-31](#offline_predictions_with_sagemaker_batc)
    shows how SageMaker Batch Transform works in detail.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接将预处理和后处理代码注入到批处理转换容器中，以自定义预测流程。预处理代码在*inference.py*中指定，并将请求从原始数据（即`review_body`文本）转换为机器可读的特征（即BERT标记）。然后，这些特征被馈送到模型进行推理。模型预测结果随后通过*inference.py*中的后处理代码传递，将模型预测转换为人类可读的响应后保存到S3中。[Figure 9-31](#offline_predictions_with_sagemaker_batc)详细展示了SageMaker批处理转换的工作原理。
- en: '![](assets/dsaw_0931.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0931.png)'
- en: 'Figure 9-31\. Offline predictions with SageMaker Batch Transform. Source: Amazon
    SageMaker Developer Guide.'
  id: totrans-355
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-31\. 使用SageMaker批处理转换进行离线预测。来源：Amazon SageMaker开发者指南。
- en: 'Let’s set up the batch transformer to use our *inference.py* script that we
    will show in a bit. We are specifying the S3 location of the classifier model
    that we trained in a previous chapter:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置批处理转换器以使用我们稍后将展示的*inference.py*脚本。我们正在指定在前一章节中训练的分类器模型的S3位置：
- en: '[PRE59]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Following is the *inference.py* script used by the Batch Transform Job defined
    earlier. This script has an `input_handler` for request processing and `output_handler`
    for response processing, as shown in [Figure 9-32](#preprocessing_request_handler_and_postp).
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面定义的Batch Transform作业使用的*inference.py*脚本。此脚本具有用于请求处理的`input_handler`和用于响应处理的`output_handler`，如[Figure 9-32](#preprocessing_request_handler_and_postp)所示。
- en: '![](assets/dsaw_0932.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0932.png)'
- en: Figure 9-32\. Preprocessing request handler and postprocessing response handler.
  id: totrans-360
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-32\. 预处理请求处理程序和后处理响应处理程序。
- en: The preprocessing handler, `input_handler`, and the postprocessing handler,
    `output_handler`, are similar to the functions used for the SageMaker REST Endpoint
    earlier. The `input_handler` function converts batches of raw text into BERT tokens
    using the Transformer library. SageMaker then passes this batched output from
    the `input_handler` into our model, which produces batches of predictions. The
    predictions are passed through the `output_handler` function, which converts the
    prediction into a JSON response. SageMaker then joins each prediction within a
    batch to its specific line of input. This produces a single, coherent line of
    output for each row that was passed in.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 输入处理程序`input_handler`和输出处理程序`output_handler`类似于之前用于SageMaker REST端点的函数。`input_handler`函数使用Transformer库将批量的原始文本转换为BERT标记。然后SageMaker将这些从`input_handler`输出的批量输出传递到我们的模型中，模型生成批量预测。预测结果经过`output_handler`函数，将预测转换为JSON响应。然后SageMaker将每个预测与其特定输入行结合起来。这为每个传入的行产生了单一的连贯输出行。
- en: Run the SageMaker Batch Transform Job
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行SageMaker批处理转换作业
- en: 'Next we will specify the input data and start the actual Batch Transform Job.
    Note that our input data is compressed using *gzip* as Batch Transform Jobs support
    many types of compression:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将指定输入数据，并启动实际的批量转换作业。请注意，我们的输入数据使用 *gzip* 进行了压缩，因为批量转换作业支持多种类型的压缩：
- en: '[PRE60]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: We specify `join_source='Input'` to force SageMaker to join our prediction with
    the original input before writing to S3\. And while not shown here, SageMaker
    lets us specify the exact input features to pass into this batch transformation
    process using `InputFilter` and the exact data to write to S3 using `OutputFilter`.
    This helps to reduce overhead, reduce cost, and improve batch prediction performance.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定 `join_source='Input'`，以强制 SageMaker 在写入 S3 之前将我们的预测结果与原始输入进行合并。尽管此处未显示，但
    SageMaker 允许我们指定要传递到批量转换过程的精确输入特征，使用 `InputFilter` 和要写入 S3 的精确数据使用 `OutputFilter`。这有助于减少开销、降低成本，并提高批量预测性能。
- en: If we are using `join_source='Input'` and `InputFilter` together, SageMaker
    will join the original inputs—including the filtered-out inputs—with the predictions
    to keep all of the data together. We can also filter the outputs to reduce the
    size of the prediction files written to S3\. The whole flow is shown in [Figure 9-33](#filtering_and_joining_inputs_to_reduce).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们同时使用 `join_source='Input'` 和 `InputFilter`，SageMaker 将原始输入（包括被过滤掉的输入）与预测结果合并，以保持所有数据的完整性。我们还可以过滤输出，以减小写入
    S3 的预测文件的大小。整个流程如 [图 9-33](#filtering_and_joining_inputs_to_reduce) 所示。
- en: '![](assets/dsaw_0933.png)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0933.png)'
- en: Figure 9-33\. Filtering and joining inputs to reduce overhead and improve performance.
  id: totrans-368
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-33\. 过滤和合并输入以减少开销和提高性能。
- en: Review the Batch Predictions
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看批量预测
- en: 'Once the Batch Transform Job completes, we can review the generated comma-separated
    *.out* files that contain our `review_body` inputs and `star_rating` predictions
    as shown here:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 批量转换作业完成后，我们可以查看生成的逗号分隔的 *.out* 文件，其中包含我们的 `review_body` 输入和 `star_rating` 预测，如下所示：
- en: '[PRE61]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Here are a few sample predictions:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些示例预测：
- en: '[PRE62]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: At this point, we have performed a large number of predictions and generated
    comma-separated output files. With a little bit of application code (SQL, Python,
    Java, etc.), we can use these predictions to power natural-language-based applications
    to improve the customer service experience, for example.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经进行了大量预测，并生成了逗号分隔的输出文件。借助一点应用程序代码（SQL、Python、Java 等），我们可以利用这些预测来支持基于自然语言的应用程序，以改进客户服务体验，例如。
- en: AWS Lambda Functions and Amazon API Gateway
  id: totrans-375
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS Lambda 函数和亚马逊 API 网关
- en: We can also deploy our models as serverless APIs with Lambda. When a prediction
    request arrives, the Lambda function loads the model and executes the inference
    function code. Models can be loaded directly from within the Lambda function or
    from a data store like Amazon S3 and EFS. Lambda functions are callable from many
    AWS services, including Amazon Simple Queue Service and S3, to effectively implement
    event-based predictions.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将模型部署为 Lambda 的无服务器 API。当预测请求到达时，Lambda 函数加载模型并执行推理函数代码。模型可以直接从 Lambda
    函数内部加载，也可以从 Amazon S3 和 EFS 等数据存储加载。Lambda 函数可以从许多 AWS 服务调用，包括 Amazon Simple Queue
    Service 和 S3，以有效地实现基于事件的预测。
- en: We can use the “provisioned concurrency” feature of Lambda to pre-load the model
    into the function and greatly improve prediction latency. Amazon API Gateway provides
    additional support for application authentication, authorization, caching, rate-limiting,
    and web application firewall rules. [Figure 9-34](#implement_serverless_inference_with_aws)
    shows how we implement serverless inference with Lambda and API Gateway.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Lambda 的“预置并发”功能预加载模型到函数中，大大提高预测延迟。亚马逊 API 网关还提供了额外的支持，用于应用程序身份验证、授权、缓存、速率限制和
    Web 应用程序防火墙规则。[图 9-34](#implement_serverless_inference_with_aws) 显示了我们如何使用 Lambda
    和 API 网关实现无服务器推理。
- en: '![](assets/dsaw_0934.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0934.png)'
- en: Figure 9-34\. Serverless inference with AWS Lambda.
  id: totrans-379
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-34\. 使用 AWS Lambda 进行无服务器推理。
- en: Optimize and Manage Models at the Edge
  id: totrans-380
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在边缘优化和管理模型
- en: We can leverage Amazon SageMaker Neo Compilation Jobs to optimize our model
    for specific hardware platforms such as AWS Inferentia, NVIDIA GPUs, Intel CPUs,
    and ARM CPUs. SageMaker Neo frees us from manually tuning our models to specific
    hardware and software configurations found in different CPU and GPU architectures,
    or edge device platforms with limited compute and storage resources. The SageMaker
    Neo compiler converts models into efficient and compact formats using device-specific
    instruction sets. These instructions perform low-latency machine learning inference
    on the target device directly.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用Amazon SageMaker Neo编译作业来优化我们的模型，使其适配特定的硬件平台，例如AWS Inferentia、NVIDIA GPU、Intel
    CPU和ARM CPU。SageMaker Neo使我们摆脱了手动调整模型以适配不同CPU和GPU架构或具有有限计算和存储资源的边缘设备平台的繁琐工作。SageMaker
    Neo编译器使用设备特定的指令集将模型转换为高效且紧凑的格式。这些指令集直接在目标设备上执行低延迟的机器学习推理。
- en: Note
  id: totrans-382
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In 2019, AWS made [SageMaker Neo](https://oreil.ly/CkO1f) open source to allow
    processor vendors, device manufacturers, and software developers to collaborate
    and bring ML models to a diverse set of hardware-optimized platforms.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年，AWS开源了[SageMaker Neo](https://oreil.ly/CkO1f)，以便处理器供应商、设备制造商和软件开发人员可以合作，并将ML模型带到多种硬件优化平台上。
- en: Once the model is compiled by SageMaker Neo, SageMaker Edge Manager cryptographically
    signs the model, packages the model with a lightweight runtime, and uploads the
    model package to an S3 bucket in preparation for deployment. SageMaker Edge Manager
    manages models across all registered edge devices, tracks model versions, collects
    health metrics, and periodically captures model inputs and outputs to detect model
    drift and degradation.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型由SageMaker Neo编译，SageMaker Edge Manager将对模型进行加密签名，打包模型与轻量级运行时，并将模型包上传到S3存储桶，以便部署。SageMaker
    Edge Manager管理所有注册的边缘设备上的模型，跟踪模型版本，收集健康指标，并定期捕获模型输入和输出，以检测模型漂移和退化。
- en: Deploy a PyTorch Model with TorchServe
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TorchServe部署PyTorch模型
- en: TorchServe is an open source collaboration between AWS, Facebook, and the PyTorch
    community. With TorchServe, we can serve PyTorch models in production as REST
    endpoints similar to TensorFlow Serving. SageMaker provides native TorchServe
    integration, which allows us to focus on the business logic of the prediction
    request versus the infrastructure code.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe是AWS、Facebook和PyTorch社区之间的开源协作项目。借助TorchServe，我们可以将PyTorch模型作为REST端点在生产中提供服务，类似于TensorFlow
    Serving。SageMaker提供了本地的TorchServe集成，允许我们专注于预测请求的业务逻辑，而不是基础设施代码。
- en: 'Similar to the TensorFlow Serving–based SageMaker Endpoint we created earlier,
    we need to provide a Python-based request and response handler called *inference.py*
    to transform raw review text from the REST request from JSON to PyTorch input
    BERT vectors. Additionally, *inference.py* needs to transform the PyTorch `star_rating`
    classification response back into JSON to return to the calling application. The
    following is a relevant snippet from *inference.py*:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前创建的基于TensorFlow Serving的SageMaker端点类似，我们需要提供一个基于Python的请求和响应处理程序，称为*inference.py*，将原始的评论文本从JSON格式的REST请求转换为PyTorch输入的BERT向量。此外，*inference.py*还需要将PyTorch
    `star_rating`分类响应转换回JSON格式，以返回给调用应用程序。以下是*inference.py*的相关片段：
- en: '[PRE63]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Let’s deploy our model as a SageMaker Endpoint with our *inference.py* request/response
    handler:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们的*inference.py*请求/响应处理程序将模型部署为SageMaker端点：
- en: '[PRE64]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Now we can make a prediction by passing review text to our review classifier
    endpoint:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过将评论文本传递给我们的评论分类器端点来进行预测：
- en: '[PRE65]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: TensorFlow-BERT Inference with AWS Deep Java Library
  id: totrans-393
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AWS Deep Java Library进行TensorFlow-BERT推理
- en: 'Let’s import the required Java libraries from AWS Deep Java Library (DJL):'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从AWS Deep Java Library (DJL)导入所需的Java库：
- en: '[PRE66]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Next, let’s download the pre-trained DistilBERT TensorFlow model:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们下载预训练的DistilBERT TensorFlow模型：
- en: '[PRE67]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Next, we set up the BERT Tokenizer and define the Translator to transform raw
    text into BERT embeddings:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置了BERT Tokenizer并定义了Translator来将原始文本转换为BERT嵌入：
- en: '[PRE68]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Last, we load the model and make some predictions with BERT and Java!
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们加载模型，并使用BERT和Java进行一些预测！
- en: '[PRE69]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Reduce Cost and Increase Performance
  id: totrans-402
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少成本并提高性能
- en: In this section, we describe multiple ways to reduce cost and increase performance
    by packing multiple models into a single SageMaker deployment container, utilizing
    GPU-based Elastic Inference Accelerators, optimizing our trained model for specific
    hardware, and utilizing inference-optimized hardware such as the AWS Inferentia
    chip.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了通过将多个模型打包到单个SageMaker部署容器中、利用基于GPU的弹性推理加速器、优化我们的训练模型以适应特定硬件，并利用AWS
    Inferentia芯片等推理优化硬件的多种方式来降低成本并提高性能。
- en: Delete Unused Endpoints and Scale In Underutilized Clusters
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除未使用的端点并在资源利用率低的集群下进行缩放
- en: SageMaker Endpoints are long-running resources and are easy to leave running
    after a successful blue/green deployment, for example. We should remove unused
    resources as soon as possible. We can set up CloudWatch alerts to notify us when
    a SageMaker Endpoint is not receiving invocations. Similarly, we should remember
    to scale in a SageMaker Endpoint cluster if the cluster is overprovisioned and
    underutilized.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker端点是长期运行的资源，在成功的蓝/绿部署后很容易保持运行状态。我们应尽快删除未使用的资源。我们可以设置CloudWatch警报，以在SageMaker端点未收到调用时通知我们。同样，如果集群过度配置且资源利用不足，我们应记得缩减SageMaker端点集群。
- en: Deploy Multiple Models in One Container
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在一个容器中部署多个模型
- en: If we have a large number of similar models that we can serve through a shared
    serving container—and don’t need to access all the models at the same time—we
    can deploy multiple models within a single SageMaker Endpoint. When there is a
    long tail of ML models that are infrequently accessed, using one endpoint can
    efficiently serve inference traffic and enable significant cost savings.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有大量相似的模型可以通过共享服务容器提供，并且不需要同时访问所有模型，我们可以在单个SageMaker端点中部署多个模型。当存在一长尾的ML模型很少被访问时，使用一个端点可以有效地服务推理流量并实现显著的成本节约。
- en: Each of the SageMaker Endpoints can automatically load and unload models based
    on traffic and resource utilization. For example, if traffic to Model 1 goes to
    zero and Model 2 traffic spikes, SageMaker will dynamically unload Model 1 and
    load another instance of Model 2\. We can invoke a specific model variant by specifying
    the target model name as a parameter in our prediction request, as shown in [Figure 9-35](#invoke_a_specific_model_within_a_sagema).
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 每个SageMaker端点可以根据流量和资源利用率自动加载和卸载模型。例如，如果对模型1的流量为零而模型2的流量激增，则SageMaker将动态卸载模型1并加载模型2的另一个实例。我们可以通过在预测请求中指定目标模型名称作为参数来调用特定的模型变体，如[图9-35](#invoke_a_specific_model_within_a_sagema)所示。
- en: '![](assets/dsaw_0935.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0935.png)'
- en: Figure 9-35\. Invoke a specific model within a SageMaker Endpoint that hosts
    multiple models.
  id: totrans-410
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-35\. 在托管多个模型的SageMaker端点内调用特定模型。
- en: This lets us train two different category-specific TensorFlow models—`Digital_Software`
    and `Gift_Card`, for example—and deploy them to a single endpoint for convenience
    and cost-savings purposes. Here is code to deploy the two models into a single
    SageMaker Endpoint.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够训练两种不同的类别特定的TensorFlow模型—`Digital_Software`和`Gift_Card`，例如—并将它们部署到一个单一的端点以方便和节省成本。以下是将这两个模型部署到单个SageMaker端点的代码。
- en: 'For TensorFlow, we need to package the models as follows:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 对于TensorFlow，我们需要按以下方式打包模型：
- en: '[PRE70]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Attach a GPU-Based Elastic Inference Accelerator
  id: totrans-415
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附加基于GPU的弹性推理加速器
- en: Elastic Inference Accelerator (EIA) is a low-cost, dynamically attached, GPU-powered
    add-on for SageMaker instances. While standalone GPU instances are a good fit
    for model training on large datasets, they are typically oversized for smaller-batch
    inference requests, which consume small amounts of GPU resources.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性推理加速器（EIA）是SageMaker实例的低成本、动态附加的GPU驱动附件。虽然独立的GPU实例非常适合大型数据集的模型训练，但对于消耗小量GPU资源的小批量推理请求来说通常是过度配置的。
- en: While AWS offers a wide range of instance types with different GPU, CPU, network
    bandwidth, and memory combinations, our model may use a custom combination. With
    EIAs, we can start by choosing a base CPU instance and add GPUs until we find
    the right balance for our model inference needs. Otherwise, we may be forced to
    optimize one set of resources like CPU and RAM but underutilize other resources
    like GPU and network bandwidth.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: AWS提供了多种不同GPU、CPU、网络带宽和内存组合的实例类型，我们的模型可能使用自定义组合。使用EIAs，我们可以从选择基础CPU实例开始，并添加GPU，直到找到适合我们模型推理需求的平衡点。否则，我们可能被迫优化一个资源集合如CPU和RAM，但未充分利用其他资源如GPU和网络带宽。
- en: 'Here is the code to deploy our same model but with EIA:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是部署我们同一模型但带有EIA的代码：
- en: '[PRE72]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Optimize a Trained Model with SageMaker Neo and TensorFlow Lite
  id: totrans-420
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SageMaker Neo和TensorFlow Lite优化训练模型
- en: SageMaker Neo takes a trained model and performs a series of hardware-specific
    optimizations, such as 16-bit quantization, graph pruning, layer fusing, and constant
    folding for up to 2x model-prediction speedups with minimal accuracy loss. SageMaker
    Neo works across popular AI and machine learning frameworks, including TensorFlow,
    PyTorch, Apache MXNet, and XGBoost.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Neo接受经过训练的模型，并执行一系列针对硬件的特定优化，如16位量化、图形修剪、层融合和常量折叠，以最小化精度损失获得高达2倍的模型预测加速。SageMaker
    Neo适用于流行的AI和机器学习框架，包括TensorFlow、PyTorch、Apache MXNet和XGBoost。
- en: SageMaker Neo parses the model, optimizes the graph, quantizes tensors, and
    generates hardware-specific code for a variety of target environments, including
    Intel x86 CPUs, NVIDIA GPUs, and AWS Inferentia, as shown in [Figure 9-36](#sagemaker_neo_parses_modelscomma_optimi).
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Neo解析模型、优化图形、量化张量，并为包括Intel x86 CPU、NVIDIA GPU和AWS Inferentia在内的各种目标环境生成硬件特定代码，如[图9-36](#sagemaker_neo_parses_modelscomma_optimi)所示。
- en: '![](assets/dsaw_0936.png)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0936.png)'
- en: Figure 9-36\. SageMaker Neo delivers model compilation as a service.
  id: totrans-424
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-36\. SageMaker Neo提供模型编译作为服务。
- en: SageMaker Neo supports TensorFlow Lite (TFLite), a lightweight, highly optimized
    TensorFlow runtime interpreter and code generator for small devices with limited
    memory and compute resources. SageMaker Neo uses the TFLite converter to perform
    hardware-specific optimizations for the TensorFlow Lite runtime interpreter, as
    shown in [Figure 9-37](#tflite_interpreter_left_parenthesissour).
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Neo支持TensorFlow Lite（TFLite），这是一个针对内存和计算资源有限的小型设备高度优化的轻量级TensorFlow运行时解释器和代码生成器。SageMaker
    Neo使用TFLite转换器执行针对硬件的特定优化，以用于TensorFlow Lite运行时解释器，如[图9-37](#tflite_interpreter_left_parenthesissour)所示。
- en: '![](assets/dsaw_0937.png)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0937.png)'
- en: 'Figure 9-37\. TFLite interpreter. Source: [*TensorFlow*](https://oreil.ly/QWiV8).'
  id: totrans-427
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-37\. TFLite解释器。来源：[*TensorFlow*](https://oreil.ly/QWiV8)。
- en: 'We can choose to optimize for small size (`tf.lite.Optimize.OPTIMIZE_FOR_SIZE`),
    optimize for low latency (`tf.lite.OPTIMIZE_FOR_LATENCY`), or balance size and
    performance (`tf.lite.Optimize.DEFAULT`). Here is the TFLite code that performs
    16-bit quantization on a TensorFlow model with a balance between size and performance:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择优化小尺寸（`tf.lite.Optimize.OPTIMIZE_FOR_SIZE`）、优化低延迟（`tf.lite.OPTIMIZE_FOR_LATENCY`）或在尺寸和性能之间取得平衡（`tf.lite.Optimize.DEFAULT`）。以下是在TensorFlow模型上执行16位量化的TFLite代码，以在尺寸和性能之间取得平衡：
- en: '[PRE73]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Here is the prediction code that leads to an order-of-magnitude speedup in
    prediction time due to the quantization:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是由于量化而导致预测时间提升一个数量级的预测代码：
- en: '[PRE74]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Use Inference-Optimized Hardware
  id: totrans-432
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用推理优化硬件
- en: AWS Inferentia, is an inference-optimized chip used by the Amazon “Inf” instance
    types. The chip accelerates 16-bit and 8-bit floating-point operations generated
    by the AWS Neuron compiler to optimize our model for the AWS Inferentia chip and
    SageMaker Neo and Neuron runtimes (see in [Figure 9-38](#sagemaker_neoapostrophes_neuron_optimiz)).
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Inferentia是亚马逊“Inf”实例类型使用的推理优化芯片。该芯片通过AWS Neuron编译器生成的16位和8位浮点操作来优化我们的模型，以适应AWS
    Inferentia芯片以及SageMaker Neo和Neuron运行时（见[图9-38](#sagemaker_neoapostrophes_neuron_optimiz)）。
- en: '![](assets/dsaw_0938.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0938.png)'
- en: Figure 9-38\. SageMaker Neuron compiler and Neo runtime for the AWS Inferentia
    chip.
  id: totrans-435
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-38\. SageMaker Neuron编译器和AWS Inferentia芯片的Neo运行时。
- en: Summary
  id: totrans-436
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we moved our models out of the research lab and into the end-user
    application domain. We showed how to measure, improve, and deploy our models using
    real-world, production-ready fundamentals such as canary rollouts, blue/green
    deployments, and A/B tests. We demonstrated how to perform data-drift, model-drift,
    and feature-attribution-drift detection. In addition, we performed batch transformations
    to improve throughput for offline model predictions. We closed out with tips on
    how to reduce cost and improve performance using SageMaker Neo, TensorFlow Lite,
    SageMaker Multimodel Endpoints, and inference-optimized hardware such as EIA and
    AWS Inferentia.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将我们的模型从研究实验室移至最终用户应用领域。我们展示了如何利用真实世界的、生产就绪的基础设施，如金丝雀发布、蓝/绿部署和A/B测试来衡量、改进和部署我们的模型。我们演示了如何进行数据漂移、模型漂移和特征归因漂移检测。此外，我们进行了批处理转换以提高离线模型预测的吞吐量。最后，我们提供了使用SageMaker
    Neo、TensorFlow Lite、SageMaker多模型端点以及推理优化硬件（如EIA和AWS Inferentia）来降低成本和提高性能的提示。
- en: In [Chapter 10](ch10.html#pipelines_and_mlops), we bring the feature engineering,
    model training, model validation, and model deploying steps into a single, unified,
    and end-to-end automated pipeline using SageMaker Pipelines, AWS Step Functions,
    Apache Airflow, Kubeflow, and various other open source options.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 10 章](ch10.html#pipelines_and_mlops) 中，我们利用 SageMaker Pipelines、AWS Step
    Functions、Apache Airflow、Kubeflow 和其他多种开源选项，将特征工程、模型训练、模型验证和模型部署步骤统一到一个端到端自动化流水线中。
