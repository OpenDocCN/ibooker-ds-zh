- en: 8 Evolving autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 进化自编码器
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Introducing convolutional autoencoders
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍卷积自编码器
- en: Discussing genetic encoding in a convolutional autoencoder network
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论卷积自编码器网络中的遗传编码
- en: Applying mutation and mating to develop an evolutionary autoencoder
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用变异和交配来开发进化自编码器
- en: Building and evolving autoencoder architecture
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和进化自编码器架构
- en: Introducing a convolutional variational autoencoder
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍卷积变分自编码器
- en: In the last chapter, we covered how convolutional neural network (CNN) architecture
    could be adapted using evolutionary algorithms. We used genetic algorithms to
    encode a `gene` sequence defining a CNN model for image classification. The outcome
    was successfully building more optimized networks for image recognition tasks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了如何使用进化算法来调整卷积神经网络（CNN）架构。我们使用遗传算法来编码定义CNN模型的`基因`序列，用于图像分类。结果是成功构建了更优化的网络，用于图像识别任务。
- en: In this chapter, we continue to extend the fundamentals and explore evolving
    autoencoders (AEs). We take some of our experience from building evolving CNN
    architecture in the last chapter and apply it to convolutional AEs. Then, we move
    on to more advanced variational AEs and explore novel ways of evolving model loss.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们继续扩展基础知识并探索进化的自编码器（AE）。我们从上一章构建进化的CNN架构的经验中汲取了一些经验，并将其应用于卷积自编码器。然后，我们转向更高级的变分自编码器，并探索进化模型损失的新方法。
- en: AEs are a foundation to DL that introduces unsupervised and representation learning.
    Chances are if you have spent any time studying DL, you have encountered AEs and
    variational AEs. From the perspective of EDL, they introduce some novel applications
    we explore in this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是深度学习的基础，它引入了无监督和代表性学习。很可能，如果你花了一些时间研究深度学习，你一定遇到过自编码器和变分自编码器。从进化深度学习的角度来看，它们引入了一些我们在本章中探索的新应用。
- en: AEs come in several variations, from under complete or standard to deep and
    convolutional. The deep convolutional AE is a great one to begin with, since it
    extends many ideas from previous chapters, and it’s where we start this chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器有多种变体，从不完全或标准到深度和卷积。深度卷积自编码器是一个很好的起点，因为它扩展了前几章中的许多想法，这也是我们本章的起点。
- en: 8.1 The convolution autoencoder
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 卷积自编码器
- en: In this section, we explore and review a convolutional AE written in Keras.
    This is the same code we use for building an evolutionary, or evo, AE later in
    the chapter. For those new to AEs, the next section reviews the main principles
    of training, building, and retraining.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探索并回顾了一个用Keras编写的卷积自编码器。这是我们在本章后面构建进化型或evo自编码器时使用的相同代码。对于新接触自编码器的人来说，下一节回顾了训练、构建和重新训练的主要原则。
- en: 8.1.1 Introducing autoencoders
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 介绍自编码器
- en: AEs are often used to introduce the concepts of unsupervised and representative
    learning. *Unsupervised learning* is the process of training models using no labels.
    *Representative learning* is when we train models to understand the differences
    between input features.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器通常用于介绍无监督和代表性学习的概念。*无监督学习*是使用无标签训练模型的过程。*代表性学习*是指我们训练模型来理解输入特征之间的差异。
- en: Figure 8.1 shows a simple convolutional AE that is comprised of convolutional,
    `MaxPool`, and `UpSampling` layers. Aside from the addition of convolution, this
    model architecture is standard for an AE.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1显示了一个简单的由卷积、`MaxPool`和`UpSampling`层组成的卷积自编码器。除了添加卷积之外，这种模型架构对于自编码器来说是标准的。
- en: '![](../Images/CH08_F01_Lanham.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F01_Lanham.png)'
- en: Figure 8.1 A convolutional AE
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 卷积自编码器
- en: An AE works by funneling the input through a narrow channel called the *latent*
    or *feature representation view*—the middle part. This middle part is also known
    as the *latent* or *hidden encoding* of the image.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器通过将输入通过称为*潜在*或*特征表示视图*的狭窄通道——中间部分——来工作。这个中间部分也被称为图像的*潜在*或*隐藏编码*。
- en: The latent encoding of an image is learned through iterations of passing images
    into the encoder and then measuring the difference in the output. Typically, we
    measure this difference or loss using mean squared error or pixel-wise loss of
    the input and output images. Through iterating, the middle part learns to encapsulate
    the features of the input image.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将图像迭代地输入编码器并测量输出差异来学习图像的潜在编码。通常，我们使用均方误差或输入和输出图像的像素级损失来测量这个差异或损失。通过迭代，中间部分学会封装输入图像的特征。
- en: Figure 8.2 shows an example of plotting the learned encoding from an AE trained
    on the MNIST Handwritten Digits dataset. In the figure, the encoding/latent space
    is converted to two dimensions using *t*-distributed stochastic neighbor embedding
    (*t*-SNE). By visualizing this plot, you can clearly see how the model learns
    to differentiate between the various classes of digits.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 展示了在 MNIST 手写数字数据集上训练的 AE 学习到的编码的示例。在图中，编码/潜在空间通过 *t*-分布随机邻域嵌入 (*t*-SNE)
    转换为二维。通过可视化这个图表，你可以清楚地看到模型是如何学习区分不同数字类别的。
- en: '![](../Images/CH08_F02_Lanham.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F02_Lanham.png)'
- en: Figure 8.2 A mapping of AE latent space, showing a clustering of classes
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 展示了 AE 潜在空间的一个映射，显示了类别的聚类
- en: An AE uses unsupervised learning to train, which means none of the data fed
    into the model needs to be labeled. Essentially, the model learns through self-training
    by comparing how well the input and generated output can represent the encoded
    features. This simplifies the training of the model and, at the same time, creates
    a powerful feature encoding extractor.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: AE 使用无监督学习进行训练，这意味着模型中输入的数据无需标记。本质上，模型通过比较输入和生成的输出如何表示编码特征的好坏来通过自我训练进行学习。这简化了模型的训练，同时，也创建了一个强大的特征编码提取器。
- en: '*Representation learning*, or what may also be referred to as *generative deep
    learning*, is a relatively new field. We take a close look at GDL in the next
    chapter, but for now, let’s jump back into the code and see how an AE works.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*表示学习*，或可能也被称为 *生成式深度学习*，是一个相对较新的领域。我们将在下一章中详细探讨 GDL，但现在，让我们回到代码中，看看 AE 是如何工作的。'
- en: 8.1.2 Building a convolutional autoencoder
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 构建卷积自编码器
- en: The AE we look at in the next notebook incorporates convolutional layers to
    better extract features in images. Applying convolution to an AE model introduces
    additional complexity in the network architecture. In future sections, this example
    also demonstrates how applying evolution to optimize these networks is advantageous.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下一个笔记本中查看的 AE 集成了卷积层，以更好地从图像中提取特征。将卷积应用于 AE 模型会在网络架构中引入额外的复杂性。在未来的章节中，这个例子还将展示如何应用进化来优化这些网络的优势。
- en: Open the EDL_8_1_Autoencoders.ipynb notebook in Colab. Refer to the appendix
    if you need a review of opening notebooks in Colab.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Colab 中打开 EDL_8_1_Autoencoders.ipynb 笔记本。如需回顾在 Colab 中打开笔记本的方法，请参阅附录。
- en: 'Scroll down, and then select the Stage 1: AEs cell. From the menu, select Runtime
    > Run Before. This runs the notebook, loading the data and displaying an example
    plot, as shown in Figure 8.3\. Several sections of code have been covered in past
    chapters and won’t be reviewed again here.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动页面，然后选择“阶段 1：AE 细胞”。从菜单中选择“运行前”。这将运行笔记本，加载数据并显示如图 8.3 所示的示例图表。过去几章中已经涵盖了几个代码部分，这里将不再进行回顾。
- en: '![](../Images/CH08_F03_Lanham.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F03_Lanham.png)'
- en: Figure 8.3 Fashion-MNIST standard training dataset
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 Fashion-MNIST 标准训练数据集
- en: 'Next, we come to the first block of code of interest, shown in listing 8.1:
    building the AE. The first layer set up is the input layer, defined by image shape
    (28×28 and 1 channel). Next, a convolutional layer is added with 64 filters, using
    a kernel size of 3×3\. Then, after each CNN layer, a `MaxPool` layer reduces/aggregates
    the input into the next layer. The final layer added is a `MaxPool` layer that
    represents the latent or hidden view of the input.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将来到第一个感兴趣的代码块，如图表 8.1 所示：构建 AE。第一层设置是输入层，由图像形状（28×28 和 1 个通道）定义。接下来，添加了一个使用
    3×3 核大小的 64 个滤波器的卷积层。然后，在每个 CNN 层之后，一个 `MaxPool` 层将输入减少/聚合到下一层。最后添加的层是一个 `MaxPool`
    层，它代表了输入的潜在或隐藏视图。
- en: 'Listing 8.1 EDL_8_1_AE.ipynb: The encoder'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.1 EDL_8_1_AE.ipynb：编码器
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Defines the input layer
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义输入层
- en: ❷ The 2D convolution layer
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 2D 卷积层
- en: ❸ The MaxPool layer
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 最大池化层
- en: Now that we have built the encoder model to output the latent or encoded view,
    we need to rebuild the image using further convolutional layers and a special
    layer called `UpSampling`. `UpSampling` layers can be thought of as the opposite
    of `pooling` layers. Their effect is converting the latent view generated by the
    encoder back into a full image. This is done by successively convolving the input
    and `UpSampling` to successive layers. At the end of this output chain, we add
    a final CNN layer that converts the convolved output to a single channel. If we
    were using color images, we would instead convert the output to three channels,
    as shown in the following listing.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了编码器模型以输出潜在或编码视图，我们需要使用进一步的卷积层和称为`UpSampling`的特殊层来重建图像。`UpSampling`层可以被认为是`pooling`层的相反。它们的效果是将编码器生成的潜在视图转换回完整图像。这是通过连续卷积输入和`UpSampling`到连续层来完成的。在这个输出链的末尾，我们添加一个最终的CNN层，将卷积输出转换为单个通道。如果我们使用彩色图像，我们将输出转换为三个通道，如下面的列表所示。
- en: 'Listing 8.2 EDL_8_1_AE.ipynb: The decoder'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.2 EDL_8_1_AE.ipynb：解码器
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ The 2D convolutional layer
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 2D卷积层
- en: ❷ The 2D UpSampling layer
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 2D上采样层
- en: ❸ The final CNN layer for output to 1 channel
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 输出到1个通道的最终CNN层
- en: We combine the models by feeding the corresponding input and output layers into
    a Keras model. Then, we compile the model using an Adam optimizer and MSE for
    loss. After that, we plot a model summary and use `plot_model` to output a nice
    visual of the completed model, as shown in the following listing.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将相应的输入和输出层输入到Keras模型中来组合模型。然后，我们使用Adam优化器和MSE损失函数来编译模型。之后，我们绘制模型摘要并使用`plot_model`输出完成的模型的美观视觉，如下面的列表所示。
- en: 'Listing 8.3 EDL_8_1_AE.ipynb: Building the model'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.3 EDL_8_1_AE.ipynb：构建模型
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Builds from input and output layers
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从输入和输出层构建
- en: ❷ Compiles with Adam and MSE
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用Adam和MSE编译
- en: ❸ Outputs the model summary
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 输出模型摘要
- en: ❹ Generates a plot of the model
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 生成模型的图表
- en: Run the cells that build the encoder and decoder and build the model. Figure
    8.4 shows the summary output of building the model. By looking at each successive
    layer, you can visualize how the model shrinks the input space in the latent encoding
    and then rebuilds it. It is important to note the size of the respective CNN layers
    and how they reduce and then increase in size.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 运行构建编码器和解码器以及构建模型的单元格。图8.4显示了构建模型的摘要输出。通过查看每个连续层，你可以可视化模型如何在潜在编码中缩小输入空间，然后重建它。重要的是要注意各个CNN层的大小以及它们如何减少然后增加大小。
- en: '![](../Images/CH08_F04_Lanham.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F04_Lanham.png)'
- en: Figure 8.4 AE model summary explained
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 自动编码器模型摘要解释
- en: The next couple of cells set up output code for training the model. Go ahead
    and run those cells, including the training code. Having reviewed this code previously,
    we won’t revisit it here, other than to look at example output after 10 epochs
    of training shown in figure 8.5.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几个单元格设置了训练模型的输出代码。请运行这些单元格，包括训练代码。由于之前已经审查过这段代码，这里我们不再重复，除了查看图8.5所示的10个训练周期后的示例输出。
- en: '![](../Images/CH08_F05_Lanham.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F05_Lanham.png)'
- en: Figure 8.5 Training an AE example cell output
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 训练自动编码器示例单元格输出
- en: As the model trains, the output, as shown in figure 8.5, goes from showing fuzzy
    representations to clear features. AEs can require extensive training, and this
    simple example will likely never be able to accurately depict finer-grained features.
    However, it does do a good job differentiating between the various classes effectively.
    A good indicator of how well the model is training is to compare sandal class
    images to the original or against sneakers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型的训练，输出，如图8.5所示，从模糊表示变为清晰特征。自动编码器可能需要大量的训练，这个简单的例子可能永远无法准确描绘更细粒度的特征。然而，它确实有效地区分了各种类别。一个衡量模型训练效果的良好指标是将凉鞋类图像与原始图像或运动鞋进行比较。
- en: 8.1.3 Learning exercises
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.3 学习练习
- en: 'Use the following exercises to improve your understanding of AEs:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下练习来提高你对自动编码器（AE）的理解：
- en: Try using a different dataset, like the MNIST Handwritten Digits dataset.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用不同的数据集，例如MNIST手写数字数据集。
- en: Alter the model hyperparameters, like learning rate and batch size, to see what
    effect this has on training.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改模型超参数，如学习率和批量大小，以查看这对训练有何影响。
- en: Add or remove convolutional layers from both the encoder and decoder. Be sure
    to keep both sides of the AE balanced.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从编码器和解码器中添加或删除卷积层。确保保持自动编码器的两边平衡。
- en: While this simple AE works reasonably well, we want to improve on the model’s
    ability to generalize the learning of the representations. In the next section,
    we advance to add generalization features, like dropout and batch normalization
    layers.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个简单的自动编码器工作得相当不错，但我们希望提高模型泛化学习表示的能力。在下一节中，我们将添加泛化特征，如dropout和批归一化层。
- en: 8.1.4 Generalizing a convolutional AE
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.4 泛化卷积自动编码器
- en: In chapter 7, we covered, in some depth, how convolutional layers function by
    extracting features. We also learned that CNN models can do *too good* of a job
    of identifying features. To compensate for this, we often add a layer called `Dropout`,
    which can help generalize feature extraction.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，我们深入探讨了卷积层如何通过提取特征来工作。我们还了解到，CNN模型在识别特征方面可能做得**太好**。为了补偿这一点，我们通常会添加一个名为`Dropout`的层，这有助于泛化特征提取。
- en: Figure 8.6 shows how dropout layers work by randomly disabling network nodes
    for each training iteration, not each epoch. Disabling random neurons through
    each training iteration causes the model to better generalize and reduce memorization.
    This results in training loss and validation loss remaining consistent.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6展示了dropout层如何通过在每个训练迭代中随机禁用网络节点来工作，而不是在每个epoch中。通过每个训练迭代禁用随机神经元导致模型更好地泛化并减少记忆化。这导致训练损失和验证损失保持一致。
- en: '![](../Images/CH08_F06_Lanham.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F06_Lanham.png)'
- en: Figure 8.6 A demonstration of `Dropout`
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 `Dropout`的演示
- en: Vanishing and exploding gradients are other factors that come into play when
    training CNN and, in particular, networks with several layers. This happens because
    the weights/parameters in the network may need to become very large or small because
    of inputs passing through multiple layers. To compensate for this, we introduce
    a normalization step between layers, called `BatchNormalization`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 消失和爆炸梯度是训练CNN时，特别是在具有多层网络的CNN中起作用的另一个因素。这是因为网络中的权重/参数可能需要变得非常大或非常小，因为输入通过多个层。为了补偿这一点，我们在层之间引入一个称为`BatchNormalization`的正则化步骤。
- en: Figure 8.7 shows how `BatchNormalization` is calculated over a convolutional
    feature map. In the figure, the mean and variance are calculated for each feature
    map, and this is used to normalize the values of the feature map as inputs to
    the next layer. This results in data being kept centered at about 0, also significantly
    reducing problems with vanishing or exploding gradients.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7展示了如何在卷积特征图上计算`BatchNormalization`。在图中，为每个特征图计算均值和方差，并使用这些值来正则化特征图的值，作为下一层的输入。这导致数据保持在约0的位置，也显著减少了消失或爆炸梯度的问题。
- en: '![](../Images/CH08_F07_Lanham.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F07_Lanham.png)'
- en: Figure 8.7 The `BatchNormalization` process
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 `BatchNormalization`过程
- en: Normalization occurs using the following equation. Each input value is subtracted
    from the mean and then divided by the square root of the variance, or standard
    deviation, *σ*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化使用以下方程进行。每个输入值都从平均值中减去，然后除以方差的平方根，即标准差*σ*。
- en: '![](../Images/CH08_F06_Lanham_formula.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F06_Lanham_formula.png)'
- en: Now that we understand how we create more generalized models and avoid exploding
    and vanishing gradients, we can move on to incorporating these features into the
    AE.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何创建更通用的模型并避免爆炸和消失梯度，我们可以继续将这些特性集成到自动编码器中。
- en: 8.1.5 Improving the autoencoder
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.5 改进自动编码器
- en: By adding `BatchNormalization` and `Dropout` layers, we can improve the simple
    AE we looked at previously. We continue with the same notebook but now look at
    adding these new layer types in the following walkthrough.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加`BatchNormalization`和`Dropout`层，我们可以改进之前查看的简单自动编码器。我们继续使用相同的笔记本，但现在在以下说明中查看添加这些新层类型。
- en: Reopen the EDL_8_1_Autoencoder.ipynb notebook in Colab. Refer to the appendix
    if you need assistance. Go ahead and run all the cells in the notebook via Runtime
    > Run All from the menu. Scroll down to the section starting with Improving the
    Autoencoder.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在Colab中重新打开EDL_8_1_Autoencoder.ipynb笔记本。如有需要，请参考附录。通过菜单中的运行 > 运行所有来运行笔记本中的所有单元格。向下滚动到以改进自动编码器开始的章节。
- en: We start by looking at the updated encoder section of the model in listing 8.4\.
    Most of the code is the same as the code we last looked at, but notice the inclusion
    of the batch normalization and dropout layers. The parameter passed into the dropout
    layers is the amount or percentage of neurons that will be disabled over each
    training iteration.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先查看列表 8.4 中模型的更新编码器部分。大部分代码与上次我们查看的代码相同，但请注意包括了批量归一化和dropout层。传递给dropout层的参数是每个训练迭代中将被禁用的神经元数量或百分比。
- en: 'Listing 8.4 EDL_8_1_AE.ipynb: An improved encoder'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.4 EDL_8_1_AE.ipynb：改进的编码器
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ The BatchNormalization layer
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ BatchNormalization层
- en: ❷ The Dropout layer
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ Dropout层
- en: Following this, we, of course, look at the improved decoder section, as shown
    in the following listing. Again, the only difference is the inclusion of `BatchNormalization`
    and `Dropout` layers in the decoder section.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，我们当然会查看改进的解码器部分，如下所示。同样，唯一的区别是在解码器部分包括了`BatchNormalization`和`Dropout`层。
- en: 'Listing 8.5 EDL_8_1_AE.ipynb: An improved decoder'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.5 EDL_8_1_AE.ipynb：改进的解码器
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ The BatchNormalization layer
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ BatchNormalization层
- en: ❷ The Dropout layer
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ Dropout层
- en: Figure 8.8 shows the output of training this “improved” model over 10 epochs.
    If you compare this figure to figure 8.5, you can clearly see these “improvements”
    are not as effective as the original model.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8显示了在10个epoch上训练这个“改进”模型的输出。如果你将这个图与图 8.5进行比较，你可以清楚地看到这些“改进”并不像原始模型那样有效。
- en: '![](../Images/CH08_F08_Lanham.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F08_Lanham.png)'
- en: Figure 8.8 Training the improved AE example cell output
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8 训练改进的AE示例单元格输出
- en: 'So if these layer types are about improving model performance, why are we getting
    such poor results? The answer, in this case, is simple: we are overusing the features
    of `BatchNormalization` and `Dropout`. This generally means we need to tune the
    network architecture in a manual fashion to improve model performance. Instead,
    we look at how to optimize AE model development with EC next.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果这些层类型是关于提高模型性能的，为什么我们会得到如此糟糕的结果呢？在这个情况下，答案是简单的：我们过度使用了`BatchNormalization`和`Dropout`的功能。这通常意味着我们需要手动调整网络架构以提高模型性能。相反，我们来看看如何使用EC优化AE模型开发。
- en: 8.2 Evolutionary AE optimization
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 进化AE优化
- en: We have already seen how we can automatically optimize a CNN model using GAs
    called EvoCNN. In the following walkthrough, we take the same approach as we did
    previously but introduce the added complexity of AEs. This means our model architecture
    needs to adhere to stricter guidelines.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何使用名为EvoCNN的GAs自动优化CNN模型。在接下来的教程中，我们采取与之前相同的方法，但引入了AE的附加复杂性。这意味着我们的模型架构需要遵循更严格的指南。
- en: 8.2.1 Building the AE gene sequence
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 构建AE基因序列
- en: Our first step in building a GA AE optimizer is to build a pattern to encode
    the architecture into a `gene` sequence. We build off previous examples but this
    time introduce the constraints of an AE. Likewise, this model also improves on
    the EvoCNN project by allowing `BatchNormalization` and `Dropout` to be added.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 构建GA AE优化器的第一步是构建一个模式，将架构编码成一个`基因`序列。我们基于之前的例子，但这次引入了AE的约束。同样，这个模型也通过允许添加`BatchNormalization`和`Dropout`来改进了EvoCNN项目。
- en: Open the EDL_8_2_Evo_Autoencoder_Encoding.ipynb notebook in Colab. Please refer
    to the appendix for instructions if needed. Go ahead and run all the cells in
    the notebook via Runtime > Run All. Scroll down to the section titled Encoding
    the Autoencoder. We reviewed most of the following code in chapter 7, so we only
    review the highlights here.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在Colab中打开EDL_8_2_Evo_Autoencoder_Encoding.ipynb笔记本。如需说明，请参阅附录。通过运行 > 运行所有单元格来运行笔记本中的所有单元格。滚动到标题为“编码自动编码器”的部分。我们在第7章中回顾了以下大部分代码，所以这里我们只回顾要点。
- en: 'Start by looking at the `create_offspring` function. If you recall, this was
    the main function for creating the entire `gene` sequence, but this version differs.
    This time, the function is broken into two loops: one for the encoder section
    and the other for the decoder section. The encoder section loops over the layers
    and randomly checks if another convolution layer should be added. If a layer is
    added, then it continues to randomly check if a BN and/or dropout layer should
    also be added. Notice in this example, we automatically add a `MaxPool` layer
    to account for the funnel or reduction architecture of the AE.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 首先查看`create_offspring`函数。如果您还记得，这是创建整个`基因`序列的主要函数，但这次版本有所不同。这次，函数被拆分为两个循环：一个用于编码部分，另一个用于解码部分。编码部分遍历层并随机检查是否应该添加另一个卷积层。如果添加了层，然后它将继续随机检查是否也应该添加BN和/或dropout层。注意在这个例子中，我们自动添加一个`MaxPool`层来考虑自动编码机的漏斗或减少架构。
- en: The second loop for the decoder is set up to mirror the architecture of the
    encoder. It, therefore, loops through the same number of iterations as the encoder.
    This time, it adds up convolutional layer codes to represent the combination of
    `UpSampling` and convolution layers. After that, a chance check is applied to
    add `BatchNormalization` and/or `Dropout` layers, as shown in the following listing.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的第二个循环设置为与编码器架构相匹配。因此，它遍历与编码器相同的迭代次数。这次，它添加了卷积层代码来表示`UpSampling`和卷积层的组合。之后，应用一个机会检查来添加`BatchNormalization`和/或`Dropout`层，如下面的列表所示。
- en: 'Listing 8.6 EDL_8_2_Evo_AE_Encoding.ipynb: Creating the `gene` sequence'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.6 EDL_8_2_Evo_AE_Encoding.ipynb：创建`基因`序列
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ The first layer is always convolution.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第一层总是卷积。
- en: ❷ Chance to add another convolution layer
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 有机会添加另一个卷积层
- en: ❸ Chance to add a BatchNormalization layer
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 有机会添加一个批归一化层
- en: ❹ Chance to add a Dropout layer
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 有机会添加一个Dropout层
- en: ❺ Loop through encoder layers to create a decoder.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 遍历编码器层以创建解码器。
- en: Notice that we change the encoding pattern of the `gene` sequence to account
    for convolutional/`MaxPool` layers and `UpSampling`/convolutional layers. You
    can see this minor change in the tokens we have set in the code cell. Now, the
    encoding tokens representing the encoder convolutional layers are defined as `CONV_LAYER`,
    and the decoder `UpSampling` or convolution layers are `UPCONV_LAYER`, as shown
    in the following listing.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们改变了`基因`序列的编码模式，以考虑卷积/`MaxPool`层和`UpSampling`/卷积层。您可以在代码单元格中设置的标记中看到这个小的变化。现在，表示编码器卷积层的编码标记被定义为`CONV_LAYER`，解码器`UpSampling`或卷积层被定义为`UPCONV_LAYER`，如下面的列表所示。
- en: 'Listing 8.7 EDL_8_2_Evo_AE_Encoding.ipynb: The `gene` sequence tokens'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.7 EDL_8_2_Evo_AE_Encoding.ipynb：`基因`序列标记
- en: '[PRE6]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ The encoder convolution/pooling layer
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 编码器卷积/池化层
- en: ❷ The decoder UpSampling/convolution layer
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 解码器UpSampling/卷积层
- en: Likewise, the functions to generate the encoder layers (`CONV_LAYER`) and decoder
    layers (`UPCONV_LAYER`) become simplified, as shown in the following listing.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，生成编码器层（`CONV_LAYER`）和解码器层（`UPCONV_LAYER`）的函数也简化了，如下面的列表所示。
- en: 'Listing 8.8 EDL_8_2_Evo_AE_Encoding.ipynb: Generating layers'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.8 EDL_8_2_Evo_AE_Encoding.ipynb：生成层
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ The encoder convolution/pooling layer
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 编码器卷积/池化层
- en: ❷ The decoder UpSampling/convolution
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 解码器UpSampling/卷积
- en: Similarly, the functions to add the BN and dropout layers are simplified, as
    shown in the following listing.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，添加BN和dropout层的函数也简化了，如下面的列表所示。
- en: 'Listing 8.9 EDL_8_2_Evo_AE_Encoding.ipynb: Generating special layers'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.9 EDL_8_2_Evo_AE_Encoding.ipynb：生成特殊层
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Generates the BN layer
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生成BN层
- en: ❷ Generates the Dropout layer
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 生成Dropout层
- en: Next, we look at building the model by parsing the `gene` sequence. This code
    is quite long, so we break it into the relevant sections, starting with the initial
    parsing in listing 8.10\. We start by looping over each `gene` and checking if
    it matches a layer token. If it does, we add the respective layer and options
    to the model. In the case of the encoder convolutional layers (`CONV_LAYER`),
    if the input shape is greater than `(7, 7)`, we add a `MaxPool` layer. This ensures
    our model maintains a fixed latent view.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过解析`基因`序列来构建模型。这段代码相当长，所以我们将其拆分为相关部分，从列表8.10中的初始解析开始。我们首先遍历每个`基因`并检查它是否匹配层标记。如果匹配，我们将相应的层和选项添加到模型中。在编码器卷积层（`CONV_LAYER`）的情况下，如果输入形状大于`(7,
    7)`，我们将添加一个`MaxPool`层。这确保我们的模型保持固定的潜在视图。
- en: 'Listing 8.10 EDL_8_2_Evo_AE_Encoding.ipynb: Building the model—Parsing'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.10 EDL_8_2_Evo_AE_Encoding.ipynb：构建模型——解析
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ The input_layer is always the same.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入层始终相同。
- en: ❷ Loops over genes
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历基因
- en: ❸ The encoder convolutional layer
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 编码卷积层
- en: ❹ If the shape is greater than 7, 7, add pooling.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 如果形状大于 7, 7，则添加池化。
- en: Moving down a little, we can see the continuation of adding layers from inspecting
    the tokens, as shown in the following listing. This time, though, for the `UPCONV_LAYER`
    decoder layers, we check if the model is the same as the input size. After all,
    we don’t want the resulting images to be too big or too small.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 向下移动一点，我们可以通过检查标记来看到添加层的延续，如下面的列表所示。不过，这次对于 `UPCONV_LAYER` 解码器层，我们检查模型是否与输入大小相同。毕竟，我们不想生成的图像太大或太小。
- en: 'Listing 8.11 EDL_8_2_Evo_AE_Encoding.ipynb: Building the model—Layers'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.11 EDL_8_2_Evo_AE_Encoding.ipynb：构建模型——层
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Adds a BN layer
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 添加 BN 层
- en: ❷ Adds a Dropout layer
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 添加 Dropout 层
- en: ❸ Adds a decoder UpSampling/convolution layer
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 添加解码器 UpSampling/卷积层
- en: ❹ Checks if the model is complete
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 检查模型是否完整
- en: The function completes by building the model, compiling it, and returning it.
    Before doing that, though, we confirm the model is not too small by checking the
    shape of the last decoder layer, as shown in the following listing. If the output
    is too small, we add another `UpSampling` layer to double the size from `14,`
    `14` to `28,` `28`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 函数通过构建模型、编译它并返回它来完成。但在做这些之前，我们通过检查最后一个解码器层的形状来确认模型不是太小，如下面的列表所示。如果输出太小，我们添加另一个
    `UpSampling` 层，将大小从 `14, 14` 增加到 `28, 28`。
- en: 'Listing 8.12 EDL_8_2_Evo_AE_Encoding.ipynb: Building the model—Compile'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.12 EDL_8_2_Evo_AE_Encoding.ipynb：构建模型——编译
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Makes sure the final model is not too small
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 确保最终模型不是太小
- en: ❷ Converts back to a single channel
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 转换回单通道
- en: ❸ Combines the input/output layers
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 合并输入/输出层
- en: To test the `build_model` function, the next block of code, shown in listing
    8.13, creates 100 random offspring and evaluates the size of the models. This
    code generates random `individual` `gene` sequences and then builds corresponding
    models from those sequences. Along the way, the code tracks the minimum and maximum
    models generated.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试 `build_model` 函数，下一个代码块，如列表 8.13 所示，创建了 100 个随机后代并评估了模型的大小。此代码生成随机的 `individual`
    `gene` 序列，然后从这些序列构建相应的模型。在这个过程中，代码跟踪生成的最小和最大模型。
- en: 'Listing 8.13 EDL_8_2_Evo_AE_Encoding.ipynb: Evaluating `build_model`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.13 EDL_8_2_Evo_AE_Encoding.ipynb：评估 `build_model`
- en: '[PRE12]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Creates a random gene sequence
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个随机基因序列
- en: ❷ Builds the model from the sequence
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从序列构建模型
- en: ❸ Counts the model parameters
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算模型参数
- en: Scrolling down further shows the output, as summarized in figure 8.9\. In the
    figure, a minimum size parameter model is used to train the model over 10 epochs.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 向下滚动进一步可以看到输出，如图 8.9 所总结。在图中，使用最小尺寸参数模型在 10 个周期内训练模型。
- en: '![](../Images/CH08_F09_Lanham.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F09_Lanham.png)'
- en: Figure 8.9 The output of the minimum size model built from randomly generated
    offspring
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 从随机生成的后代构建的最小尺寸模型的输出
- en: This randomly generated model using `create_offspring` and `build_model` appears
    to be better than our last “improved” AE, which is promising, since it is also
    an approximate minimum size model. Be sure to check the code and test the training
    for a maximum size model as well. Keep in mind that the sample size in this example
    only uses 100 variations.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这个使用 `create_offspring` 和 `build_model` 随机生成的模型似乎比我们上一个“改进”的 AE 更好，这是有希望的，因为它也是一个近似的最小尺寸模型。务必检查代码并测试最大尺寸模型的训练。记住，在这个例子中，样本大小只使用了
    100 种变化。
- en: 8.2.2 Learning exercises
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 学习练习
- en: 'Use the following exercises to improve your understanding:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下练习来提高你的理解：
- en: Create a list of `individuals` by calling `create_offspring` in a loop and then
    print and compare the various models.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在循环中调用 `create_offspring` 创建 `individuals` 列表，然后打印并比较各种模型。
- en: Change the base probability from 0.5 in listing 8.6 to another value. See what
    effect this has on the generated models using exercise 1.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将列表 8.6 中的基本概率从 0.5 改为另一个值。通过练习 1 看看这会对生成的模型产生什么影响。
- en: We now have a way to create a `gene` sequence that, in turn, can be used to
    build an AE model. As we learned in chapter 6, our next stage is to build the
    custom functions to `mate` and `mutate` these `gene` sequences. This is where
    we continue this project in the next section.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一种创建`基因`序列的方法，反过来，可以使用它来构建AE模型。正如我们在第6章所学，我们的下一步是构建自定义的`配对`和`变异`这些`基因`序列的函数。这就是我们在下一节继续这个项目的地方。
- en: 8.3 Mating and mutating the autoencoder gene sequence
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 自动编码器基因序列的配对和变异
- en: Just like we did for the EvoCNN project in chapter 7, we also need to create
    custom `mutation` and `mating`/`crossover` operators. These custom operators are
    quite like what we used previously, so again, we only review some highlights here.
    After adding the genetic operators, we test the EvoAE.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第7章的EvoCNN项目中做的那样，我们还需要创建自定义的`变异`和`配对`/`交叉`算子。这些自定义算子与我们之前使用的非常相似，所以我们再次只回顾一些亮点。在添加遗传算子后，我们测试了EvoAE。
- en: Open the EDL_8_3_EvoAutoencoder.ipynb notebook in Colab. Refer to the appendix
    if you need assistance. Scroll down to the Creating Mating/Mutation Operators
    section, and then select the next code cell. From the menu, select Runtime > Run
    Before to execute all the previous cells in the notebook. Be patient, and wait
    for the sample training to finish.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在Colab中打开EDL_8_3_EvoAutoencoder.ipynb笔记本。如需帮助，请参阅附录。向下滚动到创建配对/变异算子部分，然后选择下一个代码单元格。从菜单中选择运行
    > 运行之前，以执行笔记本中的所有前一个单元格。请耐心等待，直到样本训练完成。
- en: Figure 8.10 demonstrates the `crossover` process—this time with the modified
    `gene` sequences that represent the AE architecture. Notice that the number of
    encoder convolutional layers and decoder convolutional layers are always equal.
    This is required to achieve the funnel effect of AE.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10展示了`交叉`过程——这次使用的是代表AE架构的修改后的`基因`序列。请注意，编码器卷积层和解码器卷积层的数量始终相等。这是实现AE的漏斗效应所必需的。
- en: '![](../Images/CH08_F10_Lanham.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F10_Lanham.png)'
- en: Figure 8.10 Evolutionary AE `crossover`
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 进化AE `交叉`
- en: 'Fortunately, most of the `crossover`/`mating` code written in chapter 7 works
    on our updated `gene` sequence, and we won’t need to revisit it here. DEAP calls
    the `crossover` function/operator, passing in the two parent `individuals` during
    evolution. Inside this function, the core of the work happens in the `swap_layers`
    function we covered previously. As shown in the following listing, the only difference
    in this function is the modification of the main layer structures we wanted to
    support: `Convolution` (encoder), `UpConvolution` (decoder), `BatchNormalization`,
    and `Dropout`.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，第7章中编写的大多数`交叉`/`配对`代码都可以在我们的更新后的`基因`序列上运行，我们不需要在这里重新访问它。DEAP在进化过程中调用`交叉`函数/算子，在进化过程中传入两个父`个体`。在这个函数内部，我们之前介绍过的`swap_layers`函数是工作的核心。如下面的列表所示，这个函数的唯一区别是我们想要支持的顶层结构的修改：`卷积`（编码器）、`上卷积`（解码器）、`批归一化`和`Dropout`。
- en: 'Listing 8.14 EDL_8_3_EvoAE.ipynb: Performing `crossover`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.14 EDL_8_3_EvoAE.ipynb：执行`交叉`
- en: '[PRE13]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Swaps encoder convolution layers
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 交换编码器卷积层
- en: ❷ Swaps decoder upconvolutional layers
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 交换解码器上卷积层
- en: ❸ Swaps BN
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 交换BN
- en: ❹ Swaps Dropout
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 交换Dropout
- en: Performing `mutation` requires a bit more attention to perform modifications
    to the various layer types in the architecture. We start by looking at the main
    `mutation` function, which is called by DEAP evolution and takes a single `individual`.
    This function heavily uses the `mutate_layers` function and applies it to only
    layers that can be modified. Notice we omit BN layers, since they don’t require
    additional parameters, as shown in the following listing.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 执行`变异`需要对架构中各种层类型的修改更加注意。我们首先查看主要的`变异`函数，该函数由DEAP进化调用并接受单个`个体`。此函数大量使用`mutate_layers`函数并将其应用于仅可修改的层。请注意，我们省略了BN层，因为它们不需要额外的参数，如下面的列表所示。
- en: 'Listing 8.15 EDL_8_3_EvoAE.ipynb: The `mutation` function'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.15 EDL_8_3_EvoAE.ipynb：`变异`函数
- en: '[PRE14]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Mutates encoder convolution layers
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 变异编码器卷积层
- en: ❷ Mutates dropout layers
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 变异Dropout层
- en: ❸ Mutates decoder upconvolution layers
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 变异解码器上卷积层
- en: The `mutate_layers` function highlights how each layer is selected for `mutation`.
    The layers are gathered by type and checked for a chance of `mutation`. Notice
    that, currently, the chance is always 100%. If a layer is chosen for `mutation`,
    its sequence is passed into the `mutate` function to be `mutated`, as shown in
    the following listing.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`mutate_layers` 函数突出了如何为 `mutation` 选择每一层。层按类型收集，并检查 `mutation` 的可能性。注意，目前，这种可能性总是
    100%。如果选择了一层进行 `mutation`，则其序列将传递给 `mutate` 函数进行 `mutation`，如下所示。'
- en: 'Listing 8.16 EDL_8_3_EvoAE.ipynb: The `mutate_layers` function'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.16 EDL_8_3_EvoAE.ipynb：`mutate_layers` 函数
- en: '[PRE15]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Gets layers of type
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取类型为的层
- en: ❷ Checks for chance mutation
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 检查随机突变
- en: ❸ Catches exceptions
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 捕获异常
- en: ❹ Mutates the layer
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 突变层
- en: The `mutate` function performs the specific `mutation` of the respective layer
    types, as shown in listing 8.17\. Each layer type has a slightly different form
    of `mutation` applied relevant to the layer type. If an unknown layer type is
    passed into `mutate`, an error is thrown, meaning the `gene` sequence is likely
    corrupt or broken. This, as in nature, results in an unviable offspring that is
    terminated from further execution.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`mutate` 函数执行相应层类型的特定 `mutation`，如下所示。每种层类型都应用了与层类型相关的不同形式的 `mutation`。如果将未知层类型传递给
    `mutate`，则会抛出错误，这意味着 `gene` 序列可能已损坏或中断。这，正如自然界一样，导致了一个不可行的后代，该后代将终止进一步执行。'
- en: 'Listing 8.17 EDL_8_3_EvoAE.ipynb: The `mutate` function'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.17 EDL_8_3_EvoAE.ipynb：`mutate` 函数
- en: '[PRE16]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Mutates encoder CNN layers
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 突变编码器 CNN 层
- en: ❷ Mutates decoder CNN layers
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 突变解码器 CNN 层
- en: ❸ Mutates dropout layers
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 突变 dropout 层
- en: ❹ The layer code does not match, so throw an error.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 层代码不匹配，因此抛出错误。
- en: At the end of the `mating`/`mutation` cells, there is code to test the respective
    operators by creating new offspring and passing them through the `crossover` or
    `mutation` functions. With the `mating`/`mutation` operators constructed, we can
    move on to evolving an AE architecture in the next section.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `mating`/`mutation` 单元结束时，有代码用于通过创建新的后代并传递给 `crossover` 或 `mutation` 函数来测试相应的操作符。构建了
    `mating`/`mutation` 操作符后，我们可以继续到下一节，进化 AE 架构。
- en: 8.4 Evolving an autoencoder
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 生成自动编码器
- en: Evolving the AE is now a relatively simple matter of just adding DEAP. Again,
    a lot of the code here is the same as in previous examples. This means we refer
    here to just the highlights, changes, and points of interest.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，进化 AE 只是一个相对简单的过程，只需添加 DEAP。同样，这里的大部分代码与之前的示例相同。这意味着我们在这里只关注重点、变更和感兴趣的点。
- en: Open the EDL_8_4_EvoAutoencoder.ipynb notebook in Colab. Run the whole notebook
    via Runtime > Run All from the menu. This notebook can take a long time to run,
    so it’s best to start as soon as possible.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Colab 中打开 EDL_8_4_EvoAutoencoder.ipynb 笔记本。通过菜单中的“运行”>“运行所有”来运行整个笔记本。这个笔记本运行可能需要很长时间，因此最好尽早开始。
- en: The AE architecture can take a considerable amount of time to evolve. As such,
    we use a previously covered data reduction trick to make evolving less time-consuming.
    Referring to the data loading cell, notice how we reduce the size of the training
    and validation sets by simply taking slices of the original dataset, as shown
    in the following listing. This is just for the purposes of demonstrating how the
    code runs and operates. Obviously, if the goal was to create an optimized model,
    we would be better off using the full dataset.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器（AE）架构的进化可能需要相当长的时间。因此，我们使用之前介绍过的数据减少技巧，以使进化过程更加高效。参考数据加载单元，注意我们如何通过简单地从原始数据集中取切片来减少训练集和验证集的大小，如下所示。这只是为了演示代码的运行和操作方式。显然，如果目标是创建一个优化的模型，我们最好使用完整的数据集。
- en: 'Listing 8.18 EDL_8_4_EvoAE.ipynb: Reducing the dataset size'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.18 EDL_8_4_EvoAE.ipynb：减少数据集大小
- en: '[PRE17]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Reduces the training size
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 减少训练大小
- en: ❷ Reduces the testing/validation size
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 减少测试/验证大小
- en: Next, we review all the base DEAP set up code to create the GA solver for performing
    the architecture optimization, shown in listing 8.19\. We register the main `fitness`
    function as `FunctionMin`, since our goal is to minimize `fitness`. Next, the
    `create_ offspring` function is registered for creating new `individuals`. Then,
    the code completes with the registration of the custom `crossover` and `mutation`
    functions.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们回顾所有基础 DEAP 设置代码，以创建用于执行架构优化的 GA 求解器，如列表 8.19 所示。我们将主要的 `fitness` 函数注册为
    `FunctionMin`，因为我们的目标是最小化 `fitness`。然后，注册 `create_offspring` 函数以创建新的 `individuals`。最后，代码通过注册自定义的
    `crossover` 和 `mutation` 函数来完成。
- en: 'Listing 8.19 EDL_8_4_EvoAE.ipynb: Setting up DEAP'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.19 EDL_8_4_EvoAE.ipynb：设置DEAP
- en: '[PRE18]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Registers the target function and minimum fitness
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 注册目标函数和最小适应度
- en: ❷ Registers the initial AE function
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 注册初始AE函数
- en: ❸ Registers the custom crossover
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 注册自定义交叉
- en: ❹ Registers the custom mutate
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 注册自定义变异
- en: As shown in listing 8.20, the `evaluate` function is up next. This is where
    each network model architecture is evaluated. Previously, we registered a list
    called `fitness` to hold all the evaluated `fitness`. We did this to better track
    the maximum observed `fitness`. Inside the function, we first call `build_model`
    to create a model based on the `individual` `gene` sequence. After this, we call
    the `train` function to train the model and return the model plus `history`. From
    the `history` function, we extract the last validation history value and observe
    that as the model’s `fitness`. If no errors occur generating the model and training,
    then the `fitness` is returned, clamped between 0 and maximum `fitness`. We use
    the `np.nanman` function to avoid returning `nan` values. If an error is encountered,
    we return the maximum observed `fitness`.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如列表8.20所示，下一个是`evaluate`函数。这是评估每个网络模型架构的地方。之前，我们注册了一个名为`fitness`的列表来保存所有评估的`fitness`。我们这样做是为了更好地跟踪最大观察到的`fitness`。在函数内部，我们首先调用`build_model`根据`个体``基因`序列创建模型。之后，我们调用`train`函数来训练模型并返回模型加`history`。从`history`函数中，我们提取最后一个验证历史值并将其视为模型的`fitness`。如果没有错误生成模型和训练，则返回`fitness`，夹在0和最大`fitness`之间。我们使用`np.nanman`函数来避免返回`nan`值。如果遇到错误，则返回最大观察到的`fitness`。
- en: 'Listing 8.20 EDL_8_4_EvoAE.ipynb: The `evaluate` function'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.20 EDL_8_4_EvoAE.ipynb：`evaluate`函数
- en: '[PRE19]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ The global variable to track fitness
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 用于跟踪适应度的全局变量
- en: ❷ The build_model from the gene sequence
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从基因序列构建模型
- en: ❸ Trains the model
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 训练模型
- en: ❹ Returns the clamped fitness value
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 返回夹紧的适应度值
- en: ❺ If there is an error, return the maximum observed fitness.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 如果出现错误，返回最大观察到的适应度。
- en: Figure 8.11 shows the results of evolving the architecture with an initial `population`
    of 100 `individuals` run over 3 `generations`. From these initial results, you
    can see this presents an interesting approach to self-optimizing model architectures.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11显示了使用初始`种群`100 `个体`运行3 `代`进化架构的结果。从这些初始结果中，你可以看到这提供了一种自我优化模型架构的有趣方法。
- en: '![](../Images/CH08_F11_Lanham.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F11_Lanham.png)'
- en: Figure 8.11 The results of evolving AE architecture
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 进化AE架构的结果
- en: It is quite likely your results may vary, and this sample runs better with greater
    initial `populations`, which, again, can be time-consuming to run. Compare figure
    8.11 to figures 8.5 and 8.8\. Are the results better or worse than you expected?
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能你的结果会有所不同，这个样本在更大的初始`种群`下运行得更好，这再次表明运行起来可能很耗时。将图8.11与图8.5和图8.8进行比较。结果是否如你所预期的好或坏？
- en: Training AEs and other RL networks can be time-consuming, and we often need
    more than 3 epochs, as in the last notebook. The evolutionary output from this
    notebook demonstrates the possibilities stemming from evolving architecture for
    AEs.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 训练AE和其他RL网络可能很耗时，我们通常需要超过3个epoch，就像上一个笔记本中那样。这个笔记本中的进化输出展示了从进化架构中产生的可能性。
- en: 8.4.1 Learning exercises
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 学习练习
- en: 'Continue exploring the evolutionary AE by working through these exercises:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 通过完成以下练习继续探索进化的AE：
- en: Increase or decrease the number of training samples in listing 8.18.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在列表8.18中增加或减少训练样本的数量。
- en: Change the target dataset. A good option is the MNIST Handwritten Digits dataset.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改目标数据集。一个好的选择是MNIST手写数字数据集。
- en: Try tuning the hyperparameters of learning rate and batch size to see what effect
    this has on evolving a model.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试调整学习率和批大小超参数，看看这对进化模型有什么影响。
- en: To wrap this chapter up, we continue looking at AEs—but with a twist. Instead
    of straight mapping of the encoding to decoding, we try adding a sampling layer
    to implement a variational AE in the next section.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结本章，我们继续研究AE，但有一些变化。不是直接将编码映射到解码，我们尝试在下一节中添加一个采样层来实现变分AE。
- en: 8.5 Building variational autoencoders
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 构建变分自编码器
- en: Variational AEs are an extension to AEs that learn by understanding the differences
    in learned representations by understanding sampling loss. This is an important
    concept we need to cover before jumping into the next chapter on evolutionary
    generative DL.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自动编码器（VAEs）是自动编码器（AEs）的扩展，通过理解采样损失中的学习表示差异来学习。在我们跳入下一章关于进化生成式深度学习之前，这是一个我们需要覆盖的重要概念。
- en: For the next notebook project, we look at building a variational AE to perform
    the same analysis as the previous notebooks. Experienced practitioners of DL are
    likely familiar with this pattern, but we review it further in the next section,
    just in case.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下一个笔记本项目，我们将探讨构建一个变分自动编码器（VAE），以执行与之前笔记本相同的分析。深度学习（DL）的资深从业者可能对这种模式很熟悉，但为了以防万一，我们将在下一节进一步回顾。
- en: '8.5.1 Variational autoencoders: A review'
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.1 变分自动编码器：回顾
- en: 'Architecturally, variational autoencoders (VAEs) are nearly identical, except
    for one key difference within the middle encoding layer: in a VAE, the middle
    layer becomes a sampling layer that learns to represent the encoding input and
    translate that learned representation back to an original image. By learning the
    representation of inputs, the VAE is then able to generate new outputs based on
    this understanding.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在架构上，变分自动编码器（VAEs）几乎完全相同，除了中间编码层中的一个关键差异：在VAE中，中间层变成了一个采样层，该层学习表示编码输入并将这种学习到的表示转换回原始图像。通过学习输入的表示，VAE能够根据这种理解生成新的输出。
- en: 'Figure 8.12 shows how a VAE differs from the traditional AE architecture shown
    in figure 8.1\. We can see in the figure the latent encoding vector is replaced
    by two learned parameters: mean (*µ*) and variance (*σ*). These learned parameters
    are then used to sample or generate a new latent encoding vector, called *Z*,
    that is then pushed into the decoder.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12展示了VAE与图8.1中所示的传统自动编码器架构的不同之处。我们可以看到，潜在编码向量被两个学习到的参数所取代：均值（*µ*）和方差（*σ*）。这些学习到的参数随后用于采样或生成一个新的潜在编码向量，称为*Z*，然后将其推入解码器。
- en: '![](../Images/CH08_F12_Lanham.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F12_Lanham.png)'
- en: Figure 8.12 Variational AE
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 变分自动编码器
- en: Instead of learning to compress and extract relevant features, as is done in
    the AE, the VAE learns how an input is represented by training the network to
    output the mean and variance of the input. Then based on this learned representation,
    a sampling layer generates a new latent encoding vector, called *Z*, that is fed
    back through the decoder.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 与在自动编码器（AE）中学习压缩和提取相关特征不同，VAE通过训练网络输出输入的均值和方差来学习输入的表示方式。然后基于这种学习到的表示，采样层生成一个新的潜在编码向量，称为*Z*，并将其反馈到解码器中。
- en: Since the VAE learns representations across a known space, we can generate values
    from this space by walking over the range of mean and variance learned by the
    model. Figure 8.13 demonstrates the results of a VAE by iterating over the range
    of mean and variance to output what the model is learning. Now that we have an
    overview of a VAE, we can move on to building it in the next section.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 由于VAE在已知空间中学习表示，我们可以通过遍历模型学习到的均值和方差的范围来生成该空间中的值。图8.13通过遍历均值和方差的范围来演示VAE的结果，从而输出模型正在学习的内容。现在我们已经对VAE有了概述，我们可以在下一节继续构建它。
- en: '![](../Images/CH08_F13_Lanham.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F13_Lanham.png)'
- en: Figure 8.13 Sample learned output of VAE, showing the learned 2D manifold
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13展示了VAE学习到的2D流形样本输出
- en: 8.5.2 Implementing a VAE
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.2 实现VAE
- en: A VAE is simply an AE that swaps out the middle encoding for a sampling mechanism.
    Structurally, a VAE and AE are identical, but in this notebook, we introduce another
    pattern for implementing this architecture. Not only does this simplify the architecture,
    but it sets the stage for other innovations in later chapters.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: VAE简单来说就是一个将中间编码替换为采样机制的自动编码器。在结构上，VAE和AE是相同的，但在这个笔记本中，我们介绍了实现这种架构的另一种模式。这不仅简化了架构，还为后续章节中的其他创新奠定了基础。
- en: Open the EDL_8_5_VAE.ipynb notebook in Colab. Refer to the appendix if you need
    assistance. Go ahead and run all the cells of the notebook by selecting Runtime
    > Run All from the menu.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在Colab中打开EDL_8_5_VAE.ipynb笔记本。如需帮助，请参阅附录。从菜单中选择运行 > 运行所有单元格来运行笔记本中的所有单元格。
- en: Scroll down to the cell labeled Network Hyperparameters. We start by looking
    at the notebook’s hyperparameters in detail, shown in listing 8.21, since a couple
    of new inputs are introduced. The code starts by setting the `input_shape` using
    the extracted `image_size` from the dataset loaded in the previous cell. Then,
    a base kernel size and the number of filters are set; we see later how these are
    used. After that, `latent_dim` is set to `2`, representing the number of middle
    dimensions the encoder outputs. In this case, the latent dimensions, `2`, are
    representative of the input mean and variance.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动到标记为“网络超参数”的单元格。我们首先详细查看笔记本的超参数，如列表8.21所示，因为引入了一些新的输入。代码首先使用前一个单元格中加载的数据集提取的`image_size`设置`input_shape`。然后，设置基础内核大小和过滤器数量；我们稍后会看到这些是如何被使用的。之后，将`latent_dim`设置为`2`，表示编码器输出的中间维度的数量。在这种情况下，潜在的维度`2`代表输入的均值和方差。
- en: 'Listing 8.21 EDL_8_5_VAE.ipynb: VAE hyperparameters'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.21 EDL_8_5_VAE.ipynb：VAE超参数
- en: '[PRE20]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ The input_shape is defined by the image_size and channels.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入形状由image_size和通道定义。
- en: ❷ The kernel size for convolutional layers
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 卷积层的内核大小
- en: ❸ The base number filters for CNN
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ CNN的基础过滤器数量
- en: ❹ The size of the latent/middle dimension
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 潜在的/中间维度的尺寸
- en: The next cell showing the construction of the convolutional layers of the encoder
    is quite different from the AE, as shown in listing 8.22\. The VAE is constructed
    in a loop that adds successive CNN layers. At each iteration, the next layer doubles
    the number of filters. One thing to note is the omission of pooling layers to
    create the reduction of the funnel effect of an AE. Instead, we increase the `strides`
    from `1` to `2`, which reduces the output dimensions by two. Thus, an image of
    28×28 would reduce to 14×14 in the output.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个单元格显示了编码器卷积层的构建，与AE相比相当不同，如列表8.22所示。VAE是通过循环添加连续的CNN层来构建的。在每次迭代中，下一层将过滤器数量加倍。需要注意的是，省略了池化层以创建AE的漏斗效应的减少。相反，我们将`strides`从`1`增加到`2`，这减少了输出维度的一半。因此，28×28的图像将减少到14×14。
- en: 'Listing 8.22 EDL_8_5_VAE.ipynb: Building an encoder CNN'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.22 EDL_8_5_VAE.ipynb：构建编码器CNN
- en: '[PRE21]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Sets up the initial input layer
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置初始输入层
- en: ❷ Doubles the number of filters for each step of CNN
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将CNN每一步的过滤器数量加倍
- en: ❸ Replaces pooling by increasing stride
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过增加步长来替换池化
- en: ❹ Captures the final shape
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 捕获最终形状
- en: Moving down to the next cell, we can see how the output from the encoder CNN
    layers reduces to the latent dimension and outputs the mean and variance, as shown
    in listing 8.23\. To do this, a `Flatten` layer is added to squash the output
    of the encoder to a `Dense` layer. After that, two more `Dense` layers are added
    that produce the sample mean `z_mean` and variance `z_log_var`. These values are
    then passed into the sampling layer `z`, a custom layer constructed using lambda
    that takes as input the `sampling` function, the desired output shape, and the
    mean and variance as inputs. Pay special attention to how the `latent_space` hyperparameter
    is used to define the input and output shapes of the sampling layer.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 向下滚动到下一个单元格，我们可以看到编码器CNN层的输出如何减少到潜在维度并输出均值和方差，如列表8.23所示。为此，添加了一个`Flatten`层来压缩编码器的输出到一个`Dense`层。之后，添加了两个更多的`Dense`层，它们产生样本均值`z_mean`和方差`z_log_var`。这些值随后被传递到采样层`z`，这是一个使用lambda构建的自定义层，它接受`sampling`函数、期望的输出形状以及均值和方差作为输入。特别注意`latent_space`超参数是如何用来定义采样层的输入和输出形状的。
- en: 'Listing 8.23 EDL_8_5_VAE.ipynb: Building latent sampling'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.23 EDL_8_5_VAE.ipynb：构建潜在采样
- en: '[PRE22]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Flattens the output from the encoder
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将编码器的输出展平
- en: ❷ Reduces to latent_dim to produce the mean and variance
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将潜在维度减少到产生均值和方差
- en: ❸ Generates the sampling layer
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 生成采样层
- en: ❹ Instantiates the encoder model
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 实例化编码器模型
- en: Figure 8.14 shows the `model.summary` of the encoder model, annotated with the
    layer structure on the side. Notice how the model flattens the convolutional layers
    of the encoder and then pushes the input into a `Dense` layer of size `16`. This
    is further broken apart into two parallel layers—one for mean and the other for
    variance. That is later combined in the sampling layer that then outputs a vector
    of a size of latent dimensions to be consumed by the decoder.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14显示了编码器模型的`model.summary`，旁边标注了层结构。注意模型如何展平编码器的卷积层，然后将输入推入大小为`16`的`Dense`层。这进一步分解成两个并行层——一个用于均值，另一个用于方差。这些值随后被传递到采样层`z`，这是一个使用lambda构建的自定义层，它接受`sampling`函数、期望的输出形状以及均值和方差作为输入。特别注意`latent_space`超参数是如何用来定义采样层的输入和输出形状的。
- en: '![](../Images/CH08_F14_Lanham.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F14_Lanham.png)'
- en: Figure 8.14 An annotated summary of the encoder model
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14 编码器模型的注释总结
- en: Before we get to the decoder, let’s review the `sampling` function, shown in
    the code cell at the top of the file and listing 8.24, that takes as input the
    mean and variance. Inside the function, the values for mean and variance are unpacked
    from `args`. Then, we extract two shape values from the mean input, first using
    `K.shape` to return the tensor shape and the second `K.int_shape` to return a
    tuple. Simply put, this sets up the size of output of the sampled vector. We then
    create a random sample tensor of size `(batch, dim)` called `epsilon`, which becomes
    the base randomized vector. After this, we scale the vector by applying the mean
    and variance to determine a final output, vector `z`.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们到达解码器之前，让我们回顾一下 `sampling` 函数，该函数在文件的顶部代码单元格中显示，并在列表 8.24 中列出，它接受均值和方差作为输入。在函数内部，均值和方差的值从
    `args` 中解包。然后，我们从均值输入中提取两个形状值，首先使用 `K.shape` 返回张量形状，然后使用 `K.int_shape` 返回一个元组。简单来说，这设置了采样向量的输出大小。然后，我们创建一个大小为
    `(batch, dim)` 的随机样本张量，称为 `epsilon`，它成为基础随机向量。之后，通过应用均值和方差来缩放向量，以确定最终的输出向量 `z`。
- en: 'Listing 8.24 EDL_8_5_VAE.ipynb: The `sampling` function'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.24 EDL_8_5_VAE.ipynb：`sampling` 函数
- en: '[PRE23]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Extracts the tensor size for the batch parameter
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 提取批处理参数的张量大小
- en: ❷ Extracts a tuple for the dimensions
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 提取维度元组
- en: ❸ Samples from the normal distribution
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从正态分布中采样
- en: ❹ Returns the sampled vector, offset by epsilon
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 返回偏移 epsilon 的采样向量
- en: The architecture of the decoder model has also been simplified, but we still
    need to handle the z sampling layer output from the encoder. This time, we construct
    an `Input` layer of size `latent_dim` that matches the final sampling output from
    the encoder, z, as shown in listing 8.25\. Next, a new `Dense` layer is expanded
    to match the size of the final decoder output, which is then reshaped with a `Reshape`
    layer to match the original encoder output. A simple way to think about this is
    the new middle sampling function is just swapping the middle latent encoding we
    built with an AE. However, we still need to keep the dimensions of the data consistent.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器模型的架构也已经简化，但我们仍然需要处理来自编码器的 z 采样层输出。这次，我们构建一个大小为 `latent_dim` 的 `Input` 层，以匹配编码器的最终采样输出
    z，如列表 8.25 所示。接下来，一个新的 `Dense` 层被扩展以匹配最终解码器输出的大小，然后使用 `Reshape` 层重塑以匹配原始编码器输出。简单来说，新的中间采样函数只是用我们构建的
    AE 的中间潜在编码交换。然而，我们仍然需要保持数据维度的一致性。
- en: 'Listing 8.25 EDL_8_5_VAE.ipynb: Unpacking the decoder inputs'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.25 EDL_8_5_VAE.ipynb：解包解码器输入
- en: '[PRE24]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ The input layer constructed
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建输入层
- en: ❷ Adds a dense layer to rebuild the shape
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 添加一个密集层来重建形状
- en: ❸ Reshapes the output to match the decoder input
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将输出重塑以匹配解码器输入
- en: After that, we can see the remainder of the decoder being constructed, as shown
    in listing 8.26\. The first thing to note is the use of `Conv2DTranspose` layers
    instead of `Conv2D` and `UpSampling`, as used previously. Put simply, this layer
    type is a more explicit reverse of the convolution process. Again, the layers
    are added in a loop, but this time, the number of `filters` is reduced after each
    iteration, with the remaining `filters` left after building the encoder. After
    that, a single `Conv2DTranspose` layer is used to reduce the output to a single
    channel.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以看到解码器的其余部分正在构建，如列表 8.26 所示。首先要注意的是，使用 `Conv2DTranspose` 层而不是之前使用的 `Conv2D`
    和 `UpSampling`。简单来说，这种层类型是卷积过程的更明确的反转。同样，层是通过循环添加的，但这次在每个迭代之后，`filters` 的数量减少，剩余的
    `filters` 在构建编码器后留下。之后，使用单个 `Conv2DTranspose` 层将输出减少到单个通道。
- en: 'Listing 8.26 EDL_8_5_VAE.ipynb: Building the decoder'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.26 EDL_8_5_VAE.ipynb：构建解码器
- en: '[PRE25]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Uses Conv2DTranspose layers
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 Conv2DTranspose 层
- en: ❷ The strides value is set to 2 for expansion.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将步长值设置为 2 以进行扩展。
- en: ❸ Decreases the filters after each layer
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在每个层后减少过滤器数量
- en: ❹ Adds a final transpose layer for single-channel output
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 添加一个最终的转置层以实现单通道输出
- en: ❺ Instantiates the model
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 实例化模型
- en: Figure 8.15 shows an annotated view of the decoder model summary. As you can
    see, this part is simpler and more modular than an AE. Notice how the input in
    the model is now just two inputs generated from the encoder `sampling` layer.
    This, however, allows us to walk or sample through the space that the decoder
    learns to generate from.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15显示了解码器模型摘要的注释视图。正如你所见，这部分比AE简单且更模块化。注意模型中的输入现在只是来自编码器`sampling`层的两个输入。然而，这使我们能够遍历或采样解码器学习生成的空间。
- en: '![](../Images/CH08_F15_Lanham.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH08_F15_Lanham.png)'
- en: Figure 8.15 Annotated view of the decoder model
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 解码器模型的注释视图
- en: The other major difference between a VAE and AE is the way loss is calculated.
    Recall in an AE, we calculated loss using pixel-wise loss calculated with MSE.
    This was simple and worked well. However, if we calculated loss in this manner
    for a VAE, we would miss the nuance of learning the input distribution. Instead,
    with a VAE, we measure the distributional loss between the input and output using
    a measure of divergence.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: VAE与AE之间的另一个主要区别在于损失计算的方式。回想一下，在AE中，我们使用MSE（均方误差）计算像素级的损失。这很简单且效果良好。然而，如果我们以这种方式为VAE计算损失，我们将错过学习输入分布的细微差别。相反，使用VAE，我们通过一个发散度度量来衡量输入和输出之间的分布损失。
- en: This requires us to add a specialized loss determination to the Keras model.
    We start by first calculating the base reconstruction loss—the loss calculated
    by comparing the input image against the output image. After that, we calculate
    the `kl_loss`, or Kullback-Leibler divergence, which is the statistical distance
    between two probability distributions. The calculation to determine this amount
    of divergence from the input and learned representation is shown in the following
    listing. We cover statistical distance and loss calculations like this in greater
    depth in chapter 9\. Finally, the mean of the `kl_loss` and `reconstruction_loss`
    is added as a new loss metric to the model with the `add_loss` function.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要我们在Keras模型中添加一个专门的损失确定。我们首先计算基础重建损失——通过比较输入图像与输出图像来计算的损失。之后，我们计算`kl_loss`，即Kullback-Leibler发散度，这是两个概率分布之间的统计距离。从输入和学习的表示中确定这种发散度的计算方法在下面的列表中展示。我们在第9章中更深入地讨论了类似这样的统计距离和损失计算。最后，使用`add_loss`函数将`kl_loss`和`reconstruction_loss`的均值添加为新的损失度量到模型中。
- en: 'Listing 8.27 EDL_8_5_VAE.ipynb: Building the VAE model'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.27 EDL_8_5_VAE.ipynb：构建VAE模型
- en: '[PRE26]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Builds the base reconstruction_loss
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建基础重建损失
- en: ❷ Expands the base reconstruction_loss by the image size
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过图像大小扩展基础重建损失
- en: ❸ Calculates the kl_loss
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算kl_loss
- en: ❹ Takes the mean of the reconstruction_loss and kl_loss
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算重建损失和kl损失的均值
- en: ❺ Adds the loss measure to the model
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将损失度量添加到模型中
- en: Scroll down a little further to examine the training output shown in figure
    8.16\. In this example, we use a normalized loss metric, the loss divided by maximum
    observed loss, to track the training of the model. By tracking a normalized loss,
    we can switch between other forms of reconstruction and statistical distance/divergence
    measures. Go ahead and try switching the flag on the reconstruction loss from
    MSE to binary cross entropy to observe training differences. Both these measures
    generate output on different scales, but by normalizing the loss, we can compare
    measures.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 向下滚动一点，检查图8.16所示的训练输出。在这个例子中，我们使用归一化损失度量，即损失除以最大观察到的损失，来跟踪模型的训练。通过跟踪归一化损失，我们可以切换到其他形式的重建和统计距离/发散度度量。尝试将重建损失的标志从MSE切换到二元交叉熵，以观察训练差异。这两种度量在不同的尺度上生成输出，但通过归一化损失，我们可以比较度量。
- en: Finally, we can observe output like figure 8.13, which shows the `generation`
    of sampled images by cycling through sample mean and variance parameters. This
    code creates a large grid image of size 10×10, which is just a NumPy array. Then,
    linear spaces or lists of values are generated for a range of values of mean and
    variance, as shown in listing 8.28\. After that, the code loops through these
    values, mean/variance, and uses them as inputs into the decoder model. The decoder
    model then uses the `predict` function to generate an image based on the mean/variance
    values. This predicted or generated image is then plotted into the grid, as shown
    in the results of figure 8.13.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以观察到如图8.13所示的输出，它显示了通过循环样本均值和方差参数来`生成`样本图像。此代码创建了一个大小为10×10的大网格图像，它只是一个NumPy数组。然后，为均值和方差的多个值生成线性空间或值列表，如列表8.28所示。之后，代码遍历这些值，均值/方差，并将它们作为解码器模型的输入。解码器模型然后使用`predict`函数根据均值/方差值生成图像。然后，这个预测或生成的图像被绘制到网格中，如图8.13的结果所示。
- en: 'Listing 8.28 EDL_8_5_VAE.ipynb: Generating manifold images'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.28 EDL_8_5_VAE.ipynb：生成流形图像
- en: '[PRE27]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ Creates linear space values
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建线性空间值
- en: ❷ Loops through values
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历值
- en: ❸ Creates a sample parameters tensor
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建一个样本参数张量
- en: ❹ The decoder generates an output image.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 解码器生成输出图像。
- en: ❺ Plots the figure into the image grid
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将图形绘制到图像网格中
- en: '![](../Images/CH08_F16_Lanham.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F16_Lanham.png)'
- en: Figure 8.16 An example VAE output
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16 一个VAE输出示例
- en: The results produced by a VAE can be quite good in a short time. On top of that,
    we have more control over the model and can easily identify what the model is
    learning. This, in turn, makes it easier to optimize the model, given several
    options.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: VAE在短时间内产生的结果可以相当好。除此之外，我们对模型有更多的控制权，可以轻松地识别模型正在学习的内容。这反过来又使得优化模型变得更容易，因为有几个选项可供选择。
- en: 8.5.3 Learning exercises
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.3 学习练习
- en: 'Use the following exercises to help improve your understanding of VAE:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下练习来帮助提高你对VAE的理解：
- en: Alter various hyperparameters in listing 8.21 and then rerun the notebook. See
    what effect each of the hyperparameters has on the generated results.
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改列表8.21中的各种超参数，然后重新运行笔记本。看看每个超参数对生成的结果有什么影响。
- en: Increase or decrease the number encoder and decoder model layers in listings
    8.22 and 8.26 and then rerun the notebook.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在列表8.22和8.26中增加或减少编码器和解码器模型层的数量，然后重新运行笔记本。
- en: Adjust and tune the VAE model, so it produces the best version of figure 8.13.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整和调整VAE模型，使其产生最佳版本的图8.13。
- en: Reviewing and understanding how a VAE works is essential background for chapter
    9\. Take an opportunity to understand the basics of what we just covered, especially
    understanding how images can be generated from the learned representation space.
    This information is fundamental to the next chapter’s journey into evolutionary
    generative DL.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 审查和理解VAE的工作原理是第9章的必要背景。抓住机会理解我们刚刚覆盖的基本内容，特别是理解如何从学习到的表示空间生成图像。这些信息是下一章进入进化生成深度学习旅程的基础。
- en: Summary
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: AEs are the foundation of generative modeling/learning and use unsupervised
    learning to train the model.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AEs是生成建模/学习的基础，并使用无监督学习来训练模型。
- en: AEs function by encoding data down to a latent/middle representation and then
    rebuilding or decoding the data back to its original form.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AEs通过将数据编码到潜在/中间表示，然后重建或解码数据回到其原始形式来工作。
- en: The internal middle/latent representation requires a middle bottleneck to reduce
    or compress the data. The process of compression allows the model to learn a latent
    representation of data.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部中间/潜在表示需要一个中间瓶颈来减少或压缩数据。压缩过程允许模型学习数据的潜在表示。
- en: Complex AEs that use convolutional (CNN) layers can be complicated to build.
    Neuroevolution can be used to build a layered architecture that defines encoder
    and decoder sections. The use of convolutional layers in the encoder and decoder
    requires additional `UpSampling` layers and matching layer configurations. These
    specialized configurations can be encoded into custom genetic sequences.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用卷积（CNN）层的复杂AE可能很难构建。可以使用神经进化来构建定义编码器和解码器部分的分层架构。在编码器和解码器中使用卷积层需要额外的`UpSampling`层和匹配的层配置。这些专用配置可以编码到定制的遗传序列中。
- en: Custom `mutation` and `crossover` operators can be developed to handle the custom
    genetic encoding needed for building an evolutionary AE.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以开发定制的`变异`和`交叉`算子来处理构建进化AE所需的定制遗传编码。
- en: Training an evolutionary AE that builds an evolved architecture may take some
    time to explore multiple architectures.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个构建进化架构的进化自动编码器可能需要一些时间来探索多个架构。
- en: The learned latent representation in a variational AE can be used to visualize
    what the internal representation looks like. Variational AEs are an extension
    to AE that uses a middle sampling layer to disconnect the encoder from the decoder.
    Disconnecting the encoder or decoder provides for better performance.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变分自动编码器（VAE）中学习的潜在表示可以用来可视化内部表示的样子。变分自动编码器是对自动编码器的一种扩展，它使用一个中间采样层来将编码器与解码器断开连接。断开编码器或解码器可以提供更好的性能。
