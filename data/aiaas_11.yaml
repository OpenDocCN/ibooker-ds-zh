- en: 8 Gathering data at scale for real-world AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 在真实世界人工智能中大规模收集数据
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Selecting sources of data for AI applications
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择人工智能应用的数据来源
- en: Building a serverless web crawler to find sources for large-scale data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个无服务器网络爬虫以寻找大规模数据来源
- en: Extracting data from websites using AWS Lambda
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AWS Lambda从网站中提取数据
- en: Understanding compliance, legal aspects, and politeness considerations for large-scale
    data gathering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解大规模数据收集的合规性、法律方面和礼貌考虑
- en: Using CloudWatch Events as a bus for event-driven serverless systems
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CloudWatch Events作为事件驱动无服务器系统的总线
- en: Performing service orchestration using AWS Step Functions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AWS Step Functions进行服务编排
- en: In chapter 7, we dealt with the application of natural language processing (NLP)
    techniques to product reviews. We showed how sentiment analysis and classification
    of text can be achieved with AWS Comprehend using streaming data in a serverless
    architecture. In this chapter, we are concerned with data gathering.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，我们讨论了自然语言处理（NLP）技术在产品评论中的应用。我们展示了如何使用AWS Comprehend在无服务器架构中使用流数据实现情感分析和文本分类。在本章中，我们关注数据收集。
- en: 'According to some estimates, data scientists spend 50-80% of their time collecting
    and preparing data.[1](#pgfId-1092384) [2](#pgfId-1092387) Many data scientists
    and machine learning practitioners will say that finding good quality data and
    preparing it correctly are the biggest challenges faced when performing analytics
    and machine learning tasks. It is clear that the value of applying machine learning
    is only as good as the quality of the data that is fed into the algorithm. Before
    we jump straight into developing any AI solution, there are a few key questions
    to be answered concerning the data that will be used:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 根据一些估计，数据科学家花费50-80%的时间在收集和准备数据。[1](#pgfId-1092384) [2](#pgfId-1092387) 许多数据科学家和机器学习实践者会说，在执行分析和机器学习任务时，找到高质量的数据并正确准备它是面临的最大挑战。显然，应用机器学习的价值仅取决于输入算法的数据质量。在我们直接开发任何AI解决方案之前，有一些关键问题需要回答，这些问题涉及到将要使用的数据：
- en: What data is required and in what format?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要哪些数据以及数据格式是什么？
- en: What sources of data are available?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用的数据来源有哪些？
- en: How will the data be cleansed?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据将如何被清洗？
- en: A good understanding of data gathering concepts is key to a functional machine
    learning application. Once you have learned to source and adapt data to your application’s
    needs, your chances of producing the desired results are greatly increased!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据收集概念的良好理解是功能机器学习应用的关键。一旦你学会了根据应用需求获取和调整数据，你产生预期结果的机会将大大增加！
- en: '8.1 Scenario: Finding events and speakers'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 场景：寻找事件和演讲者
- en: Let’s consider a problem that a lot of software developers have--finding relevant
    conferences to attend. Imagine that we wanted to build a system to solve this
    problem. Users will be able to search for conferences on topics of interest, and
    see who’s speaking at the conference, what the location is, and when it takes
    place. We can also imagine extending this to recommend conferences to users who
    have searched for or “liked” other events.[3](#pgfId-1092398)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个许多软件开发者都会遇到的问题——寻找要参加的相关会议。想象一下，如果我们想要构建一个解决这个问题的系统。用户将能够搜索感兴趣主题的会议，并查看会议的演讲者、地点以及会议时间。我们还可以想象扩展这个功能，向搜索过或“喜欢”过其他活动的用户推荐会议。[3](#pgfId-1092398)
- en: '![](../Images/CH08_F01_Elger.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F01_Elger.png)'
- en: Figure 8.1 Our data gathering application will crawl conference websites and
    extract event and speaker information.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 我们的数据收集应用程序将爬取会议网站并提取事件和演讲者信息。
- en: The first challenge in building such a system is in collecting and cataloging
    the data on conference events. There is no existing, complete, structured source
    of such data. We can find websites for relevant events using an internet search
    engine, but then comes the problem of finding and extracting the event location,
    dates, and speaker and topic information. This is a perfect opportunity to apply
    web crawling and scraping! Let’s summarize our requirements with figure 8.1.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 构建这样一个系统的第一个挑战在于收集和编目会议事件的数据。目前没有现成的、完整的、结构化的数据来源。我们可以使用搜索引擎找到相关事件的网站，但接下来就是找到和提取事件地点、日期以及演讲者和主题信息的问题。这是一个应用网络爬虫和抓取的绝佳机会！让我们用图8.1总结我们的需求。
- en: 8.1.1 Identifying data required
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 确定所需数据
- en: 'The first step in identifying your data is to start with the problem you are
    solving. If you have a clear picture of what you are going to achieve, work back
    from there, and determine what data is required and what attributes it should
    have. The kind of data required is significantly impacted by two factors:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 识别你的数据的第一个步骤是从你正在解决的问题开始。如果你对你要实现的目标有一个清晰的了解，就从这个地方开始工作，并确定所需的数据及其属性。所需数据类型受到两个因素的影响：
- en: Are *training and validation* necessary?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练和验证*是否必要？'
- en: If so, will your data have to be labelled?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是这样，你的数据是否需要标注？
- en: Throughout this book, we have been using managed AI services. A major advantage
    of this approach is that it often eliminates the need for training. Services that
    do not require you to train with your own data come with pre-trained models that
    are ready for use with your test data set. In other cases, you might need a training
    step.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们一直在使用托管AI服务。这种方法的一个主要优势是它通常消除了训练的需要。不需要使用自己的数据进行训练的服务附带预训练模型，这些模型可以与你的测试数据集一起使用。在其他情况下，你可能需要执行训练步骤。
- en: Training, validation, and test data In the development of machine learning models,
    a data set is typically divided into three sets, as shown in figure 8.2.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 训练、验证和测试数据 在机器学习模型开发过程中，通常将数据集分为三个集合，如图8.2所示。
- en: '![](../Images/CH08_F02_Elger.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F02_Elger.png)'
- en: Figure 8.2 Training, validation, and test data during model development and
    test
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 模型开发和测试中的训练、验证和测试数据
- en: A larger proportion of the data, the training set, is used to train the algorithm.
    The validation set (or development set) is used to select the algorithm and measure
    its performance. Finally, the test set is an independent set used to check how
    well the algorithm generalizes with data not used in training.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 更大的数据比例，即训练集，用于训练算法。验证集（或开发集）用于选择算法并衡量其性能。最后，测试集是一个独立的集合，用于检查算法在未用于训练的数据上的泛化能力。
- en: You might remember the topic of supervised and unsupervised learning from chapter
    1\. It is important to understand which approach you are using because supervised
    learning will require data to be annotated with labels.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得第一章中提到的监督学习和无监督学习。了解你使用的方法很重要，因为监督学习需要将数据标注上标签。
- en: In chapter 1, we presented a table of managed AWS AI services. This table is
    extended in appendix A, showing the data requirements and training support for
    each service. You can use this as a reference in planning your AI-enabled application.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们展示了一个托管AWS AI服务的表格。这个表格在附录A中得到了扩展，显示了每个服务的数据要求和训练支持。你可以在规划你的AI赋能应用程序时使用这个作为参考。
- en: If you are not using managed AI services, but instead selecting an algorithm
    and training a custom model, the effort required for gathering and preparing data
    is large. There are many considerations to getting data that will produce accurate
    results that work well within the domain of test data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有使用托管AI服务，而是选择一个算法并训练一个自定义模型，那么收集和准备数据所需的工作量很大。有许多考虑因素需要确保数据能够产生准确的结果，并在测试数据集的领域内良好工作。
- en: Selecting representative data
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 选择代表性数据
- en: When selecting data to train a machine learning model, it is critical to ensure
    that the data is representative of data in the wild. Problems occur when your
    data makes assumptions that result in a prejudiced outcome. Selection of good
    training data is important to reduce *overfitting*. Overfitting occurs when a
    model is too specific to a training data set and is not able to generalize.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择用于训练机器学习模型的数据时，确保数据能够代表真实世界中的数据至关重要。当你的数据做出导致偏见结果的假设时，就会出现问题。选择良好的训练数据对于减少*过拟合*至关重要。当模型对训练数据集过于特定，无法泛化时，就会发生过拟合。
- en: A team of researches at the University of Washington illustrated the issue of
    *selection bias* by training a machine learning model to detect whether a picture
    contained a wolf or a husky dog. By deliberately choosing wolf pictures with snow
    in the background and husky dog pictures with grass in the background, they trained
    an algorithm that was actually only effective at detecting grass vs. snow. When
    they presented the result to a set of test subjects, people still reported that
    they trusted the algorithm’s ability to detect huskies and wolves![4](#pgfId-1096213)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 华盛顿大学的一组研究人员通过训练一个机器学习模型来检测图片中是否包含狼或哈士奇狗，说明了*选择偏差*的问题。他们故意选择有雪背景的狼图片和有草背景的哈士奇狗图片来训练算法，实际上这个算法只对检测草与雪有效。当他们将结果展示给一组测试对象时，人们仍然报告说他们相信算法检测哈士奇和狼的能力![4](#pgfId-1096213)
- en: We also know that training with data from systems that are the output of human
    biases can result in the algorithms inheriting existing harmful societal biases.
    This was infamously shown when Microsoft’s “Tay” Twitter bot was shut down after
    it started generating racist, hateful tweets.[5](#pgfId-1096217)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还知道，使用来自具有人类偏见输出的系统的数据进行训练可能会导致算法继承现有的有害社会偏见。当微软的“Tay”推特机器人开始生成种族主义、仇恨言论后被迫关闭时，这一情况被臭名昭著地展示出来。[5](#pgfId-1096217)
- en: 'A few rules can be applied to choosing good data:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择好的数据时，可以应用以下几条规则：
- en: Data should have representations of all scenarios that may be encountered “in
    the wild” (like huskies on backgrounds other than snow!).
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据应该有所有可能遇到的场景的表示（比如除了雪以外的背景上的哈士奇！）。
- en: For classification, you should have sufficient, and preferably roughly equal,
    representations of all classes.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于分类，你应该有足够，并且最好是所有类别的近似相等，的表示。
- en: For labelling, consider whether the labels can be assigned without ambiguity,
    or how to deal with it if not. You might have cases where the label to assign
    is not clear (“Is that a husky or a dog?”).
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于标签，考虑是否可以无歧义地分配标签，或者如果不可以，如何处理这种情况。你可能会有这样的情况，要分配的标签并不明确（“那是一只哈士奇还是一只狗？”）。
- en: Regularly inspect a reasonably-sized random selection of your data manually
    to verify that there is nothing unexpected occurring. It is worth taking some
    time for this, because bad data will never produce good results!
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定期手动检查你数据的一个合理大小的随机样本，以验证是否发生了意外的情况。花些时间做这件事是值得的，因为坏数据永远不会产生好的结果！
- en: In this book, we are largely concerned with using pre-trained, managed services.
    For a more in-depth understanding of machine learning training optimization, data
    wrangling, and feature engineering, we recommend *Real-World Machine Learning*
    by Brink, Richards, and Fetherolf from Manning Publications, 2017\.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们主要关注使用预训练的、托管的服务。对于机器学习训练优化、数据整理和特征工程的更深入理解，我们推荐Brink、Richards和Fetherolf所著的《Real-World
    Machine Learning》，由Manning Publications于2017年出版。[*Real-World Machine Learning*](https://www.manning.com/books/real-world-machine-learning)。
- en: 8.1.2 Sources of data
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 数据来源
- en: One of the key points discussed in chapter 1 was how recent successes in the
    field of AI have been made possible by the availability of vast volumes of data.
    The internet itself is a public source of data, and by using the internet in our
    daily lives, we are constantly contributing to the growing volumes of incredibly
    detailed data. Big technology companies (Google, Facebook, Amazon) have had great
    success in AI. A large factor in this is their access to data and expertise in
    data gathering.[6](#pgfId-1092451) For everyone else, there are many ways of sourcing
    data for AI applications. Appendix C contains a list of public data sets and other
    data sources that may be a great fit for your application.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 第一章讨论的关键点之一是，人工智能领域的近期成功是如何通过大量数据的可用性得以实现的。互联网本身就是数据的一个公共来源，通过在我们的日常生活中使用互联网，我们不断为不断增长的大量极其详细的数据做出贡献。大型科技公司（如谷歌、Facebook、亚马逊）在人工智能领域取得了巨大成功。这其中的一个重要因素是他们可以访问数据，并且在数据收集方面有专业知识。[6](#pgfId-1092451)
    对于其他人来说，有许多方法可以为人工智能应用获取数据。附录C包含了一列可能非常适合你应用需求的公共数据集和其他数据来源。
- en: 8.1.3 Preparing data for training
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.3 准备训练数据
- en: 'Once you have gathered data for training, there is still plenty of work to
    be done:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你收集了用于训练的数据，还有很多工作要做：
- en: Deal with missing data. You might need to remove records, interpolate or extrapolate
    data, or use some other means of avoiding problems with missing fields. In other
    cases, it is better to leave missing fields empty, as this can be an important
    input into the algorithm. For more on this topic, take a look at chapter 1, “Exploring
    Data,” of *Exploring Data Science* by John Mount and Nina Zumel.[7](#pgfId-1092459)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理缺失数据。你可能需要删除记录、插值或外推数据，或使用其他方法避免缺失字段的问题。在其他情况下，最好将缺失字段留空，因为这可能是算法的重要输入。关于这个话题的更多信息，请参阅John
    Mount和Nina Zumel所著的《探索数据科学》第1章“探索数据”。[7](#pgfId-1092459)
- en: Get the data in the right format. This could mean applying a consistent format
    for date or currency values. In image recognition, it might mean cropping, resizing,
    and changing color formats. Many pre-trained networks are trained on 224x224 RGB
    data, so if you want to analyse very high-resolution data (where too much information
    will be lost if images are resized), then these networks may not be suitable without
    modification.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取正确的数据格式。这可能意味着为日期或货币值应用一致的格式。在图像识别中，这可能意味着裁剪、调整大小和更改颜色格式。许多预训练的网络是在224x224
    RGB数据上训练的，所以如果你想要分析非常高分辨率的图像数据（如果调整大小，则可能会丢失太多信息），那么这些网络可能在不修改的情况下不适用。
- en: We have briefly covered some of the data sources available to machine learning
    engineers. It should be clear that the internet has been a major source of large-scale
    data volumes. Much internet data is not available via an API or in structured
    files, but is published on websites intended to be consumed with a web browser.
    Gathering data from this valuable trove requires crawling, scraping, and extraction.
    This is the topic we will cover next.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要介绍了机器学习工程师可用的数据源。应该很明显，互联网是大规模数据量的主要来源。大量互联网数据无法通过API或结构化文件访问，而是发布在旨在用网络浏览器消费的网站上。从这些宝贵的资源中收集数据需要爬取、抓取和提取。这是我们接下来要讨论的主题。
- en: 8.2 Gathering data from the web
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 从网络收集数据
- en: The remainder of this chapter will look in more detail at gathering data from
    websites. Though some data may be available in pre-packaged, structured formats,
    accessible as either flat files or through an API, this is not the case with web
    pages.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的剩余部分将更详细地探讨从网站收集数据。尽管一些数据可能以预包装、结构化的格式提供，可以通过平面文件或API访问，但网页并非如此。
- en: 'Web pages are an unstructured source of information such as product data, news
    articles, and financial data. Finding the right web pages, retrieving them, and
    extracting relevant information are non-trivial. The processes required to do
    this are known as *web crawling* and *web scraping*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 网页是产品数据、新闻文章和财务数据等非结构化信息来源。找到合适的网页、检索它们并提取相关信息是非平凡的。完成这些所需的过程被称为*网络爬取*和*网络抓取*：
- en: '*Web crawling* is the process of fetching web content and navigating to linked
    pages according to a specific strategy.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*网络爬取*是根据特定策略抓取网页内容并导航到链接页面的过程。'
- en: '*Web scraping* follows the crawling process to extract specific data from content
    that has been fetched.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*网络抓取*跟随爬取过程，从已抓取的内容中提取特定数据。'
- en: Figure 8.3 shows how the two processes combine to produce meaningful, structured
    data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3展示了两个过程如何结合以产生有意义的、结构化的数据。
- en: '![](../Images/CH08_F03_Elger.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3](../Images/CH08_F03_Elger.png)'
- en: Figure 8.3 Web page crawling and scraping process overview. In this chapter,
    we are concerned with the *crawler* part of this picture and the pages it produces
    as output.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 网页爬取和抓取过程概述。在本章中，我们关注的是这幅图中的*爬取器*部分及其产生的输出页面。
- en: Recall our conference speaker information-gathering scenario from the start
    of the chapter. Our first step in creating a solution for this scenario will be
    to build a serverless web crawling system.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下本章开头提到的会议演讲者信息收集场景。为这个场景创建解决方案的第一步将是构建一个无服务器网络爬取系统。
- en: 8.3 Introduction to web crawling
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 网络爬取简介
- en: The crawler for our scenario will be a *generic* crawler. Generic crawlers can
    crawl any site with an unknown structure. Site-specific crawlers are usually created
    for large sites, with specific selectors for findings links and content. An example
    of a site-specific crawler could be one written to crawl particular products from
    amazon.com, or auctions from ebay.com.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们场景中的爬取器将是一个*通用*爬取器。通用爬取器可以爬取任何结构未知的网站。特定网站的爬取器通常是为大型网站创建的，具有用于查找链接和内容的特定选择器。一个特定网站爬取器的例子可能是编写来爬取amazon.com特定产品或ebay.com拍卖的爬取器。
- en: Examples of well-known crawlers include
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一些知名爬虫的例子包括
- en: Search engines such as Google, Bing, Yandex, or Baidu
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索引擎如Google、Bing、Yandex或Baidu
- en: GDELT Project ([https://www.gdeltproject.org](https://www.gdeltproject.org)),
    an open database of human society and global events
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GDELT项目（[https://www.gdeltproject.org](https://www.gdeltproject.org)），一个关于人类社会和全球事件的开源数据库
- en: OpenCorporates ([https://opencorporates.com](https://opencorporates.com)), the
    largest open database of companies in the world
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenCorporates（[https://opencorporates.com](https://opencorporates.com)），世界上最大的开放公司数据库
- en: Internet Archive ([https://archive.org](https://archive.org)), a digital library
    of internet sites and other cultural artifacts in digital form
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 互联网档案馆（[https://archive.org](https://archive.org)），一个包含互联网网站和其他数字形式的文化遗产的数字图书馆
- en: CommonCrawl ([https://commoncrawl.org/](https://commoncrawl.org/)), an open
    repository of web crawl data
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CommonCrawl（[https://commoncrawl.org/](https://commoncrawl.org/)），一个开放的网页爬取数据存储库
- en: One challenge for web crawling is the sheer number of web pages to visit and
    analyze. When we are performing the crawling task, we may need arbitrarily large
    compute resources. Once the crawling process is complete, our compute resource
    requirement drops. This scalable, bursty compute requirement is an ideal fit for
    on-demand, cloud computing, and Serverless!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 网络爬取的一个挑战是访问和分析的网页数量庞大。当我们执行爬取任务时，我们可能需要任意大的计算资源。一旦爬取过程完成，我们的计算资源需求就会下降。这种可扩展的、突发性的计算需求非常适合按需、云计算和无服务器！
- en: 8.3.1 Typical web crawler process
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 典型网络爬虫过程
- en: 'To understand how a web crawler might work, consider how a web browser allows
    a user to navigate a web page manually:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解网络爬虫可能的工作方式，可以考虑网页浏览器如何允许用户手动导航网页：
- en: The user enters a web page URL into a web browser.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户将网页URL输入到网页浏览器中。
- en: The browser fetches the page’s first HTML file.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 浏览器获取网页的第一个HTML文件。
- en: The HTML file is parsed by the browser to find other required content such as
    CSS, JavaScript, and images.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 浏览器解析HTML文件以找到其他所需内容，如CSS、JavaScript和图片。
- en: Links are rendered. When the user clicks on a link, the process is repeated
    for a new URL.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 链接会被渲染。当用户点击链接时，过程会为新的URL重复。
- en: The following listing shows the HTML source for a very simple example web page.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了非常简单的示例网页的HTML源代码。
- en: Listing 8.1 Example web page HTML source
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.1 示例网页HTML源代码
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ External link
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 外部链接
- en: ❷ Absolute internal link
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 绝对内部链接
- en: ❸ Relative internal link
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 相对内部链接
- en: ❹ Image resource
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 图片资源
- en: ❺ Paragraph text
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 段落文本
- en: ❻ JavaScript source
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ JavaScript源代码
- en: We have shown the structure of a very basic page. In reality, a single HTML
    page can contain hundreds of hyperlinks, both internal and external. The set of
    pages required to be crawled for a given application is known as the *crawl space*.
    Let’s talk about the architecture of a typical web crawler and how it is structured
    to deal with various sizes of crawl space.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了非常基础的页面结构。实际上，一个单独的HTML页面可以包含数百个超链接，包括内部和外部链接。为特定应用需要爬取的页面集合被称为*爬取空间*。让我们讨论典型网络爬虫的架构以及它是如何构建以处理不同大小的爬取空间的。
- en: 8.3.2 Web crawler architecture
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 网络爬虫架构
- en: A typical web crawler architecture is illustrated in figure 8.4\. Let’s get
    an understanding of each component of the architecture and how it relates to our
    conference website scenario before describing how this might be realized with
    a serverless approach.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的网络爬虫架构如图8.4所示。在描述如何使用无服务器方法实现之前，让我们先了解架构的每个组件以及它们如何与我们的会议网站场景相关。
- en: '![](../Images/CH08_F04_Elger.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F04_Elger.png)'
- en: Figure 8.4 Components of a web crawler. There are distinct responsibilities
    for each component that can guide us in our software architecture.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 网络爬虫的组件。每个组件都有明确的职责，这可以指导我们在软件架构中的工作。
- en: The *frontier* maintains a database of URLs to be crawled. This is initially
    populated with the conference websites. From there, URLs of individual pages on
    the site are added here.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*前端*维护一个待爬取URL的数据库。这最初由会议网站填充。从这里，网站上单个页面的URL被添加到这里。'
- en: The *fetcher* takes a URL and retrieves the corresponding document.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*获取器*接受一个URL并检索相应的文档。'
- en: The *parser* takes the fetched document, parses it, and extracts required information
    from it. We will not be looking for specific speaker details or anything conference-specific
    at this point.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*解析器*获取获取到的文档，解析它，并从中提取所需信息。在此阶段，我们不会寻找特定的演讲者详细信息或任何会议特定的内容。'
- en: The *strategy worker or generator* is one of the most crucial components of
    a web crawler, since it determines the *crawl space*. URLs generated by the strategy
    worker are fed back into the frontier. The strategy worker decides
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*策略工作器或生成器*是网络爬虫最关键组件之一，因为它决定了*爬取空间*。策略工作器生成的URL被反馈到前沿。策略工作器决定'
- en: Which links should be followed
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该跟随哪些链接
- en: The priority of links to be crawled
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要爬取的链接的优先级
- en: The crawl depth
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爬取深度
- en: When to revisit/recrawl pages if required
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要，何时重新访问/重新爬取页面
- en: The *item store* is where the extracted documents or data are stored.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*项目存储*是提取的文档或数据存储的地方。'
- en: The *scheduler* takes a set of URLs, initially the seed URLs, and schedules
    the *fetcher* to download resources. The scheduler is responsible for ensuring
    that the crawler behaves politely toward web servers, that no duplicate URLs are
    fetched, and that URLs are normalized.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*调度器*接受一组URL，最初是种子URL，并安排*获取器*下载资源。调度器负责确保爬虫对Web服务器表现得礼貌，不抓取重复的URL，并且URL是规范化的。'
- en: Is crawling really amenable to serverless architecture?
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 爬取是否真的适合无服务器架构？
- en: If at this point you’re wondering whether serverless architecture really is
    a valid choice for the implementation of a web crawler, you have a good point!
    Web crawlers, operating at scale, require fast, efficient storage; caching; and
    plenty of compute power for multiple, resource-intensive page rendering processes.
    Serverless applications, on the other hand, are typically characterized by short-term,
    event-driven computation and the absence of fast, local disk storage.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在在怀疑无服务器架构是否真的是实现网络爬虫的有效选择，你有一个很好的观点！大规模运行的爬虫需要快速、高效的存储；缓存；以及大量的计算能力来处理多个资源密集型的页面渲染过程。另一方面，无服务器应用程序通常以短期、事件驱动的计算和缺乏快速、本地磁盘存储为特征。
- en: So, is the system in this chapter worthy of production use, or are we embarking
    on a wild experiment to see how far we can push our cloud-native ideology?! There
    are definite advantages to using a more traditional “farm” of servers such as
    Amazon Elastic Compute Cloud (EC2) instances. If your crawling needs require a
    constantly-running workload at large volumes, you might be better off choosing
    a traditional approach.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，本章中的系统是否值得在生产中使用，或者我们是在进行一项疯狂的实验，看看我们能将我们的云原生理念推进多远？！使用更传统的“农场”服务器，如亚马逊弹性计算云（EC2）实例，确实有明显的优势。如果你的爬取需求需要大量持续运行的工作负载，你可能更适合选择传统方法。
- en: We must remember the hidden cost of maintaining and running this infrastructure,
    the operating system, and any underlying frameworks. Also, our crawling scenario
    is for on-demand extraction of data regarding specific conference websites. This
    “bursty” behavior is suitable to an elastic, utility computing paradigm. A serverless
    implementation may not be optimal from a caching perspective, but for our scenario,
    that does not have a major impact. We are more than happy with this approach,
    given that we are paying $0 while the system is not running and we don’t have
    to worry about operating system patches, maintenance, or container orchestration
    and service discovery.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须记住维护和运行此基础设施、操作系统以及任何底层框架的隐藏成本。此外，我们的爬取场景是针对特定会议网站的数据按需提取。这种“突发”行为适合弹性、实用计算范式。从缓存的角度来看，无服务器实现可能不是最优的，但就我们的场景而言，这不会产生重大影响。我们对此方法非常满意，因为我们不需要支付任何费用，当系统不运行时，我们也不必担心操作系统的补丁、维护或容器编排和服务发现。
- en: For our web crawler, we are dealing with conferences. Since these constitute
    a minority of all web pages, there is no need to crawl the entire web for such
    sites. Instead, we will provide the crawler with a “seed” URL.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的网络爬虫，我们处理的是会议。由于这些构成了所有网页中的少数，因此没有必要为这些网站爬取整个网络。相反，我们将为爬虫提供一个“种子”URL。
- en: On the conference sites themselves, we will crawl local hyperlinks. We will
    not follow hyperlinks to external domains. Our goal is to find the pages that
    contain the required data, such as speaker information and dates. We are not interested
    in crawling the entire conference site, and for this reason we will also use a
    *depth limit* to stop crawling after reaching a given depth in the link graph.
    The crawl depth is the number of links that have been followed from the seed URL.
    A depth limit stops the process from going beyond a specified depth.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在会议网站本身，我们将抓取本地超链接。我们不会跟随指向外部域的超链接。我们的目标是找到包含所需数据的页面，例如演讲者和日期。我们不对爬取整个会议网站感兴趣，因此我们还将使用*深度限制*来在达到链接图中的给定深度后停止爬取。爬取深度是从种子URL开始跟随的链接数量。深度限制阻止过程超出指定的深度。
- en: Basic crawlers versus rendering crawlers
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基本爬虫与渲染爬虫
- en: Basic crawlers will fetch only HTML pages and will not evaluate JavaScript.
    This leads to a much simpler and faster crawl process. However, this may result
    in us excluding valuable data.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 基本爬虫只会抓取HTML页面，而不会评估JavaScript。这导致爬虫过程更加简单和快速。然而，这可能会导致我们排除有价值的数据。
- en: It is now very common to have web pages that are rendered dynamically in the
    browser by JavaScript. Single-page applications (SPAs) using frameworks like React
    or Vue.js are examples of this. Some sites use server-side rendering with these
    frameworks, and others perform pre-rendering to return fully-rendered HTML to
    search engine crawlers as a search engine optimization (SEO) technique. We cannot
    rely on these being universally employed. For these reasons, we are opting to
    employ full rendering of web pages, including JavaScript evaluation.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在非常常见的是，网页由JavaScript在浏览器中动态渲染。使用React或Vue.js等框架的单页应用程序（SPA）是此类示例。一些网站使用这些框架的服务器端渲染，而其他网站则执行预渲染，以将完全渲染的HTML作为搜索引擎优化（SEO）技术返回给搜索引擎爬虫。我们不能依赖这些在普遍应用。因此，我们选择采用完整的网页渲染，包括JavaScript评估。
- en: 'There are a number of options for rendering web pages when there is no user
    or screen available:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当没有用户或屏幕可用时，渲染网页有多种选项：
- en: Splash ([https://scrapinghub.com/splash](https://scrapinghub.com/splash)), a
    browser designed for web scraping applications.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Splash ([https://scrapinghub.com/splash](https://scrapinghub.com/splash))，一个专为网络爬虫应用设计的浏览器。
- en: Headless Chrome ([http://mng.bz/r2By](http://mng.bz/r2By)) with the Puppeteer
    API. This simply runs the popular Chrome browser and allows us to control it programatically.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Puppeteer API的Headless Chrome ([http://mng.bz/r2By](http://mng.bz/r2By))。这简单地运行了流行的Chrome浏览器，并允许我们以编程方式控制它。
- en: Headless Firefox ([http://mng.bz/V8qG](http://mng.bz/V8qG)) with Selenium ([https://www.seleniumhq.org](https://www.seleniumhq.org)).
    This option is a Firefox-based alternative to Puppeteer.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Selenium ([https://www.seleniumhq.org](https://www.seleniumhq.org))的Headless
    Firefox ([http://mng.bz/V8qG](http://mng.bz/V8qG))。这是Puppeteer的基于Firefox的替代方案。
- en: For our solution, we are going to use headless Chrome. We have chosen this option
    because there are readily available Serverless Framework plugins for use with
    AWS Lambda.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的解决方案，我们将使用无头Chrome。我们选择这个选项是因为有现成的Serverless Framework插件可用于AWS Lambda。
- en: Legal and compliance considerations for web crawling
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 网络爬虫的法律和合规性考虑
- en: 'The legality of web crawling can be a contentious area. On one hand, the site
    owner is making content publicly available. On the other hand, heavy-handed crawling
    can have an adverse impact on the site’s availability and server load. Needless
    to say, the following does not represent legal advice. Here are just a few best
    practices that are regarded as polite behavior:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 网络爬虫的合法性可能是一个有争议的领域。一方面，网站所有者使内容公开可用。另一方面，过度的爬取可能会对网站的可用性和服务器负载产生不利影响。不用说，以下内容不构成法律建议。这里只是几项被视为礼貌行为的最佳实践：
- en: Identify your crawler using the `User-Agent` string. Provide a way for site
    owners to contact you, e.g. `AIaaSBookCrawler/1.0;` `+https://aiasaservicebook.com)`.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`User-Agent`字符串识别您的爬虫。为网站所有者提供一种联系方式，例如`AIaaSBookCrawler/1.0;` `+https://aiasaservicebook.com)`。
- en: Respect a site’s `robots.txt`. This file allows site owners to say what pages
    you may and may not crawl.[8](#pgfId-1098652)
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尊重网站的`robots.txt`。此文件允许网站所有者说明您可以和不可以爬取的页面。[8](#pgfId-1098652)
- en: Use a site’s API, if available, instead of web scraping.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果可用，请使用网站的API而不是网络爬虫。
- en: Limit the number of requests per second per domain.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制每个域每秒的请求数量。
- en: Cease crawling a site immediately if requested by the site owner.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果网站所有者要求，立即停止爬取网站。
- en: Only crawl publicly accessible content. Never use login credentials.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只爬取公开可访问的内容。永远不要使用登录凭证。
- en: Use caching to reduce load on the target server. Don’t refetch the same page
    repeatedly in a short amount of time.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用缓存以减少目标服务器的负载。不要在短时间内重复检索同一页面。
- en: Material gathered from websites generally falls under copyright and intellectual
    property legislation. Make sure that you respect this.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从网站上收集的材料通常属于版权和知识产权法规的范畴。请确保您尊重这一点。
- en: In particular, we need to make sure that we limit the concurrency per domain/IP
    address, or that we choose a reasonable delay between requests. These requirements
    will be a consideration in our serverless crawler architecture.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是我们需要确保我们限制每个域名/IP地址的并发性，或者选择在请求之间选择合理的延迟。这些要求将是我们无服务器爬虫架构的考虑因素。
- en: At the time of writing, the AWS Acceptable Use Policy prevents “Monitoring or
    crawling of a System that impairs or disrupts the System being monitored or crawled”
    ([https://aws.amazon.com/aup/](https://aws.amazon.com/aup/)).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，AWS可接受使用政策禁止“监控或爬取损害或干扰被监控或爬取的系统的行为”([https://aws.amazon.com/aup/](https://aws.amazon.com/aup/))。
- en: Note also that some websites implement mechanisms to prevent web scraping. This
    can be done by detecting IP address or user agent. Solutions like CloudFlare ([https://www.cloudflare.com/products/bot-management/](https://www.cloudflare.com/products/bot-management/))
    or Google reCaptcha ([https://developers.google.com/recaptcha/docs/invisible](https://developers.google.com/recaptcha/docs/invisible))
    use more elaborate approaches.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 还请注意，一些网站实施了防止网络爬虫的机制。这可以通过检测IP地址或用户代理来实现。像CloudFlare([https://www.cloudflare.com/products/bot-management/](https://www.cloudflare.com/products/bot-management/))或Google
    reCaptcha([https://developers.google.com/recaptcha/docs/invisible](https://developers.google.com/recaptcha/docs/invisible))这样的解决方案使用了更复杂的方法。
- en: 8.3.3 Serverless web crawler architecture
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.3 无服务器网络爬虫架构
- en: Let’s take a first look at how we will map our system to the canonical architecture
    developed in chapter 1\. Figure 8.5 provides us with a breakdown of the system’s
    layers and how services collaborate to deliver the solution.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看看我们将如何将我们的系统映射到第1章中开发的规范架构。图8.5为我们提供了系统层级的分解以及服务如何协作以提供解决方案。
- en: '![](../Images/CH08_F05_Elger.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F05_Elger.png)'
- en: Figure 8.5 Serverless web crawler system architecture. The system is composed
    of customservices implemented using AWS Lambda and AWS Step Functions. SQS and
    the CloudWatchEvents service are used for asynchronous communication. Internal
    API Gateways are used for synchronous communication. S3 and DynamoDB are used
    for data storage.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 无服务器网络爬虫系统架构。系统由使用AWS Lambda和AWS Step Functions实现的定制服务组成。SQS和CloudWatchEvents服务用于异步通信。内部API网关用于同步通信。S3和DynamoDB用于数据存储。
- en: 'The system architecture shows the layers of the system across all services.
    Note that, in this system, we have no front-end web application:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 系统架构显示了所有服务中系统的各个层级。请注意，在这个系统中，我们没有前端Web应用程序：
- en: Synchronous tasks in the frontier and fetch services are implemented using AWS
    Lambda. For the first time, we introduce AWS Step Functions to implement the scheduler.
    It will be responsible for orchestrating the fetcher based on data in the frontier.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前端和抓取服务中的同步任务使用AWS Lambda实现。首次引入AWS Step Functions来实现调度器。它将负责根据前端的数据协调抓取器。
- en: The strategy service is asynchronous and reacts to events on the event bus indicating
    that new URLs have been discovered.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略服务是异步的，并响应事件总线上的事件，表明已发现新的URL。
- en: Synchronous communication between internal services in our system is handled
    with API Gateway. We have chosen CloudWatch Events and SQS for asynchronous communication.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们系统中内部服务之间的同步通信由API网关处理。我们选择了CloudWatch Events和SQS进行异步通信。
- en: Shared parameters are published to Systems Manager Parameter Store. IAM is used
    to manage privileges between services.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享参数发布到系统管理器参数存储。IAM用于管理服务之间的权限。
- en: DynamoDB is used for frontier URL storage. An S3 bucket is used as our item
    store.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DynamoDB用于前端URL存储。一个S3存储桶用作我们的项目存储。
- en: Tip If you want to learn more about web scraping, look at chapter 6, “Intelligent
    Web Crawling,” of *Collective Intelligence in Action* by Satnam Alag.[9](#pgfId-1092618)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果您想了解更多关于网络爬虫的信息，请参阅Satnam Alag所著的《Collective Intelligence in Action》第6章，“智能网络爬虫”。[9](#pgfId-1092618)
- en: Build or buy? Evaluating third-party managed services
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 建造还是购买？评估第三方托管服务
- en: There’s a certain irony in writing a book that espouses the virtues of managed
    services, emphasizes the importance of focusing on your core business logic, and
    also dedicates a chapter to building a web crawler from scratch.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 写一本宣扬托管服务优点、强调关注核心业务逻辑重要性的书，同时还在书中专门用一章来从头构建网络爬虫，这有一定的讽刺意味。
- en: 'Our crawler is quite simple and also domain-specific. This is some justification
    for writing our own implementation. However, we know from experience that simple
    systems grow in complexity over time. Therefore, implementing your own *anything*
    should be your last resort. Here are two rules of thumb for modern application
    development:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的爬虫相当简单，并且具有特定领域。这是编写我们自己的实现的一些理由。然而，我们知道从经验中，简单的系统会随着时间的推移而变得复杂。因此，实现你自己的任何东西应该是你的最后选择。以下是现代应用开发的两个经验法则：
- en: Minimize the amount of code you write! The majority of code you write should
    concern unique business logic. Where possible, avoid writing code for any part
    of your system that is mundane and implemented in many other software systems,
    frequently called undifferentiated heavy lifting.a
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化你编写的代码量！你编写的绝大多数代码应该关注独特的业务逻辑。在可能的情况下，避免编写任何系统中的代码，这些代码是平凡的，并且在许多其他软件系统中实现，通常被称为无差别的重劳动。
- en: Use cloud managed services. While you can follow rule 1 by using libraries,
    frameworks, and components, these can have their own maintenance burden, and you
    still have to maintain the infrastructure they run on. Integrating with cloud
    managed services relieves you of this significant burden.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用云托管服务。虽然你可以通过使用库、框架和组件来遵循第1条规则，但这些可能有自己的维护负担，而且你仍然需要维护它们运行的基础设施。与云托管服务集成可以减轻你这一重大负担。
- en: Such services can be found outside the realm of your chosen cloud provider.
    Even if Amazon Web Services has no off-the-shelf web crawling and scraping services,
    look beyond AWS and evaluate the third-party offerings. This is a worthwhile exercise
    for any service you are thinking of building. For example, if you want to implement
    a search feature in your application, you might evaluate a fully-managed Elasticsearch
    service such as Elastic ([https://www.elastic.co](https://www.elastic.co)) or
    a managed search and discovery API like Algolia ([https://www.algolia.com/](https://www.algolia.com/)).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这些服务可能超出了你选择的云提供商的范围。即使亚马逊网络服务没有现成的网络爬虫和抓取服务，也要超越AWS，评估第三方提供的服务。这对于你打算构建的任何服务来说都是一项值得做的练习。例如，如果你想在你应用程序中实现搜索功能，你可能评估一个完全托管的Elasticsearch服务，如Elastic
    ([https://www.elastic.co](https://www.elastic.co))，或者一个托管的搜索和发现API，如Algolia ([https://www.algolia.com/](https://www.algolia.com/))).
- en: If you are interested in evaluating third-party web scraping services, take
    a look at the following:[10](#pgfId-1098921)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感兴趣于评估第三方网络爬虫服务，请查看以下内容：[10](#pgfId-1098921)
- en: Grepsr ([https://www.grepsr.com](https://www.grepsr.com))
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grepsr ([https://www.grepsr.com](https://www.grepsr.com))
- en: Import.io ([https://www.import.io](https://www.import.io))
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Import.io ([https://www.import.io](https://www.import.io))
- en: ScrapingHub ScrapyCloud ([https://scrapinghub.com/scrapy-cloud](https://scrapinghub.com/scrapy-cloud))
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ScrapingHub ScrapyCloud ([https://scrapinghub.com/scrapy-cloud](https://scrapinghub.com/scrapy-cloud))
- en: 8.4 Implementing an item store
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 实现项目存储
- en: We will start our walkthrough of the crawler implementation with the simplest
    service of all, the item store. As part of our conference-site crawling process,
    the item store will store a copy of each page crawled on each conference website.
    First, grab the code so you can explore in more detail.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始对爬虫实现过程的讲解，从所有服务中最简单的一个，即项目存储。作为我们会议网站爬取过程的一部分，项目存储将存储每个会议网站上爬取的每一页的副本。首先，获取代码以便你可以更详细地探索。
- en: 8.4.1 Getting the code
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 获取代码
- en: The code for the item store is in the directory `chapter8-9/item-store`. Similar
    to our previous examples, this directory contains a `serverless.yml` file declaring
    our APIs, functions, and other resources. We will explain the contents as we go,
    before deploying and testing the item store.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 项目存储的代码位于`chapter8-9/item-store`目录中。类似于我们之前的示例，这个目录包含一个`serverless.yml`文件，声明我们的API、函数和其他资源。在我们部署和测试项目存储之前，我们将逐步解释其内容。
- en: 8.4.2 The item store bucket
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.2 项目存储存储桶
- en: Taking a look at `serverless.yml` in the item store, we see an S3 Bucket and
    not much else! We are implementing the simplest possible store.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目存储中查看`serverless.yml`文件，我们看到的是一个S3存储桶，没有其他内容！我们正在实现最简单的存储方案。
- en: Other services may write directly to our bucket or list objects and fetch objects
    using the AWS SDK S3 API. All that is required is that they have the correct permissions
    in their IAM roles and policies.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 其他服务可以直接写入我们的存储桶或列出对象，并使用 AWS SDK S3 API 抓取对象。所需的一切是他们 IAM 角色和策略中具有正确的权限。
- en: 8.4.3 Deploying the item store
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.3 部署项目存储
- en: 'Deploying the item store is straightforward. Given that we are deploying an
    S3 bucket, we will define its globally-unique name in a `.env` file within the
    `chapter8-9` directory:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 部署项目存储非常简单。鉴于我们正在部署一个 S3 存储桶，我们将在 `chapter8-9` 目录中的 `.env` 文件中定义其全局唯一名称：
- en: '[PRE1]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'No further configuration is required. The default region is `eu-west-1`. If
    you want to specify a different region, provide it using the `--region` argument
    to the `serverless deploy` command:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 无需进一步配置。默认区域为 `eu-west-1`。如果您想指定不同的区域，请使用 `serverless deploy` 命令的 `--region`
    参数提供：
- en: '[PRE2]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: That’s it! The item store is deployed and ready. Let’s move on to the next service
    in our crawler application.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！项目存储已部署并准备就绪。让我们继续到爬虫应用程序中的下一个服务。
- en: 8.5 Creating a frontier to store and manage URLs
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 创建一个用于存储和管理 URL 的前沿
- en: Our frontier will store all seed URLs for conference sites and newly-discovered
    URLs found during the crawl process. We are using DynamoDB for storage. Our goal
    here is to leverage DynamoDB’s API for inserting and querying, with a minimal
    layer of abstraction on top.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的前沿将存储所有用于会议网站的种子 URL 以及在爬取过程中发现的新 URL。我们正在使用 DynamoDB 进行存储。我们的目标是利用 DynamoDB
    的 API 进行插入和查询，并在其上添加最小层级的抽象。
- en: 8.5.1 Getting the code
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.1 获取代码
- en: The code for the frontier service is in the directory `chapter8-9/frontier-service`.
    This directory contains a `serverless.yml` file declaring our APIs, functions,
    and other resources. We will explain the contents as we go, before deploying and
    testing the frontier service.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 前沿服务的代码位于 `chapter8-9/frontier-service` 目录中。此目录包含一个 `serverless.yml` 文件，声明我们的
    API、函数和其他资源。我们将在部署和测试前沿服务之前解释其内容。
- en: 8.5.2 The frontier URL database
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.2 前沿 URL 数据库
- en: 'The frontier URL database stores all URLs that are intended to be fetched,
    have been fetched, or failed to have been fetched. The service is required to
    have an interface supporting the following actions:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 前沿 URL 数据库存储所有打算抓取、已抓取或未能抓取的 URL。服务需要有一个支持以下操作的接口：
- en: Insert a *seed URL*.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 插入一个*种子 URL*。
- en: Update the status of a URL to `PENDING`, `FETCHED`, or `FAILED`.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 URL 的状态更新为 `PENDING`、`FETCHED` 或 `FAILED`。
- en: Insert a batch of newly-discovered URLs (links) that have been deemed eligible
    for fetching.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 插入一批被认为适合抓取的新发现 URL（链接）。
- en: Get a set of URLs for a given seed URL, filtered by a status parameter and a
    maximum record limit.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据状态参数和最大记录限制获取给定种子 URL 的 URL 集合。
- en: The data model for our frontier database is illustrated by the example in table
    8.1.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们前沿数据库的数据模型通过表 8.1 中的示例进行说明。
- en: Table 8.1 Frontier URL database example
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.1 前沿 URL 数据库示例
- en: '| Seed | URL | Status | Depth |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 种子 | URL | 状态 | 深度 |'
- en: '| `http://microxchg.io` | `http://microxchg.io` | `FETCHED` | `0` |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| `http://microxchg.io` | `http://microxchg.io` | `FETCHED` | `0` |'
- en: '| `http://microxchg.io` | `http://microxchg.io/2019/index.html` | `FETCHED`
    | `1` |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| `http://microxchg.io` | `http://microxchg.io/2019/index.html` | `FETCHED`
    | `1` |'
- en: '| `http://microxchg.io` | `http://microxchg.io/2019/all-speakers.html` | `PENDING`
    | `2` |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| `http://microxchg.io` | `http://microxchg.io/2019/all-speakers.html` | `PENDING`
    | `2` |'
- en: '| `https://www.predictconference.com` | `https://www.predictconference.com`
    | `PENDING` | `0` |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| `https://www.predictconference.com` | `https://www.predictconference.com`
    | `PENDING` | `0` |'
- en: Our “primary key” in this instance is a combination of the seed and the URL.
    The *seed* attribute is the *partition key* or *hash*, whereas the *url* attribute
    is the *sort key* or *range*. This ensures we don’t insert duplicates into the
    database.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的“主键”是种子和 URL 的组合。*种子*属性是*分区键*或*哈希值*，而*url*属性是*排序键*或*范围键*。这确保我们不会将重复项插入数据库中。
- en: In addition to our table key, we will define a *secondary index*. This allows
    us to quickly search based on seed URL and status.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们的表键之外，我们还将定义一个*二级索引*。这允许我们根据种子 URL 和状态快速搜索。
- en: We can see from the sample data in table 8.1 that the full URL is included in
    the `url` field, not just the relative path. This allows us to support external
    URLs linked from the seed in the future, and saves us the inconvenience of having
    to reconstruct URLs when we go to fetch the content.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 从表8.1中的示例数据中我们可以看到，完整的URL包含在`url`字段中，而不仅仅是相对路径。这允许我们在将来支持从种子链接的外部URL，并避免了在获取内容时需要重建URL的不便。
- en: The DynamoDB table resource definition for the *frontier* table can be found
    in the service’s `serverless.yml` file, and is shown in the following listing.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 前沿表的DynamoDB资源定义可以在服务的`serverless.yml`文件中找到，如下所示。
- en: Listing 8.2 Frontier DynamoDB table definition
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.2前沿DynamoDB表定义
- en: '[PRE3]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ The table name is defined as frontier.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 表名定义为frontier。
- en: ❷ The table’s key is comprised of the seed and url attributes.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 表的键由seed和url属性组成。
- en: ❸ A secondary index, frontierStatus, is defined to allow queries to be run using
    the seed and status attributes.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义了一个二级索引，frontierStatus，允许使用seed和status属性进行查询。
- en: '❹ In this case, we choose provisioned throughput with five read and write capacity
    units. Alternatively, we could specify BillingMode: PAY_PER_REQUEST to deal with
    unpredictable loads.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '❹ 在这种情况下，我们选择配置吞吐量，具有五个读写容量单位。或者，我们可以指定BillingMode: PAY_PER_REQUEST来处理不可预测的负载。'
- en: Serverless databases
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 无服务器数据库
- en: We have already presented a few examples of working with DynamoDB. DynamoDB
    is a NoSQL database, suitable for unstructured document storage. It is possible,
    and sometimes very desirable, to model relational data in DynamoDB.ab In general,
    DynamoDB is more suitable when you have a clear picture of how the data will be
    accessed, and can design keys and indices to accommodate the access patterns.
    Relational databases are more suitable when you are storing structured data, but
    want to support arbitrary access patterns in the future. This is what Structured
    Query Language (SQL), the interface supported by an RDBMS, is very good at.[11](#pgfId-1099105)[12](#pgfId-1099108)
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了一些与DynamoDB一起工作的示例。DynamoDB是一种NoSQL数据库，适用于非结构化文档存储。在DynamoDB中建模关系数据是可能的，有时也是非常有必要的。一般来说，当您对数据的访问方式有清晰的了解，并且可以设计键和索引来适应访问模式时，DynamoDB更合适。当您存储结构化数据，但希望在将来支持任意访问模式时，关系型数据库更合适。这正是结构化查询语言（SQL），关系型数据库管理系统支持的接口，非常擅长的。[11](#pgfId-1099105)[12](#pgfId-1099108)
- en: Relational databases are optimized for a lower number of long-running connections
    from servers. As a result, a large number of short-lived connections from Lambda
    functions can result in poor performance. As an alternative to the *serverful*
    RDBMS, Amazon Aurora Serverless is a serverless relational database solution that
    avoids the need to provision instances. It supports auto-scaling and on-demand
    access, allowing you to pay per second of usage. It is also possible to run queries
    against Aurora Serverless with the Data API using the AWS SDK in a Lambda function
    ([http://mng.bz/ZrZA](http://mng.bz/ZrZA)). This solution avoids the problem of
    creating short-lived database connections.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 关系型数据库针对从服务器发出的少量长时间运行的连接进行了优化。因此，来自Lambda函数的大量短暂连接可能会导致性能不佳。作为对*全服务器*关系型数据库管理系统（RDBMS）的替代方案，Amazon
    Aurora Serverless是一种无服务器关系型数据库解决方案，避免了需要配置实例的需求。它支持自动扩展和按需访问，允许您按秒计费。您还可以使用Lambda函数中的AWS
    SDK通过Data API对Aurora Serverless运行查询([http://mng.bz/ZrZA](http://mng.bz/ZrZA))。此解决方案避免了创建短暂数据库连接的问题。
- en: 8.5.3 Creating the frontier API
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.3 创建前沿API
- en: We have described the DynamoDB table that is central to the frontier service.
    We need a way to get URLs for conference sites into the system. Let’s now take
    a look at the API Gateway and Lambda functions that allow external services to
    interact with the frontier in order to achieve this.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经描述了前沿服务中至关重要的DynamoDB表。我们需要一种方法将会议网站的URL引入系统。现在让我们看看API Gateway和Lambda函数，这些函数允许外部服务与前沿交互，以实现这一目标。
- en: The APIs supported by the frontier service are shown in table 8.2.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 前沿服务支持的API列于表8.2中。
- en: Table 8.2 Frontier service APIs
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.2前沿服务API
- en: '| Path | Method | Lambda Function | Description |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 路径 | 方法 | Lambda函数 | 描述 |'
- en: '| `frontier-url/{seed}/{url}` | `POST` | `create` | Adds a URL for a seed |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| `frontier-url/{seed}/{url}` | `POST` | `create` | 为种子添加URL |'
- en: '| `frontier-url/{seed}` | `POST` | `create` | Adds a new seed |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| `frontier-url/{seed}` | `POST` | `create` | 添加新的种子 |'
- en: '| `frontier-url/{seed}/{url}` | `PATCH` | `update` | Updates the status of
    a URL |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| `frontier-url/{seed}/{url}` | `PATCH` | `update` | 更新URL的状态 |'
- en: '| `frontier-url` | `PUT` | `bulkInsert` | Creates a batch of URLs |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| `frontier-url` | `PUT` | `bulkInsert` | 创建一批URL |'
- en: '| `frontier-url/{seed}` | `GET` | `list` | Lists URLs for a seed by status,
    up to a specified maximum number of records |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| `frontier-url/{seed}` | `GET` | `list` | 根据状态列出种子URL，最多指定记录数 |'
- en: The definition for each API can be found in the `serverless.yml` configuration
    for `frontier-service`. This configuration also defines a Systems Manager Parameter
    Store variable for the service’s API. We are not using DNS for the API, so it
    cannot be discovered by other services using a known name. Instead, the API Gateway’s
    generated URL is registered in Parameter Store to be found by services with the
    correct IAM permissions.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 每个API的定义可以在`frontier-service`的`serverless.yml`配置文件中找到。此配置还定义了一个用于服务API的 Systems
    Manager Parameter Store变量。我们没有使用DNS来配置API，因此它不能通过已知名称被其他服务发现。相反，API Gateway生成的URL已注册在参数存储中，以便具有正确IAM权限的服务可以找到。
- en: For simplicity, all of our Lambda code is implemented in `handler.js`. It includes
    the logic to create and execute the DynamoDB SDK calls. If you take a look at
    this code, you’ll recognize much of it as being similar to our handlers from chapters
    4 and 5\. One significant difference is that we have introduced a library called
    *Middy* to alleviate much of the boilerplate. Middy is a middleware library that
    allows you to intercept calls to a Lambda before and after they are invoked, in
    order to perform common actions ([https://middy.js.org](https://middy.js.org)).
    A middleware is simply a set of functions that hook into the lifecycle of your
    event handler. You can use any of Middy’s built-in middlewares or any third-party
    middleware, or write your own.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们所有的Lambda代码都实现在了`handler.js`中。它包括创建和执行DynamoDB SDK调用的逻辑。如果您查看此代码，您会发现其中很多与第4章和第5章中的处理器相似。一个显著的区别是我们引入了一个名为*Middy*的库来减少模板代码。Middy是一个中间件库，允许您在Lambda被调用之前和之后拦截调用，以执行常见操作（[https://middy.js.org](https://middy.js.org)）。中间件简单来说是一组函数，它们会钩入您的事件处理器的生命周期。您可以使用Middy的任何内置中间件或任何第三方中间件，或者编写自己的。
- en: For our frontier handlers, we set up the Middy middleware as shown in the next
    listing.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的前端处理器，我们设置了如下一列表中的Middy中间件。
- en: Listing 8.3 Frontier handler middleware initialization
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.3 前端处理器中间件初始化
- en: '[PRE4]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Middy wraps the plain Lambda handler.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Middy包装了普通的Lambda处理器。
- en: ❷ lambda-logger-middleware logs requests and responses in a development environment.[13](#pgfId-1104470)
    We use it with the Pino logger, introduced in chapter 6.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ lambda-logger-middleware在开发环境中记录请求和响应。[13](#pgfId-1104470) 我们与第6章中引入的Pino日志记录器一起使用它。
- en: ❸ httpEventNormalizer adds default empty objects for queryStringParameters and
    pathParameters.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ httpEventNormalizer为queryStringParameters和pathParameters添加默认空对象。
- en: ❹ jsonBodyParser automatically parses the body and provides an object.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ jsonBodyParser自动解析体并提供一个对象。
- en: ❺ validator validates the input body and parameters against a JSON schema we
    define.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ validator验证输入体和参数与我们定义的JSON模式。
- en: ❻ cors automatically adds CORS headers to the response.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ cors自动向响应中添加CORS头。
- en: ❼ middy-autoproxy response converts simple JSON object responses to Lambda Proxy
    HTTP responses.[14](#pgfId-1097958)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ middy-autoproxy response将简单的JSON对象响应转换为Lambda Proxy HTTP响应。[14](#pgfId-1097958]
- en: ❽ httpErrorHandler handles errors that contain the properties statusCode and
    message, creating a matching HTTP response.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ httpErrorHandler处理包含statusCode和message属性的错误，创建相应的HTTP响应。
- en: This middleware configuration can easily be replicated across all our services
    to avoid common, repetitive Lambda boilerplate.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这种中间件配置可以轻松地复制到我们所有的服务中，以避免常见的重复的Lambda模板代码。
- en: 8.5.4 Deploying and testing the frontier
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.4 部署和测试前端
- en: 'The frontier service is configured with the `serverless-offline` and `serverless-dynamodb-local`
    plugins, as outlined in chapter 6\. As a result, we can run the API and Lambda
    functions with a DynamoDB environment locally. To get this up and running, we
    must install the DynamoDB database:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 前端服务配置了`serverless-offline`和`serverless-dynamodb-local`插件，如第6章所述。因此，我们可以在本地运行具有DynamoDB环境的API和Lambda函数。为了使这一切正常运行，我们必须安装DynamoDB数据库：
- en: '[PRE5]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `npm start` command kicks off the script to run our offline frontier service.
    By default, the API runs on `localhost` port 4000\. You can test the API from
    the command line using `cURL`:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`npm start`命令启动脚本以运行我们的离线前端服务。默认情况下，API运行在`localhost`的4000端口。您可以使用`cURL`从命令行测试API：'
- en: '[PRE6]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once you are satisfied that everything is working as expected locally, deploy
    the frontier to your AWS account:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你确认本地一切按预期工作，将前沿服务部署到你的AWS账户：
- en: '[PRE7]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can use the AWS command line or the Management Console to inspect the DynamoDB
    table and index that have been created. Then move on to the service where all
    the real work happens--the fetcher!
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用AWS命令行或管理控制台检查已创建的DynamoDB表和索引。然后转到所有实际工作发生的服务——检索器！
- en: 8.6 Building the fetcher to retrieve and parse web pages
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.6 构建检索器以检索和解析网页
- en: Now that we have a frontier service that can respond to requests for batches
    of URLs to be fetched, we are ready to implement our fetcher. The code for this
    service is in the `chapter8-9/fetch-service` directory. Figure 8.6 shows the physical
    architecture of the fetcher implementation and the order of steps it performs
    as it retrieves conference website pages.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一个可以响应批量URL检索请求的前沿服务，我们准备实现我们的检索器。此服务的代码位于`chapter8-9/fetch-service`目录中。图8.6显示了检索器实现的物理架构以及它在检索会议网站页面时执行的步骤顺序。
- en: '![](../Images/CH08_F06_Elger.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F06_Elger.png)'
- en: Figure 8.6 The fetcher implementation integrates with the parameter store, the
    frontier API, an embedded headless web browser, the item store, and the event
    bus.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 检索器实现与参数存储、前沿API、嵌入式无头网络浏览器、项目存储和事件总线集成。
- en: This service accepts *fetch* requests for a batch of URLs. The page retrieval,
    rendering, and parsing steps are executed for each URL in series.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 此服务接受对一批URL的*fetch*请求。对于每个URL，依次执行页面检索、渲染和解析步骤。
- en: Note We haven’t defined any Lambda triggers for the fetch handler. Rather than
    using an API Gateway or an asynchronous event, we are going to allow this handler
    to be invoked directly using the AWS Lambda SDK. This is a special case because
    our fetcher implementation results in a long-running Lambda, fetching multiple
    pages. An API Gateway would time out in 30 seconds maximum. An event-based trigger
    is unsuitable, as we want to have synchronous invocation from the scheduler.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们尚未为检索处理程序定义任何Lambda触发器。我们不会使用API网关或异步事件，而是将允许此处理程序直接通过AWS Lambda SDK调用。这是一个特殊情况，因为我们的检索器实现导致Lambda长时间运行，检索多个页面。API网关最多30秒就会超时。基于事件的触发器不合适，因为我们希望从调度器有同步调用。
- en: 8.6.1 Configuring and controlling a headless browser
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6.1 配置和控制无头浏览器
- en: The configuration of the service (`serverless.yml`) includes a new plugin, `serverless-plugin-chrome`
    ([https://github.com/adieuadieu/serverless-chrome](https://github.com/adieuadieu/serverless-chrome)),
    as shown in the following listing.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 服务的配置（`serverless.yml`）包括一个新插件，`serverless-plugin-chrome`（[https://github.com/adieuadieu/serverless-chrome](https://github.com/adieuadieu/serverless-chrome)），如下所示。
- en: Listing 8.4 The fetch service `serverless.yml` loads and configures the Chrome
    plugin
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.4 检索服务`serverless.yml`加载和配置Chrome插件
- en: '[PRE8]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ The plugin is specified in the serverless.yml plugins section. It results
    in the browser being opened before the handler is invoked.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在serverless.yml的插件部分指定了插件。它导致在调用处理程序之前打开浏览器。
- en: ❷ Browser command-line arguments are provided. To create a useful screenshot,
    we provide a resolution and hide any scrollbars.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 提供浏览器命令行参数。为了创建有用的截图，我们提供分辨率并隐藏任何滚动条。
- en: This plugin automatically installs the Google Chrome web browser in *headless*
    mode (i.e., with no user interface) when the Lambda function is loaded. We can
    then control the browser programatically using the `chrome-remote-interface` module
    ([https://github.com/cyrus-and/chrome-remote-interface](https://github.com/cyrus-and/chrome-remote-interface)).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 此插件在Lambda函数加载时自动以*无头*模式（即没有用户界面）安装Google Chrome网络浏览器。然后我们可以使用`chrome-remote-interface`模块（[https://github.com/cyrus-and/chrome-remote-interface](https://github.com/cyrus-and/chrome-remote-interface)）以编程方式控制浏览器。
- en: 8.6.2 Capturing page output
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6.2 捕获页面输出
- en: Our primary goal is to gather the HTML and links. The links will be processed
    by the strategy worker to determine whether or not they should be fetched. We
    capture a screenshot of the page so that we have the option to develop a front-end
    application with a better visualization of the fetched content.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要目标是收集HTML和链接。链接将由策略工作器处理，以确定是否应该检索它们。我们捕获页面截图，以便我们有开发前端应用并更好地可视化检索内容的选项。
- en: In figure 8.5 we showed a *parser* component in the crawler architecture. In
    our implementation, the parser is implemented as part of the fetcher. This is
    both a simplification and an optimization. In our fetcher, we already incurred
    the overhead of loading a web browser and having it parse and render the page.
    It is a very simple step to use the browser’s DOM API to query the page and extract
    links.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在图8.5中，我们在爬虫架构中展示了*解析器*组件。在我们的实现中，解析器作为fetcher的一部分实现。这既是一种简化也是一种优化。在我们的fetcher中，我们已经承担了加载网络浏览器并使其解析和渲染页面的开销。使用浏览器的DOM
    API查询页面并提取链接是一个非常简单的步骤。
- en: All of the browser interaction and extraction code is encapsulated in a Node.js
    module, `browser.js`. An extract is shown in the following listing.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 所有浏览器交互和提取代码都被封装在Node.js模块`browser.js`中。以下列表显示了提取示例。
- en: Listing 8.5 Browser module load function
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.5 Browser模块加载函数
- en: '[PRE9]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Load the correct URL and wait for the document to be loaded.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载正确的URL并等待文档加载。
- en: ❷ Query the page’s Document Object Model (DOM) to extract links using JavaScript.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用JavaScript查询页面的文档对象模型（DOM）以提取链接。
- en: ❸ Capture the page’s generated HTML.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 捕获页面生成的HTML。
- en: ❹ Grab the text from the page and any '<iframe>'s within.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从页面和任何`<iframe>`中获取文本。
- en: ❺ Create a screenshot image of the page.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 创建页面的截图图像。
- en: When the `browser` module’s `load` function is invoked with a URL, it performs
    the following actions.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 当`browser`模块的`load`函数被一个URL调用时，它执行以下操作。
- en: 8.6.3 Fetching multiple pages
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6.3 获取多个页面
- en: The fetch service’s Lambda handler accepts multiple URLs. The idea is to allow
    our Lambda function to load and process as many pages as feasible. We optimize
    these invocations so that all URLs sent to a `fetch` invocation are from the same
    seed URL. This increases the likelihood that they have similar content and can
    benefit from caching performed in the browser. URLs are fetched in sequence by
    our Lambda function. This behavior could easily be altered, adding support for
    parallel fetchers to optimize the process even further.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: fetch服务的Lambda处理程序接受多个URL。我们的想法是允许我们的Lambda函数尽可能多地加载和处理页面。我们优化这些调用，使得发送给`fetch`调用的所有URL都来自同一个种子URL。这增加了它们具有相似内容并从浏览器中执行的缓存中受益的可能性。我们的Lambda函数按顺序获取URL。这种行为很容易改变，添加对并行获取器的支持，以进一步优化过程。
- en: All links found on a page are published to our system’s event bus. This allows
    any other service subscribing to these events to react asynchronously. For our
    event bus, we are using CloudWatch Events. The fetch service publishes discovered
    links in batches of up to 10 (the CloudWatch limit), shown in the next listing.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 页面上找到的所有链接都会发布到我们的系统事件总线。这允许任何订阅这些事件的其它服务异步地做出反应。对于我们的事件总线，我们使用CloudWatch Events。fetch服务以最多10批（CloudWatch的限制）的形式发布发现的链接，如下面的列表所示。
- en: Listing 8.6 Generating CloudWatch events for discovered URLs
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.6 为发现的URL生成CloudWatch事件
- en: '[PRE10]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ We can only send 10 events at a time using the CloudWatch Events API, so we
    extract 10 and then process the rest recursively.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们一次只能使用CloudWatch Events API发送10个事件，所以我们提取10个，然后递归地处理剩余的。
- en: ❷ The Detail property is the JSON payload of the event.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ Detail属性是事件的JSON有效负载。
- en: ❸ We identify the origin of the event.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们识别事件的来源。
- en: ❹ The event type is used to match events on the receiving event in a CloudWatch
    rule.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用事件类型在CloudWatch规则中匹配接收事件。
- en: ❺ The event batch is sent using the CloudWatch Events API in the AWS SDK.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用AWS SDK中的CloudWatch Events API发送事件批。
- en: CloudWatch Events as an event bus
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: CloudWatch Events作为事件总线
- en: In chapter 2, we described messaging technology and the distinction between
    queue systems and pub/sub systems. For our “URL Discovered” messages, we would
    like a pub/sub model. This allows multiple subscribers to respond to such events,
    and does not make any assumptions about what they do. The approach aids us in
    our mission to reduce coupling between services.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2章中，我们描述了消息传递技术和队列系统与pub/sub系统之间的区别。对于我们的“URL发现”消息，我们希望有一个pub/sub模型。这允许多个订阅者对这类事件做出反应，并且不对他们做什么做出任何假设。这种方法有助于我们减少服务之间的耦合。
- en: 'In AWS, we have a few pub/sub options:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS中，我们有几种pub/sub选项：
- en: Simple Notification Service (SNS)
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单通知服务（SNS）
- en: Kinesis Streams, used in chapter 7
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第7章中使用的Kinesis Streams
- en: Managed Streaming for Kafka (MSK) for users already using Kafka
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已使用Kafka的用户可用的托管流式传输Kafka（MSK）
- en: DynamoDB Streams, a system that publishes changes to DynamoDB data, built on
    top of Kinesis Streams
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DynamoDB Streams，一个发布到DynamoDB数据的系统，建立在Kinesis Streams之上
- en: CloudWatch Events, a simple service requiring almost no setup
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CloudWatch Events，一个几乎不需要设置的简单服务
- en: CloudWatch Events has the advantage of requiring little setup. We don’t need
    to declare any topic or configure shards. We can just send events using the AWS
    SDK. Any service wishing to react to these events needs to create a CloudWatch
    rule to match incoming events and trigger the target. Examples of possible targets
    include SQS, Kinesis, and of course, Lambda.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: CloudWatch Events的优势在于需要很少的设置。我们不需要声明任何主题或配置分片。我们只需使用AWS SDK发送事件。任何希望对这些事件做出反应的服务都需要创建一个CloudWatch规则来匹配传入的事件并触发目标。可能的目标包括SQS、Kinesis，当然还有Lambda。
- en: For each successful page fetch, the Frontier URL Update API is invoked to mark
    the URL as `FETCHED`. Any failed page loads result in the URL being marked as
    `FAILED`.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次成功的页面抓取，都会调用Frontier URL更新API来标记URL为`FETCHED`。任何失败的页面加载都会导致URL被标记为`FAILED`。
- en: 8.6.4 Deploying and testing the fetcher
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6.4 部署和测试fetcher
- en: 'To deploy the fetcher to AWS, test locally. First, we install module dependencies:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 要将fetcher部署到AWS，先在本地测试。首先，我们安装模块依赖项：
- en: '[PRE11]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we use a serverless local invocation. Our local invocation will attempt
    to copy content to our item store S3 bucket. It will also publish events to CloudWatch
    events relating to links discovered in the page that is fetched. As a result,
    ensure your AWS credentials are configured either using the `AWS_` environment
    variables or using an AWS profile. Run the `invoke local` command, passing the
    test event provided with the Fetch service code:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用无服务器本地调用。我们的本地调用将尝试将内容复制到我们的项目存储S3桶中。它还将发布与抓取的页面中发现的链接相关的CloudWatch事件。因此，请确保您的AWS凭证已通过`AWS_`环境变量或使用AWS配置文件进行配置。运行`invoke
    local`命令，传递Fetch服务代码中提供的测试事件：
- en: '[PRE12]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You should see Google Chrome run and load a web page ([https://fourtheorem.com](https://fourtheorem.com)).
    On some platforms, the invocation may not exit even after it has completed, and
    may have to be killed manually. When the invocation is complete, you can navigate
    to the S3 bucket for the item store in the AWS Management Console. There you will
    find a single folder containing an HTML file and a screenshot. Download them and
    see the results of your excellent work so far! We are now ready to deploy to AWS:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到Google Chrome运行并加载了一个网页（[https://fourtheorem.com](https://fourtheorem.com)）。在某些平台上，即使调用完成后，调用也可能不会退出，可能需要手动终止。当调用完成后，您可以在AWS管理控制台中导航到项目存储的S3桶。在那里您将找到一个包含HTML文件和屏幕截图的单个文件夹。下载它们并查看您迄今为止的优秀工作成果！我们现在准备部署到AWS：
- en: '[PRE13]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 8.7 Determining the crawl space in a strategy service
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.7 策略服务中确定爬取空间
- en: 'The process of determining a crawl space in any web crawler is specific to
    the domain and application. In our scenario, we are making a number of assumptions
    that will simplify our crawl strategy:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何网络爬虫中确定爬取空间的过程是特定于领域和应用的。在我们的场景中，我们做出了一些假设，这将简化我们的爬取策略：
- en: The crawler follows local links only.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 爬虫只遵循本地链接。
- en: The crawl strategy for each *seed* is independent. We don’t require any handling
    of duplicate content across links found by crawling different seeds.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个种子策略的爬取策略是独立的。我们不需要处理通过爬取不同种子找到的链接之间的重复内容。
- en: Our crawl strategy obeys a crawl depth limit.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的爬取策略遵循爬取深度限制。
- en: Let’s explore the crawler service implementation. You can find the code in `chapter8-9/strategy-service`.
    The diagram in figure 8.7 presents the physical structure of this service.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索爬虫服务实现。您可以在`chapter8-9/strategy-service`中找到代码。图8.7展示了该服务的物理结构。
- en: '![](../Images/CH08_F07_Elger.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F07_Elger.png)'
- en: Figure 8.7 The strategy service implementation is tied to CloudWatch Events
    via SQS. It also integrates with the parameter store and the frontier API.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 策略服务实现通过SQS与CloudWatch Events相关联。它还集成了参数存储和前沿API。
- en: You can see that this service is quite simple. It handles a batch of events
    as shown in listing 8.7\. An extract from the `handler.js` can be found in `chapter8-9/strategy-service`.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到这个服务非常简单。它处理列表8.7中显示的事件批次。`handler.js`的摘录可以在`chapter8-9/strategy-service`中找到。
- en: Listing 8.7 Page crawl strategy
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.7 页面爬取策略
- en: '[PRE14]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Each record in the event is parsed to extract the link and the page where
    it was discovered.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 事件中的每条记录都会被解析以提取链接和发现它的页面。
- en: ❷ A Frontier record for the new page is created. It contains the referring page’s
    URL, the link text label, and the incremented crawl depth.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为新页面创建一个前沿记录。它包含引用页面的URL、链接文本标签和增加的爬取深度。
- en: ❸ Items that exceed the maximum crawl depth are excluded.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 超过最大爬取深度的项目将被排除。
- en: ❹ Items from a different domain are excluded.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 来自不同域的项目将被排除。
- en: ❺ The Frontier’s Bulk Insert API is called with eligible items using the Axios
    HTTP library.[15](#pgfId-1102527)
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用Axios HTTP库通过Frontier的Bulk Insert API调用合格的项目。[15](#pgfId-1102527)
- en: The events we have just processed were sent by the fetch service using the CloudWatch
    Events API. To understand how they are received by the strategy service, refer
    to figure 8.7 and the `serverless.yml` extract from `strategy-service`, shown
    in listing 8.8.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚处理的事件是由fetch服务使用CloudWatch Events API发送的。要了解它们是如何被策略服务接收的，请参考图8.7和`strategy-service`的`serverless.yml`摘录，如列表8.8所示。
- en: Listing 8.8 CloudWatch events received by strategy service via SQS
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.8 通过SQS接收到的策略服务CloudWatch事件
- en: '[PRE15]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ We define a SQS queue. This is the trigger for the handleDiscoveredUrls Lambda
    handler.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们定义了一个SQS队列。这是handleDiscoveredUrls Lambda处理器的触发器。
- en: ❷ The SQS queue is given a resource policy granting the CloudWatch Events service
    permission to send messages to the queue.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 给定SQS队列的资源策略，授予CloudWatch Events服务向队列发送消息的权限。
- en: ❸ A CloudWatch rule is defined to match events of a given pattern.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义了一个CloudWatch规则来匹配给定模式的事件。
- en: '❹ The rule matches events with DetailType: url-discovered.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '❹ 规则匹配DetailType: url-discovered类型的事件。'
- en: ❺ The SQS queue is specified as the target for the rule.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将SQS队列指定为规则的靶标。
- en: ❻ The body of the messages sent to the target is the message payload.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 发送到目标的消息体是消息负载。
- en: 'Let’s deploy the strategy service straight to AWS:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将策略服务直接部署到AWS：
- en: '[PRE16]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We are now ready to build the final part of the crawler.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以构建爬虫的最后一部分了。
- en: 8.8 Orchestrating the crawler with a scheduler
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.8 使用调度器编排爬虫
- en: 'The last component of the web crawler, the scheduler, is where the process
    of crawling a site starts and is tracked until the end. Designing this kind of
    process with a serverless mindset is challenging for anyone used to larger, monolithic
    architectures. In particular, for any given site, we need to enforce the following
    requirements:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 网络爬虫的最后一个组件，调度器，是爬取网站流程开始的地方，并一直跟踪到结束。对于习惯于更大、单体架构的人来说，以无服务器的心态设计这类流程具有挑战性。特别是，对于任何给定的网站，我们需要强制执行以下要求：
- en: A maximum number of concurrent fetches per site must be enforced.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须强制执行每个站点的最大并发抓取数。
- en: The process must wait a specified amount of time before proceeding to perform
    the next batch of fetches.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在执行下一批抓取之前，流程必须等待指定的时间。
- en: These requirements are related to *flow control*. It would be possible to achieve
    flow control using a purely event-driven approach. However, in order to make an
    effort to cluster requests to the same site within the same Lambda function, the
    architecture would already be reasonably complex and difficult to reason about.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这些要求与*流量控制*相关。使用纯事件驱动方法实现流量控制是可能的。然而，为了努力将同一Lambda函数内的同一站点的请求进行聚类，架构本身就已经相当复杂且难以理解。
- en: Before we address the challenge of flow control, make sure you have the code
    for this service ready to explore.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们解决流量控制挑战之前，确保你有这个服务的代码准备探索。
- en: 8.8.1 Grabbing the code
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.8.1 获取代码
- en: The scheduler service code can be found in `chapter8-9/scheduler-service`. In
    the `serverless.yml`, you will find a new plugin, `serverless-step-functions`.
    This introduces a new AWS service that will help us to orchestrate the crawling
    process.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 调度服务代码可以在`chapter8-9/scheduler-service`中找到。在`serverless.yml`中，你会找到一个新插件，`serverless-step-functions`。这引入了一个新的AWS服务，将帮助我们编排爬取过程。
- en: 8.8.2 Using Step Functions
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.8.2 使用Step Functions
- en: 'Our scheduler will implement flow control and orchestration of the process
    using an AWS Step Function. Step Functions have the following capabilities:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的调度器将使用AWS Step Function实现流程控制和过程编排。Step Functions具有以下功能：
- en: They can run for up to one year.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以运行长达一年。
- en: Step Functions integrate to many AWS services, including Lambda.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Step Functions与许多AWS服务集成，包括Lambda。
- en: Support is provided for wait steps, conditional logic, parallel task execution,
    failures, and retries.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持等待步骤、条件逻辑、并行任务执行、故障和重试。
- en: Step Functions are defined in JSON using a specific syntax called *Amazon States
    Language (ASL)*. The `serverless-step-function` plugin allows us to define ASL
    for our function in the serverless configuration file under the `stepFunctions`
    section. We are using YAML in our Serverless Framework configuration. This format
    is converted to JSON before the resources are created as part of the underlying
    CloudFormation stack. Figure 8.8 illustrates the flow of the scheduler Step Function.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: Step Functions 使用称为 *Amazon States Language (ASL)* 的特定语法在 JSON 中定义。`serverless-step-function`
    插件允许我们在 `stepFunctions` 部分下的 serverless 配置文件中定义我们的函数的 ASL。我们在 Serverless Framework
    配置中使用 YAML。在作为底层 CloudFormation 堆栈的一部分创建资源之前，此格式将转换为 JSON。图 8.8 阐述了调度器 Step Function
    的流程。
- en: We have already learned how the other component services are built. We covered
    the APIs and event handling they use to interact with the rest of the system.
    The end-to-end crawl process, as managed by the scheduler, has also been shown.
    In particular, the `Wait` and `Check Batch Count` steps in the process show how
    control flow is easily managed with a Step Function. The ASL code listing for
    the Step Function state machine is shown in listing 8.9.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了如何构建其他组件服务。我们介绍了它们用于与系统其他部分交互的 API 和事件处理。由调度器管理的端到端爬取过程也已经展示。特别是，过程中的
    `Wait` 和 `Check Batch Count` 步骤展示了如何使用 Step Function 容易地管理控制流。Step Function 状态机的
    ASL 代码列表显示在列表 8.9 中。
- en: '![](../Images/CH08_F08_Elger.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F08_Elger.png)'
- en: Figure 8.8 The scheduler is implemented as an AWS Step Function. It makes synchronous
    invocations to Lambdas defined within the scheduler service, as well as the fetch
    Lambda in the fetch service.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8 调度器作为 AWS Step Function 实现。它对调度器服务内定义的 Lambda 以及抓取服务中的抓取 Lambda 进行同步调用。
- en: Listing 8.9 The ASL for the scheduler service state machine
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.9 调度服务状态机的 ASL
- en: '[PRE17]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ The state machine invokes the putSeed Lambda to start the crawl process.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 状态机调用 `putSeed Lambda` 以启动爬取过程。
- en: ❷ A batch of URLs is retrieved using the getBatch Lambda function.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用 `getBatch Lambda` 函数检索一批 URL。
- en: ❸ The number of URLs in the batch is checked in a Choice state. This is an example
    of how simple flow control is implemented in a Step Function. If the count is
    zero, the state machine terminates with the Done state. Otherwise, it advances
    to the Fetch state.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在 Choice 状态中检查批次的 URL 数量。这是在 Step Function 中实现简单流程控制的示例。如果计数为零，状态机将使用完成状态终止。否则，它将前进到
    Fetch 状态。
- en: ❹ The fetch service’s Lambda, discovered using the parameter store, is invoked
    with the batch of URLs from the frontier.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用参数存储发现抓取服务的 Lambda，并使用来自前沿的 URL 批次进行调用。
- en: ❺ Once the fetch is complete, the state machine waits 30 seconds to ensure polite
    crawling behavior. The state machine then loops back to the Get URL Batch state
    to process more pages.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 一旦抓取完成，状态机等待 30 秒以确保礼貌的爬取行为。然后状态机返回到 Get URL Batch 状态以处理更多页面。
- en: 8.8.3 Deploying and testing the scheduler
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.8.3 部署和测试调度器
- en: Once we have deployed the scheduler, you should be able to initiate a crawl
    process for any given seed URL. Let’s begin!
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们部署了调度器，你应该能够为任何给定的种子 URL 启动爬取过程。让我们开始吧！
- en: '[PRE18]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We now have all services in place to run a crawl. A crawl process is initiated
    by starting a Step Function execution. This can be done using the AWS command
    line. First, we use the `list-state-machines` command to find the ARN of the `CrawlScheduler`
    Step Function:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经部署了所有服务以运行爬取过程。通过启动 Step Function 执行来启动爬取过程。这可以通过使用 AWS 命令行完成。首先，我们使用
    `list-state-machines` 命令来查找 `CrawlScheduler` Step Function 的 ARN：
- en: '[PRE19]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'An example of the output returned follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例显示了返回的输出：
- en: '[PRE20]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we start a State Machine execution by providing the ARN and passing JSON
    containing the seed URL:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过提供 ARN 并传递包含种子 URL 的 JSON 来启动状态机执行：
- en: '[PRE21]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As an alternative to using the CLI, we can start a Step Function execution in
    the AWS Management Console. Navigate to the Step Functions service in your browser
    and select the `CrawlScheduler` service. You should see something similar to the
    screen in figure 8.9.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 作为使用 CLI 的替代方案，我们可以在 AWS 管理控制台中启动 Step Function 执行。在浏览器中导航到 Step Functions 服务并选择
    `CrawlScheduler` 服务。你应该会看到类似于图 8.9 中的屏幕。
- en: '![](../Images/CH08_F09_Elger.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F09_Elger.png)'
- en: Figure 8.9 The Step Function view in the AWS Management Console allows you to
    start a new execution and view the progress of existing executions. You can also
    inspect or edit the ASL JSON from here.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 AWS管理控制台中的步骤函数视图允许您启动新的执行并查看现有执行的进度。您还可以从这里检查或编辑ASL JSON。
- en: Select *Start Execution*. From here, you can enter the JSON object to be passed
    to the start state. In our case, the JSON object requires one property--the URL
    of the site to be crawled. This is shown in figure 8.10.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 选择“启动执行”。从这里，您可以输入要传递给起始状态的JSON对象。在我们的例子中，JSON对象需要一个属性--要爬取的网站的URL。这如图8.10所示。
- en: '![](../Images/CH08_F10_Elger.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F10_Elger.png)'
- en: Figure 8.10 A crawl process can be started by providing the site URL in the
    Start Execution option on the Step Functions console.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 通过在步骤函数控制台上的“启动执行”选项中提供网站URL，可以启动爬取过程。
- en: Once the execution has started, the console will take you to the execution view.
    From here, you can see a very useful visualization of the progress of the Step
    Function execution, shown in figure 8.11\. By clicking on any state, you can see
    the input data, output data, and any error.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦执行开始，控制台将带您进入执行视图。从这里，您可以查看步骤函数执行进度的非常有用的可视化，如图8.11所示。通过点击任何状态，您可以看到输入数据、输出数据和任何错误。
- en: '![](../Images/CH08_F11_Elger.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F11_Elger.png)'
- en: Figure 8.11 The visual workflow in the Step Functions console allows users to
    monitor the progress of an execution.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 步骤函数控制台中的可视化工作流程允许用户监控执行的进度。
- en: When the Step Function execution is complete, take a look at the contents of
    the item store S3 bucket. You should find a nice collection of files relating
    the most important pages linked from the seed URL. An example of one page’s contents
    is shown in figure 8.12.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 当步骤函数执行完成后，查看项目存储S3存储桶的内容。您应该找到与种子URL链接的最重要页面相关的一组文件。一个页面的内容示例如图8.12所示。
- en: '![](../Images/CH08_F12_Elger.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F12_Elger.png)'
- en: Figure 8.12 The item store can be browsed from the S3 Console. This allows us
    to inspect the generated HTML and visually check the page’s screenshot.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 可以从S3控制台浏览项目存储。这允许我们检查生成的HTML并直观地检查页面的截图。
- en: This type of data, gathered from many conference websites, will form the basis
    for intelligent data extraction in chapter 9\. However, before you move on, take
    some time to navigate the components of the crawler in the AWS Console. Start
    with the Step Function, following each phase in order. Look at the CloudWatch
    logs for the fetch service, the strategy service, and the frontier. The flow of
    data and events should match the diagrams in figure 8.9, and this exercise should
    help to cement everything we have described in the chapter.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的数据，从许多会议网站上收集而来，将构成第9章中智能数据提取的基础。然而，在继续之前，花些时间在AWS控制台中导航爬虫的组件。从步骤函数开始，依次跟随每个阶段。查看fetch服务、策略服务和前哨站的CloudWatch日志。数据和事件的流程应与图8.9中的图示相匹配，这个练习应该有助于巩固我们在本章中描述的所有内容。
- en: In the next chapter, we will dive into extracting specific information from
    textual data using named entity recognition.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨使用命名实体识别从文本数据中提取特定信息。
- en: Summary
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The type of AI algorithm or managed service dictates what data is required for
    AI applications.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI算法类型或托管服务决定了AI应用所需的数据类型。
- en: If you don’t have the right data already, look at finding publicly available
    data sources or generating your own datasets.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您还没有合适的数据，考虑寻找公开可用的数据源或生成自己的数据集。
- en: Web crawlers and scrapers find and extract data from websites.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络爬虫和抓取器从网站上查找和提取数据。
- en: DynamoDB secondary indexes can be used to perform additional queries.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用DynamoDB辅助索引执行额外的查询。
- en: It is possible to architect and build a serverless application for web crawling,
    particularly for specific small sets of sites.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以设计和构建一个用于网络爬取的无服务器应用程序，特别是针对特定的网站小集合。
- en: Event-driven serverless systems can use CloudWatch events (or EventBridge) as
    an event bus.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要控制流的过程可以使用AWS步骤函数进行编排。
- en: Processes requiring control flow can be orchestrated using AWS Step Functions.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要控制流的过程可以使用AWS步骤函数进行编排。
- en: Warning Chapter 9 continues to build on this system and we provide instructions
    on how to remove the deployed resources at the end of Chapter 9\. If you are not
    planning on working on Chapter 9 for some time, please ensure that you fully remove
    all cloud resources deployed in this chapter in order to avoid additional charges!
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 警告第9章将继续构建这个系统，并在第9章末尾提供如何移除已部署资源的说明。如果您计划在一段时间内不处理第9章，请确保您完全移除本章中部署的所有云资源，以避免额外收费！
- en: '* * *'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '1.Gil Press, “Cleaning Big Data: Most Time-Consuming, Least Enjoyable Data
    Science Task, Survey Says,” Forbes, March 23, 2016, [https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says).'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 1.Gil Press，《清理大数据：调查称这是耗时最长、最不愉快的数据科学任务》，福布斯，2016年3月23日，[https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says).
- en: 2.Steve Lohr, “For Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights,”
    New York Times, August 17, 2014, [https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html](https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 2.史蒂夫·洛尔，《对于大数据科学家来说，“清洁工作”是洞察力的关键障碍》，纽约时报，2014年8月17日，[https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html](https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html).
- en: 3.This is an interesting challenge for the reader. The AWS Personalize service
    (available in Developer Preview at the time of writing) is a managed machine learning
    recommendation service that should be suitable for this application.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 3.这对读者来说是一个有趣的挑战。AWS Personalize服务（在撰写本文时处于开发者预览版）是一个托管机器学习推荐服务，应该适合这个应用。
- en: 4.[Ribeiro, Singh and Guestrin, “'`Why Should I Trust You?’ Explaining the Predictions
    of Any Classifier,” University of Washington, August 9, 2016, https://arxiv.org/pdf/1602.04938.pdf.](https://arxiv.org/pdf/1602.04938.pdf)
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 4.[Ribeiro, Singh 和 Guestrin, “'为什么我应该相信你？'解释任何分类器的预测”，华盛顿大学，2016年8月9日，https://arxiv.org/pdf/1602.04938.pdf.](https://arxiv.org/pdf/1602.04938.pdf)
- en: 5.[Ashley Rodriguez, “Microsoft’s AI millennial chatbot became a racist jerk
    after less than a day on Twitter,” Quarts, March 24, 2016, http://mng.bz/BED2.](http://mng.bz/BED2)
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 5.[Ashley Rodriguez，“微软的AI千禧年聊天机器人上线不到一天就在Twitter上变成了种族主义者”，Quarts，2016年3月24日，http://mng.bz/BED2.](http://mng.bz/BED2)
- en: 6.Tom Simonite, “AI and ‘Enormous Data’ Could Make Tech Giants Harder to Topple,”
    Wired, 13 July 2017, [http://mng.bz/dwPw](http://mng.bz/dwPw).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 6.Tom Simonite，《AI和“大量数据”可能使科技巨头更难被颠覆》，Wired，2017年7月13日，[http://mng.bz/dwPw](http://mng.bz/dwPw).
- en: 7.*Exploring Data Science*, John Mount and Nina Zumel, Manning Publications,
    2016, [https://www.manning.com/books/exploring-data-science](https://www.manning.com/books/exploring-data-science).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 7.*《探索数据科学》，John Mount 和 Nina Zumel，Manning Publications，2016年，[https://www.manning.com/books/exploring-data-science](https://www.manning.com/books/exploring-data-science).
- en: 8.[For more about robots.txt, see http://www.robotstxt.org.](http://www.robotstxt.org)
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 8.[有关robots.txt的更多信息，请参阅http://www.robotstxt.org.](http://www.robotstxt.org)
- en: 9.*Collective Intelligence in Action*, Satnam Alag, Manning Publications, 2019,
    [http://mng.bz/xrJX](http://mng.bz/xrJX).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 9.*《集体智慧实践》，Satnam Alag，Manning Publications，2019年，[http://mng.bz/xrJX](http://mng.bz/xrJX).
- en: '10.[The origin of the term undifferentiated heavy lifting is unclear. However,
    Jeff Bezos, CEO of Amazon, mentioned it in a speech at Web 2.0 Summit in 2006\.
    He said, “There is a huge amount of undifferentiated heavy lifting, which we call
    ‘muck,’ in between an idea and a successful product. We believe creating new products
    today is 70% muck and 30% new idea execution. We want to reverse that ratio.”
    (Source: Dave Kellogg, Web 2.0 Summit: Jeff Bezos, November 8, 2006, http://mng.bz/Az2x.](http://mng.bz/Az2x)'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 10.术语“无差别的重负荷”的起源尚不明确。然而，亚马逊首席执行官杰夫·贝索斯在2006年Web 2.0峰会上的演讲中提到了它。他说：“在想法和成功产品之间存在着大量的无差别的重负荷，我们称之为‘垃圾’，我们认为今天创造新产品有70%是垃圾，30%是新想法的实施。我们希望逆转这个比例。”（来源：Dave
    Kellogg，Web 2.0峰会：杰夫·贝索斯，2006年11月8日，http://mng.bz/Az2x.](http://mng.bz/Az2x)
- en: '11.[Amazon has an example of relational model in DynamoDB on their blog: http://mng.bz/RMGv.](http://mng.bz/RMGv)'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 11.亚马逊在其博客上有一个DynamoDB关系模型的示例：http://mng.bz/RMGv.](http://mng.bz/RMGv)
- en: '12.[Many advanced DynamoDB topics, including relational modeling, are covered
    in Rick Houlihan’s AWS re:Invent 2018 talk, “Amazon DynamoDB Deep Dive: Advanced
    Design Patterns for DynamoDB (DAT401),” https://www.youtube.com/watch?v=HaEPXoXVf2k.](https://www.youtube.com/watch?v=HaEPXoXVf2k)'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 12.[许多高级 DynamoDB 主题，包括关系建模，都在 Rick Houlihan 在 AWS re:Invent 2018 上的演讲中有所涉及，“Amazon
    DynamoDB 深入探讨：DynamoDB 的高级设计模式 (DAT401)，”视频链接：https://www.youtube.com/watch?v=HaEPXoXVf2k.](https://www.youtube.com/watch?v=HaEPXoXVf2k)
- en: 13.[https://github.com/eoinsha/lambda-logger-middleware](https://github.com/eoinsha/lambda-logger-middleware).
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 13.[https://github.com/eoinsha/lambda-logger-middleware](https://github.com/eoinsha/lambda-logger-middleware).
- en: 14.[https://www.npmjs.com/package/middy-autoproxyresponse](https://www.npmjs.com/package/middy-autoproxyresponse).
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 14.[https://www.npmjs.com/package/middy-autoproxyresponse](https://www.npmjs.com/package/middy-autoproxyresponse).
- en: 15.[https://github.com/axios/axios](https://github.com/axios/axios).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 15.[https://github.com/axios/axios](https://github.com/axios/axios).
