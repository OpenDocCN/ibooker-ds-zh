- en: Part 1\. The machine-learning workflow
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一部分\. 机器学习工作流程
- en: In this first part of the book, we introduce the basic machine-learning workflow.
    Each chapter covers one step of the workflow.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第一部分，我们介绍了基本的机器学习工作流程。每一章都涵盖工作流程的一个步骤。
- en: '[Chapter 1](kindle_split_011.html#ch01) introduces machine learning, what it’s
    useful for, and why you should be reading this book.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[第1章](kindle_split_011.html#ch01)介绍了机器学习，它的用途以及为什么你应该阅读这本书。'
- en: In [chapter 2](kindle_split_012.html#ch02), you’ll dive into the data-processing
    step of the basic ML workflow. You’ll look at common ways to clean up and extract
    value from real-world and messy data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](kindle_split_012.html#ch02)中，你将深入了解基本机器学习工作流程中的数据处理步骤。你将了解从现实世界和杂乱数据中清理和提取价值的一些常见方法。
- en: In [chapter 3](kindle_split_013.html#ch03), you’ll start building simple ML
    models as you learn about a few modeling algorithms and how they’re used in common
    implementations.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](kindle_split_013.html#ch03)中，当你学习一些建模算法及其在常见实现中的应用时，你将开始构建简单的机器学习模型。
- en: In [chapter 4](kindle_split_014.html#ch04), you’ll take a deeper look at our
    ML models to evaluate and optimize their performance.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](kindle_split_014.html#ch04)中，你将更深入地了解我们的机器学习模型，以评估和优化其性能。
- en: '[Chapter 5](kindle_split_015.html#ch05) is dedicated to basic feature engineering.
    Extracting features from data can be an extremely important part of building and
    optimizing the performance of an ML system.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[第5章](kindle_split_015.html#ch05)专门介绍基本特征工程。从数据中提取特征可能是构建和优化机器学习系统性能的一个极其重要的部分。'
- en: Chapter 1\. What is machine learning?
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1章\. 什么是机器学习？
- en: '*This chapter covers*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Machine-learning basics
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习基础知识
- en: Advantages of machine learning over traditional approaches
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习相较于传统方法的优点
- en: Overview of the basic machine-learning workflow
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本机器学习工作流程概述
- en: Overview of advanced methods for improving model performance
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高模型性能的高级方法概述
- en: In 1959, an IBM computer scientist named Arthur Samuel wrote a computer program
    to play checkers. Each board position was assigned a score based on its likelihood
    of leading to a win. At first, scores were based on a formula using factors such
    as the number of pieces on each side and the number of kings. It worked, but Samuel
    had an idea about how to improve its performance. He had the program play thousands
    of games against itself and used the results to refine the positional scoring.
    By the mid-1970s, the program had achieved the proficiency of a respectable amateur
    player.^([[1](#ch01fn01)])
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 1959年，一名名叫亚瑟·萨缪尔的IBM计算机科学家编写了一个程序来玩跳棋。每个棋盘位置都根据其赢得比赛的可能性分配一个分数。最初，分数是基于一个使用诸如每一边的棋子数量和王的数量等因素的公式的。它起作用了，但萨缪尔有一个关于如何提高其性能的想法。他让程序与自己玩成千上万场比赛，并使用结果来细化位置评分。到20世纪70年代中期，该程序已经达到了可敬的业余选手的水平。[1](#ch01fn01)
- en: ¹
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Jonathan Schaeffer, *One Jump Ahead: Computer Perfection at Checkers* (New
    York: Springer, 2009).'
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 约翰·施瓦茨，*一步之先：国际象棋的计算机完美*(纽约：斯普林格，2009)。
- en: Samuel had written a computer program that was able to improve its own performance
    through experience. It learned—and machine learning (ML) was born.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 萨缪尔编写了一个能够通过经验提高自身性能的计算机程序。它学习了——机器学习（ML）诞生了。
- en: The aim of this book isn’t to describe the gory mathematical details of machine-learning
    algorithms (although we’ll peel back a few layers of the onion to provide insight
    into the inner workings of the most common ones). Rather, the book’s primary purpose
    is to instruct non-experts on important aspects and common challenges when integrating
    machine learning into real-world applications and data pipelines. In this first
    chapter, we present a real business problem—reviewing loan applications—to demonstrate
    the advantages of using machine learning over some of the most common alternatives.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的目的不是描述机器学习算法的数学细节（尽管我们将剥去洋葱的一层，以提供对最常见算法内部工作原理的洞察）。相反，本书的主要目的是指导非专业人士在将机器学习集成到现实世界应用和数据管道时的重要方面和常见挑战。在本章中，我们提出一个真实的企业问题——审查贷款申请，以展示使用机器学习相较于一些最常见替代方案的优点。
- en: 1.1\. Understanding how machines learn
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1\. 理解机器学习是如何学习的
- en: When we talk about human learning, we distinguish between rote learning, or
    memorization, and true intelligence. Memorizing a telephone number or a set of
    instructions is undoubtedly learning. But when we say *learning*, we frequently
    mean something more.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论人类学习时，我们区分了死记硬背或记忆和真正的智慧。记住电话号码或一系列指令无疑是学习。但当我们说*学习*时，我们通常意味着更多的事情。
- en: When children play in groups, they observe how others respond to their actions.
    Their future social behaviors are informed by this experience. But they don’t
    rewind and replay their past. Rather, certain recognizable features of their interactions—playground,
    classroom, Mom, Dad, siblings, friends, strangers, adults, children, indoors,
    outdoors—provide clues. They assess each new situation based on the features it
    has in common with past situations. Their learning is more than gathering knowledge.
    They’re building what might be called *insight*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当孩子们在群体中玩耍时，他们会观察他人如何回应他们的行为。他们的未来社交行为将受到这种经验的影响。但他们不会重放和回放过去的经历。相反，他们互动中的一些可识别特征——操场、教室、妈妈、爸爸、兄弟姐妹、朋友、陌生人、成人、儿童、室内、室外——提供了线索。他们会根据新情况与过去情况共有的特征来评估每个新情况。他们的学习不仅仅是积累知识。他们正在构建可能被称为*洞察力*的东西。
- en: Imagine teaching a child the difference between dogs and cats by using flashcards.
    You show a card, the child makes a choice, and you place the card in one of two
    piles for right and wrong choices, respectively. As the child practices, his performance
    improves. Interestingly, it isn’t necessary to first teach the child techniques
    for cat and dog recognition. Human cognition has built-in classification mechanisms.
    All that’s needed are *examples*. After the child is proficient with the flashcards,
    he’ll be able to classify not only the images on the flashcards, but also most
    any cat or dog image, not to mention the real thing. This ability to *generalize*,
    to apply knowledge gained through training to new unseen examples, is a key characteristic
    of both human and machine learning.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下通过使用闪卡教孩子区分狗和猫。你出示一张卡片，孩子做出选择，然后你将卡片放入两个堆叠中，分别代表正确和错误的选择。随着孩子的练习，他的表现会提高。有趣的是，没有必要首先教孩子猫和狗识别的技术。人类的认知具有内置的分类机制。所需的一切只是*例子*。在孩子熟练掌握闪卡后，他不仅能够分类闪卡上的图像，还能分类大多数猫或狗的图像，更不用说真实的事物了。这种*概括*的能力，即将通过训练获得的知识应用于新的未见过的例子，是人类和机器学习的关键特征。
- en: Of course, human learning is far more sophisticated than even the most advanced
    machine-learning algorithms, but computers have the advantage of greater capacity
    to memorize, recall, and process data. Their experience comes in the form of historical
    data that’s processed—using the techniques described in this book—to create and
    optimize, through experience, algorithms that embody, if not true insight, at
    least the ability to generalize.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，人类的学习比最先进的机器学习算法还要复杂得多，但计算机在记忆、回忆和处理数据方面具有更大的优势。他们的经验以历史数据的形式出现，这些数据通过本书中描述的技术进行处理，通过经验创造和优化，至少能够体现概括的能力，即使不是真正的洞察力。
- en: Analogies between human and machine learning naturally bring to mind the term
    *artificial intelligence* (AI) and the obvious question, “What’s the difference
    between AI and machine learning?” There’s no clear consensus on this matter, but
    most (not all) agree that ML is one form of AI, and that AI is a far broader subject
    encompassing such areas as robotics, language processing, and computer vision
    systems. To increase the ambiguity even further, machine learning is being applied
    in many of these adjacent AI fields with increasing frequency. We can say that
    the discipline of machine learning refers to *a specific body of knowledge and
    an associated set of techniques*. It’s fairly clear what is, and what isn’t, machine
    learning, whereas the same can’t always be said for artificial intelligence. Paraphrasing
    Tom Mitchell’s often-cited definition, a computer program is said to learn if
    its performance of a certain task, as measured by a computable score, improves
    with experience.^([[2](#ch01fn02)])
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 人类学习和机器学习之间的类比自然让人想到术语*人工智能*（AI）以及一个明显的问题：“人工智能和机器学习有什么区别？”关于这个问题，并没有明确的共识，但大多数人（并非所有人）都同意机器学习是人工智能的一种形式，而人工智能是一个更广泛的主题，包括诸如机器人技术、语言处理和计算机视觉系统等领域。为了进一步增加这种模糊性，机器学习正在越来越多地应用于这些相邻的人工智能领域。我们可以这样说，机器学习学科指的是*一个特定的知识体系及其相关的一套技术*。对于什么是机器学习，什么不是机器学习，这一点相当清楚，而对于人工智能来说，情况则并非总是如此。用Tom
    Mitchell经常引用的定义来转述，如果一个计算机程序在执行某个任务时，其表现（通过可计算的分数来衡量）随着经验的积累而提高，那么我们就说这个程序是学习的。[^([[2](#ch01fn02)])
- en: ²
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tom Mitchell, *Machine Learning* (McGraw Hill, 1997), 2\. “A computer program
    is said to learn from experience E with respect to some class of tasks T and performance
    measure P, if its performance at tasks in T, as measured by P, improves with experience
    E.”
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Tom Mitchell, *Machine Learning* (McGraw Hill, 1997), 2\. “如果一个计算机程序在基于经验E对某些任务T和性能度量P的情况下，其T中任务的性能（通过P来衡量）随着经验E的积累而提高，那么我们就说这个程序从经验E中学习。”
- en: Kaggle, a machine-learning consultancy, ran a competition for the most accurate
    program for classifying whether images depicted a dog or cat.^([[3](#ch01fn03)])
    Competitors were provided 25,000 example images for training. Each was labeled
    to indicate the species depicted. After all the competitors had trained their
    algorithms, they were tested on their ability to classify 12,500 unlabeled test
    images.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle，一家机器学习咨询公司，举办了一场比赛，寻找最准确的用于分类图像是否描绘了狗或猫的程序。[^([[3](#ch01fn03)]) 竞赛者获得了25,000个示例图像用于训练。每个图像都被标记以指示所描绘的物种。在所有竞争者训练了他们的算法之后，他们被测试了在区分12,500个未标记测试图像的能力。
- en: ³
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Dogs vs. Cats” at [www.kaggle.com/c/dogs-vs-cats](http://www.kaggle.com/c/dogs-vs-cats).
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅“狗与猫”比赛，[www.kaggle.com/c/dogs-vs-cats](http://www.kaggle.com/c/dogs-vs-cats)。
- en: When we explain the Kaggle competition to people, they often respond by reflecting
    on the sorts of rules one might apply to accomplish dog and cat recognition. Cats’
    ears are triangular and stand up; dogs’ ears are floppy—but not always. Try to
    imagine how you might explain to a person who had never seen a dog or a cat how
    to tell the difference, without showing any examples.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们向人们解释Kaggle比赛时，他们通常会通过反思可能应用于完成狗和猫识别的规则来回应。猫的耳朵是三角形的，竖立着；狗的耳朵是松垂的——但并非总是如此。试着想象一下，如果你要向一个从未见过狗或猫的人解释如何区分它们，而又不能展示任何例子，你会怎么解释。
- en: People use a variety of methods involving shapes, colors, textures, proportions,
    and other features to learn, and to generalize, from examples. Machine learning
    also employs a variety of strategies, in various combinations, depending on the
    problem at hand.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 人们使用各种方法，包括形状、颜色、质地、比例和其他特征，从例子中学习和归纳。机器学习也采用各种策略，根据手头的问题以不同的组合方式应用。
- en: These strategies are embodied in collections of algorithms developed over the
    course of recent decades by academics and practitioners in disciplines ranging
    from statistics, computer science, robotics, and applied mathematics, to online
    search, entertainment, digital advertising, and language translation. They are
    diverse and have various strengths and weaknesses. Some of them are classifiers.
    Others predict a numeric measurement. Some measure the similarity or difference
    of comparable entities (for example, people, machines, processes, cats, dogs).
    What the algorithms have in common is learning from examples (experience) and
    the capacity to apply what they’ve learned to new, unseen cases—the ability to
    generalize.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些策略体现在近几十年来统计学、计算机科学、机器人学、应用数学等学科领域的学者和实践者开发的算法集合中，这些算法应用于在线搜索、娱乐、数字广告和语言翻译等领域。它们种类繁多，各有优缺点。其中一些是分类器。其他一些预测数值测量。还有一些测量可比实体（例如，人、机器、流程、猫、狗）的相似性或差异性。这些算法的共同之处在于从例子（经验）中学习，以及将所学知识应用于新的、未见过的案例——即泛化的能力。
- en: In the cats and dogs competition, during the learning phase, competitors’ programs
    tried over and over to perform correct classifications using many algorithms.
    In each of the millions of iterations of the learning process, the programs performed
    the classification, measured their results, and then adjusted the process ever
    so slightly, searching for incremental improvements. The winner classified 98.914%
    of the unseen test images correctly. That’s pretty good, considering the human
    error rate is around 7%. [Figure 1.1](#ch01fig01) illustrates the process. The
    machine-learning process analyzes labeled images and builds a model that is, in
    turn, used by the *recall* (prediction) process to classify unlabeled images.
    There’s one mislabeled cat in the example.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在猫狗竞赛中，在学习阶段，参赛者的程序反复尝试使用许多算法进行正确的分类。在学习过程的数百万次迭代中，程序执行分类，衡量结果，然后对过程进行微调，寻找增量改进。获胜者正确分类了98.914%的未见测试图像。考虑到人类错误率大约为7%，这相当不错。[图1.1](#ch01fig01)说明了这个过程。机器学习过程分析标记图像，并构建一个模型，该模型随后被*召回*（预测）过程用于对未标记图像进行分类。示例中有一个被错误标记的猫。
- en: Figure 1.1\. Machine-learning process for the cats and dogs competition
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.1\. 猫狗竞赛的机器学习过程
- en: '![](01fig01_alt.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig01_alt.jpg)'
- en: Please note that what we’ve described here is *supervised* machine learning,
    and it’s not the only type of ML. We discuss other types later.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在这里描述的是**监督**机器学习，它不是唯一类型的ML。我们将在后面讨论其他类型。
- en: Machine learning can be applied to a wide range of business problems, from fraud
    detection, to customer targeting and product recommendation, to real-time industrial
    monitoring, sentiment analysis, and medical diagnosis. It can take on problems
    that can’t be managed manually because of the huge amount of data that must be
    processed. When applied to large datasets, ML can sometimes find relationships
    so subtle that no amount of manual scrutiny would ever discover them. And when
    many such “weak” relationships are combined, they become strong predictors.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可以应用于广泛的商业问题，从欺诈检测到客户定位和产品推荐，再到实时工业监控、情感分析和医疗诊断。它可以处理由于必须处理的大量数据而无法手动管理的问题。当应用于大型数据集时，ML有时可以发现如此微妙的关联，以至于任何数量的手动审查都无法发现。而且当许多这样的“弱”关系结合在一起时，它们就变成了强大的预测者。
- en: The process of learning from data, and subsequently using the acquired knowledge
    to inform future decisions, is extremely powerful. Indeed, machine learning is
    rapidly becoming the engine that powers the modern data-driven economy.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据中学习的过程，以及随后使用所获得的知识来指导未来决策，是非常强大的。确实，机器学习正在迅速成为推动现代数据驱动经济的引擎。
- en: '[Table 1.1](#ch01table01) describes widely used supervised machine-learning
    techniques and some of their practical applications. This isn’t an exhaustive
    list, as the potential use cases could stretch across several pages.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[表1.1](#ch01table01)描述了广泛使用的监督机器学习技术和它们的一些实际应用。这不是一个详尽的列表，因为潜在的使用案例可能跨越几页。'
- en: Table 1.1\. Use cases for supervised machine learning, organized by the type
    of problem
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表1.1\. 按问题类型组织的监督机器学习用例
- en: '| Problem | Description | Example use cases |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 描述 | 示例用例 |'
- en: '| --- | --- | --- |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Classification | Determine the discrete class to which each individual belongs,
    based on input data | Spam filtering, sentiment analysis, fraud detection, customer
    ad targeting, churn prediction, support case flagging, content personalization,
    detection of manufacturing defects, customer segmentation, event discovery, genomics,
    drug efficacy |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 根据输入数据确定每个个体所属的离散类别 | 邮件过滤、情感分析、欺诈检测、客户广告定位、客户流失预测、支持案例标记、内容个性化、制造缺陷检测、客户细分、事件发现、基因组学、药物有效性
    |'
- en: '| Regression | Predict the real-valued output for each individual, based on
    input data | Stock-market prediction, demand forecasting, price estimation, ad
    bid optimization, risk management, asset management, weather forecasting, sports
    prediction |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 回归 | 根据输入数据预测每个个体的实值输出 | 股票市场预测、需求预测、价格估计、广告出价优化、风险管理、资产管理、天气预报、体育预测 |'
- en: '| Recommendation | Predict which alternatives a user would prefer | Product
    recommendation, job recruiting, Netflix Prize, online dating, content recommendation
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 推荐系统 | 预测用户可能偏好的替代方案 | 产品推荐、招聘、Netflix Prize、在线约会、内容推荐 |'
- en: '| Imputation | Infer the values of missing input data | Incomplete patient
    medical records, missing customer data, census data |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 填充 | 推断缺失输入数据的值 | 不完整的患者医疗记录、缺失的客户数据、人口普查数据 |'
- en: 1.2\. Using data to make decisions
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2\. 利用数据做出决策
- en: In the following example, we describe a real-world business problem that can
    benefit from a machine-learning approach. We’ll run through the various alternatives
    that are commonly used and demonstrate the advantages of the ML approach.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将描述一个可以从机器学习方法中受益的现实世界商业问题。我们将逐一介绍常用的各种替代方案，并展示机器学习方法的优势。
- en: Imagine that you’re in charge of a microlending company that provides loans
    to individuals who want to start small businesses in troubled communities. Early
    on, the company receives a few applications per week, and you’re able in a few
    days’ time to manually read each application and do the necessary background checks
    on each applicant to decide whether to approve each loan request. The schematic
    of this process is shown in [figure 1.2](#ch01fig02). Your early borrowers are
    pleased with your short turnaround time and personal service. Word of your company
    starts to spread.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你是一家微型贷款公司的负责人，该公司为想在困难社区开展小型生意的个人提供贷款。一开始，公司每周只收到几份申请，你能在几天内手动阅读每一份申请，并对每位申请者进行必要的背景调查，以决定是否批准每项贷款申请。这个过程的结构图显示在[图1.2](#ch01fig02)中。你的早期借款人对你的快速处理时间和个性化服务感到满意。你公司的名声开始传播。
- en: Figure 1.2\. The loan-approval process for the microlending example
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.2\. 微型贷款示例的贷款审批流程
- en: '![](01fig02.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![01fig02](01fig02.jpg)'
- en: As your company continues to gain popularity, the number of applicants begins
    to increase. Soon you’re receiving hundreds of applications per week. You try
    to stay up with the increased rate of applications by working extra hours, but
    the backlog of applications continues to grow. Some of your applicants grow weary
    of waiting and seek loans from your competitors. It’s obvious to you that manually
    processing each application by yourself isn’t a sustainable business process and,
    frankly, isn’t worth the stress.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你的公司继续获得人气，申请者的数量开始增加。很快，你每周就会收到数百份申请。你试图通过加班来跟上申请数量的增加，但申请的积压仍在继续增长。一些申请者因为等待而感到疲惫，并从你的竞争对手那里寻求贷款。对你来说，显然，手动处理每一份申请并不是一个可持续的商业流程，而且坦白说，这也不值得承受压力。
- en: So what should you do? In this section, you’ll explore several ways to scale
    up your application-vetting process to meet your increasing business needs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你应该怎么做呢？在本节中，你将探索几种扩大你的应用程序审核流程的方法，以满足你不断增长的业务需求。
- en: 1.2.1\. Traditional approaches
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.2.1\. 传统方法
- en: 'Let’s explore two traditional data analysis approaches as applied to the application-vetting
    process: manual analysis and business rules. For each approach, we’ll walk through
    the process of implementing the technique and highlight the ways in which it falls
    short of enabling you to build a scalable business.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨两种应用于申请审核流程的传统数据分析方法：手动分析和业务规则。对于每种方法，我们将介绍实施技术的过程，并强调它在帮助你构建可扩展业务方面的不足。
- en: Hire more analysts
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 聘请更多分析师
- en: You decide to hire another analyst to help you out. You aren’t thrilled with
    the idea of spending some of your profit on a new hire, but with a second person
    vetting applications, you can process roughly twice as many applications in the
    same amount of time. This new analyst allows you to flush out the application
    backlog within a week.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你决定再雇佣另一位分析师来帮助你。你并不喜欢将一部分利润用于新员工的费用，但有了第二个人来审核申请，你可以在相同的时间内处理大约两倍数量的申请。这位新分析师让你能够在一周内清理掉申请积压。
- en: 'For the first couple of weeks, the two of you stay up with demand. Yet the
    number of applications continues to grow, doubling within a month to 1,000 per
    week. To keep up with this increased demand, you now must hire two more analysts.
    Projecting forward, you determine that this pattern of hiring isn’t sustainable:
    all of your increased revenue from new loan applicants is going directly to your
    new hires instead of to more-critical areas such as your microlending fund. *Hiring
    more analysts as demand increases hinders the growth of your business*. Further,
    you find that the hiring process is lengthy and expensive, sapping your business
    of more of its revenue. Finally, each new hire is less experienced and slower
    at processing applications than the last, and the added stress of managing a team
    of individuals is wearing on you.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在最初的几周里，你和你的同事都加班加点以满足需求。然而，申请的数量仍在增长，一个月内翻倍，每周达到1,000份。为了跟上这种增长的需求，你现在必须再雇佣两名分析师。展望未来，你确定这种雇佣模式是不可持续的：所有来自新贷款申请人的增加收入都直接用于新员工，而不是更关键的领域，如你的小额贷款基金。*随着需求的增加而雇佣更多分析师阻碍了你的业务增长*。此外，你发现招聘过程漫长且昂贵，消耗了你的更多收入。最后，每位新员工的经验都比上一任少，处理申请的速度也慢，管理一个团队的个人压力也在消耗你的精力。
- en: Aside from the obvious disadvantage of increased cost, people bring all sorts
    of conscious and unconscious biases to the decision-making process. To ensure
    consistency, you might develop detailed guidelines for the approval process and
    implement an extensive training program for new analysts, but this adds still
    more cost and probably doesn’t eliminate the bias.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 除了增加成本这一明显的缺点外，人们在决策过程中都会带来各种各样的有意识和无意识偏见。为了确保一致性，你可能会为审批流程制定详细的指南，并为新分析师实施广泛的培训计划，但这仍然会增加更多的成本，并且可能无法消除偏见。
- en: Employ business rules
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用业务规则
- en: Imagine that of the 1,000 loans whose repayment date has passed, 70% were repaid
    on time. This is shown in [figure 1.3](#ch01fig03).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，在1,000笔还款日期已过的贷款中，有70%按时还款。这显示在[图1.3](#ch01fig03)中。
- en: Figure 1.3\. After a few months of business and 2,500 loan applications, 1,000
    were approved, of which 700 applicants repaid the loan on time and the other 300
    defaulted. This initial set of observed information is critical to start building
    automation into your loan-approval process.
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.3。经过几个月的业务和2,500笔贷款申请，有1,000笔获得批准，其中700位申请人在规定时间内偿还了贷款，另外300位违约。这一初步观察到的信息对于开始将自动化引入你的贷款审批流程至关重要。
- en: '![](01fig03.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](01fig03.jpg)'
- en: You’re now in a position to begin looking for trends between the applicant data
    and incidence of loan repayment. In particular, you perform a manual search for
    a set of filtering rules that produces a subset of “good” loans that were primarily
    paid on time. Through the process of manually analyzing hundreds of applications,
    you’ve gained extensive experience about what makes each application good or bad.^([[4](#ch01fn04)])
    Through some introspection and back-testing of loan repayment status, you’ve noticed
    a few trends in the credit background checks data:^([[5](#ch01fn05)])
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你开始寻找申请数据和贷款还款发生率之间的趋势。特别是，你手动搜索一组过滤规则，以产生一个主要是按时还款的“良好”贷款子集。通过手动分析数百份申请，你获得了关于什么使每份申请好或坏的广泛经验。[^([4](#ch01fn04))]
    通过一些自我反思和对贷款还款状态的回测，你注意到了信用背景审查数据中的几个趋势。[^([5](#ch01fn05))]
- en: ⁴
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You could also use statistical correlation techniques to determine which input
    data attributes are most strongly associated with the outcome event of loan repayment.
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你还可以使用统计相关性技术来确定哪些输入数据属性与贷款还款结果事件最强烈相关。
- en: ⁵
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this example, we use the German Credit Data dataset. You can download this
    data from [http://mng.bz/95r4](http://mng.bz/95r4).
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用了德国信贷数据集。你可以从[http://mng.bz/95r4](http://mng.bz/95r4)下载这些数据。
- en: Most borrowers with a credit line of more than $7,500 defaulted on their loan.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数信用额度超过7,500美元的借款人未能按时偿还贷款。
- en: Most borrowers who had no checking account repaid their loan on time.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数没有支票账户的借款人按时偿还了贷款。
- en: Now you can design a filtering mechanism to pare down the number of applications
    that you need to process manually through those two rules.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以设计一个过滤机制，通过这两条规则减少你需要手动处理的申请数量。
- en: Your first filter is to automatically reject any applicant with a credit line
    of more than $7,500\. Looking through your historical data, you find that 44 of
    the 86 applicants with a credit line of more than $7,500 defaulted on their loan.
    Roughly 51% of these high-credit-line applicants defaulted, compared to 28% of
    the rest. This filter seems like a good way to exclude high-risk applicants, but
    you realize that only 8.6% (86 out of 1,000) of your accepted applicants had a
    credit line that was so high, meaning that you’ll still need to manually process
    more than 90% of applications. You need to do more filtering to get that number
    down to something more manageable.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你的第一个过滤器是自动拒绝任何信用额度超过7,500美元的申请人。通过查看你的历史数据，你发现86名信用额度超过7,500美元的申请人中有44人违约。这些高信用额度申请人的违约率大约为51%，而其他人的违约率为28%。这个过滤器似乎是一个排除高风险申请人的好方法，但你意识到只有8.6%（1,000名接受申请中的86名）的接受申请人的信用额度如此之高，这意味着你仍然需要手动处理超过90%的申请。你需要进行更多的过滤以将这个数字降低到更易于管理的水平。
- en: Your second filter is to automatically accept any applicant who doesn’t have
    a checking account. This seems to be an excellent filter, as 348 of the 394 (88%)
    applicants without a checking account repaid their loans on time. Including this
    second filter brings the percentage of applications that are automatically accepted
    or rejected up to 45%. Thus, you need to manually analyze only roughly half of
    the new incoming applications. [Figure 1.4](#ch01fig04) demonstrates these filtering
    rules.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你的第二个过滤器是自动接受任何没有支票账户的申请人。这似乎是一个很好的过滤器，因为348名没有支票账户的394名（88%）申请人在规定时间内偿还了贷款。包括这个第二个过滤器，自动接受或拒绝的申请比例达到了45%。因此，你只需要手动分析大约一半的新到申请。[图1.4](#ch01fig04)展示了这些过滤规则。
- en: Figure 1.4\. Filtering new applications through two business rules enables you
    to reduce manual analysis to only 52% of the incoming applications.
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.4。通过两条业务规则过滤新申请，使你能够将手动分析减少到仅占 incoming applications 的52%。
- en: '![](01fig04.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](01fig04.jpg)'
- en: With these two business rules, you can scale your business up to twice the amount
    of volume without having to hire a second analyst, because you now need to manually
    accept or reject only 52% of new applications. Additionally, based on the 1,000
    applications with known outcome, you expect your filtering mechanism to erroneously
    reject 42 out of every 1,000 applications (4.2%) and to erroneously accept 46
    of every 1,000 applications (4.6%).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这两条业务规则，你可以在不雇佣第二个分析师的情况下将业务规模扩大到原来的两倍，因为你现在只需要手动接受或拒绝52%的新申请。此外，根据已知结果的1,000个申请，你预计你的过滤机制会错误地拒绝每1,000个申请中的42个（4.2%），并且错误地接受每1,000个申请中的46个（4.6%）。
- en: 'As business grows, you’d like your system to automatically accept or reject
    a larger and larger percentage of applications without increasing losses from
    defaults. To do this, you again need to add more business rules. You soon encounter
    several problems:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 随着业务的增长，你希望你的系统能够自动接受或拒绝越来越多的申请，而不会增加违约损失。为此，你再次需要添加更多的业务规则。很快你就会遇到几个问题：
- en: Manually finding effective filters becomes harder and harder—if not impossible—as
    the filtering system grows in complexity.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着过滤系统的复杂性增加，手动找到有效的过滤器变得越来越困难——如果不是不可能的话。
- en: The business rules become so complicated and opaque that debugging them and
    ripping out old, irrelevant rules becomes virtually impossible.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 业务规则变得如此复杂和透明度低，以至于调试它们和移除旧的不相关规则几乎是不可能的。
- en: The construction of your rules has no statistical rigor. You’re pretty sure
    that better “rules” can be found by better exploration of the data, but can’t
    know for sure.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的规则构建没有统计严谨性。你相当确信可以通过更好地探索数据找到更好的“规则”，但无法确定。
- en: As the patterns of loan repayment change over time—perhaps due to changes in
    the population of applicants—the system doesn’t adapt to those changes. To stay
    up to date, the system needs to be constantly adjusted.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着还款模式随时间变化——可能是因为申请人群体发生变化——系统没有适应这些变化。为了保持最新状态，系统需要不断调整。
- en: 'All these drawbacks can be traced to a single debilitating weakness in a business
    rules approach: the system doesn’t automatically learn from data.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些缺点都可以追溯到业务规则方法中的一个单一致命弱点：系统不会自动从数据中学习。
- en: Data-driven systems, from simple statistical models to more-sophisticated machine-learning
    workflows, can overcome these problems.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 数据驱动系统，从简单的统计模型到更复杂的机器学习工作流程，可以克服这些问题。
- en: 1.2.2\. The machine-learning approach
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.2.2\. 机器学习方法
- en: Finally, you decide to look into an entirely automated, data-driven approach
    to your microlending application-vetting process. Machine learning is an attractive
    option because the completely automated nature of the process will allow your
    operation to keep pace with the increasing inflow of applications. Further, unlike
    business rules, ML learns the optimal decisions *directly from the data* without
    having to arbitrarily hardcode decision rules. This graduation from rules-based
    to ML-based decision making means that your decisions will be more accurate and
    will improve over time as more loans are made. You can be sure that your ML system
    produces optimized decisions with minimal handholding.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你决定调查一种完全自动化、数据驱动的微贷款申请审核流程。机器学习是一个有吸引力的选择，因为过程的完全自动化将使你的运营能够跟上不断增长的申请流入。此外，与业务规则不同，机器学习直接从数据中学习最佳决策，而不需要任意地硬编码决策规则。从基于规则的决策到基于机器学习的决策的转变意味着你的决策将更加准确，并且随着时间的推移，随着更多贷款的发放，将得到改进。你可以确信你的机器学习系统会产生最小人工干预的优化决策。
- en: In machine learning, the data provides the foundation for deriving insights
    about the problem at hand. To determine whether to accept each new loan application,
    ML uses historical *training data* to predict the best course of action for each
    new application. To get started with ML for loan approval, you begin by assembling
    the training data for the 1,000 loans that have been granted. This training data
    consists of the input data for each loan application, along with the known outcome
    of whether each loan was repaid on time. The input data, in turn, consists of
    a set of *features*—numerical or categorical metrics that capture the relevant
    aspects of each application—such as the applicant’s credit score, gender, and
    occupation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，数据为推导关于当前问题的见解提供了基础。为了确定是否接受每个新的贷款申请，机器学习使用历史*训练数据*来预测每个新申请的最佳行动方案。要开始使用机器学习进行贷款审批，你首先需要组装1,000个已批准贷款的训练数据。这些训练数据包括每个贷款申请的输入数据，以及每个贷款是否按时偿还的已知结果。输入数据反过来又包括一组*特征*——数值或分类指标，它们捕捉了每个申请的相关方面——例如申请人的信用评分、性别和职业。
- en: In [figure 1.5](#ch01fig05) historical data trains the machine-learning model.
    Then, as new loan applications come in, predictions of the probability of future
    repayment are generated instantaneously from the application data.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图1.5](#ch01fig05)中，历史数据训练了机器学习模型。然后，随着新的贷款申请到来，从申请数据中即时生成未来还款概率的预测。
- en: Figure 1.5\. Basic ML workflow, as applied to the microloan example
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.5\. 微贷款示例中的基本机器学习工作流程
- en: '![](01fig05_alt.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](01fig05_alt.jpg)'
- en: ML modeling, then, determines how the input data for each applicant can be used
    to *best predict* the loan outcome. By finding and using patterns in the training
    set, ML produces a model (you can think of this as a black box, for now) that
    produces a prediction of the outcome for each new applicant, based on that applicant’s
    data.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，机器学习建模确定了如何使用每个申请人的输入数据来*最佳预测*贷款结果。通过在训练集中找到并使用模式，机器学习产生了一个模型（现在你可以将其视为一个黑盒），根据每个新申请人的数据预测每个新申请人的结果。
- en: 'The next step is to select an ML algorithm to use. Machine learning comes in
    many flavors, ranging from simple statistical models to more-sophisticated approaches.
    Here, we compare two examples: the first is a simple parametric model, and the
    second a nonparametric ensemble of classification trees. Don’t let the terminology
    scare you. Machine learning employs a lot of algorithms and lots of ways to categorize
    them, as you’ll soon see.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是选择一个要使用的机器学习算法。机器学习有多种风味，从简单的统计模型到更复杂的算法。在这里，我们比较两个例子：第一个是一个简单的参数模型，第二个是一个非参数的分类树集成。不要让术语吓到你。机器学习使用了很多算法和很多方法来分类它们，正如你很快就会看到的。
- en: Most traditional statistical business models fall into the first category. These
    parametric models use simple, fixed equations to express the relationship between
    the outcome and the inputs. Data is then used to learn the best values of the
    unknown terms in the equation. Approaches such as linear regression, logistic
    regression, and autoregressive models all fit under this category. Regression
    models are covered in more detail in [chapter 3](kindle_split_013.html#ch03).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数传统的统计商业模型都属于第一类。这些参数模型使用简单、固定的方程来表达结果和输入之间的关系。然后使用数据来学习方程中未知项的最佳值。线性回归、逻辑回归和自回归模型等都属于这一类别。回归模型在[第
    3 章](kindle_split_013.html#ch03)中有更详细的介绍。
- en: 'In this example, you could use logistic regression to model the loan-approval
    process. In logistic regression, the logarithm of the odds (the *log odds*) that
    each loan is repaid is modeled as a linear function of the input features. For
    example, if each new application contains three relevant features—the applicant’s
    credit line, education level, and age—then logistic regression attempts to predict
    the log odds that the applicant will default on the loan (we’ll call this y) via
    this equation:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，你可以使用逻辑回归来模拟贷款审批过程。在逻辑回归中，每笔贷款偿还的对数几率（对数几率）被模拟为输入特征的线性函数。例如，如果每个新的申请包含三个相关特征——申请人的信贷额度、教育水平和年龄——那么逻辑回归试图通过以下方程预测申请人将违约的几率（我们将称之为
    y）：
- en: '![](13fig01.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图片 13fig01](13fig01.jpg)'
- en: '|  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Log odds**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**对数几率**'
- en: 'The odds ratio is one way of expressing probability. You’ve undoubtedly heard
    someone say that a (favorite) team’s chance of winning is 3 to 1\. Odds are the
    probability of success (for example, winning) divided by the probability of failure
    (losing). Mathematically, this can be expressed as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 概率比是表示概率的一种方式。你无疑听过有人说过一支（热门）球队获胜的几率是 3 比 1。几率是成功（例如，获胜）的概率除以失败（失败）的概率。从数学上讲，这可以表示如下：
- en: Odds(A) = P(A) / P(~A) = The probability of A divided by the probability of
    not A
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 概率比（Odds(A)）= P(A) / P(~A) = A 发生的概率除以 A 不发生的概率
- en: So 3-to-1 odds is equivalent to 0.75 / 0.25 = 3 and log(3) = 0.47712...
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，3 比 1 的几率等同于 0.75 / 0.25 = 3，log(3) = 0.47712...
- en: If A were a fair coin toss, the odds of heads would be 0.5 / 0.5 = 1\. Log(1)
    = 0\. It turns out that the log(Odds) can take on any real-valued number. A log
    odds value near –∞ denotes a highly unlikely event. A value near ∞ indicates near
    certainty, and log(1) = 0 indicates an even random change. Using log-odds instead
    of regular probabilities is a mathematical trick that makes certain computations
    easier, because unlike probabilities, they’re not limited to values between 0
    and 1.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 A 是一个公平的抛硬币，那么正面的几率将是 0.5 / 0.5 = 1。log(1) = 0。结果发现，对数几率可以取任何实数值。接近 –∞ 的对数几率表示一个高度不可能的事件。接近
    ∞ 的值表示几乎确定，而 log(1) = 0 表示一个均匀的随机变化。使用对数几率而不是常规概率是一种数学技巧，因为它不像概率那样受限于 0 和 1 之间的值。
- en: '|  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The optimal values of each coefficient of the equation (in this case, β[0],
    β[1], β[2], and β[3]) are learned from the 1,000 training data examples.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 方程中每个系数（在本例中为 β[0]、β[1]、β[2] 和 β[3]）的最优值是从 1,000 个训练数据示例中学习的。
- en: When you can express the relationship between inputs and outputs in a formula
    like this one, predicting the output (y) from the inputs (credit line, education
    level, and age) is easy. All you have to do is figure out which values of β[1],
    β[2], and β[3] yield the best result when using your historical data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当你可以用这样的公式表达输入和输出之间的关系时，从输入（信贷额度、教育水平和年龄）预测输出（y）就变得容易了。你所要做的就是确定 β[1]、β[2] 和
    β[3] 的哪些值在使用你的历史数据时能产生最佳结果。
- en: But when the relationship between the inputs and the response are complicated,
    models such as logistic regression can be limited. Take the dataset in the left
    panel of [figure 1.6](#ch01fig06), for example. Here, you have two input features,
    and the task is to classify each data point into one of two classes. The two classes
    are separated in the two-dimensional feature space by a nonlinear curve, the *decision
    boundary* (depicted by the curve in the figure). In the center panel, you see
    the result of fitting a logistic regression model on this dataset. The logistic
    regression model comes up with a straight line that separates the two regions,
    resulting in many classification errors (points in the wrong region).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 但当输入和响应之间的关系复杂时，像逻辑回归这样的模型可能会受限。以[图1.6左面板](#ch01fig06)中的数据集为例。在这里，你有两个输入特征，任务是将每个数据点分类到两个类别之一。这两个类别在二维特征空间中由一条非线性曲线分开，称为*决策边界*（如图中的曲线所示）。在中间面板中，你可以看到在这个数据集上拟合逻辑回归模型的结果。逻辑回归模型提出了一条直线来分隔两个区域，导致许多分类错误（错误区域中的点）。
- en: Figure 1.6\. In this two-class classification, individual data points can belong
    to either the round class or the square class. This particular data lies in a
    two-dimensional feature space having a nonlinear decision boundary that separates
    the classes, denoted by the curve. Whereas a simple statistical model does quite
    poorly at accurately classifying the data (center), an ML model (right) is able
    to discover the true class boundary with little effort.
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.6。在这个二分类中，单个数据点可以属于圆形类别或方形类别。这个特定的数据位于一个具有非线性决策边界的二维特征空间中，该边界将类别分开，用曲线表示。而一个简单的统计模型在准确分类数据（中心）方面表现相当糟糕，而一个机器学习模型（右侧）则能够轻松地发现真实的类别边界。
- en: '![](01fig06_alt.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](01fig06_alt.jpg)'
- en: The problem here is that the model depicted in the center panel is attempting
    to explain a complicated, nonlinear phenomenon with a simple *parametric* model.
    The formal definition of parametric versus nonparametric models is complex and
    too mathematical for this book, but the gist is that parametric models work well
    when you have prior understanding of the relationship between your inputs and
    the response you’re trying to predict. If you know enough about the nonlinear
    relationship, you may be able to transform your inputs or response variables so
    that a parametric model will still work. For example, if the rate at which a certain
    disease is observed within a population is higher for older people, you might
    find a linear relationship between the probability of contracting the disease
    and the square of the subject’s age. But in the real world, you’re often presented
    with problems for which such transformations aren’t possible to guess.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题是，中间面板中描述的模型试图用一个简单的*参数*模型来解释一个复杂、非线性的现象。参数模型与非参数模型的正式定义很复杂，对于这本书来说过于数学化，但关键是参数模型在你对输入和你要预测的响应之间的关系有先验理解时表现良好。如果你对非线性关系了解足够多，你可能能够转换你的输入或响应变量，使得参数模型仍然有效。例如，如果某种疾病在人群中的观察率在老年人中更高，你可能会发现患病概率与受试者年龄的平方之间存在线性关系。但在现实世界中，你经常遇到无法猜测这种转换的问题。
- en: What you need are more flexible models that can automatically discover complex
    trends and structure in data without being told what the patterns look like. This
    is where *nonparametric* machine-learning algorithms come to the rescue. In the
    right-hand panel of [figure 1.6](#ch01fig06), you see the result of applying a
    nonparametric learning algorithm (in this case, a *random forest classifier*)
    to the problem. Clearly, the predicted decision boundary is much closer to the
    true boundary, and as a result, the classification accuracy is much higher than
    that of the parametric model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要的是更灵活的模型，这些模型可以自动发现数据中的复杂趋势和结构，而无需被告知这些模式看起来像什么。这就是*非参数*机器学习算法发挥作用的地方。在[图1.6右面板](#ch01fig06)中，你可以看到将一个非参数学习算法（在这种情况下，是一个*随机森林分类器*）应用于该问题的结果。显然，预测的决策边界与真实边界非常接近，因此，分类精度远高于参数模型。
- en: Because they attain such high levels of accuracy on complicated, high-dimensional,
    real-world datasets, nonparametric ML models are the approach of choice for many
    data-driven problems. Examples of nonparametric approaches include some of the
    most widely used methods in machine learning, such as k-nearest neighbors, kernel
    smoothing, support vector machines, decision trees, and ensemble methods. We describe
    all of these approaches later in the book, and the appendix provides an overview
    of some important algorithms. Linear algorithms have other properties that make
    them attractive in some cases, though. They can be easier to explain and reason
    about, and they can be faster to compute and scale to larger datasets.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们在复杂、高维、现实世界的数据集上达到了如此高的准确度，非参数机器学习模型成为许多数据驱动问题的首选方法。非参数方法的例子包括机器学习中一些最广泛使用的方法，如k最近邻、核平滑、支持向量机、决策树和集成方法。我们在本书的后面部分描述了所有这些方法，附录提供了某些重要算法的概述。线性算法还有一些其他特性，在某些情况下使它们具有吸引力。它们可能更容易解释和推理，并且可以更快地计算和扩展到更大的数据集。
- en: '|  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Further reading**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: The textbook *An Introduction to Statistical Learning* by Gareth James et al.
    (Springer, 2013) provides a detailed introduction to the most commonly used approaches
    in machine learning, at a level that’s accessible to readers without a background
    in statistics or mathematics. A PDF version is available on the author’s website
    ([www-bcf.usc.edu/~gareth/ISL/](http://www-bcf.usc.edu/~gareth/ISL/)).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Gareth James等人撰写的教科书《统计学习引论》（Springer，2013年）为没有统计学或数学背景的读者提供了对机器学习中常用方法的详细介绍。PDF版本可在作者网站上找到（[www-bcf.usc.edu/~gareth/ISL/](http://www-bcf.usc.edu/~gareth/ISL/))）。
- en: '|  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Returning to the microlending problem, the best choice for scaling up your business
    is to employ a nonparametric ML model. The model may find the exact same rules
    as those you initially found manually, but chances are that they’ll be slightly
    different in order to optimize the statistical gains. Most likely, the ML model
    will also automatically find other and deeper relationships between input variables
    and the desired outcome that you otherwise wouldn’t have thought about.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 回到微贷问题，扩大您业务的最佳选择是采用非参数机器学习模型。该模型可能会找到与您最初手动找到的完全相同的规则，但它们可能略有不同，以便优化统计收益。最有可能的是，机器学习模型还会自动找到其他更深层次的关系，这些关系是您在其他情况下不会考虑到的，即输入变量和期望结果之间的关系。
- en: 'In addition to providing an automated workflow, you may also attain higher
    accuracy, which translates directly to higher business value. Imagine that a nonparametric
    ML model yields 25% higher accuracy than a logistic regression approach. In this
    case, your ML model will make fewer mistakes on new applications: accepting fewer
    applicants who won’t repay their loan and rejecting fewer applicants who would
    have repaid their loan. Overall, this means a higher average return on the loans
    that you do make, enabling you to make more loans overall and to generate higher
    revenues for your business.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 除了提供自动化的工作流程外，您还可能获得更高的准确度，这直接转化为更高的商业价值。想象一下，一个非参数机器学习模型比逻辑回归方法高25%的准确度。在这种情况下，您的机器学习模型在新应用中犯的错误会更少：接受更少的不会偿还贷款的申请人，拒绝更少的会偿还贷款的申请人。总的来说，这意味着您发放的贷款的平均回报率更高，使您能够发放更多的贷款，并为您的业务创造更高的收入。
- en: We hope this gives you a taste of the power that machine learning can bring
    you. Before we move on to defining our basic machine-learning workflow, we’ll
    enumerate a few advantages of machine learning, as well as a few challenges with
    this approach.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这能让您领略到机器学习所能带来的力量。在我们继续定义我们的基本机器学习工作流程之前，我们将列举一些机器学习的优势以及这种方法的一些挑战。
- en: 1.2.3\. Five advantages to machine learning
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.2.3\. 机器学习的五个优势
- en: 'To wrap up our discussion of the microlending example, we list some of the
    most prominent advantages to using a machine-learning system, as compared to the
    most common alternatives of manual analysis, hardcoded business rules, and simple
    statistical models. The five advantages of machine learning are as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结我们对微贷示例的讨论，我们列出了一些使用机器学习系统相对于最常见替代方案（如手动分析、硬编码的业务规则和简单的统计模型）的一些最显著优势。机器学习的五个优势如下：
- en: '***Accurate—*** ML uses data to discover the optimal decision-making engine
    for your problem. As you collect more data, the accuracy can increase automatically.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***准确度高—*** 机器学习利用数据来发现针对您问题的最佳决策引擎。随着您收集更多数据，准确度可以自动提高。'
- en: '***Automated—*** As answers are validated or discarded, the ML model can learn
    new patterns automatically. This allows users to embed ML directly into an automated
    workflow.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***自动化——*** 当答案被验证或丢弃时，机器学习模型可以自动学习新的模式。这使用户能够将机器学习直接嵌入到自动化工作流程中。'
- en: '***Fast—*** ML can generate answers in a matter of milliseconds as new data
    streams in, allowing systems to react in real time.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***快速——*** 当新的数据流进来时，机器学习可以在毫秒内生成答案，允许系统实时反应。'
- en: '***Customizable—*** Many data-driven problems can be addressed with machine
    learning. ML models are custom built from your own data, and can be configured
    to optimize whatever metric drives your business.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***可定制——*** 许多数据驱动的问题可以用机器学习来解决。机器学习模型是根据您自己的数据定制的，并且可以配置以优化驱动您业务的各种指标。'
- en: '***Scalable—*** As your business grows, ML easily scales to handle increased
    data rates. Some ML algorithms can scale to handle large amounts of data on many
    machines in the cloud.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***可扩展——*** 随着业务的增长，机器学习可以轻松扩展以处理增加的数据速率。一些机器学习算法可以扩展到在云中的多台机器上处理大量数据。'
- en: 1.2.4\. Challenges
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.2.4\. 挑战
- en: Naturally, achieving these benefits involves a few challenges. Depending on
    the size and shape of the business problem, the degree of attendant difficulty
    ranges from child’s-play trivial to Hannibal-crossing-the-Alps colossal.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，实现这些好处涉及一些挑战。根据业务问题的规模和形状，伴随的难度程度从孩子玩耍般的简单到汉尼拔翻越阿尔卑斯山般的巨大不等。
- en: Most prominent is acquiring data in a usable form. It has been estimated that
    data scientists spend 80% of their time on data preparation.^([[6](#ch01fn06)])
    You’ve undoubtedly heard that businesses capture vastly greater quantities of
    data than ever before, and they do. You also may have heard this data referred
    to as the “exhaust” of business processes. In other words, our new treasure trove
    of data wasn’t designed to meet the input needs of our ML systems. Extracting
    useful data from the residue can be tedious and messy work.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最突出的是以可用的形式获取数据。据估计，数据科学家将80%的时间花在数据准备上.^([[6](#ch01fn06)]) 你无疑已经听说过，企业捕获的数据量比以往任何时候都要大，而且确实如此。你也可能听说过这些数据被称为业务流程的“废气”。换句话说，我们的新数据宝库并非旨在满足我们机器学习系统的输入需求。从残留物中提取有用的数据可能是繁琐和混乱的工作。
- en: ⁶
  id: totrans-131
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-132
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Steve Lohr, “For Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights,”
    *New York Times*, August 17, 2014, [http://mng.bz/7W8n](http://mng.bz/7W8n).
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 史蒂夫·洛尔，《大数据科学家面临的“清洁工工作”是洞察力的关键障碍》，《纽约时报》，2014年8月17日，[http://mng.bz/7W8n](http://mng.bz/7W8n)。
- en: 'A related challenge is formulating the problem so that machine learning can
    be applied, and will yield a result that’s actionable and measurable. In our example,
    the goal is clear: predict who will repay and who will default. The classification
    is easy to apply, and the outcome is easily measured. Fortunately, some real-world
    problems are this simple; for example, given everything we know about prospective
    customers (and we have a lot of data), predict whether they’ll purchase our product.
    This is low-hanging fruit.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相关的挑战是制定问题，以便机器学习可以应用，并将产生可操作和可衡量的结果。在我们的例子中，目标是明确的：预测谁会偿还，谁会违约。分类很容易应用，结果也容易衡量。幸运的是，一些现实世界的问题是这样的简单；例如，鉴于我们对潜在客户的了解（我们有很多数据），预测他们是否会购买我们的产品。这是低垂的果实。
- en: 'A more difficult example might be along these lines: find the optimum media
    mix and combination of advertising units to increase brand awareness for a new
    product line. Simply formulating the problem requires constructing a way of measuring
    brand awareness, an understanding of the alternative media options under consideration,
    and data that reflects pertinent experience with the alternatives and associated
    outcomes.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更困难的例子可能是这样的：找到最佳媒体组合和广告单元组合，以增加新产品线的品牌知名度。仅仅制定这个问题就需要构建一种衡量品牌知名度的方法，理解正在考虑的替代媒体选项，以及反映与替代方案和关联结果相关经验的数据。
- en: When the outcome you’re trying to predict is complicated, choosing the algorithm
    and how to apply it may be an enormous effort in itself. Cardiology researchers
    working to predict the likelihood of postoperative complications have a mind-boggling
    set of data for each patient, but ML algorithms don’t naturally slurp up electrocardiography
    (EKG) data and DNA sequences. *Feature engineering* is the process of transforming
    inputs such as these into predictive features.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当你试图预测的结果很复杂时，选择算法以及如何应用它本身可能就是一项巨大的努力。致力于预测术后并发症可能性的心脏病研究人员，对于每位患者都有一套令人眼花缭乱的资料，但机器学习算法并不自然地吸收心电图（EKG）数据和DNA序列。*特征工程*是将这些输入转换为预测特征的过程。
- en: 'We’d be remiss if we didn’t mention the bane of the predictive modeler’s existence:
    a model that fits the training data perfectly, but falls flat on its face when
    it’s used to do real predictions on data that isn’t in the training set. The problem
    is most often *overfitting*.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不提及预测模型师存在的痛苦：一个完美匹配训练数据的模型，但在使用它对不在训练集中的数据进行实际预测时却完全失败。这个问题最常见的是*过拟合*。
- en: You’ll see that machine learning can solve a great variety of problems, some
    much more easily than others. You may also notice that the value of the solution
    isn’t always proportional to the effort required. And indeed, ML isn’t a silver
    bullet for any problem. But as you’ll see in this book, machine learning is the
    perfect choice for many real-world, data-driven problems.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到机器学习可以解决各种各样的问题，有些问题比其他问题更容易解决。你也可能注意到，解决方案的价值并不总是与所需的努力成比例。确实，机器学习并不是任何问题的万能药。但正如你将在本书中看到的，机器学习是许多现实世界、数据驱动问题的完美选择。
- en: '1.3\. Following the ML workflow: from data to deployment'
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3. 按照机器学习工作流程：从数据到部署
- en: 'In this section, we introduce the main workflow for integrating machine-learning
    models into your applications or data pipelines. The *ML workflow* has five main
    components: data preparation, model building, evaluation, optimization, and predictions
    on new data. The application of these steps has an inherent order, but most real-world
    machine-learning applications require revisiting each step multiple times in an
    iterative process. These five components are detailed in [chapters 2](kindle_split_012.html#ch02)
    through [4](kindle_split_014.html#ch04), but we outline them in this introduction
    to whet your appetite for getting started. [Figure 1.7](#ch01fig07) outlines this
    workflow, and the following sections introduce these concepts from top to bottom.
    You’ll see this figure a lot throughout the book as we introduce the various components
    of the ML workflow.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了将机器学习模型集成到你的应用程序或数据管道中的主要工作流程。*机器学习工作流程*有五个主要组成部分：数据准备、模型构建、评估、优化以及在新的数据上的预测。这些步骤的应用有一个固有的顺序，但大多数现实世界的机器学习应用都需要在迭代过程中多次回顾每个步骤。这五个组成部分在[第2章](kindle_split_012.html#ch02)到[第4章](kindle_split_014.html#ch04)中详细说明，但我们在本介绍中概述它们，以激发你开始学习的兴趣。[图1.7](#ch01fig07)概述了此工作流程，接下来的几节将自上而下介绍这些概念。你将在本书中多次看到这张图，因为我们介绍了机器学习工作流程的各个组成部分。
- en: Figure 1.7\. The workflow of real-world machine-learning systems. From historical
    input data you can build a model using an ML algorithm. You then need to evaluate
    the performance of the model, and optimize accuracy and scalability to fit your
    requirements. With the final model, you can make predictions on new data.
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.7. 现实世界机器学习系统的工作流程。从历史输入数据中，你可以使用机器学习算法构建一个模型。然后你需要评估该模型的表现，并优化准确性和可扩展性以适应你的需求。使用最终模型，你可以在新的数据上进行预测。
- en: '![](01fig07.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](01fig07.jpg)'
- en: 1.3.1\. Data collection and preparation
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.1. 数据收集和准备
- en: Collecting and preparing data for machine-learning systems usually entails getting
    the data into a tabular format, if it’s not already. Think of the tabular format
    as a spreadsheet in which data is distributed in rows and columns, with each row
    corresponding to an *instance* or *example* of interest, and each column representing
    a measurement on this instance. A few exceptions and variations exist, but it’s
    fair to say that most machine-learning algorithms require data in this format.
    Don’t worry; you’ll deal with the exceptions as you encounter them. [Figure 1.8](#ch01fig08)
    shows a simple dataset in this format.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 收集和准备机器学习系统所需的数据通常意味着如果数据尚未以表格格式存在，则需要将其转换为表格格式。将表格格式想象成一个电子表格，其中数据按行和列分布，每一行对应一个感兴趣的*实例*或*例子*，每一列代表该实例的测量值。存在一些例外和变化，但可以说大多数机器学习算法都需要以这种格式提供数据。不用担心；当你遇到例外时，你会处理它们。[图1.8](#ch01fig08)展示了这种格式的一个简单数据集。
- en: Figure 1.8\. In a tabular dataset, rows are called *instances* and columns represent
    *features*.
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.8。在一个表格数据集中，行被称为*实例*，列代表*特征*。
- en: '![](01fig08_alt.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图1.8的替代文本](01fig08_alt.jpg)'
- en: 'The first thing to notice about tabular data is that individual columns usually
    include the same type of data, and rows typically include data of various types.
    In [figure 1.8](#ch01fig08), you can already identify four types of data: *Name*
    is a string variable, *Age* is an integer variable, *Income* is a floating-point
    variable, and *Marital status* is a categorical variable (taking on a discrete
    number of categories). Such a dataset is called *heterogeneous* (in contrast to
    homogeneous), and in [chapter 2](kindle_split_012.html#ch02) we explain how and
    why we’ll coerce some of these types of data into other types, depending on the
    particular machine-learning algorithm at hand.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 关于表格数据的第一件事要注意的是，各个列通常包含相同类型的数据，而行通常包含各种类型的数据。在[图1.8](#ch01fig08)中，你已能识别出四种类型的数据：*姓名*是一个字符串变量，*年龄*是一个整数变量，*收入*是一个浮点变量，而*婚姻状况*是一个分类变量（具有离散的类别数量）。这样的数据集被称为*异构的*（与同构相对），在[第2章](kindle_split_012.html#ch02)中我们解释了我们将如何以及为什么根据特定的机器学习算法将这些数据类型强制转换为其他类型。
- en: Real-world data can be “messy” in a variety of other ways. Suppose that a particular
    measurement is unavailable for an instance in the data-gathering phase, and there’s
    no way of going back to find the missing piece of information. In this case, the
    table will contain a *missing value* in one or more cells, and this can complicate
    both model building and subsequent predictions. In some cases, humans are involved
    in the data-gathering phase, and we all know how easy it is to make mistakes in
    repetitive tasks such as data recording. This can lead to some of the data being
    flat-out wrong, and you’ll have to be able to handle such scenarios, or at least
    know how well a particular algorithm behaves in the presence of misleading data.
    You’ll look closer at methods for dealing with missing and misleading data in
    [chapter 2](kindle_split_012.html#ch02).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 实际世界的数据可能以各种方式“杂乱无章”。假设在数据收集阶段，某个特定测量值对于数据中的一个实例不可用，并且无法返回去找到缺失的信息。在这种情况下，表格将包含一个或多个单元格中的*缺失值*，这可能会使模型构建和随后的预测复杂化。在某些情况下，人类参与了数据收集阶段，我们都知道在重复性任务，如数据记录中犯错误是多么容易。这可能导致一些数据完全错误，你必须能够处理这样的场景，或者至少知道特定算法在存在误导性数据时的表现如何。你将在[第2章](kindle_split_012.html#ch02)中更详细地了解处理缺失和误导性数据的方法。
- en: 1.3.2\. Learning a model from data
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.2. 从数据中学习模型
- en: The first part of building a successful machine-learning system is to ask a
    question that can be answered by the data. With this simple Person table, you
    could build an ML model that could predict whether a person is married or single.
    This information would be useful for showing relevant ads, for example.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个成功的机器学习系统的第一步是提出一个可以通过数据回答的问题。使用这个简单的个人信息表，你可以构建一个机器学习模型，可以预测一个人是已婚还是单身。例如，这样的信息对于展示相关广告将是有用的。
- en: In this case, you’d use the *Marital status* variable as the *target*, or *label*,
    and the remaining variables as *features*. The job of the ML algorithm will then
    be to find how the set of input features can successfully predict the target.
    Then, for people whose marital status is unknown, you can use the model to predict
    marital status based on the input variables for each individual. [Figure 1.9](#ch01fig09)
    shows this process on our toy dataset.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你会使用 *婚姻状况* 变量作为 *目标* 或 *标签*，其余变量作为 *特征*。然后，机器学习算法的任务就是找到如何使用输入特征集成功预测目标。然后，对于婚姻状况未知的人，你可以使用模型根据每个个体的输入变量来预测婚姻状况。[图
    1.9](#ch01fig09) 展示了在我们玩具数据集上的这一过程。
- en: Figure 1.9\. The machine-learning modeling process
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.9\. 机器学习建模过程
- en: '![](01fig09_alt.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![01fig09_alt.jpg](01fig09_alt.jpg)'
- en: At this point, think of the ML algorithm as a magical box that performs the
    mapping from input features to output data. To build a useful model, you’d need
    more than two rows. One of the advantages of machine-learning algorithms, compared
    with other widely used methods, is the ability to handle many features. [Figure
    1.9](#ch01fig09) shows only four features, of which the *Person* ID and *Name*
    probably aren’t useful in predicting marital status. Some algorithms are relatively
    immune to uninformative features, whereas others may yield higher accuracy if
    you leave those features out. [Chapter 3](kindle_split_013.html#ch03) presents
    a closer look at types of algorithms and their performance on various kinds of
    problems and datasets.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，将机器学习算法想象成一个神奇的盒子，它执行从输入特征到输出数据的映射。为了构建一个有用的模型，你需要不止两行。与其他广泛使用的方法相比，机器学习算法的一个优点是能够处理许多特征。[图
    1.9](#ch01fig09) 仅显示了四个特征，其中 *Person* ID 和 *Name* 可能对预测婚姻状况没有帮助。一些算法对无信息特征相对免疫，而其他算法如果省略这些特征可能会得到更高的准确率。[第
    3 章](kindle_split_013.html#ch03) 更详细地介绍了算法类型及其在各种问题和数据集上的性能。
- en: It’s worth noting, however, that valuable information can sometimes be extracted
    from seemingly uninformative features. A location feature may not be informative
    in itself, for example, but can lead to informative features such as population
    density. This type of data enhancement, called *feature extraction*, is important
    in real-world ML projects and is the topic of [chapters 5](kindle_split_015.html#ch05)
    and [7](kindle_split_018.html#ch07).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，值得注意的是，有时可以从看似无信息特征中提取出有价值的信息。例如，位置特征本身可能没有信息，但它可以导致如人口密度这样的信息特征。这种类型的数据增强，称为
    *特征提取*，在现实世界的机器学习项目中非常重要，也是 [第 5 章](kindle_split_015.html#ch05) 和 [第 7 章](kindle_split_018.html#ch07)
    的主题。
- en: With our ML model in hand, you can now make predictions on new data—data for
    which the target variable is unknown. [Figure 1.10](#ch01fig10) shows this process,
    using the magic-box model built in [figure 1.9](#ch01fig09).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了机器学习模型，现在可以对新数据进行预测——这些数据的目标变量是未知的。[图 1.10](#ch01fig10) 展示了这一过程，使用的是 [图
    1.9](#ch01fig09) 中构建的神奇盒子模型。
- en: Figure 1.10\. Using the model for prediction on new data
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 1.10\. 使用模型对新数据进行预测
- en: '![](01fig10_alt.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![01fig10_alt.jpg](01fig10_alt.jpg)'
- en: 'The target predictions are returned in the same form as they appeared in the
    original data used to learn the model. Using the model to make predictions can
    be seen as filling out the blank target column of the new data. Some ML algorithms
    can also output the probabilities associated with each class. In our married/single
    example, a *probabilistic* ML model would output two values for each new person:
    the probability of this person being married and the probability of the person
    being single.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 目标预测以与用于学习模型的原数据相同的形式返回。使用模型进行预测可以看作是填写新数据的目标列的空白。一些机器学习算法还可以输出与每个类别相关的概率。在我们的已婚/单身示例中，一个
    *概率性* 机器学习模型会为每个新的人输出两个值：这个人已婚的概率和这个人单身的概率。
- en: We left out a few details on the way here, but in principle you’ve just architected
    your first ML system. Every machine-learning system is about building models and
    using those models to make predictions. Let’s look at the basic machine-learning
    workflow in pseudocode to get another view of how simple it is.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在途中省略了一些细节，但在原则上，你刚刚设计出了你的第一个机器学习系统。每个机器学习系统都是关于构建模型并使用这些模型进行预测。让我们用伪代码查看基本的机器学习工作流程，以获得另一个简单性的视角。
- en: Listing 1.1\. Initial structure of an ML workflow program
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 1.1\. 机器学习工作流程程序的初始结构
- en: '[PRE0]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Although we haven’t programmed any of these functions yet, the basic structure
    is in place. By [chapter 3](kindle_split_013.html#ch03), you’ll understand these
    steps; the rest of the book ([chapters 4](kindle_split_014.html#ch04) through
    [10](kindle_split_021.html#ch10)) is about making sure you’re building the best
    model for the problem at hand.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们还没有编写这些函数，但基本结构已经就位。到第3章（kindle_split_013.html#ch03）时，你会理解这些步骤；本书的其余部分（第4章（kindle_split_014.html#ch04）到第10章（kindle_split_021.html#ch10））是关于确保你为当前问题构建了最佳模型。
- en: 1.3.3\. Evaluating model performance
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.3\. 评估模型性能
- en: Rarely is an ML system put to use without some kind of validation of the performance
    of the model. Even though we’ve skipped a lot of details in this chapter, let’s
    pretend that you know how to build a model and make predictions. You can now apply
    a clever trick to get some sense of how well your model is working before you
    use it to predict on new data.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 很少有机器学习系统在没有对模型性能进行某种验证的情况下投入使用。尽管我们在这章中省略了很多细节，但让我们假装你知道如何构建模型并进行预测。现在，你可以使用一个巧妙的技巧来了解你的模型在用于预测新数据之前的工作效果。
- en: You take out some of the data and pretend that you don’t know the target variable.
    You then build a model on the remaining data and use the held-out data (testing
    data) to make predictions. [Figure 1.11](#ch01fig11) illustrates this model-testing
    process.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你取出一些数据，假装你不知道目标变量。然后你在剩余的数据上构建一个模型，并使用保留的数据（测试数据）进行预测。[图1.11](#ch01fig11)展示了这个模型测试过程。
- en: Figure 1.11\. When using a testing set to evaluate model performance, you “pretend”
    that the target variable is unknown and compare the predictions with the true
    values.
  id: totrans-167
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.11\. 当使用测试集来评估模型性能时，你“假装”目标变量是未知的，并将预测与真实值进行比较。
- en: '![](01fig11_alt.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图片](01fig11_alt.jpg)'
- en: Let’s also look at the pseudocode for this workflow.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看看这个工作流程的伪代码。
- en: Listing 1.2\. Our ML workflow program with model evaluation
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表1.2\. 我们的机器学习工作流程程序，包含模型评估
- en: '[PRE1]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can now compare the predicted results with the known “true” values to get
    a feeling for the accuracy of the model. In the pseudocode, this functionality
    is hidden behind the `compare_predictions` function, and most of [chapter 4](kindle_split_014.html#ch04)
    is dedicated to understanding how this function looks for various types of machine-learning
    problems.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以将预测结果与已知的“真实”值进行比较，以了解模型的准确性。在伪代码中，这个功能隐藏在`compare_predictions`函数后面，而本书的第4章（kindle_split_014.html#ch04）大部分内容都是关于理解这个函数如何针对各种机器学习问题进行查找。
- en: 1.3.4\. Optimizing model performance
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.4\. 优化模型性能
- en: 'The last piece of the essential machine-learning puzzle is also covered in
    [chapter 4](kindle_split_014.html#ch04): how to use the results of your model
    evaluation to go back and make the model better. You can achieve better model
    accuracy in three ways:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习基本问题的最后一部分也包含在第4章（kindle_split_014.html#ch04）中：如何使用模型评估的结果来改进模型。你可以通过以下三种方式提高模型准确性：
- en: '***Tuning the model parameters—*** ML algorithms are configured with parameters
    specific to the underlying algorithm, and the optimal value of these parameters
    often depends on the type and structure of the data. The value of each parameter,
    or any of them combined, can have an impact on the performance of the model. We
    introduce various ways to find and select the best parameter values, and show
    how this can help in determining the best algorithm for the dataset in question.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***调整模型参数—*** 机器学习算法配置了特定于底层算法的参数，这些参数的最佳值通常取决于数据的类型和结构。每个参数的值，或者任何组合的值，都可能影响模型的表现。我们介绍了寻找和选择最佳参数值的各种方法，并展示了这如何有助于确定针对特定数据集的最佳算法。'
- en: '***Selecting a subset of features—*** Many ML problems include a large number
    of features, and the noise from those features can sometimes make it hard for
    the algorithm to find the real signal in the data, even though they might still
    be informative on their own. For many ML problems, having a lot of data is a good
    thing; but it can sometimes be a curse. And because you don’t know beforehand
    when this will affect your model performance, you have to carefully determine
    the features that make up the most general and accurate model.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***选择特征子集—*** 许多机器学习问题包括大量的特征，而这些特征的噪声有时会使得算法难以在数据中找到真正的信号，尽管它们本身可能仍然是有信息的。对于许多机器学习问题，拥有大量数据是好事；但有时也可能成为诅咒。因为你事先不知道何时这会影响你的模型性能，你必须仔细确定构成最通用和最准确模型的特征。'
- en: '***Preprocessing the data—*** If you search the internet for machine-learning
    datasets, you’ll find easy-to-use datasets that many ML algorithms can be quickly
    applied to. Most real-world datasets, however, aren’t in such a clean state, and
    you’ll have to perform cleaning and processing, a process widely referred to as
    *data munging* or *data wrangling*. The dataset may include names that are spelled
    differently, although they refer to the same entity, or have missing or incorrect
    values, and these things can hurt the performance of the model. It may sound like
    edge cases, but you’ll be surprised how often this happens even in sophisticated,
    data-driven organizations.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***数据预处理——*** 如果你在网上搜索机器学习数据集，你会找到许多易于使用的、许多机器学习算法可以快速应用的数据集。然而，大多数现实世界的数据集并不是处于如此干净的状态，你将不得不进行清理和加工，这个过程通常被称为*数据整理*或*数据清洗*。数据集可能包括拼写不同的名称，尽管它们指的是同一个实体，或者包含缺失或错误的数据值，这些都会损害模型的性能。这听起来可能像是边缘情况，但你可能会惊讶，即使在复杂的数据驱动组织中，这种情况也经常发生。'
- en: With the machine-learning essentials in place, you’ll look briefly at more-advanced
    features in the next section before learning more details about the main components
    covered in this section.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习基础知识已经建立的基础上，你将在下一节中简要了解更高级的功能，然后再深入了解本节涵盖的主要组件的详细信息。
- en: 1.4\. Boosting model performance with advanced techniques
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4. 使用高级技术提升模型性能
- en: The previous section introduced the essential steps in any real-world machine-learning
    project, and now you’ll look at additional techniques often used to improve model
    performance even further. Depending on the data and problem at hand, some of these
    techniques can provide significant gains in accuracy, but sometimes at the cost
    of speed in both training and prediction. These techniques are explained in more
    detail in [chapters 5](kindle_split_015.html#ch05) through [10](kindle_split_021.html#ch10),
    but this section outlines the main ideas.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节介绍了任何现实世界机器学习项目中必不可少的步骤，现在你将了解一些常用于进一步提高模型性能的附加技术。根据数据和问题的不同，这些技术中的一些可以在准确度上带来显著的提升，但有时也会以训练和预测速度的降低为代价。这些技术将在第5章（[kindle_split_015.html#ch05](https://wiki.example.org/kindle_split_015.html#ch05)）至第10章（[kindle_split_021.html#ch10](https://wiki.example.org/kindle_split_021.html#ch10)）中更详细地解释，但本节将概述主要思想。
- en: 1.4.1\. Data preprocessing and feature engineering
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.4.1. 数据预处理和特征工程
- en: You’ll look at various kinds of data and how to deal with common types of messiness
    in [chapter 2](kindle_split_012.html#ch02). But in addition to this essential
    data cleaning, you can go a step further and extract additional value from the
    data that might improve your model performance.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在第2章（[kindle_split_012.html#ch02](https://wiki.example.org/kindle_split_012.html#ch02)）中了解各种类型的数据以及如何处理常见的混乱类型。但除了这项基本的数据清理之外，你还可以更进一步，从数据中提取额外的价值，这可能会提高你的模型性能。
- en: 'In any problem domain, specific knowledge goes into deciding the data to collect,
    and this valuable domain knowledge can also be used to extract value from the
    collected data, in effect adding to the features of the model before model building.
    We call this process *feature engineering*, and when the previously introduced
    essential ML workflow has become second nature to you, you can find yourself spending
    almost all your time in this part of the optimization process. This is also the
    creative part of machine learning, where you get to use your knowledge and imagination
    to come up with ways to improve the model by digging into the data and extracting
    hidden value. You’ll make extensive use of our statistically validated model evaluation
    and optimization steps to distinguish what seemed like a good idea at the time
    from what is actually useful. Here are a few important examples of feature engineering:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何问题域中，特定的知识用于决定要收集的数据，并且这种宝贵的领域知识也可以用来从收集的数据中提取价值，实际上是在模型构建之前增加了模型的特征。我们称这个过程为*特征工程*，当之前介绍的基本机器学习工作流程对你来说已经变得习以为常时，你可能会发现自己几乎把所有的时间都花在这个优化过程的这部分。这也是机器学习的创造性部分，在这里你可以利用你的知识和想象力，通过深入数据并提取隐藏的价值来想出改进模型的方法。你将广泛使用我们经过统计验证的模型评估和优化步骤来区分当时看似不错的主意和真正有用的东西。以下是一些特征工程的重要示例：
- en: '***Dates and times—*** You’ll see a date or time variable in many datasets,
    but by themselves they’re not useful for ML algorithms, which tend to require
    raw numbers or categories. The information might be valuable, though. If you want
    to predict which ad to show, it’ll certainly be important to know the time of
    day, the day of the week, and the time of year. With feature engineering, this
    information can be extracted from the dates and times and made available to the
    model. Also, when dates and times appear in observations of repetitive activity,
    such as a user’s repeated visits to a website over the course of a month or year,
    they can be used to compute interval durations that may be predictive. For example,
    on a shopping site, users might visit more frequently just prior to making a purchase
    to review and compare items and prices.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***日期和时间—*** 你会在许多数据集中看到日期或时间变量，但它们本身对机器学习算法来说并不有用，因为机器学习算法往往需要原始数字或类别。然而，这些信息可能是有价值的。如果你想预测要展示哪个广告，知道一天中的时间、一周中的哪一天以及一年中的哪个时间点肯定很重要。通过特征工程，可以从日期和时间中提取这些信息，并将其提供给模型。此外，当日期和时间出现在重复活动的观察中，例如用户在一个月或一年内重复访问网站时，它们可以用来计算可能具有预测性的间隔持续时间。例如，在购物网站上，用户可能在购买前更频繁地访问网站，以查看和比较商品和价格。'
- en: '***Location—*** Location data, such as latitude/longitude coordinates or location
    names, is available in some datasets. This information can sometimes be used in
    itself, but you may be able to extract additional information that’s useful for
    a specific problem. For example, if you want to predict election results in a
    county, you might want to extract the population density, mean income, and poverty
    rate to use as numbers in your model.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***位置—*** 位置数据，如经纬度坐标或位置名称，在某些数据集中可用。这种信息有时可以单独使用，但你可能能够提取对特定问题有用的额外信息。例如，如果你想预测一个县的选举结果，你可能想要提取人口密度、平均收入和贫困率作为模型中的数字。'
- en: '***Digital media—*** This is data such as text, documents, images, and video.
    The feature engineering that makes this kind of data usable is the difficult part
    of projects like the dogs and cats competition. Edges, shapes, and color spectra
    are first extracted from the images. Then these are classified using mathematical
    transformations, the output of which is a set of features usable by the classification
    algorithms.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***数字媒体—*** 这类数据包括文本、文档、图像和视频。使这类数据可用的特征工程是像狗和猫比赛这样的项目中的难点。首先从图像中提取边缘、形状和颜色光谱。然后使用数学变换对这些进行分类，其输出是一组可供分类算法使用的特征。'
- en: Hopefully it’s clear that feature engineering can be important for real-world
    ML projects. [Chapters 5](kindle_split_015.html#ch05) and [7](kindle_split_018.html#ch07)
    go into much more detail, introducing specific feature-engineering techniques;
    you’ll learn how these techniques feed into your ML workflow so your model performance
    improves without becoming too complex and prone to overfitting. [Figure 1.12](#ch01fig12)
    illustrates feature-engineering integration into the larger ML workflow introduced
    in [section 1.3](#ch01lev1sec3).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 希望很明显，特征工程对于现实世界的机器学习项目来说可能很重要。[第5章](kindle_split_015.html#ch05)和[第7章](kindle_split_018.html#ch07)将详细介绍，介绍特定的特征工程技术；你将了解这些技术如何融入你的机器学习工作流程，以便在不变得过于复杂和容易过拟合的情况下提高模型性能。[图1.12](#ch01fig12)说明了特征工程如何集成到在[第1.3节](#ch01lev1sec3)中引入的更大的机器学习工作流程中。
- en: Figure 1.12\. Feature-engineering phase inserted in the original ML workflow
  id: totrans-188
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.12\. 在原始机器学习工作流程中插入的特征工程阶段
- en: '![](01fig12.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig12.jpg)'
- en: 1.4.2\. Improving models continually with online methods
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.4.2\. 使用在线方法持续改进模型
- en: Most traditional ML models are static or only rarely rebuilt. But in many cases,
    you’ll have data and predictions flowing back into the system, and you want the
    model to improve with time and adapt to changes in the data. Several ML algorithms
    support this type of *online learning*; [chapter 8](kindle_split_019.html#ch08)
    introduces these algorithms and their potential pitfalls. [Figure 1.13](#ch01fig13)
    shows how continual relearning can be integrated into the ML workflow.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数传统的机器学习模型都是静态的，或者很少重建。但在许多情况下，你会有数据和预测反馈到系统中，你希望模型随着时间的推移而改进，并适应数据的变化。几种机器学习算法支持这种类型的*在线学习*；[第8章](kindle_split_019.html#ch08)介绍了这些算法及其潜在的风险。[图1.13](#ch01fig13)展示了如何将持续再学习集成到机器学习工作流程中。
- en: Figure 1.13\. In this flow of an online ML system, predictions are fed back
    to the model for iterative improvements.
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.13\. 在这个在线机器学习系统的流程中，预测被反馈到模型中进行迭代改进。
- en: '![](01fig13.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](01fig13.jpg)'
- en: 1.4.3\. Scaling models with data volume and velocity
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.4.3\. 随数据量和速度缩放模型
- en: It’s well known that datasets are increasing in size and velocity more quickly
    than ever. Datasets for supervised methods, in which the target answers are in
    the training set, have traditionally been relatively small because humans were
    needed in order to acquire the answers. Today, a lot of data (including answers)
    is produced directly by sensors, machines, or computers, and we’re beginning to
    see requirements for scalable ML algorithms in order to handle these data volumes.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，数据集的大小和速度比以往任何时候增长得都要快。由于需要人类来获取答案，因此传统的监督方法数据集相对较小。今天，大量数据（包括答案）直接由传感器、机器或计算机产生，我们开始看到需要可扩展的机器学习算法来处理这些数据量。
- en: '[Chapter 9](kindle_split_020.html#ch09) presents details of machine-learning
    methods that are capable of scaling with growing dataset sizes; you’ll see how
    they compare to each other and to nonscaling algorithms.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[第9章](kindle_split_020.html#ch09)介绍了能够随着数据集大小的增长而扩展的机器学习方法的细节；你将看到它们是如何相互比较的，以及与非扩展算法的比较。'
- en: 1.5\. Summary
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5\. 摘要
- en: 'This chapter introduced machine learning as a better, more data-driven approach
    to making decisions. The main points to take away from this chapter are as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了机器学习作为一种更好的、更数据驱动的决策方法。本章的主要要点如下：
- en: Machine-learning algorithms are distinguished from rule-based systems in that
    they create their own models based on data. Supervised ML systems generalize by
    learning from the features of examples with known results.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习算法与基于规则的系统不同，因为它们根据数据创建自己的模型。监督机器学习系统通过学习具有已知结果的示例的特征来泛化。
- en: Machine learning is often more accurate, automated, fast, customizable, and
    scalable than manually constructed rule-based systems.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习通常比手动构建的基于规则的系统更准确、自动化、快速、可定制和可扩展。
- en: Machine-learning challenges include identifying and formulating problems to
    which ML can be applied, acquiring and transforming data to make it usable, finding
    the right algorithms for the problem, feature engineering, and overfitting.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习挑战包括识别和制定机器学习可以应用的问题、获取和转换数据以使其可用、找到适合问题的正确算法、特征工程和过拟合。
- en: The basic machine-learning workflow consists of data preparation, model building,
    model evaluation, optimization, and predictions on new data.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本的机器学习工作流程包括数据准备、模型构建、模型评估、优化以及在新的数据上的预测。
- en: Online learning models continually relearn by using the results of their predictions
    to update themselves.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线学习模型通过使用其预测结果来更新自己，不断地重新学习。
- en: 1.6\. Terms from this chapter
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.6\. 本章术语
- en: '| Word | Definition |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 单词 | 定义 |'
- en: '| --- | --- |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| instance or example | A single object, observation, transaction, or record.
    |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 实例或示例 | 单个对象、观察、交易或记录。 |'
- en: '| target or label | The numerical or categorical (label) attribute of interest.
    This is the variable to be predicted for each new instance. |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 目标或标签 | 感兴趣的数值或分类（标签）属性。这是每个新实例要预测的变量。 |'
- en: '| features | The input attributes that are used to predict the target. These
    also may be numerical or categorical. |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 用于预测目标的输入属性。这些也可能是数值或分类的。 |'
- en: '| model | A mathematical object describing the relationship between the features
    and the target. |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 描述特征与目标之间关系的数学对象。 |'
- en: '| training data | The set of instances with a known target to be used to fit
    an ML model. |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 训练数据 | 包含已知目标以用于拟合机器学习模型的实例集。 |'
- en: '| recall | Using a model to predict a target or label. |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 回忆 | 使用模型来预测目标或标签。 |'
- en: '| supervised machine learning | Machine learning in which, given examples for
    which the output value is known, the training process infers a function that relates
    input values to the output. |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 监督机器学习 | 在已知输出值的示例的情况下，训练过程推断一个将输入值与输出值相关联的函数的机器学习。 |'
- en: '| unsupervised machine learning | Machine-learning techniques that don’t rely
    on labeled examples, but rather try to find hidden structure in unlabeled data.
    |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 非监督机器学习 | 不依赖于标记示例的机器学习技术，而是试图在未标记数据中找到隐藏的结构。 |'
- en: '| ML workflow | The stages in the ML process: data preparation, model building,
    evaluation, optimization, and prediction. |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 机器学习工作流程 | 机器学习过程中的阶段：数据准备、模型构建、评估、优化和预测。 |'
- en: '| online machine learning | A form of machine learning in which predictions
    are made, and the model is updated, for each new example. |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 在线机器学习 | 一种机器学习方法，其中对每个新示例进行预测，并更新模型。 |'
- en: In [chapter 2](kindle_split_012.html#ch02), you’ll get into the practical matters
    of collecting data, preparing it for machine learning use, and using visualizations
    to gain the insight needed to choose the best tools and methods.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](kindle_split_012.html#ch02)中，你将深入了解收集数据、为机器学习使用准备数据以及使用可视化来获得选择最佳工具和方法所需的洞察力。
- en: Chapter 2\. Real-world data
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2章\. 现实世界数据
- en: '*This chapter covers*'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Getting started with machine learning
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始机器学习
- en: Collecting training data
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集训练数据
- en: Using data-visualization techniques
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据可视化技术
- en: Preparing your data for ML
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据以供机器学习使用
- en: In supervised machine learning, you use data to teach automated systems how
    to make accurate decisions. ML algorithms are designed to discover patterns and
    associations in historical training data; they learn from that data and encode
    that learning into a model to accurately predict a data attribute of importance
    for new data. Training data, therefore, is fundamental in the pursuit of machine
    learning. With high-quality data, subtle nuances and correlations can be accurately
    captured and high-fidelity predictive systems can be built. But if training data
    is of poor quality, the efforts of even the best ML algorithms may be rendered
    useless.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督式机器学习中，你使用数据来教导自动化系统如何做出准确的决策。机器学习算法被设计用来发现历史训练数据中的模式和关联；它们从这些数据中学习，并将这种学习编码到模型中，以准确预测新数据的重要数据属性。因此，训练数据在机器学习的追求中是基本的。有了高质量的数据，细微的差别和相关性可以准确捕捉，并且可以构建高保真度的预测系统。但如果训练数据质量差，即使是最好的机器学习算法的努力也可能变得毫无用处。
- en: This chapter serves as your guide to collecting and compiling training data
    for use in the supervised machine-learning workflow ([figure 2.1](#ch02fig01)).
    We give general guidelines for preparing training data for ML modeling and warn
    of some of the common pitfalls. Much of the art of machine learning is in exploring
    and visualizing training data to assess data quality and guide the learning process.
    To that end, we provide an overview of some of the most useful data-visualization
    techniques. Finally, we discuss how to prepare a training dataset for ML model
    building, which is the subject of [chapter 3](kindle_split_013.html#ch03).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 本章作为你收集和整理训练数据以用于监督式机器学习工作流程（[图2.1](#ch02fig01)）的指南。我们提供了为机器学习建模准备训练数据的一般指南，并警告了一些常见的陷阱。机器学习的许多艺术在于探索和可视化训练数据以评估数据质量和指导学习过程。为此，我们概述了一些最有用的数据可视化技术。最后，我们讨论了如何准备用于机器学习模型构建的训练数据集，这是[第3章](kindle_split_013.html#ch03)的主题。
- en: Figure 2.1\. The basic ML workflow. Because this chapter covers data, we’ve
    highlighted the boxes indicating historical data and new data.
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1\. 基本的机器学习工作流程。因为本章涉及数据，所以我们突出显示了表示历史数据和新的数据的方框。
- en: '![](02fig01.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig01.jpg)'
- en: 'This chapter uses a real-world machine-learning example: *churn prediction*.
    In business, *churn* refers to the act of a customer canceling or unsubscribing
    from a paid service. An important, high-value problem is to predict which customers
    are likely to churn in the near future. If a company has an accurate idea of which
    customers may unsubscribe from their service, then they may intervene by sending
    a message or offering a discount. This intervention can save companies millions
    of dollars, as the typical cost of new customer acquisition largely outpaces the
    cost of intervention on churners. Therefore, a machine-learning solution to churn
    prediction—whereby those users who are likely to churn are predicted weeks in
    advance—can be extremely valuable.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用了一个真实的机器学习示例：*客户流失预测*。在商业中，*客户流失*指的是客户取消或取消订阅付费服务的行为。一个重要且价值高的任务是预测哪些客户可能在近期内流失。如果公司能够准确了解哪些客户可能会取消他们的服务，那么他们可以通过发送消息或提供折扣来干预。这种干预可以节省公司数百万美元，因为新客户获取的典型成本远超过对流失者的干预成本。因此，客户流失预测的机器学习解决方案——即提前几周预测可能流失的用户——可以非常有价值。
- en: 'This chapter also uses datasets that are available online and widely used in
    machine-learning books and documentation: Titanic Passengers and Auto MPG datasets.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还使用了在线可用且在机器学习书籍和文档中广泛使用的数据集：泰坦尼克号乘客和汽车每加仑英里数数据集。
- en: '2.1\. Getting started: data collection'
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 开始：数据收集
- en: 'To get started with machine learning, the first step is to ask a question that’s
    suited for an ML approach. Although ML has many flavors, most real-world problems
    in machine learning deal with *predicting a target variable (or variables) of
    interest*. In this book, we cover primarily these supervised ML problems. Questions
    that are well suited for a supervised ML approach include the following:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始学习机器学习，第一步是提出一个适合机器学习方法的疑问。尽管机器学习有许多风味，但大多数现实世界的机器学习问题都涉及*预测感兴趣的目标变量（或变量）*。在这本书中，我们主要涵盖这些监督式机器学习问题。适合监督式机器学习方法的疑问包括以下内容：
- en: Which of my customers will churn this month?
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本月哪些客户会流失？
- en: Will this user click my advertisement?
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个用户会点击我的广告吗？
- en: Is this user account fraudulent?
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个用户账户是否欺诈？
- en: Is the sentiment of this tweet negative, positive, or neutral?
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这条推文的情绪是负面、正面还是中性？
- en: What will demand for my product be next month?
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下个月我的产品需求量会是多少？
- en: You’ll notice a few commonalities in these questions. First, they all require
    making assessments on one or several instances of interest. These instances can
    be people (such as in the churn question), events (such as the tweet sentiment
    question), or even periods of time (such as in the product demand question).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到这些问题中存在一些共同点。首先，它们都需要对感兴趣的某个或某些实例进行评估。这些实例可以是人（例如，在流失问题中），事件（例如，推文情绪问题），甚至时间段（例如，在产品需求问题中）。
- en: Second, each of these problems has a well-defined target of interest, which
    in some cases is binary (churn versus not churn, fraud versus not fraud), in some
    cases takes on multiple classes (negative versus positive versus neutral), or
    even hundreds or thousands of classes (picking a song out of a large library)
    and in others takes on numerical values (product demand). Note that in statistics
    and computer science, the *target* is also commonly referred to as the *response*
    or *dependent variable*. These terms may be used interchangeably.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，这些问题的每一个都有一个明确的目标，在某些情况下是二元的（流失与否，欺诈与否），在某些情况下具有多个类别（负面、正面、中性），甚至数百或数千个类别（从大型图书馆中选择一首歌），在其他情况下则具有数值（产品需求）。请注意，在统计学和计算机科学中，*目标*也常被称为*响应*或*因变量*。这些术语可以互换使用。
- en: Third, each of these problems can have sets of historical data in which the
    target is known. For instance, over weeks or months of data collection, you can
    determine which of your subscribers churned and which people clicked your ads.
    With some manual effort, you can assess the sentiment of different tweets. In
    addition to known target values, your historical data files will contain information
    about each instance that’s knowable at the time of prediction. These are *input
    features* (also commonly referred to as the *explanatory* or *independent variables*).
    For example, the product usage history of each customer, along with the customer’s
    demographics and account information, would be appropriate input features for
    churn prediction. The input features, together with the known values of the target
    variable, compose the *training set*.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，这些问题的每一个都可以有包含已知目标的历史数据集。例如，在几周或几个月的数据收集过程中，你可以确定哪些订阅者流失了，哪些人点击了你的广告。通过一些手动努力，你可以评估不同推文的情绪。除了已知的目标值之外，你的历史数据文件还将包含在预测时可知的每个实例的信息。这些是*输入特征*（也常被称为*解释性*或*自变量*）。例如，每个客户的消费历史，连同客户的人口统计信息和账户信息，将是流失预测的适当输入特征。输入特征，连同目标变量的已知值，构成了*训练集*。
- en: Finally, each of these questions comes with an implied *action* if the target
    were knowable. For example, if you knew that a user would click your ad, you would
    bid on that user and serve the user an ad. Likewise, if you knew precisely your
    product demand for the upcoming month, you would position your supply chain to
    match that demand. The role of the ML algorithm is to use the training set to
    determine how the set of input features can most accurately predict the target
    variable. The result of this “learning” is encoded in a machine-learning model.
    When new instances (with an unknown target) are observed, their features are fed
    into the ML model, which generates predictions on those instances. Ultimately,
    those predictions enable the end user to taker smarter (and faster) actions. In
    addition to producing predictions, the ML model allows the user to draw inferences
    about the relationships between the input features and the target variable.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果目标变量是可知的，每个问题都隐含着一个*行动*。例如，如果你知道一个用户会点击你的广告，你就会对该用户进行出价并展示广告。同样，如果你精确地知道下个月的产品需求，你就会调整供应链以匹配该需求。机器学习算法的作用是使用训练集来确定如何使用输入特征集最准确地预测目标变量。这种“学习”的结果编码在机器学习模型中。当观察到新的实例（具有未知的目标）时，它们的特征被输入到机器学习模型中，该模型对那些实例生成预测。最终，这些预测使最终用户能够采取更明智（更快）的行动。除了产生预测外，机器学习模型还允许用户推断输入特征与目标变量之间的关系。
- en: Let’s put all this in the context of the churn prediction problem. Imagine that
    you work for a telecom company and that the question of interest is, “Which of
    my current cell-phone subscribers will unsubscribe in the next month?” Here, each
    instance is a current subscriber. Likewise, the target variable is the binary
    outcome of whether each subscriber cancelled service during that month. The input
    features can consist of any information about each customer that’s knowable at
    the beginning of the month, such as the current duration of the account, details
    on the subscription plan, and usage information such as total number of calls
    made and minutes used in the previous month. [Figure 2.2](#ch02fig02) shows the
    first four rows of an example training set for telecom churn prediction.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将所有这些内容放在客户流失预测问题的背景下。想象一下，你为一家电信公司工作，而你感兴趣的问题是：“下个月，我的哪些当前手机用户会取消订阅？”在这里，每个实例都是一个当前用户。同样，目标变量是每个用户在那个月是否取消服务的二元结果。输入特征可以包括任何在月初可知的关于每个客户的信息，例如账户当前时长、订阅计划的详细信息以及如上个月的总通话次数和使用的分钟数等信息。[图2.2](#ch02fig02)显示了电信客户流失预测的一个示例训练集的前四行。
- en: Figure 2.2\. Training data with four instances for the telecom churn problem
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2. 用于电信客户流失问题的四个实例的训练数据
- en: '![](02fig02_alt.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig02_alt.jpg)'
- en: 'The aim of this section is to give a basic guide for properly collecting training
    data for machine learning. Data collection can differ tremendously from industry
    to industry, but several common questions and pain points arise when assembling
    training data. The following subsections provide a practical guide to addressing
    four of the most common data-collection questions:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目标是提供一个基本指南，以正确收集机器学习训练数据。数据收集可能因行业而异，但在组装训练数据时，会出现一些常见的问题和痛点。以下子节提供了一个实用指南，以解决四个最常见的数据收集问题：
- en: Which input features should I include?
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我应该包含哪些输入特征？
- en: How do I obtain known values of my target variable?
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何获得目标变量的已知值？
- en: How much training data do I need?
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我需要多少训练数据？
- en: How do I know if my training data is good enough?
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何知道我的训练数据是否足够好？
- en: 2.1.1\. Which features should be included?
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1. 应包含哪些特征？
- en: 'In machine-learning problems, you’ll typically have dozens of features that
    you could use to predict the target variable. In the telecom churn problem, input
    attributes about each customer’s demographics (age, gender, location), subscription
    plan (status, time remaining, time since last renewal, preferred status), and
    usage (calling history, text-messaging data and data usage, payment history) may
    all be available to use as input features. Only two practical restrictions exist
    on whether something may be used as an input feature:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习问题中，你通常会有一打以上的特征可以用来预测目标变量。在电信客户流失问题中，关于每个客户的人口统计信息（年龄、性别、位置）、订阅计划（状态、剩余时间、上次续订时间、首选状态）以及使用情况（通话记录、短信数据和数据使用、支付历史）都可能作为输入特征可用。关于是否可以将某物用作输入特征，存在两个实际限制：
- en: The value of the feature must be known at the time predictions are needed (for
    example, at the beginning of the month for the telecom churn example).
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征的值必须在需要预测时已知（例如，对于电信 churn 示例，在月初时）。
- en: The feature must be numerical or categorical in nature ([chapter 5](kindle_split_015.html#ch05)
    shows how non-numerical data can be transformed into features via feature engineering).
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征必须是数值或分类性质（[第5章](kindle_split_015.html#ch05)展示了如何通过特征工程将非数值数据转换为特征）。
- en: Data such as Calling History data streams can be processed into a set of numerical
    and/or categorical features by computing summary statistics on the data, such
    as total minutes used, ratio of day/night minutes used, ratio of week/weekend
    minutes used, and proportion of minutes used in network.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 呼叫历史数据流等数据可以通过计算数据摘要统计量（如总通话分钟数、白天/夜间通话分钟数比率、周内/周末通话分钟数比率以及网络使用分钟数比例）转换成一组数值和/或分类特征。
- en: Given such a broad array of possible features, which should you use? As a simple
    rule of thumb, features should be included only if they’re suspected to be related
    to the target variable. Insofar as the goal of supervised ML is to predict the
    target, features that obviously have nothing to do with the target should be excluded.
    For example, if a distinguishing identification number was available for each
    customer, it shouldn’t be used as an input feature to predict whether the customer
    will unsubscribe. Such useless features make it more difficult to detect the true
    relationships (signals) from the random perturbations in the data (noise). The
    more uninformative features are present, the lower the signal-to-noise ratio and
    thus the less accurate (on average) the ML model will be.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在如此众多的可能特征中，你应该使用哪些呢？作为一个简单的经验法则，只有当特征被认为与目标变量相关时才应该包含在内。既然监督式机器学习的目标是预测目标，那么显然与目标无关的特征应该被排除。例如，如果每个客户都有一个独特的识别号码，那么这个号码就不应该用作输入特征来预测客户是否会取消订阅。这样的无用特征使得从数据中的随机扰动（噪声）中检测真正的关联（信号）变得更加困难。不具信息量的特征越多，信噪比越低，因此机器学习模型的平均准确性就越低。
- en: Likewise, excluding an input feature because it wasn’t previously known to be
    related to the target can also hurt the accuracy of your ML model. Indeed, it’s
    the role of ML to discover new patterns and relationships in data! Suppose, for
    instance, that a feature counting the number of current unopened voicemail messages
    was excluded from the feature set. Yet, some small subset of the population has
    ceased to check their voicemail because they decided to change carriers in the
    following month. This signal would express itself in the data as a slightly increased
    conditional probability of churn for customers with a large number of unopened
    voicemails. Exclusion of that input feature would deprive the ML algorithm of
    important information and therefore would result in an ML system of lower predictive
    accuracy. Because ML algorithms are able to discover subtle, nonlinear relationships,
    features beyond the known, first-order effects can have a substantial impact on
    the accuracy of the model.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，排除一个输入特征，因为它之前并未被认定为与目标相关，也可能损害你的机器学习模型的准确性。实际上，机器学习的角色就是发现数据中的新模式和关系！例如，假设有一个特征是计算当前未打开语音邮件的数量，如果这个特征被排除在特征集之外，那么一些小部分人群因为决定在下个月更换运营商而停止检查语音邮件。这个信号会在数据中表现为拥有大量未打开语音邮件的客户
    churn 条件概率略有增加。排除这个输入特征将使机器学习算法失去重要信息，因此会导致机器学习系统的预测准确性降低。由于机器学习算法能够发现微妙的非线性关系，超出已知的一阶效应的特征可能会对模型的准确性产生重大影响。
- en: In selecting a set of input features to use, you face a trade-off. On one hand,
    throwing every possible feature that comes to mind (“the kitchen sink”) into the
    model can drown out the handful of features that contain any signal with an overwhelming
    amount of noise. The accuracy of the ML model then suffers because it can’t distinguish
    true patterns from random noise. On the other extreme, hand-selecting a small
    subset of features that you already know are related to the target variable can
    cause you to omit other highly predictive features. As a result, the accuracy
    of the ML model suffers because the model doesn’t know about the neglected features,
    which are predictive of the target.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择要使用的输入特征集时，你会面临一个权衡。一方面，将所有可能想到的特征（“大杂烩”）都放入模型中，可能会淹没那些包含任何信号的少量特征，而这些特征被大量的噪声所淹没。因此，机器学习模型的准确性会受到影响，因为它无法区分真实模式与随机噪声。另一方面，手动选择一小部分已知与目标变量相关的特征，可能会导致你遗漏其他高度预测性的特征。结果，机器学习模型的准确性会受到影响，因为模型不知道被忽视的特征，而这些特征是目标变量的预测因素。
- en: 'Faced with this trade-off, the most practical approach is the following:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 面对这种权衡，最实际的方法是以下：
- en: Include all the features that you suspect to be predictive of the target variable.
    Fit an ML model. If the accuracy of the model is sufficient, stop.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 包含所有你认为可能预测目标变量的特征。拟合一个机器学习模型。如果模型的准确性足够，则停止。
- en: Otherwise, expand the feature set by including other features that are less
    obviously related to the target. Fit another model and assess the accuracy. If
    performance is sufficient, stop.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 否则，通过包括其他与目标变量不那么明显相关的特征来扩展特征集。拟合另一个模型并评估准确性。如果性能足够，则停止。
- en: Otherwise, starting from the expanded feature set, run an ML *feature selection
    algorithm* to choose the best, most predictive subset of your expanded feature
    set.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 否则，从扩展的特征集开始，运行机器学习*特征选择算法*以选择你扩展特征集中最好、最预测性的子集。
- en: We further discuss feature selection algorithms in [chapter 5](kindle_split_015.html#ch05).
    These approaches seek the most accurate model built on a subset of the feature
    set; they retain the signal in the feature set while discarding the noise. Though
    computationally expensive, they can yield a tremendous boost in model performance.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第5章](kindle_split_015.html#ch05)进一步讨论了特征选择算法。这些方法寻求在特征集的子集上构建最准确的模型；它们在特征集中保留信号，同时丢弃噪声。尽管计算成本高昂，但它们可以在模型性能上带来巨大的提升。
- en: To finish this subsection, it’s important to note that in order to use an input
    feature, that feature doesn’t have to be present for each instance. For example,
    if the ages of your customers are known for only 75% of your client base, you
    could still use age as an input feature. We discuss ways to handle missing data
    later in the chapter.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个子节，重要的是要注意，为了使用输入特征，该特征不需要在每个实例中都存在。例如，如果你的客户群中只有75%的客户年龄已知，你仍然可以使用年龄作为输入特征。我们将在本章后面讨论处理缺失数据的方法。
- en: 2.1.2\. How can we obtain ground truth for the target variable?
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2\. 我们如何获得目标变量的真实值？
- en: One of the most difficult hurdles in getting started with supervised machine
    learning is the aggregation of training instances with a known target variable.
    This process often requires running an existing, suboptimal system for a period
    of time, until enough training data is collected. For example, in building out
    an ML solution for telecom churn, you first need to sit on your hands and watch
    over several weeks or months as some customers unsubscribe and others renew. After
    you have enough training instances to build an accurate ML model, you can flip
    the switch and start using ML in production.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始监督机器学习时，最困难的障碍之一是收集具有已知目标变量的训练实例。这个过程通常需要运行现有的次优系统一段时间，直到收集到足够的训练数据。例如，在构建电信客户流失的机器学习解决方案时，你首先需要耐心等待几周或几个月，观察一些客户取消订阅而另一些客户续订。在你收集到足够的训练实例以构建准确的机器学习模型后，你可以切换开关并开始在生产中使用机器学习。
- en: 'Each use case will have a different process by which ground truth—the actual
    or observed value of the target variable—can be collected or estimated. For example,
    consider the following training-data collection processes for a few selected ML
    use cases:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 每个用例都将有一个不同的过程，通过这个过程可以收集或估计真实值——目标变量的实际或观察到的值。例如，考虑以下几个选定的机器学习用例的训练数据收集过程：
- en: '***Ad targeting—*** You can run a campaign for a few days to determine which
    users did/didn’t click your ad and which users converted.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***广告定位——*** 你可以运行几天活动以确定哪些用户点击了你的广告，哪些用户进行了转化。'
- en: '***Fraud detection—*** You can pore over your past data to figure out which
    users were fraudulent and which were legitimate.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***欺诈检测——*** 你可以仔细检查你的历史数据，以确定哪些用户是欺诈的，哪些是合法的。'
- en: '***Demand forecasting—*** You can go into your historical supply-chain management
    data logs to determine the demand over the past months or years.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***需求预测——*** 你可以查看你过去几个月或几年的供应链管理数据日志，以确定过去几个月或几年的需求。'
- en: '***Twitter sentiment—*** Getting information on the true intended sentiment
    is considerably harder. You can perform manual analysis on a set of tweets by
    having people read and opine on tweets (or use crowdsourcing).'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Twitter情感分析——*** 获取真实意图的情感信息相当困难。你可以通过让人们阅读并评论一组推文（或使用众包）来进行手动分析。'
- en: 'Although the collection of instances of known target variables can be painful,
    both in terms of time and money, the benefits of migrating to an ML solution are
    likely to more than make up for those losses. Other ways of obtaining ground-truth
    values of the target variable include the following:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然收集已知目标变量的实例可能既耗时又耗钱，但迁移到机器学习解决方案的好处可能会远远弥补这些损失。获取目标变量真实值的其他方法包括以下：
- en: Dedicating analysts to manually look through past or current data to determine
    or estimate the ground-truth values of the target
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指派分析师手动检查过去或当前数据，以确定或估计目标的真实值
- en: Using crowdsourcing to use the “wisdom of crowds” in order to attain estimates
    of the target
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用众包来利用“群体智慧”以获得目标估计
- en: Conducting follow-up interviews or other hands-on experiments with customers
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对客户进行后续访谈或其他实际实验
- en: Running controlled experiments (for example, A/B tests) and monitoring the responses
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行控制实验（例如，A/B测试）并监控响应
- en: Each of these strategies is labor-intensive, but you can accelerate the learning
    process and shorten the time required to collect training data by collecting only
    target variables for the instances that have the most influence on the machine-learning
    model. One example of this is a method called *active learning*. Given an existing
    (small) training set and a (large) set of data with unknown response variable,
    active learning identifies the subset of instances from the latter set whose inclusion
    in the training set would yield the most accurate ML model. In this sense, active
    learning can accelerate the production of an accurate ML model by focusing manual
    resources. For more information on active learning and related methods, see the
    2009 presentation by Dasgupta and Langford from ICML.^([[1](#ch02fn01)])
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这些策略都需要大量的人工劳动，但通过仅收集对机器学习模型影响最大的实例的目标变量，你可以加速学习过程并缩短收集训练数据所需的时间。这种方法的一个例子是称为*主动学习*的方法。给定一个现有的（小）训练集和一个（大）的数据集，其中包含未知响应变量，主动学习识别出后者集中包含在训练集中将产生最准确机器学习模型的实例子集。从这个意义上说，主动学习可以通过集中人工资源来加速准确机器学习模型的生产。有关主动学习及相关方法的更多信息，请参阅Dasgupta和Langford于2009年在ICML上的演示。[^([[1](#ch02fn01))]
- en: ¹
  id: totrans-276
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-277
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See [http://videolectures.net/icml09_dasgupta_langford_actl/](http://videolectures.net/icml09_dasgupta_langford_actl/).
  id: totrans-278
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 查看[http://videolectures.net/icml09_dasgupta_langford_actl/](http://videolectures.net/icml09_dasgupta_langford_actl/)。
- en: 2.1.3\. How much training data is required?
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3\. 需要多少训练数据？
- en: Given the difficulty of observing and collecting the response variable for data
    instances, you might wonder how much training data is required to get an ML model
    up and running. Unfortunately, this question is so problem-specific that it’s
    impossible to give a universal response or even a rule of thumb.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 由于观察和收集数据实例的响应变量的困难，你可能会想知道需要多少训练数据才能使机器学习模型运行起来。不幸的是，这个问题如此特定于问题本身，以至于无法给出普遍的回应，甚至没有一个经验法则。
- en: 'These factors determine the amount of training data needed:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这些因素决定了所需训练数据量：
- en: The complexity of the problem. Does the relationship between the input features
    and target variable follow a simple pattern, or is it complex and nonlinear?
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题的复杂性。输入特征与目标变量之间的关系是简单的模式，还是复杂且非线性的？
- en: The requirements for accuracy. If you require only a 60% success rate for your
    problem, less training data is required than if you need to achieve a 95% success
    rate.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确度的要求。如果你只需要达到60%的成功率来解决你的问题，那么所需的训练数据量将少于你需要达到95%成功率的情况。
- en: The dimensionality of the feature space. If only two input features are available,
    less training data will be required than if there were 2,000 features.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征空间的维度。如果只有两个输入特征可用，所需的训练数据量将少于有2,000个特征的情况。
- en: One guiding principle to remember is that, as the training set grows, the models
    will (on average) get more accurate. (This assumes that the data remains *representative*
    of the ongoing data-generating process, which you’ll learn more about in the next
    section.) More training data results in higher accuracy because of the data-driven
    nature of ML models. Because the relationship between the features and target
    is learned entirely from the training data, the more you have, the higher the
    model’s ability to recognize and capture more-subtle patterns and relationships.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 一个需要记住的指导原则是，随着训练集的增长，模型（平均而言）将变得更加准确。（这假设数据仍然代表持续的数据生成过程，你将在下一节中了解更多。）更多的训练数据导致更高的准确率，因为机器学习模型的数据驱动特性。由于特征和目标之间的关系完全是从训练数据中学习的，所以你拥有的越多，模型识别和捕捉更微妙模式和关系的能力就越强。
- en: 'Using the telecom data from earlier in the chapter, we can demonstrate how
    the ML model improves with more training data and also offer a strategy to assess
    whether more training data is required. The telecom training dataset consists
    of 3,333 instances, each containing 19 features plus the binary outcome of unsubscribed
    versus renewed. Using this data, it’s straightforward to assess whether you need
    to collect more data. Do the following:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本章前面提到的电信数据，我们可以展示机器学习模型如何随着更多训练数据的增加而提高，并提供一种评估是否需要更多训练数据的方法。电信训练数据集包含3,333个实例，每个实例包含19个特征以及未订阅与续订的二进制结果。使用这些数据，可以轻松评估你是否需要收集更多数据。请执行以下操作：
- en: Using the current training set, choose a grid of subsample sizes to try. For
    example, with this telecom training set of 3,333 instances of training data, your
    grid could be 500; 1,000; 1,500; 2,000; 2,500; 3,000.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用当前的训练集，选择一个子样本大小的网格进行尝试。例如，对于这个包含3,333个训练数据实例的电信训练集，你的网格可以是500；1,000；1,500；2,000；2,500；3,000。
- en: For each sample size, randomly draw that many instances (without replacement)
    from the training set.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个样本大小，从训练集中随机抽取这么多实例（不重复抽取）。
- en: With each subsample of training data, build an ML model and assess the accuracy
    of that model (we talk about ML evaluation metrics in [chapter 4](kindle_split_014.html#ch04)).
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个训练数据的子样本，构建一个机器学习模型并评估该模型的准确率（我们在第4章中讨论了机器学习评估指标[chapter 4](kindle_split_014.html#ch04)）。
- en: Assess how the accuracy changes as a function of sample size. If it seems to
    level off at the higher sample sizes, the existing training set is probably sufficient.
    But if the accuracy continues to rise for the larger samples, the inclusion of
    more training instances would likely boost accuracy.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估准确率随样本大小的变化情况。如果它似乎在较大的样本大小上趋于平稳，那么现有的训练集可能足够。但如果准确率对于较大的样本继续上升，那么包含更多的训练实例可能会提高准确率。
- en: Alternatively, if you have a clear accuracy target, you can use this strategy
    to assess whether that target has been fulfilled by your current ML model built
    on the existing training data (in which case it isn’t necessary to amass more
    training data).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你有一个明确的准确率目标，你可以使用这种策略来评估你的当前基于现有训练数据的机器学习模型是否已经实现了该目标（在这种情况下，没有必要收集更多的训练数据）。
- en: '[Figure 2.3](#ch02fig03) demonstrates how the accuracy of the fitted ML model
    changes as a function of the number of training instances used with the telecom
    dataset. In this case, it’s clear that the ML model improves as you add training
    data: moving from 250 to 500 to 750 training examples produces significant improvements
    in the accuracy level. Yet, as you increase the number of training instances beyond
    2,000, the accuracy levels off. This is evidence that the ML model won’t improve
    substantially if you add more training instances. (This doesn’t mean that significant
    improvements couldn’t be made by using more features.)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.3](#ch02fig03) 展示了电信数据集中使用训练实例数量作为函数的拟合机器学习模型准确率的变化。在这种情况下，很明显，随着训练数据的增加，机器学习模型会提高：从250个到500个再到750个训练示例，准确率水平显著提高。然而，当你将训练实例的数量增加到2,000个以上时，准确率趋于平稳。这是证据表明，如果你添加更多的训练实例，机器学习模型不会显著提高。（这并不意味着通过使用更多特征不能实现显著的改进。）'
- en: Figure 2.3\. Testing whether the existing sample of 3,333 training instances
    is enough data to build an accurate telecom churn ML model. The black line represents
    the average accuracy over 10 repetitions of the assessment routine, and the shaded
    bands represent the error bands.
  id: totrans-293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.3\. 测试现有3,333个训练实例的样本是否足够构建一个准确的电信客户流失机器学习模型。黑色线条表示评估流程重复10次后的平均准确率，阴影带表示误差范围。
- en: '![](02fig03_alt.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![图片](02fig03_alt.jpg)'
- en: 2.1.4\. Is the training set representative enough?
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4\. 训练集是否足够具有代表性？
- en: Besides the size of the training set, another important factor for generating
    accurate predictive ML models is the *representativeness* of the training set.
    How similar are the instances in the training set to the instances that will be
    collected in the future? Because the goal of supervised machine learning is to
    generate accurate predictions on new data, it’s fundamental that the training
    set be representative of the sorts of instances that you ultimately want to generate
    predictions for. A training set that consists of a nonrepresentative sample of
    what future data will look like is called *sample-selection bias* or *covariate
    shift*.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 除了训练集的大小之外，生成准确预测机器学习模型的另一个重要因素是训练集的*代表性*。训练集中的实例与未来将要收集的实例有多相似？因为监督机器学习的目标是生成对新数据的准确预测，所以训练集必须代表你最终想要生成预测的实例类型。由未来数据可能呈现的非代表性样本组成的训练集被称为*样本选择偏差*或*协变量偏移*。
- en: 'A training sample could be nonrepresentative for several reasons:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 训练样本可能由于以下几个原因不具有代表性：
- en: It was possible to obtain ground truth for the target variable for only a certain,
    biased subsample of data. For example, if instances of fraud in your historical
    data were detected only if they cost the company more than $1,000, then a model
    trained on that data will have difficulty identifying cases of fraud that result
    in losses less than $1,000.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只能从某些具有偏差的子样本中获取目标变量的真实值。例如，如果你的历史数据中的欺诈实例只有在造成公司损失超过1000美元时才会被检测到，那么在该数据上训练的模型将难以识别损失低于1000美元的欺诈案例。
- en: The properties of the instances have changed over time. For example, if your
    training example consists of historical data on medical insurance fraud, but new
    laws have substantially changed the ways in which medical insurers must conduct
    their business, then your predictions on the new data may not be appropriate.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例的性质随着时间的推移而发生了变化。例如，如果你的训练示例包括医疗保险欺诈的历史数据，但新的法律大大改变了医疗保险公司必须进行业务的方式，那么你对新数据的预测可能不合适。
- en: The input feature set has changed over time. For example, say the set of location
    attributes that you collect on each customer has changed; you used to collect
    ZIP code and state, but now collect IP address. This change may require you to
    modify the feature set used for the model and potentially discard old data from
    the training set.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入特征集随着时间的推移而发生了变化。例如，假设你收集的每个客户的地理位置属性集合发生了变化；你以前收集的是邮政编码和州，但现在收集IP地址。这种变化可能需要你修改用于模型的特征集，并可能需要从训练集中丢弃旧数据。
- en: 'In each of these cases, an ML model fit to the training data may not extrapolate
    well to new data. To borrow an adage: you wouldn’t necessarily want to use your
    model trained on apples to try to predict on oranges! The predictive accuracy
    of the model on oranges would likely not be good.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，拟合到训练数据的机器学习模型可能无法很好地推广到新数据。借用一句谚语：你不会想用训练在苹果上的模型去尝试预测橙子！该模型在橙子上的预测准确性可能不会很好。
- en: To avoid these problems, it’s important to attempt to make the training set
    as representative of future data as possible. This entails structuring your training-data
    collection process in such a way that biases are removed. As we mention in the
    following section, visualization can also help ensure that the training data is
    representative.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这些问题，重要的是尽可能使训练集与未来数据相似。这意味着要以一种方式结构化你的训练数据收集过程，以便消除偏差。正如我们在下一节中提到的，可视化也可以帮助确保训练数据具有代表性。
- en: Now that you have an idea of how to collect training data, your next task is
    to structure and assemble that data to get ready for ML model building. The next
    section shows how to preprocess your training data so you can start building models
    (the topic of [chapter 3](kindle_split_013.html#ch03)).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了如何收集训练数据，你的下一个任务是结构化和组装这些数据，为机器学习模型构建做准备。下一节将展示如何预处理你的训练数据，以便开始构建模型（第3章的主题）。
- en: 2.2\. Preprocessing the data for modeling
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 为建模预处理数据
- en: Collecting data is the first step toward preparing the data for modeling, but
    sometimes you must run the data through a few preprocessing steps, depending on
    the composition of the dataset. Many machine-learning algorithms work only on
    numerical data—integers and real-valued numbers. The simplest ML datasets come
    in this format, but many include other types of features, such as categorical
    variables, and some have missing values. Sometimes you need to construct or compute
    features through feature engineering. Some numeric features may need to be rescaled
    to make them comparable or to bring them into line with a frequency distribution
    (for example, grading on the normal curve). In this section, you’ll look at these
    common data preprocessing steps needed for real-world machine learning.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 收集数据是准备数据用于建模的第一步，但有时你必须根据数据集的组成运行数据通过几个预处理步骤。许多机器学习算法仅适用于数值数据——整数和实值数。最简单的机器学习数据集以这种格式出现，但许多还包括其他类型的特征，例如分类变量，并且一些数据集存在缺失值。有时你需要通过特征工程构建或计算特征。一些数值特征可能需要缩放以使它们可比较或使它们与频率分布保持一致（例如，在正态曲线上评分）。在本节中，你将了解现实世界机器学习所需的一些常见数据预处理步骤。
- en: 2.2.1\. Categorical features
  id: totrans-306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1. 分类特征
- en: The most common type of non-numerical feature is the categorical feature. A
    feature is *categorical* if values can be placed in buckets and the order of values
    isn’t important. In some cases, this type of feature is easy to identify (for
    example, when it takes on only a few string values, such as `spam` and `ham`).
    In other cases, whether a feature is a numerical (integer) feature or categorical
    isn’t so obvious. Sometimes either may be a valid representation, and the choice
    can affect the performance of the model. An example is a feature representing
    the day of the week, which could validly be encoded as either numerical (number
    of days since Sunday) or as categorical (the names Monday, Tuesday, and so forth).
    You aren’t going to look at model building and performance until [chapters 3](kindle_split_013.html#ch03)
    and [4](kindle_split_014.html#ch04), but this section introduces a technique for
    dealing with categorical features. [Figure 2.4](#ch02fig04) points out categorical
    features in a few datasets.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的非数值特征类型是分类特征。如果一个特征可以放入桶中，并且值的顺序不重要，则该特征是*分类的*。在某些情况下，此类特征很容易识别（例如，当它只具有几个字符串值时，如`spam`和`ham`）。在其他情况下，一个特征是数值（整数）特征还是分类特征并不那么明显。有时两种可能都是有效的表示，并且选择可能会影响模型的性能。一个例子是表示一周中某天的特征，它可以有效编码为数值（自星期日以来的天数）或分类（星期一、星期二等名称）。你将不会在[第3章](kindle_split_013.html#ch03)和[第4章](kindle_split_014.html#ch04)中查看模型构建和性能，但本节介绍了一种处理分类特征的技术。[图2.4](#ch02fig04)指出了几个数据集中的分类特征。
- en: Figure 2.4\. Identifying categorical features. At the top is the simple Person
    dataset, which has a Marital Status categorical feature. At the bottom is a dataset
    with information about Titanic passengers. The features identified as categorical
    here are Survived (whether the passenger survived or not), Pclass (what class
    the passenger was traveling on), Gender (male or female), and Embarked (from which
    city the passenger embarked).
  id: totrans-308
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.4. 识别分类特征。顶部是简单的Person数据集，它有一个婚姻状况分类特征。底部是关于泰坦尼克号乘客的数据集。这里被识别为分类特征的有：Survived（乘客是否幸存），Pclass（乘客所乘的等级），Gender（男性或女性），和Embarked（乘客出发的城市）。
- en: '![](02fig04_alt.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![图片](02fig04_alt.jpg)'
- en: Some machine-learning algorithms use categorical features natively, but generally
    they need data in numerical form. You can encode categorical features as numbers
    (one number per category), but you can’t use this encoded data as a true categorical
    feature because you’ve then introduced an (arbitrary) order of categories. Recall
    that one of the properties of categorical features is that they aren’t ordered.
    Instead, you can convert each of the categories into a separate binary feature
    that has value 1 for instances for which the category appeared, and value 0 when
    it didn’t. Hence, each categorical feature is converted to a set of binary features,
    one per category. Features constructed in this way are sometimes called *dummy
    variables*. [Figure 2.5](#ch02fig05) illustrates this concept further.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习算法可以原生地使用分类特征，但通常它们需要数值形式的数据。你可以将分类特征编码为数字（每个类别一个数字），但你不能将这种编码后的数据用作真正的分类特征，因为这样你引入了（任意的）类别顺序。回想一下，分类特征的一个属性是它们是无序的。相反，你可以将每个类别转换为单独的二进制特征，对于出现该类别的实例，其值为
    1，未出现时为 0。因此，每个分类特征都转换为一系列二进制特征，每个类别一个。以这种方式构建的特征有时被称为 *虚拟变量*。[图 2.5](#ch02fig05)
    进一步说明了这个概念。
- en: Figure 2.5\. Converting categorical columns to numerical columns
  id: totrans-311
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.5. 将分类列转换为数值列
- en: '![](02fig05.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![图片](02fig05.jpg)'
- en: The pseudocode for converting the categorical features in [figure 2.5](#ch02fig05)
    to binary features looks like the following listing. Note that `categories` is
    a special NumPy type ([www.numpy.org](http://www.numpy.org)) such that `(data
    == cat)` yields a list of Boolean values.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 将 [图 2.5](#ch02fig05) 中的分类特征转换为二进制特征的伪代码如下所示。请注意，`categories` 是一种特殊的 NumPy 类型
    ([www.numpy.org](http://www.numpy.org))，使得 `(data == cat)` 产生一个布尔值列表。
- en: Listing 2.1\. Convert categorical features to numerical binary features
  id: totrans-314
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.1. 将分类特征转换为数值二进制特征
- en: '[PRE2]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '|  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-317
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: 'Readers familiar with the Python programming language may have noticed that
    the preceding example isn’t just pseudocode, but also valid Python. You’ll see
    this a lot throughout the book: we introduce a code snippet as pseudocode, but
    unless otherwise noted, it’s working code. To make the code simpler, we implicitly
    import a few helper libraries, such as `numpy` and `scipy`. Our examples will
    generally work if you include `from numpy import *`, and `from scipy import *`.
    Note that although this approach is convenient for trying out examples interactively,
    you should never use it in real applications, because the `import *` construct
    may cause name conflicts and unexpected results. All code samples are available
    for inspection and direct execution in the accompanying GitHub repository: [https://github.com/brinkar/real-world-machine-learning](https://github.com/brinkar/real-world-machine-learning).'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉 Python 编程语言的读者可能已经注意到，前面的例子不仅仅是伪代码，也是有效的 Python 代码。你会在整本书中看到很多这样的例子：我们以伪代码的形式引入代码片段，除非另有说明，否则它都是可运行的代码。为了使代码更简单，我们隐式地导入了几个辅助库，例如
    `numpy` 和 `scipy`。我们的示例通常在包含 `from numpy import *` 和 `from scipy import *` 的情况下可以工作。请注意，尽管这种方法在交互式尝试示例时很方便，但你绝对不应该在实际应用中使用它，因为
    `import *` 结构可能会导致名称冲突和意外结果。所有代码示例都可以在随附的 GitHub 仓库中检查和直接执行：[https://github.com/brinkar/real-world-machine-learning](https://github.com/brinkar/real-world-machine-learning)。
- en: '|  |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The categorical-to-numerical conversion technique works for most ML algorithms.
    But a few algorithms (such as certain types of decision-tree algorithms and related
    algorithms such as random forests) can use categorical features natively. This
    will often yield better results for highly categorical datasets, and we discuss
    this further in the next chapter. Our simple Person dataset, after conversion
    of the categorical feature to binary features, is shown in [figure 2.6](#ch02fig06).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 分类到数值的转换技术适用于大多数机器学习算法。但有一些算法（如某些类型的决策树算法和相关算法，如随机森林）可以原生地使用分类特征。对于高度分类的数据集，这通常会得到更好的结果，我们将在下一章进一步讨论这一点。在将分类特征转换为二进制特征后，我们的简单
    Person 数据集如图 2.6 所示。
- en: Figure 2.6\. The simple Person dataset after conversion of the categorical Marital
    Status feature to binary numerical features. (The original dataset is shown in
    [figure 2.4](#ch02fig04).)
  id: totrans-321
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.6. 将分类婚姻状态特征转换为二进制数值特征后的简单 Person 数据集。（原始数据集显示在 [图 2.4](#ch02fig04) 中。）
- en: '![](02fig06_alt.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![图片](02fig06_alt.jpg)'
- en: 2.2.2\. Dealing with missing data
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2. 处理缺失数据
- en: You’ve already seen a few examples of datasets with missing data. In tabular
    datasets, missing data often appears as empty cells, or cells with NaN (Not a
    Number), N/A, or None. Missing data is usually an artifact of the data-collection
    process; for some reason, a particular value couldn’t be measured for a data instance.
    [Figure 2.7](#ch02fig07) shows an example of missing data in the Titanic Passengers
    dataset.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到了一些包含缺失数据的示例数据集。在表格数据集中，缺失数据通常表现为空单元格，或包含NaN（非数字）、N/A或None的单元格。缺失数据通常是数据收集过程中的一个副产品；由于某种原因，对于某个数据实例，特定的值无法被测量。[图2.7](#ch02fig07)展示了泰坦尼克号乘客数据集中缺失数据的示例。
- en: Figure 2.7\. The Titanic Passengers dataset has missing values in the Age and
    Cabin columns. The passenger information has been extracted from various historical
    sources, so in this case the missing values stem from information that couldn’t
    be found in the sources.
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.7\. 泰坦尼克号乘客数据集中的年龄和船舱列存在缺失值。乘客信息是从各种历史来源中提取的，因此在这种情况下，缺失值源于在来源中找不到的信息。
- en: '![](02fig07_alt.jpg)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![图片](02fig07_alt.jpg)'
- en: There are two main types of missing data, which you need to handle in different
    ways. First, for some data, *the fact that it’s missing* can carry meaningful
    information that could be useful for the ML algorithm. The other possibility is
    that the data is missing only because its measurement was impossible, and the
    unavailability of the information isn’t otherwise meaningful. In the Titanic Passengers
    dataset, for example, missing values in the Cabin column may indicate that those
    passengers were in a lower social or economic class, whereas missing values in
    the Age column carry no useful information (the age of a particular passenger
    at the time simply couldn’t be found).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失数据主要有两种类型，你需要以不同的方式处理它们。首先，对于某些数据，*数据缺失的事实*可能携带着对机器学习算法有用的有意义信息。另一种可能性是，数据缺失仅仅是因为其测量是不可能的，信息的不可用本身并没有其他意义。例如，在泰坦尼克号乘客数据集中，例如，船舱列中的缺失值可能表明那些乘客处于较低的社会或经济阶层，而年龄列中的缺失值则不携带任何有用的信息（当时特定乘客的年龄根本无法找到）。
- en: Let’s first consider the case of *informative* missing data. When you believe
    that information is missing from the data, you usually want the ML algorithm to
    be able to use this information to potentially improve the prediction accuracy.
    To achieve this, you want to convert the missing values into the same format as
    the column in general. For numerical columns, this can be done by setting missing
    values to –1 or –999, depending on typical values of non-null values. Pick a number
    at one end of the numerical spectrum that will denote missing values, and remember
    that order is important for numerical columns. You don’t want to pick a value
    in the middle of the distribution of values.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先考虑*信息性*缺失数据的情况。当你认为数据中缺少信息时，你通常希望机器学习算法能够利用这些信息来潜在地提高预测准确性。为了实现这一点，你希望将缺失值转换为与列中一般格式相同。对于数值列，这可以通过将缺失值设置为-1或-999来完成，具体取决于非空值的典型值。在数值谱的一端选择一个数字来表示缺失值，并记住顺序对数值列很重要。你不希望选择分布中间的值。
- en: For a categorical column with potentially informative missing data, you can
    create a new category called Missing, None, or similar, and then handle the categorical
    feature in the usual way (for example, using the technique described in the previous
    section). [Figure 2.8](#ch02fig08) shows a simple diagram of what to do with meaningful
    missing data.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有潜在信息性缺失数据的分类列，你可以创建一个名为“缺失”、“None”或类似的新的类别，然后以通常的方式处理分类特征（例如，使用上一节中描述的技术）。[图2.8](#ch02fig08)展示了处理有意义的缺失数据的一个简单示意图。
- en: Figure 2.8\. What to do with meaningful missing data
  id: totrans-330
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.8\. 如何处理有意义的缺失数据
- en: '![](02fig08_alt.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![图片](02fig08_alt.jpg)'
- en: When the absence of a value for a data item has no informative value in itself,
    you proceed in a different way. In this case, you can’t introduce a special number
    or category because you might introduce data that’s flat-out wrong. For example,
    if you were to change any missing values in the Age column of the Titanic Passengers
    dataset to –1, you’d probably hurt the model by messing with the age distribution
    for no good reason. Some ML algorithms will be able to deal with these truly missing
    values by ignoring them. If not, you need to preprocess the data to either eliminate
    missing values or replace them by guessing the true value. This concept of replacing
    missing data is called *imputation*.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据项缺失的值本身没有信息价值时，你将采取不同的方法。在这种情况下，你不能引入一个特殊的数字或类别，因为你可能会引入完全错误的数据。例如，如果你将泰坦尼克号乘客数据集中年龄列的任何缺失值更改为-1，你可能会因为没有任何理由而破坏年龄分布而损害模型。一些机器学习算法能够通过忽略这些真正的缺失值来处理这些缺失值。如果不能，你需要预处理数据，要么消除缺失值，要么通过猜测真实值来替换它们。这种替换缺失数据的概念被称为*插补*。
- en: If you have a large dataset and only a handful of missing values, dropping the
    observations with missing data is the easiest approach. But when a larger portion
    of your observations contain missing values, the loss of perfectly good data in
    the dropped observations will reduce the predictive power of your model. Furthermore,
    if the observations with missing values aren’t randomly distributed throughout
    your dataset, this approach may introduce unexpected bias.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据集很大，但缺失值只有少数几个，那么删除带有缺失数据的观测值是最简单的方法。但是，当你的观测值中有较大一部分包含缺失值时，删除观测值中丢失的完美数据将降低你模型的预测能力。此外，如果带有缺失值的观测值在数据集中不是随机分布的，这种方法可能会引入意外的偏差。
- en: Another simple approach is to assume some temporal order to the data instances
    and replace missing values with the column value of the preceding row. With no
    other information, you’re making a guess that a measurement hasn’t changed from
    one instance to the next. Needless to say, this assumption will often be wrong,
    but less wrong than, for example, filling in zeros for the missing values, especially
    if the data is a series of sequential observations (yesterday’s temperature isn’t
    an unreasonable estimate of today’s). And for extremely big data, you won’t always
    be able to apply more-sophisticated methods, and these simple methods can be useful.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种简单的方法是假设数据实例具有某种时间顺序，并用前一行列的值替换缺失值。在没有其他信息的情况下，你是在猜测一个测量值从一个实例到下一个实例没有变化。不用说，这个假设通常是不正确的，但比例如用零填充缺失值要正确得多，尤其是如果数据是一系列连续观测（昨天的温度不是对今天温度的不合理估计）。对于极其庞大的数据，你并不总是能够应用更复杂的方法，而这些简单的方法可能是有用的。
- en: When possible, it’s usually better to use a larger portion of the existing data
    to guess the missing values. You can replace missing column values by the mean
    or median value of the column. With no other information, you assume that the
    average will be closest to the truth. Depending on the distribution of column
    values, you might want to use the median instead; the mean is sensitive to outliers.
    These are widely used in machine learning today and work well in many cases. But
    when you set all missing values to a single new value, you diminish the visibility
    of potential correlation with other variables that may be important in order for
    the algorithm to detect certain patterns in the data.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 当可能时，通常更好的做法是使用现有数据的一部分来猜测缺失值。你可以通过替换缺失的列值来使用该列的平均值或中位数。在没有其他信息的情况下，你假设平均值最接近真实值。根据列值的分布，你可能想使用中位数；平均值对异常值敏感。这些方法在今天的机器学习中广泛使用，并且在许多情况下效果良好。但是，当你将所有缺失值设置为单个新值时，你会减少潜在的相关性可见性，这些相关性对于算法检测数据中的某些模式可能很重要。
- en: What you want to do, if you can, is use all the data at your disposal to predict
    the value of the missing variable. Does this sound familiar? This is exactly what
    machine learning is about, so you’re basically thinking about building ML models
    in order to be able to build ML models. In practice, you’ll typically use a simple
    algorithm (such as linear or logistic regression, described in [chapter 3](kindle_split_013.html#ch03))
    to impute the missing data. This isn’t necessarily the same as the main ML algorithm
    used. In any case, you’re creating a pipeline of ML algorithms that introduces
    more knobs to turn in order to optimize the model in the end.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您能做的话，您想做的就是使用您所能利用的所有数据来预测缺失变量的值。这听起来熟悉吗？这正是机器学习的核心，所以您基本上是在考虑构建ML模型，以便能够构建ML模型。在实践中，您通常会使用简单的算法（如线性或逻辑回归，在第3章中描述）来填补缺失数据。这不一定与主ML算法相同。无论如何，您正在创建一个ML算法的管道，它引入了更多的旋钮来最终优化模型。
- en: Again, it’s important to realize that there’s no single best way to deal with
    truly missing data. We’ve discussed a few ways in this section, and [figure 2.9](#ch02fig09)
    summarizes the possibilities.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，处理真正缺失数据没有唯一最佳方法。我们已经在本节中讨论了几种方法，[图2.9](#ch02fig09) 总结了可能性。
- en: Figure 2.9\. Full decision diagram for handling missing values when preparing
    data for ML modeling
  id: totrans-338
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.9。为ML建模准备数据时处理缺失值的完整决策图。
- en: '![](02fig09_alt.jpg)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![图片](02fig09_alt.jpg)'
- en: 2.2.3\. Simple feature engineering
  id: totrans-340
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3。简单特征工程
- en: '[Chapter 5](kindle_split_015.html#ch05) covers domain-specific and advanced
    feature-engineering techniques, but it’s worth mentioning the basic idea of simple
    data preprocessing in order to make the model better.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '[第五章](kindle_split_015.html#ch05) 讨论了特定领域和高级特征工程技术，但为了使模型更好，值得提及简单数据预处理的基本思想。'
- en: You’ll use the Titanic example again in this section. [Figure 2.10](#ch02fig10)
    presents another look at part of the data, and in particular the Cabin feature.
    Without processing, the Cabin feature isn’t necessarily useful. Some values seem
    to include multiple cabins, and even a single cabin wouldn’t seem like a good
    categorical feature because all cabins would be separate “buckets.” If you want
    to predict, for example, whether a certain passenger survived, living in a particular
    cabin instead of the neighboring cabin may not have any predictive power.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在本节中再次使用泰坦尼克号示例。[图2.10](#ch02fig10) 展示了数据的一部分，特别是客舱功能。未经处理，客舱功能不一定有用。一些值似乎包含多个客舱，甚至单个客舱也不太可能是一个好的分类特征，因为所有客舱都会是分开的“桶”。如果您想预测，例如，某个乘客是否幸存，住在特定客舱而不是相邻客舱可能没有任何预测能力。
- en: Figure 2.10\. In the Titanic Passengers dataset, some Cabin values include multiple
    cabins, whereas others are missing. And cabin identifiers themselves may not be
    good categorical features.
  id: totrans-343
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.10。在泰坦尼克号乘客数据集中，一些客舱值包含多个客舱，而另一些则缺失。而且客舱标识符本身可能不是好的分类特征。
- en: '![](02fig10_alt.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![图片](02fig10_alt.jpg)'
- en: Living in a particular section of the ship, though, could be important for survival.
    For single cabin IDs, you could extract the letter as a categorical feature and
    the number as a numerical feature, assuming they denote different parts of the
    ship. You could even find a layout map of the Titanic and map each cabin to the
    level and side of the ship, ocean-facing versus interior, and so forth. These
    approaches don’t handle multiple cabin IDs, but because it looks like all multiple
    cabins are close to each other, extracting only the first cabin ID should be fine.
    You could also include the number of cabins in a new feature, which could also
    be relevant.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，住在船的特定部分对于生存可能很重要。对于单个客舱ID，您可以提取字母作为分类特征，数字作为数值特征，假设它们代表船的不同部分。您甚至可以找到泰坦尼克号的布局图，并将每个客舱映射到船的楼层和侧面，面向海洋还是内部，等等。这些方法不处理多个客舱ID，但由于看起来所有多个客舱都彼此靠近，因此仅提取第一个客舱ID应该足够。您还可以将客舱数量包含在一个新特征中，这也可能相关。
- en: All in all, you’ll create three new features from the Cabin feature. The following
    listing shows the code for this simple extraction.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，您将从Cabin功能中创建三个新功能。以下列表显示了这种简单提取的代码。
- en: Listing 2.2\. Simple feature extraction on Titanic cabins
  id: totrans-347
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.2。泰坦尼克号客舱的简单特征提取。
- en: '[PRE3]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'By now it should be no surprise what we mean by *feature engineering*: using
    the existing features to create new features that increase the value of the original
    data by applying our knowledge of the data or domain in question. As mentioned
    earlier, you’ll look at advanced feature-engineering concepts and common types
    of data that need to be processed to be used by most algorithms. These include
    free-form text features for things such as web pages or tweets. Other important
    features can be extracted from images, video, and time-series data as well.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，我们所说的*特征工程*的含义应该不再令人惊讶：使用现有的特征来创建新的特征，通过应用我们对数据或相关领域的知识，从而提高原始数据的价值。如前所述，你将研究高级特征工程概念和大多数算法需要处理的常见数据类型。这些包括用于网页或推文的自由格式文本特征。其他重要的特征也可以从图像、视频和时间序列数据中提取。
- en: 2.2.4\. Data normalization
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4. 数据归一化
- en: Some ML algorithms require data to be *normalized*, meaning that each individual
    feature has been manipulated to reside on the same numeric scale. The value range
    of a feature can influence the importance of the feature compared to other features.
    If one feature has values between 0 and 10, and another has values between 0 and
    1, the weight of the first feature is 10, compared to the second. Sometimes you’ll
    want to force a particular feature weight, but typically it’s better to let the
    ML algorithm figure out the relative weights of the features. To make sure all
    features are considered equally, you need to normalize the data. Often data is
    normalized to be in the range from 0 to 1, or from –1 to 1.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习算法需要数据被*归一化*，这意味着每个单独的特征都已经被处理，使其位于相同的数值尺度上。一个特征的价值范围可以影响该特征相对于其他特征的重要性。如果一个特征的价值在0到10之间，而另一个特征的价值在0到1之间，那么第一个特征的重要性是第二个特征的10倍。有时你可能想要强制设置特定的特征权重，但通常让机器学习算法确定特征的相对权重会更好。为了确保所有特征都被同等考虑，你需要对数据进行归一化。通常，数据会被归一化到0到1的范围，或者从-1到1的范围。
- en: Let’s consider how this normalization is performed. The following code listing
    implements this function. For each feature, you want the data to be distributed
    between a minimum value (typically –1) and a maximum value (typically +1). To
    achieve this, you divide the data by the total range of the data in order to get
    the data into the 0–1 range. From here, you can re-extend to the required range
    (2, in the case of –1 to +1) by multiplying with this transformed value. At last,
    you move the starting point from 0 to the minimum required value (for example,
    –1).
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑这种归一化是如何进行的。以下代码示例实现了这个函数。对于每个特征，你希望数据分布在最小值（通常是-1）和最大值（通常是+1）之间。为了实现这一点，你需要将数据除以数据的总范围，以便将数据放入0-1的范围。从这里，你可以通过乘以这个转换后的值来重新扩展到所需的范围（在-1到+1的情况下是2）。最后，你将起点从0移动到所需的最小值（例如，-1）。
- en: Listing 2.3\. Feature normalization
  id: totrans-353
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.3. 特征归一化
- en: '[PRE4]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that you return both the normalized data and the factor with which the
    data was normalized. You do this because any new data (for example, for prediction)
    will have to be normalized in the same way in order to yield meaningful results.
    This also means that the ML modeler will have to remember how a particular feature
    was normalized, and save the relevant values (factor and minimum value).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你需要返回归一化的数据和用于归一化的因子。你这样做是因为任何新的数据（例如，用于预测）都必须以相同的方式进行归一化，才能得到有意义的结果。这也意味着机器学习模型构建者将不得不记住特定特征是如何归一化的，并保存相关的值（因子和最小值）。
- en: We leave it up to you to implement a function that takes new data, the normalization
    factor, and the normalized minimum value and reapplies the normalization.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现一个函数的任务留给你，该函数接受新的数据、归一化因子和归一化的最小值，并重新应用归一化。
- en: As you expand your data-wrangling toolkit and explore a variety of data, you’ll
    begin to see that each dataset has qualities that make it uniquely interesting,
    and often challenging. But large collections of data with many variables are hard
    to fully understand by looking at tabular representations. Graphical data-visualization
    tools are indispensable for understanding the data from which you hope to extract
    hidden information.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 当你扩展你的数据处理工具集并探索各种数据时，你会发现每个数据集都有其独特的品质，使其变得独特且通常具有挑战性。但是，具有许多变量的大量数据很难通过查看表格表示来完全理解。图形数据可视化工具对于理解你希望从中提取隐藏信息的数据至关重要。
- en: 2.3\. Using data visualization
  id: totrans-358
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3. 使用数据可视化
- en: Between data collection/preprocessing and ML model building lies the important
    step of data visualization. Data visualization serves as a sanity check of the
    training features and target variable before diving into the mechanics of machine
    learning and prediction. With simple visualization techniques, you can begin to
    explore the relationship between the input features and the output target variable,
    which will guide you in model building and assist in your understanding of the
    ML model and predictions. Further, visualization techniques can tell you how representative
    the training set is and inform you of the types of instances that may be lacking.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据收集/预处理和机器学习模型构建之间，数据可视化是一个重要的步骤。在深入机器学习的机制和预测之前，数据可视化充当了对训练特征和目标变量的合理性检查。通过简单的可视化技术，你可以开始探索输入特征与输出目标变量之间的关系，这将指导你在模型构建中的工作，并帮助你理解机器学习模型和预测。此外，可视化技术可以告诉你训练集的代表性和可能缺乏的实例类型。
- en: 'This section focuses on methods for visualizing the association between the
    target variable and the input features. We recommend four visualization techniques:
    mosaic plots, box plots, density plots, and scatter plots. Each technique is appropriate
    for a different type (numeric or categorical) of input feature and target variable,
    as shown in [figure 2.11](#ch02fig11).'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 本节重点介绍可视化目标变量与输入特征之间关联的方法。我们推荐四种可视化技术：约束图、箱线图、密度图和散点图。每种技术适用于不同类型（数值或分类）的输入特征和目标变量，如图
    2.11 所示。
- en: Figure 2.11\. Four visualization techniques, arranged by the type of input feature
    and response variable to be plotted
  id: totrans-361
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.11. 按输入特征和要绘制的响应变量类型排列的四种可视化技术
- en: '![](02fig11.jpg)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![图片](02fig11.jpg)'
- en: '|  |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Further reading**'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: 'A plethora of books are dedicated to statistical visualization and plotting
    data. If you’d like to dive deeper into this topic, check out the following:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多书籍致力于统计可视化和数据绘图。如果你想要深入了解这个主题，请查看以下内容：
- en: The classic textbook *The Visual Display of Quantitative Information* by Edward
    Tufte (Graphics Press, 2001) presents a detailed look into visualizing data for
    analysis and presentation.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经典教科书《定量信息的视觉展示》由 Edward Tufte（Graphics Press，2001）所著，详细介绍了数据可视化的分析和展示方法。
- en: For R users, *R Graphics Cookbook* by Winston Chang (O’Reilly, 2013) covers
    data visualization in R, from the basics to advanced topics, with code samples
    to follow along.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 R 用户来说，Winston Chang（O’Reilly，2013）所著的《R Graphics Cookbook》涵盖了 R 中的数据可视化，从基础知识到高级主题，并提供了代码示例以供参考。
- en: For Python users, *Python Data Visualization Cookbook* by Igor Milovanović,
    Dimitry Foures, and Giuseppe Vettigli (Packt Publishing, 2015) covers the basics
    to get you up and running with Matplotlib.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 Python 用户，Igor Milovanović、Dimitry Foures 和 Giuseppe Vettigli（Packt Publishing，2015）所著的《Python
    Data Visualization Cookbook》涵盖了从基础知识到使用 Matplotlib 运行的内容。
- en: '|  |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 2.3.1\. Mosaic plots
  id: totrans-370
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1. 约束图
- en: '*Mosaic plots* allow you to visualize the relationship between two or more
    categorical variables. Plotting software for mosaic plots is available in R, SAS,
    Python, and other scientific or statistical programming languages.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '*约束图* 允许你可视化两个或更多分类变量之间的关系。用于约束图的绘图软件在 R、SAS、Python 以及其他科学或统计编程语言中都有提供。'
- en: To demonstrate the utility of mosaic plots, you’ll use one to display the relationship
    between passenger gender and survival in the Titanic Passengers dataset. The mosaic
    plot begins with a square whose sides each have length 1\. The square is then
    divided, by vertical lines, into a set of rectangles whose widths correspond to
    the proportion of the data belonging to each of the categories of the input feature.
    For example, in the Titanic data, 24% of passengers were female, so you split
    the unit square along the x-axis into two rectangles corresponding to a width
    24% / 76% of the area.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示约束图的应用，你将使用它来显示乘客性别与泰坦尼克号乘客数据集中生存率之间的关系。约束图从一个边长为 1 的正方形开始。然后，通过垂直线将正方形分割成一系列矩形，其宽度对应于属于输入特征每个类别的数据比例。例如，在泰坦尼克号数据中，24%
    的乘客是女性，因此你将单位正方形沿 x 轴分割成两个矩形，其面积宽度为 24% / 76%。
- en: Next, each vertical rectangle is split by horizontal lines into subrectangles
    whose relative areas are proportional to the percent of instances belonging to
    each category of the response variable. For example, of Titanic passengers who
    were female, 74% survived (this is the *conditional probability* of survival,
    given that the passenger was female). Therefore, the Female rectangle is split
    by a horizontal line into two subrectangles that contain 74% / 26% of the area
    of the rectangle. The same is repeated for the Male rectangle (for males, the
    breakdown is 19% / 81%).
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，每个垂直矩形被水平线分割成子矩形，这些子矩形的相对面积与响应变量每个类别的实例百分比成比例。例如，在泰坦尼克号上的女性乘客中，74%幸存（这是在乘客为女性的条件概率）。因此，女性矩形被水平线分割成两个子矩形，包含矩形面积的74%
    / 26%。对男性矩形（男性中，比例是19% / 81%）也进行相同的处理。
- en: What results is a quick visualization of the relationship between gender and
    survival. If there is no relationship, the horizontal splits would occur at similar
    locations on the y-axis. If a strong relationship exists, the horizontal splits
    will be far apart. To enhance the visualization, the rectangles are shade-coded
    to assess the statistical significance of the relationship, compared to independence
    of the input feature and response variable, with large negative residuals (“lower
    count than expected”) shaded dark gray, and large positive residuals (“higher
    count than expected”) shaded light gray; see [figure 2.12](#ch02fig12).
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 这结果是一个关于性别与生存关系的快速可视化。如果没有关系，水平分割将在y轴上的相似位置发生。如果存在强烈的关系，水平分割将相隔甚远。为了增强可视化效果，矩形被着色编码以评估关系的统计显著性，与输入特征和响应变量的独立性相比，负残差较大（“低于预期计数”）被深灰色着色，正残差较大（“高于预期计数”）被浅灰色着色；参见[图2.12](#ch02fig12)。
- en: Figure 2.12\. Mosaic plot showing the relationship between gender and survival
    on the Titanic. The visualization shows that a much higher proportion of females
    (and much smaller proportion of males) survived than would have been expected
    if survival were independent of gender. “Women and children first.”
  id: totrans-375
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.12. 展示性别与泰坦尼克号生存关系的马赛克图。可视化显示，与性别独立于生存预期相比，女性的幸存比例要高得多（男性的幸存比例要小得多）。“妇女和儿童优先。”
- en: '![](02fig12_alt.jpg)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![图片](02fig12_alt.jpg)'
- en: 'This tells you that when building a machine-learning model to predict survival
    on the Titanic, gender is an important factor to include. It also allows you to
    perform a sanity check on the relationship between gender and survival: indeed,
    it’s common knowledge that a higher proportion of women survived the disaster.
    This gives you an extra layer of assurance that your data is legitimate. Such
    data visualizations can also help you interpret and validate your machine-learning
    models, after they’ve been built.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 这说明，在构建一个用于预测泰坦尼克号生存情况的机器学习模型时，性别是一个重要的因素需要考虑。它还允许你对性别与生存之间的关系进行合理性检查：确实，众所周知，女性在这次灾难中幸存的比例更高。这为你提供了额外的保证，确保你的数据是合法的。这样的数据可视化也有助于你在模型构建后解释和验证你的机器学习模型。
- en: '[Figure 2.13](#ch02fig13) shows another mosaic plot for survival versus passenger
    class (first, second, and third). As expected, a higher proportion of first-class
    passengers (and a lower proportion of third-class passengers) survived the sinking.
    Obviously, passenger class is also an important factor in an ML model to predict
    survival, and the relationship is exactly as you should expect: higher-class passengers
    had a higher probability of survival.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2.13](#ch02fig13) 展示了另一个关于生存与乘客等级（头等舱、二等舱和三等舱）的马赛克图。正如预期的那样，头等舱乘客的幸存比例更高（三等舱乘客的幸存比例更低）。显然，乘客等级也是预测生存的机器学习模型中的一个重要因素，关系正好如你所预期：等级更高的乘客有更高的生存概率。'
- en: Figure 2.13\. Mosaic plot showing the relationship between passenger class and
    survival on the Titanic
  id: totrans-379
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.13. 展示乘客等级与泰坦尼克号生存关系的马赛克图
- en: '![](02fig13_alt.jpg)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![图片](02fig13_alt.jpg)'
- en: 2.3.2\. Box plots
  id: totrans-381
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2. 箱线图
- en: '*Box plots* are a standard statistical plotting technique for visualizing the
    distribution of a numerical variable. For a single variable, a box plot depicts
    the *quartiles* of its distribution: the minimum, 25th percentile, median, 75th
    percentile, and maximum of the values. Box-plot visualization of a single variable
    is useful to get insight into the center, spread, and skew of its distribution
    of values plus the existence of any outliers.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '*箱线图* 是一种标准的统计绘图技术，用于可视化数值变量的分布。对于一个单一变量，箱线图描绘了其分布的 *四分位数*：最小值、25百分位数、中位数、75百分位数和最大值。单一变量的箱线图可视化有助于深入了解其值分布的中心、离散度和偏度，以及是否存在任何异常值。'
- en: You can also use box plots to compare distributions when plotted in parallel.
    In particular, they can be used to visualize the difference in the distribution
    of a numerical feature as a function of the various categories of a categorical
    response variable. Returning to the Titanic example, you can visualize the difference
    in ages between survivors and fatalities by using parallel box plots, as in [figure
    2.14](#ch02fig14). In this case, it’s not clear that any differences exist in
    the distribution of passenger ages of survivors versus fatalities, as the two
    box plots look fairly similar in shape and location.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用箱线图来比较在平行图中绘制的分布。特别是，它们可以用来可视化数值特征随分类响应变量的各种类别变化的分布差异。回到泰坦尼克号的例子，你可以通过使用平行箱线图来可视化幸存者和死亡者的年龄差异，如图2.14所示。在这种情况下，幸存者和死亡者的乘客年龄分布之间似乎没有明显的差异，因为两个箱线图在形状和位置上看起来相当相似。
- en: Figure 2.14\. Box plot showing the relationship between passenger age and survival
    on the Titanic. No noticeable differences exist between the age distributions
    for survivors versus fatalities. (This alone *shouldn’t* be a reason to exclude
    age from the ML model, as it may still be a predictive factor.)
  id: totrans-384
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.14\. 箱线图展示了乘客年龄与泰坦尼克号幸存率之间的关系。幸存者和死亡者的年龄分布之间没有明显的差异。（这本身 *不应该是* 排除年龄作为机器学习模型因素的理由，因为它可能仍然是一个预测因素。）
- en: '![](02fig14_alt.jpg)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![图片](02fig14_alt.jpg)'
- en: It’s important to recognize the limitations of visualization techniques. Visualizations
    aren’t a substitute for ML modeling! Machine-learning models can find and exploit
    subtle relationships hidden deep inside the data that aren’t amenable to being
    exposed via simple visualizations. You shouldn’t automatically exclude features
    whose visualizations don’t show clear associations with the target variable. These
    features could still carry a strong association with the target when used in association
    with other input features. For example, although age doesn’t show a clear relationship
    with survival, it could be that for third-class passengers, age is an important
    predictor (perhaps for third-class passengers, the younger and stronger passengers
    could make their way to the deck of the ship more readily than older passengers).
    A good ML model will discover and expose such a relationship, and thus the visualization
    alone isn’t meant to exclude age as a feature.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到可视化技术的局限性是很重要的。可视化不能替代机器学习建模！机器学习模型可以发现并利用数据深处隐藏的微妙关系，这些关系不适合通过简单的可视化来揭示。你不应该自动排除那些可视化没有显示出与目标变量明确关联的特征。这些特征在与其他输入特征一起使用时，可能仍然与目标变量有强烈的关联。例如，尽管年龄与生存率之间没有显示出明显的关联，但可能对于三等舱乘客来说，年龄是一个重要的预测因素（也许对于三等舱乘客，年轻且强壮的乘客比年长的乘客更容易到达船的甲板）。一个好的机器学习模型将发现并揭示这种关系，因此，可视化本身并不旨在排除年龄作为一个特征。
- en: '[Figure 2.15](#ch02fig15) displays box plots exploring the relationship between
    passenger fare paid and survival outcome. In the left panel, it’s clear that the
    distributions of fare paid are highly skewed (many small values and a few large
    outliers), making the differences difficult to visualize. This is remedied by
    a simple transformation of the fare (square root, in the right panel), making
    the differences easy to spot. Fare paid has an obvious relationship with survival
    status: those paying higher fares were more likely to survive, as is expected.
    Thus, fare amount should be included in the model, as you expect the ML model
    to find and exploit this positive association.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2.15](#ch02fig15) 展示了探索乘客票价与生存结果之间关系的箱线图。在左侧面板中，很明显，支付票价的分布高度偏斜（许多小值和一些大异常值），这使得差异难以可视化。通过票价（在右侧面板中为平方根）的简单转换，可以解决这个问题，使得差异易于发现。支付票价与生存状态有明显的关联：支付更高票价的人更有可能生存，正如预期的那样。因此，票价金额应该包含在模型中，因为你期望机器学习模型能够发现并利用这种正相关关系。'
- en: Figure 2.15\. Box plots showing the relationship between passenger fare paid
    and survival on the Titanic. The square-root transformation makes it obvious that
    passengers who survived paid higher fares, on average.
  id: totrans-388
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.15\. 展示乘客票价与泰坦尼克号生存之间关系的箱线图。平方根转换使得显而易见，幸存下来的乘客平均支付了更高的票价。
- en: '![](02fig15_alt.jpg)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig15_alt.jpg)'
- en: 2.3.3\. Density plots
  id: totrans-390
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3\. 密度图
- en: Now, we move to numerical, instead of categorical, response variables. When
    the input variable is categorical, you can use box plots to visualize the relationship
    between two variables, just as you did in the preceding section. You can also
    use density plots.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将转向数值型响应变量，而不是分类型响应变量。当输入变量是分类型时，你可以使用箱线图来可视化两个变量之间的关系，就像你在前一节中所做的那样。你还可以使用密度图。
- en: '*Density plots* display the distribution of a single variable in more detail
    than a box plot. First, a smoothed estimate of the probability distribution of
    the variable is estimated (typically using a technique called *kernel smoothing*).
    Next, that distribution is plotted as a curve depicting the values that the variable
    is likely to have. By creating a single density plot of the response variable
    for each category that the input feature takes, you can easily visualize any discrepancies
    in the values of the response variable for differences in the categorical input
    feature. Note that density plots are similar to histograms, but their smooth nature
    makes it much simpler to visualize multiple distributions in a single figure.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '*密度图*比箱线图更详细地显示了单个变量的分布。首先，估计变量的概率分布的平滑估计（通常使用称为*核平滑*的技术）。接下来，该分布被绘制为曲线，表示变量可能具有的值。通过为输入特征所采取的每个类别创建响应变量的单个密度图，你可以轻松地可视化响应变量值在分类输入特征差异中的任何差异。请注意，密度图与直方图类似，但它们的平滑特性使得在单个图中可视化多个分布变得简单得多。'
- en: In the next example, you’ll use the Auto MPG dataset.^([[2](#ch02fn02)]) This
    dataset contains the miles per gallon (MPG) attained by each of a large collection
    of automobiles from 1970–82, plus attributes about each auto, including horsepower,
    weight, location of origin, and model year. [Figure 2.16](#ch02fig16) presents
    a density plot for MPG versus location of origin (United States, Europe, or Asia).
    It’s clear from the plot that Asian cars tend to have higher MPG, followed by
    European and then American cars. Therefore, location should be an important predictor
    in our model. Further, a few secondary “bumps” in the density occur for each curve,
    which may be related to different types of automobile (for example, truck versus
    sedan versus hybrid). Thus, extra exploration of these secondary bumps is warranted
    to understand their nature and to use as a guide for further feature engineering.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个例子中，你将使用 Auto MPG 数据集.^([[2](#ch02fn02)]) 该数据集包含 1970-82 年间大量汽车每加仑行驶里程（MPG），以及每辆汽车的属性，包括马力、重量、产地和车型年份。[图
    2.16](#ch02fig16) 展示了 MPG 与产地（美国、欧洲或亚洲）的密度图。从图中可以看出，亚洲汽车的平均 MPG 较高，其次是欧洲汽车，然后是美国汽车。因此，产地应该是我们模型中的一个重要预测因子。此外，每个曲线在密度图中都有几个次级“峰值”，这可能与不同类型的汽车（例如，卡车与轿车与混合动力车）有关。因此，有必要对这些次级峰值进行额外探索，以了解其本质，并作为进一步特征工程指南。
- en: ²
  id: totrans-394
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-395
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Auto MPG dataset is available at [https://archive.ics.uci.edu/ml/datasets/Auto+MPG](https://archive.ics.uci.edu/ml/datasets/Auto+MPG)
    and is standard in the R programming language, by entering `data (mtcars)`.
  id: totrans-396
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Auto MPG 数据集可在 [https://archive.ics.uci.edu/ml/datasets/Auto+MPG](https://archive.ics.uci.edu/ml/datasets/Auto+MPG)
    获取，并在 R 编程语言中是标准的，通过输入 `data (mtcars)`。
- en: Figure 2.16\. Density plot for the Auto MPG dataset, showing the distribution
    of vehicle MPG for each manufacturer region. It’s obvious from the plot that Asian
    cars tend to have the highest MPG and that cars made in the United States have
    the lowest. Region is clearly a strong indicator of MPG.
  id: totrans-397
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.16. Auto MPG 数据集的密度图，显示了每个制造商地区的车辆每加仑英里数的分布。从图中可以看出，亚洲汽车往往具有最高的每加仑英里数，而美国制造的汽车每加仑英里数最低。地区显然是每加仑英里数的强指标。
- en: '![](02fig16_alt.jpg)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig16_alt.jpg)'
- en: 2.3.4\. Scatter plots
  id: totrans-399
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.4. 散点图
- en: A *scatter plot* is a simple visualization of the relationship between two numerical
    variables and is one of the most popular plotting tools in existence. In a scatter
    plot, the value of the feature is plotted versus the value of the response variable,
    with each instance represented as a dot. Though simple, scatter plots can reveal
    both linear and nonlinear relationships between the input and response variables.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '*散点图*是两个数值变量之间关系的简单可视化，是现存最受欢迎的绘图工具之一。在散点图中，特征值与响应变量的值相对应，每个实例用一个点表示。虽然简单，但散点图可以揭示输入变量和响应变量之间的线性关系和非线性关系。'
- en: '[Figure 2.17](#ch02fig17) shows two scatter plots: one of car weight versus
    MPG, and one of car model year versus MPG. In both cases, clear relationships
    exist between the input features and the MPG of the car, and hence both should
    be used in modeling. In the left panel is a clear banana shape in the data, showing
    a nonlinear decrease in MPG for increasing vehicle weight. Likewise, the right
    panel shows an increasing, linear relationship between MPG and the model year.
    Both plots clearly indicate that the input features are useful in predicting MPG,
    and both have the expected relationship.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2.17](#ch02fig17) 展示了两个散点图：一个是汽车重量与每加仑英里数的关系，另一个是汽车型号年份与每加仑英里数的关系。在两种情况下，输入特征与汽车的每加仑英里数之间存在明显的关系，因此两者都应用于建模。在左面板中，数据呈现出明显的香蕉形状，表明随着车辆重量的增加，每加仑英里数呈非线性下降。同样，右面板显示了每加仑英里数与型号年份之间的增加的线性关系。两个图表都清楚地表明，输入特征在预测每加仑英里数时是有用的，并且两者都具有预期的关系。'
- en: Figure 2.17\. Scatter plots for the relationship of vehicle miles per gallon
    versus vehicle weight (left) and vehicle model year (right)
  id: totrans-402
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.17. 车辆每加仑英里数与车辆重量（左）和车辆型号年份（右）的关系散点图
- en: '![](02fig17_alt.jpg)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![](02fig17_alt.jpg)'
- en: 2.4\. Summary
  id: totrans-404
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4. 摘要
- en: 'In this chapter, you’ve looked at important aspects of data in the context
    of real-world machine learning:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经在现实世界的机器学习背景下了解了数据的重要方面：
- en: 'Steps in compiling your training data include the following:'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编译训练数据的步骤包括以下内容：
- en: Deciding which input features to include
  id: totrans-407
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定包含哪些输入特征
- en: Figuring out how to obtain ground-truth values for the target variable
  id: totrans-408
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定如何获得目标变量的真实值
- en: Determining when you’ve collected enough training data
  id: totrans-409
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定何时收集了足够的训练数据
- en: Keeping an eye out for biased or nonrepresentative training data
  id: totrans-410
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意是否存在有偏或非代表性的训练数据
- en: 'Preprocessing steps for training data include the following:'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据的预处理步骤包括以下内容：
- en: Recoding categorical features
  id: totrans-412
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新编码分类特征
- en: Dealing with missing data
  id: totrans-413
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理缺失数据
- en: Feature normalization (for some ML approaches)
  id: totrans-414
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征归一化（对于某些机器学习方法）
- en: Feature engineering
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程
- en: 'Four useful data visualizations are mosaic plots, density plots, box plots,
    and scatter plots:'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 四种有用的数据可视化包括散点图、密度图、箱线图和散点图：
- en: '![](051fig01_alt.jpg)'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](051fig01_alt.jpg)'
- en: With our data ready for modeling, let’s now start building machine-learning
    models!
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的模型数据准备就绪后，现在让我们开始构建机器学习模型！
- en: 2.5\. Terms from this chapter
  id: totrans-419
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5. 本章术语
- en: '| Word | Definition |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| 单词 | 定义 |'
- en: '| --- | --- |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| dummy variable | A binary feature that indicates that an observation is (or
    isn’t) a member of a category |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| 虚拟变量 | 一个二元特征，表示一个观测值是否属于某个类别 |'
- en: '| ground truth | The value of a known target variable or label for a training
    or test set |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| 真实值 | 训练集或测试集中已知的目标变量或标签的值 |'
- en: '| missing data imputation | Those features with unknown values for a subset
    of instances Replacement of the unknown values of missing data with numerical
    or categorical values |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| 缺失数据插补 | 对于实例子集的未知值，用数值或分类值替换缺失数据的未知值 |'
- en: Chapter 3\. Modeling and prediction
  id: totrans-425
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3章\. 建模与预测
- en: '*This chapter covers*'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Discovering relationships in data through ML modeling
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过机器学习建模发现数据中的关系
- en: Using models for prediction and inference
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型进行预测和推理
- en: Building classification models
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建分类模型
- en: Building regression models
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建回归模型
- en: The previous chapter covered guidelines and principles of data collection, preprocessing,
    and visualization. The next step in the machine-learning workflow is to use that
    data to begin exploring and uncovering the relationships that exist between the
    input features and the target. In machine learning, this process is done by building
    statistical models based on the data. This chapter covers the basics required
    to understand ML modeling and to start building your own models. In contrast to
    most machine-learning textbooks, we spend little time discussing the various approaches
    to ML modeling, instead focusing attention on the big-picture concepts. This will
    help you gain a broad understanding of machine-learning model building and quickly
    get up to speed on building your own models to solve real-world problems. For
    those seeking more information about specific ML modeling techniques, please see
    the appendix.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 前一章介绍了数据收集、预处理和可视化的指南和原则。机器学习工作流程的下一步是利用这些数据开始探索和揭示输入特征与目标之间存在的关系。在机器学习中，这个过程是通过基于数据建立统计模型来完成的。本章涵盖了理解机器学习建模和开始构建自己的模型所需的基本知识。与大多数机器学习教科书不同，我们花费很少的时间讨论机器学习建模的各种方法，而是专注于大局概念。这将帮助您获得对机器学习模型构建的广泛理解，并快速掌握构建自己的模型以解决现实世界问题的技能。对于那些寻求有关特定机器学习建模技术更多信息的人，请参阅附录。
- en: 'We begin the chapter with a high-level overview of statistical modeling. This
    discussion focuses on the big-picture concepts of ML modeling, such as the purpose
    of models, the ways in which models are used in practice, and a succinct look
    at types of modeling techniques in existence and their relative strengths and
    weaknesses. From there, we dive into the two most common machine-learning models:
    classification and regression. In these sections, we give more details about how
    to build models on your data. We also call attention to a few of the most common
    algorithms used in practice in the “Algorithm highlight” boxes scattered throughout
    the chapter.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从对统计建模的高级概述开始本章。这次讨论侧重于机器学习建模的大局概念，例如模型的目的、模型在实际中的应用方式，以及现有建模技术类型及其相对优缺点简要概述。从那里，我们深入探讨两种最常用的机器学习模型：分类和回归。在这些部分中，我们提供了更多关于如何在您的数据上构建模型的信息。我们还通过本章中散布的“算法亮点”框，强调了实践中常用的几个常见算法。
- en: 3.1\. Basic machine-learning modeling
  id: totrans-433
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 基本机器学习建模
- en: 'The objective of machine learning is to discover patterns and relationships
    in data and to put those discoveries to use. This process of discovery is achieved
    through the use of modeling techniques that have been developed over the past
    30 years in statistics, computer science, and applied mathematics. These various
    approaches can range from simple to tremendously complex, but all share a common
    goal: to estimate the functional relationship between the input features and the
    target variable.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的目标是发现数据中的模式和关系，并将这些发现应用于实践。这一发现过程是通过使用在过去30年中在统计学、计算机科学和应用数学中开发的建模技术来实现的。这些各种方法可以从简单到极其复杂，但它们都有一个共同的目标：估计输入特征与目标变量之间的函数关系。
- en: 'These approaches also share a common workflow, as illustrated in [figure 3.1](#ch03fig01):
    use of historical data to build and optimize a model that is, in turn, used to
    make predictions based on new data. This section prepares you for the practical
    sections later in the chapter. You’ll look at the general goal of machine learning
    modeling in the next section, and move on to seeing how the end product can be
    used and a few important aspects for differentiating between ML algorithms.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法也共享一个共同的流程，如图3.1所示：使用历史数据构建和优化模型，然后反过来使用该模型根据新数据进行预测。本节为您准备本章后面部分的实际内容。您将在下一节中查看机器学习建模的一般目标，然后继续了解最终产品如何使用以及区分不同机器学习算法的一些重要方面。
- en: Figure 3.1\. The basic ML workflow
  id: totrans-436
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1\. 基本机器学习工作流程
- en: '![](03fig01.jpg)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig01.jpg)'
- en: 3.1.1\. Finding the relationship between input and target
  id: totrans-438
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 寻找输入与目标之间的关系
- en: Let’s frame the discussion of ML modeling around an example. Recall the Auto
    MPG dataset from [chapter 2](kindle_split_012.html#ch02). The dataset contains
    metrics about automobiles, such as manufacturer region, model year, vehicle weight,
    horsepower, and number of cylinders. The purpose of the dataset is to understand
    the relationship between the input features and a vehicle’s miles per gallon (MPG)
    rating.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们围绕一个示例来讨论机器学习建模。回想一下第2章中的Auto MPG数据集[章节2](kindle_split_012.html#ch02)。该数据集包含有关汽车的各种指标，例如制造商地区、车型年份、车辆重量、马力和气缸数。该数据集的目的是了解输入特征与车辆每加仑英里数（MPG）评级之间的关系。
- en: Input features are typically referred to using the symbol X, with subscripts
    differentiating inputs when multiple input features exist. For instance, we’ll
    say that X[1] refers to manufacturer region, X[2] to model year, X[3] to vehicle
    weight, and so forth. The collection of all the input features is referred to
    as the bold **X**. Likewise, the target variable is typically referred to as Y.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 输入特征通常用符号X表示，当存在多个输入特征时，下标用于区分输入。例如，我们将X[1]表示为制造商地区，X[2]表示为车型年份，X[3]表示为车辆重量，等等。所有输入特征的集合被称为粗体**X**。同样，目标变量通常被称为Y。
- en: 'The relationship between the inputs, **X**, and output, Y, can be succinctly
    represented by this simple formula:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 输入X和输出Y之间的关系可以用这个简单的公式简洁地表示：
- en: '![](054fig01.jpg)'
  id: totrans-442
  prefs: []
  type: TYPE_IMG
  zh: '![](054fig01.jpg)'
- en: In this equation, f represents the unknown function that relates the input variables
    to the target, Y. The goal of ML modeling is to accurately estimate f by *using
    data.* The symbol € represents random noise in the data that’s unrelated to the
    function f. The function f is commonly referred to as the *signal*, whereas the
    random variable € is called the *noise*. The challenge of machine learning is
    to use data to determine what the true signal is, while ignoring the noise.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，f代表与输入变量相关联的目标变量Y的未知函数。机器学习建模的目标是通过*使用数据*来准确估计f。符号€代表与函数f无关的数据中的随机噪声。函数f通常被称为*信号*，而随机变量€被称为*噪声*。机器学习的挑战是使用数据来确定真正的信号是什么，同时忽略噪声。
- en: 'In the Auto MPG example, the function f describes the true MPG rating for an
    automobile as a function of that car’s many input features. If you knew that function
    perfectly, you could know the MPG rating for any car, real or fictional. But you
    could have numerous sources of noise, €, including (and certainly not limited
    to) the following:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 在Auto MPG示例中，函数f描述了汽车的真实每加仑英里数（MPG）评级作为该汽车众多输入特征的一个函数。如果你完全了解这个函数，你就可以知道任何汽车（无论是真实的还是虚构的）的MPG评级。但是，你可能会有许多噪声源，€，包括（但不仅限于）以下内容：
- en: Imperfect measurement of each vehicle’s MPG rating caused by small inaccuracies
    in the measuring devices—measurement noise
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于测量设备中的微小不准确造成的每辆车的MPG评级的不完美测量——测量噪声
- en: Variations in the manufacturing process, causing each car in the fleet to have
    slightly different MPG measurements—manufacturing process noise
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制造过程中的变化，导致车队中的每辆车的MPG测量值略有不同——制造过程噪声
- en: Noise in the measurement of the input features, such as weight and horsepower
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入特征测量中的噪声，例如重量和马力
- en: Lack of access to the broader set of features that would exactly determine MPG
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法访问确切确定MPG的更广泛的特征集
- en: Using the noisy data that you have from hundreds of vehicles, the ML approach
    is to use modeling techniques to find a good estimate for f. This resultant estimate
    is referred to as an *ML model*.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 使用你从数百辆车辆中获得的噪声数据，机器学习方法是通过建模技术找到f的良好估计。这个结果估计被称为*机器学习模型*。
- en: In [sections 3.2](#ch03lev1sec2) and [3.3](#ch03lev1sec3), we describe in further
    detail how these ML modeling techniques work. Indeed, the bulk of the academic
    literature on machine learning deals with how to best estimate f.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3.2节](#ch03lev1sec2)和[3.3节](#ch03lev1sec3)中，我们更详细地描述了这些机器学习建模技术是如何工作的。实际上，机器学习的大部分学术文献都涉及如何最好地估计f。
- en: 3.1.2\. The purpose of finding a good model
  id: totrans-451
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2. 寻找好模型的目的
- en: 'Assuming that you have a good estimate of f, what next? Machine learning has
    two main goals: *prediction* and *inference*.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个对f的良好估计，接下来是什么？机器学习有两个主要目标：*预测*和*推理*。
- en: Prediction
  id: totrans-453
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 预测
- en: 'After you have a model, you can use that model to generate predictions of the
    target, Y, for new data, **X**[new], by plugging those new features into the model.
    In mathematical notation, if f[est] denotes your machine-learning estimate of
    f (recall that f denotes the true relationship between the features and the target),
    then predictions for new data can be obtained by plugging the new data into this
    formula:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在你有一个模型之后，你可以通过将这些新特征插入模型来生成对新数据的目标Y的预测，**X**[new]。用数学符号表示，如果f[est]表示你对f的机器学习估计（回想一下，f表示特征与目标之间的真实关系），那么通过将新数据插入此公式可以获得对新数据的预测：
- en: Y[pred] = f[est](**X**[new])
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: Y[pred] = f[est](**X**[new])
- en: These predictions can then be used to make decisions about the new data or may
    be fed into an automated workflow.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预测可以用来对新数据做出决策，或者可能被输入到自动化工作流程中。
- en: Going back to the Auto MPG example, suppose that you have an ML model, f[est],
    that describes the relationship between MPG and the input metrics of an automobile.
    Prediction allows you to ask the question, “What would the MPG of a certain automobile
    with known input metrics be?” Such a predictive ability would be useful for designing
    automobiles, because it would allow engineers to assess the MPG rating of different
    design concepts and to ensure that the individual concepts meet MPG requirements.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 回到Auto MPG示例，假设你有一个机器学习模型f[est]，它描述了MPG与汽车输入指标之间的关系。预测能力允许你提出这样的问题：“已知输入指标的特定汽车的MPG是多少？”这种预测能力对于设计汽车非常有用，因为它允许工程师评估不同设计概念的MPG评级，并确保各个概念满足MPG要求。
- en: 'Prediction is the most common use of machine-learning systems. Prediction is
    central to many ML use cases, including these:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 预测是机器学习系统最常见的使用方式。预测是许多机器学习用例的核心，包括以下这些：
- en: Deciphering handwritten digits or voice recordings
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析手写数字或语音录音
- en: Predicting the stock market
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测股市
- en: Forecasting
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测
- en: Predicting which users are most likely to click, convert, or buy
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测哪些用户最有可能点击、转化或购买
- en: Predicting which users will need product support and which are likely to unsubscribe
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测哪些用户需要产品支持以及哪些用户可能会取消订阅
- en: Determining which transactions are fraudulent
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定哪些交易是欺诈的
- en: Making recommendations
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供建议
- en: Because of the high levels of predictive accuracy attained by machine-learning
    approaches and the rapid speed by which ML predictions can be generated, ML is
    used every day by thousands of companies for predictive purposes.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习方法达到了高预测准确率，以及ML预测可以快速生成，因此成千上万的公司在预测目的上每天都在使用ML。
- en: Inference
  id: totrans-467
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 推理
- en: 'In addition to making predictions on new data, you can use machine-learning
    models to better understand the relationships between the input features and the
    output target. A good estimate of f can enable you to answer deep questions about
    the associations between the variables at hand. For example:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对新数据进行预测外，你还可以使用机器学习模型更好地理解输入特征与输出目标之间的关系。对f的良好估计能够帮助你回答关于手头变量之间关联的深入问题。例如：
- en: Which input features are most strongly related to the target variable?
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些输入特征与目标变量最密切相关？
- en: Are those relationships positive or negative?
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些关系是正向的还是负向的？
- en: Is f a simple relationship, or is it a function that’s more nuanced and nonlinear?
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: f是一个简单的关系，还是一个更微妙和非线性的函数？
- en: 'These inferences can tell you a lot about the data-generating process and give
    clues to the factors driving relationships in the data. Returning to the Auto
    MPG example, you can use inference to answer questions such as these: Does manufacturer
    region have an effect on MPG? Which of the inputs are most strongly related to
    MPG? And are they negatively or positively related? Answers to these questions
    can give you an idea of the driving factors in automobile MPG and give clues about
    how to engineer vehicles with higher MPG.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 这些推理可以告诉你很多关于数据生成过程的信息，并为数据中关系驱动因素提供线索。回到Auto MPG示例，你可以使用推理来回答这些问题：制造商地区对MPG有影响吗？哪些输入与MPG最密切相关？它们是负相关还是正相关？对这些问题的回答可以给你一个关于汽车MPG驱动因素的概念，并为你提供如何设计更高MPG车辆的线索。
- en: 3.1.3\. Types of modeling methods
  id: totrans-473
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3\. 建模方法类型
- en: Now the time has come to dust off your statistics knowledge and dive into some
    of the mathematical details of ML modeling. Don’t worry—we’ll keep the discussion
    relatively broad and understandable for those without much of a statistics background!
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候拂去你的统计知识灰尘，深入探讨机器学习建模的一些数学细节了。别担心——我们将保持讨论相对广泛和易于理解，即使对于那些没有太多统计背景的人来说也是如此！
- en: Statistical modeling has a general trade-off between predictive accuracy and
    model interpretability. Simple models are easy to interpret, yet won’t produce
    accurate predictions (particularly for complicated relationships). Complex models
    may produce accurate predictions, but may be black-box and hard to interpret.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 统计建模在预测准确性和模型可解释性之间有一个普遍的权衡。简单的模型容易解释，但不会产生准确的预测（尤其是对于复杂的关系）。复杂的模型可能会产生准确的预测，但可能是黑盒且难以解释。
- en: 'In addition, the machine-learning model has two main types: parametric and
    nonparametric. The essential difference is that parametric models assume that
    f takes a specific functional form, whereas nonparametric models don’t make such
    strict assumptions. Therefore, parametric approaches tend to be simple and interpretable,
    but less accurate. Likewise, nonparametric approaches are usually less interpretable
    but more accurate across a broad range of problems. Let’s take a closer look at
    both parametric and nonparametric approaches to ML modeling.'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，机器学习模型主要有两种类型：参数和非参数。本质区别在于参数模型假设 f 采用特定的函数形式，而非参数模型则不做这样的严格假设。因此，参数方法通常简单且可解释，但准确性较低。同样，非参数方法在广泛的问题上通常可解释性较低但准确性更高。让我们更详细地看看参数和非参数方法在机器学习建模中的应用。
- en: Parametric methods
  id: totrans-477
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 参数方法
- en: 'The simplest example of a parametric approach is linear regression. In linear
    regression, f is assumed to be a linear combination of the numerical values of
    the inputs. The standard linear regression model is as follows:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 参数方法的简单例子是线性回归。在线性回归中，假设 f 是输入数值的线性组合。标准的线性回归模型如下：
- en: f(**X**) = β[0] + X[1] × β[1] + X[2] × β[2] + **...**
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: f(**X**) = β[0] + X[1] × β[1] + X[2] × β[2] + **...**
- en: In this equation, the unknown parameters, β[0], β[1],... can be interpreted
    as the intercept and slope parameters (with respect to each of the inputs). When
    you fit a parametric model to some data, you estimate the best values of each
    of the unknown parameters. Then you can turn around and plug those estimates into
    the formula for f(**X**) along with new data to generate predictions.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，未知参数 β[0]、β[1]、... 可以解释为截距和斜率参数（相对于每个输入）。当你将参数模型拟合到某些数据时，你估计每个未知参数的最佳值。然后你可以将这些估计值代入
    f(**X**) 的公式中，并加入新数据以生成预测。
- en: Other examples of commonly used parametric models include logistic regression,
    polynomial regression, linear discriminant analysis, quadratic discriminant analysis,
    (parametric) mixture models, and naïve Bayes (when parametric density estimation
    is used). Approaches often used in conjunction with parametric models for model
    selection purposes include ridge regression, lasso, and principal components regression.
    Further details about some of these methods are given later in this chapter, and
    a description of each approach is given in the appendix.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的参数模型的其他例子包括逻辑回归、多项式回归、线性判别分析、二次判别分析、（参数）混合模型和朴素贝叶斯（当使用参数密度估计时）。在模型选择目的上常与参数模型结合使用的方法包括岭回归、lasso和主成分回归。关于这些方法的一些更详细信息将在本章后面给出，每种方法的描述将在附录中提供。
- en: The drawback of parametric approaches is that they make strong assumptions about
    the true form of the function f. In most real-world problems, f doesn’t assume
    such a simple form, especially when there are many input variables (X). In these
    situations, parametric approaches will fit the data poorly, leading to inaccurate
    predictions. Therefore, most real-world approaches to machine learning depend
    on nonparametric machine-learning methods.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 参数方法的缺点是它们对函数 f 的真实形式做出了强烈的假设。在大多数现实世界的问题中，f 不假设这种简单的形式，尤其是在有多个输入变量（X）的情况下。在这些情况下，参数方法将数据拟合得不好，导致预测不准确。因此，大多数现实世界的机器学习方法依赖于非参数机器学习方法。
- en: Nonparametric methods
  id: totrans-483
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 非参数方法
- en: In *nonparametric* models, f doesn’t take a simple, fixed function. Instead,
    the form and complexity of f adapts to the complexity of the data. For example,
    if the relationship between X and Y is wiggly, a nonparametric approach will choose
    a function f that matches the curvy patterns. Likewise, if the relationship between
    the input and output variable is smooth, a simple function f will be chosen.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 在*非参数*模型中，f不采用简单的、固定的函数。相反，f的形式和复杂性会适应数据的复杂性。例如，如果X和Y之间的关系是曲折的，非参数方法会选择一个匹配曲线模式的函数f。同样，如果输入变量和输出变量之间的关系是平滑的，将选择一个简单的函数f。
- en: A simple example of a nonparametric model is a classification tree. A *classification
    tree* is a series of recursive binary decisions on the input features. The classification
    tree learning algorithm uses the target variable to learn the optimal series of
    splits such that the terminal leaf nodes of the tree contain instances with similar
    values of the target.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 非参数模型的一个简单例子是分类树。*分类树*是一系列关于输入特征的递归二进制决策。分类树学习算法使用目标变量来学习最优的分割序列，使得树的终端叶节点包含具有相似目标值的实例。
- en: Take, for example, the Titanic Passengers dataset. The classification tree algorithm
    first seeks the best input feature to split on, such that the resulting leaf nodes
    contain passengers who either mostly lived or mostly died. In this case, the best
    split is on the sex (male/female) of the passenger. The algorithm continues splitting
    on other input features in each of the subnodes until the algorithm can no longer
    detect any good subsequent splits.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 以泰坦尼克号乘客数据集为例。分类树算法首先寻找最佳的输入特征进行分割，使得结果叶节点包含要么大部分生存要么大部分死亡的乘客。在这种情况下，最佳的分割是乘客的性别（男性/女性）。算法继续在每个子节点上对其他输入特征进行分割，直到算法无法再检测到任何好的后续分割。
- en: 'Classification trees are nonparametric because the depth and complexity of
    the tree isn’t fixed in advance, but rather is learned from the data. If the relationship
    between the target variable and the input features is complex and there’s a sufficient
    amount of data, then the tree will grow deeper, uncovering more-nuanced patterns.
    [Figure 3.2](#ch03fig02) shows two classification trees learned from different
    subsets of the Titanic Passengers dataset. In the left panel is a tree learned
    from only 400 passengers: the resultant model is simple, consisting of only a
    single split. In the right panel is a tree learned from 891 passengers: the larger
    amount of data enables the model to grow in complexity and find more-detailed
    patterns in the data.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 分类树是非参数的，因为树的深度和复杂性不是预先固定的，而是从数据中学习得到的。如果目标变量与输入特征之间的关系复杂，并且有足够的数据量，那么树将生长得更深，揭示出更细微的模式。[图3.2](#ch03fig02)展示了从不同的泰坦尼克号乘客数据集子集中学习到的两个分类树。在左侧面板中，是从只有400名乘客中学习到的树：得到的模型很简单，只包含一个分割。在右侧面板中，是从891名乘客中学习到的树：更多的数据量使得模型能够增加复杂性，并在数据中找到更详细的模式。
- en: Figure 3.2\. A decision tree is an example of a nonparametric ML algorithm,
    because its functional form isn’t fixed. The tree model can grow in complexity
    with larger amounts of data to capture more-complicated patterns. In each terminal
    node of the tree, the ratio represents the number of training instances in that
    node that died versus lived.
  id: totrans-488
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.2\. 决策树是非参数机器学习算法的一个例子，因为它的函数形式不是固定的。树模型可以随着数据量的增加而增加复杂性，以捕捉更复杂的模式。在树的每个终端节点中，比例代表该节点中死亡与生存的训练实例数。
- en: '![](03fig02_alt.jpg)'
  id: totrans-489
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig02_alt.jpg)'
- en: Other examples of nonparametric approaches to machine learning include k-nearest
    neighbors, splines, basis expansion methods, kernel smoothing, generalized additive
    models, neural nets, bagging, boosting, random forests, and support vector machines.
    Again, more details about some of these methods are given later in this chapter,
    and a description of each approach is given in the appendix.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 非参数机器学习方法的其他例子包括k近邻、样条、基函数展开方法、核平滑、广义加性模型、神经网络、Bagging、Boosting、随机森林和支持向量机。同样，关于这些方法的一些更详细的内容将在本章后面给出，每种方法的描述将在附录中提供。
- en: 3.1.4\. Supervised versus unsupervised learning
  id: totrans-491
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4\. 监督学习与无监督学习
- en: 'Machine-learning problems fall into two camps: supervised and unsupervised.
    *Supervised problems* are ones in which you have access to the target variable
    for a set of training data, and *unsupervised problems* are ones in which there’s
    no identified target variable.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习问题可以分为两大类：监督学习和无监督学习。*监督学习问题*是指对于一组训练数据，你可以访问目标变量，而*无监督学习问题*是指没有明确的目标变量。
- en: All the examples so far in this book fall in the supervised camp. These problems
    each contain a target of interest (Did the Titanic passenger survive? Did the
    customer churn? What’s the MPG?) and a set of training data with known values
    of the target. Indeed, most problems in machine learning are supervised in nature,
    and most ML techniques are designed to solve supervised problems. We spend the
    vast majority of this book describing how to solve supervised problems.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 本书迄今为止的所有示例都属于监督学习范畴。这些问题每个都包含一个感兴趣的目标（泰坦尼克号乘客是否幸存？客户是否流失？MPG是多少？）以及一组具有目标已知值的训练数据。实际上，机器学习中的大多数问题本质上是监督性的，大多数机器学习技术都是设计用来解决监督学习问题的。我们在这本书的大部分内容中描述了如何解决监督学习问题。
- en: 'In unsupervised learning, you have access to only input features, and don’t
    have an associated target variable. So what kinds of analyses can you perform
    if there’s no target available? The unsupervised learning approach has two main
    classes:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，你只能访问输入特征，没有相关的目标变量。如果没有任何目标可用，你能进行哪些类型的分析？无监督学习方法有两个主要类别：
- en: '***Clustering—*** Use the input features to discover natural groupings in the
    data and to divide the data into those groups. Methods: k-means, Gaussian mixture
    models, and hierarchical clustering.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***聚类——*** 使用输入特征来发现数据中的自然分组，并将数据划分为这些组。方法：k-means、高斯混合模型和层次聚类。'
- en: '***Dimensionality reduction—*** Transform the input features into a small number
    of coordinates that capture most of the variability of the data. Methods: principal
    component analysis (PCA), multidimensional scaling, manifold learning.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***降维——*** 将输入特征转换为少数坐标，这些坐标可以捕捉到数据的大部分变异性。方法：主成分分析（PCA）、多维缩放和流形学习。'
- en: Both clustering and dimensionality reduction have wide popularity (particularly,
    k-means and PCA), yet are often abused and used inappropriately when a supervised
    approach is warranted.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类和降维（尤其是k-means和PCA）非常受欢迎，但在需要监督方法的情况下，它们往往被滥用和不恰当地使用。
- en: But unsupervised problems do play a significant role in machine learning, often
    in support of supervised problems, either to help compile training data for learning
    or to derive new input features on which to learn. You’ll return to the topic
    of unsupervised learning in [chapter 8](kindle_split_019.html#ch08).
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 但无监督问题在机器学习中确实扮演着重要的角色，通常在支持监督问题方面发挥作用，要么帮助编译学习所需的数据，要么推导出新的输入特征以进行学习。你将在第8章[返回无监督学习的话题](kindle_split_019.html#ch08)。
- en: 'Now, let’s transition to the more practical aspects of ML modeling. Next we
    describe the steps needed to start building models on your own data and the practical
    considerations of choosing which algorithm to use. We break up the rest of the
    chapter into two sections corresponding to the two most common problems in machine
    learning: classification and regression. We begin with the topic of classification.'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们转向机器学习建模的更实际方面。接下来，我们将描述开始在自己的数据上构建模型所需的步骤，以及选择使用哪种算法的实际考虑。我们将本章的其余部分分为两个部分，对应于机器学习中最常见的两个问题：分类和回归。我们首先从分类这个主题开始。
- en: '3.2\. Classification: predicting into buckets'
  id: totrans-500
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2. 分类：预测到桶中
- en: In machine learning, *classification* describes the prediction of new data into
    buckets (classes) by using a *classifier* built by the machine-learning algorithm.
    Spam detectors put email into Spam and No Spam buckets, and handwritten digit
    recognizers put images into buckets from 0 through 9, for example. In this section,
    you’ll learn how to build classifiers based on the data at hand. [Figure 3.3](#ch03fig03)
    illustrates the process of classification.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，*分类*描述了使用机器学习算法构建的*分类器*将新数据预测到桶（类别）中的过程。例如，垃圾邮件检测器将电子邮件放入垃圾邮件和非垃圾邮件桶中，手写数字识别器将图像放入从0到9的桶中。在本节中，你将学习如何根据手头的数据构建分类器。[图3.3](#ch03fig03)说明了分类的过程。
- en: Figure 3.3\. A classification process. Rectangles and circles are divided by
    a classifier into classes A and B. This is a case of binary classification with
    only two classes.
  id: totrans-502
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.3。一个分类过程。矩形和圆形被分类器分为A和B两类。这是一个只有两个类别的二元分类案例。
- en: '![](03fig03.jpg)'
  id: totrans-503
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig03.jpg)'
- en: Let’s again use an example. In [chapter 2](kindle_split_012.html#ch02), you
    looked at the Titanic Passengers dataset for predicting survival of passengers
    onboard the ill-fated ship. [Figure 3.4](#ch03fig04) shows a subset of this data.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次用一个例子来说明。在[第2章](kindle_split_012.html#ch02)中，你研究了泰坦尼克号乘客数据集，以预测船上乘客的生存情况。[图3.4](#ch03fig04)显示了这些数据的一个子集。
- en: Figure 3.4\. A subset of the Titanic Passengers dataset
  id: totrans-505
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.4\. 泰坦尼克号乘客数据集的一个子集
- en: '![](03fig04_alt.jpg)'
  id: totrans-506
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig04_alt.jpg)'
- en: As we’ve previously discussed, typically the best way to start an ML project
    is to get a feel for the data by visualizing it. For example, it’s considered
    common knowledge that more women than men survived the Titanic, and you can see
    that this is the case from the mosaic plot in [figure 3.5](#ch03fig05) (if you’ve
    forgotten about mosaic plots, look back at [section 2.3.1](kindle_split_012.html#ch02lev2sec9)).
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前讨论的，通常开始一个机器学习项目的最佳方式是通过可视化数据来感受数据。例如，普遍认为女性比男性在泰坦尼克号上幸存的可能性更大，你可以从[图3.5](#ch03fig05)中的频率图中看到这一点（如果你忘记了频率图，请回顾[第2.3.1节](kindle_split_012.html#ch02lev2sec9)）。
- en: Figure 3.5\. Mosaic plot showing overwhelming support for the idea that more
    women than men survived the disaster.
  id: totrans-508
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.5\. 频率图显示女性比男性在灾难中幸存的可能性更大。
- en: '![](03fig05_alt.jpg)'
  id: totrans-509
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig05_alt.jpg)'
- en: 'By using the visualization techniques in [section 2.3](kindle_split_012.html#ch02lev1sec3),
    you can get a feeling for the performance of each feature in the Titanic Passengers
    dataset. But it’s important to realize that just because a single feature looks
    good or bad, it doesn’t necessarily show the performance of the feature in combination
    with one or more other features. Maybe the age together with the sex and social
    status divides the passengers much better than any single feature. In fact, this
    is one of the main reasons to use machine-learning algorithms in the first place:
    to find signals in many dimensions that humans can’t discover easily.'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用[第2.3节](kindle_split_012.html#ch02lev1sec3)中的可视化技术，你可以感受到泰坦尼克号乘客数据集中每个特征的表现。但重要的是要意识到，仅仅因为单个特征看起来好或不好，并不意味着它与其他一个或多个特征结合时能表现出该特征的性能。也许年龄与性别和社会地位的结合比任何单个特征都能更好地划分乘客。实际上，这正是最初使用机器学习算法的主要原因之一：寻找人类难以轻易发现的多个维度上的信号。
- en: The following subsections introduce the methodology for building classification
    models and making predictions. You’ll look at a few specific algorithms and the
    difference between linear and nonlinear algorithms.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的子节介绍了构建分类模型和进行预测的方法。你将了解一些具体的算法以及线性算法和非线性算法之间的区别。
- en: 3.2.1\. Building a classifier and making predictions
  id: totrans-512
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 构建分类器并进行预测
- en: The first order of business is to choose the classification algorithm to use
    for building the classifier. Many algorithms are available, and each has pros
    and cons for different data and deployment requirements. The appendix provides
    a table of algorithms and a comparison of their properties. You’ll use this table
    throughout the book for selecting algorithms to try for different problems. In
    this section, the choice of algorithm isn’t essential; in the next chapter, you’ll
    learn how to properly measure the performance of the algorithm and choose the
    best for the job.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 首要任务是选择用于构建分类器的分类算法。有许多算法可供选择，每种算法针对不同的数据和部署需求都有其优缺点。附录提供了一个算法表及其特性的比较。你将在本书中通过这个表来选择尝试不同问题的算法。在本节中，算法的选择不是必要的；在下一章，你将学习如何正确衡量算法的性能并选择最适合的算法。
- en: The next step is to ready the data for modeling. After exploring some of the
    features in the dataset, you may want to preprocess the data to deal with categorical
    features, missing values, and so on (as discussed in [chapter 2](kindle_split_012.html#ch02)).
    The preprocessing requirements are also dependent on the specific algorithm, and
    the appendix lists these requirements for each algorithm.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是为建模准备数据。在探索数据集的一些特征之后，你可能想要预处理数据以处理分类特征、缺失值等问题（如[第2章](kindle_split_012.html#ch02)中所述）。预处理需求也取决于特定的算法，附录中列出了每个算法的需求。
- en: 'For the Titanic survival model, you’ll start by choosing a simple classification
    algorithm: logistic regression.^([[1](#ch03fn01)]) For logistic regression, you
    need to do the following:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 对于泰坦尼克号生存模型，你将首先选择一个简单的分类算法：逻辑回归.^([[1](#ch03fn01)]) 对于逻辑回归，你需要做以下几步：
- en: ¹
  id: totrans-516
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-517
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The *regression* in logistic regression doesn’t mean it’s a regression algorithm.
    Logistic regression expands linear regression with a logistic function to make
    it suitable for classification.
  id: totrans-518
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 逻辑回归中的*回归*并不意味着它是一个回归算法。逻辑回归通过逻辑函数扩展线性回归，使其适合分类。
- en: Impute missing values.
  id: totrans-519
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填充缺失值。
- en: Expand categorical features.
  id: totrans-520
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扩展分类特征。
- en: From [chapter 2](kindle_split_012.html#ch02), you know that the Fare feature
    is heavily skewed. In this situation, it’s advantageous (for some ML models) to
    transform the variable to make the feature distribution more symmetric and to
    reduce the potentially harmful impact of outliers. Here, you’ll choose to transform
    Fare by taking the square root.
  id: totrans-521
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[第2章](kindle_split_012.html#ch02)中，你知道票价特征严重倾斜。在这种情况下，将变量转换为使特征分布更加对称，并减少异常值可能产生的有害影响是有利的（对于某些机器学习模型）。在这里，你将选择通过开平方根来转换票价。
- en: The final dataset that you’ll use for modeling is shown in [figure 3.6](#ch03fig06).
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 你将用于建模的最终数据集在[图3.6](#ch03fig06)中显示。
- en: Figure 3.6\. The first five rows of the Titanic Passengers dataset after processing
    categorical features and missing values, and transforming the Fare variable by
    taking the square root (see the `prepare_data` function in the source code repository).
    All features are now numerical, which is the preferred format for most ML algorithms.
  id: totrans-523
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '[图3.6](#ch03fig06)。在处理分类特征和缺失值，并将票价变量通过开平方根转换后的泰坦尼克号乘客数据集的前五行（参见源代码仓库中的`prepare_data`函数）。所有特征现在都是数值型，这是大多数机器学习算法首选的格式。'
- en: '![](03fig06_alt.jpg)'
  id: totrans-524
  prefs: []
  type: TYPE_IMG
  zh: '![图片](03fig06_alt.jpg)'
- en: You can now go ahead and build the model by running the data through the logistic
    regression algorithm. This algorithm is implemented in the scikit-learn Python
    package, and the model-building and prediction code look like the following listing.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以通过运行数据通过逻辑回归算法来构建模型。该算法在scikit-learn Python包中实现，模型构建和预测代码如下所示。
- en: Listing 3.1\. Building a logistic regression classifier with scikit-learn
  id: totrans-526
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '[列表3.1](#ch03list01)。使用scikit-learn构建逻辑回归分类器'
- en: '![](062fig01_alt.jpg)'
  id: totrans-527
  prefs: []
  type: TYPE_IMG
  zh: '![图片](062fig01_alt.jpg)'
- en: After building the model, you predict the survival of previously unseen passengers
    based on their features. The model expects features in the format given in [figure
    3.6](#ch03fig06), so any new passengers will have to be run through exactly the
    same processes as the training data. The output of the `predict` function will
    be 1 if the passenger is predicted to survive, and 0 otherwise.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建模型后，你可以根据乘客的特征预测之前未见过的乘客的生存情况。模型期望的特征格式如[图3.6](#ch03fig06)所示，因此任何新的乘客都必须经过与训练数据完全相同的处理过程。`predict`函数的输出将为1，如果预测乘客幸存，否则为0。
- en: It’s useful to visualize the classifier by plotting the decision boundary. Given
    two of the features in the dataset, you can plot the boundary that separates surviving
    passengers from the dead, according to the model. [Figure 3.7](#ch03fig07) shows
    this for the Age and square-root Fare features.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 通过绘制决策边界来可视化分类器是有用的。[图3.7](#ch03fig07)展示了根据模型，使用数据集中的两个特征（年龄和票价平方根）来区分幸存乘客和死亡乘客的边界。
- en: Figure 3.7\. The decision boundary for the Age and sqrt(Fare) features. The
    diamonds show passengers who survived, whereas circles denote passengers who died.
    The light background denotes the combinations of Age and Fare that are predicted
    to yield survival. Notice that a few instances overlap the boundary. The classifier
    isn’t perfect, but you’re looking in only two dimensions. The algorithm on the
    full dataset finds this decision boundary in 10 dimensions, but that becomes harder
    to visualize.
  id: totrans-530
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '[图3.7](#ch03fig07)。年龄和票价平方根特征的决策边界。菱形表示幸存乘客，而圆圈表示死亡乘客。浅色背景表示预测为生存的年龄和票价的组合。请注意，一些实例与边界重叠。分类器并不完美，但你只考虑了两个维度。在完整数据集上的算法在10个维度上找到这个决策边界，但可视化变得更加困难。'
- en: '![](03fig07_alt.jpg)'
  id: totrans-531
  prefs: []
  type: TYPE_IMG
  zh: '![图片](03fig07_alt.jpg)'
- en: '|  |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Algorithm highlight: logistic regression**'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '**算法重点：逻辑回归**'
- en: In these “Algorithm highlight” boxes, you’ll take a closer look at the basic
    ideas behind the algorithms used throughout the book. This allows curious readers
    to try to code up, with some extra research, basic working versions of the algorithms.
    Even though we focus mostly on the use of existing packages in this book, understanding
    the basics of a particular algorithm can sometimes be important to fully realize
    the predictive potential.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些“算法亮点”框中，您将更深入地了解本书中使用的算法背后的基本思想。这允许好奇的读者尝试编写，通过一些额外的研究，算法的基本工作版本。尽管我们主要关注本书中现有包的使用，但了解特定算法的基本原理有时对于完全实现预测潜力很重要。
- en: The first algorithm you’ll look at is the *logistic regression algorithm*, arguably
    the simplest ML algorithm for classification tasks. It’s helpful to think about
    the problem as having only two features and a dataset divided into two classes.
    [Figure 3.7](#ch03fig07) shows an example, with the features Age and sqrt(Fare);
    the target is Survived or Died. To build the classifier, you want to find the
    line that best splits the data into the target classes. A line in two dimensions
    can be described by two parameters. These two numbers are the parameters of the
    model that you need to determine.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 您将首先查看的算法是*逻辑回归算法*，这可能是用于分类任务的最简单的机器学习算法。将问题视为只有两个特征并且数据集分为两类是有帮助的。[图3.7](#ch03fig07)展示了示例，其中特征为年龄和sqrt(Fare)；目标是幸存或死亡。为了构建分类器，您希望找到将数据最佳地分割成目标类别的线。二维中的一条线可以用两个参数来描述。这两个数字是您需要确定的模型参数。
- en: 'The algorithm then consists of the following steps:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 算法随后包括以下步骤：
- en: You can start the search by picking the parameter values at random, hence placing
    a random line in the two-dimensional figure.
  id: totrans-537
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过随机选择参数值来开始搜索，因此在二维图中放置一条随机线。
- en: Measure how well this line separates the two classes. In logistic regression,
    you use the statistical *deviance* for the goodness-of-fit measurement.
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测量这条线如何将两个类别分开。在逻辑回归中，您使用统计的*偏差*来衡量拟合优度。
- en: Guess new values of the parameters and measure the separation power.
  id: totrans-539
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 估计参数的新值并测量其分离能力。
- en: Repeat until there are no better guesses. This is an *optimization* procedure
    that can be done with a range of optimization algorithms. Gradient descent is
    a popular choice for a simple optimization algorithm.
  id: totrans-540
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复此过程，直到没有更好的猜测。这是一个*优化*过程，可以使用多种优化算法来完成。梯度下降是简单优化算法中的一种流行选择。
- en: This approach can be extended to more dimensions, so you’re not limited to two
    features in this model. If you’re interested in the details, we strongly encourage
    you to research further and try to implement this algorithm in your programming
    language of choice. Then look at an implementation in a widely used ML package.
    We’ve left out plenty of details, but the preceding steps remain the basis of
    the algorithm.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以扩展到更多维度，因此您在这个模型中不必局限于两个特征。如果您对细节感兴趣，我们强烈建议您进一步研究并尝试在您选择的编程语言中实现此算法。然后查看广泛使用的机器学习包中的实现。我们省略了很多细节，但前面的步骤仍然是算法的基础。
- en: 'Some properties of logistic regression include the following:'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的一些特性包括以下内容：
- en: The algorithm is relatively simple to understand, compared to more-complex algorithms.
    It’s also computationally simple, making it scalable to large datasets.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与更复杂的算法相比，该算法相对容易理解。它也是计算上简单的，这使得它可以扩展到大型数据集。
- en: The performance will degrade if the decision boundary that separates the classes
    needs to be highly nonlinear. See [section 3.2.2](#ch03lev2sec6).
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要高度非线性的决策边界来分离类别，性能将下降。参见[第3.2.2节](#ch03lev2sec6)。
- en: Logistic regression algorithms can sometimes overfit the data, and you often
    need to use a technique called *regularization* that limits this danger. See [section
    3.2.2](#ch03lev2sec6) for an example of overfitting.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归算法有时会过度拟合数据，您通常需要使用一种称为*正则化*的技术来限制这种风险。参见[第3.2.2节](#ch03lev2sec6)中的过度拟合示例。
- en: '|  |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Further reading**'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: If you want to learn more about logistic regression and its use in the real
    world, check out *Applied Logistic Regression* by David Hosmer et al. (Wiley,
    2013).
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于逻辑回归及其在现实世界中的应用，请查看David Hosmer等人所著的*应用逻辑回归*（Wiley，2013年）。
- en: '|  |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 3.2.2\. Classifying complex, nonlinear data
  id: totrans-551
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 对复杂、非线性数据进行分类
- en: 'Looking at [figure 3.7](#ch03fig07), you can understand why logistic regression
    is a linear algorithm: the decision boundary is a straight line. Of course, your
    data might not be well separated by a straight line, so for such datasets you
    should use a nonlinear algorithm. But nonlinear algorithms are typically more
    demanding computationally and don’t scale well to large datasets. You’ll look
    further at the scalability of various types of algorithms in [chapter 8](kindle_split_019.html#ch08).'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到[图3.7](#ch03fig07)，你可以理解为什么逻辑回归是一个线性算法：决策边界是一条直线。当然，你的数据可能不会很好地被直线分开，因此对于这样的数据集，你应该使用非线性算法。但非线性算法通常在计算上要求更高，并且不适合大规模数据集。你将在[第8章](kindle_split_019.html#ch08)中进一步了解各种类型算法的可扩展性。
- en: 'Looking again at the appendix, you can pick a nonlinear algorithm for modeling
    the Titanic Passengers dataset. A popular method for nonlinear problems is a support
    vector machine with a nonlinear kernel. Support vector machines are linear by
    nature, but by using a kernel, this model becomes a powerful nonlinear method.
    You can change a single line of code in [listing 3.1](#ch03ex01) to use this new
    algorithm, and the decision boundary is plotted in [figure 3.8](#ch03fig08):'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 再次查看附录，你可以选择一个非线性算法来建模泰坦尼克号乘客数据集。对于非线性问题，一个流行的方法是使用具有非线性核的支持向量机。支持向量机本质上是线性的，但通过使用核，这个模型变成了一个强大的非线性方法。你可以在[代码清单3.1](#ch03ex01)中更改一行代码来使用这个新算法，并且决策边界在[图3.8](#ch03fig08)中绘制：
- en: '[PRE5]'
  id: totrans-554
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Figure 3.8\. Nonlinear decision boundary of the Titanic survival support vector
    machine classifier with a nonlinear kernel. The light background denotes the combinations
    of Age and Fare that are predicted to yield survival.
  id: totrans-555
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.8. 带有非线性核的泰坦尼克号生存支持向量机分类器的非线性决策边界。浅色背景表示预测为生存的年龄和船票的组合。
- en: '![](03fig08_alt.jpg)'
  id: totrans-556
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig08_alt.jpg)'
- en: 'You can see that the decision boundary in [figure 3.8](#ch03fig08) is different
    from the linear one in [figure 3.7](#ch03fig07). What you see here is a good example
    of an important concept in machine learning: overfitting. The algorithm is capable
    of fitting well to the data, almost at the single-record level, and you risk losing
    the ability to make good predictions on new data that wasn’t included in the training
    set; the more complex you allow the model to become, the higher the risk of overfitting.'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到[图3.8](#ch03fig08)中的决策边界与[图3.7](#ch03fig07)中的线性决策边界不同。这里你看到的是机器学习中的一个重要概念：过拟合的例子。算法能够很好地拟合数据，几乎在单个记录级别，你可能会失去对新数据做出良好预测的能力，这些新数据没有包含在训练集中；你允许模型变得越复杂，过拟合的风险就越高。
- en: Usually, you can avoid overfitting a nonlinear model by using model parameters
    built into the algorithm. By tweaking the parameters of the model, keeping the
    data unchanged, you can obtain a better decision boundary. Note that you’re currently
    using intuition to determine when something is overfitting; in [chapter 4](kindle_split_014.html#ch04),
    you’ll learn how to use data and statistics to quantify this intuition. For now,
    you’ll use our (the authors’) experience and tweak a certain parameter called
    *gamma*. You don’t need to know what gamma is at this point, only that it helps
    control the risk of overfitting. In [chapter 5](kindle_split_015.html#ch05), you’ll
    see how to optimize the model parameters without only guessing at better values.
    Setting gamma = 0.1 in the SVM classifier, you obtain the much improved decision
    boundary shown in [figure 3.9](#ch03fig09).
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你可以通过使用算法内建的模式参数来避免非线性模型的过拟合。通过调整模型的参数，保持数据不变，你可以获得更好的决策边界。请注意，你目前正使用直觉来判断何时出现过拟合；在[第4章](kindle_split_014.html#ch04)中，你将学习如何使用数据和统计学来量化这种直觉。现在，你将使用（作者们的）经验并调整一个称为*gamma*的参数。你目前不需要知道gamma是什么，只需知道它有助于控制过拟合的风险。在[第5章](kindle_split_015.html#ch05)中，你将看到如何仅通过猜测更好的值来优化模型参数。在SVM分类器中将gamma设置为0.1，你将获得如图3.9所示的决策边界的大幅改进。
- en: Figure 3.9\. Decision boundary of nonlinear RBF-kernel SVM with gamma = 0.1
  id: totrans-559
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.9. 带有gamma = 0.1的非线性RBF核SVM的决策边界
- en: '![](03fig09_alt.jpg)'
  id: totrans-560
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig09_alt.jpg)'
- en: '|  |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Algorithm highlight: support vector machines**'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '**算法亮点：支持向量机**'
- en: The support vector machine (SVM) algorithm is a popular choice for both linear
    and nonlinear problems. It has some interesting theoretical and practical properties
    that make it useful in many scenarios.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）算法是线性和非线性问题的流行选择。它具有一些有趣的理论和实践特性，使其在许多场景中非常有用。
- en: The main idea behind the algorithm is, as with logistic regression discussed
    previously, to find the line (or equivalent in higher dimensions) that separates
    the classes optimally. Instead of measuring the distance to all points, SVMs try
    to find the largest *margin* between only the points on either side of the decision
    line. The idea is that there’s no reason to worry about points that are well within
    the boundary, only ones that are close. In the following image, you can see that
    lines H[1] and H[2] are bad separation boundaries, because the distance to the
    closest point on both sides of the line isn’t the largest it can be. H[3] is the
    optimal line.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法背后的主要思想，就像之前讨论的逻辑回归一样，是找到最优地分离类别的线（或更高维度的等价物）。SVMs试图找到仅位于决策线两侧的点之间的最大*间隔*，而不是测量所有点到线的距离。想法是，没有必要担心那些很好地位于边界内的点，只有那些接近边界的点。在下面的图像中，你可以看到线H[1]和H[2]是糟糕的分离边界，因为到线两侧最近点的距离不是最大的。H[3]是最佳线。
- en: '![](066fig01.jpg)'
  id: totrans-565
  prefs: []
  type: TYPE_IMG
  zh: '![](066fig01.jpg)'
- en: An SVM decision boundary (H[3]) is often superior to decision boundaries found
    by other ML algorithms.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: SVM决策边界（H[3]）通常优于其他机器学习算法找到的决策边界。
- en: 'Although this algorithm is also linear in the sense that the separation boundary
    is linear, SVMs are capable of fitting to nonlinear data, as you saw earlier in
    this section. SVMs use a clever technique in order to fit to nonlinear data: the
    kernel trick. A *kernel* is a mathematical construct that can “warp” the space
    where the data lives. The algorithm can then find a linear boundary in this warped
    space, making the boundary nonlinear in the original space.'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个算法在某种意义上也是线性的，因为分离边界是线性的，但SVMs能够拟合非线性数据，正如你在这部分前面看到的。SVMs使用一种巧妙的技术来拟合非线性数据：核技巧。*核*是一种数学结构，可以“扭曲”数据所在的空间。然后算法可以在扭曲的空间中找到一个线性边界，使得边界在原始空间中是非线性的。
- en: '|  |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Further reading**'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: 'Hundreds of books have been written about machine-learning algorithms, covering
    everything from their theoretical foundation and efficient implementation to their
    practical use. If you’re looking for a more rigorous treatment of these topics,
    we recommend two classic texts on ML algorithms:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 关于机器学习算法的书已经写了数百本，涵盖了从它们的理论基础和高效实现到它们实际应用的所有内容。如果你在寻找对这些主题更严谨的处理，我们推荐两本关于机器学习算法的经典著作：
- en: '*The Elements of Statistical Learning: Data Mining, Inference, and Prediction*
    by Trevor Hastie et al. (Springer, 2009).'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《*统计学习基础：数据挖掘、推理和预测*》由特里弗·哈斯蒂等人（Springer，2009年）著。
- en: '*Pattern Recognition and Machine Learning* by Christopher Bishop (Springer,
    2007).'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《*模式识别与机器学习*》由克里斯托弗·贝斯希（Springer，2007年）著。
- en: '|  |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 3.2.3\. Classifying with multiple classes
  id: totrans-575
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3\. 多类别分类
- en: Up to this point, you’ve looked at classification into only two classes. In
    some cases, you’ll have more than two classes. A good real-world example of multiclass
    classification is the handwritten digit recognition problem. Whenever you send
    old-school mail to your family, a robot reads the handwritten ZIP code and determines
    where to send the letter, and good digit recognition is essential in this process.
    A public dataset, the MNIST database,^([[2](#ch03fn02)]) is available for research
    into these types of problems. This dataset consists of 60,000 images of handwritten
    digits. [Figure 3.10](#ch03fig10) shows a few of the handwritten digit images.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你只看到了两类的分类。在某些情况下，你将会有超过两个类别。多类别分类的一个好的现实世界例子是手写数字识别问题。每当你给家人寄老式邮件时，机器人会读取手写的ZIP代码并确定信件的去向，而在这一过程中良好的数字识别是至关重要的。一个公开的数据集，MNIST数据库，^([[2](#ch03fn02)])可用于研究这类问题。这个数据集包含60,000个手写数字的图像。[图3.10](#ch03fig10)展示了其中的一些手写数字图像。
- en: ²
  id: totrans-577
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-578
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can find the MNIST Database of Handwritten Digits at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
  id: totrans-579
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以在[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)找到手写数字的MNIST数据库。
- en: Figure 3.10\. Four randomly chosen handwritten digits from the MNIST database
  id: totrans-580
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.10\. 从MNIST数据库中随机选择的四个手写数字
- en: '![](03fig10.jpg)'
  id: totrans-581
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig10.jpg)'
- en: The images are 28 × 28 pixels each, but we convert each image into 28² = 784
    features, one feature for each pixel. In addition to being a multiclass problem,
    this is also a high-dimensional problem. The pattern that the algorithm needs
    to find is a complex combination of many of these features, and the problem is
    nonlinear in nature.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图像都是28 × 28像素，但我们把每个图像转换成28² = 784个特征，每个像素一个特征。除了是一个多类问题外，这也是一个高维问题。算法需要找到的是许多这些特征的复杂组合，并且这个问题本质上是非线性的。
- en: 'To build the classifier, you first choose the algorithm to use from the appendix.
    The first nonlinear algorithm on the list that natively supports multiclass problems
    is the k-nearest neighbors classifier, which is another simple but powerful algorithm
    for nonlinear ML modeling. You need to change only one line in [listing 3.1](#ch03ex01)
    to use the new algorithm, but you’ll also include a function for getting the full
    prediction probabilities instead of just the final prediction:'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建分类器，你首先需要从附录中选择要使用的算法。列表中第一个原生支持多类问题的非线性算法是k-最近邻分类器，这是另一种简单但强大的非线性机器学习建模算法。你只需要在[列表3.1](#ch03ex01)中更改一行来使用新算法，但你还需要包含一个用于获取完整预测概率而不是仅获取最终预测的函数：
- en: '[PRE6]'
  id: totrans-584
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Building the k-nearest neighbors classifier and making predictions on the four
    digits shown in [figure 3.10](#ch03fig10), you obtain the table of probabilities
    shown in [figure 3.11](#ch03fig11).
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 构建k-最近邻分类器并对[图3.10](#ch03fig10)中显示的四个数字进行预测，你将获得[图3.11](#ch03fig11)中显示的概率表。
- en: 'You can see that the predictions for digits 1 and 3 are spot on, and there’s
    only a small (10%) uncertainty for digit 4\. Looking at the second digit (3),
    it’s not surprising that this is hard to classify perfectly. This is the main
    reason to get the full probabilities in the first place: to be able to take action
    on things that aren’t perfectly certain. This is easy to understand in the case
    of a post office robot routing letters; if the robot is sufficiently uncertain
    about some digits, maybe we should have a good old human look at it before we
    send it out wrong.'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，对数字1和3的预测非常准确，对数字4的预测只有很小的（10%）不确定性。看第二个数字（3），它难以完美分类并不奇怪。这正是最初获取完整概率的主要原因：能够对那些不是完全确定的事情采取行动。在邮局机器人分拣信件的情况下，这很容易理解；如果机器人对某些数字的确定性足够低，那么在我们将其发送出去之前，也许我们应该让一个好人先看看它。
- en: Figure 3.11\. Table of predicted probabilities from a k-nearest neighbors classifier,
    as applied to the MNIST dataset
  id: totrans-587
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.11。k-最近邻分类器应用于MNIST数据集的预测概率表
- en: '![](03fig11_alt.jpg)'
  id: totrans-588
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig11_alt.jpg)'
- en: '|  |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Algorithm highlight: k-nearest neighbors**'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '**算法亮点：k-最近邻算法**'
- en: The *k-nearest neighbors algorithm* is a simple yet powerful nonlinear ML method.
    It’s often used when model training should be quick, but predictions are typically
    slower. You’ll soon see why this is the case.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '*k-最近邻算法*是一种简单但强大的非线性机器学习方法。它通常在模型训练需要快速进行，但预测通常较慢的情况下使用。你很快就会看到这是为什么。'
- en: 'The basic idea is that you can classify a new data record by comparing it with
    similar records from the training set. If a dataset record consists of a set of
    numbers, *n[i]*, you can find the *distance* between records via the usual distance
    formula:'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想是，你可以通过将新数据记录与训练集中的相似记录进行比较来对新的数据记录进行分类。如果一个数据集记录由一组数字*n[i]*组成，你可以通过通常的距离公式找到记录之间的*距离*：
- en: '![](068equ01.jpg)'
  id: totrans-593
  prefs: []
  type: TYPE_IMG
  zh: '![](068equ01.jpg)'
- en: When making predictions on new records, you find the *closest* known record
    and assign that class to the new record. This would be a 1-nearest neighbor classifier,
    as you’re using only the closest neighbor. Usually you’d use 3, 5, or 9 neighbors
    and pick the class that’s most common among neighbors (you use odd numbers to
    avoid ties).
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 当对新记录进行预测时，你找到*最近的*已知记录，并将该类别分配给新记录。这将是一个1-最近邻分类器，因为你只使用最近的邻居。通常你会使用3、5或9个邻居，并选择在邻居中最常见的类别（你使用奇数以避免平局）。
- en: The training phase is relatively quick, because you index the known records
    for fast distance calculations to new data. The prediction phase is where most
    of the work is done, finding the closest neighbors from the entire dataset.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 训练阶段相对较快，因为你为已知记录索引以快速计算新数据之间的距离。预测阶段是大部分工作所在的地方，即从整个数据集中找到最近的邻居。
- en: The previous simple example uses the usual Euclidean distance metric. You can
    also use more-advanced distance metrics, depending on the dataset at hand.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的简单示例使用了常用的欧几里得距离度量。你也可以根据数据集使用更高级的距离度量。
- en: K-nearest neighbors is useful not only for classification, but for regression
    as well. Instead of taking the most common class of neighbors, you take the average
    or median values of the target values of the neighbors. [Section 3.3](#ch03lev1sec3)
    further details regression.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: K-最近邻不仅对分类有用，对回归也有用。你不仅取邻居中最常见的类别，还取邻居的目标值的平均值或中位数。[第3.3节](#ch03lev1sec3)进一步详细介绍了回归。
- en: '|  |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '3.3\. Regression: predicting numerical values'
  id: totrans-599
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 回归：预测数值
- en: Not every machine-learning problem is about putting records into classes. Sometimes
    the target variable takes on numerical values—for example, when predicting dollar
    values in a financial model. We call the act of predicting numerical values *regression*,
    and the model itself a *regressor*. [Figure 3.12](#ch03fig12) illustrates the
    concept of regression.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每个机器学习问题都是关于将记录放入类别中。有时目标变量取数值——例如，在金融模型中预测美元价值时。我们将预测数值的行为称为*回归*，而模型本身称为*回归器*。[图3.12](#ch03fig12)说明了回归的概念。
- en: Figure 3.12\. In this regression process, the regressor is predicting the numerical
    value of a record.
  id: totrans-601
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.12\. 在这个回归过程中，回归器正在预测一条记录的数值。
- en: '![](03fig12.jpg)'
  id: totrans-602
  prefs: []
  type: TYPE_IMG
  zh: '![图3.12](03fig12.jpg)'
- en: As an example of a regression analysis, you’ll use the Auto MPG dataset introduced
    in [chapter 2](kindle_split_012.html#ch02). The goal is to build a model that
    can predict the average miles per gallon of a car, given various properties of
    the car such as horsepower, weight, location of origin, and model year. [Figure
    3.13](#ch03fig13) shows a small subset of this data.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 作为回归分析的例子，你将使用[第2章](kindle_split_012.html#ch02)中介绍的汽车燃油效率数据集。目标是构建一个模型，可以预测汽车的平均每加仑英里数，给定汽车的属性，如马力、重量、产地和车型年份。[图3.13](#ch03fig13)显示了这些数据的一个小子集。
- en: Figure 3.13\. Small subset of the Auto MPG data
  id: totrans-604
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.13\. 汽车燃油效率数据的小子集
- en: '![](03fig13_alt.jpg)'
  id: totrans-605
  prefs: []
  type: TYPE_IMG
  zh: '![图3.13的替代文本](03fig13_alt.jpg)'
- en: In [chapter 2](kindle_split_012.html#ch02), you discovered useful relationships
    between the MPG rating, the car weight, and the model year. These relationships
    are shown in [figure 3.14](#ch03fig14).
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](kindle_split_012.html#ch02)中，你发现了燃油效率（MPG）评级、汽车重量和车型年份之间的有用关系。这些关系在[图3.14](#ch03fig14)中展示。
- en: Figure 3.14\. Using scatter plots, you can see that Vehicle Weight and Model
    Year are useful for predicting MPG. See [chapter 2](kindle_split_012.html#ch02)
    for more details.
  id: totrans-607
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.14\. 使用散点图，你可以看到车辆重量和车型年份对于预测MPG是有用的。更多详情请见[第2章](kindle_split_012.html#ch02)。
- en: '![](03fig14_alt.jpg)'
  id: totrans-608
  prefs: []
  type: TYPE_IMG
  zh: '![图3.14的替代文本](03fig14_alt.jpg)'
- en: In the next section, you’ll look at how to build a basic linear regression model
    to predict the miles per gallon values of this dataset of vehicles. After successfully
    building a basic model, you’ll look at more-advanced algorithms for modeling nonlinear
    data.
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将了解如何构建一个基本的线性回归模型来预测这些车辆数据集的每加仑英里数。在成功构建基本模型之后，你将了解用于建模非线性数据的更高级算法。
- en: 3.3.1\. Building a regressor and making predictions
  id: totrans-610
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 构建回归器并进行预测
- en: 'Again, you’ll start by choosing an algorithm to use and getting the data into
    a suitable format. Arguably, the linear regression algorithm is the simplest regression
    algorithm. As the name indicates, this is a linear algorithm, and the appendix
    shows the data preprocessing needed in order to use this algorithm. You need to
    (1) impute missing values and (2) expand categorical features. Our Auto MPG dataset
    has no missing values, but there’s one categorical column: Origin. After expanding
    the Origin column (as described in [section 2.2.1](kindle_split_012.html#ch02lev2sec5)
    in [chapter 2](kindle_split_012.html#ch02)), you obtain the data format shown
    in [figure 3.15](#ch03fig15).'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，你将首先选择一个算法来使用，并将数据格式化为合适的格式。线性回归算法可以说是最简单的回归算法。正如其名所示，这是一个线性算法，附录显示了使用此算法所需的数据预处理。你需要（1）填补缺失值和（2）扩展分类特征。我们的汽车燃油效率数据集没有缺失值，但有一个分类列：产地。在扩展产地列（如[第2章](kindle_split_012.html#ch02)中的[第2.2.1节](kindle_split_012.html#ch02lev2sec5)所述）之后，你将获得[图3.15](#ch03fig15)中所示的数据格式。
- en: Figure 3.15\. The Auto MPG data after expanding the categorical Origin column
  id: totrans-612
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.15\. 扩展Origin列后的汽车燃油效率数据
- en: '![](03fig15_alt.jpg)'
  id: totrans-613
  prefs: []
  type: TYPE_IMG
  zh: '![图3.15的替代文本](03fig15_alt.jpg)'
- en: 'You can now use the algorithm to build the model. Again, you can use the code
    structure defined in [listing 3.1](#ch03ex01) and change this line:'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以使用该算法来构建模型。再次，你可以使用[代码清单3.1](#ch03ex01)中定义的代码结构，并更改此行：
- en: '[PRE7]'
  id: totrans-615
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With the model in hand, you can make predictions. In this example, however,
    you’ll split the dataset into a training set and a testing set before building
    the model. In [chapter 4](kindle_split_014.html#ch04), you’ll learn much more
    about how to evaluate models, but you’ll use some simple techniques in this section.
    By training a model on only some of the data while holding out a testing set,
    you can subsequently make predictions on the testing set and see how close your
    predictions come to the actual values. If you were training on all the data and
    making predictions on some of that training data, you’d be cheating, as the model
    is more likely to make good predictions if it’s seen the data while training.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 拿到模型后，你可以进行预测。然而，在这个例子中，在构建模型之前，你需要将数据集分成训练集和测试集。在[第4章](kindle_split_014.html#ch04)中，你将学习更多关于如何评估模型的知识，但在这个部分你将使用一些简单的技术。通过在仅使用部分数据训练模型的同时保留测试集，你可以在测试集上进行预测，并查看你的预测与实际值有多接近。如果你在所有数据上训练并在部分训练数据上做出预测，那么你就是在作弊，因为模型在训练时看到数据更有可能做出好的预测。
- en: '[Figure 3.16](#ch03fig16) shows the results of making predictions on a held-out
    testing set, and how they compare to the known values. In this example, you train
    the model on 80% of the data and use the remaining 20% for testing.'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.16](#ch03fig16)展示了在保留测试集上进行预测的结果，以及它们与已知值的比较。在这个例子中，你使用80%的数据来训练模型，并使用剩余的20%进行测试。'
- en: Figure 3.16\. Comparing MPG predictions on a held-out testing set to actual
    values
  id: totrans-618
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.16\. 将保留测试集中的MPG预测值与实际值进行比较
- en: '![](03fig16.jpg)'
  id: totrans-619
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig16.jpg)'
- en: A useful way to compare more than a few rows of predictions is to use our good
    friend, the scatter plot, once again. For regression problems, both the actual
    target values and the predicted values are numeric. Plotting the predictions against
    each other in a scatter plot, introduced in [chapter 2](kindle_split_012.html#ch02),
    you can visualize how well the predictions follow the actual values. This is shown
    for the held-out Auto MPG test set in [figure 3.17](#ch03fig17). This figure shows
    great prediction performance, as the predictions all fall close to the optimal
    diagonal line. By looking at this figure, you can get a sense of how your ML model
    might perform on new data. In this case, a few of the predictions for higher MPG
    values seem to be underestimated, and this may be useful information for you.
    For example, if you want to get better at estimating high MPG values, you might
    need to find more examples of high MPG vehicles, or you might need to obtain higher-quality
    data in this regime.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 比较多个预测值的一个有用方法是再次使用我们的好朋友，散点图。对于回归问题，实际的目标值和预测值都是数值。在[第2章](kindle_split_012.html#ch02)中介绍的散点图中，将预测值相互对比，你可以可视化预测值如何跟随实际值。这可以在[图3.17](#ch03fig17)中看到的保留测试集的Auto
    MPG测试集中展示。此图显示了出色的预测性能，因为所有预测值都接近最优的对角线。通过观察此图，你可以了解你的机器学习模型在新数据上可能的表现。在这种情况下，对于更高的MPG值的一些预测似乎被低估了，这可能对你是有用的信息。例如，如果你想提高估计高MPG值的能力，你可能需要找到更多高MPG车辆的例子，或者你可能需要在这个范围内获得更高质量的数据。
- en: Figure 3.17\. A scatter plot of the actual versus predicted values on the held-out
    test set. The diagonal line shows the perfect regressor. The closer all of the
    predictions are to this line, the better the model.
  id: totrans-621
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.17\. 实际值与预测值在保留测试集中的散点图。对角线表示完美的回归器。所有预测值都接近这条线，则模型越好。
- en: '![](03fig17_alt.jpg)'
  id: totrans-622
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig17_alt.jpg)'
- en: '|  |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Algorithm highlight: linear regression**'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: '**算法亮点：线性回归**'
- en: Like logistic regression for classification, *linear regression* is arguably
    the simplest and most widely used algorithm for building regression models. The
    main strengths are linear scalability and a high level of interpretability.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 就像用于分类的逻辑回归一样，*线性回归*可以说是构建回归模型最简单且最广泛使用的算法。其主要优势是线性可扩展性和高度的可解释性。
- en: This algorithm plots the dataset records as points, with the target variable
    on the y-axis, and fits a straight line (or plane, in the case of two or more
    features) to these points. The following figure illustrates the process of optimizing
    the distance from the points to the straight line of the model.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法将数据集记录绘制为点，目标变量位于y轴上，并将一条直线（或平面，如果有两个或更多特征）拟合到这些点上。以下图示了优化点到模型直线距离的过程。
- en: '![](072fig01_alt.jpg)'
  id: totrans-627
  prefs: []
  type: TYPE_IMG
  zh: '![](072fig01_alt.jpg)'
- en: Demonstration of how linear regression determines the best-fit line. Here, the
    dark line is the optimal linear regression fitted line on this dataset, yielding
    a smaller mean-squared deviation from the data to any other possible line (such
    as the dashed line shown).
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归如何确定最佳拟合线的演示。在这里，深色线是此数据集上的最佳线性回归拟合线，它相对于任何其他可能的线（如图中所示虚线所示）具有更小的均方偏差。
- en: A straight line can be described by two parameters for lines in two dimensions,
    and so on. You know this from the a and b in y = a × x + b from the basic math.
    These parameters are fitted to the data, and when optimized, they completely describe
    the model and can be used to make predictions on new data.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维空间中，一条直线可以用两个参数来描述，以此类推。你知道这个，从基本的数学公式 y = a × x + b 中的 a 和 b。这些参数与数据拟合，当优化后，它们完全描述了模型，并且可以用来对新数据进行预测。
- en: '|  |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 3.3.2\. Performing regression on complex, nonlinear data
  id: totrans-631
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2\. 对复杂、非线性数据进行回归
- en: In some datasets, the relationship between features can’t be fitted by a linear
    model, and algorithms such as linear regression may not be appropriate if accurate
    predictions are required. Other properties, such as scalability, may make lower
    accuracy a necessary trade-off. Also, there’s no guarantee that a nonlinear algorithm
    will be more accurate, as you risk overfitting to the data. As an example of a
    nonlinear regression model, we introduce the random forest algorithm. Random forest
    is a popular method for highly nonlinear problems for which accuracy is important.
    As evident in the appendix, it’s also easy to use, as it requires minimal preprocessing
    of data. In [figures 3.18](#ch03fig18) and [3.19](#ch03fig19), you can see the
    results of making predictions on the Auto MPG test set via the random forest model.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些数据集中，特征之间的关系无法用线性模型来拟合，如果需要准确预测，线性回归算法可能不适用。其他属性，如可扩展性，可能使得降低准确性成为必要的权衡。此外，非线性算法不一定更准确，因为你可能会过度拟合数据。作为一个非线性回归模型的例子，我们介绍了随机森林算法。随机森林是用于高度非线性问题且准确性很重要的一种流行方法。正如附录中所示，它也很容易使用，因为它需要的数据预处理最少。在[图3.18](#ch03fig18)和[3.19](#ch03fig19)中，你可以看到通过随机森林模型对Auto
    MPG测试集进行预测的结果。
- en: Figure 3.18\. Table of actual versus predicted MPG values for the nonlinear
    random forest regression model
  id: totrans-633
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.18\. 非线性随机森林回归模型中实际与预测MPG值的表格
- en: '![](03fig18.jpg)'
  id: totrans-634
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig18.jpg)'
- en: Figure 3.19\. Comparison of MPG data versus predicted values for the nonlinear
    random forest regression model
  id: totrans-635
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.19\. 非线性随机森林回归模型中MPG数据与预测值的比较
- en: '![](03fig19_alt.jpg)'
  id: totrans-636
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig19_alt.jpg)'
- en: This model isn’t much different from the linear algorithm, at least visually.
    It’s not clear which of the algorithms performs the best in terms of accuracy.
    In the next chapter, you’ll learn how to quantify the performance (often called
    the *accuracy score* of the model) so you can make meaningful measurements of
    how good the prediction accuracy is.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型与线性算法在视觉上并没有太大的区别。在准确性方面，哪个算法表现最好并不明显。在下一章中，你将学习如何量化性能（通常称为模型的*准确度得分*），这样你就可以对预测准确度进行有意义的测量。
- en: '|  |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Algorithm highlight: random forest**'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: '**算法亮点：随机森林**'
- en: For the last algorithm highlight of this chapter, we introduce the *random forest*
    (RF) algorithm. This highly accurate nonlinear algorithm is widely used in real-world
    classification and regression problems.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章最后一个算法亮点，我们介绍了*随机森林*（RF）算法。这个高度准确的非线性算法在现实世界的分类和回归问题中得到了广泛应用。
- en: The basis of the RF algorithm is the decision tree. Imagine that you need to
    make a decision about something, such as what to work on next. Some variables
    can help you decide the best course of action, and some variables weigh higher
    than others. In this case, you might ask first, “How much money will this make
    me?” If the answer is less than $10, you can choose to not go ahead with the task.
    If the answer is more than $10, you might ask the next question in the decision
    tree, “Will working on this make me happy?” and answer with a yes/no. You can
    continue to build out this tree until you’ve reached a conclusion and chosen a
    task to work on.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: RF算法的基础是决策树。想象一下，你需要就某事做出决定，比如下一步做什么。一些变量可以帮助你决定最佳的行动方案，而一些变量的权重高于其他变量。在这种情况下，你可能会首先问，“这能让我赚多少钱？”如果答案是少于10美元，你可以选择不继续这项任务。如果答案是超过10美元，你可能会在决策树中提出下一个问题，“从事这项工作会让我快乐吗？”并回答是或否。你可以继续构建这个树，直到你得出结论并选择一个任务来工作。
- en: The decision tree algorithm lets the computer figure out, based on the training
    set, which variables are the most important, and put them in the top of the tree,
    and then gradually use less-important variables. This allows it to combine variables
    and say, “If the amount is greater than $10 and makes me happy, and amount of
    work less than 1 hour, then yes.”
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法让计算机根据训练集确定哪些变量是最重要的，并将它们放在树的顶部，然后逐渐使用不那么重要的变量。这使得它能够结合变量并说，“如果金额大于10并且让我开心，而且工作量少于1小时，那么就是。”
- en: A problem with decision trees is that the top levels of the tree have a huge
    impact on the answer, and if the new data doesn’t follow exactly the same distribution
    as the training set, the ability to generalize might suffer. This is where the
    random forest method comes in. By building a collection of decision trees, you
    mitigate this risk. When making the answer, you pick the majority vote in the
    case of classification, or take the mean in case of regression. Because you use
    votes or means, you can also give back full probabilities in a natural way that
    not many algorithms share.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的一个问题是树的顶层对答案有巨大影响，如果新数据没有遵循与训练集完全相同的分布，泛化能力可能会受到影响。这就是随机森林方法出现的地方。通过构建决策树集合，你可以降低这种风险。在做出答案时，在分类的情况下选择多数投票，在回归的情况下取平均值。由于你使用投票或平均值，你也可以以自然的方式返回完整的概率，这是许多算法所不具备的。
- en: Random forests are also known for other kinds of advantages, such as their immunity
    to unimportant features, noisy datasets in terms of missing values, and mislabeled
    records.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林还以其其他优点而闻名，例如对不重要特征的免疫力、缺失值方面的噪声数据集以及错误标记的记录。
- en: '|  |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 3.4\. Summary
  id: totrans-646
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 摘要
- en: 'In this chapter, we introduced machine-learning modeling. Here we list the
    main takeaways from the chapter:'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了机器学习建模。以下是我们从本章中获得的主要收获：
- en: The purpose of modeling is to describe the relationship between the input features
    and the target variable.
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的目的是描述输入特征与目标变量之间的关系。
- en: You can use models either to generate predictions for new data (whose target
    is unknown) or to infer the true associations (or lack thereof) present in the
    data.
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用模型来为新数据（其目标未知）生成预测，或者推断数据中存在的真实关联（或不存在）。
- en: There are hundreds of methods for ML modeling. Some are parametric, meaning
    that the form of the mathematical function relating the features to the target
    is fixed in advance. Parametric models tend to be more highly interpretable yet
    less accurate than nonparametric approaches, which are more flexible and can adapt
    to the true complexity of the relationship between the features and the target.
    Because of their high levels of predictive accuracy and their flexibility, nonparametric
    approaches are favored by most practitioners of machine learning.
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习建模有数百种方法。其中一些是参数化的，这意味着将特征与目标相关联的数学函数的形式在事先是固定的。参数化模型通常比非参数方法更易于解释，但准确性较低，而非参数方法更灵活，可以适应特征与目标之间关系的真实复杂性。由于它们高度的预测准确性和灵活性，非参数方法受到大多数机器学习实践者的青睐。
- en: Machine-learning methods are further broken into supervised and unsupervised
    methods. Supervised methods require a training set with a known target, and unsupervised
    methods don’t require a target variable. Most of this book is dedicated to supervised
    learning.
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习方法进一步分为监督学习和无监督学习。监督学习方法需要一个包含已知目标的训练集，而无监督学习方法不需要目标变量。本书的大部分内容都致力于监督学习。
- en: The two most common problems in supervised learning are classification, in which
    the target is categorical, and regression, in which the target is numerical. In
    this chapter, you learned how to build both classification and regression models
    and how to employ them to make predictions on new data.
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习中最常见的两个问题是分类，其中目标是分类的，和回归，其中目标是数值的。在本章中，你学习了如何构建分类和回归模型，以及如何使用它们对新数据进行预测。
- en: You also dove more deeply into the problem of classification. Linear algorithms
    can define linear decision boundaries between classes, whereas nonlinear methods
    are required if the data can’t be separated linearly. Using nonlinear models usually
    has a higher computational cost.
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你还更深入地研究了分类问题。线性算法可以定义类之间的线性决策边界，而非线性方法在数据不能线性分离时是必需的。使用非线性模型通常具有更高的计算成本。
- en: In contrast to classification (in which a categorical target is predicted),
    you predict a numerical target variable in regression models. You saw examples
    of linear and nonlinear methods and how to visualize the predictions of these
    models.
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与分类（其中预测的是分类目标）相反，在回归模型中预测的是数值目标变量。你看到了线性和非线性方法的例子以及如何可视化这些模型的预测。
- en: 3.5\. Terms from this chapter
  id: totrans-655
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5. 本章术语
- en: '| Word | Definition |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
  zh: '| 词 | 定义 |'
- en: '| --- | --- |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| model | The base product from using an ML algorithm on training data. |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 使用机器学习算法在训练数据上得到的基产品。 |'
- en: '| prediction | Predictions are performed by pulling new data through the model.
    |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
  zh: '| 预测 | 通过将新数据通过模型进行预测来执行预测。 |'
- en: '| inference | The act of gaining insight into the data by building the model
    and not making predictions. |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | 通过构建模型并不仅仅进行预测来获取对数据的洞察。 |'
- en: '| (non)parametric | Parametric models make assumptions about the structure
    of the data. Nonparametric models don’t. |'
  id: totrans-661
  prefs: []
  type: TYPE_TB
  zh: '| (非)参数 | 参数模型对数据的结构做出假设。非参数模型则不做出假设。 |'
- en: '| (un)supervised | Supervised models, such as classification and regression,
    find the mapping between the input features and the target variable. Unsupervised
    models are used to find patterns in the data without a specified target variable.
    |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
  zh: '| (无)监督 | 监督模型，如分类和回归，寻找输入特征和目标变量之间的映射。无监督模型用于在数据中寻找模式，而不需要指定的目标变量。 |'
- en: '| clustering | A form of unsupervised learning that puts data into self-defined
    clusters. |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
  zh: '| 聚类 | 一种无监督学习方法，将数据放入自定义的簇中。 |'
- en: '| dimensionality reduction | Another form of unsupervised learning that can
    map high-dimensional datasets to a lower-dimensional representation, usually for
    plotting in two or three dimensions. |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
  zh: '| 维度降低 | 另一种无监督学习方法，可以将高维数据集映射到低维表示，通常用于在二维或三维中进行绘图。 |'
- en: '| classification | A supervised learning method that predicts data into buckets.
    |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 一种预测数据到桶中的监督学习方法。 |'
- en: '| regression | The supervised method that predicts numerical target values.
    |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
  zh: '| 回归 | 一种预测数值目标值的监督方法。 |'
- en: In the next chapter, you’ll look at creating and testing models, the exciting
    part of machine learning. You’ll see whether your choice of algorithms and features
    is going to work to solve the problem at hand. You’ll also see how to rigorously
    validate a model to see how good its predictions are likely to be on new data.
    And you’ll learn about validation methods, metrics, and some useful visualizations
    for assessing your models’ performance.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习创建和测试模型，这是机器学习的精彩部分。你将看到你的算法和特征选择是否能够解决当前的问题。你还将了解如何严格验证模型，以查看其预测在新数据上可能有多好。你还将了解用于评估模型性能的验证方法、指标和一些有用的可视化方法。
- en: Chapter 4\. Model evaluation and optimization
  id: totrans-668
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4章. 模型评估和优化
- en: '*This chapter covers*'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Using cross-validation for properly evaluating the predictive performance of
    models
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用交叉验证来正确评估模型的预测性能
- en: Overfitting and how to avoid it
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合及其避免方法
- en: Standard evaluation metrics and visualizations for binary and multiclass classification
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准二元和多类分类的评估指标和可视化方法
- en: Standard evaluation metrics and visualizations for regression models
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准回归模型的评估指标和可视化方法
- en: Optimizing your model by selecting the optimal parameters
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过选择最佳参数来优化你的模型
- en: After you fit a machine-learning model, the next step is to assess the accuracy
    of that model. Before you can put a model to use, you need to know how well it’s
    expected to predict on new data. If you determine that the predictive performance
    is quite good, you can be comfortable in deploying that model in production to
    analyze new data. Likewise, if you assess that the predictive performance isn’t
    good enough for the task at hand, you can revisit your data and model to try to
    improve and optimize its accuracy. (The last section of this chapter introduces
    simple model optimization. [Chapters 5](kindle_split_015.html#ch05), [7](kindle_split_018.html#ch07),
    and [9](kindle_split_020.html#ch09) cover more-sophisticated methods of improving
    the predictive accuracy of ML models.)
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 在你拟合了一个机器学习模型之后，下一步是评估该模型的准确性。在你能够将模型投入使用之前，你需要知道它在新的数据上预测的效果如何。如果你确定预测性能相当好，你可以在生产环境中部署该模型来分析新数据。同样，如果你评估预测性能不足以完成当前任务，你可以回顾你的数据和模型，尝试改进和优化其准确性。（本章的最后部分介绍了简单的模型优化方法。[第5章](kindle_split_015.html#ch05)、[第7章](kindle_split_018.html#ch07)和[第9章](kindle_split_020.html#ch09)介绍了更复杂的提高机器学习模型预测准确性的方法。）
- en: Properly assessing the predictive performance of an ML model is a nontrivial
    task. We begin this chapter by introducing statistically rigorous techniques to
    evaluate the predictive performance of ML models, demonstrating both pictorially
    and with pseudocode how to perform correct validation of a model.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 正确评估机器学习模型的预测性能是一项非同小可的任务。我们本章首先介绍统计上严格的评估机器学习模型预测性能的技术，通过图表和伪代码演示如何正确验证模型。
- en: From there, we dive into assessment of ML classification models, focusing on
    the typical evaluation metrics and graphical tools used by machine-learning practitioners.
    Then we introduce analogous evaluation tools for regression models. Finally, we
    describe a simple way to optimize the predictive performance of a model through
    parameter tuning.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里，我们深入探讨机器学习分类模型的评估，重点关注机器学习从业者常用的典型评估指标和图形工具。然后，我们介绍回归模型的类似评估工具。最后，我们描述了一种通过参数调整来优化模型预测性能的简单方法。
- en: By the end of the chapter, you’ll be equipped with the means and know-how to
    evaluate the predictive accuracy of the ML models that you built in [chapter 3](kindle_split_013.html#ch03)
    and to optimize those models for predictive accuracy (see [figure 4.1](#ch04fig01)).
    This model evaluation provides the information you need to determine whether the
    model you built is good enough for your use case or requires further optimization.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将具备评估你在[第3章](kindle_split_013.html#ch03)中构建的机器学习模型的预测准确性的手段和知识，以及优化这些模型以实现预测准确性的方法（参见[图4.1](#ch04fig01)）。这种模型评估提供了你需要的信息，以确定你构建的模型是否足够好，或者是否需要进一步的优化。
- en: Figure 4.1\. Evaluation and optimization in the ML workflow
  id: totrans-679
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.1\. 机器学习工作流程中的评估和优化
- en: '![](04fig01.jpg)'
  id: totrans-680
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig01.jpg)'
- en: '4.1\. Model generalization: assessing predictive accuracy for new data'
  id: totrans-681
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 模型泛化：评估新数据的预测准确性
- en: The primary goal of supervised machine learning is accurate prediction. You
    want your ML model to be as accurate as possible when predicting on new data (for
    which the target variable is unknown). Said differently, you want your model,
    which has been built from training data, to generalize well to new data. That
    way, when you deploy the model in production, you can be assured that the predictions
    generated are of high quality.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机器学习的主要目标是准确的预测。你希望你的机器学习模型在预测新数据（目标变量未知）时尽可能准确。换句话说，你希望从训练数据构建的模型能够很好地泛化到新数据。这样，当你将模型部署到生产环境中时，你可以确信生成的预测结果是高质量的。
- en: Therefore, when you evaluate the performance of a model, you want to determine
    *how well that model will perform on new data*. This seemingly simple task is
    wrought with complications and pitfalls that can befuddle even the most experienced
    ML users. This section describes the difficulties that arise when evaluating ML
    models and proposes a simple workflow to overcome those menacing issues and achieve
    unbiased estimates of model performance.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当你评估模型的表现时，你想要确定“该模型在新数据上的表现如何”。这个看似简单的任务充满了复杂性和陷阱，即使是经验最丰富的机器学习用户也可能感到困惑。本节描述了评估机器学习模型时出现的困难，并提出了一个简单的流程来克服这些问题，以实现模型性能的无偏估计。
- en: '4.1.1\. The problem: overfitting and model optimism'
  id: totrans-684
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. 问题：过拟合和模型乐观
- en: To describe the challenges associated with estimating the predictive accuracy
    of a model, it’s easiest to start with an example.
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 要描述与估计模型预测准确度相关的挑战，从例子开始最容易。
- en: Imagine that you want to predict the production of bushels of corn per acre
    on a farm as a function of the proportion of that farm’s planting area that was
    treated with a new pesticide. You have training data for 100 farms for this regression
    problem. As you plot the target (bushels of corn per acre) versus the feature
    (percent of the farm treated), it’s clear that an increasing, nonlinear relationship
    exists, and that the data also has random fluctuations (see [figure 4.2](#ch04fig02)).
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你想要预测一个农场每英亩玉米产量作为使用一种新农药处理的种植面积比例的函数。你为此回归问题有100个农场的训练数据。当你绘制目标（每英亩玉米产量）与特征（农场处理的百分比）之间的关系时，很明显存在一个增加的非线性关系，并且数据也存在随机波动（见[图4.2](#ch04fig02)）。
- en: Figure 4.2\. The training data for the corn production regression problem contains
    a clear signal and noise.
  id: totrans-687
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.2\. 玉米产量回归问题的训练数据包含明显的信号和噪声。
- en: '![](04fig02_alt.jpg)'
  id: totrans-688
  prefs: []
  type: TYPE_IMG
  zh: '![图片](04fig02_alt.jpg)'
- en: 'Now, suppose you want to use a simple nonparametric ML regression modeling
    technique to build a predictive model for corn production as a function of proportion
    of land treated. One of the simplest ML regression models is kernel smoothing.
    *Kernel smoothing* operates by taking local averages: for each new data point,
    the value of the target variable is modeled as the average of the target variable
    for only the training data whose feature value is close to the feature value of
    the new data point. A single parameter, called the *bandwidth parameter*, controls
    the size of the window for the local averaging.'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设你想要使用一种简单的非参数机器学习回归建模技术来构建一个玉米产量作为土地处理比例的函数的预测模型。最简单的机器学习回归模型之一是核平滑。*核平滑*通过取局部平均值来操作：对于每个新的数据点，目标变量的值被建模为仅与新的数据点的特征值相近的训练数据的目标变量值的平均值。一个称为*带宽参数*的单个参数控制局部平均的窗口大小。
- en: '[Figure 4.3](#ch04fig03) demonstrates what happens for various values of the
    kernel-smoothing bandwidth parameters. For large values of the bandwidth, almost
    all of the training data is averaged together to predict the target, at each value
    of the input parameter. This causes the model to be flat and to underfit the obvious
    trend in the training data. Likewise, for small values of the bandwidth, only
    one or two training instances are used to determine the model output at each feature
    value. Therefore, the model effectively traces every bump and wiggle in the data.
    This susceptibility to model the intrinsic noise in the data instead of the true
    signal is called *overfitting*. Where you want to be is somewhere in the Goldilocks
    zone: not too underfit and not too overfit.'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.3](#ch04fig03)展示了核平滑带宽参数各种值时的情况。对于带宽参数的大值，几乎所有的训练数据都被平均在一起来预测目标，对于每个输入参数的每个值。这导致模型变得平坦，并且欠拟合训练数据中的明显趋势。同样，对于带宽参数的小值，每个特征值处的模型输出仅使用一个或两个训练实例来确定。因此，模型实际上追踪了数据中的每一个波峰和波谷。这种对数据内在噪声而不是真实信号的建模敏感性被称为*过拟合*。你想要达到的是在金发姑娘区域：既不过度欠拟合，也不过度过拟合。'
- en: Figure 4.3\. Three fits of a kernel-smoothing regression model to the corn production
    training set. For small values of the bandwidth parameter, the model is overfit,
    resulting in an overly bumpy model. For large values of the bandwidth parameter,
    the model is underfit, resulting in a model that’s too flat. A good choice of
    the tuning parameter results in a fit that looks just right.
  id: totrans-691
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.3\. 核平滑回归模型对玉米产量训练集的三次拟合。对于带宽参数的小值，模型过拟合，导致模型过于崎岖。对于带宽参数的大值，模型欠拟合，导致模型过于平坦。合适的调整参数选择会导致看起来恰到好处的拟合。
- en: '![](04fig03_alt.jpg)'
  id: totrans-692
  prefs: []
  type: TYPE_IMG
  zh: '![图片](04fig03_alt.jpg)'
- en: 'Now, let’s get back to the problem at hand: determining how well your ML model
    will generalize to predict the corn output from data on different farms. The first
    step in this process is to select an evaluation metric that captures the quality
    of your predictions. For regression, the standard metric for evaluation is *mean
    squared error (MSE)*, which is the average squared difference between the true
    value of the target variable and the model-predicted value (later in this chapter,
    you’ll learn about other evaluation metrics for regression and classification).'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到手头的问题：确定你的机器学习模型将如何泛化以预测来自不同农场的玉米产量数据。这个过程的第一步是选择一个评估指标，该指标可以捕捉你预测的质量。对于回归，评估的标准指标是*均方误差（MSE）*，它是目标变量的真实值与模型预测值之间平均平方差的度量（在本章的后面，你将了解回归和分类的其他评估指标）。
- en: 'This is where things get tricky. Evaluated on the training set, the error (measured
    by MSE) of our model predictions gets ever smaller as the bandwidth parameter
    decreases. This is expected: the more flexibility that you allow the model, the
    better it’ll do at tracing the patterns (both the signal and the noise) in the
    training data. But the models with smallest bandwidth are severely overfit to
    the training data because they trace every random fluctuation in the training
    set. Using these models to predict on new data will result in poor predictive
    accuracy, because the new data will have its own unique random noise signatures
    that are different from those in the training set.'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是事情变得复杂的地方。在训练集上评估，随着带宽参数的减小，我们模型预测的错误（用MSE衡量）会越来越小。这是预期的：你允许模型越灵活，它在追踪训练数据中的模式（信号和噪声）方面做得越好。但是，具有最小带宽的模型对训练数据严重过拟合，因为它们追踪训练集中的每一个随机波动。使用这些模型来预测新数据会导致预测精度差，因为新数据将具有其独特的随机噪声特征，这些特征与训练集中的不同。
- en: Thus, a divergence occurs between the training set error and the generalization
    error of an ML model. This divergence is exemplified on the corn production data
    in [figure 4.4](#ch04fig04). For small values of the bandwidth parameter, the
    MSE evaluated on the training set is extremely small, whereas the MSE evaluated
    on new data (in this case, 10,000 new instances) is much larger. Simply put, the
    performance of the predictions of a model evaluated on the training set isn’t
    indicative of the performance of that model on new data. Therefore, it’s dangerous
    to evaluate the performance of a model on the same data that was used to train
    the model.
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，训练集错误和机器学习模型的泛化错误之间出现了差异。这种差异在[图4.4](#ch04fig04)中的玉米产量数据中得到了例证。对于带宽参数的小值，训练集上评估的MSE非常小，而新数据（在这种情况下，10,000个新实例）上的MSE要大得多。简单来说，在训练集上评估的模型预测的性能并不能表明该模型在新数据上的性能。因此，在用于训练模型的数据上评估模型的性能是危险的。
- en: Figure 4.4\. Comparison of the training set error to the error on new data for
    the corn production regression problem. The training set error is an overly optimistic
    measure of the performance of the model for new data, particularly for small values
    of the bandwidth parameter. Using the training set error as a surrogate for the
    prediction error on new data will get you into a lot of trouble.
  id: totrans-696
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.4。玉米产量回归问题中训练集错误与新数据错误的比较。训练集错误是衡量新数据模型性能的一个过于乐观的指标，尤其是对于带宽参数的小值。将训练集错误用作新数据预测错误的替代指标将给你带来很多麻烦。
- en: '![](04fig04_alt.jpg)'
  id: totrans-697
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig04_alt.jpg)'
- en: '|  |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Caution about Double-dipping the training data
  id: totrans-699
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关于双重使用训练数据的警告
- en: Using the training data for both model fitting and evaluation purposes can lead
    you to be overly optimistic about the performance of the model. This can cause
    you to ultimately choose a suboptimal model that performs poorly when predicting
    on new data.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练数据既用于模型拟合又用于评估可能会导致你对模型的性能过于乐观。这可能导致你最终选择一个次优模型，该模型在预测新数据时表现不佳。
- en: '|  |'
  id: totrans-701
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: As you see in the corn production data, choosing the model with the smallest
    training set MSE causes the selection of the model with the smallest bandwidth.
    On the training set, this model yields an MSE of 0.08\. But when applied to new
    data, the same model yields an MSE of 0.50, which is much worse than the optimal
    model (bandwidth = 0.12 and MSE = 0.27).
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在玉米产量数据中看到的，选择具有最小训练集均方误差（MSE）的模型会导致选择具有最小带宽的模型。在训练集上，此模型产生的MSE为0.08。但是，当应用于新数据时，相同的模型产生的MSE为0.50，这比最优模型（带宽=0.12和MSE=0.27）差得多。
- en: You need an evaluation metric that better approximates the performance of the
    model on new data. This way, you can be confident about the accuracy of your model
    when deployed to make predictions on new data. This is the topic of the next subsection.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个更好的评估指标，它能更好地近似模型在新数据上的性能。这样，当你将模型部署到新数据上进行预测时，你可以对模型的准确性有信心。这是下一小节的主题。
- en: '4.1.2\. The solution: cross-validation'
  id: totrans-704
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2. 解决方案：交叉验证
- en: 'We’ve diagnosed the challenge in model evaluation: the training set error isn’t
    indicative of the model error when applied to new data. To get a good estimate
    of what your error rate will be for new data, you must use a more sophisticated
    methodology called *cross-validation* (often abbreviated *CV*) that rigorously
    employs the training set to evaluate what the accuracy will be on new data.'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经诊断了模型评估中的挑战：当应用于新数据时，训练集错误并不能指示模型错误。为了得到新数据错误率的良好估计，你必须使用一种更复杂的方法，称为**交叉验证**（通常缩写为**CV**），该方法严格使用训练集来评估新数据上的准确性。
- en: The two most commonly used methods for cross-validation are the holdout method
    and k-fold cross-validation.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证中最常用的两种方法是保留方法和k折交叉验证。
- en: The holdout method
  id: totrans-707
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 保留方法
- en: Using the same training data to both fit and evaluate the accuracy of a model
    produces accuracy metrics that are overly optimistic. The easiest way around this
    is to use separate training and testing subsets. You use only the training subset
    to fit the model, and only the testing subset to evaluate the accuracy of the
    model.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的训练数据来拟合和评估模型的准确性会产生过于乐观的准确性指标。解决这个问题最简单的方法是使用独立的训练集和测试集。你只使用训练集来拟合模型，而只使用测试集来评估模型的准确性。
- en: This approach is referred to as the *holdout method*, because a random subset
    of the training data is held out from the training process. Practitioners typically
    leave out 20–40% of the data as the testing subset. [Figure 4.5](#ch04fig05) depicts
    the basic algorithmic flow of the holdout method, and [listing 4.1](#ch04ex01)
    provides the Python pseudocode.
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被称为**保留方法**，因为从训练过程中保留了一个随机子集的训练数据。从业者通常将20-40%的数据作为测试集。![图4.5](#ch04fig05)描述了保留方法的基本算法流程，[列表4.1](#ch04ex01)提供了Python伪代码。
- en: Figure 4.5\. Flowchart of the holdout method of cross-validation.
  id: totrans-710
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.5. 交叉验证的保留方法的流程图。
- en: '![](04fig05_alt.jpg)'
  id: totrans-711
  prefs: []
  type: TYPE_IMG
  zh: '![图片](04fig05_alt.jpg)'
- en: Listing 4.1\. Cross-validation with the holdout method
  id: totrans-712
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.1. 使用保留方法的交叉验证
- en: '![](082fig01_alt.jpg)'
  id: totrans-713
  prefs: []
  type: TYPE_IMG
  zh: '![图片](082fig01_alt.jpg)'
- en: 'Now, let’s apply the holdout method to the corn production data. For each value
    of the bandwidth parameter, you apply the holdout method (using a 70/30 split)
    and compute the MSE on the predictions for the held-out 30% of data. [Figure 4.6](#ch04fig06)
    demonstrates how the holdout method estimates of the MSE stack up to the MSE of
    the model when applied to new data. Two main things stand out:'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将保留方法应用于玉米产量数据。对于带宽参数的每个值，你应用保留方法（使用70/30的分割）并计算保留的30%数据的预测MSE。[图4.6](#ch04fig06)展示了保留方法估计的MSE如何与新数据上应用的模型MSE相比较。两个主要的事情很突出：
- en: The error estimates computed by the holdout method are close to the new-data
    error of the model. They’re certainly much closer than the training set error
    estimates ([figure 4.4](#ch04fig04)), particularly for small-bandwidth values.
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留方法计算的错误估计接近模型的新数据误差。它们当然比训练集错误估计（[图4.4](#ch04fig04)）要接近得多，尤其是对于小带宽值。
- en: The holdout error estimates are noisy. They bounce around wildly compared to
    the smooth curve that represents the error on new data.
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留误差估计是嘈杂的。与代表新数据错误的平滑曲线相比，它们波动很大。
- en: Figure 4.6\. Comparison of the holdout error MSE to the MSE on new data, using
    the corn production dataset. The holdout error is an unbiased estimate of the
    error of each model on new data. But it’s a noisy estimator that fluctuates wildly
    between 0.14 and 0.40 for bandwidths in the neighborhood of the optimal model
    (bandwidth = 0.12).
  id: totrans-717
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.6. 使用玉米产量数据集比较保留误差MSE与新数据上的MSE。保留误差是每个模型在新数据上误差的无偏估计。但它是一个嘈杂的估计器，在最佳模型（带宽=0.12）附近的带宽中波动很大，介于0.14和0.40之间。
- en: '![](04fig06_alt.jpg)'
  id: totrans-718
  prefs: []
  type: TYPE_IMG
  zh: '![图片](04fig06_alt.jpg)'
- en: You could beat down the noise by doing repeated random training-testing splits
    and averaging the result. But over multiple iterations, each data point will be
    assigned to the testing set a different number of times, which could bias the
    result.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过重复进行随机训练-测试分割并平均结果来降低噪声。但是，在多次迭代中，每个数据点将被分配到测试集的不同次数，这可能会对结果产生偏差。
- en: A better approach is to do k-fold cross-validation.
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更好的方法是进行k折交叉验证。
- en: K-fold cross-validation
  id: totrans-721
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: K折交叉验证
- en: A better but more computationally intensive approach to cross-validation is
    *k-fold cross-validation*. Like the holdout method, k-fold cross-validation relies
    on quarantining subsets of the training data during the learning process. The
    primary difference is that k-fold CV begins by randomly splitting the data into
    *k* disjoint subsets, called *folds* (typical choices for k are 5, 10, or 20).
    For each fold, a model is trained on all the data *except* the data from that
    fold and is subsequently used to generate predictions for the data from that fold.
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更好但计算量更大的交叉验证方法是**k折交叉验证**。与保留法类似，k折交叉验证在训练过程中依赖于隔离训练数据的一部分。主要区别在于，k折CV首先将数据随机分为*k*个不相交的子集，称为**folds**（k的典型选择是5、10或20）。对于每个folds，使用除了该folds数据之外的所有数据进行模型训练，并随后使用该模型对该folds的数据进行预测。
- en: After all k-folds are cycled through, the predictions for each fold are aggregated
    and compared to the true target variable to assess accuracy. [Figure 4.7](#ch04fig07)
    illustrates k-fold cross-validation, and [listing 4.2](#ch04ex02) provides the
    pseudocode.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有k折都经过循环后，将每个folds的预测汇总并与其真实目标变量进行比较，以评估准确性。[图4.7](#ch04fig07)说明了k折交叉验证，[列表4.2](#ch04ex02)提供了伪代码。
- en: Figure 4.7\. Flowchart of k-fold cross-validation
  id: totrans-724
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.7\. k折交叉验证流程图
- en: '![](04fig07_alt.jpg)'
  id: totrans-725
  prefs: []
  type: TYPE_IMG
  zh: '![04fig07_alt.jpg](04fig07_alt.jpg)'
- en: Finally, let’s apply k-fold cross-validation to the corn production data. For
    each value of the bandwidth parameter, you apply k-fold cross-validation with
    k = 10 and compute the cross-validated MSE on the predictions. [Figure 4.8](#ch04fig08)
    demonstrates how the k-fold cross-validation MSE estimates stack up to the MSE
    of the model when applied to new data. Clearly, the k-fold cross-validation error
    estimate is close to the error of the model on future data.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们将k折交叉验证应用于玉米产量数据。对于带宽参数的每个值，你使用k=10进行k折交叉验证，并在预测上计算交叉验证MSE。[图4.8](#ch04fig08)展示了k折交叉验证MSE估计与应用于新数据时模型MSE的关系。显然，k折交叉验证误差估计接近模型在未来的数据上的误差。
- en: Figure 4.8\. Comparison of the k-fold cross-validation error MSE to the MSE
    on new data, using the corn production dataset. The k-fold CV error is a good
    estimate for how the model will perform on new data, allowing you to use it confidently
    to forecast the error of the model and to select the best model.
  id: totrans-727
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.8\. 使用玉米产量数据集比较k折交叉验证误差MSE与新数据上的MSE。k折CV误差是评估模型在新数据上表现的一个很好的估计，允许你自信地预测模型的误差并选择最佳模型。
- en: '![](04fig08_alt.jpg)'
  id: totrans-728
  prefs: []
  type: TYPE_IMG
  zh: '![04fig08_alt.jpg](04fig08_alt.jpg)'
- en: Listing 4.2\. Cross-validation with k-fold cross-validation
  id: totrans-729
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.2\. 使用k折交叉验证的交叉验证
- en: '![](086fig01_alt.jpg)'
  id: totrans-730
  prefs: []
  type: TYPE_IMG
  zh: '![086fig01_alt.jpg](086fig01_alt.jpg)'
- en: 4.1.3\. Some things to look out for when using cross-validation
  id: totrans-731
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3\. 使用交叉验证时需要注意的一些事项
- en: Cross-validation gives you a way to estimate how accurately your ML models will
    predict when deployed in the wild. This is extremely powerful, because it enables
    you to select the best model for your task.
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证为你提供了一种估计你的机器学习模型在野外部署时的预测准确性的方法。这非常强大，因为它使你能够为你的任务选择最佳模型。
- en: 'But when you apply cross-validation to real-world data, you need to watch out
    for a few things:'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，当你将交叉验证应用于实际数据时，需要注意以下几点：
- en: 'Cross-validation methods (including both the holdout and k-fold methods) assume
    that the training data forms a representative sample from the population of interest.
    If you plan to deploy the model to predict on new data, that data should be well
    represented by the training data. If not, the cross-validation error estimates
    may be overly optimistic for the error rates on future data. Solution: Ensure
    that any potential biases in the training data are addressed and minimized.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉验证方法（包括保留法和k折法）假设训练数据构成了感兴趣人群的代表性样本。如果你计划将模型部署到预测新数据，那么这些数据应该由训练数据很好地表示。如果不是这样，交叉验证误差估计可能对未来的数据误差率过于乐观。解决方案：确保解决并最小化训练数据中的任何潜在偏差。
- en: 'Some datasets use features that are temporal—for instance, using last month’s
    revenue to forecast this month’s revenue. If this is the case with your data,
    you must ensure that features that are available in the future can never be used
    to predict the past. Solution: You can structure your cross-validation holdout
    set or k-folds so that all the training set data is collected previous to the
    testing set.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些数据集使用时间序列特征——例如，使用上个月的收入来预测这个月的收入。如果你的数据是这样的，你必须确保未来可用的特征永远不能用来预测过去。解决方案：你可以构建交叉验证的保留集或k折，以确保所有训练集数据都在测试集之前收集。
- en: 'The larger the number of folds used in k-fold cross-validation, the better
    the error estimates will be, but the longer your program will take to run. Solution:
    Use at least 10 folds (or more) when you can. For models that train and predict
    quickly, you can use leave-one-out cross-validation (k = number of data instances).'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在k折交叉验证中使用更多的折数，误差估计将更好，但你的程序运行时间会更长。解决方案：当你能的时候，至少使用10折（或更多）。对于训练和预测速度快的模型，你可以使用留一法交叉验证（k
    = 数据实例数）。
- en: Next, you’ll build off of these cross-validation tools and take a deeper look
    at how to perform rigorous model evaluation for classification models.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将在此基础上构建交叉验证工具，并深入探讨如何对分类模型进行严格的模型评估。
- en: 4.2\. Evaluation of classification models
  id: totrans-738
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2. 分类模型的评估
- en: We begin our discussion of evaluating classification models by presenting problems
    with only two classes, also known as *binary classification*. [Chapter 3](kindle_split_013.html#ch03)
    introduced binary classification in machine learning as a powerful method for
    predicting a positive/negative outcome based on many factors or variables. A good
    example of binary classification is the detection of diseases or survival predictions.
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过介绍只有两个类别的例子来开始对评估分类模型的讨论，这也被称为*二元分类*。[第三章](kindle_split_013.html#ch03)介绍了在机器学习中，二元分类是一种基于许多因素或变量的强大方法，用于预测正/负结果。二元分类的一个好例子是疾病检测或生存预测。
- en: Imagine that you want to predict whether a Titanic passenger would survive,
    based on personal, social, and economic factors. You’d gather everything you know
    about the passengers and train a classifier that could relate all this information
    to their survival probability. You first saw this example in [chapter 2](kindle_split_012.html#ch02),
    but the first five rows of the Titanic Passengers dataset is shown again in [figure
    4.9](#ch04fig09).
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想根据个人、社会和经济因素预测泰坦尼克号乘客是否会幸存。你会收集关于乘客的所有信息，并训练一个可以将所有这些信息与他们的生存概率相关联的分类器。你第一次看到这个例子是在[第二章](kindle_split_012.html#ch02)，但泰坦尼克号乘客数据集的前五行再次在[图4.9](#ch04fig09)中展示。
- en: Figure 4.9\. The first five rows of the Titanic Passengers dataset. The target
    column indicates whether a passenger survived the sinking of the ship or died.
  id: totrans-741
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.9。泰坦尼克号乘客数据集的前五行。目标列指示乘客是否在船沉没中幸存或死亡。
- en: '![](04fig09_alt.jpg)'
  id: totrans-742
  prefs: []
  type: TYPE_IMG
  zh: '![图片](04fig09_alt.jpg)'
- en: To build your classifier, you feed this dataset into a classification algorithm.
    Because the dataset consists of different types of data, you have to make sure
    the algorithm knows how to deal with these types. As discussed in the previous
    chapters, you might need to process the data prior to training the model, but
    for this chapter you’ll view the classifier as a black box that has learned the
    mapping from the input variables to the target variable. The goal of this section
    is to evaluate the model in order to optimize the prediction accuracy and compare
    with other models.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建你的分类器，你需要将这个数据集输入到分类算法中。因为这个数据集包含不同类型的数据，你必须确保算法知道如何处理这些类型。正如前几章所讨论的，你可能需要在训练模型之前处理数据，但在这个章节中，你将把分类器视为一个黑盒，它已经学会了从输入变量到目标变量的映射。本节的目标是评估模型，以优化预测准确率并与其他模型进行比较。
- en: 'With the data ready, you move to the next task: cross-validation. You’ll divide
    the full dataset into training and testing sets and use the holdout method of
    cross-validation. The model will be built on a training set and evaluated on a
    held-out testing set. It’s important to reiterate that your goal isn’t necessarily
    to obtain the maximum model accuracy on the training data, but to obtain the highest
    predictive accuracy on unseen data. In the model-building phase, you’re not yet
    in possession of this data, by definition, so you pretend that some of the training
    data is hidden for the learning algorithm.'
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备就绪后，你将进入下一个任务：交叉验证。你需要将整个数据集分成训练集和测试集，并使用交叉验证的保留法。模型将在训练集上构建并在保留的测试集上评估。重要的是要重申，你的目标不一定是在训练数据上获得最大的模型准确率，而是要在未见过的数据上获得最高的预测准确率。在模型构建阶段，根据定义，你还没有掌握这些数据，所以你假装一些训练数据对学习算法是隐藏的。
- en: '[Figure 4.10](#ch04fig10) illustrates the dataset-splitting step in this particular
    example.'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.10](#ch04fig10)说明了这个特定示例中的数据分割步骤。'
- en: Figure 4.10\. Splitting the full dataset into training and testing sets allows
    you to evaluate the model.
  id: totrans-746
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.10\. 将整个数据集分成训练集和测试集，可以让你评估模型。
- en: '![](04fig10_alt.jpg)'
  id: totrans-747
  prefs: []
  type: TYPE_IMG
  zh: '![04fig10_alt.jpg](04fig10_alt.jpg)'
- en: 'With the training set ready, you can build the classifier and make predictions
    on the testing set. Following the holdout method of [figure 4.5](#ch04fig05),
    you obtain a list of prediction values: 0 (died) or 1 (survived) for all rows
    in the test set. You then go to step 3 of the evaluation workflow and compare
    these predictions to the actual survival values to obtain a performance metric
    that you can optimize.'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好训练集后，你可以构建分类器并在测试集上进行预测。按照[图4.5](#ch04fig05)的保留法，你将获得一系列预测值：对于测试集中的所有行，值为0（死亡）或1（存活）。然后你进入评估工作流程的第3步，将这些预测值与实际存活值进行比较，以获得一个可以优化的性能指标。
- en: The simplest performance measure of a classification model is to calculate the
    fraction of *correct* answers; if three out of four rows were correctly predicted,
    you’d say the *accuracy* of the model on this particular validation set is 3/4
    = 0.75, or 75%. [Figure 4.11](#ch04fig11) illustrates this result. The following
    sections introduce more-sophisticated ways of performing this comparison.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型的简单性能度量是计算*正确*答案的分数；如果四行中有三行被正确预测，那么你可以说该模型在这个特定验证集上的*准确率*是3/4 = 0.75，或75%。[图4.11](#ch04fig11)说明了这个结果。以下几节将介绍进行这种比较的更复杂的方法。
- en: Figure 4.11\. Comparing the testing set predictions with the actual values gives
    you the accuracy of the model.
  id: totrans-750
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.11\. 将测试集预测值与实际值进行比较，可以得到模型的准确率。
- en: '![](04fig11_alt.jpg)'
  id: totrans-751
  prefs: []
  type: TYPE_IMG
  zh: '![04fig11_alt.jpg](04fig11_alt.jpg)'
- en: 4.2.1\. Class-wise accuracy and the confusion matrix
  id: totrans-752
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 按类别准确率和混淆矩阵
- en: 'The predictions provide more information than simply being correct or not.
    For example, you can analyze the accuracy per class (how many were predicted to
    survive but actually died or survived). For binary classification, you can be
    wrong in two ways: predicting 0 when the correct value is 1, or predicting 1 when
    the correct value is 0\. In the same way, you can be correct in two ways. [Figure
    4.12](#ch04fig12) illustrates.'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 预测提供的信息比仅仅是正确或错误更多。例如，你可以分析每个类别的准确率（预测存活但实际上死亡或存活的人数）。对于二元分类，你可以以两种方式出错：预测0而正确值是1，或者预测1而正确值是0。同样，你也可以以两种方式正确。
    [图4.12](#ch04fig12)说明了这一点。
- en: Figure 4.12\. Counting the class-wise accuracy and error rate gives you more
    information on the model accuracy.
  id: totrans-754
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.12\. 计算按类别准确率和错误率可以为你提供有关模型准确率的更多信息。
- en: '![](04fig12_alt.jpg)'
  id: totrans-755
  prefs: []
  type: TYPE_IMG
  zh: '![04fig12_alt.jpg](04fig12_alt.jpg)'
- en: In many classification problems, it’s useful to go beyond the simple counting
    accuracy and look at this class-wise accuracy, or class confusion. It turns out
    to be useful to display these four numbers in a two-by-two diagram called a *confusion
    matrix*, shown in [figure 4.13](#ch04fig13).
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多分类问题中，除了简单的计数准确率之外，查看这种按类别准确率或类别混淆情况是有用的。结果显示，将这些四个数字显示在一个称为*混淆矩阵*的二维图中非常有用，如图[图4.13](#ch04fig13)所示。
- en: Figure 4.13\. Organizing the class-wise accuracy into a confusion matrix
  id: totrans-757
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.13\. 将按类别准确率组织到混淆矩阵中
- en: '![](04fig13.jpg)'
  id: totrans-758
  prefs: []
  type: TYPE_IMG
  zh: '![04fig13.jpg](04fig13.jpg)'
- en: Each element in the matrix shows the class-wise accuracy or confusion between
    the positive and the negative class. [Figure 4.14](#ch04fig14) relates the specific
    confusion matrix in [figure 4.13](#ch04fig13) to the general concept of receiver
    operating characteristics (ROCs) that you’ll employ widely throughout the rest
    of this book. Although these terms can be a bit confusing at first, they’ll become
    important when talking to other people about the performance of your model.
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵中的每个元素都显示了正负类之间的分类准确度或混淆。![图 4.14](#ch04fig14) 将 [图 4.13](#ch04fig13) 中的特定混淆矩阵与接收器操作特征（ROCs）的一般概念联系起来，这是你在本书的其余部分广泛应用的。虽然这些术语一开始可能有点令人困惑，但当你与其他人谈论你模型的性能时，它们将变得很重要。
- en: Figure 4.14\. The confusion matrix for your binary classifier tested on only
    four rows. The ROC metrics pointed out in the figure are chopped up and explained
    in the bottom box.
  id: totrans-760
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '![图 4.14](04fig14.jpg)'
- en: '![](04fig14.jpg)'
  id: totrans-761
  prefs: []
  type: TYPE_IMG
  zh: '![图片](04fig14.jpg)'
- en: 4.2.2\. Accuracy trade-offs and ROC curves
  id: totrans-762
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '![图 4.15](#ch04fig15) 展示了排序概率向量和设置 0.7 阈值的过程。现在所有高于这条线的行都被预测为生存，你可以将它们与实际标签进行比较，以获得在此特定阈值下的混淆矩阵和
    ROC 指标。如果你为从 0 到 1 的所有阈值都遵循这个过程，你定义了*ROC 曲线*，如 [图 4.16](#ch04fig16) 所示。'
- en: So far you’ve looked only at predictions for which the output is the predicted
    class; in our Titanic example, 1 for survival and 0 otherwise. Machine-learning
    predictions usually hold a degree of uncertainty, and many classification algorithms
    output not only the zero-one predictions, but the full prediction *probabilities*.
    For example, what was simply predicted as *survived* in our Titanic model may
    have had a probability of survival of 0.8, 0.99, or 0.5\. It’s clear that there’s
    a big difference in the confidence of these answers, and in this section you’ll
    take advantage of this information to evaluate your models in more detail.
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你只看了输出为预测类别的预测；在我们的泰坦尼克号例子中，1 表示生存，否则为 0。机器学习预测通常包含一定的不确定性，许多分类算法不仅输出零一预测，还输出完整的预测*概率*。例如，在我们的泰坦尼克号模型中简单地预测为*生存*的实例可能具有
    0.8、0.99 或 0.5 的生存概率。很明显，这些答案的置信度有很大的差异，在本节中，你将利用这些信息更详细地评估你的模型。
- en: The output of a *probabilistic classifier* is what we call the *probability
    vectors* or *class probabilities*. For every row in the test set, you get a real-valued
    number from 0 to 1 for every class in the classifier (summing to 1). Until now,
    you’ve made predictions by considering probabilities above 0.5 to determine the
    class predictions, from which you calculated all the performance metrics from
    the previous section. We say that the *threshold* that determines the class is
    0.5\. It’s clear that you could choose any other threshold and would get different
    values for all of your metrics.
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: '*概率分类器*的输出就是我们所说的*概率向量*或*类别概率*。对于测试集中的每一行，你都会为分类器中的每个类别得到一个从 0 到 1 的实数值（总和为
    1）。到目前为止，你通过考虑大于 0.5 的概率来确定类别预测，并据此计算上一节中所有性能指标。我们说确定类别的*阈值*是 0.5。很明显，你可以选择任何其他阈值，并会得到所有指标的不同值。'
- en: '[Figure 4.15](#ch04fig15) shows the process of sorting the probability vectors
    and setting a threshold of 0.7\. All rows above the line are now predicted to
    survive, and you can compare these to the actual labels to get the confusion matrix
    and the ROC metrics at this particular threshold. If you follow this process for
    all thresholds from 0 to 1, you define the *ROC curve*, shown in [figure 4.16](#ch04fig16).'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.15](04fig15_alt.jpg)'
- en: Figure 4.15\. A subset of probabilistic predictions from the Titanic test set.
    After sorting the full table by decreasing survival probability, you can set a
    threshold and consider all rows above this threshold as survived. Note that the
    indices are maintained so you know which original row the instance refers to.
  id: totrans-766
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '![图 4.15](04fig15_alt.jpg)'
- en: '![](04fig15_alt.jpg)'
  id: totrans-767
  prefs: []
  type: TYPE_IMG
  zh: 4.2.2. 准确度权衡和 ROC 曲线
- en: From [figure 4.16](#ch04fig16), you can read out the confusion matrix at all
    thresholds, making the ROC curve a powerful visualization tool when you’re evaluating
    classifier performance. Given the true and predicted labels from any cross-validation
    process, the ROC curve is calculated as shown in [listing 4.3](#ch04ex03).
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: 从 [图 4.16](#ch04fig16) 中，你可以读出所有阈值点的混淆矩阵，这使得 ROC 曲线在评估分类器性能时成为一个强大的可视化工具。给定任何交叉验证过程中的真实和预测标签，ROC
    曲线按照 [列表 4.3](#ch04ex03) 中的方法计算。
- en: Figure 4.16\. The ROC curve defined by calculating the confusion matrix and
    ROC metrics at 100 threshold points from 0 to 1\. By convention, you plot the
    false-positive rate on the x-axis and the true-positive rate on the y-axis.
  id: totrans-769
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.16\. 通过计算从 0 到 1 的 100 个阈值点的混淆矩阵和 ROC 指标定义的 ROC 曲线。按照惯例，你在 x 轴上绘制假阳性率，在
    y 轴上绘制真阳性率。
- en: '![](04fig16_alt.jpg)'
  id: totrans-770
  prefs: []
  type: TYPE_IMG
  zh: '![图片](04fig16_alt.jpg)'
- en: Listing 4.3\. The ROC curve
  id: totrans-771
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.3\. ROC 曲线
- en: '![](092fig01_alt.jpg)'
  id: totrans-772
  prefs: []
  type: TYPE_IMG
  zh: '![图片](092fig01_alt.jpg)'
- en: If you follow the ROC curve, you see that when the false-positive rate increases,
    the true-positive rate decreases. This trade-off is the “no free lunch” of machine
    learning because you’re able to sacrifice the fraction of instances that you classify
    correctly for more certainty that you’re correct, and vice versa, depending on
    your choice of the probability threshold parameter.
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你跟随 ROC 曲线，你会看到当假阳性率增加时，真阳性率会降低。这种权衡是机器学习的“没有免费午餐”，因为你能够为了获得更高的正确性而牺牲你正确分类的实例比例，反之亦然，这取决于你选择的概率阈值参数。
- en: In real-world scenarios, this trade-off can be extremely important to evaluate.
    If you’re classifying whether a patient has cancer or not, it’s much better to
    classify a few extra healthy patients as sick, and avoid classifying any sick
    patients as healthy. So you’d select the threshold that would minimize the false-negative
    rate, and hence maximize the true-positive rate and place you as far as possible
    in the top of the ROC plot while sacrificing the false-positive rate.
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实场景中，这种权衡可能非常重要，需要评估。如果你正在分类一个患者是否有癌症，将几个额外的健康患者错误地分类为患病患者，并避免将任何患病患者错误地分类为健康患者，会更好。因此，你会选择一个可以最小化假阴性率的阈值，从而最大化真阳性率，并将你尽可能放置在
    ROC 图表的顶部，同时牺牲假阳性率。
- en: Another good example is spam filters, where you’ll need to choose between unwanted
    emails being shown in your inbox or wanted emails being dumped in the spam folder.
    Or credit-card companies detecting fraudulent activities—would you rather call
    your customers often with false alarms or risk missing potential fraudulent transactions?
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个很好的例子是垃圾邮件过滤器，你需要在你收件箱中显示不想要的电子邮件或把想要的电子邮件放入垃圾邮件文件夹之间做出选择。或者信用卡公司检测欺诈活动——你更愿意经常给客户发出虚假警报，还是冒着错过潜在欺诈交易的风险？
- en: 'In addition to the trade-off information, the ROC curve itself also provides
    a view of the overall performance of the classifier. A perfect classifier would
    have no false positives and no missed detections, so the curve would be pushed
    to the top-left corner, as illustrated in [figure 4.17](#ch04fig17). This leads
    us naturally to another evaluation metric: the area under the ROC curve (AUC).
    The larger this area, the better the classification performance. The AUC is a
    widely used choice for evaluating and comparing models, although in most cases
    it’s important to inspect the full ROC curve in order to understand the performance
    trade-offs. You’ll use the ROC curve and the AUC evaluation metric to validate
    classification models throughout the rest of this book.'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 除了权衡信息之外，ROC 曲线本身也提供了对分类器整体性能的视图。一个完美的分类器不会有假阳性也不会有漏检，所以曲线会推向左上角，如图 [图 4.17](#ch04fig17)
    所示。这自然引导我们到另一个评估指标：ROC 曲线下方的面积（AUC）。这个面积越大，分类性能越好。AUC 是评估和比较模型时广泛使用的选择，尽管在大多数情况下，检查完整的
    ROC 曲线对于理解性能权衡非常重要。你将在本书的其余部分使用 ROC 曲线和 AUC 评估指标来验证分类模型。
- en: 'Figure 4.17\. The ROC curve illustrates the overall model performance. You
    can quantify this by defining the AUC metric: the area under the ROC curve.'
  id: totrans-777
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.17\. ROC 曲线说明了整体模型性能。你可以通过定义 AUC 指标来量化这一点：ROC 曲线下方的面积。这个面积越大，分类性能越好。AUC
    是评估和比较模型时广泛使用的选择，尽管在大多数情况下，检查完整的 ROC 曲线对于理解性能权衡非常重要。你将在本书的其余部分使用 ROC 曲线和 AUC 评估指标来验证分类模型。
- en: '![](04fig17_alt.jpg)'
  id: totrans-778
  prefs: []
  type: TYPE_IMG
  zh: '![图片](04fig17_alt.jpg)'
- en: Listing 4.4\. The area under the ROC curve
  id: totrans-779
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.4\. ROC 曲线下方的面积
- en: '![](093fig01_alt.jpg)'
  id: totrans-780
  prefs: []
  type: TYPE_IMG
  zh: '![图片](093fig01_alt.jpg)'
- en: 4.2.3\. Multiclass classification
  id: totrans-781
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3\. 多类分类
- en: So far you’ve looked only at binary, or two-class, classification problems,
    but luckily you can use many of the same tools for multiclass classifiers. A well-known
    multiclass classification problem is that of handwritten digit recognition. We
    all send physical mail from time to time, and there’s a good chance that a machine-learning
    algorithm has been used in the process of determining the endpoint address of
    your letter. That sounds like a huge challenge if your handwriting is anything
    like ours, but such automated systems have nevertheless been in use by postal
    services for decades.
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你只看过二进制或双类别的分类问题，但幸运的是，你可以使用许多相同的工具来处理多类分类器。一个著名的多类分类问题就是手写数字识别。我们时不时都会发送实体邮件，有很大可能性在这个过程中已经使用了机器学习算法来确定你的信件的终点地址。如果你的字迹像我们一样，这听起来像是一个巨大的挑战，但这样的自动化系统已经由邮政服务使用了几十年。
- en: Because of the early success of machine learning on handwritten digit recognition,
    this example has been used throughout the ML literature as a benchmark of multiclass
    classification performance. The idea is to scan the handwritten digits and divide
    them into images with one letter in each. You then use image-processing algorithms
    or build a multiclass classifier on the raw grayscale pixels that can predict
    the digit. [Figure 4.18](#ch04fig18) shows a few examples of the handwritten digit
    dataset known as MNIST.
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习在手写数字识别上的早期成功，这个例子在ML文献中一直被用作多类分类性能的基准。想法是扫描手写数字，并将它们分成每个图像中有一个字母的图像。然后你使用图像处理算法或在原始灰度像素上构建一个多类分类器来预测数字。[图4.18](#ch04fig18)显示了名为MNIST的手写数字数据集的一些示例。
- en: Figure 4.18\. Handwritten digits in the MNIST dataset. The entire dataset consists
    of 80,000 such digits, each in a 28 x 28–pixel image. Without any image processing,
    each row of our dataset then consists of a known label (0 to 9) and 784 features
    (one for each of the 28 x 28 pixels).
  id: totrans-784
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.18\. MNIST数据集中的手写数字。整个数据集包含80,000个这样的数字，每个数字都是一个28 x 28像素的图像。没有任何图像处理，我们的数据集的每一行都包含一个已知的标签（0到9）和784个特征（每个28
    x 28像素一个）。
- en: '![](04fig18.jpg)'
  id: totrans-785
  prefs: []
  type: TYPE_IMG
  zh: '![图4.18的替代文本](04fig18.jpg)'
- en: You use the random forest algorithm (introduced in [chapter 3](kindle_split_013.html#ch03))
    to build a classifier from the training set, and you generate the confusion matrix
    from the held-out testing set. Remember that you’ve worked with the confusion
    matrix for only binary classification. Luckily, you can easily define it for multiple
    classes, as every element in the matrix is the class on the row versus the class
    on the column. For the MNIST classifier, you can see in [figure 4.19](#ch04fig19)
    that most of the power is located on the matrix diagonal, as it should be, because
    it shows the number of instances that are *correctly* classified for each digit.
    The largest nondiagonal items show where the classifier is most confused. Inspecting
    the figure, you can see that the greatest confusion occurs between digits 4 and
    9, 3 and 5, and 7 and 9, which makes sense, given what you know about the shape
    of the digits.
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用随机森林算法（在第3章中介绍）从训练集构建一个分类器，并从保留的测试集生成混淆矩阵。记住，你只处理过二类分类的混淆矩阵。幸运的是，你可以轻松地为多个类别定义它，因为矩阵中的每个元素都是行上的类别与列上的类别。对于MNIST分类器，你可以在[图4.19](#ch04fig19)中看到，大部分的力量都位于矩阵的对角线上，正如它应该的那样，因为它显示了每个数字被*正确*分类的实例数量。最大的非对角元素显示了分类器最困惑的地方。检查这个图，你可以看到最大的混淆发生在数字4和9、3和5、以及7和9之间，这符合你对数字形状的了解。
- en: Figure 4.19\. The confusion matrix for the 10-class MNIST handwritten digit
    classification problem
  id: totrans-787
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.19\. 10类MNIST手写数字分类问题的混淆矩阵
- en: '![](04fig19_alt.jpg)'
  id: totrans-788
  prefs: []
  type: TYPE_IMG
  zh: '![图4.19的替代文本](04fig19_alt.jpg)'
- en: The reason for displaying the class-wise accuracy in the form of a matrix is
    to take advantage of our excellent visual abilities to process more information.
    In [figure 4.19](#ch04fig19), you can clearly see how applying contrast to the
    confusion matrix can help take advantage of this ability.
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 以矩阵形式显示类别准确率的原因是为了利用我们出色的视觉能力来处理更多信息。在[图4.19](#ch04fig19)中，你可以清楚地看到如何通过对比度增强混淆矩阵来利用这种能力。
- en: So how do you generate the ROC curve for multiclass classifiers? The ROC curve
    is in principle applicable to only binary classification problems, because you
    divide the predictions into *positive* and *negative* classes in order to get
    ROC metrics such as the true-positive rate and false-positive rate commonly used
    on the ROC curve axis. To simulate binary classification in a multiclass problem,
    you use the *one-versus-all* trick. For each class, you denote the particular
    class as the *positive* class, and everything else as the *negative* class, and
    you draw the ROC curve as usual. The 10 ROC curves from running this process on
    the MNIST classifier are shown in [figure 4.20](#ch04fig20). The most accurately
    classified digits are 0 and 1, consistent with the confusion matrix in [figure
    4.19](#ch04fig19). The confusion matrix, however, is generated from the most probable
    class predictions, whereas the ROC curve shows the performance of the class at
    all probability thresholds.
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何为多类分类器生成ROC曲线呢？原则上，ROC曲线仅适用于二元分类问题，因为你在预测中将类别分为*正类*和*负类*，以便获得ROC曲线轴上常用的如真正例率（true-positive
    rate）和假正例率（false-positive rate）等ROC指标。为了在多类问题中模拟二元分类，你使用*一对多*技巧。对于每个类别，你将特定类别标记为*正类*，其余所有类别作为*负类*，然后像往常一样绘制ROC曲线。运行MNIST分类器进行此过程得到的10个ROC曲线如图4.20所示。最准确分类的数字是0和1，这与图4.19中的混淆矩阵一致。然而，混淆矩阵是由最可能的类别预测生成的，而ROC曲线则显示了类别在所有概率阈值下的性能。
- en: Figure 4.20\. The ROC curves for each class of the MNIST 10-class classifier
    using the one-versus-all method for simulating a binary classification problem.
    Note that because the classifier is so good, we’ve zoomed closely into the top
    corner of the ROC curve in order to see any differences in the model performance
    between the classes. The AUC is calculated for each class and also shows a well-performing
    model overall.
  id: totrans-791
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.20。使用一对多方法模拟二元分类问题时，MNIST 10类分类器每个类别的ROC曲线。请注意，由于分类器非常出色，我们已将ROC曲线的右上角放大，以便观察不同类别之间模型性能的差异。每个类别的AUC都进行了计算，并且整体模型表现良好。
- en: '![](04fig20_alt.jpg)'
  id: totrans-792
  prefs: []
  type: TYPE_IMG
  zh: '![图4.20的替代文本](04fig20_alt.jpg)'
- en: Keep in mind, however, that the multiclass ROC curves don’t show the entire
    confusion matrix on the curve. In principle, there’s a full 10 x 10 confusion
    matrix at every point on the ROC curve, but we can’t visualize this in a sufficiently
    simple way. In the multiclass case, it’s therefore important to look at both the
    confusion matrix and the ROC curve.
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，多类ROC曲线并不在曲线上显示整个混淆矩阵。原则上，在ROC曲线的每个点上都有一个完整的10 x 10混淆矩阵，但我们无法以足够简单的方式可视化这一点。在多类情况下，因此重要的是要查看混淆矩阵和ROC曲线。
- en: 4.3\. Evaluation of regression models
  id: totrans-794
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3. 回归模型的评估
- en: You’ve already looked at regression models in previous chapters. Generally,
    *regression* is the term you use for models that predict a numeric outcome, such
    as an integer or floating-point value. For regression, you use a different set
    of performance metrics that we introduce in this section.
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经在之前的章节中查看过回归模型。通常，*回归*是用于预测数值结果（如整数或浮点值）的模型的术语。对于回归，你使用本节中介绍的不同性能指标集。
- en: You’ll use the Auto MPG dataset, first introduced in [chapter 2](kindle_split_012.html#ch02),
    as the working example in this section. [Figure 4.21](#ch04fig21) shows a small
    subset of this dataset. You run this dataset through all the necessary data transformations
    (see [section 2.2](kindle_split_012.html#ch02lev1sec2) for more information about
    data transformations) and choose an appropriate model as discussed in [chapters
    2](kindle_split_012.html#ch02) and [3](kindle_split_013.html#ch03). In this case,
    you’re interested in measuring the model performance.
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用在[第2章](kindle_split_012.html#ch02)中首次介绍的Auto MPG数据集作为本节的工作示例。[图4.21](#ch04fig21)显示了该数据集的一个小子集。你将此数据集通过所有必要的数据转换（有关数据转换的更多信息，请参阅[第2.2节](kindle_split_012.html#ch02lev1sec2)），并选择一个适当模型，如[第2章](kindle_split_012.html#ch02)和[第3章](kindle_split_013.html#ch03)中讨论的那样。在这种情况下，你感兴趣的是测量模型性能。
- en: Figure 4.21\. A subset of the Auto MPG dataset
  id: totrans-797
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.21。Auto MPG数据集的一个子集
- en: '![](04fig21_alt.jpg)'
  id: totrans-798
  prefs: []
  type: TYPE_IMG
  zh: '![图4.21的替代文本](04fig21_alt.jpg)'
- en: 'Using the basic model-evaluation workflow introduced at the beginning of this
    chapter, you use the data and your choice of algorithm to build a cross-validated
    regression model. Potential model performance metrics used in this process are
    introduced in the following sections, but [figure 4.22](#ch04fig22) shows the
    most basic visualization of the regression performance on which those metrics
    are based: the scatter plot of predicted versus actual values.'
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本章开头介绍的基模型评估工作流程，您使用数据和所选算法构建一个交叉验证的回归模型。在以下各节中介绍了在此过程中使用的潜在模型性能指标，但 [图 4.22](#ch04fig22)
    展示了基于这些指标的回归性能的最基本可视化：预测值与实际值之间的散点图。
- en: Figure 4.22\. Scatter plot of the predicted MPG versus actual values from the
    testing set. The diagonal line shows the optimal model.
  id: totrans-800
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.22\. 测试集中预测 MPG 与实际值的散点图。对角线表示最佳模型。
- en: '![](04fig22_alt.jpg)'
  id: totrans-801
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig22_alt.jpg)'
- en: 4.3.1\. Using simple regression performance metrics
  id: totrans-802
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 使用简单的回归性能指标
- en: 'In contrast to classification models, regression carries no simple notion of
    a *correct* prediction. A numeric prediction is in general unlikely to be exactly
    right, but it can be *close to* or *far from* the correct value. This is also
    a consequence of the nature of what it means to be a *correct* value, because
    we usually consider numerical measurements to be drawn from a distribution with
    a degree of uncertainty known as the *error*. This section introduces two simple
    metrics to measure the regression performance: the root-mean-square error (the
    square root of the MSE) and the R-squared value.'
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 与分类模型不同，回归没有简单的 *正确* 预测的概念。数值预测通常不太可能完全正确，但它可以是 *接近* 或 *远离* 正确值。这也是正确值含义本质的后果，因为我们通常认为数值测量是从具有称为
    *误差* 的不确定程度的分布中抽取的。本节介绍了两个简单的指标来衡量回归性能：均方根误差（MSE 的平方根）和 R² 值。
- en: The simplest form of performance measurement of a regression model is the *root-mean-square
    error*, or *RMSE*. This estimator looks at the difference from each of the predicted
    values to the known values, and calculates the mean in a way that’s immune to
    the fact that predicted values can be both higher and lower than the actual values.
    [Figure 4.23](#ch04fig23) illustrates RMSE calculation.
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型性能的最简单测量方法是 *均方根误差*，或称 *RMSE*。这个估计量查看每个预测值与已知值之间的差异，并以一种不受预测值可能高于或低于实际值这一事实影响的方式计算平均值。[图
    4.23](#ch04fig23) 展示了 RMSE 的计算过程。
- en: 'Figure 4.23\. An RMSE calculation: in the equation, y[i] and x[i] are the i^(th)
    target and feature vector, respectively, and f(x) denotes the application of the
    model to the feature vector, returning the predicted target value.'
  id: totrans-805
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.23\. RMSE 计算：在方程中，y[i] 和 x[i] 分别表示第 i 个目标和特征向量，而 f(x) 表示将模型应用于特征向量，返回预测的目标值。
- en: '![](04fig23_alt.jpg)'
  id: totrans-806
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig23_alt.jpg)'
- en: To encourage a better understanding of the details in the RMSE calculation,
    the following listing shows a code snippet.
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: 为了鼓励更好地理解 RMSE 计算的细节，以下列表展示了一段代码片段。
- en: Listing 4.5\. The root-mean-square error
  id: totrans-808
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.5\. 均方根误差
- en: '[PRE8]'
  id: totrans-809
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The advantage of RMSE is that the result is in the same units as the values
    themselves, but it’s also a disadvantage in the sense that the RMSE value depends
    on the scale of the problem, and thus isn’t easily comparable across datasets.
    If the predicted or actual values are larger numbers, the RMSE will be correspondingly
    higher. Although this isn’t a problem when comparing models in the same project,
    it can be a challenge to understand the overall model performance and compare
    it to other models in general.
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE 的优点是结果与自身值具有相同的单位，但这也意味着 RMSE 值依赖于问题的规模，因此在不同数据集之间不容易进行比较。如果预测值或实际值是较大的数字，RMSE
    将相应地更高。尽管在比较同一项目中的模型时这不是问题，但理解整体模型性能并将其与其他模型进行比较可能是一个挑战。
- en: To overcome this, often it’s worthwhile to also compute the R-squared, or R²,
    metric, whose response is relative and always in the 0–1 range. If the model can
    predict the data better, the R-squared value is closer to 1\. The following listing
    shows more details of the R-squared calculation.
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一点，通常计算 R² 或 R² 指标也是值得的，其响应是相对的，并且始终在 0–1 范围内。如果模型可以更好地预测数据，R² 值将更接近 1。以下列表展示了
    R² 计算的更多细节。
- en: Listing 4.6\. The R-squared calculation
  id: totrans-812
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.6\. R² 计算方法
- en: '[PRE9]'
  id: totrans-813
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Whether using MSE, RMSE, or R² as the evaluation metric, you should always
    keep the following in mind:'
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: 无论使用MSE、RMSE还是R²作为评估指标，你都应该始终记住以下几点：
- en: Always use cross-validation to assess the model. If you don’t, the metrics will
    always improve with increasing model complexity, causing overfitting.
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总是使用交叉验证来评估模型。如果不这样做，随着模型复杂性的增加，指标总是会提高，导致过拟合。
- en: Wherever possible, the evaluation metric should align with the problem at hand.
    For instance, if predicting MPG from automobile features, an RMSE of 5 means that
    you expect the average prediction to differ from the true MPG by 5 miles per gallon.
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在可能的情况下，评估指标应该与手头的问题相一致。例如，如果从汽车特征预测MPG，RMSE为5意味着你期望平均预测值与真实MPG相差5英里/加仑。
- en: In addition, regression uses lots of other evaluation metrics, many of which
    have built-in penalization for overfitting (and thus don’t require cross-validation).
    Examples include the Akaikie information criterion (AIC) and Bayesian information
    criterion (BIC). Most textbooks on regression analysis cover these and more advanced
    topics.
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，回归还使用了许多其他评估指标，其中许多内置了对过拟合的惩罚（因此不需要交叉验证）。例如，包括赤池信息准则（AIC）和贝叶斯信息准则（BIC）。大多数回归分析教科书都涵盖了这些以及更高级的主题。
- en: 4.3.2\. Examining residuals
  id: totrans-818
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2. 检查残差
- en: In the previous section, you saw how the residuals, the distance between the
    predicted and actual values, were used for both of the simple metrics introduced.
    These residuals can also be interesting to analyze visually themselves.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你看到了如何使用残差，即预测值和实际值之间的距离，用于介绍的两个简单指标。这些残差本身也值得进行视觉分析。
- en: '[Figure 4.24](#ch04fig24) shows an example residual plot for our MPG dataset.
    This presents the same information as in the scatter plot in [figure 4.23](#ch04fig23),
    but zoomed in on the scale of the residuals. In an ideal case, you expect the
    residuals to be distributed randomly around the 0-line. In the lower end of the
    figure, MPG values from 10 to 35, it looks like the residuals are randomly distributed
    around the 0-line, maybe with a slight bias toward overestimating the values.
    At values 35–45, however, you can see a clear bias toward underestimating the
    values, resulting in larger residual values. You can use this information to improve
    the model, either by tweaking model parameters, or by processing or amending the
    data. If you can acquire additional data, you could try to obtain labels for a
    few more high-MPG examples. In this case, you could find a few more high-MPG cars
    and add them to the dataset in order to improve the predictions in that part of
    the scale.'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.24](#ch04fig24) 展示了我们MPG数据集的一个示例残差图。这展示了与图4.23（#ch04fig23）中的散点图相同的信息，但聚焦于残差的尺度。在理想情况下，你期望残差随机分布在0线周围。在图的下端，MPG值从10到35，看起来残差是随机分布在0线周围的，可能略微偏向高估值。然而，在35-45的值中，你可以看到明显的低估值的倾向，导致较大的残差值。你可以使用这些信息来改进模型，要么通过调整模型参数，要么通过处理或修正数据。如果你可以获取更多数据，你可以尝试为一些高MPG示例获取标签。在这种情况下，你可以找到一些更多的高MPG汽车并将它们添加到数据集中，以改进该尺度部分的预测。'
- en: Figure 4.24\. The residual plot from predictions on the MPG dataset. At the
    horizontal 0-line, the residual is 0.
  id: totrans-821
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.24. 在MPG数据集上的预测残差图。在水平0线上，残差为0。
- en: '![](04fig24_alt.jpg)'
  id: totrans-822
  prefs: []
  type: TYPE_IMG
  zh: '![04fig24_alt](04fig24_alt.jpg)'
- en: You’ve seen how cross-validation is used to test models and some of the performance
    metrics you can use to evaluate the results. For the simplest models, this is
    a matter of training, testing, and computing the appropriate performance metric(s).
    More-sophisticated algorithms have tuning parameters—knobs that can be turned
    by the user—that affect how they’re trained and applied. Each combination of settings
    yields a different mode. In the next section, you’ll see how sometimes a small
    adjustment can make a big difference in the results.
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到了如何使用交叉验证来测试模型以及你可以用来评估结果的一些性能指标。对于最简单的模型，这只是一个训练、测试和计算适当的性能指标的问题。更复杂的算法有可调整的参数——用户可以调整的旋钮，这会影响它们的训练和应用方式。每种设置组合产生不同的模式。在下一节中，你将看到有时微小的调整可以在结果中产生很大的差异。
- en: 4.4\. Model optimization through parameter tuning
  id: totrans-824
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4. 通过参数调整进行模型优化
- en: Most machine-learning models come endowed with one or more *tuning parameters*
    that control the inner workings of the learning algorithm. These tuning parameters
    typically control the complexity of the relationship between the input features
    and target variable. As a result, the tuning parameters can have a strong influence
    on the fitted model and its predictive accuracy on new data.
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习模型都内置了一个或多个*调整参数*，这些参数控制学习算法的内部工作原理。这些调整参数通常控制输入特征与目标变量之间关系的复杂性。因此，调整参数可以对拟合模型及其在新数据上的预测准确性产生强烈影响。
- en: For example, in [section 4.1](#ch04lev1sec1) you saw how a single turning parameter
    (the bandwidth in a kernel-smoothing regression algorithm) can cause wildly different
    model fits in the corn production dataset. For small values of the bandwidth parameter,
    the regression function was overly bumpy and overfit the data. Likewise, for large
    values of the bandwidth parameter, the regression function was too smooth and
    underfit the data.
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[第4.1节](#ch04lev1sec1)中，您看到了单个调整参数（核平滑回归算法中的带宽）如何导致在玉米产量数据集中产生截然不同的模型拟合。对于带宽参数的小值，回归函数过于颠簸，过度拟合了数据。同样，对于带宽参数的大值，回归函数过于平滑，欠拟合了数据。
- en: This section introduces a rigorous methodology to optimize ML models with respect
    to the machine-learning algorithm tuning parameters.
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了一种严格的方法来优化机器学习算法的调整参数。
- en: 4.4.1\. ML algorithms and their tuning parameters
  id: totrans-828
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1\. 机器学习算法及其调整参数
- en: 'Each machine-learning algorithm contains a different set of tuning parameters
    that control how the algorithm uses training data to build a model. As the algorithms
    become more sophisticated, typically the tuning parameters become more numerous
    and esoteric. Here are the standard tuning parameters for some of the popular
    classification algorithms that you learned about in [chapter 3](kindle_split_013.html#ch03),
    listed in order of increasing complexity:'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: 每个机器学习算法都包含一组不同的调整参数，这些参数控制算法如何使用训练数据来构建模型。随着算法变得更加复杂，通常调整参数的数量也会更多，且更加复杂。以下是您在[第3章](kindle_split_013.html#ch03)中学习的一些流行分类算法的标准调整参数，按复杂度递增的顺序列出：
- en: '***Logistic regression—*** None'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***逻辑回归—*** 无'
- en: '***K-nearest neighbors—*** Number of nearest neighbors to average'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***K近邻—*** 用于平均的最近邻数量'
- en: '***Decision trees—*** Splitting criterion, max depth of tree, minimum samples
    needed to make a split'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***决策树—*** 分割标准，树的最大深度，进行分割所需的最小样本数'
- en: '***Kernel SVM—*** Kernel type, kernel coefficient, penalty parameter'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***核支持向量机—*** 核类型，核系数，惩罚参数'
- en: '***Random forest—*** Number of trees, number of features to split in each node,
    splitting criterion, minimum samples needed to make a split'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***随机森林—*** 树的数量，每个节点中用于分割的特征数，分割标准，进行分割所需的最小样本数'
- en: '***Boosting—*** Number of trees, learning rate, max depth of tree, splitting
    criterion, minimum samples needed to make a split'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***提升—*** 树的数量，学习率，树的最大深度，分割标准，进行分割所需的最小样本数'
- en: 'As an example, think back to [chapter 3](kindle_split_013.html#ch03), where
    you applied a kernel SVM to the Titanic Passengers dataset. You saw that the model
    fit for two choices of the kernel coefficient parameter (called *gamma*) in [figures
    3.8](kindle_split_013.html#ch03fig08) and [3.9](kindle_split_013.html#ch03fig09).
    Note that the fits are different: setting gamma = 0.01 produces a complex, segmented
    decision boundary between the two classes, whereas setting gamma = 0.1 creates
    a smoother model. In this case, the fitted model is highly sensitive to the choice
    of the tuning parameter gamma.'
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 作为例子，回想一下[第3章](kindle_split_013.html#ch03)，在那里您将核支持向量机应用于泰坦尼克号乘客数据集。您看到，在[图3.8](kindle_split_013.html#ch03fig08)和[图3.9](kindle_split_013.html#ch03fig09)中，两种核系数参数（称为*gamma*）的选择会导致模型拟合不同：设置gamma
    = 0.01产生了一个复杂、分割的决策边界，而设置gamma = 0.1则创建了一个更平滑的模型。在这种情况下，拟合的模型对调整参数gamma的选择非常敏感。
- en: What makes this difficult is that the appropriate choice for each tuning parameter
    for a given algorithm is entirely dependent on the problem and data at hand. What
    works well for one problem isn’t necessarily appropriate for the next problem.
    Relying on heuristics and rule-of-thumb default tuning parameter settings may
    lead to poor predictive performance. Rigorous selection of tuning parameters is
    critical to ensure that your models are as accurate as they can be with the given
    data.
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: 使这变得困难的是，对于给定算法的每个调整参数的正确选择完全依赖于特定的问题和数据。对于一个问题有效的东西不一定适用于下一个问题。依赖于启发式和经验法则的默认调整参数设置可能会导致预测性能不佳。严格选择调整参数对于确保您的模型尽可能准确至关重要。
- en: 4.4.2\. Grid search
  id: totrans-838
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2\. 网格搜索
- en: 'The standard way to optimize the choice of tuning parameters for an ML model
    is via a brute-force *grid search*. As you map out the following basic grid-search
    algorithm, note that this strategy ties together the material on cross-validation
    and model evaluation from the previous sections of this chapter. The grid search
    algorithm is as follows:'
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的优化机器学习模型调整参数的方法是通过穷举搜索的网格搜索。在规划以下基本的网格搜索算法时，请注意，这种策略将本章前几节中关于交叉验证和模型评估的材料结合起来。网格搜索算法如下：
- en: Choose the evaluation metric that you want to maximize (for example, AUC for
    classification, R² for regression).
  id: totrans-840
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您想要最大化的评估指标（例如，分类中的AUC，回归中的R²）。
- en: Choose which ML algorithm you want to use (for example, random forest).
  id: totrans-841
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您想要使用的机器学习算法（例如，随机森林）。
- en: Select which tuning parameters you want to optimize over (for example, number
    of trees and number of features per split) and the array of values to test for
    each parameter.
  id: totrans-842
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您想要优化的调整参数（例如，树的数量和每个分割的特征数量）以及每个参数要测试的值数组。
- en: Define the grid as the Cartesian product between the arrays of each tuning parameter.
    For example, if the arrays are [50, 100, 1000] for number of trees and [10, 15]
    for number of features per split, then the grid is [(50,10), (50,15), (100,10),
    (100,15), (1000,10), (1000,15)].
  id: totrans-843
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义网格为每个调整参数数组的笛卡尔积。例如，如果树的数量数组为[50, 100, 1000]，每个分割的特征数量数组为[10, 15]，则网格为[(50,10),
    (50,15), (100,10), (100,15), (1000,10), (1000,15)]。
- en: For each combination of tuning parameters in the grid, use the training set
    to perform cross-validation (using either the hold-out or k-fold-CV method) and
    compute the evaluation metric on the cross-validated predictions.
  id: totrans-844
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于网格中每个调整参数的组合，使用训练集进行交叉验证（使用保留法或k折交叉验证方法），并在交叉验证预测上计算评估指标。
- en: Finally, select the set of tuning parameters corresponding to the largest value
    of the evaluation metric. This is the optimized model.
  id: totrans-845
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，选择与评估指标最大值对应的调整参数集。这就是优化后的模型。
- en: Why does this work? Grid search does an extensive search over the possible combinations
    of values for each of the tuning parameters. For each combination, it estimates
    the performance of that model on new data by comparing the cross-validated predictions
    to the true target variable. Then, the model with the best estimated accuracy
    (for new data) is chosen. This model has the highest likelihood of performing
    the best when applied to new data.
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这有效？网格搜索在调整参数的每个可能值组合上进行广泛的搜索。对于每个组合，它通过比较交叉验证预测与真实目标变量来估计该模型在新数据上的性能。然后，选择具有最佳估计准确度（对于新数据）的模型。这个模型在应用于新数据时最有可能表现最佳。
- en: Let’s apply grid search to the Titanic Passengers dataset. You’ll use AUC as
    your optimization metric and SVM with a radial basis function (RBF) kernel as
    your classification algorithm. You can, in principle, also use grid search to
    select the best kernel. Indeed, you could use grid search to select between different
    algorithms!
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将网格搜索应用于泰坦尼克号乘客数据集。您将使用AUC作为优化指标，并使用具有径向基函数（RBF）核的SVM作为分类算法。原则上，您也可以使用网格搜索来选择最佳核。实际上，您可以使用网格搜索在不同的算法之间进行选择！
- en: 'Next, you select which tuning parameters to optimize over. For kernel SVM with
    an RBF kernel, you have two standard tuning parameters: the kernel coefficient,
    gamma; and the penalty parameter, C. The following listing shows how to run a
    grid search over those two parameters for this problem.'
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您选择要优化的调整参数。对于具有RBF核的核SVM，您有两个标准调整参数：核系数gamma和惩罚参数C。以下列表显示了如何运行针对这两个参数的网格搜索。
- en: Listing 4.7\. Grid search with kernel SVM
  id: totrans-849
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.7\. 基于核SVM的网格搜索
- en: '![](ch04ex07-0.jpg)'
  id: totrans-850
  prefs: []
  type: TYPE_IMG
  zh: '![](ch04ex07-0.jpg)'
- en: '![](ch04ex07-1.jpg)'
  id: totrans-851
  prefs: []
  type: TYPE_IMG
  zh: '![](ch04ex07-1.jpg)'
- en: 'You find with the Titanic dataset that the maximum cross-validated AUC is 0.670,
    and it occurs at the tuning parameter vector (gamma = 0.01, C = 6). A contour
    plot showing the AUC evaluated over the grid as in [figure 4.25](#ch04fig25) can
    be informative. A few factors jump out from this plot:'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 你在泰坦尼克号数据集中发现，最大交叉验证AUC为0.670，出现在调整参数向量(gamma = 0.01, C = 6)。一个等高线图显示了在[图4.25](#ch04fig25)中网格上评估的AUC，这可能是有用的。以下是一些从这个图中跳出的因素：
- en: The maximum occurs at the boundary of the grid (gamma = 0.01), meaning that
    you’d want to rerun the grid search on an expanded grid.
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大值出现在网格的边界(gamma = 0.01)，这意味着你想要在扩展的网格上重新运行网格搜索。
- en: A high amount of sensitivity exists in the accuracy of the predictions to the
    numerical value of the gamma parameter, meaning that you need to increase the
    granularity of sampling of that parameter.
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测的准确性对gamma参数数值的敏感性很高，这意味着你需要增加该参数采样的粒度。
- en: The maximum value occurs near gamma = 0, so expressing the grid on a log scale
    (for example, 10^(-4), 10^(-3), 10^(-2), 10^(-1)) is sensible.
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大值出现在gamma = 0附近，因此在对数尺度上表示网格（例如，10^(-4), 10^(-3), 10^(-2), 10^(-1)）是合理的。
- en: There’s not much sensitivity of the AUC as a function of C, so you can use a
    coarse sampling of that parameter.
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AUC作为C函数的敏感性不高，因此你可以使用该参数的粗略采样。
- en: Figure 4.25\. Contour plot showing the cross-validated AUC as a function of
    the two tuning parameters, gamma and C. The maximum occurs way off to the upper
    left, meaning that you need to expand the search and focus on that region.
  id: totrans-857
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.25。等高线图显示了交叉验证的AUC作为两个调整参数gamma和C的函数。最大值出现在左上角，这意味着你需要扩大搜索范围并关注该区域。
- en: '![](04fig25_alt.jpg)'
  id: totrans-858
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig25_alt.jpg)'
- en: 'Rerunning the grid search on a modified grid, you find that the maximum AUC
    is 0.690 and it occurs at (gamma = 0.08, C = 20). The value of optimizing over
    the tuning parameters is clear: a single model with arbitrary choice of tuning
    parameter could have attained a result as poor as AUC = 0.5 (no better than random
    guessing); the grid-search optimized model boosts the accuracy up to AUC = 0.69.'
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: 在修改后的网格上重新运行网格搜索，你发现最大AUC为0.690，出现在(gamma = 0.08, C = 20)。优化调整参数的价值是明显的：一个模型如果随意选择调整参数，其结果可能差到AUC
    = 0.5（不如随机猜测）；网格搜索优化的模型将准确度提升到AUC = 0.69。
- en: Note that grid search doesn’t absolutely ensure that you’ve chosen the best
    set of tuning parameters. Because of the limitations caused by choosing a finite
    grid of possible values to try, the actual best value might have landed somewhere
    between the values of the grid. Readers who have some familiarity with optimization
    might be wondering why more-sophisticated optimization routines aren’t traditionally
    used for tuning-parameter selection. The short answer is that the world of derivative-free,
    nonconvex optimization hasn’t yet become part of the standard ML toolkit. The
    longer answer is that ML researchers on the cutting edge of the field are beginning
    to incorporate these methods into tuning-parameter optimization strategies.
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，网格搜索并不绝对确保你选择了最佳的调整参数集。由于选择有限的可能值网格所造成的限制，实际的最佳值可能位于网格值之间。对优化有一定了解的读者可能会想知道为什么更复杂的优化程序传统上没有被用于调整参数的选择。简短的回答是，无导数、非凸优化的世界尚未成为标准机器学习工具箱的一部分。更长的回答是，该领域的机器学习研究人员正在开始将这些方法纳入调整参数优化策略中。
- en: 4.5\. Summary
  id: totrans-861
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5. 摘要
- en: 'In this chapter, you learned the basics of evaluating ML model performance.
    Here’s a quick rundown of the main takeaways:'
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了评估机器学习模型性能的基础知识。以下是主要收获的简要概述：
- en: When you evaluate models, you can’t double-dip the training data and use it
    for evaluation as well as training.
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你评估模型时，你不能双重使用训练数据，既用于评估也用于训练。
- en: Cross-validation is a more robust method of model evaluation.
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉验证是模型评估的一种更稳健的方法。
- en: Holdout cross-validation is the simplest form of cross-validation. A testing
    set is held out for prediction, in order to better estimate the model’s capability
    to be generalized.
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续交叉验证是交叉验证的最简单形式。为了更好地估计模型泛化能力，保留一个测试集用于预测。
- en: In k-fold cross-validation, k-folds are held out one at a time, providing more-confident
    estimates of model performance. This improvement comes at a higher computational
    cost. If available, the best estimate is obtained if k = number of samples, also
    known as leave-one-out cross-validation.
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在k折交叉验证中，每次保留一个k折，提供对模型性能的更自信的估计。这种改进是以更高的计算成本为代价的。如果可能，当k = 样本数时，可以获得最佳估计，也称为留一法交叉验证。
- en: 'The basic model-evaluation workflow is as follows:'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本模型评估工作流程如下：
- en: Acquire and preprocess the dataset for modeling ([chapter 2](kindle_split_012.html#ch02))
    and determine the appropriate ML method and algorithm ([chapter 3](kindle_split_013.html#ch03)).
  id: totrans-868
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取并预处理数据集以进行建模（[第2章](kindle_split_012.html#ch02)）并确定适当的机器学习方法（[第3章](kindle_split_013.html#ch03)）。
- en: Build models and make predictions by using either the holdout or k-fold cross-validation
    methods, depending on the computing resources available.
  id: totrans-869
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据可用的计算资源，使用留出法或k折交叉验证方法构建模型并进行预测。
- en: Evaluate the predictions with the performance metric of choice, depending on
    whether the ML method is classification or regression.
  id: totrans-870
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据机器学习方法是否为分类或回归，使用所选的性能指标评估预测。
- en: Tweak the data and model until the desired model performance is obtained. In
    [chapters 5](kindle_split_015.html#ch05)–[8](kindle_split_019.html#ch08), you’ll
    see various methods for increasing the model performance in common real-world
    scenarios.
  id: totrans-871
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整数据和模型，直到获得所需的模型性能。在[第5章](kindle_split_015.html#ch05)–[第8章](kindle_split_019.html#ch08)中，你会看到在常见的实际场景中提高模型性能的各种方法。
- en: For classification models, we introduced a few model-performance metrics to
    be used in step 3 of the workflow. These techniques include simple counting accuracy,
    the confusion matrix, receiver-operator characteristics, the ROC curve, and the
    area under the ROC curve.
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于分类模型，我们在工作流程的第3步中介绍了一些模型性能指标。这些技术包括简单的计数准确率、混淆矩阵、接收者操作特征、ROC曲线和ROC曲线下的面积。
- en: For regression models, we introduced the root-mean-square error and R-squared
    estimators. Simple visualizations, such as the prediction-versus-actual scatter
    plot and the residual plot, are useful.
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于回归模型，我们介绍了均方根误差和R平方估计量。简单的可视化，如预测值与实际值的散点图和残差图，很有用。
- en: You can use a grid-search algorithm to optimize a model with respect to tuning
    parameters.
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用网格搜索算法来优化模型，以调整参数。
- en: 4.6\. Terms from this chapter
  id: totrans-875
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6. 本章术语
- en: '| Word | Definition |'
  id: totrans-876
  prefs: []
  type: TYPE_TB
  zh: '| 单词 | 定义 |'
- en: '| --- | --- |'
  id: totrans-877
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| underfitting/overfitting | Using a model that’s too simple or too complex,
    respectively, for the problem at hand. |'
  id: totrans-878
  prefs: []
  type: TYPE_TB
  zh: '| 欠拟合/过拟合 | 分别使用过于简单或过于复杂的模型来处理问题。|'
- en: '| evaluation metric | A number that characterizes the performance of the model.
    |'
  id: totrans-879
  prefs: []
  type: TYPE_TB
  zh: '| 评估指标 | 描述模型性能的数字。|'
- en: '| mean squared error | A specific evaluation metric used in regression models.
    |'
  id: totrans-880
  prefs: []
  type: TYPE_TB
  zh: '| 均方误差 | 用于回归模型的一种特定评估指标。|'
- en: '| cross-validation | The method of splitting the training set into two or more
    training/testing sets in order to better assess the accuracy. |'
  id: totrans-881
  prefs: []
  type: TYPE_TB
  zh: '| 交叉验证 | 将训练集分成两个或多个训练/测试集的方法，以更好地评估准确性。|'
- en: '| holdout method | A form of cross-validation in which a single test set is
    held out of the model-fitting routine for testing purposes. |'
  id: totrans-882
  prefs: []
  type: TYPE_TB
  zh: '| 留出法 | 一种交叉验证形式，其中保留一个单独的测试集，用于模型拟合过程中的测试。|'
- en: '| k-fold cross-validation | A kind of cross-validation in which data is split
    into k random disjoint sets (folds). The folds are held out one at a time, and
    cross-validated on models built on the remainder of the data. |'
  id: totrans-883
  prefs: []
  type: TYPE_TB
  zh: '| k折交叉验证 | 一种交叉验证方法，其中数据被分成k个随机不相交的集合（折）。每次保留一个折，并在剩余数据上构建的模型上进行交叉验证。|'
- en: '| confusion matrix | A matrix showing for each class the number of predicted
    values that were correctly classified or not. |'
  id: totrans-884
  prefs: []
  type: TYPE_TB
  zh: '| 混淆矩阵 | 一个矩阵，显示每个类别的预测值中正确分类或未正确分类的数量。|'
- en: '| receiver operating characteristic (ROC) | A number representing true positives,
    false positives, true negatives, or false negatives. |'
  id: totrans-885
  prefs: []
  type: TYPE_TB
  zh: '| 接收者操作特征（ROC） | 表示真阳性、假阳性、真阴性或假阴性的数字。|'
- en: '| area under the ROC curve (AUC) | An evaluation metric for classification
    tasks defined from the area under the ROC curve of false positives versus true
    positives. |'
  id: totrans-886
  prefs: []
  type: TYPE_TB
  zh: '| 面积下限（AUC） | 一个用于分类任务的评估指标，定义为假阳性与真阳性之间的ROC曲线下的面积。|'
- en: '| tuning parameter | An internal parameter to a machine-learning algorithm,
    such as the bandwidth parameter for kernel-smoothing regression. |'
  id: totrans-887
  prefs: []
  type: TYPE_TB
  zh: '| 调整参数 | 机器学习算法的一个内部参数，例如核平滑回归中的带宽参数。|'
- en: '| grid search | A brute-force strategy for selecting the best values for the
    tuning parameters to optimize an ML model. |'
  id: totrans-888
  prefs: []
  type: TYPE_TB
  zh: '| 网格搜索 | 一种用于选择调整参数最佳值以优化ML模型的暴力策略。|'
- en: In the next chapter, you’ll start looking at improving your models by focusing
    on their features. In addition to basic feature-engineering techniques, you’ll
    learn advanced methods for extracting information out of text, images, and time-series
    data. You’ll also see how to select the best features to optimize the performance
    of the model and avoid overfitting.
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将开始关注通过关注其特征来改进您的模型。除了基本特征工程技术外，您还将学习从文本、图像和时间序列数据中提取信息的高级方法。您还将了解如何选择最佳特征以优化模型性能并避免过拟合。
- en: Chapter 5\. Basic feature engineering
  id: totrans-890
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5章\. 基本特征工程
- en: '*This chapter covers*'
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Understanding the importance of feature engineering for your machine-learning
    project
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解特征工程对您的机器学习项目的重要性
- en: Using basic feature-engineering processes, including processing dates and times
    and simple texts
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基本的特征工程过程，包括处理日期和时间以及简单的文本
- en: Selecting optimal features and reducing the statistical and computational complexity
    of the model
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择最佳特征并减少模型的统计和计算复杂性
- en: Using feature engineering at model-building and prediction time
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模型构建和预测时使用特征工程
- en: The first four chapters have shown you how to fit, evaluate, and optimize a
    supervised machine-learning algorithm, given a set of input features and a target
    of interest. But where do those input features come from? How do you go about
    defining and calculating features? And how do practitioners know whether they’re
    using the right set of features for their problem?
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: 前四章向您展示了如何根据一组输入特征和感兴趣的输出目标来拟合、评估和优化一个监督式机器学习算法。但是，那些输入特征从何而来？你是如何定义和计算特征的？以及从业者如何知道他们是否使用了适合他们问题的正确特征集？
- en: '5.1\. Motivation: why is feature engineering useful?'
  id: totrans-897
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 动机：为什么特征工程有用？
- en: In this chapter, we explore how to create features from raw input data—a process
    referred to as *feature engineering*—and walk through a few examples of simple
    feature-engineering processes. This will set the groundwork for the more sophisticated
    feature--engineering algorithms covered in [chapter 7](kindle_split_018.html#ch07).
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何从原始输入数据中创建特征——这个过程被称为*特征工程*——并介绍了一些简单的特征工程过程示例。这将为本章7.0节中介绍的更复杂的特征工程算法奠定基础。
- en: 5.1.1\. What is feature engineering?
  id: totrans-899
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1\. 什么是特征工程？
- en: 'Feature engineering is the practice of using mathematical transformations of
    raw input data to create new features to be used in an ML model. The following
    are examples of such transformations:'
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是一种使用原始输入数据的数学变换来创建用于ML模型的新特征的做法。以下是一些这样的变换示例：
- en: Dividing total dollar amount by total number of payments to get a ratio of dollars
    per payment
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将总金额除以总支付次数，以获得每笔支付的金额比率
- en: Counting the occurrence of a particular word across a text document
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算文本文档中特定单词的出现次数
- en: Computing statistical summaries (such as mean, median, standard deviation, and
    skew) of a distribution of user ping times to assess network health
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算用户ping时间分布的统计摘要（例如，均值、中位数、标准差和偏度）以评估网络健康
- en: Joining two tables (for example, payments and support) on user ID
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在用户ID上连接两个表（例如，支付和支持）
- en: Applying sophisticated signal-processing tools to an image and summarizing their
    output (for example, histogram of gradients)
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将复杂的信号处理工具应用于图像并总结其输出（例如，梯度直方图）
- en: 'Before diving into a few examples to demonstrate feature engineering in action,
    let’s consider a simple question: why use feature engineering?'
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨几个示例以展示特征工程的实际应用之前，让我们考虑一个简单的问题：为什么使用特征工程？
- en: 5.1.2\. Five reasons to use feature engineering
  id: totrans-907
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2\. 使用特征工程的理由
- en: This section describes a few ways that feature engineering provides value in
    a machine-learning application. This list isn’t exhaustive, but rather introduces
    a few of the primary ways that feature engineering can boost the accuracy and
    computational efficiency of your ML models.
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了特征工程在机器学习应用中提供价值的一些方法。这个列表并不全面，而是介绍了特征工程可以提升您的ML模型准确性和计算效率的几种主要方式。
- en: Transform original data to relate to the target
  id: totrans-909
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将原始数据转换为与目标相关
- en: You can use feature engineering to produce transformations of your original
    data that are more closely related to the target variable. Take, for instance,
    a personal finance dataset that contains the current bank account balance and
    credit debt of each customer. If you’re building a model to predict whether each
    customer will become delinquent in payments three months from now, then the engineered
    feature of
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用特征工程产生与目标变量更紧密相关的原始数据转换。以一个包含每位客户当前银行账户余额和信用债务的个人金融数据集为例。如果您正在构建一个模型来预测每位客户三个月后是否会成为支付违约者，那么工程化特征
- en: '[PRE10]'
  id: totrans-911
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: would likely be highly predictive of the target.
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: 可能对目标变量有很高的预测性。
- en: In this case, although the raw inputs are present in the original dataset, the
    ML model will have an easier time of finding the relationship between debt-to-balance
    ratio and future delinquency if the engineered feature is directly used as an
    input. This will result in improved accuracy of predictions.
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，尽管原始数据集中存在原始输入，但如果将工程化特征直接用作输入，机器学习模型将更容易找到债务与余额比率与未来违约之间的关系。这将导致预测准确性的提高。
- en: Bring in external data sources
  id: totrans-914
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 引入外部数据源
- en: Feature engineering enables practitioners to bring external data sources into
    their ML models. Imagine that you run an internet subscription service. The first
    time each customer logs in, you want to predict the lifetime value of that customer.
    Among a variety of metrics, you could capture the geographic location of each
    user. Although this data could be fed in directly as a categorical feature (for
    example, IP address or postal code), the model will likely have a difficult time
    determining the location-based signals that matter (in this case, those might
    be average income of each location, or urban versus rural).
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程使从业者能够将外部数据源引入他们的机器学习模型中。想象一下，你运营一个互联网订阅服务。每位客户首次登录时，你想要预测该客户的终身价值。在众多指标中，你可以捕捉到每个用户的地理位置。尽管这些数据可以直接作为分类特征（例如，IP地址或邮政编码）输入，但模型可能很难确定基于位置的信号（在这种情况下，这些可能是每个位置的平均收入，或城市与农村的差异）。
- en: You can do better by bringing in third-party demographic data. For example,
    this would allow you to compute the average income and population density of each
    user’s location and to insert those factors directly into the training set. Now,
    instead of relying on the model to infer such subtle relationships from the raw
    location data, those predictive factors immediately become easier to deduce. Further,
    the feature engineering of location into income and population density enables
    you to assess which of these derivatives of location matter the most.
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入第三方人口统计数据，您可以做得更好。例如，这将允许您计算每个用户位置的平均收入和人口密度，并将这些因素直接插入到训练集中。现在，您不再需要模型从原始位置数据中推断这些微妙的关系，这些预测因素立即变得更容易推断。此外，将位置特征工程化为收入和人口密度使您能够评估哪些位置衍生因素最重要。
- en: Use unstructured data sources
  id: totrans-917
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用非结构化数据源
- en: Feature engineering enables you to use unstructured data sources in ML models.
    Many data sources aren’t inherently structured into feature vectors that can be
    directly inserted into the ML framework presented in the first four chapters.
    Unstructured data such as text, time series, images, video, log data, and clickstreams
    account for the vast majority of data that’s created. Feature engineering is what
    enables ML practitioners to produce ML feature vectors out of these kinds of raw
    data streams.
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程使您能够在机器学习模型中使用非结构化数据源。许多数据源并非天生就是特征向量，可以直接插入到第一至四章中介绍的机器学习框架中。非结构化数据，如文本、时间序列、图像、视频、日志数据和点击流，占到了创建的数据的绝大多数。特征工程正是使机器学习从业者能够从这些原始数据流中产生机器学习特征向量。
- en: This chapter touches on some rather simple examples of feature engineering on
    text data. Subsequent chapters introduce the most commonly used types of feature
    engineering for text, images, and time-series data.
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涉及一些关于文本数据特征工程相对简单的例子。随后的章节将介绍文本、图像和时间序列数据中最常用的特征工程类型。
- en: Create features that are more easily interpreted
  id: totrans-920
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 创建更易于解释的特征
- en: Feature engineering empowers ML practitioners to create features that are more
    interpretable and actionable. Often, using ML to find patterns in data can be
    useful for making accurate predictions, but you may face limitations in the interpretability
    of the model and the ultimate utility of the model to drive changes. In these
    cases, it may be more valuable to engineer new features that are more indicative
    of the processes that drive the data generation and the link between the raw data
    and the target variable.
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程赋予机器学习从业者创建更可解释和可操作特征的能力。通常，使用机器学习在数据中寻找模式对于做出准确预测是有用的，但你可能会面临模型可解释性和模型最终驱动变化的实际效用方面的限制。在这些情况下，可能更有价值的是设计新的特征，这些特征更能表明驱动数据生成的过程以及原始数据与目标变量之间的联系。
- en: Consider a simple example of machines that manufacture computer hardware. You
    could use the raw machine data, such as measurement of signal response and other
    processing signals, to build ML models to predict part failure. But features such
    as time since the last machine tune-up and volume of hardware produced can provide
    insight into the changeable aspects of the manufacturing process.
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个简单的例子，即制造计算机硬件的机器。你可以使用原始机器数据，例如信号响应的测量和其他处理信号，来构建机器学习模型以预测部件故障。但诸如上次机器调校时间以及生产的硬件数量等特征可以提供对制造过程可变方面的洞察。
- en: Enhance creativity by using large sets of features
  id: totrans-923
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通过使用大量特征来增强创造力
- en: Feature engineering empowers you to throw in large sets of features to see what
    sticks. You can create as many features as you can dream up and see which of them
    carries predictive power when thrown in to train a model. This allows ML practitioners
    to escape from a rigid mindset when creating and testing features and could result
    in newly discovered trends and patterns.
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程使你能够尝试大量特征以查看哪些能够粘附。你可以创造出尽可能多的特征，并查看在训练模型时哪些特征具有预测力。这允许机器学习从业者从创建和测试特征时的僵化思维中解脱出来，并可能导致新发现的趋势和模式。
- en: Although overfitting becomes a concern when dozens or hundreds of features are
    used to train an ML model, rigorous feature-selection algorithms can be used to
    pare down the set of features to something more manageable. (For example, you
    can automatically determine that your predictions with the top 10 features are
    as good as or better than your predictions with all 1,000 features.) We describe
    these algorithms later this chapter, in [section 5.3](#ch05lev1sec3).
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然当使用数十或数百个特征来训练机器学习模型时，过拟合成为一个关注点，但严格的特征选择算法可以被用来减少特征集，使其更易于管理。（例如，你可以自动确定使用前10个特征的预测与使用所有1,000个特征的预测一样好或更好。）我们将在本章后面描述这些算法，在[5.3节](#ch05lev1sec3)中。
- en: 5.1.3\. Feature engineering and domain expertise
  id: totrans-926
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3\. 特征工程和领域专业知识
- en: 'Another way to conceptualize feature engineering is as a mechanism that imbues
    domain expertise into a machine-learning model. What we mean by this is simple:
    for each problem at hand, knowledge about the data and systems under study is
    accumulated over time. For some problems, these patterns will be straightforward
    enough to be easily learned by an ML model. But for more-challenging problems,
    the ML models stand to improve significantly from the codification of that domain
    expertise into the feature set. The following are examples of statements of domain
    expertise that could easily be coded into ML features:'
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种理解特征工程的方式是将领域专业知识注入机器学习模型中。我们所说的简单来说就是：对于手头的每个问题，关于研究的数据和系统的知识会随着时间的推移而积累。对于某些问题，这些模式将足够简单，以至于机器学习模型可以轻松地学习。但对于更具挑战性的问题，机器学习模型将从将领域专业知识编码到特征集中而显著改进。以下是将容易编码到机器学习特征中的领域专业知识声明的例子：
- en: Web conversions are always higher on Tuesday (include the Boolean feature “Is
    it Tuesday?”).
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络转化率在星期二总是更高（包括布尔特征“是否是星期二？”）。
- en: Household power consumption increases with higher temperature (include temperature
    as a feature).
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 家庭电力消耗随着温度的升高而增加（包括温度作为特征）。
- en: Spam emails typically come from free email accounts (engineer the Boolean feature
    “Is from free email account?” or email domain).
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 垃圾邮件通常来自免费电子邮件账户（设计布尔特征“是否来自免费电子邮件账户？”或电子邮件域名）。
- en: Loan applicants with recently opened credit cards default more often (use the
    feature “Days since last credit card opened”).
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近开设信用卡的贷款申请人更频繁地违约（使用“上次开设信用卡天数”特征）。
- en: Customers often switch their cell-phone provider after others in their network
    also switch providers (engineer a feature that counts the number of people in
    a subscriber’s network who recently switched).
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户通常在他们网络中的其他人也更换了服务提供商之后才会更换自己的手机服务提供商（设计一个功能来统计最近更换了服务提供商的订阅者网络中的人数）。
- en: Clearly, the list of potential domain expertise tidbits could go on and on.
    Indeed, the standard operating procedure for many companies is to use long lists
    of these ad hoc rules to make decisions and predictions. These business rules
    are a perfect set of engineered features on which to start building ML models!
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，潜在领域专业知识片段的列表可以无限延续。实际上，许多公司的标准操作程序是使用这些临时规则的长列表来做出决策和预测。这些业务规则是构建机器学习模型的一个完美的工程特征集！
- en: Turned on its head, feature engineering can be a way to *test* the preconceived
    notions that are held by domain experts. If there’s any question about whether
    a particular hypothesis holds any merit, it can be codified and used as a feature
    in an ML model. Then, the accuracy of the model can be tested with and without
    that feature to assess the conditional importance of the feature in predicting
    the target variable. If the gains in accuracy are negligible, this is evidence
    of the lack of added value of that idea.
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来说，特征工程可以是一种检验领域专家持有的先入为主的观念的方法。如果有任何疑问，关于某个特定假设是否具有任何价值，它可以被编码并用作机器模型中的一个特征。然后，可以通过有无该特征来测试模型的准确性，以评估该特征在预测目标变量中的条件重要性。如果准确性的提升可以忽略不计，这表明该想法的附加值不足。
- en: Next, we present a few examples of simple feature engineering to show how these
    processes work in practice. We describe how feature engineering fits into the
    overall ML workflow and demonstrate how the predictive accuracy of ML models can
    be improved by employing some relatively straightforward feature-engineering processes.
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将展示一些简单的特征工程示例，以展示这些过程在实际中是如何工作的。我们描述了特征工程如何融入整体机器学习工作流程，并展示了通过采用一些相对简单的特征工程过程如何提高机器学习模型的预测准确性。
- en: 5.2\. Basic feature-engineering processes
  id: totrans-936
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 基本特征工程过程
- en: Before diving into our example, let’s revisit our basic ML workflow to show
    how feature engineering extends what you’ve seen so far. [Figure 5.1](#ch05fig01)
    illustrates the workflow.
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入我们的例子之前，让我们回顾一下我们基本的机器学习工作流程，以展示特征工程是如何扩展你迄今为止所看到的。![图5.1](#ch05fig01)展示了工作流程。
- en: Figure 5.1\. How feature engineering fits into the basic ML workflow. You extend
    the training data with features before building the model. When making predictions,
    you need to push new data through the same feature-engineering pipeline to ensure
    that the answers make sense.
  id: totrans-938
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.1\. 特征工程如何融入基本的机器学习工作流程。你在构建模型之前扩展训练数据中的特征。在做出预测时，你需要将新数据通过相同的特征工程管道，以确保答案是有意义的。
- en: '![](05fig01.jpg)'
  id: totrans-939
  prefs: []
  type: TYPE_IMG
  zh: '![](05fig01.jpg)'
- en: The feature-engineering extension of the workflow allows you to expand on the
    training data to increase the accuracy of the ML algorithm. To ensure that feature
    engineering is used properly, you need to run the prediction data through the
    same feature-engineering pipeline that was applied to the training data. This
    ensures that predictions are generated by using exactly the same process as applied
    to the training data.
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程的特征工程扩展允许你扩展训练数据以增加机器学习算法的准确性。为了确保特征工程得到正确使用，你需要将预测数据通过应用于训练数据的相同特征工程管道。这确保了预测是通过与应用于训练数据完全相同的过程生成的。
- en: '5.2.1\. Example: event recommendation'
  id: totrans-941
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 示例：活动推荐
- en: 'To illustrate feature-engineering concepts, this section introduces an example
    from the real world: a challenge from the data science competition site Kaggle
    ([www.kaggle.com](http://www.kaggle.com)).'
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明特征工程的概念，本节引入了一个来自现实世界的例子：来自数据科学竞赛网站Kaggle（[www.kaggle.com](http://www.kaggle.com)）的挑战。
- en: Imagine that you’re running an event-recommendation site and want to predict
    whether an event (such as a meeting, a happy hour, or a lecture) is interesting
    to a particular user. You have a set of training data describing which users have
    shown interest in which events in the past, and some information about the users
    and the events themselves. Your goal is to build an ML model to predict whether
    a particular event is interesting to a user—a binary classification model.
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，您正在运行一个事件推荐网站，并希望预测一个特定用户是否对某个事件（如会议、Happy Hour或讲座）感兴趣。您有一组训练数据，描述了过去哪些用户对哪些事件表示了兴趣，以及一些关于用户和事件本身的信息。您的目标是构建一个机器学习模型来预测特定事件是否对用户感兴趣——一个二分类模型。
- en: The data and information about the challenge are available at [www.kaggle.com/c/event-recommendation-engine-challenge](http://www.kaggle.com/c/event-recommendation-engine-challenge)
    after you sign up on the Kaggle website (if you haven’t already). The base datasets
    are the train.csv, events.csv, and users.csv files, which can be joined together
    on user and event identifiers. You limit the dataset to the events that have an
    explicit interested or not-interested selection and to the basic numerical and
    categorical features. [Figure 5.2](#ch05fig02) shows a selection of this initial
    training dataset.
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: 在您在Kaggle网站上注册后（如果您还没有注册），可以在[www.kaggle.com/c/event-recommendation-engine-challenge](http://www.kaggle.com/c/event-recommendation-engine-challenge)找到关于挑战的数据和信息。基础数据集包括train.csv、events.csv和users.csv文件，这些文件可以根据用户和事件标识符进行合并。您将数据集限制在具有明确感兴趣或未感兴趣选择的活动中，以及基本的数值和分类特征。![图5.2](#ch05fig02)显示了初始训练数据集的一个选择。
- en: Figure 5.2\. A sample of the datasets used for training the event-recommendations
    model
  id: totrans-945
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.2. 用于训练事件推荐模型的数据集样本
- en: '![](05fig02_alt.jpg)'
  id: totrans-946
  prefs: []
  type: TYPE_IMG
  zh: '![05fig02_alt.jpg]'
- en: 'Your sample training data contains the following features:'
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: 您的样本训练数据包含以下特征：
- en: '`invited`—A Boolean indicating whether the individual was invited to the event'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`invited`——一个布尔值，表示个人是否被邀请参加活动'
- en: '`birthyear`—The year the person was born'
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`birthyear`——人员的出生年份'
- en: '`gender`—The gender of the person'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gender`——人员的性别'
- en: '`timezone`—The time zone of the current location of the individual'
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timezone`——个人的当前位置时区'
- en: '`lat/lng`—The latitude/longitude of the event'
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lat/lng`——事件的纬度/经度'
- en: To start, you’ll build and evaluate a model based on only these six features.
    It’s clear that this dataset is limited in terms of the patterns that can identify
    whether each user will be interested in the event. As you continue this section,
    you’ll use a few straightforward feature-engineering transformations to extend
    the feature set and layer on new information.
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您将基于这六个特征构建和评估一个模型。很明显，这个数据集在识别每个用户是否对事件感兴趣的模式方面是有限的。随着您继续本节，您将使用一些简单的特征工程转换来扩展特征集并添加新信息。
- en: 'You’ll build an initial binary classification model to predict the target variable,
    `interested`, from the six input features. Following the ML workflow from [chapters
    1](kindle_split_011.html#ch01)–[4](kindle_split_014.html#ch04), you do the following:'
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: 您将构建一个初始的二分类模型，从六个输入特征中预测目标变量`interested`。遵循第1章到第4章的ML工作流程[chapters 1](kindle_split_011.html#ch01)–[4](kindle_split_014.html#ch04)，您需要执行以下操作：
- en: Perform initial data-processing exercises (convert categorical columns to numerical,
    impute missing values).
  id: totrans-955
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行初始数据处理练习（将分类列转换为数值，填充缺失值）。
- en: Do the model training (using the random forest algorithm).
  id: totrans-956
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行模型训练（使用随机森林算法）。
- en: Evaluate the model (using 10-fold cross-validation and ROC curves). [Figure
    5.3](#ch05fig03) shows the cross-validated ROC curve. It attains an AUC score
    of 0.81.
  id: totrans-957
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型（使用10折交叉验证和ROC曲线）。[图5.3](#ch05fig03)显示了交叉验证的ROC曲线。它达到了0.81的AUC分数。
- en: Figure 5.3\. Cross-validated ROC curve and AUC metric for the simple event-recommendation
    model
  id: totrans-958
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.3. 简单事件推荐模型的交叉验证ROC曲线和AUC指标
- en: '![](05fig03_alt.jpg)'
  id: totrans-959
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![05fig03_alt.jpg]'
- en: 5.2.2\. Handling date and time features
  id: totrans-960
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2. 处理日期和时间特征
- en: Next, you’ll use feature engineering to try to improve the results of your first
    model. In addition to the data shown in [figure 5.2](#ch05fig02), each event in
    the dataset has an associated `start_time`. This data element is an ISO-8601 UTC
    string representing when the event is scheduled to begin. The data field has formatting
    like `2012-10-02 15:53:05.754000+00:00`, representing yyyy-mm-dd hh:mm:ss.mmmmmm_HH:MM.
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将使用特征工程尝试改进第一个模型的结果。除了图5.2中显示的数据外，数据集中的每个事件都有一个相关的`start_time`。这个数据元素是一个ISO-8601
    UTC字符串，表示事件计划开始的时间。数据字段具有类似于`2012-10-02 15:53:05.754000+00:00`的格式，表示yyyy-mm-dd
    hh:mm:ss.mmmmmm_HH:MM。
- en: 'The types of ML models described in [chapter 3](kindle_split_013.html#ch03)
    can support numerical or categorical input features, of which a `datetime` string
    is neither. Therefore, you can’t simply insert the column of strings directly
    into the model. What you can do, however, is perform transformations of the `datetime`
    elements into numerical features that capture the information encoded within the
    `datetime` string. This simple yet powerful concept of feature engineering can
    enable you to transform each `datetime` string into a smattering of features,
    such as these:'
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](kindle_split_013.html#ch03)中描述的ML模型可以支持数值或分类输入特征，其中`datetime`字符串不属于这两种。因此，你不能简单地将字符串列直接插入模型中。然而，你可以执行将`datetime`元素转换为数值特征的转换，这些特征可以捕获`datetime`字符串中编码的信息。这个简单而强大的特征工程概念可以使你将每个`datetime`字符串转换为一系列特征，例如这些：
- en: Hour of the day
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一天中的小时
- en: Day of the week
  id: totrans-964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 星期
- en: Month of the year
  id: totrans-965
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年份的月份
- en: Minute of the hour
  id: totrans-966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小时
- en: Quarter of the year
  id: totrans-967
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年份的季度
- en: '[Figure 5.4](#ch05fig04) shows the first five rows of data that result from
    converting your single `start_time` feature into 10 `datetime` features.'
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.4](#ch05fig04)显示了将你的单个`start_time`特征转换为10个`datetime`特征后得到的前五行数据。'
- en: Figure 5.4\. Additional date-time columns extracted from the timestamp column
    for the event-recommendation dataset
  id: totrans-969
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.4\. 从事件推荐数据集的时间戳列中提取的附加日期时间列
- en: '![](05fig04_alt.jpg)'
  id: totrans-970
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig04_alt.jpg)'
- en: Next, you build a random forest model on this new, 16-feature dataset. Our cross-validated
    ROC curve is shown in [figure 5.5](#ch05fig05).
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将在这个新的16特征数据集上构建随机森林模型。我们的交叉验证ROC曲线如图5.5所示。
- en: Figure 5.5\. Cross-validated ROC curve for model including date-time features
  id: totrans-972
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.5\. 包含日期时间特征的模型的交叉验证ROC曲线
- en: '![](05fig05_alt.jpg)'
  id: totrans-973
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig05_alt.jpg)'
- en: The AUC of the model has increased from 0.81 to 0.85\. Clearly, there was hidden
    value in the `start_time` information that, when imbued into the ML model via
    feature engineering, helped improve the model’s accuracy. Most likely, events
    that occur on particular days of the week and at certain times of day are more
    popular than others.
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的AUC从0.81增加到0.85。显然，`start_time`信息中隐藏的价值，当通过特征工程注入到ML模型中时，有助于提高模型的准确性。最有可能的是，在特定星期几和一天中的特定时间发生的事件比其他事件更受欢迎。
- en: 5.2.3\. Working with simple text features
  id: totrans-975
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. 处理简单文本特征
- en: 'In addition to the time of the event, the data includes basic text features
    from simple, natural language–processing routines. In the same way that `datetime`
    features can’t be used directly by the model because they’re neither numerical
    nor categorical, arbitrary text can’t be fed into the ML algorithm without some
    kind of processing that turns the data into one of the two accepted types. To
    turn text into ML features, you employ a method called *bag of words*. The idea
    is simple in principle: count the number of occurrences of each word that appears
    in the text and insert a column in the dataset with the counts for that word.
    As always, though, you’ll have a few complicating factors to deal with.'
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: 除了事件的时间外，数据还包括来自简单自然语言处理程序的文本特征。与`datetime`特征不能直接由模型使用一样，因为它们既不是数值也不是分类的，任意文本不能在没有某种将数据转换为两种接受类型之一的处理的情况下输入到ML算法中。要将文本转换为ML特征，你将使用一种称为“词袋”的方法。在原则上，这个想法很简单：计算文本中每个单词出现的次数，并在数据集中插入一个包含该单词计数的列。然而，你仍将有一些复杂因素需要处理。
- en: 'The features that you feed to your ML algorithm must be homogeneous: there
    must be the same number of features, and they must correspond to the same underlying
    concept, for all of the instances in your dataset. For example, if the first instance
    contains five occurrences of the word *family*, and the next instance doesn’t,
    you must choose to either include a column for Family and set the count to 0 for
    the second instance, or leave it off both instances. Usually, you work with the
    entire text corpus of the dataset to decide which words get a column and which
    don’t. In most cases, you build the bag of words for the entire dataset and include
    only the top-occurring words to get a column in your dataset. You can then have
    a catchall column for the rest of the words, which in principle determines the
    length of the text outside the selected top words.'
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: 您提供给机器学习算法的特征必须是一致的：对于数据集中的所有实例，特征的数量必须相同，并且它们必须对应于相同的基本概念。例如，如果第一个实例包含五个“家庭”这个词的出现，而下一个实例没有，您必须选择在第二个实例中包含一个“家庭”列并将计数设置为0，或者在不包含这两个实例的情况下。通常，您会使用整个数据集的文本语料库来决定哪些单词应该有列，哪些不应该。在大多数情况下，您会为整个数据集构建词袋，并只包含出现频率最高的单词以在数据集中创建列。然后，您可以有一个包含所有剩余单词的通用列，这在原则上决定了所选顶级单词之外文本的长度。
- en: Now, let’s say you’re selecting the top 100 words to get a Counts column in
    your dataset. You’ll get a bunch of columns with counts for common but not useful
    words, such as *is*, *and*, and *the*. In the field of natural language processing,
    these words are known as *stop words* and are usually purged from the text before
    performing the bag-of-words counting.
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设您正在选择前100个单词以在数据集中创建一个计数列。您将得到一些列，这些列包含常见但无用的单词的计数，例如“是”、“和”和“the”。在自然语言处理领域，这些单词被称为“停用词”，通常在执行词袋计数之前从文本中删除。
- en: We introduce more-advanced text feature concepts in the next chapter, but the
    last complicating factor to mention here is that the bag-of-words dataset quickly
    becomes large and sparse. We have a lot of features mostly filled with zeros,
    because a particular word usually isn’t likely to appear in a random passage of
    text. The English dictionary is large (with more than 200 thousand words in use),
    and only a small fraction of those words are used in most texts. Some ML problems
    have a much narrower space, in which a class of words is more represented than
    in general. For example, [figure 5.6](#ch05fig06) shows a few instances of the
    count features for the top words in our event-recommendation example; the sparsity
    of the data is clear. Some ML algorithms, such as naïve Bayes classifiers, handle
    sparse data well (by requiring no extra memory for the 0’s), whereas most others
    don’t.
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章介绍更高级的文本特征概念，但在此要提到的最后一个复杂因素是，词袋数据集很快就会变得很大且稀疏。我们有很多特征，大部分都是0，因为特定的单词通常不太可能出现在随机文本段落中。英语词典很大（有超过20万个在使用的单词），但其中只有一小部分单词在大多数文本中使用。一些机器学习问题有一个更窄的空间，其中一类单词比一般情况更常见。例如，[图5.6](#ch05fig06)显示了我们事件推荐示例中顶级单词的计数特征的一些实例；数据稀疏性很明显。一些机器学习算法，如朴素贝叶斯分类器，可以很好地处理稀疏数据（不需要为0分配额外内存），而大多数其他算法则不行。
- en: Figure 5.6\. A slice of the bag-of-words data for the event-recommendation example.
    These numbers are the counts of the top-occurring words in the event descriptions.
    A large fraction of the cells contain 0, so we call the dataset *sparse*.
  id: totrans-980
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.6. 事件推荐示例的词袋数据的一个切片。这些数字是事件描述中顶级出现单词的计数。大部分单元格包含0，因此我们称该数据集为“稀疏”。
- en: '![](05fig06_alt.jpg)'
  id: totrans-981
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig06_alt.jpg)'
- en: In events.csv of our event-recommendation example, 100 features represent the
    bag of words for the 100 top-occurring words. You want to use these as features
    in the model, because particular events might be more popular than others. [Figure
    5.7](#ch05fig07) shows the resulting ROC curve after adding these features to
    the model.
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的事件推荐示例的events.csv中，100个特征代表100个顶级出现单词的词袋。您希望将这些用作模型中的特征，因为某些事件可能比其他事件更受欢迎。[图5.7](#ch05fig07)显示了将这些特征添加到模型后得到的ROC曲线。
- en: Figure 5.7\. Cross-validated ROC curve for full model including date-time and
    text features
  id: totrans-983
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.7. 包含日期时间特征和文本特征的完整模型的交叉验证ROC曲线
- en: '![](05fig07_alt.jpg)'
  id: totrans-984
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig07_alt.jpg)'
- en: The AUC metric in [figure 5.7](#ch05fig07) doesn’t increase from your previous
    model that included only basic and date-time features. This tells you that a particular
    event description isn’t more likely to be interesting for users just because of
    the text. This model doesn’t address the interests of individual users, but the
    user base in general. In a real recommendation engine, you could build a model
    for each user or each class of user. Other popular methods for recommendation
    engines use connections between events, users, and user friends to find recommendations.
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.7](#ch05fig07) 中的AUC指标与只包含基本和日期时间特征的先前模型相比没有增加。这告诉你，仅仅因为文本，特定的事件描述更有可能对用户感兴趣。这个模型没有解决个别用户的兴趣，而是针对一般用户群体。在真实的推荐引擎中，你可以为每个用户或每个用户类别构建一个模型。其他流行的推荐引擎方法使用事件、用户和用户朋友之间的联系来寻找推荐。'
- en: 5.3\. Feature selection
  id: totrans-986
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 特征选择
- en: Compared to basic statistical methods and human pattern-recognition abilities,
    one of the main advantages of machine-learning algorithms is the ability to handle
    a larger number of features. Most ML algorithms can handle thousands or millions
    of features. It’s often a useful strategy to add more features in order to increase
    the accuracy of the model. But in machine learning, as in many other cases, more
    isn’t always better.
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: 与基本的统计方法和人类模式识别能力相比，机器学习算法的主要优势之一是处理更多特征的能力。大多数机器学习算法可以处理数千或数百万个特征。增加更多特征以提高模型准确性的策略通常是有用的。但在机器学习中，就像在许多其他情况下一样，更多并不总是更好。
- en: Because more features enable the model to learn the mapping from features to
    the target in more detail, there’s a risk that the model is overfitting the data.
    This increases the appeared accuracy of the model at training time, but might
    hurt the performance of predictions on new, unseen data. [Figure 5.8](#ch05fig08)
    shows an example of overfitting (we first discussed overfitting in [chapter 3](kindle_split_013.html#ch03)).
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: 由于更多特征使模型能够更详细地学习特征到目标的映射，存在模型过度拟合数据的危险。这增加了模型在训练时的出现准确率，但可能会损害对新、未见数据的预测性能。[图5.8](#ch05fig08)
    展示了过度拟合的一个例子（我们首次在[第3章](kindle_split_013.html#ch03)中讨论了过度拟合）。
- en: Figure 5.8\. The decision boundary of two models fit on the same training data.
    In the model on the top, the decision boundary is too detailed, and the classification
    performance on new data can be affected.
  id: totrans-989
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.8\. 两个模型在相同训练数据上的决策边界。在顶部的模型中，决策边界过于详细，可能会影响新数据的分类性能。
- en: '![](05fig08_alt.jpg)'
  id: totrans-990
  prefs: []
  type: TYPE_IMG
  zh: '![图片描述](05fig08_alt.jpg)'
- en: In this section, you’ll look at methods for selecting a subset of features in
    order to avoid overfitting, and thus increase the accuracy of the model when applied
    to new data. Some algorithms are more or less susceptible to overfitting, but
    it might be worth the effort to perform some of these optimizations if model accuracy
    is of particular importance.
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将了解选择特征子集的方法，以避免过度拟合，从而在应用于新数据时提高模型的准确性。某些算法或多或少容易受到过度拟合的影响，但如果模型准确性特别重要，那么进行一些这些优化可能值得。
- en: Another advantage of a smaller number of features, and thus smaller models,
    is that the computational cost of training and prediction is usually related to
    the number of features. By spending some time in the model-development phase,
    you can save time when retraining or when making predictions.
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
  zh: 较少的特征数量和因此较小的模型的一个优点是，训练和预测的计算成本通常与特征数量相关。在模型开发阶段花些时间，可以在重新训练或进行预测时节省时间。
- en: Finally, feature selection and the related concept of *feature importance* can
    help you gain insight into the model and therefore the data used to build the
    model. In some cases, the goal of building the model might not even be to make
    predictions, but to get a view into important features of the model; you can use
    knowledge about the most significant features to discover certain patterns such
    as credit status being correlated with certain demographic or social factors.
    A cost could be associated with obtaining the data for specific features, and
    there’s no need to suffer loss if the feature is unimportant for the model at
    hand. The importance of particular features can also reveal valuable insights
    into the predictions returned by the model. In many real-world use cases, it’s
    important to understand something about *why* a certain prediction was made, and
    not just the particular answer.
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，特征选择和相关概念*特征重要性*可以帮助你深入了解模型，因此可以了解用于构建模型的原始数据。在某些情况下，构建模型的目标甚至可能不是进行预测，而是了解模型的重要特征；你可以使用关于最显著特征的知识来发现某些模式，例如信用状况与某些人口或社会因素相关。获取特定特征的数据可能存在成本，如果该特征对当前模型不重要，则无需承受损失。特定特征的重要性还可以揭示模型返回的预测中的宝贵见解。在许多现实世界的用例中，了解为什么做出某个特定预测很重要，而不仅仅是特定的答案。
- en: With that, you should be well motivated to look more deeply into feature selection
    and the most common methods used. The simplest way to select the optimal subset
    of features is to try all combinations of features—for example, building a model
    for all subsets of features and using your knowledge from [chapter 4](kindle_split_014.html#ch04)
    to measure the performance of the model. Unfortunately, even with a small number
    of features, this approach quickly becomes infeasible. You have to use techniques
    that can approximate the optimal subset of features. In the next few subsections,
    you’ll investigate some of these methods. One of the most widely used classes
    of methods is forward selection/backward elimination, covered in the next subsection.
    Other heuristic methods are covered later in the chapter.
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，你应该有足够的动力深入了解特征选择和最常用的方法。选择最优特征子集的最简单方法就是尝试所有特征组合——例如，为所有特征子集构建模型，并使用你在[第4章](kindle_split_014.html#ch04)中获得的知识来衡量模型的性能。不幸的是，即使特征数量很少，这种方法也会迅速变得不可行。你必须使用可以近似最优特征子集的技术。在接下来的几个小节中，你将研究这些方法中的一些。最广泛使用的方法类别之一是前向选择/后向消除，将在下一小节中介绍。其他启发式方法将在本章后面介绍。
- en: '|  |'
  id: totrans-995
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Some algorithms have built-in feature selection**'
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
  zh: '**一些算法具有内置的特征选择功能**'
- en: Although the methods discussed in this section are applicable to any machine-learning
    algorithm, some algorithms have advantages in the realm of feature selection because
    they have similar behavior built in. In all cases, however, these built-in methods
    are unlikely to yield results comparable to the general methods, but might be
    significantly more efficient computationally. As a consequence, it might be useful
    to try the built-in methods before falling back on the more computationally intense
    general methods, or even use the built-in methods as a seed to save computation
    time on the general methods.
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本节讨论的方法适用于任何机器学习算法，但某些算法在特征选择领域具有优势，因为它们具有内置的类似行为。然而，在所有情况下，这些内置方法不太可能产生与通用方法相当的结果，但可能在计算上显著更高效。因此，在回退到更计算密集的通用方法之前，尝试内置方法可能是有用的，或者甚至可以将内置方法用作种子来节省通用方法上的计算时间。
- en: Examples of built-in feature-selection methods are the weights assigned to features
    in linear and logistic regression algorithms and the feature importances in decision
    trees and ensemble variants such as random forests, which capture (in a computationally
    efficient manner) the amount that predictive accuracy is expected to decrease
    if a feature were replaced with random noise. We can inspect the top feature importances
    of the random forest event-recommendation model from the previous section.
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: 内置特征选择方法的例子包括线性回归和逻辑回归算法中分配给特征的权重，以及决策树和随机森林等集成变体中的特征重要性，这些方法以计算效率高的方式捕捉到如果用随机噪声替换一个特征，预测准确性预期会降低的量。我们可以从上一节中检查随机森林事件推荐模型的顶级特征重要性。
- en: '![](119fig01.jpg)'
  id: totrans-999
  prefs: []
  type: TYPE_IMG
  zh: '![图片](119fig01.jpg)'
- en: The random forest feature importances for the top seven features in the eventrecommendation
    model. By this measure, the birth year of a user and the time zone of the event
    are the two most important indicators of whether an event will be of interest
    to a user.
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: 事件推荐模型中前七个特征的随机森林特征重要性。根据这一衡量标准，用户的出生年份和事件所在的时区是判断事件是否对用户感兴趣的两个最重要的指标。
- en: '|  |'
  id: totrans-1001
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 5.3.1\. Forward selection and backward elimination
  id: totrans-1002
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1. 前向选择和后向消除
- en: One of the most widely used sets of methods for approximating the best subset
    of features is the iterative selection methods that you’ll look into here. The
    general concept is to start from no features and iteratively find the best features
    to add, or start from all features and iteratively remove the worst. The search
    is stopped when all features have been added or removed, when the increase in
    accuracy levels off, or at a predetermined size of the feature set.
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: 用于近似最佳特征子集的最广泛使用的方法集是迭代选择方法，你将在下面探讨。一般概念是从没有特征开始，迭代地找到要添加的最佳特征，或者从所有特征开始，迭代地移除最差的。当所有特征都已添加或移除，当准确性的提高达到平稳，或者达到预定的特征集大小时，搜索停止。
- en: These methods are referred to as *forward selection* and *backward elimination*,
    respectively. They don’t guarantee finding the best subset of features, which
    is why we call it an *approximation*. One of the features that was left out, or
    removed, might have more predictive power when paired with a particular subset
    of features that hasn’t yet been reached when the feature is removed. Remember
    that the power of machine learning comes from the ability to find patterns by
    combining many features. Or said differently, a weak feature may be strong in
    the presence of just the right set of other features.
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法分别被称为*前向选择*和*后向消除*。它们不能保证找到最佳特征子集，这就是为什么我们称之为*近似*。被遗漏或移除的特征中，可能有一些在与尚未达到的特征组合时具有更强的预测能力。记住，机器学习的力量来自于通过组合许多特征来发现模式的能力。或者换句话说，一个弱特征在存在恰好的一组其他特征时可能变得强大。
- en: In practice, however, forward selection or backward elimination works well to
    find a good subset of features with a much smaller computational complexity than
    the exhaustive search. When the number of features is particularly large, however,
    even this approach can be computationally infeasible. In those cases, it might
    be necessary to rely on built-in feature importance measures or other search heuristics,
    which we present in the next section.
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实践中，前向选择或后向消除在以比穷举搜索小得多的计算复杂度找到良好的特征子集方面效果很好。然而，当特征数量特别大时，即使是这种方法也可能在计算上不可行。在这些情况下，可能需要依赖于内置的特征重要性度量或其他搜索启发式方法，我们将在下一节中介绍。
- en: The process of forward feature selection is shown in [figure 5.9](#ch05fig09).
    Depending on the number of features, many models might need to be built. If the
    algorithm is run to the end, you’ll need to build N + (N – 1) + (N – 2)...(N –
    N + 2) + (N – N + 1), or ![](120fig01.jpg). For 20, 100, 500, or 1,000 features,
    this is 210; 5,050; 125,250; and 500,500 model builds, respectively. In addition,
    each cross-validation iteration requires k models to be built, so if the model
    build takes any significant amount of time, this also becomes unmanageable. For
    smaller sets of features, or when running a smaller number of iterations (for
    example, because the increase in accuracy quickly levels off), this approach is
    effective in practice.
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: 前向特征选择的过程如图5.9所示。根据特征的数量，可能需要构建许多模型。如果算法运行到结束，你需要构建N + (N – 1) + (N – 2)...(N
    – N + 2) + (N – N + 1)，或者![图5.9](120fig01.jpg)。对于20、100、500或1000个特征，这分别是210、5,050、125,250和500,500个模型构建。此外，每次交叉验证迭代都需要构建k个模型，因此如果模型构建需要任何显著的时间，这也变得难以管理。对于特征集较小的集合，或者当运行较少的迭代次数（例如，因为准确性的提高迅速达到平稳）时，这种方法在实践中是有效的。
- en: Figure 5.9\. The process of forward feature selection. Beginning at the leftmost
    box, features are added iteratively until the best set of features–in terms of
    the cross-validated evaluation metric–is chosen.
  id: totrans-1007
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.9. 前向特征选择的过程。从最左边的框开始，特征被迭代添加，直到根据交叉验证评估指标选择出最佳特征集。
- en: '![](05fig09.jpg)'
  id: totrans-1008
  prefs: []
  type: TYPE_IMG
  zh: '![图5.9](05fig09.jpg)'
- en: '[Figure 5.10](#ch05fig10) shows the equivalent process of backward elimination.
    The computational requirements are the same as forward selection, so the choice
    between forward and backward methods is usually a question of the problem at hand
    and the choice of algorithm. Some algorithms, for example, perform worse on a
    very small set of features, in which case backward elimination is the better approach.'
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.10](#ch05fig10) 展示了向后消除的等效过程。计算需求与正向选择相同，因此选择正向或向后方法通常取决于具体问题和算法的选择。例如，某些算法在非常小的特征集上表现较差，在这种情况下，向后消除是更好的方法。'
- en: Figure 5.10\. The process of backward feature elimination
  id: totrans-1010
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.10\. 向后特征消除的过程
- en: '![](05fig10.jpg)'
  id: totrans-1011
  prefs: []
  type: TYPE_IMG
  zh: '![图片 5.10](05fig10.jpg)'
- en: A useful way to visualize an iterative feature-selection procedure is by plotting
    a vertical bar chart like the one in [figure 5.11](#ch05fig11).
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: 通过绘制类似于 [图 5.11](#ch05fig11) 的垂直条形图是一种可视化迭代特征选择过程的有用方法。
- en: Figure 5.11\. The iterative feature-selection bar chart, showing the evolution
    of accuracy in a feature-selection procedure—in this case, a forward selection
    algorithm. Any measure of accuracy (see [chapter 4](kindle_split_014.html#ch04))
    that makes sense for the problem should be used here.
  id: totrans-1013
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.11\. 迭代特征选择条形图，显示特征选择过程中的准确度演变——在这种情况下，是正向选择算法。对于问题有意义的任何准确度度量（见 [第 4 章](kindle_split_014.html#ch04)）都应在此处使用。
- en: '![](05fig11.jpg)'
  id: totrans-1014
  prefs: []
  type: TYPE_IMG
  zh: '![图片 5.11](05fig11.jpg)'
- en: As we mentioned previously, some ML algorithms have built-in methods for feature
    selection. Instead of making feature selection based on built-in feature rankings,
    you can use a hybrid approach in which the built-in feature importance is used
    to find the best or worst feature in each iteration of the forward selection or
    backward elimination process. This can significantly reduce the computation time
    when you have many features, but will likely yield less-accurate approximations
    of the optimal feature subset.
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，一些机器学习算法具有内置的特征选择方法。您可以使用混合方法，而不是基于内置特征排名进行特征选择，这种方法在正向选择或向后消除过程的每次迭代中使用内置特征重要性来找到最佳或最差特征。当您有大量特征时，这可以显著减少计算时间，但可能会产生对最优特征子集的更不精确的近似。
- en: 5.3.2\. Feature selection for data exploration
  id: totrans-1016
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2\. 数据探索的特征选择
- en: Feature selection can be used for more than avoiding overfitting or making the
    model leaner. A powerful use of feature selection is to gain insight into the
    model and the training data. In fact, in some cases, you might want to build a
    classifier only in order to run a feature-selection algorithm, and not for making
    predictions.
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择不仅可以用于避免过拟合或使模型更精简。特征选择的一个强大用途是深入了解模型和训练数据。实际上，在某些情况下，您可能只想构建一个分类器来运行特征选择算法，而不是为了做出预测。
- en: You can use feature selection to perform an exploratory analysis of the data
    that was used for building the model. From the feature-selection procedure, you
    know the most important features—the most informative set of features for predicting
    the target variable from all of the features. This tells you something about the
    data, which can be useful by itself. Imagine that your task is to predict whether
    a patient is likely to have cancer. Because you’re not certain about the cause
    of the specific form of cancer, you add all the features you can get your hands
    on and use feature selection to find the top features. You’re not only gaining
    a better cross-validated accuracy, but also using the data to indicate which factors
    are most likely to cause the disease or at least correlate with the probability
    of diagnosis. The discussed methods of feature selection don’t tell you whether
    the features are powerful in the positive or negative direction (in the case of
    binary classification), but you can easily visualize the specific feature against
    the target variable to understand this (for example, using the visualizations
    in [section 2.3](kindle_split_012.html#ch02lev1sec3)).
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用特征选择对用于构建模型的原始数据进行探索性分析。从特征选择过程中，你可以知道最重要的特征——从所有特征中预测目标变量的最有信息量的特征集。这告诉你有关数据的一些信息，这本身可能很有用。想象一下，你的任务是预测患者是否可能患有癌症。因为你不确定特定形式癌症的原因，你添加了你所能获取的所有特征，并使用特征选择来找到顶级特征。你不仅获得了更好的交叉验证准确率，而且使用数据来指示哪些因素最有可能导致疾病，或者至少与诊断概率相关。所讨论的特征选择方法并没有告诉你特征在正方向还是负方向上是否强大（在二元分类的情况下），但你可以很容易地将特定特征与目标变量进行可视化，以理解这一点（例如，使用[第2.3节](kindle_split_012.html#ch02lev1sec3)中的可视化）。
- en: Another unsupervised use case for feature selection is for *dimensionality reduction*.
    One of the great challenges when working with datasets with more than three variables
    is how to visualize the data. The human brain has been optimized for a three-dimensional
    world, and we have a hard time grasping more than that. In real-world data, however,
    having only three features is extremely unlikely, and you need to employ various
    techniques to visualize high-dimensional datasets. You can use feature selection
    as a way to show how the ML algorithms can divide the data into classes, for example,
    by simply plotting the two or three best features against the target variable.
    [Figure 5.12](#ch05fig12) shows an example of a decision boundary shown in two
    dimensions, even though many more features were used to build the model.
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个特征选择的无监督用例是用于**降维**。当处理包含三个以上变量的数据集时，一个巨大的挑战是如何可视化数据。人类大脑已经优化适应三维世界，我们很难理解超过三维的信息。然而，在现实世界的数据中，只有三个特征的情况极为罕见，你需要采用各种技术来可视化高维数据集。你可以使用特征选择来展示机器学习算法如何将数据划分为类别，例如，只需简单地将两个或三个最佳特征与目标变量进行对比。图[5.12](#ch05fig12)展示了在二维中显示的决策边界示例，尽管构建模型使用了更多特征。
- en: Figure 5.12\. This decision boundary for classifying into circles and diamonds
    has only two features. The model was built on many more features, but the sqrt(Fare)
    and Age features were found by the feature-selection algorithm to be important
    in this particular problem (Titanic survival prediction). This plot was first
    introduced in [chapter 3](kindle_split_013.html#ch03).
  id: totrans-1020
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.12. 这个将数据分类为圆形和钻石的决策边界只有两个特征。模型是在许多更多特征的基础上构建的，但特征选择算法发现sqrt(Fare)和Age特征在这个特定问题（泰坦尼克号生存预测）中非常重要。这个图表最初在[第3章](kindle_split_013.html#ch03)中介绍。
- en: '![](05fig12_alt.jpg)'
  id: totrans-1021
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig12_alt.jpg)'
- en: In the next section, you’ll see an example of how feature selection can be useful
    in real-world problems.
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将看到特征选择在现实世界问题中如何有用的一个示例。
- en: 5.3.3\. Real-world feature selection example
  id: totrans-1023
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3. 现实世界的特征选择示例
- en: For a great use case of feature engineering and feature selection in the real
    world, let’s look at an example from science.
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特征工程和特征选择在现实世界中的伟大用例，让我们看看科学中的一个例子。
- en: Your task is to find real supernova events from huge astronomical images among
    a large number of so-called bogus events (events that look real, but aren’t).
    [Figure 5.13](#ch05fig13) shows examples of real and bogus events.
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: 你的任务是从小型天文图像中从大量所谓的虚假事件（看起来真实但并非如此的事件）中找到真正的超新星事件。[图5.13](#ch05fig13)展示了真实和虚假事件的示例。
- en: Figure 5.13\. Real supernova images are shown in the panel on the left. Bogus
    candidate events are shown in the panel on the right.^([[1](#ch05fn01)]) The job
    of the classifier is to learn the difference between these two types of candidates
    from features extracted from the images. (These are obvious examples; many others
    are hard to classify, even for trained persons.)
  id: totrans-1026
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.13。真实超新星图像显示在左面板中。虚假候选事件显示在右面板中.^([[1](#ch05fn01)])分类器的工作是学习从图像中提取的特征之间的差异，以区分这两种类型的候选者。（这些是明显的例子；许多其他例子很难分类，即使是训练有素的人。）
- en: ¹
  id: totrans-1027
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-1028
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The supernova images and data graphs in this section originally appeared in
    the 2013 *Monthly Notices of the Royal Astronomical Society*, volume 435, issue
    2, pages 1047-1060 ([http://mnras.oxfordjournals.org/content/435/2/1047](http://mnras.oxfordjournals.org/content/435/2/1047)).
  id: totrans-1029
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本节中的超新星图像和数据图最初发表在2013年《皇家天文学会月报》第435卷第2期，第1047-1060页([http://mnras.oxfordjournals.org/content/435/2/1047](http://mnras.oxfordjournals.org/content/435/2/1047))。
- en: '![](05fig13.jpg)'
  id: totrans-1030
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig13.jpg)'
- en: The real/bogus classifier is built by first processing the raw image data into
    a set of features, some of which are discussed in the next chapter. You then run
    the featurized data through a random forest algorithm to build the classifier,
    and perform various model optimizations such as the ones outlined in [chapters
    3](kindle_split_013.html#ch03) and [4](kindle_split_014.html#ch04). The last part
    before putting this model into the live stream from the telescope is to determine
    the best features, avoid overfitting, and make the model as small as possible
    in order to support the real-time requirements of the project. The feature-selection
    plot, a slightly more advanced version of [figure 5.11](#ch05fig11), is shown
    in [figure 5.14](#ch05fig14).
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
  zh: 真实/虚假分类器是通过首先将原始图像数据处理成一组特征来构建的，其中一些将在下一章中讨论。然后，您将特征化数据通过随机森林算法来构建分类器，并执行各种模型优化，如第3章和第4章中概述的优化。在将此模型投入望远镜的实时流之前，最后一部分是确定最佳特征，避免过拟合，并使模型尽可能小，以支持项目的实时需求。特征选择图，是[图5.11](#ch05fig11)的一个稍微高级的版本，显示在[图5.14](#ch05fig14)中。
- en: Figure 5.14\. The feature-selection plot showing a backward elimination process.
    Each feature from the bottom up was selected for removal as the algorithm progressed,
    and in each step the customized evaluation metric of missed detection rate (MDR)
    at 1% false-positive rate (FPR) was computed. The bars show the performance metric
    obtained at each step (smaller is better in this case) by removing the feature
    (with standard deviation from cross-validation). After removing 23 features (out
    of 44), the cross-validated performance gain levels off and eventually becomes
    much worse when too many features have been removed. In the end, a significant
    5 percentage points were gained in model performance by removing noisy features.
  id: totrans-1032
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.14。特征选择图显示了反向消除过程。随着算法的进行，从底部向上选择每个特征进行删除，并在每个步骤中计算1%假阳性率（FPR）下的漏检率（MDR）的定制评估指标。条形图显示了通过删除特征（带有交叉验证的标准差）在每一步获得的性能指标（在这种情况下，越小越好）。删除了23个特征（共44个）后，交叉验证的性能提升趋于平稳，当删除太多特征时最终变得非常糟糕。最终，通过删除噪声特征，模型性能提高了5个百分点。
- en: '![](05fig14_alt.jpg)'
  id: totrans-1033
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig14_alt.jpg)'
- en: Now, by knowing which features are most important for the model, you can plot
    these features against real and bogus events in order to visualize how a particular
    feature helps solve the problem. [Figure 5.15](#ch05fig15) shows the performance
    of four of the best features.
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过了解哪些特征对模型最重要，您可以绘制这些特征与真实和虚假事件的关系图，以可视化特定特征如何帮助解决问题。[图5.15](#ch05fig15)显示了四个最佳特征的性能。
- en: Figure 5.15\. Visualization of the performance of four individual features chosen
    by our feature-selection algorithm to be among the best features for our model.
    The histograms show the number of real or bogus events that take on a particular
    value of the feature. You can see that the distributions of real versus bogus
    events are different in the *amp* and *flux_ratio* features, and they’re selected
    as the top-performing features in our feature-selection procedure.
  id: totrans-1035
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.15。我们特征选择算法选择的四个最佳特征中四个单个特征的性能可视化。直方图显示了具有特定特征值的真实或虚假事件的数量。您可以看到，在*振幅*和*通量比*特征中，真实事件与虚假事件的分布不同，它们被选为特征选择过程中的顶级性能特征。
- en: '![](05fig15_alt.jpg)'
  id: totrans-1036
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig15_alt.jpg)'
- en: 5.4\. Summary
  id: totrans-1037
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4. 摘要
- en: 'This chapter introduced feature engineering, which transforms raw data to improve
    the accuracy of ML models. The primary takeaways from this chapter are as follows:'
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了特征工程，它将原始数据转换为提高机器学习模型准确度。本章的主要收获如下：
- en: Feature engineering is the process of applying mathematical transformations
    to raw data to create new input features for ML modeling. The transformations
    can range from simple to extremely complex.
  id: totrans-1039
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程是将数学变换应用于原始数据以创建新的输入特征用于机器学习建模的过程。这些变换可以从简单到极其复杂。
- en: 'Feature engineering is valuable for the following five reasons:'
  id: totrans-1040
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程有以下五个原因很有价值：
- en: It can create features that are more closely related to the target variable.
  id: totrans-1041
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以创建与目标变量更紧密相关的特征。
- en: It enables you to bring in external data sources.
  id: totrans-1042
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使您能够引入外部数据源。
- en: It allows you to use unstructured data.
  id: totrans-1043
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许您使用非结构化数据。
- en: It can enable you to create features that are more interpretable.
  id: totrans-1044
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以使您创建更可解释的特征。
- en: It gives you the freedom to create lots of features and then choose the best
    subset via feature selection.
  id: totrans-1045
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它给您自由创建大量特征，然后通过特征选择选择最佳子集。
- en: There’s an intricate link between feature engineering and domain knowledge.
  id: totrans-1046
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程与领域知识之间存在复杂的联系。
- en: 'Feature engineering fits into the overall ML workflow in two places:'
  id: totrans-1047
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程在整体机器学习工作流程中适合两个地方：
- en: On the training dataset, prior to fitting a model
  id: totrans-1048
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练数据集上，在拟合模型之前
- en: On the prediction dataset, prior to generating predictions
  id: totrans-1049
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预测数据集上，在生成预测之前
- en: 'Two types of simple feature engineering can be used on a problem of event recommendation:'
  id: totrans-1050
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在事件推荐问题中可以使用两种类型的简单特征工程：
- en: Extraction of features from date-time information
  id: totrans-1051
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从日期时间信息中提取特征
- en: Feature engineering on natural language text
  id: totrans-1052
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言文本的特征工程
- en: Feature selection is a rigorous way to select the most predictive subset of
    features from a dataset.
  id: totrans-1053
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征选择是从数据集中选择最具预测性特征子集的一种严谨方法。
- en: 5.5\. Terms from this chapter
  id: totrans-1054
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5. 本章术语
- en: '| Word | Definition |'
  id: totrans-1055
  prefs: []
  type: TYPE_TB
  zh: '| 单词 | 定义 |'
- en: '| --- | --- |'
  id: totrans-1056
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| feature engineering | Transforming input data to extract more value and improve
    the predictive accuracy of ML models |'
  id: totrans-1057
  prefs: []
  type: TYPE_TB
  zh: '| 特征工程 | 将输入数据转换为提取更多价值并提高机器学习模型预测准确度的过程 |'
- en: '| feature selection | Process of choosing the most predictive subset of features
    out of a larger set |'
  id: totrans-1058
  prefs: []
  type: TYPE_TB
  zh: '| 特征选择 | 从更大的特征集中选择最具预测性的特征子集的过程 |'
- en: '| forward selection | A version of feature selection that iteratively adds
    the feature that increases the accuracy of model the most, conditional on the
    current active feature set |'
  id: totrans-1059
  prefs: []
  type: TYPE_TB
  zh: '| 前向选择 | 一种特征选择版本，根据当前活动特征集，迭代地添加增加模型准确度最大的特征 |'
- en: '| backward elimination | A version of feature selection that removes the feature
    that decreases the accuracy of model the most, conditional on the current active
    feature set |'
  id: totrans-1060
  prefs: []
  type: TYPE_TB
  zh: '| 后向消除 | 一种特征选择版本，根据当前活动特征集，移除降低模型准确度最大的特征 |'
- en: '| bag of words | A method for turning arbitrary text into numerical features
    for use by the ML algorithm |'
  id: totrans-1061
  prefs: []
  type: TYPE_TB
  zh: '| 词袋模型 | 一种将任意文本转换为数值特征以供机器学习算法使用的方法 |'
- en: '[Chapter 7](kindle_split_018.html#ch07) expands on the simple feature-engineering
    approaches presented here so you can perform more-advanced feature engineering
    on data such as text, images, and time series. In the next chapter we’ll use what
    we’ve learned in a full-chapter example.'
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
  zh: '[第7章](kindle_split_018.html#ch07)扩展了这里所展示的简单特征工程方法，以便您可以对文本、图像和时间序列等数据进行更高级的特征工程。在下一章中，我们将使用本章所学的内容进行一个全章示例。'
