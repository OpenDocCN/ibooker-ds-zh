- en: 1 Introduction to deep reinforcement learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 深度强化学习简介
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中
- en: You will learn what deep reinforcement learning is and how it is different from
    other machine learning approaches.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将学习深度强化学习是什么，以及它与其他机器学习方法有何不同。
- en: You will learn about the recent progress in deep reinforcement learning and
    what it can do for a variety of problems.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将了解深度强化学习的最新进展以及它能为各种问题带来什么。
- en: You will know what to expect from this book and how to get the most out of it.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将了解这本书的内容以及如何从中获得最大收益。
- en: I visualize a time when we will be to robots what dogs are to humans, and I’m
    rooting for the machines.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我想象着一个时代，我们将对机器人产生如同狗对人类那样的影响，我为机器人的进步感到自豪。
- en: — Claude Shannon Father of the information age and contributor to the field
    of artificial intelligence
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ——克劳德·香农 信息时代之父，人工智能领域的贡献者
- en: Humans naturally pursue feelings of happiness. From picking out our meals to
    advancing our careers, every action we choose is derived from our drive to experience
    rewarding moments in life. Whether these moments are self-centered pleasures or
    the more generous of goals, whether they bring us immediate gratification or long-term
    success, they’re still our perception of how important and valuable they are.
    And to some extent, these moments are the reason for our existence.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 人类天生追求幸福感。从挑选我们的食物到提升我们的职业生涯，我们选择的每一个行动都源于我们体验生活中有益时刻的驱动力。这些时刻是自私的愉悦还是更慷慨的目标，它们是否带给我们即时的满足或长期的成功，它们仍然是我们对它们重要性和价值的感知。在某种程度上，这些时刻是我们存在的理由。
- en: Our ability to achieve these precious moments seems to be correlated with intelligence;
    “intelligence” is defined as the ability to acquire and apply knowledge and skills.
    People who are deemed by society as intelligent are capable of trading not only
    immediate satisfaction for long-term goals, but also a good, certain future for
    a possibly better, yet uncertain, one. Goals that take longer to materialize and
    that have unknown long-term value are usually the hardest to achieve, and those
    who can withstand the challenges along the way are the exception, the leaders,
    the intellectuals of society.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现这些珍贵时刻的能力似乎与智力相关；“智力”被定义为获取和应用知识和技能的能力。社会认为智力高的人能够为了长期目标而放弃即时满足，为了可能更好但不确定的未来而放弃一个确定但可能更好的未来。那些需要较长时间才能实现且具有未知长期价值的目标通常是最难实现的，而那些能够承受沿途挑战的人是例外，是领导者，是社会中的知识分子。
- en: In this book, you learn about an approach, known as deep reinforcement learning,
    involved with creating computer programs that can achieve goals that require intelligence.
    In this chapter, I introduce deep reinforcement learning and give suggestions
    to get the most out of this book.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，你将了解一种被称为深度强化学习的方法，它涉及创建能够实现需要智能的目标的计算机程序。在本章中，我介绍了深度强化学习，并给出了一些建议，帮助你从这本书中获得最大收益。
- en: What is deep reinforcement learning?
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是深度强化学习？
- en: Deep reinforcement learning (DRL) is a machine learning approach to artificial
    intelligence concerned with creating computer programs that can solve problems
    requiring intelligence. The distinct property of DRL programs is learning through
    trial and error from feedback that’s simultaneously sequential, evaluative, and
    sampled by leveraging powerful non-linear function approximation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）是一种机器学习方法，涉及创建能够解决需要智能问题的计算机程序。DRL程序的独特属性是通过利用强大的非线性函数逼近，从同时具有序列性、评估性和样本性的反馈中通过试错进行学习。
- en: I want to unpack this definition for you one bit at a time. But, don’t get too
    caught up with the details because it’ll take me the whole book to get you grokking
    deep reinforcement learning. The following is the *introduction* to what you learn
    about in this book. As such, it’s repeated and explained in detail in the chapters
    ahead.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我将逐步为你解释这个定义。但是，不要过于纠结于细节，因为这需要我整本书的时间来帮助你理解深度强化学习。以下是对你在本书中将要学习内容的**介绍**。因此，它将在接下来的章节中重复并详细解释。
- en: If I succeed with my goal for this book, after you complete it, you should understand
    this definition precisely. You should be able to tell why I used the words that
    I used, and why I didn’t use more or fewer words. But, for this chapter, simply
    sit back and plow through it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我实现了这本书的目标，在你完成阅读后，你应该能够精确地理解这个定义。你应该能够解释我为什么使用这些词，以及为什么没有使用更多或更少的词。但是，对于这一章，只需放松并深入阅读即可。
- en: Deep reinforcement learning is a machine learning approach to artificial intelligence
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度强化学习是一种人工智能的机器学习方法
- en: '*artificial intelligence* (AI) is a branch of computer science involved in
    the creation of computer programs capable of demonstrating intelligence. Traditionally,
    any piece of software that displays cognitive abilities such as perception, search,
    planning, and learning is considered part of AI. Several examples of functionality
    produced by AI software are'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*人工智能*（AI）是计算机科学的一个分支，涉及创建能够展示智能的计算机程序。传统上，任何显示认知能力（如感知、搜索、规划和学习）的软件都被认为是AI的一部分。由AI软件产生的功能示例包括'
- en: The pages returned by a search engine
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索引擎返回的页面
- en: The route produced by a GPS app
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPS应用生成的路线
- en: The voice recognition and the synthetic voice of smart-assistant software
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能助手软件的语音识别和合成语音
- en: The products recommended by e-commerce sites
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子商务网站推荐的产品
- en: The follow-me feature in drones
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无人机中的跟随功能
- en: '![](../Images/01_01.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01_01.png)'
- en: Subfields of artificial intelligence
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的子领域
- en: 'All computer programs that display intelligence are considered AI, but not
    all examples of AI can learn. *machine learning* (ML) is the area of AI concerned
    with creating computer programs that can solve problems requiring intelligence
    by learning from data. There are three main branches of ML: supervised, unsupervised,
    and reinforcement learning.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 所有显示智能的计算机程序都被认为是AI，但并非所有AI的例子都能学习。*机器学习*（ML）是AI的一个领域，涉及创建能够通过学习数据来解决需要智能的问题的计算机程序。ML有三个主要分支：监督学习、无监督学习和强化学习。
- en: '![](../Images/01_02.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01_02.png)'
- en: Main branches of machine learning
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的主要分支
- en: '*supervised learning* (SL) is the task of learning from labeled data. In SL,
    a human decides which data to collect and how to label it. The goal in SL is to
    generalize. A classic example of SL is a handwritten-digit-recognition application:
    a human gathers images with handwritten digits, labels those images, and trains
    a model to recognize and classify digits in images correctly. The trained model
    is expected to generalize and correctly classify handwritten digits in new images.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*监督学习*（SL）是从标记数据中学习的任务。在SL中，人类决定收集哪些数据以及如何标记它们。SL的目标是泛化。SL的一个经典例子是手写数字识别应用：人类收集带有手写数字的图像，标记这些图像，并训练一个模型来正确识别和分类图像中的数字。期望训练好的模型能够泛化并在新的图像中正确分类手写数字。'
- en: '*unsupervised learning* (UL) is the task of learning from unlabeled data. Even
    though data no longer needs labeling, the methods used by the computer to gather
    data still need to be designed by a human. The goal in UL is to compress. A classic
    example of UL is a customer segmentation application; a human collects customer
    data and trains a model to group customers into clusters. These clusters compress
    the information, uncovering underlying relationships in customers.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*无监督学习*（UL）是从未标记数据中学习的任务。尽管数据不再需要标记，但计算机用于收集数据的方法仍然需要由人类设计。UL的目标是压缩。UL的一个经典例子是客户细分应用；人类收集客户数据并训练一个模型将客户分组到聚类中。这些聚类压缩了信息，揭示了客户之间的潜在关系。'
- en: '*reinforcement learning* (RL) is the task of learning through trial and error.
    In this type of task, no human labels data, and no human collects or explicitly
    designs the collection of data. The goal in RL is to act. A classic example of
    RL is a Pong-playing agent; the agent repeatedly interacts with a Pong emulator
    and learns by taking actions and observing their effects. The trained agent is
    expected to act in such a way that it successfully plays Pong.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化学习*（RL）是通过试错来学习的任务。在这种类型的任务中，没有人类标记数据，也没有人类收集或明确设计数据集。RL的目标是行动。RL的一个经典例子是玩Pong的代理；代理反复与Pong模拟器交互，通过采取行动并观察其效果来学习。期望训练好的代理能够以成功玩Pong的方式行动。'
- en: A powerful recent approach to ML, called *deep learning* (DL), involves using
    multi-layered non-linear function approximation, typically neural networks. DL
    isn’t a separate branch of ML, so it’s not a different task than those described
    previously. DL is a collection of techniques and methods for using neural networks
    to solve ML tasks, whether SL, UL, or RL. DRL is simply the use of DL to solve
    RL tasks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一种强大的近期ML方法，称为*深度学习*（DL），涉及使用多层非线性函数逼近，通常是神经网络。DL不是一个独立的ML分支，因此它不同于之前描述的任务。DL是一系列技术和方法，用于使用神经网络来解决ML任务，无论是SL、UL还是RL。DRL仅仅是使用DL来解决RL任务。
- en: '![](../Images/01_03.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01_03.png)'
- en: Deep learning is a powerful toolbox
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一个强大的工具箱
- en: 'The bottom line is that DRL is an approach to a problem. The field of AI defines
    the problem: creating intelligent machines. One of the approaches to solving that
    problem is DRL. Throughout the book, will you find comparisons between RL and
    other ML approaches, but only in this chapter will you find definitions and a
    historical overview of AI in general. It’s important to note that the field of
    RL includes the field of DRL, so although I make a distinction when necessary,
    when I refer to RL, remember that DRL is included.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，强化学习（RL）是一种解决问题的方法。人工智能领域定义了这个问题：创造智能机器。解决这个问题的方法之一是强化学习（RL）。在整个书中，你会找到RL与其他机器学习（ML）方法的比较，但只有在这一章中，你会找到人工智能的一般定义和历史概述。需要注意的是，强化学习（RL）领域包括深度强化学习（DRL）领域，所以尽管我在必要时会做出区分，但当我提到RL时，请记住DRL也包括在内。
- en: Deep reinforcement learning is concerned with creating computer programs
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度强化学习关注的是创建计算机程序
- en: At its core, DRL is about complex sequential decision-making problems under
    uncertainty. But, this is a topic of interest in many fields; for instance, *control
    theory* (CT) studies ways to control complex known dynamic systems. In CT, the
    dynamics of the systems we try to control are usually known in advance. *Operations
    research* (OR), another instance, also studies decision-making under uncertainty,
    but problems in this field often have much larger action spaces than those commonly
    seen in DRL. *psychology* studies human behavior, which is partly the same “complex
    sequential decision-making under uncertainty” problem.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）的核心是关于在不确定性下的复杂序列决策问题。但是，这是一个在许多领域都感兴趣的话题；例如，*控制理论*（CT）研究控制复杂已知动态系统的方法。在CT中，我们试图控制的系统的动力学通常事先是已知的。*运筹学*（OR）是另一个例子，它也研究不确定性下的决策，但这个领域的问题通常比DRL中常见的动作空间要大得多。*心理学*研究人类行为，这在某种程度上与“不确定性下的复杂序列决策”问题相同。
- en: '![](../Images/01_04.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/01_04.png)'
- en: The synergy between similar fields
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 相似领域之间的协同作用
- en: The bottom line is that you have come to a field that’s influenced by a variety
    of others. Although this is a good thing, it also brings inconsistencies in terminologies,
    notations, and so on. My take is the computer science approach to this problem,
    so this book is about building computer programs that solve complex decision-making
    problems under uncertainty, and as such, you can find code examples throughout
    the book.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，你已经进入了一个受到许多其他领域影响的领域。尽管这是好事，但也带来了术语、符号等方面的不一致。我的观点是计算机科学方法来解决这个问题，因此这本书是关于构建解决不确定性下复杂决策问题的计算机程序，因此你可以在整本书中找到代码示例。
- en: In DRL, these computer programs are called *agents*. An agent is a decision
    maker *Only* and nothing else. That means if you’re training a robot to pick up
    objects, the robot arm isn’t part of the agent. Only the code that makes decisions
    is referred to as the agent.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在DRL中，这些计算机程序被称为*代理*。代理只是一个决策者，没有其他角色。这意味着如果你在训练一个机器人来捡起物体，机器人手臂不是代理的一部分。只有做出决策的代码才被称为代理。
- en: Deep reinforcement learning agents can solve problems that require intelligence
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度强化学习代理可以解决需要智能的问题
- en: On the other side of the agent is the **environment**. The environment is everything
    outside the agent; everything the agent has no total control over. Again, imagine
    you’re training a robot to pick up objects. The objects to be picked up, the tray
    where the objects lay, the wind, and everything outside the decision maker are
    part of the environment. That means the robot arm is also part of the environment
    because it isn’t part of the agent. And even though the agent can decide to move
    the arm, the actual arm movement is noisy, and thus the arm is part of the environment.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在代理的另一边是**环境**。环境是代理之外的一切；代理无法完全控制的一切。再次想象你正在训练一个机器人来捡起物体。要捡起的物体、放置物体的托盘、风以及决策者之外的一切都是环境的一部分。这意味着机器人手臂也是环境的一部分，因为它不是代理的一部分。即使代理可以决定移动手臂，实际的手臂运动是嘈杂的，因此手臂也是环境的一部分。
- en: 'This strict boundary between the agent and the environment is counterintuitive
    at first, but the decision maker, the agent, can only have a single role: *making
    decisions*. Everything that comes after the decision gets bundled into the environment.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这种代理和环境之间的严格界限一开始可能感觉不太直观，但决策者，即代理，只能有一个单一的角色：*做出决策*。决策之后的所有事物都被打包进环境中。
- en: '![](../Images/01_05.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/01_05.png)'
- en: Boundary between agent and environment
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 代理和环境之间的界限
- en: Chapter 2 provides an in-depth survey of all the components of DRL. The following
    is a preview of what you’ll learn in chapter 2.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第二章深入探讨了DRL的所有组成部分。以下是对第二章内容的预览。
- en: The environment is represented by a set of variables related to the problem.
    For instance, in the robotic arm example, the location and velocities of the arm
    would be part of the variables that make up the environment. This set of variables
    and all the possible values that they can take are referred to as the *state space*.
    A state is an instantiation of the state space, a set of values the variables
    take.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 环境由与问题相关的一系列变量表示。例如，在机器人臂的例子中，手臂的位置和速度将是构成环境的变量的一部分。这些变量及其所有可能的值被称为*状态空间*。状态是状态空间的一个实例化，是一组变量所取的值。
- en: Interestingly, often, agents don’t have access to the actual full state of the
    environment. The part of a state that the agent can observe is called an *Observation*.
    Observations depend on states but are what the agent can see. For instance, in
    the robotic arm example, the agent may only have access to camera images. While
    an exact location of each object exists, the agent doesn’t have access to this
    specific state. Instead, the observations the agent perceives are derived from
    the states. You’ll often see in the literature *states* being used interchangeably,
    including in this book. I apologize in advance for the inconsistencies. Simply
    know the differences and be aware of the lingo; that’s what matters.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，通常情况下，智能体无法访问环境的实际完整状态。智能体可以观察到的状态部分被称为*观察*。观察依赖于状态，但却是智能体所能看到的。例如，在机器人臂的例子中，智能体可能只能访问摄像头图像。虽然每个物体的确切位置都存在，但智能体无法访问这个特定的状态。相反，智能体感知到的观察是从状态中派生出来的。你经常会在文献中看到*状态*被交替使用，包括在这本书中。我提前为这些不一致之处道歉。只需了解这些差异，并注意术语；这才是最重要的。
- en: '![](../Images/01_06.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片1](../Images/01_06.png)'
- en: States vs. observations
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 状态与观察
- en: At each state, the environment makes available a set of actions the agent can
    choose from. The agent influences the environment through these actions. The environment
    may change states as a response to the agent’s action. The function that’s responsible
    for this mapping is called the transition function. The environment may also provide
    a reward signal as a response. The function responsible for this mapping is called
    the *reward function*. The set of transition and reward functions is referred
    to as the *model* of the environment.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个状态下，环境会提供一组智能体可以选择的动作。智能体通过这些动作影响环境。环境可能会根据智能体的动作改变状态。负责这种映射的函数称为转换函数。环境还可能提供奖励信号作为响应。负责这种映射的函数称为*奖励函数*。转换和奖励函数的集合被称为环境的*模型*。
- en: '![](../Images/01_07.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片2](../Images/01_07.png)'
- en: The reinforcement learning cycle
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习周期
- en: The environment commonly has a well-defined task. The goal of this task is defined
    through the reward function. The reward-function signals can be simultaneously
    sequential, evaluative, and sampled. To achieve the goal, the agent needs to demonstrate
    intelligence, or at least cognitive abilities commonly associated with intelligence,
    such as long-term thinking, information gathering, and generalization.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 环境通常有一个定义良好的任务。这个任务的目标是通过奖励函数来定义的。奖励函数的信号可以是同时序列的、评估的和采样的。为了实现目标，智能体需要表现出智能，或者至少是与智能相关联的认知能力，如长期思考、信息收集和泛化。
- en: 'The agent has a three-step process: the agent interacts with the environment,
    the agent evaluates its behavior, and the agent improves its responses. The agent
    can be designed to learn mappings from observations to actions called policies.
    The agent can be designed to learn the model of the environment on mappings called
    models. The agent can be designed to learn to estimate the reward-to-go on mappings
    called value functions.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体有三个步骤的过程：智能体与环境交互，智能体评估其行为，智能体改进其响应。智能体可以被设计成学习从观察到的动作映射，称为策略。智能体可以被设计成学习环境的模型映射，称为模型。智能体可以被设计成学习估计奖励到映射，称为价值函数。
- en: Deep reinforcement learning agents improve their behavior through trial-and-error
    learning
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度强化学习智能体通过试错学习改进其行为
- en: The interactions between the agent and the environment go on for several cycles.
    Each cycle is called a *time step*. At each time step, the agent observes the
    environment, takes action, and receives a new observation and reward. The set
    of the state, the action, the reward, and the new state is called an *experience*.
    Every experience has an opportunity for learning and improving performance.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 代理与环境之间的交互会持续几个周期。每个周期被称为一个*时间步长*。在每个时间步长，代理观察环境，采取行动，并接收新的观察和奖励。状态、行动、奖励和新的状态集被称为一个*经验*。每个经验都有学习和改进性能的机会。
- en: '![](../Images/01_08.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/01_08.png)'
- en: Experience tuples
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 经验元组
- en: 'The task the agent is trying to solve may or may not have a natural ending.
    Tasks that have a natural ending, such as a game, are called *episodic tasks*.
    Conversely, tasks that don’t are called *continuing tasks*, such as learning forward
    motion. The sequence of time steps from the beginning to the end of an episodic
    task is called an *episode*. Agents may take several time steps and episodes to
    learn to solve a task. Agents learn through trial and error: they try something,
    observe, learn, try something else, and so on.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 代理试图解决的问题可能有一个自然的结束，也可能没有。有自然结束的任务，如游戏，被称为*事件任务*。相反，没有自然结束的任务，如学习前进运动，被称为*持续任务*。从事件任务开始到结束的时间步序列被称为一个*事件*。代理可能需要几个时间步和事件来学习解决一个任务。代理通过试错来学习：他们尝试一些东西，观察，学习，再尝试其他东西，如此循环。
- en: You’ll start learning more about this cycle in chapter 4, which contains a type
    of environment with a single step per episode. Starting with chapter 5, you’ll
    learn to deal with environments that require more than a single interaction cycle
    per episode.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在第4章中开始学习更多关于这个循环的内容，该章包含一个每个事件只有一个步骤的环境。从第5章开始，你将学习如何处理每个事件需要多个交互周期才能完成的环境。
- en: Deep reinforcement learning agents learn from sequential feedback
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度强化学习代理从序列反馈中学习
- en: The action taken by the agent may have delayed consequences. The reward may
    be sparse and only manifest after several time steps. Thus the agent must be able
    to learn from sequential feedback. Sequential feedback gives rise to a problem
    referred to as the *temporal credit assignment problem*. The temporal credit assignment
    problem is the challenge of determining which state and/or action is responsible
    for a reward. When there’s a temporal component to a problem, and actions have
    delayed consequences, it’s challenging to assign credit for rewards.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 代理采取的行动可能具有延迟后果。奖励可能稀疏，并且只在几个时间步之后才显现。因此，代理必须能够从序列反馈中学习。序列反馈引发了被称为*时间信用分配问题*的问题。时间信用分配问题是指确定哪个状态和/或行动对奖励负责的挑战。当问题有时间成分，并且行动有延迟后果时，为奖励分配信用是一个挑战。
- en: '![](../Images/01_09.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/01_09.png)'
- en: The difficulty of the temporal credit assignment problem
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 时间信用分配问题的难度
- en: In chapter 3, we’ll study the ins and outs of sequential feedback in isolation.
    That is, your programs learn from simultaneously sequential, supervised (as opposed
    to evaluative), and exhaustive (as opposed to sampled) feedback.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我们将研究孤立情况下序列反馈的来龙去脉。也就是说，你的程序会从同时序列的、监督的（与评估的相对）、以及穷尽的（与抽样的相对）反馈中学习。
- en: Deep reinforcement learning agents learn from evaluative feedback
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度强化学习代理从评估反馈中学习
- en: The reward received by the agent may be weak, in the sense that it may provide
    no supervision. The reward may indicate goodness and not correctness, meaning
    it may contain no information about other potential rewards. Thus the agent must
    be able to learn from *evaluative feedback*. Evaluative feedback gives rise to
    the need for exploration. The agent must be able to balance the gathering of information
    with the exploitation of current information. This is also referred to as the
    *exploration versus exploitation trade-off*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 代理收到的奖励可能较弱，从某种意义上说，它可能不提供监督。奖励可能表明良好，但不正确，这意味着它可能不包含关于其他潜在奖励的信息。因此，代理必须能够从*评估反馈*中学习。评估反馈引发了探索的需求。代理必须能够在收集信息和利用当前信息之间取得平衡。这也被称为*探索与利用的权衡*。
- en: '![](../Images/01_10.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/01_10.png)'
- en: The difficulty of the exploration vs. exploitation trade-off
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 探索与利用权衡的难度
- en: In chapter 4, we’ll study the ins and outs of evaluative feedback in isolation.
    That is, your programs will learn from feedback that is simultaneously one-shot
    (as opposed to sequential), evaluative, and exhaustive (as opposed to sampled).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章中，我们将单独研究评估反馈的细节。也就是说，你的程序将从同时具有一次性（与序列性相对）、评估性和详尽性（与样本性相对）的反馈中学习。
- en: Deep reinforcement learning agents learn from sampled feedback
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度强化学习代理从样本反馈中学习
- en: The reward received by the agent is merely a sample, and the agent doesn’t have
    access to the reward function. Also, the state and action spaces are commonly
    large, even infinite, so trying to learn from sparse and weak feedback becomes
    a harder challenge with samples. Therefore, the agent must be able to learn from
    sampled feedback, and it must be able to generalize.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 代理收到的奖励只是一个样本，代理无法访问奖励函数。此外，状态和动作空间通常很大，甚至是无限的，因此从稀疏和微弱的反馈中学习变得更具挑战性。因此，代理必须能够从样本反馈中学习，并且必须能够泛化。
- en: '![](../Images/01_11.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/01_11.png)'
- en: The difficulty of learning from sampled feedback
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从样本反馈中学习的难度
- en: Agents that are designed to approximate policies are called *policy-based*;
    agents that are designed to approximate value functions are called *value-based*;
    agents that are designed to approximate models are called *model-based*; and agents
    that are designed to approximate both policies and value functions are called
    *actor-critic*. Agents can be designed to approximate one or more of these components.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 被设计用来近似策略的代理被称为**基于策略**的；被设计用来近似价值函数的代理被称为**基于价值**的；被设计用来近似模型的代理被称为**基于模型**的；而那些被设计用来近似策略和价值函数的代理被称为**演员-评论家**。代理可以被设计来近似一个或多个这些组件。
- en: Deep reinforcement learning agents use powerful non-linear function approximation
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度强化学习代理使用强大的非线性函数近似
- en: The agent can approximate functions using a variety of ML methods and techniques,
    from decision trees to SVMs to neural networks. However, in this book, we use
    only neural networks; this is what the “deep” part of DRL refers to, after all.
    Neural networks aren’t necessarily the best solution to every problem; neural
    networks are data hungry and challenging to interpret, and you must keep these
    facts in mind. However, neural networks are among the most potent function approximations
    available, and their performance is often the best.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 代理可以使用各种机器学习方法和技巧来近似函数，从决策树到支持向量机再到神经网络。然而，在这本书中，我们只使用神经网络；这也就是DRL中“深度”部分的含义。神经网络并不一定是每个问题的最佳解决方案；神经网络需要大量数据且难以解释，你必须牢记这些事实。然而，神经网络是可用的最强大的函数近似之一，它们的性能通常也是最好的。
- en: '![](../Images/01_12.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/01_12.png)'
- en: A simple feed-forward neural network
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的前馈神经网络
- en: '*artificial neural networks* (ANN) are multi-layered non-linear function approximators
    loosely inspired by the biological neural networks in animal brains. An ANN isn’t
    an algorithm, but a structure composed of multiple layers of mathematical transformations
    applied to input values.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**（ANN）是多层的非线性函数近似器，其灵感来源于动物大脑中的生物神经网络。ANN不是一个算法，而是一个由多个数学变换层组成的结构，这些变换应用于输入值。'
- en: From chapter 3 through chapter 7, we only deal with problems in which agents
    learn from exhaustive (as opposed to sampled) feedback. Starting with chapter
    8, we study the full DRL problem; that is, using deep neural networks so that
    agents can learn from sampled feedback. Remember, DRL agents learn from feedback
    that’s simultaneously sequential, evaluative, and sampled.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从第3章到第7章，我们只处理代理从详尽（与样本相对）反馈中学习的问题。从第8章开始，我们研究完整的DRL问题；也就是说，使用深度神经网络，使代理能够从样本反馈中学习。记住，DRL代理从同时具有序列性、评估性和样本性的反馈中学习。
- en: The past, present, and future of deep reinforcement learning
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度强化学习的过去、现在和未来
- en: History isn’t necessary to gain skills, but it can allow you to understand the
    context around a topic, which in turn can help you gain motivation, and therefore,
    skills. The history of AI and DRL should help you set expectations about the future
    of this powerful technology. At times, I feel the hype surrounding AI is actually
    productive; people get interested. But, right after that, when it’s time to put
    in work, hype no longer helps, and it’s a problem. Although I’d like to be excited
    about AI, I also need to set realistic expectations.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 历史并非获得技能的必要条件，但它可以帮助你理解一个主题的背景，这反过来又可以帮助你获得动力，从而获得技能。AI和DRL的历史应该帮助你设定对这项强大技术未来的期望。有时，我感觉围绕AI的炒作实际上是有益的；人们对此感兴趣。但是，当需要付出努力的时候，炒作就不再有帮助，这成为一个问题。虽然我很想对AI感到兴奋，但我也需要设定现实的期望。
- en: Recent history of artificial intelligence and deep reinforcement learning
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人工智能和深度强化学习的近期历史
- en: The beginnings of DRL could be traced back many years, because humans have been
    intrigued by the possibility of intelligent creatures other than ourselves since
    antiquity. But a good beginning could be Alan Turing’s work in the 1930s, 1940s,
    and 1950s that paved the way for modern computer science and AI by laying down
    critical theoretical foundations that later scientists leveraged.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习的开端可以追溯到很多年前，因为自古以来，人类就对除了我们自己之外的智能生物的可能性感到着迷。但一个良好的开端可以追溯到20世纪30年代、40年代和50年代，当时艾伦·图灵的工作为现代计算机科学和AI的发展奠定了关键的理论基础，这些基础后来被科学家们利用。
- en: 'The most well-known of these is the Turing Test, which proposes a standard
    for measuring machine intelligence: if a human interrogator is unable to distinguish
    a machine from another human on a chat Q&A session, then the computer is said
    to count as intelligent. Though rudimentary, the Turing Test allowed generations
    to wonder about the possibilities of creating smart machines by setting a goal
    that researchers could pursue.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 其中最著名的是图灵测试，它提出了一种衡量机器智能的标准：如果一名人类审问者在聊天问答环节中无法区分一台机器和另一名人类，那么这台计算机就被认为是有智能的。尽管图灵测试非常基础，但它让几代人开始思考通过设定一个研究人员可以追求的目标来创造智能机器的可能性。
- en: The formal beginnings of AI as an academic discipline can be attributed to John
    McCarthy, an influential AI researcher who made several notable contributions
    to the field. To name a few, McCarthy is credited with coining the term “artificial
    intelligence” in 1955, leading the first AI conference in 1956, inventing the
    Lisp programming language in 1958, cofounding the MIT AI Lab in 1959, and contributing
    important papers to the development of AI as a field over several decades.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能作为一门学术学科的正式开端可以归功于约翰·麦卡锡（John McCarthy），这位有影响力的AI研究人员对该领域做出了多项显著贡献。例如，麦卡锡在1955年提出了“人工智能”这一术语，1956年领导了第一次AI会议，1958年发明了Lisp编程语言，1959年共同创立了麻省理工学院AI实验室，并在几十年的时间里为AI作为一门学科的发展做出了重要贡献。
- en: Artificial intelligence winters
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人工智能寒冬
- en: All the work and progress of AI early on created a great deal of excitement,
    but there were also significant setbacks. Prominent AI researchers suggested we
    would create human-like machine intelligence within years, but this never came.
    Things got worse when a well-known researcher named James Lighthill compiled a
    report criticizing the state of academic research in AI. All of these developments
    contributed to a long period of reduced funding and interest in AI research known
    as the first *AI winter*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 早期人工智能的所有工作和进步都引起了极大的兴奋，但也伴随着重大的挫折。杰出的AI研究人员曾建议我们将在几年内创造出类似人类的机器智能，但这一目标从未实现。当一位知名研究者James
    Lighthill编制了一份批评AI学术研究现状的报告时，情况变得更糟。所有这些发展共同导致了长达一段时间的资金减少和对AI研究的兴趣降低，这一时期被称为第一次*人工智能寒冬*。
- en: 'The field continued this pattern throughout the years: researchers making progress,
    people getting overly optimistic, then overestimating—leading to reduced funding
    by government and industry partners.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 该领域在多年间持续这种模式：研究人员取得进展，人们过度乐观，然后高估——导致政府和行业合作伙伴减少资金投入。
- en: '![](../Images/01_13.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/01_13.png)'
- en: Al funding pattern through the years
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 年度AI资金模式
- en: The current state of artificial intelligence
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人工智能的当前状态
- en: 'We are likely in another highly optimistic time in AI history, so we must be
    careful. Practitioners understand that AI is a powerful tool, but certain people
    think of AI as this magic black box that can take any problem in and out comes
    the best solution ever. Nothing can be further from the truth. Other people even
    worry about AI gaining consciousness, as if that was relevant, as Edsger W. Dijkstra
    famously said: “The question of whether a computer can think is no more interesting
    than the question of whether a submarine can swim.”'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能正处于人工智能历史中的另一个高度乐观的时期，因此我们必须谨慎。从业者明白人工智能是一个强大的工具，但有些人将人工智能视为一个神奇的黑色盒子，它可以接受任何问题并给出最佳解决方案。这离事实相去甚远。其他人甚至担心人工智能会获得意识，好像这很重要，正如著名计算机科学家埃德加·W·迪杰斯特拉所说：“计算机能否思考的问题并不比潜水艇能否游泳的问题更有趣。”
- en: But, if we set aside this Hollywood-instilled vision of AI, we can allow ourselves
    to get excited about the recent progress in this field. Today, the most influential
    companies in the world make the most substantial investments to AI research. Companies
    such as Google, Facebook, Microsoft, Amazon, and Apple have invested in AI research
    and have become highly profitable thanks, in part, to AI systems. Their significant
    and steady investments have created the perfect environment for the current pace
    of AI research. Contemporary researchers have the best computing power available
    and tremendous amounts of data for their research, and teams of top researchers
    are working together, on the same problems, in the same location, at the same
    time. Current AI research has become more stable and more productive. We have
    witnessed one AI success after another, and it doesn’t seem likely to stop anytime
    soon.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们抛开好莱坞植入的人工智能愿景，我们可以让自己对这一领域的近期进展感到兴奋。如今，世界上最有影响力的公司对人工智能研究投入了最多的资金。像谷歌、Facebook、微软、亚马逊和苹果这样的公司投资于人工智能研究，并因人工智能系统而变得高度盈利。他们的重大和持续投资为当前的人工智能研究速度创造了完美的环境。当代研究人员拥有最好的计算能力和大量数据用于他们的研究，顶尖的研究团队在同一地点、同一时间共同研究相同的问题。当前的AI研究变得更加稳定和高效。我们已经见证了一个接一个的AI成功，而且似乎不会很快停止。
- en: Progress in deep reinforcement learning
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度强化学习的进展
- en: The use of artificial neural networks for RL problems started around the 1990s.
    One of the classics is the backgammon-playing computer program, *TD*-Gammon, created
    by Gerald Tesauro et al. *TD*-Gammon learned to play backgammon by learning to
    evaluate table positions on its own through RL. Even though the techniques implemented
    aren’t precisely considered DRL, *TD*-Gammon was one of the first widely reported
    success stories using ANNs to solve complex RL problems.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络在强化学习问题上的应用始于20世纪90年代。其中之一是Gerald Tesauro等人创建的经典的国际象棋程序*TD*-Gammon。*TD*-Gammon通过学习通过强化学习评估棋盘位置来学习玩国际象棋。尽管实施的技术并不精确地被认为是深度强化学习，但*TD*-Gammon是第一个广泛报道的，使用人工神经网络解决复杂强化学习问题的成功案例。
- en: '![](../Images/01_14.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/01_14.png)'
- en: TD-Gammon architecture
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: TD-Gammon架构
- en: In 2004, Andrew Ng et al. developed an autonomous helicopter that taught itself
    to fly stunts by observing hours of human-experts flights. They used a technique
    known as *inverse reinforcement learning,* in which an agent learns from expert
    demonstrations. The same year, Nate Kohl and Peter Stone used a class of DRL methods
    known as *policy-gradient methods* to develop a soccer-playing robot for the RoboCup
    tournament. They used RL to teach the agent forward motion. After only three hours
    of training, the robot achieved the fastest forward-moving speed of any robot
    with the same hardware.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在2004年，Andrew Ng等人开发了一架能够通过观察数小时人类专家飞行的飞行特技的自主直升机。他们使用了一种称为*逆强化学习*的技术，其中智能体从专家演示中学习。同年，Nate
    Kohl和Peter Stone使用一类称为*策略梯度方法*的深度强化学习方法开发了一个用于RoboCup锦标赛的足球机器人。他们使用强化学习来教智能体前进运动。经过仅三小时的训练，该机器人达到了任何具有相同硬件的机器人中最快的向前移动速度。
- en: There were other successes in the 2000s, but the field of DRL really only started
    growing after the DL field took off around 2010\. In 2013 and 2015, Mnih et al.
    published a couple of papers presenting the DQN algorithm. DQN learned to play
    Atari games from raw pixels. Using a convolutional neural network (CNN) and a
    single set of hyperparameters, DQN performed better than a professional human
    player in 22 out of 49 games.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在2000年代还有其他一些成功，但强化学习领域真正开始增长是在大约2010年深度学习领域起飞之后。2013年和2015年，Mnih等人发表了几篇论文，介绍了DQN算法。DQN通过原始像素学会了玩Atari游戏。使用卷积神经网络（CNN）和一组超参数，DQN在49场比赛中有22场的表现优于专业人类玩家。
- en: '![](../Images/01_15.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/01_15.png)'
- en: Atari DQN network architecture
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Atari DQN网络架构
- en: 'This accomplishment started a revolution in the DRL community: In 2014, Silver
    et al. released the deterministic policy gradient (DPG) algorithm, and a year
    later Lillicrap et al. improved it with deep deterministic policy gradient (DDPG).
    In 2016, Schulman et al. released trust region policy optimization (TRPO) and
    generalized advantage estimation (GAE) methods, Sergey Levine et al. published
    Guided Policy Search (GPS), and Silver et al. demoed AlphaGo. The following year,
    Silver et al. demonstrated AlphaZero. Many other algorithms were released during
    these years: double deep Q-networks (DDQN), prioritized experience replay (PER),
    proximal policy optimization (PPO), actor-critic with experience replay (ACER),
    asynchronous advantage actor-critic (A3C), advantage actor-critic (A2C), actor-critic
    using Kronecker-factored trust region (ACKTR), Rainbow, Unicorn (these are actual
    names, BTW), and so on. In 2019, Oriol Vinyals et al. showed the AlphaStar agent
    beat professional players at the game of StarCraft II. And a few months later,
    Jakub Pachocki et al. saw their team of Dota-2-playing bots, called Five, become
    the first AI to beat the world champions in an e-sports game.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这一成就引发了强化学习社区的革命：2014年，Silver等人发布了确定性策略梯度（DPG）算法，一年后Lillicrap等人通过深度确定性策略梯度（DDPG）对其进行了改进。2016年，Schulman等人发布了信任域策略优化（TRPO）和广义优势估计（GAE）方法，Sergey
    Levine等人发表了引导策略搜索（GPS），Silver等人展示了AlphaGo。次年，Silver等人展示了AlphaZero。在那几年里，还发布了许多其他算法：双深度Q网络（DDQN）、优先经验回放（PER）、近端策略优化（PPO）、带经验回放的演员-评论家（ACER）、异步优势演员-评论家（A3C）、优势演员-评论家（A2C）、使用克罗内克分解信任域的演员-评论家（ACKTR）、Rainbow、Unicorn（顺便说一句，这些都是真实名称），等等。2019年，Oriol
    Vinyals等人展示了AlphaStar智能体在星际争霸II游戏中击败了职业玩家。几个月后，Jakub Pachocki等人看到他们的Dota-2玩机器人团队，名为Five，成为第一个在电子竞技游戏中击败世界冠军的AI。
- en: Thanks to the progress in DRL, we’ve gone in two decades from solving backgammon,
    with its 10^(20) perfect-information states, to solving the game of Go, with its
    10^(170) perfect-information states, or better yet, to solving StarCraft II, with
    its 10^(270) imperfect-information states. It’s hard to conceive a better time
    to enter the field. Can you imagine what the next two decades will bring us? Will
    you be part of it? DRL is a booming field, and I expect its rate of progress to
    continue.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了强化学习的进步，我们在二十年的时间里从解决具有10^(20)完美信息状态的宾果游戏，到解决具有10^(170)完美信息状态的围棋游戏，或者更好的是，解决具有10^(270)不完美信息状态的星际争霸II游戏。这是一个难以想象的更好的进入该领域的时间。你能想象下一个二十年会给我们带来什么吗？你将成为其中的一员吗？强化学习是一个蓬勃发展的领域，我预计其进步速度将继续。
- en: '![](../Images/01_16.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/01_16.png)'
- en: 'Game of Go: enormous branching factor'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 围棋游戏：巨大的分支因子
- en: Opportunities ahead
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前方的机遇
- en: 'I believe AI is a field with unlimited potential for positive change, regardless
    of what fear-mongers say. Back in the 1750s, there was chaos due to the start
    of the industrial revolution. Powerful machines were replacing repetitive manual
    labor and mercilessly displacing humans. Everybody was concerned: machines that
    can work faster, more effectively, and more cheaply than humans? These machines
    will take all our jobs! What are we going to do for a living now? And it happened.
    But the fact is that many of these jobs were not only unfulfilling, but also dangerous.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信，无论恐惧主义者说什么，AI都是一个具有无限积极变革潜力的领域。回到1750年代，由于工业革命的开始，出现了混乱。强大的机器正在取代重复性的体力劳动，无情地取代人类。每个人都担心：这些机器能比人类工作更快、更有效、更便宜？这些机器将夺走我们所有的饭碗！我们以后靠什么生活？但事实是，许多这些工作不仅不令人满意，而且危险。
- en: One hundred years after the industrial revolution, the long-term effects of
    these changes were benefiting communities. People who usually owned only a couple
    of shirts and a pair of pants could get much more for a fraction of the cost.
    Indeed, change was difficult, but the long-term effects benefited the entire world.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 工业革命一百年后，这些变化的长期影响开始造福社区。那些通常只拥有几件衬衫和一条裤子的普通人，只需花费一小部分成本就能获得更多。的确，变革是困难的，但长期效果惠及了整个世界。
- en: The digital revolution started in the 1970s with the introduction of personal
    computers. Then, the internet changed the way we do things. Because of the internet,
    we got big data and cloud computing. ML used this fertile ground for sprouting
    into what it is today. In the next couple of decades, the changes and impact of
    AI on society may be difficult to accept at first, but the long-lasting effects
    will be far superior to any setback along the way. I expect in a few decades humans
    won’t even need to work for food, clothing, or shelter because these things will
    be automatically produced by AI. We’ll thrive with abundance.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 数字革命始于20世纪70年代，随着个人电脑的引入。随后，互联网改变了我们的做事方式。由于互联网，我们得到了大数据和云计算。机器学习利用这片肥沃的土壤茁壮成长，成为今天的模样。在接下来的几十年里，AI对社会的影响和变化可能一开始难以接受，但长期效果将远远优于途中的任何挫折。我预计在几十年后，人类甚至不需要为食物、衣物或住所而工作，因为这些都是由AI自动生产的。我们将因富足而繁荣。
- en: '![](../Images/01_17.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/01_17.png)'
- en: Workforce revolutions
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 劳动力革命
- en: As we continue to push the intelligence of machines to higher levels, certain
    AI researchers think we might find an AI with intelligence superior to our own.
    At this point, we unlock a phenomenon known as the *singularity*; an AI more intelligent
    than humans allows for the improvement of AI at a much faster pace, given that
    the self-improvement cycle no longer has the bottleneck, namely, humans. But we
    must be prudent, because this is more of an ideal than a practical aspect to worry
    about.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们继续提升机器的智能水平，某些AI研究人员认为我们可能会发现一个比我们自身更聪明的AI。在这个时候，我们将解锁一个被称为*奇点*的现象；一个比人类更聪明的AI能够以更快的速度提升AI，因为自我改进的循环不再有瓶颈，即人类。但我们必须谨慎，因为这更多的是一个理想而非实际需要担忧的方面。
- en: '![](../Images/01_18.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/01_18.png)'
- en: Singularity could be a few decades away
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 奇点可能就在几十年之后
- en: While one must be always aware of the implications of AI and strive for AI safety,
    the singularity isn’t an issue today. On the other hand, many issues exist with
    the current state of DRL, as you’ll see in this book. These issues make better
    use of our time.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们必须始终意识到AI的影响并努力追求AI安全，但奇点目前并不是一个问题。另一方面，正如你在本书中将会看到的，当前深度强化学习存在许多问题。这些问题使我们能够更好地利用时间。
- en: The suitability of deep reinforcement learning
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度强化学习的适用性
- en: You could formulate any ML problem as a DRL problem, but this isn’t always a
    good idea for multiple reasons. You should know the pros and cons of using DRL
    in general, and you should be able to identify what kinds of problems and settings
    DRL is good and not so good for.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将任何机器学习问题表述为一个深度强化学习问题，但这并不总是一个好的主意，有多个原因。你应该了解使用深度强化学习的一般优缺点，并且能够识别哪些问题和设置适合深度强化学习，哪些则不太适合。
- en: What are the pros and cons?
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有哪些优缺点？
- en: 'Beyond a technological comparison, I’d like you to think about the inherent
    advantages and disadvantages of using DRL for your next project. You’ll see that
    each of the points highlighted can be either a pro or a con depending on what
    kind of problem you’re trying to solve. For instance, this field is about letting
    the machine take control. Is this good or bad? Are you okay with letting the computer
    make the decisions for you? There’s a reason why DRL research environments of
    choice are games: it could be costly and dangerous to have agents training directly
    in the real world. Can you imagine a self-driving car agent learning not to crash
    by crashing? In DRL, the agents will have to make mistakes. Can you afford that?
    Are you willing to risk the negative consequences—actual harm—to humans? Considered
    these questions before starting your next DRL project.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 除去技术比较之外，我希望你能思考一下使用深度强化学习（DRL）在你下一个项目中的固有优势和劣势。你会发现，每个被强调的点都可能是一个优点或缺点，这取决于你试图解决的问题类型。例如，这个领域是关于让机器接管控制。这是好是坏？你能否接受让计算机为你做决策？选择游戏作为DRL研究环境的原因之一是：让代理在现实世界中直接训练可能会代价高昂且危险。你能想象一个自动驾驶汽车代理通过碰撞来学习不发生碰撞吗？在DRL中，代理将不得不犯错误。你能承担这种代价吗？你愿意承担对人类造成实际伤害的负面后果的风险吗？在你开始下一个DRL项目之前，考虑这些问题。
- en: '![](../Images/01_19.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/01_19.png)'
- en: Deep reinforcement learning agents will explore! Can you afford mistakes?
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习代理将会探索！你能承担错误吗？
- en: You’ll also need to consider how your agent will explore its environment. For
    instance, most value-based methods explore by randomly selecting an action. But
    other methods can have more strategic exploration strategies. Now, there are pros
    and cons to each, and this is a trade-off you’ll have to become familiar with.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要考虑你的代理将如何探索其环境。例如，大多数基于价值的方 法通过随机选择动作来探索。但其他方法可以拥有更具有战略性的探索策略。现在，每种方法都有其优缺点，这是你必须熟悉的权衡。
- en: Finally, training from scratch every time can be daunting, time consuming, and
    resource intensive. However, there are a couple of areas that study how to bootstrap
    previously acquired knowledge. First, there’s *transfer learning***,** which is
    about transferring knowledge gained in tasks to new ones. For example, if you
    want to teach a robot to use a hammer and a screwdriver, you could reuse low-level
    actions learned on the “pick up the hammer” task and apply this knowledge to start
    learning the “pick up the screwdriver” task. This should make intuitive sense
    to you, because humans don’t have to relearn low-level motions each time they
    learn a new task. Humans seem to form hierarchies of actions as we learn. The
    field of *Hierarchical reinforcement learning* tries to replicate this in DRL
    agents.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，每次从头开始训练可能会令人畏惧、耗时且资源密集。然而，有几个领域研究如何利用先前获得的知识。首先，有**迁移学习**，这是关于将任务中获得的知 识转移到新的任务上。例如，如果你想教机器人使用锤子和螺丝刀，你可以重用“拿起锤子”任务中学习到的低级动作，并将这些知识应用到开始学习“拿起螺丝刀”的任务中。这对你来说应该是有直觉意义的，因为人类在学习新任务时不必每次都重新学习低级动作。人类似乎在学习过程中形成了动作的层次结构。*分层强化学习*领域试图在DRL代理中复制这一点。
- en: Deep reinforcement learning’s strengths
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度强化学习的优势
- en: DRL is about mastering specific tasks. Unlike SL, in which generalization is
    the goal, RL is good at concrete, well-specified tasks. For instance, each Atari
    game has a particular task. DRL agents aren’t good at generalizing behavior across
    different tasks; it’s not true that because you train an agent to play Pong, this
    agent can also play Breakout. And if you naively try to teach your agent Pong
    and Breakout simultaneously, you’ll likely end up with an agent that isn’t good
    at either. SL, on the other hand, is pretty good a classifying multiple objects
    at once. The point is the strength of DRL is well-defined single tasks.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: DRL是关于掌握特定任务。与旨在实现泛化的SL不同，RL擅长具体、明确指定的任务。例如，每个Atari游戏都有一个特定的任务。DRL代理不擅长在不同任务之间泛化行为；并不是因为训练了一个代理来玩Pong，这个代理也能玩Breakout。如果你天真地试图同时教你的代理Pong和Breakout，你很可能会得到一个在两者上都不擅长的代理。另一方面，SL在同时分类多个对象方面相当出色。重点是DRL的优势在于定义明确的单一任务。
- en: In DRL, we use generalization techniques to learn simple skills directly from
    raw sensory input. The performance of generalization techniques, new tips, and
    tricks on training deeper networks, and so on, are some of the main improvements
    we’ve seen in recent years. Lucky for us, most DL advancements directly enable
    new research paths in DRL.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在DRL中，我们使用泛化技术直接从原始感官输入中学习简单技能。泛化技术的性能、新的技巧和技巧、以及训练更深网络等，是我们近年来看到的一些主要改进。幸运的是，大多数DL的进步直接为DRL开辟了新的研究途径。
- en: Deep reinforcement learning’s weaknesses
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度强化学习的弱点
- en: Of course, DRL isn’t perfect. One of the most significant issues you’ll find
    is that in most problems, agents need millions of samples to learn well-performing
    policies. Humans, on the other hand, can learn from a few interactions. Sample
    efficiency is probably one of the top areas of DRL that could use improvements.
    We’ll touch on this topic in several chapters because it’s a crucial one.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，DRL并不完美。你将发现的最显著问题是，在大多数问题中，代理需要数百万个样本才能学习到良好的策略。另一方面，人类可以从几次交互中学习。样本效率可能是DRL需要改进的顶级领域之一。我们将在几个章节中涉及这个话题，因为它是一个关键问题。
- en: '![](../Images/01_20.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/01_20.png)'
- en: Deep reinforcement learning agents need lots of interaction samples!
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习代理需要大量的交互样本！
- en: Another issue with DRL is with reward functions and understanding the meaning
    of rewards. If a human expert will define the rewards the agent is trying to maximize,
    does that mean that we’re somewhat “supervising” this agent? And is this something
    good? Should the reward be as dense as possible, which makes learning faster,
    or as sparse as possible, which makes the solutions more exciting and unique?
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: DRL的另一个问题是奖励函数和了解奖励的意义。如果人类专家将为代理定义其试图最大化的奖励，这意味着我们某种程度上“监督”了这个代理吗？这是好事吗？奖励应该尽可能密集，使学习更快，还是尽可能稀疏，使解决方案更令人兴奋和独特？
- en: We, as humans, don’t seem to have explicitly defined rewards. Often, the same
    person can see an event as positive or negative by simply changing their perspective.
    Additionally, a reward function for a task such as walking isn’t straightforward
    to design. Is it the forward motion that we should target, or is it not falling?
    What is the “perfect” reward function for a human walk?!
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们人类似乎并没有明确地定义奖励。通常，同一个人可以通过改变他们的观点，将一个事件视为积极或消极。此外，为像走路这样的任务设计奖励函数并不简单。我们应该针对的是前进的运动，还是不是跌倒？什么是人类行走的“完美”奖励函数？！
- en: There’s ongoing interesting research on reward signals. One I’m particularly
    interested in is called *intrinsic motivation*. Intrinsic motivation allows the
    agent to explore new actions just for the sake of it, out of curiosity. Agents
    that use intrinsic motivation show improved learning performance in environments
    with sparse rewards, which means we get to keep exciting and unique solutions.
    The point is if you’re trying to solve a task that hasn’t been modeled or doesn’t
    have a distinct reward function, you’ll face challenges.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 关于奖励信号的研究正在进行中，其中一项我特别感兴趣的是称为*内在动机*。内在动机允许代理仅仅出于好奇探索新的动作。使用内在动机的代理在稀疏奖励的环境中表现出改进的学习性能，这意味着我们可以保持令人兴奋和独特的解决方案。关键是如果你试图解决一个尚未建模或没有明确奖励函数的任务，你会面临挑战。
- en: Setting clear two-way expectations
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 明确的双向期望
- en: Let’s now touch on another important point going forward. What to expect? Honestly,
    to me, this is very important. First, I want you to know what to expect from the
    book so there are no surprises later on. I don’t want people to think that from
    this book, they’ll be able to come up with a trading agent that will make them
    rich. Sorry, I wouldn’t be writing this book if it was that simple. I also expect
    that people who are looking to learn put in the necessary work. The fact is that
    learning will come from the combination of me putting in the effort to make concepts
    understandable and you putting in the effort to understand them. I did put in
    the effort. But, if you decide to skip a box you didn’t think was necessary, we
    both lose.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们接下来谈谈另一个重要的观点。接下来可以期待什么？坦白说，对我来说，这非常重要。首先，我想让你知道你可以从这本书中期待什么，这样就不会有后续的惊喜。我不想让人们认为，通过这本书，他们能够想出一个能让他们致富的交易代理。抱歉，如果这很简单，我就不会写这本书。我也期待那些想要学习的人投入必要的工作。事实上，学习将来自于我努力使概念易于理解，以及你努力去理解它们。我确实付出了努力。但是，如果你决定跳过一个你认为不必要的框，我们都会失去。
- en: What to expect from the book?
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从这本书中可以期待什么？
- en: My goal for this book is to take you, an ML enthusiast, from no prior DRL experience
    to capable of developing state-of-the-art DRL algorithms. For this, the book is
    organized into roughly two parts. In chapters 3 through 7, you learn about agents
    that can learn from sequential and evaluative feedback, first in isolation, and
    then in interplay. In chapters 8 through 12, you dive into core DRL algorithms,
    methods, and techniques. Chapters 1 and 2 are about introductory concepts applicable
    to DRL in general, and chapter 13 has concluding remarks.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我写这本书的目标是带你，一个机器学习爱好者，从没有任何深度强化学习经验到能够开发最先进的深度强化学习算法。为此，本书分为大约两部分。在第3章到第7章，你将学习能够从顺序和评估反馈中学习的代理，首先在孤立状态下，然后在相互作用中。在第8章到第12章，你将深入研究核心的深度强化学习算法、方法和技巧。第1章和第2章是适用于深度强化学习的一般概念介绍，第13章有总结性评论。
- en: My goal for the first part (chapters 3 through 7) is for you to understand “tabular”
    RL. That is, RL problems that can be exhaustively sampled, problems in which there’s
    no need for neural networks or function approximation of any kind. Chapter 3 is
    about the sequential aspect of RL and the temporal credit assignment problem.
    Then, we’ll study, also in isolation, the challenge of learning from evaluative
    feedback and the exploration versus exploitation trade-off in chapter 4\. Last,
    you learn about methods that can deal with these two challenges simultaneously.
    In chapter 5, you study agents that learn to estimate the results of fixed behavior.
    Chapter 6 deals with learning to improve behavior, and chapter 7 shows you techniques
    that make RL more effective and efficient.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我对第一部分（第3章到第7章）的目标是让你理解“表格”强化学习。也就是说，可以彻底抽样的强化学习问题，其中不需要神经网络或任何类型的函数逼近。第3章是关于强化学习的顺序方面和时间信用分配问题。然后，我们将在第4章中单独研究从评估反馈中学习以及探索与利用之间的权衡。最后，你将学习可以同时处理这两个挑战的方法。在第5章，你研究学习估计固定行为结果的代理。第6章处理学习改进行为，第7章展示了使强化学习更有效率和高效的技巧。
- en: My goal for the second part (chapters 8 through 12) is for you to grasp the
    details of core DRL algorithms. We dive deep into the details; you can be sure
    of that. You learn about the many different types of agents from value- and policy-based
    to actor-critic methods. In chapters 8 through 10, we go deep into value-based
    DRL. In chapter 11, you learn about policy-based DRL and actor-critic, and chapter
    12 is about deterministic policy gradient (DPG) methods, soft actor-critic (SAC)
    and proximal policy optimization (PPO) methods.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我对第二部分（第8章到第12章）的目标是让你掌握核心深度强化学习算法的细节。我们将深入探讨细节；你可以确信这一点。你将了解许多不同类型的代理，从基于价值和策略的到演员-评论家方法。在第8章到第10章，我们将深入研究基于价值的深度强化学习。在第11章，你将学习基于策略的深度强化学习和演员-评论家，第12章是关于确定性策略梯度（DPG）方法、软演员-评论家（SAC）和近端策略优化（PPO）方法。
- en: The examples in these chapters are repeated throughout agents of the same type
    to make comparing and contrasting agents more accessible. You still explore fundamentally
    different kinds of problems, from small, continuous to image-based state spaces,
    and from discrete to continuous action spaces. But, the book’s focus isn’t about
    modeling problems, which is a skill of its own; instead, the focus is about solving
    already modeled environments.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这些章节中的示例在相同类型的代理之间重复出现，以便更方便地比较和对比代理。你仍然探索根本不同类型的问题，从小型连续到基于图像的状态空间，以及从离散到连续的动作空间。但，本书的重点不是关于建模问题，这是它自己的技能；相反，重点是解决已经建模的环境。
- en: '![](../Images/01_21.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/01_21.png)'
- en: Comparison of different algorithmic approaches to deep reinforcement learning
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习中不同算法方法的比较
- en: How to get the most out of this book
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何充分利用这本书
- en: There are a few things you need to bring to the table to come out grokking deep
    reinforcement learning. You need to bring a little prior basic knowledge of ML
    and DL. You need to be comfortable with Python code and simple math. And most
    importantly, you must be willing to put in the work.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了深入理解强化学习，你需要带来一些东西。你需要具备一些机器学习和深度学习的基本知识。你需要熟悉Python代码和简单的数学。最重要的是，你必须愿意投入努力。
- en: I assume that the reader has a solid basic understanding of ML. You should know
    what ML is beyond what’s covered in this chapter; you should know how to train
    simple SL models, perhaps the Iris or Titanic datasets; you should be familiar
    with DL concepts such as tensors and matrices; and you should have trained at
    least one DL model, say a convolutional neural network (CNN) on the MNIST dataset.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设读者对机器学习（ML）有扎实的理论基础。你应该知道机器学习是什么，而不仅仅是本章所涵盖的内容；你应该知道如何训练简单的监督学习（SL）模型，比如Iris或Titanic数据集；你应该熟悉深度学习（DL）的概念，如张量和矩阵；你应该至少训练过一个深度学习模型，比如在MNIST数据集上训练一个卷积神经网络（CNN）。
- en: This book is focused on DRL topics, and there’s no DL in isolation. There are
    many useful resources out there that you can leverage. But, again, you need a
    basic understanding; If you’ve trained a CNN before, then you’re fine. Otherwise,
    I highly recommend you follow a couple of DL tutorials before starting the second
    part of the book.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本书专注于强化学习（DRL）主题，其中并没有孤立的学习深度学习（DL）。外面有许多有用的资源你可以利用。但再次强调，你需要有基本的理解；如果你之前训练过卷积神经网络（CNN），那么你没问题。否则，我强烈建议你在开始阅读本书的第二部分之前，先跟随几个深度学习（DL）教程。
- en: Another assumption I’m making is that the reader is comfortable with Python
    code. Python is a somewhat clear programming language that can be straightforward
    to understand, and people not familiar with it often get something out of merely
    reading it. Now, my point is that you should be *comfortable* with it, willing
    and looking forward to reading the code. If you don’t read the code, then you’ll
    miss out on a lot.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我还假设读者对Python代码比较熟悉。Python是一种相对清晰的编程语言，易于理解，不熟悉它的人往往只需阅读就能有所收获。现在，我的观点是，你应该感到*舒适*，愿意并期待阅读代码。如果你不阅读代码，你将错过很多东西。
- en: Likewise, there are many math equations in this book, and that’s a good thing.
    Math is the perfect language, and there’s nothing that can replace it. However,
    I’m asking people to be comfortable with math, willing to read, and nothing else.
    The equations I show are heavily annotated so that people “not into math” can
    still take advantage of the resources.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这本书中有很多数学公式，这也是一件好事。数学是完美的语言，没有任何东西可以取代它。然而，我要求人们能够适应数学，愿意阅读，仅此而已。我展示的公式都有大量的注释，以便那些“不热衷于数学”的人也能利用这些资源。
- en: Finally, I’m assuming you’re willing to put in the work. By that I mean you
    really want to learn DRL. If you decide to skip the math boxes, or the Python
    snippets, or a section, or one page, or chapter, or whatever, you’ll miss out
    on a lot of relevant information. To get the most out of this book, I recommend
    you read the entire book front to back. Because of the different format, figures
    and sidebars are part of the main narrative in this book.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我假设你愿意付出努力。我的意思是，你真的想学习深度强化学习（DRL）。如果你决定跳过数学框、Python代码片段、某个部分、一页、一章，或者任何其他内容，你将错过很多相关信息。为了最大限度地利用这本书，我建议你从头到尾阅读整本书。由于格式不同，图表和侧边栏是本书主要叙述的一部分。
- en: Also, make sure you run the book source code (the next section provides more
    details on how to do this), and play around and extend the code you find most
    interesting.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请确保运行本书的源代码（下一节将提供更多关于如何操作的细节），并尝试扩展你发现最有趣的代码。
- en: Deep reinforcement learning development environment
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度强化学习开发环境
- en: Along with this book, you’re provided with a fully tested environment and code
    to reproduce my results. I created a Docker image and several Jupyter Notebooks
    so that you don’t have to mess around with installing packages and configuring
    software, or copying and pasting code. The only prerequisite is Docker. Please,
    go ahead and follow the directions at [https://github.com/mimoralea/gdrl](https://github.com/mimoralea/gdrl)
    on running the code. It’s pretty straightforward.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这本书，你还提供了一个完全测试的环境和代码来重现我的结果。我创建了一个Docker镜像和几个Jupyter笔记本，这样你就不必费心安装包和配置软件，或者复制粘贴代码。唯一的前提是Docker。请按照[https://github.com/mimoralea/gdrl](https://github.com/mimoralea/gdrl)上的说明运行代码。这相当直接。
- en: The code is written in Python, and I make heavy use of NumPy and PyTorch. I
    chose PyTorch, instead of Keras, or TensorFlow, because I found PyTorch to be
    a “Pythonic” library. Using PyTorch feels natural if you have used NumPy, unlike
    TensorFlow, for instance, which feels like a whole new programming paradigm. Now,
    my intention is not to start a “PyTorch versus TensorFlow” debate. But, in my
    experience from using both libraries, PyTorch is a library much better suited
    for research and teaching.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 代码是用Python编写的，我大量使用了NumPy和PyTorch。我选择PyTorch而不是Keras或TensorFlow，因为我发现PyTorch是一个“Pythonic”库。如果你使用过NumPy，使用PyTorch会感觉自然，而像TensorFlow这样的库则感觉像是一个全新的编程范式。现在，我的意图并不是开始一场“PyTorch与TensorFlow”的辩论。但根据我使用这两个库的经验，PyTorch是一个更适合研究和教学的库。
- en: DRL is about algorithms, methods, techniques, tricks, and so on, so it’s pointless
    for us to rewrite a NumPy or a PyTorch library. But, also, in this book, we write
    DRL algorithms from scratch; I’m not teaching you how to use a DRL library, such
    as Keras-RL, or Baselines, or RLlib. I want you to learn DRL, and therefore we
    write DRL code. In the years that I’ve been teaching RL, I’ve noticed those who
    write RL code are more likely to understand RL. Now, this isn’t a book on PyTorch
    either; there’s no separate PyTorch review or anything like that, just PyTorch
    code that I explain as we move along. If you’re somewhat familiar with DL concepts,
    you’ll be able to follow along with the PyTorch code I use in this book. Don’t
    worry, you don’t need a separate PyTorch resource before you get to this book.
    I explain everything in detail as we move along.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）涉及算法、方法、技术、技巧等等，因此我们重写NumPy或PyTorch库是没有意义的。但在这本书中，我们从头开始编写DRL算法；我不是教你如何使用深度强化学习库，如Keras-RL、Baselines或RLlib。我想让你学习深度强化学习，因此我们编写了DRL代码。在我教授强化学习的这些年里，我发现编写强化学习代码的人更有可能理解强化学习。现在，这本书也不是关于PyTorch的；没有单独的PyTorch回顾或类似的内容，只是随着我们的进展，我会解释我使用的PyTorch代码。如果你对深度学习概念有些熟悉，你将能够跟随这本书中使用的PyTorch代码。不用担心，在你到达这本书之前，你不需要单独的PyTorch资源。随着我们的进展，我会详细解释一切。
- en: As for the environments we use for training the agents, we use the popular OpenAI
    Gym package and a few other libraries that I developed for this book. But we’re
    also not going into the ins and outs of Gym. Just know that Gym is a library that
    provides environments for training RL agents. Beyond that, remember our focus
    is the RL algorithms, the solutions, not the environments, or modeling problems,
    which, needless to say, are also critical.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 至于我们用于训练代理的环境，我们使用流行的OpenAI Gym软件包以及我为这本书开发的一些其他库。但我们也不会深入探讨Gym的细节。只需知道Gym是一个提供强化学习代理训练环境的库。除此之外，记住我们的重点是强化学习算法，解决方案，而不是环境或建模问题，这些不用说也是至关重要的。
- en: Since you should be familiar with DL, I presume you know what a graphics processing
    unit (GPU) is. DRL architectures don’t need the level of computation commonly
    seen on DL models. For this reason, the use of a GPU, while a good thing, is not
    required. Conversely, unlike DL models, some DRL agents make heavy use of a central
    processing unit (CPU) and thread count. If you’re planning on investing in a machine,
    make sure to account for CPU power (well, technically, number of cores, not speed)
    as well. As you’ll see later, certain algorithms massively parallelize processing,
    and in those cases, it’s the CPU that becomes the bottleneck, not the GPU. However,
    the code runs fine in the container regardless of your CPU or GPU. But, if your
    hardware is severely limited, I recommend checking out cloud platforms. I’ve seen
    services, such as Google Colab, that offer DL hardware for free.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你应该熟悉深度学习（DL），我假设你知道什么是图形处理单元（GPU）。深度强化学习（DRL）架构不需要像深度学习模型那样常见的计算水平。因此，虽然使用GPU是个好主意，但并非必需。相反，与深度学习模型不同，一些深度强化学习代理（agent）大量使用中央处理单元（CPU）和线程数。如果你打算投资购买机器，确保考虑到CPU的功率（好吧，技术上讲，是核心数，而不是速度）。正如你稍后将会看到的，某些算法会大规模并行化处理，在这种情况下，成为瓶颈的是CPU，而不是GPU。然而，代码在容器中运行良好，无论你的CPU或GPU如何。但是，如果你的硬件非常有限，我建议检查一下云平台。我见过一些服务，例如Google
    Colab，提供免费的深度学习硬件。
- en: Summary
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep reinforcement learning is challenging because agents must learn from feedback
    that is simultaneously sequential, evaluative, and sampled. Learning from sequential
    feedback forces the agent to learn how to balance immediate and long-term goals.
    Learning from evaluative feedback makes the agent learn to balance the gathering
    and utilization of information. Learning from sampled feedback forces the agent
    to generalize from old to new experiences.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习具有挑战性，因为智能体必须从同时具有顺序性、评价性和样本性的反馈中学习。从顺序性反馈中学习迫使智能体学习如何平衡短期和长期目标。从评价性反馈中学习使智能体学会平衡信息的收集和利用。从样本性反馈中学习迫使智能体从旧经验推广到新经验。
- en: Artificial intelligence, the main field of computer science into which reinforcement
    learning falls, is a discipline concerned with creating computer programs that
    display human-like intelligence. This goal is shared across many other disciplines,
    such as control theory and operations research. Machine learning is one of the
    most popular and successful approaches to artificial intelligence. Reinforcement
    learning is one of the three branches of machine learning, along with supervised
    learning, and unsupervised learning. Deep learning, an approach to machine learning,
    isn’t tied to any specific branch, but its power helps advance the entire machine
    learning community.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能，强化学习所属的计算机科学主要领域，是一门致力于创建显示类似人类智能的计算机程序的学科。这个目标与其他许多学科，如控制理论和运筹学，是共享的。机器学习是人工智能中最受欢迎和最成功的方法之一。强化学习是机器学习的三个分支之一，与监督学习和无监督学习并列。深度学习作为一种机器学习方法，与任何特定分支无关，但其力量有助于推动整个机器学习社区的发展。
- en: Deep reinforcement learning is the use of multiple layers of powerful function
    approximators known as neural networks (deep learning) to solve complex sequential
    decision-making problems under uncertainty. Deep reinforcement learning has performed
    well in many control problems, but, nevertheless, it’s essential to have in mind
    that releasing human control for critical decision making shouldn’t be taken lightly.
    Several of the core needs in deep reinforcement learning are algorithms with better
    sample complexity, better-performing exploration strategies, and safe algorithms.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习是使用多层强大的函数逼近器，即神经网络（深度学习），在不确定性下解决复杂顺序决策问题的方法。深度强化学习在许多控制问题中表现良好，但无论如何，我们必须牢记，对于关键决策，释放人类控制不应被轻视。深度强化学习的核心需求包括具有更好样本复杂度的算法、性能更好的探索策略和安全的算法。
- en: Still, the future of deep reinforcement learning is bright, and there are perhaps
    dangers ahead as the technology matures, but more importantly, there’s potential
    in this field, and you should feel excited and compelled to bring your best and
    embark on this journey. The opportunity to be part of a potential change this
    big happens only every few generations. You should be glad you’re living during
    these times. Now, let’s be part of it.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，深度强化学习的未来光明，随着技术的成熟，也许会面临一些危险，但更重要的是，这个领域具有潜力，你应该感到兴奋并感到有必要发挥你的最佳水平，踏上这段旅程。成为这样一场巨大变革的一部分的机会，每隔几代人才能出现一次。你应该为生活在这样的时代而感到高兴。现在，让我们成为其中的一员。
- en: By now, you
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你已经
- en: Understand what deep reinforcement learning is and how it compares with other
    machine learning approaches
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解了深度强化学习是什么以及它与其他机器学习方法相比如何
- en: Are aware of the recent progress in the field of deep reinforcement learning,
    and intuitively understand that it has the potential to be applied to a wide variety
    of problems
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 意识到深度强化学习领域的最新进展，并直观地理解它具有应用于广泛问题的潜力
- en: Have a sense as to what to expect from this book, and how to get the most out
    of it
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对这本书的内容有所了解，并知道如何从中获得最大收益
- en: '| ![](../Images/icons_Tweet.png) | Tweetable FeatWork on your own and share
    your findings |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| ![推文图标](../Images/icons_Tweet.png) | 在自己的工作上努力并分享你的发现 |'
- en: '|  | At the end of every chapter, I’ll give several ideas on how to take what
    you’ve learned to the next level. If you’d like, share your results with the rest
    of the world, and make sure to check out what others have done, too. It’s a win-win
    situation, and hopefully, you’ll take advantage of it.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 每章结束时，我会提供几个想法，告诉你如何将所学内容提升到下一个层次。如果你愿意，可以将你的结果与世界分享，并确保查看其他人所做的事情。这是一个双赢的局面，希望你能充分利用它。'
- en: '**#gdrl_ch01_tf01:** Supervised, unsupervised, and reinforcement learning are
    essential machine learning branches. And while it’s crucial to know the differences,
    it’s equally important to know the similarities. Write a post analyzing how these
    different approaches compare and how they could be used together to solve an AI
    problem. All branches are going after the same goal: to create artificial general
    intelligence, it’s vital for all of us to better understand how to use the tools
    available.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch01_tf01:** 监督学习、无监督学习和强化学习是机器学习的三大重要分支。虽然了解它们之间的区别至关重要，但同样重要的是了解它们之间的相似之处。写一篇分析这些不同方法如何比较以及如何将它们结合起来解决人工智能问题的文章。所有分支都追求同一个目标：创造通用人工智能，对我们所有人来说，更好地理解如何使用可用的工具至关重要。'
- en: '**#gdrl_ch01_tf02:** I wouldn’t be surprised if you don’t have a machine learning
    or computer science background, yet are still interested in what this book has
    to offer. One essential contribution is to post resources from other fields that
    study decision-making. Do you have an operations research background? Psychology,
    philosophy, or neuroscience background? Control theory? Economics? How about you
    create a list of resources, blog posts, YouTube videos, books, or any other medium
    and share it with the rest of us also studying decision-making?'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch01_tf02:** 如果你没有机器学习或计算机科学背景，但对这本书提供的内容感兴趣，这并不会让我感到惊讶。一个重要的贡献是发布来自研究决策的其他领域的资源。你有运筹学背景吗？心理学、哲学或神经科学背景？控制理论？经济学？你为什么不创建一个资源列表、博客文章、YouTube
    视频书籍或其他任何形式的列表，并与我们这些也在研究决策的人分享呢？'
- en: '**#gdrl_ch01_tf03:** Part of the text in this chapter has information that
    could be better explained through graphics, tables, and other forms. For instance,
    I talked about the different types of reinforcement learning agents (value-based,
    policy-based, actor-critic, model-based, gradient-free). Why don’t you grab text
    that’s dense, distill the knowledge, and share your summary with the world?'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch01_tf03:** 本章的部分文本通过图形、表格和其他形式可以更好地解释。例如，我谈到了不同类型的强化学习代理（基于价值的、基于策略的、演员-评论家、基于模型的、无梯度）。你为什么不抓取密集的文本，提炼知识，并将你的总结与世界分享呢？'
- en: '**#gdrl_ch01_tf04:** In every chapter, I’m using the final hashtag as a catchall
    hashtag. Feel free to use this one to discuss anything else that you worked on
    relevant to this chapter. There’s no more exciting homework than that which you
    create for yourself. Make sure to share what you set yourself to investigate and
    your results.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch01_tf04:** 在每一章中，我都在使用最后一个标签作为通用的标签。请随意使用这个标签来讨论与本章相关的任何其他内容。没有比为自己创造作业更令人兴奋的了。确保分享你打算调查的内容和你的结果。'
- en: 'Write a tweet with your findings, tag me @mimoralea (I’ll retweet), and use
    the particular hashtag from this list to help interested folks find your results.
    There are no right or wrong results; you share your findings and check others’
    findings. Take advantage of this to socialize, contribute, and get yourself out
    there! We’re waiting for you!Here is a tweet example:“Hey, @mimoralea. I created
    a blog post with a list of resources to study deep reinforcement learning. Check
    it out at <link>. #gdrl_ch01_tf01”I’ll make sure to retweet and help others find
    your work. |'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '用你的发现写一条推文，@mimoralea（我会转发），并使用此列表中的特定标签来帮助感兴趣的人找到你的结果。没有正确或错误的结果；你分享你的发现并检查他人的发现。利用这个机会社交、贡献并让自己脱颖而出！我们正在等待你！以下是一条推文示例：“嘿，@mimoralea。我创建了一篇博客文章，列出了学习深度强化学习的资源。查看链接：<link>
    #gdrl_ch01_tf01”。我会确保转发并帮助他人找到你的作品。|'
