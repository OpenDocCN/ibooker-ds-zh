- en: Chapter 7\. Train Your First Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。训练您的第一个模型
- en: In the previous chapter, we used SageMaker Processing Jobs to transform a raw
    dataset into machine-usable features through the “feature engineering” process.
    In this chapter, we use these features to train a custom review classifier using
    TensorFlow, PyTorch, [BERT](https://oreil.ly/hmyQz), and SageMaker to classify
    reviews “in the wild” from social channels, partner websites, etc. We even show
    how to train a BERT model with Java!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们使用SageMaker处理作业通过“特征工程”过程将原始数据集转换为可用于机器的特征。在本章中，我们使用这些特征使用TensorFlow、PyTorch、[BERT](https://oreil.ly/hmyQz)和SageMaker来训练自定义评论分类器，用于从社交渠道、合作伙伴网站等“野外”分类评论。我们甚至展示了如何使用Java训练BERT模型！
- en: Along the way, we explain key concepts like the Transformers architecture, BERT,
    and fine-tuning pre-trained models. We also describe the various training options
    provided by SageMaker, including built-in algorithms and “bring-your-own” options.
    Next, we discuss the SageMaker infrastructure, including containers, networking,
    and security. We then train, evaluate, and profile our models with SageMaker.
    Profiling helps us debug our models, reduce training time, and reduce cost. Lastly,
    we provide tips to further reduce cost and increase performance when developing
    models with SageMaker.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们解释了诸如变压器架构、BERT和微调预训练模型等关键概念。我们还描述了SageMaker提供的各种培训选项，包括内置算法和“自带”选项。接下来，我们讨论SageMaker基础设施，包括容器、网络和安全性。然后，我们使用SageMaker训练、评估和配置我们的模型。性能分析帮助我们调试模型、减少训练时间和成本。最后，我们提供了进一步减少成本和提高模型性能的技巧。
- en: Understand the SageMaker Infrastructure
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解SageMaker基础设施
- en: Largely container based, SageMaker manages the infrastructure and helps us focus
    on our specific machine learning task. Out of the box, we can directly leverage
    one of many built-in algorithms that cover use cases such as natural language
    processing (NLP), classification, regression, computer vision, and reinforcement
    learning. In addition to these built-in algorithms, SageMaker also offers pre-built
    containers for many popular AI and machine learning frameworks, such as TensorFlow,
    PyTorch, Apache MXNet, XGBoost, and scikit-learn. Finally, we can also provide
    our own Docker containers with the libraries and frameworks of our choice. In
    this section, we go into more detail about the SageMaker infrastructure, including
    environment variables, S3 locations, security, and encryption.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分基于容器的，SageMaker 管理基础架构并帮助我们专注于特定的机器学习任务。开箱即用，我们可以直接利用许多内置算法，涵盖自然语言处理（NLP）、分类、回归、计算机视觉和强化学习等用例。除了这些内置算法外，SageMaker
    还为许多流行的人工智能和机器学习框架（如TensorFlow、PyTorch、Apache MXNet、XGBoost和scikit-learn）提供预构建的容器。最后，我们还可以使用自己选择的库和框架提供自己的Docker容器。在本节中，我们详细讨论了SageMaker基础设施，包括环境变量、S3位置、安全性和加密。
- en: We can choose to train on a single instance or on a distributed cluster of instances.
    Amazon SageMaker removes the burden of managing the underlying infrastructure
    and handles the undifferentiated heavy lifting for us.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择在单个实例上或在分布式实例集群上进行训练。Amazon SageMaker消除了管理底层基础设施的负担，并为我们处理了不同的重型工作。
- en: Introduction to SageMaker Containers
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍SageMaker容器
- en: When running a training job, SageMaker reads input data from Amazon S3, uses
    that data to train a model, and finally writes the model artifacts back to Amazon
    S3\. [Figure 7-1](#sagemaker_containerscomma_inputscomma_a) illustrates how SageMaker
    uses containers for training and inference. Starting from the bottom left, training
    data from S3 is made available to the Model Training instance container, which
    is pulled from Amazon Elastic Container Registry. The training job persists model
    artifacts back to the output S3 location designated in the training job configuration.
    When we are ready to deploy a model, SageMaker spins up new ML instances and pulls
    in these model artifacts to use for batch or real-time model inference.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行训练作业时，SageMaker从Amazon S3读取输入数据，使用该数据训练模型，最后将模型工件写回Amazon S3。[图 7-1](#sagemaker_containerscomma_inputscomma_a)说明了SageMaker如何使用容器进行训练和推断。从左下角开始，来自S3的训练数据可用于模型训练实例容器，该容器从Amazon
    Elastic Container Registry中拉取。训练作业将模型工件持久化到在训练作业配置中指定的输出S3位置。当准备部署模型时，SageMaker启动新的ML实例，并拉取这些模型工件用于批处理或实时模型推断。
- en: '![](assets/dsaw_0701.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0701.png)'
- en: 'Figure 7-1\. SageMaker containers, inputs, and outputs. Source: [Amazon SageMaker
    Workshop](https://oreil.ly/eu9G1).'
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-1\. SageMaker容器、输入和输出。来源：[Amazon SageMaker Workshop](https://oreil.ly/eu9G1)。
- en: 'Much like a software framework, SageMaker provides multiple “hot spots” for
    our training script to leverage. There are two hot spots worth highlighting: input/output
    data locations and environment variables.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 就像软件框架一样，SageMaker提供了多个“热点”供我们的训练脚本利用。有两个值得关注的热点是：输入/输出数据位置和环境变量。
- en: SageMaker provides our container with locations for our training input and output
    files. For example, a typical training job reads in data files, trains the model,
    and writes out a model file. Some AI and machine learning frameworks support model
    checkpointing in case our training job fails or we decide to use a previous checkpoint
    with better predictive performance than our latest model. In this case, the job
    can restart from where it left off. These input, output, and checkpoint files
    must move in and out of the ephemeral Docker container from/to more durable storage
    like S3\. Otherwise, when the training job ends and the Docker container goes
    away, the data is lost.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker为我们的容器提供了训练输入和输出文件的位置。例如，典型的训练作业读取数据文件，训练模型，并输出模型文件。一些AI和机器学习框架支持模型检查点，以防训练作业失败或我们决定使用比最新模型具有更好预测性能的先前检查点。在这种情况下，作业可以从离开的地方重新启动。这些输入、输出和检查点文件必须在短暂的Docker容器内部和更持久的存储（如S3）之间移动，否则当训练作业结束并且Docker容器消失时，数据将会丢失。
- en: While seemingly simple, this mapping is a very critical piece in the training
    performance puzzle. If this layer mapping is not optimized, our training times
    will suffer greatly. Later, we will discuss a SageMaker feature called `Pipe`
    Mode that specifically optimizes the movement of data at this layer. [Figure 7-2](#mapping_of_the_file_location_inside_the)
    shows the mapping of the file location inside the Docker container to the S3 location
    outside the container.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然看似简单，但这种映射在训练性能拼图中是非常关键的一部分。如果这个层次的映射没有优化，我们的训练时间将会大大受到影响。稍后，我们将讨论一种称为`Pipe`模式的SageMaker功能，专门优化了数据在这一层的移动。[图 7-2](#mapping_of_the_file_location_inside_the)展示了将Docker容器内部文件位置映射到容器外部S3位置的情况。
- en: '![](assets/dsaw_0702.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0702.png)'
- en: Figure 7-2\. The container file locations are mapped to S3 locations.
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-2\. 容器文件位置映射到S3位置。
- en: 'SageMaker automatically provides our container with many predefined environment
    variables, such as the number of GPUs available to the container and the log level.
    Our training script can use these SageMaker-injected environment variables to
    modify the behavior of our training job accordingly. Here is a subset of the environment
    variables that SageMaker passes through to our script from a Jupyter notebook,
    script, pipeline, etc.:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker会自动为我们的容器提供许多预定义的环境变量，例如可用于容器的GPU数量和日志级别。我们的训练脚本可以使用这些由SageMaker注入的环境变量来相应地修改我们训练作业的行为。以下是SageMaker传递给我们的脚本的环境变量子集，从Jupyter笔记本、脚本、管道等：
- en: '`SM_MODEL_DIR`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`SM_MODEL_DIR`'
- en: Directory containing the training or processing script as well as dependent
    libraries and assets (*/opt/ml/model*)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 包含训练或处理脚本以及依赖库和资产的目录 (*/opt/ml/model*)
- en: '`SM_INPUT_DIR`'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`SM_INPUT_DIR`'
- en: Directory containing input data (*/opt/ml/input*)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 包含输入数据的目录 (*/opt/ml/input*)
- en: '`SM_INPUT_CONFIG_DIR`'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`SM_INPUT_CONFIG_DIR`'
- en: Directory containing the input configuration *(/opt/ml/input/config)*
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 包含输入配置的目录 *(/opt/ml/input/config)*
- en: '`SM_CHANNELS`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`SM_CHANNELS`'
- en: S3 locations for splits of data, including “train,” “validation,” and “test”
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 包含数据拆分的S3位置，包括“训练”、“验证”和“测试”
- en: '`SM_OUTPUT_DATA_DIR`'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`SM_OUTPUT_DATA_DIR`'
- en: Directory to store evaluation results and other nontraining-related output assets
    (*/opt/ml/output/data*)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 用于存储评估结果和其他非训练相关输出资产的目录 (*/opt/ml/output/data*)
- en: '`SM_HPS`'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`SM_HPS`'
- en: Model hyper-parameters used by the algorithm
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 算法使用的模型超参数
- en: '`SM_CURRENT_HOST`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`SM_CURRENT_HOST`'
- en: Unique hostname for the current instance
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当前实例的唯一主机名
- en: '`SM_HOSTS`'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`SM_HOSTS`'
- en: Hostnames of all instances in the cluster
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中所有实例的主机名
- en: '`SM_NUM_GPUS`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`SM_NUM_GPUS`'
- en: Number of GPUs of the current instance
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当前实例的GPU数量
- en: '`SM_NUM_CPUS`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`SM_NUM_CPUS`'
- en: Number of CPUs of the current instance
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当前实例的CPU数量
- en: '`SM_LOG_LEVEL`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`SM_LOG_LEVEL`'
- en: Logging level used by the training scripts
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 训练脚本使用的日志级别
- en: '`SM_USER_ARGS`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`SM_USER_ARGS`'
- en: Additional arguments specified by the user and parsed by the training or processing
    script
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 用户指定并由训练或处理脚本解析的附加参数
- en: The `_DIR` variables map are the local filepaths internal to the Docker container
    running our training code. These map to external input and output file locations
    in S3, for example, provided by SageMaker and specified by the user when the training
    job is started. However, our training script references the local paths when reading
    input or writing output.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`_DIR`变量映射是运行我们训练代码的Docker容器内部的本地文件路径。这些路径映射到由SageMaker提供并在启动训练作业时由用户指定的S3中的外部输入和输出文件位置。然而，我们的训练脚本在读取输入或写入输出时引用本地路径。'
- en: Increase Availability with Compute and Network Isolation
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过计算和网络隔离提高可用性
- en: Network isolation is also important from a high-availability standpoint. While
    we usually discuss high availability in terms of microservices and real-time systems,
    we should also strive to increase the availability of our training jobs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从高可用性的角度来看，网络隔离同样重要。虽然我们通常在微服务和实时系统方面讨论高可用性，但我们也应努力提高训练作业的可用性。
- en: Our training scripts almost always include `pip install`ing Python libraries
    from PyPI or downloading pre-trained models from third-party model repositories
    (or “model zoos”) on the internet. By creating dependencies on external resources,
    our training job now depends on the availability of those third-party services.
    If one of these services is temporarily down, our training job may not start.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练脚本几乎总是通过`pip install`安装来自PyPI的Python库，或从互联网上的第三方模型库（或“模型动物园”）下载预训练模型。通过创建对外部资源的依赖，我们的训练作业现在依赖于这些第三方服务的可用性。如果其中一个服务暂时不可用，我们的训练作业可能无法启动。
- en: Note
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: At Netflix, we “burned” all dependencies into our Docker images and Amazon Machine
    Images (AMIs) to remove all external dependencies and achieve higher availability.
    It was absolutely critical to reduce external dependencies during rapid scale-out
    events and failover scenarios.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在Netflix，我们将所有依赖项“烧制”到我们的Docker镜像和Amazon Machine Images（AMIs）中，以消除所有外部依赖项并实现更高的可用性。在快速扩展事件和故障转移场景中，减少外部依赖项绝对至关重要。
- en: 'To improve availability, it is recommended that we reduce as many external
    dependencies as possible by copying these resources into our Docker image or into
    our own S3 bucket. This has the added benefit of reducing network latency and
    starting our training jobs faster. The following IAM policy will not start SageMaker
    Training Jobs with network isolation disabled. If we do not enable network isolation,
    the training job will fail immediately, which is exactly what we want to enforce:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高可用性，建议尽可能减少外部依赖，通过将这些资源复制到我们的Docker镜像或我们自己的S3存储桶中来实现。这样做的附加好处是减少网络延迟并更快地启动我们的训练作业。以下IAM策略将不会启动具有禁用网络隔离的SageMaker训练作业。如果我们不启用网络隔离，训练作业将立即失败，这正是我们想要强制执行的：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Computer and network isolation also improve security and reduce the risk of
    attackers gaining access to our data. As a security best practice, all SageMaker
    components should be used in a Virtual Private Cloud (VPC) without direct internet
    connectivity. This requires that we carefully configure IAM roles, VPC Endpoints,
    subnets, and security groups for least-privilege access policies for Amazon S3,
    SageMaker, Redshift, Athena, CloudWatch, and any other AWS service used by our
    data science workflows. In [Chapter 12](ch12.html#secure_data_science_on_aws),
    we will dive deeper into using compute isolation, network isolation, VPC Endpoints,
    and IAM policies to secure our data science environments.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 计算和网络隔离还可以提高安全性并减少攻击者获取我们数据的风险。作为安全最佳实践，所有SageMaker组件应在没有直接互联网连接的虚拟私有云（VPC）中使用。这要求我们仔细配置IAM角色、VPC终端节点、子网和安全组，以最小特权访问策略管理Amazon
    S3、SageMaker、Redshift、Athena、CloudWatch和数据科学工作流中使用的任何其他AWS服务。在[第12章](ch12.html#secure_data_science_on_aws)中，我们将深入探讨如何使用计算隔离、网络隔离、VPC终端节点和IAM策略来保护我们的数据科学环境。
- en: Deploy a Pre-Trained BERT Model with SageMaker JumpStart
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SageMaker JumpStart部署预训练的BERT模型
- en: SageMaker JumpStart provides access to pre-built machine learning solutions
    and pre-trained models from AWS, TensorFlow Hub, and PyTorch Hub across many use
    cases and tasks, such as fraud detection, predictive maintenance, demand forecasting,
    NLP, object detection, and image classification, as shown in [Figure 7-3](#jumpstart_provides_access_to_pre_built).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker JumpStart提供访问来自AWS、TensorFlow Hub和PyTorch Hub的预构建机器学习解决方案和预训练模型，涵盖了许多用例和任务，如欺诈检测、预测性维护、需求预测、NLP、目标检测和图像分类，如图[7-3](#jumpstart_provides_access_to_pre_built)所示。
- en: '![](assets/dsaw_0703.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0703.png)'
- en: Figure 7-3\. Deploy pre-trained models with SageMaker JumpStart.
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-3\. 使用SageMaker JumpStart部署预训练模型。
- en: SageMaker JumpStart is useful when we want to quickly test a solution or model
    on our dataset and generate a baseline set of evaluation metrics. We can quickly
    rule out models that do not work well with our data and, conversely, dive deeper
    into the solutions and models that do work well.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望快速在我们的数据集上测试解决方案或模型并生成基线评估指标时，SageMaker JumpStart非常有用。我们可以快速排除不适合我们数据的模型，并深入研究那些确实适合的解决方案和模型。
- en: Let’s fine-tune a pre-trained BERT model with the Amazon Customer Reviews Dataset
    and deploy the model to production in just a few clicks within SageMaker Studio,
    as shown in [Figure 7-4](#jumpstart_lets_us_fine_tune_and_deploy).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用亚马逊客户评论数据集对预训练的BERT模型进行微调，并在SageMaker Studio中仅需点击几下即可将模型部署到生产环境，如[图7-4](#jumpstart_lets_us_fine_tune_and_deploy)所示。
- en: '![](assets/dsaw_0704.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0704.png)'
- en: Figure 7-4\. SageMaker JumpStart lets us fine-tune and deploy a pre-trained
    BERT model with just a few clicks.
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-4\. 使用SageMaker JumpStart，我们只需点击几下即可对预训练的BERT模型进行微调和部署。
- en: 'After fine-tuning the chosen BERT model with the Amazon Customer Reviews Dataset,
    SageMaker JumpStart deploys the model so we can start making predictions right
    away:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用亚马逊客户评论数据集对选择的BERT模型进行微调后，SageMaker JumpStart部署模型，因此我们可以立即开始进行预测：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output will look similar to this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Develop a SageMaker Model
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发一个SageMaker模型
- en: Just as Amazon.com provides many options to customers through the Amazon.com
    Marketplace, Amazon SageMaker provides many options for building, training, tuning,
    and deploying models. We will dive deep into model tuning in [Chapter 8](ch08.html#train_and_optimize_models_at_scale)
    and deploying in [Chapter 9](ch09.html#deploy_models_to_production). There are
    three main options depending on the level of customization needed, as shown in
    [Figure 7-5](#sagemaker_options_to_buildcomma_trainco).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如亚马逊为顾客提供了通过亚马逊市场多种选择一样，亚马逊SageMaker为构建、训练、调整和部署模型提供了多种选择。我们将在[第8章](ch08.html#train_and_optimize_models_at_scale)深入研究模型调优，并在[第9章](ch09.html#deploy_models_to_production)讨论部署。根据所需的定制程度，有三个主要选项，如[图7-5](#sagemaker_options_to_buildcomma_trainco)所示。
- en: '![](assets/dsaw_0705.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0705.png)'
- en: Figure 7-5\. SageMaker has three options to build, train, optimize, and deploy
    our model.
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-5\. SageMaker有三个选项来构建、训练、优化和部署我们的模型。
- en: Built-in Algorithms
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内置算法
- en: SageMaker provides built-in algorithms that are ready to use out of the box
    across a number of different domains, such as NLP, computer vision, anomaly detection,
    and recommendations. Simply point these highly optimized algorithms at our data
    and we will get a fully trained, easily deployed machine learning model to integrate
    into our application. These algorithms, shown in the following chart, are targeted
    toward those of us who don’t want to manage a lot of infrastructure but rather
    want to reuse battle-tested algorithms designed to work with very large datasets
    and used by tens of thousands of customers. Additionally, they provide conveniences
    such as large-scale distributed training to reduce training times and mixed-precision
    floating-point support to improve model-prediction latency.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker提供了一些内置算法，可以直接用于多个不同领域，如NLP、计算机视觉、异常检测和推荐等。只需将这些高度优化的算法指向我们的数据，我们将获得一个完全训练良好、易于部署的机器学习模型，可以集成到我们的应用程序中。这些算法如下图所示，针对那些不想管理大量基础设施、希望重复使用经过战斗测试的算法，设计用于处理非常大型数据集并被数以万计的客户使用。此外，它们提供诸如大规模分布式训练以减少训练时间和混合精度浮点支持以提高模型预测延迟的便利。
- en: '| **Classification**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '| **分类**'
- en: Linear learner
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性学习者
- en: XGBoost
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost
- en: KNN
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KNN
- en: '| **Computer vision**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '| **计算机视觉**'
- en: Image classification
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分类
- en: Object detection
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测
- en: Semantic segmentation
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义分割
- en: '| **Working with text**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '| **处理文本**'
- en: BlazingText
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BlazingText
- en: Supervised
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督
- en: Unsupervised
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督
- en: '|'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Regression**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '| **回归**'
- en: Linear learner
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性学习者
- en: XGBoost
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost
- en: KNN
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KNN
- en: '| **Anomaly detection**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '| **异常检测**'
- en: Random cut forests
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机切分森林
- en: IP insights
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IP见解
- en: '| **Topic modeling**'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '| **主题建模**'
- en: LDA
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDA
- en: NTM
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NTM
- en: '|'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Sequence translation**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '| **序列翻译**'
- en: Seq2Seq
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seq2Seq
- en: '| **Recommendation**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '| **推荐**'
- en: Factorization machines
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因子分解机
- en: '| **Clustering**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '| **聚类**'
- en: KMeans
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K均值
- en: '|'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Feature reduction**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '| **特征减少**'
- en: PCA
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA
- en: Object2Vec
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Object2Vec
- en: '| **Forecasting**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '| **预测**'
- en: DeepAR
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepAR
- en: '|   |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|   |'
- en: Bring Your Own Script
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自带脚本
- en: SageMaker offers a more customizable option to “bring your own script,” often
    called *Script Mode*. Script Mode lets us focus on our training script, while
    SageMaker provides highly optimized Docker containers for each of the familiar
    open source frameworks, such as TensorFlow, PyTorch, Apache MXNet, XGBoost, and
    scikit-learn, as shown in [Figure 7-6](#popular_ai_and_machine_learning_framewo).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker提供了更多可定制选项“自定义脚本”，通常称为*Script Mode*。Script Mode让我们专注于我们的训练脚本，而SageMaker为每个熟悉的开源框架（如TensorFlow、PyTorch、Apache
    MXNet、XGBoost和scikit-learn）提供了高度优化的Docker容器，如[图 7-6](#popular_ai_and_machine_learning_framewo)所示。
- en: '![](assets/dsaw_0706.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0706.png)'
- en: Figure 7-6\. Popular AI and machine learning frameworks supported by Amazon
    SageMaker.
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-6\. 受 Amazon SageMaker 支持的流行人工智能和机器学习框架。
- en: This option is a good balance of high customization and low maintenance. Most
    of the remaining SageMaker examples in this book will utilize Script Mode with
    TensorFlow and BERT for NLP and natural language understanding (NLU) use cases,
    as shown in [Figure 7-7](#script_mode_with_bert_and_tensorflow_is).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这个选项是高度定制和低维护的良好平衡。本书中其余的大部分SageMaker示例将利用Script Mode与TensorFlow和BERT进行NLP和自然语言理解（NLU）用例，如[图
    7-7](#script_mode_with_bert_and_tensorflow_is)所示。
- en: '![](assets/dsaw_0707.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0707.png)'
- en: Figure 7-7\. SageMaker Script Mode with BERT and TensorFlow is a good balance
    of high customization and low maintenance.
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-7\. SageMaker 脚本模式与 BERT 和 TensorFlow 的结合是高度定制和低维护的良好平衡。
- en: Bring Your Own Container
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义容器部署
- en: The most customizable option is “bring your own container.” This option lets
    us build and deploy our own Docker container to SageMaker. This Docker container
    can contain any library or framework. While we maintain complete control over
    the details of the training script and its dependencies, SageMaker manages the
    low-level infrastructure for logging, monitoring, injecting environment variables,
    injecting hyper-parameters, mapping dataset input and output locations, etc. This
    option is targeted toward a more low-level machine learning practitioner with
    a systems background—or scenarios where we need to use our own Docker container
    for compliance and security reasons. Converting an existing Docker image to run
    within SageMaker is simple and straightforward—just follow the steps listed in
    this [AWS open source project](https://oreil.ly/7Rn86).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最可定制的选项是“自定义容器部署”。此选项允许我们构建和部署自己的Docker容器到SageMaker。这个Docker容器可以包含任何库或框架。虽然我们对训练脚本及其依赖项的详细控制权，SageMaker管理低级基础设施，如日志记录、监控、注入环境变量、注入超参数、映射数据集输入和输出位置等。这个选项面向具有系统背景的更低级别的机器学习从业者，或者需要出于合规性和安全性原因使用自己的Docker容器的场景。将现有的Docker镜像转换为SageMaker内运行的步骤简单明了，只需按照此[AWS开源项目](https://oreil.ly/7Rn86)中列出的步骤进行即可。
- en: A Brief History of Natural Language Processing
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理简史
- en: In the previous chapter, we transformed raw Amazon Customer Reviews into BERT
    feature vectors to ultimately build a review classifier model to predict the `star_rating`
    from `review_body` text. Before we build our natural language model, we want to
    provide some background on NLP.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们将原始的亚马逊客户评论转换为BERT特征向量，最终构建了一个评论分类器模型，用于从“review_body”文本预测“star_rating”。在构建自然语言模型之前，我们想要介绍一些关于NLP的背景知识。
- en: 'In 1935, a famous British linguist, J. R. Firth, said the following: “The complete
    meaning of a word is always contextual, and no study of meaning apart from context
    can be taken seriously.” Fast forward 80 years to 2013: word vectors, or “word
    embeddings,” began to dominate language representations, as shown in [Figure 7-8](#evolution_of_nlp_algorithms_and_archite).
    These word embeddings capture the contextual relationships between words in a
    set of documents, or “corpus,” as it is commonly called.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 1935年，著名的英国语言学家J.R.弗斯（J. R. Firth）曾说过：“一个词的完整意义始终是在其语境中，任何脱离上下文的意义研究都不能被认真对待。”80年后的2013年：词向量或“词嵌入”开始主导语言表示，如[图
    7-8](#evolution_of_nlp_algorithms_and_archite)所示。这些词嵌入捕捉了文档集（通常称为“语料库”）中单词之间的上下文关系。
- en: '![](assets/dsaw_0708.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0708.png)'
- en: Figure 7-8\. Evolution of NLP algorithms and architectures.
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-8\. 自然语言处理（NLP）算法和架构的演变。
- en: Word2Vec and GloVe are two of the popular NLP algorithms from the past decade.
    They both use contextual information to create vector representations of our text
    data in a vector space that lets us perform mathematical computations such as
    word similarity and word differences.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 和 GloVe 是过去十年中流行的两种 NLP 算法。它们都使用上下文信息在向量空间中创建文本数据的向量表示，使我们能够进行诸如单词相似性和单词差异等数学计算。
- en: FastText continues the innovation of contextual NLP algorithms and builds word
    embeddings using subword tokenization. This allows FastText to learn non-English
    language models with relatively small amounts of data compared to other models.
    Amazon SageMaker offers a built-in, pay-as-you-go SageMaker algorithm called *BlazingText*
    that uses an implementation of FastText optimized for AWS. This algorithm was
    shown in [“Built-in Algorithms”](#built_in_algorithm).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: FastText 继续创新上下文 NLP 算法，并使用子词标记化构建单词嵌入。这使得 FastText 能够使用相对较少的数据量学习非英语语言模型。Amazon
    SageMaker 提供了一个内置的、按使用量付费的 SageMaker 算法，称为 *BlazingText*，它使用了针对 AWS 优化的 FastText
    实现。这个算法在 [“Built-in Algorithms”](#built_in_algorithm) 中展示过。
- en: There are some drawbacks to this generation of NLP models, however, as they
    are all forms of static word embeddings. While static embeddings capture the semantic
    meanings of words, they don’t actually understand high-level language concepts.
    In fact, once the embeddings are created, the actual model is often discarded
    after training (i.e., Word2Vec, GloVe) and simply preserve the word embeddings
    to use as features for classical machine learning algorithms such as logistic
    regression and XGBoost.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这一代 NLP 模型存在一些缺点，因为它们都是静态词嵌入的形式。虽然静态嵌入捕捉了单词的语义意义，但它们实际上并不理解高级语言概念。事实上，一旦创建了嵌入，实际模型通常在训练后被丢弃（即
    Word2Vec、GloVe），仅保留单词嵌入以作为传统机器学习算法（如逻辑回归和 XGBoost）的特征使用。
- en: 'ELMo preserves the trained model and uses two long short-term memory (LSTM)
    network branches: one to learn from left to right and one to learn from right
    to left. The context is captured in the LSTM state and updated after every word
    in both network branches. Therefore ELMo does not learn a true bidirectional contextual
    representation of the words and phrases in the corpus, but it performs very well
    nonetheless.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ELMo 保留训练后的模型，并使用两个长短期记忆（LSTM）网络分支：一个从左到右学习，一个从右到左学习。上下文被捕获在 LSTM 状态中，并在每个网络分支中的每个单词后更新。因此，ELMo
    并没有学习语料库中单词和短语的真正的双向上下文表示，但它表现仍然非常好。
- en: Note
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: An LSTM is a special type of recurrent neural network (RNN) that selectively
    chooses which information to remember and which information to forget. This allows
    the LSTM to utilize memory and compute efficiently, avoid the vanishing gradient
    problem, and maintain very good predictive power. A gated recurrent unit is another
    variant of an RNN that is simpler than LSTM and performs very well. However, ELMo
    specifically uses LSTM.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 是一种特殊类型的循环神经网络（RNN），它选择性地选择要记住和要遗忘的信息。这使得 LSTM 能够高效地利用内存和计算资源，避免消失梯度问题，并保持非常好的预测能力。门控循环单元是另一种比
    LSTM 更简单且表现良好的 RNN 变体。然而，ELMo 具体使用 LSTM。
- en: GPT and the newer GPT-2 and GPT-3 models (GPT-n) preserve the trained model
    and use a neural network architecture called the “Transformer” to learn the contextual
    word representations. Transformers were popularized along with their attention-mechanism
    counterpart in the 2017 paper titled [“Attention Is All You Need”](https://oreil.ly/mHHL0).
    Transformers offer highly parallel computation to enable higher throughput, better
    performance, and more-efficient utilization of compute resources. LSTM and ELMo
    do not support parallel computations.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 和更新的 GPT-2 和 GPT-3 模型（GPT-n）保留训练后的模型，并使用称为“Transformer”的神经网络架构来学习上下文词表示。Transformers
    与其注意力机制伴侣一起在 2017 年的《Attention Is All You Need》论文中广为人知。Transformer 提供高度并行计算，以实现更高的吞吐量、更好的性能和更高效的计算资源利用。LSTM
    和 ELMo 不支持并行计算。
- en: The GPT-n transformer uses a directional, left-to-right “masked self-attention”
    mechanism to learn the left-to-right contextual representation, as shown in [Figure 7-9](#gpt_napostrophes_masked_self_attention).
    This prevents the model from peeking ahead to see the next words in the sentence.
    Even with this limitation, GPT-n performs very well on text generation tasks because
    of this left-to-right mechanism.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-n transformer 使用一种定向的、从左到右的“掩码自注意力”机制来学习左到右的上下文表示，如 [图 7-9](#gpt_napostrophes_masked_self_attention)
    所示。这防止模型提前窥视句子中的下一个单词。即使有此限制，GPT-n 在文本生成任务中表现非常出色，因为它采用了这种左到右的机制。
- en: '![](assets/dsaw_0709.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0709.png)'
- en: Figure 7-9\. GPT-n’s masked self-attention mechanism.
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-9\. GPT-n 的掩码自注意力机制。
- en: In 2018, a new neural network-based algorithm for NLP was released called *Bidirectional
    Encoder Representations from Transformers* (BERT). BERT has revolutionized the
    field of NLP and NLU and is now widely used throughout the industry at Facebook,
    LinkedIn, Netflix, Amazon, and many other AI-first companies. BERT builds on the
    highly parallelizable Transformer architecture and adds true bidirectional self-attention
    that looks both forward and backward. BERT’s self-attention mechanism improves
    upon the GPT-n backward-looking, masked self-attention mechanism.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 2018 年，推出了一种新的基于神经网络的自然语言处理算法称为*双向编码器表示转换*（BERT）。BERT 彻底改变了自然语言处理（NLP）和自然语言理解（NLU）领域，并且现在广泛应用于
    Facebook、LinkedIn、Netflix、Amazon 等许多以人工智能为先的公司。BERT 建立在高度可并行化的 Transformer 架构基础上，并添加了真正的双向自注意力机制，可以同时向前和向后查看。BERT
    的自注意力机制改进了 GPT-n 的向后查看、掩码自注意力机制。
- en: BERT Transformer Architecture
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT Transformer 架构
- en: At its core, the BERT Transformer architecture uses an attention mechanism to
    “pay attention” to specific and interesting words or phrases as it traverses the
    corpus. Specifically, the BERT transformer uses “self-attention” to attend every
    token in the data to all other tokens in the input sequence. Additionally, BERT
    uses “multiheaded attention” to handle ambiguity in the meanings of words, also
    called *polysemy* (Greek *poly* = many, *sema* = sign). An example of attention
    is shown in [Figure 7-10](#the_quotation_markself_attentionquotati) where the
    word *it* attends highly to the word *movie* as well as the words *funny* and
    *great*, though to a lesser degree than the word *movie*.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: BERT Transformer 架构的核心是使用注意机制来“关注”语料库中特定和有趣的单词或短语。具体来说，BERT transformer 使用“自注意力”机制来关注数据中的每个标记，以及输入序列中的所有其他标记。此外，BERT
    使用“多头注意力”来处理单词含义上的歧义，也称为*多义性*（希腊语 *poly* = 多, *sema* = 符号）。一个注意力的示例显示在 [图 7-10](#the_quotation_markself_attentionquotati)
    中，其中单词 *it* 高度关注单词 *movie*，以及单词 *funny* 和 *great*，尽管相对于单词 *movie*，关注程度较低。
- en: '![](assets/dsaw_0710.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0710.png)'
- en: Figure 7-10\. The “self-attention” mechanism attends every token in the data
    to all other tokens in the input sequence.
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-10\. “自注意力”机制关注数据中的每个标记到输入序列中的所有其他标记。
- en: 'Without this bidirectional attention, an algorithm would potentially create
    the same embedding for the word *bank* for the following two sentences: “A thief
    stole money from the *bank* vault” and “Later, he was arrested while fishing on
    a river *bank*.” Note that the word *bank* has a different meaning in each sentence.
    This is easy for humans to distinguish because of our lifelong, natural “pre-training,”
    but this is not easy for a machine without similar pre-training. BERT distinguishes
    between these two words (tokens) by learning different vectors for each token
    in the context of a specific (sequence). The learned token vector is called the
    “input token vector representation,” and the learned sentence vector is called
    the “pooled text vector representation.”'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 没有这种双向注意力，算法可能会为以下两个句子中的单词 *bank* 创建相同的嵌入：“A thief stole money from the *bank*
    vault” 和 “Later, he was arrested while fishing on a river *bank*.” 注意，单词 *bank*
    在每个句子中有不同的含义。人类因为有终身的自然“预训练”，很容易区分这一点，但是对于没有类似预训练的机器来说并不容易。BERT 通过学习在特定上下文中为每个标记学习不同的向量来区分这两个单词（标记）。学习的标记向量称为“输入标记向量表示”，学习的句向量称为“汇聚文本向量表示”。
- en: BERT’s transformer-based sequence model consists of several transformer blocks
    stacked upon each other. The pre-trained BERT[Base] model consists of 12 such
    transformer blocks, while the BERT[Large] model consists of 24 transformer blocks.
    Each transformer block implements a multihead attention layer and a fully connected
    feed-forward layer. Each layer is wrapped with a skip connection (residual connection)
    and a layer normalization module.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 基于 Transformer 的序列模型由多个堆叠的 Transformer 块组成。预训练的 BERT[Base] 模型包含 12 个这样的
    Transformer 块，而 BERT[Large] 模型包含 24 个 Transformer 块。每个 Transformer 块实现多头注意力层和全连接前馈层。每一层都带有跳跃连接（残差连接）和层归一化模块。
- en: We add another layer to fine-tune the model to a specific NLP task. For text
    classification, we would add a classifier layer. After the training data is processed
    by all transformer blocks, the data passes through the fine-tuning layer and learns
    parameters specific to our NLP task and dataset. [Figure 7-11](#bert_model_architecture)
    shows the BERT architecture.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们增加了一个额外的层来对模型进行微调，以适应特定的 NLP 任务。对于文本分类，我们会添加一个分类器层。训练数据经过所有 Transformer 块处理后，数据通过微调层，并学习特定于我们的
    NLP 任务和数据集的参数。[图 7-11](#bert_model_architecture)展示了 BERT 的架构。
- en: '![](assets/dsaw_0711.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0711.png)'
- en: Figure 7-11\. BERT model architecture.
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-11\. BERT 模型架构。
- en: Let’s have a closer look at how BERT implements attention. We can think of attention
    as the process of assigning a weight to the input tokens based on their importance
    to the NLP task to solve. In more mathematical terms, attention is a function
    that takes an input sequence X and returns another sequence Y, composed of vectors
    of the same length of those in X. Each vector in Y is a weighted average of the
    vectors in X, as shown in [Figure 7-12](#attention_is_the_weighted_average_of_th).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看一看 BERT 如何实现注意力。我们可以将注意力视为根据其对解决 NLP 任务的重要性而分配权重给输入标记的过程。更具数学化的术语，注意力是一个函数，接受一个输入序列
    X 并返回另一个序列 Y，由与 X 中相同长度的向量组成。Y 中的每个向量都是 X 中向量的加权平均，如[图 7-12](#attention_is_the_weighted_average_of_th)所示。
- en: '![](assets/dsaw_0712.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0712.png)'
- en: Figure 7-12\. Attention is the weighted average of the input vectors.
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-12\. 注意力是输入向量的加权平均。
- en: The weights express how much the model attends to each input vector in X when
    computing the weighted average. So how does BERT calculate the attention weights?
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 权重表达了模型在计算加权平均时对 X 中每个输入向量的关注程度。那么，BERT 如何计算注意力权重呢？
- en: A compatibility function assigns a score to each pair of words indicating how
    strongly they *attend* to one another. In a first step, the model creates a query
    vector (for the word that is paying attention) and a key vector (for the word
    being paid attention to) as linear transformations from the actual value vector.
    The compatibility score is then calculated as the dot product of the query vector
    of one word and the key vector of the other. The score is then normalized by applying
    the softmax function. The result is the attention weight, as shown in [Figure 7-13](#attention_weights_are_the_normalized_do).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 兼容性函数为每对单词分配一个分数，指示它们彼此*关注*的强度。首先，模型创建一个查询向量（用于注意的单词）和一个键向量（用于被注意的单词），作为实际值向量的线性变换。然后计算兼容性分数，作为一个单词的查询向量与另一个单词的键向量的点积。通过应用
    softmax 函数对分数进行归一化。结果就是注意力权重，如[图 7-13](#attention_weights_are_the_normalized_do)所示。
- en: '![](assets/dsaw_0713.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0713.png)'
- en: Figure 7-13\. Attention weights are the normalized dot product of the query
    and key vectors.
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-13\. 注意力权重是查询向量和键向量的归一化点积。
- en: Training BERT from Scratch
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始训练 BERT
- en: While we can use BERT as is without training from scratch, it’s useful to know
    how BERT uses word masking and next sentence prediction—in parallel—to learn and
    understand language.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以直接使用 BERT 而无需从头开始训练，了解 BERT 如何使用单词屏蔽和下一个句子预测—并行—来学习和理解语言是很有用的。
- en: Masked Language Model
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 掩蔽语言模型
- en: As BERT sees new text, it masks 15% of the words in each sentence, or “sequence,”
    in BERT terminology. BERT then predicts the masked words and corrects itself (aka
    “updates the model weights”) when it predicts incorrectly. This is called the
    *Masked Language Model* or Masked LM. Masking forces the model to learn the surrounding
    words for each sequence, as shown in [Figure 7-14](#bert_masks_onefivepercent_of_input_toke).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当 BERT 看到新文本时，它会在每个句子或“序列”中遮蔽 15% 的词（在 BERT 术语中称为“序列”）。然后，BERT 预测遮蔽的词并在预测错误时进行自我纠正（即“更新模型权重”）。这被称为*遮蔽语言模型*或
    Masked LM。遮蔽强迫模型学习每个序列的周围词语，如 [图 7-14](#bert_masks_onefivepercent_of_input_toke)
    所示。
- en: '![](assets/dsaw_0714.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0714.png)'
- en: Figure 7-14\. BERT Masked LM masks 15% of input tokens and learns by predicting
    the masked tokens—correcting itself when it predicts the wrong word.
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-14\. BERT Masked LM 遮蔽 15% 的输入标记，并通过预测遮蔽的标记来学习——当它预测错误的时候会进行自我纠正。
- en: 'To be more concrete, BERT is trained by forcing it to predict masked words
    (actually tokens) in a sentence. For example, if we feed in the contents of this
    book, we can ask BERT to predict the missing word in the following sentence: “This
    book is called Data ____ on AWS.” Obviously, the missing word is “Science.” This
    is easy for a human who has been pre-trained on millions of documents since birth,
    but is not easy for a machine—not without training, anyway.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，BERT 是通过强迫它预测句子中的遮蔽词（实际上是标记）来训练的。例如，如果我们输入这本书的内容，我们可以让 BERT 预测下面句子中的缺失词：“这本书被称为
    Data ____ on AWS。”显然，缺失的词是“Science”。对于一个从出生开始就被预训练在数百万文档上的人类来说，这很容易，但对于机器来说并不容易——至少没有经过训练的机器。
- en: Next Sentence Prediction
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一句预测
- en: At the same time BERT is masking and predicting input tokens, it is also performing
    next sentence prediction (NSP) on pairs of input sequences. Both of these training
    tasks are optimized together to create a single accuracy score for the combined
    training efforts. This results in a more robust model capable of performing word-
    and sentence-level predictive tasks.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当 BERT 同时遮蔽和预测输入标记时，它还在输入序列对上执行下一句预测（NSP）。这两个训练任务被优化在一起，为组合训练工作创建单一的准确度分数。这导致一个更强大的模型，能够执行词级和句级预测任务。
- en: To perform NSP, BERT randomly chooses 50% of the sentence pairs and replaces
    one of the two sentences with a random sentence from another part of the document.
    BERT then predicts if the two sentences are a valid sentence pair or not, as shown
    in [Figure 7-15](#bert_performs_next_sentence_prediction). BERT will correct itself
    when it predicts incorrectly.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行 NSP，BERT 随机选择 50% 的句子对，并用文档中另一部分的随机句子替换其中的一个句子。然后，BERT 预测这两个句子是否是一个有效的句子对，如
    [图 7-15](#bert_performs_next_sentence_prediction) 所示。当 BERT 预测错误时会进行自我纠正。
- en: '![](assets/dsaw_0715.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0715.png)'
- en: Figure 7-15\. During training, BERT performs masking and NSP in parallel on
    pairs of input sequences.
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-15\. 在训练过程中，BERT 在输入序列对上同时执行遮蔽和 NSP。
- en: 'For more details on BERT, check out the 2018 paper, [“BERT: Pre-training of
    Deep Bidirectional Transformers for Language Understanding”](https://oreil.ly/LP4yX).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '想要了解更多关于 BERT 的细节，请查看 2018 年的论文[“BERT: Pre-training of Deep Bidirectional Transformers
    for Language Understanding”](https://oreil.ly/LP4yX)。'
- en: In most cases, we don’t need to train BERT from scratch. Neural networks are
    designed to be reused and continuously trained as new data arrives into the system.
    Since BERT has already been pre-trained on millions of public documents from Wikipedia
    and the Google Books Corpus, the vocabulary and learned representations are transferable
    to a large number of NLP and NLU tasks across a wide variety of domains.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，我们不需要从头开始训练 BERT。神经网络设计为可以重复使用并在新数据到达系统时进行持续训练。由于 BERT 已经在来自维基百科和 Google
    Books Corpus 的数百万公共文档上进行了预训练，其词汇表和学习表示可转移到大量的 NLP 和 NLU 任务中，跨越各种领域。
- en: Training BERT from scratch requires a lot of data and compute, but it allows
    BERT to learn a representation of the custom dataset using a highly specialized
    vocabulary. Companies like Amazon and LinkedIn have pre-trained internal versions
    of BERT from scratch to learn language representations specific to their domain.
    LinkedIn’s variant of BERT, for example, has learned a language model specific
    to job titles, resumes, companies, and business news.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始训练 BERT 需要大量的数据和计算资源，但它允许 BERT 使用高度专业化的词汇表学习定制数据集的表示。像亚马逊和 LinkedIn 这样的公司已经从头开始预训练了内部版本的
    BERT，以学习特定领域的语言表示。例如，LinkedIn 的变体已经学习了特定于职称、简历、公司和商业新闻的语言模型。
- en: Fine Tune a Pre-Trained BERT Model
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调预训练的 BERT 模型
- en: ELMo, GPT/GPT-2, and BERT preserve certain trained models known as “pre-trained
    models.” Pre-trained on millions of documents across many different domains, these
    models are good at not only predicting missing words, but also at learning the
    meaning of words, sentence structure, and sentence correlations. Their ability
    to generate meaningful, relevant, and realistic text is phenomenal and scary.
    Let’s dive deeper into BERT’s pre-trained models.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ELMo、GPT/GPT-2和BERT保留了某些被称为“预训练模型”的训练模型。这些模型在许多不同领域的数百万篇文档上进行了预训练，它们不仅擅长预测缺失的单词，还能学习单词的含义、句子结构和句子之间的关联。它们生成有意义、相关和真实的文本的能力令人称奇和畏惧。让我们深入了解一下BERT的预训练模型。
- en: BERT’s pre-trained models are, like most neural network models, just point-in-time
    snapshots of the model weights learned from the data seen to date. And like most
    models, BERT becomes even more valuable with more data.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的预训练模型与大多数神经网络模型一样，仅是从迄今为止见过的数据学习到的模型权重的即时快照。与大多数模型一样，BERT随着数据量的增加变得更加有价值。
- en: The core BERT pre-trained models come in “base” and “large” variants that differ
    by number of layers, attention heads, hidden units, and parameters, as shown in
    the following table. We see very good performance with the smaller model with
    only 12 attention heads and 110 million parameters.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的核心预训练模型分为“base”和“large”两个变种，其层数、注意力头数、隐藏单元和参数数量不同，如下表所示。我们发现即使使用只有12个注意力头和1.1亿参数的较小模型，也能得到非常好的性能。
- en: '|   | Layers | Hidden units | Parameters |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|   | 层次 | 隐藏单元 | 参数 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **BERT base** | 12 | 768 | 110M |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| **BERT base** | 12 | 768 | 110M |'
- en: '| **BERT large** | 24 | 1024 | 340M |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| **BERT large** | 24 | 1024 | 340M |'
- en: Additionally, the community has created many pre-trained versions of BERT using
    domain and language-specific datasets, including PatentBERT (US patent data),
    ClinicalBERT (healthcare data), CamemBERT (French language), GermanBERT (German
    language), and BERTje (Dutch language).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，社区已经使用领域和语言特定的数据集创建了许多BERT的预训练版本，包括PatentBERT（美国专利数据）、ClinicalBERT（医疗数据）、CamemBERT（法语）、GermanBERT（德语）和BERTje（荷兰语）。
- en: These BERT variants were pre-trained from scratch because the default BERT models,
    trained on English versions of Wikipedia and Google Books, do not share the same
    vocabulary as the custom datasets—e.g., French for CamemBERT, and healthcare terminology
    for ClinicalBERT. When training from scratch, we can reuse BERT’s neural network
    transformer architecture but throw out the pre-trained base model weights learned
    from Wikipedia and Google Books.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这些BERT的变体是从头开始预训练的，因为默认的BERT模型是在英文版本的维基百科和谷歌图书上进行训练的，并不与定制数据集（例如CamemBERT的法语和ClinicalBERT的医疗术语）共享相同的词汇。当从头开始训练时，我们可以重用BERT的神经网络变换器架构，但是舍弃从维基百科和谷歌图书学到的预训练基础模型权重。
- en: For our Amazon Customer Reviews Dataset, we can safely reuse the default BERT
    models because they share a similar vocabulary and language representation. There
    is no doubt that training BERT from scratch to learn the specific Amazon.com product
    catalog would improve accuracy on some tasks, such as entity recognition. However,
    the default BERT models perform very well on our review text, so we will keep
    things simple and “fine-tune” a default BERT model to create a custom text classifier
    using our Amazon Customer Reviews Dataset.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的亚马逊客户评论数据集，我们可以安全地重用默认的BERT模型，因为它们具有类似的词汇表和语言表示。毫无疑问，从头开始训练BERT以学习特定的亚马逊.com产品目录将提高某些任务（如实体识别）的准确性。然而，默认的BERT模型在我们的评论文本上表现非常好，因此我们将保持简单，使用“微调”默认的BERT模型来创建一个使用我们的亚马逊客户评论数据集的定制文本分类器。
- en: Let’s reuse the language understanding and semantics learned by the pre-trained
    BERT model to learn a new, domain-specific NLP task using the Amazon Customer
    Reviews Dataset. This process, called “fine-tuning,” is shown in [Figure 7-16](#using_a_pre_trained_bert_model_and_fine).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用预训练的BERT模型学习一个新的领域特定的NLP任务，使用亚马逊客户评论数据集。这个过程称为“微调”，如[图 7-16](#using_a_pre_trained_bert_model_and_fine)所示。
- en: '![](assets/dsaw_0716.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0716.png)'
- en: Figure 7-16\. We can fine-tune a pre-trained BERT model for a domain-specific
    task using a custom dataset.
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-16\. 我们可以使用自定义数据集对预训练的BERT模型进行领域特定任务的微调。
- en: The simplicity and bidirectional nature of the BERT self-attention mechanism
    allow us to fine-tune the base BERT models to a wide range of out-of-the-box,
    “downstream” NLP/NLU tasks, including text classification to analyze sentiment,
    entity recognition to detect a product name, and next sentence prediction to provide
    answers to natural language questions, as shown in [Figure 7-17](#we_can_fine_tune_the_default_bert_model).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 自注意机制的简单性和双向性使我们能够将基础 BERT 模型微调到各种即插即用的“下游” NLP/NLU 任务中，包括文本分类分析情感、实体识别检测产品名称，以及下一句预测回答自然语言问题，如[图 7-17](#we_can_fine_tune_the_default_bert_model)所示。
- en: '![](assets/dsaw_0717.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0717.png)'
- en: Figure 7-17\. We can fine-tune the default BERT models to many “downstream”
    NLP and NLU tasks.
  id: totrans-176
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-17\. 我们可以将默认的 BERT 模型微调到许多“下游” NLP 和 NLU 任务。
- en: Since fine-tuning is a supervised training process (versus pre-training, which
    is unsupervised), masking and next sentence prediction do not happen during fine-tuning—only
    during pre-training. As a result, fine-tuning is very fast and requires a relatively
    small number of samples, or reviews, in our case. This translates to lower processing
    power, lower cost, and faster training/tuning iterations.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于微调是一个有监督的训练过程（与无监督的预训练相对），在微调过程中不会进行掩码和下一句预测——这些仅在预训练期间发生。因此，微调非常快速，仅需要相对较少的样本，或者在我们的情况下是评论。这意味着更低的处理能力、更低的成本和更快的训练/调整迭代。
- en: Remember that we can use SageMaker JumpStart to try out these pre-trained models
    quickly and establish their usefulness as a solution for our machine learning
    task. By quickly fine-tuning the pre-trained BERT model to our dataset, we can
    determine if BERT is a good fit or not.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们可以使用 SageMaker JumpStart 快速尝试这些预训练模型，并确定它们作为解决方案在我们的机器学习任务中的有效性。通过快速将预训练的
    BERT 模型微调到我们的数据集，我们可以确定BERT是否合适。
- en: Since we already generated the BERT embeddings from the raw `review_body` text
    in [Chapter 6](ch06.html#prepare_the_dataset_for_model_training), we are ready
    to go! Let’s fine-tune BERT to create a custom text classifier that predicts `star_rating`
    from `review_body` using our dataset, as shown in [Figure 7-18](#fine_tuning_a_bert_model_to_create_a_cu).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经从[第 6 章](ch06.html#prepare_the_dataset_for_model_training)中原始的 `review_body`
    文本生成了 BERT 嵌入，我们已经准备好了！让我们微调 BERT，创建一个自定义文本分类器，用于从 `review_body` 预测 `star_rating`，使用我们的数据集，如[图 7-18](#fine_tuning_a_bert_model_to_create_a_cu)所示。
- en: '![](assets/dsaw_0718.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0718.png)'
- en: Figure 7-18\. We can fine-tune a BERT model to create a custom text classifier
    with our reviews dataset.
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-18\. 我们可以微调 BERT 模型，使用我们的评论数据集创建一个自定义文本分类器。
- en: We can use this classifier to predict the sentiment of an incoming customer
    service email or Twitter comment, for example. So when a new email or comment
    enters the system, we first classify the email as negative (`star_rating` 1),
    neutral (`star_rating` 3), or positive (`star_rating` 5). This can help us determine
    the urgency of the response—or help us route the message to the right person,
    as shown in [Figure 7-19](#fine_tuning_bert_to_classify_review_tex).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个分类器预测即将到来的客户服务电子邮件或 Twitter 评论的情绪，例如。因此，当新的电子邮件或评论进入系统时，我们首先将电子邮件分类为负面（`star_rating`
    1）、中性（`star_rating` 3）或正面（`star_rating` 5）。这可以帮助我们确定回复的紧急性——或者帮助我们将消息路由到正确的人员，如[图 7-19](#fine_tuning_bert_to_classify_review_tex)所示。
- en: '![](assets/dsaw_0719.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0719.png)'
- en: Figure 7-19\. We can fine-tune BERT to classify review text into `star_rating`
    categories of 1 (worst) through 5 (best).
  id: totrans-184
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-19\. 我们可以将 BERT 微调到将评论文本分类为 `star_rating` 1（最差）到 5（最好）的类别。
- en: Create the Training Script
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建训练脚本
- en: Let’s create a training script called *tf_bert_reviews.py* that creates our
    classifier using TensorFlow and Keras. We will then pass the features generated
    from the previous chapter into our classifier for model training.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个名为 *tf_bert_reviews.py* 的训练脚本，使用 TensorFlow 和 Keras 创建我们的分类器。然后，我们将前一章生成的特征传递给我们的分类器进行模型训练。
- en: Setup the Train, Validation, and Test Dataset Splits
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置训练、验证和测试数据集拆分
- en: In the last chapter, we used SageMaker Processing Jobs to transform raw Amazon
    Customer Reviews into BERT embeddings, as shown in [Figure 7-20](#bert_embeddings_as_inputs_for_model_tra).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用 SageMaker 处理作业将原始的亚马逊客户评论转换为 BERT 嵌入，如[图 7-20](#bert_embeddings_as_inputs_for_model_tra)所示。
- en: '![](assets/dsaw_0720.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0720.png)'
- en: Figure 7-20\. BERT embeddings as inputs for model training with TensorFlow.
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-20\. BERT 嵌入作为 TensorFlow 模型训练的输入。
- en: In this section, we load the train, validation, and test datasets to feed into
    our model for training. We will use TensorFlow’s `TFRecordDataset` implementation
    to load the `TFRecord`s in parallel and shuffle the data to prevent the model
    from learning the pattern in which the data is presented to the model.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们加载训练、验证和测试数据集以供模型训练使用。我们将使用 TensorFlow 的 `TFRecordDataset` 实现并行加载 `TFRecord`
    并对数据进行洗牌，以防止模型学习到数据呈现方式的模式。
- en: Note
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In AI and machine learning, randomness is celebrated. Proper shuffling of the
    training data will provide enough randomness to prevent the model from learning
    any patterns about how the data is stored on disk and/or presented to the model.
    “Bootstrapping” is a common technique to describe random sampling with replacement.
    Bootstrapping adds bias, variance, confidence intervals, and other metrics to
    the sampling process.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能和机器学习领域，随机性是被赞扬的。正确地对训练数据进行洗牌将提供足够的随机性，以防止模型学习到关于数据在磁盘上存储或者呈现给模型的任何模式。“自助法”是描述带有替换的随机抽样的常见技术。自助法为抽样过程增加了偏差、方差、置信区间和其他指标。
- en: 'In [Chapter 6](ch06.html#prepare_the_dataset_for_model_training), we created
    a SageMaker Processing Job to transform the raw `review_body` column into BERT
    embeddings using the Hugging Face Transformers library. This Processing Job stores
    the embeddings in S3 using a TensorFlow-optimized `TFRecord` file format, which
    we will use in our training job. Let’s create a helper function to load, parse,
    and shuffle the `TFRecord`s. We should recognize the `input_ids`, `input_mask`,
    `segment_ids`, and `label_ids` field names from the previous chapter:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](ch06.html#prepare_the_dataset_for_model_training)中，我们创建了一个 SageMaker
    处理作业，使用 Hugging Face Transformers 库将原始 `review_body` 列转换为 BERT 嵌入。该处理作业使用 TensorFlow
    优化的 `TFRecord` 文件格式将嵌入存储在 S3 中，我们将在训练作业中使用它们。让我们创建一个帮助函数来加载、解析和洗牌 `TFRecord`。
- en: '[PRE3]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If `is_training` is true, we are in the training phase. During the training
    phase, we want to shuffle the data between iterations. Otherwise, the model may
    pick up on patterns about how the data is stored on disk and presented to the
    model—i.e., first all the 5s, then all the 4s, 3s, 2s, 1s, etc. To discourage
    the model from learning this pattern, we shuffle the data. If `is_training` is
    false, then we are in either the validation or test phase, and we can avoid the
    shuffle overhead and iterate sequentially.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `is_training` 为真，则表示我们处于训练阶段。在训练阶段，我们希望在迭代之间对数据进行洗牌。否则，模型可能会学习到关于数据在磁盘上存储和呈现给模型的模式，即先是所有的5，然后是所有的4，3，2，1等等。为了防止模型学习到这种模式，我们对数据进行洗牌。如果
    `is_training` 为假，则表示我们处于验证或测试阶段，可以避免洗牌的开销，并按顺序迭代。
- en: 'Let’s read in the training, validation, and test datasets using the helper
    function created earlier:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用之前创建的帮助函数读取训练、验证和测试数据集：
- en: '[PRE4]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We will soon pass these train, validation, and test datasets to our model training
    process. But first, let’s set up the custom reviews classifier using TensorFlow,
    Keras, BERT, and Hugging Face.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快将把这些训练、验证和测试数据集传递给我们的模型训练过程。但首先，让我们使用 TensorFlow、Keras、BERT 和 Hugging Face
    设置自定义评论分类器。
- en: Set Up the Custom Classifier Model
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置自定义分类器模型
- en: Soon, we will feed the `review_body` embeddings and `star_rating` labels into
    a neural network to fine-tune the BERT model and train the custom review classifier,
    as shown in [Figure 7-21](#classify_reviews_into_star_rating_one_l). Note that
    the words shown in the figure may be broken up into smaller word tokens during
    tokenization. For illustrative purposes, however, we show them as full words.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 很快，我们将把 `review_body` 嵌入和 `star_rating` 标签输入到神经网络中，以微调 BERT 模型并训练自定义评论分类器，如[图7-21](#classify_reviews_into_star_rating_one_l)所示。请注意，图中显示的单词可能在标记化过程中被分成更小的词标记。但为了说明目的，我们展示它们为完整单词。
- en: '![](assets/dsaw_0721.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0721.png)'
- en: Figure 7-21\. Classify reviews into star rating 1 (worst) through 5 (best) using
    our custom classifier.
  id: totrans-203
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-21\. 使用我们的自定义分类器将评论分类为星级评分1（最差）到5（最佳）。
- en: For this, we use the Keras API with TensorFlow 2.x to add a neural classifier
    layer on top of the pre-trained BERT model to learn the `star_rating` (1–5). Remember
    that we are using a relatively lightweight variant of BERT called DistilBERT,
    which requires less memory and compute but maintains very good accuracy on our
    dataset. To reduce the size of the model, DistilBERT, a student neural network,
    was trained by a larger teacher neural network in a process called *knowledge
    distillation*, as shown in [Figure 7-22](#knowledge_distillation_trains_a_student).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们使用 TensorFlow 2.x 的 Keras API，在预训练的 BERT 模型顶部添加一个神经分类器层来学习`star_rating`（1-5）。请记住，我们使用的是一个相对轻量级的
    BERT 变体叫做 DistilBERT，它需要更少的内存和计算资源，但在我们的数据集上保持了非常好的准确性。为了减少模型的大小，DistilBERT，一个学生神经网络，通过一个更大的教师神经网络进行了知识蒸馏的过程，如[图
    7-22](#knowledge_distillation_trains_a_student)所示。
- en: '![](assets/dsaw_0722.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图](assets/dsaw_0722.png)'
- en: Figure 7-22\. Knowledge distillation trains a student model from a teacher model.
  id: totrans-206
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-22。知识蒸馏从一个教师模型训练出一个学生模型。
- en: 'Let’s load `DistilBertConfig`, map our 1-indexed `star_rating` labels to the
    0-indexed internal classes, and load our pre-trained DistilBERT model as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载`DistilBertConfig`，将我们的1索引的`star_rating`标签映射到0索引的内部类，并按如下方式加载我们预训练的 DistilBERT
    模型：
- en: '[PRE5]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'It’s important to highlight that the `from_pretrained()` function calls will
    download a large model from the Hugging Face service. We should consider downloading
    this model to our own S3 bucket and pass the S3 URI to the `from_pretrained()`
    function calls. This small change will decouple us from the Hugging Face service,
    remove a potential single point of failure, enable network isolation, and reduce
    the start time of our model training jobs. Next, let’s set up our inputs and model
    layers:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 强调`from_pretrained()`函数调用会从 Hugging Face 服务下载一个大型模型是很重要的。我们应考虑下载此模型到我们自己的 S3
    存储桶，并将 S3 URI 传递给`from_pretrained()`函数调用。这个小改动将使我们脱离 Hugging Face 服务，去除潜在的单点故障，实现网络隔离，并减少模型训练作业的启动时间。接下来，让我们设置输入和模型层：
- en: '[PRE6]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We have chosen not to train the BERT layers by specifying `trainable=False`.
    We do this on purpose to keep the underlying BERT model from changing—focusing
    only on training our custom classifier. Training the BERT layer will likely improve
    our accuracy, but the training job will take longer. Since our accuracy is pretty
    good without training the underlying BERT model, we focus only on training the
    classifier layer. Next, let’s add a Keras-based neural classifier to complete
    our neural network and prepare for model training:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择不训练 BERT 层，通过指定`trainable=False`。这是有意为之，以保持底层 BERT 模型不变——仅集中于训练我们的自定义分类器。训练
    BERT 层可能会提高我们的准确性，但训练作业会更长。由于我们在不训练底层 BERT 模型的情况下准确率已经相当不错，我们只专注于训练分类器层。接下来，让我们添加一个基于
    Keras 的神经分类器来完成我们的神经网络并准备模型训练：
- en: '[PRE7]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here is the output of the model summary showing the breakdown of trainable
    and nontrainable parameters:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是模型摘要的输出，显示了可训练和不可训练参数的详细情况：
- en: '[PRE8]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Train and Validate the Model
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和验证模型
- en: At this point, we have prepared our train, validation, and test datasets as
    input data and defined our custom classifier `model`. Let’s pull everything together
    and invoke the `fit()` function on our model using the `train_dataset` and `validation_dataset`.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经准备好了训练、验证和测试数据集作为输入数据，并定义了我们的自定义分类器`model`。让我们把所有的东西都汇总起来，并在我们的模型上使用`train_dataset`和`validation_dataset`调用`fit()`函数。
- en: 'By passing `validation_dataset`, we are using the Keras API with TensorFlow
    2.x to perform both training and validation simultaneously:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 通过传递`validation_dataset`，我们使用 TensorFlow 2.x 中的 Keras API 同时进行训练和验证：
- en: '[PRE9]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We set `shuffle=True` to shuffle our dataset and `epochs=5` to run through our
    dataset five times. The number of `epochs` (pronounced “eh-puhks”) is configurable
    and tunable. We will explore model tuning in the next chapter.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置`shuffle=True`以对数据集进行洗牌，`epochs=5`以使数据集训练五次。`epochs`的数量（发音为“eh-puhks”）是可配置和可调整的。我们将在下一章节探讨模型调优。
- en: Save the Model
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存模型
- en: 'Now, let’s save the model with the TensorFlow `SavedModel` format used by our
    predictive applications:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 TensorFlow 的`SavedModel`格式保存模型，用于我们的预测应用程序：
- en: '[PRE10]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In [Chapter 9](ch09.html#deploy_models_to_production), we will use the model
    saved in *`./tensorflow/`* with TensorFlow Serving to deploy our models and serve
    review-classification predictions at scale using SageMaker Batch Transform (offline,
    batch) and SageMaker Endpoints (online, real time).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](ch09.html#deploy_models_to_production)中，我们将使用保存在*`./tensorflow/`*中的模型与TensorFlow
    Serving部署我们的模型，并使用SageMaker批转换（离线、批处理）和SageMaker端点（在线、实时）提供评论分类预测。
- en: Launch the Training Script from a SageMaker Notebook
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从SageMaker Notebook启动训练脚本
- en: Let’s walk through the steps needed to run our training script from a SageMaker
    Notebook. Later, we will run this same script from an automated pipeline. For
    now, we run the script from the notebook. First, we will set up the metrics needed
    to monitor the training job. We’ll then configure our algorithm-specific hyper-parameters.
    Next, we’ll select the instance type and number of instances in our cluster. And
    finally, we will launch our training job.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步了解从SageMaker Notebook运行训练脚本所需的步骤。稍后，我们将从自动化流水线中运行相同的脚本。目前，我们从笔记本运行脚本。首先，我们将设置需要监视训练作业的指标。然后，我们将配置我们算法特定的超参数。接下来，我们将选择我们集群中的实例类型和实例数量。最后，我们将启动我们的训练作业。
- en: Define the Metrics to Capture and Monitor
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义要捕获和监视的指标
- en: 'We can create a metric from anything that our training script prints or logs
    to the console. Let’s assume that our TensorFlow model emits the following log
    lines with the training loss and training accuracy (`loss`, `accuracy`) as well
    as the validation loss and validation accuracy (`val_loss`, `val_accuracy`):'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据训练脚本打印或记录到控制台的任何内容创建指标。假设我们的TensorFlow模型通过以下日志行发出训练损失和训练精度（`loss`、`accuracy`），以及验证损失和验证精度（`val_loss`、`val_accuracy`）：
- en: '[PRE11]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we define four regular expressions to populate four metrics by parsing
    the values from the log lines. If we upgrade the framework—or switch to a new
    framework—these regular expressions may need adjusting. We will know when this
    happens because we will no longer see the correct model metrics in our CloudWatch
    dashboards:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义四个正则表达式来通过解析日志行中的值来填充四个指标。如果我们升级框架——或者切换到新框架——这些正则表达式可能需要调整。当这种情况发生时，我们会知道，因为我们将不再在我们的CloudWatch仪表板上看到正确的模型指标：
- en: '[PRE12]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Configure the Hyper-Parameters for Our Algorithm
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置我们算法的超参数
- en: It’s important to note that “parameters” (aka “weights”) are *what* the model
    learns during training and that “hyper-parameters” are *how* the model learns
    the parameters. Every algorithm supports a set of hyper-parameters that alter
    the algorithm’s behavior while learning the dataset. Hyper-parameters can be anything
    from the depth of a decision tree to the number of layers in our neural network.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，“参数”（也称为“权重”）是模型在训练过程中学习的内容，“超参数”则是模型在学习过程中学习参数的方式。每种算法都支持一组超参数，这些参数可以改变算法在学习数据集时的行为。超参数可以是从决策树深度到神经网络层数等任何内容。
- en: Hyper-parameter selection involves the usual trade-offs between latency and
    accuracy. For example, a deeper neural network with lots of layers may provide
    better accuracy than a shallow neural network, but the deeper network may lead
    to higher latency during inference as prediction time increases with each layer
    in the network.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数选择涉及延迟和准确性之间的通常权衡。例如，具有许多层的更深的神经网络可能比较浅的神经网络提供更好的准确性，但更深的网络可能导致推断时延迟增加，因为每层的预测时间随着网络中的层数增加而增加。
- en: While most hyper-parameters have suitable defaults based on empirical testing,
    they are highly tunable. In fact, there’s an entire subfield within machine learning
    dedicated to hyper-parameter tuning/hyper-parameter optimization.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数超参数都有基于经验测试的合适默认值，但它们可以进行高度调整。事实上，机器学习中有一个专门的子领域致力于超参数调整/超参数优化。
- en: 'We will dive deep into the art and science of hyper-parameter selection and
    optimization in [Chapter 8](ch08.html#train_and_optimize_models_at_scale) to find
    the best combination of hyper-parameters. For now, we set these hyper-parameters
    manually using our experience and intuition—as well as some lightweight, ad hoc
    empirical testing with our specific dataset and algorithm:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将深入探讨[第8章](ch08.html#train_and_optimize_models_at_scale)中的超参数选择和优化的艺术与科学，以找到最佳的超参数组合。目前，我们通过我们的经验、直觉以及一些轻量级的、临时的经验测试与特定数据集和算法来手动设置这些超参数：
- en: '[PRE13]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: When evaluating an algorithm, we should seek to understand all of the available
    hyper-parameters. Setting these hyper-parameters to suboptimal values can make
    or break a data science project. This is why the subfield of hyper-parameter optimization
    is so important.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估算法时，我们应该努力理解所有可用的超参数。将这些超参数设置为次优值可能决定了数据科学项目的成败。这就是为什么超参数优化子领域如此重要的原因。
- en: Select Instance Type and Instance Count
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择实例类型和实例数量
- en: The choice of instance type and instance count depends on our workload and budget.
    Fortunately AWS offers many different instance types, including AI/ML-optimized
    instances with ultra-fast GPUs, terabytes of RAM, and gigabits of network bandwidth.
    In the cloud, we can easily scale up our training jobs to larger instances with
    more memory and compute or scale out to tens, hundreds, or even thousands of instances
    with just one line of code.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 实例类型和实例数量的选择取决于我们的工作负载和预算。幸运的是，AWS 提供了许多不同的实例类型，包括AI/ML优化实例，配备超快速GPU、TB级内存和GB级网络带宽。在云端，我们可以轻松地扩展我们的训练作业到更大的实例，具有更多的内存和计算能力，或者扩展到数十、数百甚至数千个实例，只需一行代码即可。
- en: Let’s train with a `p4d.24xlarge` instance type with 8 NVIDIA Tesla A100 GPUs,
    96 CPUs, 1.1 terabytes of RAM, 400 gigabits per second of network bandwidth, and
    600 gigabytes per second of inter-GPU communication using NVIDIA’s NVSwitch “mesh”
    network hardware, as shown in [Figure 7-23](#mesh_communication_between_gpus_on_a_si).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `p4d.24xlarge` 实例类型进行训练，它配备 8 个 NVIDIA Tesla A100 GPU、96 个CPU、1.1TB内存、400Gb/s网络带宽和600Gb/s
    NVIDIA NVSwitch“网格”网络硬件之间的 GPU 间通信，如图[7-23](#mesh_communication_between_gpus_on_a_si)所示。
- en: '![](assets/dsaw_0723.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0723.png)'
- en: Figure 7-23\. Mesh communication between GPUs on a single instance.
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-23\. 单个实例上 GPU 之间的网格通信。
- en: 'To save cost, we would normally start small and slowly ramp up the compute
    resources needed for our specific workload to find the lowest-cost option. This
    is commonly called “right-sizing our cluster.” Empirically, we found that this
    instance type works well with our specific model, dataset, and cost budget. We
    only need one of these instances for our example, so we set the `train_instance_count`
    to 1, as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为了节约成本，我们通常会从小规模开始，逐渐增加适合我们特定工作负载所需的计算资源，以找到最低成本的选项。这通常被称为“权衡我们的集群”。根据经验，我们发现这种实例类型与我们特定的模型、数据集和成本预算非常匹配。对于我们的示例，我们只需要一个这样的实例，因此我们将
    `train_instance_count` 设置为 1，如下所示：
- en: '[PRE14]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Tip
  id: totrans-245
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: We can specify `instance_type='local'` in our SageMaker Training Job to run
    the script either inside our notebook or on our local laptop. See [“Reduce Cost
    and Increase Performance”](#reduce_cost_and_increase_performan) for more information.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 SageMaker 训练作业中指定 `instance_type='local'` 来在笔记本内部或本地笔记本上运行脚本。有关更多信息，请参见[“降低成本并提高性能”](#reduce_cost_and_increase_performan)。
- en: It’s important to choose parallelizable algorithms that benefit from multiple
    cluster instances. If our algorithm is not parallelizable, we should not add more
    instances because they will not be used. And adding too many instances may actually
    slow down our training job by creating too much communication overhead between
    the instances. Most neural-network-based algorithms like BERT are parallelizable
    and benefit from a distributed cluster when training or fine-tuning on large datasets.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 选择能够从多个集群实例中获益的可并行化算法非常重要。如果我们的算法不可并行化，那么增加实例数量是没有意义的，因为它们不会被使用。而且增加过多的实例可能会通过增加实例之间的通信开销而实际上减慢我们的训练作业速度。大多数基于神经网络的算法如
    BERT 都是可并行化的，并且在训练或对大型数据集进行微调时受益于分布式集群。
- en: Putting It All Together in the Notebook
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在笔记本中将所有内容整合在一起
- en: 'Here is our Jupyter notebook that sets up and invokes the TensorFlow training
    job using SageMaker Script Mode:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们使用 SageMaker 脚本模式设置和调用 TensorFlow 训练作业的 Jupyter 笔记本：
- en: '[PRE15]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Lastly, we call the `estimator.fit()`method with the train, validation, and
    test dataset splits to start the training job from the notebook as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用 `estimator.fit()` 方法来启动笔记本上的训练作业，使用训练、验证和测试数据集分割，具体操作如下：
- en: '[PRE17]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Download and Inspect Our Trained Model from S3
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 S3 下载并检查我们训练好的模型
- en: 'Let’s use the AWS CLI to download our model from S3 and inspect it with TensorFlow’s
    `saved_model_cli` script:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 AWS CLI 从 S3 下载我们的模型，并使用 TensorFlow 的 `saved_model_cli` 脚本进行检查：
- en: '[PRE18]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We see that the model expects two input vectors of size 64, the `max_seq_length`
    for the `input_ids` and `input_mask` vectors, and returns one output vector of
    size 5, the number of classes for the `star_rating` 1–5\. The output represents
    a confidence distribution over the five classes. The most-confident prediction
    will be our `star_rating` prediction.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到模型期望两个大小为64的输入向量，即`input_ids`和`input_mask`向量的`max_seq_length`，并返回大小为5的输出向量，即`star_rating`的五个类别。输出表示五个类别的置信度分布。最有信心的预测将是我们的`star_rating`预测。
- en: 'Let’s use `saved_model_cli` to make a quick prediction with sample data (all
    zeros) to verify that the model inputs are sufficient. The actual input and output
    values do not matter here. We are simply testing the network to make sure the
    model accepts two vectors of the expected input size and returns one vector of
    the expected output size:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`saved_model_cli`来使用示例数据（全零向量）进行快速预测，以验证模型接受预期输入大小的两个向量，并返回预期输出大小的一个向量。这里实际的输入和输出值并不重要，我们只是测试网络以确保模型接受预期输入和输出：
- en: '[PRE19]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Show Experiment Lineage for Our SageMaker Training Job
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 展示我们SageMaker训练任务的实验血统
- en: Once the hyper-parameter tuning job has finished, we can analyze the results
    directly in our notebook or through SageMaker Studio.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成超参数调整作业，我们可以直接在我们的笔记本或通过SageMaker Studio分析结果。
- en: 'Let’s summarize the experiment lineage up to this point. In [Chapter 8](ch08.html#train_and_optimize_models_at_scale),
    we will tune our hyper-parameters and extend our experiment lineage to include
    hyper-parameter optimization. In [Chapter 9](ch09.html#deploy_models_to_production),
    we will deploy the model and further extend our experiment lineage to include
    model deployment. We will then tie everything together in an end-to-end pipeline
    with full lineage tracking in [Chapter 10](ch10.html#pipelines_and_mlops):'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结到目前为止的实验血统。在[第8章](ch08.html#train_and_optimize_models_at_scale)中，我们将调整超参数，并扩展实验血统以包括超参数优化。在[第9章](ch09.html#deploy_models_to_production)中，我们将部署模型，并进一步扩展实验血统以包括模型部署。我们将在[第10章](ch10.html#pipelines_and_mlops)中将所有内容整合成端到端的流水线，并进行全程血统跟踪：
- en: '[PRE20]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '| TrialComponentName | DisplayName | max_seq_length | learning_rate | train_accuracy
    | **...** |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| TrialComponentName | DisplayName | max_seq_length | learning_rate | train_accuracy
    | **...** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| TrialComponent-2021-01-09-062410-pxuy | prepare | 64.0 | NaN | NaN | ...
    |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| TrialComponent-2021-01-09-062410-pxuy | prepare | 64.0 | NaN | NaN | ...
    |'
- en: '| tensorflow-training-2021-01-09-06-24-12-989 | train | 64.0 | 0.00001 | 0.9394
    | ... |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| tensorflow-training-2021-01-09-06-24-12-989 | train | 64.0 | 0.00001 | 0.9394
    | ... |'
- en: Show Artifact Lineage for Our SageMaker Training Job
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 展示我们SageMaker训练任务的工件血统
- en: 'We can show the artifact lineage information that has been captured for our
    SageMaker Training Job used to fine-tune our product review classifier:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以展示已捕获的SageMaker训练任务的工件血统信息，用于优化我们的产品评论分类器：
- en: '[PRE21]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output should look similar to this:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该类似于这样：
- en: '| Name/source | Direction | Type | Association type | Lineage type |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 名称/来源 | 方向 | 类型 | 协会类型 | 血统类型 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| s3://.../output/bert-test | Input | DataSet | ContributedTo | artifact |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| s3://.../output/bert-test | 输入 | 数据集 | ContributedTo | artifact |'
- en: '| s3://.../output/bert-validation | Input | DataSet | ContributedTo | artifact
    |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| s3://.../output/bert-validation | 输入 | 数据集 | ContributedTo | artifact |'
- en: '| s3://.../output/bert-train | Input | DataSet | ContributedTo | artifact |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| s3://.../output/bert-train | 输入 | 数据集 | ContributedTo | artifact |'
- en: '| 76310.../tensorflow-training:2.3.1-gpu-py37 | Input | Image | ContributedTo
    | artifact |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 76310.../tensorflow-training:2.3.1-gpu-py37 | 输入 | 图像 | ContributedTo | artifact
    |'
- en: '| s3://.../output/model.tar.gz | Output | Model | Produced | artifact |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| s3://.../output/model.tar.gz | 输出 | 模型 | 产出 | artifact |'
- en: SageMaker Lineage Tracking automatically recorded the input data, output artifacts,
    and SageMaker container image. The Association Type shows that the inputs have
    `ContributedTo` this pipeline step. Let’s continue to build up our model lineage
    graph as we tune and deploy our model in Chapters [8](ch08.html#train_and_optimize_models_at_scale)
    and [9](ch09.html#deploy_models_to_production). We will then tie everything together
    in an end-to-end pipeline with full lineage tracking in [Chapter 10](ch10.html#pipelines_and_mlops).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Lineage Tracking自动记录了输入数据、输出工件和SageMaker容器镜像。协会类型显示输入数据`ContributedTo`这个流水线步骤。让我们在[第8章](ch08.html#train_and_optimize_models_at_scale)和[第9章](ch09.html#deploy_models_to_production)中继续构建模型血统图，调整并部署模型。我们将在[第10章](ch10.html#pipelines_and_mlops)中将所有内容整合成端到端的流水线，并进行全程血统跟踪。
- en: Evaluate Models
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: After we have trained and validated our model, we can use the remaining holdout
    dataset—the test dataset—to perform our own predictions and measure the model’s
    performance. Testing the model with the test dataset helps us evaluate how well
    the model generalizes on unseen data. Therefore, we should never use the holdout
    test dataset for training or validation. Based on the test results, we may need
    to modify our algorithm, hyper-parameters, or training data. Additionally, more
    training data—and more diverse feature engineering—may help improve our evaluation
    results.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们训练和验证模型之后，我们可以使用剩余的保留测试数据集——测试数据集——来执行我们自己的预测并测量模型的性能。使用测试数据集测试模型有助于评估模型在未见数据上的泛化能力。因此，我们不应该将保留测试数据集用于训练或验证。基于测试结果，我们可能需要修改我们的算法、超参数或训练数据。此外，更多的训练数据和更多的多样化特征工程可能有助于改善我们的评估结果。
- en: 'Following is the code to evaluate the model using Keras API within TensorFlow—similar
    to how we trained and validated the model in the previous section:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用Keras API在TensorFlow中评估模型的代码，与我们在前一节中训练和验证模型的方式类似：
- en: '[PRE22]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `test_history` contains the `test_loss` and `test_accuracy`, respectively:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '`test_history`包含`test_loss`和`test_accuracy`，分别如下：'
- en: '[PRE23]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Run Some Ad Hoc Predictions from the Notebook
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从笔记本运行一些临时预测
- en: 'We can also run some cursory predictions from the notebook to quickly satisfy
    our curiosity about the model’s health. Here’s a snippet of relevant code to run
    sample predictions:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以从笔记本中运行一些临时预测，以快速满足我们对模型健康状态的好奇心。以下是运行样本预测的相关代码片段：
- en: '[PRE24]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following output shows the predicted `label` (1–5) as well as the confidence
    for each predicted `label`. In this case, the model is 92% confident that the
    `label` is 5 for the review text “This is great!”:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出显示了预测的`label`（1–5）以及每个预测`label`的置信度。在这种情况下，模型对于评论文本“This is great!”预测`label`为5的置信度为92%：
- en: '[PRE25]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Analyze Our Classifier with a Confusion Matrix
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用混淆矩阵分析我们的分类器
- en: 'A confusion matrix is a visual way of evaluating a classifier’s performance.
    Let’s create a confusion matrix to visually inspect the test results by comparing
    the predicted and actual values. We start by reading in the holdout test dataset
    that includes the raw `review_body` text:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是评估分类器性能的一种可视化方式。让我们创建一个混淆矩阵，通过比较预测和实际值来视觉检查测试结果。我们首先读取包含原始`review_body`文本的保留测试数据集：
- en: '[PRE26]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we use the `predict` function to calculate the predicted `y_test` dataset.
    We’ll compare this to the observed values, `y_actual`, using the following code:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`predict`函数计算预测的`y_test`数据集。我们将使用以下代码将其与观察到的值`y_actual`进行比较：
- en: '[PRE27]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This results in the confusion matrix shown in [Figure 7-24](#confusion_matrix_showing_the_true_left).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了混淆矩阵显示在[图 7-24](#confusion_matrix_showing_the_true_left)。
- en: '![](assets/dsaw_0724.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0724.png)'
- en: Figure 7-24\. Confusion matrix showing the true (actual) labels and predicted
    labels.
  id: totrans-298
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-24\. 显示真实（实际）标签和预测标签的混淆矩阵。
- en: Visualize Our Neural Network with TensorBoard
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorBoard可视化我们的神经网络
- en: TensorBoard is an open source visualization and exploration tool maintained
    by the TensorFlow community to provide insight into TensorFlow model training.
    SageMaker captures and saves the TensorBoard metrics in S3 during model training.
    We can then visualize these metrics directly from our SageMaker Studio notebook
    using the S3 location of the saved TensorBoard metrics, as shown in [Figure 7-25](#tensorboard_showing_loss_and_accuracy_o).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard是一个由TensorFlow社区维护的开源可视化和探索工具，用于提供有关TensorFlow模型训练的洞察。SageMaker在模型训练期间捕获并保存TensorBoard指标到S3中。然后，我们可以使用SageMaker
    Studio笔记本直接从保存的TensorBoard指标的S3位置可视化这些指标，如[图 7-25](#tensorboard_showing_loss_and_accuracy_o)所示。
- en: '![](assets/dsaw_0725.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0725.png)'
- en: Figure 7-25\. TensorBoard showing loss and accuracy over time.
  id: totrans-302
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-25\. TensorBoard显示随时间变化的损失和准确性。
- en: We can also inspect our neural network, as shown in [Figure 7-26](#tensorboard_showing_the_tensorflow_grap).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以检查我们的神经网络，如[图 7-26](#tensorboard_showing_the_tensorflow_grap)所示。
- en: '![](assets/dsaw_0726.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0726.png)'
- en: Figure 7-26\. TensorBoard showing the TensorFlow graph for BERT.
  id: totrans-305
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-26\. TensorBoard显示BERT的TensorFlow图。
- en: 'To run TensorBoard from a SageMaker Notebook, simply install it with `pip install
    tensorboard`, point to the TensorBoard logs in S3, and start the process from
    a notebook terminal as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 要从SageMaker Notebook运行TensorBoard，只需使用`pip install tensorboard`进行安装，指向S3中的TensorBoard日志，并从笔记本终端启动进程，如下所示：
- en: '[PRE28]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Using our browser, we can securely navigate to TensorBoard running in our SageMaker
    Notebook as follows:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的浏览器，我们可以安全地访问运行在我们的SageMaker笔记本中的TensorBoard，方法如下：
- en: '*https://<NOTEBOOK_NAME>.notebook.<REGION>.sagemaker.aws/proxy/6006/*'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '*https://<NOTEBOOK_NAME>.notebook.<REGION>.sagemaker.aws/proxy/6006/*'
- en: Note
  id: totrans-310
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The 6006 port was chosen by one of the Google engineers who created TensorBoard.
    The port is the term “goog” upside down!
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 6006端口是由创建TensorBoard的Google工程师之一选择的。这个端口是术语“goog”的倒过来写法！
- en: Monitor Metrics with SageMaker Studio
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在SageMaker Studio中监控指标
- en: After training, we should evaluate a model’s performance using metrics such
    as accuracy to determine if the model achieves our business objective. In our
    example, we want to measure if our model correctly predicts the `star_rating`
    from the `review_body`. Note that we performed training and validation in the
    same Keras step.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们应该使用准确率等指标来评估模型的性能，以确定模型是否达到我们的业务目标。在我们的示例中，我们希望衡量我们的模型是否能从`review_body`正确预测`star_rating`。请注意，我们在同一Keras步骤中进行了训练和验证。
- en: We can visualize our training and validation metrics directly with SageMaker
    Studio throughout the training process, as shown in [Figure 7-27](#monitor_training_and_validation_metrics).
    We visually see overfitting happening very early, so we likely want to stop the
    training job early to save money.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在整个训练过程中直接使用SageMaker Studio可视化我们的训练和验证指标，如图 7-27所示。我们很早就可以直观地看到过拟合的发生，因此我们可能希望尽早停止训练作业以节省成本。
- en: '![](assets/dsaw_0727.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0727.png)'
- en: Figure 7-27\. Monitor training and validation metrics directly within SageMaker
    Studio.
  id: totrans-316
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-27\. 直接在SageMaker Studio内监控训练和验证指标。
- en: Monitor Metrics with CloudWatch Metrics
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用CloudWatch指标监控指标
- en: We can also visualize our model metrics in CloudWatch alongside system metrics
    like CPU, GPU, and memory utilization. [Figure 7-28](#dashboard_of_training_and_validation_ac)
    shows the train and validation accuracy metrics alongside the system metrics in
    CloudWatch.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在CloudWatch中将我们的模型指标与CPU、GPU和内存利用等系统指标一起可视化。图 7-28显示了CloudWatch中训练和验证准确率指标与系统指标的仪表板。
- en: '![](assets/dsaw_0728.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0728.png)'
- en: Figure 7-28\. Dashboard of training and validation accuracy metrics in CloudWatch.
  id: totrans-320
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-28\. CloudWatch中训练和验证准确率指标的仪表板。
- en: Debug and Profile Model Training with SageMaker Debugger
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SageMaker Debugger调试和分析模型训练
- en: During training, we can use SageMaker Debugger to provide full insight into
    model training by monitoring, recording, and analyzing the state of each model
    training job—without any code changes. We can use SageMaker Debugger to stop our
    training job early to save money when we detect certain conditions like overfitting.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，我们可以使用SageMaker Debugger全面了解模型训练的情况，通过监视、记录和分析每个模型训练作业的状态，而无需进行任何代码更改。当我们检测到某些条件（如过拟合）时，我们可以使用SageMaker
    Debugger提前停止训练作业以节省成本。
- en: With SageMaker Debugger, we can interactively and visually explore the data
    captured during training, including tensor, gradient, and resource utilization.
    SageMaker Debugger captures this debugging and profiling information for both
    single-instance training jobs as well as multi-instance, distributed training
    clusters.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SageMaker Debugger，我们可以交互式地和视觉化地探索训练期间捕获的数据，包括张量、梯度和资源利用情况。SageMaker Debugger捕获这些调试和分析信息，适用于单实例训练作业以及多实例分布式训练集群。
- en: Detect and Resolve Issues with SageMaker Debugger Rules and Actions
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SageMaker Debugger规则和操作检测和解决问题
- en: Combined with CloudWatch Events, SageMaker Debugger can trigger an alert if
    a particular rule condition is met, such as bad training data, vanishing gradients,
    and exploding gradients. Bad data includes NaNs and nulls. Vanishing gradients
    occur when really small values are multiplied by other really small values and
    the result is too small for our `float` data type to store. Exploding gradients
    are the opposite of vanishing gradients. They occur when really large values are
    multiplied by other really large values and the result can’t be represented by
    the 32 bits of our `float` data type. Both vanishing and exploding gradients can
    occur in deep neural networks given the number of matrix multiplications happening
    throughout the layers. As small numbers are multiplied by other small numbers,
    they will eventually approach zero and no longer be representable with a 32-bit
    `float`.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 结合CloudWatch Events，如果满足特定规则条件，SageMaker Debugger可以触发警报，例如坏的训练数据、消失的梯度和爆炸的梯度。
    坏数据包括NaN和null值。 消失的梯度发生在当非常小的值乘以其他非常小的值时，结果对于我们的`float`数据类型来说太小而无法存储。 爆炸的梯度是消失梯度的相反。
    它们发生在当非常大的值乘以其他非常大的值时，结果不能由我们`float`数据类型的32位表示。 在深度神经网络中，由于在各层中进行的矩阵乘法的数量，消失和爆炸的梯度都可能发生。
    当小数乘以其他小数时，它们最终会接近零，并且不能再用32位`float`表示。
- en: If SageMaker Debugger triggers an alert at 3 a.m., for example, SageMaker can
    automatically stop the training job. SageMaker can also send an email or text
    message to the on-call data scientist to investigate the issue. The data scientist
    would then use SageMaker Debugger to analyze the training run, visualize the tensors,
    review the CPU and GPU system metrics, and determine the root cause of the alert.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 如果SageMaker Debugger在凌晨3点触发警报，例如，SageMaker可以自动停止训练作业。 SageMaker还可以向负责的数据科学家发送电子邮件或短信以调查问题。
    然后，数据科学家将使用SageMaker Debugger分析训练运行，可视化张量，查看CPU和GPU系统指标，并确定警报的根本原因。
- en: In addition to vanishing and exploding gradients, SageMaker Debugger also supports
    built-in rules for common debugging scenarios, such as `loss_not_decreasing`,
    `overfit`, `overtraining`, and `class_imbalance`. SageMaker launches an evaluation
    job for each SageMaker rule specified. We can also provide our own rules by providing
    a Docker image and implementation of the `Rule` framework.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 除了消失和爆炸的梯度外，SageMaker Debugger还支持常见调试场景的内置规则，例如`loss_not_decreasing`、`overfit`、`overtraining`和`class_imbalance`。
    SageMaker为每个指定的SageMaker规则启动评估作业。 我们还可以通过提供Docker镜像和`Rule`框架的实现来提供自己的规则。
- en: 'Following is the code to create two rules to detect when the training loss
    stops decreasing at an adequate rate (`loss_not_decreasing`) and when the model
    starts to `overtrain` as the validation loss increases after a number of steps
    of normally decreasing behavior. These are both signals to “early stop” the training
    job, reduce the cost of the overall training job, avoid overfitting our model
    to the training dataset, and allow the model to generalize better on new, unseen
    data. Rules are configured with thresholds to define when the rules should trigger—as
    well as the Action to take when the rules are triggered:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是创建两个规则以检测训练损失停止以充分速率减少（`loss_not_decreasing`）和模型在经过正常下降行为的若干步骤后开始`过拟合`时的代码。
    这两者都是“提前停止”训练作业的信号，减少整体训练作业的成本，避免过度拟合我们的模型到训练数据集，并允许模型在新的、未见过的数据上更好地泛化。 规则配置了阈值，以定义规则何时触发以及规则触发时采取的操作：
- en: '[PRE29]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We also need to create a debugger hook to use with the Keras API within TensorFlow
    2.x as follows:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要创建一个调试器钩子，以便在TensorFlow 2.x中与Keras API一起使用，如下所示：
- en: '[PRE30]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then we need to set the rules and debugger hook in our Estimator as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要在我们的Estimator中设置规则和调试器钩子，如下所示：
- en: '[PRE31]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Profile Training Jobs
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置训练作业
- en: 'Let’s configure some `ProfileRule`s to analyze CPU, GPU, network, and disk
    I/O metrics—as well as generate a `ProfilerReport` for our training job. Here,
    we are adding more to our existing `rules` list from the debugging section:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们配置一些`ProfileRule`来分析CPU、GPU、网络和磁盘I/O指标，并为我们的训练作业生成一个`ProfilerReport`。 在这里，我们正在从调试部分添加更多到我们现有的`rules`列表：
- en: '[PRE32]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We then need to create a `ProfilerConfig` and pass it to our Estimator as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要创建一个`ProfilerConfig`并将其传递给我们的Estimator，如下所示：
- en: '[PRE33]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[Figure 7-29](#sagemaker_debugger_deep_profiling_analy) shows a profile report
    generated by SageMaker Debugger during our training run. This report includes
    a suggestion to increase our batch size to improve GPU utilization, speed up our
    training job, and reduce cost.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-29](#sagemaker_debugger_deep_profiling_analy) 显示了 SageMaker Debugger 在我们的训练运行期间生成的概要报告。此报告包括建议增加批次大小以提高
    GPU 利用率、加快训练作业速度和减少成本。'
- en: '![](assets/dsaw_0729.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0729.png)'
- en: Figure 7-29\. SageMaker Debugger deep profiling analyzes model training jobs.
  id: totrans-342
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-29\. SageMaker Debugger 深度分析模型训练作业。
- en: Interpret and Explain Model Predictions
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释和说明模型预测
- en: 'We can also use SageMaker Debugger to track gradients, layers, and weights
    during the training process. We will use this to monitor the BERT attention mechanism
    during model training. By understanding how the model is learning, we can better
    identify model bias and potentially explain model predictions. To do this, we
    need to capture tensors, including the attention scores, query vectors, and key
    vectors as SageMaker Debugger “collections.” This information can then be used
    to plot the attention heads and individual neurons in the query and key vectors.
    Let’s create our `DebuggerHookConfig` and a `CollectionConfig` using regex to
    capture the attention tensors at a particular interval during training:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 SageMaker Debugger 跟踪训练过程中的梯度、层和权重。我们将使用它来监视 BERT 注意力机制在模型训练期间的情况。通过了解模型的学习过程，我们可以更好地识别模型偏差，并可能解释模型预测。为此，我们需要捕获张量，包括注意力分数、查询向量和键向量作为
    SageMaker Debugger “集合”。然后，可以使用这些信息来绘制注意力头部和查询键向量中的单个神经元。让我们创建我们的 `DebuggerHookConfig`
    和 `CollectionConfig`，使用正则表达式在训练期间的特定间隔捕获注意力张量：
- en: '[PRE35]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We also add the following lines in the training script’s validation loop to
    record the string representation of input tokens:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在训练脚本的验证循环中添加了以下行来记录输入标记的字符串表示：
- en: '[PRE36]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'To visualize the results, we create a trial pointing to the captured tensors:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化结果，我们创建一个指向捕获张量的试验：
- en: '[PRE37]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We will use a script that plots the attention head using [Bokeh](https://oreil.ly/ZTxLN),
    an interactive visualization library:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个脚本，使用 [Bokeh](https://oreil.ly/ZTxLN)，一个交互式可视化库，绘制注意力头部：
- en: '[PRE38]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let’s get the tensor names of the attention scores:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们获取注意力分数的张量名称：
- en: '[PRE39]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The tensor names should look similar to this since we are using a BERT model
    with 12 attention heads:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是具有 12 个注意力头的 BERT 模型，因此张量名称应该类似于这样：
- en: '[PRE40]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Next we iterate over the available tensors and retrieve the tensor values:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们迭代可用张量并检索张量值：
- en: '[PRE41]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, we retrieve the query and key output tensor names:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们检索查询和键输出张量名称：
- en: '[PRE42]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We also retrieve the string representation of the input tokens:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还检索输入标记的字符串表示：
- en: '[PRE43]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We are now ready to plot the attention head showing the attention scores between
    different tokens. The thicker the line, the higher the score. Let’s plot the first
    20 tokens using the following code, summarized in [Figure 7-30](#attention_head_view_for_the_first_twoze):'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以绘制注意力头部，显示不同标记之间的注意力分数。线条越粗，分数越高。让我们使用以下代码绘制前 20 个标记，摘要见 [图 7-30](#attention_head_view_for_the_first_twoze)：
- en: '[PRE44]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![](assets/dsaw_0730.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0730.png)'
- en: 'Figure 7-30\. Attention-head view for the first 20 tokens. Source: [“Visualizing
    Attention in Transformer-Based Language Representation Models”](https://oreil.ly/v6a5S).'
  id: totrans-365
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-30\. 前 20 个标记的注意力头部视图。来源：[“在基于 Transformer 的语言表示模型中可视化注意力”](https://oreil.ly/v6a5S)。
- en: 'Next, we retrieve the query and key vector tensors:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们检索查询和键向量张量：
- en: '[PRE45]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'With the tensor values at hand, we can plot a detailed neuron view:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 有了张量值，我们可以绘制详细的神经元视图：
- en: '[PRE46]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The resulting visualization is shown in [Figure 7-31](#neuron_view_of_query_and_key_vectors_le).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 所得可视化结果显示在 [图 7-31](#neuron_view_of_query_and_key_vectors_le) 中。
- en: '![](assets/dsaw_0731.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0731.png)'
- en: 'Figure 7-31\. Neuron view of query and key vectors. Source: [“Visualizing Attention
    in Transformer-Based Language Representation Models”](https://oreil.ly/v6a5S).'
  id: totrans-372
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-31\. 查询和键向量的神经元视图。来源：[“在基于 Transformer 的语言表示模型中可视化注意力”](https://oreil.ly/v6a5S)。
- en: The darker the color in this visualization, the more the neurons influenced
    the attention scores.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在此可视化中，颜色越深，神经元对注意力分数的影响越大。
- en: 'As mentioned, the visualization of BERT attention can help to identify the
    root cause for incorrect model predictions. There is an active debate in the industry
    right now whether or not attention can be used for model explainability. A more
    popular approach to model explainability is gradient-based toolkits that generate
    saliency maps, such as [AllenNLP Interpret](https://oreil.ly/wJLRh). The saliency
    maps identify which input tokens had the biggest influence on a model prediction
    and might be a more straightforward approach to NLP model explainability. Let’s
    use the [AllenNLP demo website](https://oreil.ly/WzARH) to create a saliency map
    when predicting the sentiment of the following review text: “a very well-made,
    funny and entertaining picture.” [Figure 7-32](#visualization_of_the_top_onezero_most_i)
    shows the top 10 most important words that led to the “Positive” prediction.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，BERT 注意力的可视化有助于识别不正确模型预测的根本原因。目前，行业内就是否可以使用注意力进行模型可解释性存在激烈的辩论。用于模型可解释性的更流行的方法是基于梯度的工具包，例如生成显著性地图的工具包
    [AllenNLP Interpret](https://oreil.ly/wJLRh)。显著性地图标识哪些输入标记对模型预测产生了最大影响，可能是 NLP
    模型解释性的更直接方法。让我们使用 [AllenNLP 演示网站](https://oreil.ly/WzARH) 来创建一个显著性地图，以预测以下评论文本的情感：“一个非常精心制作的、有趣和令人娱乐的图片”。[图7-32](#visualization_of_the_top_onezero_most_i)
    显示了导致“正面”预测的前10个最重要的词。
- en: '![](assets/dsaw_0732.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0732.png)'
- en: Figure 7-32\. Visualization of the top 10 most important words to a sentiment
    analysis prediction using [AllenNLP Interpret](https://oreil.ly/wJLRh).
  id: totrans-376
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-32。使用 [AllenNLP Interpret](https://oreil.ly/wJLRh) 对情感分析预测的前10个最重要的词进行可视化。
- en: 'We can integrate AllenNLP saliency maps into our Python applications by installing
    AllenNLP using `pip install allennlp`. In the following, we are calculating the
    integrated gradients, a measure of the influence that each token has on a prediction.
    We are specifically using a variant of BERT called RoBERTa, but AllenNLP supports
    many variants of BERT:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用 `pip install allennlp` 安装 AllenNLP，将 AllenNLP 显著性地图集成到我们的 Python 应用程序中。在接下来的过程中，我们正在计算集成梯度，这是每个标记对预测影响的一种度量。我们特别使用了称为
    RoBERTa 的 BERT 变体，但 AllenNLP 支持许多 BERT 的变体：
- en: '[PRE47]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output looks as follows:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE48]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Detect Model Bias and Explain Predictions
  id: totrans-381
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测模型偏差并解释预测
- en: Even with an unbiased dataset, there is still the potential to train a biased
    model. This sounds surprising, but there are certain hyper-parameters that may
    favor particular facets of input features differently than other facets of the
    same feature. Additionally, we should be careful when fine-tuning with pre-trained
    models that are biased. BERT, for example, is biased because of the type of data
    that it was trained on. Due to the model’s ability to learn from context, BERT
    picks up the statistical properties of the Wikipedia training data, including
    any expressed bias and social stereotypes. As we are fighting to reduce bias and
    stereotypes in our society, we should also implement mechanisms to detect and
    stop this bias from propagating into our models.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是一个无偏的数据集，仍然存在训练一个偏置模型的可能性。这听起来令人惊讶，但某些超参数可能会偏向于与相同特征的其他方面不同的输入特征的特定方面。此外，当使用存在偏见的预训练模型进行微调时，我们应当小心。例如，BERT
    由于其训练数据的类型而存在偏见。由于该模型能够从上下文中学习，BERT 获取了维基百科训练数据的统计特性，包括任何表达的偏见和社会刻板印象。正如我们在努力减少社会中的偏见和刻板印象一样，我们还应该实施机制来检测和阻止这种偏见传播到我们的模型中。
- en: SageMaker Clarify helps us to detect bias and evaluate model fairness in each
    step of our machine learning pipeline. We saw in [Chapter 5](ch05.html#explore_the_dataset)
    how to use SageMaker Clarify to detect bias and class imbalances in our dataset.
    We now use SageMaker Clarify to analyze our trained model.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Clarify 帮助我们在机器学习管道的每个步骤中检测偏差并评估模型的公平性。我们在[第5章](ch05.html#explore_the_dataset)中看到如何使用
    SageMaker Clarify 检测数据集中的偏差和类别不平衡。现在我们使用 SageMaker Clarify 来分析我们训练过的模型。
- en: For post-training bias analysis, SageMaker Clarify integrates with SageMaker
    Experiments. SageMaker Clarify will look into the training data, the labels, and
    the model predictions and run a set of algorithms to calculate common data and
    model bias metrics. We can also use SageMaker Clarify to explain model predictions
    by analyzing feature importances.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 对于后训练偏差分析，SageMaker Clarify 与 SageMaker Experiments 集成。SageMaker Clarify 将审查训练数据、标签和模型预测，并运行一组算法来计算常见的数据和模型偏差度量。我们还可以使用
    SageMaker Clarify 通过分析特征重要性来解释模型预测。
- en: Detect Bias with a SageMaker Clarify Processing Job
  id: totrans-385
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SageMaker Clarify 处理作业检测偏差
- en: Similar to the pre-training bias analysis, we can run SageMaker Clarify as a
    Processing Job to calculate post-training data and model bias metrics. Calculating
    post-training bias metrics does require a trained model because the analysis now
    includes the data, labels, and model predictions. We define our trained model
    in `ModelConfig` and specify the model prediction format in `ModelPredictedLabelConfig`.
    SageMaker Clarify performs the post-training bias analysis by comparing the model
    predictions against the labels in the training data with respect to the chosen
    facet.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于预训练偏差分析，我们可以将SageMaker Clarify作为处理作业运行，以计算后训练数据和模型偏差指标。计算后训练偏差指标需要一个经过训练的模型，因为分析现在包括数据、标签和模型预测结果。我们在`ModelConfig`中定义我们的训练模型，并在`ModelPredictedLabelConfig`中指定模型预测格式。SageMaker
    Clarify通过比较模型预测结果与训练数据中的标签，针对所选的方面执行后训练偏差分析。
- en: 'The provided training data must match the model’s expected inference inputs
    plus the label column. We have chosen to train our model with just `review_body`
    as the single input feature. However, for this example, we have added `product_category`
    as a second feature and retrained our model. We use `product_category` as the
    facet to analyze for bias and imbalance across gift cards, digital software, and
    digital video games:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的训练数据必须与模型期望的推理输入以及标签列匹配。我们选择仅使用`review_body`作为单个输入特征来训练我们的模型。然而，在这个例子中，我们添加了`product_category`作为第二个特征，并重新训练了我们的模型。我们使用`product_category`作为分析偏差和不平衡的方面，跨礼品卡、数字软件和数字视频游戏进行分析：
- en: '[PRE49]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'In `ModelConfig` we define our trained model and specify the instance type
    and count for a shadow endpoint, which SageMaker Clarify creates:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在`ModelConfig`中，我们定义了我们的训练模型，并指定了用于影子端点的实例类型和数量，SageMaker Clarify会创建这些影子端点：
- en: '[PRE50]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '`ModelPredictedLabelConfig` defines how to parse and read the model predictions.
    We can specify `label`, `probability`, `probability_threshold`, and `label_headers`.
    If our model returns a JSON output such as `{"predicted_label": 5}`, we could
    parse the prediction result by setting `label=''predicted_label''`. If the model
    output matches the provided label type and format from the training data, we can
    simply leave it as is:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '`ModelPredictedLabelConfig`定义了如何解析和读取模型预测结果。我们可以指定`label`、`probability`、`probability_threshold`和`label_headers`。如果我们的模型返回类似于`{"predicted_label":
    5}`的JSON输出，我们可以通过设置`label=''predicted_label''`来解析预测结果。如果模型输出与训练数据提供的标签类型和格式匹配，我们可以简单地保持不变：'
- en: '[PRE51]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: In `methods` we can select which post-training bias metrics to calculate.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在`methods`中，我们可以选择计算哪些后训练偏差指标。
- en: In our example, we could analyze whether our model predicts more negative star
    ratings for a specific product category (facet) value, such as `Digital Software`,
    compared to another facet value, such as `Gift Cards`. One of the corresponding
    bias metrics to check would be difference in conditional rejection (DCR), which
    compares the labels to the predicted labels for each facet (`product_category`)
    for negative classifications (rejections). In this case, we could define `star_rating==5`
    and `star_rating==4` as the positive outcomes and the other classes as negative
    outcomes.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们可以分析我们的模型是否对特定产品类别（方面）值预测更多的负面星级评分，例如`数字软件`，与其他方面值如`礼品卡`相比。其中一个相关的偏差指标是条件拒绝差异（DCR），它比较了每个方面（`product_category`）的标签与预测标签在负面分类（拒绝）上的差异。在这种情况下，我们可以定义`star_rating==5`和`star_rating==4`作为正面结果，而其他类别作为负面结果。
- en: Note
  id: totrans-395
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Post-training metrics include difference in positive proportions in predicted
    labels, disparate impact, difference in conditional acceptance, difference in
    conditional rejection, recall difference, difference in acceptance rate, difference
    in rejection rates, accuracy difference, treatment equality, conditional demographic
    disparity in predicted labels, and counterfactual fliptest.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练指标包括预测标签中正比例的差异、不公平影响、条件接受差异、条件拒绝差异、召回差异、接受率差异、拒绝率差异、准确率差异、处理平等性、预测标签中的条件人口统计差异以及反事实翻转测试。
- en: SageMaker Clarify starts the post-training bias analysis by validating the provided
    configuration inputs and output parameters. SageMaker Clarify then creates an
    ephemeral, shadow SageMaker model endpoint and deploys the trained model. The
    processing job then calculates the defined bias metrics. [Table 7-1](#post-training-table)
    shows the calculated post-training bias metrics for our model. Once the job completes,
    SageMaker Clarify generates the output files and deletes the shadow endpoint.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Clarify通过验证提供的配置输入和输出参数来启动后训练偏倚分析。然后，SageMaker Clarify创建一个临时的影子SageMaker模型端点，并部署训练好的模型。处理作业然后计算定义的偏倚度量。[表 7-1](#post-training-table)显示了我们模型的计算后训练偏倚度量。作业完成后，SageMaker
    Clarify生成输出文件并删除影子端点。
- en: Table 7-1\. Post-training bias metrics analysis results
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7-1\. 后训练偏倚度量分析结果
- en: '| name | description | value |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| name | 描述 | 值 |'
- en: '| --- | --- | --- |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| AD | Accuracy difference (AD) | -0.25 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| AD | 准确率差异（AD） | -0.25 |'
- en: '| CDDPL | Conditional demographic disparity in predicted labels (CDDPL) | -0.333333
    |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| CDDPL | 预测标签中条件人口统计差异（CDDPL） | -0.333333 |'
- en: '| DAR | Difference in acceptance rates (DAR) | -0.444444 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| DAR | 接受率差异（DAR） | -0.444444 |'
- en: '| DCA | Difference in conditional acceptance (DCA) | -0.333333 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| DCA | 条件接受差异（DCA） | -0.333333 |'
- en: '| DCR | Difference in conditional rejection (DCR) | -1.27273 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| DCR | 条件拒绝差异（DCR） | -1.27273 |'
- en: '| DI | Disparate impact (DI) | 2.22222 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| DI | 不平等影响（DI） | 2.22222 |'
- en: '| DPPL | Difference in positive proportions in predicted labels (DPPL) | -0.55
    |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| DPPL | 预测标签中正比例的差异（DPPL） | -0.55 |'
- en: '| DRR | Difference in rejection rates (DRR) | -0.909091 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| DRR | 拒绝率差异（DRR） | -0.909091 |'
- en: '| RD | Recall difference (RD) | -0.166667 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| RD | 召回率差异（RD） | -0.166667 |'
- en: '| TE | Treatment equality (TE) | -0.25 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| TE | 处理平等性（TE） | -0.25 |'
- en: In addition, SageMaker Clarify generates *analysis.json* with bias metrics and
    *report.ipynb* to visualize the bias metrics and share with our colleagues. The
    processing job also generates a bias baseline that we will use with SageMaker
    Model Monitor to detect drifts in bias on live model endpoints. We will describe
    this in more detail in [Chapter 9](ch09.html#deploy_models_to_production).
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，SageMaker Clarify生成了*analysis.json*与偏倚度量，以及*report.ipynb*用于可视化偏倚度量并与同事分享。处理工作还生成了一个偏倚基线，我们将与SageMaker
    Model Monitor一起使用，以检测在线模型端点上偏倚的漂移。我们将在[第9章](ch09.html#deploy_models_to_production)中详细描述这一点。
- en: Feature Attribution and Importance with SageMaker Clarify and SHAP
  id: totrans-412
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SageMaker Clarify和SHAP进行特征归因和重要性分析
- en: 'SageMaker Clarify also supports SHAP, a concept from game theory applied to
    machine learning content, to determine the contribution that each feature makes
    to a model’s prediction. We can use this information to select features or create
    new feature combinations. Following is the code to perform feature attribution
    and model explainability with a SageMaker Clarify Processing Job:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Clarify还支持SHAP，这是从博弈论引入到机器学习内容中的概念，用于确定每个特征对模型预测的贡献。我们可以使用这些信息来选择特征或创建新的特征组合。以下是使用SageMaker
    Clarify处理作业执行特征归因和模型可解释性的代码：
- en: '[PRE52]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'In addition to *analysis.json* and *report.ipynb*, the processing job generates
    *explanations_shap/out.csv* with SHAP values for each feature and predicted label
    in the dataset. Here is a relevant snippet from *analysis.json* for the feature
    attributions and explanations:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 除了*analysis.json*和*report.ipynb*，处理工作还生成了*explanations_shap/out.csv*，其中包含数据集中每个特征和预测标签的SHAP值。以下是*analysis.json*中关于特征归因和解释的相关片段：
- en: '[PRE53]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: We can also see the aggregated SHAP value for each feature visualized in SageMaker
    Studio, as shown in [Figure 7-33](#sagemaker_studio_showing_feature_import). This
    represents the importance of each feature toward the prediction.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在SageMaker Studio中看到每个特征的聚合SHAP值的可视化，如[图 7-33](#sagemaker_studio_showing_feature_import)所示。这代表了每个特征对预测的重要性。
- en: '![](assets/dsaw_0733.png)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0733.png)'
- en: Figure 7-33\. SageMaker Studio showing feature importance.
  id: totrans-419
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-33\. SageMaker Studio展示的特征重要性。
- en: The processing job also generates an explainability baseline that we will use
    with SageMaker Model Monitor to detect drifts in feature attribution and model
    explainability on live model endpoints. We will describe this in more detail in
    [Chapter 9](ch09.html#deploy_models_to_production).
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 处理工作还生成一个解释性基准，我们将与SageMaker Model Monitor一起使用，以检测在线模型端点上特征归因和模型可解释性的漂移。我们将在[第9章](ch09.html#deploy_models_to_production)中详细描述这一点。
- en: More Training Options for BERT
  id: totrans-421
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT的更多训练选项
- en: While this book uses a lot of TensorFlow examples, SageMaker supports other
    popular AI and machine learning frameworks, including PyTorch and Apache MXNet,
    as we will discuss in the next few sections. We will also demonstrate how to train
    deep learning models with Java using AWS’s open source [Deep Java Library](https://djl.ai).
    This is useful for enterprises looking to integrate deep learning into their Java-based
    applications.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书使用了大量的TensorFlow示例，SageMaker支持其他流行的AI和机器学习框架，包括PyTorch和Apache MXNet，正如我们将在接下来的几节中讨论的那样。我们还将演示如何使用AWS开源的[Deep
    Java Library](https://djl.ai)在Java中训练深度学习模型。这对于希望将深度学习集成到基于Java的应用程序中的企业非常有用。
- en: Convert TensorFlow BERT Model to PyTorch
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将TensorFlow BERT模型转换为PyTorch
- en: 'In some cases, we may want to try out a different framework to see if we see
    better training or inference performance. Since we are using the popular Transformers
    library for BERT, we can convert our model from TensorFlow to PyTorch in just
    a few lines of code:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们可能希望尝试不同的框架，以查看是否能获得更好的训练或推断性能。由于我们使用流行的Transformers库进行BERT，我们可以在几行代码中将我们的模型从TensorFlow转换为PyTorch：
- en: '[PRE54]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Note
  id: totrans-426
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We can also convert a PyTorch model to TensorFlow. This is a feature of the
    Transformers library and will not work for non-Transformers-based PyTorch and
    TensorFlow models.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将PyTorch模型转换为TensorFlow。这是Transformers库的一个功能，不适用于非基于Transformers的PyTorch和TensorFlow模型。
- en: 'After converting the model, we have a PyTorch version of the same model that
    we trained with TensorFlow—using the same weights. We will deploy this PyTorch
    model using the TorchServe runtime in [Chapter 9](ch09.html#deploy_models_to_production).
    TorchServe was built and optimized by AWS, Facebook, and the PyTorch Community
    to serve PyTorch model predictions and scale across AWS’s elastic infrastructure:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 在转换模型之后，我们得到了与TensorFlow训练的相同模型的PyTorch版本，使用相同的权重。我们将使用TorchServe运行时在[第9章](ch09.html#deploy_models_to_production)中部署此PyTorch模型。TorchServe由AWS、Facebook和PyTorch社区共同构建和优化，用于提供PyTorch模型预测并在AWS的弹性基础设施上扩展：
- en: '[PRE55]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Train PyTorch BERT Models with SageMaker
  id: totrans-430
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SageMaker训练PyTorch BERT模型
- en: 'PyTorch is a popular deep-learning framework with a large community of contributors
    from many companies like Facebook and AWS. PyTorch is natively supported by SageMaker,
    including distributed model training, debugging, profiling, hyper-parameter tuning,
    and model inference endpoints. Following are snippets of code that train a DistilBERT
    PyTorch model on SageMaker and save the code to S3 to deploy in [Chapter 9](ch09.html#deploy_models_to_production).
    The complete code is available in the GitHub repository for this book:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch是一个流行的深度学习框架，拥有来自多家公司（如Facebook和AWS）的大量贡献者社区。PyTorch在SageMaker中得到了原生支持，包括分布式模型训练、调试、性能分析、超参数调整和模型推断端点。以下是在SageMaker上训练DistilBERT
    PyTorch模型并将代码保存到S3以部署在[第9章](ch09.html#deploy_models_to_production)中的代码片段。本书的GitHub存储库中提供了完整的代码：
- en: '[PRE56]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Following is the Python *train.py* script that sets up the network and trains
    the model. Notice that we are using the PyTorch `DistilBertForSequenceClassification`
    and not the TensorFlow `TFDistilBertForSequenceClassification`, which uses a `TF`
    prefix to differentiate the implementation:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Python *train.py*脚本，设置网络并训练模型。请注意，我们使用的是PyTorch的`DistilBertForSequenceClassification`，而不是使用`TF`前缀来区分实现的TensorFlow的`TFDistilBertForSequenceClassification`：
- en: '[PRE57]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Train Apache MXNet BERT Models with SageMaker
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SageMaker训练Apache MXNet BERT模型
- en: 'MXNet is another popular deep-learning framework used heavily within Amazon
    and AWS for many different use cases, including demand forecast, shipping logistics,
    infrastructure resource optimization, natural language processing, computer vision,
    fraud detection, and much more. MXNet is natively supported by SageMaker, including
    distributed training, debugging, profiling, hyper-parameter tuning, and model
    inference endpoints. Following is the code to train a BERT model with MXNet:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet是另一个流行的深度学习框架，在Amazon和AWS内部广泛使用于许多不同的用例，包括需求预测、航运物流、基础设施资源优化、自然语言处理、计算机视觉、欺诈检测等。MXNet在SageMaker中得到了原生支持，包括分布式训练、调试、性能分析、超参数调整和模型推断端点。以下是使用MXNet训练BERT模型的代码：
- en: '[PRE58]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Train BERT Models with PyTorch and AWS Deep Java Library
  id: totrans-438
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PyTorch和AWS的Deep Java库训练BERT模型
- en: While Python and C are the dominant languages for data science, there are scenarios
    that require integration with the billions of lines of Java code that have been
    written since Java’s inception in the 1990s. Additionally, a lot of big data frameworks
    such as Apache Hadoop, Spark, and ElasticSearch are implemented in Java. Following
    are a series of code snippets that demonstrate how to train a BERT model with
    PyTorch using AWS Deep Learning Java that invokes TensorFlow, PyTorch, and Apache
    MXNet libraries from Java using the Java Native Interface. These examples are
    derived from the [Deep Java Library GitHub repository](https://oreil.ly/eVeQY).
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Python 和 C 是数据科学的主要语言，但有些情况需要与自 1990 年代以来编写的数十亿行 Java 代码集成。此外，许多大数据框架如 Apache
    Hadoop、Spark 和 ElasticSearch 都是用 Java 实现的。以下是一系列代码片段，演示如何使用 AWS Deep Learning
    Java 使用 Java Native Interface 从 Java 调用 TensorFlow、PyTorch 和 Apache MXNet 库来训练
    BERT 模型。这些示例来自于 [Deep Java Library GitHub 仓库](https://oreil.ly/eVeQY)。
- en: 'First, let’s define a sizable amount of imports:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义大量的导入：
- en: '[PRE59]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Next, we define a Java class to transform raw text into BERT embeddings:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个 Java 类来将原始文本转换为 BERT 嵌入：
- en: '[PRE60]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Let’s define a function to retrieve our Amazon Customer Reviews Dataset. For
    this example, we use the `Digital_Software` product category:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个函数来检索我们的亚马逊客户评论数据集。在本例中，我们使用 `Digital_Software` 产品类别：
- en: '[PRE61]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Now we retrieve a pre-trained DistilBERT PyTorch model from the Deep Java Library
    model zoo:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们从 Deep Java Library 模型仓库中获取一个预训练的 DistilBERT PyTorch 模型：
- en: '[PRE62]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Let’s construct our model to fine-tune DistilBERT with our Amazon Customer
    Reviews Dataset:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建我们的模型，用我们的亚马逊客户评论数据集来微调 DistilBERT：
- en: '[PRE63]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Finally, let’s tie everything together, transform our dataset into BERT embeddings,
    set up a Checkpoint callback listener, and train our BERT-based review classifier
    with Java!
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们将所有内容结合在一起，将我们的数据集转换为 BERT 嵌入，设置一个检查点回调监听器，并使用 Java 训练我们基于 BERT 的评论分类器！
- en: '[PRE64]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We can run some sample predictions using a custom `Translator` class that uses
    a DistilBERT tokenizer to transform raw text into BERT embeddings:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用自定义的 `Translator` 类运行一些样本预测，该类使用 DistilBERT 分词器将原始文本转换为 BERT 嵌入：
- en: '[PRE65]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Reduce Cost and Increase Performance
  id: totrans-454
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降低成本并提升性能
- en: In this section, we provide tips on how to improve performance and reduce costs
    by using hardware- and infrastructure-level optimizations such as reduced precision
    and Spot Instances. Additionally, we describe how to stop training jobs early
    when they stop improving.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供如何通过硬件和基础架构优化（如减少精度和使用 Spot 实例）来提高性能和降低成本的技巧。此外，我们描述了在训练停止改进时如何提前停止训练作业。
- en: Use Small Notebook Instances
  id: totrans-456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用小型笔记本实例
- en: As a best practice, we should do all of our heavy GPU-based computations in
    a SageMaker Processing, Training, or Batch Transform Job instead of our notebook.
    This helps us save money since we can use a smaller instance type for our longer-running
    notebook instance. If we find ourselves using a GPU instance type for our SageMaker
    Notebooks, we can likely save money by switching to a cheaper notebook instance
    type and moving our GPU-based computations into a SageMaker Training or Processing
    Job so that we only pay for the GPU for the duration of the Training or Processing
    Job.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最佳实践，我们应该将所有基于 GPU 的重型计算放在 SageMaker 处理、训练或批处理转换作业中，而不是在我们的笔记本中进行。这帮助我们节省资金，因为我们可以使用较小的实例类型来运行较长时间的笔记本实例。如果发现我们在
    SageMaker 笔记本中使用 GPU 实例类型，可以通过切换到更便宜的笔记本实例类型，并将基于 GPU 的计算移到 SageMaker 训练或处理作业中，在训练或处理作业的持续时间内仅支付
    GPU 的费用，从而节省资金。
- en: Test Model-Training Scripts Locally in the Notebook
  id: totrans-458
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在笔记本中测试模型训练脚本
- en: We can specify `instance_type='local'` in our SageMaker Training Job to run
    the script either inside a SageMaker Notebook—or on our local laptop. This lets
    us “locally” run the training job on a small subset of data in a notebook before
    launching a full-scale SageMaker Training Job. If we run in the notebook, we should
    remember that we are limited to the memory and compute resources of the notebook
    instance. Therefore, we should only run for one or two epochs using a small batch
    size on a subset of the training dataset when training inside of a notebook instance.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 SageMaker 训练作业中可以指定 `instance_type='local'` 来在 SageMaker 笔记本或我们的本地笔记本上运行脚本。这让我们可以在笔记本中“本地”运行训练作业，使用数据集的一个小子集进行一到两个
    epochs 的训练。如果在笔记本中运行，我们应记住我们受限于笔记本实例的内存和计算资源。因此，当在笔记本实例内进行训练时，应该使用较小的批次大小和数据集的子集运行。
- en: Profile Training Jobs with SageMaker Debugger
  id: totrans-460
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SageMaker 调试器分析训练任务
- en: 'Profiler provides valuable insight into bottlenecks of our training jobs and
    provides useful recommendations to fix those bottlenecks. Oftentimes, we are not
    actually compute bound but rather I/O bound. SageMaker Debugger helps identify
    these less-intuitive bottlenecks with actual data to help us increase resource
    utilization, decrease training times, and reduce cost. In this example, SageMaker
    Debugger identified a CPU bottleneck and suggests that we add more data loaders
    or enable more aggressive data prefetching:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: Profiler 提供了对我们的训练任务瓶颈的宝贵见解，并提供了有用的建议来解决这些瓶颈。通常情况下，我们实际上并不是计算受限，而是 I/O 受限。SageMaker
    调试器通过实际数据帮助我们识别这些较不直观的瓶颈，以帮助我们增加资源利用率，减少训练时间并降低成本。在这个例子中，SageMaker 调试器识别出一个 CPU
    瓶颈，并建议我们添加更多的数据加载器或启用更积极的数据预取：
- en: '[PRE66]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'SageMaker Debugger also suggests using a smaller instance or increasing the
    batch size since our GPU utilization is low:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 调试器还建议使用较小的实例或增加批处理大小，因为我们的 GPU 利用率较低：
- en: '[PRE67]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Start with a Pre-Trained Model
  id: totrans-465
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '-   使用预训练模型'
- en: Fine-tuning a pre-trained model like BERT can save us lots of time and money
    by letting us avoid tasks that have already been done for us. In some cases where
    our domain uses a vastly different language model than an option like BERT, we
    may need to train a model from scratch. However, we should try these pre-trained
    models first and see how far they get us.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 调整预训练模型如 BERT 可以通过避免已为我们完成的任务节省大量时间和金钱。在我们的领域使用与 BERT 等选项大不相同的语言模型的一些情况下，我们可能需要从头开始训练一个模型。然而，我们应该首先尝试这些预训练模型，看看它们能为我们提供多大帮助。
- en: Use 16-Bit Half Precision and bfloat16
  id: totrans-467
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 16 位半精度和 bfloat16
- en: Most models store parameters and perform calculations using full 32-bit numerical
    precision. Intuitively, if we reduce the precision to 16-bit or “reduced” or “half”
    precision, we would not only reduce the footprint of the stored parameters by
    half but also increase computation performance by 2x as the chip can perform two
    16-bit calculations on the same 32-bit hardware.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数模型使用完整的 32 位数值精度存储参数并进行计算。直觉上，如果我们将精度降低到 16 位或“减少”或“半精度”，不仅可以将存储参数的占用空间减少一半，而且由于芯片可以在同样的
    32 位硬件上执行两个 16 位计算，计算性能也会提高 2 倍。
- en: Another reduced precision 16-bit float, `bfloat16`, is a truncated version of
    `float32` that preserves the 8-bit exponent portion of a `float32` but leaves
    only 7 bits for the fraction. Note that `bfloat` is not IEEE compliant; however
    it is natively supported in modern chips from ARM, Intel, Google, and Amazon.
    [Figure 7-34](#comparison_of_floatonesixcomma_floatthr) shows a comparison of
    `float16`, `float32`, and `bfloat16`, including the number of bits used to represent
    the exponent and fraction.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个降低精度的 16 位浮点数，`bfloat16`，是 `float32` 的截断版本，保留了 `float32` 的 8 位指数部分，但只留下了
    7 位用于小数部分。请注意，`bfloat` 不符合 IEEE 标准；然而，在 ARM、Intel、Google 和 Amazon 等现代芯片中都得到了原生支持。[图
    7-34](#comparison_of_floatonesixcomma_floatthr) 显示了 `float16`、`float32` 和 `bfloat16`
    的比较，包括用于表示指数和小数部分的位数。
- en: There are downsides to reduced precision, however. While the training times
    go toward zero in this perfect world, so can accuracy and numeric instability.
    By reducing the numerical precision to 16 bits, our model may not learn as well
    as a 32-bit model. Additionally, we may experience more frequent vanishing gradients
    as the model only has 16 bits to represent the parameters and gradients. So the
    chance of the value going to 0 is much higher than using 32 bits. `bfloat16` reduces
    the chance of vanishing gradients by preserving the dynamic range of a `float32`
    through the 8-bit exponent. We can also use loss-scaling policies to reduce the
    potential for vanishing gradients.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 降低精度也有其不利因素。在这个完美的世界中，虽然训练时间趋向于零，但准确性和数值不稳定性也会如此。通过将数值精度降低到 16 位，我们的模型可能学习能力不及
    32 位模型。此外，由于模型只有 16 位来表示参数和梯度，我们可能会遇到更频繁的梯度消失现象。因此，数值值变为 0 的机会比使用 32 位要高得多。`bfloat16`通过保留
    `float32` 的 8 位指数来减少梯度消失的可能性。我们还可以使用损失缩放策略来减少梯度消失的潜力。
- en: '![](assets/dsaw_0734.png)'
  id: totrans-471
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0734.png)'
- en: 'Figure 7-34\. Comparison of `float16`, `float32`, and `bfloat`. Source: [Wikipedia](https://oreil.ly/8W544).'
  id: totrans-472
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-34。`float16`、`float32` 和 `bfloat` 的比较。来源：[维基百科](https://oreil.ly/8W544)。
- en: When deploying models to tiny devices with limited memory, we may need to reduce
    precision to 8 bit, 4 bit, or even 1 bit for our floats. The challenge is preserving
    accuracy at this lower precision.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 在将模型部署到内存有限的小型设备时，我们可能需要将浮点数的精度降低到8位、4位，甚至1位。挑战在于在这种较低精度下保持准确性。
- en: Mixed 32-Bit Full and 16-Bit Half Precision
  id: totrans-474
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合32位全精度和16位半精度
- en: The choice of 32 bit or 16 bit is yet another hyper-parameter to optimize. Some
    algorithms and datasets may be more sensitive to reduced precision than others.
    However, there is a middle ground called “mixed” precision that stores the parameters
    in 32 bits with “full precision” to maintain numerical stability but performs
    the calculations using 16-bit operations in “half precision.” Ideally, half precision
    would lead to 2x speed-up in operations utilizing half the memory. However, in
    practice we see less-than-ideal improvements due to overhead.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 选择32位或16位是另一个需要优化的超参数。一些算法和数据集对降低精度更为敏感。然而，有一个叫做“混合精度”的中间地带，它以32位“全精度”存储参数以保持数值稳定性，但使用16位“半精度”进行计算。理想情况下，半精度可以使操作速度提升2倍，同时减少一半内存的使用。然而，实际上由于开销问题，我们看到的改善并不尽如人意。
- en: 'TensorFlow and Keras offer native mixed-precision support at the network-layer
    level. Here, we set the global policy for all layers using an automatic mixed-precision
    “policy” that allows the framework to decide which layers and operations should
    utilize 16-bit half precision:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow和Keras在网络层级别提供本地的混合精度支持。在这里，我们设置全局策略，使用自动混合精度“策略”，允许框架决定哪些层和操作应该使用16位半精度：
- en: '[PRE68]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: This is effectively a “turbo button” for model training; however, we should
    treat this like any other hyper-parameter and tune it for our specific dataset
    and algorithm.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是模型训练的“加速按钮”；然而，我们应该像对待任何其他超参数一样来处理它，并针对我们特定的数据集和算法进行调优。
- en: Quantization
  id: totrans-479
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化
- en: In the future chapter on model deployment, we will describe how to reduce the
    precision of a model from 32 bit to 16 bit after training to reduce the size of
    the model and speed up the computations. The quantization process uses statistical
    methods—rooted in audio signal processing—to preserve the dynamic range of the
    parameter values. While not required, we can modify our training script to be
    “quantization-aware” and prepare the model for the post-training quantization.
    This helps to preserve model accuracy after quantizing.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型部署的未来章节中，我们将描述如何在训练后将模型的精度从32位减少到16位，以减小模型大小并加快计算速度。量化过程使用根据音频信号处理而来的统计方法，以保留参数值的动态范围。虽然不是必需的，我们可以修改我们的训练脚本以“量化感知”方式准备模型，以在量化后保持模型准确性。
- en: Use Training-Optimized Hardware
  id: totrans-481
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用训练优化的硬件
- en: AWS Trainium is a training-optimized chip designed to accelerate model-training
    workloads for popular deep learning frameworks, including TensorFlow, PyTorch,
    and Apache MXNet. AWS Trainium uses the AWS Neuron SDK and supports autocasting
    of 32-bit full-precision floating points to 16-bit `bfloat`s to increase throughput
    and reduce cost.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Trainium是一款专为流行的深度学习框架（包括TensorFlow、PyTorch和Apache MXNet）加速模型训练工作负载而设计的训练优化芯片。AWS
    Trainium使用AWS Neuron SDK，并支持将32位全精度浮点数自动转换为16位的`bfloat`以增加吞吐量并降低成本。
- en: Spot Instances and Checkpoints
  id: totrans-483
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spot实例和检查点
- en: 'If we are using an algorithm that supports checkpointing, such as TensorFlow,
    PyTorch, and Apache MXNet, we can use Spot Instances with SageMaker Training Jobs
    to save cost. Spot Instances are cheaper than on-demand instances. To train with
    Spot Instances, we specify `use_spot_instances=True` in our estimator, as shown
    here:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用支持检查点的算法（如TensorFlow、PyTorch和Apache MXNet），我们可以使用SageMaker训练作业的Spot实例来节省成本。Spot实例比按需实例更便宜。要使用Spot实例进行训练，我们在估算器中指定`use_spot_instances=True`，如下所示：
- en: '[PRE69]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Spot Instances may be terminated while the training job is running. Using the
    `max_wait` parameter, SageMaker will wait `max_wait` seconds for new Spot Instances
    to replace the previously terminated Spot Instances. After `max_wait` seconds,
    the job will end. The latest checkpoint is used to begin training from the point
    in time when the Spot Instances were terminated.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练作业正在运行时，Spot实例可能会被终止。使用`max_wait`参数，SageMaker将等待`max_wait`秒以获取新的Spot实例来替换先前终止的Spot实例。超过`max_wait`秒后，作业将结束。最新的检查点用于从Spot实例被终止的时间点开始训练。
- en: '[Figure 7-35](#sagemaker_continues_to_train_when_spot) shows an example of
    one Spot Instance replaced at Time 0 and three Spot Instances replaced at Time
    1\. However, the replacement cadence is driven by supply and demand of Spot Instances
    and is somewhat difficult to predict. Single-instance training jobs that use checkpoints
    can also benefit from Spot Instance savings.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-35](#sagemaker_continues_to_train_when_spot) 展示了一个示例，其中一个 Spot 实例在时间 0
    被替换，三个 Spot 实例在时间 1 被替换。然而，替换节奏受到 Spot 实例供需驱动，预测起来有一定难度。使用检查点的单实例训练作业也可以从 Spot
    实例的节省中受益。'
- en: '![](assets/dsaw_0735.png)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0735.png)'
- en: Figure 7-35\. Use checkpoints to continue training when spot instances are replaced.
  id: totrans-489
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-35\. 使用检查点在 Spot 实例被替换时继续训练。
- en: 'Our script then leverages the provided checkpoint location to save a checkpoint
    using a Keras `ModelCheckpoint` as follows:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们的脚本利用提供的检查点位置，使用 Keras 的 `ModelCheckpoint` 来保存检查点，如下所示：
- en: '[PRE70]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'To load the model, our script uses the checkpoint location to load the model
    as follows:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载模型，我们的脚本使用检查点位置来加载模型，如下所示：
- en: '[PRE71]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Early Stopping Rule in SageMaker Debugger
  id: totrans-494
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMaker Debugger 中的早停规则
- en: SageMaker Debugger supports a number of built-in actions to execute when a rule
    fires. For example, the `StopTraining()` action reduces cost by stopping a training
    job when the objective metric (e.g., accuracy) plateaus and no longer improves
    with additional training. The plateau is detected by a rule such as `overfit`.
    We configure the rule in terms of relative change over a given time or number
    of steps. For example, if our accuracy does not improve by 1% over one thousand
    steps, we want to stop the training job and save some money.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Debugger 支持多种内置操作，在规则触发时执行。例如，`StopTraining()` 操作通过在训练的目标指标（例如准确率）达到平稳期且随后没有进一步改善时停止训练作业来降低成本。平稳期由诸如
    `overfit` 的规则检测到。我们配置规则，以相对变化的时间或步数为基准。例如，如果我们的准确率在一千步内没有提高 1%，我们希望停止训练作业并节省一些费用。
- en: The `StopTraining()` action will end the training job abruptly when the rule
    is triggered. Similar to using Spot Instances, we should use checkpoints, specifically
    the last checkpoint before the job was stopped early.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 当触发规则时，`StopTraining()` 操作会突然结束训练作业。与使用 Spot 实例类似，我们应该使用检查点，特别是在作业提前停止前的最后一个检查点。
- en: Summary
  id: totrans-497
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we trained our first model using the Keras API within TensorFlow
    2.x, BERT, and Amazon SageMaker. We dove deep into the SageMaker infrastructure,
    model development SDKs, and SageMaker Training Jobs. We trained a model using
    SageMaker, described security best practices, and explored some cost-saving and
    performance-improvement tips.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用了 TensorFlow 2.x 的 Keras API、BERT 和 Amazon SageMaker 训练了我们的第一个模型。我们深入了解了
    SageMaker 的基础架构、模型开发 SDK 和 SageMaker 训练作业。我们使用 SageMaker 训练了一个模型，描述了安全最佳实践，并探讨了一些节省成本和提升性能的技巧。
- en: We also learned how BERT’s Transformer neural-network architecture has revolutionized
    the field of NLP and NLU by using a bidirectional approach to learn a contextual
    representation of the words in a corpus. We demonstrated how to fine-tune a pre-trained
    BERT model to build a domain-specific text classifier for product reviews. This
    is in contrast to the previous generation of NLP models such as Word2Vec, GloVe,
    and ELMo that either (1) learn only in one direction at a time, (2) throw away
    the original model and preserve only the learned embeddings, or (3) use complex
    recurrent neural network (RNNs) architectures that require a large amount of memory
    and compute.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还了解了 BERT 的 Transformer 神经网络架构如何通过双向方法学习语料库中单词的上下文表示，从而革新了自然语言处理（NLP）和自然语言理解（NLU）领域。我们演示了如何微调预训练的
    BERT 模型，以构建一个针对产品评论的领域特定文本分类器。这与前一代 NLP 模型（如 Word2Vec、GloVe 和 ELMo）形成对比，前者要么（1）仅一次性单向学习，（2）丢弃原始模型并仅保留学习的嵌入，或者（3）使用需要大量内存和计算的复杂循环神经网络（RNN）架构。
- en: In [Chapter 8](ch08.html#train_and_optimize_models_at_scale), we will retrain
    our model using different configurations and hyper-parameters in a process called
    hyper-parameter optimization or hyper-parameter tuning. Through this process,
    we will find the best model and hyper-parameter combination that provides the
    highest accuracy. We will optimize models even further to take advantage of hardware
    optimizations provided by our target deployment hardware, such as NVIDIA GPUs
    or AWS Inferentia chips. In [Chapter 9](ch09.html#deploy_models_to_production),
    we will deploy and monitor our model in production. In [Chapter 10](ch10.html#pipelines_and_mlops),
    we build an end-to-end pipeline for our model with SageMaker Pipelines, AWS Step
    Functions, Apache Airflow, Kubeflow, and other open source options.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](ch08.html#train_and_optimize_models_at_scale)，我们将使用不同的配置和超参数重新训练我们的模型，这个过程称为超参数优化或超参数调整。通过这个过程，我们将找到提供最高准确率的最佳模型和超参数组合。我们还将进一步优化模型，以利用目标部署硬件（如NVIDIA
    GPU或AWS Inferentia芯片）提供的硬件优化。在[第9章](ch09.html#deploy_models_to_production)，我们将在生产环境中部署和监控我们的模型。在[第10章](ch10.html#pipelines_and_mlops)，我们将使用SageMaker
    Pipelines、AWS Step Functions、Apache Airflow、Kubeflow和其他开源选项为我们的模型构建端到端的流水线。
