- en: Chapter 7\. Integrating Monitoring with Data Tools and Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。将监控与数据工具和系统集成
- en: 'As we discussed in [Chapter 1](ch01.html#the_data_quality_imperative), data
    quality monitoring doesn’t exist in a vacuum—it’s a key part of an organization’s
    data stack, which consists of many different systems and components. This means
    that integrations are an essential part of any data quality monitoring platform.
    There are two flavors of integrations: table stakes and differentiators. Table
    stakes integrations with data warehouses/data lakes and ETL tools are necessary
    for your platform to detect data quality issues for data at rest and data in motion.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第一章](ch01.html#the_data_quality_imperative)讨论的，数据质量监控并非孤立存在，它是组织数据堆栈的关键部分，包含许多不同的系统和组件。这意味着集成是任何数据质量监控平台的重要部分。有两种类型的集成：基础功能和差异化功能。基础功能的集成与数据仓库/数据湖和ETL工具是必需的，以便您的平台能够检测静态数据和动态数据的数据质量问题。
- en: 'Then there are the differentiators: the integrations that aren’t necessary
    to detect data quality problems but add a lot of secondary value. Examples include
    integrations with data catalogs and analytics or business intelligence (BI) tools.
    These ensure that when someone is looking at data in a different context, outside
    of the UX of the data quality fmonitoring platform, they can immediately understand
    whether that data is high quality.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然后还有差异化功能：这些集成并非必要用于检测数据质量问题，但在提供大量次要价值时非常重要。例如，与数据目录、分析或商业智能（BI）工具的集成。这些确保当某人在数据质量监控平台的数据UX之外的不同上下文中查看数据时，他们可以立即了解该数据的质量是否高。
- en: 'In this chapter, we’ll explore how to integrate with:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何与以下工具集成：
- en: Data warehouses like Snowflake and Databricks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像Snowflake和Databricks这样的数据仓库
- en: Data orchestrators like Apache Airflow and dbt
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像Apache Airflow和dbt这样的数据编排器
- en: Data catalogs like Alation and Databricks Unity Catalog
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像Alation和Databricks Unity Catalog这样的数据目录
- en: Data consumers like BI dashboards and machine learning models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像BI仪表板和机器学习模型这样的数据消费者
- en: We’ll explain why you might want to integrate with each category of tool, and
    we’ll walk through the steps needed to integrate successfully. But first, let’s
    take a big-picture look at how data quality monitoring fits into the modern data
    stack.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将解释为什么您可能希望与每类工具集成，并逐步介绍成功集成所需的步骤。但首先，让我们全局看一下数据质量监控如何融入现代数据堆栈。
- en: Monitoring Your Data Stack
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控您的数据堆栈
- en: 'An enterprise’s data stack typically consists of the following systems:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一个企业的数据堆栈通常包括以下系统：
- en: Raw data sources such as events, logs, SaaS apps, and third-party data feeds
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件、日志、SaaS应用程序和第三方数据源等原始数据源
- en: Data storage tools such as cloud data warehouses or data lakes
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据存储工具，如云数据仓库或数据湖
- en: Orchestration tools that ETL data from one format into another; for example,
    to prepare operational data for use in analytics tools
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于ETL数据从一种格式转换为另一种格式的编排工具；例如，为了准备操作数据以供分析工具使用
- en: Data catalogs and governance tools for exploring what data is available, understanding
    lineage, and auditing changes
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据目录和治理工具用于探索可用数据、理解血统和审计更改
- en: MLOps infrastructure for  managing data used specifically for machine learning
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专门用于机器学习的MLOps基础设施
- en: BI and analytics tools for gaining insights from the data
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于从数据中获取洞察的BI和分析工具
- en: Most businesses want to deploy data quality monitoring not just in one place
    in their stack, but several. For instance, they may want to have the platform
    continuously monitor key tables in their data warehouse while also integrating
    the platform’s checks as a task in their orchestration workflow so that a transformation
    can’t be completed if it introduces a data quality issue.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数企业希望在其堆栈的多个位置部署数据质量监控，而不仅仅是一个位置。例如，他们可能希望平台持续监控数据仓库中的关键表，同时将平台的检查集成为其编排工作流程中的任务，以确保在引入数据质量问题时无法完成转换。
- en: Additionally, there are places in the stack that may benefit from data quality
    *information*, even if checks aren’t actively happening there. For instance, you
    may want to automatically retrain an ML model if there is a significant shift
    in data quality. Or you might want BI analysts to be able to see when the data
    that feeds a dashboard has unresolved quality issues.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，即使在这些地方没有积极发生检查，堆栈中的某些位置可能也会受益于数据质量*信息*。例如，如果数据质量发生重大变化，您可能希望自动重新训练ML模型。或者，您可能希望BI分析师能够看到供应仪表板的数据存在未解决的质量问题时。
- en: While most tools in the data stack expose interfaces for sending and retrieving
    information, these by and large all look different, and the data itself will be
    in different formats; there’s no standard way of integrating with every data warehouse,
    for instance.  There’s been some encouraging movement in the data ecosystem toward
    standardization for data quality integrations—for example, Alation’s Open Data
    Quality Initiative and dbt’s Semantic Layer. That said, integrations still require
    substantial engineering work and a robust API.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据堆栈中的大多数工具都提供了发送和检索信息的接口，但它们看起来大不相同，并且数据本身将以不同的格式存在；例如，没有标准的方法与每个数据仓库集成。在数据质量集成方面，数据生态系统已经有了一些鼓舞人心的向标准化的运动，例如
    Alation 的开放数据质量倡议和 dbt 的语义层。尽管如此，集成仍然需要大量的工程工作和强大的 API 支持。
- en: Making matters more difficult, most enterprises haven’t settled on a single
    tool for each part of the data stack. Often, there are independent data teams
    operating in different business verticals that have evolved to use different sets
    of tools.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使事情变得更加困难的是，大多数企业在数据堆栈的每个部分都尚未确定单一的工具。通常情况下，不同业务垂直领域中运行的独立数据团队已经演变出使用不同的工具集。
- en: If you’re building a data quality monitoring platform in house, you’ll want
    to audit these tools, estimate the amount of time needed to integrate with each,
    and prioritize your efforts. This might factor into your build versus buy decision
    (see [Chapter 8](ch08.html#operating_your_solution_at_scale) for more details).
    On the other hand, if you are working with a vendor, you will want to ensure they
    support integrations with the tools you care about, without imposing a large amount
    of work on your team.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在内部构建数据质量监控平台，您将希望审计这些工具，估计与每个工具集成所需的时间，并优先考虑您的努力。这可能会影响您的自行构建与外购决策（详见[第
    8 章](ch08.html#operating_your_solution_at_scale)以获取更多详细信息）。另一方面，如果您正在与供应商合作，您将希望确保他们支持与您关心的工具的集成，而不会给您的团队增加大量工作量。
- en: Data Warehouses
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据仓库
- en: Data warehouses (we will use this umbrella term for simplicity; it includes
    other forms of storage such as data lakes) are a table-stakes integration. It’s
    essential to monitor the data coming into the warehouse, as this is the central
    source of truth where data is in its intended “at rest” format. Furthermore, the
    data warehouse feeds downstream data consumers like ML models and analytics dashboards.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库（我们简单地使用这个总称；它包括其他形式的存储，如数据湖）是一个基础的集成。监控进入仓库的数据至关重要，因为这是数据以其预期的“静态”格式存在的中心来源。此外，数据仓库还向下游的数据消费者（如机器学习模型和分析仪表板）提供数据。
- en: Most large enterprises have not standardized around a single data warehouse,
    and it’s usually necessary to support multiple data warehouse integrations. This
    may include legacy on-premises warehouses (e.g., Teradata) if your business plans
    to use one indefinitely or is in the process of migrating to the cloud. Many companies
    will also want to monitor data from their real-time transactional databases (e.g.,
    Postgres, SQL Server, Oracle). Some organizations also choose to use multiple
    cloud data warehouses (e.g., using both Snowflake and Databricks) to take advantage
    of their relative strengths.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数大型企业没有围绕单一的数据仓库进行标准化，通常需要支持多个数据仓库的集成。这可能包括传统的本地数据仓库（例如 Teradata），如果您的业务计划无限期使用其中之一或正在迁移到云上。许多公司还希望监控来自其实时事务性数据库的数据（例如
    Postgres、SQL Server、Oracle）。一些组织还选择同时使用多个云数据仓库（例如同时使用 Snowflake 和 Databricks）以充分利用它们的相对优势。
- en: As we’ll discuss in a moment, data quality monitoring with unsupervised ML is
    especially useful when an organization has multiple data warehouses, because it
    can check that data is accurately replicated across those warehouses.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们稍后讨论时，使用无监督机器学习进行数据质量监控在一个组织拥有多个数据仓库时尤为有用，因为它可以检查数据是否准确地在这些仓库之间复制。
- en: Integrating with Data Warehouses
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与数据仓库集成
- en: The details of the backend integration will vary depending on the data warehouse.
    Most data sources support SQL-based querying, which is often the API through which
    the data is tested, statistics are computed, or samples are extracted from the
    underlying data. However, the SQL dialects can vary dramatically, and the SQL
    required for the rules, statistics, or samples will also need to vary and be tested
    on each supported platform.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 后端集成的详细信息将根据数据仓库的不同而有所不同。大多数数据源支持基于 SQL 的查询，这通常是通过 API 进行数据测试、计算统计信息或从底层数据中提取样本的方式。然而，SQL
    方言可能差异巨大，用于规则、统计或样本的 SQL 也需要在每个支持的平台上进行调整和测试。
- en: In addition, different platforms have different scalability requirements—for
    example, in Presto, it is critically important to include a time-based `WHERE`
    SQL filter in every query. In BigQuery, there is not an unbiased way to sample
    data at random without scanning a large number of rows. Legacy or transactional
    data stores may not respond well to the query load from automated monitoring,
    and the monitoring system may need to be robust to short query time-outs or long
    query queues; in this case, optimization of the SQL queries becomes paramount.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，不同的平台具有不同的可扩展性要求——例如，在 Presto 中，每个查询中都包括基于时间的`WHERE` SQL 过滤器至关重要。在 BigQuery
    中，没有一种无偏的方式可以随机抽样数据，而不需要扫描大量行。传统或事务性数据存储可能无法很好地响应来自自动化监控的查询负载，并且监控系统可能需要能够抵御短查询超时或长查询队列；在这种情况下，优化
    SQL 查询变得至关重要。
- en: 'In general, users will need the following to enable monitoring:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一般情况下，用户需要以下内容来启用监控：
- en: Network connectivity
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 网络连接性
- en: You may need to add the IPs for your monitoring application to the allowlist
    for your data warehouse.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能需要将您的监控应用程序的 IP 地址添加到数据仓库的允许列表中。
- en: Read-access credentials on the database
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库的只读访问凭证
- en: We recommend creating a dedicated service account for monitoring with read-only
    access to the tables you want to monitor.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议为监控创建一个专用服务账户，该账户具有对您要监控的表的只读访问权限。
- en: Beyond this, for many traditional warehouses, users should only need to provide
    the host, port, database name, user ID, and password. Snowflake just requires
    an account ID and database name, as shown in [Figure 7-1](#setting_up_a_snowflake_integration).
    To connect to Databricks, users will need to [generate a personal access token](https://oreil.ly/C8UZR).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于许多传统数据仓库，用户只需提供主机、端口、数据库名称、用户 ID 和密码。Snowflake 只需要帐户 ID 和数据库名称，如[图 7-1](#setting_up_a_snowflake_integration)所示。要连接到
    Databricks，用户需要[生成个人访问令牌](https://oreil.ly/C8UZR)。
- en: '![Setting up a Snowflake integration](assets/adqm_0701.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![设置 Snowflake 集成](assets/adqm_0701.png)'
- en: Figure 7-1\. Setting up a Snowflake integration.
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-1\. 设置 Snowflake 集成。
- en: With the appropriate access, on the backend, you’ll need to set up processes
    that scan the data warehouse for every queryable object that the credentials have
    access to, including tables, views, and materialized views. These objects should
    be organized and presented to users so that they can configure tables for monitoring.
    See [Figure 7-2](#filtering_by_data_source).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通过适当的访问权限，在后端，您需要设置扫描数据仓库以获取每个可查询对象的过程，这些对象是凭证有权访问的，包括表、视图和物化视图。这些对象应该组织和呈现给用户，以便他们可以配置监控表。参见[图
    7-2](#filtering_by_data_source)。
- en: '![Filtering by data source](assets/adqm_0702.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![按数据源筛选](assets/adqm_0702.png)'
- en: Figure 7-2\. Filtering by data source. See a full-sized version of this image
    at [*https://oreil.ly/adqm_7_2*](https://oreil.ly/adqm_7_2).
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-2\. 按数据源筛选。查看此图片的完整版本，请访问[*https://oreil.ly/adqm_7_2*](https://oreil.ly/adqm_7_2)。
- en: As described in [Chapter 2](ch02.html#data_quality_monitoring_strategies_and),
    it’s also important to extract metadata from the data warehouse, such as when
    a table was last updated and what the volume of the last update was, which is
    useful for observability. The metadata is also used for lineage, which is useful
    for root cause analysis. Modern cloud data warehouses will expose much of this
    information via API, such as the Databricks [data lineage API](https://oreil.ly/zTimS).
    It’s much more cost-effective to extract and track metadata than it is to run
    unsupervised ML on the entire data warehouse, so, as we’ve touched on elsewhere
    in the book, most organizations will monitor the majority of tables using metadata-based
    observability, saving deep data quality monitoring for their most important tables.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[第 2 章](ch02.html#data_quality_monitoring_strategies_and)所述，从数据仓库提取元数据也很重要，比如表上次更新的时间以及上次更新的数据量，这对可观察性很有用。元数据还用于血统，这对根本原因分析很有用。现代化的云数据仓库将通过
    API 公开大部分此类信息，例如 Databricks 的[数据血统 API](https://oreil.ly/zTimS)。提取和跟踪元数据比在整个数据仓库上运行无监督的机器学习更具成本效益，因此，正如本书其他地方提到的那样，大多数组织将使用基于元数据的可观察性来监控大部分表格，将深度数据质量监控保留给最重要的表格。
- en: How often should you scan for new objects and metadata in the data warehouse?
    We recommend running this process daily to detect new tables and changes to table
    schema, as this mirrors the frequency at which most data quality checks will occur.
    If you anticipate scenarios where you will want to make updates and see those
    reflected in the tool in real time, it may be useful to provide a way for users
    to manually trigger a data warehouse refresh (e.g., via a REST endpoint).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据仓库中，您应该多频繁地扫描新对象和元数据？我们建议每天运行此过程，以检测新表和表结构的更改，因为这与大多数数据质量检查发生的频率相匹配。如果您预期会有需要实时更新并在工具中看到这些变化的情况，则提供一种让用户手动触发数据仓库刷新的方法可能会很有用（例如，通过
    REST 端点）。
- en: To illustrate further, [Figure 7-3](#a_simplified_example_of_how_the_anomalo)
    shows a simplified example of how the Anomalo architecture integrates with all
    the other systems in a typical deployment and clearly delineates what runs inside
    of the data quality monitoring platform (the dashed box). The platform itself
    can run on a single virtual machine using Open Container Initiative (OCI) containers
    (e.g., via Docker) or, to facilitate horizontal scaling, can be deployed on a
    Kubernetes cluster using a Helm chart.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要进一步说明，[图 7-3](#a_simplified_example_of_how_the_anomalo)展示了 Anomalo 架构如何与典型部署中的所有其他系统集成的简化示例，并清楚地划分了运行在数据质量监控平台内部的内容（虚线框）。该平台本身可以在单个虚拟机上运行，使用开放容器倡议（OCI）容器（例如，通过
    Docker），或者为了水平扩展，可以使用 Helm 图表在 Kubernetes 集群上部署。
- en: '![A simplified example of how the Anomalo architecture integrates with all
    the other systems in a typical deployment](assets/adqm_0703.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![Anomalo 架构如何与典型部署中的所有其他系统集成的简化示例](assets/adqm_0703.png)'
- en: Figure 7-3\. A simplified example of how the Anomalo architecture integrates
    with all the other systems in a typical deployment.
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-3\. Anomalo 架构如何与典型部署中的所有其他系统集成的简化示例。
- en: Starting from the bottom, users interact with the platform via a web frontend
    or API clients, secured via single sign-on (SSO) integrations with enterprise
    identity providers (IdPs) and API keys. The web frontend provides rich graphical
    user interfaces for interacting with the platform from the browser and communicates
    with the web server via HTTPS.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从底部开始，用户通过 Web 前端或 API 客户端与平台交互，通过企业身份提供商（IdP）的单点登录（SSO）集成和 API 密钥进行安全保护。Web
    前端提供了丰富的图形用户界面，用于通过浏览器与平台进行交互，并通过 HTTPS 与 Web 服务器通信。
- en: The web server responds to all the external requests from SSO, APIs, or browser
    sessions via HTTPS. Backend controllers then create jobs that are queued and scheduled
    for execution. There is a dynamic pool of workers that can pick up those jobs
    and process them (these could be metadata retrieval tasks, SQL queries, check
    logic, or ML jobs).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Web 服务器通过 HTTPS 响应来自 SSO、API 或浏览器会话的所有外部请求。然后，后端控制器创建作业，这些作业排队并计划执行。有一个动态工作池可以接手这些作业并处理它们（这些作业可以是元数据检索任务、SQL
    查询、检查逻辑或机器学习作业）。
- en: All of these internal systems maintain a shared state in an internal database
    (we use Postgres). This database can itself be hosted inside of the environment
    (in Docker or Kubernetes) or can be a managed Postgres service like AWS RDS.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些内部系统在内部数据库中维护共享状态（我们使用Postgres）。该数据库本身可以托管在环境内（如Docker或Kubernetes中），也可以是像AWS
    RDS这样的托管Postgres服务。
- en: The bulk of the action takes place in the workers that are performing the checks.
    Those workers will send notifications to external channels (e.g., Slack, Microsoft
    Teams, PagerDuty) and read from the data warehouses, data lakes, or databases
    that are being monitored (e.g., Snowflake, Databricks, BigQuery, Amazon Redshift).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分操作发生在执行检查的工作者中。这些工作者将向外部通道（例如Slack、Microsoft Teams、PagerDuty）发送通知，并从正在监控的数据仓库、数据湖或数据库中读取数据（例如Snowflake、Databricks、BigQuery、Amazon
    Redshift）。
- en: They will also write large objects they process, such as samples of record-level
    data or visualizations they produce, into a cloud object store such as AWS S3\.
    In a typical deployment, this is a file storage bucket that the customer controls.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 他们还会将处理的大型对象，例如记录级数据的样本或生成的可视化内容，写入云对象存储（如AWS S3）。在典型的部署中，这是客户控制的文件存储桶。
- en: When runtime errors are encountered by any of the workers, they write information
    about the exception to an external platform like Sentry, which is used to debug
    the exception (without including any of the customer data).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当任何工作者遇到运行时错误时，它们会将有关异常的信息写入像Sentry这样的外部平台，用于调试异常（不包括任何客户数据）。
- en: Finally, the entire platform can be upgraded automatically. A controller monitors
    the Anomalo container registry for a new deployment tagged to a specific customer
    or environment and then automatically upgrades all of the internal components
    in a safe and reliable way.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，整个平台可以自动升级。控制器监视Anomalo容器注册表，以寻找标记为特定客户或环境的新部署，然后以安全可靠的方式自动升级所有内部组件。
- en: Security
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全性
- en: 'Perhaps it goes without saying, but we’ll say it anyway: when you integrate
    with a business data source, you take on a significant responsibility to protect
    that data, which could contain personally identifiable information (PII) and other
    sensitive information. Depending on your use case, you may need to conform to
    certain compliance and legal requirements around data processing (see an example
    of [our data processing agreement](https://oreil.ly/vCTzJ)).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 或许不用说，但我们还是说一下：当您与业务数据源集成时，您承担了保护数据的重大责任，这些数据可能包含个人身份信息（PII）和其他敏感信息。根据您的使用情况，您可能需要遵守围绕数据处理的特定合规和法律要求（请参阅我们的[数据处理协议示例](https://oreil.ly/vCTzJ)）。
- en: When developing a solution for third-party customers, there are milestones you
    should aim for with respect to data security. SOC 2 certification is the industry
    standard for service providers storing customer data in the cloud. Developed by
    the [AICPA](https://oreil.ly/dEz8X), SOC 2 is an extensive auditing procedure
    that ensures that a company is handling customer data securely and in a manner
    that protects the organization as well as the privacy of its customers.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在为第三方客户开发解决方案时，应该考虑与数据安全相关的里程碑。 SOC 2认证是存储客户数据在云中的服务提供商的行业标准。由[AICPA](https://oreil.ly/dEz8X)开发的SOC
    2是一种广泛的审计程序，确保公司安全地处理客户数据，并保护组织及其客户隐私。
- en: Reconciling Data Across Multiple Warehouses
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在多个数据仓库之间进行数据对账
- en: When your organization leverages multiple data storage solutions, you often
    have duplicate data represented in more than one place and want to ensure that
    the data is the same in both tables. One example is ensuring that data is the
    same before and after a migration.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的组织利用多种数据存储解决方案时，通常在多个位置表示重复数据，并希望在两个表中确保数据在迁移前后是相同的。一个例子是在迁移前后确保数据的一致性。
- en: 'Monitoring to ensure that data is the same usually takes two forms: either
    rule-based testing or unsupervised machine learning.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 监控以确保数据的一致性通常采用两种形式：基于规则的测试或无监督机器学习。
- en: Comparing datasets with rule-based testing
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用基于规则的测试比较数据集
- en: 'If two tables are identical across two data warehouses, there are three conditions
    that must be true:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个数据仓库中的两个表完全相同，则必须满足三个条件：
- en: The schemas are the same. The tables must have the same columns, with the same
    types, in the same order.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架构是相同的。表必须具有相同的列，相同的类型，以相同的顺序排列。
- en: The set of primary keys are the same. The tables’ rows must join one-to-one
    on the primary keys.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主键集是相同的。表的行必须在主键上一对一连接。
- en: Each row has the same values. When joined on the primary keys, the values in
    both tables’ rows must be the same.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一行具有相同的值。在主键连接时，两个表的行中的值必须相同。
- en: If all three of these are true, then the two tables must have entirely identical
    rows. Between the two, there’s no data that has been added, duplicated, lost,
    or modified.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这三个条件都成立，则两个表必须完全相同。在两者之间，没有已添加、重复、丢失或修改的数据。
- en: However, the rule-based testing approach requires the data to be in the same
    data warehouse, such that joins between the tables can be evaluated at scale.
    This is impractical for very large datasets and introduces a risk of false positives
    caused by issues in the ETL process to move the data into the same data warehouse
    platform.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，基于规则的测试方法要求数据位于同一个数据仓库中，以便能够在规模上评估表之间的连接。这对于非常大的数据集来说是不实际的，并且引入了因ETL过程中将数据移动到同一数据仓库平台中的问题而导致的假阳性风险。
- en: Comparing datasets with unsupervised machine learning
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用无监督机器学习比较数据集
- en: In [Chapter 4](ch04.html#automating_data_quality_monitoring_wi), we discussed
    how the unsupervised ML approach for data quality monitoring can also be used
    to compare two datasets on demand. In this case, you would sample data from tables
    in the respective data warehouses, then train a model to predict whether data
    came from warehouse A or warehouse B. By explaining the model’s predictions using
    SHAP values, you can pinpoint and root-cause the differences in the sample data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第四章](ch04.html#automating_data_quality_monitoring_wi)中，我们讨论了无监督机器学习方法如何用于按需比较两个数据集的数据质量监控。在这种情况下，您将从各自的数据仓库中的表中抽取数据样本，然后训练模型来预测数据是来自仓库A还是仓库B。通过使用SHAP值解释模型的预测，您可以精确定位和根本原因样本数据中的差异。
- en: While this approach may not catch *every* row-level issue like a rule will,
    it scales to massive datasets and allows you to compare data from different source
    data warehouses without any ETL process.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种方法可能无法像规则那样捕捉每个行级问题，但它适用于大规模数据集，并允许您比较来自不同源数据仓库的数据，而无需进行任何ETL过程。
- en: Comparing summary statistics
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较摘要统计信息
- en: The final approach for comparing two tables is to compute some summary statistics
    about each table and then compare those statistics to ensure that they are identical.
    For example, you might compute the number of rows and average price grouped by
    product category to compare that product price listing data was consistently replicated
    from one environment to another. This has the advantage of being very scalable
    and allowing you to ensure that the data is identical in some small ways. But
    it cannot compare all the records in the table if the table is large.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 比较两个表的最终方法是计算每个表的一些摘要统计信息，然后比较这些统计信息以确保它们是相同的。例如，您可以计算按产品类别分组的行数和平均价格，以比较产品价格列表数据是否在不同环境之间一致地复制。这种方法具有很好的可伸缩性，并允许您确保某些小方面的数据是相同的。但是，如果表很大，它无法比较表中的所有记录。
- en: In practice, we’ve found a combination of the latter two approaches—ensuring
    the row counts and some key statistics are identical when aggregated, and ensuring
    there are no significant distribution differences using ML when drawing samples—is
    the most scalable and reliable way to reconcile datasets across platforms.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们发现后两种方法的组合——在聚合时确保行数和一些关键统计信息相同，并确保在绘制样本时使用ML没有显著的分布差异——是跨平台数据集对账的最可扩展和可靠的方法。
- en: Data Orchestrators
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据编排器
- en: Different business users will want to see or perform data quality checks in
    different places. In some cases, they’ll want to do data quality checks while
    the data is in flight—in other words, undergoing ingestion and transformation—before
    it gets to the “at rest” state in your data warehouse. Sometimes, they’ll want
    to perform data quality checks even before the data enters the data warehouse
    at all.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的业务用户希望在不同的位置查看或执行数据质量检查。在某些情况下，他们希望在数据传输和转换过程中进行数据质量检查——换句话说，在数据进入您的数据仓库的“静止”状态之前。有时候，他们甚至希望在数据进入数据仓库之前就执行数据质量检查。
- en: This is where orchestrators and ETL tools like Apache Airflow, dbt, Fivetran,
    Databricks Workflows, or Prefect come in.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是编排器和ETL工具如Apache Airflow、dbt、Fivetran、Databricks Workflows或Prefect的作用所在。
- en: Integrating with Orchestrators
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与编排器集成
- en: The standard pattern for integrating with an orchestration or workflow tool
    is to create jobs that allow your monitoring solution to slot into the data orchestration
    DAG. Put simply, the DAG is a series of tasks that complete in a particular order,
    with upstream and downstream dependencies.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 与编排或工作流工具集成的标准模式是创建作业，允许您的监控解决方案插入数据编排DAG。简单地说，DAG是一系列按特定顺序完成的任务，具有上游和下游依赖关系。
- en: A common DAG is to first extract data from the appropriate sources, for example,
    from user logs and a staging environment. These could be the independent tasks
    that kick off the DAG, as seen in [Figure 7-4](#an_example_data_orchestration_dag_that).
    After completing both tasks, data can be merged and transformed into the desired
    format before finally being loaded into the desired place for storage. Orchestration
    tools automate and schedule this process, giving visibility into when a given
    task completes or fails.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的DAG是首先从适当的源中提取数据，例如用户日志和分级环境。这些可能是启动DAG的独立任务，如[图 7-4](#an_example_data_orchestration_dag_that)所示。完成这两个任务后，可以合并和转换数据到所需的格式，最后加载到存储的适当位置。编排工具自动化和调度此过程，提供在特定任务完成或失败时的可见性。
- en: By integrating your monitoring platform with an orchestration tool, you can
    run checks at essentially any point in the DAG. For instance, you might want to
    run checks after the transformation stage, or after the data extraction stage
    (or both).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将您的监控平台与编排工具集成，您可以在DAG的几乎任何时刻运行检查。例如，您可能希望在转换阶段之后或数据提取阶段之后（或两者都）运行检查。
- en: '![An example data orchestration DAG that includes data quality checks before
    publishing data](assets/adqm_0704.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![一个包含数据质量检查的数据编排DAG示例，在发布数据之前](assets/adqm_0704.png)'
- en: Figure 7-4\. An example data orchestration DAG that includes data quality checks
    before publishing data.
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-4\. 一个包含数据质量检查的数据编排DAG示例，在发布数据之前。
- en: 'We recommend that you support three types of functions at a minimum:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议您至少支持三种类型的功能：
- en: Run checks
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 运行检查
- en: Connect to your monitoring solution and run the checks that have been configured
    for the table. In [Figure 7-4](#an_example_data_orchestration_dag_that), this
    is `AnomaloRunCheck`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到您的监控解决方案，并运行已配置为表的检查。在[图 7-4](#an_example_data_orchestration_dag_that)中，这是`AnomaloRunCheck`。
- en: Job sensor
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 作业传感器
- en: You will need some way to determine when your monitoring system has finished
    running checks. We do this via a polling method. In [Figure 7-4](#an_example_data_orchestration_dag_that),
    this is `AnomaloJobComplete`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要一种方法来确定监控系统何时完成运行检查。我们通过轮询方法来实现此目的。在[图 7-4](#an_example_data_orchestration_dag_that)中，这是`AnomaloJobComplete`。
- en: Validate checks
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 校验检查
- en: This allows the user to decide whether they care about all of the checks passing
    or only some of them. Once we get the output of the checks, we can ensure that
    if we have failed any “must-pass” checks, we raise an exception that stops the
    workflow from completing and triggers a notification to the user. In [Figure 7-4](#an_example_data_orchestration_dag_that),
    this step is `AnomaloPassFail`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这使用户可以决定他们是否关心所有检查是否通过，还是仅有些检查通过。一旦我们获得检查的输出，我们可以确保如果未通过任何“必须通过”检查，我们会引发异常，停止工作流的完成并触发向用户的通知。在[图 7-4](#an_example_data_orchestration_dag_that)中，这一步是`AnomaloPassFail`。
- en: 'Often, it’s useful to package this functionality into a library for users of
    the ETL tool. The code behind these functions is not particularly complex. Here
    is an example of an Airflow operator for running checks:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，将此功能打包到ETL工具的库中对用户非常有用。这些功能背后的代码并不特别复杂。这里是一个用于运行检查的Airflow运算符示例：
- en: '[PRE0]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`AnomaloRunCheckOperator` triggers a job that runs all of the checks on a given
    table. `table_name` is the full name of the table in Anomalo, and `anomalo_conn_id`
    is the connection ID to connect to the platform.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`AnomaloRunCheckOperator`触发一个作业，运行给定表中的所有检查。`table_name`是Anomalo中表的完整名称，`anomalo_conn_id`是连接到平台的连接ID。'
- en: The next file we’ll walk through sets up an example Airflow DAG that ingests
    data and then runs checks on it using `AnomaloRunCheckOperator`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将演示如何设置一个示例Airflow DAG，该DAG摄取数据，然后使用`AnomaloRunCheckOperator`对其运行检查。
- en: 'We start by importing the Airflow modules and classes required to define a
    DAG, as well as custom Anomalo operators and sensors:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入定义DAG所需的Airflow模块和类，以及自定义的Anomalo运算符和传感器：
- en: '[PRE1]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we define default arguments for DAG tasks and we set up basic parameters
    for the DAG:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为DAG任务定义默认参数，并设置DAG的基本参数：
- en: '[PRE2]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The DAG consists of several tasks. There is `ingest_transform_data`, the initial
    task to bring in the data and do some operations on it. Once we do that, we’ll
    want to have a task that can run checks on a table using `AnomaloRunCheckOperator`.
    Note that the following steps are all indented, as they are happening inside of
    the Airflow DAG context manager:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: DAG 包括多个任务。有`ingest_transform_data`，这是将数据引入并对其进行一些操作的初始任务。完成后，我们希望有一个任务可以使用`AnomaloRunCheckOperator`在表格上运行检查。请注意，以下步骤均缩进，因为它们发生在Airflow
    DAG上下文管理器内部：
- en: '[PRE3]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As discussed previously, we want to poll until the Anomalo job is complete,
    and we do this with the `anomalo_sensor` task:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前讨论的，我们希望在 Anomalo 作业完成之前轮询，并使用`anomalo_sensor`任务完成这一操作：
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When the job is complete, we will run a task to validate various data quality
    checks and finally publish the data:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 工作完成后，我们将运行一个任务来验证各种数据质量检查，并最终发布数据：
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'With these tasks defined, it’s just a matter of setting up task dependencies
    to form the DAG:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 定义好这些任务后，只需设置任务依赖关系来形成DAG：
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Data Catalogs
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据目录
- en: Since many enterprises today are working with vast volumes of diverse datasets,
    data catalogs are becoming an increasingly essential part of a data practitioner’s
    toolkit. A data catalog provides a centralized view of data assets, metadata,
    and relationships, making it easier to discover, understand, and use data across
    an organization. Examples include Alation, Databricks Unity Catalog, DataHub,
    and Atlan.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 由于今天许多企业正在处理大量不同类型的数据集，数据目录正成为数据从业者工具箱中越来越重要的一部分。数据目录提供了数据资产、元数据和关系的集中视图，使组织更容易发现、理解和使用数据。例如
    Alation、Databricks Unity Catalog、DataHub 和 Atlan。
- en: Integrations between data catalogs and data quality tools are a natural evolution—a
    result of enterprises realizing that understanding their data and ensuring that
    their data is high quality are interrelated goals. Both of these steps can be
    seen as part of data governance, which is a framework that defines the policies,
    processes, and standards for managing data assets across an organization. It ensures
    data is properly accessible, protected, and trusted to meet the needs of the organization
    and its stakeholders.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 数据目录与数据质量工具之间的集成是自然而然的发展——企业意识到了理解其数据和确保数据高质量是相互关联的目标。这两个步骤都可以视为数据治理的一部分，它是一个框架，用于定义组织中管理数据资产的政策、流程和标准。它确保数据得到适当的访问、保护和信任，以满足组织及其利益相关者的需求。
- en: As an example of the continuing push for more connections between data quality
    tools and data catalogs, in 2022, Alation launched the [Open Data Quality Initiative](https://oreil.ly/RSXVl),
    which “offers an open DQ API, developer documentation, onboarding, integration
    best practices, and comarketing support.” They added a Data Quality tab to the
    catalog (see [Figure 7-5](#as_part_of_their_open_data_quality_init)), where data
    quality systems can push information via API.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作为进一步促进数据质量工具与数据目录之间连接的示例，Alation 在 2022 年推出了[开放数据质量倡议](https://oreil.ly/RSXVl)，提供“开放的
    DQ API、开发者文档、入门、集成最佳实践和共同营销支持”。他们在目录中添加了一个数据质量标签（参见[图 7-5](#as_part_of_their_open_data_quality_init)），数据质量系统可以通过
    API 推送信息。
- en: '![As part of their Open Data Quality Initiative, Alation added hooks for showing
    data quality checks.](assets/adqm_0705.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![作为他们的开放数据质量倡议的一部分，Alation添加了显示数据质量检查的钩子。](assets/adqm_0705.png)'
- en: Figure 7-5\. As part of their Open Data Quality Initiative, Alation added hooks
    for showing data quality checks. See a full-sized version of this image at [*https://oreil.ly/adqm_7_5*](https://oreil.ly/adqm_7_5).
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-5。作为他们的开放数据质量倡议的一部分，Alation添加了显示数据质量检查的钩子。查看此图的完整版本，请访问[*https://oreil.ly/adqm_7_5*](https://oreil.ly/adqm_7_5)。
- en: Integrations between catalogs and data quality monitoring can be bidirectional.
    Inside the catalog, it’s very helpful to be able to see information about data
    quality right next to information about the table, without having to switch contexts.
    Alternatively, from within the monitoring platform, it can also be useful to have
    deep links into the catalog to explore table usage and metadata.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 目录与数据质量监控之间的集成可以是双向的。在目录内部，能够直接看到与表格相关的数据质量信息非常有帮助，而无需切换上下文。或者，从监控平台内部，深入到目录中也可以很有用，以探索表格使用情况和元数据。
- en: 'Showing detailed data quality information inside the catalog (see [Figure 7-6](#data_quality_checks_appear_in_the_catal))
    is powerful for several reasons. A data catalog is often a business’s go-to tool
    for exploring datasets that might be helpful for a particular use case, such as
    developing a new analytics dashboard. At this stage, an analyst might be choosing
    from hundreds of tables that they don’t know anything about. It’s essential to
    understand if the data in a given table is any good: Is it valid and approved,
    or has it never been tested? Having data quality details right at an analyst’s
    fingertips can ensure that the business makes decisions backed by trustworthy
    information.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在目录内显示详细的数据质量信息（参见 [图 7-6](#data_quality_checks_appear_in_the_catal)）有几个重要的原因。数据目录通常是企业探索数据集的主要工具，可能有助于特定用例，例如开发新的分析仪表板。在此阶段，分析师可能正在从数百个他们一无所知的表格中进行选择。重要的是了解给定表中的数据是否良好：它是有效和已批准的，还是从未经过测试？在分析师的指尖提供数据质量详细信息可以确保企业基于可信赖的信息做出决策。
- en: '![Data quality checks appear in the catalog’s overview for a table, and any
    issues are flagged in the UI.](assets/adqm_0706.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![数据质量检查显示在表的目录概述中，任何问题都会在 UI 中标记。](assets/adqm_0706.png)'
- en: Figure 7-6\. Data quality checks appear in the catalog’s overview for a table,
    and any issues are flagged in the UI.
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-6\. 数据质量检查显示在表的目录概述中，任何问题都会在 UI 中标记。
- en: Additionally, one of the jobs of a data catalog is to not only collate assets
    but to tell the business which ones are the most popular and commonly used. By
    integrating data quality information into the catalog, it’s possible to quickly
    see which of the popular tables are actually validated. If a manager is running
    an initiative to improve data quality, this can be very helpful for prioritizing
    where to invest resources. Overall, it serves as a gauge of whether the business
    is operating on trustworthy data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据目录的一个任务不仅是整理资产，还要告诉企业哪些资产最受欢迎和常用。通过将数据质量信息整合到目录中，可以快速看到哪些受欢迎的表格实际上已验证。如果管理者正在推动改进数据质量的倡议，这对于确定投资资源的优先级非常有帮助。总体而言，它作为企业是否依赖可信赖数据的一个评估标准。
- en: Integrating with Catalogs
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与目录集成
- en: A baseline integration with a data catalog involves tagging whether tables have
    checks or monitoring set up at all, showing the results of data quality checks
    in the catalog, and deep linking to the monitoring platform for more information.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据目录的基本集成包括标记表格是否完全设置了检查或监控，显示数据质量检查结果在目录中，并深度链接到监控平台获取更多信息。
- en: However, more advanced integrations are becoming possible as catalogs add increased
    support for data quality tools to plug in. For instance, if your monitoring tool
    provides additional visualizations (e.g., root cause analysis), you might consider
    bringing those into the catalog as well.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着目录增加对数据质量工具的增强支持，更先进的集成正在变得可能。例如，如果您的监控工具提供了额外的可视化功能（例如根本原因分析），您可能考虑将其整合到目录中。
- en: Generally, integrating with data catalogs follows a publishing model where the
    data quality monitor will aggregate its results and publish them to a designated
    location that is owned by the catalog. The integration is primarily API based,
    so it’s important to have a secure and enterprise-ready API.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，与数据目录集成遵循一个发布模型，数据质量监视器将汇总其结果并发布到目录拥有的指定位置。集成主要基于 API，因此拥有安全且企业级准备的 API 非常重要。
- en: 'For instance, you can provide a REST API that returns monitoring results as
    JSON. Endpoints might include:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以提供一个返回监控结果的 REST API，格式为 JSON。端点可能包括：
- en: '`GET /get_checks_for_table`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`GET /get_checks_for_table`'
- en: Return a list of all currently configured checks/validations for a given table
    ID.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 返回给定表格 ID 的当前配置检查/验证的列表。
- en: '`POST /run_checks`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`POST /run_checks`'
- en: Manually run all or a subset of checks on a given table.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定表格上手动运行所有或部分检查。
- en: '`GET /get_run_results`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`GET /get_run_results`'
- en: Obtain the results of executed checks from a single run.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 获取单次运行的执行检查结果。
- en: 'For a `GET /get_run_results` call, check metadata you might want to provide
    includes:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `GET /get_run_results` 调用，检查你可能需要提供的元数据包括：
- en: 'Check ID: identifier for a specific data check'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查 ID：特定数据检查的标识符
- en: 'Check run ID: identifier for an instance of a check run'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查运行 ID：检查运行实例的标识符
- en: 'Completion time: when the check run was completed'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完成时间：检查运行完成的时间
- en: 'Creation metadata: information about who created the check and when'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建元数据：关于谁何时创建检查的信息
- en: 'Last edited metadata: information about the last edit, including time and editor'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后编辑的元数据：关于最后编辑的信息，包括时间和编辑者
- en: 'Error status: whether the check encountered an error'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误状态：检查是否遇到错误
- en: 'Evaluation message: text detailing the check evaluation'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估消息：详细描述检查评估情况的文本
- en: 'Exception details: messages and traceback for any exceptions'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常详细信息：任何异常的消息和回溯信息
- en: 'Historical context: summary of the check’s performance over time'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 历史上下文：检查随时间推移的表现摘要
- en: 'Sample data: SQL queries and URLs for good and bad data samples'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本数据：用于良好和坏数据样本的SQL查询和URL
- en: 'Result statistics: quantitative metrics and their names, related to the check'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果统计：与检查相关的定量指标及其名称
- en: 'Success flag: Boolean indicating the check’s success or failure'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成功标志：指示检查成功或失败的布尔值
- en: 'Check configuration: including type, description, and priority level'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查配置：包括类型、描述和优先级级别
- en: 'Triage status: current status in the triage workflow'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类状态：在分类工作流中的当前状态
- en: The trickiest part of the integration, in our experience, is matching table
    IDs correctly, as table names aren’t always a perfect identifier (they may be
    represented slightly differently for different data catalogs) and every enterprise’s
    environment will be slightly different.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的经验中，集成最棘手的部分是正确匹配表ID，因为表名并不总是一个完美的标识符（它们在不同的数据目录中可能会有稍微不同的表示），而且每个企业的环境都会有所不同。
- en: 'You’ll also need to think about scheduling: How often do you want to push results
    to the catalog and/or pull updates from the catalog? As a first step, you can
    run updates using a script and eventually build a scheduler into your platform
    to automate the process daily or on some other cadence.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要考虑安排问题：您希望多频繁地将结果推送到目录和/或从目录中获取更新？作为第一步，您可以使用脚本运行更新，最终将调度程序集成到您的平台中，以便每天或在其他某种节奏上自动化这个过程。
- en: Data Consumers
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据消费者
- en: Though still nascent, another area where data quality integrations are powerful—and
    likely to become more popular—is applications that consume data, such as BI and
    analytics tools and MLOps platforms.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管仍处于初期阶段，数据质量集成的另一个强大领域——并且可能变得更受欢迎——是消费数据的应用程序，如BI和分析工具以及MLOps平台。
- en: Analytics and BI Tools
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析和BI工具
- en: An analytics tool like Tableau, because it serves as the destination where data
    is used, would be a good candidate for showing data quality monitoring information,
    such as whether the dataset that powers a dashboard has been validated as high
    quality. In fact, Tableau has several data quality APIs, such as an API to [add
    a data quality warning](https://oreil.ly/2zMbN). If you use BI dashboards to track
    KPIs, consider syncing this metric monitoring with your data quality tool so that
    you can easily root cause any drift in the underlying data.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 像Tableau这样的分析工具，因为它作为数据使用的目的地，是显示数据质量监控信息的好选择，例如数据集是否已验证为高质量。实际上，Tableau有几个数据质量API，比如一个用于[添加数据质量警告](https://oreil.ly/2zMbN)的API。如果您使用BI仪表板来跟踪关键绩效指标，请考虑将此度量监控与您的数据质量工具同步，以便您可以轻松地找到根本数据漂移的原因。
- en: Data-driven organizations often run into the problem of inconsistent definitions
    for their business metrics. For instance, one part of the business might define
    `view_count` as excluding very short views, say those less than 10 milliseconds
    in duration, while another group might include all views, leading to what appears
    to be a data quality issue—but it’s really just a lack of consistency.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 数据驱动的组织通常会遇到业务指标定义不一致的问题。例如，业务的一部分可能会将`view_count`定义为不包括非常短的视图，比如持续时间少于10毫秒的视图，而另一组可能会包括所有视图，这看起来像是数据质量问题，但实际上只是缺乏一致性。
- en: We’ve been interested in how dbt is tackling this problem with the launch of
    dbt metrics and the dbt Semantic Layer. This allows organizations to centrally
    define key business metrics like revenue, customer count, and churn rate in dbt.
    It’s possible for data quality monitoring tools to be dbt “Metrics Ready integrations”
    and use dbt metrics definitions. For instance, if your tool monitors KPIs, you
    can automatically ingest the metrics that have been defined in dbt.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直对dbt如何通过推出dbt指标和dbt语义层来解决这个问题感兴趣。这使得组织能够在dbt中集中定义关键的业务指标，如收入、客户数量和流失率。数据质量监控工具可以成为dbt“指标就绪集成”，并使用dbt指标定义。例如，如果您的工具监视关键绩效指标，可以自动接收在dbt中定义的指标。
- en: MLOps
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLOps
- en: If your company is using an MLOps platform like Amazon SageMaker, most likely
    you already have some level of model monitoring built in. However, most platforms
    monitor metrics like model performance, latency, and uptime rather than the harder-to-detect
    issues of incorrect data flowing into models and resulting in suboptimal predictions.
    By integrating data quality monitoring with MLOps tools, you could build a system
    that warns data scientists when models need to be retrained because the underlying
    data has drifted, or when there are errors like NULLs in the data that could cause
    a model to behave erratically.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的公司正在使用像 Amazon SageMaker 这样的 MLOps 平台，很可能已经内置了某种模型监控水平。然而，大多数平台监控的指标通常是模型性能、延迟和可用性，而不是数据流入模型时更难检测到的问题，导致预测结果不佳。通过将数据质量监控与
    MLOps 工具集成，您可以构建一个系统，当模型需要重新训练时，警告数据科学家底层数据已发生漂移，或者存在像数据中的 NULL 值这样的错误可能导致模型行为不稳定。
- en: '[Figure 7-7](#example_mlops_architecture_diagram_left) is an example MLOps
    architecture diagram adapted from the Google Cloud Architecture Center.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-7](#example_mlops_architecture_diagram_left) 是从 Google Cloud 架构中心调整过的示例
    MLOps 架构图。'
- en: '![](assets/adqm_0707.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/adqm_0707.png)'
- en: 'Figure 7-7\. Example MLOps architecture diagram (adapted from “MLOps: Continuous
    Delivery and Automation Pipelines in Machine Learning,” Cloud Architecture Center,
    [*https://oreil.ly/rfAmt*](https://oreil.ly/rfAmt)).'
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 7-7\. 示例 MLOps 架构图（来源于“MLOps: Continuous Delivery and Automation Pipelines
    in Machine Learning”，Cloud Architecture Center，[*https://oreil.ly/rfAmt*](https://oreil.ly/rfAmt)）。'
- en: 'This complex flow is designed to automate the continuous integration, delivery,
    and training of ML systems. Here are the points where you might want to introduce
    data quality monitoring:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这种复杂的流程旨在自动化 ML 系统的持续集成、交付和训练。以下是您可能希望引入数据质量监控的几个关键点：
- en: Source data
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 源数据
- en: Source data, typically from a cloud data warehouse or data lake, is transformed
    into signals for the model that are then stored in the feature store. It’s essential
    to monitor this data as it’s the entry point for training ML models.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 源数据通常来自云数据仓库或数据湖，被转换为模型的信号，然后存储在特征存储中。监控这些数据非常重要，因为这是训练 ML 模型的入口点。
- en: Feature store
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储
- en: The feature store may be replicated in your data warehouse, and this is yet
    another set of tables you want to monitor. As discussed in [Chapter 3](ch03.html#assessing_the_business_impact_of_automa),
    you should look in particular for changes in the percentage of NULL values, distribution
    shifts, and correlation changes. Any of these could indicate risks that the feature
    computation is incorrect, or the model might experience shocks that could cause
    unexpected behavior.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储可能已在您的数据仓库中复制，这是您希望监控的另一组表。如 [第 3 章](ch03.html#assessing_the_business_impact_of_automa)
    所讨论的，特别是要查找 NULL 值百分比的变化、分布偏移和相关性变化。这些任何一个都可能表明特征计算不正确，或者模型可能遇到会导致意外行为的冲击。
- en: Training data
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据
- en: Training data is fetched in batch from the feature store. It may be helpful
    to use data quality monitoring to compare the current training snapshot with prior
    training datasets or with production data. You may have some model risk if the
    distribution of the training data has changed suddenly.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据从特征存储中批量获取。使用数据质量监控来比较当前训练快照与先前的训练数据集或生产数据可能会有所帮助。如果训练数据的分布突然发生变化，您可能会面临一些模型风险。
- en: Model performance
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 模型性能
- en: In the model evaluation step of training, you can log data like the training
    and inference times, test set performance, and feature importance scores. Model
    monitoring is ideally integrated into your MLOps toolkit, but there’s nothing
    to stop a data quality platform from also monitoring these metrics if they’re
    available in your data warehouse.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练的模型评估步骤中，您可以记录训练和推断时间、测试集性能和特征重要性分数等数据。模型监控理想情况下已集成到您的 MLOps 工具包中，但如果这些指标在您的数据仓库中可用，数据质量平台也可以监控这些指标。
- en: Feature serving
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 特征服务
- en: When your model is online and making predictions, it can be valuable to fetch
    the features at prediction time, log them, and compare them to the feature values
    used for the latest training run. This can tell you if you’re experiencing a significant
    difference between the training and real-world data, which would mean that your
    model may perform suboptimally in production.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的模型在线并进行预测时，获取预测时的特征、记录它们，并与最新训练运行使用的特征值进行比较，这是非常有价值的。这可以告诉您在训练和实际数据之间是否存在显著差异，这意味着您的模型在生产环境中可能表现不佳。
- en: Predictions and business logic
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 预测和业务逻辑
- en: While you can monitor your machine learning model’s predictions directly, we’d
    argue it’s even more important to monitor the *business logic driven by those
    predictions*. For instance, are you suddenly marking a large percentage of transactions
    as fraudulent? This may indicate that there is a real trend in the external data,
    but it might also mean that your model is performing erratically due to a data
    quality issue.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您可以直接监控机器学习模型的预测，但我们认为监控由这些预测驱动的*业务逻辑*更为重要。例如，您是否突然将大量交易标记为欺诈？这可能表明外部数据中存在真实的趋势，但也可能意味着您的模型由于数据质量问题而表现不稳定。
- en: Conclusion
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: When you automate data quality monitoring at scale, you create ways to validate
    whether that data is high quality (or not). Notifications are the main way to
    resolve data quality issues, but to really get users across your organization
    engaging with your system—and with data quality as a whole at your company—you
    have to go beyond notifications and integrate information and signals from your
    monitoring into your entire data workflow. In this chapter, we’ve given you the
    tools to do so.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在大规模自动化数据质量监控时，您可以创建验证数据质量（或否）的方法。通知是解决数据质量问题的主要方式，但要真正让您组织内的用户与系统互动，并在公司整体上关注数据质量，您需要超越通知，并将监控的信息和信号整合到整个数据工作流程中。在本章中，我们为您提供了这样做的工具。
