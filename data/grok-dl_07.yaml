- en: 'Chapter 8\. Learning signal and ignoring noise: introduction to regularization
    and batching'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第8章。学习信号和忽略噪声：正则化和批处理简介
- en: '**In this chapter**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章内容**'
- en: Overfitting
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合
- en: Dropout
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout
- en: Batch gradient descent
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量梯度下降
- en: “With four parameters I can fit an elephant, and with five I can make him wiggle
    his trunk.”
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “用四个参数我可以画一头大象，用五个参数我甚至可以让它扭动它的鼻子。”
- en: ''
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*John von Neumann, mathematician, physicist, computer scientist, and polymath*'
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*约翰·冯·诺伊曼，数学家、物理学家、计算机科学家和通才*'
- en: Three-layer network on MNIST
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MNIST上的三层网络
- en: Let’s return to the MNIST dataset and attempt to classify it with- h the new
    network
  id: totrans-9
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们回到MNIST数据集，并尝试使用新的网络进行分类
- en: In last several chapters, you’ve learned that neural networks model correlation.
    The hidden layers (the middle one in the three-layer network) can even create
    intermediate correlation to help solve for a task (seemingly out of midair). How
    do you know the network is creating good correlation?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的几章中，你已经了解到神经网络建模相关性。隐藏层（三层网络中的中间层）甚至可以创建中间相关性来帮助解决问题（似乎是凭空而来）。你怎么知道网络正在创建好的相关性呢？
- en: When we discussed stochastic gradient descent with multiple inputs, we ran an
    experiment where we froze one weight and then asked the network to continue training.
    As it was training, the dots found the bottom of the bowls, as it were. You saw
    the weights become adjusted to minimize the error.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论具有多个输入的随机梯度下降时，我们进行了一个实验，其中我们冻结了一个权重，然后要求网络继续训练。在训练过程中，点就像找到了碗底。你看到权重被调整以最小化错误。
- en: When we froze the weight, the frozen weight still found the bottom of the bowl.
    For some reason, the bowl moved so that the frozen weight value became optimal.
    Furthermore, if we unfroze the weight to do some more training, it wouldn’t learn.
    Why? Well, the error had already fallen to 0\. As far as the network was concerned,
    there was nothing more to learn.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将权重冻结时，冻结的权重仍然找到了碗底。由于某种原因，碗移动了，使得冻结的权重值变得最优。此外，如果我们解冻权重进行更多训练，它就不会学习。为什么？嗯，错误已经下降到0。对于网络来说，再也没有什么可以学习的了。
- en: 'This begs the question, what if the input to the frozen weight was important
    to predicting baseball victory in the real world? What if the network had figured
    out a way to accurately predict the games in the training dataset (because that’s
    what networks do: they minimize error), but it somehow forgot to include a valuable
    input?'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这就引出了一个问题，如果冻结的权重的输入对预测现实世界中的棒球胜利很重要怎么办？如果网络已经找到了一种准确预测训练数据集中比赛的方法（因为这就是网络所做的事情：最小化错误），但它却忘记了包含一个有价值的输入怎么办？
- en: Unfortunately, this phenomenon—overfitting—is extremely common in neural networks.
    We could say it’s the archnemesis of neural networks; and the more powerful the
    neural network’s expressive power (more layers and weights), the more prone the
    network is to overfit. An everlasting battle is going on in research, where people
    continually find tasks that need more powerful layers but then have to do lots
    of problem-solving to make sure the network doesn’t overfit.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这种现象——过拟合——在神经网络中非常普遍。我们可以说它是神经网络的宿敌；神经网络的表达能力越强（更多层和权重），就越容易过拟合。在研究中，人们不断发现需要更强大层级的任务，但随后又必须进行大量问题解决以确保网络不过拟合。
- en: In this chapter, we’re going to study the basics of *regularization*, which
    is key to combatting overfitting in neural networks. To do this, we’ll start with
    the most powerful neural network (three-layer network with `relu` hidden layer)
    on the most challenging task (MNIST digit classification).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究正则化的基础知识，这是对抗神经网络过拟合的关键。为此，我们将从最强大的神经网络（具有`relu`隐藏层的三层网络）开始，在最具挑战性的任务（MNIST数字分类）上进行。
- en: To begin, go ahead and train the network, as shown next. You should see the
    same results as those listed. Alas, the network learned to perfectly predict the
    training data. Should we celebrate?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，按照下面的步骤训练网络。你应该看到与列出的相同的结果。唉，网络学会了完美地预测训练数据。我们应该庆祝吗？
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***1* Returns x if x > 0; returns 0 otherwise**'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 当x大于0时返回x；否则返回0**'
- en: '***2* Returns 1 for input > 0; returns 0 otherwise**'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 当输入大于0时返回1；否则返回0**'
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Well, that was easy
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嗯，这很简单
- en: The neural network perfectly learned to predict all 1,000 images
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络完美地学会了预测所有1000张图像
- en: In some ways, this is a real victory. The neural network was able to take a
    dataset of 1,000 images and learn to correlate each input image with the correct
    label.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些方面，这是一个真正的胜利。神经网络能够从1,000张图像的数据集中学习，将每个输入图像与正确的标签相关联。
- en: How did it do this? It iterated through each image, made a prediction, and then
    updated each weight ever so slightly so the prediction was better next time. Doing
    this long enough on all the images eventually reached a state where the network
    could correctly predict on all the images.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 它是如何做到这一点的？它逐个遍历每张图像，做出预测，然后略微更新每个权重，以便下一次预测更好。在所有图像上这样做足够长时间后，网络最终达到了能够正确预测所有图像的状态。
- en: 'Here’s a non-obvious question: how well will the neural network do on an image
    it hasn’t seen before? In other words, how well will it do on an image that wasn’t
    part of the 1,000 images it was trained on? The MNIST dataset has many more images
    than just the 1,000 you trained on; let’s try it.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个不那么明显的问题：神经网络在它之前从未见过的图像上的表现会怎样？换句话说，它在它训练的1,000张图像之外的图像上的表现会怎样？MNIST数据集包含的图像比训练的1,000张图像多得多；让我们试试。
- en: 'In the notebook from the previous code are two variables: `test_images` and
    `test_labels`. If you execute the following code, it will run the neural network
    on these images and evaluate how well the network classifies them:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码笔记本中有两个变量：`test_images` 和 `test_labels`。如果你执行以下代码，它将在这些图像上运行神经网络并评估网络对它们的分类效果：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The network did horribly! It predicted with an accuracy of only 70.7%. Why does
    it do so terribly on these new testing images when it learned to predict with
    100% accuracy on the training data? How strange.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的表现非常糟糕！它的准确率只有70.7%。为什么它在这些新的测试图像上的表现如此糟糕，尽管它在训练数据上学会了以100%的准确率进行预测？真奇怪。
- en: This 70.7% number is called the *test accuracy*. It’s the accuracy of the neural
    network on data the network was *not* trained on. This number is important because
    it simulates how well the neural network will perform if you try to use it in
    the real world (which gives the network only images it hasn’t seen before). This
    is the score that matters.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个70.7%的数字被称为*测试准确率*。这是神经网络在它没有训练过的数据上的准确率。这个数字很重要，因为它模拟了如果你尝试在现实世界中使用神经网络（这给网络只有它之前没有见过的图像）时，神经网络的表现会怎样。这是重要的分数。
- en: Memorization vs. generalization
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 记忆与泛化
- en: Memorizing 1,000 images is easier than generalizing to all images
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 记忆1,000张图像比泛化到所有图像要容易
- en: Let’s consider again how a neural network learns. It adjusts each weight in
    each matrix so the network is better able to take *specific inputs* and make a
    *specific prediction*. Perhaps a better question might be, “If we train it on
    1,000 images, which it learns to predict perfectly, why does it work on other
    images at all?”
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次考虑神经网络是如何学习的。它调整每个矩阵中的每个权重，以便网络能够更好地处理*特定输入*并做出*特定预测*。也许一个更好的问题可能是，“如果我们用1,000张图像来训练它，它学会了完美预测，为什么它还要在其他图像上工作呢？”
- en: As you might expect, when the fully trained neural network is applied to a new
    image, it’s guaranteed to work well only if the new image is *nearly identical
    to an image from the training data*. Why? Because the neural network learned to
    transform input data to output data for only *very specific input configurations*.
    If you give it something that doesn’t look familiar, it will predict randomly.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所预期，当完全训练好的神经网络应用于新图像时，它只有在新的图像几乎与训练数据中的图像完全相同的情况下才能保证表现良好。为什么？因为神经网络学会了将输入数据转换为输出数据，只针对*非常特定的输入配置*。如果你给它一些看起来不熟悉的东西，它将随机预测。
- en: This makes neural networks kind of pointless. What’s the point of a neural network
    working only on the data you trained it on? You already know the correct classifications
    for those datapoints. Neural networks are useful only if they work on data you
    don’t already know the answer to.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得神经网络变得有点没有意义。一个只在你训练它的数据上工作的神经网络的目的是什么？你已经知道这些数据点的正确分类。神经网络只有在处理你不知道答案的数据时才有用。
- en: 'As it turns out, there’s a way to combat this. Here I’ve printed out both the
    training *and testing* accuracy of the neural network *as it was training* (every
    10 iterations). Notice anything interesting? You should see a clue to better networks:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，有一种方法可以对抗这种情况。在这里，我已经打印出了神经网络在训练过程中（每10次迭代）的*训练*和*测试*准确率。你注意到什么有趣的东西了吗？你应该看到更好的网络的线索：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Overfitting in neural networks
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络中的过拟合
- en: Neural networks can get worse if you train them too much!
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如果你过度训练神经网络，它可能会变得更差！
- en: For some reason, the *test* accuracy went up for the first 20 iterations and
    then slowly decreased as the network trained more and more (during which time
    the *training* accuracy was still improving). This is common in neural networks.
    Let me explain the phenomenon via an analogy.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于某种原因，*测试*准确率在前20次迭代中上升，然后在网络训练得越来越多时（在此期间*训练*准确率仍在提高）缓慢下降。这在神经网络中很常见。让我通过一个类比来解释这个现象。
- en: Imagine you’re creating a mold for a common dinner fork, but instead of using
    it to create other forks, you want to use it to identify whether a particular
    utensil is a fork. If an object fits in the mold, you’ll conclude that the object
    is a fork, and if it doesn’t, you’ll conclude that it’s *not* a fork.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你正在为一个常见的餐叉制作模具，但不是用它来制作其他叉子，而是想用它来识别某个特定的餐具是否是叉子。如果一个物体能放入模具，你就会得出结论说这个物体是叉子，如果不能，你就会得出结论说它*不是*叉子。
- en: Let’s say you set out to make this mold, and you start with a wet piece of clay
    and a big bucket of three-pronged forks, spoons, and knives. You then press each
    of the forks into the same place in the mold to create an outline, which sort
    of looks like a mushy fork. You repeatedly place all the forks in the clay over
    and over, hundreds of times. When you let the clay dry, you then find that none
    of the spoons or knives fit into this mold, but all the forks do. Awesome! You
    did it. You correctly made a mold that can fit only the shape of a fork.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你开始制作这个模具，你从一个湿的粘土块和一个装满三叉叉子、勺子和刀的大桶开始。然后你反复将所有叉子压入模具的同一位置以创建一个轮廓，这有点像一团糊状的叉子。你反复将所有叉子放入粘土中，数百次。当你让粘土干燥后，你会发现没有勺子或刀能放入这个模具，但所有叉子都能。太棒了！你做到了。你正确地制作了一个只能适合叉子形状的模具。
- en: But what happens if someone hands you a four-pronged fork? You look at your
    mold and notice that there’s a specific outline for three thin prongs in the clay.
    The four-pronged fork doesn’t fit. Why not? It’s still a fork.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果有人给你一个四叉叉子会发生什么？你看看你的模具，注意到粘土中有一个特定的三细叉轮廓。四叉叉子不合适。为什么？它仍然是一个叉子。
- en: It’s because the clay wasn’t molded on any four-pronged forks. It was molded
    only on the three-pronged variety. In this way, the clay has *overfit* to recognize
    only the types of forks it was “trained” to shape.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为粘土没有在四叉叉子上塑形。它只塑形了三叉叉子。这样，粘土就*过度拟合*了，只能识别它“训练”过的形状类型。
- en: This is exactly the same phenomenon you just witnessed in the neural network.
    It’s an even closer parallel than you might think. One way to view the weights
    of a neural network is as a high-dimensional shape. As you train, this shape *molds*
    around the shape of the data, learning to distinguish one pattern from another.
    Unfortunately, the images in the testing dataset were *slightly* different from
    the patterns in the training dataset. This caused the network to fail on many
    of the testing examples.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是你刚才在神经网络中看到的相同现象。这甚至比你想象的更接近。一种看待神经网络权重的方法是将其视为一个高维形状。随着训练的进行，这个形状*塑造*着数据的形状，学习区分不同的模式。不幸的是，测试数据集中的图像与训练数据集中的模式*略有*不同。这导致网络在许多测试示例上失败。
- en: A more official definition of a neural network that overfits is a neural network
    that has learned the *noise* in the dataset instead of making decisions based
    only on the *true signal*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更正式的过度拟合神经网络的定义是：一个神经网络学会了数据集中的*噪声*，而不是仅基于*真实信号*做出决策。
- en: Where overfitting comes from
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过度拟合的来源
- en: What causes neural networks to overfit?
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 什么导致了神经网络过度拟合？
- en: Let’s alter this scenario a bit. Picture the fresh clay again (unmolded). What
    if you pushed only a single fork into it? Assuming the clay was very thick, it
    wouldn’t have as much detail as the previous mold (which was imprinted many times).
    Thus, it would be only a *very general shape of a fork*. This shape might be compatible
    with both the three- and four-pronged varieties of fork, because it’s still a
    fuzzy imprint.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微改变一下这个场景。再次想象那块新鲜的粘土（未成型的）。如果你只把一个叉子压进去会怎样？假设粘土非常厚，它不会有之前模具（印制多次）那么多的细节。因此，它只会是一个*非常一般的叉子形状*。这个形状可能与三叉和四叉的叉子都兼容，因为它仍然是一个模糊的印痕。
- en: Assuming this information, the mold got worse at the testing dataset as you
    imprinted more forks because it learned more-detailed information about the training
    dataset it was being molded to. This caused it to reject images that were even
    the slightest bit off from what it had repeatedly seen in the training data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这个信息，随着你印制更多的叉子，模具在测试数据集上变得更差，因为它学到了更多关于训练数据集的详细信息。这导致它拒绝那些与它在训练数据中反复看到的哪怕是一点点不同的图像。
- en: What is this *detailed information* in the images that’s incompatible with the
    test data? In the fork analogy, it’s the number of prongs on the fork. In images,
    it’s generally referred to as *noise*. In reality, it’s a bit more nuanced. Consider
    these two dog pictures.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像中的*详细信息*是什么，与测试数据不兼容？在分叉的类比中，它就是叉子上的叉数。在图像中，这通常被称为*噪声*。在现实中，它要复杂一些。考虑这两张狗的照片。
- en: '![](Images/f0151-01.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0151-01.jpg)'
- en: Everything that makes these pictures unique beyond what captures the essence
    of “dog” is included in the term *noise*. In the picture on the left, the pillow
    and the background are both noise. In the picture on the right, the empty, middle
    blackness of the dog is a form of noise as well. It’s really the edges that tell
    you it’s a dog; the middle blackness doesn’t tell you anything. In the picture
    on the left, the middle of the dog has the furry texture and color of a dog, which
    could help the classifier correctly identify it.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 任何使这些照片在捕捉“狗”的本质之外变得独特的东西都包含在*噪声*这个术语中。在左边的图片中，枕头和背景都是噪声。在右边的图片中，狗中间的空黑部分也是一种噪声。实际上，是边缘告诉你这是一只狗；中间的黑色区域并没有告诉你任何东西。在左边的图片中，狗的中间部分有狗的毛茸茸的质感和颜色，这有助于分类器正确地识别它。
- en: How do you get neural networks to train only on the *signal* (the essence of
    a dog) and ignore the noise (other stuff irrelevant to the classification)? One
    way is *early stopping*. It turns out a large amount of noise comes in the fine-grained
    detail of an image, and most of the signal (for objects) is found in the general
    shape and perhaps color of the image.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何让神经网络只训练在*信号*（狗的本质）上，而忽略噪声（与分类无关的其他东西）？一种方法就是*提前停止*。结果证明，大量的噪声都存在于图像的细粒度细节中，而大部分的信号（对于物体）都存在于图像的一般形状和可能的颜色中。
- en: 'The simplest regularization: Early stopping'
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最简单的正则化：提前停止
- en: Stop training the network when it starts getting worse
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 当网络开始变差时停止训练
- en: How do you get a neural network to ignore the fine-grained detail and capture
    only the general information present in the data (such as the general shape of
    a dog or of an MNIST digit)? You don’t let the network train long enough to learn
    it.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何让神经网络忽略细粒度细节，只捕捉数据中存在的通用信息（如狗的一般形状或MNIST数字的一般形状）？你不让网络训练足够长的时间来学习它。
- en: In the fork-mold example, it takes many forks imprinted many times to create
    the perfect outline of a three-pronged fork. The first few imprints generally
    capture only the shallow outline of a fork. The same can be said for neural networks.
    As a result, *early stopping* is the cheapest form of regularization, and if you’re
    in a pinch, it can be quite effective.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在分叉模具的例子中，需要多次印制许多叉子才能创造出三叉叉的完美轮廓。最初几次的印制通常只能捕捉到叉子的浅轮廓。对于神经网络来说也是如此。因此，*提前停止*是成本最低的正则化形式，如果你处于困境中，它可能非常有效。
- en: 'This brings us to the subject this chapter is all about: *regularization*.
    Regularization is a subfield of methods for getting a model to *generalize* to
    new datapoints (instead of just memorizing the training data). It’s a subset of
    methods that help the neural network learn the signal and ignore the noise. In
    this case, it’s a toolset at your disposal to create neural networks that have
    these properties.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这把我们带到了本章的主题：*正则化*。正则化是模型对新数据点*泛化*的方法的一个子领域（而不是仅仅记住训练数据）。它是帮助神经网络学习信号并忽略噪声的方法的一个子集。在这种情况下，它是一套你可以使用的工具，以创建具有这些特性的神经网络。
- en: '|  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Regularization**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化**'
- en: Regularization is a subset of methods used to encourage generalization in learned
    models, often by increasing the difficulty for a model to learn the fine-grained
    details of training data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是用于鼓励学习模型泛化的方法的一个子集，通常通过增加模型学习训练数据细粒度细节的难度来实现。
- en: '|  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The next question might be, how do you know when to stop? The only real way
    to know is to run the model on data that isn’t in the training dataset. This is
    typically done using a second test dataset called a *validation set*. In some
    circumstances, if you used the test set for knowing when to stop, you could *overfit
    to the test set*. As a general rule, you don’t use it to control training. You
    use a validation set instead.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的问题可能是，你如何知道何时停止？唯一真正知道的方法是将模型运行在不在训练数据集中的数据上。这通常是通过使用第二个测试数据集，称为*验证集*来完成的。在某些情况下，如果你使用测试集来决定何时停止，你可能会对测试集*过拟合*。一般来说，你不使用它来控制训练。相反，你使用验证集。
- en: 'Industry standard regularization: Dropout'
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 行业标准正则化：Dropout
- en: 'The method: Randomly turn off neurons (set them to 0) during training'
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 方法：在训练过程中随机关闭神经元（将它们设置为0）
- en: This regularization technique is as simple as it sounds. During training, you
    randomly set neurons in the network to 0 (and usually the deltas on the same nodes
    during backpropagation, but you technically don’t have to). This causes the neural
    network to train exclusively using *random subsections* of the neural network.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这种正则化技术听起来很简单。在训练过程中，你随机将网络中的神经元设置为0（通常是在反向传播期间同一节点的delta，但技术上你不必这样做）。这导致神经网络仅使用神经网络的*随机子集*进行训练。
- en: Believe it or not, this regularization technique is generally accepted as the
    go-to, state-of-the-art regularization technique for the vast majority of networks.
    Its methodology is simple and inexpensive, although the intuitions behind *why*
    it works are a bit more complex.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 信不信由你，这种正则化技术通常被广泛接受为大多数网络的首选、最先进的正则化技术。其方法简单且成本低廉，尽管它背后的*为什么*它有效的原因要复杂一些。
- en: '|  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Why dropout works (perhaps oversimplified)**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么Dropout有效（可能过于简化**）'
- en: Dropout makes a big network act like a little one by randomly training little
    subsections of the network at a time, and little networks don’t overfit.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout通过随机训练网络的小部分子集，每次只训练一小部分，使得大网络表现得像一个小网络，而小网络不会过拟合。
- en: '|  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: It turns out that the smaller a neural network is, the less it’s able to overfit.
    Why? Well, small neural networks don’t have much expressive power. They can’t
    latch on to the more granular details (noise) that tend to be the source of overfitting.
    They have room to capture only the big, obvious, high-level features.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，神经网络越小，它越不容易过拟合。为什么？好吧，小神经网络没有太多的表达能力。它们不能抓住更细粒度的细节（噪声），这些细节往往是过拟合的来源。它们只能捕捉到大的、明显的、高级特征。
- en: This notion of *room* or *capacity* is really important to keep in your mind.
    Think of it like this. Remember the clay analogy? Imagine if the clay was made
    of sticky rocks the size of dimes. Would that clay be able to make a good imprint
    of a fork? Of course not. Those stones are much like the weights. They form around
    the data, capturing the patterns you’re interested in. If you have only a few,
    larger stones, they can’t capture nuanced detail. Each stone instead is pushed
    on by large parts of the fork, more or less *averaging* the shape (ignoring fine
    creases and corners).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个关于*空间*或*容量*的概念非常重要，需要牢记在心。可以这样想。还记得粘土的比喻吗？想象一下，如果粘土是由像一角硬币大小的粘土石头制成的，那么这种粘土能够做出一个好的叉子印吗？当然不能。这些石头就像权重。它们围绕着数据形成，捕捉你感兴趣的模式。如果你只有几个较大的石头，它们就不能捕捉细微的细节。每个石头基本上是由叉子的大部分推动，或多或少地*平均*形状（忽略细小的褶皱和角落）。
- en: Now, imagine clay made of very fine-grained sand. It’s made up of millions and
    millions of small stones that can fit into every nook and cranny of a fork. This
    is what gives big neural networks the expressive power they often use to overfit
    to a dataset.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下由非常细小的沙子制成的粘土。它由数百万个小石头组成，可以填充叉子的每一个角落。这就是大神经网络通常用来对数据集过拟合的表达能力。
- en: How do you get the power of a large neural network with the resistance to overfitting
    of the small neural network? Take the big neural network and turn off nodes randomly.
    What happens when you take a big neural network and use only a small part of it?
    It behaves like a small neural network. But when you do this randomly over potentially
    millions of different subnetworks, the sum total of the entire network still maintains
    its expressive power. Neat, eh?
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如何获得大神经网络的强大能力，同时具有小神经网络的抗过拟合能力？将大神经网络中的节点随机关闭。当你将一个大神经网络只使用其中的一小部分时，它会表现得像一个小神经网络。但是，当你随机地在数百万个不同的子网络中这样做时，整个网络的总体表达能力仍然保持不变。这不是很酷吗？
- en: 'Why dropout works: Ensembling works'
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dropout为什么有效：集成工作
- en: Dropout is a form of training a bunch of networks and averaging them
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Dropout是一种训练多个网络并取平均的方法
- en: 'Something to keep in mind: neural networks always start out randomly. Why does
    this matter? Well, because neural networks learn by trial and error, this ultimately
    means every neural network learns a little differently. It may learn equally effectively,
    but no two neural networks are ever exactly the same (unless they start out exactly
    the same for some random or intentional reason).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的是：神经网络总是从随机状态开始。这有什么关系呢？因为神经网络通过试错来学习，这最终意味着每个神经网络的学习可能同样有效，但没有任何两个神经网络是完全相同的（除非它们由于某种随机或故意的原因从完全相同的状态开始）。
- en: 'This has an interesting property. When you overfit two neural networks, no
    two neural networks overfit in exactly the same way. Overfitting occurs only until
    every training image can be predicted perfectly, at which point the error == 0
    and the network stops learning (even if you keep iterating). But because each
    neural network starts by predicting randomly and then adjusting its weights to
    make better predictions, each network inevitably makes different mistakes, resulting
    in different updates. This culminates in a core concept:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这有一个有趣的特点。当你过拟合两个神经网络时，没有两个神经网络会以完全相同的方式过拟合。过拟合只会持续到每个训练图像都可以被完美预测，此时错误率等于0，网络停止学习（即使你继续迭代）。但由于每个神经网络都是从随机预测开始，然后调整其权重以做出更好的预测，因此每个网络不可避免地会犯不同的错误，导致不同的更新。这最终导致一个核心概念：
- en: '|  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Although it’s likely that large, unregularized neural networks will overfit
    to noise, it’s unlikely they will overfit to the *same* noise.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大型、未正则化的神经网络可能会过拟合噪声，但它们不太可能过拟合到**相同的**噪声。
- en: '|  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Why don’t they overfit to the same noise? Because they start randomly, and
    they stop training once they’ve learned enough noise to disambiguate between all
    the images in the training set. The MNIST network needs to find only a handful
    of random pixels that happen to correlate with the output labels, to overfit.
    But this is contrasted with, perhaps, an even more important concept:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么它们不会过拟合到相同的噪声呢？因为它们是从随机状态开始的，一旦它们学会了足够多的噪声来区分训练集中的所有图像，就会停止训练。MNIST网络只需要找到几个随机像素，这些像素恰好与输出标签相关联，以实现过拟合。但与此相对的是，也许一个更加重要的概念：
- en: '|  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Neural networks, even though they’re randomly generated, still start by learning
    the biggest, most broadly sweeping features before learning much about the noise.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络，尽管它们是随机生成的，但仍然首先学习最大的、最广泛的特点，然后再学习关于噪声的更多内容。
- en: '|  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'The takeaway is this: if you train 100 neural networks (all initialized randomly),
    they will each tend to latch onto different noise but similar broad *signal*.
    Thus, when they make mistakes, they will often make *differing* mistakes. If you
    allowed them to vote equally, their noise would tend to cancel out, revealing
    only what they all learned in common: *the signal*.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 吸取的教训是：如果你训练100个神经网络（所有初始化都是随机的），它们各自倾向于锁定不同的噪声，但具有相似的广泛**信号**。因此，当它们犯错时，它们通常会犯**不同的**错误。如果你允许它们平等投票，它们的噪声往往会相互抵消，只揭示它们共同学习的内容：**信号**。
- en: Dropout in code
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dropout在代码中的应用
- en: Here’s how to use dropout in the real world
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这里是如何在现实世界中使用dropout的
- en: 'In the MNIST classification model, let’s add dropout to the hidden layer, such
    that 50% of the nodes are turned off (randomly) during training. You may be surprised
    that this is only a three-line change in the code. Following is a familiar snippet
    from the previous neural network logic, with the dropout mask added:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在MNIST分类模型中，让我们在隐藏层中添加dropout，这样在训练过程中将有50%的节点被随机关闭。你可能惊讶地发现这只是在代码中做了三行改动。以下是来自之前神经网络逻辑的一个熟悉的片段，其中添加了dropout掩码：
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To implement dropout on a layer (in this case, `layer_1`), multiply the `layer_1`
    values by a random matrix of 1s and 0s. This has the effect of randomly turning
    off nodes in `layer_1` by setting them to equal 0\. Note that `dropout_mask` uses
    what’s called a *50% Bernoulli distribution* such that 50% of the time, each value
    in `dropout_mask` is 1, and (1 – 50% = 50%) of the time, it’s 0.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要在层（在这种情况下，`layer_1`）上实现dropout，将`layer_1`的值乘以一个由1s和0s组成的随机矩阵。这会随机关闭`layer_1`中的节点，将它们设置为等于0。请注意，`dropout_mask`使用所谓的**50%伯努利分布**，这意味着50%的时间，`dropout_mask`中的每个值都是1，而（1
    - 50% = 50%）的时间，它是0。
- en: This is followed by something that may seem a bit peculiar. You multiply `layer_1`
    by 2\. Why do you do this? Remember that `layer_2` will perform a weighted sum
    of `layer_1`. Even though it’s weighted, it’s still a *sum* over the values of
    `layer_1`. If you turn off half the nodes in `layer_1`, that sum will be cut in
    half. Thus, `layer_2` would increase its sensitivity to `layer_1`, kind of like
    a person leaning closer to a radio when the volume is too low to better hear it.
    But at test time, when you no longer use dropout, the volume would be back up
    to normal. This throws off `layer_2`’s ability to listen to `layer_1`. You need
    to counter this by multiplying `layer_1` by (1 / the percentage of turned on nodes).
    In this case, that’s 1/0.5, which equals 2\. This way, the volume of `layer_1`
    is the same between training and testing, despite dropout.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是可能显得有些奇特的事情。你将`layer_1`乘以2。你为什么要这样做？记住，`layer_2`将对`layer_1`执行加权求和。尽管它是加权的，但它仍然是对`layer_1`的值的求和。如果你关闭`layer_1`中一半的节点，这个和将减半。因此，`layer_2`将增加对`layer_1`的敏感性，有点像当音量太低而听不清楚时，一个人会靠近收音机。但在测试时间，当你不再使用dropout时，音量会恢复到正常。这会干扰`layer_2`监听`layer_1`的能力。你需要通过将`layer_1`乘以（1
    / 打开的节点百分比）来对此进行对抗。在这种情况下，那就是1/0.5，等于2。这样，`layer_1`在训练和测试时的音量是相同的，尽管有dropout。
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '***1* Returns x if x > 0; returns 0 otherwise**'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 如果x大于0则返回x；否则返回0**'
- en: '***2* Returns 1 for input > 0**'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 对于输入大于0的情况返回1**'
- en: Dropout evaluated on MNIST
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在MNIST上评估Dropout
- en: 'If you remember from before, the neural network (without dropout) previously
    reached a test accuracy of 81.14% before falling down to finish training at 70.73%
    accuracy. When you add dropout, the neural network instead behaves this way:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得之前的内容，没有dropout的神经网络之前在测试准确率达到81.14%后下降到70.73%的准确率完成训练。当你添加dropout时，神经网络反而表现出这种行为：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Not only does the network instead peak at a score of 82.36%, it also doesn’t
    overfit nearly as badly, finishing training with a testing accuracy of 81.81%.
    Notice that the dropout also slows down `Training-Acc`, which previously went
    straight to 100% and stayed there.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅网络在得分82.36%时达到峰值，它也没有那么严重地过拟合，最终以81.81%的测试准确率完成训练。注意，dropout也减缓了`Training-Acc`的上升速度，之前它直接升到100%并保持在那里。
- en: 'This should point to what dropout really is: it’s noise. It makes it more difficult
    for the network to train on the training data. It’s like running a marathon with
    weights on your legs. It’s harder to train, but when you take off the weights
    for the big race, you end up running quite a bit faster because you trained for
    something that was much more difficult.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该指向dropout真正是什么：它是噪声。它使网络在训练数据上训练变得更加困难。这就像在腿上绑着重物跑马拉松。训练起来更难，但当你为一场难度更大的比赛脱下重物时，你最终会跑得相当快，因为你训练的是一件更困难的事情。
- en: Batch gradient descent
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量梯度下降
- en: Here’s a method for increasing the speed of training and the rate of convergence
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这里有一种提高训练速度和收敛率的方法
- en: 'In the context of this chapter, I’d like to briefly apply a concept introduced
    several chapters ago: mini-batched stochastic gradient descent. I won’t go into
    too much detail, because it’s something that’s largely taken for granted in neural
    network training. Furthermore, it’s a simple concept that doesn’t get more advanced
    even with the most state-of-the-art neural networks.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的背景下，我想简要应用几章前引入的一个概念：小批量随机梯度下降。我不会过多地详细介绍，因为这主要是神经网络训练中被默认接受的东西。此外，它是一个简单的概念，即使是最先进的神经网络也不会变得更加复杂。
- en: Previously we trained one training example at a time, updating the weights after
    each example. Now, let’s train 100 training examples at a time, averaging the
    weight updates among all 100 examples. The training/testing output is shown next,
    followed by the code for the training logic.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们一次训练一个训练示例，在每个示例之后更新权重。现在，让我们一次训练100个训练示例，对所有100个示例的平均权重更新进行平均。接下来显示训练/测试输出，然后是训练逻辑的代码。
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Notice that the training accuracy has a smoother trend than it did before. Taking
    an average weight update consistently creates this kind of phenomenon during training.
    As it turns out, individual training examples are very noisy in terms of the weight
    updates they generate. Thus, averaging them makes for a smoother learning process.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，训练准确率比之前更加平滑。持续进行平均权重更新会在训练过程中产生这种现象。实际上，单个训练示例在生成的权重更新方面非常嘈杂。因此，平均它们会使学习过程更加平滑。
- en: '[PRE9]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '***1* Returns x if x > 0**'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 如果x大于0则返回x**'
- en: '***2* Returns 1 for input > 0**'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 当输入大于0时返回1**'
- en: The first thing you’ll notice when running this code is that it runs much faster.
    This is because each `np.dot` function is now performing 100 vector dot products
    at a time. CPU architectures are much faster at performing dot products batched
    this way.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这段代码时，你首先会注意到它运行得更快。这是因为每个`np.dot`函数现在一次执行100个向量点积。CPU架构以这种方式批量执行点积要快得多。
- en: There’s more going on here, however. Notice that `alpha` is 20 times larger
    than before. You can increase it for a fascinating reason. Imagine you were trying
    to find a city using a very wobbly compass. If you looked down, got a heading,
    and then ran 2 miles, you’d likely be way off course. But if you looked down,
    took 100 headings, and then averaged them, running 2 miles would probably take
    you in the general right direction.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里还有更多的事情发生。请注意，`alpha`现在比之前大20倍。你可以出于一个有趣的原因增加它。想象一下，如果你试图使用一个非常不稳定的指南针找到一个城市。如果你低头看了一下，得到了一个航向，然后跑了2英里，你很可能会偏离航线。但如果你低头看了100次航向，然后取平均值，跑2英里可能会让你大致朝正确的方向前进。
- en: Because the example takes an average of a noisy signal (the average weight change
    over 100 training examples), it can take bigger steps. You’ll generally see batching
    ranging from size 8 to as high as 256\. Generally, researchers pick numbers randomly
    until they find a `batch_size`/`alpha` pair that seems to work well.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这个例子取了一个噪声信号的平均值（100个训练示例中的平均权重变化），它可以采取更大的步长。你通常会看到批量大小从8到高达256。通常，研究人员会随机选择数字，直到他们找到一个似乎工作良好的`batch_size`/`alpha`对。
- en: Summary
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter addressed two of the most widely used methods for increasing the
    accuracy and training speed of almost any neural architecture. In the following
    chapters, we’ll pivot from sets of tools that are universally applicable to nearly
    all neural networks, to special-purpose architectures that are advantageous for
    modeling specific types of phenomena in data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了两种几乎适用于任何神经网络架构以提高准确性和训练速度的最常用方法。在接下来的章节中，我们将从适用于几乎所有神经网络的通用工具集转向针对特定类型现象建模的有利架构。
