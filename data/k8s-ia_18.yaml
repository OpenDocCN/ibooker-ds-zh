- en: Chapter 16\. Advanced scheduling
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第 16 章\. 高级调度
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Using node taints and pod tolerations to keep pods away from certain nodes
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用节点污点和 pod 容忍度将 pod 保持远离某些节点
- en: Defining node affinity rules as an alternative to node selectors
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将节点亲和性规则定义为节点选择器的替代方案
- en: Co-locating pods using pod affinity
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 pod 亲和性将 pod 部署在一起
- en: Keeping pods away from each other using pod anti-affinity
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 pod 反亲和性将 pod 保持彼此远离
- en: Kubernetes allows you to affect where pods are scheduled. Initially, this was
    only done by specifying a node selector in the pod specification, but additional
    mechanisms were later added that expanded this functionality. They’re covered
    in this chapter.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 允许你影响 pod 的调度位置。最初，这仅通过在 pod 规范中指定节点选择器来完成，但后来又添加了额外的机制来扩展这一功能。这些内容在本章中介绍。
- en: 16.1\. Using taints and tolerations to repel pods from certain nodes
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 16.1\. 使用污点和容忍度将 pod 从某些节点排斥
- en: The first two features related to advanced scheduling that we’ll explore here
    are the node taints and pods’ tolerations of those taints. They’re used for restricting
    which pods can use a certain node. A pod can only be scheduled to a node if it
    tolerates the node’s taints.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里将要探索的前两个与高级调度相关的功能是节点污点以及 pod 对这些污点的容忍度。它们用于限制哪些 pod 可以使用某个节点。只有当 pod 容忍节点的污点时，它才能被调度到该节点。
- en: This is somewhat different from using node selectors and node affinity, which
    you’ll learn about later in this chapter. Node selectors and node affinity rules
    make it possible to select which nodes a pod can or can’t be scheduled to by specifically
    adding that information to the pod, whereas taints allow rejecting deployment
    of pods to certain nodes by only adding taints to the node without having to modify
    existing pods. Pods that you want deployed on a tainted node need to opt in to
    use the node, whereas with node selectors, pods explicitly specify which node(s)
    they want to be deployed to.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这与使用节点选择器和节点亲和性有些不同，你将在本章后面的内容中了解到。节点选择器和节点亲和性规则使得可以通过在 pod 中特别添加相关信息来选择 pod
    可以或不可以被调度到哪些节点，而污点允许通过仅在节点上添加污点来拒绝 pod 的部署，而无需修改现有的 pod。你想要部署在污点节点上的 pod 需要选择加入以使用该节点，而与节点选择器不同，pod
    明确指定它们想要部署到的节点。
- en: 16.1.1\. Introducing taints and tolerations
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 16.1.1\. 介绍污点和容忍度
- en: The best path to learn about node taints is to see an existing taint. [Appendix
    B](index_split_138.html#filepos1737471) shows how to set up a multi-node cluster
    with the `kubeadm` tool. By default, the master node in such a cluster is tainted,
    so only Control Plane pods can be deployed on it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 了解节点污点的最佳途径是查看现有的污点。[附录 B](index_split_138.html#filepos1737471) 展示了如何使用 `kubeadm`
    工具设置多节点集群。默认情况下，此类集群中的主节点被污点标记，因此只有控制平面 pod 可以部署在其上。
- en: Displaying a node’s taints
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 显示节点的污点
- en: You can see the node’s taints using `kubectl describe node`, as shown in the
    following listing.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `kubectl describe node` 来查看节点的污点，如下面的列表所示。
- en: Listing 16.1\. Describing the master node in a cluster created with `kubeadm`
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 16.1\. 描述使用 `kubeadm` 创建的集群中的主节点
- en: '`$ kubectl describe node master.k8s` `Name:         master.k8s Role: Labels:      
    beta.kubernetes.io/arch=amd64               beta.kubernetes.io/os=linux              
    kubernetes.io/hostname=master.k8s               node-role.kubernetes.io/master=
    Annotations:  node.alpha.kubernetes.io/ttl=0               volumes.kubernetes.io/controller-managed-attach-detach=true
    Taints:       node-role.kubernetes.io/master:NoSchedule` `1` `...`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl describe node master.k8s` `Name:         master.k8s` `Role:          
    master.k8s` `Labels:       beta.kubernetes.io/arch=amd64` `             beta.kubernetes.io/os=linux`
    `             kubernetes.io/hostname=master.k8s` `             node-role.kubernetes.io/master=
    Annotations:  node.alpha.kubernetes.io/ttl=0` `             volumes.kubernetes.io/controller-managed-attach-detach=true`
    `Taints:       node-role.kubernetes.io/master:NoSchedule` `1` `...`'
- en: 1 The master node has one taint.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 主节点有一个污点。
- en: The master node has a single taint. Taints have a key, value, and an effect,
    and are represented as `<key>=<value>:<effect>`. The master node’s taint shown
    in the previous listing has the key `node-role.kubernetes.io/master`, a `null`
    value (not shown in the taint), and the effect of `NoSchedule`.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点有一个单一的污点。污点有一个键、一个值和一个效果，表示为 `<key>=<value>:<effect>`。前一个列表中显示的主节点的污点具有键
    `node-role.kubernetes.io/master`，一个 `null` 值（在污点中未显示）和效果 `NoSchedule`。
- en: This taint prevents pods from being scheduled to the master node, unless those
    pods tolerate this taint. The pods that tolerate it are usually system pods (see
    [figure 16.1](#filepos1491042)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个污点阻止 pod 被调度到主节点，除非这些 pod 容忍这个污点。容忍它的 pod 通常都是系统 pod（参见[图 16.1](#filepos1491042)）。
- en: Figure 16.1\. A pod is only scheduled to a node if it tolerates the node’s taints.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1\. 只有当pod容忍节点的污点时，才会将pod调度到节点。
- en: '![](images/00160.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00160.jpg)'
- en: Displaying a pod’s tolerations
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 显示pod的容忍度
- en: In a cluster installed with `kubeadm`, the kube-proxy cluster component runs
    as a pod on every node, including the master node, because master components that
    run as pods may also need to access Kubernetes Services. To make sure the kube-proxy
    pod also runs on the master node, it includes the appropriate toleration. In total,
    the pod has three tolerations, which are shown in the following listing.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`kubeadm`安装的集群中，kube-proxy集群组件作为pod在每个节点上运行，包括主节点，因为作为pod运行的master组件可能也需要访问Kubernetes服务。为了确保kube-proxy
    pod也运行在主节点上，它包括适当的容忍度。总共，该pod有三个容忍度，如下所示。
- en: Listing 16.2\. A pod’s tolerations
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.2\. pod的容忍度
- en: '`$ kubectl describe po kube-proxy-80wqm -n kube-system` `... Tolerations:   
    node-role.kubernetes.io/master=:NoSchedule                 node.alpha.kubernetes.io/notReady=:Exists:NoExecute
                    node.alpha.kubernetes.io/unreachable=:Exists:NoExecute ...`'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl describe po kube-proxy-80wqm -n kube-system` `... 容忍度：    node-role.kubernetes.io/master=:NoSchedule              
    node.alpha.kubernetes.io/notReady=:Exists:NoExecute               node.alpha.kubernetes.io/unreachable=:Exists:NoExecute
    ...`'
- en: As you can see, the first toleration matches the master node’s taint, allowing
    this kube-proxy pod to be scheduled to the master node.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，第一个容忍度与主节点的污点匹配，允许此kube-proxy pod被调度到主节点。
- en: '|  |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: Disregard the equal sign, which is shown in the pod’s tolerations, but not in
    the node’s taints. Kubectl apparently displays taints and tolerations differently
    when the taint’s/toleration’s value is `null`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 忽略显示在pod的容忍度中但不在节点的污点中的等号。Kubectl在污点/容忍度的值为`null`时似乎以不同的方式显示污点和容忍度。
- en: '|  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Understanding taint effects
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 理解污点效果
- en: The two other tolerations on the kube-proxy pod define how long the pod is allowed
    to run on nodes that aren’t ready or are unreachable (the time in seconds isn’t
    shown, but can be seen in the pod’s YAML). Those two tolerations refer to the
    `NoExecute` instead of the `NoSchedule` effect.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: kube-proxy pod上的另外两个容忍度定义了pod在未就绪或不可达的节点上运行的时间长度（秒数未显示，但可以在pod的YAML中看到）。这两个容忍度指的是`NoExecute`效果而不是`NoSchedule`效果。
- en: 'Each taint has an effect associated with it. Three possible effects exist:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 每个污点都与一个效果相关联。存在三种可能的效果：
- en: '`NoSchedule`, which means pods won’t be scheduled to the node if they don’t
    tolerate the taint.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NoSchedule`，这意味着如果pod不容忍污点，则不会将其调度到节点。'
- en: '`PreferNoSchedule` is a soft version of `NoSchedule`, meaning the scheduler
    will try to avoid scheduling the pod to the node, but will schedule it to the
    node if it can’t schedule it somewhere else.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PreferNoSchedule`是`NoSchedule`的软版本，意味着调度器会尝试避免将pod调度到节点，但如果无法在其他地方调度，则会将其调度到节点。'
- en: '`NoExecute`, unlike `NoSchedule` and `PreferNoSchedule` that only affect scheduling,
    also affects pods already running on the node. If you add a `NoExecute` taint
    to a node, pods that are already running on that node and don’t tolerate the `NoExecute`
    taint will be evicted from the node.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NoExecute`与仅影响调度的`NoSchedule`和`PreferNoSchedule`不同，它还会影响节点上已运行的pod。如果您向节点添加`NoExecute`污点，则已在该节点上运行且不容忍`NoExecute`污点的pod将被从节点驱逐。'
- en: 16.1.2\. Adding custom taints to a node
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 16.1.2\. 向节点添加自定义污点
- en: 'Imagine having a single Kubernetes cluster where you run both production and
    non-production workloads. It’s of the utmost importance that non-production pods
    never run on the production nodes. This can be achieved by adding a taint to your
    production nodes. To add a taint, you use the `kubectl taint` command:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下拥有一个单一的Kubernetes集群，您在其中运行生产和非生产工作负载。非生产pod永远不会在生产节点上运行这一点至关重要。这可以通过向生产节点添加污点来实现。要添加污点，您使用`kubectl
    taint`命令：
- en: '`$ kubectl taint node node1.k8s node-type=production:NoSchedule` `node "node1.k8s"
    tainted`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl taint node node1.k8s node-type=production:NoSchedule` `node "node1.k8s"
    污点`'
- en: This adds a taint with key `node-type`, value `production` and the `NoSchedule`
    effect. If you now deploy multiple replicas of a regular pod, you’ll see none
    of them are scheduled to the node you tainted, as shown in the following listing.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这添加了一个具有键`node-type`、值`production`和`NoSchedule`效果的污点。如果您现在部署常规pod的多个副本，您将看到它们都没有被调度到您已污点的节点，如下所示。
- en: Listing 16.3\. Deploying pods without a toleration
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.3\. 部署没有容忍度的pod
- en: '`$ kubectl run test --image busybox --replicas 5 -- sleep 99999` `deployment
    "test" created` `$ kubectl get po -o wide` `NAME                READY  STATUS   
    RESTARTS   AGE   IP          NODE test-196686-46ngl   1/1    Running   0         
    12s   10.47.0.1   node2.k8s test-196686-73p89   1/1    Running   0          12s  
    10.47.0.7   node2.k8s test-196686-77280   1/1    Running   0          12s   10.47.0.6  
    node2.k8s test-196686-h9m8f   1/1    Running   0          12s   10.47.0.5   node2.k8s
    test-196686-p85ll   1/1    Running   0          12s   10.47.0.4   node2.k8s`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl run test --image busybox --replicas 5 -- sleep 99999` `deployment
    "test" created` `$ kubectl get po -o wide` `NAME                READY  STATUS   
    RESTARTS   AGE   IP          NODE test-196686-46ngl   1/1    Running   0         
    12s   10.47.0.1   node2.k8s test-196686-73p89   1/1    Running   0          12s  
    10.47.0.7   node2.k8s test-196686-77280   1/1    Running   0          12s   10.47.0.6  
    node2.k8s test-196686-h9m8f   1/1    Running   0          12s   10.47.0.5   node2.k8s
    test-196686-p85ll   1/1    Running   0          12s   10.47.0.4   node2.k8s`'
- en: Now, no one can inadvertently deploy pods onto the production nodes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，没有人可以意外地将Pod部署到生产节点上。
- en: 16.1.3\. Adding tolerations to pods
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 16.1.3\. 为Pod添加容忍度
- en: To deploy production pods to the production nodes, they need to tolerate the
    taint you added to the nodes. The manifests of your production pods need to include
    the YAML snippet shown in the following listing.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要将生产Pod部署到生产节点，它们需要容忍您添加到节点上的污点。您生产Pod的清单需要包含以下列表中所示的YAML片段。
- en: 'Listing 16.4\. A production Deployment with a toleration: production-deployment.yaml'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.4\. 具有容忍度的生产Deployment：production-deployment.yaml
- en: '`apiVersion: extensions/v1beta1 kind: Deployment metadata:   name: prod spec:
      replicas: 5   template:     spec:       ...       tolerations:       - key:
    node-type` `1` `Operator: Equal` `1` `value: production` `1` `effect: NoSchedule`
    `1`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: extensions/v1beta1 kind: Deployment metadata:   name: prod spec:
      replicas: 5   template:     spec:       ...       tolerations:       - key:
    node-type` `1` `Operator: Equal` `1` `value: production` `1` `effect: NoSchedule`
    `1`'
- en: 1 This toleration allows the pod to be scheduled to production nodes.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 此容忍度允许Pod被调度到生产节点。
- en: If you deploy this Deployment, you’ll see its pods get deployed to the production
    node, as shown in the next listing.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您部署此Deployment，您将看到其Pod被部署到生产节点，如下一列表所示。
- en: Listing 16.5\. Pods with the toleration are deployed on production `node1`
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.5\. 具有容忍度的Pod被部署在生产`node1`上
- en: '`$ kubectl get po -o wide` `NAME                READY  STATUS    RESTARTS  
    AGE   IP          NODE prod-350605-1ph5h   0/1    Running   0          16s   10.44.0.3`
    `node1``.k8s prod-350605-ctqcr   1/1    Running   0          16s   10.47.0.4  
    node2.k8s prod-350605-f7pcc   0/1    Running   0          17s   10.44.0.6` `node1``.k8s
    prod-350605-k7c8g   1/1    Running   0          17s   10.47.0.9   node2.k8s prod-350605-rp1nv  
    0/1    Running   0          17s   10.44.0.4` `node1``.k8s`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po -o wide` `NAME                READY  STATUS    RESTARTS  
    AGE   IP          NODE prod-350605-1ph5h   0/1    Running   0          16s   10.44.0.3`
    `node1``.k8s prod-350605-ctqcr   1/1    Running   0          16s   10.47.0.4  
    node2.k8s prod-350605-f7pcc   0/1    Running   0          17s   10.44.0.6` `node1``.k8s
    prod-350605-k7c8g   1/1    Running   0          17s   10.47.0.9   node2.k8s prod-350605-rp1nv  
    0/1    Running   0          17s   10.44.0.4` `node1``.k8s`'
- en: As you can see in the listing, production pods were also deployed to `node2`,
    which isn’t a production node. To prevent that from happening, you’d also need
    to taint the non-production nodes with a taint such as `node-type=non-production:NoSchedule`.
    Then you’d also need to add the matching toleration to all your non-production
    pods.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 正如列表所示，生产型Pod也被部署到了`node2`，而它并不是一个生产节点。为了防止这种情况发生，您还需要对非生产节点施加一个污点，例如`node-type=non-production:NoSchedule`。然后，您还需要为所有非生产Pod添加相应的容忍度。
- en: 16.1.4\. Understanding what taints and tolerations can be used for
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 16.1.4\. 理解污点和容忍度可以用于什么
- en: Nodes can have more than one taint and pods can have more than one toleration.
    As you’ve seen, taints can only have a key and an effect and don’t require a value.
    Tolerations can tolerate a specific value by specifying the `Equal` operator (that’s
    also the default operator if you don’t specify one), or they can tolerate any
    value for a specific taint key if you use the `Exists` operator.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 节点可以有多个污点，Pod也可以有多个容忍度。如您所见，污点只能有一个键和一个效果，不需要值。容忍度可以通过指定`Equal`运算符（如果不指定，这也是默认运算符）来容忍特定值，或者如果使用`Exists`运算符，它们可以容忍特定污点键的任何值。
- en: Using taints and tolerations during scheduling
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在调度过程中使用污点和容忍度
- en: Taints can be used to prevent scheduling of new pods (`NoSchedule` effect) and
    to define unpreferred nodes (`PreferNoSchedule` effect) and even evict existing
    pods from a node (`NoExecute`).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 污点可以用来防止新Pod的调度（`NoSchedule`效果），定义不首选的节点（`PreferNoSchedule`效果），甚至从节点驱逐现有的Pod（`NoExecute`）。
- en: You can set up taints and tolerations any way you see fit. For example, you
    could partition your cluster into multiple partitions, allowing your development
    teams to schedule pods only to their respective nodes. You can also use taints
    and tolerations when several of your nodes provide special hardware and only part
    of your pods need to use it.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以按任何你认为合适的方式设置污点和容忍度。例如，你可以将集群划分为多个分区，允许你的开发团队只将Pod调度到各自的节点。你也可以在多个节点提供特殊硬件且只有部分Pod需要使用该硬件时使用污点和容忍度。
- en: Configuring how long after a node failure a pod is rescheduled
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 配置节点故障后Pod重新调度的延迟时间
- en: You can also use a toleration to specify how long Kubernetes should wait before
    rescheduling a pod to another node if the node the pod is running on becomes unready
    or unreachable. If you look at the tolerations of one of your pods, you’ll see
    two tolerations, which are shown in the following listing.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用容忍度来指定在Pod所在的节点变得不可就绪或不可达后，Kubernetes应该在重新调度Pod到另一个节点之前等待多长时间。如果你查看你的Pod的容忍度，你会看到两个容忍度，如下所示。
- en: Listing 16.6\. Pod with default tolerations
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.6\. 默认容忍度的Pod
- en: '`$ kubectl get po prod-350605-1ph5h -o yaml` `...   tolerations:   - effect:
    NoExecute` `1` `key: node.alpha.kubernetes.io/notReady` `1` `operator: Exists`
    `1` `tolerationSeconds: 300` `1` `- effect: NoExecute` `2` `key: node.alpha.kubernetes.io/unreachable`
    `2` `operator: Exists` `2` `tolerationSeconds: 300` `2`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po prod-350605-1ph5h -o yaml` `...   tolerations:   - effect:
    NoExecute   1   key: node.alpha.kubernetes.io/notReady   1   operator: Exists   1   tolerationSeconds:
    300   1   - effect: NoExecute   2   key: node.alpha.kubernetes.io/unreachable   2   operator:
    Exists   2   tolerationSeconds: 300   2`'
- en: 1 The pod tolerates the node being notReady for 300 seconds, before it needs
    to be rescheduled.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 Pod可以容忍节点notReady状态持续300秒，然后需要重新调度。
- en: 2 The same applies to the node being unreachable.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 同样适用于节点不可达的情况。
- en: These two tolerations say that this pod tolerates a node being `notReady` or
    `unreachable` for `300` seconds. The Kubernetes Control Plane, when it detects
    that a node is no longer ready or no longer reachable, will wait for 300 seconds
    before it deletes the pod and reschedules it to another node.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个容忍度说明，这个Pod可以容忍节点在`notReady`或`unreachable`状态持续`300`秒。当Kubernetes控制平面检测到节点不再就绪或不可达时，它会在删除Pod并将其重新调度到另一个节点之前等待300秒。
- en: These two tolerations are automatically added to pods that don’t define them.
    If that five-minute delay is too long for your pods, you can make the delay shorter
    by adding those two tolerations to the pod’s spec.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个容忍度会自动添加到未定义它们的Pod中。如果这个五分钟的延迟对于你的Pod来说太长了，你可以通过将这两个容忍度添加到Pod的规范中来缩短延迟。
- en: '|  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This is currently an alpha feature, so it may change in future versions of Kubernetes.
    Taint-based evictions also aren’t enabled by default. You enable them by running
    the Controller Manager with the `--feature-gates=Taint-BasedEvictions=true` option.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这目前是一个alpha特性，因此它可能在Kubernetes的未来版本中发生变化。基于污点的驱逐默认也是未启用的。你可以通过运行带有`--feature-gates=Taint-BasedEvictions=true`选项的Controller
    Manager来启用它们。
- en: '|  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 16.2\. Using node affinity to attract pods to certain nodes
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 16.2\. 使用节点亲和将Pod吸引到特定节点
- en: As you’ve learned, taints are used to keep pods away from certain nodes. Now
    you’ll learn about a newer mechanism called node affinity, which allows you to
    tell Kubernetes to schedule pods only to specific subsets of nodes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所学的，污点（taints）用于将Pod从某些节点上移除。现在你将了解一种称为节点亲和（node affinity）的新机制，它允许你告诉Kubernetes只将Pod调度到特定的节点子集。
- en: Comparing node affinity to node selectors
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 比较节点亲和与节点选择器
- en: The initial node affinity mechanism in early versions of Kubernetes was the
    `node-Selector` field in the pod specification. The node had to include all the
    labels specified in that field to be eligible to become the target for the pod.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes早期版本中的初始节点亲和机制是Pod规范中的`node-Selector`字段。节点必须包含该字段中指定的所有标签才能有资格成为Pod的目标。
- en: Node selectors get the job done and are simple, but they don’t offer everything
    that you may need. Because of that, a more powerful mechanism was introduced.
    Node selectors will eventually be deprecated, so it’s important you understand
    the new node affinity rules.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 节点选择器完成了工作且很简单，但它们并不提供你可能需要的所有功能。因此，引入了一种更强大的机制。节点选择器最终将被弃用，因此了解新的节点亲和规则非常重要。
- en: Similar to node selectors, each pod can define its own node affinity rules.
    These allow you to specify either hard requirements or preferences. By specifying
    a preference, you tell Kubernetes which nodes you prefer for a specific pod, and
    Kubernetes will try to schedule the pod to one of those nodes. If that’s not possible,
    it will choose one of the other nodes.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 与节点选择器类似，每个Pod都可以定义自己的节点亲和性规则。这允许你指定硬要求或偏好。通过指定偏好，你告诉Kubernetes你希望为特定Pod使用的节点，Kubernetes将尝试将Pod调度到这些节点之一。如果不可能，它将选择其他节点之一。
- en: Examining the default node labels
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 检查默认节点标签
- en: Node affinity selects nodes based on their labels, the same way node selectors
    do. Before you see how to use node affinity, let’s examine the labels of one of
    the nodes in a Google Kubernetes Engine cluster (GKE) to see what the default
    node labels are. They’re shown in the following listing.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 节点亲和性根据节点的标签选择节点，这与节点选择器的方式相同。在你了解如何使用节点亲和性之前，让我们检查Google Kubernetes Engine集群（GKE）中的一个节点的标签，以查看默认节点标签是什么。它们在以下列表中显示。
- en: Listing 16.7\. Default labels of a node in GKE
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.7\. GKE中节点的默认标签
- en: '`$ kubectl describe node gke-kubia-default-pool-db274c5a-mjnf` `Name:     gke-kubia-default-pool-db274c5a-mjnf
    Role: Labels:   beta.kubernetes.io/arch=amd64           beta.kubernetes.io/fluentd-ds-ready=true
              beta.kubernetes.io/instance-type=f1-micro           beta.kubernetes.io/os=linux
              cloud.google.com/gke-nodepool=default-pool           failure-domain.beta.kubernetes.io/region=europe-west1`
    `1` `failure-domain.beta.kubernetes.io/zone=europe-west1-d` `1` `kubernetes.io/hostname=gke-kubia-default-pool-db274c5a-mjnf`
    `1`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl describe node gke-kubia-default-pool-db274c5a-mjnf` `Name:     gke-kubia-default-pool-db274c5a-mjnf
    Role: Labels:   beta.kubernetes.io/arch=amd64           beta.kubernetes.io/fluentd-ds-ready=true
              beta.kubernetes.io/instance-type=f1-micro           beta.kubernetes.io/os=linux
              cloud.google.com/gke-nodepool=default-pool           failure-domain.beta.kubernetes.io/region=europe-west1`
    `1` `failure-domain.beta.kubernetes.io/zone=europe-west1-d` `1` `kubernetes.io/hostname=gke-kubia-default-pool-db274c5a-mjnf`
    `1`'
- en: 1 These three labels are the most important ones related to node affinity.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 这三个标签是与节点亲和性最相关的最重要的标签。
- en: 'The node has many labels, but the last three are the most important when it
    comes to node affinity and pod affinity, which you’ll learn about later. The meaning
    of those three labels is as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 节点有许多标签，但在节点亲和性和Pod亲和性方面，最后三个标签是最重要的。你将在后面了解这些标签的含义。以下是对这三个标签含义的说明：
- en: '`failure-domain.beta.kubernetes.io/region` specifies the geographical region
    the node is located in.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`failure-domain.beta.kubernetes.io/region` 指定了节点所在的地理区域。'
- en: '`failure-domain.beta.kubernetes.io/zone` specifies the availability zone the
    node is in.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`failure-domain.beta.kubernetes.io/zone` 指定了节点所在的可用区。'
- en: '`kubernetes.io/hostname` is obviously the node’s hostname.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubernetes.io/hostname` 显然是节点的主机名。'
- en: These and other labels can be used in pod affinity rules. In [chapter 3](index_split_028.html#filepos271328),
    you already learned how you can add a custom label to nodes and use it in a pod’s
    node selector. You used the custom label to deploy pods only to nodes with that
    label by adding a node selector to the pods. Now, you’ll see how to do the same
    using node affinity rules.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这些和其他标签可以在Pod亲和性规则中使用。在[第3章](index_split_028.html#filepos271328)中，你已经学习了如何向节点添加自定义标签并在Pod的节点选择器中使用它。你通过向Pod添加节点选择器来使用自定义标签，以便只在该标签的节点上部署Pod。现在，你将看到如何使用节点亲和性规则来完成同样的操作。
- en: 16.2.1\. Specifying hard node affinity rules
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 16.2.1\. 指定硬节点亲和规则
- en: In the example in [chapter 3](index_split_028.html#filepos271328), you used
    the node selector to deploy a pod that requires a GPU only to nodes that have
    a GPU. The pod spec included the `nodeSelector` field shown in the following listing.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](index_split_028.html#filepos271328)的示例中，你使用了节点选择器来部署一个只要求在具有GPU的节点上运行的Pod。Pod规范中包含了以下列表所示的`nodeSelector`字段。
- en: 'Listing 16.8\. A pod using a node selector: kubia-gpu-nodeselector.yaml'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.8\. 使用节点选择器的Pod：kubia-gpu-nodeselector.yaml
- en: '`apiVersion: v1 kind: Pod metadata:   name: kubia-gpu spec:   nodeSelector:`
    `1` `gpu: "true"` `1` `...`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Pod metadata:   name: kubia-gpu spec:   nodeSelector:`
    `1` `gpu: "true"` `1` `...`'
- en: 1 This pod is only scheduled to nodes that have the gpu=true label.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 此Pod仅调度到具有`gpu=true`标签的节点。
- en: The `nodeSelector` field specifies that the pod should only be deployed on nodes
    that include the `gpu=true` label. If you replace the node selector with a node
    affinity rule, the pod definition will look like the following listing.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`nodeSelector`字段指定Pod应该只部署在包含`gpu=true`标签的节点上。如果你用节点亲和性规则替换节点选择器，Pod定义将类似于以下列表。'
- en: 'Listing 16.9\. A pod using a `nodeAffinity` rule: kubia-gpu-nodeaffinity.yaml'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.9\. 使用`nodeAffinity`规则的Pod：kubia-gpu-nodeaffinity.yaml
- en: '`apiVersion: v1 kind: Pod metadata:   name: kubia-gpu spec:   affinity:    
    nodeAffinity:       requiredDuringSchedulingIgnoredDuringExecution:         nodeSelectorTerms:
            - matchExpressions:           - key: gpu             operator: In            
    values:             - "true"`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Pod metadata:   name: kubia-gpu spec:   affinity:    
    nodeAffinity:       requiredDuringSchedulingIgnoredDuringExecution:         nodeSelectorTerms:
            - matchExpressions:           - key: gpu             operator: In            
    values:             - "true"`'
- en: The first thing you’ll notice is that this is much more complicated than a simple
    node selector. But that’s because it’s much more expressive. Let’s examine the
    rule in detail.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 您首先会注意到，这比简单的节点选择器要复杂得多。但这是因为它具有更强的表达能力。让我们详细检查这个规则。
- en: Making sense of the long nodeAffinity attribute name
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 理解长节点亲和性属性名称
- en: As you can see, the pod’s spec section contains an `affinity` field that contains
    a `node-Affinity` field, which contains a field with an extremely long name, so
    let’s focus on that first.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，Pod的spec部分包含一个包含`nodeAffinity`字段的`affinity`字段，该字段包含一个极其长的字段，所以让我们首先关注这个字段。
- en: 'Let’s break it down into two parts and examine what they mean:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其分为两部分，并检查它们的意义：
- en: '`requiredDuringScheduling...` means the rules defined under this field specify
    the labels the node must have for the pod to be scheduled to the node.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requiredDuringScheduling...`表示在此字段下定义的规则指定了节点必须具有的标签，以便Pod可以调度到该节点。'
- en: '`...IgnoredDuringExecution` means the rules defined under the field don’t affect
    pods already executing on the node.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`...IgnoredDuringExecution`表示在此字段下定义的规则不会影响已经在节点上运行的Pod。'
- en: At this point, let me make things easier for you by letting you know that affinity
    currently only affects pod scheduling and never causes a pod to be evicted from
    a node. That’s why all the rules right now always end with `IgnoredDuringExecution`.
    Eventually, Kubernetes will also support `RequiredDuringExecution`, which means
    that if you remove a label from a node, pods that require the node to have that
    label will be evicted from such a node. As I’ve said, that’s not yet supported
    in Kubernetes, so let’s not concern ourselves with the second part of that long
    field any longer.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，让我通过让您知道亲和性目前仅影响Pod调度，永远不会导致Pod从节点中被驱逐，来简化事情。这就是为什么所有当前的规则都以`IgnoredDuringExecution`结束。最终，Kubernetes也将支持`RequiredDuringExecution`，这意味着如果您从节点中删除一个标签，需要该标签的节点才能调度Pod的Pod将被从该节点驱逐。正如我所说的，这还不是Kubernetes支持的功能，所以我们不再关注那个长字段的第二部分。
- en: Understanding nodeSelectorTerms
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 理解nodeSelectorTerms
- en: By keeping what was explained in the previous section in mind, it’s easy to
    understand that the `nodeSelectorTerms` field and the `matchExpressions` field
    define which expressions the node’s labels must match for the pod to be scheduled
    to the node. The single expression in the example is simple to understand. The
    node must have a `gpu` label whose value is set to `true`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 通过记住上一节中解释的内容，很容易理解`nodeSelectorTerms`字段和`matchExpressions`字段定义了节点标签必须匹配哪些表达式，以便Pod可以调度到该节点。示例中的单个表达式很容易理解。节点必须有一个值为`true`的`gpu`标签。
- en: This pod will therefore only be scheduled to nodes that have the `gpu=true`
    label, as shown in [figure 16.2](#filepos1511962).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个Pod将只调度到具有`gpu=true`标签的节点，如图16.2所示。
- en: Figure 16.2\. A pod’s node affinity specifies which labels a node must have
    for the pod to be scheduled to it.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2\. Pod的节点亲和性指定了节点必须具有哪些标签，以便Pod可以调度到该节点。
- en: '![](images/00178.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00178.jpg)'
- en: Now comes the more interesting part. Node also affinity allows you to prioritize
    nodes during scheduling. We’ll look at that next.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是更有趣的部分。节点亲和性还允许您在调度期间优先考虑节点。我们将在下一部分查看这一点。
- en: 16.2.2\. Prioritizing nodes when scheduling a pod
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 16.2.2\. 在调度Pod时优先考虑节点
- en: The biggest benefit of the newly introduced node affinity feature is the ability
    to specify which nodes the Scheduler should prefer when scheduling a specific
    pod. This is done through the `preferredDuringSchedulingIgnoredDuringExecution`
    field.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 新引入的节点亲和性功能最大的好处是能够指定调度特定Pod时调度器应该优先考虑哪些节点。这是通过`preferredDuringSchedulingIgnoredDuringExecution`字段实现的。
- en: Imagine having multiple datacenters across different countries. Each datacenter
    represents a separate availability zone. In each zone, you have certain machines
    meant only for your own use and others that your partner companies can use. You
    now want to deploy a few pods and you’d prefer them to be scheduled to `zone1`
    and to the machines reserved for your company’s deployments. If those machines
    don’t have enough room for the pods or if other important reasons exist that prevent
    them from being scheduled there, you’re okay with them being scheduled to the
    machines your partners use and to the other zones. Node affinity allows you to
    do that.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你拥有多个分布在不同国家的数据中心。每个数据中心代表一个单独的可用区。在每个区域，你有一些仅用于你自己的机器，以及一些你的合作伙伴公司可以使用的机器。现在你想要部署几个Pod，你希望它们被调度到`zone1`以及为你公司部署预留的机器上。如果那些机器没有足够的空间来容纳Pod，或者存在其他重要原因阻止它们在那里调度，你也会接受它们被调度到你的合作伙伴使用的机器和其他区域。节点亲和性允许你做到这一点。
- en: Labeling nodes
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 标记节点
- en: First, the nodes need to be labeled appropriately. Each node needs to have a
    label that designates the availability zone the node belongs to and a label marking
    it as either a dedicated or a shared node.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，节点需要被适当地标记。每个节点都需要一个标签来指定节点所属的可用区，以及一个标记来表明它是一个专用节点还是一个共享节点。
- en: '[Appendix B](index_split_138.html#filepos1737471) explains how to set up a
    three-node cluster (one master and two worker nodes) in VMs running locally. In
    the following examples, I’ll use the two worker nodes in that cluster, but you
    can also use Google Kubernetes Engine or any other multi-node cluster.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[附录B](index_split_138.html#filepos1737471) 解释了如何在本地运行的VM中设置一个三节点集群（一个主节点和两个工作节点）。在以下示例中，我将使用该集群中的两个工作节点，但你也可以使用Google
    Kubernetes Engine或任何其他多节点集群。'
- en: '|  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Minikube isn’t the best choice for running these examples, because it runs only
    one node.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Minikube不是运行这些示例的最佳选择，因为它只运行一个节点。
- en: '|  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: First, label the nodes, as shown in the next listing.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，按照以下列表标记节点。
- en: Listing 16.10\. Labeling nodes
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.10\. 标记节点
- en: '`$ kubectl label node node1.k8s availability-zone=zone1` `node "node1.k8s"
    labeled` `$ kubectl label node node1.k8s share-type=dedicated` `node "node1.k8s"
    labeled` `$ kubectl label node node2.k8s availability-zone=zone2` `node "node2.k8s"
    labeled` `$ kubectl label node node2.k8s share-type=shared` `node "node2.k8s"
    labeled` `$ kubectl get node -L availability-zone -L share-type` `NAME        
    STATUS    AGE       VERSION   AVAILABILITY-ZONE   SHARE-TYPE master.k8s   Ready    
    4d        v1.6.4    <none>              <none> node1.k8s    Ready     4d       
    v1.6.4` `zone1               dedicated` `node2.k8s    Ready     4d        v1.6.4`
    `zone2               shared`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl label node node1.k8s availability-zone=zone1` `node "node1.k8s"
    labeled` `$ kubectl label node node1.k8s share-type=dedicated` `node "node1.k8s"
    labeled` `$ kubectl label node node2.k8s availability-zone=zone2` `node "node2.k8s"
    labeled` `$ kubectl label node node2.k8s share-type=shared` `node "node2.k8s"
    labeled` `$ kubectl get node -L availability-zone -L share-type` `NAME        
    STATUS    AGE       VERSION   AVAILABILITY-ZONE   SHARE-TYPE master.k8s   Ready    
    4d        v1.6.4    <none>              <none> node1.k8s    Ready     4d       
    v1.6.4` `zone1               dedicated` `node2.k8s    Ready     4d        v1.6.4`
    `zone2               shared`'
- en: Specifying preferential node affinity rules
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 指定优先节点亲和规则
- en: With the node labels set up, you can now create a Deployment that prefers `dedicated`
    nodes in `zone1`. The following listing shows the Deployment manifest.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置了节点标签之后，你现在可以创建一个偏好于`zone1`中的`dedicated`节点的Deployment。以下列表显示了Deployment的清单。
- en: 'Listing 16.11\. Deployment with preferred node affinity: preferred-deployment.yaml'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.11\. 带有首选节点亲和性的Deployment：preferred-deployment.yaml
- en: '`apiVersion: extensions/v1beta1 kind: Deployment metadata:   name: pref spec:
      template:     ...     spec:       affinity:         nodeAffinity:          
    preferredDuringSchedulingIgnoredDuringExecution:` `1` `- weight: 80` `2` `preference:`
    `2` `matchExpressions:` `2` `- key: availability-zone` `2` `operator: In` `2`
    `values:` `2` `- zone1` `2` `- weight: 20` `3` `preference:` `3` `matchExpressions:`
    `3` `- key: share-type` `3` `operator: In` `3` `values:` `3` `- dedicated` `3`
    `...`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: extensions/v1beta1 kind: Deployment metadata:   name: pref spec:
        ...     spec:       affinity:         nodeAffinity:           preferredDuringSchedulingIgnoredDuringExecution:`
    `1` `- weight: 80` `2` `preference:` `2` `matchExpressions:` `2` `- key: availability-zone`
    `2` `operator: In` `2` `values:` `2` `- zone1` `2` `- weight: 20` `3` `preference:`
    `3` `matchExpressions:` `3` `- key: share-type` `3` `operator: In` `3` `values:`
    `3` `- dedicated` `3` `...`'
- en: 1 You’re specifying preferences, not hard requirements.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 你指定的是偏好，而不是硬性要求。
- en: 2 You prefer the pod to be scheduled to zone1\. This is your most important
    preference.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 你希望Pod被调度到zone1。这是你最重要的偏好。
- en: 3 You also prefer that your pods be scheduled to dedicated nodes, but this is
    four times less important than your zone preference.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你还希望你的Pod被调度到专用节点上，但这比你的区域偏好重要四倍。
- en: Let’s examine the listing closely. You’re defining a node affinity preference,
    instead of a hard requirement. You want the pods scheduled to nodes that include
    the labels `availability-zone=zone1` and `share-type=dedicated`. You’re saying
    that the first preference rule is important by setting its `weight` to `80`, whereas
    the second one is much less important (`weight` is set to `20`).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细检查列表。你正在定义一个节点亲和性偏好，而不是一个硬性要求。你希望Pod被调度到包含标签`availability-zone=zone1`和`share-type=dedicated`的节点上。你通过将其`weight`设置为`80`来说明第一个偏好规则很重要，而第二个则不那么重要（`weight`设置为`20`）。
- en: Understanding how node preferences work
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 理解节点偏好如何工作
- en: If your cluster had many nodes, when scheduling the pods of the Deployment in
    the previous listing, the nodes would be split into four groups, as shown in [figure
    16.3](#filepos1520407). Nodes whose `availability-zone` and `share-type` labels
    match the pod’s node affinity are ranked the highest. Then, because of how the
    weights in the pod’s node affinity rules are configured, next come the `shared`
    nodes in `zone1`, then come the `dedicated` nodes in the other zones, and at the
    lowest priority are all the other nodes.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的集群有很多节点，当在前面列表中调度Deployment的Pod时，节点会被分成四个组，如图16.3所示。[图16.3](#filepos1520407)。具有与Pod节点亲和性匹配的`availability-zone`和`share-type`标签的节点排名最高。然后，由于Pod节点亲和性规则中配置的权重，接下来是`zone1`中的`shared`节点，然后是其他区域的`dedicated`节点，最后是所有其他节点。
- en: Figure 16.3\. Prioritizing nodes based on a pod’s node affinity preferences
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3\. 根据Pod的节点亲和性偏好优先级节点
- en: '![](images/00196.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00196.jpg)'
- en: Deploying the pods in the two-node cluster
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在双节点集群中部署Pod
- en: If you create this Deployment in your two-node cluster, you should see most
    (if not all) of your pods deployed to `node1`. Examine the following listing to
    see if that’s true.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在一个双节点集群中创建这个Deployment，你应该会看到大多数（如果不是全部）Pod都部署到了`node1`。查看以下列表，看看这是否属实。
- en: Listing 16.12\. Seeing where pods were scheduled
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.12\. 查看Pod的调度位置
- en: '`$ kubectl get po -o wide` `NAME                READY   STATUS    RESTARTS 
    AGE   IP          NODE pref-607515-1rnwv   1/1     Running   0         4m    10.47.0.1  
    node2.k8s pref-607515-27wp0   1/1     Running   0         4m    10.44.0.8` `node1``.k8s
    pref-607515-5xd0z   1/1     Running   0         4m    10.44.0.5` `node1``.k8s
    pref-607515-jx9wt   1/1     Running   0         4m    10.44.0.4` `node1``.k8s
    pref-607515-mlgqm   1/1     Running   0         4m    10.44.0.6` `node1``.k8s`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po -o wide` '
- en: Out of the five pods that were created, four of them landed on `node1` and only
    one landed on `node2`. Why did one of them land on `node2` instead of `node1`?
    The reason is that besides the node affinity prioritization function, the Scheduler
    also uses other prioritization functions to decide where to schedule a pod. One
    of those is the `Selector-SpreadPriority` function, which makes sure pods belonging
    to the same ReplicaSet or Service are spread around different nodes so a node
    failure won’t bring the whole service down. That’s most likely what caused one
    of the pods to be scheduled to `node2`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建的五个Pod中，有四个部署到了`node1`，只有一个部署到了`node2`。为什么其中一个Pod会部署到`node2`而不是`node1`呢？原因在于，除了节点亲和性优先级函数之外，调度器还会使用其他优先级函数来决定Pod的调度位置。其中之一就是`Selector-SpreadPriority`函数，它确保属于同一ReplicaSet或Service的Pod被分散到不同的节点上，这样节点故障就不会导致整个服务崩溃。这很可能是导致其中一个Pod被调度到`node2`的原因。
- en: You can try scaling the Deployment up to 20 or more and you’ll see the majority
    of pods will be scheduled to `node1`. In my test, only two out of the 20 were
    scheduled to `node2`. If you hadn’t defined any node affinity preferences, the
    pods would have been spread around the two nodes evenly.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试将Deployment扩展到20个或更多，你会发现大多数Pod都会被调度到`node1`。在我的测试中，只有两个Pod被调度到了`node2`。如果你没有定义任何节点亲和性偏好，Pod将被均匀地分散到两个节点上。
- en: 16.3\. Co-locating pods with pod affinity and anti-affinity
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 16.3\. 将具有亲和性和反亲和性的Pod放置在一起
- en: You’ve seen how node affinity rules are used to influence which node a pod is
    scheduled to. But these rules only affect the affinity between a pod and a node,
    whereas sometimes you’d like to have the ability to specify the affinity between
    pods themselves.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到了如何使用节点亲和性规则来影响 Pod 调度到哪个节点。但这些规则只影响 Pod 与节点之间的亲和性，而有时你可能希望有指定 Pod 之间亲和性的能力。
- en: For example, imagine having a frontend and a backend pod. Having those pods
    deployed near to each other reduces latency and improves the performance of the
    app. You could use node affinity rules to ensure both are deployed to the same
    node, rack, or datacenter, but then you’d have to specify exactly which node,
    rack, or datacenter to schedule them to, which is not the best solution. It’s
    better to let Kubernetes deploy your pods anywhere it sees fit, while keeping
    the frontend and backend pods close together. This can be achieved using pod affinity.
    Let’s learn more about it with an example.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象有一个前端 Pod 和一个后端 Pod。将这些 Pod 部署在彼此附近可以减少延迟并提高应用程序的性能。你可以使用节点亲和性规则来确保它们都部署到同一节点、机架或数据中心，但这样你就必须指定确切哪个节点、机架或数据中心来调度它们，这并不是最佳解决方案。更好的做法是让
    Kubernetes 将你的 Pod 部署到它认为合适的地方，同时保持前端和后端 Pod 靠近。这可以通过使用 Pod 亲和性来实现。让我们通过一个例子来了解更多。
- en: 16.3.1\. Using inter-pod affinity to deploy pods on the same node
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 16.3.1\. 使用节点亲和性将 Pod 部署到同一节点
- en: You’ll deploy a backend pod and five frontend pod replicas with pod affinity
    configured so that they’re all deployed on the same node as the backend pod.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你将部署一个后端 Pod 和五个前端 Pod 副本，并配置 Pod 亲和性，以确保它们都部署在后端 Pod 相同的节点上。
- en: 'First, deploy the backend pod:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，部署后端 Pod：
- en: '`$ kubectl run backend -l app=backend --image busybox -- sleep 999999` `deployment
    "backend" created`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl run backend -l app=backend --image busybox -- sleep 999999` `deployment
    "backend" created`'
- en: This Deployment is not special in any way. The only thing you need to note is
    the `app=backend` label you added to the pod using the `-l` option. This label
    is what you’ll use in the frontend pod’s `podAffinity` configuration.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Deployment 没有任何特殊之处。你需要注意的唯一一件事是使用 `-l` 选项添加到 Pod 中的 `app=backend` 标签。这就是你将在前端
    Pod 的 `podAffinity` 配置中使用的标签。
- en: Specifying pod affinity in a pod definition
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pod 定义中指定 pod affinity
- en: The frontend pod’s definition is shown in the following listing.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 前端 Pod 的定义如下所示。
- en: 'Listing 16.13\. Pod using `podAffinity`: frontend-podaffinity-host.yaml'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 16.13\. 使用 `podAffinity` 的 Pod：frontend-podaffinity-host.yaml
- en: '`apiVersion: extensions/v1beta1 kind: Deployment metadata:   name: frontend
    spec:   replicas: 5   template:     ...     spec:       affinity:         podAffinity:`
    `1` `requiredDuringSchedulingIgnoredDuringExecution:` `2` `- topologyKey: kubernetes.io/hostname`
    `3` `labelSelector:` `3` `matchLabels:` `3` `app: backend` `3` `...`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: extensions/v1beta1 kind: Deployment metadata:   name: frontend
    spec:   replicas: 5   template:     ...     spec:       affinity:         podAffinity:`
    `1` `requiredDuringSchedulingIgnoredDuringExecution:` `2` `- topologyKey: kubernetes.io/hostname`
    `3` `labelSelector:` `3` `matchLabels:` `3` `app: backend` `3` `...`'
- en: 1 Defining podAffinity rules
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 定义 podAffinity 规则
- en: 2 Defining a hard requirement, not a preference
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 定义硬性要求，而不是偏好
- en: 3 The pods of this Deployment must be deployed on the same node as the pods
    that match the selector.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 此 Deployment 的 Pod 必须部署在与选择器匹配的 Pod 相同的节点上。
- en: The listing shows that this Deployment will create pods that have a hard requirement
    to be deployed on the same node (specified by the `topologyKey` field) as pods
    that have the `app=backend` label (see [figure 16.4](#filepos1527059)).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 列表显示，这个 Deployment 将创建具有硬性要求部署在具有 `app=backend` 标签的 Pod 相同节点（由 `topologyKey`
    字段指定）上的 Pod（参见[图 16.4](#filepos1527059)）。
- en: Figure 16.4\. Pod affinity allows scheduling pods to the node where other pods
    with a specific label are.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.4\. Pod 亲和性允许将 Pod 调度到具有特定标签的其他 Pod 所在的节点。
- en: '![](images/00017.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00017.jpg)'
- en: '|  |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Instead of the simpler `matchLabels` field, you could also use the more expressive
    `matchExpressions` field.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 除了更简单的 `matchLabels` 字段外，你也可以使用更表达性的 `matchExpressions` 字段。
- en: '|  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Deploying a pod with pod affinity
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 部署具有 Pod 亲和性的 Pod
- en: 'Before you create this Deployment, let’s see which node the backend pod was
    scheduled to earlier:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在你创建此 Deployment 之前，让我们看看后端 Pod 之前被调度到了哪个节点：
- en: '`$ kubectl get po -o wide` `NAME                   READY  STATUS   RESTARTS 
    AGE  IP         NODE backend-257820-qhqj6   1/1    Running  0         8m   10.47.0.1`
    `node2``.k8s`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po -o wide` `NAME                   READY  STATUS   RESTARTS 
    AGE  IP         NODE backend-257820-qhqj6   1/1    Running  0         8m   10.47.0.1`
    `node2``.k8s`'
- en: When you create the frontend pods, they should be deployed to `node2` as well.
    You’re going to create the Deployment and see where the pods are deployed. This
    is shown in the next listing.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 当你创建前端 Pod 时，它们也应该部署到 `node2` 上。你将创建 Deployment 并查看 Pod 的部署位置。这将在下一个列表中展示。
- en: Listing 16.14\. Deploying frontend pods and seeing which node they’re scheduled
    to
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 16.14\. 部署前端 Pod 并查看它们被调度到哪个节点
- en: '`$ kubectl create -f frontend-podaffinity-host.yaml` `deployment "frontend"
    created` `$ kubectl get po -o wide` `NAME                   READY  STATUS    RESTARTS 
    AGE  IP         NODE backend-257820-qhqj6   1/1    Running   0         8m   10.47.0.1`
    `node2``.k8s frontend-121895-2c1ts  1/1    Running   0         13s  10.47.0.6`
    `node2``.k8s frontend-121895-776m7  1/1    Running   0         13s  10.47.0.4`
    `node2``.k8s frontend-121895-7ffsm  1/1    Running   0         13s  10.47.0.8`
    `node2``.k8s frontend-121895-fpgm6  1/1    Running   0         13s  10.47.0.7`
    `node2``.k8s frontend-121895-vb9ll  1/1    Running   0         13s  10.47.0.5`
    `node2``.k8s`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl create -f frontend-podaffinity-host.yaml` `deployment "frontend"
    created` `$ kubectl get po -o wide` `NAME                   READY  STATUS    RESTARTS 
    AGE  IP         NODE backend-257820-qhqj6   1/1    Running   0         8m   10.47.0.1`
    `node2``.k8s frontend-121895-2c1ts  1/1    Running   0         13s  10.47.0.6`
    `node2``.k8s frontend-121895-776m7  1/1    Running   0         13s  10.47.0.4`
    `node2``.k8s frontend-121895-7ffsm  1/1    Running   0         13s  10.47.0.8`
    `node2``.k8s frontend-121895-fpgm6  1/1    Running   0         13s  10.47.0.7`
    `node2``.k8s frontend-121895-vb9ll  1/1    Running   0         13s  10.47.0.5`
    `node2``.k8s`'
- en: All the frontend pods were indeed scheduled to the same node as the backend
    pod. When scheduling the frontend pod, the Scheduler first found all the pods
    that match the `labelSelector` defined in the frontend pod’s `podAffinity` configuration
    and then scheduled the frontend pod to the same node.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 所有前端 Pod 确实都被调度到了与后端 Pod 相同的节点。在调度前端 Pod 时，调度器首先找到所有与前端 Pod `podAffinity` 配置中定义的
    `labelSelector` 匹配的 Pod，然后将前端 Pod 调度到同一节点。
- en: Understanding how the scheduler uses pod affinity rules
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 理解调度器如何使用 Pod 亲和性规则
- en: What’s interesting is that if you now delete the backend pod, the Scheduler
    will schedule the pod to `node2` even though it doesn’t define any pod affinity
    rules itself (the rules are only on the frontend pods). This makes sense, because
    otherwise if the backend pod were to be deleted by accident and rescheduled to
    a different node, the frontend pods’ affinity rules would be broken.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，如果你现在删除后端 Pod，即使调度器本身没有定义任何 Pod 亲和性规则（规则仅在前端 Pod 上），调度器也会将 Pod 调度到 `node2`。这是有道理的，因为否则如果后端
    Pod 被意外删除并重新调度到不同的节点，前端 Pod 的亲和性规则将会被破坏。
- en: You can confirm the Scheduler takes other pods’ pod affinity rules into account,
    if you increase the Scheduler’s logging level and then check its log. The following
    listing shows the relevant log lines.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你增加调度器的日志级别并检查其日志，你可以确认调度器考虑了其他 Pod 的 Pod 亲和性规则。以下列表显示了相关的日志行。
- en: Listing 16.15\. Scheduler log showing why the backend pod is scheduled to `node2`
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 16.15\. 调度器日志显示为什么后端 Pod 被调度到 `node2`
- en: '`... Attempting to schedule pod: default/backend-257820-qhqj6 ... ... ... backend-qhqj6
    -> node2.k8s: Taint Toleration Priority, Score: (10) ... backend-qhqj6 -> node1.k8s:
    Taint Toleration Priority, Score: (10)` `... backend-qhqj6 -> node2.k8s: InterPodAffinityPriority,
    Score: (10)``... backend-qhqj6 -> node1.k8s: InterPodAffinityPriority, Score:
    (0)` `... backend-qhqj6 -> node2.k8s: SelectorSpreadPriority, Score: (10) ...
    backend-qhqj6 -> node1.k8s: SelectorSpreadPriority, Score: (10) ... backend-qhqj6
    -> node2.k8s: NodeAffinityPriority, Score: (0) ... backend-qhqj6 -> node1.k8s:
    NodeAffinityPriority, Score: (0) ... Host node2.k8s => Score 100030 ... Host node1.k8s
    => Score 100022 ... Attempting to bind backend-257820-qhqj6 to node2.k8s`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`... 尝试调度 Pod：default/backend-257820-qhqj6 ... ... ... backend-qhqj6 -> node2.k8s:
    污点容忍优先级，得分：(10) ... backend-qhqj6 -> node1.k8s: 污点容忍优先级，得分：(10)` `... backend-qhqj6
    -> node2.k8s: Pod 亲和性优先级，得分：(10)``... backend-qhqj6 -> node1.k8s: Pod 亲和性优先级，得分：(0)`
    `... backend-qhqj6 -> node2.k8s: 选择器扩散优先级，得分：(10) ... backend-qhqj6 -> node1.k8s:
    选择器扩散优先级，得分：(10) ... backend-qhqj6 -> node2.k8s: 节点亲和性优先级，得分：(0) ... backend-qhqj6
    -> node1.k8s: 节点亲和性优先级，得分：(0) ... 主节点 node2.k8s => 得分 100030 ... 主节点 node1.k8s
    => 得分 100022 ... 尝试将 backend-257820-qhqj6 绑定到 node2.k8s`'
- en: If you focus on the two lines in bold, you’ll see that during the scheduling
    of the backend pod, `node2` received a higher score than `node1` because of inter-pod
    affinity.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你关注两条加粗的行，你将看到在调度后端 Pod 时，由于 Pod 亲和性，`node2` 收到的得分高于 `node1`。
- en: 16.3.2\. Deploying pods in the same rack, availability zone, or geographic region
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 16.3.2\. 在同一机架、可用区或地理区域内部署 Pod
- en: In the previous example, you used `podAffinity` to deploy frontend pods onto
    the same node as the backend pods. You probably don’t want all your frontend pods
    to run on the same machine, but you’d still like to keep them close to the backend
    pod—for example, run them in the same availability zone.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，您使用了 `podAffinity` 将前端 Pod 部署到与后端 Pod 相同的节点上。您可能不希望所有前端 Pod 都运行在同一台机器上，但您仍然希望将它们保持靠近后端
    Pod——例如，在同一个可用区运行它们。
- en: Co-locating pods in the same availability zone
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一可用区协同放置 Pod
- en: The cluster I’m using runs in three VMs on my local machine, so all the nodes
    are in the same availability zone, so to speak. But if the nodes were in different
    zones, all I’d need to do to run the frontend pods in the same zone as the backend
    pod would be to change the `topologyKey` property to `failure-domain.beta.kubernetes.io/zone`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用的集群在我的本地机器上的三个虚拟机上运行，所以所有节点都在同一个可用区，换句话说。但如果节点在不同的区域，要运行与后端 Pod 在同一区域的前端
    Pod，只需将 `topologyKey` 属性更改为 `failure-domain.beta.kubernetes.io/zone`。
- en: Co-locating pods in the same geographical region
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一地理区域内协同放置 Pod
- en: To allow the pods to be deployed in the same region instead of the same zone
    (cloud providers usually have datacenters located in different geographical regions
    and split into multiple availability zones in each region), the `topologyKey`
    would be set to `failure-domain.beta.kubernetes.io/region`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 要允许 Pod 在同一区域而不是同一区域（云服务提供商通常在不同的地理区域拥有数据中心，并在每个区域中分割成多个可用区）部署，`topologyKey`
    应设置为 `failure-domain.beta.kubernetes.io/region`。
- en: Understanding how topologyKey works
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 `topologyKey` 的工作原理
- en: The way `topologyKey` works is simple. The three keys we’ve mentioned so far
    aren’t special. If you want, you can easily use your own `topologyKey`, such as
    `rack`, to have the pods scheduled to the same server rack. The only prerequisite
    is to add a `rack` label to your nodes. This scenario is shown in [figure 16.5](#filepos1534147).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`topologyKey` 的工作方式很简单。我们之前提到的三个键并不特殊。如果您愿意，可以轻松地使用自己的 `topologyKey`，例如 `rack`，以便将
    Pod 调度到同一服务器机架。唯一的前提是向您的节点添加一个 `rack` 标签。这种情况在 [图 16.5](#filepos1534147) 中有所展示。'
- en: Figure 16.5\. The `topologyKey in podAffinity` determines the scope of where
    the pod should be scheduled to.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.5. `podAffinity` 中的 `topologyKey` 决定了 Pod 应该被调度到的范围。
- en: '![](images/00038.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00038.jpg)'
- en: For example, if you had 20 nodes, with 10 in each rack, you’d label the first
    ten as `rack=rack1` and the others as `rack=rack2`. Then, when defining a pod’s
    `podAffinity`, you’d set the `toplogyKey` to `rack`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你有 20 个节点，每个机架有 10 个，你将前十个标记为 `rack=rack1`，其余的标记为 `rack=rack2`。然后，当定义 Pod
    的 `podAffinity` 时，你会将 `toplogyKey` 设置为 `rack`。
- en: When the Scheduler is deciding where to deploy a pod, it checks the pod’s `pod-Affinity`
    config, finds the pods that match the label selector, and looks up the nodes they’re
    running on. Specifically, it looks up the nodes’ label whose key matches the `topologyKey`
    field specified in `podAffinity`. Then it selects all the nodes whose label matches
    the values of the pods it found earlier. In [figure 16.5](#filepos1534147), the
    label selector matched the backend pod, which runs on Node 12\. The value of the
    `rack` label on that node equals `rack2`, so when scheduling a frontend pod, the
    Scheduler will only select among the nodes that have the `rack=rack2` label.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当调度器决定部署 Pod 的位置时，它会检查 Pod 的 `pod-Affinity` 配置，找到匹配标签选择器的 Pod，并查找它们运行的节点。具体来说，它会查找节点标签中键与
    `podAffinity` 中指定的 `topologyKey` 字段匹配的标签。然后，它选择所有标签与它之前找到的 Pod 的值匹配的节点。在 [图 16.5](#filepos1534147)
    中，标签选择器匹配了运行在 Node 12 上的后端 Pod。该节点上 `rack` 标签的值等于 `rack2`，因此当调度前端 Pod 时，调度器将只选择具有
    `rack=rack2` 标签的节点。
- en: '|  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: By default, the label selector only matches pods in the same namespace as the
    pod that’s being scheduled. But you can also select pods from other namespaces
    by adding a `namespaces` field at the same level as `label-Selector`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，标签选择器仅匹配与正在调度的 Pod 在同一命名空间中的 Pod。但您也可以通过添加与 `label-Selector` 同级的 `namespaces`
    字段来选择来自其他命名空间的 Pod。
- en: '|  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 16.3.3\. Expressing pod affinity preferences instead of hard requirements
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 16.3.3. 表达 Pod 亲和力偏好而不是硬性要求
- en: Earlier, when we talked about node affinity, you saw that `nodeAffinity` can
    be used to express a hard requirement, which means a pod is only scheduled to
    nodes that match the node affinity rules. It can also be used to specify node
    preferences, to instruct the Scheduler to schedule the pod to certain nodes, while
    allowing it to schedule it anywhere else if those nodes can’t fit the pod for
    any reason.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，当我们讨论节点亲和性时，你看到`nodeAffinity`可以用来表达一个硬性要求，这意味着Pod只被调度到符合节点亲和性规则的节点上。它也可以用来指定节点偏好，指示调度器将Pod调度到特定的节点，如果这些节点因为任何原因无法容纳Pod，则允许调度到其他任何地方。
- en: The same also applies to `podAffinity`. You can tell the Scheduler you’d prefer
    to have your frontend pods scheduled onto the same node as your backend pod, but
    if that’s not possible, you’re okay with them being scheduled elsewhere. An example
    of a Deployment using the `preferredDuringSchedulingIgnoredDuringExecution` pod
    affinity rule is shown in the next listing.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这也适用于`podAffinity`。你可以告诉调度器你希望你的前端Pod被调度到与你的后端Pod相同的节点上，但如果那不可能，你也能接受它们被调度到其他地方。下面是一个使用`preferredDuringSchedulingIgnoredDuringExecution`
    Pod亲和性规则的Deployment示例。
- en: Listing 16.16\. Pod affinity preference
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.16\. Pod亲和性偏好
- en: '`apiVersion: extensions/v1beta1 kind: Deployment metadata:   name: frontend
    spec:   replicas: 5   template:     ...     spec:       affinity:         podAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:` `1` `- weight: 80`
    `2` `podAffinityTerm:` `2` `topologyKey: kubernetes.io/hostname` `2` `labelSelector:`
    `2` `matchLabels:` `2` `app: backend` `2` `containers: ...`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: extensions/v1beta1 kind: Deployment metadata:   name: frontend
    spec:   replicas: 5   template:     ...     spec:       affinity:         podAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:` `1` `- weight: 80`
    `2` `podAffinityTerm:` `2` `topologyKey: kubernetes.io/hostname` `2` `labelSelector:`
    `2` `matchLabels:` `2` `app: backend` `2` `containers: ...`'
- en: 1 Preferred instead of Required
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 偏好而非必需
- en: 2 A weight and a podAffinity term is specified as in the previous example
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 在上一个示例中指定了权重和一个Pod亲和性项
- en: As in `nodeAffinity` preference rules, you need to define a weight for each
    rule. You also need to specify the `topologyKey` and `labelSelector`, as in the
    hard-requirement `podAffinity` rules. [Figure 16.6](#filepos1539056) shows this
    scenario.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 与`nodeAffinity`偏好规则一样，你需要为每个规则定义一个权重。你还需要指定`topologyKey`和`labelSelector`，就像硬性要求的`podAffinity`规则中那样。[图16.6](#filepos1539056)展示了这个场景。
- en: Figure 16.6\. Pod affinity can be used to make the Scheduler prefer nodes where
    pods with a certain label are running.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.6\. 可以使用Pod亲和性来使调度器偏好运行具有特定标签的Pod的节点。
- en: '![](images/00057.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00057.jpg)'
- en: Deploying this pod, as with your `nodeAffinity` example, deploys four pods on
    the same node as the backend pod, and one pod on the other node (see the following
    listing).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 部署这个Pod，就像你的`nodeAffinity`示例一样，将四个Pod部署到与后端Pod相同的节点上，另一个Pod部署到其他节点上（见以下列表）。
- en: Listing 16.17\. Pods deployed with `podAffinity` preferences
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.17\. 使用`podAffinity`偏好的Pod部署
- en: '`$ kubectl get po -o wide` `NAME                   READY  STATUS   RESTARTS 
    AGE  IP          NODE backend-257820-ssrgj   1/1    Running  0         1h   10.47.0.9  
    node2.k8s frontend-941083-3mff9  1/1    Running  0         8m   10.44.0.4   node1.k8s
    frontend-941083-7fp7d  1/1    Running  0         8m   10.47.0.6   node2.k8s frontend-941083-cq23b 
    1/1    Running  0         8m   10.47.0.1   node2.k8s frontend-941083-m70sw  1/1   
    Running  0         8m   10.47.0.5   node2.k8s frontend-941083-wsjv8  1/1    Running 
    0         8m   10.47.0.4   node2.k8s`'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po -o wide` `NAME                   READY  STATUS   RESTARTS 
    AGE  IP          NODE backend-257820-ssrgj   1/1    Running  0         1h   10.47.0.9  
    node2.k8s frontend-941083-3mff9  1/1    Running  0         8m   10.44.0.4   node1.k8s
    frontend-941083-7fp7d  1/1    Running  0         8m   10.47.0.6   node2.k8s frontend-941083-cq23b 
    1/1    Running  0         8m   10.47.0.1   node2.k8s frontend-941083-m70sw  1/1   
    Running  0         8m   10.47.0.5   node2.k8s frontend-941083-wsjv8  1/1    Running 
    0         8m   10.47.0.4   node2.k8s`'
- en: 16.3.4\. Scheduling pods away from each other with pod anti-affinity
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 16.3.4\. 使用Pod反亲和性将Pod调度到彼此远离的位置
- en: You’ve seen how to tell the Scheduler to co-locate pods, but sometimes you may
    want the exact opposite. You may want to keep pods away from each other. This
    is called pod anti-affinity. It’s specified the same way as pod affinity, except
    that you use the `podAntiAffinity` property instead of `podAffinity`, which results
    in the Scheduler never choosing nodes where pods matching the `podAntiAffinity`’s
    label selector are running, as shown in [figure 16.7](#filepos1541209).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到了如何告诉调度器将Pod放置在一起，但有时你可能想要完全相反的效果。你可能希望将Pod彼此隔离开来。这被称为Pod反亲和性。它与Pod亲和性的指定方式相同，只是你使用`podAntiAffinity`属性而不是`podAffinity`，这会导致调度器永远不会选择运行匹配`podAntiAffinity`标签选择器的Pod的节点，如图16.7所示。
- en: Figure 16.7\. Using pod anti-affinity to keep pods away from nodes that run
    pods with a certain label.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7\. 使用Pod反亲和性将Pod与运行具有特定标签的Pod的节点隔离开来。
- en: '![](images/00077.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00077.jpg)'
- en: An example of why you’d want to use pod anti-affinity is when two sets of pods
    interfere with each other’s performance if they run on the same node. In that
    case, you want to tell the Scheduler to never schedule those pods on the same
    node. Another example would be to force the Scheduler to spread pods of the same
    group across different availability zones or regions, so that a failure of a whole
    zone (or region) never brings the service down completely.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Pod反亲和性的一个例子是，当两组Pod在同一个节点上运行时，它们会相互干扰性能。在这种情况下，你希望告诉调度器永远不要将这些Pod调度到同一个节点上。另一个例子是强制调度器将同一组的Pod分散到不同的可用区或区域，这样整个区域（或区域）的故障永远不会完全使服务中断。
- en: Using anti-affinity to spread apart pods of the same Deployment
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 使用反亲和性将同一Deployment的Pod分散开来
- en: Let’s see how to force your frontend pods to be scheduled to different nodes.
    The following listing shows how the pods’ anti-affinity is configured.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何强制将你的前端Pod调度到不同的节点。以下列表显示了如何配置Pod的反亲和性。
- en: 'Listing 16.18\. Pods with anti-affinity: frontend-podantiaffinity-host.yaml'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.18\. 具有反亲和性的Pod：frontend-podantiaffinity-host.yaml
- en: '`apiVersion: extensions/v1beta1 kind: Deployment metadata:   name: frontend
    spec:   replicas: 5   template:     metadata:       labels:` `1` `app: frontend`
    `1` `spec:       affinity:         podAntiAffinity:` `2` `requiredDuringSchedulingIgnoredDuringExecution:`
    `2` `- topologyKey: kubernetes.io/hostname` `3` `labelSelector:` `3` `matchLabels:`
    `3` `app: frontend` `3` `containers: ...`'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: extensions/v1beta1 kind: Deployment metadata: name: frontend spec:
    replicas: 5 template: metadata: labels: app: frontend spec: affinity: podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution: topologyKey: kubernetes.io/hostname
    labelSelector: matchLabels: app: frontend containers: ...`'
- en: 1 The frontend pods have the app=frontend label.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 前端Pod具有app=frontend标签。
- en: 2 Defining hard-requirements for pod anti-affinity
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 定义Pod反亲和性的硬性要求
- en: 3 A frontend pod must not be scheduled to the same machine as a pod with app=frontend
    label.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 前端Pod不得被调度到与具有app=frontend标签的Pod相同的机器上。
- en: This time, you’re defining `podAntiAffinity` instead of `podAffinity`, and you’re
    making the `labelSelector` match the same pods that the Deployment creates. Let’s
    see what happens when you create this Deployment. The pods created by it are shown
    in the following listing.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，你定义的是`podAntiAffinity`而不是`podAffinity`，并且你使`labelSelector`与Deployment创建的相同Pod匹配。让我们看看创建此Deployment时会发生什么。它创建的Pod如下所示。
- en: Listing 16.19\. Pods created by the Deployment
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.19\. 由Deployment创建的Pod
- en: '`$ kubectl get po -l app=frontend -o wide` `NAME                    READY 
    STATUS   RESTARTS  AGE  IP         NODE frontend-286632-0lffz   0/1    Pending 
    0         1m   <none> frontend-286632-2rkcz   1/1    Running  0         1m   10.47.0.1 
    node2.k8s frontend-286632-4nwhp   0/1    Pending  0         1m   <none> frontend-286632-h4686  
    0/1    Pending  0         1m   <none> frontend-286632-st222   1/1    Running 
    0         1m   10.44.0.4  node1.k8s`'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po -l app=frontend -o wide` `NAME                    READY 
    STATUS   RESTARTS  AGE  IP         NODE frontend-286632-0lffz   0/1    Pending 
    0         1m   <none> frontend-286632-2rkcz   1/1    Running  0         1m   10.47.0.1 
    node2.k8s frontend-286632-4nwhp   0/1    Pending  0         1m   <none> frontend-286632-h4686  
    0/1    Pending  0         1m   <none> frontend-286632-st222   1/1    Running 
    0         1m   10.44.0.4  node1.k8s`'
- en: As you can see, only two pods were scheduled—one to `node1`, the other to `node2`.
    The three remaining pods are all `Pending`, because the Scheduler isn’t allowed
    to schedule them to the same nodes.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，只有两个Pod被调度——一个调度到`node1`，另一个调度到`node2`。剩下的三个Pod都是`Pending`状态，因为调度器不允许将它们调度到相同的节点。
- en: Using preferential pod anti-affinity
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 使用优先Pod反亲和性
- en: In this case, you probably should have specified a soft requirement instead
    (using the `preferredDuringSchedulingIgnoredDuringExecution` property). After
    all, it’s not such a big problem if two frontend pods run on the same node. But
    in scenarios where that’s a problem, using `requiredDuringScheduling` is appropriate.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你可能应该指定一个软性要求而不是硬性要求（使用`preferredDuringSchedulingIgnoredDuringExecution`属性）。毕竟，如果两个前端Pod运行在同一节点上并不是一个大问题。但在那种情况下，使用`requiredDuringScheduling`是合适的。
- en: As with pod affinity, the `topologyKey` property determines the scope of where
    the pod shouldn’t be deployed to. You can use it to ensure pods aren’t deployed
    to the same rack, availability zone, region, or any custom scope you create using
    custom node labels.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 与Pod亲和性一样，`topologyKey`属性决定了Pod不应部署到的范围。你可以使用它来确保Pod不会被部署到同一机架、可用区、区域或任何你使用自定义节点标签创建的任何自定义范围。
- en: 16.4\. Summary
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 16.4. 摘要
- en: In this chapter, we looked at how to ensure pods aren’t scheduled to certain
    nodes or are only scheduled to specific nodes, either because of the node’s labels
    or because of the pods running on them.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何确保Pod不会被调度到某些节点或只被调度到特定节点，这可能是由于节点的标签或运行在其上的Pod。
- en: You learned that
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 你了解到
- en: If you add a taint to a node, pods won’t be scheduled to that node unless they
    tolerate that taint.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你向节点添加污点，除非Pod容忍该污点，否则Pod不会被调度到该节点。
- en: 'Three types of taints exist: `NoSchedule` completely prevents scheduling, `Prefer-NoSchedule`
    isn’t as strict, and `NoExecute` even evicts existing pods from a node.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在三种类型的污点：`NoSchedule`完全阻止调度，`Prefer-NoSchedule`不那么严格，而`NoExecute`甚至可以将现有Pod从节点中驱逐出去。
- en: The `NoExecute` taint is also used to specify how long the Control Plane should
    wait before rescheduling the pod when the node it runs on becomes unreachable
    or unready.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NoExecute`污点也用于指定当节点变得不可达或未准备好时，控制平面应该等待多长时间才重新调度Pod。'
- en: Node affinity allows you to specify which nodes a pod should be scheduled to.
    It can be used to specify a hard requirement or to only express a node preference.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点亲和性允许你指定Pod应该调度到哪些节点。它可以用来指定硬性要求或仅表达节点偏好。
- en: Pod affinity is used to make the Scheduler deploy pods to the same node where
    another pod is running (based on the pod’s labels).
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod亲和性用于使调度器将Pod部署到运行另一个Pod的同一节点（基于Pod的标签）。
- en: Pod affinity’s `topologyKey` specifies how close the pod should be deployed
    to the other pod (onto the same node or onto a node in the same rack, availability
    zone, or availability region).
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod亲和性的`topologyKey`指定了Pod应该部署得有多接近另一个Pod（在同一节点上或在同一机架、可用区或可用区域内节点上）。
- en: Pod anti-affinity can be used to keep certain pods away from each other.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod反亲和性可以用来保持某些Pod彼此远离。
- en: Both pod affinity and anti-affinity, like node affinity, can either specify
    hard requirements or preferences.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与节点亲和性一样，Pod亲和性和反亲和性都可以指定硬性要求或偏好。
- en: In the next chapter, you’ll learn about best practices for developing apps and
    how to make them run smoothly in a Kubernetes environment.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将了解开发应用程序的最佳实践以及如何在Kubernetes环境中使它们运行顺畅。
