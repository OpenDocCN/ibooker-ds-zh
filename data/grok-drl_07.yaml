- en: 7 Achieving goals more effectively and efficiently
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 更有效地实现目标
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中
- en: You will learn about making reinforcement learning agents more effective at
    reaching optimal performance when interacting with challenging environments.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将学习如何使强化学习代理在与具有挑战性的环境交互时更有效地达到最佳性能。
- en: You will learn about making reinforcement learning agents more efficient at
    achieving goals by making the most of the experiences.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将学习如何通过充分利用经验来使强化学习代理更有效地实现目标。
- en: You will improve on the agents presented in the previous chapters to have them
    make the most out of the data they collect and therefore optimize their performance
    more quickly.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将改进上一章中展示的代理，使它们能够充分利用收集到的数据，从而更快地优化其性能。
- en: Efficiency is doing things right; effectiveness is doing the right things.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 效率是正确地做事；有效性是做正确的事。
- en: — Peter Drucker Founder of modern management and Presidential Medal of Freedom
    recipient
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: — 彼得·德鲁克 现代管理之父和自由勋章获得者
- en: In this chapter, we improve on the agents you learned about in the previous
    chapter. More specifically, we take on two separate lines of improvement. First,
    we use the *λ*-return that you learned about in chapter 5 for the policy evaluation
    requirements of the generalized policy iteration pattern. We explore using the
    *λ*-return for both on-policy and off-policy methods. Using the *λ*-return with
    eligibility traces propagates credit to the right state-action pairs more quickly
    than standard methods, making the value-function estimates get near the actual
    values faster.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们改进了上一章中介绍过的代理。更具体地说，我们采取了两个独立的改进方向。首先，我们使用第5章中学习的*λ*-回报来满足广义策略迭代模式中的策略评估需求。我们探讨了使用*λ*-回报来进行在线和离线方法。使用带有资格痕迹的*λ*-回报比标准方法更快地将信用传播到正确的状态-动作对，从而使价值函数估计更快地接近实际值。
- en: Second, we explore algorithms that use experience samples to learn a model of
    the environment, a Markov decision process (MDP). By doing so, these methods extract
    the most out of the data they collect and often arrive at optimality more quickly
    than methods that don’t. The group of algorithms that attempt to learn a model
    of the environment is referred to as *model-based reinforcement learning*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们探索使用经验样本来学习环境模型，即马尔可夫决策过程（MDP）的算法。通过这样做，这些方法能够从收集的数据中提取出最大价值，并且通常比那些不使用模型的方法更快地达到最优解。试图学习环境模型的算法组被称为**基于模型的强化学习**。
- en: It’s important to note that even though we explore these lines of improvements
    separately, nothing prevents you from trying to combine them, and it’s perhaps
    something you should do after finishing this chapter. Let’s get to the details
    right away.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，尽管我们分别探索了这些改进方向，但没有任何东西阻止你尝试将它们结合起来，也许在完成本章后你应该这样做。让我们立即深入了解细节。
- en: '| ŘŁ | With An RL AccentPlanning vs. model-free RL vs. model-based RL |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| ŘŁ | 基于规划的强化学习 vs. 无模型强化学习 vs. 基于模型的强化学习 |'
- en: '|  | **Planning:** Refers to algorithms that require a model of the environment
    to produce a policy. Planning methods can be of state-space planning type, which
    means they use the state space to find a policy, or they can be of plan-space
    planning type, meaning they search in the space of all possible plans (think about
    genetic algorithms.) Examples of planning algorithms that we’ve learned about
    in this book are value iteration and policy iteration.**Model-free RL:** Refers
    to algorithms that don’t use models of the environments, but are still able to
    produce a policy. The unique characteristic here is these methods obtain policies
    without the use of a map, a model, or an MDP. Instead, they use trial-and-error
    learning to obtain policies. Several examples of model-free RL algorithms that
    we have explored in this book are MC, SARSA, and Q-learning.**Model-based RL:**
    Refers to algorithms that can learn, but don’t require, a model of the environment
    to produce a policy. The distinction is they don’t require models in advance,
    but can certainly make good use of them if available, and more importantly, attempt
    to learn the models through interaction with the environment. Several examples
    of model-based RL algorithms we learn about in this chapter are Dyna-Q and trajectory
    sampling. |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '|  | **规划：**指的是需要环境模型来生成策略的算法。规划方法可以是状态空间规划类型，这意味着它们使用状态空间来找到策略，或者它们可以是计划空间规划类型，这意味着它们在所有可能计划的空间中搜索（想想遗传算法）。本书中我们学习过的规划算法示例包括价值迭代和政策迭代。**无模型强化学习：**指的是不使用环境模型但仍然能够生成策略的算法。这里的独特特性是这些方法在没有地图、模型或MDP的情况下获得策略。相反，它们使用试错学习来获得策略。本书中我们探索过的无模型强化学习算法的几个例子是MC、SARSA和Q-learning。**基于模型的强化学习：**指的是可以学习但不需要环境模型来生成策略的算法。区别在于它们不需要预先模型，但当然可以很好地利用它们，更重要的是，它们试图通过与环境的交互来学习模型。本章我们学习的基于模型的强化学习算法的几个例子是Dyna-Q和轨迹采样。
    |'
- en: Learning to improve policies using robust targets
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用鲁棒目标改进策略的学习
- en: The first line of improvement we discuss in this chapter is using more robust
    targets in our policy-evaluation methods. Recall that in chapter 5, we explored
    policy-evaluation methods that use different kinds of targets for estimating value
    functions. You learned about the Monte Carlo and *TD* approaches, but also about
    a target called the *λ*-return that uses a weighted combination of targets obtained
    using all visited states.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们讨论的第一个改进方案是在我们的策略评估方法中使用更鲁棒的目标。回想一下，在第5章中，我们探讨了使用不同类型的目标来估计价值函数的策略评估方法。你学习了蒙特卡洛和*TD*方法，还了解了一种称为*λ*-回报的目标，它使用所有访问过的状态获得的目标的加权组合。
- en: '*TD*(*λ*)) is the prediction method that uses the *λ*-return for our policy
    evaluation needs. However, as you remember from the previous chapter, when dealing
    with the control problem, we need to use a policy-evaluation method for estimating
    action-value functions, and a policy-improvement method that allows for exploration.
    In this section, we discuss control methods similar to SARSA and Q-learning, but
    use instead the *λ*-return.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*TD*(*λ*)是我们用于策略评估需要的预测方法。然而，正如你从上一章所记得的，在处理控制问题时，我们需要使用策略评估方法来估计动作值函数，以及允许探索的策略改进方法。在本节中，我们讨论类似于SARSA和Q学习的控制方法，但使用的是*λ*-回报。'
- en: '| ![](../Images/icons_Concrete.png) | A Concrete ExampleThe slippery walk seven
    environment |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| ![具体示例](../Images/icons_Concrete.png) | 滑坡行走环境 |'
- en: '|  | To introduce the algorithms in this chapter, we use the same environment
    we used in the previous chapter, called slippery walk seven (SWS). However, at
    the end of the chapter, we test the methods in much more challenging environments.Recall
    that the SWS is a walk, a single-row grid-world environment, with seven non-terminal
    states. Remember that this environment is a “slippery” walk, meaning that it’s
    noisy, that action effects are stochastic. If the agent chooses to go left, there’s
    a chance it does, but there’s also some chance that it goes right, or that it
    stays in place.![](../Images/07_01_Sidebar01.png)Slippery walk seven environment
    MDPAs a refresher, above is the MDP of this environments. But remember and always
    have in mind that the agent doesn’t have any access to the transition probabilities.
    The dynamics of this environment are unknown to the agent. Also, to the agent,
    there are no relationships between the states in advance. |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '|  | 为了介绍本章中的算法，我们使用与上一章相同的同一个环境，称为滑行七（SWS）。然而，在本章结束时，我们在更具挑战性的环境中测试了这些方法。回想一下，SWS是一种行走，一个单行网格世界环境，有七个非终止状态。记住，这个环境是一个“滑行”的行走，这意味着它是嘈杂的，动作效果是随机的。如果代理选择向左走，它可能会这样做，但也有可能向右走，或者保持在原地。![滑行七环境
    MDP](../Images/07_01_Sidebar01.png)滑行七环境 MDP作为一个复习，上面是此环境的MDP。但请记住，并且始终牢记，代理无法访问过渡概率。这个环境的动态对代理来说是未知的。此外，对代理来说，预先没有状态之间的关系。|'
- en: 'SARSA(λ): Improving policies after each step based on multi-step estimates'
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SARSA(λ)：根据多步估计在每一步改进策略
- en: '*sARSA***(***λ***)** is a straightforward improvement to the original SARSA
    agent. The main difference between SARSA and SARSA(*λ*) is that instead of using
    the one-step bootstrapping target—the *TD* target, as we do in SARSA, in SARSA(*λ*),
    we use the *λ*-return. And that’s it; you have SARSA(*λ*). Seriously! Did you
    see how learning the basics makes the more complex concepts easier?'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*sARSA***(***λ***)**是对原始SARSA代理的一个简单改进。SARSA和SARSA(*λ*)之间的主要区别在于，在SARSA(*λ*)中，我们使用*λ*-回报，而不是像在SARSA中那样使用一步引导目标——*TD*目标。就是这样；你就有SARSA(*λ*)了。真的！你看到学习基础知识是如何让更复杂的概念变得容易了吗？'
- en: Now, I’d like to dig a little deeper into the concept of eligibility traces
    that you first read about in chapter 5\. The type of eligibility trace I introduced
    in chapter 5 is called the *accumulating trace*. However, in reality, there are
    multiple ways of tracing state or state-action pairs responsible for a reward.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我想深入探讨一下你最初在第五章中读到的资格迹的概念。我在第五章中介绍的资格迹类型被称为*累积迹*。然而，在现实中，有多种方式来追踪导致奖励的状态或状态-动作对。
- en: In this section, we dig deeper into the accumulating trace and adapt it for
    solving the control problem, but we also explore a different kind of trace called
    the *replacing trace* and use them both in the SARSA(*λ*) agent.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们深入挖掘累积迹，并将其应用于解决控制问题，同时我们还探索了一种不同类型的迹，称为*替换迹*，并在SARSA(*λ*)代理中使用这两种迹。
- en: '| 0001 | A Bit Of HistoryIntroduction of the SARSA and SARSA(*λ*) agents |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 0001 | 历史简介：SARSA 和 SARSA(*λ*)代理介绍'
- en: '|  | In 1994, Gavin Rummery and Mahesan Niranjan published a paper titled “Online
    Q-Learning Using Connectionist Systems,” in which they introduced an algorithm
    they called at the time “Modified Connectionist Q-Learning.” In 1996, Singh and
    Sutton dubbed this algorithm SARSA because of the quintuple of events that the
    algorithm uses: (*S*[*t*], *A*[*t*], *R*[*t*+1], *S*[*t*+1], *A*[*t*+1]). People
    often like knowing where these names come from, and as you’ll soon see, RL researchers
    can get pretty creative with these names.Funny enough, before this open and “unauthorized”
    rename of the algorithm, in 1995 in his PhD thesis titled “Problem Solving with
    Reinforcement Learning,” Gavin issued Sutton an apology for continuing to use
    the name “Modified Q-Learning” despite Sutton’s preference for “SARSA.” Sutton
    also continued to use SARSA, which is ultimately the name that stuck with the
    algorithm in the RL community. By the way, Gavin’s thesis also introduced the
    SARSA(λ) agent.After obtaining his PhD in 1995, Gavin became a programmer and
    later a lead programmer for the company responsible for the series of the Tomb
    Raider games. Gavin has had a successful career as a game developer.Mahesan, who
    became Gavin’s PhD supervisor after the unexpected death of Gavin’s original supervisor,
    followed a more traditional academic career holding lecturer and professor roles
    ever since his graduation in 1990. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|  | 1994年，Gavin Rummery和Mahesan Niranjan发表了一篇题为“使用连接主义系统进行在线Q学习”的论文，在论文中，他们介绍了一种当时他们称之为“修改后的连接主义Q学习”的算法。1996年，Singh和Sutton将此算法命名为SARSA，因为该算法使用了五个事件：(*S*[*t*],
    *A*[*t*], *R*[*t*+1], *S*[*t*+1], *A*[*t*+1])。人们常常想知道这些名称的由来，正如你很快就会看到的，强化学习研究人员在这些名称上可以非常富有创意。有趣的是，在这项算法的公开和“未经授权”的更名之前，1995年，在题为“使用强化学习进行问题解决”的博士论文中，Gavin向Sutton道歉，因为他继续使用“修改后的Q学习”这一名称，尽管Sutton更喜欢“SARSA”。Sutton也继续使用SARSA，这最终成为了强化学习社区中该算法的名称。顺便说一下，Gavin的论文还介绍了SARSA(λ)智能体。在1995年获得博士学位后，Gavin成为了一名程序员，后来成为负责《古墓丽影》系列游戏的公司的首席程序员。Gavin作为一名游戏开发者取得了成功。Mahesan在Gavin原来的导师意外去世后成为他的博士导师，自1990年毕业以来，他一直从事更传统的学术生涯，担任讲师和教授等职务。
    |'
- en: For adapting the accumulating trace to solving the control problem, the only
    necessary change is that we must now track the visited state-action pairs, instead
    of visited states. Instead of using an eligibility vector for tracking visited
    states, we use an eligibility matrix for tracking visited state-action pairs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将累积跟踪应用于解决控制问题，唯一必要的改变是我们现在必须跟踪访问过的状态-动作对，而不是访问过的状态。我们不再使用资格向量来跟踪访问过的状态，而是使用资格矩阵来跟踪访问过的状态-动作对。
- en: The replace-trace mechanism is also straightforward. It consists of clipping
    eligibility traces to a maximum value of one; that is, instead of accumulating
    eligibility without bound, we allow traces to only grow to one. This strategy
    has the advantage that if your agents get stuck in a loop, the traces still don’t
    grow out of proportion. The bottom line is that traces, in the replace-trace strategy,
    are set to one when a state-action pair is visited, and decay based on the *λ*
    value just like in the accumulate-trace strategy.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 替换跟踪机制也很简单。它包括将资格跟踪剪裁到最大值为一；也就是说，我们不再无限制地累积资格，而是允许跟踪仅增长到一。这种策略的优势在于，如果您的智能体陷入循环，跟踪仍然不会不成比例地增长。总之，在替换跟踪策略中，当访问状态-动作对时，跟踪被设置为
    一，并根据 *λ* 值衰减，就像在累积跟踪策略中一样。
- en: '| 0001 | A Bit Of HistoryIntroduction of the eligibility trace mechanism |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 0001 | 历史简介：资格跟踪机制的介绍 |'
- en: '|  | The general idea of an eligibility trace mechanism is probably due to
    A. Harry Klopf, when, in a 1972 paper titled “Brain Function and Adaptive Systems—A
    Heterostatic Theory,” he described how synapses would become “eligible” for changes
    after reinforcing events. He hypothesized: *“When a neuron fires, all of its excitatory
    and inhibitory synapses that were active during the summation of potentials leading
    to the response are eligible to undergo changes in their transmittances.”*However,
    in the context of RL, Richard Sutton’s PhD thesis (1984) introduced the mechanism
    of eligibility traces. More concretely, he introduced the accumulating trace that
    you’ve learned about in this book, also known as the conventional accumulating
    trace.The replacing trace, on the other hand, was introduced by Satinder Singh
    and Richard Sutton in a 1996 paper titled *“Reinforcement Learning with Replacing
    Eligibility Traces,”* which we discuss in this chapter.They found a few interesting
    facts. First, they found that the replace-trace mechanism results in faster and
    more reliable learning than the accumulate-trace one. They also found that the
    accumulate-trace mechanism is biased, while the replace-trace one is unbiased.
    But more interestingly, they found relationships between *TD*(1), MC, and eligibility
    traces.More concretely, they found that *TD*(1) with replacing traces is related
    to first-visit MC and that *TD*(1) with accumulating traces is related to every-visit
    MC. Moreover, they found that the offline version of the replace-trace *TD*(1)
    is identical to first-visit MC. It’s a small world! |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | 可塑性痕迹机制的一般思想可能归功于A. Harry Klopf，他在1972年一篇题为“大脑功能和自适应系统——一种异质理论”的论文中，描述了在强化事件之后，突触将如何成为“可塑性”的。他假设：“当一个神经元放电时，所有在导致响应的电位总和期间活跃的兴奋性和抑制性突触都有资格在它们的传递性上发生变化。”然而，在强化学习的背景下，Richard
    Sutton的博士论文（1984年）引入了可塑性痕迹机制。更具体地说，他引入了你在本书中学到的累积痕迹，也称为传统累积痕迹。另一方面，替换痕迹是由Satinder
    Singh和Richard Sutton在1996年一篇题为《使用替换可塑性痕迹的强化学习》的论文中引入的，我们将在本章中讨论。他们发现了一些有趣的事实。首先，他们发现替换痕迹机制比累积痕迹机制导致的学习更快、更可靠。他们也发现累积痕迹机制是有偏的，而替换痕迹机制是无偏的。但更有趣的是，他们发现了TD(1)、MC和可塑性痕迹之间的关系。更具体地说，他们发现带有替换痕迹的TD(1)与首次访问MC相关，而带有累积痕迹的TD(1)与每次访问MC相关。此外，他们发现离线版本的替换痕迹TD(1)与首次访问MC相同。这是一个小世界！|'
- en: '![](../Images/07_02.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07_02.png)'
- en: Accumulating traces in the SWS environment
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: SWS环境中的累积痕迹
- en: '| ![](../Images/icons_Boil.png) | Boil It DownFrequency and recency heuristics
    in the accumulating-trace mechanism |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| ![图标](../Images/icons_Boil.png) | 简化频率和近期启发式在累积痕迹机制中的应用'
- en: '|  | The accumulating trace combines a frequency and a recency heuristic. When
    your agent tries a state-action pair, the trace for this pair is incremented by
    one. Now, imagine there’s a loop in the environment, and the agent tries the same
    state-action pair several times. Should we make this state-action pair “more”
    responsible for rewards obtained in the future, or should we make it just responsible?Accumulating
    traces allow trace values higher than one while replacing traces don’t. Traces
    have a way for combining frequency (how often you try a state-action pair) and
    recency (how long ago you tried a state-action pair) heuristics implicitly encoded
    in the trace mechanism. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | 累积痕迹结合了频率和近期启发式。当你的智能体尝试一个状态-动作对时，这个对对的痕迹会增加一。现在，想象一下环境中有一个循环，智能体多次尝试相同的状态-动作对。我们应该使这个状态-动作对“更”多地负责未来获得的奖励，还是让它只负责？累积痕迹允许痕迹值高于一，而替换痕迹则不允许。痕迹有一种方法，在痕迹机制中隐式地编码了频率（你尝试状态-动作对的频率）和近期（你尝试状态-动作对的时间）启发式。|'
- en: '![](../Images/07_03.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07_03.png)'
- en: Replacing traces in the SWS environment
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: SWS环境中的替换痕迹
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe SARSA(λ) agent |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| ![图标](../Images/icons_Python.png) | 我会说PythonSARSA(λ)智能体 |'
- en: '|  |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① The SARSA lambda agent is a mix of the SARSA and the *TD* lambda methods.②
    Here’s the lambda_ hyperparameter (ending in _ because the word lambda is reserved
    in Python).③ The replacing_traces variable sets the algorithm to use replacing
    or accumulating traces.④ We use the usual variables as we have before ...⑤ ...
    including the Q-function and the tracking matrix.⑥ These are the eligibility traces
    that will allow us to keep track of states eligible for updates.⑦ The rest is
    the same as before with the select_action function, and the vectors alphas and
    epsilons.⑧ Every new episode, we set the eligibility of every state to zero.⑨
    We then reset the environment and the done flag as usual.⑩ We select the action
    of the initial state.⑪ We enter the interaction loop.⑫ We send the action to the
    environment and receive the experience tuple.⑬ We select the action to use at
    the next state using the Q-table and the epsilon corresponding to this episode.⑭
    We calculate the *TD* target and the *TD* error as in the original SARSA.⑮ Then,
    we increment the state-action pair trace, and clip it to 1 if it’s a replacing
    trace.⑯ And notice this! We’re applying the *TD* error to all eligible state-action
    pairs at once. Even though we’re using the entire Q-table, E will be mostly 0,
    and greater than zero for eligible pairs.⑰ We decay the eligibilities.⑱ Update
    the variables.⑲ Save Q and pi.⑳ At the end of training we extract V, pi, and return.
    |
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ① SARSA lambda 代理是 SARSA 和 *TD* lambda 方法的混合体。② 这里是 lambda_ 超参数（以 _ 结尾，因为 lambda
    这个词在 Python 中是保留的）。③ replacing_traces 变量设置算法使用替换或积累痕迹。④ 我们使用之前常用的变量...⑤ ...包括
    Q 函数和跟踪矩阵。⑥ 这些是允许我们跟踪可更新状态的资格痕迹。⑦ 其余部分与之前相同，包括 select_action 函数以及 alphas 和 epsilons
    向量。⑧ 每个新剧集开始时，我们将每个状态的可选性设置为零。⑨ 然后我们像往常一样重置环境和 done 标志。⑩ 我们选择初始状态的动作。⑪ 我们进入交互循环。⑫
    我们将动作发送到环境中，并接收经验元组。⑬ 我们使用 Q 表和对应剧集的 epsilon 选择下一个状态的动作。⑭ 我们像原始 SARSA 一样计算 *TD*
    目标和 *TD* 错误。⑮ 然后，我们增加状态-动作对的痕迹，并在是替换痕迹的情况下将其剪辑到 1。⑯ 注意这一点！我们一次将 *TD* 错误应用于所有合格的州-动作对。尽管我们使用整个
    Q 表，但 E 将大部分为 0，对于合格的配对大于零。⑰ 我们衰减可选性。⑱ 更新变量。⑲ 保存 Q 和 pi。⑳ 训练结束时提取 V、pi 并返回。 |
- en: '| ![](../Images/icons_Miguel.png) | Miguel''s AnalogyAccumulating and replacing
    traces, and a gluten- and banana-free diet |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| ![Miguel的类比](../Images/icons_Miguel.png) | 积累和替换痕迹，以及无麸质和无香蕉饮食 |'
- en: '|  | A few months back, my daughter was having trouble sleeping at night. Every
    night, she would wake up multiple times, crying very loudly, but unfortunately,
    not telling us what the problem was.After a few nights, my wife and I decided
    to do something about it and try to “trace” back the issue so that we could more
    effectively “assign credit” to what was causing the sleepless nights.We put on
    our detective hats (if you’re a parent, you know what this is like) and tried
    many things to diagnose the problem. After a week or so, we narrowed the issue
    to foods; we knew the bad nights were happening when she ate certain foods, but
    we couldn’t determine which foods exactly were to blame. I noticed that throughout
    the day, she would eat lots of carbs with gluten, such as cereal, pasta, crackers,
    and bread. And, close to bedtime, she would snack on fruits.An “accumulating trace”
    in my brain pointed to the carbs. “Of course!” I thought, “Gluten is evil; we
    all know that. Plus, she is eating all that gluten throughout the day.” If we
    trace back and accumulate the number of times she ate gluten, gluten was clearly
    eligible, was clearly to blame, so we did remove the gluten.But, to our surprise,
    the issue only subsided, it didn’t entirely disappear as we hoped. After a few
    days, my wife remembered she had trouble eating bananas at night when she was
    a kid. I couldn’t believe it, I mean, bananas are fruits, and fruits are only
    good for you, right? But funny enough, in the end, removing bananas got rid of
    the bad nights. Hard to believe!But, perhaps if I would’ve used a “replacing trace”
    instead of an “accumulating trace,” all of the carbs she ate multiple times throughout
    the day would have received a more conservative amount of blame.Instead, because
    I was using an accumulating trace, it seemed to me that the many times she ate
    gluten were to blame. Period. I couldn’t see clearly that the recency of the bananas
    played a role.The bottom line is that accumulating traces can “exaggerate” when
    confronted with frequency while replacing traces moderate the blame assigned to
    frequent events. This moderation can help the more recent, but rare events surface
    and be taken into account.Don’t make any conclusions, yet. Like everything in
    life, and in RL, it’s vital for you to know the tools and don’t just dismiss things
    at first glance. I’m just showing you the available options, but it’s up to you
    to use the right tools to achieve your goals. |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | 几个月前，我的女儿晚上睡觉有困难。每天晚上，她都会多次醒来，大声哭泣，但不幸的是，她并没有告诉我们问题是什么。经过几晚之后，我和妻子决定采取一些措施，试图“追溯”问题，以便我们能更有效地“归因”导致失眠的原因。我们戴上了侦探帽（如果你是父母，你就知道这是什么感觉）并尝试了许多方法来诊断问题。大约一周后，我们将问题缩小到食物上；我们知道她吃某些食物的晚上会不好，但我们无法确定具体是哪些食物要承担责任。我注意到，在整个白天，她会吃很多含有麸质的碳水化合物，比如谷物、面条、饼干和面包。而且，接近睡觉时间，她会吃一些水果。我大脑中的“累积痕迹”指向了碳水化合物。“当然！”我想，“麸质是邪恶的；我们都知道这一点。而且，她整天都在吃那么多麸质。”如果我们追溯并累积她吃麸质的次数，麸质显然是合格的，显然是罪魁祸首，所以我们确实移除了麸质。但是，出乎我们的意料，问题只是有所缓解，并没有完全消失，正如我们希望的那样。几天后，我的妻子回想起她小时候晚上吃香蕉会有困难。我简直不敢相信，我的意思是，香蕉是水果，水果都是有益的，对吧？但有趣的是，最后，移除香蕉消除了那些糟糕的夜晚。难以置信！但是，也许如果我使用的是“替代痕迹”而不是“累积痕迹”，她一天中多次吃的所有碳水化合物都会得到更保守的责备。相反，因为我使用的是累积痕迹，在我看来，她多次吃麸质是罪魁祸首。就是这样。我无法清楚地看到香蕉的近期性发挥了作用。总的来说，累积痕迹在面对频率时可能会“夸大”，而替代痕迹则会减轻频繁事件所分配的责备。这种减轻可以帮助近期但罕见的事件浮出水面，并得到考虑。不要急于下结论。就像生活中的每一件事，在现实生活中的每一件事一样，了解工具并不仅仅是一眼就否定事物是至关重要的。我只是向你展示了可用的选项，但使用正确的工具来实现你的目标取决于你。
    |'
- en: 'Watkins’s *Q*(*λ*): Decoupling behavior from learning, again'
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Watkins的 *Q*(*λ*)：再次将行为与学习分离
- en: And, of course, there’s an off-policy control version of the *λ* algorithms.
    *Q*(*λ*) is an extension of Q-learning that uses the *λ*-return for policy-evaluation
    requirements of the generalized policy-iteration pattern. Remember, the only change
    we’re doing here is replacing the *TD* target for off-policy control (the one
    that uses the max over the action in the next state) with a *λ*-return for off-policy
    control. There are two different ways to extend Q-learning to eligibility traces,
    but, I’m only introducing the original version, commonly referred to as *Watkins’s
    Q(**λ*).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，还有*λ*算法的离策略控制版本。*Q*(*λ*)是Q学习的扩展，它使用*λ*-回报来满足广义策略迭代模式的策略评估需求。记住，我们在这里做的唯一改变是将离策略控制的*TD*目标（使用下一个状态中动作的最大值）替换为*λ*-回报。将Q学习扩展到资格迹有两种不同的方法，但我只介绍原始版本，通常被称为*Watkins的Q(**λ**)。
- en: '| 0001 | A Bit Of HistoryIntroduction of the Q-learning and *Q*(*λ*) agents
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 0001 | 历史简介：Q学习与*Q*(*λ*)代理介绍 |'
- en: '|  | In 1989, the Q-learning and *Q*(*λ*) methods were introduced by Chris
    Watkins in his PhD thesis titled “Learning from Delayed Rewards,” which was foundational
    to the development of the current theory of reinforcement learning.Q-learning
    is still one of the most popular reinforcement learning algorithms, perhaps because
    it’s simple and it works well. *Q*(*λ*) is now referred to as Watkins’s *Q*(*λ*)
    because there’s a slightly different version of *Q*(*λ*)—due to Jing Peng and
    Ronald Williams—that was worked between 1993 and 1996 (that version is referred
    to as Peng’s *Q*(*λ*).)In 1992, Chris, along with Peter Dayan, published a paper
    titled “Technical Note Q-learning,” in which they proved a convergence theorem
    for Q-learning. They showed that Q-learning converges with probability 1 to the
    optimum action-value function, with the assumption that all state-action pairs
    are repeatedly sampled and represented discretely.Unfortunately, Chris stopped
    doing RL research almost right after that. He went on to work for hedge funds
    in London, then visited research labs, including a group led by Yann LeCun, always
    working AI-related problems, but not so much RL. For the past 22+ years, Chris
    has been a Reader in Artificial Intelligence at the University of London.After
    finishing his 1991 PhD thesis titled “Reinforcing Connectionism: Learning the
    Statistical Way.” (Yeah, connectionism is what they called neural networks back
    then—“deep reinforcement learning” you say? Yep!) Peter went on a couple of postdocs,
    including one with Geoff Hinton at the University of Toronto. Peter was a postdoc
    advisor to Demis Hassabis, cofounder of DeepMind. Peter has held many director
    positions at research labs, and the latest is the Max Planck Institute.Since 2018
    he’s been a Fellow of the Royal Society, one of the highest awards given in the
    UK. |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | 1989年，Chris Watkins在他的博士论文《从延迟奖励中学习》中介绍了Q学习与*Q*(*λ*)方法，这篇论文是当前强化学习理论的基石。Q学习仍然是最受欢迎的强化学习算法之一，可能是因为它简单且效果良好。*Q*(*λ*)现在被称为Watkins的*Q*(*λ*)，因为有一个稍微不同的*Q*(*λ*)版本——由Jing
    Peng和Ronald Williams在1993年至1996年之间工作（这个版本被称为Peng的*Q*(*λ*)。）1992年，Chris与Peter Dayan一起发表了一篇题为“技术笔记Q学习”的论文，其中他们证明了Q学习的收敛定理。他们证明了在所有状态-动作对反复采样并离散表示的假设下，Q学习以概率1收敛到最优动作值函数。不幸的是，Chris在那之后几乎停止了RL研究。他继续在伦敦的避险基金工作，然后访问研究实验室，包括由Yann
    LeCun领导的一个小组，一直从事与AI相关的问题，但不是那么多的RL。在过去的22+年里，Chris一直是伦敦大学的人工智能讲师。在完成他的1991年博士论文《强化连接主义：以统计方式学习》后（是的，那时候他们把神经网络称为连接主义——“深度强化学习”吗？是的！）Peter继续进行了一两个博士后研究，包括在多伦多大学的Geoff
    Hinton那里。Peter是DeepMind的共同创始人Demis Hassabis的博士后导师。Peter在研究实验室担任了许多董事职位，最新的职位是马克斯·普朗克研究所。自2018年以来，他一直是英国皇家学会的院士，这是在英国授予的最高奖项之一。'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe Watkins’s *Q*(*λ*) agent
    1/3 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python：Watkins的*Q*(*λ*)代理 1/3
    |'
- en: '|  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① The Q lambda agent is a mix between the Q-learning and the *TD* lambda methods.②
    Here are the lambda_ and the replacing_traces hyperparameters.③ Useful variables④
    The Q-table⑤ The eligibility traces matrix for all state-action pairs⑥ The usual
    suspects⑦ To be continued ... |
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ① Q lambda代理是Q学习和*TD* lambda方法的混合体。② 这里是lambda_和replacing_traces超参数。③ 有用的变量④
    Q表⑤ 所有状态-动作对的资格迹矩阵⑥ 常见的问题⑦ 待续... |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe Watkins’s *Q*(*λ*) agent
    2/3 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python：Watkins的*Q*(*λ*)代理 2/3
    |'
- en: '|  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ⑧ Continues on the episodes loop⑨ Okay. Because Q lambda is an off-policy method
    we must use E with care. We’re learning about the greedy policy, but following
    an exploratory policy. First we fill E with zeros as before.⑩ Reset the environment
    and done.⑪ But, notice how we are preselecting the action as in SARSA, but we
    didn’t do that in Q-learning. This is because we need to check whether our next
    action is greedy!⑫ Enter the interaction loop.⑬ Step the environment and get the
    experience.⑭ We select the next_action SARSA-style!⑮ And use it to verify that
    the action on the next step will still come from the greedy policy.⑯ On this step,
    we still calculate the *TD* target as in regular Q-learning, using the max.⑰ And
    use the *TD* target to calculate the *TD* error.⑱ We continue from this line on
    the next page. |
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 在场景循环中继续⑨ 好的。因为Q lambda是一种离策略方法，我们必须谨慎地使用E。我们正在学习贪婪策略，但遵循探索性策略。首先，我们像之前一样用零填充E。⑩
    重置环境并完成。⑪ 但是，注意我们是如何像SARSA那样预先选择动作的，但在Q-learning中我们没有这样做。这是因为我们需要检查我们的下一个动作是否是贪婪的！⑫
    进入交互循环。⑬ 步进环境并获得经验。⑭ 我们以SARSA风格选择下一个动作！⑮ 并用它来验证在下一步中采取的动作仍然来自贪婪策略。⑯ 在这一步，我们仍然像常规Q-learning那样计算*TD*目标，使用最大值。⑰
    并使用*TD*目标来计算*TD*误差。⑱ 我们从下一页的这条线继续。 |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe Watkins’s *Q*(*λ*) agent
    3/3 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说PythonWatkins的*Q*(*λ*)智能体 3/3 |'
- en: '|  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ⑲ Again, calculate a *TD* error using the target and the current estimate of
    the state-action pair. Notice, this isn’t next_state, this is state!!!⑳ The other
    approach to replace-trace control methods is to zero out all action values of
    the current state and then increment the current action.㉑ We increment the eligibility
    of the current state-action pair by 1.㉒ And as before, we multiply the entire
    eligibility trace matrix by the error and the learning rate corresponding to episode
    e, then move the entire Q toward that error. By doing so, we’re effectively dropping
    a signal to all visited states to a various degree.㉓ Notice this too. If the action
    we’ll take on the next state (which we already selected) is a greedy action, then
    we decay the eligibility matrix as usual, otherwise, we must reset the eligibility
    matrix to zero because we’ll no longer be learning about the greedy policy.㉔ At
    the end of the step, we update the state and action to be the next state and action.㉕
    We save Q and pi.㉖ And at the end of training we also save V and the final pi.㉗
    Finally, we return all this. |
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ⑲ 再次，使用目标和当前状态-动作对的当前估计来计算*TD*误差。注意，这不是下一个状态，这是状态！！！⑳ 替换迹控制方法的另一种方法是清除当前状态的所有动作值，然后增加当前动作。㉑
    我们将当前状态-动作对的资格增量增加1。㉒ 并且像之前一样，我们将整个资格迹矩阵乘以错误和对应于该段落的错误率，然后将整个Q移动到那个错误。通过这样做，我们实际上是将信号降低到所有访问状态的各种程度。㉓
    注意这一点。如果我们将在下一个状态采取的动作（我们已选择）是贪婪动作，那么我们像往常一样衰减资格矩阵，否则，我们必须将资格矩阵重置为零，因为我们不再学习关于贪婪策略。㉔
    步骤结束时，我们更新状态和动作，使它们成为下一个状态和动作。㉕ 我们保存Q和pi。㉖ 并且在训练结束时，我们也保存V和最终的pi。㉗ 最后，我们返回所有这些。
    |
- en: Agents that interact, learn, and plan
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交互、学习和规划智能体
- en: In chapter 3, we discussed planning algorithms such as value iteration (VI)
    and policy iteration (PI). These are planning algorithms because they require
    a model of the environment, an MDP. Planning methods calculate optimal policies
    offline. On the other hand, in the last chapter I presented model-free reinforcement
    learning methods, perhaps even suggesting that they were an improvement over planning
    methods. But are they?
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我们讨论了如值迭代（VI）和政策迭代（PI）之类的规划算法。这些是规划算法，因为它们需要一个环境模型，一个MDP。规划方法在离线计算最优策略。另一方面，在上一章中，我介绍了无模型的强化学习方法，也许甚至暗示它们是规划方法的改进。但它们是吗？
- en: The advantage of model-free RL over planning methods is that the former doesn’t
    require MDPs. Often MDPs are challenging to obtain in advance; sometimes MDPs
    are even impossible to create. Imagine representing the game of Go with *10*^(170)
    possible states or StarCraft II with *10*^(1685) states. Those are significant
    numbers, and that doesn’t even include the action spaces or transition function,
    imagine! Not requiring an MDP in advance is a practical benefit.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 无模型RL相对于规划方法的优点是，前者不需要MDP。通常，MDP在事先很难获得；有时MDP甚至无法创建。想象一下用*10*^(170)可能的状态来表示围棋或用*10*^(1685)状态来表示星际争霸II。这些是很大的数字，而且这还不包括动作空间或转换函数，想象一下！事先不需要MDP是一个实际的好处。
- en: 'But, let’s think about this for a second: what if we don’t require an MDP in
    advance, but perhaps learn one as we interact with the environment? Think about
    it: as you walk around a new area, you start building a map in your head. You
    walk around for a while, find a coffee shop, get coffee, and you know how to get
    back. The skill of learning maps should be intuitive to you. Can reinforcement
    learning agents do something similar to this?'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，让我们稍微思考一下：如果我们事先不需要MDP，而是在与环境交互的过程中学习一个MDP会怎样呢？想想看：当你在一个新区域散步时，你开始在脑海中构建地图。你走了会儿，找到了一家咖啡馆，喝了咖啡，然后你知道怎么回去。学习地图的技能对你来说应该是直观的。强化学习代理能否做类似的事情？
- en: In this section, we explore agents that interact with the environment, like
    the model-free methods, but they also learn models of the environment from these
    interactions, MDPs. By learning maps, agents often require fewer experience samples
    to learn optimal policies. These methods are called *model-based reinforcement
    learning*. Note that in the literature, you often see VI and PI referred to as
    planning methods, but you may also see them referred to as model-based methods.
    I prefer to draw the line and call them planning methods because they require
    an MDP to do anything useful at all. SARSA and Q-learning algorithms are model-free
    because they do not require and do not learn an MDP. The methods that you learn
    about in this section are model-based because they do not require, but do learn
    and use an MDP (or at least an approximation of an MDP).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探索与环境交互的代理，如无模型方法，但它们也通过这些交互学习环境的模型，MDP。通过学习地图，代理通常需要更少的经验样本来学习最优策略。这些方法被称为*基于模型的强化学习*。请注意，在文献中，你经常看到VI和PI被引用为规划方法，但你也可能看到它们被引用为基于模型的方法。我更喜欢划清界限，将它们称为规划方法，因为它们需要MDP才能做任何有用的事情。SARSA和Q-learning算法是无模型的，因为它们不需要也不学习MDP。在本节中你将学习的方法是基于模型的，因为它们不需要，但学习并使用MDP（或者至少是MDP的近似）。|
- en: '| ŘŁ | With An RL AccentSampling models vs. distributional models |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| ŘŁ | 带有RL口音的采样模型与分布模型 |'
- en: '|  | **Sampling models**: Refers to models of the environment that produce
    a single sample of how the environment will transition given some probabilities;
    you sample a transition from the model.**Distributional models**: Refers to models
    of the environment that produce the probability distribution of the transition
    and reward functions. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | **采样模型**：指的是环境模型，它根据某些概率产生环境如何转移的单个样本；你从模型中采样一个转移。**分布模型**：指的是环境模型，它产生转移和奖励函数的概率分布。|'
- en: 'Dyna-Q: Learning sample models'
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dyna-Q：学习样本模型
- en: One of the most well-known architectures for unifying planning and model-free
    methods is called *dyna-Q*. Dyna-Q consists of interleaving a model-free RL method,
    such as Q-learning, and a planning method, similar to value iteration, using both
    experiences sampled from the environment and experiences sampled from the learned
    model to improve the action-value function.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 将规划和无模型方法统一的最著名的架构之一被称为*dyna-Q*。Dyna-Q由交替使用无模型RL方法，如Q-learning，和类似于价值迭代的规划方法组成，使用从环境中采样的经验和从学习模型中采样的经验来改进动作值函数。
- en: In Dyna-Q, we keep track of both the transition and reward function as three-dimensional
    tensors indexed by the state, the action, and the next state. The transition tensor
    keeps count of the number of times we’ve seen the three-tuple *(s, a, s')* indicating
    how many times we arrived at state *s'* from state *s* when selecting action *a*.
    The reward tensor holds the average reward we received on the three-tuple *(s,
    a, s')* indicating the expected reward when we select action *a* on state *s*
    and transition to state *s'*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在Dyna-Q中，我们跟踪状态、动作和下一个状态的三维张量，作为转移和奖励函数。转移张量记录了看到三元组*(s, a, s')*的次数，表示在执行动作*a*时从状态*s*到达状态*s'*的次数。奖励张量持有我们在三元组*(s,
    a, s')*上获得的平均奖励，表示当我们选择动作*a*在状态*s*并转移到状态*s'*时的预期奖励。
- en: '![](../Images/07_04.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07_04.png)'
- en: A model-based reinforcement learning architecture
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的强化学习架构
- en: '| 0001 | A Bit Of HistoryIntroduction of the Dyna-Q agent |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 0001 | 一点历史介绍Dyna-Q代理 |'
- en: '|  | Ideas related to model-based RL methods can be traced back many years,
    and are credited to several researchers, but there are three main papers that
    set the foundation for the Dyna architecture.The first is a 1981 paper by Richard
    Sutton and Andrew Barto titled “An Adaptive Network that Constructs and Uses an
    Internal Model of Its World,” then a 1990 paper by Richard Sutton titled “Integrated
    Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic
    Programming,” and, finally, a 1991 paper by Richard Sutton titled “Dyna, an Integrated
    Architecture for Learning, Planning, and Reacting,” in which the general architecture
    leading to the specific Dyna-Q agent was introduced. |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | 与基于模型的强化学习方法相关的想法可以追溯到很多年，归功于几位研究人员，但有三篇主要论文为Dyna架构奠定了基础。第一篇是1981年由理查德·萨顿和安德鲁·巴特罗撰写的论文“An
    Adaptive Network that Constructs and Uses an Internal Model of Its World”，然后是1990年由理查德·萨顿撰写的论文“Integrated
    Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic
    Programming”，最后是1991年由理查德·萨顿撰写的论文“Dyna, an Integrated Architecture for Learning,
    Planning, and Reacting”，其中介绍了导致特定Dyna-Q代理的通用架构。 |'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe Dyna-Q agent 1/3 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说PythonDyna-Q代理 1/3 |'
- en: '|  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① Dyna-Q is similar to the Q-learning agent, but it also learns a model of the
    environment and it uses that model to improve the estimates.② This n_planning
    hyperparameter is the number of updates to the estimates that will run from the
    learned model.③ Most of the first part of the algorithm is the same.④ We initialize
    the Q-function to zero, and so on.⑤ But then, we create a function to keep track
    of the transition function.⑥ And another one to keep track of the reward signal.⑦
    Then initialize the exploration strategy select_action, and the alphas and epsilons
    vectors, as usual.⑧ To be continued ... |
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ① Dyna-Q与Q-learning代理类似，但它还学习环境模型，并使用该模型来改进估计。② 这个n_planning超参数是运行从学习模型中得到的估计更新的次数。③
    算法的前大部分是相同的。④ 我们将Q函数初始化为零，等等。⑤ 但然后，我们创建一个函数来跟踪转移函数。⑥ 另一个来跟踪奖励信号。⑦ 然后初始化探索策略select_action，以及alpha和epsilon向量，就像往常一样。⑧
    待续 ... |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe Dyna-Q agent 2/3 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说PythonDyna-Q代理 2/3 |'
- en: '|  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ⑨ Continues on the episode loop⑩ For each new episode, we start by resetting
    the environment and obtaining the initial state. We also set the ‘done’ flag to
    False and enter the step-interaction loop.⑪ We select the action, as in original
    Q-learning (inside the loop only).⑫ We step the environment and get the experience
    tuple.⑬ Then, start learning the model! We increment the transition count for
    the state-action-next_state triplet indicating that full transition happened once
    more.⑭ We also attempt to calculate an incremental mean of the reward signal.
    Get the difference.⑮ Then use that difference and the transition count to learn
    the reward signal.⑯ We calculate the *TD* target as usual, Q-learning style (off-policy,
    using the max) ...⑰ ... and the *TD* error, too, using the *TD* target and the
    current estimate.⑱ Finally, update the Q-function.⑲ And right before we get into
    the planning steps, we back up the next state variable.⑳ To be continued ... |
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 在情节循环中继续⑩ 对于每个新的情节，我们首先重置环境并获得初始状态。我们还将“完成”标志设置为False并进入步骤交互循环。⑪ 我们选择动作，就像原始Q-learning（只在循环内）一样。⑫
    我们对环境进行步骤操作并获得经验元组。⑬ 然后，开始学习模型！我们增加状态-动作-下一个状态三元组的转移计数，表示完整的转移再次发生。⑭ 我们还尝试计算奖励信号的增量平均值。获取差异。⑮
    然后使用该差异和转移计数来学习奖励信号。⑯ 我们像通常的Q-learning风格一样计算*TD*目标（离线策略，使用最大值）...⑰ ... 以及*TD*误差，也使用*TD*目标和当前估计。⑱
    最后，更新Q函数。⑲ 在我们进入规划步骤之前，我们备份下一个状态变量。⑳ 待续 ... |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe Dyna-Q agent 3/3 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说PythonDyna-Q代理 3/3 |'
- en: '|  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ㉑ We continue from the planning loop.㉒ First, we want to make sure there have
    been updates to the Q-function before, otherwise, there’s not much to plan.㉓ Then
    we select a state from a list of states already visited by the agent in experience.㉔
    We then select an action that has been taken on that state.㉕ We use the count
    matrix to calculate probabilities of a next state and then a next state.㉖ Use
    the reward model as the reward.㉗ And update the Q-function using that simulated
    experience!㉘ At the end of the planning steps, we set the state as the next state.㉙
    The rest is the same. |
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ㉑ 我们从规划循环继续。㉒ 首先，我们想要确保 Q 函数之前已经进行了更新，否则，就没有太多可以规划的。㉓ 然后我们从智能体在经验中已经访问过的状态列表中选择一个状态。㉔
    我们然后选择在该状态下已经采取的动作。㉕ 我们使用计数矩阵来计算下一个状态的概率，然后是下一个状态。㉖ 使用奖励模型作为奖励。㉗ 然后使用那个模拟经验来更新
    Q 函数！㉘ 在规划步骤结束时，我们将状态设置为下一个状态。㉙ 其余的都是一样的。 |
- en: '| ![](../Images/icons_Tally.png) | Tally it UpModel-based methods learn the
    transition and reward function (transition below) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Tally.png) | 总结结果模型方法学习转移和奖励函数（转移见下文） |'
- en: '|  | ![](../Images/07_04_Sidebar17.png) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/07_04_Sidebar17.png) |'
- en: 'Trajectory sampling: Making plans for the immediate future'
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 轨迹采样：为立即未来制定计划
- en: In Dyna-Q, we learn the model as previously described, adjust action-value functions
    as we do in vanilla Q-learning, and then run a few planning iterations at the
    end of the algorithm. Notice that if we removed the model-learning and planning
    lines from the code, we’d be left with the same Q-learning algorithm that we had
    in the previous chapter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Dyna-Q 中，我们学习模型的方式如前所述，调整动作值函数的方式与 vanilla Q-learning 相同，然后在算法的末尾运行几个规划迭代。请注意，如果我们从代码中移除模型学习和规划行，我们将剩下与上一章相同的
    Q-learning 算法。
- en: At the planning phase, we only sample from the state-action pairs that have
    been visited, so that the agent doesn’t waste resources with state-action pairs
    about which the model has no information. From those visited state-action pairs,
    we sample a state uniformly at random and then sample action from previously selected
    actions, also uniformly at random. Finally, we obtain the next state and reward
    sampling from the probabilities of transition given that state-action pair. But
    doesn’t this seem intuitively incorrect? We’re planning by using a state selected
    uniformly at random!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在规划阶段，我们只从已经访问过的状态-动作对中进行采样，这样智能体就不会在模型没有信息的状态-动作对上浪费资源。从那些访问过的状态-动作对中，我们随机均匀地采样一个状态，然后从之前选定的动作中随机采样动作。最后，我们从给定状态-动作对的转移概率中采样下一个状态和奖励。但这看起来似乎直观上是不正确的？我们是通过使用随机选择的状态来规划！
- en: 'Couldn’t this technique be more effective if we used a state that we expect
    to encounter during the current episode? Think about it for a second. Would you
    prefer prioritizing planning your day, week, month, and year, or would you instead
    plan a random event that “could” happen in your life? Say that you’re a software
    engineer: would you prefer planning reading a programming book, and working on
    that side project, or a future possible career change to medicine? Planning for
    the immediate future is the smarter approach. *trajectory sampling* is a model-based
    RL method that does just that.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用在当前场景中预期会遇到的状态，这种技术是否会更有效？思考一下。你更愿意优先规划你的日子、周、月和年，还是更愿意规划一个“可能”发生在你生活中的随机事件？比如说你是一名软件工程师：你更愿意规划阅读编程书籍，并着手那个副项目，还是规划一个可能的未来职业转变到医学？为立即未来制定计划是一种更明智的方法。*轨迹采样*是一种基于模型的强化学习方法，正是如此。
- en: '| ![](../Images/icons_Boil.png) | Boil It DownTrajectory sampling |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Boil.png) | 简化问题轨迹采样 |'
- en: '|  | While Dyna-Q samples the learned MDP uniformly at random, trajectory sampling
    gathers trajectories, that is, transitions and rewards that can be encountered
    in the immediate future. You’re planning your week, not a random time in your
    life. It makes more sense to do it this way.The traditional trajectory-sampling
    approach is to sample from an initial state until reaching a terminal state using
    the on-policy trajectory, in other words, sampling actions from the same behavioral
    policy at the given time step.However, you shouldn’t limit yourself to this approach;
    you should experiment. For instance, my implementation samples starting from the
    current state, instead of an initial state, to a terminal state within a preset
    number of steps, sampling a policy greedy with respect to the current estimates.But
    you can try something else. As long as you’re sampling a trajectory, you can call
    that trajectory sampling. |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | 当Dyna-Q随机均匀地采样学习到的MDP时，轨迹采样收集轨迹，即未来可能遇到的转换和奖励。你正在规划你的周计划，而不是你生活中的某个随机时间。这样做更有意义。传统的轨迹采样方法是从初始状态开始，使用在线策略轨迹采样直到达到终端状态，换句话说，在给定的时间步中从相同的行为策略中采样动作。然而，你不应该局限于这种方法；你应该尝试。例如，我的实现是从当前状态开始采样，而不是从初始状态开始，到预设步数内的终端状态，采样一个相对于当前估计的贪婪策略。但你可以尝试其他方法。只要你在采样轨迹，你就可以称之为轨迹采样。
    |'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe trajectory-sampling agent
    1/3 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python轨迹采样代理 1/3 |'
- en: '|  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① Trajectory sampling is, for the most part, the same as Dyna-Q, with a few
    exceptions.② Instead of n_planning we use a max_trajectory_depth to restrict the
    trajectory length.③ Most of the algorithm is the same as Dyna-Q.④ The Q-function,
    and so on⑤ We create the same variables to model the transition function ...⑥
    ... and another one for the reward signal.⑦ The select_action function, the alphas
    vector, and epsilons vector are all the same.⑧ To be continued ... |
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ① 轨迹采样在大多数情况下与Dyna-Q相同，但有几点例外。② 我们使用max_trajectory_depth来限制轨迹长度，而不是n_planning。③
    算法的大部分与Dyna-Q相同。④ Q函数，等等⑤ 我们创建相同的变量来模拟转换函数...⑥ ...以及另一个用于奖励信号。⑦ select_action函数、alphas向量和epsilons向量都是相同的。⑧
    待续... |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe trajectory-sampling agent
    2/3 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python轨迹采样代理 2/3 |'
- en: '|  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '⑨ Continues on the episode loop⑩ Again, each new episode, we start by resetting
    the environment and obtaining the initial state. We also set the done flag to
    False and enter the step interaction loop.⑪ We select the action.⑫ We step the
    environment and get the experience tuple.⑬ We learn the model just like in Dyna-Q:
    increment the transition count for the state-action-next_state triplet indicating
    that full transition occurred.⑭ Then, again, calculate an incremental mean of
    the reward signal; first, get the difference.⑮ Then, use that difference and the
    transition count to learn the reward signal.⑯ We calculate the *TD* target as
    usual.⑰ The *TD* error using the *TD* target and the current estimate⑱ Then, update
    the Q-function.⑲ And right before we get into the planning steps, we back up the
    next state variable.⑳ To be continued ... |'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 继续在剧集循环中⑩ 再次，每个新的剧集，我们首先重置环境并获得初始状态。我们还将done标志设置为False并进入步骤交互循环。⑪ 我们选择动作。⑫
    我们对环境进行步骤操作并获得经验元组。⑬ 我们像在Dyna-Q中一样学习模型：增加状态-动作-下一个状态三元组的转换计数，表示完整转换发生。⑭ 然后，再次计算奖励信号的增量平均值；首先，获取差异。⑮
    然后，使用该差异和转换计数来学习奖励信号。⑯ 我们像往常一样计算*TD*目标。⑰ 使用*TD*目标和当前估计的*TD*误差⑱ 然后，更新Q函数。⑲ 在我们进入规划步骤之前，我们备份下一个状态变量。⑳
    待续... |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe trajectory-sampling agent
    3/3 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python轨迹采样代理 3/3 |'
- en: '|  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ㉑ Notice we are now using a max_trajectory_depth variable, but are still planning.㉒
    We still check for the Q-function to have any difference, so it’s worth our compute.㉓
    Select the action either on-policy or off-policy (using the greedy policy).㉔ If
    we haven’t experienced the transition, planning would be a mess, so break out.㉕
    Otherwise, we get the probabilities of next_state and sample the model accordingly.㉖
    Then, get the reward as prescribed by the reward-signal model.㉗ And continue updating
    the Q-function as if with real experience.㉘ Notice here we update the state variable
    right before we loop and continue the on-policy planning steps.㉙ Outside the planning
    loop, we restore the state, and continue real interaction steps.㉚ Everything else
    as usual |
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ㉑ 注意我们现在正在使用max_trajectory_depth变量，但我们仍在规划。㉒ 我们仍然检查Q函数是否有任何差异，所以它值得我们的计算。㉓ 选择动作要么在策略内，要么在策略外（使用贪婪策略）。㉔
    如果我们没有经历过转换，规划将会一团糟，所以退出。㉕ 否则，我们得到next_state的概率并相应地采样模型。㉖ 然后，根据奖励信号模型获得奖励。㉗ 并且继续像真实经验一样更新Q函数。㉘
    注意在这里我们在循环之前更新状态变量，并继续策略内规划步骤。㉙ 在规划循环之外，我们恢复状态，并继续真实的交互步骤。㉚ 其他一切照旧 |
- en: '| ![](../Images/icons_Tally.png) | Tally it UpDyna-Q and trajectory sampling
    sample the learned model differently |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](../Images/icons_Tally.png) | 计数器增加Dyna-Q和轨迹采样以不同的方式采样学习模型 |'
- en: '|  | ![](../Images/07_04_Sidebar22.png) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/07_04_Sidebar22.png) |'
- en: '| ![](../Images/icons_Concrete.png) | A Concrete ExampleThe frozen lake environment
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](../Images/icons_Concrete.png) | 具体示例冻结湖环境 |'
- en: '|  | In chapter 2, we developed the MDP for an environment called frozen lake
    (FL). As you remember, FL is a simple grid-world (GW) environment. It has discrete
    state and action spaces, with 16 states and four actions.The goal of the agent
    is to go from a start location to a goal location while avoiding falling into
    holes. In this particular instantiation of the frozen lake environment, the goal
    is to go from state 0 to state 15\. The challenge is that the surface of the lake
    is frozen, and therefore slippery, very slippery.![](../Images/07_05_Sidebar23.png)The
    frozen lake environmentThe FL environment is a 4 × 4 grid with 16 cells, states
    0–15, top-left to bottom-right. State 0 is the only state in the initial state
    distribution, meaning that on every new episode, the agent shows up in that START
    state. States 5, 7, 11, 12, and 15 are terminal states: once the agent lands on
    any of those states, the episode terminates. States 5, 7, 11, and 12 are holes,
    and state 15 is the “GOAL.” What makes “holes” and “GOAL” be any different is
    the reward function. All transitions landing on the GOAL state, state 15, provide
    a +1 reward, while every other transition in the entire grid world provides a
    0 reward, no reward. The agent will naturally try to get to that +1 transition,
    and that involves avoiding the holes. The challenge of the environment is that
    actions have stochastic effects, so the agent moves only a third of the time as
    intended. The other two-thirds is split evenly in orthogonal directions. If the
    agent tries to move out of the grid world, it will bounce back to the cell from
    which it tried to move. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | 在第二章中，我们为称为冻结湖（FL）的环境开发了MDP。正如你所记得的，FL是一个简单的网格世界（GW）环境。它具有离散的状态和动作空间，有16个状态和四个动作。代理的目标是从起始位置移动到目标位置，同时避免掉入坑中。在这个特定的冻结湖环境实例中，目标是从状态0移动到状态15。挑战在于湖面被冰封，因此非常滑。![图片](../Images/07_05_Sidebar23.png)冻结湖环境FL环境是一个4×4的网格，有16个单元格，状态0-15，从左上角到底部右侧。状态0是初始状态分布中的唯一状态，这意味着在每次新的一集中，代理都会出现在那个起始状态。状态5、7、11、12和15是终端状态：一旦代理落在这些状态中的任何一个，集会就结束了。状态5、7、11和12是坑，状态15是“目标”。使“坑”和“目标”不同的因素是奖励函数。所有落在目标状态，即状态15的转换都提供+1的奖励，而整个网格世界中其他所有的转换都提供0奖励，没有奖励。代理自然会尝试到达那个+1的转换，这涉及到避开坑。环境的挑战在于动作具有随机效果，因此代理只有三分之一的动作是按照预期移动的。其他三分之二均匀地分布在正交方向上。如果代理试图离开网格世界，它将弹回到它试图移动的单元格。
    |'
- en: '| ![](../Images/icons_Details.png) | It''s In The DetailsHyperparameter values
    for the frozen lake environment |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](../Images/icons_Details.png) | 细节之处Hyperparameter values for the frozen
    lake environment |'
- en: '|  | The frozen lake (FL) environment is a more challenging environment than,
    for instance, the slippery walk seven (SWS) environment. Therefore, one of the
    most important changes we need to make is to increase the number of episodes the
    agent interacts with the environment.While in the SWS environment, we allow the
    agent to interact for only 3,000 episodes; in the FL environment, we let the agent
    gather experience for 10,000 episodes. This simple change also automatically adjusts
    the decay schedule for both alpha and epsilon.Changing the value of the n_episodes
    parameter from 3,000 to 10,000 automatically changes the amount of exploration
    and learning of the agent. Alpha now decays from an initial value of 0.5 to a
    minimum value of 0.01 after 50% of the total episodes, which is 5,000 episodes,
    and epsilon decays from an initial value of 1.0 to a minimum value of 0.1 after
    90% of the total episodes, which is 9,000 episodes.![](../Images/07_05_Sidebar24_alpha-epsilon-fl.png)Finally,
    it’s important to mention that I’m using a gamma of 0.99, and that the frozen
    lake environment, when used with OpenAI Gym, is automatically wrapped with a time
    limit Gym Wrapper. This “time wrapper” instance makes sure the agent terminates
    an episode with no more than 100 steps. Technically speaking, these two decisions
    (gamma and the time wrapper) change the optimal policy and value function the
    agent learns, and should not be taken lightly. I recommend playing with the FL
    environment in chapter 7’s Notebook and changing gamma to different values (1,
    0.5, 0) and also removing the time wrapper by getting the environment instance
    attribute “unwrapped,” for instance, “env = env.unwrapped.” Try to understand
    how these two things affect the policies and value functions found. |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | 冰湖（FL）环境比例如滑行七（SWS）环境更具挑战性。因此，我们需要做出的最重要的改变之一是增加智能体与环境交互的回合数。在SWS环境中，我们允许智能体仅交互3,000回合；在FL环境中，我们让智能体积累10,000回合的经验。这个简单的改变也自动调整了alpha和epsilon的衰减计划。将n_episodes参数的值从3,000改为10,000会自动改变智能体的探索和学习量。Alpha现在在总回合数的50%后（即5,000回合）从初始值0.5衰减到最小值0.01，而epsilon在总回合数的90%后（即9,000回合）从初始值1.0衰减到最小值0.1。![alpha-epsilon-fl](../Images/07_05_Sidebar24_alpha-epsilon-fl.png)最后，重要的是要提到，我使用的是gamma值为0.99，并且当使用OpenAI
    Gym时，冰湖环境会自动用时间限制Gym Wrapper包装。这个“时间包装器”实例确保智能体在不超过100步的情况下结束回合。从技术上来说，这两个决定（gamma和时间包装器）改变了智能体学习的最优策略和值函数，不应轻视。我建议在第七章的笔记本中尝试FL环境，并将gamma值改为不同的值（1，0.5，0），并且通过获取环境实例属性“unwrapped”，例如，“env
    = env.unwrapped”，来移除时间包装器。尝试理解这两者如何影响找到的策略和值函数。 |'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpModel-based RL methods get estimates
    closer to actual in fewer episodes |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| ![图标_总计](../Images/icons_Tally.png) | 总结一下：基于模型的RL方法在更少的回合中获得更接近实际的估计 |'
- en: '|  | ![](../Images/07_05_Sidebar25.png) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | ![示例图片](../Images/07_05_Sidebar25.png) |'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpBoth traces and model-based methods
    are efficient at processing experiences |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| ![图标_总计](../Images/icons_Tally.png) | 总结一下：轨迹和基于模型的方法都有效地处理经验 |'
- en: '|  | ![](../Images/07_05_Sidebar26.png) |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | ![示例图片](../Images/07_05_Sidebar26.png) |'
- en: '| ![](../Images/icons_Concrete.png) | A Concrete ExampleThe frozen lake 8 x
    8 environment |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| ![图标_混凝土](../Images/icons_Concrete.png) | 一个具体的例子：8 x 8 的冰湖环境 |'
- en: '|  | How about we step it up and try these algorithms in a challenging environment?This
    one is called frozen lake 8 × 8 (FL8×8) and as you might expect, this is an 8-by-8
    grid world with properties similar to the FL. The initial state is state 0, the
    state on the top-left corner; the terminal, and GOAL state is state 63, the state
    on the bottom-right corner. The stochasticity of action effects is the same: the
    agent moves to the intended cell with a mere 33.33% chance, and the rest is split
    evenly in orthogonal directions.![](../Images/07_06_Sidebar27.png)The main difference
    in this environment, as you can see, is that there are many more holes, and obviously
    they’re in different locations. States 19, 29, 35, 41, 42, 46, 49, 52, 54, and
    59 are holes; that’s a total of 10 holes!Similar to the original FL environment,
    in FL8×8, the right policy allows the agent to reach the terminal state 100% of
    the episodes. However, in the OpenAI Gym implementation, agents that learn optimal
    policies do not find these particular policies because of gamma and the time wrapper
    we discussed. Think about it for a second: given the stochasticity of these environments,
    a safe policy could terminate in zero rewards for the episode due to the time
    wrapper. Also, given a gamma value less than one, the more steps the agent takes,
    the lower the reward will impact the return. For these reasons, safe policies
    aren’t necessarily optimal policies; therefore, the agent doesn’t learn them.
    Remember that the goal isn’t simply to find a policy that reaches the goal 100%
    of the times, but to find a policy that reaches the goal within 100 steps in FL
    and 200 steps in FL8×8\. Agents may need to take risks to accomplish this goal.
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们要不要提高难度，尝试在这些具有挑战性的环境中应用这些算法呢？这个环境被称为8×8冰湖（FL8×8）。正如你所预期的那样，这是一个8×8的网格世界，其属性与FL相似。初始状态是状态0，位于左上角；终止和目标状态是状态63，位于右下角。动作效果的不确定性是相同的：代理以仅仅33.33%的概率移动到目标单元格，其余的以均匀的方式分布在正交方向上。![图片](../Images/07_06_Sidebar27.png)正如你所见，这个环境的主要区别在于有很多更多的洞，而且显然它们位于不同的位置。状态19、29、35、41、42、46、49、52、54和59是洞；总共有10个洞！与原始FL环境相似，在FL8×8中，正确的策略允许代理在100%的回合中达到终止状态。然而，在OpenAI
    Gym的实现中，学习到最优策略的代理并没有找到这些特定的策略，这是由于我们讨论过的gamma和时间包装器。想想看：考虑到这些环境的不确定性，一个安全的策略可能会因为时间包装器而在这个回合中获得零奖励。此外，给定一个小于一的gamma值，代理采取的步骤越多，奖励对回报的影响就越低。因此，安全的策略不一定是最优策略；因此，代理没有学习到它们。记住，目标不仅仅是找到一个100%达到目标的策略，而是在FL中在100步内，在FL8×8中在200步内达到目标。代理可能需要冒险才能实现这个目标。
    |'
- en: '| ![](../Images/icons_Details.png) | It''s In The DetailsHyperparameter values
    for the frozen lake 8 × 8 environment |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Details.png) | 它在于细节冰湖8×8环境的超参数值 |'
- en: '|  | The frozen lake 8 × 8 (FL8×8) environment is the most challenging discrete
    state- and action-space environment that we discuss in this book. This environment
    is challenging for a number of reasons: first, 64 states is the largest number
    of states we’ve worked with, but more importantly having a single non-zero reward
    makes this environment particularly challenging.What that really means is agents
    will only know they’ve done it right once they hit the terminal state for the
    first time. Remember, this is randomly! After they find the non-zero reward transition,
    agents such as SARSA and Q-learning (not the lambda versions, but the vanilla
    ones) will only update the value of the state from which the agent transitioned
    to the GOAL state. That’s a one-step back from the reward. Then, for that value
    function to be propagated back one more step, guess what, the agent needs to randomly
    hit that second-to-final state. But, that’s for the non-lambda versions. With
    SARSA(λ) and Q(λ), the propagation of values depends on the value of lambda. For
    all the experiments in this chapter, I use a lambda of 0.5, which more or less
    tells the agent to propagate the values half the trajectory (also depending on
    the type of traces being used, but as a ballpark).Surprisingly enough, the only
    change we make to these agents is the number of episodes we let them interact
    with the environments. While in the SWS environment we allow the agent to interact
    for only 3,000 episodes, and in the FL environment we let the agent gather experience
    for 10,000 episodes; in FL8×8 we let these agents gather 30,000 episodes. This
    means that alpha now decays from an initial value of 0.5 to a minimum value of
    0.01 after 50% of the total episodes, which is now 15,000 episodes, and epsilon
    decays from an initial value of 1.0 to a minimum value of 0.1 after 90% of the
    total episodes, which is now 27,000 episodes.![](../Images/07_06_Sidebar28_alpha-epsilon-fl-8x8.png)
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | 冰冻湖8×8（FL8×8）环境是我们在这本书中讨论的最具挑战性的离散状态空间和动作空间环境。这个环境之所以具有挑战性，有以下几个原因：首先，64个状态是我们处理过的状态数量最多的情况，但更重要的是，只有一个非零奖励使得这个环境特别具有挑战性。这实际上意味着智能体只有在第一次达到终端状态时才会知道他们已经做对了。记住，这是随机的！在他们找到非零奖励的转换之后，像SARSA和Q-learning（不是lambda版本，而是原始版本）这样的智能体只会更新智能体转换到目标状态的那些状态的价值。这比奖励退回了一步。然后，为了使那个价值函数再退回一步，猜猜看，智能体需要随机地击中那个倒数第二个状态。但是，这是对于非lambda版本的情况。对于SARSA(λ)和Q(λ)，价值的传播取决于lambda的值。在本章的所有实验中，我使用了一个lambda值为0.5，这大致告诉智能体传播价值占轨迹的一半（也取决于所使用的痕迹类型，但作为一个大致估计）。令人惊讶的是，我们对这些智能体所做的唯一改变是允许它们与环境交互的次数。在SWS环境中，我们只允许智能体进行3,000次交互，在FL环境中，我们让智能体积累10,000次经验；在FL8×8中，我们让这些智能体积累30,000次经验。这意味着alpha现在从初始值0.5衰减到最小值0.01，这是总交互次数的50%，现在是15,000次，而epsilon在总交互次数的90%后衰减到最小值0.1，现在是27,000次。![alpha-epsilon-fl-8x8](../Images/07_06_Sidebar28_alpha-epsilon-fl-8x8.png)
    |'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpOn-policy methods no longer keep
    up, off-policy with traces and model-based do |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| ![计数图标](../Images/icons_Tally.png) | 总结一下，基于策略的方法不再跟上，基于痕迹的离策略和基于模型的方法可以
    |'
- en: '|  | ![](../Images/07_06_Sidebar29.png) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | ![侧边栏29](../Images/07_06_Sidebar29.png) |'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpSome model-based methods show
    large error spikes to be aware of |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| ![计数图标](../Images/icons_Tally.png) | 总结一下，一些基于模型的方法会显示出大的误差峰值以引起注意 |'
- en: '|  | ![](../Images/07_06_Sidebar30.png) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | ![侧边栏30](../Images/07_06_Sidebar30.png) |'
- en: Summary
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned about making RL more effective and efficient. By
    effective, I mean that agents presented in this chapter are capable of solving
    the environment in the limited number of episodes allowed for interaction. Other
    agents, such as vanilla SARSA, or Q-learning, or even Monte Carlo control, would
    have trouble solving these challenges in the limited number of steps; at least,
    for sure, they’d have trouble solving the FL8x8 environment in only 30,000 episodes.
    That’s what effectiveness means to me in this chapter; agents are successful in
    producing the desired results.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何使强化学习（RL）更加有效和高效。这里的“有效”指的是本章中提到的智能体能够在有限的交互次数内解决环境问题。其他智能体，如原始的SARSA、Q-learning，甚至是蒙特卡洛控制，在有限的步骤内解决这些挑战会有困难；至少，对于FL8x8环境来说，它们在只有30,000次交互的情况下会遇到困难。这就是本章中“有效”的含义；智能体能够成功产生预期的结果。
- en: We also explored more efficient algorithms. And by efficient here, I mean data-efficient;
    I mean that the agents we introduced in this chapter can do more with the same
    data than other agents. SARSA(*λ*) and Q(*λ*), for instance, can propagate rewards
    to value-function estimates much quicker than their vanilla counterparts, SARSA
    and Q-learning. By adjusting the *λ* hyperparameter, you can even assign credit
    to all states visited in an episode. A value of 1 for *λ* is not always the best,
    but at least you have the option when using SARSA(*λ*) and Q(*λ*).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探讨了更高效的算法。在这里，我指的是数据效率；我的意思是，我们在这章中引入的代理可以用相同的数据做更多的事情，比其他代理。例如，SARSA(λ)和Q(λ)可以比它们的原始版本SARSA和Q-learning更快地将奖励传播到价值函数估计中。通过调整λ超参数，你甚至可以将信用分配给一个场景中访问过的所有状态。λ的值为1并不总是最好的，但至少在使用SARSA(λ)和Q(λ)时你有这个选项。
- en: 'You also learned about model-based RL methods, such as Dyna-Q and trajectory
    sampling. These methods are sample efficient in a different way. They use samples
    to learn a model of the environment; if your agent lands 100% of 1M samples on
    state *s''* when taking action *a*, in state *s*, why not use that information
    to improve value functions and policies? Advanced model-based deep reinforcement
    learning methods are often used in environments in which gathering experience
    samples is costly: domains such as robotic, or problems in which you don’t have
    a high-speed simulation, or where hardware requires large financial resources.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你还学习了基于模型的方法，例如Dyna-Q和轨迹采样。这些方法以不同的方式提高样本效率。它们使用样本来学习环境模型；如果你的代理在状态`s`中采取行动`a`时，将100%的100万个样本落在状态`s'`上，为什么不利用这些信息来改进价值函数和政策呢？高级基于模型的深度强化学习方法通常用于收集经验样本成本高昂的环境：例如机器人领域，或者没有高速模拟的问题，或者硬件需要大量资金资源。
- en: For the rest of the book, we’re moving on to discuss the subtleties that arise
    when using non-linear function approximation with reinforcement learning. Everything
    that you’ve learned so far still applies. The only difference is that instead
    of using vectors and matrices for holding value functions and policies, now we
    move into the world of supervised learning and function approximation. Remember,
    in DRL, agents learn from feedback that’s simultaneously sequential (as opposed
    to one-shot), evaluative (as opposed to supervised), and sampled (as opposed to
    exhaustive). We haven’t touched the “sampled” part yet; agents have always been
    able to visit all states or state-action pairs, but starting with the next chapter,
    we concentrate on problems that cannot be exhaustively sampled.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的剩余部分，我们将讨论在使用非线性函数近似时出现的微妙之处。你之前学到的所有内容仍然适用。唯一的区别是，我们现在从监督学习和函数近似的世界开始，而不是使用向量和矩阵来存储价值函数和政策。记住，在DRL中，代理从同时具有顺序性（与一次性相反）、评价性（与监督相反）和样本性（与穷举相反）的反馈中学习。我们还没有触及“样本性”的部分；代理总是能够访问所有状态或状态-动作对，但从下一章开始，我们将专注于无法穷举样本的问题。
- en: By now, you
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你已经
- en: Know how to develop RL agents that are more effective at reaching their goals
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知道如何开发更有效的RL代理以实现其目标
- en: Know how to make RL agents that are more sample efficient
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知道如何创建更高效的样本RL代理
- en: Know how to deal with feedback that is simultaneously sequential and evaluative
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知道如何处理同时具有顺序性和评价性的反馈
- en: '| ![](../Images/icons_Tweet.png) | Tweetable FeatWork on your own and share
    your findings |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| ![推文图标](../Images/icons_Tweet.png) | 在自己的工作上努力，并分享你的发现'
- en: '|  | Here are several ideas on how to take what you’ve learned to the next
    level. If you’d like, share your results with the rest of the world and make sure
    to check out what others have done, too. It’s a win-win situation, and hopefully,
    you''ll take advantage of it.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 这里有一些想法，如何将你所学的知识提升到下一个层次。如果你愿意，可以与世界分享你的成果，并确保查看其他人所做的事情。这是一个双赢的局面，希望你能充分利用它。'
- en: '**#gdrl_ch07_tf01:** I only test on the frozen lake 8 × 8 environment the algorithms
    presented in this chapter, but are you curious about how the algorithms in the
    previous chapter compare? Well, do that! Go to the book’s Notebooks and copy the
    algorithms from the previous chapter into this chapter’s Notebook, and then run,
    and collect information to compare all the algorithms.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch07_tf01:** 我只测试了本章中提出的算法在冻结的8 × 8湖环境中的效果，但你好奇上一章的算法如何吗？好吧，那就这么做！去书中的Notebooks，将上一章的算法复制到这一章的Notebook中，然后运行并收集信息来比较所有算法。'
- en: '**#gdrl_ch07_tf02:** There are many more advanced algorithms for the tabular
    case. Compile a list of interesting algorithms and share the list with the world.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch07_tf02:** 表格情况下的高级算法还有很多。整理一份有趣的算法列表，并与世界分享。'
- en: '**#gdrl_ch07_tf03:** Now, implement one algorithm from your list and implement
    another algorithm from someone else’s list. If you’re the first person posting
    to this hashtag, then implement two of the algorithms on your list.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch07_tf03:** 现在，从你的列表中实现一个算法，并从别人的列表中实现另一个算法。如果你是第一个使用这个标签的人，那么就实现你列表中的两个算法。'
- en: '**#gdrl_ch07_tf04:** There’s a fundamental algorithm called prioritized sweeping.
    Can you investigate this algorithm, and tell us more about it? Make sure you share
    your implementation, add it to this chapter’s Notebook, and compare it with the
    other algorithms in this chapter.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch07_tf04:** 有一个基本的算法叫做优先遍历。你能调查这个算法，并告诉我们更多关于它的信息吗？确保你分享你的实现，将其添加到本章的笔记本中，并与本章的其他算法进行比较。'
- en: '**#gdrl_ch07_tf05:** Create and environment like Frozen Lake 8×8, but much
    more complex, something like frozen lake 16 × 16, perhaps? Now test all the algorithms,
    and see how they perform. Are there any of the algorithms in this chapter that
    do considerably better than other algorithms?'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch07_tf05:** 创建一个类似于8×8的Frozen Lake环境，但更复杂，比如16×16的Frozen Lake？现在测试所有算法，看看它们的性能如何。有没有哪个算法在本章中明显优于其他算法？'
- en: '**#gdrl_ch07_tf06:** In every chapter, I’m using the final hashtag as a catchall
    hashtag. Feel free to use this one to discuss anything else that you worked on
    relevant to this chapter. There’s no more exciting homework than that which you
    create for yourself. Make sure to share what you set yourself to investigate and
    your results.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch07_tf06:** 在每一章中，我都使用最后一个标签作为通用的标签。请随意使用这个标签来讨论与本章相关的任何其他工作。没有比你自己创造的任务更令人兴奋的作业了。确保分享你设定要调查的内容和你的结果。'
- en: 'Write a tweet with your findings, tag me @mimoralea (I’ll retweet), and use
    the particular hashtag from the list to help interested folks find your results.
    There are no right or wrong results; you share your findings and check others’
    findings. Take advantage of this to socialize, contribute, and get yourself out
    there! We’re waiting for you!Here’s a tweet example:“Hey, @mimoralea. I created
    a blog post with a list of resources to study deep reinforcement learning. Check
    it out at <link>. #gdrl_ch01_tf01”I’ll make sure to retweet and help others find
    your work. |'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 用你的发现写一条推文，@mimoralea（我会转发），并使用列表中的特定标签来帮助感兴趣的人找到你的结果。没有正确或错误的结果；你分享你的发现并检查别人的发现。利用这个机会社交，做出贡献，让自己脱颖而出！我们正在等待你！以下是一条推文示例：“嘿，@mimoralea。我创建了一个包含资源列表的博客文章，用于研究深度强化学习。查看它在这里<链接>。#gdrl_ch01_tf01”我会确保转发并帮助其他人找到你的作品。|
