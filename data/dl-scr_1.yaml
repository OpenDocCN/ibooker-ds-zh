- en: Part 2\. Throwing neural nets at a search engine
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二部分. 将神经网络应用于搜索引擎
- en: 'Now that you know something about the fundamentals of search and deep learning,
    you can start throwing neural networks at a search engine wherever you see fit,
    right? In theory, yes; in practice, no. Deep neural networks aren’t magic: you
    need to use extreme care when deciding where and how using such powerful techniques
    makes sense. [Chapters 3](kindle_split_015.xhtml#ch03)–[6](kindle_split_018.xhtml#ch06)
    look at tasks that every modern search engine commonly performs and highlight
    their limitations. As we identify them, we’ll explore how to use deep learning
    to mitigate such issues. You’ll see how to better solve the search engine task,
    either by looking at example output or by using more rigorous information-retrieval
    metrics.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了搜索和深度学习的基础知识，你可以在任何合适的地方开始将神经网络应用于搜索引擎，对吧？从理论上讲，是的；在实践中，不是。深度神经网络并非魔法：在决定何时以及如何使用如此强大的技术时，你需要极端小心。[第3章](kindle_split_015.xhtml#ch03)–[6章](kindle_split_018.xhtml#ch06)探讨了现代搜索引擎通常执行的任务，并突出了它们的局限性。当我们识别出这些问题时，我们将探讨如何使用深度学习来减轻这些问题。你将看到如何更好地解决搜索引擎任务，无论是通过查看示例输出还是使用更严格的信息检索指标。
- en: Chapter 3\. From plain retrieval to text generation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3章 从简单检索到文本生成
- en: '*This chapter covers*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Expanding queries
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展查询
- en: Using search logs to build training data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用搜索日志构建训练数据
- en: Understanding recurrent neural networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解循环神经网络
- en: Generating alternative queries with RNNs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RNN生成替代查询
- en: 'In the early days of the internet and search engines (late 1990s), people only
    searched for keywords. Users might have typed “movie zemeckis future” to find
    information about the movie *Back to the Future*, directed by Robert Zemeckis.
    Although search engines have evolved, and today we can type queries using natural
    language, many users still rely on keywords when searching. For these users, it
    would be advantageous if the search engine could generate a proper query based
    on the keywords they type: for example, taking “movie Zemeckis future” and generating
    “Back to the Future by Robert Zemeckis.” Let’s call the generated query an *alternative
    query*, in the sense that it’s an alternative (text) representation of the information
    need expressed by the user.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在互联网和搜索引擎的早期（20世纪90年代末），人们只搜索关键词。用户可能会输入“movie zemeckis future”来查找由罗伯特·泽米吉斯执导的电影《回到未来》的信息。尽管搜索引擎已经发展，如今我们可以使用自然语言进行查询，但许多用户在搜索时仍然依赖于关键词。对于这些用户来说，如果搜索引擎能够根据他们输入的关键词生成合适的查询将是有益的：例如，将“movie
    Zemeckis future”生成“罗伯特·泽米吉斯导演的《回到未来》”。让我们称生成的查询为*替代查询*，从意义上讲，它是用户表达的信息需求的另一种（文本）表示。
- en: This chapter will teach you how to add text-generation capabilities to your
    search engine so that, given a user query, it will generate a few alternative
    queries to run under the hood together with the original one. The goal is to express
    the query in additional ways so as to widen the net of the search—without asking
    the user to think of or type in alternatives. To add text generation to a search
    engine, you’ll use a powerful architecture for neural networks called a *recurrent
    neural network* (RNN).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将教你如何给你的搜索引擎添加文本生成功能，这样，给定一个用户查询，它将生成几个替代查询，与原始查询一起在后台运行。目标是以额外的方式表达查询，以扩大搜索范围——而不需要用户思考或输入替代方案。为了将文本生成添加到搜索引擎中，你将使用一种称为*循环神经网络*（RNN）的强大神经网络架构。
- en: Recurrent neural networks have the same flexibility as the unembellished feed-forward
    networks you learned about in [chapter 2](kindle_split_013.xhtml#ch02). But RNNs
    also have the advantage of being able to deal with long sequences of inputs and
    outputs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络具有与你在第2章中了解的无装饰的前馈网络相同的灵活性。但RNN还有处理长序列输入和输出的优势。
- en: Before you learn how to use RNNs, let’s remember what you did with feed-forward
    networks. You used them with a specific model, word2vec, to improve synonym expansion
    so a query could be expanded using one (or more) of its synonyms. Better synonym
    expansion increases the effectiveness of the search engine by returning more-relevant
    documents. Word2vec uses a specifically designed neural network to generate dense
    vector representations for words. Such vectors can be used to calculate the similarity
    of two words by their vectors’ distances, as in the synonym expansion case. But
    they can also be used as inputs for more complex neural network architectures,
    like RNNs. This is exactly how you’ll use them in this chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习如何使用循环神经网络（RNNs）之前，让我们回顾一下您是如何使用前馈网络的。您使用特定的模型，即word2vec，来改进同义词扩展，以便查询可以通过其同义词之一（或多个）进行扩展。更好的同义词扩展通过返回更多相关文档来提高搜索引擎的有效性。Word2vec使用专门设计的神经网络来生成密集的向量表示，这些向量可以用来通过它们的向量距离计算两个词的相似度，就像同义词扩展的情况一样。但它们也可以用作更复杂神经网络架构的输入，如RNNs。这正是您在本章中将使用它们的方式。
- en: '|  |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: 'In practice, it’s common to train neural networks to accomplish specific tasks
    by arranging neuron activation functions, layers, and their connections, depending
    on the problem at hand. The rest of this book will introduce you to various neural
    network architectures, each addressing a different kind of problem. For example,
    in the computer vision field, where network inputs are usually images or videos,
    it’s common to use *convolutional neural networks* (CNNs). In CNNs, each layer
    has a distinct, specific function: there are convolutional layers, pooling layers,
    and so on. At the same time, the aggregation of these layers allows you to build
    a deep neural network where pixels are incrementally transformed into something
    more abstract: for instance, pixels → edges → objects →. We looked briefly at
    these in [chapter 1](kindle_split_012.xhtml#ch01) and will take a closer look
    in [chapter 8](kindle_split_021.xhtml#ch08).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，通常通过安排神经元激活函数、层及其连接来训练神经网络以完成特定任务，具体取决于手头的问题。本书的其余部分将向您介绍各种神经网络架构，每个架构解决不同类型的问题。例如，在计算机视觉领域，网络输入通常是图像或视频，通常使用*卷积神经网络*（CNNs）。在CNNs中，每一层都有独特的、特定的功能：有卷积层、池化层等等。同时，这些层的聚合允许您构建一个深度神经网络，其中像素逐渐被转换成更抽象的东西：例如，像素
    → 边缘 → 对象 →。我们在[第1章](kindle_split_012.xhtml#ch01)中简要介绍了这些内容，将在[第8章](kindle_split_021.xhtml#ch08)中更深入地探讨。
- en: '|  |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: In [chapter 1](kindle_split_012.xhtml#ch01), you saw how a user can express
    an information need as a variety of slightly different versions, and how even
    small changes in the way a query is written can influence which documents are
    returned first. So when training a neural network to generate output queries from
    input queries, it’s useful to go beyond just the words in a query, apart from
    their context. The aim is to generate text queries that are semantically similar
    to the input query; doing so enables the search engine to return search results
    based on different ways of expressing the same fundamental need (via the query).
    You can use an RNN to generate text in natural language and then integrate that
    generated text into a search engine. The rest of this chapter will teach you how
    RNNs work, how to tune them to generate alternative queries, and how an RNN-backed
    search engine offers improved effectiveness in returning relevant results for
    end users.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](kindle_split_012.xhtml#ch01)中，您看到了用户如何以各种略有不同的版本表达信息需求，以及查询的书写方式甚至微小变化如何影响首先返回哪些文档。因此，当训练神经网络从输入查询生成输出查询时，除了查询中的单词及其上下文之外，还有必要超越这些内容。目标是生成与输入查询在语义上相似的文本查询；这样做使得搜索引擎能够根据表达相同基本需求的不同方式（通过查询）返回搜索结果。您可以使用RNN生成自然语言文本，然后将生成的文本集成到搜索引擎中。本章的其余部分将向您介绍RNN的工作原理，如何调整它们以生成替代查询，以及基于RNN的搜索引擎如何提高为最终用户提供相关结果的有效性。
- en: '3.1\. Information need vs. query: Bridging the gap'
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 信息需求与查询：弥合差距
- en: '[Chapter 1](kindle_split_012.xhtml#ch01) talked about the fundamental problem
    of how users can best express an information need. But as a user, do you really
    want to spend a lot of time thinking about how to word a query? Imagine yourself
    on your way to work on public transport early in the morning, searching for information
    on your phone. You don’t have the time or the brainpower (it’s early!) to come
    up with the best way to interact with a search engine.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[第一章](kindle_split_012.xhtml#ch01)讨论了用户如何最好地表达信息需求的基本问题。但作为一个用户，你真的想花很多时间去思考如何措辞查询吗？想象一下，你早上很早就在公共交通工具上上班，用手机搜索信息。你没有时间或精力（太早了！）去想出与搜索引擎互动的最佳方式。'
- en: If you ask users to explain the information they need in three or four sentences,
    you’re likely to get a detailed explanation of the specific need and its detailed
    context. But if you ask the same person to express what they’re looking for in
    a short query of no more than five or six words, the chances are high that they
    won’t be able to do it, because it’s not always easy to compress a detailed requirement
    into a short sequence of words. As search engineers, we need to do something to
    bridge this gap between user intent and the resulting queries.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你要求用户用三或四句话解释他们所需的信息，你可能会得到对具体需求和其详细背景的详细解释。但如果你要求同一个人用不超过五或六字的简短查询来表达他们想要寻找的内容，他们很可能做不到，因为这并不总是容易将详细的需求压缩成一系列简短的词语。作为搜索工程师，我们需要做些事情来弥合用户意图和生成的查询之间的差距。
- en: 3.1.1\. Generating alternative queries
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 生成替代查询
- en: A well-known technique to help users write queries is providing a hint with
    suggested text while the user is typing the query. This lets the search engine
    UI guide the user while they write. The search engine makes an explicit effort
    to help the user type a “good” query (we’ll take a detailed look at how it does
    this in [chapter 4](kindle_split_016.xhtml#ch04)). Another approach to fill the
    gap between information need and the user-entered query is to postprocess the
    query right after it enters the search engine system but before it’s executed.
    Such a postprocessing task’s responsibility is to use the entered query to create
    a new one that’s “better” to some extent. Of course, “better” can mean different
    things in this context; this chapter focuses on producing a query that expresses
    the same information need in various ways, to increase the likelihood that
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 帮助用户编写查询的一个常用技术是在用户输入查询时提供带有建议文本的提示。这可以让搜索引擎用户界面在用户编写查询时引导他们。搜索引擎会明确地努力帮助用户输入一个“好”的查询（我们将在第4章中详细探讨它是如何做到这一点的[第4章](kindle_split_016.xhtml#ch04)）。另一种填补信息需求与用户输入查询之间差距的方法是在查询进入搜索引擎系统后但执行之前对其进行后处理。这种后处理任务的职责是使用输入的查询创建一个新的查询，它在某种程度上是“更好”的。当然，在这个上下文中，“更好”可能有不同的含义；本章的重点是产生一个以各种方式表达相同信息需求的查询，以增加找到所需知识的机会
- en: A relevant document is included in the result set
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关文档包含在结果集中
- en: More-relevant documents are ranked first in the search results
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在搜索结果中，更相关的文档排在前面
- en: This is usually done manually and incrementally these days—you might fire a
    first query about, for instance, “latest research in artificial intelligence”;
    then a second one such as “what is deep learning”; and then a third one, like
    “recurrent neural networks for search.” The term *manually* refers to the fact
    that in this example, you run a query, look at the results, reason about them,
    write and run another query, look at the results, reason about them, and so on,
    until you either get the knowledge you’re looking for, or you give up.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 目前这通常是通过手动和逐步的方式完成的——例如，你可能先发出一个关于“人工智能最新研究”的初始查询；然后是第二个查询，比如“什么是深度学习”；接着是第三个查询，例如“用于搜索的循环神经网络。”术语*手动*指的是在这个例子中，你运行一个查询，查看结果，对其进行分析，编写并运行另一个查询，查看结果，对其进行分析，如此循环，直到你找到你寻找的知识，或者你放弃。
- en: 'The goal is to produce a set of alternative queries without any interaction
    with the user. Such queries should have the same or similar meaning with respect
    to the original query, but using different words (while still being correctly
    spelled). To see how this should work, let’s go back to the example of the query
    “movie Zemeckis future.” If you enter that phrase, the search engine should do
    the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是生成一组不与用户互动的替代查询。这些查询在原始查询的意义上应该相同或相似，但使用不同的词语（同时保持正确的拼写）。为了了解这是如何工作的，让我们回到查询“Zemeckis未来电影”的例子。如果你输入这个短语，搜索引擎应该执行以下操作：
- en: '**1**.  Accept the user-entered query “movie Zemeckis future.”'
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**. 接受用户输入的查询“movie Zemeckis future。”'
- en: '**2**.  Pass the query through the query time-analysis chain and produce the
    transformed version of the user query—in this case, assuming you’ve configured
    a filter to lowercase capital letters.'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**. 将查询通过查询时间分析链传递并生成用户查询的转换版本——在这种情况下，假设您已配置了一个过滤器将大写字母转换为小写。'
- en: '**3**.  Pass the filtered query “movie zemeckis future” to the RNN and obtain
    one or more alternative queries as output, such as “Back to the Future by Robert
    Zemeckis.”'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**. 将过滤后的查询“movie zemeckis future”传递给RNN，并获得一个或多个替代查询作为输出，例如“罗伯特·泽米基斯的《回到未来》。”'
- en: '**4**.  Transform the original filtered query and the generated alternative
    query into a form that’s implementation-specific to the search engine (a *parsed*
    query).'
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**4**. 将原始过滤查询和生成的替代查询转换为搜索引擎特定的实现形式（*解析*查询）。'
- en: '**5**.  Run the queries against the inverted indexes.'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**5**. 在倒排索引上运行查询。'
- en: As you can see in [figure 3.1](#ch03fig01), you’ll be setting up the search
    engine to use a neural network at search time to generate appropriate alternative
    queries to add to the query entered by the user. You’ll keep the original query
    as it was written by the user and add the generated queries as additional *optional*
    queries. Toward the end of the chapter, we’ll discuss how to best use the generated
    queries.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[图3.1](#ch03fig01)中看到的，您将设置搜索引擎在搜索时间使用神经网络来生成适当的替代查询以添加到用户输入的查询。您将保留用户原始输入的查询，并将生成的查询作为额外的*可选*查询添加。在本章末尾，我们将讨论如何最好地使用生成的查询。
- en: Figure 3.1\. Alternative query generation
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1\. 替代查询生成
- en: '![](Images/03fig01_alt.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/03fig01_alt.jpg)'
- en: '*Automatic query expansion* is the name of the technique of generating (portions
    of) queries under the hood to maximize the number of relevant results for the
    end user. In some sense, synonym expansion (which you saw in [chapter 2](kindle_split_013.xhtml#ch02))
    is a special case of automatic query expansion if you use it at query time only
    (not to index synonyms, but only to expand synonyms for terms in the query).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*自动查询扩展* 是一种在幕后生成（部分）查询的技术，以最大化最终用户的相关结果数量。在某种程度上，如果仅在查询时间使用（不是索引同义词，而是仅扩展查询中的同义词），同义词扩展（您在[第2章](kindle_split_013.xhtml#ch02)中看到的）是自动查询扩展的一个特例。'
- en: 'Your goal is to use this query-expansion feature to improve the query engine
    as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 您的目标是使用此查询扩展功能来改进查询引擎如下：
- en: Minimizing queries with zero results. Providing an alternative text representation
    for a query is more likely to produce hits on search results.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化零结果查询。为查询提供替代文本表示更有可能产生搜索结果。
- en: Improving recall (the fraction of relevant documents that are retrieved, with
    respect to a certain query) by including results you’d have missed otherwise.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过包含您否则会错过的结果来提高召回率（相对于某个查询，检索到的相关文档的比例）。
- en: Improving precision by giving a boost to results that match both the original
    query and an alternative query (which implies that the alternative queries are
    close to the original).
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过提高与原始查询和替代查询（这表明替代查询接近原始查询）都匹配的结果的权重来提高精确度。
- en: '|  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注
- en: Query expansion isn’t just for neural networks; this approach can be implemented
    using various algorithms. You could, theoretically, replace the neural network
    in the query-expansion model with a black box. Before the advent of (deep) RNNs,
    other approaches existed for generating natural language (this is a subfield of
    natural language processing called *natural language generation*). At the end
    of the chapter, I’ll offer a brief comparison to other methods, to illustrate
    “the unreasonable effectiveness of recurrent neural networks.”^([[1](#ch03fn01)])
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 查询扩展不仅限于神经网络；这种方法可以使用各种算法实现。理论上，您可以将查询扩展模型中的神经网络替换为一个黑盒。在（深度）RNN出现之前，存在其他用于生成自然语言的方法（这是自然语言处理的一个子领域，称为*自然语言生成*）。在本章末尾，我将简要比较其他方法，以说明“循环神经网络的不合理有效性。”^([[1](#ch03fn01)])
- en: ¹
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Andrej Karpathy, “The Unreasonable Effectiveness of Recurrent Neural Networks,”
    May 21, 2015, [http://mng.bz/Mxl2](http://mng.bz/Mxl2).
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Andrej Karpathy，“循环神经网络的不合理有效性”，2015年5月21日，[http://mng.bz/Mxl2](http://mng.bz/Mxl2)。
- en: '|  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Before seeing RNNs in action, as is the case with many machine learning scenarios,
    it’s crucial to take a close look at how you train the model, along with what
    kind of data you should use and why. As you may recall, in supervised learning,
    you tell the algorithm how you want the model to produce an output with respect
    to a certain input. Thus the way you structure inputs and outputs depends a lot
    on what you want to achieve. The next section takes a quick tour of three possible
    ways to prepare the data to be fed into the RNN.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在看到RNN（递归神经网络）的实际应用之前，就像许多机器学习场景一样，仔细考虑如何训练模型以及应该使用什么类型的数据以及为什么这样做至关重要。如您所回忆的那样，在监督学习中，您告诉算法模型如何根据特定的输入产生输出。因此，您构建输入和输出的方式在很大程度上取决于您想要实现的目标。下一节将简要介绍三种准备要输入RNN的数据的可能方法。
- en: 3.1.2\. Data preparation
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 数据准备
- en: I’ve chosen RNNs to implement query expansion because they’re surprisingly good
    at and flexible for learning to generate sequences of text, including sequences
    that don’t appear in the training data but that still “make sense.” Additionally,
    RNNs usually require less tuning compared to other natural language generation
    algorithms that use grammars, Markov chains, and so on. All this sounds great,
    but what do you expect to happen when generating alternative queries in practice?
    What should the generated queries look like? As is all too often true in computer
    science, the answer is ... *it depends!*
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择RNN来实现查询扩展，因为它们在生成文本序列方面出奇地好，而且非常灵活，包括生成训练数据中未出现但仍然“有意义”的序列。此外，与使用语法、马尔可夫链等其他自然语言生成算法相比，RNN通常需要更少的调整。这一切听起来都很棒，但在实际生成替代查询时，您期望发生什么？生成的查询应该是什么样子？正如计算机科学中经常发生的那样，答案是
    ... *这取决于！*
- en: It’s important to define what you want to achieve. If you think about the case
    where a user enters the query “books about artificial intelligence,” you could
    provide other queries (or sentences) that carry the same semantic information,
    like “publications from the field of artificial intelligence” or “books dealing
    with the topic of intelligent machines.” At the same time, you need to consider
    how useful such alternative representations would be in your search engine—the
    possible alternative queries may give zero results if you have no documents dealing
    with the topic of artificial intelligence! You don’t want to generate an alternative
    query representation that’s perfect but not useful. Instead, you can look closely
    at user queries and provide alternative representations that are built on the
    information they contain; or you can make the query-generation algorithm obtain
    information from the indexed data rather than the user data, so that the generated
    alternative queries better reflect what’s already in the search engine (and mitigate
    the problem of an alternative query returning no results).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 定义您想要实现的目标非常重要。如果您考虑用户输入查询“关于人工智能的书籍”的情况，您可以提供其他具有相同语义信息的查询（或句子），例如“人工智能领域的出版物”或“处理智能机器主题的书籍。”同时，您还需要考虑这些替代表示在您的搜索引擎中的有用性——如果没有任何处理人工智能主题的文档，可能的替代查询可能会给出零结果！您不希望生成一个完美但无用的替代查询表示。相反，您可以仔细查看用户查询，并基于它们包含的信息提供替代表示；或者您可以让查询生成算法从索引数据而不是用户数据中获取信息，这样生成的替代查询就能更好地反映搜索引擎中已有的内容（并减轻替代查询返回零结果的问题）。
- en: 'In real life, you often have access to *query logs*, which are flat records
    of what users have queried via the search engine with minimal information about
    the results. You can gain many insights from looking at query logs. For instance,
    you can clearly see when people fail to find what they’re looking for, because
    they will submit queries that are similar in meaning. You can also observe how
    users switch from searching for one topic to another. For the sake of an example,
    let’s say you’re building a search engine for a media company that provides political,
    cultural, and fashion news to users. Here’s a sample query log:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，您通常可以访问*查询日志*，这是用户通过搜索引擎查询的扁平记录，其中包含关于结果的最少信息。您可以从查看查询日志中获得许多见解。例如，您可以清楚地看到人们未能找到他们想要的东西，因为他们会提交意义相似的查询。您还可以观察用户如何从一个主题切换到另一个主题。为了举例说明，假设您正在为一家向用户提供政治、文化和时尚新闻的媒体公司构建搜索引擎。以下是一个示例查询日志：
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***1* The query “covfefe” returned 100 results, and the first two resulting
    document identifiers are doc113 and doc588.**'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 查询“covfefe”返回了100个结果，前两个结果文档标识符是doc113和doc588。'
- en: Assume that this is part of a huge query log of user activity on the search
    engine. Now, imagine you have to build a *training set* from this query log—a
    collection of examples of inputs associated with desired outputs—correlating similar
    queries so that you can build training examples where the input is a query and
    the target output is one or more correlated queries. In this case, each example
    will consist of one input query and one or more output queries. In practice, it’s
    common to use query logs for such learning tasks because
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这是搜索引擎用户活动的大量查询日志的一部分。现在，想象一下你必须从这个查询日志中构建一个*训练集*——一个与期望输出相关联的输入示例集合，关联相似查询，以便你可以构建输入是一个查询，目标输出是一个或多个相关查询的训练示例。在这种情况下，每个示例将包括一个输入查询和一个或多个输出查询。在实践中，使用查询日志进行此类学习任务是常见的，因为
- en: Query logs reflect the behavior of users on that specific system, so the resulting
    model will behave relatively close to the actual users and data.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询日志反映了特定系统上用户的行为，因此生成的模型将相对接近实际用户和数据。
- en: Using or generating other datasets may incur additional costs while possibly
    training a model that’s based on different data, users, domains, and so on.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '使用或生成其他数据集可能会产生额外的成本，同时可能训练出一个基于不同数据、用户、领域等模型。 '
- en: 'In the current example, imagine that you have two related queries: “men clothing
    latest trends” and “Paris fashion week.” You can use them interchangeably as input
    and output for training a neural network. A nontrivial decision you need to make
    is how to measure the correlation (similarity) of two queries. Your general knowledge
    tells you that the two queries are similar in the sense that the Paris fashion
    week event has a significant influence on clothing (fashion) trends (for both
    men and women), so you may decide to set “Paris fashion week” as an alternative
    representation of the “men clothing latest trends” query; see [figure 3.2](#ch03fig02).
    But in this context, neither the search engine nor the neural network knows anything
    about the topic of fashion—they just see input and output texts and vectors.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前示例中，想象一下你有两个相关的查询：“men clothing latest trends”和“Paris fashion week”。你可以将它们互换使用作为训练神经网络的输入和输出。你需要做出的一个非平凡的决定是如何衡量两个查询之间的相关性（相似度）。你的常识告诉你，这两个查询在某种意义上是相似的，因为巴黎时装周活动对服装（时尚）趋势（男性和女性）有显著影响，所以你可能决定将“Paris
    fashion week”设置为“men clothing latest trends”查询的另一种表示形式；参见[图3.2](#ch03fig02)。但在这个上下文中，搜索引擎和神经网络对时尚主题一无所知——它们只是看到输入和输出文本以及向量。
- en: Figure 3.2\. Learning from queries
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.2\. 从查询中学习
- en: '![](Images/03fig02_alt.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/03fig02_alt.jpg)'
- en: 'Each line from the query log contains a user-entered query associated with
    its search results: more precisely, the document IDs of the matching results.
    But this isn’t what you need. Your training examples have to be composed of an
    input query and one or more output queries that are similar or by some means correlated
    to the input. So before you can train the network, you need to process the lines
    of the search log and create a training set. This type of work, which involves
    manipulating and tweaking the data, is often called *data preparation* or *preprocessing*.
    Although it may sound a bit tedious, it’s crucial for the effectiveness of any
    associated machine learning task.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 查询日志中的每一行都包含一个与搜索结果关联的用户输入查询：更确切地说，是匹配结果的文档ID。但这不是你所需要的。你的训练示例必须由一个输入查询和一个或多个与输入相似或以某种方式相关的输出查询组成。因此，在你能够训练网络之前，你需要处理搜索日志的行并创建一个训练集。这种涉及操纵和调整数据的工作通常被称为*数据准备*或*预处理*。虽然这可能听起来有点繁琐，但对于任何相关的机器学习任务的有效性至关重要。
- en: 'The following sections look at three different ways of selecting input and
    output sequences for a neural network to use to learn to generate alternative
    queries: correlating queries that generate similar search result sets, that come
    from the same users in specific time windows, or that contain similar search terms.
    Each of these options will yield specific side effects related to the way the
    neural network will learn to generate new queries.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下几节将探讨三种不同的方法来选择输入和输出序列，以便神经网络可以用来学习生成替代查询：关联生成相似搜索结果集的查询、来自特定时间窗口的同一用户的查询，或包含相似搜索术语的查询。这些选项中的每一个都将产生与神经网络学习生成新查询的方式相关的特定副作用。
- en: Correlating queries that generate similar search result sets
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关联生成相似搜索结果集的查询
- en: The first approach groups queries that share a portion of their associated search
    result. For example, you could extract the following from the example query log.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法将共享部分相关搜索结果的查询分组。例如，你可以从以下示例查询日志中提取以下内容。
- en: Listing 3.1\. Correlating queries using shared results
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.1\. 使用共享结果关联查询
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***1* Shares doc1 and doc5**'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 共享文档doc1和doc5**'
- en: '***2* Shares doc5**'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 共享文档doc5**'
- en: '***3* Shares doc1**'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 共享文档doc1**'
- en: '***4* Shares doc113**'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 共享文档doc113**'
- en: By correlating queries having shared documents in the search log, you can see
    that “latest trends” can generate “covfefe” and vice versa, and the artificial
    intelligence–related queries seem to suggest good alternatives.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过关联搜索日志中具有共享文档的查询，你可以看到“最新趋势”可以生成“covfefe”，反之亦然，与人工智能相关的查询似乎暗示了良好的替代方案。
- en: 'Note that “latest trends” refers to a relative concept: the latest trends one
    day may (or will) be significantly different than those tomorrow or next week.
    If you assume the *covfefe* trend lasted one week, it would be bad for the neural
    network to generate “covfefe” as an alternative query for “latest trends” one
    month after *covfefe* showed up in the news. As the real world outside of a search
    engine changes, you need to be careful about using data that is up to date, or
    at least avoid potential problems by removing training examples that may cause
    bad results, as in this case.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，“最新趋势”是一个相对概念：一天内的最新趋势可能与明天或下周的显著不同。如果你假设*covfefe*趋势持续了一周，那么在*covfefe*出现在新闻一个月后，神经网络生成“covfefe”作为“最新趋势”的替代查询将是不好的。随着搜索引擎外部的现实世界发生变化，你需要小心使用最新数据，或者至少通过移除可能导致不良结果的训练示例来避免潜在问题，就像在这个例子中一样。
- en: Correlating queries that come from the same users in specific time windows
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关联特定时间窗口内来自同一用户的查询
- en: The second potential approach relies on the assumption that users search for
    similar things in small time windows. For example, if you’re searching for “that
    specific restaurant I went to, but I can’t recall its name,” you’ll perform multiple
    searches that relate to the same information need. The key point of this approach
    is to identify accurate time windows in the query logs so that queries related
    to the same information need can be grouped together (regardless of their results).
    In practice, identifying search sessions that relate to the same need isn’t necessarily
    simple and depends on how informative the search logs are. For instance, if the
    search log is a flat list of concurrent anonymous searches for all users, it will
    be difficult to say which queries were performed by a single user. If you instead
    have information about every user, such as their IP address, you can try to identify
    a search session per topic.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种潜在的方法依赖于用户在小型时间窗口内搜索相似内容的假设。例如，如果你在搜索“我去的那个特定的餐厅，但我记不起它的名字”，你会执行多个与相同信息需求相关的搜索。这种方法的关键点是识别查询日志中的准确时间窗口，以便可以将与相同信息需求相关的查询分组在一起（无论其结果如何）。在实践中，识别与相同需求相关的搜索会话并不一定简单，这取决于搜索日志的信息量。例如，如果搜索日志是所有用户同时进行的匿名搜索的扁平列表，那么很难说哪些查询是由单个用户执行的。如果你有关于每个用户的信息，例如他们的IP地址，你可以尝试根据每个主题识别一个搜索会话。
- en: Let’s assume that the sample search log comes from a single user. The time information
    on each line indicates that the first two queries were run in a two-minute window,
    whereas the others were run a long time apart. So you could correlate the first
    two queries—”artificial intelligence” and “books about AI”—and skip the others.
    But in real life, people may be doing multiple things concurrently, like wanting
    to get information about a technical topic while going to work but also needing
    information about public transport time tables or traffic on the highway. In such
    cases, it’s difficult to distinguish which queries are semantically correlated
    without looking at the query terms, which you do in the third approach.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 假设样本搜索日志来自单个用户。每行的计时信息表明前两个查询是在两分钟内运行的，而其他查询则相隔很长时间。因此，你可以关联前两个查询——“人工智能”和“关于人工智能的书籍”——并跳过其他查询。但在现实生活中，人们可能同时做很多事情，比如在上班的同时想获取关于技术主题的信息，同时也需要关于公共交通时间表或高速公路交通的信息。在这种情况下，不查看查询术语很难区分哪些查询在语义上是相关的，这在第三种方法中是查看查询术语的。
- en: Correlating queries that contain similar search terms
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关联包含相似搜索词的查询
- en: Using similar terms to correlate queries is tricky to implement. On one hand,
    it sounds simple. You can find common terms among the queries in the search log,
    as shown next.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用类似术语来关联查询在实现上很棘手。一方面，它听起来很简单。你可以找到搜索日志中查询之间的共同术语，如下所示。
- en: Listing 3.2\. Correlating queries using search terms
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.2\. 使用搜索词关联查询
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***1* Shares “artificial” and “intelligence” terms**'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 共享“人工”和“智能”术语**'
- en: '***2* Shares nothing**'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 没有共享任何内容**'
- en: Here, you’ve lost some information that was carried by the query results, as
    you can see in comparison with the previous listing; in addition, the training
    set is much smaller and poorer. Let’s look at “books about AI.” This is surely
    related to “artificial intelligence” and, perhaps, to “artificial intelligence
    hype.” But simple term matching fails to capture the fact that *AI* is short for
    *artificial intelligence*. You can mitigate that issue by applying synonym expansion
    techniques, as you learned in [chapter 2](kindle_split_013.xhtml#ch02); doing
    so requires an additional preprocessing step to generate new search log lines
    in which synonyms are expanded. In this example, if your synonym expansion algorithm
    can map the term “AI” to the composite term “artificial intelligence,” you’ll
    get the following input/output pairs.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你丢失了一些由查询结果携带的信息，正如你可以在与前面的列表比较中看到的那样；此外，训练集要小得多，质量也较差。让我们看看“关于人工智能的书籍”。这肯定与“人工智能”有关，也许还与“人工智能炒作”有关。但简单的术语匹配无法捕捉到“AI”是“人工智能”的缩写这一事实。你可以通过应用同义词扩展技术来减轻这个问题，正如你在[第2章](kindle_split_013.xhtml#ch02)中学到的；这样做需要额外的预处理步骤来生成新的搜索日志行，其中同义词被扩展。在这个例子中，如果你的同义词扩展算法可以将术语“AI”映射到复合术语“人工智能”，你将得到以下输入/输出对。
- en: Listing 3.3\. Correlating queries using search terms and synonym expansion
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.3\. 使用搜索词和同义词扩展关联查询
- en: '[PRE3]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '***1* Additional mapping; shares “artificial” and “intelligence terms”**'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 额外的映射；共享“人工”和“智能”术语**'
- en: 'With respect to the former results, you now have an additional mapping: using
    synonyms generated by the new input query “books about artificial intelligence,”
    which didn’t exist in the original search log. Although this seems fine, be careful,
    because there may be more than one synonym for each term in each query. That’s
    often the case with large dictionaries like WordNet and also when using word embeddings
    based on similarity (such as word2vec) to expand synonyms. Having more data for
    training neural networks is usually desirable, but it has to be of good quality
    to give good results. Let’s not forget that this is a preprocessing stage to train
    a neural network that will be used to generate sequences. If you feed the neural
    network with text sequences that don’t make much sense (not all synonyms of a
    certain word fit well in every possible context), it will generate sequences with
    little or no meaning.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 关于前面的结果，你现在有一个额外的映射：使用由新输入查询“关于人工智能的书籍”生成的同义词，这在原始搜索日志中不存在。尽管这似乎没问题，但请注意，每个查询中的每个术语可能有多个同义词。这通常是像WordNet这样的大型词典的情况，也适用于基于相似性（如word2vec）的词嵌入来扩展同义词的情况。通常，为训练神经网络提供更多数据是可取的，但必须保证数据质量以获得良好的结果。我们不要忘记，这是一个预处理阶段，用于训练一个将用于生成序列的神经网络。如果你向神经网络提供没有太多意义的文本序列（不是所有某个词的同义词都适合每个可能的环境），它将生成几乎没有意义的序列。
- en: If you plan to use synonym expansion, you probably should *not* expand on every
    possible synonym; you could instead do so only for input queries that don’t have
    a corresponding alternative query, such as “books about AI” in the previous example.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划使用同义词扩展，你可能应该*不*对每个可能的同义词进行扩展；你可以选择只对没有对应替代查询的输入查询进行扩展，例如前面例子中的“关于人工智能的书籍”。
- en: Selecting output sequences from the indexed data
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从索引数据中选择输出序列
- en: 'If the techniques described so far don’t work well enough on your data—for
    example, user-entered queries often give too few or zero results—you can get some
    help from the indexed data. In many real-life scenarios, indexed documents have
    a title, which is usually relatively short. Such a title can be used as a query
    if it’s correlated to the original input query. Let’s again choose the query “movie
    Zemeckis future.” Running it on a movie search engine (such as IMDB) would probably
    return something like this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果到目前为止描述的技术在你的数据上效果不够好——例如，用户输入的查询通常给出太少或零结果——你可以从索引数据中获得一些帮助。在许多实际场景中，索引文档有一个标题，这通常相对较短。这样的标题如果与原始输入查询相关联，可以用作查询。让我们再次选择查询“movie
    Zemeckis future”。在电影搜索引擎（如IMDB）上运行它可能会返回类似以下内容：
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s imagine how this document was retrieved:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一下这个文档是如何被检索到的：
- en: The term “movie” is on a stopword list on a search engine about movies, so it
    didn’t match.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “movie”这个术语在电影搜索引擎的停用词列表中，所以它没有匹配。
- en: The term “Zemeckis” matched in both the `writers` and `director` fields.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 术语“Zemeckis”在`writers`和`director`字段中都匹配。
- en: The term “future” matched in the `title` field.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 术语“future”在`title`字段中匹配。
- en: 'Put yourself in the shoes of someone looking at both the queries and the results:
    as the user types a query, if you saw the user entering “movie Zemeckis future,”
    you could immediately tell they should have typed a query like “back to the future”
    instead. That’s exactly the type of training example you can pass to a neural
    network, composed of an input (“movie Zemeckis future”) and a target output (“back
    to the future”). You can preprocess the search log so that the target alternative
    query to be generated by the neural network is the query that would return the
    best result. Doing so will likely help reduce the number of queries with zero
    results, because the hints in the alternative queries don’t come from the user-generated
    queries but rather from the text of relevant documents. To build training examples,
    you associate a query with the titles of the top two or three relevant documents
    from the search log, as in [figure 3.3](#ch03fig03).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在查看查询和结果：当用户输入查询时，如果你看到用户输入“movie Zemeckis future”，你就能立即告诉他们应该输入“back
    to the future”这样的查询。这正是你可以传递给神经网络的训练示例的类型，它由输入（“movie Zemeckis future”）和目标输出（“back
    to the future”）组成。你可以预处理搜索日志，以便神经网络生成的目标替代查询是能够返回最佳结果的查询。这样做可能会帮助减少零结果查询的数量，因为替代查询中的提示不是来自用户生成的查询，而是来自相关文档的文本。为了构建训练示例，你将查询与搜索日志中排名前两到三个的相关文档的标题相关联，如[图3.3](#ch03fig03)所示。
- en: Figure 3.3\. Learning from the titles of relevant documents
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.3\. 从相关文档的标题中学习
- en: '![](Images/03fig03_alt.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/03fig03_alt.jpg)'
- en: You may wonder, why not use the search engine instead of a neural network to
    generate alternative queries? That approach would constrain the set of alternative
    queries for a certain input text to what the search engine can already do in terms
    of matching. For example, “movie Zemeckis future” will always give the same set
    of alternative queries if you use the search engine to generate them. In the case
    of the example query, that would work—but what if the user typed “movie spielberg
    future” (confusing the movie’s producer with its director)? There’s no match in
    the search engine with the term “spielberg.” So the search engine might return
    a lot of movies Steven Spielberg has directed that involve the term “future,”
    but it wouldn’t return *Back to the Future*. The key takeaway is that you aren’t
    limited to using queries to train the neural network, as long as the target output
    is correlated with the input in a way that’s useful for representing an alternative
    query.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么不使用搜索引擎而不是神经网络来生成替代查询呢？这种方法将限制某个输入文本的替代查询集，使其仅限于搜索引擎在匹配方面的能力。例如，如果你使用搜索引擎生成“movie
    Zemeckis future”的替代查询，它总是会给出相同的查询集。在示例查询的情况下，这可能会奏效——但是，如果用户输入“movie spielberg
    future”（将电影的制片人误认为是导演）呢？在搜索引擎中没有与“spielberg”这个词的匹配。因此，搜索引擎可能会返回许多史蒂文·斯皮尔伯格执导的涉及“future”一词的电影，但不会返回《回到未来》。关键点是，你并不局限于使用查询来训练神经网络，只要目标输出与输入以某种有用的方式相关联，以便表示替代查询。
- en: Unsupervised streams of text sequences
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 无监督的文本序列流
- en: 'A completely different approach for feeding an RNN for text generation is to
    perform unsupervised learning over streams of text. As mentioned in [chapter 1](kindle_split_012.xhtml#ch01),
    this is a form of machine learning where the learning algorithm isn’t told anything
    about good (or bad) output; the algorithm just builds a model of the data as accurately
    as possible. You’ll see that this is probably the most surprising way RNNs can
    learn to generate text: no one is telling them what good output is, so they learn
    to reproduce good-quality text sequences on the basis of the inputs.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为文本生成向RNN提供数据的一个完全不同的方法是，在文本流上执行无监督学习。如[第1章](kindle_split_012.xhtml#ch01)所述，这是一种机器学习方法，其中学习算法没有被告知有关良好（或不良）输出的任何信息；算法只是尽可能准确地构建数据模型。你会发现这可能是RNN学习生成文本最令人惊讶的方式：没有人告诉它们什么是好的输出，因此它们基于输入学习复制高质量的文本序列。
- en: 'In the search log example, you take the queries one after the other, removing
    everything else:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索日志示例中，你一个接一个地处理查询，移除其他所有内容：
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As you can see, this is plain text. All you need to do is decide how to identify
    the end of a query. In this case, you might use the carriage return character
    (`\n`) as a delimiter for two consecutive queries, and the text-generation algorithm
    will stop whenever it generates a carriage return. This approach is tempting,
    because it requires almost no preprocessing: the data to be used can come from
    anywhere because it’s just plain text. You’ll see the pros and cons later in this
    chapter.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这是纯文本。你所需做的就是决定如何识别查询的结尾。在这种情况下，你可能使用回车符（`\n`）作为两个连续查询的分隔符，文本生成算法将在生成回车符时停止。这种方法很有吸引力，因为它几乎不需要预处理：要使用的数据可以来自任何地方，因为它只是纯文本。你将在本章后面看到其优缺点。
- en: 3.1.3\. Wrap-up of generating data
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3\. 数据生成总结
- en: 'Here’s a quick summary of what we’ve discussed in this section:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是对本节所讨论内容的简要总结：
- en: Performing supervised learning over similar queries gives you the advantage
    of being able to specify what you think are good, similar queries. The downside
    is that the neural network’s effectiveness will be based on how good you are at
    defining when two queries are similar during the data-preparation phase.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在相似查询上执行监督学习可以让你能够指定你认为好的、相似的查询。缺点是神经网络的效率将取决于你在数据准备阶段定义两个查询何时相似的能力。
- en: You may not want to explicitly specify when two queries are similar, but rather
    let the relevant documents for a query provide the alternative query text. This
    will make the neural network generate alternative queries whose text comes from
    the indexed documents (for example, the document titles) and will likely reduce
    the number of queries with few or zero results.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能不想明确指定两个查询何时相似，而是让查询的相关文档提供替代查询文本。这将使神经网络生成替代查询，其文本来自索引文档（例如，文档标题），并可能减少查询结果很少或为零的数量。
- en: An unsupervised approach considers the stream of queries from the search log
    as a sequence of plausible consecutive words, so little data preparation is needed.
    The advantages of this approach are that it’s simple to implement and can closely
    capture which consecutive queries (and hence topics) users tend to be interested
    in.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种无监督的方法将搜索日志中的查询流视为一系列可能的连续单词，因此不需要太多的数据准备。这种方法的优势在于它易于实现，并且可以紧密捕捉用户倾向于感兴趣的连续查询（以及因此的主题）。
- en: There are many alternatives and considerable room for creativity to build new
    ways to generate data that suits the need of your users. The key point is to be
    careful how you prepare data for your system. We’ll assume that you’ve chosen
    one of the approaches discussed here; next, we’ll examine how RNNs learn to generate
    sequences of text.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多替代方案和相当大的创造性空间来构建适合用户需求的数据生成新方法。关键点是要小心地为你的系统准备数据。我们假设你已经选择了这里讨论的方法之一；接下来，我们将检查RNN是如何学习生成文本序列的。
- en: 3.2\. Learning over sequences
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 序列学习
- en: In [chapter 1](kindle_split_012.xhtml#ch01), you saw what the general architecture
    of a neural network looks like, with input and output layers at the edges of the
    network and hidden layers in between. Then, in [chapter 2](kindle_split_013.xhtml#ch02),
    we started looking at two less-general neural network models (continuous-bag-of-words
    and skip-gram) used to implement the word2vec algorithm. The architectures discussed
    so far can be used to model how an input can be mapped into its corresponding
    output. In the case of the skip-gram model, you map an input vector representing
    a certain word to an output vector representing a fixed number of words.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](kindle_split_012.xhtml#ch01)中，你看到了神经网络的一般架构，输入层和输出层位于网络的边缘，隐藏层位于其中。然后，在[第2章](kindle_split_013.xhtml#ch02)中，我们开始探讨两种不太通用的神经网络模型（连续词袋模型和跳字模型），这些模型用于实现word2vec算法。到目前为止讨论的架构可以用来模拟输入如何映射到其对应的输出。在跳字模型的情况下，你将代表某个单词的输入向量映射到代表固定数量单词的输出向量。
- en: 'Let’s think of a simple feed-forward neural network you could use to detect
    the language used in text sentences: for example, for the four languages English,
    German, Portuguese, and Italian. This is called a *multiclass classification task*,
    where the input is a piece of text and the output is one of three or more possible
    classes assigned to that input (the document-categorization example from [chapter
    1](kindle_split_012.xhtml#ch01) is also a multiclass classification task). In
    this example, a neural network that can perform such a task will have four output
    neurons, one for each class (language). Only one output neuron will be set to
    `1` in the output layer, to signal that the input belongs to a certain class.
    For example, if the value of output neuron 1 is `1`, then the input text is classified
    as English; if the value of output neuron 2 is `1`, then the input text is classified
    as German; and so on.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的前馈神经网络，它可以用来检测文本句子中使用的语言：例如，用于英语、德语、葡萄牙语和意大利语这四种语言。这被称为*多类分类任务*，其中输入是一段文本，输出是分配给该输入的三个或更多可能类别中的一个（[第1章](kindle_split_012.xhtml#ch01)中的文档分类示例也是一个多类分类任务）。在这个例子中，能够执行此类任务的神经网络将具有四个输出神经元，每个类别（语言）一个。输出层中只有一个输出神经元会被设置为`1`，以表示输入属于某个类别。例如，如果输出神经元1的值为`1`，则输入文本被分类为英语；如果输出神经元2的值为`1`，则输入文本被分类为德语；依此类推。
- en: 'The dimension of the input layer is much trickier to define. If you assume
    you’re working with fixed-size text sequences, you can design the input layer
    accordingly. For language detection, you need several words, so let’s assume you’ll
    set up the input layer with nine neurons: one per input word; see [figure 3.4](#ch03fig04).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层的维度定义起来要复杂得多。如果你假设你正在处理固定大小的文本序列，你可以相应地设计输入层。对于语言检测，你需要几个单词，所以让我们假设你将设置输入层为九个神经元：每个输入词一个；参见[图3.4](#ch03fig04)。
- en: Figure 3.4\. Feed-forward neural network with nine inputs and four outputs for
    language detection
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.4\. 用于语言检测的前馈神经网络，具有九个输入和四个输出
- en: '![](Images/03fig04_alt.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![注意](Images/03fig04_alt.jpg)'
- en: '|  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: 'In practice, it would be difficult to use such one-to-one mapping between words
    and neurons: as you saw with the one-hot-encoding technique described for word2vec,
    each word is represented as a vector of all zeros except one, whose size is equal
    to the size of the entire vocabulary. In this case, if you were to use one-hot
    encoding, the input layer would contain 9 * *size(vocabulary)* neurons. But because
    we’re focusing on fixed-size inputs, this isn’t important here.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，使用这种单词和神经元之间的一对一映射会很困难：正如你在word2vec中描述的one-hot编码技术所看到的，每个单词都被表示为一个除了一个值为1之外其余都是0的向量，其大小等于整个词汇表的大小。在这种情况下，如果你使用one-hot编码，输入层将包含9
    * *size(vocabulary)*个神经元。但由于我们关注的是固定大小的输入，这在这里并不重要。
- en: '|  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Clearly, you have a problem if a text sequence has fewer than nine words: you
    need to fill (or *pad*) it with some fake filler words. For longer sequences,
    you’ll do language detection nine words at a time. Consider the text of a movie
    review. The contents might be in one language—for example, Italian—but deal with
    a movie whose title is in its original language—for example, English. If you split
    the review text into nine-word sequences, the output may be in either “Italian”
    or “English,” depending on the portion of text fed into the neural network.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，如果一个文本序列少于九个单词，你就遇到了问题：你需要用一些假的填充词来填充（或*填充*）它。对于较长的序列，你将每次检测九个单词的语言。考虑一下电影评论的文本。内容可能是一种语言——例如，意大利语——但处理的电影标题可能是其原始语言——例如，英语。如果你将评论文本分成九个单词的序列，输出可能是“意大利语”或“英语”，这取决于输入到神经网络的部分文本。
- en: With this limitation in mind, how can you make a neural network learn from sequences
    of inputs whose size isn’t known in advance? If you knew the size of each sequence
    you wanted the network to learn, you could make the input layer long enough to
    include the entire sequence. But doing so would hurt performance in the case of
    long sequences, because learning from a larger input requires more neurons in
    the hidden layer in order for the network to give accurate results. Thus this
    solution wouldn’t scale well. RNNs can handle unbounded sequences of text by keeping
    their input and output layer sizes fixed, so they’re perfect for learning to generate
    sequences of text in the use case of automatically expanding queries.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这种限制，你如何让一个神经网络从大小事先未知的输入序列中学习呢？如果你知道网络要学习的每个序列的大小，你可以使输入层足够长，以包含整个序列。但这样做会在长序列的情况下损害性能，因为从更大的输入中学习需要隐藏层有更多的神经元，以便网络给出准确的结果。因此，这种解决方案的扩展性不好。RNN通过保持其输入和输出层的大小固定，可以处理无界的文本序列，因此在自动扩展查询的使用案例中，它们非常适合学习生成文本序列。
- en: 3.3\. Recurrent neural networks
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3. 循环神经网络
- en: You can think of an RNN as a neural network that can remember information about
    its inputs as it processes them, so that the outputs produced by subsequent inputs
    also depend on previously seen inputs. At the same time, the size of the input
    layer (and the output layer, if the RNN generates sequences) is fixed.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将RNN想象成一个神经网络，它在处理输入信息的同时能够记住关于这些输入的信息，因此后续输入产生的输出也依赖于之前看到的输入。同时，输入层（以及如果RNN生成序列，输出层）的大小是固定的。
- en: For now, this is a bit abstract, but you’ll come to understand how it works
    in practice and why it’s important. Let’s try to generate text sequences *without*
    an RNN, using a feed-forward neural network with five inputs and four outputs.
    The language-detection example used one input for each word, but in practice it’s
    often more convenient to use characters instead of strings. The reason is that
    the number of possible words is much larger than the number of available characters,
    and it can be easier for the network to learn how to handle all possible combinations
    of 255 characters than all possible combinations of more than 300,000 words.^([[2](#ch03fn02)])
    Using the one-hot-encoding technique, a character would be represented with a
    vector of size 255, and a word taken from the *Oxford English Dictionary* would
    be represented as a vector of size 301,000! The neural network input layer would
    need 301,000 neurons for one word, as opposed to 255 neurons for one character.
    On the other hand, a word represents a combination of characters that has meaning.
    At the character level, such information isn’t available, and therefore a neural
    network with character inputs must first learn to generate meaningful words from
    characters; that isn’t the case if you use words as inputs. In the end, it’s a
    trade-off.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，这有点抽象，但你会了解到它在实际中是如何工作的，以及为什么它很重要。让我们尝试使用一个没有RNN的文本序列生成，使用具有五个输入和四个输出的前馈神经网络。语言检测示例使用一个输入对应每个单词，但在实践中，通常更方便使用字符而不是字符串。原因是可能的单词数量远大于可用的字符数量，对于网络来说，学习如何处理255个字符的所有可能的组合可能比学习300,000多个单词的所有可能的组合更容易。使用one-hot编码技术，一个字符可以用一个大小为255的向量表示，而从*牛津高阶英汉双解大词典*中选取的一个单词可以用一个大小为301,000的向量表示！神经网络输入层需要一个有301,000个神经元的单词输入，而不是一个字符的255个神经元。另一方面，一个单词代表具有意义的字符组合。在字符级别，这种信息是不可用的，因此具有字符输入的神经网络必须首先学会从字符生成有意义的单词；如果你使用单词作为输入，则不是这种情况。最终，这是一个权衡。
- en: ²
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This number is constantly increasing; see the *Oxford English Dictionary*, [www.oed.com](http://www.oed.com).
  id: totrans-125
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个数字是不断增长的；参见[牛津高阶英汉双解大词典](http://www.oed.com)。
- en: For example, when using characters, the sentence “the big brown fox jumped over
    the lazy dog” can be split into chunks of five characters. Then, each input is
    fed into the neural network with five input neurons; see [figure 3.5](#ch03fig05).
    You can pass an entire sequence to the network regardless of the size of the input
    layer. It appears that you can use a “simple” neural network—you don’t need an
    RNN.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当使用字符时，句子“the big brown fox jumped over the lazy dog”可以被分成五个字符的块。然后，每个输入都被送入具有五个输入神经元的神经网络；参见[图3.5](#ch03fig05)。你可以将整个序列传递给网络，无论输入层的大小如何。看起来你可以使用一个“简单”的神经网络——你不需要RNN。
- en: Figure 3.5\. Neural network ingesting an input sequence with a fixed input layer
    of five neurons
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.5\. 神经网络以五个神经元为固定输入层摄入输入序列
- en: '![](Images/03fig05_alt.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/03fig05_alt.jpg)'
- en: 'But imagine if humans listening to someone talking had to understand what that
    person was saying by only hearing words composed of five characters and forgetting
    each sequence as soon as they heard the next. For example, if someone said “my
    name is Yoda,” you’d get each of the following sequences, without remembering
    all the others:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 但想象一下，如果人类在听别人说话时，只能通过听到由五个字符组成的单词来理解那个人在说什么，并且每次听到下一个序列时就会忘记之前的序列。例如，如果有人说“my
    name is Yoda”，你会得到以下每个序列，而不会记住所有其他的：
- en: '[PRE6]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now you’re asked to repeat what you heard. Weird! With such a short fixed input,
    you may rarely get entire words, and each input is always detached from the rest
    of the sentence.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在要求你重复你所听到的内容。真奇怪！在如此短的固定输入下，你可能很少能听到完整的单词，而且每次输入总是与句子的其余部分分离。
- en: 'What makes it possible to understand a sentence is that each time you hear
    a five-character sequence, you keep track of what you received immediately before
    that. Let’s say you have a memory of size 10:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使理解一个句子成为可能的是，每次你听到一个五个字符的序列时，你都会追踪在那之前立即接收到的内容。假设你有一个大小为10的内存：
- en: '[PRE7]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is a huge simplification of how both humans and neural networks work with
    input and memory, but it should be enough for you to see the rationale behind
    the effectiveness of RNNs (a simple one is shown in [figure 3.6](#ch03fig06))
    in working with sequences versus plain feed-forward neural networks.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对人类和神经网络如何处理输入和记忆的巨大简化，但应该足以让你看到RNN（如[图3.6](#ch03fig06)所示）在处理序列与普通前馈神经网络相比有效性的原因。
- en: Figure 3.6\. A recurrent neural network
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.6\. 循环神经网络
- en: '![](Images/03fig06_alt.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/03fig06_alt.jpg)'
- en: 3.3.1\. RNN internals and dynamics
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. RNN内部结构和动态
- en: These special neural networks are called *recurrent* because, via simple looping
    connections in the hidden-layer neurons, the network becomes capable of operations
    that depend on the current input and the previous state of the network with respect
    to the previous input. In the case of learning to generate the text “my name is
    Yoda,” the internal state of the RNN can be thought of as the memory that makes
    it possible to understand the sentence. Let’s pick a single neuron in the hidden
    layer in an RNN, as shown in [figure 3.7](#ch03fig07).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特殊的神经网络被称为*循环*神经网络，因为通过隐藏层神经元中的简单循环连接，网络能够执行依赖于当前输入和相对于前一个输入的网络状态的运算。在学会生成文本“my
    name is Yoda”的情况下，RNN的内部状态可以被视为使理解句子成为可能的记忆。让我们以[图3.7](#ch03fig07)中显示的RNN隐藏层中的一个神经元为例。
- en: Figure 3.7\. A recurrent neuron in the hidden layer of an RNN
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.7\. RNN隐藏层中的循环神经元
- en: '![](Images/03fig07_alt.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/03fig07_alt.jpg)'
- en: The recurrent neuron combines the signal from the input neuron (the arrow from
    the neuron on the left) *and* a signal stored internally (the looping arrow),
    which plays the role of the memory in the Yoda example. As you can see, this single
    neuron processes an input, transforming it into an output, given its internal
    state (the hidden-layer weights and activation function). It also updates its
    state as a function of the new input and its current state. This is exactly what
    the neuron needs to do to learn to relate subsequent inputs. By *relate*, I mean
    that during training, the network will learn, for example, that characters that
    form meaningful words are more likely to appear nearby.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经元结合了来自输入神经元的信号（来自左侧神经元的箭头）*和*一个内部存储的信号（循环箭头），在“Yoda”例子中，这个信号扮演了记忆的角色。正如你所看到的，这个单个神经元处理一个输入，将其转换为输出，给定其内部状态（隐藏层权重和激活函数）。它还根据新的输入和当前状态更新其状态。这正是神经元需要做的，以便学习关联后续输入。通过*关联*，我的意思是，在训练期间，网络将学习，例如，构成有意义的单词的字符更有可能在附近出现。
- en: Going back to the Yoda example, the RNN would learn that, having seen the characters
    *Y* and *o*, the most probable character to generate is *d*, because the sequence
    “Yod” has already been seen. This is a significant simplification of the learning
    dynamics of an RNN, but it gives you a basic overview.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 回到“Yoda”的例子，RNN会学习到，在看到字符*Y*和*o*之后，最可能生成的字符是*d*，因为序列“Yod”已经出现过。这是RNN学习动态的一个重大简化，但它为你提供了一个基本的概述。
- en: Cost functions
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 成本函数
- en: As in many machine learning algorithms, a neural network learns to minimize
    the errors it commits when trying to create “good” outputs from inputs. The good
    outputs you provide during training, together with the inputs, tell the network
    how much it’s wrong when it then performs a prediction. The amount of such error
    is usually measured by a *cost function* (also called a loss function). The aim
    of a learning algorithm is to optimize the algorithm parameters (in the case of
    a neural network, optimize the weights) so that the loss (or cost) is as low as
    possible.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 正如许多机器学习算法一样，神经网络学习最小化它在尝试从输入创建“好的”输出时犯的错误。你在训练期间提供的好的输出，连同输入一起，告诉网络它在进行预测时犯了多少错误。这种错误的量通常通过*成本函数*（也称为损失函数）来衡量。学习算法的目标是优化算法参数（在神经网络的情况下，优化权重），使得损失（或成本）尽可能低。
- en: 'I mentioned earlier that an RNN for text generation implicitly learns how likely
    certain sequences of text are in terms of probability. In the previous example,
    the sequence “Yoda” could have probability 0.7, whereas the sequence “ode” may
    have a probability of 0.01\. An appropriate cost function compares the probabilities
    calculated by the neural network (with its current weights) against the actual
    probabilities in the input text; for example, the sequence “Yoda” would have an
    actual probability of about 1 in the example text. This gives the amount of loss
    (the error). Several different cost functions exist, but one that intuitively
    performs this type of comparison is called a *cross-entropy cost function*; we’ll
    use it in the RNN examples. You can think of such a cost function as measuring
    how much the probabilities calculated by the neural network differ from what they
    should be with respect to a certain output. For example, if a network learning
    over the Yoda sentence says the probability of the word “Yoda” is 0.00000001,
    it’s probably going to have a high loss: the correct probability should be high,
    because “Yoda” is one of the few known good sequences in the input text.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前提到，用于文本生成的RNN隐式地学习了一定文本序列的概率可能性。在之前的例子中，“Yoda”序列可能有0.7的概率，而“ode”序列可能只有0.01的概率。一个合适的成本函数会将神经网络（使用其当前权重）计算出的概率与输入文本中的实际概率进行比较；例如，在示例文本中，“Yoda”序列的实际概率大约是1。这给出了损失（错误）的量。存在几种不同的成本函数，但一种直观地执行此类比较的成本函数被称为*交叉熵成本函数*；我们将在RNN示例中使用它。你可以将此类成本函数视为衡量神经网络计算的概率与它们相对于某个输出应该是什么之间的差异程度。例如，如果一个网络在“Yoda”句子上学习，说“Yoda”这个词的概率是0.00000001，它很可能会有一个很高的损失：正确的概率应该是高的，因为“Yoda”是输入文本中已知的好序列之一。
- en: Cost functions play a key role in machine learning, because they define the
    goal of the learning algorithm. Different cost functions are used for different
    types of problems. For example, the cross-entropy cost function is useful for
    classification tasks, whereas a *mean squared error* cost function is useful when
    a neural network needs to predict real values.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数在机器学习中扮演着关键角色，因为它们定义了学习算法的目标。不同的成本函数用于不同类型的问题。例如，交叉熵成本函数适用于分类任务，而均方误差成本函数则适用于神经网络需要预测实值时。
- en: The mathematical foundations of cost functions would probably require an entire
    chapter; because the focus of this book is the applications of deep learning for
    search, we won’t go into more detail. But I’ll suggest the right cost function
    to use, depending on the specific problem being solved, as we proceed through
    the book.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数的数学基础可能需要整整一章来阐述；因为本书的重点是深度学习在搜索中的应用，所以我们不会深入探讨。但我会根据我们在书中解决的问题，建议使用正确的成本函数。
- en: Unrolling RNNs
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 展开RNN
- en: You may have noticed that the only aesthetic difference between a feed-forward
    network and an RNN is in some looping arrows in the hidden layers. The word *recurrent*
    refers to such loops.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，前馈网络和RNN之间唯一的审美差异在于隐藏层中的一些循环箭头。单词 *recurrent* 就是指这样的循环。
- en: A better way to visualize how an RNN works in practice is to *unroll* it. Imagine
    unrolling an RNN into a set of finite connected copies of the same network. This
    is useful in practice when implementing an RNN, but it also makes it easier to
    see how RNNs naturally fit into sequence learning.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更好地可视化RNN在实际中如何工作的方法是展开它。想象一下将RNN展开成一组有限连接的相同网络副本。这在实际实现RNN时很有用，同时也使得更容易看到RNN如何自然地融入序列学习。
- en: In the Yoda example, I said that a memory of 10 characters helps you keep in
    mind the previously entered characters as you see new inputs. An RNN has this
    capability of keeping track of previous inputs (with respect to context) by means
    of recurrent neurons or layers. If you let the recurrent layer of an RNN “explode”
    into a set of 10 copies of the layer, you *unroll* the RNN by 10 (see [figure
    3.8](#ch03fig08)).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在尤达的例子中，我说过10个字符的内存可以帮助你在看到新的输入时记住之前输入的字符。RNN通过循环神经元或层具有跟踪先前输入（相对于上下文）的能力。如果你让RNN的循环层“爆炸”成10个该层的副本，你就可以通过10步（见[图3.8](#ch03fig08)）来展开RNN。
- en: Figure 3.8\. An unrolled recurrent neural network reading “my name is Yoda”
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.8\. 一个展开的循环神经网络读取“我的名字是尤达”
- en: '![](Images/03fig08_alt.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/03fig08_alt.jpg)'
- en: 'You’re feeding the sentence “my name is Yoda” to an RNN unrolled for 10 steps.
    Let’s focus on the highlighted node in [figure 3.8](#ch03fig08): you can see that
    it receives inputs from its input (the character *s*) and the previous node in
    the hidden (unrolled) layer, which in turn receives input from the character *i*
    and the previous node in the hidden layer; this goes back until the first input.
    The idea is that each node receives information about plain input (a character
    of the sequence) and, backward, from previous inputs and internal states of the
    network for such previous inputs.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在将句子“我的名字是尤达”输入到一个展开10步的RNN中。让我们关注[图3.8](#ch03fig08)中突出显示的节点：你可以看到它从其输入（字符
    *s*）和隐藏（展开）层中的前一个节点接收输入，而这个节点反过来又从字符 *i* 和隐藏层中的前一个节点接收输入；这个过程一直追溯到第一个输入。其理念是每个节点都接收有关普通输入（序列中的一个字符）的信息，以及从先前输入和网络内部状态（对于这些先前输入）的逆向信息。
- en: On the other hand, going forward, you can see that the output to the first character
    (*m*) only depends on the input and the internal state (weights) of the network;
    whereas the output to character *y* depends on the input, the current state, *and*
    the previous state as it was for the first character, *m*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，当你继续前进时，你可以看到第一个字符（*m*）的输出只依赖于输入和网络的内状态（权重）；而字符 *y* 的输出则依赖于输入、当前状态，*以及*与第一个字符
    *m* 相同的前一个状态。
- en: Thus the `unrolls` parameter is the number of steps the network can look back
    in time when generating the output for the current input. In practice, when setting
    up RNNs, you can decide how many steps you want to use to unroll the network.
    The more steps you have, the better the RNNs will be able to handle longer sequences,
    although they will also require more data and more time to train. Now you should
    have a basic idea of how an RNN handles sequences of inputs like text and keeps
    track of past sequences, when generating values in the output layers.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`unrolls`参数是网络在生成当前输入的输出时可以回溯时间的步数。在实践中，当设置RNN时，你可以决定使用多少步来展开网络。步数越多，RNN将能更好地处理更长的序列，尽管它们也需要更多的数据和更多的时间来训练。现在你应该对RNN如何处理文本等输入序列的序列，并在输出层生成值时跟踪过去序列有一个基本的了解。
- en: 'Backpropagation through time: How RNNs learn'
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 时间反向传播：RNN是如何学习的
- en: '[Chapter 2](kindle_split_013.xhtml#ch02) briefly introduced backpropagation,
    the most widely used algorithm for feed-forward neural network training. RNNs
    can be thought as feed-forward networks with an additional dimension: time. The
    effectiveness of RNNs lies in their ability to learn to correctly take into account
    information from previous input, using a learning algorithm called *backpropagation
    through time*. It’s essentially an extension of simple backpropagation where the
    number of weights to learn is much higher than in plain feed-forward neural networks,
    due to the loops in the recurrent layer, because RNNs have weights that control
    how past information flows through. We just looked at the concept of unrolling
    an RNN. Backpropagation through time (BPTT) adjusts the weights of the recurrent
    layers; so, the more unrolls you have, the more parameters must be adjusted to
    get good results. Essentially, BPTT makes the (recurrent) neural network automatically
    learn not just the weights on the connections between neurons belonging to different
    layers, but also how past information needs to be combined with the current inputs,
    via additional weights.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[第二章](kindle_split_013.xhtml#ch02)简要介绍了反向传播，这是最广泛使用的用于前馈神经网络训练的算法。可以将RNN视为具有额外维度：时间的前馈网络。RNN的有效性在于它们能够通过称为“时间反向传播”的学习算法正确地考虑先前输入的信息。它本质上是对简单反向传播的扩展，由于递归层中的循环，需要学习的权重数量比普通前馈神经网络要多得多，因为RNN有控制过去信息如何流动的权重。我们刚刚看到了展开RNN的概念。时间反向传播（BPTT）调整递归层的权重；因此，你拥有的展开越多，就必须调整更多的参数才能得到好的结果。本质上，BPTT使（递归）神经网络自动学习不仅不同层神经元之间的连接权重，而且如何通过额外的权重将过去信息与当前输入相结合。'
- en: The reasons for unrolling an RNN should now be clearer. It’s a way to limit
    the number of recursions the loop performs into a recurrent neuron or layer so
    learning and predicting are bounded and don’t recur indefinitely (which would
    make it difficult to compute the value in a recurrent neuron).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应该更清楚地了解为什么要展开RNN。这是一种限制循环在递归神经元或层中执行递归次数的方法，以便学习和预测是有界的，不会无限循环（这将使得在递归神经元中计算值变得困难）。
- en: 3.3.2\. Long-term dependencies
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2\. 长期依赖
- en: 'Let’s consider what an RNN for generating queries would look like. Imagine
    that you have two similar queries, such as “books about artificial intelligence”
    and “books about machine learning.” (This is a simple example: the two sequences
    are exactly the same length.) One of the first things to do is decide the size
    of the hidden layers and the number of unrolls. In the previous section, you learned
    that the number of unrolls controls how far the network can look back in time.
    For that to work properly, the network needs to be powerful enough, which means
    it needs more neurons in the hidden layer to correctly handle the information
    coming from the past as the number of unrolls grows. The number of neurons in
    a layer defines the maximum *power* of the network. It’s also important to note
    that if you want a network with many neurons (and layers), you’ll need to provide
    lots of data in order for the network to perform well in terms of the accuracy
    of the outputs.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一下用于生成查询的 RNN 会是什么样子。想象一下，你有两个相似的查询，例如“关于人工智能的书籍”和“关于机器学习的书籍。”（这是一个简单的例子：这两个序列的长度完全相同。）首先要做的事情之一是决定隐藏层的大小和展开的数量。在上一个章节中，你了解到展开的数量控制着网络可以回溯多远的时间。为了正常工作，网络需要足够强大，这意味着它需要在隐藏层中拥有更多的神经元来正确处理随着展开数量增长而从过去传入的信息。一个层中的神经元数量定义了网络的最高*能力*。还重要的是要注意，如果你想有一个拥有许多神经元（和层）的网络，你需要提供大量的数据，以便网络在输出准确度方面表现良好。
- en: 'The number of unrolls is related to *long-term dependency*: a scenario where
    words may have semantic correlations even though they appear further from each
    other in a sequence of text. For instance, look at the following sentence, where
    words that are distant from each other are highly correlated:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 展开的数量与*长期依赖性*相关：一个场景，其中单词可能在文本序列中相隔较远，但仍然具有语义相关性。例如，看看以下句子，其中相隔较远的单词具有高度相关性：
- en: '*In 2017, despite what happened during the 2016 Finals, Golden State Warriors
    won the championship again.*'
  id: totrans-163
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*在 2017 年，尽管 2016 年决赛期间发生了什么，金州勇士队再次赢得了冠军。*'
- en: Reading this phrase, you can easily understand that the word “championship”
    refers to the year “2017.” But a not-so-smart algorithm may link “championship”
    to “2016,” because that’s also a likely pair to generate. This algorithm would
    fail to take into account that the word “2016” refers to “Finals” in the incidental
    sentence. This is an example of a long-term dependency. Depending on the data
    you’re dealing with, you may need to take this into account to make an RNN work
    effectively.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读这个短语，你可以轻松理解“冠军”这个词指的是“2017 年”。但一个不太聪明的算法可能会将“冠军”与“2016 年”联系起来，因为这也是一个可能生成的配对。这个算法会忽略“2016
    年”这个词在附带句子中指的是“决赛”。这是一个长期依赖性的例子。根据你处理的数据，你可能需要考虑这一点，以便使 RNN 有效地工作。
- en: Using more unrolls helps mitigate long-term dependency problems, but in general
    you may never know how far apart two correlated words can be (or characters, or
    even phrases). To fix this problem, researchers came up with an improved RNN architecture
    called a *long short-term memory* (LSTM) network.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更多的展开有助于缓解长期依赖性问题，但通常你可能永远不知道两个相关单词可以相隔多远（或字符，甚至短语）。为了解决这个问题，研究人员提出了一种改进的
    RNN 架构，称为*长短期记忆*（LSTM）网络。
- en: 3.3.3\. Long short-term memory networks
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3. 长短期记忆网络
- en: So far, you’ve seen that a layer in a normal RNN is composed of a number of
    neurons with looping connections. On the other hand, an LSTM network layer is
    slightly more complex.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经看到，一个正常的 RNN 层由具有循环连接的多个神经元组成。另一方面，LSTM 网络层稍微复杂一些。
- en: 'LSTM layers can decide the following:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 层可以决定以下内容：
- en: Which information should go through the next unroll
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该通过下一个展开传递哪些信息
- en: Which information should be used to update the values of the LSTM internal state
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该使用哪些信息来更新 LSTM 内部状态
- en: Which information should be used as the next possible internal state
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该使用哪些信息作为下一个可能的内部状态
- en: Which information to output
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该输出哪些信息
- en: 'With respect to vanilla RNNs (the most basic form, as shown in the previous
    section), there are many more parameters to learn in an LSTM. It’s the equivalent
    of a sound engineer in a recording studio tweaking an equalizer (the LSTM), versus
    turning the volume knob (the RNN): an equalizer is much more complex to operate,
    but if you tune it correctly, you can get much better sound quality. The neurons
    of an LSTM layer have more weights, which are adjusted to make them learn when
    to remember information and when to forget it. This makes training LSTM networks
    more computationally expensive than training RNNs.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 与简单的 RNN（如前一小节所示的最基本形式）相比，LSTM 需要学习更多的参数。这相当于录音室中的音响工程师调整均衡器（LSTM），而不是旋转音量旋钮（RNN）：均衡器操作起来更复杂，但如果调整得当，可以得到更好的音质。LSTM
    层的神经元有更多的权重，这些权重被调整以使它们学会何时记住信息，何时忘记信息。这使得训练 LSTM 网络比训练 RNN 更为计算密集。
- en: A lighter-weight version of LSTM neurons, but still slightly more complex than
    vanilla RNN neurons, is the *gated recurrent unit* (GRU).^([[3](#ch03fn03)]) There’s
    a lot more to know about LSTMs, but the key point here is that they perform extremely
    well with long-term dependencies and therefore are a good fit for the use case
    of generating queries.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一种比标准 LSTM 神经元更轻量级的版本，但仍然比简单的 RNN 神经元稍微复杂一些，是**门控循环单元**（GRU）.^([[3](#ch03fn03)）关于
    LSTM 的知识还有很多，但关键点在于它们在处理长期依赖关系方面表现极好，因此非常适合用于生成查询的场景。
- en: ³
  id: totrans-175
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-176
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Kyunghyun Cho et al., “Learning Phrase Representations Using RNN Encoder-Decoder
    for Statistical Machine Translation” (September 3, 2014), [https://arxiv.org/abs/1406.1078v3](https://arxiv.org/abs/1406.1078v3).
  id: totrans-177
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见 Kyunghyun Cho 等人，“使用 RNN 编码器-解码器学习短语表示以进行统计机器翻译”（2014年9月3日），[https://arxiv.org/abs/1406.1078v3](https://arxiv.org/abs/1406.1078v3)。
- en: 3.4\. LSTM networks for unsupervised text generation
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 无监督文本生成的 LSTM 网络
- en: In Deeplearning4j, you can use an out-of-the-box implementation of LSTM networks.
    Let’s set up a simple neural network configuration for an RNN with one hidden
    LSTM layer. You’ll build an RNN that can sample text outputs of 50 characters.
    Although this isn’t a long sequence, it should be enough to handle short text
    queries (for example, “books about artificial intelligence” is 35 characters).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Deeplearning4j 中，你可以使用 LSTM 网络的现成实现。让我们为具有一个隐藏 LSTM 层的 RNN 设置一个简单的神经网络配置。你将构建一个可以生成
    50 个字符文本输出的 RNN。尽管这不是一个很长的序列，但它应该足以处理短文本查询（例如，“关于人工智能的书籍”有 35 个字符）。
- en: The unroll parameter should ideally be larger than the target text sample (output)
    size, so you can handle longer sequences of input. The following code will configure
    an RNN with 50 neurons in the input and output layers and 200 neurons in the hidden
    (recurrent) layer, unrolling it 10 time steps.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: unroll 参数理想情况下应该大于目标文本样本（输出）的大小，这样你就可以处理更长的输入序列。以下代码将配置一个具有 50 个输入和输出层神经元以及
    200 个隐藏（循环）层神经元的 RNN，并展开 10 个时间步。
- en: Listing 3.4\. Sample LSTM configuration
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.4\. LSTM 配置示例
- en: '[PRE8]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '***1* Number of neurons in the hidden (LSTM) layer**'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 隐藏（LSTM）层的神经元数量**'
- en: '***2* Number of neurons in the input and output layers**'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 输入层和输出层的神经元数量**'
- en: '***3* Number of unrolls for the RNN**'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* RNN 的展开次数**'
- en: '***4* Declares the LSTM layer with 50 inputs (nIn) and 200 outputs (nOut),
    using the tanh activation function**'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 声明 LSTM 层具有 50 个输入（nIn）和 200 个输出（nOut），并使用 tanh 激活函数**'
- en: '***5* Declares the output layer with 200 inputs (nIn) and 50 outputs (nOut),
    using the softmax activation function. The cost function is also declared here.**'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 声明输出层具有 200 个输入（nIn）和 50 个输出（nOut），并使用 softmax 激活函数。成本函数也在此声明。**'
- en: '***6* Declares the time dimension of the RNN (LSTM) with unrollSize as a parameter
    of the backpropagation-through-time algorithm**'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 声明 RNN（LSTM）的时间维度，将 unrollSize 作为时间反向传播算法的参数** '
- en: 'It’s important to note a few details about this architecture:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意这个架构的几个细节：
- en: You specify the loss-function parameter for the cross-entropy cost function.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你指定了交叉熵损失函数的损失函数参数。
- en: You use the tanh activation function on input and hidden layers.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在输入层和隐藏层使用 tanh 激活函数。
- en: You use a softmax activation function in the output layer.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在输出层使用 softmax 激活函数。
- en: Using the cross-entropy cost function is closely tied to the use of a softmax
    function in the output layer. A softmax function in the output layer transforms
    each of its incoming signals into an estimated probability with respect to the
    other signals, generating a *probability distribution*, where each such value
    is between 0 and 1 and the sum of all the resulting values is equal to 1.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用交叉熵损失函数与在输出层中使用softmax函数紧密相关。输出层中的softmax函数将每个输入信号转换为一个相对于其他信号的估计概率，生成一个*概率分布*，其中每个这样的值介于0和1之间，所有这些值的总和等于1。
- en: 'In the context of character-level text generation, you’ll have one neuron for
    each character in the data used to train the network. Once the softmax function
    is applied to the values generated by the hidden LSTM layer, each character will
    have an assigned probability (a number between 0 and 1). In the Yoda example,
    the data consists of 10 characters, so the output layer will contain 10 neurons.
    The softmax function makes the output layer contain a probability for each character:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在字符级文本生成的上下文中，您将为网络中使用的每个字符有一个神经元。一旦softmax函数应用于隐藏LSTM层生成的值，每个字符都将分配一个概率（一个介于0和1之间的数字）。在Yoda示例中，数据由10个字符组成，因此输出层将包含10个神经元。softmax函数使得输出层为每个字符包含一个概率：
- en: '[PRE9]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see, the most probable character comes from the neuron associated
    with the character *d* (probability = 0.408).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，最可能的字符来自与字符*d*相关的神经元（概率=0.408）。
- en: 'Let’s pass some sample text to this LSTM network and see what it learns to
    generate. Before generating text for your queries, though, let’s first try something
    simpler to understand. This will help you make sure the network is doing its job
    correctly. We’ll use some text written in natural language: specifically, pieces
    of literature taken from the Gutenberg project ([www.gutenberg.org](http://www.gutenberg.org)),
    such as “Queen. This is mere madness; And thus a while the fit will work on him.”
    You’re going to teach the RNN to (re)write Shakespearean poems and comedies (see
    [figure 3.9](#ch03fig09))!'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们向这个LSTM网络传递一些示例文本，看看它学会了生成什么。不过，在为您的查询生成文本之前，我们先尝试一些更简单的东西，以便确保网络正在正确地执行其任务。我们将使用自然语言写成的文本：具体来说，是从古腾堡计划([www.gutenberg.org](http://www.gutenberg.org))中选取的文学作品片段，例如“Queen.
    This is mere madness; And thus a while the fit will work on him.” 您将教会RNN（重新）编写莎士比亚的诗和喜剧（见图3.9）！
- en: Figure 3.9\. Generating Shakespearean text
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.9. 生成莎士比亚文本
- en: '![](Images/03fig09_alt.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/03fig09_alt.jpg)'
- en: 'This will be your first experience with an RNN, so it’s good to start with
    the simplest possible approach to train it. You’ll perform unsupervised training
    of the network by feeding it text from Shakespeare’s works, one line at a time,
    as illustrated in [figure 3.10](#ch03fig10). (The input and output layer sizes
    are set to 10 for the sake of readability). As you go through the text of Shakespeare’s
    works, you take excerpts of *unroll size* + 1 and feed them, one character at
    a time, into the input layer. The expected result in the output layer is the next
    character in the input excerpt: for example, given the sentence “work on him,”
    you’ll see the inputs receiving characters for “work on hi,” and the corresponding
    outputs “ork on him.” This way, you train the network to generate the next character,
    by also looking back at the previous 10 characters.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是您第一次使用RNN（循环神经网络）的经验，因此从最简单的方法开始训练它是很好的。您将通过逐行提供莎士比亚作品中的文本来对网络进行无监督训练，如图3.10所示。[图3.10](#ch03fig10)。（为了便于阅读，输入和输出层的大小设置为10）。当您阅读莎士比亚作品的文本时，您会提取*展开大小*
    + 1的摘录，并逐个字符地将其输入到输入层。输出层中预期的结果是输入摘录中的下一个字符：例如，给定句子“work on him”，您将看到输入接收“work
    on hi”的字符，相应的输出是“ork on him”。这样，您通过同时回顾前10个字符来训练网络生成下一个字符。
- en: Figure 3.10\. Feeding the unrolled RNN with unsupervised sequence learning
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.10. 使用无监督序列学习向展开的RNN提供数据
- en: '![](Images/03fig10_alt.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/03fig10_alt.jpg)'
- en: 'You configured the LSTM earlier; now you’ll train it by iterating over the
    character sequences from the Shakespearean texts. First, you initialize the network
    with the configuration defined earlier:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 您之前已经配置了LSTM；现在您将通过遍历莎士比亚文本中的字符序列来训练它。首先，您使用之前定义的配置初始化网络：
- en: '[PRE10]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As mentioned, you’re building an RNN that generates text sequences one character
    at a time. Therefore, you’ll use a `DataSetIterator` (the DL4J API for iterating
    over datasets) that creates character sequences: a `CharacterIterator` ([http://mng.bz/y1ZJ](http://mng.bz/y1ZJ)).
    You can skip some of the details regarding the `CharacterIterator`. You initialize
    it with'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，你正在构建一个RNN，它一次生成一个文本序列。因此，你将使用`DataSetIterator`（DL4J遍历数据集的API），它创建字符序列：一个`CharacterIterator`([http://mng.bz/y1ZJ](http://mng.bz/y1ZJ))。你可以跳过一些关于`CharacterIterator`的细节。你用以下方式初始化它：
- en: The source file that contains the text to perform unsupervised training
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含要执行无监督训练的文本的源文件
- en: The number of examples that should be fed together into the network before it
    updates its weights (called the *mini-batch* parameter)
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在网络更新其权重之前应该一起输入网络中的示例数量（称为*小批量*参数）
- en: The length of each example sequence
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个示例序列的长度
- en: 'Here’s the code to iterate over Shakespearean text characters:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这是遍历莎士比亚文本字符的代码：
- en: '[PRE11]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now you have all the pieces of the puzzle to train the network. Training a
    `MultiLayerNetwork` is done with the `fit(Dataset)` method:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经拥有了训练网络的全部拼图。使用`fit(Dataset)`方法来训练`MultiLayerNetwork`：
- en: '[PRE12]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '***1* You can set listeners to look into the training process (for example,
    to check that the loss is going down over time).**'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 你可以设置监听器来查看训练过程（例如，检查损失是否随时间下降）。**'
- en: '***2* Iterates over the dataset content**'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 遍历数据集内容**'
- en: '***3* Trains the network on each portion of the dataset**'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 在数据集的每个部分上训练网络**'
- en: 'You want to check that the value of the loss generated by the network during
    training steadily declines over time. This is useful as a sanity check: a neural
    network with appropriate settings will see this number steadily decline. The following
    log shows that over 10 iterations, the loss went from 4176 to 3490 (with some
    ups and downs in between):'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 你想检查网络在训练过程中生成的损失值是否随着时间的推移稳步下降。这很有用，可以作为合理性检查：具有适当设置的神经网络将看到这个数字稳步下降。以下日志显示，在10次迭代中，损失从4176下降到3490（中间有一些起伏）：
- en: '[PRE13]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If you plot the score and loss of more such values (for example, 100), you may
    see something like [figure 3.11](#ch03fig11).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你绘制更多这样的值（例如，100）的得分和损失，你可能看到类似[图3.11](#ch03fig11)的东西。
- en: Figure 3.11\. Plotting a loss trend
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.11\. 绘制损失趋势
- en: '![](Images/03fig11_alt.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/03fig11_alt.jpg)'
- en: 'Let’s see some sequences (of 50 characters each) generated by this RNN after
    a few minutes of learning:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个RNN在几分钟学习后生成的序列（每个序列50个字符）：
- en: '...o me a fool of s itter thou go A known that fig..'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '...o me a fool of s itter thou go A known that fig..'
- en: ..ou hepive beirel true; They truth fllowsus; and..
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ..ou hepive beirel true; They truth fllowsus; and..
- en: ..ot; suck you a lingerity again! That is abys. T...
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ..ot; 再次把你吸进永恒！这是深渊。T...
- en: ..old told thy denuless fress When now Majester s...
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ..old told thy denuless fress When now Majester s...
- en: Although you can recognize that the grammar isn’t too bad, and some portions
    may even make sense, you can clearly see that this isn’t something of good quality.
    You probably wouldn’t want to use this network to write a query in natural language
    for an end user, given its poor outcomes. A complete example of Shakespeare text
    generation with a similar LSTM (with one hidden recurrent layer) can be found
    in the DL4J examples project ([http://mng.bz/7ew9](http://mng.bz/7ew9)).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可以认识到语法并不太糟糕，一些部分甚至可能有意义，但你明显可以看出这不是高质量的东西。鉴于其糟糕的结果，你可能不会想使用这个网络为最终用户编写自然语言的查询。一个类似的LSTM（具有一个隐藏循环层）的莎士比亚文本生成完整示例可以在DL4J示例项目中找到([http://mng.bz/7ew9](http://mng.bz/7ew9))。
- en: One good thing about RNNs is that it’s been demonstrated that adding more hidden
    layers often improves the accuracy of the generated results.^([[4](#ch03fn04)])
    This means that, given enough data, increasing the number of hidden layers can
    make deeper RNNs work better. To see if this applies in this use case, let’s build
    an LSTM network with two hidden layers.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的一个好处是，已经证明添加更多的隐藏层通常会提高生成结果的准确性.^([[4](#ch03fn04)]) 这意味着，给定足够的数据，增加隐藏层的数量可以使更深的RNN工作得更好。为了看看这在这个用例中是否适用，让我们构建一个具有两个隐藏层的LSTM网络。
- en: ⁴
  id: totrans-228
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-229
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Razvan Pascanu et al., “How to Construct Deep Recurrent Neural Networks”
    (April 24, 2014), [https://arxiv.org/abs/1312.6026](https://arxiv.org/abs/1312.6026).
  id: totrans-230
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Razvan Pascanu等人，“如何构建深度循环神经网络”（2014年4月24日），[https://arxiv.org/abs/1312.6026](https://arxiv.org/abs/1312.6026)。
- en: Listing 3.5\. Configuring an LSTM with two hidden layers
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.5\. 配置具有两个隐藏层的LSTM
- en: '[PRE14]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '***1* In this new configuration, you add a second hidden LSTM layer identical
    to the first.**'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 在这个新的配置中，你添加一个与第一个相同的第二个隐藏层LSTM层。**'
- en: With this configuration, you again train the neural network using the same dataset,
    so the code for training remains the same. Note how you generate the output text
    from the trained network. Because this is an RNN, you use the DL4J API `network.rnnTimeStep(INDArray)`,
    which takes an input vector, produces an output vector using the previous RNN
    state, and then updates it. A further call to `rnnTimeStep` will use this previously
    stored internal state to produce the output.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种配置，你再次使用相同的训练数据集训练神经网络，因此训练代码保持不变。注意你如何从训练好的网络生成输出文本。因为这是一个循环神经网络（RNN），你使用DL4J
    API的`network.rnnTimeStep(INDArray)`，它接受一个输入向量，使用先前的RNN状态生成一个输出向量，然后更新它。对`rnnTimeStep`的进一步调用将使用之前存储的内部状态来生成输出。
- en: As discussed earlier, the input to this RNN is a sequence of characters, each
    of which is represented in a one-hot-encoded manner. The Shakespearean text contains
    255 distinct characters, so a character input will be represented by a size-255
    vector whose values are all set to `0` except one that has a value of `1`. Each
    position corresponds to a character, so setting the vector value at a certain
    position to `1` means that input vector represents that specific character. The
    output generated by the RNN with respect to the input will be a probability distribution,
    because you’re using the softmax activation function in the output layer. Such
    a distribution will tell you which characters are more likely to be generated
    in response to the corresponding input character (and previous inputs, as per
    information stored in the RNN layer). A probability distribution is like a mathematical
    function that can output all possible characters, but with a greater probability
    of outputting some than others. For example, in a vector generated by an RNN trained
    over the sentence “my name is Yoda,” the character *y* is more likely to be generated
    by such a distribution than the character *n* when the previous input character
    is *m* (and hence the sequence *my* is more likely than *mn*). Such a probability
    distribution is used to generate the output character.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这个RNN的输入是一个字符序列，每个字符都以one-hot编码的方式表示。莎士比亚文本包含255个不同的字符，因此一个字符输入将由一个大小为255的向量表示，其值都设置为`0`，只有一个值为`1`。每个位置对应一个字符，所以将向量值在某个位置设置为`1`意味着输入向量代表该特定字符。RNN相对于输入生成的输出将是一个概率分布，因为你输出层使用的是softmax激活函数。这样的分布将告诉你哪些字符更有可能作为对应输入字符（以及之前输入，根据RNN层中存储的信息）的响应被生成。一个概率分布就像一个数学函数，可以输出所有可能的字符，但输出某些字符的概率更高。例如，在一个由训练句子“my
    name is Yoda”的RNN生成的向量中，当先前的输入字符是*m*时，字符*y*比字符*n*更有可能被这样的分布生成（因此序列*my*比*mn*更有可能）。这样的概率分布用于生成输出字符。
- en: You first convert an initialization character sequence (for example, a user
    query) to a sequence of character vectors.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先将初始化字符序列（例如，用户查询）转换为字符向量序列。
- en: Listing 3.6\. One-hot-encoding a character sequence
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.6\. 对字符序列进行one-hot编码
- en: '[PRE15]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '***1* Creates an input vector of the required size**'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 创建一个所需大小的输入向量**'
- en: '***2* Iterates over each character in the input sequence**'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 遍历输入序列中的每个字符**'
- en: '***3* Gets the index of each character**'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 获取每个字符的索引**'
- en: '***4* Creates a one-hot-encoded vector for each character, with the value at
    position “index” set to 1**'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 为每个字符创建一个one-hot编码向量，将“索引”位置上的值设置为1**'
- en: 'For each character vector, you generate an output vector of character probabilities
    and convert it into an actual character by sampling (extracting a probable result)
    from the generated distribution:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个字符向量，你生成一个字符概率输出向量，并将其通过采样（提取一个可能的结果）转换为实际的字符：
- en: '[PRE16]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '***1* Predicts the probability distribution over the given input character
    (vector)**'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 预测给定输入字符（向量）上的概率分布**'
- en: '***2* Samples a probable character from the generated distribution**'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 从生成的分布中采样一个可能的字符**'
- en: '***3* Converts the index of the sampled character to an actual character**'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将采样的字符索引转换为实际字符**'
- en: 'In the Shakespearean text, you initialize the input sequence with a random
    character, and then the RNN generates subsequent characters. With the text-generation
    part covered, you can see that having two hidden LSTM layers gives better results:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在莎士比亚的文本中，你用一个随机字符初始化输入序列，然后RNN生成后续字符。在文本生成部分完成后，你可以看到有两个隐藏LSTM层可以得到更好的结果：
- en: '... ou for Sir Cathar Will I have in Lewfork what lies ...'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '... 你为Sir Cathar Will我在Lewfork有什么 ...'
- en: '... , like end. OTHELLO. I speak on, come go’ds, and ...'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '... ，就像结束。奥赛罗。我在说，来吧，上帝，然后 ...'
- en: '... , we have berowire to my years sword; And more ...'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '... ，我们已经在我的岁月里用剑；还有更多 ...'
- en: '... Oh! nor he did he see our strengh ...'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '... 哦！他没有看到我们的力量 ...'
- en: '... WARDEER. This graver lord. CAMILL. Would I am be ...'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '... WARDEER。这位更严肃的贵族。卡米拉。我 ...'
- en: '... WALD. Husky so shall we have said? MACBETH. She h ...'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '... WALD。哈斯基，我们就这样说了？麦克白。她 ...'
- en: As expected, the generated text looks more accurate than the text generated
    with the first LSTM, which had one hidden layer. At this point, you may be wondering
    what would happen if you added another hidden LSTM layer. Would the results be
    even better? How many hidden layers should a perfect network for this text-generation
    case have? You can easily answer the first question by trying the example with
    a network that has three LSTM hidden layers. It’s more difficult, and perhaps
    impossible, to come up with an accurate response for the second question. Finding
    the best architecture and network settings is a complex process; you’ll find more
    details about RNNs toward the end of this chapter when we talk about using them
    in production.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，生成的文本看起来比只有一个隐藏层的第一个LSTM生成的文本更准确。在这个时候，你可能想知道如果你添加另一个隐藏LSTM层会发生什么。结果会更好吗？一个完美的网络在这个文本生成案例中应该有多少隐藏层？你可以通过尝试具有三个LSTM隐藏层的网络来轻松回答第一个问题。对于第二个问题，给出一个准确的回答可能更困难，甚至不可能。找到最佳架构和网络设置是一个复杂的过程；你将在本章末尾关于在生产中使用RNN时找到更多关于RNN的细节。
- en: 'Using the same configuration as earlier, but with an additional (third) hidden
    LSTM layer, the samples look like these:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与之前相同的配置，但增加了一个（第三个）隐藏LSTM层，样本看起来像这样：
- en: '... J3K. Why, the saunt thou his died There is hast ...'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '... J3K。为什么，你走了，他的死在那里，你有 ...'
- en: '... RICHERS. Ha, she will travel, Kate. Make you about ...'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '... RICHERS。哈哈，她会旅行，凯特。让你 ...'
- en: '... or beyond There the own smag; know it is that l ...'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '... 或者更远的地方，那里是自己的smag；知道它是 ...'
- en: '... or him stepping I saw, above a world’s best fly ...'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '... 或者他跨步，我看到，在世界上最好的飞 ...'
- en: 'Given the parameters you set in the neural network (layer size, sequence size,
    unroll size, and so on), adding a fourth hidden LSTM layer wouldn’t improve the
    results. In fact, they’d be slightly worse (for example, “... CHOPY. Wencome.
    My lord ‘tM times our mabultion ...”): adding more layers means adding power but
    also complexity to the network. Training requires more and more time and data;
    sometimes it isn’t possible to generate better results just by adding another
    hidden layer. In [chapter 9](kindle_split_022.xhtml#ch09), we’ll discuss a few
    techniques for addressing this balance between the needs of computational resources
    (CPU, data, time) and result accuracy in practice.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 给定你在神经网络中设置的参数（层大小、序列大小、展开大小等），添加一个第四个隐藏LSTM层不会提高结果。事实上，它们会稍微差一些（例如，“... CHOPY。欢迎。我的主人‘tM
    times our mabultion ...”）：添加更多层意味着增加了网络的功率和复杂性。训练需要更多的时间和数据；有时仅仅通过添加另一个隐藏层并不能生成更好的结果。在第[9章](kindle_split_022.xhtml#ch09)中，我们将讨论一些在实际中解决计算资源（CPU、数据、时间）需求与结果准确度之间平衡的技术。
- en: 3.4.1\. Unsupervised query expansion
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1. 无监督查询扩展
- en: Now that you’ve seen how an RNN based on LSTMs works in the case of literary
    text, let’s assemble a network to generate alternative queries. In the literature
    example, you passed the text to the RNN (unsupervised learning) because that was
    the simplest way to understand and visualize how such a network works. Now, let’s
    look at using this same approach for query expansion. You can try it on publicly
    available resources like the web09-bst dataset ([http://boston.lti.cs.cmu.edu/Data/web08-bst/planning.html](http://boston.lti.cs.cmu.edu/Data/web08-bst/planning.html)),
    which contains queries from actual information retrieval systems. You expect that
    the RNN will learn to generate queries similar to those found in a search log,
    one per line. Consequently, the data-preparation task consists of grabbing all
    the queries from the search log and writing them in a single file, one after the
    other.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了基于LSTMs的RNN在文学文本中的工作方式，让我们组装一个网络来生成替代查询。在文献示例中，你将文本传递给RNN（无监督学习），因为这是理解并可视化此类网络工作的最简单方式。现在，让我们看看如何使用这种方法进行查询扩展。你可以在像web09-bst数据集([http://boston.lti.cs.cmu.edu/Data/web08-bst/planning.html](http://boston.lti.cs.cmu.edu/Data/web08-bst/planning.html))这样的公共资源上尝试它，该数据集包含来自实际信息检索系统的查询。你期望RNN能够学会生成与搜索日志中找到的查询相似的查询，每行一个。因此，数据准备任务包括从搜索日志中抓取所有查询并将它们写入一个单独的文件，一个接一个。
- en: 'Here’s an excerpt from the query log:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是查询日志的一个摘录：
- en: '[PRE17]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '***1* The query part consists of “artificial intelligence.”**'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 查询部分由“人工智能。”组成**'
- en: '***2* The query part consists of “books about AI.”**'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 查询部分由“关于人工智能的书籍。”组成**'
- en: 'Using only the query part of each line, you get a text file like this:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用每行的查询部分，你会得到一个像这样的文本文件：
- en: '[PRE18]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Once you have that, you can pass it to an LSTM network like that described in
    the previous section. The number of hidden layers depends on various constraints;
    two is usually a good starting value. As shown in the graph back in [figure 3.1](#ch03fig01),
    you’ll build your query-expansion algorithm in a query parser, so the user isn’t
    exposed to the alternative query generation. For the sake of this example, you’ll
    extend a Lucene `QueryParser`, whose responsibility is to build a Lucene `Query`
    from a `String` (a user-entered query, in this case).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了这个，你就可以将它传递给一个LSTM网络，就像前一个章节中描述的那样。隐藏层的数量取决于各种约束；通常两个是一个好的起始值。如图3.1中的图表所示，你将在查询解析器中构建你的查询扩展算法，这样用户就不会接触到替代查询生成。为了这个示例，你将扩展一个Lucene
    `QueryParser`，其责任是从字符串（在这种情况下是用户输入的查询）构建一个Lucene `Query`。
- en: Listing 3.7\. Lucene query parser for alternative query expansion
  id: totrans-271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.7\. Lucene查询解析器用于替代查询扩展
- en: '[PRE19]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '***1* The query parser translates a String into a parsed query to be run against
    the Lucene index.**'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 查询解析器将字符串转换为解析查询，以便在Lucene索引上运行。**'
- en: '***2* RNN used by the custom query parser to generate alternative queries**'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 自定义查询解析器使用的RNN来生成替代查询**'
- en: '***3* Initializes a Lucene Boolean query to contain the original user-entered
    query and the optional queries created by the RNN**'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 初始化一个Lucene布尔查询，包含原始用户输入的查询和RNN创建的可选查询**'
- en: '***4* Adds a mandatory clause for the user-entered query (the results for that
    query need to be shown)**'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 为用户输入的查询添加一个强制子句（需要显示该查询的结果）**'
- en: '***5* Lets the RNN generate some samples to be used as additional queries**'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 允许RNN生成一些样本作为额外的查询**'
- en: '***6* Parses text generated by the RNN and includes it as an optional clause**'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 解析RNN生成的文本，并将其作为可选子句包含在内**'
- en: '***7* Builds and returns the final query as a combination of the user-entered
    query and the RNN-generated queries**'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 构建并返回最终的查询，该查询是用户输入的查询和RNN生成的查询的组合**'
- en: '***8* This method does query encoding, RNN prediction, and output decoding
    into a new query, as in the Shakespearean example.**'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 此方法执行查询编码、RNN预测和输出解码到新的查询，就像莎士比亚示例中那样。**'
- en: You initialize the query parser with the RNN and use it to build a number of
    alternative queries that are added as optional clauses appended to the original
    query. All the magic is contained in the portion of the code that generates new
    query strings from the original one.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用RNN初始化查询解析器，并使用它构建一系列替代查询，这些查询作为可选子句附加到原始查询之后。所有魔法都包含在从原始查询生成新查询字符串的代码部分中。
- en: 'The RNN receives the user-entered query as an input and produces a new query
    as output. Remember, neural networks “talk” by means of vectors, so you need to
    transform the text query into a vector. You perform one-hot encoding of the characters
    of the user-entered query. Once the input text is converted into a vector, you
    can sample the output query one character at a time. Looking back at the Shakespearean
    example, you did the following:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: RNN接收用户输入的查询作为输入，并产生一个新的查询作为输出。记住，神经网络通过向量“说话”，因此你需要将文本查询转换为向量。你对用户输入查询的字符执行单热编码。一旦输入文本被转换为向量，你就可以逐个字符地采样输出查询。回顾莎士比亚的例子，你做了以下操作：
- en: '**1**.  Encoded the user-entered query into a series of one-hot-encoded character
    vectors'
  id: totrans-283
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**.  将用户输入的查询编码为一系列单热编码的字符向量'
- en: '**2**.  Fed this sequence to the network'
  id: totrans-284
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**.  将这个序列输入到网络中'
- en: '**3**.  Got the first output character vector, transformed it into a character,
    and fed this generated character back into the network'
  id: totrans-285
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**.  获取第一个输出字符向量，将其转换为字符，并将这个生成的字符反馈到网络中'
- en: '**4**.  Iterated the previous step until an ending character was found (such
    as the carriage return character, in this case)'
  id: totrans-286
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**4**.  重复前一步直到找到结束字符（例如，在本例中的回车字符）'
- en: Practically, this means if you feed the RNN a user-entered query that’s something
    like a common term, the RNN will probably “complete” the query by adding relevant
    terms. If you instead feed the RNN a query that looks like a finished query, the
    RNN will probably generate a query that you could find near the user-entered query
    in a search log. With all this in place, you can now generate alternative queries
    by using the following settings.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这意味着如果你向RNN输入一个类似于常见术语的用户输入查询，RNN可能会“完成”查询，添加相关术语。如果你向RNN输入一个看起来像完成查询的查询，RNN可能会生成一个你可以在搜索日志中找到的与用户输入查询相近的查询。有了所有这些，你现在可以使用以下设置生成替代查询。
- en: Listing 3.8\. Trying `AltQueriesQueryParser` using an LSTM with two hidden layers
  id: totrans-288
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.8\. 使用具有两个隐藏层的LSTM尝试`AltQueriesQueryParser`
- en: '[PRE20]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '***1* The size of LSTM layers**'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* LSTM层的大小**'
- en: '***2* Number of examples to put into a mini-batch**'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 放入迷你批次的示例数量**'
- en: '***3* Length of each input sequence to make the RNN learn to generate new ones**'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 每个输入序列的长度，以便RNN学习生成新的序列**'
- en: '***4* Unroll size (as a parameter of backpropagation through time)**'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 展开大小（作为时间反向传播的参数）**'
- en: '***5* Number of times the RNN should iterate over the same data**'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* RNN应该遍历相同数据的次数**'
- en: '***6* Number of hidden LSTM layers in the RNN network**'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* RNN网络中的隐藏LSTM层数**'
- en: '***7* Gradient descent learning rate**'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 梯度下降学习率**'
- en: '***8* Source file containing the queries**'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 包含查询的源文件**'
- en: '***9* Builds an iterator over text characters of the file containing queries**'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 构建一个遍历包含查询的文件中文本字符的迭代器**'
- en: '***10* Algorithm used to initialize the network weights**'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10* 用于初始化网络权重的算法**'
- en: '***11* Update algorithm used to update parameters while performing gradient
    descent**'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***11* 在执行梯度下降时用于更新参数的更新算法**'
- en: '***12* Activation function to be used in the hidden layers**'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***12* 在隐藏层中使用的激活函数**'
- en: '***13* Sets up a score-iteration listener that outputs the value of loss every
    10 iterations (of backpropagation through time)**'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***13* 设置一个每10次（时间反向传播）迭代输出损失值的分数迭代监听器**'
- en: '***14* Analyzer used to identify terms in the query text**'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***14* 用于识别查询文本中术语的分析器**'
- en: '***15* Instantiates the AltQueriesQueryParser**'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***15* 实例化AltQueriesQueryParser**'
- en: '***16* Creates a few sample queries**'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***16* 创建一些示例查询**'
- en: '***17* Prints the alternative queries generated by the custom parser**'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***17* 打印由自定义解析器生成的替代查询**'
- en: 'The standard output will contain the following:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 标准输出将包含以下内容：
- en: '[PRE21]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '***1* The query “latest trends” is expanded in a more specific query about
    trends about AI; this boosts AI-related results.**'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* “最新趋势”的查询被扩展为一个更具体的关于AI趋势的查询；这增加了与AI相关的结果。**'
- en: '***2* The second query looks weird, but it isn’t from the RNN perspective:
    the characters composing “covfefe” and “coffee” are almost identical and in similar
    positions.**'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 第二个查询看起来很奇怪，但从RNN的角度来看并不是：组成“covfefe”和“coffee”的字符几乎相同，并且位于相似的位置。**'
- en: '***3* The alternative query for “music events” is similar but more specific.**'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* “音乐事件”的替代查询类似但更具体。**'
- en: Note that no terms are shared between the input and output queries.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，输入和输出查询之间没有共享任何术语。
- en: 'The first alternative queries generated sound like more specific versions of
    the original, which may not be what the user wants. You can see that “latest trends”
    is in parentheses: the RNN is generating “about AI” and “about artificial intelligence”
    to sort of complete the sentence. If you ask a generic question about “latest
    trends,” the query parser will be cautious in generating more-specific versions
    of an original query, if no more context is given (in this example case, “latest
    trends” is too generic). If you don’t want alternative queries like those for
    the first query here, you can use a trick to hint to the RNN that it should try
    to generate a completely new query. The data you feed the RNN is split into sequences,
    one per line, delimited by a carriage return, so here’s the trick: add a carriage
    return character at the end of the user-entered query. The RNN is used to observing
    sequences of the form `wordA wordB wordC CR` (or, more precisely, character streams
    that often have a space character in between), where `CR` is a carriage return.
    Implicitly, the `CR` character tells the RNN that the sequence of text before
    `CR` is finished and that a new sequence of text is starting. If you take the
    user-entered query “latest trends” and let the query parser add `CR` at the end
    of it, the RNN will try to generate a new sequence starting from a carriage return
    character. This makes it much more likely that the RNN will generate text that
    sounds like a new query rather than a more specific version of the original query.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的第一个替代查询听起来像是原始查询的更具体版本，这可能不是用户想要的。你可以看到“最新趋势”被放在括号中：RNN正在生成“关于AI”和“关于人工智能”来试图完成句子。如果你对一个关于“最新趋势”的通用问题提问，如果没有提供更多上下文（在这个例子中，“最新趋势”过于通用），查询解析器在生成原始查询的更具体版本时会非常谨慎。如果你不希望第一个查询有类似的替代查询，你可以使用一个技巧来提示RNN尝试生成一个全新的查询。你提供给RNN的数据被分成序列，每行一个，由回车符分隔，所以这里有一个技巧：在用户输入的查询末尾添加一个回车符。RNN用于观察形式为`wordA
    wordB wordC CR`（或者更精确地说，字符流之间通常有一个空格字符）的序列，其中`CR`是回车符。隐含地，`CR`字符告诉RNN在`CR`之前的文本序列已经结束，并且一个新的文本序列即将开始。如果你将用户输入的查询“最新趋势”并让查询解析器在末尾添加`CR`，RNN将尝试从回车符开始生成一个新的序列。这使得RNN生成看起来像新查询的文本而不是原始查询的更具体版本的可能性大大增加。
- en: 3.5\. From unsupervised to supervised text generation
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5\. 从无监督到监督文本生成
- en: The approach that you’ve just seen for generating alternative queries is nice,
    but you want something better than nice; you’re focused on providing a tool that
    changes the lives of your end users. You want to make sure the search engine operates
    better than before, or all this effort will have been useless.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚才看到的生成替代查询的方法不错，但你想要的不仅仅是不错；你专注于提供一个能够改变最终用户生活的工具。你想要确保搜索引擎比以前运行得更好，否则所有这些努力都将毫无意义。
- en: In the query expansion use case, a key role is defined by the way the RNN learns.
    You’ve seen how the RNN performs unsupervised learning from a text file containing
    many user queries that aren’t directly related. [Section 3.1.2](#ch03lev2sec2)
    also mentioned more-complex alternatives, creating examples that had the desired
    alternative query with respect to a certain input query.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在查询扩展用例中，RNN的学习方式定义了一个关键角色。你已经看到了RNN如何从一个包含许多用户查询的文本文件中进行无监督学习，这些查询与直接相关的内容没有关系。[第3.1.2节](#ch03lev2sec2)还提到了更复杂的替代方案，创建了具有期望替代查询的示例，这些替代查询与某个输入查询相关。
- en: In this section, I’ll briefly introduce supervised text generation for search
    (for example, using search logs) with two different algorithms.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将简要介绍使用两种不同的算法进行搜索（例如，使用搜索日志）的监督文本生成。
- en: 3.5.1\. Sequence-to-sequence modeling
  id: totrans-318
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1\. 序列到序列建模
- en: You’ve learned about LSTMs and how they’re good at handling sequences. Doing
    supervised learning for the task of building alternative queries requires providing
    a desired target sequence to be generated with respect to an input sequence. In
    [section 3.1.2](#ch03lev2sec2), where we discussed data preparation, you saw that
    you can obtain training examples by deriving them from the search logs.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解了LSTM以及它们在处理序列方面的优势。对于构建替代查询的任务进行监督学习需要提供与输入序列相关的期望目标序列。在[第3.1.2节](#ch03lev2sec2)中，我们讨论了数据准备，你看到可以通过从搜索日志中推导出训练示例。
- en: So if you have pairs like “latest research in AI” → “recent publications in
    artificial intelligence,” you can use them in an RNN architecture, as shown in
    [figure 3.12](#ch03fig12).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你有像“latest research in AI”→“recent publications in artificial intelligence”这样的对，你可以在RNN架构中使用它们，如图[图3.12](#ch03fig12)所示。
- en: Figure 3.12\. Supervised sequence learning with a single LSTM
  id: totrans-321
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.12\. 使用单个LSTM的监督序列学习
- en: '![](Images/03fig12_alt.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/03fig12_alt.jpg)'
- en: With such input/output pairs, it’s much more difficult for the RNN (or LSTM)
    to learn. In the previous unsupervised approach, the network was learning to generate
    the next character in the sequence in order to teach the RNN to reproduce the
    input sequence.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这样的输入/输出对，RNN（或LSTM）学习起来要困难得多。在之前的无监督方法中，网络正在学习生成序列中的下一个字符，以便教会RNN复制输入序列。
- en: 'In a supervised learning scenario, you’re instead trying to teach the neural
    network to generate a sequence of output characters that might be completely different
    from the input characters. Let’s look at an example. If you have the input sequence
    “latest resea,” it’s easy to guess that the next character will be *r*. The RNN
    output to be learned looks one character ahead in time:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习场景中，你试图教会神经网络生成一系列输出字符，这些字符可能与输入字符完全不同。让我们看一个例子。如果你有输入序列“latest resea”，很容易猜测下一个字符将是*r*。要学习的RNN输出在时间上提前一个字符：
- en: '[PRE22]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'On the other hand, if you use the portion of the sentence “recent pub” as the
    target output, the RNN should do something like this:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你使用句子“recent pub”的部分作为目标输出，RNN应该做类似这样的事情：
- en: '[PRE23]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This task is clearly much more difficult, so I’ll now introduce a fascinating
    architecture called *sequence-to-sequence* models. This architecture uses two
    LSTM networks:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务显然要困难得多，所以我现在将介绍一个名为*序列到序列*模型的结构。这个架构使用两个LSTM网络：
- en: The *encoder* takes the input sequence as a sequence of word vectors (not characters).
    It generates an output vector called a *thought vector* that corresponds to the
    last hidden state of the LSTM, rather than generating a probability distribution
    like the previous model.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器将输入序列作为一系列词向量（而不是字符）。它生成一个称为思维向量的输出向量，对应于LSTM的最后隐藏状态，而不是像之前模型那样生成概率分布。
- en: The *decoder* takes the thought vector as an input and generates an output sequence
    that represents a probability distribution to be used to sample the output sequence.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器将思维向量作为输入并生成一个输出序列，该序列代表一个概率分布，用于采样输出序列。
- en: 'This architecture is also called *seq2seq* (see [figure 3.13](#ch03fig13)).
    We’ll inspect it in more detail in [chapter 7](kindle_split_020.xhtml#ch07), because
    it’s also used to perform machine translation (transforming one sequence written
    in a certain language into a corresponding sequence in another target language).
    Seq2seq is also often used to build conversational models for chatbots. In the
    context of search, what’s interesting is the concept of the thought vector: a
    vectorized representation of the user’s intent. There’s a lot of research in this
    area.^([[5](#ch03fn05)]) Although it’s called a thought vector, what the RNN learns
    is based on the given inputs and outputs. In this case, if the input is a query
    and the output is another query, the thought vector can be seen as the vector
    that can map the input query to the output query. If the output query is relevant
    with respect to the input query, then the thought vector encodes the information
    about how a relevant alternative query can be generated from that input query,
    a distributed representation of user intent.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构也被称为*seq2seq*（见[图3.13](#ch03fig13)）。我们将在[第7章](kindle_split_020.xhtml#ch07)中更详细地检查它，因为它也用于执行机器翻译（将一种语言写成的序列转换为另一种目标语言的对应序列）。Seq2seq也常用于构建聊天机器人的对话模型。在搜索的背景下，有趣的是思维向量的概念：它是用户意图的向量表示。在这个领域有很多研究.^([[5](#ch03fn05)])
    虽然它被称为思维向量，但RNN学习的是基于给定的输入和输出。在这种情况下，如果输入是一个查询，输出是另一个查询，那么思维向量可以看作是能够将输入查询映射到输出查询的向量。如果输出查询与输入查询相关，那么思维向量编码了有关如何从输入查询生成相关替代查询的信息，这是用户意图的分布式表示。
- en: ⁵
  id: totrans-332
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-333
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See, for example, Ryan Kiros et al., “Skip-Thought Vectors” (June 22, 2015),
    [https://arxiv.org/pdf/1506.06726v1.pdf](https://arxiv.org/pdf/1506.06726v1.pdf);
    Shuai Tang et al., “Trimming and Improving Skip-thought Vectors” (June 9, 2017),
    [https://arxiv.org/abs/1706.03148](https://arxiv.org/abs/1706.03148); and Yoshua
    Bengio, “The Consciousness Prior” (September 25, 2017), [https://arxiv.org/abs/1709.08568](https://arxiv.org/abs/1709.08568).
  id: totrans-334
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 例如，参见Ryan Kiros等人，“Skip-Thought Vectors”（2015年6月22日）[https://arxiv.org/pdf/1506.06726v1.pdf](https://arxiv.org/pdf/1506.06726v1.pdf)；Shuai
    Tang等人，“Trimming and Improving Skip-thought Vectors”（2017年6月9日）[https://arxiv.org/abs/1706.03148](https://arxiv.org/abs/1706.03148)；以及Yoshua
    Bengio，“The Consciousness Prior”（2017年9月25日）[https://arxiv.org/abs/1709.08568](https://arxiv.org/abs/1709.08568)。
- en: Figure 3.13\. Sequence-to-sequence modeling for queries
  id: totrans-335
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.13\. 查询的序列到序列建模
- en: '![](Images/03fig13_alt.jpg)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/03fig13_alt.jpg)'
- en: 'Because we’ll take a closer look at sequence-to-sequence models in [chapter
    7](kindle_split_020.xhtml#ch07), for now we’ll use a previously trained seq2seq
    model where related input and desired output queries have been extracted from
    a search log on the basis of two measures:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们将更详细地研究序列到序列模型在第7章中，所以现在我们将使用一个之前训练好的seq2seq模型，其中相关输入和期望输出查询是基于两个度量从搜索日志中提取的：
- en: How close in time they were fired, as seen in the search log
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从搜索日志中可以看到它们被触发的间隔时间
- en: Whether they share at least one search result
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是否至少共享一个搜索结果
- en: 'In DL4J, you load this previously created model from the filesystem and pass
    it to the previously defined `AltQueriesQueryParser`:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在DL4J中，你从文件系统中加载之前创建的模型，并将其传递给之前定义的`AltQueriesQueryParser`：
- en: '[PRE24]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '***1* Restores a previously persisted neural network model from a file**'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 从文件中恢复之前持久化的神经网络模型**'
- en: '***2* Builds the AltQueriesQueryParser using the neural network implementing
    the seq2seq model. Note that you no longer need the CharacterIterator.**'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 使用实现seq2seq模型的神经网络构建AltQueriesQueryParser。请注意，你不再需要CharacterIterator。**'
- en: 'In order to use the sequence-to-sequence model, you need to change the way
    you generate the sequence. In the unsupervised approach, you sampled characters
    from the output probability distributions. In this case, you’ll instead generate
    the sequence from the `decoder` LSTM at the word level. Here are some results
    given by the `AltQueryParser` using the seq2seq model:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用序列到序列模型，你需要改变生成序列的方式。在无监督方法中，你从输出概率分布中采样字符。在这种情况下，你将生成从`decoder` LSTM的词级别序列。以下是一些`AltQueryParser`使用seq2seq模型给出的结果：
- en: '[PRE25]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '***1* This result may look weird at first, but there’s actually a foundation
    for the Museum of Contemporary Art in Chicago.**'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 这个结果一开始看起来可能很奇怪，但实际上它是芝加哥当代艺术博物馆的基础。**'
- en: '***2* The input query about a music event generates a query that contains a
    city and the name of another event (although the Monmouth Festival takes place
    in Oregon).**'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 关于音乐活动的查询生成一个包含城市和另一个活动名称的查询（尽管蒙茅斯节在俄勒冈州举行）。**'
- en: '***3* A query about toys for kids generates a query for Mexican yellow shoes.
    If it’s Christmas, this is a good result (gift for kids and for ... someone who
    may like yellow shoes)!**'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 关于儿童玩具的查询生成一个关于墨西哥黄色鞋子的查询。如果是圣诞节，这是一个好结果（给孩子们的礼物，以及...可能喜欢黄色鞋子的人！）**'
- en: 3.6\. Considerations for production systems
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6\. 生产系统考虑事项
- en: 'Training RNNs was tedious, and it was even worse with LSTMs. Nowadays, we have
    frameworks like DL4J that can run on CPUs or on graphical processing units (GPUs),
    or can even run in a distributed manner (for example, via Apache Spark). Other
    frameworks like TensorFlow have dedicated hardware (tensor-processing units [TPUs]!),
    and so on. But it’s not trivial to set up an RNN to work well. You may have to
    train several different models to come up with the one that works best on your
    data. By the way, not only are there theoretical constraints around setting up
    LSTMs, but the data you use to train also defines what they can do at test time:
    for example, when using them on unseen queries.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 训练RNNs很繁琐，使用LSTMs更是如此。如今，我们有了像DL4J这样的框架，它可以在CPU或图形处理单元（GPU）上运行，甚至可以以分布式方式运行（例如，通过Apache
    Spark）。其他框架如TensorFlow有专门的硬件（张量处理单元[TPUs]！），等等。但设置一个表现良好的RNN并不简单。你可能需要训练几个不同的模型，才能找到最适合你数据的那个。顺便说一句，不仅设置LSTMs存在理论上的限制，你用来训练的数据也定义了它们在测试时的能力：例如，当使用它们处理未见过的查询时。
- en: In practice, it took several hours of trial and error to come up with good settings
    for the different parameters in the unsupervised approach. This process will take
    less time as you become more experienced with the dynamics of LSTMs (and, in general,
    of neural networks). For instance, the Shakespeare example contains sequences
    that are much longer than queries. Queries are short—on average, between 10 and
    50 characters—whereas lines from *Macbeth* can contain 300 characters. So the
    example-length parameter for the Shakespeare example (200) is longer than that
    used for learning to generate queries (50).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，为了找到无监督方法中不同参数的良好设置，需要花费数小时进行试错。随着你对LSTM（以及一般神经网络）的动态性越来越熟悉，这个过程将花费更少的时间。例如，莎士比亚示例包含比查询长得多的序列。查询很短——平均在10到50个字符之间——而《麦克白》的台词可以包含300个字符。因此，莎士比亚示例的示例长度参数（200）比用于学习生成查询的参数（50）要长。
- en: 'Also consider the hidden structures in text. Text from Shakespearean comedies
    usually has the following pattern: `CHARACTERNAME : SOMETEXT PUNCTUATION CR`,
    whereas queries are just sequences of words followed by a carriage return. Queries
    can contain both formal and informal sentences, with words like “myspaceeee” that
    can confuse the RNN. So whereas the Shakespearean text needed only one hidden
    layer to give okay results, the LSTM needed at least two hidden layers to perform
    in a useful way.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '还应考虑文本中的隐藏结构。莎士比亚喜剧的文本通常具有以下模式：`CHARACTERNAME : SOMETEXT PUNCTUATION CR`，而查询只是单词序列后跟一个换行符。查询可以包含正式和非正式句子，其中包含像“myspaceeee”这样的单词可能会混淆RNN。因此，虽然莎士比亚文本只需要一个隐藏层就能给出不错的结果，但LSTM至少需要两个隐藏层才能以有用的方式进行操作。'
- en: The decision about whether to perform unsupervised LSTM training over characters
    versus using a sequence-to-sequence model depends first on the data you have.
    If you aren’t able to generate good training examples (where the output query
    is a relevant alternative to the input query), you should probably go with the
    unsupervised approach. The architecture is also lighter, and training will likely
    take less time.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 是否在字符级别上执行无监督LSTM训练，还是使用序列到序列模型，首先取决于你拥有的数据。如果你无法生成好的训练示例（其中输出查询是输入查询的相关替代），你可能应该选择无监督方法。该架构也更轻量，训练可能需要更少的时间。
- en: A key point to take into account is that during training, the loss values should
    be tracked to make sure they’re steadily declining. You saw a graph of the loss
    generated by plotting the values outputted by the `ScoreIterationListener` while
    training the unsupervised LSTM. It’s useful to do this to make sure training is
    going well. If the loss begins to increase or stops decreasing at a value far
    from zero, you probably need to tune the network parameters.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑的一个关键点是，在训练过程中，应该跟踪损失值以确保它们稳步下降。你在训练无监督LSTM时，通过绘制`ScoreIterationListener`输出的值所生成的损失图已经看到了。这样做有助于确保训练进展顺利。如果损失开始增加或停止在远离零的值下降，你可能需要调整网络参数。
- en: 'The most important parameter is the learning rate. This value (usually between
    0 and 1) determines the speed at which the gradient descent algorithm goes downhill
    toward points where the error is low. If the learning rate is too high (closer
    to 1: for example, 0.9), it will result in the loss starting to diverge (increasing
    to infinity). If the learning rate is instead too low (closer to 0: for example,
    0.0000001), the gradient descent may take too long to reach a point with low error.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的参数是学习率。这个值（通常在0到1之间）决定了梯度下降算法向低误差点下降的速度。如果学习率太高（接近1：例如，0.9），会导致损失开始发散（增加到无穷大）。如果学习率太低（接近0：例如，0.0000001），梯度下降可能需要太长时间才能达到低误差点。
- en: Summary
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Neural networks can learn to generate text, even in the form of natural language.
    This is useful for silently generating queries that are executed together with
    user-entered queries to provide better search results.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络可以学习生成文本，甚至是以自然语言的形式。这对于无声生成与用户输入查询一起执行的查询很有用，以提供更好的搜索结果。
- en: Recurrent neural networks are helpful for the task of text generation, because
    they’re adept at handling even long sequences of text.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络对于文本生成任务很有帮助，因为它们擅长处理即使是长文本序列。
- en: Long short-term memory networks are an extension of RNNs that can deal with
    long-term dependencies. They work better than plain RNNs when dealing with natural
    language text where related concepts or words may be a significant distance apart
    in a sentence.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长短期记忆网络是RNNs的扩展，可以处理长期依赖。在处理自然语言文本时，相关概念或单词可能在句子中相隔很远，长短期记忆网络比普通的RNN表现更好。
- en: Providing deeper layers in neural networks can help in cases where the network
    requires more computational power for handling larger datasets and/or more-complex
    patterns.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在神经网络中提供更深的层可以帮助处理需要更多计算能力来处理更大的数据集和/或更复杂模式的网络。
- en: Sometimes it’s useful to look closely at how a neural network is generating
    its outputs. Small adjustments (like the `CR` trick) can make a difference in
    the quality of the results.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时候仔细观察神经网络如何生成其输出是有用的。小的调整（如`CR`技巧）可以在结果的质量上产生差异。
- en: Sequence-to-sequence models and thought vectors are powerful tools for learning
    to generate sequences of text in a supervised manner.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列到序列模型和思维向量是学习以监督方式生成文本序列的强大工具。
- en: Chapter 4\. More-sensitive query suggestions
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4章\. 更敏感的查询建议
- en: '*This chapter covers*'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Common approaches to composing query suggestions
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 撰写查询建议的常见方法
- en: Character-level neural language models
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符级神经网络语言模型
- en: Tuning parameters in neural networks
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整神经网络中的参数
- en: 'We’ve covered the fundamentals of neural networks and looked at the construction
    of both shallow and deep architectures for these networks. In practical terms,
    you now know how to integrate neural networks into a search engine to boost the
    search engine with two key features: synonym expansion and alternative query generation.
    Both of these features work on the search engine to make it smarter and return
    better results to the user. But can you do anything to improve the wording of
    the query itself? In particular, can you do anything to help the user write better
    queries—queries that deliver the results that come closest to what the user is
    looking for?'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了神经网络的基础知识，并探讨了这些网络浅层和深层架构的构建。从实际应用的角度来看，你现在知道如何将神经网络集成到搜索引擎中，以通过两个关键特性来增强搜索引擎：同义词扩展和替代查询生成。这两个特性都作用于搜索引擎，使其变得更智能，并向用户返回更好的结果。但你能否做些什么来改善查询本身的语言？特别是，你能做些什么来帮助用户写出更好的查询——即那些最接近用户所寻找结果的查询？
- en: 'The answer, of course, is yes. You’re no doubt accustomed to a search engine
    providing you with suggestions as you type in your query. This autocomplete function
    is designed to speed up the querying process by suggesting words or sentences
    that could make up a meaningful query. For instance, if a user starts typing “boo,”
    the autocomplete feature may provide the rest of the word the user is likely to
    be writing: “book,” for example, or a complete sentence that starts with “boo,”
    such as “books about deep learning.” Helping users compose their queries is likely
    to speed things up and help users avoid typos and similar errors. But this functionality
    also gives the search engine the opportunity to provide hints to help the user
    compose a better query. These hints are words or sentences that make sense in
    the context of the specific query the user is writing. The words “book” and “boomerang”
    share the same “boo” prefix, so if a user starts typing “boo,” the search engine
    might suggest that they choose either “book” or “boomerang” to complete the query.
    But if the user types “big parks where I can play boo,” it’s clear that suggesting
    “boomerang” would make more sense than suggesting “book.”'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，答案是肯定的。你无疑已经习惯了在输入查询时，搜索引擎为你提供建议。这个自动完成功能旨在通过建议可能构成有意义查询的单词或句子来加快查询过程。例如，如果用户开始输入“boo”，自动完成功能可能会提供用户可能继续输入的单词的其余部分：“book”，例如，或者以“boo”开头的完整句子，例如“关于深度学习的书籍。”帮助用户撰写他们的查询可能会加快速度，并帮助用户避免拼写错误和类似错误。但这个功能也给了搜索引擎提供提示的机会，以帮助用户撰写更好的查询。这些提示是在用户撰写的特定查询上下文中有意义的单词或句子。单词“book”和“boomerang”共享相同的“boo”前缀，所以如果用户开始输入“boo”，搜索引擎可能会建议他们选择“book”或“boomerang”来完成查询。但如果用户输入“big
    parks where I can play boo”，那么建议“boomerang”比建议“book”更有意义。
- en: By generating these hints, autocomplete also has an impact on the effectiveness
    of the search engine. Imagine if, instead of “big parks where I can play boomerang,”
    the search engine suggested “big parks where I can play book.” That would certainly
    return fewer relevant search results.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 通过生成这些提示，自动补全也对搜索引擎的有效性产生了影响。想象一下，如果搜索引擎建议的不是“我可以玩飞盘的大公园”，而是“我可以玩书的公园”，那么这将肯定返回更少的相关的搜索结果。
- en: Suggestions also give the search engine a chance to favor certain queries (and
    therefore documents to be matched) over others. This can be useful, for example,
    for marketing purposes. If the owner of the search engine of an e-commerce website
    wants to sell books more than boomerangs, they may want to suggest “big parks
    where I can play book” rather than “big parks where I can play boomerang.” If
    you know the topics that users look for most often, you may want to suggest terms
    related to those recurring topics more frequently.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 建议还给了搜索引擎一个机会，优先考虑某些查询（以及因此要匹配的文档）而不是其他查询。这可能很有用，例如，用于营销目的。如果一个电子商务网站搜索引擎的所有者想比飞盘卖更多的书，他们可能希望建议“我可以玩书的公园”而不是“我可以玩飞盘的公园”。如果您知道用户最常查找的主题，您可能希望更频繁地建议与这些重复主题相关的术语。
- en: 'Autocomplete is a common feature in search engines, so there are already plenty
    of algorithms to create it. What can neural networks help you with here? In a
    word: sensitivity. A *sensitive* suggestion is one that accurately interprets
    what the user is looking for and rewords it in a manner that will more likely
    deliver relevant results. This chapter will build on what you’ve learned about
    neural nets to get them to generate more-sensitive suggestions.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 自动补全是搜索引擎中常见的功能，因此已经有了大量的算法来创建它。神经网络在这里能帮助你什么？一句话：敏感性。一个*敏感*的建议是指准确解释用户所寻找的内容，并以一种更有可能提供相关结果的方式重新措辞。本章将基于您对神经网络的了解，使它们能够生成更敏感的建议。
- en: 4.1\. Generating query suggestions
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 生成查询建议
- en: You know from [chapter 3](kindle_split_015.xhtml#ch03) that deep neural networks
    can learn to generate text that looks like it was written by a human. You saw
    this at work when you generated alternative queries. Now you’ll see how to use
    and extend such neural nets so they can outperform the current most widely used
    algorithms for autocompletion by generating better, more-sensitive query suggestions.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 您从[第三章](kindle_split_015.xhtml#ch03)中了解到，深度神经网络可以学习生成看起来像是人类所写文本。您在生成替代查询时看到了这一点。现在您将看到如何使用和扩展这样的神经网络，以便它们可以通过生成更好、更敏感的查询建议来超越当前最广泛使用的自动补全算法。
- en: 4.1.1\. Suggesting while composing queries
  id: totrans-375
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. 在编写查询时建议
- en: In [chapter 2](kindle_split_013.xhtml#ch02), we discussed how to help users
    of a search engine look for song lyrics, in the common scenario in which the user
    doesn’t recall a song title exactly. In that context, we introduced the synonym-expansion
    technique, to allow users to fire a possibly incomplete or incorrect query (for
    example, “music is my aircraft”) that was fixed by expanding synonyms under the
    hood (“music is my aeroplane”) using the word2vec algorithm. Synonym expansion
    is a useful technique, but perhaps you could do something simpler to help a user
    recall that the song chorus is “music is my *aeroplane*” and not “music is my
    aircraft” by suggesting the right words while the user types the query. You can
    avoid letting the user run a suboptimal query, in the sense that they already
    know “aircraft” isn’t the right word.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二章](kindle_split_013.xhtml#ch02)中，我们讨论了如何帮助搜索引擎用户查找歌词，在用户无法准确回忆起歌曲标题的常见场景中。在那个背景下，我们介绍了同义词扩展技术，允许用户发出可能不完整或不正确的查询（例如，“音乐是我的飞机”），通过在幕后使用word2vec算法扩展同义词（例如，“音乐是我的飞机”）来修正查询。同义词扩展是一种有用的技术，但也许你可以做一些更简单的事情来帮助用户回忆起歌曲副歌是“音乐是我的
    *飞机*”而不是“音乐是我的飞机”，在用户输入查询时建议正确的词语。你可以避免让用户运行次优查询，因为用户已经知道“飞机”不是正确的词语。
- en: 'Having good autocompletion algorithms offers two benefits:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有良好的自动补全算法提供了两个好处：
- en: Fewer queries with few or zero results (affects recall)
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少结果很少或为零的查询（影响召回率）
- en: Fewer queries with low relevance (affects precision)
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少低相关性的查询（影响准确度）
- en: If the *suggester* algorithm is good, it won’t output nonexistent words, or
    terms that never occurred in the indexed data. This means it’s unlikely that a
    query using terms suggested by such algorithm will return no results. Let’s think
    about the “music is my aircraft” example. Provided you don’t have synonym expansion
    enabled, there’s probably no song that contains all such terms; therefore, the
    best results will contain “music” and “my,” or “my” and “aircraft,” with low relevance
    to the user’s information need (and hence a low *score*). Ideally, once the user
    enters “music is my,” the suggester algorithm will offer the hint “aeroplane,”
    because that’s a sentence the search engine has already seen (indexed).
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 如果建议算法很好，它就不会输出不存在的单词，或者从未出现在索引数据中的术语。这意味着使用此类算法建议的查询不太可能返回无结果。让我们以“音乐是我的飞机”这个例子来思考。假设您没有启用同义词扩展，可能没有包含所有这些术语的歌曲；因此，最佳结果将包含“音乐”和“我的”，或者“我的”和“飞机”，与用户的信息需求相关性低（因此得分低）。理想情况下，一旦用户输入“音乐是我的”，建议算法将提供提示“飞机”，因为这是搜索引擎已经看到的（已索引）句子。
- en: 'We just touched an important point that plays a key role in generating effective
    suggestions: where do suggestions come from? Most commonly, they originate from
    the following places:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚触及了一个在生成有效建议中扮演关键角色的重要观点：建议从哪里来？最常见的情况是，它们通常源于以下地方：
- en: Static (handcrafted) dictionaries of words or sentences to be used for suggestions
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于建议的静态（手工制作）单词或句子词典
- en: Chronology of previously entered queries (for example, taken from a query log)
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 历史查询的编年史（例如，从查询日志中获取）
- en: Indexed documents taken from various portions of the documents (title, main
    text content, authors, and so on)
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文档的各个部分（如标题、正文内容、作者等）索引的文档
- en: In the rest of this chapter, we’ll explore obtaining suggestions from these
    sources by using common techniques from the fields of information retrieval and
    natural language processing (NLP). You’ll also see how they compare with suggesters
    based on neural network language models, a longstanding NLP technique implemented
    through neural networks, in terms of features and accuracy of results.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将探讨通过使用信息检索和自然语言处理（NLP）领域的常用技术来从这些来源获取建议。您还将看到它们在特征和结果准确性方面与基于神经网络语言模型的建议者相比如何。
- en: 4.1.2\. Dictionary-based suggesters
  id: totrans-386
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 基于词典的建议者
- en: 'Back in the old days, when search engines required many handcrafted algorithms,
    a common approach was to build a dictionary of words that could be used to help
    users type queries. Such dictionaries usually contained important words only,
    such as main concepts that were closely related to that specific domain. For example,
    a search engine for a shop selling musical instruments might have used a dictionary
    containing terms like “guitar,” “bass,” “drums,” and “piano.” It would have been
    very difficult to fill the dictionary with all the relevant English words by hand-compiling
    it. Instead, it’s possible to make such dictionaries build themselves (for example,
    using a script) by looking at the query logs, getting the user-entered queries,
    and extracting a list of the 1,000 (for example) most frequently used terms. That
    way, you can avoid misspelled words in the dictionary, by means of the frequency
    threshold (hopefully, people type queries without typos most of the times). Given
    this scenario, dictionaries can still be a good resource for query history–based
    suggestions: you can use that data to suggest the same queries or portions of
    them.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在很久以前，当搜索引擎需要许多手工制作算法时，一种常见的方法是构建一个单词词典，用于帮助用户输入查询。这样的词典通常只包含重要的单词，例如与特定领域紧密相关的主要概念。例如，一个销售乐器的商店搜索引擎可能使用了一个包含诸如“吉他”、“贝斯”、“鼓”和“钢琴”等术语的词典。手动编译包含所有相关英语单词的词典将非常困难。相反，通过查看查询日志，获取用户输入的查询，并提取出（例如）1000个最常用术语的列表，可以自动构建这样的词典。这样，您可以通过频率阈值避免词典中的拼写错误（希望人们大多数时候输入查询时没有错误）。在这种场景下，词典仍然可以成为基于查询历史的建议的好资源：您可以使用这些数据来建议相同的查询或其部分。
- en: Let’s build a dictionary-based suggester using Lucene APIs, with terms from
    previous queries. Over the course of the chapter, you’ll implement this API using
    different sources and suggestion algorithms; this will help you compare them and
    evaluate which one to choose, depending on the use case.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Lucene API构建一个基于字典的建议器，使用来自先前查询的术语。在整章中，您将使用不同的来源和建议算法实现此API；这将帮助您比较它们，并根据用例评估选择哪一个。
- en: 4.2\. Lucene Lookup APIs
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2. Lucene Lookup API
- en: 'Suggestion and autocompletion features are provided by means of the `Lookup`
    API in Apache Lucene ([http://mng.bz/zM0a](http://mng.bz/zM0a)). The life cycle
    of a lookup usually includes the following phases:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Lucene（[http://mng.bz/zM0a](http://mng.bz/zM0a)）的`Lookup` API提供了建议和自动完成功能。lookup的生命周期通常包括以下阶段：
- en: '***Build—*** The lookup is built from a data source (for example, a dictionary).'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***构建—*** Lookup是从数据源（例如，字典）构建的。'
- en: '***Lookup—*** The lookup is used to provide suggestions based on a sequence
    of characters (and some other, optional, parameters).'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***查找—*** lookup用于根据字符序列（以及一些其他可选参数）提供建议。'
- en: '***Rebuild—*** The lookup is rebuilt if the data to be used for suggestion
    is updated or a new source needs to be used.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***重建—*** 如果用于建议的数据被更新或需要使用新的源，则重建lookup。'
- en: '***Store and load—*** The lookup is persisted (for example, for future reuse)
    and loaded (for example, from a previously saved lookup on disk).'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***存储和加载—*** lookup被持久化（例如，用于将来重用）并加载（例如，从磁盘上先前保存的lookup）。'
- en: 'Let’s build a lookup using a dictionary. You’ll use a file containing the 1,000
    previously entered queries as recorded in the search engine log. The queries.txt
    file looks like this, with one query per line:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用字典构建一个lookup。您将使用一个包含1,000个先前输入查询的文件，这些查询记录在搜索引擎日志中。queries.txt文件看起来像这样，每行一个查询：
- en: '[PRE26]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You can build a `Dictionary` from this plain text file and pass it to `Lookup`
    to build the dictionary-based suggester:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从这个纯文本文件中构建一个`字典`，并将其传递给`Lookup`以构建基于字典的建议器：
- en: '[PRE27]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '***1* Instantiates a Lookup**'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 实例化一个Lookup**'
- en: '***2* Locates the input file containing the queries (one per line)**'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 定位包含查询的输入文件（每行一个查询）**'
- en: '***3* Creates a plain text dictionary that reads from the queries file**'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 创建一个从查询文件读取的纯文本字典**'
- en: '***4* Builds the Lookup using the data from the Dictionary**'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 使用字典中的数据构建Lookup**'
- en: As you can see, the `Lookup` implementation called `JaspellLookup`, which is
    based on a *ternary search tree*, is fed data from a dictionary containing past
    queries. A ternary search tree (TST; [https://en.wikipedia.org/wiki/Ternary_search_tree](https://en.wikipedia.org/wiki/Ternary_search_tree))
    like that shown in [figure 4.1](#ch04fig01) is a data structure in which strings
    are stored in a way that recalls the shape of a tree. A TST is a particular type
    of tree called a *prefix tree* (or *trie*), where each node in the tree represents
    a character and has a maximum of three child nodes.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`Lookup`实现名为`JaspellLookup`，它基于一个*三叉搜索树*，并从包含过去查询的字典中获取数据。如图4.1所示的三叉搜索树（TST；[https://en.wikipedia.org/wiki/Ternary_search_tree](https://en.wikipedia.org/wiki/Ternary_search_tree)）是一种字符串以树形结构存储的数据结构。TST是一种特定的树，称为*前缀树*（或*字典树*），其中树中的每个节点代表一个字符，并且最多有三个子节点。
- en: Figure 4.1\. Ternary search tree
  id: totrans-404
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.1. 三叉搜索树
- en: '![](Images/04fig01_alt.jpg)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig01_alt.jpg)'
- en: 'Such data structures are particularly useful for autocompletion, because they’re
    efficient in terms of speed when searching for strings that have a certain prefix.
    That’s why prefix trees are often used in the context of autocompletion: as a
    user searches for “mu,” the trie can efficiently return all the strings in the
    tree that start with “mu.”'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据结构特别适用于自动完成，因为它们在搜索具有特定前缀的字符串时在速度方面效率很高。这就是为什么前缀树经常在自动完成的上下文中使用：当用户搜索“mu”时，字典树可以有效地返回树中所有以“mu”开头的字符串。
- en: 'Now that you’ve built your first suggester, let’s see it in action. You’ll
    split the query “music is my aircraft” into progressively bigger sequences and
    pass them to the lookup to get suggestions, simulating the way a user types a
    query in a search engine user interface. You’ll start with “m,” then “mu,” “mus,”
    “musi,” and so on, and see what kind of results you get based on past queries.
    To generate such *incremental inputs*, use the following code:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经构建了第一个建议者，让我们看看它的实际效果。你将查询“music is my aircraft”分割成越来越大的序列，并将它们传递给查找以获取建议，模拟用户在搜索引擎用户界面中输入查询的方式。你将从“m”开始，然后是“mu”，“mus”，“musi”，依此类推，并查看基于过去查询得到的结果。要生成这样的**增量输入**，请使用以下代码：
- en: '[PRE28]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '***1* Each step creates a substring of the original input where the ending
    index i is bigger.**'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 每一步都会创建一个原始输入的子字符串，其中结束索引i更大。**'
- en: 'Lucene’s `Lookup#lookup` API accepts a sequence of characters (the input of
    the user typing the query) and a few other parameters, such as if you only want
    more-popular suggestions (for example, strings found more frequently in the dictionary)
    and the maximum number of such suggestions to retrieve. Using the list of incremental
    inputs, you can generate the suggestions for each such substring:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: Lucene的`Lookup#lookup` API接受一个字符序列（用户输入的查询输入）和一些其他参数，例如，如果你只想获取更受欢迎的建议（例如，在字典中找到的更频繁的字符串）以及要检索此类建议的最大数量。使用增量输入列表，你可以为每个此类子字符串生成建议：
- en: '[PRE29]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '***1* Uses Lookup to obtain a maximum of two results for a given substring
    (such as “mu”), regardless of their frequency (morePopular is set to false)**'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 使用查找为给定的子字符串（如“mu”）获取最多两个结果，无论它们的频率如何（morePopular设置为false）**'
- en: 'You obtain a `List` of `LookupResult`s, each composed of a `key` that’s the
    suggested string and a `value` that’s the `weight` of that suggestion; this weight
    can be thought of as a measure of how relevant or frequent the suggester implementation
    thinks the related string is, so its value may vary depending on the lookup implementation
    used. Let’s show each suggestion result together with its weight:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 你会获得一个`LookupResult`的`List`，每个`LookupResult`由一个`key`组成，即建议的字符串，以及一个`value`，即该建议的`weight`；这个权重可以被视为建议者实现认为相关字符串的相关性或频率的度量，因此其值可能取决于所使用的查找实现。让我们一起展示每个建议结果及其权重：
- en: '[PRE30]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'If you pass all the generated substrings of “music is my aircraft” to the suggester,
    the results are as follows:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将“music is my aircraft”生成的所有子字符串传递给建议者，结果如下：
- en: '[PRE31]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '***1* No more suggestions**'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 没有更多建议**'
- en: You get no suggestions for inputs beyond “music i.” Not too good. The reason
    is that you’ve built a lookup based solely on entire query strings; you didn’t
    provide a means for the suggester to split such lines into smaller text units.
    The lookup wasn’t able to suggest “is” after “music” because no previously entered
    query started with “music is.” That’s a significant limitation. On the other hand,
    this kind of suggestion is handy for chronological autocompletion, where a user
    sees queries they entered in the past as soon as they begin typing a new query.
    For example, if a user ran a query that was the same as one they ran a week before,
    it would appear as a suggestion if the implementation used a dictionary of previously
    entered queries.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“music i.”之后的输入，你不会得到任何建议。这不太好。原因是，你基于整个查询字符串构建了一个查找；你没有提供让建议者将这些行分割成更小文本单元的手段。查找无法在“music”之后建议“is”，因为没有之前输入的查询以“music
    is”开头。这是一个重大的限制。另一方面，这种建议对于按时间顺序自动完成很有用，用户在开始输入新查询时就能看到他们之前输入的查询。例如，如果用户运行了一个与一周前运行相同的查询，那么如果实现使用之前输入查询的字典，它就会作为一个建议出现。
- en: 'But you want to do more:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 但你想要做更多：
- en: Suggest not just entire strings that the user typed in the past, but also the
    words that composed past queries (for example, “music,” “is,” “my,” and “aircraft”).
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不仅建议用户过去输入的整个字符串，还建议组成过去查询的单词（例如，“music”，“is”，“my”和“aircraft”）。
- en: Suggest query strings even if the user types a word that’s in the middle of
    a previously entered query. For example, the previous method gives results if
    the query string *starts* with what that user is typing, but you’d like to suggest
    “music is my aircraft” even if the user types “my a.”
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使用户输入的单词是之前输入查询的中间部分，也要建议查询字符串。例如，之前的方法如果查询字符串**以**用户输入的内容开头，则会给出结果，但如果你希望即使在用户输入“my
    a”的情况下也能建议“music is my aircraft”。
- en: Suggest word sequences that are grammatically and semantically correct, but
    may not have been previously typed by any user.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建议语法和语义正确但可能尚未被任何用户输入的单词序列。
- en: 'The suggestion functionality should be able to compose natural language to
    help users write better-sounding queries:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 建议功能应该能够组合自然语言来帮助用户写出听起来更好的查询：
- en: Make suggestions that reflect the data from the search engine. It would be extremely
    frustrating for a user if a suggestion led to an empty list of results.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供反映搜索引擎数据的建议。如果建议导致一个空的结果列表，这对用户来说将是非常令人沮丧的。
- en: Help users disambiguate when a query may have different scopes among the possible
    interpretations.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当一个查询可能有不同的范围时，帮助用户消除歧义。
- en: Imagine a query like “neuron connectivity,” which could relate to the field
    of neuroscience as well as to artificial neural networks. It would be helpful
    to give the user a hint that such a query might hit very different domains, and
    let them filter the results before firing the query.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个查询如“神经元连接”，这可能涉及神经科学领域，也可能涉及人工神经网络。向用户提供这样的提示是有帮助的，即这样的查询可能会触及非常不同的领域，并让他们在执行查询之前过滤结果。
- en: In the following sections, we’ll examine each of these points and see how using
    neural networks allows you to achieve more accurate suggestions when compared
    to other techniques.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将检查这些点，并看看使用神经网络如何使你比其他技术获得更准确的建议。
- en: 4.3\. Analyzed suggesters
  id: totrans-428
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 分析过的建议器
- en: 'Think about typing a query in a web search engine. In many cases, you don’t
    know the entire query you’re going to write. This wasn’t true years ago, when
    most web search was based on keywords and people had to think in advance: “What
    are the most important words I have to look for in order to get search results
    that are relevant?” That approach involved a lot more trial and error than searching
    does now. Today, good web search engines give useful hints while you’re typing
    a query; so you type, look at the suggestions, select one, begin typing again,
    and look for additional suggestions, select another one, and so on.'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下在网页搜索引擎中输入查询的过程。在许多情况下，你可能不知道将要输入的完整查询。这在几年前并不成立，因为那时大多数网页搜索都是基于关键词，人们必须提前思考：“为了获得相关的搜索结果，我必须查找哪些最重要的单词？”这种方法比现在的搜索涉及更多的试错。如今，好的网页搜索引擎在你输入查询时会提供有用的提示；因此，你输入，查看建议，选择一个，再次开始输入，并寻找更多建议，选择另一个，如此循环。
- en: 'Let’s run a simple experiment and see what suggestions I got when I searched
    for “books about search and deep learning” on Google. When I typed “book,” the
    results were generic, as shown in [figure 4.2](#ch04fig02) (as you might expect,
    because “book” can have a lot of different meanings in various contexts). One
    of the suggestions was about bookings for going on vacation in Italy (Roma, Ischia,
    Sardegna, Firenze, Ponza). At this stage, the suggestions weren’t much different
    than what you created using the dictionary-based suggester with Lucene in the
    previous section: all the suggestions started with “book.”'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行一个简单的实验，看看当我在 Google 上搜索“关于搜索和深度学习的书籍”时，我得到了哪些建议。当我输入“book”时，结果很一般，如[图
    4.2](#ch04fig02) 所示（正如你所预期的那样，因为“book”在不同的语境中可能有多种不同的含义）。其中一条建议是关于在意大利度假的预订（罗马，伊斯基亚，撒丁岛，佛罗伦萨，蓬察）。在这个阶段，建议与你在上一节中使用
    Lucene 的基于字典的建议器创建的建议没有太大区别：所有建议都以“book”开头。
- en: Figure 4.2\. Suggestions for “book”
  id: totrans-431
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.2\. “book” 的建议
- en: '![](Images/04fig02_alt.jpg)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig02_alt.jpg)'
- en: 'I didn’t select any of the suggestions, because none of them were relevant
    to my search intent. So I continued typing: “books about sear” (see [figure 4.3](#ch04fig03)).'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有选择任何建议，因为它们都不符合我的搜索意图。所以我继续输入：“关于搜索的书籍”（见[图 4.3](#ch04fig03))。
- en: Figure 4.3\. Suggestions for “books about sear”
  id: totrans-434
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.3\. “关于搜索的书籍”的建议
- en: '![](Images/04fig03_alt.jpg)'
  id: totrans-435
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig03_alt.jpg)'
- en: 'The suggestions became more meaningful and closer to my search intent, although
    the first results weren’t relevant (books about search engine optimization, books
    about searching for identity, books about search and rescue). The fifth suggestion
    was probably the closest. It’s interesting to note that I also got the following:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然最初的结果并不相关（关于搜索引擎优化的书籍，关于寻找身份的书籍，关于搜索和救援的书籍），但建议变得更加有意义，更接近我的搜索意图。有趣的是，我还得到了以下内容：
- en: An *infix* suggestion (a suggestion string containing new tokens placed infix—between
    two existing tokens of the original string). In the “books about google search”
    suggestion, the word “google” is between “about” and “sear” in the query I typed.
    Keep this in mind, because this is something you’ll want to achieve later; but
    we’ll skip it for now.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中缀建议（一个包含新标记的中缀建议字符串——位于原始字符串的两个现有标记之间）。在“关于谷歌搜索引擎”的建议中，单词“谷歌”位于我输入的查询中的“关于”和“搜”之间。请记住这一点，因为这是你以后想要实现的事情；但我们现在先跳过它。
- en: A suggestion that skipped the word “about” (the last three, “books search...”).
    Keep this in mind also; you can discard terms from the query while giving suggestions.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跳过单词“关于”的建议（最后三个，“书籍搜索...”）。也要记住这一点；在给出建议时，你可以从查询中删除术语。
- en: 'I selected the “books about search engines” suggestion, typed “and,” and got
    the results shown in [figure 4.4](#ch04fig04). Looking at the results, you probably
    realize that the topic of integration of search engines and deep learning doesn’t
    have much book coverage: none of the suggestions hints “deep learning.” A more
    important thing to note is that the suggester seems to have discarded some of
    the query text when giving me hints; in the suggestions box, the results all start
    with “engine and.” But this might be a user interface issue, because the suggestions
    seem accurate; they’re not about engines in general, but rather clearly reflect
    that *engine* refers to a search engine. Here’s another idea to keep in mind for
    later: you may want to discard some of the query text as is becomes longer.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择了“关于搜索引擎的书籍”的建议，输入了“和”，并得到了如图4.4所示的搜索结果。查看结果，你可能意识到搜索引擎与深度学习整合的主题在书籍中覆盖不多：没有任何建议暗示“深度学习”。更重要的是要注意的是，建议者似乎在给我提示时丢弃了一些查询文本；在建议框中，所有结果都以“engine
    and”开头。但这可能是一个用户界面问题，因为建议似乎是准确的；它们不是关于通用引擎，而是清楚地反映了*engine*指的是搜索引擎。这里还有一个要记住的想法：你可能想要在查询变得过长时丢弃一些查询文本。
- en: Figure 4.4\. Suggestions for “books about search engines and”
  id: totrans-440
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.4\. “关于搜索引擎的书”的建议
- en: '![](Images/04fig04_alt.jpg)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/04fig04_alt.jpg)'
- en: 'I kept trying. The final suggestion, shown in [figure 4.5](#ch04fig05), was
    the query I intended to type initially, with a small modification: I planned to
    type “books about search and deep learning,” and the suggestion was “books about
    search engines and deep learning.”'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 我一直在尝试。最终的提示，如图4.5所示，是我最初打算输入的查询，经过微小修改：我打算输入“关于搜索和深度学习的书籍”，而建议是“关于搜索引擎和深度学习的书籍”。
- en: Figure 4.5\. Suggestions for “books about search engines and dee”
  id: totrans-443
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.5\. “关于搜索引擎和深度学习”的建议
- en: '![](Images/04fig05_alt.jpg)'
  id: totrans-444
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/04fig05_alt.jpg)'
- en: 'This experiment wasn’t intended to demonstrate how the Google search engine
    implements autocompletion. Rather, we wanted to observe some of the possibilities
    when working with autocompletion:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实验的目的不是展示谷歌搜索引擎如何实现自动补全。相反，我们想观察在使用自动补全时的一些可能性：
- en: Suggestions of single words (“books”)
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词建议（“书籍”）
- en: Suggestions of multiple words (“search engines”)
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多词建议（“搜索引擎”）
- en: Suggestions of whole phrases
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个短语的建议
- en: This will help you reason and decide what’s useful in practice for your own
    search engine applications.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 这将帮助你推理并决定对你自己的搜索引擎应用在实践中有用的内容。
- en: 'Beyond the granularity of the suggestions (single word, multiword, sentence,
    and so on), we observed that some suggestions had these characteristics:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 除了建议的粒度（单词、多词、句子等）之外，我们还观察到一些建议具有以下特征：
- en: Words removed from the query (“books search engines”)
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从查询中删除的单词（“书籍搜索引擎”）
- en: Infix suggestions (“books about google search”)
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中缀建议（“关于谷歌搜索引擎”）
- en: Prefix removed (“books about” wasn’t part of the final suggestions)
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前缀被移除（“关于书籍”不是最终建议的一部分）
- en: 'All this, and much more, is possible by applying text analysis to the incoming
    query and the data from the dictionary you use to build a suggester. You can,
    for example, remove certain terms by using a stopword filter. Or you can break
    long queries into multiple subsequences, and generate suggestions for each subsequence
    by using a filter that breaks a text stream at a certain length. This fits nicely
    with the fact that text analysis is heavily used within search engines. Lucene
    has such a lookup implementation, called `AnalyzingSuggester`. Instead of relying
    on a fixed data structure, it uses text analysis to let you define how text should
    be manipulated, first when building the lookup and again when passing a piece
    of text to the lookup to get suggestions:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对传入的查询和用于构建建议器的字典中的数据进行文本分析，可以实现所有这些，以及更多。例如，您可以通过使用停用词过滤器来删除某些术语。或者，您可以将长查询分解成多个子序列，并使用一个在特定长度处断开文本流的过滤器为每个子序列生成建议。这与文本分析在搜索引擎中被广泛使用的事实相吻合。Lucene有一个名为`AnalyzingSuggester`的查找实现，它不是依赖于固定的数据结构，而是使用文本分析来让您定义文本应该如何被操作，首先是在构建查找时，再次是在将文本传递给查找以获取建议时：
- en: '[PRE32]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '***1* When you build the lookup, you use a StandardAnalyzer that removes stopwords
    and splits tokens on whitespace.**'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 当您构建查找时，您使用一个StandardAnalyzer，它会删除停用词并在空白处拆分标记。**'
- en: '***2* When you look for suggestions, you use the same analyzer used at build
    time.**'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 当您查找建议时，您使用与构建时相同的分析器。**'
- en: '***3* You need to provide a Directory on the filesystem because the AnalyzingSuggester
    uses it internally to create the required data structures for generating suggestions.**'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 您需要在文件系统中提供一个目录，因为AnalyzingSuggester内部使用它来创建生成建议所需的数据结构。**'
- en: '***4* Creates an AnalyzingSuggester instance**'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 创建一个AnalyzingSuggester实例**'
- en: The `AnalyzingSuggester` can be created using separate `Analyzer`s for build
    and lookup times; this allows you to be creative when setting up the suggester.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '`AnalyzingSuggester`可以使用用于构建和查找时间的单独`Analyzer`来创建；这允许您在设置建议器时具有创造性。'
- en: 'Internally, this lookup implementation uses a *finite state transducer*: a
    data structure used in several places in Lucene. You can think of a finite state
    transducer (FST) as a graph in which each edge is associated with a character
    and, optionally, a weight (see [figure 4.6](#ch04fig06)). At build time, all possible
    suggestions that come from applying the build-time analyzer to the dictionary
    entries are compiled into a big FST. At query time, traversing the FST with the
    (analyzed) input query will produce all the possible paths and, consequently,
    suggestion strings to output:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，这个查找实现使用了一个*有限状态转换器*：Lucene中多处使用的数据结构。您可以将有限状态转换器（FST）想象成一个图，其中每个边都与一个字符相关联，并且可选地与一个权重相关联（参见[图4.6](#ch04fig06)）。在构建时，将构建时分析器应用于字典条目所产生的一切可能建议都被编译成一个大的FST。在查询时，使用（分析过的）输入查询遍历FST将产生所有可能的路径，从而产生输出建议字符串：
- en: '[PRE33]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '***1* The dictionary-based suggester wasn’t able to provide suggestions past
    this point.**'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 基于字典的suggester无法提供此点之后的建议。**'
- en: '***2* No more suggestions**'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 没有更多建议**'
- en: Figure 4.6\. A finite state transducer
  id: totrans-465
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.6\. 一个有限状态转换器
- en: '![](Images/04fig06_alt.jpg)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/04fig06_alt.jpg)'
- en: The earlier ternary search tree–based suggester stopped providing suggestions
    beyond “music i,” because no entry in the dictionary started with “music is.”
    But this analyzed suggester, even though the dictionary is the same, is able to
    provide more suggestions.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的基于ternary搜索树的建议器在“music i”之后停止提供建议，因为字典中没有条目以“music is”开头。但这个分析建议器，尽管字典相同，仍然能够提供更多的建议。
- en: 'In the case of “music is,” the token “music” matches some suggestions, and
    therefore the related results are provided, even though “is” doesn’t give any
    suggestions. Even more interestingly, when the query becomes “music is my,” some
    suggestions contain both “music” and “my.” But at a certain point, where there
    are too many tokens that don’t match (starting with “music is my ai”), the lookup
    stops providing suggestions because they might be too poorly related to the given
    query. This is a definite improvement on the previous implementation and solves
    one of the issues: you can provide suggestions based on single tokens, not just
    on entire strings.'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在“music is”的情况下，“music”这个标记与一些建议相匹配，因此提供了相关结果，尽管“is”没有提供任何建议。更有趣的是，当查询变为“music
    is my”时，一些建议同时包含“music”和“my”。但是，在某个点上，当有太多不匹配的标记（从“music is my ai”开始）时，查找停止提供建议，因为这些可能与给定查询的关系太差。这是对之前实现的一个明显改进，解决了一个问题：你可以根据单个标记提供建议，而不仅仅是整个字符串。
- en: 'You can also enhance things by using a slightly modified version of `AnalyzingSuggester`
    that works better with infix suggestions:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过使用稍微修改过的`AnalyzingSuggester`版本来增强这些功能，它更适合中缀建议：
- en: '[PRE34]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'By using this infix suggester, you get fancier results:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这个中缀建议器，你可以得到更复杂的结果：
- en: '[PRE35]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'You don’t get results starting with “m,” “mu,” or “mus”; instead, such sequences
    are used to match the most important part of a string, like “2007 s550 mercedes,”
    “2007 qualifying times for the boston marathon,” “2007 nissan murano,” and “2007
    mustang rims com.” Another noticeable difference is that token matching can happen
    in the middle of a suggestion (that’s why it’s called *infix*):'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 你不会得到以“m”、“mu”或“mus”开头的查询结果；相反，这些序列用于匹配字符串的最重要部分，例如“2007 s550 mercedes”、“2007
    qualifying times for the boston marathon”、“2007 nissan murano”和“2007 mustang rims
    com”。另一个明显的区别是，标记匹配可以在建议的中间发生（这就是为什么它被称为*中缀*）：
- en: '[PRE36]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: With the `AnalyzingInfixSuggester`, you get infix suggestions. It takes the
    input sequence, analyzes it so that tokens are created, and then suggests matches
    based on prefix matches of any such tokens. But you still have the problems of
    making suggestions closer to the data stored in the search engine, making suggestions
    look more like natural language, and being able to better disambiguate when two
    words have different meanings. Additionally, you aren’t getting any suggestions
    when you begin typing “aircraft,” as not enough tokens match.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`AnalyzingInfixSuggester`，你可以得到中缀建议。它接受输入序列，分析它以创建标记，然后基于这些标记的前缀匹配提供建议。但你仍然面临使建议更接近搜索引擎中存储的数据、使建议看起来更像自然语言以及更好地区分两个单词具有不同意义的问题。此外，当你开始输入“aircraft”时，你不会得到任何建议，因为匹配的标记不足。
- en: Now that you have some experience with the problem of providing good suggestions,
    we’ll discuss language models. First we’ll explore models implemented through
    natural language processing (*ngrams*), and then we’ll look at those implemented
    via neural networks (neural language models).
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你对提供良好建议的问题有一些经验了，我们将讨论语言模型。首先，我们将探索通过自然语言处理（*ngrams*）实现的模型，然后我们将查看通过神经网络（神经语言模型）实现的模型。
- en: 4.4\. Using language models
  id: totrans-477
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4. 使用语言模型
- en: 'In the suggestions shown in the previous sections, some text sequences made
    little sense: for example, “music by the the.” You fed the suggester data from
    previously entered queries, so in some entry a user must have made a mistake by
    typing “the” twice. Beyond that, you’ve provided suggestions consisting of the
    entire query. Although this is fine if you want to use autocompletion to return
    the entire text of previous queries (this might be useful if you were searching
    for a book in an online bookstore), it doesn’t work well for composing new queries.'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几节中显示的建议中，一些文本序列几乎毫无意义：例如，“the the 的音乐。”你向建议器提供了之前输入的查询数据，因此在某些输入中，用户可能由于两次输入“the”而犯了一个错误。除此之外，你还提供了包含整个查询的建议。虽然如果你想要使用自动补全来返回之前查询的整个文本（如果你在在线书店中搜索一本书可能很有用），这是可以的，但它对于编写新查询并不适用。
- en: 'In medium to large search engines, the search logs contain a huge number of
    diverse queries—coming up with a good suggester algorithm is difficult because
    of the number and diversity of such text sequences. For example, if you look at
    the web09-bst dataset ([http://boston.lti.cs.cmu.edu/Data/web08-bst/planning.html](http://boston.lti.cs.cmu.edu/Data/web08-bst/planning.html)),
    you’ll find queries such as “hobbs police department,” “ipod file sharing,” and
    “liz taylor’s biography.” Such queries look good and can be used as sources for
    the suggester algorithm. On the other hand, you can also find queries like “hhhhh,”
    “hqwebdev,” and “hhht hootdithuinshithins.” You probably don’t want the suggester
    to provide similar suggestions! The problem isn’t filtering out “hhhh,” which
    can be cleared out of the dataset by removing all lines or words that contain
    three or more consecutive characters that are the same. Filtering out “hqwebdev”
    is much harder: it contains the word “webdev” (shortened version of “web developer”),
    prefixed by “hq.” Such a query might make sense (for example, there’s a website
    with this name), but you don’t want to use over-specific suggestions for a general-purpose
    suggester service. The challenge is to work with diverse text sequences, some
    of which may not make sense to use because they’re too specific and therefore
    rare. One way to address this is to use *language models*.'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 在中到大型的搜索引擎中，搜索日志包含大量多样化的查询——由于这些文本序列的数量和多样性，提出一个好的建议算法是困难的。例如，如果你查看web09-bst数据集（[http://boston.lti.cs.cmu.edu/Data/web08-bst/planning.html](http://boston.lti.cs.cmu.edu/Data/web08-bst/planning.html)），你会找到诸如“hobbs
    police department”、“ipod file sharing”和“liz taylor’s biography”这样的查询。这样的查询看起来不错，可以用作建议算法的来源。另一方面，你还可以找到诸如“hhhhh”、“hqwebdev”和“hhht
    hootdithuinshithins”这样的查询。你可能不希望建议器提供类似的建议！问题不是过滤掉“hhhh”，可以通过删除包含三个或更多连续相同字符的所有行或单词来从数据集中清除。过滤掉“hqwebdev”要困难得多：它包含单词“webdev”（“web
    developer”的缩写），前面有“hq”前缀。这样的查询可能有意义（例如，有一个网站使用这个名称），但你不想为通用建议器服务使用过于具体的建议。挑战在于处理多样化的文本序列，其中一些可能因为过于具体而罕见，不适合使用。解决这一问题的方法之一是使用*语言模型*。
- en: '|  |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Language models**'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '**语言模型**'
- en: In the NLP field, a language model’s main task is to predict the probability
    of a certain sequence of text. Probability is a measure of how likely a certain
    event is, and it ranges between 0 and 1\. So if you take the weird query we saw
    earlier—”music by the the”—and pass it to a language model, you’ll get a low probability
    (for example, 0.05). Language models represent a probability distribution and
    therefore can help predict the likelihood of a certain word or character sequence
    in a certain context. Language models can help with excluding sequences that are
    unlikely (low probability) and with generating previously unseen word sequences,
    because they capture which sequences are most likely (even though they may not
    appear in the text).
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）领域，语言模型的主要任务是预测一定序列文本的概率。概率是衡量某个事件发生可能性的度量，其范围在0到1之间。所以，如果你将我们之前看到的奇怪查询——“the
    the music”——传递给一个语言模型，你将得到一个低概率（例如，0.05）。语言模型表示一个概率分布，因此可以帮助预测在特定语境下某个单词或字符序列的可能性。语言模型可以帮助排除不太可能（低概率）的序列，并生成之前未见过的单词序列，因为它们捕捉到了最可能的序列（即使这些序列可能没有出现在文本中）。
- en: '|  |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Language models are often implemented by calculating the probabilities of ngrams.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型通常通过计算n-gram的概率来实现。
- en: '|  |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Ngrams**'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '**N-gram**'
- en: An *ngram* is a sequence of characters made up of *n* consecutive *units*, where
    a unit can be a character (“a,” “b,” “c,” ...) or a word (“music,” “is,” “my,”
    ...). Imagine an ngram language model (using words as units) where *n* = 2\. An
    ngram with *n* = 2 is also known as a *bigram*; an ngram with *n* = 3 is also
    known as a *trigram*. A bigram language model can estimate the probability of
    pairs of words like “music concert” or “music sofa.” A good language model would
    assign a probability to the bigram “music concert” that’s higher with respect
    to the probability of the “music sofa” bigram.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*n-gram*是由*n*个连续*单元*组成的字符序列，其中单元可以是字符（“a”，“b”，“c”，...）或单词（“music”，“is”，“my”，...）。想象一个使用单词作为单元的n-gram语言模型（其中*n*
    = 2）。*n* = 2的n-gram也称为*bigram*；*n* = 3的n-gram也称为*trigram*。一个bigram语言模型可以估计“music
    concert”或“music sofa”这样的单词对的概率。一个好的语言模型会给“music concert”这个bigram分配一个相对于“music
    sofa”bigram更高的概率。
- en: '|  |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'As an implementation note, the probability of (a sequence of) ngrams for a
    language model can be calculated in a number of ways. Most of them rely on the
    *Markov assumption* that the probability of some future event (for example, the
    next character or word) depends only on a limited history of preceding events
    (characters or words). So if you use an ngram model with *n* = 2, also called
    a *bigram model*, the probability of the *next* word, given a *current* word,
    is given by counting the number of occurrences of the two words “music is” and
    dividing that result by the number of occurrences of the current word (“music”)
    alone. For example, the probability that the next word is “is,” given the current
    word “music,” can be written as P(*is*|*music*). To calculate the probability
    of a word given a sequence of words greater than two—for example, the probability
    of “aeroplane” given “music is my,” you split that sentence into bigrams, calculate
    the probabilities of all such bigrams, and multiply them:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 作为实现说明，语言模型中（一系列）n-gram 的概率可以通过多种方式计算。其中大多数依赖于 *马尔可夫假设*，即某些未来事件（例如，下一个字符或单词）的概率仅依赖于先前事件（字符或单词）的有限历史。因此，如果你使用
    *n* = 2 的 n-gram 模型，也称为 *二元模型*，给定一个 *当前* 单词的 *下一个* 单词的概率是通过计算两个单词“music is”的出现次数，并将该结果除以当前单词（“music”）单独的出现次数来得到的。例如，给定当前单词“music”，下一个单词是“is”的概率可以表示为
    P(*is*|*music*)。要计算给定超过两个单词的序列（例如，给定“music is my”的“aeroplane”）的单词概率，你可以将那个句子分解成二元组，计算所有这样的二元组的概率，并将它们相乘：
- en: P(*music is my aeroplane*) = P(*is*|*music*) * P(*my*|*is*) * P(*aeroplane*|*my*)
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(*music is my aeroplane*) = P(*is*|*music*) * P(*my*|*is*) * P(*aeroplane*|*my*)
- en: 'For reference, many ngram language models use a slightly more advanced method
    called *stupid backoff*^([[1](#ch04fn01)]) that tries first to calculate the probability
    of ngrams with a higher *n* (for example, *n* = 3) and then recursively falls
    back to smaller ngrams (such as *n* = 2) if an ngram with the current *n* doesn’t
    exist in the data. Such fallback probabilities are discounted so that probabilities
    from bigger ngrams have more positive influence on the overall probability measure.
    Lucene has an ngram-based language model lookup called `FreeTextSuggester` (using
    the stupid backoff algorithm) that uses an analyzer to decide how to split ngrams:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 作为参考，许多 n-gram 语言模型使用一种稍微先进的方法，称为 *stupid backoff*^([[1](#ch04fn01)]），它首先尝试计算更高
    *n*（例如，*n* = 3）的 n-gram 的概率，然后如果当前 *n* 的 n-gram 在数据中不存在，就递归地回退到较小的 n-gram（例如，*n*
    = 2）。这种回退概率会被折算，以便更大的 n-gram 的概率对整体概率度量有更多的积极影响。Lucene 有一个基于 n-gram 的语言模型查找，称为
    `FreeTextSuggester`（使用愚蠢回退算法），它使用分析器来决定如何分割 n-gram：
- en: ¹
  id: totrans-492
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-493
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See section 4 of Thorsten Brants et al., “Large Language Models in Machine Translation,”
    [http://www.aclweb.org/anthology/D07-1090.pdf](http://www.aclweb.org/anthology/D07-1090.pdf).
  id: totrans-494
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅 Thorsten Brants 等人撰写的第 4 节，“机器翻译中的大型语言模型”，[http://www.aclweb.org/anthology/D07-1090.pdf](http://www.aclweb.org/anthology/D07-1090.pdf)。
- en: '[PRE37]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let’s see it in action, with *n* set to 2, on the query “music is my aircraft”:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将 *n* 设置为 2，在查询“music is my aircraft”中看看它的实际应用效果：
- en: '[PRE38]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '***1* One of the suggestions for “music is m” matched the desired query (“is
    my”) one character in advance.**'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 对于“music is m”的建议中有一个与期望查询（“is my”）提前一个字符匹配。**'
- en: '***2* The suggestions for “music is my” (“my space,” “my life”) aren’t what
    you’re looking for, but they sound good.**'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 对于“music is my”的建议（“my space”，“my life”）并不是你想要的，但听起来不错。**'
- en: '***3* The suggestions for “music is my ai” aren’t much good (“my aim,” “air”)
    but are closer to what you wanted.**'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 对于“music is my ai”的建议并不太好（“my aim”，“air”）但更接近你想要的。**'
- en: '***4* The suggestions for “music is my airc” caused a match four characters
    in advance (“aircraft”) and a funny sentence (“airconditioning”).**'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 对于“music is my airc”的建议导致提前四个字符匹配（“aircraft”）和一句有趣的句子（“airconditioning”）。**'
- en: One positive thing is that the language model–based suggester always gives suggestions.
    There’s no point when the end user can’t count on suggestions, even if they aren’t
    particularly accurate. That’s an advantage over previous methods. Most important,
    you can see the stream of suggestions from “music” onward.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 一件积极的事情是，基于语言模型的建议器总是给出建议。即使建议并不特别准确，当最终用户无法依赖建议时，也没有任何意义。这是相对于先前方法的一个优势。最重要的是，你可以看到从“music”开始的建议流。
- en: '|  |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-504
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注记
- en: You may wonder how bigram-based models can predict entire words from portions
    of words. Similarly to the `AnalyzingSuggester`, the `FreeTextSuggester` builds
    a finite state transducer from the ngrams.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道基于二元模型是如何从单词的部分预测整个单词的。与`AnalyzingSuggester`类似，`FreeTextSuggester`从ngram构建一个有限状态转换器。
- en: '|  |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'With the ngram language model, you can generate queries like “music is my space,”
    “music is my life,” and even “music is my airconditioning” that don’t appear in
    the search log. So you’ve reached the goal of generating new sequences of words.
    But due to the nature of ngrams (a fixed sequence of tokens), longer queries aren’t
    provided with full suggestions: thus “music is my aircraft” wasn’t included in
    the suggestions in the final stages, just “aircraft.” This isn’t necessarily bad,
    but it highlights the fact that such ngram language models aren’t very effective
    for calculating good probabilities for long sentences; therefore they may give
    weird suggestions like “music is my airconditioning.”'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ngram语言模型，你可以生成像“音乐是我的空间”、“音乐是我的生活”甚至“音乐是我的空调”这样的查询，这些查询在搜索日志中并未出现。因此，你已经达到了生成新单词序列的目标。但由于ngram（固定序列的标记）的性质，较长的查询不会提供完整的建议：因此，“音乐是我的飞机”在最终阶段的建议中并未包含，只包含“飞机”。这并不一定是个坏事情，但它突出了这样一个事实，即这种ngram语言模型在计算长句的良好概率方面并不非常有效；因此，它们可能会给出像“音乐是我的空调”这样的奇怪建议。
- en: All that you’ve just learned relates to existing methods for generating suggestions.
    I wanted you to see all the issues that affect these approaches before diving
    into neural language models, which aggregate capabilities from each of these methods.
    Another disadvantage of these models that we’ve ignored so far is that they need
    manually curated dictionaries, as you saw in the word2vec example—something that
    isn’t sustainable in practice. You need solutions that automatically adapt to
    changing data rather than requiring manual interventions. To do that, you’ll use
    the search engine to feed the suggester. The suggestions generated with such data
    will be based on the indexed content. If documents are indexed, the suggester
    will be updated as well. In the next section, we’ll look at these content-based
    suggesters.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚学到的一切都与现有的生成建议的方法相关。我想在你深入研究聚合了这些方法各自能力的神经语言模型之前，让你看到影响这些方法的所有问题。我们之前忽略的这些模型的另一个缺点是，它们需要手动编写的词典，正如你在word2vec示例中看到的那样——这在实践中是不可持续的。你需要能够自动适应数据变化的解决方案，而不是需要手动干预。为此，你将使用搜索引擎来为建议器提供数据。使用此类数据生成的建议将基于索引内容。如果文档被索引，建议器也会更新。在下一节中，我们将探讨这些基于内容的建议器。
- en: 4.5\. Content-based suggesters
  id: totrans-509
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5. 基于内容的建议器
- en: 'With *content-based* suggesters, the content comes directly from the search
    engine. Consider the search engine for a book shop. It’s probable that users will
    look for book titles or authors much more often than they will search through
    the text of a book. Each book that’s indexed has separate fields for title, author(s),
    and, eventually, the text of the book. Also, as new books are indexed and old
    ones go out of production, you need to add the new documents to the search engine
    and delete the ones related to books that can’t be bought anymore. The same thing
    needs to happen for the suggestions: you don’t want to miss suggesting new titles,
    and you want to avoid suggesting titles for books that are no longer being sold.'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于*内容*的建议器，内容直接来自搜索引擎。以书店的搜索引擎为例。用户很可能比搜索书籍文本更频繁地查找书名或作者。每本书索引都有单独的字段，用于书名、作者（们）以及最终的书本文本。此外，随着新书的索引和旧书的停产，你需要将新文档添加到搜索引擎中，并删除与无法购买的书籍相关的文档。对于建议也是如此：你不想错过建议新书名，也不想建议那些不再销售的书籍的书名。
- en: 'So the suggester must be kept up to date. If any document is removed from the
    index, the suggester may keep the suggestions that were built from that text,
    but they may be of little use. Suppose two books have been indexed: *Lucene in
    Action* and *Oauth2 in Action*. A suggester using only the text from the books’
    titles will be based on the following (lowercased) terms: “lucene,” “in,” “action,”
    “oauth2.” If you remove the *Lucene in Action* book, the list of terms will be
    trimmed down to this: “in,” “action,” “oauth2.” You can keep the “lucene” token
    in the suggester; in that case, if the user types an “L,” the suggester will suggest
    “lucene.” The problem is that a query for “lucene” won’t return any results. That’s
    why you should remove terms from the suggester when they have no possible match
    at search time.'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，建议器必须保持最新。如果任何文档从索引中删除，建议器可能会保留从该文本构建的建议，但它们可能用处不大。假设有两本书被索引了：“Lucene in
    Action”和“Oauth2 in Action”。仅使用书籍标题文本的建议器将基于以下（小写）术语：“lucene”，“in”，“action”，“oauth2”。如果您删除“Lucene
    in Action”这本书，术语列表将缩减为以下内容：“in”，“action”，“oauth2”。您可以将“lucene”标记保留在建议器中；在这种情况下，如果用户输入“L”，建议器将建议“lucene”。问题是查询“lucene”不会返回任何结果。这就是为什么您应该在搜索时没有可能匹配的情况下从建议器中删除术语的原因。
- en: You can access the inverted index that contains data about the book titles and
    use those terms the same way you use the lines of a static dictionary. In Lucene,
    feeding lookups with data from the index can be done using a `DocumentDictionary`.
    A `DocumentDictionary` reads data from the search engine, specifically from an
    `IndexReader` (a view on the search engine at a certain point in time), using
    one field for fetching the terms (to be used for suggestions) and another field
    to eventually calculate the suggestion weights (how important a suggestion is).
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以访问包含书籍标题数据的倒排索引，并像使用静态字典的行一样使用这些术语。在Lucene中，可以使用`DocumentDictionary`从索引中获取数据来执行查找操作。`DocumentDictionary`从搜索引擎读取数据，特别是从`IndexReader`（在某个时间点的搜索引擎视图）中读取，使用一个字段来获取术语（用于建议），另一个字段来最终计算建议权重（建议的重要性）。
- en: 'Let’s build a dictionary from data indexed into the `title` field in the search
    engine. You’ll give more weight to titles whose rating is higher. Suggestions
    coming from books with a higher rating will be shown first:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从搜索引擎中索引到`title`字段的数据中构建一个字典。您会给评分更高的标题赋予更高的权重。来自评分较高的书籍的建议将首先显示：
- en: '[PRE39]'
  id: totrans-514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '***1* Gets a view (an IndexReader) on the search engine**'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 获取搜索引擎的视图（IndexReader）**'
- en: '***2* Creates a dictionary based on the contents of the title field, and lets
    rating decide how much weight a suggestion has**'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 基于标题字段的内容创建字典，并让评分决定建议的权重**'
- en: '***3* Builds the lookup with the data from the index, just as with a static
    dictionary**'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 使用索引中的数据构建查找，就像使用静态字典一样**'
- en: You can guide the user to select the search results you want them to find—for
    instance, as the owner of the book shop, you may be happier if higher-rated books
    are shown more often. Other metrics to boost the suggestion may be related to
    prices, so that a user is given more frequent suggestions of books that have higher
    or lower prices.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以引导用户选择他们想要找到的搜索结果——例如，作为书店的老板，您可能更希望高评分的书籍更频繁地显示。其他可以提升建议的指标可能与价格有关，这样用户就会更频繁地收到价格更高或更低的书籍的建议。
- en: Now that you’re all set as far as getting the data for suggestions from the
    search engine, we can look at neural language models. We expect them to be able
    to mix all the good things from the methods discussed so far with better accuracy,
    composing queries that sound like they were typed by a human.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经从搜索引擎中获取了建议数据，我们可以看看神经语言模型。我们期望它们能够将迄今为止讨论的方法中的所有优点与更高的准确性结合起来，组成听起来像是人类输入的查询。
- en: 4.6\. Neural language models
  id: totrans-520
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6\. 神经语言模型
- en: A neural language model is supposed to have the same capabilities as other types
    of language models, such as the ngram models. The difference lies in how they
    learn to predict probabilities and how much better their predictions are. [Chapter
    3](kindle_split_015.xhtml#ch03) introduced a recurrent neural network (RNN) that
    tried to reproduce text from Shakespeare’s works. We were focused on how RNNs
    work, but in practice you were setting up a *character-level neural language model*!
    You saw that RNNs are very good at learning sequences of text in an unsupervised
    way, so that they can generate good new sequences based on previously seen ones.
    A language model learns to get accurate probabilities for text sequences, so this
    looks like a perfect fit for RNNs.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 神经语言模型应该具有与其他类型的语言模型相同的功能，例如ngram模型。区别在于它们学习预测概率的方式以及它们的预测有多好。[第3章](kindle_split_015.xhtml#ch03)介绍了一个尝试重现莎士比亚作品文本的循环神经网络（RNN）。我们关注的是RNN的工作原理，但在实践中你设置了一个*字符级神经语言模型*！你看到RNN在无监督方式学习文本序列方面非常出色，因此它们可以根据之前看到的序列生成好的新序列。语言模型学习获取文本序列的准确概率，因此这看起来非常适合RNN。
- en: 'Let’s start with a simple RNN that’s *not* deep and that implements a character-level
    language model: the model will predict the probabilities of all the possible output
    characters, given a sequence of input characters. Let’s visualize it:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从简单的RNN开始，它不是深度网络，并实现了一个字符级语言模型：该模型将根据输入字符序列预测所有可能输出字符的概率。让我们可视化它：
- en: '[PRE40]'
  id: totrans-523
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: You know that a neural network uses vectors for inputs and outputs; the output
    layer of the RNN you used for text generation in [chapter 3](kindle_split_015.xhtml#ch03)
    produced a vector holding a real number (between 0 and 1) for each possible output
    character. This number represents the probability of the character being outputted
    from the network. You also saw that generating probability distributions (the
    probability for all the possible characters, in this case) is accomplished by
    the softmax function. Now that you know what the output layer does, you can add
    a recurrent layer in the middle whose responsibility is to remember previously
    seen sequences, and an input layer for sending input characters to the network.
    The result is illustrated in the diagram in [figure 4.7](#ch04fig07).
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道神经网络使用向量作为输入和输出；你在[第3章](kindle_split_015.xhtml#ch03)中用于文本生成的RNN的输出层产生了包含每个可能输出字符的实数（介于0和1之间）的向量。这个数字代表了从网络输出该字符的概率。你也看到了，生成概率分布（在这种情况下，所有可能字符的概率）是通过softmax函数实现的。现在，既然你知道输出层的作用，你可以在中间添加一个循环层，其责任是记住之前看到的序列，并为网络发送输入字符的输入层。结果如图4.7[图](#ch04fig07)所示。
- en: Figure 4.7\. RNN for sequence learning
  id: totrans-525
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.7\. 用于序列学习的RNN
- en: '![](Images/04fig07_alt.jpg)'
  id: totrans-526
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig07_alt.jpg)'
- en: 'With DL4J, you configured such a network when generating alternative queries
    in [chapter 3](kindle_split_015.xhtml#ch03) as follows:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DL4J，你在[第3章](kindle_split_015.xhtml#ch03)中生成替代查询时配置了这样的网络，如下所示：
- en: '[PRE41]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '***1* Size of the hidden layer**'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1* 隐藏层的大小**'
- en: '***2* Input and output size**'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2* 输入和输出大小**'
- en: '***3* Number of unrolls of the RNN**'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3* RNN的展开次数**'
- en: Although the fundamental architecture is the same (LSTM network with one or
    more hidden layers), the goal here is different with respect to what you want
    to achieve in the alternative query generation use case. For alternative queries,
    you need the RNN to get a query and output a new query. In this case, you want
    the RNN to guess a good completion for the query the user is writing before they’re
    finished typing it. This is exactly the same RNN architecture used for generating
    text from Shakespeare’s works.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基本架构相同（一个或多个隐藏层的LSTM网络），但这里的目的是相对于你想要在替代查询生成用例中实现的目标不同。对于替代查询，你需要RNN获取一个查询并输出一个新的查询。在这种情况下，你希望RNN在用户完成输入之前猜测用户正在编写的查询的好完成。这正是用于从莎士比亚作品中生成文本的相同RNN架构。
- en: 4.7\. Character-based neural language model for suggestions
  id: totrans-533
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7\. 基于字符的神经语言模型用于建议
- en: 'In [chapter 3](kindle_split_015.xhtml#ch03), you fed the RNN a `CharacterIterator`
    that iterated over the characters in a file. So far, you’ve built suggestions
    from text files. The plan is to use the neural network as a tool to help the search
    engine, so the data to feed it should come from the search engine itself. Let’s
    index the Hot 100 Billboard dataset:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第3章](kindle_split_015.xhtml#ch03) 中，你向 RNN 提供了一个 `CharacterIterator`，它遍历文件中的字符。到目前为止，你已经从文本文件中构建了建议。计划是使用神经网络作为工具来帮助搜索引擎，因此要提供给它的数据应来自搜索引擎本身。让我们索引
    Hot 100 Billboard 数据集：
- en: '[PRE42]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '***1* Creates an IndexWriter to put documents into the index**'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 创建一个 IndexWriter 将文档放入索引**'
- en: '***2* Reads each line of the dataset, one at a time**'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 逐行读取数据集**'
- en: '***3* Doesn’t use the header line**'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 不使用标题行**'
- en: '***4* Each row in the file has the following attributes, separated by a comma:
    Rank, Song, Artist, Year, Lyrics, Source.**'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 文件中的每一行都有以下属性，由逗号分隔：排名，歌曲，艺术家，年份，歌词，来源。**'
- en: '***5* Indexes the rank of the song into a dedicated field (with its stored
    value)**'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 将歌曲排名索引到专用字段（及其存储值）**'
- en: '***6* Indexes the title of the song into a dedicated field (with its stored
    value)**'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 将歌曲标题索引到专用字段（及其存储值）**'
- en: '***7* Indexes the artist who played the song into a dedicated field (with its
    stored value)**'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 将演唱歌曲的艺术家索引到专用字段（及其存储值）**'
- en: '***8* Indexes the song lyrics into a dedicated field (with its stored value)**'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 将歌曲歌词索引到专用字段（及其存储值）**'
- en: '***9* Adds the created Lucene document to the index**'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 将创建的 Lucene 文档添加到索引**'
- en: '***10* Persists the index into the filesystem**'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10* 将索引持久化到文件系统**'
- en: 'You can use the indexed data to build a character LSTM–based lookup implementation
    called `CharLSTMNeuralLookup`. Similarly to what you’ve been doing for the `FreeTextSuggester`,
    you can use a `DocumentDictionary` to feed the `CharLSTMNeuralLookup`:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用索引数据构建一个基于字符 LSTM 的查找实现，称为 `CharLSTMNeuralLookup`。类似于你为 `FreeTextSuggester`
    所做的，你可以使用 `DocumentDictionary` 来喂养 `CharLSTMNeuralLookup`：
- en: '[PRE43]'
  id: totrans-547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '***1* Creates a DocumentDictionary whose content is fetched from the indexed
    song lyrics**'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 创建一个内容从索引歌曲歌词中获取的 DocumentDictionary**'
- en: '***2* Creates the lookup based on the charLSTM**'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 基于charLSTM创建查找**'
- en: '***3* Trains the charLSTM-based lookup**'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 训练基于charLSTM的查找**'
- en: The `DocumentDictionary` will fetch the text from the `lyrics` field. In order
    to instantiate the `CharLSTMNeuralLookup`, you need to pass the network configuration
    as a constructor parameter so that
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '`DocumentDictionary` 将从 `lyrics` 字段获取文本。为了实例化 `CharLSTMNeuralLookup`，你需要将网络配置作为构造函数参数传递，以便'
- en: At build time, the LSTM will iterate over the characters of the Lucene document
    values and learn to generate similar sequences.
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在构建时，LSTM 将遍历 Lucene 文档值中的字符，并学习生成相似的序列。
- en: At runtime, the LSTM will generate characters based on the portion of the query
    already written by the user.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在运行时，LSTM 将根据用户已写入的查询部分生成字符。
- en: 'Completing the previous code, the `CharLSTMNeuralLookup` constructor requires
    the parameters for building and training the LSTM:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 完成前面的代码，`CharLSTMNeuralLookup` 构造函数需要构建和训练 LSTM 的参数：
- en: '[PRE44]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'As mentioned earlier, neural networks require a lot of data to produce good
    results. Take care when choosing how the neural network is configured to work
    with these datasets. In particular, it’s common for a configuration that made
    a neural network work well on one dataset not to result in the same quality on
    a different dataset. Consider the number of training samples with respect to the
    number of neural network weights to be learned by the network. The number of examples
    should always be greater than the number of learnable parameters: the neural network
    weights.'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，神经网络需要大量数据才能产生良好的结果。在选择神经网络如何与这些数据集一起工作时请谨慎。特别是，一个在某个数据集上使神经网络工作得很好的配置，可能不会在另一个数据集上产生相同的质量。考虑训练样本的数量与网络需要学习的神经网络权重的数量。示例数量应始终大于可学习的参数数量：神经网络权重。
- en: 'If you have a `MultiLayerNetwork` and a `DataSet`, you can compare them:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个 `MultiLayerNetwork` 和一个 `DataSet`，你可以比较它们：
- en: '[PRE45]'
  id: totrans-558
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Another aspect we haven’t considered yet is the *initialization* of the network
    weights. When you start training a neural network, what are the initial values
    of the weights? Bad ideas for weight initialization are setting all the weights
    to the same value (zero is even worse), and setting the weights to random values.
    Weight initialization schemes are extremely important for the neural network’s
    ability to learn quickly. In this case, good weight initialization schemes are
    `NORMAL` and `XAVIER`. Both refer to probability distributions with certain properties;
    you can read about them on the DL4J cheat sheet ([http://mng.bz/K19K](http://mng.bz/K19K)).
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有考虑的另一个方面是网络权重的 *初始化*。当你开始训练一个神经网络时，权重的初始值是什么？对于权重初始化的坏主意是将所有权重设置为相同的值（零更糟），以及将权重设置为随机值。权重初始化方案对于神经网络快速学习的能力至关重要。在这种情况下，好的权重初始化方案是
    `NORMAL` 和 `XAVIER`。两者都指的是具有某些特性的概率分布；你可以在DL4J作弊单上了解它们（[http://mng.bz/K19K](http://mng.bz/K19K)）。
- en: 'To predict outputs from the neural network, you use the same code used to generate
    alternative queries. Because this LSTM works at character level, you output one
    character at a time:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从神经网络预测输出，你使用与生成替代查询相同的代码。因为这种LSTM在字符级别上工作，你一次输出一个字符：
- en: '[PRE46]'
  id: totrans-561
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '***1* Predicts a probability distribution over the given input character (vector)**'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 预测给定输入字符（向量）的概率分布**'
- en: '***2* Samples a probable character from the generated distribution**'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 从生成的分布中采样一个可能的字符**'
- en: '***3* Converts the index of the sampled character to an actual character**'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将采样字符的索引转换为实际字符**'
- en: 'You can now implement the `Lookup#lookup` API with the neural language model.
    The neural language model has an underlying neural network and an object (`CharacterIterator`)
    that consults the dataset used for training. The primary reason to consult it
    is for one-hot-encoding mapping—for example, you need to be able to reconstruct
    which character corresponds to a certain one-hot-encoded vector (and vice versa):'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以使用神经网络语言模型实现`Lookup#lookup` API。神经网络语言模型有一个底层的神经网络和一个对象（`CharacterIterator`），它咨询用于训练的数据集。咨询它的主要原因是进行one-hot-encoding映射——例如，你需要能够重建哪个字符对应于某个one-hot-encoded向量（反之亦然）：
- en: '[PRE47]'
  id: totrans-566
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '***1* Prepares the list of results**'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 准备结果列表**'
- en: '***2* Samples num text sequences from the network, given the input string entered
    by a user**'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 从网络中采样num文本序列，给定用户输入的字符串**'
- en: '***3* Adds the sampled outputs to the list of results, using their probabilities
    (from the softmax function) as suggestion weights**'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将采样输出添加到结果列表中，使用它们的概率（来自softmax函数）作为建议权重**'
- en: 'The `CharLSTMNeuralLookup` also needs to implement the build API. That’s where
    the neural network will train (or retrain):'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '`CharLSTMNeuralLookup` 还需要实现构建API。这就是神经网络将进行训练（或重新训练）的地方：'
- en: '[PRE48]'
  id: totrans-571
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '***1* Extracts text to be used for suggestions from the lyrics field, weighted
    by the song’s rank value**'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 从歌词字段中提取用于建议的文本，按歌曲的排名值加权**'
- en: 'Because the character LSTM uses a `CharacterIterator`, you convert data from
    the `Dictionary` (an `InputIterator` object) into a `CharacterIterator`, and pass
    it to the neural network for training (as a side effect, this means having a temporary
    file on disk to hold the data extracted from the index to train the network):'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 因为字符LSTM使用`CharacterIterator`，你将数据从`Dictionary`（一个`InputIterator`对象）转换为`CharacterIterator`，并将其传递给神经网络进行训练（作为副作用，这意味着在磁盘上有一个临时文件来保存从索引中提取的数据以训练网络）：
- en: '[PRE49]'
  id: totrans-574
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '***1* Creates a temporary file**'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 创建一个临时文件**'
- en: '***2* Fetches text coming from the lyrics field in the Lucene index (Lucene
    uses BytesRef instead of String for performance reasons)**'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 从Lucene索引中的歌词字段获取文本（出于性能原因，Lucene使用BytesRef而不是String）**'
- en: '***3* Writes the text into the temporary file**'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将文本写入临时文件**'
- en: '***4* Releases resources for writing into the temporary file**'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 释放写入临时文件的资源**'
- en: '***5* Creates a CharacterIterator (using the CharLSTMNeuralLookup configuration
    parameters)**'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 创建一个CharacterIterator（使用CharLSTMNeuralLookup配置参数）**'
- en: '***6* Builds and trains the LSTM (using the CharLSTMNeuralLookup configuration
    parameters)**'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 使用CharLSTMNeuralLookup配置参数构建和训练LSTM**'
- en: '***7* Removes the temporary file**'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 删除临时文件**'
- en: 'Before going forward and using this `Lookup` in a search application, you need
    to make sure the neural language model works well and gives good results. Like
    other algorithms in computer science, neural networks aren’t magic: you need to
    set them up correctly if you want them to work well.'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续前进并使用此`Lookup`进行搜索应用之前，你需要确保神经网络语言模型运行良好并给出良好的结果。像计算机科学中的其他算法一样，神经网络并不是魔法：如果你想它们运行良好，你需要正确设置它们。
- en: 4.8\. Tuning the LSTM language model
  id: totrans-583
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8\. 调整 LSTM 语言模型
- en: Instead of doing what you did in [chapter 3](kindle_split_015.xhtml#ch03) and
    adding more layers to the network, you’ll start simple with a single layer and
    adjust other parameters, and see if one layer is sufficient. The most important
    reason to do this is that as the complexity of the network grows (for example,
    more layers), the data and time required for the training phase to generate a
    good model (which gives good results) grows as well. So although small, shallow
    networks can’t beat deeper ones with lots of different data, this language-modeling
    example is a good place to learn to start simple and go deeper only when needed.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 与你在[第 3 章](kindle_split_015.xhtml#ch03)中做的不同，你不会增加网络中的更多层，而是从单层开始，调整其他参数，看看一层是否足够。这样做最重要的原因在于，随着网络复杂性的增加（例如，更多层），用于训练阶段生成良好模型（即给出良好结果）所需的数据和时间也会增加。因此，尽管小型、浅层网络无法击败具有大量不同数据的深层网络，但这个语言建模示例是一个很好的地方，可以学习从简单开始，只在需要时深入。
- en: As you work more with neural networks, you’ll get to know how to best set and
    tune them. For now, you know that when data is large and diverse, it may be a
    good idea to have a deep RNN for language modeling. But let’s be pragmatic and
    see if that’s true. To do so, you need a way to evaluate the neural network learning
    process. Neural network training is an *optimization problem*, where you want
    to optimize the weights in the connections between the neurons in order to let
    them generate the results you desire. In practice, this means you have an initial
    set of weights in each layer, according to a chosen weight-initialization scheme.
    These weights are adjusted during training so that the error the network commits
    when trying to predict outputs decreases as training goes on. If the error committed
    by the network doesn’t decrease or, even worse, increases, you’ve done something
    wrong in your setup. In [chapter 3](kindle_split_015.xhtml#ch03), we talked about
    *cost functions* that measure such error, and the fact the neural network–training
    algorithm’s objective is to minimize these cost functions. A good way to begin
    measuring whether the training is doing well is to plot the network cost (or loss)
    over time and make sure it keeps decreasing as backpropagation proceeds.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你更多地使用神经网络，你会了解到如何最佳地设置和调整它们。目前，你知道当数据量大且多样化时，对于语言建模来说，可能有一个深层的 RNN 是一个好主意。但让我们保持务实，看看这是否正确。为了做到这一点，你需要一种方法来评估神经网络学习过程。神经网络训练是一个*优化问题*，你希望优化神经元之间连接的权重，以便让它们生成你希望的结果。在实践中，这意味着你根据选择的权重初始化方案在每个层中都有一个初始权重集。这些权重在训练过程中进行调整，以便随着训练的进行，网络在尝试预测输出时犯的错误逐渐减少。如果网络犯下的错误没有减少，甚至更糟，增加了，那么你在设置中犯了错误。在第
    3 章[中](kindle_split_015.xhtml#ch03)，我们讨论了*成本函数*，它衡量这种错误，以及神经网络训练算法的目标是最小化这些成本函数。一个好的开始是测量训练是否进行得好的方法，就是绘制网络成本（或损失）随时间的变化，并确保它在反向传播过程中持续下降。
- en: To make sure your neural language model will give good results, you need to
    track whether the cost decreases. In the case of DL4J, you can use `TrainingListener`s
    like the `ScoreIterationListener` from [chapter 3](kindle_split_015.xhtml#ch03)
    (which logs the loss value) or, even better, a `StatsListener`, which has a proper
    user interface and will collect and send statistics to a remote server so you
    can better monitor the learning process. [Figure 4.8](#ch04fig08) shows how such
    a server displays the learning process.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保你的神经网络语言模型能给出良好的结果，你需要跟踪成本是否下降。在 DL4J 的情况下，你可以使用`TrainingListener`，如第 3
    章[中的](kindle_split_015.xhtml#ch03)`ScoreIterationListener`（它记录损失值），或者更好的是`StatsListener`，它有一个合适的用户界面，并将收集并发送统计数据到远程服务器，以便你更好地监控学习过程。[图
    4.8](#ch04fig08)显示了这样一个服务器如何显示学习过程。
- en: Figure 4.8\. DL4J Training UI
  id: totrans-587
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.8\. DL4J 训练界面
- en: '![](Images/04fig08_alt.jpg)'
  id: totrans-588
  prefs: []
  type: TYPE_IMG
  zh: '![图片 4.8](Images/04fig08_alt.jpg)'
- en: The Overview page of the DL4J Training UI contains a lot of information about
    the training process; for now, we’ll focus on the score versus iteration panel
    at upper left. That’s where the score should decrease as the number of iterations
    grows over time, ideally to near zero. At upper right, you can see some general
    information about the network parameters and training speed. We’ll skip the graphs
    at the bottom, because they show more detailed information about the size of the
    parameters (such as weights) and how they vary over time.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: DL4J训练UI的概览页面包含有关训练过程的大量信息；目前，我们将关注左上角的分数与迭代数面板。那里是分数应该随着迭代次数的增长而逐渐降低的地方，理想情况下接近零。在右上角，您可以查看有关网络参数和训练速度的一些一般信息。我们将跳过底部的图表，因为它们显示了更多关于参数大小（如权重）及其随时间变化的信息。
- en: 'It’s easy to set up this UI:'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 设置这个UI很简单：
- en: '[PRE50]'
  id: totrans-591
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '***1* Initializes the user interface backend**'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 初始化用户界面后端**'
- en: '***2* Configures where the network information is to be stored—in this case,
    in memory**'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 配置网络信息存储的位置——在这种情况下，在内存中**'
- en: '***3* Attaches the StatsStorage instance to the UI so the contents of StatsStorage
    will be displayed**'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将StatsStorage实例附加到UI，以便显示StatsStorage的内容**'
- en: 'Having configured and started the UI server, you now tell the neural network
    to send statistics to it by adding a `StatsListener`:'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置并启动UI服务器后，您现在告诉神经网络通过添加`StatsListener`将其统计信息发送到它：
- en: '[PRE51]'
  id: totrans-596
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '***1* Neural network to monitor**'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 监控神经网络**'
- en: '***2* Initializes the network (for example, sets the initial weights in the
    layers)**'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 初始化网络（例如，设置层的初始权重）**'
- en: '***3* Uses the StatsListener**'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 使用StatsListener**'
- en: '***4* Lets training start**'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 启动训练**'
- en: As soon as training begins, you can access the DL4J UI at http://localhost:9000
    from a web browser. When you do, you’ll see the Overview page.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦开始训练，您就可以通过网页浏览器访问http://localhost:9000上的DL4J UI。当您这样做时，您将看到概览页面。
- en: 'Let’s start with a character LSTM network that has two hidden layers, and see
    how it performs on the queries dataset by looking at the DL4J training UI (see
    [figure 4.9](#ch04fig09)). As you can see, the score decreases very little with
    the number of iterations, which means you probably won’t get good results. A common
    mistake is to overengineer the neural network: you started with two 300-dimensional
    hidden layers, and maybe that’s too many. Earlier in the chapter, I mentioned
    that it’s not a good idea to have more weights to be learned than training samples.
    Let’s double-check the logs:'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从具有两个隐藏层的字符LSTM网络开始，通过查看DL4J训练UI（见[图4.9](#ch04fig09)）来了解它在查询数据集上的表现。如您所见，随着迭代次数的增加，分数下降很少，这意味着您可能不会得到很好的结果。一个常见的错误是过度设计神经网络：您从两个300维的隐藏层开始，可能太多了。在本章早期，我提到学习比训练样本多的权重不是一个好主意。让我们再次检查日志：
- en: '[PRE52]'
  id: totrans-603
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Figure 4.9\. Character-level LSTM neural language model with 2 hidden layers
    (300 neurons each)
  id: totrans-604
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.9\. 具有两个隐藏层（每个300个神经元）的字符级LSTM神经网络语言模型
- en: '![](Images/04fig09_alt.jpg)'
  id: totrans-605
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig09_alt.jpg)'
- en: The number of training examples is 100 times smaller than the number of parameters
    to be learned. Because of that, it’s unlikely that the training will achieve a
    good set of weights. You don’t have enough data!
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 训练示例的数量比要学习的参数数量少100倍。正因为如此，训练不太可能得到一组好的权重。您没有足够的数据！
- en: 'You need to either get more data or use a simpler neural network, with fewer
    parameters to be learned. Assuming you can’t do the former, let’s go for the latter:
    configure a simpler, smaller neural network that has one hidden layer with 80
    neurons, and check the logs again:'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要获取更多数据或使用一个更简单的神经网络，具有更少的参数需要学习。假设您无法做到前者，那么让我们选择后者：配置一个更简单、更小的神经网络，它有一个包含80个神经元的隐藏层，并再次检查日志：
- en: '[PRE53]'
  id: totrans-608
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[Figure 4.10](#ch04fig10) shows a nicer loss curve; it degrades smoothly, although
    the final point isn’t close to zero.'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.10](#ch04fig10)显示了一个更漂亮的损失曲线；它平滑地下降，尽管最终点并不接近零。'
- en: Figure 4.10\. Character-level LSTM neural language model with one hidden layer
    (80 neurons)
  id: totrans-610
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.10\. 具有一个隐藏层（80个神经元）的字符级LSTM神经网络语言模型
- en: '![](Images/04fig10_alt.jpg)'
  id: totrans-611
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig10_alt.jpg)'
- en: 'The goal is to have the final loss reach a value that steadily remains close
    to zero. Let’s test it anyway, with the “music is my aircraft” query—you can expect
    suboptimal results because the neural network didn’t find a combination of weights
    with a low cost:'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是让最终损失值稳定地接近零。无论如何，让我们用“音乐是我的飞机”查询来测试它——您可以预期结果不会很理想，因为神经网络没有找到具有低成本的权重组合：
- en: '[PRE54]'
  id: totrans-613
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: These results are worse than those with previous solutions that weren’t based
    on neural networks! Let’s compare the results from this first neural language
    model to those from the ngram language model and from the `AnalyzingSuggester`.
    [Table 4.1](#ch04table01) shows that although the neural language model always
    gives results, many of them don’t make much sense.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果比那些基于神经网络的解决方案还要差！让我们比较一下这个第一个神经语言模型的结果与ngram语言模型和`AnalyzingSuggester`的结果。[表4.1](#ch04table01)显示，尽管神经语言模型总是给出结果，但其中许多并没有太多意义。
- en: Table 4.1\. Comparing results of different `Suggester` implementations
  id: totrans-615
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.1\. 比较不同`Suggester`实现的结果
- en: '| Input | Neural | Ngram | Analyzing |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | 神经 | Ngram | Analyzing |'
- en: '| --- | --- | --- | --- |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| “m” | musorida hosking floa | my | m |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '| “m” | musorida hosking floa | my | m |'
- en: '| “music” | music tents in sauraborls | music | music |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| “music” | music tents in sauraborls | music | music |'
- en: '| “music is” | music island kn5 stendattion | island | music |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| “music is” | music island kn5 stendattion | island | music |'
- en: '| “music is my ai” | music is my airborty cioderopaship | my aim |   |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| “music is my ai” | music is my airborty cioderopaship | my aim |   |'
- en: '| “music is my aircr” | music is my aircrichs of nwire | aircraft |   |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| “music is my aircr” | music is my aircrichs of nwire | aircraft |   |'
- en: What is “sauraborls” in the “music tents in sauraborls” suggestion? And what
    is “stendattion” from the “music island kn5 stendattion” suggestion? As the length
    of the text to be predicted grows, the neural language model starts returning
    sequences of characters that don’t form meaningful words—it fails at estimating
    good probabilities for longer inputs. That’s exactly what you expected after observing
    the learning curve.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: “music tents in sauraborls”建议中的“sauraborls”是什么？而“music island kn5 stendattion”建议中的“stendattion”又是什么？随着要预测的文本长度的增加，神经语言模型开始返回不构成有意义的单词的字符序列——它无法为较长的输入估计良好的概率。这正是你在观察学习曲线后所期望的。
- en: 'You want the network to learn better, so let’s look at one of the most important
    configuration parameters when setting up the training for a neural network: the
    *learning rate*. The learning rate defines how much the weights of the neural
    network are changed with respect to the (gradient) cost. A high learning rate
    may cause the neural network to never find a good set of weights, because the
    weights are changed too much and a good combination is never found. A low learning
    rate may slow learning so much that a good set of weights isn’t found before all
    the data is used for learning.'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望网络学习得更好，所以让我们看看设置神经网络训练时最重要的配置参数之一：*学习率*。学习率定义了神经网络权重相对于（梯度）成本的改变量。高学习率可能会导致神经网络永远找不到一个好的权重集，因为权重改变太多，而一个好的组合永远不会被发现。低学习率可能会使学习速度变得如此之慢，以至于在所有数据用于学习之前，可能找不到一个好的权重集。
- en: 'Let’s increase the number of neurons in the layer just a bit, to 90, and start
    the training again:'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微增加层中的神经元数量，到90个，然后再次开始训练：
- en: '[PRE55]'
  id: totrans-626
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The number of neural network parameters is slightly smaller than the number
    of available training examples, so you shouldn’t add more parameters going forward.
    Once you’ve finished training, let’s get the lookup results:'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络参数的数量略小于可用的训练示例数量，所以你未来不应该添加更多参数。一旦完成训练，让我们获取查找结果：
- en: '[PRE56]'
  id: totrans-628
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The quality of the suggested results has improved. Many of them are composed
    of correct English words; some of them are even funny, like “music is my aircraft
    popper” and “music is my aircraftless theatre”! Let’s take another look at the
    Overview tab of the just-trained neural language model (see [figure 4.11](#ch04fig11)).
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 建议结果的质量有所提高。其中许多由正确的英语单词组成；其中一些甚至很有趣，比如“music is my aircraft popper”和“music
    is my aircraftless theatre”！让我们再次查看刚刚训练好的神经语言模型的概述标签（见图4.11）。
- en: Figure 4.11\. More parameters but still suboptimal convergence
  id: totrans-630
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.11\. 更多的参数但仍然次优收敛
- en: '![](Images/04fig11_alt.jpg)'
  id: totrans-631
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig11_alt.jpg)'
- en: The loss is decreasing better, but it still didn’t reach a small enough value,
    so the learning rate probably isn’t set correctly yet. Let’s try to boost it by
    setting the learning rate to a higher value. It was set to 0.1, so let’s try 0.4—a
    very high value! [Figure 4.12](#ch04fig12) shows the network being trained again.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 损失正在更好地减少，但它仍然没有达到足够小的值，所以学习率可能还没有设置正确。让我们尝试将其提升到更高的值。它被设置为0.1，所以让我们尝试0.4——一个非常高的值！[图4.12](#ch04fig12)显示了网络再次进行训练。
- en: Figure 4.12\. Higher learning rate
  id: totrans-633
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.12\. 更高的学习率
- en: '![](Images/04fig12_alt.jpg)'
  id: totrans-634
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig12_alt.jpg)'
- en: The result is a lower loss, and the neural network reached that with more parameters.
    This means it knows more about the training data. We’ll stop here and consider
    ourselves satisfied with these outputs.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是更低的损失，神经网络通过更多的参数达到了这一点。这意味着它对训练数据了解更多。我们在这里停止，并对自己这些输出感到满意。
- en: For optimal training, more iterations would be required; adjusting other parameters
    may give better-looking shapes and better-sounding suggestions. We’ll discuss
    neural network tuning further in the final chapter of the book.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最佳训练，需要更多的迭代；调整其他参数可能会给出更好看的形状和更好听的建议。我们将在本书的最后一章进一步讨论神经网络调整。
- en: 4.9\. Diversifying suggestions using word embeddings
  id: totrans-637
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.9\. 使用词嵌入多样化建议
- en: 'In [chapter 2](kindle_split_013.xhtml#ch02), you saw how useful it is to use
    word embeddings for synonym expansion. This section shows how to mix them with
    the results of LSTM-generated suggestions to provide more diverse suggestions
    for the end user. In production systems, it’s common to combine the results of
    different models to provide a good user experience. The word2vec model lets you
    create a vectorized representation of a word. Such vectors are learned by a shallow
    neural network by looking at the surrounding context (other nearby words) of each
    word. The nice thing about word2vec and similar algorithms for representing words
    as vectors is that they place similar words close together in the vector space:
    for example, the vectors representing “aircraft” and “aeroplane” will be very
    close to one another.'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 2 章](kindle_split_013.xhtml#ch02) 中，你看到了使用词嵌入进行同义词扩展是多么有用。本节展示了如何将它们与 LSTM
    生成的建议结果混合，为最终用户提供更多样化的建议。在生产系统中，通常将不同模型的结果结合起来，以提供良好的用户体验。word2vec 模型允许你创建一个单词的向量表示。这些向量是通过观察每个单词的周围上下文（其他附近的单词）由浅层神经网络学习得到的。word2vec
    和类似算法将单词表示为向量的好处是它们在向量空间中将相似单词放置在一起：例如，“aircraft”和“aeroplane”表示的向量将非常接近。
- en: 'Let’s build a word2vec model from the Lucene index containing the song lyrics,
    similar to what you did in [chapter 2](kindle_split_013.xhtml#ch02):'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个包含歌曲歌词的 Lucene 索引中构建一个 word2vec 模型，类似于你在 [第 2 章](kindle_split_013.xhtml#ch02)
    中所做的那样：
- en: '[PRE57]'
  id: totrans-640
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '***1* Creates a DataSetIterator over the contents of the Lucene lyrics field**'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 在 Lucene 歌词字段的内容上创建一个 DataSetIterator**'
- en: '***2* Configures a word2vec model with word vectors of size 100**'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 配置一个大小为 100 的 word2vec 模型**'
- en: '***3* Performs word2vec model training**'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 执行 word2vec 模型训练**'
- en: '***4* Builds the neural language model with the previously trained LSTM, the
    CharacterIterator, and the word2vec model**'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 使用之前训练的 LSTM、CharacterIterator 和 word2vec 模型构建神经语言模型**'
- en: With the word2vec model trained on the same data, you can now combine it with
    the `CharLSTMNeuralLookup` and generate more suggestions. You’ll define a `CharLSTMWord2VecLookup`
    that extends the `CharLSTMNeuralLookup` class. This `Lookup` implementation requires
    a `Word2Vec` instance. At lookup time, it goes over the string suggested by the
    LSTM network, and then the word2vec model is used to find the nearest neighbor(s)
    for each word in the string. These nearest neighbors are used to create a new
    suggestion. For example, the sequence “music is my aircraft” generated by the
    LSTM will be split into its tokens “music,” “is,” “my,” and “aircraft.” The word2vec
    model will check, for example, the nearest neighbors of the word “aircraft” and
    find “aeroplane,” and then create the additional suggestion “music is my aeroplane.”
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 使用在相同数据上训练的 word2vec 模型，你现在可以将其与 `CharLSTMNeuralLookup` 结合起来生成更多建议。你将定义一个 `CharLSTMWord2VecLookup`，它扩展了
    `CharLSTMNeuralLookup` 类。这个 `Lookup` 实现需要一个 `Word2Vec` 实例。在查找时，它会遍历 LSTM 网络建议的字符串，然后使用
    word2vec 模型为字符串中的每个单词找到最近邻。这些最近邻被用来创建一个新的建议。例如，由 LSTM 生成的序列“music is my aircraft”将被分割成其标记“music”、“is”、“my”和“aircraft”。word2vec
    模型将检查例如“aircraft”单词的最近邻，找到“aeroplane”，然后创建额外的建议“music is my aeroplane”。
- en: Listing 4.1\. Extended neural language model with Word2Vec
  id: totrans-646
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.1\. 带有 Word2Vec 的扩展神经语言模型
- en: '[PRE58]'
  id: totrans-647
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '***1* Gets the suggestions generated by the LSTM network**'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 获取由 LSTM 网络生成的建议**'
- en: '***2* Divides the suggestion string into its tokens (words)**'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 将建议字符串分割成其标记（单词）**'
- en: '***3* Finds the top two nearest neighbors of each token**'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 为每个标记找到前两个最近邻**'
- en: '***4* For each nearest neighbor, checks whether it’s similar enough to the
    input word**'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 对于每个最近邻，检查它是否足够类似于输入单词**'
- en: '***5* Creates an enhanced suggestion using the word suggested by word2vec**'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 使用 word2vec 建议的单词创建一个增强的建议**'
- en: '***6* Simple suggestion enhancement implementation: substitutes the original
    word with its nearest neighbor word**'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 简单建议增强实现：用最近的邻居词替换原始词**'
- en: 'Back at the beginning of [chapter 2](kindle_split_013.xhtml#ch02), a user wanted
    to find the lyrics of a song whose title they couldn’t exactly recall. With a
    word2vec model for synonym expansion, you can return the correct song even when
    the query doesn’t match the title, by means of generated synonyms. With this combination
    of a neural language model and a word2vec model to generate suggestions, you manage
    to let the user avoid searching completely: the user types “music is my airc...”
    and gets the suggestion “music is my aeroplane,” so no actual search is performed,
    but the user’s information need is satisfied!'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 回到[第2章](kindle_split_013.xhtml#ch02)的开头，一个用户想要找到一首他们无法确切记得标题的歌曲的歌词。通过同义词扩展的word2vec模型，即使查询不匹配标题，你也可以通过生成的同义词返回正确的歌曲。通过这种神经语言模型和word2vec模型的组合来生成建议，你成功地让用户避免了完全搜索：用户输入“music
    is my airc...”并得到建议“music is my aeroplane”，因此没有实际进行搜索，但用户的情报需求得到了满足！
- en: Summary
  id: totrans-655
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Search suggestions are important to help users write good queries.
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索建议对于帮助用户编写良好的查询非常重要。
- en: The data for generating such suggestions can be static (for example, dictionaries
    of previously entered queries) or dynamic (such as documents stored in the search
    engine).
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成此类建议的数据可以是静态的（例如，以前输入的查询的词典）或动态的（例如，存储在搜索引擎中的文档）。
- en: You can use text analysis and/or ngram language models to build good suggester
    algorithms.
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用文本分析和/或ngram语言模型来构建良好的建议算法。
- en: Neural language models are language models based on neural networks, such as
    RNNs (or LSTMs).
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经语言模型是基于神经网络的语模型，例如RNN（或LSTM）。
- en: By using neural language models, you can get better-sounding suggestions.
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用神经语言模型，你可以获得更好听的建议。
- en: It’s important to monitor the neural network training process to make sure you
    get good results.
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控神经网络训练过程以确保你获得良好的结果非常重要。
- en: You can combine the results of the original suggester with word vectors to augment
    the diversity of the suggestions.
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以将原始建议器的结果与词向量相结合，以增加建议的多样性。
- en: Chapter 5\. Ranking search results with word embeddings
  id: totrans-663
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5章\. 使用词嵌入对搜索结果进行排名
- en: '*This chapter covers*'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Statistical and probabilistic retrieval models
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计和概率检索模型
- en: Working with the ranking algorithm in Lucene
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与Lucene的排名算法一起工作
- en: Neural information retrieval models
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经信息检索模型
- en: Using averaged word embeddings to rank search results
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用平均词嵌入对搜索结果进行排名
- en: Since [chapter 2](kindle_split_013.xhtml#ch02), we’ve been building components
    based on neural networks that can improve a search engine. These components aim
    to help the search engine better capture user intent by expanding synonyms, generating
    alternative representations of a query, and giving smarter suggestions while the
    user is typing a query. As these approaches show, a query can be expanded, adapted,
    and transformed before matching with the terms stored in the inverted indexes
    is performed. Then, as mentioned in [chapter 1](kindle_split_012.xhtml#ch01),
    the terms of the query are used to find matching documents.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 自从[第2章](kindle_split_013.xhtml#ch02)以来，我们一直在构建基于神经网络的可改进搜索引擎的组件。这些组件旨在通过扩展同义词、生成查询的替代表示以及在使用查询时提供更智能的建议，帮助搜索引擎更好地捕捉用户意图。正如这些方法所显示的，在执行与倒排索引中存储的术语匹配之前，查询可以扩展、适应和转换。然后，如[第1章](kindle_split_012.xhtml#ch01)中提到的，查询的术语用于查找匹配的文档。
- en: These matching documents, also known as *search results*, are sorted according
    to how closely they’re predicted to match the input query. This task of sorting
    the results is known as *ranking* or *scoring*. The ranking function has a fundamental
    impact on the *relevance* of search results, so getting it right means the search
    engine will have higher *precision*, and users will receive the most relevant
    and important information first. Getting ranking right isn’t a one-shot process;
    rather, it’s an incremental one. In real life, you’ll use an existing ranking
    algorithm, create a new one, or use a combination of existing and new ranking
    functions. Many times you’ll have to fine-tune them to accurately capture what
    your users are looking for, how they’re writing queries, and so on.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 这些匹配的文档，也称为*搜索结果*，是根据它们与输入查询的匹配程度进行排序的。这个对结果进行排序的任务被称为*排名*或*评分*。排名函数对搜索结果的相关性有根本性的影响，因此正确地完成它意味着搜索引擎将具有更高的*精确度*，用户将首先收到最相关和最重要的信息。正确地进行排名不是一个一次性过程；而是一个逐步的过程。在现实生活中，你会使用现有的排名算法，创建一个新的算法，或者使用现有和新排名函数的组合。很多时候，你必须调整它们以准确地捕捉到用户正在寻找的内容，他们是如何编写查询的，等等。
- en: In this chapter, you’ll learn about common ranking functions, information retrieval
    models, and how a search engine “decides” which results to show first. Then I’ll
    show you how to improve your search engine’s ranking functions by using dense
    vector representations of text (words, sentences, documents, and so on). Also
    known as *embeddings*, these vector representations of text can help your ranking
    functions to do a better job matching and scoring documents according to the user’s
    intent.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解常见的排名函数、信息检索模型以及搜索引擎“决定”显示哪些结果为第一页的过程。然后我会向你展示如何通过使用文本（单词、句子、文档等）的密集向量表示来改进搜索引擎的排名函数。这些文本的向量表示也称为*嵌入*，可以帮助你的排名函数更好地匹配和评分文档，以符合用户的意图。
- en: 5.1\. The importance of ranking
  id: totrans-672
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 排名的意义
- en: 'A somewhat funny meme that floated around on the internet for a while said,
    “The best place to hide a dead body is page two of Google.” This is of course
    a hyperbolic sentence that applies mostly to web search (searching for content,
    such as pages, from websites). But it says a lot about the degree to which users
    expect search engines to be good at returning relevant results. It’s often mentally
    easier for a user to write a better query than to scroll down and click the Page
    2 button on the results page. The meme could be rephrased as, “If it didn’t show
    up on the first page, it can’t be relevant.” This tells you why relevance is important.
    You can assume the following:'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 一段时间内在互联网上流传的一个有点好笑的梗说：“藏尸的最佳地点是谷歌的第二页。”这当然是一个夸张的句子，主要适用于网络搜索（从网站搜索内容，如页面）。但它说明了用户对搜索引擎返回相关结果的能力的期望程度。对于用户来说，编写一个更好的查询通常比滚动并点击结果页面上的第二页按钮在心理上更容易。这个梗可以重新表述为：“如果没有出现在第一页，那就不是相关的。”这告诉你相关性为什么很重要。你可以假设以下内容：
- en: '***Users are lazy.*** They don’t want to scroll down or look at more than two
    or three results before deciding whether the search results are good. Returning
    thousands of results is often useless.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***用户很懒惰。*** 他们不想在决定搜索结果是否好之前滚动或查看两个或三个以上的结果。返回数千个结果通常是无用的。'
- en: '***Users are uninformed.*** They don’t know how a search engine works internally;
    they just write a query and hope to get good results.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***用户缺乏信息。*** 他们不知道搜索引擎内部是如何工作的；他们只是编写一个查询，希望得到好的结果。'
- en: If a search engine ranking function works well, you can return the top 10 to
    20 results, and the user will be satisfied. Note that this approach can also have
    a positive impact on the performance of the search engine, because the user won’t
    browse through all the matching documents.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 如果搜索引擎的排名函数工作得很好，你可以返回前10到20个结果，用户就会感到满意。请注意，这种方法也可以对搜索引擎的性能产生积极影响，因为用户不会浏览所有匹配的文档。
- en: You may wonder if the relevance problem applies in all cases, though. For example,
    if you have a short query that consists of one or two words and that clearly identifies
    a small set of search results, the relevance problem is less evident. Think about
    all the search queries you’ve performed on Google just to retrieve a Wikipedia
    page. For example, imagine that you want to find the page that describes Bernhard
    Riemann. It’s annoying to enter the en.wikipedia.org URL, type `Bernhard Riemann`
    in the Wikipedia Search text box, and click the magnifying glass button to get
    the results. It’s much faster to type `Bernhard Riemann` in the Google search
    box—and you’ll most probably get the Wikipedia page as the first or second search
    result on the first page. This is an example where you (think you) know in advance
    what you want to retrieve (you’re lazy, but you were informed about what you wanted
    and you knew from prior experience how the search engine usually works when searching
    for people). But in many cases, this doesn’t apply. Put yourself in the shoes
    of an undergrad math student who isn’t interested in generic information about
    Riemann, but instead wants to understand why his works are considered important
    in several different fields of science. The student doesn’t know in advance the
    specific resources they want; they know the *type* of resource needed and will
    type a query based on that. So such a student may type a query like `the importance
    of Bernhard Riemann works` or `Bernhard Riemann influence in academic research`.
    If you run these two queries on Google yourself,
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，你可能想知道相关性问题是否在所有情况下都适用。例如，如果你有一个由一个或两个单词组成的简短查询，并且清楚地识别了一小部分搜索结果，那么相关性问题就不那么明显了。想想你在谷歌上执行的所有搜索查询，只是为了检索维基百科页面。例如，想象你想找到描述伯恩哈德·黎曼的页面。输入en.wikipedia.org
    URL，在维基百科搜索文本框中键入`Bernhard Riemann`，然后点击放大镜按钮以获取结果，这很烦人。在谷歌搜索框中键入`Bernhard Riemann`要快得多——你很可能会在第一页的第一或第二个搜索结果中找到维基百科页面。这是一个例子，其中你（认为）事先就知道你想要检索的内容（你很懒，但你已经被告知了你想要什么，并且你知道从以往的经验中，搜索引擎在搜索人物时通常是如何工作的）。但在许多情况下，这并不适用。把自己放在一个对黎曼的通用信息不感兴趣，但想了解他的作品为什么在几个不同的科学领域中都被认为是重要的本科数学学生的位置上。学生事先不知道他们想要的具体资源；他们知道所需的资源类型，并将基于这一点输入查询。因此，这样的学生可能会输入像`Bernhard
    Riemann作品的重要性`或`Bernhard Riemann在学术研究中的影响`这样的查询。如果你自己运行这两个查询，
- en: You’ll see different search results for each query.
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你会看到每个查询都有不同的搜索结果。
- en: Search results that appear in both cases are in a different order.
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这两种情况下出现的搜索结果顺序是不同的。
- en: 'More notably, at the time of writing, the first query returns the Wikipedia
    page as the first result, whereas the second query’s first result is “herbart’s
    influence on bernhard riemann.” That’s odd because it turns the user’s intent
    upside down: the student wanted to know how Riemann influenced others, not vice
    versa (the second result, “riemann’s contribution to differential geometry,” sounds
    much more relevant). This is the kind of problem that makes ranking search results
    difficult.'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 更值得注意的是，在撰写本文时，第一个查询将维基百科页面作为第一个结果返回，而第二个查询的第一个结果是“赫尔巴特对伯恩哈德·黎曼的影响。”这很奇怪，因为它颠倒了用户的意图：学生想知道黎曼如何影响他人，而不是反过来（第二个结果“黎曼对微分几何的贡献”听起来要相关得多）。这就是使搜索结果排名困难的问题之一。
- en: 'Let’s now see how ranking comes into play in the life cycle of a query (see
    also [figure 5.1](#ch05fig01)):'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看排名如何在查询的生命周期中发挥作用（参见[图5.1](#ch05fig01)）：
- en: '**1**.  A query written by the user is parsed, analyzed, and broken down into
    a set of term clauses (an encoded query).'
  id: totrans-682
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**.  用户编写的查询被解析、分析和分解成一组术语子句（编码查询）。'
- en: '**2**.  The encoded query is executed against the search engine data structures
    (for each term, a lookup in the inverted index table is performed).'
  id: totrans-683
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**.  对编码查询执行搜索引擎数据结构的操作（对于每个术语，在倒排索引表中执行查找）。'
- en: '**3**.  The matching documents are collected and passed to the ranking function.'
  id: totrans-684
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**.  收集匹配的文档并将其传递给排名函数。'
- en: '**4**.  Each document is scored by the ranking function.'
  id: totrans-685
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**4**.  排名函数对每个文档进行评分。'
- en: '**5**.  Typically, the list of search results is composed of such documents,
    sorted according to their score in descending order (the first result has the
    highest score).'
  id: totrans-686
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**5**.  通常，搜索结果列表由这样的文档组成，按其分数降序排列（第一个结果分数最高）。'
- en: Figure 5.1\. Querying, retrieving, and ranking
  id: totrans-687
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.1\. 查询、检索和排名
- en: '![](Images/05fig01_alt.jpg)'
  id: totrans-688
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig01_alt.jpg)'
- en: The ranking function takes a bunch of search results and assigns each one a
    score value that’s an indicator of its importance with respect to the input query.
    The higher the score, the more important the document.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 排名函数接受一组搜索结果，并为每个结果分配一个分数值，该分数值是相对于输入查询的重要性的指标。分数越高，文档的重要性就越大。
- en: 'Additionally, when ranking results, a smart search engine should consider the
    following:'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在排名结果时，智能搜索引擎还应考虑以下因素：
- en: '***User history—*** Record the past activity of a user and take it into consideration
    when ranking. For example, recurring terms in past queries may indicate a user’s
    interest in a certain topic, so search results on that same topic should have
    a higher ranking.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***用户历史记录—*** 记录用户过去的活动，并在排名时考虑这些活动。例如，过去查询中重复出现的术语可能表明用户对某个特定主题感兴趣，因此该主题的搜索结果应该有更高的排名。'
- en: '***User’s geographical location—*** Record the user’s location, and increase
    the score of search results written in the appropriate language.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***用户的地理位置—*** 记录用户的地理位置，并增加用适当语言编写的搜索结果的分数。'
- en: '***Temporal changes in information—*** Recall the “latest trends” query from
    [chapter 3](kindle_split_015.xhtml#ch03). Such a query should match not only the
    words “latest” and/or “trends,” but should also boost the score of newer documents
    (more recent information).'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***信息的时间变化—*** 回想一下“第3章”中的“最新趋势”查询。[章节链接](kindle_split_015.xhtml#ch03)。这样的查询不仅应该匹配“最新”和/或“趋势”这两个词，还应该提高较新文档（更近期的信息）的分数。'
- en: '***All possible context clues—*** Look for signals to provide more context
    to the query. For example, look at the search logs to see whether a query was
    previously performed; if so, check the next query in the search log to see if
    there are any shared results, and give them a higher ranking.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***所有可能的上下文线索—*** 寻找提供更多查询上下文的信号。例如，查看搜索日志以查看查询是否之前已经执行过；如果是这样，检查搜索日志中的下一个查询以查看是否有任何共享的结果，并给予它们更高的排名。'
- en: 'We’ll now dive into answering the key question: how does the search engine
    decide how to rank search results with respect to a given query?'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将深入探讨回答关键问题：搜索引擎是如何决定如何根据给定的查询来排名搜索结果的？
- en: 5.2\. Retrieval models
  id: totrans-696
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 检索模型
- en: 'So far, we’ve talked about the task of ranking a document as a function that
    takes a document as input and generates a score value representing the document’s
    relevance. In practice, ranking functions are often part of an *information retrieval
    model* (IR model). Such a model defines how the search engine tackles the entire
    problem of providing relevant results with respect to an information need: from
    query parsing to matching, retrieving, and ranking search results. The rationale
    for having a model is that it’s hard to come up with a ranking function that gives
    an accurate score without knowing how the search engine handles a query. In a
    query like “+riemann -influenced influencing,” if a document contains both the
    terms “riemann” and “influencing,” the resulting final score should be a combination
    of the scores for the first and second terms (score = score(riemann) \+ score(influencing));
    but the “riemann” term has a mandatory constraint (the \+ sign), so it should
    contribute a higher score than “influencing,” which is optional.'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了将文档排名作为一个函数的任务，该函数接受一个文档作为输入并生成一个表示文档相关性的分数值。在实践中，排名函数通常是*信息检索模型*（IR模型）的一部分。这样的模型定义了搜索引擎如何处理与信息需求相关的整个问题，从查询解析到匹配、检索和排名搜索结果。拥有一个模型的原因是，在没有了解搜索引擎如何处理查询的情况下，很难提出一个能够给出准确分数的排名函数。在一个像“+riemann
    -influenced influencing”这样的查询中，如果一个文档同时包含“riemann”和“influencing”这两个术语，最终的分数应该是第一个和第二个术语分数的组合（分数
    = score(riemann) + score(influencing)）；但是“riemann”术语有一个强制约束（加号），因此它应该比可选的“influencing”术语贡献更高的分数。
- en: 'Thus the way a search engine calculates the relevance of a document with respect
    to a query has an impact on the design and infrastructure behind the search engine.
    Since [chapter 1](kindle_split_012.xhtml#ch01), we’ve assumed that when text is
    fed into the search engine, it’s analyzed and split into chunks that can be altered
    depending on tokenizers and token filters. This text analysis chain generates
    terms that end up in inverted indexes, also known as *posting lists*. The search-by-keyword
    use case motivated the choice of posting lists to efficiently retrieve documents
    by matching terms. Similarly, the choice of how to rank query-document pairs may
    impact system requirements: for example, the ranking function may need to access
    more information about the indexed data than just the presence or absence of a
    term in the posting list. A widely used set of retrieval models called *statistical
    models* makes decisions about ranking a certain document based on how frequently
    a matching term appears within a specific document and the entire document set.'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，搜索引擎计算文档与查询相关性的方式会影响搜索引擎的设计和基础设施。自[第1章](kindle_split_012.xhtml#ch01)以来，我们假设当文本输入搜索引擎时，它会被分析并分割成可以根据分词器和分词过滤器进行更改的块。这个文本分析链生成最终出现在倒排索引中的术语，也称为*倒排列表*。按关键词搜索用例促使选择倒排列表以有效地通过匹配术语检索文档。同样，选择如何对查询-文档对进行排名可能会影响系统需求：例如，排名函数可能需要访问比倒排列表中术语的存在或不存在更多的索引数据信息。一个广泛使用的检索模型集合称为*统计模型*，它根据匹配术语在特定文档和整个文档集中出现的频率来决定对某个文档进行排名。
- en: 'In previous chapters, we’ve already gone beyond simple matching of terms between
    queries and documents. We’ve used synonym expansion to generate synonym terms:
    for example, at search time, to extend the number of possible ways a user can
    “say” the same thing (at a word level). We expanded this approach in [chapter
    3](kindle_split_015.xhtml#ch03) by generating new alternative queries in addition
    to the original query entered by the user.'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们已经超越了查询和文档之间简单的术语匹配。我们使用了同义词扩展来生成同义词术语：例如，在搜索时，扩展用户“说”相同事情（在词级）的可能方式数量。我们在[第3章](kindle_split_015.xhtml#ch03)中扩展了这种方法，除了生成用户输入的原查询之外，还生成了新的替代查询。
- en: 'All this work aims to build a search engine that’s eager to understand the
    semantics of text:'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些工作的目标都是构建一个渴望理解文本语义的搜索引擎：
- en: '***In the synonym expansion case—*** Whether you type “hello” or “hi,” you’re
    *semantically* saying the same thing.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在同义词扩展的情况下——** 无论你输入“hello”还是“hi”，你都是在*语义上*表达相同的意思。'
- en: '***In the alternative query expansion case—*** If you type “latest trends,”
    you get alternative queries that are spelled differently but are semantically
    close to the original.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在替代查询扩展的情况下——** 如果你输入“最新趋势”，你会得到拼写不同但语义上接近原始查询的替代查询。'
- en: Overall, the (simplified) idea is that a document that’s relevant with respect
    to a certain query should be returned even if there’s no exact match between the
    query and the indexed terms. Synonyms and alternative query representations provide
    a wider range of relevant query terms that can match the document terms. Those
    methods make it more probable that you’ll find a document using semantically similar
    words or queries. In an ideal world, a search engine would go beyond query-document
    term matching and understand the user’s information need. Based on that, it would
    return results relevant to that need, again not constraining retrieval to term
    matching.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，（简化的）想法是，与某个查询相关的文档应该被返回，即使查询与索引术语之间没有完全匹配。同义词和替代查询表示提供了更广泛的与文档术语匹配的相关查询术语。这些方法使得你更有可能通过语义上相似的字词或查询找到文档。在一个理想的世界里，搜索引擎会超越查询-文档术语匹配，理解用户的信息需求。基于这一点，它会返回与该需求相关的结果，再次不将检索限制在术语匹配上。
- en: Creating a search engine with good semantic understanding capabilities is difficult.
    The good news is that, as you’ll see, techniques based on deep learning can help
    a lot in reducing the gap between a plain query string and the actual user intent.
    Think about the *thought vector* you briefly met in [chapter 3](kindle_split_015.xhtml#ch03)
    when we looked at seq2seq models. You can think of it as the kind of representation
    of user intent you need in order to go beyond simple term matching.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个具有良好语义理解能力的搜索引擎是困难的。好消息是，正如你将看到的，基于深度学习的技术可以在很大程度上帮助缩小普通查询字符串和实际用户意图之间的差距。回想一下[第3章](kindle_split_015.xhtml#ch03)中我们查看seq2seq模型时简要遇到的*思想向量*。你可以将其视为你需要的一种用户意图表示，以便超越简单的术语匹配。
- en: 'A good retrieval model should consider semantics. As you can imagine, this
    semantic perspective applies to ranking documents, as well. For example:'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的检索模型应该考虑语义。正如你可以想象的那样，这种语义视角也适用于文档排名。例如：
- en: When ranking a result whose matching terms came from one of the alternative
    queries generated by a LSTM network, should such documents score differently than
    documents that matched based on terms from the original user query?
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当对由LSTM网络生成的替代查询之一生成的匹配术语进行排名时，这些文档的分数应该与基于原始用户查询术语匹配的文档的分数不同吗？
- en: If you plan to use representations generated via deep learning (for example,
    thought vectors) to capture user intent, how do you use them to retrieve and rank
    results?
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你计划使用通过深度学习（例如，思想向量）生成的表示来捕捉用户意图，你将如何使用它们来检索和排名结果？
- en: 'We’ll now begin an exploration that will touch on the following:'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将开始一项探索，它将涉及以下内容：
- en: More-traditional retrieval models
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更传统的检索模型
- en: Extending traditional models that use vector representations of text learned
    through neural networks (this will be our main focus)
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展使用通过神经网络学习到的文本向量表示的传统模型（这将是我们的主要关注点）
- en: Neural IR models that rely purely on deep neural networks
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅依赖于深度神经网络的神经信息检索模型
- en: 5.2.1\. TF-IDF and the vector space model
  id: totrans-712
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. TF-IDF和向量空间模型
- en: In [chapter 1](kindle_split_012.xhtml#ch01), I mentioned term frequency–inverse
    document frequency (TF-IDF) and the vector space model (VSM). Let’s take a closer
    look at them to understand how they work. The fundamental purpose of a ranking
    function is to assign a score to a query-document pair. A common way to measure
    the importance of a document with respect to a query is based on calculating and
    fetching statistics for query and document terms. Such retrieval models are called
    *statistical models for information retrieval*.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](kindle_split_012.xhtml#ch01)中，我提到了词频-逆文档频率（TF-IDF）和向量空间模型（VSM）。让我们更深入地了解一下它们是如何工作的。排名函数的基本目的是为查询-文档对分配一个分数。衡量文档相对于查询重要性的一个常见方法是基于计算和检索查询和文档术语的统计数据。这类检索模型被称为*信息检索的统计模型*。
- en: 'Suppose you have the query “bernhard riemann influence” and two resulting documents:
    document1 = “riemann bernhard - life and works of bernhard riemann,” and document2
    = “thomas bernhard biography - bio and influence in literature.” Both the query
    and the documents are made up of terms. When you look at which of them matched,
    you observe the following:'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个查询“bernhard riemann influence”和两个结果文档：document1 = “riemann bernhard -
    bernhard riemann的生活和工作，”和document2 = “thomas bernhard传记 - 传记和文学中的影响。”查询和文档都由术语组成。当你查看哪些术语匹配时，你会观察到以下情况：
- en: Document1 matched the terms “riemann” and “bernhard.” Both terms matched twice.
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Document1匹配了术语“riemann”和“bernhard”。这两个术语各匹配了两次。
- en: Document2 matched the terms “bernhard” and “influence.” Both terms matched once.
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Document2匹配了术语“bernhard”和“influence”。这两个术语各匹配了一次。
- en: The term frequency for document1 is 2 for each matching term, and document2’s
    term frequency for its two matching terms is 1.
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于document1，每个匹配术语的词频是2，而document2的两个匹配术语的词频是1。
- en: The document frequency for “bernhard” is 2 (it appears in both documents; you
    don’t count repeated occurrences in a singular document). The document frequency
    for “riemann” is 1, and the document frequency for “influence” is 1.
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “bernhard”的文档频率是2（它在两个文档中都出现；你不会在单个文档中计算重复出现）。 “riemann”的文档频率是1，“influence”的文档频率也是1。
- en: '|  |'
  id: totrans-719
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Term frequency and document frequency**'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: '**词频和文档频率**'
- en: Often, statistical models combine *term frequency* and *document frequency*
    to come up with a measure of the relevance of a document, given a query. The rationale
    behind the choice of these metrics is that calculating frequencies and statistics
    about terms give you a measure of how informative each of them is. More specifically,
    the number of times a query term appears in a document gives a measure of how
    pertinent that document could be to that query; this is the *term frequency*.
    On the other hand, terms that rarely appear in the indexed data are considered
    more important and informative than more common terms (terms like “the” and “in”
    usually aren’t informative, because they’re much too common). The frequency of
    a term within all the indexed documents is called the *document frequency*.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，统计模型会将*术语频率*和*文档频率*结合起来，以衡量给定查询的文档的相关性。选择这些指标的理由是，计算术语的频率和统计数据可以衡量每个术语的信息量。更具体地说，查询术语在文档中出现的次数可以衡量该文档对该查询的相关性；这就是*术语频率*。另一方面，很少出现在索引数据中的术语被认为比更常见的术语（如“the”和“in”）更重要和更有信息量（因为这些术语太常见了，通常没有信息量）。术语在所有索引文档中的频率称为*文档频率*。
- en: '|  |'
  id: totrans-722
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: If you sum all the term frequencies of each matching term, the score is 4 for
    document1 and 2 for document2.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将每个匹配术语的术语频率相加，则document1的评分为4，document2的评分为2。
- en: Let’s add a document3 whose content is “riemann hypothesis - a deep dive into
    a mathematical mystery” and score it against the same query. Document3 has a score
    of 1 because only the “riemann” term matches. This isn’t good, because document3
    is more relevant than document2, although it isn’t pertinent to Riemann’s influence.
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们添加一个名为document3的文档，其内容为“黎曼猜想 - 深入数学之谜”并对其与相同查询进行评分。由于只有“riemann”这个术语匹配，document3的评分为1。这并不好，因为document3比document2更相关，尽管它与黎曼的影响无关。
- en: 'A better way to express ranking is to score each document using the sum of
    the logarithms of term frequencies divided by the logarithm of the document frequency.
    This famous weighting scheme is called TF-IDF:'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更好的表达排名的方法是使用术语频率的对数除以文档频率的对数的对数之和来评分每个文档。这个著名的加权方案被称为TF-IDF：
- en: weight(term) = (1 + log(tf(term))) * log(*N*/df(term))
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: weight(term) = (1 + log(tf(term))) * log(*N*/df(term))
- en: '*N* is the number of indexed documents. With the new document3 added, the document
    frequency for the term “riemann” is now 2. Using the previous equation for each
    matching term, you add each TF-IDF and obtain the following scores:'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: '*N* 是索引文档的数量。随着新文档3的添加，术语“riemann”的文档频率现在为2。使用每个匹配术语的先前方程，你将每个TF-IDF相加，得到以下评分：'
- en: score(document1) = tf-idf(*riemann*) \+ tf-idf(*bernhard*) = 1.28 \+ 1.28 =
    2.56
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: score(document1) = tf-idf(*riemann*) + tf-idf(*bernhard*) = 1.28 + 1.28 = 2.56
- en: score(document2) = tf-idf(*bernhard*) \+ tf-idf(*influence*) = 1 \+ 1 = 2
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: score(document2) = tf-idf(*bernhard*) + tf-idf(*influence*) = 1 + 1 = 2
- en: score(document3) = tf-idf(*riemann*) = 1
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: score(document3) = tf-idf(*riemann*) = 1
- en: You’ve just seen that TF-IDF–based scoring only relies on pure frequencies of
    terms, so a document that isn’t relevant (document2) is scored higher than a somewhat-relevant
    document (document3). This is a case where the retrieval model is missing semantic
    understanding of query intent, as discussed in the previous section.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚看到，基于TF-IDF的评分只依赖于术语的纯频率，因此一个不相关的文档（document2）的评分高于一个稍微相关的文档（document3）。这是一个检索模型缺少对查询意图语义理解的情况，如前节所述。
- en: In this book so far, you’ve encountered vectors many times. Using them in information
    retrieval isn’t a novel idea; VSM relies on representing queries and documents
    as vectors and measures how similar they are based on a TF-IDF weighting scheme.
    Each document can be represented by a one-dimensional vector with size equal to
    the number of existing terms in the index. Each position in the vector represents
    a term having a value equal to the TF-IDF value for that document for that term.
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你在本书中多次遇到了向量。在信息检索中使用它们并不是一个新想法；VSM依赖于将查询和文档表示为向量，并根据TF-IDF加权方案衡量它们的相似度。每个文档都可以用一个一维向量表示，其大小等于索引中现有术语的数量。向量中的每个位置代表一个术语，其值等于该文档中该术语的TF-IDF值。
- en: The same can be done for queries, because they’re also made up of terms; the
    only difference is the fact that term frequencies can be either local (frequency
    of query terms as they appear in the query) or from the index (frequency of query
    terms as they appear in the indexed data). This way, you represent documents and
    queries as vectors. This representation is called *bag-of-words*, because the
    information about positions of terms is lost—every document or query is represented
    as a collection of words, as in [table 5.1](#ch05table01).
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 对于查询也可以这样做，因为它们也由术语组成；唯一的区别是术语频率可以是局部的（查询中查询术语出现的频率）或来自索引（索引数据中查询术语出现的频率）。这样，你可以将文档和查询表示为向量。这种表示称为*词袋*，因为术语位置的信息丢失——每个文档或查询都表示为单词的集合，如[表5.1](#ch05table01)所示。
- en: Table 5.1\. Bag-of-words representations
  id: totrans-734
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表5.1\. 词袋表示
- en: '| Terms | bernhard | bio | dive | hypothesis | in | influence | into | life
    | mathematical | riemann |'
  id: totrans-735
  prefs: []
  type: TYPE_TB
  zh: '| 术语 | bernhard | bio | dive | hypothesis | in | influence | into | life |
    mathematical | riemann |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-736
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **doc1** | 1.28 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.28 |'
  id: totrans-737
  prefs: []
  type: TYPE_TB
  zh: '| **doc1** | 1.28 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.28 |'
- en: '| **doc2** | 1.0 | 1.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
  id: totrans-738
  prefs: []
  type: TYPE_TB
  zh: '| **doc2** | 1.0 | 1.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
- en: '| **doc3** | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 |'
  id: totrans-739
  prefs: []
  type: TYPE_TB
  zh: '| **doc3** | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 |'
- en: 'The vectors of “bernhard riemann influence” and “riemann influence bernhard”
    look exactly the same: the facts that the two queries are different and that the
    first query is more meaningful than the second one aren’t captured. Now that the
    documents and queries are represented in a vector space, you want to calculate
    which document best matches the input query. You do that by calculating the *cosine
    similarity* between each document and the input query; that will give you the
    final ranking for each document. The cosine similarity is a measure of the amplitude
    of the angle between a document and the query vectors. [Figure 5.2](#ch05fig02)
    shows vectors for the input query, document1, and document2 in a (simplified,
    two-dimensional) vector space that only considers the terms “bernhard” and “riemann.”'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: “bernhard riemann influence”和“riemann influence bernhard”的向量看起来完全相同：两个查询不同以及第一个查询比第二个查询更有意义的事实没有被捕捉到。现在，文档和查询以向量空间的形式表示，你想要计算哪个文档与输入查询最匹配。你通过计算每个文档与输入查询之间的*余弦相似度*来完成这项工作；这将给出每个文档的最终排名。余弦相似度是文档和查询向量之间角度振幅的度量。[图5.2](#ch05fig02)显示了输入查询、document1和document2在只考虑“bernhard”和“riemann”术语的（简化、二维）向量空间中的向量。
- en: Figure 5.2\. Cosine similarity
  id: totrans-741
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.2\. 余弦相似度
- en: '![](Images/05fig02_alt.jpg)'
  id: totrans-742
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig02_alt.jpg)'
- en: 'The similarity between the query vector and a document is evaluated by looking
    at the existing angle between the two vectors. The smaller the angle, the more
    similar the two vectors are. When you apply this to the vectors from [table 5.1](#ch05table01),
    you get the following similarity scores:'
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看两个向量之间的现有角度来评估查询向量和文档之间的相似度。角度越小，两个向量越相似。当你将此应用于[表5.1](#ch05table01)中的向量时，你会得到以下相似度分数：
- en: '[PRE59]'
  id: totrans-744
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: With just three documents, the resulting vector’s size is 10 (the number of
    columns equals the number of the terms used across the documents). In production
    systems, this value would be much higher. So one problem with this bag-of-words
    representation is that the size of vectors grows linearly with the number of existing
    terms (all the distinct words contained in the indexed documents). This is another
    reason word vectors like those generated by word2vec are better than bag-of-words
    vectors. Word2vec-generated vectors have a fixed size, so they don’t grow with
    the number of terms in the search engine; therefore, resource consumption is much
    lower when using them. (Word2vec-generated vectors do better at capturing word
    semantics, as explained in [chapter 2](kindle_split_013.xhtml#ch02).)
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用三个文档，得到的向量大小为10（列数等于文档中使用的术语数量）。在生产系统中，这个值会高得多。因此，这种词袋表示法的一个问题是向量的尺寸会随着现有术语（索引文档中包含的所有不同单词）数量的增加而线性增长。这也是为什么像word2vec生成的词向量比词袋向量更好的另一个原因。Word2vec生成的向量具有固定的大小，因此它们不会随着搜索引擎中术语数量的增加而增长；因此，使用它们时的资源消耗要低得多。（Word2vec生成的向量在捕捉词义方面做得更好，如第2章所述。）
- en: Despite these limitations, VSM and TF-IDF are used often with good results in
    many production systems. Before discussing other information retrieval models,
    let’s get pragmatic, ingest the documents with Lucene, and see how they’re scored
    using TF-IDF and VSM.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些限制，VSM 和 TF-IDF 在许多生产系统中经常使用，并且取得了良好的效果。在讨论其他信息检索模型之前，让我们务实一些，使用 Lucene
    读取文档，并查看它们如何使用 TF-IDF 和 VSM 进行评分。
- en: 5.2.2\. Ranking documents in Lucene
  id: totrans-747
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 在 Lucene 中排名文档
- en: In Lucene, the `Similarity` API serves as the base for ranking functions. Lucene
    comes with some information retrieval models implemented out of the box, such
    as VSM with TF-IDF (which was the default used up to version 5), Okapi BM25, Divergence
    from Randomness, language models, and others. `Similarity` needs to be set at
    both indexing and search time. In Lucene 7, the VSM + TF-IDF similarity is `ClassicSimilarity`.
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Lucene 中，`Similarity` API 作为排名函数的基础。Lucene 内置了一些信息检索模型，例如 VSM 与 TF-IDF（这是版本
    5 之前使用的默认设置），Okapi BM25，随机性差异，语言模型等。`Similarity` 需要在索引和搜索时间都进行设置。在 Lucene 7 中，VSM
    + TF-IDF 相似度为 `ClassicSimilarity`。
- en: 'At index time, `Similarity` is set in the `IndexWriterConfig`:'
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 在索引时间，`Similarity` 在 `IndexWriterConfig` 中设置：
- en: '[PRE60]'
  id: totrans-750
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '***1* Creates a configuration for indexing**'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 创建索引配置**'
- en: '***2* Sets the similarity to ClassicSimilarity**'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 设置相似度为 ClassicSimilarity**'
- en: '***3* Creates an IndexWriter using the configured Similarity**'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 使用配置的相似度创建 IndexWriter**'
- en: 'At search time, `Similarity` is set in the `IndexSearcher`:'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索时间，`Similarity` 在 `IndexSearcher` 中设置：
- en: '[PRE61]'
  id: totrans-755
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '***1* Opens an IndexReader**'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 打开 IndexReader**'
- en: '***2* Creates an IndexSearcher over the reader**'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 在读取器上创建 IndexSearcher**'
- en: '***3* Sets Similarity in the IndexSearcher**'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 在 IndexSearcher 中设置 Similarity**'
- en: 'If you index and search over the earlier three documents, you can see whether
    the ranking behaves as you expect:'
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你索引和搜索前三个文档，你可以看到排名是否如你所预期：
- en: '[PRE62]'
  id: totrans-760
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '***1* You can define the features of a Lucene field yourself (storing values,
    storing term positions, and so on).**'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 你可以自己定义 Lucene 字段的特性（存储值、存储词位置等）。**'
- en: '***2* For each document, creates a new Document and adds the contents in a
    title field**'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 为每个文档创建一个新的 Document 并在标题字段中添加内容**'
- en: '***3* Adds all three documents and commits the changes**'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 添加所有三个文档并提交更改**'
- en: 'To check how each search result is scored by the `Similarity` class with respect
    to a query, you can ask Lucene to “explain” it. The output of an `explain` consists
    of text describing how each matching term contributes to the final score of each
    search result:'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查每个搜索结果是如何根据查询由 `Similarity` 类评分的，你可以要求 Lucene “解释”它。`explain` 的输出由描述每个匹配术语如何贡献每个搜索结果最终分数的文本组成：
- en: '[PRE63]'
  id: totrans-765
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '***1* Writes a query**'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 编写查询**'
- en: '***2* Parses the user-entered query**'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 解析用户输入的查询**'
- en: '***3* Performs the search**'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 执行搜索**'
- en: '***4* Prints the document title and score on the standard output**'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 在标准输出上打印文档标题和分数**'
- en: '***5* Gets an explanation of how the score was calculated**'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 获取分数计算的解释**'
- en: 'With `ClassicSimilarity`, you get the following `explain` output:'
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `ClassicSimilarity`，你将得到以下 `explain` 输出：
- en: '[PRE64]'
  id: totrans-772
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'As expected, the ranking respects what’s described in the previous section.
    You can see from the explanation that each term that matched the query contributes
    according to its weight as they’re summed:'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，排名尊重了上一节中描述的内容。你可以从解释中看到，每个与查询匹配的术语都根据其权重进行贡献，它们被相加：
- en: '[PRE65]'
  id: totrans-774
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: On the other hand, the scores aren’t exactly the same as when you manually compute
    TF-IDF weights for terms. The reason is that there are many possible variations
    of TF-IDF schemes. For example, here Lucene calculates inverse document frequency
    as
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，分数与手动计算 TF-IDF 权重的分数并不完全相同。原因是 TF-IDF 方案有许多可能的变体。例如，这里 Lucene 计算逆文档频率为
- en: log(*N*+1)/(df(term)+1)
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: log(*N*+1)/(df(term)+1)
- en: instead of
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是
- en: log(*N*/df(term))
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: log(*N*/df(term))
- en: 'Additionally, Lucene doesn’t take the logarithm of the term frequency but rather
    uses the term frequency as it is. Lucene also uses *normalization*, a technique
    to mitigate the fact that documents with more terms would score too high with
    respect to short documents (with fewer terms), which can be approximated as `1.0
    / Math.sqrt(numberOfTerms))`. Using this normalization technique, calculating
    the cosine similarity between a query vector and a document vector is equivalent
    to calculating their scalar product:'
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Lucene 不对词频取对数，而是直接使用词频。Lucene 还使用 *规范化*，这是一种减轻文档中词数较多相对于词数较少的短文档（词数较少）评分过高的技术，可以近似为
    `1.0 / Math.sqrt(numberOfTerms))`。使用这种规范化技术，计算查询向量与文档向量之间的余弦相似度相当于计算它们的标量积：
- en: '[PRE66]'
  id: totrans-780
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Lucene doesn’t store vectors. It’s enough to be able to compute the TF-IDF for
    each matching term and combine the results to compute the score.
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: Lucene 不存储向量。能够计算每个匹配词的 TF-IDF 并将结果组合起来计算分数就足够了。
- en: 5.2.3\. Probabilistic models
  id: totrans-782
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. 概率模型
- en: You’ve learned about some VSM theory and how it’s applied in practice in Lucene.
    You’ve also seen scores being calculated using term statistics. In this section,
    you’ll learn about probabilistic retrieval models, where scores are still calculated
    on the basis of probabilities. The search engine ranks a document using its probability
    of relevance with respect to the query.
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解了某些 VSM 理论及其在 Lucene 中的实际应用。你也看到了使用词频统计计算分数。在本节中，你将了解概率检索模型，其中分数仍然基于概率计算。搜索引擎使用文档相对于查询的相关性概率来对文档进行排序。
- en: 'Probabilities are a powerful tool to address uncertainty. We’ve discussed how
    hard it is to bridge the gap between user intent and relevant search results.
    Probabilistic models try to model ranking by measuring how probable it is that
    a certain document is relevant with respect to the input query. If you roll a
    six-sided die, each side has a 1/6 probability of being the result: for example,
    the probability of rolling a 3 is P(3) = 0.16\. But in practice, if you roll a
    die six times, you probably won’t get all six different results. Probability is
    an estimation of how likely a certain event is to occur—this doesn’t imply that
    it will occur exactly that often.'
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 概率是解决不确定性的强大工具。我们讨论了如何弥合用户意图与相关搜索结果之间的差距。概率模型试图通过测量某个文档相对于输入查询的相关性概率来建模排序。如果你掷一个六面的骰子，每个面有
    1/6 的概率成为结果：例如，掷出 3 的概率是 P(3) = 0.16。但在实际中，如果你掷骰子六次，你很可能不会得到所有六个不同的结果。概率是对某个事件发生的可能性的估计——这并不意味着它将恰好发生那么频繁。
- en: 'The unconditional probability of rolling any number on a die is 1/6, but what
    about the probability of rolling two consecutive results that are the same? Such
    a conditional probability can be expressed as P(event|condition). For the ranking
    task, you can estimate the probability that a certain document is relevant (with
    respect to a given query). This is represented as P(*r* = 1|*x*), where *r* is
    a binary measure of relevance:'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 掷骰子得到任何数字的无条件概率是 1/6，但连续掷出相同结果的概率是多少？这样的条件概率可以表示为 P(event|condition)。对于排序任务，你可以估计某个文档相对于给定查询的相关性概率。这表示为
    P(*r* = 1|*x*)，其中 *r* 是一个二元的相关性度量：
- en: '*r* = 1 : relevant, *r* = 0 : not relevant'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*r* = 1 : 相关，*r* = 0 : 不相关'
- en: 'In a probabilistic retrieval model, you generally rank all documents with respect
    to a given query by P(*r* = 1|*x*). This is best expressed by the *probability
    ranking principle*: if retrieved documents are ordered by decreasing probability
    of relevance to the data available, then the system’s effectiveness is the best
    that can be obtained for the data.'
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率检索模型中，你通常通过 P(*r* = 1|*x*) 对给定查询的所有文档进行排序。这最好用 *概率排序原则* 来表达：如果检索到的文档按与可用数据的关联性递减的概率排序，那么系统对于这些数据的效率就是最佳。
- en: 'One of the most famous and widely adopted probabilistic models is *Okapi BM25*.
    Briefly, it tries to mitigate two limitations of TF-IDF:'
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名且广泛采用的概率模型之一是 *Okapi BM25*。简而言之，它试图减轻 TF-IDF 的两个局限性：
- en: Limit the impact of term frequency to avoid excessive scoring based on frequently
    repeated terms.
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制词频的影响，以避免过度依赖频繁重复的词进行评分。
- en: Provide a better estimate of the importance of the document frequency of a certain
    term.
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供对某个词的文档频率重要性的更好估计。
- en: BM25 expresses the conditional probability P(*r* = 1|*x*) by means of two probabilities
    that depend on term frequencies. So BM25 approximates probabilities by calculating
    probability distribution over term frequencies.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: BM25通过两种依赖于词频的概率来表达条件概率 P(*r* = 1|*x*)。因此，BM25通过计算词频的概率分布来近似概率。
- en: 'Consider the “bernhard riemann influence” example. In a classic TF-IDF scheme,
    a high term frequency can lead to a high score. So, if you have a dummy document4
    that contains lots of “bernhard” occurrences (“bernhard bernhard bernhard bernhard
    bernhard bernhard bernhard bernhard bernhard bernhard”), it may score higher than
    more-relevant documents. If you index it into the previously built index, you
    get the following outputs with TF-IDF and VSM (`ClassicSimilarity`):'
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 以“伯恩哈德·黎曼影响”为例。在经典的TF-IDF方案中，高词频可能导致高分数。因此，如果你有一个包含大量“伯恩哈德”出现的虚拟文档4（“伯恩哈德 伯恩哈德
    伯恩哈德 伯恩哈德 伯恩哈德 伯恩哈德 伯恩哈德 伯恩哈德”），它的分数可能高于更相关的文档。如果你将其索引到先前构建的索引中，你会得到以下TF-IDF和VSM
    (`ClassicSimilarity`) 的输出：
- en: '[PRE67]'
  id: totrans-793
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'As you can see, the dummy document is returned as the second result, which
    is strange. Additionally, document4’s score is almost equal to the first-ranked
    result: the search engine ranked this dummy document as important, but it isn’t.
    Let’s set the `BM25Similarity` in Lucene (the default since version 6) using the
    same code as for the `ClassicSimilarity` tests:'
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，虚拟文档作为第二个结果返回，这很奇怪。此外，文档4的分数几乎与排名第一的结果相等：搜索引擎将这个虚拟文档视为重要，但实际上它并不是。让我们使用与`ClassicSimilarity`测试相同的代码设置Lucene中的`BM25Similarity`（自版本6以来是默认设置）：
- en: '[PRE68]'
  id: totrans-795
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'With BM25 similarity set, the ranking is as follows:'
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 使用BM25相似度集，排名如下：
- en: '[PRE69]'
  id: totrans-797
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The dummy document is ranked third instead of second. Although this isn’t optimal,
    the score has greatly decreased as compared to the most relevant document. The
    reason is that BM25 “squashes” the term frequency to keep it below a certain configurable
    threshold. In this case, BM25 mitigated the impact of the high term frequency
    for the term “bernhard.”
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟文档被排名第三而不是第二。虽然这并不理想，但与最相关的文档相比，分数已经大幅下降。原因是BM25“压缩”了词频，以保持其低于某个可配置的阈值。在这种情况下，BM25减轻了“伯恩哈德”一词的高词频的影响。
- en: The other good thing about BM25 is that it tries to estimate the probability
    of terms that appear together in a document. The document frequency of a number
    of terms in a document is given by the sum of the logs of the probability that
    each single term appears in that document.
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: BM25的另一个优点是它试图估计文档中共同出现的词的概率。文档中多个词的文档频率由每个单独词在该文档中出现的概率的对数之和给出。
- en: 'But BM25 also has some limitations:'
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 但BM25也有一些局限性：
- en: Like TF-IDF, BM25 is a bag-of-words model, so it disregards term ordering when
    ranking.
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与TF-IDF一样，BM25是一个词袋模型，因此在排名时不考虑词的顺序。
- en: Although in general it performs well, BM25 is based on heuristics (functions
    that reach a fairly good result but aren’t guaranteed to work in general) that
    may not apply well to your data (you may have to adjust those heuristics).
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然总体上表现良好，但BM25基于启发式方法（达到相当好的结果但并不保证在一般情况下都有效）可能不适合你的数据（你可能需要调整这些启发式方法）。
- en: BM25 performs some approximation and simplification on probability estimation,
    which causes less-acceptable results in some cases (it doesn’t work well with
    long documents).
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BM25在概率估计上进行了一些近似和简化，这在某些情况下会导致不太令人满意的结果（它不适合长文档）。
- en: Other probabilistic approaches to ranking based on language models are generally
    better at plain probability estimations, but this doesn’t always result in better
    scoring. In general, BM25 is an okay baseline ranking function.
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: 基于语言模型的排名的其他概率方法通常在普通概率估计方面表现更好，但这并不总是导致更好的评分。一般来说，BM25是一个不错的基线排名函数。
- en: Now that we’ve explored some of the most commonly used ranking models for search
    engines, let’s dive into how neural networks can help make those models better
    and also provide completely new (and better) ranking models.
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了搜索引擎中最常用的排名模型，让我们深入了解神经网络如何帮助改进这些模型，并提供完全新的（更好的）排名模型。
- en: 5.3\. Neural information retrieval
  id: totrans-806
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3. 神经信息检索
- en: So far, we’ve tackled the problem of effective ranking by looking at terms and
    their local (per document) and global (per collection) frequencies. If you want
    to use neural networks to obtain a better ranking function, you need to think
    in terms of vectors. Actually, this doesn’t solely apply to neural networks. You’ve
    seen that even the classic VSM treats documents and queries as vectors and measures
    their similarity using cosine distance. One problem is that the size of such vectors
    can grow enormously (linearly) with the number of indexed words.
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们通过查看术语及其局部（每个文档）和全局（每个集合）频率来处理有效排名的问题。如果你想使用神经网络获得更好的排名函数，你需要从向量的角度思考。实际上，这不仅仅适用于神经网络。你已经看到，即使是经典的VSM也将文档和查询视为向量，并使用余弦距离来衡量它们的相似性。一个问题是这样向量的大小可以随着索引单词数量的增加而急剧增长（线性增长）。
- en: 'Before neural information retrieval, other techniques were developed to provide
    more compact (fixed-size) representations of words. These were mainly based on
    matrix-factorization algorithms such as the *latent semantic indexing* (LSI) algorithm,
    which is based on *singular value decomposition* (SVD) factorization. In short,
    in LSI, you create a matrix of terms and documents for each document row: put
    a 1 in each element where the document contains the corresponding term, and a
    0 for all others. Then transform (factorize) this sparse matrix (lots of zeros)
    with a reduced SVD factorization method, resulting in three (denser) matrixes
    whose product is a good approximation of the original. Each resulting document
    row has fixed dimensionality and is no longer sparse. Query vectors can also be
    transformed using SVD factorized matrixes. (A somewhat similar technique is called
    *latent Dirichlet allocation* [LDA].) The “juice” here is that no term matching
    is required; query and document vectors are compared so that the most similar
    document vectors are ranked first.'
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络信息检索之前，已经开发出其他技术来提供更紧凑（固定大小）的词表示。这些技术主要基于矩阵分解算法，如基于**奇异值分解**（SVD）分解的**潜在语义索引**（LSI）算法。简而言之，在LSI中，为每个文档行创建一个术语和文档的矩阵：在文档包含相应术语的每个元素中放置一个1，其他所有元素为0。然后使用简化的SVD分解方法转换（分解）这个稀疏矩阵（许多零），得到三个（更密集）的矩阵，它们的乘积是原始矩阵的良好近似。每个结果文档行具有固定的维度，并且不再是稀疏的。查询向量也可以使用SVD分解矩阵进行转换。（一种类似的技术称为**潜在狄利克雷分配**[LDA]。）这里的“精华”在于不需要术语匹配；查询和文档向量通过比较来排序，使得最相似的文档向量排在前面。
- en: Learning good representations of data is one of the tasks deep learning can
    do best. We’ll now look at using such vector representations for ranking. You’re
    already familiar with the algorithm we’ll use—word2vec—which learns distributed
    representations of words. Word vectors are located close to one another when the
    words they represent appear in similar contexts and, hence, have similar semantics.
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: 学习良好的数据表示是深度学习可以做得最好的任务之一。我们现在将探讨使用这种向量表示进行排名。你已经熟悉我们将使用的算法——word2vec，它学习单词的分布式表示。当表示的单词出现在相似上下文中时，它们的单词向量彼此靠近，因此具有相似的语义。
- en: 5.4\. From word to document vectors
  id: totrans-810
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 从单词到文档向量
- en: Let’s start building a retrieval system based on vectors generated by word2vec.
    The goal is to rank documents against queries, but word2vec gives vectors for
    words, not sequences of words. So the first thing to do is find a way to use these
    word vectors to represent documents and queries. A query will usually be composed
    of more than one word, as will indexed documents. For example, let’s take the
    word vectors for each of the terms in the query “bernhard riemann influence” and
    plot them as shown in [figure 5.3](#ch05fig03).
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从基于word2vec生成的向量构建检索系统开始。目标是按查询对文档进行排名，但word2vec给出的是单词的向量，而不是单词序列的向量。所以首先要做的是找到一种方法来使用这些单词向量来表示文档和查询。查询通常由多个单词组成，索引文档也是如此。例如，让我们取查询“bernhard
    riemann influence”中每个术语的单词向量，并如图[图5.3](#ch05fig03)所示进行绘制。
- en: Figure 5.3\. Word vectors for “bernhard,” “riemann,” and “influence”
  id: totrans-812
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.3\. “bernhard”、“riemann”和“influence”的单词向量
- en: '![](Images/05fig03_alt.jpg)'
  id: totrans-813
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig03_alt.jpg)'
- en: 'A simple method to create document vectors from word vectors is to average
    the word vectors into a single document vector. This is a straightforward mathematical
    operation: every element at position *j* in each vector is added, and the total
    is then divided by the number of vectors being averaged (the same as an arithmetic
    averaging operation). You can do that with DL4J vectors (`INDArrays` objects)
    as follows:'
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: '***1* 包含查询中输入的术语（“bernhard”，“riemann”和“influence”）的数组**'
- en: '[PRE70]'
  id: totrans-815
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: The `mean` vector is the result of the averaging operation. In [figure 5.4](#ch05fig04),
    as expected, the average vector sits at the center of the three word vectors.
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: '`mean`向量是平均操作的结果。在[图5.4](#ch05fig04)中，正如预期的那样，平均向量位于三个词向量的中心。'
- en: Figure 5.4\. Averaging the “bernhard,” “riemann,” and “influence” word vectors
  id: totrans-817
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.4\. 平均“bernhard”，“riemann”和“influence”词向量
- en: '![](Images/05fig04_alt.jpg)'
  id: totrans-818
  prefs: []
  type: TYPE_IMG
  zh: 从词向量创建文档向量的简单方法是将词向量平均到一个文档向量中。这是一个直接的数学操作：每个向量中位置*j*的每个元素相加，然后将总和除以正在平均的向量数量（与算术平均操作相同）。你可以使用DL4J向量（`INDArrays`对象）这样做：
- en: Note that this technique can be applied to both documents and queries, because
    they’re compositions of words. For each document-query pair, you can calculate
    the document vectors by averaging the word vectors and then assign the score to
    each document based on how close their respective averaged word vectors are. This
    is similar to what you do in the VSM scenario; the big difference is that the
    values of these document vectors aren’t calculated using TF-IDF but come from
    averaging word2vec vectors. In summary, these dense vectors are less heavy in
    terms of required memory (and space, if stored to disk) and more informative in
    terms of semantics.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这项技术可以应用于文档和查询，因为它们都是单词的组合。对于每个文档-查询对，你可以通过平均词向量来计算文档向量，然后根据各自的平均词向量之间的接近程度为每个文档分配分数。这与你在VSM场景中所做的工作类似；主要区别在于这些文档向量的值不是使用TF-IDF计算的，而是来自平均word2vec向量。总之，这些密集向量在所需内存（以及如果存储到磁盘上的空间）方面更轻，在语义方面更具有信息量。
- en: 'Let’s repeat the earlier experiment but rank documents using averaged word
    vectors. You first feed word2vec data from the search engine:'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: '***2* 创建一个DL4J迭代器，可以从标题字段上的读取器中读取数据**'
- en: '[PRE71]'
  id: totrans-821
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '***1* Creates a reader over the search engine document set**'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 在搜索引擎文档集上创建一个读取器**'
- en: '***2* Creates a DL4J iterator that can read data from the reader on the title
    field**'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦提取了词向量，你就可以构建查询和文档向量：
- en: '***3* Configures word2vec**'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 配置word2vec**'
- en: '***4* You’re working with a super-small dataset, so you use very small vectors.**'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 由于你正在处理一个非常小的数据集，所以你使用非常小的向量。**'
- en: '***5* Lets word2vec learn word vectors**'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 让word2vec学习词向量**'
- en: 'Once you’ve extracted the word vectors, you can build query and document vectors:'
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: '![](Images/05fig04_alt.jpg)'
- en: '[PRE72]'
  id: totrans-828
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '***1* Array containing the terms entered in the query (“bernhard,” “riemann,”
    and “influence”)**'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们重复之前的实验，但使用平均词向量对文档进行排序。你首先从搜索引擎中提供word2vec数据：
- en: '***2* Converts the query terms into a query vector by averaging the word vectors
    of the query terms**'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 通过平均查询项的词向量将查询项转换为查询向量**'
- en: '***3* For each search result: ignores the score as given by Lucene and transforms
    the results into document vectors**'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 对于每个搜索结果：忽略Lucene给出的分数，并将结果转换为文档向量**'
- en: '***4* Gets the document title**'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 获取文档标题**'
- en: '***5* Extracts the terms contained in that document (using the IndexReader#getTermVector
    API)**'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 从该文档中提取包含的术语（使用IndexReader#getTermVector API）**'
- en: '***6* Converts the document terms into a document vector using the averaging
    technique shown earlier**'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 使用前面展示的平均技术将文档项转换为文档向量**'
- en: '***7* Calculates the cosine similarity between the query and document vector
    and prints it**'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 计算查询向量和文档向量之间的余弦相似度并打印出来**'
- en: 'For the sake of readability, the outputs are shown manually ordered from highest
    to lowest scores:'
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高可读性，输出结果按照从高到低的分数手动排序：
- en: '[PRE73]'
  id: totrans-837
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '***1* The top-scored document is relevant, regardless of low term frequency.**'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 包含查询中输入的术语（“bernhard”，“riemann”和“influence”）的数组**'
- en: '***2* The second document isn’t relevant with respect to user intent.**'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 第二个文档与用户意图无关。**'
- en: '***3* The dummy document**'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 模拟文档**'
- en: '***4* The (probably) most relevant document has the lowest score.**'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* （可能）最相关的文档具有最低的分数。**'
- en: 'This is strange: the technique you expected to help you get a better ranking
    ranked the dummy document better than the most relevant one! The reasons are the
    following:'
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: 这很奇怪：你期望帮助你获得更好排名的技术反而使伪文档比最相关的文档排名更好！原因如下：
- en: There isn’t enough training data available for word2vec to provide word vectors
    that carefully represent word semantics. Four short documents contain far too
    few word-context pairs for the word2vec neural network to adjust its hidden layer
    weights accurately.
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用于word2vec的培训数据不足，无法提供仔细表示单词语义的单词向量。四个简短的文档包含的单词上下文对太少，以至于word2vec神经网络无法准确调整其隐藏层权重。
- en: If you pick the document vector of the top-scored document, it will be equal
    to the word vector for the word “bernhard.” The query vector is an average vector
    of the vectors for “bernhard,” “riemann,” and “influence”; therefore, these vectors
    will always be close together in the vector space.
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你选择得分最高的文档的文档向量，它将等于单词“bernhard”的单词向量。查询向量是“bernhard”、“riemann”和“influence”的向量的平均值；因此，这些向量在向量空间中总是彼此靠近。
- en: 'Let’s visualize the second statement by plotting the generated document-query
    vectors in a (reduced) two-dimensional space: see [figure 5.5](#ch05fig05). As
    expected, document4 and the query embeddings are so close that their labels almost
    overlap.'
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过在（减少的）二维空间中绘制生成的文档-查询向量来可视化第二个陈述：参见[图5.5](#ch05fig05)。正如预期的那样，document4和查询嵌入非常接近，以至于它们的标签几乎重叠。
- en: Figure 5.5\. Similarity between query and document embeddings
  id: totrans-846
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.5\. 查询和文档嵌入之间的相似性
- en: '![](Images/05fig05_alt.jpg)'
  id: totrans-847
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig05_alt.jpg)'
- en: 'One way to improve these results is to make sure the word2vec algorithm has
    more training data. For example, you can start with an English dump from Wikipedia
    and index each page’s title and content in Lucene. Additionally, you can mitigate
    the impact of text fragments like the one from document4, which mostly (or only)
    contains single terms that also appear in the query. A common technique to do
    so is to smooth the averaged document vectors by using term frequencies. Instead
    of dividing each word vector by the document length, you divide each word vector
    by its term frequency according to the following pseudo-code:'
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 提高这些结果的一种方法是要确保word2vec算法有更多的培训数据。例如，你可以从维基百科的英文数据开始，并在Lucene中索引每页的标题和内容。此外，你可以通过使用词频来平滑平均文档向量来减轻来自像document4这样的文本片段的影响，这些文本片段主要（或仅）包含在查询中也出现的单个术语。这样做的一种常见技术是使用词频来平滑平均文档向量。而不是将每个单词向量除以文档长度，你根据以下伪代码将每个单词向量除以其词频：
- en: '[PRE74]'
  id: totrans-849
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'This can be implemented in Lucene and DL4J as follows:'
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以在Lucene和DL4J中如下实现：
- en: '[PRE75]'
  id: totrans-851
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '***1* Initializes all the vector values to zero**'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 将所有向量值初始化为零**'
- en: '***2* Iterates over all the existing terms of the current doc**'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 遍历当前文档的所有现有术语**'
- en: '***3* Fetches the next term**'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 获取下一个术语**'
- en: '***4* Obtains the term-frequency value for the current term**'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 获取当前术语的词频值**'
- en: '***5* Extracts the word embedding for the current term and then divides its
    values by the term-frequency value**'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 提取当前术语的单词嵌入，然后将其值除以词频值**'
- en: '***6* Sums the current vector for the current term with the vector to be returned**'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 将当前术语的当前向量与要返回的向量相加**'
- en: When I introduced averaged word vectors, you saw that such document vectors
    are placed right at the center of their composing word vectors. In [figure 5.6](#ch05fig06),
    you can see that term-frequency smoothing can help detach the generated document
    vectors from sitting at the center of the word vectors, nearer to the less-frequent
    (and hopefully more important) word.
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: 当我介绍平均单词向量时，你看到这些文档向量正好位于其组成单词向量的中心。在[图5.6](#ch05fig06)中，你可以看到词频平滑可以帮助将生成的文档向量从单词向量的中心分离出来，更靠近出现频率较低（并且希望更重要）的单词。
- en: Figure 5.6\. Averaged word vector smoothed by term frequencies
  id: totrans-859
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.6\. 通过词频平滑的平均单词向量
- en: '![](Images/05fig06_alt.jpg)'
  id: totrans-860
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig06_alt.jpg)'
- en: 'The terms “bernhard” and “riemann” are more frequent than “influence,” and
    the generated document vector `tf` is closer to the `influence` word vector. This
    has a positive impact: documents whose term frequency is low are better ranked
    but still lie close enough to the query vector:'
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: 术语“bernhard”和“riemann”比“influence”更常见，生成的文档向量`tf`更接近`influence`单词向量。这有积极的影响：词频低的文档排名更好，但仍然足够接近查询向量：
- en: '[PRE76]'
  id: totrans-862
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'For the first time, the dummy document receives the lowest score. If you switch
    from plain term frequencies to TF-IDF as smoothing factors for generating averaged
    document vectors from word embeddings, you get the following ranking:'
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: 首次，虚拟文档获得了最低的分数。如果你将平文词频转换为TF-IDF作为从词嵌入生成平均文档向量的平滑因子，你会得到以下排名：
- en: '[PRE77]'
  id: totrans-864
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'With the TF-IDF–based smoothing (see, for example, [figure 5.7](#ch05fig07)),
    the ranking of documents is the best you can achieve. You got away from strict
    term weighting–based similarity: the most relevant document has a term frequency
    of 1 for the term “riemann,” whereas the document with the highest term frequency
    has the lowest score. From a semantic perspective, the most relevant documents
    are scored higher than the others.'
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: 基于TF-IDF的平滑（例如，参见[图5.7](#ch05fig07)），文档的排名是你可以达到的最佳水平。你摆脱了基于严格词频权重的相似度：最相关的文档在“riemann”一词上的词频为1，而词频最高的文档得分最低。从语义角度来看，最相关的文档得分高于其他文档。
- en: Figure 5.7\. Averaged word vector smoothed with TF-IDF
  id: totrans-866
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.7\. 使用TF-IDF平滑的平均词向量
- en: '![](Images/05fig07_alt.jpg)'
  id: totrans-867
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig07_alt.jpg)'
- en: 5.5\. Evaluations and comparisons
  id: totrans-868
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5\. 评估和比较
- en: 'Are you happy with this means of ranking documents based on TF-IDF averaged
    word vectors? In the previous example, you trained word2vec with specific settings:
    layer size set to 60, skip-gram model, window size set to 6, and so on. The ranking
    was optimized with respect to a specific query and a set of four documents. Although
    this is a useful exercise for learning the pros and cons of different approaches,
    you can’t do such fine-grained optimizations for all possible input queries, especially
    for large knowledge bases. Given that relevance is so difficult to get right,
    it’s good to find ways to automate the evaluation of ranking effectiveness. So
    before we jump on other ways to address ranking (such as with the help of neural
    text embeddings), let’s quickly introduce some tooling to speed up evaluating
    ranking functions.'
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: 你对基于TF-IDF平均词向量的文档排名方式满意吗？在之前的例子中，你使用特定的设置训练了word2vec：层大小设置为60，skip-gram模型，窗口大小设置为6，等等。排名是根据特定的查询和一组四个文档进行优化的。尽管这是一个了解不同方法优缺点的有用练习，但你无法对所有可能的输入查询进行如此精细的优化，尤其是对于大型知识库。鉴于相关性如此难以准确获得，找到自动化评估排名有效性的方法是个不错的选择。因此，在我们探讨其他解决排名问题的方法（例如，借助神经文本嵌入）之前，让我们快速介绍一些工具，以加快评估排名函数的速度。
- en: 'A nice tool for evaluating the effectiveness of Lucene-based search engines
    is Lucene for Information Retrieval (Lucene4IR). It originated from a collaboration
    between people from research and industry.^([[1](#ch05fn01)]) A quick tutorial
    can be found at [http://mng.bz/YP7N](http://mng.bz/YP7N). Lucene4IR makes it possible
    to try out different indexing, retrieval, and ranking strategies over standard
    information retrieval datasets. To try it, you can run Lucene4IR’s `IndexerApp`,
    `RetrievalApp`, and `ExampleStatsApp` in sequence. Doing so will index, search,
    and record statistics over returned versus relevant results: for example, according
    to the chosen Lucene configuration (`Similarity`, `Analyzers`, and so on). By
    default, these apps run on the CACM dataset ([http://mng.bz/GWZq](http://mng.bz/GWZq))
    using `BM25Similarity`.'
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: 评估基于Lucene的搜索引擎有效性的一个不错工具是Lucene for Information Retrieval (Lucene4IR)。它源于研究者和行业人士之间的合作.^([[1](#ch05fn01)])
    可以在[http://mng.bz/YP7N](http://mng.bz/YP7N)找到快速教程。Lucene4IR使得在标准信息检索数据集上尝试不同的索引、检索和排名策略成为可能。要尝试它，你可以依次运行Lucene4IR的`IndexerApp`、`RetrievalApp`和`ExampleStatsApp`。这样做将索引、搜索并记录返回的相关结果：例如，根据选择的Lucene配置（`Similarity`、`Analyzers`等）。默认情况下，这些应用程序在CACM数据集([http://mng.bz/GWZq](http://mng.bz/GWZq))上使用`BM25Similarity`运行。
- en: ¹
  id: totrans-871
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-872
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See Leif Azzopardi et al., “Lucene4IR: Developing Information Retrieval Evaluation
    Resources using Lucene,” *ACM SIGIR Forum* 50, no. 2 (December 2016), [http://sigir.org/wp-content/uploads/2017/01/p058.pdf](http://sigir.org/wp-content/uploads/2017/01/p058.pdf).'
  id: totrans-873
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Leif Azzopardi等人，“Lucene4IR：使用Lucene开发信息检索评估资源”，*ACM SIGIR论坛* 50，第2期（2016年12月），[http://sigir.org/wp-content/uploads/2017/01/p058.pdf](http://sigir.org/wp-content/uploads/2017/01/p058.pdf)。
- en: 'Once you’ve performed data evaluation with Lucen4IR tools, you can measure
    precision, recall, and other IR metrics using the `trec_eval` tool (a tool developed
    to measure the quality of search results on the data from the TREC conference
    series; [http://trec.nist.gov](http://trec.nist.gov)). Here’s an example of `trec_eval`
    terminal output on the CACM dataset using BM25 ranking:'
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你使用Lucen4IR工具进行了数据评估，你可以使用`trec_eval`工具（为衡量TREC会议系列数据上的搜索结果质量而开发的工具；[http://trec.nist.gov](http://trec.nist.gov)）来衡量精确度、召回率和其他信息检索指标。以下是在CACM数据集上使用BM25排名的`trec_eval`终端输出的示例：
- en: '[PRE78]'
  id: totrans-875
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '***1* Number of queries performed**'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 执行的查询数量**'
- en: '***2* Number of returned results**'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 返回结果的数量**'
- en: '***3* Number of relevant results**'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 相关结果的数量**'
- en: '***4* Number of returned results that are also relevant**'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 返回的既是相关结果的数量**'
- en: '***5* Mean average precision**'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 平均平均精度**'
- en: '***6* R-precision**'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* R-精度**'
- en: '***7* P_5, P_10, and so on give the precision at 5, 10, and so on retrieved
    documents.**'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* P_5, P_10等给出在5、10等检索文档中的精确度。**'
- en: 'If you change the Lucene `Similarity` parameter in the Lucene4IR configuration
    file and again run `RetrievalApp` and `ExampleStatsApp`, you can observe how precision,
    recall, and other measures commonly used in IR change in the dataset. Here’s an
    example of `trec_eval` terminal output on the CACM dataset using a language model–based
    ranking (Lucene’s `LMJelinekMercerSimilarity`^([[2](#ch05fn02)])):'
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更改Lucene4IR配置文件中的`Similarity`参数，并再次运行`RetrievalApp`和`ExampleStatsApp`，你可以观察在数据集中常用的精确度、召回率和其他度量如何变化。以下是在CACM数据集上使用基于语言模型排名（Lucene的`LMJelinekMercerSimilarity`^([[2](#ch05fn02)]))的`trec_eval`终端输出的示例：
- en: ²
  id: totrans-884
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-885
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Chengxiang Zhai and John Lafferty, “A Study of Smoothing Methods for Language
    Models Applied to Ad Hoc Information Retrieval,” [http://mng.bz/zM8a](http://mng.bz/zM8a).
  id: totrans-886
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Chengxiang Zhai和John Lafferty，“对应用于Ad Hoc信息检索的语言模型平滑方法的探讨”，[http://mng.bz/zM8a](http://mng.bz/zM8a)。
- en: '[PRE79]'
  id: totrans-887
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'In this case, `Similarity` was switched to use language models to estimate
    probabilities of relevance. The results are worse than with BM25: all the metrics
    have slightly lower values.'
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`Similarity`被切换为使用语言模型来估计相关性的概率。结果比BM25差：所有指标值都有所降低。
- en: The nice thing about using these tools together is that you can evaluate how
    well your decisions impact the accuracy of search results in a series of quick,
    easy steps. This doesn’t guarantee that you can achieve perfect rankings, but
    you can use this approach to define the baseline ranking function for your search
    engine and data. After this short intro to Lucene4IR, you’re encouraged to develop
    your own `Similarity`—for example, based on word2vec—and see whether it makes
    a difference with respect to `BM25Similarity` and so on.
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些工具一起的优点在于，你可以通过一系列快速、简单的步骤来评估你的决策如何影响搜索结果的准确性。这并不能保证你能达到完美的排名，但你可以使用这种方法来定义你的搜索引擎和数据的基线排名函数。在简要介绍了Lucene4IR之后，你被鼓励开发自己的`Similarity`——例如，基于word2vec——并看看它是否与`BM25Similarity`等有所区别。
- en: 5.5.1\. Similarity based on averaged word embeddings
  id: totrans-890
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.1\. 基于平均词嵌入的相似度
- en: You saw the effectiveness of document embeddings generated using word vectors
    in the small experiment with the “bernhard riemann influence” sample query. At
    the same time, in real life, you need better evidence for the effectiveness of
    a retrieval model. In this section, you’ll work on `Similarity` implementations
    based on averaged word2vec word vectors. You’ll then measure their effectiveness
    on a small dataset using the Lucene4IR project. These measures will give you a
    sense of how well these ranking models behave in general.
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: 你在小实验中看到了使用词向量生成的文档嵌入的有效性，实验样本是“bernhard riemann influence”。同时，在现实生活中，你需要更好的证据来证明检索模型的有效性。在本节中，你将基于平均word2vec词向量实现`Similarity`。然后，你将使用Lucene4IR项目在小型数据集上测量它们的有效性。这些度量将给你一个关于这些排名模型总体表现的感觉。
- en: 'Extending a Lucene `Similarity` correctly is a difficult task that requires
    some insights into how Lucene works. We’ll focus on the relevant bits of the `Similarity`
    API to use document embeddings to score documents against queries. Let’s start
    by creating a `WordEmbeddingsSimilarity` that creates document embeddings via
    averaged word embeddings. It requires a trained word2vec model, a smoothing method
    to average word vectors to combine them in a document vector, and a Lucene field
    from which to fetch document content:'
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
  zh: 正确扩展 Lucene `Similarity` 是一项困难的任务，需要了解 Lucene 的工作原理。我们将关注 `Similarity` API 的相关部分，以使用文档嵌入对查询进行评分。让我们首先创建一个
    `WordEmbeddingsSimilarity`，它通过平均词嵌入创建文档嵌入。它需要一个训练好的 word2vec 模型，一个平滑方法来平均词向量以将它们组合到文档向量中，以及一个用于获取文档内容的
    Lucene 字段：
- en: '[PRE80]'
  id: totrans-893
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The Lucene `Similarity` will implement the following two methods:'
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: Lucene `Similarity` 将实现以下两个方法：
- en: '[PRE81]'
  id: totrans-895
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'The most important part for this task is to implement the `EmbeddingsSimScorer`,
    which is responsible for scoring documents:'
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务来说，最重要的部分是实现 `EmbeddingsSimScorer`，它负责对文档进行评分：
- en: '[PRE82]'
  id: totrans-897
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '***1* Generates the query vector**'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 生成查询向量**'
- en: '***2* Generates the document vector**'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 生成文档向量**'
- en: '***3* Calculates the cosine similarity between document and query vectors,
    and uses that as a document score**'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 计算文档和查询向量之间的余弦相似度，并将其作为文档得分**'
- en: 'As you can see, the `score` method does what you did in the previous section,
    but within the `Similarity` class. The only difference with respect to the previous
    approach is that the `toDenseAverageVector` utility class also takes a `Smoothing`
    parameter that specifies how to average word vectors:'
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，`score` 方法在 `Similarity` 类中执行了你之前章节中做的事情，但与之前的方法相比，唯一的区别是 `toDenseAverageVector`
    工具类还接受一个 `Smoothing` 参数，该参数指定了如何平均词向量：
- en: '[PRE83]'
  id: totrans-902
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '`getQueryVector` does exactly the same thing, but instead of iterating over
    `docTerms`, it iterates over the terms in the query.'
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: '`getQueryVector` 完全执行相同的功能，但它不是遍历 `docTerms`，而是遍历查询中的术语。'
- en: 'The Lucene4IR project comes with tools to run evaluations over the CACM dataset,
    which you can do using different `Similarity`s. Following the instructions in
    the Lucene4IR README ([http://mng.bz/0WGx](http://mng.bz/0WGx)), you can generate
    statistics to evaluate different rankings. For example, here’s the precision over
    the first five results using different `Similarity`s:'
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: Lucene4IR 项目附带用于在 CACM 数据集上运行评估的工具，你可以使用不同的 `Similarity`s 来执行此操作。按照 Lucene4IR
    README 中的说明（[http://mng.bz/0WGx](http://mng.bz/0WGx)），你可以生成统计信息来评估不同的排名。例如，以下是使用不同
    `Similarity`s 在前五个结果上的精度：
- en: '[PRE84]'
  id: totrans-905
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: These are some interesting numbers. First, VSM with TF-IDF weighting isn’t the
    worst result. The word-embeddings `Similarity` is 2% better than the others; not
    bad. But one simple takeaway from this quick evaluation is that the effectiveness
    of a ranking model can change depending on the data, so you should take care when
    choosing a model. Theoretical results and evaluations must always be measured
    against real-life usage of your search engine.
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是一些有趣的数据。首先，VSM 使用 TF-IDF 加权并不是最差的结果。词嵌入 `Similarity` 比其他方法好 2%，这还不错。但从这个快速评估中可以得出的一个简单结论是，排名模型的有效性可能会根据数据而变化，因此在选择模型时应该小心。理论结果和评估必须始终与你的搜索引擎的实际使用情况进行衡量。
- en: 'It’s also important to decide what to optimize the ranking for. Often, it’s
    difficult to get high precision together with high recall, for example. Let’s
    introduce another metric for evaluating a ranking model’s effectiveness: *normalized
    discounted cumulative gain* (NDCG). NDCG measures the usefulness, or *gain*, of
    a document based on its position in the results list. The gain is accumulated
    from the top of the results list to the bottom, so that the gain contributed by
    each result decreases with ranking. If you evaluate the NDCG of the previous `Similarity`s
    over the CACM dataset, the results are even more interesting:'
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: 还很重要的一点是要决定优化排名的目标。通常，很难同时获得高精度和高召回率，例如。让我们引入另一个用于评估排名模型有效性的指标：*归一化折损累积增益*（NDCG）。NDCG根据文档在结果列表中的位置来衡量其有用性或*增益*。增益从结果列表的顶部累积到底部，因此每个结果对排名的贡献随着排名的降低而减少。如果你评估
    CACM 数据集上先前 `Similarity`s 的 NDCG，结果会更加有趣：
- en: '[PRE85]'
  id: totrans-908
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: VSM and BM25 perform exactly the same; the word embeddings–based ranking function
    got a slightly better NDCG value. So if you’re interested in a more precise ranking
    over the first five results, you should probably choose word embeddings–based
    rankings, but this evaluation suggests that for an overall higher NDCG, this may
    not make a significant difference.
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: VSM和BM25表现完全相同；基于词嵌入的排名函数获得了略好的NDCG值。所以如果你对前五个结果的更精确排名感兴趣，你可能应该选择基于词嵌入的排名，但这个评估表明，对于更高的整体NDCG，这可能不会产生显著差异。
- en: 'Additionally, a good solution that’s also supported by recent research can
    be to mix classic and neural ranking models by using multiple scoring functions
    at the same time.^([[3](#ch05fn03)]) You can do that by using the `MultiSimilarity`
    class in Lucene. If you perform the same evaluation but with different flavors
    of `MultiSimilarity`, you can see that mixing language modeling and word vectors
    yields the best NDCG value:'
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一个既受近期研究支持又好的解决方案是，通过同时使用多个评分函数来混合经典和神经排名模型。[^[[3](#ch05fn03)]] 你可以通过使用Lucene中的`MultiSimilarity`类来实现这一点。如果你使用相同的评估方法，但使用不同的`MultiSimilarity`版本，你可以看到混合语言模型和词向量可以获得最佳的NDCG值：
- en: ³
  id: totrans-911
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-912
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dwaipayan Roy et al., “Representing Documents and Queries as Sets of Word Embedded
    Vectors for Information Retrieval,” Neu-IR ‘16 SIGIR Workshop on Neural Information
    Retrieval (July 21, 2016, Pisa, Italy), [https://arxiv.org/abs/1606.07869](https://arxiv.org/abs/1606.07869).
  id: totrans-913
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Dwaipayan Roy等人，“将文档和查询表示为单词嵌入向量的集合以进行信息检索”，Neu-IR ‘16 SIGIR Workshop on Neural
    Information Retrieval（2016年7月21日，意大利比萨），[https://arxiv.org/abs/1606.07869](https://arxiv.org/abs/1606.07869)。
- en: '[PRE86]'
  id: totrans-914
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: Summary
  id: totrans-915
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Classic retrieval models like VSM and BM25 provide good baselines for ranking
    documents, but they lack semantic understanding of text capabilities.
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经典检索模型，如VSM和BM25，为排名文档提供了良好的基线，但它们缺乏对文本的语义理解能力。
- en: Neural information retrieval models aim to provide better semantic understanding
    capabilities for ranking documents.
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经信息检索模型旨在为排名文档提供更好的语义理解能力。
- en: Distributed representations of words (like those generated by word2vec) can
    be combined to generate document embeddings for queries and documents.
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以将单词的分布式表示（如word2vec生成的）组合起来，为查询和文档生成文档嵌入。
- en: Averaged word embeddings can be used to generate effective Lucene `Similarity`s,
    which can achieve good results when evaluated against IR datasets.
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均词嵌入可以用来生成有效的Lucene `Similarity`s，当与信息检索数据集进行评估时，可以获得良好的结果。
- en: Chapter 6\. Document embeddings for rankings and recommendations
  id: totrans-920
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6章\. 用于排名和推荐的文档嵌入
- en: '*This chapter covers*'
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Generating document embeddings using paragraph vectors
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用段落向量生成文档嵌入
- en: Using paragraph vectors for ranking
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用段落向量进行排名
- en: Retrieving related content
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索相关内容
- en: Improving related-content retrieval with paragraph vectors
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用段落向量改进相关内容检索
- en: In the previous chapter, I introduced you to neural information retrieval models
    by building a ranking function based on averaged word embeddings. You averaged
    word embeddings generated by word2vec to obtain a *document embedding*, a dense
    representation of a sequence of words, that demonstrated high precision in ranking
    documents according to user intent.
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我通过基于平均词嵌入构建排名函数，向你介绍了神经信息检索模型。你通过word2vec生成的平均词嵌入来获得一个*文档嵌入*，这是一个单词序列的密集表示，它根据用户意图在排名文档时表现出高精度。
- en: The drawback of common retrieval models such as Vector Space Model with TF-IDF
    and BM25, however, is that they only look at single terms when ranking documents.
    This approach can lead to suboptimal results because the context information of
    those terms is discarded. With this drawback in mind, let’s see how you can generate
    document embeddings that look not just at single words, but at the whole text
    fragments surrounding those words. A vector representation created from these
    context-enhanced document embeddings will carry as much semantic information as
    possible, thus improving the ranking function’s precision even more.
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的检索模型，如带有TF-IDF和BM25的向量空间模型，然而，它们的缺点是它们在排名文档时只考虑单个术语。这种方法可能导致次优结果，因为那些术语的上下文信息被丢弃了。考虑到这个缺点，让我们看看你如何生成文档嵌入，这些嵌入不仅考虑单个单词，还考虑围绕这些单词的整个文本片段。从这些增强的上下文文档嵌入中创建的向量表示将尽可能携带语义信息，从而进一步提高排名函数的精度。
- en: Word embeddings are very good for capturing word semantics, but the meaning
    and deep semantics of text documents don’t depend on the meaning of words alone.
    It would be nice to be able to learn semantics about phrases or longer pieces
    of text instead of just words. In the previous chapter, you did that by averaging
    word embeddings. Going forward, you’ll discover that you can do better in terms
    of accuracy. In this chapter, we’ll explore a technique for learning document
    embeddings directly. Using extensions of the word2vec neural network learning
    algorithms, you can generate document embeddings for text sequences of different
    granularity (sentences, paragraphs, documents, and so on). You’ll experiment with
    this technique and show how it provides better numbers when used for ranking.
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: 单词嵌入非常适合捕捉词义，但文本文档的意义和深层语义并不仅仅取决于单词的意义。如果能学习短语或更长的文本片段的语义会更好。在前一章中，你是通过平均单词嵌入来实现这一点的。展望未来，你会发现你可以从准确性方面做得更好。在本章中，我们将探讨一种直接学习文档嵌入的技术。通过扩展
    word2vec 神经网络学习算法，你可以为不同粒度的文本序列（句子、段落、文档等）生成文档嵌入。你将尝试这种技术，并展示它如何在使用排名时提供更好的结果。
- en: Additionally, you’ll learn how to use document embeddings to find related content.
    *Related content* consists of documents (texts, videos, and so on) that are semantically
    correlated. When you show a single search result (such as when the user clicks
    it in a search results web page), it’s common to display other content that, for
    example, deals with similar topics or was created by the same author. Doing so
    is useful to capture the user’s attention and provide them with content they might
    like but that may not appear on the first search results page.
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还将学习如何使用文档嵌入来查找相关内容。*相关内容*包括语义相关的文档（文本、视频等）。当你展示单个搜索结果（例如，当用户在搜索结果网页上点击它时），通常还会显示其他内容，例如，涉及类似主题或由同一作者创建的内容。这样做有助于吸引用户的注意力，并为他们提供他们可能喜欢但可能不会出现在第一个搜索结果页上的内容。
- en: 6.1\. From word to document embeddings
  id: totrans-930
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1. 从单词到文档嵌入
- en: In this section, I’ll introduce an extension of word2vec that aims to learn
    document embeddings during neural network training. This is different than the
    previously used method of mixing word vectors (averaging and, eventually, smoothing
    them, such as with TF-IDF weights) and often gives better results when capturing
    document semantics.^([[1](#ch06fn01)]) This method, also known as *paragraph vectors*,
    extends the two word2vec architectures (the continuous-bag-of-words [CBOW] and
    skip-gram models), incorporating information about the current document in the
    context.^([[2](#ch06fn02)]) Word2vec performs unsupervised learning of word embeddings
    by using fragments of texts of a certain size, called the *window*, for training
    the neural network to either predict the context given a word belonging to that
    context or predict the word given a context the word belongs to.
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将介绍 word2vec 的一种扩展，旨在在神经网络训练期间学习文档嵌入。这与之前使用的混合单词向量的方法（如使用 TF-IDF 权重进行平均和最终平滑）不同，在捕捉文档语义时通常能给出更好的结果。[1](#ch06fn01)
    这种方法也称为*段落向量*，扩展了两个 word2vec 架构（连续词袋[CBOW]和跳字模型），在上下文中包含了关于当前文档的信息。[2](#ch06fn02)
    Word2vec 通过使用称为*窗口*的特定大小的文本片段进行无监督学习，训练神经网络预测给定上下文中的单词或给定单词所属上下文中的单词。
- en: ¹
  id: totrans-932
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-933
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See the comparisons in Andrew M. Dai, Christopher Olah, and Quoc V. Le, “Document
    Embedding with Paragraph Vectors,” [https://arxiv.org/pdf/1507.07998.pdf](https://arxiv.org/pdf/1507.07998.pdf).
  id: totrans-934
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅 Andrew M. Dai、Christopher Olah 和 Quoc V. Le 的比较研究，“使用段落向量的文档嵌入”，[https://arxiv.org/pdf/1507.07998.pdf](https://arxiv.org/pdf/1507.07998.pdf)。
- en: ²
  id: totrans-935
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-936
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Quoc Le and Tomas Mikolov, “Distributed Representations of Sentences and
    Documents,” [http://cs.stanford.edu/~quocle/paragraph_vector.pdf](http://cs.stanford.edu/~quocle/paragraph_vector.pdf).
  id: totrans-937
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅 Quoc Le 和 Tomas Mikolov 的论文，“句子和文档的分布式表示”，[http://cs.stanford.edu/~quocle/paragraph_vector.pdf](http://cs.stanford.edu/~quocle/paragraph_vector.pdf)。
- en: 'Specifically, a CBOW neural network has three layers (see [figure 6.1](#ch06fig01)):'
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，一个 CBOW 神经网络有三个层次（参见[图6.1](#ch06fig01)）：
- en: Input layer containing context words
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含上下文单词的输入层
- en: Hidden layer containing one vector for each word
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含每个单词一个向量的隐藏层
- en: Output layer containing the word to predict
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含要预测的单词的输出层
- en: Figure 6.1\. Word2vec CBOW model
  id: totrans-942
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1. Word2vec CBOW模型
- en: '![](Images/06fig01.jpg)'
  id: totrans-943
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig01.jpg)'
- en: The intuition provided by the paragraph vector–based methods can either decorate
    or replace the context with a label representing a document, so that the neural
    network will learn to correlate words and contexts with labels, rather than words
    with other words.
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: 基于段落向量的方法提供的直觉可以装饰或替换上下文，用一个表示文档的标签来表示，这样神经网络就会学会将单词和上下文与标签相关联，而不是单词与单词相关联。
- en: 'The CBOW model is expanded so that the input layer also contains the label
    of the document containing the current text fragment. During training, each text
    fragment is tagged with a label. Such text fragments can be entire documents or
    portions of a document, like sections, paragraphs, or sentences. The *value* of
    the label generally isn’t important;^([[3](#ch06fn03)]) a label can be *doc_123456*
    or *tag-foo-bar* or any kind of machine-generated string. The important thing
    is that labels should be unique within a document: two different text fragments
    shouldn’t be tagged with the same label if they don’t belong to the same piece
    of text.'
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: CBOW模型被扩展，使得输入层也包含包含当前文本片段的文档的标签。在训练过程中，每个文本片段都会被标记一个标签。这样的文本片段可以是整个文档或文档的部分，如章节、段落或句子。标签的*值*通常并不重要；^([[3](#ch06fn03)])标签可以是*doc_123456*或*tag-foo-bar*或任何类型的机器生成的字符串。重要的是标签在文档中应该是唯一的：如果两个不同的文本片段不属于同一文本片段，则不应使用相同的标签。
- en: ³
  id: totrans-946
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-947
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unless you intend to use it in some way other than training, such as using the
    labels generated by the network as document identifiers when indexing them after
    training has finished.
  id: totrans-948
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 除非你打算以某种方式使用它，除了训练之外，例如在训练完成后索引时使用网络生成的标签作为文档标识符。
- en: As you can see in [figure 6.2](#ch06fig02), the architecture of this model is
    similar to CBOW; it just adds an input label representing the document in the
    input layer. Consequently, the hidden layer needs to be equipped with a vector
    for each label, so that at the end of the training, you have a vector representation
    for each label. The interesting thing about this approach is that it allows you
    to handle documents of different granularities. You can use labels for either
    entire documents or smaller parts of them, like paragraphs or sentences. These
    labels act as a sort of memory that wires contexts to (missing) words; therefore,
    this method is called the *distributed memory model of paragraph vectors* (PV-DM).
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在[图6.2](#ch06fig02)中看到的，该模型的架构与CBOW类似；它只是在输入层中添加了一个表示文档的输入标签。因此，隐藏层需要为每个标签配备一个向量，这样在训练结束时，您就有每个标签的向量表示。这个方法有趣的地方在于，它允许你处理不同粒度的文档。你可以为整个文档或其较小部分使用标签，如段落或句子。这些标签充当一种记忆，将上下文与（缺失的）单词连接起来；因此，这种方法被称为*段落向量的分布式记忆模型*（PV-DM）。
- en: Figure 6.2\. Distributed memory model of paragraph vectors
  id: totrans-950
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.2. 段落向量的分布式记忆模型
- en: '![](Images/06fig02.jpg)'
  id: totrans-951
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig02.jpg)'
- en: 'In the case of documents like “riemann hypothesis - a deep dive into a mathematical
    mystery,” it makes sense to use a single label, because the text is relatively
    short. But for longer documents, like Wikipedia pages, it may be useful to create
    a label for each paragraph or sentence. Let’s pick the first paragraph of Riemann’s
    Wikipedia page: “Georg Friedrich Bernhard Riemann (17 September 1826 – 20 July
    1866) was a German mathematician who made contributions to analysis, number theory,
    and differential geometry. In the field of real analysis, he is mostly known for
    the first rigorous formulation of the integral, the Riemann integral, and his
    work on Fourier series.” You can tag each sentence with a different label and
    generate a vector representation that will help find similar sentences instead
    of similar Wikipedia pages.'
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像“riemann hypothesis - a deep dive into a mathematical mystery”这样的文档，使用单个标签是有意义的，因为文本相对较短。但对于较长的文档，如维基百科页面，为每个段落或句子创建标签可能是有用的。让我们以Riemann的维基百科页面的第一段为例：“Georg
    Friedrich Bernhard Riemann（1826年9月17日 - 1866年7月20日）是一位德国数学家，他对分析、数论和微分几何做出了贡献。在实分析领域，他最著名的是首次严格地表述了积分，即黎曼积分，以及他对傅里叶级数的研究。”你可以用不同的标签标记每个句子，并生成一个向量表示，这将有助于找到类似的句子，而不是类似的维基百科页面。
- en: 'Paragraph vectors also extend the word2vec skip-gram model with the *distributed-bag-of-words*
    model (PV-DBOW). The continuous skip-gram model uses a neural network with three
    layers:'
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
  zh: 段落向量还通过*分布式词袋模型*（PV-DBOW）扩展了word2vec skip-gram模型。连续skip-gram模型使用一个具有三个层的神经网络：
- en: Input layer with one input word
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层包含一个输入单词
- en: Hidden layer containing a vector representation for each word in the vocabulary
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含词汇表中每个单词的向量表示的隐藏层
- en: Output layer containing a number of words representing the predicted context
    with respect to the input word
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含表示输入词预测上下文的单词数量的输出层
- en: The DBOW model with paragraph vectors (see [figure 6.3](#ch06fig03)) inputs
    labels instead of words, so the network learns to predict portions of text belonging
    to the document, paragraph, or sentence having that label.
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: DBOW模型与段落向量（见[图6.3](#ch06fig03)）输入标签而不是单词，因此网络学会预测属于该标签的文档、段落或句子的文本片段。
- en: Figure 6.3\. Distributed-bag-of-words model with paragraph vectors
  id: totrans-958
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.3\. 基于段落向量的分布式词袋模型
- en: '![](Images/06fig03.jpg)'
  id: totrans-959
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig03.jpg)'
- en: 'Both PV-DBOW and PV-DM models can be used to calculate similarities between
    labeled documents. Just as in word2vec, they achieve surprisingly good results
    when capturing the document semantics. Let’s try using paragraph vectors on the
    example scenario with the DL4J `ParagraphVectors` implementation:'
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: PV-DBOW和PV-DM模型都可以用来计算标记文档之间的相似性。就像word2vec一样，它们在捕捉文档语义时取得了惊人的好结果。让我们尝试使用DL4J的`ParagraphVectors`实现来在示例场景中使用段落向量：
- en: '[PRE87]'
  id: totrans-961
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '***1* Configures paragraph vectors**'
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 配置段落向量**'
- en: '***2* Sets the document embedding dimensions**'
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 设置文档嵌入维度**'
- en: '***3* As in word2vec, you can set the minimum frequency threshold for a word
    to be used during learning.**'
  id: totrans-964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 如同word2vec，你可以在学习期间设置单词的最小频率阈值。**'
- en: '***4* Selects the chosen paragraph vector model: in this case, PV-DM**'
  id: totrans-965
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 选择所选的段落向量模型：在这种情况下，PV-DM**'
- en: '***5* Finalizes the configuration**'
  id: totrans-966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 最终确定配置**'
- en: '***6* Performs (unsupervised) learning over the input data**'
  id: totrans-967
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 在输入数据上执行（无监督）学习**'
- en: 'Similar to what you did with word2vec, you can ask the paragraph vector models
    questions:'
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: 与你使用word2vec所做的一样，你可以向段落向量模型提问：
- en: What are the nearest labels to label xyz? This will allow you to find the most-similar
    documents (because each document is tagged with a label).
  id: totrans-969
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与xyz标签最接近的标签是什么？这将帮助你找到最相似的文档（因为每个文档都带有标签）。
- en: What are the nearest labels, given a new piece of text? This will make it possible
    to use paragraph vectors on documents or queries that aren’t part of the training
    set.
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一段新文本，最近的标签是什么？这将使得可以在不属于训练集的文档或查询上使用段落向量。
- en: 'If you use titles from Wikipedia pages to train paragraph vectors, you can
    look for Wikipedia pages whose titles are semantically similar to input text.
    Suppose you want to get information about your next big trip to South America.
    You can get the top three closest documents to the sentence “Travelling in South
    America” from the paragraph vector model you just trained:'
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用维基百科页面的标题来训练段落向量，你可以查找与输入文本语义相似的维基百科页面标题。假设你想要获取关于你即将到来的南美大型旅行的信息。你可以从你刚刚训练的段落向量模型中获取与句子“在南美洲旅行”最接近的前三个文档：
- en: '[PRE88]'
  id: totrans-972
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '***1* Gets the labels nearest to the given input string**'
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 获取给定输入字符串最近的标签**'
- en: '***2* Each label is in the form “doc_”+ documentId, so you only get the document
    identifier part to fetch the document from the index.**'
  id: totrans-974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 每个标签的形式是“doc_”+ 文档Id，因此你只能获取文档标识符部分以从索引中获取文档。**'
- en: '***3* Retrieves the Lucene document having the given ID**'
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 获取具有给定ID的Lucene文档**'
- en: '***4* Prints the document title on the console**'
  id: totrans-976
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 在控制台上打印文档标题**'
- en: 'The output is as follows:'
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE89]'
  id: totrans-978
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '***1* São Tomé and Príncipe (a South American republic) information about transport
    and telecommunications**'
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 圣多美和普林西比（一个南美共和国）关于运输和电信的信息**'
- en: '***2* Not very relevant**'
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 并不太相关**'
- en: If you train paragraph vectors using the entire text of Wikipedia pages, instead
    of just the title, you get more-relevant results. This is mostly due to the fact
    that paragraph vectors, like word2vec, learn text representations by looking at
    the context, and this is more difficult with shorter text (titles) than with longer
    text (an entire Wikipedia page).
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用维基百科页面的全部文本而不是仅标题来训练段落向量，你会得到更相关的结果。这主要是因为段落向量，就像word2vec一样，通过查看上下文来学习文本表示，而与较短的文本（标题）相比，这在与较长的文本（整个维基百科页面）中更困难。
- en: 'The output when training with the entire text of Wikipedia pages is as follows:'
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: 使用维基百科页面的全部文本进行训练时的输出如下：
- en: '[PRE90]'
  id: totrans-983
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Document embeddings like those generated by paragraph vectors aim to provide
    a good representation of the semantics of the entire text, in the form of a vector.
    You can use them in the context of search to address the problem of semantic understanding
    in ranking. The similarity between such embeddings depends more on the meaning
    of text and less on simple term matching.
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于段落向量生成的文档嵌入旨在以向量的形式提供整个文本的语义表示。您可以在搜索的上下文中使用它们来解决排名中的语义理解问题。此类嵌入之间的相似性更多地取决于文本的意义，而不是简单的术语匹配。
- en: 6.2\. Using paragraph vectors in ranking
  id: totrans-985
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 在排名中使用段落向量
- en: 'Using paragraph vectors in ranking is simple: you can ask the model to either
    provide the vector for an already-trained label or document, or train a new vector
    for a new piece of text (such as an unseen document or query). Whereas with word
    vectors you have to decide how to combine them (you did it at ranking time, but
    you could have done so at indexing time), paragraph vector–based models make it
    possible to fetch query and document embeddings easily, to compare and rank them.'
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: 在排名中使用段落向量很简单：您可以要求模型提供已训练标签或文档的向量，或者为新文本（如未见过的文档或查询）训练一个新的向量。与词向量不同，您必须决定如何组合它们（您在排名时做了这件事，但您可以在索引时做），基于段落向量的模型使得轻松获取查询和文档嵌入、比较和排名成为可能。
- en: 'Before jumping into using paragraph vectors for ranking, let’s take a step
    back. The previous section talked about using the data indexed in Lucene to train
    a paragraph vector model. That can be done by implementing a `LabelAwareIterator`:
    an iterator over document contents that also assigns a label to each Lucene document.
    You tag each Lucene document with its internal Lucene document identifier, resulting
    in a label that looks like *doc_1234*:'
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入使用段落向量进行排名之前，让我们退一步。上一节讨论了使用 Lucene 中索引的数据来训练段落向量模型。这可以通过实现一个 `LabelAwareIterator`
    来完成：一个遍历文档内容的迭代器，同时也为每个 Lucene 文档分配一个标签。您将每个 Lucene 文档标记为其内部 Lucene 文档标识符，从而得到一个类似
    *doc_1234* 的标签：
- en: '[PRE91]'
  id: totrans-988
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '***1* FieldValuesLabelAwareIterator fetches sequences from an IndexReader (a
    read view on the search engine).**'
  id: totrans-989
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* FieldValuesLabelAwareIterator 从 IndexReader（搜索引擎的读取视图）中获取序列。**'
- en: '***2* The content will be fetched from a single field, not from all possible
    fields in the Lucene document.**'
  id: totrans-990
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 内容将从单个字段中获取，而不是从 Lucene 文档的所有可能字段中获取。**'
- en: '***3* Identifier of the current document being fetched, initialized as 0**'
  id: totrans-991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 正在获取的当前文档的标识符，初始化为 0**'
- en: '***4* If the current identifier is less than the number of documents in the
    index, there are more documents to read.**'
  id: totrans-992
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 如果当前标识符小于索引中的文档数量，还有更多文档要读取。**'
- en: '***5* Fetches content from the Lucene index**'
  id: totrans-993
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 从 Lucene 索引中获取内容**'
- en: '***6* Creates a new LabelledDocument to be used to train DL4J’s ParagraphVectors.
    The internal Lucene identifier is used as the label.**'
  id: totrans-994
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 创建一个新的 LabelledDocument 以用于训练 DL4J 的 ParagraphVectors。内部 Lucene 标识符用作标签。**'
- en: '***7* Sets the content of the specified Lucene field into the LabelledDocument**'
  id: totrans-995
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 将指定 Lucene 字段的内文设置为 LabelledDocument**'
- en: 'You initialize the iterator for paragraph vectors this way:'
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以这样初始化段落向量的迭代器：
- en: '[PRE92]'
  id: totrans-997
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '***1* Creates an IndexReader**'
  id: totrans-998
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 创建一个 IndexReader**'
- en: '***2* Defines the field to be used**'
  id: totrans-999
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 定义要使用的字段**'
- en: '***3* Creates the iterator**'
  id: totrans-1000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 创建迭代器**'
- en: '***4* Sets the iterator in ParagraphVectors**'
  id: totrans-1001
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 在 ParagraphVectors 中设置迭代器**'
- en: '***5* Builds a paragraph vectors model (still to be trained)**'
  id: totrans-1002
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 构建段落向量模型（尚未训练）**'
- en: '***6* Lets paragraph vectors perform (unsupervised) learning**'
  id: totrans-1003
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 让段落向量执行（无监督）学习**'
- en: 'Once the model has finished training, you can use the paragraph vectors to
    rescore documents after the retrieval phase:'
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型完成训练，您可以使用段落向量在检索阶段之后重新评分文档：
- en: '[PRE93]'
  id: totrans-1005
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '***1* Creates an IndexSearcher to perform the first query that identifies the
    result set**'
  id: totrans-1006
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 创建一个 IndexSearcher 以执行第一个查询，该查询标识结果集**'
- en: '***2* Tries to fetch an existing vector representation for the current query.
    This can fail because you’ve trained the model over search engine content, not
    queries.**'
  id: totrans-1007
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 尝试获取当前查询的现有向量表示。这可能失败，因为您在搜索引擎内容上训练了模型，而不是查询。**'
- en: '***3* If the query vector doesn’t exist, lets the underlying neural network
    train and infer a vector on that new piece of text (whose label will be the entire
    text of the String)**'
  id: totrans-1008
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 如果查询向量不存在，让底层神经网络在该新文本上训练并推断一个向量（其标签将是整个文本的字符串）**'
- en: '***4* Performs a search**'
  id: totrans-1009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 执行搜索**'
- en: '***5* Builds the label of the current doc**'
  id: totrans-1010
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 构建当前文档的标签**'
- en: '***6* Fetches the existing vector for the document that has the specified label**'
  id: totrans-1011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 获取具有指定标签的文档的现有向量**'
- en: '***7* Calculates the score as the cosine similarity between the query and document
    vector**'
  id: totrans-1012
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 计算分数为查询与文档向量之间的余弦相似度**'
- en: '***8* Prints the results on the console**'
  id: totrans-1013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 在控制台打印结果**'
- en: 'This code shows how easy it is to fetch a distributed representation for queries
    and documents without having to work on the word embeddings. For the sake of readability,
    the results are again shown as scored from best to worst (even though the code
    doesn’t do that). The ranking is very much in line with the actual relevance of
    the returned documents, and the scores are consistent with the document relevance:'
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码展示了如何轻松获取查询和文档的分布式表示，而无需处理词嵌入。为了提高可读性，结果再次按从最好到最差的顺序显示（尽管代码并没有这样做）。排名与返回文档的实际相关性非常一致，分数与文档相关性一致：
- en: '[PRE94]'
  id: totrans-1015
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: The two most relevant documents have high (and very close) scores, and the third
    has a significantly lower score; that’s okay, because it’s not relevant. Finally,
    the dummy document has a score close to zero.
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: 最相关的两个文档具有高（并且非常接近）的分数，第三个文档的分数明显较低；这是可以接受的，因为它并不相关。最后，虚拟文档的分数接近零。
- en: 6.2.1\. Paragraph vector–based similarity
  id: totrans-1017
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1. 基于段落向量的相似性
- en: 'You can introduce a `ParagraphVectorsSimilarity` that uses paragraph vectors
    to measure the similarity between a query and a document. The interesting part
    of this similarity is the implementation of the `SimScorer#score` API:'
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以引入一个使用段落向量来衡量查询与文档之间相似性的 `ParagraphVectorsSimilarity`。这个相似性的有趣之处在于 `SimScorer#score`
    API 的实现：
- en: '[PRE95]'
  id: totrans-1019
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '***1* Extracts a paragraph vector for the text of the query. If the query has
    never been seen before, this will imply performing a training step for the paragraph
    vector network.**'
  id: totrans-1020
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 从查询文本中提取段落向量。如果查询之前从未见过，这将意味着对段落向量网络执行训练步骤。**'
- en: '***2* Extracts the paragraph vector for the document with the label equal to
    its document identifier**'
  id: totrans-1021
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 提取具有与其文档标识符相等的标签的文档的段落向量**'
- en: '***3* If a vector with the given label (docId) can’t be found, performs a training
    step over the paragraph vector network to extract the new vector**'
  id: totrans-1022
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 如果找不到具有给定标签（docId）的向量，则在段落向量网络上执行训练步骤以提取新向量**'
- en: '***4* Calculates the cosine similarity between the query and document paragraph
    vectors, and uses it as the score for the given document**'
  id: totrans-1023
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 计算查询与文档段落向量之间的余弦相似度，并将其用作给定文档的分数**'
- en: 6.3\. Document embeddings and related content
  id: totrans-1024
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3. 文档嵌入和相关内容
- en: 'As a user, you may have experienced the feeling that a certain search result
    is *almost* good, but for some reason it isn’t good enough. Think about searching
    for “a book about implementing neural network algorithms” on a retail site. You
    get the search results: the first is a book titled *Learning to Program Neural
    Nets*, so you click that result and are taken to a page containing more details
    about the book. You realize that you like the book’s contents. The author is a
    recognized authority on the subject, but he uses Python as the programming language
    for his teaching examples, which you don’t know well enough. You may wonder, “Is
    there a similar book, but using Java to teach how to program neural nets?” The
    retail site may show you a list of similar related books, in the hope that if
    you don’t want to buy the one with examples written in Python, you may instead
    buy another book with similar contents (which may include a book that has examples
    in Java).'
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: 作为用户，您可能经历过这种感觉：某个搜索结果“几乎”是好的，但出于某种原因它还不够好。想想在零售网站上搜索“关于实现神经网络算法的书籍”。您得到了以下搜索结果：第一个结果是一本名为
    *Learning to Program Neural Nets* 的书，所以您点击了那个结果，并被带到包含更多关于这本书的详细信息的页面。您意识到您喜欢这本书的内容。作者是该主题的公认权威，但他使用
    Python 作为他的教学示例的编程语言，您对此不太熟悉。您可能会想，“有没有类似的书籍，但使用 Java 来教授如何编写神经网络程序？”零售网站可能会向您展示一系列类似的相关书籍，希望如果您不想购买用
    Python 编写的示例，您可能会选择另一本内容相似的书籍（可能包括用 Java 编写的示例）。
- en: In this section, you’ll see how to provide such related content by finding additional
    documents in the search engine that are similar not just because they’re from
    the same author or have some words in common, but because there is a more meaningful
    semantic correlation between two such documents. This should remind you of the
    semantic-understanding issue we addressed when ranking functions using document
    embeddings learned with paragraph vectors.
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将了解如何通过在搜索引擎中找到更多类似文档来提供此类相关内容，这些文档之所以相似，不仅仅是因为它们来自同一作者或有一些共同词汇，而是因为这两份文档之间存在更有意义的语义相关性。这应该会提醒您，我们在使用段落向量学习到的文档嵌入进行排名函数时解决的语义理解问题。
- en: 6.3.1\. Search, recommendations, and related content
  id: totrans-1027
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1\. 搜索、推荐和相关信息
- en: To illustrate how important it is to indicate appropriate related content in
    a search engine, let’s consider the stream of actions a user performs on a video-sharing
    platform (such as YouTube). The primary (or even only) interface is the search
    box where users enter a query. Suppose the user types `Lucene tutorial` in the
    search box and clicks the Search button. A list of search results is shown, and
    the user eventually picks one they find interesting. From then on, it’s common
    for the user to stop searching and instead click videos in the Related box or
    column. Typical recommendations for a video entitled “Lucene tutorial” could be
    videos with titles like “Lucene for beginners,” “Intro to search engines,” and
    “Building recommender systems with Lucene.” The user can click any of these recommendations;
    for example, if they learned enough from the “Lucene tutorial” video, they might
    jump to watching a more advanced video; otherwise, they might want to watch another
    introductory video, or one introducing search engines, if they realized that additional
    prior knowledge was required to understand how to use Lucene. This process of
    consuming retrieved content and then navigating through related content can go
    on indefinitely. Thus, providing relevant related content is of the utmost importance
    to best satisfy user needs.
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明在搜索引擎中指明适当的相关内容是多么重要，让我们考虑用户在视频分享平台（如YouTube）上执行的操作流。主要（或甚至唯一）的界面是搜索框，用户在此输入查询。假设用户在搜索框中输入`Lucene教程`并点击搜索按钮。显示了一列搜索结果，用户最终选择了一个他们觉得有趣的。从那时起，用户停止搜索并点击相关框或列中的视频是很常见的。对于标题为“Lucene教程”的视频，典型的推荐可能包括标题为“Lucene入门”、“搜索引擎简介”和“使用Lucene构建推荐系统”的视频。用户可以点击任何这些推荐；例如，如果他们从“Lucene教程”视频中获得了足够的知识，他们可能会跳转到观看更高级的视频；否则，他们可能想观看另一个入门视频，或者如果他们意识到需要额外的先验知识来理解如何使用Lucene，他们可能想观看介绍搜索引擎的视频。这种消费检索到的内容然后通过相关内容导航的过程可以无限进行下去。因此，提供相关的相关内容对于最好满足用户需求至关重要。
- en: 'The contents of the Related box can even shift the user’s intent toward topics
    that are far from the initial query. In the previous example, the user wanted
    to learn how to use Lucene. The search engine provided a related item whose main
    topic wasn’t directly related to Lucene: it was about building a machine learning
    system for generating recommendations based on Lucene. That’s a big switch, from
    needing information about working with Lucene as a beginner to learning about
    recommender systems based on Lucene (a more advanced topic).'
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: 相关框的内容甚至可以改变用户的意图，使其转向与初始查询相去甚远的主题。在先前的例子中，用户想要学习如何使用Lucene。搜索引擎提供的相关项目的主要主题与Lucene并没有直接关系：它是关于基于Lucene构建机器学习系统以生成推荐的内容。这是一个很大的转变，从需要有关作为新手与Lucene一起工作的信息，转变为学习基于Lucene的推荐系统（一个更高级的主题）。
- en: 'This brief example also applies to e-commerce websites. The main purpose of
    such websites is to sell you something. So although you’re encouraged to search
    for the product you (may) need, you’re also flooded with lots of “recommended
    for you” items. These recommendations are based on factors such as these:'
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简短的例子也适用于电子商务网站。这类网站的主要目的是向您销售产品。因此，尽管鼓励您搜索您（可能）需要的产品，但您也会被大量的“为您推荐”的项目所淹没。这些推荐基于以下因素：
- en: Which products you searched for in the past
  id: totrans-1031
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你过去搜索过的产品
- en: Which topics you search for most
  id: totrans-1032
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你搜索最多的主题
- en: New products
  id: totrans-1033
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新产品
- en: Which products you saw (browsed or clicked) recently
  id: totrans-1034
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你最近看到的（浏览或点击的）产品
- en: 'One of the main points of this flood of recommendations is *user retention*:
    an e-commerce site wants to keep you browsing and searching as long as possible,
    hoping that any of the products they sell will be interesting enough for you to
    buy it.'
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
  zh: 这些建议洪流的主要目的是*用户保留*：一个电子商务网站希望尽可能长时间地让您浏览和搜索，希望他们出售的任何产品都足够有趣，让您购买。
- en: 'This goes beyond buying and selling. Such capabilities are very important for
    many applications, such as in the field of healthcare: for example, a doctor looking
    at a patient’s medical records would benefit from being able to look at similar
    medical records from other patients (and their histories) in order to make a better
    diagnosis. We’ll focus now on implementing algorithms for retrieving related or
    similar documents with respect to an input document, based on their contents.
    First, we’ll look at how to get the search engine to extract related content,
    and then you’ll see how to use different approaches to build document embeddings
    to overcome some limitations of the first approach; see [figure 6.4](#ch06fig04).
    We’ll also take this chance to discuss how to use paragraph vectors to perform
    document classification, which is useful in the context of providing semantically
    relevant suggestions.'
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅仅局限于买卖。这样的能力对于许多应用都非常重要，例如在医疗保健领域：例如，一位医生在查看患者的病历时，如果能查看其他患者（及其病史）的类似病历将有助于做出更好的诊断。现在，我们将专注于实现基于输入文档内容检索相关或相似文档的算法。首先，我们将探讨如何让搜索引擎提取相关内容，然后您将看到如何使用不同的方法构建文档嵌入以克服第一种方法的一些局限性；参见[图6.4](#ch06fig04)。我们还将借此机会讨论如何使用段落向量进行文档分类，这在提供语义相关建议的背景下非常有用。
- en: Figure 6.4\. Using neural networks to retrieve related content
  id: totrans-1037
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.4. 使用神经网络检索相关内容
- en: '![](Images/06fig04_alt.jpg)'
  id: totrans-1038
  prefs: []
  type: TYPE_IMG
  zh: '![图像6.4的替代文本](Images/06fig04_alt.jpg)'
- en: 6.3.2\. Using frequent terms to find similar content
  id: totrans-1039
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.2. 使用频繁术语查找相似内容
- en: In the previous chapter, you saw how the TF-IDF weighting scheme for ranking
    relies on term and document frequencies to provide a measure of a document’s importance.
    The rationale behind TF-IDF ranking is that a document’s importance grows with
    the local frequency and global rarity of its terms, with respect to an input query.
    Building on these assumptions, you can define an algorithm to find documents that
    are similar to an input document, solely based on the search engine’s retrieval
    capabilities.
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您看到了TF-IDF加权方案如何用于排名，它依赖于术语和文档频率来提供文档重要性的度量。TF-IDF排名背后的原理是，与输入查询相比，文档的重要性随着其术语的局部频率和全局稀有性的增加而增长。基于这些假设，您可以定义一个算法，仅基于搜索引擎的检索能力来找到与输入文档相似的文档。
- en: Wikipedia dumps can be a good collection to evaluate the effectiveness of an
    algorithm for retrieving related content. Each Wikipedia page contains content
    and useful metadata (title, categories, references, and even some links to related
    content in the “See also” section). Several available tools can be used to index
    Wikipedia dumps into Lucene, such as the `lucene-benchmark` module ([http://mng.bz/A2Qo](http://mng.bz/A2Qo)).
    Suppose you’ve indexed each Wikipedia page with its title and text into two separate
    Lucene indexes. Given the search results returned by a query, you want to fetch
    the five most-similar documents to show to the user as related content. To do
    that, you pick each search result, extract the most important terms from its content
    (in this case, from the `text` field), and perform another query using the extracted
    terms (see [figure 6.5](#ch06fig05)). The first five resulting documents can be
    used as related content.
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科数据包可以是一个很好的集合，用于评估检索相关内容的算法的有效性。每个维基百科页面都包含内容和有用的元数据（标题、类别、参考文献，甚至在“另见”部分中甚至有一些相关内容的链接）。可以使用几个可用的工具将维基百科数据包索引到Lucene中，例如`lucene-benchmark`模块([http://mng.bz/A2Qo](http://mng.bz/A2Qo))。假设您已将每个维基百科页面的标题和文本分别索引到两个单独的Lucene索引中。给定查询返回的搜索结果，您希望检索显示给用户的相关内容的五个最相似文档。为此，您选择每个搜索结果，从其内容中提取最重要的术语（在这种情况下，从`text`字段中提取），并使用提取的术语执行另一个查询（参见[图6.5](#ch06fig05))。前五个结果文档可以用作相关内容。
- en: Figure 6.5\. Retrieving related content by using a document’s most important
    terms, weighted by TF-IDF scheme
  id: totrans-1042
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.5. 使用文档最重要的术语（根据TF-IDF方案加权）检索相关内容
- en: '![](Images/06fig05_alt.jpg)'
  id: totrans-1043
  prefs: []
  type: TYPE_IMG
  zh: '![图像6.5的替代文本](Images/06fig05_alt.jpg)'
- en: 'Suppose you run the query “travel hints” and get a search result about a traffic
    circle in New Jersey called Ledgewood Circle. You take all the terms contained
    in the Wikipedia page [https://en.wikipedia.org/wiki/Ledgewood_Circle](https://en.wikipedia.org/wiki/Ledgewood_Circle)
    and extract those that have at least a term frequency of 2 and a document frequency
    of 5\. This way, you obtain the following list of terms:'
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您运行了“旅行提示”查询并得到了关于新泽西州Ledgewood Circle交通圈的搜索结果。您从维基百科页面 [https://en.wikipedia.org/wiki/Ledgewood_Circle](https://en.wikipedia.org/wiki/Ledgewood_Circle)
    中提取所有包含的术语，并提取那些至少具有2个词频和5个文档频率的术语。这样，您就获得了以下术语列表：
- en: '[PRE96]'
  id: totrans-1045
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: You then use these terms as a query to obtain the documents to be used as related
    content presented to the end user.
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您使用这些术语作为查询来获取用于向最终用户展示的相关内容文档。
- en: Lucene lets you do this using a component called `MoreLikeThis` (MLT, [http://mng.bz/ZZlR](http://mng.bz/ZZlR)),
    which can extract the most important terms from a `Document` and create a `Query`
    object to be run via the same `IndexSearcher` used to run the original query.
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: Lucene 允许您使用一个名为 `MoreLikeThis` (MLT, [http://mng.bz/ZZlR](http://mng.bz/ZZlR))
    的组件来完成此操作，该组件可以从 `Document` 中提取最重要的术语并创建一个 `Query` 对象，通过与运行原始查询相同的 `IndexSearcher`
    来运行。
- en: Listing 6.1\. Searching and getting related content via MLT
  id: totrans-1048
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.1\. 通过MLT搜索和获取相关内容
- en: '[PRE97]'
  id: totrans-1049
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '***1* Defines an Analyzer to be used while searching and while extracting terms
    from search results’ content**'
  id: totrans-1050
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 定义在搜索和从搜索结果内容中提取术语时使用的分析器**'
- en: '***2* Creates an MLT instance**'
  id: totrans-1051
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 创建一个MLT实例**'
- en: '***3* Specifies the Analyzer to be used by MLT**'
  id: totrans-1052
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 指定MLT使用的分析器**'
- en: '***4* Creates an IndexSearcher**'
  id: totrans-1053
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 创建一个IndexSearcher**'
- en: '***5* Defines which field to use when doing the first query and when looking
    for related content via the MLT-generated query**'
  id: totrans-1054
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 定义在执行第一次查询以及通过MLT生成的查询查找相关内容时使用的字段**'
- en: '***6* Creates a QueryParser**'
  id: totrans-1055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 创建一个QueryParser**'
- en: '***7* Parses the user-entered query**'
  id: totrans-1056
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 解析用户输入的查询**'
- en: '***8* Executes the query, and returns the top 10 search results**'
  id: totrans-1057
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 执行查询，并返回前10个搜索结果**'
- en: '***9* Retrieves the Document object related to the current search result**'
  id: totrans-1058
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 获取与当前搜索结果相关的文档对象**'
- en: '***10* Prints the current document’s title and score**'
  id: totrans-1059
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10* 打印当前文档的标题和分数**'
- en: '***11* Extracts the content of the text field from the current Document**'
  id: totrans-1060
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***11* 从当前文档中提取文本字段的文本内容**'
- en: '***12* Uses MLT to generate a query based on the content of the retrieved Document,
    by extracting the most important terms (TF-IDF ranking-wise)**'
  id: totrans-1061
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***12* 使用MLT根据检索到的文档的内容生成查询，通过提取最重要的术语（TF-IDF排名方式）**'
- en: '***13* Performs the query generated by MLT**'
  id: totrans-1062
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***13* 执行由MLT生成的查询**'
- en: '***14* Prints the title of the Document found by the Query generated by MLT**'
  id: totrans-1063
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***14* 打印由MLT生成的查询找到的文档的标题**'
- en: 'No machine learning is involved to extract related content: you use the search
    engine’s capabilities to return related documents containing the most important
    terms from a search result. Here’s some example output for the “travel hints”
    query and “Ledgewood Circle” search result:'
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: 提取相关内容不涉及机器学习：您使用搜索引擎的能力来返回包含搜索结果中最重要术语的相关文档。以下是对“旅行提示”查询和“Ledgewood Circle”搜索结果的示例输出：
- en: '[PRE98]'
  id: totrans-1065
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'The first three related documents (not counting the “Ledgewood Circle” document)
    are similar to the original document. They all relate to something correlated
    with traffic circles, such as a tunnel, highway, or interstate. The fourth document,
    though, is completely unrelated: it deals with fiber optics. Let’s dig deeper
    into why this result was fetched. To do this, you can turn on Lucene’s `Explanation`:'
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: 前三个相关文档（不包括“Ledgewood Circle”文档）与原始文档相似。它们都与与交通圈相关的事物有关，例如隧道、高速公路或州际公路。然而，第四个文档则完全不相关：它涉及光纤。让我们深入探究为什么检索到了这个结果。为此，您可以启用Lucene的
    `Explanation`：
- en: '[PRE99]'
  id: totrans-1067
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '***1* Gets the Explanation for the MLT query**'
  id: totrans-1068
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 获取MLT查询的解释**'
- en: 'The explanation allows you to inspect how the terms `signal`, `10`, `travel`,
    and `new` matched:'
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: 该说明允许您检查术语 `signal`、`10`、`travel` 和 `new` 如何匹配：
- en: '[PRE100]'
  id: totrans-1070
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'The issue with this approach is that `MoreLikeThis` extracted the most important
    terms according to TF-IDF weighting. This, as you saw in the previous chapter,
    has the problem of relying on frequencies. Let’s look at these important terms
    extracted from the “Ledgewood Circle” document text: the terms “record,” “govern,”
    “left,” “depart,” “west,” “onto,” “intersect,” “1997,” “wish,” “move,” and so
    on don’t seem to suggest that the document deals with a traffic circle. If you
    try to read them as a sentence, you can’t derive much sense from it.'
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点在于`MoreLikeThis`根据TF-IDF权重提取了最重要的术语。正如你在上一章中看到的，这存在依赖频率的问题。让我们看看从“Ledgewood
    Circle”文档文本中提取的这些重要术语：术语“record”、“govern”、“left”、“depart”、“west”、“onto”、“intersect”、“1997”、“wish”、“move”等似乎并不表明该文档处理的是交通环岛。如果你尝试将它们读作一个句子，你从中无法得出太多意义。
- en: 'The `Explanation` uses the default Lucene `BM25Similarity`. In [chapter 5](kindle_split_017.xhtml#ch05),
    you saw that you can use different ranking functions and test whether you can
    get better results. If you adopt the `ClassicSimilarity` (vector-space model with
    TF-IDF), you get the following:'
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
  zh: '`Explanation`使用默认的Lucene `BM25Similarity`。在[第5章](kindle_split_017.xhtml#ch05)中，你了解到你可以使用不同的排名函数并测试是否可以得到更好的结果。如果你采用`ClassicSimilarity`（具有TF-IDF的向量空间模型），你会得到以下结果：'
- en: '[PRE101]'
  id: totrans-1073
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '***1* Uses ClassicSimilarity instead of the default (only for the similar content
    search)**'
  id: totrans-1074
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 使用ClassicSimilarity代替默认设置（仅适用于相似内容搜索）**'
- en: 'Here are the results:'
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是结果：
- en: '[PRE102]'
  id: totrans-1076
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'They’re even worse: both “Cherry Tree” and “Speech processing” are completely
    unrelated to the original “Ledgewood Circle” document. Let’s try using a language
    model–based similarity, `LMDirichletSimilarity`:^([[4](#ch06fn04)])'
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: 它们甚至更糟糕：两者“Cherry Tree”和“Speech processing”都与原始的“Ledgewood Circle”文档完全不相关。让我们尝试使用基于语言模型的相似度，`LMDirichletSimilarity`:^([[4](#ch06fn04)]）
- en: ⁴
  id: totrans-1078
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-1079
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Chengxiang Zhai and John Lafferty, “A Study of Smoothing Methods for Language
    Models Applied to Ad Hoc Information Retrieval,” [http://mng.bz/RGVZ](http://mng.bz/RGVZ).
  id: totrans-1080
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见赵成祥和约翰·拉法蒂，《针对临时信息检索的语言模型平滑方法研究》，[http://mng.bz/RGVZ](http://mng.bz/RGVZ)。
- en: '[PRE103]'
  id: totrans-1081
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'The results are as follows:'
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE104]'
  id: totrans-1083
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: Interestingly enough, these results all sound good—all of them relate to infrastructures
    for cars, such as highways or tunnels.
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: 趣味的是，这些结果听起来都很好——它们都与汽车基础设施相关，如高速公路或隧道。
- en: Measuring the quality of related content using categories
  id: totrans-1085
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用分类来衡量相关内容的质量
- en: 'In [chapter 5](kindle_split_017.xhtml#ch05), you learned how important it is
    to not do single experiments. Although they allow a fine-grained understanding
    of how retrieval models work in some cases, they can’t provide an overall measure
    of how well such a model works on more data. Because Wikipedia pages come with
    categories, you can make a first evaluation of the accuracy of related content
    using them. If documents found by the related-content algorithm (in this case,
    Lucene’s `MoreLikeThis`) fall in any of the original document categories, you
    can consider them relevant. In real life, you may want to do this evaluation slightly
    differently: for example, you may also consider a suggested document relevant
    if its category is a subcategory of the original document category. You can do
    this (and much more) by building a taxonomy, extracting it from Wikipedia ([https://en.wikipedia.org/wiki/Help:Category](https://en.wikipedia.org/wiki/Help:Category)),
    or by using a DBpedia project (a crowdsourced effort to build structured information
    about content in Wikipedia; [http://wiki.dbpedia.org](http://wiki.dbpedia.org)).
    But for the sake of this chapter’s experiments, you can define an accuracy measure
    as the sum of the times a piece of related content shares one or more categories
    with the original document, divided by the number of related documents retrieved.'
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](kindle_split_017.xhtml#ch05)中，你了解到不做单一实验是多么重要。尽管它们在某些情况下可以让你更细致地理解检索模型的工作方式，但它们不能提供对模型在更多数据上表现的整体衡量。因为维基百科页面带有分类，你可以使用它们对相关内容的准确性进行初步评估。如果通过相关内容算法（在这种情况下，Lucene的`MoreLikeThis`）找到的文档属于原始文档的任何分类，你可以认为它们是相关的。在现实生活中，你可能想要以稍微不同的方式来做这个评估：例如，如果你认为建议的文档的分类是原始文档分类的子分类，你也可以认为该文档是相关的。你可以通过构建一个分类法，从维基百科（[https://en.wikipedia.org/wiki/Help:Category](https://en.wikipedia.org/wiki/Help:Category)）中提取它，或者通过使用DBpedia项目（一个构建维基百科内容结构化信息的众包努力；[http://wiki.dbpedia.org](http://wiki.dbpedia.org)）来做这件事（以及更多）。但为了本章实验的目的，你可以定义一个准确度衡量标准，即相关内容与原始文档共享一个或多个分类的次数总和，除以检索到的相关文档数量。
- en: 'Let’s use the Wikipedia page for the soccer player Radamel Falcao, which has
    lots of categories (1986 births, AS Monaco FC players, and so on). Using `BM25Similarity`
    to rank the MLT-generated `Query` gives the following top five related documents,
    with the shared category in parentheses (if any):'
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用足球运动员拉法埃尔·法尔考的维基百科页面，它有许多类别（1986年出生，摩纳哥足球俱乐部球员等）。使用`BM25Similarity`对MLT生成的`Query`进行排序，得到以下前五个相关文档，括号中（如果有）显示共享的类别：
- en: '[PRE105]'
  id: totrans-1088
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: The first four results have a category in common with Radamel Falcao’s Wikipedia
    page, but “Pelè” doesn’t. Therefore, the accuracy is 4 (the number of results
    sharing a category with Radamel Falcao’s page) divided by 5 (the number of returned
    similar results), or 0.8.
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: 前四个结果与拉法埃尔·法尔考的维基百科页面有共同类别，但“贝利”没有。因此，准确度为4（与拉法埃尔·法尔考页面共享类别的结果数量）除以5（返回的相似结果数量），即0.8。
- en: To evaluate this algorithm, you can generate a number of random queries and
    measure the defined average accuracy over the returned related content. Let’s
    generate 100 queries using words that exist in the index (to make sure at least
    one search result is returned), and then retrieve the 10 most similar documents
    using paragraph vectors and cosine similarity. For each of these related documents,
    check whether one of its categories also appears in the search result.
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估此算法，你可以生成多个随机查询，并测量定义的平均准确度，这些准确度是在返回的相关内容上测量的。让我们使用索引中存在的单词生成100个查询（以确保至少返回一个搜索结果），然后使用段落向量和余弦相似度检索最相似的10个文档。对于这些相关文档中的每一个，检查其类别是否也出现在搜索结果中。
- en: Listing 6.2\. Fetching related content and calculating accuracy
  id: totrans-1091
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.2\. 获取相关内容并计算准确度
- en: '[PRE106]'
  id: totrans-1092
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '***1* Gets the categories associated with the original Wikipedia page returned
    by a query**'
  id: totrans-1093
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 获取与查询返回的原始维基百科页面关联的类别**'
- en: '***2* Creates the related-content query with MLT**'
  id: totrans-1094
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 使用MLT创建相关内容查询**'
- en: '***3* Runs the same query with multiple Similarity implementations to evaluate
    what works best**'
  id: totrans-1095
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 使用多个相似度实现运行相同的查询以评估哪种效果最佳**'
- en: '***4* Uses a specific Similarity in the IndexSearcher**'
  id: totrans-1096
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 在IndexSearcher中使用特定的相似度**'
- en: '***5* Performs the related-content query**'
  id: totrans-1097
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 执行相关内容查询**'
- en: '***6* Initializes the accuracy to zero**'
  id: totrans-1098
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 将准确度初始化为零**'
- en: '***7* Skips a result if it’s equal to the original document**'
  id: totrans-1099
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 如果结果与原始文档相同，则跳过该结果**'
- en: '***8* Retrieves the related Document**'
  id: totrans-1100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 检索相关文档**'
- en: '***9* If any category of the related content is contained in the original Document,
    increases the accuracy**'
  id: totrans-1101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 如果相关内容的任何类别包含在原始文档中，则增加准确度**'
- en: '***10* Divides the accuracy by the number of returned related documents**'
  id: totrans-1102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10* 将准确度除以返回的相关文档数**'
- en: 'The corresponding output with `BM25Similarity`, `ClassicSimilarity`, and `LMDirichletSimilarity`
    looks like this:'
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`BM25Similarity`、`ClassicSimilarity`和`LMDirichletSimilarity`的相应输出如下：
- en: '[PRE107]'
  id: totrans-1104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Running this over 100 randomly generated queries and the corresponding 10 top
    results gives the following average accuracies:'
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: 在100个随机生成的查询及其相应的10个顶级结果上运行此操作，得到以下平均准确度：
- en: '[PRE108]'
  id: totrans-1106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: Given the fact that the best possible accuracy is 1.0, these are low accuracy
    values. The best one finds a related document with a matching category only 9%
    of the time.
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到最佳可能的准确度为1.0，这些是较低的准确值。最佳情况下，只有9%的时间能找到与匹配类别相关的文档。
- en: 'Although this is a suboptimal result, it’s useful to reason about it and the
    availability of the category information in each document. First, did you choose
    a good metric to measure the “aboutness” of the related content retrieved with
    this approach? Categories attached to Wikipedia pages are usually of good quality,
    and the “Ledgewood Circle” page’s categories are “Transportation in Morris County”
    and “Traffic circles in New Jersey.” A category like “Traffic circles” would also
    have been appropriate, but more generic. So the level of detail in the choice
    of relevant categories attached to such articles can vary and influence the accuracy
    estimates you calculate. Another thing to analyze is whether the categories are
    keywords taken from the text. In the case of Wikipedia, they aren’t, but in general
    this may not always be the case. You can think about extending the way you measure
    accuracy by including not just categories a document belongs to, but also important
    words or concepts mentioned in the text. For example, the “Ledgewood Circle” page
    contains a section about a controversy that arose back in the 1990s about a tree
    planted in the middle of the traffic circle. Such information isn’t represented
    in any way in the categories. If you want to be able to extract concepts discussed
    on a page, you can add them as additional categories (in this case, it might be
    a generic “Controversies” category). You can also think of this as tagging each
    document with a set of generic labels: these can be categories, concepts mentioned
    in the text, important words, and so on The bottom line is that your accuracy
    measure is as good as the labels or categories attached to documents. On the other
    hand, the way you build and use categories can have a significant impact on your
    evaluations.'
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是一个次优结果，但分析它以及每个文档中类别信息的可用性是有用的。首先，你是否选择了一个好的指标来衡量使用这种方法检索到的相关内容的“相关性”？维基百科页面上的分类通常质量很好，而“Ledgewood
    Circle”页面的分类是“Morris县交通”和“新泽西州的交通环”。像“交通环”这样的分类也是合适的，但更为通用。因此，选择此类文章相关分类的详细程度可能会有所不同，并会影响你计算的准确性估计。另一件要分析的事情是这些分类是否是从文本中提取的关键词。在维基百科的情况下，它们不是，但一般来说，这并不总是如此。你可以考虑通过包括文档所属的分类，以及文本中提到的关键词或概念来扩展你衡量准确性的方法。例如，“Ledgewood
    Circle”页面包含一个关于20世纪90年代中期在交通环中央种植树木引发的争议部分。这类信息在分类中没有任何表示。如果你想能够提取页面上的讨论概念，你可以将它们作为额外的分类添加（在这种情况下，可能是一个通用的“争议”分类）。你也可以将这视为为每个文档添加一组通用标签：这些可以是分类、文本中提到的概念、重要词汇等等。总之，你的准确性衡量标准与附加到文档上的标签或分类一样好。另一方面，你构建和使用分类的方式可能会对你的评估产生重大影响。
- en: 'Second, did you use the metric appropriately? You extracted the categories
    of the input document and the related content to see if any category belonged
    to both. The “Ledgewood Circle” page doesn’t have the “Traffic circle” category,
    but its category “Traffic circles in New Jersey” could be thought of as a subcategory
    of a more generic “Traffic circle” category. Extending this reasoning to all the
    categories in Wikipedia, you could imagine building a tree as shown in [figure
    6.6](#ch06fig06): the nodes are categories, and the deeper a node is, the more
    specific and fine-grained its category will be.'
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，你是否适当地使用了指标？你提取了输入文档和相关内容的分类，以查看是否有任何分类属于两者。 “Ledgewood Circle”页面没有“交通环”分类，但它的分类“新泽西州的交通环”可以被认为是更通用“交通环”分类的子分类。将这种推理扩展到维基百科中的所有分类，你可以想象构建一个如图6.6所示的树：节点是分类，节点越深，其分类就越具体和细致。
- en: Figure 6.6\. Building a taxonomy from Wikipedia categories
  id: totrans-1110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.6\. 从维基百科分类构建分类法
- en: '![](Images/06fig06_alt.jpg)'
  id: totrans-1111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig06_alt.jpg)'
- en: In this experiment, you could change the rule for matching categories from “at
    least one category should be shared between both the input and related content”
    to “at least one category should be shared between both the input and related
    content, or one of the categories of a certain document should be a specification
    of another category in the other document.” If you know more about what kinds
    of relationships exist between categories (and labels in general), you can use
    that information, too. DBpedia can be used as one such source of information about
    relationships that exist between pages. Imagine that the algorithm returns the
    “New Jersey” page as related to “Ledgewood Circle.” The main thing they have in
    common is that Ledgewood Circle is located in the state of New Jersey, specifically
    in Roxbury Township. If such information is available, it’s a great link you can
    navigate to measure the relevance of related content. For example, you could mark
    as relevant related documents that have any relation to the input document, or
    only mark documents relevant when they’re linked by any of a subset of existing
    relations.
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，你可以将匹配分类的规则从“输入和相关信息之间至少有一个分类应该共享”改为“输入和相关信息之间至少有一个分类应该共享，或者某个文档的一个分类应该是另一个文档中另一个分类的指定。”如果你对分类（以及一般标签）之间存在的各种关系有更多的了解，你也可以使用这些信息。DBpedia可以用作一个关于页面之间存在的关系的信息来源。想象一下，算法返回的“New
    Jersey”页面与“Ledgewood Circle”相关。它们的主要共同点是Ledgewood Circle位于新泽西州，具体在Roxbury镇。如果此类信息可用，它是一个可以导航以衡量相关内容相关性的绝佳链接。例如，你可以标记与输入文档有任何关系的相关文档为相关文档，或者只有当文档通过现有关系的一个子集链接时才标记文档为相关。
- en: The DBpedia project records many such relations between pages from Wikipedia.
    You can think of it as a graph whose nodes are pages; arcs are relations (with
    a name). [Figure 6.7](#ch06fig07) shows the relationships between Ledgewood Circle
    and New Jersey using RelFinder ([www.visualdataweb.org/relfinder](http://www.visualdataweb.org/relfinder)).
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: DBpedia项目记录了来自维基百科页面之间的大量此类关系。你可以将其视为一个节点是页面的图；弧是关系（带有名称）。[图6.7](#ch06fig07)显示了Ledgewood
    Circle和New Jersey之间使用RelFinder([www.visualdataweb.org/relfinder](http://www.visualdataweb.org/relfinder))的关系。
- en: Figure 6.7\. Navigating relationships between the “Ledgewood Circle” and “New
    Jersey” pages in DBpedia
  id: totrans-1114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.7\. 在DBpedia中导航“Ledgewood Circle”和“New Jersey”页面之间的关系
- en: '![](Images/06fig07_alt.jpg)'
  id: totrans-1115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig07_alt.jpg)'
- en: 'Having a good hierarchical taxonomy for categories is important when you’re
    using them to measure the accuracy of results from `MoreLikeThis` and other related-content
    algorithms. On the other hand, information about categories and their relations
    often isn’t available in practice; in such cases, methods based on unsupervised
    learning can help you find out whether two documents are similar. Let’s think
    about algorithms to learn vector representations of text, like word2vec (for words)
    or paragraph vectors (for sequences of words): when you plot them on a graph,
    similar words or documents will be located near each other. In that case, you
    can group the closest vectors together to form *clusters* (there are several ways
    to do that, but we won’t cover them here), and consider related words or documents
    as belonging to the same cluster. In the next section, we’ll look at one of the
    more straightforward usages of document embeddings: finding similar content.'
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用它们来衡量`MoreLikeThis`和其他相关内容算法的结果准确性时，拥有一个良好的分类层次结构对于分类来说非常重要。另一方面，关于分类及其关系的信息在实践中往往不可用；在这种情况下，基于无监督学习的方法可以帮助你找出两份文档是否相似。让我们考虑学习文本向量表示的算法，如word2vec（用于单词）或段落向量（用于单词序列）：当你将它们绘制在图上时，相似的单词或文档将彼此靠近。在这种情况下，你可以将最近的向量组合在一起形成*簇*（有几种方法可以做到这一点，但这里不会介绍），并将相关的单词或文档视为属于同一簇。在下一节中，我们将探讨文档嵌入的一个更直接的用途：寻找相似内容。
- en: 6.3.3\. Retrieving similar content with paragraph vectors
  id: totrans-1117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.3\. 使用段落向量检索相似内容
- en: A paragraph vector learns a fixed (distributed) vector representation for each
    sequence of words fed into its neural network architecture. You can feed an entire
    document into the network, or portions of it, such as sections of an article,
    paragraphs, or sentences. It’s up to you to define the granularity. For example,
    if you feed the network entire documents, you can ask it to return the most similar
    document it has already seen. Each ingested document (and generated vector) is
    identified by a label.
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: 段落向量学习每个输入到其神经网络架构的单词序列的固定（分布式）向量表示。你可以将整个文档输入到网络中，或者输入其部分，例如文章的章节、段落或句子。定义粒度取决于你。例如，如果你将整个文档输入到网络中，你可以要求它返回它已经看到的最相似的文档。每个摄入的文档（和生成的向量）都有一个标签。
- en: 'Let’s get back to the problem of finding related content for a search engine
    on Wikipedia pages. In the previous section, we used Lucene’s `MoreLikeThis` tool
    to extract the most important terms and then used them as a query to fetch related
    content. Unfortunately, the accuracy rates were low, primarily for these reasons:'
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到在维基百科页面上为搜索引擎查找相关内容的问题。在上一个部分中，我们使用了Lucene的`MoreLikeThis`工具来提取最重要的术语，然后使用它们作为查询来获取相关内容。不幸的是，准确率很低，主要原因如下：
- en: The most important terms extracted by `MoreLikeThis` were okay, but could be
    better.
  id: totrans-1120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MoreLikeThis`提取的最重要术语还不错，但可以更好。'
- en: If you look at the set of important terms from a document, you may not recognize
    what kind of document they came from.
  id: totrans-1121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你查看文档中的重要术语集合，你可能无法识别它们来自哪种类型的文档。
- en: 'Let’s look again at our friend the “Ledgewood Circle” page. According to MLT,
    the most important terms are as follows:'
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次看看我们的朋友“Ledgewood Circle”页面。根据MLT，最重要的术语如下：
- en: '[PRE109]'
  id: totrans-1123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'By no means would it be possible to say that these terms come from the “Ledgewood
    Circle” page, so you can’t expect very accurate related-content suggestions. With
    document embeddings, there’s no explicit information you can look at (that’s a
    general problem in deep learning: it’s not easy to understand what these black
    boxes do). A paragraph vector’s neural network adjusts each document’s vector
    values during training, as explained in [chapter 5](kindle_split_017.xhtml#ch05).'
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: 根本不可能说这些术语来自“Ledgewood Circle”页面，因此你不能期望得到非常准确的相关内容建议。通过文档嵌入，没有明确的信息可以查看（这是深度学习中的普遍问题：很难理解这些黑盒子的行为）。段落向量的神经网络在训练过程中调整每个文档的向量值，如第5章所述[chapter
    5](kindle_split_017.xhtml#ch05)。
- en: Let’s fetch related content by finding the nearest vectors to the vector representing
    the input document, using cosine similarity. To do this, you first run a user-entered
    query—for example, “Ledgewood Circle”—that returns search results. For each such
    result, you extract its vector representation and look at its nearest neighbors
    in the embeddings space. This is like navigating on a graph or map that has all
    documents plotted according to their semantic similarity. You go to the point
    that represents “Ledgewood Circle,” find the nearest points, and see which documents
    they represent. You’ll notice that the “Ledgewood Circle” vector’s neighbors will
    represent documents dealing with traffic and transportation topics; if you instead
    pick, for example, the vectors of some documents about music, you’ll see they’ll
    be located far away from “Ledgewood Circle” and its neighbors in the embedding
    space (see [figure 6.8](#ch06fig08)).
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过找到表示输入文档的向量在嵌入空间中的最近向量来获取相关内容，使用余弦相似度。为此，你首先运行一个用户输入的查询——例如，“Ledgewood
    Circle”——它返回搜索结果。对于每个这样的结果，你提取其向量表示并查看其在嵌入空间中的最近邻。这就像在一个所有文档都根据它们的语义相似性绘制的图或地图上导航。你到达代表“Ledgewood
    Circle”的点，找到最近的点，并查看它们代表哪些文档。你会注意到“Ledgewood Circle”向量的邻居将代表处理交通和运输主题的文档；如果你选择，例如，一些关于音乐的文档的向量，你会发现它们在嵌入空间中离“Ledgewood
    Circle”及其邻居很远（见[图6.8](#ch06fig08)）。
- en: Figure 6.8\. Paragraph vectors for “Ledgewood Circle” and its neighbors, compared
    with music-related paragraph vectors
  id: totrans-1126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.8. “Ledgewood Circle”及其邻近段落向量与音乐相关段落向量的比较
- en: '![](Images/06fig08_alt.jpg)'
  id: totrans-1127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig08_alt.jpg)'
- en: 'Similarly to what you do for ranking, you first feed the paragraph vector network
    the indexed data:'
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: 与你用于排名的做法类似，你首先将索引数据输入到段落向量网络中：
- en: '[PRE110]'
  id: totrans-1129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Once that’s done, you can use DL4J’s built-in `nearestLabels` method to find
    the document vectors closest to the “Ledgewood Circle” vector. Internally, this
    method uses cosine similarity to measure how close two vectors are:'
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，你可以使用DL4J的内置`nearestLabels`方法来找到最接近“Ledgewood Circle”向量的文档向量。内部，此方法使用余弦相似度来衡量两个向量有多接近：
- en: '[PRE111]'
  id: totrans-1131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '***1* Runs the original query**'
  id: totrans-1132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 运行原始查询**'
- en: '***2* For each result, builds a label**'
  id: totrans-1133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 为每个结果构建标签**'
- en: '***3* Fetches the document embedding for the search result**'
  id: totrans-1134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 获取搜索结果的文档嵌入**'
- en: '***4* Finds the labels of the nearest vectors to the search result vector**'
  id: totrans-1135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 寻找与搜索结果向量最近的向量的标签**'
- en: '***5* For each nearest vector, parses its label and fetches the corresponding
    Lucene Document**'
  id: totrans-1136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 对于每个最近的向量，解析其标签并获取相应的Lucene文档**'
- en: 'The results are as follows:'
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE112]'
  id: totrans-1138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'Just from looking at this simple example, the results seem to be better than
    those given by MLT. There are no off-topic results: they all relate to transportation
    (whereas MLT returned the “Modal dispersion” page, which refers to optics).'
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
  zh: 只看这个简单的例子，结果似乎比MLT给出的结果要好。没有不相关的结果：它们都与交通有关（而MLT返回了“Modal dispersion”页面，它指的是光学）。
- en: 'To confirm your good feelings, you can do the same thing you did to measure
    the effectiveness of `MoreLikeThis` by calculating the average accuracy of this
    method. To make a fair comparison, use the same approach of checking whether any
    of the search result’s categories (such as “Ledgewood Circle”) also appear in
    the related-content categories. Using the same randomly generated queries used
    when evaluating MLT, paragraph vectors yield the following average accuracy:'
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认你的良好感觉，你可以通过计算此方法的平均准确率来测量`MoreLikeThis`的有效性。为了进行公平的比较，使用相同的方法检查搜索结果的任何类别（例如“Ledgewood
    Circle”）是否也出现在相关内容类别中。使用在评估MLT时使用的相同随机生成的查询，段落向量产生了以下平均准确率：
- en: '[PRE113]'
  id: totrans-1141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: The best average accuracy for MLT was 0.09; 0.37 is much better.
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
  zh: MLT的最佳平均准确率为0.09；0.37要好得多。
- en: Finding similar documents with close semantics is one of the key advantages
    of using document embeddings and is also why they’re so useful in natural language
    processing and search. As you’ve seen, they can be used in various ways, including
    for ranking and to retrieve similar content. Paragraph vectors aren’t the only
    way you can learn document embeddings, though. You used averaged word embeddings
    in [chapter 5](kindle_split_017.xhtml#ch05), but researchers keep working on better
    and more advanced ways of extracting word and document embeddings.
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文档嵌入找到语义相似的文档是使用文档嵌入的关键优势之一，也是为什么它们在自然语言处理和搜索中如此有用的原因。正如你所见，它们可以用各种方式使用，包括用于排名和检索相似内容。然而，段落向量并不是学习文档嵌入的唯一方法。你在[第5章](kindle_split_017.xhtml#ch05)中使用了平均词嵌入，但研究人员仍在努力研究提取词和文档嵌入的更好和更高级的方法。
- en: 6.3.4\. Retrieving similar content with vectors from encoder-decoder models
  id: totrans-1144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.4\. 使用编码器-解码器模型中的向量检索相似内容
- en: '[Chapters 3](kindle_split_015.xhtml#ch03) and [4](kindle_split_016.xhtml#ch04)
    introduced a deep neural network architecture called the *encoder-decoder* (or
    *sequence-to-sequence* [seq2seq]) model. You may remember that this model consists
    of an encoder LSTM network and a decoder LSTM network. The encoder transforms
    an input sequence of words into a fixed-length dense vector as output; this output
    is the input to the decoder, which turns it back into a sequence of words as the
    final output (see [figure 6.9](#ch06fig09)). You’ve used such an architecture
    to produce alternative query representations and to help users type a query. In
    this case, you’re instead interested in using the output of the encoder network,
    the so-called *thought vector*.'
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: '[第3章](kindle_split_015.xhtml#ch03) 和 [第4章](kindle_split_016.xhtml#ch04) 介绍了一种称为*编码器-解码器*（或*序列到序列*
    [seq2seq]）的深度神经网络架构。你可能记得，该模型由一个编码器LSTM网络和一个解码器LSTM网络组成。编码器将输入序列的单词转换为一个固定长度的密集向量作为输出；这个输出是解码器的输入，解码器将其转换回一个单词序列作为最终输出（见[图6.9](#ch06fig09)）。你已经使用这种架构来生成替代查询表示并帮助用户输入查询。在这种情况下，你感兴趣的是使用编码器网络的输出，所谓的*思维向量*。'
- en: Figure 6.9\. Encoder-decoder model
  id: totrans-1146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.9\. 编码器-解码器模型
- en: '![](Images/06fig09_alt.jpg)'
  id: totrans-1147
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig09_alt.jpg)'
- en: The reason it’s called a thought vector is that it’s meant to be a compressed
    representation of the input text sequence, which, when decoded correctly, generates
    a desirable output sequence. Seq2seq models, as you’ll see in the next chapter,
    are also used for machine translation; they can transform a sentence in an input
    language into a translated output sequence. You want to extract such thought vectors
    for the input sequences (documents, sentences, and so on) and use them the same
    way you used paragraph vectors to measure similarity between documents.
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: 它被称为思想向量的原因在于它旨在是输入文本序列的压缩表示，当正确解码时，可以生成期望的输出序列。Seq2seq 模型，正如你将在下一章看到的，也用于机器翻译；它们可以将输入语言的句子转换成翻译后的输出序列。你想要提取这样的思想向量用于输入序列（文档、句子等），并像使用段落向量来衡量文档之间的相似度一样使用它们。
- en: First, you need to hook into the training phase so you can “save” the embeddings
    as they’re generated one step at a time. You place them in a `WeightLookupTable`,
    which is the entity responsible for holding word vectors in word2vec and paragraph
    vectors in `ParagraphVectors` objects. With DL4J, you can hook into the training
    phase with a `TrainingListener` that captures the forward pass as the thought
    vector is generated by the encoder LSTM. You extract the input vector and transform
    it back into a sequence by retrieving words one at a time from the original corpus.
    Then, you extract the thought vector and put the sequence with its thought vector
    into the `WeightLookupTable`.
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要钩入训练阶段，这样你就可以“保存”在生成时一步一步生成的嵌入。你将它们放在一个 `WeightLookupTable` 中，这是负责在 word2vec
    中持有词向量以及在 `ParagraphVectors` 对象中持有段落向量的实体。使用 DL4J，你可以通过一个 `TrainingListener` 钩入训练阶段，该监听器捕获编码器
    LSTM 生成思想向量时的正向传递。你提取输入向量，并通过从原始语料库中逐个检索单词将其转换回一个序列。然后，你提取思想向量，并将带有其思想向量的序列放入
    `WeightLookupTable`。
- en: Listing 6.3\. Extracting thought vectors during encoder-decoder training
  id: totrans-1150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.3\. 在编码器-解码器训练期间提取思想向量
- en: '[PRE114]'
  id: totrans-1151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '***1* Fetches the network input (a sequence of words transformed into vectors)
    from the input layer**'
  id: totrans-1152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 从输入层获取网络输入（将单词序列转换为向量）**'
- en: '***2* Fetches the thought vector from the thought-vector layer**'
  id: totrans-1153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 从思想向量层获取思想向量**'
- en: '***3* Rebuilds the sequence one word at a time from the input vector**'
  id: totrans-1154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 逐个单词从输入向量中重建序列**'
- en: '***4* Merges the words together in a sequence (as a String)**'
  id: totrans-1155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 将序列中的单词合并在一起（作为一个字符串）**'
- en: '***5* Records the thought vector associated with the input text sequence**'
  id: totrans-1156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 记录与输入文本序列相关的思想向量**'
- en: With these vectors, you can reach the same level of accuracy as paragraph vectors;
    the difference lies in the fact that you can decide how to influence them. These
    thought vectors are generated as an intermediate product of encoder and decoder
    LSTM networks. You can decide what to include in the encoder input and what to
    include in the decoder output in the training phase. If you put documents belonging
    to the same category at the edges of the network, the generated thought vectors
    will learn to output documents whose categories are the same. Therefore, you can
    achieve much higher accuracy.
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些向量，你可以达到与段落向量相同级别的准确度；区别在于你可以决定如何影响它们。这些思想向量是编码器和解码器 LSTM 网络的中间产品。你可以在训练阶段决定包含在编码器输入中的内容以及包含在解码器输出中的内容。如果你将属于同一类别的文档放在网络的边缘，生成的思想向量将学会输出具有相同类别的文档。因此，你可以实现更高的准确度。
- en: If you take the encoder-decoder LSTM defined in [chapters 3](kindle_split_015.xhtml#ch03)
    and [4](kindle_split_016.xhtml#ch04) and train it with documents belonging to
    the same category, you’ll get an average accuracy of 0.77\. That’s much higher
    than even paragraph vectors!
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用第 3 章（kindle_split_015.xhtml#ch03）和第 4 章（kindle_split_016.xhtml#ch04）中定义的编码器-解码器
    LSTM 并用属于同一类别的文档进行训练，你将得到平均准确率为 0.77。这比段落向量还要高得多！
- en: Summary
  id: totrans-1159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Paragraph vector models provide distributed representations for sentences and
    documents at configurable granularity (sentence, paragraph, or document).
  id: totrans-1160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 段落向量模型以可配置的粒度（句子、段落或文档）为句子和文档提供分布式表示。
- en: Ranking functions based on paragraph vectors can be more effective than old-school
    statistical models and those based on word embedding because they capture semantics
    at a sentence or document level.
  id: totrans-1161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于段落向量的排名函数可能比老式的统计模型和基于词嵌入的模型更有效，因为它们在句子或文档级别捕获语义。
- en: Paragraph vectors can also be used to effectively retrieve related content based
    on document semantics, to decorate search results.
  id: totrans-1162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 段落向量也可以用于根据文档语义有效地检索相关内容，以装饰搜索结果。
- en: Thought vectors can be extracted from seq2seq models to retrieve related content
    based on document semantics, to decorate search results.
  id: totrans-1163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以从seq2seq模型中提取思维向量，以根据文档语义检索相关内容，以装饰搜索结果。
