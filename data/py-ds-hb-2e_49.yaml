- en: 'Chapter 44\. In Depth: Decision Trees and Random Forests'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第44章\. 深入探讨：决策树和随机森林
- en: 'Previously we have looked in depth at a simple generative classifier (naive
    Bayes; see [Chapter 41](ch41.xhtml#section-0505-naive-bayes)) and a powerful discriminative
    classifier (support vector machines; see [Chapter 43](ch43.xhtml#section-0507-support-vector-machines)).
    Here we’ll take a look at another powerful algorithm: a nonparametric algorithm
    called *random forests*. Random forests are an example of an *ensemble* method,
    meaning one that relies on aggregating the results of a set of simpler estimators.
    The somewhat surprising result with such ensemble methods is that the sum can
    be greater than the parts: that is, the predictive accuracy of a majority vote
    among a number of estimators can end up being better than that of any of the individual
    estimators doing the voting! We will see examples of this in the following sections.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们深入研究了一个简单的生成分类器（朴素贝叶斯；见 [第41章](ch41.xhtml#section-0505-naive-bayes)）和一个强大的判别分类器（支持向量机；见
    [第43章](ch43.xhtml#section-0507-support-vector-machines)）。在这里，我们将看看另一种强大的算法：一个称为
    *随机森林* 的非参数算法。随机森林是一种 *集成* 方法的示例，意味着它依赖于聚合一组更简单的估算器的结果。这样的集成方法的一个令人惊讶的结果是，总和可以大于各部分之和：也就是说，多个估算器之间的多数投票的预测准确度最终可能会比任何进行投票的单个估算器的准确度更高！我们将在以下部分看到这方面的例子。
- en: 'We begin with the standard imports:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从标准导入开始：
- en: '[PRE0]'
  id: totrans-3
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Motivating Random Forests: Decision Trees'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推动随机森林的动机：决策树
- en: Random forests are an example of an ensemble learner built on decision trees.
    For this reason, we’ll start by discussing decision trees themselves.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是建立在决策树上的集成学习器的一个例子。因此，我们将首先讨论决策树本身。
- en: 'Decision trees are extremely intuitive ways to classify or label objects: you
    simply ask a series of questions designed to zero in on the classification. For
    example, if you wanted to build a decision tree to classify animals you come across
    while on a hike, you might construct the one shown in [Figure 44-1](#fig_images_in_0508-decision-tree).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是极其直观的分类或标记对象的方式：你只需提出一系列旨在对分类进行精准定位的问题。例如，如果你想构建一个用于对徒步时遇到的动物进行分类的决策树，你可以构建如
    [图 44-1](#fig_images_in_0508-decision-tree) 所示的决策树。
- en: '![05.08 decision tree](assets/05.08-decision-tree.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![05.08 决策树](assets/05.08-decision-tree.png)'
- en: Figure 44-1\. An example of a binary decision tree^([1](ch44.xhtml#idm45858730712544))
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 44-1\. 二叉决策树的示例^([1](ch44.xhtml#idm45858730712544))
- en: 'The binary splitting makes this extremely efficient: in a well-constructed
    tree, each question will cut the number of options by approximately half, very
    quickly narrowing the options even among a large number of classes. The trick,
    of course, comes in deciding which questions to ask at each step. In machine learning
    implementations of decision trees, the questions generally take the form of axis-aligned
    splits in the data: that is, each node in the tree splits the data into two groups
    using a cutoff value within one of the features. Let’s now look at an example
    of this.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分割使其极其高效：在构造良好的树时，每个问题将使选项数量减少约一半，非常快速地将选项缩小，即使在大量类别中也是如此。当然，关键在于决定每一步要问什么问题。在决策树的机器学习实现中，问题通常采用数据中的轴对齐分割形式：即，树中的每个节点都使用一个特征内的截止值将数据分为两组。现在让我们看一个示例。
- en: Creating a Decision Tree
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建决策树
- en: Consider the following two-dimensional data, which has one of four class labels
    (see [Figure 44-2](#fig_0508-random-forests_files_in_output_8_0)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下二维数据，它具有四个类标签之一（参见 [图 44-2](#fig_0508-random-forests_files_in_output_8_0)）。
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![output 8 0](assets/output_8_0.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![output 8 0](assets/output_8_0.png)'
- en: Figure 44-2\. Data for the decision tree classifier
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 44-2\. 决策树分类器的数据
- en: A simple decision tree built on this data will iteratively split the data along
    one or the other axis according to some quantitative criterion, and at each level
    assign the label of the new region according to a majority vote of points within
    it. [Figure 44-3](#fig_images_in_0508-decision-tree-levels) presents a visualization
    of the first four levels of a decision tree classifier for this data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些数据构建的简单决策树将根据某些定量标准迭代地沿着一个或另一个轴将数据分割，并在每个级别根据其中的点的多数投票确定新区域的标签。[图 44-3](#fig_images_in_0508-decision-tree-levels)
    展示了此数据的决策树分类器的前四个级别的可视化。
- en: '![05.08 decision tree levels](assets/05.08-decision-tree-levels.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![05.08 决策树级别](assets/05.08-decision-tree-levels.png)'
- en: Figure 44-3\. Visualization of how the decision tree splits the data^([2](ch44.xhtml#idm45858730648048))
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图44-3\. 决策树如何分割数据的可视化^([2](ch44.xhtml#idm45858730648048))
- en: Notice that after the first split, every point in the upper branch remains unchanged,
    so there is no need to further subdivide this branch. Except for nodes that contain
    all of one color, at each level *every* region is again split along one of the
    two features.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第一个分割后，上层每个点保持不变，因此无需进一步细分此分支。除了包含同一颜色的节点外，在每个级别*每个*区域再次沿着两个特征之一进行分割。
- en: 'This process of fitting a decision tree to our data can be done in Scikit-Learn
    with the `DecisionTreeClassifier` estimator:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scikit-Learn中，可以使用`DecisionTreeClassifier`估计器来拟合决策树到我们的数据：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s write a utility function to help us visualize the output of the classifier:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个实用函数来帮助我们可视化分类器的输出：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now we can examine what the decision tree classification looks like (see [Figure 44-4](#fig_0508-random-forests_files_in_output_17_0)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看一下决策树分类的样子（参见[图44-4](#fig_0508-random-forests_files_in_output_17_0)）。
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![output 17 0](assets/output_17_0.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![output 17 0](assets/output_17_0.png)'
- en: Figure 44-4\. Visualization of a decision tree classification
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图44-4\. 决策树分类的可视化
- en: 'If you’re running this notebook live, you can use the helper script included
    in the online [appendix](https://oreil.ly/etDrN) to bring up an interactive visualization
    of the decision tree building process:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在实时运行此笔记本，您可以使用在线[附录](https://oreil.ly/etDrN)中包含的辅助脚本来打开决策树构建过程的交互式可视化：
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Notice that as the depth increases, we tend to get very strangely shaped classification
    regions; for example, at a depth of five, there is a tall and skinny purple region
    between the yellow and blue regions. It’s clear that this is less a result of
    the true, intrinsic data distribution, and more a result of the particular sampling
    or noise properties of the data. That is, this decision tree, even at only five
    levels deep, is clearly overfitting our data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，随着深度的增加，我们往往会得到非常奇怪形状的分类区域；例如，在深度为五时，在黄色和蓝色区域之间有一个高而瘦的紫色区域。显然，这不是真实的内在数据分布的结果，而更多地是数据的特定采样或噪声特性的结果。也就是说，即使在仅深度为五的情况下，这棵决策树明显地过拟合了我们的数据。
- en: Decision Trees and Overfitting
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树和过拟合
- en: 'Such overfitting turns out to be a general property of decision trees: it is
    very easy to go too deep in the tree, and thus to fit details of the particular
    data rather than the overall properties of the distributions it is drawn from.
    Another way to see this overfitting is to look at models trained on different
    subsets of the data—for example, in [Figure 44-5](#fig_images_in_0508-decision-tree-overfitting)
    we train two different trees, each on half of the original data.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种过拟合事实上是决策树的一个普遍特性：很容易使树的深度过深，从而适应特定数据的细节，而不是它们抽取自的分布的总体特性。另一种看待这种过拟合的方法是查看在不同数据子集上训练的模型——例如，在[图44-5](#fig_images_in_0508-decision-tree-overfitting)中，我们训练了两棵不同的树，每棵树都使用了原始数据的一半。
- en: '![05.08 decision tree overfitting](assets/05.08-decision-tree-overfitting.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![05.08 decision tree overfitting](assets/05.08-decision-tree-overfitting.png)'
- en: Figure 44-5\. An example of two randomized decision trees^([3](ch44.xhtml#idm45858730171984))
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图44-5\. 两棵随机决策树的示例^([3](ch44.xhtml#idm45858730171984))
- en: It is clear that in some places the two trees produce consistent results (e.g.,
    in the four corners), while in other places the two trees give very different
    classifications (e.g., in the regions between any two clusters). The key observation
    is that the inconsistencies tend to happen where the classification is less certain,
    and thus by using information from *both* of these trees, we might come up with
    a better result!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，在某些地方，两棵树产生一致的结果（例如，在四个角落），而在其他地方，两棵树给出非常不同的分类结果（例如，在任意两个簇之间的区域）。关键观察是，这种不一致往往发生在分类不确定的地方，因此通过使用*这两棵*树的信息，我们可能会得到更好的结果！
- en: 'If you are running this notebook live, the following function will allow you
    to interactively display the fits of trees trained on a random subset of the data:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在实时运行此笔记本，以下函数将允许您交互地显示在数据的随机子集上训练的树的拟合情况：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Just as using information from two trees improves our results, we might expect
    that using information from many trees would improve our results even further.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 正如利用两棵树的信息可以改进我们的结果一样，我们可能期望利用许多树的信息进一步改进我们的结果。
- en: 'Ensembles of Estimators: Random Forests'
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成估计器：随机森林
- en: This notion—that multiple overfitting estimators can be combined to reduce the
    effect of this overfitting—is what underlies an ensemble method called *bagging*.
    Bagging makes use of an ensemble (a grab bag, perhaps) of parallel estimators,
    each of which overfits the data, and averages the results to find a better classification.
    An ensemble of randomized decision trees is known as a *random forest*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多个过拟合估计器组合以减少过拟合效应的概念，是支持一种称为*bagging*的集成方法的基础。Bagging利用一个并行估计器的集合（可能是一个抓袋），每个估计器都会对数据过拟合，并对结果进行平均以找到更好的分类。随机化决策树的集成称为*随机森林*。
- en: This type of bagging classification can be done manually using Scikit-Learn’s
    `BaggingClassifier` meta-estimator, as shown here (see [Figure 44-6](#fig_0508-random-forests_files_in_output_28_0)).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这种袋装分类可以通过Scikit-Learn的`BaggingClassifier`元估计器手动完成，如下所示（见[图 44-6](#fig_0508-random-forests_files_in_output_28_0)）。
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In this example, we have randomized the data by fitting each estimator with
    a random subset of 80% of the training points. In practice, decision trees are
    more effectively randomized by injecting some stochasticity in how the splits
    are chosen: this way all the data contributes to the fit each time, but the results
    of the fit still have the desired randomness. For example, when determining which
    feature to split on, the randomized tree might select from among the top several
    features. You can read more technical details about these randomization strategies
    in the [Scikit-Learn documentation](https://oreil.ly/4jrv4) and references within.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们通过对训练点的随机80%子集拟合每个估计器来随机化数据。在实践中，通过在如何选择分割时注入一些随机性来更有效地随机化决策树：这样每次都会使所有数据对拟合有贡献，但拟合结果仍具有所需的随机性。例如，在确定要分割哪个特征时，随机树可能从顶部几个特征中选择。您可以在[Scikit-Learn文档](https://oreil.ly/4jrv4)和其中的参考文献中阅读有关这些随机化策略的更多技术细节。
- en: '![output 28 0](assets/output_28_0.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![output 28 0](assets/output_28_0.png)'
- en: Figure 44-6\. Decision boundaries for an ensemble of random decision trees
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 44-6\. 随机决策树集成的决策边界
- en: In Scikit-Learn, such an optimized ensemble of randomized decision trees is
    implemented in the `RandomForestClassifier` estimator, which takes care of all
    the randomization automatically. All you need to do is select a number of estimators,
    and it will very quickly—in parallel, if desired—fit the ensemble of trees (see
    [Figure 44-7](#fig_0508-random-forests_files_in_output_30_0)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scikit-Learn中，这样一个优化的随机决策树集成是通过`RandomForestClassifier`估计器实现的，它自动处理所有随机化。你只需选择一些估计器，它将非常快速地（如果需要的话是并行的）拟合树的集成（见[图 44-7](#fig_0508-random-forests_files_in_output_30_0)）。
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![output 30 0](assets/output_30_0.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![output 30 0](assets/output_30_0.png)'
- en: Figure 44-7\. Decision boundaries for a random forest, which is an optimized
    ensemble of decision trees
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 44-7\. 随机森林的决策边界，这是一组优化的决策树集成
- en: We see that by averaging over one hundred randomly perturbed models, we end
    up with an overall model that is much closer to our intuition about how the parameter
    space should be split.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到通过对一百个随机扰动模型进行平均，最终得到一个与我们关于参数空间如何分割的直觉更接近的整体模型。
- en: Random Forest Regression
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林回归
- en: In the previous section we considered random forests within the context of classification.
    Random forests can also be made to work in the case of regression (that is, with
    continuous rather than categorical variables). The estimator to use for this is
    the `RandomForestRegressor`, and the syntax is very similar to what we saw earlier.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们考虑了随机森林在分类的上下文中。随机森林也可以在回归的情况下工作（即使用连续变量而不是分类变量）。用于此目的的估计器是`RandomForestRegressor`，其语法与我们之前看到的非常相似。
- en: Consider the following data, drawn from the combination of a fast and slow oscillation
    (see [Figure 44-8](#fig_0508-random-forests_files_in_output_33_0)).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下数据，这些数据来自快速和慢速振荡的组合（见[图 44-8](#fig_0508-random-forests_files_in_output_33_0)）。
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![output 33 0](assets/output_33_0.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![output 33 0](assets/output_33_0.png)'
- en: Figure 44-8\. Data for random forest regression
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 44-8\. 随机森林回归的数据
- en: Using the random forest regressor, we can find the best-fit curve ([Figure 44-9](#fig_0508-random-forests_files_in_output_35_0)).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机森林回归器，我们可以找到最佳拟合曲线（见[图 44-9](#fig_0508-random-forests_files_in_output_35_0)）。
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![output 35 0](assets/output_35_0.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![output 35 0](assets/output_35_0.png)'
- en: Figure 44-9\. Random forest model fit to the data
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 44-9\. 随机森林模型拟合数据
- en: Here the true model is shown in the smooth gray curve, while the random forest
    model is shown by the jagged red curve. The nonparametric random forest model
    is flexible enough to fit the multiperiod data, without us needing to specifying
    a multi-period model!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里显示了真实模型的平滑灰色曲线，而随机森林模型则通过锯齿状红色曲线展示。非参数随机森林模型足够灵活，能够拟合多期数据，而无需指定多期模型！
- en: 'Example: Random Forest for Classifying Digits'
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：用于分类数字的随机森林
- en: 'In [Chapter 38](ch38.xhtml#section-0502-introducing-scikit-learn), we worked
    through an example using the digits dataset included with Scikit-Learn. Let’s
    use that again here to see how the random forest classifier can be applied in
    this context:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 38 章](ch38.xhtml#section-0502-introducing-scikit-learn)中，我们通过一个使用Scikit-Learn提供的数字数据集的示例来工作。让我们再次使用它来看看随机森林分类器在这种情况下的应用：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: To remind us what we’re looking at, we’ll visualize the first few data points
    (see [Figure 44-10](#fig_0508-random-forests_files_in_output_40_0)).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提醒我们正在查看的内容，我们将可视化前几个数据点（参见[图 44-10](#fig_0508-random-forests_files_in_output_40_0)）。
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![output 40 0](assets/output_40_0.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![output 40 0](assets/output_40_0.png)'
- en: Figure 44-10\. Representation of the digits data
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 44-10\. 数字数据的表示
- en: 'We can classify the digits using a random forest as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用随机森林对数字进行分类，如下所示：
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s look at the classification report for this classifier:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个分类器的分类报告：
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: And for good measure, plot the confusion matrix (see [Figure 44-11](#fig_0508-random-forests_files_in_output_46_0)).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 并且为了更直观，绘制混淆矩阵（参见[图 44-11](#fig_0508-random-forests_files_in_output_46_0)）。
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We find that a simple, untuned random forest results in a quite accurate classification
    of the digits data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，一个简单的未调整的随机森林能够对数字数据进行相当准确的分类。
- en: '![output 46 0](assets/output_46_0.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![output 46 0](assets/output_46_0.png)'
- en: Figure 44-11\. Confusion matrix for digit classification with random forests
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 44-11\. 使用随机森林进行数字分类的混淆矩阵
- en: Summary
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: 'This chapter provided a brief introduction to the concept of ensemble estimators,
    and in particular the random forest, an ensemble of randomized decision trees.
    Random forests are a powerful method with several advantages:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本章简要介绍了集成估计器的概念，特别是随机森林，它是随机化决策树的集成。随机森林是一种功能强大的方法，具有多个优点：
- en: Both training and prediction are very fast, because of the simplicity of the
    underlying decision trees. In addition, both tasks can be straightforwardly parallelized,
    because the individual trees are entirely independent entities.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于底层决策树的简单性，训练和预测都非常快。此外，由于每棵树都是独立实体，因此这两个任务可以直接并行化。
- en: 'The multiple trees allow for a probabilistic classification: a majority vote
    among estimators gives an estimate of the probability (accessed in Scikit-Learn
    with the `predict_proba` method).'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多棵树允许进行概率分类：估算器的多数投票给出了概率的估计（在Scikit-Learn中通过`predict_proba`方法访问）。
- en: The nonparametric model is extremely flexible and can thus perform well on tasks
    that are underfit by other estimators.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非参数模型非常灵活，因此在其他估算器欠拟合的任务上表现良好。
- en: 'A primary disadvantage of random forests is that the results are not easily
    interpretable: that is, if you would like to draw conclusions about the *meaning*
    of the classification model, random forests may not be the best choice.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的一个主要缺点是结果不易解释：也就是说，如果你想对分类模型的*含义*得出结论，随机森林可能不是最佳选择。
- en: ^([1](ch44.xhtml#idm45858730712544-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/xP9ZI).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch44.xhtml#idm45858730712544-marker)) 生成此图的代码可在[在线附录](https://oreil.ly/xP9ZI)中找到。
- en: ^([2](ch44.xhtml#idm45858730648048-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/H4WFg).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch44.xhtml#idm45858730648048-marker)) 生成此图的代码可在[在线附录](https://oreil.ly/H4WFg)中找到。
- en: ^([3](ch44.xhtml#idm45858730171984-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/PessV).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch44.xhtml#idm45858730171984-marker)) 生成此图的代码可在[在线附录](https://oreil.ly/PessV)中找到。
