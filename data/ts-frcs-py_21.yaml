- en: '18 Capstone: Forecasting the electric power consumption of a household'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第18章：预测家庭的电力消耗
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Developing deep learning models to predict a household’s electric power consumption
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发深度学习模型以预测家庭的电力消耗
- en: Comparing various multi-step deep learning models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较各种多步深度学习模型
- en: Evaluating the mean absolute error and selecting the champion model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估平均绝对误差并选择冠军模型
- en: Congratulations on making it this far! In chapters 12 to 17, we dove headfirst
    into deep learning for time series forecasting. You learned that statistical models
    become inefficient or unusable when you have large datasets, which usually means
    more than 10,000 data points, with many features. We must then revert to using
    deep learning models, which can leverage all the available information while remaining
    computationally efficient, to produce forecasting models.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你走到了这一步！在第12章到第17章中，我们深入探讨了时间序列预测的深度学习。你了解到，当数据集很大时，统计模型变得低效或无法使用，这通常意味着超过10,000个数据点，并且具有许多特征。因此，我们必须转而使用深度学习模型，这些模型可以利用所有可用信息，同时保持计算效率，以产生预测模型。
- en: 'Just as we had to design a new forecasting procedure in chapter 6 when we started
    modeling time series with the ARMA(*p*,*q*) model, modeling with deep learning
    techniques required us to use yet another modeling procedure: creating windows
    of data with the `DataWindow` class. This class plays a vital role in modeling
    with deep learning, as it allows us to format our data appropriately to create
    a set of inputs and labels for our models, as shown in figure 18.1.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第6章开始使用ARMA(*p*,*q*)模型建模时间序列时必须设计一个新的预测程序一样，使用深度学习技术建模需要我们使用另一种建模程序：使用`DataWindow`类创建数据窗口。这个类在深度学习建模中起着至关重要的作用，因为它允许我们适当地格式化我们的数据，为我们的模型创建一组输入和标签，如图18.1所示。
- en: '![](../../OEBPS/Images/18-01.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/18-01.png)'
- en: Figure 18.1 Example of a data window. This data window has 24 timesteps as input
    and 24 timesteps as output. The model will then use 24 hours of input to generate
    24 hours of predictions. The total length of the data window is the sum of the
    lengths of inputs and labels. In this case, we have a total length of 48 timesteps.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.1 数据窗口的示例。这个数据窗口有24个时间步作为输入和24个时间步作为输出。然后模型将使用24小时的输入来生成24小时的预测。数据窗口的总长度是输入和标签长度的总和。在这种情况下，我们总共有48个时间步。
- en: This data windowing step allows us to produce a wide variety of models, from
    simple linear models to deep neural networks, long short-term memory (LSTM) networks,
    and convolutional neural networks (CNNs). Furthermore, data windowing can be used
    for different scenarios, allowing to us create single-step models where we predict
    only the next timestep, multi-step models where we predict a sequence of future
    steps, and multi-output models where we predict more than one target variable.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据窗口步骤使我们能够产生各种模型，从简单的线性模型到深度神经网络、长短期记忆（LSTM）网络和卷积神经网络（CNN）。此外，数据窗口可以用于不同的场景，使我们能够创建单步模型，其中我们只预测下一个时间步，多步模型，其中我们预测一系列未来的步骤，以及多输出模型，其中我们预测多个目标变量。
- en: Having worked with deep learning in the last several chapters, it’s time to
    apply our knowledge to a capstone project. In this chapter, we’ll walk through
    the steps of a forecasting project using deep learning models. We’ll first look
    at the project and describe the data that we’ll use. Then we’ll cover the data
    wrangling and preprocessing steps. Although those steps do not relate directly
    to time series forecasting, they are crucial steps in any machine learning project.
    We’ll then focus on the modeling steps, where we’ll try a set of deep learning
    models to uncover the best performer.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几章中与深度学习合作后，现在是时候将我们的知识应用到综合项目中了。在本章中，我们将通过使用深度学习模型来指导预测项目的步骤。我们首先查看项目并描述我们将使用的数据。然后我们将涵盖数据整理和预处理步骤。尽管这些步骤与时间序列预测没有直接关系，但它们是任何机器学习项目中的关键步骤。然后我们将专注于建模步骤，我们将尝试一系列深度学习模型，以发现最佳表现者。
- en: 18.1 Understanding the capstone project
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.1 理解综合项目
- en: 'For this project, we’ll use a dataset that tracks the electric power consumption
    of a household. The “Individual household electric power consumption” dataset
    is openly available from the UC Irvine Machine Learning Repository: [https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption](https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将使用一个追踪家庭电力消耗的数据集。UCI机器学习仓库公开提供的“个人家庭电力消耗”数据集：[https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption](https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption)。
- en: Forecasting electric energy consumption is a common task with worldwide applications.
    In developing countries, it can help in planning the construction of power grids.
    In countries where the grid is already developed, forecasting energy consumption
    ensures that the grid can provide enough energy to power all households efficiently.
    With accurate forecasting models, energy companies can better plan the load on
    the grid, ensuring that they are producing enough energy during peak times or
    have sufficient energy reserves to meet the demand. Also, they can avoid producing
    too much electricity, which, if it’s not stored, could cause an imbalance in the
    grid, posing a risk of disconnection. Thus, forecasting electric energy consumption
    is an important problem that has consequences in our daily lives.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 预测电能消耗是一个具有全球应用性的常见任务。在发展中国家，它可以帮助规划电网的建设。在电网已经发达的国家，预测能源消耗确保电网能够高效地为所有家庭提供足够的能源。有了准确的预测模型，能源公司可以更好地规划电网的负载，确保在高峰时段生产足够的能源，或者有足够的能源储备来满足需求。此外，它们可以避免生产过多的电力，如果电力没有被储存，可能会导致电网不平衡，从而面临断电的风险。因此，预测电能消耗是一个重要的日常生活中的问题，它具有深远的影响。
- en: To develop our forecasting model, we’ll use the power consumption dataset mentioned
    previously, which contains the electric consumption for a house in Sceaux, France,
    between December 2006 and November 2010\. The data spans 47 months and was recorded
    at every minute, meaning that we have more than two million data points.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发我们的预测模型，我们将使用之前提到包含法国Sceaux市一栋房屋在2006年12月至2010年11月之间电力消耗的数据集。数据跨度为47个月，并且每分钟记录一次，这意味着我们拥有超过两百万个数据点。
- en: The dataset contains a total of nine columns, listed in table 18.1\. The main
    target is the global active power, as it represents the real power used in a circuit.
    This is the component that is used by the appliances. Reactive power, on the other
    hand, moves between the source and the load of a circuit, so it does not produce
    any useful work.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含总共九列，如表18.1所示。主要目标是全局有功功率，因为它代表了电路中实际使用的功率。这是电器使用的部分。另一方面，无功功率在电路的源和负载之间移动，因此它不会产生任何有用的功。
- en: Table 18.1 Description of the columns in the dataset
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 表18.1 数据集中列的描述
- en: '| Column name | Description |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 列名 | 描述 |'
- en: '| Date | Date in the following format: dd/mm/yyyy |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 日期 | 以下格式的日期：dd/mm/yyyy |'
- en: '| Time | Time in the following format: hh:mm:ss |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 时间 | 以下格式的日期：hh:mm:ss |'
- en: '| Global_active_power | The global active power in kilowatts |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 全球有功功率 | 千瓦（kW）表示的全局有功功率 |'
- en: '| Global_reactive_power | The global reactive power in kilowatts |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 全球无功功率 | 千瓦（kW）表示的全局无功功率 |'
- en: '| Voltage | Voltage in volts |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 电压 | 伏特（V）表示的电压 |'
- en: '| Global_intensity | The current intensity in amperes |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 全球电流强度 | 安培（A）表示的电流强度 |'
- en: '| Sub_metering_1 | Energy consumed in the kitchen by a dishwasher, oven, and
    microwave in watt-hours |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 子计量1 | 洗碗机、烤箱和微波炉在厨房消耗的电能，单位为瓦时（Wh） |'
- en: '| Sub_metering_2 | Energy consumed in the laundry room by a washing machine,
    tumble-dryer, refrigerator, and light in watt-hours |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 子计量2 | 洗衣机、烘干机、冰箱和照明在洗衣房消耗的电能，单位为瓦时（Wh） |'
- en: '| Sub_metering_3 | Energy consumed by a water heater and air conditioner in
    watt-hours |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 子计量3 | 热水器和空调在瓦时（Wh）表示的电能消耗 |'
- en: This dataset does not include any weather information, which could potentially
    be a strong predictor of energy consumption. We can safely expect that during
    hot summer days, the air conditioning unit will function for longer, thus requiring
    more electrical power. The same can be expected during cold winter days, because
    heating a house requires a large amount of energy. This data is not available
    here, but in a professional setting we could request this type of data to augment
    our dataset and potentially produce better models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集不包含任何天气信息，这可能是能源消耗的强大预测因素。我们可以安全地预期，在炎热的夏日，空调将运行更长时间，从而需要更多的电力。在寒冷的冬日，供暖房屋也需要大量的能源。这些数据在这里不可用，但在专业环境中，我们可以请求这类数据来增强我们的数据集，并可能产生更好的模型。
- en: Now that you have a general understanding of the problem and the dataset, let’s
    define the objective of this project and the steps we’ll take to achieve it.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对问题和数据集有了总体了解，让我们定义这个项目的目标和我们将采取的步骤。
- en: 18.1.1 Objective of this capstone project
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.1.1 本综合项目的目标
- en: The objective of this capstone project is to create a model that can forecast
    the next 24 hours of global active power. If you feel confident, this objective
    should be sufficient for you to download the dataset, work it on your own, and
    compare your process to the one presented in this chapter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个综合项目的目标是创建一个可以预测未来24小时全球活跃功率的模型。如果你有信心，这个目标应该足以让你下载数据集，自己处理，并将你的过程与本章中展示的过程进行比较。
- en: 'Otherwise, here are the steps that need to be done:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，以下是需要执行的步骤：
- en: 'Data wrangling and preprocessing. This step is optional. It is not directly
    linked to time series forecasting, but it is an important step in any machine
    learning project. You can safely skip this step and start at step 2 with a clean
    dataset:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据处理和预处理。这一步是可选的。它并不直接与时间序列预测相关联，但在任何机器学习项目中都是一个重要的步骤。你可以安全地跳过这一步，从步骤2开始，使用干净的数据集：
- en: Calculate the number of missing values.
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算缺失值的数量。
- en: Impute the missing values.
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 补充缺失值。
- en: Express each variable as a numerical value (all data is originally stored as
    strings).
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个变量表示为数值（所有数据最初都存储为字符串）。
- en: Combine the Date and Time columns into a `DateTime` object.
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将日期和时间列合并为一个`DateTime`对象。
- en: Determine whether the data sampled at every minute is usable for forecasting.
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定每分钟采样的数据是否可用于预测。
- en: Resample the data by hour.
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按小时重采样数据。
- en: Remove any incomplete hours.
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除任何不完整的小时。
- en: 'Feature engineering:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征工程：
- en: Identify any seasonality.
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别任何季节性。
- en: Encode the time with a sine and cosine transformation.
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用正弦和余弦变换对时间进行编码。
- en: Scale the data.
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缩放数据。
- en: 'Split the data:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分割数据：
- en: Make a 70:20:10 split to create training, validation, and test sets.
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行70:20:10的分割以创建训练集、验证集和测试集。
- en: 'Prepare for deep learning modeling:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备深度学习建模：
- en: Implement the `DataWindow` class.
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现`DataWindow`类。
- en: Define the `compile_and_fit` function.
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`compile_and_fit`函数。
- en: Create a dictionary of column indices and column names.
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含列索引和列名称的字典。
- en: 'Model with deep learning:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 深度学习模型：
- en: Train at least one baseline model.
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练至少一个基线模型。
- en: Train a linear model.
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个线性模型。
- en: Train a deep neural network.
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个深度神经网络。
- en: Train an LSTM.
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个LSTM。
- en: Train a CNN.
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个CNN。
- en: Train a combination of LSTM and CNN.
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练LSTM和CNN的组合。
- en: Train an autoregressive LSTM.
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个自回归LSTM。
- en: Select the best-performing model.
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择表现最好的模型。
- en: You now have all the steps required to successfully complete this capstone project.
    I highly recommend that you try it on your own first, as that will reveal what
    you have mastered and what you need to review. At any point, you can refer to
    the following sections for a detailed walkthrough of each step.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在拥有了完成这个综合项目所需的所有步骤。我强烈建议你先自己尝试，因为这会揭示你已经掌握的内容和需要复习的内容。在任何时候，你都可以参考以下部分，以详细了解每个步骤。
- en: 'The entire solution is available on GitHub: [https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH18](https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH18).
    Note that the data files were too large to be included in the repository, so you’ll
    need to download the dataset separately. Good luck!'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 整个解决方案可在GitHub上找到：[https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH18](https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH18)。请注意，数据文件太大，无法包含在存储库中，因此你需要单独下载数据集。祝你好运！
- en: 18.2 Data wrangling and preprocessing
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.2 数据处理和预处理
- en: Data wrangling is the process of transforming data into a form that is easily
    usable for modeling. This step usually involves exploring missing data, filling
    in blank values, and ensuring that the data has the right type, meaning that numbers
    are numerical values and not strings. This is a complex step, and it’s probably
    the most vital one in any machine learning project. Having poor quality data at
    the start of a forecasting project is a guarantee that you’ll have poor quality
    forecasts. You can skip this section of the chapter if you wish to focus solely
    on time series forecasting, but I highly recommend that you go through it, as
    it will really help you become comfortable with the dataset.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗是将数据转换成易于建模使用的形式的过程。这一步通常涉及探索缺失数据、填充空白值，并确保数据具有正确的类型，这意味着数字是数值而不是字符串。这是一个复杂的步骤，可能是任何机器学习项目中最重要的一个。在预测项目开始时拥有质量较差的数据将保证你会有质量较差的预测。如果你只想专注于时间序列预测，你可以跳过本章的这一部分，但我强烈建议你阅读它，因为它真的会帮助你熟悉数据集。
- en: 'Note If you have not done so already, you can download the “Individual household
    electric power consumption” dataset from the UC Irvine Machine Learning Repository:
    [https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption](https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你还没有这样做，你可以从UC Irvine机器学习仓库下载“个人家庭电力消耗”数据集：[https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption](https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption)。
- en: To perform this data wrangling, you can start by importing libraries that will
    be useful for data manipulation and visualization into a Python script or Jupyter
    Notebook.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此数据清洗，你可以从将用于数据操作和可视化的库导入Python脚本或Jupyter Notebook开始。
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Whenever `numpy` and TensorFlow are used, I like to set a random seed to ensure
    that the results can be reproduced. If you do not set a seed, your results might
    vary, and if you set a seed that’s different than mine, your results will differ
    from those shown here.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 每当使用`numpy`和TensorFlow时，我喜欢设置一个随机种子以确保结果可以重现。如果你没有设置种子，你的结果可能会有所不同；如果你设置了一个与我不同的种子，你的结果将与这里显示的不同。
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The next step is to read the data file into a `DataFrame`. We are working with
    a raw text file, but we can still use the `read_csv` method from `pandas`. We
    simply need to specify the separator, which is a semicolon in this case.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将数据文件读入一个`DataFrame`。我们正在处理一个原始文本文件，但仍然可以使用`pandas`的`read_csv`方法。我们只需指定分隔符，在这种情况下是一个分号。
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ We can use this method with a .txt file as long as we specify the separator.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❶只要我们指定了分隔符，我们就可以使用这种方法来处理.txt文件。
- en: We can optionally display the first five rows with `df.head()` and the last
    five rows with `df.tail()`. This will show us that our data starts on December
    16, 2006, at 5:24 p.m. and ends on November 26, 2010, at 9:02 p.m. and that the
    data was collected at every minute. We can also display the shape of our data
    with `df.shape`, showing us that we have 2,075,529 rows and 9 columns.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择使用`df.head()`显示前五行，使用`df.tail()`显示最后五行。这将显示我们的数据从2006年12月16日下午5:24开始，到2010年11月26日下午9:02结束，并且数据是每分钟收集的。我们还可以使用`df.shape`显示我们数据的形状，这会显示我们有2,075,529行和9列。
- en: 18.2.1 Dealing with missing data
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.2.1 处理缺失数据
- en: Now let’s check for missing values. We can do this by chaining the `isna()`
    method with the `sum()` method. This returns the sum of missing values for each
    column of our dataset.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来检查缺失值。我们可以通过将`isna()`方法与`sum()`方法链接起来来完成这项工作。这会返回我们数据集中每一列的缺失值总和。
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: From the output shown in figure 18.2, only the Sub_metering_3 column has missing
    values. In fact, about 1.25% of its values are missing, according to the documentation
    of the data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从图18.2所示的输出中，只有Sub_metering_3列有缺失值。实际上，根据数据文档，其大约1.25%的值是缺失的。
- en: '![](../../OEBPS/Images/18-02.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/18-02.png)'
- en: Figure 18.2 Output of the total number of missing values in our dataset. You
    can see that only the Sub_metering_3 column has missing values.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.2展示了我们数据集中缺失值的总数。您可以看到，只有Sub_metering_3列存在缺失值。
- en: There are two options we can explore for dealing with the missing values. First,
    we could simply delete this column, since no other features have missing values.
    Second, we could fill in the missing values with a certain value. This process
    is called *imputing*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失值有两种可选方案。首先，我们可以简单地删除这一列，因为其他特征没有缺失值。其次，我们可以用某个特定值填充缺失值。这个过程被称为*插补*。
- en: 'We’ll first check whether there are many consecutive missing values. If that
    is the case, it is preferable to get rid of the column, as imputing many consecutive
    values will likely introduce a nonexistent trend in our data. Otherwise, if the
    missing values are dispersed across time, it is reasonable to fill them. The following
    code block outputs the length of the longest sequence of consecutive missing values:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先检查是否存在许多连续的缺失值。如果是这样，最好是删除这一列，因为插补许多连续的值可能会在我们的数据中引入一个不存在的趋势。否则，如果缺失值分散在时间上，填充它们是合理的。以下代码块输出了最长连续缺失值序列的长度：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This outputs a length of 7,226 consecutive minutes of missing data, which is
    equivalent to roughly 5 days. In this case, the gap is definitely too large to
    fill with missing values, so we’ll remove this column from the dataset.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这输出了7,226分钟的连续缺失数据长度，相当于大约5天。在这种情况下，这个差距肯定太大，无法用缺失值填充，所以我们将从数据集中删除这一列。
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We no longer have any missing data in our dataset, so we can move on to the
    next step.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集中不再有任何缺失数据，因此我们可以继续下一步。
- en: 18.2.2 Data conversion
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.2.2 数据转换
- en: Now let’s check if our data has the right type. We should be studying numerical
    data, as our dataset is a collection of sensor readings.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来检查数据是否具有正确的类型。我们应该研究数值数据，因为我们的数据集是传感器读数的集合。
- en: We can output the type of each column using `df.dtypes`, which shows us that
    each column is of `object` type. In `pandas` this means that our data is mostly
    text, or a mix of numeric and non-numeric values.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`df.dtypes`输出每一列的类型，这显示每一列都是`object`类型。在`pandas`中这意味着我们的数据主要是文本，或者数值和非数值值的混合。
- en: We can convert each column to a numerical value with the `to_numeric` function
    from `pandas`. This is essential, as our models expect numerical data. Note that
    we will not convert the date and time columns to numerical values—these will be
    processed in a later step.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`pandas`中的`to_numeric`函数将每一列转换为数值。这是非常重要的，因为我们的模型期望的是数值数据。请注意，我们不会将日期和时间列转换为数值——这些将在后续步骤中处理。
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can optionally check the type of each column again using `df.dtypes` to make
    sure that the values were converted correctly. This will show that every column
    from Global_active_power to Sub_metering_2 is now a `float64` as expected.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择再次使用`df.dtypes`检查每一列的类型，以确保值已正确转换。这将显示从Global_active_power到Sub_metering_2的每一列现在都是预期的`float64`类型。
- en: 18.2.3 Data resampling
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.2.3 数据重采样
- en: The next step is to check if data sampled every minute is appropriate for modeling.
    It is possible that data sampled every minute is too noisy to build a performant
    predictive model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是检查每分钟采样的数据是否适合建模。有可能每分钟采样的数据太嘈杂，无法构建性能良好的预测模型。
- en: To check this, we’ll simply plot our target to see what it looks like. The resulting
    plot is shown in figure 18.3.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查这一点，我们将简单地绘制我们的目标值，看看它看起来像什么。结果图如图18.3所示。
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../../OEBPS/Images/18-03.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/18-03.png)'
- en: Figure 18.3 The first 24 hours of recorded global active power sampled every
    minute. You can see that the data is quite noisy.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.3 记录的全球活动功率的第一天24小时，每分钟采样一次。你可以看到数据相当嘈杂。
- en: Figure 18.3 shows that the data is very noisy, with large oscillations or flat
    sequences occurring at every minute. This kind of pattern is difficult to forecast
    using a deep learning model, since it seems to move at random. Also, we could
    question the need to forecast electricity consumption by the minute, as changes
    to the grid cannot occur in such short amounts of time.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.3显示数据非常嘈杂，每分钟都会出现大的波动或平坦序列。这种模式使用深度学习模型进行预测是困难的，因为它似乎是在随机移动。此外，我们也可以质疑是否需要每分钟预测电力消耗，因为电网的变化不可能在如此短的时间内发生。
- en: Thus, we definitely need to resample our data. In this case, we’ll resample
    by the hour. That way, we’ll hopefully smooth out the data and uncover a pattern
    that may be easier to predict with a machine learning model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们肯定需要重采样我们的数据。在这种情况下，我们将按小时重采样。这样，我们希望平滑数据并揭示一个可能更容易用机器学习模型预测的模式。
- en: To do this, we’ll need a `datetime` data type. We can combine the Date and Time
    columns to create a new column that holds the same information with a `datetime`
    data type.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们需要一个`datetime`数据类型。我们可以将日期和时间列合并，创建一个新的列，该列包含与`datetime`数据类型相同的信息。
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ This step will take a long time. Do not worry if it seems like your code is
    hanging.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这个步骤将花费很长时间。如果您的代码看起来像是挂起了，请不要担心。
- en: Now we can resample our data. In this case, we’ll take an hourly sum of each
    variable. That way we’ll know the total electrical power consumed by the household
    every hour.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以重采样我们的数据。在这种情况下，我们将对每个变量进行每小时的总和。这样我们就会知道家庭每小时消耗的总电力。
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Remember that our data started on December 16, 2006, at 5:24 p.m. and ended
    on November 26, 2010, at 9:02 p.m. With the new resampling, we now have a sum
    of each column per hour, which means that we have data that starts on December
    16, 2006, at 5 p.m. and ends on November 26, 2010, at 9 p.m. However, the first
    and last rows of data do not have a full 60 minutes in their sum. The first row
    computed the sum from 5:24 p.m. to 5:59 p.m., which is 35 minutes. The last row
    computed the sum from 9:00 p.m. to 9:02 p.m., which is only 2 minutes. Therefore,
    we’ll remove the first and last rows of data so that we are working only with
    sums over full hours.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们的数据从2006年12月16日下午5:24开始，到2010年11月26日下午9:02结束。通过新的重采样，我们现在每小时每列都有一个总和，这意味着我们的数据从2006年12月16日下午5点开始，到2010年11月26日下午9点结束。然而，数据的第一行和最后一行的总和没有完整的60分钟。第一行计算了从下午5:24到下午5:59的总和，这是35分钟。最后一行计算了从晚上9:00到晚上9:02的总和，只有2分钟。因此，我们将删除第一行和最后一行数据，这样我们只处理整个小时的总和。
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Finally, this process has changed the index. I personally prefer to have the
    index as integers and the dates as a column, so we’ll simply reset the index of
    our `DataFrame`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这个过程已经改变了索引。我个人更喜欢将索引作为整数，将日期作为列，所以我们将简单地重置我们的`DataFrame`的索引。
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can optionally check the shape of our data using `hourly_df.shape`, and we
    would see that we now have 34,949 rows of data. This is a drastic drop from the
    original two million rows. Nevertheless, a dataset of this size is definitely
    suitable for deep learning methods.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以可选地检查我们数据的形状，使用`hourly_df.shape`，我们会看到我们现在有34,949行数据。这比原始的两百万行有大幅减少。尽管如此，这样一个规模的数据集绝对适合深度学习方法。
- en: 'Let’s plot our target again to see if resampling our data generated a discernible
    pattern that can be forecast. Here we’ll plot the first 15 days of global active
    power sampled hourly:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次绘制我们的目标，看看重采样我们的数据是否生成了一个可辨别的模式，可以进行预测。这里我们将绘制全球有功功率的前15天每小时采样的数据：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can see in figure 18.4, we now have a smoother pattern of global active
    power. Furthermore, we can discern daily seasonality, although it is not as apparent
    as previous examples in this book.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在图18.4中看到的，我们现在有一个更平滑的全局有功功率模式。此外，我们可以辨别出日季节性，尽管它不如本书中之前的例子那么明显。
- en: '![](../../OEBPS/Images/18-04.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/18-04.png)'
- en: Figure 18.4 Total global active power sampled every hour. We now have a smoother
    pattern with daily seasonality. This is ready to be forecast with a deep learning
    model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.4 每小时采样的总全球有功功率。我们现在有一个更平滑的模式，带有日季节性。这可以使用深度学习模型进行预测。
- en: With the data wrangling done, we can save our dataset as a CSV file so we have
    a clean version of our data. This will be our starting file for the next section.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 数据整理完成后，我们可以将我们的数据集保存为CSV文件，这样我们就有一个干净的数据版本。这将是下一节开始的文件。
- en: '[PRE13]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 18.3 Feature engineering
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.3 特征工程
- en: At this point, we have a clean dataset with no missing values and a smoothed
    pattern that will be easier to predict using deep learning techniques. Whether
    you followed along with the last section or not, you can read a clean version
    of the data and start working on the feature engineering.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们有一个干净的数据集，没有缺失值，并且有一个平滑的模式，这将更容易使用深度学习技术进行预测。无论您是否跟随着上一节，您都可以阅读数据的干净版本，并开始进行特征工程。
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 18.3.1 Removing unnecessary columns
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.3.1 移除不必要的列
- en: The first step in feature engineering is to display the basic statistics for
    each column. This is especially useful for detecting whether there are any variables
    that do not vary greatly. Such variables should be removed, since if they are
    almost constant over time, they are not predictive of our target.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程的第一步是显示每个列的基本统计信息。这特别有助于检测是否有任何变量变化不大。这样的变量应该被移除，因为如果它们在时间上几乎保持不变，它们就不能预测我们的目标。
- en: 'We can get a description of each column using the `describe` method from `pandas`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用来自`pandas`的`describe`方法获取每列的描述：
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see in figure 18.5, Sub_metering_1 is likely not a good predictor
    for our target, since its constant value will not explain the variations in global
    active power. We can safely remove this column and keep the rest.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如图18.5所示，Sub_metering_1可能不是我们目标的良好预测因子，因为其恒定值无法解释全局活动功率的变化。我们可以安全地移除此列并保留其余部分。
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../../OEBPS/Images/18-05.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/18-05.png)'
- en: Figure 18.5 A description of each column in our dataset. You’ll notice that
    Sub_metering_1 has a value of 0 for 75% of the time. Because this variable doesn’t
    vary much over time, it can be removed from the set of features.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.5 数据集中每列的描述。你会注意到Sub_metering_1有75%的时间值为0。因为这个变量随时间变化不大，所以可以从特征集中移除。
- en: 18.3.2 Identifying the seasonal period
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.3.2 识别季节性周期
- en: With our target being global active power in a household, it is likely that
    we’ll have some seasonality. We can expect that at night, less electrical power
    will be used. Similarly, there may be a peak in consumption when people come back
    from work during the week. Thus, it is reasonable to assume that there will be
    some seasonality in our target.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的目标是家庭的全局活动功率，我们可能会有些季节性。我们可以预期在夜间，将使用较少的电力。同样，在工作日人们下班回家时可能会出现消费高峰。因此，假设我们的目标将存在一些季节性是合理的。
- en: We can plot our target to see if we can visually detect the period.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以绘制目标数据以查看是否可以直观地检测到周期。
- en: '[PRE17]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In figure 18.6 you can see that our target has some cyclical behavior, but the
    seasonal period is hard to determine from the graph. While our hypothesis about
    daily seasonality makes sense, we need to make sure that it is present in our
    data. One way to do it is with a Fourier transform.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在图18.6中，你可以看到我们的目标有一些周期性行为，但从图中很难确定季节性周期。虽然我们对每日季节性的假设是有道理的，但我们需要确保它在我们的数据中存在。一种方法是通过傅里叶变换来实现。
- en: '![](../../OEBPS/Images/18-06.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/18-06.png)'
- en: Figure 18.6 Total global active power in the first 15 days. While there is clear
    cyclical behavior, the seasonal period is hard to determine from the graph only.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.6 前15天的总全局活动功率。虽然存在明显的周期性行为，但仅从图中很难确定季节性周期。
- en: Without diving into the details, a Fourier transform basically allows us to
    visualize the frequency and amplitude of a signal. Hence, we can treat our time
    series as a signal, apply a Fourier transform, and find the frequencies with large
    amplitudes. Those frequencies will determine the seasonal period. The great advantage
    of this method is that it is independent of the seasonal period. It can identify
    yearly, weekly, and daily seasonality, or any specific period we wish to test.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 不深入细节，傅里叶变换基本上允许我们可视化信号的频率和振幅。因此，我们可以将我们的时间序列视为一个信号，应用傅里叶变换，并找到振幅大的频率。这些频率将决定季节性周期。这种方法的一个巨大优势是它独立于季节性周期。它可以识别年度、周度和日季节性，或我们希望测试的任何特定周期。
- en: 'Note For more information about Fourier transforms, I suggest reading Lakshay
    Akula’s “Analyzing seasonality with Fourier transforms using Python & SciPy” blog
    post, which does a great job of gently introducing Fourier transforms for analyzing
    seasonality: [http://mng.bz/7y2Q](http://mng.bz/7y2Q).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：有关傅里叶变换的更多信息，我建议阅读Lakshay Akula的“使用Python & SciPy分析季节性”博客文章，该文章很好地介绍了傅里叶变换以分析季节性：[http://mng.bz/7y2Q](http://mng.bz/7y2Q)。
- en: For our situation, let’s test for weekly and daily seasonality.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的情况，让我们测试每周和每日的季节性。
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Apply a Fourier transform on our target.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 对我们的目标应用傅里叶变换。
- en: ❷ Get the number of frequencies from the Fourier transform.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取傅里叶变换中的频率数。
- en: ❸ Find out how many hours are in the dataset.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 查找数据集中的小时数。
- en: ❹ Get the number of hours in a week.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 获取一周中的小时数。
- en: ❺ Get the number of weeks in the dataset.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 获取数据集中的周数。
- en: ❻ Get the frequency of a week in the dataset.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 获取数据集中一周的频率。
- en: ❼ Plot the frequency and amplitude.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 绘制频率和振幅。
- en: ❽ Label the weekly and daily frequencies.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 标记每周和每日频率。
- en: In figure 18.7 you can see the amplitude of the weekly and daily frequencies.
    The weekly frequency does not show any visible peak, meaning that its amplitude
    is very small. Therefore, there is no weekly seasonality.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在图18.7中，你可以看到每周和每日频率的振幅。每周频率没有显示出任何明显的峰值，这意味着其振幅非常小。因此，不存在每周的季节性。
- en: '![](../../OEBPS/Images/18-07.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/18-07.png)'
- en: Figure 18.7 Amplitude of the weekly and daily seasonality in our target. You
    can see that the amplitude of the weekly seasonality is close to 0, while there
    is a visible peak for the daily seasonality. Therefore, we indeed have daily seasonality
    for our target.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.7展示了我们目标中每周和每日季节性的振幅。你可以看到，每周季节性的振幅接近0，而每日季节性有一个明显的峰值。因此，我们的目标确实存在日季节性。
- en: Looking at the daily frequency, however, you’ll notice a clear peak in the figure.
    This tells us that we indeed have daily seasonality in our data. Thus, we will
    encode our timestamp using a sine and cosine transformation to express the time
    while keeping its daily seasonal information. We did the same thing in chapter
    12 when preparing our data for modeling with deep learning.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从每日频率来看，你会在图中注意到一个明显的峰值。这告诉我们，我们的数据确实存在日季节性。因此，我们将使用正弦和余弦变换来编码时间戳，以表达时间的同时保留其日季节性信息。我们在第12章准备数据用于深度学习建模时也做了同样的事情。
- en: '[PRE19]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Our feature engineering is complete, and the data is ready to be scaled and
    split into training, validation, and test sets.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的特征工程已完成，数据已准备好进行缩放并分割为训练集、验证集和测试集。
- en: 18.3.3 Splitting and scaling the data
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.3.3 数据的分割和缩放
- en: The final step is to split the dataset into training, validation, and test sets,
    and to scale the data. Note that we’ll first split the data, so that we scale
    it using only the information from the training set, thus avoiding information
    leakage. Scaling the data will decrease training time and improve the performance
    of our models.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是将数据集分割为训练集、验证集和测试集，并对数据进行缩放。请注意，我们首先分割数据，因此我们将使用仅来自训练集的信息来缩放它，从而避免信息泄露。缩放数据将减少训练时间并提高我们模型的表现。
- en: We’ll split the data 70:20:10 for the training, validation, and test sets respectively.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分别将数据分割为70%、20%和10%用于训练集、验证集和测试集。
- en: '[PRE20]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next, we’ll fit the scaler to the training set only, and scale each individual
    set.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将仅对训练集进行缩放器的拟合，并对每个单独的集合进行缩放。
- en: '[PRE21]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We can now save each set to be used later for modeling.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将每个集合保存下来，以便稍后用于建模。
- en: '[PRE22]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We are now ready to move on to the modeling step.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以继续进行建模步骤。
- en: 18.4 Preparing for modeling with deep learning
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.4 使用深度学习进行建模的准备
- en: In the last section, we produced the three sets of data required for training
    deep learning models. Recall that the objective of this project is to predict
    the global active power consumption in the next 24 hours. This means that we must
    build a univariate multi-step model, since we are forecasting only one target
    24 timesteps into the future.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们生成了训练深度学习模型所需的三个数据集。回想一下，这个项目的目标是预测未来24小时的全球活跃电力消耗。这意味着我们必须构建一个单变量多步模型，因为我们只预测一个目标24个时间步的未来。
- en: We will build two baselines, a linear model, a deep neural network model, a
    long short-term memory (LSTM) model, a convolutional neural network (CNN), a combination
    of CNN and LSTM, and finally an autoregressive LSTM. In the end, we will use the
    mean absolute error (MAE) to determine which model is the best. The one that achieves
    the lowest MAE on the test set will be the top-performing model.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建两个基线，一个线性模型，一个深度神经网络模型，一个长短期记忆（LSTM）模型，一个卷积神经网络（CNN），一个CNN和LSTM的组合，最后是一个自回归LSTM。最后，我们将使用平均绝对误差（MAE）来确定哪个模型是最好的。在测试集上实现最低MAE的模型将是性能最好的模型。
- en: Note that we’ll use the MAE as our evaluation metric and the mean squared error
    (MSE) as the loss function, just as we have since chapter 13.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将使用平均绝对误差（MAE）作为评估指标，以及均方误差（MSE）作为损失函数，正如我们从第13章开始所做的那样。
- en: 18.4.1 Initial setup
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.4.1 初始设置
- en: Before moving on to modeling, we first need to import the required libraries,
    as well as define our `DataWindow` class and a function to train our models.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行建模之前，我们首先需要导入所需的库，以及定义我们的`DataWindow`类和一个用于训练模型的函数。
- en: We’ll start off by importing the necessary Python libraries for modeling.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先导入建模所需的Python库。
- en: '[PRE23]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Make sure you have TensorFlow 2.6 installed, as this is the latest version at
    the time of writing. You can check the version of TensorFlow using `print(tf.__version__)`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你已经安装了TensorFlow 2.6，因为这是撰写时的最新版本。你可以使用`print(tf.__version__)`来检查TensorFlow的版本。
- en: Optionally, you can set parameters for the plots. In this case, I prefer to
    specify a size and remove the grid on the axes.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，你可以为图表设置参数。在这种情况下，我更喜欢指定大小并移除坐标轴上的网格。
- en: '[PRE24]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Then you can set a random seed. This ensures constant results when training
    models. Recall that the initialization of deep learning models is random, so training
    the same model twice in a row might result in slightly different performance.
    Thus, to ensure reproducibility, we set a random seed.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以设置一个随机种子。这确保了在训练模型时结果的一致性。请记住，深度学习模型的初始化是随机的，因此连续两次训练相同的模型可能会得到略微不同的性能。因此，为了确保可重复性，我们设置了随机种子。
- en: '[PRE25]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Next, we need to read the training set, validation set, and test set so they
    are ready for modeling.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要读取训练集、验证集和测试集，以便它们为建模做好准备。
- en: '[PRE26]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Finally, we’ll build a dictionary to store the column names and their corresponding
    indexes. This will be useful later on for building the baseline models and creating
    windows of data.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将构建一个字典来存储列名及其对应的索引。这将在构建基线模型和创建数据窗口时非常有用。
- en: '[PRE27]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We’ll now move on to defining the `DataWindow` class.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将继续定义`DataWindow`类。
- en: 18.4.2 Defining the DataWindow class
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.4.2 定义DataWindow类
- en: The `DataWindow` class allows us to quickly create windows of data for training
    deep learning models. Each window of data contains a set of inputs and a set of
    labels. The model is then trained to produce predictions as close as possible
    to the labels using the inputs.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataWindow`类允许我们快速创建用于训练深度学习模型的数据窗口。每个数据窗口包含一组输入和一组标签。然后模型被训练以使用输入产生尽可能接近标签的预测。'
- en: An entire section of chapter 13 was dedicated to implementing the `DataWindow`
    class step by step, and we have been using it ever since, so we will go straight
    to its implementation. The only change here will be the name of the default column
    to plot when we visualize the predictions against the labels.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 第13章的一个整个部分都是专门用于逐步实现`DataWindow`类，自从那时起我们就一直在使用它，所以我们将直接进入其实现。这里唯一的改变将是当我们将预测与标签可视化时，默认要绘制的列名。
- en: Listing 18.1 Implementation of class to create windows of data
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 列表18.1 创建数据窗口的类实现
- en: '[PRE28]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Set the default name of our target to be the global active power.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将我们的目标默认名称设置为全局有功功率。
- en: With the `DataWindow` class defined, we only need a function to compile and
    train the different models we’ll develop.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了`DataWindow`类后，我们只需要一个函数来编译和训练我们将开发的不同的模型。
- en: 18.4.3 Utility function to train our models
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.4.3 训练我们模型的实用函数
- en: Our final step before launching our experiments is to build a function that
    automates the training process. This is the `compile_and_fit` function that we
    have been using since chapter 13.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动我们的实验之前，我们的最后一步是构建一个自动化训练过程的函数。这就是我们从第13章开始使用的`compile_and_fit`函数。
- en: Recall that this function takes in a model and a window of data. Then it implements
    early stopping, meaning that the model will stop training if the validation loss
    does not change for three consecutive epochs. This is also the function in which
    we specify the loss function to be the MSE and the evaluation metric to be the
    MAE.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这个函数接受一个模型和一个数据窗口。然后它实现了早停机制，意味着如果验证损失在连续三个epoch内没有变化，模型将停止训练。这也是我们指定损失函数为均方误差（MSE）和评估指标为平均绝对误差（MAE）的函数。
- en: '[PRE29]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: At this point, we have everything we need to start developing models to forecast
    the next 24 hours of global active power.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经拥有了开始开发预测未来24小时全球有功功率的模型所需的一切。
- en: 18.5 Modeling with deep learning
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.5 使用深度学习建模
- en: The training, validation, and test sets are ready, as well as the `DataWindow`
    class and the function that will train our models. Everything is set for us to
    start building deep learning models.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集、验证集和测试集都已准备就绪，以及`DataWindow`类和训练我们模型的函数。一切准备就绪，我们可以开始构建深度学习模型。
- en: 'We’ll first implement two baselines, and then we’ll train models with increasing
    complexity: a linear model, a deep neural network, an LSTM, a CNN, a CNN and LSTM
    model, and an autoregressive LSTM. Once all the models are trained, we’ll select
    the best model by comparing the MAE on the test set. The model with the lowest
    MAE will be the one that we recommend.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将实现两个基线模型，然后我们将训练具有递增复杂性的模型：一个线性模型、一个深度神经网络、一个LSTM、一个CNN、一个CNN和LSTM模型，以及一个自回归LSTM模型。一旦所有模型都训练完毕，我们将通过比较测试集上的MAE来选择最佳模型。MAE最低的模型将是我们的推荐模型。
- en: 18.5.1 Baseline models
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.5.1 基线模型
- en: 'Every forecasting project must start with a baseline model. Baselines serve
    as a benchmark for our more sophisticated models, as they can only be better in
    comparison to a certain benchmark. Building baseline models also allows us to
    assess whether the added complexity of a model really generates a significant
    benefit. It is possible that a complex model does not perform much better than
    a baseline, in which case implementing a complex model is hard to justify. In
    this case, we’ll build two baseline models: one that repeats the last known value
    and another that repeats the last 24 hours of data.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 每个预测项目都必须从一个基线模型开始。基线模型作为我们更复杂模型的基准，因为它们只能与某个基准相比更好。构建基线模型还允许我们评估模型增加的复杂性是否真正产生了显著的收益。可能存在复杂模型的表现并不比基线模型好多少的情况，在这种情况下，实施复杂模型是难以证明其合理性的。在这种情况下，我们将构建两个基线模型：一个重复最后已知值，另一个重复最后24小时的数据。
- en: We’ll start by creating the window of data that will be used. Recall that the
    objective is to forecast the next 24 hours of global active power. Thus, the length
    of our label sequence is 24 timesteps, and the shift will also be 24 timesteps.
    We’ll also use an input length of 24.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建将使用的数据窗口。记住，目标是预测未来24小时的全球活跃功率。因此，我们的标签序列长度为24个时间步，位移也将是24个时间步。我们还将使用24个时间步的输入长度。
- en: '[PRE30]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Next, we’ll implement a class that will repeat the last known value of the input
    sequence as a prediction for the next 24 hours.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现一个类，该类将重复输入序列的最后已知值作为对未来24小时的预测。
- en: '[PRE31]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We can now generate predictions using this baseline and store its performance
    in a dictionary. This dictionary will store the performance of each model so that
    we can compare them at the end. Note that we will not display the MAE of each
    model as we build them. We will compare the evaluation metrics once all the models
    are trained.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用这个基线生成预测，并将其性能存储在字典中。这个字典将存储每个模型的性能，以便我们可以在最后进行比较。请注意，我们不会在构建模型时显示每个模型的MAE。我们将在所有模型训练完毕后比较评估指标。
- en: '[PRE32]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We can visualize the predictions using the `plot` method of the `DataWindow`
    class, as shown in figure 18.8\. It will display three plots in the figure, as
    specified in the `DataWindow` class.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`DataWindow`类的`plot`方法来可视化预测结果，如图18.8所示。图中将显示三个图表，如`DataWindow`类中指定的。
- en: '[PRE33]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '![](../../OEBPS/Images/18-08.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/18-08.png)'
- en: Figure 18.8 Predictions from the baseline model, which simply repeats the last
    known input value
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.8 基线模型的预测，简单地重复最后一个已知的输入值
- en: In figure 18.8 we have a working baseline—the forecasts correspond to a flat
    line with the same value as the last input. You may get a slightly different plot,
    since the cached sample batch used to create the plots may not be the same. However,
    the model’s metrics will be identical to what is shown here, as long as the random
    seeds are equal.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在图18.8中，我们有一个工作基线——预测与最后一个输入值相同的水平线。你可能得到一个略有不同的图表，因为用于创建图表的缓存样本批次可能不同。然而，只要随机种子相同，模型的指标将与这里显示的相同。
- en: Next, let’s implement a baseline model that repeats the input sequence. Remember
    that we identified daily seasonality in our target, so this is equivalent to forecasting
    the last known season.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们实现一个重复输入序列的基线模型。记住，我们在目标中识别出了日季节性，因此这相当于预测最后已知的季节。
- en: '[PRE34]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Once it’s defined, we can generate predictions and store the baseline’s performance
    for comparison. We can also visualize the generated predictions, as shown in figure
    18.9.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了基线，我们就可以生成预测并存储基线的性能以进行比较。我们还可以可视化生成的预测，如图18.9所示。
- en: '[PRE35]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](../../OEBPS/Images/18-09.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/18-09.png)'
- en: Figure 18.9 Predicting the last season as a baseline
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.9 作为基线预测的最后季节
- en: In figure 18.9 you’ll see that the predictions are equal to the input sequence,
    which is the expected behavior for this baseline model. Feel free to print out
    the MAE for each model as you build them. I’ll display them at the end of the
    chapter in a bar chart to determine which model should be selected.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在图18.9中，你会看到预测值等于输入序列，这是该基线模型预期的行为。在构建模型时，你可以随意打印出每个模型的MAE。我将在本章末尾以条形图的形式展示它们，以确定哪个模型应该被选择。
- en: With the baseline models in place, we can move on to the slightly more complex
    linear model.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在基线模型就绪后，我们可以继续到稍微复杂一点的线性模型。
- en: 18.5.2 Linear model
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.5.2 线性模型
- en: One of the simplest models we can build is a linear model. This model consists
    of only an input layer and an output layer. Thus, only a sequence of weights is
    computed to generate predictions that are as close as possible to the labels.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以构建的最简单的模型之一是线性模型。这个模型仅由一个输入层和一个输出层组成。因此，仅计算一系列权重以生成尽可能接近标签的预测。
- en: In this case, we’ll build a model with one `Dense` output layer that has only
    one neuron, since we are predicting only one target. We’ll then train the model
    and store its performance.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将构建一个只有一个`Dense`输出层的模型，该层只有一个神经元，因为我们只预测一个目标。然后我们将训练模型并存储其性能。
- en: '[PRE36]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: As always, we can visualize the predictions using the `plot` method, as shown
    in figure 18.10.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们可以使用`plot`方法可视化预测结果，如图18.10所示。
- en: '[PRE37]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](../../OEBPS/Images/18-10.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/18-10.png)'
- en: Figure 18.10 Predictions generated from a linear model
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.10 线性模型生成的预测
- en: Now let’s add hidden layers and implement a deep neural network.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们添加隐藏层并实现一个深度神经网络。
- en: 18.5.3 Deep neural network
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.5.3 深度神经网络
- en: The previous linear model did not have any hidden layers; it was simply an input
    layer and an output layer. Now we’ll add hidden layers, which will help us model
    nonlinear relationships in the data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的线性模型没有任何隐藏层；它只是一个输入层和一个输出层。现在我们将添加隐藏层，这将帮助我们模拟数据中的非线性关系。
- en: Here we’ll stack two `Dense` layers with 64 neurons and use ReLU as the activation
    function. Then we’ll train the model and store its performance for comparison.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将堆叠两个具有64个神经元的`Dense`层，并使用ReLU作为激活函数。然后我们将训练模型并存储其性能以供比较。
- en: '[PRE38]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: You can optionally visualize the predictions with `multi_window.plot(dense)`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择使用`multi_window.plot(dense)`可视化预测结果。
- en: The next model we’ll implement is the long short-term memory model.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要实现的是长短期记忆模型。
- en: 18.5.4 Long short-term memory (LSTM) model
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.5.4 长短期记忆（LSTM）模型
- en: The main advantage of the long short-term memory (LSTM) model is that it keeps
    information from the past in memory. This makes it especially suitable for treating
    sequences of data, like time series. It allows us to combine information from
    the present and the past to produce a prediction.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）模型的主要优势是它能将过去的信息保存在记忆中。这使得它特别适合处理数据序列，如时间序列。它允许我们结合现在和过去的信息来做出预测。
- en: We’ll feed the input sequence through an `LSTM` layer before sending it to the
    output layer, which remains a `Dense` layer with one neuron. We’ll then train
    the model and store its performance in the dictionary for comparison at the end.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在将输入序列发送到输出层之前，通过一个`LSTM`层，输出层仍然是一个具有一个神经元的`Dense`层。然后我们将训练模型并将性能存储在字典中以供比较。
- en: '[PRE39]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We can visualize the predictions from the LSTM—they are shown in figure 18.11.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以可视化LSTM的预测结果——它们在图18.11中展示。
- en: '[PRE40]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![](../../OEBPS/Images/18-11.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/18-11.png)'
- en: Figure 18.11 Predictions generated from the LSTM model
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.11 LSTM模型生成的预测
- en: Now let’s implement a convolutional neural network.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们实现一个卷积神经网络。
- en: 18.5.5 Convolutional neural network (CNN)
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.5.5 卷积神经网络（CNN）
- en: A convolutional neural network (CNN) uses the convolution function to reduce
    the feature space. This effectively filters our time series and performs feature
    selection. Furthermore, a CNN is faster to train than an LSTM since the operations
    are parallelized, whereas the LSTM must treat one element of the sequence at a
    time.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）使用卷积函数来减少特征空间。这有效地过滤了时间序列并执行了特征选择。此外，由于操作是并行化的，CNN的训练速度比LSTM快，而LSTM必须一次处理序列中的一个元素。
- en: Because the convolution operation reduces the feature space, we must provide
    a slightly longer input sequence to make sure that the output sequence contains
    24 timesteps. How much longer it needs to be depends on the length of the kernel
    that performs the convolution operation. In this case, we’ll use a kernel length
    of 3\. This is an arbitrary choice, so feel free to experiment with different
    values, although your results might differ from what is shown here. Given that
    we need 24 labels, we can calculate the input sequence using equation 18.1.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 由于卷积操作会减少特征空间，我们必须提供一个稍微长一点的输入序列，以确保输出序列包含24个时间步长。需要多长取决于执行卷积操作的核的长度。在这种情况下，我们将使用一个长度为3的核。这是一个任意的选择，所以您可以自由地尝试不同的值，尽管您的结果可能与这里展示的不同。鉴于我们需要24个标签，我们可以使用方程式18.1来计算输入序列。
- en: input length = label length + kernel length – 1
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 输入长度 = 标签长度 + 核长度 - 1
- en: Equation 18.1
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式18.1
- en: This forces us to define a window of data specifically for the CNN model. Note
    that since we are defining a new window of data, the sample batch used for plotting
    will differ from the one used so far.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这迫使我们为CNN模型定义一个特定的数据窗口。请注意，由于我们正在定义一个新的数据窗口，用于绘图的样本批次将与之前使用的不同。
- en: We now have all the necessary information to define a window of data for the
    CNN model.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了为CNN模型定义数据窗口所需的所有必要信息。
- en: '[PRE41]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Next, we’ll send the input through a `Conv1D` layer, which filters the input
    sequence. Then it is fed to a `Dense` layer with 32 neurons for learning before
    going to the output layer. As always, we’ll train the model and store its performance
    for comparison.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将输入通过一个`Conv1D`层，该层过滤输入序列。然后它被送入一个有32个神经元的`Dense`层进行学习，然后再进入输出层。像往常一样，我们将训练模型并存储其性能以供比较。
- en: '[PRE42]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: We can now visualize the predictions.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以可视化预测结果了。
- en: '[PRE43]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: You will notice in figure 18.12 that the input sequence differs from our previous
    methods because working with a CNN involves windowing the data again to account
    for the convolution kernel length. The training, validation, and test sets remain
    unchanged, so it is still valid to compare all the models’ performance.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 你会在图18.12中注意到，输入序列与我们的先前方法不同，因为使用CNN涉及再次对数据进行窗口化以考虑卷积核长度。训练、验证和测试集保持不变，因此仍然可以比较所有模型的性能。
- en: '![](../../OEBPS/Images/18-12.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/18-12.png)'
- en: Figure 18.12 Predictions generated by the CNN model
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.12 CNN模型生成的预测
- en: Now let’s combine the CNN model with the LSTM model.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将CNN模型与LSTM模型结合起来。
- en: 18.5.6 Combining a CNN with an LSTM
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.5.6 将CNN与LSTM结合
- en: We know that LSTM is good at treating sequences of data, while CNN can filter
    a sequence of data. Therefore, it is interesting to test whether filtering a sequence
    before feeding it to an LSTM can result in a better-performing model.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道LSTM擅长处理数据序列，而CNN可以过滤数据序列。因此，测试在将数据序列过滤后再输入到LSTM中是否会产生性能更好的模型是有趣的。
- en: We’ll feed the input sequence to a `Conv1D` layer, but use an LSTM layer for
    learning this time. Then we’ll send the information to the output layer. Again,
    we’ll train the model and store its performance.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将输入序列送入一个`Conv1D`层，但这次使用LSTM层进行学习。然后我们将信息发送到输出层。再次，我们将训练模型并存储其性能。
- en: '[PRE44]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The predictions are visualized in figure 18.13.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 预测结果如图18.13所示。
- en: '[PRE45]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '![](../../OEBPS/Images/18-13.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/18-13.png)'
- en: Figure 18.13 Predictions from a CNN combined with an LSTM model
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.13 CNN与LSTM模型结合的预测
- en: Finally, let’s implement an autoregressive LSTM model.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们实现一个自回归LSTM模型。
- en: 18.5.7 The autoregressive LSTM model
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.5.7 自回归LSTM模型
- en: The final model that we’ll implement is an autoregressive LSTM (ARLSTM) model.
    Instead of generating the entire output sequence in a single shot, the autoregressive
    model will generate one prediction at a time and use that prediction as an input
    to generate the next one. This kind of architecture is present in state-of-the-art
    forecasting models, but it comes with a caveat. If the model generates a very
    bad first prediction, this mistake will be carried on to the next predictions,
    which will magnify the errors. Nevertheless, it is worth testing this model to
    see if it works well in our situation.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现的最终模型是一个自回归LSTM（ARLSTM）模型。与一次生成整个输出序列不同，自回归模型将一次生成一个预测，并将该预测作为输入生成下一个预测。这种架构存在于最先进的预测模型中，但有一个缺点。如果模型生成一个非常糟糕的第一个预测，这个错误将会传递到下一个预测中，从而放大错误。尽管如此，测试这个模型以查看它在我们的情况下是否表现良好是值得的。
- en: The first step is defining the class that implements the ARLSTM model. This
    is the same class that we used in chapter 17.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是定义实现ARLSTM模型的类。这与我们在第17章中使用的类相同。
- en: Listing 18.2 Class to implement an ARLSTM model
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 列表18.2 实现ARLSTM模型的类
- en: '[PRE46]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We can then use this class to initialize our model. We’ll train the model on
    the `multi_ window` and store its performance for comparison.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用这个类来初始化我们的模型。我们将在`multi_window`上训练模型并存储其性能以供比较。
- en: '[PRE47]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We can then visualize the predictions of the autoregressive LSTM model, as shown
    in figure 18.14.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以可视化自回归LSTM模型的预测，如图18.14所示。
- en: '[PRE48]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![](../../OEBPS/Images/18-14.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/18-14.png)'
- en: Figure 18.14 Predictions from the ARLSTM model
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.14 ARLSTM模型的预测
- en: Now that we have built a wide variety of models, let’s select the best one based
    on its MAE on the test set.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了各种模型，让我们根据测试集上的MAE选择最佳模型。
- en: 18.5.8 Selecting the best model
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 18.5.8 选择最佳模型
- en: We have built many models for this project, from a linear model to an ARLSTM
    model. Now let’s visualize the MAE of each model to determine the champion.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为这个项目构建了许多模型，从线性模型到ARLSTM模型。现在让我们可视化每个模型的MAE，以确定冠军。
- en: We’ll plot the MAE on both the validation and test sets. The result is shown
    in figure 18.15.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在验证集和测试集上绘制MAE图。结果如图18.15所示。
- en: '[PRE49]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![](../../OEBPS/Images/18-15.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/18-15.png)'
- en: Figure 18.15 Comparing the MAE of all models tested. The ARLSTM model achieved
    the lowest MAE on the test set.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.15 比较所有测试模型的MAE。ARLSTM模型在测试集上实现了最低的MAE。
- en: Figure 18.15 shows that all the models performed much better than the baselines.
    Furthermore, our champion is the ARLSTM model, since it achieved a MAE of 0.074
    on the test set, which is the lowest MAE of all. Thus, we would recommend using
    this model to forecast the global active power over the next 24 hours.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.15显示，所有模型的表现都远远优于基线模型。此外，我们的冠军是ARLSTM模型，因为它在测试集上实现了0.074的MAE，这是所有模型中最低的。因此，我们建议使用这个模型来预测未来24小时的全球有功功率。
- en: 18.6 Next steps
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.6 下一步
- en: Congratulations on completing this capstone project! I hope that you were successful
    in completing it on your own and that you feel confident in your knowledge of
    forecasting time series using deep learning models.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜您完成这个毕业设计项目！我希望您能够独立完成它，并且对使用深度学习模型进行时间序列预测的知识感到自信。
- en: I highly encourage you to make this project your own. You can turn this project
    into a multivariate forecasting problem by forecasting more than one target. You
    could also change the forecast horizon. In short, make changes and play around
    with the models and the data and see what you can achieve on your own.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议您将这个项目变成您自己的。您可以通过预测多个目标将这个项目转变为一个多元预测问题。您还可以更改预测范围。简而言之，进行更改并尝试不同的模型和数据，看看您能独立完成什么。
- en: In the next chapter, we’ll start the final part of this book, where we’ll automate
    the forecasting process. There are many libraries that can generate accurate predictions
    with minimal steps, and they are often used in the industry, making this an essential
    tool for time series forecasting. We’ll look at a widely used library called Prophet.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始这本书的最后一部分，我们将自动化预测过程。有许多库可以以最少的步骤生成准确的预测，并且它们在工业界中经常被使用，这使得它们成为时间序列预测的必备工具。我们将探讨一个广泛使用的库，称为Prophet。
