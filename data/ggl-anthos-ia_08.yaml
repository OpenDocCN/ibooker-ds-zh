- en: 8 Working at the edge and the telco world
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 在边缘和电信世界工作
- en: Giovanni Galloro
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Giovanni Galloro
- en: This chapter covers
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Evolution of telco network functions toward cloud native network functions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电信网络功能向云原生网络功能的演变
- en: Edge application use cases
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Edge应用场景
- en: Anthos-specific capabilities for supporting telco and edge workloads
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持电信和边缘工作负载的Anthos特定功能
- en: Google Distributed Cloud Edge
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google分布式云边缘
- en: 'This chapter is about using Anthos as an enabling platform for edge and telco
    workloads, which fall into the following two categories:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章主要介绍如何使用Anthos作为边缘和电信工作负载的启用平台，这些工作负载分为以下两类：
- en: '*Cloud native network functions* —An evolution of telecom network functions,
    either already virtualized or still deployed as physical appliances, toward containerized
    workloads to reach greater efficiency, performance, and ease of management. This
    evolution will be driven also by new 5G-related network functions.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*云原生网络功能* —电信网络功能的演变，无论是已经虚拟化还是仍以物理设备部署，向容器化工作负载发展，以实现更高的效率、性能和管理便捷性。这一演变也将由新的5G相关网络功能驱动。'
- en: '*New edge applications*—Workloads to be deployed in edge locations, near the
    end customer, to reduce latency and enable new types of applications such as autonomous
    driving, smart cities, smart video surveillance, augmented reality, virtual reality,
    and remote healthcare/surgery. Often this type of application will benefit and
    be powered by 5G networks and, in large part, will be deployed as containerized
    workloads.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*新的边缘应用*—将在边缘位置、靠近最终用户处部署的工作负载，以减少延迟并启用新的应用类型，如自动驾驶、智能城市、智能视频监控、增强现实、虚拟现实以及远程医疗/手术。通常此类应用将受益于并由5G网络提供支持，在很大程度上将以容器化工作负载的形式部署。'
- en: 8.1 Evolution of telecom applications
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 电信应用演变
- en: In this section, you will find a recap of the evolution of telecom network functions
    toward network functions virtualization and cloud native network functions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将找到电信网络功能向网络功能虚拟化和云原生网络功能演变的回顾。
- en: 8.1.1 Introduction to network functions virtualization
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 网络功能虚拟化简介
- en: Traditionally, network operators used to implement network functions on dedicated,
    proprietary hardware appliances. From the mid 2010s, the concept of network functions
    virtualization (NFV) emerged as telcos looked at virtualizing their networking
    functionality, following the same pattern that led to the virtualization of IT
    servers, to consolidate the many network equipment types into industry-standard
    high-volume servers, switches, and storage to reduce costs and increase efficiency,
    agility, and resilience.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，网络运营商通常在专用的、专有的硬件设备上实现网络功能。从2010年代中期开始，随着电信运营商考虑虚拟化其网络功能，网络功能虚拟化（NFV）的概念出现，遵循了导致IT服务器虚拟化的相同模式，将多种网络设备类型整合到行业标准的大批量服务器、交换机和存储中，以降低成本并提高效率、敏捷性和弹性。
- en: 'A trend started to transform network appliances in virtual network functions
    (VNFs): virtual machines deployed on industry standard x86 servers through a hypervisor.
    As depicted in figure 8.1, the three main working domains of an NFV architecture
    follow:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一种趋势开始将网络设备转化为虚拟网络功能（VNFs）：通过虚拟机管理程序在行业标准x86服务器上部署的虚拟机。如图8.1所示，NFV架构的三个主要工作域如下：
- en: '*Virtualized network functions (VNFs**)*—x86-compliant virtual machine versions
    of the network appliances'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*虚拟化网络功能（VNFs**）*—符合x86规范的虚拟机版本的网络设备'
- en: '*Network function virtualized infrastructure (NFVI**)*—All hardware (servers,
    storage, network gear) and software components (virtualization software) that
    build up the environment hosting the virtual network functions'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*网络功能虚拟化基础设施（NFVI**）*—构建托管虚拟网络功能的硬件（服务器、存储、网络设备）和软件组件（虚拟化软件）'
- en: '*Management and orchestration (MANO**)*—Life cycle management and orchestration
    of physical or software resources that support the infrastructure virtualization
    and the life cycle management of the virtual network functions'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*管理和编排（MANO**）*—支持基础设施虚拟化和虚拟网络功能生命周期管理及编排的物理或软件资源'
- en: '![08-01](../../OEBPS/Images/08-01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![08-01](../../OEBPS/Images/08-01.png)'
- en: Figure 8.1 High-level NFV architecture framework
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 高级NFV架构框架
- en: 8.1.2 NFV use cases
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 NFV用例
- en: 'Some virtualized network functions created as part of this initiative, with
    different success in the adoption, follow:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 作为此倡议的一部分创建的一些虚拟化网络功能，其采用程度不同，如下所示：
- en: '*vCPE* *(virtualization of home and enterprise CPE [customer premises equipment**])*—Routers
    into the operator network. With this approach, the advanced routing and network
    functions are moved from the access router, which service providers traditionally
    deployed in enterprise premises or in consumer homes, to VNFs running on industry-standard
    hardware in the provider’s own NFVI. The customer’s on-prem appliance is replaced
    with simpler hardware.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*vCPE* *(家庭和企业CPE [客户场所设备] 的虚拟化)*—将路由器引入运营商网络。采用这种方法，高级路由和网络功能从传统的服务提供商在企业场所或消费者家中部署的接入路由器转移到运行在运营商自己的NFVI（网络功能虚拟化基础设施）上行业标准硬件上的VNFs。客户的本地设备被更简单的硬件所取代。'
- en: '*vPE* *(virtualization of PE [provider edge] routers*—In this approach, the
    routers deployed in the service provider edge, which typically connect with those
    deployed on customer premises, are also virtualized.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*vPE* *(PE [提供商边缘] 路由的虚拟化)*—在此方法中，部署在服务提供商边缘的路由器，通常与部署在客户场所的路由器相连，也被虚拟化。'
- en: '*vEPC (evolved packet core) virtualization*—Virtualization of network functions
    that are part of the mobile core networks and IP multimedia subsystem: mobility
    management entity, serving gateway, and packet data network gateway.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*vEPC (演进分组核心) 虚拟化*—虚拟化移动核心网络和IP多媒体子系统的网络功能：移动管理实体、服务网关和分组数据网络网关。'
- en: '*vCDN*—Virtualization of CDNs. This use case aims to virtualize third-party
    CDN appliances that are usually deployed on-prem.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*vCDN*—CDN（内容分发网络）的虚拟化。本用例旨在虚拟化通常在本地部署的第三方CDN设备。'
- en: '*vRAN*—Virtualization of mobile base stations in radio access networks (RANs)
    was initially considered a use case for NFV, mainly because mobile base stations
    account for most of the total cost of ownership and energy consumption of mobile
    networks. This approach didn’t effectively materialize in NFV, but the aim to
    get the benefits of containerized workloads described earlier and the need for
    new radio network functions related to 5G is pushing the transformation of these
    functions to CNFs.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*vRAN*—无线接入网络（RANs）中移动基站的虚拟化最初被认为是NFV的一个用例，主要是因为移动基站占移动网络总拥有成本和能耗的大部分。这种方法在NFV中并没有有效实现，但获得前面描述的容器化工作负载的好处以及需要与5G相关的新的无线网络功能的目标正在推动这些功能的转型到CNFs（云原生网络功能）。'
- en: 8.1.3 Evolution to cloud native network functions
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.3 向云原生网络功能的演进
- en: Telco operators and network functions vendors are looking at container-based
    cloud native network functions, as an evolution of VNFs, to fully realize the
    above-mentioned NFV benefits and add the improvements carried by cloud native
    applications in terms of portability, agility, manageability, and efficiency.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 电信运营商和网络功能供应商正在考虑基于容器的云原生网络功能，作为VNFs（虚拟网络功能）的演进，以充分实现上述NFV（网络功能虚拟化）的好处，并增加云原生应用在可移植性、敏捷性、可管理性和效率方面的改进。
- en: 'Various initiatives inside Cloud Native Computing Foundation (CNCF) aim to
    support telecom operators (and network vendors) in obtaining the benefits touted
    by cloud native technologies. These are mainly led by the Telecom User Group ([https://github.com/cncf/telecom-user-group](https://github.com/cncf/telecom-user-group)),
    which produced various assets, including a white paper available in the repository
    and the following definition for cloud native network functions:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 云原生计算基金会（CNCF）内部的各项倡议旨在支持电信运营商（和网络供应商）获得云原生技术所宣称的好处。这些倡议主要由电信用户组（[https://github.com/cncf/telecom-user-group](https://github.com/cncf/telecom-user-group)）领导，该用户组产生了各种资产，包括存储库中可用的白皮书和以下关于云原生网络功能的定义：
- en: A cloud-native network function (CNF) is a cloud-native application that implements
    network functionality. A CNF consists of one or more microservices and has been
    developed using Cloud Native Principles including immutable infrastructure, declarative
    APIs, and a “repeatable deployment process.”
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 云原生网络功能（CNF）是一个实现网络功能的云原生应用。一个CNF由一个或多个微服务组成，并已使用云原生原则开发，包括不可变基础设施、声明性API和“可重复部署过程”。
- en: 8.2 New edge applications
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 新边缘应用
- en: The following paragraphs contain a description of the characteristics of new
    edge applications, which take advantage of 5G networks’ higher bandwidth and lower
    latency.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下文包含了对利用5G网络更高带宽和更低延迟的新边缘应用特性的描述。
- en: 8.2.1 5G as the enabler of new edge applications
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 5G作为新边缘应用的推动者
- en: 'Characteristics of the 5G network, including its larger contiguous spectrum,
    more advanced radio antenna technologies (Massive MIMO), better modulation schemes,
    and changes to/optimization of signaling flows between the core and RAN, provide
    network capabilities with significantly higher bandwidth and lower latency. Telco
    service providers and applications/digital services developers are looking to
    use these characteristics in applications that will have more devices connected
    and will exchange information at a very high speed, enabling improved scenarios:
    autonomous driving, smart cities, smart factories, smart video surveillance, augmented
    reality, virtual reality, and remote health care/surgery.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 5G网络的特点，包括其更大的连续频谱、更先进的无线电天线技术（大规模MIMO）、更好的调制方案以及核心和RAN之间信号流的变化/优化，提供了具有显著更高带宽和更低延迟的网络能力。电信服务提供商和应用程序/数字服务开发者正在寻求利用这些特性，在将会有更多设备连接并且将以非常高的速度交换信息的应用中使用，从而实现改进的场景：自动驾驶、智能城市、智能工厂、智能视频监控、增强现实、虚拟现实和远程医疗/手术。
- en: To align with these requirements, workloads will be deployed, in many cases,
    in edge locations, near the end devices or user, to do near-real-time data processing
    and analysis of data, allowing smart devices to act and respond to inputs without
    sending that data to the cloud and back. This kind of application will largely
    be deployed as containerized workloads. Generally, 5G will be mainly software
    defined, continuing the transformation started with NFV; will have a further need
    for automation, due the speeds and volumes it will handle; and will be based on
    open source software.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足这些要求，工作负载将在许多情况下部署在边缘位置，靠近终端设备或用户，以进行近乎实时的数据处理和分析，使智能设备能够在不将数据发送到云端并返回的情况下进行操作和响应。这类应用将主要作为容器化工作负载部署。通常，5G将主要基于软件定义，继续NFV开始的转型；将需要进一步的自动化，因为其将处理的速度和体积；并将基于开源软件。
- en: 8.2.2 Edge computing
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 边缘计算
- en: Edge computing will allow applications to respond quickly, provide near-real-time
    insights, be less dependent on the network connection to the central datacenter
    or cloud, and reduce the amount of data that is transmitted centrally. Gartner
    predicts that by 2025, 75% of enterprise-generated data will be created and processed
    outside a traditional centralized data center or cloud (see [http://mng.bz/rdDE](http://mng.bz/rdDE)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘计算将使应用能够快速响应，提供近乎实时的洞察，减少对连接到中央数据中心或云的网络依赖，并减少需要集中传输的数据量。Gartner预测，到2025年，75%的企业生成数据将在传统的集中式数据中心或云之外创建和处理（见[http://mng.bz/rdDE](http://mng.bz/rdDE)）。
- en: 'As shown in figure 8.2, edge infrastructure will be deployed in locations with
    widespread distribution and, in many cases, smaller than a central datacenter,
    as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如图8.2所示，边缘基础设施将在广泛分布的位置部署，在许多情况下，其规模小于中央数据中心，如下所示：
- en: '*Telco edge*—Telco operators’ small data centers, points of presence, and network
    cabinets'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*电信边缘*—电信运营商的小型数据中心、接入点和网络机柜'
- en: '*Public cloud edge*—Cloud providers’/global broadcasters’ points of presence
    and CDN edges'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*公共云边缘*—云提供商/全球广播商的接入点和CDN边缘'
- en: '*Enterprise edge*—Enterprise/end users’ locations as branch offices, retail
    stores, warehouses, and factories'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*企业边缘*—企业/终端用户的分支机构、零售店、仓库和工厂'
- en: '![08-02](../../OEBPS/Images/08-02.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![08-02](../../OEBPS/Images/08-02.png)'
- en: Figure 8.2 Edge deployment
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 边缘部署
- en: Compute node numbers will typically grow, and the need will arise to have a
    central management plane capable of managing a larger compute fleet in the order
    of tens of thousands.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 计算节点数量通常会增长，将出现需要具有管理数以万计的计算集群的能力的中央管理平面。
- en: 8.2.3 Edge application examples
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.3 边缘应用示例
- en: 'Some examples follow of applications and use cases that will take advantage
    of deployed-at-edge locations and perform analysis and predictions on images,
    video, audio, and other types of data through local execution of AI models:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些应用和用例的示例，这些应用和用例将利用部署在边缘位置的优势，并通过本地执行AI模型对图像、视频、音频和其他类型的数据进行分析和预测：
- en: '*Predictive maintenance*—Analyzing manufacturing plants and machine data on-site
    to predict faults before they happen and optimize uptime and maintenance team
    work'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测性维护*—在现场分析制造工厂和机器数据，以预测故障发生之前的情况，并优化正常运行时间和维护团队的工作'
- en: '*Manufacturing quality check*—Analyzing pictures and videos of products on
    the assembly line to check conformance to quality standards'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*制造质量检查*—分析装配线上的产品图片和视频，以检查是否符合质量标准'
- en: '*Worker safety*—Analyzing correct security measure implementation and safety
    equipment usage through pictures and videos'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*工人安全*—通过图片和视频分析正确的安全措施实施和安全设备的使用'
- en: '*Diagnostic services and patient monitoring*—Using a computer vision solution
    deployed to the edge to process imaging, which could improve diagnostic accuracy
    and exam efficiency'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*诊断服务和患者监测*—使用部署到边缘的计算机视觉解决方案处理图像，这可以提高诊断准确性和检查效率'
- en: '*Queue and shelf management in retail stores*—Analyzing video feeds to check
    how many people are in line or the availability of goods on shelves and open cash
    registers or fill goods accordingly'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*零售店的队列和货架管理*—分析视频流以检查排队人数或货架上的商品可用性，以及打开现金收银机或相应地补充商品'
- en: '*Self-driving cars and industrial vehicles*—Low-latency processing for data
    ingested and gathered by autonomous vehicles'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自动驾驶汽车和工业车辆*—为自动驾驶车辆摄取和收集的数据提供低延迟处理'
- en: '*Manufacturing workers’ guidance and training through AR/VR*—AI models deployed
    on the edge with object detection and recognition, live capture, and the like'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过AR/VR进行制造工人指导和培训*—在边缘部署的AI模型，具有物体检测和识别、实时捕获等功能'
- en: '*Logistics tracking*—AI models that recognize packages, items, pallets, and
    vehicles in transit from images and videos and update tracking systems'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*物流跟踪*—通过图像和视频识别运输中的包裹、物品、托盘和车辆，并更新跟踪系统的AI模型'
- en: '*Inventory management and production planning*—Near-real-time analysis of asset
    status through images to provide inputs to production and supply chain fulfillment'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*库存管理和生产计划*—通过图像对资产状态进行近实时分析，为生产和供应链履行提供输入'
- en: 8.3 Anthos as a platform for edge and telco workloads
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 Anthos作为边缘和电信工作负载的平台
- en: In this section, we’ll discuss how Anthos can provide the foundation to support
    edge Anthos deployments and the execution of telco workloads. We’ll also delve
    into specific solutions Google has designed for this purpose.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论Anthos如何提供基础支持边缘Anthos部署和电信工作负载的执行。我们还将深入了解谷歌为此目的设计的特定解决方案。
- en: 8.3.1 Google Distributed Cloud Edge
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 Google分布式云边缘
- en: 'Google Distributed Cloud Edge (GDCE), shown in figure 8.3, is a fully managed
    solution from Google, designed to support telco virtualized and cloud native network
    functions and edge applications, including software, OS, and hardware (servers
    and TOR network switches) in Google-managed racks. It’s based on Anthos on bare
    metal (described in detail in chapter 17) and can be deployed in the following
    site types, described in the previous sections:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '如图8.3所示的Google分布式云边缘（GDCE），是谷歌提供的一项全面管理解决方案，旨在支持电信虚拟化和云原生网络功能以及边缘应用程序，包括软件、操作系统和硬件（服务器和TOR网络交换机），这些都在谷歌管理的机架上。它基于Anthos在裸机（在第17章中详细描述）上，并且可以部署在前述章节中描述的以下站点类型： '
- en: '*Public cloud edge*—Google edge locations'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*公共云边缘*—谷歌边缘位置'
- en: '*Telco edge*—Owned by telco providers'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*电信边缘*—由电信提供商拥有'
- en: '*Enterprise edge*—Owned by the final customer (such as in retail stores, factory
    floors, branch offices, and stadiums)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*企业边缘*—由最终客户拥有（例如在零售店、工厂车间、分支机构和大体育场）'
- en: '![08-03](../../OEBPS/Images/08-03.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![08-03](../../OEBPS/Images/08-03.png)'
- en: Figure 8.3 Google Distributed Cloud Edge high-level architecture
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 Google分布式云边缘高级架构
- en: GDCE provides fleet management capabilities to manage all the hardware and software
    assets and supports both containerized workloads and virtual machines through
    Anthos on bare metal capabilities.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: GDCE为用户提供车队管理功能，以管理所有硬件和软件资产，并通过在裸机上的Anthos功能支持容器化工作负载和虚拟机。
- en: GDCE also provides users a VPN connection to GCP, allowing users to interact
    with other applications running in a customer’s VPC and other GCP services. To
    service high-performance and low-latency workloads, GDCE also provides several
    high network performance features such as SR-IOV and DPDK. Google Cloud operates
    and manages the underlying infrastructure, extending the cloud experience to the
    customer premises.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: GDCE还为用户提供VPN连接到GCP，使用户能够与在客户VPC中运行的其他应用程序和其他GCP服务进行交互。为了服务高性能和低延迟工作负载，GDCE还提供了一些高网络性能特性，如SR-IOV和DPDK。谷歌云负责运营和管理底层基础设施，将云体验扩展到客户现场。
- en: Google provides the compute, network, and storage hardware. Those are shipped
    to the target location and managed by Google. The Google operations team manages
    and monitors the Google gear remotely.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Google提供计算、网络和存储硬件。这些硬件被运送到目标位置并由Google管理。Google运营团队远程管理和监控Google设备。
- en: The customer needs to have a designated contact person (part of the customer
    personnel, not a Google person) at the target location with access to the gear
    and permissions to perform basic administrative tasks. Those include, for example,
    cold restart, replacing of parts, ability to run local diagnostics on the gear,
    and common system administration tasks. After the hardware is connected and operational,
    users can consume the service using standard Google API tools and use the service.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 客户需要在目标位置指定一个联系人（客户人员的一部分，不是Google人员），该联系人有权访问设备并执行基本管理任务。这些包括，例如，冷启动、更换部件、在设备上运行本地诊断的能力以及常见的系统管理任务。硬件连接并运行后，用户可以使用标准的Google
    API工具使用该服务。
- en: A GDCE Kubernetes cluster consists of a control plane and worker nodes. Worker
    nodes are organized into node pools. The control plane is hosted on GCP in a single
    compute region. Worker nodes are servers on GDCE racks, with each worker node
    using a full physical server. A rack or a group of racks with shared space, power,
    cooling, and contiguous network fabric is defined as a GDCE zone.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: GDCE Kubernetes集群由控制平面和工作节点组成。工作节点组织成节点池。控制平面托管在GCP的单个计算区域。工作节点是GDCE机架上的服务器，每个工作节点使用一个完整的物理服务器。一个机架或一组共享空间、电源、冷却和连续网络布线的机架群定义为一个GDCE区域。
- en: Google Distributed Cloud Edge container resource model
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Google分布式云边缘容器资源模型
- en: 'GDCE has a different resource model compared to Anthos and other Anthos deployment
    options. Here you will find a list of the container resources used to describe,
    implement, and manage the compute architecture of a GDCE implementation, described
    schematically in the figure 8.4:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: GDCE的资源模型与Anthos和其他Anthos部署选项不同。在这里，您可以找到用于描述、实现和管理GDCE实现计算架构的容器资源列表，如图8.4所示：
- en: '*Zone*—A zone represents a set of machines sharing a network fabric or a single
    fault domain. A zone can represent one rack or a number of racks placed in the
    same location. The GDCE zone resource is different from GCE compute zones. Listing
    the GCE compute zones will not return the GDCE zones. A zone represents a single
    failure domain. To deploy fault-tolerant applications with high availability and
    help protect against failures, users need to deploy applications across multiple
    zones linked to the same region.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*区域*—区域代表一组共享网络布线或单个故障域的机器。区域可以代表一个机架或放置在同一位置的多个机架。GDCE区域资源与GCE计算区域不同。列出GCE计算区域不会返回GDCE区域。区域代表单个故障域。为了部署具有高可用性和容错能力的应用，并帮助保护免受故障的影响，用户需要将应用部署到与同一区域相连的多个区域。'
- en: '*Machine*—A machine represents a physical server. The machine resource metadata
    includes which rack it is on and other properties and tags. Machines are read-only
    resources for users. At deployment, a machine is assigned to a specific region
    based on the physical location of the GDCE system. Each physical machine is a
    node within the Distributed Cloud Edge cluster. A machine can be part of a Kubernetes
    cluster deployed only in its designated region.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器*—机器代表一个物理服务器。机器资源元数据包括它所在的机架以及其他属性和标签。机器是用户只读资源。在部署时，根据GDCE系统的物理位置，将机器分配到特定的区域。每个物理机器都是分布式云边缘集群中的一个节点。机器可以是其指定区域内部署的Kubernetes集群的一部分。'
- en: '*Cluster* —A cluster consists of a control plane and zero or more node pools.
    It is housed in a specific region and can connect node pools only from that region.
    If a user tries to connect a node pool with machines homed in a different region,
    the operation will fail.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*集群* —集群由控制平面和零个或多个节点池组成。它位于特定区域，并且只能连接来自该区域的节点池。如果用户尝试连接位于不同区域的机器节点池，操作将失败。'
- en: '*Node pool*—A node pool is a logical grouping of machines in a GDCE zone and
    is used to add worker nodes to clusters.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*节点池*—节点池是GDCE区域内机器的逻辑分组，用于向集群添加工作节点。'
- en: '*VPN connection*—GDCE supports setting up a VPN connection to a GCP project
    allowing workloads running on a GDCE Kubernetes cluster to connect directly to
    GCP resources. At least one node pool should be created in the cluster before
    establishing a VPN connection.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*VPN连接*——GDCE支持设置VPN连接到GCP项目，允许在GDCE Kubernetes集群上运行的工作负载直接连接到GCP资源。在建立VPN连接之前，应在集群中创建至少一个节点池。'
- en: '![08-04](../../OEBPS/Images/08-04.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![08-04](../../OEBPS/Images/08-04.png)'
- en: Figure 8.4 Google Distributed Cloud Edge container resource model
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 Google分布式云边缘容器资源模型
- en: Google Distributed Cloud Edge network resource model
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Google分布式云边缘网络资源模型
- en: In addition to Kubernetes cluster resources, APIs, and the default Kubernetes
    Pod network, GDCE also allows customers to provision additional networks in a
    GDCE zone and connect them with customer networks for different purposes. For
    example, in a network functions use case, a customer might create an operations,
    administration, and management network and a signal network, each with different
    multiple subnets that connect to the secondary interfaces of the network function
    Pods.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Kubernetes集群资源、API和默认的Kubernetes Pod网络之外，GDCE还允许客户在GDCE区域内配置额外的网络，并将它们与客户网络连接起来，用于不同的目的。例如，在网络功能用例中，客户可能会创建一个操作、管理和维护网络以及一个信号网络，每个网络都有不同的多个子网，这些子网连接到网络功能Pod的次要接口。
- en: 'Figure 8.5 describes the resources and their relationships for the network
    model in GDCE. From the high level, the following five types of resources are
    related to the edge network configuration:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5描述了GDCE中网络模型所涉及的资源及其关系。从高层次来看，以下五种类型的资源与边缘网络配置相关：
- en: '*Network*—A virtual network in a GDCE zone with private address space, which
    may contain one or more subnetworks. A network is isolated from other networks
    in the same GDCE zone.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*网络*——GDCE区域中的虚拟网络，具有私有地址空间，可能包含一个或多个子网。网络与其他GDCE区域中的网络隔离。'
- en: '*Subnetwork*—A layer-2 (VLAN) subnet in a GDCE network. A subnetwork has its
    own broadcast domain and customer-assigned classless interdomain routing. Subnetworks
    within a network can reach each other. Subnetworks of different networks in a
    GDCE zone cannot reach each other.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*子网*——GDCE网络中的第二层（VLAN）子网。子网有自己的广播域和客户分配的无类别域间路由。网络内的子网可以相互通信。GDCE区域内不同网络中的子网无法相互通信。'
- en: '*Interconnect*—Represents a bundled logical link of one or more physical links
    between GDCE and a customer network. An interconnect can be created only at GDCE
    site initiation time. Multiple interconnects are typically configured to provide
    high availability.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*互连*——代表GDCE与客户网络之间一个或多个物理链接的逻辑链路捆绑。互连只能在GDCE站点初始化时创建。通常配置多个互连以提供高可用性。'
- en: '*Interconnect attachment*—A virtual link provisioned on top of an interconnect
    based on customer requests, to provide an isolated connection between a GDCE network
    and a customer network (e.g., a VRF). Packets flowing through an interconnect
    attachment will be untagged or tagged with a customer-specified VLAN ID.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*互连附加*——根据客户请求在互连之上提供的虚拟链接，以在GDCE网络和客户网络（例如，VRF）之间提供隔离的连接。通过互连附加流动的数据包将是不标记的或标记为客户指定的VLAN
    ID。'
- en: '*Router*—A virtual routing instance to configure routing functionalities for
    a network in a GDCE zone. For example, customers can use it to configure a BGP
    peering session over an interconnect attachment between a GDCE network and a customer
    network, or over a Pod subnet, so that certain Pods can advertise prefixes to
    GDCE. By default, the routes received from subnetworks will be readvertised.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*路由器*——用于在GDCE区域内配置网络路由功能的虚拟路由实例。例如，客户可以使用它配置GDCE网络与客户网络之间的互连附加上的BGP对等会话，或者通过Pod子网，以便某些Pod可以向GDCE通告前缀。默认情况下，从子网接收到的路由将被重新广播。'
- en: '![08-05](../../OEBPS/Images/08-05.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![08-05](../../OEBPS/Images/08-05.png)'
- en: Figure 8.5 Google Distributed Cloud Edge network resource model
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 Google分布式云边缘网络资源模型
- en: 'These resources are similar to the Google Cloud network abstractions, with
    a few differences, described here:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这些资源类似于Google Cloud网络抽象，但有一些差异，如下所述：
- en: All network resources created for GDCE are local to the GDCE zone. A GDCE network
    doesn’t have direct connectivity to a GCP VPC. Networks in different GDCE zones
    also don’t have direct connectivity to each other, unless they are intentionally
    connected by the customer.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为GDCE创建的所有网络资源都位于GDCE区域内部。GDCE网络无法直接连接到GCP VPC。不同GDCE区域内的网络也无法直接相互连接，除非客户有意将其连接。
- en: GDCE subnetworks support VLAN and are thus isolated from each other in layer
    2.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GDCE子网支持VLAN，因此在第二层中相互隔离。
- en: System administrators should create and maintain all network resources. An app
    developer/owner has only view access to network resources.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 系统管理员应创建和维护所有网络资源。应用程序开发者/所有者只能查看网络资源。
- en: 8.3.2 Anthos capabilities for telco and edge workloads
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 Anthos针对电信和边缘工作负载的功能
- en: Besides the specific characteristics of GDCE, Anthos capabilities are available
    in the standard product to help the deployment, execution, and management of edge
    and telco workloads. Some of these capabilities have been designed specifically
    for this purpose and are mainly provided with Anthos on bare metal, which is the
    most suitable version for these workloads and is also the core of GDCE. Other
    capabilities, such as Anthos Config Management, are provided for general purpose
    but can be adapted for edge and telco applications.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 除了GDCE的具体特性外，Anthos的功能在标准产品中也是可用的，以帮助部署、执行和管理边缘和电信工作负载。其中一些功能是专门为此目的设计的，并且主要在Anthos裸金属上提供，这是最适合这些工作负载的版本，也是GDCE的核心。其他功能，如Anthos
    Config Management，虽然提供的是通用功能，但也可以适应边缘和电信应用。
- en: Multiple network interfaces for Pods
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Pod的多个网络接口
- en: A common containerized network functions (CNFs) requirement is to have additional
    network interfaces provisioned to Pods on top of the default Kubernetes interface.
    This is often needed to keep separation between the data plane and the management/control
    plane, for performance or security reasons, or to isolate network flows for other
    reasons. Anthos provides this capability on Anthos on bare metal, using a specific
    implementation of the Multus CNI plug-in.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的容器化网络功能（CNFs）需求是在默认的Kubernetes接口之上为Pods提供额外的网络接口。这通常是为了在数据平面和管理/控制平面之间保持分离，出于性能或安全原因，或者为了隔离其他原因的网络流。Anthos通过使用Multus
    CNI插件的特定实现，在裸金属上的Anthos上提供此功能。
- en: Architecture
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 建筑学
- en: 'This capability allows a single Pod to connect to multiple networks. The default
    Kubernetes interface will be seen in the Pod as eth0, whereas additional interfaces,
    created through the Multus CNI, will be seen by default as net1, net2, and so
    on, and can be configured. The CNI used for the additional interfaces can be as
    follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能允许单个Pod连接到多个网络。默认的Kubernetes接口在Pod中显示为eth0，而通过Multus CNI创建的额外接口默认显示为net1、net2等，并可进行配置。用于额外接口的CNI可以是以下之一：
- en: IPvlan
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IPvlan
- en: MacVLAN
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MacVLAN
- en: Bridge
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 桥接
- en: SR-IOV
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SR-IOV
- en: Setup
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 设置
- en: 'To enable this feature, the user must perform the following three actions:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用此功能，用户必须执行以下三个步骤：
- en: '*Enable multi-NIC.* Enable multi-NIC for pods by adding the multipleNetworkInterfaces
    field to the clusterNetwork section of the Anthos bare metal cluster custom resource
    and setting it to true, as shown here:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*启用多网络接口。* 通过将multipleNetworkInterfaces字段添加到Anthos裸金属集群自定义资源的clusterNetwork部分并将它设置为true，来为Pods启用多网络接口，如下所示：'
- en: '[PRE0]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Specify network interfaces.* Use the NetworkAttachmentDefinition custom resources
    to specify additional network interfaces. The NetworkAttachmentDefinition custom
    resources correspond to the networks that are available for the Pods. It’s possible
    to specify these custom resources within the cluster configuration manifest at
    cluster creation time or add them directly to an existing target cluster:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*指定网络接口。* 使用NetworkAttachmentDefinition自定义资源来指定额外的网络接口。NetworkAttachmentDefinition自定义资源对应于Pods可用的网络。这些自定义资源可以在集群创建时的集群配置清单中指定，或者直接添加到现有的目标集群中：'
- en: '[PRE1]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Assign network interfaces to Pods*. You can enable multiple NICs in the Pod
    or deployment manifest through the k8s.v1.cni.cncf.io/networks: annotation, using
    the value corresponding to the specific NetworkAttachmentDefinition custom resource
    and its namespace, as in the following example where the network interfaces are
    specified by names of two NetworkAttachmentDefinition custom resources, gke-network-1
    and gke-network-2, in the default namespace of the target cluster:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*将网络接口分配给Pod*。您可以通过k8s.v1.cni.cncf.io/networks:注解在Pod或部署清单中启用多个NIC，使用对应于特定NetworkAttachmentDefinition自定义资源和其命名空间的值，如下例所示，其中网络接口由两个NetworkAttachmentDefinition自定义资源的名称指定，即gke-network-1和gke-network-2，在目标集群的默认命名空间中：'
- en: '[PRE2]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Restricting network interfaces to a node pool
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 将网络接口限制在节点池中
- en: 'Use the k8s.v1.cni.cncf.io/nodeSelector annotation to specify the pool of nodes
    for which a NetworkAttachmentDefinition custom resource is valid. Anthos clusters
    on bare metal force any Pods that reference this custom resource to be deployed
    on those specific nodes. In the following example, Anthos clusters on bare metal
    force deployment of all Pods that are assigned the gke-network-1 network interface
    to the multinicNP node pool. Anthos clusters on bare metal labels a node pool
    with the baremetal.cluster.gke.io/node-pool label accordingly:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用k8s.v1.cni.cncf.io/nodeSelector注解来指定对于哪些节点，NetworkAttachmentDefinition自定义资源是有效的。在裸金属上的Anthos集群强制任何引用此自定义资源的Pod部署在这些特定的节点上。在以下示例中，裸金属上的Anthos集群强制将所有分配给multinicNP节点池的gke-network-1网络接口的Pod进行部署。裸金属上的Anthos集群相应地使用baremetal.cluster.gke.io/node-pool标签标记节点池：
- en: '[PRE3]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: A note on the SR-IOV plug-in
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 关于SR-IOV插件的说明
- en: SR-IOV is a specification that essentially enables the virtualization of physical
    PCIe devices. It allows you to segment a compliant network device, recognized
    on the host node as a physical function (PF, which usually represents a single
    NIC port), into multiple virtual functions (VFs) that can be directly accessed
    by a virtualized workload. SR-IOV direct access to network hardware provides enhanced
    performance, so it is widely used in VM-based VNFs.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: SR-IOV是一种规范，它本质上允许物理PCIe设备的虚拟化。它允许您将符合规范的网络设备（在主机节点上识别为物理功能PF，通常代表单个NIC端口）分割成多个虚拟功能（VF），这些虚拟功能可以直接被虚拟化工作负载访问。SR-IOV对网络硬件的直接访问提供了增强的性能，因此它被广泛应用于基于VM的VNFs。
- en: The SR-IOV CNI plug-in ([https://github.com/intel/sriov-cni](https://github.com/intel/sriov-cni))
    enables a Kubernetes Pod to attach directly to an SR-IOV VF and also bind the
    VF to a DPDK driver, which provides enhanced network performance for cloud native
    network functions as previously done for VNFs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: SR-IOV CNI插件([https://github.com/intel/sriov-cni](https://github.com/intel/sriov-cni))允许Kubernetes
    Pod直接连接到SR-IOV VF，并将VF绑定到DPDK驱动程序，这为云原生网络功能提供了增强的网络性能，就像之前为VNFs所做的那样。
- en: '![08-06](../../OEBPS/Images/08-06.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![08-06](../../OEBPS/Images/08-06.png)'
- en: Figure 8.6 CNF using multiple NICs to connect different network segments
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 使用多个NIC连接不同的网络段
- en: In figure 8.6, you can see an example diagram showing multiple CNFs chained
    together to provide different services (firewall, deep packet inspection, SD-WAN)
    using multi-NIC Pods to connect multiple network segments.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在图8.6中，您可以看到一个示例图，展示了多个CNF串联在一起，通过使用多NIC Pod连接多个网络段来提供不同的服务（防火墙、深度包检测、SD-WAN）。
- en: Running VM-based workloads on Anthos on bare metal
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在裸金属上的Anthos上运行基于VM的工作负载
- en: Not all the network functions will be deployed as CNFs, and some will stay as
    VNFs, so a coexistence between the two deployment models will be required for
    some time. To cater to this situation and other requirements of running part of
    a workload as a VM rather than as a container, Anthos on bare metal provides the
    possibility of running VM-based workloads through Anthos VM Runtime, based on
    KubeVirt.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有网络功能都将部署为CNFs，其中一些将保持为VNFs，因此对于一段时间内两种部署模型的共存将是必需的。为了满足这种情况以及其他将部分工作负载作为VM而不是容器运行的要求，裸金属上的Anthos提供了通过Anthos
    VM Runtime运行基于VM工作负载的可能性，该Runtime基于KubeVirt。
- en: A synthetic description of the task you need to perform to run VMs on Anthos
    on bare metal is shown in the next sections.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中展示了在裸金属上的Anthos上运行VM所需的任务的综合描述。
- en: Enabling Anthos VM Runtime
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 启用Anthos VM Runtime
- en: 'To enable Anthos VM Runtime you need to do the following:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用Anthos VM Runtime，您需要执行以下操作：
- en: 'Update the VMRuntime custom resource to set enabled to true as shown in the
    next example:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新VMRuntime自定义资源，将enabled设置为true，如下例所示：
- en: '[PRE4]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If your node doesn’t support hardware virtualization, or you aren’t sure, set
    useEmulation to true. If available, hardware virtualization provides better performance
    than software emulation. The useEmulation field defaults to false, if it isn’t
    specified.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您的节点不支持硬件虚拟化，或者您不确定，请将useEmulation设置为true。如果可用，硬件虚拟化比软件仿真提供更好的性能。如果未指定，useEmulation字段默认为false。
- en: 'You can change the image format used for the VMs you create by setting the
    vmImageFormat field that supports two disk image format values: raw and qcow2.
    If you don’t set vmImageFormat, the Anthos VM Runtime uses the raw disk image
    format to create VMs. The raw format may provide improved performance over qcow2,
    a copy-on-write format, but may use more disk.'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过设置支持两种磁盘镜像格式值（raw和qcow2）的vmImageFormat字段来更改您创建的虚拟机使用的镜像格式。如果您不设置vmImageFormat，Anthos
    VM Runtime将使用raw磁盘镜像格式创建虚拟机。与qcow2（一种写时复制格式）相比，raw格式可能提供更好的性能，但可能使用更多的磁盘。
- en: 'Save the configuration and verify that the VMRuntime custom resource is enabled:
    you can execute kubectl describe vmruntime vmruntime and check that the description
    shows VMRuntime.Status.Ready set to true.'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存配置并验证VMRuntime自定义资源是否启用：您可以执行kubectl describe vmruntime vmruntime并检查描述是否显示VMRuntime.Status.Ready设置为true。
- en: Creating a VM
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 创建虚拟机
- en: 'Before creating a VM, it’s recommended to configure a cloud-init file to ensure
    that you have console access to the VM once it’s created. You can create a custom
    cloud-init file in two ways. The easiest way is to specify the --os=<OPERATING_SYSTEM>
    flag when creating the VM. This method automatically configures a simple cloud-init
    file and works for the following operating systems:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建虚拟机之前，建议配置一个cloud-init文件以确保您在创建虚拟机后可以访问控制台。您可以通过以下两种方式创建自定义cloud-init文件。最简单的方法是在创建虚拟机时指定--os=<OPERATING_SYSTEM>标志。此方法自动配置一个简单的cloud-init文件，适用于以下操作系统：
- en: Ubuntu
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ubuntu
- en: CentOS
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CentOS
- en: Debian
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Debian
- en: Fedora
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fedora
- en: 'Once your VM is created, you can access it for the first time with the following
    credentials and then change the password:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 您的虚拟机创建后，您可以使用以下凭据首次访问它，然后更改密码：
- en: '[PRE5]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If your image contains a different Linux-based OS or you need a more advanced
    configuration, you can manually create a custom cloud-init file and specify the
    path to that file by specifying the --cloud-init-file=<path/to/file> flag. In
    its most basic form, the cloud-init file is a YAML file that contains the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的镜像包含不同的基于Linux的操作系统或您需要更高级的配置，您可以手动创建一个自定义cloud-init文件，并通过指定--cloud-init-file=<path/to/file>标志来指定该文件的路径。在其最基本的形式中，cloud-init文件是一个包含以下内容的YAML文件：
- en: '[PRE6]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To create a VM using kubectl, you need to use the following steps:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用kubectl创建虚拟机，您需要遵循以下步骤：
- en: 'Install the virtctl plug-in with the following command: sudo -E ./bmctl install
    virtctl.'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令安装virtctl插件：sudo -E ./bmctl install virtctl。
- en: 'Execute the command kubectl virt create vm. The next example contains parameters:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行命令kubectl virt create vm。以下示例包含参数：
- en: '[PRE7]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The parameters are explained here:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 参数在此处解释：
- en: '*VM_NAME*—The name of the VM that you want to create.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*VM_NAME*—您想要创建的虚拟机名称。'
- en: '*MODE*—The access mode of the boot disk. Possible values are ReadWriteOnce
    (default) or ReadWriteMany.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MODE*—启动磁盘的访问模式。可能的值是ReadWriteOnce（默认）或ReadWriteMany。'
- en: '*DISK_SIZE*—The size you want for the boot disk. The default value is 20Gi.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DISK_SIZE*—您希望启动磁盘的大小。默认值是20Gi。'
- en: '*DISK_CLASS*—The storage class of the boot disk. The default value is local-shared.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DISK_CLASS*—启动磁盘的存储类别。默认值是local-shared。'
- en: '*FILE_PATH*—The full path of the customized cloud-init file. Depending on the
    image, this may be required to gain console access to the VM after it is created.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*FILE_PATH*—自定义cloud-init文件的完整路径。根据镜像的不同，这可能是在创建虚拟机后获得控制台访问权限所必需的。'
- en: '*CPU_NUMBER*—The number of CPUs you want to configure for the VM. The default
    value is 1.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*CPU_NUMBER*—您想要为虚拟机配置的CPU数量。默认值是1。'
- en: '*IMAGE_NAME*—The VM image, which can be ubuntu20.04 (default), centos8, or
    a URL of the image.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*IMAGE_NAME*—虚拟机镜像，可以是ubuntu20.04（默认）、centos8或镜像的URL。'
- en: '*MEMORY_SIZE*—The memory size of the VM. The default value is 4Gi.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MEMORY_SIZE*—虚拟机的内存大小。默认值是4Gi。'
- en: If parameters are not specified, the default values are used.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未指定参数，则使用默认值。
- en: Alternatively, it’s possible to apply a manifest defining a VirtualMachine custom
    resource, which also enables popular GitOps deployment methods (declarative and
    asynchronous management).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，还可以应用一个定义VirtualMachine自定义资源的清单，这还启用了流行的GitOps部署方法（声明性和异步管理）。
- en: Orchestration and automation for large compute fleets
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 大型计算集群的编排和自动化
- en: One of the impacts of edge and radio access network CNFs deployments is the
    need to have a central control plane capable of managing a wider and more granularly
    distributed compute fleet, and, moreover, deploying several application instances
    that could be orders of magnitude bigger than what is typically deployed in traditional
    data centers or public clouds. In some cases, you will need to deploy instances
    of the same application on tens, hundreds, or thousands of compute clusters, distributed
    in edge locations. Often, you’ll need to define the locations where the instances
    must be deployed, dynamically based on specific criteria, with very short notice
    to adapt to changing needs from network, monitored objects, or application users.
    Central configuration and policy management capabilities provided by Anthos Config
    Management will be key to satisfying this requirement.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘和无线接入网络 CNFs 部署的一个影响是需要一个中央控制平面，能够管理更广泛且更细粒度分布的计算集群，并且部署多个应用程序实例，这些实例可能比在传统的数据中心或公共云中部署的实例大几个数量级。在某些情况下，您可能需要在成百上千个计算集群上部署相同应用程序的实例，这些集群分布在边缘位置。通常，您需要根据特定标准动态定义实例必须部署的位置，以极短的通知时间来适应网络、监控对象或应用程序用户的变化需求。Anthos
    Config Management 提供的中央配置和政策管理能力将满足这一需求的关键。
- en: Anthos Config Management
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos Config Management
- en: 'Anthos Config Management capabilities are described extensively in chapter
    11\. Here we discuss a couple of them that are especially useful for managing
    applications and network functions deployments on a large fleet of clusters:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos Config Management 的功能在第 11 章中进行了详细描述。在这里，我们讨论了其中一些特别适用于在大量集群上管理应用程序和网络功能部署的功能：
- en: Multiple repository mode
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多仓库模式
- en: Cluster selectors
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群选择器
- en: Multiple repository mode
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 多仓库模式
- en: 'Enabling multirepository mode on Anthos Config Management allows you to sync
    configuration from multiple repositories to the same set of clusters, as shown
    in figure 8.7\. A single *root repository*, typically managed by a central platform
    team, hosts cluster and centrally defined namespace-scoped configurations, whereas
    optional *namespace repositories* are used to configure objects in specific namespaces.
    This capability extends ACM usage, beyond platform configuration and policies
    management, to the deployment of applications: it is possible to delegate the
    setup and control of a namespace repository to an application release team. Centrally
    defined namespaces resources are inherited, whereas the application team is free
    to configure application-related ones (deployments, config maps, etc.). If conflicts
    arise between the root and the namespace repository, only the declaration in the
    root repository is applied to the cluster.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Anthos Config Management 中启用多仓库模式允许您将来自多个仓库的配置同步到同一组集群中，如图 8.7 所示。一个单一的 *root
    仓库*，通常由中央平台团队管理，托管集群和中央定义的命名空间范围内的配置，而可选的 *命名空间仓库* 用于配置特定命名空间中的对象。这种能力扩展了 ACM
    的使用范围，不仅限于平台配置和政策管理，还包括应用程序的部署：可以将命名空间仓库的设置和控制委托给应用程序发布团队。中央定义的命名空间资源是继承的，而应用程序团队可以自由配置与应用程序相关的资源（部署、配置映射等）。如果根仓库和命名空间仓库之间出现冲突，只有根仓库中的声明应用于集群。
- en: '![08-07](../../OEBPS/Images/08-07.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![08-07](../../OEBPS/Images/08-07.png)'
- en: Figure 8.7 Anthos Config Management with multiple repositories
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7 多仓库的 Anthos Config Management
- en: Namespace repositories are defined by a RepoSync resource, deployed in the specific
    namespace by the central platform team or directly by the application team, if
    delegated by the central team.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间仓库由 RepoSync 资源定义，由中央平台团队在特定命名空间中部署，或者如果由中央团队委托，则由应用程序团队直接部署。
- en: 'The diagram in figure 8.8 represents the structures of a root repository and
    two namespace repositories where numbers identify the following:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8 中的图表示了根仓库和两个命名空间仓库的结构，其中数字标识以下内容：
- en: 1\. The root configuration defined by the central admin/platform team.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 由中央管理员/平台团队定义的根配置。
- en: 2\. The configuration defined by the central team, common to multiple namespaces
    because it’s placed in their parent folder and is inherited by the namespaces.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 由中央团队定义的配置，由于它位于它们的父文件夹中，因此对命名空间是通用的。
- en: 3\. Resources defined by the central team in each specific namespace folder,
    including the RepoSync resource. (This can be delegated to the applications team,
    too.)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 中央团队在每个特定命名空间文件夹中定义的资源，包括 RepoSync 资源。（这也可以委托给应用程序团队。）
- en: 4, 5\. Application configuration manifests managed by the application team.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 4, 5. 应用配置清单由应用团队管理。
- en: '![08-08](../../OEBPS/Images/08-08.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![08-08](../../OEBPS/Images/08-08.png)'
- en: Figure 8.8 ACM example config with multiple repositories
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 ACM示例配置具有多个仓库
- en: Cluster selectors
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 集群选择器
- en: ACM provides two specific resource objects that allow you to selectively choose
    clusters where a specific configuration or deployment is applied, using the Kubernetes
    label and selector approach.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ACM提供了两个特定的资源对象，允许您使用Kubernetes标签和选择器方法，有选择地选择应用特定配置或部署的集群。
- en: 'Cluster objects identify specific clusters managed by ACM and assign them arbitrary
    labels to identify all cluster-relevant attributes (location, hardware capabilities,
    purpose, etc.):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 集群对象识别ACM管理的特定集群，并分配任意标签以识别所有集群相关属性（位置、硬件能力、目的等）：
- en: '[PRE8]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'ClusterSelector objects are used to select only clusters with a given label
    or combination of labels. The following ClusterSelector selects only clusters
    with the environment: production and kind: edge labels—useful, for example, to
    target all production clusters deployed in edge locations:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ClusterSelector对象用于选择仅具有给定标签或标签组合的集群。以下ClusterSelector仅选择具有环境：生产和环境：边缘标签的集群——例如，用于针对所有在边缘位置部署的生产集群：
- en: '[PRE9]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'ClusterSelector objects can be referenced, using the configmanagement.gke.io/
    cluster-selector: annotation, from any ACM-managed Kubernetes resource to select
    on which cluster that resource will be deployed.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '可以使用configmanagement.gke.io/ cluster-selector: 注解从任何ACM管理的Kubernetes资源引用ClusterSelector对象，以选择该资源将在哪个集群上部署。'
- en: 'The following ClusterRole will be created only on production clusters deployed
    in edge locations:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 以下ClusterRole仅将在边缘位置部署的生产集群上创建：
- en: '[PRE10]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ClusterSelector objects can also be referenced by application-specific resources
    to selectively define the clusters that will host the application instance, and,
    if multiple repositories are used, in RepoSync resources to sync the specific
    namespace repository only to specific clusters.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ClusterSelector对象也可以由应用特定资源引用，以有选择地定义将托管应用实例的集群，并且如果使用多个仓库，则在RepoSync资源中仅同步特定命名空间仓库到特定集群。
- en: 'In figure 8.9, you can see an example of a distributed unit CNF, defined by
    an operator-managed custom resource (kind: DU), deployed on hundreds of Anthos
    clusters labeled with cnfs: du-slice-1 but not on the ones dedicated to central
    units (cnfs: cu-cp-slice1, cu-up-slice-1).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '在图8.9中，您可以看到一个由操作员管理的自定义资源（kind: DU）定义的分布式单元CNF示例，该资源部署在数百个标记为cnfs: du-slice-1的Anthos集群上，但不在专门用于中央单元（cnfs:
    cu-cp-slice1, cu-up-slice-1）的集群上。'
- en: '![08-09](../../OEBPS/Images/08-09.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![08-09](../../OEBPS/Images/08-09.png)'
- en: Figure 8.9 ACM example config using cluster selectors to deploy different CNFs
    to different clusters
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 ACM示例配置使用集群选择器将不同的CNFs部署到不同的集群
- en: 'All these configurations are driven by continuously synchronizing with a Git
    repository (root and, if present, namespace) that acts as the source of truth.
    All the fleet will regularly converge to the desired state, so, for example, the
    following things occur:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些配置都是通过持续与Git仓库（根目录和，如果存在，命名空间）同步来驱动的，该仓库作为事实来源。整个集群将定期收敛到期望状态，因此，例如，以下事情会发生：
- en: 'Any cluster assigned the cnfs: du-slice-1 label will immediately have the CNF
    deployed.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '被分配cnfs: du-slice-1标签的任何集群将立即部署CNF。'
- en: Any cluster that changes destination or purpose by changing any of the labels
    will have the CNF deleted and the new desired configuration immediately applied.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何通过更改标签更改目的地或目的的集群将删除CNF，并立即应用新的期望配置。
- en: Any update to the CNF configuration will be immediately deployed on all the
    desired clusters.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNF配置的任何更新都将立即部署到所有期望的集群。
- en: '8.3.3 Solution architecture example: Smart retail'
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.3 解决方案架构示例：智能零售
- en: In addition to the deployment examples provided in this chapter, here we will
    describe an edge application architecture built on Anthos for a retailer.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 除了本章中提供的部署示例之外，我们还将描述一个基于Anthos为零售商构建的边缘应用架构。
- en: One of the goals of the retailer is to have the smallest possible infrastructure
    deployed in stores, leaving only video cameras as endpoints. The application deployed
    is a real-time queue management, as shown in figure 8.10\. It starts with a video
    camera monitoring the checkout line at the cash register and streaming the video
    feed in real time through a 5G modem in the store over a 5G connection to a telecom
    operator edge location. The telecom operator network edge receives all 5G traffic,
    and the software there intelligently sends the retail store traffic to an Anthos
    bare metal cluster deployed in this edge. There, a container-based ML app runs
    machine learning inference on the incoming video feed and detects how many people
    are in line at the retail store. If that number crosses a certain threshold, a
    notification to open another cash register is sent back to the store over the
    same low-latency 5G path on which the video feed came in.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 零售商的一个目标是在商店中部署尽可能小的基础设施，只留下视频摄像头作为端点。部署的应用是实时排队管理，如图 8.10 所示。它从监控收银台结账线的视频摄像头开始，并通过商店中的
    5G 调制解调器实时传输视频流，通过 5G 连接到电信运营商的边缘位置。电信运营商的边缘网络接收所有 5G 流量，那里的软件智能地将零售店流量发送到在此边缘部署的
    Anthos 裸金属集群。在那里，基于容器的机器学习应用对传入的视频流进行机器学习推理，并检测零售店中有多少人排队。如果这个数字超过某个阈值，就会通过相同的低延迟
    5G 路径发送通知到商店，该路径与视频流进入的路径相同。
- en: '![08-10](../../OEBPS/Images/08-10.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![08-10](../../OEBPS/Images/08-10.png)'
- en: Figure 8.10 High-level architecture for an edge smart retail solution
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 边缘智能零售解决方案的高级架构
- en: In addition, the AI/ML app also sends metadata about this and other events to
    Google Cloud. Later, Google Cloud securely processes this information to provide
    insight to the retailer and to train future iterations of the models. With Google
    Cloud’s sophisticated AI/ML product suite, we can easily train and deploy highly
    accurate models for any applications anywhere.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，AI/ML 应用还将有关此事件和其他事件的元数据发送到 Google Cloud。稍后，Google Cloud 安全地处理这些信息，为零售商提供洞察力，并训练未来模型的迭代。凭借
    Google Cloud 精密的 AI/ML 产品套件，我们可以轻松训练和部署适用于任何应用的任何地点的高度精确模型。
- en: This architecture can be extended to other smart retail applications, such as
    per-customer personalization and digital signage, real-time recommendations, contactless
    checkout, and automatic restocking of shelves. All the latency-sensitive processing,
    such as video or image ML inference, happens at the telecom edge, whereas all
    the non-real-time components, like model training and data analytics, run in the
    cloud.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构可以扩展到其他智能零售应用，例如按客户定制的个性化、数字标牌、实时推荐、无接触结账和货架自动补货。所有对延迟敏感的处理，如视频或图像机器学习推理，都在电信边缘发生，而所有非实时组件，如模型训练和数据分析，则在云端运行。
- en: Summary
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Market trends and continuously evolving platform capabilities drive telco network
    functions and edge applications toward Kubernetes and Anthos as the ideal deployment
    platform.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 市场趋势和持续发展的平台能力推动电信网络功能和边缘应用向 Kubernetes 和 Anthos 作为理想的部署平台发展。
- en: NFV had the goal of transitioning telco network functions from dedicated, proprietary
    hardware appliances to virtual machines deployed on industry-standard x86 servers.
    This transformation has started but has not completely materialized as expected.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NFV 的目标是实现电信网络功能从专用、专有硬件设备向在行业标准 x86 服务器上部署的虚拟机的转变。这种转型已经开始，但并未完全按照预期实现。
- en: 5G is driving the rise of new applications, deployed on edge locations as containerized
    workloads and using 5G networks’ higher bandwidth and lower latency, providing
    near-real-time data processing and allowing smart devices to act and respond to
    inputs without sending that data to the cloud and back.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 5G 正在推动新应用的兴起，这些应用以容器化工作负载的形式部署在边缘位置，并使用 5G 网络的高带宽和低延迟，提供近乎实时的数据处理，并允许智能设备在不将数据发送到云端并返回的情况下进行操作和响应。
- en: 'Anthos provides specific capabilities that are key enablers for the deployment,
    execution, and management of edge and telco workloads: VM runtime, hardware accelerators,
    multiple network interfaces per Pod, and large fleet management.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthos 提供了关键能力，这些能力对于边缘和电信工作负载的部署、执行和管理至关重要：虚拟机运行时、硬件加速器、每个 Pod 的多个网络接口以及大型车队管理。
- en: Google Distributed Cloud Edge (GDCE) is a fully managed solution from Google,
    based on Anthos on bare metal, designed to support telco virtualized and cloud
    native network functions and edge applications and including software, OS, and
    hardware in Google-managed racks. It can be deployed in Google, telco, or enterprise
    edge.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google 分布式云边缘 (GDCE) 是 Google 提供的一个完全托管解决方案，基于裸金属上的 Anthos，旨在支持电信虚拟化和云原生网络功能以及边缘应用，并包括软件、操作系统和硬件，这些都在
    Google 管理的机架上。它可以在 Google、电信或企业边缘部署。
