- en: 6 Scaling applications across multiple Pods with controllers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 使用控制器在多个Pod上扩展应用
- en: 'The basic idea for scaling applications is simple: run more Pods. Kubernetes
    abstracts networking and storage away from the compute layer, so you can run many
    Pods, which are copies of the same app, and just plug them into the same abstractions.
    Kubernetes calls those Pods replicas, and in a multinode cluster, they’ll be distributed
    across many nodes. This gives you all the benefits of scale: greater capacity
    to handle load and high availability in case of failure—all in a platform that
    can scale up and down in seconds.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展应用的基本想法很简单：运行更多的Pod。Kubernetes将网络和存储从计算层抽象出来，这样你就可以运行许多Pod，它们是相同应用的副本，并将它们仅插入到相同的抽象中。Kubernetes将这些Pod称为副本，在多节点集群中，它们将分布到许多节点上。这为你提供了所有扩展的好处：更大的处理负载的能力和故障情况下的高可用性——所有这些都在一个可以在几秒钟内扩展和缩减的平台中。
- en: Kubernetes also provides some alternative scaling options to meet different
    application requirements, and we’ll work through them all in this chapter. The
    one you’ll use most often is the Deployment controller, which is actually the
    simplest, but we’ll spend time on the others, too, so you understand how to scale
    different types of applications in your cluster.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes还提供了一些替代的扩展选项来满足不同的应用需求，我们将在本章中逐一介绍。你将最常使用的是Deployment控制器，它实际上是简单的，但我们也将在其他方面花费时间，这样你就能了解如何在你的集群中扩展不同类型的应用。
- en: 6.1 How Kubernetes runs apps at scale
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 Kubernetes如何运行大规模应用
- en: The Pod is the unit of compute in Kubernetes, and you learned in chapter 2 that
    you don’t usually run Pods directly; instead, you define another resource to manage
    them for you. That resource is a controller, and we’ve used Deployment controllers
    ever since. A controller spec includes a Pod template, which it uses to create
    and replace Pods. It can use that same template to create many replicas of a Pod.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Pod是Kubernetes中的计算单元，你在第2章中了解到你通常不会直接运行Pod；相反，你定义另一个资源来为你管理它们。这个资源是一个控制器，我们自从那时起就一直在使用Deployment控制器。控制器规范包括一个Pod模板，它使用该模板来创建和替换Pod。它可以使用相同的模板来创建Pod的多个副本。
- en: Deployments are probably the resource you’ll use most in Kubernetes, and you’ve
    already had lots of experience with them. Now it’s time to dig a bit deeper and
    learn that Deployments don’t actually manage Pods directly—that’s done by another
    resource called a ReplicaSet. Figure 6.1 shows the relationship among Deployment,
    ReplicaSet, and Pods.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Deployments可能是你在Kubernetes中最常用的资源，你已经对它们有很多经验了。现在，是时候深入挖掘一下，了解Deployments实际上并不直接管理Pods——这是由另一个称为ReplicaSet的资源完成的。图6.1显示了Deployment、ReplicaSet和Pods之间的关系。
- en: '![](../Images/6-1.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图6-1](../Images/6-1.jpg)'
- en: Figure 6.1 Every software problem can be solved by adding another layer of abstraction.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 每个软件问题都可以通过添加另一层抽象来解决。
- en: You’ll use a Deployment to describe your app in most cases; the Deployment is
    a controller that manages ReplicaSets, and the ReplicaSet is a controller that
    manages Pods. You can create a ReplicaSet directly rather than using a Deployment,
    and we’ll do that for the first few exercises, just to see how scaling works.
    The YAML for a ReplicaSet is almost the same as for a Deployment; it needs a selector
    to find the resources it owns and a Pod template to create resources. Listing
    6.1 shows an abbreviated spec.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，你会使用Deployment来描述你的应用；Deployment是一个控制器，它管理ReplicaSet，而ReplicaSet是一个控制器，它管理Pods。你可以直接创建ReplicaSet而不是使用Deployment，我们将在前几个练习中这样做，只是为了看看扩展是如何工作的。ReplicaSet的YAML几乎与Deployment相同；它需要一个选择器来找到它拥有的资源，以及一个Pod模板来创建资源。列表6.1显示了简化的规范。
- en: Listing 6.1 whoami.yaml, a ReplicaSet without a Deployment
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.1 whoami.yaml，一个没有Deployment的ReplicaSet
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The only things different in this spec from the Deployment definitions we’ve
    used are the object type ReplicaSet and the `replicas` field, which states how
    many Pods to run. This spec uses a single replica, which means Kubernetes will
    run a single Pod.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前使用的Deployment定义相比，这个规范中唯一不同的是对象类型ReplicaSet和`replicas`字段，它说明了要运行多少个Pod。这个规范使用单个副本，这意味着Kubernetes将运行一个Pod。
- en: Try it now Deploy the ReplicaSet, along with a LoadBalancer Service, which uses
    the same label selector as the ReplicaSet to send traffic to the Pods.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 部署ReplicaSet，以及一个使用与ReplicaSet相同标签选择器的LoadBalancer服务，将流量发送到Pods。
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can see my output in figure 6.2\. There’s nothing new here; the ReplicaSet
    owns a single Pod, and when you delete that Pod, the ReplicaSet replaces it. I’ve
    removed the `kubectl` `describe` output in the final command, but if you run that,
    you’ll see it ends with a list of events, where the ReplicaSet writes activity
    logs on how it created Pods.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图6.2中看到我的输出。这里没有新的内容；ReplicaSet拥有一个Pod，当你删除该Pod时，ReplicaSet会替换它。我在最后的命令中移除了`kubectl
    describe`的输出，但如果你运行它，你会看到它以一系列事件结束，其中ReplicaSet记录了它是如何创建Pod的活动日志。
- en: '![](../Images/6-2.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-2.jpg)'
- en: 'Figure 6.2 Working with a ReplicaSet is just like working with a Deployment:
    it creates and manages Pods.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 使用ReplicaSet就像使用Deployment一样：它创建和管理Pod。
- en: The ReplicaSet replaces deleted Pods because it constantly runs a control loop,
    checking that the number of objects it owns matches the number of replicas it
    should have. You use the same mechanism when you scale up your application—you
    update the ReplicaSet spec to set a new number of replicas, and then the control
    loop sees that it needs more and creates them from the same Pod template.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSet替换已删除的Pod，因为它不断运行一个控制循环，检查它拥有的对象数量是否与它应有的副本数量相匹配。当你扩展你的应用程序时，你使用相同的机制——你更新ReplicaSet规范以设置新的副本数量，然后控制循环看到它需要更多，并从相同的Pod模板中创建它们。
- en: Try it now Scale up the application by deploying an updated ReplicaSet definition
    that specifies three replicas.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 扩展应用程序，通过部署一个更新的ReplicaSet定义，指定三个副本。
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'My output, shown in figure 6.3, raises a couple of questions: How does Kubernetes
    manage to scale the app so quickly, and how do the HTTP responses come from different
    Pods?'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我在图6.3中展示的输出引发了一些问题：Kubernetes是如何如此快速地扩展应用的，以及HTTP响应是如何从不同的Pod中产生的？
- en: '![](../Images/6-3.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-3.jpg)'
- en: Figure 6.3 Scaling ReplicaSets is fast, and at scale, a Service can distribute
    requests to many Pods.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 扩展ReplicaSet很快，在扩展规模时，Service可以分配请求到多个Pod。
- en: 'The first is simple to answer: this is a single-node cluster, so every Pod
    will run on the same node, and that node has already pulled the Docker image for
    the app. When you scale up in a production cluster, it’s likely that new Pods
    will be scheduled to run on nodes that don’t have the image locally, and they’ll
    need to pull the image before they can run the Pod. The speed at which you can
    scale is bounded by the speed at which your images can be pulled, which is why
    you need to invest time in optimizing your images.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题很简单回答：这是一个单节点集群，所以每个Pod都会运行在同一个节点上，而这个节点已经拉取了应用的Docker镜像。当你在一个生产集群中扩展时，新Pod可能会被调度到没有本地镜像的节点上，它们在运行Pod之前需要拉取镜像。你可以扩展的速度受限于你可以拉取镜像的速度，这就是为什么你需要投入时间来优化你的镜像。
- en: As to how we can make an HTTP request to the same Kubernetes Service and get
    responses from different Pods, that’s all down to the loose coupling between Services
    and Pods. When you scaled up the ReplicaSet, there were suddenly multiple Pods
    that matched the Service’s label selector, and when that happens, Kubernetes load-balances
    requests across the Pods. Figure 6.4 shows how the same label selector maintains
    the relationship between ReplicaSet and Pods and between Service and Pods.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 至于我们如何向同一个Kubernetes Service发起HTTP请求并从不同的Pod获取响应，这完全归因于Service和Pod之间的松散耦合。当你扩展ReplicaSet时，突然出现了多个匹配Service标签选择器的Pod，当这种情况发生时，Kubernetes会在Pod之间负载均衡请求。图6.4展示了相同的标签选择器如何维护ReplicaSet和Pod之间以及Service和Pod之间的关系。
- en: '![](../Images/6-4.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-4.jpg)'
- en: Figure 6.4 A Service with the same label selector as a ReplicaSet will use all
    of its Pods.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 与ReplicaSet具有相同标签选择器的Service将使用其所有Pod。
- en: 'The abstraction between networking and compute is what makes scaling so easy
    in Kubernetes. You may be experiencing a warm glow about now—suddenly all the
    complexity starts to fit into place, and you see how the separation between resources
    is the enabler for some very powerful features. This is the core of scaling: you
    run as many Pods as you need, and they all sit behind one Service. When consumers
    access the Service, Kubernetes distributes the load between Pods.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 网络和计算之间的抽象使得在Kubernetes中扩展变得如此容易。你现在可能感到一种温暖的喜悦——突然间，所有的复杂性开始变得有序，你看到了资源分离是如何成为一些非常强大功能的推动力的。这是扩展的核心：你需要运行多少Pod就运行多少，它们都位于一个Service后面。当消费者访问Service时，Kubernetes会在Pod之间分配负载。
- en: Load balancing is a feature of all the Service types in Kubernetes. We’ve deployed
    a LoadBalancer Service in these exercises, and that receives traffic into the
    cluster and sends it to the Pods. It also creates a ClusterIP for other Pods to
    use, and when Pods communicate within the cluster, they also benefit from load
    balancing.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡是Kubernetes中所有服务类型的一个功能。我们在这些练习中部署了一个LoadBalancer服务，它接收集群中的流量并将其发送到Pod。它还创建了一个ClusterIP供其他Pod使用，当Pod在集群内部通信时，它们也受益于负载均衡。
- en: Try it now Deploy a new Pod, and use it to call the who-am-I Service internally,
    using the ClusterIP, which Kubernetes resolves from the service name.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看：部署一个新的Pod，并使用它通过ClusterIP调用内部的who-am-I服务，Kubernetes会根据服务名称解析这个ClusterIP。
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As shown in figure 6.5, the behavior for a Pod consuming an internal Service
    is the same as for external consumers, and requests are load-balanced across the
    Pods. When you run this exercise, you may see the requests distributed exactly
    equally, or you may see some Pods responding more than once, depending on the
    vagaries of the network.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如图6.5所示，Pod消耗内部服务的行为与外部消费者相同，请求在Pod之间进行负载均衡。当你运行这个练习时，你可能看到请求被完全均匀地分配，或者你可能看到一些Pod响应多次，这取决于网络的不可预测性。
- en: '![](../Images/6-5.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-5.jpg)'
- en: 'Figure 6.5 The world inside the cluster: Pod-to-Pod networking also benefits
    from Service load balancing.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5集群内部的世界：Pod到Pod的网络也受益于服务负载均衡。
- en: In chapter 3, we covered Services and how the ClusterIP address is an abstraction
    from the Pod’s IP address, so when a Pod is replaced, the application is still
    accessible using the same Service address. Now you see that the Service can be
    an abstraction across many Pods, and the same networking layer that routes traffic
    to a Pod on any node can load-balance across multiple Pods.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三章中，我们介绍了服务以及ClusterIP地址是如何从Pod的IP地址抽象出来的，所以当一个Pod被替换时，应用程序仍然可以通过相同的服务地址访问。现在你看到服务可以在许多Pod之间进行抽象，并且路由流量到任何节点上的Pod的网络层也可以在多个Pod之间进行负载均衡。
- en: 6.2 Scaling for load with Deployments and ReplicaSets
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 使用Deployment和ReplicaSet进行负载扩展
- en: 'ReplicaSets make it incredibly easy to scale your app: you can scale up or
    down in seconds just by changing the number of replicas in the spec. It’s perfect
    for stateless components that run in small, lean containers, and that’s why applications
    built for Kubernetes typically use a distributed architecture, breaking down functionality
    across many pieces, which can be individually updated and scaled.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSet使得扩展你的应用变得极其简单：你只需通过在规范中更改副本的数量，就能在几秒钟内进行扩展或缩减。这对于运行在小巧、精简容器中的无状态组件来说非常完美，这也是为什么为Kubernetes构建的应用程序通常使用分布式架构，将功能分解成许多部分，这些部分可以单独更新和扩展。
- en: Deployments add a useful management layer on top of ReplicaSets. Now that we
    know how they work, we won’t be using ReplicaSets directly anymore—Deployments
    should be your first choice for defining applications. We won’t explore all the
    features of Deployments until we get to application upgrades and rollbacks in
    chapter 9, but it’s useful to understand exactly what the extra abstraction gives
    you. Figure 6.6 shows this.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Deployment在ReplicaSet之上添加了一个有用的管理层。现在我们知道了它们是如何工作的，我们就不会再直接使用ReplicaSet了——Deployment应该是定义应用程序的首选。我们不会在第九章中探索Deployment的所有功能，直到我们讨论应用程序的升级和回滚，但了解额外的抽象能给你带来什么是有用的。图6.6展示了这一点。
- en: '![](../Images/6-6.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-6.jpg)'
- en: Figure 6.6 Zero is a valid number of desired replicas; Deployments scale down
    old ReplicaSets to zero.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6显示，零是期望副本的有效数量；Deployment将旧的ReplicaSet缩放到零。
- en: A Deployment is a controller for ReplicaSets, and to run at scale, you include
    the same `replicas` field in the Deployment spec, and that is passed to the ReplicaSet.
    Listing 6.2 shows the abbreviated YAML for the Pi web application, which explicitly
    sets two replicas.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Deployment是ReplicaSet的控制器，为了实现大规模运行，你需要在Deployment规范中包含相同的`replicas`字段，并将其传递给ReplicaSet。列表6.2显示了Pi网络应用程序的缩写YAML，它明确设置了两个副本。
- en: Listing 6.2 web.yaml, a Deployment to run multiple replicas
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.2 web.yaml，一个运行多个副本的Deployment
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The label selector for the Deployment needs to match the labels defined in the
    Pod template, and those labels are used to express the chain of ownership from
    Pod to ReplicaSet to Deployment. When you scale a Deployment, it updates the existing
    ReplicaSet to set the new number of replicas, but if you change the Pod spec in
    the Deployment, it replaces the ReplicaSet and scales the previous one down to
    zero. That gives the Deployment a lot of control over how it manages the update
    and how it deals with any problems.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 部署的标签选择器需要与 Pod 模板中定义的标签匹配，这些标签用于表达从 Pod 到 ReplicaSet 再到 Deployment 的所有权链。当你扩展
    Deployment 时，它会更新现有的 ReplicaSet 以设置新的副本数量，但如果你在 Deployment 中更改 Pod 规范，它会替换 ReplicaSet
    并将之前的副本数降至零。这使得 Deployment 在管理更新和处理任何问题时拥有很大的控制权。
- en: Try it now Create a Deployment and Service for the Pi web application, and make
    some updates to see how the ReplicaSets are managed.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：为 Pi 网络应用程序创建一个 Deployment 和 Service，并进行一些更新以查看如何管理 ReplicaSet。
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This exercise shows that the ReplicaSet is still the scale mechanism: when
    you increase or decrease the number of replicas in your Deployment, it just updates
    the ReplicaSet. The Deployment is the, well, deployment mechanism, and it manages
    application updates through multiple ReplicaSets. My output, which appears in
    figure 6.7, shows how the Deployment waits for the new ReplicaSet to be fully
    operational before completely scaling down the old one.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习表明 ReplicaSet 仍然是扩展机制：当你增加或减少 Deployment 中的副本数量时，它只是更新 ReplicaSet。Deployment
    是，嗯，部署机制，它通过多个 ReplicaSet 管理应用程序更新。我的输出，如图 6.7 所示，显示了 Deployment 在完全缩减旧的副本之前等待新的
    ReplicaSet 完全运行。
- en: '![](../Images/6-7.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-7.jpg)'
- en: Figure 6.7 Deployments manage ReplicaSets to keep the desired number of Pods
    available during updates.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 展示了 Deployment 如何管理 ReplicaSet 以在更新期间保持所需数量的 Pod。
- en: You can use the kubectl `scale` command as a shortcut for scaling controllers.
    You should use it sparingly because it’s an imperative way to work, and it’s much
    better to use declarative YAML files, so that the state of your apps in production
    always exactly matches the spec stored in source control. But if your app is underperforming
    and the automated deployment takes 90 seconds, it’s a quick way to scale—as long
    as you remember to update the YAML file, too.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 kubectl `scale` 命令作为扩展控制器的快捷方式。你应该谨慎使用它，因为它是一种命令式的工作方式，而使用声明式的 YAML 文件会更好，这样你的应用程序在生产中的状态总是与源控制中存储的规范完全匹配。但如果你的应用程序性能不佳，自动部署需要
    90 秒，那么这是一个快速扩展的方法——只要记得更新 YAML 文件。
- en: Try it now Scale up the Pi application using kubectl directly, and then see
    what happens with the ReplicaSets when another full deployment happens.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：使用 kubectl 直接扩展 Pi 应用程序，然后看看在再次进行完整部署时 ReplicaSet 会发生什么。
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You’ll see two things when you apply the updated YAML: the app scales back
    down to three replicas, and the Deployment does that by scaling the new ReplicaSet
    down to zero Pods and scaling the old ReplicaSet back up to three Pods. Figure
    6.8 shows that the updated Deployment results in three new Pods being created.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当你应用更新的 YAML 文件时，你会看到两件事：应用程序的副本数缩减到三个，Deployment 通过将新的 ReplicaSet 的 Pod 数量缩减到零并恢复旧的
    ReplicaSet 到三个 Pod 来实现这一点。图 6.8 展示了更新后的 Deployment 导致创建了三个新的 Pod。
- en: '![](../Images/6-8.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-8.jpg)'
- en: Figure 6.8 Deployments know the spec for their ReplicaSets and can roll back
    by scaling an old ReplicaSet.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 展示了 Deployment 了解其 ReplicaSet 的规范，并且可以通过扩展旧的 ReplicaSet 来回滚。
- en: It shouldn’t be a surprise that the Deployment update overwrote the manual scale
    level; the YAML definition is the desired state, and Kubernetes does not attempt
    to retain any part of the current spec if the two differ. It might be more of
    a surprise that the Deployment reused the old ReplicaSet instead of creating a
    new one, but that’s a more efficient way for Kubernetes to work, and it’s possible
    because of more labels.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Deployment 更新覆盖了手动缩放级别并不令人惊讶；YAML 定义是期望状态，如果两者不同，Kubernetes 不会尝试保留当前规范中的任何部分。更令人惊讶的是，Deployment
    重新使用了旧的 ReplicaSet 而不是创建一个新的，但这是一种更高效的工作方式，这要归功于更多的标签。
- en: Pods created from Deployments have a generated name that looks random but actually
    isn’t. The Pod name contains a hash of the template in the Pod spec for the Deployment,
    so if you make a change to the spec that matches a previous Deployment, then it
    will have the same template hash as a scaled-down ReplicaSet, and the Deployment
    can find that ReplicaSet and scale it up again to effect the change. The Pod template
    hash is stored in a label.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从Deployment创建的Pod有一个看起来随机的生成名称，但实际上并非如此。Pod名称包含Deployment的Pod规范中的模板哈希，因此如果你对规范进行更改，它与之前的Deployment匹配，那么它将具有与缩小的ReplicaSet相同的模板哈希，Deployment可以找到该ReplicaSet并将其再次扩展以实施更改。Pod模板哈希存储在标签中。
- en: Try it now Check out the labels for the Pi Pods and ReplicaSets to see the template
    hash.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：检查Pi Pods和ReplicaSets的标签以查看模板哈希。
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Figure 6.9 shows that the template hash is included in the object name, but
    this is just for convenience—Kubernetes uses the labels for management.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9显示模板哈希包含在对象名称中，但这只是为了方便——Kubernetes使用标签进行管理。
- en: '![](../Images/6-9.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片6-9](../Images/6-9.jpg)'
- en: Figure 6.9 Object names generated by Kubernetes aren’t just random—they include
    the template hash.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 Kubernetes生成的对象名称不仅仅是随机的——它们包括模板哈希。
- en: Knowing the internals of how a Deployment is related to its Pods will help you
    understand how changes are rolled out and clear up any confusion when you see
    lots of ReplicaSets with desired Pod counts of zero. But the interaction between
    the compute layer in the Pods and the network layer in the Services works in the
    same way.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 了解部署与其Pods之间的内部关系将有助于你理解变更是如何分阶段实施的，并在你看到许多ReplicaSet具有零期望Pod计数时消除任何混淆。但Pod中的计算层与Service中的网络层之间的交互方式是相同的。
- en: In a typical distributed application, you’ll have different scale requirements
    for each component, and you’ll make use of Services to achieve multiple layers
    of load balancing between them. The Pi application we’ve deployed so far has only
    a ClusterIP Service—it’s not a public-facing component. The public component is
    a proxy (actually, it’s a reverse proxy because it handles incoming traffic rather
    than outgoing traffic), and that uses a LoadBalancer Service. We can run both
    the web component and the proxy at scale and achieve load balancing from the client
    to the proxy Pods and from the proxy to the application Pods.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个典型的分布式应用中，你将为每个组件有不同的扩展需求，并且你会使用Service在它们之间实现多层的负载均衡。我们迄今为止部署的Pi应用只有一个ClusterIP
    Service——它不是一个面向公众的组件。公众组件是一个代理（实际上，它是一个反向代理，因为它处理传入流量而不是传出流量），它使用一个LoadBalancer
    Service。我们可以以规模运行Web组件和代理，并从客户端到代理Pod以及从代理到应用Pod实现负载均衡。
- en: Try it now Create the proxy Deployment, which runs with two replicas, along
    with a Service and ConfigMap, which sets up the integration with the Pi web app.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：创建一个运行两个副本的代理Deployment，以及一个Service和ConfigMap，以设置与Pi网络应用的集成。
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If you open the developer tools in your browser and look at the network requests,
    you can find the response headers sent by the proxy. These include the hostname
    of the proxy server—which is actually the Pod name—and the web page itself includes
    the name of the web application Pod that generated the response. My output, which
    appears in figure 6.10, shows a response that came from the proxy cache.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打开浏览器中的开发者工具并查看网络请求，你可以找到代理发送的响应头。这些包括代理服务器的主机名——实际上是Pod名称——以及网页本身包含生成响应的Web应用Pod的名称。我的输出，如图6.10所示，显示了一个来自代理缓存的响应。
- en: '![](../Images/6-10.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片6-10](../Images/6-10.jpg)'
- en: Figure 6.10 The Pi responses include the name of the Pod that sent them, so
    you can see the load balancing at work.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 Pi响应包括发送它们的Pod名称，因此你可以看到负载均衡的工作情况。
- en: 'This configuration is a simple one, which makes it easy to scale. The Pod spec
    for the proxy uses two volumes: a ConfigMap to load the proxy configuration file
    and an `EmptyDir` to store the cached responses. ConfigMaps are read-only, so
    one ConfigMap can be shared by all the proxy Pods. `EmptyDir` volumes are writable,
    but they’re unique to the Pod, so each proxy gets its own volume to use for cache
    files. Figure 6.11 shows the setup.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置很简单，这使得它易于扩展。代理的Pod规范使用两个卷：一个ConfigMap用于加载代理配置文件，一个`EmptyDir`用于存储缓存的响应。ConfigMap是只读的，因此一个ConfigMap可以被所有代理Pod共享。`EmptyDir`卷是可写的，但它们对Pod是唯一的，因此每个代理都得到自己的卷来用于缓存文件。图6.11显示了设置。
- en: '![](../Images/6-11.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片6-11](../Images/6-11.jpg)'
- en: Figure 6.11 Running Pods at scale—some types of volume can be shared, whereas
    others are unique to the Pod.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 在规模上运行 Pods——某些类型的卷可以共享，而其他类型的卷则是 Pod 独有的。
- en: This architecture presents a problem, which you’ll see if you request Pi to
    a high number of decimal places and keep refreshing the browser. The first request
    will be slow because it is computed by the web app; subsequent responses will
    be fast, because they come from the proxy cache, but soon your request will go
    to a different proxy Pod that doesn’t have that response in its cache, so the
    page will load slowly again.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构会带来一个问题，如果你请求 Pi 的高精度数字并不断刷新浏览器，你会看到这个问题。第一个请求会慢，因为它是由网络应用程序计算的；后续的响应会快，因为它们来自代理缓存，但很快你的请求就会转到没有该响应的缓存的不同代理
    Pod，所以页面会再次加载缓慢。
- en: It would be nice to fix this by using shared storage, so every proxy Pod had
    access to the same cache. Doing so will bring us back to the tricky area of distributed
    storage that we thought we’d left behind in chapter 5, but let’s start with a
    simple approach and see where it gets us.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用共享存储来修复这个问题，那么每个代理 Pod 都可以访问相同的缓存会很好。这样做将把我们带回到我们在第5章中认为已经留下的分布式存储的棘手领域，但让我们先从一个简单的方法开始，看看它能带我们走到哪里。
- en: Try it now Deploy an update to the proxy spec, which uses a `HostPath` volume
    for cache files instead of an `EmptyDir`. Multiple Pods on the same node will
    use the same volume, which means they’ll have a shared proxy cache.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：部署一个更新到代理规范，它使用 `HostPath` 卷来存储缓存文件，而不是 `EmptyDir`。同一节点上的多个 Pod 将使用相同的卷，这意味着它们将共享代理缓存。
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now you should be able to refresh away to your heart’s content, and responses
    will always come from the cache, no matter which proxy Pod you are directed to.
    Figure 6.12 shows all my proxy Pods responding to requests, which are shared between
    them by the Service.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该能够随心所欲地刷新，而且无论你被引导到哪个代理 Pod，响应总是来自缓存。图6.12显示了所有我的代理 Pods 正在响应请求，这些请求通过
    Service 在它们之间共享。
- en: '![](../Images/6-12.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图6-12](../Images/6-12.jpg)'
- en: Figure 6.12 At scale, you can see all the Pod logs with kubectl, using a label
    selector.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 在规模上，你可以使用 kubectl 和标签选择器查看所有 Pod 的日志。
- en: For most stateful applications, this approach wouldn’t work. Apps that write
    data tend to assume they have exclusive access to the files, and if another instance
    of the same app tries to use the same file location, you’d get unexpected but
    disappointing results—like the app crashing or the data being corrupted. The reverse
    proxy I’m using is called Nginx; it’s unusually lenient here, and it will happily
    share its cache directory with other instances of itself.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数有状态的应用程序，这种方法是不可行的。写入数据的应用程序往往假设它们对文件有独占访问权，如果相同应用程序的另一个实例尝试使用相同的文件位置，你可能会得到意外但令人失望的结果——比如应用程序崩溃或数据损坏。我使用的反向代理是
    Nginx；在这里它非常宽容，并且愿意与其他实例共享其缓存目录。
- en: If your apps need scale and storage, you have a couple of other options for
    using different types of controller. In the rest of this chapter, we’ll look at
    the DaemonSet; the final type is the StatefulSet, which gets complicated quickly,
    and we’ll come to it in chapter 8 where it gets most of the chapter to itself.
    DaemonSets and StatefulSets are both Pod controllers, and although you’ll use
    them a lot less frequently than Deployments, you need to know what you can do
    with them because they enable some powerful patterns.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的应用需要扩展和存储，你可以选择使用不同类型的控制器。在本章的剩余部分，我们将探讨 DaemonSet；最后一种类型是 StatefulSet，它很快就会变得复杂，我们将在第8章中详细讨论，那时它将占据大部分章节。DaemonSet
    和 StatefulSet 都是 Pod 控制器，尽管你使用它们的频率会比 Deployments 低得多，但你仍需要了解你可以用它们做什么，因为它们可以启用一些强大的模式。
- en: 6.3 Scaling for high availability with DaemonSets
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 使用 DaemonSet 实现高可用性扩展
- en: The DaemonSet takes its name from the Linux daemon, which is usually a system
    process that runs constantly as a single instance in the background (the equivalent
    of a Windows Service in the Windows world). In Kubernetes, the DaemonSet runs
    a single replica of a Pod on every node in the cluster, or on a subset of nodes,
    if you add a selector in the spec.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: DaemonSet 的名字来源于 Linux 守护进程，它通常是一个在后台以单个实例持续运行的系统进程（在 Windows 世界中相当于 Windows
    服务）。在 Kubernetes 中，DaemonSet 在集群的每个节点上运行一个 Pod 的单个副本，或者如果你在规范中添加了一个选择器，它可以在节点的一个子集上运行。
- en: DaemonSets are common for infrastructure-level concerns, where you might want
    to grab information from every node and send it on to a central collector. A Pod
    runs on each node, grabbing just the data for that node. You don’t need to worry
    about any resource conflicts, because there will be only one Pod on the node.
    We’ll use DaemonSets later in this book to collect logs from Pods, and metrics
    about the node’s activity.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 守护集在基础设施级别的关注点中很常见，您可能希望从每个节点获取信息并将其发送到中央收集器。每个节点上运行一个Pod，仅获取该节点的数据。您不需要担心任何资源冲突，因为节点上只有一个Pod。我们将在本书的后面使用守护集从Pod收集日志以及关于节点活动的指标。
- en: 'You can also use them in your own designs when you want high availability without
    the load requirements for many replicas on each node. A reverse proxy is a good
    example: a single Nginx Pod can handle many thousands of concurrent connections,
    so you don’t necessarily need a lot of them, but you may want to be sure there’s
    one running on every node, so a local Pod can respond wherever the traffic lands.
    Listing 6.3 shows the abbreviated YAML for a DaemonSet—it looks much like the
    other controllers but without the replica count.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当您想要高可用性而不需要每个节点上许多副本的负载要求时，您也可以在自己的设计中使用它们。反向代理是一个很好的例子：单个Nginx Pod可以处理成千上万的并发连接，因此您不一定需要很多，但您可能想确保每个节点上都有一个运行，以便本地Pod可以在流量到达的地方响应。列表6.3显示了守护集的缩写YAML——它看起来与其他控制器很相似，但没有副本数量。
- en: Listing 6.3 nginx-ds.yaml, a DaemonSet for the proxy component
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.3 nginx-ds.yaml，代理组件的守护集
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This spec for the proxy still uses a `HostPath` volume. That means each Pod
    will have its own proxy cache, so we don’t get ultimate performance from a shared
    cache. This approach would work for other stateful apps, which are fussier than
    Nginx, because there’s no issue with multiple instances using the same data files.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 此代理的规范仍然使用`HostPath`卷。这意味着每个Pod都将有自己的代理缓存，因此我们无法从共享缓存中获得最佳性能。这种方法适用于其他比Nginx更挑剔的有状态应用程序，因为没有多个实例使用相同的数据文件的问题。
- en: Try it now You can’t convert from one type of controller to another, but we
    can make the change from Deployment to DaemonSet without breaking the app.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试手动删除。您不能从一种控制器类型转换为另一种类型，但我们可以在不破坏应用程序的情况下将更改从部署更改为守护集。
- en: '[PRE11]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Figure 6.13 shows my output. Creating the DaemonSet before removing the Deployment
    means there are always Pods available to receive requests from the Service. Deleting
    the Deployment first would make the app unavailable until the DaemonSet started.
    If you check the HTTP response headers, you should also see that your request
    came from the proxy cache, because the new DaemonSet Pod uses the same `HostPath`
    volume as the Deployment Pods.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13显示了我的输出。在删除部署之前创建守护集意味着始终有Pod可用以接收来自服务的请求。如果首先删除部署，则应用程序将不可用，直到守护集启动。如果您检查HTTP响应头，您也应该看到您的请求来自代理缓存，因为新的守护集Pod使用与部署Pod相同的`HostPath`卷。
- en: '![](../Images/6-13.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-13.jpg)'
- en: Figure 6.13 You need to plan the order of the deployment for a big change to
    keep your app online.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 在进行重大更改时，您需要规划部署的顺序以保持您的应用程序在线。
- en: I’m using a single-node cluster, so my DaemonSet runs a single Pod; with more
    nodes, I’d have one Pod on each node. The control loop watches for nodes joining
    the cluster, and any new nodes will be scheduled to start a replica Pod as soon
    as they join. The controller also watches the Pod status, so if a Pod is removed,
    then a replacement starts up.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在使用单节点集群，因此我的守护集运行一个Pod；如果有更多节点，我将在每个节点上有一个Pod。控制循环监视加入集群的节点，任何新节点都会在加入后立即调度启动一个副本Pod。控制器还监视Pod状态，如果Pod被删除，则会启动一个替换。
- en: Try it now Manually delete the proxy Pod. The DaemonSet will start a replacement.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试手动删除代理Pod。守护集将启动一个替换。
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If you refresh your browser while the Pod is being deleted, you’ll see it doesn’t
    respond until the DaemonSet has started a replacement. This is because you’re
    using a single-node lab cluster. Services send traffic only to running Pods, so
    in a multinode environment, the request would go to a node that still had a healthy
    Pod. Figure 6.14 shows my output.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当Pod正在被删除时，如果您刷新浏览器，您会看到它不会响应，直到守护集启动了一个替换。这是因为您正在使用单节点实验室集群。服务只向正在运行的Pod发送流量，所以在多节点环境中，请求会发送到仍然有健康Pod的节点。图6.14显示了我的输出。
- en: '![](../Images/6-14.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-14.jpg)'
- en: Figure 6.14 DaemonSets watch nodes and Pods to ensure the desired replica count
    is always met.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 守护集监控节点和Pod，以确保始终满足所需的副本数量。
- en: Situations where you need a DaemonSet are often a bit more nuanced than just
    wanting to run a Pod on every node. In this proxy example, your production cluster
    might have only a subset of nodes that can receive traffic from the internet,
    so you’d want to run proxy Pods only on those nodes. You can achieve that with
    labels, adding whatever arbitrary label you’d like to identify your nodes and
    then selecting that label in the Pod spec. Listing 6.4 shows this with a `nodeSelector`
    field.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 需要使用 DaemonSet 的情况通常比仅仅想在每个节点上运行 Pod 要复杂一些。在这个代理示例中，你的生产集群可能只有一小部分节点可以接收来自互联网的流量，因此你只想在这些节点上运行代理
    Pod。你可以通过标签来实现这一点，添加任何你想要的任意标签来识别你的节点，然后在 Pod 规范中选择该标签。列表 6.4 使用 `nodeSelector`
    字段展示了这一点。
- en: Listing 6.4 nginx-ds-nodeSelector.yaml, a DaemonSet with node selection
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.4 nginx-ds-nodeSelector.yaml，具有节点选择的 DaemonSet
- en: '[PRE13]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The DaemonSet controller doesn’t just watch to see nodes joining the cluster;
    it looks at all nodes to see if they match the requirements in the Pod spec. When
    you deploy this change, you’re telling the DaemonSet to run on only nodes that
    have the label `kiamol` set to the value of `ch06`. There will be no matching
    nodes in your cluster, so the DaemonSet will scale down to zero.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: DaemonSet 控制器不仅监视节点加入集群，它还会查看所有节点，以查看它们是否与 Pod 规范中的要求匹配。当你部署这个更改时，你是在告诉 DaemonSet
    只在设置了标签 `kiamol` 为 `ch06` 值的节点上运行。在你的集群中将没有匹配的节点，因此 DaemonSet 将缩放到零。
- en: Try it now Update the DaemonSet to include the node selector from listing 6.4\.
    Now there are no nodes that match the requirements, so the existing Pod will be
    removed. Then label a node, and a new Pod will be scheduled.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 更新 DaemonSet 以包括列表 6.4 中的节点选择器。现在没有节点符合要求，因此现有的 Pod 将被删除。然后标记一个节点，将调度一个新的
    Pod。
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You can see the control loop for the DaemonSet in action in figure 6.15\. When
    the node selector is applied, no nodes meet the selector, so the desired replica
    count for the DaemonSet drops to zero. The existing Pod is one too many for the
    desired count, so it is removed. Then, when the node is labeled, there’s a match
    for the selector, and the desired count increases to one, so a new Pod is created.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图 6.15 中看到 DaemonSet 的控制循环正在起作用。当应用节点选择器时，没有节点符合选择器，因此 DaemonSet 的期望副本计数降至零。现有的
    Pod 对于期望计数来说过多，因此它被删除。然后，当节点被标记时，有一个匹配选择器的节点，因此期望计数增加到一，因此创建一个新的 Pod。
- en: '![](../Images/6-15.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片 6-15](../Images/6-15.jpg)'
- en: Figure 6.15 DaemonSets watch nodes and their labels, as well as the current
    Pod status.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15 DaemonSet 监视节点及其标签，以及当前的 Pod 状态。
- en: DaemonSets have a different control loop from ReplicaSets because their logic
    needs to watch node activity as well as Pod counts, but fundamentally, they are
    both controllers that manage Pods. All controllers are responsible for the life
    cycle of their managed objects, but the links can be broken. We’ll use the DaemonSet
    in one more exercise to show how Pods can be set free from their controllers.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: DaemonSet 与 ReplicaSet 有不同的控制循环，因为它们的逻辑需要监视节点活动以及 Pod 数量，但本质上，它们都是管理 Pod 的控制器。所有控制器都负责其管理对象的生命周期，但链接可能会断开。我们将在下一个练习中使用
    DaemonSet 来展示如何使 Pod 从其控制器中解放出来。
- en: Try it now Kubectl has a `cascade` option on the `delete` command, which you
    can use to delete a controller without deleting its managed objects. Doing so
    leaves orphaned Pods behind, which can be adopted by another controller if they
    are a match for their previous owner.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 Kubectl 的 `delete` 命令有一个 `cascade` 选项，你可以使用它来删除控制器而不删除其管理对象。这样做会在后面留下孤儿
    Pod，如果它们与之前所有者的匹配，则可以被另一个控制器收养。
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Figure 6.16 shows the same Pod survives through the DaemonSet being deleted
    and recreated. The new DaemonSet requires a single Pod, and the existing Pod is
    a match for its template, so it becomes the manager of the Pod. When this DaemonSet
    is deleted, the Pod is removed too.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.16 显示了相同的 Pod 在 DaemonSet 被删除和重新创建后仍然存活。新的 DaemonSet 需要单个 Pod，现有的 Pod 与其模板匹配，因此它成为
    Pod 的管理者。当这个 DaemonSet 被删除时，Pod 也会被删除。
- en: '![](../Images/6-16.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片 6-16](../Images/6-16.jpg)'
- en: Figure 6.16 Orphaned Pods have lost their controller, so they're not part of
    a highly available set anymore.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.16 孤儿 Pod 已经失去了其控制器，因此它们不再是高可用集的一部分。
- en: Putting a halt on cascading deletes is one of those features you’re going to
    use rarely, but you’ll be very glad you knew about it when you do need it. In
    this scenario, you might be happy with all your existing Pods but have some maintenance
    tasks coming up on the nodes. Rather than have the DaemonSet adding and removing
    Pods while you work on the nodes, you could delete it and reinstate it after the
    maintenance is done.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 禁止级联删除是那些你很少会使用但当你需要时会非常高兴知道的功能之一。在这种情况下，你可能对现有的所有Pod都感到满意，但节点上即将有一些维护任务。与其在处理节点时让DaemonSet添加和删除Pod，你可以在维护完成后删除它并重新启用它。
- en: The example we’ve used here for DaemonSets is about high availability, but it’s
    limited to certain types of application—where you want multiple instances and
    it’s acceptable for each instance to have its own independent data store. Other
    applications where you need high availability might need to keep data synchronized
    between instances, and for those, you can use StatefulSets. Don’t skip on to chapter
    8 yet, though, because you’ll learn some neat patterns in chapter 7 that help
    with stateful apps, too.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用的DaemonSets示例是关于高可用性的，但它仅限于某些类型的应用程序——你想要多个实例，并且可以接受每个实例都有自己的独立数据存储。其他需要高可用性的应用程序可能需要在实例之间同步数据，对于这些应用程序，你可以使用StatefulSets。不过，现在不要跳到第8章，因为在第7章中你将学习一些有助于有状态应用程序的巧妙模式。
- en: StatefulSets, DaemonSets, ReplicaSets, and Deployments are the tools you use
    to model your apps, and they should give you enough flexibility to run pretty
    much anything in Kubernetes. We’ll finish this chapter with a quick look at how
    Kubernetes actually manages objects that own other objects, and then we’ll review
    how far we’ve come in this first section of the book.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSets、DaemonSets、ReplicaSets和Deployments是您用来建模应用程序的工具，它们应该为您提供足够的灵活性，在Kubernetes中运行几乎任何东西。我们将以快速查看Kubernetes实际上如何管理拥有其他对象的对象来结束本章，然后我们将回顾我们在本书第一部分的进展情况。
- en: 6.4 Understanding object ownership in Kubernetes
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 理解Kubernetes中的对象所有权
- en: 'Controllers use a label selector to find objects that they manage, and the
    objects themselves keep a record of their owner in a metadata field. When you
    delete a controller, its managed objects still exist but not for long. Kubernetes
    runs a garbage collector process that looks for objects whose owner has been deleted,
    and it deletes them, too. Object ownership can model a hierarchy: Pods are owned
    by ReplicaSets, and ReplicaSets are owned by Deployments.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器使用标签选择器来查找它们管理的对象，而对象本身则在元数据字段中保留其所有者的记录。当你删除一个控制器时，它管理的对象仍然存在，但不会持续太久。Kubernetes运行一个垃圾收集器进程，寻找所有者已被删除的对象，并将它们也删除。对象所有权可以模拟一个层次结构：Pods属于ReplicaSets，而ReplicaSets属于Deployments。
- en: Try it now Look at the owner reference in the metadata fields for all Pods and
    ReplicaSets.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看。查看所有Pod和ReplicaSets的元数据字段中的所有者引用。
- en: '[PRE16]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Figure 6.17 shows my output, where all of my Pods are owned by some other object,
    and all but one of my ReplicaSets are owned by a Deployment.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17显示了我的输出，其中所有Pod都由某个其他对象拥有，而除了一个之外的所有ReplicaSets都由一个Deployment拥有。
- en: '![](../Images/6-17.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-17.jpg)'
- en: Figure 6.17 Objects know who their owners are—you can find this in the object
    metadata.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17 对象知道它们的拥有者是谁——你可以在对象元数据中找到这一点。
- en: Kubernetes does a good job of managing relationships, but you need to remember
    that controllers track their dependents using the label selector alone, so if
    you fiddle with labels, you could break that relationship. The default delete
    behavior is what you want most of the time, but you can stop cascading deletes
    using kubectl and delete only the controller—that removes the owner reference
    in the metadata for the dependents, so they don’t get picked up by the garbage
    collector.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes在管理关系方面做得很好，但你需要记住，控制器仅使用标签选择器跟踪其依赖项，所以如果你篡改标签，可能会破坏这种关系。默认的删除行为是大多数时候你想要的，但你可以使用kubectl停止级联删除，只删除控制器——这会从依赖项的元数据中删除所有者引用，因此它们不会被垃圾收集器选中。
- en: We’re going to finish up with a look at the architecture for the latest version
    of the Pi app, which we’ve deployed in this chapter. Figure 6.18 shows it in all
    its glory.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将结束对最新版本的Pi应用程序架构的探讨，该应用程序在本章中已部署。图6.18展示了它的全部辉煌。
- en: '![](../Images/6-18.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-18.jpg)'
- en: 'Figure 6.18 The Pi application: no annotations necessary—the diagram should
    be crystal clear.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18 Pi应用程序：无需注释——图表应该非常清晰。
- en: '*Quite a lot* is going on in this diagram: it’s a simple app, but the deployment
    is complex because it uses lots of Kubernetes features to get high availability,
    scale, and flexibility. By now you should be comfortable with all those Kubernetes
    resources, and you should understand how they fit together and when to use them.
    Around 150 lines of YAML define the application, but those YAML files are all
    you need to run this app on your laptop or on a 50-node cluster in the cloud.
    When someone new joins the project, if they have solid Kubernetes experience—or
    if they’ve read the first six chapters of this book—they can be productive straight
    away.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*相当多*的事情在这个图中正在进行：它是一个简单的应用程序，但由于它使用了大量的Kubernetes功能来实现高可用性、可扩展性和灵活性，所以部署很复杂。到现在你应该对所有的这些Kubernetes资源都很熟悉，你应该了解它们是如何配合在一起以及何时使用它们。大约150行的YAML定义了应用程序，但那些YAML文件就是你需要在你的笔记本电脑上或在云中的50节点集群上运行此应用程序所需的所有内容。当新成员加入项目时，如果他们有扎实的Kubernetes经验——或者如果他们已经阅读了这本书的前六章——他们可以立即开始工作。'
- en: That’s all for the first section. My apologies if you had to take a few extended
    lunchtimes this week, but now you have all the fundamentals of Kubernetes, with
    best practices built in. All we need to do is tidy up before you attempt the lab.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分就到这里。如果你这周不得不延长午餐时间，我感到很抱歉，但现在你已经掌握了Kubernetes的所有基础知识，其中包含了最佳实践。我们只需要在你尝试实验室之前整理一下。
- en: Try it now All the top-level objects in this chapter had a `kiamol` label applied.
    Now that you understand cascading deletes, you’ll know that when you delete all
    those objects, all their dependents get deleted, too.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试吧！本章所有顶级对象都应用了`kiamol`标签。现在你了解了级联删除，你会知道当你删除所有这些对象时，它们的依赖项也会被删除。
- en: '[PRE17]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 6.5 Lab
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 实验室
- en: Kubernetes has changed a lot over the last few years. The controllers we’ve
    used in this chapter are the recommended ones, but there have been alternatives
    in the past. Your job in this lab is to take an app spec that uses some older
    approaches and update it to use the controllers you’ve learned about.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 过去几年中，Kubernetes发生了很大变化。本章中我们使用的控制器是推荐的，但过去有过替代方案。在这个实验室中，你的任务是取一个使用一些较旧方法的app规范，并将其更新为使用你学到的控制器。
- en: Start by deploying the app in `ch06/lab/numbers`—it’s the random-number app
    from chapter 3 but with a strange configuration. And it’s broken.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，在`ch06/lab/numbers`目录下部署应用程序——它是第3章中的随机数应用程序，但配置很奇怪。而且它坏了。
- en: You need to update the web component to use a controller that supports high
    load. We’ll want to run dozens of these in production.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要更新Web组件以使用支持高负载的控制器。我们希望在生产中运行数十个这样的实例。
- en: The API needs to be updated, too. It needs to be replicated for high availability,
    but the app uses a hardware random-number generator attached to the server, which
    can be used by only one Pod at a time. Nodes with the right hardware have the
    label `rng=hw` (you’ll need to simulate that in your cluster).
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API也需要更新。它需要复制以实现高可用性，但应用程序使用连接到服务器的硬件随机数生成器，一次只能由一个Pod使用。具有正确硬件的节点具有标签`rng=hw`（你需要在你的集群中模拟这一点）。
- en: This isn’t a clean upgrade, so you need to plan your deployment to make sure
    there’s no downtime for the web app.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这不是一个干净的升级，所以你需要规划你的部署以确保Web应用没有停机时间。
- en: 'Sounds scary, but you shouldn’t find this too bad. My solution is on GitHub
    for you to check: [https://github.com/sixeyed/kiamol/blob/master/ch06/lab/README.md](https://github.com/sixeyed/kiamol/blob/master/ch06/lab/README.md).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来很可怕，但你不必觉得这太糟糕。我的解决方案在GitHub上供你检查：[https://github.com/sixeyed/kiamol/blob/master/ch06/lab/README.md](https://github.com/sixeyed/kiamol/blob/master/ch06/lab/README.md)。
