- en: 3 Convolutional neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 卷积神经网络
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: How tensors represent spatial data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量如何表示空间数据
- en: Defining convolutions and their uses
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义卷积及其用途
- en: Building and training a convolutional neural network (CNN)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和训练卷积神经网络（CNN）
- en: Adding pooling to make CNNs more robust
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过添加池化使卷积神经网络更鲁棒
- en: Augmenting image data to improve accuracy
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过增强图像数据来提高准确性
- en: Convolutional neural networks (CNNs) revitalized the field of neural networks
    while simultaneously ushering in a new branding of deep learning starting in 2011
    and 2012\. CNNs are still at the heart of many of the most successful applications
    of deep learning, including self-driving cars, speech recognition systems used
    by smart devices, and optical character recognition. All of this stems from the
    fact that *convolutions* are powerful yet simple tools that help us encode information
    about the problem into the design of our network architecture. Instead of focusing
    on feature engineering, we spend more time engineering the *architectures* of
    our networks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）在2011年和2012年同时复兴了神经网络领域，并开启了深度学习的新时代。CNNs仍然是许多深度学习最成功应用的核心，包括自动驾驶汽车、智能设备使用的语音识别系统和光学字符识别。这一切都源于卷积是一种强大而简单的工具，它帮助我们将关于问题的信息编码到网络架构的设计中。我们不是专注于特征工程，而是更多地关注我们网络的架构设计。
- en: The success of convolutions comes from their ability to learn spatial patterns,
    which has made them the default method to use for any data resembling an image.
    When you apply a convolution to an image, you can learn to detect simple patterns
    like horizontal or vertical lines, changes in color, or grid patterns. When you
    stack convolutions in layers, they begin to recognize more complex patterns, building
    upon the simpler convolutions that came before.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积成功的原因在于它们能够学习空间模式，这使得它们成为处理任何类似图像的数据的默认方法。当你对一个图像应用卷积时，你可以学会检测简单的模式，如水平或垂直线、颜色变化或网格模式。当你将卷积堆叠在层中时，它们开始识别更复杂的模式，建立在之前更简单的卷积之上。
- en: Our goal in this chapter is to teach you all the basics needed to build your
    own CNNs for new image-classification problems. First, we discuss how images are
    represented to a neural network. That images are 2D is an important structure
    or meaning we will encode into the specific way we organize data in our tensors.
    You should always care about the structure of your data because picking the right
    architecture to match the structure is the best way to improve the accuracy of
    your model. Next, we remove the mystery of what a convolution is, show how convolutions
    can detect simple patterns, and explain why they’re a good approach for data structured
    like an image. Then, we’ll create a convolutional layer, which can act as a replacement
    for the `nn.Linear` layer we used in the previous chapter. Finally, we build some
    CNNs and discuss a few additional tricks to improve their accuracy.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是教会你构建自己的卷积神经网络（CNN）所需的所有基础知识，以解决新的图像分类问题。首先，我们讨论了图像是如何被表示到神经网络中的。图像是二维的这一点是一个重要的结构或意义，我们将将其编码到我们组织张量数据的具体方式中。你应该始终关注你数据的结构，因为选择与结构相匹配的正确架构是提高模型准确性的最佳方式。接下来，我们将揭示卷积的神秘面纱，展示卷积如何检测简单模式，并解释为什么它们是处理图像结构数据的良好方法。然后，我们将创建一个卷积层，它可以作为前一章中使用的`nn.Linear`层的替代品。最后，我们将构建一些CNN，并讨论一些提高其准确性的额外技巧。
- en: 3.1 Spatial structural prior beliefs
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 空间结构先验信念
- en: As of now, you know how to build and train a very simple neural network. What
    you have learned applies to any kind of tabular (also called *columnar*) data,
    where your data and features may be organized in a spreadsheet. However, other
    algorithms (e.g., random forests and XGBoost) are usually better for such data.
    If all you have is columnar data, you probably do not want to use a neural network.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经知道如何构建和训练一个非常简单的神经网络。你所学的知识适用于任何类型的表格数据（也称为*列式*数据），其中你的数据和特征可能以电子表格的形式组织。然而，其他算法（例如随机森林和XGBoost）通常更适合此类数据。如果你只有列式数据，你可能不想使用神经网络。
- en: 'Neural networks are really useful and start to out-perform other methods when
    we use them to impose a *prior belief*. We use the words *prior belief* in a very
    literal way: there is something we *believe* is true about how the data/problem/world
    works *prior* to ever looking at the data.[¹](#fn5) Specifically, deep learning
    has been most successful at imposing *structural* priors. By how we design the
    network, we impart some knowledge about the intrinsic nature or structure of the
    data. The most common types of structure encoded into neural networks are spatial
    correlation (i.e., images in this chapter) and sequential relationships (e.g.,
    weather changes from one day to the next). Figure 3.1 shows some of the cases
    where you want to use a CNN.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络非常有用，当我们使用它们来施加*先验信念*时，它们开始优于其他方法。我们用非常字面的方式使用“先验信念”这个词：在我们查看数据之前，我们相信数据/问题/世界是如何工作的。具体来说，深度学习在施加*结构*先验方面最成功。通过我们设计网络的方式，我们向数据内在性质或结构传递了一些知识。编码到神经网络中最常见的结构类型是空间相关性（即，本章中的图像）和序列关系（例如，天气从一天到另一天的变化）。图3.1展示了你想要使用CNN的一些情况。
- en: '![](../Images/CH03_F01_Raff.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F01_Raff.png)'
- en: Figure 3.1 Columnar data (data that could go in a spreadsheet) should use fully
    connected layers,because there is no structure to the data and fully connected
    layers impart no prior beliefs. Audio andimages have spatial properties that match
    how CNNs see the world, so you should almost always usea CNN for those kinds of
    data. It is hard to listen to a book, so we’ll stick with images instead of audio.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 列式数据（可以放入电子表格中的数据）应使用全连接层，因为数据没有结构，全连接层不会传递先验信念。音频和图像具有与卷积神经网络观察世界的方式相匹配的空间属性，因此对于这类数据，你应该几乎总是使用卷积神经网络。听一本书很难，所以我们还是坚持使用图像而不是音频。
- en: There are several ways to encode structure that we know (or believe) about the
    problem into a neural network, and the list is growing all the time. For now,
    we will talk about CNNs, which have dominated the image-based world. First we
    need to learn how an image and its structure are encoded in PyTorch as tensors
    so that we can understand how a convolution can use this structure. Previously,
    we had input with no structure. Our data could be represented by an (*N*,*D*)
    matrix with N data points and D features. We could have rearranged the order of
    the features, and doing so would not have changed the meaning behind the data
    because there was no structure or importance to how the data was organized. All
    that mattered was that if column j corresponded to a specific feature, we’d always
    put that feature’s value in column j (i.e., we just needed to be consistent).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以将我们已知（或相信）的问题结构编码到神经网络中，而且这个列表还在不断增长。现在，我们将讨论占主导地位的基于图像的世界的卷积神经网络（CNN）。首先，我们需要了解如何在PyTorch中将图像及其结构编码为张量，这样我们才能理解卷积如何使用这种结构。之前，我们的输入没有结构。我们的数据可以用一个(*N*,*D*)矩阵来表示，其中N是数据点，D是特征。我们可以重新排列特征的顺序，这样做不会改变数据的含义，因为数据组织没有结构或重要性。唯一重要的是，如果列j对应一个特定的特征，我们总是将那个特征的价值放在列j中（即，我们只需要保持一致）。
- en: Images, however, are structured. There is an order to the pixels. If you shuffled
    the pixels around, you would fundamentally change the meaning of a picture. In
    fact, you would probably end up with an incomprehensible image if you did that.
    Figure 3.2 shows how this works.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，图像是有结构的。像素之间存在顺序。如果你打乱了像素的顺序，你将从根本上改变图片的意义。实际上，如果你这样做，你可能会得到一个无法理解的图像。图3.2展示了这是如何工作的。
- en: '![](../Images/CH03_F02_Raff.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F02_Raff.png)'
- en: 'Figure 3.2 Shuffling your data will destroy the structure in your data. Left:
    For columnar data, shuffling has no real impact because the data has no special
    structure. Right: When an image is shuffled, it is no longer recognizable. It
    is the structural nature of images that pixels near each other are related to
    each other. CNNs encode the idea that items located near each other are related,
    making CNNs a good fit for images.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 打乱你的数据将破坏你的数据结构。左：对于列式数据，打乱没有实际影响，因为数据没有特殊结构。右：当图像被打乱时，它就不再可识别。图像的结构性质在于相邻像素之间相互关联。卷积神经网络编码了这样一个想法：彼此靠近的项是相关的，这使得卷积神经网络非常适合图像。
- en: Suppose we have N images, and each has a height H and a width W. As a starting
    point, we might consider a matrix of image data to have the shape
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们拥有N张图像，每张图像的高度为H，宽度为W。作为一个起点，我们可能会考虑一个图像数据的矩阵形状
- en: (*N*,*W*,*H*)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: (*N*,*W*,*H*)
- en: which gives us a three-dimensional tensor. This would be fine if we had black-and-white
    images only. But what about color? We need to add some *channels* to our representation.
    Every channel has the same width and height but represents a different perceptual
    concept. Color is usually represented with red, green, and blue (RGB) *channels*,
    and we interpret the mixture of red, green, and blue to create a final color image.
    This is shown in figure 3.3.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们一个三维张量。如果我们只有黑白图像，这将是可行的。但彩色图像怎么办？我们需要在我们的表示中添加一些*通道*。每个通道具有相同的宽度和高度，但代表不同的感知概念。颜色通常使用红色、绿色和蓝色（RGB）*通道*来表示，我们通过混合红色、绿色和蓝色来创建最终的彩色图像。这如图3.3所示。
- en: '![](../Images/CH03_F03_Raff.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F03_Raff.png)'
- en: Figure 3.3 Color images are represented by three sub-images of the same size,
    called *channels*. Each channel represents a different concept, the most common
    being red, green, and blue (RGB). In general, every channel represents a different
    kind of feature at different locations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3中的彩色图像由三个相同大小的子图像表示，称为*通道*。每个通道代表一个不同的概念，最常见的是红色、绿色和蓝色（RGB）。一般来说，每个通道代表不同位置的不同类型的特征。
- en: 'To include color, we need to add a dimension to the tensor for the channels:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了包含颜色，我们需要向张量添加一个通道维度：
- en: (*N*,*C*,*W*,*H*)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (*N*,*C*,*W*,*H*)
- en: Now we have a four-dimensional tensor with structure. By *structure*, we mean
    that the axes of the tensor and the order in which we access data have specific
    meanings (we can’t shuffle them). If `x` is a batch of color images, `x[3,2,0,0]`
    says, “From the fourth image (*N*=3), grab the blue value (*C*=2) of the upper-left
    pixel (0,0).” Or, we could grab the red, green, and blue values using `x[3,:,0,0]`.
    This means we are processing the pixel values at locations i and j; we know we
    need to access the index `x[:,:,i,j]`. More importantly, we need to know something
    about the *neighboring pixel at the bottom right*, which we can access using `x[:,:,i+1,j+1]`.
    Thanks to the input being *structured*, this is true regardless of the values
    of i and j. Convolutions use this approach so that when a convolution looks at
    pixel location *i*, *j* in an image, it can also consider the neighboring pixel
    locations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个具有结构的四维张量。通过*结构*，我们指的是张量的轴和访问数据的顺序具有特定的含义（我们不能随意调整它们）。如果`x`是一批彩色图像，`x[3,2,0,0]`表示，“从第四张图像（*N*=3）中获取左上像素（0,0）的蓝色值（*C*=2）。”或者，我们可以使用`x[3,:,0,0]`获取红色、绿色和蓝色值。这意味着我们在处理位置i和j的像素值；我们知道我们需要访问索引`x[:,:,i,j]`。更重要的是，我们需要了解*右下角相邻像素*的信息，我们可以使用`x[:,:,i+1,j+1]`来访问。由于输入是*结构化的*，无论i和j的值如何，这都是正确的。卷积使用这种方法，当卷积查看图像中位置*i*、*j*的像素时，它也可以考虑相邻像素的位置。
- en: Note RGB is the most common standard for images, but it is not the only option.
    Other popular ways to represent data use hue, saturation, and value (HSV) and
    cyan, magenta, yellow, and key (that is, black) (CMYK). These standards are often
    called *color spaces*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意RGB是图像最常用的标准，但并非唯一选项。其他流行的数据表示方法使用色调、饱和度和值（HSV）以及青色、品红色、黄色和黑色（即黑色）（CMYK）。这些标准通常被称为*颜色空间*。
- en: 3.1.1  Loading MNIST with PyTorch
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 使用PyTorch加载MNIST
- en: While it has become a bit cliché, we will start exploring what all this means
    using the ubiquitous MNIST dataset. It’s a collection of black-and-white images
    of the digits 0 through 9; each is 28 pixels wide and 28 pixels tall. PyTorch
    has a convenient loader for this dataset in a package called `torchvision`. If
    you are doing *anything* with images and PyTorch, you almost certainly want to
    use this package. Although MNIST is a toy problem, we will work with it in most
    chapters because it allows us to run examples in just a few minutes, whereas real
    datasets take hours to *weeks* for a *single run*. I’ve designed these chapters
    so that the approaches and lessons you will learn transfer to real problems.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这已经有点陈词滥调，但我们将从无处不在的MNIST数据集开始探索这一切的含义。这是一个包含0到9数字的黑白图像集合；每个图像宽度为28像素，高度为28像素。PyTorch在名为`torchvision`的包中提供了一个方便的加载器来处理这个数据集。如果你用PyTorch处理图像，你几乎肯定想使用这个包。尽管MNIST是一个玩具问题，但我们将在大多数章节中使用它，因为它允许我们在几分钟内运行示例，而真实数据集则需要数小时甚至数周才能完成一次运行。我已经设计这些章节，以便你将学到的方法和经验可以应用到实际问题中。
- en: 'We load the `torchvision` package as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如下加载`torchvision`包：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we can load the MNIST dataset using the following code. The first argument,
    `./data`, tells PyTorch where we would like the data stored, and `download=True`
    says to download the dataset if it’s not already there. MNIST has a predefined
    training and testing split, which we can obtain by setting the `train` flag to
    `True` or `False`, respectively:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用以下代码加载 MNIST 数据集。第一个参数 `./data` 告诉 PyTorch 我们希望数据存储在哪里，`download=True`
    表示如果数据集尚未存在，则下载数据集。MNIST 有预定义的训练和测试分割，我们可以通过将 `train` 标志设置为 `True` 或 `False` 分别获取：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now you will notice that the `type` of the data returned is *not* a tensor.
    We get a `PIL.Image.Image` ([https://pillow.readthedocs.io/en/stable](https://pillow.readthedocs.io/en/stable))
    because the dataset *is* images. We need to use a `transform` to convert the images
    to tensors, which is why we import the `transforms` package from `torchvision`.
    We can simply specify the `ToTensor` transform, which converts a Python Imaging
    Library (PIL) image into a PyTorch tensor where the minimum possible value is
    0.0 and the max is 1.0, so it’s already in a pretty good numerical range for us
    to work with. Let’s redefine these dataset objects to do that right now. All it
    takes is adding `transform=transforms.ToTensor()` to the method call, as shown
    next, where we load the train and test splits and print the shape of the first
    example of the training set:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你会注意到返回的数据的 `type` 并不是一个张量。我们得到一个 `PIL.Image.Image` ([https://pillow.readthedocs.io/en/stable](https://pillow.readthedocs.io/en/stable))，因为数据集
    *是* 图像。我们需要使用一个 `transform` 将图像转换为张量，这就是为什么我们从 `torchvision` 导入 `transforms` 包。我们可以简单地指定
    `ToTensor` 转换，它将 Python Imaging Library (PIL) 图像转换为 PyTorch 张量，其中最小可能值是 0.0，最大值是
    1.0，因此它已经在我们可工作的良好数值范围内。让我们现在重新定义这些数据集对象来做这件事。只需在方法调用中添加 `transform=transforms.ToTensor()`
    即可，如下所示，其中我们加载训练和测试分割并打印训练集第一个示例的形状：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We have accessed a single example from the dataset, and it has a shape of (1,28,28)
    for *C* = 1 channels (it’s black and white) and a width and height of 28 pixels.
    If we want to visualize a tensor representation of an image that is grayscale,
    `imshow` expects it to have only a width and height (i.e., a shape of (*W*,*H*)).
    The `imshow` function also needs us to tell it explicitly to use grayscale. Why?
    Because `imshow` is meant for a wider class of scientific visualization where
    you might want other options, instead:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经从数据集中获取了一个单独的示例，它对于 *C* = 1 个通道（它是黑白图像）的宽度和高度为 28 像素，形状为 (1,28,28)。如果我们想可视化一个图像的张量表示，该图像是灰度的，`imshow`
    期望它只有宽度和高度（即形状为 (*W*,*H*)）。`imshow` 函数还需要我们明确地告诉它使用灰度。为什么？因为 `imshow` 是为更广泛的科学可视化设计的，你可能希望有其他选项：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/CH03_UN01_Raff.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_UN01_Raff.png)'
- en: 'OK, clearly that is the digit 5\. Since we are learning how images are presented
    as tensors, let’s do a color version. If we stack three copies of the same digit
    on top of each other, we will have a tensor of shape (3,28,28). Because the *structure*
    of the tensor has meaning, this instantaneously makes it a color image by virtue
    of having three channels. The following code does exactly that, stacking the first
    grayscale image three times and printing its shape:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，很明显，那是一个数字 5。由于我们正在学习图像如何表示为张量，让我们做一个彩色版本。如果我们把相同的数字堆叠在顶部，我们将得到一个形状为 (3,28,28)
    的张量。因为张量的 *结构* 有意义，这立即使它成为一个彩色图像，因为它有三个通道。下面的代码正是这样做的，将第一张灰度图像堆叠三次并打印其形状：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now let’s visualize the color version. Here we need to be a little careful.
    In PyTorch, an image is represented as (*N*,*C*,*W*,*H*),[²](#fn6) but `imshow`
    expects a single image as (*W*,*H*,*C*). So we need to *permute* the dimensions
    when using `imshow`. If our tensor has r dimensions, the `permute` function takes
    r inputs: the indexes 0, 1, …, *r* − 1 of the original tensor in the *new order*
    in which we want them to appear. Since our image has (*C*,*W*,*H*) right now,
    maintaining that order means (0,1,2). We want the channel at index 0 to become
    the last dimension, width first, and height second, which is (1,2,0). Let’s try
    it:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来可视化彩色版本。这里我们需要稍微小心一点。在 PyTorch 中，一个图像表示为 (*N*,*C*,*W*,*H*),[²](#fn6)，但
    `imshow` 期望一个单独的图像为 (*W*,*H*,*C*)。因此，在使用 `imshow` 时我们需要对维度进行 *置换*。如果我们的张量有 r 个维度，`permute`
    函数需要 r 个输入：原始张量在 *新顺序* 中出现的索引 0, 1, …, *r* − 1。由于我们的图像当前是 (*C*,*W*,*H*)，保持这个顺序意味着
    (0,1,2)。我们希望索引 0 的通道成为最后一个维度，宽度优先，高度其次，即 (1,2,0)。让我们试试：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/CH03_UN02_Raff.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_UN02_Raff.png)'
- en: 'Why is this *color* image still black and white? Because the original image
    was black and white. We have the same value copied in the red, green, and blue
    channels, which is how you represent a black-and-white image in color. If we zero
    out the red and blue channels, we get a green number:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这张*彩色*图像仍然是黑白的？因为原始图像是黑白的。我们在红色、绿色和蓝色通道中复制了相同的值，这就是如何在彩色图像中表示黑白图像。如果我们清零红色和蓝色通道，我们得到一个绿色的数字：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ No Red. We’re leaving Green alone.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 没有红色。我们让绿色保持不变。
- en: ❷ No Blue
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 没有蓝色
- en: '![](../Images/CH03_UN03_Raff.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_UN03_Raff.png)'
- en: Changing the color of this image is an example of
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 改变这张图像的颜色是
- en: How the different channels impact what the data represents
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的通道如何影响数据所代表的内容
- en: What it means for this to be a *structured* data representation
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这意味着这是一种*结构化*的数据表示
- en: 'Just to make sure these two points are clear, let’s stack three different images
    together into one color image. We will reuse the same numeral 5 as in the first
    image in the stack. It will go into the red channel, so we should see a red 5
    mixed with two other digits that are green and blue, respectively:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保这两点清晰，让我们将三幅不同的图像堆叠成一幅彩色图像。我们将重复使用堆叠中第一幅图像中的相同数字5。它将进入红色通道，因此我们应该看到红色5与两个其他数字混合，分别是绿色和蓝色：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Grabs 3 images
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 捕获3个图像
- en: ❷ Drops the labels
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 丢弃标签
- en: '![](../Images/CH03_UN04_Raff.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_UN04_Raff.png)'
- en: You should see a red 5, a green 0, and a blue 4\. Where two of the images overlap,
    the colors blend because they were placed in separate color channels. For example,
    in the middle, the 4 and 5 intersect, and red + blue = purple. The order of the
    data has meaning, and we can’t arbitrarily reorder things without potentially
    destroying the structure and thus the data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到一个红色的5，一个绿色的0和一个蓝色的4。在两幅图像重叠的地方，颜色混合了，因为它们被放置在不同的颜色通道中。例如，在中间，4和5相交，红色+蓝色=紫色。数据顺序有意义，我们不能随意重新排序，否则可能会破坏结构，从而破坏数据。
- en: 'Let’s look at this more explicitly. What happens if we shuffle the data within
    a channel? Does it have the same important structured meaning? Let’s look at the
    digit 5 one last time but randomly shuffle the values in the tensor:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更明确地看看这个问题。如果我们在一个通道内打乱数据，会发生什么？它是否仍然具有相同的重要结构意义？让我们最后一次看看数字5，但随机打乱张量中的值：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/CH03_UN05_Raff.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_UN05_Raff.png)'
- en: As you can see, this has *completely* changed the meaning of the image. Instead
    of being a 5, it’s . . . nothing, really. The locations of a value and its *nearby
    values* are intrinsically part of that value’s meaning. The value of one pixel
    cannot be separated from its neighbors. This is the *structural spatial prior*
    we try to capture in this chapter. Now that we have learned how to represent the
    structure of images with tensors, we can learn about convolutions to exploit that
    structure.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这完全改变了图像的意义。它不再是5，实际上什么都没有。一个值的地理位置及其*邻近值*本质上是该值意义的一部分。一个像素的值不能与其邻居分离。这是我们试图在本章中捕捉的*结构化空间先验*。现在我们已经学会了如何用张量表示图像的结构，我们可以学习卷积来利用这种结构。
- en: 3.2 What are convolutions?
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 卷积是什么？
- en: 'What do we change now that we have our data shaped like an image? We would
    like to put a *prior* into our model: the *spatial relationship*. The prior that
    convolutions encode is that *things near each other are related, and things far
    from each other have no relationship*. Think about the pictures of the digit 5
    in the previous section. Pick any black pixel: most of its neighboring pixels
    are also black. Pick any white pixel: most of its neighbors are white or a shade
    of white. This is a kind of spatial correlation. It doesn’t really matter where
    in the image this happens because it tends to happen *everywhere* by the nature
    of this *being an image*.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了像图像一样的数据形状，我们该改变什么？我们希望在模型中放入一个*先验*：*空间关系*。卷积编码的先验是*相邻的事物相关，而远离的事物没有关系*。想想上一节中数字5的图片。选择任何一个黑色像素：它的大多数邻近像素也是黑色的。选择任何一个白色像素：它的大多数邻居是白色或白色色调。这是一种空间相关性。这实际上并不重要发生在图像的哪个位置，因为这种*图像的本质*使得它倾向于*无处不在*发生。
- en: A *convolution* is a mathematical function with two inputs. Convolutions take
    an *input* image and a *filter* (also called a *kernel*) and output a new image.
    The goal is for the filter to recognize certain patterns from the input and highlight
    them in the output. A convolution can be used to impose a spatial prior on any
    tensor with r dimensions; a simple example is shown in figure 3.4\. Right now,
    we are just trying to understand what a convolution does—we get to how it works
    in a moment.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积*是一个有两个输入的数学函数。卷积接受一个*输入*图像和一个*滤波器*（也称为*核*），并输出一个新图像。目标是让滤波器从输入中识别某些模式并在输出中突出显示它们。卷积可以用于对具有r维的任何张量施加空间先验；图3.4中展示了简单的一个例子。现在，我们只是试图了解卷积的作用——我们稍后会了解它是如何工作的。'
- en: '![](../Images/CH03_F04_Raff.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH03_F04_Raff.png)'
- en: Figure 3.4 Example of a convolution. An input image and a filter are combined
    by the convolution. The output is a new image that has been altered. The purpose
    of the filter is to identify or recognize certain patterns in the input. In this
    example, the filter identifies the horizontal line at the top of a numeral 7.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 卷积的示例。输入图像和滤波器通过卷积结合。输出是一个经过修改的新图像。滤波器的目的是识别或识别输入中的某些模式。在这个例子中，滤波器识别了数字7顶部的水平线。
- en: While we keep referring to *images*, convolutions are not constrained to only
    working on two-dimensional data. To help us understand how convolutions work,
    we will start with a one-dimensional example because it makes the math easier
    to walk through. Once we understand 1D convolutions, the 2D version we use for
    images will follow very quickly. Because we want to create multiple layers of
    convolutions, we will also learn about *padding*, which is necessary for that
    purpose. Finally, we will talk about weight sharing, which is a different way
    of thinking about convolutions that re-emerges throughout the book.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们一直在提到*图像*，但卷积并不局限于只处理二维数据。为了帮助我们理解卷积是如何工作的，我们将从一个一维例子开始，因为这使数学更容易理解。一旦我们理解了一维卷积，我们用于图像的二维版本将很快跟上。因为我们想要创建多个卷积层，我们还将学习*填充*，这对于此目的来说是必要的。最后，我们将讨论权重共享，这是一种在本书中重新出现的关于卷积的不同思考方式。
- en: 3.2.1  1D convolutions
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 一维卷积
- en: To understand how a convolution works, let’s talk about a one-dimensional image
    first, as it is easier to show the details in 1D than 2D. A 1D image has a shape
    of (*C*,*W*) for the number of channels and the width. There is no height because
    we are talking about just 1D, not 2D. For a 1D input with (*C*,*W*) shape, we
    can define a filter with a shape of (*C*,*K*). We get to choose the value of K,
    and we need C to match up between the image and the filter. Since the number of
    channels must *always* match, we call this a “filter of size K” for short. If
    we apply a filter of size K to an input of shape (*C*,*W*), we get an output that
    has shape (*C*,*W*−2⋅⌊*K*/2⌋).[³](#fn7) Let’s look at how that works infigure
    3.5.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解卷积是如何工作的，我们先从一维图像说起，因为在一维中比在二维中更容易展示细节。一维图像的形状为(*C*,*W*)，代表通道数和宽度。没有高度，因为我们只讨论一维，而不是二维。对于一个形状为(*C*,*W*)的一维输入，我们可以定义一个形状为(*C*,*K*)的滤波器。我们可以选择K的值，并且需要C在图像和滤波器之间匹配。由于通道数必须*始终*匹配，我们简称这种滤波器为“大小为K的滤波器”。如果我们将大小为K的滤波器应用于形状为(*C*,*W*)的输入，我们得到一个形状为(*C*,*W*−2⋅⌊*K*/2⌋)的输出。[³](#fn7)
    让我们看看图3.5中它是如何工作的。
- en: '![](../Images/CH03_F05_Raff.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH03_F05_Raff.png)'
- en: Figure 3.5 A 1D input “1, 0, 2, –1, 1, 2” is convolved with the filter “1, 0,
    –1”. This means we take every subsequence of three input items, multiply those
    items with the filter values, and then add the results together.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 一个一维输入“1, 0, 2, –1, 1, 2”与滤波器“1, 0, –1”进行卷积。这意味着我们取输入项的每个长度为三的子序列，将这些项与滤波器值相乘，然后将结果相加。
- en: 'An input of shape (1,6) is on the left, and we are applying a filter of size
    3 that has the values [1,0,−1]. The output is on the right. For each output, you
    can see arrows coming in for the *spatially relevant* inputs for that output.
    So for the first output, − 1, only the first three inputs are relevant; nothing
    else in the input impacts that specific output. Its value is calculated by multiplying
    the first three inputs with the three values in the kernel and then summing them.
    The second value of the output is computed by using the second set of three input
    values. Notice it is *always* the same three values from the filter, applied to
    *every* position in the input. The following code shows how to implement this
    in raw Python:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 左边是一个形状为(1,6)的输入，我们正在应用一个大小为3的过滤器，其值为[1,0,−1]。输出在右边。对于每个输出，你可以看到箭头指向该输出对应的*空间相关*输入。所以对于第一个输出，−
    1，只有前三个输入是相关的；输入中的其他内容不会影响该特定输出。其值是通过将前三个输入与核中的三个值相乘然后求和来计算的。输出的第二个值是通过使用第二组三个输入值来计算的。注意，它*总是*使用过滤器中的相同三个值，应用于输入的每个位置。以下代码展示了如何在原始Python中实现这一点：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Slides the filter over the input
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将过滤器滑过输入
- en: ❷ Applies the filter at this location
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在此位置应用过滤器
- en: ❸ The output is ready to use.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 输出已准备好使用。
- en: In effect, we are *sliding* the filter across every location in the input, computing
    a value at each location, and storing it in the output. That’s what a convolution
    is. The size of the output shrinks by 2 ⋅ ⌊3/2⌋ because we run out of values at
    the edges of the input. Next we’ll show how to make this work in 2D, and then
    we’ll have the foundation of CNNs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们在输入的每个位置上滑动过滤器，计算每个位置的一个值，并将其存储在输出中。这就是卷积的含义。由于输入边缘的值不足，输出的大小减少了2 ⋅ ⌊3/2⌋。接下来，我们将展示如何在2D中实现这一点，然后我们将拥有CNN的基础。
- en: 3.2.2  2D convolutions
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 2D卷积
- en: 'As we increase the number of dimensions r in our tensor, the idea of convolutions
    and how they work stays the same: we slide a filter around the input, multiply
    the values in the filter with each area of the image, and then take the sum. We
    simply make the filter shape match accordingly. Let’s look at a 2D example that
    aligns with the images we will try to process: figure 3.6 introduces the ⊛ operator,
    which means *convolve*.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们张量中维度r数量的增加，卷积的概念及其工作方式保持不变：我们在输入周围滑动一个过滤器，将过滤器中的值与图像的每个区域相乘，然后求和。我们只需使过滤器形状相应匹配。让我们看看一个与我们将尝试处理的图像相一致的2D示例：图3.6介绍了⊛运算符，它表示*卷积*。
- en: '![](../Images/CH03_F06_Raff.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F06_Raff.png)'
- en: Figure 3.6 An image of a numeral 1 convolved with a 2D filter. The green region
    shows the part of the image currently being convolved, with the values from the
    filter shown in light blue. The result in the output is shown in orange. By sliding
    the green/orange areas together over the entire image, you produce the output
    result.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 一个数字1与2D过滤器卷积的图像。绿色区域显示了当前正在卷积的图像部分，过滤器中的值以浅蓝色显示。输出中的结果显示为橙色。通过将绿色/橙色区域在整个图像上滑动，你将产生输出结果。
- en: 'Again, the 2D output results from multiplying the filter values (pairwise)
    at each location and summing them all. The highlighted regions of the input are
    used to create the values in the output. In the bottom-right corner of the input
    cells, you see the filter value we are multiplying by. In deep learning, we almost
    always use square filters, meaning all r dimensions of the filter have the same
    number of values. So, in this case, we would call this a 2D filter of size K,
    or just size K for short. In this case, *K* = 3. The code for a 2D convolution
    doubles the number of loops:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，2D输出是通过在每个位置将过滤器值（成对）相乘然后求和得到的。输入中突出显示的区域用于创建输出中的值。在输入单元格的右下角，你可以看到我们正在乘以的过滤器值。在深度学习中，我们几乎总是使用方形过滤器，这意味着过滤器的所有r维都有相同数量的值。因此，在这种情况下，我们可以称之为大小为K的2D过滤器，或者简称为K。在这种情况下，*K*
    = 3。2D卷积的代码将循环次数加倍：
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Slides the filter over the rows
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将过滤器滑过行
- en: ❷ Slides the filter over the columns
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将过滤器滑过列
- en: ❸ Applies the filter at this location
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在此位置应用过滤器
- en: ❹ Builds up a row of the output
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 构建输出行
- en: ❺ Adds the row to the final output. The output is ready for use.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将行添加到最终输出。输出已准备好使用。
- en: 'Since this 2D input has a shape of (1,6,6) and the kernel has a shape of (1,3,3),
    we shrink the width and height by 2 ⋅ ⌊3/2⌋ = 2. That means the height is 6 pixels
    − 2 = 4 pixels, and we get the same result for the width: 6 − 2 = 4 pixels wide.
    We now have the exact operation that is the foundation of CNNs for image classification.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个二维输入的形状为 (1,6,6) 且核的形状为 (1,3,3)，我们将宽度和高度缩小 2 ⋅ ⌊3/2⌋ = 2。这意味着高度是 6 像素 −
    2 = 4 像素，我们得到的宽度结果相同：6 − 2 = 4 像素宽。我们现在有了 CNN 图像分类的基础操作。
- en: 3.2.3  Padding
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3  填充
- en: Notice that every time we apply a convolution, the output becomes skinnier and
    shorter than the original input. That means if we kept applying convolutions over
    and over again, we would eventually be left with nothing. This is not something
    we want, because we will create multiple layers of convolutions. Most modern deep
    learning design practices keep the input and output the same size so that we can
    more easily reason about the shapes of our networks and make them as deep as we
    like without worrying about the input disappearing. The solution is called *padding*.
    You should almost always use padding by default so you can change your architecture
    without changing the shapes of your tensors. Figure 3.7 shows how this works for
    the same 2D image.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，每次我们应用卷积时，输出都会比原始输入更瘦更长。这意味着如果我们不断地应用卷积，最终我们将一无所有。这不是我们想要的，因为我们将会创建多个卷积层。大多数现代深度学习设计实践都保持输入和输出大小相同，这样我们就可以更容易地推理我们网络的形状，并且可以将其做得尽可能深，而不用担心输入会消失。解决方案称为*填充*。你应该几乎总是默认使用填充，这样你就可以在不改变张量形状的情况下更改你的架构。图
    3.7 展示了这对于相同的二维图像是如何工作的。
- en: '![](../Images/CH03_F07_Raff.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F07_Raff.png)'
- en: Figure 3.7 The same convolution as before, with the same input and filter—but
    the input is padded with the value 0 to make the image larger by one pixel in
    each direction. This causes the output image to become the same width and height
    as the original image.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 与之前相同的卷积，具有相同的输入和滤波器——但是输入被填充了值为 0 的像素，使图像在每个方向上变大一个像素。这导致输出图像的宽度和高度与原始图像相同。
- en: We add an imaginary row/column of zeros all the way around the image and process
    it as if it was larger than it actually is. This specific example is called *zero
    padding by one* because we’re adding one value to all the edges of the image,
    and the value was filled with 0\. If we use a convolutional filter of size K,
    we can use padding of ⌊*K*/2⌋ to make sure our output stays the same size as our
    input. Again, even though the height and width can be padded to different degrees,
    we generally use the same amount of padding in each dimension because our filters
    have the same size in each dimension.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图像周围添加了一行/列的零，并像它实际上更大一样处理它。这个特定的例子称为*零填充一次*，因为我们向图像的所有边缘添加了一个值，而这个值被填充为
    0。如果我们使用大小为 K 的卷积滤波器，我们可以使用 ⌊*K*/2⌋ 的填充来确保我们的输出保持与输入相同的大小。再次强调，尽管高度和宽度可以填充到不同的程度，但我们通常在每个维度上使用相同的填充量，因为我们的滤波器在每个维度上具有相同的大小。
- en: 3.2.4  Weight sharing
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.4  权重共享
- en: There is another way to think about convolutions, which introduces an important
    concept called *weight sharing*. Let’s look at the 1D case again because it is
    simpler to write code for. Imagine you have a neural network *f*[Θ](⋅) with parameters
    (or *weights*) Θ, which takes in an input vector z with K features, **z** ∈ ℝ^K.
    Now let’s say we have a larger input x with *C* = 1 channels and D features, where
    *D* > *K*. We can’t use *f*[Θ](⋅) on x because the shapes *D* ≠ *K* do not match.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种思考卷积的方法，引入了一个重要的概念，称为*权重共享*。让我们再次看看一维情况，因为它更容易编写代码。想象你有一个神经网络 *f*[Θ](⋅)，其参数（或*权重*）为
    Θ，它接收一个具有 K 个特征的输入向量 z，**z** ∈ ℝ^K。现在假设我们有一个更大的输入 x，具有 *C* = 1 个通道和 D 个特征，其中 *D*
    > *K*。我们不能在 x 上使用 *f*[Θ](⋅)，因为形状 *D* ≠ *K* 不匹配。
- en: 'One way we could apply the network *f*[Θ](⋅) to this larger dataset would be
    to *slide* the network across slices of the input and *share* the weights Θ for
    each position. Some Python pseudocode looks like this:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将网络 *f*[Θ](⋅) 应用到这个更大的数据集的一种方法是将网络*滑动*到输入的切片上，并*共享*每个位置的权重 Θ。一些 Python 伪代码如下所示：
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Some input vector
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 一些输入向量
- en: 'Now, if we defined our network as `f = nn.Linear(K, 1)`, it will implement
    *exactly a 1D convolution*. This insight can teach us some important properties
    about convolutions and how to use them to design deep neural networks. Right now,
    the primary thing this teaches us is that *convolutions are linear operations
    that work spatially*. As with the `nn.Linear` layer from the last chapter, a convolution
    followed by a second convolution is equivalent to just one slightly different
    convolution. That means:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们定义我们的网络为`f = nn.Linear(K, 1)`，它将实现*精确的1D卷积*。这个洞察力可以教会我们一些关于卷积的重要性质以及如何使用它们来设计深度神经网络。目前，这教会我们的主要事情是，*卷积是线性操作，在空间上工作*。就像上一章中的`nn.Linear`层一样，一个卷积后面跟着第二个卷积相当于一个稍微不同的卷积。这意味着：
- en: Never repeat convolutions, because doing so is redundant.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 永远不要重复卷积，因为这样做是多余的。
- en: Include a nonlinear activation function after using convolutions.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用卷积后包含一个非线性激活函数。
- en: Note If we had a rectangular kernel of size (1,3,5), the width of the output
    image would be 2 ⋅ ⌊3/2⌋ = 2 smaller than the input. The height of the output
    would be 2 ⋅ ⌊5/2⌋ = 4, removing *two* from each side. While rectangular kernels
    are possible, they are rarely used. Also notice that we have been sticking with
    kernels that have an *odd* size—nothing divisible by two. This is mostly for representational
    convenience because the filter is using an exact center from the input to produce
    each output. Like rectangular kernels, filters with even sizes are possible but
    are rarely used.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果我们有一个大小为（1,3,5）的矩形核，输出图像的宽度将是输入宽度的2 ⋅ ⌊3/2⌋ = 2个像素更小。输出图像的高度将是2 ⋅ ⌊5/2⌋
    = 4，每边减去*两个*像素。虽然矩形核是可能的，但很少使用。同时请注意，我们一直使用的是*奇数大小*的核——没有能被2整除的。这主要是因为表示上的方便，因为过滤器使用输入的精确中心来产生每个输出。像矩形核一样，偶数大小的过滤器也是可能的，但很少使用。
- en: 3.3 How convolutions benefit image processing
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 卷积如何有利于图像处理
- en: We have spent a lot of time talking about what convolutions are. Now it’s time
    to see what they can do. Convolutions have a rich history of use in computer vision
    applications; this simple operation can define many useful things, as long as
    we select the appropriate kernel.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经花费了很多时间讨论卷积是什么。现在，是时候看看它们能做什么了。卷积在计算机视觉应用中有着丰富的历史；只要我们选择合适的核，这个简单的操作就可以定义许多有用的东西。
- en: 'To start, let’s again look at a *specific* image of the digit 4 from MNIST.
    We load the SciPy `convolve` function and define the `img_index` so you can change
    what image you are processing and see how these convolutions work across other
    inputs:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们再次查看MNIST中数字4的*特定*图像。我们加载SciPy的`convolve`函数并定义`img_index`，这样你就可以更改你正在处理的图像，并看到这些卷积在其他输入上的工作方式：
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/CH03_UN06_Raff.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_UN06_Raff.png)'
- en: One common computer vision operation is to *blur* an image. Blurring involves
    taking a local average pixel value and replacing every pixel with the average
    of its neighbors. This can be useful to wash out small noisy artifacts or soften
    a sharp edge. It’s done with a *blur kernel*, where
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的计算机视觉操作是对图像进行*模糊*。模糊涉及取局部平均像素值并用其邻居的平均值替换每个像素。这可以用来消除小的噪声伪影或软化锐利的边缘。这是通过一个*模糊核*完成的，其中
- en: '![](../Images/CH03_F07_Raff_EQ01.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F07_Raff_EQ01.png)'
- en: 'We can take this math and convert it directly into code. The matrix is an `np.asarray`
    call, we’ve already loaded the image, and the convolution ⊛ is done with the `convolve`
    function. When we show the output image, we get a blurry version of the digit
    4:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个数学公式直接转换为代码。矩阵是通过`np.asarray`调用的，我们已经加载了图像，卷积操作⊛是通过`convolve`函数完成的。当我们展示输出图像时，我们得到数字4的模糊版本：
- en: '[PRE13]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/CH03_UN07_Raff.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_UN07_Raff.png)'
- en: 'An especially common application of convolutions is to perform *edge detection*.
    In any computer vision application, it’s good to know where the edges are. They
    can help you determine where the edges of a road are (you want your car to stay
    in its lane) or find objects (edges in different shapes are easy to recognize).
    In the case of this 4, the edges are the outline of the digit. So if all the pixels
    in a local area of the image are the *same*, we want everything to cancel out
    and result in no output. We only want there to be an output when there is a local
    change. Again, this can be described as a kernel where everything around the current
    pixel is negative, and the center pixel counts for the same weight as all its
    neighbors:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积的一个特别常见的应用是执行 *边缘检测*。在任何计算机视觉应用中，知道边缘在哪里都是很好的。它们可以帮助你确定道路的边缘在哪里（你希望你的车保持在车道内）或者找到物体（不同形状的边缘容易识别）。在这个4的例子中，边缘是数字的轮廓。所以如果图像局部区域的所有像素都是
    *相同* 的，我们希望所有内容都相互抵消，结果没有输出。我们只希望在存在局部变化时才有输出。再次强调，这可以描述为一个核，其中当前像素周围的所有内容都是负的，中心像素的权重与所有邻居相同：
- en: '![](../Images/CH03_F07_Raff_EQ02.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F07_Raff_EQ02.png)'
- en: 'This filter is maximized when everything around a pixel is different than itself.
    Let’s see what happens:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 此过滤器在像素周围所有内容都与自身不同时达到最大值。让我们看看会发生什么：
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ We can find edges by focusing on the difference between a pixel and its neighbors.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们可以通过关注像素与其邻居之间的差异来找到边缘。
- en: '![](../Images/CH03_UN08_Raff.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_UN08_Raff.png)'
- en: As promised, the filter found the edges of the digit. The response occurs only
    at the edges because that’s where the most changes are. Outside the digit, there
    is no response because the high center weight cancels out all the neighbors, which
    is also true for the *inside* region of the digit. Thus, we have now found all
    the edges in the image.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 正如承诺的那样，滤波器找到了数字的边缘。响应只发生在边缘，因为那里变化最大。在数字外部没有响应，因为中心的高权重抵消了所有邻居，这在数字的 *内部* 区域也是正确的。因此，我们现在已经找到了图像中的所有边缘。
- en: 'We also may want to look for edges at a specific angle. If we constrain ourselves
    to a 3 × 3 kernel, it is easiest to find horizontal or vertical edges. Let’s create
    one for horizontal edges by making the kernel values change signs across the horizontal
    of the filter:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可能想要寻找特定角度的边缘。如果我们将自己限制在3 × 3核，最容易找到水平和垂直边缘。让我们通过使滤波器水平方向上的核值改变符号来创建一个用于水平边缘的滤波器：
- en: '![](../Images/CH03_F07_Raff_EQ03.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F07_Raff_EQ03.png)'
- en: '[PRE15]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ We could look for only horizontal edges.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们可以只寻找水平边缘。
- en: '![](../Images/CH03_UN09_Raff.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_UN09_Raff.png)'
- en: 'We identified only the horizontal edges of the image, which are primarily the
    bottom bar of the 4\. As we define more useful kernels, you can imagine how we
    might compose a *combination* of filters to start recognizing higher-level concepts.
    Imagine if we only had the vertical and horizontal filters: we would not be able
    to classify all 10 digits, but figure 3.8 shows how we could narrow down the answer.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只识别了图像的水平边缘，主要是4的底部横条。随着我们定义更有用的核，你可以想象我们如何组合 *组合* 滤波器来开始识别更高级的概念。想象一下，如果我们只有垂直和水平滤波器：我们无法分类所有10个数字，但图3.8显示了我们可以如何缩小答案。
- en: '![](../Images/CH03_F08_Raff.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F08_Raff.png)'
- en: Figure 3.8 Example where we have only a vertical and a horizontal edge filter.
    If only the vertical filter turns on, we are probably looking at the digit 1\.
    If only the horizontal filter turns on, we could be looking at a 7, but we would
    like an additional diagonal filter to help us confirm. If both the horizontal
    and vertical filters respond, we could be looking at a 4 or a 9; it’s hard to
    tell without more filters.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 仅具有垂直和水平边缘滤波器的示例。如果只有垂直滤波器开启，我们可能正在查看数字1。如果只有水平滤波器开启，我们可能正在查看数字7，但我们希望有一个额外的对角线滤波器来帮助我们确认。如果水平和垂直滤波器都响应，我们可能正在查看数字4或9；没有更多的滤波器很难判断。
- en: 'Designing all the filters you might need by hand was a big part of computer
    vision for many decades. Convolutions on top of convolutions could also identify
    larger concepts: for example, after we identified horizontal and vertical edges,
    a new filter might take those as input and look for an empty space in the center
    with horizontal edges on top and vertical edges on the side. This would get us
    an O-like shape that could tell the difference between a 9 and a 4.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多年代中，手动设计所有可能需要的过滤器是计算机视觉的一个重要部分。在卷积之上进行卷积也可以识别更大的概念：例如，在我们识别了水平和垂直边缘之后，一个新的过滤器可能将这些作为输入，并寻找中心有一个水平边缘在上、侧面有一个垂直边缘的空隙。这将得到一个类似O的形状，可以区分9和4。
- en: Thanks to deep learning, though, we don’t have to go through the mental effort
    of imagining and testing all the filters we might need. Instead, we can let the
    neural network learn the filters itself. That way, we save ourselves from a labor-intensive
    process, and the kernels are optimized for the specific problem we care about.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，多亏了深度学习，我们不需要费尽心思去想象和测试我们可能需要的所有过滤器。相反，我们可以让神经网络自己学习过滤器。这样，我们节省了自己繁重的过程，并且核函数针对我们关心的特定问题进行了优化。
- en: '3.4 Putting it into practice: Our first CNN'
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 实践应用：我们的第一个CNN
- en: Now that we have discussed what a convolution is, let’s look at some mathematical
    symbols and PyTorch code. We’ve seen that we can take an image *I* ∈ ℝ^(*C*, *W*,
    *H*) and apply a convolution using a filter *g* ∈ *R*^(*C*, *K*, *K*) to get a
    new result image ℝ^(*W*′, *H*′). We can write this in math as
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了什么是卷积，让我们看看一些数学符号和PyTorch代码。我们看到了我们可以取一个图像 *I* ∈ ℝ^(*C*, *W*, *H*)
    并使用一个过滤器 *g* ∈ *R*^(*C*, *K*, *K*) 进行卷积，以得到一个新的结果图像 ℝ^(*W*′, *H*′)。我们可以用数学公式表示为
- en: '![](../Images/ch3-eqs-to-illustrator0x.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/ch3-eqs-to-illustrator0x.png)'
- en: That means each filter looks at all C input channels on its own. Figure 3.9
    shows that using the 1D input example since it is easier to visualize.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着每个过滤器都会单独查看其上的所有C输入通道。图3.9展示了使用1D输入示例，因为它更容易可视化。
- en: '![](../Images/CH03_F09_Raff.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F09_Raff.png)'
- en: Figure 3.9 Example of a 1D convolution with two channels. Because there are
    two channels, the 1D input looks like a matrix and has two axes. The filter also
    has two axes, one for each channel. This filter slides from top to bottom and
    produces *one* output channel. One filter always produces one output channel,
    regardless of however many input channels there are.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 1D卷积具有两个通道的示例。因为有两个通道，1D输入看起来像一个矩阵，有两个轴。过滤器也有两个轴，每个通道一个。这个过滤器从上到下滑动，并产生一个输出通道。一个过滤器总是产生一个输出通道，无论有多少输入通道。
- en: Because the input has a shape of (*C*,*W*), the filter has a shape of (*C*,*K*).
    So when the input has multiple channels, the kernel will have a value for each
    channel separately. That means for a color image, we could have a filter that
    looks for “red horizontal lines, blue vertical lines, and no green” all in one
    operation. But it also means that after applying *one filter*, we get *one output*.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 因为输入的形状为 (*C*,*W*)，所以过滤器的形状为 (*C*,*K*)。因此，当输入有多个通道时，核将分别为每个通道单独有一个值。这意味着对于彩色图像，我们可以在一个操作中找到一个过滤器，它寻找“红色水平线、蓝色垂直线，没有绿色”。但这也意味着在应用
    *一个过滤器* 之后，我们得到 *一个输出*。
- en: 3.4.1  Making a convolutional layer with multiple filters
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 使用多个过滤器构建卷积层
- en: Considering the previous examples, we probably want more than one filter. We
    want *C*[out] different filters; and let’s use *C*[in] to indicate the number
    of channels in the input. In this case, we have a tensor that represents all the
    filters as *G* ∈ ℝ^(*C*[out], *C*[in], *K*, *K*), so we get a new result *R* ∈
    ℝ^(*K*, *W*′, *H*′) when we write *R* = *I* ⊛ *G*.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到前面的示例，我们可能需要不止一个过滤器。我们想要 *C*[out] 个不同的过滤器；让我们用 *C*[in] 来表示输入中的通道数。在这种情况下，我们有一个表示所有过滤器的张量
    *G* ∈ ℝ^(*C*[out], *C*[in], *K*, *K*)，因此当我们写 *R* = *I* ⊛ *G* 时，我们得到一个新的结果 *R*
    ∈ ℝ^(*K*, *W*′, *H*′)。
- en: How do we convert this math notation to convolving input image I with a set
    of multiple filters G? PyTorch provides the `nn.Conv1d`, `nn.Conv2d`, and `nn.Conv3d`
    functions for handling this for us. Each of these implements a convolutional layer
    for one-, two-, and three-dimensional data. Figure 3.10 shows what is happening
    as a mechanical process.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将这个数学符号转换为使用多个过滤器G对输入图像I进行卷积？PyTorch提供了`nn.Conv1d`、`nn.Conv2d`和`nn.Conv3d`函数来处理这个问题。这些函数中的每一个都实现了一个卷积层，用于一维、二维和三维数据。图3.10展示了作为一个机械过程正在发生的事情。
- en: '![](../Images/CH03_F10_Raff.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F10_Raff.png)'
- en: 'Figure 3.10 The `nn.Conv2d` functions defines a convolutional layer and works
    in three steps. First, it takes the input image, which has *C*[in] channels. As
    part of the construction, `nn.Conv2d` takes the number of filters to use: *C*[out].
    Each filter is applied to the input. one at a time, and the results are combined.
    So the input tensor’s shape is (*B*,*C*[in],*W*,*H*), and the output tensor has
    a shape of (*B*,*C*[out],*W*,*H*).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 `nn.Conv2d`函数定义了一个卷积层，并在三个步骤中工作。首先，它接受一个输入图像，该图像具有*C*[in]通道。作为构建的一部分，`nn.Conv2d`接受要使用的过滤器数量：*C*[out]。每个过滤器依次应用于输入，并将结果组合。因此，输入张量的形状是(*B*,*C*[in],*W*,*H*)，输出张量的形状为(*B*,*C*[out],*W*,*H*)。
- en: 'The same process is used by all three standard convolution sizes: `Conv1d`
    works on tensors that are (Batch, Channels, Width), `Conv2d` does (Batch, Channels,
    Width, Height), and `Conv2d` does (Batch, Channels, Width, Height, Depth). The
    number of channels in the input is *C*[in], and the convolutional layer is made
    of *C*[out] filters/kernels. Since each of these filters produces one output channel,
    the output of this layer has *C*[out] channels. The value K defines the size of
    the kernel being used.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三种标准卷积大小都使用相同的过程：`Conv1d`处理形状为（Batch, Channels, Width）的张量，`Conv2d`处理形状为（Batch,
    Channels, Width, Height）的张量，而`Conv2d`处理形状为（Batch, Channels, Width, Height, Depth）的张量。输入中的通道数是*C*[in]，卷积层由*C*[out]个过滤器/核组成。由于每个过滤器产生一个输出通道，因此该层的输出具有*C*[out]个通道。值K定义了正在使用的核的大小。
- en: 3.4.2  Using multiple filters per layer
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 使用每层多个过滤器
- en: To help drive home how this works, let’s dive a little deeper and show the full
    process in detail. Figure 3.11 shows all the steps and math for an input image
    with *C*[in] = 3, *C*[out] = 2, and *K* = 3.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助大家更好地理解这个过程，让我们深入探讨并详细展示整个过程的全部步骤。图3.11展示了输入图像（*C*[in] = 3，*C*[out] = 2，*K*
    = 3）的所有步骤和数学计算。
- en: '![](../Images/CH03_F11_Raff.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F11_Raff.png)'
- en: 'Figure 3.11 Example showing how convolutions are applied to multiple channels
    of a single image. Left: The input image, with padding shown in red. Middle: The
    parameters of two filters, *W*[1] and *W*[2] (with their associated bias terms,
    *b*[1], *b*[2]). Each filter is convolved with the input to produce one channel.
    Right: The results are stacked to create a new image that has two channels, one
    for each filter. This whole process is done for us by the PyTorch `nn.Conv2d`
    class.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11示例展示了如何将卷积应用于单个图像的多个通道。左：输入图像，红色显示填充。中：两个过滤器（*W*[1]和*W*[2]）的参数（及其相关的偏置项，*b*[1]，*b*[2]）。每个过滤器依次与输入进行卷积以产生一个通道。右：结果堆叠以创建一个新图像，该图像具有两个通道，每个通道对应一个过滤器。这个过程由PyTorch的`nn.Conv2d`类为我们完成。
- en: The input image has *C*[in] = 3 channels, and we will process it using `nn.Conv2d(C_in, C_out, 3, padding=3//2)(x) = output`.
    Since *C*[out] = 2, that means we process the input with two *different* filters,
    add the bias terms for every location, and get two resulting images that have
    the same height and width as the original image (because we used padding). The
    results are stacked into one larger single image with two channels because we
    specified *C*[out] = 2.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像具有*C*[in] = 3个通道，我们将使用`nn.Conv2d(C_in, C_out, 3, padding=3//2)(x) = output`对其进行处理。由于*C*[out]
    = 2，这意味着我们使用两个*不同*的过滤器处理输入，为每个位置添加偏置项，并得到两个具有与原始图像相同高度和宽度的结果图像（因为我们使用了填充）。由于我们指定*C*[out]
    = 2，结果被堆叠成一个具有两个通道的更大的单张图像。
- en: 3.4.3  Mixing convolutional layers with linear layers via flattening
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.3 通过展平将卷积层与线性层混合
- en: 'When we had a fully connected layer, we wrote something like figure 3.12 for
    a single hidden layer with n hidden units/neurons. The mathematical notation we
    would use to describe a network with one *convolutional* hidden layer is very
    similar:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有一个全连接层时，我们为单个隐藏层（n个隐藏单元/神经元）编写了如图3.12所示的图。我们用来描述一个具有一个*卷积*隐藏层的网络的数学符号非常相似：
- en: '![](../Images/ch3-eqs-to-illustrator2x.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ch3-eqs-to-illustrator2x.png)'
- en: '![](../Images/CH03_F12_Raff.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F12_Raff.png)'
- en: 'Figure 3.12 One hidden layer in a fully connected network. Left: The PyTorch
    `Module`s that match the equation on the right. Colors show which `Module` maps
    to which part of the equation.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12全连接网络中的一个隐藏层。左：与右侧方程匹配的PyTorch `Module`。颜色显示哪个`Module`映射到方程的哪个部分。
- en: 'That equation may look a little scary, but if we slow down, it’s not that bad.
    It’s intimidating because we have included the shape of each tensor in the equation
    and annotated where our fancy new `nn.Conv2d` `module` comes into play. We’ve
    included the shape information so you can see how we are processing different
    sized inputs—if we remove those extra details, it’s not as scary-looking:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 那个方程可能看起来有点吓人，但如果我们放慢速度，其实并不那么糟糕。它之所以令人畏惧，是因为我们在方程中包含了每个张量的形状，并标注了我们的新 `nn.Conv2d`
    `module` 如何发挥作用。我们包含形状信息是为了让您了解我们如何处理不同大小的输入——如果我们去掉那些额外细节，它看起来就不会那么吓人：
- en: '*f*(**x**) = tanh(**x**⊛**W**^((*h*[1]))+**b**^((*h*[1])))**W**^((out)) + **b**^(**(**out**)**)      
    **(3.1)**'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*(**x**) = tanh(**x**⊛**W**^((*h*[1]))+**b**^((*h*[1])))**W**^((out)) + **b**^(**(**out**)**)      
    **(3.1)**'
- en: Now it’s clear that the only thing we have changed is replacing the dot product
    (a linear operation denoted by {}^⊤) with convolution (a spatially linear operation
    denotedby ⊛).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在很清楚，我们唯一改变的是将点积（一个由 {}^⊤ 表示的线性操作）替换为卷积（一个由 ⊛ 表示的空间线性操作）。
- en: 'This is *almost* perfect, but we have one issue: the output of the convolution
    has a shape of (*C*,*W*,*H*), but our linear layer (**W**^((out)) + **b**^(**(**out**)**))
    expects something of shape (*C*×*W*×*H*)—that’s *one* dimension with all three
    original dimensions collapsed into it. Essentially, we need to *reshape* the output
    of our convolutions to remove the spatial interpretation so our linear layer can
    process the result and compute some predictions; see figure 3.13.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎是完美的，但我们有一个问题：卷积的输出形状为 (*C*,*W*,*H*)，但我们的线性层 (**W**^((out)) + **b**^(**(**out**)**))
    预期的是形状为 (*C*×*W*×*H*) 的东西——那就是 *一个* 维度，其中包含了所有三个原始维度。本质上，我们需要将卷积的输出 *重塑*，以去除空间解释，这样我们的线性层就可以处理结果并计算一些预测；参见图
    3.13。
- en: '![](../Images/CH03_F13_Raff.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F13_Raff.png)'
- en: 'Figure 3.13 One hidden layer in a convolutional network. Left: The PyTorch
    `Module`s that match the equation on the right. Colors show which `Module` maps
    to which part of the equation.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13 卷积网络中的一个隐藏层。左：与右侧方程匹配的 PyTorch `Module`。颜色显示哪个 `Module` 映射到方程的哪个部分。
- en: This is called *flattening* and is a common operation in modern deep learning.
    We can consider equation 3.1 on the previous page to have implicitly used this
    flatten operation. PyTorch provides a `module` to do this called `nn.Flatten`.
    We often write this with implicit bias terms as *f*(**x**) = tanh (**x**⊛**W**^((*h*[1])))**W**^((out)).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为 *展平*，是现代深度学习中的一种常见操作。我们可以认为上一页的方程 3.1 隐含地使用了这种展平操作。PyTorch 提供了一个名为 `nn.Flatten`
    的 `module` 来执行此操作。我们通常将带有隐式偏差项的 *f*(**x**) = tanh (**x**⊛**W**^((*h*[1])))**W**^((out))
    写作。
- en: 3.4.4  PyTorch code for our first CNN
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.4  PyTorch 首个 CNN 的代码
- en: 'Let’s finally define some code for training this CNN-based model. First we
    need to grab the CUDA compute device and create a `DataLoader` for the training
    and testing sets. We use a batch size B of 32:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们最终定义一些用于训练基于 CNN 的模型的代码。首先，我们需要获取 CUDA 计算设备并为训练集和测试集创建一个 `DataLoader`。我们使用批大小
    B 为 32：
- en: '[PRE16]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now we will define some variables. Again, because PyTorch works in batches of
    data, when we think in a PyTorch context, our tensor shapes begin with B; and
    since the input consists of images, the initial shape is (*B*,*C*,*W*,*H*). We
    have *B* = 32 because we defined that and *C* = 1 because MNIST is black and white.
    We’ll define a few helper variables like `K` to represent our filter size and
    `filters` for how many filters we wantto build.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将定义一些变量。同样，由于 PyTorch 在数据批次中工作，当我们从 PyTorch 的角度思考时，我们的张量形状以 B 开头；由于输入由图像组成，初始形状是
    (*B*,*C*,*W*,*H*)。我们定义了 *B* = 32，因为我们定义了它，*C* = 1，因为 MNIST 是黑白图像。我们将定义一些辅助变量，如
    `K` 来表示我们的滤波器大小，以及 `filters` 来表示我们想要构建的滤波器数量。
- en: 'The first model is `model_linear` because it uses only `nn.Linear` layers.
    It begins with calling `nn.Flatten()`. Notice the specific comment we put into
    the code `#(B, C, W, H) -> (B, C*W*H) = (B,D)`: this is to remind us that we are
    changing the shape of the tensor using this operation. The original shape (*B*,*C*,*W*,*H*)
    is on the left, and the new shape (*B*,*C*×*W*×*H*) is on the right. Since we
    have the variable D to represent the total number of features, we also include
    a note of the value it is equal to: `=(B,D)`. It is very easy to lose track of
    the shapes of tensors when writing code, and this is one of the easiest ways to
    introduce bugs. I always include comments like this when the shape is altered
    by a tensor.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个模型是 `model_linear`，因为它只使用了 `nn.Linear` 层。它从调用 `nn.Flatten()` 开始。注意我们在代码中放入的特定注释
    `#(B, C, W, H) -> (B, C*W*H) = (B,D)`：这是为了提醒我们，我们正在使用这个操作改变张量的形状。原始形状 (*B*,*C*,*W*,*H*)
    在左边，新的形状 (*B*,*C*×*W*×*H*) 在右边。由于我们有变量 D 来表示特征的总数，我们还包括了一个关于它等于什么值的注释：`=(B,D)`。在编写代码时，很容易丢失张量的形状，这是引入错误的最简单方法之一。当张量的形状被改变时，我总是包括这样的注释。
- en: '[PRE17]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '❶ We use the number of values in the input to help determine the size of subsequent
    layers: 28 * 28 images.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们使用输入中的值数来帮助确定后续层的大小：28 * 28 图像。
- en: ❷ How many channels are in the input?
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 输入中有多少个通道？
- en: ❸ How many classes are there?
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 有多少个类别？
- en: ❹ How many filters should we use?
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们应该使用多少个过滤器？
- en: ❺ How large should our filters be?
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 我们应该使用多大的过滤器？
- en: ❻ For comparison, let’s define a linear model of similar complexity.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 为了比较，让我们定义一个类似复杂性的线性模型。
- en: '❼ Simple convolutional network. Conv2d follows the pattern Conv2d(# of input
    channels, #filtersoutput-channels, #filter-size).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 简单的卷积网络。Conv2d 按照模式 Conv2d（输入通道数，输出通道数，滤波器大小）。
- en: ❽ *x* ⊛ *G*
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ *x* ⊛ *G*
- en: ❾ Activation functions work on any size tensor.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 激活函数作用于任何大小的张量。
- en: ❿ Converts from (B, C, W, H) ->(B, D) so we can use a Linear layer.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 从 (B, C, W, H) 转换为 (B, D)，这样我们就可以使用线性层。
- en: '`model_linear` is a simple fully connected layer for us to compare against.
    Our first CNN is defined by `model_cnn`, where we use the `nn.Conv2d` module to
    input a convolution. Then we can apply our nonlinear activation function tanh
    just like before. We only flatten the tensor once we are ready to use a `nn.Linear`
    layer to reduce the tensor to a set of predictions for each class. That’s why
    the `nn.Flatten()` module occurs right before the call to `nn.Linear`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`model_linear` 是一个简单的全连接层，我们可以用它来比较。我们的第一个 CNN 由 `model_cnn` 定义，我们使用 `nn.Conv2d`
    模块输入一个卷积。然后我们可以像以前一样应用我们的非线性激活函数 tanh。我们只有在准备好使用 `nn.Linear` 层将张量减少为每个类的一组预测时才对张量进行一次展平。这就是为什么
    `nn.Flatten()` 模块出现在调用 `nn.Linear` 之前的原因。'
- en: 'Does a CNN perform better than a fully connected model? Let’s find out. We
    can train both a CNN and a fully connected model, measure accuracy on the test
    set, and look at the accuracy after each epoch:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 的表现是否优于全连接模型？让我们来看看。我们可以训练一个 CNN 和一个全连接模型，在测试集上测量精度，并查看每个时期的精度：
- en: '[PRE18]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/CH03_UN10_Raff.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_UN10_Raff.png)'
- en: 'One epoch of training our CNN has *better accuracy* than our fully connected
    network ever achieves. While our CNN is about 1.138× slower to train, the results
    are well worth it. Why does it perform so much better? Because we have given the
    network information about the problem (convolutions) via the structure of the
    domain (data consists of images). This does not mean CNNs are always better: if
    the assumptions for a CNN are not true or accurate, they will not perform well.
    Remember that convolutions impart the prior belief that things located near each
    other are related, but things far from each other are not related.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 CNN 训练的一个时期比我们全连接网络所能达到的 *更好精度*。虽然我们的 CNN 训练速度大约慢了 1.138 倍，但结果是非常值得的。为什么它表现得如此出色？因为我们通过域的结构（数据由图像组成）向网络提供了关于问题的信息（卷积）。这并不意味着
    CNN 总是更好的：如果 CNN 的假设不真实或不准确，它们将不会表现良好。记住，卷积传递了这样的先验信念：彼此靠近的事物是相关的，而彼此远离的事物则不是相关的。
- en: 3.5 Adding pooling to mitigate object movement
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 添加池化以减轻物体移动
- en: Like feed-forward networks, we can make convolutional networks more powerful
    by stacking more layers with nonlinearities inserted in between. But before we
    do that, there is a special type of layer we like to use with CNNs called a *pooling*
    layer.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 与前馈网络一样，我们可以通过堆叠更多带有非线性插入的层来使卷积网络更强大。但在我们这样做之前，我们喜欢与 CNN 一起使用的特殊类型的层，称为 *池化*
    层。
- en: 'Pooling helps us solve the issue that we aren’t fully exploiting the spatial
    nature of our data. That may seem confusing: we just significantly increased our
    accuracy with a simple switch to `nn.Conv2d`, and we spent a lot of time talking
    about how convolutions encode this spatial prior by sliding a set of weights over
    the input and applying them at every location. The problem is that we *eventually*
    switch to using a fully connected layer, which does not understand the spatial
    nature of the data. For this reason, the `nn.Linear` layer learns to look for
    values (or objects) at *very specific* locations.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 池化帮助我们解决了我们没有充分利用数据空间性质的问题。这可能看起来有些令人困惑：我们刚刚通过简单地切换到`nn.Conv2d`显著提高了准确性，并且我们花了大量时间讨论卷积如何通过在输入上滑动一组权重并在每个位置应用它们来编码这种空间先验。问题是，我们*最终*切换到使用全连接层，它不理解数据的空间性质。因此，`nn.Linear`层学会在*非常特定*的位置寻找值（或对象）。
- en: 'This is not a huge problem for MNIST because all the digits are aligned so
    they are in the center of the image. But imagine a digit that is not perfectly
    center-aligned with your image. This is a very real potential problem—and pooling
    helps us solve it. Let’s quickly grab an image from the MNIST dataset and create
    two altered versions by moving the content *ever so slightly* up or down by one
    pixel:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 对于MNIST来说这不是一个大问题，因为所有的数字都是对齐的，所以它们位于图像的中心。但想象一下，一个数字没有与你的图像完美对齐的情况。这是一个非常真实的问题——池化可以帮助我们解决这个问题。让我们快速从MNIST数据集中获取一张图片，并通过将其内容向上或向下移动一个像素来创建两个修改过的版本：
- en: '[PRE19]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Moves to the lower right, then upper left
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 移动到右下角，然后是左上角
- en: ❷ Plots the images
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 绘制图像
- en: '![](../Images/CH03_UN11_Raff.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_UN11_Raff.png)'
- en: Clearly, all three versions of the image are the same digit. It does not matter
    that we shifted the content up or down, left or right, by just a few pixels. *But
    our model does not know this*. If we classify different versions of image, there
    is a good chance we will get it wrong.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，所有三个版本的图像都是相同的数字。我们向上或向下、向左或向右移动内容仅几个像素并不重要。*但我们的模型并不知道这一点*。如果我们对图像的不同版本进行分类，有很大可能会出错。
- en: 'Let’s quickly put this model into `eval()` mode and write a function to get
    the predictions for a single image. That happens with the below `pred` function,
    which takes an image as input:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速将这个模型放入`eval()`模式，并编写一个函数来获取单个图像的预测。这发生在下面的`pred`函数中，它接受一个图像作为输入：
- en: '[PRE20]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ eval mode since we are not training
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 由于我们不在训练模式
- en: ❷ Always turn off gradients when evaluating.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 评估时始终关闭梯度。
- en: ❸ Finds the width/height of the image
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 找到图像的宽度和高度
- en: ❹ Reshapes it as (B, C, W, H)
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将其重塑为(B, C, W, H)
- en: ❺ Gets the logits
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 获取logits
- en: ❻ Turns the logits into probabilities
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将logits转换为概率
- en: ❼ Converts the prediction to a NumPy array
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 将预测转换为NumPy数组
- en: This is a simple way to apply a model to *single* images. PyTorch always expects
    things to be in batches, so we reshape the input to have a batch dimension, which
    is equal to 1 since there are no other images. The `if not isinstance` check is
    some defensive code you can add to make sure your code works for both NumPy and
    PyTorch input tensors. Also remember that the `CrossEntropy` loss we used handles
    softmax implicitly. So when we use a model trained with `CrossEntropy`, we need
    to call `F.softmax` to transform the outputs into probabilities.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种简单的方法将模型应用于*单个*图像。PyTorch始终期望事物以批量的形式存在，所以我们重新调整输入以包含一个批处理维度，由于没有其他图像，这个维度等于1。`if
    not isinstance`检查是一些防御性代码，你可以添加以确保你的代码对NumPy和PyTorch输入张量都有效。还要记住，我们使用的`CrossEntropy`损失函数隐式地处理softmax。所以当我们使用`CrossEntropy`训练的模型时，我们需要调用`F.softmax`将输出转换为概率。
- en: 'With that out of the way, we can get predictions for all three images to see
    if tiny changes to the image can significantly change the network’s prediction.
    Remember, each image differs by shifting the image to the lower right or upper
    left by one pixel. Intuitively, we expect very little to change:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 清理完这些，我们可以为三张图片都获取预测结果，看看图像的微小变化是否可以显著改变网络的预测。记住，每张图像都通过将图像向右下角或左上角移动一个像素来有所不同。直观上，我们预期变化非常小：
- en: '[PRE21]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Clearly, we want all three examples to receive the same classification. They
    are *essentially* the same image, yet the outputs swing from a reasonably confident
    and correct 78.2% down to an erroneous 31.5%. The problem is that a small shift
    or translation causes the predictions to change significantly.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们希望所有三个示例都得到相同的分类。它们*本质上*是相同的图像，但输出从合理的自信和正确的78.2%下降到错误的31.5%。问题是，微小的移动或平移会导致预测发生显著变化。
- en: What we desire is a property called *translation invariance*. Being invariant
    to property X means our output does not change based on changes to X. We do not
    want translations (shifting up/down) to change our decisions—we want to be translation
    invariant.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望的是一个称为*平移不变性*的性质。对属性X的不变性意味着我们的输出不会根据X的变化而变化。我们不希望平移（上下移动）改变我们的决策——我们希望具有平移不变性。
- en: Pooling can enable us to obtain partial translation invariance. Specifically,
    we will look at *max pooling*. What is max pooling? Similar to convolution, we
    apply the same function to multiple locations in an image. We generally stick
    to even-sized pooling filters. As the name implies, we slide the `max` function
    around the image. You can describe this as having a kernel size K, which is the
    size of the window from which to select the maximum. The big difference here is
    that we move the `max` function by K pixels at a time, where we moved only 1 pixel
    at a time when performing a convolution (seefigure 3.14).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 池化可以使我们获得部分平移不变性。具体来说，我们将查看*最大池化*。什么是最大池化？与卷积类似，我们在图像的多个位置应用相同的函数。我们通常坚持使用偶数大小的池化滤波器。正如其名所示，我们滑动`max`函数在图像周围。你可以将其描述为具有内核大小K，这是从中选择最大值的窗口大小。这里的大不同之处在于，我们每次移动`max`函数时移动K像素，而在执行卷积时我们每次只移动1像素（见图3.14）。
- en: '![](../Images/CH03_F14_Raff.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F14_Raff.png)'
- en: Figure 3.14 Example of max pooling for *K* = 2 (top) and *K* = 3 (bottom). Each
    region of the input (left) has a color indicating the group of pixels participating
    in the pooling, and the output (right) shows which value was selected from the
    input region. Note that the output is smaller than the input by a factor of K.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14展示了最大池化的示例，其中*K* = 2（顶部）和*K* = 3（底部）。输入的每个区域（左侧）都有颜色表示参与池化的像素组，输出（右侧）显示从输入区域选择了哪个值。请注意，输出比输入小K倍。
- en: The choice of how many pixels to slide by is called the *stride*. By default,
    practitioners tend to use `stride=1` for convolutions so that *every* possible
    location is evaluated. We use `stride=K` for pooling so the input is shrunk by
    a factor of K. For any operation, if you use `stride=Z` for any (positive integer)
    value of Z, the result will be smaller by a factor of Z along each dimension.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 选择滑动多少像素的选择称为*步长*。默认情况下，从业者倾向于使用`stride=1`进行卷积，以便评估*每个*可能的位置。我们使用`stride=K`进行池化，使输入缩小K倍。对于任何操作，如果你使用`stride=Z`（对于任何正整数Z）的值，结果将沿每个维度缩小Z倍。
- en: The intuition behind pooling is that it gives us more robustness to slight changes
    in values. Consider the upper-left part of figure 3.14 if you shifted every value
    over to the right by one position. Five of the output values *would not change*,
    giving the pooling operation a minor degree of *invariance* to *translating* the
    image by one pixel. This is not perfect, but it helps reduce the impact of such
    alterations. If we accumulate this effect through multiple rounds of pooling,
    we can make the effect stronger.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 池化的直觉在于它使我们对值的微小变化具有更强的鲁棒性。考虑图3.14的左上角，如果你将每个值向右移动一个位置。五个输出值*不会改变*，这给池化操作带来了一定程度的对*平移*图像一个像素的*不变性*。这并不完美，但它有助于减少这种变化的影响。如果我们通过多轮池化累积这种效果，我们可以使效果更强。
- en: Just like before, PyTorch provides `nn.MaxPool1d`, `nn.MaxPool2d`, and `nn.MaxPool3d`
    for almost all your needs. The function takes the kernel size as input, which
    is also the stride. Having a stride of K means we shrink the size of each shape
    dimension by a factor of K. So if our input has a shape of (*B*,*C*,*W*,*H*),
    the output of `nn.MaxPool2d(K)` will be a shape of (*B*, *C*, *W*/*K*, *H*/*K*).
    Since C remains the same, we apply this simple operation to every channel independently.
    If we do max pooling with a 2 × 2 filter (the norm for most applications), we
    end up with an image one-fourth the size (half as many rows and half as many columns).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 就像之前一样，PyTorch提供了`nn.MaxPool1d`、`nn.MaxPool2d`和`nn.MaxPool3d`来满足你几乎所有的需求。该函数接受内核大小作为输入，这同时也是步长。步长为K意味着我们将每个形状维度的尺寸缩小K倍。所以如果我们的输入形状为(*B*,*C*,*W*,*H*)，`nn.MaxPool2d(K)`的输出形状将为(*B*,
    *C*, *W*/*K*, *H*/*K*）。由于C保持不变，我们独立地对每个通道应用这个简单的操作。如果我们使用2 × 2的滤波器（大多数应用的规范）进行最大池化，我们最终得到一个四分之一大小的图像（行数和列数减半）。
- en: 3.5.1  CNNs with max pooling
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.1  CNNs with max pooling
- en: 'It’s easy to add pooling to our model’s definition: just insert `nn.MaxPool2d(2)`
    into `nn.Sequential`. But *where* should we use max pooling? First, let’s talk
    bout *how many times* to apply max pooling. Every time pooling is applied, we
    shrink the width (and height, if 2D) by a factor of K. So n rounds of pooling
    means shrinking by a factor of *K*^n, which will make the image very small very
    quickly. For MNIST, we only have 28 pixels of width, so we can do at most four
    rounds of pooling with a size of *K* = 2. That’s because five rounds would give
    us 28/2⁵ = 28/32, which is *less than a pixel of output*.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 向我们的模型定义中添加池化很容易：只需将`nn.MaxPool2d(2)`插入到`nn.Sequential`中即可。但*在哪里*使用最大池化呢？首先，让我们谈谈*应用多少次*最大池化。每次应用池化，都会将宽度（以及高度，如果是二维的话）缩小K倍。所以n轮池化意味着缩小到*K*ⁿ倍，这将使图像非常小。对于MNIST，我们的宽度只有28像素，所以我们可以使用大小为*K*
    = 2的最大池化进行最多四轮。这是因为五轮将给我们28/2⁵ = 28/32，这*小于一个像素的输出*。
- en: Does doing four rounds of pooling make more sense? Try to imagine if it was
    a problem *you* were asked to solve. Four rounds of pooling means shrinking the
    image to just 28/2⁴ = 28/16 = 1.75 pixels tall. If you could not guess what digit
    was being represented with 1.75 pixels, your CNN probably couldn’t either. Visually
    shrinking your data is a good way to estimate the maximum amount of pooling you
    should apply to most problems. Using two or three rounds of pooling is a good
    initial lower bound or estimate for images up to 256 × 256 pixels.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 进行四轮池化是否更有意义？试着想象一下，如果这个问题是你被要求解决的。四轮池化意味着将图像缩小到仅28/2⁴ = 28/16 = 1.75像素高。如果你不能猜出用1.75像素表示的数字是什么，那么你的CNN可能也无法做到。通过视觉上缩小数据是一种很好的方法来估计你应该应用于大多数问题的最大池化量。对于高达256
    × 256像素的图像，使用两到三轮池化是一个良好的初始下限或估计。
- en: Note Most modern applications of CNNs are on images smaller than 256 × 256.
    That’s very large to process with modern GPUs and techniques. In practice, if
    your images are larger than that, the first step is to resize them to have at
    most 256 pixels along any dimension. If you really *need* to process at a larger
    resolution, you probably want someone on your team with relevant experience, because
    the tricks of working at that scale are unique and often require very expensive
    hardware.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：大多数现代CNN应用都是在小于256 × 256的图像上。这对于现代GPU和技术来说处理起来非常大。实际上，如果你的图像大于这个尺寸，第一步是将它们调整大小，使任何维度上的像素数最多为256。如果你真的*需要*在更高的分辨率下处理，你可能需要团队中有相关经验的人，因为在这个规模上工作的技巧是独特的，并且通常需要非常昂贵的硬件。
- en: Every application of pooling shrinks the image by a factor of K, which also
    means the network has less data to process after every round of pooling. If you
    are working with very large images, pooling can help reduce the time it takes
    to train larger models and the memory costs of training. If neither of these is
    a problem, common practice is to increase the number of filters by *K*× after
    every round of pooling so that the total computation done at every layer remains
    roughly the same (i.e., twice as many filters on half as many rows/columns about
    balances out).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 每次应用池化都会将图像缩小K倍，这也意味着在每轮池化之后，网络处理的数据更少。如果你处理的是非常大的图像，池化可以帮助减少训练更大模型所需的时间和训练的内存成本。如果你不认为这些问题是问题，那么在每轮池化之后增加滤波器的数量是常见的做法，增加的倍数为*K*，这样每层的总计算量大致保持不变（即，在行/列数量减半的情况下，增加两倍的滤波器数量可以平衡）。
- en: 'Let’s quickly try this on our MNIST data. The following code defines a deeper
    CNN with multiple layers of convolution and two rounds of max pooling:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速在我们的MNIST数据上尝试一下。以下代码定义了一个具有多层卷积和两轮最大池化的更深的CNN：
- en: '[PRE22]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Why reduce the number of units into the Linear layer by a factor of 4²? Because
    pooling a 2×2 grid down to one value means going from four values to one, and
    we do this twice.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为什么要将线性层中的单元数量减少到4²倍？因为将2×2网格池化到单个值意味着从四个值减少到一个，我们这样做两次。
- en: 'Now, if we run the same shifted test image through our model, we should see
    different results. Max pooling is not a *perfect* solution to the translation
    problem, so the scores for each shifted version of the image still change. But
    they don’t change *as much*. That is overall a good thing because it makes our
    model more *robust* to real-life issues. Data will not always be perfectly aligned,
    so we want to make the model resilient to the kinds of issues we expect to see
    in real-life test data:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们将相同的偏移测试图像通过我们的模型，我们应该看到不同的结果。最大池化并不是解决翻译问题的**完美**解决方案，因此图像每个偏移版本的分数仍然会变化。但它们的变化**不那么大**。这总体上是一件好事，因为它使我们的模型对现实生活中的问题更加**鲁棒**。数据并不总是完美对齐，因此我们希望模型能够对我们在现实生活中的测试数据中预期看到的问题具有弹性：
- en: '[PRE23]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we can look at the accuracy of this new and larger network we trained,
    which is shown in the following plot. Adding more layers causes our network to
    take *much* longer to converge, but once it does, it can eke out a little better
    accuracy:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以查看我们训练的这个新的大网络的准确性，如下面的图表所示。增加更多层导致我们的网络收敛需要**更长**的时间，但一旦收敛，它就能获得略微更好的准确性：
- en: '[PRE24]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../Images/CH03_UN12_Raff.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH03_UN12_Raff.png)'
- en: It is common for using more layers to cause training to take longer to converge
    *and* longer to process for each epoch. This double whammy is offset only by the
    fact that using more layers—making models *deeper*—is how we tend to obtain the
    best possible accuracy. If you continued to train this deeper model for more epochs,
    you will see it continue to climb higher than we could achieve with our first
    model that contained only one `nn.Conv2d` layer.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更多层通常会导致训练需要更长的时间来收敛，并且每个epoch的处理时间也会更长。这种双重打击只有通过使用更多层——使模型**更深**——才能得到缓解，这是我们通常获得最佳可能准确性的方法。如果你继续对这个更深的模型进行更多epoch的训练，你会看到它的准确度会继续提高，这超出了我们最初只包含一个`nn.Conv2d`层的模型所能达到的水平。
- en: Warning Just like ice cream, you can have too much of a good thing and make
    a network too deep to learn. In chapters 5 and 6, you’ll learn improved techniques
    that can help you build networks with as many as 100 to 200 layers—and that’s
    about the limit on how deep we can train networks today while still obtaining
    some benefit. Note that 5 to 20 layers are often good enough, and depth is always
    a tradeoff between compute cost and diminishing returns.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 就像冰淇淋一样，好事做得太多也会适得其反，使得网络太深以至于难以学习。在第5章和第6章中，你将学习到改进的技术，这些技术可以帮助你构建多达100到200层的网络——这大约是我们今天在仍然获得一些好处的情况下可以训练网络的深度极限。请注意，5到20层通常已经足够，深度始终是计算成本和递减回报之间的权衡。
- en: As we continue in this book, we’ll learn about newer and better approaches that
    help resolve these issues and make convergence faster and better. But I want to
    take you on this slower and somewhat more painful path so you understand *why*
    these newer techniques were developed and what problems they solve. This deeper
    understanding will help you be better prepared for the more advanced techniques
    we tackle by the time you finish the book.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们继续阅读本书，我们将了解一些新的、更好的方法，这些方法有助于解决这些问题，并使收敛更快、更好。但我希望带你走一条更慢、也有些痛苦的路径，这样你就能理解**为什么**这些新技术被开发出来，以及它们解决了哪些问题。这种更深入的理解将帮助你为在完成本书时我们将要处理的更高级技术做好准备。
- en: 3.6 Data augmentation
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6 数据增强
- en: 'It may seem a little anticlimactic, but you now know everything you need to
    start training and building CNNs for new problems. All it took to implement a
    CNN in PyTorch was replacing `nn.Linear` layers with `nn.Conv2d`, followed by
    a `nn.Flatten` before the end. But there is one more big secret to applying CNNs
    in practice: using *data augmentation*. In general, neural networks are data-hungry,
    meaning they learn best when you have a *huge* amount of *diverse* data. Since
    it takes time to obtain data, we will instead *augment* our real data by creating
    new, fake data based on the real data.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可能有些令人失望，但现在你已经知道了一切，可以开始训练和构建用于新问题的CNN了。在PyTorch中实现CNN只需将`nn.Linear`层替换为`nn.Conv2d`，然后在结束时之前进行`nn.Flatten`。但应用CNN的实践中还有一个更大的秘密：使用**数据增强**。一般来说，神经网络是数据饥渴的，这意味着当你有大量**多样化**的数据时，它们学得最好。由于获取数据需要时间，因此我们将通过根据真实数据创建新的、虚假的数据来**增强**我们的真实数据。
- en: 'The idea is simple. If we are working with 2D images, we can apply many transforms
    to an image that do not change the meaning of its content but alter the pixels.
    For example, we can rotate the image by a few degrees without altering what the
    content means. PyTorch provides a number of transforms in the `torchvision.transforms`
    package; let’s take a look at some of them:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法很简单。如果我们处理的是2D图像，我们可以应用许多转换到图像上，这些转换不会改变其内容的含义，但会改变像素。例如，我们可以旋转图像几度而不改变其内容的含义。PyTorch在`torchvision.transforms`包中提供了一系列转换；让我们看看其中的一些：
- en: '[PRE25]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Built-in transformations given aggressive values to make their impact obvious
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 给出内置转换的激进值以使它们的影响明显
- en: ❷ Converts the Tensor image back to a PIL image using a transform
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用转换将张量图像转换回PIL图像
- en: ❸ Plots a random application of each transform
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 绘制每个转换的随机应用
- en: '![](../Images/CH03_UN13_Raff.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_UN13_Raff.png)'
- en: The first thing you should notice is that transforms are almost always *randomized*,
    and every time we apply one, it gives a different result. These new results are
    our augmented data. For example, specifying `degrees=45` says the maximum rotation
    is ± 45^∘ degrees, and the amount applied is a randomly chosen value in that range.
    This is done to increase the *diversity* of inputs our model sees. Some transforms
    do not always apply themselves and offer the `p` argument to control the probability
    of being chosen. We set these as `p=1.0` so you would definitely see them have
    an impact on the test image. For real use, you probably want to pick a value of
    `p=0.5` or `p=0.15`. Like many things, the specific value to use will depend on
    your data.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该注意的第一件事是，转换几乎总是**随机化**的，每次我们应用一个转换，它都会给出不同的结果。这些新结果就是我们的增强数据。例如，指定`degrees=45`表示最大旋转是±45^∘度，应用的数量是随机选择该范围内的值。这样做是为了增加模型看到的输入的**多样性**。一些转换并不总是应用自己，并提供`p`参数来控制被选中的概率。我们将这些设置为`p=1.0`，这样你肯定会看到它们对测试图像有影响。对于实际使用，你可能想要选择`p=0.5`或`p=0.15`的值。像许多事情一样，要使用的具体值将取决于你的数据。
- en: 'Not every transform should *always* be used. Make sure that your transforms
    preserve the essence or meaning of your data. For example, horizontal and vertical
    flips are not a good idea for the MNIST dataset: a vertical flip applied to the
    digit 9 could turn it into a 6, and that *changes the meaning of the image*. Your
    best bet for selecting a good set of transforms is to apply them to data and *look
    at the results yourself*; if you can’t tell what the correct answer is anymore,
    chances are your CNN can’t either.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每个转换都应该**总是**被使用。确保你的转换保留了数据的本质或含义。例如，水平和垂直翻转对于MNIST数据集来说不是一个好主意：对数字9进行垂直翻转可能会将其变成6，这会**改变图像的含义**。选择一组好的转换的最佳方法是应用它们到数据上，并**亲自查看结果**；如果你不能再判断正确的答案是什么，那么你的CNN可能也做不到。
- en: 'But once you select a set of transforms you are comfortable with, this is a
    simple and powerful approach to improve your model’s accuracy. Following is a
    short example of using the `Compose` transform to create a sequence of transforms
    in a larger pipeline, which we can apply to augment training data on the fly.
    All the image-based datasets PyTorch provides have the `transform` argument so
    that you can perform these alterations:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 但一旦你选择了一组你感到舒适的转换，这是一个简单而强大的方法来提高你模型的准确性。以下是一个使用`Compose`转换在更大管道中创建转换序列的简短示例，我们可以将其应用于即时增强训练数据。PyTorch提供的所有基于图像的数据集都有`transform`参数，这样你就可以执行这些更改：
- en: '[PRE26]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Note A new and important optional argument has been specified in the`DataLoader`
    class: the `num_workers` flag controls how many threads are used to *preload*
    batches of data for training. While the GPU is busy crunching a batch of data,
    each thread can prepare the next batch to be ready to go when the GPU is done.
    You should always use this flag because it helps you use your GPU more efficiently.
    It’s essential when you start using transforms because the CPU will have to spend
    time processing the images, which keeps your GPU idly waiting.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在`DataLoader`类中指定了一个新的重要可选参数：`num_workers`标志控制用于**预加载**训练数据批次的线程数量。当GPU忙于处理数据批次时，每个线程可以准备下一个批次，以便GPU完成时可以立即使用。你应该始终使用此标志，因为它有助于你更有效地使用GPU。当你开始使用转换时，这是至关重要的，因为CPU将不得不花费时间处理图像，这会让GPU闲置等待。
- en: 'Now we can redefine the same network we used to show max pooling and call the
    same training method. The data augmentation happens automatically by defining
    these new data loaders. A simple `ToTensor` transform is all we use for the test
    set because we want the test set to be *deterministic*—that means if we run the
    same model five times on the test set, we get the same answer five times:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以重新定义之前用来展示最大池化的相同网络，并调用相同的训练方法。数据增强会自动通过定义这些新的数据加载器来实现。对于测试集，我们只使用简单的`ToTensor`转换，因为我们希望测试集是**确定性的**——这意味着如果我们对测试集运行相同的模型五次，我们将得到相同的答案五次：
- en: '[PRE27]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can now plot the result showing the difference in validation accuracy. With
    a careful choice of augmentation, we helped our model learn faster and converge
    to a better-quality solution, 96.2% accuracy instead of 95.7%:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以绘制结果图，展示验证准确率的变化。通过仔细选择增强，我们帮助模型更快地学习并收敛到更好的解决方案，准确率达到96.2%，而不是95.7%：
- en: '[PRE28]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![](../Images/CH03_UN14_Raff.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_UN14_Raff.png)'
- en: Designing good data augmentation pipelines is the feature engineering counterpart
    to deep learning. If you do it well, it can have a massive impact on your results
    and be the difference between success and failure. Because PyTorch uses PIL images
    as its foundation, you can also write custom transforms to add into the pipeline.
    This is where you can import tools like `scikit-image` that provide more advanced
    computer vision transforms that you can apply. The impact of good data augmentation
    will also grow as we learn how to build more sophisticated networks and when you
    work on more complex datasets than MNIST. Data augmentation also increases the
    value of training for more epochs. Without augmentation, each epoch revisits the
    exact same data; with augmentation, your model sees a different variant of the
    data that helps it better generalize to new data. Our 10 epochs aren’t enough
    to see the full benefit.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 设计良好的数据增强管道是深度学习中的特征工程对应物。如果你做得好，它可以对你的结果产生巨大影响，并成为成功与失败的区别。因为PyTorch以PIL图像为基础，你也可以编写自定义转换并将其添加到管道中。这就是你可以导入像`scikit-image`这样的工具的地方，它提供了更多高级计算机视觉转换，你可以应用。随着我们学习如何构建更复杂的网络，以及当你处理比MNIST更复杂的数据集时，良好的数据增强的影响也会增长。数据增强还会增加更多轮次训练的价值。没有增强，每个轮次都会重新访问完全相同的数据；有了增强，你的模型会看到数据的不同变体，这有助于它更好地泛化到新数据。我们的10个轮次不足以看到全部的好处。
- en: 'Despite the importance of good data augmentation, you can go far in deep learning
    without it. We will not use data augmentation for most of this book: part of the
    reason is to keep our examples simpler without having to explain the choices of
    data augmentation pipelines for every new dataset, and the other reason is that
    augmentation is *domain-specific*. What works for images *only* works for images.
    You need to come up with a new set of transforms for audio data, and text is a
    difficult realm in which to perform augmentation. Most of the techniques we will
    learn in this book can be applied to a fairly broad class of problems.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管良好的数据增强非常重要，但在深度学习中即使没有它也能走得很远。在这本书的大部分内容中，我们不会使用数据增强：部分原因是保持示例简单，无需为每个新的数据集解释数据增强管道的选择，另一个原因是增强是**领域特定的**。对图像**仅**有效的增强方法对图像有效。你需要为音频数据想出一套新的转换，而在文本领域进行增强是困难的。在这本书中我们将学习的大部分技术都可以应用于相当广泛的类别问题。
- en: Exercises
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Share and discuss your solutions on the Manning online platform at Inside Deep
    Learning Exercises ([https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945)).
    Once you submit your own answers, you will be able to see the solutions submitted
    by other readers, and see which ones the author judges to be the best.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在Manning在线平台Inside Deep Learning Exercises（[https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945)）上分享和讨论你的解决方案。一旦你提交了自己的答案，你将能够看到其他读者提交的解决方案，并看到哪些是作者认为最好的。
- en: Note that these exercises are intentionally *exploratory* in nature. The goal
    is for you to learn about and discover a number of common trends and properties
    for working with CNNs through your own code and experience.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这些练习在本质上是有意**探索性的**。目标是让你通过自己的代码和经验了解和发现许多与CNN工作相关的常见趋势和属性。
- en: Try training all the networks in this chapter for 40 epochs instead of 10\.
    What happens?
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试将本章中所有网络的训练轮数从10轮增加到40轮。会发生什么？
- en: Load the CIFAR10 dataset from `torchvision`, and try to build your own CNN.
    Try using 2 to 10 layers of convolutions and 0 to 2 rounds of max pooling. What
    seems to work best?
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `torchvision` 加载 CIFAR10 数据集，并尝试构建自己的卷积神经网络。尝试使用 2 到 10 层的卷积和 0 到 2 轮的最大池化。什么看起来效果最好？
- en: Go through the transforms provided and see which makes sense for CIFAR10 via
    visual inspection. Do any transforms work for CIFAR10 that do not make sense for
    MNIST?
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查提供的转换，通过视觉检查看看哪些对 CIFAR10 有意义。是否有任何对 CIFAR10 有用但对 MNIST 不合理的转换？
- en: Train a new CIFAR10 model with the transforms you selected. What impact does
    this have on accuracy?
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你选择的转换训练一个新的 CIFAR10 模型。这对准确率有什么影响？
- en: Try altering the size of the convolutional filters in CIFAR10 and MNIST. What
    impact does it have?
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试改变 CIFAR10 和 MNIST 中卷积滤波器的大小。这有什么影响？
- en: Create a new custom `Shuffle` transform that applies the same fixed reordering
    of the pixels in an image. How does using this transform impact your CIFAR10 model?
    *Hint:* Look at the `Lambda` transform to help you implement this.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的自定义 `Shuffle` 转换，该转换应用于图像中像素的相同固定重排序。使用这个转换如何影响你的 CIFAR10 模型？**提示**：查看
    `Lambda` 转换来帮助你实现这一点。
- en: Summary
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: To represent images in a PyTorch tensor, we use multiple channels, where the
    channel describes something of a different nature about the image (e.g., red,
    green, and blue channels).
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 PyTorch 张量中表示图像时，我们使用多个通道，其中通道描述了图像的某种不同性质（例如，红色、绿色和蓝色通道）。
- en: Convolution is a mathematical operation that takes a kernel (a small tensor)
    and applies a function over every location in a larger input tensor to produce
    an output. This makes convolutions *spatial* in nature.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积是一种数学运算，它将一个核（一个小张量）应用于较大输入张量中的每个位置，以产生输出。这使得卷积在本质上具有**空间**性。
- en: A convolutional layer learns multiple different kernels to apply to the input
    to create multiple outputs.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层学习多个不同的核，以应用于输入以创建多个输出。
- en: Convolutions don’t capture translation (shifts up or down) invariance, and we
    can use pooling to make our models more robust to translations.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积不能捕捉平移（上下移动）不变性，我们可以使用池化来使我们的模型对平移更加鲁棒。
- en: Both convolutions and pooling have a stride option that dictates how many pixels
    we move when sliding across the image.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积和池化都有步长选项，它决定了我们在图像上滑动时移动多少像素。
- en: When our data is spatial (e.g., images), we can embed a prior (convolutions
    are spatial) into our network to learn faster and better solutions.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们的数据是空间性的（例如，图像）时，我们可以将先验（卷积是空间性的）嵌入到我们的网络中，以更快更好地学习解决方案。
- en: We can augment our training data by selecting a set of transformations to apply
    to the data, improving our model’s accuracy.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过选择应用数据的一组转换来增强我们的训练数据，从而提高模型准确率。
- en: '* * *'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ¹ If you have any friends who call themselves Bayesian, they might take offense
    at this definition, but that’s OK. We are not being Bayesians today. Bayesian
    statistics often involves a more precise definition of a prior; see [http://mng.bz/jjJp](http://mng.bz/jjJp)
    (Scott Lynch, 2007) for an introduction.[↩](#fnref5)
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 如果你有自称贝叶斯的朋友，他们可能会对这个定义感到冒犯，但没关系。我们今天不是在讨论贝叶斯。贝叶斯统计学通常涉及对先验的更精确定义；参见[http://mng.bz/jjJp](http://mng.bz/jjJp)（斯科特·林奇，2007年）以获取介绍。[↩](#fnref5)
- en: ² Different frameworks support different orderings for a variety of nuanced
    reasons that we are not going to get into. Let’s focus on the basics and default
    PyTorch behavior.[↩](#fnref6)
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ² 不同的框架支持不同的排序，原因多种多样，我们不会深入探讨。让我们专注于基础知识以及 PyTorch 的默认行为。[↩](#fnref6)
- en: '³ ⌊*x*⌋ is the floor function that rounds a value down: e.g., ⌊9.9⌋ = 9. The
    ceiling function is ⌈9.1⌉ = 10.[↩](#fnref7)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ³ ⌊*x*⌋ 是向下取整函数，它将一个值向下舍入：例如，⌊9.9⌋ = 9。天花板函数是 ⌈9.1⌉ = 10。[↩](#fnref7)
