- en: Chapter 2\. Introduction to Data Analysis with PySpark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章。使用 PySpark 进行数据分析介绍
- en: Python is the most widely used language for data science tasks. The prospect
    of being able to do statistical computing and web programming using the same language
    contributed to its rise in popularity in the early 2010s. This has led to a thriving
    ecosystem of tools and a helpful community for data analysis, often referred to
    as the PyData ecosystem. This is a big reason for PySpark’s popularity. Being
    able to leverage distributed computing via Spark in Python helps data science
    practitioners be more productive because of familiarity with the programming language
    and presence of a wide community. For that same reason, we have opted to write
    our examples in PySpark.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Python 是数据科学任务中使用最广泛的语言。能够使用同一语言进行统计计算和 web 编程的前景，促成了它在2010年代初期的流行。这导致了一个繁荣的工具生态系统和一个对数据分析非常有帮助的社区，通常被称为
    PyData 生态系统。这是 PySpark 受欢迎的一个重要原因。能够通过 Python 中的 Spark 利用分布式计算帮助数据科学从业者提高生产力，因为他们熟悉这种编程语言并且有一个广泛的社区。出于同样的原因，我们选择在
    PySpark 中编写我们的示例。
- en: It’s difficult to express how transformative it is to do all of your data munging
    and analysis in a single environment, regardless of where the data itself is stored
    and processed. It’s the sort of thing that you have to experience to understand,
    and we wanted to be sure that our examples captured some of that magic feeling
    we experienced when we first started using PySpark. For example, PySpark provides
    interoperability with pandas, which is one of the most popular PyData tools. We
    will explore this feature further in the chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一个环境中完成所有数据清洗和分析工作，不论数据本身存储和处理的位置如何，都会产生多么深远的变革，这是很难用言语表达的。这种感受需要亲身经历才能理解，我们希望我们的示例能够捕捉到我们首次开始使用
    PySpark 时所体验到的那种神奇感觉。例如，PySpark 支持与 pandas 的互操作性，后者是最流行的 PyData 工具之一。我们将在本章进一步探讨这一功能。
- en: In this chapter, we will explore PySpark’s powerful DataFrame API via a data
    cleansing exercise. In PySpark, the DataFrame is an abstraction for datasets that
    have a regular structure in which each record is a row made up of a set of columns,
    and each column has a well-defined data type. You can think of a dataframe as
    the Spark analogue of a table in a relational database. Even though the naming
    convention might make you think of a `pandas.DataFrame` object, Spark’s DataFrames
    are a different beast. This is because they represent distributed datasets on
    a cluster, not local data where every row in the data is stored on the same machine.
    Although there are similarities in how you use DataFrames and the role they play
    inside the Spark ecosystem, there are some things you may be used to doing when
    working with dataframes in pandas or R that do not apply to Spark, so it’s best
    to think of them as their own distinct entity and try to approach them with an
    open mind.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过数据清洗练习探索 PySpark 强大的 DataFrame API。在 PySpark 中，DataFrame 是一种数据集的抽象，具有正则结构，其中每条记录是由一组列组成的行，每列具有明确定义的数据类型。你可以将
    DataFrame 视为 Spark 中关系数据库表的类比。尽管命名惯例可能让你以为是 `pandas.DataFrame` 对象，但 Spark 的 DataFrames
    是完全不同的东西。这是因为它们代表了集群上的分布式数据集，而不是本地数据，其中每一行数据都存储在同一台机器上。尽管在使用 DataFrames 和它们在 Spark
    生态系统中扮演角色的方式上有些相似之处，但在使用 pandas 或 R 中处理数据框时习惯的一些方法在 Spark 中并不适用，因此最好将它们视为独立的实体，并以开放的心态去接触它们。
- en: As for data cleansing, it is the first step in any data science project, and
    often the most important. Many clever analyses have been undone because the data
    analyzed had fundamental quality problems or underlying artifacts that biased
    the analysis or led the data scientist to see things that weren’t really there.
    Hence, what better way to introduce you to working with data using PySpark and
    DataFrames than a data cleansing exercise?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 至于数据清洗，这是任何数据科学项目的第一步，通常也是最重要的一步。许多聪明的分析由于分析的数据存在根本性质量问题或潜在的伪造信息而未能实现。因此，介绍如何使用
    PySpark 和 DataFrames 处理数据的最好方式，不是进行数据清洗练习吗？
- en: First, we will introduce PySpark’s fundamentals and practice them using a sample
    dataset from the University of California, Irvine, Machine Learning Repository.
    We’ll reiterate why PySpark is a good choice for data science and introduce its
    programming model. Then we’ll set up PySpark on our system or cluster and analyze
    our dataset using PySpark’s DataFrame API. Most of your time using PySpark for
    data analysis will center around the DataFrame API, so get ready to become intimately
    familiar with it. This will prepare us for the following chapters where we delve
    into various machine learning algorithms.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍 PySpark 的基础知识，并使用来自加州大学尔湾分校机器学习库的样本数据集进行实践。我们将重申为什么 PySpark 是进行数据科学的良好选择，并介绍其编程模型。然后，我们将在我们的系统或集群上设置
    PySpark，并使用 PySpark 的 DataFrame API 分析我们的数据集。在使用 PySpark 进行数据分析时，你将大部分时间集中在 DataFrame
    API 上，因此准备好对其进行深入了解。这将为我们进入后续章节并深入探讨各种机器学习算法做好准备。
- en: You don’t need to deeply understand how Spark works under the hood for performing
    data science tasks. However, understanding basic concepts about Spark’s architecture
    will make it easier to work with PySpark and make better decisions when writing
    code. That is what we will cover in the next section.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对于执行数据科学任务，你不需要深入理解 Spark 在底层是如何工作的。然而，理解关于 Spark 架构的基本概念将使得在使用 PySpark 时更容易工作，并在编写代码时做出更好的决策。这就是我们将在下一节中讨论的内容。
- en: When using the DataFrame API, your PySpark code should provide comparable performance
    with Scala. If you’re using a UDF or RDDs, you will have a performance impact.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 DataFrame API 时，你的 PySpark 代码应该提供与 Scala 相当的性能。如果使用 UDF 或 RDD，将会影响性能。
- en: Spark Architecture
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 架构
- en: '![aaps 0201](assets/aaps_0201.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![aaps 0201](assets/aaps_0201.png)'
- en: Figure 2-1\. Spark architecture diagram
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1\. Spark 架构图
- en: '[Figure 2-1](#spark_architecture_diagram) depicts the Spark architecture through
    high-level components. Spark applications run as independent sets of processes
    on a cluster or locally. At a high level, a Spark application is comprised of
    a driver process, a cluster manager, and a set of executor processes. The driver
    program is the central component and responsible for distributing tasks across
    executor processes. There will always be just one driver process. When we talk
    about scaling, we mean increasing the number of executors. The cluster manager
    simply manages resources.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-1](#spark_architecture_diagram) 通过高级组件展示了 Spark 架构。Spark 应用程序作为独立的进程集合在集群或本地上运行。在高层次上，一个
    Spark 应用程序由驱动程序进程、集群管理器和一组执行器进程组成。驱动程序是中心组件，负责在执行器进程间分发任务。始终只有一个驱动程序进程。当我们谈论扩展性时，我们指的是增加执行器的数量。集群管理器简单地管理资源。'
- en: Spark is a distributed, data-parallel compute engine. In the data-parallel model,
    more data partitions equals more parallelism. Partitioning allows for efficient
    parallelism. A distributed scheme of breaking up data into chunks or partitions
    allows Spark executors to process only data that is close to them, minimizing
    network bandwidth. That is, each executor’s core is assigned its own data partition
    to work on. Remember this whenever a choice related to partitioning comes up.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是一个分布式、数据并行的计算引擎。在数据并行模型中，更多的数据分区意味着更多的并行性。分区允许高效的并行处理。将数据分解成块或分区的分布式方案允许
    Spark 执行器仅处理靠近它们的数据，从而最小化网络带宽。换句话说，每个执行器的核心被分配到自己的数据分区上进行处理。在涉及分区选择时，请记住这一点。
- en: 'Spark programming starts with a dataset, usually residing in some form of distributed,
    persistent storage like the Hadoop distributed file system (HDFS) or a cloud-based
    solution such as AWS S3 and in a format like Parquet. Writing a Spark program
    typically consists of a few steps:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 编程始于数据集，通常驻留在分布式持久存储中，如 Hadoop 分布式文件系统（HDFS）或云解决方案（例如 AWS S3），并以 Parquet
    格式存储。编写 Spark 程序通常包括以下几个步骤：
- en: Define a set of transformations on the input dataset.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一组对输入数据集的转换。
- en: Invoke actions that output the transformed datasets to persistent storage or
    return results to the driver’s local memory. These actions will ideally be performed
    by the worker nodes, as depicted on the right in [Figure 2-1](#spark_architecture_diagram).
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用将转换后的数据集输出到持久存储或将结果返回到驱动程序本地内存的操作。这些操作理想情况下应由工作节点执行，如 [图 2-1](#spark_architecture_diagram)
    右侧所示。
- en: Run local computations that operate on the results computed in a distributed
    fashion. These can help you decide what transformations and actions to undertake
    next.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行在分布式方式下计算的本地计算。这些计算可以帮助你决定接下来应采取哪些转换和操作。
- en: 'It’s important to remember that all of PySpark’s higher-level abstractions
    still rely on the same philosophy that has been present in Spark since the very
    beginning: the interplay between storage and execution. Understanding these principles
    will help you make better use of Spark for data analysis.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，PySpark 的所有高级抽象仍然依赖于自 Spark 诞生以来存在的相同哲学：存储和执行之间的相互作用。理解这些原则将帮助您更好地利用
    Spark 进行数据分析。
- en: Next, we will install and set up PySpark on our machine so that we can start
    performing data analysis. This is a one-time exercise that will help us run the
    code examples from this and following chapters.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在我们的机器上安装和设置 PySpark，以便我们可以开始进行数据分析。这是一个一次性的操作，将帮助我们运行本章及后续章节的代码示例。
- en: Installing PySpark
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 PySpark
- en: The examples and code in this book assume you have Spark 3.1.1 available. For
    the purpose of following the code examples, install PySpark from the [PyPI repository](https://oreil.ly/t0WBZ).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的示例和代码假设您已经安装了 Spark 3.1.1。为了跟随代码示例，从 [PyPI repository](https://oreil.ly/t0WBZ)
    安装 PySpark。
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that PySpark requires Java 8 or later to be installed. If you want SQL,
    ML, and/or MLlib as extra dependencies, that’s an option too. We will need these
    later.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，PySpark 需要安装 Java 8 或更高版本。如果需要 SQL、ML 和/或 MLlib 作为额外的依赖项，这也是一个选择。我们稍后会需要这些。
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Installing from PyPI skips the libraries required to run Scala, Java, or R.
    Full releases can be obtained from the [Spark project site](https://oreil.ly/pK2Wi).
    Refer to the [Spark documentation](https://oreil.ly/FLh4U) for instructions on
    setting up a Spark environment, whether on a cluster or simply on your local machine.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从 PyPI 安装会跳过运行 Scala、Java 或 R 所需的库。完整的发行版本可以从 [Spark project site](https://oreil.ly/pK2Wi)
    获取。请参考 [Spark documentation](https://oreil.ly/FLh4U) 来了解在集群或本地机器上设置 Spark 环境的说明。
- en: 'Now we’re ready to launch the `pyspark-shell`, which is an REPL for the Python
    language that also has some Spark-specific extensions. This is similar to the
    Python or IPython shell that you may have used. If you’re just running these examples
    on your personal computer, you can launch a local Spark cluster by specifying
    `local[N]`, where `N` is the number of threads to run, or `*` to match the number
    of cores available on your machine. For example, to launch a local cluster that
    uses eight threads on an eight-core machine:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备启动 `pyspark-shell`，这是 Python 语言的 REPL，同时具有一些特定于 Spark 的扩展。这类似于您可能使用过的
    Python 或 IPython shell。如果您只是在个人计算机上运行这些示例，可以通过指定 `local[N]` 来启动本地 Spark 集群，其中
    `N` 是要运行的线程数，或者 `*` 来匹配计算机上可用的核心数。例如，要在八核机器上启动一个使用八个线程的本地集群：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A Spark application itself is often referred to as a Spark *cluster*. That is
    a logical abstraction and is different from a physical cluster (multiple machines).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 应用本身通常被称为 Spark *集群*。这是一个逻辑抽象，不同于物理集群（多台机器）。
- en: 'If you have a Hadoop cluster that runs a version of Hadoop that supports YARN,
    you can launch the Spark jobs on the cluster by using the value of `yarn` for
    the Spark master:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有一个运行支持 YARN 的 Hadoop 集群，可以使用 `yarn` 作为 Spark 主节点的值在集群上启动 Spark 作业：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The rest of the examples in this book will not show a `--master` argument to
    `spark-shell`, but you will typically need to specify this argument as appropriate
    for your environment.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本书其余示例将不显示 `--master` 参数到 `spark-shell`，但您通常需要根据环境设置适当的参数。
- en: You may need to specify additional arguments to make the Spark shell fully utilize
    your resources. A list of arguments can be found by executing `pyspark --help`.
    For example, when running Spark with a local master, you can use `--driver-memory
    2g` to let the single local process use 2 GB of memory. YARN memory configuration
    is more complex, and relevant options like `--executor-memory` are explained in
    the [Spark on YARN documentation](https://oreil.ly/3bRjy).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能需要指定额外的参数以使 Spark shell 充分利用您的资源。通过执行 `pyspark --help` 可以找到参数列表。例如，在本地主节点运行
    Spark 时，您可以使用 `--driver-memory 2g` 来让单个本地进程使用 2 GB 内存。YARN 内存配置更为复杂，相关选项如 `--executor-memory`
    在 [Spark on YARN documentation](https://oreil.ly/3bRjy) 中有解释。
- en: 'The Spark framework officially supports four cluster deployment modes: standalone,
    YARN, Kubernetes, and Mesos. More details can be found in the [Deploying Spark
    documentation](https://oreil.ly/hG2a5).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 框架正式支持四种集群部署模式：独立模式、YARN、Kubernetes 和 Mesos。更多详细信息可以在 [Deploying Spark
    documentation](https://oreil.ly/hG2a5) 中找到。
- en: 'After running one of these commands, you will see a lot of log messages from
    Spark as it initializes itself, but you should also see a bit of ASCII art, followed
    by some additional log messages and a prompt:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这些命令之一后，您将看到大量来自Spark的日志消息，因为它初始化自身，但您还应该看到一些ASCII艺术，随后是一些额外的日志消息和提示：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can run the `:help` command in the shell. This will prompt you to either
    start an interactive help mode or ask for help about specific Python objects.
    In addition to the note about `:help`, the Spark log messages indicated “SparkSession
    available as *spark*.” This is a reference to the `SparkSession`, which acts as
    an entry point to all Spark operations and data. Go ahead and type `spark` at
    the command line:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在shell中可以运行`:help`命令。这会提示您启动交互式帮助模式或请求有关特定Python对象的帮助。除了关于`:help`的说明外，Spark日志消息还指出“SparkSession
    可用作 *spark*。”这是对`SparkSession`的引用，它作为所有Spark操作和数据的入口点。请继续在命令行输入`spark`：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The REPL will print the string form of the object. For the `SparkSession` object,
    this is simply its name plus the hexadecimal address of the object in memory.
    (`DEADBEEF` is a placeholder; the exact value you see here will vary from run
    to run.) In an interactive Spark shell, the Spark driver instantiates a SparkSession
    for you, while in a Spark application, you create a SparkSession object yourself.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: REPL将打印对象的字符串形式。对于`SparkSession`对象，这只是它在内存中的名称加上十六进制地址。（`DEADBEEF`是一个占位符；您在这里看到的确切值会因运行而异。）在交互式Spark
    shell中，Spark驱动程序会为您实例化一个SparkSession，而在Spark应用程序中，您需要自己创建一个SparkSession对象。
- en: In Spark 2.0, the SparkSession became a unified entry point to all Spark operations
    and data. Previously used entry points such as SparkContext, SQLContext, HiveContext,
    SparkConf, and StreamingContext can be accessed through it too.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark 2.0中，SparkSession成为了所有Spark操作和数据的统一入口点。以前使用的入口点如SparkContext、SQLContext、HiveContext、SparkConf和StreamingContext也可以通过它访问。
- en: 'What exactly do we do with the `spark` variable? `SparkSession` is an object,
    so it has methods associated with it. We can see what those methods are in the
    PySpark shell by typing the name of a variable, followed by a period, followed
    by tab:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们究竟如何处理`spark`变量？`SparkSession`是一个对象，因此它有与之关联的方法。我们可以在PySpark shell中通过输入变量名称，然后加上一个句点，然后加上tab键来查看这些方法是什么：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Out of all the methods provided by SparkSession, the ones that we’re going to
    use most often allow us to create DataFrames. Now that we have set up PySpark,
    we can set up our dataset of interest and start using PySpark’s DataFrame API
    to interact with it. That’s what we will do in the next section.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在SparkSession提供的所有方法中，我们将经常使用的是允许我们创建DataFrames的方法。现在我们已经设置好了PySpark，我们可以设置我们感兴趣的数据集，并开始使用PySpark的DataFrame
    API与之交互。这是我们将在下一节中做的事情。
- en: Setting Up Our Data
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置我们的数据
- en: The UC Irvine Machine Learning Repository is a fantastic source for interesting
    (and free) datasets for research and education. The dataset we’ll analyze was
    curated from a record linkage study performed at a German hospital in 2010, and
    it contains several million pairs of patient records that were matched according
    to several different criteria, such as the patient’s name (first and last), address,
    and birthday. Each matching field was assigned a numerical score from 0.0 to 1.0
    based on how similar the strings were, and the data was then hand-labeled to identify
    which pairs represented the same person and which did not. The underlying values
    of the fields that were used to create the dataset were removed to protect the
    privacy of the patients. Numerical identifiers, the match scores for the fields,
    and the label for each pair (match versus nonmatch) were published for use in
    record linkage research.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: UC Irvine机器学习存储库是一个非常好的资源，提供用于研究和教育的有趣（且免费）的数据集。我们将分析的数据集是从2010年在德国医院进行的一项记录链接研究中策划的，包含数百万对根据多种不同标准（如患者姓名（名和姓）、地址和生日）匹配的患者记录。每个匹配字段根据字符串相似性被分配了从0.0到1.0的数值分数，然后手动标记数据以识别哪些对表示同一人，哪些不是。用于创建数据集的字段的基础值已被删除，以保护患者的隐私。数值标识符、字段的匹配分数以及每对的标签（匹配与非匹配）已发布，供记录链接研究使用。
- en: 'From the shell, let’s pull the data from the repository:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从shell中，让我们从存储库中拉取数据：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If you have a Hadoop cluster handy, you can create a directory for the block
    data in HDFS and copy the files from the dataset there:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有一个Hadoop集群方便的话，可以在HDFS中为块数据创建一个目录，并将数据集的文件复制到那里：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To create a dataframe for our record linkage dataset, we’re going to use the
    `S⁠p⁠a⁠r⁠k​S⁠e⁠s⁠s⁠i⁠o⁠n` object. Specifically, we will use the `csv` method on
    its Reader API:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要为我们的记录链接数据集创建一个DataFrame，我们将使用`S⁠p⁠a⁠r⁠k​S⁠e⁠s⁠s⁠i⁠o⁠n`对象。具体来说，我们将在其Reader
    API上使用`csv`方法：
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'By default, every column in a CSV file is treated as a `string` type, and the
    column names default to `_c0`, `_c1`, `_c2`, and so on. We can look at the head
    of a dataframe in the shell by calling its `show` method:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，CSV文件中的每一列都被视为`string`类型，列名默认为`_c0`、`_c1`、`_c2`等。我们可以通过调用其`show`方法在shell中查看DataFrame的头部：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can see that the first row of the DataFrame is the name of the header columns,
    as we expected, and that the CSV file has been cleanly split up into its individual
    columns. We can also see the presence of the `?` strings in some of the columns;
    we will need to handle these as missing values. In addition to naming each column
    correctly, it would be ideal if Spark could properly infer the data type of each
    of the columns for us.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到DataFrame的第一行是标头列的名称，正如我们所预期的那样，并且CSV文件已被干净地分割为其各个列。我们还可以看到一些列中存在`?`字符串；我们需要将这些处理为缺失值。除了正确命名每列之外，如果Spark能够正确推断每列的数据类型将是理想的。
- en: 'Fortunately, Spark’s CSV reader provides all of this functionality for us via
    options that we can set on the Reader API. You can see the full list of options
    that the API takes in the [`pyspark` documentation](https://oreil.ly/xiLj1). For
    now, we’ll read and parse the linkage data like this:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Spark的CSV读取器通过我们可以在Reader API上设置的选项为我们提供了所有这些功能。您可以在[`pyspark`文档](https://oreil.ly/xiLj1)中看到API接受的完整选项列表。现在，我们将像这样读取和解析链接数据：
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'When we call `show` on the `parsed` data, we see that the column names are
    set correctly and the `?` strings have been replaced by `null` values. To see
    the inferred type for each column, we can print the schema of the `parsed` DataFrame
    like this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在`parsed`数据上调用`show`时，我们看到列名已正确设置，并且`?`字符串已被`null`值替换。要查看每列的推断类型，我们可以像这样打印`parsed`
    DataFrame的架构：
- en: '[PRE12]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Each `Column` instance contains the name of the column, the most specific data
    type that could handle the type of data contained in each record, and a boolean
    field that indicates whether or not the column may contain null values, which
    is true by default. In order to perform the schema inference, Spark must do *two*
    passes over the dataset: one pass to figure out the type of each column, and a
    second pass to do the actual parsing. The first pass can work on a sample if desired.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`Column`实例包含列的名称，能处理每条记录中数据类型的最具体数据类型，以及一个布尔字段，指示该列是否可以包含空值，默认为true。为了执行架构推断，Spark必须对数据集进行*两次*遍历：第一次遍历以确定每列的类型，第二次遍历执行实际的解析。如果需要，第一次遍历可以对样本进行处理。
- en: If you know the schema that you want to use for a file ahead of time, you can
    create an instance of the `pyspark.sql.types.StructType` class and pass it to
    the Reader API via the `schema` function. This can have a significant performance
    benefit when the dataset is very large, since Spark will not need to perform an
    extra pass over the data to figure out the data type of each column.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您提前知道要为文件使用的架构，可以创建`pyspark.sql.types.StructType`类的实例，并通过`schema`函数将其传递给Reader
    API。当数据集非常大时，这可以显著提高性能，因为Spark不需要再次遍历数据以确定每列的数据类型。
- en: 'Here is an example of defining a schema using `StructType` and `StructField`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用`StructType`和`StructField`定义架构的示例：
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Another way to define the schema is using DDL (data definition language) statements:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种定义架构的方法是使用DDL（数据定义语言）语句：
- en: '[PRE14]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'DataFrames have a number of methods that enable us to read data from the cluster
    into the PySpark REPL on our client machine. Perhaps the simplest of these is
    `first`, which returns the first element of the DataFrame into the client:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrames具有多种方法，使我们能够将数据从集群读取到我们客户端机器上的PySpark REPL中。其中最简单的方法可能是`first`，它将DataFrame的第一个元素返回到客户端：
- en: '[PRE15]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `first` method can be useful for sanity checking a dataset, but we’re generally
    interested in bringing back larger samples of a DataFrame into the client for
    analysis. When we know that a DataFrame contains only a small number of records,
    we can use the `toPandas` or `collect` method to return all the contents of a
    DataFrame to the client as an array. For extremely large DataFrames, using these
    methods can be dangerous and cause an out-of-memory exception. Because we don’t
    know how big the linkage dataset is just yet, we’ll hold off on doing this right
    now.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`first`方法对于对数据集进行健全性检查很有用，但我们通常对将DataFrame的较大样本带回客户端进行分析感兴趣。当我们知道一个DataFrame只包含少量记录时，我们可以使用`toPandas`或`collect`方法将DataFrame的所有内容作为数组返回到客户端。对于非常大的DataFrames，使用这些方法可能是危险的，并且可能导致内存不足的异常。因为我们还不知道链接数据集有多大，所以暂时不会这样做。'
- en: In the next several sections, we’ll use a mix of local development and testing
    and cluster computation to perform more munging and analysis of the record linkage
    data, but if you need to take a moment to drink in the new world of awesome that
    you have just entered, we certainly understand.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将使用本地开发和测试以及集群计算的混合方式执行更多的数据处理和数据分析工作，但如果你需要花一点时间来沉浸在你刚刚进入的新的精彩世界中，我们当然会理解。
- en: Analyzing Data with the DataFrame API
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DataFrame API分析数据
- en: The DataFrame API comes with a powerful set of tools that will likely be familiar
    to data scientists who are used to Python and SQL. In this section, we will begin
    to explore these tools and how to apply them to the record linkage data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame API配备了一组强大的工具，这些工具对于习惯于Python和SQL的数据科学家可能会很熟悉。在本节中，我们将开始探索这些工具以及如何将其应用于记录链接数据。
- en: 'If we look at the schema of the `parsed` DataFrame and the first few rows of
    data, we see this:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看`parsed` DataFrame的模式和前几行数据，我们会看到：
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The first two fields are integer IDs that represent the patients who were matched
    in the record.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前两个字段是表示记录中匹配的患者的整数ID。
- en: The next nine fields are (possibly missing) numeric values (either doubles or
    ints) that represent match scores on different fields of the patient records,
    such as their names, birthdays, and locations. The fields are stored as integers
    when the only possible values are match (1) or no-match (0), and doubles whenever
    partial matches are possible.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下面的九个字段是（可能缺失的）数字值（可以是double或int），表示患者记录不同字段的匹配评分，比如他们的姓名、生日和位置。当字段的唯一可能值是匹配（1）或不匹配（0）时，字段存储为整数，而在可能存在部分匹配时存储为双精度数。
- en: The last field is a boolean value (`true` or `false`) indicating whether or
    not the pair of patient records represented by the line was a match.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个字段是一个布尔值（`true`或`false`），表示该行代表的患者记录对是否匹配。
- en: 'Our goal is to come up with a simple classifier that allows us to predict whether
    a record will be a match based on the values of the match scores for the patient
    records. Let’s start by getting an idea of the number of records we’re dealing
    with via the `count` method:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是制定一个简单的分类器，让我们能够根据患者记录的匹配评分的值来预测记录是否匹配。让我们通过`count`方法来了解我们要处理的记录数量的概念：
- en: '[PRE17]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This is a relatively small dataset—certainly small enough to fit in memory on
    one of the nodes in a cluster or even on your local machine if you don’t have
    a cluster available. Thus far, every time we’ve processed the data, Spark has
    reopened the file, reparsed the rows, and then performed the action requested,
    like showing the first few rows of the data or counting the number of records.
    When we ask another question, Spark will do these same operations, again and again,
    even if we have filtered the data down to a small number of records or are working
    with an aggregated version of the original dataset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相对较小的数据集—肯定足够小，可以在集群中的一个节点或者甚至在你的本地机器的内存中存储，如果你没有可用的集群。到目前为止，每当我们处理数据时，Spark都会重新打开文件，重新解析行，然后执行请求的操作，比如显示数据的前几行或计算记录的数量。当我们提出另一个问题时，Spark将再次执行这些操作，即使我们已经将数据筛选到少量记录中，或者正在使用原始数据集的聚合版本。
- en: 'This isn’t an optimal use of our compute resources. After the data has been
    parsed once, we’d like to save the data in its parsed form on the cluster so that
    we don’t have to reparse it every time we want to ask a new question. Spark supports
    this use case by allowing us to signal that a given DataFrame should be cached
    in memory after it is generated by calling the `cache` method on the instance.
    Let’s do that now for the `parsed` DataFrame:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是我们计算资源的最佳使用方式。数据解析完成后，我们希望将数据保存在集群上的解析形式，这样每次需要提出新问题时就不必重新解析。Spark 通过允许我们在实例上调用`cache`方法来信号化指定
    DataFrame 应在生成后缓存在内存中来支持这种用例。现在让我们为`parsed` DataFrame 进行这样的操作：
- en: '[PRE18]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Once our data has been cached, the next thing we want to know is the relative
    fraction of records that were matches versus those that were nonmatches:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 数据缓存后，我们想知道的下一件事是记录的匹配与非匹配的相对比例：
- en: '[PRE19]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Instead of writing a function to extract the `is_match` column, we simply pass
    its name to the `groupBy` method on the DataFrame, call the `count` method to,
    well, count the number of records inside each grouping, sort the resulting data
    in descending order based on the `count` column, and then cleanly render the result
    of the computation in the REPL with `show`. Under the covers, the Spark engine
    determines the most efficient way to perform the aggregation and return the results.
    This illustrates the clean, fast, and expressive way to do data analysis that
    Spark provides.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要编写函数来提取`is_match`列，只需将其名称传递给 DataFrame 的`groupBy`方法，调用`count`方法来计算每个分组内的记录数量，根据`count`列按降序排序，然后使用`show`在
    REPL 中清晰地呈现计算结果。在幕后，Spark 引擎确定执行聚合并返回结果的最有效方法。这展示了 Spark 提供的进行数据分析的干净、快速和表达方式。
- en: 'Note that there are two ways we can reference the names of the columns in the
    DataFrame: either as literal strings, like in `groupBy("is_match")`, or as `Column`
    objects by using the `col` function that we used on the `count` column. Either
    approach is valid in most cases, but we needed to use the `col` function to call
    the `desc` method on the resulting `count` column object.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们可以有两种方式引用 DataFrame 中列的名称：一种是作为字面字符串，例如`groupBy("is_match")`中的用法，另一种是通过使用我们在`count`列上使用的`col`函数获取的`Column`对象。在大多数情况下，这两种方法都是有效的，但我们需要使用`col`函数调用结果`count`列对象上的`desc`方法。
- en: 'You may have noticed that the functions in the DataFrame API are similar to
    the components of a SQL query. This isn’t a coincidence, and in fact we have the
    option to treat any DataFrame we create as if it were a database table and to
    express our questions using familiar and powerful SQL syntax. First, we need to
    tell the Spark SQL execution engine the name it should associate with the `parsed`
    DataFrame, since the name of the variable itself (“parsed”) isn’t available to
    Spark:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，DataFrame API 中的函数与 SQL 查询的组件相似。这不是巧合，事实上，我们可以选择将我们创建的任何 DataFrame
    视为数据库表，并使用熟悉和强大的 SQL 语法来表达我们的问题。首先，我们需要告诉 Spark SQL 执行引擎应将`parsed` DataFrame 关联的名称，因为变量名称本身（"parsed"）对于
    Spark 是不可用的：
- en: '[PRE20]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Because the `parsed` DataFrame is available only during the length of this PySpark
    REPL session, it is a *temporary* table. Spark SQL may also be used to query persistent
    tables in HDFS if we configure Spark to connect to an Apache Hive metastore that
    tracks the schemas and locations of structured datasets.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`parsed` DataFrame 仅在此 PySpark REPL 会话期间可用，它是一个*临时*表。如果我们配置 Spark 连接到跟踪结构化数据集架构和位置的
    Apache Hive 元存储，Spark SQL 也可以用于查询 HDFS 中的持久表。
- en: 'Once our temporary table is registered with the Spark SQL engine, we can query
    it like this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的临时表在 Spark SQL 引擎中注册，我们可以像这样查询它：
- en: '[PRE21]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You have the option of running Spark either by using an ANSI 2003-compliant
    version of Spark SQL (the default) or in HiveQL mode by calling the `enableHiveSupport`
    method when you create a `SparkSession` instance via its Builder API.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择通过调用`enableHiveSupport`方法在创建`SparkSession`实例时使用 ANSI 2003 兼容版本的 Spark SQL（默认方式）或在
    HiveQL 模式下运行 Spark。
- en: 'Should you use Spark SQL or the DataFrame API to do your analysis in PySpark?
    There are pros and cons to each: SQL has the benefit of being broadly familiar
    and expressive for simple queries. It also lets you query data using JDBC/ODBC
    connectors from databases such as PostgreSQL or tools such as Tableau. The downside
    of SQL is that it can be difficult to express complex, multistage analyses in
    a dynamic, readable, and testable way—all areas where the DataFrame API shines.
    Throughout the rest of the book, we use both Spark SQL and the DataFrame API,
    and we leave it as an exercise for the reader to examine the choices we made and
    translate our computations from one interface to the other.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PySpark 中，您应该使用 Spark SQL 还是 DataFrame API 来进行分析呢？每种方法都有其利弊：SQL 的优点在于它广泛被认知，并且对于简单查询而言表达能力强。它还允许您使用
    JDBC/ODBC 连接器从诸如 PostgreSQL 或像 Tableau 这样的工具中查询数据。然而，SQL 的缺点在于，在动态、可读且可测试的方式下表达复杂的多阶段分析可能会很困难——而
    DataFrame API 在这些方面表现出色。在本书的其余部分中，我们既使用 Spark SQL 又使用 DataFrame API，并留给读者作为一个练习来审视我们所做的选择，并将我们的计算从一种接口转换到另一种接口。
- en: We can apply functions one by one to our DataFrame to obtain statistics such
    as count and mean. However, PySpark offers a better way to obtain summary statistics
    for DataFrames, and that’s what we will cover in the next section.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以逐个将函数应用于我们的 DataFrame，以获取诸如计数和平均值之类的统计数据。然而，PySpark 提供了一种更好的方法来获取 DataFrames
    的汇总统计数据，这就是我们将在下一节中讨论的内容。
- en: Fast Summary Statistics for DataFrames
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrames 的快速汇总统计
- en: 'Although there are many kinds of analyses that may be expressed equally well
    in SQL or with the DataFrame API, there are certain common things that we want
    to be able to do with dataframes that can be tedious to express in SQL. One such
    analysis that is especially helpful is computing the min, max, mean, and standard
    deviation of all the non-null values in the numerical columns of a dataframe.
    In PySpark, this function has the same name that it does in pandas, `describe`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多种类的分析可以在 SQL 或 DataFrame API 中同样有效地表达，但有些常见的数据框架操作在 SQL 中表达起来可能很乏味。其中一种特别有帮助的分析是计算数据框架数值列中所有非空值的最小值、最大值、平均值和标准差。在
    PySpark 中，这个函数与 pandas 中的函数同名，即 `describe`：
- en: '[PRE22]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `summary` DataFrame has one column for each variable in the `parsed` DataFrame,
    along with another column (also named `summary`) that indicates which metric—`count`,
    `mean`, `stddev`, `min`, or `max`—is present in the rest of the columns in the
    row. We can use the `select` method to choose a subset of the columns to make
    the summary statistics easier to read and compare:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`summary` DataFrame 拥有 `parsed` DataFrame 中每个变量的一列，以及另一列（也称为 `summary`），指示其余列中的哪个指标——`count`、`mean`、`stddev`、`min`
    或 `max`——存在。我们可以使用 `select` 方法选择列的子集，以便更容易阅读和比较汇总统计信息：'
- en: '[PRE23]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note the difference in the value of the `count` variable between `cmp_fname_c1`
    and `cmp_fname_c2`. While almost every record has a non-null value for `cmp_fname_c1`,
    less than 2% of the records have a non-null value for `cmp_fname_c2`. To create
    a useful classifier, we need to rely on variables that are almost always present
    in the data—unless the fact that they are missing indicates something meaningful
    about whether the record matches.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `count` 变量在 `cmp_fname_c1` 和 `cmp_fname_c2` 之间的值的差异。几乎每条记录的 `cmp_fname_c1`
    都有非空值，而仅不到 2% 的记录有 `cmp_fname_c2` 的非空值。要创建一个有用的分类器，我们需要依赖几乎总是出现在数据中的变量——除非它们的缺失反映出记录是否匹配的有意义信息。
- en: 'Once we have an overall feel for the distribution of the variables in our data,
    we want to understand how the values of those variables are correlated with the
    value of the `is_match` column. Therefore, our next step is to compute those same
    summary statistics for just the subsets of the `parsed` DataFrame that correspond
    to matches and nonmatches. We can filter DataFrames using either SQL-style `where`
    syntax or with `Column` objects using the DataFrame API and then use `describe`
    on the resulting DataFrames:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对数据中变量的分布有了整体的了解，我们希望了解这些变量的值如何与 `is_match` 列的值相关联。因此，我们的下一步是仅针对与匹配和非匹配对应的
    `parsed` DataFrame 子集计算相同的汇总统计数据。我们可以使用类似 SQL 的 `where` 语法或使用 DataFrame API 中的
    `Column` 对象来过滤 DataFrames，然后在结果 DataFrames 上使用 `describe`：
- en: '[PRE24]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The logic inside the string we pass to the `where` function can include statements
    that would be valid inside a `WHERE` clause in Spark SQL. For the filtering condition
    that uses the DataFrame API, we use the `==` operator on the `is_match` column
    object to check for equality with the boolean object `False`, because that is
    just Python, not SQL. Note that the `where` function is an alias for the `filter`
    function; we could have reversed the `where` and `filter` calls in the above snippet
    and everything would have worked the same way.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们传递给 `where` 函数的字符串内部逻辑可以包含在 Spark SQL 中的 `WHERE` 子句中有效的语句。对于使用 DataFrame API
    的过滤条件，我们使用 `==` 运算符来检查 `is_match` 列对象与布尔对象 `False` 是否相等，因为这只是 Python，而不是 SQL。请注意，`where`
    函数是 `filter` 函数的别名；我们可以在上述片段中颠倒 `where` 和 `filter` 的调用顺序，一切仍将正常工作。
- en: We can now start to compare our `match_summary` and `miss_summary` DataFrames
    to see how the distribution of the variables changes depending on whether the
    record is a match or a miss. Although this is a relatively small dataset, doing
    this comparison is still somewhat tedious—what we really want is to transpose
    the `match_summary` and `miss_summary` DataFrames so that the rows and columns
    are swapped, which would allow us to join the transposed DataFrames together by
    variable and analyze the summary statistics, a practice that most data scientists
    know as “pivoting” or “reshaping” a dataset. In the next section, we’ll show you
    how to perform these transformations.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始比较 `match_summary` 和 `miss_summary` DataFrame，以查看变量分布如何随记录是匹配还是未匹配而变化。尽管这是一个相对较小的数据集，进行这种比较仍然有些繁琐——我们真正想要的是转置
    `match_summary` 和 `miss_summary` DataFrame，以便行和列被交换，这将允许我们通过变量连接转置的 DataFrame
    并分析汇总统计信息，这是大多数数据科学家所知的“透视”或“重塑”数据集的实践。在下一节中，我们将展示如何执行这些转换。
- en: Pivoting and Reshaping DataFrames
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据透视和重塑 DataFrame
- en: We can transpose the DataFrames entirely using functions provided by PySpark.
    However, there is another way to perform this task. PySpark allows conversion
    between Spark and pandas DataFrames. We will convert the DataFrames in question
    into pandas DataFrames, reshape them, and convert them back to Spark DataFrames.
    We can safely do this because of the small size of the `summary`, `match_summary`,
    and `miss_summary` DataFrames since pandas DataFrames reside in memory. In upcoming
    chapters, we will rely on Spark operations for such transformations on larger
    datasets.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 PySpark 提供的函数完全转置 DataFrames。但是，还有一种执行此任务的方法。 PySpark 允许在 Spark 和 pandas
    DataFrames 之间进行转换。我们将问题中的 DataFrames 转换为 pandas DataFrames，重塑它们，然后将它们转换回 Spark
    DataFrames。由于 `summary`、`match_summary` 和 `miss_summary` DataFrames 的大小较小，因此我们可以安全地执行此操作，因为
    pandas DataFrames 位于内存中。在接下来的章节中，我们将依靠 Spark 操作来处理较大数据集上的这些转换。
- en: Conversion to/from pandas DataFrames is possible because of the Apache Arrow
    project, which allows efficient data transfer between JVM and Python processes.
    The PyArrow library was installed as a dependency of the Spark SQL module when
    we installed `pyspark[sql]` using pip.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Apache Arrow 项目的存在，允许在 JVM 和 Python 进程之间高效传输数据，所以可以进行 Spark 和 pandas DataFrames
    之间的转换。当我们使用 pip 安装 `pyspark[sql]` 时，PyArrow 库作为 Spark SQL 模块的依赖项被安装。
- en: 'Let’s convert `summary` into a pandas DataFrame:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 `summary` 转换为 pandas DataFrame：
- en: '[PRE25]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can now use pandas functions on the `summary_p` DataFrame:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在 `summary_p` DataFrame 上使用 pandas 函数：
- en: '[PRE26]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can now perform a transpose operation to swap rows and columns using familiar
    pandas methods on the DataFrame:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用 DataFrame 上熟悉的 pandas 方法执行转置操作，以交换行和列：
- en: '[PRE27]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We have successfully transposed the `summary_p` pandas DataFrame. Convert it
    into a Spark DataFrame using SparkSession’s `createDataFrame` method:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已成功转置了 `summary_p` pandas DataFrame。使用 SparkSession 的 `createDataFrame` 方法将其转换为
    Spark DataFrame：
- en: '[PRE28]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We are not done yet. Print the schema of the `summaryT` DataFrame:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有完成。打印 `summaryT` DataFrame 的模式：
- en: '[PRE29]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In the summary schema, as obtained from the `describe` method, every field
    is treated as a string. Since we want to analyze the summary statistics as numbers,
    we’ll need to convert the values from strings to doubles:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在从 `describe` 方法获取的汇总模式中，每个字段都被视为字符串。由于我们希望将汇总统计信息作为数字进行分析，因此需要将值从字符串转换为双精度数：
- en: '[PRE30]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now that we have figured out how to transpose a summary DataFrame, let’s implement
    our logic into a function that we can reuse on the `match_summary` and `m⁠i⁠s⁠s⁠_​s⁠u⁠m⁠m⁠a⁠r⁠y`
    DataFrames:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经找到了如何转置汇总 DataFrame 的方法，让我们将我们的逻辑实现为一个函数，我们可以在 `match_summary` 和 `miss_summary`
    DataFrame 上重复使用：
- en: '[PRE31]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now in your Spark shell, use the `pivot_summary` function on the `match_summary`
    and `miss_summary` DataFrames:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在你的 Spark shell 中，对 `match_summary` 和 `miss_summary` DataFrames 使用 `pivot_summary`
    函数：
- en: '[PRE32]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now that we have successfully transposed the summary DataFrames, we can join
    and compare them. That’s what we will do in the next section. Further, we will
    also select desirable features for building our model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经成功地转置了汇总的 DataFrames，我们可以连接并比较它们。这就是我们将在下一节中做的事情。此外，我们还将选择适合建立模型的理想特征。
- en: Joining DataFrames and Selecting Features
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接 DataFrames 和选择特征
- en: 'So far, we have used Spark SQL and the DataFrame API only to filter and aggregate
    the records from a dataset, but we can also use these tools to perform joins (inner,
    left outer, right outer, or full outer) on DataFrames. Although the DataFrame
    API includes a `join` function, it’s often easier to express these joins using
    Spark SQL, especially when the tables we are joining have a large number of column
    names in common and we want to be able to clearly indicate which column we are
    referring to in our select expressions. Let’s create temporary views for the `match_summaryT`
    and `miss_summaryT` DataFrames, join them on the `field` column, and compute some
    simple summary statistics on the resulting rows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们仅使用 Spark SQL 和 DataFrame API 进行数据集的过滤和聚合，但我们也可以使用这些工具在 DataFrames 上执行连接操作（内连接、左连接、右连接或全连接）。虽然
    DataFrame API 包括一个 `join` 函数，但通常使用 Spark SQL 更容易表达这些连接操作，特别是当我们要连接的表有很多列名相同时，并且我们希望能够清楚地指示在选择表达式中正在引用哪个列时。让我们为
    `match_summaryT` 和 `miss_summaryT` DataFrames 创建临时视图，在 `field` 列上进行连接，并对结果行计算一些简单的汇总统计信息：
- en: '[PRE33]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'A good feature has two properties: it tends to have significantly different
    values for matches and nonmatches (so the difference between the means will be
    large), and it occurs often enough in the data that we can rely on it to be regularly
    available for any pair of records. By this measure, `cmp_fname_c2` isn’t very
    useful because it’s missing a lot of the time, and the difference in the mean
    value for matches and nonmatches is relatively small—0.09, for a score that ranges
    from 0 to 1\. The `cmp_sex` feature also isn’t particularly helpful because even
    though it’s available for any pair of records, the difference in means is just
    0.03.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的特征具有两个特性：它倾向于在匹配和非匹配情况下具有显著不同的值（因此平均值之间的差异将很大），并且在数据中经常出现，我们可以依赖它定期出现在任何一对记录中。按此标准，`cmp_fname_c2`
    并不是非常有用，因为它大部分时间都缺失，并且匹配和非匹配的平均值之间的差异相对较小——0.09，对于一个从 0 到 1 的分数来说。`cmp_sex` 特征也不是特别有帮助，因为即使它对于任何一对记录都是可用的，平均值之间的差异仅为
    0.03。
- en: 'Features `cmp_plz` and `cmp_by`, on the other hand, are excellent. They almost
    always occur for any pair of records, and there is a very large difference in
    the mean values (more than 0.77 for both features). Features `cmp_bd`, `cmp_lname_c1`,
    and `cmp_bm` also seem beneficial: they are generally available in the dataset,
    and the difference in mean values for matches and nonmatches is substantial.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`cmp_plz` 和 `cmp_by` 特征则非常优秀。它们几乎总是出现在任何一对记录中，并且平均值之间的差异非常大（这两个特征都超过了 0.77）。`cmp_bd`、`cmp_lname_c1`
    和 `cmp_bm` 特征似乎也是有益的：它们通常在数据集中可用，并且匹配和非匹配的平均值之间的差异很大。
- en: 'Features `cmp_fname_c1` and `cmp_lname_c2` are more of a mixed bag: `cmp_fname_c1`
    doesn’t discriminate all that well (the difference in the means is only 0.28)
    even though it’s usually available for a pair of records, whereas `cmp_lname_c2`
    has a large difference in the means, but it’s almost always missing. It’s not
    quite obvious under what circumstances we should include these features in our
    model based on this data.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`cmp_fname_c1` 和 `cmp_lname_c2` 特征则有些复杂：`cmp_fname_c1` 并不能很好地区分（平均值之间的差异仅为
    0.28），即使它通常对于一对记录来说是可用的，而 `cmp_lname_c2` 在平均值之间有很大的差异，但几乎总是缺失。根据这些数据，不太明显在什么情况下应该在我们的模型中包含这些特征。'
- en: 'For now, we’re going to use a simple scoring model that ranks the similarity
    of pairs of records based on the sums of the values of the obviously good features:
    `cmp_plz`, `cmp_by`, `cmp_bd`, `cmp_lname_c1`, and `cmp_bm`. For the few records
    where the values of these features are missing, we’ll use 0 in place of the `null`
    value in our sum. We can get a rough feel for the performance of our simple model
    by creating a dataframe of the computed scores and the value of the `is_match`
    column and evaluating how well the score discriminates between matches and nonmatches
    at various thresholds.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将使用一个简单的评分模型，根据显然良好特征的值之和对记录对的相似性进行排名：`cmp_plz`、`cmp_by`、`cmp_bd`、`cmp_lname_c1`和`cmp_bm`。对于这些特征值缺失的少数记录，我们将在求和中使用0替代`null`值。通过创建计算得分和`is_match`列的DataFrame，我们可以大致了解我们简单模型的性能，并评估得分在各种阈值下如何区分匹配和非匹配。
- en: Scoring and Model Evaluation
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评分和模型评估
- en: For our scoring function, we are going to sum up the value of five fields (`cmp_lname_c1`,
    `cmp_plz`, `cmp_by`, `cmp_bd`, and `cmp_bm`). We will use `expr` from `pyspark.sql.functions`
    for doing this. The `expr` function parses an input expression string into the
    column that it represents. This string can even involve multiple columns.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的评分函数，我们将对五个字段（`cmp_lname_c1`、`cmp_plz`、`cmp_by`、`cmp_bd`和`cmp_bm`）的值进行求和。我们将使用`pyspark.sql.functions`中的`expr`来实现这一点。`expr`函数将输入的表达式字符串解析成对应的列。这个字符串甚至可以涉及多个列。
- en: 'Let’s create the required expression string:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建所需的表达式字符串：
- en: '[PRE34]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We can now use the `sum_expression` string for calculating the score. When
    summing up the values, we will account for and replace null values with 0 using
    DataFrame’s `fillna` method:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`sum_expression`字符串来计算分数。在对值进行求和时，我们将使用DataFrame的`fillna`方法考虑并替换为0的空值：
- en: '[PRE35]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The final step in creating our scoring function is to decide what threshold
    the score must exceed in order for us to predict that the two records represent
    a match. If we set the threshold too high, then we will incorrectly mark a matching
    record as a miss (called the *false-negative* rate), whereas if we set the threshold
    too low, we will incorrectly label misses as matches (the *false-positive* rate).
    For any nontrivial problem, we always have to trade some false positives for some
    false negatives, and the question of what the threshold value should be usually
    comes down to the relative cost of the two kinds of errors in the situation to
    which the model is being applied.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 创建评分函数的最后一步是决定分数必须超过什么阈值，以便我们预测两个记录表示匹配。如果我们设置的阈值过高，那么我们将错误地将匹配记录标记为错过（称为*假阴性*率），而如果我们将阈值设置得太低，我们将错误地将错过标记为匹配（*假阳性*率）。对于任何非平凡的问题，我们总是需要在两种错误类型之间进行某种权衡，阈值值应该是多少的问题通常取决于模型应用的情况中两种错误类型的相对成本。
- en: 'To help us choose a threshold, it’s helpful to create a *contingency table*
    (which is sometimes called a *cross tabulation*, or *crosstab*) that counts the
    number of records whose scores fall above/below the threshold value crossed with
    the number of records in each of those categories that were/were not matches.
    Since we don’t know what threshold value we’re going to use yet, let’s write a
    function that takes the `scored` DataFrame and the choice of threshold as parameters
    and computes the crosstabs using the DataFrame API:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们选择一个阈值，创建一个*列联表*（有时称为*交叉表*或*crosstab*），计算分数高于/低于阈值的记录数，并将这些类别中记录数与每个类别中的匹配/非匹配数量交叉。因为我们还不知道将使用什么阈值，让我们编写一个函数，该函数接受`scored`
    DataFrame和阈值选择作为参数，并使用DataFrame API计算交叉表：
- en: '[PRE36]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Note that we are including the `selectExpr` method of the DataFrame API to dynamically
    determine the value of the field named `above` based on the value of the `t` argument
    using Python’s f-string formatting syntax, which allows us to substitute variables
    by name if we preface the string literal with the letter `f` (yet another handy
    bit of Scala implicit magic). Once the `above` field is defined, we create the
    crosstab with a standard combination of the `groupBy`, `pivot`, and `count` methods
    that we used before.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在DataFrame API中包括了`selectExpr`方法，根据`t`参数的值使用Python的f-string格式化语法动态确定名为`above`的字段的值，这使我们能够按名称替换变量，如果我们在字符串文字前加上字母`f`（这是另一个Scala隐式魔法的方便部分）。一旦定义了`above`字段，我们就使用之前使用的`groupBy`、`pivot`和`count`方法创建交叉表。
- en: 'By applying a high threshold value of 4.0, meaning that the average of the
    five features is 0.8, we can filter out almost all of the nonmatches while keeping
    over 90% of the matches:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用高阈值值为4.0，意味着五个特征的平均值为0.8，我们可以筛选掉几乎所有非匹配项，同时保留超过90%的匹配项：
- en: '[PRE37]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'By applying a lower threshold of 2.0, we can ensure that we capture *all* of
    the known matching records, but at a substantial cost in terms of false positive
    (top-right cell):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用较低的阈值 2.0，我们可以确保捕捉到*所有*已知的匹配记录，但在假阳性方面会付出很大的代价（右上角的单元格）：
- en: '[PRE38]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Even though the number of false positives is higher than we want, this more
    generous filter still removes 90% of the nonmatching records from our consideration
    while including every positive match. Even though this is pretty good, it’s possible
    to do even better; see if you can find a way to use some of the other values from
    `MatchData` (both missing and not) to come up with a scoring function that successfully
    identifies every `true` match at the cost of less than 100 false positives.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管假阳性的数量高于我们的期望，这种更宽松的筛选仍然从我们的考虑中移除了90%的非匹配记录，同时包含每一个正匹配。虽然这已经相当不错了，但还有可能做得更好；看看是否能找到一种方法来利用`MatchData`的其他值（包括缺失和非缺失的值）来设计一个评分函数，以成功识别每一个`true`匹配，而成本低于100个假阳性。
- en: Where to Go from Here
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何继续前进
- en: If this chapter was your first time carrying out data preparation and analysis
    with PySpark, we hope that you got a feel for what a powerful foundation these
    tools provide. If you have been using Python and Spark for a while, we hope that
    you will pass this chapter along to your friends and colleagues as a way of introducing
    them to that power as well.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果本章是您首次使用PySpark进行数据准备和分析，我们希望您能感受到这些工具提供的强大基础。如果您已经使用Python和Spark一段时间了，我们希望您能把本章推荐给您的朋友和同事，作为向他们介绍这种强大力量的一种方式。
- en: Our goal for this chapter was to provide you with enough knowledge to be able
    to understand and complete the rest of the examples in this book. If you are the
    kind of person who learns best through practical examples, your next step is to
    continue on to the next set of chapters, where we will introduce you to MLlib,
    the machine learning library designed for Spark.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是为您提供足够的知识，以便能够理解并完成本书中其余示例。如果您是通过实际示例学习最佳的人，那么您的下一步是继续学习下一组章节，我们将在那里向您介绍MLlib，这是专为Spark设计的机器学习库。
