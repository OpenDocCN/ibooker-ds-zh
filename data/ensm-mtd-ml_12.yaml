- en: 9 Explaining your ensembles
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 解释你的集成
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding glass-box versus black-box and global versus local interpretability
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解玻璃箱模型与黑箱模型，以及全局解释性与局部解释性
- en: Using global black-box methods to understand pretrained ensemble behavior
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用全局黑箱方法来理解预训练集成行为
- en: Using local black-box methods to explain pretrained ensemble predictions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用局部黑箱方法解释预训练集成预测
- en: Training and using explainable global and local glass-box ensembles from scratch
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从零开始训练和使用可解释的全局和局部玻璃箱集成
- en: When training and deploying models, we’re usually concerned about *what* the
    model prediction is. Equally important, however, is *why* the model made the prediction
    that it did. Understanding a model’s predictions is a critical component of building
    robust machine-learning pipelines. This is especially true when machine-learning
    models are used in high-stakes applications such as in health care or finance.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和部署模型时，我们通常关注模型预测的**是什么**。然而，同样重要的是，我们也需要了解模型做出预测的**原因**。理解模型的预测是构建稳健机器学习管道的关键组成部分。这在机器学习模型应用于高风险应用（如医疗保健或金融）时尤其如此。
- en: For example, in a medical diagnosis task such as diabetes diagnosis, understanding
    why the model made a specific diagnosis can provide users (in this case, doctors)
    with additional insights that can guide them toward better prescriptions, preventative
    care, or palliative care. This increased transparency, in turn, increases trust
    in the machine-learning system, allowing the users for whom the models have been
    developed to use them with confidence.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在糖尿病诊断等医疗诊断任务中，理解模型为何做出特定诊断可以为用户（在这种情况下，医生）提供额外的见解，指导他们做出更好的处方、预防性护理或姑息性护理。这种增加的透明度反过来又增加了对机器学习系统的信任，使得模型的开发用户可以自信地使用它们。
- en: Understanding the reasons behind a model’s predictions is also extremely useful
    in model debugging, identifying failure cases, and finding ways to improve model
    performance. Furthermore, model debugging can also help pinpoint biases and problems
    with the data itself.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 理解模型预测背后的原因对于模型调试、识别失败案例和找到提高模型性能的方法也极为有用。此外，模型调试还可以帮助识别数据本身的偏差和问题。
- en: Machine-learning models can be characterized as black-box models and glass-box
    models. Black-box models are typically challenging to understand owing to their
    complexity (e.g., deep neural networks). The predictions of such models require
    specialized tools to be *explainable*. Many of the ensembles covered in this book,
    such as random forests and gradient boosting, are black-box machine-learning models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型可以被描述为黑箱模型和玻璃箱模型。由于复杂性（例如，深度神经网络），黑箱模型通常难以理解。这类模型的预测需要专门的工具来使其**可解释**。本书中涵盖的许多集成（如随机森林和梯度提升）都是黑箱机器学习模型。
- en: Glass-box models are more intuitive and easier to understand (e.g., decision
    trees). The structure of such models makes them inherently *interpretable*. In
    this chapter, we explore the concepts of explainability and interpretability from
    the perspective of ensemble methods.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 玻璃箱模型更直观，更容易理解（例如，决策树）。这类模型的架构使它们本质上具有**可解释性**。在本章中，我们将从集成方法的角度探讨可解释性和可理解性的概念。
- en: Interpretability methods are also characterized as global or local. Global methods
    attempt to broadly explain a model’s features and relevance to decision making
    across different types of examples. Local methods attempt to specifically explain
    a model’s decision-making process with respect to individual examples and predictions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性方法也被描述为全局或局部。全局方法试图广泛地解释模型在不同类型示例中的特征和决策的相关性。局部方法试图具体解释模型针对单个示例和预测的决策过程。
- en: 'Section 9.1 introduces the basics of black-box and glass-box machine-learning
    models. This section also reintroduces two well-known machine-learning models
    from the perspective of interpretability: decision trees and generalized linear
    models (GLMs).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 9.1节介绍了黑箱和玻璃箱机器学习模型的基础知识。本节还从可解释性的角度重新介绍了两个著名的机器学习模型：决策树和广义线性模型（GLMs）。
- en: 'Section 9.2 introduces this chapter’s case study: data-driven marketing. This
    application is used in the rest of the section to illustrate techniques for interpretability
    and explainability.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 9.2节介绍了本章的案例研究：数据驱动营销。本节其余部分将使用该应用来说明可解释性和可理解性的技术。
- en: 'Section 9.3 introduces three techniques for global black-box explainability:
    permutation feature importance, partial dependence plots, and global surrogate
    models. Section 9.4 introduces two methods for local black-box explainability:
    LIME and SHAP. The black-box methods introduced in sections 9.3 and 9.4 are model-agnostic;
    that is, they can be used for any machine-learning black box. In these sections,
    we specifically focus on how they can be used for ensemble methods. Section 9.5
    introduces a glass-box method called explainable boosting machines, a new ensemble
    method that is designed to be directly interpretable and provides both global
    and local interpretability.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 第9.3节介绍了三种用于全局黑盒可解释性的技术：排列特征重要性、部分依赖图和全局代理模型。第9.4节介绍了两种用于局部黑盒可解释性的方法：LIME和SHAP。第9.3节和第9.4节中介绍的黑盒方法是模型无关的；也就是说，它们可以用于任何机器学习黑盒。在这些章节中，我们特别关注它们如何用于集成方法。第9.5节介绍了一种称为可解释提升机（explainable
    boosting machines）的玻璃盒方法，这是一种旨在直接可解释的新集成方法，它提供了全局和局部可解释性。
- en: 9.1 What is interpretability?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 什么是可解释性？
- en: We first introduce the basics of interpretability and explainability for machine-learning
    models generally, before moving to how these concepts apply to ensemble methods
    specifically. The notions of interpretability and explainability of a machine-learning
    model are related to its structure (e.g., a tree, a network, or a linear model)
    and its parameters (e.g., split and leaf values in trees, layer weights in neural
    networks, feature weights in linear models). Our goal is to understand a model’s
    behavior in terms of its input features, output predictions, and the model internals
    (i.e., structure and parameters).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍机器学习模型的可解释性和可解释性的基础知识，然后转向这些概念如何具体应用于集成方法。机器学习模型的可解释性和可解释性与其结构（例如，树、网络或线性模型）及其参数（例如，树中的分割和叶值、神经网络中的层权重、线性模型中的特征权重）相关。我们的目标是根据输入特征、输出预测和模型内部（即结构和参数）来理解模型的行为。
- en: 9.1.1 Black-box vs. glass-box models
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 黑盒与玻璃盒模型
- en: '*Black-box machine-learning models* are difficult to describe in terms of their
    model internals. This can be because we don’t have access to the internal model
    structure and parameters (e.g., if it was trained by someone else). Even in cases
    where we do have access to the model internals, the model itself may be sufficiently
    complex that it’s not easy to analyze and establish an intuitive understanding
    of the relationship between its inputs and outputs (see figure 9.1).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*黑盒机器学习模型*在描述其模型内部结构方面比较困难。这可能是因为我们没有访问内部模型结构和参数（例如，如果它是由其他人训练的）。即使在我们可以访问模型内部的情况下，模型本身可能足够复杂，以至于分析其输入和输出之间的关系并建立直观理解并不容易（见图9.1）。'
- en: '![CH09_F01_Kunapuli](../Images/CH09_F01_Kunapuli.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F01_Kunapuli](../Images/CH09_F01_Kunapuli.png)'
- en: Figure 9.1 With black-box machine-learning models, we can only use the input-output
    pairs to analyze and explain model behavior. The model internals in a black box
    are either unavailable or aren’t directly interpretable. With glass-box machine-learning
    models, in addition to input-output pairs, the model internals are also intuitively
    interpretable.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 在黑盒机器学习模型中，我们只能使用输入-输出对来分析和解释模型行为。黑盒中的模型内部要么不可用，要么不能直接解释。在玻璃盒机器学习模型中，除了输入-输出对之外，模型内部也是直观可解释的。
- en: Neural networks and deep learning models are often cited as examples of black-box
    models, owing to the considerable complexity arising from their multilayered structure
    and large number of network parameters.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络和深度学习模型通常被引用为黑盒模型的例子，这归因于它们多层结构和大量网络参数带来的相当复杂性。
- en: 'These models essentially function as black boxes: given an input example, they
    provide a prediction, but their inner workings are opaque to us. This makes interpreting
    model behavior pretty hard.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型本质上作为黑盒运行：给定一个输入示例，它们提供预测，但它们的内部工作原理对我们来说是透明的。这使得解释模型行为相当困难。
- en: Many of the ensemble methods we’ve seen so far—random forests, AdaBoost, gradient
    boosting, and Newton boosting—are all effectively black-box models to us. This
    is because, even though the individual base estimators themselves may be intuitive
    and interpretable, the process of ensembling introduces complex interactions between
    the features, which, in turn, makes it hard to interpret the ensemble and its
    predictions. Black-box models typically require *black-box explainers*, which
    are explanation models that aim to explain model behavior using only a model’s
    inputs and outputs, but not its internals.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止看到的大多数集成方法——随机森林、AdaBoost、梯度提升和牛顿提升——对我们来说都是有效的黑盒模型。这是因为，尽管单个基估计器本身可能直观且可解释，但集成过程引入了特征之间的复杂交互，这使得解释集成及其预测变得困难。黑盒模型通常需要*黑盒解释器*，这些解释模型旨在仅使用模型输入和输出，而不是其内部结构来解释模型行为。
- en: Glass-box machine-learning models, on the other hand, are easier to understand.
    This is often because their model structures are immediately intuitive or comprehensible
    to humans.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，玻璃盒机器学习模型更容易理解。这通常是因为它们的模型结构对人类来说是立即直观或可理解的。
- en: 'For example, consider a simple task of diabetes diagnosis from only two features:
    age and blood-glucose test result (glc). Let’s say that we’ve learned two machine-learning
    models that have identical predictive performance: a fourth-degree polynomial
    classifier and a decision-tree classifier.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个简单的任务，即仅从两个特征：年龄和血糖测试结果（glc）中进行糖尿病诊断。假设我们已经学习了两个具有相同预测性能的机器学习模型：一个四次多项式分类器和一棵决策树分类器。
- en: The data set for this example is shown in figure 9.2, where patients who don’t
    have diabetes (class=-1) are denoted by squares and patients who have diabetes
    (class=+1) are denoted by circles. The two classification models are also shown.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的数据集如图9.2所示，其中没有糖尿病（类别=-1）的患者用方块表示，有糖尿病（类别=+1）的患者用圆圈表示。两个分类模型也显示在图中。
- en: '![CH09_F02_Kunapuli](../Images/CH09_F02_Kunapuli.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F02_Kunapuli](../Images/CH09_F02_Kunapuli.png)'
- en: 'Figure 9.2 The problem space of diabetic patients who have to be classified
    as having diabetes (circles) or not having diabetes (squares) is based on two
    features: age and glc. Two machine-learning models—a fourth-degree polynomial
    classifier and a decision-tree classifier—are trained to have roughly similar
    predictive performance. However, the nature of their model internals (structure
    and parameters) means that decision trees are more intuitive for explanations
    and for understanding model behavior (see section 9.1.2).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 需要被分类为有糖尿病（圆圈）或无糖尿病（方块）的糖尿病患者的问题空间基于两个特征：年龄和glc。两个机器学习模型——一个四次多项式分类器和一棵决策树分类器——被训练以具有大致相似的预测性能。然而，它们模型内部结构（结构和参数）的性质意味着决策树对于解释和理解模型行为来说更直观（参见9.1.2节）。
- en: 'The first model is a fourth-degree polynomial classifier. This classifier has
    an additive structure made up of weighted feature powers, and the weights are
    the model parameters:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个模型是一个四次多项式分类器。这个分类器由加权特征幂的加性结构组成，权重是模型参数：
- en: '![CH09_F02_Kunapuli-eqs-0x](../Images/CH09_F02_Kunapuli-eqs-0x.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F02_Kunapuli-eqs-0x](../Images/CH09_F02_Kunapuli-eqs-0x.png)'
- en: This function returns either +1 (diabetes = TRUE) or -1 (diabetes = FALSE).
    Even with the full model available to us, given a new patient and resulting diagnostic
    prediction (say, diabetes = TRUE), it’s not immediately clear why the model made
    the decision it did. Was it because of the patient’s age? Their blood-glucose
    test result? Both of these factors? This information is buried within complex
    mathematical calculations that aren’t easy for us to infer by simply looking at
    the model, its structure, and parameters.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数返回+1（糖尿病=TRUE）或-1（糖尿病=FALSE）。即使我们有完整的模型，给定一个新患者和相应的诊断预测（例如，糖尿病=TRUE），也不立即清楚模型为何做出这样的决定。是因为患者的年龄？他们的血糖测试结果？这两个因素？这些信息隐藏在复杂的数学计算中，我们仅通过观察模型、其结构和参数很难推断出来。
- en: Now, let’s consider a second model, a decision tree with a single decision node
    of the form
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑第二个模型，一个具有单个决策节点形式的决策树
- en: '![CH09_F02_Kunapuli-eqs-1x](../Images/CH09_F02_Kunapuli-eqs-1x.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F02_Kunapuli-eqs-1x](../Images/CH09_F02_Kunapuli-eqs-1x.png)'
- en: This function also returns either +1 (diabetes = TRUE) or -1 (diabetes = FALSE).
    However, the structure of this decision tree is easily interpretable as
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数也返回+1（糖尿病=TRUE）或-1（糖尿病=FALSE）。然而，这个决策树的结构很容易解释为
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This model’s interpretation is pretty straightforward: any patient who is over
    the age of 45 and has a blood glucose test result over 140 will be diagnosed as
    having diabetes.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的解释相当直接：任何年龄超过 45 岁且血糖测试结果超过 140 的患者将被诊断为糖尿病。
- en: In summary, even though the full model internals of the polynomial classifier
    are available to us, the model might as well be a black box since the model internals
    aren’t intuitive or interpretable. On the other hand, the inherent nature of how
    the decision tree represents the knowledge it has learned allows for easier interpretation,
    making it a glass-box model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，尽管多项式分类器的完整模型内部结构对我们来说是可用的，但由于模型内部不直观或不可解释，模型可能仍然像一个黑盒。另一方面，决策树如何表示其学习到的知识的天生性质使得它更容易解释，使其成为一个玻璃盒模型。
- en: 'In the rest of this section, we’ll explore two familiar machine-learning models
    that are also glass-box models: decision trees (and decision rules) and generalized
    linear models (GLMs). This will set us up to better understand the notions of
    interpretability and explainability for ensembles as both GLMs and decision trees
    are commonly used as base estimators in many ensemble methods.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分，我们将探讨两种熟悉的机器学习模型，它们也是玻璃盒模型：决策树（和决策规则）以及广义线性模型（GLMs）。这将使我们更好地理解集成模型的解释性和可解释性概念，因为
    GLMs 和决策树通常被用作许多集成方法中的基础估计器。
- en: 9.1.2 Decision trees (and decision rules)
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 决策树（和决策规则）
- en: Decision trees are arguably the most interpretable of machine-learning models
    as they implement decision-making as a sequential process of asking and answering
    questions. The tree structure of a decision tree and its feature-based splitting
    functions are easy to interpret, as we’ll see. This makes decision trees glass-box
    models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可以说是机器学习模型中最可解释的，因为它们将决策实现为一个询问和回答问题的连续过程。决策树的结构及其基于特征的分割函数很容易解释，正如我们将看到的。这使得决策树成为玻璃盒模型。
- en: 'Let’s begin by training a decision tree on the well-known Iris data set, which
    is available in scikit-learn. The task is a three-way classification of irises
    into three species, *Iris setosa*, *Iris versicolour*, and *Iris virginica*, based
    on four features: sepal height, sepal width, petal height, and petal width. This
    exceedingly simple data set only has 150 training examples and will serve as a
    good teaching example for the notion of visualization.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从在著名的 Iris 数据集上训练决策树开始，该数据集在 scikit-learn 中可用。任务是按照四个特征：花瓣高度、花瓣宽度、花瓣宽度和花瓣宽度，将鸢尾花分为三种物种：*Iris
    setosa*、*Iris versicolour* 和 *Iris virginica*。这个极其简单的数据集只有 150 个训练示例，将作为可视化概念的优良教学示例。
- en: Interpreting decision trees in practice
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 实践中解释决策树
- en: The following listing loads the data set, trains a decision-tree classifier,
    and visualizes it. Once visualized, we can interpret the learned decision-tree
    model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码示例加载数据集，训练决策树分类器，并可视化它。一旦可视化，我们就可以解释学习到的决策树模型。
- en: Listing 9.1 Training and interpreting decision trees
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.1 训练和解释决策树
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Loads the Iris data set and splits the data into training and test sets
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载 Iris 数据集并将数据分为训练集和测试集
- en: ❷ Uses entropy as the criterion to measure quality of splits during learning
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在学习过程中使用熵作为衡量分割质量的标准
- en: ❸ Trains a decision-tree classifier and evaluates its test set performance
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 训练决策树分类器并评估其测试集性能
- en: ❹ Exports the tree internals to dot format and then renders using graphviz
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将树内部结构导出为 dot 格式，然后使用 graphviz 进行渲染
- en: The resulting decision tree achieves 91.3% accuracy on the Iris data set. Note
    that, as Iris is a very simple data set, many different high-accuracy decision
    trees can be trained,
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的决策树在 Iris 数据集上达到了 91.3% 的准确率。请注意，由于 Iris 是一个非常简单的数据集，可以训练出许多不同的高准确率决策树，
- en: one of which is shown here. We visualize it using the open source graph visualization
    software graph- viz package (see figure 9.3), which is used to render lists, trees,
    graphs, and networks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中之一在此处展示。我们使用开源的图形可视化软件 graph-viz 包（见图 9.3）来可视化它，该软件用于渲染列表、树、图和网络。
- en: '![CH09_F03_Kunapuli](../Images/CH09_F03_Kunapuli.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F03_Kunapuli](../Images/CH09_F03_Kunapuli.png)'
- en: 'Figure 9.3 Decision tree learned on the Iris data set for classification of
    irises into three species: *Iris setosa*, *Iris versicolour*, and *Iris virginica*.
    The standard convention for splits is followed here: if the split function evaluates
    to true, we proceed to the right branch; if it evaluates to false, the left branch.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 在Iris数据集上学习用于将鸢尾花分类为三种物种（*Iris setosa*、*Iris versicolour*和*Iris virginica*）的决策树。这里遵循了标准分裂惯例：如果分裂函数评估为真，则继续向右分支；如果评估为假，则向左分支。
- en: The first thing we notice is that only two of the four features, petal width
    and length, are enough to achieve over 90% accuracy. Thus, this decision tree
    has learned a *sparse model* by using only a subset of the features. But we can
    glean far more than that from this.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先注意到，只有四个特征中的两个，即花瓣宽度和长度，就足以达到超过90%的准确率。因此，这个决策树通过仅使用特征子集就学习了一个*稀疏模型*。但我们能从中得到的信息远不止这些。
- en: A nice property of decision trees is that each path from root node to leaf node
    represents it. At every split, since an example can either go left or right only,
    the example can only end up at one of the three leaf nodes. This means that each
    leaf node (and by extension, each path from root to leaf, i.e., each rule) partitions
    the overall population into a subpopulation. Let’s actually see this in action.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的一个良好特性是，从根节点到叶节点的每一条路径都代表了它。在每次分裂时，由于一个示例只能向左或向右移动，因此示例只能最终结束在三个叶节点之一。这意味着每个叶节点（以及由此扩展的从根到叶的每条路径，即每条规则）将总体人口划分为一个子人口。让我们实际看看这是如何运作的。
- en: 'Since there are three leaf nodes, there are three decision rules, which we
    can write in Python syntax to understand them easily:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有三个叶节点，因此有三个决策规则，我们可以用Python语法来写出它们，以便更容易理解：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In general, every decision tree can be expressed as a set of decision rules,
    which are more easily comprehensible to humans owing to their if-then structure.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，每个决策树都可以表示为一组决策规则，由于它们的if-then结构，这些规则对人类来说更容易理解。
- en: NOTE The interpretability of decision trees can be subjective and depends on
    the depth of the tree and the number of leaf nodes. Trees of small-to-medium depth
    (say, up to depth 3 or 4) and approximately 8-15 nodes are generally more intuitive
    and easier to understand. As the tree depth and number of leaf nodes increase,
    the number and length of decision rules we’ll have to contend with and interpret
    also increase. This makes deep and complex decision trees more like black boxes
    and also rather difficult to interpret.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：决策树的解释性可能是主观的，并且取决于树的深度和叶节点的数量。深度为中小型（例如，深度为3或4）且节点数约为8-15的树通常更直观，更容易理解。随着树深和叶节点数量的增加，我们需要处理和解释的决策规则的数量和长度也会增加。这使得深度和复杂的决策树更像黑盒，并且也相当难以解释。
- en: Remember that every example that passes through a decision tree must end up
    at one and only one of the leaf nodes. Thus, the set of paths from the root to
    the leaves will fully cover all the examples. What’s more, the tree/rules will
    partition the space of all irises into three nonoverlapping subpopulations, each
    corresponding to one of the three species. This is very helpful for visualization
    and interpretation, as shown in figure 9.4.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，每个通过决策树的示例都必须最终结束在唯一的叶节点上。因此，从根到叶的路径集将完全覆盖所有示例。更重要的是，树/规则将将所有鸢尾花的空间划分为三个非重叠的子人口，每个对应于三种物种之一。这在可视化和理解方面非常有帮助，如图9.4所示。
- en: '![CH09_F04_Kunapuli](../Images/CH09_F04_Kunapuli.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F04_Kunapuli](../Images/CH09_F04_Kunapuli.png)'
- en: Figure 9.4 Decision trees partition the feature space into nonoverlapping subspaces,
    where each subspace denotes a subpopulation of the examples.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 决策树将特征空间划分为非重叠的子空间，其中每个子空间表示示例的子人口。
- en: Feature importances
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性
- en: 'We know from the tree that two features are used: petal length and petal width.
    But how much did each feature contribute to the model? This is the notion of feature
    importance in which we ascribe a score to each feature depending on how much it
    influences overall decision making in a model. In a decision tree, feature importances
    can be computed very easily!'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从树中我们知道使用了两个特征：花瓣长度和花瓣宽度。但每个特征对模型贡献了多少？这就是特征重要性的概念，我们根据每个特征对模型整体决策影响的大小为每个特征分配一个分数。在决策树中，特征重要性可以非常容易地计算出来！
- en: Let’s compute the feature importances for each feature in the tree shown earlier
    in figure 9.3, keeping in mind a couple of important details. First, the training
    set consisted of 127 training examples (samples = 127 in the root node). Next,
    this tree was trained using entropy as the split-quality criterion (refer to listing
    9.1).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算在图9.3中较早显示的树中每个特征的特性重要性，同时记住一些重要的细节。首先，训练集由127个训练示例（根节点中的样本 = 127）组成。其次，此树使用熵作为分裂质量标准进行训练（参见图9.1）。
- en: Thus, to measure feature importance, we simply compute how much each feature
    decreases entropy overall after the split. To avoid skewing our perception of
    splits with a very small or very large proportion of examples, we’ll also weight
    the entropy decrease.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了衡量特征重要性，我们只需计算每个特征在分裂后整体熵减少的程度。为了避免因示例比例非常小或非常大而扭曲我们对分裂的感知，我们还将对熵减少进行加权。
- en: 'More precisely, for each split node, we compute how much its (weighted) entropy
    decreases with respect to its child nodes after the split:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 更确切地说，对于每个分裂节点，我们计算分裂后其（加权）熵相对于其子节点的减少程度：
- en: '![CH09_F04_Kunapuli-eqs-2x](../Images/CH09_F04_Kunapuli-eqs-2x.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F04_Kunapuli-eqs-2x](../Images/CH09_F04_Kunapuli-eqs-2x.png)'
- en: 'For the node [petal_width <= 0.8]:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于节点 [petal_width <= 0.8]：
- en: '![CH09_F04_Kunapuli-eqs-3x](../Images/CH09_F04_Kunapuli-eqs-3x.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F04_Kunapuli-eqs-3x](../Images/CH09_F04_Kunapuli-eqs-3x.png)'
- en: 'For the node [petal_length <= 4.85]:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于节点 [petal_length <= 4.85]：
- en: '![CH09_F04_Kunapuli-eqs-4x](../Images/CH09_F04_Kunapuli-eqs-4x.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F04_Kunapuli-eqs-4x](../Images/CH09_F04_Kunapuli-eqs-4x.png)'
- en: 'Since the other two features aren’t used in the model, their feature importances
    will be zero. The final step is to normalize the feature importances so that they
    sum to 1:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其他两个特征在模型中没有使用，它们的特征重要性将为零。最后一步是将特征重要性归一化，使它们的总和为1：
- en: '![CH09_F04_Kunapuli-eqs-5x](../Images/CH09_F04_Kunapuli-eqs-5x.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F04_Kunapuli-eqs-5x](../Images/CH09_F04_Kunapuli-eqs-5x.png)'
- en: 'In practice, we don’t have to compute feature importances ourselves as most
    implementations of decision-tree learning do so. For example, the feature importances
    of the decision tree we just trained from listing 9.1 can be obtained directly
    from the model (compare with our preceding computation):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们不必自己计算特征重要性，因为大多数决策树学习的实现都这样做。例如，我们可以直接从模型中获取我们刚刚从列表9.1中训练的决策树的特征重要性（与我们的先前计算进行比较）：
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Finally, the preceding example showed the interpretability of decision trees
    for classification problems. Decision-tree regressors can also be interpreted
    in the same way; the only difference is that the leaf nodes will be regression
    values instead of class labels.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，前面的例子展示了决策树在分类问题中的可解释性。决策树回归器也可以以相同的方式进行解释；唯一的区别是叶子节点将是回归值而不是类标签。
- en: 9.1.3 Generalized linear models
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.3 广义线性模型
- en: 'We now revisit GLMs, which were originally introduced in chapter 7, section
    7.1.4\. Recall that GLMs extend linear models through a (nonlinear) link function,
    *g*(*y*). For example, linear regression uses the identity link to relate the
    regression values y to the data *x*:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在重新审视GLMs，它们最初在第7章第7.1.4节中介绍。回想一下，GLMs通过一个（非线性）链接函数 *g*(*y*) 扩展线性模型。例如，线性回归使用恒等链接将回归值
    *y* 与数据 *x* 相关联：
- en: '![CH09_F04_Kunapuli-eqs-7x](../Images/CH09_F04_Kunapuli-eqs-7x.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F04_Kunapuli-eqs-7x](../Images/CH09_F04_Kunapuli-eqs-7x.png)'
- en: 'Here, the data point *x* = [*x*[1], ⋅⋅⋅ ,*x*[d]]'' is described by *d* features,
    and the linear model is parameterized by the linear coefficients *β*[1], ⋅⋅⋅ ,*β*[d]
    and the intercept (sometimes called the bias) *β*[0]. Another example of a GLM
    is logistic regression, which uses the logit link to relate class probabilities
    *p* to the data x:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，数据点 *x* = [*x*[1], ⋅⋅⋅ ,*x*[d]]' 由 *d* 个特征描述，线性模型由线性系数 *β*[1], ⋅⋅⋅ ,*β*[d]
    和截距（有时称为偏差）*β*[0] 参数化。GLM的另一个例子是逻辑回归，它使用logit链接将类概率 *p* 与数据 x 相关联：
- en: '![CH09_F04_Kunapuli-eqs-8x](../Images/CH09_F04_Kunapuli-eqs-8x.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F04_Kunapuli-eqs-8x](../Images/CH09_F04_Kunapuli-eqs-8x.png)'
- en: GLMs are interpretable due to their linear and additive structure. The linear
    parameters themselves give us an intuitive sense of each feature’s contribution
    to the overall prediction. The additive structure ensures that the overall prediction
    depends on the individual contributions from each feature.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: GLMs由于其线性加性结构而具有可解释性。线性参数本身为我们提供了对每个特征对整体预测贡献的直观感觉。加性结构确保整体预测依赖于每个特征的个别贡献。
- en: 'For example, consider that we’ve trained a logistic regression model for the
    diabetes diagnosis task discussed earlier, to classify if a patient has diabetes,
    using two features: age and blood-glucose test result (glc). Let’s say the learned
    model is (with *p*(*y* = 1) as *p*):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们已经为之前讨论的糖尿病诊断任务训练了一个逻辑回归模型，用于根据年龄和血糖测试结果（glc）来分类病人是否患有糖尿病。让我们说学到的模型是（*p*(y
    = 1)作为*p*）：
- en: '![CH09_F04_Kunapuli-eqs-9x](../Images/CH09_F04_Kunapuli-eqs-9x.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F04_Kunapuli-eqs-9x](../Images/CH09_F04_Kunapuli-eqs-9x.png)'
- en: Recall that if *p* is the probability of a positive diagnosis, then *p*/(1 –
    *p*) are the *odds* that the patient has the diagnosis. Thus, logistic regression
    represents the log-odds of a positive diabetes diagnosis as a weighted combination
    of the features age and glc.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，如果*p*是阳性诊断的概率，那么*p/(1 – *p*)是病人有诊断的几率。因此，逻辑回归将阳性糖尿病诊断的对数几率表示为年龄和glc特征的加权组合。
- en: Feature weights
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 特征权重
- en: How can we interpret the feature weights? If age is increased by 1, log (*p*/(1
    – *p*)) will increase by 0.5 (because the model is linear and additive). Thus,
    for a patient who is a year older, their log-odds of a positive diabetes diagnosis
    are log (*p*/(1 – *p*)) = 0.5\. Consequently, their odds of a positive diabetes
    diagnosis are (*p*/(1 – *p*)) = *e*^(0.5) = 1.65, or 65% more.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解释特征权重？如果年龄增加1，log(*p/(1 – *p*))将增加0.5（因为模型是线性和可加的）。因此，对于一个年龄增加一岁的病人，他们患有糖尿病的log几率是log(*p/(1
    – *p*)) = 0.5。因此，他们患有糖尿病的几率是(*p/(1 – *p*)) = *e^(0.5) = 1.65，即增加65%。
- en: In a similar vein, if glc is increased by 1, log (*p*/(1 – *p*)) will decrease
    by 0.29 (note the minus in the weight, indicating a decrease). Thus, for a patient
    whose glc increases by 1, their odds of a positive diabetes diagnosis are (*p*/(1
    – *p*)) = *e*^(0.29) = 0.75, or 25% less.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果glc增加1，log(*p/(1 – *p*))将减少0.29（注意权重中的负号，表示减少）。因此，对于一个glc增加1的病人，他们患有糖尿病的几率是(*p/(1
    – *p*)) = *e^(0.29) = 0.75，即减少25%。
- en: Let’s take this intuition and see how we can interpret a more realistic logistic
    regression model. We begin by training a logistic regression model on the Breast
    Cancer data set that was first introduced in chapter 2's case study. The task
    is binary classification for breast cancer diagnosis. Each example in the data
    set is characterized by 30 features extracted from an image of the breast mass.
    These features represent properties such as the radius, perimeter, area, concavity,
    and so on of the breast mass.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用这种直觉来探讨如何解释一个更现实的逻辑回归模型。我们首先在第二章案例研究中首次引入的乳腺癌数据集上训练一个逻辑回归模型。任务是乳腺癌诊断的二分类。数据集中的每个例子都由从乳腺肿块图像中提取的30个特征来表征。这些特征代表乳腺肿块的属性，如半径、周长、面积、凹凸性等。
- en: Interpreting GLMs in practice
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 实际应用中解释广义线性模型
- en: The next listing loads the data set, trains a logistic regression classifier,
    and visualizes the increase or decrease in the odds of a positive breast cancer
    diagnosis of each feature.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表加载数据集，训练逻辑回归分类器，并可视化每个特征阳性乳腺癌诊断几率的变化或减少。
- en: Listing 9.2 Training and interpreting logistic regression
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.2 训练和解释逻辑回归
- en: '[PRE4]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Loads the Breast Cancer data set and splits the data into training and test
    sets
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载乳腺癌数据集并将数据分为训练集和测试集
- en: ❷ Preprocesses the features to ensure they are all the same scale
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 预处理特征以确保它们具有相同的尺度
- en: ❸ Trains a logistic regression classifier and evaluates its test set performance
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 训练逻辑回归分类器并评估其测试集性能
- en: ❹ Computes the increase or decrease in odds as “exp(weight) - 1”
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算几率的变化为“exp(weight) - 1”
- en: ❺ Visualizes the change in odds as a bar chart
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将几率的变化以条形图的形式可视化
- en: The odds of each feature *i* are calculated from the weights as *odds*[i] =
    *e*^(w[i]). The change in odds is calculated as *change*[i] = *odds*[i] - 1 =
    *e*^(w[i]) - 1, and visualized in figure 9.5.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 每个特征*i*的几率是根据权重计算的，即*odds*[i] = *e^(w[i])。几率的变化计算为*change*[i] = *odds*[i] -
    1 = *e^(w[i]) - 1，并在图9.5中可视化。
- en: '![CH09_F05_Kunapuli](../Images/CH09_F05_Kunapuli.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F05_Kunapuli](../Images/CH09_F05_Kunapuli.png)'
- en: Figure 9.5 Interpreting a logistic regression, that is, a linear model for classification,
    for breast cancer diagnosis. Positive feature weights lead to increased odds of
    breast cancer, negative feature weights lead to decreased odds of breast cancer,
    and zero feature weights don’t affect the odds of breast cancer.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 解释逻辑回归，即用于乳腺癌诊断的线性分类模型。正特征权重会导致乳腺癌的概率增加，负特征权重会导致乳腺癌的概率减少，而零特征权重不会影响乳腺癌的概率。
- en: If the feature weight *w*[i] > 0, then *odds*[i] > 1, and it will increase the
    odds of a positive diagnosis (*change*[i] > 0). If a feature weight *w*[i] < 0,
    then *odds*[i] < 1, and it will decrease the odds of a positive diagnosis (*change*[i]
    < 0). If a feature weight *w*[i] = 0, then *odds*[i] = 1 and that feature doesn’t
    affect the diagnosis (*change*[i] = 0).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征权重 *w*[i] 大于 0，则 *odds*[i] 大于 1，这将增加阳性诊断的概率 (*change*[i] 大于 0)。如果特征权重 *w*[i]
    小于 0，则 *odds*[i] 小于 1，这将减少阳性诊断的概率 (*change*[i] 小于 0)。如果特征权重 *w*[i] 等于 0，则 *odds*[i]
    等于 1，并且该特征不会影响诊断 (*change*[i] 等于 0)。
- en: This last part is an important component of learning *sparse linear models*,
    where we train a model as a mixture of zero and nonzero feature weights. A zero
    feature weight means that that feature doesn’t contribute to the model and can
    be effectively dropped. This, in turn, allows for a sparser feature set and leaner,
    more interpretable models!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这最后一部分是学习稀疏线性模型的重要组件，其中我们将模型训练为零和零特征权重的混合体。零特征权重意味着该特征不会对模型做出贡献，并且可以被有效地删除。这反过来又允许具有更稀疏的特征集和更精简、更可解释的模型！
- en: NOTE The interpretability of linear models is dependent on the relative scaling
    between the features. For example, age might range from 18 to 65, while salary
    might range from $30,000 to $90,000\. This disparity in features affects the underlying
    weight learning, and the feature with the higher weight range (in this case, salary)
    will dominate the models. When we interpret such models, we might incorrectly
    ascribe greater significance to such features. To train a robust model that considers
    all the features equally during learning, care must be taken to properly preprocess
    the data to ensure all features are in the same numerical range.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：线性模型的可解释性取决于特征之间的相对缩放。例如，年龄可能在 18 到 65 岁之间，而薪水可能在 30,000 美元到 90,000 美元之间。这种特征差异会影响底层权重学习，具有更高权重范围的特征（在本例中为薪水）将主导模型。当我们解释此类模型时，我们可能会错误地将更大的重要性归因于这些特征。为了训练一个在训练过程中平等考虑所有特征的稳健模型，必须小心地预处理数据，以确保所有特征都在相同的数值范围内。
- en: Linear regression models can also be interpreted similarly. In this case, rather
    than compute the odds, we can compute the contribution of each feature to the
    regression value directly, since the regression value *y* = *β*[0] + *β*[1]*x*[1]
    + ⋅⋅⋅ + *β*[d]*x*[d].
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型也可以以类似的方式进行解释。在这种情况下，我们不是计算概率，而是可以直接计算每个特征对回归值贡献的大小，因为回归值 *y* = *β*[0]
    + *β*[1]*x*[1] + ⋅⋅⋅ + *β*[d]*x*[d]。
- en: '9.2 Case study: Data-driven marketing'
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 案例研究：数据驱动营销
- en: In the rest of this chapter, we’ll explore how we can train both black-box and
    glass-box ensembles in the context of a machine-learning task from the domain
    of data-driven marketing. Data-driven marketing aims to use customer and socioeconomic
    information to identify customers who will be most receptive to certain types
    of marketing strategies. This allows businesses to target specific customers with
    advertisements, offers, and sales in an optimal and personalized way.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将探讨如何在数据驱动营销领域的机器学习任务中训练黑盒和玻璃盒集成。数据驱动营销旨在利用客户和社会经济信息来识别最有可能对某些类型的营销策略做出积极响应的客户。这允许企业以最优化和个性化的方式针对特定客户进行广告、优惠和销售。
- en: 9.2.1 Bank Marketing data set
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 银行营销数据集
- en: We’ll consider the Bank Marketing data set[¹](#pgfId-1176195) from the UCI Machine
    Learning Repository ([http://mng.bz/VpXP](http://mng.bz/VpXP)), where the data
    comes from a phone-based direct marketing campaign of a Portuguese bank. The task
    is to predict if a customer will subscribe to a fixed-term deposit.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将考虑来自 UCI 机器学习仓库的银行营销数据集[¹](#pgfId-1176195)（[http://mng.bz/VpXP](http://mng.bz/VpXP)），其中数据来自一家葡萄牙银行的基于电话的直接营销活动。任务是预测客户是否会订阅定期存款。
- en: 'This data set is also available with the source code. For each customer in
    the data set, there are four types of features: demographic attributes, details
    of the last phone contact, overall campaign information pertaining to this customer,
    and general socioeconomic indicators. The details are illustrated in table 9.1.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集也附带源代码。对于数据集中的每位客户，都有四种类型的特征：人口统计属性、上次电话联系详情、与该客户相关的整体营销活动信息，以及一般社会经济指标。详细信息见表9.1。
- en: Table 9.1 Features and target of the Bank Marketing data set, grouped by the
    feature, type, and source
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.1 按特征、类型和来源分组展示银行营销数据集的特征和目标
- en: '| Feature | Type | Feature description |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 类型 | 特征描述 |'
- en: '| **Client demographic attributes and financial indicators** |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| **客户人口统计属性和金融指标** |'
- en: '| age | Continuous | Age of the customer |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| age | 连续 | 客户年龄 |'
- en: '| job | Categorical | Type of job (12 categories, e.g., blue-collar, retired,
    self-employed, student, services, etc., and unknown) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| job | 分类 | 职业类型（12个类别，例如，蓝领、退休、自雇、学生、服务行业等，以及未知）|'
- en: '| marital | Categorical | Marital status (divorced, married, single, unknown)
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| marital | 分类 | 婚姻状况（离婚、已婚、单身、未知）|'
- en: '| education | Categorical | Highest education (8 categories, e.g., high school,
    university degree, professional course, and unknown) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| education | 分类 | 最高教育程度（8个类别，例如，高中、大学学位、专业课程，以及未知）|'
- en: '| default | Categorical | Does customer have credit in default? (yes, no, unknown)
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| default | 分类 | 客户是否有信用违约？（是、否、未知）|'
- en: '| housing | Categorical | Does customer have a housing loan? (yes, no, unknown)
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| housing | 分类 | 客户是否有住房贷款？（是、否、未知）|'
- en: '| loan | Categorical | Does customer have a personal loan? (yes, no, unknown)
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| loan | 分类 | 客户是否有个人贷款？（是、否、未知）|'
- en: '| **Date and time conditions of last marketing contact** |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **上次营销接触的日期和时间条件** |'
- en: '| contact | Binary | Contact communication type (cell phone, telephone) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| contact | 二元 | 联系沟通类型（手机、电话）|'
- en: '| month | Categorical | Last contact month (12 categories: January-December)
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| month | 分类 | 上次联系月份（12个类别：一月至十二月）|'
- en: '| day-of-week | Categorical | Last contact weekday (5 categories: Monday-Friday)
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| day-of-week | 分类 | 上次联系的工作日（5个类别：周一至周五）|'
- en: '| **Marketing campaign details from current and previous campaigns** |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| **当前和上次营销活动的营销活动详情** |'
- en: '| campaign | Continuous | Total number of contacts during this campaign |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| campaign | 连续 | 在此次营销活动中的总联系次数 |'
- en: '| pdays | Continuous | Number of days since the last contact in previous campaign
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| pdays | 连续 | 上次营销活动以来与上次联系的天数 |'
- en: '| previous | Continuous | Number of contacts performed before this campaign
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| previous | 连续 | 在此次营销活动之前的联系次数 |'
- en: '| poutcome | Categorical | Outcome of previous marketing campaign (3 categories:
    failure, nonexistent, success) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| poutcome | 分类 | 上次营销活动的结果（3个类别：失败、不存在、成功）|'
- en: '| **General social and economic indicators** |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| **一般社会和经济指标** |'
- en: '| emp.var.rate | Continuous | Employment variation rate: quarterly indicator
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| emp.var.rate | 连续 | 就业变化率：季度指标 |'
- en: '| cons.price.idx | Continuous | Consumer price index: monthly indicator |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| cons.price.idx | 连续 | 消费者价格指数：月度指标 |'
- en: '| cons.conf.idx | Continuous | Consumer confidence index: monthly indicator
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| cons.conf.idx | 连续 | 消费者信心指数：月度指标 |'
- en: '| euribor3m | Continuous | Euribor three-month rate: daily indicator |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| euribor3m | 连续 | 欧洲银行同业拆借利率三个月期：日度指标 |'
- en: '| nr.employed | Continuous | Number of employees: quarterly indicator |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| nr.employed | 连续 | 员工人数：季度指标 |'
- en: '| **Prediction target** |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| **预测目标** |'
- en: '| subscribed? | Binary | Has the customer subscribed to a term deposit? |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| subscribed? | 二元 | 客户是否订阅了定期存款？|'
- en: 'It’s important to note that this data set is extremely imbalanced: only 10%
    of the customers in the data set subscribed to a term deposit as a result of this
    marketing campaign.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，这个数据集极度不平衡：在此营销活动下，只有10%的客户订阅了定期存款。
- en: Listing 9.3 loads the data set, splits the data into training and test sets,
    and preprocesses them. The continuous features are scaled to between 0 and 1 using
    scikit-learn’s MinMaxEncoder, and the categorical features are encoded with OrdinalEncoder.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.3加载数据集，将数据分为训练集和测试集，并进行预处理。连续特征使用scikit-learn的MinMaxEncoder缩放到0到1之间，分类特征使用OrdinalEncoder进行编码。
- en: Listing 9.3 Loading and preprocessing the Bank Marketing data set.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.3 加载和预处理银行营销数据集。
- en: '[PRE5]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Loads the data set
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载数据集
- en: ❷ Drops the “duration” column (see NOTE for a more detailed explanation)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 删除“duration”列（见注释以获取更详细的解释）
- en: ❸ Splits the data frame into features and labels
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将数据帧分为特征和标签
- en: ❹ Splits into train and test sets with stratified sampling to preserve class
    balances
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用分层抽样将数据分为训练集和测试集以保持类别平衡
- en: ❺ Preprocesses labels using “LabelEncoder”
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用“LabelEncoder”对标签进行预处理
- en: ❻ Preprocesses continuous features with “MinMaxEncoder” and categorical features
    with “OrdinalEncoder”
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用“MinMaxEncoder”对连续特征进行预处理，使用“OrdinalEncoder”对分类特征进行预处理
- en: To prevent data and target leakage (see chapter 8), we ensure that scaling and
    encoding functions are only fit to the training set before applying to the test
    set.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止数据与目标泄露（见第8章），我们确保在应用于测试集之前，缩放和编码函数仅适用于训练集。
- en: NOTE The original data set contains a feature called *duration*, which refers
    to the duration of the last phone call. Longer calls are highly correlated with
    the outcome of the call because longer calls indicate more engaged customers who
    are likelier to subscribe. However, unlike other features, which are known before
    making the call, we can’t possibly know a call’s duration ahead of time. In this
    way, the duration feature essentially behaves like a target variable since both
    duration and subscribed will immediately be known after a call. To build a realistic
    predictive model that can be deployed in practice with all features available
    *before* calling, we drop this feature from our modeling.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：原始数据集包含一个名为*duration*的特征，它指的是最后一次电话的持续时间。较长的通话与通话的结果高度相关，因为较长的通话表明客户更加投入，更有可能订阅。然而，与其他特征不同，这些特征在拨打电话之前是已知的，我们不可能提前知道通话的持续时间。因此，持续时间特征本质上就像一个目标变量，因为通话后持续时间与订阅状态都将立即知晓。为了构建一个可以在实践中部署的、在拨打电话前包含所有特征的现实预测模型，我们从我们的模型中删除了这个特征。
- en: 9.2.2 Training ensembles
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 训练集成
- en: 'We’ll now train two ensembles (from two different packages) on this data set:
    xgboost.XGBoostClassifier and sklearn.RandomForestClassifier. Both these models
    will be complex ensembles of 200 decision trees (weighted ensembles, in the case
    of XGBoost) and are effectively black boxes. Once trained, we’ll explore how to
    make these black boxes explainable in section 9.3.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将在这个数据集上训练两个集成（来自不同的包）：xgboost.XGBoostClassifier和sklearn.RandomForestClassifier。这两个模型都将是由200个决策树组成的复杂集成（在XGBoost的情况下是加权集成），并且实际上是黑盒。一旦训练完成，我们将在第9.3节中探讨如何使这些黑盒可解释。
- en: Listing 9.4 shows how we can train an XGBoost ensemble over this data set. We
    use a randomized grid search combined with 5-fold cross validation (CV) and early
    stopping (see chapter 6 for additional details) to select among various hyperparameters
    such as learning rate and regularization parameters.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.4展示了我们如何在这个数据集上训练XGBoost集成。我们使用随机网格搜索结合5折交叉验证（CV）和提前停止（见第6章以获取更多详细信息）来选择各种超参数，如学习率和正则化参数。
- en: Listing 9.4 Training XGBoost on the Bank Marketing data set
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.4 在银行营销数据集上训练XGBoost
- en: '[PRE6]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Creates a grid of hyperparameters for XGBoost
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为XGBoost创建超参数网格
- en: ❷ Initializes early stopping and sets early stopping rounds to 15
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化提前停止并设置提前停止轮数为15
- en: ❸ Sets the classification loss for XGBoost to the logistic loss
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将XGBoost的分类损失设置为逻辑损失
- en: ❹ Saves the best XGBoost model after CV
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在CV后保存最佳XGBoost模型
- en: Note also that one of the hyperparameters is scale_pos_weight, which allows
    us to weight positive and negative training examples differently. This is necessary
    since the Bank Marketing data set is imbalanced (10%:90% positive-to-negative
    example ratio). By weighting the positive examples more, we can ensure that their
    contribution isn’t drowned out by the larger proportion of negative examples.
    Here, we use cross validation to identify a weight for positive examples from
    among 5, 10, 50 and 100.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，其中一个超参数是scale_pos_weight，它允许我们以不同的方式对正负训练示例进行加权。这是必要的，因为银行营销数据集是不平衡的（正负例比例为10%:90%）。通过更多地加权正例，我们可以确保它们的贡献不会被更大的负例比例所淹没。在这里，我们使用交叉验证从5、10、50和100中确定正例的权重。
- en: This listing trains an XGBoostClassifier that achieves around 87.24% test set
    accuracy and 74.67% balanced accuracy. We can use a similar procedure to train
    a random forest over this data set. The main difference is that we set the class
    weights for positive examples to 10.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表训练了一个XGBoostClassifier，在测试集上实现了大约87.24%的准确率和74.67%的平衡准确率。我们可以使用类似的程序来训练这个数据集上的随机森林。主要区别在于我们为正例设置了类别权重为10。
- en: Listing 9.5 Training a random forest on the Bank Marketing data set
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.5 在银行营销数据集上训练随机森林
- en: '[PRE7]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Creates a grid of hyperparameters for “RandomForestClassifier”
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为“RandomForestClassifier”创建一个超参数网格
- en: ❷ Sets the weights for negative-to-positive examples to be 1:10
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将正负样本的权重设置为1:10
- en: ❸ Saves the best random forest after CV
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在交叉验证后保存最佳随机森林
- en: This listing trains a RandomForestClassifier that achieves around 84% test set
    accuracy.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表训练了一个RandomForestClassifier，在测试集上达到了大约84%的准确率。
- en: 9.2.3 Feature importances in tree ensembles
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.3 树集成中的特征重要性
- en: Most of the ensembles in this book (including XGBoostClassifier and RandomForestClassifier
    trained in the previous subsection) are tree ensembles as they use decision trees
    as their base estimators. One way to compute feature importances for an ensemble
    is to simply average the feature importances from the individual base decision
    trees!
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的大多数集成（包括在前一小节中训练的XGBoostClassifier和RandomForestClassifier）都是树集成，因为它们使用决策树作为其基本估计器。计算集成特征重要性的一个方法就是简单地平均各个基本决策树的特征重要性！
- en: 'In fact, the implementations of random forest (in scikit-learn) and XGBoost
    do this already, and we can obtain the ensemble feature importances using the
    following:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，随机森林（在scikit-learn中）和XGBoost的实现已经这样做了，我们可以使用以下方法获得集成特征重要性：
- en: '[PRE8]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We visualize and compare the feature importances of both these ensembles in
    figure 9.6 to interpret and understand their decision making.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图9.6中可视化和比较这两个集成的特征重要性，以解释和理解它们的决策过程。
- en: '![CH09_F06_Kunapuli](../Images/CH09_F06_Kunapuli.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F06_Kunapuli](../Images/CH09_F06_Kunapuli.png)'
- en: Figure 9.6 Feature importances of the ensembles learned by XGBoost (left bars)
    and random forest (right bars) classifiers
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 XGBoost（左柱）和随机森林（右柱）分类器学习的集成特征重要性
- en: Both ensembles ascribe significant importance to the socioeconomic indicator
    variables, in particular, nr.employed and emp.var.rate (which indicate unemployment
    rates), euribor3m (interbanking interest rates, which indicate macroeconomic stability),
    and cons.conf.idx (which indicates consumer optimism regarding their expected
    financial situation).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个集成都赋予了社会经济指标变量（尤其是nr.employed和emp.var.rate，它们表示失业率）、euribor3m（银行间利率，表示宏观经济稳定性）和cons.conf.idx（表示消费者对其预期财务状况的乐观程度）重要的权重。
- en: 'The XGBoost model, however, is strongly reliant on just one of the variables
    over the others: nr.employed. The overall takeaway from interpreting this is that
    people are more likely to subscribe to a fixed-term deposit account when the overall
    economic picture is without uncertainty or fluctuations, and is optimistic.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，XGBoost模型却高度依赖于其中一个变量，即nr.employed。从对这一点的解读中可以得出的总体结论是，当整体经济形势没有不确定性或波动，且乐观时，人们更有可能订阅定期存款账户。
- en: Feature importances allow us to understand what a model is doing overall and
    over different types of examples. That is, feature importances are a type of global
    explainability method.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性使我们能够理解模型在整体上以及在不同类型的示例上做了什么。也就是说，特征重要性是一种全局可解释性方法。
- en: 9.3 Black-box methods for global explainability
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 全局可解释性的黑盒方法
- en: 'Methods for machine-learning model explainability can be categorized into two
    types:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型可解释性的方法可以分为两种类型：
- en: Global methods attempt to generally explain a model’s decision-making process,
    and what factors are broadly relevant.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全局方法试图一般性地解释模型的决策过程以及哪些因素是广泛相关的。
- en: Local methods attempt to specifically explain a model’s decision-making process
    with respect to individual examples and predictions.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局部方法试图具体解释模型针对单个示例和预测的决策过程。
- en: Global explainability speaks to a model’s sensible behavior over a large number
    of examples when deployed or used in practice, whereas local explainability speaks
    to a model’s individual predictions on single examples that allow the user to
    make decisions on what to do next.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 全局可解释性涉及模型在部署或实际使用时在大量示例上的合理行为，而局部可解释性则涉及模型对单个示例的个别预测，这使用户能够决定下一步该做什么。
- en: In this section, we look at some global explainability methods for black-box
    models. These approaches only consider a model’s inputs and outputs and don’t
    use the model internals (hence, black box) to explain model behavior. For this
    reason, they can be used for global explainability of any machine-learning method
    and are also called *model-agnostic methods*.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨一些黑盒模型的全球可解释性方法。这些方法只考虑模型的输入和输出，不使用模型内部结构（因此，称为黑盒）来解释模型行为。因此，它们可以用于任何机器学习方法的全球可解释性，也被称为*模型无关方法*。
- en: 9.3.1 Permutation feature importance
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 排列特征重要性
- en: Feature importance in a machine-learning model refers to a score that indicates
    how good a feature is in a model, that is, how effective the feature is in a model’s
    decision-making process.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习模型中，特征重要性是指一个分数，表示特征在模型中的好坏，即特征在模型决策过程中的有效性。
- en: We’ve already seen how we can compute feature importances for decision trees
    and, by aggregation, for tree-based ensembles that use decision trees as base
    estimators. For tree-based methods, the feature importance calculation uses model
    internals such as the tree structure and split parameters. But what if these model
    internals aren’t available? Is there a black-box equivalent method for obtaining
    feature importances in such situations?
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何计算决策树的特征重要性，以及通过聚合，对于使用决策树作为基估计器的基于树的集成方法。对于基于树的方法，特征重要性计算使用模型内部信息，如树结构和分割参数。但如果这些模型内部信息不可用呢？在这种情况下，是否有黑盒等效方法来获取特征重要性？
- en: 'There is indeed: *permutation feature importance*. Recall that decision-tree
    feature importance scores each feature by how much it decreases the split criterion
    (e.g., Gini impurity or entropy for classification, squared error for regression).
    In contrast, permutation feature importance scores each feature by how much it
    increases the test error after we permute (shuffle) that feature’s values.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 确实存在：*排列特征重要性*。回想一下，决策树特征重要性分数是通过评估每个特征降低分割标准（例如，用于分类的基尼不纯度或熵，用于回归的平方误差）的程度来对每个特征进行评分的。相比之下，排列特征重要性是通过评估我们在排列（打乱）该特征的值之后，该特征增加测试错误的程度来对每个特征进行评分的。
- en: 'The intuition here is straightforward: if a feature is more important, then
    “messing with it” affects its ability to contribute to predictions and will increase
    the test error. If a feature is less important, then messing with it won’t have
    much of an effect on the model’s predictions and won’t affect the test error.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的直觉很简单：如果一个特征更重要，那么“干扰”它会影响其对预测的贡献，并增加测试错误。如果一个特征不太重要，那么干扰它对模型预测的影响不会很大，也不会影响测试错误。
- en: We “mess” with a feature by randomly permuting its values. This effectively
    snaps any relationship between that feature and its prediction. The procedure
    of permutation feature importance is illustrated in figure 9.7.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过随机排列特征值来“干扰”一个特征。这有效地切断了该特征与其预测之间的任何关系。排列特征重要性的过程在图9.7中进行了说明。
- en: '![CH09_F07_Kunapuli](../Images/CH09_F07_Kunapuli.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F07_Kunapuli](../Images/CH09_F07_Kunapuli.png)'
- en: Figure 9.7 The procedure for computing permutation feature importance illustrated
    for the third feature. This procedure is repeated for all features. Permutation
    feature importance uses only inputs and outputs to estimate feature importance
    and doesn’t use model internals (making this a model-agnostic approach).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 计算排列特征重要性的过程，以第三个特征为例。这个过程会重复应用于所有特征。排列特征重要性仅使用输入和输出估计特征重要性，不使用模型内部信息（这使得它成为一个模型无关的方法）。
- en: 'The permutation feature importance is elegant and simple in how it scores features
    without access to model internals. Here are some important technical details to
    keep in mind, though:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 排列特征重要性在如何评分特征而不访问模型内部信息方面既优雅又简单。尽管如此，这里有一些重要的技术细节需要记住：
- en: Permutation feature importance is a before-and-after score. It tries to estimate
    how the model’s predictive performance changes from before to after we shuffle
    (permute) features. To get a robust and unbiased estimate of the before-and-after
    model performance, it’s essential that we use a hold-out test set!
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排列特征重要性是一个前后评分。它试图估计在我们打乱（排列）特征之后，模型的预测性能如何从之前到之后发生变化。为了得到一个稳健且无偏的模型性能前后估计，使用保留的测试集是至关重要的！
- en: 'There are many ways to evaluate a model’s predictive performance depending
    on the task (classification or regression), the data set, and our own modeling
    goals. For this task, for instance, consider the following performance metrics:'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据任务（分类或回归）、数据集以及我们的建模目标，有许多方法可以评估模型的预测性能。对于这个任务，例如，考虑以下性能指标：
- en: '*Balanced accuracy*—Since this is a classification task, accuracy is a natural
    choice for a model evaluation metric. However, this data set is imbalanced with
    a 1:10 ratio of positive-to-negative examples. To account for this, we can use
    balanced accuracy, which ensures this skew is considered by weighting predictions
    by class size.'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*平衡准确率*——由于这是一个分类任务，准确率自然成为模型评估指标的一个选择。然而，这个数据集正负样本的比例为1:10，是不平衡的。为了解决这个问题，我们可以使用平衡准确率，它通过按类别大小加权预测来确保这种偏差被考虑在内。'
- en: '*Recall*—The purpose of this model is to identify high-value customers who
    will subscribe to fixed-term deposits. From this perspective, we want to minimize
    false negatives, or customers that our model thinks won’t subscribe, but who actually
    will! This type of wrong prediction costs us customers, and recall is a good metric
    to minimize such false negatives.'
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*召回率*——这个模型的目的在于识别那些会订阅定期存款的高价值客户。从这个角度来看，我们希望最小化错误否定，即我们的模型认为不会订阅但实际上会订阅的客户！这种类型的错误预测会损失我们客户，而召回率是一个很好的指标来最小化这种错误否定。'
- en: This procedure randomly shuffles feature values. As with any randomized approach,
    it’s a good idea to repeat the process several times and average the result (analogous
    to how we use k folds in cross validation).
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个过程随机打乱特征值。与任何随机方法一样，重复这个过程几次并平均结果是一个好主意（类似于我们在交叉验证中使用k折的方法）。
- en: Permutation feature importance in practice
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 实践中的排列特征重要性
- en: The following listing computes permutation feature importances for the XGBClassifier
    trained in the previous section using balanced_accuracy.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表计算了在上一节中训练的XGBClassifier的排列特征重要性，使用的是平衡准确率。
- en: Listing 9.6 Computing permutation feature importance
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.6 计算排列特征重要性
- en: '[PRE9]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Uses a hold-out test set to compute feature importances
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用保留的测试集来计算特征重要性
- en: ❷ Different metrics can be used to evaluate model performance and feature importance.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 可以使用不同的指标来评估模型性能和特征重要性。
- en: ❸ Repeats randomized shuffling of features
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 重复随机打乱特征
- en: Figure 9.8 compares feature importance of the XGBoost model with the permutation
    importance computed using balanced accuracy and recall, and then visualizes the
    top-10 features identified by each approach.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8比较了XGBoost模型的特征重要性以及使用平衡准确率和召回率计算出的排列重要性，并可视化每种方法识别的前10个特征。
- en: '![CH09_F08_Kunapuli](../Images/CH09_F08_Kunapuli.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F08_Kunapuli](../Images/CH09_F08_Kunapuli.png)'
- en: 'Figure 9.8 Feature importances computed by XGBoost versus black-box permutation
    feature importances computed for the XGBoost model using two different metrics:
    balanced accuracy and recall'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 XGBoost计算的特征重要性与使用两个不同指标（平衡准确率和召回率）计算的XGBoost模型的黑盒排列特征重要性进行了比较。
- en: Interestingly, while all three approaches identify the importance of nr.employed
    (the number of employees), euribor3m (the interbank borrowing rate) emerges as
    a key indicator when scoring features using balanced accuracy or recall. A little
    deeper reflection might shine a light as to why. In a healthier economy, better
    interbank borrowing rates allow for better interest rates, which, in turn, favorably
    influence customers to subscribe to a fixed deposit account.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，尽管三种方法都认同nr.employed（雇员数量）的重要性，但在使用平衡准确率或召回率来评分特征时，euribor3m（银行间借贷利率）成为了一个关键指标。稍微深入思考一下，可能会揭示其中的原因。在一个更健康的经济中，更好的银行间借贷利率允许更好的利率，这反过来又有利于吸引客户订阅定期存款账户。
- en: Aside from the socioeconomic indicators, other features such as contact (cell
    phone vs. telephone contact) and campaign (total number of contacts during this
    campaign) also emerge as important indicators of whether a customer will subscribe
    to a fixed-term deposit.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 除了社会经济指标外，其他特征，如联系（手机与电话联系）和活动（在此活动中联系的总数），也成为了客户是否会订阅定期存款的重要指标。
- en: Some demographic features such as marital, age, and education also begin to
    emerge as important when scored using recall, where we aim to decrease the false
    negatives and identify as many high-value customers as possible. Again, it’s not
    hard to see that effectively identifying high-value customers is reliant on their
    personal demographic indicators.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 一些人口统计特征，如婚姻状况、年龄和教育程度，在召回率评分中也开始变得重要，我们的目标是减少错误否定，并尽可能多地识别高价值客户。再次强调，有效地识别高价值客户依赖于他们的个人人口统计指标。
- en: NOTE Care must be taken with correlated features because they contain similar
    information. When two features, for example, are correlated, and one of them is
    permuted, the model can still use the other unpermuted feature without any decrease
    in performance (because they both contain similar, information). Since the scores
    before and after permutation are similar the permutation feature importance scores
    for both the correlated features will be small. From this, we may incorrectly
    conclude that both features are unimportant, when, in fact, they may both be important.
    This situation is even worse when we have three, four, or a cluster of correlated
    features. One way to handle this situation is to preprocess the data by clustering
    features into groups and using a representative feature from each feature group.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：必须小心处理相关特征，因为它们包含相似的信息。例如，当两个特征相关时，其中一个被置换，模型仍然可以使用未置换的其他特征而不会降低性能（因为它们都包含相似的信息）。由于置换前后的分数相似，相关特征的置换特征重要性分数将很小。从这个结果中，我们可能会错误地得出结论，认为两个特征都不重要，而实际上它们可能都很重要。当我们有三个、四个或是一组相关特征时，这种情况会更糟。处理这种情况的一种方法是通过聚类将特征分组，并使用每个特征组的一个代表性特征。
- en: 9.3.2 Partial dependence plots
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 部分依赖图
- en: '*Partial dependence plots* (PDPs) are another useful black-box approach that
    helps us identify the nature of the relationship between a feature and the target.
    Unlike permutation feature importance, which uses randomization to elicit the
    importance of a feature, the partial dependence relationship is identified using
    *marginalization*, or summing out.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '*部分依赖图*（PDPs）是另一种有用的黑盒方法，它帮助我们识别特征与目标之间的关系的本质。与使用随机化来激发特征重要性的置换特征重要性不同，部分依赖关系是通过边缘化或求和来识别的。'
- en: Let’s say that we’re interested in computing the partial dependence between
    the target *y* and the *k*th feature, *X*[k]. Let the data set with the remaining
    features be *X*[rest]. We have a black-box model *y* = *f*([*X*[k],*X*[rest]]).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们感兴趣的是计算目标 *y* 与第 *k* 个特征 *X*[k] 之间的部分依赖。设剩余特征的数据集为 *X*[rest]。我们有一个黑盒模型 *y*
    = *f*([*X*[k],*X*[rest]])。
- en: 'To obtain the partial dependence function ![CH09_F08_Kunapuli-eqs-13x](../Images/CH09_F08_Kunapuli-eqs-13x.png) from
    this black box, we simply have to sum over all possible values of all the other
    features *X*[rest]; that is, we marginalize the other features. Mathematically,
    summing over all possible values of the other features is equivalent to integrating
    over them:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 要从这个黑盒中获得部分依赖函数 ![CH09_F08_Kunapuli-eqs-13x](../Images/CH09_F08_Kunapuli-eqs-13x.png)，我们只需对所有其他特征
    *X*[rest] 的所有可能值进行求和；也就是说，我们边缘化其他特征。从数学上讲，对所有其他特征的可能的值进行求和等同于对它们进行积分：
- en: '![CH09_F08_Kunapuli-eqs-14x](../Images/CH09_F08_Kunapuli-eqs-14x.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F08_Kunapuli-eqs-14x](../Images/CH09_F08_Kunapuli-eqs-14x.png)'
- en: 'However, since computing this integral isn’t really feasible, we’ll need to
    approximate it. We can do so very easily using a set of *n* examples:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于计算这个积分实际上并不可行，我们需要对其进行近似。我们可以使用一组 *n* 个例子非常容易地做到这一点：
- en: '![CH09_F08_Kunapuli-eqs-15x](../Images/CH09_F08_Kunapuli-eqs-15x.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F08_Kunapuli-eqs-15x](../Images/CH09_F08_Kunapuli-eqs-15x.png)'
- en: This equation gives us a straightforward way of computing the partial dependence
    function for a feature *X*[k].
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式为我们计算特征 *X*[k] 的部分依赖函数提供了一种直接的方法。
- en: 'For different values of *a*, we simply replace the entire column with a. Thus,
    for each *a*, we create a new data set *X*^([*a*]), where the *k*th feature takes
    the value *a* for every example. The predictions of this modified data set using
    our black-box model will be *y*^([*a*]) = *f*(*X*^([*a*])). The prediction vector
    *y*^([*a*]) is a length-*n* vector, containing the predictions of each test example
    in the modified data set. We can now average over these predictions to give us
    one pair of points:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不同的 *a* 值，我们只需将整个列替换为 a。因此，对于每个 *a*，我们创建一个新的数据集 *X*^([*a*])，其中第 *k* 个特征在每一个例子中都取值为
    *a*。使用我们的黑盒模型对这个修改后的数据集进行预测的结果将是 *y*^([*a*]) = *f*(*X*^([*a*]))。预测向量 *y*^([*a*])
    是一个长度为 *n* 的向量，包含修改后的数据集中每个测试例子的预测。我们现在可以对这些预测进行平均，以得到一对点：
- en: '![CH09_F08_Kunapuli-eqs-16x](../Images/CH09_F08_Kunapuli-eqs-16x.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F08_Kunapuli-eqs-16x](../Images/CH09_F08_Kunapuli-eqs-16x.png)'
- en: We repeat this procedure for different values of α to generate the full PDP.
    This is illustrated in figure 9.9 for two values, *a* = 0,1 and *a* = 0.4.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复这个步骤，对不同的 α 值进行操作，以生成完整的 PDP。这在图 9.9 中用两个值 *a* = 0,1 和 *a* = 0.4 进行了说明。
- en: 'PDPs are intuitive to create and use, though they can be somewhat time consuming
    as new modified versions of the data set have to be created and evaluated for
    each point in the dependence plot. Here are some important technical details to
    keep in mind:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: PDPs易于创建和使用，尽管它们可能有些耗时，因为必须为依赖图中的每个点创建和评估数据集的新修改版本。以下是一些需要记住的重要技术细节：
- en: Partial dependence tries to relate a model’s output to input features, that
    is, model behavior in terms of what it has learned. For this reason, it’s best
    to create and visualize a PDP with the training set.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部分依赖试图将模型的输出与输入特征联系起来，即从模型学习到的行为。因此，最好使用训练集创建和可视化PDP。
- en: Remember that the overall partial dependence function is created by averaging
    across *n* examples; that is, each training example can be used to create an example-specific
    partial dependence function. This partial dependence between a specific example
    and its output is called the *individual conditional expectation* (ICE).
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记住，整体部分依赖函数是通过平均 *n* 个示例创建的；也就是说，每个训练示例都可以用来创建一个特定于示例的部分依赖函数。这种特定示例与其输出之间的部分依赖称为*个体条件期望*（ICE）。
- en: '![CH09_F09_Kunapuli](../Images/CH09_F09_Kunapuli.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F09_Kunapuli](../Images/CH09_F09_Kunapuli.png)'
- en: Figure 9.9 Two points in the PDP for the third feature computed at *X*[3] =
    0.1 and *X*[3] = 0.4\. Observe that we set the third column (feature) to 0.1 and
    0.3, respectively, to get two data sets. Each of these data sets produce two sets
    of predictions, which are averaged to produce two points on the PDP.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9展示了在 *X*[3] = 0.1 和 *X*[3] = 0.4 处计算的第三个特征的部分依赖图中的两个点。观察我们发现，我们将第三列（特征）分别设置为0.1和0.3，以获得两个数据集。这些数据集各自产生两组预测，这些预测被平均以在PDP上产生两个点。
- en: Partial dependence plots in practice
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 实践中的部分依赖图
- en: The next listing illustrates how to construct PDPs for the XGBoostClassifier
    trained earlier in section 9.2 on the Bank Marketing data set.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表说明了如何为在9.2节中较早训练的Bank Marketing数据集上的XGBoostClassifier构建PDPs。
- en: Listing 9.7 Creating PDPs
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.7 创建PDPs
- en: '[PRE10]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Features we want to compute PDPs for
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们想要计算PDPs的特征
- en: ❷ List of all the features in the data set
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 数据集中所有特征的列表
- en: ❸ Plots individual conditional expectations for each example or the average
    PDP
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为每个示例或平均部分依赖图绘制单个条件期望
- en: ❹ Sets whether we want partial dependence with predictions or prediction probabilities
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 设置我们是否想要带有预测或预测概率的部分依赖
- en: 'Figure 9.10 shows the partial dependence function of four high-scoring variables:
    euribor3m, nr.employed, contact, and emp.var.rate from the Bank Marketing data
    set.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10显示了银行营销数据集中四个高得分变量的部分依赖函数：euribor3m、nr.employed、contact和emp.var.rate。
- en: '![CH09_F10_Kunapuli](../Images/CH09_F10_Kunapuli.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F10_Kunapuli](../Images/CH09_F10_Kunapuli.png)'
- en: Figure 9.10 PDPs of four variables in the Bank Marketing data set
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10展示了银行营销数据集中四个变量的PDPs
- en: The PDPs give us further insight into how different variables behave and how
    they influence predictions. Note that, in listing 9.7, we set response_method
    to 'predict_proba'. Thus, the plots in figure 9.10 show how each variable (partially)
    influences the prediction probability of a customer subscribing to a fixed-deposit
    account. Higher prediction probabilities indicate that those attributes are more
    helpful in identifying high-value customers.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: PDPs让我们进一步了解不同变量如何表现以及它们如何影响预测。注意，在列表9.7中，我们将response_method设置为'predict_proba'。因此，图9.10中的图表显示了每个变量（部分）如何影响客户订阅定期存款账户的预测概率。更高的预测概率表明这些属性在识别高价值客户方面更有帮助。
- en: For example, low values of euribor3m (e.g., in the range 0-0.5) generally correspond
    to higher subscription likelihoods. As discussed previously, this makes sense
    as lower bank borrowing rates typically mean lower customer interest rates, which
    would be attractive to a potential customer.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，euribor3m的值较低（例如，在0-0.5的范围内）通常对应着更高的订阅可能性。如前所述，这是有道理的，因为较低的银行借款利率通常意味着较低的客户利率，这对潜在客户来说是有吸引力的。
- en: A similar conclusion—that lower unemployment rates are also likely to influence
    potential customers into opening fixed-deposit accounts—can also be drawn from
    the variables emp.var.rate and nr.employed.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的结论——较低的失业率也可能影响潜在客户开设定期存款账户——也可以从变量emp.var.rate和nr.employed中得出。
- en: NOTE As with permutation feature importance, a key assumption in the procedure
    for PDPs is that the feature we’re interested in, *X*[k], isn’t correlated with
    the remaining features, *X*[rest]. This independence assumption is what allows
    us to marginalize the remaining features by summing over them. If the features
    *X*[rest] are correlated, then marginalizing over them destroys some component
    of *X*[k] as well, and we no longer have an accurate view of how much *X*[k] contributes
    to the predictions.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：与排列特征重要性类似，PDPs（部分依赖图）过程的一个关键假设是我们感兴趣的特性，*X*[k]，与剩余特性，*X*[rest]，不相关。这个独立性假设使我们能够通过求和来边缘化剩余特性。如果特性
    *X*[rest] 是相关的，那么对它们的边缘化将破坏 *X*[k] 的某些组成部分，我们就无法准确了解 *X*[k] 对预测的贡献程度。
- en: One important limitation of PDPs is that it’s only possible to create plots
    of partial dependence functions of one variable (curves), two variables (contours),
    or three variables (surface plots). Beyond three variables, it becomes impossible
    to visualize multivariable partial dependence without breaking features down into
    smaller groups of two or three.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: PDPs（部分依赖图）的一个重要限制是，只能创建一个变量的部分依赖函数（曲线）、两个变量的（等高线）或三个变量的（表面图）的图表。超过三个变量，在不将特性分解成两个或三个更小组的情况下，就无法可视化多变量的部分依赖。
- en: 9.3.3 Global surrogate models
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3 全局代理模型
- en: Black-box explanations such as feature importance and partial dependence attempt
    to identify the effect of an individual feature or group of features on predictions.
    In this section, we explore a more holistic approach that aims to approximate
    the behavior of the black-box model in an interpretable way.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 黑盒解释，如特征重要性和部分依赖，试图识别单个特性或特性组对预测的影响。在本节中，我们探讨一种更全面的方法，旨在以可解释的方式近似黑盒模型的行为。
- en: 'The idea of a surrogate model is extremely simple: we train a second model
    that mimics the behavior of the black-box model. However, the surrogate model
    itself is a glass box and inherently explainable.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 代理模型的想法极其简单：我们训练第二个模型来模仿黑盒模型的行为。然而，代理模型本身是一个玻璃盒，本质上是可以解释的。
- en: 'Once trained, we can use the surrogate glass-box model to explain the predictions
    of the black-box model, as illustrated in figure 9.11:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，我们可以使用代理玻璃盒模型来解释黑盒模型的预测，如图9.11所示：
- en: A surrogate data set (*X*^s[trn],*y*^s[trn])is used to train the surrogate model.
    The original data that was used to train the black-box model can also be used
    to train the surrogate model, if it’s available. If not, an alternate data sample
    from the original problem space is used. The key is to ensure that the surrogate
    data set has the same distribution as the original data set that was used to train
    the black-box model.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个代理数据集 (*X*^s[trn],*y*^s[trn])用于训练代理模型。用于训练黑盒模型的原始数据也可以用来训练代理模型，如果它可用的话。如果不可用，则使用来自原始问题空间的替代数据样本。关键是确保代理数据集与用于训练黑盒模型的原始数据集具有相同的分布。
- en: The surrogate model is trained on the predictions of the original black-box
    model. This is because the idea is to fit a surrogate model to *mimic* the behavior
    of the black-box model so that we can explain the black box using the surrogate.
    Once trained, if the surrogate predictions (*y*^s[pred])match the black-box predictions
    (*y*^b[pred]),then the surrogate model can be used to explain the predictions.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理模型是在原始黑盒模型的预测上训练的。这是因为我们的想法是拟合一个代理模型来*模仿*黑盒模型的行为，这样我们就可以使用代理来解释黑盒。一旦训练完成，如果代理预测
    (*y*^s[pred]) 与黑盒预测 (*y*^b[pred]) 匹配，那么代理模型就可以用来解释预测。
- en: Any glass-box model can be used as a surrogate model. This includes decision
    trees and GLMs, which can then be interpreted as shown earlier in section 9.11.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何玻璃盒模型都可以用作代理模型。这包括决策树和GLMs（广义线性模型），然后可以像在9.11节中之前展示的那样进行解释。
- en: '![CH09_F11_Kunapuli](../Images/CH09_F11_Kunapuli.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F11_Kunapuli](../Images/CH09_F11_Kunapuli.png)'
- en: Figure 9.11 The procedure to train a global surrogate model from the predictions
    of a black-box model. Both models are trained on the same surrogate training examples.
    However, the surrogate model is trained on the predictions of the black-box model,
    so that it can learn to mimic its predictions. If the black box and surrogate
    make the same prediction, then the surrogate can be used to explain the black-box
    model’s prediction.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11 从黑盒模型的预测中训练全局代理模型的流程。两个模型都在相同的代理训练示例上进行了训练。然而，代理模型是在黑盒模型的预测上进行训练的，以便它可以学习模仿其预测。如果黑盒和代理做出相同的预测，那么代理就可以用来解释黑盒模型的预测。
- en: The fidelity-interpretability tradeoff
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 保真度-可解释性权衡
- en: Let’s train a surrogate decision tree to explain the behavior of the XGBoost
    model that was originally trained on the Bank Marketing data set. The original
    training set is also used as the surrogate training set.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练一个代理决策树来解释在Bank Marketing数据集上最初训练的XGBoost模型的行为。原始训练集也被用作代理训练集。
- en: 'Keep in mind that we want to trade off between two criteria while training
    the model: the surrogate’s fidelity to the black-box model and the surrogate’s
    explainability. The surrogate’s fidelity measures how well it can mimic the black-box
    model’s predictive behavior. More precisely, we measure how similar the surrogate
    model’s predictions (*y*^s[pred]) are to the black-box model’s predictions (*y*^b[pred]).'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在训练模型时，我们希望在两个标准之间进行权衡：代理模型对黑盒模型的保真度和代理模型的可解释性。代理模型的保真度衡量它模仿黑盒模型预测行为的能力有多好。更精确地说，我们衡量代理模型预测（*y*^s[pred]）与黑盒模型预测（*y*^b[pred]）的相似程度。
- en: For binary classification problems, we can do this using metrics such as accuracy
    or *R*² score (see chapter 1). For regression problems, we can do this with metrics
    such as mean squared error (MSE), or *R*² again. Higher *R*² scores indicate better
    fidelity between the black-box model and its surrogate.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二元分类问题，我们可以使用准确度或*R*²分数（见第1章）等指标来完成这项工作。对于回归问题，我们可以使用均方误差（MSE）或*R*²等指标。更高的*R*²分数表明黑盒模型与其代理之间的保真度更好。
- en: The surrogate’s explainability depends on its complexity. Let’s say that we
    want to train a decision-tree surrogate. Recall from our discussion in section
    9.1 that we need to limit the number of leaf nodes in the surrogate model for
    it to be human-interpretable as too many leaf nodes might lead to model complexity
    and overwhelm the interpreter.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 代理模型的可解释性取决于其复杂性。假设我们想要训练一个决策树代理模型。回想一下我们在第9.1节中的讨论，我们需要限制代理模型中叶节点的数量，以便它能够被人类解释，因为过多的叶节点可能会导致模型复杂化并使解释者感到不知所措。
- en: Training global surrogate models in practice
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上训练全局代理模型
- en: To train a useful surrogate model, we’ll need to find the sweet spot in the
    fidelity-interpretability tradeoff. This sweet spot will be a surrogate model
    that approximates the black-box’s predictions pretty well but is also not so complex
    that it defies any interpretation (possibly by inspection).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练一个有用的代理模型，我们需要在保真度-可解释性权衡中找到最佳平衡点。这个最佳平衡点将是一个能够很好地近似黑盒预测但又不至于过于复杂以至于无法进行解释（可能通过检查）的代理模型。
- en: Figure 9.12 shows the fidelity-explainability tradeoff for a decision-tree surrogate
    trained for the XGBoost model. The surrogate is trained on the same Bank Marketing
    training set that was used to train the XGBoost model in section 9.1.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12显示了为XGBoost模型训练的决策树代理模型的保真度-可解释性权衡。代理模型是在第9.1节中用于训练XGBoost模型的相同Bank Marketing训练集上训练的。
- en: '![CH09_F12_Kunapuli](../Images/CH09_F12_Kunapuli.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F12_Kunapuli](../Images/CH09_F12_Kunapuli.png)'
- en: Figure 9.12 The fidelity-explainability tradeoff for the Bank Marketing data
    set. The black-box model is an XGBoost ensemble, while the surrogate is a decision
    tree trained on the black-box predictions.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12 Bank Marketing数据集的保真度-可解释性权衡。黑盒模型是一个XGBoost集成，而代理模型是在黑盒预测上训练的决策树。
- en: We increase the surrogate’s complexity (characterized by the number of leaf
    nodes), while keeping an eye on the fidelity (*R*² score) between the black-box
    and surrogate predictions. A decision-tree surrogate with 14 leaf nodes seems
    to achieve the ideal tradeoff between fidelity and complexity for explainability.
    Listing 9.8 trains a surrogate decision-tree model with these specifications.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在保持对黑盒预测和代理预测之间保真度（*R*²分数）的关注的同时，增加代理模型的复杂性（以叶节点数量表征）。一个具有14个叶节点的决策树代理模型似乎在保真度和复杂性之间实现了可解释性的理想权衡。列表9.8展示了具有这些规格的代理决策树模型的训练过程。
- en: Listing 9.8 Training a surrogate model
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.8 训练代理模型
- en: '[PRE11]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Sets the maximum possible leaf nodes to 14
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将最大可能的叶子节点数设置为14
- en: ❷ Sets the minimum samples in a leaf node to 20 to avoid overfitting
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将叶子节点中的最小样本数设置为20，以避免过拟合
- en: ❸ Sets the class weights to 1 for negative examples and 10 for positive examples
    to account for the class imbalance
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将负例的类别权重设置为1，将正例的类别权重设置为10，以解决类别不平衡问题
- en: Figure 9.13 shows the decision-tree surrogate for the XGBoost model.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13显示了XGBoost模型的决策树代理
- en: '![CH09_F13_Kunapuli](../Images/CH09_F13_Kunapuli.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F13_Kunapuli](../Images/CH09_F13_Kunapuli.png)'
- en: Figure 9.13 Surrogate model trained from the predictions of the XGBoost model,
    which was originally trained on the Bank Marketing data set. This tree has 14
    leaf nodes. Inspecting and analyzing this tree can yield many insights, such as
    the highlighted path from root to leaf (nodes with dashed borders).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13 从原始在Bank Marketing数据集上训练的XGBoost模型的预测中训练得到的代理模型。此树有14个叶子节点。检查和分析此树可以得出许多见解，例如从根节点到叶子节点的突出路径（带有虚线边框的节点）。
- en: Several variables appear in the highlighted path from root node to leaf node.
    These variables describe a high-value subpopulation and provide insights into
    potentially successful strategies.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 几个变量出现在从根节点到叶子节点的突出路径中。这些变量描述了一个高价值子群体，并提供了关于可能成功的策略的见解。
- en: For example, the socioeconomic variables, such as nr.employed and euribor3m,
    identify favorable societal circumstances during which to launch a successful
    campaign. In addition, [day_of_week <= 1.5] suggests that calling these high-value
    customers on Monday (day_of_week = 0) or Tuesday (day_of_week = 1) is a good strategy.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，社会经济变量，如nr.employed和euribor3m，识别出有利于发起成功活动的有利社会环境。此外，[day_of_week <= 1.5]表明，在周一（day_of_week
    = 0）或周二（day_of_week = 1）联系这些高价值客户是一个好策略。
- en: We can also look at other paths and nodes to get further insights. The node
    age <= 0.147 is obtained on the preprocessed data, where 0.147 corresponds to
    40 before rescaling. This suggests that customers who are under 40 years of age
    are high value.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看其他路径和节点以获得更深入的见解。节点age <= 0.147是在预处理数据上获得的，其中0.147对应于缩放前的40。这表明年龄在40岁以下的客户是高价值的。
- en: Yet another useful node is [default <= 0.5], which suggests that customers who
    have no previous defaults are high value. You may be able to identify other viable
    strategies for identifying high-value customers and strategies as well.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的节点是[default <= 0.5]，这表明没有先前违约记录的客户是高价值的。您可能能够识别出识别高价值客户和策略的其他可行策略。
- en: 9.4 Black-box methods for local explainability
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 用于局部可解释性的黑盒方法
- en: The previous section introduced methods for global explainability, which aim
    to explain a model’s global behavioral trends across different types of input
    examples and subpopulations. In this section, we’ll explore methods for *local
    explainability*, which aim to explain a model’s individual predictions. The explanations
    allow users (e.g., doctors using a diagnostic system) to trust the predictions
    and take actions based on them. This is tied to the user’s ability to understand
    why a model made a particular decision.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节介绍了全局可解释性的方法，这些方法的目的是解释模型在不同类型的输入示例和子群体中的全局行为趋势。在本节中，我们将探讨局部可解释性的方法，这些方法的目的是解释模型的个别预测。这些解释允许用户（例如，使用诊断系统的医生）信任预测并据此采取行动。这与用户理解模型为何做出特定决定的能力相关。
- en: 9.4.1 Local surrogate models with LIME
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.1 使用LIME的局部代理模型
- en: The first method we’ll look at is called *Locally Interpretable Model-Agnostic
    Explanations* (LIME). As the name rather transparently suggests, LIME is (1) a
    model-agnostic method, which means it can be used with any machine-learning model
    black box; and (2) a local interpretability method that is used to explain a model’s
    individual predictions.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要探讨的第一个方法是称为*局部可解释的模型无关解释*（LIME）。正如其名相当明显地暗示的那样，LIME是（1）一个模型无关的方法，这意味着它可以与任何机器学习模型黑盒一起使用；并且（2）一个用于解释模型个别预测的局部可解释性方法。
- en: LIME is, in fact, a *local surrogate method*. It uses a linear model to approximate
    a black-box model in the locality of the example whose predictions we’re interested
    in explaining. This intuition is shown in figure 9.14, in the complex surface
    of a black-box model and an interpretable linear surrogate model that approximates
    black-box behavior around a single example of interest.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: LIME 实际上是一种 *局部代理方法*。它使用线性模型来近似我们感兴趣解释的示例的局部黑盒模型。这种直觉在图 9.14 中显示，其中有一个黑盒模型的复杂表面和一个可解释的线性代理模型，该模型近似单个感兴趣示例周围的黑盒行为。
- en: '![CH09_F14_Kunapuli](../Images/CH09_F14_Kunapuli.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F14_Kunapuli](../Images/CH09_F14_Kunapuli.png)'
- en: Figure 9.14 LIME creates a surrogate training set of examples in the locality
    of the example whose prediction needs to be explained. These examples are further
    weighted by their distance. This is indicated by the sizes of the surrogate examples,
    with closer examples getting higher weights (and shown larger). A weighted loss
    function is used to fit a linear surrogate model, which provides local explanations.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14 LIME 在需要解释预测的示例的局部创建了一个代理训练示例集。这些示例根据它们的距离进一步加权。这通过代理示例的大小表示，较近的示例获得更高的权重（并显示更大）。使用加权损失函数来拟合线性代理模型，这提供了局部解释。
- en: The fidelity-interpretability tradeoff again
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 保真度-可解释性权衡再次出现
- en: Given a training example whose predictions we want to explain, LIME trains a
    local surrogate to be the model with the best tradeoff between fidelity and interpretability.
    In the previous section, we trained a decision-tree surrogate to optimize the
    fidelity-interpretability tradeoff.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们想要解释预测的训练示例，LIME 训练一个局部代理，使其成为在保真度和可解释性之间具有最佳权衡的模型。在上一节中，我们训练了一个决策树代理来优化保真度-可解释性权衡。
- en: 'Let’s write this down more formally. First, we denote the black-box model by
    *ƒ*[b](*x*) and the surrogate model by *ƒ*[s](*x*''). We measure fidelity between
    the predictions of the black box (*ƒ*[b]) and the surrogate (*ƒ*[s]) using the
    *R*² score. We measure interpretability of the surrogate model by using the number
    of leaf nodes in the tree: fewer leaf nodes generally lead to better interpretability.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更正式地写下这一点。首先，我们用 *ƒ*[b](*x*) 表示黑盒模型，用 *ƒ*[s](*x*') 表示代理模型。我们使用 *R*² 分数来衡量黑盒
    (*ƒ*[b]) 和代理 (*ƒ*[s]) 预测之间的保真度。我们通过树中的叶子节点数量来衡量代理模型的可解释性：叶子节点越少，通常可解释性越好。
- en: 'Let’s say that we want to explain the predictions of the black box on example
    *x*. For decision-tree surrogate training, we try to find a decision tree that
    optimizes the following:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要解释黑盒在示例 *x* 上的预测。对于决策树代理训练，我们试图找到一个决策树，以优化以下内容：
- en: '![CH09_F14_Kunapuli-eqs-20x](../Images/CH09_F14_Kunapuli-eqs-20x.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F14_Kunapuli-eqs-20x](../Images/CH09_F14_Kunapuli-eqs-20x.png)'
- en: 'In a similar vein, LIME trains a linear surrogate by optimizing the following:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，LIME 通过优化以下内容来训练线性代理：
- en: '![CH09_F14_Kunapuli-eqs-21x](../Images/CH09_F14_Kunapuli-eqs-21x.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F14_Kunapuli-eqs-21x](../Images/CH09_F14_Kunapuli-eqs-21x.png)'
- en: 'Here, the examples *x*'', called surrogate training examples, will be used
    to train the surrogate model. The loss function that is used to measure fidelity
    is a simple weighted MSE that measures the disparity in the predictions of the
    black box and the surrogate:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用称为代理训练示例的 *x*' 来训练代理模型。用于衡量保真度的损失函数是一个简单的加权均方误差（MSE），它衡量黑盒和代理预测之间的差异：
- en: '![CH09_F14_Kunapuli-eqs-22x](../Images/CH09_F14_Kunapuli-eqs-22x.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F14_Kunapuli-eqs-22x](../Images/CH09_F14_Kunapuli-eqs-22x.png)'
- en: The surrogate is a linear model of the form *ƒ*[s](*x*') = *β*[0] + *β*[1]*x*[1]'+
    ⋅⋅⋅ + *β*[d]*x*[d]',and x' is a surrogate example. As we’ve seen in section 9.1,
    the interpretability of linear models depends on the number of features. Fewer
    features make analyzing their corresponding parameters *β*[k] easier. Thus, LIME
    seeks to train sparser linear models with more zero parameters to promote interpretability
    (remember from chapter 7 that L1 regularization can help with this).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 代理是一个线性模型，形式为 *ƒ*[s](*x*') = *β*[0] + *β*[1]*x*[1]'+ ⋅⋅⋅ + *β*[d]*x*[d]', 其中
    x' 是代理示例。正如我们在第 9.1 节中看到的，线性模型的可解释性取决于特征的数量。特征越少，分析相应的参数 *β*[k] 就越容易。因此，LIME 试图训练具有更多零参数的稀疏线性模型，以促进可解释性（记住在第
    7 章中提到的 L1 正则化可以帮助做到这一点）。
- en: But what makes LIME local? How can we train a local surrogate model? How do
    we obtain surrogate examples *x*'? And what are these local weights (*π*[x]) in
    the preceding equation? The answers are in how LIME creates and uses surrogate
    examples.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 但是什么让 LIME 变得局部？我们如何训练一个局部的代理模型？我们如何获得代理示例 *x*'? 以及前一个方程中的这些局部权重 (*π*[x]) 是什么？答案在于
    LIME 如何创建和使用代理示例。
- en: Sampling surrogate examples for local explainability
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 为局部可解释性采样代理示例
- en: We now have a well-defined fidelity-interpretability criterion to train our
    surrogate model. If we used the entire training set, we would obtain a global
    surrogate model.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个明确的保真度-可解释性标准来训练我们的代理模型。如果我们使用整个训练集，我们将获得一个全局代理模型。
- en: To train a local surrogate model, we need data points that are close to or similar
    to our example of interest. LIME creates a local surrogate training set by sampling
    and smoothing.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练一个局部代理模型，我们需要接近或相似于我们感兴趣示例的数据点。LIME 通过采样和平滑创建一个局部的代理训练集。
- en: 'Let’s say that we’re interested in explaining the prediction of the black box
    on an example with five features: *x* = [*x*[1],*x*[2],*x*[3],*x*[4],*x*[5]].
    LIME samples data in a neighborhood of *x* as follows:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们感兴趣的是解释具有五个特征的示例上黑盒的预测：*x* = [*x*[1],*x*[2],*x*[3],*x*[4],*x*[5]]。LIME 如下在
    *x* 的邻域中采样数据：
- en: '*Perturb*—Randomly generate perturbations for each feature. For continuous
    features, perturbations are randomly sampled from the normal distribution, *ϵ*~*N*(0,1).
    For categorical features, these are randomly sampled from the multivariate distribution
    over *K* category values, *ϵ*~Cat(*K*). This generates one surrogate example *x*''
    = [*x*[1] + *ϵ*[1], *x*[2] + *ϵ*[2], *x*[3] + *ϵ*[3], *x*[4] + *ϵ*[4], *x*[5]
    + *ϵ*[5]]. This example can now also be labeled using the black box, *y* = *f*[b]*(**x*'').
    This continues until we obtain a surrogate set *Z* in the locality of *x*.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*扰动*——为每个特征随机生成扰动。对于连续特征，扰动从正态分布中随机采样，*ϵ*~*N*(0,1)。对于分类特征，这些是从 *K* 个类别值的多变量分布中随机采样，*ϵ*~Cat(*K*)。这生成一个代理示例
    *x*'' = [*x*[1] + *ϵ*[1], *x*[2] + *ϵ*[2], *x*[3] + *ϵ*[3], *x*[4] + *ϵ*[4], *x*[5]
    + *ϵ*[5]]。现在这个示例也可以使用黑盒进行标记，*y* = *f*[b]*(**x*'')。这个过程会继续，直到我们在 *x* 的局部区域内获得一个代理集
    *Z*。'
- en: '*Smooth*—Each surrogate training example is also assigned a weight using the
    exponential smoothing kernel: *π*[x](*x*'') = exp(-*γ* ⋅ *D*(*x*,*x*'')²). Here,
    *D*(*x*,*x*'') is the distance between our example that needs to be explained
    *x* and a perturbed sample *x*''. Surrogate training examples that are further
    from *x* get smaller weights, and those that are closer to *x* get higher weights.
    Thus, this function encourages the surrogate model to prioritize surrogate examples
    that are more local when training a linear approximation. The smoothing parameter
    *γ* > 0 controls the width of the kernel. Increasing γ allows LIME to consider
    larger neighborhoods, making the model less local.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*平滑*——每个代理训练示例也使用指数平滑核分配一个权重：*π*[x](*x*'') = exp(-*γ* ⋅ *D*(*x*,*x*'')²)。在这里，*D*(*x*,*x*'')
    是需要解释的示例 *x* 与扰动样本 *x*'' 之间的距离。距离 *x* 更远的代理训练示例将获得更小的权重，而距离 *x* 更近的示例将获得更高的权重。因此，这个函数鼓励代理模型在训练线性近似时优先考虑更局部的代理示例。平滑参数
    *γ* > 0 控制核的宽度。增加 γ 允许 LIME 考虑更大的邻域，使模型更不局部。'
- en: Now that we have a surrogate training set in the locality of the example *x*,
    we can train a linear model. The goal is to train it to induce sparsity (as many
    zero parameters as possible). LIME supports training of sparse linear models with
    L1 regularization, such as Least Absolute Shrinkage and Selection (LASSO) or elastic
    net. These models are covered in chapter 7 for linear regression and can be easily
    extended to logistic regression for classification as well.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在示例 *x* 的局部区域内拥有了一个代理训练集，我们可以训练一个线性模型。目标是训练它以产生稀疏性（尽可能多的零参数）。LIME 支持使用
    L1 正则化训练稀疏线性模型，例如最小绝对收缩和选择（LASSO）或弹性网络。这些模型在第 7 章中介绍，可以很容易地扩展到用于分类的逻辑回归。
- en: NOTE Keen observers may have noticed that the exponential kernel, with *D* as
    the Euclidean distance, is the same as a radial basis function (RBF) kernel that
    is used in support vector machines and other kernel methods. From that perspective,
    the exponential smoothing kernel is essentially a similarity function. Points
    that are closer are considered more similar and will have higher weights.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：敏锐的观察者可能会注意到，以欧几里得距离 *D* 为条件的指数核与支持向量机和其他核方法中使用的径向基函数（RBF）核是相同的。从这个角度来看，指数平滑核本质上是一个相似度函数。距离较近的点被认为是更相似的，并将具有更高的权重。
- en: LIME in practice
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 实践中的LIME
- en: 'LIME is available as a package through Python’s two most popular package managers:
    pip and conda. The package’s GitHub page ([https://github.com/marcotcr/lime](https://github.com/marcotcr/lime))
    also contains additional documentation and a number of examples illustrating how
    to use LIME for classification, regression, and applications in text and image
    analytics.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: LIME可以通过Python最流行的两个包管理器：pip和conda获得。该包的GitHub页面（[https://github.com/marcotcr/lime](https://github.com/marcotcr/lime)）还包含额外的文档和多个示例，说明如何使用LIME进行分类、回归以及在文本和图像分析中的应用。
- en: In listing 9.9, we use LIME to explain the predictions of a test set example
    from the Bank Marketing data set. Test example 3104 is a customer who did subscribe,
    which the XGBoost model identified with 64% confidence, a true positive example.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表9.9中，我们使用LIME来解释来自银行营销数据集的测试集示例的预测。测试示例3104是一位已经订阅的客户，XGBoost模型以64%的置信度将其识别为真阳性示例。
- en: Listing 9.9 Using LIME to explain XGBoost predictions
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.9使用LIME解释XGBoost预测
- en: '[PRE12]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Identifies the categorical features and their indices explicitly (for visualization)
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 明确识别分类特征及其索引（用于可视化）
- en: ❷ Passes the training set, which is sometimes used for sampling, especially
    continuous features
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过训练集，有时用于采样，特别是连续特征
- en: ❸ Identifies the feature names and class names explicitly (for visualization)
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 明确识别特征名称和类别名称（用于可视化）
- en: ❹ Sets the kernel width for this data set (identified here by trial and error)
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 为此数据集设置核宽度（通过试错法确定）
- en: ❺ Explains the predictions of test example 3104
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 解释测试示例3104的预测
- en: ❻ Visualizes the explanation as a bar chart
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将解释可视化为条形图
- en: Figure 9.15 visualizes the local weights identified by LIME to explain this
    example.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15可视化了LIME为解释此示例而识别的局部权重。
- en: '![CH09_F15_Kunapuli](../Images/CH09_F15_Kunapuli.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F15_Kunapuli](../Images/CH09_F15_Kunapuli.png)'
- en: Figure 9.15 Explanations generated by LIME for test example 3104 (a true positive
    prediction). Features that contributed to a negative prediction (won’t subscribe)
    will be negative and on the left of zero. Features that contributed to a positive
    prediction (will subscribe) will be positive and on the right of zero.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15展示了LIME为测试示例3104（一个真阳性预测）生成的解释。对负面预测（不会订阅）有贡献的特征将是负值，位于零的左侧。对正面预测（会订阅）有贡献的特征将是正值，位于零的右侧。
- en: The features and feature values (of the example being explained) are shown in
    the y-axis. The x-axis shows LIME feature importances.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 被解释示例的特征和特征值（在y轴上显示）。x轴显示LIME特征重要性。
- en: Aside from socioeconomic trends, let’s look at the personalized features of
    this customer. The variables with the biggest effect are contact (=0), whether
    they were contacted by cellular or landline (here, 0 = cellular); and default,
    whether they have prior banking defaults in their prior history (here, 0 = they
    don’t have prior defaults).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 除了社会经济趋势之外，让我们看看这位客户的个性化特征。影响最大的变量是联系（=0），他们是否被手机或固定电话联系（在这里，0 = 手机）；以及违约，他们是否有之前的银行违约历史（在这里，0
    = 他们没有之前的违约）。
- en: These interpretations will be intuitive to even non-AI users, such as in sales
    and marketing, who might further analyze them to fine-tune future marketing campaigns.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这些解释对非AI用户来说也将是直观的，例如在销售和营销领域，他们可能会进一步分析它们以微调未来的营销活动。
- en: 9.4.2 Local interpretability with SHAP
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.2 使用SHAP的局部可解释性
- en: 'In this section, we’ll cover another widely used local interpretability approach:
    *SHapley Additive exPlanations* (SHAP). SHAP is another model-agnostic black-box
    explainer similar to LIME that is used to explain individual predictions (hence,
    local interpretability) through feature importance.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍另一种广泛使用的局部可解释性方法：*Shapley加性解释*（SHAP）。SHAP是另一个类似于LIME的模型无关黑盒解释器，用于通过特征重要性解释单个预测（因此，局部可解释性）。
- en: SHAP is a feature-attribution technique that computes feature importance based
    on each feature’s contribution to the overall prediction. SHAP is built on the
    concept of Shapley values, which comes from the field of cooperative game theory.
    In this section, we’ll learn what Shapley values are, how they can be applied
    to computing feature importances, and how we can compute them efficiently in practice.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP是一种特征归因技术，它根据每个特征对整体预测的贡献来计算特征重要性。SHAP建立在Shapley值的概念之上，该概念来自合作博弈论领域。在本节中，我们将学习Shapley值是什么，它们如何应用于计算特征重要性，以及如何在实践中高效地计算它们。
- en: Understanding Shapley values
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 理解Shapley值
- en: Let’s say a group of four data scientists (Ava, Ben, Cam, and Dev) work collaboratively
    on a Kaggle Challenge and win first place with total prize money of $20,000\.
    Being a fair-minded group, they decide to split the prize money based on their
    contributions. They do this by trying to figure out how well they work in various
    combinations. Since they’ve worked together a lot in the past, they write down
    how well they work individually, and also in groups of two and in groups of three.
    These values representing each combination's effectiveness are shown in figure
    9.16.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一组四个数据科学家（Ava、Ben、Cam和Dev）在Kaggle挑战中合作并赢得了一等奖，总奖金为20,000美元。作为一个公平的团队，他们决定根据他们的贡献来分割奖金。他们通过尝试了解他们在各种组合中的工作效果来实现这一点。由于他们过去合作了很多，他们记录了他们单独工作以及以两人和三人小组工作时的效果。这些表示每种组合有效性的值在图9.16中显示。
- en: '![CH09_F16_Kunapuli](../Images/CH09_F16_Kunapuli.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F16_Kunapuli](../Images/CH09_F16_Kunapuli.png)'
- en: Figure 9.16 All possible coalitions of Ava, Ben, Cam, and Dev, and their corresponding
    values (in units of $1,000). The last coalition contains all four friends and
    has a value of $20,000, the total prize money. There is one coalition of size
    0, four coalitions of size 1, six coalitions of size 2, four coalitions of size
    3, and one coalition of size 4\. This table is called the characteristic function.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.16展示了Ava、Ben、Cam和Dev的所有可能的联盟及其对应的价值（以1,000美元为单位）。最后一个联盟包含所有四个朋友，价值为20,000美元，即总奖金。有一个大小为0的联盟，四个大小为1的联盟，六个大小为2的联盟，四个大小为3的联盟，以及一个大小为4的联盟。这个表称为特征函数。
- en: This table lists every possible combination of Ava, Ben, Cam, and Dev, also
    known as a *coalition*. Associated with each coalition is its value (prize money
    in $1,000 units), which indicates how much each coalition is worth had they been
    the only ones working on this project.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 此表列出了Ava、Ben、Cam和Dev的每一种可能的组合，也称为联盟。与每个联盟相关联的是其价值（以1,000美元为单位），这表明如果他们是这个项目唯一的工作者，每个联盟的价值是多少。
- en: For example, the coalition of Ava alone has a value of $7,000, while the coalition
    of Ava, Ben, and Dev has a value of $13,000\. The last coalition of all four of
    them, called the *grand coalition*, has a value of $20,000, the overall prize
    money.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，仅Ava的联盟价值为7,000美元，而Ava、Ben和Dev的联盟价值为13,000美元。所有四个人的最后一个联盟，称为*大联盟*，价值为20,000美元，即总奖金。
- en: The *Shapley value* allows us to attribute the overall prize money to each of
    these four team members across all the coalitions possible. It essentially helps
    us determine team member importance to the overall collaboration and helps us
    determine a fair way to split the overall value of the collaboration (in this
    case, the prize money).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '*Shapley值*使我们能够将总奖金分配给所有可能的联盟中的这四个团队成员。它实际上帮助我们确定团队成员对整体协作的重要性，并帮助我们确定分割整体价值（在这种情况下，奖金）的公平方式。'
- en: 'The Shapley value of each team member *p* (also called player) is computed
    in a very intuitive manner: we look at how the value *of each coalition changes*,
    with and without that team member. More formally:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 每个团队成员*p*（也称为玩家）的Shapley值是以非常直观的方式计算的：我们观察在有和没有该团队成员的情况下，每个联盟的价值是如何变化的。更正式地说：
- en: '![CH09_F16_Kunapuli-eqs-25x](../Images/CH09_F16_Kunapuli-eqs-25x.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F16_Kunapuli-eqs-25x](../Images/CH09_F16_Kunapuli-eqs-25x.png)'
- en: 'This equation might look intimidating at first, but it’s actually quite simple.
    Figure 9.17 illustrates the components of this equation when computing the Shapley
    values for Dev (team member 4): (1) coalitions *with* Dev on the first row, (2)
    corresponding coalitions *without* Dev on the second row, and (3) the weighted
    difference between the two on the third row.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程一开始可能看起来令人畏惧，但实际上非常简单。图9.17展示了在计算Dev（团队成员4）的Shapley值时，该方程的组成部分：（1）第一行中的包含Dev的联盟，（2）第二行中对应的没有Dev的联盟，以及（3）第三行中两者之间的加权差异。
- en: '![CH09_F17_Kunapuli](../Images/CH09_F17_Kunapuli.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F17_Kunapuli](../Images/CH09_F17_Kunapuli.png)'
- en: 'Figure 9.17 Computing the Shapley values for Dev. The top row is all the coalitions
    with Dev. The middle row shows the corresponding coalitions without Dev. The last
    row shows the individual weighted differences in the values of the coalitions.
    Summing across the last row gives us the Shapley values for Dev: *φ*[Dev] = 6.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.17 计算Dev的Shapley值。最上面一行是所有包含Dev的联盟。中间一行显示的是没有Dev的对应联盟。最后一行显示了联盟价值中的个体加权差异。将最后一行相加，我们得到Dev的Shapley值：*φ*[Dev]
    = 6。
- en: The weights are computed using *n*, the total number of team members (in this
    case, four), and *n*[s], the coalition size. For example, for the coalition *S*
    = {Ava, Cam}, *n*[s] = 2\. The weights for the coalition without Dev (*S*) and
    with Dev (*S* ∪ Dev) will both be (1!2!)/4! = (1/12). Other weights can be computed
    similarly.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 权重是使用 *n*，团队成员总数（在这种情况下，为四个），和 *n*[s]，联盟大小来计算的。例如，对于联盟 *S* = {Ava, Cam}，*n*[s]
    = 2。没有Dev的联盟（*S*）和包含Dev的联盟（*S* ∪ Dev）的权重都将为 (1!2!)/4! = (1/12)。其他权重可以类似地计算。
- en: Summing all the weighted differences in the last row in figure 9.17 gives us
    the Shapley value for Dev, *φ*[Dev] = 6\. Similarly, we can also obtain *φ*[Ava]
    = 4.667, *φ*[Ben] = 4.333, and *φ*[Cam] = 5\. This suggests that an equitable
    way (according to the characteristic function in figure 9.16) to attribute the
    prize money based on contribution is $4,667, $4,333, $5,000, and $6,000, respectively,
    between Ava, Ben, Cam, and Dev.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 将图9.17最后一行的所有加权差异相加，我们得到Dev的Shapley值，*φ*[Dev] = 6。同样，我们也可以得到*φ*[Ava] = 4.667，*φ*[Ben]
    = 4.333，和*φ*[Cam] = 5。这表明，根据图9.16中的特征函数，一个公平的方式来根据贡献分配奖金是Ava、Ben、Cam和Dev分别获得4,667美元、4,333美元、5,000美元和6,000美元。
- en: 'The Shapley value has some interesting theoretical properties. First, observe
    that *φ*[Ava] *+* *φ*[Ben] *+* *φ*[Cam] *+* *φ*[Dev] = 20\. That is, the Shapley
    values sum to the value of the grand coalition:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: Shapley值有一些有趣的理论属性。首先，观察*φ*[Ava] *+* *φ*[Ben] *+* *φ*[Cam] *+* *φ*[Dev] = 20。也就是说，Shapley值的总和等于大联盟的价值：
- en: '![CH09_F17_Kunapuli-eqs-27x](../Images/CH09_F17_Kunapuli-eqs-27x.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F17_Kunapuli-eqs-27x](../Images/CH09_F17_Kunapuli-eqs-27x.png)'
- en: This property of the Shapley value, called *efficiency*, ensures that the value
    of the overall collaboration is exactly broken down and attributed to each team
    member in the collaboration.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: Shapley值的这个属性，称为 *效率*，确保整体协作的价值正好分解并归因于协作中的每个团队成员。
- en: Another important property is *additivity*, which ensures that if we have two
    value functions, the overall Shapley value computed using a joint value function
    is equal to the sum of the individual Shapley values. This has some important
    implications for ensemble methods because it allows us to add Shapley values across
    individual base estimators to obtain the Shapley values across the entire ensemble.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的属性是 *可加性*，它确保如果我们有两个值函数，使用联合值函数计算的整体Shapley值等于各个Shapley值的总和。这对于集成方法有一些重要的含义，因为它允许我们将各个基础估计器的Shapley值相加，以获得整个集成中的Shapley值。
- en: So, what does the Shapley value have to do with explainability? Analogous to
    the case of the four data scientist friends, features in a machine-learning problem
    collaborate together to make predictions. The Shapley value allows us to attribute
    how much each feature contributed to the overall prediction.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，Shapley值与可解释性有什么关系呢？类似于四个数据科学家朋友的情况，机器学习问题中的特征协同工作以做出预测。Shapley值允许我们分配每个特征对整体预测的贡献程度。
- en: Shapley values as feature importance
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: Shapley值作为特征重要性
- en: Let’s say that we want to explain the predictions of a black-box model *ƒ* on
    an example *x*. The Shapley value of a feature *j* is computed as
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要解释黑盒模型 *ƒ* 在示例 *x* 上的预测。特征 *j* 的Shapley值计算如下：
- en: '![CH09_F17_Kunapuli-eqs-29x](../Images/CH09_F17_Kunapuli-eqs-29x.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F17_Kunapuli-eqs-29x](../Images/CH09_F17_Kunapuli-eqs-29x.png)'
- en: We use the black-box model as the characteristic/value function. As before,
    we consider all possible coalitions with and without the feature *j*.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将黑盒模型作为特征/值函数。与之前一样，我们考虑了包含和不包含特征 *j* 的所有可能的联盟。
- en: 'Now, we can compute the Shapley values for all the features. As before, the
    Shapley value for feature importance estimation is efficient and attributes a
    part of the overall prediction to each feature:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以计算所有特征的特征值。与之前一样，特征重要性估计的特征值计算是高效的，并将整体预测的一部分归因于每个特征：
- en: '![CH09_F17_Kunapuli-eqs-30x](../Images/CH09_F17_Kunapuli-eqs-30x.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F17_Kunapuli-eqs-30x](../Images/CH09_F17_Kunapuli-eqs-30x.png)'
- en: 'The Shapley value is theoretically well motivated and has some very attractive
    properties that make it a robust measure of feature importance. There is one significant
    limitation to using this procedure directly in practice: scalability.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: Shapley值在理论上得到了很好的论证，并且具有一些非常吸引人的属性，使其成为特征重要性的稳健度量。直接在实践中使用此程序有一个显著的局限性：可扩展性。
- en: 'The Shapley computation uses trained models to score feature importance. In
    fact, it will need to use one trained model for each coalition of features. For
    example, for our diabetes diagnosis model from earlier with two features—age and
    glc—we’ll have to train three models, one for each coalition: *ƒ*[1] (age), *ƒ*[2]
    (glc), and *ƒ*[3] (age, glc).'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: Shapley计算使用训练好的模型来评分特征重要性。实际上，它将需要为每个特征联盟使用一个训练好的模型。例如，对于之前提到的具有两个特征——年龄和glc的糖尿病诊断模型，我们将需要训练三个模型，每个联盟一个：*ƒ*[1]（年龄），*ƒ*[2]（glc），和*ƒ*[3]（年龄，glc）。
- en: In general, if we have *d* features, we’ll have 2^d total coalitions, and we’ll
    have to train 2^d - 1 models (we don’t train a model for the null coalition).
    For instance, the Bank Marketing data set has 19 features and will require the
    training of 2^(19) - 1 = 524,287 models! This is simply absurd in practice.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果我们有*d*个特征，我们将有2^d个总联盟，并且我们需要训练2^d - 1个模型（我们不训练空联盟的模型）。例如，银行营销数据集有19个特征，将需要训练2^(19)
    - 1 = 524,287个模型！这在实践中是荒谬的。
- en: SHAP
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP
- en: 'What can we do in the face of such combinatorial infeasibility? What we always
    do: approximate and sample. Inspired by LIME, the SHAP method aims to learn a
    linear surrogate function whose parameters are the Shapley values for each feature.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 面对这样的组合不可行性，我们能做什么？我们总是这样做：近似和采样。受LIME的启发，SHAP方法旨在学习一个线性代理函数，其参数是每个特征的Shapley值。
- en: 'Analogous to LIME, given a black-box model *ƒ*[b](*x*), SHAP also trains a
    surrogate model *ƒ*[s](*x*'') using a loss function that has a form identical
    to LIME’s. Unlike LIME, however, we have to accommodate the notion of coalitions
    in the loss function:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于LIME，给定一个黑盒模型*ƒ*[b](*x*)，SHAP也使用与LIME形式相同的损失函数训练一个代理模型*ƒ*[s](*x*')。然而，与LIME不同的是，我们必须在损失函数中考虑联盟的概念：
- en: '![CH09_F17_Kunapuli-eqs-31x](../Images/CH09_F17_Kunapuli-eqs-31x.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F17_Kunapuli-eqs-31x](../Images/CH09_F17_Kunapuli-eqs-31x.png)'
- en: 'Let’s understand this loss function and SHAP by seeing what it does similarly
    to and differently from LIME (also see figure 9.18). As before, let’s say that
    we’re interested in explaining the prediction of the black-box model on an example
    with five features *x* = [*x*[1],*x*[2],*x*[3],*x*[4],*x*[5]]:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过观察它与LIME（也见图9.18）的相似之处和不同之处来理解这个损失函数和SHAP。就像之前一样，假设我们感兴趣的是解释一个具有五个特征*x*
    = [*x*[1],*x*[2],*x*[3],*x*[4],*x*[5]]的示例上的黑盒模型的预测：
- en: 'LIME creates surrogate examples x'' by randomly perturbing the original example
    x. SHAP uses an involved two-step approach to create surrogate examples:'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LIME通过随机扰动原始示例x来创建代理示例x'。SHAP使用一个复杂的两步方法来创建代理示例：
- en: SHAP generates a random coalition vector, *z*, which is a 0-1 vector indicating
    if a feature should be in the coalition or not. For example, *z* = [1,1,0,0,1]
    represents a coalition of the first, second, and fifth features.
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: SHAP生成一个随机联盟向量*z*，它是一个0-1向量，表示一个特征是否应该包含在联盟中。例如，*z* = [1,1,0,0,1]表示包含第一个、第二个和第五个特征的联盟。
- en: SHAP creates a surrogate example from *z* by using a mapping function *x*' =
    *h*[x](*z*). Wherever *z*[j] = 1, we set *x*'[j]= *x*[j], the original feature
    value from the example of interest *x*. Wherever *z*[j] = 0, we set *x*'[j]= *x*[j]^(rand),a
    feature value from another randomly selected example *x*^(rand). With the choice
    of z above, our surrogate example would be *x*' = [*x*[1],*x*[2],*x*[3]^(rand),
    *x*[4]^(rand),*x*[5]].
  id: totrans-357
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: SHAP通过使用映射函数*x*' = *h*[x](*z*)从*z*创建一个代理示例。当*z*[j] = 1时，我们设置*x*'[j]= *x*[j]，即从感兴趣的示例*x*中获取的原始特征值。当*z*[j]
    = 0时，我们设置*x*'[j]= *x*[j]^(rand)，即从另一个随机选择的示例*x*^(rand)中获取的特征值。根据上述z的选择，我们的代理示例将是*x*'
    = [*x*[1],*x*[2],*x*[3]^(rand), *x*[4]^(rand),*x*[5]]。
- en: '![CH09_F18_Kunapuli](../Images/CH09_F18_Kunapuli.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F18_Kunapuli](../Images/CH09_F18_Kunapuli.png)'
- en: Figure 9.18 SHAP creates a surrogate training set of examples in the locality
    of the example whose prediction needs to be explained.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.18 SHAP在需要解释预测的示例的局部创建了一个代理训练示例集。
- en: Thus, each surrogate example is a patchwork of features from the original training
    example we want to explain and another random training example. The idea is that
    features belonging to the coalition get feature values from the example of interest,
    and features not belonging to the coalition get random “realistic feature values”
    from other examples in the data set.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个代理示例都是原始训练示例中我们想要解释的特征和另一个随机训练示例中特征的混合体。这个想法是，属于联盟的特征从感兴趣的示例中获取特征值，而不属于联盟的特征从数据集中的其他示例中获取随机的“现实特征值”。
- en: LIME weights surrogate examples x' inversely by their distance from x using
    the RBF/exponential kernel. SHAP weights surrogate examples x' using the Shapley
    kernel, which is simply the weight from the Shapley computation, *π*[*x*](*z*)
    = ((*d* - *n*[*z*] - 1)! *n*[*z*]!)/*d*!), where *d* is the total number of features,
    and *n*[z] is the coalition size (number of 1s in *z*). Intuitively, this weight
    reflects the number of other similar coalitions, with a similar number of zero
    and nonzero features.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LIME 使用 RBF/指数核根据代理示例 x' 与 x 的距离成反比地为其加权。SHAP 使用 Shapley 核为代理示例 x' 加权，这仅仅是 Shapley
    计算中的权重，π[*x*](*z*) = ((*d* - *n*[*z*] - 1)! *n*[*z*]!)/*d*!)，其中 *d* 是特征总数，*n*[z]
    是联盟大小（*z* 中 1 的数量）。直观地说，这个权重反映了具有相似数量零和非零特征的类似联盟的数量。
- en: Now that we have a surrogate training set in the locality of the example *x*,
    we can train a linear model. A version of SHAP called KernelSHAP uses linear regression
    for training. The weights of this linear model will be the approximate Shapley
    values for each feature.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一个在示例 *x* 附近的代理训练集，我们可以训练一个线性模型。一种名为 KernelSHAP 的 SHAP 版本使用线性回归进行训练。这个线性模型的权重将是每个特征的近似
    Shapley 值。
- en: SHAP in practice
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP 在实践中的应用
- en: 'Like LIME, SHAP is also available as a package available through Python’s two
    most popular package managers: pip and conda. Please see SHAP''s GitHub page ([https://github.com/slundberg/shap](https://github.com/slundberg/shap))
    for documentation and a number of examples illustrating how to use it for classification,
    regression, and applications for text, image, and even genomic data.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 与 LIME 类似，SHAP 也作为 Python 两个最受欢迎的包管理器：pip 和 conda 中的包提供。请参阅 SHAP 的 GitHub 页面
    ([https://github.com/slundberg/shap](https://github.com/slundberg/shap)) 以获取文档和许多示例，说明如何将其用于分类、回归以及文本、图像甚至基因组数据的应用。
- en: In this section, we’ll use a version of SHAP called TreeSHAP that is specifically
    designed to be used for tree-based models, including individual decision trees
    and ensembles. TreeSHAP is a special variant of SHAP that exploits the unique
    structure of decision trees to calculate the Shapley values efficiently.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用一种名为 TreeSHAP 的 SHAP 版本，该版本专门设计用于树模型，包括单个决策树和集成。TreeSHAP 是 SHAP 的一个特殊变体，它利用决策树的独特结构来有效地计算
    Shapley 值。
- en: As mentioned before, Shapley values have a nice property called additivity.
    For us, this means that if we have a model that is an additive combination of
    trees, that is, tree ensembles (e.g., bagging, random forests, gradient boosting,
    and Newton boosting, among others), then the Shapley value of the ensemble is
    simply the sum of the Shapley values of the individual trees.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Shapley 值有一个称为可加性的良好属性。对我们来说，这意味着如果我们有一个是树的可加组合的模型，即树集成（例如，bagging、随机森林、梯度提升和牛顿提升等），那么集成
    Shapley 值就是单个树 Shapley 值的总和。
- en: Because TreeSHAP can efficiently compute the Shapley values of each feature
    in each individual tree in an ensemble, we can efficiently get the Shapley values
    of the entire ensemble. Finally, unlike LIME, TreeSHAP doesn’t require us to furnish
    a surrogate data set because the trees themselves contain all the information
    (feature splits, leaf values/predictions, example counts, etc.) needed.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 TreeSHAP 可以有效地计算集成中每个单个树中每个特征的 Shapley 值，我们可以有效地得到整个集成的 Shapley 值。最后，与 LIME
    不同，TreeSHAP 不需要我们提供代理数据集，因为树本身包含所有所需的信息（特征分割、叶值/预测、示例计数等）。
- en: TreeSHAP supports many of the ensemble methods discussed in this book, including
    XGBoost. The following listing shows how to compute and interpret the Shapley
    values for test example 3104 of the Bank Marketing data set using an XGBoost model.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: TreeSHAP 支持本书中讨论的许多集成方法，包括 XGBoost。以下列表展示了如何使用 XGBoost 模型计算和解释 Bank Marketing
    数据集测试示例 3104 的 Shapley 值。
- en: Listing 9.10 Using TreeSHAP to explain XGBoost predictions
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.10 使用 TreeSHAP 解释 XGBoost 预测
- en: '[PRE13]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Explains the predictions of test example 3104
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 解释测试示例 3104 的预测
- en: ❷ Visualizes Shapley values using a waterfall plot
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用瀑布图可视化 Shapley 值
- en: ❸ Visualizes Shapley values using a force plot
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用力图可视化 Shapley 值
- en: 'This listing visualizes Shapley values in two ways: as a waterfall plot (figure
    9.19) and as a force plot (figure 9.20). Keep in mind that SHAP explains classifier
    models in terms of their prediction probabilities (confidence). For a classifier,
    the x-axis values will be the log-odds, with 0.0 representing even odds (1:1)
    of classification, or 50% prediction probability as a positive example.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表以两种方式可视化 Shapley 值：作为瀑布图（图 9.19）和作为力图（图 9.20）。请记住，SHAP 以预测概率（置信度）来解释分类器模型。对于分类器，x
    轴的值将是 log-odds，其中 0.0 代表分类的平等概率（1:1），或 50% 预测概率作为一个正面例子。
- en: 'The waterfall plot in figure 9.19 shows the individual contributions of each
    feature to the overall prediction for example 3104\. As we can see, the individual
    predictive contributions of each feature add up to the overall final prediction:
    0.518\. This is a clear visual illustration of the additive nature of SHAP’s explanations.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.19 中的瀑布图显示了每个特征对示例 3104 的整体预测的个体贡献。正如我们所见，每个特征的个体预测贡献加起来等于最终的预测值：0.518。这是
    SHAP 解释加性特性的一个清晰的视觉说明。
- en: '![CH09_F19_Kunapuli](../Images/CH09_F19_Kunapuli.png)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F19_Kunapuli](../Images/CH09_F19_Kunapuli.png)'
- en: Figure 9.19 A waterfall plot to visualize Shapley values. The values along the
    left side of the plot show the feature values for test example 3104, while the
    text in the bars shows their Shapley values.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.19 中的瀑布图用于可视化 Shapley 值。图中左侧的值显示测试示例 3104 的特征值，而条形中的文本显示它们的 Shapley 值。
- en: The force plot in figure 9.20 allows for a more intuitive view of how the features
    contribute to a prediction. The plot is centered around the prediction (0.518)
    and visualizes how much the features force the prediction through a positive or
    negative explanation.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.20 中的力图允许更直观地查看特征如何对预测做出贡献。该图以预测（0.518）为中心，并可视化特征通过正或负解释推动预测的程度。
- en: '![CH09_F20_Kunapuli](../Images/CH09_F20_Kunapuli.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F20_Kunapuli](../Images/CH09_F20_Kunapuli.png)'
- en: Figure 9.20 A force plot to visualize Shapley values. The features pointing
    to the right push the prediction on this example (0.52) to be higher than the
    average prediction (-0.194). The features pointing to the left push the prediction
    lower and closer to base value. The contributions add up to the overall prediction,
    which is higher than the base value for this example. The feature values of the
    example being explained are shown along with the features under the force plot.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.20 中的力图用于可视化 Shapley 值。指向右侧的特征将此示例（0.52）的预测推高，高于平均预测（-0.194）。指向左侧的特征将预测推低，并接近基值。贡献加起来等于整体预测，对于此示例，整体预测高于基值。解释示例的特征值以及力图下的特征一起显示。
- en: 'NOTE LIME and SHAP are both *additive* local explainability methods. This means
    that they can be extended to global explainability in a rather straightforward
    manner: global feature importances from either method can be obtained by averaging
    over local feature importances computed with a task-relevant data set.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：LIME 和 SHAP 都是 *加性* 本地可解释方法。这意味着它们可以以相当直接的方式扩展到全局可解释性：可以通过对与任务相关的数据集计算出的本地特征重要性进行平均，从任一方法中获得全局特征重要性。
- en: One drawback of LIME and SHAP is that they are designed only to compute and
    evaluate individual feature importances, and not feature interactions. SHAP offers
    some support for visualizing feature interactions in a manner similar to PDPs.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: LIME 和 SHAP 的一个缺点是它们仅设计用于计算和评估单个特征的重要性，而不是特征交互。SHAP 提供了一些支持，以类似于 PDPs 的方式可视化特征交互。
- en: However, like PDPs, SHAP doesn’t have any mechanism to automatically identify
    important interacting feature groups and forces us to visualize all pairs, which
    can be overwhelming. For example, with 19 features in the Bank Marketing data
    set, we’ll have 171 pairwise feature interactions.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与 PDPs 一样，SHAP 没有任何机制可以自动识别重要的交互特征组，并迫使我们可视化所有成对的特征，这可能会令人不知所措。例如，在 Bank
    Marketing 数据集中有 19 个特征，我们将有 171 对特征交互。
- en: 'In real-world applications, since many features depend on each other, it’s
    important to also understand how feature interactions come into play in decision
    making. In the next section, we’ll learn about one such method: explainable boosting
    machines.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的应用中，由于许多特征相互依赖，了解特征交互在决策中的作用也很重要。在下一节中，我们将了解一种这样的方法：可解释提升机。
- en: '9.5 Glass-box ensembles: Training for interpretability'
  id: totrans-385
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 玻璃盒集成：训练以实现可解释性
- en: We’ve learned about model-agnostic explainability methods. These methods can
    take a model that was already trained (e.g., by an ensemble learner such as XGBoost)
    and attempt to explain the model itself (global) or its predictions (local).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了模型无关的可解释性方法。这些方法可以接受已经训练好的模型（例如，由 XGBoost 等集成学习器训练）并尝试解释模型本身（全局）或其预测（局部）。
- en: 'But instead of treating our ensembles as a black box, can we train an explainable
    ensemble from scratch? Can this ensemble method still perform well *and* be explainable?
    These are the types of questions that motivated the development of *explainable
    boosting machines* (EBMs), a type of glass-box ensemble method. Some key highlights
    of EBMs are as follows:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们是否可以训练一个可解释的集成从零开始？这种集成方法是否仍然可以表现良好并且是可解释的？这些问题是推动**可解释提升机**（EBMs）发展的动力，这是一种玻璃盒集成方法。EBMs
    的一些关键亮点如下：
- en: EBMs can be used for both global explainability and local explainability of
    individual examples!
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EBMs 可以用于全局可解释性和单个示例的局部可解释性！
- en: EBMs learn a fully factorized model; that is, the model components only depend
    on individual features or pairs of features. These components provide interpretability
    directly, and EBMs need no additional computations (like SHAP or LIME) to generate
    explanations.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EBMs 学习一个完全分解的模型；也就是说，模型组件只依赖于单个特征或特征对。这些组件直接提供可解释性，EBMs 不需要额外的计算（如 SHAP 或 LIME）来生成解释。
- en: EBMs are a type of *generalized additive model* (GAM), which are nonlinear extensions
    of GLMs discussed in this chapter and elsewhere in the book. Similar to GLMs,
    each component of a GAM only depends on one feature.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EBMs 是一种**广义加性模型**（GAM），是本章以及书中其他地方讨论的 GLMs 的非线性扩展。与 GLMs 类似，GAM 的每个组件只依赖于一个特征。
- en: EBMs can also detect important *pairwise feature interactions*. Thus, EBMs extend
    the GAMs to include components of two features.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EBMs 也可以检测重要的**成对特征交互**。因此，EBMs 将 GAMs 扩展到包括两个特征的组件。
- en: EBMs use a *cyclic* training approach, where a very large number of base estimators
    are trained by repeated passes through all the features. This approach is also
    parallelizable, which makes EBMs an efficient training approach.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EBMs 使用一种**循环**的训练方法，其中通过重复遍历所有特征来训练大量基础估计器。这种方法也是可并行的，这使得 EBMs 成为一个高效的训练方法。
- en: In the next two sections, we’ll see how EBMs work conceptually, as well as how
    we can train and use them in practice.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个部分中，我们将了解 EBMs 的概念工作方式，以及如何在实践中训练和使用它们。
- en: 9.5.1 Explainable boosting machines
  id: totrans-394
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.1 可解释提升机
- en: 'EBMs have two key components: they are generalized additive models (GAMs),
    and they have feature interactions. This allows the model representation to be
    broken down into smaller components, allowing for better interpretation.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: EBMs 有两个关键组件：它们是广义加性模型（GAMs），并且它们具有特征交互。这允许模型表示分解成更小的组件，从而实现更好的解释。
- en: GAMs with feature interactions
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 具有特征交互的 GAMs
- en: 'We’re familiar with the concept of the GLM, which uses link functions *g*(*y*)
    to relate targets to linear models over features:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 我们熟悉 GLM 的概念，它使用链接函数 *g*(*y*) 将目标与特征上的线性模型相关联：
- en: '![CH09_F20_Kunapuli-eqs-33x](../Images/CH09_F20_Kunapuli-eqs-33x.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F20_Kunapuli-eqs-33x](../Images/CH09_F20_Kunapuli-eqs-33x.png)'
- en: 'Each component of the GLM *β*[j]*x*[j] only depends on one feature *x*[j].
    The GAM extends this nonlinear model over the features:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: GLM 的每个组件 *β*[j]*x*[j] 只依赖于一个特征 *x*[j]。GAM 将这个非线性模型扩展到特征上：
- en: '![CH09_F20_Kunapuli-eqs-34x](../Images/CH09_F20_Kunapuli-eqs-34x.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F20_Kunapuli-eqs-34x](../Images/CH09_F20_Kunapuli-eqs-34x.png)'
- en: As with the GLM, each component of a GAM *ƒ*(*x*[j]) also depends on only one
    feature *x*[j]. Keep in mind that both GLMs and GAMs can be viewed as ensembles,
    with each component of the ensemble depending on only one feature! This has important
    implications for training.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 与 GLM 类似，GAM 的每个组件 *ƒ*(*x*[j]) 也只依赖于一个特征 *x*[j]。记住，GLMs 和 GAMs 都可以被视为集成，其中集成的每个组件只依赖于一个特征！这对训练有重要影响。
- en: 'EBMs further extend GAMs to include pairwise components as well. However, since
    the number of feature pairs can be very large, EBMs only include a small number
    of important feature pairs:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: EBMs 进一步扩展 GAMs 以包括成对组件。然而，由于特征对的数量可能非常大，EBMs 只包括少量重要的特征对：
- en: '![CH09_F20_Kunapuli-eqs-35x](../Images/CH09_F20_Kunapuli-eqs-35x.png)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F20_Kunapuli-eqs-35x](../Images/CH09_F20_Kunapuli-eqs-35x.png)'
- en: 'This is also shown in figure 9.21 for the diabetes diagnosis problem from earlier,
    but with three variables: age, blood glucose level (glc), and body mass index
    (bmi). This example EBM contains components for all three features individually,
    and one pairwise component rather than all three combinations.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 这也在图9.21中展示了，这是之前提到的糖尿病诊断问题，但增加了三个变量：年龄、血糖水平（glc）和体质指数（bmi）。这个例子中的EBM包含所有三个特征的单个组件，以及一个成对组件而不是所有三个组合。
- en: '![CH09_F21_Kunapuli](../Images/CH09_F21_Kunapuli.png)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F21_Kunapuli](../Images/CH09_F21_Kunapuli.png)'
- en: 'Figure 9.21 An EBM is a generalized additive model consisting of nonlinear
    components that depend on only one feature as well as nonlinear components that
    depend on pairs of features. This example shows an EBM for diabetes diagnosis
    dependent on three variables: age, glc, and bmi. Though there are three pairs
    of variables (age-glc, glc-bmi, age-bmi), this EBM includes only one of them that
    it has deemed significant. The explainable boosting model is also an ensemble.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.21一个EBM是一个广义加性模型，由仅依赖于一个特征的非线性组件以及依赖于特征对的非线性组件组成。这个例子展示了依赖于三个变量（年龄、glc和bmi）的糖尿病诊断EBM。尽管有三个变量对（年龄-glc、glc-bmi、年龄-bmi），但这个EBM只包括它认为有意义的其中一个。可解释的增强模型也是一个集成。
- en: Since each component is a function of only one or two variables, once learned,
    we can immediately visualize the dependence between each variable (or pair of
    variables) and the target. In addition, the EBM avoids incorporating all pairwise
    components, and only selects the most effectual ones. This avoids model bloat
    and improves explainability. By carefully choosing the structure of the EBM, we
    can train an explainable ensemble, which makes this a glass-box method.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个组件仅是单个或两个变量的函数，一旦学习，我们就可以立即可视化每个变量（或变量对）与目标之间的依赖关系。此外，EBM避免了包含所有成对组件，并且只选择最有效的组件。这避免了模型膨胀并提高了可解释性。通过仔细选择EBM的结构，我们可以训练一个可解释的集成，这使得它成为一个玻璃盒方法。
- en: But what about model performance? Is it possible to train an EBM effectively
    to perform as well as existing ensemble methods?
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 但模型性能如何？是否可以有效地训练一个EBM，使其与现有的集成方法一样表现良好？
- en: Training EBMs
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 训练EBMs
- en: 'As with GLMs and GAMs, the EBM is also an ensemble of base components over
    individual features as well as feature pairs. This is important because it allows
    us to train EBMs sequentially using simple modifications of our favorite ensemble
    learner: gradient boosting. EBMs are similarly trained using a two-stage procedure:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 与GLMs和GAMs一样，EBM也是基于单个特征以及特征对的基组件的集成。这很重要，因为它允许我们通过简单修改我们最喜欢的集成学习器：梯度提升法来顺序训练EBM。EBM也使用两阶段程序进行类似训练：
- en: In the first stage, the EBM fits components for each feature *ƒ*[j](*x*[j]).
    This is done through a cyclical and sequential training process over several thousand
    iterations, one feature at a time. In iteration *t*, for feature *j*, we fit a
    very shallow treetj using gradient boosting. Once we cycle through all the features
    within an iteration, we move on to the next iteration. This procedure is illustrated
    in figure 9.22.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一阶段，EBM为每个特征 *ƒ*[j](*x*[j]) 配置组件。这是通过在数千次迭代中循环和顺序训练过程来完成的，每次只处理一个特征。在迭代 *t*
    中，对于特征 *j*，我们使用梯度提升法拟合一个非常浅的树tj。一旦我们循环遍历了迭代中所有特征，我们就进入下一个迭代。这个过程在图9.22中得到了说明。
- en: '![CH09_F22_Kunapuli](../Images/CH09_F22_Kunapuli.png)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F22_Kunapuli](../Images/CH09_F22_Kunapuli.png)'
- en: Figure 9.22 The first stage of the training procedure for EBMs, where models
    for each feature are trained sequentially and cyclically, with one model per feature
    per iteration. The trees trained are shallow, and the learning rate is very low.
    However, over a very large number of iterations, a sufficiently complex nonlinear
    model for each feature can be learned. A similar procedure is also followed for
    the second stage of training EBMs, where models for pairwise feature interactions
    are trained.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.22展示了EBM训练过程的第一阶段，其中每个特征的模型都是顺序和循环训练的，每个迭代中每个特征一个模型。训练的树很浅，学习率非常低。然而，在非常大量的迭代中，可以为每个特征学习到一个足够复杂的非线性模型。对于EBM训练的第二阶段，也遵循了类似的程序，其中对特征对的交互模型进行训练。
- en: The partially trained EBM *g*(*y*) = *ƒ*[1](*x*[1]) + ⋅⋅⋅ + *ƒ*[d](*x*[d]) is
    now frozen and used to evaluate and score all possible feature pairs (*x*[i],*x*[j]).
    This enables EBM to determine critically important feature interaction pairs (*x*[a],*x*[b])
    ⋅⋅⋅ (*x*[u],*x*[v]) in the data. A small number of relevant feature pairs are
    selected.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部分训练好的 EBM *g*(*y*) = *ƒ*[1](*x*[1]) + ⋅⋅⋅ + *ƒ*[d](*x*[d]) 现在已被冻结，并用于评估和评分所有可能的特征对
    (*x*[i],*x*[j])。这使得 EBM 能够确定数据中至关重要的特征交互对 (*x*[a],*x*[b]) ⋅⋅⋅ (*x*[u],*x*[v])。选择了一小部分相关的特征对。
- en: 'In the second stage, the EBM fits shallow trees tree^t[jk] for each feature
    pair *ƒ*[jk](*x*[j],*x*[k]) in a manner identical to the first stage. This produces
    a fully trained EBM: *g*(*y*) = *ƒ*[1](*x*[1]) + ⋅⋅⋅ + *ƒ*[d](*x*[d]) + *ƒ*[ab](*x*[a],*x*[b])
    + ⋅⋅⋅ + *ƒ*[uv](*x*[u],*x*[v]).'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二阶段，EBM 以与第一阶段相同的方式为每个特征对 *ƒ*[jk](*x*[j],*x*[k]) 拟合浅层树树^t[jk]。这产生了一个完全训练好的
    EBM：*g*(*y*) = *ƒ*[1](*x*[1]) + ⋅⋅⋅ + *ƒ*[d](*x*[d]) + *ƒ*[ab](*x*[a],*x*[b])
    + ⋅⋅⋅ + *ƒ*[uv](*x*[u],*x*[v])。
- en: 'From figure 9.22, we can see that each individual component *ƒ*[j](*x*[j])
    is actually an ensemble of thousands of shallow trees:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 从图9.22中，我们可以看到每个单独的组件 *ƒ*[j](*x*[j]) 实际上是由成千上万的浅层树组成的集合：
- en: '![CH09_F21_Kunapuli-eqs-36x](../Images/CH09_F21_Kunapuli-eqs-36x.png)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F21_Kunapuli-eqs-36x](../Images/CH09_F21_Kunapuli-eqs-36x.png)'
- en: 'Similarly, each feature interaction component is also an ensemble:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，每个特征交互组件也是一个集合：
- en: '![CH09_F22_Kunapuli-eqs-37x](../Images/CH09_F22_Kunapuli-eqs-37x.png)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F22_Kunapuli-eqs-37x](../Images/CH09_F22_Kunapuli-eqs-37x.png)'
- en: 'So how exactly is this EBM a glass box? In three ways:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这个 EBM 究竟是如何成为一个透明的“玻璃箱”的呢？有三种方式：
- en: '*Local interpretability*—For a classification problem, given a specific example,
    if we want to explain x, we can get the log-odds of prediction from the EBM as
    *ƒ*[1](*x*[1]) + ⋅⋅⋅ + *ƒ*[d](*x*[d]) + *ƒ*[ab](*x*[a],*x*[b]) + ⋅⋅⋅ + *ƒ*[uv](*x*[u],*x*[v]).
    By construction, the EBM is already a fully decomposed and additive model, allowing
    us to simply grab the contribution of each feature *ƒ*[j](*x*[j]) or feature pair
    *ƒ*[jk](*x*[j],*x*[k]). For regression, we can get the contribution to the overall
    regression value similarly. In both cases, there is no additional procedure like
    LIME or SHAP, and there is no need to approximate using linear models!'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*局部可解释性*——对于一个分类问题，给定一个特定的例子，如果我们想解释 x，我们可以从 EBM 中获取预测的对数几率作为 *ƒ*[1](*x*[1])
    + ⋅⋅⋅ + *ƒ*[d](*x*[d]) + *ƒ*[ab](*x*[a],*x*[b]) + ⋅⋅⋅ + *ƒ*[uv](*x*[u],*x*[v])。由于
    EBM 已经是一个完全分解和加性模型，我们可以简单地获取每个特征 *ƒ*[j](*x*[j]) 或特征对 *ƒ*[jk](*x*[j],*x*[k]) 的贡献。对于回归，我们可以以类似的方式获取对整体回归值的贡献。在这两种情况下，都没有像
    LIME 或 SHAP 这样的额外程序，也不需要使用线性模型进行近似！'
- en: '*Global interpretability*—Since we have each component *ƒ*[j](*x*[j]) or *ƒ*[jk](*x*[j],*x*[k]),
    we can also plot this over the feature ranges of *x*[j] and/or *x*[k]. This will
    produce a *dependency plot* for the features *x*[j] and/or *x*[k] over all possible
    values they can take. This tells us how the model behaves in the aggregate.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*全局可解释性*——由于我们有每个组件 *ƒ*[j](*x*[j]) 或 *ƒ*[jk](*x*[j],*x*[k])，我们还可以在 *x*[j] 和/或
    *x*[k] 的特征范围内绘制它们。这将产生一个 *依赖图*，显示所有可能的值下 *x*[j] 和/或 *x*[k] 的特征。这告诉我们模型的整体行为。'
- en: '*Feature interactions*—Unlike SHAP or LIME, the model also inherently identifies
    key feature interactions, by design. This provides additional insights into model
    behavior and helps explain predictions better.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征交互*——与 SHAP 或 LIME 不同，该模型也通过设计内在地识别关键特征交互。这提供了对模型行为的额外见解，并有助于更好地解释预测。'
- en: 9.5.2 EBMs in practice
  id: totrans-424
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.2 实践中的 EBMs
- en: EBMs are available as part of the InterpretML package. In addition to EBMs,
    the InterpretML package also provides wrappers for LIME and SHAP, allowing us
    to use them in one framework. InterpretML also includes some nice functionalities
    for visualization. In this section, though, we’ll only explore how to train, visualize,
    and interpret EBMs with InterpretML.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: EBMs 作为 InterpretML 包的一部分可用。除了 EBMs，InterpretML 包还提供了 LIME 和 SHAP 的包装器，使我们能够在同一个框架中使用它们。InterpretML
    还包括一些用于可视化的良好功能。然而，在本节中，我们只将探索如何使用 InterpretML 训练、可视化和解释 EBMs。
- en: NOTE InterpretML can be installed through pip and Anaconda. The package’s documentation
    page ([https://interpret.ml/](https://interpret.ml/)) contains additional information
    on how to use various glass-box and black-box models.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：InterpretML 可以通过 pip 和 Anaconda 安装。该包的文档页面 ([https://interpret.ml/](https://interpret.ml/))
    包含有关如何使用各种透明和黑盒模型的额外信息。
- en: 'Listing 9.11 shows how we can train EBMs on the Bank Marketing data set. Like
    random forests and XGBoost models trained in section 9.2, we’ll have to account
    for the class imbalance in the data. We do this by weighting positive examples
    by 5.0 and negative examples by 1.0 during training. This listing also creates
    two visualizations: one for local explainability (of test example 3104) and another
    for global explainability (using feature importances and dependency plots).'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.11展示了如何在Bank Marketing数据集上训练EBM。与第9.2节中训练的随机森林和XGBoost模型一样，我们必须考虑数据中的类别不平衡。我们在训练过程中通过将正面示例加权重5.0和负面示例加权重1.0来实现这一点。此列表还创建了两个可视化：一个用于局部可解释性（测试示例3104）和另一个用于全局可解释性（使用特征重要性和依赖图）。
- en: Listing 9.11 Training and visualizing EBMs using InterpretML
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.11 使用InterpretML训练和可视化EBM
- en: '[PRE14]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Weights examples 1:5 to account for class imbalance
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 以1:5的比例加权重以考虑类别不平衡
- en: '❷ Identifies the feature type for EBM: categorical, continuous'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为EBM识别特征类型：分类、连续
- en: ❸ Initializes and trains an EBM with these weights
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用这些权重初始化和训练EBM
- en: ❹ Initializes the InterpretML visualizers
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 初始化InterpretML可视化器
- en: ❺ Test example 3104 explanation
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 测试示例3104解释
- en: ❻ Local explanations (for test example 3104)
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 测试示例3104的局部解释
- en: ❼ Global explanations (feature importances and dependency plots)
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 全局解释（特征重要性和依赖图）
- en: ExplainableBoostingClassifier trains for 5,000 rounds by default, with support
    for early stopping. ExplainableBoostingClassifier also limits the number of pairwise
    interactions to 10 (by default, though this can be set by the user). Since this
    data set has 19 features, there will be 171 total pairwise interactions, of which
    the model will pick the top 10.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: ExplainableBoostingClassifier默认训练5,000轮，支持提前停止。ExplainableBoostingClassifier还限制成对交互的数量为10（默认情况下，尽管这可以由用户设置）。由于此数据集有19个特征，总共有171个成对交互，模型将选择前10个。
- en: The trained EBM model has an overall accuracy of 86.69% and balanced accuracy
    of 74.59%. The XGBoost model trained in section 9.2 has an overall accuracy of
    87.24% and balanced accuracy of 74.67%. The EBM model is pretty comparable to
    the XGBoost model! The key difference is that the XGBoost model is a black box,
    while the EBM is a glass box.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好的EBM模型整体准确率为86.69%，平衡准确率为74.59%。第9.2节中训练的XGBoost模型整体准确率为87.24%，平衡准确率为74.67%。EBM模型与XGBoost模型相当！关键区别在于XGBoost模型是一个黑盒，而EBM是一个玻璃盒。
- en: So, what can we get out of this glass box? Figure 9.23 shows the local explanations
    of test example 3104\. The local explanations show how much each feature and feature
    interaction pair in the model contributes to the overall positive or negative
    prediction.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们能从这个玻璃盒中得到什么？图9.23展示了测试示例3104的局部解释。局部解释显示了模型中每个特征及其特征交互对对整体正面或负面预测的贡献程度。
- en: '![CH09_F23_Kunapuli](../Images/CH09_F23_Kunapuli.png)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F23_Kunapuli](../Images/CH09_F23_Kunapuli.png)'
- en: Figure 9.23 Local explainability of test example 3104, with individual features
    (e.g., euribor3m and poutcome) and pairwise features (e.g., month × day_of_week).
    The value of each EBM component and their contribution to the overall prediction
    (Sub? = YES) is shown.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.23展示了测试示例3104的局部可解释性，包括单个特征（例如，euribor3m和poutcome）和成对特征（例如，月份×星期几）。每个EBM组件的值及其对整体预测的贡献（Sub?
    = YES）显示出来。
- en: Test example 3104 is a positive example (i.e., Sub?=YES, meaning that the customer
    did subscribe to a fixed-term deposit account). The EBM model has correctly classified
    this example, with confidence (prediction probability) of 66.1%.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 测试示例3104是一个正面示例（即Sub?=YES，表示客户确实订阅了定期存款账户）。EBM模型已正确分类此示例，置信度（预测概率）为66.1%。
- en: 'The trained EBM model uses several features such as nr.employed that we know
    are important, similar to other approaches. This trained EBM also uses three pairwise
    features to make a prediction for 3104: month x day_of_week, day_of_week x cons.conf.idx,
    default x month. The highest pairwise feature interaction is month x day_of_week,
    which contributes a positive amount to the overall prediction. Contrast this to
    LIME and SHAP explanations of the XGBoost black box, which could only identify
    month since they don’t support feature interactions explicitly. The EBM model
    is able to learn to use a finer-grained feature and also explain its importance!
    The takeaway here is that the EBM model is explicitly structured to incorporate
    feature interactions and to explain them.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好的EBM模型使用了一些特征，例如nr.employed，这些特征我们知道是重要的，与其他方法类似。这个训练好的EBM还使用了三个成对特征来预测3104：月份
    x 星期几，星期几 x cons.conf.idx，default x 月份。最高的成对特征交互是月份 x 星期几，它对整体预测贡献了正值。这与LIME和SHAP对XGBoost黑盒的解释形成对比，因为它们只能识别月份，因为它们不支持显式地处理特征交互。EBM模型能够学习使用更细粒度的特征，并解释其重要性！这里的启示是，EBM模型明确地构建了包含特征交互并解释它们的结构。
- en: EBMs can also provide global interpretability in terms of feature importances.
    The overall importance is obtained by averaging (the absolute values) of individual
    feature importances over the entire training set.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: EBMs还可以在特征重要性方面提供全局可解释性。整体重要性是通过在整个训练集上平均（绝对值）个体特征重要性来获得的。
- en: 'The overall model contains 30 components: 19 individual feature components,
    10 pairwise feature components, and 1 intercept. The top-15 feature and pairwise
    feature importances are shown in figure 9.24\. These results are in general agreement
    with previous feature importance measures computed using other methods such as
    SHAP and LIME.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 整个模型包含30个组件：19个个体特征组件，10个成对特征组件，以及1个截距。图9.24显示了前15个特征和成对特征的重要性。这些结果与使用其他方法（如SHAP和LIME）计算出的先前特征重要性度量总体上是一致的。
- en: '![CH09_F24_Kunapuli](../Images/CH09_F24_Kunapuli.png)'
  id: totrans-446
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F24_Kunapuli](../Images/CH09_F24_Kunapuli.png)'
- en: Figure 9.24 Global explainability of the trained EBM model, showing feature
    importance scores
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.24展示了训练好的EBM模型的全局可解释性，显示了特征重要性分数
- en: Finally, we can also obtain dependency plots directly from the EBM (as described
    earlier in figure 9.22). Figure 9.25 shows the dependency plot for age and how
    it influences whether someone will subscribe to a fixed-deposit account.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还可以直接从EBM（如前文图9.22所述）获得依赖图。图9.25显示了年龄的依赖图以及它如何影响某人是否会订阅定期存款账户。
- en: '![CH09_F25_Kunapuli](../Images/CH09_F25_Kunapuli.png)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F25_Kunapuli](../Images/CH09_F25_Kunapuli.png)'
- en: Figure 9.25 Dependency plot for age. The x-axis bins representing age are scaled
    to the range 0-1 during preprocessing. The raw ages are in the range 17-98\. Scores
    are negative for people in the range 0.2-0.4, which corresponds to ages 33-49\.
    This suggests that absent any other information, people in this age range are
    typically not likely to subscribe to a fixed-deposit account.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.25展示了年龄的依赖图。预处理期间，代表年龄的x轴区间被缩放到0-1的范围。原始年龄在17-98岁之间。对于0.2-0.4范围内的分数是负值，这对应于33-49岁。这表明，如果没有其他信息，这个年龄范围内的人通常不太可能订阅定期存款账户。
- en: Summary
  id: totrans-451
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Black-box models are typically challenging to understand owing to their complexity.
    The predictions of such models require specialized tools to be explainable.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黑盒模型由于其复杂性，通常难以理解。这类模型的预测需要专门的工具才能解释。
- en: Glass-box models are more intuitive and easier to understand. The structure
    of such models makes them inherently interpretable.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 白盒模型更直观，更容易理解。这类模型的结构使它们本质上具有可解释性。
- en: Most ensemble methods are typically black-box methods.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数集成方法通常是黑盒方法。
- en: Global methods attempt to generally explain a model’s overall decision-making
    process and the broadly relevant factors.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全局方法试图一般性地解释模型的整体决策过程和广泛相关的因素。
- en: Local methods attempt to specifically explain a model’s decision-making process
    with respect to individual examples and predictions.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局部方法试图具体解释模型针对单个示例和预测的决策过程。
- en: Feature importance is an interpretability method that assigns scores to features
    based on their contribution to correct prediction of a target variable.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征重要性是一种可解释性方法，它根据特征对目标变量正确预测的贡献来分配分数。
- en: Decision trees are commonly used glass-box models and can be expressed as a
    set of decision rules, which are easily interpretable by humans.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树是常用的玻璃盒模型，可以用一组决策规则表示，这些规则对人类来说很容易理解。
- en: The interpretability of decision trees depends on their complexity (depth and
    number of leaf nodes). More complex trees are less intuitive and harder to understand.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树的可解释性取决于它们的复杂性（深度和叶节点数）。更复杂的树更不直观，更难理解。
- en: Generalized linear models (GLMs) are another commonly used glass-box model.
    Their feature weights can be interpreted as feature importances as they determine
    how much each feature contributes to the overall decision.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广义线性模型（GLMs）是另一种常用的玻璃盒模型。它们的特征权重可以解释为特征重要性，因为它们决定了每个特征对整体决策的贡献程度。
- en: Permutation feature importance is a black-box method for global interpretability.
    It tries to estimate how the model’s predictive performance changes from before
    to after we shuffle/permute features.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新排列特征重要性是一种用于全局可解释性的黑盒方法。它试图估计在重新排列/重新排列特征之前和之后，模型的预测性能如何变化。
- en: Partial dependence plots (PDPs) comprise another black-box method for global
    interpretability. Partial dependences are identified using marginalization or
    summing out of other variables.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局部依赖性图（PDPs）是另一种用于全局可解释性的黑盒方法。局部依赖性是通过边缘化或求和其他变量来确定的。
- en: Surrogate models are often used to mimic or approximate the behavior of a black-box
    model. Surrogate models are glass boxes and inherently explainable.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理模型通常用于模拟或近似黑盒模型的行为。代理模型是玻璃盒，本质上可解释。
- en: Global surrogate models, such as decision trees, train models to optimize the
    fidelity-interpretability tradeoff.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全局代理模型，如决策树，训练模型以优化保真度-可解释性权衡。
- en: Locally Interpretable Model-Agnostic Explanation (LIME) is a local surrogate
    model that trains a linear model in the neighborhood of the example we want to
    explain.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局部可解释模型无关解释（LIME）是一种局部代理模型，在我们要解释的示例的邻域内训练一个线性模型。
- en: LIME also optimizes the fidelity-interpretability tradeoff and does so with
    a surrogate training set generated by perturbing features in the local neighborhood
    of the example to be explained.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LIME还优化了保真度-可解释性权衡，并通过通过扰动要解释的示例的局部邻域中的特征来生成的代理训练集来实现这一点。
- en: Shapley values allow us to attribute the overall contribution of individual
    features (feature importances) by considering their contributions across all possible
    combinations of features.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shapley值允许我们通过考虑所有可能的特征组合中的贡献来分配单个特征的总体贡献（特征重要性）。
- en: Shapley values are infeasible to compute directly for real-world data sets with
    many features and examples.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于具有许多特征和示例的真实世界数据集，Shapley值直接计算是不可行的。
- en: SHapley Additive exPlanations (SHAP) is a local surrogate model that trains
    a local linear model to approximate Shapley values.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SHapley Additive exPlanations (SHAP)是一种局部代理模型，它训练一个局部线性模型来近似Shapley值。
- en: For tree-based models, a specially designed variant called TreeSHAP is used
    to compute the Shapley values efficiently.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于基于树的模型，使用特别设计的变体TreeSHAP来有效地计算Shapley值。
- en: Shapley values and SHAP both have the additivity property, which allows us to
    aggregate Shapley values when ensembling individual models.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shapley值和SHAP都具有可加性属性，这允许我们在集成单个模型时聚合Shapley值。
- en: One drawback of LIME and SHAP is that they are fundamentally designed only to
    compute and evaluate individual feature importances, and not feature interactions.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LIME和SHAP的一个缺点是它们本质上只设计用来计算和评估单个特征的相对重要性，而不是特征之间的相互作用。
- en: The explainable boosting machine (EBM) is a type of glass-box model that can
    be used for both global explainability and local explainability of individual
    examples.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释的增强机器（EBM）是一种玻璃盒模型，可用于全局可解释性和单个示例的局部可解释性。
- en: EBMs learn a fully factorized model; that is, the model components only depend
    on individual features or pairs of features. These components provide interpretability
    directly, and EBMs need no additional computations (like SHAP or LIME) to generate
    explanations.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EBMs学习一个完全分解的模型；也就是说，模型组件只依赖于单个特征或特征对。这些组件直接提供可解释性，EBMs不需要额外的计算（如SHAP或LIME）来生成解释。
- en: EBMs are a type of generalized additive model (GAM), which are nonlinear extensions
    of GLMs.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EBMs是一种广义加性模型（GAM），是GLM的非线性扩展。
- en: EBMs can also detect important pairwise feature interactions. Thus, EBMs extend
    the GAMs to include components of two features.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EBMs 还可以检测重要的成对特征交互。因此，EBMs 将 GAMs 扩展到包括两个特征的组件。
- en: EBMs use a cyclic training approach, where a very large number of base estimators
    are trained by repeated passes through all the features. This approach is also
    parallelizable, which makes using EBMs an efficient training approach.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EBMs 使用循环训练方法，通过重复遍历所有特征来训练大量的基础估计器。这种方法也是可并行的，这使得使用 EBMs 成为一个高效的训练方法。
- en: '* * *'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.) S. Moro, P. Cortez and P. Rita, “A Data-Driven Approach to Predict the
    Success of Bank Telemarketing,” *Decision Support Systems*, 62:22-31, June 2014.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: (1.) S. Moro, P. Cortez 和 P. Rita, “基于数据驱动预测银行电话营销成功的方法,” *决策支持系统*, 第 62 卷，第
    22-31 页，2014 年 6 月。
