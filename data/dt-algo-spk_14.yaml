- en: Chapter 10\. Practical Data Design Patterns
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。实用数据设计模式
- en: The goal of this chapter is to introduce some practical data design patterns
    that are useful in solving common data problems. We will focus on actual design
    patterns that are used in big data solutions and deployed in production environments.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是介绍一些在解决常见数据问题中有用的实用数据设计模式。我们将专注于实际的设计模式，这些设计模式用于大数据解决方案，并且已在生产环境中部署。
- en: As in the previous chapter, I’ll provide simple examples to illustrate the use
    of each one and show you how to use Spark’s transformations to implement them.
    I’ll also talk more about the concept of monoids, to help you better understand
    reduction transformations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一章一样，我将提供简单的示例来说明每个设计模式的使用，并向您展示如何使用 Spark 的转换来实现它们。我还将更多地讨论单子的概念，帮助您更好地理解归约转换。
- en: 'The best design patterns book available is the iconic computer science book
    *Design Patterns: Elements of Reusable Object-Oriented Software* by Erich Gamma,
    Richard Helm, Ralph Johnson, and John Vlissides (known as The “Gang of Four”).
    Rather than present data design patterns similar to those in the “Gang of Four”
    book, I will focus on practical, informal data design patterns that have been
    used in production environments.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的设计模式书籍是由 Erich Gamma、Richard Helm、Ralph Johnson 和 John Vlissides（被称为“四人帮”）编写的经典计算机科学书籍
    *设计模式：可复用面向对象软件的基础*。与“四人帮”书中类似的数据设计模式不同，我将专注于实际的、非正式的生产环境中使用的数据设计模式。
- en: The data design patterns that we’ll cover in this chapter can help us to write
    scalable solutions to be deployed on Spark clusters. However, be aware that when
    it comes to adopting and using design patterns, there is no silver bullet. Every
    pattern should be tested for performance and scalability using real data, in an
    environment similar to your production environment.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们将要讨论的数据设计模式可以帮助我们编写可部署在 Spark 集群上的可扩展解决方案。然而，要注意的是，在采用和使用设计模式时，并没有银弹。每个模式都应该使用真实数据在类似于生产环境的环境中进行性能和可扩展性测试。
- en: Note
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'For a general introduction to design patterns in software engineering, see
    the aforementioned *Design Patterns: Elements of Reusable Object-Oriented Software*
    by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides (Addison-Wesley).
    To learn more about design patterns in MapReduce, see [*MapReduce Design Patterns*](https://www.oreilly.com/library/view/mapreduce-design-patterns/9781449341954/)
    by Donald Miner and Adam Shook and my book [*Data Algorithms*](https://www.oreilly.com/library/view/data-algorithms/9781491906170/)
    (both published by O’Reilly).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 关于软件工程中设计模式的一般介绍，请参阅前面提到的 *设计模式：可复用面向对象软件的基础*，作者是 Erich Gamma、Richard Helm、Ralph
    Johnson 和 John Vlissides（Addison-Wesley 出版）。要了解更多关于 MapReduce 中设计模式的信息，请参阅 [*MapReduce
    设计模式*](https://www.oreilly.com/library/view/mapreduce-design-patterns/9781449341954/)，作者是
    Donald Miner 和 Adam Shook，以及我的书籍 [*数据算法*](https://www.oreilly.com/library/view/data-algorithms/9781491906170/)（均由
    O'Reilly 出版）。
- en: 'The design patterns that I will cover in this chapter include:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在本章中介绍的设计模式包括：
- en: In-mapper combining
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部映射器组合
- en: Top-10
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前10名
- en: MinMax
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大最小
- en: The composite pattern/monoids
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复合模式/单子
- en: Binning
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分箱
- en: Sorting
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排序
- en: We’ll start with a useful summarization design pattern, using an in-mapper combiner.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个有用的总结设计模式开始，使用一个内部映射器组合器。
- en: In-Mapper Combining
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在内部映射器组合中
- en: 'In the MapReduce paradigm, a combiner (also known as a semi-reducer) is a process
    that runs locally on each worker to aggregate data before it’s sent across the
    network to the reducer(s). In frameworks like Hadoop, this is typically viewed
    as an optional local optimization. An *in-mapper combiner* performs a further
    optimization by performing the aggregation in memory as it receives each (key,
    value) pair from the mapper, rather than writing them all to to the local disk
    and then aggregating the values by key. (Spark performs all its processing in
    memory, so this is how it operates by default.) The aim of the in-mapper combining
    design pattern is for the mapper to efficiently combine and summarize its output
    as much as possible, so it emits fewer intermediate (key, value) pairs for the
    sort and shuffle and reducers (such as `reduceByKey()` or `groupByKey()`) to handle.
    For example, for a classic word count problem, given an input record such as:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MapReduce 范式中，组合器（也称为半减少器）是在每个工作节点上本地运行的过程，用于在数据发送到减少器之前对数据进行聚合。在像 Hadoop
    这样的框架中，这通常被视为一个可选的本地优化。*In-mapper combiner* 执行进一步的优化，通过在收到每个（键，值）对时在内存中执行聚合，而不是将它们全部写入本地磁盘，然后按键聚合值。
    （Spark 在内存中执行所有处理，因此默认情况下是这样操作的。）In-mapper 组合设计模式的目的是使 Mapper 能够尽可能高效地组合和总结其输出，以便在排序、洗牌和减少器（如
    `reduceByKey()` 或 `groupByKey()`）处理时发出更少的中间（键，值）对。例如，对于经典的单词计数问题，给定如下输入记录：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'without using the in-mapper combining design pattern we would generate the
    following (key, value) pairs to send to the reducers:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不使用 In-mapper 组合设计模式，我们将生成以下（键，值）对发送到减少器：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The problem is that for a very large dataset this approach would generate a
    huge number of `(word, 1)` pairs, which would be inefficient and keep the cluster
    network too busy. Using the in-mapper combining design pattern, we aggregate the
    data by key before sending it across the network, summarizing and reducing the
    mapper’s output before the shuffle is performed. For example, as in this case
    there are three instances of `(fox, 1)` and two instances of `(jumped, 1)`, these
    (key, value) pairs will be combined into the following output:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，对于非常大的数据集，这种方法会生成大量的 `(单词, 1)` 对，这样做效率低下，并且会使集群网络非常忙碌。使用 In-mapper 组合设计模式，我们在数据发送到网络之前通过键进行聚合，汇总和减少
    Mapper 的输出，在执行洗牌之前完成这一过程。例如，在这种情况下有三个 `(fox, 1)` 实例和两个 `(jumped, 1)` 实例，这些（键，值）对将组合成以下输出：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: While the reduction is not significant in this toy example, if we have a large
    dataset with a lot of repeated words this design pattern can help us to achieve
    significantly better performance by generating far fewer intermediate (key, value)
    pairs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在这个简单的示例中减少不显著，但如果我们有一个包含大量重复单词的大型数据集，这种设计模式可以通过生成更少的中间（键，值）对显著提升性能。
- en: 'To further demonstrate the concept behind this design pattern, the following
    sections will present three solutions to the problem of counting the frequency
    of characters in a set of documents. In simple terms, we want to find how many
    times each unique character appears in a given corpus. We will discuss the following
    solutions:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步演示这种设计模式背后的概念，接下来的部分将介绍三种解决方案来计算一组文档中每个字符出现频率的问题。简单来说，我们想要找出每个唯一字符在给定语料库中出现的次数。我们将讨论以下解决方案：
- en: Basic MapReduce algorithm
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本的 MapReduce 算法
- en: In-mapper combining per record
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每条记录的 In-mapper 组合
- en: In-mapper combining per partition
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区内的 In-mapper 组合
- en: Basic MapReduce Algorithm
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本的 MapReduce 算法
- en: To count the characters in a set of documents, we split each input record into
    a set of words, split each word into a character array, then (key, value) pairs
    where the key is a single character from the character array and the value is
    1 (the frequency count of one character). This is the basic MapReduce design pattern
    that does not use any custom data types, and the reducer simply sums up the frequencies
    of each single unique character. The problem with this solution is that for large
    datasets it will emit a large number of (key, value) pairs, which can overload
    the network and harm the performance of the overall solution. The large number
    of (key, value) pairs emitted can also slow down the sort and shuffle phase (grouping
    values for the same key). The different stages of this approach are illustrated
    in [Figure 10-1](#character_count_basic_mapreduce_algorithm).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算一组文档中的字符数，我们将每个输入记录拆分为一组单词，将每个单词拆分为字符数组，然后生成(key, value)对，其中key是字符数组中的单个字符，value为1（表示一个字符的频率计数）。这是一种基本的MapReduce设计模式，不使用任何自定义数据类型，而reducer简单地求和每个单一唯一字符的频率。这种解决方案的问题在于，对于大数据集，它会生成大量的(key,
    value)对，这可能会导致网络负载过大，从而影响整体解决方案的性能。生成的大量(key, value)对还可能减慢排序和洗牌阶段（对相同key的值进行分组）。这种方法的不同阶段如[图10-1](#character_count_basic_mapreduce_algorithm)所示。
- en: '![daws 1002](Images/daws_1002.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![daws 1002](Images/daws_1002.png)'
- en: 'Figure 10-1\. Character count: basic MapReduce algorithm'
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1\. 字符计数：基本的MapReduce算法
- en: Given an `RDD[String]`, a PySpark implementation of this algorithm is provided
    next.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个`RDD[String]`，下面提供了该算法的PySpark实现。
- en: 'First we define a simple function that accepts a single `String` record and
    returns a list of (key, value) pairs, where the key is a character and the value
    is 1 (the frequency of that character):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们定义一个简单的函数，接受单个`String`记录，并返回(key, value)对列表，其中key是字符，value是1（该字符的频率）：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO1-1)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO1-1)'
- en: Tokenize a record into an array of words.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 将记录标记为单词数组。
- en: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO1-2)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO1-2)'
- en: Create an empty list as `pairs`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个空列表作为`pairs`。
- en: '[![3](Images/3.png)](#co_practical_data_design_patterns_CO1-3)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_practical_data_design_patterns_CO1-3)'
- en: Iterate over words.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代每个单词。
- en: '[![4](Images/4.png)](#co_practical_data_design_patterns_CO1-4)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_practical_data_design_patterns_CO1-4)'
- en: Iterate over a single word.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历单个单词。
- en: '[![5](Images/5.png)](#co_practical_data_design_patterns_CO1-5)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_practical_data_design_patterns_CO1-5)'
- en: Add each character (`c`) as `(c, 1)` to `pairs`.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个字符(`c`)作为`(c, 1)`添加到`pairs`中。
- en: '[![6](Images/6.png)](#co_practical_data_design_patterns_CO1-6)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_practical_data_design_patterns_CO1-6)'
- en: Return a list of `(c, 1)` for all characters in a given record.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 返回给定记录中所有字符的`(c, 1)`列表。
- en: 'This `mapper()` function can be simplified as:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`mapper()`函数可以简化为：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we use the `mapper()` function to count the frequencies of unique characters:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`mapper()`函数来计算唯一字符的频率：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO2-1)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO2-1)'
- en: Create an `RDD[String]` from the input data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入数据创建一个`RDD[String]`。
- en: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO2-2)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO2-2)'
- en: Map each record into a collection of characters and flatten it as a new `RDD[Character,
    1]`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 将每条记录映射为一组字符，并将其展开为新的`RDD[Character, 1]`。
- en: '[![3](Images/3.png)](#co_practical_data_design_patterns_CO2-3)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_practical_data_design_patterns_CO2-3)'
- en: Find the frequency of each unique character.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 查找每个唯一字符的频率。
- en: Next, we’ll look at a more efficient implementation that uses the in-mapper
    combining design pattern.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍一个更高效的实现，它使用内映射器组合设计模式。
- en: In-Mapper Combining per Record
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 记录级别的内映射器组合
- en: 'This section introduces the in-mapper combining per record design pattern,
    also known as local aggregation per record solution. It’s similar to the basic
    Spark MapReduce algorithm, with the exception that for a given input record we
    aggregate frequencies for each character before emitting (key, value) pairs. In
    other words, in this solution we emit (key, value) pairs where the key is a unique
    character within a given input record and the value is the aggregated frequency
    of that character within that record. Then we use `reduceByKey()` to aggregate
    all the frequencies for each unique character. This solution uses local aggregation
    by leveraging the associativity and commutativity of the `reduce()` function to
    combine values before sending them across the network. For example, for the following
    input record:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了每条记录的In-Mapper Combining设计模式，也称为本地聚合每条记录解决方案。它类似于基本的Spark MapReduce算法，唯一的区别是对于给定的输入记录，我们在发射`(key,
    value)`对之前聚合每个字符的频率。换句话说，在这个解决方案中，我们发射`(key, value)`对，其中key是给定输入记录中的唯一字符，value是该记录中该字符的累计频率。然后我们使用`reduceByKey()`来聚合每个唯一字符的所有频率。该解决方案利用`reduce()`函数的结合性和交换性，在发送到网络之前将值组合起来。例如，对于以下的输入记录：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'we will emit the following (key, value) pairs:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将发射以下的`(key, value)`对：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Given an `RDD[String]`, a PySpark solution is provided next.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个`RDD[String]`，下面提供了一个PySpark解决方案。
- en: 'First we define a simple function that accepts a single `String` record (an
    element of the input `RDD[String]`) and returns a list of (key, value) pairs,
    where the key is a unique character and the value is the aggregated frequency
    of that character:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义一个简单的函数，接受一个单个的`String`记录（输入`RDD[String]`的一个元素），并返回一个`(key, value)`对列表，其中key是一个唯一的字符，value是该字符的累计频率：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO3-1)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO3-1)'
- en: The `collections` module provides high-performance container data types.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`collections`模块提供高性能的容器数据类型。'
- en: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO3-2)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO3-2)'
- en: Create an empty `dict[String, Integer]`. `defaultdict` is a `dict` subclass
    that calls a factory function to supply missing values.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个空的`dict[String, Integer]`。`defaultdict`是一个`dict`的子类，它调用一个工厂函数来提供缺失的值。
- en: '[![3](Images/3.png)](#co_practical_data_design_patterns_CO3-3)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_practical_data_design_patterns_CO3-3)'
- en: Tokenize the input record into an array of words.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入记录分词为一个单词数组。
- en: '[![4](Images/4.png)](#co_practical_data_design_patterns_CO3-4)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_practical_data_design_patterns_CO3-4)'
- en: Iterate over words.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历单词。
- en: '[![5](Images/5.png)](#co_practical_data_design_patterns_CO3-5)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_practical_data_design_patterns_CO3-5)'
- en: Iterate over each word.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历每个单词。
- en: '[![6](Images/6.png)](#co_practical_data_design_patterns_CO3-6)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_practical_data_design_patterns_CO3-6)'
- en: Aggregate characters.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合字符。
- en: '[![7](Images/7.png)](#co_practical_data_design_patterns_CO3-7)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_practical_data_design_patterns_CO3-7)'
- en: Flatten the dictionary into a list of `(character, frequency)`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 将字典扁平化为`(character, frequency)`列表。
- en: '[![8](Images/8.png)](#co_practical_data_design_patterns_CO3-8)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[![8](Images/8.png)](#co_practical_data_design_patterns_CO3-8)'
- en: Return the flattened list of `(character, frequency)`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 返回扁平化的`(character, frequency)`列表。
- en: 'Next, we use the `local_aggregator()` function to count the frequencies of
    unique characters:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`local_aggregator()`函数来计算唯一字符的频率：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This solution will emit many fewer (key, value) pairs, which is an improvement
    over the previous solution. This means there will be less load on the network
    and the sort and shuffle will execute faster than with the basic algorithm. However,
    there’s still a potential problem with this implementation: although it will scale
    out if we don’t have too many mappers, because we instantiate and use a dictionary
    per mapper, if we have a lot of mappers we might run into OOM errors.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案将发射较少的`(key, value)`对，这比之前的解决方案更好。这意味着网络负载较小，排序和洗牌的执行速度比基本算法更快。然而，这个实现仍然存在潜在的问题：虽然它会在不太多的映射器情况下扩展，因为我们在每个映射器中实例化和使用一个字典，如果映射器数量很多，我们可能会遇到内存溢出错误。
- en: Next, I’ll present another version of this design pattern that avoids this problem
    and is even more efficient.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将展示这一设计模式的另一个版本，避免了这个问题并且更加高效。
- en: In-Mapper Combining per Partition
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分区内的In-Mapper Combining
- en: This final solution aggregates the frequencies of each character per partition
    (rather than per record) of input data, where each partition may be comprised
    of thousands or millions of input records. To do this, we will again build a hash
    table of `dict[Character, Integer]`, but this time for the characters of a given
    input partition instead of an input record. The mapper will then emit (key, value)
    pairs comprised of entries from the hash table, where the key is `dict.Entry.getKey()`
    and the value is `dict.Entry.getValue()`. This is a very compact data representation,
    since every entry of `dict[Character, Integer]` is equivalent to *`N`* basic (key,
    value) pairs, where *`N`* is equal to `dict.Entry.getValue()`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此最终解决方案会聚合输入数据每个分区（而不是每个记录）中每个字符的频率，其中每个分区可能包含数千或数百万条输入记录。为此，我们再次构建一个`dict[Character,
    Integer]`的哈希表，但这次是针对给定输入分区的字符而不是输入记录。映射器然后会发出由哈希表条目组成的（键，值）对，其中键是`dict.Entry.getKey()`，值是`dict.Entry.getValue()`。这是一种非常紧凑的数据表示，因为`dict[Character,
    Integer]`的每个条目相当于*N*个基本的（键，值）对，其中*N*等于`dict.Entry.getValue()`。
- en: So, in this solution we use a single hash table per input partition to keep
    track of the frequencies of all characters in all records in that partition. After
    the mapper finishes processing the partition (using PySpark’s `mapPartitions()`
    transformation), we emit all the (key, value) pairs from the frequencies table
    (the hash table we built). Then the reducers will sum up the frequencies from
    all the partitions and find the final count of characters. This solution is more
    efficient than the previous two because it emits even fewer (key, value) pairs,
    resulting in even less load on the network and less work for the sort and shuffle
    phase. It will also scale out better than the previous two solutions, because
    using a single hash table per input partition eliminates the risk of OOM problems.
    Even if we partition our input into thousands of partitions, this solution scales
    out very well.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这个解决方案中，我们使用每个输入分区一个哈希表来跟踪该分区所有记录中所有字符的频率。在映射器完成处理分区后（使用 PySpark 的 `mapPartitions()`
    转换），我们发出频率表（我们构建的哈希表）中的所有（键，值）对。然后减少器将汇总所有分区的频率，并找出字符的最终计数。这个解决方案比前两种更有效，因为它发出的（键，值）对更少，从而减少了网络负载和排序和洗牌阶段的工作量。它也比前两种解决方案更好地扩展，因为使用每个输入分区一个哈希表消除了内存溢出问题的风险。即使我们将输入分区划分成成千上万个分区，这个解决方案也可以很好地扩展。
- en: 'As an example, for the following input partition (as opposed to a single record):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于以下输入分区（而不是单个记录）：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'we will emit the following (key, value) pairs:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将会发出以下（键，值）对：
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: One consideration when using this design pattern is that you need to be careful
    about the size of the hash table, to be sure it will not cause bottlenecks. For
    the character count problem, the size of the hash table for each mapper (per input
    partition) will be very small (since we have a limited number of unique characters),
    so there’s no danger of a performance bottleneck.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用这种设计模式时需要考虑的一个因素是，你需要小心哈希表的大小，确保它不会成为瓶颈。对于字符计数问题，每个映射器（每个输入分区）的哈希表大小将非常小（因为我们只有有限数量的唯一字符），所以不会有性能瓶颈的危险。
- en: Given an `RDD[String]`, the PySpark solution is provided next.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个`RDD[String]`，PySpark 解决方案如下所示。
- en: 'First we define a simple function that accepts a single input partition (comprised
    of many input records) and returns a list of (key, value) pairs, where the key
    is a character and the value is the aggregated frequency of that character:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们定义一个简单的函数，接受一个由许多输入记录组成的单个输入分区，并返回一个（键，值）对列表，其中键是字符，值是该字符的聚合频率：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO4-1)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO4-1)'
- en: '`partition_iterator` represents a single input partition comprised of a set
    of records.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`partition_iterator` 表示由一组记录组成的单个输入分区。'
- en: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO4-2)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO4-2)'
- en: Create an empty `dict[String, Integer]`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个空的`dict[String, Integer]`。
- en: '[![3](Images/3.png)](#co_practical_data_design_patterns_CO4-3)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_practical_data_design_patterns_CO4-3)'
- en: Get a single record from a partition.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个分区获取单个记录。
- en: '[![4](Images/4.png)](#co_practical_data_design_patterns_CO4-4)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_practical_data_design_patterns_CO4-4)'
- en: Tokenize the record into an array of words.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 将记录标记为单词数组。
- en: '[![5](Images/5.png)](#co_practical_data_design_patterns_CO4-5)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_practical_data_design_patterns_CO4-5)'
- en: Iterate over words.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历单词。
- en: '[![6](Images/6.png)](#co_practical_data_design_patterns_CO4-6)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_practical_data_design_patterns_CO4-6)'
- en: Iterate over each word.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历每个单词。
- en: '[![7](Images/7.png)](#co_practical_data_design_patterns_CO4-7)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_practical_data_design_patterns_CO4-7)'
- en: Aggregate characters.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合字符。
- en: '[![8](Images/8.png)](#co_practical_data_design_patterns_CO4-8)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[![8](Images/8.png)](#co_practical_data_design_patterns_CO4-8)'
- en: Flatten the dictionary into a list of `(character, frequency)`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 将字典扁平化为`(character, frequency)`列表。
- en: '[![9](Images/9.png)](#co_practical_data_design_patterns_CO4-9)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[![9](Images/9.png)](#co_practical_data_design_patterns_CO4-9)'
- en: Return the flattened list of `(character, frequency)`.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 返回扁平化的`(character, frequency)`列表。
- en: 'Next, we use the `inmapper_combiner()` function to count frequencies of unique
    characters:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`inmapper_combiner()`函数来计算唯一字符的频率：
- en: '[PRE13]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO5-1)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO5-1)'
- en: Create an `RDD[String]` from input file(s).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入文件(s)创建一个`RDD[String]`。
- en: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO5-2)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO5-2)'
- en: The `mapPartitions()` transformation returns a new RDD by applying a function
    to each input partition (as opposed to a single input record) of this RDD.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapPartitions()`转换通过将函数应用于该RDD的每个输入分区（而不是单个输入记录）来返回一个新的RDD。'
- en: This solution will emit far fewer (key, value) pairs than the previous ones.
    It’s quite efficient, since we instantiate and use a single dictionary per input
    partition (rather than per input record). This greatly reduces the amount of data
    that needs to be transferred between the mappers and reducers, easing the load
    on the network and speeding up the sort and shuffle phase. The in-mapper combining
    algorithm makes good use of combiners as optimizers, and this solution scales
    out extremely well. Even if we have a large number of mappers, this will not cause
    OOM errors. However, it should be noted that this algorithm may run into a problem
    if the number of unique keys grows too large for the associative array to fit
    in memory, as memory paging will significantly affect performance. If this is
    the case, you will have to revert to the basic MapReduce approach.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这种解决方案将比先前的解决方案发出远少于的(key, value)对。它非常高效，因为我们在每个输入分区中实例化和使用一个单一的字典（而不是每个输入记录）。这极大地减少了需要在mapper和reducer之间传输的数据量，减轻了网络负载并加快了排序和洗牌阶段的速度。内映射合并算法充分利用组合器作为优化器，这种解决方案极好地扩展了。即使有大量的mapper，也不会导致OOM错误。然而，应该注意的是，如果唯一键的数量增长到关联数组无法适应内存的大小，内存分页将显著影响性能。如果出现这种情况，您将不得不恢复到基本的MapReduce方法。
- en: In implementing the in-mapper combining per partition design pattern, we use
    Spark’s powerful `mapPartitions()` transformation to transform each input partition
    into a single `dict[Character, Integer]`, and then we aggregate these into a single
    final `dict[Character, Integer]`. For character counting and other applications
    where you want to extract a small amount of information from a large dataset,
    this algorithm is efficient and faster than other approaches. In the case of the
    character counting problem, the size of the associative array (per mapper partition)
    is bounded by the number of unique characters, so there are no scalability bottlenecks
    when using this design pattern.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现按分区设计模式的内映射合并时，我们使用Spark强大的`mapPartitions()`转换将每个输入分区转换为一个单一的`dict[Character,
    Integer]`，然后将这些聚合成一个最终的单一的`dict[Character, Integer]`。对于字符计数及其他需要从大数据集中提取少量信息的应用，该算法比其他方法更高效快速。在字符计数问题中，关联数组（每个mapper分区）的大小受限于唯一字符的数量，因此在使用这种设计模式时不会遇到可扩展性瓶颈。
- en: 'To recap, the in-mapper combining per partition design pattern offers several
    important advantages in terms of efficiency and scalability:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '总结一下，在按分区设计模式的内映射合并方案中，在效率和可扩展性方面提供了几个重要优势：  '
- en: Greatly reduces the number of (key, value) pairs emitted
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大大减少了发出的(key, value)对的数量
- en: Requires far less sorting and shuffling of (key, value) pairs
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要较少的(key, value)对的排序和洗牌
- en: Makes good use of combiners as optimizers
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 充分利用组合器作为优化器
- en: Scales out very well
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 很好地扩展了
- en: 'But there are also a few disadvantages to be aware of:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 但也需要注意一些缺点：
- en: More difficult to implement (requires custom functions for handling each partition)
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更难实现（需要处理每个分区的自定义函数）
- en: Underlying object (per mapper partition) is more heavyweight
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 底层对象（每个mapper分区）更加重量级
- en: Fundamental limitation in terms of size of the underlying object (for the character
    count problem, an associative array per mapper partition)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础限制是底层对象的大小（对于字符计数问题，每个映射器分区使用一个关联数组）
- en: Next, we’ll look at a few other common use cases where you want to extract a
    small amount of information from a large dataset, and see what the best approaches
    are.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一下其他几个常见的用例，您希望从大型数据集中提取少量信息，并查看最佳方法是什么。
- en: Top-10
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前10
- en: 'Creating a top-10 list is a common task in many data-intensive operations.
    For example, we might ask the following questions:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个前10列表在许多数据密集型操作中是一个常见的任务。例如，我们可能会问以下问题：
- en: What were the top 10 URLs visited during the last day/week/month?
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在过去的一天/一周/一个月内访问的前10个URL是哪些？
- en: What were the top 10 items purchased from Amazon during the last day/week/month?
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在过去的一天/一周/一个月内，亚马逊上购买的前10件物品是什么？
- en: What were the top 10 search queries on Google in the last day/week/month?
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在过去的一天/一周/一个月内，Google上前10个搜索查询是什么？
- en: What were the top 10 most liked items on Facebook yesterday?
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 昨天Facebook上最受欢迎的前10个物品是什么？
- en: What are the top 10 cartoons of all time?
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有史以来最受欢迎的前10个卡通是什么？
- en: A simple design pattern for answering these kinds of questions is illustrated
    in [Figure 10-2](#top_10_design_pattern).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 用于回答这些问题的简单设计模式在[图10-2](#top_10_design_pattern)中说明。
- en: '![daws 1003](Images/daws_1003.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![daws 1003](Images/daws_1003.png)'
- en: Figure 10-2\. The top-10 design pattern
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-2。前10设计模式
- en: 'For example, say we have a table with two columns, `url` and `frequency`. Finding
    the top 10 most visited URLs with a SQL query is straightforward:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个包含两列`url`和`frequency`的表。使用SQL查询查找最受欢迎的前10个访问量最高的URL很简单：
- en: '[PRE14]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finding the top *N* (where *N* > 0) records in Spark is also easy. Given an
    `RDD[(String, Integer)]` where the key is a string representing a URL and the
    value is the frequency of visits to that URL, we can use `RDD.takeOrdered(*N*)`
    to find the top-*N* list. The general format of `RDD.takeOrdered()` is:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中查找前*N*（其中*N* > 0）条记录也很容易。给定一个`RDD[(String, Integer)]`，其中键是表示URL的字符串，值是访问该URL的频率，我们可以使用`RDD.takeOrdered(*N*)`来找到前*N*列表。`RDD.takeOrdered()`的一般格式是：
- en: '[PRE15]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Assuming that *N* is an integer greater than 0, we have various options for
    finding a top-N list efficiently with `RDD.takeOrdered()`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 假设*N*是大于0的整数，我们有多种选项可以有效地找到一个顶级-N列表，使用`RDD.takeOrdered()`：
- en: '[PRE16]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: For the sake of argument, let’s assume that `takeOrdered()` does not perform
    optimally with large datasets. What other options do we have?
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，假设`takeOrdered()`在大型数据集中表现不佳。我们还有哪些其他选择？
- en: 'Given a large set of (key, value) pairs where the key is a `String` and the
    value is an `Integer`, we want an independent, reusable solution to the problem
    of finding the top *N* keys (where *N* > 0)—that is, a design pattern that enables
    us to produce reusable code to answer questions like the ones mentioned earlier
    when working with big data. This kind of question is common for data consisting
    of (key, value) pairs. This is essentially a filtering task: you filter out unwanted
    data and keep just the top *N* items. The top 10 function is also a function that
    is commutative and associative, and therefore using partitioners, combiners, and
    reducers will always produce correct results.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个大量的（键，值）对集合，其中键是一个`String`，值是一个`Integer`，我们希望找到一个独立的、可重用的解决方案来解决找到前*N*个键（其中*N*
    > 0）的问题——即一种设计模式，使我们能够生成可重用的代码来回答像前面提到的那些问题，当处理大数据时。这种问题对于由（键，值）对组成的数据很常见。这本质上是一个过滤任务：您过滤掉不需要的数据，只保留前*N*个项目。前10函数也是可交换和可结合的函数，因此使用分区器、合并器和减少器将始终产生正确的结果。
- en: 'That is, given a top-10 function `T` and a set of values (such as frequencies)
    `{a, b, c}` for the same key, then we can write:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，给定一个顶级-10函数`T`和一组值（例如频率）`{a, b, c}`对于相同的键，然后我们可以写：
- en: Commutative
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可交换
- en: '[PRE17]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Associative
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可结合
- en: '[PRE18]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Tip
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: For additional details on the top-10 list design pattern, refer to [*MapReduce
    Design Patterns*](https://www.oreilly.com/library/view/mapreduce-design-patterns/9781449341954/)
    by Donald Miner and Adam Shook.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 有关前10列表设计模式的详细信息，请参考Donald Miner和Adam Shook的《*MapReduce设计模式*》（https://www.oreilly.com/library/view/mapreduce-design-patterns/9781449341954/）。
- en: This section provides a complete PySpark solution for the top-10 design pattern.
    Given an `RDD[(String, Integer)]`, the goal is to find the top-10 list for that
    RDD. In our solution, we assume that all keys are unique. If the keys are not
    unique, then you may use the `reduceByKey()` transformation (before finding the
    top-10) to make them unique.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了一个完整的 PySpark 解决方案，用于 top-10 设计模式。给定一个 `RDD[(String, Integer)]`，目标是找到该
    RDD 的前 10 名列表。在我们的解决方案中，我们假设所有键都是唯一的。如果键不唯一，则可以在找到前 10 名之前使用 `reduceByKey()` 转换使它们唯一。
- en: Our solution will generalize the problem and will be able to find a top-*N*
    list (for *N* > 0). For example, we will be able to find the top 10 cats, top
    50 most visited websites, or top 100 search queries.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解决方案将推广问题并能够找到一个 top-*N* 列表（对于 *N* > 0）。例如，我们将能够找到前 10 名猫、前 50 名最访问的网站或前
    100 个搜索查询。
- en: Top-N Formalized
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正式化的 Top-N
- en: 'Let’s start by formalizing the problem. Let *N* be an integer number greater
    than 0\. Let `L` be a list of pairs of `(T, Integer)`, where `T` can be any type
    (such as `String`, `Integer`, etc.), `L.size() = s`, and `s > N`. The elements
    of `L` are:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始正式化这个问题。让 *N* 是大于 0 的整数。让 `L` 是一个 `(T, Integer)` 对的列表，其中 `T` 可以是任何类型（如
    `String`、`Integer` 等），`L.size() = s`，且 `s > N`。`L` 的元素是：
- en: <math alttext="StartSet left-parenthesis upper K Subscript i Baseline comma
    upper V Subscript i Baseline right-parenthesis comma i equals 1 comma 2 comma
    period period period comma s EndSet" display="block"><mrow><mo>{</mo> <mrow><mo>(</mo>
    <msub><mi>K</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>V</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>,</mo> <mi>i</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mn>2</mn>
    <mo>,</mo> <mo>.</mo> <mo>.</mo> <mo>.</mo> <mo>,</mo> <mi>s</mi> <mo>}</mo></mrow></math>
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartSet left-parenthesis upper K Subscript i Baseline comma
    upper V Subscript i Baseline right-parenthesis comma i equals 1 comma 2 comma
    period period period comma s EndSet" display="block"><mrow><mo>{</mo> <mrow><mo>(</mo>
    <msub><mi>K</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>V</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>,</mo> <mi>i</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mn>2</mn>
    <mo>,</mo> <mo>.</mo> <mo>.</mo> <mo>.</mo> <mo>,</mo> <mi>s</mi> <mo>}</mo></mrow></math>
- en: 'where *K[i]* has a data type of `T` and *V[i]* is an `Integer` type (this is
    the frequency of *K[i]*). Let `{sort(L)}` be a sorted list where the sorting is
    done by using frequency as a key. This gives us:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *K[i]* 的数据类型为 `T`，*V[i]* 是 `Integer` 类型（这是 *K[i]* 的频率）。让 `{sort(L)}` 是一个排序列表，排序是通过使用频率作为键来完成的。这给了我们：
- en: <math alttext="StartSet left-parenthesis upper A Subscript j Baseline comma
    upper B Subscript j Baseline right-parenthesis comma 1 less-than-or-equal-to j
    less-than-or-equal-to upper S comma upper B 1 greater-than-or-equal-to upper B
    2 greater-than-or-equal-to period period period upper B Subscript s Baseline EndSet"
    display="block"><mrow><mo>{</mo> <mrow><mo>(</mo> <msub><mi>A</mi> <mi>j</mi></msub>
    <mo>,</mo> <msub><mi>B</mi> <mi>j</mi></msub> <mo>)</mo></mrow> <mo>,</mo> <mn>1</mn>
    <mo>≤</mo> <mi>j</mi> <mo>≤</mo> <mi>S</mi> <mo>,</mo> <msub><mi>B</mi> <mn>1</mn></msub>
    <mo>≥</mo> <msub><mi>B</mi> <mn>2</mn></msub> <mo>≥</mo> <mo>.</mo> <mo>.</mo>
    <mo>.</mo> <mo>≥</mo> <msub><mi>B</mi> <mi>s</mi></msub> <mo>}</mo></mrow></math>
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartSet left-parenthesis upper A Subscript j Baseline comma
    upper B Subscript j Baseline right-parenthesis comma 1 less-than-or-equal-to j
    less-than-or-equal-to upper S comma upper B 1 greater-than-or-equal-to upper B
    2 greater-than-or-equal-to period period period upper B Subscript s Baseline EndSet"
    display="block"><mrow><mo>{</mo> <mrow><mo>(</mo> <msub><mi>A</mi> <mi>j</mi></msub>
    <mo>,</mo> <msub><mi>B</mi> <mi>j</mi></msub> <mo>)</mo></mrow> <mo>,</mo> <mn>1</mn>
    <mo>≤</mo> <mi>j</mi> <mo>≤</mo> <mi>S</mi> <mo>,</mo> <msub><mi>B</mi> <mn>1</mn></msub>
    <mo>≥</mo> <msub><mi>B</mi> <mn>2</mn></msub> <mo>≥</mo> <mo>.</mo> <mo>.</mo>
    <mo>.</mo> <mo>≥</mo> <msub><mi>B</mi> <mi>s</mi></msub> <mo>}</mo></mrow></math>
- en: 'where *(A[j], B[j])* are in `L`. Then, the top-*N* of list `L` is defined as:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *(A[j], B[j])* 在 `L` 中。然后，列表 `L` 的 top-*N* 定义为：
- en: <math alttext="top hyphen upper N left-parenthesis upper L right-parenthesis
    equals StartSet left-parenthesis upper A Subscript j Baseline comma upper B Subscript
    j Baseline right-parenthesis comma 1 less-than-or-equal-to j less-than-or-equal-to
    upper N comma upper B 1 greater-than-or-equal-to upper B 2 greater-than-or-equal-to
    period period period upper B Subscript upper N Baseline greater-than-or-equal-to
    upper B Subscript upper N plus 1 Baseline greater-than-or-equal-to period period
    period upper B Subscript s Baseline EndSet" display="block"><mrow><mtext>top-N(L)</mtext>
    <mo>=</mo> <mo>{</mo> <mrow><mo>(</mo> <msub><mi>A</mi> <mi>j</mi></msub> <mo>,</mo>
    <msub><mi>B</mi> <mi>j</mi></msub> <mo>)</mo></mrow> <mo>,</mo> <mn>1</mn> <mo>≤</mo>
    <mi>j</mi> <mo>≤</mo> <mi>N</mi> <mo>,</mo> <msub><mi>B</mi> <mn>1</mn></msub>
    <mo>≥</mo> <msub><mi>B</mi> <mn>2</mn></msub> <mo>≥</mo> <mtext>...</mtext> <mo>≥</mo>
    <msub><mi>B</mi> <mi>N</mi></msub> <mo>≥</mo> <msub><mi>B</mi> <mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>≥</mo> <mtext>...</mtext> <mo>≥</mo> <msub><mi>B</mi> <mi>s</mi></msub> <mo>}</mo></mrow></math>
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="top hyphen upper N left-parenthesis upper L right-parenthesis
    equals StartSet left-parenthesis upper A Subscript j Baseline comma upper B Subscript
    j Baseline right-parenthesis comma 1 less-than-or-equal-to j less-than-or-equal-to
    upper N comma upper B 1 greater-than-or-equal-to upper B 2 greater-than-or-equal-to
    period period period upper B Subscript upper N Baseline greater-than-or-equal-to
    upper B Subscript upper N plus 1 Baseline greater-than-or-equal-to period period
    period upper B Subscript s Baseline EndSet" display="block"><mrow><mtext>top-N(L)</mtext>
    <mo>=</mo> <mo>{</mo> <mrow><mo>(</mo> <msub><mi>A</mi> <mi>j</mi></msub> <mo>,</mo>
    <msub><mi>B</mi> <mi>j</mi></msub> <mo>)</mo></mrow> <mo>,</mo> <mn>1</mn> <mo>≤</mo>
    <mi>j</mi> <mo>≤</mo> <mi>N</mi> <mo>,</mo> <msub><mi>B</mi> <mn>1</mn></msub>
    <mo>≥</mo> <msub><mi>B</mi> <mn>2</mn></msub> <mo>≥</mo> <mtext>...</mtext> <mo>≥</mo>
    <msub><mi>B</mi> <mi>N</mi></msub> <mo>≥</mo> <msub><mi>B</mi> <mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>≥</mo> <mtext>...</mtext> <mo>≥</mo> <msub><mi>B</mi> <mi>s</mi></msub> <mo>}</mo></mrow></math>
- en: 'For our top-*N* solution, we will use Python’s `SortedDict`, a sorted mutable
    mapping. The design of this type is simple: `SortedDict` inherits from `dict`
    to store items and maintains a sorted list of keys. Sorted dict keys must be hashable
    and comparable. The hash and total ordering of keys must not change while they
    are stored in the sorted dict.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的 top-*N* 解决方案，我们将使用 Python 的 `SortedDict`，一个有序的可变映射。这种类型的设计很简单：`SortedDict`
    继承自 `dict` 来存储项目，并维护一个键的排序列表。排序字典键必须是可散列的和可比较的。键的散列和总序在存储在排序字典中时不能更改。
- en: To implement top-*N*, we need a hash table data structure whose keys can have
    a total order, such as `SortedDict` (in our case, keys represent frequencies).
    The dictionary is ordered according to the natural ordering of its keys. We can
    build this structure with `sortedcontainer.SortedDict()`. We will keep adding
    `(frequency, url)` pairs to the `SortedDict`, but keep its size at *N*. When the
    size is *N*+1, we will pop out the smallest frequency using `SortedDict.popitem(0)`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现 top-*N*，我们需要一个哈希表数据结构，其键可以有总序，例如`SortedDict`（在我们的情况下，键表示频率）。该字典按其键的自然顺序排序。我们可以使用
    `sortedcontainer.SortedDict()` 构建这个结构。我们将继续向 `SortedDict` 添加 `(frequency, url)`
    对，但保持其大小为 *N*。当大小为 *N*+1 时，我们将使用 `SortedDict.popitem(0)` 弹出最小频率。
- en: 'An example of how this works is shown here:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了这个工作的一个例子：
- en: '[PRE19]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Next, I’ll present a top-10 solution using PySpark.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将介绍一个使用 PySpark 的 top-10 解决方案。
- en: PySpark Solution
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PySpark 解决方案
- en: The PySpark solution is pretty straightforward. We use the `mapPartitions()`
    transformation to find the local top *N* (where *N* > 0) for each partition, and
    pass these to a single reducer. The reducer then finds the final top-*N* list
    from all the local top-*N* lists passed from the mappers. In general, in most
    MapReduce algorithms having a single reducer is problematic and will cause a performance
    bottleneck if one reducer on one server receives all the data—potentially a very
    large volume—and all the other nodes are doing nothing, all the pressure and load
    will be on that one node, causing a bottleneck). However, in this case, our single
    reducer will not cause a performance problem. Why not? Let’s assume that we have
    5,000 partitions. Each mapper will only generate 10 (key, value) pairs, which
    means our single reducer will only get 50,000 records—a volume of data that’s
    not likely to cause performance bottleneck in a Spark cluster.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 解决方案非常直接。我们使用 `mapPartitions()` 转换来找到每个分区的本地 top *N*（其中 *N* > 0），并将这些传递给单个
    reducer。Reducer 然后从所有从 mappers 传递的本地 top *N* 列表中找到最终的 top-*N* 列表。通常，在大多数 MapReduce
    算法中，如果一个 reducer 在一个服务器上接收到所有数据，可能会导致性能瓶颈。然而，在这种情况下，我们的单个 reducer 不会引起性能问题。为什么不会呢？假设我们有
    5000 个分区。每个 mapper 只会生成 10 个（键，值）对，这意味着我们的单个 reducer 只会得到 50000 条记录——这样的数据量不太可能在
    Spark 集群中引起性能瓶颈。
- en: A high-level overview of the PySpark solution for the top-10 design pattern
    is presented in [Figure 10-3](#high_level_solution_for_top_10_design_pattern).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 关于PySpark解决方案中关于顶级设计模式的高级概述在[图 10-3](#high_level_solution_for_top_10_design_pattern)中提供。
- en: '![daws 1004](Images/daws_1004.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![daws 1004](Images/daws_1004.png)'
- en: Figure 10-3\. PySpark implementation of the Top-10 design pattern
  id: totrans-176
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-3\. PySpark中的顶级设计模式实现
- en: Input is partitioned into smaller chunks, and each chunk is sent to a mapper.
    Each mapper emits a local top-10 list to be sent to the reducer(s). Here, we use
    a single reducer key so that the output from all mappers will be consumed by a
    single reducer.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 输入被分割成较小的块，并且每个块被发送到一个mapper。每个mapper发出一个本地的前10名列表以供一个单一的reducer消费。在这里，我们使用单个reducer键，以便来自所有mapper的输出将被单个reducer消耗。
- en: 'Let `spark` be an instance of `SparkSession`. This is how the top-10 problem
    is solved by using the `mapPartitions()` transformation and a custom Python function
    named `top10_design_pattern()`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让`spark`成为`SparkSession`的一个实例。这就是通过使用`mapPartitions()`转换和一个名为`top10_design_pattern()`的自定义Python函数解决前10名问题的方式：
- en: '[PRE20]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To complete the implementation, here I’ll present the `top10_design_pattern()`
    function, which finds the top-10 for each partition (containing a set of (key,
    value) pairs):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成实现，这里我将介绍`top10_design_pattern()`函数，该函数查找每个分区（包含一组(key, value)对）的前10名：
- en: '[PRE21]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO6-1)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO6-1)'
- en: '`partition_iterator` is an iterator for a single partition; it iterates over
    a set of `(URL, frequency)` pairs.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`partition_iterator`是单个分区的迭代器；它迭代一组`(URL, frequency)`对。'
- en: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO6-2)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO6-2)'
- en: Create an empty `SortedDict` of `(Integer, String)` pairs.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个空的`(Integer, String)`对的`SortedDict`。
- en: '[![3](Images/3.png)](#co_practical_data_design_patterns_CO6-3)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_practical_data_design_patterns_CO6-3)'
- en: Iterate over a set of `(URL, frequency)` pairs.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代一组`(URL, frequency)`对。
- en: '[![4](Images/4.png)](#co_practical_data_design_patterns_CO6-4)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_practical_data_design_patterns_CO6-4)'
- en: Put the `(frequency, URL)` pairs into a `SortedDict`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 将`(frequency, URL)`对放入`SortedDict`中。
- en: '[![5](Images/5.png)](#co_practical_data_design_patterns_CO6-5)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_practical_data_design_patterns_CO6-5)'
- en: Limit the size of the sorted dictionary to 10 (remove the lowest frequencies).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 限制排序字典的大小为10（删除最低频率的条目）。
- en: '[![6](Images/6.png)](#co_practical_data_design_patterns_CO6-6)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_practical_data_design_patterns_CO6-6)'
- en: Convert the `SortedDict` (which is a local top-10 list) into a list of `(k,
    v)` pairs.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 将`SortedDict`（这是一个本地前10名列表）转换为`(k, v)`对的列表。
- en: '[![7](Images/7.png)](#co_practical_data_design_patterns_CO6-7)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_practical_data_design_patterns_CO6-7)'
- en: Return a local top-10 list for a single partition.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 返回单个分区的本地前10名列表。
- en: Each mapper accepts a partition of elements, where each element is a pair of
    `(URL, frequency)`. The number of partitions is typically determined by the data
    size and the available resources in the cluster (nodes, cores, memory, etc.),
    or it can be set explicitly by the programmer. After the mapper finishes creating
    its local top-10 list as a `SortedDict[Integer, String]`, the function returns
    that list. Note that we use a single dictionary (such as a `SortedDict`) per partition,
    and not per element of the source RDD. As described in [“In-Mapper Combining per
    Partition”](#in_mapper_combining), this greatly improves the efficiency of the
    operation by reducing the network load and the work to be done in the sort and
    shuffle phase.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 每个mapper接受元素的分区，其中每个元素都是`(URL, frequency)`对。分区的数量通常由数据大小和集群中可用资源（节点、核心、内存等）决定，或者可以由程序员明确设置。在mapper完成创建其本地的`SortedDict[Integer,
    String]`作为单个字典（例如`SortedDict`）的本地前10名列表后，该函数返回该列表。请注意，我们每个分区使用单个字典（如`SortedDict`），而不是源RDD的每个元素。正如[“在Mapper中的每个分区组合”](#in_mapper_combining)所描述的那样，通过减少网络负载以及在排序和洗牌阶段要做的工作，这显著提高了操作的效率。
- en: A complete solution using `mapPartitions()` is provided in the book’s GitHub
    repository.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 书的GitHub存储库中提供了使用`mapPartitions()`的完整解决方案。
- en: Finding the Bottom 10
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找底部的10个
- en: 'In the previous section, I showed you how to find the top-10 list. To find
    the bottom-10 list, we just need to replace this line of code:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我向您展示了如何找到前10名列表。要找到底部的10个列表，我们只需要替换此行代码：
- en: '[PRE22]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO7-1)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO7-1)'
- en: If the size of the `SortedDict` is larger than 10…
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`SortedDict`的大小大于10…
- en: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO7-2)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO7-2)'
- en: …then remove the URL with the lowest frequency from the dictionary.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: …然后从字典中删除频率最低的URL。
- en: 'with this:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 'with this:'
- en: '[PRE23]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO8-1)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO8-1)'
- en: If the size of the `SortedDict` is larger than 10…
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`SortedDict`的大小大于10…
- en: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO8-2)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO8-2)'
- en: …then remove the URL with the highest frequency from the dictionary.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: …然后从字典中删除频率最高的URL。
- en: 'Next let’s discuss how to partition your input. Partitioning RDDs is a combination
    of art and science. What is the right number of partitions for your cluster? There
    is no magic formula for calculating this; it depends on the number of cluster
    nodes, the number of cores per server, and the amount of RAM available. There’s
    some trial and error involved, but a good rule of thumb is to start with the following:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来让我们讨论如何对输入进行分区。RDD的分区是艺术和科学的结合体。对于你的集群，什么是正确的分区数？没有计算这个的魔法公式；这取决于集群节点的数量，每个服务器的核心数以及可用的RAM数量。这里涉及一些试错，但一个好的经验法则是从以下开始：
- en: '[PRE24]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'When you create an RDD, if you do not set the number of partitions explicitly,
    the Spark cluster manager will set it to a default number (based on the available
    resources). You can also set the number yourself, as shown here:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 当你创建一个RDD时，如果没有显式设置分区数，Spark集群管理器将根据可用资源将其设置为默认值。你也可以像这样自己设置分区数：
- en: '[PRE25]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This creates an `RDD[String]` with 16 partitions.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个带有`16 partitions`的`RDD[String]`。
- en: 'For an existing RDD, you can reset the new number of partitions by using the
    `coalesce()` function:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 对于现有的RDD，你可以使用`coalesce()`函数重新设置新的分区数：
- en: '[PRE26]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The newly created `rdd2` (another `RDD[T]`) will have 40 partitions. The `coalesce()`
    function is defined as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 新创建的`rdd2`（另一个`RDD[T]`）将有40个分区。`coalesce()`函数定义如下：
- en: '[PRE27]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Unlike `repartition()`, which can be used to increase or decrease the number
    of partitions but involves shuffling across the cluster, `coalesce()` can only
    be used to decrease the number of partitions and in most cases does not require
    a shuffle.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 与`repartition()`不同，它可以用于增加或减少分区数，但涉及跨集群的洗牌，`coalesce()`只能用于减少分区数，在大多数情况下不需要洗牌。
- en: Next, I’ll introduce the MinMax design pattern, which is used to distill a small
    amount of information from a large dataset of numbers.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将介绍MinMax设计模式，用于从大量数字数据集中提取少量信息。
- en: MinMax
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MinMax
- en: MinMax is a numerical summarization design pattern. Given a set of billions
    of numbers, the goal is to find the minimum, maximum, and count of all of the
    numbers. This pattern can be used in scenarios where the data you are dealing
    with or you want to aggregate is of a numerical type and the data can be grouped
    by specific fields. To help you understand the concept of the MinMax design pattern,
    I am going to present three different solutions with quite different performance.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: MinMax是一种数值汇总设计模式。给定一组数十亿的数字，目标是找到所有数字的最小值、最大值和计数。这种模式可以用于数据为数值类型并且可以按特定字段分组的场景。为了帮助你理解MinMax设计模式的概念，我将介绍三种性能差异显著的解决方案。
- en: 'Solution 1: Classic MapReduce'
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案1：经典MapReduce
- en: 'The naive approach is to emit the following (key, value) pairs:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是发射以下（键，值）对：
- en: '[PRE28]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The sort and shuffle will then group all values by three keys, `min`, `max`,
    and `count`, and finally we can use a reducer to iterate through all the numbers
    and find the global `min`, `max`, and `count`. The problem with this approach
    is that we have to move potentially millions of (key, value) pairs across the
    network and then create three huge `Iterable<T>` data structures (where `T` is
    a numeric type, such as `Long` or `Double`). This solution might run into serious
    performance problems and will not scale. Furthermore, in the reduction phase it
    will not effectively utilize all of the cluster resources due to having only three
    unique keys.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，排序和洗牌将按三个键`min`、`max`和`count`分组所有值，最后我们可以使用一个减少器来迭代所有数字并找到全局的`min`、`max`和`count`。这种方法的问题在于，我们必须在网络上移动可能数百万的（键，值）对，然后创建三个巨大的`Iterable<T>`数据结构（其中`T`是数值类型，如`Long`或`Double`）。这种解决方案可能会遇到严重的性能问题，并且不会扩展。此外，在减少阶段，由于只有三个唯一键，它不会有效地利用所有的集群资源。
- en: 'Solution 2: Sorting'
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案2：排序
- en: The next solution is to sort all the numbers and then find the top (`max`),
    bottom (`min`), and `count` of the dataset. If the performance is acceptable,
    this is a valid solution; however, for a large dataset the sorting time might
    be unacceptably long. In other words, this solution will not scale out either.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个解决方案是对所有数字进行排序，然后找到数据集的顶部(`max`)、底部(`min`)和`count`。如果性能可接受，则这是一个有效的解决方案；然而，对于大型数据集，排序时间可能会不可接受的长。换句话说，此解决方案也不会扩展。
- en: 'Solution 3: Spark’s mapPartitions()'
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案3：Spark的mapPartitions()
- en: 'The final solution, which is the most efficient from a performance and scalability
    point of view, splits the data into `*N*` chunks (partitions) and then uses Spark’s
    `mapPartitions()` transformation to emit three (key, value) pairs from each partition:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的解决方案，从性能和可扩展性的角度来看最高效，将数据分割为`*N*`个块（分区），然后使用Spark的`mapPartitions()`转换从每个分区发出三个（键，值）对：
- en: '[PRE29]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Finally, we find the global `min`, `max`, and `count` from all partitions. This
    solution scales out very well. No matter how many partitions you have, this will
    work and will not create OOM errors. For example, suppose you have 500 billion
    numbers in your dataset (assume one or more numbers per record), and you partition
    it into 100,000 chunks. In the worst case (one number per record), each partition
    will have 5 million records. Each of these partitions will emit the three pairs
    shown above. Then, you just need to find the `min`, `max`, and `count` of 100,000
    x 3 pairs = 300,000 numbers. This is a trivial task that will not cause any scalability
    problems.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们从所有分区中找到全局的`min`、`max`和`count`。这个解决方案的可扩展性非常好。无论你有多少分区，这都会起作用，并且不会创建OOM错误。例如，假设您的数据集中有5000亿个数字（假设每条记录一个或多个数字），并且将其分成100,000个块。在最坏的情况下（每条记录一个数字），每个分区将有500万条记录。这些分区中的每一个将发出上述三对。然后，您只需要找到100,000
    x 3对 = 300,000个数字的`min`、`max`和`count`。这是一个微不足道的任务，不会造成任何可扩展性问题。
- en: A high-level view of this solution is illustrated in [Figure 10-4](#high_level_solution_for_minmax_design_pattern).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案的高层次视图在[图10-4](#high_level_solution_for_minmax_design_pattern)中说明。
- en: '![daws 1006](Images/daws_1006.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![daws 1006](Images/daws_1006.png)'
- en: Figure 10-4\. A PySpark implementation of the MinMax design pattern
  id: totrans-236
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-4\. MinMax设计模式的PySpark实现
- en: 'Let’s assume that our input records have the following format:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的输入记录具有以下格式：
- en: '[PRE30]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here are a few sample records:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些示例记录：
- en: '[PRE31]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Here’s the PySpark solution for solving the MinMax problem (the complete program
    with sample input is available on [GitHub](https://oreil.ly/TO5ji), in the file
    *minmax_use_mappartitions.py*):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是解决MinMax问题的PySpark解决方案（包含完整程序和示例输入的GitHub链接，在文件*minmax_use_mappartitions.py*中可用）：
- en: '[PRE32]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[![1](Images/1.png)](ch02.xhtml#comarker1)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](ch02.xhtml#comarker1)'
- en: Return a new RDD from the given input.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 从给定的输入返回一个新的RDD。
- en: '[![2](Images/2.png)](ch02.xhtml#comarker2)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](ch02.xhtml#comarker2)'
- en: Return an RDD of `(min, max, count)` from each partition by applying the `minmax()`
    function.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 应用`minmax()`函数从每个分区返回一个RDD，其中包含`(min, max, count)`。
- en: '[![3](Images/3.png)](ch02.xhtml#comarker3)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](ch02.xhtml#comarker3)'
- en: Collect `(min, max, count)` from all partitions as a list.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 作为列表从所有分区收集`(min, max, count)`。
- en: '[![4](Images/4.png)](ch02.xhtml#comarker4)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](ch02.xhtml#comarker4)'
- en: Find the (`final_min`, `final_max`, `final_count`) by calling the function `find_min_max_count()`.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用函数`find_min_max_count()`找到(`final_min`, `final_max`, `final_count`)。
- en: 'I’ve defined the `minmax()` function as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我定义了`minmax()`函数如下：
- en: '[PRE33]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO9-1)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO9-1)'
- en: The `iterator` is of type `itertools.chain`.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '`iterator`的类型为`itertools.chain`。'
- en: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO9-2)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO9-2)'
- en: Iterate the `iterator` (`record` holds a single record).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代`iterator`（`record`保存一个记录）。
- en: '[![3](Images/3.png)](#co_practical_data_design_patterns_CO9-3)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_practical_data_design_patterns_CO9-3)'
- en: Tokenize the input and build an array of numbers.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 对输入进行标记化，并构建一个数字数组。
- en: '[![4](Images/4.png)](#co_practical_data_design_patterns_CO9-4)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_practical_data_design_patterns_CO9-4)'
- en: If this is the first record, find the `min`, `max`, and `count`.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是第一条记录，找到`min`、`max`和`count`。
- en: '[![5](Images/5.png)](#co_practical_data_design_patterns_CO9-5)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_practical_data_design_patterns_CO9-5)'
- en: If this is not the first record, update `local_min`, `local_max`, and `local_count`.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这不是第一条记录，更新`local_min`、`local_max`和`local_count`。
- en: '[![6](Images/6.png)](#co_practical_data_design_patterns_CO9-6)'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_practical_data_design_patterns_CO9-6)'
- en: Finally, return a triplet from each partition.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，从每个分区返回一个三元组。
- en: What if some of the partitions are empty (i.e., contain no data)? There are
    many reasons that this can occur, and it’s important to handle empty partitions
    gracefully (for more on this, see [Chapter 3](ch03.xhtml#Chapter-03)).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某些分区为空（即不包含数据），有许多原因可能导致这种情况发生，因此优雅地处理空分区非常重要（详见[第三章](ch03.xhtml#Chapter-03)）。
- en: 'I’ll show you how to do this next. Error handling in Python is done through
    the use of exceptions that are caught in `try` blocks and handled in `except`
    blocks. In Python, if an error is encountered in a `try` block, code execution
    is stopped and control is transferred down to the `except` block. Let’s see how
    we can implement this in our MinMax solution:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我将向您展示如何处理这个问题。Python中的错误处理通过捕获`try`块中的异常并在`except`块中处理来完成。在Python中，如果在`try`块中遇到错误，则会停止代码执行，并将控制转移到`except`块。让我们看看如何在我们的MinMax解决方案中实现这一点：
- en: '[PRE34]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO10-1)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO10-1)'
- en: The `iterator` is of type `itertools.chain`.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`iterator`的类型为`itertools.chain`。'
- en: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO10-2)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO10-2)'
- en: Print the type of the `iterator` (for debugging only).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 打印`iterator`的类型（仅用于调试）。
- en: '[![3](Images/3.png)](#co_practical_data_design_patterns_CO10-3)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_practical_data_design_patterns_CO10-3)'
- en: Try to get the first record from the `iterator`. If successful, then `first_record`
    is initialized to the first record of a partition.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试从`iterator`获取第一个记录。如果成功，则将`first_record`初始化为分区的第一个记录。
- en: '[![4](Images/4.png)](#co_practical_data_design_patterns_CO10-4)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_practical_data_design_patterns_CO10-4)'
- en: If you are here, it means that the partition is empty; return a null value.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在这里，这意味着分区为空；返回一个空值。
- en: '[![5](Images/5.png)](#co_practical_data_design_patterns_CO10-5)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_practical_data_design_patterns_CO10-5)'
- en: Set `min`, `max`, and `count` from the first record.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 设置`min`、`max`和`count`为第一个记录的值。
- en: '[![6](Images/6.png)](#co_practical_data_design_patterns_CO10-6)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_practical_data_design_patterns_CO10-6)'
- en: Iterate the `iterator` for records 2, 3, etc. (`record` holds a single record).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代`iterator`以获取记录2、3等（`record`保存单个记录）。
- en: '[![7](Images/7.png)](#co_practical_data_design_patterns_CO10-7)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_practical_data_design_patterns_CO10-7)'
- en: Finally, return a triplet from each partition.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，从每个分区返回一个三元组。
- en: How should we test the handling of empty partitions? The program *minmax_force_empty_partitions.py*
    (available in the book’s GitHub repository) forces the creation of empty partitions
    and handles them gracefully. You can force the creation of empty partitions by
    setting the number of partitions higher than the number of input records. For
    example, if your input has `*N*` records, setting the number of partitions to
    `*N*+3` will cause the partitioner to create up to three empty partitions.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何测试空分区的处理？程序*minmax_force_empty_partitions.py*（在书的GitHub存储库中可用）强制创建空分区并优雅地处理它们。您可以通过将分区数设置为高于输入记录数来强制创建空分区。例如，如果您的输入有`*N*`条记录，则将分区数设置为`*N*+3`将导致分区器创建多达三个空分区。
- en: The Composite Pattern and Monoids
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复合模式和单子
- en: This section explores the concept of the composite pattern and monoids, introduced
    in [Chapter 4](ch04.xhtml#unique_chapter_id_04), and delves into how to use them
    in the context of Spark and PySpark.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了复合模式和单子的概念，这些概念在[第四章](ch04.xhtml#unique_chapter_id_04)中介绍，并深入探讨了如何在Spark和PySpark的上下文中使用它们。
- en: The composite pattern is a structural design pattern (also called a partitioning
    design pattern) that can be used when a group of objects can be treated the same
    as a single object in that group. You can use it to create hierarchies and groups
    of objects, resulting in a tree-like structure with leaves (objects) and composites
    (subgroups). This is illustrated in UML notation in [Figure 10-5](#composite_pattern_diagram).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 复合模式是一种结构设计模式（也称为分区设计模式），当一组对象可以像该组中的单个对象一样对待时，可以使用它。您可以使用它创建层次结构和对象组，从而形成具有叶子（对象）和复合物（子组）的树状结构。这在UML表示法中以[图10-5](#composite_pattern_diagram)说明。
- en: '![daws 1007](Images/daws_1007.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![daws 1007](Images/daws_1007.png)'
- en: Figure 10-5\. UML diagram of the Composite design pattern
  id: totrans-287
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-5\. 复合设计模式的UML图
- en: With this design pattern, once you’ve composed objects into this tree-like structure,
    you can work with that structure as if it were a singular object. A key feature
    is the ability to run methods recursively over the whole tree structure and sum
    up the results. This pattern can be implemented with PySpark reducers.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这种设计模式，一旦你把对象组合成这种类似树结构的形式，你就可以像操作单一对象一样操作这个结构。其关键特点是能够递归地运行方法覆盖整个树结构并汇总结果。这种模式可以通过
    PySpark 的 reducer 实现。
- en: A simple example of the use of the composite pattern is adding a set of numbers
    (over a set of keys), as illustrated in [Figure 10-6](#composite_pattern_example_addition).
    Here, the numbers are the leaves and the composites are the addition operator.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 复合模式的一个简单应用示例是对一组数字进行加法（在一组键上），如 [图 10-6](#composite_pattern_example_addition)
    所示。这里，数字是叶子节点，而复合是加法运算符。
- en: '![daws 1008](Images/daws_1008.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![daws 1008](Images/daws_1008.png)'
- en: 'Figure 10-6\. Composite pattern example: addition'
  id: totrans-291
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-6\. 复合模式示例：加法
- en: Next, I’ll discuss the concept of monoids in the context of the composite pattern.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将在复合模式的背景下讨论幺半群的概念。
- en: Monoids
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 幺半群
- en: In [Chapter 4](ch04.xhtml#unique_chapter_id_04), we discussed the use of monoids
    in reduction transformations. Here, we will look at monoids in the context of
    the composite pattern, which is commonly used in big data for composing (such
    as through addition and concatenation operators) and aggregating sets of data
    points. From the pattern’s definition, it should be obvious that there is a commonality
    between monoids and the composite pattern.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第4章](ch04.xhtml#unique_chapter_id_04) 中，我们讨论了在减少转换中使用幺半群。在这里，我们将在复合模式的背景下看看幺半群的应用，这在大数据中常用于组合（例如通过加法和连接运算符）和聚合数据点集合。从模式的定义来看，很明显幺半群与复合模式有着共同点。
- en: 'As a refresher, let’s take a look at the definition of monoids from [Wikipedia](https://oreil.ly/5TXsW):'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个复习，让我们从 [维基百科](https://oreil.ly/5TXsW) 查看一下幺半群的定义：
- en: In abstract algebra, a branch of mathematics, a monoid is a set equipped with
    an associative binary operation and an identity element. Monoids are semigroups
    with identity. Such algebraic structures occur in several branches of mathematics.
    For example, the functions from a set into itself form a monoid with respect to
    function composition. More generally, in category theory, the morphisms of an
    object to itself form a monoid, and, conversely, a monoid may be viewed as a category
    with a single object. In computer science and computer programming, the set of
    strings built from a given set of characters is a free monoid.
  id: totrans-296
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在抽象代数中，数学的一个分支中，幺半群是带有可结合的二元操作和单位元素的集合。幺半群是带单位元的半群。这样的代数结构出现在数学的几个分支中。例如，从一个集合到它自身的函数形成一个关于函数合成的幺半群。更一般地说，在范畴论中，对象到自身的态射形成一个幺半群，反过来，一个幺半群可以看作是具有单一对象的范畴。在计算机科学和计算机编程中，从给定字符集构建的字符串集合是一个自由幺半群。
- en: 'The MapReduce programming model is an application of monoids in computer science.
    As we’ve seen, it consists of three functions: `map()`, `combine()`, and `reduce()`.
    These functions are very similar to the `map()` and `flatMap()` functions (`combine()`
    is an optional operation) and reduction transformations in Spark. Given a dataset,
    `map()` maps arbitrary data to elements of a specific monoid, `combine()` aggregates/folds
    data at a local level (worker nodes in cluster), and `reduce()` aggregates/folds
    those elements, so that in the end we produce just one element.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 编程模型是计算机科学中幺半群的一个应用。正如我们所见，它由三个函数组成：`map()`、`combine()` 和 `reduce()`。这些函数与
    Spark 中的 `map()` 和 `flatMap()` 函数以及减少转换非常相似（`combine()` 是可选操作）。给定一个数据集，`map()`
    将任意数据映射到特定幺半群的元素，`combine()` 在本地级别（集群中的工作节点）聚合/折叠数据，而 `reduce()` 则聚合/折叠这些元素，最终产生一个元素。
- en: So, in terms of programming language semantics, a monoid is just an interface
    with one abstract value and one abstract method. The abstract method for a monoid
    is the append operation (it can be an addition operator on integers or a concatenation
    operator on string objects). The abstract value for a monoid is the identity value,
    defined as the value you can append to any value that will always result in the
    original value, unmodified. For example, the identity value for collection data
    structures is the empty collection, because appending a collection to an empty
    collection will typically produce the same collection unmodified. For adding a
    set of integers, the identity value is zero, and for concatenating strings it’s
    an empty string (a string of length zero).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在编程语言语义上，幺半群只是一个具有一个抽象值和一个抽象方法的接口。幺半群的抽象方法是附加操作（它可以是整数的加法运算符或字符串对象的连接运算符）。幺半群的抽象值是身份值，定义为可以附加到任何值而始终产生原始值未修改的值。例如，集合数据结构的身份值是空集合，因为将集合附加到空集合通常会产生相同的未修改集合。对于添加一组整数，身份值是零，对于连接字符串，它是空字符串（长度为零的字符串）。
- en: Next, we will briefly review MapReduce’s combines and abstract algebra’s monoids
    and see how they are related to each other. As you’ll see, when your MapReduce
    operations (e.g., `map()` and `reduceByKey()` transformations in Spark) are not
    monoids, it is very hard (if not impossible) to use combiners efficiently.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将简要回顾MapReduce的组合器和抽象代数的幺半群，并看看它们之间的关系。正如你将看到的那样，当你的MapReduce操作（例如在Spark中的`map()`和`reduceByKey()`转换）不是幺半群时，要有效地使用组合器是非常困难的（如果不是不可能的话）。
- en: 'In the MapReduce paradigm, the mapper is not constrained, but the reducer is
    required to be (the iterated application of) an associative operation. The combiner
    (as an optional plug-in component) is a “local reducer” process that operates
    only on data generated by one server. Successful use of combiners reduces the
    amount of intermediate data generated by the mappers on a given single server
    (that is why it is called local). Combiners can be used as a MapReduce optimization
    to reduce network traffic by decreasing the amount of (key, value) pairs sent
    from mappers to reducers. Typically, combiners have the same interface as reducers.
    A combiner must have the following characteristics:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在MapReduce范式中，Mapper没有约束，但Reducer需要是（迭代应用的）可结合操作。组合器（作为可选的插件组件）是一个“本地减少器”进程，仅在一个服务器生成的数据上操作。成功使用组合器可以通过减少在给定单个服务器上由Mapper生成的中间数据量来减少网络流量（这就是为什么称为本地）。组合器可以用作MapReduce的优化，通过减少从Mapper到Reducer发送的（键，值）对的数量来减少网络流量。通常，组合器具有与Reducer相同的接口。组合器必须具有以下特征：
- en: Receives as input all the data emitted by the mapper instances on a given server
    (this is called a local aggregation)
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接收给定服务器上所有由Mapper实例发出的数据作为输入（这称为本地聚合）
- en: Sends its output to the reducers
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将其输出发送给Reducers
- en: Side-effect free (combiners may run an indeterminate number of times)
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无副作用（组合器可能运行不确定次数）
- en: Has the same input and output key and value types
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有相同的输入和输出键和值类型
- en: Runs in memory after the map phase
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在映射阶段后在内存中运行
- en: 'We can define a combiner skeleton as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义组合器的框架如下：
- en: '[PRE35]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This template illustrates that the (key, value) pairs generated by a combiner
    must be of the same type as the (key, value) pairs emitted by the mapper. For
    example, if a mapper outputs `(T[1], T[2])` pairs (where the key is of type `T[1]`
    and the value is of type `T[2]`), then a combiner has to emit (`T[1]`, `T[2]`)
    pairs as well.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 此模板说明了组合器生成的（键，值）对必须与Mapper发出的（键，值）对的类型相同。例如，如果Mapper输出 `(T[1], T[2])` 对（其中键是类型为
    `T[1]`，值是类型为 `T[2]`），那么组合器也必须发出 (`T[1]`, `T[2]`) 对。
- en: The Hadoop MapReduce framework does not have an explicit `combine()` function,
    but a `Combiner` class can be added between the `Map` and `Reduce` classes to
    reduce the amount of data transferred to the reducer. The combiner is specified
    with the `Job.setCombinerClass()` method.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop MapReduce框架没有显式的`combine()`函数，但可以通过在`Map`和`Reduce`类之间添加`Combiner`类来减少传输到Reducer的数据量。组合器通过`Job.setCombinerClass()`方法指定。
- en: The goal of the combiner should be to “monoidify” the intermediate value emitted
    by the mapper—as we saw in [Chapter 4](ch04.xhtml#unique_chapter_id_04), this
    is the guiding principle for designing efficient MapReduce algorithms.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 组合器的目标应该是通过Mapper发出的中间值“单体化”，正如我们在[第4章](ch04.xhtml#unique_chapter_id_04)中看到的那样，这是设计高效MapReduce算法的指导原则。
- en: Some programming languages, like Haskell, have direct support for monoids. In
    Haskell, a monoid is “a type with a rule for how two elements of that type can
    be combined to make another element of the same type.” We’ll look at some examples
    in the following sections, but first let’s have a quick refresher on monoids.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 一些编程语言，如 Haskell，直接支持幺半群。在 Haskell 中，幺半群是“具有一条规则的类型，该规则说明如何将该类型的两个元素组合以生成该类型的另一个元素”。我们将在接下来的章节中看一些例子，但首先让我们快速回顾一下幺半群。
- en: 'A monoid is a triplet (`S`, `f`, `e`), where:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 幺半群是一个三元组 (`S`, `f`, `e`)，其中：
- en: '`S` is a set (called the underlying set of the monoid).'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`S`是一个集合（称为幺半群的底层集合）。'
- en: '`f` is a mapping called the binary operation of the monoid (`f : S x S → S`).'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`f` 是幺半群的二元操作称为映射 (`f : S x S → S`)。'
- en: '`e` is the identity operation of the monoid (`e` ∈ `S`).'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`e`是幺半群的单位操作（`e` ∈ `S`）。'
- en: 'A monoid with binary operation `+` (note that here `+` denotes the binary operation,
    not a mathematical addition operator) satisfies the following three axioms (note
    that `f(a,b) = a + b`):'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 带有二元操作 `+` 的幺半群（请注意，这里的 `+` 表示二元操作，而不是数学上的加法运算）满足以下三条公理（请注意 `f(a,b) = a + b`）：
- en: Closure
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 闭合性
- en: For all `a`, `b` in `S`, the result of the operation `(a + b)` is also in `S`.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 对于集合 `S` 中的所有 `a`、`b`，操作 `(a + b)` 的结果也在 `S` 中。
- en: Associativity
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 结合性
- en: 'For all `a`, `b`, and `c` in `S`, the following equation holds:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 对于集合 `S` 中的所有 `a`、`b` 和 `c`，以下方程成立：
- en: '[PRE36]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Identity element
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 单位元素
- en: 'There exists an element `e` in `S` such that for all elements `a` in `S`, the
    following two equations hold:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在集合 `S` 中存在一个元素 `e`，使得对于集合 `S` 中的所有元素 `a`，以下两个方程成立：
- en: '[PRE37]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In mathematical notation, we can write these as follows:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学符号中，我们可以这样写：
- en: Closure
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 闭合性
- en: '∀ `a,b` ∈ `S: a + b` ∈ `S`'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '∀ `a,b` ∈ `S: a + b` ∈ `S`'
- en: Associativity
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 结合性
- en: '[PRE38]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Identity element
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 单位元素
- en: '[PRE39]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'A monoid operation might (but isn’t required to) have other properties, such
    as:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 幺半群操作可能（但不一定）具有其他属性，例如：
- en: Idempotency
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 幂等性
- en: '[PRE40]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Commutativity
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 可交换性
- en: '[PRE41]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'To form a monoid, first we need a type *S*, which can define a set of values
    such as integers: `{0, -1, +1, -2, +2, ...}`. The second component is a binary
    function:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 要形成一个幺半群，首先我们需要一个类型 *S*，该类型可以定义值集合，如整数：`{0, -1, +1, -2, +2, ...}`。第二个组成部分是一个二元函数：
- en: '*+ : S x S → S*'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*+ : S x S → S*'
- en: 'Then we need to make sure that for any two values *x* and *y* in *S*:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要确保对于集合 *S* 中的任意两个值 *x* 和 *y*：
- en: '*x + y : S*'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x + y : S*'
- en: For example, if type *S* is a set of integers, then the binary operation may
    be addition (`+`), multiplication (`*`), or division (`/`). Finally, as the third
    and most important ingredient, we need the binary operation to follow the specified
    set of laws. If it does, then we say (*S*, +, *e*) is a monoid, where *e* in *S*
    is the identity element (such as 0 for addition and 1 for multiplication).
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果类型 *S* 是整数集合，则二元操作可以是加法（`+`）、乘法（`*`）或除法（`/`）。最后，作为第三个和最重要的成分，我们需要二元操作遵循指定的一组法则。如果是这样，我们说
    (*S*, +, *e*) 是一个幺半群，其中 *e* 在 *S* 中是单位元素（例如加法为0，乘法为1）。
- en: Tip
  id: totrans-342
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Note that the binary division operation (`/`) over a set of real numbers is
    not a monoid:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，实数集的二元除法操作（`/`）不是幺半群：
- en: '[PRE42]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In a nutshell, monoids capture the notion of combining arbitrarily many things
    into a single thing, together with a notion of an empty thing called the identity
    element or value. One simple example is addition of natural numbers `{1, 2, 3,
    ...}`. The addition function `+` allows us to combine arbitrarily many natural
    numbers into a single natural number, the sum. The identity value is the number
    0\. Another example is string concatenation, where the concatenation operator
    allows us to combine arbitrarily many strings into a single string; in this case
    the identity value is the empty string.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 简言之，幺半群捕捉将任意多个东西组合成单个东西的概念，同时还有一个称为空物或值的身份元素或值的概念。一个简单的例子是自然数的加法 `{1, 2, 3,
    ...}`。加法函数 `+` 允许我们将任意多个自然数组合成单个自然数，即和。身份值是数字 0。另一个例子是字符串连接，其中连接运算符允许我们将任意多个字符串组合成单个字符串；在这种情况下，身份值是空字符串。
- en: Monoidal and Non-Monoidal Examples
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 幺半群和非幺半群的例子
- en: Spark uses combiners in the `reduceByKey()` transformation, so to use this transformation
    effectively you must be sure that the reduction function is a monoid—that is,
    a monoid is a set (denoted by `S`) that is closed under an associative binary
    operation `(f)` and has an identity element `I` in `S` such that for all `x` in
    `S`, `f(I,` `x)` `=` `x` and `f(x,` `I)` `=` `x`. To help you understand the concept
    of monoids, here I provide some monoidal and non-monoidal examples.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在`reduceByKey()`转换中使用合并器，因此要有效使用这个转换，你必须确保缩减函数是一个幺半群——即，幺半群是一个集合（用`S`表示），它在一个结合的二元操作`(f)`下是封闭的，并且在`S`中有一个单位元`I`，使得对于所有`x`在`S`中，`f(I,
    x) = x` 和 `f(x, I) = x`。为了帮助您理解幺半群的概念，我在这里提供一些幺半群和非幺半群的例子。
- en: Maximum over a set of integers
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 整数集上的最大值
- en: 'The set `S = {0, 1, 2, ...}` is a commutative monoid for the `MAX` (maximum)
    operation, whose identity element is 0:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 集合`S = {0, 1, 2, ...}`是`MAX`（最大）操作下的交换幺半群，其单位元是`0`：
- en: '[PRE43]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Subtraction over a set of integers
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 整数集上的减法
- en: 'Subtraction (`-`) over a set of integers does not define a monoid; this operation
    is not associative:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 整数集上的减法（`-`）不定义一个幺半群；此操作不是结合的：
- en: '[PRE44]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Addition over a set of integers
  id: totrans-354
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 整数集上的加法
- en: 'Addition (`+`) over a set of integers defines a monoid; this operation is commutative
    and associative and the identity element is 0:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 整数集上的加法（`+`）定义了一个幺半群；此操作是交换的和结合的，单位元是`0`：
- en: '[PRE45]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We can formalize this as follows, where `e(+)` defines an identity element:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将此形式化如下，其中`e(+)`定义了一个单位元：
- en: '[PRE46]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Union and intersection over integers
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 整数的并集和交集
- en: Union or intersection over a set of integers forms a monoid. The binary function
    is union/intersection and the identity element is an empty set, `{}`.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 整数集的并集或交集形成一个幺半群。二元函数是并集/交集，单位元是空集`{}`。
- en: Multiplication over a set of integers
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 整数集上的乘法
- en: The set of natural numbers `N = {0, 1, 2, 3, ...}` forms a commutative monoid
    under multiplication (the identity element is 1).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 自然数集合`N = {0, 1, 2, 3, ...}`在乘法下形成一个交换幺半群（单位元为1）。
- en: Mean over a set of integers
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 整数集上的平均值
- en: 'On the other hand, the set of natural numbers, `N = {0, 1, 2, 3, ...}` does
    not form a monoid under the `MEAN` (average) function. The following example shows
    that the mean of means of an arbitrary subset of a set of values is not the same
    as the mean of the complete set of values:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，自然数集`N = {0, 1, 2, 3, ...}`在`MEAN`（平均）函数下不构成幺半群。以下例子表明，对于值集的任意子集的平均值的平均值与完整值集的平均值不同：
- en: '[PRE47]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Therefore, if you want to find the average of values for an `RDD[(key, integer)]`,
    you may not use the following transformation (which might yield the incorrect
    value due to partitioning):'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你想要计算`RDD[(key, integer)]`的值的平均数，你不能使用以下转换（由于分区而可能导致不正确的值）：
- en: '[PRE48]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The correct way to find the average per key is to make that function a monoid:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 找到每个键的平均值的正确方法是使该函数成为一个幺半群：
- en: '[PRE49]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Median over a set of integers
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 整数集上的中位数
- en: 'The set of natural numbers likewise does not form a monoid under the `MEDIAN`
    function:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 自然数集合同样不在`MEDIAN`函数下构成幺半群：
- en: '[PRE50]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Concatenation over lists
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 列表的串联
- en: 'List concatenation (`+`) with an empty list (`[]`) is a monoid. For any list
    `L`, we can write:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 列表的串联（`+`）与空列表（`[]`）是一个幺半群。对于任何列表`L`，我们可以写：
- en: '[PRE51]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Also, note that the concatenation function is associative. Given two lists,
    say `[1,2,3]` and `[7,8]`, we can join them together using `+` to get `[1,2,3,7,8]`.
    However, except in the case of concatenation with the empty list (or string, set,
    etc.), it is not commutative: `[1,2,3]+[7,8] = [1,2,3,7,8]` while `[7,8]+[1,2,3]
    = [7,8,1,2,3]`.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，串联函数是可结合的。给定两个列表，例如`[1,2,3]`和`[7,8]`，我们可以使用`+`将它们连接在一起以得到`[1,2,3,7,8]`。然而，除了与空列表（或字符串、集合等）的串联外，它不是交换的：`[1,2,3]+[7,8]=[1,2,3,7,8]`
    而 `[7,8]+[1,2,3]=[7,8,1,2,3]`。
- en: Matrix example
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩阵示例
- en: 'Let `N = {1, 2, 3, ...}`, and let *m, n ∈ N*. Then the set of *m* × *n* matrices
    with integer entries, written as *Z^(m×n)*, satisfies properties that make it
    a monoid under addition:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 设`N = {1, 2, 3, ...}`，且*m, n ∈ N*。则带整数条目的*m* × *n*矩阵的集合，写作*Z^(m×n)*，满足使其在加法下成为幺半群的属性：
- en: Closure is guaranteed by the definition.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 闭包由定义保证。
- en: The associative property is guaranteed by the associative property of its elements.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其元素的结合性质由其元素的结合性质保证。
- en: The additive identity is `0`, the zero matrix.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加法恒元是`0`，即零矩阵。
- en: These examples should help you understand what it means for a reduction function
    to be a monoid. Spark’s `reduceByKey()` is an efficient transformation that merges
    the values for each key using an associative and commutative reduce function.
    We have to make sure that its reduce function is a monoid, or we might not get
    correct reduction results.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例应该帮助您理解减少函数作为幺半群的含义。Spark的`reduceByKey()`是一个有效的转换，使用关联和交换的reduce函数合并每个key的值。我们必须确保其reduce函数是一个幺半群，否则可能得不到正确的减少结果。
- en: Non-Monoid MapReduce Example
  id: totrans-383
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非幺半群MapReduce示例
- en: 'Given a large number of (key, value) pairs where the keys are strings and the
    values are integers, the goal for this non-monoid example is to find the average
    of all the values by key. Suppose we have the following data in a table called
    `mytable`, with `key` and `value` columns:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 给定大量的(key, value)对，其中key为字符串，value为整数，对于这个非幺半群示例，我们的目标是按key找到所有值的平均值。假设我们在名为`mytable`的表中有以下数据，有`key`和`value`列：
- en: '[PRE52]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'In SQL, this is accomplished as follows:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在SQL中，可以通过以下方式实现：
- en: '[PRE53]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Here’s an initial version of a MapReduce algorithm, where the mapper is not
    generating monoid outputs for the mean/average function:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个MapReduce算法的初始版本，其中mapper未生成mean/average函数的幺半群输出：
- en: Mapper function
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: Mapper函数
- en: '[PRE54]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Reducer function
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: Reducer函数
- en: '[PRE55]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'There are a few problems with this first attempt at a MapReduce algorithm:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 这个第一次尝试的MapReduce算法存在几个问题：
- en: The algorithm is not very efficient; it will require a lot of work to be done
    in the sort and shuffle phase.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法效率不高；在排序和洗牌阶段需要完成大量工作。
- en: We cannot use the reducer as a combiner, because we know that the mean of means
    of arbitrary subsets of a set of values is not the same as the mean of the complete
    set of values.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不能将reducer作为combiner使用，因为我们知道一组值的任意子集的平均值并不等于所有值的平均值。
- en: What changes can we make to enable us to use our reducer as a combiner, so that
    we can lessen the load on the network and speed up the sort and shuffle phase?
    We need to change the output of the mapper, so that it’s a monoid. This will ensure
    that our combiners and reducers will behave correctly.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做哪些改变以使我们的Reducer可以作为Combiner使用，以减少网络负载并加快排序和洗牌阶段的速度？我们需要改变mapper的输出，使其成为一个幺半群。这将确保我们的combiners和reducers能够正确运行。
- en: Let’s take a look at how we can do that.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何做到这一点。
- en: Monoid MapReduce Example
  id: totrans-398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 幺半群MapReduce示例
- en: 'In this section, I’ll revise the mapper to generate (key, value) pairs where
    the key is a string and the value is a pair `(sum, count)`. The `(sum, count)`
    data structure is a monoid, and the identity element is `(0, 0)`. The proof is
    given here:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将修改mapper以生成(key, value)对，其中key是字符串，value是一对`(sum, count)`。`(sum, count)`数据结构是一个幺半群，单位元素是`(0,
    0)`。证明在这里给出：
- en: '[PRE56]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now, let’s write a mapper for our monoid data type:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为我们的幺半群数据类型编写一个mapper：
- en: '[PRE57]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'As you can see, the key is the same as before, but the value is a pair of `(sum,
    count)`. Now, the output of the mapper is a monoid where the identity element
    is `(0, 0)`. The element-wise sum operation can be performed as:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，key与以前相同，但value是一对`(sum, count)`。现在，mapper的输出是一个幺半群，其中单位元素是`(0, 0)`。可以执行逐元素求和操作如下：
- en: '[PRE58]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Because the mappers output monoids, the mean function will now be calculated
    correctly. Suppose the values for a single key are `{1, 2, 3, 4, 5}`, and `{1,
    2, 3}` go to partition 1 and `{4, 5}` go to partition 2:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 因为mapper输出了幺半群，现在mean函数将被正确计算。假设单个key的值为`{1, 2, 3, 4, 5}`，而`{1, 2, 3}`进入分区1，`{4,
    5}`进入分区2：
- en: '[PRE59]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The revised algorithm is as follows, where for a given pair `(sum, count)`,
    `pair.1` refers to `sum` and `pair.2` refers to `count`. Here is our combiner:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的算法如下，对于给定的一对`(sum, count)`，`pair.1`表示`sum`，`pair.2`表示`count`。这是我们的combiner：
- en: '[PRE60]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'And here is our reducer:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的reducer：
- en: '[PRE61]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Since our mapper is generating a monoidal data type, we know that our combiner
    will execute properly and our reducer will produce the correct results.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的mapper生成了幺半群数据类型，我们知道我们的combiner将正确执行，并且我们的reducer将产生正确的结果。
- en: PySpark Implementation of Monoidal Mean
  id: totrans-412
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Monoidal Mean的PySpark实现
- en: The goal of this section is to provide a solution that will enable us to use
    combiners to aggregate values when the goal is to find the mean across partitions.
    To compute the mean of all values for the same key, we can group the values using
    Spark’s `groupByKey()` transformation, then find the sum and divide by the count
    of number (per key). However, this is not an optimal solution because, as we’ve
    seen in earlier chapters, for a large dataset using `groupByKey()` can lead to
    OOM errors.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目标是提供一个解决方案，使我们能够在使用组合器来聚合值以找到分区间平均值时使用组合器。为了计算相同键的所有值的平均值，我们可以使用Spark的`groupByKey()`转换来对值进行分组，然后找到总和并除以数目（每个键）。然而，这并不是一个最优解决方案，因为正如我们在前面章节中看到的，对于大数据集使用`groupByKey()`可能会导致OOM错误。
- en: 'For the solution presented here, for a given `(key, number)` pair we will emit
    a tuple of `(key, (number, 1))`, where the associated value for a key denotes
    a pair of `(sum, count)`:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这里呈现的解决方案，对于给定的`(key, number)`对，我们将发出`(key, (number, 1))`的元组，其中键的关联值表示`(sum,
    count)`对：
- en: '[PRE62]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Earlier, I demonstrated that using `(sum, count)` as the value will enable
    us to use combiners and reducers to properly calculate the mean. Instead of `groupByKey()`
    we will use the very efficient `reduceByKey()` transformation. This is how the
    reduction function will work:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候，我展示了使用`(sum, count)`作为值将使我们能够使用组合器和减少器正确计算平均值。我们将使用非常高效的`reduceByKey()`转换而不是`groupByKey()`。这是归约函数的工作方式：
- en: '[PRE63]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Once the reduction is done, we’ll use an additional mapper to find the average
    by dividing the sum by the count.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦归约完成，我们将使用额外的映射器通过将总和除以计数来找到平均值。
- en: 'The input record format will be:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 输入记录的格式将是：
- en: '[PRE64]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'For example:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE65]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'At a high level, the PySpark solution is comprised of the following four steps:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，PySpark解决方案由以下四个步骤组成：
- en: Read the input and create the first RDD as an `RDD[String]`.
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取输入并创建第一个RDD作为`RDD[String]`。
- en: Apply `map()` to create an `RDD[key, (number, 1)]`.
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用`map()`来创建`RDD[key, (number, 1)]`。
- en: Perform the reduction with `reduceByKey()`, which will create an `RDD[key, (sum,
    count)]`.
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`reduceByKey()`执行归约操作，将创建一个`RDD[key, (sum, count)]`。
- en: Apply `mapValue()` to create the final RDD as an `RDD[key, (sum / count)]`.
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用`mapValue()`来创建最终的RDD作为`RDD[key, (sum / count)]`。
- en: The complete PySpark program (*average_monoid_driver.py*) is available on GitHub.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的PySpark程序（*average_monoid_driver.py*）可以在GitHub上找到。
- en: 'First, we need two simple Python functions to help us in using Spark transformations.
    The first function, `create_pair()`, accepts a `String` object as `"key,number"`
    and returns a `(key, (number, 1))` pair:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要两个简单的Python函数来帮助我们使用Spark转换。第一个函数`create_pair()`接受一个`String`对象作为`"key,number"`并返回一个`(key,
    (number, 1))`对：
- en: '[PRE66]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The second function, `add_pairs()`, accepts two pairs,`(sum1, count1)` and
    `(sum2, count2)`, and returns their sum as `(sum1+sum2, count1+count2)`:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个函数`add_pairs()`接受两个对`(sum1, count1)`和`(sum2, count2)`，并返回它们的和`(sum1+sum2,
    count1+count2)`：
- en: '[PRE67]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Here is the complete PySpark solution:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 这是完整的PySpark解决方案：
- en: '[PRE68]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO11-1)'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_practical_data_design_patterns_CO11-1)'
- en: Import the `print()` function.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 导入`print()`函数。
- en: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO11-2)'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_practical_data_design_patterns_CO11-2)'
- en: Import system-specific parameters and functions.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 导入系统特定的参数和函数。
- en: '[![3](Images/3.png)](#co_practical_data_design_patterns_CO11-3)'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_practical_data_design_patterns_CO11-3)'
- en: Import `SparkSession` from the `pyspark.sql` module.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 从`pyspark.sql`模块导入`SparkSession`。
- en: '[![4](Images/4.png)](#co_practical_data_design_patterns_CO11-4)'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_practical_data_design_patterns_CO11-4)'
- en: Make sure that we have two parameters in the command line.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在命令行中有两个参数。
- en: '[![5](Images/5.png)](#co_practical_data_design_patterns_CO11-5)'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_practical_data_design_patterns_CO11-5)'
- en: Create an instance of `SparkSession` using the builder pattern.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 使用构建器模式创建`SparkSession`的实例。
- en: '[![6](Images/6.png)](#co_practical_data_design_patterns_CO11-6)'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_practical_data_design_patterns_CO11-6)'
- en: Define the input path (this can be a file or a directory containing any number
    of files).
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 定义输入路径（可以是文件或包含任意数量文件的目录）。
- en: '[![7](Images/7.png)](#co_practical_data_design_patterns_CO11-7)'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_practical_data_design_patterns_CO11-7)'
- en: Read the input and create the first RDD as an `RDD[String]`, where each object
    has the format `"key,number"`.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 读取输入并创建第一个RDD作为`RDD[String]`，其中每个对象的格式为`"key,number"`。
- en: '[![8](Images/8.png)](#co_practical_data_design_patterns_CO11-8)'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '[![8](Images/8.png)](#co_practical_data_design_patterns_CO11-8)'
- en: Create the `key_number_one` RDD as an `RDD[key, (number, 1)]`.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`key_number_one` RDD作为`RDD[key, (number, 1)]`。
- en: '[![9](Images/9.png)](#co_practical_data_design_patterns_CO11-9)'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '[![9](Images/9.png)](#co_practical_data_design_patterns_CO11-9)'
- en: Aggregate `(sum1, count1)` with `(sum2, count2)` and create `(sum1+sum2, count1+count2)`
    as values.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `(sum1, count1)` 和 `(sum2, count2)` 聚合，并创建 `(sum1+sum2, count1+count2)` 作为值。
- en: '[![10](Images/10.png)](#co_practical_data_design_patterns_CO11-10)'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '[![10](Images/10.png)](#co_practical_data_design_patterns_CO11-10)'
- en: Apply the `mapValues()` transformation to find the final average per key.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 应用 `mapValues()` 转换以找到每个键的最终平均值。
- en: Functors and Monoids
  id: totrans-455
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 函数子与单子
- en: You’ve now seen several examples of monoids and their use in the MapReduce framework—but
    we can even apply higher-order functions (like functors) to monoids. A functor
    is an object that is a function (it’s a function and an object at the same time).
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经看到了几个单子及其在MapReduce框架中的使用示例，但我们甚至可以将高阶函数（如函数子）应用于单子。函数子是一个既是函数又是对象的对象。
- en: 'First, I’ll present the use of a functor on a monoid through a simple example.
    Let `MONOID = (t, e, f)` be a monoid, where `T` is a type (set of values), `e`
    is the identity element, and `f` is the `+` binary plus function:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我将通过一个简单的例子介绍如何在单子上使用函数子。让 `MONOID = (t, e, f)` 是一个单子，其中 `T` 是一个类型（值的集合），`e`
    是单位元素，`f` 是二进制加法函数 `+`：
- en: '[PRE69]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Then we can define a functor `Prod` as follows:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以如下定义函数子 `Prod`：
- en: '[PRE70]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'And we can define other functors, such as `Square`, as follows:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以定义其他函数子，例如 `Square`，如下所示：
- en: '[PRE71]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'We can also define a functor between two monoids. Let (*M*[1], *f*[1], *e*[1])
    and (*M*[2], *f*[2], *e*[2]) be monoids. A functor:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以定义两个单子之间的函数子。设 (*M*[1], *f*[1], *e*[1]) 和 (*M*[2], *f*[2], *e*[2]) 是单子。一个函数子：
- en: '*F : (M[1], f[1], e[1]) → (M[2], f[2], e[2])*'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*F : (M[1], f[1], e[1]) → (M[2], f[2], e[2])*'
- en: 'is specified by an object map (monoids are categories with a single object)
    and an arrow map *F : M[1] → M[2]*. and the following conditions will hold:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '由对象映射指定（单子是具有单个对象的范畴）和箭头映射 *F : M[1] → M[2]*。并且以下条件将成立：'
- en: '*∀a,b ∈ M[1], F(f[1](a,b)) = f[2](F(a), F(b))*'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*∀a,b ∈ M[1], F(f[1](a,b)) = f[2](F(a), F(b))*'
- en: '*F(e[1]) = e[2]*'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*F(e[1]) = e[2]*'
- en: 'A functor between two monoids is just a monoid homomorphism (a map between
    monoids that preserves the monoid operation and maps the identity element of the
    first monoid to that of the second monoid). For example, for the `String` data
    type, a function `Length()` that counts the number of letters in a word is a monoid
    homomorphism:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 两个单子之间的函数子只是一个单子同态（在保持单子操作并映射第一个单子的单位元素到第二个单子的单位元素之间的映射）。例如，对于 `String` 数据类型，一个函数
    `Length()`，它计算一个单词中字母的数量，是一个单子同态：
- en: '`Length("") = 0` (the length of an empty string is 0).'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Length("") = 0`（空字符串的长度为0）。'
- en: 'If `Length(x) = m` and `Length(y) = n`, then concatenation `x + y` of strings
    has `m + n` letters. For example:'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `Length(x) = m` 和 `Length(y) = n`，则字符串的连接 `x + y` 有 `m + n` 个字母。例如：
- en: '[PRE72]'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Again, having mappers create monoids guarantees that the reducers can take advantage
    of using combiners effectively and correctly.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，使映射器创建单子可以确保减少器能够有效且正确地利用组合器。
- en: Conclusion on Using Monoids
  id: totrans-473
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对单子的使用结论
- en: 'As we’ve observed, in the MapReduce paradigm (which is the foundation of Hadoop,
    Spark, Tez, and other frameworks), if your mapper generates monoids you can utilize
    combiners for optimization and efficiency purposes. Using combiners reduces network
    traffic and speeds up MapReduce’s sort and shuffle phase, because there’s less
    data to process. You also saw some examples of how to monoidify MapReduce algorithms.
    In general, combiners can be used when the function you want to apply is both
    commutative and associative (properties of a monoid). For example, the classic
    word count function is a monoid over a set of integers with the `+` operation
    (here you can use a combiner). However, the mean function (which is not associative)
    over a set of integers does not form a monoid. To use combiners effectively in
    a case like this, we need to ensure that the output from the mappers is monoidal.
    Next, we’ll turn our attention to a few important data organization patterns:
    binning and sorting.'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所观察到的，在MapReduce范式中（这是Hadoop、Spark、Tez等框架的基础），如果您的映射器生成单子，则可以利用组合器进行优化和效率目的。使用组合器减少了网络流量并加速了MapReduce的排序和洗牌阶段，因为要处理的数据更少。您还看到了如何将MapReduce算法单子化的一些示例。一般而言，当您要应用的函数是可交换和可结合的（单子的性质）时，可以使用组合器。例如，经典的单词计数函数在整数集合上是一个具有
    `+` 操作的单子（这里可以使用组合器）。然而，平均函数（不是可结合的）在整数集合上不形成单子。要在这种情况下有效地使用组合器，我们需要确保映射器的输出是单子的。接下来，我们将关注一些重要的数据组织模式：分桶和排序。
- en: Binning
  id: totrans-475
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分桶
- en: Binning is a way to group a number of more or less continuous values into a
    smaller number of “bins,” or buckets. For example, if you have census data about
    a group of people, you might want to map their individual ages into a smaller
    number of age intervals, such as `0-5`, `6-10`, `11-15`, …, `96-100+`. An important
    advantage of binning is that it narrows the range of the data you need to search
    for a specific value. For example, if you know that someone is 14 years old, to
    find them you only need to search in the bin labeled `11-15`. In other words,
    binning can help us to do faster queries by examining a slice of the data rather
    than the whole dataset. The binning design pattern moves the records into categories
    (bins) irrespective of the initial order of the records.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: Binning 是一种将多个或多少连续的数值分组成较少数量的“bin”或“桶”的方法。例如，如果你有关于一群人口普查数据，你可能希望将他们的年龄映射到较少数量的年龄区间，比如`0-5`，`6-10`，`11-15`，…，`96-100+`。Binning
    的一个重要优势是它缩小了搜索特定值所需的数据范围。例如，如果你知道有人是 14 岁，你只需要在标记为`11-15`的 bin 中搜索他们。换句话说，Binning
    可以通过检查数据的一个切片而不是整个数据集来帮助我们更快地进行查询。Binning 设计模式将记录移动到类别（bin），而不考虑记录的初始顺序。
- en: '[Figure 10-7](#binning_by_chromosome) illustrates another example. In genomics
    data, chromosomes are labeled as `{chr1, chr2, ..., chr22, chrX, chrY, chrMT}`.
    A human being has 3 billion pairs of chromosomes, where `chr1` has about 250 million
    positions, `chr7` has 160 million positions, and so on. If you want to find a
    variant key of `10:100221486:100221486:G`, you’ll have to search billions of records,
    which is very inefficient. Binning can help speed up the process: if we group
    the data by chromosomes, to find this variant key we can just look in the bin
    labeled `chr10` rather than searching all of the data.'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '[Figure 10-7](#binning_by_chromosome) 展示了另一个例子。在基因组数据中，染色体标记为`{chr1, chr2,
    ..., chr22, chrX, chrY, chrMT}`。一个人类有 30 亿对染色体，其中`chr1`大约有 2.5 亿个位置，`chr7`有 1.6
    亿个位置，依此类推。如果你想找到一个变体键为`10:100221486:100221486:G`，你将不得不搜索数十亿条记录，这是非常低效的。Binning
    可以帮助加快这一过程：如果我们按染色体对数据进行分组，要找到这个变体键，我们只需要查看标记为`chr10`的 bin，而不是搜索所有数据。'
- en: '![daws 1009](Images/daws_1009.png)'
  id: totrans-478
  prefs: []
  type: TYPE_IMG
  zh: '![daws 1009](Images/daws_1009.png)'
- en: Figure 10-7\. Binning by chromosome
  id: totrans-479
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-7\. 按染色体进行 Binning
- en: To implement the binning algorithm in PySpark, first we read our input and create
    a DataFrame with the proper columns. Then, we create an additional column, called
    `chr_id`, which will denote a bin for a chromosome. The `chr_id` column will have
    values in the set `{chr1, chr2, ..., chr22, chrX, chrY, chrMT}`.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 PySpark 中实现 Binning 算法，首先我们读取输入并创建一个带有正确列的 DataFrame。然后，我们创建一个额外的列，称为`chr_id`，它将表示染色体的一个
    bin。`chr_id` 列将具有集合`{chr1, chr2, ..., chr22, chrX, chrY, chrMT}`中的值。
- en: It is possible to implement binning in several layers—for example, first by
    chromosome and then by the modulo of the start position—as illustrated in [Figure 10-8](#binning_by_chromosome_2).
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在多个层次上实现 Binning —— 例如，首先按染色体，然后按起始位置的模数—— 如 [Figure 10-8](#binning_by_chromosome_2)
    所示。
- en: '![daws 1010](Images/daws_1010.png)'
  id: totrans-482
  prefs: []
  type: TYPE_IMG
  zh: '![daws 1010](Images/daws_1010.png)'
- en: Figure 10-8\. Binning by chromosome
  id: totrans-483
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-8\. 按染色体进行 Binning
- en: 'This will be quite helpful because we might have millions of variants per chromosome;
    an additional layer of binning can help us to further reduce the query time by
    allowing us to examine an even thinner slice of the data. Before I show you how
    to implement binning by start position, let’s take a look at the variant structure:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 这将非常有帮助，因为我们可能每条染色体有数百万个变体；再加一层 Binning 可以通过允许我们检查更薄的数据切片进一步减少查询时间。在我展示如何按起始位置实现
    Binning 之前，让我们先看一下变体结构：
- en: '[PRE73]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'A simple binning algorithm will be to partition the `start_position` into 101
    bins (depending on the volume of data you may select a different number, but it
    should be a prime number). Therefore, our bin values will be `{0, 1, 2, ..., 100}`.
    We’ll then create another new column called `modulo`, and its value will be defined
    as:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的 Binning 算法是将`start_position`划分为 101 个 bin（取决于数据量，你可能会选择一个不同的数量，但应为质数）。因此，我们的
    bin 值将是`{0, 1, 2, ..., 100}`。然后，我们将创建另一个新列称为`modulo`，其值将定义为：
- en: '[PRE74]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: For example, for a variant of `10:100221486:100221486:G`, the `module` value
    will be `95` (`100221486 % 101 = 95`).
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于变体`10:100221486:100221486:G`，`module`值将是`95`（`100221486 % 101 = 95`）。
- en: 'Continuing with the example of genomics data, suppose we have the following
    data (note that I’ve included only a few columns here, to keep the example simple).
    First, we create a DataFrame from this data:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 继续以基因组数据示例为例，假设我们有以下数据（请注意，我在这里只包括了几列，以保持示例简单）。首先，我们从这些数据创建一个 DataFrame：
- en: '[PRE75]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Next, we create a binning function for a `chr_id`, to be extracted from a given
    `variant_key`:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为`chr_id`创建一个分箱函数，该函数从给定的`variant_key`中提取：
- en: '[PRE76]'
  id: totrans-492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'To use the `extract_chr()` function, first we have to create a UDF:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`extract_chr()`函数，首先我们必须创建一个 UDF：
- en: '[PRE77]'
  id: totrans-494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'To create a second level of binning, we need another Python function to find
    `start_position % 101`:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建第二级分箱，我们需要另一个 Python 函数来找到`start_position % 101`：
- en: '[PRE78]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'We then define another UDF to use this function to create the `modulo` column:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义另一个 UDF 来使用此函数创建`modulo`列：
- en: '[PRE79]'
  id: totrans-498
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'We can save our DataFrame in Parquet format without binning as follows:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 DataFrame 保存为 Parquet 格式，不进行分箱，如下所示：
- en: '[PRE80]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Or, to save the data with binning information, we can use the `partitionBy()`
    function:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，为了保存带有分箱信息的数据，我们可以使用`partitionBy()`函数：
- en: '[PRE81]'
  id: totrans-502
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Sorting
  id: totrans-503
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 排序
- en: Sorting of data records is a common task in many programming languages, such
    as Python and Java. Sorting refers to any process of arranging records systematically,
    and can involve ordering (arranging records in a sequence ordered by some criterion)
    or categorizing (grouping items with similar properties). With ordering, the sorting
    can be done either in the normal order of low to high (ascending) or the normal
    order of high to low (descending). There are many well-known sorting algorithms—such
    as quick sort, bubble sort, and heap sort—with different time complexities.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 数据记录的排序是许多编程语言中的常见任务，例如 Python 和 Java。排序是指任何按照系统化排列记录的过程，并且可以涉及排序（按某些标准排列顺序排列记录）或分类（具有相似属性的项目分组）。在排序中，可以按照正常的低到高（升序）或正常的高到低（降序）顺序进行排序。有许多著名的排序算法——如快速排序、冒泡排序和堆排序——具有不同的时间复杂度。
- en: 'PySpark offers several functions for sorting RDDs and DataFrames, here are
    a few of them:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 提供了几个用于排序 RDD 和 DataFrame 的函数，以下是其中几个：
- en: '[PRE82]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Use of these sorting functions is straightforward.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些排序函数非常简单。
- en: Summary
  id: totrans-508
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: MapReduce design patterns are common patterns in data analytics. These design
    patterns enable us to solve similar data problems in an efficient manner.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 设计模式是数据分析中常见的模式。这些设计模式使我们能够以高效的方式解决类似的数据问题。
- en: 'Data design patterns can be classified into several different categories, such
    as:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 数据设计模式可以分为几个不同的类别，例如：
- en: Summarization patterns
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要模式
- en: Get a top-level view by summarizing and grouping data. Examples include in-mapper
    combining (which we used to solve the word count problem) and MinMax.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 通过汇总和分组数据来获得顶层视图。例如，包括内映射组合（用于解决词频统计问题）和 MinMax。
- en: Filtering patterns
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤模式
- en: View data subsets by using predicates. An example is the top-10 pattern.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 使用谓词查看数据子集。一个示例是前10位模式。
- en: Data organization patterns
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 数据组织模式
- en: Reorganize data to work with other systems, or to make MapReduce analysis easier.
    Examples include binning and sorting algorithms.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 重新组织数据以便与其他系统配合使用，或者使 MapReduce 分析更容易。例如，包括分箱和排序算法。
- en: Join patterns
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 连接模式
- en: Analyze different datasets together to discover interesting relationships.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 分析不同的数据集以发现有趣的关系。
- en: Meta patterns
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 元模式
- en: Piece together several patterns to solve multistage problems, or to perform
    several analytics in the same job.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 将多个模式拼接在一起以解决多阶段问题，或在同一作业中执行多个分析。
- en: Input and output patterns
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和输出模式
- en: Customize the way you use a persistent store (such as HDFS or S3) to load or
    store data.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义使用持久存储（如 HDFS 或 S3）加载或存储数据的方式。
- en: In the next chapter we’ll look at design patterns for performing joins, which
    is an important transformation between two large datasets.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将研究执行连接的设计模式，这是两个大数据集之间的重要转换。
