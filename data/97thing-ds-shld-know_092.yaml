- en: Chapter 86\. Are Ethics Nothing More than Constraints and Guidelines for Proper
    Societal Behavior?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第86章。伦理只是适当社会行为的约束和准则吗？
- en: Bill Schmarzo
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比尔·施马尔佐
- en: '![](Images/Bill_Schmarzo.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/Bill_Schmarzo.png)'
- en: Chief Innovation Officer, Hitachi Vantara
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 日立万达创新官员
- en: 'Twitter, when used for good, can be a marvelous sharing and learning environment.
    For example, one of my Twitter followers made an interesting statement in response
    to my blog post [“AI Ethics Challenge: Understanding Passive Versus Proactive
    Ethics”](https://oreil.ly/TTeZu). They posted: *“*Reducing ethics reasoning to
    a utility function misses the level of abstraction that ethics provide to act
    across contexts and situations. Then you don’t have ethics anymore, you have constraints.”'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 当Twitter用于积极用途时，可以成为一个奇妙的分享和学习环境。例如，我的一个Twitter关注者在回应我的博客文章[“AI伦理挑战：理解被动与主动伦理”](https://oreil.ly/TTeZu)时发表了一个有趣的声明：*“将伦理推理简化为效用函数，忽略了伦理在跨越不同情境和场景中提供的抽象水平。那么你不再有伦理，你只有约束。”*
- en: 'Constraints? Interesting. Or put another way: *are ethics just the set of constraints,
    rules, and guidelines that dictate how one is expected to act or behave within
    a properly functioning society?*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 约束？有趣。或者换个角度说：*伦理只是一套约束、规则和准则，指导人们在一个正常运作的社会中如何行动或行为？*
- en: While I don’t feel qualified to talk about ethics from a general society perspective,
    the discussion of ethics from an AI perspective is certainly within my domain
    of experience and should be everyone’s concern. And that means we need to have
    a discussion about the creation of the *AI utility function*. The AI utility function
    comprises the constraints, rules, and guidelines that guide the actions and adaption
    of the AI model.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我觉得自己不够资格从一般社会的角度谈论伦理问题，但从人工智能的角度讨论伦理问题显然是我熟悉的领域，也应该是每个人的关注点。这意味着我们需要讨论*AI效用函数*的创建。AI效用函数包括指导AI模型行动和适应的约束、规则和准则。
- en: When creating AI-enabled autonomous entities—entities that make decisions, take
    actions, learn, and adapt with minimal human intervention—the definition of the
    AI utility function is critical. The AI ethics challenge is to define and encode
    those ethics (constraints, rules, and guidelines) into the math that makes up
    the AI utility function and drives the operations of autonomous entities. The
    AI utility function must understand these ethics in order to take the most appropriate
    or “right” actions. If we are going to transform into a world of AI-enabled autonomous
    entities—cars, trucks, and so on—then we must master encoding these ethics into
    math.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建AI使能的自主实体——即做出决策、采取行动、学习并适应，几乎不需要人类干预的实体时——AI效用函数的定义至关重要。AI伦理的挑战在于定义和编码这些伦理（约束、规则和准则），将其编码成构成AI效用函数的数学公式，并驱动自主实体的运作。AI效用函数必须理解这些伦理，以便采取最合适或“正确”的行动。如果我们要转向一个由AI使能的自主实体——如汽车、卡车等——组成的世界，那么我们必须精通将这些伦理编码成数学公式。
- en: Asimov’s Three Laws of Robotics Ethics
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阿西莫夫的三大机器人定律伦理
- en: '[Isaac Asimov](https://oreil.ly/QuPJX) was an American writer who was well
    known for his science fiction works. In a 1942 short story, Asimov first set forth
    his “Three Laws of Robotics,” within which a robot must be expected to behave
    in order to have a properly functioning society. The three laws are:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[艾萨克·阿西莫夫](https://oreil.ly/QuPJX)是一位美国作家，以其科幻作品闻名。在1942年的一篇短篇小说中，阿西莫夫首次提出了他的“三大机器人定律”，在这些定律中，机器人必须表现出适当的行为以确保社会的正常运作。这三大定律分别是：'
- en: First Law
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 第一定律
- en: A robot may not injure a human being, or, through inaction, allow a human being
    to come to harm.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一个机器人不得伤害人类，也不得通过不作为使人类受到伤害。
- en: Second Law
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第二定律
- en: A robot must obey the orders given it by human beings, except where such orders
    would conflict with the First Law.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个机器人必须服从人类的命令，除非这些命令与第一定律相冲突。
- en: Third Law
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 第三定律
- en: A robot must protect its own existence, as long as such protection does not
    conflict with the First or Second Law.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个机器人必须保护自己的存在，只要这种保护不与第一或第二定律相冲突。
- en: 'I postulated in the blog post [“Isaac Asimov: The 4th Law of Robotics”](https://oreil.ly/eEkMs)
    that we might need to come up with a Fourth Law of Robotics.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我在博客文章[“艾萨克·阿西莫夫：机器人学伦理的第四定律”](https://oreil.ly/eEkMs)中提出，我们可能需要制定第四定律。
- en: There will be situations in which these autonomous entities will be forced to
    make life-and-death decisions about which humans to save and which humans to kill—for
    example, an autonomous vehicle deciding between saving its passenger or a pedestrian.
    Isaac Asimov didn’t envision needing a law to govern robots in these sorts of
    situations, where it isn’t the life of the robot versus the life of a human in
    debate but a choice between the lives of multiple humans!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，这些自主实体将被迫做出关于该拯救哪些人类、该杀害哪些人类的生死抉择，例如，自动驾驶车辆在救其乘客还是行人之间做出选择。艾萨克·阿西莫夫并没有预见到需要一项法律来管理这些情况，这并不是机器人生命与人类生命之间的辩论，而是在多个人类生命之间做出选择的问题！
- en: 'Surveys have been conducted to understand what to do in a situation in which
    the autonomous car has to make a life-and-death decision between saving a passenger
    and sparing pedestrians. The article [“Will Your Driverless Car Be Willing to
    Kill You to Save the Lives of Others?”](https://oreil.ly/m7caN) found the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 已经进行了调查，以了解在自动驾驶汽车需要在拯救乘客和避免撞到行人之间做出生死抉择的情况下应该采取什么行动。文章[“你的无人驾驶车是否愿意杀你以拯救他人的生命？”](https://oreil.ly/m7caN)得出了以下结论：
- en: 'In one survey, 76% of people agreed that a driverless car should sacrifice
    its passenger rather than plow into and kill 10 pedestrians. They agreed, too,
    that it was moral for autonomous vehicles to be programmed in this way: it minimized
    deaths the cars caused. And the view held even when people were asked to imagine
    themselves or a family member traveling in the car.'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在一项调查中，76%的人同意无人驾驶汽车应该牺牲其乘客，而不是撞向并杀死10名行人。他们也认同，将自动驾驶汽车以此方式编程是道德的：这样做可以最大程度地减少汽车造成的死亡。即使被问及想象自己或家人乘坐这种车辆的情况，这种观点依然存在。
- en: But hold on—while in theory 76% favor saving the pedestrians over the passenger,
    the sentiment changes when it involves *you*!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，要稍等一下——虽然理论上有76%的人倾向于救行人而不是乘客，但当事情牵涉到*你*的时候，情况就会改变！
- en: When people were asked whether they would buy a car controlled by such a moral
    algorithm, their enthusiasm cooled. Those surveyed said they would much rather
    purchase a car programmed to protect themselves instead of pedestrians. In other
    words, driverless cars that occasionally sacrificed their drivers for the greater
    good were a fine idea, but only for other people.
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当被问及是否愿意购买由这种道德算法控制的汽车时，人们的热情稍减。被调查者表示，他们更愿意购买为了保护自己而编程的汽车，而不是为了行人。换句话说，偶尔为了大局牺牲司机的无人驾驶车是个好主意，但只限于别人使用。
- en: 'Riddle me this, Batman: Would the “programmed” reaction of an autonomous car
    in these life-and-death situations impact your decision to buy a particular brand
    of autonomous car?'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你能否回答这个谜题，蝙蝠侠：在这些生死抉择的情况下，无人驾驶车的“程序化”反应是否会影响你购买特定品牌无人驾驶汽车的决定？
- en: Another study published in the journal *Science*, [“The Social Dilemma of Autonomous
    Vehicles”](https://oreil.ly/f7w7H), highlighted the ethical dilemmas self-driving
    car manufacturers are faced with. About 2,000 people were polled, and the majority
    believed that autonomous cars should always make the decision to cause the fewest
    number of fatalities. On the other hand, most people also said they would buy
    a self-driving car *only if it meant their safety was a priority*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项发表在《科学》期刊上的研究，[“自动驾驶汽车的社会困境”](https://oreil.ly/f7w7H)，突出了自动驾驶汽车制造商面临的伦理困境。大约有2,000人参与了调查，大多数人认为自动驾驶汽车应该始终做出减少死亡人数的决策。另一方面，大多数人也表示，只有在自己的安全是优先考虑时，他们才会购买自动驾驶汽车。
- en: I’m not sure that we want individual companies or our political leaders programming
    the rules that guide these sorts of life-and-death decisions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我不确定我们是否希望个别公司或我们的政治领导人制定规则来指导这些生死抉择的规则。
- en: But if not them, then who?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不是他们，那又会是谁呢？
- en: Summary
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: '*Are ethics just the set of constraints, rules, and guidelines that dictate
    how one is expected to act or behave within a properly functioning society?*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*伦理道德是否仅仅是一套约束、规则和指导方针，用来规范一个正常运作社会中的人们如何行动或行为？*'
- en: For some, the Bible may be the ultimate book on ethics. (I can’t speak for any
    other religious books such as the Quran or the Tanakh or the Tripitaka because
    I have not been exposed to them.) The Bible is full of constraints, rules, and
    guidelines on the ethics that guide proper individual and societal actions and
    behaviors, encoded through commandments, stories, and parables.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些人来说，圣经可能是伦理道德的终极书籍。（我不能评论其他宗教书籍，比如《古兰经》、《旧约圣经》或《三藏经》，因为我没有接触过它们。）圣经充满了通过诫命、故事和寓言编码的约束、规则和指导原则，这些原则指导着正确的个人和社会行为。
- en: If we can create constraints, rules, and guidelines—encoded in the AI utility
    function—that guide the proper actions and behaviors of AI-enabled autonomous
    entities, then maybe we have a chance to really pull this AI thing off. We just
    need to get the most qualified leaders in our society to start identifying, validating,
    valuing, and prioritizing those constraints, rules, and guidelines that will need
    to comprise the AI utility function.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够创建约束、规则和指导原则——通过AI实用功能编码——来指导AI增强的自主实体的正确行动和行为，那么也许我们真的有机会实现这个AI的事情。我们只需要让社会中最有资格的领导者开始识别、验证、重视和优先考虑那些将构成AI实用功能的约束、规则和指导原则。
