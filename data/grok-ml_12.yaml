- en: 12 Combining models to maximize results: Ensemble learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 结合模型以最大化结果：集成学习
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中
- en: what ensemble learning is, and how it is used to combine weak classifiers into
    a stronger one
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成学习的概念以及它是如何将弱分类器组合成更强的分类器的
- en: using bagging to combine classifiers in a random way
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机森林以随机方式组合分类器
- en: using boosting to combine classifiers in a cleverer way
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用提升方法以更巧妙的方式组合分类器
- en: 'some of the most popular ensemble methods: random forests, AdaBoost, gradient
    boosting, and XGBoost'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些最受欢迎的集成方法：随机森林、AdaBoost、梯度提升和XGBoost
- en: '![](../Images/12-unnumb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-unnumb.png)'
- en: After learning many interesting and useful machine learning models, it is natural
    to wonder if it is possible to combine these classifiers. Thankfully, we can,
    and in this chapter, we learn several ways to build stronger models by combining
    weaker ones. The two main methods we learn in this chapter are bagging and boosting.
    In a nutshell, bagging consists of constructing a few models in a random way and
    joining them together. Boosting, on the other hand, consists of building these
    models in a smarter way by picking each model strategically to focus on the previous
    models’ mistakes. The results that these ensemble methods have shown in important
    machine learning problems has been tremendous. For example, the Netflix Prize,
    which was awarded to the best model that fits a large dataset of Netflix viewership
    data, was won by a group that used an ensemble of different models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习了众多有趣且实用的机器学习模型之后，自然会想知道是否可以将这些分类器结合起来。幸运的是，我们可以做到，在本章中，我们将学习几种通过组合弱模型来构建更强模型的方法。本章中我们学习的两种主要方法是集成学习和提升学习。简而言之，集成学习是通过随机构建几个模型并将它们结合起来。而提升学习则通过战略性地选择每个模型来关注前一个模型的错误，以更智能地构建这些模型。这些集成方法在重要的机器学习问题中展现出的结果是巨大的。例如，Netflix
    Prize，该奖项授予了最适合Netflix大量观众数据的最佳模型，由一个使用不同模型集成的团队赢得。
- en: In this chapter, we learn some of the most powerful and popular bagging and
    boosting models, including random forests, AdaBoost, gradient boosting, and XGBoost.
    The majority of these are described for classification, and some are described
    for regression. However, most of the ensemble methods work in both cases.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了一些最强大和最受欢迎的集成学习和提升模型，包括随机森林、AdaBoost、梯度提升和XGBoost。其中大部分模型用于分类，而一些则用于回归。然而，大多数集成方法在这两种情况下都适用。
- en: 'A bit of terminology: throughout this book, we have referred to machine learning
    models as models, or sometimes regressors or classifiers, depending on their task.
    In this chapter, we introduce the term *learner*, which also refers to a machine
    learning model. In the literature, it is common to use the terms *weak learner*
    and *strong learner* when talking about ensemble methods. However, there is no
    difference between a machine learning model and a learner.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一些术语：在这本书中，我们根据其任务将机器学习模型称为模型，有时也称为回归器或分类器。在本章中，我们引入了“学习器”这个术语，它也指代机器学习模型。在文献中，当谈论集成方法时，常用“弱学习器”和“强学习器”这两个术语。然而，机器学习模型和学习者之间并没有区别。
- en: 'All the code for this chapter is available in this GitHub repository: [https://github.com/luisguiserrano/manning/tree/master/Chapter_12_Ensemble_Methods](https://github.com/luisguiserrano/manning/tree/master/Chapter_12_Ensemble_Methods).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码都可在以下GitHub仓库中找到：[https://github.com/luisguiserrano/manning/tree/master/Chapter_12_Ensemble_Methods](https://github.com/luisguiserrano/manning/tree/master/Chapter_12_Ensemble_Methods)。
- en: With a little help from our friends
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在朋友的帮助下
- en: 'Let’s visualize ensemble methods using the following analogy: Imagine that
    we have to take an exam that consists of 100 true/false questions on many different
    topics, including math, geography, science, history, and music. Luckily, we are
    allowed to call our five friends—Adriana, Bob, Carlos, Dana, and Emily—to help
    us. There is a small constraint, which is that all of them work full time, and
    they don’t have time to answer all 100 questions, but they are more than happy
    to help us with a subset of them. What techniques can we use to get their help?
    Two possible techniques follow:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下类比来可视化集成方法：想象一下，我们必须参加一个包含100个不同主题（包括数学、地理、科学、历史和音乐）的真假题考试。幸运的是，我们可以叫上我们的五个朋友——Adriana、Bob、Carlos、Dana和Emily——来帮助我们。有一个小限制，就是他们都全职工作，没有时间回答所有100个问题，但他们非常乐意帮助我们回答其中的一部分。我们可以使用什么技术来获取他们的帮助？以下有两种可能的技术：
- en: '**Technique 1**: For each of the friends, pick several random questions, and
    ask them to answer them (make sure every question gets an answer from at least
    one of our friends). After we get the responses, answer the test by selecting
    the option that was most popular among those who answered that question. For example,
    if two of our friends answered “True” and one answered “False” on question 1,
    then we answer question 1 as “True” (if there are ties, we can pick one of the
    winning responses randomly).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**技术1**：对于每个朋友，挑选几个随机问题，并让他们回答这些问题（确保每个问题至少有一个朋友回答）。在得到回应后，通过选择那些回答该问题的人中最受欢迎的选项来回答测试。例如，如果我们的两个朋友在问题1上回答“True”，一个朋友回答“False”，那么我们回答问题1为“True”（如果有平局，我们可以随机选择一个获胜的回应）。'
- en: '**Technique 2**: We give the exam to Adriana and ask her to answer only the
    questions she is the surest about. We assume that those answers are good and remove
    them from the test. Now we give the remaining questions to Bob, with the same
    instructions. We continue in this fashion until we pass it to all the five friends.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**技术2**：我们给Adriana考试，并要求她只回答她最有把握的问题。我们假设这些答案是好的，并将它们从测试中删除。现在我们将剩余的问题给Bob，并给予相同的指示。我们继续这样做，直到将问题传递给所有五个朋友。'
- en: Technique 1 resembles a bagging algorithm, and technique 2 resembles a boosting
    algorithm. To be more specific, bagging and boosting use a set of models called
    *weak learners* and combine them into a *strong learner* (as illustrated in figure
    12.1).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 技术1类似于Bagging算法，技术2类似于Boosting算法。更具体地说，Bagging和Boosting使用一组称为“弱学习器”的模型，并将它们组合成一个“强学习器”（如图12.1所示）。
- en: '![](../Images/12-1.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-1.png)'
- en: Figure 12.1 Ensemble methods consist of joining several weak learners to build
    a strong learner.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 集成方法包括将多个弱学习器结合在一起构建强学习器。
- en: '**Bagging**: Build random sets by drawing random points from the dataset (with
    replacement). Train a different model on each of the sets. These models are the
    weak learners. The strong learner is then formed as a combination of the weak
    models, and the prediction is done by voting (if it is a classification model)
    or averaging the predictions (if it is a regression model).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**Bagging**：通过从数据集中随机抽取点（有放回）来构建随机集合。在每个集合上训练不同的模型。这些模型是弱学习器。然后，通过投票（如果是分类模型）或平均预测（如果是回归模型）来形成强学习器。'
- en: '**Boosting**: Start by training a random model, which is the first weak learner.
    Evaluate it on the entire dataset. Shrink the points that have good predictions,
    and enlarge the points that have poor predictions. Train a second weak learner
    on this modified dataset. We continue in this fashion until we build several models.
    The way to combine them into a strong learner is the same way as with bagging,
    namely, by voting or by averaging the predictions of the weak learner. More specifically,
    if the learners are classifiers, the strong learner predicts the most common class
    predicted by the weak learners (thus the term *voting*), and if there are ties,
    by choosing randomly among them. If the learners are regressors, the strong learner
    predicts the average of the predictions given by the weak learners.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**Boosting**：首先训练一个随机模型，这是第一个弱学习器。在整个数据集上评估它。缩小预测良好的点，放大预测不良的点。在这个修改后的数据集上训练第二个弱学习器。我们继续这样做，直到构建了几个模型。将它们组合成强学习器的方式与Bagging相同，即通过投票或平均弱学习器的预测。更具体地说，如果学习器是分类器，强学习器预测弱学习器预测的最常见的类别（因此称为“投票”），如果有平局，则随机选择。如果学习器是回归器，强学习器预测弱学习器给出的预测的平均值。'
- en: Most of the models in this chapter use decision trees (both for regression and
    classification) as the weak learners. We do this because decision trees lend themselves
    very well to this type of approach. However, as you read the chapter, I encourage
    you to think of how you would combine other types of models, such as perceptrons
    and SVMs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的大多数模型都使用决策树（无论是回归还是分类）作为弱学习器。我们这样做是因为决策树非常适合这种类型的方法。然而，当你阅读本章时，我鼓励你思考如何结合其他类型的模型，例如感知器和SVMs。
- en: We’ve spent an entire book building very good learners. Why do we want to combine
    several weak learners instead of simply building a strong learner from the start?
    One reason is that ensemble methods have been shown to overfit much less than
    other models. In a nutshell, it is easy for one model to overfit, but if you have
    several models for the same dataset, the combination of them overfits less. In
    a sense, it seems that if one learner makes a mistake, the others tend to correct
    it, and on average, they work better.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们花费了一整本书的时间构建非常好的学习器。为什么我们想要结合多个弱学习器，而不是一开始就构建一个强学习器呢？一个原因是集成方法已被证明比其他模型过度拟合的情况要少得多。简而言之，一个模型过度拟合是很容易的，但如果你有多个针对同一数据集的模型，它们的组合过度拟合会更少。从某种意义上说，如果有一个学习器犯了错误，其他的学习器往往会纠正它，并且平均来说，它们会表现得更好。
- en: 'We learn the following models in this chapter. The first one is a bagging algorithm,
    and the last three are boosting:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了以下模型。第一个是一个集成算法，最后三个是提升算法：
- en: Random forests
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: AdaBoost
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaBoost
- en: Gradient boosting
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升
- en: XGBoost
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost
- en: All these models work for regression and classification. For educational purposes,
    we learn the first two as classification models and the last two as regression
    models. The process is similar for both classification and regression. However,
    read each of them and imagine how it would work in both cases. To learn how all
    these algorithms work for classification and regression, see the links to videos
    and reading material in appendix C that explain both cases in detail.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些模型都适用于回归和分类。为了教育目的，我们将前两个作为分类模型学习，后两个作为回归模型学习。分类和回归的过程类似。然而，请阅读每个模型，并想象它们在两种情况下是如何工作的。要了解所有这些算法在分类和回归中的工作方式，请参阅附录C中链接的视频和阅读材料，其中详细解释了这两种情况。
- en: 'Bagging: Joining some weak learners randomly to build a strong learner'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成：随机地将一些弱学习器组合起来构建强学习器
- en: 'In this section we see one of the most well-known bagging models: a *random
    forest*. In a random forest, the weak learners are small decision trees trained
    on random subsets of the dataset. Random forests work well for classification
    and regression problems, and the process is similar. We will see random forests
    in a classification example. The code for this section follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到最著名的集成模型之一：**随机森林**。在随机森林中，弱学习器是在数据集的随机子集上训练的小决策树。随机森林在分类和回归问题中都表现良好，其过程类似。我们将通过一个分类示例来了解随机森林。本节的代码如下：
- en: '**Notebook**: Random_forests_and_AdaBoost.ipynb'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**笔记本**：Random_forests_and_AdaBoost.ipynb'
- en: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_12_Ensemble_Methods/Random_forests_and_AdaBoost.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_12_Ensemble_Methods/Random_forests_and_AdaBoost.ipynb)'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_12_Ensemble_Methods/Random_forests_and_AdaBoost.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_12_Ensemble_Methods/Random_forests_and_AdaBoost.ipynb)'
- en: We use a small dataset of spam and ham emails, similar to the one we used in
    chapter 8 with the naive Bayes model. The dataset is shown in table 12.1 and plotted
    in figure 12.2\. The features of the dataset are the number of times the words
    “lottery” and “sale” appear in the email, and the “yes/no” label indicates whether
    the email is spam (yes) or ham (no).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个小的垃圾邮件和正常邮件数据集，类似于我们在第8章中使用朴素贝叶斯模型时使用的数据集。数据集显示在表12.1中，并在图12.2中绘制。数据集的特征是电子邮件中“彩票”和“销售”单词出现的次数，而“是/否”标签表示电子邮件是否为垃圾邮件（是）或正常邮件（否）。
- en: Table 12.1 Table of spam and ham emails, together with the number of appearances
    of the words “lottery” and “sale” on each email
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.1 邮件垃圾邮件和正常邮件表，以及每个邮件中“彩票”和“销售”单词出现的次数
- en: '| Lottery | Sale | Spam |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| Lottery | Sale | Spam |'
- en: '| 7 | 8 | 1 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 8 | 1 |'
- en: '| 3 | 2 | 0 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2 | 0 |'
- en: '| 8 | 4 | 1 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 4 | 1 |'
- en: '| 2 | 6 | 0 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 6 | 0 |'
- en: '| 6 | 5 | 1 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 5 | 1 |'
- en: '| 9 | 6 | 1 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 6 | 1 |'
- en: '| 8 | 5 | 0 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 5 | 0 |'
- en: '| 7 | 1 | 0 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 1 | 0 |'
- en: '| 1 | 9 | 1 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 9 | 1 |'
- en: '| 4 | 7 | 0 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 7 | 0 |'
- en: '| 1 | 3 | 0 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 3 | 0 |'
- en: '| 3 | 10 | 1 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 10 | 1 |'
- en: '| 2 | 2 | 1 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 | 1 |'
- en: '| 9 | 3 | 0 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 3 | 0 |'
- en: '| 5 | 3 | 0 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 3 | 0 |'
- en: '| 10 | 1 | 0 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 1 | 0 |'
- en: '| 5 | 9 | 1 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 9 | 1 |'
- en: '| 10 | 8 | 1 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 8 | 1 |'
- en: '![](../Images/12-2.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12-2.png)'
- en: Figure 12.2 Figure 12.2  The plot of the dataset in table 12.1\. Spam emails
    are represented by triangles and ham emails by squares. The horizontal and vertical
    axes represent the number of appearances of the words “lottery” and “sale,” respectively.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 图12.2  表12.1中数据集的绘图。垃圾邮件用三角形表示，正常邮件用正方形表示。水平和垂直轴分别表示“彩票”和“销售”单词出现的次数。
- en: First, (over)fitting a decision tree
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，对决策树进行（过度）拟合
- en: Before we get into random forests, let’s fit a decision tree classifier to this
    data and see how well it performs. Because we’ve learned this in chapter 9, figure
    12.3 shows only the final result, but we can see the code in the notebook. On
    the left of figure 12.3, we can see the actual tree (quite deep!), and on the
    right, we can see the plot of the boundary. Notice that it fits the dataset very
    well, with a 100% training accuracy, although it clearly overfits. The overfitting
    can be noticed on the two outliers that the model tries to classify correctly,
    without noticing they are outliers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解随机森林之前，让我们将决策树分类器拟合到这些数据上，看看它的表现如何。因为我们已经在第9章中学过这个内容，图12.3只显示了最终结果，但我们可以在笔记本中看到代码。在图12.3的左侧，我们可以看到实际的树（相当深！），在右侧，我们可以看到由这个决策树定义的边界。注意，它很好地拟合了数据集，训练准确率达到100%，尽管它显然过拟合了。过拟合可以在模型试图正确分类的两个异常值上观察到，而没有注意到它们是异常值。
- en: '![](../Images/12-3.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-3.png)'
- en: 'Figure 12.3 Left: A decision tree that classifies our dataset. Right: The boundary
    defined by this decision tree. Notice that it splits the data very well, although
    it hints at overfitting, because a good model would treat the two isolated points
    as outliers, instead of trying to classify them correctly.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3 左：一个分类我们的数据集的决策树。右：由这个决策树定义的边界。注意，它很好地分割了数据，尽管它暗示了过拟合，因为一个好的模型会将两个孤立点视为异常值，而不是试图正确地分类它们。
- en: In the next sections, we see how to solve this overfitting problem by fitting
    a random forest.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将看到如何通过拟合随机森林来解决这个过拟合问题。
- en: Fitting a random forest manually
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 手动拟合随机森林
- en: In this section, we learn how to fit a random forest manually, although this
    is only for educational purposes, because this is not the way to do it in practice.
    In a nutshell, we pick random subsets from our dataset and train a weak learner
    (decision tree) on each one of them. Some data points may belong to several subsets,
    and others may belong to none. The combination of them is our strong learner.
    The way the strong learner makes predictions is by letting the weak learners vote.
    For this dataset, we use three weak learners. Because the dataset has 18 points,
    let’s consider three subsets of 6 data points each, as shown in figure 12.4.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何手动拟合随机森林，尽管这只是为了教育目的，因为在实际操作中并不是这样做的。简而言之，我们从数据集中随机选择子集，并在每个子集上训练一个弱学习器（决策树）。一些数据点可能属于多个子集，而另一些可能不属于任何子集。这些子集的组合构成了我们的强学习器。强学习器进行预测的方式是通过让弱学习器进行投票。对于这个数据集，我们使用三个弱学习器。因为数据集有18个点，所以让我们考虑三个包含6个数据点的子集，如图12.4所示。
- en: '![](../Images/12-4.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-4.png)'
- en: Figure 12.4 The first step to build a random forest is to split our data into
    three subsets. This is a splitting of the dataset shown in figure 12.2.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4 构建随机森林的第一步是将我们的数据分成三个子集。这是图12.2中所示的数据集分割。
- en: Next, we proceed to build our three weak learners. Fit a decision tree of depth
    1 on each of these subsets. Recall from chapter 9 that a decision tree of depth
    1 contains only one node and two leaves. Its boundary consists of a single horizontal
    or vertical line that splits the dataset as best as possible. The weak learners
    are illustrated in figure 12.5.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们继续构建我们的三个弱学习器。在每个子集上拟合一个深度为1的决策树。回想一下第9章的内容，深度为1的决策树只包含一个节点和两个叶子。它的边界由一条尽可能好地分割数据集的单个水平或垂直线组成。弱学习器如图12.5所示。
- en: '![](../Images/12-5.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-5.png)'
- en: Figure 12.5 The three weak learners that form our random forest are decision
    trees of depth 1\. Each decision tree fits one of the corresponding three subsets
    from figure 12.4.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5 形成我们的随机森林的三个弱学习器都是深度为1的决策树。每个决策树拟合图12.4中对应的三个子集之一。
- en: We combine these into a strong learner by voting. In other words, for any input,
    each of the weak learners predicts a value of 0 or 1\. The prediction the strong
    learner makes is the most common output of the three. This combination can be
    seen in figure 12.6, where the weak learners are on the top and the strong learner
    on the bottom.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过投票将这些组合成一个强学习器。换句话说，对于任何输入，每个弱学习器预测一个0或1的值。强学习器做出的预测是三个中最常见的输出。这种组合可以在图12.6中看到，其中弱学习器在顶部，强学习器在底部。
- en: Note that the random forest is a good classifier, because it classifies most
    of the points correctly, but it allows a few mistakes in order to not overfit
    the data. However, we don’t need to train these random forests manually, because
    Scikit-Learn has functions for this, which we see in the next section.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，随机森林是一个好的分类器，因为它正确分类了大多数点，但它允许一些错误，以避免过度拟合数据。然而，我们不需要手动训练这些随机森林，因为Scikit-Learn有相应的函数，我们将在下一节中看到。
- en: '![](../Images/12-6.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-6.png)'
- en: Figure 12.6 The way to obtain the predictions of the random forest is by combining
    the predictions of the three weak learners. On the top, we can see the three boundaries
    of the decision trees from figure 12.5\. On the bottom, we can see how the three
    decision trees vote to obtain the boundary of the corresponding random forest.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6 获得随机森林预测的方法是通过结合三个弱学习者的预测。在顶部，我们可以看到图12.5中的三个决策树边界。在底部，我们可以看到三个决策树如何投票以获得相应随机森林的边界。
- en: Training a random forest in Scikit-Learn
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scikit-Learn中训练随机森林
- en: 'In this section, we see how to train a random forest using Scikit-Learn. In
    the following code, we make use of the `RandomForestClassifier` package. To begin,
    we have our data in two Pandas DataFrames called `features` and `labels`, as shown
    next:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了如何使用Scikit-Learn训练随机森林。在下面的代码中，我们使用了`RandomForestClassifier`包。首先，我们的数据存储在两个名为`features`和`labels`的Pandas
    DataFrame中，如下所示：
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the previous code, we specified that we want five weak learners with the
    `n_estimators` hyperparameter. These weak learners are again decision trees, and
    we have specified that their depth is 1 with the `max_depth` hyperparameter. The
    plot of the model is shown in figure 12.7\. Note how this model makes some mistakes
    but manages to find a good boundary, where the spam emails are those with a lot
    of appearances of the words “lottery” and “sale” (top right of the plot) and the
    ham emails are those with not many appearances of these words (bottom left of
    the figure).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，我们指定了想要五个弱学习者，使用`n_estimators`超参数。这些弱学习者再次是决策树，我们使用`max_depth`超参数指定了它们的深度为1。模型的图示如图12.7所示。注意这个模型犯了一些错误，但设法找到了一个好的边界，其中垃圾邮件是那些有很多“彩票”和“销售”等词语出现的邮件（图示的右上角），而ham邮件是那些这些词语出现不多的邮件（图示的左下角）。
- en: '![](../Images/12-7.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-7.png)'
- en: Figure 12.7 The boundary of the random forest obtained with Scikit-Learn. Notice
    that it classifies the dataset well, and it treats the two misclassified points
    as outliers, instead of trying to classify them correctly.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7 使用Scikit-Learn获得的随机森林边界。请注意，它很好地分类了数据集，并将两个误分类的点视为异常值，而不是尝试正确分类它们。
- en: Scikit-Learn also allows us to visualize and plot the individual weak learners
    (see the notebook for the code). The weak learners are shown in figure 12.8\.
    Notice that not all the weak learners are useful. For instance, the first one
    classifies every point as ham.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn还允许我们可视化和绘制单个弱学习者（请参阅笔记本中的代码）。弱学习者如图12.8所示。请注意，并非所有弱学习者都有用。例如，第一个将每个点分类为ham。
- en: '![](../Images/12-8.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-8.png)'
- en: Figure 12.8 The random forest is formed by five weak learners obtained using
    Scikit-Learn. Each one is a decision tree of depth 1\. They combine to form the
    strong learner shown in figure 12.7.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.8 通过Scikit-Learn获得的五个弱学习者组成的随机森林。每个都是一个深度为1的决策树。它们组合形成了图12.7中显示的强学习者。
- en: In this section, we used decision trees of depth 1 as weak learners, but in
    general, we can use trees of any depth we want. Try retraining this model using
    decision trees of higher depth by varying the `max_depth` hyperparameter, and
    see what the random forest looks like!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用了深度为1的决策树作为弱学习者，但通常，我们可以使用任何深度的树。尝试通过改变`max_depth`超参数重新训练此模型，使用更深的决策树，看看随机森林会是什么样子！
- en: 'AdaBoost: Joining weak learners in a clever way to build a strong learner'
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdaBoost：以巧妙的方式结合弱学习者以构建强学习者
- en: Boosting is similar to bagging in that we join several weak learners to build
    a strong learner. The difference is that we don’t select the weak learners at
    random. Instead, each learner is built by focusing on the weaknesses of the previous
    learners. In this section, we learn a powerful boosting technique called AdaBoost,
    developed by Freund and Schapire in 1997 (see appendix C for the reference). AdaBoost
    is short for adaptive boosting, and it works for regression and classification.
    However, we will use it in a classification example that illustrates the training
    algorithm very clearly.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 提升与袋装类似，我们通过结合多个弱学习器来构建一个强学习器。不同之处在于，我们不是随机选择弱学习器。相反，每个学习器都是通过关注先前学习器的弱点来构建的。在本节中，我们将学习一种由Freund和Schapire于1997年开发的强大提升技术，称为AdaBoost（参见附录C获取参考文献）。AdaBoost代表自适应提升，它适用于回归和分类。然而，我们将在一个分类示例中使用它，该示例非常清楚地说明了训练算法。
- en: 'In AdaBoost, like in random forests, each weak learner is a decision tree of
    depth 1\. Unlike random forests, each weak learner is trained on the whole dataset,
    rather than on a portion of it. The only caveat is that after each weak learner
    is trained, we modify the dataset by enlarging the points that have been incorrectly
    classified, so that future weak learners pay more attention to these. In a nutshell,
    AdaBoost works as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在AdaBoost中，就像在随机森林中一样，每个弱学习器都是深度为1的决策树。与随机森林不同，每个弱学习器都是在整个数据集上训练的，而不是在数据集的一部分上。唯一的注意事项是，在每个弱学习器训练后，我们通过扩大被错误分类的点来修改数据集，这样未来的弱学习器就会更加关注这些点。简而言之，AdaBoost的工作原理如下：
- en: Pseudocode for training an AdaBoost model
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 训练AdaBoost模型的伪代码
- en: Train the first weak learner on the first dataset.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一个数据集上训练第一个弱学习器。
- en: 'Repeat the following step for each new weak learner:'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对每个新的弱学习器重复以下步骤：
- en: 'After a weak learner is trained, the points are modified as follows:'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练完一个弱学习器后，点被修改如下：
- en: The points that are incorrectly classified are enlarged.
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 被错误分类的点被放大。
- en: Train a new weak learner on this modified dataset.
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个修改后的数据集上训练一个新的弱学习器。
- en: In this section, we develop this pseudocode in more detail over an example.
    The dataset we use has two classes (triangles and squares) and is plotted in figure
    12.9.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过一个示例更详细地开发这个伪代码。我们使用的这个数据集有两个类别（三角形和正方形），如图12.9所示。
- en: '![](../Images/12-9.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图12-9](../Images/12-9.png)'
- en: Figure 12.9 The dataset that we will classify using AdaBoost. It has two labels
    represented by a triangle and a square.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.9 我们将使用AdaBoost进行分类的数据集。它有两个标签，分别由三角形和正方形表示。
- en: 'A big picture of AdaBoost: Building the weak learners'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost的总体图景：构建弱学习器
- en: Over the next two subsections, we see how to build an AdaBoost model to fit
    the dataset shown in figure 12.9\. First we build the weak learners that we’ll
    then combine into one strong learner.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个小节中，我们将看到如何构建一个AdaBoost模型来拟合图12.9中所示的数据集。首先，我们构建将组合成一个强学习器的弱学习器。
- en: The first step is to assign to each of the points a weight of 1, as shown on
    the left of figure 12.10\. Next, we build a weak learner on this dataset. Recall
    that the weak learners are decision trees of depth 1\. A decision tree of depth
    1 corresponds to the horizontal or vertical line that best splits the points.
    Several such trees do the job, but we’ll pick one—the vertical line illustrated
    in the middle of figure 12.10—which correctly classifies the two triangles to
    its left and the five squares to its right, and incorrectly classifies the three
    triangles to its right. The next step is to enlarge the three incorrectly classified
    points to give them more importance under the eyes of future weak learners. To
    enlarge them, recall that each point initially has a weight of 1\. We define the
    *rescaling factor* of this weak learner as the number of correctly classified
    points divided by the number of incorrectly classified points. In this case, the
    rescaling factor is 7/3 = 2.33\. We proceed to rescale every misclassified point
    by this rescaling factor, as illustrated on the right of figure 12.10.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将每个点的权重分配为1，如图12.10的左侧所示。接下来，我们在该数据集上构建一个弱学习器。回想一下，弱学习器是深度为1的决策树。深度为1的决策树对应于最佳分割点的水平或垂直线。有几个这样的树可以完成这项工作，但我们将选择一个——图12.10中间所示的垂直线，它正确分类了左侧的两个三角形和右侧的五个正方形，并错误分类了右侧的三个三角形。下一步是将三个错误分类的点放大，以便在未来的弱学习器眼中赋予它们更多的重视。为了放大它们，回想一下，每个点最初有一个权重为1。我们定义这个弱学习器的*缩放因子*为正确分类的点数除以错误分类的点数。在这种情况下，缩放因子是7/3
    = 2.33。我们继续通过这个缩放因子将每个错误分类的点进行缩放，如图12.10的右侧所示。
- en: '![](../Images/12-10.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-10.png)'
- en: 'Figure 12.10 Fitting the first weak learner of the AdaBoost model. Left: The
    dataset, where each point gets assigned a weight of 1\. Middle: A weak learner
    that best fits this dataset. Right: The rescaled dataset, where we have enlarged
    the misclassified points by a rescaling factor of 7/3.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.10 AdaBoost模型第一个弱学习器的拟合。左侧：数据集，其中每个点被分配一个权重为1。中间：最佳拟合此数据集的弱学习器。右侧：缩放后的数据集，我们通过7/3的缩放因子放大了错误分类的点。
- en: Now that we’ve built the first weak learner, we build the next ones in the same
    manner. The second weak learner is illustrated in figure 12.11\. On the left of
    the figure, we have the rescaled dataset. The second weak learner is one that
    fits this dataset best. What do we mean by that? Because points have different
    weights, we want the weak learner for which the sum of the weights of the correctly
    classified points is the highest. This weak learner is
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了第一个弱学习器，我们以同样的方式构建下一个。第二个弱学习器如图12.11所示。图象的左侧是缩放后的数据集。第二个弱学习器是最佳拟合这个数据集的弱学习器。我们这是什么意思呢？因为点有不同的权重，我们希望弱学习器中正确分类的点的权重之和是最高的。这个弱学习器是
- en: the horizontal line in the middle of figure 12.11\. We now proceed to calculate
    the rescaling factor. We need to slightly modify its definition, because the points
    now have weights. The rescaling factor is the ratio between the sum of the weights
    of the correctly classified points and the sum of the weights of the incorrectly
    classified points. The first term is 2.33 + 2.33 + 2.33 + 1 + 1 + 1 + 1 = 11,
    and the second is 1 + 1 + 1 = 3\. Thus, the rescaling factor is 11/3 = 3.67\.
    We proceed to multiply the weights of the three misclassified points by this factor
    of 3.67, as illustrated on the right of figure 12.11.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.11中间的水平线。我们现在继续计算缩放因子。我们需要稍微修改其定义，因为现在点有权重。缩放因子是正确分类的点的权重之和与错误分类的点的权重之和的比率。第一个项是2.33
    + 2.33 + 2.33 + 1 + 1 + 1 + 1 = 11，第二个是1 + 1 + 1 = 3。因此，缩放因子是11/3 = 3.67。我们继续将三个错误分类的点的权重乘以这个3.67的因子，如图12.11的右侧所示。
- en: '![](../Images/12-111.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-111.png)'
- en: 'Figure 12.11 Fitting the second weak learner of the AdaBoost model. Left: The
    rescaled dataset from figure 12.10\. Middle: A weak learner that best fits the
    rescaled dataset—this means, the weak learner for which the sum of weights of
    the correctly classified points is the largest. Right: The new rescaled dataset,
    where we have enlarged the misclassified points by a rescaling factor of 11/3.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.11 AdaBoost模型第二个弱学习器的拟合。左侧：图12.10中的缩放后的数据集。中间：最佳拟合缩放后数据集的弱学习器——这意味着，对于正确分类的点的权重之和最大的弱学习器。右侧：新的缩放后的数据集，我们通过11/3的缩放因子放大了错误分类的点。
- en: We continue in this fashion until we’ve built as many weak learners as we want.
    For this example, we build only three weak learners. The third weak learner is
    a vertical line, illustrated in figure 12.12.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续这样下去，直到我们构建了我们想要的那么多弱学习器。对于这个例子，我们只构建了三个弱学习器。第三个弱学习器是一条垂直线，如图12.12所示。
- en: '![](../Images/12-12a.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-12a.png)'
- en: 'Figure 12.12 Fitting the third weak learner of the AdaBoost model. Left: The
    rescaled dataset from figure 12.11\. Right: A weak learner that best fits this
    rescaled dataset.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.12 AdaBoost模型中第三个弱学习器的拟合。左：来自图12.11的缩放数据集。右：最适合此缩放数据集的弱学习器。
- en: This is how we build the weak learners. Now, we need to combine them into a
    strong learner. This is similar to what we did with random forests, but using
    a little more math, as shown in the next section.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们构建弱学习器的方法。现在，我们需要将它们组合成一个强学习器。这与我们使用随机森林所做的方法类似，但需要使用更多的数学知识，如下一节所示。
- en: Combining the weak learners into a strong learner
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 将弱学习器组合成强学习器
- en: Now that we’ve built the weak learners, in this section, we learn an effective
    way to combine them into a strong learner. The idea is to get the classifiers
    to vote, just as they did in the random forest classifier, but this time, good
    learners get more of a say than poor learners. In the event that a classifier
    is *really* bad, then its vote will actually be negative.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了弱学习器，在本节中，我们学习了一种有效的方法将它们组合成一个强学习器。想法是让分类器进行投票，就像它们在随机森林分类器中所做的那样，但这次，好的学习器比差的学习器有更多的发言权。如果分类器真的很差，那么它的投票实际上将是负数。
- en: 'To understand this, imagine we have three friends: Truthful Teresa, Unpredictable
    Umbert, and Lying Lenny. Truthful Teresa almost always tells the truth, Lying
    Lenny almost always lies, and Unpredictable Umbert says the truth roughly half
    of the time and lies the other half. Out of these three friends, which is the
    least useful one?'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这一点，想象我们有三个朋友：真诚的特蕾莎、不可预测的乌尔伯特和说谎的伦尼。真诚的特蕾莎几乎总是说实话，说谎的伦尼几乎总是撒谎，不可预测的乌尔伯特大约一半的时间说实话，另一半时间撒谎。在这三个朋友中，哪一个是最没有用的？
- en: The way I see it, Truthful Teresa is very reliable, because she almost always
    tells the truth, so we can trust her. Among the other two, I prefer Lying Lenny.
    If he almost always lies when we ask him a yes-or-no question, we simply take
    as truth the opposite of what he tells us, and we’ll be correct most of the time!
    On the other hand, Unpredictable Umbert serves us no purpose if we have no idea
    whether he’s telling the truth or lying. In that case, if we were to assign a
    score to what each friend says, I’d give Truthful Teresa a high positive score,
    Lying Lenny a high negative score, and Unpredictable Umbert a score of zero.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为，真诚的特蕾莎非常可靠，因为她几乎总是说实话，所以我们可以信任她。在其他人中，我更喜欢说谎的伦尼。如果他几乎总是在我们问他一个是非问题时撒谎，我们就简单地接受他告诉我们的相反事实，我们大多数时候都会是正确的！另一方面，如果我们不知道不可预测的乌尔伯特是在说实话还是在撒谎，那么他对我们来说就没有任何用处。在这种情况下，如果我们给每个朋友说的话分配一个分数，我会给真诚的特蕾莎一个很高的正分，给说谎的伦尼一个很高的负分，给不可预测的乌尔伯特一个零分。
- en: 'Now imagine that our three friends are weak learners trained in a dataset with
    two classes. Truthful Teresa is a classifier with very high accuracy, Lying Lenny
    is one with very low accuracy, and Unpredictable Umbert is one with an accuracy
    close to 50%. We want to build a strong learner where the prediction is obtained
    by a weighted vote from the three weak learners. Thus, to each of the weak learners,
    we assign a score, and that is how much the vote of the learner will count in
    the final vote. Furthermore, we want to assign these scores in the following way:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象我们的三个朋友是在一个有两个类别的数据集上训练的弱学习器。真诚的特蕾莎是一个准确率非常高的分类器，说谎的伦尼是一个准确率非常低的分类器，不可预测的乌尔伯特是一个准确率接近50%的分类器。我们想要构建一个强学习器，其预测是通过三个弱学习器的加权投票得到的。因此，我们给每个弱学习器分配一个分数，这就是学习者的投票在最终投票中将占多少权重。此外，我们想要以下方式分配这些分数：
- en: The Truthful Teresa classifier gets a high positive score.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真诚的特蕾莎分类器得到一个很高的正分。
- en: The Unpredictable Umbert classifier gets a score close to zero.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不可预测的乌尔伯特分类器得到一个接近零的分数。
- en: The Lying Lenny classifier gets a high negative score.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 说谎的伦尼分类器得到一个很高的负分。
- en: 'In other words, the score of a weak learner is a number that has the following
    properties:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，弱学习器的分数是一个具有以下属性的数字：
- en: Is positive when the accuracy of the learner is greater than 0.5
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当学习器的准确率大于0.5时为正数
- en: Is 0 when the accuracy of the model is 0.5
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当模型的准确率为0.5时为0
- en: Is negative when the accuracy of the learner is smaller than 0.5
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当学习者的准确度小于0.5时，它是负数
- en: Is a large positive number when the accuracy of the learner is close to 1
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当学习者的准确度接近1时，它是一个大的正数
- en: Is a large negative number when the accuracy of the learner is close to 0
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当学习者的准确度接近0时，它是一个大的负数
- en: To come up with a good score for a weak learner that satisfies properties 1–5
    above, we use a popular concept in probability called the *logit*, or *log-odds*,
    which we discuss next.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到一个满足上述1-5个特性的弱学习者的良好得分，我们使用概率论中的一个流行概念，称为*logit*或*对数概率*，我们将在下面讨论。
- en: Probability, odds, and log-odds
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 概率、赔率和对数概率
- en: 'You may have seen in gambling that probabilities are never mentioned, but they
    always talk about *odds*. What are these odds? They are similar to probability
    in the following sense: if we run an experiment many times and record the number
    of times a particular outcome occurred, the probability of this outcome is the
    number of times it occurred divided by the total number of times we ran the experiment.
    The odds of this outcome are the number of times it occurred divided by the number
    of times it didn’t occur.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在赌博中看到过，人们从不提及概率，但他们总是谈论*赔率*。这些赔率是什么？在以下意义上，它们与概率相似：如果我们多次运行一个实验并记录特定结果发生的次数，那么这个结果发生的概率是它发生的次数除以我们运行实验的总次数。这个结果的对数概率是它发生的次数除以它没有发生的次数。
- en: 'For example, the probability of obtaining 1 when we roll a die is 1/6, but
    the odds are 1/5\. If a particular horse wins 3 out of every 4 races, then the
    probability of that horse winning a race is 3/4, and the odds are 3/1 = 3\. The
    formula for odds is simple: if the probability of an event is *x*, then the odds
    are ![](../Images/12_12_E01.png). For instance, in the dice example, the probability
    is 1/6 and the odds are'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，掷骰子得到1的概率是1/6，但赔率是1/5。如果一匹马在每四场比赛中赢三场，那么这匹马赢得比赛的概率是3/4，赔率是3/1=3。赔率的公式很简单：如果事件的概率是*x*，那么赔率是![](../Images/12_12_E01.png)。例如，在骰子例子中，概率是1/6，赔率是
- en: '![](../Images/12_12_E02.png).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/12_12_E02.png).'
- en: Notice that because the probability is a number between 0 and 1, then the odds
    are a number between 0 and ∞.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，因为概率是一个介于0和1之间的数，所以赔率是一个介于0和∞之间的数。
- en: 'Now let’s get back to our original goal. We are looking for a function that
    satisfies properties 1–5 above. The odds function is close, but not quite there,
    because it outputs only positive values. The way to turn the odds into a function
    that satisfies properties 1–5 above is by taking the logarithm. Thus, we obtain
    the log-odds, also called the logit, defined as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回到我们的原始目标。我们正在寻找一个满足上述1-5个特性的函数。概率函数很接近，但还不够，因为它只输出正值。将概率转换为满足上述1-5个特性的函数的方法是取对数。因此，我们得到了对数概率，也称为logit，其定义如下：
- en: '![](../Images/12_12_E03.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12_12_E03.png)'
- en: Figure 12.13 shows the graph of the log-odds function ![](../Images/12_12_E04.png).
    Notice that this function satisfies properties 1–5.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.13显示了对数概率函数的图像![](../Images/12_12_E04.png)。请注意，这个函数满足1-5个特性。
- en: Therefore, all we need to do is use the log-odds function to calculate the score
    of each of the weak learners. We apply this log-odds function on the accuracy.
    Table 12.2 contains several values for the accuracy of a weak learner and the
    log-odds of this accuracy. Notice that, as desired, models with high accuracy
    have high positive scores, models with low accuracy have high negative scores,
    and models with accuracy close to 0.5 have scores close to 0.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们所需做的就是使用对数概率函数来计算每个弱学习者的得分。我们将此对数概率函数应用于准确度。表12.2包含了一些弱学习者的准确度和该准确度的对数概率。请注意，正如预期的那样，准确度高的模型具有高的正得分，准确度低的模型具有高的负得分，而准确度接近0.5的模型得分接近0。
- en: '![](../Images/12-13.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12-13.png)'
- en: Figure 12.13 The curve shows the plot of the log-odds function with respect
    to the accuracy. Notice that for small values of the accuracy, the log-odds is
    a very large negative number, and for higher values of the accuracy, it is a very
    large positive number. When the accuracy is 50% (or 0.5), the log-odds is precisely
    zero.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.13 该曲线显示了对数概率函数相对于准确度的图像。请注意，对于准确度较小的值，对数概率是一个非常大的负数，而对于准确度较高的值，它是一个非常大的正数。当准确度为50%（或0.5）时，对数概率恰好为零。
- en: Table 12.2 Several values for the accuracy of a weak classifier, with the corresponding
    score, calculated using the log-odds. Notice that the models with very low accuracy
    get large negative scores, the values with very high accuracy get large positive
    scores, and the values with accuracy close to 0.5 get scores close to 0.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.2展示了弱分类器的几个准确度值，以及使用对数几率计算出的相应分数。请注意，准确度非常低的模型会得到很大的负分数，准确度非常高的值会得到很大的正分数，而接近0.5的准确度值会得到接近0的分数。
- en: '| Accuracy | Log-odds (score of the weak learner) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 | 对数几率（弱学习器的分数） |'
- en: '| 0.01 | –4.595 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 0.01 | –4.595 |'
- en: '| 0.1 | –2.197 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 0.1 | –2.197 |'
- en: '| 0.2 | –1.386 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 0.2 | –1.386 |'
- en: '| 0.5 | 0 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 | 0 |'
- en: '| 0.8 | 1.386 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 0.8 | 1.386 |'
- en: '| 0.9 | 2.197 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 0.9 | 2.197 |'
- en: '| 0.99 | 4.595 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 0.99 | 4.595 |'
- en: Combining the classifiers
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 组合分类器
- en: Now that we’ve settled on the log-odds as the way to define the scores for all
    the weak learners, we can proceed to join them to build the strong learner. Recall
    that the accuracy of a weak learner is the sum of the scores of the correctly
    classified points divided by the sum of the scores of all the points, as shown
    in figures 12.10–12.12.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经确定对数几率是定义所有弱学习器分数的方法，我们就可以继续将它们组合起来构建强学习器。回想一下，弱学习器的准确度是正确分类的点分数总和除以所有点分数总和，如图12.10-12.12所示。
- en: 'Weak learner 1:'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弱学习器1：
- en: '![](../Images/12_13_E01.png)'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../Images/12_13_E01.png)'
- en: 'Weak learner 2:'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弱学习器2：
- en: '![](../Images/12_13_E02.png)'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../Images/12_13_E02.png)'
- en: 'Weak learner 3:'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弱学习器3：
- en: '![](../Images/12_13_E03.png)'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../Images/12_13_E03.png)'
- en: The prediction that the strong learner makes is obtained by the weighted vote
    of the weak classifiers, where each classifier’s vote is its score. A simple way
    to see this is to change the predictions of the weak learners from 0 and 1 to
    –1 and 1, multiplying each prediction by the score of the weak learner, and adding
    them. If the resulting prediction is greater than or equal to zero, then the strong
    learner predicts a 1, and if it is negative, then it predicts a 0\. The voting
    process is illustrated in figure 12.14, and the predictions in figure 12.15\.
    Notice also in figure 12.15 that the resulting classifier classified every point
    in the dataset correctly.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 强学习器的预测是通过弱分类器的加权投票得到的，其中每个分类器的投票是其分数。一个简单的方法是将弱学习器的预测值从0和1改为-1和1，并将每个预测值乘以弱学习器的分数，然后相加。如果得到的预测值大于或等于零，则强学习器预测为1，如果为负，则预测为0。投票过程如图12.14所示，预测结果如图12.15所示。注意，在图12.15中，得到的分类器正确地分类了数据集中的每个点。
- en: '![](../Images/12-141.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-141.png)'
- en: Figure 12.14 How to combine the weak learners into a strong learner in the AdaBoost
    model. We score each of the weak learners using the log-odds and make them vote
    based on their scores (the larger the score, the more voting power that particular
    learner has). Each of the regions in the bottom diagram has the sum of the scores
    of the weak learners. Note that to simplify our calculations, the predictions
    from the weak learners are +1 and –1, instead of 1 and 0.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.14展示了如何在AdaBoost模型中将弱学习器组合成一个强学习器。我们使用对数几率对每个弱学习器进行评分，并根据它们的评分进行投票（评分越高，该学习器的投票权重越大）。底部图中的每个区域都包含弱学习器评分的总和。请注意，为了简化计算，弱学习器的预测值是+1和-1，而不是1和0。
- en: '![](../Images/12-151.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-151.png)'
- en: Figure 12.15 How to obtain the predictions for the AdaBoost model. Once we have
    added the scores coming from the weak learners (shown in figure 12.14), we assign
    a prediction of 1 if the sum of scores is greater than or equal to 0 and a prediction
    of 0 otherwise.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.15展示了如何获得AdaBoost模型的预测。一旦我们添加了来自弱学习器的分数（如图12.14所示），如果分数总和大于或等于0，则分配预测值为1，否则分配预测值为0。
- en: Coding AdaBoost in Scikit-Learn
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scikit-Learn中编码AdaBoost
- en: 'In this section, we see how to use Scikit-Learn to train an AdaBoost model.
    We train it on the same spam email dataset that we used in the section “Fitting
    a random forest manually” and plotted in figure 12.16\. We continue using the
    following notebook from the previous sections:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示如何使用Scikit-Learn训练一个AdaBoost模型。我们使用与“手动拟合随机森林”部分相同的垃圾邮件数据集进行训练，并在图12.16中进行了绘制。我们继续使用上一节中的以下笔记本：
- en: '**Notebook**: Random_forests_and_AdaBoost.ipynb'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**笔记**：Random_forests_and_AdaBoost.ipynb'
- en: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_12_Ensemble_Methods/Random_forests_and_AdaBoost.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_12_Ensemble_Methods/Random_forests_and_AdaBoost.ipynb)'
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_12_Ensemble_Methods/Random_forests_and_AdaBoost.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_12_Ensemble_Methods/Random_forests_and_AdaBoost.ipynb)'
- en: '![](../Images/12-161.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-161.png)'
- en: Figure 12.16 In this dataset, we train an AdaBoost classifier using Scikit-Learn.
    This is the same spam dataset from the section “Bagging,” where the features are
    the number of appearances of the words “lottery” and “spam,” and the spam emails
    are represented by triangles and the ham emails by squares.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.16 在此数据集中，我们使用Scikit-Learn训练一个AdaBoost分类器。这是来自“Bagging”部分的相同垃圾邮件数据集，其中特征是单词“彩票”和“垃圾邮件”出现的次数，垃圾邮件用三角形表示，正常邮件用正方形表示。
- en: 'The dataset is in two Pandas DataFrames called `features` and `labels`. The
    training is done using the `AdaBoostClassifier` package in Scikit-Learn. We specify
    that this model will use six weak learners with the `n_estimators` hyperparameter,
    as shown next:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含两个Pandas DataFrame，分别称为`features`和`labels`。训练使用Scikit-Learn中的`AdaBoostClassifier`包完成。我们指定此模型将使用六个弱学习器，如以下所示：
- en: '[PRE1]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The boundary of the resulting model is plotted in figure 12.17.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 结果模型的边界在图12.17中绘制。
- en: We can go a bit further and explore the six weak learners and their scores (see
    notebook for the code). Their boundaries are plotted in figure 12.18, and as is
    evident in the notebook, the scores of all the weak learners are 1.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以更进一步，探索六个弱学习者和它们的分数（参见笔记本中的代码）。它们的边界在图12.18中绘制，并且正如笔记本中所示，所有弱学习者的分数都是1。
- en: '![](../Images/12-17.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-17.png)'
- en: Figure 12.17 The result of the AdaBoost classifier on the spam dataset in figure
    12.16\. Notice that the classifier does a good job fitting the dataset and doesn’t
    overfit much.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.17 AdaBoost分类器在图12.16的垃圾邮件数据集上的结果。请注意，分类器很好地拟合了数据集，并且没有过度拟合太多。
- en: '![](../Images/12-18.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-18.png)'
- en: Figure 12.18 The six weak learners in our AdaBoost model. Each one of them is
    a decision tree of depth 1\. They combine into the strong learner in figure 12.17.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.18 我们AdaBoost模型中的六个弱学习器。每一个都是深度为1的决策树。它们组合成图12.17中的强学习器。
- en: Note that the strong learner in figure 12.17 is obtained by assigning a score
    of 1 to each of the weak learners in figure 12.18 and letting them vote.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，图12.17中的强学习器是通过将图12.18中每个弱学习者的分数设为1，并让它们投票得到的。
- en: 'Gradient boosting: Using decision trees to build strong learners'
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升：使用决策树构建强大的学习器
- en: 'In this section, we discuss gradient boosting, one of the most popular and
    successful machine learning models currently. Gradient boosting is similar to
    AdaBoost, in that the weak learners are decision trees, and the goal of each weak
    learner is to learn from the mistakes of the previous ones. One difference between
    gradient boosting and AdaBoost is that in gradient boosting, we allow decision
    trees of depth more than 1\. Gradient boosting can be used for regression and
    classification, but for clarity, we use a regression example. To use it for classification,
    we need to make some small tweaks. To find out more about this, check out links
    to videos and reading material in appendix C. The code for this section follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论梯度提升，这是目前最受欢迎且最成功的机器学习模型之一。梯度提升与AdaBoost类似，因为弱学习器是决策树，每个弱学习器的目标是从前一个学习者的错误中学习。梯度提升与AdaBoost之间的一个区别在于，在梯度提升中，我们允许决策树的深度超过1。梯度提升可用于回归和分类，但为了清晰起见，我们使用回归示例。要了解更多信息，请查看附录C中视频和阅读材料的链接。本节的代码如下：
- en: '**Notebook**: Gradient_boosting_and_XGBoost.ipynb'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**笔记本**：Gradient_boosting_and_XGBoost.ipynb'
- en: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_12_Ensemble_Methods/Gradient_boosting_and_XGBoost.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_12_Ensemble_Methods/Gradient_boosting_and_XGBoost.ipynb)'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_12_Ensemble_Methods/Gradient_boosting_and_XGBoost.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_12_Ensemble_Methods/Gradient_boosting_and_XGBoost.ipynb)'
- en: The example we use is the same one as in the section “Decision trees for regression”
    in chapter 9, in which we studied the level of engagement of certain users with
    an app. The feature is the age of the user, and the label is the number of days
    that the user engages with the app (table 12.3). The plot of the dataset is shown
    in figure 12.19.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的例子与第9章中“决策树回归”部分相同的例子，我们在其中研究了某些用户与一个应用的互动程度。特征是用户的年龄，标签是用户与应用互动的天数（表12.3）。数据集的图示在图12.19中。
- en: '![](../Images/12-19.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-19.png)'
- en: Figure 12.19 The plot of the user engagement dataset from table 12.3\. The horizontal
    axis represents the age of the users, and the vertical axis represents the days
    per week that the user uses our app.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.19 表12.3中用户互动数据集的图示。横轴代表用户的年龄，纵轴代表用户每周使用我们应用的天数。
- en: Table 12.3 A small dataset with eight users, their age, and their engagement
    with our app. The engagement is measured in the number of days when they opened
    the app in one week. We’ll fit this dataset using gradient boosting.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.3 一个包含八个用户、他们的年龄以及他们与我们应用的互动的小数据集。互动是通过他们在一周内打开应用的天数来衡量的。我们将使用梯度提升来拟合这个数据集。
- en: '| Feature (age) | Label (engagement) |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 特征（年龄） | 标签（互动） |'
- en: '| 10 | 7 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 7 |'
- en: '| 20 | 5 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 5 |'
- en: '| 30 | 7 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 30 | 7 |'
- en: '| 40 | 1 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 40 | 1 |'
- en: '| 50 | 2 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 2 |'
- en: '| 60 | 1 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 60 | 1 |'
- en: '| 70 | 5 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 70 | 5 |'
- en: '| 80 | 4 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 80 | 4 |'
- en: 'The idea of gradient boosting is that we’ll create a sequence of trees that
    fit this dataset. The two hyperparameters that we’ll use for now are the number
    of trees, which we set to five, and the learning rate, which we set to 0.8\. The
    first weak learner is simple: it is the decision tree of depth 0 that best fits
    the dataset. A decision tree of depth 0 is simply a node that assigns the same
    label to each point in the dataset. Because the error function we are minimizing
    is the mean square error, then this optimal value for the prediction is the average
    value of the labels. The average value of the labels of this dataset is 4, so
    our first weak learner is a node that assigns a prediction of 4 to every point.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升的想法是我们将创建一个适合这个数据集的树序列。我们现在将使用的两个超参数是树的数目，我们将其设置为五，以及学习率，我们将其设置为0.8。第一个弱学习器很简单：它是最适合数据集的深度为0的决策树。深度为0的决策树只是一个将相同的标签分配给数据集中每个点的节点。因为我们最小化的误差函数是均方误差，所以这个预测的最优值是标签的平均值。这个数据集标签的平均值是4，所以我们的第一个弱学习器是一个将预测值4分配给每个点的节点。
- en: The next step is to calculate the residual, which is the difference between
    the label and the prediction made by this first weak learner, and fit a new decision
    tree to these residuals. As you can see, what this is doing is training a decision
    tree to fill in the gaps that the first tree has left. The labels, predictions,
    and residuals are shown in table 12.4.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是计算残差，即标签与第一个弱学习器做出的预测之间的差异，并拟合一个新的决策树来这些残差。正如你所看到的，这是在训练一个决策树来填补第一个树留下的空缺。标签、预测和残差在表12.4中显示。
- en: The second weak learner is a tree that fits these residuals. The tree can be
    as deep as we’d like, but for this example, we’ll make sure all the weak learners
    are of depth at most 2\. This tree is shown in figure 12.20 (together with its
    boundary), and its predictions are in the rightmost column of table 12.4\. This
    tree has been obtained using Scikit-Learn; see the notebook for the procedure.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个弱学习器是一个拟合这些残差的树。树可以深到我们想要的程度，但在这个例子中，我们将确保所有弱学习器的深度最多为2。这棵树在图12.20（及其边界）中显示，其预测在表12.4的最右边列中。这棵树是使用Scikit-Learn获得的；请参阅笔记本以获取过程。
- en: '![](../Images/12-20.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-20.png)'
- en: Figure 12.20 The second weak learner in the gradient boosting model. This learner
    is a decision tree of depth 2 pictured on the left. The predictions of this weak
    learner are shown on the plot on the right.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.20 梯度提升模型中的第二个弱学习器。这个学习器是一个深度为2的决策树，如图左所示。这个弱学习器的预测在右边的图上显示。
- en: Table 12.4 The predictions from the first weak learner are the average of the
    labels. The second weak learner is trained to fit the residuals of the first weak
    learner.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.4 第一个弱学习器的预测是标签的平均值。第二个弱学习器被训练来拟合第一个弱学习器的残差。
- en: '| Feature (age) | Label (engagement) | Prediction from weak learner 1 | Residual
    | Prediction from weak learner 2 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 特征（年龄） | 标签（互动） | 弱学习器1的预测 | 残差 | 弱学习器2的预测 |'
- en: '| 10 | 7 | 4 | 3 | 3 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 7 | 4 | 3 | 3 |'
- en: '| 20 | 5 | 4 | 2 | 2 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 5 | 4 | 2 | 2 |'
- en: '| 30 | 7 | 4 | 3 | 2 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 30 | 7 | 4 | 3 | 2 |'
- en: '| 40 | 1 | 4 | –3 | –2.667 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 40 | 1 | 4 | –3 | –2.667 |'
- en: '| 50 | 2 | 4 | –2 | –2.667 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 2 | 4 | –2 | –2.667 |'
- en: '| 60 | 1 | 4 | –3 | –2.667 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 60 | 1 | 4 | –3 | –2.667 |'
- en: '| 70 | 5 | 4 | 1 | 0.5 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 70 | 5 | 4 | 1 | 0.5 |'
- en: '| 80 | 4 | 4 | 0 | 0.5 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 80 | 4 | 4 | 0 | 0.5 |'
- en: The idea is to continue in this fashion, calculating new residuals and training
    a new weak learner to fit these residuals. However, there’s a small caveat—to
    calculate the prediction from the first two weak learners, we first multiply the
    prediction of the second weak learner by the learning rate. Recall that the learning
    rate we’re using is 0.8\. Thus, the combined prediction of the first two weak
    learners is the prediction of the first one (4) plus 0.8 times the prediction
    of the second one. We do this because we don’t want to overfit by fitting our
    training data too well. Our goal is to mimic the gradient descent algorithm, by
    slowly walking closer and closer to the solution, and this is what we achieve
    by multiplying the prediction by the learning rate. The new residuals are the
    original labels minus the combined predictions of the first two weak learners.
    These are calculated in table 12.5.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这种想法是继续这种方式，计算新的残差并训练一个新的弱学习器来拟合这些残差。然而，有一个小的注意事项——为了计算前两个弱学习器的预测值，我们首先将第二个弱学习器的预测值乘以学习率。回想一下，我们使用的学习率是0.8。因此，前两个弱学习器的组合预测是第一个的预测值（4）加上第二个预测值的0.8倍。我们这样做是因为我们不希望过度拟合，即训练数据拟合得太好。我们的目标是模仿梯度下降算法，通过逐渐接近解，这就是我们通过乘以学习率所实现的。新的残差是原始标签减去前两个弱学习器的组合预测。这些在表12.5中计算。
- en: Table 12.5 The labels, the predictions from the first two weak learners, and
    the residual. The prediction from the first weak learner is the average of the
    labels. The prediction from the second weak learner is shown in figure 12.20\.
    The combined prediction is equal to the prediction of the first weak learner plus
    the learning rate (0.8) times the prediction of the second weak learner. The residual
    is the difference between the label and the combined prediction from the first
    two weak learners.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.5 标签、前两个弱学习器的预测值以及残差。第一个弱学习器的预测值是标签的平均值。第二个弱学习器的预测值显示在图12.20中。组合预测等于第一个弱学习器的预测值加上学习率（0.8）乘以第二个弱学习器的预测值。残差是标签与第一个两个弱学习器的组合预测之间的差值。
- en: '| Label | Prediction from weak learner 1 | Prediction from weak learner 2 |
    Prediction from weak learner 2 times the learning rate | Prediction from weak
    learners 1 and 2 | Residual |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | 弱学习器1的预测值 | 弱学习器2的预测值 | 弱学习器2乘以学习率的预测值 | 弱学习器1和2的预测值 | 残差 |'
- en: '| 7 | 4 | 3 | 2.4 | 6.4 | 0.6 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 4 | 3 | 2.4 | 6.4 | 0.6 |'
- en: '| 5 | 4 | 2 | 1.6 | 5.6 | –0.6 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 4 | 2 | 1.6 | 5.6 | –0.6 |'
- en: '| 7 | 4 | 2 | 1.6 | 5.6 | 1.4 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 4 | 2 | 1.6 | 5.6 | 1.4 |'
- en: '| 1 | 4 | –2.667 | –2.13 | 1.87 | –0.87 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 4 | –2.667 | –2.13 | 1.87 | –0.87 |'
- en: '| 2 | 4 | –2.667 | –2.13 | 1.87 | 0.13 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 4 | –2.667 | –2.13 | 1.87 | 0.13 |'
- en: '| 1 | 4 | –2.667 | –2.13 | 1.87 | –0.87 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 4 | –2.667 | –2.13 | 1.87 | –0.87 |'
- en: '| 5 | 4 | 0.5 | 0.4 | 4.4 | 0.6 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 4 | 0.5 | 0.4 | 4.4 | 0.6 |'
- en: '| 4 | 4 | 0.5 | 0.4 | 4.4 | –0.4 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 4 | 0.5 | 0.4 | 4.4 | –0.4 |'
- en: Now we can proceed to fit a new weak learner on the new residuals and calculate
    the combined prediction of the first two weak learners. We obtain this by adding
    the prediction for the first weak learner and 0.8 (the learning rate) times the
    sum of the predictions of the second and the third weak learner. We repeat this
    process for every weak learner we want to build. Instead of doing it by hand,
    we can use the `GradientBoostingRegressor` package in Scikit-Learn (the code is
    in the notebook). The next few lines of code show how to fit the model and make
    predictions. Note that we have set the depth of the trees to be at most 2, the
    number of trees to be five, and the learning rate to be 0.8\. The hyperparameters
    used for this are `max_depth`, `n_estimators`, and `learning_rate`. Note, too,
    that if we want five trees, we must set the `n_estimators` hyperparameter to four,
    because the first tree isn’t counted.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以继续在新的残差上拟合一个新的弱学习器，并计算前两个弱学习器的组合预测。我们通过将第一个弱学习器的预测值加上0.8（学习率）乘以第二个和第三个弱学习器的预测值之和来获得这个值。我们为想要构建的每个弱学习器重复这个过程。我们可以使用Scikit-Learn中的`GradientBoostingRegressor`包（代码在笔记本中）来代替手动操作。接下来的几行代码显示了如何拟合模型并进行预测。请注意，我们已将树的深度设置为最多2层，树的数量设置为5，学习率设置为0.8。用于此的超参数是`max_depth`、`n_estimators`和`learning_rate`。此外，请注意，如果我们想要五棵树，我们必须将`n_estimators`超参数设置为4，因为第一棵树不计入。
- en: '[PRE2]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The plot for the resulting strong learner is shown in figure 12.21\. Notice
    that it does a good job predicting the values.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 结果强学习器的图示如图12.21所示。请注意，它在预测值方面做得很好。
- en: '![](../Images/12-211.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-211.png)'
- en: Figure 12.21 The plot of the predictions of the strong learner in our gradient
    boosting regressor. Note that the model fits the dataset quite well.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.21 我们梯度提升回归器中强学习器预测的图示。请注意，该模型很好地拟合了数据集。
- en: However, we can go a little further and actually plot the five weak learners
    we obtain. The details for this are in the notebook, and the five weak learners
    are shown in figure 12.22\. Notice that the predictions of the last weak learners
    are much smaller than those of the first ones, because each weak learner is predicting
    the error of the previous ones, and these errors get smaller and smaller at each
    step.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以更进一步，实际上绘制我们获得的五个弱学习器。这些细节在笔记本中，五个弱学习器如图12.22所示。请注意，最后一个弱学习器的预测值远小于第一个，因为每个弱学习器都在预测前一个的误差，而这些误差在每一步都会越来越小。
- en: '![](../Images/12-22.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-22.png)'
- en: Figure 12.22 The five weak learners in the gradient boosting model. The first
    one is a decision tree of depth 0 that always predicts the average of the labels.
    Each successive weak learner is a decision tree of depth at most 2, which fits
    the residuals from the prediction given by the previous weak learners. Note that
    the predictions of the weak learners get smaller, because the residuals get smaller
    when the predictions of the strong learner get closer to the labels.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.22 梯度提升模型中的五个弱学习器。第一个是一个深度为0的决策树，它总是预测标签的平均值。每个后续的弱学习器都是一个深度最多为2的决策树，它拟合先前弱学习器给出的预测的残差。请注意，弱学习器的预测值越来越小，因为当强学习器的预测值越来越接近标签时，残差也会越来越小。
- en: 'Finally, we can use Scikit-Learn or a manual calculation to see that the predictions
    are the following:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用Scikit-Learn或手动计算来查看预测如下：
- en: Age = 10, prediction = 6.87
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄 = 10，预测 = 6.87
- en: Age = 20, prediction = 5.11
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄 = 20，预测 = 5.11
- en: Age = 30, prediction = 6.71
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄 = 30，预测 = 6.71
- en: Age = 40, prediction = 1.43
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄 = 40，预测 = 1.43
- en: Age = 50, prediction = 1.43
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄 = 50，预测 = 1.43
- en: Age = 60, prediction = 1.43
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄 = 60，预测 = 1.43
- en: Age = 70, prediction = 4.90
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄 = 70，预测 = 4.90
- en: Age = 80, prediction = 4.10
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄 = 80，预测 = 4.10
- en: 'XGBoost: An extreme way to do gradient boosting'
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XGBoost：一种极端的梯度提升方法
- en: 'XGBoost, which stands for *extreme gradient boosting*, is one of the most popular,
    powerful, and effective gradient boosting implementations. Created by Tianqi Chen
    and Carlos Guestrin in 2016 (see appendix C for the reference), XGBoost models
    often outperform other classification and regression models. In this section,
    we discuss how XGBoost works, using the same regression example from the section
    “Gradient boosting: Using decision trees to build strong learners.”'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost，代表**极端梯度提升**，是最受欢迎、最强大和最有效的梯度提升实现之一。由陈天奇和卡洛斯·古斯特林于2016年创建（参见附录C获取参考文献），XGBoost模型通常优于其他分类和回归模型。在本节中，我们将讨论XGBoost的工作原理，使用与“梯度提升：使用决策树构建强学习器”部分相同的回归示例。
- en: XGBoost uses decision trees as the weak learners, and just like in the previous
    boosting methods we learned, each weak learner is designed to focus on the weaknesses
    of the previous ones. More specifically, each tree is built to fit the residuals
    of the predictions of the previous trees. However, there are some small differences,
    such as the way we build the trees, which is using a metric called the *similarity
    score*. Furthermore, we add a pruning step to prevent overfitting, in which we
    remove the branches of the trees if they don’t satisfy certain conditions. In
    this section, we cover this in more detail.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost使用决策树作为弱学习器，就像我们在之前学习的提升方法中一样，每个弱学习器都是设计来关注前一个的弱点。更具体地说，每棵树都是构建来拟合先前树预测的残差。然而，也有一些小的不同，比如我们构建树的方式，我们使用一个称为**相似度得分**的度量。此外，我们添加了一个剪枝步骤来防止过拟合，其中如果树的分支不满足某些条件，我们会移除这些分支。在本节中，我们将更详细地介绍这一点。
- en: 'XGBoost similarity score: A new and effective way to measure similarity in
    a set'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost相似度得分：在集合中衡量相似性的新而有效的方法
- en: In this subsection, we see the main building block of XGBoost, which is a way
    to measure how similar the elements of a set are. This metric is aptly called
    the *similarity score*. Before we learn it, let’s do a small exercise. Among the
    following three sets, which one has the most amount of similarity, and which one
    has the least?
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将看到XGBoost的主要构建块，这是一种衡量集合元素相似度的方法。这个指标恰当地被称为*相似度得分*。在我们学习它之前，让我们做一个小的练习。在以下三个集合中，哪一个具有最大的相似度，哪一个具有最小的相似度？
- en: '**Set 1**: {10, –10, 4}'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集合1**：{10, –10, 4}'
- en: '**Set 2**: {7, 7, 7}'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集合2**：{7, 7, 7}'
- en: '**Set 3**: {7}'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集合3**：{7}'
- en: If you said that set 2 has the most amount of similarity and set 1 has the least
    amount, your intuition is correct. In set 1, the elements are very different from
    each other, so this one has the least amount of similarity. Between sets 2 and
    3, it’s not so clear, because both sets have the same element, but a different
    number of times. However, set 2 has the number seven appearing three times, whereas
    set 3 has it appearing only once. Therefore, in set 2, the elements are more homogeneous,
    or more similar, than in set 3.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为集合2的相似度最高，而集合1的相似度最低，你的直觉是正确的。在集合1中，元素彼此之间非常不同，因此这一组的相似度最低。在集合2和3之间，情况并不那么明确，因为这两个集合都有相同的元素，但出现的次数不同。然而，集合2中数字七出现了三次，而集合3中只出现了一次。因此，在集合2中，元素比集合3更均匀，或者说更相似。
- en: 'To quantify similarity, consider the following metric. Given a set {*a*[1],
    *a*[2], …, *a*[n]}, the similarity score is the square of the sum of the elements,
    divided by the number of elements, namely, ![](../Images/12_22_E01.png). Let’s
    calculate the similarity score for the three sets above, shown next:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化相似度，考虑以下指标。给定一个集合{*a*[1]，*a*[2]，…，*a*[n]}，相似度得分是元素总和的平方除以元素的数量，即![公式](../Images/12_22_E01.png)。让我们计算上面三个集合的相似度得分，如下所示：
- en: '![](../Images/12_22_E02.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![公式](../Images/12_22_E02.png)'
- en: Note that as expected, the similarity score of set 2 is the highest, and that
    of set 1 is the lowest.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，正如预期的那样，集合2的相似度得分是最高的，而集合1的得分最低。
- en: note This similarity score is not perfect. One can argue that the set {1, 1,
    1} is more similar than the set {7, 8, 9}, yet the similarity score of {1, 1,
    1} is 3, and the similarity score of {7, 8, 9} is 192\. However, for the purposes
    of our algorithm, this score still works. The main goal of the similarity score
    is to be able to separate the large and small values well, and this goal is met,
    as we’ll see in the current example.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这个相似度得分并不完美。有人可能会认为集合{1, 1, 1}比集合{7, 8, 9}更相似，但集合{1, 1, 1}的相似度得分是3，而集合{7,
    8, 9}的相似度得分是192。然而，对于我们的算法来说，这个得分仍然有效。相似度得分的主要目标是能够很好地分离大值和小值，正如我们将在当前示例中看到的那样。
- en: There is a hyperparameter λ associated with the similarity score, which helps
    prevent overfitting. When used, it is added to the denominator of the similarity
    score, which gives the formula ![](../Images/12_22_E03.png). Thus, for example,
    if λ = 2, the similarity score of set 1 is now ![](../Images/12_22_E04.png) We
    won’t use the λ hyperparameter in our example, but when we get to the code, we’ll
    see how to set it to any value we want.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 与相似度得分相关联的超参数λ有助于防止过拟合。当使用时，它被添加到相似度得分的分母中，给出了公式![公式](../Images/12_22_E03.png)。例如，如果λ
    = 2，集合1的相似度得分现在是![公式](../Images/12_22_E04.png)。在我们的示例中，我们不会使用λ超参数，但在我们到达代码时，我们将看到如何将其设置为任何我们想要的值。
- en: Building the weak learners
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 构建弱学习器
- en: In this subsection, we see how to build each one of the weak learners. To illustrate
    this process, we use the same example from the section “Gradient boosting,” shown
    in table 12.3\. For convenience, the same dataset is shown in the two leftmost
    columns of table 12.6\. This is a dataset of users of an app, in which the feature
    is the age of the users, and the label is the number of days per week in which
    they engage with the app. The plot of this dataset is shown in figure 12.19.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将看到如何构建每个弱学习器。为了说明这个过程，我们使用了“梯度提升”部分中的相同示例，如表12.3所示。为了方便起见，相同的数据集显示在表12.6的最左两列中。这是一个应用程序用户的数据集，其中特征是用户的年龄，标签是他们每周与该应用程序互动的天数。该数据集的图示如图12.19所示。
- en: Table 12.6 The same dataset as in table 12.3, containing users, their age, and
    the number of days per week in which they engaged with our app. The third column
    contains the predictions from the first weak learner in our XGBoost model. These
    predictions are all 0.5 by default. The last column contains the residual, which
    is the difference between the label and the prediction.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.6 与表12.3中相同的数据库，包含用户、他们的年龄以及他们每周与我们应用互动的天数。第三列包含XGBoost模型中第一个弱学习器的预测。这些预测默认都是0.5。最后一列包含残差，即标签与预测之间的差异。
- en: '| Feature (age) | Label (engagement) | Prediction from the first weak learner
    | Residual |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 特征（年龄） | 标签（参与度） | 第一个弱学习器的预测 | 残差 |'
- en: '| 10 | 7 | 0.5 | 6.5 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 7 | 0.5 | 6.5 |'
- en: '| 20 | 5 | 0.5 | 4.5 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 5 | 0.5 | 4.5 |'
- en: '| 30 | 7 | 0.5 | 6.5 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 30 | 7 | 0.5 | 6.5 |'
- en: '| 40 | 1 | 0.5 | 0.5 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 40 | 1 | 0.5 | 0.5 |'
- en: '| 50 | 2 | 0.5 | 1.5 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 2 | 0.5 | 1.5 |'
- en: '| 60 | 1 | 0.5 | 0.5 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 60 | 1 | 0.5 | 0.5 |'
- en: '| 70 | 5 | 0.5 | 4.5 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 70 | 5 | 0.5 | 4.5 |'
- en: '| 80 | 4 | 0.5 | 3.5 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 80 | 4 | 0.5 | 3.5 |'
- en: The process of training an XGBoost model is similar to that of training gradient
    boosting trees. The first weak learner is a tree that gives a prediction of 0.5
    to each data point. After building this weak learner, we calculate the residuals,
    which are the differences between the label and the predicted label. These two
    quantities can be found in the two rightmost columns of table 12.6.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 训练XGBoost模型的过程与训练梯度提升树的过程类似。第一个弱学习器是一棵对每个数据点预测0.5的树。在构建这个弱学习器之后，我们计算残差，即标签与预测标签之间的差异。这两个量可以在表12.6的最后两列中找到。
- en: Before we start building the remaining trees, let’s decide how deep we want
    them to be. To keep this example small, let’s again use a maximum depth of 2\.
    That means that when we get to depth 2, we stop building the weak learners. This
    is a hyperparameter, which we’ll see in more detail in the section “Training an
    XGBoost model in Python.”
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始构建剩余的树之前，让我们决定我们希望它们有多深。为了保持这个例子的小型化，我们再次使用最大深度为2。这意味着当我们达到深度2时，我们停止构建弱学习器。这是一个超参数，我们将在“在Python中训练XGBoost模型”部分中更详细地了解。
- en: 'To build the second weak learner, we need to fit a decision tree to the residuals.
    We do this using the similarity score. As usual, in the root node, we have the
    entire dataset. Thus, we begin by calculating the similarity score of the entire
    dataset as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建第二个弱学习器，我们需要将决策树拟合到残差上。我们使用相似度得分来完成这项工作。通常情况下，在根节点，我们有整个数据集。因此，我们首先计算整个数据集的相似度得分，如下所示：
- en: '![](../Images/12_22_E05.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12_22_E05.png)'
- en: 'Now, we proceed to split the node using the age feature in all the possible
    ways, as we did with decision trees. For each split, we calculate the similarity
    score of the subsets corresponding to each of the leaves and add them. That is
    the combined similarity score corresponding to that split. The scores are the
    following:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们继续使用与决策树相同的方式，根据年龄特征以所有可能的方式分割节点。对于每个分割，我们计算对应于每个叶子的子集的相似度得分并将它们相加。这就是该分割对应的组合相似度得分。得分如下：
- en: 'Split for the root node, with dataset {6.5, 4.5, 6.5, 0.5, 1.5, 0.5, 4.5, 3.5},
    and similarity score = 98:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 根节点分割，数据集{6.5, 4.5, 6.5, 0.5, 1.5, 0.5, 4.5, 3.5}，相似度得分=98：
- en: 'Split at 15:'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在15处分割：
- en: 'Left node: {6.5}; similarity score: 42.25'
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左节点：{6.5}; 相似度得分：42.25
- en: 'Right node: {4.5, 6.5, 0.5, 1.5, 0.5, 4.5, 3.5}; similarity score: 66.04'
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右节点：{4.5, 6.5, 0.5, 1.5, 0.5, 4.5, 3.5}; 相似度得分：66.04
- en: 'Combined similarity score: 108.29'
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组合相似度得分：108.29
- en: 'Split at 25:'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在25处分割：
- en: 'Left node: {6.5, 4.5}; similarity score: 60.5'
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左节点：{6.5, 4.5}; 相似度得分：60.5
- en: 'Right node: {6.5, 0.5, 1.5, 0.5, 4.5, 3.5}; similarity score: 48.17'
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右节点：{6.5, 0.5, 1.5, 0.5, 4.5, 3.5}; 相似度得分：48.17
- en: 'Combined similarity score: 108.67'
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组合相似度得分：108.67
- en: 'Split at 35:'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在35处分割：
- en: 'Left node: {6.5, 4.5, 6.5}; similarity score: 102.08'
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左节点：{6.5, 4.5, 6.5}; 相似度得分：102.08
- en: 'Right node: {0.5, 1.5, 0.5, 4.5, 3.5}; similarity score: 22.05'
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右节点：{0.5, 1.5, 0.5, 4.5, 3.5}; 相似度得分：22.05
- en: '**Combined similarity score: 124.13**'
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组合相似度得分：124.13**'
- en: 'Split at 45:'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在45处分割：
- en: 'Left node: {6.5, 4.5, 6.5, 0.5}; similarity score: 81'
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左节点：{6.5, 4.5, 6.5, 0.5}; 相似度得分：81
- en: 'Right node: {1.5, 0.5, 4.5, 3.5}; similarity score: 25'
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右节点：{1.5, 0.5, 4.5, 3.5}; 相似度得分：25
- en: 'Combined similarity score: 106'
  id: totrans-276
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组合相似度得分：106
- en: 'Split at 55:'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在55处分割：
- en: 'Left node: {6.5, 4.5, 6.5, 0.5, 1.5}; similarity score: 76.05'
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左节点：{6.5, 4.5, 6.5, 0.5, 1.5}; 相似度得分：76.05
- en: 'Right node: {0.5, 4.5, 3.5}; similarity score: 24.08'
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右节点：{0.5, 4.5, 3.5}; 相似度得分：24.08
- en: 'Combined similarity score: 100.13'
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组合相似度得分：100.13
- en: 'Split at 65:'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在65处分割：
- en: 'Left node: {6.5, 4.5, 6.5, 0.5, 1.5, 0.5}; similarity score: 66.67'
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左节点：{6.5, 4.5, 6.5, 0.5, 1.5, 0.5}; 相似度得分：66.67
- en: 'Right node: {4.5, 3.5}; similarity score: 32'
  id: totrans-283
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右节点：{4.5, 3.5}; 相似度得分：32
- en: 'Combined similarity score: 98.67'
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组合相似度得分：98.67
- en: 'Split at 75:'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在75处分割：
- en: 'Left node: {6.5, 4.5, 6.5, 0.5, 1.5, 0.5, 4.5}; similarity score: 85.75'
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左节点：{6.5, 4.5, 6.5, 0.5, 1.5, 0.5, 4.5}; 相似度得分：85.75
- en: 'Right node: {3.5}; similarity score: 12.25'
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右节点：{3.5}; 相似度得分：12.25
- en: 'Combined similarity score: 98'
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组合相似度得分：98
- en: As shown in these calculations, the split with the best combined similarity
    score is at age = 35\. This is going to be the split at the root node.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如这些计算所示，具有最佳组合相似度得分的分割是在年龄=35岁。这将作为根节点的分割。
- en: Next, we proceed to split the datasets at each of the nodes in the same way.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们以相同的方式在每个节点的数据集上进行分割。
- en: 'Split for the left node, with dataset {6.5, 4.5, 6.5} and similarity score
    102.08:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 左节点的分割，数据集为{6.5, 4.5, 6.5}，相似度得分为102.08：
- en: 'Split at 15:'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在15处分割：
- en: 'Left node: {6.5}; similarity score: 42.25'
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左节点：{6.5}; 相似度得分：42.25
- en: 'Right node: {4.5, 6.5}; similarity score: 60.5'
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右节点：{4.5, 6.5}; 相似度得分：60.5
- en: 'Similarity score: 102.75'
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相似度得分：102.75
- en: 'Split at 25:'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在25处分割：
- en: 'Left node: {6.5, 4.5}; similarity score: 60.5'
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左节点：{6.5, 4.5}; 相似度得分：60.5
- en: 'Right node: {6.5}; similarity score: 42.25'
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右节点：{6.5}; 相似度得分：42.25
- en: 'Similarity score: 102.75'
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相似度得分：102.75
- en: Both splits give us the same combined similarity score, so we can use any of
    the two. Let’s use the split at 15\. Now, on to the right node.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 两个分割给出了相同的组合相似度得分，因此我们可以使用其中的任何一个。让我们使用15处的分割。现在，转向右节点。
- en: 'Split for the right node, with dataset {0.5, 1.5, 0.5, 4.5, 3.5} and similarity
    score 22.05:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 右节点的分割，数据集为{0.5, 1.5, 0.5, 4.5, 3.5}，相似度得分为22.05：
- en: 'Split at 45:'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在45处分割：
- en: 'Left node: {0.5}; similarity score: 0.25'
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左节点：{0.5}; 相似度得分：0.25
- en: 'Right node: {1.5, 0.5, 4.5, 3.5}; similarity score: 25'
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右节点：{1.5, 0.5, 4.5, 3.5}; 相似度得分：25
- en: 'Similarity score: 25.25'
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相似度得分：25.25
- en: 'Split at 55:'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在55处分割：
- en: 'Left node: {0.5, 1.5}; similarity score: 2'
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左节点：{0.5, 1.5}; 相似度得分：2
- en: 'Right node: {0.5, 4.5, 3.5}; similarity score: 24.08'
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右节点：{0.5, 4.5, 3.5}; 相似度得分：24.08
- en: 'Similarity score: 26.08'
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相似度得分：26.08
- en: 'Split at 65:'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在65处分割：
- en: 'Left node: {0.5, 1.5, 0.5}; similarity score: 2.08'
  id: totrans-311
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左节点：{0.5, 1.5, 0.5}; 相似度得分：2.08
- en: 'Right node: {4.5, 3.5}; similarity score: 32'
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右节点：{4.5, 3.5}; 相似度得分：32
- en: '**Similarity score: 34.08**'
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相似度得分：34.08**'
- en: 'Split at 75:'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在75处分割：
- en: 'Left node: {0.5, 1.5, 0.5, 4.5}; similarity score: 12.25'
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左节点：{0.5, 1.5, 0.5, 4.5}; 相似度得分：12.25
- en: 'Right node: {3.5}; similarity score: 12.25'
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右节点：{3.5}; 相似度得分：12.25
- en: 'Similarity score: 24.5'
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相似度得分：24.5
- en: From here, we conclude that the best split is at age = 65\. The tree now has
    depth 2, so we stop growing it, because this is what we decided at the beginning
    of the algorithm. The resulting tree, together with the similarity scores at the
    nodes, is shown in figure 12.23.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们得出结论，最佳分割是在年龄=65岁。树现在深度为2，所以我们停止生长，因为这正是我们在算法开始时决定的。结果树以及节点处的相似度得分如图12.23所示。
- en: '![](../Images/12-231.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-231.png)'
- en: Figure 12.23 The second weak learner in our XGBoost classifier. For each of
    the nodes, we can see the split based on the age feature, the labels corresponding
    to that node, and the similarity score for each set of labels. The split chosen
    for each node is the one that maximizes the combined similarity score of the leaves.
    For each of the leaves, you can see the corresponding labels and their similarity
    score.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.23 我们XGBoost分类器中的第二个弱学习器。对于每个节点，我们可以看到基于年龄特征的分割，该节点对应的标签，以及每组标签的相似度得分。每个节点选择的分割是最大化叶子节点组合相似度得分的分割。对于每个叶子节点，你可以看到相应的标签及其相似度得分。
- en: That’s (almost) our second weak learner. Before we continue building more weak
    learners, we need to do one more step to help reduce overfitting.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这（几乎）就是我们的第二个弱学习器。在我们继续构建更多弱学习器之前，我们需要再进行一步操作来帮助减少过拟合。
- en: 'Tree pruning: A way to reduce overfitting by simplifying the weak learners'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 树剪枝：通过简化弱学习器来减少过拟合的一种方法
- en: A great feature of XGBoost is that it doesn’t overfit much. For this, it uses
    several hyperparameters that are described in detail in the section “Training
    an XGBoost model in Python.” One of them, the minimum split loss, prevents a split
    from happening if the combined similarity scores of the resulting nodes are not
    significantly larger than the similarity score of the original node. This difference
    is called the similarity gain. For example, in the root node of our tree, the
    similarity score is 98, and the combined similarity score of the nodes is 124.13\.
    Thus, the similarity gain is 124.13 – 98 = 26.13\. Similarly, the similarity gain
    of the left node is 0.67, and that of the right node is 12.03, as shown in figure
    12.24.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost的一个很好的特性是它不太容易过拟合。为此，它使用几个超参数，这些超参数在“在Python中训练XGBoost模型”部分中详细描述。其中之一，最小分割损失，如果结果节点的综合相似度得分与原始节点的相似度得分没有显著差异，则阻止分割发生。这种差异称为相似度增益。例如，在我们的树根节点中，相似度得分是98，节点的综合相似度得分是124.13。因此，相似度增益是124.13
    – 98 = 26.13。同样，左侧节点的相似度增益是0.67，右侧节点的相似度增益是12.03，如图12.24所示。
- en: '![](../Images/12-24.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-24.png)'
- en: 'Figure 12.24 On the left, we have the same tree from figure 12.23, with an
    extra piece of information: the similarity gain. We obtain this by subtracting
    the similarity score for each node from the combined similarity score of the leaves.
    We only allow splits with a similarity gain higher than 1 (our minimum split loss
    hyperparameter), so one of the splits is no longer permitted. This results in
    the pruned tree on the right, which now becomes our weak learner.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.24 在图12.23的左侧，我们有与图12.23相同的树，额外的一条信息：相似度增益。我们通过从每个节点的相似度得分中减去叶子节点的综合相似度得分来获得这个值。我们只允许具有高于1（我们的最小分割损失超参数）的相似度增益的分割，因此其中一个分割不再被允许。这导致右侧的剪枝树，现在它变成了我们的弱学习器。
- en: We’ll set the minimum split loss to 1\. With this value, the only split that
    is prevented is the one on the left node (age ≤ 15). Thus, the second weak learner
    looks like the one on the right of figure 12.24.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将最小分割损失设置为1。使用这个值，唯一被阻止的分割是左侧节点（年龄≤15）的分割。因此，第二个弱学习器看起来就像图12.24右侧的树。
- en: Making the predictions
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 进行预测
- en: Now that we’ve built our second weak learner, it’s time to use it to make predictions.
    We obtain predictions the same way we obtain them from any decision tree, namely,
    by averaging the labels in the corresponding leaf. The predictions for our second
    weak learner are seen in figure 12.25.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了第二个弱学习器，是时候用它来进行预测了。我们通过从任何决策树中获取预测的方式获得预测，即通过平均相应叶子的标签。第二个弱学习器的预测可以在图12.25中看到。
- en: Now, on to calculate the combined prediction for the first two weak learners.
    To avoid overfitting, we use the same technique that we used in gradient boosting,
    which is multiplying the
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来计算前两个弱学习器的组合预测。为了避免过拟合，我们使用与梯度提升中相同的技巧，即乘以
- en: prediction of all the weak learners (except the first one) by the learning rate.
    This is meant to emulate the gradient descent method, in which we slowly converge
    to a good prediction after several iterations. We use a learning rate of 0.7\.
    Thus, the combined prediction of the first two weak learners is equal to the prediction
    of the first weak learner plus the prediction of the second weak learner times
    0.7\. For example, for the first data point, this prediction is
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 通过学习率对所有的弱学习器（除了第一个）进行预测。这是为了模拟梯度下降法，在经过几次迭代后，我们逐渐收敛到一个好的预测。我们使用的学习率是0.7。因此，前两个弱学习器的组合预测等于第一个弱学习器的预测加上第二个弱学习器预测乘以0.7。例如，对于第一个数据点，这个预测是
- en: 0.5 + 5.83 . 0.7 = 4.58.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 0.5 + 5.83 * 0.7 = 4.58。
- en: '![](../Images/12-25.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-25.png)'
- en: Figure 12.25 The second weak learner in our XGBoost model after being pruned.
    This is the same tree from figure 12.24, with its predictions. The prediction
    at each leaf is the average of the labels corresponding to that leaf.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.25 在我们的XGBoost模型中被剪枝后的第二个弱学习器。这是与图12.24相同的树，带有其预测。每个叶子的预测是相应叶子标签的平均值。
- en: The fifth column of table 12.7 contains the combined prediction of the first
    two weak learners.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.7的第五列包含前两个弱学习器的组合预测。
- en: Table 12.7 The labels, predictions from the first two weak learners, and the
    residual. The combined prediction is obtained by adding the prediction from the
    first weak learner (which is always 0.5) plus the learning rate (0.7) times the
    prediction from the second weak learner. The residual is again the difference
    between the label and the combined prediction.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.7：标签、前两个弱学习者的预测和残差。综合预测是通过将第一个弱学习者的预测（总是0.5）加上学习率（0.7）乘以第二个弱学习者的预测得到的。残差是标签和综合预测之间的差异。
- en: '| Label (engagement) | Prediction from weak learner 1 | Prediction from weak
    learner 2 | Prediction from weak learner 2 times the learning rate | Combined
    prediction | Residual |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 标签（参与度） | 弱学习者1的预测 | 弱学习者2的预测 | 弱学习者2乘以学习率 | 综合预测 | 残差 |'
- en: '| 7 | 0.5 | 5.83 | 4.08 | 4.58 | 2.42 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.5 | 5.83 | 4.08 | 4.58 | 2.42 |'
- en: '| 5 | 0.5 | 5.83 | 4.08 | 4.58 | 0.42 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.5 | 5.83 | 4.08 | 4.58 | 0.42 |'
- en: '| 7 | 0.5 | 5.83 | 4.08 | 4.58 | 2.42 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.5 | 5.83 | 4.08 | 4.58 | 2.42 |'
- en: '| 1 | 0.5 | 0.83 | 0.58 | 1.08 | –0.08 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.5 | 0.83 | 0.58 | 1.08 | –0.08 |'
- en: '| 2 | 0.5 | 0.83 | 0.58 | 1.08 | 0.92 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.5 | 0.83 | 0.58 | 1.08 | 0.92 |'
- en: '| 1 | 0.5 | 0.83 | 0.58 | 1.08 | –0.08 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.5 | 0.83 | 0.58 | 1.08 | –0.08 |'
- en: '| 5 | 0.5 | 4 | 2.8 | 3.3 | 1.7 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.5 | 4 | 2.8 | 3.3 | 1.7 |'
- en: '| 4 | 0.5 | 4 | 2.8 | 3.3 | 0.7 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.5 | 4 | 2.8 | 3.3 | 0.7 |'
- en: Notice that the combined predictions are closer to the labels than the predictions
    of the first weak learner. The next step is to iterate. We calculate new residuals
    for all the data points, fit a tree to them, prune the tree, calculate the new
    combined predictions, and continue in this fashion. The number of trees we want
    is another hyperparameter that we can choose at the start. To continue building
    these trees, we resort to a useful Python package called `xgboost`.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，综合预测比第一个弱学习者的预测更接近标签。下一步是迭代。我们计算所有数据点的新的残差，为它们拟合一棵树，修剪树，计算新的综合预测，并以此类推。我们想要的树的数量是另一个超参数，我们可以在开始时选择。为了继续构建这些树，我们求助于一个有用的Python包，称为`xgboost`。
- en: Training an XGBoost model in Python
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中训练XGBoost模型
- en: 'In this section, we learn how to train the model to fit the current dataset
    using the `xgboost` Python package. The code for this section is in the same notebook
    as the previous one, shown here:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习如何使用`xgboost` Python包训练模型以适应当前数据集。本节的代码与前一节相同，如下所示：
- en: '**Notebook**: Gradient_boosting_and_XGBoost.ipynb'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Notebook**：Gradient_boosting_and_XGBoost.ipynb'
- en: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_12_Ensemble_Methods/Gradient_boosting_and_XGBoost.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_12_Ensemble_Methods/Gradient_boosting_and_XGBoost.ipynb)'
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_12_Ensemble_Methods/Gradient_boosting_and_XGBoost.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_12_Ensemble_Methods/Gradient_boosting_and_XGBoost.ipynb)'
- en: 'Before we start, let’s revise the hyperparameters that we’ve defined for this
    model:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，让我们回顾一下为该模型定义的超参数：
- en: 'number of estimators The number of weak learners. Note: in the `xgboost` package,
    the first weak learner is not counted among the estimators. For this example,
    we set it to 3, which will give us four weak learners.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: estimators数量：弱学习者的数量。注意：在`xgboost`包中，第一个弱学习者不计入估计器之中。对于本例，我们将其设置为3，这将给我们四个弱学习者。
- en: maximum depth The maximum depth allowed for each one of the decision trees (weak
    learners). We set it to 2.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 最大深度：每个决策树（弱学习者）允许的最大深度。我们将其设置为2。
- en: lambda parameter The number added to the denominator of the similarity score.
    We set it to 0.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: lambda参数：添加到相似度分数分母中的数字。我们将其设置为0。
- en: minimum split loss The minimum gain in similarity score to allow for a split
    to happen. We set it to 1.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 最小分割损失：允许分割发生的相似度分数的最小增益。我们将其设置为1。
- en: learning rate The predictions from the second to last weak learners are multiplied
    by the learning rate. We set it to 0.7.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率：从倒数第二个弱学习者的预测乘以学习率。我们将其设置为0.7。
- en: 'With the following lines of code, we import the package, build a model called
    `XGBRegressor`, and fit it to our dataset:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码行，我们导入包，构建一个名为`XGBRegressor`的模型，并将其拟合到我们的数据集：
- en: '[PRE3]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The plot of the model is shown in figure 12.26\. Notice that it fits the dataset
    well.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的图表显示在图12.26中。注意，它很好地拟合了数据集。
- en: '![](../Images/12-26.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12-26.png)'
- en: Figure 12.26 The plot of the predictions of our XGBoost model. Note that it
    fits the dataset well.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.26：我们的XGBoost模型预测的图表。注意，它很好地拟合了数据集。
- en: The `xgboost` package also allows us to look at the weak learners, and they
    appear in figure 12.24\. The trees obtained in this fashion already have the labels
    multiplied by the learning rate of 0.7, which is clear when compared with the
    predictions of the tree obtained manually in figure 12.25 and the second tree
    from the left in figure 12.27.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '`xgboost`包还允许我们查看弱学习者，它们出现在图12.24中。以这种方式获得的树已经将标签乘以学习率0.7，与图12.25中手动获得的树的预测值和图12.27中左侧第二棵树的预测值相比，这一点很清楚。'
- en: '![](../Images/12-27.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-27.png)'
- en: Figure 12.27 The four weak learners that form the strong learner in our XGBoost
    model. Note that the first one always predicts 0.5\. The other three are quite
    similar in shape, which is a coincidence. However, notice that the predictions
    from each of the trees get smaller, because each time we are fitting smaller residuals.
    Furthermore, notice that the second weak learner is the same tree we obtained
    manually in figure 12.25, where the only difference is that in this tree, the
    predictions are already multiplied by the learning rate of 0.7.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.27 形成我们XGBoost模型中强学习者的四个弱学习者。注意，第一个总是预测0.5。其他三个在形状上相当相似，这是一个巧合。然而，请注意，每棵树的预测值都在减小，因为每次我们都在拟合更小的残差。此外，请注意第二个弱学习者是我们图12.25中手动获得的同一棵树，唯一的区别是，在这棵树中，预测值已经乘以了学习率0.7。
- en: 'Thus, to obtain the predictions of the strong learner, we need to add only
    the prediction of every tree. For example, for a user who is 20 years old, the
    predictions are the following:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了获得强学习者的预测值，我们只需要添加每棵树的预测值。例如，对于一个20岁的用户，预测值如下：
- en: 'Weak learner 1: 0.5'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弱学习者1：0.5
- en: 'Weak learner 2: 4.08'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弱学习者2：4.08
- en: 'Weak learner 3: 1.22'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弱学习者3：1.22
- en: 'Weak learner 4: –0.57'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弱学习者4：-0.57
- en: 'Thus, the prediction is 0.5 + 5.83 + 1.22 – 0.57 = 5.23\. The predictions for
    the other points follow:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，预测值为 0.5 + 5.83 + 1.22 – 0.57 = 5.23。其他点的预测值如下：
- en: Age = 10; prediction = 6.64
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄 = 10；预测值 = 6.64
- en: Age = 20; prediction = 5.23
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄 = 20；预测值 = 5.23
- en: Age = 30; prediction = 6.05
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄 = 30；预测值 = 6.05
- en: Age = 40; prediction = 1.51
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄 = 40；预测值 = 1.51
- en: Age = 50; prediction = 1.51
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄 = 50；预测值 = 1.51
- en: Age = 60; prediction = 1.51
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄 = 60；预测值 = 1.51
- en: Age = 70; prediction = 4.39
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄 = 70；预测值 = 4.39
- en: Age = 80; prediction = 4.39
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年龄 = 80；预测值 = 4.39
- en: Applications of ensemble methods
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成方法的应用
- en: Ensemble methods are some of the most useful machine learning techniques used
    nowadays because they exhibit great levels of performance with relatively low
    cost. One of the places where ensemble methods are used most is in machine learning
    challenges, such as the Netflix Challenge. The Netflix Challenge was a competition
    that Netflix organized, where they anonymized some data and made it public. The
    competitors’ goal was to build a better recommendation system than Netflix itself;
    the best system would win one million dollars. The winning team used a powerful
    combination of learners in an ensemble to win. For more information on this, check
    the reference in appendix C.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法是当今最实用的机器学习技术之一，因为它们在相对较低的成本下表现出极高的性能。集成方法应用最广泛的地方之一是机器学习挑战，例如Netflix挑战。Netflix挑战是Netflix组织的一项比赛，他们将一些数据匿名化并公开。参赛者的目标是构建一个比Netflix本身更好的推荐系统；最佳系统将赢得一百万美元。获胜团队使用了一个强大的集成学习者的组合来获胜。有关更多信息，请参阅附录C中的参考文献。
- en: Summary
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: Ensemble methods consist of training several weak learners and combining them
    into a strong one. They are an effective way to build powerful models that have
    had great results with real datasets.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成方法包括训练多个弱学习者并将它们组合成一个强大的学习者。它们是构建强大模型的有效方法，这些模型在真实数据集上取得了显著成果。
- en: Ensemble methods can be used for regression and for classification.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成方法可用于回归和分类。
- en: 'There are two major types of ensemble methods: bagging and boosting.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成方法主要有两种类型：Bagging和Boosting。
- en: Bagging, or bootstrap aggregating, consists of building successive learners
    on random subsets of our data and combining them into a strong learner that makes
    predictions based on a majority vote.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagging，或称为自助聚合，包括在数据随机子集上构建连续的学习者，并将它们组合成一个强大的学习者，该学习者基于多数投票进行预测。
- en: Boosting consists of building a sequence of learners, where each learner focuses
    on the weaknesses of the previous one, and combining them into a strong classifier
    that makes predictions based on a weighted vote of the learners.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boosting包括构建一系列的学习者，其中每个学习者专注于前一个学习者的弱点，并将它们组合成一个强大的分类器，该分类器基于学习者的加权投票进行预测。
- en: AdaBoost, gradient boosting, and XGBoost are three advanced boosting algorithms
    that produce great results with real datasets.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaBoost、梯度提升和XGBoost是三种先进的提升算法，它们在真实数据集上产生了很好的结果。
- en: Applications of ensemble methods range widely, from recommendation algorithms
    to applications in medicine and biology.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成方法的用途非常广泛，从推荐算法到医学和生物学中的应用。
- en: Exercises
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Exercise 12.1
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 练习12.1
- en: A boosted strong learner *L* is formed by three weak learners, *L*[1], *L*[2],
    and *L*[3]. Their weights are 1, 0.4, and 1.2, respectively. For a particular
    point, *L*[1] and *L*[2] predict that its label is positive, and *L*[3] predicts
    that it’s negative. What is the final prediction the learner *L* makes on this
    point?
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 通过三个弱学习器*L*、*L*[1]、*L*[2]和*L*[3]形成了一个强提升学习器*L*。它们的权重分别是1、0.4和1.2。对于特定的一个点，*L*[1]和*L*[2]预测其标签为正，而*L*[3]预测其为负。学习器*L*对这个点的最终预测是什么？
- en: Exercise 12.2
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 练习12.2
- en: We are in the middle of training an AdaBoost model on a dataset of size 100\.
    The current weak learner classifies 68 out of the 100 data points correctly. What
    is the weight that we’ll assign to this learner in the final model?
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在对大小为100的数据集训练一个AdaBoost模型。当前的弱学习器正确分类了100个数据点中的68个。我们在最终模型中将分配给这个学习器的权重是多少？
