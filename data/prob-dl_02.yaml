- en: 1 Introduction to probabilistic deep learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 概率深度学习简介
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: What is a probabilistic model?
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是概率模型？
- en: What is deep learning and when do you use it?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习是什么以及何时使用它？
- en: Comparing traditional machine learning and deep learning approaches for image
    classification
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较传统机器学习和深度学习方法在图像分类中的应用
- en: The underlying principles of both curve fitting and neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线拟合和神经网络的基本原理
- en: Comparing non-probabilistic and probabilistic models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较非概率模型和概率模型
- en: What probabilistic deep learning is and why it’s useful
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率深度学习是什么以及为什么它有用
- en: '![](../Images/1-unnumb.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1-unnumb.png)'
- en: Deep learning (*DL*) is one of the hottest topics in data science and artificial
    intelligence today. DL has only been feasible since 2012 with the widespread usage
    of GPUs, but you’re probably already dealing with DL technologies in various areas
    of your daily life. When you vocally communicate with a digital assistant, when
    you translate text from one language into another using the free DeepL translator
    service (DeepL is a company producing translation engines based on DL), or when
    you use a search engine such as Google, DL is doing its magic behind the scenes.
    Many state-of-the-art DL applications such as text-to-speech translations boost
    their performance using probabilistic DL models. Further, safety critical applications
    like self-driving cars use Bayesian variants of probabilistic DL.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（*DL*）是当今数据科学和人工智能领域最热门的话题之一。深度学习自从2012年随着GPU的广泛应用而变得可行以来，但你可能已经在日常生活的各个领域处理深度学习技术了。当你与数字助手进行语音交流时，当你使用免费的DeepL翻译服务（DeepL是一家基于深度学习生产翻译引擎的公司）将一种语言翻译成另一种语言，或者当你使用像Google这样的搜索引擎时，深度学习正在幕后施展其魔法。许多最先进的深度学习应用，如文本到语音翻译，通过使用概率深度学习模型来提升其性能。此外，自动驾驶汽车等安全关键应用使用基于贝叶斯理论的概率深度学习变体。
- en: In this chapter, you will get a first high-level introduction to DL and its
    probabilistic variants. We use simple examples to discuss the differences between
    non-probabilistic and probabilistic models and then highlight some advantages
    of probabilistic DL models. We also give you a first impression of what you gain
    when working with Bayesian variants of probabilistic DL models. In the remaining
    chapters of the book, you will learn how to implement DL models and how to tweak
    them to get their more powerful probabilistic variants. You will also learn about
    the underlying principles that enable you to build your own models and to understand
    advanced modern models so that you can adapt them for your own purposes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将获得对深度学习及其概率变体的初步高级介绍。我们使用简单的例子来讨论非概率模型和概率模型之间的区别，并突出概率深度学习模型的一些优点。我们还给你一个关于与概率深度学习的贝叶斯变体一起工作时你将获得的第一印象。在本书的剩余章节中，你将学习如何实现深度学习模型以及如何调整它们以获得更强大的概率变体。你还将了解使你能够构建自己的模型和理解高级现代模型的基本原理，以便你可以根据自身目的进行适配。
- en: 1.1 A first look at probabilistic models
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 概率模型初探
- en: Let’s first get an idea of what a probabilistic model can look like and how
    you can use it. We use an example from daily life to discuss the difference between
    a non-probabilistic model and a probabilistic model. We then use the same example
    to highlight some advantages of a probabilistic model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先了解一下概率模型可能的样子以及如何使用它。我们用一个日常生活中的例子来讨论非概率模型和概率模型之间的区别。然后我们用同一个例子来突出概率模型的一些优点。
- en: 'In our cars, most of us use a *satellite navigational system (* satnav--a.k.a.
    GPS) that tells us how to get from A to B. For each suggested route, the satnav
    also predicts the needed travel time. Such a predicted travel time can be understood
    as a best guess. You know you’ll sometimes need more time and sometimes less time
    when taking the same route from A to B. But a standard satnav is non-probabilistic:
    it predicts only a single value for the travel time and does not tell you a possible
    range of values. For an example, look at the left panel in figure 1.1, where you
    see two routes going from Croxton, New York, to the Museum of Modern Art (MoMA),
    also in New York, with a predicted travel time that is the satnav’s best guess
    based on previous data and the current road conditions.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的车里，我们大多数人使用的是卫星导航系统（简称satnav，即GPS），它告诉我们如何从A地到B地。对于每条建议的路线，satnav还会预测所需的旅行时间。这种预测的旅行时间可以理解为一种最佳猜测。你知道当你从A地到B地走相同的路线时，有时需要更多的时间，有时需要更少的时间。但标准的satnav是非概率的：它只预测旅行时间的一个值，不会告诉你可能的值范围。例如，看看图1.1左侧的面板，你看到两条从纽约的Croxton到现代艺术博物馆（MoMA，也在纽约）的路线，预测的旅行时间是satnav基于以前的数据和当前道路状况的最佳猜测。
- en: Let’s imagine a fancier satnav that uses a probabilistic model. It not only
    gives you a best guess for the travel time, but also captures the uncertainty
    of that travel time. The probabilistic prediction of the travel time for a given
    route is provided as a distribution. For example, look at the right panel of figure
    1.1\. You see two Gaussian bell curves describing the predicted travel-time distributions
    for the two routes.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一个更高级的卫星导航系统，它使用概率模型。它不仅为你提供一个最佳猜测的旅行时间，还捕捉了该旅行时间的不确定性。对于给定路线的旅行时间概率预测以分布的形式提供。例如，看看图1.1的右侧面板。你看到两个高斯钟形曲线描述了两条路线的预测旅行时间分布。
- en: How can you benefit from knowing these distributions of the predicted travel
    time? Imagine you are a New York cab driver. At Croxton, an art dealer boards
    your taxi. She wants to participate in a great art auction that starts in 25 minutes
    and offers you a generous tip ($500) if she arrives there on time. That’s quite
    an incentive!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这些预测旅行时间的分布能给你带来什么好处？想象你是一名纽约出租车司机。在Croxton，一位艺术经销商上了你的出租车。她想要参加一场25分钟后开始的盛大艺术品拍卖会，如果她准时到达，会给你丰厚的小费（500美元）。这可是相当大的激励！
- en: Your satnav tool proposes two routes (see the left panel of figure 1.1). As
    a first impulse, you would probably choose the upper route because, for this route,
    it estimates a travel time of 19 minutes, which is shorter than the 22 minutes
    for the other route. But, fortunately, you always have the newest gadgets, and
    your satnav uses a probabilistic model that not only outputs the mean travel time
    but also a whole distribution of travel times. Even better, you know how to make
    use of the outputted distribution for the travel times.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你的satnav工具提出了两条路线（见图1.1的左侧面板）。作为一个第一反应，你可能倾向于选择上面的路线，因为对于这条路线，它估计的旅行时间是19分钟，比另一条路线的22分钟短。但幸运的是，你总是拥有最新的设备，你的satnav使用的是概率模型，不仅输出平均旅行时间，还输出整个旅行时间分布。更好的是，你知道如何利用输出的旅行时间分布。
- en: '![](../Images/1-1.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/1-1.png)'
- en: Figure 1.1 Travel time prediction of the satnav. On the left side of the map,
    you see a deterministic version--just a single number is reported. On the right
    side, you see the probability distributions for the travel time of the two routes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 satnav的旅行时间预测。在地图的左侧，你看到的是一个确定性版本——只报告一个数字。在右侧，你看到两条路线的旅行时间概率分布。
- en: 'You realize that in your current situation, the mean travel time is not very
    interesting. What really matters to you is the following question: With which
    route do you have the better chance of getting the $500 tip? To answer this question,
    you can look at the distributions on the right side of figure 1.1\. After a quick
    eyeball analysis, you conclude that you have a better chance of getting the tip
    when taking the lower route, even though it has a larger mean travel time. The
    reason is that the narrow distribution of the lower route has a larger fraction
    of the distribution corresponding to travel times shorter than 25 minutes. To
    support your assessment with hard numbers, you can use the satnav tool with the
    probabilistic model to compute for both distributions the probability of arriving
    at MoMA in less than 25 minutes. This probability corresponds to the proportion
    of the area under the curve left of the dashed line in figure 1.1, which indicates
    a critical value of 25 minutes. Letting the tool compute the probabilities from
    the distribution, you know that your chance of getting the tip is 93% when taking
    the lower route and only 69% when taking the upper road.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你意识到，在你当前的情况下，平均旅行时间并不是很有趣。真正对你来说重要的是以下问题：哪条路线你更有可能得到500美元的小费？为了回答这个问题，你可以查看图1.1右侧的分布。经过快速目测分析，你得出结论，即使平均旅行时间更长，选择下方的路线你得到小费的机会更大。原因是下方路线的分布较窄，其中对应于25分钟以下旅行时间的分布比例更大。为了用硬性数据支持你的评估，你可以使用带有概率模型的导航工具来计算两种分布到达MoMA少于25分钟的概率。这个概率对应于图1.1中虚线左侧曲线下面积的比例，这表明25分钟是一个关键值。让工具从分布中计算概率，你知道选择下方的路线时得到小费的机会是93%，而选择上方的路线时只有69%。
- en: As discussed in this cab driver example, the main advantages of probabilistic
    models are that these can capture the uncertainties in most real-world applications
    and provide essential information for decision making. Other examples of the use
    of probabilistic models include self-driving cars or digital medicine probabilistic
    models. You can also use probabilistic DL to generate new data that is similar
    to your observed data. A famous fun application is to create realistic looking
    faces of non-existing people. We talk about this in chapter 6\. Let’s first look
    at DL from a bird’s-eye view before peeking into the curve-fitting part.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在这个出租车司机例子中讨论的那样，概率模型的主要优势是它们可以捕捉大多数实际应用中的不确定性，并为决策提供必要的信息。概率模型的其他应用例子包括自动驾驶汽车或数字医学概率模型。你还可以使用概率深度学习（DL）生成与观察数据相似的新数据。一个著名的有趣应用是创建看起来真实不存在的人的面孔。我们将在第6章中讨论这一点。在深入了解曲线拟合部分之前，让我们先从宏观的角度看看深度学习（DL）。
- en: 1.2 A first brief look at deep learning (*DL*)
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 深度学习（*DL*）的初步了解
- en: What is DL anyway? When asked for a short elevator pitch, we would say that
    it’s a machine learning(ML) technique based on artificial neural networks(NNs)
    and that it’s loosely inspired by the way the human brain works. Before giving
    our personal definition of DL, we first want to give you an idea of what an artificial
    NN looks like (see figure 1.2).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）究竟是什么呢？当被要求给出一个简短的电梯式介绍时，我们会说它是一种基于人工神经网络（NN）的机器学习（ML）技术，并且它松散地受到人脑工作方式的启发。在我们给出自己对深度学习（DL）的定义之前，我们首先想给你一个关于人工神经网络（NN）外观的初步概念（见图1.2）。
- en: '![](../Images/1-2.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片1-2](../Images/1-2.png)'
- en: Figure 1.2 An example of an artificial neural network (NN) model with three
    hidden layers. The input layers hold as many neurons as we have numbers to describe
    the input.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 一个包含三个隐藏层的人工神经网络（NN）模型示例。输入层包含与我们描述输入所需数量相等的神经元。
- en: In figure 1.2, you can see a typical traditional artificial NN with three hidden
    layers and several neurons in each layer. Each neuron within a layer is connected
    with each neuron in the next layer.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在图1.2中，你可以看到一个典型的传统人工神经网络（NN），它包含三个隐藏层以及每层中的几个神经元。同一层的每个神经元都与下一层的每个神经元相连。
- en: An artificial NN is inspired by the brain that consists of up to billions of
    neurons processing, for example, all sensory perceptions such as vision or hearing.
    Neurons within the brain aren’t connected to every other neuron, and a signal
    is processed through a hierarchical network of neurons. You can see a similar
    hierarchical network structure in the artificial NN shown in figure 1.2\. While
    a biological neuron is quite complex in how it processes information, a neuron
    in an artificial NN is a simplification and abstraction of its biological counterpart.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络受到大脑的启发，大脑由多达数十亿个神经元组成，处理例如视觉或听觉等所有感官感知。大脑中的神经元并不与每个其他神经元相连，信号通过神经元分层网络进行处理。你可以在图1.2中看到类似分层网络结构的人工神经网络。虽然生物神经元在处理信息方面相当复杂，但人工神经网络中的神经元是其生物对应物的简化和抽象。
- en: To get a first idea about an artificial NN, you can better imagine a neuron
    as a container for a number. The neurons in the input layer are correspondingly
    holding the numbers of the input data. Such input data could, for example, be
    the age (in years), income (in dollars), and height (in inches) of a customer.
    All neurons in the following layers get the weighted sum of the values from the
    connected neurons in the previous layer as their input. In general, the different
    connections aren’t equally important but have weights, which determine the influence
    of the incoming neuron’s value on the neuron’s value in the next layer. (Here
    we omit that this input is further transformed within the neuron.) DL models are
    NNs, but they also have a large number of hidden layers (not just three as in
    the example from figure 1.2).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要对人工神经网络有一个初步的了解，你最好将神经元想象成一个数字的容器。输入层中的神经元相应地持有输入数据的数字。例如，这些输入数据可以是客户的年龄（以年为单位）、收入（以美元为单位）和身高（以英寸为单位）。后续层中的所有神经元都接收来自前一层的连接神经元的加权值之和作为它们的输入。一般来说，不同的连接并不同等重要，但具有权重，这些权重决定了输入神经元值对下一层神经元值的影响。（这里我们省略了输入在神经元内部进一步转换的情况。）深度学习模型是神经网络，但它们也有大量的隐藏层（不仅仅是图1.2中的例子中的三个）。
- en: The weights (strength of connections between neurons) in an artificial NN need
    to be learned for the task at hand. For that learning step, you use training data
    and tune the weights to optimally fit the data. This step is called fitting. Only
    after the fitting step can you use the model to do predictions on new data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络中的权重（神经元之间连接的强度）需要学习以完成当前任务。为此学习步骤，你使用训练数据并调整权重以最佳地拟合数据。这一步骤被称为拟合。只有完成拟合步骤后，你才能使用模型对新数据进行预测。
- en: Setting up a DL system is always a two-stage process. In the first step, you
    choose an architecture. In figure 1.2, we chose a network with three layers in
    which each neuron from a given layer is connected to each neuron in the next layer.
    Other types of networks have different connections, but the principle stays the
    same. In the next step, you tune the weights of the model so that the training
    data is best described. This fitting step is usually done using a procedure called
    gradient descent. You’ll learn more about gradient descent in chapter 3.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 设置深度学习系统总是一个两阶段的过程。在第一步中，你选择一个架构。在图1.2中，我们选择了一个包含三个层的网络，其中每一层的每个神经元都与下一层的每个神经元相连。其他类型的网络有不同的连接方式，但原理保持不变。在下一步中，你调整模型的权重，以便最好地描述训练数据。这一调整步骤通常使用称为梯度下降的程序来完成。你将在第3章中了解更多关于梯度下降的内容。
- en: Note that this two-step procedure is nothing special to DL but is also present
    in standard statistical modeling and ML. The underlying principles of fitting
    are the same for DL, ML, and statistics. We’re convinced that you can profit a
    lot by using the knowledge that was gained in the field of statistics during the
    last centuries. This book acknowledges the heritage of traditional statistics
    and builds on it. Because of this, you can understand much of DL by looking at
    something as simple as linear regression, which we introduce in this chapter and
    use throughout the book as an easy example. You’ll see in chapter 4 that linear
    regression already is a probabilistic model providing more information than just
    one predicted output value for each sample. In that chapter, you’ll learn how
    to pick an appropriate distribution to model the variability of the outcome values.
    In chapter 5, we’ll show you how to use the TensorFlow Probability framework to
    fit such a probabilistic DL model. You can then transfer this approach to new
    situations allowing you to design and fit appropriate probabilistic DL models
    that not only provide high performance predictions but also capture the noise
    of the data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个两步程序对深度学习来说并不特别，它也存在于标准的统计建模和机器学习中。拟合的潜在原理对于深度学习、机器学习和统计学是相同的。我们坚信，你可以通过使用在过去几个世纪中在统计学领域获得的知识而受益良多。这本书承认了传统统计学的遗产，并在此基础上构建。因此，你可以通过观察像线性回归这样简单的东西来理解深度学习的许多内容，我们将在本章介绍线性回归，并在整本书中将其作为易于理解的例子使用。你将在第4章中看到，线性回归已经是一个概率模型，它为每个样本提供的信息不仅仅是一个预测输出值。在第4章中，你将学习如何选择一个合适的分布来模拟结果值的变异性。在第5章中，我们将向你展示如何使用TensorFlow
    Probability框架来拟合这样的概率深度学习模型。然后你可以将这种方法转移到新的情境中，允许你设计和拟合适当的概率深度学习模型，这些模型不仅提供高性能的预测，而且还能捕捉数据的噪声。
- en: 1.2.1 A success story
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.1 一个成功的故事
- en: 'DL has revolutionized areas that so far have been especially hard to master
    with traditional ML approaches but that are easy to solve by humans, such as the
    ability to recognize objects in images (computer vision) and to process written
    text (natural language processing) or, more generally, any kind of perception
    tasks. Image classification is far from being only an academic problem and is
    used for a variety of applications:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已经革命性地改变了那些迄今为止用传统机器学习方法难以掌握，但人类却容易解决的领域，例如识别图像中的对象（计算机视觉）和处理书面文本（自然语言处理），或者更普遍地说，任何类型的感知任务。图像分类远非仅仅是学术问题，它被用于各种应用：
- en: Face recognition
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人脸识别
- en: Diagnostics of brain tumors in MRI data
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在MRI数据中对脑肿瘤进行诊断
- en: Recognition of road signs for self-driving cars
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为自动驾驶汽车识别路标
- en: Although DL reveals its potential in different application areas, probably the
    easiest to grasp is in the field of computer vision. We therefore use computer
    vision to motivate DL by one of its biggest success stories.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习在不同应用领域都展现出了其潜力，但可能最容易理解的是在计算机视觉领域。因此，我们使用计算机视觉通过其最大的成功故事之一来激发深度学习。
- en: In 2012, DL made a splash when Alex Krizhevsky from Geoffrey Hinton’s lab crushed
    all competitors in the internationally renowned ImageNet competition with a DL-based
    model. In this competition, teams from leading computer vision labs trained their
    models on a big data set of ~1 million images with the goal of teaching these
    to distinguish 1,000 different classes of image content. Examples for such classes
    are ships, mushrooms, and leopards. In the competition, all trained models had
    to list the five most probable classes for a set of new test images. If the right
    class wasn’t among the proposed classes, the test image counted as an error (see
    figure 1.3, which shows how DL-based approaches took image classification by storm).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 2012年，深度学习在Alex Krizhevsky（来自杰弗里·辛顿的实验室）使用基于深度学习的模型在国际知名的ImageNet竞赛中击败所有竞争对手时引起了轰动。在这个竞赛中，来自领先计算机视觉实验室的团队在包含约100万张图像的大数据集上训练他们的模型，目的是教会这些模型区分1000种不同的图像内容类别。这些类别的例子包括船只、蘑菇和豹子。在竞赛中，所有训练好的模型都必须列出针对一组新测试图像的五种最可能的类别。如果正确的类别不在提出的类别中，则测试图像被视为错误（见图1.3，它展示了基于深度学习的方法是如何横扫图像分类的）。
- en: '![](../Images/1-3.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/1-3.png)'
- en: Figure 1.3 The impressive results of DL in the ImageNet competition
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 深度学习在ImageNet竞赛中的令人印象深刻的结果
- en: 'Before DL entered the competition, the best programs had an error rate of ~25%.
    In 2012, Krizhevsky was the first to use DL and achieved a huge drop in the error
    rate (by 10% to only ~15%). Only a year later, in 2013, almost all competitors
    used DL, and in 2015, different DL-based models reached the level of human performance,
    which is about 5%. You might wonder why humans misclassify 1 image in 20 (5%).
    A fun fact: there are 170 different dog breeds in that data set, which makes it
    a bit harder for humans to correctly classify the images.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习进入竞赛之前，最好的程序的错误率约为25%。2012年，Krizhevsky首次使用深度学习，将错误率大幅降低（降低了10%，降至约15%）。仅仅一年后，2013年，几乎所有竞争者都开始使用深度学习，到2015年，基于深度学习的不同模型达到了人类水平，大约为5%。你可能想知道为什么人类在20张图片中会错误分类1张（5%）。一个有趣的事实：数据集中有170种不同的狗品种，这使得人类正确分类图像变得有些困难。
- en: 1.3 Classification
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 分类
- en: Let’s look at the differences between non-probabilistic, probabilistic, and
    Bayesian probabilistic classification. DL is known to outperform traditional methods,
    especially in image classification tasks. Before going into details, we want to
    use a face recognition problem to give you a feeling for the differences and the
    commonalities between a DL approach and a more traditional approach to face recognition.
    As a side note, face recognition is actually the application that initially brought
    us into contact with DL.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看非概率性、概率性和贝叶斯概率性分类之间的区别。深度学习因其优于传统方法而闻名，尤其是在图像分类任务中。在深入细节之前，我们想通过一个人脸识别问题来给你一个深度学习方法和更传统的人脸识别方法之间的差异和共性的感觉。作为旁注，人脸识别实际上是让我们最初接触深度学习的应用。
- en: As statisticians, we had a collaboration project with some computer science
    colleagues for doing face recognition on a Raspberry Pi minicomputer. The computer
    scientists challenged us by kidding about the age of the used statistical methods.
    We took the challenge and brought them to a surprised silence by proposing DL
    to tackle our face recognition problem. The success in this first project triggered
    many other joint DL projects, and our interests grew, looking deeper into the
    underlying principles of these models.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 作为统计学家，我们与一些计算机科学同事合作，在树莓派微型计算机上进行了人脸识别项目。计算机科学家通过嘲笑我们使用的统计方法的时代来挑战我们。我们接受了挑战，并提出了深度学习来解决我们的人脸识别问题，这让他们感到惊讶。这个第一个项目的成功触发了许多其他联合深度学习项目，我们的兴趣也随之增长，开始深入研究这些模型的基本原理。
- en: Let’s look at a specific task. Sara and Chantal were together on holidays and
    took many pictures, each showing at least one of them. The task is to create a
    program that can look at a photo and determine which of the two women is in the
    photo. To get a training data set, we labeled 900 pictures, 450 for each woman,
    with the name of the pictured woman. You can imagine that images can be very different
    at first sight because the women might be pictured from different angles, laughing
    or tired, dressed up or casual, or having a bad hair day. Still, for you, the
    task is quite easy. But for a computer, an image is only an array of pixel values,
    and programming it to tell the difference between two women is far from trivial.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个具体任务。Sara和Chantal在假期中一起旅行并拍了好多照片，每张照片至少有她们中的一个。任务是创建一个程序，可以查看照片并确定照片中的两位女士中哪一位在照片中。为了获得训练数据集，我们标记了900张照片，每位女士450张，并附上了照片中女士的名字。你可以想象，从第一眼看上去，图像可能会有很大的不同，因为女士们可能从不同的角度被拍摄，可能是笑着或疲惫的，可能是盛装打扮或休闲的，或者可能是在一个糟糕的发型日。尽管如此，对你来说，这个任务相当简单。但对于计算机来说，图像只是一个像素值的数组，编程它来区分两位女士远非易事。
- en: 1.3.1 Traditional approach to image classification
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.1 图像分类的传统方法
- en: A traditional approach to image classification doesn’t directly start with the
    pixel values of the images but tackles the classification task in a two-step process.
    As a first step, experts in the field define features that are useful to classify
    the images. A simple example of such a feature would be the mean intensity value
    of all pixels, which can be useful to distinguish night shots from pictures taken
    during the day. Usually these features are more complex and tailored to a specific
    task. In the face recognition problem, you can think about easily understandable
    features like the length of the nose, width of the mouth, or the distance between
    the eyes (figure 1.4).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类的传统方法并不是直接从图像的像素值开始，而是通过两步过程来处理分类任务。首先，该领域的专家定义出对图像分类有用的特征。这样一个特征的简单例子就是所有像素的平均强度值，这可以用来区分夜间拍摄的照片和白天拍摄的照片。通常这些特征更为复杂，并且针对特定任务进行定制。在人脸识别问题中，你可以考虑一些容易理解的特征，比如鼻子的长度、嘴巴的宽度，或者眼睛之间的距离（图1.4）。
- en: '![](../Images/1-4.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图1.4](../Images/1-4.png)'
- en: Figure 1.4 Chantal (left) has a large distance between the eyes and a rather
    small mouth. Sara (right) has a small distance between the eyes and a rather large
    mouth.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 Chantal（左）的眼睛间距较大，嘴巴相对较小。Sara（右）的眼睛间距较小，嘴巴相对较大。
- en: 'But these kinds of high-level features often are difficult to determine because
    many aspects need to be taken into account, such as mimics, scale, receptive angle,
    or light conditions. Therefore, non-DL approaches often use less interpretable,
    low-level features like SIFT features (Scale-Invariant Feature Transform), capturing
    local image properties such as magnification or rotation that are invariant to
    transformations. You can, for example, think about an edge detector: an edge won’t
    disappear if the image is rotated or scaled.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些高级特征往往很难确定，因为需要考虑许多方面，例如表情、尺度、接收角度或光照条件。因此，非深度学习（non-DL）方法通常使用更不可解释的低级特征，如SIFT特征（尺度不变特征变换），捕捉局部图像属性，如放大或旋转，这些属性对变换是不变的。例如，你可以考虑边缘检测器：如果图像被旋转或缩放，边缘不会消失。
- en: Already this simple example makes clear that feature engineering, meaning defining
    and extracting those properties from the image that are important for the classification,
    is a complicated and time-consuming task. It usually requires a high level of
    expertise. The (slow) progress in many applications of computer vision like face
    recognition was mainly driven by the construction of new and better features.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 就这个简单的例子来说，特征工程，即定义和从图像中提取对分类重要的属性，是一项复杂且耗时的任务。这通常需要高水平的专业知识。在许多计算机视觉应用（如人脸识别）中的（缓慢）进步主要是由构建新的和更好的特征所驱动的。
- en: NOTE You need to extract all these features from all images before you can tackle
    the actual classification task.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在处理实际的分类任务之前，你需要从所有图像中提取所有这些特征。
- en: After the features-extraction step, the values of these features represent each
    image. In order to identify Sara or Chantal from this feature representation of
    the image, you need to choose and fit a classification.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征提取步骤之后，这些特征值代表每个图像。为了从图像的特征表示中识别Sara或Chantal，你需要选择并拟合一个分类器。
- en: 'What is the task of such a classification model? It should discriminate between
    the different class labels. To visualize this idea, let’s imagine that an image
    is described by only two features: say, distance of the eyes and width of the
    mouth. (We are aware that in most real cases, a good characterization of an image
    requires many more features.)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的分类模型的任务是什么？它应该能够区分不同的类别标签。为了可视化这个想法，让我们想象一个图像只由两个特征来描述：比如说，眼睛的距离和嘴巴的宽度。（我们意识到在大多数实际情况下，对图像的良好描述需要许多更多的特征。）
- en: Because the women aren’t always pictured head on but from different viewpoints,
    the apparent distance between the eyes isn’t always the same for the same women.
    The apparent width of the mouth can vary even more, depending if the woman laughs
    or makes an air-kiss. When representing each image of the pictured woman by these
    two features, the feature space can be visualized by a 2D plot. One axis indicates
    the eye distance and the other axis shows the mouth width (see figure 1.5). Each
    image is represented by a point; images of Sara are labeled with an S and images
    of Chantal with a C.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于女性不总是正面呈现，而是从不同的视角呈现，因此对于同一女性，眼睛之间的明显距离并不总是相同的。嘴巴的明显宽度可能会变化更大，这取决于女性是否在笑或做出飞吻。当用这两个特征来表示被描绘的女性的每一张图像时，特征空间可以通过二维图来可视化。一个轴表示眼睛距离，另一个轴显示嘴巴宽度（见图1.5）。每个图像都表示为一个点；Sara的图像用S标记，Chantal的图像用C标记。
- en: '![](../Images/1-5.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图1.5](../Images/1-5.png)'
- en: Figure 1.5 A 2D space spanned by the features mouth width and eye distance.
    Each point represents an image described by these two features (S for Sara and
    C for Chantal). The dashed line is a decision boundary separating the two classes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 由特征嘴巴宽度和眼睛距离构成的二维空间。每个点代表由这两个特征描述的图像（S代表Sara，C代表Chantal）。虚线是分隔两个类别的决策边界。
- en: 'One way you can think about a non-probabilistic classification model is that
    the model defines decision boundaries (see the dashed line in figure 1.5) that
    split the feature space into different regions. Each resulting region corresponds
    to one class label. In our example, we’ve determined a Sara region and a Chantal
    region. You can now use this decision boundary to classify new images from which
    you only know the values for the two features: if the corresponding point in the
    2D feature space ends up in the Sara region, you classify it as Sara; otherwise,
    as Chantal.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以这样考虑一个非概率分类模型：模型定义了决策边界（见图1.5中的虚线），将特征空间分割成不同的区域。每个区域对应一个类别标签。在我们的例子中，我们确定了Sara区域和Chantal区域。现在你可以使用这个决策边界来对新图像进行分类，这些新图像你只知道两个特征的值：如果二维特征空间中的对应点最终落在Sara区域，你将其分类为Sara；否则，分类为Chantal。
- en: You might know from your data analysis experiences some methods like the following,
    which you can use for classification. (Don’t worry if you aren’t familiar with
    these methods.)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能从你的数据分析经验中知道一些如下所示的方法，你可以用它们进行分类。（如果你不熟悉这些方法，请不要担心。）
- en: Logistic or multinomial regression
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归或多项式回归
- en: Random forest
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Support vector machines
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Linear discriminant analysis
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性判别分析
- en: Most classification models, including the listed methods and also DL, are parametric
    models, meaning the model has some parameters that determine the course of the
    boundaries. The model is only ready to actually perform a classification or class
    probability prediction after replacing the parameters by certain numbers. Fitting
    is about how to find these numbers and how to quantify the certainty of these
    numbers.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数分类模型，包括列出的方法和深度学习（DL），都是参数模型，这意味着模型有一些参数决定了边界的走向。模型只有在用某些数字替换这些参数之后，才能准备进行实际的分类或类别概率预测。拟合就是关于如何找到这些数字以及如何量化这些数字的确定性。
- en: 'Fitting the model to a set of training data with known class labels determines
    the values of the parameter and fixes the decision boundaries in the feature space.
    Depending on the classification method and the number of parameters, these decision
    boundaries could be simple straight lines or a complex boundary with wiggles.
    You can summarize the traditional workflow to set up a classification method in
    three steps:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型拟合到一组具有已知类别标签的训练数据中，确定了参数的值并固定了特征空间中的决策边界。根据分类方法和参数的数量，这些决策边界可以是简单的直线或带有波动的复杂边界。你可以将设置分类方法的传统工作流程总结为三个步骤：
- en: Defining and extracting features from the raw data
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义并从原始数据中提取特征
- en: Choosing a parametric model
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择参数模型
- en: Fitting the classification model to the data by tuning its parameter
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调整参数来拟合分类模型到数据
- en: To evaluate the performance of the models, you use a validation data set that
    is not used during the training. A validation data set in the face recognition
    example would consist of new images of Chantal and Sara that were not part of
    the training data set. You then can use the trained model to predict the class
    label and use the percentage of correct classifications as a (non-probabilistic)
    performance measure.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型的性能，你使用一个在训练过程中未使用的验证数据集。在人脸识别的例子中，验证数据集将包括Chantal和Sara的新图像，这些图像不是训练数据集的一部分。然后你可以使用训练好的模型来预测类别标签，并使用正确分类的百分比作为（非概率性）性能指标。
- en: Depending on the situation, one or another classification method will achieve
    better results on the validation data set. However, in classical image classification,
    the most important ingredient for success isn’t the choice of the classification
    algorithm but the quality of the extracted image features. If the extracted features
    take different values for images from different classes, you’ll see a clear separation
    of the respective points in the feature space. In such a situation, many classification
    models show a high classification performance.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 根据具体情况，一种或另一种分类方法将在验证数据集上取得更好的结果。然而，在经典图像分类中，成功最重要的因素不是分类算法的选择，而是提取的图像特征的质量。如果提取的特征对不同类别的图像具有不同的值，你将在特征空间中看到相应点的清晰分离。在这种情况下，许多分类模型都表现出很高的分类性能。
- en: With the example of discriminating Sara from Chantal, you went through the traditional
    image classification workflow. For getting good features, you first had to recognize
    that these two women differ in their mouth width and their eye distance. With
    these specific features, you saw that it is easy to build a good classifier. However,
    for discriminating between two other women, these features might not do the trick,
    and you would need to start over with the feature developing process again. This
    is a common drawback when working with customized features.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以区分Sara和Chantal为例，你经历了传统的图像分类工作流程。为了获得好的特征，你首先必须认识到这两位女士的嘴巴宽度和眼睛距离不同。有了这些特定的特征，你看到构建一个好的分类器很容易。然而，要区分其他两位女士，这些特征可能不起作用，你需要重新开始特征开发过程。这是使用定制特征时的一个常见缺点。
- en: 1.3.2 Deep learning approach to image classification
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.2 图像分类的深度学习方法
- en: In contrast to the traditional approach to image classification, the DL approach
    starts directly from the raw image data and uses only the pixel values as the
    input features to the model. In this feature representation of an image, the numbers
    of pixels define the dimension of the feature space. For a low-resolution picture
    with 100 × 100 pixels, this already amounts to 10,000.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的图像分类方法相比，深度学习（DL）方法直接从原始图像数据开始，并且只使用像素值作为模型输入特征。在这种图像特征表示中，像素的数量定义了特征空间的维度。对于一个100
    × 100像素的低分辨率图片，这已经相当于10,000个像素。
- en: Besides such a high dimension, the main challenge is that pixel similarity of
    two pictures doesn’t imply that the two images correspond to the same class label.
    Figure 1.6 illustrates where the images in the same column obviously correspond
    to the same class but are different on the pixel level. Simultaneously, images
    in the same row of figure 1.6 show high pixel similarity but don’t correspond
    to the same class.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 除了高维度之外，主要挑战是两张图片的像素相似性并不一定意味着这两张图片对应于相同的类别标签。图1.6说明了同一列中的图像显然对应于同一类，但在像素级别上是不同的。同时，图1.6中同一行的图像显示出高像素相似性，但并不对应于同一类。
- en: '![](../Images/1-6.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片1-6](../Images/1-6.png)'
- en: Figure 1.6 The left column shows two images of the class dog. The right column
    shows two images of the class table. When comparing the pictures on the pixel
    level, the two images in the same column are less similar than the two images
    in the same row, even if one image in a row shows a dog and the other image displays
    a table.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 左列显示了同一类狗的两个图像。右列显示了同一类桌子（table）的两个图像。在像素级别比较图片时，同一列中的两个图像比同一行中的两个图像更不相似，即使一行中的一个图像显示的是狗，而另一个图像显示的是桌子。
- en: The core idea of DL is to replace the challenging and time-consuming task of
    feature engineering by incorporating the construction of appropriate features
    into the fitting process. Also, DL can’t do any magic so, similar to traditional
    image analysis, the features have to be constructed from the pixel values at hand.
    This is done via the hidden layers of the DL model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）的核心思想是通过将适当特征的构建纳入拟合过程来替代具有挑战性和耗时特征工程任务。此外，深度学习（DL）不能做任何魔法，因此，类似于传统的图像分析，特征必须从手头的像素值构建。这是通过深度学习（DL）模型的隐藏层完成的。
- en: Each neuron combines its inputs to yield a new value, and in this manner, each
    layer yields a new feature representation of the input. Using many hidden layers
    allows the NN to decompose a complicated transformation from the raw data to the
    outcome in a hierarchy of simple transformations. When going from layer to layer,
    you get a more and more abstract representation of the image that becomes better
    suited for discriminating between the classes. You’ll learn more about this in
    chapter 2, where you’ll see that during the fitting process of a DL model, a hierarchy
    of successively more complex features is learned. This then allows you to discriminate
    between the different classes without the need of manually specifying the appropriate
    features.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元将其输入组合以产生新的值，并且以这种方式，每一层都产生输入的新特征表示。使用许多隐藏层允许神经网络（NN）将原始数据到结果之间的复杂转换分解为一系列简单的转换。当从一层到另一层时，你会得到越来越抽象的图像表示，这更适合区分不同的类别。你将在第2章中了解更多关于这一点，你将看到在深度学习（DL）模型的拟合过程中，会学习到一系列越来越复杂的特征。这然后允许你区分不同的类别，而无需手动指定适当的特征。
- en: Branding DL (Deep learning)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（Deep learning）的定位（Branding DL）
- en: In the earlier days of machine learning (ML), neural networks (NNs) were already
    around, but it was technically impossible to train deep NNs with many layers,
    mainly because of a lack of computer power and training data. With the technical
    obstacles resolved, some tricks have been discovered that made it possible to
    train NNs with several hundred layers.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习（ML）的早期，神经网络（NNs）已经存在，但技术上无法训练具有许多层的深度神经网络，这主要是因为缺乏计算能力和训练数据。随着技术障碍的解决，已经发现了一些技巧，使得训练具有数百层的神经网络成为可能。
- en: Why do we talk about DL instead of artificial NNs? DL sells better than artificial
    NNs. This might sound disrespectful, but such rebranding was probably a smart
    move, especially because NNs haven’t delivered what was promised during the last
    decades and, therefore, gained a somewhat bad reputation. We work with “deep”
    NNs with many hidden layers. This leads to a deep hierarchy in the construction
    of features, allowing these to become more abstract with every step up in the
    hierarchy.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们谈论深度学习（DL）而不是人工神经网络（NNs）？深度学习（DL）比人工神经网络（NNs）更受欢迎。这听起来可能有些不尊重，但这样的重新定位可能是一次明智的举动，特别是由于神经网络在过去的几十年中没有实现承诺，因此获得了一些不良声誉。我们使用具有许多隐藏层的“深度”神经网络。这导致在特征构建中有一个深层次的结构，使得随着层次结构的每一步上升，特征变得更加抽象。
- en: 'After defining the architectures, the network can be understood as a parametric
    model that often contains millions of parameters. The model takes an input *x*
    and produces an output *y*. This is true for every DL model (including reinforcement
    learning). The DL modeling workflow can be summarized in two steps:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义架构之后，网络可以被理解为一个包含数百万参数的参数模型。该模型接受输入 *x* 并产生输出 *y*。这对于每个深度学习（DL）模型（包括强化学习）都是正确的。深度学习（DL）建模工作流程可以总结为两个步骤：
- en: Defining the DL model architecture
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义深度学习（DL）模型架构
- en: Fitting the DL model to the raw data
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将深度学习（DL）模型拟合到原始数据
- en: The next sections discuss what is meant by non-probabilistic and probabilistic
    classification models and what benefits you can get from a Bayesian variant of
    a probabilistic classification model.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将讨论非概率和概率分类模型的意义，以及你可以从概率分类模型的贝叶斯变体中获得哪些好处。
- en: 1.3.3 Non-probabilistic classification
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.3 非概率分类
- en: 'Let’s first look at non-probabilistic classification. To make it easy and illustrative,
    we use the image classification example again. The goal in image classification
    is to predict for a given image which class it corresponds to. In the ImageNet
    competition in section 1.2, there were 1,000 different classes. In the face recognition
    example, there were only two classes: Chantal and Sara.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看非概率分类。为了使其简单易懂，我们再次使用图像分类的例子。图像分类的目标是预测给定图像对应的类别。在1.2节中提到的ImageNet竞赛中，有1,000个不同的类别。在人脸识别的例子中，只有两个类别：Chantal和Sara。
- en: In non-probabilistic image classification, you only get the predicted class
    label for each image. More precisely, a non-probabilistic image classifier takes
    an image as input and then predicts only the best guess for the class as output.
    In the face recognition example, it would either output Chantal or Sara. You can
    also think about a non-probabilistic model as a deterministic model without any
    uncertainty. When looking with probabilistic glasses at a non-probabilistic model,
    it seems that a non-probabilistic model is always certain. The non-probabilistic
    model predicts with a probability of one that the image belongs to one specific
    class.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在非概率图像分类中，你只能得到每个图像的预测类别标签。更准确地说，一个非概率图像分类器接收一个图像作为输入，然后只预测类别标签作为输出。在人脸识别的例子中，它可能会输出Chantal或Sara。你也可以将非概率模型视为一个没有不确定性的确定性模型。用概率的视角来看非概率模型，它似乎总是确定的。非概率模型以100%的概率预测图像属于一个特定的类别。
- en: Imagine a situation where the image shows Chantal where she dyes her hair the
    same color as that of Sara and the hair covers Chantal’s face. For a human being,
    it’s quite hard to tell if the image shows Chantal or Sara. But the non-probabilistic
    classifier still provides a predicted class label (for example, Sara) without
    indicating any uncertainty. Or imagine an even more extreme situation where you
    provide an image that shows neither Chantal nor Sara (see figure 1.7). Which prediction
    will you get from the classifier? You would like the classifier to tell you that
    it is not able to make a reliable prediction. But a non-probabilistic classifier
    still yields either Chantal or Sara as a prediction without giving a hint of any
    uncertainty. To tackle such challenges of handling difficult or novel situations,
    we turn to probabilistic models and their Bayesian variants. These can express
    their uncertainty and indicate potentially unreliable predictions.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一种情况，图像显示Chantal染发，颜色与Sara相同，头发覆盖了Chantal的脸。对于人类来说，很难判断图像显示的是Chantal还是Sara。但非概率分类器仍然提供了一个预测的类别标签（例如，Sara），并没有表明任何不确定性。或者想象一个更加极端的情况，你提供了一张既不是Chantal也不是Sara的图像（见图1.7）。分类器会给出哪种预测？你希望分类器告诉你它无法做出可靠的预测。但非概率分类器仍然给出Chantal或Sara作为预测，而没有给出任何不确定性的提示。为了应对处理困难或新颖情况这样的挑战，我们转向概率模型及其贝叶斯变体。这些模型可以表达它们的不确定性，并指出可能不可靠的预测。
- en: '![](../Images/1-7.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片1-7](../Images/1-7.png)'
- en: Figure 1.7 A non-probabilistic image classifier for face recognition takes as
    input an image and yields as outcome a class label. Here the predicted class label
    is Chantal, but only the upper image really shows Chantal. The lower image shows
    a woman who is neither Chantal nor Sara.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7 人脸识别的非概率图像分类器接收一个图像作为输入，并输出一个类别标签。这里预测的类别标签是Chantal，但只有上面的图像真正显示了Chantal。下面的图像显示的是一个既不是Chantal也不是Sara的女士。
- en: 1.3.4 Probabilistic classification
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.4 概率分类
- en: The special thing in probabilistic classification is that you not only get the
    best guess for the class label but also a measure for the uncertainty of the classification.
    The uncertainty is expressed by a probability distribution. In the face recognition
    example, a probabilistic classifier would take a face image and then output a
    certain probability for Chantal and for Sara. Both probabilities add up to 1 (see
    figure 1.8).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率分类中，特别之处在于你不仅得到对类别标签的最佳猜测，还能得到分类的不确定性度量。这种不确定性通过概率分布来表示。在人脸识别的例子中，概率分类器会接收一张人脸图像，然后输出对Chantal和Sara的概率。这两个概率加起来等于1（见图1.8）。
- en: '![](../Images/1-8.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片1-8](../Images/1-8.png)'
- en: Figure 1.8 A probabilistic image classifier for face recognition takes as input
    an image and yields as outcome a probability for each class label. In the upper
    panel, the image shows Chantal, and the classifier predicts a probability of 0.85
    for the class Chantal and a probability of 0.15 for the class Sara. In the lower
    panel, the image shows neither Chantal nor Sara, and the classifier predicts a
    probability of 0.8 for the class Chantal and a probability of 0.2 for the class
    Sara.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8 一个用于人脸识别的概率图像分类器以图像为输入，并输出每个类别标签的概率。在上部面板中，图像显示的是Chantal，分类器预测Chantal类别的概率为0.85，Sara类别的概率为0.15。在下部面板中，图像显示的不是Chantal也不是Sara，分类器预测Chantal类别的概率为0.8，Sara类别的概率为0.2。
- en: To give a best single guess, you would pick the class with the highest probability.
    It is common to think about the probability of the predicted class as an uncertainty
    of the prediction. This is the case when all the images are sufficiently similar
    to the training data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给出最佳的单个猜测，你会选择概率最高的类别。通常认为预测类别的概率是预测的不确定性。当所有图像都足够类似于训练数据时，这种情况是成立的。
- en: But in reality, this is not always the case. Imagine that you provide the classifier
    with an image that shows neither Chantal nor Sara. The classifier has no other
    choice than to assign probabilities to the classes Chantal or Sara. But you would
    hope that the classifier shows its uncertainty by assigning more or less equal
    probabilities to the two possible but wrong classes. Unfortunately, this is often
    not the case when working with probabilistic NN models. Instead, often quite high
    probabilities are still assigned to one of the possible but wrong classes (see
    figure 1.8). To tackle this problem, in part 3 of our book, we extend the probabilistic
    models by taking a Bayesian approach, which can add an additional uncertainty
    that you can use to detect novel classes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 但在现实中，情况并不总是如此。想象一下，你向分类器提供了一张既不是Chantal也不是Sara的图像。分类器除了为Chantal或Sara的类别分配概率外别无选择。但你希望分类器通过为两个可能的但错误的类别分配更多或更少的相等概率来显示其不确定性。不幸的是，当使用概率NN模型工作时，这通常不是情况。相反，通常还会为其中一个可能的但错误的类别分配相当高的概率（参见图1.8）。为了解决这个问题，在我们的书籍第三部分，我们通过采用贝叶斯方法扩展概率模型，这可以添加额外的不确定性，你可以用它来检测新类别。
- en: 1.3.5 Bayesian probabilistic classification
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.5 贝叶斯概率分类
- en: The nice thing about Bayesian models is that these can express uncertainty about
    their predictions. In our face recognition example, the non-Bayesian probabilistic
    model predicts an outcome distribution that consists of the probability for Chantal
    and the probability for Sara, which add up to 1\. But how certain is the model
    about the assigned probabilities? Bayesian models can give an answer to this question.
    In part 3 of this book, you will learn how this is done in detail. At this point,
    let’s just note that you can ask a Bayesian model several times and get different
    answers when you ask it. This reflects the uncertainty inherent in the model (see
    figure 1.9). Don’t worry if you do not see how you get these different model outputs
    for the same input. You will learn about that in the third part of the book.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯模型的好处是，这些模型可以表达其预测的不确定性。在我们的面部识别示例中，非贝叶斯概率模型预测了一个结果分布，包括Chantal的概率和Sara的概率，总和为1。但模型对分配的概率有多确定？贝叶斯模型可以回答这个问题。在本书的第三部分，你将详细了解这是如何完成的。在此阶段，我们只需注意，你可以多次询问贝叶斯模型，并得到不同的答案。这反映了模型内在的不确定性（参见图1.9）。如果你不明白如何为相同的输入得到这些不同的模型输出，请不要担心。你将在本书的第三部分学到这一点。
- en: The main advantage of Bayesian models is that these can indicate a non-reliable
    prediction by a large spread of the different sets of predictions (see lower panel
    of figure 1.9). In this way, you have a better chance to identify novel classes
    like the young lady in the lower panel of figure 1.9 who is neither Chantal nor
    Sara.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯模型的优点在于，这些模型可以通过不同预测集的大范围分布来指示不可靠的预测（参见图1.9的下部面板）。这样，你就有更好的机会识别出如图1.9下部面板中的年轻女士这样的新类别，她既不是Chantal也不是Sara。
- en: '![](../Images/1-9.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图1-9](../Images/1-9.png)'
- en: Figure 1.9 A Bayesian probabilistic image classifier for face recognition takes
    as input an image and yields as outcome a distribution of probability sets for
    the two class labels. In the upper panel, the image is showing Chantal, and the
    predicted sets of probabilities all predict a large probability for Chantal and
    an accordingly low probability for Sara. In the lower panel, the image shows a
    lady who is neither Chantal nor Sara, so the classifier predicts different sets
    of probabilities indicating a high uncertainty.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.9 一个用于人脸识别的贝叶斯概率图像分类器以图像为输入，并输出两个类别标签的概率分布集。在上面的面板中，图像显示的是Chantal，预测的概率集都预测了Chantal的高概率和相应地Sara的低概率。在下方的面板中，图像显示的是既不是Chantal也不是Sara的女士，因此分类器预测了不同的概率集，表明高度的不确定性。
- en: 1.4 Curve fitting
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 曲线拟合
- en: 'We want to finish this introductory chapter talking about the differences in
    probabilistic and non-probabilistic DL methods on regression tasks. Regression
    is sometimes also referred to as curve fitting. This reminds one of the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想在介绍章节的结尾讨论概率和非概率深度学习回归任务中的差异。回归有时也被称为曲线拟合。这让人想起了以下内容：
- en: All the impressive achievements of deep learning amount to just curve fitting.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习所有的令人印象深刻的成就都归结为仅仅是曲线拟合。
- en: --Judea Pearl, 2018
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: --Judea Pearl, 2018
- en: 'When we heard that Judea Pearl, the winner of the prestigious Turing Award
    in 2011 (the computer science equivalent of the Nobel prize), claimed DL to be
    just curve fitting (the same curve fitting done in simple analysis like linear
    regression for centuries), at first we were surprised and even felt a bit offended.
    How could he be so disrespectful about our research subject, which, moreover,
    showed such impressive results in practice? Our relative calmness is probably
    due to the fact that we aren’t computer scientists but have a background in physics
    and statistical data analysis. Curve fitting isn’t just curve fitting for us.
    However, giving his statement a second thought, we can see his point: the underlying
    principles of DL and curve fitting are identical in many respects.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们听说Judea Pearl，2011年获得声望极高的图灵奖（计算机科学的诺贝尔奖）的获得者，声称深度学习仅仅是曲线拟合（与简单的分析如线性回归几个世纪以来所做的相同的曲线拟合），起初我们感到惊讶，甚至有点冒犯。他怎么能对我们的研究主题如此不尊重，毕竟，它在实践中展示了如此令人印象深刻的成果？我们相对的平静可能是因为我们不是计算机科学家，而是有物理学和统计数据分析的背景。曲线拟合对我们来说不仅仅是曲线拟合。然而，对他的声明进行第二次思考，我们可以看到他的观点：深度学习和曲线拟合在许多方面具有相同的基本原理。
- en: 1.4.1 Non-probabilistic curve fitting
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.1 非概率曲线拟合
- en: 'Let’s first take a closer look at the non-probabilistic aspects of traditional
    curve-fitting methods. Loosely speaking, non-probabilistic curve fitting is the
    science of putting lines through data points. With linear regression in its most
    simple form, you put a straight line through the data points (see figure 1.10).
    In that figure, we assume that we have only one feature, *x*, to predict a continuous
    variable, *y*. In this simple case, the linear regression model has only two parameters,
    a and *b* :'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先仔细看看传统曲线拟合方法的非概率方面。粗略地说，非概率曲线拟合是穿过数据点的科学。以最简单的线性回归形式，你将一条直线穿过数据点（见图1.10）。在那张图中，我们假设只有一个特征，*x*，来预测一个连续变量，*y*。在这个简单的情况下，线性回归模型只有两个参数，a和*b*：
- en: '*y* = *a* ⋅ *x* + *b*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *a* ⋅ *x* + *b*'
- en: '![](../Images/1-10.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/1-10.png)'
- en: Figure 1.10 Scatter plot and regression model for the systolic blood pressure(SBP)
    example. The dots are the measured data points; the straight line is the linear
    model. For three age values (22, 47, 71), the positions of the horizontal lines
    indicate the predicted best guesses for the SBP (11, 139, 166).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10 血压（SBP）示例的散点图和回归模型。点代表测量数据点；直线是线性模型。对于三个年龄值（22，47，71），水平线的位置表示对SBP的最佳猜测预测值（11，139，166）。
- en: After the definition of the model, the parameters a and *b* need to be determined
    so that the model can be actually used to predict a single best guess for the
    value of *y* when given *x*. In the context of ML and DL, this step of finding
    good parameter values is called training. But how are networks trained? The training
    of the simple linear regression and DL models is done by fitting the model’s parameters
    to the training data--a.k.a. curve fitting.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义模型之后，需要确定参数a和*b*，以便模型能够实际用于预测给定*x*时*y*的单个最佳猜测值。在机器学习和深度学习的背景下，这一步寻找良好参数值的过程被称为训练。但网络是如何训练的呢？简单线性回归和深度学习模型的训练是通过将模型的参数拟合到训练数据来完成的——也就是曲线拟合。
- en: Note that the number of parameters can be vastly different, ranging from 2 in
    the 1D linear regression case to 500 million for advanced DL models. The whole
    procedure is the same as in linear regression. You’ll learn in chapter 3 how to
    fit the parameter of a non-probabilistic linear regression model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，参数的数量可能会有很大的差异，从一维线性回归案例中的2个到高级深度学习模型中的5亿个不等。整个过程与线性回归相同。你将在第3章中学习如何拟合非概率线性回归模型的参数。
- en: So, what do we mean when we say a non-probabilistic model is fit to data? Let’s
    look at the model *y* = *a* ⋅ *x* + *b* for a concrete example of predicting the
    blood pressure *y* based on the age *x*. Figure 1.10 is a plot of the systolic
    blood pressure (SBP) against the age for 33 American women. Figure 1.10 shows
    concrete realizations with *a* = 1.70 and *b* = 87.7 (the solid line). In a non-probabilistic
    model, for each age value you get only one best guess for the SBP for women of
    this age. In figure 1.10, this is demonstrated for three age values (22, 47, and
    71), where the predicted best guesses for the SBP (111, 139, and 166) are indicated
    by the positions of the dashed horizontal lines.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，当我们说非概率模型拟合到数据时，我们指的是什么？让我们通过一个具体的例子来看一下模型*y* = *a* ⋅ *x* + *b*，这个例子是根据年龄*x*预测血压*y*。图1.10是33位美国女性的收缩压（SBP）与年龄的图表。图1.10显示了*a*
    = 1.70和*b* = 87.7（实线）的具体实现。在非概率模型中，对于每个年龄值，你只能得到这个年龄女性收缩压的一个最佳猜测值。在图1.10中，这通过三个年龄值（22岁、47岁和71岁）来展示，其中预测的最佳猜测值（111、139和166）由虚线水平线的位置表示。
- en: 1.4.2 Probabilistic curve fitting
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.2 概率曲线拟合
- en: What do you get when you fit a probabilistic model to the same data? Instead
    of only a single best guess for the blood pressure, you get a whole probability
    distribution. This tells you that women with the same age might well have different
    SBPs (see figure 1.11). In the non-probabilistic linear regression, an SBP of
    111 is predicted for 22-year-old women (see figure 1.10). Now, when looking at
    the predicted distribution for 22-year-old women, SBP values close to 111 (the
    peak of the distribution) are expected with higher probability than values further
    away from 111.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将概率模型拟合到相同的数据时，你会得到什么？你不会只得到一个关于血压的最佳猜测值，而会得到一个完整的概率分布。这告诉你，相同年龄的女性可能会有不同的收缩压（见图1.11）。在非概率线性回归中，预测22岁女性的收缩压为111（见图1.10）。现在，当查看22岁女性的预测分布时，接近111（分布的峰值）的收缩压值比远离111的值更有可能。
- en: '![](../Images/1-11.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/1-11.png)'
- en: Figure 1.11 Scatter plot and regression model for the systolic blood pressure
    (SBP) example. The dots are the measured data points. At each age value (22, 47,
    71), a Gaussian distribution is fitted that describes the probability distribution
    of possible SBP values of women in these age groups. For the three age values,
    the predicted probability distributions are shown. The solid line indicates the
    positions of the mean values of all distributions corresponding to the ages between
    16 and 90 years. The upper and lower dashed lines indicate an interval in which
    95% of all values are expected by the model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.11展示了收缩压（SBP）示例的散点图和回归模型。点代表测量数据点。在每个年龄值（22岁、47岁、71岁）处，拟合高斯分布来描述这些年龄组女性可能的收缩压值的概率分布。对于这三个年龄值，显示了预测的概率分布。实线表示对应于16至90岁之间所有分布的平均值的位置。上下的虚线表示模型预期95%的所有值所在的区间。
- en: The solid line in figure 1.10 indicates the positions of the mean values of
    all distributions corresponding to the age values between 16 and 90 years. The
    solid line in figure 1.11 exactly matches the regression line in figure 1.10,
    which is predicted from a non-probabilistic model. The dashed lines that are parallel
    to the mean indicate an interval in which 95% of all individual SBP values are
    expected by the model.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10中的实线表示16至90岁之间所有分布对应平均值的位臵。图1.11中的实线与图1.10中的回归线完全吻合，该回归线是由非概率模型预测得出的。与平均值平行的虚线表示一个区间，模型预计在这个区间内95%的个体收缩压值（SBP）都会出现。
- en: How do you find the optimal values for the parameters in a non-probabilistic
    and a probabilistic model? Technically, you use a loss function that describes
    how poorly the model fits the (training) data and then minimizes it by tuning
    the weights of the model. You’ll learn about loss functions and how to use these
    for fitting non-probabilistic or probabilistic models in chapters 3, 4, and 5\.
    You’ll then see the difference between the loss function of a non-probabilistic
    and a probabilistic model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何找到非概率模型和概率模型中参数的最佳值？技术上，你使用一个损失函数来描述模型与（训练）数据拟合得有多差，然后通过调整模型的权重来最小化它。你将在第3章、第4章和第5章中了解损失函数以及如何使用这些函数来拟合非概率或概率模型。你将看到非概率模型和概率模型损失函数之间的区别。
- en: The discussed linear regression model is, of course, simple. We use it mainly
    to explain the underlying principles that stay the same when turning to complex
    DL models. In real world applications, you would often not assume a linear dependency,
    and you would also not always want to assume that the variation of the data stays
    constant. You’ll see in chapter 2 that it’s easy to set up a NN that can model
    non-linear relationships. In chapters 4 and 5, you’ll see that it is also not
    hard to build a probabilistic model for regression tasks that can model data with
    non-linear behavior and changing variations (see figure 1.12). To evaluate the
    performance of a trained regression model, you should always use a validation
    data set that is not used during training. In figure 1.12, you can see the predictions
    of a probabilistic DL model on a new validation set that shows that the model
    is able to capture the non-linear behavior of the data and also the changing data
    variation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论的线性回归模型当然是简单的。我们主要用它来解释在转向复杂深度学习（DL）模型时保持不变的基本原理。在实际应用中，你通常不会假设数据之间存在线性依赖关系，你也不总是想假设数据的变化保持恒定。在第2章中，你将看到设置一个能够模拟非线性关系的神经网络是多么容易。在第4章和第5章中，你将看到构建一个能够模拟具有非线性行为和变化变化的回归任务的概率模型也不是很难（见图1.12）。为了评估训练好的回归模型的表现，你应该始终使用一个在训练过程中未使用的验证数据集。在图1.12中，你可以看到对一个新的验证集的预测，这表明模型能够捕捉到数据中的非线性行为以及数据变化的变化。
- en: '![](../Images/1-12.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/1-12.png)'
- en: Figure 1.12 Scatter plot and validation data predictions from a (non-Bayesian)
    probabilistic regression model. The model is fitted on some simulated data with
    a non-linear dependency between *x* and *y* and with non-constant data variation.
    The solid line indicates the positions of the mean values of all predicted distributions.
    The upper and lower dashed lines indicate an interval in which 95% of all values
    are expected by the model.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.12展示了来自（非贝叶斯）概率回归模型的散点图和验证数据预测。该模型是在一些具有*x*和*y*之间非线性依赖关系以及非恒定数据变化的模拟数据上拟合的。实线表示所有预测分布的平均值位臵。上、下虚线表示模型预计95%的所有值都将出现的区间。
- en: What happens if we use the model to predict the outcome of *x* values outside
    the range of the training data? You can get a first glimpse when looking at figure
    1.12, where we only have data between -5 and 25 but show the predictions in a
    wider range between -10 and 30\. It seems that the model is especially certain
    about its predictions in the ranges where it has never seen the data. That is
    strange and not a desirable property of a model! The reason for the model’s shortcoming
    is that it only captures the data variation--it does not capture the uncertainty
    about the fitted parameters. In statistics, there are different approaches known
    to capture this uncertainty; the Bayesian approach is among these. When working
    with DL models, the Bayesian approach is the most feasible and appropriate. You
    will learn about that in the last two chapters of this book.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用模型来预测训练数据范围之外的 *x* 值的输出结果会怎样呢？您可以在查看图 1.12 时获得初步的了解，其中我们只有 -5 到 25 之间的数据，但展示了
    -10 到 30 更宽范围内的预测结果。看起来模型对其从未见过数据的范围内的预测特别自信。这很奇怪，并不是模型所期望的特性！模型不足的原因是它只捕捉了数据变化——它没有捕捉到拟合参数的不确定性。在统计学中，有几种已知的方法可以捕捉这种不确定性；贝叶斯方法是其中之一。当与深度学习模型一起工作时，贝叶斯方法是可行且合适的。您将在本书的最后两章中了解到这一点。
- en: 1.4.3 Bayesian probabilistic curve fitting
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.3 贝叶斯概率曲线拟合
- en: The main selling point of a Bayesian DL model is its potential to sound the
    alarm in case of novel situations for which the model was not trained. For a regression
    model, this corresponds to extrapolation, meaning you use your model in a data
    range that is outside the range of the training data. In figure 1.13, you can
    see the result of a Bayesian variant of the NN that produces the fit shown in
    figure 1.12\. It is striking that only the Bayesian variant of the NN raises the
    uncertainty when leaving the range of the training data. This is a nice property
    because it can indicate that your model might yield unreliable predictions.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯深度学习模型的主要卖点是其能够在模型未训练的新颖情况下发出警报。对于一个回归模型来说，这对应于外推，意味着您在训练数据范围之外的数据范围内使用模型。在图
    1.13 中，您可以看到一个贝叶斯变种的神经网络，它产生了图 1.12 中所示的拟合结果。值得注意的是，只有神经网络的贝叶斯变体在离开训练数据范围时提高了不确定性。这是一个很好的特性，因为它可以表明您的模型可能产生不可靠的预测。
- en: '![](../Images/1-13.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图 1-13](../Images/1-13.png)'
- en: Figure 1.13 Scatter plot and validation data predictions from a Bayesian probabilistic
    regression model. The model was fitted on some simulated data with non-linear
    dependency between *x* and *y* and non-constant data variation. The solid line
    indicates the positions of the mean values of all predicted distributions. The
    upper and lower dashed lines indicate an interval in which 95% of all values are
    expected by the model.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.13 显示了贝叶斯概率回归模型的散点图和验证数据预测。该模型是在一些具有 *x* 和 *y* 之间非线性依赖性和非恒定数据变化的模拟数据上拟合的。实线表示所有预测分布的均值位置。上、下虚线表示模型预期
    95% 的所有值所在的区间。
- en: 1.5 When to use and when not to use DL?
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 何时使用和何时不使用深度学习？
- en: Recently, DL has had several extraordinary success stories. You therefore might
    ask yourself whether you should forget about traditional ML approaches and use
    DL instead. The answer depends on the situation and the task at hand. In this
    section, we cover when not to use DL as well as what problems DL is useful for.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习已经取得了几个非凡的成功故事。因此，您可能会问自己是否应该忘记传统的机器学习方法，转而使用深度学习。答案取决于具体情况和任务。在本节中，我们将讨论何时不应使用深度学习以及深度学习有哪些用途。
- en: 1.5.1 When not to use DL
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5.1 不宜使用深度学习的情形
- en: 'DL typically has millions of parameters and, therefore, usually needs a lot
    of data to be trained. If you only have access to a limited number of features
    that describe each instance, then DL isn’t the way to go. This includes the following
    applications:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习通常有数百万个参数，因此通常需要大量数据来训练。如果您只能访问有限数量的特征来描述每个实例，那么深度学习并不是一个合适的选择。这包括以下应用：
- en: Predict the scores of a student in their first university year based on only
    their scores in high school
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据学生在高中时的成绩预测他们在大学第一年的成绩
- en: Predict the risk for a heart attack within the next year based on the sex, age,
    BMI (body mass index), blood pressure, and blood cholesterol concentration of
    a person
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据一个人的性别、年龄、BMI（体重指数）、血压和血液胆固醇浓度预测下一年内发生心脏病发作的风险
- en: Classify the sex of a turtle based on its weight, its height, and the length
    of its feet
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据乌龟的体重、身高和脚的长度来分类乌龟的性别
- en: Also, in situations where you have only few training data and you know exactly
    which features determine the outcome of interest (and it’s easy for you to extract
    these features from your raw data), then you should go for these features and
    use those as a basis for a traditional ML model. Imagine, for example, you get
    images from a soccer player collection of different individual French and Dutch
    soccer players. You know that the jerseys of the French team are always blue,
    and those of the Dutch team are always orange. If your task is to develop a classifier
    that discriminates between players of these two teams, it’s probably best to decide
    if the number of blue pixels (the French team) in the image is larger than the
    number of orange pixels (the Dutch team). All other features (such as hair color,
    for example) that seem to discriminate between the two teams would add noise rather
    than help with the classification of new images. It’s therefore probably not a
    good idea to extract and use additional features for your classifier.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在只有少量训练数据且你知道哪些特征决定了感兴趣的结果（并且你很容易从原始数据中提取这些特征）的情况下，你应该选择这些特征，并以此为基础构建一个传统的机器学习模型。例如，假设你从不同法国和荷兰足球运动员的收藏中获取图像。你知道法国队的球衣总是蓝色，而荷兰队的球衣总是橙色。如果你的任务是开发一个区分这两个队伍的球员的分类器，那么最好决定图像中蓝色像素（法国队）的数量是否大于橙色像素（荷兰队）的数量。所有其他似乎可以区分两个队伍的特征（例如，例如，发色）都会增加噪声而不是帮助对新图像的分类。因此，提取和使用额外的特征作为你的分类器可能不是一个好主意。
- en: 1.5.2 When to use DL
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5.2 何时使用深度学习
- en: DL is the method of choice in situations where each instance is described by
    complex raw data (like images, text, or soun*D*) and where it isn’t easy to formulate
    the critical features that characterize the different classes. DL models are then
    able to extract features from the raw data that often outperform models that rely
    on handcrafted features. Figure 1.14 displays various tasks in which DL recently
    changed the game.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是当每个实例都由复杂原始数据（如图像、文本或声音*D*）描述，且难以确定表征不同类别的关键特征时的情况下的首选方法。深度学习模型能够从原始数据中提取特征，这些特征通常优于依赖于手工特征的模型。图
    1.14 展示了深度学习近期改变游戏规则的各种任务。
- en: '![](../Images/1-14.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/1-14.png)'
- en: Figure 1.14 The various tasks recently solved by DL that were out-of-reach for
    traditional ML for a long time
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.14 机器学习长期无法触及，而深度学习近期成功解决的各项任务
- en: 1.5.3 When to use and when not to use probabilistic models?
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5.3 何时使用概率模型，何时不使用？
- en: You will see in this book that for most DL models, it is possible to set up
    a probabilistic version of the model. You get the probabilistic version basically
    for free. In these cases, you can only gain when using the probabilistic variant
    because it provides not only the information that you get from the non-probabilistic
    version of the model, but also additional information that can be essential for
    decision making. If you use a Bayesian variant of a probabilistic model, you have
    the additional advantage of getting a measure that includes the model’s parameter
    uncertainty. Having an uncertainty measure is especially important to identify
    situations in which your model might yield unreliable predictions.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，你会发现对于大多数深度学习模型，可以设置模型的概率版本。你基本上可以免费获得概率版本。在这些情况下，你只能通过使用概率变体来获得收益，因为它不仅提供了从模型的非概率版本中获得的信息，而且还提供了对决策至关重要的附加信息。如果你使用概率模型的贝叶斯变体，你还有额外的优势，即获得一个包括模型参数不确定性的度量。拥有不确定性度量对于识别模型可能产生不可靠预测的情况尤为重要。
- en: 1.6 What you’ll learn in this book
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6 本书你将学到什么
- en: This book gives you a hands-on introduction to probabilistic DL. We’ll provide
    exercises and code demos as Jupyter notebooks, which allow you to get experiences
    and so gain a deeper understanding of the concepts. To benefit from this book,
    you should already know how to run simple Python programs and how to fit a model
    to data (a simple model such as a linear regression is fine). For a deep understanding
    of the more advanced sections (indicated by an asterisk after the end of the heading),
    you should be fluent with intermediate math such as matrix algebra and differential
    calculus, as well as with intermediate statistics such as interpreting probability
    distributions. You will learn how to
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 本书为您提供了概率深度学习的实战入门。我们将提供练习和代码演示作为Jupyter笔记本，让您获得经验，从而更深入地理解概念。为了从本书中受益，您应该已经知道如何运行简单的Python程序，以及如何将模型拟合到数据上（如线性回归这样的简单模型即可）。为了深入理解更高级的部分（标题结尾后有星号标记），您应该熟练掌握中级数学，如矩阵代数和微分微积分，以及中级统计学，如解释概率分布。您将学习如何
- en: Implement DL models with different architectures by using the Keras framework
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Keras框架实现具有不同架构的深度学习模型
- en: Implement a probabilistic DL model, predicting from a given input a whole distribution
    for the outcome
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个概率深度学习模型，从给定的输入预测整个结果的分布
- en: For a given task, choose an appropriate outcome distribution and loss function
    by using the maximum likelihood principle and the TensorFlow Probability framework
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于给定的任务，使用最大似然原理和TensorFlow Probability框架选择合适的输出分布和损失函数
- en: Set up flexible probabilistic DL models such as currently used state-of-the-art
    models for image generation of text to speech translations
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置灵活的概率深度学习模型，例如目前用于图像生成和文本到语音翻译的当前最先进模型
- en: Build Bayesian variants of DL models that can express uncertainties, letting
    you identify non-reliable predictions
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建能够表达不确定性的贝叶斯深度学习模型变体，让您能够识别不可靠的预测
- en: We’ll introduce you to the different DL architectures in the next chapter.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章介绍不同的深度学习架构。
- en: Summary
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Machine learning (ML) methods were invented to allow a computer to learn from
    data.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习（ML）方法是为了使计算机能够从数据中学习而发明的。
- en: Artificial neural networks (NNs) are ML methods that start from raw data and
    include the feature extraction process as part of the model.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经网络（NNs）是机器学习方法，它们从原始数据开始，并将特征提取过程作为模型的一部分。
- en: Deep learning (*DL*) methods are NNs that are called deep because they have
    a large number of layers.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习（*DL*）方法是一种称为深度神经网络（NNs），因为它们具有大量的层。
- en: DL outperforms traditional ML methods in perceptual tasks such as grasping the
    content of an image or translating text to speech.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在感知任务中，如抓取图像内容或文本到语音翻译，深度学习优于传统的机器学习方法。
- en: Curve fitting is a technique that fits a model (the curve or a distribution)
    to data.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线拟合是一种技术，它将模型（曲线或分布）拟合到数据上。
- en: DL and curve fitting are similar and rely on the same principles. These principles
    are at the heart of this book. Understanding these lets you build better-performing
    DL models in terms of accuracy, calibration, and the ability to quantify uncertainty
    measures for the predictions.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习（*DL*）方法和曲线拟合相似，并依赖于相同的原则。这些原则是本书的核心。理解这些原则可以让您在准确性、校准以及量化预测的不确定性度量方面构建性能更好的深度学习模型。
- en: Probabilistic models go beyond single value predictions and capture the variation
    of real data and the uncertainty of the model fit, which allows for better decision
    making.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率模型超越了单值预测，并捕捉真实数据的变异性以及模型拟合的不确定性，这有助于做出更好的决策。
- en: Bayesian variants of probabilistic models can help to identify unreliable predictions.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率模型的贝叶斯变体可以帮助识别不可靠的预测。
