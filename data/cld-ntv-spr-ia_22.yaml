- en: Appendix B Kubernetes in production with DigitalOcean
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录B：使用DigitalOcean在生产环境中部署Kubernetes
- en: This appendix covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录涵盖
- en: Running a Kubernetes cluster on DigitalOcean
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在DigitalOcean上运行Kubernetes集群
- en: Running a PostgreSQL database on DigitalOcean
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在DigitalOcean上运行PostgreSQL数据库
- en: Running Redis on DigitalOcean
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在DigitalOcean上运行Redis
- en: Running RabbitMQ using a Kubernetes Operator
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kubernetes Operator运行RabbitMQ
- en: Running Keycloak using a Helm chart
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Helm图表运行Keycloak
- en: Kubernetes is the de facto standard for deploying and managing containerized
    workloads. We’ve been relying on a local Kubernetes cluster to deploy applications
    and services in the Polar Bookshop system throughout the book. For production,
    we need something else.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes是部署和管理容器化工作负载的事实标准。我们在整本书中一直依赖本地Kubernetes集群来部署Polar Bookshop系统中的应用程序和服务。对于生产环境，我们需要其他东西。
- en: All major cloud providers offer a managed Kubernetes service. In this appendix,
    you’ll see how to use DigitalOcean to spin up a Kubernetes cluster. We’ll also
    rely on other managed services provided by the platform, including PostgreSQL
    and Redis. Finally, this appendix will guide you through the deployment of RabbitMQ
    and Keycloak directly in Kubernetes.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 所有主要云服务提供商都提供托管Kubernetes服务。在本附录中，您将了解如何使用DigitalOcean启动Kubernetes集群。我们还将依赖平台提供的其他托管服务，包括PostgreSQL和Redis。最后，本附录将指导您在Kubernetes中直接部署RabbitMQ和Keycloak。
- en: Before moving on, you need to ensure that you have a DigitalOcean account. When
    you sign up, DigitalOcean offers a 60-day free trial with a $100 credit that is
    more than enough to go through the examples in chapter 15\. Follow the instructions
    on the official website to create an account and start a free trial ([https://try.digitalocean.com/freetrialoffer](https://try.digitalocean.com/freetrialoffer)).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，您需要确保您有一个DigitalOcean账户。当您注册时，DigitalOcean提供60天的免费试用，并附带100美元的信用额度，这足以完成第15章中的示例。按照官方网站上的说明创建账户并开始免费试用（[https://try.digitalocean.com/freetrialoffer](https://try.digitalocean.com/freetrialoffer)）。
- en: Note The source code repository accompanying this book contains additional instructions
    for setting up a Kubernetes cluster on a few different cloud platforms, in case
    you’d like to use something other than DigitalOcean.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本书附带的源代码存储库包含在几个不同的云平台上设置Kubernetes集群的额外说明，以防您想使用除DigitalOcean以外的其他服务。
- en: There are two main options for interacting with the DigitalOcean platform. The
    first one is through the web portal ([https://cloud.digitalocean.com](https://cloud.digitalocean.com)),
    which is very convenient for exploring the available services and their features.
    The second option is via doctl, the DigitalOcean CLI. That’s what we’re going
    to use in the following sections.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 与DigitalOcean平台交互有两种主要选项。第一个是通过Web门户（[https://cloud.digitalocean.com](https://cloud.digitalocean.com)），这对于探索可用服务和其功能非常方便。第二个选项是通过doctl，DigitalOcean的CLI。这就是我们将在以下部分使用的方法。
- en: 'You can find instructions for installing doctl on the official website ([https://docs
    .digitalocean.com/reference/doctl/how-to/install](https://docs.digitalocean.com/reference/doctl/how-to/install)).
    If you’re on macOS or Linux, you can easily install it using Homebrew:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在官方网站上找到安装doctl的说明（[https://docs .digitalocean.com/reference/doctl/how-to/install](https://docs.digitalocean.com/reference/doctl/how-to/install)）。如果您使用的是macOS或Linux，您可以使用Homebrew轻松安装它：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can follow the subsequent instructions on the same doctl page to generate
    an API token and grant doctl access to your DigitalOcean account.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以遵循同一doctl页面上的后续说明来生成API令牌并授予doctl对您的DigitalOcean账户的访问权限。
- en: 'Note In a real production scenario, you would automate the platform management
    tasks using a tool like Terraform or Crossplane. That is usually the responsibility
    of the platform team, not of application developers, so I won’t add extra complexity
    here by introducing yet another tool. Instead we’ll use the DigitalOcean CLI directly.
    If you’re interested in Terraform, Manning has a book in its catalog on the subject:
    *Terraform in Action* by Scott Winkler (Manning, 2021; [https://www.manning.com/books/terraform-in-action](https://www.manning.com/books/terraform-in-action)).
    For Crossplane, I recommend reading chapter 4 of *Continuous Delivery for Kubernetes*
    by Mauricio Salatino ([https://livebook.manning.com/book/continuous-delivery-for-kubernetes/chapter-4](https://livebook.manning.com/book/continuous-delivery-for-kubernetes/chapter-4)).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在实际的生产场景中，您会使用像Terraform或Crossplane这样的工具来自动化平台管理任务。这通常是平台团队的职责，而不是应用开发者的职责，因此我不会通过引入另一个工具来增加额外的复杂性。相反，我们将直接使用DigitalOcean
    CLI。如果您对Terraform感兴趣，Manning在其目录中有一本关于该主题的书：Scott Winkler的《Terraform in Action》（Manning，2021；[https://www.manning.com/books/terraform-in-action](https://www.manning.com/books/terraform-in-action)）。对于Crossplane，我建议阅读Mauricio
    Salatino的《Continuous Delivery for Kubernetes》的第4章（[https://livebook.manning.com/book/continuous-delivery-for-kubernetes/chapter-4](https://livebook.manning.com/book/continuous-delivery-for-kubernetes/chapter-4)）。
- en: B.1 Running a Kubernetes cluster on DigitalOcean
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.1 在DigitalOcean上运行Kubernetes集群
- en: The first resource we need to create on DigitalOcean is a Kubernetes cluster.
    You could rely on the IaaS capabilities offered by the platform and install a
    Kubernetes cluster manually on top of virtual machines. Instead, we’ll move up
    the abstraction staircase and go for a solution managed by the platform. When
    we use DigitalOcean Kubernetes ([https://docs.digitalocean.com/products/kubernetes](https://docs.digitalocean.com/products/kubernetes)),
    the platform will take care of many infrastructural concerns, so that we developers
    can focus more on application development.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在DigitalOcean上需要创建的第一个资源是一个Kubernetes集群。您可以选择依赖平台提供的IaaS能力，在虚拟机之上手动安装一个Kubernetes集群。相反，我们将提升抽象层次，选择由平台管理的解决方案。当我们使用DigitalOcean
    Kubernetes ([https://docs.digitalocean.com/products/kubernetes](https://docs.digitalocean.com/products/kubernetes))时，平台将负责许多基础设施问题，这样我们开发者就可以更多地专注于应用开发。
- en: You can straightforwardly create a new Kubernetes cluster using doctl. I promised
    that we would deploy Polar Bookshop in a real production environment, and that’s
    what we’ll do, although I won’t ask you to size and configure the cluster as I
    would in a real scenario.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用doctl直接创建一个新的Kubernetes集群。我承诺我们将在一个真实的生产环境中部署Polar Bookshop，这就是我们将要做的，尽管我不会要求您像在真实场景中那样对集群进行规模和配置。
- en: For starters, setting up a Kubernetes cluster is not a developer’s responsibility—it’s
    a job for the platform team. Second, it would require more in-depth coverage of
    Kubernetes than is provided by this book to fully understand the configuration.
    Third, I don’t want you to incur extra costs on DigitalOcean for using a lot of
    computational resources and services. Cost optimization is a cloud property that
    applies to real applications. However, it might become expensive if you’re trying
    things out or running demo applications. Please keep an eye on your DigitalOcean
    account to monitor when your free trial and $100 credit expire.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，设置Kubernetes集群不是开发者的责任——这是平台团队的工作。其次，要完全理解配置，需要比本书提供的更深入地了解Kubernetes。第三，我不想您在DigitalOcean上使用大量计算资源和服务的额外成本。成本优化是适用于真实应用的云属性。然而，如果您在尝试新事物或运行演示应用时，这可能会变得很昂贵。请密切关注您的DigitalOcean账户，以监控您的免费试用和$100信用额度何时到期。
- en: 'Each cloud resource can be created in a data center hosted in a specific geographical
    region. For better performance, I recommend you choose one near to you. I’ll use
    “Amsterdam 3” (ams3), but you can get the complete list of regions with the following
    command:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每个云资源都可以在特定地理区域内的数据中心中创建。为了获得更好的性能，我建议您选择离您较近的一个。我将使用“Amsterdam 3”（ams3），但您可以使用以下命令获取完整的区域列表：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s go ahead and initialize a Kubernetes cluster using DigitalOcean Kubernetes
    (DOKS). It will be composed of three worker nodes, for which you can decide the
    technical specifications. You can choose between different options in terms of
    CPU, memory, and architecture. I’ll use nodes with 2 vCPU and 4 GB of memory:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续使用DigitalOcean Kubernetes（DOKS）初始化一个Kubernetes集群。它将由三个工作节点组成，您可以为它们决定技术规格。您可以在CPU、内存和架构方面选择不同的选项。我将使用具有2个vCPU和4GB内存的节点：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Defines the name of the cluster to create
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义要创建的集群名称
- en: ❷ Provides the requested specifications for the worker nodes
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为工作节点提供所需的规格
- en: ❸ The data center region of your choice, such as “ams3”
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 您选择的数据中心区域，例如“ams3”
- en: Note If you’d like to know more about the different compute options and their
    prices, you can use the doctl compute size list command.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果您想了解更多关于不同计算选项及其价格的信息，您可以使用 doctl compute size list 命令。
- en: 'The cluster provisioning will take a few minutes. In the end, it will print
    out the unique ID assigned to the cluster. Take note, since you’ll need it later.
    You can fetch the cluster ID at any time by running the following command (I have
    filtered the results for the sake of clarity):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 集群配置需要几分钟。最后，它将打印出分配给集群的唯一 ID。请注意，因为您稍后需要它。您可以通过运行以下命令在任何时候获取集群 ID（我已经为了清晰起见过滤了结果）：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'At the end of the cluster provisioning, doctl will also configure the context
    for your Kubernetes CLI so that you can interact with the cluster running on DigitalOcean
    from your computer, similar to what you’ve done so far with your local cluster.
    You can verify the current context for kubectl by running the following command:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群配置完成之后，doctl 还会为您配置 Kubernetes CLI 的上下文，以便您可以从您的计算机上与 DigitalOcean 上运行的集群进行交互，类似于您迄今为止与本地集群所做的那样。您可以通过运行以下命令来验证当前的
    kubectl 上下文：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note If you want to change the context, you can run kubectl config use-context
    <context-name>.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果您想更改上下文，您可以通过运行 kubectl config use-context <context-name> 命令来实现。
- en: 'Once the cluster is provisioned, you can get information about the worker nodes
    as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦集群配置完成，您可以通过以下方式获取关于工作节点的信息：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Do you remember the Octant dashboard you used to visualize the workloads on
    your local Kubernetes cluster? You can now use it to get information about the
    cluster on DigitalOcean as well. Open a Terminal window and start Octant with
    the following command:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 您还记得您用来可视化本地 Kubernetes 集群工作负载的 Octant 仪表板吗？现在您可以使用它来获取关于 DigitalOcean 上集群的信息。打开一个终端窗口，并使用以下命令启动
    Octant：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Octant will open in your browser and show data from your current Kubernetes
    context, which should be the cluster on DigitalOcean. From the upper-right menu,
    you can switch between contexts from the drop-down box, as shown in figure B.1.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Octant 将在您的浏览器中打开并显示您当前 Kubernetes 上下文的数据，这应该是 DigitalOcean 上的集群。从右上角的菜单中，您可以通过下拉框在上下文之间切换，如图
    B.1 所示。
- en: '![B-1](../Images/B-1.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![B-1](../Images/B-1.png)'
- en: Figure B.1 Octant lets you visualize workloads from different Kubernetes clusters
    by switching contexts.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.1 Octant 允许您通过切换上下文来可视化来自不同 Kubernetes 集群的工作负载。
- en: 'As I mentioned in chapter 9, Kubernetes doesn’t come packaged with an Ingress
    Controller; it’s up to you to install one. Since we’ll rely on an Ingress resource
    to allow traffic from the public internet to the cluster, we need to install an
    Ingress Controller. Let’s install the same one we used locally: ingress-nginx.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在第 9 章中提到的，Kubernetes 并没有打包 Ingress 控制器；安装一个 Ingress 控制器取决于您。由于我们将依赖 Ingress
    资源来允许来自公共互联网到集群的流量，我们需要安装一个 Ingress 控制器。让我们安装我们在本地使用的同一个：ingress-nginx。
- en: In your polar-deployment repository, create a new kubernetes/platform/production
    folder, and copy over the content from the Chapter15/15-end/polar-deployment/kubernetes/platform/production
    folder in the source code repository accompanying the book.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的 polar-deployment 仓库中，创建一个新的 kubernetes/platform/production 文件夹，并将源代码仓库中附带的书中的
    Chapter15/15-end/polar-deployment/kubernetes/platform/production 文件夹的内容复制过来。
- en: 'Then open a Terminal window, navigate to the kubernetes/platform/production/ingress-nginx
    folder in your polar-deployment project, and run the following command to deploy
    ingress-nginx to your production Kubernetes cluster:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后打开一个终端窗口，导航到您的 polar-deployment 项目中的 kubernetes/platform/production/ingress-nginx
    文件夹，并运行以下命令将 ingress-nginx 部署到您的生产 Kubernetes 集群：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Feel free to open the file and look at the instructions before running it.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行它之前，请随意打开文件并查看说明。
- en: Note You might need to make the script executable first with the command chmod
    +x deploy.sh.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您可能需要先使用命令 chmod +x deploy.sh 使脚本可执行。
- en: In the next section, you’ll see how to initialize a PostgreSQL database on DigitalOcean.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将了解如何在 DigitalOcean 上初始化一个 PostgreSQL 数据库。
- en: B.2 Running a PostgreSQL database on DigitalOcean
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.2 在 DigitalOcean 上运行 PostgreSQL 数据库
- en: In most of the book, you’ve been running PostgreSQL database instances as containers,
    both in Docker and in your local Kubernetes cluster. In production, we’d like
    to take advantage of the platform and use a managed PostgreSQL service provided
    by DigitalOcean ([https://docs.digitalocean.com/products/databases/postgresql](https://docs.digitalocean.com/products/databases/postgresql)).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的大部分内容中，您都已经在 Docker 和本地 Kubernetes 集群中以容器形式运行 PostgreSQL 数据库实例。在生产环境中，我们希望利用平台优势，并使用
    DigitalOcean 提供的托管 PostgreSQL 服务（[https://docs.digitalocean.com/products/databases/postgresql](https://docs.digitalocean.com/products/databases/postgresql)）。
- en: The applications we developed throughout the book are cloud native and follow
    the 15-Factor methodology. As such, they treat backing services as attached resources
    that can be swapped without changing anything in the application code. Furthermore,
    we followed the environment parity principle and used a real PostgreSQL database
    both for development and testing, and it’s the same database we want to use in
    production.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本书开发的应用程序是云原生，遵循 15-Factor 方法。因此，它们将后端服务视为可以替换而不更改应用程序代码的附加资源。此外，我们遵循环境等价原则，在开发和测试中使用了真实的
    PostgreSQL 数据库，并且这是我们希望在生产中使用的相同数据库。
- en: Moving from a PostgreSQL container running in your local environment to a managed
    service with high availability, scalability, and resilience is a matter of changing
    the values of a few configuration properties for Spring Boot. How great is that?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从在本地环境中运行的 PostgreSQL 容器迁移到具有高可用性、可扩展性和弹性的托管服务，只需更改 Spring Boot 的几个配置属性值即可。这有多么方便？
- en: 'First, create a new PostgreSQL server named polar-postgres, as shown in the
    following code snippet. We’ll use PostgreSQL 14, which is the same version we
    used for development and testing. Remember to replace <your_region> with the geographical
    region you’d like to use. It should be the same as the region you used for the
    Kubernetes cluster. In my case, it’s ams3:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建一个名为 polar-postgres 的新 PostgreSQL 服务器，如下面的代码片段所示。我们将使用 PostgreSQL 14，这与我们用于开发和测试的版本相同。请记住用
    <your_region> 替换您希望使用的地理位置区域。它应该与您用于 Kubernetes 集群的区域相同。在我的情况下，它是 ams3：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The database server provisioning will take several minutes. You can verify
    the installation status with the following command (I have filtered the result
    for the sake of clarity):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库服务器配置将需要几分钟。您可以使用以下命令验证安装状态（我已经为了清晰起见过滤了结果）：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: When the database is online, your database server is ready. Take note of the
    database server ID. You’ll need it later.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据库上线时，您的数据库服务器就准备好了。请注意数据库服务器 ID。您稍后需要用到它。
- en: 'To mitigate unnecessary attack vectors, you can configure a firewall so that
    the PostgreSQL server is only accessible from the Kubernetes cluster created previously.
    Remember that I asked you to take notes of the resource IDs for PostgreSQL and
    Kubernetes? Use them in the following command to configure the firewall and secure
    access to the database server:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻不必要的攻击向量，您可以配置防火墙，以便 PostgreSQL 服务器只能从之前创建的 Kubernetes 集群访问。请记住，我要求您记录 PostgreSQL
    和 Kubernetes 的资源 ID？在以下命令中使用它们来配置防火墙并确保数据库服务器的安全访问：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, let’s create two databases to be used by Catalog Service (polardb_catalog)
    and Order Service (polardb_order). Remember to replace <postgres_id> with your
    PostgreSQL resource ID:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们为目录服务（polardb_catalog）和订单服务（polardb_order）创建两个数据库。请记住用 <postgres_id>
    替换您的 PostgreSQL 资源 ID：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, let’s retrieve the details for connecting to PostgreSQL. Remember
    to replace <postgres_id> with your PostgreSQL resource ID:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们检索连接到 PostgreSQL 的详细信息。请记住用 <postgres_id> 替换您的 PostgreSQL 资源 ID：
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Before concluding this section, let’s create some Secrets in the Kubernetes
    cluster with the PostgreSQL credentials required by the two applications. In a
    real-world scenario, we should create dedicated users for the two applications
    and grant limited privileges. For simplicity, we’ll use the admin account for
    both.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本节之前，让我们在 Kubernetes 集群中为两个应用程序所需的 PostgreSQL 凭据创建一些秘密。在实际场景中，我们应该为两个应用程序创建专用用户并授予有限的权限。为了简化，我们将使用管理员账户为两者服务。
- en: 'First, create a Secret for Catalog Service using the information returned by
    the previous doctl command:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用前一个 doctl 命令返回的信息为目录服务创建一个秘密：
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Similarly, create a Secret for Order Service. Pay attention to the slightly
    different syntax required by Spring Data R2DBC for the URL:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，为订单服务创建一个秘密。请注意，Spring Data R2DBC 对 URL 的语法要求略有不同：
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: That’s it for PostgreSQL. In the next section, you’ll see how to initialize
    Redis using DigitalOcean.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: PostgreSQL的部分就到这里。在下一节中，您将看到如何使用DigitalOcean初始化Redis。
- en: B.3 Running Redis on DigitalOcean
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.3 在DigitalOcean上运行Redis
- en: In most of the book, you’ve been running Redis instances as containers, both
    in Docker and in your local Kubernetes cluster. In production we’d like to take
    advantage of the platform and use a managed Redis service provided by DigitalOcean
    ([https://docs.digitalocean.com/products/databases/redis/](https://docs.digitalocean.com/products/databases/redis/)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的大部分内容中，您都已经在Docker和您本地的Kubernetes集群中以容器形式运行Redis实例。在生产环境中，我们希望利用平台优势，使用由DigitalOcean提供的托管Redis服务([https://docs.digitalocean.com/products/databases/redis/](https://docs.digitalocean.com/products/databases/redis/))。
- en: Once again, since we followed the 15-Factor methodology, we can swap the Redis
    backing service used by Edge Service without changing anything in the application
    code. We’ll only need to change a few configuration properties for Spring Boot.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，由于我们遵循了15-Factor方法论，我们可以在不更改应用程序代码的情况下替换Edge Service使用的Redis后端服务。我们只需要更改Spring
    Boot的一些配置属性。
- en: 'First, create a new Redis server named polar-redis as shown in the following
    code snippet. We’ll use Redis 7, which is the same version we used for development
    and testing. Remember to replace <your_region> with the geographical region you’d
    like to use. It should be the same region you used for the Kubernetes cluster.
    In my case, it’s ams3:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建一个名为polar-redis的新Redis服务器，如下面的代码片段所示。我们将使用Redis 7，这与我们用于开发和测试的版本相同。请记住将<your_region>替换为您希望使用的地理位置区域。它应该与您用于Kubernetes集群的区域相同。在我的情况下，它是ams3：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The Redis server provisioning will take several minutes. You can verify the
    installation status with the following command (I have filtered the result for
    the sake of clarity):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Redis服务器的配置将需要几分钟。您可以使用以下命令验证安装状态（为了清晰起见，我已经过滤了结果）：
- en: '[PRE16]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: When the server is online, your Redis server is ready. Take note of the Redis
    resource ID. You’ll need it later.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当服务器在线时，您的Redis服务器就绪。请注意Redis资源ID。您稍后需要用到它。
- en: 'To mitigate unnecessary attack vectors, we can configure a firewall so that
    the Redis server is only accessible from the Kubernetes cluster created previously.
    Remember that I asked you to take notes of the resource IDs for Redis and Kubernetes?
    Use them in the following command to configure the firewall and secure access
    to the Redis server:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻不必要的攻击向量，我们可以配置防火墙，使得Redis服务器只能从之前创建的Kubernetes集群访问。记得我让您记录Redis和Kubernetes的资源ID了吗？在以下命令中使用它们来配置防火墙并确保Redis服务器的安全访问：
- en: '[PRE17]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, let’s retrieve the details for connecting to Redis. Remember to replace
    <redis_id> with your Redis resource ID:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们获取连接到Redis的详细信息。请记住将<redis_id>替换为您的Redis资源ID：
- en: '[PRE18]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Before concluding this section, let’s create a Secret in the Kubernetes cluster
    with the Redis credentials required by Edge Service. In a real-world scenario,
    we should create a dedicated user for the application and grant limited privileges.
    For simplicity, we’ll use the default account. Populate the Secret with the information
    returned by the previous doctl command:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本节之前，让我们在Kubernetes集群中创建一个Secret，用于存储Edge Service所需的Redis凭证。在现实场景中，我们应该为应用程序创建一个专用用户并授予有限的权限。为了简化，我们将使用默认账户。使用上一个doctl命令返回的信息填充Secret：
- en: '[PRE19]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: That’s it for Redis. The following section will cover how to deploy RabbitMQ
    using a Kubernetes Operator.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Redis的部分就到这里。下一节将介绍如何使用Kubernetes Operator部署RabbitMQ。
- en: B.4 Running RabbitMQ using a Kubernetes Operator
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.4 使用Kubernetes Operator运行RabbitMQ
- en: In the previous sections, we initialized and configured PostgreSQL and Redis
    servers that are offered and managed by the platform. We can’t do the same for
    RabbitMQ because DigitalOcean doesn’t have a RabbitMQ offering, similar to other
    cloud providers like Azure or GCP.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们初始化并配置了由平台提供和管理的PostgreSQL和Redis服务器。我们无法对RabbitMQ做同样的事情，因为DigitalOcean没有提供类似Azure或GCP等其他云服务提供商的RabbitMQ服务。
- en: A popular and convenient way of deploying and managing services like RabbitMQ
    in a Kubernetes cluster is to use the *operator* pattern. Operators are “software
    extensions to Kubernetes that make use of custom resources to manage applications
    and their components” ([https://kubernetes.io/docs/concepts/extend-kubernetes/operator](https://kubernetes.io/docs/concepts/extend-kubernetes/operator)).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 集群中部署和管理像 RabbitMQ 这样的服务的一种流行且方便的方法是使用 *operator* 模式。Operators
    是“Kubernetes 的软件扩展，它使用自定义资源来管理应用程序及其组件”([https://kubernetes.io/docs/concepts/extend-kubernetes/operator](https://kubernetes.io/docs/concepts/extend-kubernetes/operator))。
- en: Think about RabbitMQ. To use it in production, you’ll need to configure it for
    high availability and resilience. Depending on the workload, you might want to
    scale it dynamically. When a new version of the software is available, you’ll
    need a reliable way of upgrading the service and migrating existing constructs
    and data. You could perform all those tasks manually. Or you could use an Operator
    to capture all those operational requirements and instruct Kubernetes to take
    care of them automatically. In practice, an Operator is an application that runs
    on Kubernetes and interacts with its API to accomplish its functionality.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下 RabbitMQ。为了在生产环境中使用它，你需要对其进行高可用性和弹性的配置。根据工作负载，你可能希望动态地对其进行扩展。当软件的新版本可用时，你需要一种可靠的方式来升级服务并迁移现有的结构和数据。你可以手动执行所有这些任务。或者，你可以使用
    Operator 来捕获所有这些操作需求，并指导 Kubernetes 自动处理它们。实际上，Operator 是一个在 Kubernetes 上运行的应用程序，它与
    Kubernetes API 交互以实现其功能。
- en: The RabbitMQ project provides an official Operator to run the event broker on
    a Kubernetes cluster ([www.rabbitmq.com](http://www.rabbitmq.com)). I have already
    configured all the necessary resources to use the RabbitMQ Kubernetes Operator
    and prepared a script to deploy it.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: RabbitMQ 项目提供了一个官方的 Operator，用于在 Kubernetes 集群上运行事件代理([www.rabbitmq.com](http://www.rabbitmq.com))。我已经配置了所有必要的资源来使用
    RabbitMQ Kubernetes Operator，并准备了一个脚本来部署它。
- en: Open a Terminal window, go to your Polar Deployment project (polar-deployment),
    and navigate to the kubernetes/platform/production/rabbitmq folder. You should
    have copied that folder over to your repository when configuring the Kubernetes
    cluster. If that’s not the case, please do so now from the source code repository
    accompanying this book (Chapter15/15-end/polar-deployment/platform/production/rabbitmq).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 打开一个终端窗口，转到你的 Polar Deployment 项目（polar-deployment），然后导航到 kubernetes/platform/production/rabbitmq
    文件夹。当你配置 Kubernetes 集群时，你应该已经将此文件夹复制到你的仓库中。如果不是这种情况，请现在从本书附带源代码仓库（第15章/15-end/polar-deployment/platform/production/rabbitmq）中执行此操作。
- en: 'Then run the following command to deploy RabbitMQ to your production Kubernetes
    cluster:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然后运行以下命令将 RabbitMQ 部署到你的生产 Kubernetes 集群：
- en: '[PRE20]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Feel free to open the file and look at the instructions before running it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行之前，你可以自由地打开文件并查看说明。
- en: Note You might need to make the script executable first with the command chmod
    +x deploy.sh.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你可能需要先使用命令 chmod +x deploy.sh 使脚本可执行。
- en: 'The script will output details about all the operations performed to deploy
    RabbitMQ. Finally, it will create a polar-rabbitmq-credentials Secret with the
    credentials that Order Service and Dispatcher Service will need to access RabbitMQ.
    You can verify that the Secret has been successfully created as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本将输出有关部署 RabbitMQ 所执行的所有操作的详细信息。最后，它将创建一个 polar-rabbitmq-credentials Secret，其中包含订单服务和调度服务访问
    RabbitMQ 所需的凭据。你可以按照以下方式验证 Secret 是否已成功创建：
- en: '[PRE21]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The RabbitMQ broker is deployed in a dedicated rabbitmq-system namespace. Applications
    can interact with it at polar-rabbitmq.rabbitmq-system.svc.cluster.local on port
    5672.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: RabbitMQ 代理部署在专门的 rabbitmq-system 命名空间中。应用程序可以在 polar-rabbitmq.rabbitmq-system.svc.cluster.local
    的 5672 端口上与之交互。
- en: That’s it for RabbitMQ. In the next section, you’ll see how to deploy a Keycloak
    server to a production Kubernetes cluster.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: RabbitMQ 的部署就到这里了。在下一节中，你将了解如何将 Keycloak 服务器部署到生产 Kubernetes 集群。
- en: B.5 Running Keycloak using a Helm chart
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.5 使用 Helm 图表运行 Keycloak
- en: 'As with RabbitMQ, DigitalOcean doesn’t provide a managed Keycloak service.
    The Keycloak project is working on an Operator, but it’s still in beta at the
    time of writing, so we’ll deploy it using a different approach: Helm charts.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 与 RabbitMQ 一样，DigitalOcean 不提供托管 Keycloak 服务。Keycloak 项目正在开发一个 Operator，但在撰写本文时，它仍处于测试阶段，因此我们将使用不同的方法：Helm
    图表。
- en: Think of Helm as a package manager. To install software on your computer, you
    would use one of the operating system package managers, like apt (Ubuntu), Homebrew
    (macOS), or Chocolatey (Windows). In Kubernetes you can similarly use Helm, but
    they’re called *charts* instead of *packages*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 将Helm视为一个包管理器。要在您的计算机上安装软件，您会使用操作系统包管理器之一，例如apt（Ubuntu）、Homebrew（macOS）或Chocolatey（Windows）。在Kubernetes中，您可以使用Helm，但它们被称为*charts*而不是*packages*。
- en: 'Go ahead and install Helm on your computer. You can find the instructions on
    the official website ([https://helm.sh](https://helm.sh)). If you are on macOS
    or Linux, you can install Helm with Homebrew:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，请在您的计算机上安装Helm。您可以在官方网站上找到说明（[https://helm.sh](https://helm.sh)）。如果您使用的是macOS或Linux，可以使用Homebrew安装Helm：
- en: '[PRE22]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: I have already configured all the necessary resources to use the Keycloak Helm
    chart provided by Bitnami ([https://bitnami.com](https://bitnami.com)), and I’ve
    prepared a script to deploy it. Open a Terminal window, go to your Polar Deployment
    project (polar-deployment), and navigate to the kubernetes/platform/production/keycloak
    folder. You should have copied that folder over to your repository when configuring
    the Kubernetes cluster. If that’s not the case, please do so now from the source
    code repository accompanying this book (Chapter15/15-end/polar-deployment/platform/production/keycloak).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经配置了所有必要的资源来使用Bitnami提供的Keycloak Helm图表（[https://bitnami.com](https://bitnami.com)），并准备了一个脚本用于部署它。打开一个终端窗口，转到您的Polar
    Deployment项目（polar-deployment），然后导航到kubernetes/platform/production/keycloak文件夹。您应该在配置Kubernetes集群时将此文件夹复制到您的仓库中。如果不是这种情况，请现在从本书附带源代码仓库（Chapter15/15-end/polar-deployment/platform/production/keycloak）中执行此操作。
- en: 'Then run the following command to deploy Keycloak to your production Kubernetes
    cluster:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然后运行以下命令将Keycloak部署到您的生产Kubernetes集群：
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Feel free to open the file and look at the instructions before running it.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行之前，您可以自由地打开文件并查看说明。
- en: Note You might need to make the script executable first with the command chmod
    +x deploy.sh.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您可能需要首先使用命令chmod +x deploy.sh使脚本可执行。
- en: The script will output details about all the operations performed to deploy
    Keycloak and print the admin username and password you can use to access the Keycloak
    Admin Console. Feel free to change the password after your first login. Note the
    credentials down, since you might need them later. The deployment can take several
    minutes to complete, so it’s a good time to take a break and drink a beverage
    of your choice as a reward for everything you have accomplished so far. Good job!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本将输出所有部署Keycloak的操作的详细信息，并打印出您可以使用它访问Keycloak管理控制台的管理员用户名和密码。首次登录后，您可以自由更改密码。请注意记录凭证，因为您可能以后还需要它们。部署可能需要几分钟才能完成，所以这是一个休息和享用您选择的饮料作为对迄今为止所做一切奖励的好时机。干得好！
- en: 'Finally, the script will create a polar-keycloak-client-credentials Secret
    with the Client secret that Edge Service will need to authenticate with Keycloak.
    You can verify that the Secret has been successfully created as follows. The value
    is generated randomly by the script:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，脚本将创建一个包含客户端密钥的polar-keycloak-client-credentials Secret，这是Edge服务用于与Keycloak进行身份验证所需的。您可以按照以下方式验证Secret是否已成功创建。该值由脚本随机生成：
- en: '[PRE24]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note The Keycloak Helm chart spins up a PostgreSQL instance inside the cluster
    and uses it to persist the data used by Keycloak. We could have integrated it
    with the PostgreSQL service managed by DigitalOcean, but the configuration on
    the Keycloak side would have been quite complicated. If you’d like to use an external
    PostgreSQL database, you can refer to the Keycloak Helm chart documentation ([https://bitnami.com/stack/keycloak/helm](https://bitnami.com/stack/keycloak/helm)).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Keycloak Helm图表在集群内部启动一个PostgreSQL实例，并使用它来持久化Keycloak使用的数据。我们本来可以将其与DigitalOcean管理的PostgreSQL服务集成，但Keycloak这一侧的配置将会相当复杂。如果您想使用外部PostgreSQL数据库，可以参考Keycloak
    Helm图表文档（[https://bitnami.com/stack/keycloak/helm](https://bitnami.com/stack/keycloak/helm)）。
- en: 'The Keycloak server is deployed in a dedicated keycloak-system namespace. Applications
    can interact with it at polar-keycloak.keycloak-system.svc.cluster.local on port
    8080 from within the cluster. It’s also exposed outside the cluster via a public
    IP address. You can find the external IP address with the following command:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Keycloak服务器部署在专用的keycloak-system命名空间中。应用程序可以在集群内部通过polar-keycloak.keycloak-system.svc.cluster.local地址的8080端口与之交互。它也通过公网IP地址暴露在集群外部。您可以使用以下命令找到外部IP地址：
- en: '[PRE25]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The platform might take a few minutes to provision a load balancer. During the
    provisioning, the EXTERNAL-IP column will show a <pending> status. Wait and try
    again until an IP address is shown. Note it down, since we’re going to use it
    in multiple scenarios.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 平台可能需要几分钟来配置负载均衡器。在配置过程中，EXTERNAL-IP 列将显示 <pending> 状态。等待并重试，直到显示 IP 地址。请注意记录，因为我们将在多个场景中使用它。
- en: Since Keycloak is exposed via a public load balancer, you can use the external
    IP address to access the Admin Console. Open a browser window, navigate to http://<external-ip>/admin,
    and log in with the credentials returned by the previous deployment script.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Keycloak 通过公共负载均衡器暴露，你可以使用外部 IP 地址来访问管理控制台。打开浏览器窗口，导航到 http://<external-ip>/admin，并使用之前部署脚本返回的凭据登录。
- en: 'Now that you have a public DNS name for Keycloak, you can define a couple of
    Secrets to configure the Keycloak integration in Edge Service (OAuth2 Client),
    Catalog Service, and Order Service (OAuth2 Resource Servers). Open a Terminal
    window, navigate to the kubernetes/platform/production/keycloak folder in your
    polar-deployment project, and run the following command to create the Secrets
    that the applications will use to integrate with Keycloak. Feel free to open the
    file and look at the instructions before running it. Remember to replace <external-ip>
    with the external IP address assigned to your Keycloak server:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你有了 Keycloak 的公共 DNS 名称，可以定义几个 Secret 来配置边缘服务（OAuth2 客户端）、目录服务、订单服务（OAuth2
    资源服务器）中的 Keycloak 集成。打开终端窗口，导航到你的 polar-deployment 项目中的 kubernetes/platform/production/keycloak
    文件夹，并运行以下命令来创建应用程序将用于与 Keycloak 集成的 Secrets。在运行之前，你可以自由打开文件查看说明。请记住将 <external-ip>
    替换为你 Keycloak 服务器分配的外部 IP 地址：
- en: '[PRE26]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: That’s it for Keycloak. The following section will show you how to deploy Polar
    UI to the production cluster.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Keycloak 的内容到此结束。下一节将展示如何将 Polar UI 部署到生产集群。
- en: B.6 Running Polar UI
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.6 运行 Polar UI
- en: Polar UI is a single-page application built with Angular and served by NGINX.
    As you saw in chapter 11, I have already prepared a container image you can use
    to deploy this application, since frontend development is out of scope for this
    book.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Polar UI 是一个使用 Angular 构建、由 NGINX 提供服务的单页应用程序。正如你在第 11 章中看到的，我已经准备了一个容器镜像，你可以使用它来部署此应用程序，因为前端开发不在此书的范围之内。
- en: Open a Terminal window, go to your Polar Deployment project (polar-deployment),
    and navigate to the kubernetes/platform/production/polar-ui folder. You should
    have copied that folder over to your repository when configuring the Kubernetes
    cluster. If that’s not the case, please do so now from the source code repository
    accompanying this book (Chapter15/15-end/polar-deployment/platform/production/polar-ui).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 打开终端窗口，进入你的 Polar 部署项目（polar-deployment），然后导航到 kubernetes/platform/production/polar-ui
    文件夹。在配置 Kubernetes 集群时，你应该已经将此文件夹复制到你的仓库中。如果不是这种情况，请现在从本书附带源代码仓库（Chapter15/15-end/polar-deployment/platform/production/polar-ui）中执行此操作。
- en: 'Then run the following command to deploy Polar UI to your production Kubernetes
    cluster. Feel free to open the file and look at the instructions before running
    it:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然后运行以下命令将 Polar UI 部署到你的生产 Kubernetes 集群。在运行之前，你可以自由打开文件查看说明：
- en: '[PRE27]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note You might need to make the script executable first with the command chmod
    +x deploy.sh.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你可能需要先使用命令 chmod +x deploy.sh 使脚本可执行。
- en: Now that you have Polar UI and all the main platform services up and running,
    you can proceed with reading chapter 15 and complete the configuration of all
    the Spring Boot applications in Polar Bookshop for production deployment.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Polar UI 和所有主要平台服务都已启动并运行，你可以继续阅读第 15 章，并完成 Polar Bookshop 中所有 Spring Boot
    应用程序的生产部署配置。
- en: B.7 Deleting all cloud resources
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.7 删除所有云资源
- en: When you’re done experimenting with the Polar Bookshop project, follow the instructions
    in this section to delete all the cloud resources created on DigitalOcean. That’s
    fundamental to avoid incurring unexpected costs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当你完成对 Polar Bookshop 项目的实验后，请按照本节中的说明删除在 DigitalOcean 上创建的所有云资源。这是避免产生意外费用的基本要求。
- en: 'First, delete the Kubernetes cluster:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，删除 Kubernetes 集群：
- en: '[PRE28]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, delete the PostgreSQL and Redis databases. You’ll need to know their
    IDs first, so run this command to extract that information:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，删除 PostgreSQL 和 Redis 数据库。首先你需要知道它们的 ID，所以运行以下命令来提取该信息：
- en: '[PRE29]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then go ahead and delete both of them using the resource identifiers returned
    by the previous command:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然后继续使用之前命令返回的资源标识符删除这两个资源：
- en: '[PRE30]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Finally, open a browser window, navigate to the DigitalOcean web interface ([https://cloud.digitalocean.com](https://cloud.digitalocean.com)),
    and go through the different categories of cloud resources in your account to
    verify that there’s no outstanding services. If there are, delete them. There
    could be load balancers or persistent volumes created as a side effect of creating
    a cluster or a database, and that may not have been deleted by the previous commands.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，打开浏览器窗口，导航到 DigitalOcean 网络界面 ([https://cloud.digitalocean.com](https://cloud.digitalocean.com))，并检查您账户中的不同云资源类别，以确认没有未结清的服务。如果有，请删除它们。创建集群或数据库时可能会作为副作用创建负载均衡器或持久卷，而这些可能没有被之前的命令删除。
