- en: Appendix. Installing Hadoop and friends
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录. 安装 Hadoop 及相关工具
- en: This appendix contains instructions on how to install Hadoop and other tools
    that are used in the book.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录包含有关如何安装 Hadoop 以及书中使用的其他工具的说明。
- en: '|  |'
  id: totrans-2
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Getting started quickly with Hadoop
  id: totrans-3
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 快速开始使用 Hadoop
- en: 'The quickest way to get up and running with Hadoop is to download a preinstalled
    virtual machine from one of the Hadoop vendors. Following is a list of the popular
    VMs:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预安装的虚拟机是快速开始使用 Hadoop 的最快方式之一。以下是一些流行的虚拟机列表：
- en: Cloudera Quickstart VM—[http://www.cloudera.com/content/cloudera-content/cloudera-docs/DemoVMs/Cloudera-QuickStart-VM/cloudera_quickstart_vm.html](http://www.cloudera.com/content/cloudera-content/cloudera-docs/DemoVMs/Cloudera-QuickStart-VM/cloudera_quickstart_vm.html)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloudera Quickstart VM—[http://www.cloudera.com/content/cloudera-content/cloudera-docs/DemoVMs/Cloudera-QuickStart-VM/cloudera_quickstart_vm.html](http://www.cloudera.com/content/cloudera-content/cloudera-docs/DemoVMs/Cloudera-QuickStart-VM/cloudera_quickstart_vm.html)
- en: Hortonworks Sandbox—[http://hortonworks.com/products/hortonworkssandbox/](http://hortonworks.com/products/hortonworkssandbox/)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hortonworks Sandbox—[http://hortonworks.com/products/hortonworkssandbox/](http://hortonworks.com/products/hortonworkssandbox/)
- en: MapR Sandbox for Hadoop—[http://doc.mapr.com/display/MapR/MapR+Sandbox+for+Hadoop](http://doc.mapr.com/display/MapR/MapR+Sandbox+for+Hadoop)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MapR Sandbox for Hadoop—[http://doc.mapr.com/display/MapR/MapR+Sandbox+for+Hadoop](http://doc.mapr.com/display/MapR/MapR+Sandbox+for+Hadoop)
- en: '|  |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: A.1\. Code for the book
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1\. 书中的代码
- en: Before we get to the instructions for installing Hadoop, let’s get you set up
    with the code that accompanies this book. The code is hosted on GitHub at [https://github.com/alexholmes/hiped2](https://github.com/alexholmes/hiped2).
    To get you up and running quickly, there are prepackaged tarballs that don’t require
    you to build the code—just install and go.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入安装 Hadoop 的说明之前，让我们先为您设置好这本书所附带的代码。代码托管在 GitHub 上，网址为 [https://github.com/alexholmes/hiped2](https://github.com/alexholmes/hiped2)。为了让您快速启动，有一些预包装的
    tarball，您无需构建代码——只需安装即可。
- en: Downloading
  id: totrans-11
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 下载
- en: First you’ll need to download the most recent release of the code from [https://github.com/alexholmes/hiped2/releases](https://github.com/alexholmes/hiped2/releases).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要从 [https://github.com/alexholmes/hiped2/releases](https://github.com/alexholmes/hiped2/releases)
    下载代码的最新版本。
- en: Installing
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安装
- en: 'The second step is to unpackage the tarball into a directory of your choosing.
    For example, the following untars the code into /usr/local, the same directory
    where you’ll install Hadoop:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是将 tarball 解压到您选择的目录中。例如，以下操作将代码解压到 /usr/local 目录，这是您将安装 Hadoop 的同一目录：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Adding the home directory to your path
  id: totrans-16
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将主目录添加到您的路径中
- en: 'All the examples in the book assume that the home directory for the code is
    in your path. The methods for doing this differ by operating system and shell.
    If you’re on Linux using Bash, then the following should work (use of the single
    quotes for the second command is required to avoid variable substitution):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 书中的所有示例都假设代码的主目录在您的路径中。完成此操作的方法因操作系统和 shell 而异。如果您使用的是 Linux Bash，则以下命令应该可以工作（第二个命令需要使用单引号以避免变量替换）：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Running an example job
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 运行示例作业
- en: 'You can run the following commands to test your installation. This assumes
    that you have a running Hadoop setup (if you don’t, please jump to [section A.3](#app01lev1sec3)):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令来测试您的安装。这假设您有一个正在运行的 Hadoop 设置（如果您没有，请跳转到 [章节 A.3](#app01lev1sec3)）：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Downloading the sources and building
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 下载源代码并构建
- en: 'There are some techniques (such as Avro code generation) that require access
    to the full sources. First, check out the sources using git:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有些技术（如 Avro 代码生成）需要访问完整源代码。首先，使用 git 检出源代码：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Set up your environment so that some techniques know where the source is installed:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 设置您的环境，以便某些技术知道源代码安装的位置：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can build the project using Maven:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 Maven 构建项目：
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This generates a target/hip-<version>-package.tar.gz file, which is the same
    file that’s uploaded to GitHub when releases are made.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个 target/hip-<version>-package.tar.gz 文件，这是在发布时上传到 GitHub 的同一文件。
- en: A.2\. Recommended Java versions
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2\. 推荐的 Java 版本
- en: The Hadoop project keeps a list of recommended Java versions that have been
    proven to work well with Hadoop in production. For details, take a look at “Hadoop
    Java Versions” on the Hadoop Wiki at [http://wiki.apache.org/hadoop/HadoopJavaVersions](http://wiki.apache.org/hadoop/HadoopJavaVersions).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 项目维护了一个推荐列表，列出了在生产环境中与 Hadoop 一起工作表现良好的 Java 版本。有关详细信息，请参阅 Hadoop Wiki
    上的“Hadoop Java Versions”页面 [http://wiki.apache.org/hadoop/HadoopJavaVersions](http://wiki.apache.org/hadoop/HadoopJavaVersions)。
- en: A.3\. Hadoop
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3\. Hadoop
- en: This section covers installing, configuring, and running the Apache distribution
    of Hadoop. Please refer to distribution-specific instructions if you’re working
    with a different distribution of Hadoop.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖 Apache Hadoop 分发的安装、配置和运行。如果你使用的是其他 Hadoop 分发版，请参考特定分发的说明。
- en: Apache tarball installation
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Apache tarball 安装
- en: The following instructions are for users who want to install the tarball version
    of the vanilla Apache Hadoop distribution. This is a a pseudo-distributed setup
    and not for a multi-node cluster.^([[1](#app01fn01)])
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下说明适用于想要安装 Apache Hadoop 分发版 tarball 版本的用户。这是一个伪分布式设置，不适用于多节点集群.^([[1](#app01fn01)])
- en: ¹ Pseudo-distributed mode is when you have all the Hadoop components running
    on a single host.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹ 伪分布式模式是指所有 Hadoop 组件都在单个主机上运行。
- en: 'First you’ll need to download the tarball from the Apache downloads page at
    [http://hadoop.apache.org/common/releases.html#Download](http://hadoop.apache.org/common/releases.html#Download)
    and extract the tarball under /usr/local:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要从 Apache 下载页面 [http://hadoop.apache.org/common/releases.html#Download](http://hadoop.apache.org/common/releases.html#Download)
    下载 tarball，并在 /usr/local 下解压：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '|  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Installation directory for users that don’t have root privileges
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 没有root权限的用户安装目录
- en: If you don’t have root permissions on your host, you can install Hadoop under
    a different directory and substitute instances of /usr/local in the following
    instructions with your directory name.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有主机上的 root 权限，你可以在不同的目录下安装 Hadoop，并在以下说明中将 /usr/local 替换为你的目录名。
- en: '|  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Configuration for pseudo-distributed mode for Hadoop 1 and earlier
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Hadoop 1 及更早版本的伪分布式模式配置
- en: The following instructions work for Hadoop version 1 and earlier. Skip to the
    next section if you’re working with Hadoop 2.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下说明适用于 Hadoop 1 及更早版本。如果你正在使用 Hadoop 2，请跳到下一节。
- en: 'Edit the file /usr/local/hadoop/conf/core-site.xml and make sure it looks like
    the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑文件 /usr/local/hadoop/conf/core-site.xml，并确保其看起来如下所示：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then edit the file /usr/local/hadoop/conf/hdfs-site.xml and make sure it looks
    like the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后编辑文件 /usr/local/hadoop/conf/hdfs-site.xml，并确保其看起来如下所示：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, edit the file /usr/local/hadoop/conf/mapred-site.xml and make sure
    it looks like the following (you may first need to copy mapred-site.xml.template
    to mapred-site.xml):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，编辑文件 /usr/local/hadoop/conf/mapred-site.xml，并确保其看起来如下所示（你可能首先需要将 mapred-site.xml.template
    复制到 mapred-site.xml）：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Configuration for pseudo-distributed mode for Hadoop 2
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Hadoop 2 的伪分布式模式配置
- en: The following instructions work for Hadoop 2\. See the previous section if you’re
    working with Hadoop version 1 and earlier.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下说明适用于 Hadoop 2。如果你正在使用 Hadoop 1 及更早版本，请参阅上一节。
- en: 'Edit the file /usr/local/hadoop/etc/hadoop/core-site.xml and make sure it looks
    like the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑文件 /usr/local/hadoop/etc/hadoop/core-site.xml，并确保其看起来如下所示：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then edit the file /usr/local/hadoop/etc/hadoop/hdfs-site.xml and make sure
    it looks like the following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后编辑文件 /usr/local/hadoop/etc/hadoop/hdfs-site.xml，并确保其看起来如下所示：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, edit the file /usr/local/hadoop/etc/hadoop/mapred-site.xml and make sure
    it looks like the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，编辑文件 /usr/local/hadoop/etc/hadoop/mapred-site.xml，并确保其看起来如下所示：
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, edit the file /usr/local/hadoop/etc/hadoop/yarn-site.xml and make
    sure it looks like the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，编辑文件 /usr/local/hadoop/etc/hadoop/yarn-site.xml，并确保其看起来如下所示：
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Set up SSH
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置 SSH
- en: 'Hadoop uses Secure Shell (SSH) to remotely launch processes such as the Data-Node
    and TaskTracker, even when everything is running on a single node in pseudo-distributed
    mode. If you don’t already have an SSH key pair, create one with the following
    command:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 使用 Secure Shell (SSH) 在伪分布式模式下远程启动进程，如 Data-Node 和 TaskTracker，即使所有内容都在单个节点上运行。如果你还没有
    SSH 密钥对，可以使用以下命令创建一个：
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You’ll need to copy the .ssh/id_rsa file to the authorized_keys file:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要将 .ssh/id_rsa 文件复制到 authorized_keys 文件中：
- en: '[PRE15]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You’ll also need an SSH agent running so that you aren’t prompted to enter your
    password a bazillion times when starting and stopping Hadoop. Different operating
    systems have different ways of running an SSH agent, and there are details online
    for CentOS and other Red Hat derivatives^([[2](#app01fn02)]) and for OS X.^([[3](#app01fn03)])
    Google is your friend if you’re running on a different system.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要运行一个 SSH 代理，这样在启动和停止 Hadoop 时就不会被要求输入密码无数次。不同的操作系统有不同的运行 SSH 代理的方式，CentOS
    和其他 Red Hat 衍生版^([[2](#app01fn02)]) 以及 OS X 的详细信息可以在网上找到。如果你运行的是不同的系统，Google 是你的朋友。
- en: ² See the Red Hat Deployment Guide section on “Configuring ssh-agent” at [www.centos.org/docs/5/html/5.2/Deployment_Guide/s3-openssh-config-ssh-agent.html](http://www.centos.org/docs/5/html/5.2/Deployment_Guide/s3-openssh-config-ssh-agent.html).
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ² 请参阅 Red Hat 部署指南中关于“配置 ssh-agent”的[www.centos.org/docs/5/html/5.2/Deployment_Guide/s3-openssh-config-ssh-agent.html](http://www.centos.org/docs/5/html/5.2/Deployment_Guide/s3-openssh-config-ssh-agent.html)部分。
- en: ³ See “Using SSH Agent With Mac OS X Leopard” at [www-uxsup.csx.cam.ac.uk/~aia21/osx/leopard-ssh.html](http://www-uxsup.csx.cam.ac.uk/~aia21/osx/leopard-ssh.html).
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³ 请参阅“在 Mac OS X Leopard 中使用 SSH Agent”的[www-uxsup.csx.cam.ac.uk/~aia21/osx/leopard-ssh.html](http://www-uxsup.csx.cam.ac.uk/~aia21/osx/leopard-ssh.html)。
- en: 'To verify that the agent is running and has your keys loaded, try opening an
    SSH connection to the local system:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证代理正在运行并且已加载您的密钥，请尝试打开到本地系统的 SSH 连接：
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: If you’re prompted for a password, the agent’s not running or doesn’t have your
    keys loaded.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您被提示输入密码，则代理没有运行或没有加载您的密钥。
- en: Java
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Java
- en: You need a current version of Java (1.6 or newer) installed on your system.
    You’ll need to ensure that the system path includes the binary directory of your
    Java installation. Alternatively, you can edit /usr/local/hadoop/conf/hadoop-env.sh,
    uncomment the JAVA_HOME line, and update the value with the location of your Java
    installation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要在您的系统上安装当前版本的 Java（1.6 或更高版本）。您需要确保系统路径包括您的 Java 安装的二进制目录。或者，您可以编辑 /usr/local/hadoop/conf/hadoop-env.sh，取消注释
    JAVA_HOME 行，并使用您的 Java 安装位置更新值。
- en: Environment settings
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 环境设置
- en: 'For convenience, it’s recommended that you add the Hadoop binary directory
    to your path. The following code shows what you can add to the bottom of your
    Bash shell profile file in ~/.bash_profile (assuming you’re running Bash):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，建议您将 Hadoop 二进制目录添加到您的路径中。以下代码显示了您可以在 ~/.bash_profile（假设您正在运行 Bash）的底部添加的内容：
- en: '[PRE17]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Format HDFS
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 格式化 HDFS
- en: Next you need to format HDFS. The rest of the commands in this section assume
    that the Hadoop binary directory exists in your path, as per the preceding instructions.
    On Hadoop 1 and earlier, type
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来您需要格式化 HDFS。本节中此后的命令假设 Hadoop 二进制目录已存在于您的路径中，如前所述。在 Hadoop 1 及更早版本上，键入
- en: '[PRE18]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: On Hadoop versions 2 and newer, type
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Hadoop 2 及更高版本上，键入
- en: '[PRE19]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: After HDFS has been formatted, you’re ready to start Hadoop.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在 HDFS 格式化后，您就可以开始使用 Hadoop 了。
- en: Starting Hadoop 1 and earlier
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 启动 Hadoop 1 及更早版本
- en: 'A single command can be used to start Hadoop on versions 1 and earlier:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在版本 1 及更早版本上，可以使用单个命令启动 Hadoop：
- en: '[PRE20]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After running the start script, use the `jps` Java utility to check that all
    the processes are running. You should see the following output (with the exception
    of the process IDs, which will be different):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 运行启动脚本后，使用 `jps` Java 工具检查所有进程是否正在运行。您应该看到以下输出（进程 ID 除外，将不同）：
- en: '[PRE21]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: If any of these processes aren’t running, check the logs directory (/usr/local/hadoop/logs)
    to see why the processes didn’t start correctly. Each of the preceding processes
    has two output files that can be identified by name and should be checked for
    errors.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些进程中的任何一个没有运行，请检查日志目录 (/usr/local/hadoop/logs)，以查看进程为什么无法正确启动。前面提到的每个进程都有两个可以通过名称识别的输出文件，应该检查是否有错误。
- en: The most common error is that the HDFS formatting step, which I showed earlier,
    was skipped.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的错误是前面展示的 HDFS 格式化步骤被跳过了。
- en: Starting Hadoop 2
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 启动 Hadoop 2
- en: 'The following commands are required to start Hadoop version 2:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 Hadoop 版本 2 需要以下命令：
- en: '[PRE22]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After running the start script, use the `jps` Java utility to check that all
    the processes are running. You should see the output that follows, although the
    ordering and process IDs will differ:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 运行启动脚本后，使用 `jps` Java 工具检查所有进程是否正在运行。您应该看到以下输出，尽管顺序和进程 ID 将不同：
- en: '[PRE23]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: If any of these processes aren’t running, check the logs directory (/usr/local/hadoop/logs)
    to see why the processes didn’t start correctly. Each of the preceding processes
    has two output files that can be identified by name and should be checked for
    errors. The most common error is that the HDFS formatting step, which I showed
    earlier, was skipped.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些进程中的任何一个没有运行，请检查日志目录 (/usr/local/hadoop/logs)，以查看进程为什么无法正确启动。前面提到的每个进程都有两个可以通过名称识别的输出文件，应该检查是否有错误。最常见错误是前面展示的
    HDFS 格式化步骤被跳过了。
- en: Creating a home directory for your user on HDFS
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 HDFS 上为您的用户创建一个家目录
- en: Once Hadoop is up and running, the first thing you’ll want to do is create a
    home directory for your user. If you’re running on Hadoop 1, the command is
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Hadoop启动并运行，您首先想要做的是为您自己的用户创建一个主目录。如果您在Hadoop 1上运行，命令是
- en: '[PRE24]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: On Hadoop 2, you’ll run
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop 2上，您将运行
- en: '[PRE25]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Verifying the installation
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 验证安装
- en: 'The following commands can be used to test your Hadoop installation. The first
    two commands create a directory in HDFS and create a file in HDFS:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令可以用来测试您的Hadoop安装。前两个命令在HDFS中创建一个目录并创建一个文件：
- en: '[PRE26]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next you want to run a word-count MapReduce job. On Hadoop 1 and earlier, run
    the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您想运行一个单词计数MapReduce作业。在Hadoop 1及更早版本上，运行以下命令：
- en: '[PRE27]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'On Hadoop 2, run the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop 2上，运行以下命令：
- en: '[PRE28]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Examine and verify the MapReduce job outputs on HDFS (the outputs will differ
    based on the contents of the config files that you used for the job inputs):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 检查并验证HDFS上的MapReduce作业输出（输出将根据您用于作业输入的配置文件内容而有所不同）：
- en: '[PRE29]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Stopping Hadoop 1
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 停止Hadoop 1
- en: 'To stop Hadoop 1, use the following command:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要停止Hadoop 1，请使用以下命令：
- en: '[PRE30]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Stopping Hadoop 2
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 停止Hadoop 2
- en: 'To stop Hadoop 2, use the following commands:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 要停止Hadoop 2，请使用以下命令：
- en: '[PRE31]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Just as with starting, the `jps` command can be used to verify that all the
    Hadoop processes have stopped.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 就像启动一样，`jps`命令可以用来验证所有Hadoop进程是否已停止。
- en: Hadoop 1.x UI ports
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Hadoop 1.x UI端口
- en: There are a number of web applications in Hadoop. [Table A.1](#app01table01)
    lists them, along with the ports they run on and their URLs (assuming they’re
    running on the local host, as is the case if you have a pseudo-distributed installation
    running).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop中有许多Web应用程序。[表A.1](#app01table01)列出了它们，包括它们运行的端口和URL（假设它们运行在本地主机上，如果您有一个伪分布式安装运行，情况就是这样）。
- en: Table A.1\. Hadoop 1.x web applications and ports
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表A.1. Hadoop 1.x Web应用程序和端口
- en: '| Component | Default port | Config parameter | Local URL |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 组件 | 默认端口 | 配置参数 | 本地URL |'
- en: '| --- | --- | --- | --- |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| MapReduce JobTracker | 50030 | mapred.job.tracker.http.address | http://127.0.0.1:50030/
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| MapReduce作业跟踪器 | 50030 | mapred.job.tracker.http.address | http://127.0.0.1:50030/
    |'
- en: '| MapReduce TaskTracker | 50060 | mapred.task.tracker.http.address | http://127.0.0.1:50060/
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| MapReduce任务跟踪器 | 50060 | mapred.task.tracker.http.address | http://127.0.0.1:50060/
    |'
- en: '| HDFS NameNode | 50070 | dfs.http.address | http://127.0.0.1:50070/ |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| HDFS NameNode | 50070 | dfs.http.address | http://127.0.0.1:50070/ |'
- en: '| HDFS DataNode | 50075 | dfs.datanode.http.address | http://127.0.0.1:50075/
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| HDFS数据节点 | 50075 | dfs.datanode.http.address | http://127.0.0.1:50075/ |'
- en: '| HDFS Secondary-NameNode | 50090 | dfs.secondary.http.address | http://127.0.0.1:50090/
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| HDFS辅助-NameNode | 50090 | dfs.secondary.http.address | http://127.0.0.1:50090/
    |'
- en: '| HDFS Backup and Checkpoint Node | 50105 | dfs.backup.http.address | http://127.0.0.1:50105/
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| HDFS备份和检查点节点 | 50105 | dfs.backup.http.address | http://127.0.0.1:50105/
    |'
- en: 'Each of these URLs supports the following common paths:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 每个这些URL都支持以下常见路径：
- en: '***/logs*** —This shows a listing of all the files under hadoop.log.dir. By
    default, this is under $HADOOP_HOME/logs on each Hadoop node.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***/logs*** — 这显示了hadoop.log.dir下所有文件的列表。默认情况下，这位于每个Hadoop节点的$HADOOP_HOME/logs下。'
- en: '***/logLevel*** —This can be used to view and set the logging levels for Java
    packages.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***/logLevel*** — 这可以用来查看和设置Java包的日志级别。'
- en: '***/metrics*** —This shows JVM and component-level statistics. It’s available
    in Hadoop 0.21 and newer (not in 1.0, 0.20.x, or earlier).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***/metrics*** — 这显示了JVM和组件级别的统计信息。它在Hadoop 0.21及更高版本中可用（不在1.0、0.20.x或更早版本中）。'
- en: '***/stacks*** —This shows a stack dump of all the current Java threads in the
    daemon.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***/stacks*** — 这显示了守护进程中所有当前Java线程的堆栈转储。'
- en: Hadoop 2.x UI ports
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Hadoop 2.x UI端口
- en: There are a number of web applications in Hadoop. [Table A.2](#app01table02)
    lists them, including the ports that they run on and their URLs (assuming they’re
    running on the local host, as is the case if you have a pseudo-distributed installation
    running).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop中有许多Web应用程序。[表A.2](#app01table02)列出了它们，包括它们运行的端口和URL（假设它们运行在本地主机上，如果您有一个伪分布式安装运行，情况就是这样）。
- en: Table A.2\. Hadoop 2.x web applications and ports
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表A.2. Hadoop 2.x Web应用程序和端口
- en: '| Component | Default port | Config parameter | Local URL |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 组件 | 默认端口 | 配置参数 | 本地URL |'
- en: '| --- | --- | --- | --- |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| YARN ResourceManager | 8088 | yarn.resourcemanager.webapp.address | http://localhost:8088/cluster
    |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| YARN资源管理器 | 8088 | yarn.resourcemanager.webapp.address | http://localhost:8088/cluster
    |'
- en: '| YARN NodeManager | 8042 | yarn.nodemanager.webapp.address | http://localhost:8042/node
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| YARN节点管理器 | 8042 | yarn.nodemanager.webapp.address | http://localhost:8042/node
    |'
- en: '| MapReduce Job History | 19888 | mapreduce.jobhistory.webapp.address | http://localhost:19888/jobhistory
    |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| MapReduce作业历史记录 | 19888 | mapreduce.jobhistory.webapp.address | http://localhost:19888/jobhistory
    |'
- en: '| HDFS Name-Node | 50070 | dfs.http.address | http://127.0.0.1:50070/ |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| HDFS名称节点 | 50070 | dfs.http.address | http://127.0.0.1:50070/ |'
- en: '| HDFS DataNode | 50075 | dfs.datanode.http.address | http://127.0.0.1:50075/
    |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| HDFS数据节点 | 50075 | dfs.datanode.http.address | http://127.0.0.1:50075/ |'
- en: A.4\. Flume
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4\. Flume
- en: Flume is a log collection and distribution system that can transport data across
    a large number of hosts into HDFS. It’s an Apache project originally developed
    by Cloudera.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Flume是一个日志收集和分发系统，可以将数据传输到大量主机上的HDFS。它是由Cloudera最初开发的Apache项目。
- en: '[Chapter 5](kindle_split_015.html#ch05) contains a section on Flume and how
    it can be used.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[第5章](kindle_split_015.html#ch05) 包含了关于Flume及其使用方法的章节。'
- en: Getting more information
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 获取更多信息
- en: '[Table A.3](#app01table03) lists some useful resources to help you become more
    familiar with Flume.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[表A.3](#app01table03) 列出了一些有用的资源，以帮助您更熟悉Flume。'
- en: Table A.3\. Useful resources
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表A.3\. 有用资源
- en: '| Resource | URL |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | 网址 |'
- en: '| --- | --- |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Flume main page | [http://flume.apache.org/](http://flume.apache.org/) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Flume主页 | [http://flume.apache.org/](http://flume.apache.org/) |'
- en: '| Flume user guide | [http://flume.apache.org/FlumeUserGuide.html](http://flume.apache.org/FlumeUserGuide.html)
    |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Flume用户指南 | [http://flume.apache.org/FlumeUserGuide.html](http://flume.apache.org/FlumeUserGuide.html)
    |'
- en: '| Flume Getting Started guide | [https://cwiki.apache.org/confluence/display/FLUME/Getting+Started](https://cwiki.apache.org/confluence/display/FLUME/Getting+Started)
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Flume入门指南 | [https://cwiki.apache.org/confluence/display/FLUME/Getting+Started](https://cwiki.apache.org/confluence/display/FLUME/Getting+Started)
    |'
- en: Installation on Apache Hadoop 1.x systems
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在Apache Hadoop 1.x系统上的安装
- en: Follow the Getting Started guide referenced in the resources.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 按照资源中引用的入门指南进行操作。
- en: Installation on Apache Hadoop 2.x systems
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在Apache Hadoop 2.x系统上的安装
- en: 'If you’re trying to get Flume 1.4 to work with Hadoop 2, follow the Getting
    Started guide to install Flume. Next, you’ll need to remove the protobuf and guava
    JARs from Flume’s lib directory because they conflict with the versions bundled
    with Hadoop 2:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您试图让Flume 1.4与Hadoop 2一起工作，请按照入门指南安装Flume。接下来，您需要从Flume的lib目录中删除protobuf和guava
    JARs，因为它们与Hadoop 2捆绑的版本冲突：
- en: '[PRE32]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: A.5\. Oozie
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5\. Oozie
- en: Oozie is an Apache project that started life inside Yahoo. It’s a Hadoop workflow
    engine that manages data processing activities.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Oozie是一个起源于雅虎的Apache项目。它是一个Hadoop工作流引擎，用于管理数据处理活动。
- en: Getting more information
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 获取更多信息
- en: '[Table A.4](#app01table04) lists some useful resources to help you become more
    familiar with Oozie.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[表A.4](#app01table04) 列出了一些有用的资源，以帮助您更熟悉Oozie。'
- en: Table A.4\. Useful resources
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表A.4\. 有用资源
- en: '| Resource | URL |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | 网址 |'
- en: '| --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Oozie project page | [https://oozie.apache.org/](https://oozie.apache.org/)
    |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Oozie项目页面 | [https://oozie.apache.org/](https://oozie.apache.org/) |'
- en: '| Oozie Quick Start | [https://oozie.apache.org/docs/4.0.0/DG_QuickStart.html](https://oozie.apache.org/docs/4.0.0/DG_QuickStart.html)
    |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Oozie快速入门 | [https://oozie.apache.org/docs/4.0.0/DG_QuickStart.html](https://oozie.apache.org/docs/4.0.0/DG_QuickStart.html)
    |'
- en: '| Additional Oozie resources | [https://oozie.apache.org/docs/4.0.0/index.html](https://oozie.apache.org/docs/4.0.0/index.html)
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 其他Oozie资源 | [https://oozie.apache.org/docs/4.0.0/index.html](https://oozie.apache.org/docs/4.0.0/index.html)
    |'
- en: Installation on Hadoop 1.x systems
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在Hadoop 1.x系统上的安装
- en: Follow the Quick Start guide to install Oozie. The Oozie documentation has installation
    instructions.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 按照快速入门指南安装Oozie。Oozie文档中包含安装说明。
- en: 'If you’re using Oozie 4.4.0 and targeting Hadoop 2.2.0, you’ll need to run
    the following commands to patch your Maven files and perform the build:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用Oozie 4.4.0并针对Hadoop 2.2.0，您需要运行以下命令来修补您的Maven文件并执行构建：
- en: '[PRE33]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Installation on Hadoop 2.x systems
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在Hadoop 2.x系统上的安装
- en: 'Unfortunately Oozie 4.0.0 doesn’t play nicely with Hadoop 2\. To get Oozie
    working with Hadoop, you’ll first need to download the 4.0.0 tarball from the
    project page and then unpackage it. Next, run the following command to change
    the Hadoop version being targeted:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Oozie 4.0.0与Hadoop 2不兼容。为了使Oozie与Hadoop一起工作，您首先需要从项目页面下载4.0.0的tarball，然后解包它。接下来，运行以下命令以更改目标Hadoop版本：
- en: '[PRE34]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now all you need to do is target the hadoop-2 profile in Maven:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您只需要在Maven中针对hadoop-2配置文件：
- en: '[PRE35]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: A.6\. Sqoop
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.6\. Sqoop
- en: Sqoop is a tool for importing data from relational databases into Hadoop and
    vice versa. It can support any JDBC-compliant database, and it also has native
    connectors for efficient data transport to and from MySQL and PostgreSQL.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Sqoop 是一种工具，用于将数据从关系型数据库导入到 Hadoop，反之亦然。它支持任何 JDBC 兼容的数据库，并且它还提供了用于高效数据传输到 MySQL
    和 PostgreSQL 的本地连接器。
- en: '[Chapter 5](kindle_split_015.html#ch05) contains details on how imports and
    exports can be performed with Sqoop.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[第 5 章](kindle_split_015.html#ch05) 包含了使用 Sqoop 进行导入和导出的详细信息。'
- en: Getting more information
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 获取更多信息
- en: '[Table A.5](#app01table05) lists some useful resources to help you become more
    familiar with Sqoop.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 A.5](#app01table05) 列出了一些有用的资源，可以帮助您更熟悉 Sqoop。'
- en: Table A.5\. Useful resources
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 A.5\. 有用资源
- en: '| Resource | URL |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | URL |'
- en: '| --- | --- |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Sqoop project page | [http://sqoop.apache.org/](http://sqoop.apache.org/)
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Sqoop 项目页面 | [http://sqoop.apache.org/](http://sqoop.apache.org/) |'
- en: '| Sqoop User Guide | [http://sqoop.apache.org/docs/1.4.4/SqoopUserGuide.html](http://sqoop.apache.org/docs/1.4.4/SqoopUserGuide.html)
    |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| Sqoop 用户指南 | [http://sqoop.apache.org/docs/1.4.4/SqoopUserGuide.html](http://sqoop.apache.org/docs/1.4.4/SqoopUserGuide.html)
    |'
- en: Installation
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '| 安装 |'
- en: 'Download the Sqoop tarball from the project page. Pick the version that matches
    with your Hadoop installation and explode the tarball. The following instructions
    assume that you’re installing under /usr/local:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 从项目页面下载 Sqoop tarball。选择与您的 Hadoop 安装匹配的版本，并解压 tarball。以下说明假设您正在 /usr/local
    下安装：
- en: '[PRE36]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '|  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Sqoop 2
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Sqoop 2
- en: This book currently covers Sqoop version 1\. When selecting which tarball to
    download, please note that version 1.99.x and newer are the Sqoop 2 versions,
    so be sure to pick an older version.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 本书目前涵盖 Sqoop 版本 1。在选择要下载的 tarball 时，请注意，1.99.x 版本及更高版本是 Sqoop 2 版本，因此请确保选择一个较旧的版本。
- en: '|  |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'If you’re planning on using Sqoop with MySQL, you’ll need to download the MySQL
    JDBC driver tarball from [http://dev.mysql.com/downloads/connector/j/](http://dev.mysql.com/downloads/connector/j/),
    explode it into a directory, and then copy the JAR file into the Sqoop lib directory:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您计划使用 Sqoop 与 MySQL 一起使用，您需要从 [http://dev.mysql.com/downloads/connector/j/](http://dev.mysql.com/downloads/connector/j/)
    下载 MySQL JDBC 驱动程序的 tarball，将其解压到一个目录中，然后将 JAR 文件复制到 Sqoop lib 目录：
- en: '[PRE37]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: To run Sqoop, there are a few environment variables that you may need to set.
    They’re listed in [table A.6](#app01table06).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 Sqoop 时，您可能需要设置一些环境变量。它们列在 [表 A.6](#app01table06) 中。
- en: Table A.6\. Sqoop environment variables
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 A.6\. Sqoop 环境变量
- en: '| Environment variable | Description |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 环境变量 | 描述 |'
- en: '| --- | --- |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| JAVA_HOME | The directory where Java is installed. If you have the Sun JDK
    installed on Red Hat, this would be /usr/java/latest. |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| JAVA_HOME | Java 安装所在的目录。如果您在 Red Hat 上安装了 Sun JDK，这将是指 /usr/java/latest。|'
- en: '| HADOOP_HOME | The directory of your Hadoop installation. |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| HADOOP_HOME | 您 Hadoop 安装所在的目录。|'
- en: '| HIVE_HOME | Only required if you’re planning on using Hive with Sqoop. Refers
    to the directory where Hive was installed. |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| HIVE_HOME | 仅在您计划使用 Hive 与 Sqoop 一起使用时才需要。指 Hive 安装所在的目录。|'
- en: '| HBASE_HOME | Only required if you’re planning on using HBase with Sqoop.
    Refers to the directory where HBase was installed. |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| HBASE_HOME | 仅在您计划使用 Sqoop 与 HBase 一起使用时才需要。指 HBase 安装所在的目录。|'
- en: The /usr/local/sqoop/bin directory contains the binaries for Sqoop. [Chapter
    5](kindle_split_015.html#ch05) contains a number of techniques that show how the
    binaries are used for imports and exports.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: /usr/local/sqoop/bin 目录包含 Sqoop 的二进制文件。[第 5 章](kindle_split_015.html#ch05) 包含了展示如何使用二进制文件进行导入和导出的多种技术。
- en: A.7\. HBase
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.7\. HBase
- en: HBase is a real-time, key/value, distributed, column-based database modeled
    after Google’s BigTable.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: HBase 是一个基于 Google 的 BigTable 模式的实时、键/值、分布式、基于列的数据库。
- en: Getting more information
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 获取更多信息
- en: '[Table A.7](#app01table07) lists some useful resources to help you become more
    familiar with HBase.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 A.7](#app01table07) 列出了一些有用的资源，可以帮助您更熟悉 HBase。'
- en: Table A.7\. Useful resources
  id: totrans-210
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 A.7\. 有用资源
- en: '| Resource | URL |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | URL |'
- en: '| --- | --- |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Apache HBase project page | [http://hbase.apache.org/](http://hbase.apache.org/)
    |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| Apache HBase 项目页面 | [http://hbase.apache.org/](http://hbase.apache.org/)
    |'
- en: '| Apache HBase Quick Start | [http://hbase.apache.org/book/quickstart.html](http://hbase.apache.org/book/quickstart.html)
    |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| Apache HBase 快速入门 | [http://hbase.apache.org/book/quickstart.html](http://hbase.apache.org/book/quickstart.html)
    |'
- en: '| Apache HBase Reference Guide | [http://hbase.apache.org/book/book.html](http://hbase.apache.org/book/book.html)
    |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| Apache HBase 参考指南 | [http://hbase.apache.org/book/book.html](http://hbase.apache.org/book/book.html)
    |'
- en: '| Cloudera blog post on HBase Dos and Don’ts | [http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/](http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/)
    |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| Cloudera关于HBase的“做与不做”博客文章 | [http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/](http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/)
    |'
- en: Installation
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安装
- en: Follow the installation instructions in the Quick Start guide at [https://hbase.apache.org/book/quickstart.html](https://hbase.apache.org/book/quickstart.html).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 按照位于[https://hbase.apache.org/book/quickstart.html](https://hbase.apache.org/book/quickstart.html)的快速入门指南中的安装说明进行操作。
- en: A.8\. Kafka
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.8\. Kafka
- en: Kafka is a publish/subscribe messaging system built by LinkedIn.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka是由LinkedIn构建的一个发布/订阅消息系统。
- en: Getting more information
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 获取更多信息
- en: '[Table A.8](#app01table08) lists some useful resources to help you become more
    familiar with Kafka.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[表A.8](#app01table08) 列出了一些有用资源，以帮助您更熟悉Kafka。'
- en: Table A.8\. Useful resources
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表A.8\. 有用资源
- en: '| Resource | URL |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | URL |'
- en: '| --- | --- |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Kafka project page | [http://kafka.apache.org/](http://kafka.apache.org/)
    |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Kafka项目页面 | [http://kafka.apache.org/](http://kafka.apache.org/) |'
- en: '| Kafka documentation | [http://kafka.apache.org/documentation.html](http://kafka.apache.org/documentation.html)
    |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| Kafka文档 | [http://kafka.apache.org/documentation.html](http://kafka.apache.org/documentation.html)
    |'
- en: '| Kafka Quick Start | [http://kafka.apache.org/08/quickstart.html](http://kafka.apache.org/08/quickstart.html)
    |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| Kafka快速入门 | [http://kafka.apache.org/08/quickstart.html](http://kafka.apache.org/08/quickstart.html)
    |'
- en: Installation
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安装
- en: Follow the installation instructions in the Quick Start guide.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 按照快速入门指南中的安装说明进行操作。
- en: A.9\. Camus
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.9\. Camus
- en: Camus is a tool for importing data in Kafka into Hadoop.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Camus是一个将Kafka中的数据导入Hadoop的工具。
- en: Getting more information
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 获取更多信息
- en: '[Table A.9](#app01table09) lists some useful resources to help you become more
    familiar with Camus.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[表A.9](#app01table09) 列出了一些有用资源，以帮助您更熟悉Camus。'
- en: Table A.9\. Useful resources
  id: totrans-235
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表A.9\. 有用资源
- en: '| Resource | URL |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | URL |'
- en: '| --- | --- |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Camus project page | [https://github.com/linkedin/camus](https://github.com/linkedin/camus)
    |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| Camus项目页面 | [https://github.com/linkedin/camus](https://github.com/linkedin/camus)
    |'
- en: '| Camus Overview | [https://github.com/linkedin/camus/wiki/Camus-Overview](https://github.com/linkedin/camus/wiki/Camus-Overview)
    |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| Camus概述 | [https://github.com/linkedin/camus/wiki/Camus-Overview](https://github.com/linkedin/camus/wiki/Camus-Overview)
    |'
- en: Installation on Hadoop 1
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在Hadoop 1上安装
- en: 'Download the code from the 0.8 branch in GitHub, and run the following command
    to build it:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 从GitHub的0.8分支下载代码，并运行以下命令来构建它：
- en: '[PRE38]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Installation on Hadoop 2
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在Hadoop 2上安装
- en: At the time of writing, the 0.8 version of Camus doesn’t support Hadoop 2\.
    You have a couple of options to get it working—if you’re just experimenting with
    Camus, you can download a patched version of the code from my GitHub project.
    Alternatively, you can patch the Maven build files.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Camus的0.8版本不支持Hadoop 2。您有几个选项可以使它工作——如果您只是对Camus进行实验，可以从我的GitHub项目中下载修补过的代码版本。或者，您可以修补Maven构建文件。
- en: Using my patched GitHub project
  id: totrans-245
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用我的修补过的GitHub项目
- en: 'Download my cloned and patched version of Camus from GitHub and build it just
    as you would the Hadoop 1 version:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 从GitHub下载我克隆并修补过的Camus版本，并像Hadoop 1版本一样构建它：
- en: '[PRE39]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Patching the Maven build files
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 修补Maven构建文件
- en: 'If you want to patch the original Camus files, you can do that by taking a
    look at the patch I applied to my own clone: [https://mng.bz/Q8GV](https://mng.bz/Q8GV).'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想修补原始的Camus文件，可以通过查看我自己的克隆中应用的补丁来完成：[https://mng.bz/Q8GV](https://mng.bz/Q8GV)。
- en: A.10\. Avro
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.10\. Avro
- en: Avro is a data serialization system that provides features such as compression,
    schema evolution, and code generation. It can be viewed as a more sophisticated
    version of a SequenceFile, with additional features such as schema evolution.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Avro是一个提供压缩、模式演进和代码生成等功能的数据序列化系统。它可以看作是SequenceFile的一个更复杂的版本，具有模式演进等附加功能。
- en: '[Chapter 3](kindle_split_013.html#ch03) contains details on how Avro can be
    used in MapReduce as well as with basic input/output streams.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[第3章](kindle_split_013.html#ch03) 包含了关于如何在MapReduce以及与基本输入/输出流中使用Avro的详细信息。'
- en: Getting more information
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 获取更多信息
- en: '[Table A.10](#app01table10) lists some useful resources to help you become
    more familiar with Avro.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[表A.10](#app01table10) 列出了一些有用资源，以帮助您更熟悉Avro。'
- en: Table A.10\. Useful resources
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表A.10\. 有用资源
- en: '| Resource | URL |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | URL |'
- en: '| --- | --- |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Avro project page | [http://avro.apache.org/](http://avro.apache.org/) |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| Avro项目页面 | [http://avro.apache.org/](http://avro.apache.org/) |'
- en: '| Avro issue tracking page | [https://issues.apache.org/jira/browse/AVRO](https://issues.apache.org/jira/browse/AVRO)
    |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| Avro 问题跟踪页面 | [https://issues.apache.org/jira/browse/AVRO](https://issues.apache.org/jira/browse/AVRO)
    |'
- en: '| Cloudera blog about Avro use | [http://blog.cloudera.com/blog/2011/12/apache-avro-at-richrelevance/](http://blog.cloudera.com/blog/2011/12/apache-avro-at-richrelevance/)
    |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| Cloudera 关于 Avro 的博客 | [http://blog.cloudera.com/blog/2011/12/apache-avro-at-richrelevance/](http://blog.cloudera.com/blog/2011/12/apache-avro-at-richrelevance/)
    |'
- en: '| CDH usage page for Avro | [http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/5.0/CDH5-Installation-Guide/cdh5ig_avro_usage.html](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/5.0/CDH5-Installation-Guide/cdh5ig_avro_usage.html)
    |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| CDH 使用 Avro 的页面 | [http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/5.0/CDH5-Installation-Guide/cdh5ig_avro_usage.html](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/5.0/CDH5-Installation-Guide/cdh5ig_avro_usage.html)
    |'
- en: Installation
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安装
- en: Avro is a full-fledged Apache project, so you can download the binaries from
    the downloads link on the Apache project page.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: Avro 是一个完整的 Apache 项目，因此您可以从 Apache 项目页面上的下载链接下载二进制文件。
- en: A.11\. Apache Thrift
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.11\. Apache Thrift
- en: Apache Thrift is essentially Facebook’s version of Protocol Buffers. It offers
    very similar data-serialization and RPC capabilities. In this book, I use it with
    Elephant Bird to support Thrift in MapReduce. Elephant Bird currently works with
    Thrift version 0.7.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Thrift 实质上是 Facebook 的 Protocol Buffers 版本。它提供了非常相似的数据序列化和 RPC 功能。在这本书中，我使用它与
    Elephant Bird 一起支持 MapReduce 中的 Thrift。Elephant Bird 目前支持 Thrift 版本 0.7。
- en: Getting more information
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 获取更多信息
- en: Thrift documentation is lacking, something which the project page attests to.
    [Table A.11](#app01table11) lists some useful resources to help you become more
    familiar with Thrift.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: Thrift 文档不足，这一点在项目页面上得到了证实。[表A.11](#app01table11) 列出了一些有用的资源，以帮助您更熟悉 Thrift。
- en: Table A.11\. Useful resources
  id: totrans-268
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表A.11\. 有用资源
- en: '| Resource | URL |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | 网址 |'
- en: '| --- | --- |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Thrift project page | [http://thrift.apache.org/](http://thrift.apache.org/)
    |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| Thrift 项目页面 | [http://thrift.apache.org/](http://thrift.apache.org/) |'
- en: '| Blog post with a Thrift tutorial | [http://bit.ly/vXpZ0z](http://bit.ly/vXpZ0z)
    |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 包含 Thrift 教程的博客文章 | [http://bit.ly/vXpZ0z](http://bit.ly/vXpZ0z) |'
- en: Building Thrift 0.7
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建 Thrift 0.7
- en: 'To build Thrift, download the 0.7 tarball and extract the contents. You may
    need to install some Thrift dependencies:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建 Thrift，请下载 0.7 版本的 tarball 并提取内容。您可能需要安装一些 Thrift 依赖项：
- en: '[PRE40]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Build and install the native and Java/Python libraries and binaries:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 构建并安装原生和 Java/Python 库及二进制文件：
- en: '[PRE41]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Build the Java library. This step requires Ant to be installed, instructions
    for which are available in the Apache Ant Manual at [http://ant.apache.org/manual/index.html](http://ant.apache.org/manual/index.html):'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 构建 Java 库。此步骤需要安装 Ant，相关说明可在 Apache Ant 手册中找到，网址为 [http://ant.apache.org/manual/index.html](http://ant.apache.org/manual/index.html)：
- en: '[PRE42]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Copy the Java JAR into Hadoop’s lib directory. The following instructions are
    for CDH:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Java JAR 文件复制到 Hadoop 的 lib 目录。以下说明适用于 CDH：
- en: '[PRE43]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: A.12\. Protocol Buffers
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.12\. Protocol Buffers
- en: Protocol Buffers is Google’s data serialization and Remote Procedure Call (RPC)
    library, which is used extensively at Google. In this book, we’ll use it in conjunction
    with Elephant Bird and Rhipe. Elephant Bird requires version 2.3.0 of Protocol
    Buffers (and won’t work with any other version), and Rhipe only works with Protocol
    Buffers version 2.4.0 and newer.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: Protocol Buffers 是 Google 的数据序列化和远程过程调用（RPC）库，在 Google 中被广泛使用。在这本书中，我们将与 Elephant
    Bird 和 Rhipe 一起使用它。Elephant Bird 需要 Protocol Buffers 的 2.3.0 版本（与其他版本不兼容），而 Rhipe
    只与 Protocol Buffers 版本 2.4.0 及更高版本兼容。
- en: Getting more information
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 获取更多信息
- en: '[Table A.12](#app01table12) lists some useful resources to help you become
    more familiar with Protocol Buffers.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '[表A.12](#app01table12) 列出了一些有用的资源，以帮助您更熟悉 Protocol Buffers。'
- en: Table A.12\. Useful resources
  id: totrans-286
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表A.12\. 有用资源
- en: '| Resource | URL |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | 网址 |'
- en: '| --- | --- |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Protocol Buffers project page | [http://code.google.com/p/protobuf/](http://code.google.com/p/protobuf/)
    |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| Protocol Buffers 项目页面 | [http://code.google.com/p/protobuf/](http://code.google.com/p/protobuf/)
    |'
- en: '| Protocol Buffers Developer Guide | [https://developers.google.com/protocol-buffers/docs/overview?csw=1](https://developers.google.com/protocol-buffers/docs/overview?csw=1)
    |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| Protocol Buffers 开发者指南 | [https://developers.google.com/protocol-buffers/docs/overview?csw=1](https://developers.google.com/protocol-buffers/docs/overview?csw=1)
    |'
- en: '| Protocol Buffers downloads page, containing a link for version 2.3.0 (required
    for use with Elephant Bird) | [http://code.google.com/p/protobuf/downloads/list](http://code.google.com/p/protobuf/downloads/list)
    |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 协议缓冲区下载页面，包含 2.3.0 版本的链接（与 Elephant Bird 一起使用所需） | [http://code.google.com/p/protobuf/downloads/list](http://code.google.com/p/protobuf/downloads/list)
    |'
- en: Building Protocol Buffers
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建 Protocol Buffers
- en: To build Protocol Buffers, download the 2.3 or 2.4 (2.3 for Elephant Bird and
    2.4 for Rhipe) source tarball from [http://code.google.com/p/protobuf/downloads](http://code.google.com/p/protobuf/downloads)
    and extract the contents.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建 Protocol Buffers，从 [http://code.google.com/p/protobuf/downloads](http://code.google.com/p/protobuf/downloads)
    下载 2.3 或 2.4 版本的源 tarball（2.3 版本用于 Elephant Bird，2.4 版本用于 Rhipe）并提取内容。
- en: 'You’ll need a C++ compiler, which can be installed on 64-bit RHEL systems with
    the following command:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要一个 C++ 编译器，可以在 64 位 RHEL 系统上使用以下命令安装：
- en: '[PRE44]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Build and install the native libraries and binaries:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 构建和安装原生库和二进制文件：
- en: '[PRE45]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Build the Java library:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 构建 Java 库：
- en: '[PRE46]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Copy the Java JAR into Hadoop’s lib directory. The following instructions are
    for CDH:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Java JAR 文件复制到 Hadoop 的 lib 目录中。以下说明适用于 CDH：
- en: '[PRE47]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: A.13\. Snappy
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.13. Snappy
- en: Snappy is a native compression codec developed by Google that offers fast compression
    and decompression times. It can’t be split (as opposed to LZOP compression). In
    the book’s code examples, which don’t require splittable compression, we’ll use
    Snappy because of its time efficiency.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: Snappy 是由 Google 开发的原生压缩编解码器，提供快速的压缩和解压缩时间。它不能分割（与 LZOP 压缩相反）。在本书的代码示例中，由于不需要可分割的压缩，我们将使用
    Snappy，因为它的时间效率更高。
- en: Snappy is integrated into the Apache distribution of Hadoop since versions 1.0.2
    and 2.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: Snappy 自 1.0.2 和 2 版本以来已集成到 Apache Hadoop 发行版中。
- en: Getting more information
  id: totrans-305
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 获取更多信息
- en: '[Table A.13](#app01table13) lists some useful resources to help you become
    more familiar with Snappy.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 A.13](#app01table13) 列出了一些有用的资源，可以帮助您更熟悉 Snappy。'
- en: Table A.13\. Useful resources
  id: totrans-307
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 A.13. 有用资源
- en: '| Resource | URL |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | URL |'
- en: '| --- | --- |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Google’s Snappy project page | [http://code.google.com/p/snappy/](http://code.google.com/p/snappy/)
    |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| Google 的 Snappy 项目页面 | [http://code.google.com/p/snappy/](http://code.google.com/p/snappy/)
    |'
- en: '| Snappy integration with Hadoop | [http://code.google.com/p/hadoop-snappy/](http://code.google.com/p/hadoop-snappy/)
    |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| Snappy 与 Hadoop 的集成 | [http://code.google.com/p/hadoop-snappy/](http://code.google.com/p/hadoop-snappy/)
    |'
- en: A.14\. LZOP
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.14. LZOP
- en: LZOP is a compression codec that can be used to support splittable compression
    in MapReduce. [Chapter 4](kindle_split_014.html#ch04) has a section dedicated
    to working with LZOP. In this section we’ll cover how to build and set up your
    cluster to work with LZOP.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: LZOP 是一种压缩编解码器，可用于在 MapReduce 中支持可分割压缩。第 4 章有一个专门介绍如何使用 LZOP 的部分。在本节中，我们将介绍如何构建和设置您的集群以使用
    LZOP。
- en: Getting more information
  id: totrans-314
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 获取更多信息
- en: '[Table A.14](#app01table14) shows a useful resource to help you become more
    familiar with LZOP.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 A.14](#app01table14) 展示了一个有用的资源，可以帮助您更熟悉 LZOP。'
- en: Table A.14\. Useful resource
  id: totrans-316
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 A.14. 有用资源
- en: '| Resource | URL |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | URL |'
- en: '| --- | --- |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Hadoop LZO project maintained by Twitter | [https://github.com/twitter/hadoop-lzo](https://github.com/twitter/hadoop-lzo)
    |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 由 Twitter 维护的 Hadoop LZO 项目 | [https://github.com/twitter/hadoop-lzo](https://github.com/twitter/hadoop-lzo)
    |'
- en: Building LZOP
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建 LZOP
- en: 'The following steps walk you through the process of configuring LZOP compression.
    Before you do this, there are a few things to consider:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将指导您配置 LZOP 压缩。在您这样做之前，有一些事情需要考虑：
- en: It’s highly recommended that you build the libraries on the same hardware that
    you have deployed in production.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强烈建议您在部署生产环境中使用的相同硬件上构建库。
- en: All of the installation and configuration steps will need to be performed on
    any client hosts that will be using LZOP, as well as all the DataNodes in your
    cluster.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有安装和配置步骤都需要在所有将使用 LZOP 的客户端主机以及您集群中的所有 DataNode 上执行。
- en: These steps are for Apache Hadoop distributions. Please refer to distribution-specific
    instructions if you’re using a different distribution.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些步骤适用于 Apache Hadoop 发行版。如果您使用的是其他发行版，请参阅特定发行版的说明。
- en: Twitter’s LZO project page has instructions on how to download dependencies
    and build the project. Follow the Building and Configuring section on the project
    home page.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter 的 LZO 项目页面提供了有关如何下载依赖项和构建项目的说明。请遵循项目主页上的“构建和配置”部分。
- en: Configuring Hadoop
  id: totrans-326
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 配置 Hadoop
- en: 'You need to configure Hadoop core to be aware of your new compression codecs.
    Add the following lines to your core-site.xml. Make sure you remove the newlines
    and spaces so that there are no whitespace characters between the commas:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要配置 Hadoop 核心来使它了解您的新压缩编解码器。将以下行添加到您的 core-site.xml 中。确保删除换行符和空格，以便在逗号之间没有空白字符：
- en: '[PRE48]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The value for `io.compression.codecs` assumes that you have the Snappy compression
    codec already installed. If you don’t, remove `org.apache.hadoop.io.compress.SnappyCodec`
    from the value.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '`io.compression.codecs` 的值假设您已经安装了 Snappy 压缩编解码器。如果没有，请从值中删除 `org.apache.hadoop.io.compress.SnappyCodec`。'
- en: A.15\. Elephant Bird
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.15\. Elephant Bird
- en: Elephant Bird is a project that provides utilities for working with LZOP-compressed
    data. It also provides a container format that supports working with Protocol
    Buffers and Thrift in MapReduce.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: Elephant Bird 是一个提供用于处理 LZOP 压缩数据的实用程序的项目。它还提供了一个容器格式，支持在 MapReduce 中使用 Protocol
    Buffers 和 Thrift。
- en: Getting more information
  id: totrans-332
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 获取更多信息
- en: '[Table A.15](#app01table15) shows a useful resource to help you become more
    familiar with Elephant Bird.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 A.15](#app01table15) 展示了一个有用的资源，以帮助您更熟悉 Elephant Bird。'
- en: Table A.15\. Useful resource
  id: totrans-334
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 A.15\. 有用资源
- en: '| Resource | URL |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | URL |'
- en: '| --- | --- |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Elephant Bird project page | [https://github.com/kevinweil/elephant-bird](https://github.com/kevinweil/elephant-bird)
    |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| Elephant Bird 项目页面 | [https://github.com/kevinweil/elephant-bird](https://github.com/kevinweil/elephant-bird)
    |'
- en: At the time of writing, the current version of Elephant Bird (4.4) doesn’t work
    with Hadoop 2 due the use of an incompatible version of Protocol Buffers. To get
    Elephant Bird to work in this book, I had to build a version of the project from
    the trunk that works with Hadoop 2 (as will 4.5 when it is released).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，由于使用了不兼容版本的 Protocol Buffers，Elephant Bird（4.4）的当前版本与 Hadoop 2 不兼容。为了使
    Elephant Bird 在本书中工作，我不得不从 trunk 构建一个与 Hadoop 2 兼容的项目版本（4.5 版本发布时也将如此）。
- en: A.16\. Hive
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.16\. Hive
- en: Hive is a SQL interface on top of Hadoop.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 是 Hadoop 之上的 SQL 接口。
- en: Getting more information
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 获取更多信息
- en: '[Table A.16](#app01table16) lists some useful resources to help you become
    more familiar with Hive.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 A.16](#app01table16) 列出了一些有用的资源，以帮助您更熟悉 Hive。'
- en: Table A.16\. Useful resources
  id: totrans-343
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 A.16\. 有用资源
- en: '| Resource | URL |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | URL |'
- en: '| --- | --- |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Hive project page | [http://hive.apache.org/](http://hive.apache.org/) |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| Hive 项目页面 | [http://hive.apache.org/](http://hive.apache.org/) |'
- en: '| Getting Started | [https://cwiki.apache.org/confluence/display/Hive/GettingStarted](https://cwiki.apache.org/confluence/display/Hive/GettingStarted)
    |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 入门 | [https://cwiki.apache.org/confluence/display/Hive/GettingStarted](https://cwiki.apache.org/confluence/display/Hive/GettingStarted)
    |'
- en: Installation
  id: totrans-348
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安装
- en: Follow the installation instructions in Hive’s Getting Started guide.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 按照 Hive 入门指南中的安装说明进行操作。
- en: A.17\. R
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.17\. R
- en: R is an open source tool for statistical programming and graphics.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: R 是一个用于统计编程和图形的开源工具。
- en: Getting more information
  id: totrans-352
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 获取更多信息
- en: '[Table A.17](#app01table17) lists some useful resources to help you become
    more familiar with R.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 A.17](#app01table17) 列出了一些有用的资源，以帮助您更熟悉 R。'
- en: Table A.17\. Useful resources
  id: totrans-354
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 A.17\. 有用资源
- en: '| Resource | URL |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | URL |'
- en: '| --- | --- |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| R project page | [http://www.r-project.org/](http://www.r-project.org/) |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| R 项目页面 | [http://www.r-project.org/](http://www.r-project.org/) |'
- en: '| R function search engine | [http://rseek.org/](http://rseek.org/) |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| R 函数搜索引擎 | [http://rseek.org/](http://rseek.org/) |'
- en: Installation on Red Hat–based systems
  id: totrans-359
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在基于 Red Hat 的系统上安装
- en: 'Installing R from Yum makes things easy: it will figure out RPM dependencies
    and install them for you.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Yum 安装 R 使事情变得简单：它将确定 RPM 依赖关系并为您安装它们。
- en: 'Go to [http://www.r-project.org/](http://www.r-project.org/), click on CRAN,
    select a download region that’s close to you, select Red Hat, and pick the version
    and architecture appropriate for your system. Replace the URL in `baseurl` in
    the following code and execute the command to add the R mirror repo to your Yum
    configuration:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 [http://www.r-project.org/](http://www.r-project.org/)，点击 CRAN，选择一个靠近您的下载区域，选择
    Red Hat，并选择适合您系统的版本和架构。替换以下代码中的 `baseurl` URL 并执行命令以将 R 镜像仓库添加到您的 Yum 配置中：
- en: '[PRE49]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'A simple Yum command can be used to install R on 64-bit systems:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在 64 位系统上可以使用简单的 Yum 命令安装 R：
- en: '[PRE50]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '|  |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Perl-File-Copy-Recursive RPM
  id: totrans-366
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Perl-File-Copy-Recursive RPM
- en: On CentOS, the Yum install may fail, complaining about a missing dependency.
    In this case, you may need to manually install the perl-File-Copy-Recursive RPM
    (for CentOS you can get it from [http://mng.bz/n4C2](http://mng.bz/n4C2)).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CentOS 上，Yum 安装可能会失败，并抱怨缺少依赖项。在这种情况下，你可能需要手动安装 perl-File-Copy-Recursive RPM（对于
    CentOS，你可以从 [http://mng.bz/n4C2](http://mng.bz/n4C2) 获取）。
- en: '|  |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Installation on non–Red Hat systems
  id: totrans-369
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非 Red Hat 系统上的安装
- en: Go to [http://www.r-project.org/](http://www.r-project.org/), click on CRAN,
    select a download region that’s close to you, and select the appropriate binaries
    for your system.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 [http://www.r-project.org/](http://www.r-project.org/)，点击 CRAN，选择一个靠近你的下载区域，并选择适合你系统的相应二进制文件。
- en: A.18\. RHadoop
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.18\. RHadoop
- en: RHadoop is an open source tool developed by Revolution Analytics for integrating
    R with MapReduce.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: RHadoop 是由 Revolution Analytics 开发的一个开源工具，用于将 R 与 MapReduce 集成。
- en: Getting more information
  id: totrans-373
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 获取更多信息
- en: '[Table A.18](#app01table18) lists some useful resources to help you become
    more familiar with RHadoop.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 A.18](#app01table18) 列出了一些有用的资源，帮助你更熟悉 RHadoop。'
- en: Table A.18\. Useful resources
  id: totrans-375
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 A.18\. 有用资源
- en: '| Resource | URL |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | URL |'
- en: '| --- | --- |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| RHadoop project page | [https://github.com/RevolutionAnalytics/RHadoop/wiki](https://github.com/RevolutionAnalytics/RHadoop/wiki)
    |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| RHadoop 项目页面 | [https://github.com/RevolutionAnalytics/RHadoop/wiki](https://github.com/RevolutionAnalytics/RHadoop/wiki)
    |'
- en: '| RHadoop downloads and prerequisites | [https://github.com/RevolutionAnalytics/RHadoop/wiki/Downloads](https://github.com/RevolutionAnalytics/RHadoop/wiki/Downloads)
    |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| RHadoop 下载和先决条件 | [https://github.com/RevolutionAnalytics/RHadoop/wiki/Downloads](https://github.com/RevolutionAnalytics/RHadoop/wiki/Downloads)
    |'
- en: rmr/rhdfs installation
  id: totrans-380
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: rmr/rhdfs 安装
- en: 'Each node in your Hadoop cluster will require the following components:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 你的 Hadoop 集群中的每个节点都需要以下组件：
- en: R (installation instructions are in [section A.17](#app01lev1sec17)).
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R（安装说明在 [A.17 节](#app01lev1sec17) 中）。
- en: A number of RHadoop and dependency packages
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多 RHadoop 和依赖包
- en: RHadoop requires that you set environment variables to point to the Hadoop binary
    and the streaming JAR. It’s best to stash this in your .bash_profile (or equivalent).
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: RHadoop 需要你设置环境变量以指向 Hadoop 二进制文件和 streaming JAR 文件。最好将其存放在你的 .bash_profile（或等效文件）中。
- en: '[PRE51]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We’ll focus on the `rmr` and `rhdfs` RHadoop packages, which provide MapReduce
    and HDFS integration with R. Click on the `rmr` and `rhdfs` download links on
    [https://github.com/RevolutionAnalytics/RHadoop/wiki/Downloads](https://github.com/RevolutionAnalytics/RHadoop/wiki/Downloads).
    Then execute the following commands:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点关注 `rmr` 和 `rhdfs` RHadoop 包，它们提供了与 R 的 MapReduce 和 HDFS 集成。点击 [https://github.com/RevolutionAnalytics/RHadoop/wiki/Downloads](https://github.com/RevolutionAnalytics/RHadoop/wiki/Downloads)
    上的 `rmr` 和 `rhdfs` 下载链接。然后执行以下命令：
- en: '[PRE52]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'If you get an error installing rJava, you may need to set `JAVA_HOME` and reconfigure
    R prior to running the rJava installation:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你安装 rJava 时遇到错误，你可能需要在运行 rJava 安装之前设置 `JAVA_HOME` 并重新配置 R：
- en: '[PRE53]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Test that the `rmr` package was installed correctly by running the following
    command—if no error messages are generated, this means you have successfully installed
    the RHadoop packages.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下命令来测试 `rmr` 包是否正确安装——如果没有生成错误消息，这意味着你已经成功安装了 RHadoop 包。
- en: '[PRE54]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: A.19\. Mahout
  id: totrans-392
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.19\. Mahout
- en: Mahout is a predictive analytics project that offers both in-JVM and MapReduce
    implementations for some of its algorithms.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: Mahout 是一个预测分析项目，它为其一些算法提供了 JVM 内部和 MapReduce 实现。
- en: Getting more information
  id: totrans-394
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 获取更多信息
- en: '[Table A.19](#app01table19) lists some useful resources to help you become
    more familiar with Mahout.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 A.19](#app01table19) 列出了一些有用的资源，帮助你更熟悉 Mahout。'
- en: Table A.19\. Useful resources
  id: totrans-396
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 A.19\. 有用资源
- en: '| Resource | URL |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | URL |'
- en: '| --- | --- |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Mahout project page | [http://mahout.apache.org/](http://mahout.apache.org/)
    |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| Mahout 项目页面 | [http://mahout.apache.org/](http://mahout.apache.org/) |'
- en: '| Mahout downloads | [https://cwiki.apache.org/confluence/display/MAHOUT/Downloads](https://cwiki.apache.org/confluence/display/MAHOUT/Downloads)
    |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| Mahout 下载 | [https://cwiki.apache.org/confluence/display/MAHOUT/Downloads](https://cwiki.apache.org/confluence/display/MAHOUT/Downloads)
    |'
- en: Installation
  id: totrans-401
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安装
- en: Mahout should be installed on a node that has access to your Hadoop cluster.
    Mahout is a client-side library and doesn’t need to be installed on your Hadoop
    cluster.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: Mahout 应该安装在可以访问你的 Hadoop 集群的节点上。Mahout 是一个客户端库，不需要在 Hadoop 集群上安装。
- en: Building a Mahout distribution
  id: totrans-403
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 构建 Mahout 发行版
- en: 'To get Mahout working with Hadoop 2, I had to check out the code, modify the
    build file, and then build a distribution. The first step is to check out the
    code:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 要使 Mahout 与 Hadoop 2 一起工作，我不得不检出代码，修改构建文件，然后构建一个发行版。第一步是检出代码：
- en: '[PRE55]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Next you need to modify pom.xml and remove the following section from the file:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要修改 pom.xml 文件并从文件中删除以下部分：
- en: '[PRE56]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Finally, build a distribution:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，构建一个发行版：
- en: '[PRE57]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: This will generate a tarball located at distribution/target/mahout-distribution-1.0-SNAPSHOT.tar.gz,
    which you can install using the instructions in the next section.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在 distribution/target/mahout-distribution-1.0-SNAPSHOT.tar.gz 位置生成一个 tarball，你可以使用下一节中的说明进行安装。
- en: Installing Mahout
  id: totrans-411
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 安装 Mahout
- en: Mahout is packaged as a tarball. The following instructions will work on most
    Linux operating systems.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: Mahout 以 tarball 的形式打包。以下说明适用于大多数 Linux 操作系统。
- en: If you’re installing an official Mahout release, click on the “official release”
    links on the Mahout download page and select the current release. If Mahout 1
    hasn’t yet been released and you want to use Mahout with Hadoop 2, follow the
    instructions in the previous section to generate the tarball.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在安装官方的 Mahout 版本，请点击 Mahout 下载页面上的“官方版本”链接并选择当前版本。如果 Mahout 1 尚未发布，而你想要与
    Hadoop 2 一起使用 Mahout，请按照上一节中的说明生成 tarball。
- en: 'Install Mahout using the following instructions:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下说明安装 Mahout：
- en: '[PRE58]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'For convenience, it’s worthwhile updating your ~/.bash_profile to export a
    `MAHOUT_HOME` environment variable to your installation directory. The following
    command shows how this can be performed on the command line (the same command
    can be copied into your bash profile file):'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，值得更新你的 ~/.bash_profile，将 `MAHOUT_HOME` 环境变量导出到你的安装目录。以下命令展示了如何在命令行上执行此操作（相同的命令可以复制到你的
    bash 配置文件中）：
- en: '[PRE59]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
