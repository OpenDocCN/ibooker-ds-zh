- en: 'Chapter 45\. In Depth: Principal Component Analysis'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 45 章\. 深入解析：主成分分析
- en: 'Up until now, we have been looking in depth at supervised learning estimators:
    those estimators that predict labels based on labeled training data. Here we begin
    looking at several unsupervised estimators, which can highlight interesting aspects
    of the data without reference to any known labels.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在深入研究监督学习估计器：那些根据标记的训练数据预测标签的估计器。在这里，我们开始研究几个无监督估计器，这些估计器可以突出数据的一些有趣方面，而不需要参考任何已知的标签。
- en: In this chapter we will explore what is perhaps one of the most broadly used
    unsupervised algorithms, principal component analysis (PCA). PCA is fundamentally
    a dimensionality reduction algorithm, but it can also be useful as a tool for
    visualization, noise filtering, feature extraction and engineering, and much more.
    After a brief conceptual discussion of the PCA algorithm, we will explore a couple
    examples of these further applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨或许是最广泛使用的无监督算法之一，即主成分分析（PCA）。PCA 本质上是一种降维算法，但它也可以作为可视化工具、噪声过滤器、特征提取和工程等工具使用。在简短讨论了PCA算法的概念后，我们将探索其进一步应用的几个例子。
- en: 'We begin with the standard imports:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从标准导入开始：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Introducing Principal Component Analysis
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入主成分分析
- en: Principal component analysis is a fast and flexible unsupervised method for
    dimensionality reduction in data, which we saw briefly in [Chapter 38](ch38.xhtml#section-0502-introducing-scikit-learn).
    Its behavior is easiest to visualize by looking at a two-dimensional dataset.
    Consider these 200 points (see [Figure 45-1](#fig_0509-principal-component-analysis_files_in_output_4_0)).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析是一种快速灵活的无监督数据降维方法，我们在[第 38 章](ch38.xhtml#section-0502-introducing-scikit-learn)中简要介绍过。通过查看一个二维数据集，最容易理解其行为。考虑这
    200 个点（见[图 45-1](#fig_0509-principal-component-analysis_files_in_output_4_0)）。
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![output 4 0](assets/output_4_0.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![output 4 0](assets/output_4_0.png)'
- en: Figure 45-1\. Data for demonstration of PCA
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 45-1\. 主成分分析演示数据
- en: 'By eye, it is clear that there is a nearly linear relationship between the
    *x* and *y* variables. This is reminiscent of the linear regression data we explored
    in [Chapter 42](ch42.xhtml#section-0506-linear-regression), but the problem setting
    here is slightly different: rather than attempting to *predict* the *y* values
    from the *x* values, the unsupervised learning problem attempts to learn about
    the *relationship* between the *x* and *y* values.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从眼睛的角度来看，很明显*x*和*y*变量之间存在近乎线性的关系。这让人想起我们在[第 42 章](ch42.xhtml#section-0506-linear-regression)中探索的线性回归数据，但这里的问题设置略有不同：与尝试从*x*值预测*y*值不同，无监督学习问题试图学习*x*和*y*值之间的*关系*。
- en: 'In principal component analysis, this relationship is quantified by finding
    a list of the *principal axes* in the data, and using those axes to describe the
    dataset. Using Scikit-Learn’s `PCA` estimator, we can compute this as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在主成分分析中，通过找到数据中的一组*主轴*来量化这种关系，并使用这些轴来描述数据集。使用Scikit-Learn的`PCA`估计器，我们可以按如下方式计算：
- en: '[PRE2]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The fit learns some quantities from the data, most importantly the components
    and explained variance:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合从数据中学习到一些量，最重要的是分量和解释方差：
- en: '[PRE3]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To see what these numbers mean, let’s visualize them as vectors over the input
    data, using the components to define the direction of the vector and the explained
    variance to define the squared length of the vector (see [Figure 45-2](#fig_0509-principal-component-analysis_files_in_output_11_0)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这些数字的含义，让我们将它们视为向量在输入数据上的可视化，使用分量定义向量的方向和解释方差定义向量的平方长度（见[图 45-2](#fig_0509-principal-component-analysis_files_in_output_11_0)）。
- en: '[PRE5]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![output 11 0](assets/output_11_0.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![output 11 0](assets/output_11_0.png)'
- en: Figure 45-2\. Visualization of the principal axes in the data
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 45-2\. 数据中主轴的可视化
- en: These vectors represent the principal axes of the data, and the length of each
    vector is an indication of how “important” that axis is in describing the distribution
    of the data—more precisely, it is a measure of the variance of the data when projected
    onto that axis. The projection of each data point onto the principal axes are
    the principal components of the data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些向量代表了数据的主轴，每个向量的长度表示描述数据分布中该轴的“重要性”的指标——更精确地说，它是数据在投影到该轴上时的方差的度量。将每个数据点投影到主轴上得到了数据的主成分。
- en: If we plot these principal components beside the original data, we see the plots
    shown in [Figure 45-3](#fig_images_in_0509-pca-rotation).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这些主成分与原始数据一起绘制，我们会看到在[图 45-3](#fig_images_in_0509-pca-rotation)中显示的图形。
- en: '![05.09 PCA rotation](assets/05.09-PCA-rotation.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![05.09 PCA rotation](assets/05.09-PCA-rotation.png)'
- en: Figure 45-3\. Transformed principal axes in the data^([1](ch45.xhtml#idm45858728565136))
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 45-3\. 数据中转换后的主轴^([1](ch45.xhtml#idm45858728565136))
- en: This transformation from data axes to principal axes is an *affine transformation*,
    which means it is composed of a translation, rotation, and uniform scaling.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据轴转换为主轴的这种变换是*仿射变换*，这意味着它由平移、旋转和均匀缩放组成。
- en: While this algorithm to find principal components may seem like just a mathematical
    curiosity, it turns out to have very far-reaching applications in the world of
    machine learning and data exploration.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种寻找主成分的算法可能看起来只是一种数学上的好奇，但它事实证明在机器学习和数据探索领域有着非常广泛的应用。
- en: PCA as Dimensionality Reduction
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA作为降维
- en: Using PCA for dimensionality reduction involves zeroing out one or more of the
    smallest principal components, resulting in a lower-dimensional projection of
    the data that preserves the maximal data variance.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PCA进行降维涉及将一个或多个最小主成分置零，结果是数据的低维投影，保留了最大的数据方差。
- en: 'Here is an example of using PCA as a dimensionality reduction transform:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用PCA作为降维转换的示例：
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The transformed data has been reduced to a single dimension. To understand the
    effect of this dimensionality reduction, we can perform the inverse transform
    of this reduced data and plot it along with the original data (see [Figure 45-4](#fig_0509-principal-component-analysis_files_in_output_18_0)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后的数据已经降维到了单一维度。为了理解这种降维的效果，我们可以对这个降维后的数据执行逆变换，并将其与原始数据一起绘制（参见[图 45-4](#fig_0509-principal-component-analysis_files_in_output_18_0)）。
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![output 18 0](assets/output_18_0.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![output 18 0](assets/output_18_0.png)'
- en: Figure 45-4\. Visualization of PCA as dimensionality reduction
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 45-4\. PCA作为降维的可视化
- en: 'The light points are the original data, while the dark points are the projected
    version. This makes clear what a PCA dimensionality reduction means: the information
    along the least important principal axis or axes is removed, leaving only the
    component(s) of the data with the highest variance. The fraction of variance that
    is cut out (proportional to the spread of points about the line formed in the
    preceding figure) is roughly a measure of how much “information” is discarded
    in this reduction of dimensionality.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 浅色点是原始数据，而深色点是投影版本。这清楚地展示了PCA降维的含义：移除沿着最不重要的主轴或轴的信息，只留下数据方差最大的分量。被削减的方差比例（与在前述图中形成的线周围的点的散布成比例）大致是这种降维中丢失的“信息”量的度量。
- en: 'This reduced-dimension dataset is in some senses “good enough” to encode the
    most important relationships between the points: despite reducing the number of
    data features by 50%, the overall relationships between the data points are mostly
    preserved.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个降维后的数据集在某些意义上是“足够好”的，能够编码点之间最重要的关系：尽管将数据特征的数量减少了50%，但数据点之间的整体关系大部分得到了保留。
- en: 'PCA for Visualization: Handwritten Digits'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA用于可视化：手写数字
- en: The usefulness of dimensionality reduction may not be entirely apparent in only
    two dimensions, but it becomes clear when looking at high-dimensional data. To
    see this, let’s take a quick look at the application of PCA to the digits dataset
    we worked with in [Chapter 44](ch44.xhtml#section-0508-random-forests).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在只有两个维度时，降维的实用性可能并不完全明显，但当我们查看高维数据时，它变得更加清晰。为了看到这一点，让我们快速看一下我们在[第44章](ch44.xhtml#section-0508-random-forests)中处理的数字数据集应用PCA的情况。
- en: 'We’ll start by loading the data:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从加载数据开始：
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Recall that the digits dataset consists of 8 × 8–pixel images, meaning that
    they are 64-dimensional. To gain some intuition into the relationships between
    these points, we can use PCA to project them into a more manageable number of
    dimensions, say two:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 请回忆，数字数据集由8×8像素的图像组成，这意味着它们是64维的。为了对这些点之间的关系有一些直观的理解，我们可以使用PCA将它们投影到一个更易处理的维度，比如说两个维度：
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We can now plot the first two principal components of each point to learn about
    the data, as seen in [Figure 45-5](#fig_0509-principal-component-analysis_files_in_output_25_0).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以绘制每个点的前两个主成分，以了解数据，如在[图 45-5](#fig_0509-principal-component-analysis_files_in_output_25_0)中所见。
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![output 25 0](assets/output_25_0.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![output 25 0](assets/output_25_0.png)'
- en: Figure 45-5\. PCA applied to the handwritten digits data
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图45-5\. 应用于手写数字数据的 PCA
- en: 'Recall what these components mean: the full data is a 64-dimensional point
    cloud, and these points are the projection of each data point along the directions
    with the largest variance. Essentially, we have found the optimal stretch and
    rotation in 64-dimensional space that allows us to see the layout of the data
    in two dimensions, and we have done this in an unsupervised manner—that is, without
    reference to the labels.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下这些组件的含义：完整的数据是一个64维点云，而这些点是每个数据点沿着最大方差方向的投影。基本上，我们在64维空间中找到了允许我们在两个维度上看到数据布局的最佳拉伸和旋转，而且这是无监督的方式完成的，即没有参考标签。
- en: What Do the Components Mean?
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组件的含义是什么？
- en: 'We can go a bit further here, and begin to ask what the reduced dimensions
    *mean*. This meaning can be understood in terms of combinations of basis vectors.
    For example, each image in the training set is defined by a collection of 64 pixel
    values, which we will call the vector <math alttext="x"><mi>x</mi></math> :'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以再进一步，开始问减少的维度意味着什么。这种意义可以用基向量的组合来理解。例如，训练集中的每个图像由一组64个像素值定义，我们将其称为向量<math
    alttext="x"><mi>x</mi></math>：
- en: <math alttext="x equals left-bracket x 1 comma x 2 comma x 3 ellipsis x 64 right-bracket"
    display="block"><mrow><mi>x</mi> <mo>=</mo> <mo>[</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>3</mn></msub>
    <mo>⋯</mo> <msub><mi>x</mi> <mn>64</mn></msub> <mo>]</mo></mrow></math>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="x equals left-bracket x 1 comma x 2 comma x 3 ellipsis x 64 right-bracket"
    display="block"><mrow><mi>x</mi> <mo>=</mo> <mo>[</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>3</mn></msub>
    <mo>⋯</mo> <msub><mi>x</mi> <mn>64</mn></msub> <mo>]</mo></mrow></math>
- en: 'One way we can think about this is in terms of a pixel basis. That is, to construct
    the image, we multiply each element of the vector by the pixel it describes, and
    then add the results together to build the image:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用像素基础来考虑这一点。也就是说，为了构建图像，我们将向量的每个元素乘以它描述的像素，然后将结果相加以构建图像：
- en: <math alttext="normal i normal m normal a normal g normal e left-parenthesis
    x right-parenthesis equals x 1 dot left-parenthesis normal p normal i normal x
    normal e normal l 1 right-parenthesis plus x 2 dot left-parenthesis normal p normal
    i normal x normal e normal l 2 right-parenthesis plus x 3 dot left-parenthesis
    normal p normal i normal x normal e normal l 3 right-parenthesis ellipsis x 64
    dot left-parenthesis normal p normal i normal x normal e normal l 64 right-parenthesis"
    display="block"><mrow><mi>image</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>·</mo> <mrow><mo>(</mo> <mi>pixel</mi>
    <mn>1</mn> <mo>)</mo></mrow> <mo>+</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>·</mo>
    <mrow><mo>(</mo> <mi>pixel</mi> <mn>2</mn> <mo>)</mo></mrow> <mo>+</mo> <msub><mi>x</mi>
    <mn>3</mn></msub> <mo>·</mo> <mrow><mo>(</mo> <mi>pixel</mi> <mn>3</mn> <mo>)</mo></mrow>
    <mo>⋯</mo> <msub><mi>x</mi> <mn>64</mn></msub> <mo>·</mo> <mrow><mo>(</mo> <mi>pixel</mi>
    <mn>64</mn> <mo>)</mo></mrow></mrow></math>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal i normal m normal a normal g normal e left-parenthesis
    x right-parenthesis equals x 1 dot left-parenthesis normal p normal i normal x
    normal e normal l 1 right-parenthesis plus x 2 dot left-parenthesis normal p normal
    i normal x normal e normal l 2 right-parenthesis plus x 3 dot left-parenthesis
    normal p normal i normal x normal e normal l 3 right-parenthesis ellipsis x 64
    dot left-parenthesis normal p normal i normal x normal e normal l 64 right-parenthesis"
    display="block"><mrow><mi>image</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>·</mo> <mrow><mo>(</mo> <mi>pixel</mi>
    <mn>1</mn> <mo>)</mo></mrow> <mo>+</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>·</mo>
    <mrow><mo>(</mo> <mi>pixel</mi> <mn>2</mn> <mo>)</mo></mrow> <mo>+</mo> <msub><mi>x</mi>
    <mn>3</mn></msub> <mo>·</mo> <mrow><mo>(</mo> <mi>pixel</mi> <mn>3</mn> <mo>)</mo></mrow>
    <mo>⋯</mo> <msub><mi>x</mi> <mn>64</mn></msub> <mo>·</mo> <mrow><mo>(</mo> <mi>pixel</mi>
    <mn>64</mn> <mo>)</mo></mrow></mrow></math>
- en: 'One way we might imagine reducing the dimensionality of this data is to zero
    out all but a few of these basis vectors. For example, if we use only the first
    eight pixels, we get an eight-dimensional projection of the data ([Figure 45-6](#fig_images_in_0509-digits-pixel-components)).
    However, it is not very reflective of the whole image: we’ve thrown out nearly
    90% of the pixels!'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以想象一种降低数据维度的方法是将除了少数基向量之外的所有值归零。例如，如果我们仅使用前八个像素，我们得到数据的八维投影（见[图 45-6](#fig_images_in_0509-digits-pixel-components)）。然而，这并不太反映整个图像：我们几乎丢弃了90%的像素！
- en: '![05.09 digits pixel components](assets/05.09-digits-pixel-components.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![05.09 手写数字像素组件](assets/05.09-digits-pixel-components.png)'
- en: Figure 45-6\. A naive dimensionality reduction achieved by discarding pixels^([2](ch45.xhtml#idm45858728053776))
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图45-6\. 通过丢弃像素实现的天真降维^([2](ch45.xhtml#idm45858728053776))
- en: The upper row of panels shows the individual pixels, and the lower row shows
    the cumulative contribution of these pixels to the construction of the image.
    Using only eight of the pixel-basis components, we can only construct a small
    portion of the 64-pixel image. Were we to continue this sequence and use all 64
    pixels, we would recover the original image.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 上排面板显示单独的像素，下排显示这些像素对图像构建的累积贡献。仅使用八个像素基础组件，我们只能构建64像素图像的一小部分。如果我们继续这个序列并使用所有64个像素，我们将恢复原始图像。
- en: 'But the pixel-wise representation is not the only choice of basis. We can also
    use other basis functions, which each contain some predefined contribution from
    each pixel, and write something like:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 但像素级表示并不是唯一的基础选择。我们还可以使用其他基函数，每个基函数都包含来自每个像素的一些预定义贡献，并编写如下内容：
- en: <math alttext="i m a g e left-parenthesis x right-parenthesis equals normal
    m normal e normal a normal n plus x 1 dot left-parenthesis normal b normal a normal
    s normal i normal s 1 right-parenthesis plus x 2 dot left-parenthesis normal b
    normal a normal s normal i normal s 2 right-parenthesis plus x 3 dot left-parenthesis
    normal b normal a normal s normal i normal s 3 right-parenthesis ellipsis" display="block"><mrow><mi>i</mi>
    <mi>m</mi> <mi>a</mi> <mi>g</mi> <mi>e</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mi>mean</mi> <mo>+</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>·</mo>
    <mrow><mo>(</mo> <mi>basis</mi> <mn>1</mn> <mo>)</mo></mrow> <mo>+</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>·</mo> <mrow><mo>(</mo> <mi>basis</mi> <mn>2</mn> <mo>)</mo></mrow>
    <mo>+</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>·</mo> <mrow><mo>(</mo> <mi>basis</mi>
    <mn>3</mn> <mo>)</mo></mrow> <mo>⋯</mo></mrow></math>
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="i m a g e left-parenthesis x right-parenthesis equals normal
    m normal e normal a normal n plus x 1 dot left-parenthesis normal b normal a normal
    s normal i normal s 1 right-parenthesis plus x 2 dot left-parenthesis normal b
    normal a normal s normal i normal s 2 right-parenthesis plus x 3 dot left-parenthesis
    normal b normal a normal s normal i normal s 3 right-parenthesis ellipsis" display="block"><mrow><mi>i</mi>
    <mi>m</mi> <mi>a</mi> <mi>g</mi> <mi>e</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mi>mean</mi> <mo>+</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>·</mo>
    <mrow><mo>(</mo> <mi>basis</mi> <mn>1</mn> <mo>)</mo></mrow> <mo>+</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>·</mo> <mrow><mo>(</mo> <mi>basis</mi> <mn>2</mn> <mo>)</mo></mrow>
    <mo>+</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>·</mo> <mrow><mo>(</mo> <mi>basis</mi>
    <mn>3</mn> <mo>)</mo></mrow> <mo>⋯</mo></mrow></math>
- en: PCA can be thought of as a process of choosing optimal basis functions, such
    that adding together just the first few of them is enough to suitably reconstruct
    the bulk of the elements in the dataset. The principal components, which act as
    the low-dimensional representation of our data, are simply the coefficients that
    multiply each of the elements in this series. [Figure 45-7](#fig_images_in_0509-digits-pca-components)
    shows a similar depiction of reconstructing the same digit using the mean plus
    the first eight PCA basis functions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 可以被看作是选择最优基函数的过程，使得仅添加前几个基函数就足以适当地重构数据集中的大部分元素。主成分作为我们数据的低维表示，实际上只是乘以这一系列中每个元素的系数。[图 45-7](#fig_images_in_0509-digits-pca-components)展示了使用平均值加上前八个
    PCA 基函数重建相同数字的类似描述。
- en: '![05.09 digits pca components](assets/05.09-digits-pca-components.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![05.09 手写数字 PCA 组件](assets/05.09-digits-pca-components.png)'
- en: Figure 45-7\. A more sophisticated dimensionality reduction achieved by discarding
    the least important principal components (compare to [Figure 45-6](#fig_images_in_0509-digits-pixel-components))^([3](ch45.xhtml#idm45858728025120))
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 45-7\. 通过丢弃最不重要的主成分实现的更复杂的降维（与[图 45-6](#fig_images_in_0509-digits-pixel-components)比较)^([3](ch45.xhtml#idm45858728025120))
- en: 'Unlike the pixel basis, the PCA basis allows us to recover the salient features
    of the input image with just a mean, plus eight components! The amount of each
    pixel in each component is the corollary of the orientation of the vector in our
    two-dimensional example. This is the sense in which PCA provides a low-dimensional
    representation of the data: it discovers a set of basis functions that are more
    efficient than the native pixel basis of the input data.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 与像素基础不同，PCA基础允许我们仅通过平均值加上八个组件来恢复输入图像的显著特征！每个像素在每个组件中的量是我们二维示例中向量方向的必然结果。这就是PCA提供数据低维表示的方式：它发现一组比输入数据的原生像素基础更有效的基础函数。
- en: Choosing the Number of Components
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择组件的数量
- en: A vital part of using PCA in practice is the ability to estimate how many components
    are needed to describe the data. This can be determined by looking at the cumulative
    *explained variance ratio* as a function of the number of components (see [Figure 45-8](#fig_0509-principal-component-analysis_files_in_output_34_0)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中使用PCA的一个重要部分是估计需要多少个组件来描述数据。这可以通过查看组件数量作为累积*解释方差比*的函数来确定（参见[图 45-8](#fig_0509-principal-component-analysis_files_in_output_34_0)）。
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This curve quantifies how much of the total, 64-dimensional variance is contained
    within the first <math alttext="upper N"><mi>N</mi></math> components. For example,
    we see that with the digits data the first 10 components contain approximately
    75% of the variance, while you need around 50 components to describe close to
    100% of the variance.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此曲线量化了在前<math alttext="upper N"><mi>N</mi></math>个组件中包含的总64维方差的比例。例如，我们看到对于数字数据，前10个组件包含大约75%的方差，而您需要约50个组件来描述接近100%的方差。
- en: '![output 34 0](assets/output_34_0.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![output 34 0](assets/output_34_0.png)'
- en: Figure 45-8\. The cumulative explained variance, which measures how well PCA
    pre‐ serves the content of the data
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 45-8\. 累积解释方差，用于衡量PCA保留数据内容的效果
- en: This tells us that our two-dimensional projection loses a lot of information
    (as measured by the explained variance) and that we’d need about 20 components
    to retain 90% of the variance. Looking at this plot for a high-dimensional dataset
    can help you understand the level of redundancy present in its features.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，我们的二维投影丢失了大量信息（由解释方差度量），我们需要大约20个组件来保留90%的方差。查看高维数据集的此图可以帮助您了解其特征中存在的冗余水平。
- en: PCA as Noise Filtering
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA作为噪声过滤器
- en: 'PCA can also be used as a filtering approach for noisy data. The idea is this:
    any components with variance much larger than the effect of the noise should be
    relatively unaffected by the noise. So, if you reconstruct the data using just
    the largest subset of principal components, you should be preferentially keeping
    the signal and throwing out the noise.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: PCA还可以用作噪声数据的过滤方法。其思想是：任何方差远大于噪声影响的成分应该相对不受噪声影响。因此，如果您仅使用主成分的最大子集重建数据，则应优先保留信号并丢弃噪声。
- en: Let’s see how this looks with the digits data. First we will plot several of
    the input noise-free input samples ([Figure 45-9](#fig_0509-principal-component-analysis_files_in_output_37_0)).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看数字数据的情况。首先，我们将绘制几个无噪声输入样本（[图 45-9](#fig_0509-principal-component-analysis_files_in_output_37_0)）。
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![output 37 0](assets/output_37_0.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![output 37 0](assets/output_37_0.png)'
- en: Figure 45-9\. Digits without noise
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 45-9\. 无噪声的数字
- en: Now let’s add some random noise to create a noisy dataset, and replot it ([Figure 45-10](#fig_0509-principal-component-analysis_files_in_output_40_0)).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们添加一些随机噪声以创建一个带噪声的数据集，并重新绘制它（[图 45-10](#fig_0509-principal-component-analysis_files_in_output_40_0)）。
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![output 40 0](assets/output_40_0.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![output 40 0](assets/output_40_0.png)'
- en: Figure 45-10\. Digits with Gaussian random noise added
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 45-10\. 添加了高斯随机噪声的数字
- en: 'The visualization makes the presence of this random noise clear. Let’s train
    a PCA model on the noisy data, requesting that the projection preserve 50% of
    the variance:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化使得随机噪声的存在变得明显。让我们在嘈杂数据上训练一个PCA模型，并要求投影保留50%的方差：
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here 50% of the variance amounts to 12 principal components, out of the 64 original
    features. Now we compute these components, and then use the inverse of the transform
    to reconstruct the filtered digits; [Figure 45-11](#fig_0509-principal-component-analysis_files_in_output_44_0)
    shows the result.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这里50%的方差相当于12个主成分，而原始的64个特征。现在我们计算这些成分，然后使用变换的逆来重构经过滤波的数字；[图45-11](#fig_0509-principal-component-analysis_files_in_output_44_0)展示了结果。
- en: '[PRE16]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![output 44 0](assets/output_44_0.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![output 44 0](assets/output_44_0.png)'
- en: Figure 45-11\. Digits “denoised” using PCA
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图45-11\. 使用 PCA 进行“去噪”处理的数字
- en: This signal preserving/noise filtering property makes PCA a very useful feature
    selection routine—for example, rather than training a classifier on very high-dimensional
    data, you might instead train the classifier on the lower-dimensional principal
    component representation, which will automatically serve to filter out random
    noise in the inputs.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种信号保留/噪声过滤特性使得 PCA 成为非常有用的特征选择例程——例如，不是在非常高维度的数据上训练分类器，而是在较低维度的主成分表示上训练分类器，这将自动过滤输入中的随机噪声。
- en: 'Example: Eigenfaces'
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：特征脸
- en: 'Earlier we explored an example of using a PCA projection as a feature selector
    for facial recognition with a support vector machine (see [Chapter 43](ch43.xhtml#section-0507-support-vector-machines)).
    Here we will take a look back and explore a bit more of what went into that. Recall
    that we were using the Labeled Faces in the Wild (LFW) dataset made available
    through Scikit-Learn:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前探讨了使用 PCA 技术作为支持向量机的特征选择器进行人脸识别的示例（见[第43章](ch43.xhtml#section-0507-support-vector-machines)）。现在让我们回顾一下，并深入探讨这背后的更多内容。回想一下，我们使用的是由
    Scikit-Learn 提供的 Labeled Faces in the Wild（LFW）数据集：
- en: '[PRE17]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let’s take a look at the principal axes that span this dataset. Because this
    is a large dataset, we will use the `"random"` eigensolver in the `PCA` estimator:
    it uses a randomized method to approximate the first <math alttext="upper N"><mi>N</mi></math>
    principal components more quickly than the standard approach, at the expense of
    some accuracy. This trade-off can be useful for high-dimensional data (here, a
    dimensionality of nearly 3,000). We will take a look at the first 150 components:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看涵盖此数据集的主轴。由于这是一个大数据集，我们将在`PCA`估计器中使用`"random"`特征求解器：它使用随机方法来更快地近似前<math
    alttext="upper N"><mi>N</mi></math>个主成分，而不是标准方法，以牺牲一些准确性。这种权衡在高维数据（这里接近3,000维）中非常有用。我们将看一看前150个成分：
- en: '[PRE18]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In this case, it can be interesting to visualize the images associated with
    the first several principal components (these components are technically known
    as *eigenvectors*, so these types of images are often called *eigenfaces*; as
    you can see in [Figure 45-12](#fig_0509-principal-component-analysis_files_in_output_51_0),
    they are as creepy as they sound):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，可以通过可视化与前几个主成分相关联的图像来进行探索（这些成分在技术上称为*特征向量*，因此这些类型的图像通常被称为*特征脸*；正如您在[图45-12](#fig_0509-principal-component-analysis_files_in_output_51_0)中所见，它们听起来就像它们看起来那样可怕）：
- en: '[PRE19]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![output 51 0](assets/output_51_0.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![output 51 0](assets/output_51_0.png)'
- en: Figure 45-12\. A visualization of eigenfaces learned from the LFW dataset
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图45-12\. 从 LFW 数据集学习的特征脸的可视化
- en: 'The results are very interesting, and give us insight into how the images vary:
    for example, the first few eigenfaces (from the top left) seem to be associated
    with the angle of lighting on the face, and later principal vectors seem to be
    picking out certain features, such as eyes, noses, and lips. Let’s take a look
    at the cumulative variance of these components to see how much of the data information
    the projection is preserving (see [Figure 45-13](#fig_0509-principal-component-analysis_files_in_output_53_0)).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 结果非常有趣，并且为我们提供了关于图像变化的见解：例如，前几个特征脸（左上角）似乎与脸部的光照角度有关，而后来的主成分似乎在挑选出特定的特征，如眼睛、鼻子和嘴唇。让我们看一看这些成分的累计方差，以查看投影保留了多少数据信息（见[图45-13](#fig_0509-principal-component-analysis_files_in_output_53_0)）。
- en: '[PRE20]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![output 53 0](assets/output_53_0.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![output 53 0](assets/output_53_0.png)'
- en: Figure 45-13\. Cumulative explained variance for the LFW data
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图45-13\. LFW 数据的累计解释方差
- en: The 150 components we have chosen account for just over 90% of the variance.
    That would lead us to believe that using these 150 components, we would recover
    most of the essential characteristics of the data. To make this more concrete,
    we can compare the input images with the images reconstructed from these 150 components
    (see [Figure 45-14](#fig_0509-principal-component-analysis_files_in_output_56_0)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择的150个组件占了超过90%的方差。这使我们相信，使用这150个组件，我们将恢复数据的大部分基本特征。为了更具体化，我们可以将输入图像与从这些150个组件重建的图像进行比较（参见[图
    45-14](#fig_0509-principal-component-analysis_files_in_output_56_0)）。
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![output 56 0](assets/output_56_0.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![output 56 0](assets/output_56_0.png)'
- en: Figure 45-14\. 150-dimensional PCA reconstruction of the LFW data
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 45-14\. LFW 数据的150维 PCA 重建
- en: 'The top row here shows the input images, while the bottom row shows the reconstruction
    of the images from just 150 of the ~3,000 initial features. This visualization
    makes clear why the PCA feature selection used in [Chapter 43](ch43.xhtml#section-0507-support-vector-machines)
    was so successful: although it reduces the dimensionality of the data by nearly
    a factor of 20, the projected images contain enough information that we might,
    by eye, recognize the individuals in each image. This means our classification
    algorithm only needs to be trained on 150-dimensional data rather than 3,000-dimensional
    data, which, depending on the particular algorithm we choose, can lead to much
    more efficient classification.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的顶部行显示了输入图像，而底部行显示了仅从约3000个初始特征中的150个进行图像重建。这种可视化清楚地说明了PCA特征选择在[第43章](ch43.xhtml#section-0507-support-vector-machines)中为何如此成功：虽然它将数据的维度减少了近20倍，但投影图像包含足够的信息，使我们可以通过肉眼识别每个图像中的个体。这意味着我们的分类算法只需要在150维数据上进行训练，而不是3000维数据，根据我们选择的特定算法，这可能会导致更高效的分类。
- en: Summary
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter we explored the use of principal component analysis for dimensionality
    reduction, visualization of high-dimensional data, noise filtering, and feature
    selection within high-dimensional data. Because of its versatility and interpretability,
    PCA has been shown to be effective in a wide variety of contexts and disciplines.
    Given any high-dimensional dataset, I tend to start with PCA in order to visualize
    the relationships between points (as we did with the digits data), to understand
    the main variance in the data (as we did with the eigenfaces), and to understand
    the intrinsic dimensionality (by plotting the explained variance ratio). Certainly
    PCA is not useful for every high-dimensional dataset, but it offers a straightforward
    and efficient path to gaining insight into high-dimensional data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了主成分分析在降维、高维数据可视化、噪声过滤和高维数据特征选择中的应用。由于其多功能性和可解释性，PCA已被证明在各种背景和学科中都非常有效。对于任何高维数据集，我倾向于从PCA开始，以便可视化数据点之间的关系（就像我们在数字数据中所做的那样），理解数据中的主要方差（就像我们在特征脸中所做的那样），并理解内在的维度（通过绘制解释方差比）。当然，PCA并非对每个高维数据集都有用，但它提供了一条直观和高效的路径，以洞察高维数据。
- en: PCA’s main weakness is that it tends to be highly affected by outliers in the
    data. For this reason, several robust variants of PCA have been developed, many
    of which act to iteratively discard data points that are poorly described by the
    initial components. Scikit-Learn includes a number of interesting variants on
    PCA in the `sklearn​.decom⁠position` submodule; one example is `SparsePCA`, which
    introduces a regularization term (see [Chapter 42](ch42.xhtml#section-0506-linear-regression))
    that serves to enforce sparsity of the components.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的主要弱点是它往往受到数据中异常值的影响。因此，已经开发了几种鲁棒性较强的PCA变体，其中许多变体通过迭代地丢弃初始组件描述不佳的数据点来作用。Scikit-Learn在`sklearn​.decom⁠position`子模块中包括了许多有趣的PCA变体；一个例子是`SparsePCA`，它引入了一个正则化项（参见[第42章](ch42.xhtml#section-0506-linear-regression)），用于强制组件的稀疏性。
- en: In the following chapters, we will look at other unsupervised learning methods
    that build on some of the ideas of PCA.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将研究其他建立在PCA思想基础上的无监督学习方法。
- en: ^([1](ch45.xhtml#idm45858728565136-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/VmpjC).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch45.xhtml#idm45858728565136-marker)) 可在[在线附录](https://oreil.ly/VmpjC)中找到生成此图的代码。
- en: ^([2](ch45.xhtml#idm45858728053776-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/ixfc1).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch45.xhtml#idm45858728053776-marker)) 可在[在线附录](https://oreil.ly/ixfc1)中找到生成此图的代码。
- en: ^([3](ch45.xhtml#idm45858728025120-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/WSe0T).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch45.xhtml#idm45858728025120-marker)) 生成此图的代码可以在[在线附录](https://oreil.ly/WSe0T)中找到。
