- en: 7 Project overview and system architecture
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 项目概述和系统架构
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Providing a high-level overall design of our system
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供我们系统的整体高级设计
- en: Optimizing the data ingestion component for multiple epochs of the dataset
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化数据摄取组件以处理数据集的多个epoch
- en: Deciding which distributed model training strategy best minimizes overhead
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定哪种分布式模型训练策略最能最小化开销
- en: Adding model server replicas for high-performance model serving
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为高性能模型服务添加模型服务器副本
- en: Accelerating the end-to-end workflow of our machine learning system
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加速我们机器学习系统的端到端工作流程
- en: In the previous chapters, we learned to choose and apply the correct patterns
    for building and deploying distributed machine learning systems to gain practical
    experience managing and automating machine learning tasks. In chapter 2, I introduced
    a couple of practical patterns that can be incorporated into data ingestion, usually
    the first process of a distributed machine learning system and responsible for
    monitoring incoming data and performing necessary preprocessing steps to prepare
    for model training.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了如何选择和应用正确的模式来构建和部署分布式机器学习系统，以获得实际经验来管理和自动化机器学习任务。在第2章中，我介绍了一些可以融入数据摄取的实用模式，通常这是分布式机器学习系统的第一个过程，负责监控传入的数据并执行必要的预处理步骤以准备模型训练。
- en: In chapter 3, we explored some challenges dealing with the distributed training
    component, and I introduced a couple of practical patterns that can be incorporated
    into the component. The distributed training component is the most critical part
    of a distributed machine learning system and is what makes the system unique from
    general distributed systems. In chapter 4, we covered the challenges involved
    in distributed model serving systems, and I introduced a few commonly used patterns.
    You can use replicated services to achieve horizontal scaling and the sharded
    services pattern to process large model serving requests. You also learned how
    to assess model serving systems and determine whether the event-driven design
    is beneficial in real-world scenarios.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我们探讨了处理分布式训练组件的一些挑战，并介绍了一些可以融入该组件的实用模式。分布式训练组件是分布式机器学习系统中最关键的部分，也是使系统区别于一般分布式系统的原因。在第4章中，我们涵盖了分布式模型服务系统中涉及到的挑战，并介绍了一些常用的模式。你可以使用副本服务来实现水平扩展，使用分片服务模式来处理大量的模型服务请求。你还学习了如何评估模型服务系统，并确定在现实场景中事件驱动设计是否有益。
- en: In chapter 5, we discussed machine learning workflows, one of the most essential
    components in machine learning systems, as it connects all other components in
    a machine learning system. Finally, in chapter 6, we discussed some operational
    efforts and patterns that can greatly accelerate the end-to-end workflow and reduce
    maintenance and communication efforts when engineering teams collaborate with
    teams of data scientists or machine learning practitioners before the systems
    become production ready.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章中，我们讨论了机器学习工作流，这是机器学习系统中最基本的部分之一，因为它将机器学习系统中的所有其他组件连接起来。最后，在第6章中，我们讨论了一些可以极大地加速端到端工作流程并减少在系统成为生产就绪之前工程团队与数据科学家或机器学习实践者合作时的维护和沟通努力的运营努力和模式。
- en: For the remaining chapters of the book, we will build an end-to-end machine
    learning system to apply what we learned previously. You will gain hands-on experience
    implementing many patterns we’ve previously discussed. You’ll learn how to solve
    problems at a larger scale and take what you’ve developed on your laptop to large
    distributed clusters. In this chapter, we’ll go through the project background
    and system components. Then we’ll go through the challenges related to the components
    and discuss the patterns we can apply to address them.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的剩余章节中，我们将构建一个端到端机器学习系统来应用之前学到的知识。你将获得实际操作经验来实现我们之前讨论的许多模式。你将学习如何在大规模上解决问题，并将你在笔记本电脑上开发的内容扩展到大型分布式集群。在本章中，我们将介绍项目背景和系统组件。然后，我们将讨论与组件相关的挑战，并讨论我们可以应用以解决这些挑战的模式。
- en: Note that although we won’t dive into the implementation details in this chapter,
    in the remaining chapters, we’ll use several popular frameworks and cutting-edge
    technologies—particularly TensorFlow, Kubernetes, Kubeflow, Docker, and Argo Workflows—to
    build the components of a distributed machine learning workflow.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管我们不会在本章深入探讨实现细节，但在接下来的章节中，我们将使用几个流行的框架和前沿技术——特别是TensorFlow、Kubernetes、Kubeflow、Docker和Argo
    Workflows——来构建分布式机器学习工作流程的组件。
- en: 7.1 Project overview
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 项目概述
- en: For this project, we will build an image classification system that takes raw
    images downloaded from the data source, performs necessary data cleaning steps,
    builds a machine learning model in a distributed Kubernetes cluster, and then
    deploys the trained model to the model serving system for users to use. We also
    want to establish an end-to-end workflow that is efficient and reusable. Next,
    I will introduce the project background and the overall system architecture and
    components.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我们将构建一个图像分类系统，该系统从数据源下载原始图像，执行必要的数据清理步骤，在分布式Kubernetes集群中构建机器学习模型，然后将训练好的模型部署到模型服务系统中供用户使用。我们还希望建立一个高效且可重用的端到端工作流程。接下来，我将介绍项目背景、整体系统架构和组件。
- en: 7.1.1 Project background
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 项目背景
- en: We will build an end-to-end machine learning system to apply what we learned
    previously. We’ll build a data ingestion component that downloads the Fashion-MNIST
    dataset and a model training component to train and optimize the image classification
    model. Once the final model is trained, we’ll build a high-performance model serving
    system to start making predictions using the trained model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个端到端的机器学习系统来应用我们之前学到的知识。我们将构建一个数据摄取组件，用于下载Fashion-MNIST数据集，并构建一个模型训练组件来训练和优化图像分类模型。一旦最终模型训练完成，我们将构建一个高性能的模型服务系统，开始使用训练好的模型进行预测。
- en: As previously mentioned, we will use several frameworks and technologies to
    build distributed machine learning workflow components. For example, we’ll use
    TensorFlow with Python to build the classification model on the Fashion-MNIST
    dataset and make predictions. We’ll use Kubeflow to run distributed machine learning
    model training on a Kubernetes cluster. Furthermore, we’ll use Argo Workflows
    to build a machine learning pipeline that consists of many important components
    of a distributed machine learning system. The basics of these technologies will
    be introduced in the next chapter, and you’ll gain hands-on experience with them
    before diving into the actual implementation of the project in chapter 9\. In
    the next section, we’ll examine the project’s system components.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将使用几个框架和技术来构建分布式机器学习工作流程组件。例如，我们将使用TensorFlow和Python在Fashion-MNIST数据集上构建分类模型并进行预测。我们将使用Kubeflow在Kubernetes集群上运行分布式机器学习模型训练。此外，我们将使用Argo
    Workflows构建一个由分布式机器学习系统的重要组件组成的机器学习管道。这些技术的基础知识将在下一章中介绍，你将在实际项目实现之前通过它们获得实践经验。在下一节中，我们将检查项目的系统组件。
- en: 7.1.2 System components
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 系统组件
- en: Figure 7.1 is the architecture diagram of the system we will be building. First,
    we will build the data ingestion component responsible for ingesting data and
    storing the dataset in the cache using some of the patterns discussed in chapter
    2\. Next, we will build three different model training steps that train different
    models and incorporate the collective communication pattern addressed in chapter
    3. Once we finish the model training steps, we will build the model selection
    step that picks the top model. The selected optimal model will be used for model
    serving in the following two steps. At the end of the model serving steps, we
    aggregate the predictions and present the result to users. Finally, we want to
    ensure all these steps are part of a reproducible workflow that can be executed
    at any time in any environment.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1是我们将要构建的系统架构图。首先，我们将构建一个数据摄取组件，负责摄取数据并将数据集存储在缓存中，使用第2章中讨论的一些模式。接下来，我们将构建三个不同的模型训练步骤，分别训练不同的模型，并纳入第3章中讨论的集体通信模式。完成模型训练步骤后，我们将构建模型选择步骤，选择最佳模型。选定的最优模型将在接下来的两个步骤中用于模型服务。在模型服务步骤结束时，我们将汇总预测结果并向用户展示。最后，我们希望确保所有这些步骤都是可重复的工作流程的一部分，可以在任何时间、任何环境中执行。
- en: We’ll build the system based on the architecture diagram in Figure 7.1 and dive
    into the details of the individual components. We’ll also discuss the patterns
    we can use to address the challenges in building those components.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将根据图7.1中的架构图来构建系统，并深入探讨各个组件的细节。我们还将讨论我们可以使用的模式来解决构建这些组件的挑战。
- en: '![07-01](../../OEBPS/Images/07-01.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![07-01](../../OEBPS/Images/07-01.png)'
- en: Figure 7.1 The architecture diagram of the end-to-end machine learning system
    we will be building
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 我们将要构建的端到端机器学习系统的架构图
- en: 7.2 Data ingestion
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 数据摄取
- en: For this project, we will use the Fashion-MNIST dataset, introduced in section
    2.2, to build the data ingestion component, as shown in figure 7.2\. This dataset
    consists of a training set of 60,000 examples and a test set of 10,000 examples.
    Each example is a 28 × 28 grayscale image that represents one Zalando’s article
    image associated with a label from 10 classes. Recall that the Fashion-MNIST dataset
    is designed to serve as a direct drop-in replacement for the original MNIST dataset
    for benchmarking machine learning algorithms. It shares the same image size and
    structure of training and testing splits.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我们将使用第2.2节中介绍的Fashion-MNIST数据集来构建数据摄取组件，如图7.2所示。此数据集包含一个包含60,000个示例的训练集和一个包含10,000个示例的测试集。每个示例是一个28
    × 28的灰度图像，代表与10个类别标签之一相关的Zalando文章图像。回想一下，Fashion-MNIST数据集是为了作为原始MNIST数据集的直接替换而设计的，用于基准测试机器学习算法。它共享相同的图像大小和训练/测试分割的结构。
- en: '![07-02](../../OEBPS/Images/07-02.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![07-02](../../OEBPS/Images/07-02.png)'
- en: Figure 7.2 The data ingestion component (dark box) in the end-to-end machine
    learning system
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 端到端机器学习系统中的数据摄取组件（深色框）
- en: As a recap, figure 7.3 is a screenshot of the collection of images for all 10
    classes (T-shirt/top, trouser, pullover, dress, coat, sandal, shirt, sneaker,
    bag, and ankle boot) from Fashion-MNIST, where each class takes three rows in
    the screenshot.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 作为回顾，图7.3是Fashion-MNIST中所有10个类别（T恤/上衣、裤子、开衫、连衣裙、外套、凉鞋、衬衫、运动鞋、包和踝靴）的图像集合的截图，其中每个类别在截图中占三行。
- en: '![07-03](../../OEBPS/Images/07-03.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![07-03](../../OEBPS/Images/07-03.png)'
- en: Figure 7.3 A screenshot of the collection of images from the Fashion-MNIST dataset
    for all 10 classes (T-shirt/top, trouser, pullover, dress, coat, sandal, shirt,
    sneaker, bag, and ankle boot)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 Fashion-MNIST数据集中所有10个类别（T恤/上衣、裤子、开衫、连衣裙、外套、凉鞋、衬衫、运动鞋、包和踝靴）的图像集合的截图
- en: Figure 7.4 is a closer look at the first few example images in the training
    set together with their corresponding text labels.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4是训练集中前几个示例图像的近距离观察，以及它们对应的文本标签。
- en: '![07-04](../../OEBPS/Images/07-04.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![07-04](../../OEBPS/Images/07-04.png)'
- en: Figure 7.4 A closer look at the first few example images in the training set
    with their corresponding labels in text
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 训练集中前几个示例图像的近距离观察，以及它们对应的文本标签
- en: The downloaded Fashion-MNIST dataset should only take 30 MBs on disk if compressed.
    It’s easy to load the entire downloaded dataset into memory at once.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果压缩，下载的Fashion-MNIST数据集在磁盘上应该只占用30 MB。一次性将整个下载的数据集加载到内存中很容易。
- en: 7.2.1 The problem
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 问题
- en: Although the Fashion-MNIST data is not large, we may want to perform additional
    computations before feeding the dataset into the model, which is common for tasks
    that require additional transformations and cleaning. We may want to resize, normalize,
    or convert the images to grayscale. We also may want to perform complex mathematical
    operations such as convolution operations, which can require large additional
    memory space allocations. Our available computational resources may or may not
    be sufficient after we load the entire dataset in memory, depending on the distributed
    cluster size.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Fashion-MNIST数据集不大，但在将数据集输入模型之前，我们可能需要进行额外的计算，这对于需要额外转换和清理的任务来说是常见的。我们可能想要调整大小、归一化，或将图像转换为灰度。我们还可能想要执行复杂的数学运算，如卷积运算，这可能需要分配大量的额外内存空间。在将整个数据集加载到内存后，我们的可用计算资源可能足够，也可能不足，这取决于分布式集群的大小。
- en: In addition, the machine learning model we are training from this dataset requires
    multiple epochs on the training dataset. Suppose training one epoch on the entire
    training dataset takes 3 hours. If we want to train two epochs, the time needed
    for model training would double, as shown in figure 7.5.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们从该数据集训练的机器学习模型需要在训练数据集上运行多个epoch。假设在整个训练数据集上训练一个epoch需要3小时。如果我们想训练两个epoch，模型训练所需的时间将翻倍，如图7.5所示。
- en: '![07-05](../../OEBPS/Images/07-05.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![07-05](../../OEBPS/Images/07-05.png)'
- en: Figure 7.5 A diagram of model training for multiple epochs at time t0, t1, etc.
    where we spent 3 hours for each epoch
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 在时间t0、t1等处进行多个epoch的模型训练的示意图，我们每个epoch花费了3小时
- en: In real-world machine learning systems, a larger number of epochs is often needed,
    and training each epoch sequentially is inefficient. In the next section, we will
    discuss how we can tackle that inefficiency.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的机器学习系统中，通常需要更多的epoch，并且依次训练每个epoch效率低下。在下一节中，我们将讨论如何解决这种低效问题。
- en: 7.2.2 The solution
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 解决方案
- en: 'Let’s take a look at the first challenge we have: the mathematical operations
    in the machine learning algorithms may require a lot of additional memory space
    allocations while computational resources may or may not be sufficient. Given
    that we don’t have too much free memory, we should not load the entire Fashion-MNIST
    dataset into memory directly. Let’s assume that the mathematical operations that
    we want to perform on the dataset can be performed on subsets of the entire dataset.
    Then, we could use the batching pattern introduced in chapter 2, which would group
    a number of data records from the entire dataset into batches, which will be used
    to train the machine learning model sequentially on each batch.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们面临的第一大挑战：在机器学习算法中的数学运算可能需要大量的额外内存空间分配，而计算资源可能或可能不足。鉴于我们没有太多的空闲内存，我们不应该直接将整个Fashion-MNIST数据集加载到内存中。假设我们想要在数据集上执行的数学运算可以在整个数据集的子集上执行。那么，我们可以使用第2章中引入的批处理模式，该模式将整个数据集的一定数量的数据记录分组到批次中，这些批次将被用来依次在每个批次上训练机器学习模型。
- en: To apply the batching pattern, we first divide the dataset into smaller subsets
    or mini-batches, load each individual mini-batch of example images, perform expensive
    mathematical operations on each batch, and then use only one mini-batch of images
    in each model training iteration. For example, we can perform convolution or other
    heavy mathematical operations on the first mini-batch, which consists of only
    20 images, and then send the transformed images to the machine learning model
    for model training. We then repeat the same process for the remaining mini-batches
    while continuing to perform model training.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用批处理模式，我们首先将数据集分成更小的子集或小批量，加载每个单独的小批量示例图像，对每个批次执行昂贵的数学运算，然后在每个模型训练迭代中只使用一个小批量图像。例如，我们可以对只包含20个图像的第一个小批量执行卷积或其他复杂的数学运算，然后将转换后的图像发送到机器学习模型进行模型训练。然后，我们重复相同的流程对剩余的小批量进行操作，同时继续进行模型训练。
- en: Since we’ve divided the dataset into many small subsets (mini-batches), we can
    avoid any potential problems with running out of memory when performing various
    heavy mathematical operations on the entire dataset necessary for achieving an
    accurate classification model on the Fashion-MNIST dataset. We can then handle
    even larger datasets using this approach by reducing the size of the mini-batches.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经将数据集分成许多小子集（小批量），我们可以在对整个数据集执行必要的各种复杂数学运算以实现Fashion-MNIST数据集上的准确分类模型时避免任何潜在的内存不足问题。然后，我们可以通过减小小批量的大小来使用这种方法处理更大的数据集。
- en: With the help of the batching pattern, we are no longer concerned about potential
    out-of-memory problems when ingesting the dataset for model training. We don’t
    have to load the entire dataset into memory at once, and instead, we are consuming
    the dataset batch by batch sequentially. For example, if we have a dataset with
    1,000 records, we can first take 500 of the 1,000 records to form a batch and
    then train the model using this batch of records. Subsequently, we can repeat
    this batching and model training process for the remaining records. Figure 7.6
    illustrates this process, where the original dataset gets divided into two batches
    and processed sequentially. The first batch gets consumed to train the model at
    time t0, and the second batch gets consumed at time t1.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在批处理模式的帮助下，我们不再担心在数据集导入模型训练时可能出现的内存不足问题。我们不必一次性将整个数据集加载到内存中，而是按批次顺序消耗数据集。例如，如果我们有一个包含1,000条记录的数据集，我们可以首先取500条记录形成一个批次，然后使用这个批次记录来训练模型。随后，我们可以对剩余的记录重复此批处理和模型训练过程。图7.6说明了这个过程，其中原始数据集被分成两个批次并依次处理。第一个批次在时间t0被消耗以训练模型，第二个批次在时间t1被消耗。
- en: '![07-06](../../OEBPS/Images/07-06.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![07-06](../../OEBPS/Images/07-06.png)'
- en: Figure 7.6 The dataset is divided into two batches and processed sequentially.
    The first batch is consumed to train the model at time t0, and the second batch
    is consumed at time t1.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 数据集被分为两个批次并依次处理。第一个批次在时间t0被消耗以训练模型，第二个批次在时间t1被消耗。
- en: 'Now, let’s tackle the second challenge mentioned in section 7.2.1: we want
    to avoid wasting time if we need to train a machine learning model that involves
    iterating on multiple epochs of the original dataset. Recall that, in chapter
    2, we talked about the caching pattern, which would solve this type of problem.
    With the help of the caching pattern, we can greatly speed up the re-access to
    the dataset for the model training process that involves training on the same
    dataset for multiple epochs.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们解决7.2.1节中提到的第二个挑战：如果我们需要训练一个涉及迭代原始数据集多个迭代的机器学习模型，我们希望避免浪费时间。回想一下，在第2章中，我们讨论了缓存模式，这将解决这类问题。借助缓存模式，我们可以极大地加快涉及在相同数据集上多次训练的模型训练过程的重新访问数据集的速度。
- en: We can’t do anything special to the first epoch since it’s the first time the
    machine learning model has seen the entire training dataset. We can store the
    cache of the training examples in memory, making it much faster to re-access when
    needed for the second and subsequent epochs.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一次迭代，我们无法做任何特殊的事情，因为这是机器学习模型第一次看到整个训练数据集。我们可以将训练示例的缓存存储在内存中，使其在需要时重新访问时更快。
- en: Let’s assume that the single laptop we use to train the model has sufficient
    computational resources such as memory and disk space. As soon as the machine
    learning model consumes each training example from the entire dataset, we can
    hold off recycling and instead keep the consumed training examples in memory.
    For example, in figure 7.7, after we have finished fitting the model for the first
    epoch, we can store a cache for both batches used for the first epoch of model
    training.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们用来训练模型的单个笔记本电脑具有足够的计算资源，例如内存和磁盘空间。一旦机器学习模型消耗了整个数据集的每个训练示例，我们就可以推迟回收，而是将消耗的训练示例保留在内存中。例如，在图7.7中，在我们完成第一次迭代的模型拟合后，我们可以为第一次模型训练使用的两个批次存储缓存。
- en: '![07-07](../../OEBPS/Images/07-07.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![07-07](../../OEBPS/Images/07-07.png)'
- en: Figure 7.7 A diagram of model training for multiple epochs at time t0, t1, etc.
    using cache, making reading from the data source repeatedly unnecessary
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 使用缓存在时间t0、t1等多次迭代中训练模型的示意图，使得从数据源反复读取变得不必要
- en: Then, we can start training the model for the second epoch by feeding the stored
    in-memory cache to the model directly without repeatedly reading from the data
    source for future epochs. Next, we will discuss the model training component we
    will build in our project.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过直接向模型提供存储在内存中的缓存来开始第二次迭代的模型训练，而不需要为未来的迭代从数据源反复读取。接下来，我们将讨论我们将在项目中构建的模型训练组件。
- en: 7.2.3 Exercises
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 练习
- en: Where do we store the cache?
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在哪里存储缓存？
- en: Can we use the batching pattern when the Fashion-MNIST dataset gets large?
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当Fashion-MNIST数据集变得很大时，我们能否使用批处理模式？
- en: 7.3 Model training
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 模型训练
- en: In the previous section, we’ve talked about the data ingestion component of
    the system we are building and how we can use the caching and batching pattern
    to handle large datasets and make the system more efficient. Next, let’s discuss
    the model training component we are building. Figure 7.8 is a diagram of the model
    training component in the overall architecture.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了我们正在构建的系统中的数据摄取组件，以及我们如何使用缓存和批处理模式来处理大型数据集并使系统更高效。接下来，让我们讨论我们正在构建的模型训练组件。图7.8是整体架构中模型训练组件的示意图。
- en: '![07-08](../../OEBPS/Images/07-08.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![07-08](../../OEBPS/Images/07-08.png)'
- en: Figure 7.8 The model training component (dark boxes) in the end-to-end machine
    learning system
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 端到端机器学习系统中的模型训练组件（深色框）
- en: In the diagram, three different model training steps are followed by a model
    selection step. These model training steps can train three different models competing
    with each other for better statistical performance. The dedicated model selection
    step then picks the top model, which will be used in the subsequent components
    in the end-to-end machine learning workflow.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，三个不同的模型训练步骤之后跟着一个模型选择步骤。这些模型训练步骤可以训练三个不同的模型，它们相互竞争以获得更好的统计性能。专门的模型选择步骤随后选择最佳模型，该模型最终将在端到端机器学习工作流程的后续组件中使用。
- en: In the next section, we will look more closely at the model training component
    in figure 7.8 and discuss potential problems when implementing this component.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将更仔细地查看图7.8中的模型训练组件，并讨论实现此组件时可能遇到的问题。
- en: 7.3.1 The problem
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 问题
- en: In chapter 3, I introduced the parameter server and the collective communication
    patterns. The parameter server pattern is handy when the model is too large to
    fit in a single machine, such as the one for tagging entities in the 8 million
    YouTube videos (section 3.2). The collective communication pattern is useful to
    speed up the training process for medium-sized models when the communication overhead
    is significant. Which pattern should we select for our model training component?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我介绍了参数服务器和集体通信模式。当模型太大而无法适应单台机器时，参数服务器模式很有用，例如用于标记800万YouTube视频中的实体（第3.2节）。集体通信模式在通信开销显著时，有助于加速中等规模模型的训练过程。我们应该为我们的模型训练组件选择哪种模式？
- en: 7.3.2 The solution
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 解决方案
- en: With the help of parameter servers, we can effectively resolve the challenge
    of building an extremely large machine learning model that may not fit a single
    machine. Even when the model is too large to fit in a single machine, we can still
    successfully train the model efficiently with parameter servers. For example,
    figure 7.9 is an architecture diagram of the parameter server pattern using multiple
    parameter servers. Each worker node takes a subset of the dataset, performs calculations
    required in each neural network layer, and sends the calculated gradients to update
    one model partition stored in one of the parameter servers.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过参数服务器的帮助，我们可以有效地解决构建可能不适合单台机器的极大型机器学习模型的挑战。即使模型太大而无法适应单台机器，我们仍然可以使用参数服务器高效地成功训练模型。例如，图7.9是使用多个参数服务器的参数服务器模式架构图。每个工作者节点处理数据集的一个子集，执行每个神经网络层所需的计算，并将计算出的梯度发送到更新存储在参数服务器中的一个模型分区。
- en: '![07-09](../../OEBPS/Images/07-09.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![07-09](../../OEBPS/Images/07-09.png)'
- en: Figure 7.9 A machine learning training component with multiple parameter servers
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 多参数服务器机器学习训练组件
- en: Because all workers perform calculations in an asynchronous fashion, the model
    partitions each worker node uses to calculate the gradients may not be up to date.
    For instance, two workers can block each other when sending gradients to the same
    parameter server, which makes it hard to gather the calculated gradients on time
    and requires a strategy to resolve the blocking problem. Unfortunately, in real-world
    distributed training systems where parameter servers are incorporated, multiple
    workers may send the gradients at the same time, and thus many blocking communications
    must be resolved.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有工作者都以异步方式执行计算，模型分区的每个工作者节点用于计算梯度的分区可能不是最新的。例如，两个工作者在向同一个参数服务器发送梯度时可能会互相阻塞，这使得及时收集计算出的梯度变得困难，并需要一种策略来解决阻塞问题。不幸的是，在包含参数服务器的现实世界分布式训练系统中，多个工作者可能同时发送梯度，因此必须解决许多阻塞通信。
- en: Another challenge comes when deciding the optimal ratio between the number of
    workers and the number of parameter servers. For example, many workers are sending
    gradients to the same parameter server at the same time; the problem gets even
    worse, and eventually, the blocking communications between different workers or
    parameter servers become a bottleneck.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当决定工作者数量与参数服务器数量之间的最佳比例时，又出现了一个挑战。例如，许多工作者同时向同一个参数服务器发送梯度；问题变得更加严重，最终，不同工作者或参数服务器之间的阻塞通信成为瓶颈。
- en: Now, let’s return to our original application, the Fashion-MNIST classification
    model. The model we are building is not as large as large recommendation system
    models; it can easily fit in a single machine if we give the machine sufficient
    computational resources. It’s only 30 MBs in compressed form. Thus, the collective
    communication model is perfect for the system we are building.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到我们的原始应用，即Fashion-MNIST分类模型。我们正在构建的模型并不像大型推荐系统模型那样大；如果我们给机器足够的计算资源，它就可以轻松地适应单台机器。它以压缩形式只有30
    MB。因此，集体通信模型非常适合我们正在构建的系统。
- en: Now, without parameter servers, each worker node stores a copy of the entire
    set of model parameters, as shown in figure 7.10\. I previously mentioned that
    every worker consumes some portion of data and calculates the gradients needed
    to update the model parameters stored locally on this worker node (see chapter
    3). We want to aggregate all the gradients as soon as all worker nodes have successfully
    completed their calculation of gradients. We also want to make sure every worker’s
    entire set of model parameters is updated based on the aggregated gradients. In
    other words, each worker should store a copy of the exact same updated model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，没有参数服务器的情况下，每个工作者节点存储整个模型参数集的副本，如图7.10所示。我之前提到，每个工作者消费部分数据并计算更新存储在本工作者节点上的模型参数所需的梯度（见第3章）。我们希望在所有工作者节点成功完成梯度计算后立即聚合所有梯度。我们还想确保每个工作者的整个模型参数集基于聚合的梯度进行更新。换句话说，每个工作者应该存储一个与更新后的模型完全相同的副本。
- en: '![07-10](../../OEBPS/Images/07-10.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![07-10](../../OEBPS/Images/07-10.png)'
- en: Figure 7.10 Distributed model training component with only worker nodes, where
    every worker stores a copy of the entire set of model parameters and consumes
    partitions of data to calculate the gradients
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10展示了仅包含工作者节点的分布式模型训练组件，其中每个工作者存储整个模型参数集的副本，并消费数据分区来计算梯度
- en: Going back to the architecture diagram in figure 7.8, each model training step
    uses the collective communication pattern, taking advantage of the underlying
    network infrastructure to perform allreduce operations to communicate gradients
    between multiple workers. The collective communication pattern also allows us
    to train multiple medium-sized machine learning models in a distributed setting.
    Once the model is trained, we can start a separate process to pick the top model
    to be used for model serving. This step is pretty intuitive, and I’ll defer the
    implementation details to chapter 9\. In the next section, we will discuss the
    model serving component of our system.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 回到图7.8中的架构图，每个模型训练步骤都使用集体通信模式，利用底层网络基础设施执行allreduce操作，在多个工作者之间通信梯度。集体通信模式还允许我们在分布式环境中训练多个中等规模的机器学习模型。一旦模型训练完成，我们可以启动一个单独的过程来挑选出将被用于模型服务的最佳模型。这一步骤相当直观，我将把实现细节推迟到第9章。在下一节中，我们将讨论系统中的模型服务组件。
- en: 7.3.3 Exercises
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.3 练习
- en: Why isn’t the parameter server pattern a good fit for our model?
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么参数服务器模式不适合我们的模型？
- en: Does each worker store different parts of the model when using the collective
    communication pattern?
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用集体通信模式时，每个工作者是否存储模型的不同部分？
- en: 7.4 Model serving
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 模型服务
- en: We’ve talked about both the data ingestion and model training components of
    the system we are building. Next, let’s discuss the model server component, which
    is essential to the end-user experience. Figure 7.11 shows the serving training
    component in the overall architecture.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了我们正在构建的系统中的数据摄取和模型训练组件。接下来，让我们讨论模型服务器组件，这对于最终用户体验至关重要。图7.11显示了整体架构中的模型服务训练组件。
- en: '![07-11](../../OEBPS/Images/07-11.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![07-11](../../OEBPS/Images/07-11.png)'
- en: Figure 7.11 The model serving component (dark boxes) in the end-to-end machine
    learning system
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11展示了端到端机器学习系统中的模型服务组件（深色框）
- en: Next, let’s take a look at a potential problem and its solution we will encounter
    when we begin building this component.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看在开始构建此组件时可能会遇到的一个潜在问题和其解决方案。
- en: 7.4.1 The problem
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 问题
- en: The model serving system needs to take raw images uploaded by users and send
    the requests to the model server to make inferences using the trained model. These
    model serving requests are being queued and waiting to be processed by the model
    server.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务系统需要接收用户上传的原始图像，并将请求发送到模型服务器，使用训练好的模型进行推理。这些模型服务请求正在排队等待模型服务器处理。
- en: If the model serving system is a single-node server, it can only serve a limited
    number of model serving requests on a first-come, first-served basis. As the number
    of requests grows in the real world, the user experience suffers when users must
    wait a long time to receive the model serving result. In other words, all requests
    are waiting to be processed by the model serving system, but the computational
    resources are limited to this single node. How do we build a more efficient model
    serving system?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型服务系统是一个单节点服务器，它只能基于先到先得的原则服务有限数量的模型服务请求。随着实际应用中请求数量的增长，当用户必须等待很长时间才能收到模型服务结果时，用户体验会受到影响。换句话说，所有请求都在等待被模型服务系统处理，但计算资源仅限于这个单节点。我们如何构建一个更高效的模型服务系统？
- en: 7.4.2 The solution
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 解决方案
- en: The previous section lays a perfect use case for the replicated services pattern
    discussed in chapter 4\. Our model serving system takes the images uploaded by
    users and sends requests to the model server. In addition, unlike the simple single-server
    design, the system has multiple model server replicas to process the model serving
    requests asynchronously. Each model server replica takes a single request, retrieves
    the previously trained classification model from the model training component,
    and classifies the images that don’t existed in the Fashion-MNIST dataset.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节为第4章中讨论的复制服务模式提供了一个完美的用例。我们的模型服务系统接收用户上传的图像并发送请求到模型服务器。此外，与简单的单服务器设计不同，系统具有多个模型服务器副本以异步处理模型服务请求。每个模型服务器副本处理单个请求，从模型训练组件检索先前训练的分类模型，并对请求中不存在于Fashion-MNIST数据集中的图像进行分类。
- en: With the help of the replicated services pattern, we can easily scale up our
    model server by adding model server replicas to the single-server model serving
    system. The new architecture is shown in figure 7.12\. The model server replicas
    can handle many requests at a time since each replica can process individual model
    serving requests independently.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用复制服务模式，我们可以轻松地将模型服务器通过添加模型服务器副本到单服务器模型服务系统中进行扩展。新的架构如图7.12所示。模型服务器副本可以同时处理多个请求，因为每个副本可以独立处理单个模型服务请求。
- en: '![07-12](../../OEBPS/Images/07-12.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![07-12](../../OEBPS/Images/07-12.png)'
- en: Figure 7.12 The system architecture of the replicated model serving services
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 复制模型服务服务的系统架构
- en: Multiple model serving requests from users are sent to the model server replicas
    at the same time after we’ve introduced them. We also need to define a clear mapping
    relationship between the requests and the model server replicas, which determines
    which requests are processed by which of the model server replicas.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在引入它们之后，来自用户的多个模型服务请求同时发送到模型服务器副本。我们还需要定义请求和模型服务器副本之间的明确映射关系，这决定了哪些请求由哪个模型服务器副本处理。
- en: To distribute the model server requests among the replicas, we need to add an
    additional load balancer layer. For example, the load balancer takes multiple
    model serving requests from our users. It then distributes the requests evenly
    among the model server replicas, which are responsible for processing individual
    requests, including model retrieval and inference on the new data in the request.
    Figure 7.13 illustrates this process.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在副本之间分配模型服务器请求，我们需要添加一个额外的负载均衡器层。例如，负载均衡器从我们的用户那里接收多个模型服务请求。然后，它将请求均匀地分配给模型服务器副本，这些副本负责处理单个请求，包括模型检索和请求中的新数据的推理。图7.13说明了这个过程。
- en: '![07-13](../../OEBPS/Images/07-13.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![07-13](../../OEBPS/Images/07-13.png)'
- en: Figure 7.13 A diagram showing how a loader balancer distributes requests evenly
    across the model server replicas
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 展示了负载均衡器如何将请求均匀地分配到模型服务器副本的图表
- en: The load balancer uses different algorithms to determine which request goes
    to which particular model server replica. Example algorithms for load balancing
    include round robin, least-connection method, and hashing.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器使用不同的算法来确定哪个请求发送到哪个特定的模型服务器副本。负载均衡的示例算法包括轮询、最少连接方法和哈希。
- en: Note that from our original architecture diagram in figure 7.11, there are two
    individual steps for model serving, each using different models. Each model serving
    step consists of a model serving service with multiple replicas to handle model
    serving traffic for different models.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，从我们原始的架构图7.11中，模型服务有两个独立的步骤，每个步骤使用不同的模型。每个模型服务步骤由一个模型服务服务及其多个副本组成，以处理不同模型的模型服务流量。
- en: 7.4.3 Exercises
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.3 练习
- en: What happens when we don’t have a load balancer as part of the model serving
    system?
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们没有在模型服务系统中包含负载均衡器时会发生什么？
- en: 7.5 End-to-end workflow
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 端到端工作流
- en: Now that we’ve looked at the individual components, let’s see how to compose
    an end-to-end workflow that consists of all those components in a scalable and
    efficient way. We will also incorporate a few patterns from chapter 5 into the
    workflow. Figure 7.14 is a diagram of the end-to-end workflow we are building.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经研究了单个组件，让我们看看如何以可扩展和高效的方式组合所有这些组件的端到端工作流。我们还将把第5章中的一些模式融入到工作流中。图7.14是我们正在构建的端到端工作流的示意图。
- en: '![07-14](../../OEBPS/Images/07-14.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![07-14](../../OEBPS/Images/07-14.png)'
- en: Figure 7.14 The architecture diagram of the end-to-end machine learning system
    we will build
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 我们将构建的端到端机器学习系统的架构图
- en: Instead of paying attention to individual components, we will look at the entire
    machine learning system, which chains all the components together in an end-to-end
    workflow.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不再关注单个组件，而是将查看整个机器学习系统，该系统将所有组件以端到端工作流的形式连接在一起。
- en: 7.5.1 The problems
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.1 问题
- en: First, the Fashion-MNIST dataset is static and does not change over time. However,
    to design a more realistic system, let’s assume we’ll manually update the Fashion-MNIST
    dataset regularly. Whenever the updates happen, we may want to rerun the entire
    machine learning workflow to train a fresh machine learning model that includes
    the new data. In other words, we need to execute the data ingestion step every
    time when changes happen. In the meantime, when the dataset is not updated, we
    want to experiment with new machine learning models. Thus, we still need to execute
    the entire workflow, including the data ingestion step. The data ingestion step
    is usually very time consuming, especially for large datasets. Is there a way
    to make this workflow more efficient?
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Fashion-MNIST数据集是静态的，不会随时间变化。然而，为了设计一个更现实化的系统，让我们假设我们将定期手动更新Fashion-MNIST数据集。每当更新发生时，我们可能希望重新运行整个机器学习工作流，以训练包含新数据的全新机器学习模型。换句话说，每次发生变化时，我们都需要执行数据摄取步骤。与此同时，当数据集未更新时，我们想要尝试新的机器学习模型。因此，我们仍然需要执行整个工作流，包括数据摄取步骤。数据摄取步骤通常非常耗时，尤其是对于大型数据集。有没有一种方法可以使这个工作流更高效？
- en: Second, we want to build a machine learning workflow that can train different
    models and then select the top model, which will be used in model serving to generate
    predictions using the knowledge from both models. Due to the variance of completion
    time for each of the model training steps in the existing machine learning workflow,
    the start of each following step, such as model selection and model serving, depends
    on the completion of the previous steps. However, this sequential execution of
    steps in the workflow is quite time-consuming and blocks the rest of the steps.
    For example, say one model training step takes much longer to complete than the
    rest of the steps. The model selection step that follows can only start to execute
    after this long-running model training step has completed. As a result, the entire
    workflow is delayed by this particular step. Is there a way to accelerate this
    workflow so it will not be affected by the duration of individual steps?
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们希望构建一个机器学习工作流，该工作流能够训练不同的模型，然后选择最佳模型，该模型将在模型服务中使用来自两个模型的知识来生成预测。由于现有机器学习工作流中每个模型训练步骤完成时间的差异，每个后续步骤（如模型选择和模型服务）的开始都依赖于前一步的完成。然而，工作流中步骤的这种顺序执行非常耗时，并阻塞了其他步骤。例如，假设某个模型训练步骤的完成时间比其他步骤长得多。接下来的模型选择步骤只能在长时间运行的模型训练步骤完成后才能开始执行。因此，整个工作流因这一特定步骤而延迟。有没有一种方法可以加速这个工作流，使其不会受到单个步骤持续时间的影响？
- en: 7.5.2 The solutions
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.2 解决方案
- en: For the first problem, we can use the step memoization pattern from chapter
    5\. Recall that step memoization can help the system decide whether a step should
    be executed or skipped. With the help of step memoization, a workflow can identify
    steps with redundant workloads that can be skipped without being re-executed and
    thus greatly accelerate the execution of the end-to-end workflow.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个问题，我们可以使用第5章中提到的步骤记忆化模式。回想一下，步骤记忆化可以帮助系统决定是否执行或跳过某个步骤。借助步骤记忆化，工作流可以识别出那些可以跳过而不需要重新执行的重载步骤，从而大大加速端到端工作流的执行。
- en: For instance, figure 7.15 contains a simple workflow that only executes the
    data ingestion step when we know the dataset has been updated. In other words,
    we don’t want to re-ingest the data that’s already collected if the new data has
    not been updated.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，图7.15包含一个简单的流程，只有在我们知道数据集已更新时才会执行数据摄取步骤。换句话说，如果新数据没有更新，我们不想重新摄取已经收集的数据。
- en: '![07-15](../../OEBPS/Images/07-15.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![07-15](../../OEBPS/Images/07-15.png)'
- en: Figure 7.15 A diagram of skipping the data ingestion step when the dataset has
    not been updated
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 当数据集未更新时跳过数据摄取步骤的示意图
- en: Many strategies can be used to determine whether the dataset has been updated.
    With a predefined strategy, we can conditionally reconstruct the machine learning
    workflow and control whether we would like to include a data ingestion step to
    be re-executed, as shown in figure 7.16.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用许多策略来确定数据集是否已更新。使用预定义的策略，我们可以有条件地重建机器学习工作流程，并控制是否希望重新执行数据摄取步骤，如图7.16所示。
- en: Cache is one way to identify whether a dataset has been updated. Since we suppose
    our Fashion-MNIST dataset is being updated regularly on a fixed schedule (e.g.,
    once a month), we can create a time-based *cache* that stores the location of
    the ingested and cleaned dataset (assuming the dataset is located in a remote
    database) and the timestamp of its last updated time.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存是识别数据集是否已更新的方法之一。由于我们假设我们的Fashion-MNIST数据集正在按照固定的时间表（例如，每月一次）定期更新，我们可以创建一个基于时间的*缓存*，该缓存存储了已摄取和清理的数据集的位置（假设数据集位于远程数据库中）以及其最后更新时间戳。
- en: As in figure 7.16, the data ingestion step in the workflow will then be constructed
    and executed dynamically based on whether the last updated timestamp is within
    a particular window. For example, if the time window is set to two weeks, we consider
    the ingested data as fresh if it has been updated within the past two weeks. The
    data ingestion step will be skipped, and the following model training steps will
    use the already ingested dataset from the location in the cache. The time window
    can be used to control how old a cache can be before we consider the dataset fresh
    enough to be used directly for model training instead of re-ingesting the data
    from scratch.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '正如图7.16所示，工作流程中的数据摄取步骤将根据最后更新时间戳是否在特定窗口内动态构建和执行。例如，如果时间窗口设置为两周，那么如果数据在过去两周内更新过，我们就认为摄取的数据是新鲜的。数据摄取步骤将被跳过，接下来的模型训练步骤将使用缓存中的位置处已经摄取的数据集。时间窗口可以用来控制缓存可以有多旧，在我们认为数据集足够新鲜可以直接用于模型训练而不是从头开始重新摄取数据之前。 '
- en: '![07-16](../../OEBPS/Images/07-16.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![07-16](../../OEBPS/Images/07-16.png)'
- en: Figure 7.16 The workflow has been triggered. We check whether the data has been
    updated within the last two weeks by accessing the cache. If the data is fresh,
    we can skip the unnecessary data ingestion step and execute the model training
    step directly.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16 工作流程已被触发。我们通过访问缓存来检查数据是否在过去两周内更新过。如果数据是新鲜的，我们可以跳过不必要的数据摄取步骤，并直接执行模型训练步骤。
- en: 'Now, let’s take a look at the second problem: sequential execution of the steps
    blocks the subsequent steps in the workflow and is inefficient. The synchronous
    and asynchronous patterns introduced in chapter 5 can help.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看第二个问题：步骤的顺序执行会阻塞工作流程中的后续步骤，这是低效的。第5章中介绍的同步和异步模式可以有所帮助。
- en: When a short-running model training step finishes—for example, model training
    step 2 in figure 7.17—we successfully obtain a trained machine learning model.
    In fact, we can use this already-trained model directly in our model serving system
    without waiting for the rest of the model training steps to complete. As a result,
    users will be able to see the results of image classification from their model
    serving requests that contain videos as soon as we have trained one model from
    one of the steps in the workflow. After a second model training step (figure 7.17,
    model training step 3) finishes, the two trained models are sent to model serving.
    Now, users benefit from the aggregated results obtained from both models.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个短运行模型训练步骤完成时——例如，图7.17中的模型训练步骤2——我们就成功获得了一个训练好的机器学习模型。实际上，我们可以在模型服务系统中直接使用这个已经训练好的模型，而无需等待模型训练的其他步骤完成。因此，当我们在工作流程中的某个步骤训练了一个模型后，用户就能立即从包含视频的模型服务请求中看到图像分类的结果。当第二个模型训练步骤（图7.17，模型训练步骤3）完成后，两个训练好的模型被发送到模型服务。现在，用户能够从两个模型聚合的结果中受益。
- en: '![07-17](../../OEBPS/Images/07-17.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![07-17](../../OEBPS/Images/07-17.png)'
- en: Figure 7.17 After a second model training step finishes, we can pass the two
    trained models directly to model serving. The aggregated inference results will
    be presented to users instead of only the results from the first model.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17 在第二个模型训练步骤完成后，我们可以直接将两个训练好的模型传递给模型服务。将展示给用户的将是聚合的推理结果，而不仅仅是第一个模型的输出。
- en: As a result, we can continue to use the trained models for model selection and
    model serving; in the meantime, the long-running model training steps are still
    running. In other words, they execute asynchronously without depending on each
    other’s completion. The workflow can proceed and execute the next step before
    the previous one finishes. The long-running model training step will no longer
    block the entire workflow. Instead, it can continue to use the already-trained
    models from the short-running model training steps in the model serving system.
    Thus, it can start handling users’ model serving requests.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以继续使用训练好的模型进行模型选择和模型服务；同时，长时间运行的模型训练步骤仍在进行。换句话说，它们在不依赖于彼此完成的情况下异步执行。工作流程可以继续进行并执行下一个步骤，在之前的步骤完成之前。长时间运行的模型训练步骤将不再阻塞整个工作流程。相反，它可以使用来自短时间运行的模型训练步骤的已训练模型继续在模型服务系统中使用。因此，它可以开始处理用户的模型服务请求。
- en: 7.5.3 Exercises
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.3 练习
- en: Which component can benefit the most from step memoization?
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个组件最能从步骤记忆化中受益？
- en: How do we tell whether a step’s execution can be skipped if its workflow has
    been triggered to run again?
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一个步骤的工作流程已被触发再次运行，我们如何判断该步骤的执行是否可以跳过？
- en: 7.6 Answers to exercises
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.6 练习答案
- en: Section 7.2
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第7.2节
- en: In memory
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在内存中
- en: 'Yes'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是
- en: Section 7.3
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第7.3节
- en: There are blocking communications between workers and parameter servers.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工作器和参数服务器之间存在阻塞通信。
- en: No, each worker stores exactly the same copy of the model.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不，每个工作器存储的是模型的确切相同副本。
- en: Section 7.4
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第7.4节
- en: We cannot balance or distribute the model serving requests among the replicas.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们无法在副本之间平衡或分配模型服务请求。
- en: Section 7.5
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第7.5节
- en: The data ingestion component
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据摄入组件
- en: Using the metadata in the step cache
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用步骤缓存中的元数据
- en: Summary
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The data ingestion component uses the caching pattern to speed up the processing
    of multiple epochs of the dataset.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据摄入组件使用缓存模式来加速处理数据集多个epoch的处理。
- en: The model training component uses the collective communication pattern to avoid
    the potential communication overhead between workers and parameter servers.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练组件使用集体通信模式来避免工作器和参数服务器之间潜在的通信开销。
- en: We can use model server replicas, which are capable of handling many requests
    at one time since each replica processes individual model serving requests independently.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用模型服务器副本，因为每个副本可以独立处理模型服务请求，所以它们能够同时处理许多请求。
- en: We can chain all our components into a workflow and use caching to effectively
    skip time-consuming components such as data ingestion.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将所有组件链接成一个工作流程，并使用缓存来有效地跳过耗时组件，如数据摄入。
