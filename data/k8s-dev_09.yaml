- en: 7 Internal services and load balancing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 内部服务和负载均衡
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Creating internal services
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建内部服务
- en: Routing packets in Kubernetes between virtual IP addresses of Pods and Services
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中在 Pod 和服务的虚拟 IP 地址之间路由数据包
- en: Discovering the IP address of internal services
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现内部服务的 IP 地址
- en: Configuring HTTP load balancers with Ingress
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Ingress 配置 HTTP 负载均衡器
- en: Provisioning TLS certificates to create HTTPS endpoints
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 TLS 证书以创建 HTTPS 端点
- en: Internal services are a way to scale how you develop and serve your application
    by splitting your application into multiple smaller services. These individual
    services can be on different development cycles (possibly by different teams)
    and use completely different programming languages and technology from each other.
    After all, as long as you can containerize it, you can run it in Kubernetes. No
    longer do you need to worry whether your application deployment platform can run
    what you need it to run.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 内部服务是一种通过将应用程序拆分为多个较小的服务来扩展您开发和服务应用程序的方式。这些单独的服务可以处于不同的开发周期中（可能由不同的团队完成）并使用彼此完全不同的编程语言和技术。毕竟，只要您可以将它容器化，您就可以在
    Kubernetes 中运行它。您不再需要担心您的应用程序部署平台是否可以运行您需要运行的内容。
- en: In this chapter, we’ll look at how to configure and discover internal services
    in the cluster, as well as how Kubernetes gives each of these a cluster-local
    IP address and implements internal network routing to make them addressable by
    other Pods in the cluster. We’ll also look at how you can expose multiple services
    on a single external IP using Ingress and how Ingress can handle TLS termination
    so you can offer HTTPS endpoints for your application without needing to configure
    TLS certificates in your applications.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何在集群中配置和发现内部服务，以及 Kubernetes 如何为这些服务分配集群本地 IP 地址并实现内部网络路由，以便其他集群中的
    Pod 可以访问它们。我们还将探讨如何使用 Ingress 在单个外部 IP 上公开多个服务，以及 Ingress 如何处理 TLS 终止，这样您就可以为应用程序提供
    HTTPS 端点，而无需在应用程序中配置 TLS 证书。
- en: 7.1 Internal services
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 内部服务
- en: There are many reasons to create services that are completely internal to your
    cluster. Perhaps you’ve adopted a microservice architecture, or you’re integrating
    an open source service, or you just want to connect two applications together
    that are written in different languages.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多原因需要创建完全属于您集群内部的服务的。可能您采用了微服务架构，或者您正在集成开源服务，或者您只是想连接两个用不同语言编写的应用程序。
- en: In chapter 3, I introduced Services of the type `LoadBalancer` as a way to get
    external traffic on a public IP. Service is also used to connect internal services
    but using cluster IP addresses. Kubernetes supports a few different Service types;
    the two used for internal services are `ClusterIP` and `NodePort`.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3 章中，我介绍了 `LoadBalancer` 类型的服务，作为在公共 IP 上获取外部流量的方式。服务还用于连接内部服务，但使用集群 IP 地址。Kubernetes
    支持几种不同的服务类型；用于内部服务的是 `ClusterIP` 和 `NodePort`。
- en: The *ClusterIP* type gives you a virtual IP address in the Kubernetes cluster.
    This IP is addressable from any Pod within your cluster (like from your main application).
    The *NodePort* type additionally reserves a high port number on each node in the
    cluster, allowing you to access it from outside the cluster (since the node’s
    IP is directly addressable from the network). Internal cluster communication is
    typically done with `ClusterIP`, while `NodePort` is used for routing external
    traffic, including with Ingress. In both cases, Kubernetes configures the network
    to proxy requests to the Pods that back the service.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClusterIP` 类型为您在 Kubernetes 集群中提供了一个虚拟 IP 地址。此 IP 地址可以从您集群中的任何 Pod 访问（例如，从您的主要应用程序）。`NodePort`
    类型还在每个集群节点上保留了一个高端口号，允许您从集群外部访问它（因为节点的 IP 地址可以从网络直接访问）。内部集群通信通常使用 `ClusterIP`，而
    `NodePort` 用于路由外部流量，包括与 Ingress 一起使用。在这两种情况下，Kubernetes 都会配置网络以代理请求到支持服务的 Pod。'
- en: Note In fact, three service types get a cluster IP, not just the `ClusterIP`
    type. The `NodePort` and `LoadBalancer` types are getting *additional* access
    methods beyond that of the `ClusterIP` *type, and are also accessible over a cluster
    IP.*
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 注意实际上，三种服务类型都获得集群 IP，而不仅仅是 `ClusterIP` 类型。`NodePort` 和 `LoadBalancer` 类型获得了
    `ClusterIP` 类型之外的 *附加* 访问方法，并且也可以通过集群 IP 访问。
- en: 7.1.1 Kubernetes cluster networking
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 Kubernetes 集群网络
- en: Now might be a good time for a quick primer on Kubernetes networking. Each Pod
    gets its own IP address, and can communicate with all other Pods in the cluster
    directly without needing NAT (Network Address Translation). Containers within
    the Pod share the same IP. This property of Kubernetes makes Pods behave a bit
    like VMs, and is convenient as you don’t need to worry about port conflict between
    Pods on the node (i.e., multiple Pods can run a container on port 80). Nodes have
    their own IP, which is assigned to the network interface of the VM, while the
    Pod IP uses a virtual network interface where traffic is routed via the node’s
    interface.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可能是快速介绍Kubernetes网络的好时机。每个Pod都有自己的IP地址，并且可以直接与集群中的所有其他Pod通信，而无需NAT（网络地址转换）。Pod内的容器共享相同的IP。Kubernetes的这个特性使得Pod的行为有点像虚拟机，这很方便，因为您不需要担心节点上Pod之间的端口冲突（即，多个Pod可以在端口80上运行容器）。节点有自己的IP，该IP分配给虚拟机的网络接口，而Pod
    IP使用一个虚拟网络接口，其中流量通过节点的接口进行路由。
- en: Services (other than headless services, covered in Chapter 9) are assigned a
    virtual IP. The virtual IP isn’t routed to a single Pod or node, but rather uses
    some networking glue on the node to balance traffic to the Pods that back the
    Service. This networking glue is provided by Kubernetes (using iptables or IP
    Virtual Service [IPVS]) and handles the traffic routing. A list of Pods that back
    the Service and their IPs is maintained on the node and is used for this routing.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 服务（除了在第9章中提到的无头服务）被分配了一个虚拟IP。这个虚拟IP不会路由到单个Pod或节点，而是使用节点上的某些网络粘合剂来平衡支持服务的Pod之间的流量。这种网络粘合剂由Kubernetes（使用iptables或IP虚拟服务[IPVS]）提供，并处理流量路由。节点上维护着一个支持服务的Pod及其IP列表，用于此路由。
- en: When a request is made from a Pod to a Service over the cluster IP or node port,
    that request is first handled by the networking glue on the node, which has an
    updated list from the Kubernetes control plane of every Pod that belongs to that
    Service (and which nodes those Pods are on). It will pick one of the Pod IPs at
    random and route the request to that Pod via its node (figure 7.1). Fortunately,
    all this happens quite seamlessly; your app can simply make a request like `HTTP`
    `GET` using the IP of the service, and everything behaves as you’d expect.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当从Pod通过集群IP或节点端口向服务发起请求时，该请求首先由节点上的网络粘合剂处理，该粘合剂拥有来自Kubernetes控制平面的每个属于该服务的Pod的更新列表（以及这些Pod所在的节点）。它将随机选择一个Pod
    IP，并通过其节点将该请求路由到该Pod（图7.1）。幸运的是，所有这些操作都非常顺畅；您的应用程序可以简单地使用服务的IP发起一个类似`HTTP GET`的请求，一切都会如您所期望的那样运行。
- en: '![07-01](../../OEBPS/Images/07-01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![07-01](../../OEBPS/Images/07-01.png)'
- en: Figure 7.1 IP routing for an internal service named Robohash. The `Frontend-1`
    Pod makes an internal request to the service. The iptables’ routing glue on the
    node has a list of Pods for the Service, which is supplied by the Kubernetes control
    plane, and selects the Pod named `Robohash-2` at random. The request is then routed
    to that Pod via its node.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1展示了名为Robohash的内部服务的IP路由。`Frontend-1` Pod向服务发起内部请求。节点上的iptables路由粘合剂有一个服务Pod列表，该列表由Kubernetes控制平面提供，并随机选择名为`Robohash-2`的Pod。请求随后通过该Pod所在的节点路由到该Pod（图7.1）。然后，请求通过其节点路由到该Pod。
- en: What this all means is that when it’s time for you to deploy an internal service,
    you can achieve this by creating a Service of type `ClusterIP`, thereby obtaining
    an IP address that the other Pods in your cluster (like your app’s frontend) can
    communicate with seamlessly. This IP address automatically balances the load between
    all Pod replicas of the internal service. As a developer, you don’t typically
    need to worry about all the networking glue that makes this possible, but hopefully,
    this section has given you at least a base understanding of how it works.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着当您需要部署内部服务时，您可以通过创建一个类型为`ClusterIP`的服务来实现，从而获得一个其他Pod（如您的应用程序的前端）可以无缝通信的IP地址。这个IP地址会自动平衡内部服务所有Pod副本之间的负载。作为开发者，您通常不需要担心使这一切成为可能的网络粘合剂，但希望本节至少为您提供了对它是如何工作的基本理解。
- en: 7.1.2 Creating an internal service
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 创建内部服务
- en: Now that you hopefully understand a bit about how Kubernetes networking works
    under the hood, let’s build an internal service that can be used by other Pods
    in the cluster. As an example, let’s deploy a new internal service to our app.
    For this, I’m going to use a neat open source library called Robohash that can
    generate cute robot avatars for users based on a hash (like a hash of their IP).
    For your own deployments, internal services can be things as simple as avatar
    generators, other parts of your application, or even entire database deployments.
    The following listing shows the Deployment for this new container.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可能已经对Kubernetes网络内部的工作原理有了更深的理解，让我们构建一个可以被集群中其他Pod使用的内部服务。作为一个例子，让我们在我们的应用程序中部署一个新的内部服务。为此，我将使用一个叫做Robohash的
    neat开源库，它可以基于哈希（比如IP的哈希）为用户生成可爱的机器人头像。对于你自己的部署，内部服务可以是像头像生成器这样简单的东西，也可以是应用程序的其他部分，甚至是整个数据库部署。以下列表显示了新容器的部署。
- en: Listing 7.1 Chapter07/7.1_InternalServices/robohash-deploy.yaml
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.1 第07章/7.1_内部服务/robohash-deploy.yaml
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ The Robohash container
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Robohash容器
- en: This time, instead of exposing this service to the world with a Service of type
    `LoadBalancer`, we’ll keep it internal with a `ClusterIP` type Service. The following
    listing provides the internal service definition for our Robohash Deployment.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们不会使用类型为`LoadBalancer`的Service将此服务暴露给世界，而是将其保持为内部服务，使用类型为`ClusterIP`的Service。以下列表提供了我们的Robohash部署的内部服务定义。
- en: Listing 7.2 Chapter07/7.1_InternalServices/robohash-service.yaml
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.2 第07章/7.1_内部服务/robohash-service.yaml
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Defines a local service
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个本地服务
- en: 'Since this isn’t a `LoadBalancer`-type Service like we used in chapter 3, it
    doesn’t have an external IP. After creating both resources, to try it out you
    can use `kubectl` port forwarding:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这不是像我们在第3章中使用的那种`LoadBalancer`类型的Service，它没有外部IP。在创建这两个资源之后，要尝试它，你可以使用`kubectl`端口转发：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now you can browse to http://localhost:8080 on your local machine and check
    out the service. To generate a test avatar, try something like http://localhost:8080/example.
    You should see an autogenerated robot avatar image like the one in figure 7.2.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以在本地机器上浏览到http://localhost:8080并检查该服务。要生成一个测试头像，尝试类似http://localhost:8080/example的地址。你应该会看到一个自动生成的机器人头像图像，如图7.2所示。
- en: '![07-02](../../OEBPS/Images/07-02.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![07-02](../../OEBPS/Images/07-02.png)'
- en: Figure 7.2 Example robot avatar (robot parts designed by Zikri Kader, assembled
    by Robohash.org, and licensed under CC-BY)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 示例机器人头像（机器人部件由Zikri Kader设计，由Robohash.org组装，并授权于CC-BY）
- en: Next, let’s use this internal service from another service—our frontend—and
    build out our microservice architecture (figure 7.3)!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用这个内部服务从另一个服务——我们的前端——构建我们的微服务架构（图7.3）！
- en: '![07-03](../../OEBPS/Images/07-03.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![07-03](../../OEBPS/Images/07-03.png)'
- en: Figure 7.3 Sequence diagram of a simple microservice configuration
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 简单微服务配置的序列图
- en: 'To access this internal service from other Pods, you can reference its cluster
    IP. To view the cluster IP assigned, query the Service:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要从其他Pod访问此内部服务，你可以引用其集群IP。要查看分配的集群IP，查询服务：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this case, you can access the service from other Pods on the given cluster
    IP (seen as `10.63.254.218` in the previous output), such as by making an `HTTP`
    `GET` request to `http://10.63.254.218/example`. This address will only be reachable
    from other Pods within the cluster.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你可以从给定集群IP（在之前的输出中显示为`10.63.254.218`）上的其他Pod访问该服务，例如通过向`http://10.63.254.218/example`发送一个`HTTP`
    `GET`请求。这个地址只能在集群内的其他Pod中访问。
- en: 7.1.3 Service discovery
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 服务发现
- en: In the previous example, we used `kubectl` `get` `service` to look up the internal
    cluster IP address assigned to our Service. While you could simply take that IP
    address and hardcode it into your application, doing this isn’t great for portability.
    You may wish to deploy the same application in a few different places, like locally
    on your development machine, in a staging environment, and in production (how
    to set up these different environments is covered in chapter 11). If you reference
    the IP directly, you will need to update your code every time.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个示例中，我们使用了`kubectl` `get` `service`来查找分配给我们的服务的内部集群IP地址。虽然你可以简单地取出这个IP地址并将其硬编码到你的应用程序中，但这样做并不利于可移植性。你可能希望在几个不同的地方部署相同的应用程序，比如在本地开发机器上、在预发布环境中，以及在生产环境中（如何设置这些不同的环境在第11章中有介绍）。如果你直接引用IP地址，每次都需要更新你的代码。
- en: 'It’s better to discover the IP address dynamically from the Pod that needs
    to call the service, much like we discovered the IP address using `kubectl`. Kubernetes
    offers Pods two ways to perform service discovery: using a DNS lookup, or an environment
    variable. The DNS lookup works cluster-wide, while the environment variable is
    only for Pods within the same namespace.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最好从需要调用服务的 Pod 动态发现 IP 地址，就像我们使用 `kubectl` 发现 IP 地址一样。Kubernetes 为 Pods 提供了两种进行服务发现的方式：使用
    DNS 查询或环境变量。DNS 查询在集群范围内工作，而环境变量仅适用于同一命名空间内的 Pods。
- en: Service discovery using environment variables
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用环境变量进行服务发现
- en: Kubernetes automatically creates an environment variable for each Service, populates
    it with the cluster IP, and makes the IP available in every Pod created after
    the Service is created. The variable follows a naming conversion whereby our example
    `robohash-internal` Service gets the environment variable `ROBOHASH_INTERNAL_SERVICE_HOST`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 自动为每个服务创建一个环境变量，用集群 IP 填充它，并在创建服务后的每个 Pod 中提供 IP 地址。变量遵循命名转换规则，我们的示例
    `robohash-internal` 服务获得环境变量 `ROBOHASH_INTERNAL_SERVICE_HOST`。
- en: 'Rather than figuring out the correct conversion, you can view a list of all
    such environment variables available to your Pod by running the `env` command
    on your Pod with `exec` (with truncated output):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是找出正确的转换，你可以通过在 Pod 上运行 `env` 命令（使用 `exec`，输出被截断）来查看你的 Pod 可用的所有此类环境变量的列表：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The benefit of this approach is that it’s extremely fast. Environment variables
    are just string constants, with no dependencies external to the Pod itself. It
    also frees you to specify any DNS server you like to serve the other requests
    of the Pod (e.g., `8.8.8.8`). The downside is that only those Services in the
    same namespace of the Pod are populated in environment variables, and that ordering
    matters: the Service must be created before the Pod for the Pod to get the Service’s
    environment variable.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点是它非常快。环境变量只是字符串常量，没有依赖于 Pod 本身之外的外部依赖。这也让你可以指定任何你喜欢的 DNS 服务器来处理 Pod 的其他请求（例如，`8.8.8.8`）。缺点是只有与
    Pod 同一命名空间中的服务会被填充到环境变量中，并且顺序很重要：服务必须在 Pod 之前创建，这样 Pod 才能获取到服务的环境变量。
- en: 'If you find yourself in a situation where you need to restart your Deployment’s
    Pods to pick up changes to the Service, you can do so with the following command
    (no change to the Pod needed):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发现自己处于需要重启 Deployment 的 Pods 以获取服务更改的情况，你可以使用以下命令（不需要更改 Pod）：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: One common way to reference these variables is to provide the complete HTTP
    endpoint of the internal service in its own environment variable defined in the
    Deployment. This allows your container to be even more portable and able to run
    outside of Kubernetes (e.g., in Docker Compose). The following listing shows you
    how to embed the value of the automatically generated environment variable (`ROBOHASH_INTERNAL_
    SERVICE_HOST`) in your own custom environment variable (`AVATAR_ENDPOINT`), which
    your application will ultimately consume.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 引用这些变量的一个常见方式是在 Deployment 中定义自己的环境变量，提供内部服务的完整 HTTP 端点。这允许你的容器更加便携，能够在 Kubernetes
    之外运行（例如，在 Docker Compose 中）。以下列表显示了如何将自动生成的环境变量（`ROBOHASH_INTERNAL_SERVICE_HOST`）的值嵌入到自己的自定义环境变量（`AVATAR_ENDPOINT`）中，该变量最终将由你的应用程序使用。
- en: Listing 7.3 Chapter07/7.1_InternalServices/timeserver-deploy-env.yaml
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.3 Chapter07/7.1_InternalServices/timeserver-deploy-env.yaml
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Service discovery using environment variables
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用环境变量进行服务发现
- en: Using this additional layer of indirection, where our custom environment variable
    references the Kubernetes one, is useful as now we can run this container standalone
    in Docker (just populate `AVATAR_ENDPOINT` with the endpoint of the internal service
    wherever it’s running) or switch to DNS-based lookups.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种额外的间接层，其中我们的自定义环境变量引用 Kubernetes 的环境变量，现在我们可以将这个容器独立运行在 Docker 中（只需在运行内部服务的任何地方用内部服务的端点填充
    `AVATAR_ENDPOINT`）或切换到基于 DNS 的查找。
- en: 'In summary, environment variable discovery has a few advantages:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，环境变量发现有几个优点：
- en: Superfast performance (they are string constants)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极快性能（它们是字符串常量）
- en: No dependency on other DNS Kubernetes components
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不依赖于其他 DNS Kubernetes 组件
- en: 'And it has some disadvantages:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 它也有一些缺点：
- en: Only available to Pods in the same namespace
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅适用于同一命名空间中的 Pods
- en: Pods must be created after the Service is created
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pods 必须在服务创建之后创建
- en: Service Discovery using DNS
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DNS 进行服务发现
- en: The other way to discover services is via the cluster’s internal DNS service.
    For Services running in a different namespace than the Pod, this is the only option
    for discovery. The Service’s name is exposed as a DNS host, so you can simply
    do a DNS lookup on `robohash-internal` (or use `http:/``/robohash-internal` as
    your HTTP path), and it will resolve. When calling the service from other namespaces,
    append the namespace—for example, use `robohash-internal.default` to call the
    service `robohash-internal` in the `default` namespace.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 发现服务的另一种方式是通过集群的内部DNS服务。对于在Pod所在的不同命名空间中运行的服务，这是唯一的发现选项。服务的名称作为DNS主机暴露，因此您可以对`robohash-internal`（或使用`http://robohash-internal`作为您的HTTP路径）进行DNS查找，它将解析。当从其他命名空间调用服务时，请附加命名空间——例如，使用`robohash-internal.default`来调用`default`命名空间中的服务`robohash-internal`。
- en: The only downside to this approach is that it’s a little slower to resolve that
    IP address, as a DNS lookup is needed. In many Kubernetes clusters, this DNS service
    will be running on the same node, so it’s pretty fast; in others, it may require
    a hop to the DNS service running on a different node or a managed DNS service,
    so be sure to cache the result.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的唯一缺点是，由于需要DNS查找，解析IP地址会稍微慢一些。在许多Kubernetes集群中，这个DNS服务将在同一节点上运行，所以它相当快；在其他集群中，可能需要跳转到运行在不同节点上的DNS服务或托管DNS服务，因此请确保缓存结果。
- en: Since we previously made the endpoint URL an environment variable of the Deployment,
    we can easily update the variable, this time giving it the service name (`http://robohash-internal`).
    The complete Deployment will look like the following listing.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们之前将端点URL设置为Deployment的环境变量，我们可以轻松地更新该变量，这次给它提供服务名称（`http://robohash-internal`）。完整的Deployment将如下所示。
- en: Listing 7.4 Chapter07/7.1_InternalServices/timeserver-deploy-dns.yaml
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.4 第07章/7.1_InternalServices/timeserver-deploy-dns.yaml
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Service discovery using DNS
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用DNS进行服务发现
- en: 'In summary, DNS-based service discovery has a few advantages:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，基于DNS的服务发现有几个优点：
- en: Can be called from any namespace in the cluster
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以从集群中的任何命名空间调用
- en: No ordering dependencies
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有顺序依赖
- en: 'And it has some disadvantages:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 它也有一些缺点：
- en: Slightly slower than using an environment variable (which is a constant)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比使用环境变量（这是一个常量）略慢
- en: Dependency on the internal DNS service
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖于内部DNS服务
- en: So, using environment variables and DNS lookups are two ways that our front-end
    service can discover the internal service’s internal Pod IP, rather than having
    that IP address hardcoded. Since these discovery methods are Kubernetes-specific,
    it’s recommended that you supply the path as an environment variable to the container
    as we did in the example. Then, you can easily supply a completely different path
    when running the container outside of Kubernetes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用环境变量和DNS查找是我们前端服务发现内部服务的内部Pod IP的两种方式，而不是将IP地址硬编码。由于这些发现方法具有Kubernetes特定性，建议您像示例中那样将路径作为环境变量提供给容器。然后，您可以在Kubernetes外部运行容器时轻松提供完全不同的路径。
- en: Putting it all together
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起
- en: Let’s make a call from the timeserver app to our internal Robohash service on
    a new endpoint, `/avatar`. All this new endpoint does is read an image from the
    internal service and return it.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从timeserver应用向新的端点`/avatar`上的内部Robohash服务发起调用。这个新端点所做的只是从内部服务读取一个图像并将其返回。
- en: Listing 7.5 Chapter07/timeserver5/server.py
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.5 第07章/timeserver5/server.py
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now that our application actually uses the internal service, we can deploy
    it all to Kubernetes:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应用程序实际上使用了内部服务，我们可以将其全部部署到Kubernetes中：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Wait for the external IP to be provisioned and then try out the `/avatar` URL.
    You should be greeted with a robot avatar. Switch timeserver-deploy-dns.yaml for
    timeserver-deploy-env.yaml to use the alternative discovery method with the same
    end result.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 等待外部IP配置完成，然后尝试使用`/avatar` URL。您应该会看到一个机器人头像。将timeserver-deploy-dns.yaml替换为timeserver-deploy-env.yaml以使用具有相同结果的替代发现方法。
- en: We are now using a microservice architecture! Using this technique, you can
    have multiple internal services that can be deployed and managed separately (perhaps
    by different teams). You can add separate services using open source tooling or
    simply bring together different components of your application written in different
    languages.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在正在使用微服务架构！使用这种技术，您可以拥有多个可以单独部署和管理的内部服务（可能由不同的团队管理）。您可以使用开源工具添加单独的服务，或者简单地汇集您用不同语言编写的应用程序的不同组件。
- en: '7.2 Ingress: HTTP(S) load balancing'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 入口：HTTP(S)负载均衡
- en: So far in the book, we’ve been creating external IPs using Services of type
    `LoadBalancer`. This provides you with a so-called layer-4 (L4) load balancer,
    which balances requests at the network layer and can work with a variety of protocols
    (e.g., TCP, UDP, SCTP). You configure the Service with your desired protocol and
    port, and you get an IP that will balance traffic over your Pods. If you expose
    an HTTP service over a load balancer, you need to implement your own TLS termination
    handling (i.e., configuring certificates and running an HTTPS endpoint), and all
    traffic to that endpoint will get routed to one set of Pods (based on the `matchLabels`
    rules). There is no option for exposing two or more separate services directly
    on the same load balancer (although one can proxy requests to the other internally).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们一直在使用类型为 `LoadBalancer` 的服务创建外部 IP。这为您提供了一个所谓的第 4 层（L4）负载均衡器，它在网络层平衡请求，并且可以与各种协议（例如，TCP、UDP、SCTP）一起工作。您使用所需的协议和端口配置服务，然后您会得到一个将在您的
    Pods 上平衡流量的 IP。如果您通过负载均衡器公开 HTTP 服务，您需要实现自己的 TLS 终止处理（即配置证书并运行 HTTPS 端点），并且所有到该端点的流量都将被路由到一组
    Pods（基于 `matchLabels` 规则）。没有直接在同一个负载均衡器上公开两个或更多独立服务的选项（尽管可以在内部代理请求到另一个服务）。
- en: When you are publishing an HTTP app specifically, you may get more utility from
    a so-called layer-7 (L7) load balancer, which balances at the HTTP request layer
    and can do more fancy things, like terminate HTTPS connections (meaning it will
    handle the HTTPS details for you), and perform path-based routing so that you
    can serve a single domain host with multiple services. In Kubernetes, an HTTP
    load balancer is created with an Ingress object.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当您专门发布 HTTP 应用程序时，您可能可以从所谓的第 7 层（L7）负载均衡器中获得更多实用功能，该负载均衡器在 HTTP 请求层进行平衡，并且可以执行更多复杂的功能，例如终止
    HTTPS 连接（这意味着它将为您处理 HTTPS 的细节），并执行基于路径的路由，以便您可以使用多个服务为单个域名主机提供服务。在 Kubernetes
    中，HTTP 负载均衡器是通过入口对象创建的。
- en: Ingress lets you place multiple internal services behind a single external IP
    with load balancing. You can direct HTTP requests to different backend services
    based on their URI path (`/foo`, `/bar`), hostname (`foo.example.com,` `bar.example.com`),
    or both (figure 7.4). The ability to have multiple services running on a single
    IP, and potentially serving different paths under a single domain name, is unique
    to Ingress, because if you’d exposed them with individual Services of type `LoadBalancer`
    like in the earlier chapters, the services would have separate IP addresses, necessitating
    separate domains (e.g., `foo.example.com` to address one, and `bar.example.com`
    to address the other).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 入口允许您在单个外部 IP 后面放置多个内部服务，并实现负载均衡。您可以根据它们的 URI 路径（`/foo`、`/bar`）、主机名（`foo.example.com`、`bar.example.com`）或两者（图
    7.4）将 HTTP 请求定向到不同的后端服务。能够在单个 IP 上运行多个服务，并且可能在不同域名下提供不同路径的能力是入口独有的，因为如果您像前几章中那样使用类型为
    `LoadBalancer` 的独立服务公开它们，则服务将具有不同的 IP 地址，需要不同的域名（例如，使用 `foo.example.com` 来访问一个，使用
    `bar.example.com` 来访问另一个）。
- en: '![07-04](../../OEBPS/Images/07-04.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![07-04](../../OEBPS/Images/07-04.png)'
- en: Figure 7.4 The Ingress’s rule list, or URL map, allows one HTTP load balancer
    to handle the traffic for multiple services.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 入口的规则列表，或 URL 映射，允许一个 HTTP 负载均衡器处理多个服务的流量。
- en: The property of Ingress being able to place multiple services under one host
    is useful when scaling up your application. When you need to break up your services
    into multiple services for developer efficiency (e.g., teams wanting to manage
    their own deployment lifecycle) or scaling (e.g., being able to scale aspects
    of the application separately), you can use Ingress to route the requests while
    not altering any public-facing URLs. For example, let’s say that your application
    has a path that is a particularly CPU-intensive request. You might wish to move
    it to its own service to allow it to be scaled separately. Ingress allows you
    to make such changes seamlessly to end users.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 入口能够将多个服务放置在单个主机下的属性，在扩展您的应用程序时非常有用。当您需要将服务拆分成多个服务以提高开发效率（例如，团队想要管理自己的部署生命周期）或进行扩展（例如，能够单独扩展应用程序的某些方面）时，您可以使用入口来路由请求，同时不更改任何面向公众的
    URL。例如，假设您的应用程序有一个特别占用 CPU 的路径。您可能希望将其移动到自己的服务中，以便它可以单独扩展。入口允许您无缝地对最终用户进行此类更改。
- en: Listing 7.6 provides an example Ingress where the routes are served by different
    backends. In this example, we’ll expose an internal Timeserver Service on the
    root path (`/`), and an internal Robohash Service on `/robohash`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.6 提供了一个示例 Ingress，其中路由由不同的后端提供服务。在这个例子中，我们将暴露根路径（`/`）上的内部 Timeserver 服务，以及
    `/robohash` 上的内部 Robohash 服务。
- en: Listing 7.6 Chapter07/7.2_Ingress/ingress_path.yaml
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.6 第07章/7.2_Ingress/ingress_path.yaml
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ First path, handled by the timeserver-internal Service
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第一路径，由 timeserver-internal 服务处理
- en: ❷ Second path, handled by the robohash-internal Service
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 第二路径，由 robohash-internal 服务处理
- en: Listing 7.7 shows a variation using different hosts. Each of these hosts can
    also have multiple paths using the format in listing 7.6.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.7 展示了使用不同主机的变体。这些主机也可以使用列表 7.6 中的格式拥有多个路径。
- en: Listing 7.7 Chapter07/7.2_Ingress/ingress_host.yaml
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.7 第07章/7.2_Ingress/ingress_host.yaml
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ First host, handled by the timeserver-internal Service
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第一主机，由 timeserver-internal 服务处理
- en: ❷ Second host, handled by the robohash-internal Service
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 第二主机，由 robohash-internal 服务处理
- en: The Ingress references internal services that are specified as Services with
    the type `NodePort`*, such as the one in the following listing.*
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 引用了指定为 `NodePort` 类型的服务作为内部服务，如下列所示。[*]
- en: Listing 7.8 Chapter07/7.2_Ingress/timeserver-service-internal.yaml
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.8 第07章/7.2_Ingress/timeserver-service-internal.yaml
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ To use an internal service with Ingress, it needs to be of type NodePort
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 要使用 Ingress 中的内部服务，它需要是 NodePort 类型
- en: 'Ingress objects can be configured to perform exact matching (i.e., only requests
    exactly matching the path given will be routed to the Service) or prefix matching
    (i.e., all requests matching the path prefix will be routed) with the `pathType`
    property. I’m not going to go into a lot of detail here, as the official docs
    do a great job. One facet worth reproducing is the rule on multiple matches:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过 `pathType` 属性配置 Ingress 对象以执行精确匹配（即，只有与给定路径完全匹配的请求将被路由到服务）或前缀匹配（即，所有与路径前缀匹配的请求都将被路由）。这里我不会详细介绍，因为官方文档已经做得很好。值得重现的一个方面是关于多个匹配的规则：
- en: In some cases, multiple paths within an Ingress will match a request. In those
    cases, precedence will be given first to the longest matching path. If two paths
    are still equally matched, precedence will be given to paths with an exact path
    type over prefix path type.[¹](#pgfId-1088668)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，Ingress 内部的多个路径将匹配一个请求。在这些情况下，优先级将首先给予最长匹配的路径。如果两个路径仍然完全匹配，则优先级将给予具有精确路径类型的路径，而不是前缀路径类型。[¹](#pgfId-1088668)
- en: As you may have seen in listing 7.6, there was a path for `/` and a second one
    for `/robohash`. A request to `/robohash` will be routed to the second Service,
    even though it also matches the first path. If you’ve used other routing mechanisms
    in the past (like Apache URL rewriting), often the preference would go to the
    *first* rule matched—not so in Kubernetes, where the longer matched rule gets
    preference. I find this design convenient, as it matches well with developer intent.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在列表 7.6 中所见，存在一个 `/` 路径和一个 `/robohash` 的第二个路径。对 `/robohash` 的请求将被路由到第二个服务，即使它也匹配第一个路径。如果你过去使用过其他路由机制（如
    Apache URL 重写），通常优先级会给予第一个匹配的规则——但在 Kubernetes 中并非如此，其中较长的匹配规则会获得优先级。我发现这种设计很方便，因为它很好地符合开发者的意图。
- en: 'To deploy this example, delete the previous example if it’s running (`kubectl`
    `delete` `-f` `Chapter07/7.1_InternalServices`), and run the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署此示例，如果之前运行的示例仍在运行，请先删除它（`kubectl` `delete` `-f` `Chapter07/7.1_InternalServices`），然后运行以下命令：
- en: '[PRE13]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Once your Ingress has an IP, you can browse to it. Try the `/robohash` path
    to connect to the Robohash service via the Ingress. Note that the resources backing
    the Ingress may take a bit of extra time to be provisioned. Even after you have
    the IP address and browse to it, you may see a `404` `error` for a time. I suggest
    trying again in about 5 minutes to give the cloud provider time to update the
    Ingress.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的 Ingress 有了一个 IP，你就可以浏览它。尝试 `/robohash` 路径通过 Ingress 连接到 Robohash 服务。请注意，支持
    Ingress 的资源可能需要一些额外的时间来配置。即使你已经有了 IP 地址并浏览了它，你可能会看到一段时间的 `404` 错误。我建议大约 5 分钟后再试一次，以便给云服务提供商一些时间来更新
    Ingress。
- en: 'To debug problems with the Ingress, you can use `kubectl` `describe` `ingress`.
    The following is what I saw when I described the Ingress shortly after it had
    an IP assigned, but before it was ready:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要调试 Ingress 的问题，可以使用 `kubectl` `describe` `ingress`。以下是我描述 Ingress 时的所见，当时它已经分配了
    IP，但尚未就绪：
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Backend status is unknown
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 后端状态未知
- en: 'The following shows the status after waiting a few more minutes. Notice how
    the annotation changed from `Unknown` to `HEALTHY`. After that, I was able to
    browse to the IP and access the service:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在等待几分钟后的状态。注意注释如何从`Unknown`变为`HEALTHY`。之后，我能够浏览到IP并访问服务：
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Backend status is now healthy
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 后端状态现在是健康的
- en: 'Cost-saving tip: Saving IPs with Ingress'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 节省成本技巧：使用Ingress保存IP
- en: A benefit of Ingress is that by using host-based routing, you can host several
    services, all with the same external IP address. The Ingress inspects the `Host`
    header in the HTTP request and routes traffic accordingly. This contrasts with
    Services of type `LoadBalancer`, where each gets its own IP address assigned and
    performs no HTTP request-based routing.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress的一个好处是，通过使用基于主机的路由，你可以托管多个服务，所有这些服务都使用相同的公网IP地址。Ingress会检查HTTP请求中的`Host`头，并根据此进行流量路由。这与类型为`LoadBalancer`的服务形成对比，其中每个服务都分配了自己的IP地址，并且不执行基于HTTP请求的路由。
- en: Cloud providers often charge based on load-balancing rules, which roughly translates
    into how many load-balancing external IP addresses are assigned. By using an Ingress
    to combine several services into one, rather than each being exposed with its
    own IP, you can likely save money.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商通常根据负载均衡规则收费，这大致等同于分配了多少个负载均衡的外部IP地址。通过使用Ingress将多个服务组合成一个，而不是每个服务都使用自己的IP进行暴露，你可能会节省一些费用。
- en: If your cloud provider groups HTTP load balancers (Ingress) and network load
    balancers (Services of type `LoadBalancer`) separately and has a minimum rule
    fee (like Google Cloud does at the time of writing), you may want to use one or
    the other exclusively until you need more than the minimum.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的云服务提供商将HTTP负载均衡器（Ingress）和网络负载均衡器（类型为`LoadBalancer`的服务）分开管理，并且有最低规则费用（例如，在撰写本文时，谷歌云有最低规则费用），那么你可能想要在需要超过最低费用之前，只使用其中之一。
- en: 'Another trick, but one I don’t recommend, is running your own Ingress *controller*.
    This technique (not covered in this book) means deploying an open source component
    as a load balancer to implement the Kubernetes Ingress functionality, overriding
    the default implementation of your cloud provider. This approach means that the
    Ingress objects and Services objects with type `LoadBalancer` get treated as the
    same rule types for billing, which can save money if you need both, but there’s
    a sacrifice: you now need to manage this component yourself. Are you an expert
    at debugging Kubernetes Ingress controllers? In my experience, better to go all-in
    using standard Ingress objects or stick with pure load balancers if you need to
    save money.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个技巧，但我不推荐的是运行自己的Ingress *控制器*。这种技术（本书未涉及）意味着部署一个开源组件作为负载均衡器来实现Kubernetes Ingress功能，覆盖云提供商的默认实现。这种方法意味着Ingress对象和类型为`LoadBalancer`的服务对象在计费上被视为相同的规则类型，如果你需要两者，这可以节省一些费用，但有一个牺牲：你现在需要自己管理这个组件。你是Kubernetes
    Ingress控制器调试方面的专家吗？根据我的经验，最好是全盘使用标准的Ingress对象，或者如果你需要节省费用，就坚持使用纯负载均衡器。
- en: 7.2.1 Securing connections with TLS
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 使用TLS保护连接
- en: Another useful property of Ingress is that it will perform TLS encryption for
    you. Modern web applications are typically hosted as secure HTTPS applications
    with TLS, which is important for security but comes with some overhead for the
    application server. Depending on the server middleware you are using, you may
    see performance gains by letting the Ingress load balancer handle the TLS connection
    (acting as a so-called TLS terminator) and communicate to the backend only over
    HTTP—via a secure network like that of your cloud provider, of course (figure
    7.5). If you prefer, the Ingress can re-encrypt traffic and connect to your services
    over HTTPS, but there is no option to pass the unmodified encrypted traffic directly
    through from the client to the backend For that, you’d use a Service of type `LoadBalancer`,
    as we did in chapter 3.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress的另一个有用特性是它会为你执行TLS加密。现代Web应用程序通常以安全的HTTPS应用程序的形式托管，使用TLS，这对于安全性很重要，但会给应用程序服务器带来一些开销。根据你使用的服务器中间件，你可能通过让Ingress负载均衡器处理TLS连接（充当所谓的TLS终止器）并通过HTTP与后端通信（当然是通过云提供商的网络安全网络，如图7.5所示）来获得性能提升。如果你愿意，Ingress可以重新加密流量并通过HTTPS连接到你的服务，但没有任何选项可以直接将未修改的加密流量从客户端直接传递到后端。为此，你需要使用类型为`LoadBalancer`的服务，就像我们在第3章中所做的那样。
- en: '![07-05](../../OEBPS/Images/07-05.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![07-05](../../OEBPS/Images/07-05.png)'
- en: Figure 7.5 The Ingress terminates HTTPS (TLS) traffic and can forward it to
    the serving Pods over plain HTTP or HTTPS connections.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 入口终止HTTPS (TLS)流量，并将其通过普通HTTP或HTTPS连接转发到服务Pod。
- en: Now that the Ingress is terminating your TLS connections, you’ll need to set
    it up with certificates. If, like me, you’ve done this a few times on different
    systems, you might be dreading this step. Fortunately, Kubernetes makes it a breeze!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在Ingress正在终止你的TLS连接，你需要使用证书来设置它。如果你像我一样，已经在不同的系统上做过几次，你可能对这一步感到担忧。幸运的是，Kubernetes让这个过程变得非常简单！
- en: You just need to import your certificate and key as a Kubernetes Secret and
    then reference that secret in your Ingress configuration. A Kubernetes Secret
    is simply a data object in your cluster used to contain things like TLS keys.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需要将你的证书和密钥作为Kubernetes密钥导入，然后在Ingress配置中引用该密钥。Kubernetes密钥只是你集群中的一个数据对象，用于包含像TLS密钥这样的东西。
- en: 'To do this, normally you would follow the instructions of your certificate
    authority to create a certificate, the end product of which would include the
    two files we need: the private key that you created and the certificate issued
    by the certificate authority.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，通常你会遵循证书颁发机构的说明来创建证书，最终产品将包括我们需要的两个文件：你创建的私钥和证书颁发机构签发的证书。
- en: 'For demonstration purposes, we can create our own self-signed certificate in
    lieu of a trusted certificate. Note that while this will provide the same encryption
    for the connection, there is no identity verification, and you’ll see scary messages
    in your browser. The following commands will create such a certificate:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示目的，我们可以创建自己的自签名证书来代替受信任的证书。请注意，虽然这将为连接提供相同的加密，但没有身份验证，你会在浏览器中看到一些令人恐惧的消息。以下命令将创建这样的证书：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Once you have your private key and certificate, whether you created them with
    the previous instructions or by following the instructions of your certificate
    authority, you can now create the Kubernetes secret:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了私钥和证书，无论是根据前面的说明创建的还是根据证书颁发机构的说明创建的，你现在可以创建Kubernetes密钥了：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You may notice the imperative `kubectl` `create` command here. This is one of
    the few times I recommend using an imperative command rather than defining the
    configuration in a file because it’s simpler than creating the object manually
    and Base64-encoding all the data. If you want to see the config that got created
    with this command, you can easily view it with `kubectl` `get` `-o` `yaml` `secret`
    `my-tls-cert`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到这里的强制`kubectl` `create`命令。这是我推荐使用强制命令而不是在文件中定义配置的少数几次之一，因为它比手动创建对象并Base64编码所有数据要简单。如果你想查看这个命令创建的配置，你可以很容易地使用`kubectl`
    `get` `-o` `yaml` `secret` `my-tls-cert`来查看。
- en: The final step is to reference this secret in our Ingress, as in the following
    listing.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是在我们的Ingress中引用这个密钥，如下所示。
- en: Listing 7.9 Chapter07/7.2.1_TLS/ingress_tls.yaml
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.9 第07章/7.2.1_TLS/ingress_tls.yaml
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ References the TLS secret
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 引用TLS密钥
- en: 'Referencing the Services of type `NodePort` created in the previous section,
    we can create this new Ingress with a TLS secret:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 引用上一节中创建的`NodePort`类型的服务，我们可以使用TLS密钥创建这个新的Ingress：
- en: '[PRE19]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Remember that the provisioning step can take a while, even from the point where
    the Ingress has received an IP. If you use a self-signed certificate, you will
    see some scary warnings in the browser.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，即使Ingress已经收到了IP，预配步骤也可能需要一段时间。如果你使用的是自签名证书，你会在浏览器中看到一些令人恐惧的警告。
- en: 'To test out the domain name route in this Ingress (`example.com` in the example),
    you’ll need to configure the DNS for the domain you’ve used with the IP of the
    Ingress. To test locally, you can also edit your hosts file and add the IP and
    domain name (to find instructions on how to do that, a Google search for “update
    hosts file in <your operating system version>” should do the trick!). The IP of
    the Ingress can be found with `kubectl` `get` `ingress.` The following is what
    my Ingress object looks like, and the entry I added to my local hosts file:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试这个Ingress中的域名路由（例如示例中的`example.com`），你需要配置你使用的域名的DNS，并使用Ingress的IP。要本地测试，你也可以编辑你的hosts文件并添加IP和域名（要找到如何操作的说明，可以在“如何在<你的操作系统版本>中更新hosts文件”的Google搜索中找到答案！）。你可以使用`kubectl`
    `get` `ingress`来找到Ingress的IP。以下是我的Ingress对象的外观，以及我添加到本地hosts文件中的条目：
- en: '[PRE20]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now, provided that you’ve configured your host, you should be able to browse
    to https://example.com. If you generate a self-signed certificate, you’ll get
    a scary browser error, which, in this case, is fine to click through. To actually
    publish your service to the world, you’ll want to go back and request a certificate
    from an actual certificate authority and use that to create the TLS secret instead.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设你已经配置了你的主机，你应该能够浏览到 https://example.com。如果你生成一个自签名证书，你会得到一个令人恐惧的浏览器错误，在这种情况下，点击通过是可以的。要实际上将你的服务发布到世界，你需要返回并从实际的证书颁发机构请求一个证书，并使用它来创建
    TLS 秘密。
- en: Once again, the nice thing about Kubernetes is that all this configuration is
    in the form of Kubernetes objects rather than random files on a host, making it
    straightforward to reproduce the environment elsewhere.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，Kubernetes 的好处在于所有这些配置都是以 Kubernetes 对象的形式存在，而不是主机上的随机文件，这使得在其他地方重现环境变得简单直接。
- en: Using GKE? Try managed certificates
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GKE？尝试使用托管证书
- en: The previous instructions are for adding a tried-and-true CA certificate to
    your Kubernetes Ingress object. If you’re using Google Kubernetes Engine (GKE)
    and want an even simpler approach, you can use a managed certificate instead.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的说明是关于向你的 Kubernetes Ingress 对象添加经过验证的 CA 证书。如果你使用 Google Kubernetes Engine
    (GKE) 并希望采用更简单的方法，你可以使用托管证书。
- en: With a managed certificate, you skip the CA signing step and the copying of
    your private key and certificate into Kubernetes as a Secret. Instead, you need
    first to prove ownership of the domain to Google (in the Google Cloud console),
    create a GKE-specific ManagedCertificate object listing the (sub)domains you wish
    to provision certificates for, and then reference that object in your Ingress.
    Google will then provision and manage the certificates automatically. It’s all
    pretty straightforward, so I’ll let the official docs^a be your guide for this
    one.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用托管证书，你可以跳过 CA 签名步骤以及将你的私钥和证书复制到 Kubernetes 作为秘密的过程。相反，你首先需要向 Google（在 Google
    Cloud 控制台中）证明你对域的所有权，创建一个 GKE 特定的 ManagedCertificate 对象，列出你希望为哪些（子）域名提供证书，然后在你的
    Ingress 中引用该对象。Google 将自动提供和管理证书。这一切都很简单，所以我会让官方文档^a 成为你的指南。
- en: ^a [https://cloud.google.com/kubernetes-engine/docs/how-to/managed-certs](https://cloud.google.com/kubernetes-engine/docs/how-to/managed-certs)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ^a [https://cloud.google.com/kubernetes-engine/docs/how-to/managed-certs](https://cloud.google.com/kubernetes-engine/docs/how-to/managed-certs)
- en: Summary
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Kubernetes offers several tools to create, discover, connect, and expose multiple
    services when your requirements exceed what can be hosted in a single container.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你的需求超出单个容器可以托管的内容时，Kubernetes 提供了多种工具来创建、发现、连接和公开多个服务。
- en: Internal services are a way to connect a wide range of workloads that can be
    written in a different language, be on a different release schedule, or simply
    need to scale independently.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部服务是一种连接各种工作负载的方式，这些工作负载可以用不同的语言编写，处于不同的发布计划，或者简单地需要独立扩展。
- en: Internal services can be exposed on a cluster IP that allows them to be called
    from other Pods in the cluster.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部服务可以暴露在集群 IP 上，允许它们被集群中的其他 Pods 调用。
- en: 'Kubernetes offers two forms of service discovery to find these internal service
    IPs: environment variables and DNS.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 提供了两种服务发现形式来查找这些内部服务 IP：环境变量和 DNS。
- en: Ingress can be used to expose multiple internal services to the internet using
    a single IP, with routing performed by path or host name.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ingress 可以用来通过单个 IP 向互联网公开多个内部服务，路由通过路径或主机名执行。
- en: Ingress is an HTTP(S) load balancer, which can be configured with multiple TLS
    certificates to perform TLS termination.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ingress 是一个 HTTP(S) 负载均衡器，可以配置多个 TLS 证书以执行 TLS 终止。
- en: By performing TLS termination at the load balancer layer, you can save configuration
    effort in your application and reduce CPU overhead.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在负载均衡器层执行 TLS 终止，你可以节省应用程序的配置工作量并减少 CPU 负载。
- en: '* * *'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.) [https://kubernetes.io/docs/concepts/services-networking/ingress/#multiple-matches](https://kubernetes.io/docs/concepts/services-networking/ingress/#multiple-matches)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ^(1.) [https://kubernetes.io/docs/concepts/services-networking/ingress/#multiple-matches](https://kubernetes.io/docs/concepts/services-networking/ingress/#multiple-matches)
