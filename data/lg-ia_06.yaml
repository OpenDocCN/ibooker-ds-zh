- en: 4 Using Fluentd to output log events
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 使用 Fluentd 输出日志事件
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Using output plugins for files, MongoDB, and Slack
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用文件、MongoDB 和 Slack 的输出插件
- en: Applying different buffering options with Fluentd
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Fluentd 应用不同的缓冲选项
- en: Reviewing the benefits of buffering
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看缓冲的优势
- en: Handling buffer overloads and other risks of buffering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理缓冲过载和其他缓冲风险
- en: Adding formatters to structure log events
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加格式化器以结构化日志事件
- en: Chapter 3 demonstrated how log events can be captured and how helper plugins
    such as parsers come into play. But capturing data is only of value if we can
    do something meaningful with it, such as delivery to an endpoint formatted so
    the log events can be used—for example, storing the events in a log analytics
    engine or sending a message to an operations (Ops) team to investigate. This chapter
    is about showing how Fluentd enables us to do that. We look at how Fluentd output
    plugins can be used from files, as well as how Fluentd works with MongoDB and
    collaboration/social tools for rapid notifications with Slack.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '第 3 章演示了如何捕获日志事件以及辅助插件（如解析器）如何发挥作用。但是，如果我们不能对数据进行有意义的操作，例如将事件交付到格式化后的端点以便使用——例如，将事件存储在日志分析引擎中或将消息发送到操作（Ops）团队进行调查，那么捕获数据就只有价值。本章将展示
    Fluentd 如何使我们能够做到这一点。我们将探讨 Fluentd 输出插件如何从文件中使用，以及 Fluentd 如何与 MongoDB 和 Slack
    的协作/社交工具配合进行快速通知。 '
- en: This chapter will continue to use the LogSimulator, and we will also use a couple
    of other tools, such as MongoDB and Slack. As before, complete configurations
    are available in the download pack from Manning or via the GitHub repository,
    allowing us to focus on the configuration of the relevant plugin(s). Installation
    steps for MongoDB and Slack are covered in appendix A.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将继续使用 LogSimulator，我们还将使用一些其他工具，例如 MongoDB 和 Slack。与之前一样，完整的配置可在 Manning 的下载包中找到，或通过
    GitHub 仓库获取，这样我们就可以专注于相关插件（s）的配置。MongoDB 和 Slack 的安装步骤在附录 A 中介绍。
- en: 4.1 File output plugin
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 文件输出插件
- en: Compared to the *tail* (file input) plugin, we are less likely to use the file
    output plugin, as typically we will want to output to a tool that allows us to
    query, analyze, and visualize the events. There will, of course, be genuine cases
    where file output is needed for production. However, it is one of the best options
    as a stepping-stone to something more advanced, as it is easy to see outcomes
    and the impact of various plugins, such as parsers and filters. Logging important
    events to file also lends itself to easily archiving the log events for future
    reference if necessary (e.g., audit log events to support legal requirements).
    To that end, we will look at the file output before moving on to more sophisticated
    outputs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 与 *tail*（文件输入）插件相比，我们不太可能使用文件输出插件，因为我们通常希望输出到允许我们查询、分析和可视化事件的工具。当然，对于生产环境中确实需要文件输出的情况，这将是最佳选择之一。然而，它是一个很好的起点，因为我们可以轻松地看到各种插件（如解析器和过滤器）的结果和影响。将重要事件记录到文件也便于在必要时轻松存档日志事件以供将来参考（例如，审计日志事件以支持法律要求）。因此，在继续探讨更复杂的输出之前，我们将查看文件输出。
- en: 'With the file output (and, by extension, any output that involves directly
    or indirectly writing physical storage), we need to consider several factors:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件输出（以及由此扩展的任何涉及直接或间接写入物理存储的输出）中，我们需要考虑几个因素：
- en: Where can we write to in the file system, as dictated by storage capacity and
    permissions?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以在文件系统中写入哪里，这取决于存储容量和权限？
- en: Does that location have enough capacity (both allocated and physical capacity)?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 那个位置是否有足够的容量（分配的容量和物理容量）？
- en: How much I/O throughput can the physical hardware deliver?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物理硬件可以提供多少 I/O 吞吐量？
- en: Is there latency on data access (NAS and SAN devices are accessed through networks)?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据访问是否存在延迟（NAS 和 SAN 设备通过网络访问）？
- en: While infrastructure performance isn’t likely to impact development work, it
    is extremely important in preproduction (e.g., performance testing environments)
    and production environments. It is worth noting that device performance is essential
    for the file plugin. Other output plugins are likely to be using services that
    will include logic to optimize I/O (e.g., database caching, optimization of allocated
    file space). With output plugins, we have likely consolidated multiple sources
    of log events. Therefore, we could end up with a configuration that has Fluentd
    writing all the inputs to one file or location. The physical performance considerations
    can be mitigated using buffers (as we will soon see) and caching.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基础设施性能不太可能影响开发工作，但在预生产（例如，性能测试环境）和生产环境中却极为重要。值得注意的是，设备性能对于文件插件至关重要。其他输出插件可能正在使用包括优化
    I/O（例如，数据库缓存、分配文件空间的优化）逻辑的服务。使用输出插件，我们可能已经整合了多个日志事件的来源。因此，我们可能会得到一个配置，其中 Fluentd
    将所有输入写入一个文件或位置。可以通过缓冲区（正如我们很快将看到的）和缓存来减轻物理性能方面的考虑。
- en: 4.1.1 Basic file output
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 基本文件输出
- en: Let’s start with a relatively basic configuration for Fluentd. In all the previous
    chapters’ examples, we have just seen the content written to the console. Now,
    rather than a console, we should simply push everything to a file. To do this,
    we need a new `match` directive in the configuration, but we’ll carry on using
    the file source for log events.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 Fluentd 的一个相对基本的配置开始。在前几章的示例中，我们只看到了写入控制台的内容。现在，而不是控制台，我们应该简单地将所有内容推送到一个文件。为此，我们需要在配置中添加一个新的
    `match` 指令，但我们将继续使用文件源来处理日志事件。
- en: To illustrate that an output plugin can handle multiple inputs within the configuration,
    we have included the self-monitoring source configuration illustrated in the previous
    chapter, in addition to a log file source. To control the frequency of log events
    generated by the Fluentd’s `self_monitor`, we can define another attribute called
    `emit_interval`, which takes a duration value—for example, 10s (10 seconds). The
    value provided by `emit_interval` is the time between log events being generated
    by Fluentd. Self-monitoring can include details like how many events have been
    processed, how many worker processes are managed, and so on.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明输出插件可以在配置中处理多个输入，我们除了包含前一章中展示的自监控源配置外，还包含了一个日志文件源。为了控制 Fluentd 的 `self_monitor`
    生成的日志事件的频率，我们可以定义另一个属性，称为 `emit_interval`，它接受一个持续时间值——例如，10s（10秒）。`emit_interval`
    提供的值是 Fluentd 生成日志事件之间的时间。自监控可以包括诸如已处理的事件数量、管理的工人数等详细信息。
- en: At a minimum, the file output plugin simply requires the `type` attribute to
    be defined and a path pointing to a location for the output using the `path` attribute.
    In the following listing, we can see the relevant parts of our `Chapter4/Fluentd/rotating-file-read-file-out.conf`
    file. The outcome of this configuration may surprise you, but let’s see what happens.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 至少，文件输出插件只需要定义 `type` 属性，并使用 `path` 属性指定输出位置。在以下列表中，我们可以看到我们的 `Chapter4/Fluentd/rotating-file-read-file-out.conf`
    文件的相关部分。这个配置的结果可能会让你感到惊讶，但让我们看看会发生什么。
- en: Listing 4.1 Chapter4/Fluentd/rotating-file-read-file-out.conf—match extract
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.1 Chapter4/Fluentd/rotating-file-read-file-out.conf—match extract
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Changes the plugin type to file
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将插件类型更改为文件
- en: ❷ The location of the file to be written
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 要写入的文件位置
- en: 'The outcome of using this new `match` directive can be seen if the LogSimulator
    and Fluentd are run with the following commands:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此新的 `match` 指令的结果，如果使用以下命令运行 LogSimulator 和 Fluentd，就可以看到：
- en: '`fluentd -c ./Chapter4/Fluentd/rotating-file-read-file-out.conf`'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fluentd -c ./Chapter4/Fluentd/rotating-file-read-file-out.conf`'
- en: '`groovy LogSimulator.groovy ./Chapter4/SimulatorConfig/jul-log-output2.properties./TestData/medium-source.txt`'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groovy LogSimulator.groovy ./Chapter4/SimulatorConfig/jul-log-output2.properties./TestData/medium-source.txt`'
- en: The easy presumption would be that all the content gets written to a file called
    `fluentd-file-output`. However, what has happened is that a folder is created
    using the last part of the path as its name (i.e., `fluentd-file-output`), and
    you will see two files in that folder. The file will appear with a semi-random
    name (to differentiate the different buffer files) and a metadata file with the
    same base name. What Fluentd has done is to implicitly make use of a buffering
    mechanism. The adoption of a buffer with a default option is not unusual in output
    plugins; some do forgo the use of buffering—for example, the *stdout* plugin.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 容易的假设是所有内容都写入一个名为`fluentd-file-output`的文件。然而，实际上发生的是使用路径的最后部分作为其名称创建了一个文件夹（即，`fluentd-file-output`），您将在该文件夹中看到两个文件。文件将以半随机名称出现（以区分不同的缓冲区文件），并且有一个具有相同基本名称的元数据文件。Fluentd所做的是隐式地使用缓冲机制。在输出插件中使用默认选项的缓冲区并不罕见；有些插件放弃了缓冲区的使用——例如，*stdout*插件。
- en: 4.1.2 Basics of buffering
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 缓冲区的基本原理
- en: Buffering, as you may recall from chapter 1, is a Fluentd helper plugin. Output
    plugins need to be aware of the impact they can have on I/O performance. As a
    result, most output plugins use a buffer plugin that can behave *synchronously*
    or *asynchronously*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 缓冲区，如您可能从第1章回忆起来，是Fluentd的一个辅助插件。输出插件需要意识到它们可能对I/O性能产生的影响。因此，大多数输出插件都使用一个可以**同步**或**异步**行为的缓冲区插件。
- en: The synchronous approach means that as log events are collected into *chunks*,
    as soon as a chunk is full, it is written to storage. The asynchronous approach
    utilizes an additional queue stage. The queue stage interacting with the output
    channel is executed in a separate thread, so the filling of chunks shouldn’t be
    impacted by any I/O performance factors.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 同步方法意味着当日志事件收集到*块*中时，一旦块满了，它就被写入存储。异步方法利用一个额外的队列阶段。与输出通道交互的队列阶段在单独的线程中执行，因此块填充不应受到任何I/O性能因素的影响。
- en: The previous example had not explicitly defined a buffer; we saw the output
    plugin applying a default to using a file buffer. This makes more sense when you
    realize that the file output plugin supports the ability to compress the output
    file using gzip, increasing effectiveness the more content you compress at once.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的例子没有明确定义缓冲区；我们看到输出插件应用了默认的文件缓冲区。当你意识到文件输出插件支持使用gzip压缩输出文件的能力时，这更有意义，压缩的内容越多，效果越好。
- en: In figure 4.1, we have numbered the steps. As the arrows’ varying paths indicate,
    steps in the life cycle can be bypassed. All log events start in step 1, but if
    no buffering is in use, the process immediately moves to step 5, where the physical
    I/O operation occurs, and then we progress to step 6\. If there is an error, step
    6 can send the logic back to the preceding step to try again. This is very much
    dependent upon the plugin implementation but is common to plugins such as database
    plugins. If the retries fail or the plugin doesn’t support the concept, some plugins
    support the idea of a secondary plugin to be specified.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在图4.1中，我们已对步骤进行了编号。正如箭头的不同路径所示，生命周期中的步骤可以被绕过。所有日志事件都从步骤1开始，但如果未使用缓冲区，则过程立即移动到步骤5，在那里发生物理I/O操作，然后我们继续到步骤6。如果发生错误，步骤6可以将逻辑发送回前面的步骤再次尝试。这非常依赖于插件实现，但这是数据库插件等插件中常见的做法。如果重试失败或插件不支持该概念，一些插件支持指定二级插件的想法。
- en: '![](../Images/CH04_F01_Wilkins.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1](../Images/CH04_F01_Wilkins.png)'
- en: Figure 4.1 Log event passing through the output life cycle (e.g., the italic
    steps are only used when things go wrong)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 日志事件通过输出生命周期（例如，斜体步骤仅在出现问题时使用）
- en: A secondary plugin is another output plugin that can be called (step 7). Typically,
    a secondary plugin would be as simple as possible to minimize the chance of a
    problem so that the log events could be recovered later. For example, suppose
    the output plugin called a remote service from the Fluentd node (e.g., on a different
    network, in a separate server cluster, or even a data center). In that case, the
    secondary plugin could be a simple file output to a local storage device.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 二级插件是另一个可以被调用的输出插件（步骤7）。通常，二级插件会尽可能简单，以最小化出现问题的可能性，以便稍后可以恢复日志事件。例如，假设输出插件从Fluentd节点调用远程服务（例如，在不同的网络、单独的服务器集群，甚至数据中心）。在这种情况下，二级插件可以是一个简单的文件输出到本地存储设备。
- en: NOTE We would always recommend that a secondary output be implemented with the
    fewest dependencies on software and infrastructure. Needing to fall to a secondary
    plugin strongly suggests broader potential problems. So the more straightforward,
    less dependent it is on other factors, the more likely that the output won’t be
    disrupted. Files support this approach very well.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们始终建议实现一个具有最少软件和基础设施依赖的二级输出。需要降级到二级插件强烈表明存在更广泛的问题。因此，它越简单，对其他因素的依赖性越少，输出就越不可能被中断。文件很好地支持这种方法。
- en: If buffering has been configured, then steps 1 through 3 would be performed.
    But then the following action would depend upon whether the buffering was asynchronous.
    If it was synchronous, then the process would jump to step 5, and we would follow
    the same steps described. For asynchronous buffering, the chunk of logs goes into
    a separate process managing a queue of chunks to be written. Step 4 represents
    the buffer operating asynchronously. As the chunks fill, they are put into a queue
    structure, waiting for the output mechanism to take each chunk to output the content.
    This means that the following log event to be processed is not held up by the
    I/O activity of steps 5 onward.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果已配置缓冲，则步骤 1 到 3 将会被执行。但接下来的操作将取决于缓冲是否为异步。如果是同步的，则过程将跳转到步骤 5，我们将遵循相同的步骤。对于异步缓冲，日志块将进入一个单独的过程，该过程管理一个待写入的块队列。步骤
    4 代表缓冲异步操作。随着块的填充，它们被放入队列结构中，等待输出机制将每个块输出内容。这意味着要处理的下一个日志事件不会被步骤 5 及以后的 I/O 操作所阻塞。
- en: Understanding gzip compression
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 gzip 压缩
- en: Gzip is the GNU implementation of a ZLIB compression format, defined by IETF
    RFC’s 1950, 1951, and 6713\. Zip files use an algorithm known as Lempel-Ziv coding
    (LZ77) to compress the contents. In simple terms, the algorithm works by looking
    for reoccurring patterns of characters; when a reoccurrence is found, that string
    is replaced with a reference to the previous occurrence. So the larger the string
    occurrences identified as reoccurring, the more effective reference becomes, giving
    more compression—the bigger the file, the more likely to find reoccurrences.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Gzip 是 GNU 对 ZLIB 压缩格式的实现，该格式由 IETF RFC 的 1950、1951 和 6713 定义。Zip 文件使用一种称为 Lempel-Ziv
    编码（LZ77）的算法来压缩内容。简单来说，该算法通过寻找字符的重复模式来工作；当找到重复时，该字符串会被替换为对前一次出现的引用。因此，识别为重复的字符串越大，引用就越有效，从而提供更多的压缩——文件越大，找到重复的可能性就越高。
- en: 'Out of the box, Fluentd provides the following buffer types:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Fluentd 提供以下缓冲类型：
- en: '*Memory*'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*内存*'
- en: '*File*'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文件*'
- en: As you may have realized, the path is used as the folder location to hold its
    buffered content and contains both the content and a metadata file. Using file
    I/O for the buffer doesn’t give much of a performance boost in terms of the storage
    device unless you establish a *RAM disk* (aka, *RAM drive)*. A file-based buffer
    still provides some benefits; the way the file is used is optimized (keeping files
    open, etc.). It also acts as a staging area to accumulate content before applying
    compression (as noted earlier, the more data involved in a zip compression, the
    greater the compression possible). In addition, the logged content won’t be lost
    as a result of some form of process or hardware failure, and the buffer can be
    picked back up when the server and/or Fluentd restart.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所意识到的那样，路径被用作文件夹位置来存储其缓冲内容，并包含内容和元数据文件。使用文件 I/O 作为缓冲在存储设备方面不会带来很大的性能提升，除非您建立了一个
    *RAM 磁盘*（也称为 *RAM 驱动器*）。基于文件的缓冲仍然提供一些好处；文件的使用方式被优化（保持文件打开等）。它还充当一个临时区域，在应用压缩之前积累内容（如前所述，参与
    zip 压缩的数据越多，可能的压缩就越大）。此外，由于某种形式的进程或硬件故障，记录的内容不会丢失，并且当服务器和/或 Fluentd 重新启动时，缓冲可以重新恢复。
- en: NOTE RAM drives work by allocating a chunk of memory for storage and then telling
    the OS’s file system that it is an additional storage device. Applications that
    use this storage believe they are writing to a physical device like a disk, but
    the content is actually written to memory. More information can be found at [www.techopedia.com/definition/2801/ram-disk](http://www.techopedia.com/definition/2801/ram-disk).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：RAM 驱动通过为存储分配一块内存，然后告诉操作系统的文件系统它是一个额外的存储设备。使用此存储的应用程序认为它们正在写入一个像磁盘这样的物理设备，但实际上内容是写入内存的。更多信息可以在
    [www.techopedia.com/definition/2801/ram-disk](http://www.techopedia.com/definition/2801/ram-disk)
    找到。
- en: With buffers in many plugins being involved by default, or explicitly, we should
    look at how to start configuring buffer behaviors. We know when the events move
    from the buffer to the output destination, how frequently such actions occur,
    and how these configurations impact performance. The life cycle illustrated in
    figure 4.1 provides clues as to the configuration possibilities.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多插件默认或明确地涉及缓冲区，我们应该看看如何开始配置缓冲区行为。我们知道当事件从缓冲区移动到输出目标时，此类动作发生的频率，以及这些配置如何影响性能。图
    4.1 中所示的生命周期图提供了配置可能性的线索。
- en: 4.1.3 Chunks and Controlling Buffering
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.3 块和缓冲区控制
- en: As figure 4.1 shows, the buffer’s core construct is the idea of a chunk. The
    way we configure chunks, aside from synchronous and asynchronous, will influence
    the performance. Chunks can be controlled through the allocation of storage space
    (allowing for a reserved piece of contiguous memory or disk to be used) or through
    a period. For example, all events during a period go into a single chunk, or a
    chunk will continue to fill with events until a specific number of log events
    or the chunk reaches a certain size. Separation of the I/O from the chunk filling
    is beneficial if there is a need to provide connection retries for the I/O, as
    might be the case with shared services or network remote services, such as databases.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 4.1 所示，缓冲区的核心结构是块的概念。我们配置块的方式，除了同步和异步之外，还会影响性能。块可以通过分配存储空间（允许使用连续的内存或磁盘的一部分）或通过时间段来控制。例如，一个时间段内的所有事件都进入一个块，或者一个块将继续填充事件，直到达到特定的日志事件数量或块达到一定的大小。如果需要为
    I/O 提供连接重试，这种分离 I/O 和块填充是有益的，例如在共享服务或网络远程服务（如数据库）的情况下。
- en: In both approaches, it is possible through the configuration to set attributes
    so that log events don’t end up lingering in the buffer because a threshold is
    never fully met. Which approach to adopt will be influenced by the behavior of
    your log event source and the tradeoff of performance against resources available
    (e.g., memory), as well as the amount of acceptable latency in the log events
    moving downstream.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在两种方法中，通过配置可以设置属性，使得日志事件不会因为阈值从未完全满足而滞留在缓冲区中。采用哪种方法将受你的日志事件源的行为以及性能与可用资源（例如，内存）之间的权衡以及可接受的延迟量（在日志事件向下移动时）的影响。
- en: Personally, I tend to use size constraints, which provides a predictable system
    behavior; this may reflect my Java background and preference not to start unduly
    tuning the virtual machine.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 个人而言，我倾向于使用大小约束，这提供了可预测的系统行为；这可能反映了我 Java 背景和偏好，不希望过度调整虚拟机。
- en: Table 4.1 shows the majority of the possible controls on a buffer. Where the
    control can have some subtlety in how it can behave, we’ve included more elaboration.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1 显示了缓冲区上大多数可能的控制。当控制可以有一些微妙的行为时，我们包括了更详细的说明。
- en: Table 4.1 Buffer configuration controls
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1 缓冲区配置控制
- en: '| Attribute | Description |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 属性 | 描述 |'
- en: '| `timekey` | This is the number of seconds each chunk will be responsible
    for holding (by default, 1 day). The `time` attribute of a `log_event` then determines
    which chunk to add the event to. For example, if our timekey was set to 300 (seconds)
    and the chunk started on the hour, then when an event timestamped 10:00:01 arrived
    and further events arrived every 30 seconds, an additional 9 more events would
    be held in the first chunk. The next chunk would hold events that started arriving
    after 10:05:00, so the next event would be 10:05:01. |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| `timekey` | 这是指每个块将负责保持的秒数（默认为 1 天）。`log_event` 的 `time` 属性然后确定将事件添加到哪个块。例如，如果我们的
    timekey 设置为 300（秒）且块从小时开始，那么当 10:00:01 标记的事件到达，并且每 30 秒到达更多事件时，第一个块将保留额外的 9 个事件。下一个块将保留
    10:05:00 后开始到达的事件，因此下一个事件将是 10:05:01。 |'
- en: '| `timekey` | If we had additional out-of-order events arriving before 10:05
    (e.g., with timestamps 10:03:15 and 10:03:55), but they didn’t arrive until 10:04:31,
    then they would still be added to the first chunk.This behavior can be further
    modified by the `timekey_wait` attribute. |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| `timekey` | 如果在 10:05 之前有额外的乱序事件到达（例如，带有时间戳 10:03:15 和 10:03:55），但它们直到 10:04:31
    才到达，那么它们仍然会被添加到第一个块中。这种行为可以通过 `timekey_wait` 属性进一步修改。 |'
- en: '| `timekey_wait` | This is the number of seconds after the end of a chunk’s
    storage period before the chunk is written. This defaults to 60 seconds.Extending
    our `timekey` example, if this value was set to 60s (60 seconds), then that chunk
    would be held in memory until 10:06 before being flushed. If another event was
    received at 10:05:21 with a timestamp of 10:04:49, this would go in our first
    chunk, not the chunk covering the received time. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| `timekey_wait` | 这是块存储周期结束后，块写入之前需要等待的秒数。默认为 60 秒。扩展我们的 `timekey` 示例，如果此值设置为
    60s（60 秒），则该块将在 10:06 之前保留在内存中，然后才会刷新。如果收到的时间戳为 10:04:49 的另一个事件在 10:05:21，这将进入我们的第一个块，而不是覆盖接收时间的块。
    |'
- en: '| `chunk_limit_size` | This defines the maximum size of a chunk, which defaults
    to 8 MB for memory and 256 MB for a file buffer. The chances of increasing this
    threshold are small, but you may consider reducing it to limit the maximum footprint
    of a container or constraints of an IoT device. Remember that you can operate
    multiple chunks. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| `chunk_limit_size` | 这定义了块的最大大小，默认为内存中的 8 MB 和文件缓冲区中的 256 MB。提高此阈值的可能性很小，但您可以考虑将其降低以限制容器的最大占用空间或物联网设备的限制。请记住，您可以操作多个块。
    |'
- en: '| `chunk_limit_records` | This defines the maximum number of log events in
    a single chunk. If the size of log events can fluctuate wildly in size, this will
    need to be considered. A number of large logs could create a very large chunk,
    creating risks around memory exhaustion and varying durations for writing chunks.
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| `chunk_limit_records` | 这定义了单个块中日志事件的最多数量。如果日志事件的大小波动很大，则需要考虑这一点。大量的大型日志可能会创建一个非常大的块，从而产生内存耗尽和块写入持续时间变化的风险。
    |'
- en: '| `total_limit_size` | This is the limit of storage allowed for all chunks
    before the new events received will be dropped with error events lost accordingly.This
    defaults to 512 MB for memory and 64 GB for file. |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| `total_limit_size` | 这是所有块允许的存储限制，在新事件接收之前，将丢弃错误事件。默认为内存中的 512 MB 和文件中的 64
    GB。 |'
- en: '| `chunk_full_``threshold` | Once the percentage of the buffer’s capacity exceeds
    this value, the chunk is treated as full and moved to the I/O stage. This defaults
    to 0.95\. If log events are very large relative to the allocated memory, you may
    consider lowering this threshold to ensure more predictable performance, particularly
    if you limit the queue size. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| `chunk_full_`阈值` | 一旦缓冲区容量的百分比超过此值，块就被视为已满，并移动到 I/O 阶段。默认为 0.95。如果日志事件相对于分配的内存非常大，您可以考虑降低此阈值以确保更可预测的性能，尤其是如果您限制了队列大小。
    |'
- en: '| `queued_chunks_limit_size` | This defines the number of chunks in the queue
    waiting to be persisted as required. Ideally, this should never be larger than
    the `flush_thread_count`.The default value is 1. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `queued_chunks_limit_size` | 这定义了队列中等待按需持久化的块的数量。理想情况下，这个值不应大于 `flush_thread_count`。默认值是
    1。 |'
- en: '| `compress` | This accepts only the values of either `text` (default) or `gzip`.
    When `gzip` is set, then compression will be applied. If other compression mechanisms
    are introduced, the options available will expand. |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `compress` | 仅接受 `text`（默认）或 `gzip` 的值。当设置为 `gzip` 时，则应用压缩。如果引入了其他压缩机制，则可用的选项将扩展。
    |'
- en: '| `flush_at_shutdown` | This tells the buffer whether it should write everything
    to the output before allowing Fluentd to shut down gracefully. For the memory
    buffer, this defaults to true but is false for a file, as the contents can be
    recovered on startup. We recommend setting it to true in most cases, given that
    you may not know when Fluentd will restart and process the cached events (if it
    can). |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| `flush_at_shutdown` | 这告诉缓冲区是否应在允许 Fluentd 优雅地关闭之前将所有内容写入输出。对于内存缓冲区，默认为 true，但对于文件，默认为
    false，因为内容可以在启动时恢复。鉴于您可能不知道 Fluentd 将何时重新启动并处理缓存的事件（如果可以的话），我们建议在大多数情况下将其设置为 true。
    |'
- en: '| `flush_interval` | This is a duration defining how frequently the buffered
    content should be written to the output storage mechanism. This means we can configure
    behavior centered on either volumes or time intervals. This defaults to 60 seconds
    (60s). |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| `flush_interval` | 这是一个持续时间，定义了缓冲内容应该写入输出存储机制的频率。这意味着我们可以配置基于体积或时间间隔的行为。默认为
    60 秒（60s）。 |'
- en: '| `flush_mode` | Accepted values are'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '| `flush_mode` | 接受的值是'
- en: '`default`—Uses `lazy` if chunk keys are defined, otherwise `interval`'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`default`—如果定义了块键，则使用 `lazy`，否则使用 `interval`'
- en: '`lazy`—Flush/write chunks once per `timekey`.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lazy`—每次 `timekey` 时刷新/写入块一次。'
- en: '`interval`—Flush/write chunks per specified time via `flush_interval`.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`interval`—通过 `flush_interval` 指定的时间间隔刷新/写入块。'
- en: '`immediate`—Flush/write chunks immediately after events are appended into chunks.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`immediate`—在事件被追加到块中后立即刷新/写入块。'
- en: '|'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| `flush_thread_count` | The number of threads to be used to write chunks.
    A number above 1 will create parallel threads—this may not be desirable depending
    on the output type. For example, if a connection pool to a database can handle
    multiple connections, more than 1 is worth considering. But more than 1 on a file
    could create contention or write collisions. This defaults to 1. |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| `flush_thread_count` | 用于写入块的线程数量。大于1的数字将创建并行线程——这取决于输出类型，可能不是所希望的。例如，如果数据库连接池可以处理多个连接，那么超过1是值得考虑的。但在文件上超过1可能会造成竞争或写入冲突。默认为1。|'
- en: '| `flush_thread_interval` | The length of time the flush thread should sleep
    before looking to see if a flush is required. Expressed as a number of seconds
    in floating-point format and defaults to 1. |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| `flush_thread_interval` | 清洗线程在检查是否需要刷新之前应该休眠的时间长度。以浮点数格式表示秒数，默认为1。|'
- en: '| `delayed_commit_timeout` | When using the asynchronous I/O, we need to set
    a maximum time to allow the thread to run before we assume it must be experiencing
    an error. If this time is exceeded (default 60s), then the thread is stopped.
    This needs to be tuned to take into account how responsive the target system is.
    For example, writing large chunks to a remote database will take longer than writing
    small chunks to a local file system. |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| `delayed_commit_timeout` | 当使用异步I/O时，我们需要设置一个最大时间，允许线程运行，在我们认为它必须遇到错误之前。如果这个时间超过了（默认60秒），则停止线程。这需要根据目标系统的响应性进行调整。例如，将大块数据写入远程数据库将比将小块数据写入本地文件系统花费更长的时间。|'
- en: '| `overflow_action` | If the input into the buffer is faster than we can write
    content out of the buffer, we will experience an overflow condition. This configuration
    allows us to define how to address that problem. Options are'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '| `overflow_action` | 如果缓冲区的输入速度超过我们能够从缓冲区中写入内容的速度，我们将遇到溢出条件。此配置允许我们定义如何解决这个问题。选项有'
- en: '`throw_exception`—Throw an exception that will appear in the Fluentd log as
    a `BufferOverflowError`; this is the default.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`throw_exception`—抛出一个异常，该异常将作为`BufferOverflowError`出现在Fluentd日志中；这是默认设置。'
- en: '`block`—Block input processing to allow events to be written.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`block`—阻止输入处理以允许写入事件。'
- en: '`interval`—Flush/write chunks per specified time via `flush_interval`.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`interval`—通过`flush_interval`按指定时间刷新/写入块。'
- en: '`drop_oldest_chunk`—Drop the oldest chunk of data to free a chunk up for use.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drop_oldest_chunk`—丢弃最旧的块数据以释放一个块供使用。'
- en: Throwing exceptions may be okay when an overflow scenario is never expected,
    and the potential loss of log events is a risk worth taking. But in more critical
    areas, we would suggest consciously choosing an alternative, such as `interval`.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抛出异常在永远不会预期溢出场景时可能是可以接受的，并且潜在的日志事件丢失是一个值得承担的风险。但在更关键的区域，我们建议有意识地选择一个替代方案，例如`interval`。
- en: '|'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: We can amend the configuration with the understanding of buffer behavior (or
    use the prepared one). As we recommend flushing on shutdown, we should set this
    to true (`flush_at_shutdown true`). As we want to quickly see the impact of the
    changes, let’s set the maximum number of records to 10 (`chunk_limit_records 10`)
    and the maximum time before flushing a chunk to 30 seconds (`flush_interval 30`).
    Otherwise, if we have between 1 and 9 log events in the buffer, they’ll never
    get flushed if the sources stop creating log events. Finally, we have added an
    extra layer of protection in our configuration by imposing a time-out of the buffer
    write process. We can see this in the following listing.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过理解缓冲区行为（或使用准备好的配置）来修改配置。由于我们建议在关闭时刷新，我们应该将其设置为true（`flush_at_shutdown
    true`）。由于我们希望快速看到更改的影响，让我们将最大记录数设置为10（`chunk_limit_records 10`）并将刷新块的最大时间设置为30秒（`flush_interval
    30`）。否则，如果缓冲区中有1到9个日志事件，如果源停止创建日志事件，它们将永远不会被刷新。最后，我们在配置中增加了一层额外的保护，通过强制执行缓冲区写入过程的时间超时。我们可以在以下列表中看到这一点。
- en: Listing 4.2 Chapter4/Fluentd/rotating-file-read-file-out2.conf—match extract
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.2 第4章/Fluentd/rotating-file-read-file-out2.conf—匹配提取
- en: '[PRE1]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ By default, the file buffer doesn’t flush on shutdown, as events won’t be
    lost by stopping the Fluentd instance. However, it is desirable to see all events
    completed at shutdown. There is the risk that a configuration change will mean
    the file buffer isn’t picked up on restart, resulting in log events effectively
    being in limbo.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 默认情况下，文件缓冲区在关闭时不会刷新，因为停止Fluentd实例不会导致事件丢失。然而，在关闭时看到所有事件完成是可取的。存在这样的风险，即配置更改意味着文件缓冲区在重启时不会被选中，导致日志事件实际上处于悬而未决的状态。
- en: ❷ As we understand our log content and want to see things happen very quickly,
    we will use several logs rather than the capacity to control the chunk size, which
    indirectly influences how soon events are moved from the buffer to the output
    destination.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 由于我们理解日志内容并希望事情发生得非常快，我们将使用多个日志而不是控制块大小的能力，这间接影响事件从缓冲区移动到输出目标的速度。
- en: ❸ As the buildup of self-monitoring events will be a lot slower than our file
    source, forcing a flush on time as well will ensure we can see these events come
    through to the output at a reasonable frequency.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 由于自我监控事件的累积速度将比我们的文件源慢得多，因此强制按时间刷新也将确保我们可以以合理的频率看到这些事件通过输出。
- en: 'To run this scenario, let’s reset (delete the `structured-rolling-log.*` and
    `rotating-file-read.pos_file` files and the `fluentd-file-output` folder) and
    run again, using each of these commands in a separate shell:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行这个场景，让我们重置（删除`structured-rolling-log.*`和`rotating-file-read.pos_file`文件以及`fluentd-file-output`文件夹）并再次运行，在每个单独的shell中使用这些命令：
- en: '`fluentd -c Chapter4/Fluentd/rotating-file-read-file-out2.conf`'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fluentd -c Chapter4/Fluentd/rotating-file-read-file-out2.conf`'
- en: '`groovy LogSimulator.groovy Chapter4/SimulatorConfig/jul-log-file2.properties
    ./TestData/medium-source.txt`'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groovy LogSimulator.groovy Chapter4/SimulatorConfig/jul-log-file2.properties
    ./TestData/medium-source.txt`'
- en: Do not shut down Fluentd once the log simulator has completed. We will see that
    the folder `fluentd-file-output` is still created with both buffer files as before.
    But at the same time, we will see files with the naming `of fluentd-file-output.<date>_<incrementing
    number>.log` (e.g., `fluentd-file-output.20200505_12.log`). Open any one of these
    files, and you will see 10 lines of log data. You will notice that the log data
    is formatted as a date timestamp, tag name, and then the payload body, reflecting
    the standard composition of a log event. If you scan through the files, you will
    find incidents where the tag is not simpleFile but self. This reflects that we
    have kept the source reporting on the self-monitoring and matches with our simpleFile,
    which is tracking the rotating log files.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦日志模拟器完成，不要关闭Fluentd。我们会看到文件夹`fluentd-file-output`仍然被创建，并且像之前一样包含缓冲文件。但与此同时，我们还会看到以`fluentd-file-output.<date>_<incrementing
    number>.log`（例如，`fluentd-file-output.20200505_12.log`）命名的文件。打开这些文件中的任何一个，你会看到10行日志数据。你会注意到日志数据是以日期时间戳、标签名称，然后是有效负载体格式化的，反映了日志事件的标准化组成。如果你浏览这些文件，你会找到标签不是simpleFile而是self的事件。这反映了我们保留了源报告在自我监控上，并且与我们的simpleFile相匹配，simpleFile正在跟踪旋转日志文件。
- en: 'Finally, close down Fluentd gracefully. In Windows, the easiest way to do this
    is in the shell: press `CTRL-c` once (and only once), and respond yes to the shutdown
    prompt (in Linux, the interrupt events can be used). Once we can see that Fluentd
    has shut down in the console, look for the last log file and examine it. There
    is an element of timing involved, but if you inspect the last file, chances are
    it will have less than 10 records in it, as the buffer will have flushed to the
    output file whatever log events it had at shutdown.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，优雅地关闭Fluentd。在Windows中，在shell中这样做最简单：按一次`CTRL-c`（并且只按一次），然后对关闭提示响应yes（在Linux中，可以使用中断事件）。一旦我们可以在控制台中看到Fluentd已关闭，查找最后一个日志文件并检查它。这涉及到一些时间因素，但如果检查最后一个文件，它很可能包含少于10条记录，因为缓冲区在关闭时将刷新到输出文件中任何日志事件。
- en: Buffer sizing error
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 缓冲区大小错误
- en: If you set a buffer to be smaller than a single log event, then handling that
    log event will fail, with an error like
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你设置缓冲区小于单个日志事件，那么处理该日志事件将失败，出现如下错误
- en: '[PRE2]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 4.1.4 Retry and backoff
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.4 重试和退避
- en: The use of buffers also allows Fluentd to provide a retry mechanism. In the
    event of issues like transient network drops, we can tell the buffer when it recognizes
    an issue to perform a retry rather than just losing the log events. For retries
    to work without creating new problems, we need to define controls that tell the
    buffer how long or how many times to retry before abandoning data. In addition
    to this, we can define how long to wait before retrying. We can stipulate retrying
    forever (attribute `retry_forever` is set to true), but we recommend using such
    an option with great care.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 缓冲区的使用还允许Fluentd提供重试机制。在出现如短暂网络中断等问题的情况下，我们可以告诉缓冲区在识别到问题时进行重试，而不是仅仅丢失日志事件。为了使重试工作而不产生新的问题，我们需要定义控制措施，告诉缓冲区在放弃数据之前重试多长时间或多少次。此外，我们还可以定义在重试之前等待多长时间。我们可以规定无限期重试（将`retry_forever`属性设置为true），但我们建议非常谨慎地使用此类选项。
- en: There are two ways for retry to work using the retry_type attribute—through
    either a fixed interval (`periodic`) or through an exponential backoff (`exponential_backoff`).
    The exponential backoff is the default model, and each retry attempt that fails
    results in the retry delay doubling. For example, if the retry interval was 1
    second, the second retry would be 2 seconds, the third retry 4 seconds, and so
    on. We can control the initial or repeated wait period between retries by defining
    `retry_wait` with a numeric value representing seconds (e.g., 1 for 1 second and
    60 for 1 minute).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`retry_type`属性有两种方式来实现重试：通过固定间隔（`periodic`）或通过指数回退（`exponential_backoff`）。指数回退是默认模型，每次重试失败都会导致重试延迟加倍。例如，如果重试间隔是1秒，第二次重试将是2秒，第三次重试是4秒，依此类推。我们可以通过定义`retry_wait`（使用表示秒数的数值）来控制重试之间的初始或重复等待周期。例如，1表示1秒，60表示1分钟。
- en: Unless we want to retry forever, we need to provide a means to determine whether
    or not to keep retrying. For the periodic retry model, we can control this by
    number or time. This is done by either setting a maximum period to retry writing
    each chunk (`retry_timeout`) or a maximum number of retry attempts (`retry_max_attempts`).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 除非我们希望无限期地重试，否则我们需要提供一种方法来确定是否继续重试。对于周期性重试模型，我们可以通过数量或时间来控制。这是通过设置重试写入每个块的最大周期（`retry_timeout`）或最大重试尝试次数（`retry_max_attempts`）来实现的。
- en: For the backoff approach, we can stipulate a number of backoffs (`retry_exponential_backoff_base`)
    or the maximum duration that a backoff can go before stopping (`retry_max_interval`).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回退方法，我们可以规定回退次数（`retry_exponential_backoff_base`）或回退可以持续的最大时长，在停止之前（`retry_max_interval`）。
- en: Suppose we wanted to configure the buffer retry to be an exponential backoff
    starting at 3 seconds. With a maximum of 10 attempts, we could end up with a peak
    retry interval of nearly 26 minutes. The configuration attributes we would need
    to configure are
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要配置缓冲重试为从3秒开始的指数回退。在最多10次尝试的情况下，我们可能会达到近26分钟的重试间隔峰值。我们需要配置的配置属性包括
- en: '[PRE3]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The important thing with the exponential backoff is to ensure you’re aware of
    the possible total time. Once that exponential curve gets going, the time extends
    very quickly. In this example, the first 5 retries would happen inside a minute,
    but intervals really start to stretch out after that.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 指数回退的重要之处在于确保你了解可能的总时间。一旦指数曲线开始，时间会迅速延长。在这个例子中，前5次重试将在一分钟内发生，但之后间隔会真正开始拉长。
- en: 4.1.5 Putting configuring buffering size settings into action
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.5 将配置缓冲大小设置付诸实践
- en: You’ve been asked to help the team better understand buffering. There is an
    agreement that an existing configuration should be altered to help this. Copy
    the configuration file `/Chapter4/Fluentd/rotating-file-read-file-out2.conf` and
    modify it so that the configuration of the buffer chunks is based on the size
    of 500 bytes (see appendix B for how to express storage sizes).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你被要求帮助团队更好地理解缓冲机制。有一个共识，即应该修改现有的配置以帮助实现这一点。复制配置文件 `/Chapter4/Fluentd/rotating-file-read-file-out2.conf`
    并修改它，以便缓冲块配置基于500字节的尺寸（参见附录B了解如何表示存储大小）。
- en: Run the modified configuration to show the impact on the output files.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 运行修改后的配置以展示对输出文件的影响。
- en: As part of the discussion, one of the questions that came up is if the output
    plugin is subject to intermittent network problems, what options do we have to
    prevent the loss of any log information?
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论过程中，出现的一个问题是如果输出插件受到间歇性网络问题的影响，我们有哪些选项可以防止任何日志信息的丢失？
- en: Answer
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 答案
- en: The following listing shows the buffer configuration that would be included
    in the result.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了将包含在结果中的缓冲区配置。
- en: Listing 4.3 Chapter4/ExerciseResults/rotating-file-read-file-out2-Answer.conf
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.3 `Chapter4/ExerciseResults/rotating-file-read-file-out2-Answer.conf`
- en: '[PRE4]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Note that we’ve retained the delay and flush time, so if the buffer stops
    filling, it will get forced out anyway.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 注意，我们保留了延迟和刷新时间，所以如果缓冲区停止填充，它仍然会被强制输出。
- en: ❷ Size-based constraint rather than time-based
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 基于大小的约束而不是基于时间的
- en: 'A complete configuration file is provided at `Chapter4/ExerciseResults/rotating
    -file-read-file-out2-Answer.conf`. The rate of change to the log files is likely
    to appear different, but the same content will be there. To address the question
    of mitigating the risk of log loss, several options could be applied:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的配置文件在`Chapter4/ExerciseResults/rotating -file-read-file-out2-Answer.conf`中提供。日志文件的变化率可能看起来不同，但内容将是相同的。为了解决减轻日志丢失风险的问题，可以应用几种选项：
- en: Configure the retry and backoff parameters on the buffer to retry storing the
    events rather than losing the information.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在缓冲区上配置重试和退避参数，以便在丢失信息之前重试存储事件。
- en: Use the capability of defining a *secondary* log mechanism, such as a local
    file, so the events are not lost. Provide a means for the logs to be injected
    into the Kafka stream at a later date. This could even be an additional source.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用定义一个*二级*日志机制的能力，例如本地文件，这样事件就不会丢失。提供一种方法，以便在稍后日期将日志注入到Kafka流中。这甚至可以是一个额外的来源。
- en: 4.2 Output formatting options
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 输出格式化选项
- en: How we structure the output of the log events is as important as how we apply
    structure to the input. Not surprisingly, a formatter plugin can be included in
    the output plugin. With a formatter plugin, it’s reasonable to expect several
    prebuilt formatters. Let’s look at the out-of-the-box formatters typically encountered;
    the complete set of formatters are detailed in appendix C.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何结构化日志事件的输出，与我们如何对输入应用结构一样重要。不出所料，格式化插件可以包含在输出插件中。有了格式化插件，合理地期望有几个预构建的格式化器。让我们看看通常遇到的“开箱即用”的格式化器；完整的格式化器集在附录C中详细说明。
- en: 4.2.1 out_file
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 输出文件
- en: 'This is probably the simplest formatter available and has already been implicitly
    used. The formatter works using a delimiter between the values `time`, `tag,`
    and `record`. By default, the *delimiter* is a tab character. This can only be
    changed to a comma or space character using the values `comma` or `space` for
    the attribute delimiter; for example, `delimiter comma`. Which fields are output
    can also be controlled with Boolean-based attributes:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是最简单的格式化器，并且已经被隐式使用。格式化器通过在`time`、`tag`和`record`值之间使用分隔符来工作。默认情况下，`*分隔符*`是一个制表符字符。这只能通过将属性分隔符的值更改为`comma`或`space`来更改，例如，`delimiter
    comma`。也可以通过基于布尔值的属性来控制输出哪些字段：
- en: '`output_tag`—This takes a true or false value to determine whether the tag(s)
    are included in the line (by default, the second value in the line).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_tag`—这个属性接受真或假值来决定是否在行中包含标签（默认情况下，行中的第二个值）。'
- en: '`output_time`—This takes true or false to define whether the time is included
    (by default, the first value in the line). You may wish to omit the time if you
    have it incorporated into the core event record.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_time`—这个属性接受真或假来定义是否包含时间（默认情况下，行中的第一个值）。如果你已经在核心事件记录中包含了时间，你可能希望省略时间。'
- en: '`time_format`—This can be used to define the way the date and time are used
    in the path. If undefined and the timekey is set, then the timekey will inform
    the structure of the format. Specifically:'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`time_format`—这个属性可以用来定义日期和时间在路径中使用的方式。如果未定义且设置了timekey，则timekey将告知格式的结构。具体来说：'
- en: 0...60 seconds then `'%Y%m%d%H%M%S'`
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0...60秒则使用`'%Y%m%d%H%M%S'`
- en: 60...3600 seconds then `'%Y%m%d%H%M'`
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 60...3600秒则使用`'%Y%m%d%H%M'`
- en: 3600...86400 seconds then `'%Y%m%d%H'`
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3600...86400秒则使用`'%Y%m%d%H'`
- en: NOTE If the formatter does not recognize the attribute value set, it will ignore
    the value provided and use the default value.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果格式化器无法识别设置的属性值，它将忽略提供的值并使用默认值。
- en: 4.2.2 json
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 JSON
- en: This formatter treats the log event record as a JSON payload on a single line.
    The timestamp and tags associated with the event are discarded.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个格式化器将日志事件记录视为单行上的JSON有效负载。与事件关联的时间戳和标签被丢弃。
- en: 4.2.3 ltsv
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.3 LTSV
- en: 'As with the `out_file` formatter and the parser, the limiters can be changed
    using *delimiter* (each labeled value) and `label_delimiter` for separating the
    value and label. For example, if delimiter was set to `delimiter ;` and `label_delimiter
    =` was set, then if the record was expressed as `{"my1stValue":" blah", "secondValue":
    "more blah", "thirdValue": "you guessed – blah"}`, the output would become `my1st
    Value=blah; secondValue= more blah; thirdValue=you guessed – blah.`'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '与`out_file`格式化程序和解析器一样，可以使用`*delimiter*`（每个标签值）和`label_delimiter`来更改限制器，以分隔值和标签。例如，如果将分隔符设置为`delimiter
    ;`并且设置了`label_delimiter =`，那么如果记录表示为`{"my1stValue":" blah", "secondValue": "more
    blah", "thirdValue": "you guessed – blah"`，输出将变为`my1st Value=blah; secondValue=
    more blah; thirdValue=you guessed – blah.`。'
- en: As the values are label values, the need for separating records by lines is
    reduced, and therefore it is possible to switch off the use of new lines to separate
    each record with `add_newline false` (by default, the value is set to true).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 由于值是标签值，因此减少了对按行分隔记录的需求，因此可以通过将`add_newline false`（默认值为true）关闭来停止使用新行来分隔每个记录。
- en: 4.2.4 csv
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.4 csv
- en: Just like ltsv and `out_file` the delimiter can be defined by setting the attribute
    `delimiter`. The attribute has a default, which is a comma. Additionally, the
    csv output allows us to define which values can be included in the output using
    the `fields` attribute. If I use the record again to illustrate if my event is
    `{"my1stValue":" blah"`, `"secondValue":" more blah", "thirdValue":"you guessed
    – blah}`, and I set the fields attribute to be the `secondValue, thirdValue fields`,
    then the output would be `"more blah", "you guessed – blah"`. If desired, the
    quoting of each value can be disabled with a Boolean value for `force_quotes`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 就像ltsv和`out_file`一样，可以通过设置属性`delimiter`来定义分隔符。该属性有一个默认值，即逗号。此外，csv输出允许我们使用`fields`属性定义可以包含在输出中的值。如果我用记录再次说明，如果我的事件是`{"my1stValue":"
    blah"`，`"secondValue":" more blah"`，`"thirdValue":"you guessed – blah"`，并且我将字段属性设置为`secondValue,
    thirdValue fields`，那么输出将是`"more blah"`，`"you guessed – blah"`。如果需要，可以通过`force_quotes`的布尔值禁用每个值的引号。
- en: 4.2.5 msgpack
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.5 msgpack
- en: As with the *msgpack* parser, the formatter works with the MessagePack framework,
    which takes the core log event record and uses the MessagePack library to compress
    the content. Typically, we would only expect to see this used with HTTP forward
    output plugins where the recipient expects MessagePack content. To get similar
    compression performance gains, we can use gzip for file and block storage.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 与`msgpack`解析器一样，格式化程序与MessagePack框架一起工作，该框架使用核心日志事件记录，并使用MessagePack库来压缩内容。通常，我们只期望在期望接收MessagePack内容的HTTP转发输出插件中使用它。为了获得类似的压缩性能提升，我们可以使用gzip进行文件和块存储。
- en: NOTE You may have noticed that most formatters (`out_file` being an exception)
    omit adding the time and keys from the log event. As a result, if you want those
    to be carried through, it will be necessary to ensure that they are incorporated
    into the log event record or that the output plugin utilizes the values explicitly.
    Adding additional data into the log event payload can be done using the *inject*
    plugin, which can be used within a *match* or *filter* directive. We will pick
    up the inject feature when we address filtering in chapter 6.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你可能已经注意到，大多数格式化程序（`out_file`除外）省略了从日志事件中添加时间和键。因此，如果你想保留这些信息，将需要确保它们被包含在日志事件记录中，或者输出插件明确地使用这些值。可以使用`*inject*`插件将附加数据添加到日志事件有效负载中，该插件可以在`*match*`或`*filter*`指令中使用。我们将在第6章讨论过滤时介绍注入功能。
- en: 4.2.6 Applying formatters
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.6 应用格式化程序
- en: We can extend our existing configuration to move from the current implicit configuration
    to being explicit. Let’s start with the simplest output using the default `out_
    file` formatter, using a comma as a delimiter (`delimiter comma`) and excluding
    the log event tag (`output_tag false`). We will continue to use the same sources
    as before to demonstrate the effect of formatters. This `out_file` formatter configuration
    is illustrated in the following listing.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将现有的配置扩展为从当前的隐式配置转换为显式配置。让我们从使用默认的`out_file`格式化程序开始，使用逗号作为分隔符（`delimiter
    comma`）并排除日志事件标签（`output_tag false`）。我们将继续使用之前相同的源来演示格式化程序的效果。以下列表显示了此`out_file`格式化程序配置。
- en: Listing 4.4 Chapter4/Fluentd/rotating-file-read-file-out3.conf—formatter configuration
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.4 第4章/Fluentd/rotating-file-read-file-out3.conf—格式化程序配置
- en: '[PRE5]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ To reduce the number of files being generated, we have made the number of
    records a lot larger per file.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为了减少生成的文件数量，我们已将每个文件中的记录数设置得很大。
- en: ❷ We explicitly define the output formatter to be out_file, as we want to override
    the default formatter behavior.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们明确地将输出格式器定义为 out_file，因为我们想覆盖默认的格式化行为。
- en: ❸ Replace the tab delimiter with a comma.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将制表符分隔符替换为逗号。
- en: ❹ Exclude the tag information from the output.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从输出中排除标签信息。
- en: 'Assuming the existing log files and outputs have been removed, we can start
    the example using the following commands:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 假设现有的日志文件和输出已经被移除，我们可以使用以下命令开始示例：
- en: '`fluentd -c Chapter4/Fluentd/rotating-file-read-file-out3.conf`'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fluentd -c Chapter4/Fluentd/rotating-file-read-file-out3.conf`'
- en: '`groovy LogSimulator.groovy Chapter4/SimulatorConfig/jul-log-file2.properties
    ./TestData/medium-source.txt`'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groovy LogSimulator.groovy Chapter4/SimulatorConfig/jul-log-file2.properties
    ./TestData/medium-source.txt`'
- en: 4.2.7 Putting JSON formatter configuration into action
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.7 将 JSON 格式化配置投入实际应用
- en: Your organization has decided that as standard practice, all output should be
    done using JSON structures. This approach ensures that any existing or applied
    structural meaning to the log events is not lost. To support this goal, the configuration
    file `/Chapter4/Fluentd/rotating-file-read-file-out3.conf` will need modifying.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 您的组织已决定，作为标准做法，所有输出都应使用 JSON 结构完成。这种方法确保了日志事件中任何现有的或应用的结构意义不会丢失。为了支持这一目标，需要修改配置文件
    `/Chapter4/Fluentd/rotating-file-read-file-out3.conf`。
- en: Answer
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 答案
- en: The format declaration in the configuration file should be reduced to look like
    the fragment in the following listing.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件中的格式声明应减少到类似于以下列表中的片段。
- en: Listing 4.5 Chapter4/ExerciseResults/rotating-file-file-out3-Answer.conf
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.5 Chapter4/ExerciseResults/rotating-file-file-out3-Answer.conf
- en: '[PRE6]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: A complete example configuration to this answer can be found in `/Chapter4/
    ExerciseResults/rotating-file-read-file-out3-Answer.conf`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 本答案的完整示例配置可以在 `/Chapter4/ ExerciseResults/rotating-file-read-file-out3-Answer.conf`
    中找到。
- en: 4.3 Sending log events to MongoDB
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 将日志事件发送到 MongoDB
- en: While outputting to some form of file-based storage is a simple and easy way
    of storing log events, it doesn’t lend itself well to performing any analytics
    or data processing. To make log analysis a practical possibility, Fluentd needs
    to converse with systems capable of performing analysis, such as SQL or NoSQL
    database engines, search tools such as Elasticsearch, and even SaaS services such
    as Splunk and Datadog.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然将日志事件输出到某种形式的基于文件的存储是一种简单且易于存储日志事件的方法，但它并不适合执行任何分析或数据处理。为了使日志分析成为实际可能，Fluentd
    需要与能够执行分析的系统进行交互，例如 SQL 或 NoSQL 数据库引擎、搜索工具如 Elasticsearch，甚至是 SaaS 服务如 Splunk
    和 Datadog。
- en: Chapter 1 highlighted that Fluentd has no allegiance to any particular log analytics
    engine or vendor, differentiating Fluentd from many other tools. As a result,
    many vendors have found Fluentd to be an attractive solution to feed log events
    to their product or service. To make adoption very easy, vendors have developed
    their adaptors.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 第 1 章强调了 Fluentd 对任何特定的日志分析引擎或供应商都没有忠诚度，这使得 Fluentd 与许多其他工具区分开来。因此，许多供应商发现 Fluentd
    是将日志事件输入其产品或服务的有吸引力的解决方案。为了使采用变得非常容易，供应商已经开发了他们的适配器。
- en: We have opted to use MongoDB to help illustrate the approach for getting events
    into a tool capable of analytical processing logs. While MongoDB is not dedicated
    to textual search like Elasticsearch, its capabilities are well suited to log
    events with a good JSON structure. MongoDB is very flexible and undemanding in
    getting started, so don’t worry if you’ve not used MongoDB. The guidance for installing
    MongoDB can be found in appendix A.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择使用 MongoDB 来帮助说明将事件输入到能够进行日志分析的工具的方法。虽然 MongoDB 不像 Elasticsearch 那样专注于文本搜索，但其功能非常适合具有良好
    JSON 结构的日志事件。MongoDB 在入门时非常灵活且要求不高，所以如果你没有使用过 MongoDB，请不要担心。MongoDB 的安装指南可以在附录
    A 中找到。
- en: Summary of MongoDB
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB 概述
- en: We do not want to get too sidetracked by the mechanics of MongoDB, but it’s
    worth summarizing some basic ideas. Most readers will be familiar with relational
    databases and their concepts. The role of the database structure in both MongoDB
    and a relational DB is analogous. Within a database schema is a set of tables.
    In MongoDB, the nearest to this is a *collection*. Unlike a relational database,
    a collection can contain pretty much anything, which is why it is sometimes described
    as using a *document model*. You could say that each entry in a collection is
    roughly comparable to a record in a table containing a DB-assigned ID and a BLOB
    (binary large object) or text data type. Commonly the row or document of MongoDB
    is a structured text object, usually JSON. MongoDB can then search and index parts
    of these structures, making for a flexible solution. For Fluentd, it means we
    can store log events that may have different record structures.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不希望过多地偏离 MongoDB 的机制，但总结一些基本概念是值得的。大多数读者都熟悉关系型数据库及其概念。数据库结构在 MongoDB 和关系型数据库中的作用是相似的。在数据库模式中有一组表。在
    MongoDB 中，与之最接近的是“集合”。与关系型数据库不同，集合可以包含几乎所有内容，这就是为什么它有时被描述为使用“文档模型”。可以说，集合中的每个条目大致相当于一个包含
    DB 分配的 ID 和 BLOB（大型二进制对象）或文本数据类型的表中的记录。通常，MongoDB 的行或文档是一个结构化文本对象，通常是 JSON 格式。MongoDB
    可以然后搜索和索引这些结构的一部分，从而提供一种灵活的解决方案。对于 Fluentd 来说，这意味着我们可以存储可能具有不同记录结构的日志事件。
- en: More recent versions of the MongoDB engine provide the means to validate the
    content structure going into a collection. This provides some predictability in
    the content. If this is used, then we can exploit Fluentd to structure the necessary
    payload.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 更新版的 MongoDB 引擎提供了验证进入集合的内容结构的手段。这为内容提供了一定的可预测性。如果使用此功能，那么我们可以利用 Fluentd 来结构化必要的有效负载。
- en: In addition to controlling how strictly the contents must adhere to a structure
    for each document, Mongo also incorporates the idea of *capped size*. This feature
    allows us to set limits on the amount of storage used by the collection, and the
    collection operates as a FIFO (first in, first out) list.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 除了控制每个文档的内容必须严格遵循结构外，Mongo 还引入了“固定大小”的概念。此功能允许我们限制集合使用的存储量，并且集合作为FIFO（先进先出）列表运行。
- en: 'When you want to empty a table in a relational database, it is often easier
    to simply drop and re-create the table. The MongoDB equivalent is to delete the
    collection; however, if this is the only collection within the database, the database
    will be removed by MongoDB. There are two options: only delete the collection’s
    contents and keep the collection, or create a second empty collection so the database
    is not empty.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当您想要清空关系型数据库中的表时，通常简单地删除并重新创建表会更简单。MongoDB 的等效操作是删除集合；然而，如果这是数据库中唯一的集合，MongoDB
    将会删除整个数据库。有两种选择：仅删除集合的内容并保留集合，或者创建一个空的第二个集合，以保持数据库不为空。
- en: You can discover a lot more with the book *MongoDB in Action* by Kyle Banker,
    et al. ([www.manning.com/books/mongodb-in-action-second-edition](https://www.manning.com/books/mongodb-in-action-second-edition)).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过阅读Kyle Banker等人所著的《MongoDB in Action》一书来发现更多内容（[www.manning.com/books/mongodb-in-action-second-edition](https://www.manning.com/books/mongodb-in-action-second-edition)）。
- en: 4.3.1 Deploying MongoDB Fluentd plugin
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 部署 MongoDB Fluentd 插件
- en: The MongoDB input plugin is provided in the Treasure Data Agent build of Fluentd
    but not in the vanilla deployment. If we want Fluentd to work with MongoDB, we
    need to install the RubyGem if it isn’t already in place.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB 输入插件包含在 Fluentd 的 Treasure Data Agent 构建中，但不包含在标准部署中。如果我们想让 Fluentd
    与 MongoDB 一起工作，我们需要安装 RubyGem，如果它尚未安装的话。
- en: To determine whether the installation is needed and perform installations of
    gems, we can use a wrapper utility that leverages the RubyGems tool called *fluent-gem*.
    To see if the gem is already installed, run the command `fluent-gem list` from
    the command line. The command will show the locally installed gems, which contain
    Fluentd and its plugins. At this stage, there should be no indication of a *fluent-plugin-mongo*
    gem. We can, therefore, perform the installation using the command `fluent -gem
    install fluent-plugin-mongo`. This will retrieve and install the latest stable
    instance of the gem, including the documentation and dependencies, such as the
    MongoDB driver.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定是否需要安装以及执行 gems 的安装，我们可以使用一个利用 RubyGems 工具的包装工具，称为 *fluent-gem*。要查看 gem
    是否已经安装，请在命令行中运行 `fluent-gem list` 命令。该命令将显示本地安装的 gems，其中包含 Fluentd 及其插件。在此阶段，不应有任何关于
    *fluent-plugin-mongo* gem 的指示。因此，我们可以使用命令 `fluent -gem install fluent-plugin-mongo`
    来执行安装。这将检索并安装该 gem 的最新稳定版本，包括文档和依赖项，如 MongoDB 驱动程序。
- en: 4.3.2 Configuring the Mongo output plugin for Fluentd
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 配置 Fluentd 的 Mongo 输出插件
- en: Within a match, we need to reference the `mongo` plugin and set the relevant
    attributes. Like connecting to any database, we will need to provide an address
    (which could be achieved via a *host* [name] and *port* [number on the network])
    and user and password, or via a *connection* *string* (e.g., `mongodb://127.0.0.1:27017/Fluentd`).
    In our example configuration, we have adopted the former approach and avoided
    the issue of credentials. The database (schema) and collection (table comparable
    to a relational table) are needed to know where to put the log events.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个匹配规则中，我们需要引用 `mongo` 插件并设置相关属性。就像连接到任何数据库一样，我们需要提供一个地址（可以通过 *host* [名称] 和
    *port* [网络上的端口号] 实现）以及用户名和密码，或者通过 *connection* *string*（例如，`mongodb://127.0.0.1:27017/Fluentd`）。在我们的示例配置中，我们采用了前一种方法并避免了凭证问题。需要数据库（模式）和集合（类似于关系型数据库中的表）来确定日志事件放置的位置。
- en: Within a MongoDB output, there is no use of a formatter, as the MongoDB plugin
    assumes all content is already structured. As we have seen, without configuration,
    a default buffer will be adopted. As before, we’ll keep the buffer settings to
    support only low volumes so we can see things changing quickly. We can see the
    outcome in the following listing.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MongoDB 输出中，不需要使用格式化工具，因为 MongoDB 插件假设所有内容已经结构化。正如我们所见，如果没有配置，将采用默认的缓冲区。像以前一样，我们将保持缓冲区设置以支持低流量，这样我们可以快速看到变化。我们可以在以下列表中看到结果。
- en: Listing 4.6 Chapter4/Fluentd/rotating-file-read-mongo-out.conf—match configuration
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.6 Chapter4/Fluentd/rotating-file-read-mongo-out.conf—匹配配置
- en: '[PRE7]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Mongo is the plugin name.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Mongo 是插件名称。
- en: ❷ Identifies the MongoDB host server. In our dev setup, that’s simply the local
    machine; this could alternatively be defined by using a connection attribute.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 识别 MongoDB 主机服务器。在我们的开发设置中，这仅仅是本地机器；这也可以通过使用连接属性来定义。
- en: ❸ The port on the target server to communicate with. We can also express the
    URI by combining hostp, port, and database into a single string.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 与目标服务器通信的端口。我们也可以通过将主机名、端口和数据库组合成一个字符串来表示 URI。
- en: ❹ As a MongoDB installation can support multiple databases, we need to name
    the database.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 由于 MongoDB 安装可以支持多个数据库，我们需要命名数据库。
- en: ❺ The collection within the database we want to add log events to—a rough analogy
    to an SQL-based table
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 我们想要添加日志事件的数据库中的集合——类似于基于 SQL 的表
- en: ❻ Buffer configuration to ensure the log events are quickly incorporated into
    the MongoDB
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 缓冲区配置以确保日志事件能够快速集成到 MongoDB 中
- en: Mongo vs Mongo replica set plugins
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Mongo 与 Mongo 副本集插件
- en: If you’ve looked at the Fluentd online documentation, you may have observed
    two output plugins, *out_mongo* and *out_mongo_replset*. The key difference is
    that the *replset* (replica set) can support MongoDB’s approach to enable scaling,
    where additional instances of Mongo can be defined as replicas of a master. When
    this happens, the ideal model is to directly write activity to the master, but
    read from the replicas. In terms of configuration differences, a comma-separated
    list of nodes is needed rather than naming a single host. Each node represents
    a node in the replica group (e.g., `nodes 192.168.0.10:27017, 192.168.0.20:27017,
    192.168.0.30:27017`). The replica set name is also needed (e.g., `replica_set
    myFluentReps`). More information on the Mongo replica set mechanism is documented
    at [https://docs.mongodb.com/manual/replication/](https://docs.mongodb.com/manual/replication/).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经查看过Fluentd在线文档，你可能已经注意到两个输出插件，*out_mongo*和*out_mongo_replset*。关键区别在于*replset*（副本集）可以支持MongoDB的扩展方法，其中可以定义Mongo的额外实例作为主副本。当这种情况发生时，理想的模式是直接将活动写入主节点，但从副本节点读取。在配置差异方面，需要一个以逗号分隔的节点列表，而不是指定单个主机名。每个节点代表副本组中的一个节点（例如，`nodes
    192.168.0.10:27017, 192.168.0.20:27017, 192.168.0.30:27017`）。还需要副本集名称（例如，`replica_set
    myFluentReps`）。有关Mongo副本集机制的更多信息，请参阅[https://docs.mongodb.com/manual/replication/](https://docs.mongodb.com/manual/replication/)。
- en: Ensure that the MongoDB instance is running (this can be done with the command
    `mongod --version` in a shell window or by attempting to connect to the server
    using the Compass UI). If the MongoDB server isn’t running, then it will need
    to be started. The simplest way to do that is to run the command `mongod`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 确保MongoDB实例正在运行（这可以通过在shell窗口中运行命令`mongod --version`或通过使用Compass UI尝试连接到服务器来完成）。如果MongoDB服务器没有运行，那么它需要被启动。最简单的方法是运行命令`mongod`。
- en: 'With Mongo running, we can then run our simulated logs and Fluentd configuration
    with the commands in each shell:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 当Mongo运行时，我们可以在每个shell中运行我们的模拟日志和Fluentd配置命令：
- en: '`groovy LogSimulator.groovy Chapter4/SimulatorConfig/jul-log-file2-exercise.properties
    ./TestData/medium-source.txt`'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groovy LogSimulator.groovy Chapter4/SimulatorConfig/jul-log-file2-exercise.properties
    ./TestData/medium-source.txt`'
- en: '`fluentd -c Chapter4/fluentd/rotating-file-read-mongo-out.conf`'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fluentd -c Chapter4/fluentd/rotating-file-read-mongo-out.conf`'
- en: Mongo plugin startup warning
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Mongo插件启动警告
- en: 'When Fluentd is started up with the MongoDB plugin, it will log the following
    warning: `[mongo-output]`. Since v0.8, invalid record detection will be removed
    because mongo driver v2.x and API spec don''t provide it. You may lose invalid
    records, so you should not send such records to the Mongo plugin.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 当Fluentd与MongoDB插件一起启动时，它将记录以下警告：`[mongo-output]`。从v0.8版本开始，无效记录检测将被移除，因为mongo驱动程序v2.x和API规范不提供它。你可能会丢失无效记录，因此你不应该将此类记录发送到Mongo插件。
- en: This essentially means that the MongoDB driver being used does not impose any
    structural checks on the payload (which the collection may be configured to require).
    As a result, if strong checking is being applied by the MongoDB engine, it may
    drop an update, but the information will not get passed back through the driver,
    so Fluentd will be none the wiser, resulting in data loss. For the more common
    application of Fluentd, it is better not to impose strict checks onto the payload.
    If this isn’t an option, then a filter directive could identify log events that
    will fail MongoDB’s checks.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上意味着正在使用的MongoDB驱动程序不对有效负载（集合可能已配置为需要）进行任何结构检查。因此，如果MongoDB引擎正在应用强检查，它可能会丢弃更新，但信息不会通过驱动程序返回，因此Fluentd将一无所知，导致数据丢失。对于Fluentd更常见的应用，最好不对有效负载施加严格的检查。如果这不是一个选项，那么一个过滤器指令可以识别将失败MongoDB检查的日志事件。
- en: '![](../Images/CH04_F02_Wilkins.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH04_F02_Wilkins.png)'
- en: Figure 4.2 MongoDB is viewed through the Compass UI tool, with logged content.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2通过Compass UI工具查看MongoDB，其中包含日志内容。
- en: Viewing log events in MongoDB
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在MongoDB中查看日志事件
- en: 'To see how effective MongoDB can be for querying the JSON log events, if you
    add the expression `{"msg" : {$regex : ".*software.*"}}` into the FILTER field
    and click FIND, we’ll get some results. These results will show the log events
    where the `msg` contains the word software. The query expression tells MongoDB
    to look in the documents, and if they have a top-level element called `msg`, then
    evaluate this value using the regular expression.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '要查看MongoDB在查询JSON日志事件方面的有效性，如果您将表达式`{"msg" : {$regex : ".*software.*"}}`添加到FILTER字段并点击FIND，我们将得到一些结果。这些结果将显示包含单词software的`msg`的日志事件。查询表达式告诉MongoDB在文档中查找，如果它们有一个名为`msg`的顶级元素，则使用正则表达式评估此值。'
- en: If you examine the content in MongoDB, you will see that the content stored
    is only the core log event record, not the associated time or tags.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您检查MongoDB中的内容，您将看到存储的内容仅仅是核心日志事件记录，而不是相关的时间或标签。
- en: 'To empty the collection for further executions of the scenario in a command
    shell, run the following command:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 要在命令行中清空集合以便进一步执行场景，请运行以下命令：
- en: '[PRE8]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The mongo plugin has a couple of other tricks up its sleeve. When the attribute
    `tag_mapped` is included in the configuration, the tag name is used as the collection
    name, and MongoDB will create the collection if it does not exist. This makes
    it extremely easy to separate the log events into different collections. If the
    tag names have been used hierarchically, then the tag prefix can be removed to
    simplify the *tag_ mapped* feature. This can be defined by the `remove_tag_prefix`
    attribute, which takes the name of the prefix to remove.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: mongo插件还有一些其他技巧。当配置中包含`tag_mapped`属性时，标签名称用作集合名称，如果不存在，MongoDB将创建该集合。这使得将日志事件分离到不同的集合变得极其简单。如果标签名称已经按层次结构使用，则可以删除标签前缀以简化`tag_mapped`功能。这可以通过`remove_tag_prefix`属性定义，它接受要删除的前缀名称。
- en: As collections can be established dynamically, it is possible to define characteristics
    within the configuration; for example, whether the collection should be capped
    in size.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 由于集合可以动态建立，可以在配置中定义特性；例如，集合是否应该限制大小。
- en: In this configuration, we have not formally defined any username or password.
    This is because in the configuration of MongoDB, we have not imposed the credentials
    restrictions. And incorporating credentials in the Fluentd configuration file
    is not the best practice. Techniques for safely handling credentials within Fluentd
    configuration, not just for MongoDB but also for other systems needing authentication,
    are addressed in chapter 7.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在此配置中，我们没有正式定义任何用户名或密码。这是因为我们在MongoDB配置中没有强制凭证限制。在Fluentd配置文件中包含凭证也不是最佳实践。第7章中讨论了在Fluentd配置中安全处理凭证的技术，不仅适用于MongoDB，也适用于需要身份验证的其他系统。
- en: 4.3.3 Putting MongoDB connection configuration strings into action
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 将MongoDB连接配置字符串投入实际应用
- en: 'Revise the configuration to define the connection by a connection attribute,
    not the host, port, and database used. This is best started by copying the configuration
    file `Chapter4/fluentd/rotating-file-read-mongo-out.conf`. Adjust the run commands
    to use the new configuration. The commands should look something like this:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 修改配置以通过连接属性定义连接，而不是使用主机、端口和数据库名。这最好从复制配置文件`Chapter4/fluentd/rotating-file-read-mongo-out.conf`开始。调整运行命令以使用新的配置。命令看起来可能如下所示：
- en: '`groovy LogSimulator.groovy Chapter4/SimulatorConfig/jul-log-file2-exercise.properties
    ./TestData/medium-source.txt`'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groovy LogSimulator.groovy Chapter4/SimulatorConfig/jul-log-file2-exercise.properties
    ./TestData/medium-source.txt`'
- en: '`fluentd -c Chapter4/fluentd/my-rotating-file-read-mongo-out.conf`'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fluentd -c Chapter4/fluentd/my-rotating-file-read-mongo-out.conf`'
- en: Answer
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 答案
- en: The configuration of the MongoDB connection should look like the configuration
    shown in the following listing.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB连接的配置应类似于以下列表中所示的配置。
- en: Listing 4.7 Chapter4/ExerciseResults/rotating-file-read-mongo-out-Answer.conf
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.7 Chapter4/ExerciseResults/rotating-file-read-mongo-out-Answer.conf
- en: '[PRE9]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Note the absence of the host, port, and database properties in favor of this.
    You could have used your host’s specific IP or 127.0.0.1 instead of localhost.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 注意，为了这个目的，省略了主机、端口和数据库名属性。您可以使用您主机的特定IP地址或127.0.0.1而不是localhost。
- en: The example configuration can be seen at `Chapter4/ExerciseResults/rotating-file-read-mongo-out-Answer.conf`
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 示例配置可以在`Chapter4/ExerciseResults/rotating-file-read-mongo-out-Answer.conf`中查看。
- en: Before we move on from MongoDB, we have focused on the output use of MongoDB
    so it can be used to query log events. But MongoDB can also act as an input into
    Fluentd, allowing new records added to MongoDB to be retrieved as log events.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们离开MongoDB之前，我们关注的是MongoDB的输出用途，以便它可以用于查询日志事件。但MongoDB也可以作为Fluentd的输入，允许新添加到MongoDB的记录作为日志事件检索。
- en: 4.4 Actionable log events
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 可采取行动的日志事件
- en: 'In chapter 1, we introduced the idea of making log events actionable. So far,
    we have seen ways to unify the log events. To make log events actionable, we need
    a couple of elements:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1章中，我们介绍了使日志事件可采取行动的想法。到目前为止，我们已经看到了统一日志事件的方法。为了使日志事件可采取行动，我们需要几个元素：
- en: The ability to send events to an external system that can trigger an action,
    such as a collaboration/notification platform that Ops people can see and react
    to log events or even invoke a script or tool to perform an action
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够将事件发送到外部系统，该系统能够触发动作，例如Ops人员可以看到并对此做出反应的协作/通知平台，或者调用脚本或工具执行动作
- en: Separating the log events that need to be made actionable from those that provide
    information but require no specific action
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将需要采取行动的日志事件与仅提供信息但不需要特定行动的事件分开
- en: The process of separating or filtering out events that need to be made actionable
    is addressed later in the book. But here, we can look at how to make events actionable
    without waiting until all the logs have arrived in an analytics platform and running
    its analytics processes.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的后面部分将讨论如何分离或过滤出需要采取行动的事件。但在这里，我们可以看看如何在所有日志都到达分析平台并运行其分析过程之前，如何使事件可采取行动。
- en: 4.4.1 Actionable log events through service invocation
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.1 通过服务调用实现可采取行动的日志事件
- en: 'One way to make a log event actionable is to invoke a separate application
    that can perform the necessary remediation as a result of receiving an API call.
    This could be as clever as invoking an *Ansible Tower* REST API ([http://mng.bz/Nxm1](http://mng.bz/Nxm1))
    to initiate a template job that performs some housekeeping (e.g., moving logs
    to archive storage or tell Kubernetes about the internal state of a pod). We would
    need to control how frequently an action is performed; plugins such as *flow_counter*
    and *notifier* can help. To invoke generic web services, we can use the HTTP output
    plugin, which is part of the core of Fluentd. To give a sense of the art of the
    possible, here is a summary of the general capabilities supported by this plugin:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 使日志事件可采取行动的一种方法是通过调用一个可以执行必要修复的应用程序，作为接收API调用的结果。这可能就像调用 *Ansible Tower* REST
    API ([http://mng.bz/Nxm1](http://mng.bz/Nxm1)) 来启动一个执行一些日常维护工作（例如，将日志移动到存档存储或告知Kubernetes有关Pod的内部状态）的模板作业一样聪明。我们需要控制动作执行的频率；例如，*flow_counter*
    和 *notifier* 这样的插件可以帮助。为了调用通用网络服务，我们可以使用Fluentd核心部分之一的HTTP输出插件。为了给出可能的技艺感，以下是此插件支持的一般功能的总结：
- en: Supports HTTP *post* and *put* operations
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持HTTP *post* 和 *put* 操作
- en: Allows proxy in the routing
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许在路由中使用代理
- en: Configures content type, setting the formatter automatically (a formatter can
    also be defined explicitly)
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置内容类型，自动设置格式化器（格式化器也可以显式定义）
- en: Defines headers so extra header values required can be defined (e.g., API keys)
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义头信息，以便可以定义所需的额外头值（例如，API密钥）
- en: Configures the connection to use SSL and TLS certificates, including defining
    the location of the certificates to be used, versions, ciphers to be used, etc.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置连接以使用SSL和TLS证书，包括定义要使用的证书的位置、版本、要使用的加密套件等。
- en: Creates log errors when nonsuccessful HTTP code responses are received
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当收到非成功的HTTP代码响应时，创建日志错误
- en: Supports basic authentication (no support for OAuth at the time of writing)
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持基本身份验证（截至编写时，不支持OAuth）
- en: Sets time-outs
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置超时
- en: Uses buffer plugins
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用缓冲区插件
- en: 4.4.2 Actionable through user interaction tools
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.2 通过用户交互工具实现可采取行动
- en: 'While automating problem resolution in the manner described takes us toward
    self-healing systems, not many organizations are prepared for or necessarily want
    to be this advanced. They would rather trust a quick human intervention to determine
    cause and effect than rely on an automated diagnosis where a high degree of certainty
    can be difficult. People with sufficient knowledge and the appropriate information
    can quickly determine and resolve such issues. As a result, Fluentd has a rich
    set of plugins for social collaboration mechanisms. Here are just a few examples:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式自动化问题解决使我们朝着自愈系统迈进，但并非许多组织都为这种高级状态做好了准备，或者必然想要达到这种程度。他们更愿意信任快速的人工干预来确定因果关系，而不是依赖于自动化诊断，在自动化诊断中，高度的确定性可能很难实现。拥有足够知识和适当信息的人可以迅速确定并解决此类问题。因此，Fluentd有一套丰富的插件，用于社交协作机制。以下只是几个例子：
- en: IRC (Internet Relay Chat) ([https://tools.ietf.org/html/rfc2813](https://tools.ietf.org/html/rfc2813))
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IRC (互联网中继聊天) ([https://tools.ietf.org/html/rfc2813](https://tools.ietf.org/html/rfc2813))
- en: Twilio (supporting many different comms channels) ([www.twilio.com](https://www.twilio.com))
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Twilio (支持许多不同的通讯渠道) ([www.twilio.com](https://www.twilio.com))
- en: Jabber ([https://xmpp.org](https://xmpp.org))
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jabber ([https://xmpp.org](https://xmpp.org))
- en: Redmine ([www.redmine.org](https://www.redmine.org))
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redmine ([www.redmine.org](https://www.redmine.org))
- en: Typetalk ([www.typetalk.com](https://www.typetalk.com))
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Typetalk ([www.typetalk.com](https://www.typetalk.com))
- en: PagerDuty ([www.pagerduty.com](https://www.pagerduty.com))
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PagerDuty ([www.pagerduty.com](https://www.pagerduty.com))
- en: Yammer ([www.microsoft.com/en-gb/microsoft-365/yammer](https://www.microsoft.com/en-gb/microsoft-365/yammer))
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yammer ([www.microsoft.com/en-gb/microsoft-365/yammer](https://www.microsoft.com/en-gb/microsoft-365/yammer))
- en: Slack ([https://slack.com/](https://slack.com/))
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Slack ([https://slack.com/](https://slack.com/))
- en: Obviously, we do need to include the relevant information in the social channel
    communications. A range of things can be done to help that, from good log events
    that are clear and can be linked to guidance for resolution, to Fluentd being
    configured to extract relevant information from log events to share.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们需要在社交渠道通信中包含相关信息。为了帮助做到这一点，可以采取一系列措施，从清晰的日志事件，这些事件可以链接到解决方案指南，到将Fluentd配置为从日志事件中提取相关信息以共享。
- en: 4.5 Slack to demonstrate the social output
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 Slack 示例以展示社交输出
- en: Slack has become a leading team messaging collaboration tool with a strong API
    layer and a free version that is simply limited by the size of conversation archives.
    As a cloud service, it is a great tool to illustrate the intersection of Fluentd
    and actionable log events through social platforms. While the following steps
    are specific to Slack, the principles involved here are the same for Microsoft
    Teams, Jabber, and many other collaboration services.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Slack已经成为一个领先的团队消息协作工具，它拥有强大的API层和一个免费版本，该版本仅受对话存档大小的限制。作为一个云服务，它是展示Fluentd与通过社交平台可操作日志事件交集的绝佳工具。虽然以下步骤是针对Slack的，但涉及的原则对Microsoft
    Teams、Jabber以及许多其他协作服务都是相同的。
- en: If you are already using Slack, it is tempting to use an existing group to run
    the example. To avoid irritating the other users of your Slack workspace as a
    result of them receiving notifications and messages as you fill channels up with
    test log events, we suggest setting up your own test workspace. If you aren’t
    using Slack, that’s not a problem; in appendix A, we explain how to get a Slack
    account and configure it to be ready for use. Ensure you keep a note of the API
    token during the Slack configuration, recognizable by the prefix `xoxb`.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经在使用Slack，可能会倾向于使用现有的群组来运行示例。为了避免在你用测试日志事件填满频道时，其他Slack工作空间用户收到通知和消息而感到烦恼，我们建议设置你自己的测试工作空间。如果你没有使用Slack，那也不是问题；在附录A中，我们解释了如何获取Slack账户并将其配置为准备使用。确保你在Slack配置过程中记下API令牌，该令牌以`xoxb`为前缀。
- en: Slack provides a rich set of configuration options for the interaction. Unlike
    MongoDB and many IaaS- and PaaS-level plugins, as Slack is a SaaS service, the
    resolution of the Slack instance is both simplified and hidden from us by using
    a single `token` (no need for server addresses, etc.). The `username` isn’t about
    credentials, but how to represent the bot that the Fluentd plugin behaves as;
    therefore, using a meaningful name is worthwhile. The `channel` relates to the
    Slack channel in which the messages sent will be displayed. The general channel
    exists by default, but you may wish to create a custom channel in Slack and restrict
    access to that channel if you want to control who sees the messages. After all,
    do you want everyone within an enterprise Slack setup to see every operational
    message?
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Slack 提供了一组丰富的配置选项以供交互使用。与 MongoDB 和许多 IaaS- 和 PaaS-级别的插件不同，由于 Slack 是一个 SaaS
    服务，因此 Slack 实例的解析既简化了，又通过使用单个 `token`（无需服务器地址等）隐藏于我们面前。`username` 并非关于凭证，而是关于如何表示
    Fluentd 插件所扮演的机器人；因此，使用一个有意义的名称是值得的。`channel` 与消息将被显示的 Slack 频道相关。通用频道默认存在，但如果你想在
    Slack 中创建一个自定义频道并限制对该频道的访问，以便控制谁可以看到消息，你可能希望这样做。毕竟，你希望企业 Slack 设置中的每个人都看到每个操作消息吗？
- en: The `message` and `message_keys` attributes work together with the message using
    `%s` to indicate where the values of the identified payload elements are inserted.
    The references in the `message` relate to the JSON payload elements listed in
    the `message_keys` in order sequence.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`message` 和 `message_keys` 属性与消息一起使用，使用 `%s` 来指示已识别的有效负载元素的值插入的位置。`message`
    中的引用与在 `message_keys` 中按顺序列出的 JSON 有效负载元素相关。'
- en: The `title` and `title_keys` work in a similar way to `message` and `message_keys`*,*
    but for the title displayed in the Slack UI with that message. In our case, we’re
    just going to use the `tag`. The final part is the `flush` attribute; this tells
    the plugin how quickly to push the Slack messages to the user. Multiple messages
    can be grouped up if the period is too long. To keep things moving quickly, let’s
    flush every second.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`title` 和 `title_keys` 与 `message` 和 `message_keys` 的工作方式类似，但用于 Slack UI 中显示的消息的标题。在我们的案例中，我们只是将使用
    `tag`。最后一部分是 `flush` 属性；这告诉插件如何快速将 Slack 消息推送给用户。如果周期太长，可以将多个消息分组。为了保持快速流动，让我们每秒刷新一次。'
- en: Edit the existing configuration provided (`Chapter4/Fluentd/rotating-file -read-slack-out.conf`)
    to incorporate the details captured, in the Slack setup. This is illustrated in
    the following listing.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑提供的现有配置（`Chapter4/Fluentd/rotating-file -read-slack-out.conf`），以在 Slack 设置中包含捕获的详细信息。以下列表展示了这一点。
- en: Listing 4.8 Chapter4/Fluentd/rotating-file-read-slack-out.conf—match configuration
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.8 第 4 章/Fluentd/rotating-file-read-slack-out.conf—匹配配置
- en: '[PRE10]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ This is where the token obtained from Slack is put to correctly identify the
    workspace and legitimize the connection.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这就是从 Slack 获取的令牌被放置以正确识别工作空间并合法化连接的地方。
- en: ❷ Username that will be shown in the Slack conversation
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将在 Slack 对话中显示的用户名
- en: ❸ Defines an individual emoji (icon) to associate the bot
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义与机器人关联的单独的 emoji（图标）
- en: ❹ Which channel to place the messages into, based on the name. In our demo,
    we’re just using the default channel.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 根据名称确定将消息放置在哪个频道。在我们的演示中，我们只是使用默认频道。
- en: ❺ Message to be displayed, referencing values in the order provided in the configuration.
    The message attribute works in tandem with the message_keys attribute.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 要显示的消息，按照配置中提供的顺序引用值。消息属性与消息_keys 属性协同工作。
- en: ❻ Names the log event’s JSON elements to be included in the log event. The named
    element is then taken by the message tag and inserted into the message text.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 为要包含在日志事件中的日志事件的 JSON 元素命名。然后，该命名元素由消息标签获取并插入到消息文本中。
- en: ❼ Title for the message takes the values from title_keys. Using order to map
    between the config values.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 消息的标题采用 title_keys 的值。使用顺序在配置值之间进行映射。
- en: ❽ Definition of the message’s title
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 消息标题的定义
- en: ❾ Defines the frequency at which Fluentd will get Slack to publish the log events
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 定义 Fluentd 获取 Slack 发布日志事件的频率
- en: 'Before running the solution, we also need to install the Slack plugin with
    the command `fluent-gem install fluent-plugin-slack`. Once that is installed,
    we can start up the log simulator and Fluentd with the following commands:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行解决方案之前，我们还需要使用命令 `fluent-gem install fluent-plugin-slack` 安装 Slack 插件。一旦安装完成，我们就可以使用以下命令启动日志模拟器和
    Fluentd：
- en: '`fluentd -c Chapter4/Fluentd/rotating-file-read-slack-out.conf`'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fluentd -c Chapter4/Fluentd/rotating-file-read-slack-out.conf`'
- en: '`groovy LogSimulator.groovy Chapter4/SimulatorConfig/social-logs.properties
    ./TestData/small-source.txt`'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groovy LogSimulator.groovy Chapter4/SimulatorConfig/social-logs.properties
    ./TestData/small-source.txt`'
- en: Once this is started, if you open the *#general* channel in the web client or
    app, you will see messages from Fluentd flowing through.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启动，如果你在Web客户端或应用程序中打开*#general*频道，你将看到Fluentd流过的消息。
- en: All the details for the Slack plugin can be obtained from [https://github.com/sowawa/fluent-plugin-slack](https://github.com/sowawa/fluent-plugin-slack).
    Our illustration of Slack use is relatively straightforward (figure 4.3). By using
    several plugins, we can quickly go from the source tag to routing the Slack messages
    to the most relevant individual or group directly. Alternatively, we have different
    channels for each application and can direct the messages to those channels.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Slack插件的全部细节可以从[https://github.com/sowawa/fluent-plugin-slack](https://github.com/sowawa/fluent-plugin-slack)获得。我们关于Slack使用的说明相对简单（图4.3）。通过使用几个插件，我们可以快速从源标签路由Slack消息到最相关的个人或直接。或者，我们可以为每个应用程序设置不同的频道，并将消息直接发送到这些频道。
- en: '![](../Images/CH04_F03_Wilkins.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH04_F03_Wilkins.png)'
- en: Figure 4.3 Our Fluentd log events displayed in Slack
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 在Slack中显示的我们的Fluentd日志事件
- en: 4.5.1 Handling tokens and credentials more carefully
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.1 更小心地处理令牌和凭证
- en: For a long time, good security practice has told us we should not hardwire code
    and configuration files with credentials, as anyone can look at the file and get
    sensitive credentials. In fact, if you commit to GitHub, it will flag with the
    service provider any configuration file or code that contains a string that looks
    like a security token for a service GitHub knows about. When Slack is told and
    decides it is a valid token, then be assured that it will revoke the token.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 很长时间以来，良好的安全实践告诉我们不应该将凭证硬编码到代码和配置文件中，因为任何人都可以查看文件并获取敏感凭证。实际上，如果你将代码提交到GitHub，它将标记任何包含GitHub所知的服务的安全令牌字符串的配置文件或代码。当Slack被告知并决定这是一个有效的令牌时，请放心，它将吊销该令牌。
- en: 'So how do we address such a problem? There are a range of strategies, depending
    upon your circumstances; a couple of options include the following:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何解决这个问题呢？根据你的情况，有一系列策略；以下是一些选项：
- en: Set the sensitive credentials up in environment variables with a limited session
    scope. The environment variable can be configured in several ways, such as with
    tools like Chef and Puppet setting up the values from a keystore.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用具有有限会话作用域的环境变量设置敏感凭证。可以通过多种方式配置环境变量，例如使用Chef和Puppet等工具从密钥库中设置值。
- en: Embed a means to access a keystore or a secrets management solution, such as
    HashiCorp’s Vault, into the application or configuration.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在应用程序或配置中嵌入访问密钥库或秘密管理解决方案（如HashiCorp的Vault）的方法。
- en: The configuration files may not look like it is possible to secure credentials
    based on what we have seen so far in the book. We can achieve both of these approaches
    for securely managing credentials within a Fluentd configuration file, as Fluentd
    allows us to embed Ruby fragments into the configuration. This doesn’t mean we
    need to immediately learn Ruby. For the first of these approaches, we just need
    to understand a couple of basic patterns. The approach of embedding calls to Vault
    is more challenging but can be done.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件可能看起来不像我们在这本书中看到的那样可以基于凭证进行安全保护。但是，我们可以通过在Fluentd配置文件中嵌入Ruby片段来实现这两种方法来安全地管理凭证，因为Fluentd允许我们这样做。这并不意味着我们立即需要学习Ruby。对于这些方法中的第一种，我们只需要理解几个基本模式。嵌入对Vault的调用方法更具挑战性，但可以完成。
- en: '[PRE11]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 4.5.2 Externalizing Slack configuration attributes in action
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.2 在实际操作中外部化Slack配置属性
- en: The challenge is to set up your environment so that you have an environment
    variable called *SlackToken*, which is set to hold the token you have previously
    obtained. Then customize `Chapter4/Fluentd/rotating-file-read-slack-out.conf`
    to use the environment variable, and rerun the example setup with the commands
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战在于设置你的环境，以便你有一个名为*SlackToken*的环境变量，该变量设置为保存你之前获得的令牌。然后自定义`Chapter4/Fluentd/rotating-file-read-slack-out.conf`以使用环境变量，并使用以下命令重新运行示例设置
- en: '`fluentd -c Chapter4/Fluentd/rotating-file-read-slack-out.conf`'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fluentd -c Chapter4/Fluentd/rotating-file-read-slack-out.conf`'
- en: '`groovy LogSimulator.groovy Chapter4/SimulatorConfig/social-logs.properties
    ./TestData/small-source.txt`'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groovy LogSimulator.groovy Chapter4/SimulatorConfig/social-logs.properties
    ./TestData/small-source.txt`'
- en: Confirm that log events are arriving in Slack.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 确认日志事件是否已到达Slack。
- en: Answer
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 答案
- en: By setting up the environment variable, you’ll have created a command that looks
    like either
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置环境变量，您将创建一个看起来像以下之一的命令
- en: '[PRE12]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: or for Windows or Linux
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 或者为Windows或Linux
- en: '[PRE13]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The configuration will now have changed to look like the example in the following
    listing.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 配置现在将改变，看起来像以下列表中的示例。
- en: Listing 4.9 Chapter4/rotating-file-read-slack-out-Answer.conf—match configuration
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.9 第4章/rotating-file-read-slack-out-Answer.conf—匹配配置
- en: '[PRE14]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 4.6 The right tool for the right job
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6 适合工作的正确工具
- en: 'In chapter 1, we highlighted the issue of different people wanting different
    tools for a range of reasons, such as the following:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1章中，我们强调了由于各种原因（如下所述）不同的人想要不同的工具的问题：
- en: To perform log analytics as different tools and to have different strengths
    and weaknesses
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了执行日志分析，不同的工具具有不同的优势和劣势
- en: To multicloud, so specialist teams (and cost considerations of network traffic)
    mean using different cloud vendor tools
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多云，因此专业团队（以及网络流量的成本考虑）意味着使用不同的云服务提供商工具
- en: To make decisions that influence individual preferences and politics (previous
    experience, etc.)
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了做出影响个人偏好和政策的决策（先前经验等）
- en: As we have illustrated, Fluentd can support many social platforms and protocols.
    Of course, this wouldn’t be the only place for log events to be placed. One of
    the core types of destination is a log analytics tool or platform. Fluentd has
    a large number of plugins to feed log analytics platforms; in addition to the
    two we previously mentioned, other major solutions that can be easily plugged
    in include
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所展示的，Fluentd可以支持许多社交平台和协议。当然，这不会是日志事件放置的唯一地方。核心目的地类型之一是日志分析工具或平台。Fluentd拥有大量插件来为日志分析平台提供数据；除了我们之前提到的两个之外，其他可以轻松插入的主要解决方案包括
- en: Azure Monitor
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure Monitor
- en: Graphite
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graphite
- en: Elasticsearch
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elasticsearch
- en: CloudWatch
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CloudWatch
- en: Google Stackdriver
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Stackdriver
- en: Sumo Logic
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sumo Logic
- en: Logz.io
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Logz.io
- en: Oracle Log Analytics
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oracle日志分析
- en: 'Then, of course, we can send logs to a variety of data storage solutions to
    hold for later use or perform data analytics with; for example:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当然，我们可以将日志发送到各种数据存储解决方案以备后用或进行数据分析；例如：
- en: Postgres, InfluxDB, MySQL, Couchbase, DynamoDB, Aerospike, SQL Server, Cassandra
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Postgres、InfluxDB、MySQL、Couchbase、DynamoDB、Aerospike、SQL Server、Cassandra
- en: Kafka, AWS Kinesis (time series store/event streaming)
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka，AWS Kinesis（时间序列存储/事件流）
- en: Storage areas such as AWS S3, Google Cloud Storage, Google BigQuery, WebHDFS
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储区域，如AWS S3、Google Cloud Storage、Google BigQuery、WebHDFS
- en: So, the question becomes, what are my needs and which tool(s) fit best? If our
    requirements change over time, then we add or remove our targets as needed. Changing
    the technology will probably raise more challenging questions about what to do
    with our current log events, not how to get the data into the solution.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，问题变成了，我的需求是什么，哪些工具最适合？如果我们的需求随时间变化，那么我们就根据需要添加或删除我们的目标。改变技术可能会提出更多关于如何处理我们当前的日志事件的问题，而不是如何将数据放入解决方案。
- en: Summary
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Fluentd has an extensive range of output plugins covering files, other Fluentd
    nodes, relational and document databases such as MongoDB, Elasticsearch, and so
    on.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fluentd拥有广泛的支持文件、其他Fluentd节点、关系型数据库和文档数据库（如MongoDB、Elasticsearch等）的输出插件。
- en: Plugin support extends beyond analytics and storage solutions to collaboration
    and notification tools, such as Slack. This allows Fluentd to drive more rapid
    reactions to significant log events.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 插件支持不仅限于分析和存储解决方案，还包括协作和通知工具，如Slack。这使得Fluentd能够对重要的日志事件做出更快的反应。
- en: Fluentd provides some powerful helper plugins, including formatters and buffers,
    making log event output configuration very efficient and easy to use.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fluentd提供了一些强大的辅助插件，包括格式化和缓冲，使得日志事件输出配置非常高效且易于使用。
- en: Log events can be made easy to consume by tools such as analytics and visualization
    tools. Fluentd provides the means to format log events using formatter plugins,
    such as out_file and json.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志事件可以通过分析工具和可视化工具等工具变得易于消费。Fluentd提供了使用格式化插件（如out_file和json）格式化日志事件的方法。
- en: Buffer helper plugins can support varying life cycles depending on the need,
    from the simple synchronous cache to the fully asynchronous. With this, the buffer
    storage can be organized by size or number of log events.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓冲辅助插件可以根据需要支持不同的生命周期，从简单的同步缓存到完全异步。有了这个，缓冲存储可以根据大小或日志事件的数量进行组织。
- en: Buffers can be configured to flush their contents not just on shutdown, but
    also on other conditions, such as new events being buffered for a while.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓冲区可以被配置为不仅在关闭时刷新其内容，还可以在其他条件下刷新，例如缓冲一段时间的新事件。
