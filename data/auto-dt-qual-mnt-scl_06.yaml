- en: Chapter 5\. Building a Model That Works on Real-World Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。建立适用于现实世界数据的模型
- en: In [Chapter 4](ch04.html#automating_data_quality_monitoring_wi), we shared an
    algorithm for data quality monitoring with unsupervised machine learning. It’s
    one thing to read about these steps, and quite another to build a model that performs
    well in practice on any arbitrary real-world dataset. If you don’t have strategies
    to account for nuances like seasonality, time-based features, and correlations
    across columns, your model will over- or under-alert, often dramatically.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.html#automating_data_quality_monitoring_wi)中，我们分享了一个利用无监督机器学习进行数据质量监控的算法。阅读这些步骤和在任意实际世界数据集上实际构建表现良好模型是两回事。如果您没有考虑到季节性、基于时间的特征和列之间的相关性等细微差别的策略，您的模型可能会过度或不足报警，通常是非常显著的。
- en: Beyond knowing the pitfalls to look out for, you’ll need to continuously evaluate
    your model against benchmark data to figure out where and how to improve. We’ll
    share methods for effective model testing, including thoughts on developing a
    library to introduce chaos into perfectly well-behaved data (cue evil laugh).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 除了了解需要注意的陷阱外，您还需要持续评估模型与基准数据的对比情况，以找出改进的位置和方式。我们将分享有效模型测试的方法，包括开发一个库以将完全良好的数据引入混乱（伴有邪恶笑声）的思考。
- en: Data Challenges and Mitigations
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据挑战与缓解
- en: To make your model truly valuable rather than noisy, you’ll need strategies
    to overcome the challenges presented by data in the wild.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 要使您的模型真正有价值而不是嘈杂，您需要采取策略来克服野外数据所带来的挑战。
- en: Seasonality
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 季节性
- en: Humans are very seasonal creatures. We change our behavior patterns by hour
    of the day and day of the week. We pay bills on roughly the same day every month
    and go on holiday around the same time every year. Most data, in some way or another,
    is a reflection of human behavior or is affected by human behavior, and so these
    seasonality patterns appear in almost all data we care about.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 人类是非常季节性的生物。我们的行为模式会随着一天中的时间和一周中的日期而改变。我们每个月大致在同一天支付账单，并且每年大约在同一时间度假。大多数数据在某种程度上反映了人类的行为或受人类行为的影响，因此这些季节性模式几乎出现在我们关心的所有数据中。
- en: As you’ll recall from [Chapter 4](ch04.html#automating_data_quality_monitoring_wi),
    our approach relies on comparing data from today to data from yesterday. But because
    of seasonality, it turns out this isn’t actually enough. You might run into the
    issue that (for example) today is Monday and yesterday was Sunday, and many of
    the differences in the data are due to seasonality rather than data quality.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能会回忆起的，我们的方法依赖于比较今天的数据和昨天的数据，具体可以参见[第4章](ch04.html#automating_data_quality_monitoring_wi)。但由于季节性的影响，事实证明这并不足够。例如，今天是星期一，昨天是星期天，数据的许多差异是由于季节性而非数据质量引起的问题。
- en: What if, instead, you always compared today’s data against data from the same
    day the previous week (e.g., if today is Monday, look at last week’s Monday)?
    Unfortunately, this doesn’t avoid all the potential problems. First, if your strategy
    only checks data from last week, you won’t really know how long you’ve had an
    issue—it could have been present for the entire past week, or just appeared today.
    Second, what if there was a data quality issue last Monday? If today’s data is
    normal, it might look abnormal simply because it’s different from what happened
    last Monday. Third, what if last Monday was a holiday, so everything was just
    weird last Monday?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果改为始终将今天的数据与前一周的同一天的数据进行比较（例如，如果今天是星期一，则查看上周的星期一数据）呢？不幸的是，这并不能避免所有潜在的问题。首先，如果您的策略仅检查上周的数据，您将不知道问题存在了多长时间——它可能在过去的整个上周存在，或者只是今天才出现。其次，如果上周一存在数据质量问题呢？如果今天的数据是正常的，它可能看起来异常，只是因为它与上周一发生的情况不同。第三，如果上周一是假期，那么那一天的一切都可能异常。
- en: To control for these factors, it’s essential to sample data *from multiple different
    times in the past* (yesterday, two days ago, a week ago, two weeks ago, etc.).
    If today’s data looks “normal” when compared to any of these prior dates, then
    today’s data must not be unusual.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要控制这些因素，有必要从*过去多个不同时间点*（昨天、前天、一周前、两周前等）采样数据。如果今天的数据与任何这些之前的日期比较时看起来“正常”，那么今天的数据就不应该异常。
- en: Another way to combat seasonality is to generate a lot of metadata statistics
    automatically every time you monitor data. Then you can use time series models
    on that metadata over time to identify if features have a longer-term consistent
    seasonality trend to them and dampen those features down.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种对抗季节性的方法是每次监控数据时自动生成大量的元数据统计信息。然后，您可以在时间上使用时间序列模型来识别特征是否具有长期一致的季节性趋势，并抑制这些特征。
- en: Time-Based Features
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间特征
- en: 'Here’s a problem that you’re sure to encounter almost as soon as you deploy
    the naive ML algorithm in the real world: there’s usually at least one column
    in a table that is directly correlated with time, such as a timestamp or an ID.
    It’s trivial to look at this column and know whether the data is from today or
    not! Before these time-correlated features soak up too much of our model’s attention,
    we need to identify and remove them from the sample dataset entirely.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在您将天真的ML算法部署到现实世界后，您几乎肯定会遇到的问题是：表格中通常至少有一列与时间直接相关，例如时间戳或ID。查看这些列并知道数据是否来自今天非常简单！在这些时间相关特征吸引过多模型注意力之前，我们需要识别并完全从样本数据集中删除它们。
- en: 'Our feature engineering for timestamps takes the delta between each timestamp
    and the column used to partition the data by time, so this generally removes the
    obvious correlations of “I have 10 different timestamps that are all correlated
    with time.” It’s the ones that are less obvious that are difficult to deal with.
    Examples we commonly encounter are:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的时间戳特征工程会计算每个时间戳与用于按时间分区数据的列之间的差值，因此通常会消除“我有10个不同的时间戳，它们都与时间相关”的明显相关性。难以处理的是那些不那么明显的情况。我们常遇到的示例包括：
- en: Autoincrementing IDs (each new customer gets a slightly larger ID, so you can
    always identify records from “today” as those having larger customer IDs)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自增ID（每个新客户都获得稍大的ID，因此您可以始终识别“今天”的记录为那些具有更大客户ID的记录）
- en: String- or integer-based representations of date information—such as “day of
    month” or “day of week”—that will always be changing
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示日期信息的字符串或整数，如“月中的一天”或“星期中的一天”，这些信息总是在变化
- en: Version identifiers for applications or logging semantics, which may change
    erratically or frequently depending on how often the system is updated
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序或日志语义的版本标识符可能会不规律或频繁地更改，这取决于系统更新频率。
- en: Some of these issues can be identified via simple summary statistics, such as
    checking whether a feature is always larger each day. But in practice, you’ll
    get more robust coverage by building an additional, simple version of your model
    using the complete dataset. Just look for any features that are incredibly significant
    to this model’s predictions consistently over time,^([1](ch05.html#ch01fn3)) and
    take them out of the dataset for your real model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通过简单的汇总统计信息，例如检查某个特征是否每天都比前一天大，可以识别出其中一些问题。但在实际操作中，通过使用完整数据集构建一个额外简单版本的模型，您可以获得更健壮的覆盖范围。只需查找在很长时间内对该模型的预测非常重要的任何特征^([1](ch05.html#ch01fn3))，然后将其从真实模型的数据集中删除。
- en: Chaotic Tables
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混乱的表格
- en: Datasets can be very chaotic for a few reasons. There might be humans conducting
    ad hoc processes (like marketing campaigns) on an unpredictable schedule that
    meaningfully affects the data. Alternatively, the product or service generating
    the data might be immature and in the process of being changed very rapidly by
    an Agile engineering team. If you don’t account for how chaotic a table is, your
    model will treat all data equally and over-alert on chaotic tables while under-alerting
    on tables that are more stable. (We’ll talk much more about alerts in [Chapter 6](ch06.html#implementing_notifications_while_avoidi).)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于多种原因，数据集可能非常混乱。可能会有人类根据不可预测的时间表执行特定过程（如营销活动），这些过程会显著影响数据。或者，生成数据的产品或服务可能还处于成熟阶段，并且由敏捷工程团队快速更改中。如果不考虑表格的混乱程度，您的模型会平等对待所有数据，并在混乱表格上发出过多警报，而在更稳定的表格上发出过少警报。（在[第6章](ch06.html#implementing_notifications_while_avoidi)我们将更详细地讨论警报。）
- en: '![Example of table anomaly scores and learned thresholds over time](assets/adqm_0501.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![随时间变化的表格异常分数和学习阈值示例](assets/adqm_0501.png)'
- en: Figure 5-1\. Example of table anomaly scores and learned thresholds over time.
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1\. 随时间变化的表格异常分数和学习阈值示例。
- en: Therefore, it’s important to set thresholds for notifications by building up
    a time series of the severity of changes detected by the ML model. [Figure 5-1](#example_of_table_anomaly_scores_and_lea)
    gives an example, where the overall anomaly score for the table (on a log scale)
    is based on the average magnitude of the SHAP values. This lets you use a time
    series model to learn how chaotic each table is and suggest a reasonable threshold
    for alerting that can move up and down dynamically over time.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过建立ML模型检测到的变化严重性的时间序列，设置通知阈值非常重要。[图 5-1](#example_of_table_anomaly_scores_and_lea)
    给出了一个示例，其中表的整体异常分数（对数尺度上）基于SHAP值的平均幅度。这让您可以使用时间序列模型来学习每个表的混乱程度，并建议一个合理的警报阈值，可以随时间动态上下移动。
- en: To avoid noise when the model is first beginning to observe the data in the
    table, we recommend starting with a very conservative threshold and gradually
    reducing it to the level of chaos found in the dataset. You can begin with a threshold
    that is very, very high (such that it is almost impossible for the model to alert
    on the initial run) and exponentially decay that baseline threshold down toward
    zero; a reasonable approach is to decay by a factor of two every 10 days or so.
    You can then blend that base threshold with a time series model that is fit to
    the scores you’ve been logging.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型首次开始观察表中数据时，为了避免噪音，我们建议从一个非常保守的阈值开始，并逐渐将其降低到数据集中存在的混乱水平。您可以从一个非常非常高的阈值开始（以至于模型几乎不可能在初始运行时触发警报），然后指数衰减基准阈值，使其向零逼近；一个合理的方法是每隔约10天衰减一次，衰减因子为二。然后，您可以将该基准阈值与适合您记录的分数的时间序列模型混合使用。
- en: Updated-in-Place Tables
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 就地更新表
- en: In [Chapter 4](ch04.html#automating_data_quality_monitoring_wi), we stated that
    our algorithm shouldn’t be expected to make decisions without some notion of time.
    This can be surprisingly tricky in practice due to how some types of tables are
    updated.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第四章](ch04.html#automating_data_quality_monitoring_wi)中，我们指出，我们的算法不应该在没有时间概念的情况下做出决策。由于某些类型的表格如何更新，这在实践中可能会非常棘手。
- en: 'You might be working with a few different types of tables in your data warehouse:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能在数据仓库中使用几种不同类型的表格：
- en: Static tables
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 静态表
- en: These are tables that don’t have a time column. They can be either dimension
    or lookup tables (e.g., all of the demographic information known about a given
    entity) or summary tables (e.g., a set of summary statistics about the current
    state of a set of entities).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是没有时间列的表格。它们可以是维度表或查找表（例如，关于特定实体已知的所有人口统计信息），也可以是汇总表（例如，关于一组实体当前状态的一组汇总统计信息）。
- en: For these tables, you’ll need to take snapshots of the data every day, as any
    record could be updated at any given time (or the entire table could be dropped
    and replaced).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些表，您需要每天对数据进行快照，因为任何记录都可能在任何时间更新（或整个表可能被删除并替换）。
- en: Log tables
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 日志表
- en: These are tables where the only change that ever happens is that new records
    are added to the existing table. No changes ever occur in old records. This is
    often the case with raw transactional or event-level data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是仅在现有表格中添加新记录时发生的唯一更改的表格。旧记录永远不会发生任何更改。这通常是原始交易或事件级数据的情况。
- en: These tables usually have a `created_at` time column that indicates when each
    record was created, and that can be used to partition the data into time-based
    samples.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表通常有一个`created_at`时间列，指示每条记录的创建时间，并且可以用于将数据分割成基于时间的样本。
- en: Updated-in-place (mutating) tables
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 就地更新（变异）表格
- en: These are tables that at first glance appear to be log tables, in that each
    record corresponds to a specific event or transaction, and new records are regularly
    being added to the table. However, the records themselves *can change after they
    are initially written*. For example, a table of ecommerce orders might start without
    a record that has the order date, but the shipping date isn’t known yet—it begins
    as NULL and is filled in over the coming days once the shipping date is set.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是乍看之下似乎是日志表格的表格，因为每条记录对应特定事件或交易，并且新记录定期添加到表中。然而，这些记录本身在初始写入后*可以更改*。例如，电子商务订单表可能起初没有具有订单日期的记录，但尚不知道运输日期
    —— 它从NULL开始，并在未来几天内填写一旦设置了运输日期。
- en: These tables usually have a `created_at` time column, but they will also have
    an `updated_at` time column, which tracks when each record was last updated. You’ll
    need to treat these tables carefully.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表通常具有`created_at`时间列，但它们还将具有`updated_at`时间列，用于跟踪每条记录的最后更新时间。您需要小心处理这些表。
- en: One way to detect if records in a table are frequently updated in place is to
    track how historical values in a metric change over time. For example, [Figure 5-2](#changes_in_row_count_over_time_for_a_ta)
    records the number of records in a table each day. The x-axis is the time column
    in the table (the `datecreated` column), whereas the y-axis is the date. Each
    day, you can compare the row count that you obtained on that day to what you had
    on the day prior. In [Figure 5-2](#changes_in_row_count_over_time_for_a_ta), squares
    are colored based on whether, and how much, the data has changed.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 检测表中记录是否经常更新就地的一种方法是跟踪度量指标的历史值随时间的变化。例如，[图5-2](#changes_in_row_count_over_time_for_a_ta)记录了每天表中记录的数量。x轴是表中的时间列（`datecreated`列），而y轴是日期。每天，您可以比较当天获取的行数与前一天的情况。在[图5-2](#changes_in_row_count_over_time_for_a_ta)中，方块的颜色基于数据的变化情况及其程度。
- en: '![Changes in row count over time for a table](assets/adqm_0502.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![表的时间变化中行数的变化](assets/adqm_0502.png)'
- en: Figure 5-2\. Changes in row count over time for a table. See a full-sized version
    of this image at [*https://oreil.ly/adqm_5_2*](https://oreil.ly/adqm_5_2).
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2\. 表的时间变化中行数的变化。查看此图的全尺寸版本 [*https://oreil.ly/adqm_5_2*](https://oreil.ly/adqm_5_2)。
- en: For example, for 2023-06-09 (per `datecreated`), when we first observed records
    on that date, we found only 498 rows. But the second day we checked (on 2023-06-10),
    we found 514 rows. This indicates that more records were added on that subsequent
    date.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于2023-06-09（根据`datecreated`），当我们首次观察到该日期的记录时，我们发现只有498行。但我们在第二天检查时（2023-06-10），发现有514行。这表明在随后的日期添加了更多的记录。
- en: 'We typically observe two patterns in this visualization:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常在这个可视化中观察到两种模式：
- en: A colored diagonal pattern means that a table has frequent updates to new data.
    Specifically, that new data is updated in the days immediately following its appearance
    in the table. The width of the diagonal indicates how long you have to wait for
    data to “mature” in the table. In [Figure 5-2](#changes_in_row_count_over_time_for_a_ta),
    we see a diagonal indicating that maturity takes about one day.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有彩色对角线模式意味着表中频繁更新新数据。具体而言，新数据在出现后的几天内更新。对角线的宽度表示您需要等待数据在表中“成熟”的时间。在[图5-2](#changes_in_row_count_over_time_for_a_ta)中，我们看到一个对角线，表明成熟大约需要一天的时间。
- en: A horizontal line indicates that, on a given date, a batch process was run that
    changed a bunch of historical data—often to address a data quality issue or otherwise
    migrate the dataset.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一条水平线表示，在给定日期运行了一个批处理过程，该过程更改了大量历史数据——通常是为了解决数据质量问题或迁移数据集。
- en: There’s a third type of pattern, vertical lines of change, but this is much
    less common, as it would indicate there is a specific date for which data is frequently
    changing. For visual examples of all three patterns, see [Figure 5-3](#visual_patterns_and_their_meaning_when).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 还有第三种类型的模式，即竖直变化线，但这种情况要少得多，因为它会表明有一个特定的日期，数据频繁更改。有关所有三种模式的视觉示例，请参见[图5-3](#visual_patterns_and_their_meaning_when)。
- en: '![Visual patterns and their meaning when using colors to track how data changes
    over time in an updated-in-place table](assets/adqm_0503.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![使用颜色跟踪数据随时间变化的更新就地表中的可视模式及其含义](assets/adqm_0503.png)'
- en: Figure 5-3\. Visual patterns and their meaning when using colors to track how
    data changes over time in an updated-in-place table.
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-3\. 使用颜色跟踪数据随时间变化的更新就地表中的可视模式及其含义。
- en: The problem with tables that are updated in place is that they appear to always
    have anomalies on the most recent date. For example, the percentage of orders
    with NULL shipping dates is always going to spike on recent days. However, this
    is really just a by-product of how data is being updated. If we compare the data
    from today to the data from yesterday *when we observed it yesterday*, then we
    would see that the percentage of NULL values in the `shipped_at` column is actually
    as expected.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 更新就地的表存在的问题是它们似乎总是在最近的日期上出现异常。例如，具有空运输日期的订单百分比在最近几天总是会剧增。然而，这实际上只是数据更新方式的副产品。如果我们将今天的数据与昨天*我们昨天观察到的数据*进行比较，那么我们会看到`shipped_at`列中NULL值的百分比实际上是符合预期的。
- en: For this reason, you should always take snapshots of the data in each table
    you monitor every day and compare the current data to these snapshots so that
    you can rule out any changes that might be due to updated-in-place dynamics. Since
    it can be challenging to know which tables are updated in place (and many tables
    will be updated in place without an `updated_at` time column), it’s best to assume
    this is happening—and spend some extra computational resources—than risk sending
    repeated false positive alerts due to this issue.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您应该每天对您监控的每个表格的数据进行快照，并将当前数据与这些快照进行比较，以便排除可能由于即时更新动态而引起的任何更改。由于很难知道哪些表格是原地更新的（许多表格将在没有`updated_at`时间列的情况下进行原地更新），最好假设正在发生这种情况，并花费额外的计算资源，而不是因此问题发送重复的虚假警报。
- en: Note that it can be hard to achieve a “warm start” with this approach, as you
    have to wait to take the snapshots of the data before you can trust that the algorithm
    isn’t finding issues related to updated-in-place dynamics.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用这种方法很难实现“温暖启动”，因为您必须等待数据快照，以便可以信任算法不会发现与即时更新动态相关的问题。
- en: Column Correlations
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列相关性
- en: Most datasets have a great deal of correlation structure in them (feel free
    to take a peek back at [Figure 2-4](ch02.html#the_relationships_between_columns_in_a)
    for an example). This can happen for several reasons. The same data may be captured
    in multiple different forms (e.g., identifiers and strings). Or there may be a
    hierarchy of identifiers that are all captured on the same table and used to group
    the data for different levels of business reporting. Also, tables often represent
    causal funnels where certain events must occur before other events can happen,
    and these funnels will appear as correlations in the table structure.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据集中都存在大量的相关结构（可以回顾一下[图 2-4](ch02.html#the_relationships_between_columns_in_a)）。这可能由于多种原因导致。相同的数据可以以多种不同的形式捕获（例如标识符和字符串）。或者可能存在一种层次结构的标识符，它们都被捕获在同一张表中，并用于不同级别的业务报告数据组。此外，表通常表示因果漏斗，在某些事件发生之前必须发生其他事件，这些漏斗将表现为表结构中的相关性。
- en: When columns are correlated, that means a single data quality issue could affect
    all those columns. If the algorithm isn’t careful, it might send multiple alerts
    or suggest that there are many separate issues when in fact they are all related.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当列之间存在相关性时，这意味着一个单一的数据质量问题可能会影响所有这些列。如果算法不小心的话，可能会发送多个警报或者建议存在许多单独的问题，而实际上它们都是相关联的。
- en: Data quality issues affecting multiple columns often occur in pipelines where
    data “fans out.” For example, we might start out with a column that is an integer
    ID for the location of an event. Then that column is joined to a locations table
    with metadata like location names, dates, priority levels, etc. If the original
    location identifier in the log goes missing, then the join will fail, and all
    of the other location metadata will also go missing.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 影响多列的数据质量问题通常发生在数据“分布”管道中。例如，我们可能从一个事件的整数ID列开始。然后该列与一个包含位置名称、日期、优先级等元数据的位置表进行连接。如果日志中原始的位置标识符丢失，那么连接将失败，并且所有其他位置元数据也将丢失。
- en: With SHAP values that credit how anomalous individual values are in the table,
    you can use row-level correlations to cluster the columns together and present
    them as a single issue to the user. For example, if one level in a hierarchy is
    affected by a data quality issue, we will see anomalies across multiple columns,
    but all for the same rows. With this insight, we can present a single anomaly
    rather than overwhelming users with multiple alerts about the same incident.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 利用表中个别值的SHAP值来表彰它们的异常性，您可以利用行级别的相关性将列聚合在一起，并将它们作为单个问题呈现给用户。例如，如果层级结构中的一个级别受到数据质量问题的影响，我们将看到多列存在异常，但都是相同的行。通过这种洞察力，我们可以提供单个异常，而不是给用户发送关于同一事件多次警报。
- en: 'As an example, consider the following table, which gives some sample product
    data for items stocked on grocery store shelves:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑下面的表格，其中给出了一些存放在杂货店货架上的样品产品数据：
- en: '| `Item ID` | `Department` | `Aisle` | `Product` | `Brand` | `Item` | `Size`
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| `商品ID` | `部门` | `货架` | `产品` | `品牌` | `商品` | `大小` |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| `43112` | `Refrigerated` | `Yogurt` | `Greek Yogurt` | `Chobani` | `0% Plain`
    | `32 oz.` |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| `43112` | `冷藏` | `酸奶` | `希腊酸奶` | `Chobani` | `0% 原味` | `32 盎司` |'
- en: '| `43113` | `Refrigerated` | `Yogurt` | `Greek Yogurt` | `Chobani` | `0% Plain`
    | `64 oz.` |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| `43113` | `冷藏` | `酸奶` | `希腊酸奶` | `Chobani` | `0% 原味` | `64 盎司` |'
- en: '| `43114` | `Refrigerated` | `Yogurt` | `Greek Yogurt` | `Chobani` | `2% Plain`
    | `32 oz.` |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| `43114` | `冷藏` | `酸奶` | `希腊酸奶` | `Chobani` | `2% 原味` | `32 盎司` |'
- en: '| `43115` | `Refrigerated` | `Yogurt` | `Greek Yogurt` | `Chobani` | `2% Plain`
    | `64 oz.` |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| `43115` | `冷藏` | `酸奶` | `希腊酸奶` | `Chobani` | `2% 原味` | `64 盎司` |'
- en: '| `...` | `...` | `...` | `...` | `...` | `...` | `...` |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `...` | `...` | `...` | `...` | `...` | `...` | `...` |'
- en: '| `43945` | `Refrigerated` | `Yogurt` | `Greek Yogurt` | `Fage` | `0% Plain`
    | `32 oz.` |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `43945` | `冷藏` | `酸奶` | `希腊酸奶` | `Fage` | `0% 原味` | `32 盎司` |'
- en: '| `...` | `...` | `...` | `...` | `...` | `...` | `...` |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| `...` | `...` | `...` | `...` | `...` | `...` | `...` |'
- en: In a dataset like this, an anomaly might occur at the product level (all `Greek
    Yogurt` is missing), which could be caught by features derived from the `Aisle`
    column (`Yogurt` is anomalous), `Product` column (`Greek Yogurt`), or `Brand`
    column (a collection of specific brands are anomalous). `Department` is probably
    too highly aggregated to be sensitive to an anomaly for Greek yogurt, and the
    `Item` column is too fragmented to be easily used to detect the anomaly.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样的数据集中，异常可能会发生在产品级别（所有的“希腊酸奶”都丢失了），这可能通过从“Aisle”列派生的特征（“酸奶”异常）、“Product”列（“希腊酸奶”）或“Brand”列（一组特定品牌异常）来捕捉。
    “Department”可能过于高度聚合，无法对希腊酸奶的异常敏感，而“Item”列则过于碎片化，难以用来轻松检测异常。
- en: Given that our algorithm has produced the SHAP-based anomaly scores for each
    individual record, we can apply clustering algorithms to those anomaly scores
    to detect that the anomalies in these columns are all happening on the same set
    of rows.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 给定我们的算法为每个单独的记录生成了基于SHAP的异常分数，我们可以将聚类算法应用于这些异常分数，以便检测这些列中的异常都发生在相同的一组行上。
- en: Model Testing
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型测试
- en: Given all those challenges, and what is already a fairly complex algorithm,
    how do you ensure that the model you build actually works on real-world data?
    Furthermore, how do you make iterative improvements?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于所有这些挑战，以及已经相当复杂的算法，您如何确保您构建的模型实际上在真实世界的数据上运行？此外，如何进行迭代改进？
- en: Collecting benchmark data where humans have labeled what’s anomalous and what’s
    not might seem reasonable at first glance, but as we’ve discussed elsewhere in
    this book, creating a human-labeled dataset where raters judge what’s anomalous
    is extremely expensive (not to mention subjective). You’d need tens of thousands
    of labeled anomalies to form a robust benchmark.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 收集基准数据，人类已经标记了什么是异常和什么不是可能乍一看似乎是合理的，但正如我们在本书的其他地方讨论过的那样，创建一个人工标记的数据集，其中评估员判断什么是异常的成本非常高昂（更不用说主观性了）。您需要成千上万个标记的异常才能形成一个强大的基准。
- en: So, we need a different approach. The key insight here is that realistic data
    quality issues are actually not that hard to *insert programmatically into datasets*.
    After all, issues are most often caused by code in the first place! We’ve found
    that detecting synthetic anomalies, a.k.a. “chaos,” is a good proxy for detecting
    real data quality issues. As Patches O’Houlihan proclaims in the movie *Dodgeball*,
    “If you can dodge a wrench, you can dodge a ball.”
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们需要采取不同的方法。这里的关键洞察力是，实际数据质量问题实际上并不难*以编程方式插入数据集*。毕竟，问题往往是由代码引起的！我们发现检测合成异常，即“混沌”，是检测真实数据质量问题的良好代理。正如电影《躲避球》中的Patches
    O’Houlihan宣布的那样，“如果你能躲开扳手，你就能躲开球。”
- en: 'Thus, the algorithm for benchmarking is roughly as follows: collect a representative
    sample of tabular datasets, run your model on these datasets both before and after
    introducing synthetic anomalies, and measure statistics around the runtimes and
    accuracy of your evaluation. This allows you to fine-tune your model in ways that
    you hope will improve your stats (e.g., by changing parameters or dampening features).
    Rinse and repeat.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基准测试的算法大致如下：收集一个代表性的表格数据集样本，在引入合成异常之前和之后在这些数据集上运行您的模型，并测量运行时间和评估准确性的统计数据。这使您能够通过更改参数或减少特征等方式来优化您的模型。反复进行这一过程。
- en: Let’s talk about what kinds of synthetic anomalies you can introduce and how,
    then move on to benchmarking and fine-tuning your model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈您可以引入的合成异常的种类及其方式，然后继续进行基准测试和微调您的模型。
- en: Injecting Synthetic Issues
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注入合成问题
- en: “Chaos engineering” is the idea of purposefully creating random failures in
    a system to test how the system responds. One well-known example is Netflix’s
    [Chaos Monkey](https://oreil.ly/DYkV8), a tool that randomly terminates production
    instances to test a network’s resiliency.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: “混沌工程”是故意在系统中创建随机失败以测试系统响应的想法。一个众所周知的例子是Netflix的[混沌猴](https://oreil.ly/DYkV8)，这是一个随机终止生产实例以测试网络弹性的工具。
- en: 'This idea translates well to testing data quality monitoring models. You can
    manipulate benchmark datasets with SQL to simulate real data quality problems
    that can occur in production systems. Since real data issues tend to affect only
    part of the data, it’s important to also vary *how* you apply the synthetic issues:
    to a segment, random columns, random percentages of the data, etc. Then you can
    measure your model’s performance according to ML metrics like sensitivity, specificity,
    and the [area under the curve (AUC)](https://oreil.ly/Qavzw), and also look at
    other performance characteristics such as how much time the model needs and how
    good it is at detecting certain kinds of issues compared to others.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法很好地转化为测试数据质量监控模型。您可以使用SQL操纵基准数据集，模拟在生产系统中可能发生的真实数据质量问题。由于真实数据问题往往只影响部分数据，因此重要的是还要变化*如何*应用合成问题：例如应用于一个段落、随机列、数据的随机百分比等等。然后，您可以根据ML指标如灵敏度、特异度和[曲线下面积（AUC）](https://oreil.ly/Qavzw)，以及其他性能特征，如模型所需的时间和检测某些问题相对于其他问题的能力，来衡量您的模型性能。
- en: Example
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 例子
- en: '[Figure 5-4](#sample_table_of_ticket_sales_data) shows a sample table of ticket
    sales data, each row corresponding to a listing of a number of tickets to a concert
    or sporting event. For example, we can see that `listid` number 43729 (the last
    row) was a listing for four tickets at $131 per ticket (a total of $524) for The
    Who at Reliant Stadium in Houston, Texas (seats 72k people!).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-4](#sample_table_of_ticket_sales_data)显示了门票销售数据示例表，每行对应一个音乐会或体育赛事门票的列表。例如，我们可以看到`listid`编号43729（最后一行）是在休斯敦德州依靠体育场（可容纳72k人！）为The
    Who乐队的四张票，每张票131美元（总计524美元）的列表。'
- en: '![Sample table of ticket sales data](assets/adqm_0504.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![门票销售数据示例表](assets/adqm_0504.png)'
- en: Figure 5-4\. Sample table of ticket sales data. See a full-sized version of
    this image at [*https://oreil.ly/adqm_5_4*](https://oreil.ly/adqm_5_4).
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-4\. 门票销售数据示例表。请在[*https://oreil.ly/adqm_5_4*](https://oreil.ly/adqm_5_4)查看此图的完整版。
- en: Imagine we know that for the `numtickets` column, the maximum value is 30\.
    One way to test whether our algorithm is capable of detecting a distributional
    change in the number of tickets would be to introduce some artificial chaos and
    change the values of the `numtickets` column to include the value 40.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们知道`numtickets`列的最大值是30。测试我们的算法是否能够检测到门票数量分布变化的一种方法是引入一些人为混乱，并将`numtickets`列的值更改为包含值40。
- en: It’s important that we do this only for the most recent date, as we want this
    change to appear as a sudden anomaly in the dataset. Furthermore, to make this
    data quality issue a bit more subtle, we will ensure that the chaos only applies
    to 30% of the records. We’ll include a `where_sql` clause that causes the chaos
    to apply only to tickets sold in `venuestate = ‘NY’`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 仅对最近的日期进行此操作非常重要，因为我们希望这一变化在数据集中表现为突发的异常。此外，为了使这个数据质量问题显得更加微妙，我们将确保混乱仅适用于30%的记录。我们将包括一个`where_sql`子句，使混乱仅适用于`venuestate
    = ‘NY’`中售出的门票。
- en: 'In practice, the SQL to inject this chaos into the table looks like:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，将此混乱注入表格的SQL如下所示：
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Does this sound like a silly example? Well, you’d be surprised at how often
    prices can change due to a data quality error. See, for example, the slightly
    overpriced airline ticket in [Figure 5-5](#an_exorbitant_airline_ticket_price_left).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来像是一个愚蠢的例子吗？嗯，由于数据质量错误，价格经常会变动，这点可能会让你感到惊讶。例如，看看[图 5-5](#an_exorbitant_airline_ticket_price_left)中的略为高价的航空票。
- en: '![An exorbitant airline ticket price (Shaun Walker [@sbwalker], Twitter, April
    2023, 4:50 p.m., https://twitter.com/sbwalker/status/1647024519331168257)](assets/adqm_0505.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![略为高价的航空票（Shaun Walker [@sbwalker]，Twitter，2023年4月，下午4:50，https://twitter.com/sbwalker/status/1647024519331168257）](assets/adqm_0505.png)'
- en: Figure 5-5\. An exorbitant airline ticket price (Shaun Walker [@sbwalker], Twitter,
    April 2023, 4:50 p.m., [*https://oreil.ly/BsKtS*](https://oreil.ly/BsKtS)).
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-5\. 略为高价的航空票（Shaun Walker [@sbwalker]，Twitter，2023年4月，下午4:50，[*https://oreil.ly/BsKtS*](https://oreil.ly/BsKtS)）。
- en: 'If you are testing models frequently, it’s helpful to encapsulate operations
    like these into a library.  Examples of chaos operations from Anomalo’s internal
    Chaos Llama library include:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您经常测试模型，将这些操作封装到一个库中将会很有帮助。Anomalo 内部混乱羊驼库中的混乱操作示例包括：
- en: '`ColumnGrow`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`ColumnGrow`'
- en: Multiplies a column by a random value drawn uniformly.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 将列乘以均匀抽取的随机值。
- en: '`ColumnModeDrop`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`ColumnModeDrop`'
- en: Drops rows with values equal to the mode of a given column. Requires that the
    mode represents at least a threshold fraction of the data, or else throws an error.
    This is designed to prevent chaos where the mode is very rare.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 删除具有等于给定列的众数值的行。要求众数表示至少一定比例的数据，否则会抛出错误。这旨在防止众数非常罕见的混乱情况。
- en: '`ColumnNull`'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`ColumnNull`'
- en: Turns `table.column` into NULL for a fraction of records.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `table.column` 转换为 NULL，适用于部分记录。
- en: '`ColumnRandom`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`ColumnRandom`'
- en: Replaces values in a column with random floats or integers within the column
    range, a 50/50 split of True/False for Boolean, or a hash of the string.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机浮点数或整数替换列中的值，布尔值的 True/False 比例为 50/50，或者对字符串进行哈希处理。
- en: '`TableReplicate`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`TableReplicate`'
- en: Adds additional rows to a table (randomly sampled from the original table).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在表中添加额外的行（从原始表随机抽样）。
- en: Why the Chaos Llama? Well, “Anomalo,” if said sufficiently quickly, a sufficient
    number of times in a row, may sound a bit like “llama.” And so we chose the llama
    as our mascot, and they appear frequently in our internal tooling.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么是混乱羊驼？嗯，“Anomalo”如果连续说得足够快，足够多次，可能听起来有点像“羊驼”。所以我们选择了羊驼作为我们的吉祥物，并且它们经常出现在我们的内部工具中。
- en: '![Image](assets/adqm_05in01.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/adqm_05in01.png)'
- en: Benchmarking
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准测试
- en: Benchmarks are composed of many sample datasets; we call these *backtests*.
    Each backtest represents a historical sample from a dataset over a consecutive
    period of days. For each backtest, you can run your model on each day in sequential
    order. This simulates the dataset being configured and new records arriving each
    day. At this stage, important data points to capture are an overall anomaly score
    for each table (the same type of score used in [Figure 5-1](#example_of_table_anomaly_scores_and_lea))
    and the learned dynamic threshold for alerting. This will give you a baseline
    of how anomalous your model believes the benchmark data is *before* any synthetic
    anomalies are introduced.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试由许多样本数据集组成；我们称这些为*回测*。每个回测代表数据集在连续几天内的历史样本。对于每个回测，您可以按顺序在每一天运行您的模型。这模拟了数据集的配置以及每天到达新记录的过程。在此阶段，需要捕获的重要数据点是每个表的整体异常分数（与[图
    5-1](#example_of_table_anomaly_scores_and_lea)中使用的相同类型的分数）以及学习的动态警报阈值。这将为您提供一个基线，了解模型认为基准数据在引入任何合成异常之前的异常程度。
- en: Once your initial run is complete, you can then cycle back through the data—only
    this time, each day you’ll introduce a random chaos operation into the data. Then,
    you can rerun your model to see if it’s able to detect the chaos. Again, keeping
    track of the anomaly score will tell you how sensitive your model is to each particular
    chaos operation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 完成初始运行后，您可以再次循环遍历数据集——但这次每天都会引入一个随机的混乱操作到数据中。然后，您可以重新运行模型，看看它是否能够检测到这些混乱。同样，跟踪异常分数将告诉您模型对每种特定混乱操作的敏感性。
- en: For example, [Figure 5-6](#backtest_showing_the_anomaly_score_for) summarizes
    the results for the backtest on a single table containing data from April 29 to
    May 28\. We begin on April 29, build our model for that date, log all of the results,
    and then step through each date in turn until we finish on May 28\. Then we repeat
    this process while injecting synthetic data quality issues into the dataset.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[图 5-6](#backtest_showing_the_anomaly_score_for)总结了在单个表的回测结果，其中包含从4月29日到5月28日的数据。我们从4月29日开始，为该日期建立我们的模型，记录所有结果，然后依次遍历每个日期，直到5月28日结束。然后，我们在数据集中注入合成数据质量问题的同时重复此过程。
- en: '![Backtest showing the anomaly score for the table with and without chaos.
    The model learns a threshold for alerting as it experiences more and more anomalies
    over time.](assets/adqm_0506.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![回测显示了带有混乱和无混乱情况下的异常分数。随着时间的推移，模型会学习一个用于警报的阈值，因为遇到越来越多的异常。](assets/adqm_0506.png)'
- en: Figure 5-6\. Backtest showing the anomaly score for the table with and without
    chaos. The model learns a threshold for alerting as it experiences more and more
    anomalies over time.
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-6\. 回测显示了带有混乱和无混乱情况下的异常分数。随着时间的推移，模型会学习一个用于警报的阈值，因为遇到越来越多的异常。
- en: The yellow line shows the anomaly score from the model before any chaos is introduced.
    You can see that it rises on May 3 but otherwise remains close to zero. It’s common
    for a table to have higher anomaly scores in the early days, as the model is still
    learning what columns and features represent intermittent changes that need to
    be dampened to find true anomalies. The fact that the score stays near zero indicates
    that, once these are controlled for, the dataset is very predictable and regular.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 黄线显示了在引入任何混乱之前模型的异常分数。你可以看到它在5月3日上升，但其他时间基本保持接近于零。表格在早期可能会有较高的异常分数，因为模型仍在学习哪些列和特征代表着需要抑制以找出真正异常的间歇性变化。得分接近于零表明，一旦这些因素得到控制，数据集非常可预测和规律。
- en: The blue line shows the threshold for the anomaly score. This begins at 10,
    the highest possible anomaly score (representing an extreme change that affects
    100% of the data). We hold the score at this highest value for three days, and
    then it begins to exponentially decay. At the end of the 30 days, the threshold
    has fallen all the way to below 0.3, which would be sensitive enough to detect
    a moderate anomaly.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝线显示了异常分数的阈值。它从10开始，这是最高的异常分数（代表影响100%数据的极端变化）。我们在这最高值上保持三天，然后开始指数衰减。在30天结束时，阈值已经降低到0.3以下，这足够敏感以检测到中等异常。
- en: Finally, the red line shows what happens to the anomaly score when chaos is
    added to the dataset. On the second day, the score jumps over 5—an extreme anomaly
    likely caused by a significant chaos operation. But the threshold is still so
    high (10) that this issue would be suppressed. We want to be conservative about
    alerting, especially in the early days.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，红线展示了在数据集中引入混乱时异常分数的变化情况。第二天，得分跃升至5以上——这是一个极端异常，很可能是由重大的混乱操作引起的。但阈值仍然很高（10），因此此问题会被抑制。我们希望在提醒方面保守，特别是在早期阶段。
- en: By the fourth day, however, the score jumps above 6.5, and this is just high
    enough to pass the threshold. Even in the first week, a sufficiently anomalous
    change in the data can cause the model to alert. As we go past day 30, and ultimately
    day 90, the model becomes well calibrated to the level of expected noise in the
    data and is much more sensitive to chaos.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，到了第四天，得分跃升到6.5以上，这已经足够高超过了阈值。即使在第一周，数据中足够异常的变化也可以导致模型提醒。随着时间过去，最终到了第30天和第90天，模型会逐渐校准数据中预期噪声的水平，对混乱更加敏感。
- en: In [Figure 5-7](#backtests_for_nine_sample_datasets_over), the y-axis shows
    nine backtests for nine sample datasets, each evaluated over 30 days (the x-axis).
    A red square indicates that the model sends an alert, while a yellow square indicates
    that it was close to alerting. Gray squares are days when the model does not alert.
    As you can see, alerts are infrequent in the early days.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图5-7](#backtests_for_nine_sample_datasets_over)中，y轴显示了九个样本数据集的九次回测，每个数据集评估了30天（x轴）。红色方块表示模型发送了提醒，黄色方块表示它接近提醒。灰色方块是模型未提醒的天数。如您所见，早期的提醒并不频繁。
- en: '![Backtests for nine sample datasets over 30 days. A full-sized, full-color
    version is available at [direct link from repo].](assets/adqm_0507.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![九个样本数据集的30天回测。可从[存储库直接链接]获取全尺寸、全彩色版本。](assets/adqm_0507.png)'
- en: Figure 5-7\. Backtests for nine sample datasets over 30 days. See a full-sized
    version of this image at [*https://oreil.ly/adqm_5_7*](https://oreil.ly/adqm_5_7).
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-7\. 九个样本数据集的30天回测。查看此图的完整尺寸版本[*https://oreil.ly/adqm_5_7*](https://oreil.ly/adqm_5_7)。
- en: 'Comparing the top and bottom panels, it’s clear that the model is quite sensitive
    to the introduction of chaos, and that this is true for each of the different
    datasets. However, in the bottom panel, there are many days that are still gray
    (no alert)—even though we are applying a chaos operation every day. This can happen
    for a few different reasons:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 比较顶部和底部的面板，很明显模型对引入混乱非常敏感，这对每个不同的数据集都是如此。然而，在底部面板中，有许多天仍然是灰色的（没有提醒）——即使我们每天都应用混乱操作。这可能由于几个不同的原因造成：
- en: The threshold is still set very high, as we gradually decay the threshold from
    10 (very hard to alert) down towards the learned threshold for the dataset. Even
    by day 30, this decay is not yet complete.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阈值仍然设置得很高，因为我们逐渐将阈值从10（非常难以提醒）降低到数据集的学习阈值。即使到了第30天，这种衰减还没有完成。
- en: In some cases, we introduced chaos that is very rare. It may be targeted at
    only 1% of records, making the anomaly much harder to detect.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些情况下，我们引入的混乱非常罕见。它可能仅针对1%的记录，使得异常更难以检测。
- en: In some cases, the chaos that we introduce may not actually change the data.
    For instance, if we change 5% of the values in a column to be NULL, this won’t
    make a difference if 99% of the values in the column were already NULL.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些情况下，我们引入的混乱实际上可能不会改变数据。例如，如果我们将某列中5%的值更改为NULL，则如果该列中99%的值已经是NULL，这不会有任何影响。
- en: Analyzing performance
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析表现
- en: To understand how well the model performs, there are different types of statistics
    you can compute for the entire benchmark, such as AUC, F1 score, precision, and
    recall.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解模型的表现如何，您可以为整个基准计算不同类型的统计数据，如AUC、F1分数、精确度和召回率。
- en: 'No matter what metrics you choose to look at, there are many different ways
    of slicing and dicing the results:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您选择查看哪些指标，都有许多不同的方式来分析和解释结果：
- en: By dataset
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按数据集
- en: By how many days the model has been running
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型运行了多少天
- en: By the type of chaos used
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照使用的混乱类型
- en: By the percentage of records the chaos is applied to (the *chaos fraction*)
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按记录百分比应用混乱（*混乱分数*）
- en: In practice, we look at all of these, and more, to better understand how the
    model is performing and how we might improve it.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们查看所有这些指标，以及更多内容，以更好地理解模型的表现及其改进可能性。
- en: 'The chaos fraction is particularly useful, as it gives you a sense of:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 混乱分数特别有用，因为它让您了解
- en: How well the model is performing at the limit—when applying chaos to the entire
    dataset, you expect the model to perform very, very well.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型在极限情况下的表现如何——当对整个数据集应用混乱时，您期望模型表现非常好。
- en: Where the model begins to be unable to detect issues given the sample size of
    data that you are working with.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模型开始无法检测到问题时，考虑到您正在处理的数据样本大小。
- en: In [Figure 5-8](#performance_metrics_for_a_model_benchma), the x-axis is the
    fraction of records that we’ve applied a given chaos operation to (ranging from
    1% to 95% of records), and the y-axis represents a performance statistic.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图5-8](#performance_metrics_for_a_model_benchma)中，x轴是我们应用给定混乱操作的记录分数（从1%到95%的记录），y轴表示性能统计数据。
- en: '![Performance metrics for a model benchmark over different chaos fractions](assets/adqm_0508.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![不同混乱分数下模型基准的性能指标](assets/adqm_0508.png)'
- en: Figure 5-8\. Performance metrics for a model benchmark over different chaos
    fractions.
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-8\. 不同混乱分数下模型基准的性能指标。
- en: The first panel plots the AUC statistic. This compares the anomaly score that
    we produce for a given date (our “prediction”) to the binary “outcome” of whether
    chaos was introduced on that date. The AUC measures the area under a curve traced
    out by varying a decision threshold for the score from 0.0 up to 10.0 and measuring
    the false positive and true positive rates for classifying each {table, date}
    combination as either having chaos (an anomaly) or not. When the AUC is near 0.50,
    the model is performing no better than a random guess. When the AUC is near 1.0,
    the model is perfectly able to detect chaos in the data.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个面板绘制了AUC统计数据。这比较了我们对给定日期产生的异常分数（我们的“预测”）与是否在该日期引入混乱的二元“结果”的曲线下面积。AUC通过在从0.0到10.0变化的得分的决策阈值上测量假阳性和真阳性率，来分类每个{表，日期}组合，确定是否存在混乱（异常）。当AUC接近0.50时，模型的表现不比随机猜测好。当AUC接近1.0时，模型能够完美地检测到数据中的混乱。
- en: 'We see that the AUC steadily rises from near 0.50 at 1% chaos, to close to
    0.80 at 50% chaos and above. In practice, these AUC statistics underrepresent
    how well the model performs for several reasons:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，随着混乱程度从1%逐渐上升到50%及以上，AUC稳步提高，接近0.80。在实际操作中，由于多种原因，这些AUC统计数据未能充分展示模型的表现：
- en: Some of the {table, date} combinations that do not have chaos will actually
    have real anomalies in the source data that we would want to alert on. In other
    words, not all of the “negative” examples here are true negatives.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某些{表，日期}组合虽然没有混乱，但源数据中确实存在我们需要警示的真实异常。换句话说，并非所有“负面”示例都是真负例。
- en: Some of our chaos operations are affecting a much smaller percentage of records
    than the chaos fraction would suggest. The fraction places a maximum on the percentage
    of records affected—if we’re altering the mode of a column, or an infrequent value,
    or making a change that already exists in the data, the chaos may be impossible
    to detect.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的一些混乱操作影响的记录比混乱分数表明的百分比要少得多。该分数对受影响记录的百分比设置了一个最大值——如果我们改变了列的模式，或者一个不频繁的值，或者进行已存在数据中的更改，那么混乱可能无法检测到。
- en: We’re measuring performance over the first 30 days of the model run, but in
    practice, we see the performance of the model continue to increase dramatically
    up to 90 days.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在测量模型运行的前 30 天的性能，但在实践中，我们看到模型的性能继续在 90 天内显著提高。
- en: While the AUC is helpful to understand how good our model is at distinguishing
    if chaos was applied, it doesn’t explain if the model is doing a good job of setting
    a threshold. The second panel directly measures precision, recall, and F1 score
    based on the decision to alert if the score is above or below the learned threshold.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 AUC 有助于理解我们的模型在区分是否应用混乱时的表现如何，但它并不能解释模型在设置阈值时的表现如何。第二面板直接基于决定是否在得分高于或低于学习阈值时发出警报来测量精确率、召回率和
    F1 分数。
- en: In this context, precision measures what fraction of the time the model alerts
    when there is a chaos operation present. This can help you understand how often
    you might have false positive alerts. As you can see in [Figure 5-8](#performance_metrics_for_a_model_benchma),
    when the chaos affects a very small percentage of records, precision is about
    50%. But when the chaos affects a large percentage of records, it’s up to 90%.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，精确率衡量的是模型在存在混乱操作时警报的时间比例。这可以帮助您了解假阳性警报的频率。正如您可以在[图 5-8](#performance_metrics_for_a_model_benchma)中看到的那样，当混乱影响的记录非常少时，精确率约为
    50%。但当混乱影响的记录占比较大时，精确率可以达到 90%。
- en: Recall measures the percentage of chaos operations the model is able to alert
    on. Returning to [Figure 5-8](#performance_metrics_for_a_model_benchma), you can
    see that it begins quite low (near 0) but rises to almost 50% when chaos is applied
    to more than half of the data. Again, recall may be low because chaos operations
    are difficult to detect, due to the reasons stated above.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率衡量的是模型能够在混乱操作中警报的百分比。回到[图 5-8](#performance_metrics_for_a_model_benchma)，您可以看到它开始时非常低（接近
    0），但当混乱应用到超过一半的数据时，召回率几乎达到了 50%。同样，召回率可能较低是因为混乱操作很难检测到，这是由于上述原因造成的。
- en: One way to improve recall is to make adjustments to the algorithm for thresholding
    (start lower, decay faster, or converge to a lower quantile estimate for the score),
    thus making the model more sensitive to chaos. However, this increases the likelihood
    of false positives (and alert fatigue), especially in the early days when the
    model is still being calibrated.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 提高召回率的一种方法是对阈值算法进行调整（从较低开始，更快衰减，或者收敛到得分的较低分位数估计），从而使模型对混乱更敏感。然而，这会增加假阳性（和警报疲劳）的可能性，特别是在模型仍在校准阶段的早期。
- en: The F1 score takes into account both the precision and the recall and is computed
    as `2*((precision*recall)/(precision+recall))`. There are many changes that might
    improve precision but not recall, or vice versa, and the F1 score gives an indication
    of whether a change is good or bad based on the combined effect. Note that in
    practice, the cost of false positives and false negatives can be estimated directly
    and used to make decisions about how to improve and calibrate the model.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: F1 分数同时考虑了精确率和召回率，并且计算公式为`2*((precision*recall)/(precision+recall))`。有很多改进可能会提高精确率但不会提高召回率，反之亦然，而
    F1 分数则根据综合效果指示改进是否好坏。请注意，在实践中，假阳性和假阴性的成本可以直接估计，并用于决策如何改进和校准模型。
- en: Putting it together with pseudocode
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将其与伪代码结合起来
- en: The following Python pseudocode gives an example of how you could apply the
    approach we’ve just outlined to benchmark your algorithm. It assumes you have
    the `detect_anomalies` method from [Chapter 4](ch04.html#automating_data_quality_monitoring_wi),
    and a collection of tables and their configuration information. It then runs a
    backtest for each table with and without chaos and summarizes the anomaly scores
    for each. Then it computes the AUC of the algorithm based on how accurate it is
    at “predicting” which results had chaos introduced.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Python伪代码的示例，演示了您如何应用我们刚刚概述的方法来评估您的算法性能。假设您从[第四章](ch04.html#automating_data_quality_monitoring_wi)中有`detect_anomalies`方法，并且有一组表格及其配置信息。然后，它对每个表格进行带有混乱和不带混乱的回测，并总结每个表格的异常分数。然后，根据算法在“预测”哪些结果引入混乱方面的准确性计算AUC。
- en: Please don’t take this code too literally, as it’s just meant to illustrate
    the concepts and how they fit together at a high level. In particular, we’ve glossed
    over the fact that the algorithm may maintain state from one run to the next (e.g.,
    the threshold calculation), and we haven’t produced any details for the random
    chaos generation.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 请不要太字面理解这段代码，因为它只是为了说明概念及其高层次的结合方式。特别是，我们忽略了算法可能从一次运行到下一次运行中保持状态（例如，阈值计算），并且我们没有为随机混乱生成提供任何详细信息。
- en: 'We start by defining how to calculate anomaly scores for a given date range
    and table:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义如何计算给定日期范围和表格的异常分数：
- en: '[PRE1]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then we backtest the anomaly detection logic over a range of dates to get the
    anomaly scores with and without chaos:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在一系列日期范围内对异常检测逻辑进行回测，以获取带混乱和不带混乱的异常分数：
- en: '[PRE2]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we run backtests on multiple table configurations to benchmark their
    performance:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对多种表格配置运行回测，以评估它们的性能：
- en: '[PRE3]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we calculate a single AUC metric (as an example model metric) based
    on the anomaly and chaos scores from all the tables:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们根据所有表格中的异常和混乱分数计算单个AUC指标（作为示例模型指标）：
- en: '[PRE4]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Improving the Model
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改进模型
- en: Benchmarking and analyzing performance statistics is extraordinarily helpful
    to understand and debug your model. It’s also a way to prove to users that your
    system is working as expected.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 进行基准测试和分析性能统计数据对理解和调试模型非常有帮助。它也是向用户证明您的系统按预期工作的一种方式。
- en: But one of the most important ways you can use this data is to validate changes
    to your model, to ensure you’re fine-tuning it in a way that will add value. [Figure 5-9](#summary_statistics_and_implications_for)
    shows an example of summary statistics computed for a benchmark, with the goal
    of measuring if a change to the model is moving the stats in the right direction.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，您可以使用这些数据的最重要的方式之一是验证模型的变化，以确保您在微调中增加了价值。[图 5-9](#summary_statistics_and_implications_for)展示了为基准计算的摘要统计信息的示例，其目标是衡量模型变化是否朝着正确的方向发展。
- en: '![Summary statistics and implications for a model benchmark](assets/adqm_0509.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![模型基准的摘要统计和影响](assets/adqm_0509.png)'
- en: Figure 5-9\. Summary statistics and implications for a model benchmark. See
    a full-sized version of this image at [*https://oreil.ly/adqm_5_9*](https://oreil.ly/adqm_5_9).
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-9\. 模型基准的摘要统计和影响。查看此图片的完整版本，请访问[*https://oreil.ly/adqm_5_9*](https://oreil.ly/adqm_5_9)。
- en: The hypothetical update being evaluated in [Figure 5-9](#summary_statistics_and_implications_for)
    was a new type of feature that allowed the model to be even more sensitive to
    changes in patterns in string columns (think phone numbers, identifiers, etc.).
    The change increased the benchmark’s runtime meaningfully, which suggests it may
    be important to optimize the change further to avoid increasing the overall latency
    and costs of the model.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-9](#summary_statistics_and_implications_for)中评估的假设更新是一种新类型的功能，允许模型对字符串列中的模式变化（如电话号码、标识符等）更加敏感。此变更显著增加了基准的运行时间，这表明可能需要进一步优化该变更，以避免增加模型的总体延迟和成本。'
- en: However, on the positive side, the change significantly improved both precision
    and recall by reducing the number of false negatives. The AUC also improved, although
    the percentage improvement appears small on an absolute basis. But if instead
    we ask how much greater than 0.5 did the AUC rise (since that indicates random
    behavior), the AUC improvement is actually (0.624 – 0.5) / (0.617 – 0.5) – 1 =
    5% better, which is significant.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从积极的一面来看，这一变化显著提高了精度和召回率，减少了误报。AUC也有所改善，尽管在绝对基础上看起来改进的百分比很小。但是，如果我们问的是AUC相比0.5提高了多少（因为0.5表示随机行为），AUC的改善实际上是（0.624
    - 0.5）/（0.617 - 0.5） - 1 = 5% 更好，这是显著的。
- en: Conclusion
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We’ve explored some of the attributes of real-world data that are likely to
    require care when building your model—such as the fact that data is correlated
    or gets updated in place. And we’ve walked through how you can benchmark your
    models by injecting chaos into sample datasets. The outcome of this testing will
    help you measure your model’s performance and iterate on it over time.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了在构建模型时可能需要注意的一些真实世界数据属性——比如数据的相关性或就地更新的事实。我们还详细介绍了如何通过向样本数据集注入混乱来对模型进行基准测试。这些测试的结果将帮助您衡量模型的性能并随时间进行迭代。
- en: It isn’t easy to build a model that alerts appropriately for real-world data
    issues—not missing important issues, nor over-alerting on minor changes. Once
    you have a high-quality model, though, what really matters is how you use it to
    empower the humans responsible for data quality. In the next chapter, we’ll explain
    how to leverage your model’s output to build effective notifications that help
    users get to the bottom of unusual changes in their data.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 建立一个能够适当地警示真实世界数据问题的模型并不容易——既不能错过重要问题，也不能因小而过警报。然而，一旦你拥有了高质量的模型，真正重要的是如何利用它来赋能负责数据质量的人员。在下一章中，我们将解释如何利用你的模型输出来构建有效的通知，帮助用户彻底了解数据异常变化的原因。
- en: ^([1](ch05.html#ch01fn3-marker)) This is very important, as otherwise you could
    remove a data quality issue from your dataset!
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.html#ch01fn3-marker)) 如果不这样做，你可能会从数据集中删除一个数据质量问题！
