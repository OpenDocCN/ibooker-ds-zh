- en: Part 2\. Advanced topics in GANs
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二部分\. GANs的高级主题
- en: '[Part 2](#part02) explores a selection of advanced topics in GANs. Building
    on the foundational concepts from [part 1](../Text/kindle_split_009.xhtml#part01),
    you will deepen your theoretical understanding of GANs and expand your practical
    toolkit of GAN implementations:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[第二部分](#part02)探讨了GANs的一些高级主题。在[第1部分](../Text/kindle_split_009.xhtml#part01)的基础概念之上，你将深化对GANs的理论理解，并扩展GAN实现的实际工具箱：'
- en: '[Chapter 5](../Text/kindle_split_015.xhtml#ch05) covers many of the theoretical
    and practical hurdles to training GANs and how to overcome them.'
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第5章](../Text/kindle_split_015.xhtml#ch05)涵盖了训练GANs的理论和实践障碍以及如何克服它们。'
- en: '[Chapter 6](../Text/kindle_split_016.xhtml#ch06) presents a groundbreaking
    training methodology called *Progressive GAN* that has enabled GANs to synthesize
    images with unprecedented resolution.'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第6章](../Text/kindle_split_016.xhtml#ch06)介绍了一种开创性的训练方法，称为*渐进式GAN*，它使GANs能够以前所未有的分辨率合成图像。'
- en: '[Chapter 7](../Text/kindle_split_017.xhtml#ch07) covers the use of GANs in
    semi-supervised learning (methods of training classifiers with only a small fraction
    of labeled examples), an area of immense practical importance.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第7章](../Text/kindle_split_017.xhtml#ch07)涵盖了GANs在半监督学习（仅用一小部分标记示例训练分类器的方法）中的应用，这是一个具有巨大实际重要性的领域。'
- en: '[Chapter 8](../Text/kindle_split_018.xhtml#ch08) introduces the Conditional
    GAN, a technique that enables targeted data generation by using labels (or other
    conditioning information) while training the Generator and Discriminator.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第8章](../Text/kindle_split_018.xhtml#ch08)介绍了条件GAN，这是一种在训练生成器和判别器时使用标签（或其他条件信息）来实现有针对性的数据生成的技术。'
- en: '[Chapter 9](../Text/kindle_split_019.xhtml#ch09) explores the CycleGAN, a general-purpose
    technique for image-to-image translation—turning one image (such as a photo of
    an apple) into another (such as a photo of an orange).'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第9章](../Text/kindle_split_019.xhtml#ch09)探讨了CycleGAN，这是一种通用的图像到图像翻译技术——将一个图像（如苹果的照片）转换为另一个图像（如橙子的照片）。'
- en: 'Chapter 5\. Training and common challenges: GANing for success'
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5章\. 训练和常见挑战：GANing for success
- en: '*This chapter covers*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Meeting the challenges of evaluating GANs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应对评估GANs的挑战
- en: Min-Max, Non-Saturating, and Wasserstein GANs
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小-最大、非饱和和水合GANs
- en: Using tips and tricks to best train a GAN
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用技巧和窍门来最佳训练GAN
- en: '|  |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: When reading this chapter, please remember that GANs are notoriously hard to
    both train and evaluate. As with any other cutting-edge field, opinions about
    what is the best approach are always evolving.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本章时，请记住，GANs在训练和评估方面都非常有名，难以进行。与其他任何尖端领域一样，关于最佳方法的观点总是不断演变。
- en: '|  |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Papers such as “How to Train Your DRAGAN” are a testament to both the incredible
    capacity of machine learning researchers to make bad jokes and the difficulty
    of training Generative Adversarial Networks well. Dozens of arXiv papers preoccupy
    themselves solely with the aim of improving the training of GANs, and numerous
    workshops have been dedicated to various aspects of training at top academic conferences
    (including Neural Information Processing Systems, or NIPS, one of the prominent
    machine learning conferences^([[1](#ch05fn01)])).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如“如何训练您的DRAGAN”之类的论文既证明了机器学习研究人员制造糟糕笑话的惊人能力，也说明了训练生成对抗网络（GANs）的难度。数十篇arXiv论文专注于提高GANs训练的目的，许多研讨会也致力于在顶级学术会议（包括神经信息处理系统，或NIPS，这是一场重要的机器学习会议^([[1](#ch05fn01)]))的各个方面进行培训。
- en: ¹
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: NIPS 2016 featured a workshop on GAN training with many important researchers
    in the field, which this chapter was based on. NIPS has recently changed its abbreviation
    to NeurIPS.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: NIPS 2016年举办了一次GAN训练研讨会，许多该领域的著名研究人员参加了此次研讨会，本章即基于此。NIPS最近将其缩写更改为NeurIPS。
- en: But GAN training is an evolving challenge, and so a lot of resources—including
    those presented through papers and conferences—now need a certain amount of updating.
    This chapter provides a comprehensive yet up-to-date overview of training techniques.
    In this chapter, you also finally get to experience something no one has ever
    been known to hate—math. (But we promise not to use more than strictly necessary.)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 但GAN训练是一个不断发展的挑战，因此现在需要更新大量资源——包括通过论文和会议展示的资源。本章提供了一个全面且最新的训练技术概述。在本章中，你也将最终体验到没有人会讨厌的东西——数学。（但我们承诺不会使用严格必要的更多。）
- en: Jokes aside, however, as the first chapter in the “[Advanced Topics in GANs](../Text/kindle_split_014.xhtml#part02)”
    section of this book, this is quite a dense chapter. We recommend that you go
    back and try some of the models with several parameters. Then you can return to
    this chapter, as you should be reading it with a strong understanding of not just
    what each part of a GAN does, but also the challenges in training them from your
    own experience.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 开个玩笑，然而，作为本书“[GANs高级主题](../Text/kindle_split_014.xhtml#part02)”部分的第一章，这一章内容相当密集。我们建议您回顾并尝试一些具有多个参数的模型。然后您就可以回到这一章，正如您应该带着对GAN的每个部分的功能以及训练它们所面临的挑战有深刻理解来阅读它。
- en: Like the other chapters in this advanced section, this chapter is here to teach
    you as well as to provide a useful reference for at least a couple of years to
    come. Therefore, this chapter is a summary of the tips and tricks from people’s
    experiences, blog posts, and most relevant papers. (If academia is not your cup
    of tea, now is the time to get out those doodling pens and scribble over the footnotes.)
    We look at this chapter as a short academic intermission that will give you a
    clear map indicating all the amazing present and future developments of GANs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 就像这个高级部分的其他章节一样，这一章旨在教会您，同时也为未来几年提供有用的参考。因此，这一章是对人们经验、博客文章和最相关论文中的技巧和窍门的总结。（如果您不喜欢学术，现在是时候拿出那些涂鸦笔在脚注上涂鸦了。）我们将这一章视为一个短暂的学术休息，它将为您提供一张清晰的地图，指示GAN的现在和未来的所有惊人发展。
- en: We also hope to thereby equip you with all the basic tools to understand the
    vast majority of new papers that may come out. In many books, this would be presented
    as pros and cons lists that would not give readers the full high-level understanding
    of the choices. But because GANs are such a new field, simple lists are not possible,
    as the literature has still not agreed on some aspects conclusively. GANs are
    also a fast-growing field, so we would much prefer to equip you with the ability
    to navigate it, rather than give you information that is likely to soon be outdated.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望这样让您掌握所有基本工具，以便理解可能出现的绝大多数新论文。在许多书中，这会被呈现为优缺点列表，这不会给读者提供对选择的全面高级理解。但由于GAN是一个如此新的领域，简单的列表是不可能的，因为文献还没有在一些方面达成最终共识。GAN也是一个快速发展的领域，所以我们更愿意让您具备导航这个领域的能力，而不是提供可能很快就会过时的信息。
- en: With the purpose of this chapter explained, let’s clarify where GANs sit again.
    [Figure 5.1](#ch05fig01) expands on the diagram from [chapter 2](../Text/kindle_split_011.xhtml#ch02)
    and shows the taxonomy of the models so you can understand what other generative
    techniques exist and how (dis)similar they are.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释了本章的目的之后，让我们再次明确GAN的位置。[图5.1](#ch05fig01)扩展了第二章中的图表，并展示了模型的分类学，以便您了解存在哪些其他生成技术以及它们是如何（不）相似的。
- en: Figure 5.1\. Where do GANs fit in?
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.1\. GAN在哪里？
- en: '![](../Images/05fig01_alt.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/05fig01_alt.jpg)'
- en: '(Source: “Generative Adversarial Networks (GANs),” by Ian Goodfellow, NIPS
    2016 tutorial, [http://mng.bz/4O0V](http://mng.bz/4O0V).)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：“Generative Adversarial Networks (GANs)，” by Ian Goodfellow，NIPS 2016教程，[http://mng.bz/4O0V](http://mng.bz/4O0V)）
- en: 'There are two key takeaways from this diagram:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个图表中可以得出两个关键要点：
- en: All of these generative models ultimately derive from Maximum Likelihood, at
    least implicitly.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有这些生成模型最终都源于最大似然，至少是隐式的。
- en: The variational autoencoder introduced in [chapter 2](../Text/kindle_split_011.xhtml#ch02)
    sits in the Explicit part of the tree. Remember that we had a clear loss function
    (the reconstruction loss)? Well, with GANs we do not have it anymore. Rather,
    we now have two competing loss functions that we will cover in lot more depth
    later. But as such, the system does not have a single analytical solution.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二章中引入的变分自动编码器位于树的显式部分。记住我们有一个清晰的损失函数（重建损失）吗？好吧，在GAN中我们不再有了。相反，我们现在有两个相互竞争的损失函数，我们将在后面的内容中更深入地探讨。但因此，系统没有单一的解析解。
- en: 'If you know any of the other techniques pictured, that’s great. The key idea
    is that we are moving away from explicit and tractable, into the territory of
    implicit approaches toward training. However, by now you should be wondering:
    if we do not have an explicit loss function (even though we have the two separate
    losses encountered implicitly in the “[Conflicting objectives](../Text/kindle_split_012.xhtml#ch03lev2sec3)”
    section of [chapter 3](../Text/kindle_split_012.xhtml#ch03)), how do we evaluate
    a GAN? What if you’re running parallel, large-scale experiments?'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你了解图中展示的其他任何技术，那很好。关键思想是我们正在从显式和可处理的领域转向隐式方法来训练。然而，到目前为止，你应该在想：如果我们没有显式的损失函数（尽管我们在第3章的“[冲突目标](../Text/kindle_split_012.xhtml#ch03lev2sec3)”部分的[第3章](../Text/kindle_split_012.xhtml#ch03)中遇到了两个单独的损失），我们如何评估GAN？如果你正在进行并行、大规模的实验呢？
- en: To clear up potential confusion, not all the techniques in [figure 5.1](#ch05fig01)
    come from deep learning, and we certainly do not need you to know any of them,
    other than VAEs and GANs!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了消除潜在的混淆，[图5.1](#ch05fig01)中的所有技术并不都来自深度学习，我们当然不需要你了解它们中的任何一种，除了VAEs和GANs！
- en: 5.1\. Evaluation
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1. 评估
- en: Let’s revisit the [chapter 1](../Text/kindle_split_010.xhtml#ch01) analogy about
    forging a da Vinci painting. Imagine that a forger (Generator) is trying to mimic
    da Vinci, to get the forged painting accepted at an exhibition. This forger is
    competing against an art critic (Discriminator) who is trying to accept only real
    work into the exhibition. In this circumstance, if you are the forger who is aiming
    to create a “lost piece” by this great artist in order to fool the critic with
    a flawless impersonation of da Vinci’s style, how would you evaluate how well
    you’re doing? How would each actor evaluate their performance?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新回顾一下关于伪造达芬奇画作的第[1章](../Text/kindle_split_010.xhtml#ch01)的类比。想象一下，一个伪造者（生成器）正在试图模仿达芬奇，以便将伪造的画作在展览会上获得认可。这个伪造者正在与一个艺术评论家（判别器）竞争，后者试图只接受真正的作品进入展览。在这种情况下，如果你是那个试图通过模仿达芬奇的风格创作出一幅“失传之作”来欺骗评论家的伪造者，你将如何评估你的表现？每个参与者将如何评估他们的表现？
- en: GANs are trying to solve the problem of never-ending competition between the
    forger and the art critic. Indeed, given that typically the Generator is of greater
    interest than the Discriminator, we should think about its evaluation extra carefully.
    But how would we quantify the style of a great painter or how closely we imitate
    it? How would we quantify the overall quality of the generation?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: GANs试图解决伪造者和艺术评论家之间永无止境的竞争问题。确实，鉴于通常生成器比判别器更有兴趣，我们应该格外仔细地考虑其评估。但是，我们如何量化一位伟大画家的风格或我们模仿得有多接近？我们如何量化生成的整体质量？
- en: 5.1.1\. Evaluation framework
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1. 评估框架
- en: The best solution would be to have da Vinci paint *all the paintings* that are
    possible to paint, using his style, and then see whether the image generated using
    a GAN would be somewhere in that collection. You can think of this process as
    a nonapproximate version of maximum likelihood maximization. In fact, we would
    know that the image either is or is not in this set, so no likelihood is involved.
    However, in practice, this solution is never really possible.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳解决方案是让达芬奇用他的风格绘制所有可能绘制的画作，然后看看使用GAN生成的图像是否会在那个集合中。你可以将这个过程视为最大似然最大化的一种非近似版本。实际上，我们会知道图像要么在这个集合中，要么不在，所以没有涉及到似然。然而，在实践中，这个解决方案从未真正可行。
- en: The next best thing would be to assess the image and point to instances of what
    to look for and then add up the number of errors or artifacts. But these will
    be highly localized and ultimately would always require a human critic to look
    at the art piece itself. It is a fundamentally nonscalable—although probably the
    second best—solution.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个最好的办法是评估图像，指出要寻找的实例，然后累计错误或伪影的数量。但这些将非常局部化，最终总是需要人类评论家亲自查看艺术品本身。这是一个本质上不可扩展的——尽管可能是第二好的——解决方案。
- en: We want to have a statistical way of evaluating the quality of the generated
    samples, because that would scale and would allow us to evaluate as we are experimenting.
    If we do not have an easy metric to calculate, we also cannot monitor progress.
    This is a problem especially for evaluating different experiments—imagine measuring
    or even backpropagating with a human in the loop at each, for example, hyperparameter
    initialization. This is especially a problem, given that GANs tend to be quite
    sensitive to hyperparameters. So not having a statistical metric is difficult,
    because we’d have to check back with humans every time we want to evaluate the
    quality of training.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望有一种统计方法来评估生成样本的质量，因为这可以扩展，并允许我们在实验中进行评估。如果我们没有一个容易计算的指标，我们也无法监控进度。这对于评估不同的实验尤其是一个问题——想象一下在每个实验中都要测量或甚至反向传播，例如超参数初始化。鉴于GANs对超参数非常敏感，所以没有统计指标是困难的，因为每次我们想要评估训练质量时，我们都必须回过头来检查人类。
- en: Why don’t we just use something that we already understand, such as maximum
    likelihood? It is statistical and measures something vaguely desirable, and we
    implicitly derive from it anyway. Despite this, maximum likelihood is difficult
    to use because we need to have a good estimate of the underlying distribution
    and its likelihood—and that may mean more than billions of images.^([[2](#ch05fn02)])
    There are also reasons to want to go beyond maximum likelihood, even if we just
    had a good sample—which is what we effectively have with the training set.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么不直接使用我们已理解的东西，比如最大似然？它是统计的，衡量的是某种模糊的期望值，而且我们无论如何都会从它那里隐式地推导出来。尽管如此，最大似然难以使用，因为我们需要有一个对潜在分布及其似然的良好估计——这可能意味着超过数十亿张图片。[2](#ch05fn02)
    此外，即使我们有一个好的样本——这就是我们实际上在训练集中拥有的样本——也有理由超越最大似然。
- en: ²
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We give the problems of dimensionality better treatment in [chapter 10](../Text/kindle_split_021.xhtml#ch10).
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们在[第10章](../Text/kindle_split_021.xhtml#ch10)中对维度问题进行了更好的处理。
- en: What else is wrong with maximum likelihood? After all, it is a well-established
    metric in much of the machine learning research. Generally, maximum likelihood
    has lots of desirable properties, but as we have touched on, using it is not tractable
    as an evaluation technique for GANs.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最大似然还有什么问题？毕竟，它是机器学习研究中的一个成熟指标。一般来说，最大似然有很多理想的特性，但正如我们所提到的，将其用作GANs的评估技术并不容易处理。
- en: Furthermore, in practice, approximations of maximum likelihood tend to overgeneralize
    and therefore deliver samples that are too varied to be realistic.^([[3](#ch05fn03)])
    Under maximum likelihood, we may find samples that would never occur in the real
    world, such as a dog with multiple heads or a giraffe with dozens of eyes but
    no body. But because we don’t want GAN violence to give anyone nightmares, we
    should probably weed out samples that are “too general,” using a loss function
    and/or the evaluation method.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在实践中，最大似然估计的近似往往过于泛化，因此提供的样本变化过多，不够真实。[3](#ch05fn03) 在最大似然下，我们可能会发现现实中不可能出现的样本，例如多头狗或没有身体的多头长颈鹿。但因为我们不希望GAN的暴力行为让人做噩梦，所以我们可能需要使用损失函数和/或评估方法来剔除“过于泛化”的样本。
- en: ³
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See “How (Not) to Train your Generative Model: Scheduled Sampling, Likelihood,
    Adversary?” by Ferenc Huszár, 2015, [http://arxiv.org/abs/1511.05101](http://arxiv.org/abs/1511.05101).'
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅Ferenc Huszár于2015年发表的论文“如何（不）训练你的生成模型：计划采样、似然、对抗者？”，[http://arxiv.org/abs/1511.05101](http://arxiv.org/abs/1511.05101)。
- en: Another way to think about overgeneralization is to start with a probability
    distribution of fake and real data (for example, images) and look at what the
    distance functions (a way to measure distance between real and fake images’ distributions)
    would do in cases where there should be zero probability mass. The additional
    loss due to these overgeneral samples could be tiny if they are not too different,
    for example, because these modes are close to real data in all but a few key problems
    such as multiple heads. An overgeneral metric would therefore allow creation of
    samples even when, according to the true data-generating process, there should
    not be any, such as a cow with multiple heads.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑过度泛化的另一种方法是，从假数据和真实数据（例如，图像）的概率分布开始，看看在应该为零概率质量的情况下，距离函数（一种衡量真实和假图像分布之间距离的方法）会做什么。如果这些过度泛化样本不太不同，那么由于这些样本的额外损失可能很小，例如，因为这些模式在所有关键问题（如多头）之外都接近真实数据。因此，一个过度泛化的指标将允许在根据真实数据生成过程，本不应该有任何样本的情况下创建样本，例如多头牛。
- en: That is why researchers felt that we need different evaluation principles even
    though what we are effectively doing is always maximizing likelihood. We are just
    measuring it in different ways. For those curious, KL divergence and JS divergence—which
    we will visit in a bit—are also based on maximum likelihood, so here we can treat
    them as interchangeable.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正因如此，研究人员认为我们需要不同的评估原则，尽管我们实际上总是在最大化似然。我们只是在不同的方式上测量它。对于那些好奇的人来说，KL 散度和 JS 散度——我们稍后会讨论——也是基于最大似然，所以在这里我们可以将它们视为可互换的。
- en: 'Thus you now understand that we have to be able to evaluate a sample and that
    we cannot simply use maximum likelihood to do this. In the following pages, we
    will talk about the two most commonly used and accepted metrics for statistically
    evaluating the quality of the generated samples: the *inception score (IS)* and
    *Fréchet inception distance (FID)*. The advantage of those two metrics is that
    they have been extensively validated to be highly correlated with at least some
    desirable property such as visual appeal or realism of the image. The inception
    score was designed solely around the idea that the samples should be recognizable,
    but it has also been shown to correlate with human intuition about what constitutes
    a real image, as validated by Amazon Mechanical Turkers.^([[4](#ch05fn04)])'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你现在明白我们必须能够评估一个样本，而不能简单地使用最大似然来进行评估。在接下来的几页中，我们将讨论统计评估生成样本质量最常用和接受的两种指标：*
    inception 分数 (IS)* 和 *Fréchet inception 距离 (FID)*。这两个指标的优势在于它们已经被广泛验证，与至少某些期望的特性高度相关，例如视觉吸引力或图像的真实感。inception
    分数的设计完全围绕样本应可识别的想法，但它也已经显示出与人类对构成真实图像的直觉相关联，这一点通过亚马逊机械工人验证。
- en: ⁴
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Amazon Mechanical Turk is a service that allows you to purchase people’s time
    by the hour to work on a prespecified task. It’s something like on-demand freelancers
    or Task Rabbit, but only online.
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 亚马逊机械工人是一种服务，允许你按小时购买人们的时间来完成预指定的任务。这就像按需自由职业者或 Task Rabbit，但仅限于在线。
- en: 5.1.2\. Inception score
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2\. Inception 分数
- en: 'We clearly need a good statistical evaluation method. Let’s start from a high-level
    wish list of what our ideal evaluation method would ensure:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们显然需要一个良好的统计评估方法。让我们从理想评估方法应确保的高层次愿望清单开始：
- en: The generated samples look like some real, distinguishable thing—for example,
    buckets or cows. The samples look real, and we can generate samples of items in
    our dataset. Moreover, our classifier is confident that what it sees is an item
    it recognizes. Luckily, we already have computer vision classifiers that are able
    to classify an image as belonging to a particular class, with certain confidence.
    Indeed, the score itself is named after the Inception network, which is one of
    those classifiers.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的样本看起来像一些真实、可区分的东西——例如，桶或牛。样本看起来很真实，我们可以生成数据集中物品的样本。此外，我们的分类器确信它看到的是它所识别的物品。幸运的是，我们已经有能够将图像分类为特定类别并具有一定置信度的计算机视觉分类器。实际上，这个分数本身是以
    Inception 网络命名的，而 Inception 网络就是那些分类器之一。
- en: The generated samples are varied and contain, ideally, all the classes that
    were represented in the original dataset. This point is also highly desirable
    because our samples should be representative of the dataset we gave it; if our
    MNIST-generating GAN is always missing the number 8, we would not have a good
    generative model. We should have no *interclass* (between classes) mode collapse.^([[5](#ch05fn05)])
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的样本种类繁多，理想情况下应包含原始数据集中表示的所有类别。这一点也非常理想，因为我们的样本应该代表我们给出的数据集；如果我们的MNIST生成GAN总是缺少数字8，我们就不会有一个好的生成模型。我们不应该有*类间*（类别之间）的模式坍塌.^([[5](#ch05fn05)])
- en: ⁵
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “An Introduction to Image Synthesis with Generative Adversarial Nets,” by
    He Huang et al., 2018, [https://arxiv.org/pdf/1803.04469.pdf](https://arxiv.org/pdf/1803.04469.pdf).
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅“生成对抗网络图像合成的介绍”，由He Huang等人著，2018年，[https://arxiv.org/pdf/1803.04469.pdf](https://arxiv.org/pdf/1803.04469.pdf)。
- en: Although we might have further requirements of our generative model, this is
    a good start.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可能对我们的生成模型有进一步的要求，但这是一个良好的开端。
- en: The *inception score (IS)* was first introduced in a 2016 paper that extensively
    validated this metric and confirmed that it indeed correlates with human perceptions
    of what constitutes a high-quality sample.^([[6](#ch05fn06)]) This metric has
    since become popular in the GAN research community.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '* inception分数（IS）* 首次在2016年的一篇论文中提出，该论文广泛验证了这一指标，并确认它确实与人类对高质量样本构成的认识相关.^([[6](#ch05fn06)])这一指标自那以后在GAN研究社区中变得流行。'
- en: ⁶
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Improved Techniques for Training GANS,” by Tim Salimans et al., 2016, [https://arxiv.org/pdf/1606.03498.pdf](https://arxiv.org/pdf/1606.03498.pdf).
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅“改进的GANS训练技术”，由Tim Salimans等人著，2016年，[https://arxiv.org/pdf/1606.03498.pdf](https://arxiv.org/pdf/1606.03498.pdf)。
- en: 'We have explained why we want to have this metric. Now let’s dive into the
    technical details. Computing the IS a simple process:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经解释了为什么我们想要这个指标。现在让我们深入了解技术细节。计算IS是一个简单的过程：
- en: We take the Kullback–Leibler (KL) divergence between the real and the generated
    distribution.^([[7](#ch05fn07)])
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算真实分布和生成分布之间的Kullback–Leibler（KL）散度.^([[7](#ch05fn07)])
- en: ⁷
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: We introduced KL divergence in [chapter 2](../Text/kindle_split_011.xhtml#ch02).
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们在第二章中介绍了KL散度。
- en: We exponentiate the result of step 1.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对步骤1的结果进行指数化。
- en: 'Let’s look at an example: a failure mode in an Auxiliary Classifier GAN (ACGAN),^([[8](#ch05fn08)])
    where we were trying to generate examples of daisies from the ImageNet dataset.
    When we ran the Inception network on the following ACGAN failure mode, we saw
    something like [figure 5.2](#ch05fig02); your results may differ, depending on
    your OS, TensorFlow version, and implementation details.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子：一个辅助分类器生成对抗网络（ACGAN）的失败模式，^([[8](#ch05fn08)])其中我们试图从ImageNet数据集中生成雏菊的示例。当我们对以下ACGAN失败模式运行Inception网络时，我们看到了类似[图5.2](#ch05fig02)的东西；你的结果可能因操作系统、TensorFlow版本和实现细节而异。
- en: ⁸
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸
- en: ''
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Conditional Image Synthesis with Auxiliary Classifier GANs,” by Augustus
    Odena et al., 2017, [https://arxiv.org/pdf/1610.09585.pdf](https://arxiv.org/pdf/1610.09585.pdf).
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅“Auxiliary Classifier GANs的辅助图像合成”，由Augustus Odena等人著，2017年，[https://arxiv.org/pdf/1610.09585.pdf](https://arxiv.org/pdf/1610.09585.pdf)。
- en: Figure 5.2\. ACGAN failure mode. Scores on the right indicate the softmax output.
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.2\. ACGAN失败模式。右侧的分数表示softmax输出。
- en: '![](../Images/05fig02_alt.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05fig02_alt.jpg)'
- en: '(Source: Odena, 2017, [https://arxiv.org/pdf/1610.09585.pdf](https://arxiv.org/pdf/1610.09585.pdf).)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: (来源：Odena，2017年，[https://arxiv.org/pdf/1610.09585.pdf](https://arxiv.org/pdf/1610.09585.pdf)。)
- en: The important thing to note here is that the Inception classifier is not certain
    what it’s looking at, especially among the first three categories. Humans would
    work out that it’s probably a flower, but even we are not sure. Overall confidence
    in the predictions is also quite low (scores go up to 1.00). This is an example
    of something that would receive a low IS, which matches our two requirements from
    the start of the section. Thus, our metrics journey has been a success, as this
    matches our intuition.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的重要一点是，Inception分类器并不确定它在看什么，尤其是在前三个类别中。人类会推断这可能是一朵花，但我们也不确定。总体预测的置信度也相当低（分数最高为1.00）。这是一个会得到低IS的例子，这与本节开头我们提出的两个要求相匹配。因此，我们的指标之旅是成功的，因为这符合我们的直觉。
- en: 5.1.3\. Fréchet inception distance
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3\. Fréchet inception距离
- en: 'The next problem to solve is the lack of variety of examples. Frequently, GANs
    learn only a handful of images for each class. In 2017, a new solution was proposed:
    the *Fréchet inception distance (FID)*.^([[9](#ch05fn09)]) The FID improves on
    the IS by making it more robust to noise and allowing the detection of *intraclass*
    (within class) sample omissions.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来要解决的问题是没有足够的例子。通常，GAN只为每个类别学习少量图像。2017年，提出了一种新的解决方案：*Fréchet inception distance
    (FID)*.^([[9](#ch05fn09)]) FID通过使其对噪声更鲁棒并允许检测*类内*（类内）样本缺失来改进IS。
- en: ⁹
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹
- en: ''
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium,”
    by Martin Heusel et al., 2017, [http://arxiv.org/abs/1706.08500](http://arxiv.org/abs/1706.08500).
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见“GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium”，作者Martin
    Heusel等，2017年，[http://arxiv.org/abs/1706.08500](http://arxiv.org/abs/1706.08500).
- en: This is important, because if we accept the IS baseline, then producing only
    one type of a category technically satisfies the category-being-generated-sometimes
    requirement. But, for example, if we are trying to create a cat-generation algorithm,
    this is not actually what we want (say, if we had multiple breeds of cats represented).
    Furthermore, we want the GAN to output samples that present a cat from more than
    one angle and, generally, images that are distinct.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这很重要，因为我们如果接受IS基线，那么只生成某一类的一个样本在技术上就满足了类别生成有时性的要求。但是，例如，如果我们试图创建一个猫生成算法，这实际上并不是我们想要的（比如说，如果我们有多个品种的猫）。此外，我们希望GAN能够输出从多个角度展示猫的样本，以及通常情况下独特的图像。
- en: We equally do not want the GAN to simply memorize the images. Luckily, that
    is much easier to detect—we can look at the distance between images in pixel space.
    [Figure 5.3](#ch05fig03) shows what that may look like. Technical implementation
    of the FID is again complex, but the high-level idea is that we are looking for
    a generated distribution of samples that minimizes the number of modifications
    we have to make to ensure that the generated distribution looks like the distribution
    of the true data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们同样不希望GAN简单地记忆图像。幸运的是，这要容易检测得多——我们可以查看图像在像素空间中的距离。[图5.3](#ch05fig03)显示了这可能看起来像什么。FID的技术实现再次复杂，但高级的想法是我们正在寻找一个生成的样本分布，该分布最小化了我们需要进行的修改数量，以确保生成的分布看起来像真实数据的分布。
- en: Figure 5.3\. The GAN picks up on the patterns by mostly memorizing the items,
    which also creates an undesirable outcome indicating that the GAN has not learned
    much useful information and will most likely not generalize. The proof is in the
    images. The first two rows are pairs of duplicate samples; the last row is the
    nearest neighbor of the middle row in the training set. Note that these examples
    are very low resolution as they appear in the paper, due to a low-resolution GAN
    setup.
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.3。GAN通过主要记忆项目来捕捉模式，这也产生了一个不希望看到的结果，表明GAN没有学习到多少有用的信息，并且很可能无法泛化。证据在图像中。前两行是重复样本的对；最后一行是训练集中中间行的最近邻。注意，这些例子在论文中由于低分辨率的GAN设置而具有非常低的分辨率。
- en: '![](../Images/05fig03_alt.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3](../Images/05fig03_alt.jpg)'
- en: '(Source: “Do GANs Actually Learn the Distribution? An Empirical Study,” by
    Sanjeev Arora and Yi Zhang, 2017, [https://arxiv.org/pdf/1706.08224v2.pdf](https://arxiv.org/pdf/1706.08224v2.pdf).)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: (来源：“GANs Actually Learn the Distribution? An Empirical Study”，作者Sanjeev Arora和Yi
    Zhang，2017年，[https://arxiv.org/pdf/1706.08224v2.pdf](https://arxiv.org/pdf/1706.08224v2.pdf).)
- en: The FID is calculated by running images through an Inception network. In practice,
    we compare the intermediate representations—feature maps or layers—rather than
    the final output (in other words, we *embed* them). More concretely, we evaluate
    the distance of the embedded means, the variances, and the covariances of the
    two distributions—the real and the generated one.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: FID是通过运行图像通过Inception网络来计算的。在实践中，我们比较中间表示——特征图或层——而不是最终输出（换句话说，我们*嵌入*它们）。更具体地说，我们评估两个分布——真实和生成的——嵌入均值、方差和协方差的距离。
- en: To abstract away from images, if we have a domain of well-understood classifiers,
    we can use their predictions as a measure of whether this particular sample looks
    realistic. To summarize, the FID is a way of abstracting away from a human evaluator
    and allows us to reason statistically, in terms of distributions, even about things
    as difficult to quantify as the realism of an image.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从图像中抽象出来，如果我们有一个对分类器有深入了解的领域，我们可以使用它们的预测作为衡量这个特定样本是否看起来逼真的标准。总结来说，FID是一种从人类评估者抽象出来的方法，它允许我们从分布的角度进行统计推理，甚至对于像图像的真实性这样难以量化的东西也是如此。
- en: Because this metric is so new, it is still worth waiting to see whether a flaw
    may be revealed in a later paper. But given the number of reputable authors who
    have already started using this metric, we decided to include it.^([[10](#ch05fn10)])
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这个指标如此新颖，所以仍然值得等待，看看是否会在后来的论文中揭示出其缺陷。但考虑到已经有众多信誉良好的作者开始使用这个指标，我们决定将其包括在内.^([[10](#ch05fn10)])
- en: ^(10)
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(10)
- en: ''
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Is Generator Conditioning Causally Related to GAN Performance?” by Augustus
    Odena et al., 2018, [http://arxiv.org/abs/1802.08768](http://arxiv.org/abs/1802.08768).
    See also S. Nowozin (Microsoft Research) talk at UCL, February 10, 2018.
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅Augustus Odena等人于2018年发表的“Is Generator Conditioning Causally Related to GAN
    Performance?”，[http://arxiv.org/abs/1802.08768](http://arxiv.org/abs/1802.08768)。另请参阅S.
    Nowozin（微软研究院）于2018年2月10日在UCL的演讲。
- en: 5.2\. Training challenges
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 训练挑战
- en: Training a GAN can be complicated, and we will walk you through the best practices.
    But here we provide only a high-level, accessible set of explanations that do
    not deep dive into any of the mathematics that proves the theorems or shows the
    evidence, because the details are beyond the scope of this book. But we encourage
    you to go to the sources and decide for yourself. Frequently, the authors even
    provide code samples to help you get started.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 训练GAN可能很复杂，我们将向您介绍最佳实践。但在这里，我们只提供一套高级、易于理解的解释，不深入任何证明定理或展示证据的数学，因为细节超出了本书的范围。但我们鼓励您查阅原始资料并自行决定。通常，作者甚至提供代码示例以帮助您开始。
- en: 'Here is a list of the main problems:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个主要问题的列表：
- en: '*Mode collapse*—In *mode collapse*, some of the modes (for example, classes)
    are not well represented in the generated samples. The mode collapses even though
    the real data distribution has support for the samples in this part of the distribution;
    for example, there will be no number 8 in the MNIST dataset. Note that mode collapse
    can happen even if the network has converged. We talked about *interclass* mode
    collapse during the explanation of the IS and *intraclass* mode collapse when
    discussing the FID.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模式坍塌*—在*模式坍塌*中，某些模式（例如，类别）在生成的样本中没有得到很好的表示。即使真实数据分布支持这部分分布中的样本，模式也会坍塌；例如，MNIST数据集中将不会有数字8。请注意，即使网络已经收敛，模式坍塌也可能发生。我们在解释IS时谈到了*类间*模式坍塌，在讨论FID时讨论了*类内*模式坍塌。'
- en: '*Slow convergence*—This is a big problem with GANs and unsupervised settings,
    in which generally the speed of convergence and available compute are the main
    constraints—unlike with supervised learning, in which available labeled data is
    typically the first barrier. Moreover, some people believe that compute, not data,
    is going to be the determining factor in the AI race in the future. Plus, everyone
    wants fast models that do not take days to train.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*收敛速度慢*—这是GAN和无监督设置中的一个重大问题，在这种设置中，通常收敛速度和可用计算是主要约束——与监督学习不同，在监督学习中，可用的标记数据通常是第一个障碍。此外，有些人认为计算，而不是数据，将是未来AI竞赛的决定性因素。而且，每个人都希望快速模型，不需要几天时间来训练。'
- en: '*Overgeneralization*—Here, we talk especially about cases in which modes (potential
    data samples) that should not have support (should not exist), do. For example,
    you might see a cow with multiple bodies but only one head, or vice versa. This
    happens when the GAN overgeneralizes and learns things that should not exist based
    on the real data.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*过度泛化*—在这里，我们特别讨论了模式（潜在数据样本）不应该有支持（不应该存在）的情况。例如，你可能会看到一个有多个身体但只有一个头的牛，或者相反。这种情况发生在GAN过度泛化，并基于真实数据学习不应该存在的事物时。'
- en: 'Note that mode collapse and overgeneralization can sometimes most naively be
    resolved by reinitializing the algorithm, but such an algorithm is fragile, which
    is bad. This list gives us, broadly, two key metrics: speed and quality. But even
    these two metrics are similar, as much of training is ultimately focused on closing
    the gap between the real and the generated distribution faster.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，模式坍塌和过度泛化有时可以通过重新初始化算法来最天真地解决，但这样的算法是脆弱的，这是不好的。这个列表大致给出了两个关键指标：速度和质量。但即使这两个指标相似，因为训练最终很大程度上集中在更快地缩小真实分布和生成分布之间的差距。
- en: 'So how do we resolve this? When it comes to GAN training, several techniques
    can help us improve the training process, just as you would with any other machine
    learning algorithm:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们该如何解决这个问题呢？在GAN训练方面，几种技术可以帮助我们改进训练过程，就像你使用任何其他机器学习算法一样：
- en: Adding network depth
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加网络深度
- en: Changing the game setup
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变游戏设置
- en: Min-Max design and stopping criteria that were proposed by the original paper
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原文提出的 Min-Max 设计及停止标准
- en: Non-Saturating design and stopping criteria that were proposed by the original
    paper^([[11](#ch05fn11)])
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原文提出的非饱和设计及停止标准^([[11](#ch05fn11)])
- en: ^(11)
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(11)
- en: ''
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Generative Adversarial Networks,” by Ian Goodfellow et al., 2014, [http://arxiv.org/abs/1406.2661](http://arxiv.org/abs/1406.2661).
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见 Ian Goodfellow 等人于 2014 年发表的“生成对抗网络”，[http://arxiv.org/abs/1406.2661](http://arxiv.org/abs/1406.2661)。
- en: Wasserstein GAN as a recent improvement
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水晶距离 GAN 作为最近的一项改进
- en: Number of training hacks with commentary
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有注释的训练技巧数量
- en: Normalizing the inputs
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化输入
- en: Penalizing the gradients
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对梯度进行惩罚
- en: Training the Discriminator more
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多地训练判别器
- en: Avoiding sparse gradients
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免稀疏梯度
- en: Changing to soft and noisy labels
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换为软标签和噪声标签
- en: 5.2.1\. Adding network depth
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1. 增加网络深度
- en: As with many machine learning algorithms, the easiest way to make learning more
    stable is to reduce the complexity. If you can start with a simple algorithm and
    iteratively add to it, you get more stability during training, faster convergence,
    and potentially other benefits. [Chapter 6](../Text/kindle_split_016.xhtml#ch06)
    explores this idea in more depth.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多机器学习算法一样，使学习更加稳定的最简单方法就是降低复杂性。如果你可以从一个简单的算法开始，并逐步添加，那么在训练过程中你会获得更多的稳定性，更快的收敛，以及可能的其他好处。[第
    6 章](../Text/kindle_split_016.xhtml#ch06)更深入地探讨了这一想法。
- en: You could quickly achieve stability with both a simple Generator and Discriminator
    and then add complexity as you train, as explained in one of the most mind-blowing
    GAN papers.^([[12](#ch05fn12)]) Here, the authors from NVIDIA progressively grow
    the two networks so that at the end of each training cycle, we double the output
    size of the Generator and double the input of the Discriminator. We start with
    two simple networks and train until we achieve good performance.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以快速通过一个简单的生成器和判别器实现稳定性，然后在训练过程中逐步增加复杂性，正如在一篇最令人震惊的 GAN 论文中所解释的那样.^([[12](#ch05fn12)])
    在这里，NVIDIA 的作者逐步增长两个网络，使得在每个训练周期结束时，生成器的输出大小加倍，判别器的输入也加倍。我们从一个简单的网络开始，并训练直到达到良好的性能。
- en: ^(12)
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(12)
- en: ''
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Progressive Growing of GANs for Improved Quality, Stability, and Variation,”
    by Tero Karras et al., 2017, [http://arxiv.org/abs/1710.10196](http://arxiv.org/abs/1710.10196).
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见 Tero Karras 等人于 2017 年发表的“用于提高质量、稳定性和变化的 GAN 的渐进式增长”，[http://arxiv.org/abs/1710.10196](http://arxiv.org/abs/1710.10196)。
- en: This ensures that rather than starting with a massive parameter space, which
    is orders of magnitude larger than the initial input size, we start by generating
    an image of 4 × 4 pixels and navigating this parameter space before doubling the
    size of the output. We repeat this until we reach images of size 1024 × 1024.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了，而不是从一个比初始输入大小大几个数量级的巨大参数空间开始，我们首先生成一个 4 × 4 像素的图像，并在输出大小加倍之前在这个参数空间中导航。我们重复这个过程，直到达到
    1024 × 1024 像素的图像。
- en: See how impressive this is for yourself; both the pictures in [figure 5.4](#ch05fig04)
    are generated. Now we are moving beyond the blurry 64 × 64 images that autoencoders
    can generate.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 亲自看看这是多么令人印象深刻；[图 5.4](#ch05fig04) 中的图片都是生成的。现在我们正在超越自动编码器可以生成的模糊 64 × 64 像素图像。
- en: Figure 5.4\. Full HD images generated by GANs. You may consider this a teaser
    for the next chapter, where you will be rewarded for all your hard work in this
    one.
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.4. GAN 生成的全高清图像。你可以把这看作是下一章的预告，在那里你将因在本章中所做的所有辛勤工作而得到回报。
- en: '![](../Images/05fig04_alt.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05fig04_alt.jpg)'
- en: '(Source: Karras et al., 2017, [https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196).)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：Karras 等人，2017 年，[https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)。）
- en: 'This approach has these advantages: stability, speed of training, and, most
    importantly, the quality of the samples produced as well as their scale. Although
    this paradigm is new, we expect more and more papers to use it. You should definitely
    experiment with it also, because it is a technique that can be applied to virtually
    any type of GAN.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法具有以下优点：稳定性、训练速度，以及最重要的是，生成的样本质量和规模。尽管这种范式是新的，但我们预计越来越多的论文会使用它。你绝对应该尝试一下，因为它是一种可以应用于几乎任何类型
    GAN 的技术。
- en: 5.2.2\. Game setups
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2. 游戏设置
- en: One way to think about the two-player competitive nature of GANs is to imagine
    that you are playing the game of Go or any other board game that can end at any
    point, including chess. (Indeed, this borrows from DeepMind’s approach to AlphaGo
    and its split into policy and value network.) As a player, you need to be able
    to not only know the game’s objective and therefore what both players are trying
    to accomplish, but also understand how close you are to victory. So you have *rules*
    and you have a *distance (victory) metric*—for example, the number of pawns lost.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 思考 GAN 的双玩家竞争性质的一种方法是可以想象你在玩围棋或其他任何可以随时结束的棋盘游戏，包括象棋。（实际上，这是借鉴了 DeepMind 对 AlphaGo
    的方法及其分为策略网络和价值网络。）作为一个玩家，你需要不仅知道游戏的目标以及因此两个玩家试图完成什么，还要了解你离胜利有多近。所以你有 *规则*，你有一个
    *距离（胜利）指标*——例如，失去的兵的数量。
- en: But just as not every board-game victory metric applies equally well to every
    game, some GAN victory metrics—distances or divergences—tend to be used with particular
    game setups and not with others. It is worth examining each loss function (victory
    metrics) and the player dynamics (game setup) separately.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 但正如不是每个棋盘游戏的胜利指标都同样适用于每个游戏一样，一些 GAN 胜利指标（距离或发散度）往往与特定的游戏设置一起使用，而不是与其他设置一起使用。值得单独检查每个损失函数（胜利指标）和玩家动态（游戏设置）。
- en: Here, we start to introduce some of the mathematical notation that describes
    the GAN problem. The equations are important, and we promise we won’t scare you
    with any more than necessary. The reason we introduce them is to give you a high-level
    understanding as well as equip you with the tools to understand what a lot of
    GAN researchers still do not seem to distinguish. (Maybe they should train the
    Discriminator in their head—oh, well.)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们开始介绍一些描述 GAN 问题的一些数学符号。方程式很重要，我们承诺不会用不必要的复杂度吓到你。我们引入它们的原因是，给你一个高层次的理解，并为你提供理解许多
    GAN 研究人员似乎还没有区分清楚的工具。（也许他们应该在心中训练判别器——哦，好吧。）
- en: 5.2.3\. Min-Max GAN
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. Min-Max GAN
- en: 'As we explained earlier in this book, you can think of the GAN setup from a
    game-theoretical point of view, where you have two players trying to outplay each
    other. But even the original 2014 paper mentioned that there are two versions
    of the game. In principle, the more understandable and the more theoretically
    well-grounded approach is exactly the one we described: just consider the GAN
    problem a *min-max* game. [Equation 5.1](#ch05equ01) describes the loss function
    for the Discriminator.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书前面解释的那样，你可以从博弈论的角度思考 GAN 设置，其中有两个玩家试图超越对方。但即使是原始的 2014 年论文也提到，这个游戏有两种版本。原则上，更易于理解且理论基础更扎实的正是我们所描述的方法：只需将
    GAN 问题视为一个 *min-max* 游戏。[方程式 5.1](#ch05equ01) 描述了判别器的损失函数。
- en: equation 5.1\.
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 5.1\.
- en: '![](../Images/05equ01.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05equ01.jpg)'
- en: 'The *E*s stand for expectation over either *x* (true data distribution) or
    *z* (latent space), *D* stands for the Discriminator’s function (mapping image
    to probability), and *G* stands for the Generator’s function (mapping latent vector
    to an image). This first equation should be familiar from any binary classification
    problem. If we give ourselves some freedom and get rid of the complexity, we can
    rewrite this equation as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*E* 代表对 *x*（真实数据分布）或 *z*（潜在空间）的期望，*D* 代表判别器的函数（将图像映射到概率），而 *G* 代表生成器的函数（将潜在向量映射到图像）。这个第一个方程式可以从任何二元分类问题中熟悉。如果我们给自己一些自由，并消除复杂性，我们可以将这个方程式重写如下：'
- en: '![](../Images/05unequ01.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05unequ01.jpg)'
- en: This states that the Discriminator is trying to minimize the likelihood of mistaking
    a real sample for a fake one (first part) or a fake sample for a real one (the
    second part).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明判别器正在尝试最小化将真实样本误判为伪造样本（第一部分）或将伪造样本误判为真实样本（第二部分）的可能性。
- en: Now let’s turn our attention to the Generator’s loss function in [equation 5.2](#ch05equ02).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将注意力转向 [方程式 5.2](#ch05equ02) 中的生成器的损失函数。
- en: equation 5.2\.
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 5.2\.
- en: '![](../Images/05equ02.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05equ02.jpg)'
- en: Because we have only two agents and they are competing against each other, it
    makes sense that the Generator’s loss would be a negative of the Discriminator’s.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们只有两个智能体，并且它们正在相互竞争，所以生成器的损失是判别器损失的负值是有意义的。
- en: 'Putting it all together: we have two loss functions, and one is the negative
    value of the other. The adversarial nature is clear. The Generator is trying to
    outsmart the Discriminator. As for the Discriminator, remember that it is a binary
    classifier. The Discriminator also outputs only a single number—not the binary
    class—so it’s punished for its confidence or lack thereof. The rest is just some
    fancy math to give us nice properties such as asymptotic consistency to the Jensen–Shannon
    divergence (which is a great phrase to memorize if you’re trying to curse someone).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起：我们有两个损失函数，其中一个函数是另一个函数的负值。对抗性性质是明显的。生成器试图智胜判别器。至于判别器，记住它是一个二元分类器。判别器也只输出一个数字——而不是二元类别——因此它因自信或缺乏自信而受到惩罚。其余的只是一些复杂的数学，给我们带来了一些美好的性质，例如渐近一致性到
    Jensen-Shannon 散度（如果你试图诅咒某人，这是一个很好的短语来记忆）。
- en: We previously explained why we typically don’t use maximum likelihood. Instead,
    we use measures such as the KL divergence and the Jensen–Shannon divergence (JSD)
    and, more recently the earth mover’s distance, also known as Wasserstein distance.
    But all these divergences help us understand the difference between the real and
    the generated distribution. For now, just think of the JSD as a symmetric version
    of the KL divergence, which we introduced in [chapter 2](../Text/kindle_split_011.xhtml#ch02).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前解释了为什么我们通常不使用最大似然。相反，我们使用诸如 KL 散度、Jensen-Shannon 散度 (JSD) 以及最近的地移距离（也称为
    Wasserstein 距离）等度量。但所有这些散度都有助于我们理解真实分布和生成分布之间的差异。现在，只需将 JSD 视为我们在第 2 章中介绍的 KL
    散度的对称版本即可。
- en: '|  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Definition
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 定义
- en: '*Jensen-Shannon divergence (JSD)* is a symmetric version of KL divergence.
    Whereas *KL*(*p,q*)! = *KL*(*q,p*), it is the case that *JSD*(*p,q*) == *JSD*(*q,p*).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*Jensen-Shannon 散度 (JSD)* 是 KL 散度的对称版本。而 *KL*(*p,q*)! = *KL*(*q,p*)，但 *JSD*(*p,q*)
    == *JSD*(*q,p*)。'
- en: '|  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: For those of you who want more detail, KL divergence, as well as JSD, are generally
    regarded as what GANs are ultimately trying to minimize. These are both types
    of distance metrics that help us understand how different the two distributions
    are in a high-dimensional space. Some neat proofs connect those divergences and
    the min-max version of the GAN; however, these concerns are too academic for this
    book. If this paragraph makes little sense, you’re not having a stroke; don’t
    worry. It’s just statistician things.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些想要更多细节的人来说，KL 散度以及 JSD 通常被认为是 GANs 最终试图最小化的目标。这些都是距离度量类型，帮助我们理解在多维空间中两个分布之间的差异。一些巧妙的证明将那些散度与
    GAN 的 min-max 版本联系起来；然而，这些关注点对于这本书来说过于学术化。如果这段话让你感到困惑，你并没有中风；不要担心。这只是统计学家的东西。
- en: 'We typically do not use the Min-Max GAN (MM-GAN) beyond the nice theoretical
    guarantees it gives us. It serves as a neat theoretical framework to understand
    GANs: both as a game-theoretical concept—stemming from the competitive nature
    between the two networks/players—as well as an information-theoretical one. Beyond
    that, there are ordinarily no advantages to the MM-GAN. Typically, only the next
    two setups are used.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常不会使用 Min-Max GAN (MM-GAN) 之外的任何内容，因为它给我们提供了很好的理论保证。它作为一个整洁的理论框架来理解 GANs：既是一个博弈论概念——源于两个网络/玩家之间的竞争性本质——也是一个信息论概念。除此之外，MM-GAN
    通常没有其他优势。通常，只有下述两种设置被使用。
- en: 5.2.4\. Non-Saturating GAN
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4. 非饱和 GAN
- en: 'In practice, it frequently turns out that the min-max approach creates more
    problems, such as slow convergence for the Discriminator. The original GAN paper
    proposes an alternative formulation: *Non-Saturating GAN* (NS-GAN). In this version
    of the problem, rather than trying to put the two loss functions as direct competitors
    of each other, we make the two loss functions independent, as shown in [equation
    5.3](#ch05equ03), but directionally consistent with the original formulation ([equation
    5.2](#ch05equ02)).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，经常发现 min-max 方法会带来更多问题，例如判别器的收敛速度慢。原始的 GAN 论文提出了一种替代方案：*非饱和 GAN* (NS-GAN)。在这个问题版本中，我们不是试图将两个损失函数作为直接的竞争对手，而是使两个损失函数相互独立，如
    [方程 5.3](#ch05equ03) 所示，但与原始公式 ([方程 5.2](#ch05equ02)) 方向上一致。
- en: 'Again, let’s focus on a general understanding: the two loss functions are no
    longer set directly against each other. But in [equation 5.3](#ch05equ03), you
    can see that the Generator is trying to minimize the opposite of the second term
    of the Discriminator in [equation 5.4](#ch05equ04). Basically, it is trying not
    to get caught for the samples that it generates.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，让我们关注一般理解：两个损失函数不再直接相互设置。但在[方程式5.3](#ch05equ03)中，你可以看到生成器正在尝试最小化[方程式5.4](#ch05equ04)中判别器的第二项的相反数。基本上，它试图不让其生成的样本被发现。
- en: equation 5.3\.
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式5.3。
- en: '![](../Images/05equ03.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/05equ03.jpg)'
- en: equation 5.4\.
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式5.4。
- en: '![](../Images/05equ04.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/05equ04.jpg)'
- en: The intuition for the Discriminator is the exact same as it was before—[equation
    5.1](#ch05equ01) and [equation 5.4](#ch05equ04) are identical, but the equivalent
    of [equation 5.2](#ch05equ02) has now changed. The main reason for the NS-GAN
    is that in the MM-GAN’s case, the gradients can easily *saturate*—get close to
    0, which leads to slow convergence, because the weight updates that are backpropagated
    are either 0 or tiny. Perhaps a picture would make this clearer; see [figure 5.5](#ch05fig05).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于判别器的直觉与之前完全相同—[方程式5.1](#ch05equ01)和[方程式5.4](#ch05equ04)是相同的，但[方程式5.2](#ch05equ02)的等效物现在已改变。NS-GAN的主要原因是，在MM-GAN的情况下，梯度可以轻易地**饱和**—接近0，这会导致收敛速度慢，因为反向传播的权重更新要么是0要么非常小。或许一张图能更清楚地说明这一点；参见[图5.5](#ch05fig05)。
- en: Figure 5.5\. A sketch of what the hypothesized relationships are meant to look
    like in theory. The y-axis is the loss function for the Generator, whereas D(G(z))
    is the Discriminator’s “guess” for the likelihood of the generated sample. You
    can see that Minimax (MM) stays flat for too long, thereby giving the Generator
    too little information—the gradients vanish.
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.5\. 理论上所假设的关系应该是什么样的草图。y轴是生成器的损失函数，而D(G(z))是判别器对生成样本似然性的“猜测”。你可以看到，Minimax
    (MM)保持平坦的时间太长，因此给生成器提供的信息太少——梯度消失。
- en: '![](../Images/05fig05_alt.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/05fig05_alt.jpg)'
- en: '(Source: “Understanding Generative Adversarial Networks,” by Daniel Seita,
    2017, [http://mng.bz/QQAj](http://mng.bz/QQAj).)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：“理解生成对抗网络”，作者：Daniel Seita，2017，[http://mng.bz/QQAj](http://mng.bz/QQAj)）
- en: You can see that around 0.0, the gradient of both maximum likelihood and MM-GAN
    is close to 0, which is where a lot of early training happens, whereas the NS-GAN
    has a lot higher gradient there, so training should happen much more quickly at
    the start.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，在0.0附近，最大似然和MM-GAN的梯度都接近0，这是早期训练发生的地方，而NS-GAN的梯度在那里要高得多，因此训练应该从一开始就更快。
- en: We don’t have a good theoretical understanding of why the NS variant should
    converge to the Nash equilibrium. In fact, because the NS-GAN is heuristically
    motivated, using this form no longer gives us any of the neat mathematical guarantees
    we used to get; see [figure 5.6](#ch05fig06). Because of the complexity of the
    GAN problem, however, even in the NS-GAN’s case, there is a chance that the training
    might not converge at all, although it has been empirically shown to perform better
    than the MM-GAN.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对为什么NS变体应该收敛到纳什均衡没有好的理论理解。事实上，由于NS-GAN是启发式驱动的，使用这种形式不再给我们带来我们曾经得到的任何整洁的数学保证；参见[图5.6](#ch05fig06)。然而，由于GAN问题的复杂性，即使在NS-GAN的情况下，训练可能根本不会收敛，尽管经验上已经证明它比MM-GAN表现更好。
- en: Figure 5.6\. A moment of silence, please.
  id: totrans-167
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.6\. 请保持沉默。
- en: '![](../Images/05fig06.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/05fig06.jpg)'
- en: But our dreadful sacrifice leads to significant improvement in performance.
    The neat thing about the NS approach is not only that the initial training is
    faster, but also, because the Generator learns faster, the Discriminator learns
    faster too. This is desirable, because (almost) all of us are on a tight computational
    and time budget, and the faster we can learn, the better. Some argue that the
    NS-GAN has not yet been surpassed on a fixed computational budget, and even Wasserstein
    GAN is not conclusively a better architecture.^([[13](#ch05fn13)])
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们可怕的牺牲带来了性能的显著提升。NS方法的好处不仅在于初始训练更快，而且由于生成器学习得更快，判别器也学习得更快。这是我们所希望的，因为（几乎）我们所有人都面临着紧张的计算和时间预算，我们学得越快越好。有些人认为，在固定的计算预算下，NS-GAN尚未被超越，甚至Wasserstein
    GAN也不是一个更好的架构。[13](#ch05fn13)
- en: ^(13)
  id: totrans-170
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([13](#ch05fn13))
- en: ''
  id: totrans-171
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Are GANs Created Equal? A Large-Scale Study,” by Mario Lucic et al., 2017,
    [http://arxiv.org/abs/1711.10337](http://arxiv.org/abs/1711.10337).
  id: totrans-172
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅Mario Lucic等人于2017年发表的“Are GANs Created Equal? A Large-Scale Study”，[http://arxiv.org/abs/1711.10337](http://arxiv.org/abs/1711.10337)。
- en: 5.2.5\. When to stop training
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.5\. 何时停止训练
- en: Strictly speaking, the NS-GAN
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，NS-GAN
- en: Is no longer asymptotically consistent with the JSD
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不再与JSD渐近一致
- en: Has an equilibrium state that theoretically is even more elusive
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有理论上甚至更难以捉摸的平衡状态
- en: The first point is important, because the JSD is a meaningful tool in explaining
    why an implicitly generated distribution should even converge at all to the real
    data distribution. In principle, this gives us stopping criteria; but in practice,
    this is almost pointless, because we can never verify when the true distribution
    and the generated distribution have converged. People typically decide when to
    stop by looking at the generated samples every couple of iterations. More recently,
    some people have started looking at defining stopping criteria by FID, IS, or
    the less popular sliced Wasserstein distance.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个观点很重要，因为JSD是一个有意义的工具，可以解释为什么隐式生成的分布甚至应该收敛到真实数据分布。原则上，这为我们提供了停止标准；但在实践中，这几乎毫无意义，因为我们永远无法验证真实分布和生成分布何时收敛。人们通常每隔几轮迭代就查看生成的样本来决定何时停止。最近，一些人开始通过FID、IS或不太流行的切片Wasserstein距离来定义停止标准。
- en: The second point is also important because the instability obviously causes
    training problems. One of the more important questions is knowing when to stop.
    In the two original formulations of the GAN problem, we are never given a clear
    set of conditions under which the training has finished in practice. In principle,
    we are always told that once we reach Nash equilibrium, the training is done,
    but in practice this is again hard to verify, because the high dimensionality
    makes equilibrium difficult to prove.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个观点也很重要，因为不稳定性显然会导致训练问题。一个更重要的问题是知道何时停止。在GAN问题的两个原始公式中，我们从未得到一个明确的条件集，在实际情况中，训练何时完成。原则上，我们总是被告知一旦达到纳什均衡，训练就完成了，但在实践中这又很难验证，因为高维性使得均衡难以证明。
- en: If you want to plot the loss functions of the Generator and the Discriminator,
    they would typically jump all over the place. This makes sense because they’re
    competing against each other, so if one gets better, the other one gets a larger
    loss. Just by looking at the two loss functions, it is unclear when we’ve actually
    finished training.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想绘制生成器和判别器的损失函数，它们通常会四处跳跃。这是有道理的，因为它们是在相互竞争，所以如果一个变得更好，另一个就会得到更大的损失。仅仅通过查看两个损失函数，我们就不清楚何时实际上已经完成了训练。
- en: In the NS-GAN’s defense, it should be said that it is still much faster than
    the Wasserstein GAN. As a result, the NS-GAN may get over these limitations by
    being able to run more quickly.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在NS-GAN的辩护中，应该指出，它仍然比Wasserstein GAN快得多。因此，NS-GAN可能通过能够更快地运行来克服这些限制。
- en: 5.2.6\. Wasserstein GAN
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.6\. Wasserstein GAN
- en: 'Recently, a new development in GAN training has emerged and quickly reached
    academic popularity: *Wasserstein GAN (WGAN)*.^([[14](#ch05fn14)]) It is now mentioned
    by virtually every major academic paper and many practitioners. Ultimately, the
    WGAN is important for three reasons:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，GAN训练中出现了一种新的发展，并迅速在学术界获得认可：*Wasserstein GAN (WGAN)*.^([[14](#ch05fn14)])
    它现在几乎被每篇重要的学术论文和许多从业者所提及。最终，WGAN之所以重要，有三个原因：
- en: ^(14)
  id: totrans-183
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(14)
- en: ''
  id: totrans-184
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Wasserstein GAN,” by Martin Arjovsky et al., 2017, [https://arxiv.org/pdf/1701.07875.pdf](https://arxiv.org/pdf/1701.07875.pdf).
  id: totrans-185
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅Martin Arjovsky等人于2017年发表的“Wasserstein GAN”，[https://arxiv.org/pdf/1701.07875.pdf](https://arxiv.org/pdf/1701.07875.pdf)。
- en: It significantly improves on the loss functions, which are now interpretable
    and provide clearer stopping criteria.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它显著改进了损失函数，现在它们是可解释的，并提供了更清晰的停止标准。
- en: Empirically, the WGAN tends to have better results.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经验上，WGAN往往有更好的结果。
- en: Unlike a lot of research into GANs, it has clear theoretical backing that starts
    from the loss and shows how the KL divergence that we are trying to approximate
    is ultimately not well justified theoretically or practically. Based on this theory,
    it then proposes a better loss function that mitigates this problem.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与许多关于GAN的研究不同，它有明确的理论支持，从损失函数开始，展示了我们试图逼近的KL散度在理论上或实际上最终并不充分。基于这个理论，它随后提出了一种更好的损失函数，以减轻这个问题。
- en: The importance of the first point should be fairly obvious from the previous
    section. Given the competitive nature between Generator and Discriminator, we
    don’t have a clear point at which we want to stop training. The WGAN uses the
    earth mover’s distance as a loss function that clearly correlates with the visual
    quality of the samples generated. The benefits of the second and third points
    are somewhat obvious—we want to have higher-quality samples and better theoretical
    grounding.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一节中，第一点的意义应该是相当明显的。鉴于生成器和判别器之间的竞争性，我们没有一个明确的停止训练的点。WGAN 使用地球迁移距离作为损失函数，它与生成的样本的视觉质量有明显的相关性。第二点和第三点的益处是相当明显的——我们希望有更高质量的样本和更好的理论基础。
- en: How is this magic achieved? Let’s look at the Wasserstein loss for the Discriminator—or
    the *critic*, as the WGAN calls it—in more detail. Take a look at [equation 5.5](#ch05equ05).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这种魔法是如何实现的？让我们更详细地看看判别器——或者说 WGAN 所称的评论家——的水晶损失。看看 [方程式 5.5](#ch05equ05)。
- en: equation 5.5\.
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 5.5\.
- en: '![](../Images/05equ05.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05equ05.jpg)'
- en: This equation is somewhat similar to what you have seen before (as a high-level
    simplification of [equation 5.1](#ch05equ01)), with some important differences.
    We now have the function *f[w]*, which acts as a Discriminator. The critic is
    trying to estimate the earth mover’s distance, and looks for the *maximum* difference
    between the real (first term) and the generated (second term) distribution under
    different (valid) parametrizations of the *f[w]* function. And we are now simply
    measuring the difference. The critic is trying to make the Generator’s life the
    hardest it could be by looking at different projections using *f[w]* into shared
    space in order to maximize the amount of probability mass it has to move.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程式与您之前见过的内容有些相似（作为一个高级简化版的 [方程式 5.1](#ch05equ01)），但有一些重要的区别。我们现在有函数 *f[w]*，它充当判别器。评论家试图估计地球迁移距离，并寻找在
    *f[w]* 函数的不同（有效）参数化下，真实（第一项）和生成（第二项）分布之间的最大差异。我们现在只是测量差异。评论家试图通过查看使用 *f[w]* 到共享空间的不同投影来使发生器的日子尽可能难过，以最大化它必须移动的概率质量。
- en: '[Equation 5.6](#ch05equ06) shows the Generator, as it now has to include the
    earth mover’s distance.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程式 5.6](#ch05equ06) 展示了发生器，因为它现在必须包括地球迁移距离。'
- en: equation 5.6\.
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 5.6\.
- en: '![](../Images/05equ06.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05equ06.jpg)'
- en: On a high level, in this equation we are trying to *minimize* the distance between
    the expectation of the real distribution and the expectation of the generated
    distribution. The paper that introduced the WGAN itself is complex, but the gist
    is that *f[w]* is a function satisfying a technical constraint.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程式中，从高层次来看，我们试图最小化真实分布的期望值与生成分布的期望值之间的距离。引入 WGAN 的论文本身很复杂，但要点是 *f[w]* 是一个满足技术约束的函数。
- en: '|  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: 'The technical constraint that *f[w]* satisfies is 1 – Lipschitz: for all *x*1,
    *x*2: | *f*(*x*1) – *f*(*x*2) | ≤ | *x*1 – *x*2 |.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 技术约束 *f[w]* 满足的是 1 – Lipschitz：对于所有 *x*1, *x*2：| *f*(*x*1) – *f*(*x*2) | ≤ |
    *x*1 – *x*2 |。
- en: '|  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'The problem that the Generator is trying to solve is similar to the one before,
    but let’s go into more detail anyway:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 发生器试图解决的问题与之前的问题类似，但让我们更详细地探讨一下：
- en: We draw *x* from either the real distribution (*x* ~ *P[r]*) or the generated
    distribution *x** (*g*[θ](z), where *z* ~ *p*(*z*)).
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从真实分布 (*x* ~ *P[r]*) 或生成的分布 *x** (*g*[θ](z)，其中 *z* ~ *p*(*z*)) 中抽取 *x*。
- en: The generated samples are sampled from *z* (the latent space) and then transformed
    via *g*[θ] to get the samples (*x**) in the same space and then evaluated using
    *f[w]*.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成的样本是从 *z*（潜在空间）中抽取的，然后通过 *g*[θ] 转换得到相同空间中的样本 (*x**)，然后使用 *f[w]* 进行评估。
- en: We are trying to minimize our loss function—or distance function, in this case—the
    earth mover’s distance. The actual numbers are calculated using the earth mover’s
    distance, which we will explain later.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们试图最小化我们的损失函数——在这种情况下是距离函数——地球迁移距离。实际的数字是通过地球迁移距离计算的，我们将在后面解释。
- en: 'The setup is also great because we have a much more understandable loss (for
    example, no logarithms). We also have more tunable training, because in WGAN settings,
    we have to set a clipping constant, which acts a lot like a learning rate in standard
    machine learning. This gives us an extra parameter to tune, but that can be a
    double-edged sword, if your GAN architecture ends up being very sensitive to it.
    But without going into the mathematics too much, the WGAN has two practical implications:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 该设置也很棒，因为我们有一个更易于理解的损失函数（例如，没有对数）。我们还有更多可调的训练，因为在WGAN设置中，我们必须设置一个裁剪常数，它在标准机器学习中的作用类似于学习率。这为我们提供了一个额外的可调参数，但这也可能是一把双刃剑，如果你的GAN架构最终对它非常敏感。但不过多涉及数学，WGAN有两个实际的影响：
- en: We now have clearer stopping criteria because this GAN has been validated by
    later papers that show a correlation between the Discriminator loss and the perceptual
    quality. We can simply measure the Wasserstein distance, and that helps inform
    when to stop.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们现在有更清晰的停止标准，因为这篇论文已经通过后来的论文得到了验证，这些论文显示了判别器损失与感知质量之间的相关性。我们可以简单地测量Wasserstein距离，这有助于我们决定何时停止。
- en: We can now train the WGAN to convergence. This is relevant because meta-review
    papers^([[15](#ch05fn15)]) showed that using the JS loss and the divergence between
    the Generator in the real distribution as a measure of training progress can often
    be meaningless.^([[16](#ch05fn16)]) To translate that into human terms, sometimes
    in chess, you need to lose a couple of rounds and therefore temporarily do worse
    in order to learn in a couple of iterations and ultimately do better.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们现在可以训练WGAN直到收敛。这很重要，因为元审查论文^([[15](#ch05fn15)])表明，使用JS损失和生成器在真实分布中的发散性作为训练进度的衡量标准往往是没有意义的.^([[16](#ch05fn16)])用人类的语言来说，有时在棋类游戏中，你需要输掉几轮，因此暂时做得更差，以便在几轮迭代中学习，并最终做得更好。
- en: ^(15)
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(15)
- en: ''
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: A *meta-review* is just a review of reviews. It helps researchers pool findings
    from across several papers.
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*元审查*只是对评论的评论。它帮助研究人员从多篇文章中汇总发现。'
- en: ^(16)
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(16)
- en: ''
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See “Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence at
    Every Step,” by William Fedus et al., 2018, [https://openreview.net/forum?id=ByQpn1ZA](https://openreview.net/forum?id=ByQpn1ZA).'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '请参阅威廉·费杜斯等人于2018年发表的论文“Many Paths to Equilibrium: GANs Do Not Need to Decrease
    a Divergence at Every Step”，[https://openreview.net/forum?id=ByQpn1ZA](https://openreview.net/forum?id=ByQpn1ZA)。'
- en: This may sound like magic. But this is partially because the WGAN is using a
    different distance metric than anything you’ve encountered so far. It is called
    the *earth mover’s distance*, or *Wasserstein distance*, and the idea behind it
    is clever. We will be nice for once and not torture you with more math, but let’s
    talk about this idea.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能听起来像魔法。但这部分是因为WGAN使用了一种与迄今为止你遇到的所有东西都不同的距离度量。它被称为*地球迁移距离*或*Wasserstein距离*，其背后的想法很巧妙。我们这次会善待你，不会用更多的数学来折磨你，但让我们谈谈这个想法。
- en: 'You implicitly understand that there are two distributions that are both very
    high dimensional: the real data-producing one (that we never fully see) and the
    samples from the Generator (the fake one). Think about how vast the sample space
    for even a 32 × 32 RGB (*x3* × 256 pixel values) image is. Now imagine all of
    this probability mass for both of these distributions as being just two sets of
    hills. [Chapter 10](../Text/kindle_split_021.xhtml#ch10) revisits this in more
    detail. For reference, we include [figure 5.7](#ch05fig07), but it builds largely
    on the same ideas as [chapter 2](../Text/kindle_split_011.xhtml#ch02).'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 你隐含地理解有两个非常高维的分布：一个是真实数据生成的一个（我们从未完全看到），另一个是生成器（伪造的）的样本。想想即使是32 × 32 RGB (*x3*
    × 256像素值)图像的样本空间有多么庞大。现在想象这两个分布的所有概率质量都只是两座山。[第10章](../Text/kindle_split_021.xhtml#ch10)更详细地回顾了这一点。为了参考，我们包括了[图5.7](#ch05fig07)，但它主要基于[第2章](../Text/kindle_split_011.xhtml#ch02)中的相同思想。
- en: Figure 5.7\. Plot (a) should be familiar from [chapter 2](../Text/kindle_split_011.xhtml#ch02).
    For extra clarity, we provide another view of a Gaussian distribution in plot
    (b) of the data drawn from the same distribution, but showing vertical slices
    of just the first distribution on the top and just the second distribution on
    the right. Plot (a) then is a probability density abstraction of this data, where
    the z-axis represents the probability of that point being sampled. Now, even though
    one of these is just an abstraction of the other, how would you compare the two?
    How would you make sure that they are the same even when we told you? What if
    this distribution had 3,072 possible dimensions? In this example, we have just
    two! We are building up to how we’d compare two heaps-of-sand-looking distributions
    as in (b), but remember that as our distributions get more complicated, properly
    matching like for like also gets harder.
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.7. 图(a)应该来自[第2章](../Text/kindle_split_011.xhtml#ch02)。为了增加清晰度，我们在数据图(b)中提供了高斯分布的另一种视图，这些数据来自相同的分布，但只显示了顶部第一个分布的垂直切片和右侧第二个分布的垂直切片。图(a)然后是这些数据的概率密度抽象，其中z轴表示该点被采样的概率。现在，尽管其中一个是另一个的抽象，你将如何比较这两个？你将如何确保它们在我们告诉你时是一样的时候？如果这个分布有3,072个可能的维度呢？在这个例子中，我们只有两个！我们正在构建如何比较看起来像(b)中的沙堆分布的方法，但记住，随着我们的分布变得更加复杂，正确匹配同类事物也变得更加困难。
- en: '![](../Images/05fig07_alt.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/05fig07_alt.jpg)'
- en: Imagine having to move all the ground that represents probability mass from
    the fake distribution so that the distribution looks exactly like the real distribution,
    or at least what we have seen of it. That would be like your neighbor having a
    super cool sandcastle, and you having a lot of sand and trying to make the exact
    same sandcastle. How much work would that take, to move all of that mass into
    just the right places? Hey, it’s okay, we’ve all been there; sometimes you just
    wish your sandcastle was a bit cooler and more sparkly.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你需要把代表概率质量的全部地面从虚假分布中移除，使得分布看起来完全像真实分布，或者至少像我们所看到的那样。这就像你的邻居有一个超级酷的沙堡，而你有很多沙子，试图建造一个完全相同的沙堡。要移动所有这些质量到恰到好处的地方需要多少工作量？嘿，没关系，我们都有过这样的经历；有时候你只是希望你的沙堡能更酷一些，更有光泽一些。
- en: Using an approximate version of the Wasserstein distance, we can evaluate how
    close we are to generating samples that look like they came from the real distribution.
    Why *approximate*? Well, for one because we never see the real data distribution,
    so it’s difficult to evaluate the exact earth mover’s distance.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Wasserstein距离的近似版本，我们可以评估我们生成看起来像来自真实分布的样本有多接近。为什么是*近似*？好吧，一方面是因为我们从未见过真实的数据分布，所以很难评估确切的地球移动距离。
- en: In the end, all you need to know is that the earth mover’s distance has nicer
    properties than either the JS or KL, and there are already important contributions
    building on the WGAN as well as validating its generally superior performance.^([[17](#ch05fn17)])
    Although in some cases the WGAN does not completely outperform all the others,
    it is generally at least as good in every case (though it should be noted that
    some may disagree with this interpretation).^([[18](#ch05fn18)])
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你需要知道的是，地球移动距离比JS或KL具有更好的性质，并且已经有重要的贡献建立在WGAN之上，以及验证了其通常优越的性能。[^([17](#ch05fn17))]
    尽管在某些情况下，WGAN并不完全优于其他所有方法，但它通常在每种情况下至少和它们一样好（尽管应该注意的是，有些人可能不同意这种解释）。[^([18](#ch05fn18))]
- en: ^(17)
  id: totrans-222
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([17](#ch05fn17))
- en: ''
  id: totrans-223
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Improved Training of Wasserstein GANs,” by Ishaan Gulrajani et al., 2017,
    [http://arxiv.org/abs/1704.00028](http://arxiv.org/abs/1704.00028).
  id: totrans-224
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Ishaan Gulrajani等人于2017年发表的“Improved Training of Wasserstein GANs”，[http://arxiv.org/abs/1704.00028](http://arxiv.org/abs/1704.00028)。
- en: ^(18)
  id: totrans-225
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([18](#ch05fn18))
- en: ''
  id: totrans-226
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Lucic et al., 2017, [http://arxiv.org/abs/1711.10337](http://arxiv.org/abs/1711.10337).
  id: totrans-227
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Lucic等人于2017年发表的论文，[http://arxiv.org/abs/1711.10337](http://arxiv.org/abs/1711.10337)。
- en: Overall, the WGAN (or the gradient penalty version, *WGAN-GP*) is widely used
    and has become the de facto standard in much of GAN research and practice—though
    the NS-GAN should not be forgotten anytime soon. When you see a new paper that
    does not have the WGAN as one of the benchmarks being compared and does not have
    a good justification for not including it—be careful!
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，WGAN（或者梯度惩罚版本，*WGAN-GP*）被广泛使用，并已成为GAN研究与实践中的事实标准——尽管NS-GAN在不久的将来也不应该被忘记。当你看到一篇新论文，其中没有将WGAN作为比较的基准之一，并且没有很好地解释为什么不包括它时——要小心！
- en: 5.3\. Summary of game setups
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3. 游戏设置总结
- en: 'We have presented the three core versions of the GAN setup: min-max, non-saturating,
    and Wasserstein. One of these versions will be mentioned at the beginning of every
    paper, and now you’ll have at least an idea of whether the paper is using the
    original formulation, which is more explainable but doesn’t work as well in practice;
    or the non-saturating version, which loses a lot of the mathematical guarantees
    but works much better; or the newer Wasserstein version, which has both theoretical
    grounding and largely superior performance.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了 GAN 设置的三个核心版本：min-max、非饱和和水波斯坦。每个论文的开头都会提到这些版本之一，现在您至少会知道该论文是否使用原始公式，这种公式更易于解释但在实践中效果不佳；或者非饱和版本，它失去了许多数学保证但效果更好；或者较新的水波斯坦版本，它既有理论基础又具有优越的性能。
- en: As a handy guide, [table 5.1](#ch05table01) presents a list of the NS-GAN, WGAN,
    and even the improved WGAN-GP formulations we use in this book. This is here so
    that you have the relevant versions in one place—sorry, MM-GAN. We have included
    the WGAN-GP here for completeness, because these three are the academic and industry
    go-tos.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一份便捷指南，[表 5.1](#ch05table01) 列出了我们在本书中使用的 NS-GAN、WGAN 以及甚至改进的 WGAN-GP 公式。这里列出是为了让您有一个相关版本的地方——抱歉，MM-GAN。我们包括
    WGAN-GP 是为了完整性，因为这三个是学术界和工业界的首选。
- en: Table 5.1\. Summary of loss functions^([[a](#ch05tn01)])
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 5.1\. 损失函数总结^([[a](#ch05tn01)])
- en: ^a
  id: totrans-233
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^a
- en: ''
  id: totrans-234
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Source: “Collection of Generative Models in TensorFlow,” by Hwalsuk Lee, [http://mng.bz/Xgv6](http://mng.bz/Xgv6).'
  id: totrans-235
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：“TensorFlow 中生成模型的集合”，作者 Hwalsuk Lee，[http://mng.bz/Xgv6](http://mng.bz/Xgv6)。
- en: '| Name | Value function | Notes |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 值函数 | 备注 |'
- en: '| --- | --- | --- |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| NS-GAN | L*[D]^(NS)* = *E*[log(*D*(*x*))] + *E*[log(1 – *D*(*G*(*z*)))] *L[G]^(NS)*
    = *E*[log(*D*(*G*(*z*)))] | This is one of the original formulations. Typically
    not used in practice anymore, except as a foundational block or comparison. This
    is an equivalent formulation to the NS-GAN you have seen, just without the constants.
    But these are effectively equivalent.^([[b](#ch05tn02)]) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| NS-GAN | L*[D]^(NS)* = *E*[log(*D*(*x*))] + *E*[log(1 – *D*(*G*(*z*)))] *L[G]^(NS)*
    = *E*[log(*D*(*G*(*z*)))] | 这是一种原始的公式之一。通常不再实际使用，除了作为基础块或比较。这与您所看到的 NS-GAN 等效，只是没有常数。但这些都是有效等价的。[^([[b](#ch05tn02)])]
    |'
- en: '| WGAN | *L[D]^(WGAN)* = *E*[*D*(*x*)] – *E*[*D*(*G*(*z*))] *L[G]^(WGAN)* =
    *E*[*D*(*G*(*z*))] | This is the WGAN with somewhat simplified loss. This seems
    to be creating a new paradigm for GANs. We explained this equation previously
    as [equation 5.5](#ch05equ05) in greater detail. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| WGAN | *L[D]^(WGAN)* = *E*[*D*(*x*)] – *E*[*D*(*G*(*z*)))] *L[G]^(WGAN)*
    = *E*[*D*(*G*(*z*)))] | 这是一种损失简化后的 WGAN。这似乎正在为 GAN 创造一个新的范式。我们之前在 [方程 5.5](#ch05equ05)
    中更详细地解释了此方程。|'
- en: '| WGAN-GP^([[c](#ch05tn03)]) (gradient penalties) | *L[D]^(W – GP)* = *E*[*D*(*x*)]
    – *E*[*D(G*(*z*))] + GPterm *L[G]^(W – GP)* = *E*[*D(G*(*z*))] | This is an example
    of a GAN with a gradient penalty (GP). WGAN-GP typically shows the best results.
    We have not discussed the WGAN-GP in this chapter in great detail; we include
    it here for completeness. |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| WGAN-GP^([[c](#ch05tn03)]) (梯度惩罚) | *L[D]^(W – GP)* = *E*[*D*(*x*)] – *E*[*D(G*(*z*)))]
    + GPterm *L[G]^(W – GP)* = *E*[*D(G*(*z*)))] | 这是一种带有梯度惩罚 (GP) 的 GAN 示例。WGAN-GP
    通常显示出最佳结果。我们没有在本章中详细讨论 WGAN-GP；我们将其包括在这里是为了完整性。|'
- en: ^b
  id: totrans-241
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^b
- en: ''
  id: totrans-242
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We tend to use the constants in written code, and this cleaner mathematical
    formulation in papers.
  id: totrans-243
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们倾向于在书面代码中使用常数，在论文中使用更简洁的数学公式。
- en: ^c
  id: totrans-244
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^c
- en: ''
  id: totrans-245
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is a version of the WGAN with gradient penalty that is commonly used in
    new academic papers. See Gulrajani et al., 2017, [http://arxiv.org/abs/1704.00028](http://arxiv.org/abs/1704.00028).
  id: totrans-246
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这是一种带有梯度惩罚的 WGAN 版本，在新的学术论文中常用。参见 Gulrajani 等人，2017 年，[http://arxiv.org/abs/1704.00028](http://arxiv.org/abs/1704.00028)。
- en: 5.4\. Training hacks
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 训练技巧
- en: 'We are now departing from the well-grounded academic results into the areas
    that academics or practitioners just “figured out.” These are simply hacks, and
    often you just have to try them to see if they work for you. The list in this
    section was inspired by Soumith Chintala’s 2016 post, “How to Train a GAN: Tips
    and Tricks to Make GANs Work” ([https://github.com/soumith/ganhacks](https://github.com/soumith/ganhacks)),
    but some things have changed since then.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在正从稳固的学术成果转向学术界或从业者刚刚“想出来”的领域。这些仅仅是技巧，通常您只需要尝试一下，看看它们是否适用于您。本节中的列表受到了 Soumith
    Chintala 2016 年帖子“如何训练一个 GAN：使 GAN 工作的技巧和窍门”([https://github.com/soumith/ganhacks](https://github.com/soumith/ganhacks))的启发，但自那时以来有些事情已经改变了。
- en: An example of what has changed is some of the architectural advice, such as
    the Deep Convolutional GAN (DCGAN) being a baseline for everything. Currently,
    most people start with the WGAN; in the future, the Self-Attention GAN (SAGAN
    is touched on in [chapter 12](../Text/kindle_split_023.xhtml#ch12)) may be a focus.
    In addition, some things are still true, and we regard them as universally accepted,
    such as using the Adam optimizer instead of vanilla stochastic gradient descent.^([[19](#ch0519)])
    We encourage you to check out the list, as its creation was a formative moment
    in GAN history.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 变化的一个例子是一些架构建议，例如深度卷积生成对抗网络（DCGAN）是所有事物的基线。目前，大多数人从WGAN开始；未来，自注意力生成对抗网络（SAGAN在第12章中有所涉及）可能会成为焦点。此外，一些事情仍然是真实的，我们将它们视为普遍接受的，例如使用Adam优化器而不是传统的随机梯度下降.^([[19](#ch0519)])
    我们鼓励您查看该列表，因为它的创建是GAN历史上的一个形成性时刻。
- en: ^(19)
  id: totrans-250
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(19)
- en: ''
  id: totrans-251
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why is Adam better than vanilla stochastic gradient descent (SGD)? Because Adam
    is an extension of SGD that tends to work better in practice. Adam groups several
    training hacks along with SGD into one easy-to-use package.
  id: totrans-252
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么Adam比传统的随机梯度下降（SGD）更好？因为Adam是SGD的一个扩展，在实践中往往表现得更好。Adam将几个训练技巧与SGD结合成一个易于使用的包。
- en: 5.4.1\. Normalizations of inputs
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1. 输入归一化
- en: Normalizing the images to be between –1 and 1 is still typically a good idea
    according to almost every machine learning resource, including Chintala’s list.
    We generally normalize because of the easier tractability of computations, as
    is the case with the rest of machine learning. Given this restriction on the inputs,
    it is a good idea to restrict your Generator’s final output with, for example,
    a *tanh* activation function.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 根据几乎每个机器学习资源，包括Chintala的列表，将图像归一化到-1和1之间仍然是通常的好主意。我们通常归一化是因为计算更容易处理，就像机器学习的其他部分一样。考虑到对输入的限制，使用例如*tanh*激活函数来限制生成器的最终输出是一个好主意。
- en: 5.4.2\. Batch normalization
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.2. 批标准化
- en: 'Batch normalization was discussed in detail in [chapter 4](../Text/kindle_split_013.xhtml#ch04).
    We include it here for completeness. As a note on how our perceptions of batch
    normalization have changed: originally batch norm was generally regarded as an
    extremely successful technique, but recently it has been shown to *sometimes*
    deliver bad results, especially in the Generator.^([[20](#ch05fn20)]) In the Discriminator,
    on the other hand, results have been almost universally positive.^([[21](#ch05fn21)])'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 批标准化在第4章中进行了详细讨论。我们将其包括在这里以示完整性。关于我们对批标准化的看法如何改变：最初，批标通常被认为是一个非常成功的技巧，但最近研究表明它*有时*会得到不良的结果，尤其是在生成器.^([[20](#ch05fn20)])
    相反，在判别器中，结果几乎普遍是积极的.^([[21](#ch05fn21)])
- en: ^(20)
  id: totrans-257
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(20)
- en: ''
  id: totrans-258
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Gulrajani et al., 2017, [http://arxiv.org/abs/1704.00028](http://arxiv.org/abs/1704.00028).
  id: totrans-259
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Gulrajani等人于2017年发表的《批标准化》，[http://arxiv.org/abs/1704.00028](http://arxiv.org/abs/1704.00028)。
- en: ^(21)
  id: totrans-260
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(21)
- en: ''
  id: totrans-261
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Tutorial on Generative Adversarial Networks—GANs in the Wild,” by Soumith
    Chintala, 2017, [https://www.youtube.com/watch?v=Qc1F3-Rblbw](https://www.youtube.com/watch?v=Qc1F3-Rblbw).
  id: totrans-262
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Soumith Chintala于2017年发表的《生成对抗网络教程——GAN在野外的应用》，[https://www.youtube.com/watch?v=Qc1F3-Rblbw](https://www.youtube.com/watch?v=Qc1F3-Rblbw)。
- en: 5.4.3\. Gradient penalties
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.3. 梯度惩罚
- en: This training trick builds on point 10 in Chintala’s list, which had the intuition
    that if the norms of the gradients are too high, something is wrong. Even today,
    networks such as BigGAN are innovating in this space, as we touch on in [chapter
    12](../Text/kindle_split_023.xhtml#ch12).^([[22](#ch05fn22)])
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这个训练技巧建立在Chintala列表中的第10点之上，其直觉是如果梯度的范数太高，那么可能有问题。即使今天，像BigGAN这样的网络在这个领域也在进行创新，正如我们在第12章中提到的.^([[22](#ch05fn22)])
- en: ^(22)
  id: totrans-265
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(22)
- en: ''
  id: totrans-266
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Large-Scale GAN Training for High-Fidelity Natural Image Synthesis,” by
    Andrew Brock et al., 2019, [https://arxiv.org/pdf/1809.11096.pdf](https://arxiv.org/pdf/1809.11096.pdf).
  id: totrans-267
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Andrew Brock等人于2019年发表的《大规模GAN训练用于高保真自然图像合成》，[https://arxiv.org/pdf/1809.11096.pdf](https://arxiv.org/pdf/1809.11096.pdf)。
- en: 'However, technical issues still remain: naive weighed clipping can produce
    vanishing or exploding gradients known from much of the rest of deep learning.^([[23](#ch05fn23)])
    We can restrict the gradient norm of the Discriminator output with respect to
    its input. In other words, if you change your input a little bit, your updated
    weights should not change too much. Deep learning is full of magic like this.
    This is especially important in the WGAN setting, but can be applied elsewhere.^([[24](#ch05fn24)])
    Generally, this trick has in some form been used by numerous papers.^([[25](#ch05fn25)])'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，技术问题仍然存在：简单的加权裁剪可以产生深度学习中常见的消失或爆炸梯度。[^([[23](#ch05fn23))] 我们可以限制判别器输出相对于其输入的梯度范数。换句话说，如果你稍微改变你的输入，你的更新权重不应该改变太多。深度学习充满了这样的魔法。这在
    WGAN 设置中尤为重要，但也可以应用于其他地方。[^([[24](#ch05fn24))] 通常，这种技巧以某种形式被许多论文使用。[^([[25](#ch05fn25))]
- en: ^(23)
  id: totrans-269
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(23)
- en: ''
  id: totrans-270
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Gulrajani et al., 2017, [http://arxiv.org/abs/1704.00028](http://arxiv.org/abs/1704.00028).
  id: totrans-271
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见 Gulrajani 等人于 2017 年发表的论文，[http://arxiv.org/abs/1704.00028](http://arxiv.org/abs/1704.00028)。
- en: ^(24)
  id: totrans-272
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(24)
- en: ''
  id: totrans-273
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Though here the authors call the Discriminator critic, borrowing from reinforcement
    learning, as much of that paper is inspired by it.
  id: totrans-274
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 尽管在这里作者将判别器称为批评者，借鉴了强化学习，因为那篇论文的大部分灵感都来自它。
- en: ^(25)
  id: totrans-275
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(25)
- en: ''
  id: totrans-276
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See “Least Squares Generative Adversarial Networks,” by Xudong Mao et al.,
    2016, [http://arxiv.org/abs/1611.04076](http://arxiv.org/abs/1611.04076). Also
    see “BEGAN: Boundary Equilibrium Generative Adversarial Networks,” by David Berthelot
    et al., 2017, [http://arxiv.org/abs/1703.10717](http://arxiv.org/abs/1703.10717).'
  id: totrans-277
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '参见 Xudong Mao 等人于 2016 年发表的“Least Squares Generative Adversarial Networks”，[http://arxiv.org/abs/1611.04076](http://arxiv.org/abs/1611.04076)。另见
    David Berthelot 等人于 2017 年发表的“BEGAN: Boundary Equilibrium Generative Adversarial
    Networks”，[http://arxiv.org/abs/1703.10717](http://arxiv.org/abs/1703.10717)。'
- en: Here, we can simply use the native implementation of your favorite deep learning
    framework to penalize the gradient and not focus on the implementation detail
    beyond what we described. Smarter methods have recently been published by top
    researchers (including one good fellow) and presented at ICML 2018, but their
    widespread academic acceptance has not been proven yet.^([[26](#ch05fn26)]) A
    lot of work is being done to make GANs more stable—such as Jacobian clamping,
    which is also yet to be reproduced in any meta-study—so we will need to wait and
    see which methods will make it.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以简单地使用你最喜欢的深度学习框架的原生实现来惩罚梯度，而不用关注我们描述之外的实现细节。最近，顶级研究人员（包括一位优秀的同行）已经发表了更智能的方法，并在
    ICML 2018 上进行了展示，但它们的广泛学术接受度尚未得到证明。[^([[26](#ch05fn26))] 许多工作正在进行中，以使 GANs 更加稳定——例如
    Jacobian clamping，这也在任何元研究中尚未得到再现——因此我们需要等待并看看哪些方法会成功。
- en: ^(26)
  id: totrans-279
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(26)
- en: ''
  id: totrans-280
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Odena et al., 2018, [http://arxiv.org/abs/1802.08768](http://arxiv.org/abs/1802.08768).
  id: totrans-281
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见 Odena 等人于 2018 年发表的论文，[http://arxiv.org/abs/1802.08768](http://arxiv.org/abs/1802.08768)。
- en: 5.4.4\. Train the Discriminator more
  id: totrans-282
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.4\. 更多地训练判别器
- en: 'Training the Discriminator more is an approach that has recently gained a lot
    of success. In Chintala’s original list, this is labeled as being uncertain, so
    use it with caution. There are two broad approaches:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 训练判别器更多是一种最近取得很大成功的方法。在 Chintala 的原始列表中，这被标记为不确定，所以请谨慎使用。有两种主要方法：
- en: Pretraining the Discriminator before the Generator even gets the chance to produce
    anything.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生成器有机会产生任何东西之前先预训练判别器。
- en: Having more updates for the Discriminator per training cycle. A common ratio
    is five Discriminator weight updates per one of the Generator’s.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个训练周期中为判别器提供更多的更新。一个常见的比例是每生成器更新一次，判别器更新五次。
- en: In the words of deep learning researcher and teacher Jeremy Howard, this works
    because it is “the blind leading the blind.” You need to initially and continuously
    inject information about what the real-world data looks like.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习研究员和教师 Jeremy Howard 的话说，这是因为它是“盲人领盲人”。你需要最初和持续地注入有关真实世界数据外观的信息。
- en: 5.4.5\. Avoid sparse gradients
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.5\. 避免稀疏梯度
- en: 'It intuitively makes sense that sparse gradients (such as the ones produced
    by ReLU or MaxPool) would make training harder. This is because of the following:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，稀疏梯度（如 ReLU 或 MaxPool 产生的）会使训练更困难。这是因为以下原因：
- en: 'The intuition, especially behind average pooling, can be confusing, but think
    of it this way: if we go with standard max pooling, we lose *all but the maximum*
    value for the entire receptive field of a convolution, and that makes it much
    harder to use the transposed convolutions—in DCGAN’s case—to recover the information.
    With average pooling, we at least have a sense of what the *average* value is.
    It is still not perfect—we are still losing information—but at least less than
    before, because the average is more representative than the simple maximum.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直觉，尤其是平均池化背后的直觉，可能会让人困惑，但可以这样想：如果我们采用标准的最大池化，我们将丢失整个卷积感受野中除了最大值之外的所有值，这使得使用转置卷积（在
    DCGAN 的情况下）来恢复信息变得更加困难。使用平均池化，我们至少对平均值有一个概念。这仍然不是完美的——我们仍然在丢失信息——但至少比之前少，因为平均值比简单的最大值更能代表。
- en: Another problem is information loss, if we are using, say, regular rectified
    linear unit (ReLU) activation. A way to look at this problem is to consider how
    much information is lost when applying this operation, because we might have to
    recover it later. Recall that ReLU(*x*) is simply max(*0,x*), which means that
    for all the negative values, all this information is lost forever. If instead
    we ensure that we carry over the information from the negative regions and signify
    that this information is different, we can preserve all this information.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个问题是我们使用，比如说，常规的修正线性单元（ReLU）激活时可能会出现的信息损失。看待这个问题的方法之一是考虑在应用这个操作时丢失了多少信息，因为我们可能以后需要恢复它。回想一下，ReLU(*x*)
    简单地是 max(*0,x*)，这意味着对于所有负值，所有这些信息都将永远丢失。如果我们确保从负区域传递信息，并表明这些信息是不同的，我们就可以保留所有这些信息。
- en: 'As we suggested, fortunately, a simple solution exists for both of these: we
    can use Leaky ReLU—which is something like 0.1 × *x* for negative *x*, and 1 ×
    *x* for *x* that’s at least 0—and average pooling to get around a lot of these
    problems. Other activation functions exist (such as *sigmoid*, ELU, and *tanh*),
    but people tend to use Leaky ReLU most commonly.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所建议的，幸运的是，对于这两个问题都存在一个简单的解决方案：我们可以使用 Leaky ReLU——对于负 *x*，它类似于 0.1 × *x*，而对于至少为
    0 的 *x*，则是 1 × *x*——以及平均池化来绕过许多这些问题。其他激活函数也存在（例如 *sigmoid*、ELU 和 *tanh*），但人们通常最常使用
    Leaky ReLU。
- en: '|  |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: The Leaky ReLU can be any real number, typically, 0 < *x* < 1.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU 可以是任何实数，通常情况下，0 < *x* < 1。
- en: '|  |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Overall, we are trying to minimize information loss and make the flow of information
    the most logical it can be, without asking the GAN to backpropagate the error
    in some strange way, where it also has to learn the mapping.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，我们试图最小化信息损失，并使信息流尽可能合理，而无需要求 GAN 以某种奇怪的方式反向传播错误，同时它还要学习映射。
- en: 5.4.6\. Soft and noisy labels
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.6. 软标签和噪声标签
- en: Researchers use several approaches to either add noise to labels or smooth them.
    Ian Goodfellow tends to recommend one-sided label smoothing (for example, using
    0 and 0.9 as binary labels), but generally playing around with either adding noise
    or clipping seems to be a good idea.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 研究者们采用多种方法来对标签添加噪声或进行平滑处理。伊恩·古德费洛（Ian Goodfellow）倾向于推荐单侧标签平滑（例如，使用 0 和 0.9 作为二进制标签），但通常来说，在添加噪声或裁剪方面进行尝试似乎是个不错的想法。
- en: Summary
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: You have learned why evaluation is such a difficult topic for generative models
    and how we can train a GAN well with clear criteria indicating when to stop.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你已经学会了为什么评估对于生成模型来说是一个如此困难的话题，以及我们如何通过明确的停止标准来训练一个良好的 GAN。
- en: Various evaluation techniques move beyond the naive statistical evaluation of
    distributions and provide us with something more useful that correlates with visual
    sample quality.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的评估技术超越了分布的简单统计评估，为我们提供了与视觉样本质量相关的一些更有用的信息。
- en: 'Training is performed in three setups: the game-theoretical Min-Max GAN, the
    heuristically motivated Non-Saturating GAN, and the newest and theoretically well-founded
    Wasserstein-GAN.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练在三种设置中进行：博弈论的最小-最大 GAN、启发式动机的非饱和 GAN 以及最新且理论基础良好的 Wasserstein-GAN。
- en: 'Training hacks that allow us to train faster include the following:'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下是一些允许我们更快训练的训练技巧：
- en: Normalizing inputs, which is standard in machine learning
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化输入，这在机器学习中是标准的
- en: Using gradient penalties that give us more stability in training
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用梯度惩罚，这为我们提供了在训练中的更多稳定性
- en: Helping to warm-start the Discriminator to ultimately give us a good Generator,
    because doing so sets a higher bar for the generated samples
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帮助预热判别器，最终为我们提供一个好的生成器，因为这样做为生成的样本设定了一个更高的标准
- en: Avoiding sparse gradients, because they lose too much information
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免稀疏梯度，因为它们会丢失太多信息
- en: Playing around with soft and noisy labels rather than the typical binary classification
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在软标签和噪声标签上玩弄，而不是典型的二分类
- en: Chapter 6\. Progressing with GANs
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6章\. 与GAN一起进步
- en: '*This chapter covers*'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Progressively growing Discriminator and Generator networks throughout training
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中逐步增长判别器和生成器网络
- en: Making training more stable, and the output more varied and of higher quality
    and resolution
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使训练更加稳定，输出更加多样化、质量更高和分辨率更高
- en: Using TFHub, a new central repository for models and TensorFlow code
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TFHub，一个用于模型和TensorFlow代码的新中央仓库
- en: In this chapter, we provide a hands-on tutorial to build a Progressive GAN by
    using TensorFlow and the newly released TensorFlow Hub (TFHub). The *Progressive
    GAN* (aka *PGGAN*, or *ProGAN*) is a cutting-edge technique that has managed to
    generate full-HD photorealistic images. Presented at one of the top machine learning
    conferences, the International Conference on Learning Representations (ICLR) in
    2018, this technique made such a splash that Google immediately integrated it
    as one of the few models to be part of the TensorFlow Hub. In fact, this technique
    was lauded by Yoshua Bengio—one of the grandfathers of deep learning—as “almost
    too good to be true.” When it was released, it became an instant favorite of academic
    presentations and experimental projects.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们提供了一个使用TensorFlow和最新发布的TensorFlow Hub（TFHub）构建Progressive GAN的实战教程。*Progressive
    GAN*（也称为*PGGAN*或*ProGAN*）是一种前沿技术，它成功地生成了全高清逼真图像。在2018年举办的顶级机器学习会议——国际学习表示会议（ICLR）上展出，这项技术引起了巨大的轰动，谷歌立即将其集成为数不多的TensorFlow
    Hub模型之一。事实上，这项技术受到了深度学习之父之一Yoshua Bengio的高度赞扬，称其为“几乎太好了以至于不真实。”发布后，它立即成为学术演示和实验项目的热门选择。
- en: 'We recommend that you go through this chapter with TensorFlow 1.7 or higher,
    but 1.8+ was the latest release at the time of writing, so that was the one we
    used. For TensorFlow Hub, we suggest using a version no later than 0.4.0, because
    later versions have trouble importing due to compatibility issues with TensorFlow
    1.x. After reading this chapter, you’ll be able to implement all the key improvements
    of the Progressive GAN. These four innovations are as follows:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议你使用TensorFlow 1.7或更高版本阅读本章，但写作时最新的发布版本是1.8+，所以我们使用了这个版本。对于TensorFlow Hub，我们建议使用不超过0.4.0的版本，因为后续版本由于与TensorFlow
    1.x的兼容性问题而难以导入。阅读本章后，你将能够实现Progressive GAN的所有关键改进。这四个创新如下：
- en: Progressively growing and smoothly fading in higher-resolution layers
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在更高分辨率的层中逐步增长和平滑衰减
- en: Mini-batch standard deviation
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小批量标准差
- en: Equalized learning rate
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等化学习率
- en: Pixel-wise feature normalization
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像素级特征归一化
- en: 'This chapter features two main examples:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含两个主要示例：
- en: Code for the crucial innovations of Progressive GANs—more specifically, the
    smoothly fading-in higher-resolution layers and the other three innovations as
    listed previously. The rest of the implementation of the Progressive GAN technique
    is too substantial to be included in this book.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Progressive GANs的关键创新代码——更具体地说，是平滑渐变的高分辨率层以及其他三个之前列出的创新。Progressive GAN技术的其余实现内容过于庞大，无法包含在这本书中。
- en: A pretrained, easily downloadable implementation as provided by Google on TFHub,
    which is a new centralized repository for machine learning models, similar to
    Docker Hub or Conda and PyPI repositories in the software package world. This
    implementation will allow us to do latent space interpolation to control the features
    of the generated examples. It will briefly touch on the seeding vectors in the
    latent space of the Generator so that we can get pictures that we want. You saw
    this idea in [chapters 2](../Text/kindle_split_011.xhtml#ch02) and [4](../Text/kindle_split_013.xhtml#ch04).
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google在TFHub上提供的预训练、易于下载的实现，TFHub是一个新的机器学习模型集中仓库，类似于软件包世界的Docker Hub或Conda和PyPI仓库。这个实现将使我们能够进行潜在空间插值，以控制生成的示例的特征。它将简要介绍生成器潜在空间中的种子向量，以便我们可以得到我们想要的图片。你可以在[第2章](../Text/kindle_split_011.xhtml#ch02)和[第4章](../Text/kindle_split_013.xhtml#ch04)中看到这个想法。
- en: 'The reasons we decided to implement the PGGAN using TFHub rather than from
    the ground up as we do in all the other chapters are threefold:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定使用TFHub实现PGGAN而不是像其他章节那样从头开始实现的原因有三：
- en: 'Especially for practitioners, we want to make sure you are—at least in one
    chapter—exposed to the software engineering best practices that may speed up your
    workflow. Want to try a quick GAN on your problem? Just use one of the implementations
    on TFHub. There are now many more than when we were first writing this chapter,
    including many reference implementations (for example, for BigGAN in [chapter
    12](../Text/kindle_split_023.xhtml#ch12) and NS-GAN in [chapter 5](../Text/kindle_split_015.xhtml#ch05)).
    We want to give you access to easy-to-use, state-of-the-art examples, because
    this is the way that machine learning is going—automating as much of machine learning
    as possible so we can focus on what matters the most: delivering impact. Google’s
    Cloud AutoML ([https://cloud.google.com/automl/](https://cloud.google.com/automl/))
    and Amazon SageMaker ([https://aws.amazon.com/sagemaker/](https://aws.amazon.com/sagemaker/))
    are prime examples of this trend. Even Facebook recently introduced PyTorch Hub,
    so both major frameworks now have one.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尤其对于实践者来说，我们想确保你至少在一个章节中接触到可能加快你工作流程的软件工程最佳实践。想要尝试快速GAN来解决你的问题吗？只需使用TFHub上的实现之一。现在比我们最初写这一章时多了很多，包括许多参考实现（例如，第12章中的BigGAN和第5章中的NS-GAN）。我们想让你接触到易于使用、最前沿的例子，因为这就是机器学习的趋势——尽可能自动化机器学习，这样我们就可以专注于最重要的事情：产生影响力。谷歌的Cloud
    AutoML([https://cloud.google.com/automl/](https://cloud.google.com/automl/))和亚马逊的SageMaker([https://aws.amazon.com/sagemaker/](https://aws.amazon.com/sagemaker/))是这个趋势的典型例子。甚至Facebook最近也推出了PyTorch
    Hub，因此现在两个主要框架都拥有这样的资源。
- en: The original implementation of PGGAN took the NVIDIA researchers *one to two
    months* to run, which we thought was impractical for any person to run on their
    own, especially if you want to experiment or get something wrong.^([[1](#ch06fn01)])
    TFHub still gives you a fully trainable PGGAN, so if you want to repurpose the
    days of computation for something else, you can!
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PGGAN的原始实现需要NVIDIA研究人员*一到两个月*的时间来运行，我们认为这对于任何个人来说都是不切实际的，尤其是如果你想要实验或出错的话。[1](#ch06fn01)
    TFHub仍然提供了一个完全可训练的PGGAN，所以如果你想将计算时间用于其他目的，你可以这样做！
- en: ¹
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Progressive Growing of GANs for Improved Quality, Stability, and Variation,”
    by Tero Karras, 2018, [https://github.com/tkarras/progressive_growing_of_gans](https://github.com/tkarras/progressive_growing_of_gans).
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Tero Karras于2018年发表的“Progressive Growing of GANs for Improved Quality, Stability,
    and Variation”，[https://github.com/tkarras/progressive_growing_of_gans](https://github.com/tkarras/progressive_growing_of_gans)。
- en: We still want to show you PGGANs’ most important innovations. But if we want
    to explain those well—including code—we can’t fit all the implementation details
    into one chapter, even in Keras, as all the implementations tend to be pretty
    sizeable. TFHub allows us to skip over the boilerplate code and focus on the ideas
    that matter.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们仍然想向你展示PGGANs最重要的创新。但如果我们想要很好地解释这些——包括代码——即使是在Keras中，我们也无法将所有实现细节都放入一个章节中，因为所有的实现都相当庞大。TFHub允许我们跳过样板代码，专注于重要的想法。
- en: 6.1\. Latent space interpolation
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 潜空间插值
- en: Recall from [chapter 2](../Text/kindle_split_011.xhtml#ch02) that we have this
    lower-resolution space—called *latent space*—that seeds our output. As with the
    DCGAN from [chapter 4](../Text/kindle_split_013.xhtml#ch04) and indeed the Progressive
    GAN, the initial trained latent space has semantically meaningful properties.
    It means that we can find the vector offsets that, for example, introduce eyeglasses
    to an image of a face, and the same offset will introduce glasses in new images.
    We can also pick two random vectors and then move in equal increments between
    them and so gradually—smoothly—get an image that matches the second vector.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第2章](../Text/kindle_split_011.xhtml#ch02)，我们有一个这个低分辨率空间——称为*潜空间*——它是我们输出的种子。与第4章中的DCGAN以及渐进式GAN一样，最初训练的潜空间具有语义上有意义的属性。这意味着我们可以找到一些向量偏移量，例如，将眼镜添加到一张人脸图像中，同样的偏移量会在新的图像中添加眼镜。我们还可以选择两个随机向量，然后在它们之间以相等的增量移动，从而逐渐——平滑地——得到一个与第二个向量匹配的图像。
- en: This is called *interpolation*, and you can see this process in [figure 6.1](#ch06fig01).
    As the author of BigGAN said, meaningful transitions from one vector to another
    show that the GAN has learned some underlying structure.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为*插值*，你可以在[图6.1](#ch06fig01)中看到这个过程。正如BigGAN的作者所说，从一个向量到另一个向量的有意义过渡表明GAN已经学习了一些底层结构。
- en: Figure 6.1\. We can perform latent space interpolation because the latent vector
    we send to the Generator produces consistent outcomes that are predictable in
    some ways; not only is the generative process predictable, but also the output
    is not jagged—or reacting sharply to small changes—considering the latent vector
    changes. If we, for example, want an image that is a blend of two faces, we just
    need to search somewhere around the average of the two vectors.
  id: totrans-333
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1。我们可以执行潜在空间插值，因为发送给生成器的潜在向量会产生一致的结果，这些结果在某些方面是可预测的；不仅生成过程是可预测的，而且输出也不是锯齿状的——或者对小的变化反应剧烈——考虑到潜在向量的变化。例如，如果我们想要一张融合两个面孔的图像，我们只需要在两个向量的平均值附近搜索。
- en: '![](../Images/06fig01_alt.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06fig01_alt.jpg)'
- en: 6.2\. They grow up so fast
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2. 他们成长得如此之快
- en: In previous chapters, you learned which results are easy to achieve with GANs
    and which are difficult. Moreover, things like *mode collapse* (showing only a
    few examples of the overall distribution) and lack of *convergence* (one of the
    causes of poor quality of the results) are no longer alien terms to us.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你学习了使用GAN容易实现哪些结果以及哪些结果难以实现。此外，像*模式坍塌*（只显示总体分布的几个示例）和*收敛性不足*（导致结果质量不佳的原因之一）这样的术语对我们来说已经不再陌生了。
- en: 'Recently, a Finnish NVIDIA team released a paper that has managed to blow many
    previous cutting-edge papers out of the water: “Progressive Growing of GANs for
    Improved Quality, Stability, and Variation,” by Tero Karras et al. This paper
    features four fundamental innovations, so let’s walk through them in order.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，一个芬兰的NVIDIA团队发布了一篇论文，成功地超越了之前许多前沿的论文：“用于提高质量、稳定性和多样性的GAN的渐进式增长”，由Tero Karras等人撰写。这篇论文有四个基本创新，所以让我们按顺序逐一介绍。
- en: 6.2.1\. Progressive growing and smoothing of higher-resolution layers
  id: totrans-338
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1. 高分辨率层的渐进式增长和平滑
- en: 'Before we dive into what the Progressive GAN does, let’s start with a simple
    analogy. Imagine looking at a mountain region from a bird’s-eye view: you have
    lots of valleys, which have nice creeks and villages—generally quite habitable.
    Then you have many mountain tops that are rough and generally unpleasant to live
    on because of weather conditions. This sort of represents the loss function landscape,
    where we want to minimize the loss by going down the mountain slopes and into
    the valleys, which are much nicer.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨渐进式生成对抗网络（GAN）的功能之前，让我们从一个简单的类比开始。想象一下从鸟瞰的角度观察一个山区：你看到许多山谷，那里有美丽的溪流和村庄——通常非常适合居住。然后，你还有许多山顶，由于天气条件，它们崎岖不平，通常不适宜居住。这类似于损失函数的地形，我们希望通过沿着山坡向下进入山谷来最小化损失，因为这些山谷要美好得多。
- en: We can imagine training as dropping a mountaineer into a random place in this
    mountain region and then following their path down the slope into a valley. This
    is what *stochastic gradient descent* does, and [chapter 10](../Text/kindle_split_021.xhtml#ch10)
    revisits this in a lot more detail. Now, unfortunately, if we start with a very
    complex mountain range, the mountaineer will not know which direction to travel.
    The space around our adventurer would be jagged and rough. It would be difficult
    to make out where the nicest, lowest valley is with lots of habitable lands. Instead,
    we zoom out and reduce the complexity of the mountain range to give the mountaineer
    a high-level picture of this particular area.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以想象训练就像将登山者随机地投入到这个山区的一个地方，然后跟随他们沿着山坡进入山谷的路径。这就是*随机梯度下降*所做的事情，[第10章](../Text/kindle_split_021.xhtml#ch10)更详细地回顾了这一点。现在，不幸的是，如果我们从一个非常复杂的山脉开始，登山者将不知道该走哪个方向。我们冒险者周围的空间将是锯齿状和崎岖的。很难辨认出哪个最宜居住、最低的山谷。相反，我们放大视野，降低山脉的复杂性，给登山者一个这个特定区域的总体印象。
- en: 'As our mountaineer gets closer to a valley, we can start increasing the complexity
    by zooming in on the terrain. Then we no longer see just the coarse/pixelated
    texture, but instead get to see the finer details. This approach has the advantage
    that as our mountaineer goes down the slope, they can easily make little optimizations
    to make the hiking easier. For example, they can take a path through a dried-up
    creek to make the descent into the valley even faster. That is *progressive growing*:
    increasing the resolution of the terrain as we go.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的登山者接近山谷时，我们可以通过放大地形来开始增加复杂性。然后，我们不再只看到粗糙/像素化的纹理，而是可以看到更细致的细节。这种方法的优点是，随着登山者沿着山坡下行，他们可以轻松地进行一些小的优化，使徒步旅行更加容易。例如，他们可以穿过一条干涸的溪流，使山谷的下降更加迅速。这就是*渐进式增长*：随着我们的前进，提高地形的分辨率。
- en: However, if you have ever seen an open world computer game or scrolled too quickly
    through Google Earth with 3D on, you know that quickly increasing the resolution
    of the terrain around you can be startling and unpleasant. Objects all of a sudden
    jump into existence. So instead, we progressively *smooth in* and slowly introduce
    more complexity as the mountaineer gets closer to the objective.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你曾经玩过开放世界的电脑游戏，或者用3D模式快速滚动过Google Earth，你就会知道快速提高你周围地形的分辨率可能会令人震惊且不愉快。物体突然出现。因此，我们逐渐平滑地引入，并随着登山者接近目标而缓慢地引入更多复杂性。
- en: In technical terms, we are going from a few low-resolution convolutional layers
    to many high-resolution ones as we train. Thus, we first train the early layers
    and only then introduce a higher-resolution layer, where it is harder to navigate
    the loss space. We go from something simple—for example, 4 × 4 trained for several
    steps—to something more complex—for example, 1024 × 1024 trained for several epochs,
    as shown in [figure 6.2](#ch06fig02).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度讲，随着训练的进行，我们从几个低分辨率的卷积层转向许多高分辨率的层。因此，我们首先训练早期层，然后才引入一个更高分辨率的层，在那里导航损失空间更困难。我们从简单的东西开始——例如，训练了几个步骤的4
    × 4——到更复杂的东西——例如，训练了几个时期的1024 × 1024，如图6.2所示。[图6.2](#ch06fig02)。
- en: 'Figure 6.2\. Can you see how we start with a smooth mountain range and gradually
    increase the complexity by zooming in? That is effectively what adding extra layers
    does to the loss function. This is handy, as our mountain region (loss function)
    is much easier to navigate when it is less jagged. You can think of it as follows:
    when we have a more complex structure (b), the loss function is jagged and hard
    to navigate (d), because there are so many parameters—especially in early layers—that
    can have a massive impact and generally increase the dimensionality of the problem.
    However, if we initially remove some part of the complexity (a), we can early
    on get a loss function that is much easier to navigate (c) and increases in complexity
    only as we gain confidence that we are at the approximately right part of the
    loss space. Only then do we move from (a) and (c) into (b) and (d) versions.'
  id: totrans-344
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.2。你能看到我们是怎样从一个平滑的山脉开始，并通过放大来逐渐增加复杂度吗？这正是额外层对损失函数所做的。这很有用，因为我们的山脉区域（损失函数）在变得不那么尖锐时更容易导航。你可以这样想：当我们有一个更复杂的结构（b）时，损失函数是尖锐的，难以导航（d），因为有很多参数——尤其是在早期层——可以产生巨大影响，通常会增加问题的维度。然而，如果我们最初移除一些复杂性（a），我们就可以在早期得到一个更容易导航的损失函数（c），并且只有在我们对损失空间的近似正确部分有信心时，复杂性才会增加。只有在这种情况下，我们才从（a）和（c）版本移动到（b）和（d）版本。
- en: '![](../Images/06fig02_alt.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06fig02_alt.jpg)'
- en: The problem in this scenario is that upon introducing even one more layer at
    a time (for example, from 4 × 4 to 8 × 8), we are still introducing a massive
    shock to the training. What the PGGAN authors do instead is smoothly fade in those
    layers, as in [figure 6.3](#ch06fig03), in order to give the system time to adapt
    to the higher resolution.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下的问题是，每次引入一个额外的层（例如，从4 × 4到8 × 8），我们仍然在训练中引入了巨大的冲击。PGGAN的作者所做的相反，是在[图6.3](#ch06fig03)中平滑地引入这些层，以便给系统时间来适应更高的分辨率。
- en: 'Figure 6.3\. When we’ve trained for enough steps with, say, 16 × 16 resolution
    (a), we introduce another transposed convolution in the Generator (G) and another
    convolution in the Discriminator (D) to get the “interface” between G and D to
    be 32 × 32\. But we also introduce two pathways: (1 – α) simple nearest neighbor
    upscaling, which does not have any trained parameters, but is also quite naive;
    and (α) extra transposed convolution, which requires training but will ultimately
    perform much better.'
  id: totrans-347
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.3。当我们用16 × 16分辨率（a）等训练足够步骤后，我们在生成器（G）中引入另一个转置卷积，在判别器（D）中引入另一个卷积，以获得G和D之间的“接口”为32
    × 32。但我们还引入了两个路径：（1 – α）简单的最近邻上采样，它没有训练参数，但也很天真；（α）额外的转置卷积，需要训练但最终会表现得更好。
- en: '![](../Images/06fig03_alt.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06fig03_alt.jpg)'
- en: However, rather than immediately jumping to this resolution, we smoothly fade
    in this new layer with higher resolution by a parameter alpha (α), which is between
    0 and 1\. Alpha affects how much we use either the old—but upscaled—layer or the
    natively larger one. On the side of the *D*, we simply shrink by 0.5*x* to allow
    for smoothly injecting the trained layer for discrimination. This is (b) in [figure
    6.3](#ch06fig03). When we are confident about this new layer, we keep the 32 ×
    32—(c) in the figure—and then we are getting ready to grow yet again after we
    have trained 32 × 32 properly.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们并没有立即跳到这个分辨率，而是通过参数alpha（α）平滑地引入这个更高分辨率的新的层，alpha的值在0和1之间。Alpha影响我们使用多少旧层（但已放大）或原生更大的层。在D的方面，我们简单地缩小0.5*x*，以便平滑地注入用于判别的训练层。这是图6.3中的(b)。当我们对这一新层有信心时，我们保留32
    × 32——图中的(c)，然后在我们正确训练了32 × 32之后，我们准备再次生长。
- en: 6.2.2\. Example implementation
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2\. 示例实现
- en: 'For all the innovations we’ve detailed, in this section we’ll give you working
    but isolated versions so that we can talk code. As an exercise, you may want to
    try implementing these things as one GAN network, maybe using the existing prior
    architectures. If you are ready, let’s load up ye olde, trusty machine learning
    libraries and get cracking:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们详细描述的所有创新，在本节中我们将提供工作但独立的版本，这样我们就可以讨论代码。作为一个练习，你可能想尝试将这些内容实现为一个GAN网络，也许使用现有的先验架构。如果你准备好了，让我们加载我们那可靠的、值得信赖的机器学习库，并开始工作：
- en: '[PRE0]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the code, progressive smoothing in may look something like the following
    listing.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，渐进式平滑可能看起来像以下列表。
- en: Listing 6.1\. Progressive growing and smooth upscaling
  id: totrans-354
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.1\. 渐进式生长和平滑上采样
- en: '[PRE1]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***1* Hint! If you are using pure TensorFlow rather than Keras, always remember
    scope.**'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 提示！如果你使用的是纯TensorFlow而不是Keras，始终要记住作用域。**'
- en: '***2* Now we have the originally trained layer.**'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 现在我们有了最初训练的层。**'
- en: '***3* The newly added layer not yet fully trained**'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 新增的层尚未完全训练**'
- en: '***4* This makes sure we can run the merging code.**'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 这确保我们可以运行合并代码。**'
- en: '***5* This code block should take advantage of broadcasting.**'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 此代码块应充分利用广播功能。**'
- en: Now that you have an understanding of the lower-level details of progressive
    growing and smoothing without unnecessary complexity, hopefully you can appreciate
    how general this idea is. Although Karras et al., were by no means the first to
    come up with some way of increasing model complexity during training, this seems
    like by far the most promising avenue and indeed the innovation that resonated
    the most. As of June 2019, this paper was cited over 730 times. With that context
    in mind, let’s move on to the second big innovation.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了渐进式生长和平滑的底层细节，且没有不必要的复杂性，希望你能欣赏这一想法的普适性。尽管Karras等人并非第一个在训练过程中想出某种方法来增加模型复杂性的，但这似乎是最有希望的途径，并且确实是反响最强烈的创新。截至2019年6月，这篇论文已被引用超过730次。考虑到这个背景，让我们继续探讨第二个重大创新。
- en: 6.2.3\. Mini-batch standard deviation
  id: totrans-362
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3\. 小批量标准差
- en: The next innovation introduced by Karras et al. in their paper is *mini-batch
    standard deviation*. Before we dive into it, let’s recall from [chapter 5](../Text/kindle_split_015.xhtml#ch05)
    the issue of mode collapse, which occurs when the GAN learns how to create a few
    good examples or only slight permutations on them. We generally want to produce
    the faces of all the people in the real dataset, maybe not just one picture of
    one woman.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: Karras等人在其论文中引入的下一个创新是*小批量标准差*。在我们深入探讨之前，让我们从[第5章](../Text/kindle_split_015.xhtml#ch05)回顾一下模式坍塌的问题，这是当GAN学习如何创建几个好的示例或仅对其做轻微排列时发生的。我们通常希望生成真实数据集中所有人的面孔，而不仅仅是某位女性的一个图片。
- en: 'Therefore, Karras et al. created a way for the Discriminator to tell whether
    the samples it is getting are varied enough. In essence, we calculate a single
    extra scalar statistic for the Discriminator. This statistic is the standard deviation
    of all the pixels in the mini-batch that are generated by the Generator or that
    come from the real data. That is an amazingly simple and elegant solution: now
    all the Discriminator needs to learn is that if the standard deviation is low
    in the images from the batch it is evaluating, the image is likely fake, because
    the real data has more variance.^([[2](#ch06fn02)]) The Generator has no choice
    but to increase the variance of the generated samples to have a chance to fool
    the Discriminator.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Karras 等人创造了一种方法，让判别器能够判断它所获得的样本是否足够多样化。本质上，我们为判别器计算一个额外的标量统计量。这个统计量是生成器生成的或来自真实数据的迷你批处理中所有像素的标准差。这是一个非常简单且优雅的解决方案：现在判别器需要学习的只是，如果它评估的批处理图像中的标准差低，那么图像很可能是伪造的，因为真实数据具有更多的方差。[2](#ch06fn02)
    生成器别无选择，只能增加生成样本的方差，以有机会欺骗判别器。
- en: ²
  id: totrans-365
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-366
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Some may object that this can also happen when the sampled real data includes
    a lot of very similar pictures. Though this is technically true, in practice this
    is easy to fix, and remember that the similarity would have to be so high that
    a single pass of a simple nearest neighbor clustering would reveal it.
  id: totrans-367
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有些人可能会反对，当采样的真实数据包含大量非常相似的图片时，这也可能发生。虽然从技术上讲这是正确的，但在实践中这很容易解决，记住相似度必须非常高，以至于简单的最近邻聚类就能揭示这一点。
- en: Moving beyond the intuition, the technical implementation is straightforward
    as it applies only to the Discriminator. Given that we also want to minimize the
    number of trainable parameters, we include only a single extra number, which seems
    to be enough. This number is appended as a feature map—think *dimension* or the
    last number in the `tf.shape` list.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 超越直觉，技术实现很简单，因为它只应用于判别器。鉴于我们还想最小化可训练参数的数量，我们只包含一个额外的数字，这似乎已经足够。这个数字作为特征图附加——想想
    *维度* 或 `tf.shape` 列表中的最后一个数字。
- en: 'The exact procedure is as follows and is depicted in [listing 6.2](#ch06ex02):'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 具体的步骤如下，并在 [列表 6.2](#ch06ex02) 中展示：
- en: '[4D -> 3D] We compute the standard deviation across all the images in the batch,
    across all the remaining channels—height, width, and color. We then get a single
    image with standard deviations for each pixel and each channel.'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4D -> 3D] 我们计算批处理中所有图像的标准差，以及所有剩余的通道——高度、宽度和颜色。然后我们得到一个包含每个像素和每个通道标准差的单一图像。'
- en: '[3D -> 2D] We average the standard deviations across all channels—to get a
    single feature map or matrix of standard deviations for that pixel, but with a
    collapsed color channel.'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3D -> 2D] 我们对所有通道的标准差进行平均——得到一个单独的特征图或标准差矩阵，针对该像素，但颜色通道已合并。'
- en: '[2D -> Scalar/0D] We average the standard deviations for all pixels within
    the preceding matrix to get a single scalar value.'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2D -> 标量/0D] 我们对前一个矩阵中所有像素的标准差进行平均，以得到一个单独的标量值。'
- en: Listing 6.2\. Mini-batch standard deviation
  id: totrans-373
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.2\. 小批量标准差
- en: '[PRE2]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***1* Hint! If you are using pure TensorFlow rather than Keras, always remember
    scope. A mini-batch group must be divisible by (or <=) group_size.**'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 提示！如果你使用的是纯 TensorFlow 而不是 Keras，请始终记住作用域。小批量组必须能被 group_size 整除（或 <=）。**'
- en: '***2* Just getting some shape information so that we can use it as shorthand
    as well as ensure defaults. We get the input from tf. shape, as the “pre-image”
    dimensions are typically cast as None before graph execution.**'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 仅获取一些形状信息，以便我们可以将其用作缩写，并确保默认值。我们从 tf.shape 获取输入，因为“前图像”维度在图执行之前通常被转换为
    None。**'
- en: '***3* Reshaping so that we operate on the level of the mini-batch. In this
    code, we assume the layer to be [Group (G), Mini-batch (M), Width (W), Height
    (H), Channel (C)], but be careful: different implementations use the Theano-specific
    order instead.**'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 重新塑形，以便我们在小批量级别上进行操作。在这段代码中，我们假设层为 [组 (G), 小批量 (M), 宽度 (W), 高度 (H), 通道
    (C)]，但请注意：不同的实现使用 Theano 特定的顺序。**'
- en: '***4* Centers the mean over the group [M,W,H,C]**'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 在组 [M,W,H,C] 上对均值进行中心化**'
- en: '***5* Calculates the variance of the group [M,W,H,C]**'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 计算组 [M,W,H,C] 的方差**'
- en: '***6* Calculates the standard deviation over the group [M,W,H,C]**'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 计算组 [M,W,H,C] 的标准差**'
- en: '***7* Takes the average over feature maps and pixels [M,1,1,1]**'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 对特征图和像素 [M,1,1,1] 取平均值**'
- en: '***8* Transforms the scalar value to fit groups and pixels**'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 将标量值转换为适合组和像素**'
- en: '***9* Appends as a new feature map**'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 作为新的特征图添加**'
- en: 6.2.4\. Equalized learning rate
  id: totrans-384
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.4\. 均衡学习率
- en: '*Equalized learning rate* is one of those deep learning dark art techniques
    that is probably not clear to anyone. Although the researchers do provide a short
    explanation in the PGGAN paper, they avoided the topic in oral presentations,
    suggesting that this is probably just a hack that seems to work. Frequently in
    deep learning this is the case.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '*均衡学习率*是那些深度学习暗黑艺术技巧之一，可能对任何人来说都不太清楚。尽管研究人员在PGGAN论文中提供了一个简短的解释，但他们避免了口头报告中的这个话题，这表明这可能是似乎有效的一个技巧。在深度学习中，这种情况经常发生。'
- en: Furthermore, many nuances about equalized learning rate require a solid understanding
    of the implementation of RMSProp or Adam—which is the used optimizer—and also
    of weights initialization. So don’t worry if this does not make sense to you,
    because it probably does not really make sense to anyone.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，关于均衡学习率（equalized learning rate）的许多细微之处需要你对RMSProp或Adam（所使用的优化器）的实现以及权重初始化有一个扎实的理解。所以，如果你觉得这不太明白，请不要担心，因为这可能对任何人来说都不太明白。
- en: 'But if you’re curious, the explanation goes something as follows: we need to
    ensure that all the weights (*w*) are normalized (*w’*) to be within a certain
    range such that *w’ = w/c* by a constant *c* that is different for each layer,
    depending on the shape of the weight matrix. This also ensures that if any parameters
    need to take bigger steps to reach optimum—because they tend to vary more—these
    relevant parameters can do that.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你好奇，解释是这样的：我们需要确保所有权重（*w*）都通过一个常数*c*进行归一化（*w’*），使其在某个范围内，即*w’ = w/c*，这个常数*c*对于每一层都是不同的，取决于权重矩阵的形状。这也确保了如果任何参数需要采取更大的步骤以达到最优——因为它们倾向于变化更多——这些相关的参数可以做到这一点。
- en: Karras et al. use a simple standard normal initialization and then scale the
    weights per layer at runtime. Some of you may be thinking that Adam already does
    that—yes, Adam allows learning rates to be different for different parameters,
    but there’s a catch. Adam adjusts the backpropagated gradient by the estimated
    standard deviation of the parameter, which ensures that the scale of that parameter
    is independent of the update. Adam has different learning rates in different directions,
    but does not always take into account the *dynamic range*—how much a dimension
    or feature tends to vary over given mini-batches. As some point out, this seems
    to solve a similar problem as weights initialization.^([[3](#ch06fn03)])
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: Karras等人使用简单的标准正态初始化，然后在运行时按层缩放权重。有些人可能认为Adam已经做到了这一点——是的，Adam允许不同参数有不同的学习率，但有一个问题。Adam通过参数估计的标准差调整反向传播的梯度，这确保了该参数的尺度与更新无关。Adam在不同方向上有不同的学习率，但并不总是考虑到*动态范围*——即维度或特征在给定的小批量中变化的程度。正如有些人指出的，这似乎解决了与权重初始化相似的问题。[^([3](#ch06fn03))]
- en: ³
  id: totrans-389
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-390
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Progressive Growing of GANs.md,” by Alexander Jung, 2017, [http://mng.bz/5A4B](http://mng.bz/5A4B).
  id: totrans-391
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅Alexander Jung于2017年撰写的“Progressive Growing of GANs.md”，[http://mng.bz/5A4B](http://mng.bz/5A4B)。
- en: 'However, if this is not clear, do not worry; we highly recommend two excellent
    resources: Andrew Karpathy’s 2016 computer science lecture for notes about weights
    initialization,^([[4](#ch06fn04)]) and a Distill article for details on how Adam
    works.^([[5](#ch06fn05)]) The following listing shows the equalized learning rate.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果这还不清楚，请不要担心；我们强烈推荐两个优秀的资源：Andrew Karpathy于2016年的计算机科学讲座笔记，关于权重初始化[^([4](#ch06fn04))]，以及一篇Distill文章，详细介绍了Adam的工作原理[^([5](#ch06fn05))]。以下列表显示了均衡学习率。
- en: ⁴
  id: totrans-393
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-394
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See “Lecture 5: Training Neural Networks, Part I,” by Fei-Fei Li et al. 2016,
    [http://mng.bz/6wOo](http://mng.bz/6wOo).'
  id: totrans-395
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '请参阅Fei-Fei Li等人于2016年撰写的“Lecture 5: Training Neural Networks, Part I”，[http://mng.bz/6wOo](http://mng.bz/6wOo)。'
- en: ⁵
  id: totrans-396
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-397
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Why Momentum Really Works,” by Gabriel Goh, 2017, Distill, [https://distill.pub/2017/momentum/](https://distill.pub/2017/momentum/).
  id: totrans-398
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅Gabriel Goh于2017年撰写的Distill文章“Why Momentum Really Works”，[https://distill.pub/2017/momentum/](https://distill.pub/2017/momentum/)。
- en: Listing 6.3\. Equalized learning rate
  id: totrans-399
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.3\. 均衡学习率
- en: '[PRE3]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '***1* The default value is the product of all the shape dimensions minus the
    feature maps dimension; this gives us the number of incoming connections per neuron.**'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 默认值是所有形状维度乘积减去特征图维度；这给出了每个神经元输入连接的数量。**'
- en: '***2* This uses He’s initialization constant.**^([[6](#ch06fn06)])'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 这使用He的初始化常数。[^([6](#ch06fn06))]'
- en: ⁶
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet
    Classification,” by Kaiming He et al., [https://arxiv.org/pdf/1502.01852.pdf](https://arxiv.org/pdf/1502.01852.pdf).'
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '参见Kaiming He等人撰写的“Delving Deep into Rectifiers: Surpassing Human-Level Performance
    on ImageNet Classification”，[https://arxiv.org/pdf/1502.01852.pdf](https://arxiv.org/pdf/1502.01852.pdf).'
- en: '***3* Creates a constant out of the adjustment**'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3** 将调整创建为一个常数**'
- en: '***4** Gets values for weights and then uses broadcasting to apply the adjustment*'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4** 获取权重值并使用广播来应用调整*'
- en: If you are still confused, rest assured that these initialization tricks and
    these complicated learning rate adjustments are rarely a point of differentiation
    in either academia or industry. Also, just because restricting weight values between
    –1 and 1 seems to work somewhat better in most reruns here, that does not mean
    this trick will generalize to other setups. So let’s move to better-proven techniques.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仍然感到困惑，请放心，这些初始化技巧和这些复杂的学习率调整在学术界或工业界很少成为区分点。此外，尽管限制权重值在-1和1之间似乎在大多数重跑中效果较好，但这并不意味着这个技巧可以推广到其他设置。那么，让我们转向更经证明的技术。
- en: 6.2.5\. Pixel-wise feature normalization in the generator
  id: totrans-409
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.5\. 生成器中的像素级特征归一化
- en: 'Let’s begin with some motivation for why would we even want to normalize the
    features—stability of training. Empirically, the authors from NVIDIA have discovered
    that one of the early signs of divergent training was an explosion in feature
    magnitudes. A similar observation was made by the BigGAN authors in [chapter 12](../Text/kindle_split_023.xhtml#ch12).
    So Karras et al. introduced a technique to combat this. On a broader note, this
    is frequently how GAN training is done: we observe a particular problem with the
    training, so we introduce mechanisms to prevent that problem from happening.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从为什么要对特征进行归一化的动机开始——训练的稳定性。经验上，NVIDIA的作者发现，训练发散的早期迹象之一是特征幅度的爆炸。BigGAN作者在[第12章](../Text/kindle_split_023.xhtml#ch12)中也做出了类似的观察。因此，Karras等人引入了一种对抗这种问题的技术。更广泛地说，这就是GAN训练通常是如何进行的：我们观察到训练中存在特定问题，然后我们引入机制来防止该问题发生。
- en: Note that most networks are using some form of normalization. Typically, they
    use either batch normalization or a virtual version of this technique. [Table
    6.1](#ch06table01) presents an overview of normalization techniques used in the
    GANs presented in this book so far. You saw these in [chapter 4](../Text/kindle_split_013.xhtml#ch04)
    (DCGAN) and [chapter 5](../Text/kindle_split_015.xhtml#ch05)—where we touched
    on the rest of the GANs and gradient penalties (GPs). Unfortunately, in order
    for batch normalization and its virtual equivalent to work, we must have large
    mini-batches so that the individual samples average themselves out.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，大多数网络都使用某种形式的归一化。通常，它们使用批量归一化或这种技术的虚拟版本。[表6.1](#ch06table01)展示了本书中迄今为止所展示的GAN中使用的归一化技术的概述。您在[第4章](../Text/kindle_split_013.xhtml#ch04)（DCGAN）和[第5章](../Text/kindle_split_015.xhtml#ch05)中看到了这些——在那里我们提到了其他GAN和梯度惩罚（GPs）。不幸的是，为了使批量归一化和其虚拟等效版本能够工作，我们必须有大的迷你批量，以便单个样本的平均值相互抵消。
- en: Table 6.1\. Use of normalization techniques in GANs
  id: totrans-412
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表6.1\. GAN中使用归一化技术
- en: '| Method | Authors | G normalization | D normalization |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 作者 | G归一化 | D归一化 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| DCGAN | (Radford et al., 2015, [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434))
    | Batch | Batch |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| DCGAN | (Radford et al., 2015, [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434))
    | 批量 | 批量 |'
- en: '| Improved GAN | (Salimans et al., 2016, [https://arxiv.org/pdf/1606.03498.pdf](https://arxiv.org/pdf/1606.03498.pdf))
    | Virtual batch | Virtual batch |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 改进的GAN | (Salimans et al., 2016, [https://arxiv.org/pdf/1606.03498.pdf](https://arxiv.org/pdf/1606.03498.pdf))
    | 虚拟批量 | 虚拟批量 |'
- en: '| WGAN | (Arjovsky et al., 2017, [https://arxiv.org/pdf/1701.07875.pdf](https://arxiv.org/pdf/1701.07875.pdf))
    | — | Batch |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| WGAN | (Arjovsky et al., 2017, [https://arxiv.org/pdf/1701.07875.pdf](https://arxiv.org/pdf/1701.07875.pdf))
    | — | 批量 |'
- en: '| WGAN-GP | (Gulrajani et al., 2017, [http://arxiv.org/abs/1704.00028](http://arxiv.org/abs/1704.00028))
    | Batch | Layer norm |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| WGAN-GP | (Gulrajani et al., 2017, [http://arxiv.org/abs/1704.00028](http://arxiv.org/abs/1704.00028))
    | 批量 | 层归一化 |'
- en: Based on the fact that all these major implementations use normalization, it
    is clearly important, but why not just used standard batch normalization? Unfortunately,
    batch normalization is too memory intensive at our resolution. We have to come
    up with something that allows us to work with a few examples—that fit into our
    GPU memory with the two network graphs—but still works well. Now we understand
    where the need for pixel-wise feature normalization comes from and why we use
    it.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 基于所有这些主要实现都使用归一化的事实，它显然很重要，但为什么不直接使用标准的批量归一化呢？遗憾的是，批量归一化在我们的分辨率下太占用内存。我们必须想出一种方法，允许我们使用少量示例——这些示例可以适应我们的GPU内存，并且有两个网络图——但仍然效果良好。现在我们理解了像素级特征归一化的需求以及为什么我们使用它。
- en: If we jump into the algorithm, pixel normalization takes activation magnitude
    at each layer just before the input is fed into the next layer.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们深入算法，像素归一化会在输入被馈送到下一层之前，在每个层的激活幅度上操作。
- en: '[Figure 6.4](#ch06fig04) illustrates the process of pixel-wise feature normalization.
    The exact description of step 3 is shown in [equation 6.1](#ch06equ01).'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.4](#ch06fig04)说明了像素级特征归一化的过程。步骤3的确切描述显示在[方程6.1](#ch06equ01)中。'
- en: equation 6.1\.
  id: totrans-422
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程6.1.
- en: '![](../Images/06equ01.jpg)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06equ01.jpg)'
- en: Figure 6.4\. We map out all the points in an image (step 1) to a set of vectors
    (step 2), and then we normalize them so that they are all in the same range (typically
    between 0 and 1 in the high-dimensional space), which is step 3.
  id: totrans-424
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.4. 我们将图像中的所有点（步骤1）映射到一组向量（步骤2），然后对它们进行归一化，使它们都在相同的范围内（通常在多维空间中的0到1之间），这是步骤3。
- en: '![](../Images/06fig04_alt.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06fig04_alt.jpg)'
- en: '|  |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Pixel-wise feature normalization**'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '**像素级特征归一化**'
- en: '*For* each feature map *do*'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于*每个特征图*执行*'
- en: Take the pixel value of that feature map (*fm*) at a position (*x, y*).
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取该特征图(*fm*)在位置(*x, y*)的像素值。
- en: Construct a vector for each (*x*, *y*), where
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个(*x*, *y*)构建一个向量，其中
- en: v[0,0] = [(0,0) value for *fm*[1], (0,0) value for *fm*[2], ...., (0,0) value
    for *fm[n]*]
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: v[0,0] = [(0,0) value for *fm*[1], (0,0) value for *fm*[2], ..., (0,0) value
    for *fm*[n]*]
- en: v[0,1] = [(0,1) value for *fm*[1], (0,1) value for fm[2], ...., (0,1) value
    for *fm[n]*] ...
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: v[0,1] = [(0,1) value for *fm*[1], (0,1) value for *fm*[2], ..., (0,1) value
    for *fm*[n]*] ...
- en: v[n,n] = [(*n*,*n*) value for *fm*[1], (*n*,n) value for *fm*[2], ...., (*n*,n)
    value for *fm[n]*]
  id: totrans-433
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: v[n,n] = [(*n*,*n*) value for *fm*[1], (*n*,n) value for *fm*[2], ..., (*n*,n)
    value for *fm*[n]*]
- en: Normalize each vector *v*[i,i] as defined in step 2 to have a unit norm; call
    it *n*[i,i].
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将步骤2中定义的每个向量*v*[i,i]标准化为具有单位范数；称之为*n*[i,i]。
- en: Pass that in the original tensor shape to the next layer.
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其传递到原始张量形状的下一层。
- en: '*End for*'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '*结束for*'
- en: '|  |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: This formula *normalizes* (divides by the expression under the square root)
    each vector constructed in step 2 of [figure 6.4](#ch06fig04). This expression
    is just an average of each squared value for that particular (*x*, *y*) pixel.
    One thing that may surprise you is the addition of a small noise term (ϵ). This
    is simply a way to ensure that we are not dividing by zero. The whole procedure
    is explained in greater detail in the 2012 paper “ImageNet Classification with
    Deep Convolutional Neural Networks,” by Alex Krizhevsky et al. ([http://mng.bz/om4d](http://mng.bz/om4d)).
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 该公式将[图6.4](#ch06fig04)步骤2中构建的每个向量进行*标准化*（除以平方根下的表达式）。这个表达式只是该特定(*x*, *y*)像素每个平方值的平均值。可能让你感到惊讶的是，添加了一个小的噪声项(ϵ)。这仅仅是一种确保我们不会除以零的方法。整个过程在2012年的论文“ImageNet
    Classification with Deep Convolutional Neural Networks”中得到了更详细的解释，作者为Alex Krizhevsky等人。（[http://mng.bz/om4d](http://mng.bz/om4d)）
- en: The last thing to note is that this term is applied only to the Generator, as
    the explosion in the activation magnitudes leads to an arms race only if *both*
    networks participate. The following listing shows the code.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要注意的是，这个术语仅应用于生成器，因为激活幅度的爆炸只会导致如果*两个*网络都参与的话，才会是一场军备竞赛。下面的列表显示了代码。
- en: Listing 6.4\. Pixel-wise feature normalization
  id: totrans-440
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.4. 像素级特征归一化
- en: '[PRE4]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 6.3\. Summary of key innovations
  id: totrans-442
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3. 关键创新总结
- en: We have gone through four clever ideas on how to improve GAN training; however,
    without grounding them in their effects on the training, it may be difficult to
    isolate those effects. Thankfully, the paper’s authors provide a helpful table
    to help us understand just that; see [figure 6.5](#ch06fig05).
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了四种如何改进GAN训练的巧妙想法；然而，如果没有基于它们对训练的影响，可能很难隔离这些影响。幸运的是，论文的作者提供了一个有用的表格，帮助我们理解这一点；参见[图6.5](#ch06fig05)。
- en: Figure 6.5\. Contributions of various techniques to score improvements. We can
    see that the introduction of equalized learning rate makes a big difference, and
    pixel-wise normalization adds to that, though what the authors do not tell us
    is how effective this technique would be if we had only pixel normalization and
    did not introduce equalized learning rate. We include this table only as an illustration
    of the rough magnitude of improvement we can expect from these changes—which is
    an interesting lesson on its own—but more detailed discussion follows.
  id: totrans-444
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.5\. 各种技术对得分改进的贡献。我们可以看到，引入均衡学习率产生了重大影响，像素归一化也增加了这一点，尽管作者没有告诉我们，如果我们只有像素归一化而没有引入均衡学习率，这种技术将有多有效。我们只包括这个表格，以说明我们可以期望从这些变化中获得的粗略改进程度——这本身就是一个有趣的教训——但更详细的讨论将在后面进行。
- en: '![](../Images/06fig05_alt.jpg)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06fig05_alt.jpg)'
- en: The PGGAN paper’s authors are using *sliced Wasserstein distance (SWD)*, where
    smaller is better. Recall from [chapter 5](../Text/kindle_split_015.xhtml#ch05)
    that a smaller Wasserstein—aka *earth mover’s*—distance means better results as
    quantified by the amount of probability mass one has to move to make the two distributions
    similar. The SWD means that patches of both the real data and the generated samples
    minimize this distance. The nuances of this technique are explained in the paper,
    but as the authors said during their presentation at ICLR, better measures—such
    as the Fréchet inception distance (FID)—now exist. We covered the FID in greater
    depth in [chapter 5](../Text/kindle_split_015.xhtml#ch05).
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: PGGAN论文的作者正在使用*sliced Wasserstein距离 (SWD)*，越小越好。回想一下[第5章](../Text/kindle_split_015.xhtml#ch05)，较小的Wasserstein距离（也称为*地球迁移者*距离）意味着更好的结果，这是通过必须移动多少概率质量来使两个分布相似来衡量的。SWD意味着真实数据和生成样本的块都最小化这个距离。这项技术的细微差别在论文中有解释，但正如作者在ICLR的演示中所说，现在存在更好的度量标准——例如Fréchet
    inception距离 (FID)。我们在[第5章](../Text/kindle_split_015.xhtml#ch05)中更深入地介绍了FID。
- en: One key takeaway from this table is that a mini-batch does not work well, because,
    at a megapixel resolution, we do not have enough virtual RAM to load many images
    into the GPU memory. We have to use a smaller mini-batch—which may, overall, perform
    worse—and we have to reduce the mini-batch sizes further, making our training
    difficult.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 从这张表中可以得出的一个关键结论是，小批量并不奏效，因为在兆像素分辨率下，我们没有足够的虚拟RAM来加载许多图像到GPU内存中。我们必须使用更小的小批量——这可能会在总体上表现得更差——并且我们必须进一步减少小批量的大小，这使得我们的训练变得困难。
- en: 6.4\. TensorFlow Hub and hands-on
  id: totrans-448
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. TensorFlow Hub和动手实践
- en: Google has recently announced that as part of TensorFlow Extended and the general
    move toward implementing best practices from software engineering into the machine
    learning world, Google has created a central model and code repository called
    *TensorFlow Hub*, or *TFHub*. Working with TFHub is almost embarrassingly easy,
    especially with the models that Google has put there.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌最近宣布，作为TensorFlow Extended和将软件工程的最佳实践应用于机器学习领域的一般性举措的一部分，谷歌创建了一个中央模型和代码仓库，称为*TensorFlow
    Hub*，或*TFHub*。使用TFHub几乎可以说是易如反掌，尤其是与谷歌放置在那里的模型一起使用。
- en: After importing the hub module and calling the right URL, TensorFlow downloads
    and imports the model all by itself, and you can start. These models are well-documented
    at the *same* URL that we use to download the model; just put them into your web
    browser. In fact, to get a pretrained Progressive GAN, all you need to type is
    an import statement and one line of code. That’s it!
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入hub模块并调用正确的URL后，TensorFlow会自动下载并导入模型，然后你可以开始使用。这些模型在用于下载模型的相同URL上有很好的文档说明；只需将它们放入你的网络浏览器中。实际上，要获取预训练的Progressive
    GAN，你只需要输入一条导入语句和一行代码。就是这样！
- en: The following listing shows a complete example of code that should by itself
    generate a face—based on the random seed that you specify in `latent_vector`.^([[7](#ch06fn07)])
    [Figure 6.6](#ch06fig06) displays the output.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表显示了生成面孔的完整代码示例——基于你在`latent_vector`中指定的随机种子。[图6.6](#ch06fig06)显示了输出。
- en: ⁷
  id: totrans-452
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-453
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This example was generated with the use of TFHub and is based on the example
    Colab provided at [http://mng.bz/nvEa](http://mng.bz/nvEa).
  id: totrans-454
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此示例是使用TFHub生成的，基于[http://mng.bz/nvEa](http://mng.bz/nvEa)提供的Colab示例。
- en: Listing 6.5\. Getting started with TFHub
  id: totrans-455
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.5\. 使用TFHub入门
- en: '[PRE5]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '***1* Imports the Progressive GAN from TFHub**'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 从TFHub导入Progressive GAN**'
- en: '***2* Latent dimension that gets sampled at runtime**'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 在运行时采样的潜在维度**'
- en: '***3* Changes the seed to get different faces**'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 改变种子以获取不同的面孔**'
- en: '***4* Uses the module to generate images from the latent space. Implementation
    details are online.**'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 使用模块从潜在空间生成图像。实现细节见在线资料。**'
- en: '***5* Runs the TensorFlow session and gets back the image in shape (1,128,128,3)**'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 运行TensorFlow会话并返回形状为（1,128,128,3）的图像**'
- en: 'Figure 6.6\. Output of [listing 6.5](#ch06ex05). Try changing the seed in the
    `latent_vector` definition to get different outputs. A word of warning: even though
    this random seed argument should consistently define the output we are meant to
    get, we have found that on reruns we sometimes get different results, depending
    on the version of TensorFlow. This image is obtained using 1.9.0-rc1.'
  id: totrans-462
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.6\. [代码清单6.5](#ch06ex05)的输出。尝试更改`latent_vector`定义中的种子以获取不同的输出。提醒一下：尽管这个随机种子参数应该一致地定义我们期望得到的输出，但我们发现，在重新运行时，有时会得到不同的结果，这取决于TensorFlow的版本。此图像使用1.9.0-rc1版本获得。
- en: '![](../Images/06fig06.jpg)'
  id: totrans-463
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06fig06.jpg)'
- en: Hopefully, this should be enough to get you started with Progressive GANs! Feel
    free to play around with the code and extend it. It should be noted here that
    the TFHub version of the Progressive GAN is not using the full 1024 × 1024, but
    rather just 128 × 128\. This is probably because running the full version used
    to be computationally expensive, and the model size can grow huge quickly in the
    domain of computer vision problems.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这足以让您开始使用渐进式GAN！请随意尝试修改代码并扩展它。需要注意的是，TFHub版本的渐进式GAN并没有使用完整的1024 × 1024，而是仅使用128
    × 128。这可能是由于运行完整版本曾经计算成本高昂，并且在这个计算机视觉问题领域中，模型大小可以迅速变得非常大。
- en: 6.5\. Practical applications
  id: totrans-465
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5\. 实际应用
- en: Understandably, people are curious about the practical applications and ability
    to generalize Progressive GANs. One great example we’ll present is from our colleagues
    at Kheiron Medical Technologies, based in London, England. Recently, they released
    a paper that is a great testament to both the generalizability and practical applications
    of the PGGAN.^([[8](#ch06fn08)])
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 可以理解的是，人们对渐进式GAN的实际应用和泛化能力感到好奇。我们将展示的一个很好的例子来自位于英国伦敦的Kheiron Medical Technologies的同事们。最近，他们发布了一篇论文，这篇论文是对PGGAN的泛化能力和实际应用的极好证明.^([[8](#ch06fn08)])
- en: ⁸
  id: totrans-467
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸
- en: ''
  id: totrans-468
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “High-Resolution Mammogram Synthesis Using Progressive Generative Adversarial
    Networks,” by Dimitrios Korkinof et al., 2018, [https://arxiv.org/pdf/1807.03401.pdf](https://arxiv.org/pdf/1807.03401.pdf).
  id: totrans-469
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅Dimitrios Korkinof等人撰写的“使用渐进式生成对抗网络进行高分辨率乳腺钼靶合成”，2018年，[https://arxiv.org/pdf/1807.03401.pdf](https://arxiv.org/pdf/1807.03401.pdf)。
- en: 'Using a large dataset of medical mammograms,^([[9](#ch06fn09)]) these researchers
    managed to generate realistic 1280 × 1024 synthetic images of full-field digital
    mammography (FFDM), as shown in [figure 6.7](#ch06fig07). This is a remarkable
    achievement on two levels:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大量医学乳腺钼靶数据集，^([[9](#ch06fn09)]) 这些研究人员成功生成了1280 × 1024的全场数字化乳腺钼靶（FFDM）的逼真合成图像，如图6.7所示。这是在两个层面上的一项了不起的成就：
- en: ⁹
  id: totrans-471
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹
- en: ''
  id: totrans-472
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: X-ray scans for the purposes of breast cancer screening.
  id: totrans-473
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 用于乳腺癌筛查的X射线扫描。
- en: It shows the generalizability of this technique. Think about how different images
    of mammograms are from the images of human faces—especially structurally. The
    bar for whether a tissue structure makes sense is incredibly high, and yet their
    network manages to produce samples at the highest resolution to date that frequently
    fool medical professionals.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它展示了这项技术的泛化能力。想想钼靶图像与人类面部图像的不同——特别是在结构上。组织结构的合理性标准非常高，然而他们的网络设法产生了迄今为止最高分辨率的样本，这些样本经常能欺骗医疗专业人员。
- en: It shows how these techniques can be applied to many fields and uses. For example,
    we can use this new dataset in a semi-supervised way, as you will discover in
    the next chapter. Or the synthetic dataset can be open sourced for medical research
    with arguably fewer worries from General Data Protection Regulation (GDPR) or
    other legal repercussions, as these do not belong to any one person.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它展示了这些技术可以应用于许多领域和用途。例如，我们可以在下一章中向您展示如何以半监督的方式使用这个新数据集。或者，合成数据集可以开源用于医学研究，从一般数据保护条例（GDPR）或其他法律后果的角度来看，可能担忧较少，因为这些数据不属于任何个人。
- en: Figure 6.7\. Progressive growing of FFDM. This is a great figure because it
    not only shows the progressively increasing resolution on these mammograms (e),
    but also some training statistics (a)–(d) to show you that training these GANs
    is messy for everyone, not just you.
  id: totrans-476
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.7\. FFDM的渐进式增长。这是一张很棒的图，因为它不仅展示了这些钼靶图像上分辨率逐渐提高的情况（e），还展示了部分训练统计信息（a）至（d），以表明训练这些GAN对于每个人来说都是混乱的，而不仅仅是您。
- en: '![](../Images/06fig07_alt.jpg)'
  id: totrans-477
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06fig07_alt.jpg)'
- en: '[Figure 6.8](#ch06fig08) shows how realistic these mammograms can look. These
    have been randomly sampled (so no cherry-picking) and then compared to one of
    the closest images in the dataset.'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.8](#ch06fig08)展示了这些乳腺X光片可以有多逼真。这些是随机抽取的（因此没有挑选），然后与数据集中最接近的一张图片进行了比较。'
- en: Figure 6.8\. In comparing the real and the generated datasets, the data looks
    pretty realistic and generally close to an example in the training set. In their
    subsequent work, MammoGAN, Kheiron has shown that these images fool trained and
    certified radiologists.^([[11](#ch06fn11)]) That”s generally a good sign, especially
    at this high resolution. Of course, in principle, we would love to have a statistical
    way of measuring the quality of the generation. But as we know from [chapter 5](../Text/kindle_split_015.xhtml#ch05),
    this is hard enough to do with standard images, let alone for any arbitrary GAN.
  id: totrans-479
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.8。在比较真实和生成的数据集时，数据看起来相当逼真，通常接近训练集中的示例。在其后续工作中，MammoGAN，Kheiron已经表明这些图像可以欺骗经过培训和认证的放射科医生。^([[11](#ch06fn11)])
    这通常是一个好迹象，尤其是在这种高分辨率下。当然，从原则上讲，我们希望有一种统计方法来衡量生成的质量。但正如我们在[第5章](../Text/kindle_split_015.xhtml#ch05)中了解到的，这已经足够困难，对于任何任意的GAN来说更是如此。
- en: ^(11)
  id: totrans-480
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(11)
- en: ''
  id: totrans-481
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See “MammoGAN: High-Resolution Synthesis of Realistic Mammograms,” by Dimitrios
    Korkinof et al., 2019, [https://openreview.net/pdf?id=SJeichaN5E](https://openreview.net/pdf?id=SJeichaN5E).'
  id: totrans-482
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '查看“MammoGAN: High-Resolution Synthesis of Realistic Mammograms”，作者Dimitrios
    Korkinof等，2019年，[https://openreview.net/pdf?id=SJeichaN5E](https://openreview.net/pdf?id=SJeichaN5E).'
- en: '![](../Images/06fig08_alt.jpg)'
  id: totrans-483
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06fig08_alt.jpg)'
- en: GANs may be used for many applications, not just fighting breast cancer or generating
    human faces, but also in 62 other medical GAN applications published through the
    end of July 2018.^([[10](#ch06fn10)]) We encourage you to look at them—but of
    course, not all of them use PGGANs. Generally, GANs are allowing massive leaps
    in many research fields, but are frequently applied nonintuitively. We hope to
    make these more accessible so that they can be used by more researchers. Make
    GANs, not war!
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: GANs可以用于许多应用，而不仅仅是与乳腺癌作斗争或生成人脸，还包括截至2018年7月底发布的62个其他医学GAN应用。^([[10](#ch06fn10)])
    我们鼓励您查看它们——但当然，并非所有这些都使用PGGANs。通常，GANs在许多研究领域都带来了巨大的飞跃，但它们通常被非直观地应用。我们希望使这些技术更容易获得，以便更多的研究人员可以使用。制造GANs，而不是战争！
- en: ^(10)
  id: totrans-485
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(10)
- en: ''
  id: totrans-486
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “GANs for Medical Image Analysis,” by Salome Kazeminia et al., 2018, [https://arxiv.org/pdf/1809.06222.pdf](https://arxiv.org/pdf/1809.06222.pdf).
  id: totrans-487
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 查看“GANs for Medical Image Analysis”，作者Salome Kazeminia等，2018年，[https://arxiv.org/pdf/1809.06222.pdf](https://arxiv.org/pdf/1809.06222.pdf).
- en: 'All of the techniques we presented in this chapter represent a general class
    of solving GAN problems—with a progressively more complex model. We expect this
    paradigm to pick up within GANs. The same is true for TensorFlow Hub: it is to
    TensorFlow what PyPI/Conda is to Python. Most Python programmers use them every
    week!'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们提出的所有技术代表了解决GAN问题的一般类别——模型越来越复杂。我们预计这种范式将在GAN中得到应用。对于TensorFlow Hub也是如此：它对TensorFlow的作用就像PyPI/Conda对Python的作用一样。大多数Python程序员每周都会使用它们！
- en: We hope that this new Progressive GAN technique opened your eyes to what GANs
    can do and why people are so excited about this paper. And hopefully not just
    for the cat meme vector that PGGAN can produce.^([[12](#ch06fn12)]) The next chapter
    will give you the tools so that you can start contributing to research yourself.
    See you then!
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这种新的渐进式GAN技术让您对GAN能做什么以及为什么人们对这篇论文如此兴奋有了新的认识。并且希望不仅仅是因为PGGAN可以生成的猫膜向量。^([[12](#ch06fn12)])
    下一章将为您提供工具，让您可以开始自己为研究做出贡献。到时候见！
- en: ^(12)
  id: totrans-490
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(12)
- en: ''
  id: totrans-491
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Gene Kogan’s Twitter image, 2018, [https://twitter.com/genekogan/status/1019943905318572033](https://twitter.com/genekogan/status/1019943905318572033).
  id: totrans-492
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 查看Gene Kogan的Twitter图片，2018年，[https://twitter.com/genekogan/status/1019943905318572033](https://twitter.com/genekogan/status/1019943905318572033).
- en: Summary
  id: totrans-493
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: We can achieve 1-megapixel synthetic images thanks to the state-of-the-art PGGAN
    technique.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们得益于最先进的PGGAN技术，可以生成1兆像素的合成图像。
- en: 'This technique has four key training innovations:'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种技术有四个关键的训练创新：
- en: Progressive growing and smoothing in higher-resolution layers
  id: totrans-496
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高分辨率层中的渐进式增长和平滑
- en: Mini-batch standard deviation to enforce variation in the generated samples
  id: totrans-497
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小批量标准差以强制生成样本中的变化
- en: Equalized learning rate that ensures we can take learning steps of appropriate
    sizes in each direction
  id: totrans-498
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平衡的学习率确保我们可以在每个方向上采取适当大小的学习步骤
- en: Pixel-wise vector normalization that ensures that the Generator and the Discriminator
    do not spiral out of control in an arms race
  id: totrans-499
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像素级向量归一化，确保生成器和判别器在军备竞赛中不会失控
- en: You followed a hands-on tutorial using the newly released TensorFlow Hub and
    got to use their downsampled version of the Progressive GAN to generate images!
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你跟随了一个使用新发布的TensorFlow Hub的手动教程，并有机会使用其渐进式GAN的下采样版本来生成图像！
- en: You learned about how GANs are helping to fight cancer.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你了解了GANs是如何帮助对抗癌症的。
- en: Chapter 7\. Semi-Supervised GAN
  id: totrans-502
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7章。半监督GAN
- en: '*This chapter covers*'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: The booming field of innovations based on the original GAN model
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于原始GAN模型的热门创新领域
- en: Semi-supervised learning and its immense practical importance
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半监督学习和其巨大的实际重要性
- en: Semi-Supervised GANs (SGANs)
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半监督GANs（SGANs）
- en: Implementation of an SGAN model
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SGAN模型的实现
- en: 'Congratulations—you have made it more than halfway through this book. By now,
    you not only have learned what GANs are and how they function, but also had an
    opportunity to implement two of the most canonical implementations: the original
    GAN that started it all and the DCGAN that laid the foundation for the bulk of
    the advanced GAN variants, including the Progressive GAN introduced in the previous
    chapter.'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你——你已经完成了这本书的一半以上。到现在为止，你不仅已经学会了什么是生成对抗网络（GANs）以及它们是如何工作的，还有机会实现了两种最经典的实现：最初启动一切的原始GAN和为大量高级GAN变体奠定基础的DCGAN，包括上一章中介绍的渐进式GAN。
- en: However, as with many fields, just when you think you are beginning to get a
    real hang of it, you uncover that the domain is much larger and more complex than
    initially thought. What might have seemed like a thorough understanding turns
    out to be no more than the tip of the iceberg.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，就像许多领域一样，当你认为你开始真正掌握它时，你会发现自己所了解的领域比最初想象的要大得多、复杂得多。可能看似彻底的理解实际上只是冰山一角。
- en: GANs are no exception. Since their invention, they have remained an active area
    of research with countless variations added every year. An unofficial list—aptly
    named “The GAN Zoo” ([https://github.com/hindupuravinash/the-gan-zoo](https://github.com/hindupuravinash/the-gan-zoo))—which
    seeks to track all named GAN variants (GAN implementations with distinct names
    coined by the researchers who authored them) has grown to well over 300 at the
    time of this writing. However, judging from the fact that the original GAN paper
    has been cited more than 9,000 times to date (July 2019) and ranks among the most
    cited research papers in recent years in all of deep learning, the true number
    of GAN variations invented by the research community is likely even higher.^([[1](#ch07fn01)])
    See [figure 7.1](#ch07fig01).
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: GANs也不例外。自从它们被发明以来，它们一直是一个活跃的研究领域，每年都会添加无数的变化。一个非官方的列表——恰当地命名为“GAN动物园”（[https://github.com/hindupuravinash/the-gan-zoo](https://github.com/hindupuravinash/the-gan-zoo)）——旨在追踪所有命名的GAN变体（由撰写它们的作者命名的具有不同名称的GAN实现）在撰写本文时已经增长到超过300种。然而，根据原始GAN论文至今已被引用超过9,000次（截至2019年7月）并且在过去几年中在深度学习领域排名最被引用的研究论文之一的事实，研究社区发明的GAN变体数量可能更高。[^([[1](#ch07fn01)])
    见[图7.1](#ch07fig01)。
- en: ¹
  id: totrans-511
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-512
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'According to a tracker from the Microsoft Academic (MA) search engine: [http://mng.bz/qXXJ](http://mng.bz/qXXJ).
    See also “Top 20 Research Papers on Machine Learning and Deep Learning,” by Thuy
    T. Pham, 2017, [http://mng.bz/E1eq](http://mng.bz/E1eq).'
  id: totrans-513
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 根据Microsoft Academic（MA）搜索引擎的追踪器：[http://mng.bz/qXXJ](http://mng.bz/qXXJ)。另见“机器学习和深度学习的前20篇研究论文”，作者Thuy
    T. Pham，2017，[http://mng.bz/E1eq](http://mng.bz/E1eq)。
- en: Figure 7.1\. This graph approximates the monthly cumulative count of unique
    GAN implementations published by the research community, starting from GAN’s invention
    in 2014 until the first few months of 2018\. As the chart makes clear, the field
    of generative adversarial learning has been growing exponentially since its inception,
    and there is no end in sight to this growth in interest and popularity.
  id: totrans-514
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.1。此图近似了研究社区从2014年GAN的发明开始，直到2018年初几个月每月累积的独特GAN实现的数量。正如图表所清楚显示的，生成对抗学习领域自其诞生以来一直在呈指数增长，这种兴趣和受欢迎程度的增长似乎没有尽头。
- en: '![](../Images/07fig01_alt.jpg)'
  id: totrans-515
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07fig01_alt.jpg)'
- en: '(Source: “The GAN Zoo,” by Avinash Hindupur, 2017, [https://github.com/hindupuravinash/the-gan-zoo](https://github.com/hindupuravinash/the-gan-zoo).)'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：“GAN动物园”，作者Avinash Hindupur，2017，[https://github.com/hindupuravinash/the-gan-zoo](https://github.com/hindupuravinash/the-gan-zoo)。）
- en: This, however, is no reason to despair. Although it is impossible to cover all
    these GAN variants in this book, or any book for that matter, we can cover a few
    of the key innovations that will give you a good idea of what’s out there as well
    as the unique contributions each of these variations provides to the field of
    generative adversarial learning.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不是绝望的理由。尽管不可能在这本书或任何书中涵盖所有这些GAN变体，但我们可以介绍一些关键创新，这将给你一个很好的了解，以及这些变化对生成对抗学习领域的独特贡献。
- en: It is worth noting that not all of these named variants diverge drastically
    from the original GAN. Indeed, many of them are at a high level quite similar
    to the original model, such as the DCGAN in [chapter 4](../Text/kindle_split_013.xhtml#ch04).
    Even the many complex innovations such as the Wasserstein GAN (discussed in [chapter
    5](../Text/kindle_split_015.xhtml#ch05)) focus primarily on improving the performance
    and stability of the original GAN model or one similar to it.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，并非所有这些命名的变体都与原始GAN有显著差异。事实上，其中许多在高级别上与原始模型非常相似，例如第4章中的DCGAN。即使是许多复杂的创新，如Wasserstein
    GAN（在第5章中讨论），也主要关注提高原始GAN模型或类似模型的性能和稳定性。
- en: 'In this and the following two chapters, we will focus on GAN variants that
    diverge from the original GAN not only in the architecture and underlying mathematics
    of their model implementations but also in their motivations and objectives. In
    particular, we will cover the following three GAN models:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章以及接下来的两章中，我们将重点关注那些不仅在模型实现的架构和底层数学上与原始GAN不同，而且在动机和目标上也有所不同的GAN变体。特别是，我们将涵盖以下三种GAN模型：
- en: Semi-Supervised GAN (this chapter)
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半监督GAN（本章）
- en: Conditional GAN ([chapter 8](../Text/kindle_split_018.xhtml#ch08))
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件GAN ([第8章](../Text/kindle_split_018.xhtml#ch08))
- en: CycleGAN ([chapter 9](../Text/kindle_split_019.xhtml#ch09))
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CycleGAN ([第9章](../Text/kindle_split_019.xhtml#ch09))
- en: For each of these GAN variants, you will learn about their objectives and what
    motivated them, their model architectures, and how their networks train and work.
    These topics will be covered both conceptually and through concrete examples.
    We will also provide tutorials with full working implementations of each of these
    models so that you can experience them firsthand.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些GAN变体中的每一个，你将了解它们的目标和动机，它们的模型架构，以及它们的网络如何训练和工作。这些主题将通过概念和具体示例进行讲解。我们还将提供教程，包含每个模型的完整工作实现，以便你可以亲身体验。
- en: So, without further ado, let’s dive in!
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，无需多言，让我们深入探讨吧！
- en: 7.1\. Introducing the Semi-Supervised GAN
  id: totrans-525
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1. 半监督GAN的介绍
- en: '*Semi-supervised learning* is one of the most promising areas of practical
    application of GANs. Unlike supervised learning, in which we need a label for
    every example in our dataset, and unsupervised learning, in which no labels are
    used, semi-supervised learning has a class label for only a small subset of the
    training dataset. By internalizing hidden structures in the data, semi-supervised
    learning strives to generalize from the small subset of labeled data points to
    effectively classify new, previously unseen examples. Importantly, for semi-supervised
    learning to work, the labeled and unlabeled data must come from the same underlying
    distribution.'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '*半监督学习* 是GAN在实际应用中最有前景的领域之一。与需要为数据集中的每个示例提供标签的监督学习不同，以及不使用标签的无监督学习，半监督学习只为训练数据集的一个小子集提供类别标签。通过内化数据中的隐藏结构，半监督学习力求从标记数据点的较小子集中泛化，从而有效地分类新的、以前未见过的示例。重要的是，为了使半监督学习有效，标记和无标记数据必须来自相同的潜在分布。'
- en: The lack of labeled datasets is one of the main bottlenecks in machine learning
    research and practical applications. Although unlabeled data is abundant (the
    internet is a virtually limitless source of unlabeled images, videos, and text),
    assigning class labels to them is often prohibitively expensive, impractical,
    and time-consuming. It took two and a half years to hand-annotate the original
    3.2 million images in the ImageNet—a database of labeled images that helped enable
    many of the advances in image processing and computer vision in the last decade.^([[2](#ch07fn02)])
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏标记数据集是机器学习研究和实际应用中的主要瓶颈之一。尽管无标记数据很丰富（互联网是一个几乎无限的、无标记图像、视频和文本的来源），但为它们分配类别标签通常成本高昂、不切实际且耗时。手工标注ImageNet原始的320万张图像花了两年半时间——这是一个标记图像数据库，帮助推动了过去十年中图像处理和计算机视觉的许多进步.^([[2](#ch07fn02)])
- en: ²
  id: totrans-528
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-529
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “The Data That Transformed AI Research—and Possibly the World,” by Dave
    Gershgorn, 2017, [http://mng.bz/DNVy](http://mng.bz/DNVy).
  id: totrans-530
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅Dave Gershgorn于2017年发表的《数据如何改变AI研究——以及可能改变世界》，[http://mng.bz/DNVy](http://mng.bz/DNVy)。
- en: Andrew Ng, a deep learning pioneer, Stanford professor, and former chief scientist
    of the Chinese internet giant Baidu, identified the enormous amounts of labeled
    data needed for training as the Achilles’ heel of supervised learning, which is
    used for the vast majority of today’s AI applications in industry.^([[3](#ch07fn03)])
    One of the industries that suffers most from a lack of large labeled datasets
    is medicine, for which obtaining data (for example, outcomes from clinical trials)
    often requires great effort and expenditure, not to mention the even more important
    issues of ethics and privacy.^([[4](#ch07fn04)]) Accordingly, improving the ability
    of algorithms to learn from ever-smaller quantities of labeled examples has immense
    practical importance.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习先驱、斯坦福大学教授、百度前首席科学家安德鲁·吴（Andrew Ng）将大量标注数据对于训练的重要性视为监督学习的阿基里斯之踵，而监督学习被广泛应用于当今工业界的大多数AI应用。[^([[3](#ch07fn03))]
    其中，受大型标注数据集缺乏影响最严重的行业之一是医药行业，获取数据（例如，临床试验的结果）通常需要巨大的努力和支出，更不用说伦理和隐私等更为重要的问题。[^([[4](#ch07fn04))]
    因此，提高算法从越来越少的标注示例中学习的能力具有极大的实际意义。
- en: ³
  id: totrans-532
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-533
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “What Artificial Intelligence Can and Can’t Do Right Now,” by Andrew Ng,
    2016, [http://mng.bz/lopj](http://mng.bz/lopj).
  id: totrans-534
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅安德鲁·吴（Andrew Ng）于2016年发表的《现在人工智能能做什么（以及不能做什么）》，[http://mng.bz/lopj](http://mng.bz/lopj)。
- en: ⁴
  id: totrans-535
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-536
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “What AI Can and Can’t Do (Yet) for Your Business,” by Michael Chui et al.,
    2018, [http://mng.bz/BYDv](http://mng.bz/BYDv).
  id: totrans-537
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅Michael Chui等人于2018年发表的《AI能为您的业务做什么（以及现在不能做什么）》，[http://mng.bz/BYDv](http://mng.bz/BYDv)。
- en: Interestingly, semi-supervised learning may also be one of the closest machine
    learning analogs to the way humans learn. When schoolchildren learn to read and
    write, the teacher does not have to take them on a road trip to see tens of thousands
    of examples of letters and numbers, ask them to identify these symbols, and correct
    them as needed—similarly to the way a supervised learning algorithm would operate.
    Instead, a single set of examples is all that is needed for children to learn
    letters and numerals and then be able to recognize them regardless of font, size,
    angle, lighting conditions, and many other factors. Semi-supervised learning aims
    to teach machines in a similarly efficient manner.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，半监督学习也可能是与人类学习方式最接近的机器学习类比。当学童学习阅读和写作时，老师不必带他们去旅行看成千上万的字母和数字示例，要求他们识别这些符号，并根据需要纠正他们——这与监督学习算法的操作方式相似。相反，只需要一组示例，孩子们就能学习字母和数字，然后能够识别它们，无论字体、大小、角度、光照条件以及许多其他因素。半监督学习的目标是以类似高效的方式教会机器。
- en: Serving as a source of additional information that can be used for training,
    generative models proved useful in improving the accuracy of semi-supervised models.
    Unsurprisingly, GANs have proven the most promising. In 2016, Tim Salimans, Ian
    Goodfellow, and their colleagues at OpenAI achieved almost 94% accuracy on the
    Street View House Numbers (SVHN) benchmark dataset using only 2,000 labeled examples.^([[5](#ch07fn05)])
    For comparison, the best fully supervised algorithm at the time that used labels
    for all 73,257 images in the SVHN training set achieved an accuracy of around
    98.40%.^([[6](#ch07fn06)]) In other words, the Semi-Supervised GAN achieved overall
    accuracy remarkably close to the fully supervised benchmark, while using fewer
    than 3% of the labels for training.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 作为训练的额外信息来源，生成模型在提高半监督模型准确性方面证明是有用的。不出所料，GANs证明是最有希望的。2016年，Tim Salimans、Ian
    Goodfellow及其在OpenAI的同事们仅使用2,000个标注示例在Street View House Numbers (SVHN)基准数据集上实现了近94%的准确率。[^([[5](#ch07fn05))]
    作为比较，当时使用SVHN训练集中所有73,257个图像的标签的最佳全监督算法的准确率约为98.40%。[^([[6](#ch07fn06))] 换句话说，半监督GAN在整体准确率上与全监督基准非常接近，而训练时使用的标签不到3%。
- en: ⁵
  id: totrans-540
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-541
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Improved Techniques for Training GANs,” by Ian Goodfellow et al., 2016,
    [https://arxiv.org/abs/1606.03498](https://arxiv.org/abs/1606.03498).
  id: totrans-542
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅Ian Goodfellow等人于2016年发表的《训练GANs的改进技术》，[https://arxiv.org/abs/1606.03498](https://arxiv.org/abs/1606.03498)。
- en: ⁶
  id: totrans-543
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-544
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Densely Connected Convolutional Networks,” by Gao Huang et al., 2016, [https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993).
  id: totrans-545
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅高黄等人于2016年发表的《密集连接卷积网络》，[https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)。
- en: Let’s find out how Salimans and his colleagues accomplished so much from so
    little.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看Salimans和他的同事们是如何从如此之少中取得如此多的成就。
- en: 7.1.1\. What is a Semi-Supervised GAN?
  id: totrans-547
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.1\. 什么是半监督生成对抗网络？
- en: '*Semi-Supervised GAN (SGAN)* is a Generative Adversarial Network whose Discriminator
    is a multiclass classifier. Instead of distinguishing between only two classes
    (*real* and *fake*), it learns to distinguish between *N* + 1 classes, where *N*
    is the number of classes in the training dataset, with one added for the fake
    examples produced by the Generator.'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '*半监督生成对抗网络（SGAN）* 是一种生成对抗网络，其判别器是一个多类分类器。它不是仅仅区分两类（*真实*和*伪造*），而是学习区分 *N* +
    1 类，其中 *N* 是训练数据集中类的数量，额外增加一类用于由生成器产生的伪造示例。'
- en: For example, the MNIST dataset of handwritten digits has 10 labels (one label
    for each numeral, 0 to 9), so the SGAN Discriminator trained on this dataset would
    predict between 10 + 1 = 11 classes. In our implementation, the output of the
    SGAN Discriminator will be represented as a vector of 10 class probabilities (that
    sum up to 1.0) plus another probability that represents whether the image is real
    or fake.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，手写数字的MNIST数据集有10个标签（每个数字一个标签，0到9），因此在这个数据集上训练的SGAN判别器将预测10 + 1 = 11个类别。在我们的实现中，SGAN判别器的输出将表示为一个包含10个类概率的向量（总和为1.0）以及另一个表示图像是真实还是伪造的概率。
- en: Turning the Discriminator from a binary to a multiclass classifier may seem
    like a trivial change, but its implications are more far-reaching than may appear
    at first glance. Let’s start with a diagram. [Figure 7.2](#ch07fig02) shows the
    SGAN architecture.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 将判别器从二分类器转换为多类分类器可能看起来是一个微不足道的改变，但其影响可能比乍看之下更为深远。让我们从一个图表开始。[图7.2](#ch07fig02)展示了SGAN的架构。
- en: 'Figure 7.2\. In this Semi-Supervised GAN, the Generator takes in a random noise
    vector *z* and produces a fake example *x**. The Discriminator receives three
    kinds of data inputs: fake data from the Generator, real unlabeled examples *x*,
    and real labeled examples (*x*, *y*), where *y* is the label corresponding to
    the given example. The Discriminator then outputs a classification; its goal is
    to distinguish fake examples from the real ones and, for the real examples, identify
    the correct class. Notice that the portion of examples with labels is much smaller
    than the portion of the unlabeled data. In practice, the contrast is even starker
    than the one shown, with labeled data forming only a tiny fraction (often as little
    as 1–2%) of the training data.'
  id: totrans-551
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2\. 在这个半监督生成对抗网络中，生成器接收一个随机噪声向量 *z* 并产生一个伪造示例 *x**。判别器接收三种类型的数据输入：来自生成器的伪造数据、真实未标记示例
    *x* 和真实标记示例 (*x*, *y*)，其中 *y* 是与给定示例对应的标签。然后判别器输出一个分类；其目标是区分伪造示例和真实示例，并对真实示例识别正确的类别。请注意，有标签的示例部分远小于无标签数据部分。在实践中，这种对比甚至更为鲜明，标记数据仅占训练数据的一小部分（通常仅为1-2%）。
- en: '![](../Images/07fig02_alt.jpg)'
  id: totrans-552
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07fig02_alt.jpg)'
- en: As [figure 7.2](#ch07fig02) indicates, the task of distinguishing between multiple
    classes not only impacts the Discriminator itself, but also adds complexity to
    the SGAN architecture, its training process, and its training objectives, as compared
    to the traditional GAN.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图7.2](#ch07fig02)所示，区分多个类别的任务不仅影响判别器本身，而且与传统的GAN相比，增加了SGAN架构、其训练过程和其训练目标的复杂性。
- en: 7.1.2\. Architecture
  id: totrans-554
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.2\. 架构
- en: 'The SGAN Generator’s purpose is the same as in the original GAN: it takes in
    a vector of random numbers and produces fake examples whose goal is to be indistinguishable
    from the training dataset—no change here.'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: SGAN生成器的目的是与原始GAN相同：它接收一个随机数字向量，并产生伪造示例，其目标是与训练数据集不可区分——这里没有变化。
- en: 'The SGAN Discriminator, however, diverges considerably from the original GAN
    implementation. Instead of two, it receives three kinds of inputs: fake examples
    produced by the Generator (*x**), real examples without labels from the training
    dataset (*x*), and real examples with labels from the training dataset (*x, y*),
    where *y* denotes the label for the given example *x*. Instead of binary classification,
    the SGAN Discriminator’s goal is to correctly categorize the input example into
    its corresponding class if the example is real, or reject the example as fake
    (which can be thought of as a special additional class).'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，SGAN 判别器与原始 GAN 实现有相当大的差异。它接收三种类型的输入：生成器产生的伪造示例 (*x**)、来自训练数据集的无标签真实示例 (*x*)
    和来自训练数据集的有标签真实示例 (*x, y*)，其中 *y* 表示给定示例 *x* 的标签。SGAN 判别器的目标不是二元分类，而是如果示例是真实的，则正确地将输入示例分类到其对应的类别中，或者拒绝该示例为伪造（这可以被视为一个特殊的附加类别）。
- en: '[Table 7.1](#ch07table01) summarizes the key takeaways about the two SGAN subnetworks.'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 7.1](#ch07table01) 总结了关于两个 SGAN 子网络的关键要点。'
- en: Table 7.1\. SGAN Generator and Discriminator networks
  id: totrans-558
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 7.1\. SGAN 生成器和判别器网络
- en: '|   | Generator | Discriminator |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '|   | 生成器 | 判别器 |'
- en: '| --- | --- | --- |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Input | A vector of random numbers (z) | The Discriminator receives three
    kinds of inputs:'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '| 输入 | 一组随机数向量 (z) | 判别器接收三种类型的输入：'
- en: Unlabeled real examples (x) coming from the training dataset
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自训练数据集的无标签真实示例 (x)
- en: Labeled real examples (x, y) coming from the training dataset
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自训练数据集的有标签真实示例 (x, y)
- en: Fake examples (x*) produced by the Generator
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由生成器产生的伪造示例 (x*)
- en: '|'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Output | Fake examples (x*) that strive to be as convincing as possible |
    Probabilities, indicating the likelihood that the input example belongs either
    to one of the *N* real classes or to the *fake* class |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | 努力变得尽可能令人信服的伪造示例 (x*) | 概率，表示输入示例属于 *N* 个真实类别之一或伪造类别的可能性 |'
- en: '| Goal | Generate fake examples that are indistinguishable from members of
    the training dataset by fooling the Discriminator into classifying them as real
    | Learn to assign the correct class label to real examples while rejecting all
    examples coming from the Generator as fake |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 生成伪造示例，通过欺骗判别器将其分类为真实示例，使其在训练数据集成员中难以区分 | 学习为真实示例分配正确的类别标签，同时拒绝所有来自生成器的示例作为伪造
    |'
- en: 7.1.3\. Training process
  id: totrans-568
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.3\. 训练过程
- en: Recall that in a regular GAN, we train the Discriminator by computing the loss
    for *D*(*x*) and *D*(*x**) and backpropagating the total loss to update the Discriminator’s
    trainable parameters to minimize the loss. The Generator is trained by backpropagating
    the Discriminator’s loss for *D*(*x**), seeking to maximize it, so that the fake
    examples it synthesizes are misclassified as real.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在常规 GAN 中，我们通过计算 *D*(*x*) 和 *D*(*x**) 的损失，并通过反向传播总损失来更新判别器的可训练参数以最小化损失来训练判别器。生成器通过反向传播判别器对
    *D*(*x**) 的损失来训练，目的是最大化它，从而使它合成的伪造示例被错误地分类为真实。
- en: 'To train the SGAN, in addition to *D*(*x*) and *D*(*x**), we also have to compute
    the loss for the supervised training examples: *D*((*x*, *y*)). These losses correspond
    to the dual learning objective that the SGAN Discriminator has to grapple with:
    distinguishing real examples from the fake ones while also learning to classify
    real examples to their correct classes. Using the terminology from the original
    paper, these dual objectives correspond to two kinds of losses: the *supervised
    loss* and the *unsupervised loss*.^([[7](#ch07fn07)])'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 SGAN 时，除了计算 *D*(*x*) 和 *D*(*x**) 的损失之外，我们还需要计算监督训练示例的损失：*D*((*x*, *y*)).
    这些损失对应于 SGAN 判别器必须应对的二元学习目标：在区分真实示例和伪造示例的同时，还要学会将真实示例分类到正确的类别中。使用原文中的术语，这些二元目标对应于两种类型的损失：*监督损失*
    和 *无监督损失*.^([[7](#ch07fn07)])
- en: ⁷
  id: totrans-571
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-572
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Improved Techniques for Training GANs,” by Tim Salimans et al., 2016, [https://arxiv.org/abs/1606.03498](https://arxiv.org/abs/1606.03498).
  id: totrans-573
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见 Tim Salimans 等人于 2016 年发表的“Improved Techniques for Training GANs”，[https://arxiv.org/abs/1606.03498](https://arxiv.org/abs/1606.03498).
- en: 7.1.4\. Training objective
  id: totrans-574
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.4\. 训练目标
- en: All the GAN variants you have seen so far are generative models. Their goal
    is to produce realistic-looking data samples; hence, the Generator network has
    been of primary interest. The main purpose of the Discriminator network has been
    to help the Generator improve the quality of images it produces. At the end of
    the training, we often disregard the Discriminator and use only the fully trained
    Generator to create realistic-looking synthetic data.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 你迄今为止所看到的所有GAN变体都是生成模型。它们的目标是产生看起来逼真的数据样本；因此，生成器网络一直是首要关注点。判别器网络的主要目的是帮助生成器提高其产生的图像质量。在训练结束时，我们通常忽略判别器，仅使用完全训练好的生成器来创建看起来逼真的合成数据。
- en: In contrast, in a SGAN, we care primarily about the Discriminator. The goal
    of the training process is to make this network into a semi-supervised classifier
    whose accuracy is as close as possible to a fully supervised classifier (one that
    has labels available for each example in the training dataset), while using only
    a small fraction of the labels. The Generator’s goal is to aid this process by
    serving as a source of additional information (the fake data it produces) that
    helps the Generator learn the relevant patterns in the data, enhancing its classification
    accuracy. At the end of the training, the Generator gets discarded, and we use
    the trained Discriminator as a classifier.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，在SGAN中，我们主要关注判别器。训练过程的目标是将这个网络变成一个半监督分类器，其准确率尽可能接近全监督分类器（在训练数据集中的每个示例都有标签），同时只使用一小部分标签。生成器的目标是作为额外信息的来源（它产生的伪造数据）来帮助生成器学习数据中的相关模式，从而提高其分类准确率。在训练结束时，生成器被丢弃，我们使用训练好的判别器作为分类器。
- en: Now that you’ve learned what motivated the creation of the SGAN and we’ve explained
    how the model works, it is time to see the model in action by implementing one.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了SGAN的创建动机，我们也解释了模型的工作原理，现在是时候通过实现一个模型来观察模型的实际应用了。
- en: '7.2\. Tutorial: Implementing a Semi-Supervised GAN'
  id: totrans-578
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 教程：实现半监督生成对抗网络
- en: In this tutorial, we implement an SGAN model that learns to classify handwritten
    digits in the MNIST dataset by using only 100 training examples. At the end of
    the tutorial, we compare the model’s classification accuracy to an equivalent
    fully supervised model to see for ourselves the improvement achieved by semi-supervised
    learning.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们实现了一个SGAN模型，该模型通过仅使用100个训练示例来学习在MNIST数据集中对手写数字进行分类。在教程结束时，我们将模型的分类准确率与一个等效的全监督模型进行比较，以亲自看到半监督学习带来的改进。
- en: 7.2.1\. Architecture diagram
  id: totrans-580
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.1\. 架构图
- en: '[Figure 7.3](#ch07fig03) shows a high-level diagram of the SGAN model implemented
    in this tutorial. It is a bit more complex than the general, conceptual diagram
    we introduced at the beginning of this chapter. After all, the devil is in the
    (implementation) details.'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.3](#ch07fig03)展示了本教程中实现的SGAN模型的高级示意图。它比我们在本章开头介绍的一般概念图要复杂一些。毕竟，魔鬼隐藏在（实现）细节中。'
- en: Figure 7.3\. This SGAN diagram is a high-level illustration of the SGAN we implement
    in this chapter’s tutorial. The Generator turns random noise into fake examples.
    The Discriminator receives real images with labels (*x, y*), real images without
    labels (*x*), and fake images produced by the Generator (*x**). To distinguish
    real examples from fake ones, the Discriminator uses the *sigmoid* function. To
    distinguish between the real classes, the Discriminator uses the *softmax* function.
  id: totrans-582
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.3\. 这个SGAN图是本章教程中实现的SGAN的高级说明。生成器将随机噪声转换为伪造示例。判别器接收带有标签的实图像（*x, y*）、没有标签的实图像（*x*）以及生成器产生的伪造图像（*x**）。为了区分真实示例和伪造示例，判别器使用*sigmoid*函数。为了区分真实类别，判别器使用*softmax*函数。
- en: '![](../Images/07fig03_alt.jpg)'
  id: totrans-583
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07fig03_alt.jpg)'
- en: To solve the multiclass classification problem of distinguishing between the
    real labels, the Discriminator uses the *softmax* function, which gives probability
    distribution over a specified number of classes—in our case, 10\. The higher the
    probability assigned to a given label, the more confident the Discriminator is
    that the example belongs to the given class. To compute the classification error,
    we use cross-entropy loss, which measures the difference between the output probabilities
    and the target, one-hot-encoded labels.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决多类分类问题，即区分真实标签，判别器使用*softmax*函数，它给出了指定数量类别的概率分布——在我们的案例中是10个类别。分配给某个标签的概率越高，判别器就越确信该示例属于该类别。为了计算分类误差，我们使用交叉熵损失，它衡量输出概率与目标、单热编码标签之间的差异。
- en: To output the real-versus-fake probability, the Discriminator uses the *sigmoid*
    activation function and trains its parameters by backpropagating the binary cross-entropy
    loss—the same as the GANs we implemented in [chapters 3](../Text/kindle_split_012.xhtml#ch03)
    and [4](../Text/kindle_split_013.xhtml#ch04).
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 为了输出真实与虚假的概率，判别器使用*sigmoid*激活函数，并通过反向传播二进制交叉熵损失来训练其参数——这与我们在第3章和第4章中实现的GANs相同。
- en: 7.2.2\. Implementation
  id: totrans-586
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.2\. 实现
- en: As you may notice, much of our SGAN implementation is adapted from the DCGAN
    model from [chapter 4](../Text/kindle_split_013.xhtml#ch04). This is not out of
    laziness (well, maybe a little . . .), but rather so that you can better see the
    distinct modifications needed for SGAN without any distractions from implementation
    details in unrelated parts of the network.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 如您可能注意到的，我们的大部分SGAN实现是从第4章中的DCGAN模型改编而来的。这并非出于懒惰（好吧，可能有一点），而是为了让您更好地看到SGAN所需的独特修改，而不受网络无关部分的实现细节的干扰。
- en: A Jupyter notebook with the full implementation, including added visualizations
    of the training progress, is available in our GitHub repository ([https://github.com/GANs-in-Action/gans-in-action](https://github.com/GANs-in-Action/gans-in-action)),
    under the chapter-7 folder. The code was tested with Python 3.6.0, Keras 2.1.6,
    and TensorFlow 1.8.0\. To speed up the training time, we recommend running the
    model on a GPU.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 一个包含完整实现以及添加的训练进度可视化的Jupyter笔记本可在我们的GitHub仓库（[https://github.com/GANs-in-Action/gans-in-action](https://github.com/GANs-in-Action/gans-in-action)）中找到，位于第7章文件夹下。代码已在Python
    3.6.0、Keras 2.1.6和TensorFlow 1.8.0上进行了测试。为了加快训练时间，我们建议在GPU上运行模型。
- en: 7.2.3\. Setup
  id: totrans-589
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.3\. 设置
- en: As usual, we start off by importing all the modules and libraries needed to
    run the model, as shown in the following listing.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，我们首先导入运行模型所需的全部模块和库，如下所示。
- en: Listing 7.1\. Import statements
  id: totrans-591
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.1\. 导入语句
- en: '[PRE6]'
  id: totrans-592
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We also specify the input image size, the size of the noise vector *z*, and
    the number of the real classes for the semi-supervised classification (one for
    each numeral our Discriminator will learn to identify), as shown in the following
    listing.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还指定了输入图像大小、噪声向量*z*的大小以及半监督分类中的真实类别数（每个数字对应我们的判别器将学习识别的一个类别），如下所示。
- en: Listing 7.2\. Model input dimensions
  id: totrans-594
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.2\. 模型输入维度
- en: '[PRE7]'
  id: totrans-595
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '***1* Input image dimensions**'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 输入图像维度**'
- en: '***2* Size of the noise vector, used as input to the Generator**'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 噪声向量的大小，用作生成器的输入**'
- en: '***3* Number of classes in the dataset**'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 数据集中的类别数**'
- en: 7.2.4\. The dataset
  id: totrans-599
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.4\. 数据集
- en: Although the MNIST training dataset has 50,000 labeled training images, we will
    use only a small fraction of them (specified by the `num_labeled` parameter) for
    training and pretend that all the remaining ones are unlabeled. We accomplish
    this by sampling only from the first `num_labeled` images when generating batches
    of labeled data and from the remaining (50,000 – `num_labeled`) images when generating
    batches of unlabeled examples.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管MNIST训练数据集有50,000个标记的训练图像，但我们只会使用其中的一小部分（由`num_labeled`参数指定）进行训练，并假装所有剩余的图像都是未标记的。我们通过在生成标记数据批次时只从前`num_labeled`个图像中进行采样，在生成未标记示例批次时从剩余的（50,000
    - `num_labeled`）个图像中进行采样来实现这一点。
- en: The `Dataset` object (shown in [listing 7.3](#ch07ex03)) also provides a function
    to return all the `num_labeled` training examples along with their labels as well
    as a function to return all 10,000 labeled test images in the MNIST dataset. After
    training, we will use the test set to evaluate how well the model’s classifications
    generalize to previously unseen examples.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dataset` 对象（如[列表 7.3](#ch07ex03)所示）还提供了一个函数，用于返回所有 `num_labeled` 训练示例及其标签，以及一个函数，用于返回
    MNIST 数据集中的所有 10,000 个带标签测试图像。训练后，我们将使用测试集来评估模型的分类如何泛化到之前未见过的示例。'
- en: Listing 7.3\. Dataset for training and testing
  id: totrans-602
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.3\. 训练和测试数据集
- en: '[PRE8]'
  id: totrans-603
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '***1* Number of labeled examples to use for training**'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 用于训练的带标签示例数量**'
- en: '***2* Loads the MNIST dataset**'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 加载 MNIST 数据集**'
- en: '***3* Rescales [0, 255] grayscale pixel values to [–1, 1]**'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将 [0, 255] 灰度像素值缩放到 [–1, 1]**'
- en: '***4* Expands image dimensions to width × height × channels**'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 扩展图像维度为宽度 × 高度 × 通道**'
- en: '***5* Training data**'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 训练数据**'
- en: '***6* Testing data**'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 测试数据**'
- en: '***7* Gets a random batch of labeled images and their labels**'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 获取一个随机批次的带标签图像及其标签**'
- en: '***8* Gets a random batch of unlabeled images**'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 获取一个随机批次的未标记图像**'
- en: 'In this tutorial, we will pretend that we have only 100 labeled MNIST images
    for training:'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将假装我们只有 100 个带标签的 MNIST 图像用于训练：
- en: '[PRE9]'
  id: totrans-613
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '***1* Number of labeled examples to use (the rest will be used as unlabeled)**'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 用于训练的带标签示例数量（其余将用作未标记）**'
- en: 7.2.5\. The Generator
  id: totrans-615
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.5\. 生成器
- en: The Generator network is the same as the one we implemented for the DCGAN in
    [chapter 4](../Text/kindle_split_013.xhtml#ch04). Using transposed convolution
    layers, the Generator transforms the input random noise vector into 28 × 28 ×
    1 image; see the following listing.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络与我们在[第 4 章](../Text/kindle_split_013.xhtml#ch04)中为 DCGAN 实现的网络相同。使用转置卷积层，生成器将输入的随机噪声向量转换为
    28 × 28 × 1 图像；请参阅以下列表。
- en: Listing 7.4\. SGAN Generator
  id: totrans-617
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.4\. SGAN 生成器
- en: '[PRE10]'
  id: totrans-618
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '***1* Reshapes input into a 7 × 7 × 256 tensor via a fully connected layer**'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 通过全连接层将输入重塑为 7 × 7 × 256 张量**'
- en: '***2* Transposed convolution layer, from 7 × 7 × 256 to 14 × 14 × 128 tensor**'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 转置卷积层，从 7 × 7 × 256 到 14 × 14 × 128 张量**'
- en: '***3* Batch normalization**'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 批归一化**'
- en: '***4* Leaky ReLU activation**'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* Leaky ReLU 激活**'
- en: '***5* Transposed convolution layer, from 14 × 14 × 128 to 14 × 14 × 64 tensor**'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 转置卷积层，从 14 × 14 × 128 到 14 × 14 × 64 张量**'
- en: '***6* Transposed convolution layer, from 14 × 14 × 64 to 28 × 28 × 1 tensor**'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 转置卷积层，从 14 × 14 × 64 到 28 × 28 × 1 张量**'
- en: '***7* Output layer with tanh activation**'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 输出层带有 tanh 激活**'
- en: 7.2.6\. The Discriminator
  id: totrans-626
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.6\. 判别器
- en: 'The Discriminator is the most complex part of the SGAN model. Recall that the
    SGAN Discriminator has a dual objective:'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器是 SGAN 模型中最复杂的一部分。回想一下，SGAN 判别器具有双重目标：
- en: Distinguish real examples from fake ones. For this, the SGAN Discriminator uses
    the *sigmoid* function, outputting a single output probability for binary classification.
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分真实示例和伪造示例。为此，SGAN 判别器使用 *sigmoid* 函数，输出单个输出概率，用于二元分类。
- en: For the real examples, accurately classify their label. For this, the SGAN Discriminator
    uses the *softmax* function, outputting a vector of probabilities, one for each
    of the target classes.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于真实示例，准确分类其标签。为此，SGAN 判别器使用 *softmax* 函数，输出一个概率向量，每个目标类别一个。
- en: The Core Discriminator Network
  id: totrans-630
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 核心判别器网络
- en: We start by defining the core Discriminator network. As you may notice, the
    model in [listing 7.5](#ch07ex05) is similar to the ConvNet-based Discriminator
    we implemented in [chapter 4](../Text/kindle_split_013.xhtml#ch04); in fact, it
    is exactly the same all the way until the 3 × 3 × 128 convolutional layer, its
    batch normalization, and *Leaky ReLU* activation.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义核心判别器网络。正如你可能注意到的，[列表 7.5](#ch07ex05) 中的模型与我们在[第 4 章](../Text/kindle_split_013.xhtml#ch04)中实现的基于
    ConvNet 的判别器相似；实际上，它一直相同，直到 3 × 3 × 128 卷积层、其批归一化和 *Leaky ReLU* 激活。
- en: 'After that layer, we add a *dropout*, a regularization technique that helps
    prevent overfitting by randomly dropping neurons and their connections from the
    neural network during training.^([[8](#ch07fn08)]) This forces the remaining neurons
    to reduce their codependence and develop a more general representation of the
    underlying data. The fraction of the neurons to be randomly dropped is specified
    by the rate parameter, which is set to 0.5 in our implementation: `model.add(Dropout(0.5))`.
    We add dropout because of the increased complexity of the SGAN classification
    task and to improve the model’s ability to generalize from only 100 labeled examples.'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 在那一层之后，我们添加了一个*dropout*，这是一种正则化技术，通过在训练过程中随机丢弃神经网络中的神经元及其连接来帮助防止过拟合。这迫使剩余的神经元减少它们的相互依赖，并发展出对底层数据的更一般化表示。要随机丢弃的神经元比例由速率参数指定，在我们的实现中设置为0.5：`model.add(Dropout(0.5))`。我们添加dropout是因为SGAN分类任务的复杂性增加，以及为了提高模型从仅100个标记示例中泛化的能力。
- en: ⁸
  id: totrans-633
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸
- en: ''
  id: totrans-634
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See “Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors,”
    by Geoffrey E. Hinton et al., 2012, [https://arxiv.org/abs/1207.0580](https://arxiv.org/abs/1207.0580).
    See also “Dropout: A Simple Way to Prevent Neural Networks from Overfitting,”
    by Nitish Srivastava et al., 2014, *Journal of Machine Learning Research* 15,
    1929–1958.'
  id: totrans-635
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅Geoffrey E. Hinton等人于2012年发表的“通过防止特征检测器的共适应来改进神经网络”，[https://arxiv.org/abs/1207.0580](https://arxiv.org/abs/1207.0580)。另请参阅Nitish
    Srivastava等人于2014年发表的“Dropout：防止神经网络过拟合的简单方法”，*Journal of Machine Learning Research*
    15，1929–1958。
- en: Listing 7.5\. SGAN Discriminator
  id: totrans-636
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.5\. SGAN判别器
- en: '[PRE11]'
  id: totrans-637
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '***1* Convolutional layer, from 28 × 28 × 1 into 14 × 14 × 32 tensor**'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 卷积层，从28 × 28 × 1张量转换为14 × 14 × 32张量**'
- en: '***2* Leaky ReLU activation**'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* Leaky ReLU激活**'
- en: '***3* Convolutional layer, from 14 × 14 × 32 into 7 × 7 × 64 tensor**'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 卷积层，从14 × 14 × 32张量转换为7 × 7 × 64张量**'
- en: '***4* Batch normalization**'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 批标准化**'
- en: '***5* Leaky ReLU activation**'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* Leaky ReLU激活**'
- en: '***6* Convolutional layer, from 7 × 7 × 64 tensor into 3 × 3 × 128 tensor**'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 卷积层，从7 × 7 × 64张量转换为3 × 3 × 128张量**'
- en: '***7* Dropout**'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* Dropout**'
- en: '***8* Flattens the tensor**'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 展平张量**'
- en: '***9* Fully connected layer with num_classes neurons**'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 具有num_classes个神经元的全连接层**'
- en: Note that the dropout layer is added after batch normalization and not the other
    way around; this has shown to have superior performance due to the interplay between
    the two techniques.^([[9](#ch07fn09)])
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，dropout层是在批标准化之后添加的，而不是相反；这已经证明由于两种技术的相互作用而具有更好的性能。^([[9](#ch07fn09)])
- en: ⁹
  id: totrans-648
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹
- en: ''
  id: totrans-649
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Understanding the Disharmony between Dropout and Batch Normalization by
    Variance Shift,” by Xiang Li et al., 2018, [https://arxiv.org/abs/1801.05134](https://arxiv.org/abs/1801.05134).
  id: totrans-650
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅Xiang Li等人于2018年发表的“通过方差偏移理解Dropout和批标准化的不和谐”，[https://arxiv.org/abs/1801.05134](https://arxiv.org/abs/1801.05134)。
- en: 'Also, notice that the preceding network ends with a fully connected layer with
    10 neurons. Next, we need to define the two Discriminator outputs computed from
    these neurons: one for the supervised, multiclass classification (using *softmax*)
    and the other for the unsupervised, binary classification (using *sigmoid*).'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，前面的网络以一个具有10个神经元的全连接层结束。接下来，我们需要定义从这些神经元计算出的两个判别器输出：一个用于监督的多类分类（使用*softmax*），另一个用于非监督的二分类（使用*sigmoid*）。
- en: The supervised Discriminator
  id: totrans-652
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 监督判别器
- en: In the following listing, we take the core Discriminator network implemented
    previously and use it to build the supervised portion of the Discriminator model.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的列表中，我们采用之前实现的核心判别器网络，并使用它来构建判别器模型的监督部分。
- en: 'Listing 7.6\. SGAN Discriminator: supervised'
  id: totrans-654
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.6\. SGAN判别器：监督
- en: '[PRE12]'
  id: totrans-655
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '***1* Softmax activation, outputs predicted probability distribution over the
    real classes**'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* Softmax激活，输出对真实类别的预测概率分布**'
- en: The unsupervised Discriminator
  id: totrans-657
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 非监督判别器
- en: The following listing implements the unsupervised portion of the Discriminator
    model on top of the core Discriminator network. Notice the `predict(x)` function,
    in which we transform the output of the 10 neurons (from the core Discriminator
    network) into a binary, real-versus-fake prediction.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表在核心判别器网络之上实现了判别器模型的非监督部分。请注意`predict(x)`函数，其中我们将核心判别器网络中10个神经元的输出转换为二进制真实与伪造预测。
- en: 'Listing 7.7\. SGAN Discriminator: unsupervised'
  id: totrans-659
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.7\. SGAN判别器：非监督
- en: '[PRE13]'
  id: totrans-660
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '***1* Transforms distribution over real classes into binary real-versus-fake
    probability**'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 将真实类别的分布转换为二进制真实与伪造概率**'
- en: '***2* Real-versus-fake output neuron defined previously**'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 之前定义的真实与假输出神经元**'
- en: 7.2.7\. Building the model
  id: totrans-663
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.7\. 构建模型
- en: Next, we build and compile the Discriminator and Generator models. Notice the
    use of `categorical_crossentropy` and `binary_crossentropy` loss functions for
    the supervised loss and the unsupervised loss, respectively.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们构建和编译判别器和生成器模型。注意使用`categorical_crossentropy`和`binary_crossentropy`损失函数分别用于监督损失和无监督损失。
- en: Listing 7.8\. Building the models
  id: totrans-665
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.8\. 构建模型
- en: '[PRE14]'
  id: totrans-666
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '***1* Combined Generator + Discriminator model**'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 结合生成器和判别器模型**'
- en: '***2* Core Discriminator network: these layers are shared during supervised
    and unsupervised training.**'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 核心判别器网络：这些层在监督和无监督训练期间是共享的。**'
- en: '***3* Builds and compiles the Discriminator for supervised training**'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 为监督训练构建和编译判别器**'
- en: '***4* Builds and compiles the Discriminator for unsupervised training**'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 为无监督训练构建和编译判别器**'
- en: '***5* Builds the Generator**'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 构建生成器**'
- en: '***6* Keeps Discriminator’s parameters constant for Generator training**'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 在生成器训练期间保持判别器参数不变**'
- en: '***7* Builds and compiles GAN model with fixed Discriminator to train the Generator.
    Note: uses Discriminator version with unsupervised output.**'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 使用固定的判别器构建和编译GAN模型以训练生成器。注意：使用具有无监督输出的判别器版本。**'
- en: 7.2.8\. Training
  id: totrans-674
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.8\. 训练
- en: The following pseudocode outlines the SGAN training algorithm.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 以下伪代码概述了SGAN训练算法。
- en: '|  |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**SGAN training algorithm**'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: '**SGAN训练算法**'
- en: '*For* each training iteration *do*'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于*每个训练迭代 *do*'
- en: 'Train the Discriminator (supervised):'
  id: totrans-679
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练判别器（监督）：
- en: Take a random mini-batch of labeled real examples (*x*, *y*).
  id: totrans-680
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从标记的真实示例中随机抽取一个迷你批次（*x*，*y*）。
- en: Compute *D*((*x*, *y*)) for the given mini-batch and backpropagate the multiclass
    classification loss to update *θ*^((*D*)) to minimize the loss.
  id: totrans-681
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于给定的迷你批次计算 *D*((*x*, *y*)) 并反向传播多类分类损失以更新 *θ*^((*D*)) 以最小化损失。
- en: 'Train the Discriminator (unsupervised):'
  id: totrans-682
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练判别器（无监督）：
- en: Take a random mini-batch of unlabeled real examples *x*.
  id: totrans-683
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从未标记的真实示例中随机抽取一个迷你批次 *x*。
- en: Compute *D*(*x*) for the given mini-batch and backpropagate the binary classification
    loss to update *θ*^((*D*)) to minimize the loss.
  id: totrans-684
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于给定的迷你批次计算 *D*(*x*) 并反向传播二分类损失以更新 *θ*^((*D*)) 以最小化损失。
- en: 'Take a mini-batch of random noise vectors *z* and generate a mini-batch of
    fake examples: *G*(*z*) = *x**.'
  id: totrans-685
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取一个随机噪声向量迷你批次 *z* 并生成一个假例迷你批次：*G*(*z*) = *x**。
- en: Compute *D*(*x**) for the given mini-batch and backpropagate the binary classification
    loss to update *θ*^((*D*)) to minimize the loss.
  id: totrans-686
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于给定的迷你批次计算 *D*(*x**) 并反向传播二分类损失以更新 *θ*^((*D*)) 以最小化损失。
- en: 'Train the Generator:'
  id: totrans-687
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练生成器：
- en: 'Take a mini-batch of random noise vectors *z* and generate a mini-batch of
    fake examples: *G*(*z*) = *x**.'
  id: totrans-688
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取一个随机噪声向量迷你批次 *z* 并生成一个假例迷你批次：*G*(*z*) = *x**。
- en: Compute *D*(*x**) for the given mini-batch and backpropagate the binary classification
    loss to update *θ*^((*G*)) to maximize the loss.
  id: totrans-689
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于给定的迷你批次计算 *D*(*x**) 并反向传播二分类损失以更新 *θ*^((*G*)) 以最大化损失。
- en: '*End for*'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: '*End for*'
- en: '|  |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The following listing implements the SGAN training algorithm.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表实现了SGAN训练算法。
- en: Listing 7.9\. SGAN training algorithm
  id: totrans-693
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.9\. SGAN训练算法
- en: '[PRE15]'
  id: totrans-694
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '***1* Labels for real images: all 1s**'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 真实图像的标签：全为1**'
- en: '***2* Labels for fake images: all 0s**'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 假图像的标签：全为0**'
- en: '***3* Gets labeled examples**'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 获取标记示例**'
- en: '***4* One-hot-encoded labels**'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* One-hot-encoded标签**'
- en: '***5* Gets unlabeled examples**'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 获取未标记示例**'
- en: '***6* Generates a batch of fake images**'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 生成一批假图像**'
- en: '***7* Trains on real labeled examples**'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 在真实标记示例上训练**'
- en: '***8* Trains on real unlabeled examples**'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 在真实未标记示例上训练**'
- en: '***9* Trains on fake examples**'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 在假例上训练**'
- en: '***10* Generates a batch of fake images**'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10* 生成一批假图像**'
- en: '***11* Trains the Generator**'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***11* 训练生成器**'
- en: '***12** Saves the Discriminator’s supervised classification loss to be plotted
    after training*'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***12** 将判别器的监督分类损失保存以在训练后绘制**'
- en: '***13** Outputs training progress*'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***13** 输出训练进度**'
- en: Training the model
  id: totrans-708
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'We use a smaller batch size because we have only 100 labeled examples for training.
    The number of iterations is determined by trial and error: we keep increasing
    the number until the Discriminator’s supervised loss plateaus, but not too far
    past that point (to reduce the risk of overfitting):'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用较小的批量大小，因为我们只有100个标记示例用于训练。迭代次数通过试错确定：我们不断增加迭代次数，直到判别器的监督损失达到平台期，但不要超过太多（以减少过拟合的风险）：
- en: Listing 7.10\. Training the model
  id: totrans-710
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.10\. 训练模型
- en: '[PRE16]'
  id: totrans-711
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '***1* Sets hyperparameters**'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 设置超参数**'
- en: '***2* Trains the SGAN for the specified number of iterations**'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 训练SGAN指定次数的迭代**'
- en: Model training and test accuracy
  id: totrans-714
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型训练和测试准确率
- en: And now for the moment we have all been waiting for—let’s find out how our SGAN
    performs as a classifier. During training, we see that we achieved supervised
    accuracy of 100%. Although this may seem impressive, remember that we have only
    100 labeled examples from which to sample for supervised training. Perhaps our
    model just memorized the training dataset. What matters is how well our classifier
    can generalize to the previously unseen data in the training set, as shown in
    the following listing.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来揭晓我们一直期待的时刻——让我们看看我们的SGAN作为分类器的表现。在训练过程中，我们看到我们达到了100%的监督准确率。虽然这可能看起来很令人印象深刻，但请记住，我们只有100个标记示例用于监督训练。也许我们的模型只是记住了训练数据集。重要的是我们的分类器如何将泛化到训练集中之前未见过的数据，如下面的列表所示。
- en: Listing 7.11\. Checking the accuracy
  id: totrans-716
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.11\. 检查准确率
- en: '[PRE17]'
  id: totrans-717
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '***1* Computes classification accuracy on the test set**'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 在测试集上计算分类准确率**'
- en: Drum roll, please.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓掌，请。
- en: Our SGAN is able to accurately classify about 89% of the examples in the test
    set. To see how remarkable this is, let’s compare its performance to a fully supervised
    classifier.
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的SGAN能够准确分类测试集中大约89%的示例。为了了解这一点有多么显著，让我们将其性能与一个完全监督分类器进行比较。
- en: 7.3\. Comparison to a fully supervised classifier
  id: totrans-721
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3\. 与完全监督分类器的比较
- en: To make the comparison as fair as possible, we use the same network architecture
    for the fully supervised classifier as the one used for the supervised Discriminator
    training, as shown in the following listing. The idea is that this will allow
    us to isolate the improvement to the classifier’s ability to generalize that was
    achieved through the GAN-enabled semi-supervised learning.
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尽可能公平地进行比较，我们使用与用于监督判别器训练的相同网络架构来训练完全监督分类器，如下面的列表所示。想法是这将使我们能够隔离通过GAN启用的半监督学习实现的分类器泛化能力的提升。
- en: Listing 7.12\. Fully supervised classifier
  id: totrans-723
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.12\. 完全监督分类器
- en: '[PRE18]'
  id: totrans-724
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '***1* Fully supervised classifier with the same network architecture as the
    SGAN Discriminator**'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 与SGAN判别器具有相同网络架构的完全监督分类器**'
- en: We train the fully supervised classifier by using the same 100 training examples
    we used to train our SGAN. For brevity, the training code and the code outputting
    the training and test accuracy are not shown here. You can find the code in our
    GitHub repository, in the SGAN Jupyter notebook under the chapter-7 folder.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用与训练SGAN相同的100个训练示例来训练完全监督分类器。为了简洁，这里没有展示训练代码和输出训练和测试准确率的代码。您可以在我们的GitHub仓库中找到代码，在章节-7文件夹下的SGAN
    Jupyter笔记本中。
- en: Like the SGAN Discriminator, the fully supervised classifier achieved 100% accuracy
    on the training dataset. On the test set, however, it was able to correctly classify
    only about 70% of the examples—about a whopping 20 percentage points worse than
    our SGAN. Put differently, the SGAN improved the training accuracy by almost 30%!
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 与SGAN判别器一样，完全监督分类器在训练数据集上达到了100%的准确率。然而，在测试集上，它只能正确分类大约70%的示例——比我们的SGAN差了整整20个百分点！换句话说，SGAN将训练准确率提高了近30%！
- en: With a lot more training data, the fully supervised classifier’s ability to
    generalize improves dramatically. Using the same setup and training, the fully
    supervised classifier with 10,000 labeled examples (100 times as many as we originally
    used), we achieve an accuracy of about 98%. But that would no longer be a *semi-supervised*
    setting.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 随着更多训练数据的加入，完全监督分类器的泛化能力显著提高。使用相同的设置和训练，拥有10,000个标记示例（是我们最初使用的100倍）的完全监督分类器，我们达到了大约98%的准确率。但那将不再是一个
    *半监督* 环境。
- en: 7.4\. Conclusion
  id: totrans-729
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4\. 结论
- en: In this chapter, we explored how GANs can be used for semi-supervised learning
    by teaching the Discriminator to output class labels for real examples. You saw
    that the SGAN-trained classifier’s ability to generalize from a small number of
    training examples is significantly better than a comparable, fully supervised
    classifier.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何通过教会判别器为真实示例输出类别标签来使用GAN进行半监督学习。您看到，SGAN训练的分类器从少量训练示例中泛化的能力显著优于一个可比的完全监督分类器。
- en: From a GAN innovation perspective, a key distinguishing feature of the SGAN
    is the use of labels for Discriminator training. You may be wondering whether
    labels can be leveraged for Generator training as well. Funny you should ask—that
    is what the GAN variant in the next chapter (Conditional GAN) is all about.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 从GAN创新的角度来看，SGAN的一个关键区别特征是使用标签进行判别器训练。你可能想知道是否可以利用标签进行生成器训练。你问得真巧——这正是下一章（条件GAN）的主题。
- en: Summary
  id: totrans-732
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Semi-Supervised GAN (SGAN) is a Generative Adversarial Network whose Discriminator
    learns to do the following:'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半监督GAN (SGAN) 是一种生成对抗网络，其判别器学习以下内容：
- en: Distinguish fake examples from real ones
  id: totrans-734
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分伪造示例和真实示例
- en: Assign the correct class label to the real examples
  id: totrans-735
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为真实示例分配正确的类别标签
- en: The purpose of a SGAN is to train the Discriminator into a classifier that can
    achieve superior classification accuracy from as few labeled examples as possible,
    thereby reducing the dependency of classification tasks on enormous labeled datasets.
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SGAN的目的是通过尽可能少的标记示例来训练判别器成为一个分类器，从而实现优越的分类准确率，从而减少分类任务对大量标记数据集的依赖。
- en: In our implementation, we used *softmax* and multiclass cross-entropy loss for
    the supervised task of assigning real labels, and *sigmoid* and binary cross-entropy
    for the task of distinguishing between real and fake data.
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的实现中，我们使用了*softmax*和多项式交叉熵损失来处理分配真实标签的监督任务，以及*sigmoid*和二元交叉熵来处理区分真实和伪造数据的任务。
- en: We demonstrated that SGAN’s classification accuracy on the previously unseen
    data in the test set is far superior to a comparable fully supervised classifier
    trained on the same number of labeled training examples.
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们证明了SGAN在测试集中对之前未见数据点的分类准确率远优于在相同数量的标记训练示例上训练的同类全监督分类器。
- en: Chapter 8\. Conditional GAN
  id: totrans-739
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第8章. 条件GAN
- en: '*This chapter covers*'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Using labels to train both the Generator and the Discriminator
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用标签训练生成器和判别器
- en: Teaching GANs to generate examples matching a specified label
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教会GAN生成与指定标签匹配的示例
- en: Implementing a Conditional GAN (CGAN) to generate handwritten digits of our
    choice
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现条件GAN (CGAN)以生成我们选择的任意手写数字
- en: In the previous chapter, you learned about the SGAN, which introduced you to
    the idea of using labels in GAN training. SGANs use labels to train the Discriminator
    into a powerful semi-supervised classifier. In this chapter, you’ll learn about
    the Conditional GAN (CGAN), which uses labels to train *both* the Generator and
    the Discriminator. Thanks to this innovation, a Conditional GAN allows us to direct
    the Generator to synthesize the kind of fake examples we want.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你了解了SGAN，它介绍了在GAN训练中使用标签的概念。SGANs使用标签来训练判别器成为一个强大的半监督分类器。在本章中，你将了解条件GAN
    (CGAN)，它使用标签来训练*生成器*和*判别器*。多亏了这一创新，条件GAN允许我们指导生成器合成我们想要的伪造示例。
- en: 8.1\. Motivation
  id: totrans-745
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1. 动机
- en: As you have seen throughout this book, GANs are capable of producing examples
    ranging from simple handwritten digits to photorealistic images of human faces.
    However, although we could control the domain of examples our GAN learned to emulate
    by our selection of the training dataset, we could not specify any of the characteristics
    of the data samples the GAN would generate. For instance, the DCGAN we implemented
    in [chapter 4](../Text/kindle_split_013.xhtml#ch04) could synthesize realistic-looking
    handwritten digits, but we could not control whether it would produce, say, the
    number 7 rather than the number 9 at any given time.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在本书的整个过程中所看到的，GANs能够生成从简单的手写数字到逼真的人脸图像的各种示例。然而，尽管我们可以通过选择训练数据集来控制我们的GAN学习模拟的示例域，但我们无法指定GAN将生成的数据样本的任何特征。例如，我们在[第4章](../Text/kindle_split_013.xhtml#ch04)中实现的DCGAN可以合成看起来逼真的手写数字，但我们无法控制它是否会在任何给定时间产生数字7而不是数字9。
- en: On simple datasets like the MNIST, in which examples belong to only one of 10
    classes, this concern may seem trivial. If, for instance, our goal is to produce
    the number 9, we can just keep generating examples until we get the number we
    want. On more complex data-generation tasks, however, the domain of possible answers
    gets too large for such a brute-force solution to be practical. Take, for example,
    the task of generating human faces. As impressive as the images produced by the
    Progressive GAN from [chapter 6](../Text/kindle_split_016.xhtml#ch06) are, we
    have no control over what face will get produced. There is no way to direct the
    Generator to synthesize, say, a male or a female face, let alone other features
    such as age or facial expression.
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 在像MNIST这样的简单数据集上，其中示例只属于10个类别中的一个，这种担忧可能看起来微不足道。例如，如果我们目标是生成数字9，我们只需不断生成示例，直到我们得到想要的数字。然而，在更复杂的数据生成任务中，可能的答案范围太大，以至于这种蛮力解决方案不再实用。以生成人类面孔的任务为例。尽管第6章中[Progressive
    GAN](../Text/kindle_split_016.xhtml#ch06)生成的图像令人印象深刻，但我们无法控制将生成哪种面孔。我们无法指导生成器合成，比如说，一个男性或女性面孔，更不用说其他特征，如年龄或面部表情。
- en: The ability to decide what kind of data will be generated opens the door to
    a vast array of applications. As a somewhat contrived example, imagine that we
    are detectives solving a murder mystery, and a witness describes the killer as
    a middle-aged woman with long red hair and green eyes. It would greatly expedite
    the process if instead of hiring a sketch artist (who can produce only one sketch
    at a time), we could enter the descriptive features into a computer program and
    have it output a range of faces matching the criteria. Our witness then could
    point us to the one that resembles the criminal most closely.
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 能够决定生成哪种类型的数据，为各种应用打开了大门。作为一个有些牵强的例子，想象一下我们是一群侦探在解决一起谋杀案，一个目击者描述凶手是一个中年红发绿眼的女性。如果我们能够将描述的特征输入到一个计算机程序中，并让它输出一系列符合标准的面孔，这将大大加快这个过程。我们的目击者随后可以指向最像罪犯的那一个。
- en: We are sure you can think of many other practical applications for which the
    ability to generate an image that matches the criteria of our choice would be
    a game-changer. In medical research, we could guide the creation of new drug compounds;
    in filmmaking and computer-generated imagery (CGI), we could create the exact
    scene we want with minimal input from human animators. The list goes on.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信你可以想到许多其他实际应用，其中能够生成符合我们选择标准图像的能力将是一个颠覆性的变化。在医学研究中，我们可以引导新药化合物的创建；在电影制作和计算机生成图像（CGI）中，我们可以用最少的输入从人类动画师那里创建我们想要的场景。这个列表还可以继续下去。
- en: The CGAN is one of the first GAN innovations that made targeted data generation
    possible, and arguably the most influential one. In the remainder of this chapter,
    you will learn how CGANs work and implement a small-scale version by using (you
    guessed it!) the MNIST dataset.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: CGAN（条件生成对抗网络）是首批使有针对性的数据生成成为可能GAN创新之一，可以说是最具影响力的一个。在本章的剩余部分，你将学习CGAN是如何工作的，并通过使用（你猜对了！）MNIST数据集来实现一个小规模版本。
- en: 8.2\. What is Conditional GAN?
  id: totrans-751
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2. 什么是条件生成对抗网络？
- en: Introduced in 2014 by University of Montreal PhD student Mehdi Mirza and Flickr
    AI architect Simon Osindero, *Conditional GAN* is a generative adversarial network
    whose Generator and Discriminator are conditioned during training by using some
    additional information.^([[1](#ch08fn01)]) This auxiliary information could be,
    in theory, anything, such as a class label, a set of tags, or even a written description.
    For clarity and simplicity, we will use labels as the conditioning information
    as we explain how CGAN works.
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 由蒙特利尔大学博士研究生Mehdi Mirza和Flickr人工智能架构师Simon Osindero于2014年提出，*条件生成对抗网络*是一种在训练过程中通过使用一些附加信息对生成器和判别器进行条件化的生成对抗网络。[^([[1](#ch08fn01))]]这种辅助信息在理论上可以是任何东西，比如一个类别标签，一组标签，甚至是一个书面描述。为了清晰和简单，我们将使用标签作为条件信息，在我们解释CGAN是如何工作的过程中。
- en: ¹
  id: totrans-753
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-754
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Conditional Generative Adversarial Nets,” by Mehdi Mirza and Simon Osindero,
    2014, [https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784).
  id: totrans-755
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Mehdi Mirza和Simon Osindero于2014年发表的“Conditional Generative Adversarial Nets”，[https://arxiv.org/abs/1411.1784](https://arxiv.org/abs/1411.1784)。
- en: During CGAN training, the Generator learns to produce realistic examples for
    each label in the training dataset, and the Discriminator learns to distinguish
    fake example-label pairs from real example-label pairs. In contrast to the Semi-Supervised
    GAN from the previous chapter, whose Discriminator learns to assign the correct
    label to each real example (in addition to distinguishing real examples from fake
    ones), the Discriminator in a CGAN does not learn to identify which class is which.
    It learns only to accept real, matching pairs while rejecting pairs that are mismatched
    and pairs in which the example is fake.
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: 在CGAN训练过程中，生成器学习为训练数据集中每个标签生成逼真的示例，而判别器学习区分伪造的示例-标签对和真实的示例-标签对。与上一章中的半监督GAN相比，其判别器学习为每个真实示例分配正确的标签（除了区分真实示例和伪造示例之外），CGAN中的判别器不学习识别哪个类别是哪个。它只学习接受匹配的真实对，同时拒绝不匹配的对以及示例为伪造的对。
- en: For example, the CGAN Discriminator should learn to reject the pair (![](../Images/black3.jpg),
    4), regardless of whether the example (handwritten numeral 3) is real or fake,
    because it does not match the label, 4\. The CGAN Discriminator should also learn
    to reject all image-label pairs in which the image is fake, even if the label
    matches the image.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，CGAN判别器应该学习拒绝对(![](../Images/black3.jpg), 4)，无论示例（手写的数字3）是真实还是伪造，因为它与标签4不匹配。CGAN判别器还应该学习拒绝所有图像-标签对，其中图像是伪造的，即使标签与图像匹配。
- en: Accordingly, in order to fool the Discriminator, it is not enough for the CGAN
    Generator to produce realistic-looking data. The examples it generates also need
    to match their labels. After the Generator is fully trained, this then allows
    us to specify what example we want the CGAN to synthesize by passing it the desired
    label.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了欺骗判别器，CGAN生成器不仅要生成看起来逼真的数据，它生成的示例还需要与它们的标签匹配。生成器完全训练后，这使我们能够通过传递所需的标签来指定我们想要CGAN合成的示例。
- en: 8.2.1\. CGAN Generator
  id: totrans-759
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.1\. CGAN生成器
- en: To formalize things a bit, let’s call the conditioning label *y*. The Generator
    uses the noise vector *z* and the label *y* to synthesize a fake example *G*(*z*,
    *y*) = *x**|*y* (read as “*x** given that, or conditioned on, *y*”). The goal
    of this fake example is to look (in the eyes of the Discriminator) as close as
    possible to a real example for the given label. [Figure 8.1](#ch08fig01) illustrates
    the Generator.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使事情更加形式化，让我们将条件标签称为*y*。生成器使用噪声向量*z*和标签*y*来合成一个伪造的示例*G*(*z*, *y*) = *x**|*y*（读作“*x**给定，或者基于*y*”）。这个伪造示例的目标是在判别器的眼中尽可能接近给定标签的真实示例。[图8.1](#ch08fig01)说明了生成器。
- en: 'Figure 8.1\. CGAN Generator: *G*(*z*, *y*) = *x**|*y*. Using random noise vector
    *z* and label *y* as inputs, the Generator produces a fake example *x**|*y* that
    strives to be a realistic-looking match for the label.'
  id: totrans-761
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.1\. CGAN生成器：*G*(*z*, *y*) = *x**|*y*。使用随机噪声向量*z*和标签*y*作为输入，生成器产生一个伪造的示例*x**|*y*，力求与标签看起来逼真。
- en: '![](../Images/08fig01.jpg)'
  id: totrans-762
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08fig01.jpg)'
- en: 8.2.2\. CGAN Discriminator
  id: totrans-763
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.2\. CGAN判别器
- en: The Discriminator receives real examples with labels (*x*, *y*), and fake examples
    with the label used to generate them, (*x**|*y*, *y*). On the real example-label
    pairs, the Discriminator learns how to recognize real data *and* how to recognize
    matching pairs. On the Generator-produced examples, it learns to recognize fake
    image-label pairs, thereby learning to tell them apart from the real ones.
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器接收带有标签（*x*，*y*）的真实示例和带有生成它们的标签的伪造示例（*x**|*y*，*y*）。在真实示例-标签对上，判别器学习如何识别真实数据以及如何识别匹配的对。在生成器生成的示例上，它学习识别伪造的图像-标签对，从而学习将它们与真实示例区分开来。
- en: The Discriminator outputs a single probability indicating its conviction that
    the input is a real, matching pair. The Discriminator’s goal is to learn to reject
    all fake examples and all examples that fail to match their label, while accepting
    all real example-label pairs, as shown in [figure 8.2](#ch08fig02).
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器输出一个概率值，表示它认为输入是一个真实匹配对的信心程度。判别器的目标是学习拒绝所有伪造示例和所有未能匹配其标签的示例，同时接受所有真实示例-标签对，如图8.2所示。
- en: Figure 8.2\. The CGAN Discriminator receives real examples along with their
    labels (*x*, *y*) and fake examples along with the label used to synthesize them
    (*x**|*y*, *y*). The Discriminator then outputs a probability (computed by the
    sigmoid activation function σ) indicating whether the input pair is real rather
    than fake.
  id: totrans-766
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.2。CGAN判别器接收带有标签的真实示例(*x*, *y*)和假示例，以及用于合成它们的标签(*x**|*y*, *y*)。然后判别器输出一个概率（由sigmoid激活函数σ计算），指示输入对是真实的还是假的。
- en: '![](../Images/08fig02_alt.jpg)'
  id: totrans-767
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08fig02_alt.jpg)'
- en: 8.2.3\. Summary table
  id: totrans-768
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.3. 摘要表
- en: The two CGAN subnetworks, their inputs, outputs, and objectives are summarized
    in [table 8.1](#ch08table01).
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 两个CGAN子网络、它们的输入、输出和目标在[表8.1](#ch08table01)中总结。
- en: Table 8.1\. CGAN Generator and Discriminator networks
  id: totrans-770
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表8.1。CGAN生成器和判别器网络
- en: '|   | Generator | Discriminator |'
  id: totrans-771
  prefs: []
  type: TYPE_TB
  zh: '|   | 生成器 | 判别器 |'
- en: '| --- | --- | --- |'
  id: totrans-772
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Input | A vector of random numbers and a label: (*z*, *y*) | The Discriminator
    receives the following inputs:'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: '| 输入 | 随机数字向量和标签：(*z*, *y*) | 判别器接收以下输入：'
- en: 'Real examples with labels coming from the training dataset: (*x*, *y*)'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自训练数据集的带标签的真实示例：(*x*, *y*)
- en: 'Fake examples created by the Generator to match a given label, along with the
    label: (*x**&#124;*y, y*)'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器为匹配给定标签而创建的假示例，以及标签：(*x**&#124;*y, y*)
- en: '|'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Output | Fake examples that strive to be as convincing as possible in matches
    for their labels: *G*(*z*, *y*) = *x**&#124;*y* | A single probability indicating
    whether the input example is a real, matching example-label pair |'
  id: totrans-777
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | 努力匹配其标签的尽可能令人信服的假示例：*G*(*z*, *y*) = *x**&#124;*y* | 表示输入示例是否为真实匹配示例-标签对的单一概率
    |'
- en: '| Goal | Generate realistic-looking fake data that match their labels | Distinguish
    between fake example-label pairs coming from the Generator and real example-label
    pairs coming from the training dataset |'
  id: totrans-778
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 生成与标签匹配的逼真假数据 | 区分来自生成器的假示例-标签对和来自训练数据集的真实示例-标签对 |'
- en: 8.2.4\. Architecture diagram
  id: totrans-779
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.4. 架构图
- en: Putting it all together, [figure 8.3](#ch08fig03) shows a high-level architecture
    diagram of a CGAN. Notice that for each fake example, the same label *y* is passed
    to both the Generator and the Discriminator. Also, note that the Discriminator
    is never explicitly trained to reject mismatched pairs by being trained on real
    examples with mismatching labels; its ability to identify mismatched pairs is
    a by-product of being trained to accept only real matching pairs.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起，[图8.3](#ch08fig03)展示了CGAN的高级架构图。注意，对于每个假示例，相同的标签 *y* 都传递给了生成器和判别器。此外，请注意，判别器从未被明确训练来通过在具有不匹配标签的真实示例上训练来拒绝不匹配的配对；它识别不匹配配对的能力是训练仅接受真实匹配配对的副产品。
- en: Figure 8.3\. The CGAN Generator uses a random noise vector *z* and a label *y*
    (one of the *n* possible labels) as inputs and produces a fake example *x**|*y*
    that strives to be both realistic looking and a convincing match for the label
    *y*.
  id: totrans-781
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.3。CGAN生成器使用随机噪声向量 *z* 和标签 *y*（*n* 个可能标签之一）作为输入，并生成一个既逼真又令人信服的假示例 *x**|*y*。
- en: '![](../Images/08fig03_alt.jpg)'
  id: totrans-782
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08fig03_alt.jpg)'
- en: '|  |'
  id: totrans-783
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-784
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: 'You may have noticed a pattern: for almost every GAN variant, we present you
    with a table summarizing the inputs, outputs, and objectives of the Discriminator
    and Generator networks, and with a network architecture diagram. This is not by
    accident; indeed, one of the main goals of these chapters is to give you a mental
    template—a reusable framework of sorts—for the kind of things to look for when
    you encounter GAN implementations that diverge from the original GAN. Analyzing
    the Generator and Discriminator networks and the overall model architecture are
    often the best first steps.'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到一个模式：对于几乎每个GAN变体，我们都提供了一个总结判别器和生成器网络输入、输出和目标的表格，以及网络架构图。这并非偶然；实际上，这些章节的主要目标之一是提供一个心理模板——一种可重复使用的框架——当你遇到偏离原始GAN的GAN实现时，可以查找的东西。分析生成器和判别器网络以及整体模型架构通常是最佳的第一步。
- en: '|  |'
  id: totrans-786
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The CGAN Discriminator receives fake labeled examples (*x**|*y*, *y*) produced
    by the Generator and real labeled examples (*x*, *y*), and it learns to tell whether
    a given example-label is real or fake.
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: CGAN判别器接收生成器产生的假标签示例(*x**|*y*, *y*)和真实标签示例(*x*, *y*)，并学习判断给定的示例-标签是真实还是假。
- en: Enough for theory. It’s time we put what you have learned into practice and
    implement our own CGAN model.
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 理论部分就到这里。现在是时候将你所学的知识付诸实践，并实现我们自己的 CGAN 模型了。
- en: '8.3\. Tutorial: Implementing a Conditional GAN'
  id: totrans-789
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3\. 教程：实现条件 GAN
- en: In this tutorial, we will implement a CGAN model that learns to generate handwritten
    digits of our choice. At the end, we will generate a sample of images for each
    numeral to see how well the model learned to generate targeted data.
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将实现一个 CGAN 模型，该模型学习生成我们选择的手写数字。最后，我们将为每个数字生成一个图像样本，以查看模型学习生成目标数据的效果如何。
- en: 8.3.1\. Implementation
  id: totrans-791
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.1\. 实现
- en: Our implementation is inspired by the CGAN in the open source GitHub repository
    of GAN models in Keras (the same one we used in [chapters 3](../Text/kindle_split_012.xhtml#ch03)
    and [4](../Text/kindle_split_013.xhtml#ch04)).^([[2](#ch08fn02)]) In particular,
    we use the repository’s approach of using `Embedding` layers to combine examples
    and labels into joint hidden representations (more on this later).
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实现受到了开源 GitHub 仓库中 Keras GAN 模型的 CGAN 的启发（与我们在第 3 章和第 4 章中使用的是同一个）。特别是，我们使用了该仓库中
    `Embedding` 层结合示例和标签到联合隐藏表示的方法（关于这一点稍后还会详细介绍）。
- en: ²
  id: totrans-793
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-794
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Erik Linder-Norén’s Keras-GAN GitHub repository, 2017, [https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN).
  id: totrans-795
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅 Erik Linder-Norén 的 Keras-GAN GitHub 仓库，2017，[https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN)。
- en: The rest of our CGAN model, however, diverges from the one found in the Keras-GAN
    repository. We refactored the embedding implementation to be more readable and
    added detailed explanatory comments. Crucially, we also adapted our CGAN to use
    convolutional neural networks, which yield significantly more realistic examples—recall
    the difference between the images produced by the GAN in [chapter 3](../Text/kindle_split_012.xhtml#ch03)
    and the DCGAN in [chapter 4](../Text/kindle_split_013.xhtml#ch04)!
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们的 CGAN 模型与 Keras-GAN 仓库中找到的模型有所不同。我们对嵌入实现进行了重构，使其更易于阅读，并添加了详细的解释性注释。关键的是，我们还使我们的
    CGAN 能够使用卷积神经网络，这会产生更真实的效果——回想一下第 3 章中 GAN 生成的图像与第 4 章中 DCGAN 生成的图像之间的差异！
- en: 'A Jupyter notebook with the full implementation, including added visualizations
    of the training progress, is available in our GitHub repository, under the chapter-8
    folder: [https://github.com/GANs-in-Action/gans-in-action](https://github.com/GANs-in-Action/gans-in-action).
    The code was tested with Python 3.6.0, Keras 2.1.6, and TensorFlow 1.8.0\. To
    speed up the training time, we recommend running the model on a GPU.'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: 包含添加的训练进度可视化的完整实现 Jupyter 笔记本可在我们的 GitHub 仓库的 chapter-8 文件夹中找到：[https://github.com/GANs-in-Action/gans-in-action](https://github.com/GANs-in-Action/gans-in-action)。代码已在
    Python 3.6.0、Keras 2.1.6 和 TensorFlow 1.8.0 上进行了测试。为了加快训练时间，我们建议在 GPU 上运行模型。
- en: 8.3.2\. Setup
  id: totrans-798
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.2\. 设置
- en: You guessed it—the first step is to import all the modules and libraries needed
    for our model, as shown in the following listing.
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: 你猜对了——第一步是导入我们模型所需的全部模块和库，如下所示。
- en: Listing 8.1\. Import statements
  id: totrans-800
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.1\. 导入语句
- en: '[PRE19]'
  id: totrans-801
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Just as before, we also specify the input image size, the size of the noise
    vector *z*, and the number of classes in our dataset, as shown here.
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 就像之前一样，我们在这里也指定了输入图像的大小、噪声向量 *z* 的大小以及数据集中的类别数量，具体如下。
- en: Listing 8.2\. Model input dimensions
  id: totrans-803
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.2\. 模型输入维度
- en: '[PRE20]'
  id: totrans-804
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '***1* Input image dimensions**'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 输入图像维度**'
- en: '***2* Size of the noise vector, used as input to the Generator**'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 噪声向量的大小，用作生成器的输入**'
- en: '***3* Number of classes in the dataset**'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 数据集中的类别数量**'
- en: 8.3.3\. CGAN Generator
  id: totrans-808
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.3\. CGAN 生成器
- en: 'In this section, we implement the CGAN Generator. By now, you should be familiar
    with much of this network from [chapters 4](../Text/kindle_split_013.xhtml#ch04)
    and [7](../Text/kindle_split_017.xhtml#ch07). The modifications made for the CGAN
    center around input handling, where we use embedding and element-wise multiplication
    to combine the random noise vector *z* and the label *y* into a joint representation.
    Let’s walk through what the code does:'
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们实现 CGAN 生成器。到目前为止，你应该已经熟悉了网络的大部分内容，这些内容来自第 4 章和第 7 章。我们对 CGAN 的修改主要集中在输入处理上，其中我们使用嵌入和逐元素乘法将随机噪声向量
    *z* 和标签 *y* 结合成一个联合表示。让我们来看看代码做了什么：
- en: Take label *y* (an integer from 0 to 9) and turn it into a dense vector of size
    `z_dim` (the length of the random noise vector) by using the Keras `Embedding`
    layer.
  id: totrans-810
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Keras的`Embedding`层将标签*y*（一个从0到9的整数）转换为一个大小为`z_dim`（随机噪声向量的长度）的密集向量。
- en: Combine the label embedding with the noise vector *z* into a joint representation
    by using the Keras `Multiply` layer. As its name suggests, this layer multiplies
    the corresponding entries of the two equal-length vectors and outputs a single
    vector of the resulting products.
  id: totrans-811
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Keras的`Multiply`层将标签嵌入与噪声向量*z*结合成一个联合表示。正如其名所示，该层将两个等长向量的对应条目相乘，并输出一个包含乘积的单个向量。
- en: Feed the resulting vector as input into the rest of the CGAN Generator network
    to synthesize an image.
  id: totrans-812
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将得到的向量作为输入输入到CGAN生成器网络的其余部分以合成图像。
- en: '[Figure 8.4](#ch08fig04) illustrates the process, using the label 7 as an example.'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.4](#ch08fig04)展示了使用标签7作为示例的过程。'
- en: Figure 8.4\. The steps used to combine the conditioning label (7 in this example)
    and the random noise vector z into a single joint representation
  id: totrans-814
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.4\. 将条件标签（本例中的7）和随机噪声向量z组合成单个联合表示的步骤
- en: '![](../Images/08fig04_alt.jpg)'
  id: totrans-815
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08fig04_alt.jpg)'
- en: First, we embed the label into a vector of the same size as *z*. Second, we
    multiply the corresponding elements of the embedded label and *z* (the symbol
    denotes element-wise multiplication). The resulting joined representation is then
    used as input into the CGAN Generator network.
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将标签嵌入到与*z*相同大小的向量中。其次，我们乘以嵌入的标签和*z*（符号表示逐元素乘法）。然后，将得到的联合表示用作CGAN生成器网络的输入。
- en: And finally, the following listing shows what it all looks like in Python/Keras
    code.
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，以下列表展示了所有这些在Python/Keras代码中的样子。
- en: Listing 8.3\. CGAN Generator
  id: totrans-818
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.3\. CGAN生成器
- en: '[PRE21]'
  id: totrans-819
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '***1* Reshapes input into a 7 × 7 × 256 tensor via a fully connected layer**'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 通过全连接层将输入重塑为7 × 7 × 256张量**'
- en: '***2* Transposed convolution layer, from 7 × 7 × 256 into a 14 × 14 × 128 tensor**'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 转置卷积层，从7 × 7 × 256转换为14 × 14 × 128张量**'
- en: '***3* Batch normalization**'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 批标准化**'
- en: '***4* Leaky ReLU activation**'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 漏波ReLU激活函数**'
- en: '***5* Transposed convolution layer, from 14 × 14 × 128 to a 14 × 14 × 64 tensor**'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 转置卷积层，从14 × 14 × 128转换为14 × 14 × 64张量**'
- en: '***6* Transposed convolution layer, from 14 × 14 × 64 to a 28 × 28 × 1 tensor**'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 转置卷积层，从14 × 14 × 64转换为28 × 28 × 1张量**'
- en: '***7* Output layer with tanh activation**'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 带有tanh激活函数的输出层**'
- en: '***8* Random noise vector z**'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 随机噪声向量z**'
- en: '***9* Conditioning label: integer 0–9 specifying the number G should generate**'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 条件标签：整数0–9，指定G应该生成的数量**'
- en: '***10* Label embedding: turns labels into dense vectors of size z_dim; produces
    3D tensor with shape (batch_size, 1, z_dim)**'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10* 标签嵌入：将标签转换为大小为z_dim的密集向量；生成形状为（batch_size，1，z_dim）的3D张量**'
- en: '***11* Flattens the embedding 3D tensor into 2D tensor with shape (batch_size,
    z_dim)**'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***11* 将嵌入的3D张量展平为形状为（batch_size，z_dim）的2D张量**'
- en: '***12* Element-wise product of the vectors z and the label embeddings**'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***12* 向量z和标签嵌入的逐元素乘积**'
- en: '***13* Generates image for the given label**'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***13* 为给定标签生成图像**'
- en: 8.3.4\. CGAN Discriminator
  id: totrans-833
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.4\. CGAN判别器
- en: 'Next, we implement the CGAN Discriminator. Just as in the previous section,
    the network architecture should look familiar to you, except for the piece where
    we handle the input image and its label. Here, too, we use the Keras `Embedding`
    layer to turn input labels into dense vectors. However, unlike the Generator,
    where the model input is a flat vector, the Discriminator receives three-dimensional
    images. This necessitates customized handling, described in the following steps:'
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实现CGAN判别器。就像前面的章节一样，网络架构应该对你来说很熟悉，除了我们处理输入图像及其标签的部分。在这里，我们也使用Keras的`Embedding`层将输入标签转换为密集向量。然而，与生成器不同，那里的模型输入是一个扁平向量，判别器接收三维图像。这需要定制处理，以下步骤将描述：
- en: Take a label (an integer from 0 to 9) and—using the Keras `Embedding` layer—turn
    the label into a dense vector of size 28 × 28 × 1 = 784 (the length of a flattened
    image).
  id: totrans-835
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取一个标签（一个从0到9的整数）并使用Keras的`Embedding`层将标签转换为大小为28 × 28 × 1 = 784（展开图像的长度）的密集向量。
- en: Reshape the label embeddings into the image dimensions (28 × 28 × 1).
  id: totrans-836
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将标签嵌入重塑为图像维度（28 × 28 × 1）。
- en: Concatenate the reshaped label embedding onto the corresponding image, creating
    a joint representation with the shape (28 × 28 × 2). You can think of it as an
    image with its embedded label “stamped” on top of it.
  id: totrans-837
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将重塑的标签嵌入连接到相应的图像上，创建一个形状为（28 × 28 × 2）的联合表示。你可以将其视为在其嵌入的标签“盖章”在顶部的图像。
- en: Feed the image-label joint representation as input into the CGAN Discriminator
    network. Note that in order for things to work, we have to adjust the model input
    dimensions to (28 × 28 × 2) to reflect the new input shape.
  id: totrans-838
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像-标签联合表示作为输入输入到CGAN判别器网络。请注意，为了使一切正常工作，我们必须调整模型输入维度为（28 × 28 × 2），以反映新的输入形状。
- en: Again, to make it less abstract, let’s see what the process looks like visually,
    using the label 7 as an example; see [figure 8.5](#ch08fig05).
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，为了使其更具体，让我们通过使用标签7作为示例来查看这个过程的外观；参见[图8.5](#ch08fig05)。
- en: Figure 8.5\. The steps used to combine the label (7 in this case) and the input
    image into a single joint representation
  id: totrans-840
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.5\. 将标签（本例中为7）和输入图像组合成单个联合表示的步骤
- en: '![](../Images/08fig05_alt.jpg)'
  id: totrans-841
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08fig05_alt.jpg)'
- en: First, we embed the label into a vector the size of a flattened image (28 ×
    28 × 1 = 784). Second, we reshape the embedded label into a tensor with the same
    shape as the input image (28 × 28 × 1). Third, we concatenate the reshaped label
    that is embedding onto the corresponding image. This joined representation is
    then passed as input into the CGAN Discriminator network.
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将标签嵌入到一个与展平图像大小相同的向量中（28 × 28 × 1 = 784）。其次，我们将嵌入的标签重塑为与输入图像相同的形状（28 ×
    28 × 1）。第三，我们将重塑的嵌入标签连接到相应的图像上。然后，这个联合表示被传递到CGAN判别器网络作为输入。
- en: In addition to the preprocessing steps, we have to make a few additional adjustments
    to the Discriminator network compared to the one in [chapter 4](../Text/kindle_split_013.xhtml#ch04).
    (As in the previous chapter, basing the model on our DCGAN implementation should
    make it easier to see the CGAN-specific changes without distractions from implementation
    details in unrelated parts of the model.) First, we have to adjust the model input
    dimensions to (28 × 28 × 2) to reflect the new input shape.
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: 除了预处理步骤之外，与第4章中的判别器网络相比，我们还需要对判别器网络进行一些额外的调整。（正如前一章所述，基于我们的DCGAN实现构建模型应该更容易看到CGAN特有的变化，而不会受到模型无关部分的实现细节的干扰。）首先，我们必须调整模型输入维度为（28
    × 28 × 2），以反映新的输入形状。
- en: Second, we increase the depth of the first convolutional layer from 32 to 64\.
    The reasoning behind this change is that there is more information to encode because
    of the concatenated label embedding; this network architecture indeed yielded
    better results experimentally.
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们将第一个卷积层的深度从32增加到64。这种变化背后的原因是由于连接的标签嵌入，编码的信息更多；这种网络架构在实验中确实产生了更好的结果。
- en: At the output layer, we use the *sigmoid* activation function to produce a probability
    that the input image-label pair is real rather than fake—no change here. And finally,
    the following listing is our CGAN Discriminator implementation.
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出层，我们使用 *sigmoid* 激活函数来产生一个概率，即输入图像-标签对是真实的而不是伪造的——这里没有变化。最后，以下列表是我们的CGAN判别器实现。
- en: Listing 8.4\. CGAN Discriminator
  id: totrans-846
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.4\. CGAN判别器
- en: '[PRE22]'
  id: totrans-847
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '***1* Convolutional layer, from 28 × 28 × 2 into 14 × 14 × 64 tensor**'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 卷积层，从 28 × 28 × 2 张量转换为 14 × 14 × 64 张量**'
- en: '***2* Leaky ReLU activation**'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 漏波ReLU激活**'
- en: '***3* Convolutional layer, from 14 × 14 × 64 into 7 × 7 × 64 tensor**'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 卷积层，从 14 × 14 × 64 张量转换为 7 × 7 × 64 张量**'
- en: '***4* Batch normalization**'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 批标准化**'
- en: '***5* Leaky ReLU activation**'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 漏波ReLU激活**'
- en: '***6* Convolutional layer, from 7 × 7 × 64 tensor into 3 × 3 × 128 tensor**'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 卷积层，从 7 × 7 × 64 张量转换为 3 × 3 × 128 张量**'
- en: '***7* Batch normalization**'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 批标准化**'
- en: '***8* Leaky ReLU**'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 漏波ReLU（Leaky ReLU**）'
- en: '***9* Output layer with sigmoid activation**'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 带有sigmoid激活的输出层**'
- en: '***10* Input image**'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10* 输入图像**'
- en: '***11* Label for the input image**'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***11* 输入图像的标签**'
- en: '***12* Label embedding: turns labels into dense vectors of size z_dim; produces
    3D tensor with shape (batch_size, 1, 28 × 28 × 1)**'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***12* 标签嵌入：将标签转换为大小为z_dim的密集向量；产生形状为（batch_size, 1, 28 × 28 × 1）的3D张量**'
- en: '***13* Flattens the embedding 3D tensor into a 2D tensor with shape (batch_size,
    28 × 28 × 1)**'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***13* 将嵌入的3D张量展平为形状为（batch_size, 28 × 28 × 1）的2D张量**'
- en: '***14* Reshapes label embeddings to have the same dimensions as input images**'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***14* 将标签嵌入重塑为与输入图像相同的维度**'
- en: '***15* Concatenates images with their label embeddings**'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***15* 将图像与其标签嵌入连接起来**'
- en: '***16* Classifies the image-label pair**'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***16* 对图像-标签对进行分类**'
- en: 8.3.5\. Building the model
  id: totrans-864
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.5\. 构建模型
- en: Next, we build and compile the CGAN Discriminator and Generator models, as shown
    in the following listing. Notice that in the combined model used to train the
    Generator, the same input label is passed to the Generator (to generate a sample)
    and to the Discriminator (to make a prediction).
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们构建和编译CGAN判别器和生成器模型，如下所示。注意，在用于训练生成器的组合模型中，相同的输入标签被传递给生成器（以生成样本）和判别器（以做出预测）。
- en: Listing 8.5\. Building and compiling the CGAN model
  id: totrans-866
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.5\. 构建和编译CGAN模型
- en: '[PRE23]'
  id: totrans-867
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '***1* Random noise vector z**'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 随机噪声向量z**'
- en: '***2* Image label**'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 图像标签**'
- en: '***2* Generated image for that label**'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 对该标签生成的图像**'
- en: '***4* Combined Generator -> Discriminator model** **G([z, label]) = x*** **D(x*)
    = classification**'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 组合生成器 -> 判别器模型** **G([z, label]) = x*** **D(x*) = classification**'
- en: '***5* Builds and compiles the Discriminator**'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 构建和编译判别器**'
- en: '***6* Builds the Generator**'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 构建生成器**'
- en: '***7* Keeps Discriminator’s parameters constant for Generator training**'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 在生成器训练期间保持判别器参数不变**'
- en: '***8* Builds and compiles CGAN model with fixed Discriminator to train the
    Generator**'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 使用固定判别器的CGAN模型构建和编译以训练生成器**'
- en: 8.3.6\. Training
  id: totrans-876
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.6\. 训练
- en: For the CGAN training algorithm, the details of each training iteration are
    as follows.
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CGAN训练算法，每个训练迭代的细节如下。
- en: '|  |'
  id: totrans-878
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**CGAN training algorithm**'
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: '**CGAN训练算法**'
- en: '*For* each training iteration *do*'
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于*每个训练迭代*执行*'
- en: 'Train the Discriminator:'
  id: totrans-881
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练判别器：
- en: Take a random mini-batch of real examples and their labels (*x*, *y*).
  id: totrans-882
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从随机迷你批次中取出一批真实示例及其标签 (*x*, *y*)。
- en: Compute *D*((*x*, *y*)) for the mini-batch and backpropagate the binary classification
    loss to update *θ*^((*D*)) to minimize the loss.
  id: totrans-883
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于迷你批次计算 *D*((*x*, *y*)) 并将二元分类损失反向传播以更新 *θ*^((*D*)) 以最小化损失。
- en: 'Take a mini-batch of random noise vectors and class labels (*z*, *y*) and generate
    a mini-batch of fake examples: *G*(*z*, *y*) = *x**|*y*.'
  id: totrans-884
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取出一批随机噪声向量及其类别标签 (*z*, *y*) 并生成一批假示例：*G*(*z*, *y*) = *x**|*y*。
- en: Compute *D*(*x**|*y*, *y*) for the mini-batch and backpropagate the binary classification
    loss to update *θ*^((*D*)) to minimize the loss.
  id: totrans-885
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于迷你批次计算 *D*(*x**|*y*, *y*) 并将二元分类损失反向传播以更新 *θ*^((*D*)) 以最小化损失。
- en: 'Train the Generator:'
  id: totrans-886
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练生成器：
- en: 'Take a mini-batch of random noise vectors and class labels (*z*, *y*) and generate
    a mini-batch of fake examples: *G*(*z*, *y*) = *x**|*y*.'
  id: totrans-887
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取出一批随机噪声向量及其类别标签 (*z*, *y*) 并生成一批假示例：*G*(*z*, *y*) = *x**|*y*。
- en: Compute *D*(*x**|*y*, *y*) for the given mini-batch and backpropagate the binary
    classification loss to update *θ*^((*G*)) to maximize the loss.
  id: totrans-888
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于给定的迷你批次，计算 *D*(*x**|*y*, *y*) 并将二元分类损失反向传播以更新 *θ*^((*G*)) 以最大化损失。
- en: '*End for*'
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: '*结束for*'
- en: '|  |'
  id: totrans-890
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The following listing implements this CGAN training algorithm.
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表实现了这个CGAN训练算法。
- en: Listing 8.6\. CGAN training loop
  id: totrans-892
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.6\. CGAN训练循环
- en: '[PRE24]'
  id: totrans-893
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '***1* Loads the MNIST dataset**'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 加载MNIST数据集**'
- en: '***2* Rescales [0, 255] grayscale pixel values to [–1, 1]**'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 将[0, 255]灰度像素值缩放到[–1, 1]**'
- en: '***2* Labels for real images: all 1s**'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 真图像的标签：全部为1s**'
- en: '***4* Labels for fake images: all 0s**'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 假图像的标签：全部为0**'
- en: '***5* Gets a random batch of real images and their labels**'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 获取一批随机真实图像及其标签**'
- en: '***6* Generates a batch of fake images**'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 生成一批假图像**'
- en: '***7* Trains the Discriminator**'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 训练判别器**'
- en: '***8* Generates a batch of noise vectors**'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 生成一批噪声向量**'
- en: '***9* Gets a batch of random labels**'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 获取一批随机标签**'
- en: '***10* Trains the Generator**'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10* 训练生成器**'
- en: '***11* Outputs training progress**'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***11* 输出训练进度**'
- en: '***12** Saves losses and accuracies so they can be plotted after training*'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***12** 保存损失和准确率，以便在训练后绘制**'
- en: '***13** Outputs sample of generated images*'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***13** 输出生成的图像样本**'
- en: 8.3.7\. Outputting sample images
  id: totrans-907
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.7\. 输出样本图像
- en: You may recognize the next function from [chapters 3](../Text/kindle_split_012.xhtml#ch03)
    and [4](../Text/kindle_split_013.xhtml#ch04). We used it to examine how the quality
    of the Generator-produced images improved as the training progressed. The function
    in [listing 8.7](#ch08ex07) is indeed similar, but a few crucial differences exist.
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能从[第3章](../Text/kindle_split_012.xhtml#ch03)和[第4章](../Text/kindle_split_013.xhtml#ch04)中认出下一个函数。我们使用它来检查随着训练的进行，生成器产生的图像质量是如何提高的。列表8.7中的函数确实相似，但存在一些关键差异。
- en: First, instead of a 4 × 4 grid of random handwritten digits, we are generating
    a 2 × 5 grid of numbers, 1 through 5 in the first row, and 6 through 9 in the
    second row. This allows us to inspect how well the CGAN Generator is learning
    to produce specific numerals. Second, we are displaying the label for each example
    by using the `set_title()` method.
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们不是生成一个4×4的随机手写数字网格，而是生成一个2×5的数字网格，第一行是1到5，第二行是6到9。这使得我们能够检查CGAN生成器学习生成特定数字的效果。其次，我们通过使用`set_title()`方法显示每个示例的标签。
- en: Listing 8.7\. Displaying generated images
  id: totrans-910
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.7. 显示生成的图像
- en: '[PRE25]'
  id: totrans-911
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '***1* Sample random noise**'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1. 采样随机噪声**'
- en: '***2* Gets image labels 0–9**'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2. 获取图像标签0-9**'
- en: '***3* Generates images from random noise**'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3. 从随机噪声生成图像**'
- en: '***4* Rescales image pixel values to [0, 1]**'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4. 将图像像素值缩放到[0, 1]**'
- en: '***5* Sets image grid**'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5. 设置图像网格**'
- en: '***6* Outputs a grid of images**'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6. 输出一个图像网格**'
- en: '[Figure 8.6](#ch08fig06) shows sample output from this function and illustrates
    the improvement to the CGAN-produced numerals over the course of training.'
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.6](#ch08fig06) 展示了该函数的样本输出，并说明了在训练过程中CGAN生成的数字的改进。'
- en: Figure 8.6\. Starting from random noise, GCAN learns to produce realistic-looking
    numerals for each of the labels in the training dataset.
  id: totrans-919
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.6. 从随机噪声开始，GCAN学会了为训练数据集中的每个标签生成看起来逼真的数字。
- en: '![](../Images/08fig06_alt.jpg)'
  id: totrans-920
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08fig06_alt.jpg)'
- en: 8.3.8\. Training the model
  id: totrans-921
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.8. 训练模型
- en: 'And finally, let’s run the model we just implemented:'
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们运行我们刚刚实现的模型：
- en: '[PRE26]'
  id: totrans-923
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '***1* Sets hyperparameters**'
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1. 设置超参数**'
- en: '***2* Trains the CGAN for the specified number of iterations**'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2. 对CGAN进行指定次数的迭代训练**'
- en: '8.3.9\. Inspecting the output: Targeted data generation'
  id: totrans-926
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.9. 检查输出：目标数据生成
- en: '[Figure 8.7](#ch08fig07) shows the images of digits produced by the CGAN Generator
    after it is fully trained. At each row, we instruct the Generator to synthesize
    a different numeral, from 0 to 9\. Notice that each numeral is rendered in a different
    writing style, attesting to CGAN’s ability not only to learn to produce examples
    matching every label in the training dataset, but also to capture the full diversity
    of the training data.'
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.7](#ch08fig07) 展示了CGAN生成器在完全训练后产生的数字图像。在每一行中，我们指示生成器合成一个不同的数字，从0到9。请注意，每个数字都以不同的书写风格呈现，这证明了CGAN不仅能够学习生成与训练数据集中每个标签匹配的示例，而且能够捕捉到训练数据的全部多样性。'
- en: Figure 8.7\. Each row shows a sample of images produced to match a given numeral,
    0 through 9\. As you can see, the CGAN Generator has successfully learned to produce
    every class represented in our dataset.
  id: totrans-928
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.7. 每一行展示了一个用于匹配给定数字（0到9）的图像样本。如您所见，CGAN生成器已经成功地学会了生成我们数据集中表示的每个类别。
- en: '![](../Images/08fig07.jpg)'
  id: totrans-929
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08fig07.jpg)'
- en: 8.4\. Conclusion
  id: totrans-930
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4. 结论
- en: In this chapter, you saw how labels could be used to guide the training of the
    Generator and the Discriminator to teach a GAN to produce fake examples of our
    choice. Along with the DCGAN, CGAN is one of the most influential early GAN variants
    that has inspired countless new research directions.
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您看到了如何使用标签来指导生成器和判别器的训练，以教会一个生成对抗网络（GAN）生成我们选择的假样本。除了DCGAN之外，CGAN是最有影响力的早期GAN变体之一，它激发了无数新的研究方向。
- en: Perhaps the most impactful and promising of these is the use of conditional
    adversarial networks as a general-purpose solution to image-to-image translation
    problems. This is a class of problems that seeks to translate images from one
    modality into another. Applications of image-to-image translation range from colorizing
    black-and-white photos to turning a daytime scene into nighttime and synthesizing
    a satellite view from a map view.
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些方法中，最有影响力和最有前景的是使用条件对抗网络作为图像到图像翻译问题的一般性解决方案。这类问题旨在将图像从一种模态转换为另一种模态。图像到图像翻译的应用范围从给黑白照片上色到将白天场景转换为夜晚，以及从地图视图合成卫星视图。
- en: One of the most successful early implementations based on the Conditional GAN
    paradigm is pix2pix, which uses pairs of images (one as the input and the other
    as the label) to learn to translate from one domain into another. Recall that,
    in theory as well as in practice, the conditioning information used to train a
    CGAN can be much more than just labels to provide for more complex use cases and
    scenarios. For example, for colorization tasks, an image pair would be a black-and-white
    photo (the input) and a colored version of the same photo (the label). You will
    see these illustrated in the following chapter.
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: 基于条件GAN范式最成功的早期实现之一是pix2pix，它使用图像对（一个作为输入，另一个作为标签）来学习从一个域翻译到另一个域。回想一下，在理论和实践中，用于训练CGAN的条件信息可以远不止标签，以提供更复杂的使用案例和场景。例如，对于着色任务，图像对将是一张黑白照片（输入）和同一照片的彩色版本（标签）。你将在下一章中看到这些示例。
- en: We do not cover pix2pix in detail because only about a year after its publication,
    it was eclipsed by another GAN variant that not only outperformed pix2pix’s performance
    on image-to-image translation tasks but also accomplished it without the need
    for paired images. The Cycle-Consistent Adversarial Network (or CycleGAN, as the
    technique came to be known) needs only two groups of images representing the two
    domains (for example, a group of black-and-white photos and a group of colored
    photos). You will learn all about this remarkable GAN variant in the following
    chapter.
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有详细介绍pix2pix，因为它发表后大约一年，就被另一种GAN变体所超越。这种变体不仅在图像到图像的翻译任务上超越了pix2pix的性能，而且无需配对图像就能完成这项任务。循环一致对抗网络（或CycleGAN，正如这项技术后来所知）只需要两组代表两个域的图像（例如，一组黑白照片和一组彩色照片）。你将在下一章中了解这种引人注目的GAN变体。
- en: Summary
  id: totrans-935
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概述
- en: Conditional GAN (CGAN) is a GAN variant in which both the Generator and the
    Discriminator are conditioned on auxiliary data such as a class label during training.
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件GAN（CGAN）是一种GAN变体，其中生成器和判别器在训练期间都基于辅助数据，如类标签。
- en: The additional information constrains the Generator to synthesize a certain
    type of output and the Discriminator to accept only real examples matching the
    given additional information.
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 额外的信息限制了生成器合成特定类型的输出，并使判别器只能接受与给定额外信息匹配的真实示例。
- en: As a tutorial, we implemented a CGAN that generates realistic handwritten digits
    of our choice by using MNIST class labels as our conditioning information.
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为教程，我们实现了一个CGAN，通过使用MNIST类标签作为我们的条件信息，生成我们选择的逼真的手写数字。
- en: Embedding maps an integer into a dense vector of the desired size. We used embedding
    to create a joint hidden representation from a random noise vector and a label
    (for CGAN Generator training) and from an input image and a label (for CGAN Discriminator
    training).
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入将整数映射到所需大小的密集向量。我们使用嵌入从随机噪声向量和标签（用于CGAN生成器训练）以及输入图像和标签（用于CGAN判别器训练）创建联合隐藏表示。
- en: Chapter 9\. CycleGAN
  id: totrans-940
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第9章\. CycleGAN
- en: '*This chapter covers*'
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Expanding on the idea of Conditional GANs by conditioning on an entire image
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过对整个图像进行条件化来扩展条件GANs的想法
- en: 'Exploring one of the most powerful and complex GAN architectures: CycleGAN'
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索最强大和复杂的GAN架构之一：CycleGAN
- en: Presenting an object-oriented design of GANs and the architecture of its four
    main components
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示GANs的面向对象设计和其四个主要组件的架构
- en: Implementing a CycleGAN to run a conversion of apples to oranges
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个CycleGAN以运行苹果到橙子的转换
- en: Finally, a technological breakthrough of almost universal appeal, seeing as
    everyone seems to love comparing apples to oranges. In this chapter, you will
    learn how! But this is no small feat, so we will need at least *two* sets of Discriminators
    and *two* Generators to achieve this. That obviously complicates the architecture,
    so we will have to spend more time discussing it, but at the very least, it is
    a great point to start thinking in a fully object-oriented programming (OOP) way.
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个几乎具有普遍吸引力的技术突破，因为似乎每个人都喜欢比较苹果和橙子。在本章中，你将学习如何做到这一点！但这不是一件小事，因此我们需要至少*两组*判别器和*两组*生成器来实现这一点。这显然使架构复杂化，因此我们需要花更多的时间来讨论它，但至少，这是一个很好的起点，以完全面向对象编程（OOP）的方式思考。
- en: 9.1\. Image-to-image translation
  id: totrans-947
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1\. 图像到图像的翻译
- en: One fascinating area of GANs’ application that we touched on at the end of the
    previous chapter is *image-to-image translation*. In this use, GANs have been
    massively successful—in video, static images, or even style transfer. Indeed,
    GANs have been at the forefront of many of these applications as they enable almost
    a new class of uses. Because of their visual nature, the more successful GAN variants
    typically make their rounds on YouTube and Twitter, so if you have not seen these
    videos, we encourage you to check them out by searching for *pix2pix*, *CycleGAN*,
    or *vid2vid*.
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章末尾我们提到的GANs应用的一个令人着迷的领域是*图像到图像的翻译*。在这个应用中，GANs取得了巨大的成功——在视频、静态图像，甚至是风格转换中。事实上，GANs在这些应用中处于前沿，因为它们几乎开启了一个全新的使用类别。由于它们的视觉特性，更成功的GAN变体通常在YouTube和Twitter上广为流传，所以如果您还没有看过这些视频，我们鼓励您通过搜索*pix2pix*、*CycleGAN*或*vid2vid*来查看它们。
- en: This type of translation in practice means that our input to the Generator is
    a picture, because we need our Generator (translator) to start from this image.
    In other words, we are mapping an image from one domain to another. Previously,
    the latent vector seeding the generation was typically a somewhat uninterpretable
    vector. Now we are swapping that for an input image.
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这种翻译意味着我们的生成器输入是一个图片，因为我们需要我们的生成器（翻译器）从这个图像开始。换句话说，我们正在将一个图像从一个域映射到另一个域。以前，生成过程中用于播种的潜在向量通常是一个难以理解的向量。现在我们用输入图像来替换它。
- en: A good way to think of image-to-image translation is as a special case of the
    Conditional GAN. However, in this case, we are conditioning on a complete image
    (rather than just a class)—typically of the same dimensionality as the output
    image—that is then provided to the network as a kind of a label (presented in
    [chapter 8](../Text/kindle_split_018.xhtml#ch08)). One of the first famous examples
    in this space was an image-translation work coming out of the University of California,
    Berkeley, as shown in [figure 9.1](#ch09fig01).
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像到图像翻译视为条件GAN的一个特殊情况是一个很好的思考方式。然而，在这种情况下，我们是在一个完整的图像上进行条件化（而不是仅仅一个类别）——通常与输出图像具有相同的维度性，然后将其作为标签（在第8章中介绍）提供给网络。在这个领域，第一个著名的例子是来自加州大学伯克利分校的一个图像翻译工作，如图9.1所示。
- en: Figure 9.1\. Conditional GANs provide a powerful framework for image translation
    that performs well across many domains.
  id: totrans-951
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.1\. 条件GANs提供了一个强大的框架，用于图像翻译，在许多领域都表现出色。
- en: '![](../Images/09fig01_alt.jpg)'
  id: totrans-952
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09fig01_alt.jpg)'
- en: '(Source: “Image-to-Image Translation with Conditional Adversarial Networks,”
    by Phillip Isola, [https://github.com/phillipi/pix2pix](https://github.com/phillipi/pix2pix).)'
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：“使用条件对抗网络的图像到图像翻译”，作者：Phillip Isola，[https://github.com/phillipi/pix2pix](https://github.com/phillipi/pix2pix).）
- en: 'As you can see, we can map from any of the following:'
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们可以从以下任何一种映射：
- en: From semantic labels (for example, drawing blue where a car should be and purple
    where a road should be) to photorealistic images of streets
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从语义标签（例如，在汽车应该画蓝色的地方画蓝色，在道路应该画紫色的地方画紫色）到街道的逼真图像
- en: From satellite images to a view like the one in Google Maps
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从卫星图像到类似于谷歌地图的视图
- en: From day images to night images
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从日间图像到夜间图像
- en: From black-and-white to color
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从黑白到彩色
- en: From outlines to synthesized fashion items
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从轮廓到合成的时尚物品
- en: The idea is clearly powerful and versatile; however, the issue lies with the
    need for paired data. From [chapter 8](../Text/kindle_split_018.xhtml#ch08), you
    understand that we need labels for the Conditional GAN. Because in this case we
    are using another image as a label, the mapping does not make sense unless we’re
    mapping to the corresponding image—the exact same image, except in the other domain.
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法显然非常强大且灵活；然而，问题在于需要成对的数据。从[第8章](../Text/kindle_split_018.xhtml#ch08)，您了解到我们需要为条件GAN提供标签。因为在这种情况下，我们使用另一张图像作为标签，除非我们映射到对应的图像——即在其他域中的完全相同的图像，否则这种映射没有意义。
- en: So, the night image needs to be taken from exactly the same place as the day
    image. The fashion item’s outline needs to have the exact match of a fully colored/synthesized
    item in the training set in the other domain. In other words, during training,
    the GAN needs to have access to corresponding labels of the items in the original
    domain.
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，夜间图像需要与日间图像来自完全相同的位置。时尚物品的轮廓需要与训练集中其他域中完全彩色/合成的物品的精确匹配。换句话说，在训练过程中，GAN需要能够访问原始域中物品的对应标签。
- en: This is typically done—for example, in the case of black-and-white images—by
    first taking loads of colored pictures, applying the B&W filter on all of them,
    and then using the unmodified image as one domain and the B&W-filtered images
    as the other. This ensures that we have the corresponding images in both domains.
    Then we can apply the trained GAN anywhere, but if we do not have an easy way
    of generating these “perfect” pairs, we are out of luck!
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是通过首先加载大量的彩色图片，对所有这些图片应用黑白滤镜，然后使用未修改的图像作为一个域，而黑白滤镜后的图像作为另一个域来完成的。这确保了我们在这两个域中都有相应的图像。然后我们可以在任何地方应用训练好的GAN，但如果我们没有生成这些“完美”对子的简单方法，我们就很不幸了！
- en: '9.2\. Cycle-consistency loss: There and back aGAN'
  id: totrans-963
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2. 循环一致性损失：去而复返的aGAN
- en: 'The genius insight of this UC Berkeley group was that we do not, in fact, need
    perfect pairs.^([[1](#ch091)]) Instead, we simply complete the cycle: we translate
    from one domain to another and then back again. For example, we go from summer
    picture (domain A) of a park to a winter one (domain B) and then back again to
    summer (domain A). Now we have essentially created a cycle, and, ideally, the
    original picture (*a*) and the reconstructed picture (![](../Images/acap.jpg))
    are the same. If they are not, we can measure their loss on a pixel level, thereby
    getting the first loss of our CycleGAN: *cycle-consistency loss*, which is depicted
    in [figure 9.2](#ch09fig02).'
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: 这组加州大学伯克利分校的天才洞察力是，我们实际上并不需要完美的对子.^([[1](#ch091)]) 相反，我们只是完成循环：从一个域翻译到另一个域，然后再翻译回来。例如，我们将公园的夏季图片（域A）翻译成冬季图片（域B），然后再翻译回夏季（域A）。现在我们基本上创建了一个循环，理想情况下，原始图片(*a*)和重建图片(![图片](../Images/acap.jpg))是相同的。如果它们不相同，我们可以在像素级别上衡量它们的损失，从而得到CycleGAN的第一个损失：*循环一致性损失*，这在[图9.2](#ch09fig02)中有所描述。
- en: ¹
  id: totrans-965
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-966
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial
    Networks,” by Jun-Yan Zhu et al., 2017, [https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf).
  id: totrans-967
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Jun-Yan Zhu等人2017年的论文“使用循环一致性对抗网络的未配对图像到图像翻译”，[https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf)。
- en: Figure 9.2\. Because the loss works both ways, we can now reproduce not just
    images from summer to winter, but also from winter to summer. If G is our Generator
    from A to B, and F is our Generator from B to A, then ![](../Images/acap2.jpg).
  id: totrans-968
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.2。因为损失是双向的，我们现在不仅可以从夏季到冬季再现图像，还可以从冬季到夏季再现。如果G是从A到B的生成器，F是从B到A的生成器，那么![图片](../Images/acap2.jpg)。
- en: '![](../Images/09fig02_alt.jpg)'
  id: totrans-969
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/09fig02_alt.jpg)'
- en: '(Source: Jun-Yan Zhu et al., 2017, [https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf).'
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：Jun-Yan Zhu等人，2017年，[https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf)）
- en: A common analogy is thinking about the process of *back-translation*—a sentence
    in Chinese that is translated to English and then back again to Chinese should
    give back the same sentence. If not, we can measure the cycle-consistency loss
    by how much the first and the third sentences differ.
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的类比是思考*回译*的过程——一个中文句子翻译成英文，然后再翻译回中文应该给出相同的句子。如果不是这样，我们可以通过第一句和第三句之间的差异来衡量循环一致性损失。
- en: 'To be able to use the cycle-consistency loss, we need to have two Generators:
    one translating from A to B, called *G[AB]*, sometimes referred to as simply *G*,
    and then another one translating from B to A, called *G[BA]*, referred to as *F*
    for brevity. There are technically two losses—forward cycle-consistency loss and
    backward cycle-consistency loss—but because all they mean is that ![](../Images/acap1.jpg)
    as well as ![](../Images/bacap.jpg), you may think of these as essentially the
    same, but off by one.'
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: 要能够使用循环一致性损失，我们需要有两个生成器：一个将A翻译到B，称为*G[AB]*，有时简单地称为*G*，然后另一个将B翻译到A，称为*G[BA]*，简称*F*。技术上存在两个损失——正向循环一致性损失和反向循环一致性损失——但因为他们本质上意味着![图片](../Images/acap1.jpg)以及![图片](../Images/bacap.jpg)，你可以认为这些基本上是相同的，但差一个。
- en: 9.3\. Adversarial loss
  id: totrans-973
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3. 对抗损失
- en: In addition to the cycle-consistency loss, we still have the *adversarial loss*.
    Every translation with a Generator *G[AB]* has a corresponding Discriminator *D[B]*,
    and *G[BA]* has Discriminator *D[A]*. The way to think about it is that we are
    always testing, when translating *to* domain A, whether the picture looks real;
    hence we use *D[A]* and vice versa.
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
  zh: 除了循环一致性损失之外，我们仍然有*对抗损失*。每个由生成器*G[AB]*进行的翻译都有一个相应的判别器*D[B]*，而*G[BA]*有判别器*D[A]*。可以这样考虑，我们在翻译到域A时总是测试图片是否看起来真实；因此我们使用*D[A]*，反之亦然。
- en: This is the same idea as with simpler architectures, but now, because of the
    two losses, we have two Discriminators. We need to make sure that not only the
    translation from apple to orange looks real, but also that the translation from
    our estimated orange back to reconstructed apple looks real. Recall that the adversarial
    loss ensures that the images look real, and as a result, it is still key for the
    CycleGAN to work. Hence adversarial loss is presented as second. The first Discriminator
    in the cycle is especially important—otherwise, we’d simply get noise that would
    help the GAN memorize what it should reconstruct.^([[2](#ch092)])
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
  zh: 这与更简单的架构中的想法相同，但现在，由于有两个损失，我们有两个判别器。我们需要确保不仅从苹果到橙子的转换看起来真实，而且从我们估计的橙子回到重建的苹果的转换也看起来真实。回想一下，对抗性损失确保图像看起来真实，因此它仍然是
    CycleGAN 工作的关键。因此，对抗性损失被列为第二。循环中的第一个判别器尤为重要——否则，我们只会得到帮助 GAN 记忆它应该重建的噪声。[^([3](#ch093))]
    [图 9.3](#ch09fig03) 展示了身份丢失的效果。
- en: ²
  id: totrans-976
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-977
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In practice, this is a little bit more complicated and would depend on, for
    example, whether you include both forward and backward cyclical loss. But you
    may use this as a mental model for how to think of the importance of the adversarial
    loss—remembering that we have both mappings A-B-A and B-A-B, so both Discriminators
    get to be the first one at some point.
  id: totrans-978
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在实践中，这要复杂一些，这取决于例如你是否包括正向和反向循环损失。但你可以将此作为一个心理模型来思考对抗性损失的重要性——记住我们有两个映射 A-B-A
    和 B-A-B，所以两个判别器都可能在某个时刻成为第一个。
- en: 9.4\. Identity loss
  id: totrans-979
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4\. 身份丢失
- en: 'The idea of *identity loss* is simple: we want to enforce that CycleGAN preserves
    the overall color structure (or *temperature*) of the picture. So we introduce
    a regularization term that helps us keep the tint of the picture consistent with
    the original image. Imagine this as a way of ensuring that even after applying
    many filters onto your image, you still can recover the original image.'
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: '*身份丢失* 的概念很简单：我们希望 CycleGAN 保留图片的整体色彩结构（或 *温度*）。因此，我们引入了一个正则化项，帮助我们保持图片色调与原始图像的一致性。想象一下，这是一种确保即使在你对图片应用了许多滤镜之后，你仍然可以恢复原始图片的方法。'
- en: 'This is done by feeding the images already in domain A to the Generator from
    B to A (*G[BA]*), because the CycleGAN should understand that they are already
    in the correct domain. In other words, we penalize unnecessary changes to the
    image: if we feed in a zebra and are trying to “zebrafy” an image, we get the
    same zebra back, as there is nothing to do.^([[3](#ch093)]) [Figure 9.3](#ch09fig03)
    illustrates the effects of identity loss.'
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过将域 A 中的图像输入到从 B 到 A 的生成器 (*G[BA]*) 中来实现的，因为 CycleGAN 应该理解它们已经在正确的域中。换句话说，我们惩罚对图像的不必要更改：如果我们输入一只斑马并试图“斑马化”一张图片，我们会得到同样的斑马，因为没有其他事情要做。[图
    9.3](#ch09fig03) 展示了身份丢失的效果。
- en: ³
  id: totrans-982
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-983
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jun Yan Zhu et al., 2017, [https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf).
    More at [http://mng.bz/loE8](http://mng.bz/loE8).
  id: totrans-984
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Jun Yan Zhu 等人，2017，[https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf)。更多信息请访问
    [http://mng.bz/loE8](http://mng.bz/loE8)。
- en: 'Figure 9.3\. A picture is worth a thousand words to clarify the effects of
    identity loss: there is a clear tint in the cases without identity loss, and since
    there seems to be no reason for it, so we try to penalize this behavior. Even
    in black and white, you should be able to see the difference. However, to see
    the full extent of it, check out the full-color version online.'
  id: totrans-985
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.3\. 一张图片胜过千言万语，用以阐明身份丢失的效果：在没有身份丢失的情况下，图片有明显的色调，由于似乎没有理由，所以我们试图惩罚这种行为。即使在黑白图片中，你也应该能够看到差异。然而，要看到它的全部效果，请查看在线的全彩版本。
- en: '![](../Images/09fig03_alt.jpg)'
  id: totrans-986
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/09fig03_alt.jpg)'
- en: Even though identity loss is not, strictly speaking, required for the CycleGAN
    to work, we include it for completeness. Both our implementation and the CycleGAN
    authors’ latest implementation contain it, because frequently this adjustment
    leads to empirically better results and enforces a constraint that seems reasonable.
    But even the CycleGAN paper itself mentions it only briefly as a seeming ex-post
    justification, so we do not cover it extensively.
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管从严格意义上讲，身份丢失不是 CycleGAN 工作的必要条件，但我们为了完整性而包含它。我们的实现和 CycleGAN 作者的最新实现都包含它，因为这种调整通常会导致经验上更好的结果，并强制执行一个看似合理的约束。但即使
    CycleGAN 论文本身也只简要地将其作为似乎事后的合理化，所以我们没有对其进行深入探讨。
- en: '[Table 9.1](#ch09table01) summarizes the losses you’ve learned about in this
    chapter.'
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 9.1](#ch09table01) 总结了本章中你学到的损失。'
- en: Table 9.1\. Losses
  id: totrans-989
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 9.1\. 损失
- en: '|   | Calculation | Measures | Ensures |'
  id: totrans-990
  prefs: []
  type: TYPE_TB
  zh: '|   | 计算 | 衡量 | 确保 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-991
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Adversarial loss | *L[GAN] (G,D[B],B,A)* = *E[b~p(b)]*[*logD[B]*(*b*)] *+
    E[a~p(a)]*[*log*(*1-D[B]*(*G[AB]*(*a*))] (This is just the good old NS-GAN presented
    in [chapter 5](../Text/kindle_split_015.xhtml#ch05).) | As in previous cases,
    the loss measures two terms: first is the likelihood of a given image being the
    real one rather than the translated image. Second is the part where the Generator
    may get to fool the Discriminator. Note that this formulation is only for *D[B]*,
    with equivalent *D[A]* that comes into the final loss. | That the translated images
    look real, sharp, and indistinguishable from the real ones. |'
  id: totrans-992
  prefs: []
  type: TYPE_TB
  zh: '| 对抗性损失 | *L[GAN] (G,D[B],B,A)* = *E[b~p(b)]*[*logD[B]*(*b*)] *+ E[a~p(a)]*[*log*(*1-D[B]*(*G[AB]*(*a*))]
    (这是在第5章中介绍的古老的好NS-GAN。) | 与之前的情况一样，损失衡量两个术语：第一个是给定图像是真实图像而不是翻译图像的可能性。第二个是生成器可能欺骗判别器的地方。请注意，这个公式仅适用于*D[B]*，等效的*D[A]*将在最终损失中体现。
    | 翻译图像看起来真实、清晰，与真实图像无法区分。|'
- en: '| Cycle-consistency loss: forward pass | Difference between *a* and ![](../Images/acap.jpg)
    (denoted by ![](../Images/acpu.jpg)^([[a](#ch09tn01)]) | The difference between
    the images from the original domain *a* and the twice-translated images ![](../Images/acap.jpg).
    | That the original image and the twice-translated image are the same. If this
    fails, we may not have a coherent mapping A-B-A. |'
  id: totrans-993
  prefs: []
  type: TYPE_TB
  zh: '| 循环一致性损失：正向传播 | *a*和![正向传播](../Images/acap.jpg)之间的差异（用![正向传播](../Images/acpu.jpg)^([[a](#ch09tn01)])表示）
    | 原始域*a*中的图像和两次翻译后的图像![正向传播](../Images/acap.jpg)之间的差异。 | 原始图像和两次翻译后的图像是相同的。如果这失败了，我们可能没有连贯的映射A-B-A。|'
- en: '| Cycle-consistency loss: backward pass | ![](../Images/bap.jpg) | The difference
    between the images from the original domain *b* and the twice-translated images
    ![](../Images/bacp1.jpg). | That the original image and the twice-translated image
    are the same. If this fails, we may not have a coherent mapping B-A-B. |'
  id: totrans-994
  prefs: []
  type: TYPE_TB
  zh: '| 循环一致性损失：反向传播 | ![反向传播](../Images/bap.jpg) | 原始域*b*中的图像和两次翻译后的图像![反向传播](../Images/bacp1.jpg)之间的差异。
    | 原始图像和两次翻译后的图像是相同的。如果这失败了，我们可能没有连贯的映射B-A-B。|'
- en: '| Overall loss | *L = L[GAN]*(*G,D[B],A,B*) + *L[GAN]*(*F,D[A],B,A*) + *λ*[cyc](*G*,*F*)
    | All of the four losses combined (2× adversarial because of two Generators) plus
    cyclical loss: forward and backward in one term. | That the overall translation
    is photorealistic and makes sense (provides matching pictures). |'
  id: totrans-995
  prefs: []
  type: TYPE_TB
  zh: '| 整体损失 | *L = L[GAN]*(*G,D[B],A,B*) + *L[GAN]*(*F,D[A],B,A*) + *λ*[cyc](*G*,*F*)
    | 所有四个损失相结合（由于两个生成器，因此是2×对抗性）加上循环损失：一个术语中的正向和反向。 | 整体翻译是逼真的，并且有意义（提供匹配的图片）。|'
- en: '| Identity loss (outside the overall loss, for consistency with the CycleGAN
    paper notation) | *L*[identity] = *E[a~p(a)]*[&#124;&#124; *G[BA]*(*a*) – *a*
    &#124;&#124;] *+ E[b~p(b)]* [&#124;&#124; *G[AB]* (*b*) – *b* &#124;&#124;] |
    The difference between the image in *B* and *G*[AB](*b*) and vice versa. | That
    the CycleGAN changes parts of the image only when it needs to. |'
  id: totrans-996
  prefs: []
  type: TYPE_TB
  zh: '| 标识损失（在整体损失之外，为了与CycleGAN论文的符号保持一致） | *L*[identity] = *E[a~p(a)]*[&#124;&#124;
    *G[BA]*(*a*) – *a* &#124;&#124;] *+ E[b~p(b)]* [&#124;&#124; *G[AB]* (*b*) – *b*
    &#124;&#124;] | *B*中的图像与*G*[AB](*b*)之间的差异以及反之亦然。 | CycleGAN仅在需要时才更改图像的某些部分。|'
- en: ^a
  id: totrans-997
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^a
- en: ''
  id: totrans-998
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This notation may be unfamiliar to some, but it represents the L1 norm between
    the two items. For simplicity, you may think of this as for each pixel, an absolute
    difference between it and the corresponding pixel on the reconstructed image.
  id: totrans-999
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这种符号可能对一些人来说不熟悉，但它代表了两个项目之间的L1范数。为了简单起见，你可以将其视为对于每个像素，它与重建图像上相应像素之间的绝对差异。
- en: 9.5\. Architecture
  id: totrans-1000
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5. 架构
- en: The CycleGAN setup builds directly on the CGAN architecture and is, in essence,
    two CGANs joined together—or, as the CycleGAN authors themselves point out, an
    autoencoder. Recall from [chapter 2](../Text/kindle_split_011.xhtml#ch02) that
    we had an input image *x* and the reconstructed image *x**, which was the result
    of reconstruction after being fed through the latent space *z*; see [figure 9.4](#ch09fig04).
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN的设置直接基于CGAN架构，本质上是由两个CGAN结合在一起——或者，正如CycleGAN的作者们自己指出的，是一个自动编码器。回顾[第2章](../Text/kindle_split_011.xhtml#ch02)，我们有一个输入图像*x*和重建图像*x**，这是在通过潜在空间*z*重建后的结果；参见[图9.4](#ch09fig04)。
- en: Figure 9.4\. In this image of an autoencoder from [chapter 2](../Text/kindle_split_011.xhtml#ch02),
    we used the analogy of compressing (step 1) a human concept into a more compact
    written form in a letter (step 2) and then expanding this concept out to the (imperfect)
    idea of the same notion in someone else’s head (step 3).
  id: totrans-1002
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.4。在这张来自[第2章](../Text/kindle_split_011.xhtml#ch02)的自动编码器图像中，我们使用了将人类概念（步骤1）压缩成更紧凑的书面形式在信件中（步骤2），然后将其扩展到（不完美的）同一概念在别人头脑中的想法（步骤3）的类比。
- en: '![](../Images/09fig04_alt.jpg)'
  id: totrans-1003
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5](../Images/09fig04_alt.jpg)'
- en: To translate this diagram into the CycleGAN’s world, *a* is an image in the
    A domain, *b* is an image in B, and ![](../Images/acap.jpg) is reconstructed A.
    In CycleGAN’s case, however, we are dealing with a latent space—step 2—of equal
    dimensionality. It just happens to be another meaningful domain (B) that the CycleGAN
    has to find. Even with the autoencoder, the latent space was just another domain,
    though it was not as easily interpretable.
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这个图转换为CycleGAN的世界，*a*是域A中的一个图像，*b*是域B中的一个图像，而![图9.4](../Images/acap.jpg)是重建的A。然而，在CycleGAN的情况下，我们处理的是一个具有相同维度的潜在空间——步骤2。这恰好是CycleGAN必须找到的另一个有意义的域（B）。即使对于自动编码器，潜在空间也只是另一个域，尽管它并不容易解释。
- en: 'Compared to what we know from [chapter 2](../Text/kindle_split_011.xhtml#ch02),
    the main new concept is the introduction of the adversarial losses. These and
    many other mixtures of autoencoders and GANs are an active area of research in
    themselves! So that is also a good area for interested researchers. But for now,
    think of the two mappings as two autoencoders: *F*(*G*(*a*)) and *G*(*F*(*b*)).
    We take the basic idea of the autoencoder—including a kind of *explicit* loss
    function as substituted by the cycle-consistency loss—and add Discriminators to
    it. The two Discriminators, one at each step, ensure that both translations (including
    into the kind of *latent space*) look like real images in their respective domains.'
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在[第2章](../Text/kindle_split_011.xhtml#ch02)中了解的内容相比，主要的新概念是引入了对抗性损失。这些以及许多其他自动编码器和GAN的混合体本身就是一个活跃的研究领域！因此，这也是一个对感兴趣的研究者来说很好的领域。但就目前而言，将这两个映射视为两个自动编码器：*F*(*G*(*a*))和*G*(*F*(*b*))。我们采用了自动编码器的基本思想——包括一种*显式*的损失函数，由循环一致性损失所代替——并在此基础上添加了判别器。两个判别器，每个步骤一个，确保两种转换（包括进入*潜在空间*）在其各自的域中看起来都像真实的图像。
- en: '9.5.1\. CycleGAN architecture: building the network'
  id: totrans-1006
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.5.1. CycleGAN架构：构建网络
- en: 'Before we jump into the actual implementation of the CycleGAN, let’s briefly
    look at the overall simplified implementation depicted in [figure 9.5](#ch09fig05).
    There are two flows: in the top diagram, the flow A-B-A starts from an image in
    domain A, and in the bottom diagram, the flow B-A-B starts with an image in domain
    B.'
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入CycleGAN的实际实现之前，让我们简要地看一下[图9.5](#ch09fig05)中描述的整体简化实现。有两个流程：在上面的图中，流程A-B-A从一个域A的图像开始，在下面的图中，流程B-A-B从一个域B的图像开始。
- en: Figure 9.5\. In this simplified architecture of the CycleGAN, we start with
    the input image, which either (1) goes to the Discriminator for evaluation or
    (2) is translated to one domain, evaluated by the other Discriminator, and then
    translated back.
  id: totrans-1008
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.5。在这个CycleGAN的简化架构中，我们从一个输入图像开始，这个图像要么（1）被送到判别器进行评估，要么（2）被转换到另一个域，由另一个判别器评估，然后转换回来。
- en: '![](../Images/09fig05_alt.jpg)'
  id: totrans-1009
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5](../Images/09fig05_alt.jpg)'
- en: '(Source: “Understanding and Implementing CycleGAN in TensorFlow,” by Hardik
    Bansal and Archit Rathore, 2017, [https://hardikbansal.github.io/CycleGANBlog/](https://hardikbansal.github.io/CycleGANBlog/).)'
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：“在TensorFlow中理解和实现CycleGAN”，作者Hardik Bansal和Archit Rathore，2017年，[https://hardikbansal.github.io/CycleGANBlog/](https://hardikbansal.github.io/CycleGANBlog/)。）
- en: 'The image then follows two paths: it is (1) fed to the Discriminator to get
    our decision as to whether it is real or not, and (2) (i) fed to the Generator
    to translate it to B, then (ii) evaluated by the Discriminator B to see if it
    looks real in domain B, and eventually (iii) translated back to A to allow us
    to measure the cyclic loss.'
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，图像遵循两条路径：它（1）被送入判别器以获取我们对其是否真实的判断，并且（2）（i）被送入生成器以将其转换为B，然后（ii）由判别器B评估以查看它是否在域B中看起来像真实的图像，最终（iii）转换回A以允许我们测量循环损失。
- en: The bottom image is basically an *off-by-one* cycle of the top image and follows
    all the same fundamental steps. We’ll use the apple2orange dataset, but many other
    datasets are available, including the famous horse2zebra dataset, which you can
    easily use by making a slight modification to the code and downloading the data
    by using the bash script provided.
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: 底部图像基本上是顶部图像的一个*偏移一个周期*，遵循所有相同的根本步骤。我们将使用apple2orange数据集，但还有许多其他数据集可用，包括著名的horse2zebra数据集，你可以通过稍微修改代码并使用提供的bash脚本来下载数据轻松使用。
- en: To summarize [figure 9.5](#ch09fig05) in another representation for further
    clarity, [table 9.2](#ch09table02) reviews all four major networks.
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清晰地总结[图9.5](#ch09fig05)，[表9.2](#ch09table02)回顾了所有四个主要网络。
- en: Table 9.2\. Networks
  id: totrans-1014
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表9.2. 网络
- en: '|   | Input | Output | Goal |'
  id: totrans-1015
  prefs: []
  type: TYPE_TB
  zh: '|    | 输入 | 输出 | 目标 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-1016
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Generator: from A to B | We load either a real picture from A or a translation
    from B to A. | We translate it to domain B. | Try to create realistic-looking
    images in domain B. |'
  id: totrans-1017
  prefs: []
  type: TYPE_TB
  zh: '| 生成器：从A到B | 我们加载来自A的真实图片或从B到A的翻译。 | 我们将其翻译到域B。 | 尝试在域B中创建看起来逼真的图像。 |'
- en: '| Generator: from B to A | We load either a real picture from B or a translation
    from A to B. | We translate it to domain A. | Try to create realistic-looking
    images in domain A. |'
  id: totrans-1018
  prefs: []
  type: TYPE_TB
  zh: '| 生成器：从B到A | 我们加载来自B的真实图片或从A到B的翻译。 | 我们将其翻译到域A。 | 尝试在域A中创建看起来逼真的图像。 |'
- en: '| Discriminator A | We provide a picture in the A domain—either translated
    or real. | The probability that the picture is real. | Try to not get fooled by
    the Generator from B to A. |'
  id: totrans-1019
  prefs: []
  type: TYPE_TB
  zh: '| A域判别器 | 我们提供一个A域的图片——无论是翻译的还是真实的。 | 图片为真实的概率。 | 尝试不要被从B到A的生成器欺骗。 |'
- en: '| Discriminator B | We provide a picture in the B domain—either translated
    or real. | The probability that the picture is real. | Try to not get fooled by
    the Generator from A to B. |'
  id: totrans-1020
  prefs: []
  type: TYPE_TB
  zh: '| B域判别器 | 我们提供一个B域的图片——无论是翻译的还是真实的。 | 图片为真实的概率。 | 尝试不要被从A到B的生成器欺骗。 |'
- en: 9.5.2\. Generator architecture
  id: totrans-1021
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.5.2. 生成器架构
- en: '[Figure 9.6](#ch09fig06) shows the architecture of the Generator. We have re-created
    the diagram by using the variable names from our code and included the shapes
    for your benefit. This is an example of a *U-Net* architecture, because when you
    draw it in a way that each resolution gets its own level, the network looks like
    a U.'
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9.6](#ch09fig06)展示了生成器的架构。我们使用代码中的变量名重新创建了图表，并包括了形状供您参考。这是一个*U-Net*架构的例子，因为当你以这种方式绘制它，使得每个分辨率都有自己的级别时，网络看起来像字母U。'
- en: Figure 9.6\. Architecture of the Generator. The generator itself has a contraction
    path (d0 to d3) and expanding path (u1 to u4). The contraction and expanding paths
    are sometimes referred to as encoder and decoder, respectively.
  id: totrans-1023
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.6. 生成器的架构。生成器本身有一个收缩路径（d0到d3）和一个扩展路径（u1到u4）。收缩和扩展路径有时分别被称为编码器和解码器。
- en: '![](../Images/09fig06_alt.jpg)'
  id: totrans-1024
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09fig06_alt.jpg)'
- en: 'A couple of things to note here:'
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个需要注意的事项：
- en: We are using standard convolutional layers in the encoder.
  id: totrans-1026
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在编码器中使用标准的卷积层。
- en: From those, we create *skip connections* so that the information has an easier
    time propagating through the network. In the figure, this is denoted by the outlines
    and color-coding between the d0 to d3 and u1 to u4, respectively. You can see
    that half of the blocks in the decoder are coming from those skip connections
    (notice double the number of feature maps!).^([[4](#ch094)])
  id: totrans-1027
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从这些中，我们创建*跳过连接*，以便信息更容易在网络中传播。在图中，这分别用d0到d3和u1到u4之间的轮廓和颜色编码表示。你可以看到解码器中一半的块来自这些跳过连接（注意特征图数量翻倍！）^([[4](#ch094)])
- en: ⁴
  id: totrans-1028
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-1029
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: As you will see, this just means we concatenate the entire block/tensor to the
    equivalently colored tensor in the decoder part of the Generator.
  id: totrans-1030
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 正如你将看到的，这仅仅意味着我们将整个块/张量连接到解码器部分的相应颜色张量。
- en: The decoder uses deconvolutional layers with one final convolutional layer to
    upscale the image into the equivalent size of the original image.
  id: totrans-1031
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器使用反卷积层，加上一个最终的卷积层，将图像上采样到与原始图像等效的大小。
- en: 'The autoencoder is a useful teaching tool for the architecture of the Generator
    alone as well, because the Generator has an encoder-decoder architecture:'
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
  zh: 由于生成器具有编码器-解码器架构，因此自编码器对于生成器架构本身也是一个有用的教学工具：
- en: '***Encoder—*** Step 1 from [figure 9.4](#ch09fig04): these are the convolutional
    layers that reduce the resolution of each feature map (*layer* or *slice*). This
    is the contraction path (d0 to d3).'
  id: totrans-1033
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***编码器—*** 第1步来自[图9.4](#ch09fig04)：这些是减少每个特征图分辨率（*层*或*切片*）的卷积层。这是收缩路径（d0到d3）。'
- en: '***Decoder—*** Step 3 from [figure 9.4](#ch09fig04): these are the *deconvolutional*
    layers (transposed convolutions) that upscale the image back to 128 × 128\. This
    is the expansion path (u1 to u4).'
  id: totrans-1034
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***解码器—*** 第3步来自[图9.4](#ch09fig04)：这些是*反卷积*层（转置卷积），将图像放大回128 × 128。这是扩展路径（u1到u4）。'
- en: To clarify, the autoencoder model here is useful in two ways. First, the overall
    CycleGAN architecture can be viewed as training two autoencoders.^([[5](#ch095)])
    Second, the U-Net itself has parts referred to as *encoder* and *decoder*.
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确，这里的自动编码器模型在两个方面是有用的。首先，整个CycleGAN架构可以看作是训练了两个自动编码器。[^([[5](#ch095))] 第二，U-Net本身有被称为*编码器*和*解码器*的部分。
- en: ⁵
  id: totrans-1036
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵
- en: ''
  id: totrans-1037
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Jun-Yan Zhu et al., 2017, [https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf).
  id: totrans-1038
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Jun-Yan Zhu等人，2017年，[https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf)。
- en: You may also be a bit puzzled by the downscaling and the subsequent upscaling,
    but this is just so that we compress the image to the most meaningful representation,
    but at the same time are able to add back all the detail. It’s the same reasoning
    as with the autoencoder, except now we also have a path to remember the nuances.
    This architecture—the *U-Net architecture*—has just been empirically shown in
    several domains as better performing on various segmentation tasks. The key idea
    is that although during downsampling we can focus on classification and understanding
    of large regions, including higher-resolution skip connections preserves the detail
    that can then be accurately segmented.
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能对降采样和随后的升采样也感到有些困惑，但这只是为了将图像压缩到最有意义的表示，同时还能添加回所有细节。这与自动编码器的推理相同，但现在我们还有一个路径来记住细微差别。这种架构——*U-Net架构*——已经在几个领域被实证证明在多种分割任务上表现更好。关键思想是，尽管在降采样过程中我们可以专注于分类和理解大区域，包括更高分辨率的跳过连接可以保留可以准确分割的细节。
- en: In our implementation of CycleGAN, we’ll use the U-Net architecture with skip
    connections as shown in [figure 9.6](#ch09fig06), which is more readable. However,
    many CycleGAN implementations use the ResNet architecture, which you can implement
    yourself with a bit more work.
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的CycleGAN实现中，我们将使用带有跳过连接的U-Net架构，如图[图9.6](#ch09fig06)所示，这更易于阅读。然而，许多CycleGAN实现使用ResNet架构，你可以通过一些额外的工作自己实现。
- en: '|  |'
  id: totrans-1041
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-1042
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The main advantage of ResNet is that it uses fewer parameters and introduces
    a step in the middle called *transformer*, which has residual connections in lieu
    of our encoder-decoder skip connections.
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet的主要优势是它使用了更少的参数，并在中间引入了一个称为*transformer*的步骤，该步骤使用残差连接代替我们的编码器-解码器跳过连接。
- en: '|  |'
  id: totrans-1044
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Based on our testing, at least on the dataset used, the apple2orange results
    remain the same. Instead of explicitly defining the transformer, we provide skip
    connections (as used in the diagram) from the convolutional to the deconvolutional
    layers. We will mention these similarities again in code. For now, just remember
    that.
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的测试，至少在所使用的数据集上，apple2orange的结果保持不变。我们不是明确地定义了transformer，而是提供了从卷积层到反卷积层的跳过连接（如图中所示）。我们将在代码中再次提及这些相似之处。现在，只需记住这一点。
- en: 9.5.3\. Discriminator architecture
  id: totrans-1046
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.5.3. 判别器架构
- en: The CycleGAN’s Discriminator is based on the PatchGAN architecture—we will dive
    into the technical details in the code section. One thing that may be confusing
    is that we do not get a single float as an output of this Discriminator, but rather
    a set of single-channel values that may be thought of as a set of mini-discriminators
    that we then average together.
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN的判别器基于PatchGAN架构——我们将在代码部分深入技术细节。可能令人困惑的一点是，我们从这个判别器得到的不是一个浮点数作为输出，而是一组单通道值，可以将其视为一组迷你判别器，然后我们将它们平均在一起。
- en: Ultimately, this allows the design of the CycleGAN to be fully convolutional,
    meaning that it can scale relatively easily to higher resolutions. Indeed, in
    the examples of translating video games to reality or vice versa, the CycleGAN
    authors have used an upscaled version of the CycleGAN, with only minor modifications
    thanks to the fully convolutional design. Other than that, the Discriminator should
    be a relatively straightforward implementation of the Discriminators you have
    seen before, except there are now two of them.
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，这允许CycleGAN的设计完全卷积，意味着它可以相对容易地扩展到更高的分辨率。确实，在将视频游戏转换为现实或反之亦然的例子中，CycleGAN的作者使用了CycleGAN的升级版本，由于完全卷积的设计，只有微小的修改。除此之外，判别器应该是对你之前见过的判别器的一个相对直接的实现，但现在有两个。
- en: 9.6\. Object-oriented design of GANs
  id: totrans-1049
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.6\. GANs的面向对象设计
- en: We have always used objects in TensorFlow and object-oriented programming (OOP)
    in our code, but we have usually treated the architectures more functionally,
    because they were generally simple. In the CycleGAN’s case, the architecture is
    complex, and as a result, we need a structure that allows us to keep accessing
    the original attributes and methods that we have defined. As a result, we will
    write out the CycleGAN as a Python class of its own with methods to build the
    Generator and Discriminator, and run the training.
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在TensorFlow和面向对象编程（OOP）中始终使用对象，但在我们的代码中通常更功能性地处理架构，因为它们通常是简单的。在CycleGAN的情况下，架构是复杂的，因此我们需要一个结构，允许我们继续访问我们定义的原始属性和方法。因此，我们将CycleGAN编写为一个具有构建生成器和判别器以及运行训练的方法的Python类。
- en: '9.7\. Tutorial: CycleGAN'
  id: totrans-1051
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.7\. 教程：CycleGAN
- en: In this tutorial, we’ll use the Keras-GAN implementation and use Keras with
    a TensorFlow backend.^([[6](#ch096)]) Tested as late as Keras 2.2.4 and TensorFlow
    1.12.0, `Keras_contrib` was installed from the hash 46fcdb9384b3bc9399c651b2b43640aa54098e64\.
    This time, we have to use a different dataset (also to show you that despite our
    joke from [chapter 2](../Text/kindle_split_011.xhtml#ch02), we *do know* other
    datasets). But for educational purposes, we will keep using one of the simpler
    datasets—apple2orange. Let’s jump right into it by doing all our usual imports,
    as shown in the following listing.
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将使用Keras-GAN实现，并使用具有TensorFlow后端的Keras。^([[6](#ch096)]) 在Keras 2.2.4和TensorFlow
    1.12.0的最新测试中，`Keras_contrib`是从hash 46fcdb9384b3bc9399c651b2b43640aa54098e64安装的。这次，我们必须使用不同的数据集（也为了向你展示，尽管我们在第2章（../Text/kindle_split_011.xhtml#ch02）中的玩笑，但我们*确实*知道其他数据集）。但出于教育目的，我们将继续使用其中一个较简单的数据集——apple2orange。让我们直接进入正题，按照以下列表所示进行所有我们通常的导入。
- en: ⁶
  id: totrans-1053
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶
- en: ''
  id: totrans-1054
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See the Keras-GAN GitHub repository by Erik Linder-Norén, 2017, [https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN).
  id: totrans-1055
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 查看Erik Linder-Norén于2017年的Keras-GAN GitHub仓库，[https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN)。
- en: Listing 9.1\. Import all the things
  id: totrans-1056
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.1\. 导入所有内容
- en: '[PRE27]'
  id: totrans-1057
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As promised, we’ll use the object-oriented style of programming. In the following
    listing, we create a CycleGAN class with all the initializing parameters, including
    the data loader. The data loader is defined in the GitHub repository for our book.
    It simply loads the preprocessed data.
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: 如同承诺的那样，我们将使用面向对象的编程风格。在下面的列表中，我们创建了一个包含所有初始化参数的CycleGAN类，包括数据加载器。数据加载器定义在我们的书籍GitHub仓库中。它简单地加载预处理后的数据。
- en: Listing 9.2\. Starting the CycleGAN class
  id: totrans-1059
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.2\. 开始CycleGAN类
- en: '[PRE28]'
  id: totrans-1060
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '***1* Input shape**'
  id: totrans-1061
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 输入形状**'
- en: '***2* Configures data loader**'
  id: totrans-1062
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 配置数据加载器**'
- en: '***3* Uses the DataLoader object to import a preprocessed dataset**'
  id: totrans-1063
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 使用DataLoader对象导入预处理后的数据集**'
- en: '***4* Calculates output shape of D (PatchGAN)**'
  id: totrans-1064
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 计算D（PatchGAN）的输出形状**'
- en: '***5* Number of filters in the first layer of G**'
  id: totrans-1065
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* G的第一层中的过滤器数量**'
- en: '***6* Number of filters in the first layer of D**'
  id: totrans-1066
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* D的第一层中的过滤器数量**'
- en: '***7* Cycle-consistency loss weight**'
  id: totrans-1067
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 循环一致性损失权重**'
- en: '***8* Identity loss weight**'
  id: totrans-1068
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 标识损失权重**'
- en: 'Two new terms are `lambda_cycle` and `lambda_id`. The second hyperparameter
    influences identity loss. The CycleGAN authors themselves note that this value
    influences how dramatic the changes are—especially early in the training process.^([[7](#ch097)])
    Setting a lower value leads to unnecessary changes: for example, completely inverting
    the colors early on. We have selected this value, based on rerunning the training
    process for apple2orange several times. Frequently, the process is theory-driven
    alchemy.'
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: 两个新术语是 `lambda_cycle` 和 `lambda_id`。第二个超参数影响身份损失。CycleGAN 作者本人指出，此值影响变化程度——尤其是在训练过程的早期。[7](#ch097)
    设置较低值会导致不必要的改变：例如，早期完全反转颜色。我们根据多次重新运行 apple2orange 训练过程选择了此值。通常，这个过程是理论驱动的炼金术。
- en: ⁷
  id: totrans-1070
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷
- en: ''
  id: totrans-1071
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “pytorch-CycleGAN-and-pix2pix Frequently Asked Questions,” by Jun-Yan Zhu,
    April 2019, [http://mng.bz/BY58](http://mng.bz/BY58).
  id: totrans-1072
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请参阅 Jun-Yan Zhu 的“pytorch-CycleGAN-and-pix2pix 常见问题解答”，2019 年 4 月，[http://mng.bz/BY58](http://mng.bz/BY58)。
- en: The first hyperparameter—`lambda_cycle`—controls how strictly the cycle-consistency
    loss is enforced. Setting this value higher will ensure that your original and
    reconstructed images are as close together as possible.
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个超参数—`lambda_cycle`—控制循环一致性损失的执行严格程度。设置此值较高将确保您的原始图像和重建图像尽可能接近。
- en: 9.7.1\. Building the network
  id: totrans-1074
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.7.1\. 构建网络
- en: 'So now that we have our basic parameters out of the way, we will build the
    basic network, as shown in [listing 9.3](#ch09ex03). We will start from the high-level
    view and move down. This entails the following:'
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们已经处理好了基本参数，我们将构建基本网络，如图 [清单 9.3](#ch09ex03) 所示。我们将从高层次视图开始，逐步深入。这包括以下内容：
- en: Creating the two Discriminators *D[A]* and *D[B]* and compiling them
  id: totrans-1076
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个判别器 *D[A]* 和 *D[B]* 并编译它们
- en: 'Creating the two Generators:'
  id: totrans-1077
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个生成器：
- en: Instantiating *G[AB]* and *G[BA]*
  id: totrans-1078
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化 *G[AB]* 和 *G[BA]*
- en: Creating placeholders for the image input for both directions
  id: totrans-1079
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为两个方向的图像输入创建占位符
- en: Linking them both to an image in the other domain
  id: totrans-1080
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将它们都链接到另一个域中的图像
- en: Creating placeholders for the reconstructed images back in the original domain
  id: totrans-1081
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为在原始域中重建的图像创建占位符
- en: Creating the identity loss constraint for both directions
  id: totrans-1082
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为两个方向创建身份损失约束
- en: Not making the parameters of the Discriminators trainable for now
  id: totrans-1083
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目前不使判别器的参数可训练
- en: Compiling the two Generators
  id: totrans-1084
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译两个生成器
- en: Listing 9.3\. Building the networks
  id: totrans-1085
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清单 9.3\. 构建网络
- en: '[PRE29]'
  id: totrans-1086
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '***1* Builds and compiles the Discriminators**'
  id: totrans-1087
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 构建并编译判别器**'
- en: '***2* Beginning here, we construct the computational graph of the Generators.
    These first two lines build the Generators.**'
  id: totrans-1088
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 从这里开始，我们构建生成器的计算图。这两行首先构建生成器。**'
- en: '***3* Inputs images from both domains**'
  id: totrans-1089
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 从两个域获取图像**'
- en: '***4* Translates images to the other domain**'
  id: totrans-1090
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 将图像转换到另一个域**'
- en: '***5* Translates images back to original domain**'
  id: totrans-1091
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 将图像转换回原始域**'
- en: '***6* Identity mapping of images**'
  id: totrans-1092
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 图像的身份映射**'
- en: '***7* For the combined model, we will train only the Generators.**'
  id: totrans-1093
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 对于结合模型，我们只训练生成器。**'
- en: '***8* Discriminators determine validity of translated images**'
  id: totrans-1094
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 判别器确定转换图像的有效性**'
- en: '***9* Combined model trains Generators to fool Discriminators**'
  id: totrans-1095
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 结合模型训练生成器欺骗判别器**'
- en: 'One last thing to clarify from the preceding code: the outputs from the `combined`
    model come in lists of six. This is because we always get validities (from the
    Discriminator), reconstruction, and identity losses—one for A-B-A and one for
    the B-A-B cycle—hence six. The first two are squared errors, and the rest are
    mean absolute errors. The relative weights are influenced by the `lambda` factors
    described earlier.'
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点需要从前面的代码中澄清：`combined` 模型的输出以六个列表的形式出现。这是因为我们总是得到有效性（来自判别器）、重建和身份损失——一个用于
    A-B-A，一个用于 B-A-B 循环——因此是六个。前两个是平方误差，其余是平均绝对误差。相对权重受前面描述的 `lambda` 因素影响。
- en: 9.7.2\. Building the Generator
  id: totrans-1097
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.7.2\. 构建生成器
- en: 'Next, we build the Generator code in [listing 9.4](#ch09ex04), which uses the
    skip connections as we described in [section 9.5.2](#ch09lev2sec2). This is the
    U-Net architecture. This architecture is simpler to write than the ResNet architecture,
    which some implementations use. Within our Generator function we first define
    the helper functions:'
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在 [清单 9.4](#ch09ex04) 中构建生成器代码，该代码使用我们在 [第 9.5.2 节](#ch09lev2sec2) 中描述的跳跃连接。这是
    U-Net 架构。这个架构比一些实现中使用的 ResNet 架构更简单。在我们的生成器函数中，我们首先定义辅助函数：
- en: 'Define the `conv2d()` function as follows:'
  id: totrans-1099
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `conv2d()` 函数定义为如下：
- en: Standard 2D convolutional layer
  id: totrans-1100
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标准的2D卷积层
- en: Leaky ReLU activation
  id: totrans-1101
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Leaky ReLU激活
- en: Instance normalization^([[8](#ch098)])
  id: totrans-1102
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例归一化^([[8](#ch098)])
- en: ⁸
  id: totrans-1103
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸
- en: ''
  id: totrans-1104
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: Instance normalization is similar to the batch normalization in [chapter 4](../Text/kindle_split_013.xhtml#ch04),
    except that instead of normalizing based on information from the entire batch,
    we normalize each feature map within each channel separately. Instance normalization
    often results in better-quality images for tasks such as style transfer or image-to-image
    translation—just what we need for the CycleGAN!
  id: totrans-1105
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 实例归一化类似于第4章中的批归一化，除了我们不是基于整个批次的全部信息进行归一化，而是对每个通道中的每个特征图分别进行归一化。实例归一化通常会产生更好的图像质量，适用于风格迁移或图像到图像翻译等任务——这正是CycleGAN所需要的！
- en: 'Define the `deconv2d()` function as a transposed^([[9](#ch099)]) convolution
    (aka *deconvolution*) layer that does the following:'
  id: totrans-1106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`deconv2d()`函数作为一个转置卷积（即 *反卷积*）层，它执行以下操作：
- en: ⁹
  id: totrans-1107
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹
- en: ''
  id: totrans-1108
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: Here, *transposed convolution* is—some argue—a more correct term. However, just
    think of it as the opposite of convolution, or deconvolution.
  id: totrans-1109
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这里，*转置卷积*是——有人认为——一个更准确的术语。然而，只需将其视为卷积的相反，或反卷积。
- en: Upsamples the `input_layer`
  id: totrans-1110
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提升输入层`input_layer`
- en: Possibly applies dropout if we set the dropout rate
  id: totrans-1111
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们设置了dropout率，可能会应用dropout
- en: Always applies `InstanceNormalization`
  id: totrans-1112
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总是应用`InstanceNormalization`
- en: More importantly, creates a skip connection between its output layer and the
    layer of corresponding dimensionality from the downsampling part from [figure
    9.4](#ch09fig04)
  id: totrans-1113
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更重要的是，在其输出层和从[图9.4](#ch09fig04)的下采样部分对应维度的层之间创建跳过连接
- en: '|  |'
  id: totrans-1114
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-1115
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: In step 2d, we’re using a simple `UpSampling2D`, which is not a learned parameter,
    but rather uses the nearest neighbors interpolation.
  id: totrans-1116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在步骤2d中，我们使用简单的`UpSampling2D`，这不是一个学习参数，而是使用最近邻插值。
- en: '|  |'
  id: totrans-1117
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |'
- en: 'Then we create the actual Generator:'
  id: totrans-1118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后我们创建实际的生成器：
- en: Take the input (128 × 128 × 3) and assign that to `d0`.
  id: totrans-1119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入（128 × 128 × 3）分配给`d0`。
- en: Run that through a convolutional layer `d1`, arriving at a 64 × 64 × 32 layer.
  id: totrans-1120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过卷积层`d1`运行，得到64 × 64 × 32层。
- en: Take `d1` (64 × 64 × 32) and apply `conv2d` to get 32 ×32 × 64 (`d2`).
  id: totrans-1121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`d1`（64 × 64 × 32）应用`conv2d`得到32 × 32 × 64（`d2`）。
- en: Take `d2` (32 × 32 × 64) and apply `conv2d` to get 16 × 16 × 128 (`d3`).
  id: totrans-1122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`d2`（32 × 32 × 64）应用`conv2d`得到16 × 16 × 128（`d3`）。
- en: Take `d3` (16 × 16 × 128) and apply `conv2d` to get 8 × 8 × 256 (`d4`).
  id: totrans-1123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`d3`（16 × 16 × 128）应用`conv2d`得到8 × 8 × 256（`d4`）。
- en: '`u1`: Upsample `d4` and create a skip connection between `d3` and `u1`.'
  id: totrans-1124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`u1`：上采样`d4`并在`d3`和`u1`之间创建跳过连接。'
- en: '`u2`: Upsample `u1` and create a skip connection between `d2` and `u2`.'
  id: totrans-1125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`u2`：上采样`u1`并在`d2`和`u2`之间创建跳过连接。'
- en: '`u3`: Upsample `u2` and create a skip connection between `d1` and `u3`.'
  id: totrans-1126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`u3`：上采样`u2`并在`d1`和`u3`之间创建跳过连接。'
- en: '`u4`: Use regular upsampling to arrive at a 128 × 128 × 64 image.'
  id: totrans-1127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`u4`：使用常规上采样得到128 × 128 × 64图像。'
- en: Use a regular 2D convolution to get rid of the extra feature maps and get only
    128 × 128 × 3 (height × width × color_channels)
  id: totrans-1128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用常规的2D卷积去除额外的特征图，只得到128 × 128 × 3（高度 × 宽度 × 颜色通道）
- en: Listing 9.4\. Building the generator
  id: totrans-1129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.4\. 构建生成器
- en: '[PRE30]'
  id: totrans-1130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '***1* Image input**'
  id: totrans-1131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 图像输入**'
- en: '***2* Downsampling**'
  id: totrans-1132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 下采样**'
- en: '***3* Upsampling**'
  id: totrans-1133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 上采样**'
- en: 9.7.3\. Building the Discriminator
  id: totrans-1134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.7.3\. 构建判别器
- en: Now for the Discriminator method, which uses a helper function that creates
    layers formed of 2D convolutions, `LeakyReLU`, and optionally, `InstanceNormalization`.
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是判别器方法，它使用一个辅助函数来创建由2D卷积、`LeakyReLU`和可选的`InstanceNormalization`组成的层。
- en: 'We apply these layers the following way, as shown in [listing 9.5](#ch09ex05):'
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以以下方式应用这些层，如图[列表9.5](#ch09ex05)所示：
- en: We take the input image (128 × 128 × 3) and assign that to `d1` (64 × 64 × 64).
  id: totrans-1137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将输入图像（128 × 128 × 3）分配给`d1`（64 × 64 × 64）。
- en: We take `d1` (64 × 64 × 64) and assign that to `d2` (32 × 32 × 128).
  id: totrans-1138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`d1`（64 × 64 × 64）分配给`d2`（32 × 32 × 128）。
- en: We take `d2` (32 × 32 × 128) and assign that to `d3` (16 × 16 × 256).
  id: totrans-1139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`d2`（32 × 32 × 128）分配给`d3`（16 × 16 × 256）。
- en: We take `d3` (16 × 16 × 256) and assign that to `d4` (8 × 8 × 512).
  id: totrans-1140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`d3`（16 × 16 × 256）分配给`d4`（8 × 8 × 512）。
- en: We take `d4` (8 × 8 × 512) and flatten by `conv2d` to 8 × 8 × 1.
  id: totrans-1141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`d4`（8 × 8 × 512）通过`conv2d`展平为8 × 8 × 1。
- en: Listing 9.5\. Building the Discriminator
  id: totrans-1142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.5\. 构建判别器
- en: '[PRE31]'
  id: totrans-1143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 9.7.4\. Training the CycleGAN
  id: totrans-1144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.7.4\. 训练CycleGAN
- en: With all networks written, now we will implement the method that creates our
    training loop. For the CycleGAN training algorithm, the details of each training
    iteration are as follows.
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: 所有网络编写完毕后，我们现在将实现创建训练循环的方法。对于 CycleGAN 训练算法，每个训练迭代的细节如下。
- en: '|  |'
  id: totrans-1146
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**CycleGAN training algorithm**'
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
  zh: '**CycleGAN 训练算法**'
- en: '*For* each training iteration *do*'
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于*每个训练迭代*执行*'
- en: 'Train the Discriminator:'
  id: totrans-1149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练判别器：
- en: Take a mini-batch of random images from each domain (*imgs[A]* and *imgs[B]*).
  id: totrans-1150
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从每个域（*imgs[A]* 和 *imgs[B]*）中随机抽取一个图像的小批量。
- en: Use the Generator *G[AB]* to translate *imgs[A]* to domain B and vice versa
    with *G[BA]*.
  id: totrans-1151
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用生成器 *G[AB]* 将 *imgs[A]* 转换为域 B，反之亦然，使用 *G[BA]*。
- en: Compute *D[A]*(*imgs[A]*, 1) and *D[A]*(*G[BA]*(*imgs[B]*), 0) to get the losses
    for real images in A and translated images from B, respectively. Then add these
    two losses together. The 1 and 0 in *D[A]* serve as labels.
  id: totrans-1152
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 *D[A]*(*imgs[A]*, 1) 和 *D[A]*(*G[BA]*(*imgs[B]*), 0) 以获取 A 中真实图像和从 B 转换的图像的损失。然后将这两个损失相加。*D[A]*
    中的 1 和 0 作为标签。
- en: Compute *D[B]*(*imgs[B]*, 1) and *D[B]*(*G[AB]*(*imgs[A]*), 0) to get the losses
    for real images in B and translated images from A, respectively. Then add these
    two losses together. The 1 and 0 in *D[B]* serve as labels.
  id: totrans-1153
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 *D[B]*(*imgs[B]*, 1) 和 *D[B]*(*G[AB]*(*imgs[A]*), 0) 以获取 B 中真实图像和从 A 转换的图像的损失。然后将这两个损失相加。*D[B]*
    中的 1 和 0 作为标签。
- en: 'Add the losses from steps c and d together to get a total Discriminator loss.
    Train the Generator:'
  id: totrans-1154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将步骤 c 和 d 的损失相加以获得总判别器损失。训练生成器：
- en: We use the combined model to
  id: totrans-1155
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用组合模型来
- en: Input the images from domain A (*imgs[A]*) and B (*imgs[B]*)
  id: totrans-1156
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入来自域 A (*imgs[A]*) 和 B (*imgs[B]*) 的图像
- en: The outputs are
  id: totrans-1157
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出如下
- en: 'Validity of A: *D[A]*(*G[BA]*(*imgs[B]*))'
  id: totrans-1158
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: A 的有效性：*D[A]*(*G[BA]*(*imgs[B]*))
- en: 'Validity of B: *D[B]*(*G[AB]*(*imgs[A]*))'
  id: totrans-1159
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: B 的有效性：*D[B]*(*G[AB]*(*imgs[A]*))
- en: 'Reconstructed A: *G[BA]*(*G[AB]*(*imgs[A]*))'
  id: totrans-1160
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重建 A：*G[BA]*(*G[AB]*(*imgs[A]*))
- en: 'Reconstructed B: *G[AB]*(*G[BA]*(*imgs[B]*))'
  id: totrans-1161
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重建 B：*G[AB]*(*G[BA]*(*imgs[B]*))
- en: 'Identity mapping of A: *G[BA]*(*imgs[A]*))'
  id: totrans-1162
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: A 的身份映射：*G[BA]*(*imgs[A]*))
- en: 'Identity mapping of B: *G[AB]*(*imgs[B]*))'
  id: totrans-1163
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: B 的身份映射：*G[AB]*(*imgs[B]*))
- en: We then update the parameters of both Generators inline with the cycle-consistency
    loss, identity loss, and adversarial loss with
  id: totrans-1164
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们根据循环一致性损失、身份损失和对抗损失更新两个生成器的参数
- en: Mean squared error (MSE) for the scalars (discriminator probabilities)
  id: totrans-1165
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标量（判别器概率）的均方误差 (MSE)
- en: Mean absolute error (MAE) for images (either reconstructed or identity-mapped)
  id: totrans-1166
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均方绝对误差 (MAE) 用于图像（无论是重建的还是身份映射的）
- en: '*End for*'
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
  zh: '*结束 for*'
- en: '|  |'
  id: totrans-1168
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The following listing implements this CycleGAN training algorithm.
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表实现了这个 CycleGAN 训练算法。
- en: Listing 9.6\. Training CycleGAN
  id: totrans-1170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.6. 训练 CycleGAN
- en: '[PRE32]'
  id: totrans-1171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '***1* Adversarial loss ground truths**'
  id: totrans-1172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1. 对抗损失真实值***'
- en: '***2* Now we begin to train the Discriminators. These lines translate images
    to the opposite domain.**'
  id: totrans-1173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2. 现在我们开始训练判别器。这些行将图像转换为相反的域。**'
- en: '***3* Trains the Discriminators (original images = real / translated = Fake)**'
  id: totrans-1174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3. 训练判别器（原始图像 = 真实 / 转换 = 假）***'
- en: '***4* Total Discriminator loss**'
  id: totrans-1175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4. 总判别器损失***'
- en: '***5* Trains the Generators**'
  id: totrans-1176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5. 训练生成器***'
- en: '***6* If at save interval => save generated image samples**'
  id: totrans-1177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6. 如果达到保存间隔 => 保存生成的图像样本***'
- en: '***7* This function is similar to what you have encountered and is made explicit
    in the GitHub repository.**'
  id: totrans-1178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7. 这个函数与您遇到过的类似，并在 GitHub 仓库中明确说明。**'
- en: 9.7.5\. Running CycleGAN
  id: totrans-1179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.7.5. 运行 CycleGAN
- en: 'We have written all of this complicated code and are now ready to instantiate
    a CycleGAN object and look at some results, from the sampled images:'
  id: totrans-1180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经编写了所有这些复杂的代码，现在准备好实例化一个 CycleGAN 对象，并查看一些从采样图像中获得的结果：
- en: '[PRE33]'
  id: totrans-1181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[Figure 9.7](#ch09fig07) shows some results of our hard work.'
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9.7](#ch09fig07) 展示了我们辛勤工作的部分结果。'
- en: Figure 9.7\. Apples translated into oranges, and oranges into apples. These
    are results as they appear verbatim in our Jupyter notebook. (Results may vary
    slightly based on random seeds, implementation of TensorFlow and Keras, and hyperparameters.)
  id: totrans-1183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.7. 苹果转换为橙子，橙子转换为苹果。这些是我们 Jupyter 笔记本中直接显示的结果。（结果可能因随机种子、TensorFlow 和 Keras
    的实现以及超参数而略有不同。）
- en: '![](../Images/09fig07_alt.jpg)'
  id: totrans-1184
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09fig07_alt.jpg)'
- en: 9.8\. Expansions, augmentations, and applications
  id: totrans-1185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.8. 扩展、增强和应用
- en: When you run these results, we hope you will be as impressed as we were. Because
    of the absolutely astonishing results, lots of researchers flocked to improve
    on the technique. This section details a CycleGAN extension and then discusses
    some CycleGAN applications.
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这些结果时，我们希望你会像我们一样印象深刻。由于绝对惊人的结果，许多研究人员纷纷改进这项技术。本节详细介绍了CycleGAN的扩展，然后讨论了一些CycleGAN的应用。
- en: 9.8.1\. Augmented CycleGAN
  id: totrans-1187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.8.1\. 增强CycleGAN
- en: '“Augmented CycleGAN: Learning Many-to-Many Mappings from Unpaired Data” is
    a really neat extension to standard CycleGAN that injects latent space information
    during both translations. Presented at ICML 2018 in Stockholm, Augmented CycleGAN
    gives us extra variables that drive the generative process.^([[10](#ch0100)])
    In the same way that we have used latent space in Conditional GANs’ case, we can
    use it in the CycleGAN setting over and above what CycleGAN already does.'
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
  zh: “从未配对数据中学习多对多映射的增强CycleGAN”是对标准CycleGAN的一个真正巧妙的扩展，它在翻译过程中注入了潜在空间信息。在斯德哥尔摩的ICML
    2018会议上提出，增强CycleGAN为我们提供了额外的变量，这些变量驱动着生成过程。[^([10](#ch0100))] 就像我们在条件GAN的情况下使用潜在空间一样，我们可以在CycleGAN的设置中使用它，超越CycleGAN本身所做的工作。
- en: ^(10)
  id: totrans-1189
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([10])
- en: ''
  id: totrans-1190
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See “Augmented Cyclic Adversarial Learning for Low Resource Domain Adaptation,”
    by Ehsan Hosseini-Asl, 2019, [https://arxiv.org/pdf/1807.00374.pdf](https://arxiv.org/pdf/1807.00374.pdf).
  id: totrans-1191
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见Ehsan Hosseini-Asl的“增强循环对抗学习用于低资源域适应”，2019年，[https://arxiv.org/pdf/1807.00374.pdf](https://arxiv.org/pdf/1807.00374.pdf).
- en: For example, if we have an outline of a shoe in the A domain, we can generate
    a sample in the B domain, where the same type of shoe is blue. In traditional
    CycleGAN’s case, it would always be blue. But now, with the latent variables at
    our disposal, it can be orange, yellow, or whatever we choose.
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有A域中鞋的轮廓，我们可以在B域中生成一个样本，其中相同类型的鞋是蓝色的。在传统的CycleGAN情况下，它将始终是蓝色。但现在，有了我们可用的潜在变量，它可以变成橙色、黄色，或者我们选择的任何颜色。
- en: 'This is also a useful framework to think about the limitations of the original
    CycleGAN: because we are not given any extra seeding parameters (such as an extra
    latent vector *z*), we cannot control or alter what comes out the other end. If
    from a particular handbag outline we get an image that is orange, it will always
    be orange. Augmented CycleGAN gives us more control over the outcomes, as shown
    in [figure 9.8](#ch09fig08).'
  id: totrans-1193
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是一个思考原始CycleGAN局限性的有用框架：因为我们没有给出任何额外的种子参数（例如额外的潜在向量 *z*），所以我们无法控制或改变输出结果。如果从一个特定的手提包轮廓中得到一个橙色的图像，它将始终是橙色。增强CycleGAN让我们对结果有了更多的控制，如图9.8所示。
- en: Figure 9.8\. In this information flow of the augmented CycleGAN, we have latent
    vectors Za and Zb that seed the Generator along with the image input, effectively
    reducing the problem to two CGANs joined together. This allows us to control the
    generation.
  id: totrans-1194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.8\. 在增强CycleGAN的信息流中，我们有潜在向量Za和Zb，它们与图像输入一起作为生成器的种子，有效地将问题简化为两个联合的CGAN。这使我们能够控制生成。
- en: '![](../Images/09fig08_alt.jpg)'
  id: totrans-1195
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/09fig08_alt.jpg)'
- en: '(Source: “Augmented CycleGAN: Learning Many-to-Many Mappings from Unpaired
    Data,” by Amjad Almahairi et al., 2018, [http://arxiv.org/abs/1802.10151](http://arxiv.org/abs/1802.10151).)'
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
  zh: (来源：“从未配对数据中学习多对多映射的增强CycleGAN”，作者：Amjad Almahairi等，2018年，[http://arxiv.org/abs/1802.10151](http://arxiv.org/abs/1802.10151).)
- en: 9.8.2\. Applications
  id: totrans-1197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.8.2\. 应用
- en: 'Many CycleGAN (or CycleGAN-inspired) applications have been proposed in the
    short time it has been around. They usually revolve around creating simulated
    virtual environments and subsequently making them photorealistic. For example,
    imagine you need more training data for a self-driving car company: just simulate
    it in Unity or a GTA 5 graphics engine and then use CycleGAN to translate the
    data.'
  id: totrans-1198
  prefs: []
  type: TYPE_NORMAL
  zh: 在CycleGAN（或CycleGAN启发式）出现的不长的时间里，已经提出了许多应用。它们通常围绕创建模拟的虚拟环境，然后使它们变得逼真。例如，想象一下，如果你需要为一家自动驾驶汽车公司提供更多训练数据：只需在Unity或GTA
    5图形引擎中模拟它，然后使用CycleGAN来转换数据。
- en: This works especially well if you need to have particular risk situations that
    are expensive or time-consuming to re-create (for example, car crashes, or fire
    trucks speeding to reach a destination), but you need them in your dataset. For
    a self-driving car company, this could be extremely useful to balance the dataset
    with at-risk situations, which are rare, but correct behavior is all the more
    important.
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要重现特定风险情况，而这些情况既昂贵又耗时（例如，汽车相撞，或消防车急速赶往目的地），但你需要在数据集中拥有它们，这尤其有效。对于一家自动驾驶汽车公司来说，这可能非常有用，以平衡数据集中罕见的风险情况，而正确的行为则更为重要。
- en: 'One example of this kind of framework is Cycle Consistent Adversarial Domain
    Adaptation (CyCADA).^([[11](#ch09101)]) Unfortunately, a full explanation of the
    way it works is beyond the scope of this chapter. This is because there are many
    more such frameworks: some even experiment with CycleGAN in language, music, or
    other forms of domain adaptation. To give you a sense of the complexity, [figure
    9.9](#ch09fig09) shows the architecture and design of CyCADA.'
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
  zh: 这种框架的一个例子是Cycle Consistent Adversarial Domain Adaptation (CyCADA).^([[11](#ch09101)])
    不幸的是，对这个工作方式的完整解释超出了本章的范围。这是因为还有许多这样的框架：一些甚至尝试在语言、音乐或其他形式的域适应中使用CycleGAN。为了给你一个复杂性的感觉，[图9.9](#ch09fig09)展示了CyCADA的架构和设计。
- en: ^(11)
  id: totrans-1201
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(11)
- en: ''
  id: totrans-1202
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See “CyCADA: Cycle-Consistent Adversarial Domain Adaptation,” by Judy Hoffman
    et al., 2017, [https://arxiv.org/pdf/1711.03213.pdf](https://arxiv.org/pdf/1711.03213.pdf).'
  id: totrans-1203
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '见Judy Hoffman等人于2017年发表的“CyCADA: Cycle-Consistent Adversarial Domain Adaptation”，[https://arxiv.org/pdf/1711.03213.pdf](https://arxiv.org/pdf/1711.03213.pdf)。'
- en: 'Figure 9.9\. This structure should be somewhat familiar from earlier, so hopefully
    this chapter has at least given you a head start. One extra thing to point out:
    we now have an extra step with labels and semantic understanding that gives us
    the so-called task loss. This allows us to also check the produced image for semantic
    meaning.'
  id: totrans-1204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.9。这种结构应该与之前的内容有些熟悉，所以希望这一章至少能给你一个良好的开端。有一点需要指出：我们现在有一个额外的步骤，包括标签和语义理解，这为我们提供了所谓的任务损失。这使我们能够检查生成的图像的语义意义。
- en: '![](../Images/09fig09_alt.jpg)'
  id: totrans-1205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09fig09_alt.jpg)'
- en: Summary
  id: totrans-1206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概述
- en: Image-to-image translation frameworks are frequently difficult to train because
    of the need for perfect pairs; the CycleGAN solves this by making this an unpaired
    domain translation.
  id: totrans-1207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于需要完美的成对图像，图像到图像的翻译框架通常很难训练；CycleGAN通过将其变成一个无配对域翻译来解决这一问题。
- en: 'The CycleGAN has three losses:'
  id: totrans-1208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CycleGAN有三个损失函数：
- en: Cycle-consistent, which measures the difference between the original image and
    an image translated into a different domain and back again
  id: totrans-1209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环一致性**，它衡量原始图像与转换到不同域并再次转换回来的图像之间的差异'
- en: Adversarial, which ensures realistic images
  id: totrans-1210
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对抗性**，确保生成图像的逼真度'
- en: Identity, which preserves the color space of the image
  id: totrans-1211
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**身份**，它保留图像的色彩空间'
- en: The two Generators use the U-Net architecture, and the two Discriminators use
    the PatchGAN-based architecture.
  id: totrans-1212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个生成器使用U-Net架构，两个判别器使用基于PatchGAN的架构。
- en: We implemented an object-oriented design of the CycleGAN and used it to convert
    apples to oranges.
  id: totrans-1213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们实现了CycleGAN的面向对象设计，并使用它将苹果转换为橙子。
- en: Practical applications of the CycleGAN include self-driving car training and
    extensions that allow us to create different styles of images during the translation
    process.
  id: totrans-1214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CycleGAN的实际应用包括自动驾驶汽车训练以及在翻译过程中创建不同风格图像的扩展。
