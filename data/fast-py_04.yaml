- en: 3 Concurrency, parallelism, and asynchronous processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 并发、并行性和异步处理
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Using asynchronous processing to design applications with reduced wait times
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用异步处理设计减少等待时间的应用程序
- en: Threading in Python and its limitations on writing parallel applications
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python中的线程及其在编写并行应用程序上的限制
- en: Making multiprocessing applications to take full advantage of multicore computers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使多进程应用程序充分利用多核计算机
- en: Modern CPU architectures allow for more than one sequential program to be executed
    at the same time, permitting impressive gains in processing speeds. In fact, speeds
    can increase right up to the number of parallel processing units (e.g., CPU cores)
    that are available. The bad news is that to take advantage of all this parallel
    processing speed for our programs, we need to make our code parallel-aware, and
    Python is ill-suited for writing parallel solutions. Most Python code is sequential,
    so it is not able to use all the available CPU resources. Furthermore, the implementation
    of the Python interpreter is, as we will see, not tuned for parallel processing.
    In other words, our usual Python code cannot make use of modern hardware’s capabilities,
    and it will always run at a much lower speed than the hardware allows. So we need
    to devise techniques to help Python make use of all the available CPU power.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现代CPU架构允许同时执行多个顺序程序，从而在处理速度上实现令人印象深刻的提升。实际上，速度可以增加到可用的并行处理单元（例如，CPU核心）的数量。坏消息是，为了充分利用所有这些并行处理速度来为我们的程序服务，我们需要使我们的代码并行化，而Python不适合编写并行解决方案。大多数Python代码都是顺序的，因此它无法使用所有可用的CPU资源。此外，Python解释器的实现，正如我们将看到的，并没有针对并行处理进行优化。换句话说，我们通常的Python代码无法利用现代硬件的能力，并且它将始终以比硬件允许的速度慢得多。因此，我们需要设计技术来帮助Python利用所有可用的CPU功率。
- en: In this chapter, we will learn how to do just that, starting with some approaches
    you may be familiar with in general but that have some unique Python twists. We
    will discuss concurrency, multithreading, and parallelism the Python way, including
    some strong limitations around multithreaded programming.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何做到这一点，从一些你可能熟悉的一般方法开始，但它们在Python中有着一些独特的技巧。我们将以Python的方式讨论并发、多线程和并行性，包括围绕多线程编程的一些强烈限制。
- en: We will also learn about asynchronous programming methods, which allow us to
    efficiently serve many concurrent requests without any need for parallel solutions.
    Asynchronous programming has been around for quite some time, and it is popular
    in the JavaScript/Node.JS world, but only recently has it become standardized
    in Python with the addition of new modules to facilitate asynchronous programming.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将了解异步编程方法，这些方法允许我们高效地服务许多并发请求，而无需任何并行解决方案。异步编程已经存在了一段时间，在JavaScript/Node.JS世界中很受欢迎，但直到最近，它才在Python中通过添加新的模块来实现异步编程的标准化。
- en: For this chapter, let’s assume that you are a developer working for a big software
    company. You are tasked with developing a MapReduce framework that is expected
    to be extremely fast. All the data will be in-memory, and everything will have
    to be done on a single computer. Furthermore, your service will have to be able
    to handle requests from several clients, most of whom are automated AI bots, at
    the same time. To tackle this project, you will use concurrent and parallel programming
    techniques, including multithreading and multiprocessing, to speed up the processing
    of MapReduce requests. In addition, you’ll use asynchronous programming to efficiently
    process many simultaneous queries from users.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，让我们假设你是一家大型软件公司的开发者。你被分配开发一个预期将非常快速的MapReduce框架。所有数据都将存储在内存中，并且所有操作都必须在单台计算机上完成。此外，你的服务必须能够同时处理来自多个客户端的请求，其中大多数是自动化的AI机器人。为了应对这个项目，你将使用并发和并行编程技术，包括多线程和多进程，以加快MapReduce请求的处理速度。此外，你还将使用异步编程来高效地处理来自用户的许多并发查询。
- en: 'We will divide the problem into two parts. In the first section of the chapter,
    we will build a server that is able to handle multiple requests simultaneously.
    Then we need to create the MapReduce framework itself, and this will take up most
    of the chapter after section 3.1\. We will consider three different ways to build
    the framework: sequential, multithreaded, and multiprocess. This will allow you
    to see several approaches at work, along with their benefits, tradeoffs, and limitations.
    In the final section, we’ll put the two parts together and connect the server
    with the MapReduce framework, allowing us to understand how to architect a solution
    that integrates all the parts in an efficient way.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将问题分为两部分。在章节的第一部分，我们将构建一个能够同时处理多个请求的服务器。然后我们需要创建MapReduce框架本身，这将在3.1节之后占据本章的大部分内容。我们将考虑三种不同的方法来构建框架：顺序、多线程和多进程。这将让你看到几种方法在工作，以及它们的优点、权衡和限制。在最后一节，我们将两部分结合起来，将服务器与MapReduce框架连接起来，使我们能够理解如何以高效的方式构建一个集成所有部分的解决方案。
- en: '![](../Images/CH03_F01_Antao.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F01_Antao.png)'
- en: Figure 3.1 Chapter roadmap
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 章节路线图
- en: To make the chapter topics and organization clearer, figure 3.1 provides a visual
    roadmap of the chapter. It shows the approaches that we will investigate as well
    as how they are connected to each other and/or used together. In the upper left
    corner of each box, you’ll find the section number where you’ll learn about each
    technique.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使章节主题和组织结构更加清晰，图3.1提供了一个章节的视觉路线图。它展示了我们将要调查的方法，以及它们如何相互连接和/或一起使用。在每个框的右上角，你可以找到你将学习每个技术的章节编号。
- en: Sequential processing, concurrency, and parallelism
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序处理、并发和并行
- en: Before we start, let’s briefly review the meaning of sequential processing,
    concurrency, and parallelism. Although these are basic concepts, many experienced
    developers still get them confused, so here’s a quick refresher to make sure we’re
    all using the terms in the same way.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，让我们简要回顾一下顺序处理、并发和并行的含义。尽管这些都是基本概念，但许多经验丰富的开发者仍然会混淆它们，所以这里有一个快速复习，以确保我们都在以相同的方式使用这些术语。
- en: 'Parallelism is the easiest concept to explain: tasks run in parallel when they
    are running at the same time. Concurrent tasks may run in any order: they *may*
    be run in parallel or in sequence, depending on the language and OS. So all parallel
    tasks are concurrent but not the other way around.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 并行是解释起来最容易的概念：当任务同时运行时，它们就会并行运行。并发任务可以以任何顺序运行：它们*可能*会并行或顺序运行，这取决于语言和操作系统。所以所有并行任务都是并发的，但反之则不然。
- en: 'The term *sequential* can be used in two different ways. First, it can mean
    that a certain set of tasks need to be run in a strict order. For example, to
    write in your computer, you have to first turn it on: the ordering, or sequence,
    is *imposed by the tasks themselves*. The second task can only happen after the
    execution of the first one.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: “顺序”这个术语可以用两种不同的方式使用。首先，它可以意味着一组任务需要以严格顺序运行。例如，要在电脑上写东西，你首先得打开它：顺序或序列是由任务本身强加的。第二个任务只能在第一个任务执行之后发生。
- en: Sometimes, however, sequential is used to mean a limitation that the *system*
    imposes on the order of the execution of tasks. For example, only one person at
    a time is allowed to go through a metal detector in an airport, even if two would
    be able to fit through it simultaneously.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时“顺序”被用来表示系统对任务执行顺序强加的限制。例如，在机场，一次只允许一个人通过金属探测器，即使两个人同时也能通过。
- en: 'Finally, there is the concept of preemption: this happens when a task is interrupted
    (involuntarily) for another one to run. This is related to scheduling policies
    among tasks and requires a piece of software or hardware to do it, called a *scheduler*.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有中断的概念：当一个任务被中断（非自愿地）以便运行另一个任务时，就会发生这种情况。这与任务之间的调度策略有关，需要软件或硬件来完成，这被称为**调度器**。
- en: 'The alternative to preemptive multitasking is cooperative multitasking: your
    code is responsible to tell the system when it can be interrupted to be swapped
    by another task. The following figure tries to make some of these concepts clearer.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 预先中断多任务处理的替代方案是协作多任务处理：你的代码负责告诉系统何时可以被中断并交换给另一个任务。以下图试图使这些概念更加清晰。
- en: '![](../Images/CH03_F02_Antao.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F02_Antao.png)'
- en: 'Understanding sequential, concurrent, and parallel models. Sequential execution
    occurs when all tasks are executed in sequence and never interrupted. Concurrent
    execution with no parallelism adds the possibility of a task being interrupted
    by another and later resumed. Parallelism occurs when several tasks are run at
    the same time. Even with parallelism, it is quite common for preemption to still
    occur as the number of processors/cores may not be enough for all tasks. A dream
    scenario is when there are more processors than tasks: this allows parallel execution
    of all tasks without the need for any preemption.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 理解顺序、并发和并行模型。顺序执行发生在所有任务按顺序执行且不会被中断的情况下。无并行性的并发执行增加了任务可能被另一个任务中断并在之后恢复的可能性。并行性发生在多个任务同时运行时。即使有并行性，由于处理器/核心的数量可能不足以处理所有任务，预emption
    仍然很常见。一个理想的场景是有比任务更多的处理器：这允许所有任务并行执行，而不需要任何预emption。
- en: Note We will not go over the basic features of Python multithreading and multiprocessing.
    Many introductory guides can fill the gap for you if you need a refresher, including
    *Python Concurrency with Asyncio*, by Matthew Fowler (Manning, 2022; [https://www.manning.com/books/python-concurrency-with-asyncio](https://www.manning.com/books/python-concurrency-with-asyncio)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们不会介绍 Python 多线程和多进程的基本功能。如果你需要复习，许多入门指南可以填补这个空白，包括由 Matthew Fowler 编写的《Python
    Concurrency with Asyncio》（Manning，2022年；[https://www.manning.com/books/python-concurrency-with-asyncio](https://www.manning.com/books/python-concurrency-with-asyncio))）。
- en: 3.1 Writing the scaffold of an asynchronous server
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 编写异步服务器的框架
- en: While our main job is to use a MapReduce framework to process requests, we will
    start by building the part of the server that receives the requests (i.e., provides
    an interface to clients). Processing the requests properly will be a matter for
    the rest of the chapter. In this section, we will write a server that will accept
    connections from all clients and receive MapReduce requests (both data and code).
    In doing this, we will see how asynchronous programming can help create efficient
    servers, even without the use of parallelism.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的主要任务是使用 MapReduce 框架来处理请求，但我们将首先构建服务器接收请求的部分（即，为客户端提供一个接口）。正确处理请求将是本章其余部分的内容。在本节中，我们将编写一个服务器，它将接受来自所有客户端的连接并接收
    MapReduce 请求（数据和代码）。通过这样做，我们将看到异步编程如何帮助创建高效的服务器，即使不使用并行性。
- en: The rise of asynchronous programming
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 异步编程的兴起
- en: Asynchronous programming was made popular in the JavaScript world, especially
    on the server with NodeJS. It is a particularly good model when we have a lot
    of slow IO streams that need monitoring. The most obvious example is a web server
    where most use-case data exchange is limited in size and the amount of processing
    is fast, typically in the order of milliseconds. But the asynchronous model can
    also help us to write clean concurrent and parallel programs. Furthermore, as
    we will see in the rest of the chapter, the asynchronous approach is also useful
    for more orthodox data analysis scenarios.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 异步编程在 JavaScript 世界中变得流行，尤其是在 NodeJS 服务器上。当我们有很多需要监控的慢速 I/O 流时，这是一个特别好的模型。最明显的例子是
    Web 服务器，其中大多数用例的数据交换量有限，处理速度也很快，通常在毫秒级别。但异步模型也可以帮助我们编写干净的并发和并行程序。此外，正如我们在本章的其余部分将看到的，异步方法对于更传统的数据分析场景也很有用。
- en: To clarify, being asynchronous is orthogonal to being single-threaded, multithreaded,
    or multiprocess. You can have asynchronous systems on top of any of these.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了澄清，异步与单线程、多线程或多进程是正交的。你可以在这些之上拥有异步系统。
- en: 'First, let’s look at one of the major problems that synchronous processing
    creates, so we can compare the synchronous solutions to the asynchronous solutions.
    Synchronous programming is the more common approach in the Python world and would
    probably be the first port of call for most Python programmers. But a synchronous
    (and single-process) version of a server will block while waiting for the user
    input. Since the user can take 1 ms or 1 h to actually write a request after opening
    the connection, in a synchronous world that would mean that all other clients
    would be on hold during this time. There are three potential solutions here (figure
    3.2):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看同步处理造成的主要问题之一，这样我们就可以将同步解决方案与异步解决方案进行比较。在Python世界中，同步编程是一种更常见的方法，可能是大多数Python程序员首先会尝试的方法。但是，一个同步（单进程）的服务器版本在等待用户输入时会阻塞。由于用户可能需要1毫秒或1小时来实际写入请求，在同步世界中，这意味着在此期间所有其他客户端都会处于等待状态。这里有三种可能的解决方案（图3.2）：
- en: We just block (label 1 in figure 3.2). This means that while that connection
    is being processed, everything else (e.g., attending to other users) is nonresponsive.
    This blocking of all connections is unacceptable.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只是阻塞（图3.2中的标签1）。这意味着在处理该连接的同时，其他所有事情（例如，关注其他用户）都将无响应。这种对所有连接的阻塞是不可接受的。
- en: We have a multithreaded or multiprocessing solution where a single thread or
    process is started to take care of a request (label 2 in figure 3.2). This means
    that the main process is released to take care of other incoming requests. A single-threaded
    solution is possible and lighter for cases where we have lots of IO channels producing
    little information.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有一个多线程或多进程解决方案，其中启动单个线程或进程来处理请求（图3.2中的标签2）。这意味着主进程被释放以处理其他传入的请求。对于有大量IO通道产生少量信息的情况，单线程解决方案是可能的，并且更轻量级。
- en: 'Finally, when a blocking call occurs, the alternative is for the code to somehow
    release execution control so that other pieces of code can be executed while the
    data is arriving (label 3 in figure 3.2). This is the solution that we will be
    exploring here: asynchronous processing with a single thread.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，当一个阻塞调用发生时，代码的另一种选择是以某种方式释放执行控制，以便在数据到达时可以执行其他代码片段（图3.2中的标签3）。这是我们在这里将要探索的解决方案：使用单个线程的异步处理。
- en: '![](../Images/CH03_F03_Antao.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图3.2](../Images/CH03_F03_Antao.png)'
- en: Figure 3.2 Several architectures covering synchronous single-process/thread,
    synchronous multiprocess, and asynchronous single-process implementations for
    a server
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：涵盖同步单进程/线程、同步多进程和异步单进程服务器实现的几种架构
- en: There are plenty of alternatives to these three options. For example, the solution
    at the end of the chapter will actually be a mix of solutions 2 and 3\. Another
    very common alternative in solution 2 is to have a pool of prelaunched processes
    to speed up the response. For solution 3, we are assuming that the computing tasks
    can be interrupted (an assumption we will relax later in the chapter). Finally,
    with Python, as you might know, multithreaded code is normally (although not always)
    nonparallel; we will get into this later. As we address our MapReduce project,
    we will discuss all of these solutions as well as their problems. We will follow
    the process depicted in figure 3.1\. First, we’ll attempt a naive solution with
    no parallelism at all.[¹](#pgfId-1012163) Then, we try a thread-based solution,
    which falls short of our needs. After that, we will develop a multiprocessing
    solution that will finally increase performance. In the final section, when we
    tie the network interface developed in this section with the multiprocess solution,
    we will find a place where threaded code still can be of use, although not for
    parallelism.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多替代方案可以替代这三种选项。例如，本章末尾的解决方案实际上将是方案2和3的混合。在方案2中，另一个非常常见的替代方案是拥有一个预先启动的进程池来加速响应。对于方案3，我们假设计算任务可以被中断（我们将在本章后面放松这个假设）。最后，正如你可能所知，Python中的多线程代码通常是（尽管不总是）非并行的；我们将在稍后讨论这个问题。当我们处理MapReduce项目时，我们将讨论所有这些解决方案以及它们的问题。我们将遵循图3.1中描述的过程。首先，我们将尝试一个完全没有并行的简单解决方案。[¹](#pgfId-1012163)然后，我们尝试基于线程的解决方案，但这并没有满足我们的需求。之后，我们将开发一个多进程解决方案，这将最终提高性能。在最后一节中，当我们把本节开发的网络接口与多进程解决方案结合起来时，我们会发现线程代码仍然有用，尽管不是用于并行处理。
- en: 'Tip The solution presented here is *a* solution. There are plenty of alternative
    approaches. Even if this was the best solution possible (it is not), concessions
    were made for the sake of explanation: what is defined as *best* varies with your
    criteria. Also, different problems will require completely different approaches.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：这里提出的解决方案是 *一个* 解决方案。有大量的替代方法。即使这是可能的最佳解决方案（它不是），也为了解释而做出了让步：*最佳* 的定义因你的标准而异。此外，不同的问题可能需要完全不同的方法。
- en: What you should take from here is not a set of clear-cut rules but, instead,
    a set of techniques and insights that will help you devise the best solution for
    your own problem and your specific criteria.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该从这里得到的是一套明确的规则，而是一套技术和见解，这将帮助你为你的问题和具体标准制定最佳解决方案。
- en: For now, let’s get back to our asynchronous, single-threaded, and single-process
    server.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到我们的异步、单线程和单进程服务器。
- en: 3.1.1 Implementing the scaffold for communicating with clients
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 实现与客户端通信的框架
- en: 'Our server will be based on the TCP protocol and answer on port 1936\. It is
    available in the repository in `03-concurrency/sec1-async/server.py`. Here is
    the top level of the scaffold that processes client requests:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的服务器将基于 TCP 协议，并在端口 1936 上响应。它在 `03-concurrency/sec1-async/server.py` 仓库中可用。以下是处理客户端请求的框架的顶层结构：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① We use Python’s asyncio library.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们使用 Python 的 asyncio 库。
- en: ② All our functions are declared async.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们的所有函数都被声明为 async。
- en: ③ These lines can block and pause all the other code around.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 这些行可能会阻塞并暂停周围的全部其他代码。
- en: ④ We use the start_server from asyncio to call accept_requests for each connection.
    Our server will be listening on the local interface 127.0.0.1 port 1936.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 我们使用 asyncio 的 start_server 来为每个连接调用 accept_requests。我们的服务器将在本地接口 127.0.0.1
    端口 1936 上监听。
- en: ⑤ The async keyword can be used with the keyword to make it nonblocking.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 可以使用 async 关键字与关键字一起使用，使其非阻塞。
- en: ⑥ We are telling our server object to serve requests forever.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 我们正在告诉我们的服务器对象永久地处理请求。
- en: '⑦ This is the entry point to our code: the main function is run.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 这是我们的代码的入口点：主函数被运行。
- en: This code is just a scaffold for now; we will complete the code in the last
    section of the chapter when we tie everything together. Still, there is a lot
    to unpack here. The fundamental question is why do it this way at all? Why not
    just do a “typical” synchronous version?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码现在只是一个框架；我们将在本章的最后部分完成代码，当我们把所有东西结合起来时。尽管如此，这里还有很多东西要解释。基本问题是为什么这样做？为什么不直接做一个“典型”的同步版本？
- en: The main reason is that the functions related to annotation 3—in our case, reads
    and writes from the network—can take an undetermined amount of time. Furthermore,
    network speeds are orders of magnitude slower than CPU speeds. Were we to block
    there, we would be working substantially below potential and also making other
    users wait needlessly.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 主要原因是与注释 3 相关的函数——在我们的案例中，是从网络读取和写入——可能需要不确定的时间。此外，网络速度比 CPU 速度慢几个数量级。如果我们在这里阻塞，我们将远远低于潜在的性能，并且还会让其他用户无谓地等待。
- en: All the Python infrastructure that you’ve just seen—`async`, `await`, and the
    `asyncio` module—exists to prevent blocking calls to stop other parts of the code
    that are independent of it in a single-threaded application.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚才看到的所有 Python 基础设施——`async`、`await` 和 `asyncio` 模块——都是为了防止在单线程应用程序中阻塞调用，从而阻止与它无关的其他代码部分。
- en: 3.1.2 Programming with coroutines
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 使用协程编程
- en: The `asynchronous` functions like the ones in the previous section ( i.e., the
    ones created with `async def`) are called *coroutines*. Coroutines are functions
    that voluntarily release control of execution. There is another part of the system,
    an executor, that manages all coroutines and runs them according to some policy.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 像上一节中那样的 *异步* 函数（即使用 `async def` 创建的函数）被称为 *协程*。协程是自愿释放执行控制的函数。系统中还有另一个部分，即执行器，它管理所有协程并根据某种策略运行它们。
- en: When you call a coroutine from inside another coroutine using `await`, you are
    actually telling Python that control can be diverted elsewhere at this stage.
    This is called *cooperative scheduling*, as releasing control is voluntary and
    needs to be explicitly done by the coroutine code.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在另一个协程内部使用 `await` 调用一个协程时，你实际上是在告诉 Python 在这个阶段可以转移控制权。这被称为 *合作式调度*，因为释放控制权是自愿的，并且需要由协程代码显式完成。
- en: 'Compare a coroutine system with the threading systems of most operating systems:
    there, threads are forcefully preempted and have no control over when they are
    run. That is called *preemptive scheduling*. Typical threaded code does not need
    to explicitly mark where it can be interrupted because it will be interrupted
    by force. Python threads, in this sense, work like OS system threads. It’s just
    for `async` code that voluntary preemption applies.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 将协程系统与大多数操作系统的线程系统进行比较：在那里，线程被强制抢占，无法控制其运行的时间。这被称为*抢占式调度*。典型的线程代码不需要显式标记可能被中断的位置，因为它将被强制中断。从这种意义上说，Python线程的工作方式类似于操作系统线程。这只是为了`async`代码，才适用自愿抢占。
- en: 'A typical example could be a program that is waiting for some network data
    and writing to a disk (typically an IO task). It could work like this—note that
    this is sequential, so there’s no need for threads:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的例子可能是一个等待某些网络数据并写入磁盘（通常是IO任务）的程序。它可以这样工作——请注意，这是顺序的，因此不需要线程：
- en: 'The main program schedules with the asynchronous executor two coroutines: one
    to wait for a network connection, and the other to write to a disk.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主程序使用异步执行器安排两个协程：一个等待网络连接，另一个写入磁盘。
- en: The executor selects—maybe randomly—the network coroutine to start.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器选择——可能是随机地——开始执行网络协程。
- en: The network coroutine sets up network listening. It then waits for connections.
    There are no connections at the moment so it *voluntarily* tells the executor
    to do something else.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络协程设置网络监听。然后它等待连接。目前没有连接，因此它*自愿*告诉执行器做其他事情。
- en: The executor starts the disk coroutine.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器启动磁盘协程。
- en: The disk coroutine starts writing to the disk. Writing is going slowly compared
    to CPU speeds, so the coroutine tells the executor to do something else.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 磁盘协程开始向磁盘写入。与CPU速度相比，写入速度较慢，因此协程告诉执行器做其他事情。
- en: The executor continues the network coroutine.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器继续执行网络协程。
- en: There are still no connection requests; the network coroutine yields.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目前还没有连接请求；网络协程让出。
- en: The executor schedules the disk coroutine.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器安排磁盘协程。
- en: The disk coroutine finalizes writing and terminates its work.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 磁盘协程完成写入并终止其工作。
- en: The executor lets the network coroutine run forever as there is nothing more
    to do. If the network coroutine yields, the executor just goes back to it.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于没有更多的事情要做，执行器让网络协程无限期地运行。如果网络协程让出，执行器就回到它那里。
- en: The network coroutine eventually answers a connection or maybe times out.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络协程最终响应一个连接或可能超时。
- en: The executor concludes and passes control back to the main program.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器结束并返回控制权给主程序。
- en: 'This section, along with the final one, provides examples of coroutines—remember
    that all `async def` are coroutines. But let’s do a small (a scaffold) test with
    the following subset of the previous code:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 本节，连同最后一节，提供了协程的示例——记住，所有`async def`都是协程。但让我们用以下代码片段进行一个小（一个脚手架）测试：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s see what `async` offers us here. If the code above doesn’t have the `async`
    keyword, we would expect it to throw an exception (on `reader.read()`) as `reader`
    would be `None`. However, calling `accept_requests` like this doesn’t execute
    a function but instead returns a coroutine, which is what `async def` actually
    creates.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`async`在这里提供了什么。如果上面的代码没有`async`关键字，我们预计它会在`reader.read()`抛出异常，因为`reader`将是`None`。然而，像这样调用`accept_requests`并不执行一个函数，而是返回一个协程，这正是`async
    def`实际创建的内容。
- en: The `await` call in our code tells Python that `accept_requests` can be suspended
    at that point and that something else might be run instead. Therefore, while we
    are waiting for the `reader` to send data, Python can do other stuff until the
    data arrives. If coroutines feel a bit like generators (as discussed in chapter
    2) in the sense that execution is delayed and can be suspended, then you are on
    the right track.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们代码中的`await`调用告诉Python，`accept_requests`可以在该点暂停，并且可能运行其他任务。因此，当我们等待`reader`发送数据时，Python可以执行其他任务，直到数据到达。如果协程在执行延迟和可以暂停的意义上有点像第2章中讨论的生成器，那么你就找到了正确的方向。
- en: 3.1.3 Sending complex data from a simple synchronous client
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.3 从简单的同步客户端发送复杂数据
- en: To interact with our server, we will write a simple synchronous client. This
    serves as an example of a more synchronous type of code, which is more common
    in the Python world and is quite enough for our client needs. But, more important,
    we also take the opportunity to show how to do the communication of data and code
    between processes. While the server will be further developed later, this is actually
    the final version of the client.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与我们的服务器交互，我们将编写一个简单的同步客户端。这作为一个更同步类型代码的例子，这在Python世界中更为常见，并且对于我们的客户端需求来说已经足够了。但更重要的是，我们也借此机会展示如何在进程之间进行数据和代码的通信。虽然服务器将在以后进一步开发，但这实际上是客户端的最终版本。
- en: 'Our client will submit both our code (the code can be found in `03-concurrency/
    sec1-async/client.py`) and data and will then probe the server until an answer
    is returned:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的客户端将提交我们的代码（代码可以在`03-concurrency/sec1-async/client.py`中找到）和数据，然后探测服务器直到返回答案：
- en: '[PRE2]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① marshal is used to submit code.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ① `marshal`用于提交代码。
- en: ② pickle is used to submit most high-level Python data structures.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ② `pickle`用于提交大多数高级Python数据结构。
- en: ③ Our functions are defined in a single function that returns functions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 我们的功能定义在一个返回函数的单个函数中。
- en: ④ We create a network connection here.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 我们在这里创建一个网络连接。
- en: ⑤ We create a byte representation of our code.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 我们创建我们代码的字节表示。
- en: ⑥ We receive the job_id and take care of the encoding ourselves.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 我们接收job_id并自行处理编码。
- en: ⑦ We will keep connecting until the result is ready.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 我们将一直连接，直到结果准备好。
- en: 'There is a lot to unpack here as well. Let’s start with the network code: we
    create a TCP connection with Python’s socket interface, and we use that API to
    send and receive data. All calls are potentially blocking, which is OK for this
    client.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里也有很多东西要解释。让我们从网络代码开始：我们使用Python的socket接口创建一个TCP连接，并使用该API发送和接收数据。所有调用都是可能阻塞的，这对于这个客户端来说是可接受的。
- en: Probably the most important part of this code to retain is the various alternatives
    to transfer data. In Python, the `pickle` module is the most common way to serialize
    data, which can then be transferred across processes. But it’s not an all-encompassing
    solution; for example, it cannot be used to transfer code. For code, we use the
    `marshal` module. We also use the `to_bytes` function of the `int` object to serve
    as a reminder that we can take care of the encoding ourselves in more corner-case
    situations. The most common of these is when we need a solution that is both compact
    and fast—two things that `pickle` is not. Of course, in that scenario, we will
    have the burden of encoding/decoding ourselves. We will revisit this when we deal
    with IO.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中需要保留的最重要部分可能是传输数据的各种替代方案。在Python中，`pickle`模块是最常见的序列化数据的方式，然后可以跨进程传输。但它并不是一个万能的解决方案；例如，它不能用来传输代码。对于代码，我们使用`marshal`模块。我们还使用`int`对象的`to_bytes`函数来提醒我们自己可以在更多边缘情况下自行处理编码。其中最常见的是当我们需要一个既紧凑又快速的解决方案时——这两件事`pickle`都做不到。当然，在这种情况下，我们将承担编码/解码的负担。当我们处理IO时，我们将重新审视这个问题。
- en: 'We transfer our code inside a function, `my_funs`, that returns functions.
    An alternative dialect would be to use objects. To use this code, open a terminal
    and start the server with:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在返回函数的`my_funs`函数内部传输我们的代码。另一种选择是使用对象。要使用此代码，打开终端并使用以下命令启动服务器：
- en: '[PRE3]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then run the client with:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用以下命令运行客户端：
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output will be:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将是：
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 3.1.4 Alternative approaches to interprocess communication
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.4 进程间通信的替代方法
- en: A more common approach for client/server communication would be to use a REST
    interface over HTTPS, but REST is not really helpful when we are trying to understand
    underlying concepts. We will revisit the performance effect of alternative network
    communication strategies in chapter 6\. In any case, a realistic implementation
    would surely require at least some form of encryption.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端/服务器通信的一个更常见的方法是使用HTTPS上的REST接口，但当我们试图理解底层概念时，REST并不是很有帮助。我们将在第6章中重新审视替代网络通信策略的性能影响。无论如何，一个现实实现肯定需要至少某种形式的加密。
- en: '3.1.5 The takeaway: Asynchronous programming'
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.5 吸取的经验：异步编程
- en: Asynchronous programming can be effective in processing lots of simultaneous
    user requests. Two conditions must be present for `async` to improve the response
    time. First, communication with external processes must be limited. Second, the
    amount of CPU processing per request also should be small. Since both of these
    conditions tend to be at work with web servers, `async` programming can generally
    be helpful with most web applications.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 异步编程在处理大量同时用户请求时可以非常有效。为了`async`能够提高响应时间，必须满足两个条件。首先，与外部进程的通信必须受到限制。其次，每个请求的CPU处理量也应该较小。由于这两个条件通常与Web服务器相关，因此`async`编程通常对大多数Web应用都有帮助。
- en: Furthermore, although we concentrated on the fundamentals of `async` programming
    in this section, there is much more to be said about core asynchronous functionality
    with Python. I encourage you to read up on language functionalities like asynchronous
    iterators (`async for`) and context managers (`async with`). There is also a burgeoning
    field of asynchronous libraries like aiohttp for HTTP communication as alternatives
    to the well-known synchronous requests libraries.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽管我们在本节中集中讨论了`async`编程的基础，但关于Python的核心异步功能还有很多可以说的。我鼓励您了解语言功能，如异步迭代器（`async
    for`）和上下文管理器（`async with`）。还有许多异步库，如aiohttp，用于HTTP通信，作为已知同步请求库的替代品。
- en: 3.2 Implementing a basic MapReduce engine
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 实现基本的MapReduce引擎
- en: Now let’s get on with our main objective in the chapter, which is to *implement*
    a MapReduce framework. In the first section, we took care of the communication
    architecture *around* the framework. In this section, we will start implementing
    the core of the solution. This section will set up the basic solution from which
    we will derive more computationally efficient versions later in the chapter.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续本章的主要目标，即实现一个MapReduce框架。在第一部分，我们处理了框架周围的通信架构。在本节中，我们将开始实现解决方案的核心。本节将设置一个基本解决方案，我们将在本章的后面部分从中推导出更高效的计算版本。
- en: 3.2.1 Understanding MapReduce frameworks
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 理解MapReduce框架
- en: 'Let’s start with deconstructing a MapReduce framework to see what components
    go into it. From a theoretical perspective, MapReduce computations are separated
    into at least two halves: a map and a reduce part. Let’s see this in action with
    a typical example of a MapReduce application: word counting. In this case, we’ll
    use two lines from Shakespeare’s *The Tempest*: “I am a fool. To weep at what
    I am glad of.” You can see this input in a MapReduce in figure 3.3\. Other than
    map and reduce, in practice, other components need to exist. For example, the
    results from a map need to be shuffled before being sent to reduce processes:
    if the two instances of the word `am` were sent to distinct reduce processes,
    the count would not be correct.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从分解一个MapReduce框架开始，看看它包含哪些组件。从理论角度来看，MapReduce计算被分为至少两个部分：map和reduce部分。让我们通过一个典型的MapReduce应用程序的例子：单词计数，来观察这一过程。在这种情况下，我们将使用莎士比亚的《暴风雨》中的两行：“我是个傻瓜。为我所高兴的事而哭泣。”您可以在图3.3中看到这个输入。除了map和reduce之外，在实践中，还需要存在其他组件。例如，map的结果在发送到reduce进程之前需要被打乱：如果单词`am`的两个实例被发送到不同的reduce进程，计数将不会正确。
- en: '![](../Images/CH03_F05_Antao.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F05_Antao.png)'
- en: Figure 3.3 The basics of a`map_reduce` framework using word counting as an example.
    Traditional MapReduce frameworks have several processes or threads implementing
    the map and result steps. In many cases, these can be distributed across several
    computers.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 使用单词计数作为示例的`map_reduce`框架的基本原理。传统的MapReduce框架有几个进程或线程实现map和结果步骤。在许多情况下，这些可以分布在多台计算机上。
- en: 'Word counting could be implemented with a map function that would emit an entry
    for every word found with a count of 1, and a reduce function would sum all the
    map entries for the same word. So map would emit:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 单词计数可以通过一个map函数实现，该函数会对每个找到的单词发出一个条目，计数为1，而reduce函数会汇总所有相同单词的map条目。因此，map会发出：
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And reduce would then generate:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，reduce会生成：
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Somewhere in the middle, we need to shuffle the results so that a unique word
    would be seen only by a single reduce function. For example, if `am` was seen
    by two different reduce functions, then we would end up with two counts of 1 when
    we want to see one count of 2\. In our server, the shuffle function is built-in;
    the user doesn’t need to provide it.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个地方，我们需要对结果进行洗牌，以便一个唯一的单词只被一个 reduce 函数看到。例如，如果 `am` 被两个不同的 reduce 函数看到，那么当我们想要看到一个计数为
    2 的时候，我们最终会得到两个计数为 1。在我们的服务器中，洗牌函数是内置的；用户不需要提供它。
- en: 3.2.2 Developing a very simple test scenario
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 开发一个非常简单的测试场景
- en: 'Remember that we are *implementing* a MapReduce framework ourselves. While
    we won’t be *users*, we will need to test our MapReduce framework. To do that,
    we will return to the most common exercise with MapReduce: counting words in a
    text. Our framework will then be used with many other problems, but for basic
    testing of the framework, counting words will suffice.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们正在**自己实现**一个 MapReduce 框架。虽然我们不会是**用户**，但我们需要测试我们的 MapReduce 框架。为此，我们将回到
    MapReduce 最常见的练习：在文本中统计单词。然后我们的框架将被用于许多其他问题，但为了框架的基本测试，统计单词就足够了。
- en: 'The *user code* to implement this would be as simple as the following. Remember
    this is not what we were commissioned to do; it’s just the example that we will
    use for testing:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 实现此功能的**用户代码**将像以下这样简单。记住，这并不是我们被委托去做的事情；这只是我们将用于测试的例子：
- en: '[PRE8]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① We will be using functional notation on purpose as MapReduce has functional
    origins. If you use PEP 8, your syntax checker will complain as PEP 8 says, “Always
    use a def statement instead of an assignment statement that binds a lambda expression
    directly to an identifier.” The way this is reported will depend on your linter.
    It’s up to you whether you prefer to use this notation or the PEP 8 one, which
    would be of the form def emitter(word). We will be using this code to test the
    framework that we will build across the chapter.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们将故意使用函数式表示法，因为 MapReduce 具有函数式起源。如果你使用 PEP 8，你的语法检查器将会抱怨，因为 PEP 8 说：“始终使用
    def 语句而不是将 lambda 表达式直接绑定到标识符的赋值语句。”这种报告方式将取决于你的代码检查器。你是否有偏好使用这种表示法还是 PEP 8 的那种，后者将采用
    def emitter(word) 的形式。我们将使用这段代码来测试我们在本章中构建的框架。
- en: 3.2.3 A first attempt at implementing a MapReduce framework
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 尝试实现一个 MapReduce 框架
- en: 'Remember, the previous code is what your *user* will write. We will now implement
    a MapReduce engine, which is our real goal, that will count words and do much
    more. We will start with something that works but not much more, and we will develop
    an efficient engine through the rest of the chapter using threading, parallelism,
    and asynchronous interfaces (the first version is available in `03-concurrency/sec2-naive/
    naive_server.py`):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，之前的代码是用户将要编写的代码。我们现在将实现一个 MapReduce 引擎，这是我们真正的目标，它将统计单词并做更多的事情。我们将从一个能工作但不多的事情开始，然后在接下来的章节中通过使用线程、并行性和异步接口（第一版在
    `03-concurrency/sec2-naive/naive_server.py` 中可用）来开发一个高效的引擎：
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can now use this with:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以使用以下内容：
- en: '[PRE10]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`list` forces the lazy map call to actually execute (see chapter 2 if you have
    doubts about lazy semantics), and so you will get the output:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`list` 强制惰性映射调用实际执行（如果你对惰性语义有疑问，请参阅第 2 章），因此你会得到以下输出：'
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: While the previous implementation is quite clean from a conceptual point of
    view, from an operational perspective, it fails to grasp the most important operational
    expectation for a MapReduce framework—that its functions are run in parallel.
    In the next sections, we will make sure we create an efficient parallel implementation
    in Python.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管从概念上看，之前的实现相当清晰，但从操作角度来看，它未能满足 MapReduce 框架最重要的操作期望——即其函数是并行运行的。在接下来的几节中，我们将确保在
    Python 中创建一个高效的并行实现。
- en: 3.3 Implementing a concurrent version of a MapReduce engine
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 实现一个 MapReduce 引擎的并发版本
- en: Let’s try a second time and do a concurrent framework, this time by using multithreading.
    We will use the threaded executor from the `concurrent.futures` module to manage
    our MapReduce jobs. We are doing this to have a solution that is not only concurrent
    but also parallel (i.e., allowing us to use all the compute power available)—at
    least that is what we hope.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再试一次，这次实现一个并发框架，这次通过使用多线程。我们将使用 `concurrent.futures` 模块中的线程执行器来管理我们的 MapReduce
    任务。我们这样做是为了得到一个不仅并发而且并行（即，允许我们使用所有可用的计算能力）的解决方案——至少这是我们希望做到的。
- en: 3.3.1 Using concurrent.futures to implement a threaded server
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 使用 concurrent.futures 实现线程化服务器
- en: We start with `concurrent.futures` because it is more declarative and higher
    level than the most commonly used `threading` and `multiprocessing` modules. These
    are foundational modules in the field, and we will use `multiprocessing` in the
    next section, as its lower-level interface will allow us to allocate CPU resources
    more precisely.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从`concurrent.futures`开始，因为它比最常用的`threading`和`multiprocessing`模块更声明式和更高级。这些是领域中的基础模块，我们将在下一节中使用`multiprocessing`，因为它的低级接口将允许我们更精确地分配CPU资源。
- en: 'Here is the new version (the code is available in `03-concurrency/sec3-thread/
    threaded_mapreduce_sync.py`):'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是新的版本（代码可在`03-concurrency/sec3-thread/ threaded_mapreduce_sync.py`中找到）：
- en: '[PRE12]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① We use the threaded executor from the concurrent.futures module.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们使用`concurrent.futures`模块中的线程executor。
- en: ② The executor can work as a context manager.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ② executor可以作为上下文管理器工作。
- en: ③ Executors have a map function with blocking behavior.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ③ Executors有一个具有阻塞行为的map函数。
- en: ④ We use a very simple shuffler function.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 我们使用一个非常简单的洗牌函数。
- en: Our function again takes some input along with `mapper` and `reducer` functions.
    The executor from `concurrent.futures` is responsible for thread management, although
    we can specify the number of threads we want. If not, the default is related to
    `os.cpu_count`; the actual number of threads varies across Python versions. This
    is summarized in figure 3.4.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的功能再次接受一些输入，包括`mapper`和`reducer`函数。`concurrent.futures`模块中的executor负责线程管理，尽管我们可以指定我们想要的线程数。如果没有指定，默认值与`os.cpu_count`相关；实际的线程数在不同的Python版本中有所不同。这总结在图3.4中。
- en: '![](../Images/CH03_F04_Antao.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F04_Antao.png)'
- en: Figure 3.4 Threaded execution of our MapReduce framework
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 我们的MapReduce框架的线程执行
- en: Remember that we need to make sure that the results for the same object (a word
    in our example) are sent to the correct reduce function. In our case, we implement
    a very simple version in the `distributor` default dictionary that creates an
    entry per word.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们需要确保相同对象（在我们的例子中是一个单词）的结果被发送到正确的reduce函数。在我们的情况下，我们在`distributor`默认字典中实现了一个非常简单的版本，为每个单词创建一个条目。
- en: The previous code can have a fairly big memory footprint, especially because
    the shuffler will hold all results in memory, although in a compact fashion. But
    for the sake of simplicity, we will leave it as it is.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码可能有一个相当大的内存占用，特别是因为洗牌器将所有结果都保留在内存中，尽管是以紧凑的方式。但为了简单起见，我们将保持原样。
- en: Exactly how the number of workers is managed is more or less a black box with
    `concurrent.futures`. As such, we do not know for what it has been optimized.
    Consequently, if we want to make sure we are extracting the maximum performance,
    we must be in full control of how the execution is done. If you want to fine-tune
    worker management, you will need to use the `threading` module directly.[²](#pgfId-1014957)
    We will see how to do this in the next section.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`concurrent.futures`管理工作者的数量基本上是一个黑盒。作为这样的模块，我们不知道它被优化了什么。因此，如果我们想确保我们正在提取最大的性能，我们必须完全控制执行的方式。如果你想微调工作者管理，你需要直接使用`threading`模块。[²](#pgfId-1014957)
    我们将在下一节中看到如何做到这一点。
- en: You can try this solution with
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试这个解决方案，与
- en: '[PRE13]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: and the output will be the same as in the previous section.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 并且输出将与上一节相同。
- en: 'However, the previous solution has a problem: it doesn’t allow any kind of
    interaction with the ongoing outside program. That is, when you do `executor.map`,
    you will have to wait until the complete solution is computed. This is irrelevant
    with an example with five words, but you might want to have some feedback with
    very large texts. For example, you want to be able to report the percentage of
    progress done while the code runs. This requires a somewhat different solution.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上一个解决方案有一个问题：它不允许与正在进行的程序有任何交互。也就是说，当你执行 `executor.map` 时，你必须等待完整的解决方案计算完成。对于只有五个单词的例子来说，这并不相关，但你可能希望对于非常长的文本有一些反馈。例如，你希望在代码运行时能够报告完成的百分比。这需要一种稍微不同的解决方案。
- en: 3.3.2 Asynchronous execution with futures
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 使用futures的异步执行
- en: 'First, let’s just code the map part to understand what is going on (the code
    is available in `03-concurrency/sec3-thread/threaded_mapreduce.py`):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们只编写map部分来理解正在发生的事情（代码可在`03-concurrency/sec3-thread/threaded_mapreduce.py`中找到）：
- en: '[PRE14]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① We use submit instead of map when calling the executor.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ① 在调用executor时，我们使用submit而不是map。
- en: While the map function of the executor waits for results, `submit` doesn’t.
    We will see what that means when we run this soon.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当executor的map函数等待结果时，`submit`不会等待。我们将在稍后运行时看到这意味着什么。
- en: 'We will change our emitter to be able to track what is going on:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将改变我们的发射器，以便能够跟踪正在发生的事情：
- en: '[PRE15]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `sleep` call is there to slow down the code, allowing us to track what
    is going on even with a simple example. Let’s use our map function as it is:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`sleep`调用是为了减慢代码的执行速度，这样我们甚至可以通过一个简单的示例来跟踪正在发生的事情。让我们使用我们的map函数：'
- en: '[PRE16]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'If you print the last item from the list, you might get something unexpected:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打印列表中的最后一个项目，你可能会得到一些意外的东西：
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You do not get `(''rocks'', 1)`, but instead you get a *future*. A future represents
    a potential result that can be subject to `await` and checked for its state. We
    can now allow the user to track progress like this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你不会得到`('rocks', 1)`，而是一个*未来*。未来代表一个可能的结果，它可以被`await`处理，并检查其状态。我们现在可以允许用户以这种方式跟踪进度：
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ① We put only four executors to let us track progress as we have five tasks.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们只放置了四个执行者，这样我们可以跟踪进度，因为我们有五个任务。
- en: ② We print status while there are still tasks to be done.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ② 当还有任务需要完成时，我们会打印状态。
- en: ③ Checks whether the future is done
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 检查未来是否完成
- en: ④ Sleeps for a bit as we do not want a barrage of text
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 我们会稍微休息一下，因为我们不希望出现大量的文本
- en: 'If you run the previous code, you will get a few lines with `Still not finalized...`.
    Typically for the first 10 s, you will see five and then just one. As there are
    four workers, it takes 10 s to do the first four, and then the final one can start.
    Given that this is concurrent code, this can change a bit from run to run, so
    the way threads are preempted can vary every time you run this code: it is nondeterministic.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行前面的代码，你会得到几行显示“仍然未最终确定...”。通常在前10秒内，你会看到五个，然后只剩下一个。因为有四个工作者，前四个需要10秒来完成，然后最后一个才能开始。鉴于这是并发代码，每次运行的结果可能会有所不同，因此线程被抢占的方式可能会每次都不同：它是非确定性的。
- en: 'There is one final piece of the puzzle left to do, which will be in the last
    version of the threaded executor: we need a way for the caller to be able to be
    informed of the progress. The caller will have to pass a callback function, which
    will be called when an important event occurs. In our case, that important event
    will be tracking the completion of all map and reduce jobs. This is implemented
    in the following code:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一块最后的拼图需要完成，这将包含在最后版本的线程执行者中：我们需要一种方式让调用者能够得知进度。调用者将不得不传递一个回调函数，该函数将在发生重要事件时被调用。在我们的情况下，这个重要事件将是跟踪所有map和reduce作业的完成。这已经在以下代码中实现：
- en: '[PRE19]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ① report_progress will require a callback function that will be called every
    half second with statistical information about jobs done.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ① `report_progress`将需要一个回调函数，该函数将每半秒调用一次，并带有关于已完成作业的统计信息。
- en: ② We report the progress for all map tasks.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们会报告所有map任务的进度。
- en: ③ Because the results are actually futures, we need to get those from the future
    objects.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 因为结果实际上是未来对象，所以我们需要从未来对象中获取这些结果。
- en: ④ We report the progress for all reduce tasks.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 我们会报告所有reduce任务的进度。
- en: ⑤ Because the results are actually futures, we need to get those from the future
    objects.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 因为结果实际上是未来对象，所以我们需要从未来对象中获取这些结果。
- en: 'So, every 0.5 s while the map and reduce are running, the user-supplied callback
    function will be executed. A callback can be as simple or as complicated as you
    want, although it should be fast as everything else will be waiting for it. For
    the word count example that we use for testing, we have a very simple one:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在map和reduce运行期间，每0.5秒，用户提供的回调函数将被执行。回调函数可以像你想要的那样简单或复杂，尽管它应该很快，因为其他所有内容都将等待它。对于我们在测试中使用的单词计数示例，我们有一个非常简单的示例：
- en: '[PRE20]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Note that the callback function signature is not arbitrary: it has to follow
    the protocol imposed by `report_progres`, which requires as arguments the tag
    and the number of done and not done tasks.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，回调函数的签名不是任意的：它必须遵循`report_progres`强加的协议，该协议需要标签以及已完成和未完成的任务数量作为参数。
- en: If you run
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行
- en: '[PRE21]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'you will have a few lines printing the ongoing status of the operation, followed
    by the result:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到几行打印操作正在进行的状态，然后是结果：
- en: '[PRE22]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: It would not be too difficult, for example, to use the return value as an indicator
    to the MapReduce framework to cancel the execution. This would allow us to change
    the semantics of the callback function to interrupt the process.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用返回值作为指示器来取消MapReduce框架的执行并不困难。这将允许我们更改回调函数的语义以中断进程。
- en: Unfortunately, this solution is concurrent but not parallel. This is because
    Python (or rather, CPython) only executes one thread at a time, courtesy of the
    infamous CPython GIL, the Global Interpreter Lock. Let’s look closer at the GIL
    and how it handles threads in the next section.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这个解决方案是并发的但不是并行的。这是因为Python（或者更确切地说，CPython）一次只执行一个线程，归功于臭名昭著的CPython GIL（全局解释器锁）。让我们在下一节更详细地看看GIL以及它是如何处理线程的。
- en: 3.3.3 The GIL and multithreading
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.3 GIL和多线程
- en: 'While CPython makes use of OS threads so they are preemptive threads, the GIL
    imposes a restriction so that only one thread can run at a time. So, you might
    have a multithreaded program running on a multicore computer, but you will end
    up with no parallelism. It’s actually a bit worse than that: the performance of
    thread swapping can be quite bad in multicore computers due to the friction between
    the GIL, which doesn’t allow more than one thread to run at a time, and the CPU
    and OS, which are actually optimized to do the opposite.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 CPython 使用操作系统线程，因此它们是抢占式线程，但GIL施加了一个限制，使得一次只能运行一个线程。因此，你可能在多核计算机上运行一个多线程程序，但最终你将没有任何并行性。实际上，情况可能更糟：在多核计算机上线程切换的性能可能会相当差，这是由于GIL（它不允许一次运行多个线程）与CPU和操作系统之间的摩擦，而CPU和操作系统实际上被优化来做相反的事情。
- en: This book includes a section on multithreading because any book related to performance
    would be incomplete without it. But truth be told, if you want performance, Python
    threads are rarely the best solution.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书包括一个关于多线程的章节，因为任何与性能相关的书籍如果没有它就不完整。但说实话，如果你想获得性能，Python线程很少是最好的解决方案。
- en: GIL problems are overrated. The fact is that if you need to do high-performance
    code at the thread level, Python is probably too slow anyway. At least the CPython
    implementation, but probably also Python’s dynamic features, impose a cost. You
    will want to implement any extremely efficient code in a lower-level language
    like C or Rust or by using a system like Cython or Numba, which we will study
    later on.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 'GIL问题被高估了。事实上，如果你需要在线程级别进行高性能代码，Python可能本身就太慢了。至少CPython实现，可能还有Python的动态特性，都会带来成本。你将希望用C或Rust这样的低级语言实现任何极其高效的代码，或者使用Cython或Numba这样的系统，我们将在后面学习。 '
- en: 'The GIL provides a few escape routes for lower-level code implemented in other
    languages: when you enter your lower-level solution you can actually release the
    GIL and use parallelism to your heart’s content. This is what libraries like NumPy,
    SciPy, and scikit-learn do. They have multithreaded code written in C or Fortran
    that releases the GIL and is actually parallel. So your code case can still be
    parallel in a threaded world. It’s just that the parallel part will not be written
    in Python.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: GIL提供了几个用于其他语言实现的低级代码的逃生路线：当你进入你的低级解决方案时，你实际上可以释放GIL并充分利用并行性。这正是NumPy、SciPy和scikit-learn等库所做的事情。它们用C或Fortran编写的多线程代码会释放GIL并且实际上是并行的。所以你的代码在多线程世界中仍然可以是并行的。只是并行部分不会用Python编写。
- en: But you can still write efficient parallel code in pure Python and do that at
    a level of computing granularity that makes sense in Python. You do this not with
    multithreading but with *multiprocessing*.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 但你仍然可以用纯Python编写高效的并行代码，并在Python有意义的计算粒度级别上做到这一点。你不是通过多线程，而是通过*多进程*来做到这一点。
- en: PyPy
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: PyPy
- en: While CPython is the standard implementation for Python, others exist like IronPython
    and Jython for .NET and the JVM, respectively. Another implementation worth mentioning
    is PyPy, which is not an interpreter but a just-in-time compiler. PyPy is not
    a drop-in replacement for CPython as many CPython libraries do not work directly
    with it. But it might be a faster implementation if the libraries supported are
    the ones you need. While PyPy is in many cases faster than CPython, it still does
    have a GIL, so it won’t solve that problem. In this book, we will stick with CPython,
    but for a reasonable subset of efficiency cases, PyPy might be a potential alternative.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 CPython 是 Python 的标准实现，但还存在其他实现，如IronPython和Jython，分别用于.NET和JVM。另一个值得提到的实现是PyPy，它不是一个解释器，而是一个即时编译器。PyPy不是CPython的替代品，因为许多CPython库不能直接与它一起使用。但如果支持的库是你需要的，它可能是一个更快的实现。虽然PyPy在许多情况下比CPython快，但它仍然有一个GIL，所以它不会解决这个问题。在这本书中，我们将坚持使用CPython，但在效率合理的子集情况下，PyPy可能是一个潜在的替代方案。
- en: 'A final word: if you confuse *PyPy*, the Python implementation, with *PyPI*,
    the package repository, know that you are not alone.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点：如果你把Python实现*PyPy*和包仓库*PyPI*搞混，要知道你并不孤单。
- en: 3.4 Using multiprocessing to implement MapReduce
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 使用多进程实现 MapReduce
- en: 'Due to the GIL, our multithreaded code is not really parallel. We can turn
    to two directions to solve this: we can re-implement our Python code in a lower-level
    language like C or Rust, or as our solution in this section, we can turn to multiprocessing
    to have parallelism and make usage of all CPU power available. Lower-level solutions
    will be addressed in later chapters.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 GIL（全局解释器锁），我们的多线程代码实际上并不是并行的。我们可以转向两个方向来解决这个问题：我们可以用 C 或 Rust 这样的底层语言重新实现我们的
    Python 代码，或者像本节中的解决方案一样，转向多进程以实现并行性并利用所有可用的 CPU 功率。底层解决方案将在后面的章节中讨论。
- en: 3.4.1 A solution based on concurrent.futures
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 基于 concurrent.futures 的解决方案
- en: '*In theory*, a solution based on `concurrent.futures` is quite simple. It is
    a design goal of the module to make it easy to change one of the imports from
    `ThreadPoolExecutor` to `ProcessPoolExecutor` (this code is available in `03-concurrency/sec4-multiprocess/
    futures_mapreduce.py`):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*理论上*，基于 `concurrent.futures` 的解决方案相当简单。该模块的设计目标之一是使其容易将 `ThreadPoolExecutor`
    中的一个导入更改为 `ProcessPoolExecutor`（此代码位于 `03-concurrency/sec4-multiprocess/ futures_mapreduce.py`）：'
- en: '[PRE23]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If you replace this line on the asynchronous version of the previous section,
    you will notice that something is wrong as the code seems to freeze in the reduce
    part. We need to dig down; as such, we will build a more informative `report_progress`
    function from the last section:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你替换上一节异步版本的这一行，你会注意到有问题，因为代码似乎在减少部分冻结了。我们需要深入挖掘；因此，我们将从上一节构建一个更详细的 `report_progress`
    函数：
- en: '[PRE24]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We just added two prints. If we run the code again, we get:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚添加了两个打印语句。如果我们再次运行代码，我们会得到：
- en: '[PRE25]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'It turns out that lambdas (remember that our `counter` function was written
    as a lambda) cannot be pickled. Yet multiprocessing communication is done via
    the `pickle` module. Therefore, our `counter` function cannot be transferred to
    the subprocess as is. We can just rewrite it as a `def` function:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明 lambda（记住我们的 `counter` 函数是作为 lambda 编写的）无法被序列化。因此，多进程通信是通过 `pickle` 模块完成的。因此，我们的
    `counter` 函数不能直接传输到子进程中。我们可以将其重写为一个 `def` 函数：
- en: '[PRE26]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This solves our specific test example, but the larger point is that you cannot
    simply do a drop-in replacement from the threaded executor to the process-based
    one. What other features might be different? Turn to the next section to find
    out.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这解决了我们的特定测试示例，但更重要的观点是，你不能简单地从线程执行器直接替换为基于进程的执行器。其他可能不同的功能有哪些？转向下一节以了解详情。
- en: Problems with data and code sharing using the Python multiprocessing module
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python 的多进程模块进行数据和代码共享的问题
- en: We have now seen that transmitting lambdas across processes is not possible
    with a default `pickle` configuration. Or, if you want to do that, you have to
    implement your own protocol.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，在默认的 `pickle` 配置下，无法在进程间传输 lambda 表达式。或者，如果你想这么做，你必须实现自己的协议。
- en: Generally, if `pickle` cannot handle it, then you have to take care of that
    out-off-band as multiprocessing relies on `pickle` for communication. This might
    include objects from foreign libraries, especially if they have non-Python implementations.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果 `pickle` 无法处理，那么你必须单独处理这个问题，因为多进程依赖于 `pickle` 进行通信。这可能包括来自外部库的对象，特别是如果它们有非
    Python 实现。
- en: File pointers, database connections, and sockets are either impossible to transfer
    or require extra care. With threads, all these object types can be shared, although
    we need to check whether they are thread-safe. Another problem with `pickle` is
    that it is quite slow. In cases where there is a lot of data to transfer, that
    might defeat the purpose of using multiprocessing altogether.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 文件指针、数据库连接和套接字要么无法传输，要么需要额外小心。使用线程，所有这些对象类型都可以共享，尽管我们需要检查它们是否线程安全。`pickle` 的另一个问题是它相当慢。在需要传输大量数据的情况下，这可能会完全抵消使用多进程的目的。
- en: Using Python’s communication primitives is perfectly fine for coarse-grained
    processing with low communication. But there should be some care in scenarios
    with a lot of communication overhead as the speed gained with multiprocessing
    might be lost in communication time.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python 的通信原语对于粗粒度处理且通信量低的情况是完全可以的。但在通信开销很大的场景中，应该有所注意，因为通过多进程获得的速度可能会在通信时间中丢失。
- en: 3.4.2 A solution based on the multiprocessing module
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 基于 multiprocessing 模块的解决方案
- en: '`concurrent.futures` gives us a very simple interface to do concurrent processing.
    For more obvious problems, it can be quite efficient in terms of both programmer
    productivity and computing performance. But programming simplicity comes at a
    cost: we lose control of how code is executed. What is the order of execution
    of futures? While we define the maximum number of workers, how many are really
    available at a certain point in time? Are processes recycled or recreated from
    scratch for every task? With `concurrent.futures`, this is defined by the executor,
    and we have no control over it.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`concurrent.futures`为我们提供了一个非常简单的接口来进行并发处理。对于更明显的问题，它在程序员生产力和计算性能方面都可以非常高效。但编程简单性是有代价的：我们失去了对代码执行方式的控制。futures的执行顺序是什么？虽然我们定义了最大工作线程数，但在某个特定时间点有多少是真正可用的？进程是回收还是为每个任务从头创建？在`concurrent.futures`中，这是由执行器定义的，我们无法控制它。'
- en: In our case, we actually would like to enforce some policies to achieve high
    performance. For example, we want to create all processes before work arrives
    or keep processes alive even when there are no tasks. This is because creating
    and destroying processes when requests arrive has an overhead that we prefer to
    pay when we are not dealing with processing those requests. We will begin by creating
    a process pool ourselves.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们实际上希望实施一些策略以实现高性能。例如，我们希望在任务到达之前创建所有进程，或者在没有任何任务时保持进程存活。这是因为当请求到达时创建和销毁进程有开销，我们更愿意在处理这些请求时支付这个开销。我们将首先自己创建一个进程池。
- en: 'We will start with a simple solution that doesn’t allow us to track progress
    in real-time (the code is available in `03-concurrency/sec4-multiprocess/mp_mapreduce_0.py`):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个简单的解决方案开始，这个解决方案不允许我们实时跟踪进度（代码位于`03-concurrency/sec4-multiprocess/mp_mapreduce_0.py`）：
- en: '[PRE27]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ① We import the multiprocessing module.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们导入multiprocessing模块。
- en: ② We create a pool with two processes.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们创建了一个包含两个进程的池。
- en: ③ The pool provides a synchronous map function.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 池提供了一个同步的map函数。
- en: 'This code is as simple as it gets. The only new line is the creation of a `Pool`.
    The pool is created every time a MapReduce operation is requested, so it’s not
    persistent over multiple invocations: we are paying the price of pool creation
    for every execution.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码是最简单的。唯一的新行是创建一个`Pool`。每当请求MapReduce操作时，都会创建池，因此它不是跨多个调用持久存在的：我们为每次执行支付了池创建的代价。
- en: '`CPU_count` vs. `sched_getaffinity` to determine the pool size'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`CPU_count`与`sched_getaffinity`的比较来确定池的大小'
- en: 'The previous code specifies two processes to be created in our pool. In most
    cases, you will want this number to be a function of your computing power. The
    default for the pool is `os.cpu_count`, which is actually a misnomer: it normally
    reports the number of hyperthreads, not CPUs.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码指定了在我们的池中创建两个进程。在大多数情况下，你希望这个数字是计算能力的函数。池的默认值是`os.cpu_count`，这实际上是一个误称：它通常报告的是超线程的数量，而不是CPU的数量。
- en: A slightly more rigorous alternative would be to use `len(os.sched_getaffinity())`
    as it reports all the cores that are accessible to you. Your computer might have
    more, but the OS, container, or virtual machine may have limited your access to
    part of them.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 一个稍微更严谨的替代方案是使用`len(os.sched_getaffinity())`，因为它报告了你可访问的所有核心。你的电脑可能有更多，但操作系统、容器或虚拟机可能限制了你对其中一部分的访问。
- en: 'Warning The semantics of the `Pool.map` function is eager, whereas the built-in
    map function is lazy. As such this code is not semantically equivalent:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：`Pool.map`函数的语义是急切的，而内置的map函数是懒散的。因此，这段代码在语义上并不等价：
- en: '[PRE28]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The first returns immediately and did not execute `fun`. `list(map(fun, data)`
    would be the eager equivalent. A typical development pattern is to replace `Pool.map`
    with map, as debugging code is easier if done in the same process. However, this
    is not entirely correct. On the `multiprocessing` side, you also have `imap`,
    which is a lazier version, and an asynchronous version in `map_async`.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次返回立即发生，并没有执行`fun`。`list(map(fun, data))`将是急切等价的形式。典型的发展模式是将`Pool.map`替换为map，因为如果在同一进程中调试代码会更简单。然而，这并不完全正确。在`multiprocessing`方面，你还有`imap`，这是一个更懒散的版本，以及`map_async`中的异步版本。
- en: 3.4.3 Monitoring the progress of the multiprocessing solution
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.3 监控多进程解决方案的进度
- en: 'As it stands, `map_async` does not support progress tracking. Note that it
    has callback support, but it only calls the `callback` function when all the results
    become ready. We would like something more granular: the ability to have a call
    every time an element of the iterator is ready. That’s what we need for progress
    tracking.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如目前所示，`map_async`不支持进度跟踪。请注意，它有回调支持，但它只在所有结果都准备好时调用`callback`函数。我们希望有更细粒度的功能：每次迭代器中的元素准备好时都能进行调用。这正是我们需要用于进度跟踪的。
- en: 'We will change the code to support it. While there is a `Pool.map_async` function
    that can be useful on many occasions, its callback system only reports the very
    end of the execution, and that is not enough. We need a slightly lower-level solution
    (this code is available in `03-concurrency/sec4-multiprocess/mp_mapreduce.py`):'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将修改代码以支持它。虽然有一个`Pool.map_async`函数在许多场合可能很有用，但其回调系统只报告执行的最后阶段，这还不够。我们需要一个更底层的解决方案（此代码位于`03-concurrency/sec4-multiprocess/mp_mapreduce.py`）：
- en: '[PRE29]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ① We use Pool.apply_async to start individual jobs.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们使用`Pool.apply_async`来启动单个任务。
- en: ② Note that the parameter to the function is specified as a tuple.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ② 注意，函数的参数被指定为一个元组。
- en: ③ Getting results from the async objects using its get method
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用异步对象的get方法获取结果
- en: The code is not much different from the `concurrent.futures` solution—to the
    point that we might ask whether the lack of flexibility from `concurrent.futures`
    is worthwhile for the extra simplicity.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 代码与`concurrent.futures`解决方案没有太大区别——以至于我们可能会问`concurrent.futures`缺乏灵活性是否值得额外的简单性。
- en: Our `map_reduce` function now uses a pool provided by the user, allowing for
    pool recycling. This will generally be more efficient than starting new processes
    every time a new operation is done. In our example, there is almost no overhead,
    but in more complex examples, initialization of each process might be time- and
    resource-consuming.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的`map_reduce`函数使用用户提供的池，允许池的回收利用。这通常比每次进行新操作时启动新进程更有效。在我们的例子中，几乎没有开销，但在更复杂的例子中，每个进程的初始化可能既耗时又消耗资源。
- en: 'To call this code, we now have to create the pool beforehand. This is simple:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 要调用此代码，我们现在必须先创建池。这是简单的：
- en: '[PRE30]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ① We close the pool.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们关闭池。
- en: ② We wait for all the processes to terminate.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们等待所有进程终止。
- en: We could use a pool from a context manager here, but this allows us to see that
    cleaning up a pool is not just closing all the processes; it is also waiting for
    them to exit—the `join` call. A more forceful alternative to `close` would be
    to `terminate`, which would force existing processes to terminate even without
    finalizing any ongoing work.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里可以使用上下文管理器中的池，但这让我们看到清理池不仅仅是关闭所有进程；它还包括等待它们退出——`join`调用。相对于`close`的更强替代方案是`terminate`，这将强制现有进程终止，即使没有完成任何正在进行的工作。
- en: Overcommitting or undercommitting CPU resources
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 过度分配或未充分分配CPU资源
- en: When we create our pool, we are using the default size, `os.cpu_count()`, but
    there are many situations in which you will want to undercommit resources. There
    are even situations when overcommitting will be fine.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建池时，我们使用默认大小，`os.cpu_count()`，但有许多情况下你将想要未充分分配资源。甚至有情况，过度分配资源也是可以接受的。
- en: 'The most common reason for undercommitting is when the processes are IO bound:
    too much IO can easily trash the machine as a lot of processes can cause more
    IO load than the machine can handle. This is especially true for disk IO.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的未充分分配资源的原因是当进程是I/O密集型时：过多的I/O操作很容易使机器崩溃，因为许多进程可能造成的I/O负载超过了机器的处理能力。这对于磁盘I/O尤其如此。
- en: If the processes take a lot of memory, then you also need to scale down as you
    might have reduced performance due to memory cache usage. In the worst case, the
    OS might start killing processes if the computer runs out of memory.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如果进程消耗了大量的内存，那么你也需要相应地减少资源分配，因为你可能因为内存缓存的使用而降低了性能。在最坏的情况下，如果计算机内存耗尽，操作系统可能会开始杀死进程。
- en: A typical case for overcommitting is when you are waiting for the network. This
    means that processes might sit idle for a good part of the time, and so the CPU
    resource is available.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 过度分配资源的一个典型情况是当你正在等待网络。这意味着进程可能会长时间处于空闲状态，因此CPU资源是可用的。
- en: Paradoxically, overcommitting of CPU can be useful for some CPU-bound processes—for
    example, when CPU usage per process is not continuous but occurs in bursts or
    when there is a large setup time before the computation actually begins.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 反讽的是，CPU过度分配对于一些CPU密集型过程可能是有用的——例如，当每个进程的CPU使用率不是连续的，而是以突发方式出现，或者在计算真正开始之前有大量设置时间时。
- en: 'The `report_progress` function is almost the same: it calls the callback when
    a job has finished. The call to `Future.done` is replaced by `AsyncReturn.ready`:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`report_progress`函数几乎相同：当作业完成时调用回调。将`Future.done`的调用替换为`AsyncReturn.ready`：'
- en: '[PRE31]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: You can now run the code, and it all works. But is the solution fast enough?
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以运行代码，并且一切正常。但是，这个解决方案是否足够快？
- en: 3.4.4 Transferring data in chunks
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.4 分块传输数据
- en: To answer the question of whether the solution is fast enough, we need to compare
    it to something else. As we have seen in the previous chapter and will revisit
    in a later one, chunking for disk writing can dramatically speed up disk write
    operations. Is that technique also good for CPU costs and interprocess communication?
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答解决方案是否足够快的问题，我们需要将其与其他东西进行比较。正如我们在上一章中看到的那样，并且将在稍后重提，块化对于磁盘写入可以显著加快磁盘写入操作。这种技术对于CPU成本和进程间通信也有好处吗？
- en: To answer this question, we will make a minor change to our MapReduce architecture.
    We will add a splitting phase at the beginning that will send the data not as
    single elements but as chunks. Figure 3.5 introduces a new step to figure 3.3,
    which is responsible for splitting.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，我们将对我们的MapReduce架构进行一些小的修改。我们将在开始时添加一个分割阶段，将数据作为块而不是单个元素发送。图3.5向图3.3中引入了一个新的步骤，该步骤负责分割。
- en: '![](../Images/CH03_F06_Antao.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F06_Antao.png)'
- en: Figure 3.5 A`map_reduce` framework with splitting (really chunking in our case)
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 带分割的`map_reduce`框架（在我们的情况下实际上是块化）
- en: 'Our split is actually quite simple, but advanced MapReduce frameworks can do
    substantially more advanced optimizations here. We will start by looking at the
    code that submits chunked jobs and dechunks them on the pool processes (this code
    is in `03-concurrency/sec4-multiprocess/chunk_mp_mapreduce.py`):'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分割实际上相当简单，但高级MapReduce框架可以在这里做更多高级优化。我们将首先查看提交块化作业并在池进程上解块（此代码位于`03-concurrency/sec4-multiprocess/chunk_mp_mapreduce.py`）的代码：
- en: '[PRE32]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ① We now have the chunk generator that will split an iterator in lists of chunk_size.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们现在有了块生成器，它将分割迭代器为大小为`chunk_size`的列表。
- en: ② A chunk runner is executed on the pool processes to unpack the chunk list.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ② 在池进程上执行块运行器以解包块列表。
- en: ③ We have to adapt our function to submit jobs to the pool by having some middleware
    to unpack lists.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 我们必须通过添加一些中间件来解包列表，以使我们的函数能够向池提交作业。
- en: ④ Here we call the chunk function.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 这里我们调用块函数。
- en: ⑤ We now call the middleware and not the final function directly.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 我们现在调用中间件而不是直接调用最终函数。
- en: '`chunked_async_map` is the code that will distribute the work across the pools.
    It calls the `chunk` generator to split the input into chunks of `chunk_size`.
    Note that it doesn’t call the desired function directly anymore: the first thing
    that runs on the pool processes is `chunk_runner`, which will iterate each element
    of the chunk and call the real work function, `fun`.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`chunked_async_map`是将在池之间分配工作的代码。它调用`chunk`生成器将输入分割为大小为`chunk_size`的块。请注意，它不再直接调用所需函数：在池进程上首先运行的是`chunk_runner`，它将迭代块中的每个元素并调用实际的工作函数`fun`。'
- en: 'You might think that there is a simpler implementation of the `chunk` generator,
    like this:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为`chunk`生成器的实现更简单，如下所示：
- en: '[PRE33]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The problem with the implementation is that it requires a `len(my_list)`, thus
    restricting our input to being a list. An iterator can be lazy and, hence, occupies
    less memory and probably requires less CPU to process.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 实现的问题在于它需要`len(my_list)`，因此限制了我们的输入必须是列表。迭代器可以是懒惰的，因此占用更少的内存，并且可能需要更少的CPU来处理。
- en: 'We now need to change our top MapReduce function:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要修改我们的顶级MapReduce函数：
- en: '[PRE34]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ① We add chunk_size as a parameter.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们添加`chunk_size`作为参数。
- en: ② We use extend instead of append.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们使用`extend`而不是`append`。
- en: ③ We add chunk_size as a parameter.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 我们添加`chunk_size`作为参数。
- en: The only caveat is that the result of each execution is not a single element
    anymore, but a list of elements. So we have to `extend` the list and not `append`
    to it.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的注意事项是，每次执行的结果不再是单个元素，而是一系列元素。因此，我们必须`extend`列表而不是`append`到它。
- en: 'To have a better speed test, we will use Tolstoy’s *Anna Karenina*, available
    at Project Gutenberg ([http://gutenberg.org/files/1399/1399-0.txt](http://gutenberg.org/files/1399/1399-0.txt)).
    Here is the calling code:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行更好的速度测试，我们将使用托尔斯泰的《安娜·卡列尼娜》，可在Project Gutenberg([http://gutenberg.org/files/1399/1399-0.txt](http://gutenberg.org/files/1399/1399-0.txt))找到。以下是调用代码：
- en: '[PRE35]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ① This reads all the text into a list.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这将所有文本读入一个列表。
- en: ② The chunk size is a command line parameter.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ② 块大小是一个命令行参数。
- en: ③ We print all the word counts in increasing order.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 我们按升序打印所有单词计数。
- en: I have run the previous code with a chunk size of 1, 10, 100, 1,000, and 10,000\.
    The time for each case is depicted in table 3.1.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经用块大小为1、10、100、1,000和10,000运行了之前的代码。每种情况的时间都在表3.1中展示。
- en: Table 3.1 Running times for different chunk sizes
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1 不同块大小的运行时间
- en: '| Chunk size | Time (s) |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 块大小 | 时间（秒） |'
- en: '| 1 | 114.2 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 114.2 |'
- en: '| 10 | 12.3 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 12.3 |'
- en: '| 100 | 4.3 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 4.3 |'
- en: '| 1,000 | 3.1 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 1,000 | 3.1 |'
- en: '| 10,000 | 3.1 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 10,000 | 3.1 |'
- en: 'The numbers in table 3.1 speak for themselves: chunking can massively increase
    the performance of our framework. Chunking is such an important concept that we
    will be revisiting it in other chapters.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1中的数字不言自明：chunking可以极大地提高我们框架的性能。chunking是一个如此重要的概念，我们将在其他章节中再次讨论它。
- en: Tip If you use `map` from the `Pool` object, chunking is implemented for you
    for free. You just have to add the `chunksize` parameter. The same is true for
    `map_async` and `imap`. More generally, when you use a parallel library, be sure
    to check whether the chunking functionality is present. In many cases, you will
    not need to implement it yourself.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果你使用`Pool`对象的`map`，chunking会免费为你实现。你只需添加`chunksize`参数。对于`map_async`和`imap`也是如此。更一般地说，当你使用并行库时，务必检查是否提供了chunking功能。在许多情况下，你不需要自己实现它。
- en: Shared memory
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存
- en: An alternative to the (implicit) message-passing solution presented here would
    be to use shared memory services. Naive shared memory models such as the ones
    available in Python’s built-in libraries are notoriously very bug-prone to use,
    and as such, we will not address them here. If you need memory sharing, you are
    probably in a situation in which you will have to implement your code in a lower-level
    solution anyway. We will discuss shared memory in later contexts where we use
    lower-level approaches connected to our Python code to do processing.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提出的（隐式）消息传递解决方案的替代方案将是使用共享内存服务。像Python内置库中可用的那些简单的共享内存模型，因其非常容易出错而臭名昭著，因此我们在此不予讨论。如果你需要内存共享，你可能处于必须使用底层解决方案实现代码的情况。我们将在使用与我们的Python代码相关联的底层方法进行处理的后续上下文中讨论共享内存。
- en: '3.5 Tying it all together: An asynchronous multithreaded and multiprocessing
    MapReduce server'
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 将一切整合：异步多线程和 multiprocessing MapReduce 服务器
- en: 'We have spent the chapter testing out various approaches and combinations of
    approaches using parallelism, concurrency, threading, and synchronous and asynchronous
    programming. We will now choose the most effective of these strategies and put
    them together as we return to our example problem of developing an extremely fast
    MapReduce framework. Let’s recall all the parameters of our problem: all data
    is in-memory, all the work will happen on a single computer, and our system will
    be handling requests from several clients, including automated AI bots. In this
    final section, we will finally develop a complete solution, ending up with a multiprocessing
    MapReduce implementation behind an asynchronous TCP server that will answer queries
    from multiple clients.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这一章中测试了各种方法及其组合，包括并行性、并发性、线程同步和异步编程。现在，我们将选择其中最有效的策略，并在返回到我们开发极快MapReduce框架的示例问题时将它们组合起来。让我们回顾一下我们问题的所有参数：所有数据都在内存中，所有工作将在一台计算机上完成，我们的系统将处理来自多个客户端的请求，包括自动化的AI机器人。在本节的最后，我们将最终开发一个完整的解决方案，最终得到一个异步TCP服务器后面的多进程MapReduce实现，该服务器将回答来自多个客户端的查询。
- en: 'We have already built two of the pieces: the chunked MapReduce implementation
    from the previous section and the client we made way back in section 3.1\. We
    can use these two pieces as-is. For everything else in this solution, read on.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经构建了两个部分：上一节中的分块MapReduce实现和我们在3.1节中制作的客户端。我们可以直接使用这两个部分。对于这个解决方案中的其他所有内容，请继续阅读。
- en: 3.5.1 Architecting a complete high-performance solution
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.1 架构一个完整的高性能解决方案
- en: We will design the architecture according to figure 3.6\. The frontend interfacing
    with all the clients will be asynchronous. Work will be sent to another thread
    via a queue. That thread will be responsible for managing a pool of processes
    that will do the MapReduce work.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将根据图3.6设计架构。与所有客户端交互的前端将是异步的。工作将通过队列发送到另一个线程。那个线程将负责管理一个进程池，该进程池将执行MapReduce工作。
- en: '![](../Images/CH03_F07_Antao.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F07_Antao.png)'
- en: Figure 3.6 The final architecture for the MapReduce server
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 MapReduce服务器的最终架构
- en: Our frontend TCP server will be implemented inside an asynchronous loop. There
    will be a second thread that is only responsible for managing the MapReduce multiprocessing
    pool.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的前端TCP服务器将在一个异步循环中实现。将会有一个第二线程，它只负责管理MapReduce多进程池。
- en: 'The communication between the two threads will use a `Queue` from the `queue`
    module. The entry code will set up the asynchronous server and the thread that
    manages the MapReduce pool (the code is in `03-concurrency/sec5-all/server.py`):'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 两个线程之间的通信将使用来自`queue`模块的`Queue`。入口代码将设置异步服务器和负责管理MapReduce池的线程（代码位于`03-concurrency/sec5-all/server.py`）：
- en: '[PRE36]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ① This function is called on the new thread.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这个函数在新线程中被调用。
- en: ② The pool is created inside the worker thread.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ② 池是在工作线程内部创建的。
- en: ③ The worker thread waits for some work to do.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 工作线程等待一些工作要做。
- en: ④ Results are put on the response queue.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 结果被放入响应队列。
- en: ⑤ A thread is prepared, pointing at worker as the starting point.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 准备一个线程，将其指向worker作为起点。
- en: ⑥ The thread is started.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 线程被启动。
- en: 'Our main entry point, `main`, prepares the asynchronous infrastructure as before
    and also creates and starts the thread that will manage the MapReduce pool, which
    is implemented in the function `worker`. `worker` creates the multiprocessing
    pool and deals with requests from the asynchronous server. Communication is done
    via FIFO (first-in, first-out) queues. The `queue` module makes sure that the
    queues are synchronized (i.e., that locking mechanisms are in place to assure
    threading doesn’t cause inconsistent states). There is a `queue` to receive work
    and another to return the results. All the functions in `worker` are blocking
    because nothing is there to be processed on initialization: the clients are being
    dispatched by the asynchronous part.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要入口点`main`像以前一样准备异步基础设施，并创建并启动一个将管理MapReduce池的线程，该线程在`worker`函数中实现。`worker`创建多进程池并处理来自异步服务器的请求。通信是通过FIFO（先进先出）队列完成的。`queue`模块确保队列是同步的（即，有锁定机制来确保线程不会导致不一致的状态）。有一个用于接收工作的队列，另一个用于返回结果。`worker`中的所有函数都是阻塞的，因为在初始化时没有要处理的内容：客户端正由异步部分分发。
- en: Note Queues are also a great way to communicate when you are using multiprocessing
    instead of threading. The `multiprocessing` module has a specific `Queue` class
    for this as managing interprocess communication is harder than multithreading
    versions. Some of the burden is passed to the user. Only objects that can be pickled
    can go through the queue. Because of `pickle` usage and interprocess communication,
    speed might become a concern, so be aware of that.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当使用多进程而不是线程进行通信时，队列也是一个很好的方式。`multiprocessing`模块有一个特定的`Queue`类用于此目的，因为管理进程间通信比多线程版本更困难。一些负担被传递给了用户。只有可以被pickle序列化的对象才能通过队列。由于使用了`pickle`和进程间通信，速度可能会成为一个问题，所以请注意这一点。
- en: 'Let’s start with job submission. The asynchronous part is now coded like this:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从作业提交开始。异步部分现在被编码如下：
- en: '[PRE37]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ① We write the data to work_queue, nonblocking.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们将数据写入work_queue，非阻塞方式。
- en: 'Our `submit_job` function now finally does something useful: it submits the
    job into the `work_queue`, which will be picked up by the thread running the `worker`
    function. We use `put_nowait` to avoid blocking when putting the result. In our
    case, this should not happen as the queue was initialized without constraints
    regarding size. However, we account here for the possibility of a queue with size
    limits, which you will need to consider and implement on the queue creation call
    in cases where there can be many messages on the queue.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的`submit_job`函数终于开始发挥作用了：它将作业提交到`work_queue`，这将由运行`worker`函数的线程获取。我们使用`put_nowait`来避免在放入结果时阻塞。在我们的情况下，这不应该发生，因为队列是在没有关于大小的限制的情况下初始化的。然而，我们在这里考虑了队列有大小限制的可能性，在这种情况下，你需要在创建队列的调用中考虑并实现这一点。
- en: 'The rest of the asynchronous code is as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的异步代码如下：
- en: '[PRE38]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ① We get the size of the queue to see whether something has arrived.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们获取队列的大小以查看是否有东西到达。
- en: ② We read the response from results_queue, nonblocking.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们从results_queue读取响应，非阻塞方式。
- en: ③ We provision for an empty queue.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 我们为空队列做准备。
- en: '`accept_requests` is exactly the same as in the first section and shown here
    only for completion.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '`accept_requests`与第一部分完全相同，这里仅为了完整性而展示。'
- en: '`get_results` has only a new line at the beginning: calling `get_results_queue`,
    which is responsible for checking whether the MapReduce concluded, and the results
    are transferred to the `results` dictionary. It is worth noting that the queue
    size determination via `qsize` is only approximate, so we have to consider an
    empty queue and avoid blocking until we wait for a message to arrive.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_results`只是在开头新增了一行：调用`get_results_queue`，该队列负责检查MapReduce是否完成，并将结果转移到`results`字典中。值得注意的是，通过`qsize`确定的队列大小只是一个近似值，因此我们必须考虑空队列，并在等待消息到达之前避免阻塞。'
- en: Locking and low-level synchronization with threading and multiprocessing
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 使用线程和进程进行锁定和低级同步
- en: Run away from most lower-level primitives for locking! There is a plethora of
    standard synchronization primitives that both `threading` and `multiprocessing`
    support, including locks and semaphores, along with a few others. However, our
    perspective here is that if you need to use these low-level constructs, you probably
    need to implement the code in a lower-level language anyway. So, we will deal
    with such mechanisms later in the book when we address performance by re-implementing
    parts of the codes outside of standard Python.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 避免使用大多数低级原语进行锁定！`threading`和`multiprocessing`都支持大量标准同步原语，包括锁和信号量，以及一些其他原语。然而，我们在这里的观点是，如果你需要使用这些低级构造，你可能仍然需要用更低级的语言实现代码。因此，我们将在本书的后面部分处理这些机制，那时我们将通过重新实现标准Python之外的代码部分来提高性能。
- en: While not directly related to performance, a multiprocess-related communication
    primitive commonly used is `Pipe`, as it allows communication with external applications
    using the standard input and output channels.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然与性能没有直接关系，但一个常用的多进程相关通信原语是`Pipe`，因为它允许使用标准输入和输出通道与外部应用程序进行通信。
- en: 3.5.2 Creating a robust version of the server
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.2 创建服务器的健壮版本
- en: 'Up to now, we have had little concern for errors and unexpected input. At this
    stage, we will make our code a little bit more robust. This will substantially
    increase the size of our implementation. We will make sure that when the server
    is shut down, the asynchronous server is stopped gracefully: the worker thread
    terminates, and the pool is properly closed.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们很少关注错误和意外输入。在这个阶段，我们将使我们的代码更加健壮。这将显著增加我们的实现规模。我们将确保当服务器关闭时，异步服务器能够优雅地停止：工作线程终止，并且池被正确关闭。
- en: 'Our `main` function will have to bulk up a little:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`main`函数将需要稍微增强一些：
- en: We will need to trap the user request for interruption (typically Control-C)
    and cleanup at that stage.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要捕获用户中断请求（通常是Control-C）并在该阶段进行清理。
- en: Because our asynchronous server can now be canceled, we have to catch that also.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们的异步服务器现在可以被取消，我们也必须捕获这一点。
- en: We will have to have a way to signal the working thread that it has to clean
    up.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须有一种方式来通知工作线程它需要进行清理。
- en: 'Here is the implementation (the code is in `03-concurrency/sec5-all/server_
    robust.py`):'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是实现（代码位于`03-concurrency/sec5-all/server_robust.py`）：
- en: '[PRE39]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ① We define the signal handler for interruptions.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们定义了中断的信号处理程序。
- en: ② We request the server to stop.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们请求服务器停止。
- en: ③ We wait until the server is done with requests.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 我们等待服务器完成请求。
- en: ④ We ignore the interrupt signal to make sure it is not propagated to the pool.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 我们忽略中断信号以确保它不会传播到池中。
- en: ⑤ We make sure the multiprocessing pool is initialized (i.e., ignore the pool
    ignores the input signal).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 我们确保多进程池已初始化（即忽略池忽略输入信号）。
- en: ⑥ We add signal processing to our asynchronous processor.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 我们向我们的异步处理器添加信号处理。
- en: ⑦ We catch cancellations to inform the user.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 我们捕获取消操作以通知用户。
- en: ⑧ We send -1, which is interpreted by our workers as a sign-out command.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 我们发送-1，这被我们的工作进程解释为登出命令。
- en: ⑨ We wait for all threads to finalize.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 我们等待所有线程最终完成。
- en: If you look at `main`, you will notice that now the pool is created here, just
    for efficiency, and now each process has an initializer function called `init_worker`.
    This is because when we press Control-C, we do not want the pool to interrupt
    as the signal is propagated to all of the pool. As such, we use the `signal` library
    and instruct each pool process (`signal.SIG_IGN`) to ignore the interrupt signal
    (`signal.SIGINT`).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看`main`，你会注意到现在为了效率，池在这里创建，每个进程都有一个名为`init_worker`的初始化函数。这是因为当我们按下Control-C时，我们不希望池被中断，因为信号被传播到池中的所有进程。因此，我们使用`signal`库，并指示每个池进程（`signal.SIG_IGN`）忽略中断信号（`signal.SIGINT`）。
- en: 'We want our main thread to capture the interrupt signal and process it properly.
    Because we want to be able to control the asynchronous code from the signal, we
    need to use a different way to trap it: we call `add_signal_handler` to the loop.
    We need to pass the server object, and we do that with a partial function application.
    The handler `handle_interrupt_signal` cancels the server and waits until it’s
    not serving any longer, as the cancellation might not be immediate.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望主线程能够捕获中断信号并正确处理它。因为我们希望能够从信号控制异步代码，我们需要使用不同的方式来捕获它：我们调用循环中的`add_signal_handler`。我们需要传递服务器对象，我们通过部分函数应用来实现这一点。处理程序`handle_interrupt_signal`取消服务器，并等待它不再提供服务，因为取消可能不会立即发生。
- en: 'When we run the asynchronous server, we now need to be aware of cancellations,
    so we catch that exception. Finally, we need to ask the monitor thread to clean
    up. Because the signal is passed only to the main thread, we need to do this with
    some communication mechanism: we just send `work` with a `job_id` of -1.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行异步服务器时，现在我们需要注意取消操作，因此我们捕获该异常。最后，我们需要要求监控线程进行清理。因为信号只传递给主线程，我们需要通过某种通信机制来完成这项工作：我们只需发送带有`-1`的`job_id`的`work`。
- en: Managing errors and exceptions in multithreaded and multiprocessing code
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在多线程和多进程代码中管理错误和异常
- en: Debugging multithreaded and multiprocessing code can be extremely frustrating
    even when using simple models of interprocess communication. We barely scratched
    the surface and somewhat unrealistically assumed that the architecture is well-behaved.
    If you are doing concurrent processing in your code, you should consider having
    good logging in place to help you catch problems. Whenever possible, you should
    try to make sure the problem is not related to concurrency (e.g., by trying to
    run any problematic code not in a pool or separate thread but on a single thread
    of a single process). For example, you can temporarily substitute a `multiprocessing.Pool.map`
    with a `list(map)`.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 调试多线程和多进程代码可能会非常令人沮丧，即使使用简单的进程间通信模型也是如此。我们只是触及了表面，并且有些不切实际地假设架构表现良好。如果你在代码中进行并发处理，你应该考虑实施良好的日志记录来帮助你捕获问题。在可能的情况下，你应该尽量确保问题与并发无关（例如，尝试在任何问题代码不在池或单独的线程中运行，而是在单个进程的单个线程上运行）。例如，你可以暂时用`list(map)`替换`multiprocessing.Pool.map`。
- en: As the worker thread needs to explicitly clean up, we will need to implement
    that
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 由于工作线程需要显式清理，我们需要实现这一点
- en: '[PRE40]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: ① We break out of the loop if job_id is -1\.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ① 如果`job_id`是-1，我们就会跳出循环。
- en: Summary
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Asynchronous programming can be an effective approach to efficiently process
    many simultaneous requests when communication needs and the amount of processing
    required are small; this is the most common pattern with web servers.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当通信需求和所需处理量都较小的时候，异步编程可以是一种有效地处理许多同时请求的方法；这是与Web服务器最常见的一种模式。
- en: Python is a slow language in the sense that it has a slow flagship implementation.
    This makes the ability to run parallel code even more important.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从某种意义上说，Python是一种慢速语言，因为它有一个慢速的旗舰实现。这使得运行并行代码的能力变得更加重要。
- en: Python threading is not great for performance improvement. The Global Interpreter
    Lock (GIL) requires that only one thread can run at a time. That being said, some
    other implementations of Python (e.g., IronPython) don’t have a GIL, and threaded
    code can be parallel.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python的线程对于性能提升来说并不出色。全局解释器锁（GIL）要求一次只能有一个线程运行。话虽如此，Python的一些其他实现（例如IronPython）没有GIL，并且线程代码可以是并行的。
- en: Threading can still be quite useful for architecture design purposes. While
    it is not the best avenue to approach performance, don’t discard it outright.
    There are other perspectives beyond the scope of the book where it can still be
    relevant.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程在架构设计方面仍然非常有用。虽然它不是提高性能的最佳途径，但不要完全摒弃它。在本书范围之外，还有其他一些观点，其中它仍然相关。
- en: With Python multiprocessing, it is possible to make use of all CPU cores in
    a computer even with just pure Python code.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python多进程，即使只是使用纯Python代码，也可以利用计算机上的所有CPU核心。
- en: It is generally best to keep computing granularity coarse, and too much communication
    will probably slow down your solution. When you communicate across processes,
    be sure that the overhead of communication is not a substantial source of performance
    bottlenecks.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常最好保持计算粒度粗略，过多的通信可能会减慢你的解决方案。当你跨进程通信时，确保通信的开销不是性能瓶颈的主要来源。
- en: Run away from shared memory and low-level locks when developing parallel code.
    If you think you need them, then implement a sequential solution in a lower-level
    language. Debugging parallel solutions with complex communication patterns is
    extremely difficult, as communication in parallel systems is mostly nondeterministic.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开发并行代码时，远离共享内存和低级锁。如果你认为你需要它们，那么请使用更低级的语言实现一个顺序解决方案。在具有复杂通信模式的并行解决方案中进行调试非常困难，因为并行系统中的通信大多是不可预测的。
- en: '* * *'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ¹  While the first solution that we will implement with asynchronous communication
    is naive for our use case, it is very good in other situations. For example, it
    is perfectly reasonable for most web servers as NodeJS demonstrates. As always,
    what is naive or what is best depends on your specific problem.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ¹  尽管我们将要实现的第一种异步通信解决方案对我们用例来说可能很原始，但在其他情况下它非常好。例如，对于大多数Web服务器来说，这是完全合理的，正如NodeJS所展示的那样。像往常一样，什么是原始的或什么是最优的，取决于你具体的问题。
- en: ²  Another alternative is to implement a `concurrent.futures` executor yourself,
    but in that case, you would need an understanding of the underlying modules like
    `threading` and `multiprocessing` anyway.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是自行实现一个`concurrent.futures`执行器，但在这种情况下，你仍然需要理解底层模块，如`threading`和`multiprocessing`。
