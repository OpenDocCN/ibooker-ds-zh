- en: Part 3\. Big data patterns
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3部分\. 大数据模式
- en: Now that you’ve gotten to know Hadoop and know how to best organize, move, and
    store your data in Hadoop, you’re ready to explore [part 3](#part03) of this book,
    which examines the techniques you need to know to streamline your big data computations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了Hadoop，并且知道如何在Hadoop中最佳地组织、移动和存储你的数据，你就可以探索这本书的[第3部分](#part03)，它将探讨你需要了解的技术，以简化你的大数据计算。
- en: In [chapter 6](kindle_split_018.html#ch06) we’ll examine techniques for optimizing
    MapReduce operations, such as joining and sorting on large datasets. These techniques
    make jobs run faster and allow for more efficient use of computational resources.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](kindle_split_018.html#ch06)中，我们将探讨优化MapReduce操作的技术，例如在大数据集上进行连接和排序。这些技术使作业运行得更快，并允许更有效地使用计算资源。
- en: '[Chapter 7](kindle_split_019.html#ch07) examines how graphs can be represented
    and utilized in Map-Reduce to solve algorithms such as friends-of-friends and
    PageRank. It also covers how data structures such as Bloom filters and HyperLogLog
    can be used when regular data structures can’t scale to the data sizes that you’re
    working with.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[第7章](kindle_split_019.html#ch07)探讨了如何在Map-Reduce中表示和利用图来解决如朋友的朋友和PageRank等算法。它还涵盖了当常规数据结构无法扩展到你处理的数据大小时的Bloom过滤器和高斯日志数据结构的使用。'
- en: '[Chapter 8](kindle_split_020.html#ch08) looks at how to measure, collect, and
    profile your MapReduce jobs and identify areas in your code and hardware that
    could be causing jobs to run longer than they should. It also tames MapReduce
    code by presenting different approaches to unit testing. Finally, it looks at
    how you can debug any Map-Reduce job, and offers some anti-patterns you’d best
    avoid.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[第8章](kindle_split_020.html#ch08)探讨了如何衡量、收集和配置你的MapReduce作业，并确定可能导致作业运行时间超过预期的时间的代码和硬件中的区域。它还通过展示不同的单元测试方法来驯服MapReduce代码。最后，它探讨了如何调试任何Map-Reduce作业，并提供了一些你最好避免的反模式。'
- en: Chapter 6\. Applying MapReduce patterns to big data
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6章\. 将MapReduce模式应用于大数据
- en: '*This chapter covers*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Learning how to join data with map-side and reduce-side joins
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何使用map端和reduce端连接来连接数据
- en: Understanding how a secondary sort works
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解二级排序的工作原理
- en: Discovering how partitioning works and how to globally sort data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现分区是如何工作的以及如何全局排序数据
- en: With your data safely in HDFS, it’s time to learn how to work with that data
    in MapReduce. Previous chapters showed you some MapReduce snippets in action when
    working with data serialization. In this chapter we’ll look at how to work effectively
    with big data in MapReduce to solve common problems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的数据安全地存储在HDFS中时，是时候学习如何在MapReduce中处理这些数据了。前面的章节在处理数据序列化时向你展示了MapReduce的一些代码片段。在本章中，我们将探讨如何在MapReduce中有效地处理大数据以解决常见问题。
- en: '|  |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: MapReduce basics
  id: totrans-12
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: MapReduce基础
- en: If you want to understand the mechanics of Map-Reduce and how to write basic
    MapReduce programs, it’s worth your time to read *Hadoop in Action* by Chuck Lam
    (Manning, 2010).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要了解Map-Reduce的机制以及如何编写基本的MapReduce程序，花时间去阅读Chuck Lam（Manning，2010年）的《Hadoop实战》是值得的。
- en: '|  |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: MapReduce contains many powerful features, and in this chapter we’ll focus on
    joining, sorting, and sampling. These three patterns are important because they’re
    natural operations you’ll want to perform on your big data, and the goal of your
    clusters should be to squeeze as much performance as possible out of your MapReduce
    jobs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce包含许多强大的功能，在本章中，我们将重点关注连接、排序和采样。这三个模式很重要，因为它们是你将在大数据上自然执行的操作，你的集群的目标应该是从MapReduce作业中榨取尽可能多的性能。
- en: The ability to *join* disparate and sparse data is a powerful MapReduce feature,
    but an awkward one in practice, so we’ll also look at advanced techniques for
    optimizing join operations with large datasets. Examples of joins include combining
    log files with reference data from a database and inbound link calculations on
    web graphs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 能够*连接*不同和稀疏的数据是一个强大的MapReduce功能，但在实践中却有些尴尬，因此我们还将探讨优化大型数据集连接操作的先进技术。连接的例子包括将日志文件与数据库中的参考数据合并以及网页图上的入链计算。
- en: '*Sorting* in MapReduce is also a black art, and we’ll dive into the depths
    of Map-Reduce to understand how it works by examining two techniques that everyone
    will encounter at some point: secondary sorting and total order sorting. We’ll
    wrap things up with a look at *sampling* in MapReduce, which provides the opportunity
    to quickly iterate over a large dataset by working with a small subset of that
    data.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*排序*在MapReduce中也是一种黑魔法，我们将深入Map-Reduce的深处，通过检查每个人都会遇到的两个技术来了解它是如何工作的：二级排序和全序排序。我们将通过查看MapReduce中的*抽样*来结束讨论，这通过处理数据的一个小子集提供了快速遍历大数据集的机会。'
- en: 6.1\. Joining
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 连接
- en: 'Joins are relational constructs used to combine relations together (you’re
    probably familiar with them in the context of databases). In MapReduce, joins
    are applicable in situations where you have two or more datasets you want to combine.
    An example would be when you want to combine your users (which you extracted from
    your OLTP database) with your log files (which contain user activity details).
    Various scenarios exist where it would be useful to combine these datasets together,
    such as these:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 连接是用于组合关系的关联构造（您可能熟悉在数据库上下文中的它们）。在MapReduce中，当您有两个或更多想要组合的数据集时，连接是适用的。一个例子是当您想将用户（您从OLTP数据库中提取的）与日志文件（包含用户活动细节）结合起来。存在各种场景，其中结合这些数据集会有所帮助，例如这些：
- en: You want to aggregate data based on user demographics (such as differences in
    user habits, comparing teenagers and users in their 30s).
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您希望根据用户人口统计信息（例如用户习惯的差异，比较青少年和30多岁的用户）进行数据聚合。
- en: You want to send an email to users who haven’t used the website for a prescribed
    number of days.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您希望向那些在规定天数内未使用网站的用户发送电子邮件。
- en: You want to create a feedback loop that examines a user’s browsing habits, allowing
    your system to recommend previously unexplored site features to the user.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您希望创建一个反馈循环，该循环检查用户的浏览习惯，使您的系统能够向用户推荐之前未探索过的网站功能。
- en: All of these scenarios require you to join datasets together, and the two most
    common types of joins are inner joins and outer joins. *Inner joins* compare all
    tuples in relations *L* and *R*, and produce a result if a join predicate is satisfied.
    In contrast, *outer joins* don’t require both tuples to match based on a join
    predicate, and instead can retain a record from *L* or *R* even if no match exists.
    [Figure 6.1](#ch06fig01) illustrates the different types of joins.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些场景都需要您将数据集连接在一起，最常见的两种连接类型是内连接和外连接。*内连接*比较关系*L*和*R*中的所有元组，如果满足连接谓词则产生结果。相比之下，*外连接*不需要基于连接谓词匹配两个元组，并且即使没有匹配也可以保留*L*或*R*中的记录。[图6.1](#ch06fig01)展示了不同类型的连接。
- en: Figure 6.1\. Different types of joins combining relations, shown as Venn diagrams.
    The shaded areas show data that is retained in the join.
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1\. 以维恩图形式展示的不同类型的连接关系，阴影区域显示在连接中保留的数据。
- en: '![](06fig01.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig01.jpg)'
- en: 'In this section we’ll look at three joining strategies in MapReduce that support
    the two most common types of joins (inner and outer). These three strategies perform
    the join either in the map phase or in the reduce phase by taking advantage of
    the MapReduce sort-merge architecture:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨MapReduce中的三种连接策略，这些策略支持两种最常见的连接类型（内连接和外连接）。这三种策略通过利用MapReduce的排序-合并架构，在map阶段或reduce阶段执行连接操作：
- en: '***Repartition join*** —A reduce-side join for situations where you’re joining
    two or more large datasets together'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***重新分区连接*** —适用于您要连接两个或更多大型数据集的情况的reduce端连接'
- en: '***Replication join*** —A map-side join that works in situations where one
    of the datasets is small enough to cache'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***复制连接*** —一种在其中一个数据集足够小以缓存的情况下工作的map端连接'
- en: '***Semi-join*** —Another map-side join where one dataset is initially too large
    to fit into memory, but after some filtering can be reduced down to a size that
    can fit in memory'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***半连接*** —另一种map端连接，其中一个数据集最初太大而无法放入内存，但在一些过滤之后可以减小到可以放入内存的大小'
- en: After we cover these joining strategies, we’ll look at a decision tree so you
    can determine the best join strategy for your situation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们介绍完这些连接策略之后，我们将查看一个决策树，以便您可以确定最适合您情况的最佳连接策略。
- en: Join data
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 连接数据
- en: 'The techniques will all utilize two datasets to perform the join—users and
    logs. The user data contains user names, ages, and states. The complete dataset
    follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术将利用两个数据集来执行连接操作——用户和日志。用户数据包含用户名、年龄和州。完整的数据集如下：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The logs dataset shows some user-based activity that could be extracted from
    application or webserver logs. The data includes the username, an action, and
    the source IP address. Here’s the complete dataset:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 日志数据集显示了可以从应用程序或Web服务器日志中提取的一些基于用户的活动。数据包括用户名、操作和源IP地址。以下是完整的数据集：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s get started by looking at which join method you should pick given your
    data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从查看根据您的数据应选择哪种连接方法开始。
- en: Technique 54 Picking the best join strategy for your data
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧54 为您的数据选择最佳连接策略
- en: Each of the join strategies covered in this section has different strengths
    and weaknesses, and it can be challenging to determine which one is best suited
    for the data you’re working with. This technique takes a look at different traits
    in the data and uses that information to pick the optimal approach to join your
    data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中涵盖的每种连接策略都有不同的优缺点，确定哪种最适合您正在处理的数据可能具有挑战性。这项技术将查看数据的不同特性，并使用这些信息来选择连接数据的最佳方法。
- en: Problem
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to select the optimal method to join your data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 您想要选择最佳方法来连接您的数据。
- en: Solution
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use a data-driven decision tree to pick the best join strategy.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据驱动的决策树来选择最佳连接策略。
- en: Discussion
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: '[Figure 6.2](#ch06fig02) shows a decision tree you can use.^([[1](#ch06fn01)])'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.2](#ch06fig02)显示了您可以使用的决策树。[^([1](#ch06fn01))]'
- en: ¹ This decision tree is modeled after the one presented by Spyros Blanas et
    al., in “A Comparison of Join Algorithms for Log Processing in MapReduce,” [http://pages.cs.wisc.edu/~jignesh/publ/hadoopjoin.pdf](http://pages.cs.wisc.edu/~jignesh/publ/hadoopjoin.pdf).
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹ 此决策树是根据Spyros Blanas等人提出的决策树模型，在“MapReduce中日志处理连接算法的比较”中提出的，[http://pages.cs.wisc.edu/~jignesh/publ/hadoopjoin.pdf](http://pages.cs.wisc.edu/~jignesh/publ/hadoopjoin.pdf)。
- en: Figure 6.2\. Decision tree for selecting a join strategy
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.2\. 选择连接策略的决策树
- en: '![](06fig02_alt.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig02_alt.jpg)'
- en: 'The decision tree can be summarized in the following three points:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可以总结为以下三个要点：
- en: If one of your datasets is small enough to fit into a mapper’s memory, the map-only
    replicated join is efficient.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您的数据集之一足够小，可以放入映射器的内存中，则仅映射的复制连接效率很高。
- en: If both datasets are large and one dataset can be substantially reduced by prefiltering
    elements that don’t match the other, the semi-join works well.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两个数据集都很大，并且可以通过预过滤不匹配另一个数据集的元素来显著减少其中一个数据集，那么半连接（semi-join）效果很好。
- en: If you can’t preprocess your data and your data sizes are too large to cache—which
    means you have to perform the join in the reducer—repartition joins need to be
    used.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您无法预处理数据，并且数据大小太大而无法缓存——这意味着您必须在reducer中执行连接——则需要使用重新分区连接。
- en: Regardless of which strategy you pick, one of the most fundamental activities
    you should be performing in your joins is using filters and projections.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您选择哪种策略，您在连接操作中最基本的活动之一应该是使用过滤和投影。
- en: Technique 55 Filters, projections, and pushdowns
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧55 过滤、投影和下推
- en: In this technique, we’ll examine how you can effectively use filters and projections
    in your mappers to cut down on the amount of data that you’re working with, and
    spilling, in MapReduce. This technique also examines a more advanced optimization
    called pushdowns, which can further improve your data pipeline.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项技术中，我们将探讨您如何有效地在映射器中使用过滤和投影来减少您正在处理的数据量，以及在MapReduce中的溢出。这项技术还探讨了更高级的优化，称为下推（pushdowns），这可以进一步提高您的数据管道效率。
- en: Problem
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You’re working with large data volumes and you want to efficiently manage your
    input data to optimize your jobs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您正在处理大量数据，并且想要高效地管理您的输入数据以优化您的作业。
- en: Solution
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Filter and project your data to only include the data points you’ll be using
    in your work.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤和投影您的数据，仅包括您将在工作中使用的数据点。
- en: Discussion
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Filtering and projecting data is the biggest optimization you can make when
    joining data, and when working with data in general. This is a technique that
    applies to any OLAP activity, and it’s equally effective in Hadoop.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在连接数据以及一般处理数据时，过滤和投影数据是您可以做出的最大优化。这是一种适用于任何OLAP活动的技术，在Hadoop中同样有效。
- en: Why are filtering and projection so important? They cut down on the amount of
    data that a processing pipeline needs to handle. Having less data to work with
    is important, especially when you’re pushing that data across network and disk
    boundaries. The shuffle step in MapReduce is expensive because data is being written
    to local disk and across the network, so having fewer bytes to push around means
    that your jobs and the MapReduce framework have less work to do, and this translates
    to faster jobs and less pressure on the CPU, disk, and your networking gear.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么过滤和投影如此重要？它们减少了处理管道需要处理的数据量。处理更少的数据很重要，尤其是在你推动数据跨越网络和磁盘边界时。MapReduce 中的洗牌步骤很昂贵，因为数据正在写入本地磁盘和通过网络传输，所以需要推送的字节数越少，你的作业和
    MapReduce 框架的工作量就越少，这转化为更快的作业和更少的 CPU、磁盘和网络设备的压力。
- en: '[Figure 6.3](#ch06fig03) shows a simple example of how filtering and projection
    works.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.3](#ch06fig03) 展示了过滤和投影如何工作的一个简单示例。'
- en: Figure 6.3\. Using filters and projections to reduce data sizes
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.3\. 使用过滤和投影来减少数据大小
- en: '![](06fig03_alt.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig03_alt.jpg)'
- en: 'Filters and projections should be performed as close to the data source as
    possible; in MapReduce this work is best performed in the mappers. The following
    code shows an example of a filter that excludes users under 30 and only projects
    their names and states:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤和投影应尽可能接近数据源执行；在 MapReduce 中，这项工作最好在映射器中完成。以下代码展示了排除30岁以下用户并仅投影其姓名和状态的过滤器的示例：
- en: '![](260fig01_alt.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](260fig01_alt.jpg)'
- en: The challenge with using filters in joins is that it’s possible that not all
    of the datasets you’re joining will contain the fields you want to filter on.
    If this is the case, take a look at technique 61, which discusses using a Bloom
    filter to help solve this challenge.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在连接中使用过滤器时面临的挑战是，你连接的数据集中可能并不包含你想要过滤的字段。如果情况如此，请查看技术61，它讨论了使用布隆过滤器来帮助解决这个挑战。
- en: Pushdowns
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 推下式
- en: Projection and predicate pushdowns take filtering further by pushing the projections
    and predicates down to the storage format. This is even more efficient, especially
    when working with storage formats that can skip over records or entire blocks
    based on the pushdowns.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 投影和谓词推下式通过将投影和谓词推到存储格式来进一步扩展过滤功能。这甚至更加高效，尤其是在与可以基于推下式跳过记录或整个块的存储格式一起工作时。
- en: '[Table 6.1](#ch06table01) lists the various storage formats and whether they
    support pushdowns.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[表6.1](#ch06table01) 列出了各种存储格式以及它们是否支持推下式。'
- en: Table 6.1\. Storage formats and their pushdown support
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表6.1\. 存储格式及其推下式支持
- en: '| Format | Projection pushdown supported? | Predicate pushdown supported? |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 格式 | 支持投影推下式？ | 支持谓词推下式？ |'
- en: '| --- | --- | --- |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Text (CSV, JSON, etc.) | No | No |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 文本（CSV、JSON 等） | 否 | 否 |'
- en: '| Protocol Buffers | No | No |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 协议缓冲区 | 否 | 否 |'
- en: '| Thrift | No | No |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 节俭 | 否 | 否 |'
- en: '| Avro^([[a](#ch06fn02)]) | No | No |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Avro^([[a](#ch06fn02)]) | 否 | 否 |'
- en: '| Parquet | Yes | Yes |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Parquet | 是 | 是 |'
- en: ^a Avro has both row-major and column-major storage formats.
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^a Avro 具有行主序和列主序的存储格式。
- en: '|  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Further reading on pushdowns
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 推下式存储的进一步阅读
- en: '[Chapter 3](kindle_split_013.html#ch03) contains additional details on how
    Parquet pushdowns can be used in your jobs.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[第3章](kindle_split_013.html#ch03) 包含了有关如何在作业中使用 Parquet 推下式的更多详细信息。'
- en: '|  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: It’s pretty clear that a big advantage of Parquet is its ability to support
    both types of pushdowns. If you’re working with huge datasets and regularly work
    on only a subset of the records and fields, then you should consider Parquet as
    your storage format.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，Parquet 的一大优势是它能够支持这两种类型的推下式。如果你正在处理大量数据集，并且经常只处理记录和字段的一个子集，那么你应该考虑将 Parquet
    作为你的存储格式。
- en: It’s time to move on to the actual joining techniques.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候继续实际连接技术了。
- en: 6.1.1\. Map-side joins
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1\. 映射端连接
- en: Our coverage of joining techniques will start with a look at performing joins
    in the mapper. The reason we’ll cover these techniques first is that they’re the
    optimal join strategies if your data can support map-side joins. Reduce-size joins
    are expensive by comparison due to the overhead of shuffling data between the
    mappers and reducers. As a general policy, map-side joins are preferred.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对连接技术的覆盖将从查看在映射器中执行连接开始。我们将首先介绍这些技术的原因是，如果你的数据可以支持映射端连接，那么它们是最佳连接策略。与在映射器和还原器之间洗牌数据造成的开销相比，减少大小连接更昂贵。作为一般原则，映射端连接更受欢迎。
- en: In this section we’ll look at three different flavors of map-side joins. Technique
    56 works well in situations where one of the datasets is already small enough
    to cache in memory. Technique 57 is more involved, and it also requires that one
    dataset can fit in memory after filtering out records where the join key exists
    in both datasets. Technique 58 works in situations where your data is sorted and
    distributed across your files in a certain way.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨三种不同的 map-side 连接类型。技术 56 在其中一个数据集已经足够小，可以缓存到内存中的情况下效果良好。技术 57 更为复杂，并且还要求在过滤掉两个数据集中都存在的连接键的记录后，一个数据集可以适合内存。技术
    58 在您的数据按某种方式排序并分布到文件中的情况下工作。
- en: Technique 56 Joining data where one dataset can fit into memory
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术编号 56 在一个数据集可以适合到 mapper 的内存中执行连接
- en: A replicated join is a map-side join, and it gets its name from its function—the
    smallest of the datasets is replicated to all the map hosts. The replicated join
    depends on the fact that one of the datasets being joined is small enough to be
    cached in memory.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 复制连接是一种 map-side 连接，其名称来源于其功能——数据集中最小的一个被复制到所有的 map 主机。复制连接依赖于这样一个事实，即要连接的数据集中有一个足够小，可以缓存到内存中。
- en: Problem
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to perform a join on data where one dataset can fit into your mapper’s
    memory.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 您想在可以适合您 mapper 的内存中的数据上执行连接操作。
- en: Solution
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the distributed cache to cache the smaller dataset and perform the join
    as the larger dataset is streamed to the mappers.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分布式缓存来缓存较小的数据集，并在较大的数据集被流式传输到 mapper 时执行连接。
- en: Discussion
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: You’ll use the distributed cache to copy the small dataset to the nodes running
    the map tasks^([[2](#ch06fn03)]) and use the initialization method of each map
    task to load it into a hashtable. Use the key from each record fed to the map
    function from the large dataset to look up the small dataset hashtable, and perform
    a join between the large dataset record and all of the records from the small
    dataset that match the join value. [Figure 6.4](#ch06fig04) shows how the replicated
    join works in MapReduce.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用分布式缓存将小数据集复制到运行 map 任务的节点，并使用每个 map 任务的初始化方法将其加载到散列表中。使用来自大数据集的每个记录的键来查找小数据集的散列表，并在大数据集的记录与小数据集中匹配连接值的所有记录之间执行连接。[图
    6.4](#ch06fig04) 展示了在 MapReduce 中复制的连接是如何工作的。
- en: ² Hadoop’s distributed cache copies files located on the MapReduce client host
    or files in HDFS to the slave nodes before any map or reduce tasks are executed
    on the nodes. Tasks can read these files from their local disk to use as part
    of their work.
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ² Hadoop 的分布式缓存会在任何 map 或 reduce 任务在节点上执行之前，将位于 MapReduce 客户端主机上的文件或 HDFS 中的文件复制到从节点。任务可以从它们的本地磁盘读取这些文件，作为它们工作的一部分。
- en: Figure 6.4\. Map-only replicated join
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.4\. 仅 map 的复制连接
- en: '![](06fig04.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig04.jpg)'
- en: The following code performs this join:^([[3](#ch06fn04)])
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码执行此连接：^([[3](#ch06fn04)])
- en: '³ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/replicated/simple/ReplicatedJoin.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/replicated/simple/ReplicatedJoin.java).'
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³ GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/replicated/simple/ReplicatedJoin.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/replicated/simple/ReplicatedJoin.java)。
- en: '![](262fig01_alt.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](262fig01_alt.jpg)'
- en: 'To perform this join, you first need to copy the two files you’re going to
    join to your home directory in HDFS:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此连接，您首先需要将您要连接的两个文件复制到 HDFS 中的家目录：
- en: '[PRE2]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, run the job and examine its output once it has completed:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，运行作业，并在完成后检查其输出：
- en: '[PRE3]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Hive
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Hive
- en: 'Hive joins can be converted to map-side joins by configuring the job prior
    to execution. It’s important that the largest table be the last table in the query,
    as that’s the table that Hive will stream in the mapper (the other tables will
    be cached):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 的连接操作可以通过在执行前配置作业来转换为 map-side 连接。重要的是最大的表应该是查询中的最后一个表，因为 Hive 会将这个表流式传输到
    mapper 中（其他表将被缓存）：
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '|  |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: De-emphasizing map-join hint
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 减弱 map-join 提示
- en: Hive 0.11 implemented some changes that ostensibly removed the need to supply
    map-join hints as part of the `SELECT` statement, but it’s unclear in which situations
    the hint is no longer needed (see [https://issues.apache.org/jira/browse/HIVE-3784](https://issues.apache.org/jira/browse/HIVE-3784)).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 0.11 实现了一些变化，表面上消除了在 `SELECT` 语句中提供 map-join 提示的需要，但尚不清楚在哪些情况下提示不再需要（参见
    [https://issues.apache.org/jira/browse/HIVE-3784](https://issues.apache.org/jira/browse/HIVE-3784)）。
- en: '|  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Map-side joins are not supported for full or right outer joins; they’ll execute
    as repartition joins (reduce-side joins).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在全连接或右外连接中，不支持在映射端进行连接；它们将作为重新分区连接（减少端连接）执行。
- en: Summary
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: Both inner and outer joins can be supported with replicated joins. This technique
    implemented an inner join, because only records that had the same key in both
    datasets were emitted. To convert this into an outer join, you could emit values
    being streamed to the mapper that don’t have a corresponding entry in the hashtable,
    and you could similarly keep track of hashtable entries that were matched with
    streamed map records and use the cleanup method at the end of the map task to
    emit records from the hashtable that didn’t match any of the map inputs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过复制连接支持内部和外部连接。此技术实现了一个内部连接，因为只有两个数据集中具有相同键的记录才会被输出。要将此转换为外部连接，您可以输出被发送到映射器的值，这些值在散列表中没有相应的条目，并且您可以类似地跟踪与流式映射记录匹配的散列表条目，并在映射任务结束时使用清理方法输出散列表中未与任何映射输入匹配的记录。
- en: Is there a way to further optimize map-side joins in cases where the dataset
    is small enough to cache in memory? It’s time to look at semi-joins.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集足够小以至于可以缓存在内存的情况下，是否有进一步优化映射端连接的方法？是时候看看半连接了。
- en: Technique 57 Performing a semi-join on large datasets
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 57 在大型数据集上执行半连接
- en: 'Imagine a situation where you’re working with two large datasets that you want
    to join, such as user logs and user data from an OLTP database. Neither of these
    datasets is small enough to cache in a map task’s memory, so it would seem you’ll
    have to resign yourself to performing a reduce-side join. But not necessarily—ask
    yourself this question: would one of the datasets fit into memory if you were
    to remove all records that didn’t match a record from the other dataset?'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下这种情况：您正在处理两个大型数据集，您想将它们连接起来，例如用户日志和一个OLTP数据库中的用户数据。这两个数据集都不足以在映射任务的内存中缓存，所以您可能不得不接受执行减少端连接。但并不一定——问问自己这个问题：如果您删除了所有不匹配另一个数据集的记录，其中一个数据集是否可以放入内存中？
- en: In our example there’s a good chance that the users that appear in your logs
    are a small percentage of the overall set of users in your OLTP database, so by
    removing all the OLTP users that don’t appear in your logs, you could get the
    dataset down to a size that fits into memory. If this is the case, a semi-join
    is the solution. [Figure 6.5](#ch06fig05) shows the three MapReduce jobs you need
    to execute to perform a semi-join.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，出现日志中的用户可能是您OLTP数据库中所有用户集的一小部分，因此通过删除所有未出现在日志中的OLTP用户，您可以将数据集的大小减少到适合内存的大小。如果是这种情况，半连接就是解决方案。[图
    6.5](#ch06fig05) 显示了您需要执行以执行半连接的三个MapReduce作业。
- en: Figure 6.5\. The three MapReduce jobs that comprise a semi-join
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.5\. 组成半连接的三个MapReduce作业
- en: '![](06fig05_alt.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig05_alt.jpg)'
- en: Let’s look at what’s involved in writing a semi-join.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看编写半连接涉及哪些内容。
- en: Problem
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to join large datasets together and at the same time avoid the overhead
    of the shuffle and sort phases.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 您想将大型数据集连接起来，同时避免洗牌和排序阶段的开销。
- en: Solution
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: In this technique you’ll use three MapReduce jobs to join two datasets together
    to avoid the overhead of a reducer-side join. This technique is useful in situations
    where you’re working with large datasets, but where a job can be reduced down
    to a size that can fit into the memory of a task by filtering out records that
    don’t match the other dataset.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技巧中，您将使用三个MapReduce作业将两个数据集连接起来，以避免在减少端连接中的开销。此技巧在处理大型数据集但可以通过过滤掉不匹配其他数据集的记录将作业减少到可以适应任务内存大小的情况中非常有用。
- en: Discussion
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: In this technique you’ll break down the three jobs illustrated in [figure 6.5](#ch06fig05).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技巧中，您将分解图 6.5 中所示的三个作业。
- en: Job 1
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 任务 1
- en: The function of the first MapReduce job is to produce a set of unique user names
    that exist in the log files. You do this by having the map function perform a
    projection of the user name, and in turn use the reducers to emit the user name.
    To cut down on the amount of data transferred between the map and reduce phases,
    you’ll have the map task cache all of the user names in a `HashSet` and emit the
    values of the `HashSet` in the cleanup method. [Figure 6.6](#ch06fig06) shows
    the flow of this job.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个MapReduce作业的功能是生成存在于日志文件中的唯一用户名集合。你通过让map函数执行用户名的投影，然后通过reducers发射用户名来实现这一点。为了减少map和reduce阶段之间传输的数据量，你将在map任务中将所有用户名缓存到`HashSet`中，并在清理方法中发射`HashSet`的值。[图6.6](#ch06fig06)显示了该作业的流程。
- en: Figure 6.6\. The first job in the semi-join produces a unique set of user names
    that exist in the log files.
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.6\. 半连接的第一个作业生成了存在于日志文件中的唯一用户名集合。
- en: '![](06fig06_alt.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig06_alt.jpg)'
- en: The following code shows the MapReduce job:^([[4](#ch06fn05)])
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了MapReduce作业:^([[4](#ch06fn05)])
- en: '⁴ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/semijoin/UniqueHashedKeyJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/semijoin/UniqueHashedKeyJob.java).'
  id: totrans-135
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴ GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/semijoin/UniqueHashedKeyJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/semijoin/UniqueHashedKeyJob.java).
- en: '![](266fig01_alt.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](266fig01_alt.jpg)'
- en: The result of the first job is a unique set of users that appear in the log
    files.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个作业的结果是出现在日志文件中的唯一用户集合。
- en: Job 2
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 作业2
- en: The second step is an elaborate filtering MapReduce job, where the goal is to
    remove users from the user dataset that don’t exist in the log data. This is a
    map-only job that uses a replicated join to cache the user names that appear in
    the log files and join them with the user dataset. The unique user output from
    job 1 will be substantially smaller than the entire user dataset, which makes
    it the natural selection for caching. [Figure 6.7](#ch06fig07) shows the flow
    of this job.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是一个复杂的过滤MapReduce作业，其目标是移除用户数据集中不存在于日志数据中的用户。这是一个仅使用map的作业，它使用复制连接来缓存出现在日志文件中的用户名，并将它们与用户数据集连接起来。作业1生成的唯一用户输出将比整个用户数据集小得多，这使得它成为缓存的理想选择。[图6.7](#ch06fig07)显示了该作业的流程。
- en: Figure 6.7\. The second job in the semi-join removes users from the user dataset
    missing from the log data.
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.7\. 半连接中的第二个作业从用户数据集中移除了日志数据中缺失的用户。
- en: '![](06fig07_alt.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig07_alt.jpg)'
- en: This is a replicated join, just like the one you saw in the previous technique.
    For that reason I won’t include the code here, but you can easily access it on
    GitHub.^([[5](#ch06fn06)])
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个复制连接，就像你在之前的技巧中看到的那样。因此，这里我不会包括代码，但你可以在GitHub上轻松访问它.^([[5](#ch06fn06)])
- en: '⁵ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/semijoin/ReplicatedFilterJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/semijoin/ReplicatedFilterJob.java).'
  id: totrans-143
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵ GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/semijoin/ReplicatedFilterJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/semijoin/ReplicatedFilterJob.java).
- en: Job 3
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 作业3
- en: In this final step you’ll combine the filtered users produced from job 2 with
    the original user logs. The filtered users should now be few enough to stick into
    memory, allowing you to put them in the distributed cache. [Figure 6.8](#ch06fig08)
    shows the flow of this job.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个最终步骤中，你将结合作业2生成的过滤用户与原始用户日志。现在过滤后的用户应该足够少，可以放入内存中，允许你将它们放入分布式缓存中。[图6.8](#ch06fig08)显示了该作业的流程。
- en: Figure 6.8\. The third job in the semi-join combines the users produced from
    job 2 with the original user logs.
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.8\. 半连接中的第三个作业将作业2生成的用户与原始用户日志结合起来。
- en: '![](06fig08_alt.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig08_alt.jpg)'
- en: Again you’re using the replicated join to perform this join, so I won’t show
    the code for that here—please refer to the previous technique for more details
    on replicated joins, or go straight to GitHub for the source of this job.^([[6](#ch06fn07)])
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，你使用复制连接来执行这个连接，所以这里我不会展示相应的代码——请参考之前的技巧以获取更多关于复制连接的详细信息，或者直接访问GitHub获取这个作业的源代码.^([[6](#ch06fn07)])
- en: '⁶ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/semijoin/FinalJoinJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/semijoin/FinalJoinJob.java).'
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶ GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/semijoin/FinalJoinJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/semijoin/FinalJoinJob.java).
- en: 'Run the code and look at the output produced by each of the previous steps:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码并查看前一个步骤产生的输出：
- en: '![](268fig01_alt.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](268fig01_alt.jpg)'
- en: The output shows the logical progression of the jobs in the semi-join and the
    final join output.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了半连接作业的逻辑进展和最终的连接输出。
- en: Summary
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: In this technique we looked at how to use a semi-join to combine two datasets
    together. The semi-join construct involves more steps than the other joins, but
    it’s a powerful way to use a map-side join even when working with large datasets
    (with the caveat that one of the datasets must be reduced to a size that fits
    in memory).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技巧中，我们探讨了如何使用半连接将两个数据集合并在一起。半连接结构比其他连接涉及更多步骤，但即使处理大型数据集（前提是其中一个数据集必须减小到适合内存的大小），它也是一种使用
    map-side join 的强大方式。
- en: With these three join strategies in hand, you may be wondering which one you
    should use in what circumstances.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这三种连接策略后，您可能会想知道在什么情况下应该使用哪一种。
- en: Technique 58 Joining on presorted and prepartitioned data
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 58 在预排序和预分区数据上连接
- en: 'Map-side joins are the most efficient techniques, and the previous two map-side
    strategies both required that one of the datasets could be loaded into memory.
    What if you’re working with large datasets that can’t be reduced down to a smaller
    size as required by the previous technique? In this case, a composite map-side
    join may be viable, but only if all of the following requirements are met:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Map-side joins 是最有效的方法，前两种 map-side 策略都要求其中一个数据集可以加载到内存中。如果您正在处理无法按前一种技术减小到更小大小的的大型数据集，该怎么办？在这种情况下，复合
    map-side join 可能是可行的，但前提是满足以下所有要求：
- en: None of the datasets can be loaded in memory in its entirety.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有任何一个数据集可以完整地加载到内存中。
- en: The datasets are all sorted by the join key.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有数据集都是按连接键排序的。
- en: Each dataset has the same number of files.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个数据集都有相同数量的文件。
- en: File *N* in each dataset contains the same join key *K*.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个数据集的文件 *N* 包含相同的连接键 *K*。
- en: Each file is less than the size of an HDFS block, so that partitions aren’t
    split. Or alternatively, the input split for the data doesn’t split files.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个文件的大小都小于 HDFS 块的大小，因此分区不会被分割。或者，也可以说，数据的输入拆分不会分割文件。
- en: '[Figure 6.9](#ch06fig09) shows an example of sorted and partitioned files that
    lend themselves to composite joins. This technique will look at how you can use
    the composite join in your jobs.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6.9](#ch06fig09) 展示了一个示例，说明如何使用排序和分区文件进行复合连接。这项技术将探讨如何在您的作业中使用复合连接。'
- en: Figure 6.9\. An example of sorted files used as input for the composite join
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.9\. 作为复合连接输入的排序文件示例
- en: '![](06fig09_alt.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig09_alt.jpg)'
- en: Problem
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to perform a map-side join on sorted, partitioned data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 您想在排序、分区数据上执行 map-side join。
- en: Solution
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the `CompositeInputFormat` bundled with MapReduce.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与 MapReduce 一起捆绑的 `CompositeInputFormat`。
- en: Discussion
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: The `CompositeInputFormat` is quite powerful and supports both inner and outer
    joins. The following example shows how an inner join would be performed on your
    data:^([[7](#ch06fn08)])
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`CompositeInputFormat` 非常强大，支持内连接和外连接。以下示例展示了如何在您的数据上执行内连接：^([[7](#ch06fn08)])'
- en: '⁷ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/composite/CompositeJoin.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/composite/CompositeJoin.java).'
  id: totrans-172
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷ GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/composite/CompositeJoin.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/composite/CompositeJoin.java)。
- en: '![](270fig01_alt.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](270fig01_alt.jpg)'
- en: 'The composite join requires the input files to be sorted by key (which is the
    user name in our example), so before you run the example you’ll need to sort the
    two files and upload them to HDFS:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 复合连接要求输入文件按键排序（在我们的示例中是用户名），因此在运行示例之前，您需要排序这两个文件并将它们上传到 HDFS：
- en: '[PRE5]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, run the job and examine its output once it has completed:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，运行作业并在完成后检查其输出：
- en: '[PRE6]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Hive
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Hive
- en: 'Hive supports a map-side join called a *sort-merge join*, which operates in
    much the same way as this technique. It also requires all the keys to be sorted
    in both tables, and the tables must be bucketized into the same number of buckets.
    You need to specify a number of configurables and also use the `MAPJOIN` hint
    to enable this behavior:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 支持一种称为 *sort-merge join* 的 map-side join，其操作方式与该技术非常相似。它还要求两个表中的所有键都必须排序，并且表必须划分成相同数量的桶。您需要指定一些可配置的参数，并使用
    `MAPJOIN` 指示符来启用此行为：
- en: '[PRE7]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: The composite join actually supports *N*-way joins, so more than two datasets
    can be joined. But all datasets must conform to the same set of restrictions that
    were discussed at the start of this technique.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 组合合并实际上支持*N*路合并，因此可以合并超过两个数据集。但是，所有数据集都必须符合在技术开始时讨论的限制。
- en: Because each mapper works with two or more data inputs, data locality can only
    exist with one of the datasets, so the remaining ones must be streamed from other
    data nodes.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个mapper处理两个或多个数据输入，数据局部性只能存在于一个数据集中，因此其余的必须从其他数据节点流式传输。
- en: This join is certainly restrictive in terms of how your data must exist prior
    to running the join, but if your data is already laid out that way, then this
    is a good way to join data and avoid the overhead of the shuffle in reducer-based
    joins.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这种连接在数据必须在运行连接之前存在的方式上确实有限制，但如果数据已经以这种方式布局，那么这是一种合并数据并避免基于reducer的连接中洗牌开销的好方法。
- en: 6.1.2\. Reduce-side joins
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2\. Reduce端合并
- en: If none of the map-side techniques work for your data, you’ll need to use the
    shuffle in MapReduce to sort and join your data together. The following techniques
    present a number of tips and tricks for your reduce-side joins.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果地图端的技术对您的数据不起作用，您将需要使用MapReduce中的洗牌功能来排序和合并您的数据。以下技术提供了一些关于reduce端合并的技巧和窍门。
- en: Technique 59 A basic repartition join
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧59 基本分区合并
- en: The first technique is a basic reduce-side join, which allows you to perform
    inner and outer joins.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种技术是基本的reduce端合并，允许您执行内连接和外连接。
- en: Problem
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to join together large datasets.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 您想合并大型数据集。
- en: Solution
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use a reduce-side repartition join.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 使用reduce端分区合并。
- en: Discussion
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: A repartition join is a reduce-side join that takes advantage of MapReduce’s
    sort-merge to group together records. It’s implemented as a single MapReduce job,
    and it can support an *N*-way join, where *N* is the number of datasets being
    joined.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 分区合并是一种reduce端合并，它利用MapReduce的排序合并功能来分组记录。它作为一个单独的MapReduce作业实现，并且可以支持*N*路合并，其中*N*是要合并的数据集数量。
- en: The map phase is responsible for reading the data from the various datasets,
    determining the join value for each record, and emitting that join value as the
    output key. The output value contains data that you’ll want to include when you
    combine datasets together in the reducer to produce the job output.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: map阶段负责从各种数据集中读取数据，确定每条记录的join值，并将该join值作为输出键发出。输出值包含您在reducer中合并数据集以生成作业输出时希望包含的数据。
- en: A single reducer invocation receives all of the values for a join key emitted
    by the map function, and it partitions the data into *N* partitions, where *N*
    is the number of datasets being joined. After the reducer has read all of the
    input records for the join value and partitioned them in memory, it performs a
    Cartesian product across all partitions and emits the results of each join. [Figure
    6.10](#ch06fig10) shows the repartition join at a high level.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 单个reducer调用接收由map函数发出的所有join键值，并将数据分成*N*个分区，其中*N*是要合并的数据集数量。在reducer读取所有输入记录并按内存中的分区进行分区后，它对所有分区执行笛卡尔积并发出每个合并的结果。[图6.10](#ch06fig10)展示了分区合并的高级视图。
- en: Figure 6.10\. A basic MapReduce implementation of a repartition join
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.10\. 基本的MapReduce实现分区合并
- en: '![](06fig10_alt.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig10_alt.jpg)'
- en: 'There are a number of things that your MapReduce code will need to be able
    to support for this technique:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 您的MapReduce代码需要支持以下技术：
- en: It needs to support multiple map classes, each handling a different input dataset.
    This is accomplished by using the `MultipleInputs` class.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它需要支持多个map类，每个类处理不同的输入数据集。这是通过使用`MultipleInputs`类来实现的。
- en: It needs a way to mark records being emitted by the mappers so that they can
    be correlated with the dataset of their origin. Here you’ll use the htuple project
    to easily work with composite data in MapReduce.^([[8](#ch06fn09)])
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它需要一种标记由mapper发出的记录的方法，以便可以将它们与它们的数据集关联起来。在这里，您将使用htuple项目来轻松地在MapReduce中处理复合数据.^([[8](#ch06fn09)])
- en: ⁸ htuple ([http://htuple.org](http://htuple.org)) is an open source project
    that was designed to make it easier to work with tuples in MapReduce. It was created
    to simplify secondary sorting, which is onerous in MapReduce.
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸ htuple ([http://htuple.org](http://htuple.org))是一个开源项目，旨在使在MapReduce中处理元组更容易。它是为了简化MapReduce中的二级排序而创建的。
- en: The code for the repartition join follows:^([[9](#ch06fn10)])
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 分区合并的代码如下:^([[9](#ch06fn10)])
- en: '⁹ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/repartition/SimpleRepartitionJoin.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/repartition/SimpleRepartitionJoin.java).'
  id: totrans-204
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹ GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/repartition/SimpleRepartitionJoin.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/repartition/SimpleRepartitionJoin.java).
- en: '![](273fig01_alt.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](273fig01_alt.jpg)'
- en: '![](273fig02_alt.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](273fig02_alt.jpg)'
- en: 'You can use the following commands to run the job and view the job outputs:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下命令来运行作业并查看作业输出：
- en: '[PRE8]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Summary
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: Hadoop comes bundled with a `hadoop-datajoin` module, which is a framework for
    repartition joins. It includes the main plumbing for handling multiple input datasets
    and performing the join.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop附带了一个`hadoop-datajoin`模块，这是一个用于分区连接的框架。它包括处理多个输入数据集和执行连接的主要管道。
- en: The example shown in this technique as well as the `hadoop-datajoin` code are
    the most basic form of repartition joins. Both require that all the data for a
    join key be loaded into memory before the Cartesian product can be performed.
    This may work well for your data, but if you have join keys with cardinalities
    that are larger than your available memory, then you’re out of luck. The next
    technique looks at a way you can possibly work around this problem.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 本技术中展示的示例以及`hadoop-datajoin`代码都是分区连接的最基本形式。两者都需要在执行笛卡尔积之前将连接键的所有数据加载到内存中。这可能适用于你的数据，但如果你的连接键的基数大于你的可用内存，那么你可能就无计可施了。接下来的技术将探讨一种可能绕过这个问题的方法。
- en: Technique 60 Optimizing the repartition join
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术篇60：优化分区连接
- en: The previous implementation of the repartition join is not space-efficient;
    it requires all of the output values for a given join value to be loaded into
    memory before it can perform the multiway join. It’s more efficient to load the
    smaller of the datasets into memory and then iterate over the larger datasets,
    performing the join along the way.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的分区连接实现不是空间高效的；它需要在执行多路连接之前将给定连接值的所有输出值加载到内存中。将较小数据集加载到内存中，然后遍历较大数据集，沿途执行连接会更有效。
- en: Problem
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to perform a repartition join in MapReduce, but you want to do so without
    the overhead of caching all the records in the reducer.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在MapReduce中执行分区连接，但又不想承担在reducer中缓存所有记录的开销。
- en: Solution
  id: totrans-216
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: This technique uses an optimized repartition join framework that caches just
    one of the datasets being joined to reduce the amount of data cached in the reducers.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术使用一个优化的分区连接框架，它只缓存要连接的数据集中的一项，以减少在reducer中缓存的数据量。
- en: Discussion
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: This optimized join only caches records from the smaller of the two datasets
    to cut down on the memory overhead of caching all the records. [Figure 6.11](#ch06fig11)
    shows the improved repartition join in action.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这种优化的连接只缓存两个数据集中较小的一个的记录，以减少缓存所有记录的内存开销。[图6.11](#ch06fig11)显示了改进的分区连接的实际效果。
- en: Figure 6.11\. An optimized MapReduce implementation of a repartition join
  id: totrans-220
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.11\. 优化后的MapReduce分区连接实现
- en: '![](06fig11_alt.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig11_alt.jpg)'
- en: There are a few differences between this technique and the simpler repartition
    join shown in the previous technique. In this technique you’re using a secondary
    sort to ensure that all the records from the small dataset arrive at the reducer
    before all the records from the larger dataset. To accomplish this, you’ll emit
    tuple output keys from the mapper containing the user name being joined on and
    a field identifying the originating dataset.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一种技术中展示的简单分区连接相比，这里有一些不同。在这个技术中，你使用二次排序来确保所有来自小数据集的记录在所有来自大数据集的记录之前到达reducer。为了实现这一点，你将从mapper中发出包含要连接的用户名和一个标识原始数据集的字段的元组输出键。
- en: The following code shows a new enum containing the fields that the tuple will
    contain for the map output keys. It also shows how the user mapper populates the
    tuple fields:^([[10](#ch06fn11)])
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了一个包含元组将包含的字段的枚举。它还显示了用户mapper如何填充元组字段:^([[10](#ch06fn11)])
- en: '^(10) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/repartition/StreamingRepartitionJoin.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/repartition/StreamingRepartitionJoin.java).'
  id: totrans-224
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(10) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/repartition/StreamingRepartitionJoin.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/repartition/StreamingRepartitionJoin.java).
- en: '[PRE9]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The MapReduce driver code will need to be updated to indicate which fields in
    the tuple should be used for sorting, partitioning, and grouping:^([[11](#ch06fn12)])
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 驱动代码需要更新，以指示元组中哪些字段用于排序、分区和分组：^([[11](#ch06fn12)])
- en: ^(11) Secondary sort is covered in more detail in [section 6.2.1](#ch06lev2sec7).
  id: totrans-227
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (11) 更详细的二次排序内容请见 [第 6.2.1 节](#ch06lev2sec7)。
- en: The partitioner should only partition based on the user name, so that all the
    records for a user arrive at the same reducer.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区器应该只根据用户名进行分区，这样同一个用户的全部记录都会到达同一个 reducer。
- en: Sorting should use both the user name and dataset indicator, so that the smaller
    dataset is ordered first (by virtue of the fact that the `USERS` constant is a
    smaller number than the `USER_LOGS` constant, resulting in the user records being
    sorted before the user logs).
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排序应使用用户名和数据集指示符，以便首先对较小的数据集进行排序（由于 `USERS` 常量比 `USER_LOGS` 常量小，因此用户记录会在用户日志之前排序）。
- en: 'The grouping should group on users so that both datasets are streamed to the
    same reducer invocation:'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分组应基于用户进行，以便两个数据集都流式传输到同一个 reducer 调用：
- en: '[PRE10]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Finally, you’ll modify the reducer to cache the incoming user records, and then
    join them with the user logs:^([[12](#ch06fn13)])
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将修改 reducer 以缓存传入的用户记录，然后与用户日志进行连接：^([[12](#ch06fn13)])
- en: '^(12) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/repartition/StreamingRepartitionJoin.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/repartition/StreamingRepartitionJoin.java).'
  id: totrans-233
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (12) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/repartition/StreamingRepartitionJoin.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/repartition/StreamingRepartitionJoin.java).
- en: '[PRE11]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You can use the following commands to run the job and view the job’s output:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下命令来运行作业并查看作业输出：
- en: '[PRE12]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Hive
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Hive
- en: Hive can support a similar optimization when performing repartition joins. Hive
    can cache all the datasets for a join key and then stream the large dataset so
    that it doesn’t need to be stored in memory.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行重新分区连接时，Hive 可以支持类似的优化。Hive 可以缓存连接键的所有数据集，然后流式传输大型数据集，这样就不需要将其存储在内存中。
- en: 'Hive assumes that the largest dataset is specified last in your query. Imagine
    you had two tables called users and user_logs, and user_logs was much larger.
    To join these tables, you’d make sure that the user_logs table was referenced
    as the last table in the query:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 假设你在查询中最后指定的是最大的数据集。想象一下，你有两个表，名为 users 和 user_logs，其中 user_logs 表的数据量要大得多。为了连接这两个表，你需要确保在查询中最后引用的是
    user_logs 表：
- en: '[PRE13]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If you don’t want to rearrange your query, you can alternatively use the `STREAMTABLE`
    hint to tell Hive which table is larger:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想重新排列你的查询，你可以使用 `STREAMTABLE` 指示来告诉 Hive 哪个表更大：
- en: '[PRE14]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Summary
  id: totrans-243
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: This join implementation improves on the earlier technique by buffering only
    the values of the smaller dataset. But it still suffers from the problem of all
    the data being transmitted between the map and reduce phases, which is an expensive
    network cost to incur.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这种连接实现通过仅缓冲较小数据集的值来改进早期技术。但它仍然存在所有数据在 map 和 reduce 阶段之间传输的问题，这是一个昂贵的网络成本。
- en: Further, the previous technique can support *N*-way joins, but this implementation
    only supports two-way joins.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，前面的技术可以支持 *N*-way 连接，但此实现仅支持双向连接。
- en: A simple mechanism to reduce further the memory footprint of the reduce-side
    join is to be aggressive about projections and filters in the map function, as
    discussed in technique 55.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在 map 函数中积极进行投影和过滤，可以进一步减少 reduce 端连接的内存占用，正如在技巧 55 中所讨论的那样。
- en: Technique 61 Using Bloom filters to cut down on shuffled data
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 61 使用 Bloom 过滤器减少洗牌数据
- en: Imagine that you wanted to perform a join over a subset of your data according
    to some predicate, such as “only users that live in California.” With the repartition
    job techniques covered so far, you’d have to perform that filter in the reducer,
    because only one dataset (the users) has details about the state—the user logs
    don’t have that information.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要根据某些谓词（例如“仅限居住在加利福尼亚的用户”）对数据子集执行连接操作。使用到目前为止所涵盖的重新分区作业技术，你将不得不在 reducer
    中执行该过滤，因为只有一个数据集（用户）有关于州的信息——用户日志没有该信息。
- en: In this technique we’ll look at how a Bloom filter can be used on the map side,
    which can have a big impact on your job execution time.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种技术中，我们将探讨如何在 map 端使用 Bloom 过滤器，这可以大大影响作业执行时间。
- en: Problem
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to filter data in a repartition join, but to push that filter to the
    mappers.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在重新分区连接中过滤数据，但要将该过滤器推送到 Mapper。
- en: Solution
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use a preprocessing job to create a Bloom filter, and then load the Bloom filter
    in the repartition job to filter out records in the mappers.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预处理作业创建 Bloom 过滤器，然后在重新分区作业中加载 Bloom 过滤器以过滤 Mapper 中的记录。
- en: Discussion
  id: totrans-254
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: A Bloom filter is a useful probabilistic data structure that provides membership
    qualities much like a set—the difference is that membership lookups only provide
    a definitive “no” answer, as it’s possible to get false positives. Nevertheless,
    they require a lot less memory compared to a HashSet in Java, so they’re well-suited
    to work with very large datasets.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: Bloom 过滤器是一个有用的概率数据结构，它提供的成员特性类似于集合——区别在于成员查找只提供明确的“否”答案，因为可能会得到假阳性。尽管如此，与 Java
    中的 HashSet 相比，它们需要的内存要少得多，因此非常适合与非常大的数据集一起使用。
- en: '|  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: More about Bloom filters
  id: totrans-257
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 更多关于 Bloom 过滤器的信息
- en: '[Chapter 7](kindle_split_019.html#ch07) provides details on how Bloom filters
    work and how to use MapReduce to create a Bloom filter in parallel.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[第 7 章](kindle_split_019.html#ch07) 提供了关于 Bloom 过滤器如何工作以及如何使用 MapReduce 并行创建
    Bloom 过滤器的详细信息。'
- en: '|  |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Your goal in this technique is to perform a join only on users that live in
    California. There are two steps to this solution—you’ll first run a job to generate
    the Bloom filter, which will operate on the user data and be populated with users
    that live in California. This Bloom filter will then be used in the repartition
    join to discard users that don’t exist in the Bloom filter. The reason you need
    this Bloom filter is that the mapper for the user logs doesn’t have details on
    the users’ states.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技术中，你的目标是只对居住在加利福尼亚州的用户执行连接操作。这个解决方案有两个步骤——你首先运行一个作业来生成 Bloom 过滤器，该过滤器将操作用户数据并填充居住在加利福尼亚州的用户。然后，这个
    Bloom 过滤器将在重新分区连接中使用，以丢弃不在 Bloom 过滤器中的用户。你需要这个 Bloom 过滤器的理由是，用户日志的 Mapper 没有关于用户州份的详细信息。
- en: '[Figure 6.12](#ch06fig12) shows the steps in this technique.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6.12](#ch06fig12) 展示了该技术的步骤。'
- en: Figure 6.12\. The two-step process to using a Bloom filter in a repartition
    join
  id: totrans-262
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.12\. 在重新分区连接中使用 Bloom 过滤器的两步过程
- en: '![](06fig12_alt.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图 6-12](06fig12_alt.jpg)'
- en: 'Step 1: Creating the Bloom filter'
  id: totrans-264
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第一步：创建 Bloom 过滤器
- en: The first job creates the Bloom filter containing names of users that are in
    California. The mappers generate intermediary Bloom filters, and the reducer combines
    them together into a single Bloom filter. The job output is an Avro file containing
    the serialized Bloom filter:^([[13](#ch06fn14)])
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建包含加利福尼亚州用户名称的 Bloom 过滤器。Mapper 生成中间 Bloom 过滤器，Reducer 将它们合并成一个单独的 Bloom
    过滤器。作业输出是一个包含序列化 Bloom 过滤器的 Avro 文件：^([[13](#ch06fn14)])
- en: '^(13) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/bloom/BloomFilterCreator.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/bloom/BloomFilterCreator.java).'
  id: totrans-266
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (13) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/bloom/BloomFilterCreator.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/bloom/BloomFilterCreator.java).
- en: '![](281fig01_alt.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![图 281-01](281fig01_alt.jpg)'
- en: 'Step 2: The repartition join'
  id: totrans-268
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第二步：重新分区连接
- en: The repartition join is identical to the repartition join presented in technique
    59—the only difference is that the mappers now load the Bloom filter generated
    in the first step, and when processing the map records, they perform a membership
    query against the Bloom filter to determine whether the record should be sent
    to the reducer.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 重新分区连接与第 59 技术中介绍的重新分区连接相同——唯一的区别是 Mapper 现在加载第一步生成的 Bloom 过滤器，在处理 Map 记录时，它们会对
    Bloom 过滤器执行成员查询，以确定记录是否应该发送到 Reducer。
- en: 'The reducer is unchanged from the original repartition join, so the following
    code shows two things: the abstract mapper that generalizes the loading of the
    Bloom filter and the filtering and emission, and the two subclasses that support
    the two datasets being joined:^([[14](#ch06fn15)])'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: Reducer 与原始重新分区连接没有变化，所以下面的代码展示了两个东西：一个抽象的 Mapper，它泛化了 Bloom 过滤器的加载、过滤和排放，以及支持两个要连接的数据集的两个子类：^([[14](#ch06fn15)])
- en: '^(14) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/bloom/BloomJoin.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/bloom/BloomJoin.java).'
  id: totrans-271
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (14) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/bloom/BloomJoin.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/joins/bloom/BloomJoin.java).
- en: '![](282fig01_alt.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![图 282-01](282fig01_alt.jpg)'
- en: 'The following commands run the two jobs and dump the output of the join:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令运行两个作业并输出join的结果：
- en: '![](283fig01_alt.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![图片](283fig01_alt.jpg)'
- en: Summary
  id: totrans-275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: This technique presented an effective method of performing a map-side filter
    on both datasets to minimize the network I/O between mappers and reducers. It
    also reduces the amount of data that needs to be spilled to and from disk in both
    the mappers and reducers as part of the shuffle. Filters are often the simplest
    and most optimal method of speeding up and optimizing your jobs, and they work
    just as well for repartition joins as they do for other MapReduce jobs.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 本技术提出了一种在两个数据集上执行map-side过滤的有效方法，以最小化mappers和reducers之间的网络I/O。它还减少了在shuffle过程中mappers和reducers需要写入和读取磁盘的数据量。过滤器通常是加速和优化你的作业最简单和最有效的方法，它们对于repartition
    joins和其他MapReduce作业同样有效。
- en: Why not use a hashtable rather than a Bloom filter to represent the users? To
    construct a Bloom filter with a false positive rate of 1%, you need just 9.8 bits
    for each element in the data structure. Compare this with the best-case use of
    a `HashSet` containing integers, which requires 8 bytes. Or if you were to have
    a `HashSet` that only reflected the presence of an element that ignores collision,
    you’d end up with a Bloom filter with a single hash, yielding higher false positives.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不使用散列表而不是Bloom过滤器来表示用户？为了构建一个具有1%误报率的Bloom过滤器，你只需要为数据结构中的每个元素分配9.8位。与包含整数的`HashSet`的最佳使用情况相比，它需要8字节。或者，如果你有一个只反映元素存在而忽略冲突的`HashSet`，你将得到一个只有一个散列的Bloom过滤器，导致更高的误报率。
- en: Version 0.10 of Pig will include support for Bloom filters in a mechanism similar
    to that presented here. Details can be viewed in the JIRA ticket at [https://issues.apache.org/jira/browse/PIG-2328](https://issues.apache.org/jira/browse/PIG-2328).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Pig的0.10版本将包括对Bloom过滤器的支持，其机制与这里展示的类似。详细信息可以在JIRA票据[https://issues.apache.org/jira/browse/PIG-2328](https://issues.apache.org/jira/browse/PIG-2328)中查看。
- en: In this section you learned that Bloom filters offer good space-constrained
    set membership capabilities. We looked at how you could create Bloom filters in
    Map-Reduce, and you also applied that code to a subsequent technique, which helped
    you optimize a MapReduce semi-join.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你了解到Bloom过滤器提供了良好的空间受限集合成员能力。我们探讨了如何在Map-Reduce中创建Bloom过滤器，并且你也应用了该代码到后续的技术中，这有助于你优化MapReduce半连接。
- en: 6.1.3\. Data skew in reduce-side joins
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3\. 数据倾斜在reduce-side joins中
- en: 'This section covers a common issue that’s encountered when joining together
    large datasets—that of data skew. There are two types of data skew that could
    be present in your data:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了在连接大型数据集时遇到的一个常见问题——数据倾斜。你的数据中可能存在两种类型的数据倾斜：
- en: High join-key cardinality, where you have some join keys that have a large number
    of records in one or both of the datasets. I call this *join-product skew*.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高join-key基数，即某些join keys在一个或两个数据集中有大量记录。我称之为*join-product倾斜*。
- en: Poor hash partitioning, where a minority of reducers receive a large percentage
    of the overall number of records. I refer to this as *hash-partitioning skew*.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 糟糕的hash分区，其中少数reducers接收了整体记录数的大比例。我称之为*hash-partitioning倾斜*。
- en: In severe cases, join-product skews can result in heap exhaustion issues due
    to the amount of data that needs to be cached. Hash-partitioning skew manifests
    itself as a join that takes a long time to complete, where a small percentage
    of the reducers take significantly longer to complete compared to the majority
    of the reducers.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在严重的情况下，join-product倾斜可能导致由于需要缓存的数据量而导致的堆耗尽问题。hash-partitioning倾斜表现为一个耗时较长的join，其中少数reducers的完成时间显著长于大多数reducers。
- en: The techniques in this section examine these two situations and present recommendations
    for combating them.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的技术检查了这两种情况，并提出了应对它们的建议。
- en: Technique 62 Joining large datasets with high join-key cardinality
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术编号62：使用高join-key基数连接大型数据集
- en: This technique tackles the problem of join-product skew, and the next technique
    examines hash-partitioning skew.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 本技术解决了join-product倾斜的问题，下一个技术将检查hash-partitioning倾斜。
- en: Problem
  id: totrans-288
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: Some of your join keys are high-cardinality, which results in some of your reducers
    running out of memory when trying to cache these keys.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 一些你的join keys具有高基数，这导致一些reducers在尝试缓存这些键时内存不足。
- en: Solution
  id: totrans-290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Filter out these keys and join them separately or spill them out in the reducer
    and schedule a follow-up job to join them.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤掉这些键并单独连接它们，或者在reducer中将它们溢出并安排一个后续作业来连接它们。
- en: Discussion
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: If you know ahead of time which keys are high-cardinality, you can separate
    them out into a separate join job, as shown in [figure 6.13](#ch06fig13).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你提前知道哪些键是高基数的，你可以将它们分离出来作为一个单独的连接作业，如图6.13所示。
- en: Figure 6.13\. Dealing with skew when you know the high-cardinality key ahead
    of time
  id: totrans-294
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.13. 在提前知道高基数键的情况下处理偏斜
- en: '![](06fig13_alt.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig13_alt.jpg)'
- en: If you don’t know the high-cardinality keys, you may have to build some intelligence
    into your reducers to detect these keys and write them out to a side-effect file,
    which is joined by a subsequent job, as illustrated in [figure 6.14](#ch06fig14).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不知道高基数键，你可能需要在你的reducer中构建一些智能来检测这些键并将它们写入一个副作用文件，随后由一个后续作业连接，如图6.14所示。
- en: Figure 6.14\. Dealing with skew when you don’t know high-cardinality keys ahead
    of time
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.14. 在不知道高基数键的情况下处理偏斜
- en: '![](06fig14_alt.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig14_alt.jpg)'
- en: '|  |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Hive 0.13
  id: totrans-300
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Hive 0.13
- en: The skewed key implementation was flawed in Hive versions before 0.13 ([https://issues.apache.org/jira/browse/HIVE-6041](https://issues.apache.org/jira/browse/HIVE-6041)).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hive版本0.13之前，偏斜键实现有缺陷（[https://issues.apache.org/jira/browse/HIVE-6041](https://issues.apache.org/jira/browse/HIVE-6041)）。
- en: '|  |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Hive
  id: totrans-303
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Hive
- en: 'Hive supports a skew-mitigation strategy similar to the second approach presented
    in this technique. It can be enabled by specifying the following configurables
    prior to running the job:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: Hive支持一种与该技巧中提出的第二种方法类似的偏斜缓解策略。可以在运行作业之前通过指定以下可配置项来启用：
- en: '![](285fig01_alt.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![图片](285fig01_alt.jpg)'
- en: 'You can optionally set some additional configurables to control the map-side
    join that operates on the high-cardinality keys:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以可选地设置一些额外的可配置项来控制操作高基数键的map-side join：
- en: '![](285fig02_alt.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![图片](285fig02_alt.jpg)'
- en: 'Finally, if you’re using a `GROUP BY` in your SQL, you may also want to consider
    enabling the following configuration to handle skews in the grouped data:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你在SQL中使用`GROUP BY`，你可能还想考虑启用以下配置来处理分组数据中的偏斜：
- en: '[PRE15]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Summary
  id: totrans-310
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: The options presented in this technique assume that for a given join key, only
    one dataset has high-cardinality occurrences; hence the use of a map-side join
    that caches the smaller of the datasets. If both datasets are high-cardinality,
    then you’re facing an expensive Cartesian product operation that will be slow
    to execute, as it doesn’t lend itself to the MapReduce way of doing work (meaning
    it’s not inherently splittable and parallelizable). In this case, you are essentially
    out of options in terms of optimizing the actual join. You should reexamine whether
    any back-to-basics techniques, such as filtering or projecting your data, can
    help alleviate the time required to execute the join.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 本技巧中提出的选项假设对于给定的连接键，只有一个数据集有高基数发生；因此使用了缓存较小数据集的map-side join。如果两个数据集都是高基数的，那么你将面临一个昂贵的笛卡尔积操作，这将执行缓慢，因为它不适合MapReduce的工作方式（这意味着它不是固有的可分割和可并行化的）。在这种情况下，你在优化实际连接方面实际上没有其他选择。你应该重新审视是否有一些回归基础的技术，如过滤或投影你的数据，可以帮助减轻执行连接所需的时间。
- en: The next technique looks at a different type of skew that can be introduced
    into your application as a result of using the default hash partitioner.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个技术探讨的是由于使用默认的哈希分区器而可能引入到你的应用程序中的不同类型的偏斜。
- en: Technique 63 Handling skews generated by the hash partitioner
  id: totrans-313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧63 处理由哈希分区器生成的偏斜
- en: The default partitioner in MapReduce is a hash partitioner, which takes a hash
    of each map output key and performs a modulo against the number of reducers to
    determine the reducer the key is sent to. The hash partitioner works well as a
    general partitioner, but it’s possible that some datasets will cause the hash
    partitioner to overload some reducers due to a disproportionate number of keys
    being hashed to the same reducer.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce的默认分区器是一个哈希分区器，它对每个map输出键进行哈希处理，并对其与reducer数量的模数运算来确定键将被发送到的reducer。哈希分区器作为一个通用分区器工作得很好，但有可能某些数据集会导致哈希分区器因不均衡数量的键被哈希到同一个reducer而使某些reducer过载。
- en: This is manifested by a small number of straggling reducers taking much longer
    to complete compared to the majority of reducers. In addition, when you examine
    the straggler reducer counters, you’ll notice that the number of groups sent to
    the stragglers is much higher than the others that have completed.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 这表现为少数拖沓的reducer完成时间比大多数reducer长得多。此外，当您检查拖沓reducer的计数器时，您会注意到发送给拖沓者的组数比其他已完成的其他组要高得多。
- en: '|  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Differentiating between skew caused by high-cardinality keys versus a hash partitioner
  id: totrans-317
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 区分由高基数键引起的偏斜与哈希分区器引起的偏斜
- en: You can use the MapReduce reducer counters to identify the type of data skew
    in your job. Skews introduced by a poorly performing hash partitioner will have
    a much higher number of groups (unique keys) sent to these reducers, whereas the
    symptoms of high-cardinality keys causing skew is evidenced by the roughly equal
    number of groups across all reducers but a much higher number of records for skewed
    reducers.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用MapReduce的reducer计数器来识别作业中的数据偏斜类型。由性能不佳的哈希分区器引入的偏斜将导致发送到这些reducer的组（唯一键）数量大大增加，而高基数键引起偏斜的症状则体现在所有reducer的组数大致相等，但偏斜reducer的记录数却大大增加。
- en: '|  |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Problem
  id: totrans-320
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: Your reduce-side joins are taking a long time to complete, with several straggler
    reducers taking significantly longer to complete than the majority.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 您的reduce端连接完成时间较长，有几个拖沓的reducer完成时间比大多数reducer长得多。
- en: Solution
  id: totrans-322
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use a range partitioner or write a custom partitioner that siphons skewed keys
    to a reserved set of reducers.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 使用范围分区器或编写一个自定义分区器，将偏斜键引导到一组预留的reducer。
- en: Discussion
  id: totrans-324
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'The goal of this solution is to dispense with the default hash partitioner
    and replace it with something that works better with your skewed data. There are
    two options you can explore here:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案的目的是放弃默认的哈希分区器，并替换为更适合您偏斜数据的东西。这里有您可以探索的两个选项：
- en: You can use the sampler and `TotalOrderPartitioner` that comes bundled with
    Hadoop, which replaces the hash partitioner with a range partitioner.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用Hadoop附带的自定义采样器和`TotalOrderPartitioner`，它用范围分区器替换了哈希分区器。
- en: You can write a custom partitioner that routes keys with data skew to a set
    of reducers reserved for skewed keys.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以编写一个自定义分区器，将具有数据偏斜的键路由到一组为偏斜键预留的reducer。
- en: Let’s explore both options and look at how you’d use them.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索这两个选项，并看看您将如何使用它们。
- en: Range partitioning
  id: totrans-329
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 范围分区
- en: A range partitioner will distribute map outputs based on a predefined range
    of values, where each range maps to a reducer that will receive all outputs within
    that range. This is exactly how the `TotalOrderPartitioner` works. In fact, the
    `TotalOrderPartitioner` is used by TeraSort to evenly distribute words across
    all the reducers to minimize straggling reducers.^([[15](#ch06fn16)])
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 范围分区器将根据预定义的值范围分配map输出，其中每个范围映射到一个将接收该范围内所有输出的reducer。这正是`TotalOrderPartitioner`的工作方式。事实上，`TotalOrderPartitioner`被TeraSort用于在所有reducer之间均匀分配单词，以最小化拖沓的reducer。^([15](#ch06fn16))
- en: ^(15) TeraSort is a Hadoop benchmarking tool that sorts a terabyte of data.
  id: totrans-331
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([15](#ch06fn16)) TeraSort是一个Hadoop基准测试工具，用于对TB级数据进行排序。
- en: For range partitioners such as the `TotalOrderPartitioner` to do their work,
    they need to know the output key ranges for a given job. The `TotalOrderPartitioner`
    is accompanied by a sampler that samples the input data and writes these ranges
    to HDFS, which is then used by the `TotalOrderPartitioner` when partitioning.
    More details on how to use the `TotalOrderPartitioner` and the sampler are covered
    in [section 6.2](#ch06lev1sec2).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使范围分区器如`TotalOrderPartitioner`能够工作，它们需要知道给定作业的输出键范围。`TotalOrderPartitioner`附带一个采样器，该采样器采样输入数据并将这些范围写入HDFS，然后由`TotalOrderPartitioner`在分区时使用。有关如何使用`TotalOrderPartitioner`和采样器的更多详细信息，请参阅[第6.2节](#ch06lev1sec2)。
- en: Custom partitioner
  id: totrans-333
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 自定义分区器
- en: If you already have a handle on which keys exhibit data skew, and that set of
    keys is static, you can write a custom partitioner to push these high-cardinality
    join keys to a reserved set of reducers. Imagine that you’re running a job with
    ten reducers—you could decide to use two of them for keys that are skewed, and
    then hash partition all other keys across the remainder of the reducers.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经掌握了哪些键表现出数据偏斜，并且这组键是静态的，您可以编写一个自定义分区器将这些高基数连接键推送到一组预留的reducer。想象一下，您正在运行一个有十个reducer的作业——您可以选择使用其中两个来处理偏斜的键，然后对所有其他键在剩余的reducer之间进行哈希分区。
- en: Summary
  id: totrans-335
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: Of the two approaches presented here, range partitioning is quite possibly the
    best solution, as it’s likely that you won’t know which keys are skewed, and it’s
    also possible that the keys that exhibit skew will change over time.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里提出的两种方法中，范围分区可能是最好的解决方案，因为你可能不知道哪些键是倾斜的，而且随着时间的推移，表现出倾斜的键也可能发生变化。
- en: It’s possible to have reduce-side joins in MapReduce because they sort and correlate
    the map output keys together. In the next section, we’ll look at common sorting
    techniques in MapReduce.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在MapReduce中实现reduce-side joins是可能的，因为它们会排序并关联map输出的键。在下一节中，我们将探讨MapReduce中的常见排序技术。
- en: 6.2\. Sorting
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 排序
- en: The magic of MapReduce occurs between the mappers and reducers, where the framework
    groups together all the map output records that were emitted with the same key.
    This MapReduce feature allows you to aggregate and join your data and implement
    powerful data pipelines. To execute this feature, MapReduce internally partitions,
    sorts, and merges data (which is part of the shuffle phase), and the result is
    that each reducer is streamed an ordered set of keys and accompanying values.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce的魔力发生在mapper和reducer之间，框架将具有相同键的所有map输出记录组合在一起。这个MapReduce特性允许你聚合和连接数据，并实现强大的数据处理管道。为了执行此功能，MapReduce内部对数据进行分区、排序和合并（这是洗牌阶段的一部分），结果是每个reducer都会流式传输一个有序的键值对集合。
- en: In this section we’ll explore two particular areas where you’ll want to tweak
    the behavior of MapReduce sorting.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨两个特定的领域，你将想要调整MapReduce排序的行为。
- en: First we’ll look at the secondary sort, which allows you to sort values for
    a reducer key. Secondary sorts are useful when you want some data to arrive at
    your reducer ahead of other data, as in the case of the optimized repartition
    join in technique 60\. Secondary sorts are also useful if you want your job output
    to be sorted by a secondary key. An example of this is if you want to perform
    a primary sort of stock data by stock symbol, and then perform a secondary sort
    on the time of each stock quote during a day.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将探讨二次排序，它允许你对reducer键的值进行排序。二次排序在需要某些数据在其它数据之前到达reducer的情况下很有用，例如在技术60中的优化repartition
    join。如果想要你的作业输出按二次键排序，二次排序也非常有用。一个例子是，如果你想要按股票代码对股票数据进行主要排序，然后在一天中按每个股票报价的时间进行二次排序。
- en: The second scenario we’ll cover in this section is sorting data across all the
    reducer outputs. This is useful in situations where you want to extract the top
    or bottom *N* elements from a dataset.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍第二个场景，即对所有reducer输出进行排序数据。这在需要从数据集中提取前N个或后N个元素的情况下很有用。
- en: These are important areas that allow you to perform some of the joins that we
    looked at earlier in this chapter. But the applicability of sorting isn’t limited
    to joins; sorting also allows you to provide a secondary sort over your data.
    Secondary sorts are used in many of the techniques in this book, ranging from
    optimizing the repartition join to graph algorithms such as friends-of-friends.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是重要的领域，允许你执行本章前面查看的一些连接操作。但排序的应用不仅限于连接；排序还允许你对数据进行二次排序。二次排序在本书中的许多技术中都有应用，从优化repartition
    join到朋友的朋友这样的图算法。
- en: 6.2.1\. Secondary sort
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1\. 二次排序
- en: As you saw in the discussion of joining in [section 6.1](#ch06lev1sec1), you
    need secondary sorts to allow some records to arrive at a reducer ahead of other
    records. Secondary sorts require an understanding of both data arrangement and
    data flows in MapReduce. [Figure 6.15](#ch06fig15) shows the three elements that
    affect data arrangement and flow (partitioning, sorting, and grouping) and how
    they’re integrated into MapReduce.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在[第6.1节](#ch06lev1sec1)关于连接的讨论中所见，你需要进行二次排序以允许某些记录在其它记录之前到达reducer。二次排序需要理解MapReduce中的数据排列和数据流。![图6.15](#ch06fig15)展示了影响数据排列和流（分区、排序和分组）的三个要素以及它们如何集成到MapReduce中。
- en: Figure 6.15\. An overview of where sorting, partitioning, and grouping occur
    in MapReduce
  id: totrans-346
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.15\. MapReduce中排序、分区和分组发生的位置概述
- en: '![](06fig15_alt.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig15_alt.jpg)'
- en: The partitioner is invoked as part of the map output collection process, and
    it’s used to determine which reducer should receive the map output. The sorting
    `RawComparator` is used to sort the map outputs within their respective partitions,
    and it’s used in both the map and reduce sides. Finally, the grouping `RawComparator`
    is responsible for determining the group boundaries across the sorted records.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 分区器在map输出收集过程中被调用，用于确定哪个reducer应该接收map输出。排序`RawComparator`用于对各自的分区内的map输出进行排序，并在map和reduce端都使用。最后，分组`RawComparator`负责确定排序记录之间的组边界。
- en: The default behavior in MapReduce is for all three functions to operate on the
    entire output key emitted by map functions.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在MapReduce中，默认行为是所有三个函数都操作由map函数发出的整个输出键。
- en: Technique 64 Implementing a secondary sort
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧64 实现二级排序
- en: Secondary sorts are useful when you want some of the values for a unique map
    key to arrive at a reducer ahead of other values. You can see the value of secondary
    sorting in other techniques in this book, such as the optimized repartition join
    (technique 60), and the friends-of-friends algorithm discussed in [chapter 7](kindle_split_019.html#ch07)
    (technique 68).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 当您希望某些唯一map键的值在reducer端先于其他值到达时，二级排序非常有用。您可以在本书的其他技术中看到二级排序的价值，例如优化的repartition
    join（技巧60），以及在第7章中讨论的friends-of-friends算法（技巧68）。
- en: Problem
  id: totrans-352
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to order values sent to a single reducer invocation for each key.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望对每个键的单个reducer调用发送的值进行排序。
- en: Solution
  id: totrans-354
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: This technique covers writing your partitioner, sort comparator, and grouping
    comparator classes, which are required for secondary sorting to work.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技巧涵盖了编写您的分区器、排序比较器和分组比较器类，这些类对于二级排序的正常工作是必需的。
- en: Discussion
  id: totrans-356
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: In this technique we’ll look at how to use secondary sorts to order people’s
    names. You’ll use the primary sort to order people’s last names, and the secondary
    sort to order their first names.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技术中，我们将探讨如何使用二级排序来对人员姓名进行排序。您将使用主要排序来对人员的姓氏进行排序，并使用二级排序来对他们的名字进行排序。
- en: 'To support secondary sort, you need to create a composite output key, which
    will be emitted by your map functions. The composite key will contain two parts:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持二级排序，您需要创建一个复合输出键，该键将由您的map函数发出。复合键将包含两部分：
- en: The *natural key*, which is the key to use for joining purposes
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自然键*，用于连接的键'
- en: The *secondary key*, which is the key to use to order all of the values sent
    to the reducer for the natural key
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*二级键*，是用于对自然键发送给reducer的所有值进行排序的键'
- en: '[Figure 6.16](#ch06fig16) shows the composite key for the names. It also shows
    a composite value that provides reducer-side access to the secondary key.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.16](#ch06fig16) 展示了姓名的复合键。它还显示了一个复合值，它为reducer端提供了对二级键的访问。'
- en: Figure 6.16\. The user composite key and value
  id: totrans-362
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.16\. 用户复合键和值
- en: '![](06fig16_alt.jpg)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig16_alt.jpg)'
- en: Let’s go through the partitioning, sorting, and grouping phases and implement
    them to sort the names. But before that, you need to write your composite key
    class.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一分析分区、排序和分组阶段，并实现它们以对姓名进行排序。但在那之前，您需要编写您的复合键类。
- en: Composite key
  id: totrans-365
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 复合键
- en: The composite key contains both the first and last name. It extends `WritableComparable`,
    which is recommended for `Writable` classes that are emitted as keys from map
    functions:^([[16](#ch06fn17)])
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 复合键包含姓氏和名字。它扩展了`WritableComparable`，这是推荐用于作为map函数键发出的`Writable`类的：^([[16](#ch06fn17)])
- en: '^(16) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/Person.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/Person.java).'
  id: totrans-367
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^[(16) GitHub源代码](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/Person.java).
- en: '[PRE16]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[Figure 6.17](#ch06fig17) shows the configuration names and methods that you’ll
    call in your code to set the partitioning, sorting, and grouping classes. The
    figure also shows what part of the composite key each class uses.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.17](#ch06fig17) 展示了您在代码中调用的配置名称和方法，以设置分区、排序和分组类。该图还显示了每个类使用的复合键的哪个部分。'
- en: Figure 6.17\. Partitioning, sorting, and grouping settings and key utilization
  id: totrans-370
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.17\. 分区、排序和分组设置及键利用
- en: '![](06fig17_alt.jpg)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig17_alt.jpg)'
- en: Let’s look at the implementation code for each of these classes.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些类的实现代码。
- en: Partitioner
  id: totrans-373
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分区器
- en: The partitioner is used to determine which reducer should receive a map output
    record. The default MapReduce partitioner (`HashPartitioner`) calls the `hashCode`
    method of the output key and performs a modulo with the number of reducers to
    determine which reducer should receive the output. The default partitioner uses
    the entire key, which won’t work for your composite key, because it will likely
    send keys with the same natural key value to different reducers. Instead, you
    need to write your own `Partitioner` that partitions on the natural key.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 分区器用于确定哪个reducer应该接收一个map输出记录。默认的MapReduce分区器（`HashPartitioner`）调用输出键的`hashCode`方法，并使用reducer的数量进行取模运算，以确定哪个reducer应该接收输出。默认的分区器使用整个键，这对于你的复合键来说可能不起作用，因为它可能会将具有相同自然键值的键发送到不同的reducer。相反，你需要编写自己的`Partitioner`，该分区器基于自然键进行分区。
- en: 'The following code shows the `Partitioner` interface you must implement. The
    `getPartition` method is passed the key, value, and number of partitions (also
    known as *reducers*):'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了必须实现的`Partitioner`接口。`getPartition`方法接收键、值和分区数（也称为*reducers*）：
- en: '[PRE17]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Your partitioner will calculate a hash based on the last name in the `Person`
    class and perform a modulo of that with the number of partitions (which is the
    number of reducers):^([[17](#ch06fn18)])
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 你的分区器将根据`Person`类中的姓氏计算一个哈希值，并使用分区数（即reducer的数量）进行取模运算：^([17](#ch06fn18))
- en: '^(17) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/PersonNamePartitioner.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/PersonNamePartitioner.java).'
  id: totrans-378
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([17](#ch06fn18)) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/PersonNamePartitioner.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/PersonNamePartitioner.java).
- en: '[PRE18]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Sorting
  id: totrans-380
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 排序
- en: Both the map and reduce sides participate in sorting. The map-side sorting is
    an optimization to help make the reducer sorting more efficient. You want MapReduce
    to use your entire key for sorting purposes, which will order keys according to
    both the last name and the first name.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: map端和reduce端都参与排序。map端的排序是一种优化，有助于使reducer排序更高效。你希望MapReduce使用你的整个键进行排序，这将根据姓氏和名字对键进行排序。
- en: In the following example, you can see the implementation of the `WritableComparator`,
    which compares users based on their last name and their first name:^([[18](#ch06fn19)])
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，你可以看到`WritableComparator`的实现，它根据用户的姓氏和名字进行比较：^([18](#ch06fn19))
- en: '^(18) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/PersonComparator.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/PersonComparator.java).'
  id: totrans-383
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([18](#ch06fn18)) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/PersonComparator.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/PersonComparator.java).
- en: '[PRE19]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Grouping
  id: totrans-385
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分组
- en: Grouping occurs when the reduce phase is streaming map output records from local
    disk. Grouping is the process by which you can specify how records are combined
    to form one logical sequence of records for a reducer invocation.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 分组发生在reduce阶段从本地磁盘流式传输map输出记录时。分组是你可以指定如何组合记录以形成一个逻辑记录序列的过程，用于reducer调用。
- en: When you’re at the grouping stage, all of the records are already in secondary-sort
    order, and the grouping comparator needs to bundle together records with the same
    last name:^([[19](#ch06fn20)])
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 当你处于分组阶段时，所有记录已经按照二级排序顺序排列，分组比较器需要将具有相同姓氏的记录捆绑在一起：^([19](#ch06fn20))
- en: '^(19) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/PersonNameComparator.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/PersonNameComparator.java).'
  id: totrans-388
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([19](#ch06fn20)) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/PersonNameComparator.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/PersonNameComparator.java).
- en: '[PRE20]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: MapReduce
  id: totrans-390
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: MapReduce
- en: The final steps involve telling MapReduce to use the partitioner, sort comparator,
    and group comparator classes:^([[20](#ch06fn21)])
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的步骤包括告诉MapReduce使用分区器、排序比较器和分组比较器类：^([20](#ch06fn21))
- en: '^(20) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/SortMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/SortMapReduce.java).'
  id: totrans-392
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([20](#ch06fn21)) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/SortMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/SortMapReduce.java).
- en: '[PRE21]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: To complete this technique, you need to write the map and reduce code. The mapper
    creates the composite key and emits that in conjunction with the first name as
    the output value. The reducer produces output identical to the input:^([[21](#ch06fn22)])
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这项技术，你需要编写map和reduce代码。mapper创建复合键，并与之一起输出第一个名称作为输出值。reducer产生与输入相同的输出:^([[21](#ch06fn22)])
- en: '^(21) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/SortMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/SortMapReduce.java).'
  id: totrans-395
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([21](#ch06fn21)) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/SortMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/SortMapReduce.java)。
- en: '[PRE22]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To see this sort in action, you can upload a small file with unordered names
    and test whether the secondary sort code produces output sorted by first name:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看这种排序的实际效果，你可以上传一个包含无序名称的小文件，并测试二次排序代码是否产生按姓名排序的输出：
- en: '[PRE23]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The output is sorted as expected.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 输出按预期排序。
- en: Summary
  id: totrans-400
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: As you can see in this technique, it’s nontrivial to use secondary sort. It
    requires you to write a custom partitioner, sorter, and grouper. If you’re working
    with simple data types, consider using htuple ([http://htuple.org/](http://htuple.org/)),
    an open source project I developed, which simplifies secondary sort in your jobs.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在这种技术中看到的那样，使用二次排序并不简单。它要求你编写自定义的分区器、排序器和分组器。如果你正在处理简单的数据类型，可以考虑使用我开发的开源项目htuple
    ([http://htuple.org/](http://htuple.org/))，它简化了作业中的二次排序。
- en: 'htuple exposes a `Tuple` class, which allows you to store one or more Java
    types and provides helper methods to make it easy for you to define which fields
    are used for partitioning, sorting, and grouping. The following code shows how
    htuple can be used to secondary sort on the first name, just like in the technique:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: htuple公开了一个`Tuple`类，它允许你存储一个或多个Java类型，并提供辅助方法，使你能够轻松定义用于分区、排序和分组的字段。以下代码展示了如何使用htuple在第一个名称上进行二次排序，就像在技术中一样：
- en: '![](294fig01_alt.jpg)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![](294fig01_alt.jpg)'
- en: Next we’ll look at how to sort outputs across multiple reducers.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何在多个reducer之间排序输出。
- en: 6.2.2\. Total order sorting
  id: totrans-405
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2\. 完全排序
- en: You’ll find a number of situations where you’ll want to have your job output
    in total sort order.^([[22](#ch06fn23)]) For example, if you want to extract the
    most popular URLs from a web graph, you’ll have to order your graph by some measure
    of popularity, such as Page-Rank. Or if you want to display a table in your portal
    of the most active users on your site, you’ll need the ability to sort them based
    on some criteria, such as the number of articles they wrote.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现许多情况下，你希望你的作业输出处于完全排序顺序.^([[22](#ch06fn23)]) 例如，如果你想从网页图中提取最受欢迎的URL，你必须按某些度量标准（如Page-Rank）对图进行排序。或者，如果你想在你网站的门户中显示最活跃用户的表格，你需要能够根据某些标准（如他们撰写的文章数量）对他们进行排序。
- en: ^(22) Total sort order is when the reducer records are sorted across all the
    reducers, not just within each reducer.
  id: totrans-407
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([22](#ch06fn22)) 完全排序是指reducer记录在所有reducer之间排序，而不仅仅是每个reducer内部。
- en: Technique 65 Sorting keys across multiple reducers
  id: totrans-408
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术篇 65 在多个reducer之间排序键
- en: You know that the MapReduce framework sorts map output keys prior to feeding
    them to reducers. But this sorting is only guaranteed within each reducer, and
    unless you specify a partitioner for your job, you’ll be using the default MapReduce
    partitioner, `HashPartitioner`, which partitions using a hash of the map output
    keys. This ensures that all records with the same map output key go to the same
    reducer, but the `HashPartitioner` doesn’t perform total sorting of the map output
    keys across all the reducers. Knowing this, you may be wondering how you could
    use Map-Reduce to sort keys across multiple reducers so that you can easily extract
    the top and bottom *N* records from your data.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道MapReduce框架在将输出键传递给reducer之前会对map输出键进行排序。但这种排序仅在每个reducer内部得到保证，除非你为你的作业指定分区器，否则你将使用默认的MapReduce分区器`HashPartitioner`，它使用map输出键的哈希值进行分区。这确保了具有相同map输出键的所有记录都发送到同一个reducer，但`HashPartitioner`不会对所有reducer的map输出键进行完全排序。了解这一点后，你可能想知道如何使用Map-Reduce在多个reducer之间对键进行排序，以便你可以轻松地从你的数据中提取最顶部和最底部的*N*条记录。
- en: Problem
  id: totrans-410
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want a total ordering of keys in your job output, but without the overhead
    of having to run a single reducer.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望在作业输出中键的完全排序，但不需要运行单个reducer的开销。
- en: Solution
  id: totrans-412
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: This technique covers use of the `TotalOrderPartitioner` class, a partitioner
    that is bundled with Hadoop, to assist in sorting output across all reducers.
    The partitioner ensures that output sent to the reducers is totally ordered.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术涵盖了使用与 Hadoop 一起打包的 `TotalOrderPartitioner` 类，该分区器有助于对所有 reducers 的输出进行排序。该分区器确保发送给
    reducers 的输出是完全有序的。
- en: Discussion
  id: totrans-414
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Hadoop has a built-in partitioner called the `TotalOrderPartitioner`, which
    distributes keys to specific reducers based on a partition file. The partition
    file is a precomputed SequenceFile that contains *N* – 1 keys, where *N* is the
    number of reducers. The keys in the partition file are ordered by the map output
    key comparator, and as such, each key represents a logical range of keys. To determine
    which reducer should receive an output record, the `TotalOrderPartitioner` examines
    the output key, determines which range it falls into, and maps that range into
    a specific reducer.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 有一个内置的分区器，称为 `TotalOrderPartitioner`，它根据分区文件将键分配到特定的 reducers。分区文件是一个预计算的
    SequenceFile，其中包含 *N* – 1 个键，其中 *N* 是 reducers 的数量。分区文件中的键按 map 输出键比较器排序，因此每个键代表一个逻辑键范围。为了确定哪个
    reducer 应该接收输出记录，`TotalOrderPartitioner` 检查输出键，确定它所在的范围，并将该范围映射到特定的 reducer。
- en: '[Figure 6.18](#ch06fig18) shows the two parts of this technique. You need to
    create the partition file and then run your MapReduce job using the `TotalOrderPartitioner`.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.18](#ch06fig18) 展示了该技术的两个部分。您需要创建分区文件，然后使用 `TotalOrderPartitioner` 运行您的
    MapReduce 作业。'
- en: Figure 6.18\. Using sampling and the `TotalOrderPartitioner` to sort output
    across all reducers.
  id: totrans-417
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.18\. 使用采样和 `TotalOrderPartitioner` 对所有 reducers 的输出进行排序。
- en: '![](06fig18_alt.jpg)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig18_alt.jpg)'
- en: 'First you’ll use the `InputSampler` class, which samples the input files and
    creates the partition file. You can use one of two samplers: the `RandomSampler`
    class, which as the name suggests picks random records from the input, or the
    `IntervalSampler` class, which for every record includes the record in the sample.
    Once the samples have been extracted, they’re sorted, and then *N* – 1 keys are
    written to the partition file, where *N* is the number of reducers. The `InputSampler`
    isn’t a MapReduce job; it reads records from the `InputFormat` and produces the
    partition within the process calling the code.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您将使用 `InputSampler` 类，该类从输入文件中采样并创建分区文件。您可以使用两种采样器之一：名为 `RandomSampler` 的类，正如其名称所示，它从输入中随机选择记录，或者名为
    `IntervalSampler` 的类，对于每条记录都将其包含在样本中。一旦提取了样本，它们将被排序，然后写入分区文件中的 *N* – 1 个键，其中 *N*
    是 reducers 的数量。`InputSampler` 不是一个 MapReduce 作业；它从 `InputFormat` 中读取记录，并在调用代码的过程中生成分区。
- en: The following code shows the steps you need to execute prior to calling the
    `InputSampler` function:^([[23](#ch06fn24)])
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了在调用 `InputSampler` 函数之前需要执行的步骤：^([[23](#ch06fn24)])
- en: '^(23) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/total/TotalSortMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/total/TotalSortMapReduce.java).'
  id: totrans-421
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(23) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/total/TotalSortMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/total/TotalSortMapReduce.java)。
- en: '![](296fig01_alt.jpg)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![图片](296fig01_alt.jpg)'
- en: 'Next up, you need to specify that you want to use the `TotalOrderPartitioner`
    as the partitioner for your job:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要指定您想要将 `TotalOrderPartitioner` 作为作业的分区器：
- en: '[PRE24]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You don’t want to do any processing in your MapReduce job, so you won’t specify
    the map or reduce classes. This means the identity MapReduce classes will be used,
    so you’re ready to run the code:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 您不希望在 MapReduce 作业中进行任何处理，因此您不会指定 map 或 reduce 类。这意味着将使用身份 MapReduce 类，因此您可以运行代码：
- en: '![](296fig02_alt.jpg)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![图片](296fig02_alt.jpg)'
- en: You can see from the results of the MapReduce job that the map output keys are
    indeed sorted across all the output files.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从 MapReduce 作业的结果中看到，map 输出键确实在所有输出文件中进行了排序。
- en: Summary
  id: totrans-428
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: This technique used the `InputSampler` to create the partition file, which is
    subsequently used by the `TotalOrderPartitioner` to partition map output keys.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术使用了 `InputSampler` 来创建分区文件，该文件随后被 `TotalOrderPartitioner` 用于对 map 输出键进行分区。
- en: You could also use MapReduce to generate the partition file. An efficient way
    of doing this would be to write a custom `InputFormat` class that performs the
    sampling and then output the keys to a single reducer, which in turn can create
    the partition file. This brings us to sampling, the last section of this chapter.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用MapReduce生成分区文件。一种高效的方法是编写一个自定义的`InputFormat`类，该类执行抽样并将键输出到单个reducer，然后reducer可以创建分区文件。这把我们带到了本章的最后一部分：抽样。
- en: 6.3\. Sampling
  id: totrans-431
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3. 抽样
- en: Imagine you’re working with a terabyte-scale dataset, and you have a MapReduce
    application you want to test with that dataset. Running your MapReduce application
    against the dataset may take hours, and constantly making code refinements and
    rerunning against the large dataset isn’t an optimal workflow.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你正在处理一个千兆级的数据集，并且你想要使用这个数据集测试你的MapReduce应用程序。运行你的MapReduce应用程序可能需要数小时，并且不断地对代码进行优化并重新运行大型数据集并不是一个最佳的工作流程。
- en: To solve this problem, you look to sampling, which is a statistical methodology
    for extracting a relevant subset of a population. In the context of MapReduce,
    sampling provides an opportunity to work with large datasets without the overhead
    of having to wait for the entire dataset to be read and processed. This greatly
    enhances your ability to quickly iterate when developing and debugging MapReduce
    code.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个问题，你可以考虑使用抽样，这是一种从总体中提取相关子集的统计方法。在MapReduce的上下文中，抽样提供了一种在不等待整个数据集被读取和处理的情况下处理大型数据集的机会。这大大提高了你在开发和调试MapReduce代码时快速迭代的效率。
- en: Technique 66 Writing a reservoir-sampling InputFormat
  id: totrans-434
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧66 编写水库抽样InputFormat
- en: You’re developing a MapReduce job iteratively with a large dataset, and you
    need to do testing. Testing with the entire dataset takes a long time and impedes
    your ability to rapidly work with your code.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在迭代地使用大型数据集开发MapReduce作业，并且需要进行测试。使用整个数据集进行测试需要很长时间，并阻碍了你快速与代码工作的能力。
- en: Problem
  id: totrans-436
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to work with a small subset of a large dataset during the development
    of a MapReduce job.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望在开发MapReduce作业期间使用大型数据集的一个小子集。
- en: Solution
  id: totrans-438
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Write an input format that can wrap the actual input format used to read data.
    The input format that you’ll write can be configured with the number of samples
    that should be extracted from the wrapped input format.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个可以包装实际用于读取数据的输入格式的输入格式。你将要编写的输入格式可以配置为从包装的输入格式中提取的样本数量。
- en: Discussion
  id: totrans-440
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: In this technique you’ll use reservoir sampling to choose samples. Reservoir
    sampling is a strategy that allows a single pass through a stream to randomly
    produce a sample.^([[24](#ch06fn25)]) As such, it’s a perfect fit for MapReduce
    because input records are streamed from an input source. [Figure 6.19](#ch06fig19)
    shows the algorithm for reservoir sampling.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技巧中，你将使用水库抽样来选择样本。水库抽样是一种策略，允许对数据流进行一次遍历以随机生成样本.^([24](#ch06fn25)) 因此，它非常适合MapReduce，因为输入记录是从输入源流式传输的。[图6.19](#ch06fig19)展示了水库抽样的算法。
- en: ^(24) For more information on reservoir sampling, see the Wikipedia article
    at [http://en.wikipedia.org/wiki/Reservoir_sampling](http://en.wikipedia.org/wiki/Reservoir_sampling).
  id: totrans-442
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([24](#ch06fn27)) 关于水库抽样的更多信息，请参阅维基百科上的文章：[http://en.wikipedia.org/wiki/Reservoir_sampling](http://en.wikipedia.org/wiki/Reservoir_sampling)。
- en: Figure 6.19\. The reservoir-sampling algorithm allows one pass through a stream
    to randomly produce a sample.
  id: totrans-443
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.19。水库抽样算法允许对数据流进行一次遍历以随机生成样本。
- en: '![](06fig19_alt.jpg)'
  id: totrans-444
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig19_alt.jpg)'
- en: The input split determination and record reading will be delegated to wrapped
    `InputFormat` and `RecordReader` classes. You’ll write classes that provide the
    sampling functionality and then wrap the delegated `InputFormat` and `RecordReader`
    classes.^([[25](#ch06fn26)]) [Figure 6.20](#ch06fig20) shows how the `ReservoirSamplerRecordReader`
    works.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 输入拆分确定和记录读取将委托给包装的`InputFormat`和`RecordReader`类。你将编写提供抽样功能的类，然后包装委托的`InputFormat`和`RecordReader`类.^([25](#ch06fn26))[图6.20](#ch06fig20)展示了`ReservoirSamplerRecordReader`的工作方式。
- en: ^(25) If you need a refresher on these classes, please review [chapter 3](kindle_split_013.html#ch03)
    for more details.
  id: totrans-446
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([25](#ch06fn25)) 如果你需要对这些类进行复习，请参阅第3章以获取更多详细信息。[chapter 3](kindle_split_013.html#ch03)。
- en: Figure 6.20\. The `ReservoirSamplerRecordReader` in action
  id: totrans-447
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.20。`ReservoirSamplerRecordReader`的实际操作
- en: '![](06fig20_alt.jpg)'
  id: totrans-448
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig20_alt.jpg)'
- en: The following code shows the `ReservoirSamplerRecordReader`:^([[26](#ch06fn27)])
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了`ReservoirSamplerRecordReader`：^([26](#ch06fn27)）
- en: '^(26) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sampler/ReservoirSamplerInputFormat.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sampler/ReservoirSamplerInputFormat.java).'
  id: totrans-450
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([26] GitHub源：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sampler/ReservoirSamplerInputFormat.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sampler/ReservoirSamplerInputFormat.java)).
- en: '![](299fig01_alt.jpg)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
  zh: '![图片](299fig01_alt.jpg)'
- en: To use the `ReservoirSamplerInputFormat` class in your code, you’ll use convenience
    methods to help set up the input format and other parameters, as shown in the
    following code:^([[27](#ch06fn28)])
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 要在您的代码中使用`ReservoirSamplerInputFormat`类，您将使用便利方法来帮助设置输入格式和其他参数，如下面的代码所示:^([27](#ch06fn28)])
- en: '^(27) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sampler/SamplerJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sampler/SamplerJob.java)'
  id: totrans-453
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([27] GitHub源：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sampler/SamplerJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sampler/SamplerJob.java))
- en: '![](299fig02_alt.jpg)'
  id: totrans-454
  prefs: []
  type: TYPE_IMG
  zh: '![图片](299fig02_alt.jpg)'
- en: 'You can see the sampling input format in action by running an identity job
    against a large file containing names:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过运行一个针对包含名称的大文件的标识符作业来查看采样输入格式的实际效果。
- en: '![](300fig01_alt.jpg)'
  id: totrans-456
  prefs: []
  type: TYPE_IMG
  zh: '![图片](300fig01_alt.jpg)'
- en: You configured the `ReservoirSamplerInputFormat` to extract ten samples, and
    the output file contained that number of lines.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 您已配置`ReservoirSamplerInputFormat`以提取十个样本，输出文件包含相应数量的行。
- en: Summary
  id: totrans-458
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 总结
- en: 'Sampling support in MapReduce code can be a useful development and testing
    feature when engineers are running code against production-scale datasets. That
    begs the question: what’s the best approach for integrating sampling support into
    an existing codebase? One approach would be to add a configurable option that
    would toggle the use of the sampling input format, similar to the following code:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在MapReduce代码中的采样支持可以在工程师对生产规模数据集运行代码时成为一个有用的开发和测试特性。这引发了一个问题：将采样支持集成到现有代码库中的最佳方法是什么？一种方法可能是添加一个可配置的选项，用于切换采样输入格式的使用，类似于以下代码：
- en: '[PRE25]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: You can apply this sampling technique to any of the preceding sections as a
    way to work efficiently with large datasets.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将这种采样技术应用于前面的任何部分，作为高效处理大数据集的一种方式。
- en: 6.4\. Chapter summary
  id: totrans-462
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4. 章节总结
- en: Joining and sorting are cumbersome tasks in MapReduce, and we spent this chapter
    discussing methods to optimize and facilitate their use. We looked at three different
    join strategies, two of which were on the map side, and one on the reduce side.
    The goal was to simplify joins in MapReduce, and I presented two frameworks that
    reduce the amount of user code required for joins.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 连接和排序在MapReduce中是繁琐的任务，我们在本章讨论了优化和简化它们使用的方法。我们研究了三种不同的连接策略，其中两种在map端，一种在reduce端。目标是简化MapReduce中的连接，我介绍了两个减少连接所需用户代码量的框架。
- en: We also covered sorting in MapReduce by examining how secondary sorts work and
    how you can sort all of the output across all the reducers. And we wrapped things
    up with a look at how you can sample data so that you can quickly iterate over
    smaller samples of your data.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还通过检查二级排序的工作原理以及如何对所有reducer的输出进行排序来介绍了MapReduce中的排序。我们通过查看如何采样数据以便快速遍历数据的小样本来结束讨论。
- en: We’ll cover a number of performance patterns and tuning steps in [chapter 8](kindle_split_020.html#ch08),
    which will result in faster join and sorting times. But before we get there, we’ll
    look at some more advanced data structures and algorithms, such as graph processing
    and working with Bloom filters.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第8章中介绍多个性能模式和调整步骤，这将导致更快的连接和排序时间。但在我们到达那里之前，我们将探讨一些更高级的数据结构和算法，例如图处理和使用Bloom过滤器。
- en: Chapter 7\. Utilizing data structures and algorithms at scale
  id: totrans-466
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7章. 在大规模数据中利用数据结构和算法
- en: '*This chapter covers*'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Representing and using data structures such as graphs, HyperLogLog, and Bloom
    filters in MapReduce
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在MapReduce中表示和使用数据结构，如图、HyperLogLog和Bloom过滤器
- en: Applying algorithms such as PageRank and semi-joins to large amounts of data
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将PageRank和半连接等算法应用于大量数据
- en: Learning how social network companies recommend making connections with people
    outside your network
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习社交网络公司如何推荐与网络外的人建立联系
- en: In this chapter we’ll look at how you can implement algorithms in MapReduce
    to work with internet-scale data. We’ll focus on nontrivial data, which is commonly
    represented using graphs.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何在MapReduce中实现算法以处理互联网规模的数据。我们将关注非平凡数据，这些数据通常使用图来表示。
- en: We’ll also look at how you can use graphs to model connections between entities,
    such as relationships in a social network. We’ll run through a number of useful
    algorithms that can be performed over graphs, such as shortest path and friends-of-friends
    (FoF), to help expand the interconnectedness of a network, and PageRank, which
    looks at how to determine the popularity of web pages.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将探讨如何使用图来模拟实体之间的连接，例如社交网络中的关系。我们将运行一系列在图上可以执行的有用算法，如最短路径和好友好友（FoF），以帮助扩展网络的互联性，以及PageRank，它研究如何确定网页的流行度。
- en: You’ll learn how to use Bloom filters, whose unique space-saving properties
    make them handy for solving distributed system problems in P2P (peer-to-peer)
    and distributed databases. We’ll also create Bloom filters in MapReduce and then
    look at their usefulness for filtering.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习如何使用布隆过滤器，它独特的空间节省特性使其在解决P2P（对等）和分布式数据库中的分布式系统问题时变得很有用。我们还将创建MapReduce中的布隆过滤器，并探讨它们在过滤方面的有用性。
- en: You’ll also learn about another approximate data structure called HyperLogLog,
    that provides approximate unique counts, which are invaluable in aggregation pipelines.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 你还将了解另一种近似数据结构HyperLogLog，它提供近似唯一计数，这在聚合管道中非常有价值。
- en: A chapter on scalable algorithms wouldn’t be complete without mention of sorting
    and joining algorithms, which are covered in [chapter 6](kindle_split_018.html#ch06).
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 一章关于可扩展算法的内容，如果没有提及排序和连接算法，那就不是完整的。这些算法在[第6章](kindle_split_018.html#ch06)中有详细讲解。
- en: Let’s kick things off with a look at how you can model graphs in MapReduce.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从如何使用MapReduce来模拟图开始。
- en: 7.1\. Modeling data and solving problems with graphs
  id: totrans-477
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1。使用图建模数据和解决问题
- en: Graphs are mathematical constructs that represent an interconnected set of objects.
    They’re used to represent data such as the hyperlink structure of the internet,
    social networks (where they represent relationships between users), and internet
    routing to determine optimal paths for forwarding packets.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 图是表示一组相互连接的对象的数学结构。它们用于表示数据，例如互联网的超链接结构、社交网络（其中它们表示用户之间的关系）以及互联网路由，以确定转发数据包的最佳路径。
- en: A graph consists of a number of nodes (formally called *vertices*) and links
    (informally called *edges*) that connect nodes together. [Figure 7.1](#ch07fig01)
    shows a graph with nodes and edges.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 一个图由多个节点（正式称为*顶点*）和连接节点的链接（非正式称为*边*）组成。[图7.1](#ch07fig01)展示了带有节点和边的图。
- en: Figure 7.1\. A small graph with highlighted nodes and edges
  id: totrans-480
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.1。一个带有突出显示的节点和边的简单图
- en: '![](07fig01.jpg)'
  id: totrans-481
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig01.jpg)'
- en: The edges can be directed (implying a one-way relationship) or undirected. For
    example, you would use a directed graph to model relationships between users in
    a social network, because relationships are not always bidirectional. [Figure
    7.2](#ch07fig02) shows examples of directed and undirected graphs.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 边可以是定向的（意味着单向关系）或非定向的。例如，你会使用一个有向图来模拟社交网络中用户之间的关系，因为关系并不总是双向的。[图7.2](#ch07fig02)展示了有向图和非有向图的示例。
- en: Figure 7.2\. Directed and undirected graphs
  id: totrans-483
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2。有向图和非有向图
- en: '![](07fig02.jpg)'
  id: totrans-484
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig02.jpg)'
- en: Directed graphs, where the edges have a direction, can be cyclic or acyclic.
    In cyclic graphs, it’s possible for a vertex to reach itself by traversing a sequence
    of edges. In an acyclic graph, it’s not possible for a vertex to traverse a path
    to reach itself. [Figure 7.3](#ch07fig03) shows examples of cyclic and acyclic
    graphs.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 有向图，其中边有方向，可以是循环的或非循环的。在循环图中，一个顶点可以通过遍历一系列边来达到自身。在一个非循环图中，一个顶点不可能通过遍历路径来达到自身。[图7.3](#ch07fig03)展示了循环图和非循环图的示例。
- en: Figure 7.3\. Cyclic and acyclic graphs
  id: totrans-486
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.3。循环图和非循环图
- en: '![](07fig03.jpg)'
  id: totrans-487
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig03.jpg)'
- en: To start working with graphs, you’ll need to be able to represent them in your
    code. So what are the common methods used to represent these graph structures?
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用图，你需要在代码中能够表示它们。那么，表示这些图结构常用的方法有哪些呢？
- en: 7.1.1\. Modeling graphs
  id: totrans-489
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.1。图建模
- en: Two common ways of representing graphs are with *adjacency matrices* and *adjacency
    lists*.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 表示图的两种常见方式是使用*邻接矩阵*和*邻接表*。
- en: Adjacency matrix
  id: totrans-491
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 邻接矩阵
- en: With an adjacency matrix, you represent a graph as an *N* x *N* square matrix
    *M*, where *N* is the number of nodes and *Mij* represents an edge between nodes
    *i* and *j*.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 使用邻接矩阵，你将图表示为一个 *N* x *N* 的正方形矩阵 *M*，其中 *N* 是节点的数量，而 *Mij* 表示节点 *i* 和 *j* 之间的边。
- en: '[Figure 7.4](#ch07fig04) shows a directed graph representing connections in
    a social graph. The arrows indicate one-way relationships between two people.
    The adjacency matrix shows how this graph would be represented.'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.4](#ch07fig04) 展示了一个表示社交图中连接的有向图。箭头表示两个人之间的一对一关系。邻接矩阵显示了如何表示这个图。'
- en: Figure 7.4\. An adjacency matrix representation of a graph
  id: totrans-494
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.4\. 图的邻接矩阵表示
- en: '![](07fig04.jpg)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig04.jpg)'
- en: The disadvantage of adjacency matrices are that they model both the existence
    and lack of a relationship, which makes them dense data structures requiring more
    space than adjacency lists.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 邻接矩阵的缺点是它们既表示了关系的存在，也表示了关系的缺失，这使得它们成为密集的数据结构，需要比邻接表更多的空间。
- en: Adjacency list
  id: totrans-497
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 邻接表
- en: Adjacency lists are similar to adjacency matrices, except that they don’t model
    the lack of relationships. [Figure 7.5](#ch07fig05) shows how you’d represent
    a graph using an adjacency list.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 邻接表与邻接矩阵类似，但它们不表示关系的缺失。[图7.5](#ch07fig05) 展示了如何使用邻接表表示一个图。
- en: Figure 7.5\. An adjacency list representation of a graph
  id: totrans-499
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.5\. 图的邻接表表示
- en: '![](07fig05.jpg)'
  id: totrans-500
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig05.jpg)'
- en: The advantage of the adjacency list is that it offers a sparse representation
    of the data, which is good because it requires less space. It also fits well when
    representing graphs in Map-Reduce because the key can represent a vertex, and
    the values are a list of vertices that denote a directed or undirected relationship
    node.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 邻接表的优点是它提供了数据的稀疏表示，这是好的，因为它需要更少的空间。它也适合在Map-Reduce中表示图，因为键可以表示一个顶点，而值是一个表示有向或无向关系节点的顶点列表。
- en: Next up we’ll cover three graph algorithms, starting off with the shortest-path
    algorithm.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将介绍三个图算法，首先是最短路径算法。
- en: 7.1.2\. Shortest-path algorithm
  id: totrans-503
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.2\. 最短路径算法
- en: The shortest-path algorithm is a common problem in graph theory, where the goal
    is to find the shortest route between two nodes. [Figure 7.6](#ch07fig06) shows
    an example of this algorithm on a graph where the edges don’t have a weight, in
    which case the shortest path is the path with the smallest number of hops or intermediary
    nodes between the source and destination.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 最短路径算法是图论中一个常见问题，其目标是找到两个节点之间的最短路径。[图7.6](#ch07fig06) 展示了在边没有权重的图上的该算法的一个示例，在这种情况下，最短路径是源节点和目标节点之间跳数或中间节点数最少的路径。
- en: Figure 7.6\. Example of shortest path between nodes A and E
  id: totrans-505
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.6\. 节点A和E之间的最短路径示例
- en: '![](07fig06.jpg)'
  id: totrans-506
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig06.jpg)'
- en: Applications of this algorithm include determining the shortest route between
    two addresses in traffic mapping software, routers computing the shortest path
    tree for each route, and social networks determining connections between users.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的应用包括在交通映射软件中确定两个地址之间的最短路径，路由器计算每条路径的最短路径树，以及社交网络确定用户之间的连接。
- en: Technique 67 Find the shortest distance between two users
  id: totrans-508
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧67 查找两个用户之间的最短距离
- en: Dijkstra’s algorithm is a shortest-path algorithm commonly taught in undergraduate
    computer science courses. A basic implementation uses a sequential iterative process
    to traverse the entire graph from the starting node, as seen in the algorithm
    presented in [figure 7.7](#ch07fig07).
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: Dijkstra算法是计算机科学本科课程中常教的最短路径算法。一个基本的实现使用顺序迭代过程遍历整个图，从起始节点开始，如[图7.7](#ch07fig07)中展示的算法所示。
- en: Figure 7.7\. Pseudocode for Dijkstra’s algorithm
  id: totrans-510
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.7\. Dijkstra算法的伪代码
- en: '![](07fig07_alt.jpg)'
  id: totrans-511
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig07_alt.jpg)'
- en: The basic algorithm doesn’t scale to graphs that exceed your memory sizes, and
    it’s also sequential and not optimized for parallel processing.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 基本算法不能扩展到超出你的内存大小的图，它也是顺序的，并且没有针对并行处理进行优化。
- en: Problem
  id: totrans-513
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You need to use MapReduce to find the shortest path between two people in a
    social graph.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要使用MapReduce在社交图中找到两个人之间的最短路径。
- en: Solution
  id: totrans-515
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解答
- en: Use an adjacency matrix to model a graph, and for each node, store the distance
    from the original node, as well as a backpointer to the original node. Use the
    mappers to propagate the distance to the original node, and the reducers to restore
    the state of the graph. Iterate until the target node has been reached.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 使用邻接矩阵来建模图，并为每个节点存储从原始节点到该节点的距离，以及一个指向原始节点的回指针。使用映射器来传播到原始节点的距离，并使用归约器来恢复图的初始状态。迭代直到达到目标节点。
- en: Discussion
  id: totrans-517
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: '[Figure 7.8](#ch07fig08) shows a small social network that you’ll use for this
    technique. Your goal is to find the shortest path between Dee and Joe. There are
    four paths that you can take from Dee to Joe, but only one of them results in
    the fewest hops.'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.8](#ch07fig08)显示了用于此技术的一个小社交网络。你的目标是找到Dee和Joe之间的最短路径。从Dee到Joe有四条路径可以选择，但只有其中一条路径的跳数最少。'
- en: Figure 7.8\. Social network used in this technique
  id: totrans-519
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.8. 用于此技术的社交网络
- en: '![](07fig08.jpg)'
  id: totrans-520
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig08.jpg)'
- en: You’ll implement a parallel breadth-first search algorithm to find the shortest
    path between two users. Because you’re operating on a social network, you don’t
    need to care about weights on your edges. The pseudocode for the algorithm can
    be seen in [figure 7.9](#ch07fig09).
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 你将实现一个并行广度优先搜索算法来找到两个用户之间的最短路径。由于你在一个社交网络上操作，你不需要关心边的权重。该算法的伪代码可以在[图7.9](#ch07fig09)中看到。
- en: Figure 7.9\. Pseudocode for breadth-first parallel search on graph using MapReduce
  id: totrans-522
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.9. 使用MapReduce在图上进行广度优先并行搜索的伪代码
- en: '![](07fig09.jpg)'
  id: totrans-523
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig09.jpg)'
- en: '[Figure 7.10](#ch07fig10) shows the algorithm iterations in play with your
    social graph. Just like Dijkstra’s algorithm, you’ll start with all the node distances
    set to infinite and set the distance for the starting node, Dee, at zero. With
    each MapReduce pass, you’ll determine nodes that don’t have an infinite distance
    and propagate their distance values to their adjacent nodes. You’ll continue this
    until you reach the end node.'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.10](#ch07fig10)显示了你的社交图中的算法迭代。就像Dijkstra算法一样，你将开始时将所有节点的距离设置为无限大，并将起始节点Dee的距离设置为零。随着每次MapReduce遍历，你将确定没有无限距离的节点，并将它们的距离值传播到它们的相邻节点。你将继续这样做，直到达到终点。'
- en: Figure 7.10\. Shortest path iterations through the network
  id: totrans-525
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.10. 通过网络的最短路径迭代
- en: '![](07fig10_alt.jpg)'
  id: totrans-526
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig10_alt.jpg)'
- en: You first need to create the starting point. This is done by reading in the
    social network (which is stored as an adjacency list) from the file and setting
    the initial distance values. [Figure 7.11](#ch07fig11) shows the two file formats,
    the second being the format that’s used iteratively in your MapReduce code.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先需要创建起点。这是通过从文件中读取社交网络（存储为邻接表）并设置初始距离值来完成的。[图7.11](#ch07fig11)显示了两种文件格式，第二种是你在MapReduce代码中迭代使用的格式。
- en: Figure 7.11\. Original social network file format and MapReduce form optimized
    for algorithm
  id: totrans-528
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.11. 原始社交网络文件格式和为算法优化的MapReduce格式
- en: '![](07fig11_alt.jpg)'
  id: totrans-529
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig11_alt.jpg)'
- en: 'Your first step is to create the MapReduce form from the original file. The
    following listing shows the original input file and the MapReduce-ready form of
    the input file generated by the transformation code:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 你的第一步是从原始文件创建MapReduce格式。以下列表显示了原始输入文件和由转换代码生成的MapReduce准备好的输入文件：
- en: '![](308fig01_alt.jpg)'
  id: totrans-531
  prefs: []
  type: TYPE_IMG
  zh: '![图片](308fig01_alt.jpg)'
- en: The code that generates the previous output is shown here:^([[1](#ch07fn01)])
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 生成前面输出的代码在此处显示：^([[1](#ch07fn01)])
- en: '¹ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/shortestpath/Main.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/shortestpath/Main.java).'
  id: totrans-533
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹ GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/shortestpath/Main.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/shortestpath/Main.java).
- en: '![](308fig02_alt.jpg)'
  id: totrans-534
  prefs: []
  type: TYPE_IMG
  zh: '![图片](308fig02_alt.jpg)'
- en: The structure of the MapReduce data isn’t changed across iterations of the algorithm;
    each job produces the same structure, which makes it easy to iterate, because
    the input format is the same as the output format.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce数据结构在算法的迭代过程中没有改变；每个作业都产生相同的结构，这使得迭代变得容易，因为输入格式与输出格式相同。
- en: Your map function will perform two major tasks. First, it outputs all the node
    data to preserve the original structure of the graph. If you didn’t do this, you
    couldn’t make this an interactive process, because the reducer wouldn’t be able
    to reproduce the original graph structure for the next map phase. The second task
    of the map is to output that adjacent node with its distance and a backpointer
    if the node has a non-infinite distance number. The backpointer carries information
    about the nodes visited from the starting node, so when you reach the end node,
    you know the exact path that was taken to get there. Here’s the code for the map
    function:^([[2](#ch07fn02)])
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 您的map函数将执行两个主要任务。首先，它输出所有节点数据以保留图的原始结构。如果您不这样做，您就不能将其作为一个交互式过程，因为reducer将无法为下一个map阶段重新生成原始图结构。map的第二个任务是输出具有距离和回溯指针的相邻节点（如果节点具有非无穷大的距离数）。回溯指针携带有关从起始节点访问的节点信息，因此当您到达终点节点时，您就知道到达那里的确切路径。以下是map函数的代码.^([[2](#ch07fn02)])
- en: '² GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/shortestpath/Map.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/shortestpath/Map.java).'
  id: totrans-537
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ² GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/shortestpath/Map.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/shortestpath/Map.java).
- en: '![](309fig01_alt.jpg)'
  id: totrans-538
  prefs: []
  type: TYPE_IMG
  zh: '![图片](309fig01_alt.jpg)'
- en: When outputting the original input node, as well as the adjacent nodes and the
    distances to them, the format (not contents) of the map output value is identical
    to make it easier for your reducer to read the data. To do this, you use a `Node`
    class to model the notion of a node, its adjacent nodes, and the distance from
    the starting node. Its `toString` method generates a `String` form of this data,
    which is used as the map output key, as shown in the following listing.^([[3](#ch07fn03)])
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 当输出原始输入节点以及相邻节点及其距离时，map输出值的格式（而非内容）与reducer读取数据时相同，这使得reducer更容易读取数据。为此，您使用`Node`类来表示节点的概念、其相邻节点以及从起始节点的距离。它的`toString`方法生成数据的`String`形式，该形式用作map输出键，如下所示.^([[3](#ch07fn03)])
- en: '³ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/shortestpath/Node.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/shortestpath/Node.java).'
  id: totrans-540
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³ GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/shortestpath/Node.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/shortestpath/Node.java).
- en: Listing 7.1\. The `Node` class helps with serialization in MapReduce code
  id: totrans-541
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.1. `Node`类帮助在MapReduce代码中进行序列化
- en: '[PRE26]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The reducer is invoked for each node and is supplied a list of all the adjacent
    nodes and their shortest paths. It iterates through all the adjacent nodes and
    determines the current node’s shortest path by selecting the adjacent node with
    the smallest, shortest path. The reducer then outputs the minimum distance, the
    backpointer, and the original adjacent nodes. The following listing shows this
    code.^([[4](#ch07fn04)])
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个节点，都会调用reducer，并为其提供一个包含所有相邻节点及其最短路径的列表。它遍历所有相邻节点，通过选择具有最小最短路径的相邻节点来确定当前节点的最短路径。然后，reducer输出最小距离、回溯指针和原始相邻节点。以下列表显示了此代码.^([[4](#ch07fn04)])
- en: '⁴ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/shortestpath/Reduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/shortestpath/Reduce.java).'
  id: totrans-544
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴ GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/shortestpath/Reduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/shortestpath/Reduce.java).
- en: Listing 7.2\. The reducer code for the shortest-path algorithm
  id: totrans-545
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.2. 最短路径算法的reducer代码
- en: '![](ch07ex02-0.jpg)'
  id: totrans-546
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch07ex02-0.jpg)'
- en: '![](ch07ex02-1.jpg)'
  id: totrans-547
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch07ex02-1.jpg)'
- en: 'You’re ready to run your code. You need to copy the input file into HDFS, and
    then kick off your MapReduce job, specifying the start node name (`dee`) and target
    node name (`joe`):'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 您已准备好运行代码。您需要将输入文件复制到HDFS中，然后启动MapReduce作业，指定起始节点名称（`dee`）和目标节点名称（`joe`）：
- en: '[PRE27]'
  id: totrans-549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The output of your job shows that the minimum number of hops between Dee and
    Joe is 2, and that Ali was the connecting node.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 您的工作输出显示，Dee和Joe之间的最小跳数是2，并且Ali是连接节点。
- en: Summary
  id: totrans-551
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: This exercise showed how a shortest-path algorithm could be used to determine
    the minimum number of hops between two people in a social network. An algorithm
    related to the shortest-path algorithm, called *graph diameter estimation*, attempts
    to determine the average number of hops between nodes.^([[5](#ch07fn05)]) This
    has been used to support the notion of *six degrees of separation* in large social
    network graphs with millions of nodes.^([[6](#ch07fn06)])
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习展示了如何使用最短路径算法来确定社交网络中两个人之间的最小跳数。与最短路径算法相关的一个算法，称为*图直径估计*，试图确定节点之间的平均跳数.^([[5](#ch07fn05)])
    这已被用于支持在具有数百万个节点的庞大社交网络图中的*六度分隔*概念.^([[6](#ch07fn06)))
- en: '⁵ See U. Kang et al., “HADI: Fast Diameter Estimation and Mining in Massive
    Graphs with Hadoop” (December 2008), [http://reports-archive.adm.cs.cmu.edu/anon/ml2008/CMU-ML-08-117.pdf](http://reports-archive.adm.cs.cmu.edu/anon/ml2008/CMU-ML-08-117.pdf).'
  id: totrans-553
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵参见U. Kang等人，“HADI：使用Hadoop在大型图中进行快速直径估计和挖掘”（2008年12月），[http://reports-archive.adm.cs.cmu.edu/anon/ml2008/CMU-ML-08-117.pdf](http://reports-archive.adm.cs.cmu.edu/anon/ml2008/CMU-ML-08-117.pdf)。
- en: ⁶ See Lars Backstrom et al., “Four Degrees of Separation,” [http://arxiv.org/abs/1111.4570](http://arxiv.org/abs/1111.4570).
  id: totrans-554
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶参见Lars Backstrom等人，“四度分隔”，[http://arxiv.org/abs/1111.4570](http://arxiv.org/abs/1111.4570)。
- en: '|  |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Inefficiencies of using MapReduce for iterative graph processing
  id: totrans-556
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用MapReduce进行迭代图处理的低效性
- en: Using Map-Reduce for graph processing is inefficient from an I/O perspective—each
    graph iteration is executed within a single MapReduce job. As a result, the entire
    graph structure must be written to HDFS (in triplicate, or whatever your HDFS
    replication setting is) in between jobs and then be read by the subsequent job.
    Graph algorithms that may require a large number of iterations (such as this shortest-path
    example) are best executed using Giraph, which is covered in [section 7.1.4](#ch07lev2sec5).
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 从I/O的角度来看，使用Map-Reduce进行图处理是低效的——每个图迭代都在单个MapReduce作业中执行。因此，整个图结构必须在作业之间写入HDFS（三份，或者根据您的HDFS复制设置），然后由后续作业读取。可能需要大量迭代的图算法（如这个最短路径示例）最好使用Giraph执行，这在[7.1.4节](#ch07lev2sec5)中有介绍。
- en: '|  |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The shortest-path algorithm has multiple applications, but an arguably more
    useful and utilized algorithm in social networks is friends-of-friends (FoF).
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 最短路径算法有多个应用，但在社交网络中可能更有用且更常用的算法是朋友的朋友（FoF）。
- en: 7.1.3\. Friends-of-friends algorithm
  id: totrans-560
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.3\. 朋友的朋友算法
- en: Social network sites such as LinkedIn and Facebook use the friends-of-friends
    (FoF) algorithm to help users broaden their networks.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 社交网站如LinkedIn和Facebook使用朋友的朋友（FoF）算法来帮助用户扩大他们的网络。
- en: Technique 68 Calculating FoFs
  id: totrans-562
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧68 计算FoF
- en: The friends-of-friends algorithm suggests friends that a user may know but who
    aren’t part of their immediate network. For this technique, we’ll consider a FoF
    to be in the second degree of separation, as shown in [figure 7.12](#ch07fig12).
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 朋友的朋友算法建议用户可能认识但不是他们直接网络一部分的朋友。对于这种技术，我们将FoF视为第二度分隔，如图7.12[图7.12](#ch07fig12)所示。
- en: Figure 7.12\. An example of FoF where Joe and Jon are considered FoFs to Jim
  id: totrans-564
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.12\. 一个Joe和Jon被认为是Jim的FoF的FoF示例
- en: '![](07fig12.jpg)'
  id: totrans-565
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig12.jpg)'
- en: The key ingredient to success with this approach is to order the FoFs by the
    number of common friends, which increases the chances that the user knows the
    FoF.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法取得成功的关键是要按共同朋友的数量对FoF进行排序，这样可以增加用户知道FoF的可能性。
- en: Problem
  id: totrans-567
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to implement the FoF algorithm in MapReduce.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 您想使用MapReduce实现FoF算法。
- en: Solution
  id: totrans-569
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Two MapReduce jobs are required to calculate the FoFs for each user in a social
    network. The first job calculates the common friends for each user, and the second
    job sorts the common friends by the number of connections to your friends. You
    can then recommend new friends by selecting the top FoFs based on this sorted
    list.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 计算社交网络中每个用户的FoF需要两个MapReduce作业。第一个作业计算每个用户的共同朋友，第二个作业根据与朋友之间的连接数对共同朋友进行排序。然后，您可以根据这个排序列表选择顶级FoF来推荐新朋友。
- en: Discussion
  id: totrans-571
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: You should first look at an example graph and understand what results you’re
    looking for. [Figure 7.13](#ch07fig13) shows a network of people with Jim, one
    of the users, highlighted. In this graph, Jim’s FoFs are in bold circles, and
    the number of friends that the FoF and Jim have in common is also identified.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该首先查看一个示例图，了解您正在寻找的结果。[图7.13](#ch07fig13)显示了Jim，一个用户，被突出显示的人的网络。在这个图中，Jim的FoF用粗圆圈表示，并且FoF和Jim共同拥有的朋友数量也被识别出来。
- en: Figure 7.13\. A graph representing Jim’s FoFs
  id: totrans-573
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.13。表示Jim的FoFs的图
- en: '![](07fig13_alt.jpg)'
  id: totrans-574
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig13_alt.jpg)'
- en: Your goal is to determine all the FoFs and order them by the number of friends
    in common. In this case, your expected results would have Joe as the first FoF
    recommendation, followed by Dee, and then Jon.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 你的目标是确定所有的好友好友（FoFs）并按共同好友的数量进行排序。在这种情况下，你预期的结果应该是将Joe作为第一个FoF推荐，其次是Dee，然后是Jon。
- en: 'The text file that represents the social graph for this technique is shown
    here:'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 表示此技术的社会图的文本文件如下所示：
- en: '[PRE28]'
  id: totrans-577
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This algorithm requires you to write two MapReduce jobs. The first job, the
    pseudo-code for which is shown in [figure 7.14](#ch07fig14), calculates the FoFs
    and, for each FoF, counts the number of friends in common. The result of the job
    is a line for each FoF relationship, excluding people who are already friends.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法要求你编写两个MapReduce作业。第一个作业，其伪代码如图7.14所示，计算FoFs，并为每个FoF计算共同好友的数量。作业的结果是每个FoF关系的行，不包括已经是朋友的人。
- en: Figure 7.14\. The first MapReduce job, which calculates the FoFs
  id: totrans-579
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.14。计算FoFs的第一个MapReduce作业
- en: '![](07fig14_alt.jpg)'
  id: totrans-580
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig14_alt.jpg)'
- en: 'The output when you execute this job against the graph in [figure 7.13](#ch07fig13)
    is shown here:'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 当你对此图（图7.13）执行此作业时的输出如下所示：
- en: '[PRE29]'
  id: totrans-582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The second job needs to produce output that lists FoFs in order of the number
    of common friends. [Figure 7.15](#ch07fig15) shows the algorithm. You’re using
    a secondary sort to order a user’s FoFs in order of the number of common friends.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个作业需要生成按共同好友数量排序的FoFs的输出。图7.15显示了算法。你正在使用辅助排序来按共同好友数量对用户的FoFs进行排序。
- en: Figure 7.15\. The second MapReduce job, which sorts the FoFs by the number of
    friends in common
  id: totrans-584
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.15。按共同好友数量排序FoFs的第二个MapReduce作业
- en: '![](07fig15_alt.jpg)'
  id: totrans-585
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig15_alt.jpg)'
- en: 'The output of executing this job against the output of the previous job can
    be seen here:'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此作业对前一个作业输出的输出可以在此处看到：
- en: '[PRE30]'
  id: totrans-587
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Let’s dive into the code. The following listing shows the first MapReduce job,
    which calculates the FoFs for each user.^([[7](#ch07fn07)])
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入代码。以下列表显示了第一个MapReduce作业，该作业计算每个用户的FoFs.^([[7](#ch07fn07)])
- en: '⁷ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/friendsofafriend/CalcMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/friendsofafriend/CalcMapReduce.java).'
  id: totrans-589
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/friendsofafriend/CalcMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/friendsofafriend/CalcMapReduce.java).
- en: Listing 7.3\. Mapper and reducer implementations for FoF calculation
  id: totrans-590
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.3。FoF计算的Mapper和reducer实现
- en: '![](ch07ex03-0.jpg)'
  id: totrans-591
  prefs: []
  type: TYPE_IMG
  zh: '![](ch07ex03-0.jpg)'
- en: '![](ch07ex03-1.jpg)'
  id: totrans-592
  prefs: []
  type: TYPE_IMG
  zh: '![](ch07ex03-1.jpg)'
- en: The job of the second MapReduce job in the following listing is to sort the
    FoFs so that you see FoFs with a higher number of mutual friends ahead of those
    that have a smaller number of mutual friends.^([[8](#ch07fn08)])
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表中第二个MapReduce作业的作业是对FoFs进行排序，以便你可以看到拥有更多共同好友的FoFs排在拥有较少共同好友的FoFs之前.^([[8](#ch07fn08)])
- en: '⁸ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/friendsofafriend/SortMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/friendsofafriend/SortMapReduce.java).'
  id: totrans-594
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/friendsofafriend/SortMapReduce.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/friendsofafriend/SortMapReduce.java).
- en: Listing 7.4\. Mapper and reducer implementations that sort FoFs
  id: totrans-595
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.4。排序FoFs的Mapper和reducer实现
- en: '![](ch07ex04-0.jpg)'
  id: totrans-596
  prefs: []
  type: TYPE_IMG
  zh: '![](ch07ex04-0.jpg)'
- en: '![](ch07ex04-1.jpg)'
  id: totrans-597
  prefs: []
  type: TYPE_IMG
  zh: '![](ch07ex04-1.jpg)'
- en: 'I won’t show the whole driver code, but to enable the secondary sort, I had
    to write a few extra classes as well as inform the job to use the classes for
    partitioning and sorting purposes: ^([[9](#ch07fn09)])'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会展示整个驱动代码，但为了启用辅助排序，我不得不编写几个额外的类，并通知作业使用这些类进行分区和排序的目的：^([[9](#ch07fn09)])
- en: '⁹ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/friendsofafriend/Main.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/friendsofafriend/Main.java).'
  id: totrans-599
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/friendsofafriend/Main.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/friendsofafriend/Main.java).
- en: '[PRE31]'
  id: totrans-600
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: For more details on how secondary sort works, look at [chapter 6](kindle_split_018.html#ch06).
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于辅助排序如何工作的详细信息，请参阅第6章 [kindle_split_018.html#ch06](kindle_split_018.html#ch06)。
- en: 'Copy the input file containing the friend relationships into HDFS, and then
    run the driver code to run your two MapReduce jobs. The last two arguments are
    the output directories for the two MapReduce jobs:'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 将包含朋友关系的输入文件复制到HDFS中，然后运行驱动代码来运行你的两个MapReduce作业。最后两个参数是两个MapReduce作业的输出目录：
- en: '[PRE32]'
  id: totrans-603
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'After running your code, you can look at the output in HDFS:'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 运行你的代码后，你可以在HDFS中查看输出：
- en: '[PRE33]'
  id: totrans-605
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This output verifies what you saw with your own eyes in [figure 7.13](#ch07fig13).
    Jim has three FoFs, and they’re ordered by the number of common friends.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出验证了你自己在[图7.13](#ch07fig13)中看到的内容。Jim有三个FoFs，它们按照共同朋友的数量排序。
- en: Summary
  id: totrans-607
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: This approach can be used not only as a recommendation engine to help users
    grow their networks, but also for informational purposes when the user is browsing
    the social network’s website. For example, when you view people in LinkedIn, you’ll
    be shown the degrees of separation between you and the person being viewed. This
    approach can be used to precompute that information for two hops. To reproduce
    this for three hops (for example, to show friends-of-friends-of-friends) you’d
    need to introduce a third MapReduce job to compute the third hop from the output
    of the first job.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法不仅可以作为推荐引擎帮助用户扩展他们的网络，还可以在用户浏览社交网络网站时用于信息目的。例如，当你查看LinkedIn上的人时，你会看到你与被查看的人之间的分离度。这种方法可以用来预先计算两个跳的信息。为了复制三个跳（例如，显示朋友的朋友的朋友），你需要引入第三个MapReduce作业来从第一个作业的输出中计算第三个跳。
- en: To simplify this approach, we used an undirected graph, which implies that user
    relationships are bidirectional. Most social networks don’t have such a notion,
    and the algorithm would need some minor tweaks to model directed graphs.
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化这种方法，我们使用了一个无向图，这意味着用户关系是双向的。大多数社交网络没有这样的概念，算法需要一些小的调整来模拟有向图。
- en: This example required two MapReduce jobs to complete the algorithm, which means
    that the entire graph was written to HDFS between jobs. This isn’t particularly
    inefficient given the number of jobs, but once the number of iterations over your
    graph data goes beyond two, it’s probably time to start looking at more efficient
    ways of working with your graph data. You’ll see this in the next technique, where
    Giraph will be used to calculate the popularity of web pages.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子需要两个MapReduce作业来完成算法，这意味着整个图在作业之间被写入HDFS。考虑到作业的数量，这并不特别低效，但一旦你在图数据上的迭代次数超过两次，可能就是时候开始寻找更有效的工作方式来处理你的图数据了。你将在下一个技术中看到这一点，其中将使用Giraph来计算网页的流行度。
- en: 7.1.4\. Using Giraph to calculate PageRank over a web graph
  id: totrans-611
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.4\. 使用Giraph在网页图上计算PageRank
- en: Using MapReduce for iterative graph processing introduces a number of inefficiencies,
    which are highlighted in [figure 7.16](#ch07fig16).
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MapReduce进行迭代图处理引入了许多低效性，这在[图7.16](#ch07fig16)中被突出显示。
- en: Figure 7.16\. An iterative graph algorithm implemented using MapReduce
  id: totrans-613
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.16\. 使用MapReduce实现的迭代图算法
- en: '![](07fig16_alt.jpg)'
  id: totrans-614
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig16_alt.jpg)'
- en: This isn’t something that should be of concern if your graph algorithm only
    requires one or two iterations, but beyond that the successive HDFS barriers between
    jobs will start to add up, especially with large graphs. At that point it’s time
    to look at alternative methods for graph processing, such as Giraph.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的图算法只需要一两次迭代，这并不是你应该担心的事情，但超过这个范围，作业之间的连续HDFS屏障将开始累积，尤其是在大型图的情况下。到那时，是时候考虑图处理的替代方法了，比如Giraph。
- en: This section presents an overview of Giraph and then applies it to calculate
    PageRank over a web graph. PageRank is a good fit for Giraph as it’s an example
    of an iterative graph algorithm that can require many iterations before the graph
    converges.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了Giraph的概述，并将其应用于在网页图上计算PageRank。PageRank非常适合Giraph，因为它是一个迭代图算法的例子，在图收敛之前可能需要多次迭代。
- en: An Introduction to Giraph
  id: totrans-617
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Giraph简介
- en: Giraph is an Apache project modeled after Google’s Pregel, which describes a
    system for large-scale graph processing. Pregel was designed to reduce the inefficiencies
    of using MapReduce for graph processing and to provide a programming model that
    is vertex-centric.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: Giraph是一个基于Google的Pregel的Apache项目，它描述了一个用于大规模图处理系统。Pregel被设计用来减少使用MapReduce进行图处理的低效性，并提供一个以顶点为中心的编程模型。
- en: To combat the disk and network barriers that exist in MapReduce, Giraph loads
    all the vertices into memory across a number of worker processes and keeps them
    in memory during the whole process. Each graph iteration is composed of the workers
    supplying inputs to the vertices that they manage, the vertices performing their
    processing, and the vertices then emitting messages that the framework routes
    to the appropriate adjacent vertices in the graph (as seen in [figure 7.17](#ch07fig17)).
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服MapReduce中存在的磁盘和网络屏障，Giraph将所有顶点加载到多个工作进程的内存中，并在整个过程中保持它们在内存中。每个图迭代由工作者向他们管理的顶点提供输入、顶点执行其处理以及顶点随后发出消息组成，这些消息由框架路由到图中适当的相邻顶点（如图7.17所示）。
- en: Figure 7.17\. Giraph message passing
  id: totrans-620
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.17\. Giraph消息传递
- en: '![](07fig17.jpg)'
  id: totrans-621
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig17.jpg)'
- en: Giraph uses bulk synchronous communication (BSP) to support workers’ communication.
    BSP is essentially an iterative message-passing algorithm that uses a global synchronization
    barrier between successive iterations. [Figure 7.18](#ch07fig18) shows Giraph
    workers each containing a number of vertices, and worker intercommunication and
    synchronization via the barrier.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: Giraph使用批量同步通信（BSP）来支持工作者的通信。BSP本质上是一种迭代消息传递算法，它在连续迭代之间使用全局同步屏障。[图7.18](#ch07fig18)显示了Giraph工作者，每个工作者包含多个顶点，以及通过屏障进行的工作者间通信和同步。
- en: Figure 7.18\. Giraph workers, message passing, and synchronization
  id: totrans-623
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.18\. Giraph工作者，消息传递和同步
- en: '![](07fig18_alt.jpg)'
  id: totrans-624
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig18_alt.jpg)'
- en: Technique 69 will go further into the details, but before we dive in, let’s
    take a quick look at how PageRank works.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 技巧69将进一步深入细节，但在我们深入之前，让我们快速看一下PageRank是如何工作的。
- en: A Brief Overview of PageRank
  id: totrans-626
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: PageRank简述
- en: PageRank is a formula introduced by the founders of Google during their Stanford
    years in 1998.^([[10](#ch07fn10)]) Their paper discusses an overall approach to
    crawling and indexing the web, and it includes, as part of that, a calculation
    that they titled *PageRank*, which gives a score to each web page indicating the
    page’s importance. This wasn’t the first paper to introduce a scoring mechanism
    for web pages,^([[11](#ch07fn11)]) but it was the first to weigh scores propagated
    to each outbound link based on the total number of outbound links.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: PageRank是由Google的创始人于1998年在斯坦福大学期间提出的公式。他们的论文讨论了爬取和索引整个网络的整体方法，其中包括他们称之为*PageRank*的计算，它为每个网页分配一个分数，表示网页的重要性。这不是第一个提出为网页引入评分机制的论文，^([[10](#ch07fn10)))但它是最先根据总出链数来权衡传播到每个出链的分数的。
- en: ^(10) See Sergey Brin and Lawrence Page, “The Anatomy of a Large-Scale Hypertextual
    Web Search Engine,” [http://infolab.stanford.edu/pub/papers/google.pdf](http://infolab.stanford.edu/pub/papers/google.pdf).
  id: totrans-628
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([10]) 参见Sergey Brin和Lawrence Page，“大规模超文本搜索引擎的解剖学”，[http://infolab.stanford.edu/pub/papers/google.pdf](http://infolab.stanford.edu/pub/papers/google.pdf)。
- en: ^(11) Before PageRank, the HITS link-analysis method was popular; see the “Hubs
    and Authorities” page of Christopher D. Manning, Prabhakar Raghavan, and Hinrich
    Schütze, *Introduction to Information Retrieval*, [http://nlp.stanford.edu/IR-book/html/htmledition/hubs-and-authorities-1.html](http://nlp.stanford.edu/IR-book/html/htmledition/hubs-and-authorities-1.html).
  id: totrans-629
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([11]) 在PageRank之前，HITS链接分析方法很流行；参见Christopher D. Manning、Prabhakar Raghavan和Hinrich
    Schütze的*信息检索导论*中的“枢纽和权威”页面，[http://nlp.stanford.edu/IR-book/html/htmledition/hubs-and-authorities-1.html](http://nlp.stanford.edu/IR-book/html/htmledition/hubs-and-authorities-1.html)。
- en: Fundamentally, PageRank gives pages that have a large number of inbound links
    a higher score than pages that have a smaller number of inbound links. When evaluating
    the score for a page, PageRank uses the scores for all the inbound links to calculate
    a page’s PageRank. But it penalizes individual inbound links that have a high
    number of outbound links by dividing that outbound link PageRank by the number
    of outbound links. [Figure 7.19](#ch07fig19) presents a simple example of a web
    graph with three pages and their respective PageRank values.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，PageRank给具有大量入链的页面分配比具有较少入链的页面更高的分数。在评估页面的分数时，PageRank使用所有入链的分数来计算页面的PageRank。但它通过将出链PageRank除以出链数量来惩罚具有大量出链的个别入链。[图7.19](#ch07fig19)展示了具有三个页面及其相应PageRank值的简单网页图的一个简单示例。
- en: Figure 7.19\. PageRank values for a simple web graph
  id: totrans-631
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.19\. 简单网页图的PageRank值
- en: '![](07fig19.jpg)'
  id: totrans-632
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig19.jpg)'
- en: '[Figure 7.20](#ch07fig20) shows the PageRank formula. In the formula, *|webGraph|*
    is a count of all the pages in the graph, and *d*, set to 0.85, is a constant
    damping factor used in two parts. First, it denotes the probability of a random
    surfer reaching the page after clicking on many links (this is a constant equal
    to 0.15 divided by the total number of pages), and second, it dampens the effect
    of the inbound link PageRanks by 85%.'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.20](#ch07fig20)显示了PageRank公式。在公式中，*|webGraph|*是图中所有页面的计数，而*d*，设置为0.85，是一个常数阻尼因子，用于两个部分。首先，它表示随机冲浪者在点击多个链接后到达页面的概率（这是一个等于总页面数除以0.15的常数），其次，它通过85%的比例减弱了入站链接PageRank的影响。'
- en: Figure 7.20\. The PageRank formula
  id: totrans-634
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.20。PageRank公式
- en: '![](07fig20_alt.jpg)'
  id: totrans-635
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig20_alt.jpg)'
- en: Technique 69 Calculate PageRank over a web graph
  id: totrans-636
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧69 在网络图上计算PageRank
- en: PageRank is a graph algorithm that typically requires multiple iterations, and
    as such doesn’t lend itself to being implemented in MapReduce due to the disk
    barrier overhead discussed in this section’s introduction. This technique looks
    at how you can use Giraph, which is well-suited to algorithms that require multiple
    iterations over large graphs, to implement PageRank.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: PageRank是一种通常需要多次迭代的图算法，因此由于本节引言中讨论的磁盘屏障开销，它不适合在MapReduce中实现。这项技术探讨了如何使用Giraph，它非常适合需要在大图上多次迭代的算法，来实现PageRank。
- en: Problem
  id: totrans-638
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to implement an iterative PageRank graph algorithm using Giraph.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 你想使用Giraph实现一个迭代的PageRank图算法。
- en: Solution
  id: totrans-640
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: PageRank can be implemented by iterating a MapReduce job until the graph has
    converged. The mappers are responsible for propagating node PageRank values to
    their adjacent nodes, and the reducers are responsible for calculating new PageRank
    values for each node, and for re-creating the original graph with the updated
    PageRank values.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: PageRank可以通过迭代MapReduce作业直到图收敛来实现。映射器负责将节点PageRank值传播到其相邻节点，而归约器负责为每个节点计算新的PageRank值，并使用更新后的PageRank值重新创建原始图。
- en: Discussion
  id: totrans-642
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: One of the advantages of PageRank is that it can be computed iteratively and
    applied locally. Every vertex starts with a seed value, which is 1 divided by
    the number of nodes, and with each iteration, each node propagates its value to
    all pages it links to. Each vertex in turn sums up all the inbound vertex values
    to compute a new seed value. This iterative process is repeated until such a time
    as convergence is reached.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: PageRank的一个优点是它可以迭代计算并局部应用。每个顶点都从一个种子值开始，这个值是节点数量的倒数，并且随着每次迭代，每个节点都会将其值传播到它链接的所有页面。每个顶点随后将所有入站顶点的值加起来以计算一个新的种子值。这个迭代过程会一直重复，直到达到收敛。
- en: Convergence is a measure of how much the seed values have changed since the
    last iteration. If the convergence value is below a certain threshold, it means
    that there’s been minimal change and you can stop the iteration. It’s also common
    to limit the number of iterations for large graphs where convergence takes too
    many iterations.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 收敛是衡量自上次迭代以来种子值变化程度的一个指标。如果收敛值低于某个阈值，这意味着变化很小，你可以停止迭代。对于收敛需要太多迭代的较大图，也常见限制迭代次数的做法。
- en: '[Figure 7.21](#ch07fig21) shows two iterations of PageRank against the simple
    graph you saw earlier in this chapter.'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.21](#ch07fig21)显示了PageRank对之前在本章中看到的简单图进行的两次迭代。'
- en: Figure 7.21\. An example of PageRank iterations
  id: totrans-646
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.21。PageRank迭代的示例
- en: '![](07fig21_alt.jpg)'
  id: totrans-647
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig21_alt.jpg)'
- en: '[Figure 7.22](#ch07fig22) shows the PageRank algorithm expressed as map and
    reduce phases. The map phase is responsible for preserving the graph as well as
    emitting the PageRank value to all the outbound nodes. The reducer is responsible
    for recalculating the new PageRank value for each node and including it in the
    output of the original graph.'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.22](#ch07fig22)显示了将PageRank算法表示为映射和归约阶段。映射阶段负责保留图以及向所有出站节点发出PageRank值。归约器负责为每个节点重新计算新的PageRank值，并将其包含在原始图的输出中。'
- en: Figure 7.22\. PageRank decomposed into map and reduce phases
  id: totrans-649
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.22。PageRank分解为映射和归约阶段
- en: '![](07fig22.jpg)'
  id: totrans-650
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig22.jpg)'
- en: In this technique, you’ll operate on the graph shown in [figure 7.23](#ch07fig23).
    In this graph, all the nodes have both inbound and outbound edges.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技术中，你将操作[图7.23](#ch07fig23)中显示的图。在这个图中，所有节点都有入站和出站边。
- en: Figure 7.23\. Sample web graph for this technique
  id: totrans-652
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.23。该技术的示例网络图
- en: '![](07fig23.jpg)'
  id: totrans-653
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig23.jpg)'
- en: 'Giraph supports various input and output data formats. For this technique we’ll
    use `JsonLongDoubleFloatDouble-VertexInputFormat` as the input format; it requires
    vertices to be expressed numerically along with an associated weight that we won’t
    use for this technique. We’ll map vertex A to integer 0, B to 1, and so on, and
    for each vertex we’ll identify the adjacent vertices. Each line in the data file
    represents a vertex and the directed edges to adjacent vertices:'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: Giraph支持各种输入和输出数据格式。对于这项技术，我们将使用`JsonLongDoubleFloatDouble-VertexInputFormat`作为输入格式；它要求顶点以数值形式表示，并附带一个关联的权重，我们不会使用这项技术。我们将把顶点A映射到整数0，B映射到1，依此类推，并为每个顶点识别相邻顶点。数据文件中的每一行代表一个顶点和到相邻顶点的有向边：
- en: '[PRE34]'
  id: totrans-655
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following input file represents the graph in [figure 7.23](#ch07fig23):'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输入文件表示[图7.23](#ch07fig23)中的图：
- en: '[PRE35]'
  id: totrans-657
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Copy this data into a file called webgraph.txt and upload it to HDFS:'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 将此数据复制到名为webgraph.txt的文件中，并将其上传到HDFS：
- en: '[PRE36]'
  id: totrans-659
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Your next step is to write the Giraph vertex class. The nice thing about Giraph’s
    model is that it’s simple—it provides a vertex-based API where you need to implement
    the graph-processing logic for a single iteration on that vertex. The vertex class
    is responsible for processing incoming messages from adjacent vertices, using
    them to calculate the node’s new PageRank value, and propagating the updated PageRank
    value (divided by the number of outbound edges) to the adjacent vertices, as shown
    in the following listing.^([[12](#ch07fn12)])
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 你的下一步是编写Giraph顶点类。Giraph模型的好处在于它的简单性——它提供了一个基于顶点的API，其中你需要实现该顶点上单次迭代的图处理逻辑。顶点类负责处理来自相邻顶点的传入消息，使用它们来计算节点的新PageRank值，并将更新的PageRank值（除以出度边数）传播到相邻顶点，如下面的列表所示.^([[12](#ch07fn12)])
- en: '^(12) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/pagerank/giraph/PageRankVertex.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/pagerank/giraph/PageRankVertex.java).'
  id: totrans-661
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(12) GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/pagerank/giraph/PageRankVertex.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/pagerank/giraph/PageRankVertex.java)。
- en: Listing 7.5\. The PageRank vertex
  id: totrans-662
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.5\. 页面排名顶点
- en: '![](324fig01_alt.jpg)'
  id: totrans-663
  prefs: []
  type: TYPE_IMG
  zh: '![](324fig01_alt.jpg)'
- en: '|  |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Installing Giraph
  id: totrans-665
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 安装Giraph
- en: Giraph is a Java library and is bundled with the code distribution for this
    book. Therefore, it doesn’t need to be installed for the examples in this technique
    to work. The Giraph website at [http://giraph.apache.org/](http://giraph.apache.org/)
    contains download installation for releases if you wish to play with Giraph further.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: Giraph是一个Java库，并包含在这本书的代码分发中。因此，对于这项技术中的示例，不需要安装Giraph。如果你想要进一步探索Giraph，Giraph网站[http://giraph.apache.org/](http://giraph.apache.org/)提供了下载和安装说明。
- en: '|  |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'If you push the web graph into HDFS and run your job, it will run for five
    iterations until the graph converges:'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将网页图推送到HDFS并运行你的作业，它将运行五次迭代，直到图收敛：
- en: '![](325fig01_alt.jpg)'
  id: totrans-669
  prefs: []
  type: TYPE_IMG
  zh: '![](325fig01_alt.jpg)'
- en: 'After the process has completed, you can look at the output in HDFS to see
    the Page-Rank values for each vertex:'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 处理完成后，你可以在HDFS中查看输出，以查看每个顶点的Page-Rank值：
- en: '[PRE37]'
  id: totrans-671
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: According to the output, node C (vertex 2) has the highest PageRank, followed
    by node B (vertex 1). Initially, this observation may be surprising, given that
    B has three inbound links and C has just two. But if you look at who’s linking
    to C, you can see that node B, which also has a high PageRank value, only has
    one outbound link to C, so node C gets B’s entire PageRank score in addition to
    its other inbound PageRank score from node D. Therefore, node C’s PageRank will
    always be higher than B’s.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 根据输出，节点C（顶点2）具有最高的PageRank，其次是节点B（顶点1）。考虑到B有三个入链，而C只有两个，这种观察可能令人惊讶。但如果你看看谁链接到C，你可以看到节点B，它也具有很高的PageRank值，只有一个出链指向C，因此节点C除了从节点D获得的其它入链PageRank分数外，还获得了B的全部PageRank分数。因此，节点C的PageRank将始终高于B的。
- en: Summary
  id: totrans-673
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: When you compare the code you had to write for the MapReduce compared to the
    code for Giraph, it’s clear that Giraph provides a simple and abstracted model
    that richly expresses graph concepts. Giraph’s efficiency over that of MapReduce
    results in Giraph being a compelling solution for your graph-processing needs.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将不得不为MapReduce编写的代码与Giraph的代码进行比较时，很明显，Giraph提供了一个简单且抽象的模型，该模型丰富地表达了图的概念。Giraph相对于MapReduce的效率使得Giraph成为满足你的图处理需求的一个有吸引力的解决方案。
- en: 'Giraph’s ability to scale to large graphs is highlighted by a Facebook article
    discussing how Facebook used Giraph to process a graph with a trillion edges.^([[13](#ch07fn13)])
    There are other graph technologies that you can evaluate for your needs:'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: Giraph扩展到大型图的能力在Facebook的一篇文章中被强调，该文章讨论了Facebook如何使用Giraph处理一个拥有万亿条边的图。[^([13](#ch07fn13))]
    有其他图技术你可以根据你的需求进行评估：
- en: ^(13) Avery Ching, “Scaling Apache Giraph to a trillion edges,” [https://www.facebook.com/notes/facebook-engineering/scaling-apache-giraph-to-a-trillion-edges/10151617006153920](https://www.facebook.com/notes/facebook-engineering/scaling-apache-giraph-to-a-trillion-edges/10151617006153920).
  id: totrans-676
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([13]) Avery Ching，“扩展Apache Giraph到万亿条边”，[[https://www.facebook.com/notes/facebook-engineering/scaling-apache-giraph-to-a-trillion-edges/10151617006153920](https://www.facebook.com/notes/facebook-engineering/scaling-apache-giraph-to-a-trillion-edges/10151617006153920)]。
- en: Faunus is a Hadoop-based open source project that supports HDFS and other data
    sources ([http://thinkaurelius.github.io/faunus/](http://thinkaurelius.github.io/faunus/)).
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faunus是一个基于Hadoop的开源项目，支持HDFS和其他数据源。[[http://thinkaurelius.github.io/faunus/](http://thinkaurelius.github.io/faunus/)]]
- en: GraphX is an in-memory Spark-based project. GraphX is currently not supported
    by any of the commercial Hadoop vendors, although it will soon be included in
    Cloudera CDH 5.1 ([https://amplab.cs.berkeley.edu/publication/graphx-grades/](https://amplab.cs.berkeley.edu/publication/graphx-grades/)).
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GraphX是一个基于内存的Spark项目。目前，GraphX不受任何商业Hadoop供应商的支持，尽管它将很快被包含在Cloudera CDH 5.1中。[[https://amplab.cs.berkeley.edu/publication/graphx-grades/](https://amplab.cs.berkeley.edu/publication/graphx-grades/)]]
- en: GraphLab is a C++-based, distributed, graph-processing framework out of Carnegie
    Mellon University ([http://graphlab.com/](http://graphlab.com/)).
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GraphLab是卡内基梅隆大学开发的一个基于C++的、分布式的图处理框架。[[http://graphlab.com/](http://graphlab.com/)]。
- en: Although you implemented the PageRank formula, it was made simple by the fact
    that your graph was well connected and that every node had outbound links. Pages
    with no outbound links are called *dangling pages*, and they pose a problem for
    the PageRank algorithm because they become *PageRank sinks*—their PageRank values
    can’t be further propagated through the graph. This, in turn, causes convergence
    problems because graphs that aren’t strongly connected aren’t guaranteed to converge.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管你实现了PageRank公式，但由于你的图是高度连接的，并且每个节点都有出站链接，所以这使它变得简单。没有出站链接的页面被称为*悬空页面*，它们对PageRank算法构成了问题，因为它们成为*PageRank陷阱*——它们的PageRank值不能通过图进一步传播。这反过来又导致收敛问题，因为不是强连通的图不能保证收敛。
- en: There are various approaches to solving this problem. You could remove the dangling
    nodes before your PageRank iterations and then add them back for a final Page-Rank
    iteration after the graph has converged. Or you could sum together the PageRank
    totals for all dangling pages and redistribute them across all the nodes in the
    graph. For a detailed examination of dealing with dangling pages as well as advanced
    PageRank practices, see *Google’s PageRank and Beyond* by Amy N. Langville and
    Carl Dean Meyer (Princeton University Press, 2012).
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题有各种方法。你可以在你的PageRank迭代之前移除悬空节点，然后在图收敛后添加它们以进行最终的Page-Rank迭代。或者，你可以将所有悬空页面的PageRank总和相加，并将它们重新分配到图中的所有节点。有关处理悬空节点以及高级PageRank实践的详细考察，请参阅Amy
    N. Langville和Carl Dean Meyer所著的《Google的PageRank及其超越》（普林斯顿大学出版社，2012年）。
- en: This concludes the section on graphs. As you learned, graphs are useful mechanisms
    for representing people in a social network and pages in a web. You used these
    models to discover some useful information about your data, such as finding the
    shortest path between two points and what web pages are more popular than others.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分关于图的讨论到此结束。正如你所学到的，图是表示社交网络中的人和组织网络中页面的有用机制。你使用这些模型来发现一些关于你的数据的有用信息，例如找到两点之间的最短路径以及哪些网页比其他网页更受欢迎。
- en: This brings us to the subject of the next section, Bloom filters. Bloom filters
    are a different kind of data structure from graphs. Whereas graphs are used to
    represent entities and their relationships, Bloom filters are a mechanism for
    modeling sets and performing membership queries on their data, as you’ll discover
    next.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了下一节的主题，Bloom过滤器。Bloom过滤器是一种不同于图的数据结构。虽然图用于表示实体及其关系，但Bloom过滤器是一种用于建模集合并在其数据上执行成员查询的机制，正如你接下来会发现的那样。
- en: 7.2\. Bloom filters
  id: totrans-684
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2. Bloom过滤器
- en: 'A Bloom filter is a data structure that offers a membership query mechanism
    where the answer to a lookup is one of two values: a definitive *no*, meaning
    that the item being looked up doesn’t exist in the Bloom filter, or a *maybe*,
    meaning that there’s a probability that the item exists. Bloom filters are popular
    due to their space efficiencies—representing the existence of *N* elements requires
    much less space than *N* positions in the data structure, which is why the membership
    query can yield false positive results. The amount of false positives in a Bloom
    filter can be tuned, which we’ll discuss shortly.'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 布隆过滤器是一种数据结构，它提供了一个成员查询机制，其中查找的答案有两个值之一：一个确定的*否*，意味着正在查找的项目不在布隆过滤器中，或者一个*可能*，意味着该项目存在一定的概率。布隆过滤器因其空间效率高而受到欢迎——表示*N*个元素的存在所需的空間比数据结构中的*N*个位置要少得多，这就是为什么成员查询可能会产生假阳性结果。布隆过滤器中的假阳性数量可以调整，我们将在稍后讨论。
- en: Bloom filters are used in BigTable and HBase to remove the need to read blocks
    from disk to determine if they contain a key. They’re also used in distributed
    network applications such as Squid to share cache details between multiple instances
    without having to replicate the whole cache or incur a network I/O hit in the
    case of cache misses.
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 布隆过滤器在BigTable和HBase中使用，以消除从磁盘读取块以确定它们是否包含键的需求。它们还用于分布式网络应用程序，如Squid，以在多个实例之间共享缓存细节，而无需复制整个缓存或在缓存未命中时产生网络I/O开销。
- en: The implementation of Bloom filters is simple. They use a bit array of size
    *m* bits, where initially each bit is set to `0`. They also contain *k* hash functions,
    which are used to map elements to *k* locations in the bit array.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 布隆过滤器的实现很简单。它们使用一个大小为*m*位的位阵列，其中最初每个位都设置为`0`。它们还包含*k*个哈希函数，这些函数用于将元素映射到位阵列中的*k*个位置。
- en: To add an element to a Bloom filter, it’s hashed *k* times, and a modulo of
    the hashed value and the size of the bit array is used to map the hashed value
    to a specific bit array location. That bit in the bit array is then toggled to
    `1`. [Figure 7.24](#ch07fig24) shows three elements being added to a Bloom filter
    and their locations in the bit array.
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 要向布隆过滤器添加一个元素，它被哈希*k*次，然后使用哈希值的模和位阵列的大小来将哈希值映射到特定的位阵列位置。然后，位阵列中的该位被切换到`1`。[图7.24](#ch07fig24)
    展示了三个元素被添加到布隆过滤器及其在位阵列中的位置。
- en: Figure 7.24\. Adding elements to a Bloom filter
  id: totrans-689
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.24。向布隆过滤器添加元素
- en: '![](07fig24.jpg)'
  id: totrans-690
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig24.jpg)'
- en: To check the membership of an element in the Bloom filter, just like with the
    add operation, the element is hashed *k* times, and each hash key is used to index
    into the bit array. A `true` response to the membership query is only returned
    in cases where all *k* bit array locations are set to `1`. Otherwise, the response
    to the query is `false`.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查布隆过滤器中一个元素的成员资格，就像添加操作一样，该元素被哈希*k*次，每个哈希键都用于索引位阵列。只有当所有*k*个位阵列位置都设置为`1`时，才会返回成员查询的`true`响应。否则，查询的响应是`false`。
- en: '[Figure 7.25](#ch07fig25) shows an example of a membership query where the
    item was previously added to the Bloom filter, and therefore all the bit array
    locations contained a `1`. This is an example of a true positive membership result.'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.25](#ch07fig25) 展示了一个成员查询的例子，其中项目之前已被添加到布隆过滤器中，因此所有位阵列位置都包含一个`1`。这是一个真正的阳性成员查询结果的例子。'
- en: Figure 7.25\. An example of a Bloom filter membership query that yields a true
    positive result
  id: totrans-693
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.25。一个布隆过滤器成员查询产生真正阳性结果的例子
- en: '![](07fig25_alt.jpg)'
  id: totrans-694
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig25_alt.jpg)'
- en: '[Figure 7.26](#ch07fig26) shows how you can get a false positive result for
    a membership query. The element being queried is *d*, which hadn’t been added
    to the Bloom filter. As it happens, all *k* hashes for *d* are mapped to locations
    that are set to `1` by other elements. This is an example of collision in the
    Bloom filter, where the result is a false positive.'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.26](#ch07fig26) 展示了如何得到一个成员查询的假阳性结果。正在查询的元素是*d*，它尚未被添加到布隆过滤器中。碰巧的是，*d*的所有*k*个哈希都映射到由其他元素设置的`1`的位置。这是布隆过滤器中碰撞的例子，其结果是假阳性。'
- en: Figure 7.26\. An example of a Bloom filter membership query that yields a false
    positive result
  id: totrans-696
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.26。一个布隆过滤器成员查询产生假阳性结果的例子
- en: '![](07fig26_alt.jpg)'
  id: totrans-697
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig26_alt.jpg)'
- en: 'The probability of false positives can be tuned based on two factors: *m*,
    the number of bits in the bit array, and *k*, the number of hash functions. Or
    expressed another way, if you have a desired false positive rate in mind and you
    know how many elements will be added to the Bloom filter, you can calculate the
    number of bits needed in the bit array with the equation in [figure 7.27](#ch07fig27).'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 误报的概率可以根据两个因素进行调整：*m*，位数组中的位数，和*k*，哈希函数的数量。或者用另一种方式表达，如果你有一个期望的误报率，并且你知道将要添加到布隆过滤器中的元素数量，你可以使用图7.27中的公式来计算位数组中所需的位数。
- en: Figure 7.27\. Equation to calculate the desired number of bits for a Bloom filter
  id: totrans-699
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.27\. 计算布隆过滤器所需位数数的公式
- en: '![](07fig27_alt.jpg)'
  id: totrans-700
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig27_alt.jpg)'
- en: The equation shown in [figure 7.28](#ch07fig28) assumes an optimal number of
    *k* hashes and that the hashes being produced are random over the range *{1..m}*.
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.28中显示的公式假设最优的哈希数*k*和生成的哈希值在范围*{1..m}*上是随机的。
- en: Figure 7.28\. Equation to calculate the optimal number of hashes
  id: totrans-702
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.28\. 计算最优哈希数目的公式
- en: '![](07fig28.jpg)'
  id: totrans-703
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig28.jpg)'
- en: Put another way, if you want to add 1 million elements into a Bloom filter with
    a 1% false positive rate for your membership queries, you’ll need 9,585,058 bits
    or 1.2 megabytes with seven hash functions. This is around 9.6 bits for each element.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果你想在布隆过滤器中添加100万个元素，并且你的成员查询的误报率为1%，你需要95,850,588位或1.2兆字节，使用七个哈希函数。这大约是每个元素9.6位。
- en: '[Table 7.1](#ch07table01) shows the calculated number of bits per element for
    various false positive rates.'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: '[表7.1](#ch07table01)显示了不同误报率下每个元素所需的位数计算结果。'
- en: Table 7.1\. Number of bits required per element for different false positive
    rates
  id: totrans-706
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.1\. 不同误报率下每个元素所需的位数
- en: '| False positives | Bits required per element |'
  id: totrans-707
  prefs: []
  type: TYPE_TB
  zh: '| 误报 | 每个元素所需的位数 |'
- en: '| --- | --- |'
  id: totrans-708
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 2% | 8.14 |'
  id: totrans-709
  prefs: []
  type: TYPE_TB
  zh: '| 2% | 8.14 |'
- en: '| 1% | 9.58 |'
  id: totrans-710
  prefs: []
  type: TYPE_TB
  zh: '| 1% | 9.58 |'
- en: '| 0.1% | 14.38 |'
  id: totrans-711
  prefs: []
  type: TYPE_TB
  zh: '| 0.1% | 14.38 |'
- en: With all that theory in your head, you now need to turn your attention to the
    subject of how Bloom filters can be utilized in MapReduce.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 在脑海中装满了所有这些理论之后，你现在需要将注意力转向如何利用布隆过滤器在MapReduce中应用的主题。
- en: Technique 70 Parallelized Bloom filter creation in MapReduce
  id: totrans-713
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧70：在MapReduce中并行创建布隆过滤器
- en: MapReduce is good for processing large amounts of data in parallel, so it’s
    a good fit if you want to create a Bloom filter based on a large set of input
    data. For example, let’s say you’re a large, internet, social-media organization
    with hundreds of millions of users, and you want to create a Bloom filter for
    a subset of users that are within a certain age demographic. How would you do
    this in MapReduce?
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce非常适合并行处理大量数据，因此如果你想要基于大量输入数据创建布隆过滤器，它是一个很好的选择。例如，假设你是一家大型互联网社交媒体组织，拥有数亿用户，并且你想要为一定年龄段的用户子集创建一个布隆过滤器。你如何在MapReduce中做到这一点？
- en: Problem
  id: totrans-715
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to create a Bloom filter in MapReduce.
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在MapReduce中创建一个布隆过滤器。
- en: Solution
  id: totrans-717
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Write a MapReduce job to create and output a Bloom filter using Hadoop’s built-in
    `BloomFilter` class. The mappers are responsible for creating intermediary Bloom
    filters, and the single reducer combines them together to output a combined Bloom
    filter.
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个MapReduce作业，使用Hadoop内置的`BloomFilter`类创建并输出布隆过滤器。mapper负责创建中间布隆过滤器，单个reducer将它们合并在一起以输出合并后的布隆过滤器。
- en: Discussion
  id: totrans-719
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: '[Figure 7.29](#ch07fig29) shows what this technique will do. You’ll write a
    mapper, which will process user data and create a Bloom filter containing users
    in a certain age bracket. The mappers will emit their Bloom filters, and a single
    reducer will combine them together. The final result is a single Bloom filter
    stored in HDFS in Avro form.'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.29](#ch07fig29)展示了这项技术将做什么。你将编写一个mapper，它将处理用户数据并创建包含一定年龄段用户的布隆过滤器。mapper将输出它们的布隆过滤器，而单个reducer将它们合并在一起。最终结果是存储在HDFS中的单个布隆过滤器，格式为Avro。'
- en: Figure 7.29\. A MapReduce job to create a Bloom filter
  id: totrans-721
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.29\. 创建布隆过滤器的MapReduce作业
- en: '![](07fig29_alt.jpg)'
  id: totrans-722
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig29_alt.jpg)'
- en: Hadoop comes bundled with an implementation of a Bloom filter in the form of
    the `org.apache.hadoop.util.bloom.BloomFilter` class, illustrated in [figure 7.30](#ch07fig30).
    Luckily, it’s a `Writable`, which makes it easy to ship around in MapReduce. The
    `Key` class is used to represent an element, and it is also a `Writable` container
    for a byte array.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 随带了一个 Bloom 过滤器的实现，形式为 `org.apache.hadoop.util.bloom.BloomFilter` 类，如图
    7.30 所示。[figure 7.30](#ch07fig30)。幸运的是，它是一个 `Writable`，这使得它在 MapReduce 中很容易传输。`Key`
    类用于表示一个元素，它也是一个用于字节数组的 `Writable` 容器。
- en: Figure 7.30\. The BloomFilter class in MapReduce
  id: totrans-724
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.30\. MapReduce 中的 BloomFilter 类
- en: '![](07fig30_alt.jpg)'
  id: totrans-725
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig30_alt.jpg)'
- en: 'The constructor requires that you tell it what hashing function to use. There
    are two implementations you can choose from: Jenkins and Murmur. They’re both
    faster than cryptographic hashers such as SHA-1 and they produce good distributions.
    Benchmarks indicate that Murmur has faster hashing times than Jenkins, so that’s
    what we’ll use here.'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数要求你指定要使用的哈希函数。你可以选择两种实现：Jenkins 和 Murmur。它们都比 SHA-1 这样的加密哈希器更快，并且产生良好的分布。基准测试表明
    Murmur 的哈希时间比 Jenkins 快，所以我们在这里使用 Murmur。
- en: Let’s press on with the code. Your map function will operate on your user information,
    which is a simple key/value pair, where the key is the user name, and the value
    is the user’s age:^([[14](#ch07fn14)])
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续代码。你的 map 函数将操作你的用户信息，这是一个简单的键/值对，其中键是用户名，值是用户的年龄:^([[14](#ch07fn14)])
- en: '^(14) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/bloom/BloomFilterCreator.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/bloom/BloomFilterCreator.java).'
  id: totrans-728
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (14) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/bloom/BloomFilterCreator.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/bloom/BloomFilterCreator.java).
- en: '![](330fig01_alt.jpg)'
  id: totrans-729
  prefs: []
  type: TYPE_IMG
  zh: '![](330fig01_alt.jpg)'
- en: Why do you output the Bloom filter in the `close` method, and not output it
    for every record you process in the map method? You do this to cut down on the
    amount of traffic between the map and reduce phases; there’s no reason to output
    a lot of data if you can pseudo-combine them yourself on the map side and emit
    a single `BloomFilter` per map.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么你在 `close` 方法中输出 Bloom 过滤器，而不是在 `map` 方法处理每个记录时都输出？你这样做是为了减少 map 和 reduce
    阶段之间的流量；如果你可以在 map 端自己伪合并它们，并每 map 输出一个单独的 `BloomFilter`，就没有必要输出大量数据。
- en: Your reducer’s job is to combine all the Bloom filters outputted by the mappers
    into a single Bloom filter. The unions are performed with the bitwise `OR` method
    exposed by the `BloomFilter` class. When performing a union, all the `BloomFilter`
    attributes, such as bit array size and number of hashes, must be identical:^([[15](#ch07fn15)])
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 你 reducer 的任务是合并所有 mapper 输出的 Bloom 过滤器到一个单独的 Bloom 过滤器。这些合并是通过 `BloomFilter`
    类公开的位运算 `OR` 方法来执行的。在执行合并时，所有 `BloomFilter` 属性，如位数组大小和哈希数量，必须相同:^([[15](#ch07fn15)])
- en: '^(15) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/bloom/BloomFilterCreator.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/bloom/BloomFilterCreator.java).'
  id: totrans-732
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (15) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/bloom/BloomFilterCreator.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/bloom/BloomFilterCreator.java).
- en: '![](331fig01_alt.jpg)'
  id: totrans-733
  prefs: []
  type: TYPE_IMG
  zh: '![](331fig01_alt.jpg)'
- en: 'To try this out, upload your sample user file and kick off your job. When the
    job is complete, dump the contents of the Avro file to view the contents of your
    `BloomFilter`:'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 要尝试这个，上传你的样本用户文件并启动你的作业。当作业完成时，将 Avro 文件的内容导出以查看你的 `BloomFilter` 的内容：
- en: '[PRE38]'
  id: totrans-735
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The `BloomFilterDumper` code unmarshals the `BloomFilter` from the Avro file
    and calls the `toString()` method, which in turn calls the `BitSet.toString()`
    method, which outputs the offset for each bit that is “on.”
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: '`BloomFilterDumper` 代码从 Avro 文件中反序列化 `BloomFilter` 并调用 `toString()` 方法，该方法反过来调用
    `BitSet.toString()` 方法，该方法输出每个“开启”位的偏移量。'
- en: Summary
  id: totrans-737
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: You used Avro as a serialization format for the Bloom filter. You could have
    just as easily emitted the `BloomFilter` object in your reducer, because it’s
    a `Writable`.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用了 Avro 作为 Bloom 过滤器的序列化格式。你同样可以在你的 reducer 中输出 `BloomFilter` 对象，因为它是一个 `Writable`。
- en: You used a single reducer in this technique, which will scale well to jobs that
    use thousands of map tasks and `BloomFilter`s whose bit array sizes are in the
    millions. If the time taken to execute the single reducer becomes too long, you
    can run with multiple reducers to parallelize the Bloom filter unions, and have
    a postprocessing step to combine them further into a single Bloom filter.
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技术中，你使用了单个reducer，这可以很好地扩展到使用数千个映射任务和位数组大小在百万级的`BloomFilter`的工作。如果执行单个reducer所需的时间变得过长，你可以运行多个reducer来并行化Bloom过滤器联合，并在后处理步骤中将它们进一步合并成一个单一的Bloom过滤器。
- en: Another distributed method for creating a Bloom filter would be to view the
    set of reducers as the overall bit array, and perform the hashing and output the
    hashes in the map phase. The partitioner would then partition the output to the
    relevant reducer that manages that section of the bit array. [Figure 7.31](#ch07fig31)
    illustrates this approach.
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 创建Bloom过滤器的另一种分布式方法是将reducer集合视为整体位数组，并在映射阶段进行哈希并输出哈希值。分区器随后将输出分配给管理该部分位数组的相应reducer。[图7.31](#ch07fig31)展示了这种方法。
- en: Figure 7.31\. An alternate architecture for creating Bloom filters
  id: totrans-741
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.31. 创建Bloom过滤器的另一种架构
- en: '![](07fig31_alt.jpg)'
  id: totrans-742
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig31_alt.jpg)'
- en: For code comprehensibility, you hardcoded the `BloomFilter` parameters in this
    technique; in reality, you’ll want to either calculate them dynamically or move
    them into a configuration file.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 为了代码的可读性，你在这个技术中硬编码了`BloomFilter`参数；实际上，你将希望动态计算它们或将它们移动到配置文件中。
- en: This technique resulted in the creation of a `BloomFilter`. This `BloomFilter`
    could be pulled out of HDFS and used in another system, or it could be used directly
    in Hadoop, as shown in technique 61, where a Bloom filter was used as a way to
    filter data emitted from reducers in joins.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术导致了`BloomFilter`的创建。这个`BloomFilter`可以从HDFS中提取出来用于另一个系统，或者可以直接在Hadoop中使用，如图61所示，其中Bloom过滤器被用作过滤连接中reducer输出的数据的方式。
- en: 7.3\. HyperLogLog
  id: totrans-745
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3. HyperLogLog
- en: Imagine that you’re building a web analytics system where one of the data points
    you’re calculating is the number of unique users that have visited a URL. Your
    problem domain is web-scale, so you have hundreds of millions of users. A naive
    Map-Reduce implementation of aggregation would involve using a hashtable to store
    and calculate the unique users, but this could exhaust your JVM heap when dealing
    with a large number of users. A more sophisticated solution would use a secondary
    sort so that user IDs are sorted, and the grouping occurs at the URL level so
    that you can count unique users without any storage overhead.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你正在构建一个网络分析系统，其中你计算的数据点之一是访问URL的唯一用户数量。你的问题域是网络规模的，因此你有数亿用户。一个简单的Map-Reduce聚合实现将涉及使用散列表来存储和计算唯一用户，但处理大量用户时这可能会耗尽你的JVM堆。一个更复杂的解决方案将使用二次排序，以便用户ID被排序，并且分组发生在URL级别，这样你就可以在不产生任何存储开销的情况下计算唯一用户。
- en: These solutions work well when you have the ability to process the entire dataset
    at once. But if you have a more complex aggregation system where you create aggregations
    in time buckets and you need to combine buckets together, then you’d need to store
    the entire set of unique users for each URL in each time bucket, which would explode
    your data storage needs.
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 当你能够一次性处理整个数据集时，这些解决方案工作得很好。但如果你有一个更复杂的聚合系统，你在时间桶中创建聚合，并且需要合并桶，那么你将需要存储每个时间桶中每个URL的整个唯一用户集合，这将爆炸性地增加你的数据存储需求。
- en: To combat this, you could use a probabilistic algorithm such as HyperLogLog,
    which has a significantly smaller memory footprint than a hashtable. The trade-off
    with these probabilistic data structures is accuracy, which you can tune. In some
    ways, HyperLogLog is similar to a Bloom filter, but the key difference is that
    HyperLogLog will estimate a count, whereas a Bloom filter only provides membership
    capabilities.
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，你可以使用一个概率算法，如HyperLogLog，它比散列表有显著更小的内存占用。与这些概率数据结构相关的权衡是准确性，你可以调整它。在某种程度上，HyperLogLog类似于Bloom过滤器，但关键区别在于HyperLogLog将估计一个计数，而Bloom过滤器只提供成员资格能力。
- en: In this section you’ll learn how HyperLogLog works and see how it can be used
    in MapReduce to efficiently calculate unique counts.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将了解HyperLogLog是如何工作的，并看到它如何在MapReduce中高效地计算唯一计数。
- en: 7.3.1\. A brief introduction to HyperLogLog
  id: totrans-750
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.1. HyperLogLog的简要介绍
- en: HyperLogLog was first introduced in a 2007 paper to “estimate the number of
    distinct elements of very large data ensembles.”^([[16](#ch07fn16)]) Potential
    applications include link-based spam detection on the web and data mining over
    large datasets.
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: HyperLogLog 首次在 2007 年的一篇论文中提出，用于“估计非常大的数据集合中不同元素的数量”。^([16](#ch07fn16)) 潜在的应用包括基于链接的网页垃圾邮件检测和大型数据集的数据挖掘。
- en: '^(16) Philippe Flajolet et al., “HyperLogLog: the analysis of a near-optimal
    cardinality estimation algorithm,” [http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf](http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf).'
  id: totrans-752
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([16](#ch07fn16)) Philippe Flajolet 等人，“HyperLogLog：近最优基数估计算法的分析”，[http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf](http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf)。
- en: HyperLogLog is a probabilistic cardinality estimator—it relaxes the constraint
    of exactly calculating the number of elements in a set and instead estimates the
    number of elements. Data structures that support exact set-cardinality calculations
    require storage that is proportional to the number of elements, which may not
    be optimal when working with large datasets. Probabilistic cardinality structures
    occupy less memory than their exact cardinality counterparts, and they’re applicable
    in situations where the cardinality can be off by a few percentage points.
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: HyperLogLog 是一种概率性基数估计器——它放宽了精确计算集合中元素数量的约束，而是估计元素的数量。支持精确集合基数计算的数结构需要与元素数量成比例的存储空间，这在处理大数据集时可能不是最优的。概率性基数结构比它们的精确基数对应物占用更少的内存，并且适用于基数可能偏差几个百分点的场景。
- en: HyperLogLog can perform cardinality estimation for counts beyond 10⁹ using 1.5
    KB of memory with an error rate of 2%. HyperLogLog works by counting the maximum
    number of consecutive zeros in a hash and using probabilities to predict the cardinality
    of all the unique items. [Figure 7.32](#ch07fig32) shows how a hashed value is
    represented in HyperLogLog. For additional details, refer to the HyperLogLog paper.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: HyperLogLog 可以使用 1.5 KB 的内存对超过 10⁹ 的计数进行基数估计，误差率为 2%。HyperLogLog 通过计算哈希中的最大连续零位数并使用概率来预测所有唯一项的基数来工作。[图
    7.32](#ch07fig32) 展示了哈希值在 HyperLogLog 中的表示。有关更多详细信息，请参阅 HyperLogLog 论文。
- en: Figure 7.32\. How HyperLogLog works
  id: totrans-755
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.32\. HyperLogLog 的工作原理
- en: '![](07fig32.jpg)'
  id: totrans-756
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig32.jpg)'
- en: 'There are two parameters you’ll need to tune when working with HyperLogLog:'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 HyperLogLog 时，你需要调整两个参数：
- en: The number of buckets, usually expressed by a number, *b*, which is then used
    to determine the number of buckets by calculating 2*^b*. Therefore, each increment
    in *b* doubles the number of buckets. The lower bound on *b* is 4, and the upper
    bound varies by implementation.
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 桶的数量，通常用数字 *b* 表示，然后通过计算 2^b* 来确定桶的数量。因此，*b* 的每次增加都会使桶的数量翻倍。*b* 的下限是 4，上限因实现而异。
- en: The number of bits used to represent the maximum number of consecutive zeros
    in a bucket.
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于表示桶中最大连续零位的位数。
- en: As a result, the size of the HyperLogLog is calculated by 2*^b* * bits-per-bucket.
    In typical usage, *b* is 11 and the number of bits per bucket is 5, which results
    in 10,240 bits, or 1.25 KB.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，HyperLogLog 的大小通过 2^b* 每桶位来计算。在典型使用中，*b* 是 11，每桶的位数是 5，这导致 10,240 位，或 1.25
    KB。
- en: Technique 71 Using HyperLogLog to calculate unique counts
  id: totrans-761
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 71 使用 HyperLogLog 计算唯一计数
- en: In this technique you’ll see a simple example of HyperLogLog in action. The
    summary will present some details on how HyperLogLog can be incorporated into
    your MapReduce flows.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技巧中，你将看到一个 HyperLogLog 的简单示例。总结将展示如何将 HyperLogLog 集成到你的 MapReduce 流中的一些细节。
- en: Problem
  id: totrans-763
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You’re working with a large dataset and you want to calculate distinct counts.
    You are willing to accept a small percentage of error.
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在处理一个大型数据集，并且你想计算唯一计数。你愿意接受一小部分的误差。
- en: Solution
  id: totrans-765
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use HyperLogLog.
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 HyperLogLog。
- en: Discussion
  id: totrans-767
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: For this technique you’ll use a HyperLogLog Java implementation from a GitHub
    project called java-hll ([https://github.com/aggregateknowledge/java-hll](https://github.com/aggregateknowledge/java-hll)).
    This code provides the basic HyperLogLog functions, in addition to useful functions
    that allow you to perform a union and intersect multiple logs together.
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个技巧，你将使用来自 GitHub 项目 java-hll 的 HyperLogLog Java 实现（[https://github.com/aggregateknowledge/java-hll](https://github.com/aggregateknowledge/java-hll)）。此代码提供了基本的
    HyperLogLog 函数，以及允许你执行多个日志的并集和交集的有用函数。
- en: The following example shows a simple case where your data consists of an array
    of numbers, and Google’s Guava library is used to create a hash for each number
    and add it to the HyperLogLog:^([[17](#ch07fn17)])
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了这样一个简单情况，您的数据由一个数字数组组成，并使用 Google 的 Guava 库为每个数字创建哈希并将其添加到 HyperLogLog
    中：^([[17](#ch07fn17)])。
- en: '^(17) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/hyperloglog/Example.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/hyperloglog/Example.java).'
  id: totrans-770
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([17](#ch07fn17)) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/hyperloglog/Example.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch7/hyperloglog/Example.java)。
- en: '![](335fig01_alt.jpg)'
  id: totrans-771
  prefs: []
  type: TYPE_IMG
  zh: '![](335fig01_alt.jpg)'
- en: 'Running this example yields the expected number of distinct items:'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此示例会得到预期的唯一项目数量：
- en: '[PRE39]'
  id: totrans-773
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This code can be easily adapted into a Hadoop job to perform distinct counts
    over large datasets. For example, imagine that you’re writing a MapReduce job
    to calculate the distinct number of users that visit each page on your website.
    In MapReduce, your mappers would output a URL and user ID as the key and value
    respectively, and your reducers would need to calculate the unique set of users
    for each web page. In this situation, you can use a HyperLogLog structure to efficiently
    calculate an approximate distinct count of users without the overhead that would
    be incurred by using a hash set.
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码可以轻松地改编成一个 Hadoop 作业，以对大型数据集执行不同的计数。例如，想象您正在编写一个 MapReduce 作业来计算访问您网站上每个页面的不同用户数量。在
    MapReduce 中，您的映射器会输出 URL 和用户 ID 作为键和值，而您的归约器需要计算每个网页的唯一用户集。在这种情况下，您可以使用 HyperLogLog
    结构来高效地计算用户的大致唯一计数，而无需使用哈希集带来的开销。
- en: Summary
  id: totrans-775
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: The `hll` HyperLogLog implementation used in this example has a `toBytes` method
    that you can use to serialize the HyperLogLog, and it also has a `fromBytes` method
    for deserialization. This makes it relatively straightforward to use within a
    MapReduce flow and for persistence. Avro, for example, has a `bytes` field that
    you can use to write the `hll` byte form into your records. You could also write
    your own `Writable` if you’re working with SequenceFiles.
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中使用的 `hll` HyperLogLog 实现有一个 `toBytes` 方法，您可以使用它来序列化 HyperLogLog，同时它还有一个
    `fromBytes` 方法用于反序列化。这使得它在 MapReduce 流程和持久化中使用起来相对简单。例如，Avro 有一个 `bytes` 字段，您可以将
    `hll` 字节数据写入您的记录中。如果您在使用 SequenceFiles，也可以编写自己的 `Writable`。
- en: If you’re using Scalding or Summingbird, then Algebird has a HyperLogLog implementation
    that you can use—take a look at [https://github.com/twitter/algebird](https://github.com/twitter/algebird)
    for more details.
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用 Scalding 或 Summingbird，那么 Algebird 提供了一个 HyperLogLog 实现供您使用——更多详情请参阅 [https://github.com/twitter/algebird](https://github.com/twitter/algebird)。
- en: 7.4\. Chapter summary
  id: totrans-778
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4\. 章节总结
- en: Most of the algorithms laid out in this chapter are straightforward. What makes
    things interesting is how they’re applied in MapReduce in ways that enable you
    to work efficiently with large datasets.
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中概述的大多数算法都很直接。使事情变得有趣的是，它们如何在 MapReduce 中应用，从而能够高效地处理大型数据集。
- en: The two main data structures presented were graphs—good for modeling relationships—and
    Bloom filters, which excel at compact set membership. In the case of graphs, we
    looked at how you would use them to model social networks and web graphs, and
    we went through some algorithms such as FoF and PageRank to mine some interesting
    facts about your data.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 本章仅对如何建模和处理数据进行了初步探讨。有关排序和连接的算法将在其他章节中介绍。下一章将介绍诊断和调整 Hadoop 的技术，以从您的集群中榨取尽可能多的性能。
- en: In the case of Bloom filters, we looked at how to use MapReduce to create a
    Bloom filter in parallel, and then apply that Bloom filter to optimize a semi-join
    operation in Map-Reduce.
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Bloom 过滤器的例子中，我们探讨了如何使用 MapReduce 并行创建一个 Bloom 过滤器，然后将其应用于优化 Map-Reduce 中的半连接操作。
- en: We’ve only scratched the surface in this chapter regarding how data can be modeled
    and processed. Algorithms related to sorting and joins are covered in other chapters.
    The next chapter covers techniques to diagnose and tune Hadoop to squeeze as much
    performance as you can out of your clusters.
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中只是触及了数据建模和处理的表面。有关排序和连接的算法将在其他章节中介绍。下一章将介绍诊断和调整 Hadoop 的技术，以从您的集群中榨取尽可能多的性能。
- en: Chapter 8\. Tuning, debugging, and testing
  id: totrans-783
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 8 章\. 调优、调试和测试
- en: '*This chapter covers*'
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Measuring and tuning MapReduce execution times
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量和调整 MapReduce 执行时间
- en: Debugging your applications
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调试您的应用程序
- en: Testing tips to improve the quality of your code
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高代码质量的测试技巧
- en: Imagine you’ve written a new piece of MapReduce code, and you’re executing it
    on your shiny new cluster. You’re surprised to learn that despite having a good-sized
    cluster, your job is running significantly longer than you expected. You’ve obviously
    hit a performance issue with your job, but how do you figure out where the problem
    lies?
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你已经编写了一块新的 MapReduce 代码，并在你那崭新的集群上执行它。你惊讶地发现，尽管拥有一个规模不小的集群，但你的作业运行时间比你预期的要长得多。显然，你的作业遇到了性能问题，但你是如何确定问题所在的呢？
- en: This chapter starts out by reviewing common performance problems in Map-Reduce,
    such as the lack of data locality and running with too many mappers. This tuning
    section also examines some enhancements that you can make to your jobs to increase
    their efficiency by using binary comparators in the shuffle phase and using a
    compact data format to minimize parsing and data transfer times.
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先回顾了 Map-Reduce 中常见的性能问题，例如缺乏数据局部性和使用过多映射器。本节调优部分还检查了一些你可以对作业进行的增强，通过在洗牌阶段使用二进制比较器和使用紧凑的数据格式来最小化解析和数据传输时间，从而提高作业的效率。
- en: The second part of this chapter covers some tips that will help you debug your
    applications, including instructions on how to access YARN container startup scripts,
    and some suggestions on how to design your MapReduce jobs to aid future debugging
    efforts.
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第二部分涵盖了帮助你调试应用程序的一些技巧，包括如何访问 YARN 容器启动脚本的操作说明，以及一些关于如何设计你的 MapReduce 作业以帮助未来调试工作的建议。
- en: The final section looks at how to provide adequate unit testing for MapReduce
    code and examines some defensive coding techniques you can use to minimize badly
    behaving code. All the preparation and testing in the world can’t guarantee that
    you won’t encounter any problems, and in case you do, we’ll look at how you can
    debug your job to figure out what went wrong.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个部分探讨了如何为 MapReduce 代码提供充分的单元测试，并检查了一些你可以使用的防御性编码技术，以最小化表现不佳的代码。无论准备和测试多么充分，都无法保证你不会遇到任何问题，如果确实遇到了问题，我们将探讨如何调试你的作业以找出出了什么问题。
- en: '|  |'
  id: totrans-792
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Hadoop 2
  id: totrans-793
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Hadoop 2
- en: The techniques in this chapter work with Hadoop 2\. Due to incompatibilities
    across major Hadoop versions, some of these techniques won’t work with earlier
    versions.
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的技术适用于 Hadoop 2。由于不同主要版本的 Hadoop 之间存在不兼容性，因此其中一些技术无法与早期版本兼容。
- en: '|  |'
  id: totrans-795
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 8.1\. Measure, measure, measure
  id: totrans-796
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1. 测量，测量，再测量
- en: Before you can start performance tuning, you need to have the tools and processes
    in place to capture system metrics. These tools will help you gather and examine
    empirical data related to your application and determine whether or not you’re
    suffering from a performance problem.
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始性能调优之前，你需要有工具和流程来捕获系统指标。这些工具将帮助你收集和检查与你的应用程序相关的经验数据，并确定你是否遇到了性能问题。
- en: In this section we’ll look at the tools and metrics that Hadoop provides, and
    we’ll also touch on monitoring as an additional tool in your performance-tuning
    toolkit.
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨 Hadoop 提供的工具和指标，同时也会涉及到监控作为性能调优工具包中的附加工具。
- en: It’s important to capture the CPU, memory, disk, and network utilization of
    your cluster. If possible, you should also capture MapReduce (or any other YARN
    application) statistics. Having historical and current metrics for your cluster
    will allow you to view anomalies in both hardware and software, and to correlate
    them against any other observations that may point to your work not proceeding
    at expected rates.
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要捕获集群的 CPU、内存、磁盘和网络利用率。如果可能的话，你也应该捕获 MapReduce（或任何其他 YARN 应用程序）的统计信息。拥有集群的历史和当前指标将允许你查看硬件和软件中的异常，并将它们与任何可能指向你的工作未按预期速率进行的观察结果相关联。
- en: Ultimately, the goal is to ensure that you aren’t over-utilizing or under-utilizing
    your hardware. If you’re over-utilizing your hardware, your systems are likely
    spending a considerable amount of time competing for resources, be it CPU context-switching
    or memory page-swapping. Under-utilization of your cluster means you’re not getting
    all that you can from your hardware.
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的目标是确保你不会过度使用或未充分利用你的硬件。如果你过度使用硬件，你的系统可能花费大量时间在争夺资源上，无论是 CPU 上下文切换还是内存页面交换。集群的未充分利用意味着你无法从硬件中获得全部潜力。
- en: Luckily there are a number of tools available to help you monitor your cluster,
    ranging from sar, the built-in Linux utility that collects and reports on system
    activity,^([[1](#ch08fn01)]) to more sophisticated tools such as Nagios and Ganglia.
    Nagios ([http://www.nagios.org/](http://www.nagios.org/)) and Ganglia ([http://ganglia.sourceforge.net/](http://ganglia.sourceforge.net/))
    are both open source projects designed to monitor your infrastructure, and Ganglia
    in particular provides a rich user interface with useful graphs, some of which
    can be seen in [figure 8.1](#ch08fig01). Ganglia has the added advantage of being
    able to pull statistics from Hadoop.^([[2](#ch08fn02)])
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有大量工具可供您监控集群，从收集和报告系统活动的内置 Linux 工具 sar，到更复杂的工具如 Nagios 和 Ganglia。Nagios
    ([http://www.nagios.org/](http://www.nagios.org/)) 和 Ganglia ([http://ganglia.sourceforge.net/](http://ganglia.sourceforge.net/))
    都是开源项目，旨在监控您的基础设施，特别是 Ganglia 提供了一个丰富的用户界面和有用的图表，其中一些可以在[图 8.1](#ch08fig01)中看到。Ganglia
    的额外优势在于能够从 Hadoop 中提取统计信息.^([[2](#ch08fn02)])
- en: '¹ This IBM article discusses using sar and gnuplot to generate system-activity
    graphs: David Tansley, “Using gnuplot to display data in your Web pages,” [http://www.ibm.com/developerworks/aix/library/au-gnuplot/index.html](http://www.ibm.com/developerworks/aix/library/au-gnuplot/index.html).'
  id: totrans-802
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹ 这篇 IBM 文章讨论了使用 sar 和 gnuplot 生成系统活动图：David Tansley，“使用 gnuplot 在您的网页中显示数据”，[http://www.ibm.com/developerworks/aix/library/au-gnuplot/index.html](http://www.ibm.com/developerworks/aix/library/au-gnuplot/index.html)。
- en: '² The Hadoop wiki has basic instructions on Ganglia and Hadoop integration:
    GangliaMetrics, [http://wiki.apache.org/hadoop/GangliaMetrics](http://wiki.apache.org/hadoop/GangliaMetrics).'
  id: totrans-803
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ² Hadoop 维基上有关于 Ganglia 和 Hadoop 集成的基本说明：GangliaMetrics，[http://wiki.apache.org/hadoop/GangliaMetrics](http://wiki.apache.org/hadoop/GangliaMetrics)。
- en: Figure 8.1\. Ganglia screenshots showing CPU utilization for multiple hosts
  id: totrans-804
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.1\. 显示多个主机 CPU 利用率的 Ganglia 截图
- en: '![](08fig01_alt.jpg)'
  id: totrans-805
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig01_alt.jpg)'
- en: If you’re using a commercial Hadoop distribution, it was likely bundled with
    management user interfaces that include monitoring. If you’re using the Apache
    Hadoop distribution, you should be using Apache Ambari, which simplifies provisioning,
    management, and monitoring of your cluster. Ambari uses Ganglia and Nagios behind
    the scenes.
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是商业 Hadoop 发行版，它可能捆绑了包含监控的管理用户界面。如果您使用的是 Apache Hadoop 发行版，您应该使用 Apache
    Ambari，它简化了集群的配置、管理和监控。Ambari 在幕后使用 Ganglia 和 Nagios。
- en: With your monitoring tools in place, it’s time to take a look at how to tune
    and optimize your MapReduce jobs.
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的监控工具就绪后，是时候看看如何调优和优化您的 MapReduce 作业了。
- en: 8.2\. Tuning MapReduce
  id: totrans-808
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2. 调优 MapReduce
- en: In this section we’ll cover common issues that impact the performance of MapReduce
    jobs and look at how you can address these issues. Along the way, I’ll also point
    out some best practices to help you optimize your jobs.
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍影响 MapReduce 作业性能的常见问题，并探讨如何解决这些问题。在此过程中，我还会指出一些最佳实践，以帮助您优化作业。
- en: We’ll start by looking at some of the more common issues that hamper Map-Reduce
    job performance.
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从查看一些阻碍 Map-Reduce 作业性能的更常见问题开始。
- en: 8.2.1\. Common inefficiencies in MapReduce jobs
  id: totrans-811
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.1\. MapReduce 作业中的常见低效
- en: Before we delve into the techniques, let’s take a high-level look at a MapReduce
    job and identify the various areas that can impact its performance. Take a look
    at [figure 8.2](#ch08fig02).
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究技巧之前，让我们从高层次上看看 MapReduce 作业，并确定可能影响其性能的各个区域。请参阅[图 8.2](#ch08fig02)。
- en: Figure 8.2\. Inefficiencies that can occur in various parts of a MapReduce job
  id: totrans-813
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.2\. MapReduce 作业中可能发生的各种低效情况
- en: '![](08fig02_alt.jpg)'
  id: totrans-814
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig02_alt.jpg)'
- en: The rest of this section on performance tuning covers the issues identified
    in [figure 8.2](#ch08fig02). But before we start tuning, we need to look at how
    you can easily get access to job statistics, which will help you identify areas
    in need of tuning.
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: 本节关于性能调优的其余部分涵盖了[图 8.2](#ch08fig02)中确定的问题。但在我们开始调优之前，我们需要看看您如何轻松地获取作业统计信息，这将帮助您确定需要调优的区域。
- en: Technique 72 Viewing job statistics
  id: totrans-816
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 72 查看作业统计信息
- en: The first port of call when evaluating the performance of a MapReduce job is
    the metrics that Hadoop measures for your job. In this technique you’ll learn
    how to access these metrics.
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 评估 MapReduce 作业性能的第一步是 Hadoop 为您的作业测量的指标。在本技巧中，您将学习如何访问这些指标。
- en: Problem
  id: totrans-818
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to access the metrics for a MapReduce job.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: 您想访问 MapReduce 作业的指标。
- en: Solution
  id: totrans-820
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the JobHistory UI, the Hadoop CLI, or a custom utility.
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: 使用作业历史记录 UI、Hadoop CLI 或自定义工具。
- en: Discussion
  id: totrans-822
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'MapReduce collects various system and job counters for each job and persists
    them in HDFS. You can extract these statistics in two different ways:'
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce为每个作业收集各种系统和作业计数器，并将它们持久化到HDFS中。你可以以两种不同的方式提取这些统计数据：
- en: Use the JobHistory UI.
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用作业历史界面。
- en: Use the Hadoop command-line interface (CLI) to view job and task counters and
    other metrics from the job history.
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Hadoop命令行界面（CLI）查看作业和任务计数器以及作业历史中的其他度量。
- en: '|  |'
  id: totrans-826
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Job-history retention
  id: totrans-827
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 作业历史保留
- en: The job history is kept around for one week by default. This can be altered
    by updating `mapreduce.jobhistory.max-age-ms`.
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，作业历史保留一周。这可以通过更新`mapreduce.jobhistory.max-age-ms`来更改。
- en: '|  |'
  id: totrans-829
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Let’s examine both of these tools, starting with the JobHistory UI.
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查这两个工具，从作业历史界面开始。
- en: JobHistory
  id: totrans-831
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 作业历史
- en: In Hadoop 2, JobHistory is a MapReduce-specific service that gathers metrics
    from completed MapReduce jobs and provides a user interface for viewing them.^([[3](#ch08fn03)])
    [Figure 8.3](#ch08fig03) shows how you can access the job statistics in the JobHistory
    UI.
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop 2中，作业历史是一个MapReduce特定的服务，它从完成的MapReduce作业中收集度量，并提供一个用户界面来查看它们。[图8.3](#ch08fig03)显示了如何在作业历史界面中访问作业统计信息。
- en: ³ [Chapter 2](kindle_split_011.html#ch02) contains details on how to access
    the JobHistory user interface.
  id: totrans-833
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³ [第2章](kindle_split_011.html#ch02) 包含了如何访问作业历史用户界面的详细信息。
- en: Figure 8.3\. Accessing job counters in the JobHistory UI
  id: totrans-834
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.3\. 在作业历史界面中访问作业计数器
- en: '![](08fig03_alt.jpg)'
  id: totrans-835
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig03_alt.jpg)'
- en: This screen shows you the aggregated metrics for map tasks, reduce tasks, and
    across all the tasks. In addition, each metric allows you to drill down into all
    the tasks that reported that metric. Within each metric-specific screen, you can
    sort by the metric values to quickly identify tasks that exhibit unusually high
    or low metric values.
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 此屏幕显示了映射任务、减少任务以及所有任务的聚合度量。此外，每个度量都允许你深入查看报告该度量的所有任务。在每个度量特定的屏幕中，你可以按度量值排序，以快速识别表现出异常高或低度量值的任务。
- en: '|  |'
  id: totrans-837
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Metrics improvements in Hadoop 2
  id: totrans-838
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Hadoop 2的度量改进
- en: Hadoop 2 improved the job metrics by adding CPU, memory, and garbage collection
    statistics, so you can get a good sense of the system utilization of each process.
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 2通过添加CPU、内存和垃圾收集统计信息来改进作业度量，因此你可以很好地了解每个进程的系统利用率。
- en: '|  |'
  id: totrans-840
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: All is not lost if you can’t access the JobHistory UI, as you can also access
    the data via the Hadoop CLI.
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你无法访问作业历史界面，也不要灰心，因为你可以通过Hadoop CLI访问数据。
- en: Accessing the job history with the CLI
  id: totrans-842
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用CLI访问作业历史
- en: 'The job history output is stored in the directory specified by the configurable
    `mapreduce.jobhistory.done-dir`, the default location being /tmp/hadoop-yarn/staging/history/done/
    for Apache Hadoop.^([[4](#ch08fn04)]) Within this directory, jobs are partitioned
    by the job submission date. If you know your job ID, you can search for your directory:'
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: 作业历史输出存储在由可配置的`mapreduce.jobhistory.done-dir`指定的目录中，默认位置为Apache Hadoop的/tmp/hadoop-yarn/staging/history/done/。在此目录中，作业根据提交日期进行分区。如果你知道你的作业ID，你可以搜索你的目录：
- en: ⁴ Non-Apache Hadoop distributions may have a customized value for `mapreduce.jobhistory.done-dir`—for
    example, in CDH this directory is /user/history/done.
  id: totrans-844
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴ 非Apache Hadoop发行版可能对`mapreduce.jobhistory.done-dir`有自定义值——例如，在CDH中，此目录是/user/history/done。
- en: '[PRE40]'
  id: totrans-845
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'One of the files returned from this command should be a file with a .jhist
    suffix, which is the job history file. Use the fully qualified path of this file
    with the Hadoop `history` command to view your job history details:'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令返回的文件之一应该是一个具有.jhist后缀的文件，这是作业历史文件。使用Hadoop `history`命令的完整路径来查看你的作业历史详细信息：
- en: '[PRE41]'
  id: totrans-847
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The previous output is only a small subset of the overall output produced by
    the command, and it’s worth executing it yourself to see the full metrics it exposes.
    This output is useful in quickly evaluating metrics such as average- and worst-task
    execution times.
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的输出只是命令产生的整体输出的小部分，值得你自己执行以查看它暴露的完整度量。此输出在快速评估平均和最坏的任务执行时间等度量方面很有用。
- en: Both the JobHistory UI and the CLI can be used to identify a number of performance
    issues in your job. As we go through the techniques in this section, I’ll highlight
    how the job history counters can be used to help identify issues.
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: 作业历史界面和CLI都可以用来识别作业中的许多性能问题。随着我们本节中技术的介绍，我将突出显示如何使用作业历史计数器来帮助识别问题。
- en: Let’s get things moving by looking at the optimizations that can be made on
    the map side.
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过查看可以在映射端进行的优化来开始行动。
- en: 8.2.2\. Map optimizations
  id: totrans-851
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.2\. Map优化
- en: Optimizations in the map side of a MapReduce job are usually related to the
    input data and how it’s being processed, or to your application code. Your mappers
    are responsible for reading the job inputs, so variables such as whether your
    input files are splittable, data locality, and the number of input splits all
    can have an impact on the performance of your job. Inefficiencies in your mapper
    code can also lead to longer-than-expected job execution times.
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce作业的map端优化通常与输入数据及其处理方式有关，或者与你的应用程序代码有关。你的mapper负责读取作业输入，因此诸如你的输入文件是否可分割、数据局部性和输入分割数量等变量都可能影响作业的性能。你的mapper代码中的低效也可能导致作业执行时间比预期更长。
- en: This section covers some of the data-related issues that your job could encounter.
    The application-specific issues are covered in section 8.2.6.
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了你的作业可能遇到的一些数据相关的问题。特定于应用程序的问题在8.2.6节中介绍。
- en: Technique 73 Data locality
  id: totrans-854
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术编号 73 数据局部性
- en: One of MapReduce’s biggest performance traits is the notion of “pushing compute
    to the data,” which means that map tasks are scheduled so that they read their
    inputs from local disk. Data locality isn’t guaranteed, however, and your file
    formats and cluster utilization can impact data locality. In this technique you’ll
    learn how to identify indications of lack of locality, and also learn about some
    solutions.
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce最大的性能特性之一是“将计算推送到数据”的概念，这意味着map任务被调度以从本地磁盘读取输入。然而，数据局部性并不保证，你的文件格式和集群利用率可能会影响数据局部性。在这个技术中，你将学习如何识别缺乏局部性的迹象，并了解一些解决方案。
- en: Problem
  id: totrans-856
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to detect whether you have map tasks that are reading inputs over the
    network.
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要检测是否有map任务正在通过网络读取输入。
- en: Solution
  id: totrans-858
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Examine some key counters in the job history metadata.
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: 检查作业历史元数据中的几个关键计数器。
- en: Discussion
  id: totrans-860
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: There are a number of counters in the job history that you should keep an eye
    on to make sure data locality is in play in your mappers. These are listed in
    [table 8.1](#ch08table01).
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: 在作业历史中，有一些计数器你应该密切关注，以确保数据局部性在mapper中发挥作用。这些计数器在[表8.1](#ch08table01)中列出。
- en: Table 8.1\. Counters that can indicate if nonlocal reads are occurring
  id: totrans-862
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表8.1\. 可以指示是否发生非本地读取的计数器
- en: '| Counter name | JobHistory name | You may have nonlocal reads if ... |'
  id: totrans-863
  prefs: []
  type: TYPE_TB
  zh: '| 计数器名称 | 作业历史名称 | 如果...，你可能存在非本地读取 |'
- en: '| --- | --- | --- |'
  id: totrans-864
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| HDFS_BYTES_READ | HDFS: Number of bytes read | ... this number is greater
    than the block size of the input file. |'
  id: totrans-865
  prefs: []
  type: TYPE_TB
  zh: '| HDFS_BYTES_READ | HDFS：读取的字节数 | ...这个数字大于输入文件的块大小。|'
- en: '| DATA_LOCAL_MAPS | Data-local map tasks | ... any map tasks have this value
    set to 0. |'
  id: totrans-866
  prefs: []
  type: TYPE_TB
  zh: '| DATA_LOCAL_MAPS | 数据本地map任务 | ...任何map任务的此值设置为0。|'
- en: '| RACK_LOCAL_MAPS | Rack-local map tasks | ... any map tasks have this value
    set to 1. |'
  id: totrans-867
  prefs: []
  type: TYPE_TB
  zh: '| RACK_LOCAL_MAPS | 机架本地map任务 | ...任何map任务的此值设置为1。|'
- en: 'There could be a number of causes for non-local reads:'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: 非本地读取可能有多个原因：
- en: You’re working with large files and a file format that can’t be split, which
    means that mappers need to stream some of the blocks from other data nodes.
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你正在处理大文件和无法分割的文件格式，这意味着mapper需要从其他数据节点流式传输一些块。
- en: The file format supports splitting, but you’re using an input format that doesn’t
    support splitting. An example of this is using LZOP to compress a text file and
    then using `TextInputFormat`, which doesn’t know how to split the file.
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件格式支持分割，但你使用的是不支持分割的输入格式。例如，使用LZOP压缩文本文件，然后使用`TextInputFormat`，它不知道如何分割文件。
- en: The YARN scheduler wasn’t able to schedule the map container to a node. This
    can happen if your cluster is under load.
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YARN调度器无法将map容器调度到节点。这可能发生在你的集群负载过重的情况下。
- en: 'There are a few options you can consider to address the problems:'
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以考虑几种选项来解决这些问题：
- en: When using an unsplittable file format, write files at or near the HDFS block
    size to minimize nonlocal reads.
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当使用不可分割的文件格式时，将文件写入或接近HDFS块大小，以最小化非本地读取。
- en: If you’re using the capacity scheduler, set `yarn.scheduler.capacity.node-locality-delay`
    to introduce more delay in the scheduler and thus increase the chance that a map
    task is scheduled onto a data-local node.
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用容量调度器，将`yarn.scheduler.capacity.node-locality-delay`设置为在调度器中引入更多延迟，从而增加map任务在数据本地节点上调度成功的几率。
- en: If you’re using text files, switch to a compression codec that supports splitting,
    such as LZO or bzip2.
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你正在使用文本文件，切换到支持分割的压缩编解码器，如LZO或bzip2。
- en: Next let’s look at another data-related optimization that comes into play when
    you’re working with large datasets.
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看当您处理大型数据集时，另一个与数据相关的优化。
- en: Technique 74 Dealing with a large number of input splits
  id: totrans-877
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 74 处理大量输入拆分
- en: Jobs with a large number of input splits are not optimal, because each input
    split is executed by a single mapper, and each mapper executes as a single process.
    The aggregate pressure on the scheduler and the cluster due to forking these processes
    results in slow job execution times. This technique examines some methods that
    can be used to reduce the number of input splits and still maintain data locality.
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: 具有大量输入拆分的作业不是最优的，因为每个输入拆分都由单个映射器执行，每个映射器作为一个单独的进程执行。由于这些进程的派生对调度器和集群的总体压力导致作业执行时间缓慢。此技术检查了一些可以用来减少输入拆分数量并保持数据局部性的方法。
- en: Problem
  id: totrans-879
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to optimize a job that runs with thousands of mappers.
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: 您想优化一个运行数千个映射器的作业。
- en: Solution
  id: totrans-881
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the `CombineFileInputFormat` to combine multiple blocks that are run with
    fewer mappers.
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `CombineFileInputFormat` 来组合运行较少映射器的多个块。
- en: Discussion
  id: totrans-883
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'There are two primary issues that will cause a job to require a large number
    of mappers:'
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: 两个主要问题会导致作业需要大量映射器：
- en: Your input data consists of a large number of small files. The total size of
    all these files may be small, but MapReduce will spawn a mapper for each small
    file, so your job will spend more time launching processes than it will actually
    processing the input data.
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的输入数据由大量的小文件组成。所有这些文件的总大小可能很小，但 MapReduce 将为每个小文件启动一个映射器，因此您的作业将花费更多的时间来启动进程，而不是实际处理输入数据。
- en: Your files aren’t small (they’re close to, or over, the HDFS block size), but
    your aggregate data size is large and spans thousands of blocks in HDFS. Each
    block is assigned to an individual mapper.
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的文件并不小（它们接近或超过 HDFS 块大小），但您的总数据量很大，跨越 HDFS 中的数千个块。每个块都分配给单个映射器。
- en: If your problem is related to small files, you should consider compacting these
    files together or using a container format such as Avro to store your files.
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的问题与小型文件相关，您应该考虑将这些文件压缩在一起或使用容器格式，如 Avro 来存储您的文件。
- en: 'In either of the preceding situations, you can use the `CombineFileInputFormat`,
    which will combine multiple blocks into input splits to reduce the overall number
    of input splits. It does so by examining all the blocks that are occupied by the
    input files, mapping each block to the set of data nodes that stores it, and then
    combining blocks that exist on the same data node into a single input split to
    preserve data locality. There are two concrete implementations of this abstract
    class:'
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述任何一种情况下，您都可以使用 `CombineFileInputFormat`，它将多个块组合成输入拆分，以减少整体输入拆分的数量。它是通过检查被输入文件占用的所有块，将每个块映射到存储它的数据节点集合，然后将位于同一数据节点上的块组合成一个单独的输入拆分来实现的，以保持数据局部性。这个抽象类有两个具体的实现：
- en: '`CombineTextInputFormat` works with text files and uses `TextInputFormat` as
    the underlying input format to process and emit records to the mappers.'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CombineTextInputFormat` 与文本文件一起工作，并使用 `TextInputFormat` 作为底层输入格式来处理和向映射器输出记录。'
- en: '`CombineSequenceFileInputFormat` works with SequenceFiles.'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CombineSequenceFileInputFormat` 与 SequenceFiles 一起工作。'
- en: '[Figure 8.4](#ch08fig04) compares the splits generated by `TextInputFormat`
    with those generated by `CombineTextInputFormat`.'
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8.4](#ch08fig04) 比较了 `TextInputFormat` 生成的拆分与 `CombineTextInputFormat` 生成的拆分。'
- en: Figure 8.4\. An example of how CombineTextInputFormat works with the default
    size settings
  id: totrans-892
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.4\. `CombineTextInputFormat` 与默认大小设置一起工作的一个示例
- en: '![](08fig04_alt.jpg)'
  id: totrans-893
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig04_alt.jpg)'
- en: 'There are some configurables that allow you to tune how input splits are composed:'
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些可配置的选项允许您调整输入拆分的组成方式：
- en: '**`mapreduce.input.fileinputformat.split.minsize.per.node`** —Specifies the
    minimum number of bytes that each input split should contain within a data node.
    The default value is `0`, meaning that there is no minimum size.'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`mapreduce.input.fileinputformat.split.minsize.per.node`** —指定每个输入拆分应在数据节点内包含的最小字节数。默认值是
    `0`，表示没有最小大小。'
- en: '**`mapreduce.input.fileinputformat.split.minsize.per.rack`** —Specifies the
    minimum number of bytes that each input split should contain within a single rack.
    The default value is `0`, meaning that there is no minimum size.'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`mapreduce.input.fileinputformat.split.minsize.per.rack`** —指定每个输入拆分应在单个机架内包含的最小字节数。默认值是
    `0`，表示没有最小大小。'
- en: '**`mapreduce.input.fileinputformat.split.maxsize`** —Specifies the maximum
    size of an input split. The default value is `0`, meaning that there is no maximum
    size.'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`mapreduce.input.fileinputformat.split.maxsize`** —指定输入拆分的最大大小。默认值是 `0`，表示没有最大大小。'
- en: With the default settings, you’ll end up with a maximum of one input split for
    each data node. Depending on the size of your cluster, this may hamper your parallelism,
    in which case you can play with `mapreduce.input.fileinputformat.split.maxsize`
    to allow more than one split for a node.
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: 默认设置下，您将得到每个数据节点最多一个输入拆分。根据您集群的大小，这可能会妨碍您的并行性，在这种情况下，您可以调整 `mapreduce.input.fileinputformat.split.maxsize`
    以允许一个节点有多个拆分。
- en: If the input files for a job are significantly smaller than the HDFS block size,
    it’s likely that your cluster will spend more effort starting and stopping Java
    processes than it spends performing work. If you’re suffering from this problem,
    you should consult [chapter 4](kindle_split_014.html#ch04), where I explained
    various approaches you can take to working efficiently with small files.
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个作业的输入文件明显小于 HDFS 块大小，那么您的集群可能花费更多的时间在启动和停止 Java 进程上，而不是执行工作。如果您遇到这个问题，应该查阅[第
    4 章](kindle_split_014.html#ch04)，我在那里解释了您可以采取的各种方法来高效地处理小文件。
- en: Technique 75 Generating input splits in the cluster with YARN
  id: totrans-900
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术编号 75：在 YARN 中在集群中生成输入拆分
- en: If the client that submits MapReduce jobs is not in a network that’s local to
    your Hadoop cluster, then input split calculation can be expensive. In this technique
    you’ll learn how to push the input split calculation to the MapReduce ApplicationMaster.
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: 如果提交 MapReduce 作业的客户端不在与您的 Hadoop 集群本地的网络上，那么输入拆分计算可能会很昂贵。在这个技术中，您将学习如何将输入拆分计算推送到
    MapReduce ApplicationMaster。
- en: '|  |'
  id: totrans-902
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Only on YARN
  id: totrans-903
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 仅在 YARN 上
- en: This technique only works with YARN.
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术仅适用于 YARN。
- en: '|  |'
  id: totrans-905
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Problem
  id: totrans-906
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: Your client is remote and input split calculation is taking a long time.
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: 您的客户端是远程的，输入拆分计算花费了很长时间。
- en: Solution
  id: totrans-908
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Set `yarn.app.mapreduce.am.compute-splits-in-cluster` to `true`.
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `yarn.app.mapreduce.am.compute-splits-in-cluster` 设置为 `true`。
- en: Discussion
  id: totrans-910
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: By default, input splits are calculated in the MapReduce driver. When the input
    source is HDFS, then the input format needs to perform operations such as file
    listings and file status commands to retrieve block details. When working with
    a large number of input files, this can be slow, especially when there’s network
    latency between the driver and the Hadoop cluster.
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，输入拆分是在 MapReduce 驱动程序中计算的。当输入源是 HDFS 时，输入格式需要执行文件列表和文件状态命令等操作来检索块详情。当处理大量输入文件时，这可能会很慢，尤其是在驱动程序和
    Hadoop 集群之间存在网络延迟时。
- en: The solution is to set `yarn.app.mapreduce.am.compute-splits-in-cluster` to
    `true`, pushing the input split calculation to the MapReduce ApplicationMaster,
    which runs inside the Hadoop cluster. This minimizes the time taken to calculate
    input splits and thus reduces your overall job execution time.
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是将 `yarn.app.mapreduce.am.compute-splits-in-cluster` 设置为 `true`，将输入拆分计算推送到运行在
    Hadoop 集群内部的 MapReduce ApplicationMaster，这样可以最小化计算输入拆分所需的时间，从而减少您整体作业执行时间。
- en: Emitting too much data from your mappers
  id: totrans-913
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从您的映射器中发出过多数据
- en: Outputting a lot of data from your mappers is to be avoided if possible, because
    all of that emitted data results in a lot of disk and network I/O as a result
    of the shuffle. You can use filters and projections in your mappers to cut down
    on the amount of data that you’re working with, and spilling, in MapReduce. Pushdowns
    can further improve your data pipeline. Technique 55 contains examples of filters
    and pushdowns.
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: 尽可能避免从您的映射器输出大量数据，因为这会导致由于洗牌而产生大量的磁盘和网络 I/O。您可以在映射器中使用过滤器和平面投影来减少您正在处理的数据量，并在
    MapReduce 中减少溢出。下推可以进一步改进您的数据管道。技术 55 包含了过滤器和下推的示例。
- en: 8.2.3\. Shuffle optimizations
  id: totrans-915
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.3. 洗牌优化
- en: 'The shuffle in MapReduce is responsible for organizing and delivering your
    mapper outputs to your reducers. There are two parts to the shuffle: the map side
    and the reduce side. The map side is responsible for partitioning and sorting
    data for each reducer. The reduce side fetches data from each mapper and merges
    it before supplying it to the reducer.'
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 中的洗牌负责组织和交付您的映射器输出到您的归约器。洗牌有两个部分：映射端和归约端。映射端负责为每个归约器分区和排序数据。归约端从每个映射器获取数据，在提供给归约器之前将其合并。
- en: As a result there are optimizations you can perform on both sides of the shuffle,
    including writing a combiner, which is covered in the first technique.
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可以在shuffle的两边进行优化，包括编写合并器，这在第一个技巧中已经介绍过。
- en: Technique 76 Using the combiner
  id: totrans-918
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧76 使用合并器
- en: The combiner is a powerful mechanism that aggregates data in the map phase to
    cut down on data sent to the reducer. It’s a map-side optimization, where your
    code is invoked with a number of map output values for the same output key.
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: 合并器是一个强大的机制，它聚合map阶段的输入数据以减少发送给reducer的数据量。这是一个map端的优化，其中你的代码会根据相同的输出键调用多个map输出值。
- en: Problem
  id: totrans-920
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You’re filtering and projecting your data, but your shuffle and sort are still
    taking longer than you want. How can you cut down on them even further?
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在过滤和投影你的数据，但你的shuffle和sort仍然比你想要的要长。你如何进一步减少它们？
- en: Solution
  id: totrans-922
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Define a combiner and use the `setCombinerClass` method to set it for your job.
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个合并器，并使用`setCombinerClass`方法为你的作业设置它。
- en: Discussion
  id: totrans-924
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: The combiner is invoked on the map side as part of writing map output data to
    disk in both the spill and merge phases, as shown in [figure 8.5](#ch08fig05).
    To help with grouping values together to maximize the effectiveness of a combiner,
    use a sorting step in both phases prior to calling the combiner function.
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: 合并器在spill和merge阶段将map输出数据写入磁盘时被调用，作为[图8.5](#ch08fig05)所示，它是map任务上下文中调用合并器的一部分。为了帮助将值分组以最大化合并器的有效性，在调用合并器函数之前，两个阶段都应使用排序步骤。
- en: Figure 8.5\. How the combiner is called in the context of the map task
  id: totrans-926
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.5\. 在map任务上下文中如何调用合并器
- en: '![](08fig05_alt.jpg)'
  id: totrans-927
  prefs: []
  type: TYPE_IMG
  zh: '![图片2](08fig05_alt.jpg)'
- en: 'Calling the `setCombinerClass` sets the combiner for a job, similar to how
    the map and reduce classes are set:'
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`setCombinerClass`设置作业的合并器，类似于如何设置map和reduce类：
- en: '[PRE42]'
  id: totrans-929
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Your combiner implementation must conform to the reducer specification. In this
    technique you’ll write a simple combiner whose job is to remove duplicate map
    output records. As you iterate over the map output values, you’ll only emit those
    that are contiguously unique:^([[5](#ch08fn05)])
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: 你的合并器实现必须符合reducer规范。在这个技巧中，你将编写一个简单的合并器，其任务是删除重复的map输出记录。当你遍历map输出值时，你只会发出那些连续唯一的值：^([[5](#ch08fn05)])
- en: '⁵ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch8/CombineJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch8/CombineJob.java).'
  id: totrans-931
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵ GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch8/CombineJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch8/CombineJob.java).
- en: '![](348fig01_alt.jpg)'
  id: totrans-932
  prefs: []
  type: TYPE_IMG
  zh: '![图片1](348fig01_alt.jpg)'
- en: It’s important that if you have a combiner, the function is distributive. In
    [figure 8.5](#ch08fig05) you saw that the combiner will be called multiple times
    for the same input key, and there are no guarantees about how the output values
    will be organized when they’re sent to the combiner (other than that they were
    paired with the combiner key). A distributive function is one where the end result
    is identical regardless of how inputs were combined.
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有合并器，函数必须是分配性的。在[图8.5](#ch08fig05)中，你看到合并器将多次对相同的输入键进行调用，并且当它们被发送到合并器时，关于输出值的组织没有保证（除了它们与合并器键配对之外）。一个分配性函数是指无论输入如何组合，最终结果都是相同的。
- en: Summary
  id: totrans-934
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: The combiner is a powerful tool in your MapReduce toolkit, as it helps cut down
    on the amount of data transmitted over the network between mappers and reducers.
    Binary comparators are another tool that will improve the execution times of your
    MapReduce jobs, and we’ll examine them next.
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: 合并器是MapReduce工具箱中的强大工具，因为它有助于减少映射器和reducer之间通过网络传输的数据量。二进制比较器是另一个可以改善你的MapReduce作业执行时间的工具，我们将在下一节中探讨它们。
- en: Technique 77 Blazingly fast sorting with binary comparators
  id: totrans-936
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧77 使用二进制比较器进行闪电般的快速排序
- en: When MapReduce is sorting or merging, it uses the `RawComparator` for the map
    output key to compare keys. Built-in `Writable` classes (such as `Text` and `IntWritable`)
    have byte-level implementations that are fast because they don’t require the byte
    form of the object to be unmarshaled to `Object` form for the comparison.
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: 当MapReduce进行排序或合并时，它使用`RawComparator`来比较map输出键。内置的`Writable`类（如`Text`和`IntWritable`）具有字节级别的实现，因为它们不需要将对象的字节形式反序列化为`Object`形式进行比较，所以它们运行得很快。
- en: When writing your own `Writable`, it may be tempting to implement the `Writable-Comparable`
    interface, but this can lead to longer shuffle and sort phases because it requires
    `Object` unmarshaling from byte form for comparisons.
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: 当你编写自己的`Writable`时，可能会倾向于实现`WritableComparable`接口，但这可能会导致shuffle和sort阶段变长，因为它需要从字节形式反序列化`Object`以进行比较。
- en: Problem
  id: totrans-939
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You have custom `Writable` implementations and you want to reduce the sort times
    for your jobs.
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: 你有自定义的`Writable`实现，并且你想要减少作业的排序时间。
- en: Solution
  id: totrans-941
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Write a byte-level comparator to ensure optimal comparisons during sorting.
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个字节级别的`Comparator`以确保在排序过程中的最佳比较。
- en: Discussion
  id: totrans-943
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'In MapReduce there are multiple stages where output keys are compared to each
    other when data is being sorted. To facilitate key sorting, all map output keys
    must implement the `WritableComparable` interface:'
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: 在MapReduce中，有多个阶段在数据排序时会对输出键进行比较。为了便于键排序，所有map输出键都必须实现`WritableComparable`接口：
- en: '[PRE43]'
  id: totrans-945
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'In the `PersonWritable` you created in technique 64 (when implementing a secondary
    sort), your implementation was as follows: ^([[6](#ch08fn06)])'
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: 在你根据技术64（在实现二次排序时）创建的`PersonWritable`中，你的实现如下：^([[6](#ch08fn06)])
- en: '⁶ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/Person.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/Person.java).'
  id: totrans-947
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶ GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/Person.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch6/sort/secondary/Person.java).
- en: '[PRE44]'
  id: totrans-948
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The trouble with this `Comparator` is that MapReduce stores your intermediary
    map output data in byte form, and every time it needs to sort your data it has
    to unmarshal it into `Writable` form to perform the comparison. This unmarshaling
    is expensive because it re-creates your objects for comparison purposes.
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`Comparator`的问题在于，MapReduce将你的中间map输出数据以字节形式存储，每次它需要排序你的数据时，都必须将其反序列化为`Writable`形式以执行比较。这种反序列化是昂贵的，因为它会重新创建你的对象以进行比较目的。
- en: 'If you look at the built-in `Writable`s in Hadoop, you’ll see that not only
    do they extend the `WritableComparable` interface, but they also provide their
    own custom `Comparator` that extends the `WritableComparator` class. The following
    code presents a subsection of the `WritableComparator` class:'
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看Hadoop中的内置`Writable`，你会发现它们不仅扩展了`WritableComparable`接口，还提供了它们自己的自定义`Comparator`，该`Comparator`扩展了`WritableComparator`类。以下代码展示了`WritableComparator`类的一个子集：
- en: '![](350fig01_alt.jpg)'
  id: totrans-951
  prefs: []
  type: TYPE_IMG
  zh: '![](350fig01_alt.jpg)'
- en: 'To write a byte-level `Comparator`, the `compare` method needs to be overridden.
    Let’s look at how the `IntWritable` class implements this method:'
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: 要编写一个字节级别的`Comparator`，需要重写`compare`方法。让我们看看`IntWritable`类是如何实现这个方法的：
- en: '![](351fig01_alt.jpg)'
  id: totrans-953
  prefs: []
  type: TYPE_IMG
  zh: '![](351fig01_alt.jpg)'
- en: The built-in `Writable` classes all provide `WritableComparator` implementations,
    which means you don’t need to worry about optimizing the `Comparator`s as long
    as your MapReduce job output keys use these built-in `Writable`s. But if you have
    a custom `Writable` that you use as an output key, you’ll ideally provide a `WritableComparator`.
    We’ll now revisit your `Person` class and look at how you can do this.
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: 内置的`Writable`类都提供了`WritableComparator`实现，这意味着只要你的MapReduce作业输出键使用这些内置的`Writable`，你就不需要担心优化`Comparator`。但是，如果你有一个用作输出键的自定义`Writable`，理想情况下你应该提供一个`WritableComparator`。我们现在将重新审视你的`Person`类，看看你如何做到这一点。
- en: 'In your `Person` class, you had two fields: the first and last names. Your
    implementation stored them as strings and used the `DataOutput`’s `writeUTF` method
    to write them out:'
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的`Person`类中，你有两个字段：名字和姓氏。你的实现将它们存储为字符串，并使用`DataOutput`的`writeUTF`方法将它们写入：
- en: '[PRE45]'
  id: totrans-956
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The first thing you need to understand is how your `Person` object is represented
    in byte form, based on the previous code. The `writeUTF` method writes two bytes
    containing the length of the string, followed by the byte form of the string.
    [Figure 8.6](#ch08fig06) shows how this information is laid out in byte form.
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: 首先你需要理解的是，根据之前的代码，你的`Person`对象在字节形式中的表示。`writeUTF`方法写入包含字符串长度的两个字节，然后是字符串的字节形式。[图8.6](#ch08fig06)展示了这些信息在字节形式中的布局。
- en: Figure 8.6\. Byte layout of Person
  id: totrans-958
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.6\. Person的字节布局
- en: '![](08fig06_alt.jpg)'
  id: totrans-959
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig06_alt.jpg)'
- en: You want natural ordering of your records that include both the last and first
    names, but you can’t do this directly using the byte array because the string
    lengths are also encoded in the array. Instead, the `Comparator` needs to be smart
    enough to skip over the string lengths. The following code shows how to do this:^([[7](#ch08fn07)])
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要自然排序的记录，包括姓氏和名字，但无法直接使用字节数组完成，因为字符串长度也编码在数组中。相反，`Comparator` 需要足够智能，能够跳过字符串长度。以下代码展示了如何做到这一点：^([[7](#ch08fn07)])
- en: '⁷ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch8/Person-BinaryComparator.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch8/Person-BinaryComparator.java).'
  id: totrans-961
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷ GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch8/Person-BinaryComparator.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch8/Person-BinaryComparator.java).
- en: '![](352fig01_alt.jpg)'
  id: totrans-962
  prefs: []
  type: TYPE_IMG
  zh: '![352fig01_alt.jpg](352fig01_alt.jpg)'
- en: Summary
  id: totrans-963
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: The `writeUtf` method is limited because it can only support strings that contain
    less than 65,536 characters. This is probably fine for the scenario where you’re
    working with people’s names, but if you need to work with a larger string, you
    should look at using Hadoop’s `Text` class, which can support much larger strings.
    If you look at the `Comparator` inner class in the `Text` class, you’ll see that
    its binary string comparator works in a fashion similar to the one discussed here.
    This approach could easily be extended to work with names represented with `Text`
    objects rather than Java `String` objects.
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: '`writeUtf` 方法有限制，因为它只能支持包含少于 65,536 个字符的字符串。这可能在处理人名的情况下是可行的，但如果你需要处理更大的字符串，你应该考虑使用
    Hadoop 的 `Text` 类，它可以支持更大的字符串。如果你查看 `Text` 类中的 `Comparator` 内部类，你会看到它的二进制字符串比较器的工作方式与这里讨论的类似。这种方法可以很容易地扩展到使用
    `Text` 对象而不是 Java `String` 对象表示的名称。'
- en: The next issue in performance tuning is how you can guard against the impact
    that data skews can have on your MapReduce jobs.
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: 性能调整的下一个问题是，如何防止数据偏斜对 MapReduce 作业的影响。
- en: Using a range partitioner to avoid data skew
  id: totrans-966
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用范围分区器来避免数据偏斜
- en: It’s common for a handful of the reducers to be in the long tail when it comes
    to task execution time, due to the way that the default hash partitioner works.
    If this is impacting your job, then take a look at technique 63 on handling skews
    generated by the hash partitioner.
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务执行时间较长时，通常有一小部分reducer由于默认的哈希分区器的工作方式而处于长尾。如果这影响了你的作业，那么请查看处理哈希分区器产生的偏斜的技术
    63。
- en: Technique 78 Tuning the shuffle internals
  id: totrans-968
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 78 调整洗牌内部
- en: The shuffle phase involves fetching the map output data from the shuffle service
    and merging it in the background. The sort phase, which is another merge, will
    merge the files together into a smaller number of files.
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: 洗牌阶段涉及从洗牌服务中获取映射输出数据并在后台合并。排序阶段，作为另一个合并过程，会将文件合并成更少的文件。
- en: Problem
  id: totrans-970
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to determine if a job runs slowly due to the shuffle and sort phases.
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要确定一个作业是否因为洗牌和排序阶段而运行缓慢。
- en: Solution
  id: totrans-972
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the JobHistory metadata to extract statistics related to the shuffle and
    sort execution times.
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: 使用作业历史元数据提取与洗牌和排序执行时间相关的统计信息。
- en: Discussion
  id: totrans-974
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: We’re going to look at three areas of the shuffle and for each identify areas
    that can be tuned for increased performance.
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看洗牌的三个区域，并为每个区域确定可以调整以提高性能的区域。
- en: Tuning the map side
  id: totrans-976
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 调整映射端
- en: When a mapper emits output records, they’re first stored in an in-memory buffer.
    After the buffer grows to a certain size, the data is spilled to a new file on
    disk. This process continues until the mapper has completed emitting all its output
    records. [Figure 8.7](#ch08fig07) shows this process.
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: 当映射器输出记录时，它们首先被存储在内存缓冲区中。当缓冲区增长到一定大小时，数据会被溢写到磁盘上的新文件中。这个过程会一直持续到映射器完成所有输出记录的输出。![图
    8.7](#ch08fig07)展示了这个过程。
- en: Figure 8.7\. The map-side shuffle
  id: totrans-978
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.7\. 映射端洗牌
- en: '![](08fig07_alt.jpg)'
  id: totrans-979
  prefs: []
  type: TYPE_IMG
  zh: '![08fig07_alt.jpg](08fig07_alt.jpg)'
- en: The expensive part of the map-side shuffle is the I/O related to spilling and
    merging the spill files. The merge is expensive, as all the map outputs need to
    be read from the spill files and rewritten to the merged spill file.
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: 映射端洗牌昂贵的地方是与溢写和合并溢写文件相关的 I/O。合并是昂贵的，因为所有映射输出都需要从溢写文件中读取并重写到合并的溢写文件中。
- en: An ideal mapper is able to fit all its output in the in-memory buffer, which
    means that only one spill file is required. Doing so negates the need to merge
    multiple spill files together. This isn’t possible for all jobs, but if your mapper
    filters or projects the input data so that the input data can fit into memory,
    then it’s worthwhile tuning `mapreduce.task.io.sort.mb` to be large enough to
    store the map outputs.
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: 一个理想的映射器能够将其所有输出都适应内存缓冲区，这意味着只需要一个溢出文件。这样做就消除了合并多个溢出文件的需求。并非所有作业都可行，但如果你的映射器过滤或投影输入数据，使得输入数据可以适应内存，那么调整`mapreduce.task.io.sort.mb`以足够大以存储映射输出是值得的。
- en: Examine the job counters shown in [table 8.2](#ch08table02) to understand and
    tune the shuffle characteristics of your job.
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: 检查[表8.2](#ch08table02)中显示的作业计数器，以了解和调整作业的混洗特性。
- en: Table 8.2\. Map shuffle counters
  id: totrans-983
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表8.2\. 映射混洗计数器
- en: '| Counter | Description |'
  id: totrans-984
  prefs: []
  type: TYPE_TB
  zh: '| 计数器 | 描述 |'
- en: '| --- | --- |'
  id: totrans-985
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| MAP_OUTPUT_BYTES | Use the MAP_OUTPUT_BYTES counter for your map tasks to
    determine if it’s possible to increase your mapreduce.task.io.sort.mb so that
    it can store all the map outputs. |'
  id: totrans-986
  prefs: []
  type: TYPE_TB
  zh: '| 映射输出字节数 | 使用MAP_OUTPUT_BYTES计数器来确定是否可以增加`mapreduce.task.io.sort.mb`，以便它可以存储所有映射输出。|'
- en: '| SPILLED_RECORDS MAP_OUTPUT_RECORDS | Ideally these two values will be the
    same, which indicates that only one spill occurred. |'
  id: totrans-987
  prefs: []
  type: TYPE_TB
  zh: '| 溢出记录数 映射输出记录数 | 理想情况下，这两个值将相同，这表明只发生了一次溢出。|'
- en: '| FILE_BYTES_READ FILE_BYTES_WRITTEN | Compare these two counters with MAP_OUTPUT_BYTES
    to understand the additional reads and writes that are occurring as a result of
    the spilling and merging. |'
  id: totrans-988
  prefs: []
  type: TYPE_TB
  zh: '| 读取的字节数 写入的字节数 | 将这两个计数器与MAP_OUTPUT_BYTES进行比较，以了解由于溢出和合并而发生的额外读取和写入。|'
- en: Tuning the reduce side
  id: totrans-989
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 调整减少侧
- en: On the reduce side, the map outputs for the reducer are streamed from the auxiliary
    shuffle service that runs on each slave node. The map outputs are written into
    an in-memory buffer that is merged and written to disk once the buffer reaches
    a certain size. In the background, these spilled files are continuously merged
    into a smaller number of merged files. Once the fetchers have fetched all their
    outputs, there’s a final round of merging, after which data from the merged files
    is streamed to the reducer. [Figure 8.8](#ch08fig08) shows this process.
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: 在减少侧，映射器为减少器提供的输出是从每个从节点上运行的辅助混洗服务流出的。映射输出被写入一个内存缓冲区，一旦缓冲区达到一定大小，就会合并并写入磁盘。在后台，这些溢出文件会持续合并成更少的合并文件。一旦收集器收集了所有输出，就会进行最后一轮合并，之后合并文件中的数据会流出到减少器。[图8.8](#ch08fig08)显示了此过程。
- en: Figure 8.8\. The reduce-side shuffle
  id: totrans-991
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.8\. 减少侧混洗
- en: '![](08fig08_alt.jpg)'
  id: totrans-992
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig08_alt.jpg)'
- en: Much like with the map side, the goal of tuning the reduce-size shuffle is to
    attempt to fit all the map outputs into memory to avoid spilling to disk and merging
    spilled files. By default, records are always spilled to disk even if they can
    all fit in memory, so to enable a memory-to-memory merge that bypasses disk, set
    `mapreduce.reduce.merge.memtomem.enabled` to `true`.
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
  zh: 与映射侧类似，调整减少大小混洗的目标是尝试将所有映射输出适应内存，以避免溢出到磁盘并合并溢出文件。默认情况下，即使所有记录都可以适应内存，记录也会始终溢出到磁盘，因此为了启用内存到内存的合并，绕过磁盘，将`mapreduce.reduce.merge.memtomem.enabled`设置为`true`。
- en: The job counters in [table 8.3](#ch08table03) can be used to understand and
    tune the shuffle characteristics of your job.
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: '[表8.3](#ch08table03)中的作业计数器可用于了解和调整作业的混洗特性。'
- en: Table 8.3\. Map shuffle counters
  id: totrans-995
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表8.3\. 映射混洗计数器
- en: '| Counter | Description |'
  id: totrans-996
  prefs: []
  type: TYPE_TB
  zh: '| 计数器 | 描述 |'
- en: '| --- | --- |'
  id: totrans-997
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| SPILLED_RECORDS | The number of records that are written to disk. If your
    goal is for map outputs to never touch disk, this value should be 0. |'
  id: totrans-998
  prefs: []
  type: TYPE_TB
  zh: '| 溢出记录数 | 写入磁盘的记录数。如果你的目标是映射输出永远不接触磁盘，则此值应为0。|'
- en: '| FILE_BYTES_READ FILE_BYTES_WRITTEN | These counters will give you an idea
    of how much data is being spilled and merged to disk. |'
  id: totrans-999
  prefs: []
  type: TYPE_TB
  zh: '| 读取的字节数 写入的字节数 | 这些计数器将给你一个关于有多少数据被溢出和合并到磁盘的概览。|'
- en: Shuffle settings
  id: totrans-1000
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 混洗设置
- en: '[Table 8.4](#ch08table04) shows the properties covered in this technique.'
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: '[表8.4](#ch08table04)显示了此技术涵盖的属性。'
- en: Table 8.4\. Configurables to tune the shuffle
  id: totrans-1002
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表8.4\. 可调整的混洗配置
- en: '| Name | Default value | Map side or reduce side? | Description |'
  id: totrans-1003
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 默认值 | 映射侧或减少侧？ | 描述 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-1004
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| mapreduce.task.io.sort.mb | 100 (MB) | Map | The total amount of buffer memory
    in megabytes to use when buffering map outputs. This should be approximately 70%
    of the map task’s heap size. |'
  id: totrans-1005
  prefs: []
  type: TYPE_TB
  zh: '| mapreduce.task.io.sort.mb | 100 (MB) | Map | 缓冲映射输出的总缓冲内存量（以兆字节为单位）。这应该是映射任务堆大小的约70%。'
- en: '| mapreduce.map.sort.spill.percent | 0.8 (80%) | Map | The soft limit in the
    serialization buffer. Once reached, a thread will begin to spill the contents
    to disk in the background. Note that collection will not block if this threshold
    is exceeded while a spill is already in progress, so spills may be larger than
    this threshold when it is set to less than 0.5. |'
  id: totrans-1006
  prefs: []
  type: TYPE_TB
  zh: '| mapreduce.map.sort.spill.percent | 0.8 (80%) | Map | 序列化缓冲区的软限制。一旦达到，线程将开始在后台将内容溢出到磁盘。请注意，如果溢出已经开始，则超过此阈值时收集不会阻塞，因此溢出可能大于此阈值，当设置为小于0.5时。 '
- en: '| mapreduce.task.io.sort.factor | 10 | Map and reduce | The number of streams
    to merge at once while sorting files. This determines the number of open file
    handles. Larger clusters with 1,000 or more nodes can bump this up to 100. |'
  id: totrans-1007
  prefs: []
  type: TYPE_TB
  zh: '| mapreduce.task.io.sort.factor | 10 | Map and reduce | 排序文件时一次合并的流数。这决定了打开的文件句柄的数量。对于拥有1,000个或更多节点的较大集群，可以将此值提高到100。'
- en: '| mapreduce.reduce.shuffle.parallelcopies | 5 | Reduce | The default number
    of parallel transfers run on the reduce side during the copy (shuffle) phase.
    Larger clusters with 1,000 or more nodes can bump this up to 20. |'
  id: totrans-1008
  prefs: []
  type: TYPE_TB
  zh: '| mapreduce.reduce.shuffle.parallelcopies | 5 | Reduce | 在复制（洗牌）阶段在reducer端运行的并行传输的默认数量。对于拥有1,000个或更多节点的较大集群，可以将此值提高到20。'
- en: '| mapreduce.reduce.shuffle.input.buffer.percent | 0.70 | Reduce | The percentage
    of memory to be allocated from the maximum heap size to store map outputs during
    the shuffle. |'
  id: totrans-1009
  prefs: []
  type: TYPE_TB
  zh: '| mapreduce.reduce.shuffle.input.buffer.percent | 0.70 | Reduce | 在洗牌过程中存储映射输出的最大堆大小的百分比。'
- en: '| mapreduce.reduce.shuffle.merge.percent | 0.66 | Reduce | The usage threshold
    at which an in-memory merge will be initiated, expressed as a percentage of the
    total memory allocated to storing in-memory map outputs, as defined by mapreduce.reduce.shuffle.input.buffer.percent.
    |'
  id: totrans-1010
  prefs: []
  type: TYPE_TB
  zh: '| mapreduce.reduce.shuffle.merge.percent | 0.66 | Reduce | 内存合并开始的阈值使用率，以存储内存映射输出的总内存分配的百分比表示，如由mapreduce.reduce.shuffle.input.buffer.percent定义。'
- en: '| mapreduce.reduce.merge.memtomem.enabled | false | Reduce | If all the map
    outputs for each reducer can be stored in memory, then set this property to true.
    |'
  id: totrans-1011
  prefs: []
  type: TYPE_TB
  zh: '| mapreduce.reduce.merge.memtomem.enabled | false | Reduce | 如果每个reducer的所有映射输出都可以存储在内存中，则将此属性设置为true。'
- en: Summary
  id: totrans-1012
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: The simplest way to cut down on shuffle and sort times is to aggressively filter
    and project your data, use a combiner, and compress your map outputs. These approaches
    reduce the amount of data flowing between the map and reduce tasks and lessen
    the network and CPU/disk burden related to the shuffle and sort phases.
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: 减少洗牌和排序时间的最简单方法是积极过滤和投影你的数据，使用combiner，并压缩你的映射输出。这些方法减少了映射和reducer任务之间流动的数据量，并减轻了与洗牌和排序阶段相关的网络和CPU/磁盘负担。
- en: If you’ve done all that, you can look at some of the tips outlined in this technique
    to determine if your job can be tuned so that the data being shuffled touches
    disk as little as possible.
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经做了所有这些，你可以查看本技术中概述的一些提示，以确定你的工作是否可以被调整，使得正在洗牌的数据尽可能少地触及磁盘。
- en: 8.2.4\. Reducer optimizations
  id: totrans-1015
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.4. Reducer优化
- en: Much like map tasks, reduce tasks have their own unique problems that can affect
    performance. In this section we’ll look at how common problems can affect the
    performance of reducer tasks.
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: 与映射任务类似，reducer任务也有它们自己独特的问题，这些问题可能会影响性能。在本节中，我们将探讨常见问题如何影响reducer任务的性能。
- en: Technique 79 Too few or too many reducers
  id: totrans-1017
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧79：reducer数量过少或过多
- en: For the most part, parallelism on the map side is automatically set and is a
    function of your input files and the input format you’re using. But on the reduce
    side you have total control over the number of reducers for your job, and if that
    number is too small or too large, you’re potentially not getting the most value
    out of your cluster.
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数情况，映射端的并行性是自动设置的，并且是输入文件和所使用的输入格式的函数。但在reducer端，你对作业的reducer数量有完全的控制权，如果这个数字太小或太大，你可能无法从你的集群中获得最大价值。
- en: Problem
  id: totrans-1019
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to determine if a job runs slowly due to the number of reducers.
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要确定作业运行缓慢是否由于reducer的数量。
- en: Solution
  id: totrans-1021
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: The JobHistory UI can be used to inspect the number of reducers running for
    your job.
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用JobHistory UI来检查你作业运行中的reducer数量。
- en: Discussion
  id: totrans-1023
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Use the JobHistory UI to look at the number of reducers for your job and the
    number of input records for each reducer. You may be running with too few or too
    many reducers. Running with too few reducers means that you’re not using the available
    parallelism of your cluster; running with too many reducers means the scheduler
    may have to stagger the reducer execution if there aren’t enough resources to
    execute the reducers in parallel.
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: 使用JobHistory UI查看您作业的reducer数量以及每个reducer的输入记录数。您可能正在使用过少或过多的reducer。使用过少的reducer意味着您没有充分利用集群的可用并行性；使用过多的reducer意味着如果资源不足以并行执行reducer，调度器可能需要错开reducer的执行。
- en: There are circumstances where you can’t avoid running with a small number of
    reducers, such as when you’re writing to an external resource (such as a database)
    that you don’t want to overwhelm.
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些情况下您无法避免使用少量reducer运行，例如当您正在写入外部资源（如数据库）时，您不想使其过载。
- en: Another common anti-pattern in MapReduce is using a single reducer when you
    want job output to have total order and not be ordered within the scope of a reducer’s
    output. This anti-pattern can be avoided with the `TotalOrderPartitioner`, which
    we looked at in technique 65.
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce中另一个常见的反模式是在您希望作业输出具有总顺序而不是在reducer输出范围内排序时使用单个reducer。这个反模式可以通过使用我们在技巧65中提到的`TotalOrderPartitioner`来避免。
- en: Dealing with data skew
  id: totrans-1027
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 处理数据倾斜
- en: Data skew can be easily identified—it’s manifested by a small percentage of
    your reduce tasks taking significantly longer to complete than the other tasks.
    This is usually due to one of two reasons—poor hash partitioning or high join-key
    cardinality when you’re performing joins. [Chapter 6](kindle_split_018.html#ch06)
    provides solutions to both problems in section 6.1.5.
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: 数据倾斜可以很容易地识别——表现为一小部分reduce任务完成时间显著长于其他任务。这通常是由于以下两个原因之一——较差的hash分区或在高join-key基数的情况下执行连接操作。第6章（[Chapter
    6](kindle_split_018.html#ch06)）第6.1.5节提供了这两个问题的解决方案。
- en: 8.2.5\. General tuning tips
  id: totrans-1029
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.5. 通用调整技巧
- en: In this section we’ll look at problems that can affect both map and reduce tasks.
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨可能影响map和reduce任务的问题。
- en: Compression
  id: totrans-1031
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 压缩
- en: Compression is an important part of optimizing Hadoop. You can gain substantial
    space and time savings by compressing both intermediary map outputs and job outputs.
    Compression is covered in detail in [chapter 4](kindle_split_014.html#ch04).
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩是优化Hadoop的重要部分。通过压缩中间map输出和作业输出，您可以获得实质性的空间和时间节省。压缩在第4章（[chapter 4](kindle_split_014.html#ch04)）中有详细的介绍。
- en: Using a compact data format
  id: totrans-1033
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用紧凑的数据格式
- en: Much like compression, using space-efficient file formats such as Avro and Parquet
    results in a more compact representation of your data and yields improved marshaling
    and unmarshaling times compared to storing data as text. A large part of [chapter
    3](kindle_split_013.html#ch03) is dedicated to working with these file formats.
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
  zh: 与压缩类似，使用像Avro和Parquet这样的空间高效文件格式可以更紧凑地表示您的数据，并且与将数据存储为文本相比，可以显著提高序列化和反序列化时间。第3章（[chapter
    3](kindle_split_013.html#ch03)）的大部分内容都致力于处理这些文件格式。
- en: It should also be noted that text is an especially inefficient data format to
    work with—it’s space-inefficient and computationally expensive to parse, and parsing
    data at scale can cost a surprising amount of time, especially if regular expressions
    are involved.
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
  zh: 还应注意的是，文本是一种特别低效的数据格式——它空间效率低下，解析计算成本高，并且在大规模解析数据时可能会花费令人惊讶的时间，尤其是如果涉及到正则表达式的话。
- en: Even when the end result of your work in MapReduce is a nonbinary file format,
    it’s good practice to store your intermediate data in binary form. For example,
    if you have a MapReduce pipeline involving a sequence of MapReduce jobs, you should
    consider using Avro or SequenceFiles to store your individual job outputs. The
    last job that produces the final results can use whatever output format is required
    for your use case, but intermediate jobs should use a binary output format to
    speed up the writing and reading parts of MapReduce.
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
  zh: 即使MapReduce工作的最终结果是非二进制文件格式，将中间数据以二进制形式存储也是良好的实践。例如，如果您有一个涉及一系列MapReduce作业的MapReduce管道，您应考虑使用Avro或SequenceFiles来存储您的单个作业输出。产生最终结果的最后一个作业可以使用适用于您用例的任何输出格式，但中间作业应使用二进制输出格式以加快MapReduce的读写部分。
- en: Technique 80 Using stack dumps to discover unoptimized user code
  id: totrans-1037
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧80：使用堆栈转储发现未优化的用户代码
- en: Imagine you’re running a job and it’s taking longer than you expect. You can
    often determine if this is due to inefficient code by taking several stack dumps
    and examining the output to see if the stacks are executing in the same location.
    This technique walks you through taking stack dumps of a running MapReduce job.
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你正在运行一个作业，它比你预期的耗时更长。你通常可以通过进行几次堆栈转储并检查输出以查看堆栈是否在相同的位置执行来确定这是否是由于代码低效。这项技术将指导你进行正在运行的MapReduce作业的堆栈转储。
- en: Problem
  id: totrans-1039
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to determine if a job runs slowly due to inefficiencies in your code.
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要确定作业运行缓慢是否是由于代码中的低效。
- en: Solution
  id: totrans-1041
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Determine the host and process ID of currently executing tasks, take a number
    of stack dumps, and examine them to narrow down bottlenecks in your code.
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: 确定当前执行任务的宿主机和进程ID，进行多次堆栈转储，并检查它们以缩小代码中的瓶颈。
- en: Discussion
  id: totrans-1043
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: If there’s anything particularly inefficient in your code, chances are that
    you’ll be able to discover what it is by taking some stack dumps of the task process.
    [Figure 8.9](#ch08fig09) shows how to identify the task details so that you can
    take the stack dumps.
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的代码中有什么特别低效的地方，那么通过从任务进程中获取一些堆栈转储，你很可能能够发现它。[图8.9](#ch08fig09)展示了如何识别任务详情以便你可以进行堆栈转储。
- en: Figure 8.9\. Determining the container ID and host for a MapReduce task
  id: totrans-1045
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.9\. 确定MapReduce任务的容器ID和主机
- en: '![](08fig09_alt.jpg)'
  id: totrans-1046
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig09_alt.jpg)'
- en: Now that you know the container ID and the host it’s executing on, you can take
    stack dumps of the task process, as shown in [figure 8.10](#ch08fig10).
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了容器的ID以及它正在执行的主机，你可以对任务进程进行堆栈转储，如图8.10所示。
- en: Figure 8.10\. Taking stack dumps and accessing the output
  id: totrans-1048
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.10\. 取堆栈转储和访问输出
- en: '![](08fig10_alt.jpg)'
  id: totrans-1049
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig10_alt.jpg)'
- en: Summary
  id: totrans-1050
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: The best approach to understanding what your code is spending time doing is
    to profile your code, or update your code to time how long you spend in each task.
    But using stack dumps is useful if you want to get a rough sense of whether this
    is an issue without having to change your code.
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
  zh: 理解你的代码在做什么耗时最好的方法是分析你的代码，或者更新你的代码以测量你在每个任务上花费的时间。但如果你想要大致了解是否存在问题而不必更改代码，使用堆栈转储是有用的。
- en: Stack dumps are a primitive, yet often effective, means of discovering where
    a Java process is spending its time, particularly if that process is CPU-bound.
    Clearly dumps are not as effective as using a profiler, which will more accurately
    pinpoint where time is being spent, but the advantage of stack dumps is that they
    can be performed on any running Java process. If you were to use a profiler, you’d
    need to reexecute the process with the required profiling JVM settings, which
    is a nuisance in MapReduce.
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: 堆栈转储是一种原始但通常有效的方法，用于发现Java进程在哪里花费时间，尤其是如果该进程是CPU密集型的。显然，转储不如使用分析器有效，分析器可以更准确地确定时间花费的位置，但堆栈转储的优势在于它们可以在任何运行的Java进程中执行。如果你要使用分析器，你需要重新执行进程并使用所需的配置文件JVM设置，这在MapReduce中是一个麻烦。
- en: When taking stack dumps, it’s useful to take multiple dumps with some pauses
    between successive dumps. This allows you to visually determine if the code execution
    stacks across multiple dumps are roughly in the same point. If this is the case,
    there’s a good chance the code in the stack is what’s causing the slowness.
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行堆栈转储时，在连续转储之间暂停一段时间是有用的。这允许你直观地确定代码执行堆栈是否在多个转储中大致相同。如果是这样，代码中很可能就是导致缓慢的原因。
- en: If your code isn’t in the same location across the different stack dumps, this
    doesn’t necessarily indicate that there aren’t inefficiencies. In this case, the
    best approach is to profile your code or add some measurements in your code and
    rerun the job to get a more accurate breakdown of where time is being spent.
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的代码在不同的堆栈转储中不在同一位置，这并不一定意味着没有低效。在这种情况下，最好的方法是分析你的代码或在代码中添加一些测量并重新运行作业以获得更准确的时间花费分解。
- en: Technique 81 Profiling your map and reduce tasks
  id: totrans-1055
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧81 分析你的映射和减少任务
- en: Profiling standalone Java applications is straightforward and well supported
    by a large number of tools. In MapReduce, you’re working in a distributed environment
    running multiple map and reduce tasks, so it’s less clear how you would go about
    profiling your code.
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
  zh: 分析独立的Java应用程序是简单且得到了大量工具的支持。在MapReduce中，你在一个运行多个映射和减少任务的分布式环境中工作，因此不太清楚你将如何进行代码分析。
- en: Problem
  id: totrans-1057
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You suspect that there are inefficiencies in your map and reduce code, and you
    need to identify where they exist.
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: 你怀疑你的map和reduce代码中存在低效之处，你需要确定它们在哪里。
- en: Solution
  id: totrans-1059
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use HPROF in combination with a number of MapReduce job methods, such as `setProfileEnabled`,
    to profile your tasks.
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: 将HPROF与多个MapReduce作业方法结合使用，例如`setProfileEnabled`，以分析你的任务。
- en: Discussion
  id: totrans-1061
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Hadoop has built-in support for the HPROF profiler, Oracle’s Java profiler
    built into the JVM. To get started, you don’t need to understand any HPROF settings—you
    can call `JobConf.setProfileEnabled(true)` and Hadoop will run HPROF with the
    following settings:'
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop内置了对HPROF分析器的支持，这是Oracle的Java分析器，集成在JVM中。要开始使用，你不需要了解任何HPROF设置——你可以调用`JobConf.setProfileEnabled(true)`，Hadoop将使用以下设置运行HPROF：
- en: '[PRE46]'
  id: totrans-1063
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This will generate object allocation stack sizes that are too small to be useful,
    so instead you can programmatically set custom HPROF parameters:'
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成对象分配堆栈大小太小，没有实际用途，因此你可以程序化地设置自定义的HPROF参数：
- en: '![](360fig01_alt.jpg)'
  id: totrans-1065
  prefs: []
  type: TYPE_IMG
  zh: '![](360fig01_alt.jpg)'
- en: The sample job profiled is quite simple. It parses a file containing IP addresses,
    extracts the first octet from the address, and emits it as the output value:^([[8](#ch08fn08)])
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: 被分析的任务示例相当简单。它解析包含IP地址的文件，从地址中提取第一个八位字节，并将其作为输出值输出:^([[8](#ch08fn08)])
- en: '⁸ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch8/SlowJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch8/SlowJob.java).'
  id: totrans-1067
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸ GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch8/SlowJob.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch8/SlowJob.java).
- en: '[PRE47]'
  id: totrans-1068
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'You can upload a large(ish) file of IP addresses and run your job against it,
    with the previous profiling options set:'
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以上传一个包含大量IP地址的大文件，并使用之前的分析选项运行你的作业：
- en: '[PRE48]'
  id: totrans-1070
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The HPROF option you specified earlier via the `setProfileParams` method call
    will create a text file that can be easily parsed. The file is written to the
    container’s log directory into a file titled profile.out. There are two ways of
    accessing this file: either through the JobHistory UI or by using your shell to
    `ssh` to the node that ran the task. The previous technique showed you how to
    determine the host and log directory for a task.'
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: 你通过`setProfileParams`方法调用指定的HPROF选项将创建一个易于解析的文本文件。该文件写入容器的日志目录，文件名为profile.out。有两种方法可以访问此文件：要么通过JobHistory
    UI，要么通过使用shell来`ssh`到运行任务的节点。前面的技术向您展示了如何确定任务的主机和日志目录。
- en: 'The profile.out file contains a number of stack traces, and at the bottom contains
    memory and CPU time accumulations, with references to stack traces that accounted
    for the accumulations. In the example you ran, look at the top two items, which
    accounted for the most CPU time, and correlate them with the code:'
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
  zh: profile.out文件包含多个堆栈跟踪，底部包含内存和CPU时间的累积，以及导致累积的堆栈跟踪的引用。在您运行的示例中，查看前两项，它们占用了最多的CPU时间，并将它们与代码相关联：
- en: '![](361fig01_alt.jpg)'
  id: totrans-1073
  prefs: []
  type: TYPE_IMG
  zh: '![](361fig01_alt.jpg)'
- en: The first issue identified is the use of the `String.split` method, which uses
    regular expressions to tokenize strings. Regular expressions are computationally
    expensive, especially when they’re executed over millions of records, which is
    normal when working with data volumes typical with MapReduce. One solution is
    to replace the `String.split` method with any of the `StringUtils.split` methods
    in the Apache Commons Lang library, which doesn’t use regular expressions.
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
  zh: 确定的第一个问题是使用`String.split`方法，它使用正则表达式对字符串进行标记化。正则表达式在计算上很昂贵，尤其是在处理数百万条记录时，这在处理与MapReduce典型数据量相当的数据时是正常的。一个解决方案是将`String.split`方法替换为Apache
    Commons Lang库中的任何`StringUtils.split`方法，后者不使用正则表达式。
- en: To avoid the overhead associated with the `Text` class constructor, construct
    the instance once and call the `set` method repeatedly, which is much more efficient.
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免与`Text`类构造函数相关的开销，构造实例一次，并反复调用`set`方法，这要高效得多。
- en: Summary
  id: totrans-1076
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: Running HPROF adds significant overhead to the execution of Java; it instruments
    Java classes to collect the profiling information as your code is executing. This
    isn’t something you’ll want to regularly run in production.
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: 运行HPROF会给Java的执行增加显著的开销；它通过在代码执行时对Java类进行仪器化来收集分析信息。这不是你希望在生产环境中定期运行的事情。
- en: There’s a simpler way to profile tasks by adding `-Xprof` to `mapred.child.java.opts`
    as recommended in Todd Lipcon’s excellent presentation.^([[9](#ch08fn09)])
  id: totrans-1078
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Todd Lipcon的优秀演示中建议的，通过向`mapred.child.java.opts`添加`-Xprof`来简化任务的分析。
- en: ⁹ Todd Lipcon, “Optimizing MapReduce Job Performance,” [http://www.slideshare.net/cloudera/mr-perf](http://www.slideshare.net/cloudera/mr-perf).
  id: totrans-1079
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹ 托德·利普康，“优化MapReduce作业性能”，[http://www.slideshare.net/cloudera/mr-perf](http://www.slideshare.net/cloudera/mr-perf)。
- en: In fact, the ideal way to profile your code is to isolate your map or reduce
    code in such a way that it can be executed outside of Hadoop using a profiler
    of your choice. Then you can focus on quickly iterating your profiling without
    worrying about Hadoop getting in your way.
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，理想的方式来分析你的代码是将其映射或归约代码以独立的方式隔离，这样就可以使用你选择的剖析器在Hadoop之外执行。然后你可以专注于快速迭代剖析，而不用担心Hadoop会阻碍你的工作。
- en: This wraps up our look at some of the methods you can use to tune the performance
    of your jobs and to make your jobs as efficient as possible. Next up is a look
    at various mechanisms that can help you debug your applications.
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: 这总结了我们可以使用的一些方法来调整作业的性能，并使作业尽可能高效。接下来，我们将探讨各种可以帮助你调试应用程序的机制。
- en: 8.3\. Debugging
  id: totrans-1082
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3\. 调试
- en: In this section we’ll cover a number of topics that will help with your debugging
    efforts. We’ll kick things off with a look at the task logs.
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些有助于调试工作的主题。我们将从查看任务日志开始。
- en: 8.3.1\. Accessing container log output
  id: totrans-1084
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.1\. 访问容器日志输出
- en: Accessing your task logs is the first step to figuring out what issues you’re
    having with your jobs.
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: 访问你的任务日志是确定你的作业中存在哪些问题的第一步。
- en: Technique 82 Examining task logs
  id: totrans-1086
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术编号82 检查任务日志
- en: In this technique we’ll look at ways to access task logs in the event that you
    have a problem job you want to debug.
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技术中，我们将探讨在遇到需要调试的问题作业时如何访问任务日志。
- en: Problem
  id: totrans-1088
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: Your job is failing or generating unexpected outputs, and you want to determine
    if the logs can help you figure out the problem.
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: 你的作业失败或生成意外的输出，你想确定日志是否可以帮助你找出问题。
- en: Solution
  id: totrans-1090
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Learn how to use the JobHistory or ApplicationMaster UI to view task logs. Alternatively,
    you can SSH to individual slave nodes and access the logs directly.
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: 学习如何使用作业历史或应用程序主UI查看任务日志。或者，你也可以SSH到单个从节点并直接访问日志。
- en: Discussion
  id: totrans-1092
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: When a job fails, it’s useful to look at the logs to see if they tell you anything
    about the failure. For MapReduce applications, each map and reduce task runs in
    its own container and has its own logs, so you need to identify the tasks that
    failed. The easiest way to do this is to use the JobHistory or ApplicationMaster
    UI, which in the task views provide links to the task logs.
  id: totrans-1093
  prefs: []
  type: TYPE_NORMAL
  zh: 当作业失败时，查看日志以了解它们是否提供了关于失败的信息是有用的。对于MapReduce应用程序，每个映射和归约任务都在自己的容器中运行，并有自己的日志，因此你需要识别失败的作业。最简单的方法是使用作业历史或应用程序主UI，在任务视图中提供链接到任务日志。
- en: You can also use the steps outlined in technique 80 for accessing stack dumps
    to directly access the logs on the slave node that executed a task.
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用技术80中概述的步骤直接访问执行任务的从节点上的日志。
- en: YARN will automatically delete the log files after `yarn.nodemanager.log.retain-seconds`
    seconds if log aggregation isn’t enabled, or `yarn.nodemanager.delete.debug-delay-sec`
    seconds if log aggregation is enabled.
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未启用日志聚合，YARN将在`yarn.nodemanager.log.retain-seconds`秒后自动删除日志文件，如果启用日志聚合，则将在`yarn.nodemanager.delete.debug-delay-sec`秒后删除。
- en: If a container fails to start, you’ll need to examine the NodeManager logs that
    executed the task. To do this, use the JobHistory or ApplicationMaster UI to determine
    which node executed your task, and then navigate to the NodeManager UI to examine
    its logs.
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: 如果容器启动失败，你需要检查执行该任务的NodeManager日志。为此，使用作业历史或应用程序主UI确定哪个节点执行了你的任务，然后导航到NodeManager
    UI以检查其日志。
- en: Often, when things start going wrong in your jobs, the task logs will contain
    details on the cause of the failure. Next we’ll look at how you get at the command
    used to launch a map or reduce task, which is useful when you suspect there’s
    an issue related to the environment.
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当你的作业开始出现问题时，任务日志将包含有关失败原因的详细信息。接下来，我们将看看如何获取启动映射或归约任务的命令，这在怀疑与环境相关的问题时非常有用。
- en: 8.3.2\. Accessing container start scripts
  id: totrans-1098
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.2\. 访问容器启动脚本
- en: This a useful technique in situations where you suspect there’s an issue with
    the environment or startup arguments for a container. For example, sometimes the
    classpath ordering of JARs is significant, and issues with it can cause class-loading
    problems. Also, if a container has dependencies on native libraries, the JVM arguments
    can be used to debug issues with `java.library.path`.
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种在怀疑容器环境或启动参数存在问题时的有用技巧。例如，有时 JAR 的类路径顺序很重要，其问题可能导致类加载问题。此外，如果容器依赖于原生库，可以使用
    JVM 参数来调试 `java.library.path` 的问题。
- en: Technique 83 Figuring out the container startup command
  id: totrans-1100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 83 确定容器启动命令
- en: The ability to examine the various arguments used to start a container can be
    helpful in debugging container-launching problems. For example, let’s say you’re
    trying to use a native Hadoop compression codec, but your MapReduce containers
    are failing, and the errors complain that the native compression libraries can’t
    be loaded. In this case, review the JVM startup arguments to determine if all
    of the required settings exist for native compression to work.
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: 检查启动容器所使用的各种论证能力对于调试容器启动问题非常有帮助。例如，假设你正在尝试使用原生的 Hadoop 压缩编解码器，但你的 MapReduce
    容器失败了，错误信息抱怨原生压缩库无法加载。在这种情况下，请检查 JVM 启动参数，以确定是否所有必需的设置都存在，以便原生压缩能够工作。
- en: Problem
  id: totrans-1102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You suspect that a container is failing due to missing arguments when a task
    is being launched, and you want to examine the container startup arguments.
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务启动时，你怀疑容器因缺少参数而失败，并希望检查容器启动参数。
- en: Solution
  id: totrans-1104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Set the `yarn.nodemanager.delete.debug-delay-sec` YARN configuration parameter
    to stop Hadoop from cleaning up container metadata, and use this metadata to view
    the shell script used to launch the container.
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `yarn.nodemanager.delete.debug-delay-sec` YARN 配置参数设置为停止 Hadoop 清理容器元数据，并使用此元数据来查看用于启动容器的
    shell 脚本。
- en: Discussion
  id: totrans-1106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: As the NodeManager prepares to launch a container, it creates a shell script
    that’s subsequently executed to run the container. The problem is that YARN, by
    default, removes these scripts after a job has completed. During the execution
    of a long-running application, you’ll have access to these scripts, but if the
    application is short-lived (which it may well be if you’re debugging an issue
    that causes the containers to fail off the bat), you’ll need to set `yarn.nodemanager.delete.debug-delay-sec`
    to `true`.
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: 当 NodeManager 准备启动容器时，它会创建一个随后执行的 shell 脚本以运行容器。问题是 YARN 默认情况下在作业完成后会删除这些脚本。在执行长时间运行的应用程序期间，你可以访问这些脚本，但如果应用程序是短暂的（如果你正在调试导致容器立即失败的错误，这种情况可能很常见），你需要将
    `yarn.nodemanager.delete.debug-delay-sec` 设置为 `true`。
- en: '[Figure 8.11](#ch08fig11) shows all of the steps required to gain access to
    the task shell script.'
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8.11](#ch08fig11) 展示了获取任务 shell 脚本所需的所有步骤。'
- en: Figure 8.11\. How to get to the launch_container.sh script
  id: totrans-1109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.11\. 如何获取到 launch_container.sh 脚本
- en: '![](08fig11_alt.jpg)'
  id: totrans-1110
  prefs: []
  type: TYPE_IMG
  zh: '![08fig11_alt](08fig11_alt.jpg)'
- en: It’s also useful to examine the logs of the NodeManager that attempted to launch
    the container, as they may contain container startup errors. Also double-check
    the container logs if they exist.
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: 检查尝试启动容器的 NodeManager 的日志也是有用的，因为它们可能包含容器启动错误。如果存在，请再次检查容器日志。
- en: Summary
  id: totrans-1112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: This technique is useful in situations where you want to be able to examine
    the arguments used to launch the container. If the data in the logs suggests that
    the problem with your job is with the inputs (which can be manifested by a parsing
    exception), you’ll need to figure out what kind of input is causing the problem.
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技巧在你想检查用于启动容器的参数时很有用。如果日志中的数据表明你的作业问题出在输入上（这可以通过解析异常表现出来），你需要找出导致问题的输入类型。
- en: 8.3.3\. Debugging OutOfMemory errors
  id: totrans-1114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.3\. 调试 OutOfMemory 错误
- en: OutOfMemory (OOM) errors are common in Java applications that have memory leaks
    or are trying to store too much data in memory. These memory errors can be difficult
    to track down, because often you aren’t provided enough information when a container
    exits.
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
  zh: OutOfMemory (OOM) 错误在具有内存泄漏或试图在内存中存储过多数据的 Java 应用程序中很常见。这些内存错误可能很难追踪，因为当容器退出时，通常不会提供足够的信息。
- en: Technique 84 Force container JVMs to generate a heap dump
  id: totrans-1116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 84 强制容器 JVM 生成堆转储
- en: In this technique you’ll see some useful JVM arguments that will cause Java
    to write a heap dump to disk when OOM errors occur.
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技巧中，你会看到一些有用的 JVM 参数，当发生 OOM 错误时，这些参数会导致 Java 将堆转储写入磁盘。
- en: Problem
  id: totrans-1118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: Containers are failing with OutOfMemory errors.
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: 容器因内存不足错误而失败。
- en: Solution
  id: totrans-1120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Update the container JVM arguments to include `-XX:+HeapDumpOnOutOfMemoryError
    -XX:HeapDumpPath=<path>`, where `<path>` is a common directory across all your
    slave nodes.
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: 将容器JVM参数更新为包含`-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=<path>`，其中`<path>`是跨所有从节点的一个常见目录。
- en: Discussion
  id: totrans-1122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: If you’re running a MapReduce application, you can update `mapred.child.java.opts`
    with the preceding JVM arguments. For non-MapReduce applications, you’ll need
    to figure out how to append these JVM startup arguments for the container exhibiting
    the OOM errors.
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在运行MapReduce应用程序，您可以使用前述JVM参数更新`mapred.child.java.opts`。对于非MapReduce应用程序，您需要找出如何为出现OOM错误的容器附加这些JVM启动参数。
- en: Once a container running with the preceding JVM arguments fails, you can load
    the generated dump file using jmap or your favorite profiling tool.
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦运行前述JVM参数的容器失败，您可以使用jmap或您喜欢的分析工具加载生成的转储文件。
- en: 8.3.4\. MapReduce coding guidelines for effective debugging
  id: totrans-1125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.4\. 有效的调试的MapReduce编码指南
- en: Debugging MapReduce code in production can be made a lot easier if you follow
    a handful of logging and exception-handling best practices.
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您遵循一些日志记录和异常处理最佳实践，则可以在生产环境中使MapReduce代码的调试变得更加容易。
- en: Technique 85 Augmenting MapReduce code for better debugging
  id: totrans-1127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧85：增强MapReduce代码以更好地调试
- en: Debugging a poorly written MapReduce job consumes a lot of time and can be challenging
    in production environments where access to cluster resources is limited.
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: 调试编写不良的MapReduce作业会消耗大量时间，并且在集群资源访问受限的生产环境中可能具有挑战性。
- en: Problem
  id: totrans-1129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to know the best practices to follow when writing MapReduce code.
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
  zh: 您想知道在编写MapReduce代码时应遵循的最佳实践。
- en: Solution
  id: totrans-1131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Look at how counters and logs can be used to enhance your ability to effectively
    debug and handle problem jobs.
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
  zh: 看看计数器和日志如何被用来增强您有效地调试和处理问题作业的能力。
- en: Discussion
  id: totrans-1133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Add the following features to your code:'
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下功能添加到您的代码中：
- en: Include logs that capture data related to inputs and outputs to help isolate
    where problems exist.
  id: totrans-1135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含捕获与输入和输出相关的数据的日志，以帮助隔离问题所在。
- en: Catch exceptions and provide meaningful logging output to help track down problem
    data inputs and logic errors.
  id: totrans-1136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕获异常并提供有意义的日志输出，以帮助追踪问题数据输入和逻辑错误。
- en: Think about whether you want to rethrow or swallow exceptions in your code.
  id: totrans-1137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑您是否想在代码中重新抛出或吞没异常。
- en: Use counters and task statuses that can be utilized by driver code and humans
    alike to better understand what happened during the job execution.
  id: totrans-1138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用计数器和任务状态，这些可以被驱动代码和人类 alike 利用，以更好地理解作业执行期间发生的情况。
- en: In the following code, you’ll see a number of the previously described principles
    applied.
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，您将看到许多之前描述的原则被应用。
- en: Listing 8.1\. A mapper job with some best practices applied to assist debugging
  id: totrans-1140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.1\. 应用了一些最佳实践以协助调试的mapper作业
- en: '![](ch08ex01-0.jpg)'
  id: totrans-1141
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch08ex01-0.jpg)'
- en: '![](ch08ex01-1.jpg)'
  id: totrans-1142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch08ex01-1.jpg)'
- en: The reduce task should have similar debug log statements added to write out
    each reduce input key and value and the output key and value. Doing so will help
    identify any issues between the map and reduce sides, in your reduce code, or
    in the `OutputFormat` or `RecordWriter`.
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
  zh: 减少任务应该添加类似的调试日志语句，以输出每个减少输入键和值以及输出键和值。这样做将有助于识别映射和减少之间的任何问题，或者在您的减少代码中，或者在`OutputFormat`或`RecordWriter`中。
- en: '|  |'
  id: totrans-1144
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Should exceptions be swallowed?
  id: totrans-1145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 应该吞没异常吗？
- en: In the previous code example, you caught any exceptions in your code and then
    wrote the exception to the logs, along with as much contextual information as
    possible (such as the current key and value that the reducer was working on).
    The big question is whether you should rethrow the exception or swallow it.
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码示例中，您在代码中捕获了任何异常，并将异常写入日志，同时尽可能多地包含上下文信息（例如，当前reducer正在处理的关键值）。主要问题是您是否应该重新抛出异常或吞没它。
- en: Rethrowing the exception is tempting because you’ll immediately be aware of
    any issues in your MapReduce code. But if your code is running in production and
    fails every time it encounters a problem—such as some input data that’s not handled
    correctly—the ops, dev, and QA teams will be spending quite a few cycles addressing
    each issue as it comes along.
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
  zh: 重新抛出异常很诱人，因为您将立即意识到MapReduce代码中的任何问题。但如果您的代码在生产环境中运行，并且每次遇到问题（例如一些未正确处理的数据输入）时都会失败，那么操作、开发和QA团队将花费大量时间解决每个问题。
- en: Writing code that swallows exceptions has its own problems—for example, what
    if you encounter an exception on all inputs to the job? If you write your code
    to swallow exceptions, the correct approach is to increment a counter (as in the
    code example), which the driver class should use after job completion to ensure
    that most of the input records within some tolerable threshold were successfully
    processed. If they weren’t, the workflow being processed should probably be terminated
    and the appropriate alerts be sent to notify operations.
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: 编写会吞掉异常的代码有其自身的问题——例如，如果您在作业的所有输入上遇到异常怎么办？如果您编写代码来吞掉异常，正确的方法是增加一个计数器（如代码示例所示），驱动类应在作业完成后使用它来确保大多数输入记录在可接受的阈值内都得到了成功处理。如果没有，正在处理的流程可能需要终止，并发出适当的警报以通知操作。
- en: Another approach is to not swallow exceptions and to configure record skipping
    with a call to `setMapperMaxSkipRecords` or `setReducerMaxSkipGroups`, indicating
    the number of records that you can tolerate losing if an exception is thrown when
    they’re processed. This is covered in more detail in *Hadoop in Action* by Chuck
    Lam (Manning, 2010).
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是不要吞掉异常，并通过调用`setMapperMaxSkipRecords`或`setReducerMaxSkipGroups`来配置记录跳过，这表示在处理时抛出异常时可以容忍丢失的记录数量。这在Chuck
    Lam的《Hadoop in Action》（Manning, 2010）中有更详细的介绍。
- en: '|  |'
  id: totrans-1150
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: You used counters to count the number of bad records you encountered, and the
    ApplicationMaster or JobHistory UI can be used to view the counter values, as
    shown in [figure 8.12](#ch08fig12).
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
  zh: 您使用计数器来统计遇到的坏记录数量，并且可以使用ApplicationMaster或JobHistory UI来查看计数器值，如图8.12所示[链接](https://example.org)。
- en: Figure 8.12\. Screenshot of a counter in the JobHistory counter page
  id: totrans-1152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.12. JobHistory计数器页面上的计数器截图
- en: '![](08fig12_alt.jpg)'
  id: totrans-1153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig12_alt.jpg)'
- en: 'Depending on how you executed the job, you’ll see the counters dumped on standard
    out. If you look at the logs for your tasks, you’ll also see some informative
    data related to the task:'
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您如何执行作业，您将在标准输出上看到计数器。如果您查看任务的日志，您也会看到一些与任务相关的信息性数据：
- en: '![](368fig01_alt.jpg)'
  id: totrans-1155
  prefs: []
  type: TYPE_IMG
  zh: '![图片](368fig01_alt.jpg)'
- en: Because you also updated the task status in your code, you can use the ApplicationMaster
    or JobHistory UI to easily identify the tasks that had failed records, as shown
    in [figure 8.13](#ch08fig13).
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您也在代码中更新了任务状态，因此可以使用ApplicationMaster或JobHistory UI轻松地识别出有失败记录的任务，如图8.13所示[链接](https://example.org)。
- en: Figure 8.13\. JobTracker UI showing map task and status
  id: totrans-1157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.13. 显示映射任务和状态的JobTracker UI
- en: '![](08fig13_alt.jpg)'
  id: totrans-1158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig13_alt.jpg)'
- en: Summary
  id: totrans-1159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: We looked at a handful of simple yet useful coding guidelines for your MapReduce
    code. If they’re applied and you hit a problem with your job in production, you’ll
    be in a great position to quickly narrow down the root cause of the issue. If
    the issue is related to the input, your logs will contain details about how the
    input caused your processing logic to fail. If the issue is related to some logic
    error or errors in serialization or deserialization, you can enable debug-level
    logging and better understand where things are going awry.
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了适用于您的MapReduce代码的一些简单而实用的编码指南。如果它们得到应用，并且您在生产作业中遇到问题，您将能够快速缩小问题的根本原因。如果问题是与输入相关，您的日志将包含有关输入如何导致您的处理逻辑失败的具体细节。如果问题是与某些逻辑错误或序列化/反序列化错误相关，您可以启用调试级别的日志记录，更好地了解事情出错的地方。
- en: 8.4\. Testing MapReduce jobs
  id: totrans-1161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4. 测试MapReduce作业
- en: In this section we’ll look at the best methods for testing your MapReduce code,
    as well as design aspects to consider when writing MapReduce jobs to help in your
    testing efforts.
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨测试MapReduce代码的最佳方法，以及编写MapReduce作业时需要考虑的设计方面，以帮助您在测试工作中。
- en: 8.4.1\. Essential ingredients for effective unit testing
  id: totrans-1163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.4.1. 有效的单元测试的基本要素
- en: It’s important to make sure unit tests are easy to write and to ensure that
    they cover a good spectrum of positive and negative scenarios. Let’s take a look
    at the impact that test-driven development, code design, and data have on writing
    effective unit tests.
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
  zh: 确保单元测试易于编写，并确保它们覆盖了良好的正负场景范围，这是非常重要的。让我们看看测试驱动开发、代码设计和数据对编写有效的单元测试的影响。
- en: Test-driven development
  id: totrans-1165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 测试驱动开发
- en: When it comes to writing Java code, I’m a big proponent of test-driven development
    (TDD),^([[10](#ch08fn10)]) and with MapReduce things are no different. Test-driven
    development emphasizes writing unit tests ahead of writing the code, and it recently
    has gained in importance as quick development turnaround times have become the
    norm rather than the exception. Applying test-driven development to MapReduce
    code is crucial, particularly when such code is part of a critical production
    application.
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到编写Java代码时，我强烈支持测试驱动开发（TDD），[10](#ch08fn10)]，并且对于MapReduce来说，情况也没有不同。测试驱动开发强调在编写代码之前编写单元测试，并且随着快速开发周转时间成为常态而非例外，它最近的重要性有所增加。将测试驱动开发应用于MapReduce代码至关重要，尤其是当此类代码是关键生产应用程序的一部分时。
- en: '^(10) For an explanation of test-driven development, see the Wikipedia article:
    [http://en.wikipedia.org/wiki/Test-driven_development](http://en.wikipedia.org/wiki/Test-driven_development).'
  id: totrans-1167
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (10) 关于测试驱动开发的解释，请参阅维基百科文章：[http://en.wikipedia.org/wiki/Test-driven_development](http://en.wikipedia.org/wiki/Test-driven_development)。
- en: Writing unit tests prior to writing your code forces you to structure your code
    in a way that easily facilitates testing.
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写代码之前编写单元测试迫使你以易于测试的方式结构化代码。
- en: Code design
  id: totrans-1169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码设计
- en: When you write code, it’s important to think about the best way to structure
    it so you can easily test it. Using concepts such as abstraction and dependency
    injection will go a long way toward reaching this goal.^([[11](#ch08fn11)])
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
  zh: 当你编写代码时，重要的是要考虑最佳的结构方式以便于测试。使用抽象和依赖注入等概念将有助于实现这一目标。[11](#ch08fn11)]
- en: '^(11) For an explanation of dependency injection, see the Wikipedia article:
    [http://en.wikipedia.org/wiki/Dependency_injection](http://en.wikipedia.org/wiki/Dependency_injection).'
  id: totrans-1171
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (11) 关于依赖注入的解释，请参阅维基百科文章：[http://en.wikipedia.org/wiki/Dependency_injection](http://en.wikipedia.org/wiki/Dependency_injection)。
- en: When you write MapReduce code, it’s a good idea to abstract away the code doing
    the work, which means you can test that code in regular unit tests without having
    to think about how to work with Hadoop-specific constructs. This is true not only
    for your map and reduce functions, but also for your input formats, output formats,
    data serialization, and partitioner code.
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
  zh: 当你编写MapReduce代码时，抽象出执行代码是个好主意，这意味着你可以在常规单元测试中测试该代码，而无需考虑如何与Hadoop特定的结构一起工作。这不仅适用于你的映射和减少函数，也适用于你的输入格式、输出格式、数据序列化和分区器代码。
- en: 'Let’s look at a simple example to better illustrate this point. The following
    code shows a reducer that calculates the mean for a stock:'
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来更好地说明这一点。以下代码展示了一个计算股票平均值的reducer：
- en: '[PRE49]'
  id: totrans-1174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This is a trivial example, but the way the code is structured means you can’t
    easily test this in a regular unit test, because MapReduce has constructs such
    as `Text`, `DoubleWritable`, and the `Context` class that get in your way. If
    you were to structure the code to abstract away the work, you could easily test
    the code that’s doing your work, as the following code shows:'
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的例子，但代码的结构意味着你无法在常规单元测试中轻松测试它，因为MapReduce有诸如`Text`、`DoubleWritable`和`Context`类等结构会阻碍你。如果你将代码结构化以抽象出工作，你就可以轻松测试执行工作的代码，如下面的代码所示：
- en: '[PRE50]'
  id: totrans-1176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: With this improved code layout, you can now easily test the `SMA` class that’s
    adding and calculating the simple moving average without the Hadoop code getting
    in your way.
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种改进的代码布局，你现在可以轻松地测试添加和计算简单移动平均数的`SMA`类，而无需Hadoop代码干扰。
- en: It’s the data, stupid
  id: totrans-1178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据最重要
- en: When you write unit tests, you try to discover how your code handles both positive
    and negative input data. In both cases, it’s best if the data you’re testing with
    is a representative sample from production.
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: 当你编写单元测试时，你试图发现你的代码如何处理正负输入数据。在两种情况下，使用从生产中抽取的代表性样本数据进行测试都是最好的。
- en: Often, no matter how hard you try, issues in your code in production will arise
    from unexpected input data. It’s important that when you do discover input data
    that causes a job to blow up, you not only fix the code to handle the unexpected
    data, but you also pull the data that caused the blowup and use it in a unit test
    to prove that the code can now correctly handle that data.
  id: totrans-1180
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，无论你多么努力，生产中的代码问题都可能源于意外的输入数据。当你确实发现导致任务崩溃的输入数据时，你不仅需要修复代码以处理意外数据，还需要提取导致崩溃的数据，并在单元测试中使用它来证明代码现在可以正确处理该数据。
- en: 8.4.2\. MRUnit
  id: totrans-1181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.4.2\. MRUnit
- en: MRUnit is a test framework you can use to unit-test MapReduce code. It was developed
    by Cloudera (a vendor with its own Hadoop distribution) and it’s currently an
    Apache project. It should be noted that MRUnit supports both the old (`org.apache.hadoop.mapred`)
    and new (`org.apache.hadoop.mapreduce`) MapReduce APIs.
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
  zh: MRUnit 是一个你可以用来对 MapReduce 代码进行单元测试的测试框架。它是由 Cloudera（一个拥有自己 Hadoop 分发的供应商）开发的，目前是一个
    Apache 项目。需要注意的是，MRUnit 支持旧的 (`org.apache.hadoop.mapred`) 和新的 (`org.apache.hadoop.mapreduce`)
    MapReduce API。
- en: Technique 86 Using MRUnit to unit-test MapReduce
  id: totrans-1183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 86 使用 MRUnit 进行 MapReduce 单元测试
- en: 'In this technique we’ll look at writing unit tests that use each of the four
    types of tests provided by MRUnit:'
  id: totrans-1184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技术中，我们将查看编写使用 MRUnit 提供的四种测试类型之一的单元测试：
- en: '`MapDriver` class—A map test that only tests a map function'
  id: totrans-1185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MapDriver` 类——一个仅测试 map 函数的 map 测试'
- en: '`ReduceDriver` class—A reduce test that only tests a reduce function'
  id: totrans-1186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReduceDriver` 类——一个仅测试 reduce 函数的 reduce 测试'
- en: '`MapReduceDriver` class—A map and reduce test that tests both the map and reduce
    functions'
  id: totrans-1187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MapReduceDriver` 类——一个测试 map 和 reduce 函数的 map 和 reduce 测试'
- en: '`TestPipelineMapReduceDriver` class—A pipeline test that allows a series of
    Map-Reduce functions to be exercised'
  id: totrans-1188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TestPipelineMapReduceDriver` 类——一个允许一系列 Map-Reduce 函数被测试的管道测试'
- en: Problem
  id: totrans-1189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to test map and reduce functions, as well as MapReduce pipelines.
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
  zh: 你想测试 map 和 reduce 函数，以及 MapReduce 管道。
- en: Solution
  id: totrans-1191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use MRUnit’s `MapDriver`, `ReduceDriver`, `MapReduceDriver`, and `PipelineMapReduceDriver`
    classes as part of your unit tests to test your MapReduce code.
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 MRUnit 的 `MapDriver`、`ReduceDriver`、`MapReduceDriver` 和 `PipelineMapReduceDriver`
    类作为您单元测试的一部分来测试您的 MapReduce 代码。
- en: Discussion
  id: totrans-1193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: MRUnit has four types of unit tests—we’ll start with a look at the map tests.
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
  zh: MRUnit 有四种类型的单元测试——我们将从查看 map 测试开始。
- en: Map tests
  id: totrans-1195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Map 测试
- en: Let’s kick things off by writing a test to exercise a map function. Before starting,
    let’s look at what you need to supply to MRUnit to execute the test, and in the
    process learn about how MRUnit works behind the scenes.
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从编写一个测试来测试 map 函数开始。在开始之前，让我们看看你需要向 MRUnit 提供什么来执行测试，并在过程中了解 MRUnit 在幕后是如何工作的。
- en: '[Figure 8.14](#ch08fig14) shows the interactions of the unit test with MRUnit
    and how it in turn interacts with the mapper you’re testing.'
  id: totrans-1197
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8.14](#ch08fig14) 展示了单元测试与 MRUnit 的交互以及它如何反过来与你要测试的 mapper 交互。'
- en: Figure 8.14\. MRUnit test using `MapDriver`
  id: totrans-1198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.14\. 使用 `MapDriver` 的 MRUnit 测试
- en: '![](08fig14_alt.jpg)'
  id: totrans-1199
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.14 选项](08fig14_alt.jpg)'
- en: The following code is a simple unit test of the (identity) `mapper` class in
    Hadoop:^([[12](#ch08fn12)])
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是对 Hadoop 中（身份）`mapper` 类的简单单元测试：^([12](#ch08fn12)）
- en: '^(12) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/mrunit/IdentityMapTest.java](https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/mrunit/IdentityMapTest.java).'
  id: totrans-1201
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([12](#ch08fn12)) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/mrunit/IdentityMapTest.java](https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/mrunit/IdentityMapTest.java)。
- en: '![](372fig01_alt.jpg)'
  id: totrans-1202
  prefs: []
  type: TYPE_IMG
  zh: '![图 372 选项](372fig01_alt.jpg)'
- en: 'MRUnit is not tied to any specific unit-testing framework, so if it finds an
    error, it logs the error and throws an exception. Let’s see what would happen
    if your unit test had specified output that didn’t match the output of the mapper,
    as in the following code:'
  id: totrans-1203
  prefs: []
  type: TYPE_NORMAL
  zh: MRUnit 不依赖于任何特定的单元测试框架，因此如果它发现错误，它会记录错误并抛出异常。让我们看看如果您的单元测试指定了与 mapper 输出不匹配的输出会发生什么，如下代码所示：
- en: '[PRE51]'
  id: totrans-1204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'If you run this test, your test will fail, and you’ll see the following log
    output:'
  id: totrans-1205
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个测试，你的测试将失败，你将看到以下日志输出：
- en: '[PRE52]'
  id: totrans-1206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '|  |'
  id: totrans-1207
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: MRUnit logging configuration
  id: totrans-1208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: MRUnit 日志配置
- en: 'Because MRUnit uses the Apache Commons logging, which defaults to using log4j,
    you’ll need to have a log4j.properties file in the classpath that’s configured
    to write to standard out, similar to the following:'
  id: totrans-1209
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 MRUnit 使用 Apache Commons logging，默认使用 log4j，因此你需要在类路径中有一个配置为写入标准输出的 log4j.properties
    文件，类似于以下内容：
- en: '[PRE53]'
  id: totrans-1210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '|  |'
  id: totrans-1211
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: One of the powerful features of JUnit and other test frameworks is that when
    tests fail, the failure message includes details on the cause of the failure.
    Unfortunately, MRUnit logs and throws a nondescriptive exception, which means
    you need to dig through the test output to determine what failed.
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
  zh: JUnit 和其他测试框架的一个强大功能是，当测试失败时，失败信息包括导致失败的原因的详细信息。不幸的是，MRUnit 记录并抛出一个非描述性的异常，这意味着你需要挖掘测试输出以确定什么失败了。
- en: 'What if you wanted to use the power of MRUnit, and also use the informative
    errors that JUnit provides when assertions fail? You could modify your code to
    do that and bypass MRUnit’s testing code:'
  id: totrans-1213
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想使用 MRUnit 的强大功能，同时使用 JUnit 在断言失败时提供的有信息性的错误，你可以修改你的代码来实现这一点，并绕过 MRUnit 的测试代码：
- en: '![](373fig01_alt.jpg)'
  id: totrans-1214
  prefs: []
  type: TYPE_IMG
  zh: '![图片](373fig01_alt.jpg)'
- en: 'With this approach, if there’s a mismatch between the expected and actual outputs,
    you get a more meaningful error message, which report-generation tools can use
    to easily describe what failed in the test:'
  id: totrans-1215
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，如果期望输出和实际输出之间有差异，你会得到一个更有意义的错误消息，报告生成工具可以使用它来轻松描述测试中失败的内容：
- en: '[PRE54]'
  id: totrans-1216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'To cut down on the inevitable copy-paste activities with this approach, I wrote
    a simple helper class to use JUnit asserts in combination with using the MRUnit
    driver.^([[13](#ch08fn13)]) Your JUnit test now looks like this:'
  id: totrans-1217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少使用这种方法不可避免地进行的复制粘贴活动，我编写了一个简单的辅助类，结合使用 MRUnit 驱动程序和 JUnit 断言.^([[13](#ch08fn13)])
    你的 JUnit 测试现在看起来像这样：
- en: '^(13) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/mrunit/MRUnitJUnitAsserts.java](https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/mrunit/MRUnitJUnitAsserts.java)'
  id: totrans-1218
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (13) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/mrunit/MRUnitJUnitAsserts.java](https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/mrunit/MRUnitJUnitAsserts.java)
- en: '![](373fig02_alt.jpg)'
  id: totrans-1219
  prefs: []
  type: TYPE_IMG
  zh: '![图片](373fig02_alt.jpg)'
- en: This is much cleaner and removes any mistakes that might arise from the copy-paste
    anti-pattern.
  id: totrans-1220
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做更干净，并消除了可能由复制粘贴反模式引起的任何错误。
- en: Reduce tests
  id: totrans-1221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Reduce 测试
- en: Now that we’ve looked at map function tests, let’s look at reduce function tests.
    The MRUnit framework takes a similar approach for reduce testing. [Figure 8.15](#ch08fig15)
    shows the interactions of your unit test with MRUnit, and how it in turn interacts
    with the reducer you’re testing.
  id: totrans-1222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看了 map 函数的测试，让我们看看 reduce 函数的测试。MRUnit 框架在 reduce 测试中采用类似的方法。[图 8.15](#ch08fig15)
    展示了你的单元测试与 MRUnit 的交互，以及它如何反过来与你要测试的 reducer 交互。
- en: Figure 8.15\. MRUnit test using `ReduceDriver`
  id: totrans-1223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.15\. 使用 ReduceDriver 的 MRUnit 测试
- en: '![](08fig15_alt.jpg)'
  id: totrans-1224
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig15_alt.jpg)'
- en: The following code is a simple unit test for testing the (identity) reducer
    class in Hadoop:^([[14](#ch08fn14)])
  id: totrans-1225
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是测试 Hadoop 中（身份）reducer 类的简单单元测试:^([[14](#ch08fn14)])
- en: '^(14) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/mrunit/IdentityReduceTest.java](https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/mrunit/IdentityReduceTest.java).'
  id: totrans-1226
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (14) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/mrunit/IdentityReduceTest.java](https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/mrunit/IdentityReduceTest.java).
- en: '![](374fig01_alt.jpg)'
  id: totrans-1227
  prefs: []
  type: TYPE_IMG
  zh: '![图片](374fig01_alt.jpg)'
- en: Now that we’ve completed our look at the individual map and reduce function
    tests, let’s look at how you can test a map and reduce function together.
  id: totrans-1228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了对单个 map 和 reduce 函数测试的查看，让我们看看如何一起测试 map 和 reduce 函数。
- en: MapReduce tests
  id: totrans-1229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: MapReduce 测试
- en: MRUnit also supports testing the map and reduce functions in the same test.
    You feed MRUnit the inputs, which in turn are supplied to the mapper. You also
    tell MRUnit what reducer outputs you expect.
  id: totrans-1230
  prefs: []
  type: TYPE_NORMAL
  zh: MRUnit 也支持在同一测试中测试 map 和 reduce 函数。你将输入提供给 MRUnit，这些输入随后被传递给 mapper。你还需要告诉 MRUnit
    你期望的 reducer 输出。
- en: '[Figure 8.16](#ch08fig16) shows the interactions of your unit test with MRUnit
    and how it in turn interacts with the mapper and reducer you’re testing.'
  id: totrans-1231
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8.16](#ch08fig16) 展示了你的单元测试与 MRUnit 的交互，以及它如何反过来与你要测试的 mapper 和 reducer
    交互。'
- en: Figure 8.16\. MRUnit `test using MapReduceDriver`
  id: totrans-1232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.16\. 使用 MapReduceDriver 的 MRUnit 测试
- en: '![](08fig16_alt.jpg)'
  id: totrans-1233
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig16_alt.jpg)'
- en: The following code is a simple unit test for testing the (identity) mapper and
    reducer classes in Hadoop:^([[15](#ch08fn15)])
  id: totrans-1234
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是测试 Hadoop 中（身份）mapper 和 reducer 类的简单单元测试:^([[15](#ch08fn15)])
- en: '^(15) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/mrunit/IdentityMapReduceTest.java](https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/mrunit/IdentityMapReduceTest.java).'
  id: totrans-1235
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (15) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/mrunit/IdentityMapReduceTest.java](https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/mrunit/IdentityMapReduceTest.java).
- en: '![](375fig01_alt.jpg)'
  id: totrans-1236
  prefs: []
  type: TYPE_IMG
  zh: '![图片](375fig01_alt.jpg)'
- en: Now we’ll look at the fourth and final type of test that MRUnit supports, pipeline
    tests, which are used to test multiple MapReduce jobs.
  id: totrans-1237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看看 MRUnit 支持的第四种和最后一种测试类型，即管道测试，它用于测试多个 MapReduce 作业。
- en: Pipeline tests
  id: totrans-1238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 管道测试
- en: MRUnit supports testing a series of map and reduce functions—these are called
    *pipeline tests*. You feed MRUnit one or more MapReduce functions, the inputs
    to the first map function, and the expected outputs of the last reduce function.
  id: totrans-1239
  prefs: []
  type: TYPE_NORMAL
  zh: MRUnit 支持测试一系列的 map 和 reduce 函数——这些被称为 *管道测试*。你向 MRUnit 提供一个或多个 MapReduce 函数、第一个
    map 函数的输入以及最后一个 reduce 函数的预期输出。
- en: '[Figure 8.17](#ch08fig17) shows the interactions of your unit test with the
    MRUnit pipeline driver.'
  id: totrans-1240
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8.17](#ch08fig17) 展示了你的单元测试与 MRUnit 管道驱动程序之间的交互。'
- en: Figure 8.17\. MRUnit test using `PipelineMapReduceDriver`
  id: totrans-1241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.17\. 使用 `PipelineMapReduceDriver` 的 MRUnit 测试
- en: '![](08fig17_alt.jpg)'
  id: totrans-1242
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig17_alt.jpg)'
- en: The following code is a unit test for testing a pipeline containing two sets
    of (identity) mapper and reducer classes in Hadoop:^([[16](#ch08fn16)])
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是一个单元测试，用于测试包含两组（身份）映射器和减少器类的 Hadoop 中的管道：^([[16](#ch08fn16)])
- en: '^(16) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/mrunit/PipelineTest.java](https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/mrunit/PipelineTest.java).'
  id: totrans-1244
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(16) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/mrunit/PipelineTest.java](https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/mrunit/PipelineTest.java)。
- en: '![](377fig01_alt.jpg)'
  id: totrans-1245
  prefs: []
  type: TYPE_IMG
  zh: '![](377fig01_alt.jpg)'
- en: Note that the `PipelineMapReduceDriver` is the only driver in MRUnit that doesn’t
    come in both old and new MapReduce API versions, which is why the preceding code
    uses the old MapReduce API.
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`PipelineMapReduceDriver` 是 MRUnit 中唯一一个既不提供旧版也不提供新版 MapReduce API 的驱动程序，这就是为什么前面的代码使用了旧版
    MapReduce API。
- en: Summary
  id: totrans-1247
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: What type of test should you use for your code? Take a look at [table 8.5](#ch08table05)
    for some pointers.
  id: totrans-1248
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该使用哪种类型的测试来测试你的代码？请查看 [表 8.5](#ch08table05) 以获取一些提示。
- en: Table 8.5\. MRUnit tests and when to use them
  id: totrans-1249
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 8.5\. MRUnit 测试及其使用情况
- en: '| Type of test | Works well in these situations |'
  id: totrans-1250
  prefs: []
  type: TYPE_TB
  zh: '| 测试类型 | 在这些情况下表现良好 |'
- en: '| --- | --- |'
  id: totrans-1251
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Map | You have a map-only job, and you want low-level unit tests where the
    framework takes care of testing the expected map outputs for your test map inputs.
    |'
  id: totrans-1252
  prefs: []
  type: TYPE_TB
  zh: '| Map | 你有一个只包含 map 的作业，并且你想要低级别的单元测试，其中框架负责测试你的测试 map 输入的预期 map 输出。|'
- en: '| Reduce | Your job has a lot of complexity in the reduce function, and you
    want to isolate your tests to only that function. |'
  id: totrans-1253
  prefs: []
  type: TYPE_TB
  zh: '| Reduce | 你的作业在 reduce 函数中有很多复杂性，你想要隔离测试只针对该函数。|'
- en: '| MapReduce | You want to test the combination of the map and reduce functions.
    These are higher-level unit tests. |'
  id: totrans-1254
  prefs: []
  type: TYPE_TB
  zh: '| MapReduce | 你想要测试 map 和 reduce 函数的组合。这些是更高级别的单元测试。|'
- en: '| Pipeline | You have a MapReduce pipeline where the input of each MapReduce
    job is the output from the previous job. |'
  id: totrans-1255
  prefs: []
  type: TYPE_TB
  zh: '| 管道 | 你有一个 MapReduce 管道，其中每个 MapReduce 作业的输入是前一个作业的输出。|'
- en: 'MRUnit has a few limitations, some of which we touched on in this technique:'
  id: totrans-1256
  prefs: []
  type: TYPE_NORMAL
  zh: MRUnit 有一些限制，其中一些我们在本技术中提到了：
- en: MRUnit isn’t integrated with unit-test frameworks that provide rich error-reporting
    capabilities for quicker determination of errors.
  id: totrans-1257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MRUnit 没有与提供丰富错误报告功能的单元测试框架集成，这有助于更快地确定错误。
- en: The pipeline tests only work with the old MapReduce API, so MapReduce code that
    uses the new MapReduce API can’t be tested with the pipeline tests.
  id: totrans-1258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道测试仅适用于旧版 MapReduce API，因此使用新版 MapReduce API 的 MapReduce 代码无法使用管道测试进行测试。
- en: There’s no support for testing data serialization, or `InputFormat`, `RecordReader`,
    `OutputFormat`, or `RecordWriter` classes.
  id: totrans-1259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有支持测试数据序列化，或 `InputFormat`、`RecordReader`、`OutputFormat` 或 `RecordWriter` 类。
- en: Notwithstanding these limitations, MRUnit is an excellent test framework that
    you can use to test at the granular level of individual map and reduce functions;
    MRUnit also can test a pipeline of MapReduce jobs. And because it skips the `InputFormat`
    and `OutputFormat` steps, your unit tests will execute quickly.
  id: totrans-1260
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些限制，MRUnit 仍然是一个优秀的测试框架，你可以用它来测试单个 map 和 reduce 函数的粒度级别；MRUnit 还可以测试 MapReduce
    作业的管道。而且因为它跳过了 `InputFormat` 和 `OutputFormat` 步骤，所以你的单元测试将快速执行。
- en: Next we’ll look at how you can use the `LocalJobRunner` to test some MapReduce
    constructs that are ignored by MRUnit.
  id: totrans-1261
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何使用 `LocalJobRunner` 来测试 MRUnit 忽略的一些 MapReduce 构造。
- en: 8.4.3\. LocalJobRunner
  id: totrans-1262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.4.3\. LocalJobRunner
- en: In the last section we looked at MRUnit, a great, lightweight unit-test library.
    But what if you want to test not only your map and reduce functions, but also
    the `InputFormat`, `RecordReader`, `OutputFormat`, and `RecordWriter` code, as
    well as the data serialization between the map and reduce phases? This becomes
    important if you’ve written your own input and output format classes, because
    you want to make sure you’re testing that code too.
  id: totrans-1263
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们探讨了 MRUnit，这是一个优秀的轻量级单元测试库。但如果你不仅想测试你的 map 和 reduce 函数，还想测试 `InputFormat`、`RecordReader`、`OutputFormat`
    和 `RecordWriter` 代码，以及 map 和 reduce 阶段之间的数据序列化，那会怎样？如果你已经编写了自己的输入和输出格式类，这一点尤为重要，因为你想要确保你也测试了那段代码。
- en: Hadoop comes bundled with the `LocalJobRunner` class, which Hadoop and related
    projects (such as Pig and Avro) use to write and test their MapReduce code. `LocalJobRunner`
    allows you to test all the aspects of a MapReduce job, including the reading and
    writing of data to and from the filesystem.
  id: totrans-1264
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 随带提供了 `LocalJobRunner` 类，Hadoop 和相关项目（如 Pig 和 Avro）使用它来编写和测试他们的 MapReduce
    代码。`LocalJobRunner` 允许你测试 MapReduce 作业的所有方面，包括数据在文件系统中的读写。
- en: Technique 87 Heavyweight job testing with the LocalJobRunner
  id: totrans-1265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 87：使用 `LocalJobRunner` 进行重量级作业测试
- en: Tools like MRUnit are useful for low-level unit tests, but how can you be sure
    that your code will play nicely with the whole Hadoop stack?
  id: totrans-1266
  prefs: []
  type: TYPE_NORMAL
  zh: 像 MRUnit 这样的工具对于低级单元测试很有用，但你如何确保你的代码能够与整个 Hadoop 堆栈良好地协同工作？
- en: Problem
  id: totrans-1267
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to test the whole Hadoop stack in your unit test.
  id: totrans-1268
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在单元测试中测试整个 Hadoop 堆栈。
- en: Solution
  id: totrans-1269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the `LocalJobRunner` class in Hadoop to expand the coverage of your tests
    to include code related to processing job inputs and outputs.
  id: totrans-1270
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Hadoop 中的 `LocalJobRunner` 类来扩展测试范围，包括与作业输入和输出相关的代码。
- en: Discussion
  id: totrans-1271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Using the `LocalJobRunner` makes your unit tests start to feel more like integration
    tests, because what you’re doing is testing how your code works in combination
    with the whole MapReduce stack. This is great because you can use this to test
    not only your own MapReduce code, but also to test input and output formats, partitioners,
    and advanced sort mechanisms.
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `LocalJobRunner` 使得你的单元测试开始感觉更像集成测试，因为你正在测试你的代码如何与整个 MapReduce 堆栈结合工作。这很好，因为你可以使用这个来测试不仅你的
    MapReduce 代码，还可以测试输入和输出格式、分区器以及高级排序机制。
- en: The code in the next listing shows an example of how you can use the `LocalJobRunner`
    in your unit tests.^([[17](#ch08fn17)])
  id: totrans-1273
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表中的代码展示了如何在你的单元测试中使用 `LocalJobRunner` 的示例：^([17](#ch08fn17))
- en: ^(17) [https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/localjobrunner/IdentityTest.java](https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/localjobrunner/IdentityTest.java).
  id: totrans-1274
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([17](#ch08fn17)) [https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/localjobrunner/IdentityTest.java](https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/localjobrunner/IdentityTest.java).
- en: Listing 8.2\. Using `LocalJobRunner` to test a MapReduce job
  id: totrans-1275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.2：使用 `LocalJobRunner` 测试 MapReduce 作业
- en: '![](ch08ex02-0.jpg)'
  id: totrans-1276
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch08ex02-0.jpg)'
- en: '![](ch08ex02-1.jpg)'
  id: totrans-1277
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch08ex02-1.jpg)'
- en: Writing this test is more involved because you need to handle writing the inputs
    to the filesystem and reading them back out. That’s a lot of boilerplate code
    to have to deal with for every test, and it’s probably something that you’ll want
    to factor out into a reusable helper class.
  id: totrans-1278
  prefs: []
  type: TYPE_NORMAL
  zh: 编写这个测试更复杂，因为你需要处理将输入写入到文件系统以及读取它们。这对于每个测试来说都是一大堆样板代码，这可能是你想要提取到可重用辅助类中的东西。
- en: Here’s an example of a utility class to do that; the following code shows how
    `IdentityTest` code can be condensed into a more manageable size:^([[18](#ch08fn18)])
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个实现该功能的实用类示例；以下代码展示了如何将 `IdentityTest` 代码压缩成更易于管理的尺寸：^([18](#ch08fn18))
- en: '^(18) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/localjobrunner/IdentityWithBuilderTest.java](https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/localjobrunner/IdentityWithBuilderTest.java).'
  id: totrans-1280
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([18](#ch08fn18)) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/localjobrunner/IdentityWithBuilderTest.java](https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/localjobrunner/IdentityWithBuilderTest.java).
- en: '![](380fig01_alt.jpg)'
  id: totrans-1281
  prefs: []
  type: TYPE_IMG
  zh: '![图片](380fig01_alt.jpg)'
- en: Summary
  id: totrans-1282
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: What are some of the limitations to be aware of when using `LocalJobRunner`?
  id: totrans-1283
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `LocalJobRunner` 时需要注意哪些限制？
- en: '`LocalJobRunner` runs only a single reduce task, so you can’t use it to test
    partitioners.'
  id: totrans-1284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LocalJobRunner` 只运行单个 reduce 任务，因此你不能用它来测试分区器。'
- en: As you saw, it’s also more labor intensive; you need to read and write the input
    and output data to the filesystem.
  id: totrans-1285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如你所见，这同样需要更多的劳动强度；你需要将输入和输出数据读取和写入到文件系统中。
- en: Jobs are also slow because much of the MapReduce stack is being exercised.
  id: totrans-1286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作业也运行缓慢，因为 MapReduce 堆栈的大部分都在被测试。
- en: It’s tricky to use this approach to test input and output formats that aren’t
    file-based.
  id: totrans-1287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这种方法来测试非基于文件的输入和输出格式是有些棘手的。
- en: The most comprehensive way of testing your code is covered next. It uses an
    in-memory cluster that can run multiple mappers and reducers.
  id: totrans-1288
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分将介绍测试你代码的最全面的方法。它使用一个内存中的集群，可以运行多个映射器和减少器。
- en: 8.4.4\. MiniMRYarnCluster
  id: totrans-1289
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.4.4\. MiniMRYarnCluster
- en: All of the unit-testing techniques so far have had restrictions on which parts
    of a MapReduce job could be tested. The `LocalJobRunner`, for example, only runs
    with a single map and reduce task, so you can’t simulate production jobs that
    run with multiple tasks. In this section you’ll learn about a built-in mechanism
    in Hadoop that allows you to exercise your job against the full-stack Hadoop.
  id: totrans-1290
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，所有的单元测试技术都对 MapReduce 作业可以测试的部分有所限制。例如，`LocalJobRunner` 只能运行单个映射和减少任务，因此你不能模拟运行多个任务的作业。在本节中，你将了解
    Hadoop 中允许你在全堆栈 Hadoop 上测试作业的内置机制。
- en: Technique 88 Using MiniMRYarnCluster to test your jobs
  id: totrans-1291
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 88 使用 MiniMRYarnCluster 测试你的作业
- en: The `MiniMRYarnCluster` class is included in the Hadoop testing code, and it
    supports test cases that require the complete Hadoop stack to be executed. This
    includes tests that need to test input and output format classes, including output
    committers, which can’t be tested using MRUnit or `LocalTestRunner`. In this technique,
    you’ll see how to use `MiniMRYarnCluster`.
  id: totrans-1292
  prefs: []
  type: TYPE_NORMAL
  zh: '`MiniMRYarnCluster` 类包含在 Hadoop 测试代码中，并支持需要执行完整 Hadoop 堆栈的测试用例。这包括需要测试输入和输出格式类，包括输出提交者，这些类不能使用
    MRUnit 或 `LocalTestRunner` 进行测试。在这个技术中，你将看到如何使用 `MiniMRYarnCluster`。'
- en: Problem
  id: totrans-1293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to execute your tests against an actual Hadoop cluster, giving you
    the additional assurance that your jobs work as expected.
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望针对实际的 Hadoop 集群执行你的测试，这为你提供了额外的保证，即你的作业按预期工作。
- en: Solution
  id: totrans-1295
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use `MiniMRYarnCluster` and `MiniDFSCluster`, which allow you to launch in-memory
    YARN and HDFS clusters.
  id: totrans-1296
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `MiniMRYarnCluster` 和 `MiniDFSCluster`，它们允许你启动内存中的 YARN 和 HDFS 集群。
- en: Discussion
  id: totrans-1297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: '`MiniMRYarnCluster` and `MiniDFSCluster` are classes contained in Hadoop’s
    testing code, and are used by various tests in Hadoop. They provide in-process
    YARN and HDFS clusters that give you the most realistic environment within which
    to test your code. These classes wrap the full-fledged YARN and HDFS processes,
    so you’re actually running the complete Hadoop stack in your test process. Map
    and reduce containers are launched as processes external to the test process.'
  id: totrans-1298
  prefs: []
  type: TYPE_NORMAL
  zh: '`MiniMRYarnCluster` 和 `MiniDFSCluster` 是包含在 Hadoop 测试代码中的类，并被 Hadoop 中的各种测试所使用。它们提供了进程内的
    YARN 和 HDFS 集群，为你提供了一个最真实的环境来测试你的代码。这些类封装了完整的 YARN 和 HDFS 进程，因此你实际上在你的测试过程中运行了完整的
    Hadoop 堆栈。映射和减少容器作为测试过程外部的进程启动。'
- en: There’s a useful wrapper class called `ClusterMapReduceTestCase` that encapsulates
    these classes and makes it easy to quickly write your unit tests. The following
    code shows a simple test case that tests the identity mapper and reducer:^([[19](#ch08fn19)])
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个有用的包装类 `ClusterMapReduceTestCase`，它封装了这些类，并使得快速编写单元测试变得容易。以下代码展示了一个简单的测试用例，该测试用例测试了身份映射器和减少器：^([[19](#ch08fn19)])
- en: '^(19) GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/minimrcluster/IdentityMiniTest.java](https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/minimrcluster/IdentityMiniTest.java).'
  id: totrans-1300
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(19) GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/minimrcluster/IdentityMiniTest.java](https://github.com/alexholmes/hiped2/blob/master/src/test/java/hip/ch8/minimrcluster/IdentityMiniTest.java).
- en: '![](381fig01_alt.jpg)'
  id: totrans-1301
  prefs: []
  type: TYPE_IMG
  zh: '![](381fig01_alt.jpg)'
- en: Summary
  id: totrans-1302
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 概述
- en: The only disadvantage to using the mini-clusters is the overhead of running
    your tests—each test class that extends `ClusterMapReduceTestCase` will result
    in a cluster being brought up and down, and each test has a considerable amount
    of time overhead because the full Hadoop stack is being executed.
  id: totrans-1303
  prefs: []
  type: TYPE_NORMAL
  zh: 使用微型集群的唯一缺点是运行测试的开销——每个扩展 `ClusterMapReduceTestCase` 的测试类都会导致集群的启动和关闭，并且每个测试都有相当多的时间开销，因为完整的
    Hadoop 堆栈正在执行。
- en: But using the mini-clusters will provide you with the greatest assurance that
    your code will work as you expect in production, and it’s worth considering for
    jobs that are critical to your organization.
  id: totrans-1304
  prefs: []
  type: TYPE_NORMAL
  zh: 但使用这些微型集群将为你提供最大的保证，即你的代码将在生产环境中按预期工作，这对于你组织中的关键作业来说值得考虑。
- en: Therefore, the optimal way to test your code is to use MRUnit for simpler jobs
    that use built-in Hadoop input and output classes, and only use this technique
    for test cases where you want to test input and output classes and output committers.
  id: totrans-1305
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，测试你的代码的最佳方式是使用MRUnit对使用内置Hadoop输入和输出类的简单作业进行测试，并且只在这个技术用于测试那些你想测试输入和输出类以及输出提交者的测试用例。
- en: 8.4.5\. Integration and QA testing
  id: totrans-1306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.4.5. 集成和QA测试
- en: Using the TDD approach, you wrote some unit tests using the techniques in this
    section. You next wrote the MapReduce code and got it to the point where the unit
    tests were passing. Hooray! But before you break out the champagne, you still
    want assurance that the MapReduce code is working prior to running it in production.
    The last thing you want is your code to fail in production and have to debug it
    over there.
  id: totrans-1307
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TDD方法，你使用本节中的技术编写了一些单元测试。接下来，你编写了MapReduce代码，并使其达到单元测试通过的程度。太好了！但在你打开香槟之前，你仍然想要确保MapReduce代码在投入生产运行之前是正常工作的。你最不希望的事情就是代码在生产中失败，然后不得不在那里进行调试。
- en: 'But why, you ask, would my job fail if all of my unit tests pass? Good question,
    and it could be due to a variety of factors:'
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，你可能会问，为什么我的作业会失败，尽管所有的单元测试都通过了？这是一个好问题，可能是由多种因素造成的：
- en: The data you used for your unit tests doesn’t contain all of the data aberrations
    and variances of the data used in production.
  id: totrans-1309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你用于单元测试的数据并不包含在生产中使用的数据的所有异常和变化。
- en: Volume or data skew issues could cause side effects in your code.
  id: totrans-1310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 体积或数据倾斜问题可能会在你的代码中产生副作用。
- en: Differences in Hadoop and other libraries result in behaviors different from
    those in your build environment.
  id: totrans-1311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop和其他库之间的差异导致了与构建环境不同的行为。
- en: Hadoop and operating system configuration differences between your build host
    and production may cause problems.
  id: totrans-1312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的构建主机和生产环境之间的Hadoop和操作系统配置差异可能会导致问题。
- en: Because of these factors, when you build integration or QA test environments,
    it’s crucial to ensure that the Hadoop version and configurations mirror those
    of the production cluster. Different versions of Hadoop will behave differently,
    as will the same version of Hadoop configured in different ways. When you’re testing
    changes in test environments, you’ll want to ensure a smooth transition to production,
    so do as much as you can to make sure that the version and configuration are as
    close as possible to production.
  id: totrans-1313
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些因素，当你构建集成或QA测试环境时，确保Hadoop版本和配置与生产集群相匹配至关重要。不同版本的Hadoop会有不同的行为，同样，以不同方式配置的同一版本的Hadoop也会有不同的行为。当你正在测试测试环境中的更改时，你将希望确保平稳过渡到生产，所以尽可能确保版本和配置尽可能接近生产。
- en: After your MapReduce jobs are successfully running in integration and QA, you
    can push them into production, knowing there’s a much higher probability that
    your jobs will work as expected.
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的MapReduce作业成功运行在集成和QA之后，你可以将它们推入生产，知道你的作业有更高的概率按预期工作。
- en: This wraps up our look at testing MapReduce code. We looked at some TDD and
    design principles to help you write and test your Java code, and we also covered
    some unit-test libraries that make it easier to unit-test MapReduce code.
  id: totrans-1315
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对测试MapReduce代码的探讨。我们探讨了某些TDD和设计原则，以帮助你编写和测试Java代码，我们还介绍了一些单元测试库，这些库使得对MapReduce代码进行单元测试变得更加容易。
- en: 8.5\. Chapter summary
  id: totrans-1316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5. 章节总结
- en: This chapter only scratched the surface when it comes to tuning, debugging,
    and testing. We laid the groundwork for how to tune, profile, debug, and test
    your Map-Reduce code.
  id: totrans-1317
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整、调试和测试方面，本章只是触及了表面。我们为如何调整、分析、调试和测试你的Map-Reduce代码奠定了基础。
- en: For performance tuning, it’s important that you have the ability to collect
    and visualize the performance of your cluster and jobs. In this chapter we presented
    some of the more common issues that can impact the performance of your jobs.
  id: totrans-1318
  prefs: []
  type: TYPE_NORMAL
  zh: 对于性能调整来说，重要的是你能够收集和可视化你的集群和作业的性能。在本章中，我们介绍了一些可能影响作业性能的更常见问题。
- en: If you’re running any critical MapReduce code in production, it’s crucial to
    at least follow the steps in the testing section of this chapter, where I showed
    you how to best design your code so it easily lends itself to basic unit-testing
    methodologies outside the scope of Hadoop. We also covered how the MapReduce-related
    parts of your code could be tested in both lightweight (MRUnit) and more heavyweight
    (`LocalTestRunner`) setups.
  id: totrans-1319
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在生产环境中运行任何关键的MapReduce代码，那么至少要遵循本章测试部分中的步骤，我在那里向你展示了如何最佳地设计你的代码，使其容易在Hadoop范围之外进行基本的单元测试。我们还介绍了你的代码中与MapReduce相关的部分如何在轻量级（MRUnit）和更重量级（`LocalTestRunner`）的设置中进行测试。
- en: In [part 4](kindle_split_021.html#part04), we’ll venture beyond the world of
    MapReduce and examine various systems that allow you to interact with your data
    using SQL. Most SQL systems have moved beyond MapReduce to use YARN, so our last
    chapter looks at how to write your own YARN applications.
  id: totrans-1320
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4部分](kindle_split_021.html#part04)中，我们将超越MapReduce的世界，探讨各种允许你使用SQL与你的数据交互的系统。大多数SQL系统已经超越了MapReduce，转而使用YARN，因此我们上一章探讨了如何编写自己的YARN应用程序。
