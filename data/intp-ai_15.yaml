- en: Appendix B. PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录B. PyTorch
- en: B.1 What is PyTorch?
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.1 什么是PyTorch？
- en: PyTorch is a free, open source library used for scientific computing and deep
    learning applications such as computer vision and natural language processing.
    It is Python based and was developed by Facebook’s AI Research (FAIR) lab. PyTorch
    is widely used by the research community and industry practitioners. Horace He
    conducted a recent study (available at [http://mng.bz/W7Kl](https://shortener.manning.com/W7Kl))
    that shows that the majority of the techniques published in major machine learning
    conferences in 2019 were implemented in PyTorch. Other libraries and frameworks
    like TensorFlow, Keras, CNTK, and MXNet can be used to build and train neural
    networks, but we will use PyTorch in this book. The library is pythonic and utilizes
    Python idioms well. It is, therefore, easier for researchers, data scientists,
    and engineers who are already familiar with Python to use it. PyTorch also provides
    great APIs to implement cutting-edge neural network architectures.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch是一个免费、开源的库，用于科学计算和深度学习应用，如计算机视觉和自然语言处理。它是基于Python的，由Facebook的人工智能研究（FAIR）实验室开发。PyTorch被研究界和行业从业者广泛使用。Horace
    He进行了一项最近的研究（可在[http://mng.bz/W7Kl](https://shortener.manning.com/W7Kl)找到），该研究表明，2019年在主要机器学习会议上发表的大多数技术都是在PyTorch中实现的。其他库和框架，如TensorFlow、Keras、CNTK和MXNet，也可以用于构建和训练神经网络，但我们将在这本书中使用PyTorch。该库是Pythonic的，并且很好地利用了Python惯用语。因此，对于已经熟悉Python的研究人员、数据科学家和工程师来说，使用它更容易。PyTorch还提供了实现前沿神经网络架构的出色API。
- en: B.2 Installing PyTorch
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.2 安装PyTorch
- en: 'You can install the latest stable version of PyTorch using Conda or `pip` as
    follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Conda或`pip`安装PyTorch的最新稳定版本，如下所示：
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note that along with PyTorch, the `torchvision` package is also installed.
    This package ([https://pytorch.org/vision/stable/index.html](https://pytorch.org/vision/stable/index.html))
    consists of popular datasets, implementations of cutting-edge neural network architectures,
    and common transformations done on images for computer vision tasks. You can confirm
    the installation has succeeded by importing the libraries in your Python environment
    as follows:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，除了PyTorch之外，`torchvision`包也被安装了。这个包（[https://pytorch.org/vision/stable/index.html](https://pytorch.org/vision/stable/index.html)）包含流行的数据集、前沿神经网络架构的实现以及计算机视觉任务中对图像进行的常见转换。您可以通过在Python环境中导入库来确认安装是否成功，如下所示：
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: B.3 Tensors
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.3 张量
- en: 'A tensor is a multidimensional array that is very similar to NumPy arrays.
    Tensors contain elements of a single data type and can be used on a graphics processing
    unit (GPU) for fast computing. You can initialize a PyTorch tensor from a Python
    list as follows. Note that the code in this section is formatted in such a way
    so as to reflect a Jupyter notebook or iPython environment. The line where you
    input a command is prefixed with `In:`, and the output of a command is prefixed
    with `Out:`:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 张量是一个多维数组，与NumPy数组非常相似。张量包含单一数据类型的元素，并且可以在图形处理单元（GPU）上进行快速计算。您可以从Python列表初始化PyTorch张量如下。请注意，本节中的代码格式化方式是为了反映Jupyter笔记本或iPython环境。输入命令的行以`In:`为前缀，命令的输出以`Out:`为前缀：
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For machine learning problems, NumPy is widely used. The library supports large,
    multidimensional arrays and provides a wide range of mathematical functions that
    can be used to operate on them. You can initialize a tensor from a NumPy array
    as follows. Note that the output of the print statement shows the tensor along
    with the `dtype`, or data type, of the elements. We will cover this in section
    B.3.1:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习问题，NumPy被广泛使用。该库支持大型、多维数组，并提供了一组广泛的数学函数，可以用于对这些数组进行操作。您可以从NumPy数组初始化张量如下。请注意，print语句的输出显示了张量以及元素的`dtype`，即数据类型。我们将在B.3.1节中介绍这一点：
- en: '[PRE3]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The size of the tensor or the dimension of the multidimensional array can be
    obtained as follows. The tensor initialized previously consists of two rows and
    two columns:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 张量的大小或多维数组的维度可以通过以下方式获得。之前初始化的张量由两行两列组成：
- en: '[PRE4]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can initialize an empty tensor of any size as follows. The next tensor consists
    of three rows and two columns. The values stored in the tensor are random, depending
    on the values that are stored in the bits in memory:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以初始化任何大小的空张量如下。下一个张量由三行两列组成。张量中存储的值是随机的，取决于内存中存储的位值：
- en: '[PRE5]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If we want to initialize a tensor consisting of all zeros, we can do that as
    follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想初始化一个由所有零组成张量，我们可以这样做：
- en: '[PRE6]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'A tensor consisting of all ones can be initialized as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 全为1的张量可以按照以下方式初始化：
- en: '[PRE7]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can initialize a tensor with random numbers as follows. The random numbers
    are uniformly distributed between 0 and 1:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按照以下方式使用随机数初始化张量。随机数在0和1之间均匀分布：
- en: '[PRE8]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If you run the previous command, you may not get the same result because the
    seed of the random-number generator may be different. To get consistent and reproducible
    results, you can set the seed of the random-number generator using the `manual_seed`
    function provided by PyTorch as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行之前的命令，您可能不会得到相同的结果，因为随机数生成器的种子可能不同。为了获得一致且可重复的结果，您可以使用PyTorch提供的`manual_seed`函数设置随机数生成器的种子，如下所示：
- en: '[PRE9]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: B.3.1 Data types
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.1 数据类型
- en: 'Data types (`dtype`), like NumPy `dtype`s ([http://mng.bz/Ex6X](https://shortener.manning.com/Ex6X)),
    describe the type and size of the data. Common data types for tensors follow:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据类型（`dtype`），类似于NumPy的`dtype`，描述了数据类型和大小。张量常见的数据类型如下：
- en: '`torch.float32` or `torch.float`: 32-bit floating point'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.float32` 或 `torch.float`: 32位浮点数'
- en: '`torch.float64` or `torch.double`: 64-bit floating point'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.float64` 或 `torch.double`: 64位浮点数'
- en: '`torch.int32` or `torch.int`: 32-bit signed integer'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.int32` 或 `torch.int`: 32位有符号整数'
- en: '`torch.int64` or `torch.long`: 64-bit signed integer'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.int64` 或 `torch.long`: 64位有符号整数'
- en: '`torch.bool`: Boolean'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.bool`: 布尔值'
- en: 'The full list of all the data types can be found in the PyTorch documentation
    at [https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html).
    You can determine the data type of a tensor as follows. We will be using the `tensor_from_list`
    tensor initialized earlier:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据类型的完整列表可以在PyTorch文档中找到，网址为[https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html)。您可以通过以下方式确定张量的数据类型。我们将使用之前初始化的`tensor_from_list`张量：
- en: '[PRE10]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can initialize a tensor with a given data type as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按照以下方式使用给定的数据类型初始化张量：
- en: '[PRE11]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Sets the dtype parameter to torch.float64
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将dtype参数设置为torch.float64
- en: ② A tensor initialized as a 64-bit floating point
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ② 初始化为64位浮点数的张量
- en: B.3.2 CPU and GPU tensors
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.2 CPU和GPU张量
- en: 'Tensors in PyTorch are by default loaded on the CPU. You can see this by checking
    the device that the tensor is on as follows. We will be using the random tensor
    (`tensor _rand`) initialized previously:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch中的张量默认加载在CPU上。您可以通过检查张量所在的设备来查看这一点。我们将使用之前初始化的随机张量（`tensor _rand`）：
- en: '[PRE12]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'For faster processing, you can load the tensor on a GPU. All the popular deep
    learning frameworks, including PyTorch, use CUDA, which stands for compute unified
    device architecture, to perform general-purpose computing on GPUs. CUDA is a platform
    built by NVIDIA that provides APIs to directly access the GPU. A list of GPUs
    that are CUDA-enabled can be found at [https://developer.nvidia.com/cuda-gpus#compute](https://developer.nvidia.com/cuda-gpus#compute).
    You can check whether CUDA is available on your machine as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加快处理速度，您可以在GPU上加载张量。所有流行的深度学习框架，包括PyTorch，都使用CUDA（代表计算统一设备架构）在GPU上执行通用计算。CUDA是由NVIDIA构建的平台，它提供了直接访问GPU的API。CUDA启用设备的列表可以在[https://developer.nvidia.com/cuda-gpus#compute](https://developer.nvidia.com/cuda-gpus#compute)找到。您可以通过以下方式检查您的机器上是否可用CUDA：
- en: '[PRE13]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If it is available, you can now initialize a tensor on the GPU as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可用，您现在可以按照以下方式在GPU上初始化张量：
- en: '[PRE14]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① First checks whether CUDA is available
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ① 首先检查CUDA是否可用
- en: ② If yes, obtains the CUDA-enabled device
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ② 如果是，获取CUDA启用设备
- en: ③ Initializes the tensor and sets the device to be the CUDA-enabled device
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 初始化张量并将设备设置为CUDA启用设备
- en: 'The following code snippet shows how to transfer a CPU tensor to the GPU:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何将CPU张量转移到GPU上：
- en: '[PRE15]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: B.3.3 Operations
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.3 操作
- en: 'We can perform multiple operations on a tensor. Let’s look at a simple operation
    of adding two tensors. We will first initialize two random tensors, `x` and `y`,
    as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在张量上执行多个操作。让我们看看添加两个张量的简单操作。我们首先初始化两个随机张量，`x`和`y`，如下所示：
- en: '[PRE16]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can obtain the sum of the two tensors using the `add` function, as shown
    next, or by just running `x + y`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`add`函数通过以下方式获得两个张量的和，或者直接运行`x + y`：
- en: '[PRE17]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Various other mathematical operations and functions are also provided by PyTorch.
    For an up-to-date list of all the operation, please refer to [https://pytorch.org/docs/stable/torch.html](https://pytorch.org/docs/stable/torch.html).
    PyTorch also provides a NumPy bridge to convert a tensor into a NumPy array as
    follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 还提供了各种其他数学运算和函数。有关所有操作的最新列表，请参阅 [https://pytorch.org/docs/stable/torch.html](https://pytorch.org/docs/stable/torch.html)。PyTorch
    还提供了一个 NumPy 桥接功能，可以将张量转换为 NumPy 数组，如下所示：
- en: '[PRE18]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: B.4 Dataset and DataLoader
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.4 数据集和数据加载器
- en: 'PyTorch provides a `Dataset` class that allows you to load and create custom
    datasets to be used for model training. Let’s look at a contrived example. We
    will first create a random dataset using Scikit-Learn as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 提供了一个 `Dataset` 类，允许您加载和创建用于模型训练的自定义数据集。让我们看一个假设的例子。我们首先使用 Scikit-Learn
    创建一个随机数据集，如下所示：
- en: '[PRE19]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ① Imports the make_classification function to create a random n-class classification
    dataset
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入 `make_classification` 函数以创建一个随机的 n 类分类数据集
- en: ② Sets the number of samples to 100
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将样本数设置为 100
- en: ③ Sets the number of input features to 5
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将输入特征数设置为 5
- en: ④ Sets the number of classes to 2 to generate a binary classification dataset
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将类别数设置为 2 以生成二元分类数据集
- en: ⑤ Sets the seed for the random-number generator
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 设置随机数生成器的种子
- en: 'The dataset consists of 100 samples or rows. Each sample consists of five input
    features and one target variable consisting of two classes. The values for each
    feature are sampled from a normal distribution. We can inspect the first row of
    input features as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含 100 个样本或行。每个样本包含五个输入特征和一个由两个类别组成的目标变量。每个特征的值是从正态分布中抽取的。我们可以如下检查输入特征的第一行：
- en: '[PRE20]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We will now create a custom dataset class that inherits from the `Dataset`
    class provided by PyTorch. This is shown in the next code snippet:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将创建一个继承自 PyTorch 提供的 `Dataset` 类的自定义数据集类。这将在下一个代码片段中展示：
- en: '[PRE21]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ① Imports the PyTorch Dataset class
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入 PyTorch 数据集类
- en: ② Creates the CustomDataset class that inherits from Dataset
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建一个继承自 `Dataset` 的 `CustomDataset` 类
- en: ③ Initializes the constructor
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 初始化构造函数
- en: ④ Positional arguments to the constructor are the input features matrix X and
    target variable array y.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 构造函数的位置参数是输入特征矩阵 X 和目标变量数组 y。
- en: ⑤ Optional argument that is a transformation that can be applied to the data
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 可选参数是一个可以应用于数据的转换
- en: ⑥ Overrides the __len__ method to return the length of the dataset
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 重写 `__len__` 方法以返回数据集的长度
- en: ⑦ Overrides the __getitem__ method to return the element at index idx
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 重写 `__getitem__` 方法以返回索引 idx 处的元素
- en: ⑧ Extracts the input features and targets the variable at index idx
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 从索引 idx 提取输入特征和目标变量
- en: ⑨ Applies the transformation on the features, if defined
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 如果已定义，则在特征上应用转换
- en: ⑩ Returns the features and target variable at index idx
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 在索引 idx 处返回特征和目标变量
- en: 'The constructor for the `CustomDataset` class takes in two positional arguments
    to initialize the input feature matrix `X` and the target variable `y`. There
    is also an optional argument called `transform` that we can use to apply a transformation
    function on the dataset. Note that we need to override the `__len__` and `__getitem__`
    methods provided by the `Dataset` class to return the length of the dataset and
    to extract the data at a specified index. We can initialize the custom dataset
    and inspect the length of the dataset as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`CustomDataset` 类的构造函数接受两个位置参数以初始化输入特征矩阵 `X` 和目标变量 `y`。还有一个可选参数称为 `transform`，我们可以使用它来在数据集上应用转换函数。请注意，我们需要重写
    `Dataset` 类提供的 `__len__` 和 `__getitem__` 方法，以返回数据集的长度并提取指定索引处的数据。我们可以初始化自定义数据集并检查数据集的长度，如下所示：'
- en: '[PRE22]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let’s now also inspect the first row of input features as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在也检查输入特征的第一行，如下所示：
- en: '[PRE23]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We will now create a custom dataset and apply a transformation function to
    it. We will pass in the `torch.tensor` function to transform the array of input
    features as tensors. This is shown next. We can see that the first row of input
    features is now a tensor consisting of 64-bit floating point values:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将创建一个自定义数据集并将其应用转换函数。我们将传递 `torch.tensor` 函数以将输入特征数组转换为张量。这将在下面展示。我们可以看到输入特征的第一行现在是一个包含
    64 位浮点值的张量：
- en: '[PRE24]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Some common image transformation functions like crops, flips, rotations, and
    resizing are also implemented in PyTorch as part of the `torchvision` package.
    The full list of transformations can be found at [https://pytorch.org/vision/stable/transforms.html](https://pytorch.org/vision/stable/transforms.html).
    We will use them in chapter 5.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见的图像变换函数，如裁剪、翻转、旋转和调整大小，也被PyTorch作为`torchvision`包的一部分实现。完整的变换列表可以在[https://pytorch.org/vision/stable/transforms.html](https://pytorch.org/vision/stable/transforms.html)找到。我们将在第五章中使用它们。
- en: 'Another useful data utility class to know about is `DataLoader`. This class
    takes as input an object that inherits from the `Dataset` class and a few optional
    parameters that allow you to iterate through your data. The `DataLoader` class
    provides features like data batching and shuffling and data loading in parallel
    using multiprocessing workers. The following code snippet shows you how to initialize
    a `DataLoader` object and iterate through the custom dataset created earlier:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得了解的有用数据实用类是`DataLoader`。这个类接受一个从`Dataset`类继承的对象，以及一些可选参数，允许您遍历您的数据。`DataLoader`类提供了数据批处理、打乱和并行加载数据的功能，使用多进程工作进程。以下代码片段展示了如何初始化一个`DataLoader`对象并遍历之前创建的自定义数据集：
- en: '[PRE25]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ① Imports the DataLoader class provided by PyTorch
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入PyTorch提供的DataLoader类
- en: ② Initializes the DataLoader and passes the transformed_dataset initialized
    earlier
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ② 初始化DataLoader并传递之前初始化的transformed_dataset
- en: ③ Batches the data into batches of four
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将数据分批为每批四个
- en: ④ Shuffles the dataset
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 打乱数据集
- en: ⑤ Loads the data in parallel utilizing four cores or CPUs
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 利用四个核心或CPU并行加载数据
- en: ⑥ Iterates through the loader and loads data in batches
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 遍历加载器并在批处理中加载数据
- en: ⑦ Prints the batch number and the number of rows loaded in batch
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 打印批号和每批加载的行数
- en: By executing this code, you will notice 25 batches and four rows in each batch
    because the input dataset has a length of 100 and the `batch_size` argument in
    the `DataLoader` class is set to 4\. We will use the `Dataset` and `DataLoader`
    classes later in section B.5.3 and in chapter 5.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码后，您将注意到有25个批处理，每个批处理有4行，因为输入数据集的长度为100，`DataLoader`类中的`batch_size`参数设置为4。我们将在B.5.3节和第五章中使用`Dataset`和`DataLoader`类。
- en: B.5 Modeling
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.5 建模
- en: In this section, we focus on modeling and how to build and train neural networks
    using PyTorch. We start off with automatic differentiation, which is a way to
    efficiently compute gradients and is used to optimize the weights in a neural
    network. We then cover model definition and model training.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们重点关注建模以及如何使用PyTorch构建和训练神经网络。我们首先介绍自动微分，这是一种高效计算梯度的方法，用于优化神经网络中的权重。然后，我们将介绍模型定义和模型训练。
- en: B.5.1 Automatic differentiation
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.5.1 自动微分
- en: In chapter 4, we will learn about neural networks. Neural networks consist of
    many layers of units that are interconnected with edges. Each unit in a layer
    of the network performs a mathematical operation on all the inputs to that unit
    and passes the output to the subsequent layer. The edges that interconnect units
    are associated with weights, and the objective of the learning algorithm is to
    determine the weights for all the edges such that the prediction of the neural
    network is as close to the target in the labeled dataset.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在第四章中，我们将学习神经网络。神经网络由许多通过边相互连接的层组成。网络中每一层的每个单元对其所有输入执行数学运算，并将输出传递给下一层。连接单元的边与权重相关联，学习算法的目标是确定所有边的权重，使得神经网络的预测尽可能接近标记数据集中的目标。
- en: 'An efficient way of determining the weights is using the backpropagation algorithm.
    We will learn more about this in chapter 4\. In this section, we learn about automatic
    differentiation and how it is implemented in PyTorch. Automatic differentiation
    is a way to numerically evaluate the derivative of a function. Backpropagation
    is a special case of automatic differentiation. Let’s look at a simple example
    and see how we can apply automatic differentiation in PyTorch. Consider an input
    tensor represented as *x*. The first operation that we do on this input tensor
    is to scale it by a factor of 2\. Let’s represent the output of this operation
    as *w*, where *w = 2x*. Given *w*, we now perform a second mathematical operation
    on it and represent the output tensor as *y*. This operation is shown here:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 确定权重的一个有效方法是使用反向传播算法。我们将在第4章中了解更多关于这个内容。在本节中，我们将学习自动微分及其在PyTorch中的实现。自动微分是一种数值评估函数导数的方法。反向传播是自动微分的一个特例。让我们看一个简单的例子，看看我们如何在PyTorch中应用自动微分。考虑一个表示为
    *x* 的输入张量。我们对这个输入张量进行的第一个操作是将它乘以一个因子2。让我们将这个操作的输出表示为 *w*，其中 *w = 2x*。给定 *w*，我们现在对它执行第二个数学操作，并将输出张量表示为
    *y*。这个操作如下所示：
- en: '![](../Images/B_EQ_00.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B_EQ_00.png)'
- en: 'The final operation that we perform is to simply sum all the values in tensor
    *y*. We represent the final output tensor as *z*. If we now wanted to compute
    the gradient of this output *z* with respect to the input *x*, we need to apply
    the chain rule as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行的最后一个操作是简单地求和张量 *y* 中的所有值。我们将最终的输出张量表示为 *z*。如果我们现在想计算输出 *z* 关于输入 *x* 的梯度，我们需要应用链式法则如下：
- en: '![](../Images/B_EQ_01.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B_EQ_01.png)'
- en: 'The partial derivates in this equation are given here:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 该方程中的偏导数如下所示：
- en: '![](../Images/B_EQ_02.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B_EQ_02.png)'
- en: '![](../Images/B_EQ_03.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B_EQ_03.png)'
- en: '![](../Images/B_EQ_04.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/B_EQ_04.png)'
- en: 'The computation of these gradients can be complicated for more complex mathematical
    functions. PyTorch makes this easier using the `autograd` package. The `autograd`
    package implements automatic differentiation and allows you to numerically evaluate
    the derivative of a function. By applying the chain rule as shown earlier, `autograd`
    allows you to compute the gradient of functions of arbitrary order automatically.
    Let’s see this in action by implementing the previous mathematical operations
    using tensors. We first initialize the input tensor x of size 2 × 3, consisting
    of all ones. Note that an argument called `requires_grad` is set to `True` when
    initializing the tensor. This argument lets `autograd` know to record operations
    on them for automatic differentiation:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的数学函数，这些梯度的计算可能很复杂。PyTorch通过使用 `autograd` 包使这变得更容易。`autograd` 包实现了自动微分，并允许您数值评估函数的导数。通过应用前面显示的链式法则，`autograd`
    允许您自动计算任意阶函数的梯度。让我们通过使用张量实现前面的数学操作来观察这一过程。我们首先初始化一个大小为 2 × 3 的输入张量 x，其中包含所有1。请注意，当初始化张量时，将一个名为
    `requires_grad` 的参数设置为 `True`。此参数让 `autograd` 知道要记录对它们的操作以进行自动微分：
- en: '[PRE26]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We now implement the first mathematical operation that scales tensor *x* by
    a factor of 2 to obtain tensor *w*. Note that the output of tensor *w* shows `grad_fn`,
    which is used to record the operation that was performed on *x* to obtain *w*.
    This function is used to numerically evaluate the gradient using automatic differentiation:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们实现第一个数学操作，将张量 *x* 乘以一个因子2以获得张量 *w*。请注意，张量 *w* 的输出显示了 `grad_fn`，它用于记录对 *x*
    执行的操作以获得 *w*。此函数用于使用自动微分数值评估梯度：
- en: '[PRE27]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We now implement the second mathematical operation that is used to transform
    tensor *w* into *y*:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们实现第二个数学操作，用于将张量 *w* 转换为 *y*：
- en: '[PRE28]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The final operation simply takes the sum of all values of tensor *y* to obtain
    *z*, as shown here:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的操作简单地取张量 *y* 中所有值的总和以获得 *z*，如下所示：
- en: '[PRE29]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can easily compute the gradient of tensor *z* with respect to the input
    *x* by calling the `backward` function as follows. This will apply the chain rule
    and compute the gradient of the output with respect to the input:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调用 `backward` 函数轻松地计算张量 *z* 关于输入 *x* 的梯度。这将应用链式法则并计算输出相对于输入的梯度：
- en: '[PRE30]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can see the numerical evaluation of the gradient as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到梯度的数值评估如下：
- en: '[PRE31]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'To verify whether the answer is right, let’s mathematically derive the derivative
    of *z* with respect to *x* as provided by the earlier equations. This is summarized
    next:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证答案是否正确，让我们根据前面提供的方程数学推导出*z*相对于*x*的导数。以下是总结：
- en: '![](../Images/B_EQ_05.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![B_EQ_05](../Images/B_EQ_05.png)'
- en: '![](../Images/B_EQ_06a.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![B_EQ_06a](../Images/B_EQ_06a.png)'
- en: As an exercise, I encourage you to evaluate this equation using the tensor.
    The solution of this exercise can be found in the GitHub repository associated
    with this book at [https://github.com/thampiman/interpretable-ai-book](https://github.com/thampiman/interpretable-ai-book).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，我鼓励您使用张量评估这个方程。这个练习的解决方案可以在与本书相关的GitHub仓库[https://github.com/thampiman/interpretable-ai-book](https://github.com/thampiman/interpretable-ai-book)中找到。
- en: B.5.2 Model definition
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.5.2 模型定义
- en: Let’s now see how to define a neural network using PyTorch. We will focus on
    a fully connected neural network. The contrived dataset that we generated in section
    A.4 consisted of five input features and one binary output. Let’s now define a
    fully connected neural network consisting of one input layer, two hidden layers,
    and one output layer. The input layer must consist of five units because the dataset
    contains five input features. The output layer must consist of one unit because
    we are dealing with one binary output. We have flexibility in choosing the number
    of the units in the two hidden layers. Let’s use five and three units for the
    first and second hidden layers, respectively. We take a linear combination of
    the inputs at each unit in the neural network and use the rectified linear unit
    (ReLU) activation function at the hidden layers and the sigmoid activation function
    on the output layer. See chapter 4 for more details on neural networks and activation
    functions.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看如何使用PyTorch定义一个神经网络。我们将关注一个全连接神经网络。我们在A.4节中生成的合成数据集由五个输入特征和一个二进制输出组成。现在让我们定义一个包含一个输入层、两个隐藏层和一个输出层的全连接神经网络。输入层必须包含五个单元，因为数据集包含五个输入特征。输出层必须包含一个单元，因为我们处理的是一个二进制输出。我们在选择两个隐藏层中单元的数量方面有灵活性。让我们分别为第一和第二隐藏层选择五个和三个单元。我们在神经网络中每个单元的输入上进行线性组合，并在隐藏层使用ReLU激活函数，在输出层使用sigmoid激活函数。有关神经网络和激活函数的更多详细信息，请参阅第4章。
- en: 'In PyTorch, we can use the `torch.nn.Sequential` container to define units
    and layers in the neural network in order. Each layer of units in PyTorch must
    inherit from the `torch.nn.Module` base class. PyTorch already provides a lot
    of the commonly used layers in neural networks that include linear, convolutional,
    and recurrent layers. Common activation functions like ReLU, sigmoid, and hyperbolic
    tangent (tanh) are also implemented. The full list of layers and activation functions
    can be found at [https://pytorch.org/docs/master/nn.html](https://pytorch.org/docs/master/nn.html).
    We are now ready to define the model using these building blocks as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，我们可以使用`torch.nn.Sequential`容器按顺序定义神经网络中的单元和层。PyTorch中的每个单元层都必须继承自`torch.nn.Module`基类。PyTorch已经提供了许多神经网络中常用的层，包括线性层、卷积层和循环层。常见的激活函数如ReLU、sigmoid和双曲正切（tanh）也已实现。层和激活函数的完整列表可以在[https://pytorch.org/docs/master/nn.html](https://pytorch.org/docs/master/nn.html)找到。我们现在可以使用这些构建块来定义模型，如下所示：
- en: '[PRE32]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The `Sequential` container here defines the layers in order. The first `Linear`
    module corresponds to the first hidden layer that takes in the five features in
    the dataset and produces five outputs, which are fed into the next layer. The
    `Linear` module performs a linear transformation on the inputs. The next module
    in the container defines the `ReLU` activation function for the first hidden layer.
    The following `Linear` module then takes in five input features from the first
    hidden layer, performs a linear transformation, and produces three outputs that
    are fed into the next layer. Again, the `ReLU` activation function is used in
    the second hidden layer. The final `Linear` module then takes in three input features
    from the second hidden layer and produces one output, the output layer. Because
    we are dealing with binary classification, we use the `Sigmoid` activation function
    at the output layer. If we print the model by executing the command, `print(model)`,
    we will get the following output:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里定义的 `Sequential` 容器按顺序定义层。第一个 `Linear` 模块对应于第一个隐藏层，它接收数据集中的五个特征并产生五个输出，这些输出被馈送到下一层。`Linear`
    模块对输入执行线性变换。容器中的下一个模块定义了第一个隐藏层的 `ReLU` 激活函数。接下来的 `Linear` 模块接收来自第一个隐藏层的五个输入特征，执行线性变换，并产生三个输出，这些输出被馈送到下一层。同样，在第二个隐藏层中也使用了
    `ReLU` 激活函数。最后的 `Linear` 模块接收来自第二个隐藏层的三个输入特征并产生一个输出，即输出层。因为我们处理的是二分类，所以在输出层使用
    `Sigmoid` 激活函数。如果我们通过执行命令 `print(model)` 打印模型，我们将得到以下输出：
- en: '[PRE33]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We can now see how to define the neural network as a class where the number
    of layers and units can be easily customized, as shown in the code snippet that
    follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以查看如何将神经网络定义为类，其中层数和单元数可以轻松定制，如下面的代码片段所示：
- en: '[PRE34]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ① The BinaryClassifier class that extends the Sequential container
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ① 扩展 `Sequential` 容器的 `BinaryClassifier` 类
- en: ② The constructor takes in an array called layer_dims that defines the architecture
    of the network.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ② 构造函数接收一个名为 `layer_dims` 的数组，该数组定义了网络的架构。
- en: ③ Initializes the Sequential container
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 初始化 `Sequential` 容器
- en: ④ Iterates through the layer_dims array
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 遍历 `layer_dims` 数组
- en: ⑤ Adds the Linear module for all layers and names it “linear,” followed by the
    index of the layer
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 为所有层添加线性模块，并命名为“linear”，后跟层的索引
- en: ⑥ For all hidden layers, adds the ReLU module and names it “relu,” followed
    by the index of the hidden layer
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 为所有隐藏层添加 ReLU 模块，并命名为“relu”，后跟隐藏层的索引
- en: ⑦ For the output layer, adds the Sigmoid module and names it “sigmoid,” followed
    by the index of the output layer
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 对于输出层，添加 Sigmoid 模块，并命名为“sigmoid”，后跟输出层的索引
- en: 'The `BinaryClassifier` class inherits from `torch.nn.Sequential`. The constructor
    takes in one positional argument, which is an array of integers called `layer_dims`
    that defines the number of layers and units in each layer. The length of the array
    defines the number of layers, and the element at index `i` defines the number
    of units at layer `i+1`. Within the constructor, we iterate through the `layer_dims`
    array and add a layer to the container using the `add_module` function. The implementation
    uses a linear module for all the layers and names them `linear`, followed by the
    index of the layer. We use the ReLU activation function for all hidden layers
    and the sigmoid activation function for the output layer. With this custom class
    in place, we can now initialize the binary classifier and define the structure
    easily using an array as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`BinaryClassifier` 类继承自 `torch.nn.Sequential`。构造函数接收一个位置参数，这是一个名为 `layer_dims`
    的整数数组，它定义了每层的层数和单元数。数组的长度定义了层数，而索引 `i` 处的元素定义了第 `i+1` 层的单元数。在构造函数内部，我们遍历 `layer_dims`
    数组，并使用 `add_module` 函数将一个层添加到容器中。实现使用线性模块来处理所有层，并命名为 `linear`，后跟层的索引。我们为所有隐藏层使用
    ReLU 激活函数，对于输出层使用 sigmoid 激活函数。有了这个自定义类，我们现在可以初始化二分类器，并使用数组轻松定义结构，如下所示：'
- en: '[PRE35]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ① Sets the number of input features to 5
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ① 设置输入特征的数量为 5
- en: ② Sets the number of outputs to 1
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ② 设置输出数量为 1
- en: ③ Initializes the layer_dims array that defines the structure of the network
    consisting of five units in the input layer, five units in the first hidden layer,
    three units in the second hidden layer, and one unit in the output layer
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 初始化 `layer_dims` 数组，该数组定义了由输入层五个单元、第一个隐藏层五个单元、第二个隐藏层三个单元和输出层一个单元组成的网络结构
- en: ④ Initializes the model using the BinaryClassifier class
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用 `BinaryClassifier` 类初始化模型
- en: 'We can see the structure of the network by executing `print(bc_model),` which
    gives us the following output. We will be using a similar implementation in chapter
    4:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行`print(bc_model)`，我们可以看到网络的架构，它给出了以下输出。我们将在第4章中使用类似的实现：
- en: '[PRE36]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: B.5.3 Training
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.5.3 训练
- en: 'With the model in place, we are now ready to train it on the dataset we created
    earlier. At a high level, the training loop consists of the following steps:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型就绪后，我们现在可以将其训练在之前创建的数据集上。从高层次来看，训练循环包括以下步骤：
- en: 'Loop over epochs: For each epoch, loop over batches of data.'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环epoch：对于每个epoch，循环遍历数据批次。
- en: For each mini batch of data
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个小批量数据
- en: Run the data through the model to obtain the outputs
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据通过模型以获得输出
- en: Calculate the loss
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算损失
- en: Run the backpropagation algorithm to optimize the weights
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行反向传播算法以优化权重
- en: An epoch is a hyperparameter that defines the number of times we propagate the
    entire training data in the forward and backward directions through the neural
    network. During each epoch, we load a batch of data, and for each batch, we will
    run it through the network to get the outputs, calculate the loss, and optimize
    the weights based on that loss using the backpropagation algorithm.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一个epoch是一个超参数，它定义了我们在神经网络中正向和反向传播整个训练数据的次数。在每个epoch中，我们加载一批数据，对于每一批数据，我们将其通过网络以获取输出，计算损失，并使用反向传播算法根据该损失优化权重。
- en: 'PyTorch provides lots of loss functions or criteria for optimization. Some
    of the commonly used ones follow:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供了大量的损失函数或优化标准。其中一些常用的如下：
- en: '`torch.nn.L1Loss`—This computes the mean absolute error (MAE) of the output
    prediction and the actual value. This is typically used for regression tasks.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.nn.L1Loss`—这个函数计算输出预测和实际值的平均绝对误差（MAE）。这通常用于回归任务。'
- en: '`torch.nn.MSELoss`—This computes the mean squared error (MSE) of the output
    prediction and the actual value. Like L1 loss, this is also typically used for
    regression tasks.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.nn.MSELoss`—这个函数计算输出预测和实际值的平均平方误差（MSE）。像L1损失一样，这也通常用于回归任务。'
- en: '`torch.nn.BCELoss`—This computes the binary cross entropy, or log loss, of
    the output prediction and the actual label. This function is typically used for
    binary classification tasks.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.nn.BCELoss`—这个函数计算输出预测和实际标签的二进制交叉熵或对数损失。这个函数通常用于二分类任务。'
- en: '`torch.nn.CrossEntropyLoss`—This function combines the softmax and negative
    log likelihood loss functions and is typically used for classification tasks.
    We will learn more about BCE loss and cross-entropy loss in chapter 5.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.nn.CrossEntropyLoss`—这个函数结合了softmax和负对数似然损失函数，通常用于分类任务。我们将在第5章中学习更多关于BCE损失和交叉熵损失的内容。'
- en: You can find the full list of all the loss functions at [http://mng.bz/Dx5A](http://mng.bz/Dx5A).
    Because we are dealing with only two target classes in the dataset we have created,
    we will use the BCE loss function.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[http://mng.bz/Dx5A](http://mng.bz/Dx5A)找到所有损失函数的完整列表。由于我们只处理我们创建的数据集中的两个目标类别，我们将使用BCE损失函数。
- en: 'PyTorch also provides various optimization algorithms that we can use during
    backpropagation to update the weights. We will use the Adam optimizer in this
    section. A full list of all the optimizers implemented in PyTorch can be found
    at [https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html).
    The following code snippet initializes the loss function or criterion for the
    optimizer and the Adam optimizer on all the parameters or weights in the model
    initialized in the previous section:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch还提供了各种优化算法，我们可以在反向传播过程中使用它们来更新权重。在本节中，我们将使用Adam优化器。PyTorch中实现的全部优化器的完整列表可以在[https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html)找到。以下代码片段初始化了优化器的损失函数或标准或上一节中初始化的模型的所有参数或权重：
- en: '[PRE37]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can implement the training loop as follows. Note that we are training for
    10 epochs. During each epoch, we use the `DataLoader` object initialized in section
    A.4 to load the data and labels in batches. For each mini batch of data, we first
    need to reset the gradients to zero before computing the gradients for that mini
    batch. We then run through the model in the forward direction to obtain the output.
    Then we use these outputs to compute the BCE loss. By calling the `backward` function,
    the gradient of the loss function is computed with respect to the inputs using
    automatic differentiation. We then call the `step` function in the optimizer to
    update the weights or model parameters based on the gradients computed:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下实现训练循环。注意，我们正在训练10个epoch。在每个epoch中，我们使用在A.4节中初始化的`DataLoader`对象批量加载数据和标签。对于每个数据小批量，我们首先需要将梯度重置为零，然后计算该小批量的梯度。然后我们通过模型的前向方向运行以获得输出。然后我们使用这些输出来计算BCE损失。通过调用`backward`函数，使用自动微分计算损失函数相对于输入的梯度。然后我们调用优化器中的`step`函数，根据计算出的梯度更新权重或模型参数：
- en: '[PRE38]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ① Initializes the variable for the number of epochs
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ① 初始化epoch数量的变量
- en: ② For loop for epochs
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ② 循环遍历epoch
- en: ③ Loops through each mini batch of data and labels
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 遍历每个数据和小批量标签
- en: ④ Resets the gradient to 0 for each mini batch
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 为每个小批量重置梯度到0
- en: ⑤ Runs the data through the model in the forward direction to get the output
    predictions
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将数据通过模型的前向方向运行以获取输出预测
- en: ⑥ Computes the loss by comparing with ground truth labels
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 通过与真实标签比较来计算损失
- en: ⑦ Performs backward propagation to compute the gradient of loss function with
    respect to the inputs
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 执行反向传播来计算损失函数相对于输入的梯度
- en: ⑧ Updates the parameters in the model by calling step
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 通过调用step更新模型中的参数
- en: 'Once we have trained the model, we can get a prediction for a data point as
    follows. Note that we are switching the format of the following code snippet to
    mimic a Jupyter notebook or iPython environment:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练好了模型，我们就可以对数据点进行如下预测。注意，我们将以下代码片段的格式转换为模仿Jupyter笔记本或iPython环境：
- en: '[PRE39]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output of the model is a tensor consisting of a probability measure. This
    probability measure corresponds to the output of the sigmoid activation function
    in the final layer in the neural network. You can obtain the prediction as a scalar
    as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的输出是一个包含概率度量的张量。这个概率度量对应于神经网络中最终层的sigmoid激活函数的输出。你可以如下获得预测的标量：
- en: '[PRE40]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This ends our whirlwind tour of PyTorch, and we hope you are equipped with enough
    knowledge to be able to implement and train neural networks, and to understand
    the code in this book. There are a lot of books and online resources dedicated
    to PyTorch available at [https://bookauthority.org/books/new-pytorch-books](https://bookauthority.org/books/new-pytorch-books)
    and [http://mng.bz/laBd](https://shortener.manning.com/laBd). The PyTorch documentation
    at [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)
    is also a great resource to get a much deeper understanding of the library.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对PyTorch的快速浏览，我们希望您已经拥有了足够的知识来实施和训练神经网络，并理解本书中的代码。有很多关于PyTorch的书籍和在线资源，可以在[https://bookauthority.org/books/new-pytorch-books](https://bookauthority.org/books/new-pytorch-books)和[http://mng.bz/laBd](https://shortener.manning.com/laBd)找到。PyTorch文档[https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)也是一个深入了解库的极好资源。
