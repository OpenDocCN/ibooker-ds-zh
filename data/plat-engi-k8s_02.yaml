- en: 2 Cloud-native application challenges
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 云原生应用挑战
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Working with a cloud-native application running in a Kubernetes cluster
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与在 Kubernetes 集群中运行的云原生应用一起工作
- en: Choosing between local and remote Kubernetes clusters
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地和远程 Kubernetes 集群之间进行选择
- en: Understanding the main components and Kubernetes resources
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解主要组件和 Kubernetes 资源
- en: Understanding the challenges of working with cloud-native applications
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解与云原生应用一起工作的挑战
- en: When I want to try something new, a framework, a new tool, or just a new application,
    I tend to be impatient; I want to see it running immediately. Then, when it is
    running, I want to dig deeper and understand how it works. I break things to experiment
    and validate that I understand how these tools, frameworks, or applications work
    internally. That is the sort of approach we’ll take in this chapter!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当我想尝试新事物时，无论是框架、新工具还是新应用，我往往缺乏耐心；我想立即看到它运行。然后，当它运行时，我想深入了解并理解它是如何工作的。我破坏事物以进行实验并验证我是否理解这些工具、框架或应用的内部工作原理。这正是我们在本章中将要采取的方法！
- en: To have a cloud-native application up and running, you will need a Kubernetes
    cluster. In this chapter, you will work with a local Kubernetes cluster using
    a project called KinD (Kubernetes in Docker, [https://kind.sigs.k8s.io/](https://kind.sigs.k8s.io/)).
    This local cluster will allow you to deploy applications locally for development
    and experimentation. To install a set of microservices, you will use Helm, a project
    that helps package, deploy, and distribute Kubernetes applications. You will install
    the walking skeleton services introduced in chapter 1, which implements a Conference
    application.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 要使云原生应用启动并运行，您需要一个 Kubernetes 集群。在本章中，您将使用名为 KinD（Docker 中的 Kubernetes，[https://kind.sigs.k8s.io/](https://kind.sigs.k8s.io/))
    的项目与本地 Kubernetes 集群一起工作。这个本地集群将允许您在本地部署应用以进行开发和实验。要安装一组微服务，您将使用 Helm，这是一个帮助打包、部署和分发
    Kubernetes 应用的项目。您将安装第 1 章中引入的行走骨架服务，该服务实现了一个会议应用。
- en: Once the services for the Conference application are up and running, you will
    inspect its Kubernetes resources to understand how the application was architected
    and its inner workings by using `kubectl`. Once you get an overview of the main
    pieces inside the application, you will jump ahead to try to break the application,
    finding common challenges and pitfalls that your cloud-native applications can
    face. This chapter covers the basics of running cloud-native applications in a
    modern technology stack based on Kubernetes, highlighting the good and the bad
    that come with developing, deploying, and maintaining distributed applications.
    The following chapters tackle these associated challenges by looking into projects
    whose main focus is to speed up and make more efficient the delivery of your projects.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦会议应用的服务启动并运行，您将使用 `kubectl` 检查其 Kubernetes 资源，以了解应用是如何架构的以及其内部工作原理。一旦您对应用内部的主要组件有了概述，您将跳到尝试破坏应用，寻找您的云原生应用可能面临的常见挑战和陷阱。本章涵盖了在基于
    Kubernetes 的现代技术堆栈中运行云原生应用的基础，突出了开发、部署和维护分布式应用所带来的利弊。接下来的章节将通过研究主要关注加快和使项目交付更高效的项目来应对这些相关挑战。
- en: 2.1 Running our cloud-native applications
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 运行我们的云原生应用
- en: To understand the innate challenges of cloud-native applications, we need to
    be able to experiment with a simple example that we can control, configure, and
    break for educational purposes. In the context of cloud-native applications, “simple”
    cannot be a single service, so for simple applications we will need to deal with
    the complexities of distributed applications such as networking latency, resilience
    to failure on some of the applications’ services, and eventual inconsistencies.
    To run a cloud-native application, in this case, the walking skeleton introduced
    in chapter 1, you need a Kubernetes cluster. Where this cluster is going to be
    installed and who will be responsible for setting it up are the first questions
    that developers will have. It is quite common for developers to want to run things
    locally, on their laptop or workstation, and with Kubernetes, this is possible—but
    is it optimal? Let’s analyze the advantages and disadvantages of running a local
    cluster against other options.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解云原生应用的固有挑战，我们需要能够对我们可以控制、配置和为了教育目的而破坏的简单示例进行实验。在云原生应用的背景下，“简单”不能是一个单一的服务，因此对于简单应用，我们需要处理分布式应用的复杂性，如网络延迟、某些应用服务的容错性以及最终的不一致性。要运行云原生应用，在这种情况下，第
    1 章中引入的“行走骨架”，您需要一个 Kubernetes 集群。这个集群将安装在哪里，谁将负责设置它，是开发者首先会有的问题。开发者希望在本地运行事物，在他们的笔记本电脑或工作站上，而
    Kubernetes 使得这一点成为可能——但这是否是最优的？让我们分析在本地集群与其他选项相比运行的优势和劣势。
- en: 2.1.1 Choosing the best Kubernetes environment for you
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 为您选择最佳的 Kubernetes 环境
- en: 'This section doesn’t cover a comprehensive list of all the available Kubernetes
    flavors, but it focuses on common patterns in how Kubernetes clusters can be provisioned
    and managed. There are three possible alternatives—all of them with advantages
    and drawbacks:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本节不涵盖所有可用的 Kubernetes 风味的完整列表，但它关注于 Kubernetes 集群可以如何配置和管理的一些常见模式。有三种可能的替代方案——它们都有优点和缺点：
- en: '*Local Kubernetes in your laptop/desktop computer:* I tend to discourage people
    from running Kubernetes on their laptops. As you will see in the rest of the book,
    running your software in similar environments to production is highly recommended
    to avoid problems that can be summed up as “but it works on my laptop.” These
    problems are mostly caused by the fact that when you run Kubernetes on your laptop,
    you are not running on top of a real cluster of machines. Hence, there are no
    network round-trips and no real load balancing.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*笔记本电脑/台式电脑上的本地 Kubernetes：* 我倾向于不鼓励人们在他们的笔记本电脑上运行 Kubernetes。正如您将在本书的其余部分看到的那样，在类似生产环境的环境中运行您的软件被高度推荐，以避免可以总结为“但在我的笔记本电脑上它运行正常”的问题。这些问题大多是由于当您在笔记本电脑上运行
    Kubernetes 时，您并不是在真实的机器集群上运行。因此，没有网络往返和真正的负载均衡。'
- en: '*Pros:* Lightweight, fast to get started, good for testing, experimenting,
    and local development. Good for running small applications.'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优点：* 轻量级，快速启动，适合测试、实验和本地开发。适合运行小型应用程序。'
- en: '*Cons:* Not a real cluster, it behaves differently, and has reduced hardware
    to run workloads. You will not be able to run a large application on your laptop.'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缺点：* 这不是一个真正的集群，其行为不同，且硬件资源减少，无法运行工作负载。您无法在笔记本电脑上运行大型应用程序。'
- en: '*On-premise Kubernetes in your data center:* This is a typical option for companies
    with private clouds. This approach requires the company to have a dedicated team
    and hardware to create, maintain, and operate these clusters. If your company
    is mature enough, it might have a self-service platform that allows users to request
    new Kubernetes clusters on demand.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据中心内的本地 Kubernetes：* 这是对拥有私有云的公司来说的一个典型选择。这种方法要求公司拥有一个专门的团队和硬件来创建、维护和运行这些集群。如果您的公司足够成熟，可能已经有一个自助服务平台，允许用户按需请求新的
    Kubernetes 集群。'
- en: '*Pros:* A real cluster on top of real hardware will behave closer to how a
    production cluster will work. You will have a clear picture of which features
    are available for your applications to use in your environments.'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优点：* 在真实硬件之上的真实集群将更接近于生产集群的工作方式。您将清楚地了解在您的环境中，哪些功能可供应用程序使用。'
- en: '*Cons:* It requires a mature operation team to set up clusters and give credentials
    to users, and it requires dedicated hardware for developers to work on their experiments.'
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缺点：* 需要一个成熟的运维团队来设置集群并向用户提供凭证，并且需要为开发者提供专用硬件来进行实验。'
- en: '*Managed service Kubernetes offering in a cloud provider:* I tend to be in
    favor of this approach, because using a cloud provider service allows you to pay
    for what you use, and services like Google Kubernetes Engine (GKE), Azure AKS,
    and AWS EKS are all built with a self-service approach in mind, enabling developers
    to spin up new Kubernetes clusters quickly. There are two primary considerations:'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*云提供商提供的托管服务 Kubernetes：* 我倾向于支持这种方法，因为使用云提供商的服务允许你按使用付费，像 Google Kubernetes
    Engine (GKE)、Azure AKS 和 AWS EKS 这样的服务都是基于自助服务理念构建的，使开发者能够快速启动新的 Kubernetes 集群。有两个主要考虑因素：'
- en: You need to choose one cloud provider and have an account with a big credit
    card to pay for what your teams will consume. This might involve setting up some
    caps in the budget and defining who has access. By selecting a cloud provider,
    you might be in a vendor lock-in situation if you are not careful.
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要选择一个云提供商，并拥有一个信用卡账户来支付你的团队将消耗的费用。这可能涉及到在预算中设置一些上限并定义谁有权访问。如果不小心，选择云提供商可能会使你陷入供应商锁定的情况。
- en: Everything is remote, and for developers and other teams that are used to work
    locally, this is too big of a change. It takes time for developers to adapt, because
    the tools and most of the workloads will run remotely. This is also an advantage,
    because the environments used by your developers and the applications that they
    are deploying are going to behave as if they were running in a production environment.
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有的东西都是远程的，对于习惯本地工作的开发者和其他团队来说，这是一个很大的变化。开发者需要时间来适应，因为工具和大部分的工作负载都将远程运行。这也是一个优势，因为你的开发者和他们部署的应用程序将表现得好像它们正在生产环境中运行。
- en: '*Pros:* You are working with real (fully fledged) clusters. You can define
    how many resources you need for your tasks, and when you are done, you can delete
    them to release resources. You don’t need to invest in hardware up front.'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优点：* 你正在与真实的（完全成熟的）集群一起工作。你可以定义你的任务需要多少资源，完成工作后，你可以删除它们以释放资源。你不需要预先投资硬件。'
- en: '*Cons:* You need a potentially big credit card, and you need your developers
    to work against remote clusters and services.'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缺点：* 你可能需要一个大的信用卡，并且你需要你的开发人员与远程集群和服务进行工作。'
- en: 'A final recommendation is to check the following repository, which contains
    free Kubernetes credits in major cloud providers: [https://github.com/learnk8s/free-kubernetes](https://github.com/learnk8s/free-kubernetes).
    I’ve created this repository to keep an updated list of these free trials that
    you can use to get all the examples in the book up and running on top of real
    infrastructure. Figure 2.1 summarizes the information contained in the previous
    bullet points.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的建议是检查以下存储库，其中包含主要云提供商的免费 Kubernetes 信用额度：[https://github.com/learnk8s/free-kubernetes](https://github.com/learnk8s/free-kubernetes)。我创建了此存储库以保持这些免费试用版本的最新列表，你可以使用这些试用版在真实基础设施上运行本书中的所有示例。图
    2.1 总结了前面项目符号中包含的信息。
- en: '![](../../OEBPS/Images/02-01.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-01.png)'
- en: Figure 2.1 Kubernetes cluster Local vs. Remote setups.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 Kubernetes 集群本地与远程设置。
- en: While these three options are all valid and have drawbacks, in the next sections,
    you will use Kubernetes KinD (Kubernetes in Docker, [https://kind.sigs.k8s.io/](https://kind.sigs.k8s.io/))
    to deploy the walking skeleton introduced in chapter 1 in a local Kubernetes environment
    running on your laptop/pc. Check the step-by-step tutorial located at [https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-2#creating-a-local-cluster-with-kubernetes-kind](https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-2#creating-a-local-cluster-with-kubernetes-kind)
    to create your local KinD cluster that we will use to deploy our walking skeleton,
    the Conference application.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这三个选项都是有效的，但都有缺点，在接下来的几节中，你将使用 Kubernetes KinD（Docker 中的 Kubernetes，[https://kind.sigs.k8s.io/](https://kind.sigs.k8s.io/））在你的笔记本电脑/电脑上运行的本地
    Kubernetes 环境中部署第 1 章中介绍的行走骨架。查看位于 [https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-2#creating-a-local-cluster-with-kubernetes-kind](https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-2#creating-a-local-cluster-with-kubernetes-kind)
    的分步教程，以创建我们将用于部署行走骨架、会议应用程序的本地 KinD 集群。
- en: Notice that the tutorial creates a local KinD cluster that simulates having
    three nodes and a special port mapping to allow our Ingress controller to route
    incoming traffic that we will send to http://localhost.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，教程创建了一个本地 KinD 集群，模拟拥有三个节点和特殊的端口映射，以允许我们的 Ingress 控制器路由我们发送到 http://localhost
    的传入流量。
- en: 2.1.2 Installing the walking skeleton
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 安装行走骨架
- en: 'To run containerized applications on top of Kubernetes, you will need to have
    each of the services packaged as a container image, plus you will need to define
    how these containers will be configured to run in your Kubernetes cluster. To
    do so, Kubernetes allows you to define different kinds of resources (using YAML
    format) to configure how your containers will run and communicate with each other.
    The most common kinds of resources are:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Kubernetes上运行容器化应用程序，您需要将每个服务打包成容器镜像，并且您需要定义这些容器将如何配置以在您的Kubernetes集群中运行。为此，Kubernetes允许您定义不同类型的资源（使用YAML格式）来配置您的容器将如何运行和相互通信。最常见的资源类型是：
- en: '*Deployments:* Declaratively define how many replicas of your container need
    to be up for your application to work correctly. Deployments also allow us to
    choose which container (or containers) we want to run and how these containers
    must be configured (using environment variables).'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*部署:* 声明性地定义您的容器需要多少个副本才能使应用程序正确运行。部署还允许我们选择我们想要运行的容器（或容器集），以及这些容器必须如何配置（使用环境变量）。'
- en: '*Services:* Declaratively define a high-level abstraction to route traffic
    to the containers created by your deployments. It also acts as a load balancer
    between the replicas inside your deployments. Services enable other services and
    applications inside the cluster to use the service name instead of the physical
    IP address of the containers to communicate, providing what is known as service
    discovery.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*服务:* 声明性地定义一个高级抽象，将流量路由到由您的部署创建的容器。它还充当部署内部副本之间的负载均衡器。服务使集群内的其他服务和应用程序能够使用服务名称而不是容器的物理IP地址进行通信，提供所谓的服务发现。'
- en: '*Ingress:* Declaratively define a route for routing traffic from outside the
    cluster to services inside the cluster. Using Ingress definitions, we can expose
    the services that are required by client applications running outside the cluster.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*入口:* 声明性地定义一个路由，用于将集群外部的流量路由到集群内部的服务。使用入口定义，我们可以暴露集群外运行的客户端应用程序所需的服务。'
- en: '*ConfigMap/secrets:* Declaratively define and store configuration objects to
    set up our service instances. Secrets are considered sensitive information that
    should have protected access.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ConfigMap/secret:* 声明性地定义和存储配置对象以设置我们的服务实例。Secret被认为是敏感信息，应该受到保护访问。'
- en: These YAML files will be complex and hard to manage if you have large applications
    with tens or hundreds of services. Keeping track of the changes and deploying
    applications by applying these files using `kubectl` becomes a complex job. It
    is beyond the scope of this book to cover a detailed view of these resources,
    and other resources are available such as the official Kubernetes documentation
    page ([https://kubernetes.io/docs/concepts/workloads/](https://kubernetes.io/docs/concepts/workloads/)).
    In this book, we will concentrate on how to deal with these resources for large
    applications and the tools that can help us with that task. The following section
    provides an overview of the tools to package and install components into your
    Kubernetes cluster.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有包含数十或数百个服务的大型应用程序，这些YAML文件将变得复杂且难以管理。跟踪更改并使用`kubectl`应用这些文件来部署应用程序成为一项复杂的工作。本书的范围不包括对这些资源的详细描述，其他资源如官方Kubernetes文档页面（[https://kubernetes.io/docs/concepts/workloads/](https://kubernetes.io/docs/concepts/workloads/)）也是可用的。在本书中，我们将专注于如何处理大型应用程序的资源以及可以帮助我们完成这项任务的工具。以下部分提供了将这些工具打包和安装到您的Kubernetes集群中的概述。
- en: Packaging and installing Kubernetes applications
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 打包和安装Kubernetes应用程序
- en: 'There are different tools to package and manage your Kubernetes applications.
    Most of the time, we can separate these tools into two main categories: templating
    engines and package managers. You will probably need both kinds of tools for real-life
    scenarios to get things done. Let’s discuss these two kinds of tools: why would
    you need a templating engine? What kind of packages do you want to manage?'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的工具用于打包和管理您的Kubernetes应用程序。大多数时候，我们可以将这些工具分为两大类：模板引擎和包管理器。在现实场景中，为了完成任务，您可能需要这两种类型的工具。让我们讨论这两种类型的工具：为什么你需要一个模板引擎？你想要管理哪种类型的包？
- en: A templating engine allows you to reuse the same resource definitions in different
    environments where applications might require slightly different parameters. The
    textbook example of the need to template your resources is database URLs. If your
    service needs to connect to different database instances in different environments,
    such as the testing database in the testing environment and the production database
    in the production environment, you want to avoid maintaining two copies of the
    same YAML file but with different URLs. Figure 2.2 shows how you can now add variables
    to the YAML files, and the engine will then find and replace these variables with
    different values depending on where you want to use the final (rendered) resource.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 模板引擎允许你在不同的环境中重用相同的资源定义，在这些环境中，应用程序可能需要略微不同的参数。资源模板化的典型例子是数据库 URL。如果你的服务需要连接到不同环境中的不同数据库实例，例如测试环境中的测试数据库和生产环境中的生产数据库，你希望避免维护两个具有不同
    URL 的相同 YAML 文件副本。图 2.2 展示了你现在如何向 YAML 文件添加变量，然后引擎会根据你想要使用最终（渲染）资源的位置找到并替换这些变量。
- en: '![](../../OEBPS/Images/02-02.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-02.png)'
- en: Figure 2.2 Templating engines render YAML resources by replacing variables.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 模板引擎通过替换变量来渲染 YAML 资源。
- en: 'Using a templating engine can save you a lot of time maintaining different
    copies of the same file, because when files start to pile up, maintaining them
    becomes a full-time job. There are several tools in the community to deal with
    templating Kubernetes files. Some tools just deal with YAML files, and some other
    tools are more targeted to Kubernetes resources specifically. Some projects that
    you should check out are:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模板引擎可以节省你维护相同文件不同副本的大量时间，因为当文件开始堆积时，维护它们就变成了一项全职工作。社区中有几个工具可以处理 Kubernetes
    文件的模板化。有些工具仅处理 YAML 文件，而有些工具则更专注于 Kubernetes 资源。以下是一些你应该检查的项目：
- en: '*Kustomize:* [https://kustomize.io/](https://kustomize.io/)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kustomize:* [https://kustomize.io/](https://kustomize.io/)'
- en: '*Carvel YTT:* [https://carvel.dev/ytt/](https://carvel.dev/ytt/)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Carvel YTT:* [https://carvel.dev/ytt/](https://carvel.dev/ytt/)'
- en: '*Helm Templates:* [https://helm.sh/docs/chart_best_practices/templates/#helm](https://helm.sh/docs/chart_best_practices/templates/#helm)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Helm Templates:* [https://helm.sh/docs/chart_best_practices/templates/#helm](https://helm.sh/docs/chart_best_practices/templates/#helm)'
- en: Now, what do you do with all these files? It is quite a natural urge to organize
    these files in logical packages. If you are building an application that is composed
    of different services, it might make sense to group all the resources related
    to a service inside the same directory or even in the same repository that contains
    the source code for that service. You also want to make sure that you can distribute
    these files to the teams deploying these services to different environments, and
    you quickly realize that you need to version these files in some way. This versioning
    might be related to the version of your service itself or with a high-level logical
    aggregation that makes sense for your application. When we talk about grouping,
    versioning, and distributing these resources, we are describing the responsibility
    of a package manager. Developers and operations teams are already used to working
    with package managers no matter the technology stack they use. Maven/Gradle for
    Java, NPM for NodeJS, APT-GET for Linux/Debian/Ubuntu packages, and more recently,
    containers and container registries for cloud-native applications. So, what does
    a package manager for YAML files look like? What are the package manager’s main
    responsibilities?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你该如何处理所有这些文件呢？将这些文件组织成逻辑包是一种很自然的冲动。如果你正在构建一个由不同服务组成的应用程序，将所有与某个服务相关的资源放在同一个目录内，甚至是在包含该服务源代码的同一个仓库中，这可能是有意义的。你还想确保能够将这些文件分发给部署这些服务的不同环境中的团队，并且你很快就会意识到你需要以某种方式对这些文件进行版本控制。这种版本控制可能与你的服务本身的版本有关，或者与对应用程序有意义的较高层次的逻辑聚合有关。当我们谈论分组、版本控制和分发这些资源时，我们正在描述包管理器的职责。开发者和运维团队已经习惯了使用包管理器，无论他们使用的技术栈是什么。Java
    的 Maven/Gradle，NodeJS 的 NPM，Linux/Debian/Ubuntu 软件包的 APT-GET，以及最近，用于云原生应用的容器和容器注册库。那么，YAML
    文件的包管理器是什么样的呢？包管理器的主要职责是什么？
- en: As a user, a package manager allows you to browse available packages and their
    metadata to decide which package you want to install. Once you have decided which
    package you want to use, you should be able to download it and then install it.
    Once the package is installed, you would expect, as a user, to be able to upgrade
    to a newer version of the package when it becomes available. Upgrading/updating
    a package requires manual intervention, meaning that as a user, you will explicitly
    tell the package manager to upgrade the installation of a certain package to a
    newer (or latest) version.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作为用户，包管理器允许你浏览可用的包及其元数据，以便决定你想安装哪个包。一旦你决定使用哪个包，你应该能够下载它然后安装它。一旦包安装完成，作为用户，你期望能够升级到新版本的包。升级/更新包需要手动干预，这意味着作为用户，你需要明确告诉包管理器将某个包的安装升级到新版本（或最新版本）。
- en: From a package provider’s point of view, a package manager should offer a convention
    and structure to create packages and a tool to package the files you want to distribute.
    Package managers deal with versions and dependencies, meaning that if you create
    a package, you must associate a version number with it. Some package managers
    use the *semver* (semantic versioning) approach, which uses three numbers to describe
    the package maturity (1.0.1 where these numbers represent the major, minor, and
    patch versions). A package manager doesn’t need to provide a centralized package
    repository, but they often do. This package repository is in charge of hosting
    packages for users to consume. Central repositories are useful because they provide
    access to developers with thousands of packages ready to be used. Some examples
    of these central repositories are Maven Central, NPM, Docker Hub, GitHub Container
    Registry, etc. These repositories are in charge of indexing the package’s metadata
    (which can include versions, labels, dependencies, and short descriptions) to
    make them searchable by users. These repositories also deal with access control
    to have public and private packages, but at the end of the day, the main responsibility
    of the package repository is to allow package producers to upload packages and
    package consumers to download packages from them (see figure 2.3).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从包提供者的角度来看，包管理器应该提供创建包的约定和结构，以及打包你想要分发的文件的工具。包管理器处理版本和依赖关系，这意味着如果你创建了一个包，你必须为其关联一个版本号。一些包管理器使用*semver*（语义版本控制）方法，它使用三个数字来描述包的成熟度（例如1.0.1，其中这些数字代表主版本、次版本和修补版本）。包管理器不需要提供集中的包仓库，但它们通常这么做。这个包仓库负责为用户提供包托管服务。集中式仓库很有用，因为它们为开发者提供了成千上万的可用包。这些集中式仓库的例子包括Maven
    Central、NPM、Docker Hub、GitHub Container Registry等。这些仓库负责索引包的元数据（这可能包括版本、标签、依赖关系和简短描述），以便用户可以搜索。这些仓库还处理访问控制，以拥有公共和私有包，但最终，包仓库的主要责任是允许包生产者上传包，以及包消费者从它们那里下载包（见图2.3）。
- en: '![](../../OEBPS/Images/02-03.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-03.png)'
- en: 'Figure 2.3 Package Managers’ responsibilities: build, package, and distribute'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 包管理器的责任：构建、打包和分发
- en: 'When we talk about Kubernetes, Helm is a very popular tool that provides both
    a package manager and a templating engine. But there are others worth looking
    into, such as:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论Kubernetes时，Helm是一个非常流行的工具，它既提供包管理器，又提供模板引擎。但还有其他值得关注的工具，例如：
- en: Imgpkg ([https://carvel.dev/imgpkg/](https://carvel.dev/imgpkg/)), which uses
    Container registries to store the packages.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Imgpkg ([https://carvel.dev/imgpkg/](https://carvel.dev/imgpkg/))，它使用容器注册库来存储包。
- en: Kapp ([https://carvel.dev/kapp/](https://carvel.dev/kapp/)), which provides
    higher-level abstractions to group resources as applications.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kapp ([https://carvel.dev/kapp/](https://carvel.dev/kapp/))，它提供了更高层次的抽象，可以将资源作为应用程序分组。
- en: Tools like Terraform and Pulumi that allow you to manage infrastructure as code.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似于Terraform和Pulumi这样的工具，允许你以代码的方式管理基础设施。
- en: In the following section, we will look at using Helm ([http://helm.sh](http://helm.sh))
    to install the Conference application into our Kubernetes cluster.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何使用Helm ([http://helm.sh](http://helm.sh))将会议应用程序安装到我们的Kubernetes集群中。
- en: 2.2 Installing the Conference application with a single command
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 使用单个命令安装会议应用程序
- en: Let’s install the Conference application introduced in chapter 1, section 1.4
    into our Kubernetes cluster using Helm. This Conference application allows conference
    organizers to receive proposals from potential speakers, evaluate these proposals,
    and keep an updated agenda with the approved submissions for the event. We will
    use this application throughout the book to exemplify the challenges that you
    will face while building real-life applications.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将第 1 章第 1.4 节中介绍的 Conference 应用程序使用 Helm 安装到我们的 Kubernetes 集群中。此 Conference
    应用程序允许会议组织者接收潜在演讲者的提案，评估这些提案，并保持已批准提交的更新议程。我们将在这本书中使用此应用程序来举例说明你在构建现实应用程序时可能遇到的挑战。
- en: NOTE For the complete list of steps, follow the step-by-step tutorial located
    at [https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-2](https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-2).
    It includes all the prerequisites to run the commands described in this section,
    such as creating a cluster and installing the command-line tools needed for the
    examples to work.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：要获取完整步骤列表，请遵循位于 [https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-2](https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-2)
    的逐步教程。它包括运行本节中描述的命令所需的所有先决条件，例如创建集群和安装示例工作所需的命令行工具。
- en: 'This application was built as a walking skeleton, which means it is not a complete
    application but has all the pieces required for the “Call for Proposals” flow
    to work. These services can be iterated further to support other flows and real-life
    scenarios. In the following sections, you will install the application into the
    cluster and interact with it to see how it behaves when it runs on top of Kubernetes.
    Let’s install the application with the following line:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此应用程序被构建为一个“行走骨架”，这意味着它不是一个完整的应用程序，但它包含了“提案征集”流程所需的所有组件。这些服务可以进一步迭代以支持其他流程和现实场景。在以下章节中，你将安装应用程序到集群中，并与它交互以查看它在
    Kubernetes 上运行时的行为。让我们使用以下命令安装应用程序：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You should see the output similar to listing 2.1.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到与列表 2.1 相似的输出。
- en: Listing 2.1 Helm installed the chart conference-app version 1.0.0
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.1 Helm 安装了 conference-app 版本 1.0.0
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note Since Helm 3.7+ you can package and distribute Helm Charts as OCI container
    images, the URL for the Helm Chart contains oci:// because this chart is hosted
    in Docker Hub, where the application containers are stored. Before Helm supported
    OCI images, you needed to manually add and fetch packages from a Helm Chart Repository,
    which used tar files to distribute these charts.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：自 Helm 3.7+ 开始，你可以将 Helm 图表打包并作为 OCI 容器镜像分发，Helm 图表的 URL 包含 oci://，因为此图表托管在
    Docker Hub 上，应用程序容器存储在那里。在 Helm 支持 OCI 镜像之前，你需要手动从 Helm 图表仓库添加和检索包，这些图表使用 tar
    文件进行分发。
- en: '`helm install` creates a Helm release, which means that you have created an
    application instance, in this case, the instance is called `conference`. With
    Helm, you can deploy multiple instances of the application if you want to. You
    can list Helm releases by running:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`helm install` 创建一个 Helm 发布，这意味着你已经创建了一个应用程序实例，在这种情况下，实例被称为 `conference`。使用
    Helm，如果你想要的话，可以部署多个应用程序实例。你可以通过运行以下命令列出 Helm 发布：'
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The output should look like figure 2.4.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该看起来像图 2.4。
- en: '![](../../OEBPS/Images/02-04.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-04.png)'
- en: Figure 2.4 List Helm releases
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 列出 Helm 发布版本
- en: NOTE If instead of using `helm install` you run `helm template oci://docker.io/salaboy/conference-app
    --version v1.0.0`. Helm will output the YAML files, which will apply against the
    cluster. There are situations where you might want to do that instead of `helm
    install`, for example, if you want to override values that the Helm Charts don’t
    allow you to parameterize or apply any other transformations before sending the
    request to Kubernetes.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你不使用 `helm install`，而是运行 `helm template oci://docker.io/salaboy/conference-app
    --version v1.0.0`，Helm 将输出 YAML 文件，这些文件将应用于集群。在某些情况下，你可能想这样做而不是 `helm install`，例如，如果你想覆盖
    Helm 图表不允许参数化的值或在对 Kubernetes 发送请求之前应用任何其他转换。
- en: 2.2.1 Verifying that the application is up and running
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 验证应用程序是否正在运行
- en: Once the application is deployed, containers will be downloaded to run on your
    laptop, which can take a while. Depending on your internet connection, the process
    can take up to 10 minutes because Kafka, PostgreSQL, and Redis will be downloaded
    alongside the application’s containers. The RESTARTS columns show how often the
    container has been restarted due to an error. In distributed applications, this
    is normal, as components might depend on each other, and when they are started
    at the same time, connections can fail. By design, applications should be able
    to recover from problems, and Kubernetes will automatically restart a failing
    container.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦应用程序部署，容器将被下载到您的笔记本电脑上运行，这可能需要一段时间。根据您的互联网连接，这个过程可能需要长达10分钟，因为Kafka、PostgreSQL和Redis将与应用程序容器一起下载。RESTARTS列显示了容器由于错误而重新启动的频率。在分布式应用程序中，这是正常的，因为组件可能相互依赖，当它们同时启动时，连接可能会失败。按照设计，应用程序应该能够从问题中恢复，并且Kubernetes将自动重启失败的容器。
- en: 'You can monitor the progress by listing all the pods running in your cluster,
    once again, using the `-owide` flag to get more information:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过列出您集群中运行的所有Pod来监控进度，再次使用`-owide`标志来获取更多信息：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The output should look like figure 2.5.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应类似于图2.5。
- en: '![](../../OEBPS/Images/02-05.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-05.png)'
- en: Listing 2.5 Listing application pods
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.5 列出应用程序Pod
- en: Something that you might notice in the list of pods is that we are not only
    running the application’s services, but we are also running Redis, PostgreSQL,
    and Kafka, because the C4P (Call for Proposals) and Agenda services need persistent
    storage. The application will be using Kafka to exchange asynchronous messages
    between services. Besides the services, we will have these two databases and a
    message broker (Kafka) running inside our Kubernetes cluster.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能在Pod列表中注意到，我们不仅运行了应用程序的服务，还运行了Redis、PostgreSQL和Kafka，因为C4P（Call for Proposals）和议程服务需要持久存储。应用程序将使用Kafka在服务之间交换异步消息。除了服务之外，我们还将运行这两个数据库和一个消息代理（Kafka）在我们的Kubernetes集群内部。
- en: In the output shown in figure 2.5 you need to pay attention to the READY and
    STATUS columns, where 1/1 in the READY column means that one replica of the container
    is running, and one is expected to be running. As you can see the RESTART column
    is showing 7 for the Call for Proposals Service (`conference-c4p-service`). This
    is because the service depends on Redis to be up and running for the service to
    be able to connect to it. While Redis is bootstrapping the application will try
    to connect, and if it fails, it will try to keep reconnecting. As soon as Redis
    is up, the service will connect to it. The same applies to Kafka and PostgreSQL.
    To quickly recap, our application services, the databases, and the message broker
    that we are running are shown in figure 2.6.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在图2.5所示的输出中，您需要注意READY和STATUS列，其中READY列中的1/1表示一个容器副本正在运行，另一个预期正在运行。如您所见，RESTART列显示Call
    for Proposals Service (`conference-c4p-service`)为7。这是因为该服务依赖于Redis处于运行状态，以便服务能够连接到它。当Redis启动时，应用程序将尝试连接，如果失败，它将尝试重新连接。一旦Redis启动，服务将连接到它。同样适用于Kafka和PostgreSQL。为了快速回顾，我们运行的应用程序服务、数据库和消息代理如图2.6所示。
- en: '![](../../OEBPS/Images/02-06.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-06.png)'
- en: Figure 2.6 Application services, databases, and message broker
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 应用程序服务、数据库和消息代理
- en: Notice that Pods can be scheduled in different nodes. You can check this in
    the `NODE` column; this is Kubernetes efficiently using the cluster resources.
    If all the Pods are up and running, you’ve made it! The application is now up
    and running, and you can access it by pointing your favorite browser to `http://localhost`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意Pod可以在不同的节点上调度。您可以在`NODE`列中检查这一点；这是Kubernetes高效使用集群资源。如果所有Pod都处于运行状态，您就成功了！现在应用程序正在运行，您可以通过将您喜欢的浏览器指向`http://localhost`来访问它。
- en: 'If you are interested in Helm and building your own Conference application
    Helm Chart, I recommend you to check the source code provided with the tutorials:
    [https://github.com/salaboy/platforms-on-k8s/tree/main/conference-application/helm/conference-app](https://github.com/salaboy/platforms-on-k8s/tree/main/conference-application/helm/conference-app).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对Helm和构建自己的Conference应用程序Helm Chart感兴趣，我建议您查看教程中提供的源代码：[https://github.com/salaboy/platforms-on-k8s/tree/main/conference-application/helm/conference-app](https://github.com/salaboy/platforms-on-k8s/tree/main/conference-application/helm/conference-app)。
- en: 2.2.2 Interacting with your application
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 与您的应用程序交互
- en: 'In the previous section, we installed the application into our local Kubernetes
    cluster. In this section, we will quickly interact with the application to understand
    how the services interact to accomplish a simple use case: Receiving and approving
    proposals. Remember that you can access the application by pointing your browser
    to `http://localhost`. The Conference application should look like figure 2.7.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们将应用程序安装到我们的本地Kubernetes集群中。在本节中，我们将快速与该应用程序交互，以了解服务如何交互以完成一个简单的用例：接收和批准提案。请记住，您可以通过将浏览器指向`http://localhost`来访问应用程序。会议应用程序应类似于图2.7。
- en: '![](../../OEBPS/Images/02-07.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-07.png)'
- en: Figure 2.7 Conference landing page
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 会议着陆页
- en: If you switch to the Agenda section now, you should see something like figure
    2.8.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您现在切换到议程部分，您应该会看到类似于图2.8的内容。
- en: '![](../../OEBPS/Images/02-08.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-08.png)'
- en: Figure 2.8 Conference empty Agenda when we first install the application
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 首次安装应用程序时的会议空议程
- en: The application’s Agenda page lists all the talks scheduled for the conference.
    Potential speakers can submit proposals that the conference organizers will review.
    When you start the application for the first time, there will be no talks on the
    agenda, but you can now go ahead and submit a proposal from the Call from Proposals
    section. Check figure 2.9.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序的议程页面列出了会议安排的所有演讲。潜在的演讲者可以提交会议组织者将审查的提案。当您第一次启动应用程序时，议程上不会有任何演讲，但现在您可以从提案征集部分提交提案。查看图2.9。
- en: '![](../../OEBPS/Images/02-09.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-09.png)'
- en: Figure 2.9 Submitting a proposal for organizers to review
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 向组织者提交提案以供审查
- en: Notice that there are four fields (Title, Description, Author, and Email) in
    the form to submit a proposal. Fill in all the fields and submit by clicking the
    Submit Proposal button at the bottom of the form. The organizers will use this
    information to evaluate your proposal and get in touch with you via email if your
    proposal gets approved or rejected. Once the proposal is submitted, you can go
    to the Back Office (click the arrow pointing to the right at the top menu) and
    check the Review Proposals tab, where you can Approve or Reject submitted proposals.
    You will be acting as a conference organizer on this screen; see figure 2.10.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在提交提案的表单中有四个字段（标题、描述、作者和电子邮件）。填写所有字段，然后通过点击表单底部的提交提案按钮提交。组织者将使用这些信息来评估您的提案，并在您的提案被批准或拒绝时通过电子邮件与您联系。一旦提交提案，您就可以转到后台办公室（点击顶部菜单中的指向右方的箭头）并检查审查提案标签页，在那里您可以批准或拒绝提交的提案。您将在这个屏幕上扮演会议组织者的角色；见图2.10。
- en: '![](../../OEBPS/Images/02-10.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-10.png)'
- en: Figure 2.10 Conference organizers can Accept or Reject incoming proposals
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10 会议组织者可以接受或拒绝传入的提案
- en: Approved proposals will appear on the Main Agenda page. Attendees who visit
    the page at this stage can glance at the conference’s main speakers. Figure 2.11
    shows our freshly approved proposal in the Agenda section of the main conference
    page.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 已批准的提案将出现在主议程页面上。在此阶段访问该页面的与会者可以浏览会议的主要演讲者。图2.11显示了我们在主要会议页面议程部分的新批准提案。
- en: '![](../../OEBPS/Images/02-11.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-11.png)'
- en: Figure 2.11 Your proposal is now live on the agenda!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11 您的提案现在已在议程上实时发布！
- en: 'At this stage, the potential speaker should have received an email about the
    approval or rejection of their proposal. You can check this by looking at the
    notification service logs, using `kubectl` from your terminal; see figure 2.12
    for the output of the command:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，潜在的演讲者应该已经收到了关于其提案批准或拒绝的电子邮件。您可以通过查看通知服务日志来检查这一点，使用终端中的`kubectl`；查看图2.12中的命令输出：
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../../OEBPS/Images/02-12.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-12.png)'
- en: Figure 2.12 Notifications service logs (emails and events)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12 通知服务日志（电子邮件和事件）
- en: These logs show two important aspects of the application. First, notifications
    are sent via emails to potential speakers. The organizers need to keep track of
    these communications. On the conference Back Office page, you can find the Notifications
    tab, where the content of the notifications is shown to the organizers (see figure
    2.13).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些日志显示了应用程序的两个重要方面。首先，通知通过电子邮件发送给潜在的演讲者。组织者需要跟踪这些通信。在会议后台办公室页面上，您可以找到通知标签页，其中显示了通知的内容（见图2.13）。
- en: '![](../../OEBPS/Images/02-13.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-13.png)'
- en: Figure 2.13 Notifications displayed in the Back Office
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13 在后台办公室显示的通知
- en: The second aspect displayed here is Events. All services from this application
    are emitting events when relevant actions are performed. The notification service
    is emitting an event, in this case to Kafka, for every notification that is being
    sent. This allows other services and applications to integrate with the application
    services asynchronously. Figure 2.14 shows the Events section of the Back Office.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这里显示的第二方面是事件。当执行相关操作时，此应用程序的所有服务都会发出事件。通知服务正在发出事件，在这种情况下是向 Kafka 发送通知。这允许其他服务和应用程序异步地与应用程序服务集成。图
    2.14 显示了后台办公室的事件部分。
- en: '![](../../OEBPS/Images/02-14.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-14.png)'
- en: Figure 2.14 All service events in the Back Office
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.14 后台办公室中所有服务事件
- en: Figure 2.14 shows all events emitted by the application services; notice that
    you can see all the meaningful operations being performed by the services to fulfill
    the Call for Proposals flow (New Proposal > New Agenda Item > Proposal Approved
    > Notification Sent).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.14 显示了应用程序服务发出的所有事件；请注意，您可以看到服务执行的所有有意义操作，以完成提案征集流程（新提案 > 新议程项 > 提案批准 >
    发送通知）。
- en: If you made it so far, congrats, the Conference application is working as expected.
    I encourage you to submit another proposal and reject it, to validate that the
    correct notification and events are sent to the potential speaker.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经做到这一步，恭喜您，会议应用程序按预期工作。我鼓励您提交另一个提案并拒绝它，以验证是否向潜在演讲者发送了正确的通知和事件。
- en: In this section, you installed the Conference application using Helm. Then we
    verified that the application is up and running and that potential speakers can
    submit proposals, while conference organizers can approve or reject these proposals.
    The decisions will send notifications to potential speakers via email.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您使用 Helm 安装了会议应用程序。然后我们验证了应用程序正在运行，并且潜在演讲者可以提交提案，同时会议组织者可以批准或拒绝这些提案。这些决定将通过电子邮件通知潜在演讲者。
- en: This simple application allows us to demonstrate a basic use case that we can
    now expand and improve to support real users. We have seen that installing a new
    instance of the application is quite simple. We used Helm to install a set of
    services that are connected as well as some infrastructural components such as
    Redis, PostgreSQL, and in the next section, we will go deeper into understanding
    what we have installed and how the application is working.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的应用程序使我们能够演示一个基本用例，我们现在可以扩展和改进以支持真实用户。我们已经看到安装应用程序的新实例相当简单。我们使用 Helm 安装了一组相互连接的服务以及一些基础设施组件，如
    Redis、PostgreSQL，在下一节中，我们将更深入地了解我们安装了什么以及应用程序是如何工作的。
- en: 2.3 Inspecting the walking skeleton
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 检查行走骨架
- en: If you have been using Kubernetes for a while, you probably know all about `kubectl`.
    Because this application version uses native Kubernetes deployments and services,
    you can inspect and troubleshoot these Kubernetes resources using `kubectl`.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经使用 Kubernetes 一段时间了，您可能已经对 `kubectl` 非常熟悉。因为这个应用程序版本使用的是原生 Kubernetes 部署和服务，您可以使用
    `kubectl` 检查和调试这些 Kubernetes 资源。
- en: Usually, instead of just looking at the pods running (with `kubectl get pods`),
    to understand and operate the application, you will be looking at services and
    deployments. Let’s explore the deployment resources first.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，除了查看正在运行的 pod（使用 `kubectl get pods`）之外，为了理解和操作应用程序，您将查看服务和部署。让我们首先探索部署资源。
- en: 2.3.1 Kubernetes deployments basics
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 Kubernetes 部署基础
- en: 'Let’s start with deployments. Deployments in Kubernetes are in charge of containing
    the recipe for running our containers. Deployments are also in charge of defining
    how containers will run and how they will be upgraded to newer versions when needed.
    By looking at the deployment details, you can get very useful information, such
    as:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从部署开始。在 Kubernetes 中，部署负责包含运行我们容器的配方。部署还负责定义容器将如何运行，以及当需要时如何升级到新版本。通过查看部署详情，您可以获得非常有用的信息，例如：
- en: The *container* that this deployment is using. Notice that this is just a simple
    Docker container, meaning that you can even run this container locally if you
    want to with `docker run`. This is fundamental to troubleshooting problems.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个部署所使用的 *容器*。请注意，这只是一个简单的 Docker 容器，这意味着如果您想的话，甚至可以在本地使用 `docker run` 运行这个容器。这对于故障排除是基本的。
- en: The number of *replicas* required by the deployment. For this example, it is
    set to 1, but you will change this in the next section. More replicas add more
    resiliency to the application, because these replicas can go down. Kubernetes
    will spawn new instances to keep the number of desired replicas up at all times.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署所需的 *副本* 数量。在本例中，它被设置为 1，但在下一节中你将更改此设置。更多的副本可以增加应用程序的弹性，因为这些副本可能会失败。Kubernetes
    将会启动新的实例以保持所需副本数始终不变。
- en: The *resource allocation* for the container. Depending on the load and the technology
    stack that you used to build your service, you will need to fine-tune how many
    resources Kubernetes you allow your container to use.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器的 *资源分配*。根据负载和构建服务所使用的负载和技术堆栈，你可能需要微调 Kubernetes 允许容器使用的资源数量。
- en: 'The status of the *readiness* and *liveness probes.* Kubernetes, by default,
    will monitor the health of your container. It does that by executing two probes:
    1) The “readiness probe” checks if the container is ready to answer requests,
    and 2) The “liveness probe” checks if the main process of the container is running.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*就绪* 和 *存活* 探针的状态。默认情况下，Kubernetes 会监控你的容器健康状况。它是通过执行两个探针来完成的：1) “就绪探针”检查容器是否准备好响应请求，2)
    “存活探针”检查容器的主进程是否正在运行。'
- en: The rolling updates strategy defines how our Pods will be updated to avoid downtime
    for our users. With the `RollingUpdateStrategy`, you can define how many replicas
    are allowed while triggering and updating to a newer version.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滚动更新策略定义了我们的 Pods 将如何更新以避免对用户造成停机。使用 `RollingUpdateStrategy`，你可以定义在触发和更新到新版本时允许多少副本。
- en: 'First, let’s list all the available deployments with:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们列出所有可用的部署：
- en: '[PRE5]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output should look like listing 2.2\.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应类似于列表 2.2。
- en: Listing 2.2 Listing your application’s deployments
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.2 列出应用程序的部署
- en: '[PRE6]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 2.3.2 Exploring deployments
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 探索部署
- en: In the following example, you will describe the Frontend deployment. You can
    describe each deployment in more detail with `kubectl describe deploy conference-frontend-deployment`
    (see listing 2.3).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，你将描述前端部署。你可以使用 `kubectl describe deploy conference-frontend-deployment`（参见列表
    2.3）来更详细地描述每个部署。
- en: Listing 2.3 Describing a deployment to see its details
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.3 描述部署以查看其详细信息
- en: '[PRE7]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① Shows the replicas available for this deployment. This gives you a quick indication
    about the state of your deployment.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ① 显示此部署可用的副本。这为你提供了关于部署状态的快速指示。
- en: ② The container image, including the name and tag used for this service.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ② 容器镜像，包括用于此服务的名称和标签。
- en: ③ The environment variables used to configure this container.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 用于配置此容器的环境变量。
- en: ④ Events shows us relevant information about our Kubernetes resources—in this
    case, when the replica was created.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 事件显示有关我们的 Kubernetes 资源的相关信息——在本例中，当副本被创建时。
- en: Listing 2.3 shows that describing deployments in this way is very helpful if
    for some reason the deployment is not working as expected. For example, if the
    number of replicas required is not met, describing the resource will give you
    insights into where the problem might be. Always check at the bottom for the events
    associated with the resource to get more insights about the resource status. In
    this case, the deployment was scaled to have one replica 48 minutes ago.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.3 显示，如果由于某种原因部署未按预期工作，以这种方式描述部署非常有帮助。例如，如果所需的副本数未满足，描述资源将帮助你了解问题可能出在哪里。始终检查底部与资源相关的事件，以获取更多关于资源状态的见解。在这种情况下，部署在
    48 分钟前扩展到有一个副本。
- en: As mentioned before, deployments are also responsible for coordinating version
    or configuration upgrades and rollbacks. The deployment update strategy is set
    by default to “Rolling ,” which means that the deployment will incrementally upgrade
    pods one after the other to minimize downtime. An alternative strategy called
    `Recreate` can be set, which will shut down all the pods and create new ones.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，部署还负责协调版本或配置升级和回滚。默认情况下，部署更新策略设置为“滚动”，这意味着部署将逐步升级一个接一个的 pods 以最小化停机时间。可以设置一个名为
    `Recreate` 的替代策略，该策略将关闭所有 pods 并创建新的。
- en: In contrast with pods, deployments are not ephemeral; hence, if you create a
    `Deployment`, it will be there for you to query no matter if the containers under
    the hood are failing. By default, when you create a deployment resource, Kubernetes
    creates an intermediate resource for handling and checking the deployment–requested
    replicas.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 与 pod 相比，部署不是短暂的；因此，如果您创建了一个 `Deployment`，无论底层的容器是否失败，它都会在那里供您查询。默认情况下，当您创建部署资源时，Kubernetes
    会创建一个中间资源来处理和检查部署请求的副本。
- en: 2.3.3 ReplicaSets
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 ReplicaSets
- en: 'Having multiple replicas of your containers is an important feature to scale
    your applications. If your application is experiencing loads of traffic from your
    users, you can easily scale up the number of replicas of your services to accommodate
    all the incoming requests. Similarly, if your application is not experiencing
    a large number of requests, these replicas can be scaled down to save resources.
    The object created by Kubernetes is called `ReplicaSet`, and it can be queried
    by running:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有多个容器副本是扩展应用程序的重要功能。如果您的应用程序正在经历来自用户的巨大流量，您可以轻松增加服务的副本数量以适应所有传入的请求。同样，如果您的应用程序没有经历大量的请求，这些副本可以缩小以节省资源。Kubernetes
    创建的对象称为 `ReplicaSet`，可以通过运行以下命令进行查询：
- en: '[PRE8]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The output should look like listing 2.4.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该看起来像列表 2.4。
- en: Listing 2.4 Listing the deployment’s ReplicaSets
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.4 列出部署的 ReplicaSets
- en: '[PRE9]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: These `ReplicaSet` objects are fully managed by the deployment’s resource, and
    usually, you shouldn’t need to deal with them. ReplicaSets are also essential
    when dealing with rolling updates, and you can find more information about this
    topic at [https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/](https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/).
    You will be performing updates to the application with Helm in later chapters,
    where these mechanisms will kick in.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 `ReplicaSet` 对象完全由部署的资源管理，通常，您不需要处理它们。ReplicaSets 在处理滚动更新时也非常重要，您可以在 [https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/](https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/)
    找到更多关于这个主题的信息。您将在后面的章节中使用 Helm 对应用程序进行更新，届时这些机制将启动。
- en: 'If you want to change the number of replicas for a deployment, you once again
    can use `kubectl` to do so:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想更改部署的副本数量，您仍然可以使用 `kubectl` 来实现：
- en: '[PRE10]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can try this out with the Frontend deployment:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以用前端部署来尝试这个操作：
- en: '[PRE11]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If we now list the application pods, we will see that there are two replicas
    for the frontend service:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在列出应用程序的 pod，我们会看到前端服务有两个副本：
- en: '[PRE12]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This command changes the deployment resource in Kubernetes and triggers the
    creation of a second replica for the Frontend deployment. Increasing the number
    of replicas of your user-facing services is quite common because it is the service
    that all users will hit when visiting the conference page.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令更改 Kubernetes 中的部署资源并触发为前端部署创建第二个副本。增加面向用户服务的副本数量相当常见，因为所有用户在访问会议页面时都会访问该服务。
- en: 'If we access the application right now, as end users we will not notice any
    difference, but every time we refresh, a different replica might serve us. To
    make this more evident, we can turn on a feature that is built into the Frontend
    service, which shows us more information about the application containers. You
    can enable this feature by setting an environment variable:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在访问应用程序，作为最终用户，我们不会注意到任何区别，但每次刷新时，可能由不同的副本为我们提供服务。为了使这一点更加明显，我们可以打开内置在前端服务中的一个功能，该功能显示有关应用程序容器的更多信息。您可以通过设置环境变量来启用此功能：
- en: '[PRE13]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Notice that when you change the deployment object configuration (anything inside
    `spec.template.spec` block) the rolling update mechanism of the Deployment resource
    will kick in. All the existing pods managed by this deployment will be upgraded
    to have the new specification(in this example to include the new `FEATURE_DEBUG_ENABLED`
    environment variable). This upgrade, by default, will start a new pod with the
    new specification and wait for it to be ready before terminating the old version
    of the pod. This process will be repeated until all the pods (replicas for the
    deployment) are using the new configuration.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当你更改部署对象配置（`spec.template.spec` 块内的任何内容）时，Deployment 资源的滚动更新机制将会启动。该部署管理的所有现有
    Pod 都将升级到新的规范（在本例中包括新的 `FEATURE_DEBUG_ENABLED` 环境变量）。默认情况下，这将启动一个新的 Pod 并使用新的规范，在终止旧版本的
    Pod 之前等待它就绪。这个过程将重复进行，直到所有 Pod（部署的副本）都使用新的配置。
- en: If you access the application again in your browser (you might need to access
    using Incognito Mode if the browser cached the website), in the Back Office section,
    there is a new Debug tab. You can see the Pod Name, Pod IP, the Namespace, and
    the Node name where the Pod is running for all services (figure 2.15).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你再次在浏览器中访问应用程序（如果浏览器缓存了网站，你可能需要使用隐身模式访问），在后台办公室部分，有一个新的调试标签页。你可以看到所有服务的 Pod
    名称、Pod IP、命名空间以及 Pod 运行的节点名称（如图 2.15）。
- en: '![](../../OEBPS/Images/02-15.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-15.png)'
- en: 'Figure 2.15 First replica of the Frontend answering your request (running on
    Node Name: dev-worker)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.15 前端第一个副本响应你的请求（运行在节点名称：dev-worker）
- en: If you wait for 3 seconds, the page will automatically refresh, and you should
    see the second replica answering this time, if not wait for the next cycle (figure
    2.16).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你等待 3 秒钟，页面将自动刷新，你应该看到这次是第二个副本在响应，如果不是，请等待下一个周期（如图 2.16）。
- en: '![](../../OEBPS/Images/02-16.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-16.png)'
- en: 'Figure 2.16 Second replica of the Frontend answering your request (running
    on Node Name: dev- worker3)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.16 前端第二个副本响应你的请求（运行在节点名称：dev-worker3）
- en: By default, Kubernetes will load-balance the requests between the replicas.
    Being able to scale by just changing the number of replicas, there is no need
    to deploy anything new, Kubernetes will provision a new pod (with a new container
    in it) to deal with more traffic. Kubernetes will also make sure that there is
    the amount of desired replicas at all times. You can test this by deleting one
    pod and watching how Kubernetes recreates it automatically. For this scenario,
    you need to be careful, because the web application frontend is executing several
    requests to fetch the HTML, CSS, and JavaScript libraries; hence, each of these
    requests can land in a different replica.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Kubernetes 将在副本之间进行请求的负载均衡。只需通过更改副本的数量即可进行扩展，无需部署任何新内容，Kubernetes 将分配一个新的
    Pod（其中包含新的容器）来处理更多的流量。Kubernetes 还将确保始终存在所需数量的副本。你可以通过删除一个 Pod 并观察 Kubernetes
    如何自动重新创建它来测试这一点。对于这种场景，你需要小心，因为网络应用程序的前端正在执行多个请求以获取 HTML、CSS 和 JavaScript 库；因此，每个请求都可能落在不同的副本上。
- en: 2.3.4 Connecting services
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.4 连接服务
- en: We have looked at deployments, which are in charge of getting our containers
    up and running and keeping them that way, but so far, these containers can only
    be accessed inside the Kubernetes cluster. If we want other services to interact
    with these containers, we need to look at another Kubernetes resource called `Service`.
    Kubernetes provides an advanced service-discovery mechanism that allows services
    to communicate with each other by just knowing their names. This is essential
    for connecting many services without knowing IP addresses of Kubernetes pods that
    can change over time, as they can be upgraded, rescheduled to a different node,
    or just restarted with a new IP address when something goes wrong.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了部署，它们负责将我们的容器启动并运行，并保持这种状态，但到目前为止，这些容器只能在 Kubernetes 集群内部访问。如果我们想让其他服务与这些容器交互，我们需要查看另一个名为
    `Service` 的 Kubernetes 资源。Kubernetes 提供了一种高级的服务发现机制，允许服务通过仅知道它们的名称来相互通信。这对于连接许多服务而不需要知道
    Kubernetes Pod 的 IP 地址至关重要，因为随着时间的推移，这些 IP 地址可能会发生变化，例如，它们可以被升级、重新调度到不同的节点，或者在出现问题时使用新的
    IP 地址重新启动。
- en: 2.3.5 Exploring services
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.5 探索服务
- en: 'To expose your containers to other services, you need to use a `Kubernetes
    Service` resource. Each application service defines this `Service` resource, so
    other services and clients can connect to them. In Kubernetes, services will be
    in charge of routing traffic to your application containers. These services represent
    a logical name that you can use to abstract where your containers run. If you
    have multiple replicas of your containers, the service resource will be in charge
    of load balancing the traffic among all the replicas. You can list all the services
    by running:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 要将您的容器暴露给其他服务，您需要使用一个 `Kubernetes Service` 资源。每个应用程序服务都定义了这个 `Service` 资源，因此其他服务和客户端可以连接到它们。在
    Kubernetes 中，服务将负责将流量路由到您的应用程序容器。这些服务代表了一个逻辑名称，您可以使用它来抽象容器运行的位置。如果您有多个容器的副本，服务资源将负责在所有副本之间进行流量负载均衡。您可以通过运行以下命令来列出所有服务：
- en: '[PRE14]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: After running the command, you should see something like listing 2.5.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 运行命令后，您应该看到类似于列表 2.5 的内容。
- en: Listing 2.5 Listing application’s services
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.5 列出应用程序的服务
- en: '[PRE15]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And you can also describe a service to see more information about it with:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用以下命令来描述一个服务，以获取更多关于它的信息：
- en: '[PRE16]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This should give you something like we see in listing 2.6\. Services and deployments
    are linked by the Selector property, highlighted in the following image. In other
    words, the service will route traffic to all the pods created by a deployment
    containing the label `app=frontend`.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '这应该会给出类似于列表 2.6 中的内容。服务和部署通过选择器属性链接，如下图中突出显示。换句话说，服务将路由流量到包含标签 `app=frontend`
    的部署创建的所有 pod。 '
- en: Listing 2.6 Describing the Frontend service
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.6 描述前端服务
- en: '[PRE17]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ① The selector used to match a service and a deployment
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ① 用于匹配服务和部署的选择器
- en: 2.3.6 Service discovery in Kubernetes
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.6 Kubernetes 中的服务发现
- en: By using services, if your application service needs to send a request to any
    other service, it can use the Kubernetes service’s name and port; in most cases,
    you can use port 80 if you are using HTTP requests, so you only need to use the
    service name. If you look at the source code of the services, you will see that
    HTTP requests are created against the service name; no IP addresses or Ports are
    needed.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用服务，如果您的应用程序服务需要向任何其他服务发送请求，它可以使用 Kubernetes 服务的名称和端口；在大多数情况下，如果您使用 HTTP
    请求，您可以使用端口 80，因此您只需要使用服务名称。如果您查看服务的源代码，您将看到 HTTP 请求是对服务名称的创建；不需要 IP 地址或端口。
- en: Finally, if you want to expose your services outside the Kubernetes cluster,
    you need an Ingress resource. As the name represents, this Kubernetes resource
    is in charge of routing traffic from outside the cluster to services that are
    inside the cluster. Usually, you will not expose multiple services, limiting the
    entry points for your applications.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果您想将服务暴露在 Kubernetes 集群之外，您需要一个 Ingress 资源。正如其名称所表示的，这个 Kubernetes 资源负责将集群外部的流量路由到集群内部的服务。通常，您不会暴露多个服务，这限制了您应用程序的入口点。
- en: 'You can get all the available Ingress resources by running the following command:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过运行以下命令来获取所有可用的 Ingress 资源：
- en: '[PRE18]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The output should look like listing 2.7.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该看起来像列表 2.7。
- en: Listing 2.7 Listing the application’s Ingress resources
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.7 列出应用程序的 Ingress 资源
- en: '[PRE19]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And then you can describe the Ingress resource in the same way as you did with
    other resource types to get more information about it:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以像描述其他资源类型一样描述 Ingress 资源，以获取更多关于它的信息：
- en: '[PRE20]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: You should expect the output to look like listing 2.8.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该期望输出看起来像列表 2.8。
- en: Listing 2.8 Describing the Ingress resource
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.8 描述 Ingress 资源
- en: '[PRE21]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ① All traffic going to `'`/`'` will go to the frontend:80 service.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ① 所有前往 `'`/`'` 的流量都将路由到前端：80 服务。
- en: As you can see, Ingress also uses the service’s name to route traffic. For this
    to work, you need an Ingress controller, like we installed when we created the
    KinD cluster. If you are running in a cloud provider, you might need to install
    an Ingress controller.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，Ingress 也使用服务的名称来路由流量。为了使其工作，您需要一个 Ingress 控制器，就像我们在创建 KinD 集群时安装的那样。如果您在云服务提供商上运行，可能需要安装一个
    Ingress 控制器。
- en: 'The following spreadsheet is a community resource created to keep track of
    the different options of Ingress controllers that are available for you to use:
    [http://mng.bz/K9Bn](http://mng.bz/K9Bn).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 以下电子表格是一个社区资源，用于跟踪您可用的不同 Ingress 控制器的选项：[http://mng.bz/K9Bn](http://mng.bz/K9Bn)。
- en: With Ingresses, you can configure a single entry-point and use path-based routing
    to redirect traffic to each service you need to expose. The previous Ingress resource
    in listing 2.8 routes all the traffic sent to `/` to the `frontend` service. Notice
    that Ingress rules are pretty simple, and you shouldn’t add any business logic
    routing at this level.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Ingress，您可以配置单个入口点，并使用基于路径的路由将流量重定向到您需要公开的每个服务。列表 2.8 中的上一个 Ingress 资源将发送到
    `/` 的所有流量路由到 `frontend` 服务。请注意，Ingress 规则相当简单，您不应在此级别添加任何业务逻辑路由。
- en: 2.3.7 Troubleshooting internal services
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.7 故障排除内部服务
- en: 'Sometimes, it is important to access internal services to debug or troubleshoot
    services that are not working. For such situations, you can use the `kubectl port-forward`
    command to temporarily access services that are not exposed outside of the cluster
    using an Ingress resource. For example, to access the Agenda service without going
    through the Frontend you can use the following command:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，访问内部服务以调试或排除不工作的服务的问题很重要。对于此类情况，您可以使用 `kubectl port-forward` 命令临时访问使用 Ingress
    资源未公开于集群外部的服务。例如，要访问 Agenda 服务而不通过前端，可以使用以下命令：
- en: '[PRE22]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You should see the following output (listing 2.9) and make sure that you don’t
    kill the command.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下输出（列表 2.9）并确保不要终止该命令。
- en: Listing 2.9 kubectl port-forward allows you to expose a service for debugging
    purposes
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.9 `kubectl port-forward` 允许您为调试目的公开服务
- en: '[PRE23]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: And then using your browser, use `curl` in a different tab or any other tool
    to point to `http://localhost:8080/service/info` to access the exposed Agenda
    service. The following listing shows how you can `curl` the Agenda service info
    endpoint and print a pretty/colorful JSON payload with the help of `jq`, which
    you must install separately.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用您的浏览器，在另一个标签页中使用 `curl` 或其他任何工具指向 `http://localhost:8080/service/info` 以访问公开的
    Agenda 服务。以下列表显示了如何使用 `curl` 访问 Agenda 服务信息端点，并使用 `jq`（您必须单独安装）打印一个漂亮的/彩色的 JSON
    有效负载。
- en: Listing 2.10 curl localhost:8080 to access Agenda service using port-forward
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.10 使用 port-forward 访问 Agenda 服务
- en: '[PRE24]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In this section, you have inspected the main Kubernetes resources that were
    created to run your application’s containers inside Kubernetes. By looking at
    these resources and their relationships, you can troubleshoot problems when they
    arise.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您已经检查了为在 Kubernetes 内运行您的应用程序容器而创建的主要 Kubernetes 资源。通过查看这些资源及其关系，您可以在出现问题时进行故障排除。
- en: For everyday operations, the `kubectl` command line tool might not be optimal,
    and different dashboards can be used to explore and manage your Kubernetes workloads,
    such as k9s ([https://k9scli.io/](https://k9scli.io/)), the Kubernetes dashboard
    ([https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/))
    and Skooner ([https://github.com/skooner-k8s/skooner](https://github.com/skooner-k8s/skooner)).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于日常操作，`kubectl` 命令行工具可能不是最佳选择，可以使用不同的仪表板来探索和管理您的 Kubernetes 工作负载，例如 k9s ([https://k9scli.io/](https://k9scli.io/))、Kubernetes
    仪表板 ([https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/))
    和 Skooner ([https://github.com/skooner-k8s/skooner](https://github.com/skooner-k8s/skooner))。
- en: 2.4 Cloud-native application challenges
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 云原生应用程序挑战
- en: In contrast to a monolithic application, which will go down entirely if something
    goes wrong, cloud-native applications shouldn’t crash if a service goes down.
    Cloud-native applications are designed for failure and should keep providing valuable
    functionality in the case of errors. A degraded service while fixing problems
    is better than having no access to the application. In this section, you will
    change some of the service configurations in Kubernetes to understand how the
    application will behave in different situations.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 与如果出现问题整个应用程序会崩溃的单体应用程序相比，云原生应用程序在服务崩溃时不应崩溃。云原生应用程序是为故障而设计的，应在出现错误的情况下继续提供有价值的功能。在修复问题期间提供降级服务比无法访问应用程序要好。在本节中，您将更改
    Kubernetes 中的一些服务配置，以了解应用程序在不同情况下的行为。
- en: In some cases, application/service developers will need to make sure that they
    build their services to be resilient and Kubernetes or the infrastructure will
    solve some concerns.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，应用程序/服务开发人员需要确保他们构建的服务具有弹性，Kubernetes 或基础设施将解决一些问题。
- en: 'This section covers some of the most common challenges associated with cloud-native
    applications. I find it useful to know what are the things that are going to go
    wrong in advance rather than when I am already building and delivering the application.
    This is not an extensive list; it is just the beginning to make sure that you
    don’t get stuck with problems that are widely known. The following sections will
    exemplify and highlight these challenges with the Conference application:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了与云原生应用程序相关的一些最常见挑战。我发现提前知道将要出错的事情比我在构建和交付应用程序时更有用。这不是一个详尽的列表；这只是确保您不会遇到众所周知的问题的开始。以下各节将通过会议应用程序举例说明并突出这些挑战：
- en: '*Downtime is not allowed*: If you are building and running a cloud-native application
    on top of Kubernetes, and you are still suffering from application downtime, then
    you are not capitalizing on the advantages of the technology stack that you are
    using.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不允许停机*：如果您在 Kubernetes 上构建和运行云原生应用程序，并且您仍在遭受应用程序停机，那么您没有充分利用您所使用的技术堆栈的优势。'
- en: '*Service’s built-in resiliency*: Downstream services will go down, and you
    need to ensure that your services are prepared for that. Kubernetes helps with
    dynamic service discovery, but that is not enough for your application to be resilient.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*服务的内置弹性*：下游服务会崩溃，您需要确保您的服务为这种情况做好准备。Kubernetes 帮助进行动态服务发现，但这对于您的应用程序具有弹性来说还不够。'
- en: '*Dealing with the application state is not trivial*: We must understand each
    service’s infrastructural requirements to allow Kubernetes to scale up and down
    our services efficiently.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*处理应用程序状态并非易事*：我们必须了解每个服务的架构需求，以便让 Kubernetes 能够高效地扩展和缩减我们的服务。'
- en: '*Inconsistent data*: A common problem of working with distributed applications
    is that data is not stored in a single place and tends to be distributed. The
    application will need to be ready to deal with cases where different services
    have different views of the state of the world.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据不一致*：与分布式应用程序一起工作的一个常见问题是数据不是存储在单一位置，而是倾向于分布。应用程序需要准备好处理不同服务对世界状态有不同的看法的情况。'
- en: '*Understanding how the application is working (monitoring, tracing, and telemetry)*:
    Having a clear understanding of how the application is performing and that it
    is doing what it is supposed to be doing is essential for quickly finding problems
    when things go wrong.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解应用程序的工作方式（监控、跟踪和遥测）*：在事情出错时，了解应用程序的表现以及它是否正在执行其应有的操作对于快速发现问题至关重要。'
- en: '*Application security and identity management*: Dealing with users and security
    is always an afterthought. For distributed applications, having these aspects
    clearly documented and implemented early on will help you to refine the application
    requirements by defining “who can do what and when.”'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*应用程序安全和身份管理*：处理用户和安全问题总是被放在次要位置。对于分布式应用程序，在早期就明确记录和实现这些方面将帮助您通过定义“谁可以在何时做什么”来细化应用程序需求。'
- en: 'Let’s start with the first of the challenges: Downtime is not allowed.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一个挑战开始：不允许停机。
- en: 2.4.1 Downtime is not allowed
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 不允许停机
- en: When using Kubernetes, we can easily scale up and down our services’ replicas.
    This is a great feature when your services were designed based on the assumption
    that the platform will scale them by creating new copies of the containers running
    the service. So, what happens when the service is not ready to handle replication
    or when no replicas are available for a given service?
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Kubernetes 时，我们可以轻松地扩展和缩减服务的副本。当您的服务是基于平台将通过创建运行服务的容器的新副本来扩展它们的假设而设计时，这是一个很棒的功能。那么，当服务没有准备好处理复制或没有给定服务的副本可用时会发生什么呢？
- en: 'Let’s scale up the Frontend service to have two replicas running. To achieve
    this, you can run the following command:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将前端服务扩展到有两个副本运行。为了实现这一点，您可以运行以下命令：
- en: '[PRE25]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: If one of the replicas stops running or breaks for any reason, Kubernetes will
    try to start another one to ensure that two replicas are up all the time. Figure
    2.17 shows two Frontend replicas serving traffic to the user.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果其中一个副本停止运行或因任何原因损坏，Kubernetes 将尝试启动另一个副本，以确保始终有两个副本处于运行状态。图 2.17 显示了两个前端副本正在为用户提供流量。
- en: '![](../../OEBPS/Images/02-17.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-17.png)'
- en: Figure 2.17 By having two replicas of the Frontend container running, we allow
    the application to tolerate failures and also to increase the number of concurrent
    requests that the application can handle.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.17 通过运行两个Frontend容器的副本，我们允许应用容忍故障，并增加应用可以处理的并发请求数量。
- en: You can quickly try this self-healing feature of Kubernetes by killing one of
    the two pods of the application Frontend. You can do this by running the following
    commands, as shown in listings 2.11 and 2.12.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过终止应用Frontend的两个Pod中的任意一个来快速尝试Kubernetes的这项自愈功能。您可以通过运行以下命令来完成此操作，如列表2.11和2.12所示。
- en: Listing 2.11 Checking that the two replicas are up and running
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.11 检查两个副本是否正在运行
- en: '[PRE26]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, copy one of the two Pods Id and delete it:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，复制两个Pod ID之一并删除它：
- en: '[PRE27]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Then list the pods again (listing 2.12).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 然后再次列出Pods（列表2.12）。
- en: Listing 2.12 A new replica is automatically created as soon as one goes down
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.12 一旦一个副本失败，就会自动创建一个新的副本
- en: '[PRE28]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You can see how Kubernetes (the ReplicaSet, more specifically) immediately creates
    a new pod when it detects only one running. While this new pod is being created
    and started, you have a single replica answering your requests until the second
    one is up and running. This mechanism ensures that at least two replicas answer
    your users’ requests. Figure 2.18 shows that the application still works, because
    we still have one pod serving requests.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，当Kubernetes（更具体地说，是ReplicaSet）检测到只有一个正在运行的副本时，它会立即创建一个新的Pod。在新的Pod被创建并启动的过程中，您只有一个副本在响应您的请求，直到第二个副本启动并运行。这种机制确保至少有两个副本来响应用户的请求。图2.18显示，由于我们仍然有一个Pod在处理请求，因此应用仍然可以工作。
- en: '![](../../OEBPS/Images/02-18.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-18.png)'
- en: Figure 2.18 If one of the instances fails, Kubernetes will automatically kill
    and recreate that instance. But at least the other running container can keep
    answering requests.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.18 如果其中一个实例失败，Kubernetes将自动终止并重新创建该实例。但至少其他正在运行的容器可以继续响应用户请求。
- en: 'If you have a single replica and kill the running pod, you will have downtime
    in your application until the new container is created and ready to serve requests.
    You can revert to a single replica with the following:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只有一个副本并终止了正在运行的Pod，您的应用将在新容器创建并准备好服务请求之前出现停机时间。您可以使用以下命令恢复到单个副本：
- en: '[PRE29]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Go ahead and try this out. Delete only the replica available for the Frontend
    pod:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 继续尝试。仅删除Frontend Pod可用的副本：
- en: '[PRE30]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Figure 2.19 shows the application is not working anymore, because there are
    no Frontend pods to serve incoming requests from users.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.19显示应用不再工作，因为没有Frontend Pod来服务来自用户的请求。
- en: '![](../../OEBPS/Images/02-19.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-19.png)'
- en: Figure 2.19 With a single replica being restarted, there is no backup to answer
    user requests. If there is no replica available to serve your users’ requests,
    you will experience downtime. This is exactly what we want to avoid.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.19 当单个副本重启时，没有备份来响应用户请求。如果没有副本可供服务用户的请求，您将经历停机时间。这正是我们想要避免的。
- en: After killing the pod, try to access the application by refreshing your browser
    (http://localhost). You should see “503 Service Temporarily Unavailable” in your
    browser, because the Ingress controller (not shown in the previous figure for
    simplicity) cannot find a replica running behind the Frontend service. If you
    wait for a bit, you will see the application come back up. Figure 2.20 shows the
    503 “Service Temporarily Unavailable” being returned by the NGINX Ingress controller
    component that was in charge of routing traffic to the Frontend service.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在终止Pod后，通过刷新浏览器（http://localhost）尝试访问应用。您应该在浏览器中看到“503服务暂时不可用”，因为Ingress控制器（为了简化，在之前的图中未显示）找不到位于Frontend服务后面的正在运行的副本。如果您稍等片刻，您将看到应用重新启动。图2.20显示了负责将流量路由到Frontend服务的NGINX
    Ingress控制器组件返回的503“服务暂时不可用”。
- en: '![](../../OEBPS/Images/02-20.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/02-20.png)'
- en: Figure 2.20 With a single replica being restarted, there is no backup to answer
    user requests
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.20 当单个副本重启时，没有备份来响应用户请求
- en: This error message is quite tricky, because the application takes about a second
    to get restarted and to be fully functional, so if you didn’t manage to see it,
    you can try to downscale the frontend service to zero replicas with `kubectl scale
    --replicas=0 deployments/conference-frontend-deployment` to simulate downtime.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这个错误信息相当棘手，因为应用大约需要一秒钟的时间才能重启并完全恢复正常功能，所以如果您没有看到它，您可以尝试使用`kubectl scale --replicas=0
    deployments/conference-frontend-deployment`将前端服务缩放到零个副本来模拟停机。
- en: This behavior is expected, because the Frontend service is a user-facing service.
    If it goes down, users will not be able to access any functionality, so having
    multiple replicas is recommended. From this perspective, the Frontend service
    is the most important service of the entire application, since our primary goal
    for our applications is to avoid downtime.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这种行为是可以预期的，因为前端服务是一个面向用户的服务。如果它关闭，用户将无法访问任何功能，因此建议有多个副本。从这个角度来看，前端服务是整个应用程序最重要的服务，因为我们的主要目标是避免应用程序的停机时间。
- en: In summary, pay special attention to user-facing services exposed outside of
    your cluster. Whether they are user interfaces or just APIs, ensure you have as
    many replicas as needed to deal with incoming requests. Having a single replica
    should be avoided for most use cases besides development.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，请特别注意集群外部暴露的面向用户的服务。无论是用户界面还是API，确保您有足够的副本来处理传入的请求。除了开发之外，大多数用例应避免使用单个副本。
- en: 2.4.2 Service’s resilience built-in
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 服务内置的弹性
- en: 'But now, what happens if the other services go down? For example, the Agenda
    service, is just in charge of listing all the accepted proposals to the conference
    attendees. This service is also critical, because the Agenda List is right there
    on the main page of the application. So, let’s scale the service down:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 但是现在，如果其他服务也关闭了怎么办？例如，Agenda服务仅负责向会议参与者列出所有接受的提案。这项服务同样关键，因为议程列表就在应用程序的主页上。所以，让我们缩小服务规模：
- en: '[PRE31]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Figure 2.21 shows how the application can keep working, even if one of the services
    is misbehaving.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.21显示了即使其中一个服务表现不佳，应用程序仍然可以继续工作。
- en: '![](../../OEBPS/Images/02-21.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-21.png)'
- en: Figure 2.21 No pods for the Agenda service. If a service is failing, the user
    should be able to keep using the application with limited functionality.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.21 没有Agenda服务的Pod。如果一个服务失败，用户应该能够继续使用应用程序，但功能有限。
- en: Right after running this command, the container will be killed, and the service
    will not have any container answering its requests. Try refreshing the application
    in your browser, you should see a cached response as shown in figure 2.22.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此命令后，容器将被终止，并且没有容器会响应用户的请求。尝试在浏览器中刷新应用程序，你应该会看到一个如图2.22所示的缓存响应。
- en: '![](../../OEBPS/Images/02-22.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-22.png)'
- en: Figure 2.22 If the Agenda service has no replica running, the Frontend is wise
    enough to show the user some cached entries.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.22 如果Agenda服务没有正在运行的副本，前端足够智能，会向用户显示一些缓存条目。
- en: As you can see, the application is still running, but the Agenda service is
    not available right now. Check the Debug tab in the Back Office section, which
    should show that the Agenda service is unhealthy (figure 2.23).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，应用程序仍在运行，但Agenda服务目前不可用。检查后台办公室部分的调试选项卡，它应该显示Agenda服务不健康（图2.23）。
- en: '![](../../OEBPS/Images/02-23.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-23.png)'
- en: Figure 2.23 If in Debug mode, the Back Office should show the unhealthy services.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.23 如果处于调试模式，后台办公室应显示不健康的服务。
- en: You can prepare your application for such scenarios; in this case, the Frontend
    has a cached response to at least show something to the user. If, for some reason,
    the Agenda service is down, at least the user will be able to access other services
    and other sections of the application. From the application perspective, it is
    important not to propagate the error back to the user. The user should be able
    to keep using other application services, for example, the Call for Proposals
    form, until the Agenda service is restored.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以为这样的场景准备您的应用程序；在这种情况下，前端有一个缓存响应，至少可以向用户显示一些内容。如果由于某种原因，Agenda服务关闭，至少用户将能够访问其他服务和应用程序的其他部分。从应用程序的角度来看，重要的是不要将错误传播回用户。用户应该能够继续使用其他应用程序服务，例如，提案征集表单，直到Agenda服务恢复。
- en: You need to pay special attention when developing services that will run in
    Kubernetes, as now your service is responsible for dealing with errors generated
    by downstream services. This is important to ensure that errors or services going
    down don’t bring your entire application down. Simple mechanisms such as cached
    responses will make your applications more resilient and allow you to incrementally
    upgrade these services without worrying about bringing everything down. For our
    conference scenario, having a CronJob that periodically caches the agenda entries
    might be enough. Remember, downtime is not allowed.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发将在Kubernetes中运行的服务时，你需要特别注意，因为现在你的服务负责处理由下游服务生成的错误。这很重要，以确保错误或服务崩溃不会使你的整个应用程序崩溃。简单的机制，如缓存响应，会使你的应用程序更加健壮，并允许你逐步升级这些服务，而不用担心将一切关闭。对于我们的会议场景，有一个定期缓存议程条目的CronJob可能就足够了。记住，不允许停机时间。
- en: Let’s now switch to talking about dealing with the state in our applications
    and how it is critical to understand how our application’s services handle the
    state from a scalability point of view. Since we will be talking about scalability,
    data consistency is the challenge we will try to solve next.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈如何处理我们应用程序中的状态，以及从可扩展性的角度来看理解我们的应用程序服务如何处理状态的重要性。由于我们将讨论可扩展性，数据一致性是我们接下来要尝试解决的问题。
- en: 2.4.3 Dealing with the application state is not trivial
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.3 处理应用程序状态并不简单
- en: 'Let’s scale up the agenda service again to have a single replica:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次扩展Agenda服务，使其只有一个副本：
- en: '[PRE32]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: If you have created proposals before, you will notice that as soon as the Agenda
    service goes back up, you see the accepted proposals again on the Agenda page.
    This works only because both the Agenda service and C4P Service store all the
    proposals and agenda items in external databases (PostgreSQL and Redis). In this
    context, external means outside of the pod memory. What will happen if we scale
    the Agenda service up to two replicas? See listing 2.13.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前创建过提案，你会注意到，一旦Agenda服务恢复，你会在Agenda页面上再次看到已接受的提案。这仅因为Agenda服务和C4P服务都将所有提案和议程项存储在外部数据库中（PostgreSQL和Redis）。在这个上下文中，外部意味着在pod内存之外。如果我们将Agenda服务扩展到两个副本，会发生什么？请参见列表2.13。
- en: Listing 2.13 Running with two replicas of the Agenda service
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.13 运行Agenda服务的两个副本
- en: '[PRE33]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Figure 2.24 shows the Agenda service running two replicas of the service concurrently.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.24展示了Agenda服务同时运行两个服务副本的情况。
- en: '![](../../OEBPS/Images/02-24.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-24.png)'
- en: Figure 2.24 Two replicas can now deal with more traffic. The requests being
    forwarded by the Frontend can be answered by the two available replicas, allowing
    the application to handle more load.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.24现在两个副本可以处理更多的流量。由前端转发的请求可以被两个可用的副本回答，这使得应用程序能够处理更多的负载。
- en: With two replicas dealing with your user requests, now the Frontend will have
    two instances to query. Kubernetes will do the load balancing between the two
    replicas, but your application will have no control over which replica the request
    hits. Because we are using a database to back up the data outside of the pod’s
    context, we can scale the replicas to many pods dealing with the application demand.
    Figure 2.25 shows how the Agenda service relies on Redis to store the application
    state, while the Call for Proposals uses PostgreSQL to do the same.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个副本处理你的用户请求时，现在前端将有两个实例进行查询。Kubernetes将在两个副本之间进行负载均衡，但你的应用程序将无法控制请求击中的副本。因为我们使用数据库在pod的上下文之外备份数据，所以我们可以将副本扩展到许多处理应用程序需求的pod。图2.25显示了Agenda服务如何依赖Redis来存储应用程序状态，而Call
    for Proposals则使用PostgreSQL来完成同样的工作。
- en: '![](../../OEBPS/Images/02-25.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-25.png)'
- en: Figure 2.25 Both data-sensitive services use persistent stores. Delegating state
    storage to external components, make your service stateless and easier to scale.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.25 两个数据敏感的服务都使用持久存储。将状态存储委托给外部组件，使你的服务无状态且更容易扩展。
- en: One of the limitations of this approach is the number of database connections
    that your database supports in its default configuration. If you keep scaling
    up the replicas, always consider reviewing the database connection pool settings
    to ensure that your database can handle all the connections created by all the
    replicas. But for the sake of learning, let’s imagine that we don’t have a database,
    and our Agenda service keeps all the agenda items in memory. How would the application
    behave if we started scaling up the Agenda service pods? Figure 2.26 shows the
    hypothetical case of having in-memory data inside our applications.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个局限性是数据库在其默认配置中支持的数据库连接数。如果你继续扩展副本，始终需要考虑审查数据库连接池设置，以确保数据库可以处理所有副本创建的所有连接。但为了学习，让我们假设我们没有数据库，我们的议程服务将所有议程项都保存在内存中。如果我们开始扩展议程服务
    Pod，应用程序将如何表现？图 2.26 展示了应用程序内部有内存数据的假设情况。
- en: '![](../../OEBPS/Images/02-26.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-26.png)'
- en: Figure 2.26 What would happen if the Agenda service keeps the state in-memory?
    If state is kept in memory it is quite hard to share across replicas. This makes
    scaling the service much harder.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.26 如果议程服务将状态保存在内存中会发生什么？如果状态保存在内存中，就很难在副本之间共享。这使得扩展服务变得更加困难。
- en: By scaling these services up, we have found a problem with the design of one
    of the application services. The Agenda service is keeping the state in-memory,
    and that will affect the scaling capabilities from Kubernetes. For this kind of
    scenario, when Kubernetes balances the requests across different replicas, the
    Frontend service will receive different data depending on which replica processed
    the request.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 通过扩大这些服务规模，我们发现应用程序服务设计中存在一个问题。议程服务正在内存中保持状态，这将影响 Kubernetes 的扩展能力。对于这种场景，当
    Kubernetes 在不同的副本之间平衡请求时，前端服务将根据哪个副本处理了请求而接收不同的数据。
- en: When running existing applications in Kubernetes, you will need to deeply understand
    how much data they are keeping in-memory because this will affect how you can
    scale them up. For web applications that keep HTTP sessions and require sticky
    sessions (subsequent requests going to the same replica), you need to set up HTTP
    session replication to get this working with multiple replicas. This might require
    more components being configured at the infrastructure level, such as a cache.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 当在 Kubernetes 中运行现有应用程序时，你需要深入了解它们在内存中保持多少数据，因为这将影响你如何扩展它们。对于保持 HTTP 会话并需要粘性会话（后续请求都发送到同一副本）的
    Web 应用程序，你需要设置 HTTP 会话复制以使多个副本工作。这可能需要在基础设施级别配置更多组件，例如缓存。
- en: Understanding your service requirements will help you plan and automate your
    infrastructural requirements, such as databases, caches, message brokers, etc.
    The more complex the application gets, the more dependencies on these infrastructural
    components it will have.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 了解你的服务需求将帮助你规划和自动化你的基础设施需求，例如数据库、缓存、消息代理等。应用程序越复杂，它对这些基础设施组件的依赖性就越大。
- en: As we have seen before, we have installed Redis and PostgreSQL as part of the
    application Helm Chart. This is usually not a good idea because databases and
    tools like message brokers will need special care from the operation team, who
    can choose not to run these services inside Kubernetes. We will expand on this
    topic in chapter 4 where we go deeper into how to deal with infrastructure when
    working with Kubernetes and cloud providers.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所见，我们已经将 Redis 和 PostgreSQL 作为应用程序 Helm 图的一部分安装。这通常不是一个好主意，因为数据库和像消息代理这样的工具需要运营团队的特别关注，他们可以选择不在
    Kubernetes 内运行这些服务。我们将在第 4 章中进一步探讨这个主题，我们将更深入地探讨在 Kubernetes 和云提供商合作时如何处理基础设施。
- en: 2.4.4 Dealing with inconsistent data
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.4 处理不一致的数据
- en: Having stored data in a relational data store like PostgreSQL or a NoSQL approach
    like Redis doesn’t solve the problem of having inconsistent data across different
    stores. Because these stores should be hidden away by the service API, you will
    need to have mechanisms to check that the data that the services are handling
    is consistent. In distributed systems, it is quite common to talk about “eventual
    consistency,” meaning that eventually the system will be consistent. Having eventual
    consistency is better than not having consistency at all. For this example, we
    can build a simple check mechanism that once in a while (imagine once a day) checks
    for the accepted talks in the Agenda service to see if they have been approved
    in the Call for Proposals service. If there is an entry that the Call hasn’t approved
    for the Proposal Service (C4P), then we can raise some alerts or send an email
    to the conference organizers (figure 2.27).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在关系型数据库存储如PostgreSQL或NoSQL方法如Redis中存储数据并不能解决不同存储之间数据不一致的问题。因为这些存储应该通过服务API隐藏起来，你需要有机制来检查服务处理的数据是否一致。在分布式系统中，经常谈论“最终一致性”，意味着最终系统将会是一致的。拥有最终一致性比完全没有一致性要好。在这个例子中，我们可以构建一个简单的检查机制，偶尔（想象一下每天一次）检查议程服务中已接受的演讲是否在提案征集服务中得到批准。如果有一个提案服务（C4P）尚未批准的条目，那么我们可以发出一些警报或给会议组织者发送电子邮件（图2.27）。
- en: '![](../../OEBPS/Images/02-27.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![图2.27](../../OEBPS/Images/02-27.png)'
- en: 'Figure 2.27 Consistency checks can run as CronJobs. We can execute checks against
    the application services on fixed intervals to make sure that the state is consistent.
    For example: (1) every day at midnight we query the Agenda Service (2) to verify
    that the published sessions are approved in the (3) Call For Proposals Service
    and a corresponding notification has been sent by the (4) Notifications Service.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.27 一致性检查可以作为CronJobs运行。我们可以定期执行针对应用程序服务的检查，以确保状态的一致性。例如：(1)每天午夜我们查询议程服务(2)以验证发布的会议是否在(3)提案征集服务中得到批准，并且(4)通知服务已经发送了相应的通知。
- en: In figure 2.27, we can see how a CronJob (1) will be executed every X period,
    depending on how important it is for us to fix consistency problems. Then it will
    query the Agenda service public APIs (2) to check which accepted proposals are
    being listed and compare that with the Call for Proposals service approved list
    (3). Finally, if any inconsistency is found, an email can be sent using the Notifications
    service public APIs (4).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在图2.27中，我们可以看到CronJob（1）将每隔X个周期执行一次，这取决于我们修复一致性问题的紧迫性。然后它将查询议程服务的公共API（2）以检查哪些已接受的提案被列出，并将其与提案征集服务批准的列表（3）进行比较。最后，如果发现任何不一致性，可以使用通知服务的公共API（4）发送电子邮件。
- en: Think of the simple use case this application was designed for; what other checks
    would you need? One that immediately comes to mind is verifying that emails were
    sent correctly for Rejected and Approved proposals. For this use case, emails
    are really important, and we need to ensure those emails are sent to our accepted
    and rejected speakers.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个应用程序被设计用于的简单用例；你还需要进行哪些检查？一个立即想到的是验证已拒绝和批准的提案是否正确发送了电子邮件。对于这个用例，电子邮件非常重要，我们需要确保这些电子邮件被发送到我们接受的和拒绝的演讲者。
- en: 2.4.5 Understanding how the application is working
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.5 理解应用程序的工作方式
- en: Distributed systems are complex beasts, and fully understanding how they work
    from day one can help you save time when things go wrong. This has pushed the
    monitoring, tracing, and telemetry communities hard to develop solutions that
    help us understand how things are working at any given time.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式系统是复杂的生物，从第一天开始就完全理解它们是如何工作的，这有助于你在事情出错时节省时间。这促使监控、跟踪和遥测社区努力开发出帮助我们理解在任何给定时间事物是如何运作的解决方案。
- en: The [https://opentelemetry.io/](https://opentelemetry.io/) OpenTelemetry community
    has evolved alongside Kubernetes, and it can now provide most of the tools you
    will need to monitor how your services are working. As stated on their website,
    “You can use it to instrument, generate, collect, and export telemetry data (metrics,
    logs, and traces) for analysis to understand your software’s performance and behavior.”
    Figure 2.28 shows a common use case where services all push metrics, traces, and
    logs to a centralized place that stores and aggregates the information so it can
    be displayed in dashboards or used by other tools.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://opentelemetry.io/](https://opentelemetry.io/) OpenTelemetry 社区与 Kubernetes
    一起发展，现在它可以提供您监控服务运行情况所需的大部分工具。正如他们的网站所述，“您可以使用它来对软件进行仪器化、生成、收集和导出遥测数据（指标、日志和跟踪）以进行分析，以了解软件的性能和行为。”
    图 2.28 展示了一个常见用例，其中所有服务都将指标、跟踪和日志推送到一个集中位置，该位置存储和聚合信息，以便在仪表板中显示或由其他工具使用。'
- en: '![](../../OEBPS/Images/02-28.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-28.png)'
- en: Figure 2.28 Aggregating observability from all our services in a single place
    reduces the cognitive load on the teams responsible for keeping the application
    up and running.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.28 将所有服务的可观察性聚合到单一位置可以减少负责保持应用程序正常运行团队的认知负荷。
- en: It is important to notice that OpenTelemetry focuses on both the behavior and
    performance of your software, because they will both affect your users and user
    experience. From the behavior point of view, you want to make sure that the application
    is doing what it is supposed to do, and by that, you will need to understand which
    services are calling which other services or infrastructure to perform tasks.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，OpenTelemetry 专注于您软件的行为和性能，因为它们都会影响您的用户和用户体验。从行为角度来看，您想确保应用程序正在执行它应该执行的操作，为此，您需要了解哪些服务正在调用哪些其他服务或基础设施来执行任务。
- en: Using Prometheus and Grafana allows us to see the service telemetry and build
    domain-specific dashboards to highlight certain application-level metrics, for
    example, the amount of Approved vs. Rejected proposals over time, as shown in
    figure 2.29.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Prometheus 和 Grafana 可以让我们查看服务遥测数据并构建特定领域的仪表板来突出某些应用级指标，例如，随着时间的推移，批准与拒绝提案的数量对比，如图
    2.29 所示。
- en: '![](../../OEBPS/Images/02-29.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-29.png)'
- en: Figure 2.29 Monitoring telemetry data with Prometheus and Grafana
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.29 使用 Prometheus 和 Grafana 监控遥测数据
- en: From the performance point of view, you need to ensure that services are respecting
    their Service Level Agreements (SLAs), which means that they are taking only a
    short time to answer requests. If one of your services misbehaves and takes more
    than usual, you want to know.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 从性能角度来看，您需要确保服务遵守其服务级别协议（SLA），这意味着它们以很短的时间回答请求。如果您的某个服务表现不佳，耗时超过正常水平，您希望知道。
- en: For tracing, you must modify your services to understand the internal operations
    and their performance. OpenTelemetry provides drop-in instrumentation libraries
    in most languages to externalize service metrics and traces. Figure 2.30 shows
    the OpenTelemetry architecture, where you can see the OpenTelemetry collector
    receiving information from each application agent, but also from shared infrastructure
    components.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 对于跟踪，您必须修改您的服务以了解其内部操作和性能。OpenTelemetry 在大多数语言中提供即插即用的仪器库，以外部化服务指标和跟踪。图 2.30
    展示了 OpenTelemetry 架构，您可以看到 OpenTelemetry 收集器从每个应用程序代理接收信息，同时也从共享基础设施组件接收信息。
- en: '![](../../OEBPS/Images/02-30.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/02-30.png)'
- en: 'Figure 2.30 OpenTelemetry architecture and library (Source: https://opentelemetry.io/docs/)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.30 OpenTelemetry 架构和库（来源：https://opentelemetry.io/docs/）
- en: The recommendation here is if you are creating a walking skeleton, ensure it
    has OpenTelemetry built-in. If you push monitoring to later stages of the project,
    it will be too late, things will go wrong, and finding out who is responsible
    will take too much time.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的建议是，如果您正在创建一个“行走骨架”，请确保它内置了 OpenTelemetry。如果您将监控推迟到项目的后期阶段，那就太晚了，事情会出错，找出责任人会花费太多时间。
- en: 2.4.6 Application security and identity management
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.6 应用程序安全和身份管理
- en: If you have ever built a web application, you know that providing identity management
    (user accounts and user identity) plus authentication and authorization is quite
    an endeavor. A simple way to break any application (cloud-native or not) is to
    perform actions you are not supposed to do, such as deleting all the proposed
    presentations unless you are a conference organizer.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经构建过Web应用程序，你就会知道提供身份管理（用户账户和用户身份）以及认证和授权是一项相当艰巨的任务。破坏任何应用程序（无论是云原生还是非云原生）的一个简单方法就是执行你不应该执行的操作，比如删除所有提议的演示文稿，除非你是会议组织者。
- en: This also becomes challenging in distributed systems, because authorization
    and user identity must be propagated across different services. In distributed
    architectures, it is quite common to have a component that generates requests
    on behalf of a user instead of exposing all the services for the user to interact
    directly. In our example, the Frontend service is this component. Most of the
    time, you can use this external-facing component as the barrier between external
    and internal services. For this reason, it is quite common to configure the Frontend
    service to connect with an authorization and authentication provider commonly
    using the OAuth2 protocol. Figure 2.31 shows the Frontend service interacting
    with an identity management service, which is responsible for connecting to an
    Identity Provider (Google, GitHub, your internal LDAP server) to validate the
    user credentials as well as to provide roles or group memberships that define
    what the user can and can’t do in different services. The Frontend service handles
    the login flow (authentication and authorization), but only the context is propagated
    to the backend services once that is done.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这在分布式系统中也变得具有挑战性，因为授权和用户身份必须在不同的服务之间传播。在分布式架构中，有一个代表用户生成请求的组件而不是直接暴露所有服务供用户交互是很常见的。在我们的例子中，前端服务就是这样的组件。大多数时候，你可以使用这个面向外部的组件作为外部和内部服务之间的屏障。因此，将前端服务配置为连接到使用OAuth2协议的授权和认证提供者是相当常见的。图2.31显示了前端服务与身份管理服务的交互，该服务负责连接到身份提供者（Google、GitHub、你内部的LDAP服务器）以验证用户凭据，并提供定义用户在不同服务中可以做什么和不能做什么的角色或组成员资格。前端服务处理登录流程（认证和授权），但一旦完成，只有上下文才会传播到后端服务。
- en: '![](../../OEBPS/Images/02-31.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![图2.31](../../OEBPS/Images/02-31.png)'
- en: 'Figure 2.31 Identity management: The Role/Group is propagated to the backend
    services.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.31 身份管理：角色/组被传播到后端服务。
- en: On the identity management front, you have seen that the application doesn’t
    handle users or their data, which is good for regulations such as GDPR. We might
    want to allow users to use their social media accounts to log in to our applications
    without the need for them to create separate accounts. This is usually known as
    social login.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在身份管理方面，你已经看到应用程序不处理用户或他们的数据，这对于GDPR等法规来说是个好事。我们可能希望允许用户使用他们的社交媒体账户登录到我们的应用程序，而无需他们创建单独的账户。这通常被称为社交登录。
- en: Some popular solutions bring both OAuth2 and identity management together, such
    as Keycloak ([https://www.keycloak.org/](https://www.keycloak.org/)) and Zitadel
    ([https://zitadel.com/opensource](https://zitadel.com/opensource)). These open-source
    projects provide a one-stop-shop for single sign-on solutions and advanced identity
    management. In the case of Zitadel, it also provides a managed service that you
    can use if you don’t want to install and maintain an SSO and identity management
    component inside your infrastructure.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 一些流行的解决方案将OAuth2和身份管理结合在一起，例如Keycloak ([https://www.keycloak.org/](https://www.keycloak.org/))
    和Zitadel ([https://zitadel.com/opensource](https://zitadel.com/opensource))。这些开源项目为单点登录解决方案和高级身份管理提供了一站式服务。在Zitadel的情况下，它还提供了一种托管服务，如果你不想在你的基础设施内安装和维护SSO和身份管理组件，你可以使用这项服务。
- en: The same is true with tracing and monitoring. If you are planning to have users
    (and you will probably do, sooner or later), including single sign-on and identity
    management into the walking skeleton will push you to think about the specifics
    of “who will be able to do what,” refining your use case even more.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 追踪和监控也是如此。如果你计划拥有用户（你迟早会这么做），包括单点登录和身份管理到行走骨架中会促使你思考“谁将能够做什么”的具体细节，进一步细化你的用例。
- en: 2.4.7 Other challenges
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.7 其他挑战
- en: In the previous sections, we have covered a few common challenges you will face
    while building cloud-native applications, but these are not all. Can you think
    of other ways of breaking this first version of the application?
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几节中，我们已经介绍了一些你在构建云原生应用时可能会遇到的一些常见挑战，但这些并非全部。你能想到其他破坏这个应用第一版的方法吗？
- en: Notice that tackling the challenges discussed in this chapter will help, but
    there are other challenges related to how we deliver a continuously evolving application
    composed of a growing number of services.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，解决本章讨论的挑战会有所帮助，但还有其他与如何交付由不断增长的服务组成的持续演变的应用相关的挑战。
- en: 2.5 Linking back to platform engineering
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 回顾平台工程
- en: In previous sections, we have covered many topics. We reviewed options for packaging
    and distributing Kubernetes applications, and then installing our walking skeleton
    in a Kubernetes cluster using Helm. We tested the application functionality by
    interacting with it, and finally, we jumped into analyzing common cloud-native
    challenges that teams will face when building distributed applications.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几节中，我们讨论了许多主题。我们回顾了打包和分发 Kubernetes 应用程序的选择，然后使用 Helm 在 Kubernetes 集群中安装我们的“行走骨架”。我们通过与它交互来测试应用程序的功能，最后，我们深入分析了团队在构建分布式应用程序时可能会遇到的一些常见的云原生挑战。
- en: But you might be wondering how all these topics relate to the title of this
    book, continuous delivery, and platform engineering in general. In this section,
    we will make more explicit connections to the topics introduced in chapter 1.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 但你可能想知道所有这些话题如何与本书的标题“持续交付”以及平台工程的一般概念相关。在本节中，我们将更明确地将第 1 章中介绍的主题与这些话题联系起来。
- en: First, the intention behind creating a Kubernetes cluster and running an application
    on top of it was to ensure we cover Kubernetes built-in mechanisms for resilience
    and scaling up our application services. Kubernetes provides the building blocks
    to run our applications with zero downtime, even when we are constantly updating
    them. This allows us, Kubernetes users, to release new versions of our components
    more frequently, because we are not supposed to stop the entire application from
    updating one of its parts. In chapter 8 we will see how Kubernetes’ built-in mechanisms
    can be extended to implement different release strategies.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建 Kubernetes 集群并在其上运行应用程序的目的是确保我们涵盖 Kubernetes 内置的弹性和扩展应用程序服务的能力。Kubernetes
    提供了构建块，使我们能够在不中断服务的情况下运行我们的应用程序，即使我们不断更新它们。这使得我们，Kubernetes 用户，可以更频繁地发布我们组件的新版本，因为我们不应该停止整个应用程序来更新其某个部分。在第
    8 章中，我们将看到 Kubernetes 内置机制如何扩展以实现不同的发布策略。
- en: If you are not using the capabilities offered by Kubernetes to keep releasing
    software in front of your customers, then you need to raise a red flag. Quite
    often, this can be due to old practices from before Kubernetes that are getting
    in the way, lack of automation, or not having clearly defined contracts between
    services that block dependent services from being released independently. We will
    touch on this topic several times in future chapters because this is a fundamental
    principle when trying to improve your continuous delivery practice and something
    that the platform engineering team needs to prioritize.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有使用 Kubernetes 提供的能力来持续向客户发布软件，那么你需要拉响警报。这种情况通常是由于 Kubernetes 之前的老旧做法阻碍了进程，缺乏自动化，或者服务之间没有明确定义的合同，这阻碍了依赖服务独立发布。我们将在未来的几章中多次涉及这个话题，因为这是尝试改进你的持续交付实践的基本原则，也是平台工程团队需要优先考虑的事项。
- en: In this chapter, we have also seen how to install a cloud-native application
    using a package manager that encapsulates the configuration files required to
    deploy our application. These configuration files (Kubernetes resources expressed
    as YAML files) describe our application topology and contain links to the containers
    used by each application’s service. These YAML files also contain configuration
    for each service, such as the environment variables to configure each service.
    Packaging and versioning these configuration files allows us to easily create
    new application instances in different environments, which we will cover in chapter
    4.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们也看到了如何使用封装了部署我们应用程序所需配置文件的包管理器来安装一个云原生应用程序。这些配置文件（以 YAML 文件形式表达 Kubernetes
    资源）描述了我们的应用程序拓扑，并包含每个应用程序服务使用的容器的链接。这些 YAML 文件还包含每个服务的配置，例如配置每个服务的环境变量。对这些配置文件进行打包和版本控制，使我们能够轻松地在不同的环境中创建新的应用程序实例，这将在第
    4 章中介绍。
- en: I highly recommend the book *Grokking Continuous Delivery* by Christie Wilson
    (Manning Publications, 2018) if you want to get more insights into the continuous
    delivery aspects of how configuration as code can help you deliver more software
    reliably.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要深入了解配置作为代码如何帮助你更可靠地交付更多软件的持续交付方面，我强烈推荐 Christie Wilson 的书籍《Grokking Continuous
    Delivery》（Manning Publications，2018年出版）。
- en: Because I wanted to make sure that you have an application to play around with
    and because we needed to cover Kubernetes built-in mechanisms, I’ve made a conscious
    decision to start with an already packaged application that can be easily deployed
    into any Kubernetes cluster (no matter if it is running locally or in a cloud
    provider). We can identify two different phases. One we haven’t covered yet is
    how to produce these packages that can be deployed to any Kubernetes cluster,
    and the second, which we started playing with, is when we run this application
    in a concrete cluster (we can consider this cluster an environment, maybe a development
    environment), as shown in figure 2.32.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我想确保你有一个可以玩的应用程序，并且我们需要涵盖 Kubernetes 内置机制，所以我做出了一个有意识的决策，从可以轻松部署到任何 Kubernetes
    集群（无论它是本地运行还是在云提供商上）的已打包应用程序开始。我们可以识别出两个不同的阶段。一个是我们还没有覆盖的，那就是如何生产这些可以部署到任何 Kubernetes
    集群的包，第二个阶段，我们已经开始尝试，就是在我们具体的集群中运行这个应用程序（我们可以将这个集群视为一个环境，也许是一个开发环境），如图 2.32 所示。
- en: '![](../../OEBPS/Images/02-32.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.32 应用程序的生命周期，从构建和打包到在环境中运行](../../OEBPS/Images/02-32.png)'
- en: Figure 2.32 Applications’ lifecycle from building and packaging to running inside
    an environment
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.32 应用程序的生命周期，从构建和打包到在环境中运行
- en: It is important to understand that the steps executed for our local environment
    will work for any Kubernetes cluster, no matter the cluster size and location.
    While each cloud provider will have its own security and identity mechanisms,
    the Kubernetes APIs and resources we created when we installed our application
    Helm Chart to the cluster will be the same. If you now use Helm templating capabilities
    to fine-tune your application (for example, resource consumptions and network
    configurations) for the target environment, you can easily automate these deployments
    to any Kubernetes cluster.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这一点很重要，即为我们本地环境执行的步骤将适用于任何 Kubernetes 集群，无论集群大小和位置如何。虽然每个云提供商都将有自己的安全和身份机制，但我们安装应用程序
    Helm 图表到集群时创建的 Kubernetes API 和资源将是相同的。如果你现在使用 Helm 模板功能来微调你的应用程序（例如，资源消耗和网络配置）以适应目标环境，你可以轻松地将这些部署自动化到任何
    Kubernetes 集群。
- en: Before moving on, let’s be clear that pushing developers to configure application
    instances might not be the best use of their time. A developer accessing the production
    environment that users/customers are accessing might also not be optimal. We want
    to ensure that developers are focused on building new features and improving our
    application. Figure 2.33 shows how we should be automating all the steps involved
    in building, publishing, and deploying the artifacts that developers are creating,
    making sure that they can focus on adding features to the application instead
    of manually dealing with packaging, distributing, and deploying new versions when
    they are ready. This is the primary focus of this chapter.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们明确一点，推动开发者配置应用程序实例可能不是他们时间的最佳利用方式。访问生产环境（用户/客户正在访问的环境）的开发者可能也不是最佳选择。我们希望确保开发者专注于构建新功能和改进我们的应用程序。图
    2.33 展示了我们应该自动化所有涉及构建、发布和部署开发者创建的工件步骤，确保他们可以专注于向应用程序添加功能，而不是在准备就绪时手动处理打包、分发和部署新版本。这是本章的主要关注点。
- en: '![](../../OEBPS/Images/02-33.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.33](../../OEBPS/Images/02-33.png)'
- en: Figure 2.33 Developers can focus on building features, but the platform team
    needs to automate the entire process after changes are made.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.33 开发者可以专注于构建功能，但平台团队需要在更改后自动化整个流程。
- en: Understanding the tools we can use to automate the path from source code changes
    to running software in a Kubernetes cluster is fundamental to enabling developers
    to focus on what they do best “code new features.” Another big difference we will
    tackle is that cloud-native applications are not static. As you can see in the
    previous diagram, we will not install a static application definition. We want
    to release and deploy new versions of the services as they become available.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '了解我们可以使用的工具来自动化从源代码更改到在 Kubernetes 集群中运行软件的路径，这对于使开发者能够专注于他们最擅长的事情“编写新功能”是至关重要的。我们将解决的另一个重大差异是，云原生应用程序不是静态的。正如您在前面的图中可以看到的，我们不会安装静态的应用程序定义。我们希望随着服务的可用性，发布和部署新版本的服务。 '
- en: Manually installing applications is error-prone; manually changing configurations
    in our Kubernetes clusters can make us end up in situations where we don’t know
    how to replicate the current state of our application in a different environment.
    Hence in chapters 3 and 4, we will talk about automation using what is commonly
    known as pipelines.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 手动安装应用程序容易出错；手动更改我们的 Kubernetes 集群中的配置可能会使我们陷入不知道如何在不同的环境中复制应用程序当前状态的情况。因此，在第
    3 章和第 4 章中，我们将讨论使用通常称为管道的自动化。
- en: In the next chapter, we will cover a more dynamic aspect of our distributed
    application with pipelines to deliver new versions of our services. Chapter 4
    will explore how we can manage our environments using Kubernetes-based GitOps
    tools.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过管道来覆盖我们分布式应用的更多动态方面，以交付我们服务的新版本。第 4 章将探讨我们如何使用基于 Kubernetes 的 GitOps
    工具来管理我们的环境。
- en: Summary
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Choosing between local and remote Kubernetes clusters requires serious considerations:'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地和远程 Kubernetes 集群之间进行选择需要认真考虑：
- en: You can use Kubernetes KinD to bootstrap a local Kubernetes cluster to develop
    your application. The main drawback is that your cluster is limited by your local
    resources (CPU and memory) and is not a real cluster of machines.
  id: totrans-331
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用 Kubernetes KinD 引导本地 Kubernetes 集群以开发您的应用程序。主要缺点是您的集群受限于您的本地资源（CPU 和内存），并且不是一个真正的机器集群。
- en: You can have an account in a cloud provider and do all development against a
    remote cluster. The main drawback of this approach is that most developers are
    not used to working remotely all the time and that someone needs to pay for the
    remote resources.
  id: totrans-332
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在云提供商中拥有一个账户，并针对远程集群进行所有开发。这种方法的主要缺点是，大多数开发者不习惯于一直远程工作，并且需要有人为远程资源付费。
- en: Package managers, like Helm, help you to package, distribute, and install your
    Kubernetes applications. In this chapter, you installed an application into a
    Kubernetes cluster with a single command line.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包管理器，如 Helm，可以帮助您打包、分发和安装您的 Kubernetes 应用程序。在本章中，您只需一条命令行就能将应用程序安装到 Kubernetes
    集群中。
- en: Understanding which Kubernetes resources are created by your application gives
    you an idea about how the application will behave when things go wrong and what
    extra considerations are needed in real-life scenarios.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解您的应用程序创建的 Kubernetes 资源可以给您一个关于当事情出错时应用程序将如何表现以及在实际场景中需要考虑哪些额外因素的印象。
- en: Even with very simple applications, you will face challenges that you will have
    to tackle one at a time. Knowing these challenges ahead of time helps you to plan
    and architect your services with the right mindset.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使是非常简单的应用，你也会遇到必须逐一解决的挑战。提前了解这些挑战有助于你以正确的思维方式规划和设计你的服务。
- en: 'Having a walking skeleton helps you to try different scenarios and technologies
    in a controlled environment. In this chapter, you have experimented with:'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有一个“行走骨架”可以帮助你在受控环境中尝试不同的场景和技术。在本章中，你已经尝试了：
- en: Scaling up and down your services to see first-hand how the application behaves
    when things go wrong.
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过扩展和缩减你的服务来亲眼看到当事情出错时应用程序的表现。
- en: Keeping state is hard, and we will need dedicated components to do this efficiently.
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维护状态很困难，我们需要专门的组件来高效地完成这项工作。
- en: Having at least two replicas for our services minimizes downtime. Making sure
    that the user-facing components are always up and running guarantees that even
    when things go wrong, the user will be able to interact with parts of the application.
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少为我们的服务保留两个副本可以最小化停机时间。确保用户界面组件始终处于运行状态，可以保证即使在出现问题的情况下，用户也能与应用程序的部分进行交互。
- en: Having fallbacks and built-in mechanisms to deal with problems when they arise
    makes your application more resilient.
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当问题出现时，拥有回退和内置机制来处理问题可以使你的应用更具弹性。
- en: If you have followed the linked step-by-step tutorial, you now have hands-on
    experience creating a local Kubernetes cluster, installing an application, scaling
    up and down services, and, most importantly, checking that the application is
    running as expected.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你已经遵循了链接的逐步教程，你现在已经有了一手经验创建本地Kubernetes集群，安装应用程序，扩展和缩减服务，最重要的是，检查应用程序是否按预期运行。
