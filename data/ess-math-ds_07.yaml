- en: Chapter 7\. Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 神经网络
- en: A regression and classification technique that has enjoyed a renaissance over
    the past 10 years is neural networks. In the simplest definition, a *neural network*
    is a multilayered regression containing layers of weights, biases, and nonlinear
    functions that reside between input variables and output variables. *Deep learning*
    is a popular variant of neural networks that utilizes multiple “hidden” (or middle)
    layers of nodes containing weights and biases. Each node resembles a linear function
    before being passed to a nonlinear function (called an activation function). Just
    like linear regression, which we learned about in [Chapter 5](ch05.xhtml#ch05),
    optimization techniques like stochastic gradient descent are used to find the
    optimal weight and bias values to minimize the residuals.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的10年里，一种经历复兴的回归和分类技术是神经网络。在最简单的定义中，*神经网络*是一个包含权重、偏差和非线性函数层的多层回归，位于输入变量和输出变量之间。*深度学习*是神经网络的一种流行变体，利用包含权重和偏差的多个“隐藏”（或中间）层节点。每个节点在传递给非线性函数（称为激活函数）之前类似于一个线性函数。就像我们在[第5章](ch05.xhtml#ch05)中学到的线性回归一样，优化技术如随机梯度下降被用来找到最优的权重和偏差值以最小化残差。
- en: Neural networks offer exciting solutions to problems previously difficult for
    computers to solve. From identifying objects in images to processing words in
    audio, neural networks have created tools that affect our everyday lives. This
    includes virtual assistants and search engines, as well as photo tools in our
    iPhones.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络为计算机以前难以解决的问题提供了令人兴奋的解决方案。从识别图像中的物体到处理音频中的单词，神经网络已经创造了影响我们日常生活的工具。这包括虚拟助手和搜索引擎，以及我们iPhone中的照片工具。
- en: Given the media hoopla and bold claims dominating news headlines about neural
    networks, it may be surprising that they have been around since the 1950s. The
    reason for their sudden popularity after 2010 is due to the growing availability
    of data and computing power. The ImageNet challenge between 2011 and 2015 was
    probably the largest driver of the renaissance, boosting performance on classifying
    one thousand categories on 1.4 million images to an accuracy of 96.4%.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于媒体的炒作和大胆宣称主导着关于神经网络的新闻头条，也许令人惊讶的是，它们自上世纪50年代以来就存在了。它们在2010年后突然变得流行的原因是由于数据和计算能力的不断增长。2011年至2015年之间的ImageNet挑战赛可能是复兴的最大推动力，将对140万张图像进行一千个类别的分类准确率提高到了96.4%。
- en: However, like any machine learning technique it only works on narrowly defined
    problems. Even projects to create “self-driving” cars do not use end-to-end deep
    learning, and primarily use hand-coded rule systems with convoluted neural networks
    acting as a “label maker” for identifying objects on the road. We will discuss
    this later in this chapter to understand where neural networks are actually used.
    But first we will build a simple neural network in NumPy, and then use scikit-learn
    as a library implementation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，就像任何机器学习技术一样，它只适用于狭义定义的问题。即使是创建“自动驾驶”汽车的项目也不使用端到端的深度学习，主要使用手工编码的规则系统，卷积神经网络充当“标签制造机”来识别道路上的物体。我们将在本章后面讨论这一点，以了解神经网络实际上是如何使用的。但首先我们将在NumPy中构建一个简单的神经网络，然后使用scikit-learn作为库实现。
- en: When to Use Neural Networks and Deep Learning
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何时使用神经网络和深度学习
- en: Neural networks and deep learning can be used for classification and regression,
    so how do they size up to linear regression, logistic regression, and other types
    of machine learning? You might have heard the expression “when all you have is
    a hammer, everything starts to look like a nail.” There are advantages and disadvantages
    that are situational for each type of algorithm. Linear regression and logistic
    regression, as well as gradient boosted trees (which we did not cover in this
    book), do a pretty fantastic job making predictions on structured data. Think
    of structured data as data that is easily represented as a table, with rows and
    columns. But perceptual problems like image classification are much less structured,
    as we are trying to find fuzzy correlations between groups of pixels to identify
    shapes and patterns, not rows of data in a table. Trying to predict the next four
    or five words in a sentence being typed, or deciphering the words being said in
    an audio clip, are also perceptual problems and examples of neural networks being
    used for natural language processing.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络和深度学习可用于分类和回归，那么它们与线性回归、逻辑回归和其他类型的机器学习相比如何？你可能听说过“当你手中只有一把锤子时，所有事情看起来都像钉子”。每种算法都有其特定情况下的优势和劣势。线性回归、逻辑回归以及梯度提升树（本书未涵盖）在结构化数据上做出了相当出色的预测。将结构化数据视为可以轻松表示为表格的数据，具有行和列。但感知问题如图像分类则不太结构化，因为我们试图找到像素组之间的模糊相关性以识别形状和模式，而不是表格中的数据行。尝试预测正在输入的句子中的下四五个单词，或者解密音频剪辑中的单词，也是感知问题，是神经网络用于自然语言处理的例子。
- en: In this chapter, we will primarily focus on simple neural networks with only
    one hidden layer.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将主要关注只有一个隐藏层的简单神经网络。
- en: Is Using a Neural Network Overkill?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络是否有些大材小用？
- en: Using neural networks for the upcoming example is probably overkill, as a logistic
    regression would probably be more practical. Even a [formulaic approach can be
    used](https://oreil.ly/M4W8i). However, I have always been a fan of understanding
    complex techniques by applying them to simple problems. You learn about the strengths
    and limitations of the technique rather than be distracted by large datasets.
    So with that in mind, try not to use neural networks where simpler models will
    be more practical. We will break this rule in this chapter for the sake of understanding
    the technique.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于即将介绍的例子来说，使用神经网络可能有些大材小用，因为逻辑回归可能更实用。甚至可以使用[公式方法](https://oreil.ly/M4W8i)。然而，我一直是一个喜欢通过将复杂技术应用于简单问题来理解的人。你可以了解技术的优势和局限性，而不会被大型数据集所分散注意力。因此，请尽量不要在更实用的情况下使用神经网络。为了理解技术，我们将在本章中打破这个规则。
- en: A Simple Neural Network
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个简单的神经网络
- en: Here is a simple example to get a feel for neural networks. I want to predict
    whether a font should be light (1) or dark (0) for a given color background. Here
    are a few examples of different background colors in [Figure 7-1](#bQLVHMTtrn).
    The top row looks best with light font, and the bottom row looks best with dark
    font.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简单的例子，让你对神经网络有所了解。我想要预测给定颜色背景下的字体应该是浅色（1）还是深色（0）。以下是不同背景颜色的几个示例，见[图 7-1](#bQLVHMTtrn)。顶部一行最适合浅色字体，底部一行最适合深色字体。
- en: '![emds 0701](Images/emds_0701.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0701](Images/emds_0701.png)'
- en: Figure 7-1\. Light background colors look best with dark font, and dark background
    colors look best with light font
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-1\. 浅色背景颜色最适合深色字体，而深色背景颜色最适合浅色字体
- en: In computer science one way to represent a color is with RGB values, or the
    red, green, and blue values. Each of these values is between 0 and 255 and expresses
    how these three colors are mixed to create the desired color. For example, if
    we express the RGB as (red, green, blue), then dark orange would have an RGB of
    (255,140,0) and pink would be (255,192,203). Black would be (0,0,0) and white
    would be (255,255,255).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学中，表示颜色的一种方式是使用RGB值，即红色、绿色和蓝色值。每个值都介于0和255之间，表示这三种颜色如何混合以创建所需的颜色。例如，如果我们将RGB表示为（红色，绿色，蓝色），那么深橙色的RGB值为（255,140,0），粉色为（255,192,203）。黑色为（0,0,0），白色为（255,255,255）。
- en: From a machine learning and regression perspective, we have three numeric input
    variables `red`, `green`, and `blue` to capture a given background color. We need
    to fit a function to these input variables and output whether a light (1) or dark
    (0) font should be used for that background color.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从机器学习和回归的角度来看，我们有三个数值输入变量`red`、`green`和`blue`来捕捉给定背景颜色。我们需要对这些输入变量拟合一个函数，并输出是否应该为该背景颜色使用浅色（1）或深色（0）字体。
- en: Representing Colors Through RGB
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过RGB表示颜色
- en: There are hundreds of color picker palettes online to experiment with RGB values.
    W3 Schools has one [here](https://oreil.ly/T57gu).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在线有数百种颜色选择器调色板可供尝试RGB值。W3 Schools有一个[这里](https://oreil.ly/T57gu)。
- en: Note this example is not far from how neural networks work recognizing images,
    as each pixel is often modeled as three numeric RGB values. In this case, we are
    just focusing on one “pixel” as a background color.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个例子与神经网络识别图像的工作原理并不相去甚远，因为每个像素通常被建模为三个数值RGB值。在这种情况下，我们只关注一个“像素”作为背景颜色。
- en: Let’s start high level and put all the implementation details aside. We are
    going to approach this topic like an onion, starting with a higher understanding
    and peeling away slowly into the details. For now, this is why we simply label
    as “mystery math” a process that takes inputs and produces outputs. We have three
    numeric input variables R, G, and B, which are processed by this mystery math.
    Then it outputs a prediction between 0 and 1 as shown in [Figure 7-2](#VtUKGwbfou).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从高层次开始，把所有的实现细节放在一边。我们将以洋葱的方式来处理这个主题，从更高的理解开始，然后慢慢剥离细节。目前，这就是为什么我们简单地将一个接受输入并产生输出的过程标记为“神秘数学”。我们有三个数值输入变量R、G和B，这些变量被这个神秘的数学处理。然后它输出一个介于0和1之间的预测，如[图7-2](#VtUKGwbfou)所示。
- en: '![emds 0702](Images/emds_0702.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0702](Images/emds_0702.png)'
- en: Figure 7-2\. We have three numeric RGB values used to make a prediction for
    a light or dark font
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2。我们有三个数值RGB值用于预测使用浅色或深色字体
- en: This prediction output expresses a probability. Outputting probabilities is
    the most common model for classification with neural networks. Once we replace
    RGB with their numerical values, we see that less than 0.5 will suggest a dark
    font whereas greater than 0.5 will suggest a light font as demonstrated in [Figure 7-3](#jnOWiEgusW).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个预测输出表示一个概率。输出概率是使用神经网络进行分类的最常见模型。一旦我们用它们的数值替换RGB，我们会发现小于0.5会建议使用深色字体，而大于0.5会建议使用浅色字体，如[图7-3](#jnOWiEgusW)所示。
- en: '![emds 0703](Images/emds_0703.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0703](Images/emds_0703.png)'
- en: Figure 7-3\. If we input a background color of pink (255,192,203), then the
    mystery math recommends a light font because the output probability 0.89 is greater
    than 0.5
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-3。如果我们输入一个粉色的背景色（255,192,203），那么神秘的数学会推荐使用浅色字体，因为输出概率0.89大于0.5
- en: So what is going on inside that mystery math black box? Let’s take a look in
    [Figure 7-4](#sGQdjdjUMw).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 那个神秘的数学黑匣子里到底发生了什么？让我们在[图7-4](#sGQdjdjUMw)中看一看。
- en: We are missing another piece of this neural network, the activation functions,
    but we will get to that shortly. Let’s first understand what’s going on here.
    The first layer on the left is simply an input of the three variables, which in
    this case are the red, green, and blue values. In the hidden (middle) layer, notice
    that we produce three *nodes*, or functions of weights and biases, between the
    inputs and outputs. Each node essentially is a linear function with slopes <math
    alttext="upper W Subscript i"><msub><mi>W</mi> <mi>i</mi></msub></math> and intercepts
    <math alttext="upper B Subscript i"><msub><mi>B</mi> <mi>i</mi></msub></math>
    being multiplied and summed with input variables <math alttext="upper X Subscript
    i"><msub><mi>X</mi> <mi>i</mi></msub></math> . There is a weight <math alttext="upper
    W Subscript i"><msub><mi>W</mi> <mi>i</mi></msub></math> between each input node
    and hidden node, and another set of weights between each hidden node and output
    node. Each hidden and output node gets an additional bias <math alttext="upper
    B Subscript i"><msub><mi>B</mi> <mi>i</mi></msub></math> added.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还缺少神经网络的另一个部分，即激活函数，但我们很快会讨论到。让我们先了解这里发生了什么。左侧的第一层只是三个变量的输入，这些变量在这种情况下是红色、绿色和蓝色值。在隐藏（中间）层中，请注意我们在输入和输出之间产生了三个*节点*，或者说是权重和偏置的函数。每个节点本质上是一个线性函数，斜率为<math
    alttext="upper W Subscript i"><msub><mi>W</mi> <mi>i</mi></msub></math>，截距为<math
    alttext="upper B Subscript i"><msub><mi>B</mi> <mi>i</mi></msub></math>，与输入变量<math
    alttext="upper X Subscript i"><msub><mi>X</mi> <mi>i</mi></msub></math>相乘并求和。每个输入节点和隐藏节点之间有一个权重<math
    alttext="upper W Subscript i"><msub><mi>W</mi> <mi>i</mi></msub></math>，每个隐藏节点和输出节点之间有另一组权重。每个隐藏和输出节点都会额外添加一个偏置<math
    alttext="upper B Subscript i"><msub><mi>B</mi> <mi>i</mi></msub></math>。
- en: '![emds 0704](Images/emds_0704.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0704](Images/emds_0704.png)'
- en: Figure 7-4\. The hidden layer of the neural network applies weight and bias
    values to each input variable, and the output layer applies another set of weights
    and biases to that output
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-4。神经网络的隐藏层对每个输入变量应用权重和偏置值，输出层对该输出应用另一组权重和偏置
- en: Notice the output node repeats the same operation, taking the resulting weighted
    and summed outputs from the hidden layer and making them inputs into the final
    layer, where another set of weights and biases will be applied.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，输出节点重复执行相同的操作，将隐藏层的加权和求和输出作为输入传递到最终层，其中另一组权重和偏置将被应用。
- en: In a nutshell, this is a regression just like linear or logistic regression,
    but with many more parameters to solve for. The weight and bias values are analogous
    to the *m* and *b*, or <math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math>
    and <math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math> , parameters
    in a linear regression. We do use stochastic gradient descent and minimize loss
    just like linear regression, but we need an additional tool called backpropagation
    to untangle the weight <math alttext="upper W Subscript i"><msub><mi>W</mi> <mi>i</mi></msub></math>
    and bias <math alttext="upper B Subscript i"><msub><mi>B</mi> <mi>i</mi></msub></math>
    values and calculate their partial derivatives using the chain rule. We will get
    to that later in this chapter, but for now let’s assume we have the weight and
    bias values optimized. We need to talk about activation functions first.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这是一个回归问题，就像线性回归或逻辑回归一样，但需要解决更多参数。权重和偏置值类似于*m*和*b*，或者线性回归中的<math alttext="beta
    1"><msub><mi>β</mi> <mn>1</mn></msub></math>和<math alttext="beta 0"><msub><mi>β</mi>
    <mn>0</mn></msub></math>参数。我们使用随机梯度下降和最小化损失，就像线性回归一样，但我们需要一种称为反向传播的额外工具来解开权重<math
    alttext="upper W Subscript i"><msub><mi>W</mi> <mi>i</mi></msub></math>和偏置<math
    alttext="upper B Subscript i"><msub><mi>B</mi> <mi>i</mi></msub></math>值，并使用链式法则计算它们的偏导数。我们将在本章后面详细讨论这一点，但现在让我们假设我们已经优化了权重和偏置值。我们需要先讨论激活函数。
- en: Activation Functions
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: Let’s bring in the activation functions next. An *activation function* is a
    nonlinear function that transforms or compresses the weighted and summed values
    in a node, helping the neural network separate the data effectively so it can
    be classified. Let’s take a look at [Figure 7-5](#PvLebFIsiT). If you do not have
    the activation functions, your hidden layers will not be productive and will perform
    no better than a linear regression.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来让我们介绍激活函数。*激活函数*是一个非线性函数，它转换或压缩节点中的加权和值，帮助神经网络有效地分离数据，以便进行分类。让我们看一下[图7-5](#PvLebFIsiT)。如果没有激活函数，你的隐藏层将毫无生产力，表现不会比线性回归好。
- en: '![emds 0705](Images/emds_0705.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0705](Images/emds_0705.png)'
- en: Figure 7-5\. Applying activation functions
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-5\. 应用激活函数
- en: The *ReLU activation function* will zero out any negative outputs from the hidden
    nodes. If the weights, biases, and inputs multiply and sum to a negative number,
    it will be converted to 0\. Otherwise the output is left alone. Here is the graph
    for ReLU ([Figure 7-6](#tKkerIrVkt)) using SymPy ([Example 7-1](#WIqRPnWuNe)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*ReLU 激活函数*将隐藏节点的任何负输出归零。如果权重、偏置和输入相乘并求和得到负数，它将被转换为 0。否则输出保持不变。这是使用 SymPy ([示例
    7-1](#WIqRPnWuNe)) 绘制的 ReLU 图（[图 7-6](#tKkerIrVkt))。'
- en: Example 7-1\. Plotting the ReLU function
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-1\. 绘制 ReLU 函数
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![emds 0706](Images/emds_0706.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0706](Images/emds_0706.png)'
- en: Figure 7-6\. Graph for ReLU function
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-6\. ReLU 函数图
- en: ReLU is short for “rectified linear unit,” but that is just a fancy way of saying
    “turn negative values into 0.” ReLU has gotten popular for hidden layers in neural
    networks and deep learning because of its speed and mitigation of the [vanishing
    gradient problem](https://oreil.ly/QGlM7). Vanishing gradients occur when the
    partial derivative slopes get so small they prematurely approach 0 and bring training
    to a screeching halt.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 是“修正线性单元”的缩写，但这只是一种将负值转换为 0 的花哨方式。ReLU 在神经网络和深度学习中的隐藏层中变得流行，因为它速度快，并且缓解了[梯度消失问题](https://oreil.ly/QGlM7)。梯度消失发生在偏导数斜率变得非常小，导致过早接近
    0 并使训练停滞。
- en: 'The output layer has an important job: it takes the piles of math from the
    hidden layers of the neural network and turns them into an interpretable result,
    such as presenting classification predictions. The output layer for this particular
    neural network uses the *logistic activation function*, which is a simple sigmoid
    curve. If you read [Chapter 6](ch06.xhtml#ch06), the logistic (or sigmoid) function
    should be familiar, and it demonstrates that logistic regression is acting as
    a layer in our neural network. The output node weights, biases, and sums each
    of the incoming values from the hidden layer. After that, it passes the resulting
    value through the logistic function so it outputs a number between 0 and 1\. Much
    like logistic regression in [Chapter 6](ch06.xhtml#ch06), this represents a probability
    that the given color input into the neural network recommends a light font. If
    it is greater than or equal to 0.5, the neural network is suggesting a light font,
    but less than that will advise a dark font.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层有一个重要的任务：它接收来自神经网络隐藏层的大量数学，并将其转换为可解释的结果，例如呈现分类预测。对于这个特定的神经网络，输出层使用*逻辑激活函数*，这是一个简单的
    S 形曲线。如果您阅读[第 6 章](ch06.xhtml#ch06)，逻辑（或 S 形）函数应该很熟悉，它表明逻辑回归在我们的神经网络中充当一层。输出节点的权重、偏置和从隐藏层传入的每个值求和。之后，它通过逻辑函数传递结果值，以便输出介于
    0 和 1 之间的数字。就像[第 6 章](ch06.xhtml#ch06)中的逻辑回归一样，这代表了输入到神经网络的给定颜色建议使用浅色字体的概率。如果大于或等于
    0.5，则神经网络建议使用浅色字体，否则建议使用深色字体。
- en: Here is the graph for the logistic function ([Figure 7-7](#kIPouDqVgq)) using
    SymPy ([Example 7-2](#EFehQtrsWp)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用 SymPy ([示例 7-2](#EFehQtrsWp)) 绘制的逻辑函数图（[图 7-7](#kIPouDqVgq)）。
- en: Example 7-2\. Logistic activation function in SymPy
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-2\. SymPy 中的逻辑激活函数
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![emds 0707](Images/emds_0707.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0707](Images/emds_0707.png)'
- en: Figure 7-7\. Logistic activation function
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-7\. 逻辑激活函数
- en: Note that when we pass a node’s weighted, biased, and summed value through an
    activation function, we now call that an *activated output*, meaning it has been
    filtered through the activation function. When the activated output leaves the
    hidden layer, the signal is ready to be fed into the next layer. The activation
    function could have strengthened, weakened, or left the signal as is. This is
    where the brain and synapse metaphor for neural networks comes from.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们通过激活函数传递节点的加权、偏置和求和值时，我们现在称之为*激活输出*，意味着它已通过激活函数进行了过滤。当激活输出离开隐藏层时，信号准备好被馈送到下一层。激活函数可能会增强、减弱或保持信号不变。这就是神经网络中大脑和突触的比喻的来源。
- en: Given the potential for complexity, you might be wondering if there are other
    activation functions. Some common ones are shown in [Table 7-1](#GhMbfHKelT).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于复杂性的潜在可能性，您可能想知道是否还有其他激活函数。一些常见的激活函数显示在[表 7-1](#GhMbfHKelT)中。
- en: Table 7-1\. Common activation functions
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7-1\. 常见激活函数
- en: '| Name | Typical layer used | Description | Notes |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 典型使用层 | 描述 | 注释 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Linear | Output | Leaves values as is | Not commonly used |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 线性 | 输出 | 保持值不变 | 不常用 |'
- en: '| Logistic | Output | S-shaped sigmoid curve | Compresses values between 0
    and 1, often assists binary classification |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Logistic | 输出层 | S 形 sigmoid 曲线 | 将值压缩在 0 和 1 之间，通常用于二元分类 |'
- en: '| Tangent Hyperbolic | Hidden | tanh, S-shaped sigmoid curve between -1 and
    1 | Assists in “centering” data by bringing mean close to 0 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 双曲正切 | 隐藏层 | tanh，在 -1 和 1 之间的 S 形 sigmoid 曲线 | 通过将均值接近 0 来“居中”数据 |'
- en: '| ReLU | Hidden | Turns negative values to 0 | Popular activation faster than
    sigmoid and tanh, mitigates vanishing gradient problems and computationally cheap
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| ReLU | 隐藏层 | 将负值转换为 0 | 比 sigmoid 和 tanh 更快的流行激活函数，缓解消失梯度问题，计算成本低廉 |'
- en: '| Leaky ReLU | Hidden | Multiplies negative values by 0.01 | Controversial
    variant of ReLU that marginalizes rather than eliminates negative values |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Leaky ReLU | 隐藏层 | 将负值乘以 0.01 | ReLU 的有争议变体，边缘化而不是消除负值 |'
- en: '| Softmax | Output | Ensures all output nodes add up to 1.0 | Useful for multiple
    classifications and rescaling outputs so they add up to 1.0 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Softmax | 输出层 | 确保所有输出节点加起来为 1.0 | 适用于多类别分类，重新缩放输出使其加起来为 1.0 |'
- en: This is not a comprehensive list of activation functions, and in theory any
    function could be an activation function in a neural network.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是激活函数的全面列表，理论上神经网络中任何函数都可以是激活函数。
- en: 'While this neural network seemingly supports two classes (light or dark font),
    it actually is modeled to one class: whether or not a font should be light (1)
    or not (0). If you wanted to support multiple classes, you could add more output
    nodes for each class. For instance, if you are trying to recognize handwritten
    digits 0–9, there would be 10 output nodes representing the probability a given
    image is each of those numbers. You might consider using softmax as the output
    activation when you have multiple classes as well. [Figure 7-8](#uPTTSNePsO) shows
    an example of taking a pixellated image of a digit, where the pixels are broken
    up as individual neural network inputs and then passed through two middle layers,
    and then an output layer with 10 nodes representing probabilities for 10 classes
    (for the digits 0–9).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个神经网络表面上支持两类（浅色或深色字体），但实际上它被建模为一类：字体是否应该是浅色（1）或不是（0）。如果您想支持多个类别，可以为每个类别添加更多输出节点。例如，如果您试图识别手写数字
    0-9，将有 10 个输出节点，代表给定图像是这些数字的概率。当有多个类别时，您可能还考虑在输出时使用 softmax 激活。[图 7-8](#uPTTSNePsO)
    展示了一个以像素化图像为输入的示例，其中像素被分解为单独的神经网络输入，然后通过两个中间层，最后一个输出层，有 10 个节点代表 10 个类别的概率（数字
    0-9）。
- en: '![emds 0708](Images/emds_0708.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0708](Images/emds_0708.png)'
- en: Figure 7-8\. A neural network that takes each pixel as an input and predicts
    what digit the image contains
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-8\. 一个神经网络，将每个像素作为输入，并预测图像包含的数字
- en: An example of using the MNIST dataset on a neural network can be found in [Appendix A](app01.xhtml#appendix).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络上使用 MNIST 数据集的示例可以在 [附录 A](app01.xhtml#appendix) 中找到。
- en: I Don’t Know What Activation Function to Use!
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我不知道要使用什么激活函数！
- en: If you are unsure what activations to use, current best practices gravitate
    toward ReLU for middle layers and logistic (sigmoid) for output layer. If you
    have multiple classifications in the output, use softmax for the output layer.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不确定要使用哪些激活函数，当前最佳实践倾向于在中间层使用 ReLU，在输出层使用 logistic（sigmoid）。如果输出中有多个分类，可以在输出层使用
    softmax。
- en: Forward Propagation
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前向传播
- en: Let’s capture what we have learned so far using NumPy. Note I have not optimized
    the parameters (our weight and bias values) yet. We are going to initialize those
    with random values.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 NumPy 捕获到目前为止学到的知识。请注意，我尚未优化参数（我们的权重和偏置值）。我们将用随机值初始化它们。
- en: '[Example 7-3](#OMrNeTihfU) is the Python code to create a simple feed-forward
    neural network that is not optimized yet. *Feed forward* means we are simply inputting
    a color into the neural network and seeing what it outputs. The weights and biases
    are randomly initialized and will be optimized later in this chapter, so do not
    expect a useful output yet.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 7-3](#OMrNeTihfU) 是创建一个简单的前馈神经网络的 Python 代码，尚未进行优化。*前馈* 意味着我们只是将一种颜色输入到神经网络中，看看它输出什么。权重和偏置是随机初始化的，并将在本章后面进行优化，因此暂时不要期望有用的输出。'
- en: Example 7-3\. A simple forward propagation network with random weight and bias
    values
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-3\. 一个具有随机权重和偏置值的简单前向传播网络
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A couple of things to note here. The dataset containing the RGB input values
    as well as output value (1 for light and 0 for dark) are contained in [this CSV
    file](https://oreil.ly/1TZIK). I am scaling down the input columns R, G, and B
    values by a factor of 1/255 so they are between 0 and 1\. This will help the training
    later so the number space is compressed.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几点需要注意。 包含 RGB 输入值以及输出值（1 代表亮，0 代表暗）的数据集包含在[此 CSV 文件](https://oreil.ly/1TZIK)中。
    我将输入列 R、G 和 B 的值缩小了 1/255 的因子，使它们介于 0 和 1 之间。 这将有助于后续的训练，以便压缩数字空间。
- en: Note I also separated 2/3 of the data for training and 1/3 for testing using
    scikit-learn, which we learned how to do in [Chapter 5](ch05.xhtml#ch05). `n`
    is simply the number of training data records.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我还使用 scikit-learn 将数据的 2/3 用于训练，1/3 用于测试，我们在[第 5 章](ch05.xhtml#ch05)中学习了如何做到这一点。
    `n` 简单地是训练数据记录的数量。
- en: Now bring your attention to the lines of code shown in [Example 7-4](#omJOMqoOMh).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在请注意[示例 7-4](#omJOMqoOMh)中显示的代码行。
- en: Example 7-4\. The weight matrices and bias vectors in NumPy
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-4。 NumPy 中的权重矩阵和偏置向量
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: These are declaring our weights and biases for both the hidden and output layers
    of our neural network. This may not be obvious yet but matrix multiplication is
    going to make our code powerfully simple using linear algebra and NumPy.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些声明了我们神经网络隐藏层和输出层的权重和偏置。 这可能还不明显，但矩阵乘法将使我们的代码变得强大简单，使用线性代数和 NumPy。
- en: 'The weights and biases are going to be initialized as random values between
    0 and 1\. Let’s look at the weight matrices first. When I ran the code I got these
    matrices:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 权重和偏置将被初始化为介于 0 和 1 之间的随机值。 让我们首先看一下权重矩阵。 当我运行代码时，我得到了这些矩阵：
- en: <math display="block"><mrow><msub><mi>W</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>=</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>0.034535</mn></mrow></mtd>
    <mtd><mrow><mn>0.5185636</mn></mrow></mtd> <mtd><mrow><mn>0.81485028</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>0.3329199</mn></mrow></mtd> <mtd><mrow><mn>0.53873853</mn></mrow></mtd>
    <mtd><mrow><mn>0.96359003</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>0.19808306</mn></mrow></mtd>
    <mtd><mrow><mn>0.45422182</mn></mrow></mtd> <mtd><mrow><mn>0.36618893</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math><math
    display="block"><mrow><msub><mi>W</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>0.82652072</mn></mrow></mtd>
    <mtd><mrow><mn>0.30781539</mn></mrow></mtd> <mtd><mrow><mn>0.93095565</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>W</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>=</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>0.034535</mn></mrow></mtd>
    <mtd><mrow><mn>0.5185636</mn></mrow></mtd> <mtd><mrow><mn>0.81485028</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>0.3329199</mn></mrow></mtd> <mtd><mrow><mn>0.53873853</mn></mrow></mtd>
    <mtd><mrow><mn>0.96359003</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>0.19808306</mn></mrow></mtd>
    <mtd><mrow><mn>0.45422182</mn></mrow></mtd> <mtd><mrow><mn>0.36618893</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math><math
    display="block"><mrow><msub><mi>W</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>0.82652072</mn></mrow></mtd>
    <mtd><mrow><mn>0.30781539</mn></mrow></mtd> <mtd><mrow><mn>0.93095565</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: Note that <math alttext="upper W Subscript h i d d e n"><msub><mi>W</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub></math>
    are the weights in the hidden layer. The first row represents the first node weights
    <math alttext="upper W 1"><msub><mi>W</mi> <mn>1</mn></msub></math> , <math alttext="upper
    W 2"><msub><mi>W</mi> <mn>2</mn></msub></math> , and <math alttext="upper W 3"><msub><mi>W</mi>
    <mn>3</mn></msub></math> . The second row is the second node with weights <math
    alttext="upper W 4"><msub><mi>W</mi> <mn>4</mn></msub></math> , <math alttext="upper
    W 5"><msub><mi>W</mi> <mn>5</mn></msub></math> , and <math alttext="upper W 6"><msub><mi>W</mi>
    <mn>6</mn></msub></math> . The third row is the third node with weights <math
    alttext="upper W 7"><msub><mi>W</mi> <mn>7</mn></msub></math> , <math alttext="upper
    W 8"><msub><mi>W</mi> <mn>8</mn></msub></math> , and <math alttext="upper W 9"><msub><mi>W</mi>
    <mn>9</mn></msub></math> .
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，<math alttext="upper W Subscript h i d d e n"><msub><mi>W</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub></math>
    是隐藏层中的权重。第一行代表第一个节点的权重 <math alttext="upper W 1"><msub><mi>W</mi> <mn>1</mn></msub></math>，<math
    alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math> 和 <math alttext="upper
    W 3"><msub><mi>W</mi> <mn>3</mn></msub></math>。第二行是第二个节点，带有权重 <math alttext="upper
    W 4"><msub><mi>W</mi> <mn>4</mn></msub></math>，<math alttext="upper W 5"><msub><mi>W</mi>
    <mn>5</mn></msub></math> 和 <math alttext="upper W 6"><msub><mi>W</mi> <mn>6</mn></msub></math>。第三行是第三个节点，带有权重
    <math alttext="upper W 7"><msub><mi>W</mi> <mn>7</mn></msub></math>，<math alttext="upper
    W 8"><msub><mi>W</mi> <mn>8</mn></msub></math> 和 <math alttext="upper W 9"><msub><mi>W</mi>
    <mn>9</mn></msub></math>。
- en: The output layer has only one node, meaning its matrix has only one row with
    weights <math alttext="upper W 10"><msub><mi>W</mi> <mn>10</mn></msub></math>
    , <math alttext="upper W 11"><msub><mi>W</mi> <mn>11</mn></msub></math> , and
    <math alttext="upper W 12"><msub><mi>W</mi> <mn>12</mn></msub></math> .
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层只有一个节点，意味着其矩阵只有一行，带有权重 <math alttext="upper W 10"><msub><mi>W</mi> <mn>10</mn></msub></math>，<math
    alttext="upper W 11"><msub><mi>W</mi> <mn>11</mn></msub></math> 和 <math alttext="upper
    W 12"><msub><mi>W</mi> <mn>12</mn></msub></math>。
- en: See a pattern here? Each node is represented as a row in a matrix. If there
    are three nodes, there are three rows. If there is one node, there is one row.
    Each column holds a weight value for that node.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 看到规律了吗？每个节点在矩阵中表示为一行。如果有三个节点，就有三行。如果只有一个节点，就只有一行。每一列保存着该节点的权重值。
- en: 'Let’s look at the biases, too. Since there is one bias per node, there are
    going to be three rows of biases for the hidden layer and one row of biases for
    the output layer. There’s only one bias per node so there will be only one column:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也看看偏差。由于每个节点有一个偏差，隐藏层将有三行偏差，输出层将有一行偏差。每个节点只有一个偏差，所以只有一列：
- en: <math display="block"><mrow><msub><mi>B</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>=</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>0.41379442</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>0.81666079</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>0.07511252</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math><math
    display="block"><mrow><msub><mi>B</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>0.58018555</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>B</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>=</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>0.41379442</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>0.81666079</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>0.07511252</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math><math
    display="block"><mrow><msub><mi>B</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>0.58018555</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: Now let’s compare these matrix values to our visualized neural network as shown
    in [Figure 7-9](#vSdgvskUoQ).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将这些矩阵值与我们在 [图 7-9](#vSdgvskUoQ) 中展示的神经网络进行比较。
- en: '![emds 0709](Images/emds_0709.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0709](Images/emds_0709.png)'
- en: Figure 7-9\. Visualizing our neural network against the weight and bias matrix
    values
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-9。将我们的神经网络与权重和偏差矩阵值进行可视化
- en: So besides being esoterically compact, what is the benefit of these weights
    and biases in this matrix form? Let’s bring our attention to these lines of code
    in [Example 7-5](#cKHHrQlImr).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 那么除了紧凑难懂之外，这种矩阵形式中的权重和偏差有什么好处呢？让我们把注意力转向 [示例 7-5](#cKHHrQlImr) 中的这些代码行。
- en: Example 7-5\. The activation functions and forward propagation function for
    our neural network
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-5。我们神经网络的激活函数和前向传播函数
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This code is important because it concisely executes our entire neural network
    using matrix multiplication and matrix-vector multiplication. We learned about
    these operations in [Chapter 4](ch04.xhtml#ch04). It runs a color of three RGB
    inputs through the weights, biases, and activation functions in just a few lines
    of code.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码很重要，因为它使用矩阵乘法和矩阵-向量乘法简洁地执行我们整个神经网络。我们在[第4章](ch04.xhtml#ch04)中学习了这些操作。它仅用几行代码将三个RGB输入的颜色通过权重、偏置和激活函数运行。
- en: 'I first declare the `relu()` and `logistic()` activation functions, which literally
    take a given input value and return the output value from the curve. The `forward_prop()`
    function executes our entire neural network for a given color input `X` containing
    the R, G, and B values. It returns the matrix outputs from four stages: `Z1`,
    `A1`, `Z2`, and `A2`. The “1” and “2” indicate the operations belong to layers
    1 and 2 respectively. The “Z” indicates an unactivated output from the layer,
    and “A” is activated output from the layer.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我声明`relu()`和`logistic()`激活函数，它们分别接受给定的输入值并返回曲线的输出值。`forward_prop()`函数为包含R、G和B值的给定颜色输入`X`执行整个神经网络。它返回四个阶段的矩阵输出：`Z1`、`A1`、`Z2`和`A2`。数字“1”和“2”表示操作属于第1层和第2层。“Z”表示来自该层的未激活输出，“A”是来自该层的激活输出。
- en: The hidden layer is represented by `Z1` and `A1`. `Z1` is the weights and biases
    applied to `X`. Then `A1` takes that output from `Z1` and pushes it through the
    activation ReLU function. `Z2` takes the output from `A1` and applies the output
    layer weights and biases. That output is in turn pushed through the activation
    function, the logistic curve, and becomes `A2`. The final stage, `A2`, is the
    prediction probability from the output layer, returning a value between 0 and
    1\. We call it `A2` because it is the “activated” output from layer 2.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层由`Z1`和`A1`表示。`Z1`是应用于`X`的权重和偏置。然后`A1`获取来自`Z1`的输出，并通过激活ReLU函数。`Z2`获取来自`A1`的输出，并应用输出层的权重和偏置。该输出依次通过激活函数，即逻辑曲线，变为`A2`。最终阶段，`A2`，是输出层的预测概率，返回一个介于0和1之间的值。我们称之为`A2`，因为它是来自第2层的“激活”输出。
- en: 'Let’s break this down in more detail starting with `Z1`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地分解这个，从`Z1`开始：
- en: <math alttext="upper Z 1 equals upper W Subscript h i d d e n Baseline upper
    X plus upper B Subscript h i d d e n" display="block"><mrow><msub><mi>Z</mi> <mn>1</mn></msub>
    <mo>=</mo> <msub><mi>W</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mi>X</mi> <mo>+</mo> <msub><mi>B</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub></mrow></math>
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper Z 1 equals upper W Subscript h i d d e n Baseline upper
    X plus upper B Subscript h i d d e n" display="block"><mrow><msub><mi>Z</mi> <mn>1</mn></msub>
    <mo>=</mo> <msub><mi>W</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mi>X</mi> <mo>+</mo> <msub><mi>B</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub></mrow></math>
- en: First we perform matrix-vector multiplication between <math alttext="upper W
    Subscript h i d d e n"><msub><mi>W</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub></math>
    and the input color `X`. We multiply each row of <math alttext="upper W Subscript
    h i d d e n"><msub><mi>W</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub></math>
    (each row being a set of weights for a node) with the vector `X` (the RGB color
    input values). We then add the biases to that result, as shown in [Figure 7-10](#jPQPcShSLf).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在<math alttext="upper W Subscript h i d d e n"><msub><mi>W</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub></math>和输入颜色`X`之间执行矩阵-向量乘法。我们将<math
    alttext="upper W Subscript h i d d e n"><msub><mi>W</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub></math>的每一行（每一行都是一个节点的权重集）与向量`X`（RGB颜色输入值）相乘。然后将偏置添加到该结果中，如[图 7-10](#jPQPcShSLf)所示。
- en: '![emds 0710](Images/emds_0710.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0710](Images/emds_0710.png)'
- en: Figure 7-10\. Applying the hidden layer weights and biases to an input `X` using
    matrix-vector multiplication as well as vector addition
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-10。将隐藏层权重和偏置应用于输入`X`，使用矩阵-向量乘法以及向量加法
- en: That <math alttext="upper Z 1"><msub><mi>Z</mi> <mn>1</mn></msub></math> vector
    is the raw output from the hidden layer, but we still need to pass it through
    the activation function to turn <math alttext="upper Z 1"><msub><mi>Z</mi> <mn>1</mn></msub></math>
    into <math alttext="upper A 1"><msub><mi>A</mi> <mn>1</mn></msub></math> . Easy
    enough. Just pass each value in that vector through the ReLU function and it will
    give us <math alttext="upper A 1"><msub><mi>A</mi> <mn>1</mn></msub></math> .
    Because all the values are positive, it should not have an impact.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 那个<math alttext="upper Z 1"><msub><mi>Z</mi> <mn>1</mn></msub></math>向量是隐藏层的原始输出，但我们仍然需要通过激活函数将<math
    alttext="upper Z 1"><msub><mi>Z</mi> <mn>1</mn></msub></math>转换为<math alttext="upper
    A 1"><msub><mi>A</mi> <mn>1</mn></msub></math>。很简单。只需将该向量中的每个值通过ReLU函数传递，就会给我们<math
    alttext="upper A 1"><msub><mi>A</mi> <mn>1</mn></msub></math>。因为所有值都是正值，所以不应该有影响。
- en: <math alttext="upper A 1 equals upper R e upper L upper U left-parenthesis upper
    Z 1 right-parenthesis" display="block"><mrow><msub><mi>A</mi> <mn>1</mn></msub>
    <mo>=</mo> <mi>R</mi> <mi>e</mi> <mi>L</mi> <mi>U</mi> <mrow><mo>(</mo> <msub><mi>Z</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math><math display="block"><mrow><msub><mi>A</mi>
    <mn>1</mn></msub> <mo>=</mo> <mfenced separators="" open="[" close="]"><mtable><mtr><mtd><mrow><mi>R</mi>
    <mi>e</mi> <mi>L</mi> <mi>U</mi> <mo>(</mo> <mn>1.36054964190909</mn> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd><mrow><mi>R</mi> <mi>e</mi> <mi>L</mi> <mi>U</mi> <mo>(</mo> <mn>2.15471757888247</mn>
    <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mrow><mi>R</mi> <mi>e</mi> <mi>L</mi>
    <mi>U</mi> <mo>(</mo> <mn>0.719554393391768</mn> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced separators="" open="[" close="]"><mtable><mtr><mtd><mrow><mn>1.36054964190909</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>2.15471757888247</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>0.719554393391768</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper A 1 equals upper R e upper L upper U left-parenthesis upper
    Z 1 right-parenthesis" display="block"><mrow><msub><mi>A</mi> <mn>1</mn></msub>
    <mo>=</mo> <mi>R</mi> <mi>e</mi> <mi>L</mi> <mi>U</mi> <mrow><mo>(</mo> <msub><mi>Z</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math><math display="block"><mrow><msub><mi>A</mi>
    <mn>1</mn></msub> <mo>=</mo> <mfenced separators="" open="[" close="]"><mtable><mtr><mtd><mrow><mi>R</mi>
    <mi>e</mi> <mi>L</mi> <mi>U</mi> <mo>(</mo> <mn>1.36054964190909</mn> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd><mrow><mi>R</mi> <mi>e</mi> <mi>L</mi> <mi>U</mi> <mo>(</mo> <mn>2.15471757888247</mn>
    <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mrow><mi>R</mi> <mi>e</mi> <mi>L</mi>
    <mi>U</mi> <mo>(</mo> <mn>0.719554393391768</mn> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced separators="" open="[" close="]"><mtable><mtr><mtd><mrow><mn>1.36054964190909</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>2.15471757888247</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>0.719554393391768</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: Now let’s take that hidden layer output <math alttext="upper A 1"><msub><mi>A</mi>
    <mn>1</mn></msub></math> and pass it through the final layer to get <math alttext="upper
    Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math> and then <math alttext="upper A
    2"><msub><mi>A</mi> <mn>2</mn></msub></math> . <math alttext="upper A 1"><msub><mi>A</mi>
    <mn>1</mn></msub></math> becomes the input into the output layer.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将隐藏层输出<math alttext="upper A 1"><msub><mi>A</mi> <mn>1</mn></msub></math>传递到最终层，得到<math
    alttext="upper Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math>，然后<math alttext="upper
    A 2"><msub><mi>A</mi> <mn>2</mn></msub></math>。 <math alttext="upper A 1"><msub><mi>A</mi>
    <mn>1</mn></msub></math> 成为输出层的输入。
- en: <math alttext="upper Z 2 equals upper W Subscript o u t p u t Baseline upper
    A 1 plus upper B Subscript o u t p u t" display="block"><mrow><msub><mi>Z</mi>
    <mn>2</mn></msub> <mo>=</mo> <msub><mi>W</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msub>
    <msub><mi>A</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>B</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow></math><math
    display="block"><mrow><msub><mi>Z</mi> <mn>2</mn></msub> <mo>=</mo> <mfenced open="["
    close="]"><mtable><mtr><mtd><mrow><mn>0.82652072</mn></mrow></mtd> <mtd><mrow><mn>0.3078159</mn></mrow></mtd>
    <mtd><mrow><mn>0.93095565</mn></mrow></mtd></mtr></mtable></mfenced> <mfenced
    separators="" open="[" close="]"><mtable><mtr><mtd><mrow><mn>1.36054964190909</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>2.15471757888247</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>0.719554393391768</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>+</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>0.58018555</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math><math
    display="block"><mrow><msub><mi>Z</mi> <mn>2</mn></msub> <mo>=</mo> <mfenced open="["
    close="]"><mtable><mtr><mtd><mrow><mn>2.45765202842636</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>+</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>0.58018555</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math><math
    display="block"><mrow><msub><mi>Z</mi> <mn>2</mn></msub> <mo>=</mo> <mfenced open="["
    close="]"><mtable><mtr><mtd><mrow><mn>3.03783757842636</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper Z 2 equals upper W Subscript o u t p u t Baseline upper
    A 1 plus upper B Subscript o u t p u t" display="block"><mrow><msub><mi>Z</mi>
    <mn>2</mn></msub> <mo>=</mo> <msub><mi>W</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msub>
    <msub><mi>A</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>B</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow></math><math
    display="block"><mrow><msub><mi>Z</mi> <mn>2</mn></msub> <mo>=</mo> <mfenced open="["
    close="]"><mtable><mtr><mtd><mrow><mn>0.82652072</mn></mrow></mtd> <mtd><mrow><mn>0.3078159</mn></mrow></mtd>
    <mtd><mrow><mn>0.93095565</mn></mrow></mtd></mtr></mtable></mfenced> <mfenced
    separators="" open="[" close="]"><mtable><mtr><mtd><mrow><mn>1.36054964190909</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>2.15471757888247</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>0.719554393391768</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>+</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>0.58018555</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math><math
    display="block"><mrow><msub><mi>Z</mi> <mn>2</mn></msub> <mo>=</mo> <mfenced open="["
    close="]"><mtable><mtr><mtd><mrow><mn>2.45765202842636</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>+</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>0.58018555</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math><math
    display="block"><mrow><msub><mi>Z</mi> <mn>2</mn></msub> <mo>=</mo> <mfenced open="["
    close="]"><mtable><mtr><mtd><mrow><mn>3.03783757842636</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'Finally, pass this single value in <math alttext="upper Z 2"><msub><mi>Z</mi>
    <mn>2</mn></msub></math> through the activation function to get <math alttext="upper
    A 2"><msub><mi>A</mi> <mn>2</mn></msub></math> . This will produce a prediction
    of approximately 0.95425:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将这个单个值<math alttext="upper Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math>通过激活函数传递，得到<math
    alttext="upper A 2"><msub><mi>A</mi> <mn>2</mn></msub></math>。这将产生一个约为0.95425的预测：
- en: <math display="block" class="mathml_bottom_space"><mrow><msub><mi>A</mi> <mn>2</mn></msub>
    <mo>=</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi> <mi>i</mi> <mi>s</mi> <mi>t</mi> <mi>i</mi>
    <mi>c</mi> <mrow><mo>(</mo> <msub><mi>Z</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
    <math display="block" class="mathml_bottom_space"><mrow><msub><mi>A</mi> <mn>2</mn></msub>
    <mo>=</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi> <mi>i</mi> <mi>s</mi> <mi>t</mi> <mi>i</mi>
    <mi>c</mi> <mrow><mo>(</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>3.0378364795204</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>)</mo></mrow></mrow></math> <math display="block"><mrow><msub><mi>A</mi> <mn>2</mn></msub>
    <mo>=</mo> <mn>0.954254478103241</mn></mrow></math>
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block" class="mathml_bottom_space"><mrow><msub><mi>A</mi> <mn>2</mn></msub>
    <mo>=</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi> <mi>i</mi> <mi>s</mi> <mi>t</mi> <mi>i</mi>
    <mi>c</mi> <mrow><mo>(</mo> <msub><mi>Z</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
    <math display="block" class="mathml_bottom_space"><mrow><msub><mi>A</mi> <mn>2</mn></msub>
    <mo>=</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi> <mi>i</mi> <mi>s</mi> <mi>t</mi> <mi>i</mi>
    <mi>c</mi> <mrow><mo>(</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>3.0378364795204</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>)</mo></mrow></mrow></math> <math display="block"><mrow><msub><mi>A</mi> <mn>2</mn></msub>
    <mo>=</mo> <mn>0.954254478103241</mn></mrow></math>
- en: That executes our entire neural network, although we have not trained it yet.
    But take a moment to appreciate that we have taken all these input values, weights,
    biases, and nonlinear functions and turned them all into a single value that will
    provide a prediction.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 执行我们整个神经网络，尽管我们尚未对其进行训练。但请花点时间欣赏，我们已经将所有这些输入值、权重、偏差和非线性函数转化为一个将提供预测的单个值。
- en: Again, `A2` is the final output that makes a prediction whether that background
    color need a light (1) or dark (1) font. Even though our weights and biases have
    not been optimized yet, let’s calculate our accuracy as shown in [Example 7-6](#mLlirJipiN).
    Take the testing dataset `X_test`, transpose it, and pass it through the `forward_prop()`
    function but only grab the `A2` vector with the predictions for each test color.
    Then compare the predictions to the actuals and calculate the percentage of correct
    predictions.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，`A2`是最终输出，用于预测背景颜色是否需要浅色（1）或深色（1）字体。尽管我们的权重和偏差尚未优化，让我们按照[示例 7-6](#mLlirJipiN)计算准确率。取测试数据集`X_test`，转置它，并通过`forward_prop()`函数传递，但只获取`A2`向量，其中包含每个测试颜色的预测。然后将预测与实际值进行比较，并计算正确预测的百分比。
- en: Example 7-6\. Calculating accuracy
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-6\. 计算准确率
- en: '[PRE5]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'When I run the whole code in [Example 7-3](#OMrNeTihfU), I roughly get anywhere
    from 55% to 67% accuracy. Remember, the weights and biases are randomly generated
    so answers will vary. While this may seem high given the parameters were randomly
    generated, remember that the output predictions are binary: light or dark. Therefore,
    a random coin flip might as well produce this outcome for each prediction, so
    this number should not be surprising.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当我运行[示例 7-3](#OMrNeTihfU)中的整个代码时，我大致获得55%到67%的准确率。请记住，权重和偏差是随机生成的，因此答案会有所不同。虽然这个准确率似乎很高，考虑到参数是随机生成的，但请记住，输出预测是二元的：浅色或深色。因此，对于每个预测，随机抛硬币也可能产生这种结果，所以这个数字不应令人惊讶。
- en: Do Not Forget to Check for Imbalanced Data!
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不要忘记检查是否存在数据不平衡！
- en: 'As discussed in [Chapter 6](ch06.xhtml#ch06), do not forget to analyze your
    data to check for imbalanced classes. This whole background color dataset is a
    little imbalanced: 512 colors have output of 0 and 833 have an output of 1\. This
    can skew accuracy and might be why our random weights and biases gravitate higher
    than 50% accuracy. If the data is extremely imbalanced (as in 99% of the data
    is one class), then remember to use confusion matrices to track the false positives
    and false negatives.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第6章](ch06.xhtml#ch06)中讨论的那样，不要忘记分析数据以检查是否存在不平衡的类别。整个背景颜色数据集有点不平衡：512种颜色的输出为0，833种颜色的输出为1。这可能会使准确率产生偏差，也可能是我们的随机权重和偏差倾向于高于50%准确率的原因。如果数据极度不平衡（例如99%的数据属于一类），请记住使用混淆矩阵来跟踪假阳性和假阴性。
- en: 'Does everything structurally make sense so far? Feel free to review everything
    up to this point before moving on. We just have one final piece to cover: optimizing
    the weights and biases. Hit the espresso machine or nitro coffee bar, because
    this is the most involved math we will be doing in this book!'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，结构上一切都合理吗？在继续之前，随时可以回顾一切。我们只剩下最后一个部分要讲解：优化权重和偏差。去冲杯浓缩咖啡或氮气咖啡吧，因为这是本书中我们将要做的最复杂的数学！
- en: Backpropagation
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: Before we start using stochastic gradient descent to optimize our neural network,
    a challenge we have is figuring out how to change each of the weight and bias
    values accordingly, even though they all are tangled together to create the output
    variable, which then is used to calculate the residuals. How do we find the derivative
    of each weight <math alttext="upper W Subscript i"><msub><mi>W</mi> <mi>i</mi></msub></math>
    and bias <math alttext="upper B Subscript i"><msub><mi>B</mi> <mi>i</mi></msub></math>
    variable? We need to use the chain rule, which we covered in [Chapter 1](ch01.xhtml#ch01).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始使用随机梯度下降来优化我们的神经网络之前，我们面临的挑战是如何相应地改变每个权重和偏差值，尽管它们都纠缠在一起以创建输出变量，然后用于计算残差。我们如何找到每个权重<math
    alttext="upper W Subscript i"><msub><mi>W</mi> <mi>i</mi></msub></math> 和偏差<math
    alttext="upper B Subscript i"><msub><mi>B</mi> <mi>i</mi></msub></math> 变量的导数？我们需要使用我们在[第1章](ch01.xhtml#ch01)中讨论过的链式法则。
- en: Calculating the Weight and Bias Derivatives
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算权重和偏差的导数
- en: We are not quite ready to apply stochastic gradient descent to train our neural
    network. We have to get the partial derivatives with respect to the weights <math
    alttext="upper W Subscript i"><msub><mi>W</mi> <mi>i</mi></msub></math> and biases
    <math alttext="upper B Subscript i"><msub><mi>B</mi> <mi>i</mi></msub></math>
    , and we have the chain rule to help us.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有准备好应用随机梯度下降来训练我们的神经网络。我们需要求出相对于权重<math alttext="upper W Subscript i"><msub><mi>W</mi>
    <mi>i</mi></msub></math> 和偏差<math alttext="upper B Subscript i"><msub><mi>B</mi>
    <mi>i</mi></msub></math> 的偏导数，而且我们有链式法则来帮助我们。
- en: While the process is largely the same, there is a complication using stochastic
    gradient descent on neural networks. The nodes in one layer feed their weights
    and biases into the next layer, which then applies another set of weights and
    biases. This creates an onion-like nesting we need to untangle, starting with
    the output layer.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然过程基本相同，但在神经网络上使用随机梯度下降存在一个复杂性。一层中的节点将它们的权重和偏置传递到下一层，然后应用另一组权重和偏置。这创建了一个类似洋葱的嵌套结构，我们需要从输出层开始解开。
- en: 'During gradient descent, we need to figure out which weights and biases should
    be adjusted, and by how much, to reduce the overall cost function. The cost for
    a single prediction is going to be the squared output of the neural network <math
    alttext="upper A 2"><msub><mi>A</mi> <mn>2</mn></msub></math> minus the actual
    value <math alttext="upper Y"><mi>Y</mi></math> :'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度下降过程中，我们需要找出应该调整哪些权重和偏置，以及调整多少，以减少整体成本函数。单个预测的成本将是神经网络的平方输出<math alttext="upper
    A 2"><msub><mi>A</mi> <mn>2</mn></msub></math>减去实际值<math alttext="upper Y"><mi>Y</mi></math>：
- en: <math alttext="upper C equals left-parenthesis upper A 2 minus upper Y right-parenthesis
    squared" display="block"><mrow><mi>C</mi> <mo>=</mo> <msup><mrow><mo>(</mo><msub><mi>A</mi>
    <mn>2</mn></msub> <mo>-</mo><mi>Y</mi><mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper C equals left-parenthesis upper A 2 minus upper Y right-parenthesis
    squared" display="block"><mrow><mi>C</mi> <mo>=</mo> <msup><mrow><mo>(</mo><msub><mi>A</mi>
    <mn>2</mn></msub> <mo>-</mo><mi>Y</mi><mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: 'But let’s peel back a layer. That activated output <math alttext="upper A 2"><msub><mi>A</mi>
    <mn>2</mn></msub></math> is just <math alttext="upper Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math>
    with the activation function:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 但让我们再往下一层。那个激活输出<math alttext="upper A 2"><msub><mi>A</mi> <mn>2</mn></msub></math>只是带有激活函数的<math
    alttext="upper Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math>：
- en: <math alttext="upper A 2 equals s i g m o i d left-parenthesis upper Z 2 right-parenthesis"
    display="block"><mrow><msub><mi>A</mi> <mn>2</mn></msub> <mo>=</mo> <mi>s</mi>
    <mi>i</mi> <mi>g</mi> <mi>m</mi> <mi>o</mi> <mi>i</mi> <mi>d</mi> <mrow><mo>(</mo>
    <msub><mi>Z</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper A 2 equals s i g m o i d left-parenthesis upper Z 2 right-parenthesis"
    display="block"><mrow><msub><mi>A</mi> <mn>2</mn></msub> <mo>=</mo> <mi>s</mi>
    <mi>i</mi> <mi>g</mi> <mi>m</mi> <mi>o</mi> <mi>i</mi> <mi>d</mi> <mrow><mo>(</mo>
    <msub><mi>Z</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
- en: '<math alttext="upper Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math> in turn
    is the output weights and biases applied to activation output <math alttext="upper
    A 1"><msub><mi>A</mi> <mn>1</mn></msub></math> , which comes from the hidden layer:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 转而，<math alttext="upper Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math>是应用于激活输出<math
    alttext="upper A 1"><msub><mi>A</mi> <mn>1</mn></msub></math>的输出权重和偏置，这些输出来自隐藏层：
- en: <math alttext="upper Z 2 equals upper W 2 upper A 1 plus upper B 2" display="block"><mrow><msub><mi>Z</mi>
    <mn>2</mn></msub> <mo>=</mo> <msub><mi>W</mi> <mn>2</mn></msub> <msub><mi>A</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi>B</mi> <mn>2</mn></msub></mrow></math>
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper Z 2 equals upper W 2 upper A 1 plus upper B 2" display="block"><mrow><msub><mi>Z</mi>
    <mn>2</mn></msub> <mo>=</mo> <msub><mi>W</mi> <mn>2</mn></msub> <msub><mi>A</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi>B</mi> <mn>2</mn></msub></mrow></math>
- en: '<math alttext="upper A 1"><msub><mi>A</mi> <mn>1</mn></msub></math> is built
    off <math alttext="upper Z 1"><msub><mi>Z</mi> <mn>1</mn></msub></math> which
    is passed through the ReLU activation function:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper A 1"><msub><mi>A</mi> <mn>1</mn></msub></math>是由通过ReLU激活函数传递的<math
    alttext="upper Z 1"><msub><mi>Z</mi> <mn>1</mn></msub></math>构建而成：
- en: <math alttext="upper A 1 equals upper R e upper L upper U left-parenthesis upper
    Z 1 right-parenthesis" display="block"><mrow><msub><mi>A</mi> <mn>1</mn></msub>
    <mo>=</mo> <mi>R</mi> <mi>e</mi> <mi>L</mi> <mi>U</mi> <mrow><mo>(</mo> <msub><mi>Z</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper A 1 equals upper R e upper L upper U left-parenthesis upper
    Z 1 right-parenthesis" display="block"><mrow><msub><mi>A</mi> <mn>1</mn></msub>
    <mo>=</mo> <mi>R</mi> <mi>e</mi> <mi>L</mi> <mi>U</mi> <mrow><mo>(</mo> <msub><mi>Z</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math>
- en: 'Finally, <math alttext="upper Z 1"><msub><mi>Z</mi> <mn>1</mn></msub></math>
    is the input x-values weighted and biased by the hidden layer:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，<math alttext="upper Z 1"><msub><mi>Z</mi> <mn>1</mn></msub></math>是由隐藏层加权和偏置的输入x值：
- en: <math alttext="upper Z 1 equals upper W 1 upper X plus upper B 1" display="block"><mrow><msub><mi>Z</mi>
    <mn>1</mn></msub> <mo>=</mo> <msub><mi>W</mi> <mn>1</mn></msub> <mi>X</mi> <mo>+</mo>
    <msub><mi>B</mi> <mn>1</mn></msub></mrow></math>
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper Z 1 equals upper W 1 upper X plus upper B 1" display="block"><mrow><msub><mi>Z</mi>
    <mn>1</mn></msub> <mo>=</mo> <msub><mi>W</mi> <mn>1</mn></msub> <mi>X</mi> <mo>+</mo>
    <msub><mi>B</mi> <mn>1</mn></msub></mrow></math>
- en: We need to find the weights and biases contained in the <math alttext="upper
    W 1"><msub><mi>W</mi> <mn>1</mn></msub></math> , <math alttext="upper B 1"><msub><mi>B</mi>
    <mn>1</mn></msub></math> , <math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>
    , and <math alttext="upper B 2"><msub><mi>B</mi> <mn>2</mn></msub></math> matrices
    and vectors that will minimize our loss. By nudging their slopes, we can change
    the weights and biases that have the most impact in minimizing loss. However,
    each little nudge on a weight or bias is going to propagate all the way to the
    loss function on the outer layer. This is where the chain rule can help us figure
    out this impact.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要找到包含在矩阵和向量<math alttext="upper W 1"><msub><mi>W</mi> <mn>1</mn></msub></math>，<math
    alttext="upper B 1"><msub><mi>B</mi> <mn>1</mn></msub></math>，<math alttext="upper
    W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>和<math alttext="upper B 2"><msub><mi>B</mi>
    <mn>2</mn></msub></math>中的权重和偏置，以最小化我们的损失。通过微调它们的斜率，我们可以改变对最小化损失影响最大的权重和偏置。然而，对权重或偏置进行微小调整会一直传播到外层的损失函数。这就是链式法则可以帮助我们找出这种影响的地方。
- en: 'Let’s focus on finding the relationship on a weight from the output layer <math
    alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math> and the cost function
    <math alttext="upper C"><mi>C</mi></math> . A change in the weight <math alttext="upper
    W 2"><msub><mi>W</mi> <mn>2</mn></msub></math> results in a change to the unactivated
    output <math alttext="upper Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math> . That
    then changes the activated output <math alttext="upper A 2"><msub><mi>A</mi> <mn>2</mn></msub></math>
    , which changes the cost function <math alttext="upper C"><mi>C</mi></math> .
    Using the chain rule, we can define the derivative of <math alttext="upper C"><mi>C</mi></math>
    with respect to <math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>
    as this:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们专注于找到输出层权重<math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>和成本函数<math
    alttext="upper C"><mi>C</mi></math>之间的关系。权重<math alttext="upper W 2"><msub><mi>W</mi>
    <mn>2</mn></msub></math>的变化导致未激活的输出<math alttext="upper Z 2"><msub><mi>Z</mi>
    <mn>2</mn></msub></math>的变化。然后改变激活输出<math alttext="upper A 2"><msub><mi>A</mi>
    <mn>2</mn></msub></math>，进而改变成本函数<math alttext="upper C"><mi>C</mi></math>。利用链式法则，我们可以定义关于<math
    alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>的导数如下：
- en: <math alttext="StartFraction d upper C Over d upper W 2 EndFraction equals StartFraction
    d upper Z 2 Over d upper W 2 EndFraction StartFraction d upper A 2 Over d upper
    Z 2 EndFraction StartFraction d upper C Over d upper A 2 EndFraction" display="block"><mrow><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>W</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>W</mi> <mn>2</mn></msub></mrow></mfrac>
    <mfrac><mrow><mi>d</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><mi>C</mi></mrow> <mrow><mi>d</mi><msub><mi>A</mi>
    <mn>2</mn></msub></mrow></mfrac></mrow></math>
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction d upper C Over d upper W 2 EndFraction equals StartFraction
    d upper Z 2 Over d upper W 2 EndFraction StartFraction d upper A 2 Over d upper
    Z 2 EndFraction StartFraction d upper C Over d upper A 2 EndFraction" display="block"><mrow><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>W</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>W</mi> <mn>2</mn></msub></mrow></mfrac>
    <mfrac><mrow><mi>d</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><mi>C</mi></mrow> <mrow><mi>d</mi><msub><mi>A</mi>
    <mn>2</mn></msub></mrow></mfrac></mrow></math>
- en: When we multiply these three gradients together, we get a measure of how much
    a change to <math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>
    will change the cost function <math alttext="upper C"><mi>C</mi></math> .
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将这三个梯度相乘在一起时，我们得到了改变<math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>将如何改变成本函数<math
    alttext="upper C"><mi>C</mi></math>的度量。
- en: Now we will calculate these three derivatives. Let’s use SymPy to calculate
    the derivative of the cost function with respect to <math alttext="upper A 2"><msub><mi>A</mi>
    <mn>2</mn></msub></math> in [Example 7-7](#bJeDOpjeQi).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将计算这三个导数。让我们使用SymPy计算在[示例 7-7](#bJeDOpjeQi)中关于<math alttext="upper A 2"><msub><mi>A</mi>
    <mn>2</mn></msub></math>的成本函数的导数。
- en: <math alttext="StartFraction d upper C Over d upper A 2 EndFraction equals 2
    upper A 2 minus 2 y" display="block"><mrow><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mn>2</mn>
    <msub><mi>A</mi> <mn>2</mn></msub> <mo>-</mo> <mn>2</mn> <mi>y</mi></mrow></math>
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction d upper C Over d upper A 2 EndFraction equals 2
    upper A 2 minus 2 y" display="block"><mrow><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mn>2</mn>
    <msub><mi>A</mi> <mn>2</mn></msub> <mo>-</mo> <mn>2</mn> <mi>y</mi></mrow></math>
- en: Example 7-7\. Calculating the derivative of the cost function with respect to
    <math alttext="upper A 2"><msub><mi>A</mi> <mn>2</mn></msub></math>
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-7. 计算成本函数关于<math alttext="upper A 2"><msub><mi>A</mi> <mn>2</mn></msub></math>的导数
- en: '[PRE6]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Next, let’s get the derivative of <math alttext="upper A 2"><msub><mi>A</mi>
    <mn>2</mn></msub></math> with respect to <math alttext="upper Z 2"><msub><mi>Z</mi>
    <mn>2</mn></msub></math> ([Example 7-8](#FiirvKJUSW)). Remember that <math alttext="upper
    A 2"><msub><mi>A</mi> <mn>2</mn></msub></math> is the output of an activation
    function, in this case the logistic function. So we really are just taking the
    derivative of a sigmoid curve.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们找到关于<math alttext="upper Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math>的导数与<math
    alttext="upper A 2"><msub><mi>A</mi> <mn>2</mn></msub></math>的导数（[示例 7-8](#FiirvKJUSW)）。记住<math
    alttext="upper A 2"><msub><mi>A</mi> <mn>2</mn></msub></math>是激活函数的输出，在这种情况下是逻辑函数。因此，我们实际上只是在求取S形曲线的导数。
- en: <math alttext="StartFraction d upper A 2 Over d upper Z 2 EndFraction equals
    StartFraction e Superscript minus upper Z 2 Baseline Over left-parenthesis 1 plus
    e Superscript minus upper Z 2 Baseline right-parenthesis squared EndFraction"
    display="block"><mrow><mfrac><mrow><mi>d</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow>
    <mrow><mi>d</mi><msub><mi>Z</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><msup><mi>e</mi>
    <mrow><mo>-</mo><msub><mi>Z</mi> <mn>2</mn></msub></mrow></msup> <msup><mfenced
    separators="" open="(" close=")"><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></msup></mfenced> <mn>2</mn></msup></mfrac></mrow></math>
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction d upper A 2 Over d upper Z 2 EndFraction equals
    StartFraction e Superscript minus upper Z 2 Baseline Over left-parenthesis 1 plus
    e Superscript minus upper Z 2 Baseline right-parenthesis squared EndFraction"
    display="block"><mrow><mfrac><mrow><mi>d</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow>
    <mrow><mi>d</mi><msub><mi>Z</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><msup><mi>e</mi>
    <mrow><mo>-</mo><msub><mi>Z</mi> <mn>2</mn></msub></mrow></msup> <msup><mfenced
    separators="" open="(" close=")"><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></msup></mfenced> <mn>2</mn></msup></mfrac></mrow></math>
- en: Example 7-8\. Finding the derivative of <math alttext="upper A 2"><msub><mi>A</mi>
    <mn>2</mn></msub></math> with respect to <math alttext="upper Z 2"><msub><mi>Z</mi>
    <mn>2</mn></msub></math>
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-8. 找到关于<math alttext="upper Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math>的导数与<math
    alttext="upper A 2"><msub><mi>A</mi> <mn>2</mn></msub></math>
- en: '[PRE7]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The derivative of <math alttext="upper Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math>
    with respect to <math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>
    is going to work out to be <math alttext="upper A 1"><msub><mi>A</mi> <mn>1</mn></msub></math>
    , as it is just a linear function and going to return the slope ([Example 7-9](#kPubQpquMU)).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math>关于<math alttext="upper
    W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>的导数将会得到<math alttext="upper A 1"><msub><mi>A</mi>
    <mn>1</mn></msub></math>，因为它只是一个线性函数，将返回斜率（[示例 7-9](#kPubQpquMU)）。
- en: <math alttext="StartFraction d upper Z 2 Over d upper W 1 EndFraction equals
    upper A 1" display="block"><mrow><mfrac><mrow><mi>d</mi><msub><mi>Z</mi> <mn>2</mn></msub></mrow>
    <mrow><mi>d</mi><msub><mi>W</mi> <mn>1</mn></msub></mrow></mfrac> <mo>=</mo> <msub><mi>A</mi>
    <mn>1</mn></msub></mrow></math>
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction d upper Z 2 Over d upper W 1 EndFraction equals
    upper A 1" display="block"><mrow><mfrac><mrow><mi>d</mi><msub><mi>Z</mi> <mn>2</mn></msub></mrow>
    <mrow><mi>d</mi><msub><mi>W</mi> <mn>1</mn></msub></mrow></mfrac> <mo>=</mo> <msub><mi>A</mi>
    <mn>1</mn></msub></mrow></math>
- en: Example 7-9\. Derivative of <math alttext="upper Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math>
    with respect to <math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-9. 关于<math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>的导数<math
    alttext="upper Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math>
- en: '[PRE8]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Putting it all together, here is the derivative to find how much a change in
    a weight in <math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>
    affects the cost function <math alttext="upper C"><mi>C</mi></math> :'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起，这里是找到改变<math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>中的权重会如何影响成本函数<math
    alttext="upper C"><mi>C</mi></math>的导数：
- en: <math alttext="StartFraction d upper C Over d w 2 EndFraction equals StartFraction
    d upper Z 2 Over d w 2 EndFraction StartFraction d upper A 2 Over d upper Z 2
    EndFraction StartFraction d upper C Over d upper A 2 EndFraction equals left-parenthesis
    upper A 1 right-parenthesis left-parenthesis StartFraction e Superscript minus
    upper Z 2 Baseline Over left-parenthesis 1 plus e Superscript minus upper Z 2
    Baseline right-parenthesis squared EndFraction right-parenthesis left-parenthesis
    2 upper A 2 minus 2 y right-parenthesis" display="block"><mrow><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>w</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>w</mi> <mn>2</mn></msub></mrow></mfrac>
    <mfrac><mrow><mi>d</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><mi>C</mi></mrow> <mrow><mi>d</mi><msub><mi>A</mi>
    <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mrow><mo>(</mo> <msub><mi>A</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mrow><mo>(</mo> <mfrac><msup><mi>e</mi> <mrow><mo>-</mo><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></msup> <msup><mfenced separators="" open="(" close=")"><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><msub><mi>Z</mi> <mn>2</mn></msub></mrow></msup></mfenced> <mn>2</mn></msup></mfrac>
    <mo>)</mo></mrow> <mrow><mo>(</mo> <mn>2</mn> <msub><mi>A</mi> <mn>2</mn></msub>
    <mo>-</mo> <mn>2</mn> <mi>y</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction d upper C Over d w 2 EndFraction equals StartFraction
    d upper Z 2 Over d w 2 EndFraction StartFraction d upper A 2 Over d upper Z 2
    EndFraction StartFraction d upper C Over d upper A 2 EndFraction equals left-parenthesis
    upper A 1 right-parenthesis left-parenthesis StartFraction e Superscript minus
    upper Z 2 Baseline Over left-parenthesis 1 plus e Superscript minus upper Z 2
    Baseline right-parenthesis squared EndFraction right-parenthesis left-parenthesis
    2 upper A 2 minus 2 y right-parenthesis" display="block"><mrow><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>w</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>w</mi> <mn>2</mn></msub></mrow></mfrac>
    <mfrac><mrow><mi>d</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><mi>C</mi></mrow> <mrow><mi>d</mi><msub><mi>A</mi>
    <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mrow><mo>(</mo> <msub><mi>A</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mrow><mo>(</mo> <mfrac><msup><mi>e</mi> <mrow><mo>-</mo><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></msup> <msup><mfenced separators="" open="(" close=")"><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><msub><mi>Z</mi> <mn>2</mn></msub></mrow></msup></mfenced> <mn>2</mn></msup></mfrac>
    <mo>)</mo></mrow> <mrow><mo>(</mo> <mn>2</mn> <msub><mi>A</mi> <mn>2</mn></msub>
    <mo>-</mo> <mn>2</mn> <mi>y</mi> <mo>)</mo></mrow></mrow></math>
- en: When we run an input `X` with the three input R, G, and B values, we will have
    values for <math alttext="upper A 1"><msub><mi>A</mi> <mn>1</mn></msub></math>
    , <math alttext="upper A 2"><msub><mi>A</mi> <mn>2</mn></msub></math> , <math
    alttext="upper Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math> , and <math alttext="y"><mi>y</mi></math>
    .
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行一个输入`X`与三个输入R、G和B值时，我们将得到<math alttext="upper A 1"><msub><mi>A</mi> <mn>1</mn></msub></math>、<math
    alttext="upper A 2"><msub><mi>A</mi> <mn>2</mn></msub></math>、<math alttext="upper
    Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math>和<math alttext="y"><mi>y</mi></math>的值。
- en: Don’t Get Lost in the Math!
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不要在数学中迷失！
- en: It is easy to get lost in the math at this point and forget what you were trying
    to achieve in the first place, which is finding the derivative of the cost function
    with respect to a weight ( <math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>
    ) in the output layer. When you find yourself in the weeds and forgetting what
    you were trying to do, then step back, go for a walk, get a coffee, and remind
    yourself what you were trying to accomplish. If you cannot, you should start over
    from the beginning and work your way to the point you got lost.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点很容易在数学中迷失并忘记你最初想要实现的目标，即找到成本函数关于输出层中的权重（<math alttext="upper W 2"><msub><mi>W</mi>
    <mn>2</mn></msub></math>）的导数。当你发现自己陷入困境并忘记你要做什么时，那就退一步，出去走走，喝杯咖啡，提醒自己你要达成的目标。如果你做不到，你应该从头开始，一步步地找回迷失的地方。
- en: However, this is just one component of the neural network, the derivative for
    <math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math> . Here are
    the SymPy calculations in [Example 7-10](#hqtBpgBaGT) for the rest of the partial
    derivatives we will need for chaining.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这只是神经网络的一个组成部分，对于<math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>的导数。以下是我们在[示例 7-10](#hqtBpgBaGT)中使用SymPy计算的其余部分偏导数，这些是我们在链式求导中需要的。
- en: Example 7-10\. Calculating all the partial derivatives we will need for our
    neural network
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-10。计算我们神经网络所需的所有偏导数
- en: '[PRE9]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Notice that ReLU was calculated manually rather than using SymPy’s `diff()`
    function. This is because derivatives work with smooth curves, not jagged corners
    that exist on ReLU. But it’s easy to hack around that simply by declaring the
    slope to be 1 for positive numbers and 0 for negative numbers. This makes sense
    because negative numbers have a flat line with slope 0\. But positive numbers
    are left as is with a 1-to-1 slope.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，ReLU是手动计算的，而不是使用SymPy的`diff()`函数。这是因为导数适用于平滑曲线，而不是ReLU上存在的锯齿角。但通过简单地声明斜率为正数为1，负数为0，可以轻松解决这个问题。这是有道理的，因为负数具有斜率为0的平坦线。但正数保持不变，具有1:1的斜率。
- en: 'These partial derivatives can be chained together to create new partial derivatives
    with respect to the weights and biases. Let’s get all four partial derivatives
    for the weights in <math alttext="upper W 1"><msub><mi>W</mi> <mn>1</mn></msub></math>
    , <math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math> , <math
    alttext="upper B 1"><msub><mi>B</mi> <mn>1</mn></msub></math> , and <math alttext="upper
    B 2"><msub><mi>B</mi> <mn>2</mn></msub></math> with respect to the cost function.
    We already walked through <math alttext="StartFraction d upper C Over d w 2 EndFraction"><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>w</mi> <mn>2</mn></msub></mrow></mfrac></math> . Let’s
    show it alongside the other three chained derivatives we need:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这些偏导数可以链接在一起，以创建相对于权重和偏置的新偏导数。让我们为<math alttext="upper W 1"><msub><mi>W</mi>
    <mn>1</mn></msub></math>、<math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>、<math
    alttext="upper B 1"><msub><mi>B</mi> <mn>1</mn></msub></math>和<math alttext="upper
    B 2"><msub><mi>B</mi> <mn>2</mn></msub></math>相对于成本函数的四个偏导数。我们已经讨论了<math alttext="StartFraction
    d upper C Over d w 2 EndFraction"><mfrac><mrow><mi>d</mi><mi>C</mi></mrow> <mrow><mi>d</mi><msub><mi>w</mi>
    <mn>2</mn></msub></mrow></mfrac></math>。让我们将其与其他三个链式求导一起展示：
- en: <math alttext="StartFraction d upper C Over d upper W 2 EndFraction equals StartFraction
    d upper Z 2 Over d upper W 2 EndFraction StartFraction d upper A 2 Over d upper
    Z 2 EndFraction StartFraction d upper C Over d upper A 2 EndFraction equals left-parenthesis
    upper A 1 right-parenthesis left-parenthesis StartFraction e Superscript minus
    upper Z 2 Baseline Over left-parenthesis 1 plus e Superscript minus upper Z 2
    Baseline right-parenthesis squared EndFraction right-parenthesis left-parenthesis
    2 upper A 2 minus 2 y right-parenthesis" display="block"><mrow><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>W</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>W</mi> <mn>2</mn></msub></mrow></mfrac>
    <mfrac><mrow><mi>d</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><mi>C</mi></mrow> <mrow><mi>d</mi><msub><mi>A</mi>
    <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mrow><mo>(</mo> <msub><mi>A</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mrow><mo>(</mo> <mfrac><msup><mi>e</mi> <mrow><mo>-</mo><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></msup> <msup><mfenced separators="" open="(" close=")"><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><msub><mi>Z</mi> <mn>2</mn></msub></mrow></msup></mfenced> <mn>2</mn></msup></mfrac>
    <mo>)</mo></mrow> <mrow><mo>(</mo> <mn>2</mn> <msub><mi>A</mi> <mn>2</mn></msub>
    <mo>-</mo> <mn>2</mn> <mi>y</mi> <mo>)</mo></mrow></mrow></math><math alttext="StartFraction
    d upper C Over d upper B 2 EndFraction equals StartFraction d upper Z 2 Over d
    upper B 2 EndFraction StartFraction d upper A 2 Over d upper Z 2 EndFraction StartFraction
    d upper C Over d upper A 2 EndFraction equals left-parenthesis 1 right-parenthesis
    left-parenthesis StartFraction e Superscript minus upper Z 2 Baseline Over left-parenthesis
    1 plus e Superscript minus upper Z 2 Baseline right-parenthesis squared EndFraction
    right-parenthesis left-parenthesis 2 upper A 2 minus 2 y right-parenthesis" display="block"><mrow><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>B</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>B</mi> <mn>2</mn></msub></mrow></mfrac>
    <mfrac><mrow><mi>d</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><mi>C</mi></mrow> <mrow><mi>d</mi><msub><mi>A</mi>
    <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>)</mo></mrow>
    <mrow><mo>(</mo> <mfrac><msup><mi>e</mi> <mrow><mo>-</mo><msub><mi>Z</mi> <mn>2</mn></msub></mrow></msup>
    <msup><mfenced separators="" open="(" close=")"><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><msub><mi>Z</mi> <mn>2</mn></msub></mrow></msup></mfenced> <mn>2</mn></msup></mfrac>
    <mo>)</mo></mrow> <mrow><mo>(</mo> <mn>2</mn> <msub><mi>A</mi> <mn>2</mn></msub>
    <mo>-</mo> <mn>2</mn> <mi>y</mi> <mo>)</mo></mrow></mrow></math><math alttext="StartFraction
    d upper C Over d upper W 1 EndFraction equals StartFraction d upper C Over upper
    D upper A 2 EndFraction StartFraction upper D upper A 2 Over d upper Z 2 EndFraction
    StartFraction d upper Z 2 Over d upper A 1 EndFraction StartFraction d upper A
    1 Over d upper Z 1 EndFraction StartFraction d upper Z 1 Over d upper W 1 EndFraction
    equals left-parenthesis 2 upper A 2 minus 2 y right-parenthesis left-parenthesis
    StartFraction e Superscript minus upper Z 2 Baseline Over left-parenthesis 1 plus
    e Superscript minus upper Z 2 Baseline right-parenthesis squared EndFraction right-parenthesis
    left-parenthesis upper W 2 right-parenthesis left-parenthesis upper Z 1 greater-than
    0 right-parenthesis left-parenthesis upper X right-parenthesis" display="block"><mrow><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>W</mi> <mn>1</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>D</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow></mfrac> <mfrac><mrow><mi>D</mi><msub><mi>A</mi>
    <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>Z</mi> <mn>2</mn></msub></mrow></mfrac>
    <mfrac><mrow><mi>d</mi><msub><mi>Z</mi> <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>A</mi>
    <mn>1</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><msub><mi>A</mi> <mn>1</mn></msub></mrow>
    <mrow><mi>d</mi><msub><mi>Z</mi> <mn>1</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>1</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>W</mi> <mn>1</mn></msub></mrow></mfrac>
    <mo>=</mo> <mrow><mo>(</mo> <mn>2</mn> <msub><mi>A</mi> <mn>2</mn></msub> <mo>-</mo>
    <mn>2</mn> <mi>y</mi> <mo>)</mo></mrow> <mrow><mo>(</mo> <mfrac><msup><mi>e</mi>
    <mrow><mo>-</mo><msub><mi>Z</mi> <mn>2</mn></msub></mrow></msup> <msup><mfenced
    separators="" open="(" close=")"><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></msup></mfenced> <mn>2</mn></msup></mfrac> <mo>)</mo></mrow>
    <mrow><mo>(</mo> <msub><mi>W</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mrow><mo>(</mo>
    <msub><mi>Z</mi> <mn>1</mn></msub> <mo>></mo> <mn>0</mn> <mo>)</mo></mrow> <mrow><mo>(</mo>
    <mi>X</mi> <mo>)</mo></mrow></mrow></math><math alttext="StartFraction d upper
    C Over d upper B 1 EndFraction equals StartFraction d upper C Over upper D upper
    A 2 EndFraction StartFraction upper D upper A 2 Over d upper Z 2 EndFraction StartFraction
    d upper Z 2 Over d upper A 1 EndFraction StartFraction d upper A 1 Over d upper
    Z 1 EndFraction StartFraction d upper Z 1 Over d upper B 1 EndFraction equals
    left-parenthesis 2 upper A 2 minus 2 y right-parenthesis left-parenthesis StartFraction
    e Superscript minus upper Z 2 Baseline Over left-parenthesis 1 plus e Superscript
    minus upper Z 2 Baseline right-parenthesis squared EndFraction right-parenthesis
    left-parenthesis upper W 2 right-parenthesis left-parenthesis upper Z 1 greater-than
    0 right-parenthesis left-parenthesis 1 right-parenthesis" display="block"><mrow><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>B</mi> <mn>1</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>D</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow></mfrac> <mfrac><mrow><mi>D</mi><msub><mi>A</mi>
    <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>Z</mi> <mn>2</mn></msub></mrow></mfrac>
    <mfrac><mrow><mi>d</mi><msub><mi>Z</mi> <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>A</mi>
    <mn>1</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><msub><mi>A</mi> <mn>1</mn></msub></mrow>
    <mrow><mi>d</mi><msub><mi>Z</mi> <mn>1</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>1</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>B</mi> <mn>1</mn></msub></mrow></mfrac>
    <mo>=</mo> <mrow><mo>(</mo> <mn>2</mn> <msub><mi>A</mi> <mn>2</mn></msub> <mo>-</mo>
    <mn>2</mn> <mi>y</mi> <mo>)</mo></mrow> <mrow><mo>(</mo> <mfrac><msup><mi>e</mi>
    <mrow><mo>-</mo><msub><mi>Z</mi> <mn>2</mn></msub></mrow></msup> <msup><mfenced
    separators="" open="(" close=")"><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></msup></mfenced> <mn>2</mn></msup></mfrac> <mo>)</mo></mrow>
    <mrow><mo>(</mo> <msub><mi>W</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mrow><mo>(</mo>
    <msub><mi>Z</mi> <mn>1</mn></msub> <mo>></mo> <mn>0</mn> <mo>)</mo></mrow> <mrow><mo>(</mo>
    <mn>1</mn> <mo>)</mo></mrow></mrow></math>
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction d upper C Over d upper W 2 EndFraction equals StartFraction
    d upper Z 2 Over d upper W 2 EndFraction StartFraction d upper A 2 Over d upper
    Z 2 EndFraction StartFraction d upper C Over d upper A 2 EndFraction equals left-parenthesis
    upper A 1 right-parenthesis left-parenthesis StartFraction e Superscript minus
    upper Z 2 Baseline Over left-parenthesis 1 plus e Superscript minus upper Z 2
    Baseline right-parenthesis squared EndFraction right-parenthesis left-parenthesis
    2 upper A 2 minus 2 y right-parenthesis" display="block"><mrow><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>W</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>W</mi> <mn>2</mn></msub></mrow></mfrac>
    <mfrac><mrow><mi>d</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><mi>C</mi></mrow> <mrow><mi>d</mi><msub><mi>A</mi>
    <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mrow><mo>(</mo> <msub><mi>A</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mrow><mo>(</mo> <mfrac><msup><mi>e</mi> <mrow><mo>-</mo><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></msup> <msup><mfenced separators="" open="(" close=")"><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><msub><mi>Z</mi> <mn>2</mn></msub></mrow></msup></mfenced> <mn>2</mn></msup></mfrac>
    <mo>)</mo></mrow> <mrow><mo>(</mo> <mn>2</mn> <msub><mi>A</mi> <mn>2</mn></msub>
    <mo>-</mo> <mn>2</mn> <mi>y</mi> <mo>)</mo></mrow></mrow></math><math alttext="StartFraction
    d upper C Over d upper B 2 EndFraction equals StartFraction d upper Z 2 Over d
    upper B 2 EndFraction StartFraction d upper A 2 Over d upper Z 2 EndFraction StartFraction
    d upper C Over d upper A 2 EndFraction equals left-parenthesis 1 right-parenthesis
    left-parenthesis StartFraction e Superscript minus upper Z 2 Baseline Over left-parenthesis
    1 plus e Superscript minus upper Z 2 Baseline right-parenthesis squared EndFraction
    right-parenthesis left-parenthesis 2 upper A 2 minus 2 y right-parenthesis" display="block"><mrow><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>B</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>B</mi> <mn>2</mn></msub></mrow></mfrac>
    <mfrac><mrow><mi>d</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><mi>C</mi></mrow> <mrow><mi>d</mi><msub><mi>A</mi>
    <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>)</mo></mrow>
    <mrow><mo>(</mo> <mfrac><msup><mi>e</mi> <mrow><mo>-</mo><msub><mi>Z</mi> <mn>2</mn></msub></mrow></msup>
    <msup><mfenced separators="" open="(" close=")"><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><msub><mi>Z</mi> <mn>2</mn></msub></mrow></msup></mfenced> <mn>2</mn></msup></mfrac>
    <mo>)</mo></mrow> <mrow><mo>(</mo> <mn>2</mn> <msub><mi>A</mi> <mn>2</mn></msub>
    <mo>-</mo> <mn>2</mn> <mi>y</mi> <mo>)</mo></mrow></mrow></math><math alttext="StartFraction
    d upper C Over d upper W 1 EndFraction equals StartFraction d upper C Over upper
    D upper A 2 EndFraction StartFraction upper D upper A 2 Over d upper Z 2 EndFraction
    StartFraction d upper Z 2 Over d upper A 1 EndFraction StartFraction d upper A
    1 Over d upper Z 1 EndFraction StartFraction d upper Z 1 Over d upper W 1 EndFraction
    equals left-parenthesis 2 upper A 2 minus 2 y right-parenthesis left-parenthesis
    StartFraction e Superscript minus upper Z 2 Baseline Over left-parenthesis 1 plus
    e Superscript minus upper Z 2 Baseline right-parenthesis squared EndFraction right-parenthesis
    left-parenthesis upper W 2 right-parenthesis left-parenthesis upper Z 1 greater-than
    0 right-parenthesis left-parenthesis upper X right-parenthesis" display="block"><mrow><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>W</mi> <mn>1</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>D</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow></mfrac> <mfrac><mrow><mi>D</mi><msub><mi>A</mi>
    <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>Z</mi> <mn>2</mn></msub></mrow></mfrac>
    <mfrac><mrow><mi>d</mi><msub><mi>Z</mi> <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>A</mi>
    <mn>1</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><msub><mi>A</mi> <mn>1</mn></msub></mrow>
    <mrow><mi>d</mi><msub><mi>Z</mi> <mn>1</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>1</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>W</mi> <mn>1</mn></msub></mrow></mfrac>
    <mo>=</mo> <mrow><mo>(</mo> <mn>2</mn> <msub><mi>A</mi> <mn>2</mn></msub> <mo>-</mo>
    <mn>2</mn> <mi>y</mi> <mo>)</mo></mrow> <mrow><mo>(</mo> <mfrac><msup><mi>e</mi>
    <mrow><mo>-</mo><msub><mi>Z</mi> <mn>2</mn></msub></mrow></msup> <msup><mfenced
    separators="" open="(" close=")"><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><msub><mi>Z</mi>
    <mn>2</mn
- en: We will use these chained gradients to calculate the slope for the cost function
    *C* with respect to <math alttext="upper W 1"><msub><mi>W</mi> <mn>1</mn></msub></math>
    , <math alttext="upper B 1"><msub><mi>B</mi> <mn>1</mn></msub></math> , <math
    alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math> , and <math alttext="upper
    B 2"><msub><mi>B</mi> <mn>2</mn></msub></math> .
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这些链式梯度来计算成本函数*C*相对于<math alttext="upper W 1"><msub><mi>W</mi> <mn>1</mn></msub></math>，<math
    alttext="upper B 1"><msub><mi>B</mi> <mn>1</mn></msub></math>，<math alttext="upper
    W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>和<math alttext="upper B 2"><msub><mi>B</mi>
    <mn>2</mn></msub></math>的斜率。
- en: Stochastic Gradient Descent
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: We are now ready to integrate the chain rule to perform stochastic gradient
    descent. To keep things simple, we are going to sample only one training record
    on every iteration. Batch and mini-batch gradient descent are commonly used in
    neural networks and deep learning, but there’s enough linear algebra and calculus
    to juggle just one sample per iteration.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备整合链式法则来执行随机梯度下降。为了保持简单，我们每次迭代只对一个训练记录进行采样。在神经网络和深度学习中通常使用批量梯度下降和小批量梯度下降，但是在每次迭代中只处理一个样本就足够了，因为其中涉及足够的线性代数和微积分。
- en: Let’s take a look at our full implementation of our neural network, with backpropagated
    stochastic gradient descent, in [Example 7-11](#MfDTsSwbfB).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们的神经网络的完整实现，使用反向传播的随机梯度下降，在[示例 7-11](#MfDTsSwbfB)中。
- en: Example 7-11\. Implementing a neural network using stochastic gradient descent
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-11\. 使用随机梯度下降实现神经网络
- en: '[PRE10]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: There is a lot going on here, but it builds on everything else we learned in
    this chapter. We perform 100,000 iterations of stochastic gradient descent. Splitting
    the training and testing data by 2/3 and 1/3, respectively, I get approximately
    97–99% accuracy in my testing dataset depending on how the randomness works out.
    This means after training, my neural network correctly identifies 97–99% of the
    testing data with the right light/dark font predictions.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这里涉及很多内容，但是建立在我们在本章学到的一切基础之上。我们进行了10万次随机梯度下降迭代。将训练和测试数据分别按2/3和1/3划分，根据随机性的不同，我在测试数据集中获得了大约97-99%的准确率。这意味着训练后，我的神经网络能够正确识别97-99%的测试数据，并做出正确的浅色/深色字体预测。
- en: The `backward_prop()` function is key here, implementing the chain rule to take
    the error in the output node (the squared residual), and then divide it up and
    distribute it backward to the output and hidden weights/biases to get the slopes
    with respect to each weight/bias. We then take those slopes and nudge the weights/biases
    in the `for` loop, respectively, multiplying with the learning rate `L` just like
    we did in Chapters [5](ch05.xhtml#ch05) and [6](ch06.xhtml#ch06). We do some matrix-vector
    multiplication to distribute the error backward based on the slopes, and we transpose
    matrices and vectors when needed so the dimensions between rows and columns match
    up.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`backward_prop()`函数在这里起着关键作用，实现链式法则，将输出节点的误差（平方残差）分配并向后传播到输出和隐藏权重/偏差，以获得相对于每个权重/偏差的斜率。然后我们在`for`循环中使用这些斜率，分别通过乘以学习率`L`来微调权重/偏差，就像我们在第[5](ch05.xhtml#ch05)章和第[6](ch06.xhtml#ch06)章中所做的那样。我们进行一些矩阵-向量乘法，根据斜率向后传播误差，并在需要时转置矩阵和向量，以使行和列之间的维度匹配起来。'
- en: If you want to make the neural network a bit more interactive, here’s a snippet
    of code in [Example 7-12](#wjmhutDoNG) where we can type in different background
    colors (through an R, G, and B value) and see if it predicts a light or dark font.
    Append it to the bottom of the previous code [Example 7-11](#MfDTsSwbfB) and give
    it a try!
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想让神经网络更具交互性，这里有一段代码片段在[示例 7-12](#wjmhutDoNG)中，我们可以输入不同的背景颜色（通过R、G和B值），看看它是否预测为浅色或深色字体。将其附加到之前的代码[示例 7-11](#MfDTsSwbfB)的底部，然后试一试！
- en: Example 7-12\. Adding an interactive shell to our neural network
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-12\. 为我们的神经网络添加一个交互式shell
- en: '[PRE11]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Building your own neural network from scratch is a lot of work and math, but
    it gives you insight into their true nature. By working through the layers, the
    calculus, and the linear algebra, we get a stronger sense of what deep learning
    libraries like PyTorch and TensorFlow do behind the scenes.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始构建自己的神经网络需要大量的工作和数学知识，但它让你深入了解它们的真实本质。通过逐层工作、微积分和线性代数，我们对像PyTorch和TensorFlow这样的深度学习库在幕后做了什么有了更深刻的理解。
- en: As you have gathered from reading this entire chapter, there are a lot of moving
    parts to make a neural network tick. It can be helpful to put a breakpoint in
    different parts of the code to see what each matrix operation is doing. You can
    also port the code into a Jupyter Notebook to get more visual insight into each
    step.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 从阅读本整章节中你已经了解到，要使神经网络正常运转有很多要素。在代码的不同部分设置断点可以帮助你了解每个矩阵操作在做什么。你也可以将代码移植到 Jupyter
    Notebook 中，以获得更多对每个步骤的视觉洞察。
- en: 3Blue1Brown on Backpropagation
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3Blue1Brown 关于反向传播的视频
- en: 3Blue1Brown has some classic videos talking about [backpropagation](https://youtu.be/Ilg3gGewQ5U)
    and the [calculus behind neural networks](https://youtu.be/tIeHLnjs5U8).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 3Blue1Brown有一些经典视频讨论[反向传播](https://youtu.be/Ilg3gGewQ5U)和神经网络背后的[微积分](https://youtu.be/tIeHLnjs5U8)。
- en: Using scikit-learn
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 scikit-learn
- en: There is some limited neural network functionality in scikit-learn. If you are
    serious about deep learning, you will probably want to study PyTorch or TensorFlow
    and get a computer with a strong GPU (there’s a great excuse to get that gaming
    computer you always wanted!). I have been told that all the cool kids are using
    PyTorch now. However, scikit-learn does have some convenient models available,
    including the `MLPClassifier`, which stands for “multi-layer perceptron classifier.”
    This is a neural network designed for classification, and it uses a logistic output
    activation by default.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 中有一些有限的神经网络功能。如果你对深度学习感兴趣，你可能会想学习 PyTorch 或 TensorFlow，并购买一台配备强大
    GPU 的计算机（这是购买你一直想要的游戏电脑的绝佳借口！）。我被告知现在所有酷炫的孩子都在使用 PyTorch。然而，scikit-learn 中确实有一些方便的模型可用，包括`MLPClassifier`，它代表“多层感知器分类器”。这是一个用于分类的神经网络，默认使用逻辑输出激活。
- en: '[Example 7-13](#rchKuwCjUt) is a scikit-learn version of the background color
    classification application we developed. The `activation` argument specifies the
    hidden layer.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 7-13](#rchKuwCjUt)是我们开发的背景颜色分类应用的 scikit-learn 版本。`activation`参数指定了隐藏层。'
- en: Example 7-13\. Using scikit-learn neural network classifier
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-13\. 使用 scikit-learn 神经网络分类器
- en: '[PRE12]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Running this code I get about 99.3% accuracy on my testing data.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码，我在测试数据上获得了约 99.3% 的准确率。
- en: MNIST Example Using scikit-learn
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 的 MNIST 示例
- en: To see a scikit-learn example predicting handwritten digits using the MNIST
    dataset, turn to [Appendix A](app01.xhtml#appendix).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看一个使用 MNIST 数据集预测手写数字的 scikit-learn 示例，请参阅[附录 A](app01.xhtml#appendix)。
- en: Limitations of Neural Networks and Deep Learning
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络和深度学习的局限性
- en: 'For all of their strengths, neural networks struggle with certain types of
    tasks. This flexibility with layers, nodes, and activation functions makes it
    flexible fitting to data in a nonlinear manner…probably too flexible. Why? It
    can overfit to the data. Andrew Ng, a pioneer in deep learning education and the
    former head of Google Brain, mentioned this as a problem in a press conference
    in 2021\. Asked why machine learning has not replaced radiologists yet, this was
    his answer in an [*IEEE Spectrum* article](https://oreil.ly/ljXsz):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管神经网络具有许多优势，但在某些类型的任务上仍然存在困难。这种对层、节点和激活函数的灵活性使其能够以非线性方式拟合数据...可能太灵活了。为什么？它可能对数据过拟合。深度学习教育的先驱、谷歌大脑前负责人安德鲁·吴在
    2021 年的一次新闻发布会上提到这是一个问题。在被问及为什么机器学习尚未取代放射科医生时，这是他在[*IEEE Spectrum*文章](https://oreil.ly/ljXsz)中的回答：
- en: It turns out that when we collect data from Stanford Hospital, then we train
    and test on data from the same hospital, indeed, we can publish papers showing
    [the algorithms] are comparable to human radiologists in spotting certain conditions.
  id: totrans-178
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 结果表明，当我们从斯坦福医院收集数据，然后在同一家医院的数据上进行训练和测试时，确实可以发表论文，显示[算法]在发现某些病况方面与人类放射科医生相媲美。
- en: ''
  id: totrans-179
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It turns out [that when] you take that same model, that same AI system, to an
    older hospital down the street, with an older machine, and the technician uses
    a slightly different imaging protocol, that data drifts to cause the performance
    of AI system to degrade significantly. In contrast, any human radiologist can
    walk down the street to the older hospital and do just fine.
  id: totrans-180
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 结果表明，当你将同样的模型，同样的人工智能系统，带到街对面的一家老医院，使用一台老机器，技术人员使用稍有不同的成像协议时，数据漂移会导致人工智能系统的性能显著下降。相比之下，任何一名人类放射科医生都可以走到街对面的老医院并做得很好。
- en: ''
  id: totrans-181
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So even though at a moment in time, on a specific data set, we can show this
    works, the clinical reality is that these models still need a lot of work to reach
    production.
  id: totrans-182
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因此，即使在某个特定数据集上的某个时间点上，我们可以展示这是有效的，临床现实是这些模型仍然需要大量工作才能投入生产。
- en: In other words, the machine learning overfitted to the Stanford hospital training
    and testing dataset. When taken to other hospitals with different machinery, the
    performance degraded significantly due to the overfitting.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，机器学习过度拟合了斯坦福医院的训练和测试数据集。当应用到其他设备不同的医院时，由于过度拟合，性能显著下降。
- en: 'The same challenges occur with autonomous vehicles and self-driving cars. It
    is not enough to just train a neural network on one stop sign! It has to be trained
    on countless combinations of conditions around that stop sign: good weather, rainy
    weather, night and day, with graffiti, blocked by a tree, in different locales,
    and so on. In traffic scenarios, think of all the different types of vehicles,
    pedestrians, pedestrians dressed in costumes, and infinite number of edge cases
    that will be encountered! There is simply no effective way to capture every type
    of event that is encountered on the road just by having more weights and biases
    in a neural network.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶汽车和无人驾驶汽车也面临相同的挑战。仅仅在一个停车标志上训练神经网络是不够的！它必须在围绕该停车标志的无数条件下进行训练：晴天、雨天、夜晚和白天、有涂鸦、被树挡住、在不同的地点等等。在交通场景中，想象所有不同类型的车辆、行人、穿着服装的行人以及将遇到的无限数量的边缘情况！简单地说，通过在神经网络中增加更多的权重和偏差来捕捉在道路上遇到的每种事件是没有效果的。
- en: This is why autonomous vehicles themselves do not use neural networks in an
    end-to-end manner. Instead, different software and sensor modules are broken up
    where one module may use a neural network to draw a box around an object. Then
    another module will use a different neural network to classify the object in that
    box, such as a pedestrian. From there, traditional rule-based logic will attempt
    to predict the path of the pedestrian and hardcoded logic will choose from different
    conditions on how to react. The machine learning was limited to label-making activity,
    not the tactics and maneuvers of the vehicle. On top of that, basic sensors like
    radar will simply stop if an unknown object is detected in front of the vehicle,
    and this is just another piece of the technology stack that does not use machine
    learning or deep learning.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么自动驾驶汽车本身不会以端到端的方式使用神经网络。相反，不同的软件和传感器模块被分解，其中一个模块可能使用神经网络在物体周围画一个框。然后另一个模块将使用不同的神经网络对该框中的物体进行分类，比如行人。从那里，传统的基于规则的逻辑将尝试预测行人的路径，硬编码逻辑将从不同的条件中选择如何反应。机器学习仅限于标记制作活动，而不涉及车辆的战术和机动。此外，像雷达这样的基本传感器在车辆前方检测到未知物体时将会停止，这只是技术堆栈中另一个不使用机器学习或深度学习的部分。
- en: This might be surprising given all the media headlines about neural networks
    and deep learning [beating humans in games like Chess and Go](https://oreil.ly/9zFxM),
    or even besting [pilots in combat flight simulations](https://oreil.ly/hbdYI).
    It is important to remember in reinforcement learning environments like these
    that simulations are closed worlds, where infinite amounts of labeled data can
    be generated and learned through a virtual finite world. However, the real world
    is not a simulation where we can generate unlimited amounts of data. Also, this
    is not a philosophy book so we will pass on discussions whether we live in a simulation.
    Sorry, Elon! Collecting data in the real world is expensive and hard. On top of
    that, the real world is filled with infinite unpredictability and rare events.
    All these factors drive machine learning practitioners to [resort to data entry
    labor to label pictures of traffic objects](https://oreil.ly/mhjvz) and other
    data. Autonomous vehicle startups often have to pair this kind of data entry work
    with simulated data, because the miles and edge case scenarios needed to generate
    training data are too astronomical to gather simply by driving a fleet of vehicles
    millions of miles.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管媒体头条报道神经网络和深度学习在诸如国际象棋和围棋等游戏中**击败人类**，甚至胜过[战斗飞行模拟中的飞行员](https://oreil.ly/hbdYI)，这可能令人惊讶。在这样的强化学习环境中，需要记住模拟是封闭的世界，在这里可以生成无限量的标记数据，并通过虚拟有限世界进行学习。然而，现实世界并非我们可以生成无限量数据的模拟环境。此外，这不是一本哲学书，所以我们将不讨论我们是否生活在模拟中。抱歉，埃隆！在现实世界中收集数据是昂贵且困难的。除此之外，现实世界充满了无限的不可预测性和罕见事件。所有这些因素驱使机器学习从业者[转向数据录入劳动来标记交通物体的图片](https://oreil.ly/mhjvz)和其他数据。自动驾驶汽车初创公司通常必须将这种数据录入工作与模拟数据配对，因为需要生成训练数据的里程数和边缘情况场景太过庞大，无法简单地通过驾驶数百万英里的车队来收集。
- en: 'These are all reasons why AI research likes to use board games and video games,
    because unlimited labeled data can be generated easily and cleanly. Francis Chollet,
    a renowned engineer at Google who developed Keras for TensorFlow (and also wrote
    a great book, *Deep Learning with Python*), shared some insight on this in a [*Verge*
    article](https://oreil.ly/4PDLf):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是人工智能研究喜欢使用棋盘游戏和视频游戏的原因，因为可以轻松干净地生成无限标记数据。谷歌的著名工程师弗朗西斯·乔勒特（Francis Chollet）为
    TensorFlow 开发了 Keras（还写了一本很棒的书，*Python 深度学习*），在一篇[*Verge*文章](https://oreil.ly/4PDLf)中分享了一些见解：
- en: The thing is, once you pick a measure, you’re going to take whatever shortcut
    is available to game it. For instance, if you set chess-playing as your measure
    of intelligence (which we started doing in the 1970s until the 1990s), you’re
    going to end up with a system that plays chess, and that’s it. There’s no reason
    to assume it will be good for anything else at all. You end up with tree search
    and minimax, and that doesn’t teach you anything about human intelligence. Today,
    pursuing skill at video games like Dota or StarCraft as a proxy for general intelligence
    falls into the exact same intellectual trap…
  id: totrans-188
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题在于，一旦你选择了一个度量标准，你就会采取任何可用的捷径来操纵它。例如，如果你将下棋作为智能的度量标准（我们从上世纪70年代开始一直持续到90年代），你最终会得到一个只会下棋的系统，仅此而已。没有理由认为它对其他任何事情都有好处。你最终会得到树搜索和极小化，这并不能教会你任何关于人类智能的东西。如今，将追求在像
    Dota 或 StarCraft 这样的视频游戏中的技能作为一种普遍智能的替代品，陷入了完全相同的智力陷阱...
- en: ''
  id: totrans-189
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If I set out to “solve” Warcraft III at a superhuman level using deep learning,
    you can be quite sure that I will get there as long as I have access to sufficient
    engineering talent and computing power (which is on the order of tens of millions
    of dollars for a task like this). But once I’d have done it, what would I have
    learned about intelligence or generalization? Well, nothing. At best, I’d have
    developed engineering knowledge about scaling up deep learning. So I don’t really
    see it as scientific research because it doesn’t teach us anything we didn’t already
    know. It doesn’t answer any open question. If the question was, “Can we play X
    at a superhuman level?,” the answer is definitely, “Yes, as long as you can generate
    a sufficiently dense sample of training situations and feed them into a sufficiently
    expressive deep learning model.” We’ve known this for some time.
  id: totrans-190
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果我着手使用深度学习以超人水平“解决”《魔兽争霸III》，只要我有足够的工程人才和计算能力（这类任务需要数千万美元的资金），你可以确信我会成功。但一旦我做到了，我会学到关于智能或泛化的什么？嗯，什么也没有。充其量，我会开发关于扩展深度学习的工程知识。所以我并不认为这是科学研究，因为它并没有教会我们任何我们不已经知道的东西。它没有回答任何未解之谜。如果问题是，“我们能以超人水平玩
    X 吗？”，答案肯定是，“是的，只要你能生成足够密集的训练情境样本，并将它们输入到一个足够表达力的深度学习模型中。” 这一点我们已经知道有一段时间了。
- en: That is, we have to be careful to not conflate an algorithm’s performance in
    a game with broader capabilities that have yet to be solved. Machine learning,
    neural networks, and deep learning all work narrowly on defined problems. They
    cannot broadly reason or choose their own tasks, or ponder objects they have not
    seen before. Like any coded application, they do only what they were programmed
    to do.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们必须小心，不要混淆算法在游戏中的表现与尚未解决的更广泛能力。机器学习、神经网络和深度学习都是狭义地解决定义明确的问题。它们不能广泛推理或选择自己的任务，也不能思考之前未见过的对象。就像任何编码应用程序一样，它们只会执行它们被编程要执行的任务。
- en: With whatever tool it takes, solve the problem. There should be no partiality
    to neural networks or any other tool at your disposal. With all of this in mind,
    using a neural network might not be the best option for the task in front of you.
    It is important to always consider what problem you are striving to solve without
    making a specific tool your primary objective. The use of deep learning has to
    be strategic and warranted. There are certainly use cases, but in most of your
    everyday work you will likely have more success with simpler and more biased models
    like linear regression, logistic regression, or traditional rule-based systems.
    But if you find yourself having to classify objects in images, and you have the
    budget and labor to build that dataset, then deep learning is going to be your
    best bet.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 无论使用什么工具，解决问题是最重要的。不应该偏袒神经网络或其他任何可用工具。在这一点上，使用神经网络可能不是你面临的任务的最佳选择。重要的是要始终考虑你正在努力解决的问题，而不是把特定工具作为主要目标。深度学习的使用必须是策略性的和有充分理由的。当然有使用案例，但在你的日常工作中，你可能会更成功地使用简单和更偏向的模型，如线性回归、逻辑回归或传统的基于规则的系统。但如果你发现自己需要对图像中的对象进行分类，并且有预算和人力来构建数据集，那么深度学习将是你最好的选择。
- en: Conclusion
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Neural networks and deep learning offer some exciting applications, and we just
    scratched the surface in this one chapter. From recognizing images to processing
    natural language, there continue to be use cases for applying neural networks
    and their different flavors of deep learning.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络和深度学习提供了一些令人兴奋的应用，我们在这一章中只是触及了表面。从识别图像到处理自然语言，继续应用神经网络及其不同类型的深度学习仍然有用武之地。
- en: From scratch, we learned how to structure a simple neural network with one hidden
    layer to predict whether or not a light or dark font should be used against a
    background color. We also applied some advanced calculus concepts to calculate
    partial derivatives of nested functions and applied it to stochastic gradient
    descent to train our neural network. We also touched on libraries like scikit-learn.
    While we do not have the bandwidth in this book to talk about TensorFlow, PyTorch,
    and more advanced applications, there are great resources out there to expand
    your knowledge.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 从零开始，我们学习了如何构建一个具有一个隐藏层的简单神经网络，以预测在背景颜色下是否应该使用浅色或深色字体。我们还应用了一些高级微积分概念来计算嵌套函数的偏导数，并将其应用于随机梯度下降来训练我们的神经网络。我们还涉及到了像scikit-learn这样的库。虽然在本书中我们没有足够的篇幅来讨论TensorFlow、PyTorch和更高级的应用，但有很多优秀的资源可以扩展你的知识。
- en: '[3Blue1Brown has a fantastic playlist on neural networks and backpropagation](https://oreil.ly/VjwBr),
    and it is worth watching multiple times. [Josh Starmer’s StatQuest playlist on
    neural networks](https://oreil.ly/YWnF2) is helpful as well, particularly in visualizing
    neural networks as manifold manipulation. Another great video about manifold theory
    and neural networks can be [found here on the Art of the Problem](https://youtu.be/e5xKayCBOeU).
    Finally, when you are ready to deep-dive, check out Aurélien Géron’s *Hands-On
    Machine Learning with Scikit-Learn, Keras, and TensorFlow* (O’Reilly) and Francois
    Chollet’s *Deep Learning with Python* (Manning).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[3Blue1Brown有一个关于神经网络和反向传播的精彩播放列表](https://oreil.ly/VjwBr)，值得多次观看。[Josh Starmer的StatQuest播放列表关于神经网络](https://oreil.ly/YWnF2)也很有帮助，特别是在将神经网络可视化为流形操作方面。关于流形理论和神经网络的另一个优秀视频可以在[Art
    of the Problem这里找到](https://youtu.be/e5xKayCBOeU)。最后，当你准备深入研究时，可以查看Aurélien Géron的*使用Scikit-Learn、Keras和TensorFlow进行实践机器学习*（O’Reilly）和Francois
    Chollet的*Python深度学习*（Manning）。'
- en: If you made it to the end of this chapter and feel you absorbed everything within
    reason, congrats! You have not just effectively learned probability, statistics,
    calculus, and linear algebra but also applied it to practical applications like
    linear regression, logistic regression, and neural networks. We will talk about
    how you can proceed in the next chapter and start a new phase of your professional
    growth.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你读到了本章的结尾，并且觉得你合理地吸收了一切，恭喜！你不仅有效地学习了概率、统计学、微积分和线性代数，还将其应用于线性回归、逻辑回归和神经网络等实际应用。我们将在下一章讨论你如何继续前进，并开始你职业成长的新阶段。
- en: Exercise
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Apply a neural network to the employee retention data we worked with in [Chapter 6](ch06.xhtml#ch06).
    You can import the data [here](https://tinyurl.com/y6r7qjrp). Try to build this
    neural network so it predicts on this dataset and use accuracy and confusion matrices
    to evaluate performance. Is it a good model for this problem? Why or why not?
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 将神经网络应用于我们在[第6章](ch06.xhtml#ch06)中处理的员工留存数据。您可以在[这里](https://tinyurl.com/y6r7qjrp)导入数据。尝试构建这个神经网络，使其对这个数据集进行预测，并使用准确率和混淆矩阵来评估性能。这对于这个问题是一个好模型吗？为什么？
- en: While you are welcome to build the neural network from scratch, consider using
    scikit-learn, PyTorch, or another deep learning library to save time.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然欢迎您从头开始构建神经网络，但考虑使用scikit-learn、PyTorch或其他深度学习库以节省时间。
- en: Answers are in [Appendix B](app02.xhtml#exercise_answers).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 答案在[附录B](app02.xhtml#exercise_answers)中。
