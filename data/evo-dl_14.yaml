- en: 11 Evolutionary learning with NEAT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 使用 NEAT 进行进化学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Introducing to reinforcement learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习的介绍
- en: Exploring complex problems from the OpenAI Gym
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索 OpenAI Gym 中的复杂问题
- en: Using NEAT as an agent to solve reinforcement learning problems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 NEAT 作为代理解决强化学习问题
- en: Solving Gym’s lunar lander problem with a NEAT agent
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 NEAT 代理解决 Gym 的月球着陆器问题
- en: Solving Gym’s lunar lander problem with a deep Q-network
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度 Q 网络解决 Gym 的月球着陆器问题
- en: In the last chapter, we explored NeuroEvolution of Augmenting Topologies (NEAT)
    to solve common problems we explored in previous chapters. In this chapter, we
    look at the evolution of learning itself. First, we use NEAT to develop an evolving
    agent that can solve problems typically associated with RL. Then, we look at more
    difficult RL problems and provide a NEAT solution for evolutionary learning. Finally,
    we finish the chapter by looking at how our understanding of learning itself needs
    to evolve, using a mental model called *instinctual learning*.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了神经进化拓扑增强（NEAT）来解决我们在前几章中探讨的常见问题。在这一章中，我们研究学习的进化。首先，我们使用 NEAT 开发一个可以解决通常与强化学习相关问题的进化代理。然后，我们研究更困难的强化学习问题，并提供一个用于进化学习的
    NEAT 解决方案。最后，我们通过使用称为 *本能学习* 的心理模型来探讨我们对学习本身的理解需要如何进化。
- en: 11.1 Introducing reinforcement learning
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 强化学习的介绍
- en: '*Reinforcement* *learning* (RL) is a form of learning based on animal behavior
    and psychology that attempts to replicate how organisms learn through rewards.
    If you have ever trained a pet to do a trick using some form of reward, like a
    treat or praise, then you understand the premise. Many believe the basis for understanding
    high-level conscience and how we learn is modeled in RL.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化* *学习* (RL) 是一种基于动物行为和心理学的学习形式，试图复制生物体通过奖励来学习的方式。如果你曾经用某种形式的奖励，比如零食或表扬，来训练宠物做一个小把戏，那么你就理解了这个前提。许多人认为，理解高级意识和我们学习的基础是通过强化学习来建模的。'
- en: 'Figure 11.1a shows a comparison of three forms of learning covered in this
    book: supervised learning, representative learning (generative modeling), and
    RL. There are variations in all three forms of these learning types, from self-supervised
    learning to deep reinforcement learning.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1a 展示了本书中涵盖的三种学习形式的比较：监督学习、代表性学习（生成建模）和强化学习。这三种学习类型都有所变化，从自监督学习到深度强化学习。
- en: '![](../Images/CH11_F01a_Lanham.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH11_F01a_Lanham.png)'
- en: Figure 11.1a A comparison of different forms of learning
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1a 不同形式学习的比较
- en: RL works by having the learner, or what is referred to as the *agent*, observe
    the `state` of the environment. This `observation`, or view, of the environment
    is often referred to as the current `state`. The agent consumes the observed `state`
    and makes a prediction, or `action`, based on this `state`. Then, based on that
    `action`, the environment provides a reward based on the `action` at the given
    `state`.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习通过让学习者，或称为 *代理*，观察环境的 `state` 来工作。这种对环境的 `observation` 或 `view` 通常被称为当前
    `state`。代理消耗观察到的 `state` 并基于此 `state` 进行预测，或执行 `action`。然后，基于该 `action`，环境根据给定
    `state` 的 `action` 提供奖励。
- en: This process of observing the environment and the agent performing `actions`
    continues until the agent either solves the problem or fails. The agent or learner
    learns through the accumulation of rewards promoted by the environment, where
    an `action` that typically produces a higher reward for a given `state` is considered
    more valuable.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个观察环境和代理执行 `actions` 的过程会一直持续到代理解决问题或失败为止。代理或学习者通过环境促进的奖励积累来学习，其中对于给定 `state`
    通常产生更高奖励的 `action` 被认为更有价值。
- en: The learning process for an RL agent typically involves the agent starting out
    making random `actions`. Agents use random `actions` to probe the environment
    to find those `actions` or sequences of `actions` that produce the most rewards.
    This type of learning is referred to as *trial and error* or *brute force*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习代理的学习过程通常涉及代理开始时随机执行 `actions`。代理使用随机 `actions` 来探测环境，以找到那些产生最多奖励的 `actions`
    或 `actions` 序列。这种学习被称为 *尝试错误* 或 *蛮力*。
- en: Trial and error learning vs. executive function
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试错误学习与执行功能
- en: A key criticism of RL is the use of trial and error or brute force learning.
    This form of learning is repetitive and can be extremely expensive. Often, it
    requires an agent to execute millions of `actions` to solve problems. While humans
    and other animals often learn tasks in a similar fashion, we rarely need millions
    of iterations to get good at something.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）的一个关键批评是其使用试错法或蛮力学习。这种学习形式是重复的，并且可能极其昂贵。通常，它需要代理执行数百万次的`动作`来解决一个问题。虽然人类和其他动物往往以类似的方式学习任务，但我们很少需要数百万次迭代才能在某件事上变得擅长。
- en: '*Executive function* (EF) is the process of the brain’s learning mechanism
    that allows us to plan and complete complex tasks. While RL can simulate executive
    function in agents, the mechanism is substantially different. EF allows us to
    look at complex tasks we have never previously completed and plan a series of
    actions to successfully complete such tasks. There is ongoing research to uplift
    executive function into RL agents through a variety of techniques, including evolutionary
    optimization.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*执行功能*（EF）是大脑学习机制的过程，它使我们能够规划和完成复杂任务。虽然强化学习可以在代理中模拟执行功能，但其机制与RL有显著不同。EF使我们能够查看我们以前从未完成过的复杂任务，并规划一系列动作以成功完成这些任务。目前正在进行研究，通过各种技术，包括进化优化，将执行功能提升到RL代理中。'
- en: While RL is not without faults, it does provide a mechanism that allows us to
    solve complex problems we could never consider with supervised or adversarial
    learning. RL also allows the agent to interact and make changes to an environment,
    if allowed, causing even more complex problems. To understand how RL works, we
    look at and solve a first-order problem in the next section.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然强化学习并非没有缺陷，但它确实提供了一种机制，使我们能够解决我们从未考虑过使用监督学习或对抗学习的复杂问题。如果允许，RL还允许代理与环境交互并对其进行更改，从而产生更加复杂的问题。为了理解RL是如何工作的，我们将在下一节中查看并解决一个一阶问题。
- en: 11.1.1 Q-learning agent on the frozen lake
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.1 冻结湖上的Q学习代理
- en: 'Modern RL is a combination of three algorithmic paths: trial and error, dynamic
    programming, and Monte Carlo simulation. From that basis, a form of RL called
    *Q-learning* was derived in 1996 by Chris Watkins. Q-learning has since become
    a foundational concept to RL and is often taught to students as a first step.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现代强化学习是三种算法路径的结合：试错法、动态规划和蒙特卡洛模拟。在此基础上，1996年由Chris Watkins推导出一种称为*Q学习*的强化学习形式。自那时起，Q学习已成为RL的基础概念，通常作为第一步教授给学生。
- en: Q-learning works by giving the agent the ability to quantify the quality of
    a given `action` for a known `state`, as shown in figure 11.1b. By being able
    to measure the quality of a given `action`, an agent can thereby easily select
    the correct series of `actions` to solve a given problem. The agent still needs
    to fumble around trial and error, probing the environment to derive these `action`
    or `state` qualities.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习通过给代理提供量化已知`状态`下给定`动作`质量的能力来实现，如图 11.1b 所示。通过能够衡量给定`动作`的质量，代理可以轻松选择正确的动作序列来解决给定的问题。代理仍然需要通过试错法摸索，探索环境以推导出这些`动作`或`状态`的质量。
- en: '![](../Images/CH11_F01b_Lanham.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH11_F01b_Lanham.png)'
- en: Figure 11.1b Reinforcement learning
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1b 强化学习
- en: To see how this works in practice, we start by building a Q-learning agent that
    can solve a base problem from the OpenAI Gym, called the *frozen lake problem*.
    The OpenAI Gym ([https://www.gymlibrary.dev/](https://www.gymlibrary.dev/)) is
    an open source project that encapsulates hundreds of different problem environments.
    These environments range from classic Atari games to basic control problems.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解这在实践中是如何工作的，我们首先构建一个Q学习代理，它可以解决来自OpenAI Gym的基本问题，称为*冻结湖问题*。OpenAI Gym（[https://www.gymlibrary.dev/](https://www.gymlibrary.dev/））是一个开源项目，封装了数百个不同的问题环境。这些环境从经典的Atari游戏到基本的控制问题都有。
- en: Figure 11.2 shows a diagram of the frozen lake environment we develop for the
    Q-agent to solve. The environment is a 4-by-4 grid of squares representing a frozen
    lake, where areas of the lake are frozen solid and safe to navigate across. Other
    areas of the lake are unstable and have holes that will cause the agent to fall
    and perish, thus finishing their journey or episode.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 展示了我们为Q代理解决而开发的冻结湖环境的示意图。环境是一个由四个方格组成的4x4网格，代表一个冻结的湖，其中湖的某些区域被冻结得非常坚固且安全，可以穿越。湖的其他区域是不稳定的，有洞，会导致代理掉入并死亡，从而结束他们的旅程或阶段。
- en: '![](../Images/CH11_F02_Lanham.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH11_F02_Lanham.png)'
- en: Figure 11.2 Frozen lake environment
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 冻结湖环境
- en: The goal of the frozen lake problem is for the agent to move across the grid,
    using the `actions` `up`, `down`, `left`, or `right` to traverse. When the agent
    reaches the bottom-right corner, it is complete and rewarded. If the agent falls
    through a hole in the lake, the journey is over and the agent is given a negative
    reward.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 冰冻湖问题的目标是让智能体在网格中移动，使用 `actions` 中的 `up`、`down`、`left` 或 `right` 操作来穿越。当智能体到达右下角时，任务完成并获得奖励。如果智能体掉入湖中的洞里，旅程结束，智能体将获得负奖励。
- en: Fortunately, building RL agents and environments to test them on has been made
    easier through the development of the OpenAI Gym. We dive into loading the environment
    and coding a working Q agent in the next notebook.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，通过 OpenAI Gym 的发展，构建强化学习智能体及其测试环境变得更加容易。我们将在下一笔记本中深入探讨如何加载环境和编写一个工作的 Q
    智能体。
- en: Open the EDL_11_1_FrozenLake.ipynb notebook in Google Colab. Refer to the appendix
    if you need assistance. Run all the cells in the notebook by selecting Runtime
    > Run All from the menu. The first thing we look at in the following listing is
    the installation and imports of OpenAI Gym.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Colab 中打开 EDL_11_1_FrozenLake.ipynb 笔记本。如需帮助，请参阅附录。通过选择菜单中的“运行”>“运行所有”来运行笔记本中的所有单元格。在下面的列表中，我们首先关注的是
    OpenAI Gym 的安装和导入。
- en: 'Listing 11.1 EDL_11_1_FrozenLake.ipynb: Installing OpenAI Gym'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.1 EDL_11_1_FrozenLake.ipynb：安装 OpenAI Gym
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Installs basic Gym
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 安装基本 Gym
- en: ❷ Imports the package
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 导入包
- en: After that, we look at how Gym is used to create the environment. There are
    hundreds of environments to select from, and to create one, you just need to pass
    the name to the `gym.make` function, as shown in listing 11.2\. Then, we query
    the environment to yield the size of the `action` and `state` spaces; these numbers
    represent how many discrete values are available. The frozen lake environment
    uses discrete values for both the `action` and `state` spaces. In many environments,
    we use continuous or ranged values to represent either the `action`, `state`,
    or both spaces.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将查看如何使用 Gym 创建环境。有数百个环境可供选择，要创建一个环境，只需将名称传递给 `gym.make` 函数，如列表 11.2 所示。然后，我们查询环境以获取
    `action` 和 `state` 空间的大小；这些数字表示有多少个离散值可用。冰冻湖环境使用离散值来表示 `action` 和 `state` 空间。在许多环境中，我们使用连续或范围值来表示
    `action`、`state` 或两者空间。
- en: 'Listing 11.2 EDL_11_1_FrozenLake.ipynb: Creating the environment'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.2 EDL_11_1_FrozenLake.ipynb：创建环境
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Creates the environment
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建环境
- en: ❷ Gets the size of the action space
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取动作空间的大小
- en: ❸ Gets the size of the state space
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取状态空间的大小
- en: In Q-learning, the agent or learner encapsulates its knowledge or learnings
    in a table, conveniently called a *Q-table*. The dimensions of this table, its
    columns, and its rows are defined by the `state` and available `actions`. The
    next step in the code, shown in listing 11.3, is creating this table to represent
    the agent’s knowledge. We construct this table using `np.zeros` to create an array
    sized by `action_size` and `state_size` values. The result is an array (table)
    of values, where each row represents the `state` and each column on the row represents
    the quality of the `action` at that `state`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Q 学习中，智能体或学习者将其知识或学习封装在一个称为 *Q 表* 的表中。这个表的维度、列和行由 `state` 和可用的 `actions` 定义。代码中的下一步，如列表
    11.3 所示，是创建这个表来表示智能体的知识。我们使用 `np.zeros` 创建一个由 `action_size` 和 `state_size` 值大小的数组。结果是包含值的数组（表），其中每一行代表
    `state`，每一行上的列代表在该 `state` 上的 `action` 的质量。
- en: 'Listing 11.3 EDL_11_1_FrozenLake.ipynb: Building the Q-table'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.3 EDL_11_1_FrozenLake.ipynb：构建 Q 表
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Creates an array of values, zero
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个值为零的数组
- en: ❷ The first row and first state
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 第一行和第一个状态
- en: ❸ Four actions per row
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 每行四个动作
- en: 'Next, we look at a set of standard hyperparameters for a Q-learner, shown in
    listing 11.4\. Each journey the agent takes across the lake is defined as an episode.
    The `total_episodes` hyperparameter sets the total number of episodes or journeys
    the agent will take, and the `max_steps` value defines the maximum number of steps
    an agent can take on a single journey. Two other values are also used: `learning_rate`,
    which is like the learning rate in DL, and `gamma`, which is a `discount` factor
    that controls how important future rewards are to the agent. Finally, the bottom
    group of hyperparameters controls the agent’s exploration.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看一组Q学习者的标准超参数，如列表11.4所示。代理在湖上进行的每次旅行被定义为一场游戏。`total_episodes`超参数设置了代理将进行的总游戏数或旅行次数，而`max_steps`值定义了代理在单次旅行中可以采取的最大步数。还有两个其他值也被使用：`learning_rate`，类似于DL中的学习率，以及`gamma`，它是一个控制未来奖励对代理重要性的折扣因子。最后，底部的超参数组控制代理的探索。
- en: 'Listing 11.4 EDL_11_1_FrozenLake.ipynb: Defining hyperparameters'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.4 EDL_11_1_FrozenLake.ipynb：定义超参数
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ The total number of training attempts
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练尝试的总数
- en: ❷ How quickly the agent learns
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 代理学习速度有多快
- en: ❸ The maximum number of steps in an episode
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 一场游戏中的最大步数
- en: ❹ The discount rate for future rewards
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 未来奖励的折扣率
- en: ❺ Controls agent exploration
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 控制代理探索
- en: A fundamental concept in Q-learning is the tradeoff between exploration and
    exploitation, or using the knowledge the agent has gained. When an agent initially
    starts training, its knowledge is low, represented by all zeroes in the Q-table.
    With no knowledge, the agent often relies on a random selection of `actions`.
    Then, as knowledge increases, the agent can start using values in the Q-table
    to determine the next best `action`. Unfortunately, if an agent’s knowledge is
    incomplete, always choosing the best `action` may lead to disaster. Instead, we
    introduce a hyperparameter called `epsilon` to control how frequently an agent
    explores.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习中的一个基本概念是探索与利用之间的权衡，或者使用代理获得的知识。当代理最初开始训练时，其知识较低，在Q表中表示为所有零。在没有知识的情况下，代理通常会依赖于随机选择的`动作`。然后，随着知识的增加，代理可以开始使用Q表中的值来确定下一个最佳`动作`。不幸的是，如果代理的知识不完整，总是选择最佳`动作`可能会导致灾难。因此，我们引入了一个名为`epsilon`的超参数来控制代理探索的频率。
- en: We can see how this exploration and exploitation works by looking at the `choose_action`
    function, shown in listing 11.5\. In this function, a random uniform value is
    generated and compared against `epsilon`. If the value is less than `epsilon`,
    the agent randomly selects an `action` from the `action` space and returns it.
    Otherwise, the agent picks the maximum quality `action` for the current `state`
    and returns that. As the agent trains over the environment, the `epsilon` value
    will be reduced or decayed over time to represent the agent’s accumulation of
    knowledge and inclination to explore less.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看列表11.5中显示的`choose_action`函数来了解这种探索和利用是如何工作的。在这个函数中，生成一个随机均匀值并与`epsilon`进行比较。如果值小于`epsilon`，代理将从`action`空间中随机选择一个`动作`并返回它。否则，代理将选择当前`状态`的最大质量`动作`并返回。随着代理在环境中训练，`epsilon`值将随着时间的推移减少或衰减，以表示代理知识的积累和减少探索的倾向。
- en: 'Listing 11.5 EDL_11_1_FrozenLake.ipynb: Choosing an `action`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.5 EDL_11_1_FrozenLake.ipynb：选择 `动作`
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ The chance of random action, explore
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 随机动作的概率，探索
- en: ❷ Chooses the maximum action for a given state
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为给定状态选择最大动作
- en: ❸ Randomly samples an action
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 随机采样一个动作
- en: The agent learns through an accumulation of knowledge calculated by the Q-function.
    Terms within the Q-function represent the current Q-quality values, reward, and
    application of the `discount` factor gamma. This method of learning is encapsulated
    in the `learn` function, which applies the Q-learning function, shown in the following
    listing. We don’t cover this function in greater depth here because our focus
    is on how NEAT can solve the same problem without using the Q-function.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 代理通过Q函数计算的知识积累来学习。Q函数中的术语代表当前的Q-质量值、奖励以及应用折扣因子gamma。这种学习方法封装在`learn`函数中，该函数应用了以下列表中显示的Q学习函数。我们在这里不深入探讨这个函数，因为我们的重点是NEAT如何在不使用Q函数的情况下解决相同的问题。
- en: 'Listing 11.6 EDL_11_1_FrozenLake.ipynb: The `learn` function'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.6 EDL_11_1_FrozenLake.ipynb：`learn`函数
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Calculates the quality given the state/action
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 根据状态/动作计算质量
- en: The code to train the agent is split into two loops, with the first looping
    over the episodes and the second looping over the journey or steps through each
    episode. At each step, the agent uses the `choose_action` function to select the
    next `action` and then executes the `action` by calling `env.step(action)`. The
    output from calling the `step` function is used to update the agent’s knowledge
    in the Q-table by calling the `learn` function. Then, a check is made to confirm
    whether the episode is complete or incomplete and whether the agent fell into
    a hole or made it to the end. As the agent loops over the episodes, the `epsilon`
    value is decayed or reduced, representing the agent’s reduced need to explore
    as time goes on, as shown in the following listing.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 训练智能体的代码分为两个循环，第一个循环遍历回合，第二个循环遍历每个回合的旅程或步骤。在每一步中，智能体使用`choose_action`函数选择下一个`action`，然后通过调用`env.step(action)`来执行`action`。调用`step`函数的输出用于通过调用`learn`函数更新智能体的知识在Q表中。然后，检查确认回合是否完成或不完整，以及智能体是否掉入洞中或到达了终点。随着智能体遍历回合，`epsilon`值会衰减或减少，这代表着随着时间的推移，智能体探索的需求减少，如下所示。
- en: 'Listing 11.7 EDL_11_1_FrozenLake.ipynb: The training function'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.7 EDL_11_1_FrozenLake.ipynb：训练函数
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Resets the environment
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 重置环境
- en: ❷ Selects an action and executes on the environment
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 选择动作并在环境中执行
- en: ❸ Learns and updates the Q-table
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 学习并更新Q表
- en: ❹ If it is done and the episode is complete, then it breaks
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 如果完成并且回合结束，则中断
- en: ❺ Decays epsilon exploration over time
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 随时间衰减epsilon探索
- en: In this example, we train the agent to run over a certain number of episodes,
    without consideration for improved performance. After the agent has been trained,
    we test out its knowledge by running the agent through a simulated episode over
    the environment, as shown in the following listing.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们训练智能体在一定的次数中运行，而不考虑性能的改进。在智能体训练完成后，我们通过在环境中运行模拟的智能体来测试其知识，如下所示。
- en: 'Listing 11.8 EDL_11_1_FrozenLake.ipynb: Exercising the agent'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.8 EDL_11_1_FrozenLake.ipynb：训练智能体
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Loops over 5 episodes
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历5个回合
- en: ❷ Takes/executes the maximum best action for state
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 执行状态的最大最佳动作
- en: ❸ Renders the environment
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 渲染环境
- en: Figure 11.3 shows the output of running a trained agent over five episodes.
    From the results, you can see how the agent is able to generally solve the environment
    within the max allowed steps (99). If you want to go back and try to improve how
    quickly the agent solves the environment consistently, try modifying the hyperparameters
    and then running the notebook again. The next section showcases some learning
    exercises that can help improve your knowledge of RL.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3显示了运行训练智能体五个回合的输出。从结果中，你可以看到智能体是如何在最大允许步骤（99步）内一般解决环境的。如果你想尝试改进智能体解决环境的一致性速度，尝试修改超参数，然后再次运行笔记本。下一节将展示一些有助于提高你对强化学习（RL）知识的练习。
- en: '![](../Images/CH11_F03_Lanham.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH11_F03_Lanham.png)'
- en: Figure 11.3 The output from simulating an agent on the frozen lake
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3 模拟智能体在冰面上的输出
- en: 11.1.2 Learning exercises
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.2 学习练习
- en: 'Use these exercises to improve your knowledge of the subject material:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些练习来提高你对主题材料的了解：
- en: Change the `learning_rate` and `gamma` hyperparameters from listing 11.4\. Observe
    what effect they have on agent learning.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从列表11.4中更改`learning_rate`和`gamma`超参数。观察它们对智能体学习的影响。
- en: Alter the exploration decay rate, `decay_rate`, from listing 11.4\. Observe
    what effect this has on agent training.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从列表11.4中更改探索衰减率`decay_rate`。观察这对智能体训练的影响。
- en: Alter the number of training `EPISODES`. Observe what effect this has on agent
    performance.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改训练的`EPISODES`数量。观察这对智能体性能的影响。
- en: Of course, at this stage, we could write an evolutionary optimizer to tune the
    hyperparameters, as we have done before. However, with NEAT, we can do much better
    and, in fact, replace the use of RL to solve these types of problems. Before we
    get to that, though, we look at loading more complex OpenAI Gym environments in
    the next section.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在这个阶段，我们可以编写一个进化优化器来调整超参数，就像我们之前所做的那样。然而，使用NEAT，我们可以做得更好，实际上可以替代使用强化学习（RL）来解决这类问题。不过，在我们达到这一点之前，我们将在下一节中查看如何加载更复杂的OpenAI
    Gym环境。
- en: 11.2 Exploring complex problems from the OpenAI Gym
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 探索OpenAI Gym中的复杂问题
- en: 'The OpenAI Gym provides an enormous collection of training environments designed
    for improving RL. Before we unleash NEAT on some of those environments, we have
    some additional prep to undertake to use Colab. In this chapter and the next,
    we explore various Gym environments, described in the following list:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym 提供了大量的训练环境，旨在提高强化学习（RL）。在我们将 NEAT 应用于这些环境之前，我们需要做一些额外的准备工作来使用 Colab。在本章和下一章中，我们将探索以下列表中描述的各种
    Gym 环境：
- en: '*Mountain car*—The goal here is to move a car that starts in the valley of
    two hills to the top of the target hill. To do this, the car needs to rock itself
    back and forth until it gets the momentum to climb to the top of the higher hill.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*山车*——这里的目的是将一辆从两个山丘的谷地开始的汽车开到目标山丘的顶部。为此，汽车需要来回摇晃，直到获得足够的动力爬到更高的山丘顶部。'
- en: '*Pendulum*—The goal of this problem is to apply force to the rocking arm of
    a pendulum, so it remains in an upright position. This requires knowledge of when
    to apply force, depending on the position of the pendulum.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*摆锤*——这个问题的目标是施加力量于摆动的摆臂，使其保持直立位置。这需要根据摆锤的位置知道何时施加力量。'
- en: '*Cart pole*—This classic Gym problem requires the balancing of a pole on top
    of a moving cart. Again, this requires the agent/model to balance the position
    and velocity of the cart and pole.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*小车和杆*——这个经典的 Gym 问题要求在移动的货车上平衡一个杆。同样，这需要代理/模型平衡货车的位置和速度。'
- en: '*Lunar lander*—Replicated from an old video game, the goal of lunar lander
    is to land the moon lander on a flat landing surface. The trick is that the landing
    velocity of the craft must be low enough to avoid damaging the lander and failing.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*月球着陆器*——从旧视频游戏中复制而来，月球着陆器的目标是将月球着陆器平稳地降落在平坦的着陆面上。关键是飞行器的着陆速度必须足够低，以避免损坏着陆器并失败。'
- en: In the next quick notebook, we look to set up and explore various Gym environments
    from the above list.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一本快速笔记中，我们将设置并探索上述列表中的各种 Gym 环境。
- en: Open the EDL_11_2_Gym_Setup.ipynb notebook in Google Colab. Refer to the appendix
    if you need assistance. Run all the cells in the notebook by selecting Runtime
    > Run All from the menu.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Colab 中打开 EDL_11_2_Gym_Setup.ipynb 笔记本。如需帮助，请参阅附录。通过选择菜单中的“运行”>“运行所有”来运行笔记本中的所有单元格。
- en: Since Colab is a server notebook, there typically isn’t a need to provide UI
    output. To render some of the prettier Gym environments, we must install some
    virtual interface drivers and associated helpers, as shown in the following listing.
    We also install some tools to render our environment outputs and playback as video,
    which makes our experimentation more entertaining.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Colab 是服务器端笔记本，通常不需要提供 UI 输出。为了渲染一些更美观的 Gym 环境，我们必须安装一些虚拟界面驱动程序和相关辅助工具，如下面的列表所示。我们还安装了一些工具来渲染我们的环境输出并回放为视频，这使得我们的实验更加有趣。
- en: 'Listing 11.9 EDL_11_2_Gym_Setup.ipynb: Installing required packages'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.9 EDL_11_2_Gym_Setup.ipynb：安装所需软件包
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Installs video device drivers
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 安装视频设备驱动程序
- en: ❷ Installs Gym with box2d
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 安装带有 box2d 的 Gym
- en: ❸ Graphics helpers
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 图形辅助工具
- en: ❹ For playing video/media
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 用于播放视频/媒体
- en: ❺ The template engine
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 模板引擎
- en: We need to create a virtual display and start it. The code to that requires
    just a few simple lines, as shown in the following listing.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建一个虚拟显示并启动它。实现这一点的代码只需要几行简单代码，如下面的列表所示。
- en: 'Listing 11.10 EDL_11_2_Gym_Setup.ipynb: Creating a virtual display'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.10 EDL_11_2_Gym_Setup.ipynb：创建虚拟显示
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Creates a virtual display
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建虚拟显示
- en: ❷ Starts the display
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 开始显示
- en: After the imports, we can now create an environment and render a frame to the
    output of the cell. This cell uses a Colab form to provide a list of environment
    options to choose from. Our goal is to be able to build NEAT agents/solutions
    that can tackle each of the environments. Visualizing the environments themselves
    is possible using the `env .render` function and passing in the mode as `rgb_array`
    for outputting a 2D array. This output can then be rendered using `plt.imshow`,
    as shown in the following listing.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 导入之后，我们现在可以创建一个环境并将一个帧渲染到单元格的输出中。这个单元格使用 Colab 表单提供一系列环境选项供选择。我们的目标是能够构建 NEAT
    代理/解决方案，以应对每个环境。使用 `env.render` 函数并传入模式为 `rgb_array` 以输出 2D 数组，可以可视化环境本身。然后，可以使用
    `plt.imshow` 将此输出渲染，如下面的列表所示。
- en: 'Listing 11.11 EDL_11_2_Gym_Setup.ipynb: Creating a virtual display'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.11 EDL_11_2_Gym_Setup.ipynb：创建虚拟显示
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ A list of environment names
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 环境名称列表
- en: ❷ Creates the environment
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建环境
- en: ❸ Renders a frame and plots it
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 渲染一个帧并绘制它
- en: ❹ Prints action/state spaces
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 打印动作/状态空间
- en: Rendering a single frame is OK, but what we really want is to see how the environment
    runs or plays through. The next cell we look at, shown in listing 11.12, does
    just that by creating an environment and then running an agent through the environment.
    As the simulation runs, the environment renders each frame to a list. That list
    of frames is then converted to a video and output below the cell. Figure 11.4
    shows the output of the lunar lander environment rendered to video output in the
    Colab notebook.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 渲染单个帧是可以的，但我们真正想看到的是环境如何运行或播放。我们接下来要查看的下一个单元，如列表11.12所示，通过创建一个环境和然后让智能体通过环境来运行，正是这样做的。随着模拟的运行，环境将每一帧渲染到一个列表中。然后，这个帧列表被转换成视频并输出在单元下方。图11.4显示了在Colab笔记本中渲染到视频输出的月球着陆环境。
- en: 'Listing 11.12 EDL_11_2_Gym_Setup.ipynb: Video render of the environment'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.12 EDL_11_2_Gym_Setup.ipynb：环境的视频渲染
- en: '[PRE11]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Runs for n number simulations
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 运行n次模拟
- en: ❷ For the maximum number steps in each run
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 每次运行中的最大步数
- en: ❸ Takes the maximum action and executes
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 执行最大动作
- en: ❹ Appends the rendered frame to the list
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将渲染的帧追加到列表中
- en: ❺ If done, this stops the simulation run.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 如果完成，这将停止模拟运行。
- en: ❻ Renders the collection of frames as video
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将帧集合渲染为视频
- en: '![](../Images/CH11_F04_Lanham.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F04_Lanham.png)'
- en: Figure 11.4 Video output of the lunar lander environment
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4月球着陆环境的视频输出
- en: Go ahead and run through the various other environment options to visualize
    the other possibilities we explore. All these environments are similar in that
    the `state` space is continuous and the `action` space is discrete. We measure
    the complexity of an environment by the number of possible `states`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试运行各种其他环境选项，以可视化我们探索的其他可能性。所有这些环境在`state`空间是连续的，而`action`空间是离散的方面是相似的。我们通过可能的状态数量来衡量环境的复杂性。
- en: 'Figure 11.5 shows each of the environments, the size of the `action` and `state`
    spaces, and the relative complexity. The relative complexity of each environment
    is calculated by taking the size of the `state` space and raising it to the power
    of the `action` space, given by the following formula:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5显示了每个环境，`action`和`state`空间的大小以及相对复杂性。每个环境的相对复杂性是通过将`state`空间的大小提高到`action`空间的幂来计算的，公式如下：
- en: Relative complexity = `size_state_spacesize_action_space`
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 相对复杂性 = `size_state_space` × `size_action_space`
- en: 'Take, for example, a version of the mountain car problem, where `state_space`
    = 2 and `action_space` = 3\. The relative complexity, then, would be expressed
    as the following: relative complexity = 2³ = 2 × 2 × 2 = 8.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 以山车问题的一个版本为例，其中`state_space` = 2 和 `action_space` = 3。因此，相对复杂性可以表示为以下公式：相对复杂性
    = 2³ = 2 × 2 × 2 = 8。
- en: '![](../Images/CH11_F05_Lanham.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F05_Lanham.png)'
- en: Figure 11.5 A comparison of Gym environments
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5 Gym环境的比较
- en: Typically, the subclass of Box2D environments shown in figure 11.5 is solved
    using deep reinforcement learning (DRL). DRL is an extension of RL that uses DL
    to solve the Q-equation or other RL functions. Essentially, DL replaces the need
    for a `state` or Q-table with neural networks.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，图11.5中显示的Box2D环境的子类使用深度强化学习（DRL）来解决。DRL是RL的扩展，它使用深度学习来解决Q方程或其他RL函数。本质上，深度学习取代了`state`或Q表的需求，使用神经网络。
- en: '![](../Images/CH11_F06_Lanham.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F06_Lanham.png)'
- en: Figure 11.6 Q-learning and deep Q-learning compared
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6 Q学习和深度Q学习比较
- en: Figure 11.6 shows a comparison between Q-learning and deep Q-learning or deep
    Q-network (DQN). This DQN model has proven to be very versatile and capable of
    solving a wide variety of RL problems, from classic Atari games to the cart pole
    and lunar lander problems.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6显示了Q学习和深度Q学习或深度Q网络（DQN）之间的比较。这个DQN模型已被证明非常灵活，能够解决各种RL问题，从经典的Atari游戏到小车和月球着陆问题。
- en: Deep Q-learning works by using the Q-learning function we looked at earlier
    as the check or supervisor for training a network. That means, internally, a DQN
    model learns through supervised training, which is provided in the form of the
    Q-learning equation. The learning exercises in the next section can help reinforce
    what you have learned in this section.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 深度Q学习通过使用我们之前查看的Q学习函数作为训练网络的检查或监督器来工作。这意味着，在内部，DQN模型通过监督训练来学习，这种训练以Q学习方程的形式提供。下一节的学习练习可以帮助加强你在本节中学到的内容。
- en: 11.2.1 Learning exercises
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.1 学习练习
- en: 'Complete the following exercises to improve your understanding of this section’s
    concepts:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 完成以下练习，以加深对本节概念的理解：
- en: Open and run all the simulation environments provided in the notebook. Get familiar
    with each environment’s `action` and `observation`/`state` spaces as well.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开并运行笔记本中提供的所有模拟环境。同时熟悉每个环境的`action`和`observation`/`state`空间。
- en: Search the internet for and explore other Gym environments that were either
    part of the original or have been extended by others. There are hundreds of Gym
    environments you can explore.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在互联网上搜索并探索其他Gym环境，这些环境可能是原始的，也可能是其他人扩展的。有数百个Gym环境可以探索。
- en: Add a new environment to the notebook and demonstrate how an agent can play
    randomly within this new environment.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中添加一个新的环境，并演示智能体如何在这个新环境中随机玩耍。
- en: Since the development of DQN, there have been many variations and approaches
    to employing RL with DL networks. In all cases, the basis for the learning is
    RL in the form of a Q or other derived learning equation. In the next section,
    we demonstrate how we can raise ourselves above derived RL equations and let the
    solution evolve itself.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 自从DQN（深度Q网络）的发展以来，已经出现了许多变体和采用深度学习网络结合强化学习的方法。在所有情况下，学习的基础都是Q或其他派生学习方程形式的强化学习。在下一节中，我们将展示如何超越派生的强化学习方程，让解决方案自我演化。
- en: 11.3 Solving reinforcement learning problems with NEAT
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 使用NEAT解决强化学习问题
- en: In this section, we use NEAT to solve some of the difficult RL Gym problems
    we just looked at. However, it is important to stress that the method we use to
    derive the network and solve an unknown equation is not RL but, instead, evolution
    and NEAT. While we do use the RL environments and train the agent in an RL manner,
    the underlying method is not RL.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用NEAT（神经进化算法）来解决我们刚刚查看的一些困难的强化学习Gym问题。然而，重要的是要强调，我们用来派生网络和解决未知方程的方法不是强化学习，而是进化以及NEAT。虽然我们确实使用了强化学习环境和以强化学习的方式训练智能体，但底层方法不是强化学习。
- en: Using NEAT and an evolving `population` of NEAT agents is relatively simple
    to set up, as we see in the next notebook. Open the EDL_11_3_NEAT_Gym.ipynb notebook
    in Google Colab. Refer to the appendix if you need assistance. Run all the cells
    in the notebook by selecting Runtime > Run All from the menu.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用NEAT和一个演化的NEAT智能体群体相对简单，正如我们在下一个笔记本中看到的那样。在Google Colab中打开EDL_11_3_NEAT_Gym.ipynb笔记本。如有需要，请参考附录。通过选择菜单中的“Runtime
    > Run All”来运行笔记本中的所有单元格。
- en: We just reviewed the setup code, so we can jump straight to the NEAT configuration.
    The configuration is similar to what we have seen before, but now we define the
    network `num_inputs` as equal to the `state` or `observation` space size and the
    `num_ outputs` as equal to the size of the `action` space. This means the input
    into the NEAT agent is the `state`/`observation` and the output is the `action`,
    as shown in the following listing.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚回顾了设置代码，因此我们可以直接跳到NEAT配置。配置与之前我们看到的是相似的，但现在我们定义网络`num_inputs`等于`state`或`observation`空间的大小，而`num_outputs`等于`action`空间的大小。这意味着NEAT智能体的输入是`state`/`observation`，输出是`action`，如下所示。
- en: 'Listing 11.13 EDL_11_3_NEAT_Gyms.ipynb: NEAT configuration'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.13 EDL_11_3_NEAT_Gyms.ipynb：NEAT配置
- en: '[PRE12]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ The size of the state space
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 状态空间的大小
- en: ❷ The size of the action space
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 动作空间的大小
- en: ❸ Defines the fitness threshold
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义适应度阈值
- en: ❹ Maps the state space to inputs
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将状态空间映射到输入
- en: ❺ Maps the action space to outputs
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将动作空间映射到输出
- en: Next, we revisit our test `genome` `fred` to understand how `individual` `fitness`
    is `evaluated`. We can see how `fred` is created from the `genome` configuration
    and then instantiated into a network `net`. This network is tested by passing
    in an arbitrary environment `state` and outputting an `action` set. To execute
    the `action`, the `np.argmax(action)` is used to extract the `action` index to
    use when calling `env.step`, as shown in the following listing.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们回顾我们的测试`genome` `fred`，以了解如何评估`individual` `fitness`。我们可以看到`fred`是如何从`genome`配置中创建的，然后实例化为网络`net`。这个网络通过传递一个任意的环境`state`并输出一个`action`集来测试。为了执行`action`，使用`np.argmax(action)`来提取用于调用`env.step`的`action`索引，如下所示。
- en: 'Listing 11.14 EDL_11_3_NEAT_Gyms.ipynb: The `genome` and `ENVIRONMENT`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.14 EDL_11_3_NEAT_Gyms.ipynb：`genome`和`ENVIRONMENT`
- en: '[PRE13]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Creates the environment
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建环境
- en: ❷ Configures the initial random genome
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 配置初始随机基因组
- en: ❸ Builds a network from the genome
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从基因组构建网络
- en: ❹ Inputs the state and output action
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 输入状态并输出动作
- en: ❺ Executes the action
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 执行动作
- en: As before, we can use `fred` to derive the base `genome` `evaluate` function.
    The code, shown in listing 11.15, mimics the sample video demonstration playthrough
    code we already set up. This time, though, the `fitness` of the `genome` is calculated
    based on the accumulation of rewards. That means—and this subtle difference is
    important—the `fitness` of a `genome` is the sum of rewards, but at no time does
    the agent train/learn how to consume or use those rewards. Evolution uses the
    rewards to `evaluate` the agent with the best `fitness`—the one that can accumulate
    the most rewards.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，我们可以使用 `fred` 来推导基础 `genome` `evaluate` 函数。代码，如列表 11.15 所示，模仿了我们已经设置的样本视频演示播放代码。不过，这次
    `genome` 的 `fitness` 是基于奖励的累积来计算的。这意味着——这个微妙的不同很重要——`genome` 的 `fitness` 是奖励的总和，但代理在任何时候都没有训练/学习如何消耗或使用这些奖励。进化使用奖励来
    `evaluate` 具有最佳 `fitness` 的代理——即能够累积最多奖励的代理。
- en: 'Listing 11.15 EDL_11_3_NEAT_Gyms.ipynb: Evaluating `genome` `fitness`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.15 EDL_11_3_NEAT_Gyms.ipynb：评估 `genome` `fitness`
- en: '[PRE14]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ The colab form for simulation parameters
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 模拟参数的 Colab 表单
- en: ❷ Passes the state to the network to activate the action
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将状态传递给网络以激活动作
- en: ❸ Executes the step on the environment
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在环境中执行步骤
- en: ❹ Adds the reward to the fitness
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将奖励添加到 `fitness`
- en: ❺ Replays the simulation runs
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 重新播放模拟运行
- en: This simple code can easily be converted into a set of `eval_genomes`/`eval_genome`
    functions, where `eval_genomes` is the parent function passing the `population`
    of `genomes` and `individual` `genome` evaluation is done with `eval_genome`.
    Internally, the code shown in listing 11.16 works the same as the code we looked
    at in listing 11.15, without the video frame capture code. After all, we don’t
    need to capture a video of every `genome` simulation.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这段简单的代码可以轻松地转换为 `eval_genomes`/`eval_genome` 函数集，其中 `eval_genomes` 是父函数，传递 `genomes`
    的 `population`，而 `individual` `genome` 的评估是通过 `eval_genome` 来完成的。内部，列表 11.16 中显示的代码与列表
    11.15 中我们查看的代码相同，没有视频帧捕获代码。毕竟，我们不需要为每个 `genome` 模拟捕获视频。
- en: 'Listing 11.16 EDL_11_3_NEAT_Gyms.ipynb: Evaluating `genome` `fitness`'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.16 EDL_11_3_NEAT_Gyms.ipynb：评估 `genome` `fitness`
- en: '[PRE15]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Creates the network from the genome
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从 `genome` 创建网络
- en: ❷ Passes the state to the network to activate the action
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将状态传递给网络以激活动作
- en: ❸ Returns the minimum fitness
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回最小 `fitness`
- en: ❹ Loops through the genome population
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 遍历 `genome` 种群
- en: ❺ Tests the function on fred
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在 fred 上测试函数
- en: Now, the code to evolve the `population` becomes quite simple and elegant. The
    `pop` is created and then the default `statistics` and `out` reporters are added
    for generating progress updates. After that, we use a new feature called the `neat.ParallelEvaluator`
    to provide internal multithreaded processing of the evolution. On the free version
    of Colab, this function has limited use, but if you have a powerful computer,
    you could try running this code locally for improved performance. Finally, the
    last line calls `pop.run` to run the evolution and produce a winning `genome`,
    as shown in the following listing. Figure 11.7 shows the output evolving a `population`
    of NEAT agents to solve the cart pole Gym environment.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，进化 `population` 的代码变得非常简单和优雅。创建 `pop` 并添加默认的 `statistics` 和 `out` 报告器以生成进度更新。之后，我们使用一个名为
    `neat.ParallelEvaluator` 的新功能来提供进化的内部多线程处理。在 Colab 的免费版本中，此功能的使用有限，但如果你有一台功能强大的计算机，可以尝试在本地运行此代码以获得更好的性能。最后，最后一行调用
    `pop.run` 来运行进化并产生一个获胜的 `genome`，如下所示。图 11.7 显示了输出进化的 NEAT 代理 `population` 以解决
    cart pole Gym 环境。
- en: '![](../Images/CH11_F07_Lanham.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH11_F07_Lanham.png)'
- en: Figure 11.7 The output of a NEAT agent
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 NEAT 代理的输出
- en: 'Listing 11.17 EDL_11_3_NEAT_Gyms.ipynb: Evolving the `population`'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.17 EDL_11_3_NEAT_Gyms.ipynb：进化 `population`
- en: '[PRE16]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Creates the population
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建种群
- en: ❷ Uses the standard stats reporter
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用标准的统计报告器
- en: ❸ Adds the standard out reporter
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 添加标准输出报告器
- en: ❹ Uses the parallel execution
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用并行执行
- en: ❺ Evaluates the best genome
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 评估最佳 `genome`
- en: While the `fitness` of an agent is correlated with the agent’s maximum rewards,
    it is important to stress that we are not training RL agents. Instead, the evolved
    networks are evolving their own internal function to accumulate rewards and become
    more fit. The fact that `fitness` is related to rewards is only a useful metric
    to describe `individual` performance.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `fitness` 与代理的最大奖励相关，但重要的是要强调我们并没有训练 RL 代理。相反，进化的网络正在进化它们自己的内部功能以累积奖励并变得更加适合。`fitness`
    与奖励相关的事实只是一个有用的指标，用于描述 `individual` 的性能。
- en: 11.3.1 Learning exercises
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.1 学习练习
- en: 'Use the following additional exercises to help firm your understanding of the
    content of this section:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下附加练习来帮助巩固你对本节内容的理解：
- en: Try each of the other environments using the NEAT agent to see how well or quickly
    a solution can be achieved, if at all.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 NEAT 代理尝试其他环境，看看是否能够以及如何快速地实现解决方案，如果可能的话。
- en: Alter the number of `SIMULATION_RUNS` and `SIMULATION_ITERATIONS` and then reevaluate
    the NEAT agents.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改 `SIMULATION_RUNS` 和 `SIMULATION_ITERATIONS` 的数量，然后重新评估 NEAT 代理。
- en: Alter the number of hidden neurons in the NEAT configuration, `num_hidden`,
    from listing 11.13\. See what effect this has when rerunning the various environments.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改 NEAT 配置中的隐藏神经元数量 `num_hidden`，如列表 11.13 所示。查看重新运行各种环境时这一变化的影响。
- en: So what type of learning or evolution is this called or described by? Well,
    we talk about one set of ideas and theories in the next chapter. For now, though,
    we want to see if we can improve the evolution of the NEAT agent to tackle more
    difficult problems.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这种学习或进化被称为什么或如何描述？好吧，我们将在下一章中讨论一组思想和理论。现在，我们想要看看我们是否可以改进 NEAT 代理的进化以解决更困难的问题。
- en: 11.4 Solving Gym’s lunar lander problem with NEAT agents
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.4 使用 NEAT 代理解决 Gym 的月球着陆问题
- en: Chances are if you ran the last notebook over other RL environments, you discovered
    that our process only works well for simpler RL environments. In fact, evolving
    solutions in far more complex environments, like the lunar lander problem, just
    makes no progress at all. This is because the complexity to evolve a NEAT agent/network
    using just rewards is not sufficient.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能，如果你在其他的强化学习环境中运行了上一个笔记本，你会发现我们的过程仅适用于简单的强化学习环境。实际上，在远更复杂的环境中，如月球着陆问题中进化解决方案，根本没有任何进展。这是因为仅使用奖励来进化
    NEAT 代理/网络所需的复杂性不足以实现。
- en: In chapter 12, we look at a further set of strategies that can assist us in
    solving the lunar lander problem, but for now, we look at a solution example from
    the NEAT-Python repository examples. NEAT-Python has a nice collection of examples
    designed to run without notebooks. For convenience, we have converted the lunar
    lander example to a Colab notebook to demonstrate an improved NEAT solver.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 12 章中，我们将探讨一系列可以帮助我们解决月球着陆问题的策略，但到目前为止，我们来看一个来自 NEAT-Python 仓库示例的解决方案。NEAT-Python
    拥有一系列设计用于无笔记本运行的示例。为了方便，我们将月球着陆示例转换为 Colab 笔记本，以展示改进的 NEAT 求解器。
- en: Caution The code in this example demonstrates one possible solution to evolving
    NEAT networks to solve more complex RL problems. This solution is heavily customized
    and uses complex mathematical concepts to refine `fitness` evaluation. As stated
    previously, we look at more elegant solutions in the next chapter, but feel free
    to review this notebook.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本例中的代码演示了进化 NEAT 网络以解决更复杂强化学习问题的一种可能解决方案。这个解决方案高度定制化，并使用复杂的数学概念来细化 `fitness`
    评估。如前所述，我们将在下一章中探讨更优雅的解决方案，但请随时查阅这个笔记本。
- en: Open the EDL_11_4_NEAT_LunarLander.ipynb notebook in Google Colab. Refer to
    the appendix if you need assistance. Run all the cells in the notebook by selecting
    Runtime > Run All from the menu. As always, this notebook has been extended from
    previous examples and shares a common code base. The key sections we look at in
    this notebook all center on improvements to the `fitness` evaluation and `gene`
    operators.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Colab 中打开 EDL_11_4_NEAT_LunarLander.ipynb 笔记本。如需帮助，请参阅附录。通过选择菜单中的“运行”>“运行所有”来运行笔记本中的所有单元格。一如既往，这个笔记本是从之前的示例扩展而来的，并共享一个共同的代码库。我们在本笔记本中关注的重点部分都集中在改进
    `fitness` 评估和 `gene` 操作符。
- en: We start by looking at improvements to the `gene` operators and a specialized
    `LanderGenome` class, as shown in listing 11.18\. The core of this class introduces
    a new parameter called `discount`. The premise of a `discount` parameter is to
    introduce a factor that reduces rewards over time. *Discounted rewards*, or reducing
    the influence of future or past rewards over time, is a concept developed in RL.
    The `gamma` term used in the Q-learning equation represents the decay of future
    rewards. However, in this solution, decayed rewards do not directly affect the
    `action` of agents; rather, they are used to better evaluate their `fitness`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先关注对 `gene` 操作符的改进和专门的 `LanderGenome` 类，如列表 11.18 所示。这个类的核心引入了一个新的参数，称为 `discount`。`discount`
    参数的前提是引入一个随时间减少奖励的因素。*折现奖励*，即随着时间的推移减少未来或过去奖励的影响，是强化学习中发展出的一个概念。Q-learning 方程中使用的
    `gamma` 项代表未来奖励的衰减。然而，在这个解决方案中，衰减的奖励并不直接影响代理的 `action`；相反，它们被用来更好地评估其 `fitness`。
- en: 'Listing 11.18 EDL_11_4_NEAT_LunarLander.ipynb: Customizing the `genome` config'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.18 EDL_11_4_NEAT_LunarLander.ipynb：自定义 `基因组` 配置
- en: '[PRE17]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Creates the discount parameter
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建折扣参数
- en: ❷ Sets the discount value
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置折扣值
- en: ❸ Crosses over/mates the discount
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 交叉/配对折扣
- en: ❹ Mutates the discount
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 突变折扣
- en: ❺ Calculates the genome distance
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算基因组距离
- en: The `fitness` of a `genome` is now evaluated in a `compute_fitness` function
    that no longer simulates the agent directly on the environment but, rather, uses
    a recorded history of `actions`, as shown in listing 11.19\. This history of episodes
    and steps is played back for each `genome`, where the `discount` factor within
    the `genome` is used to evaluate the importance related to previous agent `actions`.
    Essentially, the `fitness` of an agent is calculated in comparison to how other
    agents performed previously. While we can’t say this solution uses RL, it does
    use a normalized and discounted difference between previous agent rewards and
    future evolved agents.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`基因组`的`适应性`是通过一个`compute_fitness`函数来评估的，该函数不再直接在环境中模拟代理，而是使用记录的`动作`历史，如列表
    11.19 所示。这个关于剧集和步骤的历史为每个`基因组`回放，其中`基因组`内的`折扣`因子用于评估与先前代理`动作`相关的重要性。本质上，代理的`适应性`是通过与其他代理先前表现的比较来计算的。虽然我们不能说这个解决方案使用了强化学习，但它确实使用了先前代理奖励和未来进化代理之间的归一化和折扣差异。
- en: 'Listing 11.19 EDL_11_4_NEAT_LunarLander.ipynb: Computing `fitness`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.19 EDL_11_4_NEAT_LunarLander.ipynb：计算 `适应性`
- en: '[PRE18]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Creates the function to discount
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建折扣函数
- en: ❷ Loops over episodes
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历剧集
- en: ❸ Discounts rewards based on function
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 根据函数折扣奖励
- en: ❹ Loops through episode steps
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 遍历剧集步骤
- en: ❺ Calculates the difference in reward error
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算奖励误差的差异
- en: 'There is a lot of code in this notebook that revolves around simulating the
    agent environment interaction, recording it, and evaluating it across the `population`
    of `genomes`. The next key element we look at is the code that runs the simulations:
    the `simulate` function found within the `PooledErrorCompute` class. Unlike our
    previous notebook, the agent code that runs through a simulation, shown in listing
    11.20, simulates simple exploration based on the current step, giving the simulation
    the opportunity to add exploratory steps to the simulation data. Each simulation
    run is added to data to score and extract the most successful runs, where success
    is still measured in total accumulated rewards.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这个笔记本中有大量代码围绕着模拟代理环境交互、记录它并在`基因组`群体中评估它。我们接下来要查看的关键元素是运行模拟的代码：`PooledErrorCompute`类中的`simulate`函数。与之前的笔记本不同，通过模拟运行的代理代码（如列表
    11.20 所示）基于当前步骤进行简单探索，给模拟机会添加探索步骤到模拟数据中。每次模拟运行都被添加到数据中以评分和提取最成功的运行，成功仍然是通过累积的总奖励来衡量的。
- en: 'Listing 11.20 EDL_11_4_NEAT_LunarLander.ipynb: Simulating runs'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.20 EDL_11_4_NEAT_LunarLander.ipynb：模拟运行
- en: '[PRE19]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Loops over genome networks
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历基因组网络
- en: ❷ Decides to explore or exploit
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 决定探索或利用
- en: ❸ Appends the step output to the data
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将步骤输出添加到数据中
- en: ❹ Scores up the best simulation runs
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 评分最佳模拟运行
- en: ❺ Appends to the test episodes
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将数据添加到测试剧集
- en: This solution does borrow from the RL process and attempts to measure the error
    in terms of rewards. The total reward error here has a direct influence on `individual
    fitness`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案确实借鉴了强化学习过程，并试图用奖励来衡量误差。这里的总奖励误差对`个体适应性`有直接影响。
- en: Go ahead and review all the rest of the code on your own as the notebook runs—and
    it will run for quite a while. This notebook can evolve for 8+ hours, and it still
    may not be able to solve the problem.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本运行时，请自行审查所有其余的代码——并且它将运行相当长的时间。这个笔记本可能需要8小时以上才能进化，而且可能仍然无法解决问题。
- en: As this notebook trains, we see a quick convergence of `fitness`, but then,
    things quickly plateau. In fact, you may not find any positive rewards or `fitness`
    being evaluated until 1,000+ `generations` into evolution. The progress is slow,
    but if you have the patience, a NEAT agent can eventually be evolved to solve
    the lunar lander environment.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这个笔记本的训练，我们看到`适应性`快速收敛，但随后，事情迅速达到平台期。事实上，你可能直到进化到1,000+ `代`之后才找不到任何正面的奖励或`适应性`评估。进步很慢，但如果你有耐心，NEAT代理最终可以被进化来解决月球着陆环境。
- en: As much as the NEAT agents borrow RL environments and some techniques to help
    solve the problems, the actual evolution is not something we would refer to as
    DRL. Instead, we need to consider other evolutionary concepts or ideas that describe
    how we evolve an agent that self-evolves its own learning function. In essence,
    we have evolved an agent that evolved its own system of learning or learning function.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管NEAT代理借鉴了强化学习环境和一些技术来帮助解决问题，但实际的进化并不是我们所说的DRL。相反，我们需要考虑其他进化概念或想法，这些概念或想法描述了如何进化一个能够自我进化其学习函数的代理。本质上，我们已经进化了一个能够进化其自身学习系统或学习函数的代理。
- en: While it is unlikely the evolved internal learning function resembles the Q-learning
    equation, we can confirm it is able to solve complex RL environments. It is the
    evolution of this learning function that becomes the most interesting and powerful
    concept we look at in the next and final chapter.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然进化的内部学习函数不太可能类似于Q学习方程，但我们能确认它能够解决复杂的强化学习环境。正是这个学习函数的进化，成为了我们在下一章和最后一章中探讨的最有趣和最有力的概念。
- en: 11.4.1 Learning exercises
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.1 学习练习
- en: 'Performing the following exercises will help you review and improve your understanding
    of the content:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下练习将有助于你复习和改进对内容的理解：
- en: Compare the results of running this notebook against the standard NEAT Gym exercise
    we explored in the last section. How does an agent perform after that same number
    of `generations` from each notebook? Is it what you expect?
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将运行此笔记本的结果与上一节中探索的标准NEAT Gym练习进行比较。代理在相同数量的`generations`后表现如何？这是你预期的吗？
- en: Add a different environment to the notebook to see how the improved `fitness`
    evaluation increases or decreases the NEAT agent’s performance.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中添加一个不同的环境，以查看改进的`fitness`评估如何增加或减少NEAT代理的性能。
- en: Implement a different method of exploration. Right now, this notebook uses a
    fixed exploration rate. Add some variation by implementing a decaying exploration
    rate, as seen in previous examples.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现不同的探索方法。目前，这个笔记本使用固定的探索率。通过实现一个衰减的探索率，就像之前示例中看到的那样，增加一些变化。
- en: This section demonstrates the power of NEAT to evolve an `individual` that could
    internally replicate the Q-learning RL process. In the next section, we look at
    a baseline DRL implementation called DQN as a comparison to what we have done
    with NEXT.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了NEAT的强大能力，可以进化一个能够内部复制Q学习强化学习过程的`个体`。在下一节中，我们将查看一个称为DQN的基线DRL实现，并将其与我们使用NEXT所做的工作进行比较。
- en: 11.5 Solving Gym’s lunar lander problem with a deep Q-network
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.5 使用深度Q网络解决Gym的月球着陆器问题
- en: DRL first made heads turn when it was shown a deep Q-learning model could solve
    classic Atari games, using just `observation` `state` as input. This was a dramatic
    breakthrough, and since then, DRL has been shown to solve many complex tasks far
    better than humans. In this section’s notebook, we look at the classic implementation
    of DQN as an alternative to solving the lunar lander problem.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 当深度强化学习（DRL）首次展示出深度Q学习模型能够仅使用`观察``状态`作为输入解决经典的Atari游戏时，它引起了人们的关注。这是一个重大的突破，从那时起，DRL已经证明能够比人类更好地解决许多复杂任务。在本节的笔记本中，我们查看DQN的经典实现，作为解决月球着陆器问题的替代方案。
- en: Deep Q-networks on Atari
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在Atari上的深度Q网络
- en: Solving the classic Atari games using DQN worked, but it took a substantial
    number of iterations. The number of required training episodes to solve even a
    basic Atari environment, like Breakout, can be millions of episodes. Improvements
    in RL methods have since dropped the number of training episodes, but overall,
    DRL is a computationally expensive endeavor. Fortunately, unlike EC, DRL has been
    a major beneficiary of the computational enhancements made possible by deep learning.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DQN解决经典Atari游戏是有效的，但需要大量的迭代。解决甚至像Breakout这样的基本Atari环境所需的训练次数可能达到数百万次。自那时以来，强化学习方法的改进已经减少了所需的训练次数，但总体而言，DRL是一个计算成本高昂的任务。幸运的是，与EC不同，DRL是深度学习带来的计算增强的主要受益者。
- en: Open the EDL_11_5_DQN_LunarLander.ipynb notebook in Google Colab. Refer to the
    appendix if you need assistance. Run all the cells in the notebook by selecting
    Runtime > Run All from the menu.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在Google Colab中打开EDL_11_5_DQN_LunarLander.ipynb笔记本。如需帮助，请参考附录。通过选择菜单中的“运行”>“运行所有”来运行笔记本中的所有单元格。
- en: This notebook is set up to use the same environments but has the evolution code
    and libraries removed. Now, our focus is on how the DQN model works on a Gym problem.
    This means we start with the `DQNAgent` class definition, as shown in the following
    listing. The `init` function sets up the base hyperparameters and saves the `action`
    and `observation` sizes. It also adds a `memory`, which is used to store experiences
    from simulations as well as the agent’s brain or model.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这个笔记本被设置为使用相同的环境，但移除了进化和代码库。现在，我们的重点是 DQN 模型在 Gym 问题上的工作方式。这意味着我们从 `DQNAgent`
    类定义开始，如下面的列表所示。`init` 函数设置基本超参数并保存 `action` 和 `observation` 的大小。它还添加了一个 `memory`，用于存储模拟中的经验以及代理的大脑或模型。
- en: 'Listing 11.21 EDL_11_5_DQN_Gyms.ipynb: The DQN agent'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.21 EDL_11_5_DQN_Gyms.ipynb：DQN 代理
- en: '[PRE20]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Imports deep learning packages
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入深度学习包
- en: ❷ Saves the action/ observation sizes
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 保存动作/观察大小
- en: ❸ Creates a place for memories
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建一个存储记忆的地方
- en: ❹ Sets up the hyperparameters
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 设置超参数
- en: ❺ Creates the model or brain of the agent
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 创建代理的模型或大脑
- en: The DL model or brain of the agent is defined next in the `build_model` function.
    The code in this function creates a three-layer model, taking the `state` space
    as input and outputting the `action` space as output. The model is compiled with
    `mse` for loss and an `Adam` optimizer. Unique to this example is the ability
    of the model to load a file containing previously trained model weights, as shown
    in the following listing.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `build_model` 函数中接下来定义代理的深度学习模型或大脑。该函数中的代码创建了一个三层模型，以 `state` 空间作为输入，以 `action`
    空间作为输出。模型使用 `mse` 作为损失函数和 `Adam` 优化器进行编译。本例的独特之处在于模型能够加载包含先前训练的模型权重的文件，如下面的列表所示。
- en: 'Listing 11.22 EDL_11_5_DQN_Gyms.ipynb: Building the brain'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.22 EDL_11_5_DQN_Gyms.ipynb：构建大脑
- en: '[PRE21]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Starts with the base model
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从基本模型开始
- en: ❷ Adds layers to the model
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 向模型添加层
- en: ❸ The output layer matches the action size.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 输出层与动作大小匹配。
- en: ❹ Compiles the model with MSE loss
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用均方误差（MSE）损失编译模型
- en: ❺ Loads the previous model weights, if possible
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 如果可能，加载先前的模型权重
- en: Before we get to the rest of the `DQNAgent` definition, let’s review the training
    code. The code, shown in listing 11.23, starts by setting the primary hyperparameters
    of `BATCH_SIZE` and `EPISODES`. It then starts by looping through the number of
    episodes, simulating the agent until a call to `env.step` outputs `done` equals
    `True` during each one. If the agent is not done, it inputs the `state` into the
    `agent.act` function to output an `action` prediction, which is then applied to
    the `env.step` function to output the next `state`, `reward`, and `done`. Next,
    `agent.remember` is called to add the `action` and consequences to the agent’s
    memory. After each episode, when `done == True`, a call is made to `agent.remember`,
    which replays all the remembered `actions` and uses the results to train the model.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续 `DQNAgent` 定义的其他部分之前，让我们回顾一下训练代码。代码，如列表 11.23 所示，首先设置 `BATCH_SIZE` 和 `EPISODES`
    的主要超参数。然后，它开始循环遍历 episode 的数量，模拟代理直到 `env.step` 在每个 episode 中输出 `done` 等于 `True`。如果代理未完成，它将
    `state` 输入到 `agent.act` 函数以输出动作预测，然后将其应用于 `env.step` 函数以输出下一个 `state`、`reward`
    和 `done`。接下来，调用 `agent.remember` 将 `action` 和后果添加到代理的记忆中。在每个 episode 结束时，当 `done
    == True`，调用 `agent.remember`，回放所有记住的 `actions` 并使用结果来训练模型。
- en: 'Listing 11.23 EDL_11_5_DQN_Gyms.ipynb: Training the agent'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.23 EDL_11_5_DQN_Gyms.ipynb：训练代理
- en: '[PRE22]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Sets the primary hyperparameters
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置主要超参数
- en: ❷ Reshapes the state for the model
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 重新塑形状态以供模型使用
- en: ❸ Predicts and executes the action
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 预测并执行动作
- en: ❹ Remembers the action and consequences
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 记住动作和后果
- en: ❺ Replays actions and trains the agent
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 回放动作并训练代理
- en: Now, we move back to the `DQNAgent` definition and review the `act`, `remember`**,**
    and `replay` functions, as shown in listing 11.24\. The first function, `act`,
    evaluates the chance of exploration and responds with either a random `action`,
    if exploring, or a predicted `action`, if not. Second, the `remember` function
    stores the experiences of the agent as it simulates through the episodes. The
    memory used here is a dequeue class that uses a fixed size that automatically
    throws out the oldest memories as it gets full. Third, the `replay` function extracts
    a batch of experiences from the agent memory, given there are enough memories.
    This batch of experiences is then used to replay the agent’s `actions` and evaluate
    the quality of each previously executed `action` (random or predicted). The quality
    `target` of an `action` is calculated using a form of the Q-learning equation.
    This calculated value is then used to update the model, using the `fit` function
    over a single epoch. Finally, at the end of the `replay` function, the chance
    of exploration—the `exploration_rate`—is updated by the `exploration_decay`, if
    required.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们回到 `DQNAgent` 定义，并回顾列表 11.24 中的 `act`、`remember`**、**和 `replay` 函数。第一个函数
    `act` 评估探索的机会，并在探索时响应随机 `动作`，如果不是探索则响应预测 `动作`。第二个函数 `remember` 存储代理在模拟过程中积累的经验。这里使用的内存是一个出队类，它使用固定大小，并在填满时自动丢弃最老的记忆。第三个函数
    `replay` 从代理内存中提取一批经验，前提是有足够的记忆。这批经验随后用于回放代理的 `动作` 并评估每个先前执行的 `动作`（随机或预测）的质量。`动作`
    的质量 `目标` 使用 Q-learning 方程的一种形式来计算。然后使用 `fit` 函数在单个周期内使用计算出的值来更新模型。最后，在 `replay`
    函数的末尾，如果需要，通过 `exploration_decay` 更新探索的机会——`exploration_rate`。
- en: 'Listing 11.24 EDL_11_5_DQN_Gyms.ipynb: The `act`, `remember`, and `replay`
    functions'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.24 EDL_11_5_DQN_Gyms.ipynb：`act`、`remember` 和 `replay` 函数
- en: '[PRE23]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Selects a random or predicted action
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 选择随机或预测动作
- en: ❷ Appends to the dequeue memory
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将数据追加到出队内存中
- en: ❸ Checks whether the memory is larger than the batch size
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 检查内存是否大于批量大小
- en: ❹ Extracts experiences from the memory and trains
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从内存中提取经验并进行训练
- en: ❺ Evaluates the target predictions from the Q-learning function
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 评估 Q-learning 函数的目标预测
- en: Figure 11.8 shows the results of training the DQN agent on the lunar lander
    environment for 1,000 epochs. In the figure, you can see how the agent is able
    to gradually accumulate rewards as it learns to master the environment.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 展示了在月球着陆器环境中训练 DQN 代理 1,000 个周期的结果。在图中，你可以看到代理如何随着学习掌握环境而逐渐积累奖励。
- en: '![](../Images/CH11_F08_Lanham.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH11_F08_Lanham.png)'
- en: Figure 11.8 The results of training an agent on the lunar lander environment
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 在月球着陆器环境中训练代理的结果
- en: DQN and DRL are powerful advances in AI and ML that have showcased the potential
    ability for digital intelligence to solve some tasks better than humans. However,
    a couple of key challenges for DRL to overcome remain, including multitask, or
    generalized, learning. We explore how evolution can be used for potentially generalized
    forms of learning, like DRL, in the next chapter.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 和 DRL 是 AI 和 ML 中的强大进步，展示了数字智能在某些任务上可能比人类做得更好的潜力。然而，DRL 需要克服的一些关键挑战仍然存在，包括多任务或泛化学习。我们将在下一章探讨如何利用进化来探索可能用于泛化形式的学习，如
    DRL。
- en: Summary
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Reinforcement learning is another form of dynamic learning that uses rewards
    to reinforce the selection of the best appropriate `action` given a current `state`.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习是另一种动态学习形式，它使用奖励来强化给定当前 `状态` 时选择最佳适当 `动作` 的选择。
- en: Q-learning is an implementation of RL that uses a `state` or `action` lookup
    table or policy to provide a decision on the next-best possible `action` an agent
    should take.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q-learning 是一种 RL 的实现，它使用 `状态` 或 `动作` 查找表或策略来提供关于代理应采取的下一个最佳可能 `动作` 的决策。
- en: It is important to be able to differentiate between various forms of learning,
    including generative, supervised, and reinforcement learning.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够区分各种学习形式很重要，包括生成式、监督式和强化学习。
- en: The OpenAI Gym is a framework and tool kit for evaluation and exploring various
    implementations of RL or other reward/decision-solving models.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI Gym 是一个用于评估和探索 RL 或其他奖励/决策求解模型各种实现的框架和工具包。
- en: Running the OpenAI Gym within a Colab notebook can be useful for exploring various
    environments ranging in complexity.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Colab 笔记本中运行 OpenAI Gym 可以用于探索各种复杂度不同的环境。
- en: OpenAI Gym is a common reinforcement learning algorithm benchmark and exploration
    tool.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI Gym是一个常见的强化学习算法基准和探索工具。
- en: NEAT can be used to solve a variety of sample RL Gym environments by employing
    typical reinforcement learning.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NEAT可以通过使用典型的强化学习来解决各种样本强化学习Gym环境。
- en: A NEAT agent can be developed to solve more complex RL environments using sampling
    and playback techniques.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以开发一个NEAT智能体，通过采样和回放技术来解决更复杂的强化学习环境。
- en: Deep Q-learning is an advanced form of RL that employs DL in place of a Q-table
    or policy. Deep Q-networks have been used to solve complex environments, like
    the lunar lander game.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度Q学习是强化学习的一种高级形式，它使用深度学习代替Q表或策略。深度Q网络已被用于解决复杂环境，如月球着陆游戏。
