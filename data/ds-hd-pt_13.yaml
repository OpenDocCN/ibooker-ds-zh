- en: Chapter 11\. Data Leakage
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬11ç«  æ•°æ®æ³„éœ²
- en: 'In â€œLeakage in Data Mining: Formulation, Detection, and Avoidance,â€ Shachar
    Kaufman et al. (2012) identify data leakage as one of the top 10 most common problems
    in data science. In my experience, it should rank even higher: if you have trained
    enough real-life models, itâ€™s unlikely you havenâ€™t encountered it.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ã€Šæ•°æ®æŒ–æ˜ä¸­çš„æ³„æ¼ï¼šå…¬å¼åŒ–ã€æ£€æµ‹å’Œé¿å…ã€‹ä¸€ä¹¦ä¸­ï¼Œæ²™å“ˆå°”Â·è€ƒå¤«æ›¼ç­‰äººï¼ˆ2012ï¼‰å°†æ•°æ®æ³„éœ²è¯†åˆ«ä¸ºæ•°æ®ç§‘å­¦ä¸­åå¤§å¸¸è§é—®é¢˜ä¹‹ä¸€ã€‚æ ¹æ®æˆ‘çš„ç»éªŒï¼Œå®ƒåº”è¯¥æ’åæ›´é«˜ï¼šå¦‚æœä½ å·²ç»è®­ç»ƒäº†è¶³å¤Ÿå¤šçš„ç°å®ç”Ÿæ´»æ¨¡å‹ï¼Œé‚£ä¹ˆä½ å‡ ä¹ä¸å¯èƒ½æ²¡æœ‰é‡åˆ°å®ƒã€‚
- en: This chapter is devoted to discussing data leakage, some symptoms, and what
    can be done about it.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« ä¸“æ³¨äºè®¨è®ºæ•°æ®æ³„éœ²ï¼Œä¸€äº›ç—‡çŠ¶ä»¥åŠå¯é‡‡å–çš„æªæ–½ã€‚
- en: What Is Data Leakage?
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æ•°æ®æ³„éœ²ï¼Ÿ
- en: 'As the name suggests, *data leakage* occurs when some of the data used for
    training a model isnâ€™t available when you deploy your model into production, creating
    subpar predictive performance in the latter stage. This usually happens when you
    train a model:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚åç§°æ‰€ç¤ºï¼Œ*æ•°æ®æ³„éœ²*å‘ç”Ÿåœ¨ä½ ç”¨äºè®­ç»ƒæ¨¡å‹çš„ä¸€äº›æ•°æ®åœ¨ä½ å°†æ¨¡å‹éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒæ—¶ä¸å¯ç”¨ï¼Œä»è€Œå¯¼è‡´åè€…é˜¶æ®µçš„é¢„æµ‹æ€§èƒ½ä¸ä½³ã€‚è¿™é€šå¸¸å‘ç”Ÿåœ¨ä½ è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ—¶ï¼š
- en: Using data or metadata that wonâ€™t be available at the prediction stage
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åœ¨é¢„æµ‹é˜¶æ®µä¸å¯ç”¨çš„æ•°æ®æˆ–å…ƒæ•°æ®
- en: That is correlated with the outcome you want to predict
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ä½ æƒ³è¦é¢„æµ‹çš„ç»“æœç›¸å…³
- en: That creates *unrealistically high* test-sample predictive performance
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™ä¼šé€ æˆ*è¿‡é«˜*çš„æµ‹è¯•æ ·æœ¬é¢„æµ‹æ€§èƒ½
- en: 'The last item explains why leakage is a source of concern and frustration for
    data scientists: when you train a model, absent any data and model drift, you
    expect that the predictive performance on the test sample will extrapolate to
    the real world once you deploy the model in production. This wonâ€™t be the case
    if you have data leakage, and you (your stakeholders and the company) will suffer
    a big disappointment.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€æ¡è¯´æ˜äº†ä¸ºä»€ä¹ˆæ•°æ®æ³„éœ²å¯¹æ•°æ®ç§‘å­¦å®¶æ˜¯ä¸€ç§æ‹…å¿§å’Œæ²®ä¸§çš„æºå¤´ï¼šå½“ä½ è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ—¶ï¼Œåœ¨æ²¡æœ‰æ•°æ®å’Œæ¨¡å‹æ¼‚ç§»çš„æƒ…å†µä¸‹ï¼Œä½ æœŸæœ›æµ‹è¯•æ ·æœ¬ä¸Šçš„é¢„æµ‹æ€§èƒ½èƒ½å¤Ÿåœ¨ä½ å°†æ¨¡å‹æŠ•å…¥ç”Ÿäº§åå¤–æ¨åˆ°çœŸå®ä¸–ç•Œã€‚å¦‚æœå­˜åœ¨æ•°æ®æ³„éœ²ï¼Œæƒ…å†µå°±ä¸ä¼šå¦‚æ­¤ï¼Œä½ ï¼ˆä»¥åŠä½ çš„åˆ©ç›Šç›¸å…³è€…å’Œå…¬å¸ï¼‰å°†é­å—é‡å¤§å¤±æœ›ã€‚
- en: Letâ€™s go through several examples to clarify this definition.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡å‡ ä¸ªä¾‹å­æ¥æ¾„æ¸…è¿™ä¸ªå®šä¹‰ã€‚
- en: Outcome Is Also a Feature
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æœä¹Ÿæ˜¯ä¸€ä¸ªç‰¹å¾
- en: 'This is a trivial example, but helps as a benchmark for more realistic examples.
    If you train a model like this:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œä½†æœ‰åŠ©äºä½œä¸ºæ›´ç°å®ä¾‹å­çš„åŸºå‡†ã€‚å¦‚æœä½ åƒè¿™æ ·è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼š
- en: <math alttext="y equals f left-parenthesis y right-parenthesis" display="block"><mrow><mi>y</mi>
    <mo>=</mo> <mi>f</mi> <mo>(</mo> <mi>y</mi> <mo>)</mo></mrow></math>
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y equals f left-parenthesis y right-parenthesis" display="block"><mrow><mi>y</mi>
    <mo>=</mo> <mi>f</mi> <mo>(</mo> <mi>y</mi> <mo>)</mo></mrow></math>
- en: youâ€™ll get perfect performance at the training stage, but needless to say, you
    wonâ€™t be able to make a prediction when your model is deployed in production (since
    the outcome is, by definition, not available at the time of prediction).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒé˜¶æ®µï¼Œä½ ä¼šå¾—åˆ°å®Œç¾çš„æ€§èƒ½ï¼Œä½†ä¸ç”¨è¯´ï¼Œå½“ä½ çš„æ¨¡å‹éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒæ—¶ï¼Œä½ å°†æ— æ³•è¿›è¡Œé¢„æµ‹ï¼ˆå› ä¸ºç»“æœåœ¨é¢„æµ‹æ—¶æ˜¯ä¸å¯ç”¨çš„ï¼‰ã€‚
- en: A Function of the Outcome Is Itself a Feature
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æœæœ¬èº«å°±æ˜¯ä¸€ä¸ªç‰¹å¾
- en: A more realistic example is when one of the features is a function of the outcome.
    Suppose you want to make a prediction of next monthâ€™s revenue and, using the <math
    alttext="upper P times upper Q"><mrow><mi>P</mi> <mo>Ã—</mo> <mi>Q</mi></mrow></math>
    decomposition described in [ChapterÂ 2](ch02.html#ch02_decom_metrix), you include
    the unit price ( <math alttext="Revenue slash Sales"><mrow><mtext>Revenue</mtext>
    <mo>/</mo> <mtext>Sales</mtext></mrow></math> ) as a feature. Many times, the
    unit price calculation is done upstream, so you just end up using a table that
    contains prices without really knowing how they are calculated.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´ä¸ºç°å®çš„ä¾‹å­æ˜¯ï¼Œå½“ä¸€ä¸ªç‰¹å¾æ˜¯ç»“æœçš„å‡½æ•°æ—¶ã€‚å‡è®¾ä½ æƒ³è¦é¢„æµ‹ä¸‹ä¸ªæœˆçš„æ”¶å…¥ï¼Œå¹¶ä¸”ä½¿ç”¨åœ¨[ç¬¬2ç« ](ch02.html#ch02_decom_metrix)æè¿°çš„ä¸ŠPä¹˜ä»¥ä¸ŠQåˆ†è§£åŒ…æ‹¬å•ä½ä»·æ ¼ï¼ˆ<math
    alttext="Revenue slash Sales"><mrow><mtext>Revenue</mtext> <mo>/</mo> <mtext>Sales</mtext></mrow></math>ï¼‰ä½œä¸ºä¸€ä¸ªç‰¹å¾ã€‚è®¸å¤šæ—¶å€™ï¼Œå•ä½ä»·æ ¼çš„è®¡ç®—æ˜¯åœ¨ä¸Šæ¸¸å®Œæˆçš„ï¼Œå› æ­¤ä½ æœ€ç»ˆåªæ˜¯ä½¿ç”¨ä¸€ä¸ªåŒ…å«ä»·æ ¼çš„è¡¨ï¼Œè€Œä¸çœŸæ­£äº†è§£å®ƒä»¬æ˜¯å¦‚ä½•è®¡ç®—çš„ã€‚
- en: Bad Controls
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸è‰¯æ§åˆ¶
- en: As described in [ChapterÂ 10](ch10.html#ch10_linreg), itâ€™s good to include features
    that you may think help control for sources of variation, even if you donâ€™t have
    a strong hypothesis for the underlying causal mechanism. This is generally true,
    unless you include *bad controls*, which are themselves outcomes affected by the
    features.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚[ç¬¬10ç« ](ch10.html#ch10_linreg)æ‰€è¿°ï¼Œæœ€å¥½åŒ…æ‹¬ä¸€äº›ä½ è®¤ä¸ºæœ‰åŠ©äºæ§åˆ¶å˜å¼‚æºçš„ç‰¹å¾ï¼Œå³ä½¿ä½ å¯¹åº•å±‚å› æœæœºåˆ¶æ²¡æœ‰å¼ºçƒˆçš„å‡è®¾ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè¿™æ˜¯æ­£ç¡®çš„ï¼Œé™¤éä½ åŒ…æ‹¬*ä¸è‰¯æ§åˆ¶*ï¼Œè¿™äº›æ§åˆ¶æœ¬èº«å—ç‰¹å¾å½±å“ã€‚
- en: 'Take these data generating processes (DGPs) as an example:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥è¿™äº›æ•°æ®ç”Ÿæˆè¿‡ç¨‹ï¼ˆDGPsï¼‰ä¸ºä¾‹ï¼š
- en: <math alttext="StartLayout 1st Row 1st Column y Subscript t 2nd Column equals
    3rd Column f left-parenthesis bold x Subscript t minus 1 Baseline right-parenthesis
    plus epsilon Subscript t 2nd Row 1st Column z Subscript t 2nd Column equals 3rd
    Column g left-parenthesis y Subscript t Baseline right-parenthesis plus zeta Subscript
    t EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><msub><mi>y</mi>
    <mi>t</mi></msub></mtd> <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <msub><mi>ğ±</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow> <mo>+</mo> <msub><mi>Ïµ</mi> <mi>t</mi></msub></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi>z</mi> <mi>t</mi></msub></mtd> <mtd><mo>=</mo></mtd>
    <mtd columnalign="left"><mrow><mi>g</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub>
    <mo>)</mo></mrow> <mo>+</mo> <msub><mi>Î¶</mi> <mi>t</mi></msub></mrow></mtd></mtr></mtable></math>
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column y Subscript t 2nd Column equals
    3rd Column f left-parenthesis bold x Subscript t minus 1 Baseline right-parenthesis
    plus epsilon Subscript t 2nd Row 1st Column z Subscript t 2nd Column equals 3rd
    Column g left-parenthesis y Subscript t Baseline right-parenthesis plus zeta Subscript
    t EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><msub><mi>y</mi>
    <mi>t</mi></msub></mtd> <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <msub><mi>ğ±</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow> <mo>+</mo> <msub><mi>Ïµ</mi> <mi>t</mi></msub></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi>z</mi> <mi>t</mi></msub></mtd> <mtd><mo>=</mo></mtd>
    <mtd columnalign="left"><mrow><mi>g</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub>
    <mo>)</mo></mrow> <mo>+</mo> <msub><mi>Î¶</mi> <mi>t</mi></msub></mrow></mtd></mtr></mtable></math>
- en: You may think that controlling for *z* when training a model to predict *y*
    can help you clean out some of the effects. Unfortunately, since *z* wonâ€™t be
    available at the time of prediction, and is correlated with *y*, you end up with
    a nontrivial example of data leakage.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è®­ç»ƒæ¨¡å‹é¢„æµ‹*y*æ—¶ï¼Œä½ å¯èƒ½ä¼šè®¤ä¸ºæ§åˆ¶*z*å¯ä»¥å¸®åŠ©ä½ æ¸…é™¤ä¸€äº›å½±å“ã€‚ä¸å¹¸çš„æ˜¯ï¼Œç”±äºåœ¨é¢„æµ‹æ—¶*z*ä¸å¯ç”¨ï¼Œå¹¶ä¸”ä¸*y*ç›¸å…³ï¼Œä½ æœ€ç»ˆå¾—åˆ°ä¸€ä¸ªæ•°æ®æ³„æ¼çš„éå¹³å‡¡ç¤ºä¾‹ã€‚
- en: Note that leakage here arises both from using information thatâ€™s not present
    at the time of prediction *and* from including the bad control. If *z* displays
    enough autocorrelation in time, even if you control for its lagged value ( <math
    alttext="z Subscript t minus 1"><msub><mi>z</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    ), you will still have unreasonably high predictive performance.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œè¿™é‡Œçš„æ³„æ¼æ˜¯å› ä¸ºä½¿ç”¨äº†é¢„æµ‹æ—¶ä¸å¯ç”¨çš„ä¿¡æ¯ï¼Œä»¥åŠåŒ…æ‹¬äº†é”™è¯¯çš„æ§åˆ¶ã€‚å¦‚æœ*z*åœ¨æ—¶é—´ä¸Šæ˜¾ç¤ºè¶³å¤Ÿçš„è‡ªç›¸å…³æ€§ï¼Œå³ä½¿ä½ æ§åˆ¶äº†å®ƒçš„æ»åå€¼ï¼ˆ<math alttext="z
    Subscript t minus 1"><msub><mi>z</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>ï¼‰ï¼Œä½ ä»ç„¶ä¼šæœ‰ä¸åˆç†é«˜çš„é¢„æµ‹æ€§èƒ½ã€‚
- en: Mislabeling of a Timestamp
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ—¶é—´æˆ³çš„è¯¯æ ‡
- en: 'Suppose you want to measure the number of monthly active users in a given month.
    A typical query that would produce the desired metric looks like this:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ä½ æƒ³æµ‹é‡ç»™å®šæœˆä»½çš„æœˆæ´»è·ƒç”¨æˆ·æ•°ã€‚ä¸€ä¸ªå…¸å‹çš„æŸ¥è¯¢ï¼Œå°†äº§ç”Ÿæ‰€éœ€çš„æŒ‡æ ‡å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here you have effectively labeled these customers using the beginning-of-month
    timestamp, which for many purposes might make sense. Alternatively, you couldâ€™ve
    labeled them using the end-of-period timestamp, which could also be appropriate
    for different use cases.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œä½ æœ‰æ•ˆåœ°ä½¿ç”¨äº†æœˆåˆæ—¶é—´æˆ³æ¥æ ‡è®°è¿™äº›å®¢æˆ·ï¼Œå¯¹è®¸å¤šç›®çš„æ¥è¯´å¯èƒ½æ˜¯æœ‰æ„ä¹‰çš„ã€‚æˆ–è€…ï¼Œä½ ä¹Ÿå¯ä»¥ä½¿ç”¨å‘¨æœŸç»“æŸæ—¶é—´æˆ³æ¥æ ‡è®°å®ƒä»¬ï¼Œè¿™åœ¨ä¸åŒçš„ç”¨ä¾‹ä¸­ä¹Ÿå¯èƒ½æ˜¯åˆé€‚çš„ã€‚
- en: The point is that the labeling choice may create data leakage if you incorrectly
    think that the metric was measured *before* the time suggested by your timestamp
    (so you would, in practice, be using information from the *future* to predict
    the *past*). This is a common problem encountered in practice.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å…³é”®æ˜¯ï¼Œå¦‚æœä½ é”™è¯¯åœ°è®¤ä¸ºæŒ‡æ ‡æ˜¯åœ¨ä½ çš„æ—¶é—´æˆ³å»ºè®®çš„æ—¶é—´ä¹‹å‰æµ‹é‡çš„ï¼ˆå› æ­¤ï¼Œåœ¨å®è·µä¸­ï¼Œä½ ä¼šä½¿ç”¨*æœªæ¥*çš„ä¿¡æ¯æ¥é¢„æµ‹*è¿‡å»*ï¼‰ï¼Œé‚£ä¹ˆæ ‡ç­¾é€‰æ‹©å¯èƒ½ä¼šå¯¼è‡´æ•°æ®æ³„æ¼ã€‚è¿™æ˜¯å®è·µä¸­ç»å¸¸é‡åˆ°çš„é—®é¢˜ã€‚
- en: Multiple Datasets with Sloppy Time Aggregations
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤šä¸ªæ•°æ®é›†å…·æœ‰æ¾æ•£çš„æ—¶é—´èšåˆ
- en: 'Suppose you want to predict customer churn using a model like this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ä½ æƒ³ä½¿ç”¨è¿™æ ·çš„æ¨¡å‹æ¥é¢„æµ‹å®¢æˆ·æµå¤±ï¼š
- en: <math alttext="Prob left-parenthesis c h u r n Subscript t Baseline right-parenthesis
    equals f left-parenthesis normal upper Delta sales Subscript t minus 1 Superscript
    t Baseline comma num period products Subscript t Baseline right-parenthesis" display="block"><mrow><mtext>Prob</mtext>
    <mrow><mo>(</mo> <mi>c</mi> <mi>h</mi> <mi>u</mi> <mi>r</mi> <msub><mi>n</mi>
    <mi>t</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>Î”</mi>
    <msubsup><mtext>sales</mtext> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> <mi>t</mi></msubsup>
    <mo>,</mo> <mtext>num.</mtext> <msub><mtext>products</mtext> <mi>t</mi></msub>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="Prob left-parenthesis c h u r n Subscript t Baseline right-parenthesis
    equals f left-parenthesis normal upper Delta sales Subscript t minus 1 Superscript
    t Baseline comma num period products Subscript t Baseline right-parenthesis" display="block"><mrow><mtext>Prob</mtext>
    <mrow><mo>(</mo> <mi>c</mi> <mi>h</mi> <mi>u</mi> <mi>r</mi> <msub><mi>n</mi>
    <mi>t</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>Î”</mi>
    <msubsup><mtext>sales</mtext> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> <mi>t</mi></msubsup>
    <mo>,</mo> <mtext>num.</mtext> <msub><mtext>products</mtext> <mi>t</mi></msub>
    <mo>)</mo></mrow></mrow></math>
- en: 'There are two hypotheses at work here:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸¤ä¸ªå‡è®¾åœ¨èµ·ä½œç”¨ï¼š
- en: Customers who have decreased their sales in the previous period are more likely
    to churn (they are effectively signaling their decreased engagement).
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å‰ä¸€æœŸä¸­å‡å°‘é”€å”®é¢çš„å®¢æˆ·æ›´æœ‰å¯èƒ½æµå¤±ï¼ˆä»–ä»¬å®é™…ä¸Šæ­£åœ¨è¡¨æ˜ä»–ä»¬çš„å‚ä¸åº¦ä¸‹é™ï¼‰ã€‚
- en: Customers with a deeper relationship with the company, as measured by the number
    of other products they are currently using, are less likely to churn.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸å…¬å¸æœ‰æ›´æ·±å…¥å…³ç³»çš„å®¢æˆ·ï¼ˆé€šè¿‡å½“å‰ä½¿ç”¨çš„å…¶ä»–äº§å“æ•°é‡è¡¡é‡ï¼‰æ›´ä¸å®¹æ˜“æµå¤±ã€‚
- en: 'One possible cause for leakage occurs when the second feature includes information
    from the future, so that trivially, a customer who is active with one product
    next month *cannot* have churned. This might occur because you end up querying
    your data with something like the following code:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ³„æ¼çš„ä¸€ä¸ªå¯èƒ½åŸå› æ˜¯å½“ç¬¬äºŒä¸ªç‰¹å¾åŒ…å«æ¥è‡ªæœªæ¥ä¿¡æ¯æ—¶ï¼Œæ˜¾ç„¶ä¸‹ä¸ªæœˆä¸æŸä¸€äº§å“æ´»è·ƒçš„å®¢æˆ·*ä¸å¯èƒ½*å·²ç»æµå¤±ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºä½ æœ€ç»ˆæŸ¥è¯¢æ•°æ®æ—¶ä½¿ç”¨äº†å¦‚ä¸‹ä»£ç ï¼š
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The problem arises because the data scientist was sloppy when filtering the
    dates in each subquery.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜å‡ºåœ¨æ•°æ®ç§‘å­¦å®¶åœ¨æ¯ä¸ªå­æŸ¥è¯¢ä¸­è¿‡æ»¤æ—¥æœŸæ—¶å¤„ç†å¾—ä¸ç²¾ç»†ã€‚
- en: Leakage of Other Information
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…¶ä»–ä¿¡æ¯çš„æ³„æ¼
- en: 'The previous examples dealt with leakage of data, either from the features
    or the outcome itself. In the definition, I also allowed for *metadata* leakage.
    This next example will help clarify what this means. In many ML applications,
    itâ€™s normal to transform your data by standardizing it like this:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å‰é¢çš„ä¾‹å­å¤„ç†äº†æ•°æ®æ³„æ¼ï¼Œè¦ä¹ˆæ˜¯æ¥è‡ªç‰¹å¾ï¼Œè¦ä¹ˆæ˜¯æ¥è‡ªç»“æœæœ¬èº«ã€‚åœ¨å®šä¹‰ä¸­ï¼Œæˆ‘è¿˜å…è®¸*å…ƒæ•°æ®*æ³„æ¼ã€‚ä¸‹ä¸€ä¸ªä¾‹å­å°†å¸®åŠ©æ¾„æ¸…è¿™æ˜¯ä»€ä¹ˆæ„æ€ã€‚åœ¨è®¸å¤šMLåº”ç”¨ä¸­ï¼Œå°†æ•°æ®æ ‡å‡†åŒ–çš„æ–¹æ³•å¦‚ä¸‹ï¼š
- en: <math alttext="y Subscript s t d Baseline equals StartFraction y minus mean
    left-parenthesis y right-parenthesis Over std left-parenthesis y right-parenthesis
    EndFraction" display="block"><mrow><msub><mi>y</mi> <mrow><mi>s</mi><mi>t</mi><mi>d</mi></mrow></msub>
    <mo>=</mo> <mfrac><mrow><mi>y</mi><mo>-</mo><mtext>mean</mtext><mo>(</mo><mi>y</mi><mo>)</mo></mrow>
    <mrow><mtext>std</mtext><mo>(</mo><mi>y</mi><mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y Subscript s t d Baseline equals StartFraction y minus mean
    left-parenthesis y right-parenthesis Over std left-parenthesis y right-parenthesis
    EndFraction" display="block"><mrow><msub><mi>y</mi> <mrow><mi>s</mi><mi>t</mi><mi>d</mi></mrow></msub>
    <mo>=</mo> <mfrac><mrow><mi>y</mi><mo>-</mo><mtext>mean</mtext><mo>(</mo><mi>y</mi><mo>)</mo></mrow>
    <mrow><mtext>std</mtext><mo>(</mo><mi>y</mi><mo>)</mo></mrow></mfrac></mrow></math>
- en: Suppose you standardize your *training* sample using the moments from the *complete*
    dataset, which of course includes the *test* sample. There are cases where these
    leaked moments provide extra information that wonâ€™t be available in production.
    Iâ€™ll provide an example later in this chapter that showcases this type of leakage.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ä½ ä½¿ç”¨æ¥è‡ª*å®Œæ•´*æ•°æ®é›†çš„æ—¶åˆ»æ ‡å‡†åŒ–*è®­ç»ƒ*æ ·æœ¬ï¼Œå½“ç„¶åŒ…æ‹¬*æµ‹è¯•*æ ·æœ¬ã€‚æœ‰äº›æƒ…å†µä¸‹ï¼Œè¿™äº›æ³„æ¼çš„æ—¶åˆ»æä¾›äº†é¢å¤–çš„ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯åœ¨ç”Ÿäº§ä¸­å°†ä¸å¯ç”¨ã€‚åœ¨æœ¬ç« çš„åé¢æˆ‘å°†æä¾›ä¸€ä¸ªç¤ºä¾‹ï¼Œå±•ç¤ºè¿™ç§ç±»å‹çš„æ³„æ¼ã€‚
- en: Detecting Data Leakage
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ£€æµ‹æ•°æ®æ³„æ¼
- en: If your model has *unreasonably superior* predictive performance, you should
    suspect that thereâ€™s data leakage. Not so long ago, a data scientist from my team
    was presenting the results from a classification model that had an area under
    the curve (AUC) of 1! You may recall that the AUC is bounded between 0 and 1,
    where an <math alttext="AUC equals 1"><mrow><mtext>AUC</mtext> <mo>=</mo> <mn>1</mn></mrow></math>
    means that you have a perfect prediction. This was clearly suspicious, to say
    the least.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çš„æ¨¡å‹å…·æœ‰*è¿‡äºä¼˜è¶Šçš„*é¢„æµ‹æ€§èƒ½ï¼Œæ‚¨åº”æ€€ç–‘å­˜åœ¨æ•°æ®æ³„éœ²ã€‚ä¸ä¹…å‰ï¼Œæˆ‘çš„å›¢é˜Ÿä¸­çš„ä¸€ä½æ•°æ®ç§‘å­¦å®¶å±•ç¤ºäº†ä¸€ä¸ªåˆ†ç±»æ¨¡å‹çš„ç»“æœï¼Œå…¶æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰ä¸º1ï¼æ‚¨å¯èƒ½è¿˜è®°å¾—ï¼ŒAUCçš„èŒƒå›´åœ¨0åˆ°1ä¹‹é—´ï¼Œå…¶ä¸­<math
    alttext="AUC equals 1"><mrow><mtext>AUC</mtext> <mo>=</mo> <mn>1</mn></mrow></math>è¡¨ç¤ºå®Œç¾é¢„æµ‹ã€‚è¿™æ˜¾ç„¶æ˜¯éå¸¸å¯ç–‘çš„ï¼Œè‡³å°‘å¯ä»¥è¿™ä¹ˆè¯´ã€‚
- en: These extreme cases of having a perfect prediction are quite rare. In classification
    settings, I get suspicious whenever I get an AUC > 0.8, but you shouldnâ€™t take
    this as a law written in stone. Itâ€™s more of a personal heuristic that Iâ€™ve found
    useful and informative with the class of problems I have encountered in my career.^([1](ch11.html#id641))
    In regression settings itâ€™s harder to come up with similar heuristics, since the
    most common performance metric, the mean square error, is bounded from below by
    zero, but it really depends on the scale of your outcome.^([2](ch11.html#id642))
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å®Œç¾é¢„æµ‹çš„æç«¯æƒ…å†µéå¸¸ç½•è§ã€‚åœ¨åˆ†ç±»è®¾ç½®ä¸­ï¼Œå½“æˆ‘è·å¾—AUC > 0.8æ—¶ï¼Œæˆ‘ä¼šæ„Ÿåˆ°æ€€ç–‘ï¼Œä½†æ‚¨ä¸åº”å°†å…¶è§†ä¸ºé“æ¿ä¸€å—çš„æ³•å¾‹ã€‚å¯¹äºæˆ‘åœ¨èŒä¸šç”Ÿæ¶¯ä¸­é‡åˆ°çš„é—®é¢˜ç±»åˆ«ï¼Œæˆ‘å‘ç°è¿™ç§ä¸ªäººå¯å‘å¼æ–¹æ³•éå¸¸æœ‰ç”¨å’Œä¿¡æ¯æ€§ã€‚^([1](ch11.html#id641))
    åœ¨å›å½’è®¾ç½®ä¸­ï¼Œè¦æå‡ºç±»ä¼¼çš„å¯å‘æ³•åˆ™è¦å›°éš¾å¾—å¤šï¼Œå› ä¸ºæœ€å¸¸è§çš„æ€§èƒ½æŒ‡æ ‡ï¼Œå‡æ–¹è¯¯å·®ï¼Œä»ä¸‹æ–¹å—åˆ°é™åˆ¶ï¼Œä½†å®ƒç¡®å®å–å†³äºæ‚¨ç»“æœçš„è§„æ¨¡ã€‚^([2](ch11.html#id642))
- en: Ultimately, the best way to detect leakage is by comparing the real-life performance
    of a productive model with the test sample performance. If the latter is considerably
    larger, and you can rule out model or data drift, then you should look for sources
    of data leakage.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆï¼Œæ£€æµ‹æ³„éœ²çš„æœ€ä½³æ–¹æ³•æ˜¯å°†ç”Ÿäº§æ¨¡å‹çš„å®é™…æ€§èƒ½ä¸æµ‹è¯•æ ·æœ¬æ€§èƒ½è¿›è¡Œæ¯”è¾ƒã€‚å¦‚æœåè€…æ˜¾è‘—è¾ƒå¤§ï¼Œå¹¶ä¸”æ‚¨å¯ä»¥æ’é™¤æ¨¡å‹æˆ–æ•°æ®æ¼‚ç§»ï¼Œåˆ™åº”å¯»æ‰¾æ•°æ®æ³„éœ²çš„æ¥æºã€‚
- en: Tip
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æç¤º
- en: Use your and your organizationâ€™s knowledge of the modeling problem at hand to
    decide what is a suspicious level of superior predictive performance. Many times,
    detecting data leakage only happens when you deploy a model in production and
    get subpar performance relative to that of your test sample.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ©ç”¨æ‚¨å’Œç»„ç»‡å¯¹æ‰‹å¤´å»ºæ¨¡é—®é¢˜çš„çŸ¥è¯†æ¥å†³å®šä½•ä¸ºå¼‚å¸¸ä¼˜è¶Šçš„é¢„æµ‹æ€§èƒ½æ°´å¹³ã€‚è®¸å¤šæ—¶å€™ï¼Œåªæœ‰åœ¨å°†æ¨¡å‹éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒå¹¶è·å¾—ä½äºæµ‹è¯•æ ·æœ¬çš„æ€§èƒ½æ—¶ï¼Œæ‰èƒ½æ£€æµ‹åˆ°æ•°æ®æ³„éœ²ã€‚
- en: 'To show the improved performance from data leakage, I ran Monte Carlo (MC)
    simulations for two of the examples described earlier. [FigureÂ 11-1](#ch11_leak_badcontrol)
    shows the impact of including a bad control: I train models with and without data
    leakage, and the plot shows the mean and 90% confidence intervals across MC simulations.
    The mean squared error (MSE) with leakage is around a quarter of when the bad
    control is not included. With [the code in the repo](https://oreil.ly/hi693),
    you can check that when the *bad control* is independent from the outcome, thereâ€™s
    no data leakage and the models have the same performance. You can also tweak the
    degree of autocorrelation to check that even a lagged bad control can create leakage.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å±•ç¤ºç”±æ•°æ®æ³„éœ²å¸¦æ¥çš„æ€§èƒ½æ”¹å–„ï¼Œæˆ‘å¯¹å‰è¿°ä¸¤ä¸ªç¤ºä¾‹è¿è¡Œäº†è’™ç‰¹å¡ç½—ï¼ˆMCï¼‰æ¨¡æ‹Ÿã€‚[å›¾Â 11-1](#ch11_leak_badcontrol) æ˜¾ç¤ºäº†åŒ…å«ä¸è‰¯æ§åˆ¶çš„å½±å“ï¼šæˆ‘åˆ†åˆ«è®­ç»ƒäº†å¸¦æœ‰å’Œä¸å¸¦æ•°æ®æ³„éœ²çš„æ¨¡å‹ï¼Œå¹¶ä¸”å›¾ä¸­æ˜¾ç¤ºäº†åœ¨MCæ¨¡æ‹Ÿä¸­çš„å‡å€¼å’Œ90%ç½®ä¿¡åŒºé—´ã€‚åœ¨ä¸è‰¯æ§åˆ¶ç‹¬ç«‹äºç»“æœæ—¶ï¼ŒMSEçº¦ä¸ºä¸åŒ…æ‹¬ä¸è‰¯æ§åˆ¶æ—¶çš„å››åˆ†ä¹‹ä¸€ã€‚é€šè¿‡[å­˜å‚¨åº“ä¸­çš„ä»£ç ](https://oreil.ly/hi693)ï¼Œæ‚¨å¯ä»¥æ£€æŸ¥ï¼Œå³ä½¿æ»åçš„ä¸è‰¯æ§åˆ¶ä¹Ÿä¼šé€ æˆæ³„éœ²ã€‚
- en: '![bad control leakage](assets/dshp_1101.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![bad control leakage](assets/dshp_1101.png)'
- en: Figure 11-1\. Data leakage with a bad control
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 11-1\. ä¸è‰¯æ§åˆ¶ä¸‹çš„æ•°æ®æ³„éœ²
- en: In the second example, Iâ€™ll show how bad standardization and leaking moments
    can affect the performance. [FigureÂ 11-2](#ch11_leak_scaling) presents mean MSE
    as well as 90% confidence intervals from an MC simulation using the following
    DGP:^([3](ch11.html#id643))
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬äºŒä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘å°†å±•ç¤ºæ ‡å‡†åŒ–ä¸è‰¯å’Œæ³„éœ²æ—¶åˆ»å¦‚ä½•å½±å“æ€§èƒ½ã€‚[å›¾Â 11-2](#ch11_leak_scaling) å±•ç¤ºäº†ä½¿ç”¨ä»¥ä¸‹ DGP è¿›è¡Œçš„ MC
    æ¨¡æ‹Ÿçš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä»¥åŠ 90% ç½®ä¿¡åŒºé—´ï¼š^([3](ch11.html#id643))
- en: <math alttext="StartLayout 1st Row 1st Column x Subscript t 2nd Column tilde
    3rd Column upper A upper R left-parenthesis 1 right-parenthesis with a trend 2nd
    Row 1st Column y Subscript t 2nd Column equals 3rd Column f left-parenthesis x
    Subscript t Baseline right-parenthesis plus epsilon Subscript t EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><msub><mi>x</mi> <mi>t</mi></msub></mtd>
    <mtd><mo>âˆ¼</mo></mtd> <mtd columnalign="left"><mrow><mi>A</mi> <mi>R</mi> <mo>(</mo>
    <mn>1</mn> <mo>)</mo> <mtext>with</mtext> <mtext>a</mtext> <mtext>trend</mtext></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi>y</mi> <mi>t</mi></msub></mtd> <mtd><mo>=</mo></mtd>
    <mtd columnalign="left"><mrow><mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>t</mi></msub>
    <mo>)</mo></mrow> <mo>+</mo> <msub><mi>Ïµ</mi> <mi>t</mi></msub></mrow></mtd></mtr></mtable></math>
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column x Subscript t 2nd Column tilde
    3rd Column upper A upper R left-parenthesis 1 right-parenthesis with a trend 2nd
    Row 1st Column y Subscript t 2nd Column equals 3rd Column f left-parenthesis x
    Subscript t Baseline right-parenthesis plus epsilon Subscript t EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><msub><mi>x</mi> <mi>t</mi></msub></mtd>
    <mtd><mo>âˆ¼</mo></mtd> <mtd columnalign="left"><mrow><mi>A</mi> <mi>R</mi> <mo>(</mo>
    <mn>1</mn> <mo>)</mo> <mtext>with</mtext> <mtext>a</mtext> <mtext>trend</mtext></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi>y</mi> <mi>t</mi></msub></mtd> <mtd><mo>=</mo></mtd>
    <mtd columnalign="left"><mrow><mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>t</mi></msub>
    <mo>)</mo></mrow> <mo>+</mo> <msub><mi>Ïµ</mi> <mi>t</mi></msub></mrow></mtd></mtr></mtable></math>
- en: I use the first half of the sample for training purposes and the second half
    to test the model. For the *leakage* condition, I standardize the features and
    outcome using the complete dataset mean and standard deviation; for the *no leakage*
    condition, I use the moments for each corresponding sample (train and test). As
    before, itâ€™s quite manifest how the data leakage artificially improves the performance.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä½¿ç”¨æ ·æœ¬çš„å‰åŠéƒ¨åˆ†è¿›è¡Œè®­ç»ƒï¼ŒååŠéƒ¨åˆ†ç”¨äºæµ‹è¯•æ¨¡å‹ã€‚å¯¹äº*æ³„éœ²*æ¡ä»¶ï¼Œæˆ‘ä½¿ç”¨å®Œæ•´æ•°æ®é›†çš„å‡å€¼å’Œæ ‡å‡†å·®å¯¹ç‰¹å¾å’Œç»“æœè¿›è¡Œæ ‡å‡†åŒ–ï¼›å¯¹äº*æ— æ³„éœ²*æ¡ä»¶ï¼Œæˆ‘ä½¿ç”¨æ¯ä¸ªç›¸åº”æ ·æœ¬ï¼ˆè®­ç»ƒå’Œæµ‹è¯•ï¼‰çš„ç¬æ—¶ã€‚æ­£å¦‚ä»¥å‰ä¸€æ ·ï¼Œæ•°æ®æ³„éœ²å¦‚ä½•äººä¸ºåœ°æé«˜äº†æ€§èƒ½æ˜¯éå¸¸æ˜æ˜¾çš„ã€‚
- en: '![scaling leakage](assets/dshp_1102.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![scaling leakage](assets/dshp_1102.png)'
- en: Figure 11-2\. Data leakage from incorrect scaling (MSE)
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾11-2\. ç”±äºä¸æ­£ç¡®çš„ç¼©æ”¾å¯¼è‡´çš„æ•°æ®æ³„éœ²ï¼ˆMSEï¼‰
- en: Whatâ€™s the logic behind this type of leakage problem? I decided to include a
    time trend so that the mean and standard deviation from the complete dataset informs
    the algorithm *at training time* that the outcome and feature are increasing,
    thereby providing extra information that wonâ€™t be available when the model is
    deployed. Without a trend component the leakage disappears, as you can check with
    [the code in the repo](https://oreil.ly/hi693).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ç±»å‹æ³„éœ²é—®é¢˜èƒŒåçš„é€»è¾‘æ˜¯ä»€ä¹ˆï¼Ÿæˆ‘å†³å®šåŠ å…¥ä¸€ä¸ªæ—¶é—´è¶‹åŠ¿ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒæ—¶ï¼Œå®Œæ•´æ•°æ®é›†çš„å‡å€¼å’Œæ ‡å‡†å·®é€šçŸ¥ç®—æ³•ç»“æœå’Œç‰¹å¾æ­£åœ¨å¢åŠ ï¼Œä»è€Œæä¾›é¢å¤–ä¿¡æ¯ï¼Œè¿™åœ¨æ¨¡å‹éƒ¨ç½²æ—¶å°†ä¸å¯ç”¨ã€‚å¦‚æœæ²¡æœ‰è¶‹åŠ¿ç»„ä»¶ï¼Œæ³„éœ²å°±ä¼šæ¶ˆå¤±ï¼Œæ‚¨å¯ä»¥é€šè¿‡[ä»“åº“ä¸­çš„ä»£ç ](https://oreil.ly/hi693)è¿›è¡Œæ£€æŸ¥ã€‚
- en: Complete Separation
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®Œå…¨åˆ†ç¦»
- en: Before moving on, I want to discuss the topic of *complete or quasi-complete
    separation*. In classification models, you may have an unusually high AUC because
    of this phenomenon, and this may or may not signal data leakage.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»§ç»­ä¹‹å‰ï¼Œæˆ‘æƒ³è®¨è®º*å®Œå…¨æˆ–å‡†å®Œå…¨åˆ†ç¦»*çš„è¯é¢˜ã€‚åœ¨åˆ†ç±»æ¨¡å‹ä¸­ï¼Œç”±äºè¿™ç§ç°è±¡ï¼Œæ‚¨å¯èƒ½ä¼šå¾—åˆ°å¼‚å¸¸é«˜çš„AUCå€¼ï¼Œè¿™å¯èƒ½ä¼šæˆ–å¯èƒ½ä¸ä¼šè¡¨æ˜æ•°æ®æ³„éœ²ã€‚
- en: '*Complete separation* arises in the context of linear classification (think
    logistic regression) when a linear combination of the features perfectly predicts
    the outcome *y*. In cases like these, the minimum loss function (many times, the
    negative of the log likelihood function) doesnâ€™t exist. This typically happens
    when the dataset is small, when you work with imbalanced data, or when you used
    a continuous variable and a threshold to create a categorical outcome *and* include
    the variable as a feature. In the latter case, there is data leakage.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*å®Œå…¨åˆ†ç¦»*å‡ºç°åœ¨çº¿æ€§åˆ†ç±»çš„èƒŒæ™¯ä¸‹ï¼ˆè€ƒè™‘é€»è¾‘å›å½’ï¼‰ï¼Œå½“ç‰¹å¾çš„çº¿æ€§ç»„åˆå®Œç¾é¢„æµ‹ç»“æœ*y*æ—¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ€å°æŸå¤±å‡½æ•°ï¼ˆè®¸å¤šæƒ…å†µä¸‹æ˜¯å¯¹æ•°ä¼¼ç„¶å‡½æ•°çš„è´Ÿæ•°ï¼‰ä¸å­˜åœ¨ã€‚è¿™é€šå¸¸å‘ç”Ÿåœ¨æ•°æ®é›†è¾ƒå°ã€å·¥ä½œäºä¸å¹³è¡¡æ•°æ®æˆ–è€…ä½¿ç”¨è¿ç»­å˜é‡å’Œé˜ˆå€¼åˆ›å»ºåˆ†ç±»ç»“æœ*å¹¶*å°†å˜é‡ä½œä¸ºç‰¹å¾æ—¶ã€‚åœ¨åä¸€ç§æƒ…å†µä¸‹ï¼Œå­˜åœ¨æ•°æ®æ³„éœ²ã€‚'
- en: '*Quasi-complete separation* arises when a linear combination of the features
    perfectly predicts a *subset* of your observations. This is much more common,
    and may happen when you include one or several dummy variables that, when combined,
    create a subset of observations for which thereâ€™s a perfect prediction. In this
    case, you may need to check if thereâ€™s data leakage or not. For instance, it may
    be that thereâ€™s a business rule that says that cross-selling can only be offered
    to customers who live in a given state and if they have a minimum tenure. If you
    include tenure and state dummy variables, you will have quasi-complete separation
    and data leakage.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*å‡†å®Œå…¨åˆ†ç¦»*æ˜¯æŒ‡ç‰¹å¾çš„çº¿æ€§ç»„åˆå®Œç¾é¢„æµ‹è§‚å¯Ÿç»“æœçš„*å­é›†*ã€‚è¿™ç§æƒ…å†µæ›´ä¸ºå¸¸è§ï¼Œå¯èƒ½æ˜¯ç”±äºåŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªè™šæ‹Ÿå˜é‡ï¼Œè¿™äº›å˜é‡ç»„åˆåœ¨ä¸€èµ·ä¼šåˆ›å»ºä¸€ä¸ªå®Œç¾é¢„æµ‹çš„è§‚å¯Ÿå­é›†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½éœ€è¦æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ•°æ®æ³„éœ²ã€‚ä¾‹å¦‚ï¼Œå¯èƒ½å­˜åœ¨ä¸€é¡¹ä¸šåŠ¡è§„åˆ™ï¼Œè§„å®šåªæœ‰å±…ä½åœ¨ç‰¹å®šå·ä¸”æœ‰æœ€ä½ä»»èŒæœŸçš„å®¢æˆ·æ‰èƒ½æä¾›äº¤å‰é”€å”®ã€‚å¦‚æœåŒ…æ‹¬äº†ä»»èŒæœŸå’Œå·çš„è™šæ‹Ÿå˜é‡ï¼Œå°†ä¼šå‡ºç°å‡†å®Œå…¨åˆ†ç¦»å’Œæ•°æ®æ³„éœ²çš„æƒ…å†µã€‚'
- en: 'Letâ€™s simulate a case where this happens by using a latent variable approach,
    as described in [ChapterÂ 9](ch09.html#ch09_simulation). The data generating process
    is as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡ä½¿ç”¨æ½œå˜é‡æ–¹æ³•æ¥æ¨¡æ‹Ÿè¿™ç§æƒ…å†µï¼Œå¦‚[ç¬¬9ç« ](ch09.html#ch09_simulation)æ‰€è¿°ã€‚æ•°æ®ç”Ÿæˆè¿‡ç¨‹å¦‚ä¸‹ï¼š
- en: <math alttext="StartLayout 1st Row 1st Column x 1 comma x 2 2nd Column tilde
    3rd Column upper N left-parenthesis 0 comma 1 right-parenthesis 2nd Row 1st Column
    z 2nd Column equals 3rd Column alpha 0 plus alpha 1 x 1 plus alpha 2 x 2 plus
    epsilon 3rd Row 1st Column y 2nd Column equals 3rd Column bold 1 Subscript z greater-than-or-equal-to
    0 4th Row 1st Column x Subscript 3 i 2nd Column equals 3rd Column StartLayout
    Enlarged left-brace 1st Row 1st Column 1 2nd Column Blank 3rd Column for i rand
    period selected from StartSet j colon y Subscript j Baseline equals 1 EndSet with
    probability p 2nd Row 1st Column 0 2nd Column Blank 3rd Column otherwise EndLayout
    EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub></mrow></mtd> <mtd><mo>âˆ¼</mo></mtd>
    <mtd columnalign="left"><mrow><mi>N</mi> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn>
    <mo>)</mo></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi>z</mi></mtd> <mtd><mo>=</mo></mtd>
    <mtd columnalign="left"><mrow><msub><mi>Î±</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>Î±</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>Î±</mi>
    <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mi>Ïµ</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mi>y</mi></mtd> <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><msub><mn
    mathvariant="bold">1</mn> <mrow><mi>z</mi><mo>â‰¥</mo><mn>0</mn></mrow></msub></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi>x</mi> <mrow><mn>3</mn><mi>i</mi></mrow></msub></mtd>
    <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mfenced close="" open="{" separators=""><mtable><mtr><mtd
    columnalign="left"><mn>1</mn></mtd> <mtd><mrow><mtext>for</mtext> <mi>i</mi> <mtext>rand.</mtext>
    <mtext>selected</mtext> <mtext>from</mtext> <mrow><mo>{</mo> <mi>j</mi> <mo>:</mo>
    <msub><mi>y</mi> <mi>j</mi></msub> <mo>=</mo> <mn>1</mn> <mo>}</mo></mrow> <mtext>with</mtext>
    <mtext>probability</mtext> <mi>p</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>0</mn></mtd>
    <mtd><mrow><mtext>otherwise</mtext></mrow></mtd></mtr></mtable></mfenced></mtd></mtr></mtable></math>
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column x 1 comma x 2 2nd Column tilde
    3rd Column upper N left-parenthesis 0 comma 1 right-parenthesis 2nd Row 1st Column
    z 2nd Column equals 3rd Column alpha 0 plus alpha 1 x 1 plus alpha 2 x 2 plus
    epsilon 3rd Row 1st Column y 2nd Column equals 3rd Column bold 1 Subscript z greater-than-or-equal-to
    0 4th Row 1st Column x Subscript 3 i 2nd Column equals 3rd Column StartLayout
    Enlarged left-brace 1st Row 1st Column 1 2nd Column Blank 3rd Column for i rand
    period selected from StartSet j colon y Subscript j Baseline equals 1 EndSet with
    probability p 2nd Row 1st Column 0 2nd Column Blank 3rd Column otherwise EndLayout
    EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub></mrow></mtd> <mtd><mo>âˆ¼</mo></mtd>
    <mtd columnalign="left"><mrow><mi>N</mi> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn>
    <mo>)</mo></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi>z</mi></mtd> <mtd><mo>=</mo></mtd>
    <mtd columnalign="left"><mrow><msub><mi>Î±</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>Î±</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>Î±</mi>
    <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mi>Ïµ</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mi>y</mi></mtd> <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><msub><mn
    mathvariant="bold">1</mn> <mrow><mi>z</mi><mo>â‰¥</mo><mn>0</mn></mrow></msub></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi>x</mi> <mrow><mn>3</mn><mi>i</mi></mrow></msub></mtd>
    <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mfenced close="" open="{" separators=""><mtable><mtr><mtd
    columnalign="left"><mn>1</mn></mtd> <mtd><mrow><mtext>for</mtext> <mi>i</mi> <mtext>rand.</mtext>
    <mtext>selected</mtext> <mtext>from</mtext> <mrow><mo>{</mo> <mi>j</mi> <mo>:</mo>
    <msub><mi>y</mi> <mi>j</mi></msub> <mo>=</mo> <mn>1</mn> <mo>}</mo></mrow> <mtext>with</mtext>
    <mtext>probability</mtext> <mi>p</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>0</mn></mtd>
    <mtd><mrow><mtext>otherwise</mtext></mrow></mtd></mtr></mtable></mfenced></mtd></mtr></mtable></math>
- en: where <math alttext="bold 1 Subscript z greater-than-or-equal-to 0"><msub><mn
    mathvariant="bold">1</mn> <mrow><mi>z</mi><mo>â‰¥</mo><mn>0</mn></mrow></msub></math>
    is an indicator variable that takes the value 1 when the condition on the subscript
    applies and 0 otherwise.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­<math alttext="bold 1 Subscript z greater-than-or-equal-to 0"><msub><mn mathvariant="bold">1</mn>
    <mrow><mi>z</mi><mo>â‰¥</mo><mn>0</mn></mrow></msub></math>æ˜¯ä¸€ä¸ªæŒ‡ç¤ºå˜é‡ï¼Œå½“ä¸‹æ ‡æ¡ä»¶é€‚ç”¨æ—¶å–å€¼ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚
- en: 'The idea is simple: the true DGP is a binomial latent variable model with two
    covariates, but I create a third feature, used at training time, by randomly selecting
    without replacement from the observations with *y[i]* = 1\. This way I can simulate
    different degrees of separation, including the case of complete and no separation
    ( <math alttext="p equals 1"><mrow><mi>p</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    and <math alttext="p equals 0"><mrow><mi>p</mi> <mo>=</mo> <mn>0</mn></mrow></math>
    , respectively). As usual, I train a logistic regression and a gradient boosting
    classifier (GBC) without metaparameter optimization.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ€è·¯å¾ˆç®€å•ï¼šçœŸå®çš„DGPæ˜¯ä¸€ä¸ªå…·æœ‰ä¸¤ä¸ªåå˜é‡çš„äºŒé¡¹æ½œå˜é‡æ¨¡å‹ï¼Œä½†æˆ‘åœ¨è®­ç»ƒæ—¶åˆ›å»ºäº†ç¬¬ä¸‰ä¸ªç‰¹å¾ï¼Œé€šè¿‡ä»*y[i]* = 1çš„è§‚æµ‹ä¸­æ— æ›¿æ¢éšæœºé€‰æ‹©ã€‚è¿™æ ·æˆ‘å¯ä»¥æ¨¡æ‹Ÿä¸åŒç¨‹åº¦çš„åˆ†ç¦»ï¼ŒåŒ…æ‹¬å®Œå…¨åˆ†ç¦»å’Œæ— åˆ†ç¦»çš„æƒ…å†µï¼ˆ
    <math alttext="p equals 1"><mrow><mi>p</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    å’Œ <math alttext="p equals 0"><mrow><mi>p</mi> <mo>=</mo> <mn>0</mn></mrow></math>
    ï¼Œåˆ†åˆ«ï¼‰ã€‚é€šå¸¸ï¼Œæˆ‘è®­ç»ƒä¸€ä¸ªé€»è¾‘å›å½’å’Œä¸€ä¸ªæ¢¯åº¦æå‡åˆ†ç±»å™¨ï¼ˆGBCï¼‰ï¼Œè€Œä¸è¿›è¡Œå…ƒå‚æ•°ä¼˜åŒ–ã€‚
- en: I ran an MC simulation, and [FigureÂ 11-3](#ch11_qcs_lift) plots the lift in
    median AUC on the test sample across all experiments, where I benchmark everything
    with respect to the case of no separation. You can see that separation creates
    an increased AUC of up to 10% to 15% relative to the baseline, depending on whether
    I use a logistic regression or GBC.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿›è¡Œäº†ä¸€æ¬¡MCæ¨¡æ‹Ÿï¼Œ[å›¾Â 11-3](#ch11_qcs_lift) å±•ç¤ºäº†åœ¨æ‰€æœ‰å®éªŒä¸­æµ‹è¯•æ ·æœ¬ä¸Šçš„ä¸­ä½æ•°AUCæå‡ï¼Œæˆ‘å°†æ‰€æœ‰æƒ…å†µä¸æ— åˆ†ç¦»æƒ…å†µè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚æ‚¨å¯ä»¥çœ‹åˆ°ï¼Œç›¸å¯¹äºåŸºçº¿æƒ…å†µï¼Œåˆ†ç¦»å¯ä»¥ä½¿AUCå¢åŠ 10%è‡³15%ï¼Œå…·ä½“å–å†³äºæ˜¯å¦ä½¿ç”¨é€»è¾‘å›å½’æˆ–GBCã€‚
- en: '![lift in auc separation](assets/dshp_1103.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![lift in auc separation](assets/dshp_1103.png)'
- en: Figure 11-3\. Lift in AUC for quasi-complete separation
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾11-3\. å‡†å®Œå…¨åˆ†ç¦»çš„AUCæå‡
- en: The lesson here is that separation increases the AUC in a classification setting,
    and this may indicate data leakage that needs to be inspected further.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„æ•™è®­æ˜¯ï¼Œåœ¨åˆ†ç±»è®¾ç½®ä¸­ï¼Œåˆ†ç¦»å¢åŠ äº†AUCï¼Œè¿™å¯èƒ½è¡¨æ˜éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥çš„æ•°æ®æ³„éœ²ã€‚
- en: Windowing Methodology
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: çª—å£æ–¹æ³•
- en: I will now describe a windowing methodology that should help reduce the likelihood
    of data leakage in your model. As described earlier, data leakage can occur for
    many different reasons, so this is in no way a bulletproof technique. Nonetheless,
    Iâ€™ve found that it helps you discipline the process of training a model, and reduces
    some of the most obvious risks of leakage.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘å°†æè¿°ä¸€ç§çª—å£æ–¹æ³•ï¼Œå¯ä»¥å¸®åŠ©å‡å°‘æ¨¡å‹ä¸­æ•°æ®æ³„éœ²çš„å¯èƒ½æ€§ã€‚å¦‚å‰æ‰€è¿°ï¼Œæ•°æ®æ³„æ¼å¯èƒ½å‡ºç°æœ‰è®¸å¤šä¸åŒçš„åŸå› ï¼Œå› æ­¤è¿™ç»ä¸æ˜¯ä¸€ç§ä¸‡æ— ä¸€å¤±çš„æŠ€æœ¯ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘å‘ç°å®ƒæœ‰åŠ©äºè§„èŒƒè®­ç»ƒæ¨¡å‹çš„è¿‡ç¨‹ï¼Œå¹¶å‡å°‘ä¸€äº›æ˜æ˜¾çš„æ³„éœ²é£é™©ã€‚
- en: 'As a starting point, I separate the learning process into two stages:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: èµ·å§‹ç‚¹æ˜¯ï¼Œæˆ‘å°†å­¦ä¹ è¿‡ç¨‹åˆ†æˆä¸¤ä¸ªé˜¶æ®µï¼š
- en: Training stage
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒé˜¶æ®µ
- en: This is the stage where you train a model by dividing your sample into training
    and testing, etc.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æ‚¨å°†æ ·æœ¬åˆ†æˆè®­ç»ƒå’Œæµ‹è¯•ç­‰çš„é˜¶æ®µã€‚
- en: Scoring stage
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„åˆ†é˜¶æ®µ
- en: Once youâ€™ve trained your model and youâ€™ve deployed it in production, you use
    it to score a sample. It can be a one-at-a-time prediction, as in online, real-time
    scoring, or scoring of a larger sample.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‚¨è®­ç»ƒå¥½æ¨¡å‹å¹¶å°†å…¶éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæ‚¨å¯ä»¥ç”¨å®ƒæ¥å¯¹æ ·æœ¬è¿›è¡Œè¯„åˆ†ã€‚å¯ä»¥æ˜¯é€ä¸ªé¢„æµ‹ï¼Œæ¯”å¦‚åœ¨çº¿å®æ—¶è¯„åˆ†ï¼Œæˆ–è€…å¯¹è¾ƒå¤§æ ·æœ¬è¿›è¡Œè¯„åˆ†ã€‚
- en: Itâ€™s easy to forget, but in machine learning (ML), *the scoring stage reigns*.
    Itâ€™s so important that I devote [ChapterÂ 12](ch12.html#ch12_productionML) to discuss
    some necessary properties and processes that need to be set up to ensure that
    this stage is at its best. For now, just remember that this stage is where most
    value is created, and since this should be your North Star, it should be granted
    nobility status.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå®¹æ˜“å¿˜è®°ï¼Œåœ¨æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä¸­ï¼Œ*è¯„åˆ†é˜¶æ®µè‡³å…³é‡è¦*ã€‚è¿™ä¹ˆé‡è¦ä»¥è‡³äºæˆ‘ä¸“é—¨åœ¨[ç¬¬12ç« ](ch12.html#ch12_productionML)è®¨è®ºäº†éœ€è¦è®¾ç½®çš„ä¸€äº›å¿…è¦å±æ€§å’Œæµç¨‹ï¼Œä»¥ç¡®ä¿è¯¥é˜¶æ®µè¾¾åˆ°æœ€ä½³çŠ¶æ€ã€‚ç°åœ¨ï¼Œåªéœ€è®°ä½ï¼Œè¿™ä¸ªé˜¶æ®µæ˜¯åˆ›é€ æœ€å¤§ä»·å€¼çš„åœ°æ–¹ï¼Œå› æ­¤åº”è¢«èµ‹äºˆè´µæ—èˆ¬çš„åœ°ä½ã€‚
- en: Tip
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å°è´´å£«
- en: In ML, the scoring stage takes the leading role, and everything else should
    be set up to maximize the quality and timeliness of the predictions of this stage.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œè¯„åˆ†é˜¶æ®µèµ·ç€ä¸»å¯¼ä½œç”¨ï¼Œå…¶ä»–æ‰€æœ‰äº‹æƒ…éƒ½åº”è®¾ç½®ä¸ºæœ€å¤§åŒ–è¯¥é˜¶æ®µé¢„æµ‹è´¨é‡å’ŒåŠæ—¶æ€§ã€‚
- en: '[FigureÂ 11-4](#ch11_windows) shows how the window methodology works. The starting
    point is the scoring stage (downmost timeline). Suppose you want to make a prediction
    at time <math alttext="t Subscript p"><msub><mi>t</mi> <mi>p</mi></msub></math>
    . This time period serves to divide the world into two windows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[å›¾Â 11-4](#ch11_windows) å±•ç¤ºäº†çª—å£æ–¹æ³•çš„å·¥ä½œåŸç†ã€‚èµ·å§‹ç‚¹æ˜¯è¯„åˆ†é˜¶æ®µï¼ˆæœ€åº•éƒ¨æ—¶é—´è½´ï¼‰ã€‚å‡è®¾æ‚¨æƒ³åœ¨æ—¶é—´ <math alttext="t
    Subscript p"><msub><mi>t</mi> <mi>p</mi></msub></math> è¿›è¡Œé¢„æµ‹ã€‚è¿™ä¸ªæ—¶é—´æ®µç”¨æ¥å°†ä¸–ç•Œåˆ†ä¸ºä¸¤ä¸ªçª—å£ï¼š'
- en: Prediction window
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹çª—å£
- en: Youâ€™re usually interested in predicting an event or a random variable associated
    to an event. For this you need to set up a prediction window for that event to
    occur ( <math alttext="t Subscript p Baseline comma t Subscript p Baseline plus
    upper P"><mrow><msub><mi>t</mi> <mi>p</mi></msub> <mo>,</mo> <msub><mi>t</mi>
    <mi>p</mi></msub> <mo>+</mo> <mi>P</mi></mrow></math> ]. For example, you want
    to predict if a customer will churn *in the next 30 days*. Or you want to predict
    your companyâ€™s revenue *during the first quarter of the year*. Or you may want
    to predict if a customer will rate a book or a movie in *the next two weeks after*
    finishing reading or watching it.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ä½ å¯¹äº‹ä»¶æˆ–ä¸äº‹ä»¶ç›¸å…³çš„éšæœºå˜é‡æ„Ÿå…´è¶£ã€‚ä¸ºæ­¤ï¼Œæ‚¨éœ€è¦è®¾ç½®ä¸€ä¸ªé¢„æµ‹çª—å£ï¼Œä»¥ä¾¿äº‹ä»¶å‘ç”Ÿï¼ˆ <math alttext="t Subscript p Baseline
    comma t Subscript p Baseline plus upper P"><mrow><msub><mi>t</mi> <mi>p</mi></msub>
    <mo>,</mo> <msub><mi>t</mi> <mi>p</mi></msub> <mo>+</mo> <mi>P</mi></mrow></math>
    ï¼‰ã€‚ä¾‹å¦‚ï¼Œæ‚¨æƒ³é¢„æµ‹å®¢æˆ·æ˜¯å¦ä¼š*åœ¨æ¥ä¸‹æ¥çš„30å¤©å†…æµå¤±*ã€‚æˆ–è€…æ‚¨æƒ³é¢„æµ‹å…¬å¸åœ¨*å¹´åˆçš„ç¬¬ä¸€å­£åº¦çš„æ”¶å…¥*ã€‚æˆ–è€…æ‚¨å¯èƒ½æƒ³é¢„æµ‹å®¢æˆ·*åœ¨é˜…è¯»æˆ–è§‚çœ‹ä¹¦ç±æˆ–ç”µå½±åçš„æ¥ä¸‹æ¥ä¸¤å‘¨å†…*ä¼šå¯¹å…¶è¿›è¡Œè¯„åˆ†ã€‚
- en: Observation window
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è§‚å¯Ÿçª—å£
- en: Once you define a time horizon for your prediction to be evaluated, you need
    to define how much history you want to include to inform your prediction [ <math
    alttext="t Subscript p Baseline minus upper O comma t Subscript p Baseline"><mrow><msub><mi>t</mi>
    <mi>p</mi></msub> <mo>-</mo> <mi>O</mi> <mo>,</mo> <msub><mi>t</mi> <mi>p</mi></msub></mrow></math>
    ]. The name is derived from the fact that this, and only this, is the information
    we *observe* at scoring time.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‚¨å®šä¹‰äº†è¯„ä¼°é¢„æµ‹çš„æ—¶é—´èŒƒå›´ï¼Œæ‚¨éœ€è¦å®šä¹‰è¦åŒ…å«å¤šå°‘å†å²ä¿¡æ¯ä»¥æ”¯æŒæ‚¨çš„é¢„æµ‹ [ <math alttext="t Subscript p Baseline
    minus upper O comma t Subscript p Baseline"><mrow><msub><mi>t</mi> <mi>p</mi></msub>
    <mo>-</mo> <mi>O</mi> <mo>,</mo> <msub><mi>t</mi> <mi>p</mi></msub></mrow></math>
    ]ã€‚å…¶åç§°æºè‡ªäºè¿™ä¸€ç‚¹ï¼Œå› ä¸ºè¿™æ˜¯æˆ‘ä»¬åœ¨è¯„åˆ†æ—¶*è§‚å¯Ÿ*åˆ°çš„å”¯ä¸€ä¿¡æ¯ã€‚
- en: 'Note that the prediction window is *open* on the left by design: this helps
    prevent data leakage as it explicitly separates what you observe at the time of
    prediction.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œé¢„æµ‹çª—å£åœ¨å·¦ä¾§æ˜¯*å¼€æ”¾*çš„è®¾è®¡ï¼šè¿™æœ‰åŠ©äºé˜²æ­¢æ•°æ®æ³„æ¼ï¼Œå› ä¸ºå®ƒæ˜ç¡®åˆ†ç¦»äº†æ‚¨åœ¨é¢„æµ‹æ—¶æ‰€è§‚å¯Ÿåˆ°çš„å†…å®¹ã€‚
- en: '![windowing methodology](assets/dshp_1104.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![çª—å£æ–¹æ³•è®º](assets/dshp_1104.png)'
- en: Figure 11-4\. Windowing methodology
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾11-4\. çª—å£æ–¹æ³•è®º
- en: Letâ€™s go through an example to ensure that these concepts are clear. I want
    to train a churn model that predicts for each customer the likelihood they will
    churn during the next month. Since the scoring stage reigns, suppose I want to
    score all of the active users *today* ( <math alttext="t Subscript p"><msub><mi>t</mi>
    <mi>p</mi></msub></math> ). By definition, the prediction window starts from tomorrow
    and ends one month from tomorrow. At that point I have to be able to assess whether
    any of the customers churned or not. To make this prediction, I will use the last
    three months of information, so this is my observation window. Any transformations
    of the features are restricted to this timeframe. For instance, I may think that
    the most recent past matters, so I can compute the ratio of weekly interactions
    four weeks ago to last weekâ€™s (if the ratio is larger than one, engagement increased
    this last month).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç¤ºä¾‹æ¥ç¡®ä¿è¿™äº›æ¦‚å¿µæ¸…æ¥šã€‚æˆ‘æƒ³è¦è®­ç»ƒä¸€ä¸ªæµå¤±æ¨¡å‹ï¼Œé¢„æµ‹æ¯ä¸ªå®¢æˆ·åœ¨ä¸‹ä¸ªæœˆå†…æµå¤±çš„å¯èƒ½æ€§ã€‚ç”±äºè¯„åˆ†é˜¶æ®µè‡³ä¸Šï¼Œå‡è®¾æˆ‘å¸Œæœ›*ä»Šå¤©*è¯„åˆ†æ‰€æœ‰æ´»è·ƒç”¨æˆ·ï¼ˆ
    <math alttext="t Subscript p"><msub><mi>t</mi> <mi>p</mi></msub></math> ï¼‰ã€‚æŒ‰ç…§å®šä¹‰ï¼Œé¢„æµ‹çª—å£ä»æ˜å¤©å¼€å§‹ï¼Œåˆ°æ˜å¤©åä¸€ä¸ªæœˆç»“æŸã€‚åœ¨é‚£æ—¶ï¼Œæˆ‘å¿…é¡»èƒ½å¤Ÿè¯„ä¼°å®¢æˆ·æ˜¯å¦æœ‰æµå¤±ã€‚ä¸ºäº†è¿›è¡Œè¿™ç§é¢„æµ‹ï¼Œæˆ‘å°†ä½¿ç”¨æœ€è¿‘ä¸‰ä¸ªæœˆçš„ä¿¡æ¯ï¼Œè¿™æ˜¯æˆ‘çš„è§‚å¯Ÿçª—å£ã€‚ä»»ä½•ç‰¹å¾çš„è½¬æ¢éƒ½é™åˆ¶åœ¨è¿™ä¸ªæ—¶é—´èŒƒå›´å†…ã€‚ä¾‹å¦‚ï¼Œæˆ‘å¯èƒ½è®¤ä¸ºæœ€è¿‘çš„è¿‡å»å¾ˆé‡è¦ï¼Œå› æ­¤æˆ‘å¯ä»¥è®¡ç®—å››å‘¨å‰çš„æ¯å‘¨äº’åŠ¨ä¸ä¸Šå‘¨çš„æ¯”ç‡ï¼ˆå¦‚æœæ¯”ç‡å¤§äºä¸€ï¼Œåˆ™è¡¨ç¤ºä¸Šä¸ªæœˆçš„å‚ä¸åº¦å¢åŠ ï¼‰ã€‚
- en: Choosing the Length of the Windows
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€‰æ‹©çª—å£é•¿åº¦
- en: You may wonder who chooses the lengths of the observation and prediction windows,
    and what considerations are taken into account. [TableÂ 11-1](#table-11-1) summarizes
    some of the main considerations when deciding the lengths of both windows.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯èƒ½æƒ³çŸ¥é“æ˜¯è°é€‰æ‹©è§‚å¯Ÿå’Œé¢„æµ‹çª—å£çš„é•¿åº¦ï¼Œå¹¶è€ƒè™‘äº†å“ªäº›å› ç´ ã€‚[è¡¨Â 11-1](#table-11-1) æ€»ç»“äº†åœ¨ç¡®å®šè¿™ä¸¤ä¸ªçª—å£é•¿åº¦æ—¶çš„ä¸€äº›ä¸»è¦è€ƒè™‘å› ç´ ã€‚
- en: Table 11-1\. Considerations when choosing window lengths
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨11-1\. é€‰æ‹©çª—å£é•¿åº¦æ—¶çš„è€ƒè™‘å› ç´ 
- en: '|  | Prediction (*P*) | Observation (*O*) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | é¢„æµ‹ (*P*) | è§‚å¯Ÿ (*O*) |'
- en: '| --- | --- | --- |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Owner | Business - data scientist | Data scientist |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| æ‹¥æœ‰è€… | ä¸šåŠ¡ - æ•°æ®ç§‘å­¦å®¶ | æ•°æ®ç§‘å­¦å®¶ |'
- en: '| Predictive performance | Feasibility of short- vs. long-term prediction |
    Relative weight of distant past |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| é¢„æµ‹è¡¨ç° | çŸ­æœŸä¸é•¿æœŸé¢„æµ‹çš„å¯è¡Œæ€§ | è¿œæœŸè¿‡å»çš„ç›¸å¯¹æƒé‡ |'
- en: '| Data | Historical data at your disposal | Historical data at your disposal
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| æ•°æ® | æ‚¨å¯ä»¥ä½¿ç”¨çš„å†å²æ•°æ® | æ‚¨å¯ä»¥ä½¿ç”¨çš„å†å²æ•°æ® |'
- en: The length of the observation window is chosen by the data scientist, primarily
    based on the predictive performance of the model. For instance, is the recent
    past more predictive? The prediction window is primarily chosen by taking into
    account business considerations regarding the timeliness of a decision, and as
    such, it ought to be mostly owned by the business stakeholder.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è§‚å¯Ÿçª—å£çš„é•¿åº¦ç”±æ•°æ®ç§‘å­¦å®¶é€‰æ‹©ï¼Œä¸»è¦åŸºäºæ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œæœ€è¿‘çš„è¿‡å»æ›´å…·é¢„æµ‹æ€§å—ï¼Ÿé¢„æµ‹çª—å£ä¸»è¦æ˜¯é€šè¿‡è€ƒè™‘ä¸šåŠ¡å†³ç­–çš„åŠæ—¶æ€§è€Œé€‰æ‹©çš„ï¼Œå› æ­¤å®ƒåº”è¯¥ä¸»è¦ç”±ä¸šåŠ¡åˆ©ç›Šç›¸å…³è€…æ‹¥æœ‰ã€‚
- en: Itâ€™s important to acknowledge that longer prediction windows are usually less
    risky, in the sense that itâ€™s harder to be wrong (for example, predicting the
    existence of artificial general intelligence in the next one thousand years versus
    the next two years). But really short time horizons may be infeasible given the
    current granularity of your data (for example, predicting if a customer will churn
    in the next 10 minutes when you only have daily data).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦çš„æ˜¯è¦è®¤è¯†åˆ°ï¼Œè¾ƒé•¿çš„é¢„æµ‹çª—å£é€šå¸¸é£é™©è¾ƒå°ï¼Œå› ä¸ºæ›´éš¾å‡ºé”™ï¼ˆä¾‹å¦‚ï¼Œåœ¨æ¥ä¸‹æ¥çš„ä¸€åƒå¹´å†…é¢„æµ‹äººå·¥æ™ºèƒ½çš„å­˜åœ¨ä¸åœ¨æ¥ä¸‹æ¥çš„ä¸¤å¹´å†…ï¼‰ã€‚ä½†åœ¨å½“å‰æ•°æ®ç²’åº¦ä¸‹ï¼ŒçœŸæ­£çŸ­çš„æ—¶é—´è§†é‡å¯èƒ½æ˜¯ä¸å¯è¡Œçš„ï¼ˆä¾‹å¦‚ï¼Œå½“æ‚¨åªæœ‰æ¯æ—¥æ•°æ®æ—¶ï¼Œé¢„æµ‹å®¢æˆ·åœ¨æ¥ä¸‹æ¥çš„10åˆ†é’Ÿå†…æ˜¯å¦ä¼šæµå¤±ï¼‰ã€‚
- en: Finally, the length of the prediction window affects how long the observation
    window must be. If the CFO asks me to predict revenue over the next five years,
    I can either have short observation windows and dynamic forecasts (where forecasts
    are used successively as features), or I can have a long enough observation window
    to make such a heroic forecast.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œé¢„æµ‹çª—å£çš„é•¿åº¦ä¼šå½±å“è§‚å¯Ÿçª—å£çš„å¿…è¦é•¿åº¦ã€‚å¦‚æœé¦–å¸­è´¢åŠ¡å®˜è¦æ±‚æˆ‘é¢„æµ‹æœªæ¥äº”å¹´çš„æ”¶å…¥ï¼Œæˆ‘å¯ä»¥ä½¿ç”¨è¾ƒçŸ­çš„è§‚å¯Ÿçª—å£å’ŒåŠ¨æ€é¢„æµ‹ï¼ˆå…¶ä¸­è¿ç»­ä½¿ç”¨é¢„æµ‹ä½œä¸ºç‰¹å¾ï¼‰ï¼Œæˆ–è€…æˆ‘å¯ä»¥ä½¿ç”¨è¶³å¤Ÿé•¿çš„è§‚å¯Ÿçª—å£æ¥è¿›è¡Œè¿™æ ·çš„è‹±é›„èˆ¬é¢„æµ‹ã€‚
- en: The Training Stage Mirrors the Scoring Stage
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒé˜¶æ®µé•œåƒè¯„åˆ†é˜¶æ®µ
- en: 'Once these windows are defined at the scoring stage, youâ€™re now ready to set
    up and define the training stage. As you might suspect from [FigureÂ 11-4](#ch11_windows),
    the training stage should always mirror what happens in the later scoring stage:
    the observation and prediction windows for the training stage map one-to-one to
    those at the scoring stage, and are thus constrained by them.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦è¿™äº›çª—å£åœ¨è¯„åˆ†é˜¶æ®µè¢«å®šä¹‰ï¼Œæ‚¨ç°åœ¨å¯ä»¥å‡†å¤‡è®¾ç½®å’Œå®šä¹‰è®­ç»ƒé˜¶æ®µã€‚æ­£å¦‚æ‚¨å¯èƒ½ä»[å›¾Â 11-4](#ch11_windows)ä¸­æ¨æµ‹åˆ°çš„é‚£æ ·ï¼Œè®­ç»ƒé˜¶æ®µåº”è¯¥å§‹ç»ˆä¸ç¨åè¯„åˆ†é˜¶æ®µçš„æƒ…å†µç›¸ä¸€è‡´ï¼šè®­ç»ƒé˜¶æ®µçš„è§‚å¯Ÿå’Œé¢„æµ‹çª—å£ä¸è¯„åˆ†é˜¶æ®µçš„çª—å£ä¸€ä¸€æ˜ å°„ï¼Œå¹¶å› æ­¤å—å…¶çº¦æŸã€‚
- en: For instance, itâ€™s quite customary that you want to train the model with the
    most recent data at your disposal. Since youâ€™ll need <math alttext="upper P"><mi>P</mi></math>
    time periods to evaluate your prediction and <math alttext="upper O"><mi>O</mi></math>
    periods to create your features, this means that you need to set [ <math alttext="t
    Subscript p Baseline minus upper P minus upper O comma t Subscript p Baseline
    minus upper P"><mrow><msub><mi>t</mi> <mi>p</mi></msub> <mo>-</mo> <mi>P</mi>
    <mo>-</mo> <mi>O</mi> <mo>,</mo> <msub><mi>t</mi> <mi>p</mi></msub> <mo>-</mo>
    <mi>P</mi></mrow></math> ] as your training observation window, and ( <math alttext="t
    Subscript p Baseline minus upper P comma t Subscript p Baseline"><mrow><msub><mi>t</mi>
    <mi>p</mi></msub> <mo>-</mo> <mi>P</mi> <mo>,</mo> <msub><mi>t</mi> <mi>p</mi></msub></mrow></math>
    ] as your training prediction window.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œé€šå¸¸æƒ…å†µä¸‹ï¼Œæ‚¨å¸Œæœ›ä½¿ç”¨æ‰‹å¤´æœ€æ–°çš„æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚ç”±äºæ‚¨éœ€è¦<math alttext="upper P"><mi>P</mi></math>ä¸ªæ—¶é—´æ®µæ¥è¯„ä¼°æ‚¨çš„é¢„æµ‹ï¼Œä»¥åŠ<math
    alttext="upper O"><mi>O</mi></math>ä¸ªæ—¶é—´æ®µæ¥åˆ›å»ºç‰¹å¾ï¼Œè¿™æ„å‘³ç€æ‚¨éœ€è¦å°†[ <math alttext="t Subscript
    p Baseline minus upper P minus upper O comma t Subscript p Baseline minus upper
    P"><mrow><msub><mi>t</mi> <mi>p</mi></msub> <mo>-</mo> <mi>P</mi> <mo>-</mo> <mi>O</mi>
    <mo>,</mo> <msub><mi>t</mi> <mi>p</mi></msub> <mo>-</mo> <mi>P</mi></mrow></math>
    ]è®¾ä¸ºæ‚¨çš„è®­ç»ƒè§‚å¯Ÿçª—å£ï¼Œä»¥åŠ ( <math alttext="t Subscript p Baseline minus upper P comma t Subscript
    p Baseline"><mrow><msub><mi>t</mi> <mi>p</mi></msub> <mo>-</mo> <mi>P</mi> <mo>,</mo>
    <msub><mi>t</mi> <mi>p</mi></msub></mrow></math> ] ä½œä¸ºæ‚¨çš„è®­ç»ƒé¢„æµ‹çª—å£ã€‚
- en: 'Formally defining these windows helps you discipline and constrain what you
    expect to accomplish with the model. On the one hand, it ensures that only historical
    data is used for future predictions, preventing common leakage issues. You can
    see this more directly in the following equations:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¼å®šä¹‰è¿™äº›çª—å£æœ‰åŠ©äºæ‚¨çº¦æŸå’Œé™åˆ¶æ¨¡å‹çš„é¢„æœŸæˆæœã€‚ä¸€æ–¹é¢ï¼Œå®ƒç¡®ä¿åªä½¿ç”¨å†å²æ•°æ®è¿›è¡Œæœªæ¥é¢„æµ‹ï¼Œé¿å…å¸¸è§çš„æ³„æ¼é—®é¢˜ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹æ–¹ç¨‹ä¸­æ›´ç›´æ¥åœ°çœ‹åˆ°è¿™ä¸€ç‚¹ï¼š
- en: <math alttext="StartLayout 1st Row 1st Column Scoring 2nd Column colon 3rd Column
    y Subscript left-parenthesis t Sub Subscript p Subscript comma t Sub Subscript
    p Subscript plus upper P right-bracket Baseline equals f left-parenthesis upper
    X Subscript left-bracket t Sub Subscript p Subscript minus upper O comma t Sub
    Subscript p Subscript right-bracket Baseline right-parenthesis 2nd Row 1st Column
    Training 2nd Column colon 3rd Column y Subscript left-parenthesis t Sub Subscript
    p Subscript minus upper P comma t Sub Subscript p Subscript right-bracket Baseline
    equals f left-parenthesis upper X Subscript left-bracket t Sub Subscript p Subscript
    minus upper P minus upper O comma t Sub Subscript p Subscript minus upper P right-bracket
    Baseline right-parenthesis EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mtext>Scoring</mtext></mrow></mtd> <mtd><mo>:</mo></mtd>
    <mtd columnalign="left"><mrow><msub><mi>y</mi> <mrow><mo>(</mo><msub><mi>t</mi>
    <mi>p</mi></msub> <mo>,</mo><msub><mi>t</mi> <mi>p</mi></msub> <mo>+</mo><mi>P</mi><mo>]</mo></mrow></msub>
    <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>X</mi> <mrow><mo>[</mo><msub><mi>t</mi>
    <mi>p</mi></msub> <mo>-</mo><mi>O</mi><mo>,</mo><msub><mi>t</mi> <mi>p</mi></msub>
    <mo>]</mo></mrow></msub> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mtext>Training</mtext></mrow></mtd>
    <mtd><mo>:</mo></mtd> <mtd columnalign="left"><mrow><msub><mi>y</mi> <mrow><mo>(</mo><msub><mi>t</mi>
    <mi>p</mi></msub> <mo>-</mo><mi>P</mi><mo>,</mo><msub><mi>t</mi> <mi>p</mi></msub>
    <mo>]</mo></mrow></msub> <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>X</mi>
    <mrow><mo>[</mo><msub><mi>t</mi> <mi>p</mi></msub> <mo>-</mo><mi>P</mi><mo>-</mo><mi>O</mi><mo>,</mo><msub><mi>t</mi>
    <mi>p</mi></msub> <mo>-</mo><mi>P</mi><mo>]</mo></mrow></msub> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column Scoring 2nd Column colon 3rd Column
    y Subscript left-parenthesis t Sub Subscript p Subscript comma t Sub Subscript
    p Subscript plus upper P right-bracket Baseline equals f left-parenthesis upper
    X Subscript left-bracket t Sub Subscript p Subscript minus upper O comma t Sub
    Subscript p Subscript right-bracket Baseline right-parenthesis 2nd Row 1st Column
    Training 2nd Column colon 3rd Column y Subscript left-parenthesis t Sub Subscript
    p Subscript minus upper P comma t Sub Subscript p Subscript right-bracket Baseline
    equals f left-parenthesis upper X Subscript left-bracket t Sub Subscript p Subscript
    minus upper P minus upper O comma t Sub Subscript p Subscript minus upper P right-bracket
    Baseline right-parenthesis EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mtext>Scoring</mtext></mrow></mtd> <mtd><mo>:</mo></mtd>
    <mtd columnalign="left"><mrow><msub><mi>y</mi> <mrow><mo>(</mo><msub><mi>t</mi>
    <mi>p</mi></msub> <mo>,</mo><msub><mi>t</mi> <mi>p</mi></msub> <mo>+</mo><mi>P</mi><mo>]</mo></mrow></msub>
    <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>X</mi> <mrow><mo>[</mo><msub><mi>t</mi>
    <mi>p</mi></msub> <mo>-</mo><mi>O</mi><mo>,</mo><msub><mi>t</mi> <mi>p</mi></msub>
    <mo>]</mo></mrow></msub> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mtext>Training</mtext></mrow></mtd>
    <mtd><mo>:</mo></mtd> <mtd columnalign="left"><mrow><msub><mi>y</mi> <mrow><mo>(</mo><msub><mi>t</mi>
    <mi>p</mi></msub> <mo>-</mo><mi>P</mi><mo>,</mo><msub><mi>t</mi> <mi>p</mi></msub>
    <mo>]</mo></mrow></msub> <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>X</mi>
    <mrow><mo>[</mo><msub><mi>t</mi> <mi>p</mi></msub> <mo>-</mo><mi>P</mi><mo>-</mo><mi>O</mi><mo>,</mo><msub><mi>t</mi>
    <mi>p</mi></msub> <mo>-</mo><mi>P</mi><mo>]</mo></mrow></msub> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
- en: Implementing the Windowing Methodology
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®æ–½çª—å£åŒ–æ–¹æ³•
- en: 'Once you have defined them, you can enforce these on your code with something
    like the following snippet:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å®šä¹‰äº†è¿™äº›çª—å£ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç æ®µåœ¨ä»£ç ä¸­åŠ ä»¥å®æ–½ï¼š
- en: '[PRE2]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Summing up, the window methodology helps you enforce a minimal requirement that
    you can only use the past to predict the future. Other causes of data leakage
    may still be present.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“ä¸€ä¸‹ï¼Œçª—å£æ–¹æ³•å¸®åŠ©ä½ å¼ºåˆ¶æ‰§è¡Œä¸€ä¸ªæœ€å°è¦æ±‚ï¼Œå³ä½ åªèƒ½ä½¿ç”¨è¿‡å»æ¥é¢„æµ‹æœªæ¥ã€‚æ•°æ®æ³„æ¼çš„å…¶ä»–åŸå› å¯èƒ½ä»ç„¶å­˜åœ¨ã€‚
- en: 'I Have Leakage: Now What?'
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æˆ‘æœ‰æ³„æ¼ç°è±¡ï¼šç°åœ¨æ€ä¹ˆåŠï¼Ÿ
- en: 'Once you have detected the source of leakage, the solution is to remove it
    and retrain your model. In some cases this is quite obvious, but in others it
    can take substantial time and effort. Hereâ€™s a list of things you can attempt
    to identify the source of leakage:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ£€æµ‹åˆ°æ³„æ¼æºï¼Œè§£å†³æ–¹æ³•æ˜¯å°†å…¶åˆ é™¤å¹¶é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿™å¾ˆæ˜æ˜¾ï¼Œä½†åœ¨å…¶ä»–æƒ…å†µä¸‹å¯èƒ½éœ€è¦å¤§é‡æ—¶é—´å’Œç²¾åŠ›ã€‚ä»¥ä¸‹æ˜¯ä½ å¯ä»¥å°è¯•çš„ä¸€äº›äº‹é¡¹æ¥ç¡®å®šæ³„æ¼æºï¼š
- en: Check time windows.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥æ—¶é—´çª—å£ã€‚
- en: Ensure that youâ€™re always using past information to predict the future. This
    can be done by enforcing a strict time windowing process such as the one just
    described.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿ä½ å§‹ç»ˆä½¿ç”¨è¿‡å»çš„ä¿¡æ¯æ¥é¢„æµ‹æœªæ¥ã€‚è¿™å¯ä»¥é€šè¿‡æ‰§è¡Œä¸¥æ ¼çš„æ—¶é—´çª—å£å¤„ç†è¿‡ç¨‹æ¥å®ç°ï¼Œä¾‹å¦‚åˆšåˆšæè¿°çš„æ–¹æ³•ã€‚
- en: Check any data transformations done and enforce best practices.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥å·²æ‰§è¡Œçš„ä»»ä½•æ•°æ®è½¬æ¢ï¼Œå¹¶å®æ–½æœ€ä½³å®è·µã€‚
- en: A good practice is to use [scikit-learn pipelines](https://oreil.ly/iOEs1) or
    similar to ensure that the transformations are done with the correct datasets
    and that there are no leaking moments or metadata.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¥½çš„åšæ³•æ˜¯ä½¿ç”¨[scikit-learn pipelines](https://oreil.ly/iOEs1)æˆ–ç±»ä¼¼å·¥å…·ï¼Œä»¥ç¡®ä¿è½¬æ¢æ˜¯ä½¿ç”¨æ­£ç¡®çš„æ•°æ®é›†è¿›è¡Œçš„ï¼Œå¹¶ä¸”æ²¡æœ‰æ³„æ¼æ—¶åˆ»æˆ–å…ƒæ•°æ®ã€‚
- en: Ensure that you have thorough knowledge of the business processes behind the
    creation of the data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®ä¿ä½ å¯¹æ•°æ®åˆ›å»ºèƒŒåçš„ä¸šåŠ¡æµç¨‹æœ‰æ·±å…¥çš„äº†è§£ã€‚
- en: The more you know about the processes behind the creation of your data, the
    easier it is to identify potential sources of leakage or quasi-complete separation
    in the case of classification models.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ•°æ®åˆ›å»ºè¿‡ç¨‹èƒŒåçš„æµç¨‹äº†è§£å¾—è¶Šå¤šï¼Œå°±è¶Šå®¹æ˜“è¯†åˆ«æ½œåœ¨çš„æ³„æ¼æºæˆ–åœ¨åˆ†ç±»æ¨¡å‹çš„æƒ…å†µä¸‹å‡ ä¹å®Œå…¨åˆ†ç¦»çš„æƒ…å†µã€‚
- en: Iteratively remove features.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: é€æ­¥åˆ é™¤ç‰¹å¾ã€‚
- en: On a regular basis you should run a diagnostic check to identify the most predictive
    features (in some algorithms you can do this with [feature importance](https://oreil.ly/uW6PY)).
    Coupled with your knowledge of the business, this should help you identify if
    something looks *off*. You can also try iteratively removing the most important
    features to see if predictive performance changes dramatically in any iteration.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å®šæœŸè¿›è¡Œè¯Šæ–­æ£€æŸ¥ï¼Œä»¥è¯†åˆ«æœ€å…·é¢„æµ‹æ€§çš„ç‰¹å¾ï¼ˆåœ¨æŸäº›ç®—æ³•ä¸­ï¼Œå¯ä»¥é€šè¿‡[ç‰¹å¾é‡è¦æ€§](https://oreil.ly/uW6PY)è¿›è¡Œæ­¤æ“ä½œï¼‰ã€‚ç»“åˆä½ å¯¹ä¸šåŠ¡çš„äº†è§£ï¼Œè¿™åº”è¯¥æœ‰åŠ©äºç¡®å®šæ˜¯å¦æœ‰ä»€ä¹ˆå¼‚å¸¸ã€‚ä½ ä¹Ÿå¯ä»¥å°è¯•é€æ­¥åˆ é™¤æœ€é‡è¦çš„ç‰¹å¾ï¼Œçœ‹çœ‹åœ¨ä»»ä½•è¿­ä»£ä¸­é¢„æµ‹æ€§èƒ½æ˜¯å¦ä¼šæ˜¾è‘—å˜åŒ–ã€‚
- en: Key Takeaways
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…³é”®è¦ç‚¹
- en: 'These are the key takeaways from this chapter:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯æœ¬ç« çš„å…³é”®è¦ç‚¹ï¼š
- en: Why care about data leakage?
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¦å…³æ³¨æ•°æ®æ³„æ¼ï¼Ÿ
- en: Data leakage generates subpar predictive performance when the model is deployed
    in production, relative to the performance you would expect from your test sample.
    It creates organizational frustration, and you may even put at risk any stakeholder
    buy-in you may have.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ¨¡å‹åœ¨ç”Ÿäº§ä¸­éƒ¨ç½²æ—¶ï¼Œæ•°æ®æ³„éœ²ä¼šå¯¼è‡´é¢„æµ‹æ€§èƒ½ä¸ä½³ï¼Œç›¸å¯¹äºä½ åœ¨æµ‹è¯•æ ·æœ¬ä¸­æœŸæœ›çš„è¡¨ç°ã€‚å®ƒä¼šé€ æˆç»„ç»‡ä¸Šçš„æŒ«è´¥æ„Ÿï¼Œç”šè‡³å¯èƒ½å±åŠä½ å¯èƒ½å·²ç»è·å¾—çš„ä»»ä½•åˆ©ç›Šç›¸å…³è€…çš„æ”¯æŒã€‚
- en: Identifying leakage.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: è¯†åˆ«æ³„æ¼ã€‚
- en: A typical symptom of leakage is having *unusually high* predictive performance
    on your test sample. You must rely on your knowledge of the problem and the companyâ€™s
    experience with these models. Itâ€™s always a good practice to present your results
    to more experienced data scientists, and also discuss them with your business
    stakeholders. If you suspect thereâ€™s data leakage, you must start auditing your
    model.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: æ³„æ¼çš„å…¸å‹ç—‡çŠ¶æ˜¯åœ¨æµ‹è¯•æ ·æœ¬ä¸Šå…·æœ‰*å¼‚å¸¸é«˜*çš„é¢„æµ‹æ€§èƒ½ã€‚ä½ å¿…é¡»ä¾é ä½ å¯¹é—®é¢˜çš„äº†è§£ä»¥åŠå…¬å¸å¯¹è¿™äº›æ¨¡å‹çš„ç»éªŒã€‚å‘æ›´æœ‰ç»éªŒçš„æ•°æ®ç§‘å­¦å®¶å±•ç¤ºä½ çš„ç»“æœå¹¶ä¸ä¸šåŠ¡åˆ©ç›Šç›¸å…³è€…è®¨è®ºæ€»æ˜¯ä¸€ä¸ªè‰¯å¥½çš„åšæ³•ã€‚å¦‚æœä½ æ€€ç–‘å­˜åœ¨æ•°æ®æ³„æ¼ï¼Œä½ å¿…é¡»å¼€å§‹å®¡è®¡ä½ çš„æ¨¡å‹ã€‚
- en: Things to check if you suspect you have data leakage.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ€€ç–‘å­˜åœ¨æ•°æ®æ³„æ¼ï¼Œéœ€è¦æ£€æŸ¥çš„äº‹é¡¹ã€‚
- en: Check if you have enforced a strict time windowing process that guarantees that
    you always predict the future with the past, and not the other way around. Also,
    check if you have any data transformations where moments or metadata might be
    leaking.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥æ˜¯å¦å·²ç»æ‰§è¡Œäº†ä¸¥æ ¼çš„æ—¶é—´çª—å£å¤„ç†è¿‡ç¨‹ï¼Œç¡®ä¿ä½ æ€»æ˜¯ç”¨è¿‡å»æ¥é¢„æµ‹æœªæ¥ï¼Œè€Œä¸æ˜¯åè¿‡æ¥ã€‚åŒæ—¶ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨ä»»ä½•å¯èƒ½æ³„éœ²æ—¶åˆ»æˆ–å…ƒæ•°æ®çš„æ•°æ®è½¬æ¢ã€‚
- en: In ML, scoring reigns.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œå¾—åˆ†è‡³å…³é‡è¦ã€‚
- en: The litmus test for an ML model is its performance in production. You should
    direct all of your time and effort to ensure that this is the case.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ æ¨¡å‹çš„è¯•é‡‘çŸ³æ˜¯å…¶åœ¨ç”Ÿäº§ä¸­çš„è¡¨ç°ã€‚ä½ åº”è¯¥æŠ•å…¥æ‰€æœ‰æ—¶é—´å’Œç²¾åŠ›æ¥ç¡®ä¿è¿™ä¸€ç‚¹ã€‚
- en: Further Reading
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»
- en: 'In my opinion there isnâ€™t much depth in most of the published accounts on data
    leakage found in published books (many mention it just in passing). You can find
    several useful blog posts on the web: for instance, Christopher Hefeleâ€™s comment
    on data leakage at the [ICML 2013 Whale Challenge](https://oreil.ly/j7B4l) or
    Prerna Singhâ€™s post [â€œData Leakage in Machine Learning: How It Can Be Detected
    and Minimize the Riskâ€](https://oreil.ly/G92H-).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘çœ‹æ¥ï¼Œå¤§å¤šæ•°å…³äºæ•°æ®æ³„éœ²çš„å·²å‘è¡¨è´¦æˆ·ä¸­å¹¶æ²¡æœ‰æ·±å…¥æ¢è®¨ï¼ˆè®¸å¤šåªæ˜¯é¡ºå¸¦æåŠï¼‰ã€‚ä½ å¯ä»¥åœ¨ç½‘ç»œä¸Šæ‰¾åˆ°å‡ ç¯‡æœ‰ç”¨çš„åšå®¢æ–‡ç« ï¼šä¾‹å¦‚ï¼ŒChristopher Hefeleåœ¨[ICML
    2013é²¸é±¼æŒ‘æˆ˜èµ›](https://oreil.ly/j7B4l)ä¸­å¯¹æ•°æ®æ³„éœ²çš„è¯„è®ºï¼Œæˆ–è€…Prerna Singhçš„æ–‡ç« [â€œæœºå™¨å­¦ä¹ ä¸­çš„æ•°æ®æ³„éœ²ï¼šå¦‚ä½•æ£€æµ‹å’Œå‡å°‘é£é™©â€](https://oreil.ly/G92H-)ã€‚
- en: 'Kaufman et al., â€œLeakage in Data Mining: Formulation, Detection, and Avoidance,â€
    (*ACM Transactions on Knowledge Discovery from Data* 6 no. 4, 2012), is a must-read
    for anyone interested in understanding leakage. They categorize two types of data
    leakage as those coming from features and those coming from training examples.
    I decided to deviate a bit from this categorization.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Kaufmanç­‰äººçš„â€œæ•°æ®æŒ–æ˜ä¸­çš„æ³„éœ²ï¼šåˆ¶å®šã€æ£€æµ‹å’Œé¿å…â€ï¼ˆã€ŠACMæ•°æ®æŒ–æ˜çŸ¥è¯†å‘ç°äº¤æ˜“ã€‹ï¼Œ2012å¹´ç¬¬6å·ç¬¬4æœŸï¼‰æ˜¯ä»»ä½•å¯¹ç†è§£æ³„éœ²æ„Ÿå…´è¶£çš„äººå¿…è¯»çš„ã€‚ä»–ä»¬å°†æ•°æ®æ³„éœ²åˆ†ç±»ä¸ºæ¥è‡ªç‰¹å¾å’Œæ¥è‡ªè®­ç»ƒæ ·æœ¬çš„ä¸¤ç§ç±»å‹ã€‚æˆ‘å†³å®šåœ¨è¿™ä¸ªåˆ†ç±»ä¸Šåšäº›åç¦»ã€‚
- en: On the problem of complete and quasi-complete separation, the classical reference
    is A. Albert and J. A. Anderson, â€œOn the Existence of Maximum Likelihood Estimates
    in Logistic Regression Modelsâ€ (*Biometrika* 71 no. 1, 1984). A textbook presentation
    can be found in Chapter 11 of Russell Davison and James MacKinnon, *Econometric
    Theory and Methods* (Oxford University Press).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®Œå…¨åˆ†ç¦»å’Œå‡†å®Œå…¨åˆ†ç¦»é—®é¢˜ä¸Šï¼Œç»å…¸å‚è€ƒæ–‡çŒ®æ˜¯A. Albertå’ŒJ.A. Andersonçš„â€œå…³äºé€»è¾‘å›å½’æ¨¡å‹ä¸­æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„å­˜åœ¨æ€§â€ï¼ˆã€Šç”Ÿç‰©ç»Ÿè®¡å­¦ã€‹1984å¹´ç¬¬71å·ç¬¬1æœŸï¼‰ã€‚åœ¨Russell
    Davisonå’ŒJames MacKinnonçš„ã€Šè®¡é‡ç»æµå­¦ç†è®ºä¸æ–¹æ³•ã€‹ï¼ˆç‰›æ´¥å¤§å­¦å‡ºç‰ˆç¤¾ï¼‰ç¬¬11ç« ä¸­å¯ä»¥æ‰¾åˆ°æ•™ç§‘ä¹¦å¼çš„é˜è¿°ã€‚
- en: The problem of bad controls is well known in the literature of causal inference
    and causal machine learning. To the best of my knowledge, it was first labeled
    liked that by Angrist and Pischke in *Mostly Harmless Econometrics* (Princeton
    University Press). A more recent and systematic study can be found in Carlos Cinelli
    et al., â€œA Crash Course in Good and Bad Controlsâ€ (*Sociological Methods and Research*,
    2022). In this, chapter I used a rather loose version of the bad control definition.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å› æœæ¨æ–­å’Œå› æœæœºå™¨å­¦ä¹ çš„æ–‡çŒ®ä¸­ï¼Œåæ§åˆ¶é—®é¢˜æ˜¯ä¼—æ‰€å‘¨çŸ¥çš„ã€‚æ®æˆ‘æ‰€çŸ¥ï¼Œå®‰æ ¼é‡Œæ–¯ç‰¹å’Œçš®æ–¯å…‹åœ¨ã€Šå¤§éƒ¨åˆ†æ— å®³è®¡é‡ç»æµå­¦ã€‹ï¼ˆæ™®æ—æ–¯é¡¿å¤§å­¦å‡ºç‰ˆç¤¾ï¼‰ä¸­é¦–æ¬¡å°†å…¶ç§°ä¸ºè¿™æ ·ã€‚å…³äºæ›´è¿‘æœŸå’Œç³»ç»Ÿçš„ç ”ç©¶ï¼Œå¯ä»¥å‚è€ƒCarlos
    Cinelliç­‰äººçš„â€œå¥½æ§åˆ¶å’Œåæ§åˆ¶é€Ÿæˆè¯¾ç¨‹â€ï¼ˆã€Šç¤¾ä¼šå­¦æ–¹æ³•ä¸ç ”ç©¶ã€‹ï¼Œ2022å¹´ï¼‰ã€‚åœ¨è¿™ä¸€ç« èŠ‚ä¸­ï¼Œæˆ‘å¯¹åæ§åˆ¶çš„å®šä¹‰ä½¿ç”¨äº†ä¸€ä¸ªç›¸å¯¹å®½æ¾çš„ç‰ˆæœ¬ã€‚
- en: ^([1](ch11.html#id641-marker)) Also, remember that the AUC is sensitive to having
    imbalanced outcomes, so my heuristic really must be taken with a grain of salt,
    to say the least.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch11.html#id641-marker)) æ­¤å¤–ï¼Œè¯·è®°ä½AUCå¯¹äºä¸å¹³è¡¡çš„ç»“æœå¾ˆæ•æ„Ÿï¼Œå› æ­¤æˆ‘çš„å¯å‘å¼æ–¹æ³•ç¡®å®å¿…é¡»è°¨æ…å¯¹å¾…ï¼Œè‡³å°‘å¯ä»¥è¿™ä¹ˆè¯´ã€‚
- en: ^([2](ch11.html#id642-marker)) An alternative is to use the coefficient of determination
    or *R2* that is also bounded to the unit interval.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch11.html#id642-marker)) å¦ä¸€ä¸ªé€‰æ‹©æ˜¯ä½¿ç”¨å†³å®šç³»æ•°æˆ–*R2*ï¼Œå®ƒä¹Ÿè¢«é™åˆ¶åœ¨å•ä½é—´éš”å†…ã€‚
- en: ^([3](ch11.html#id643-marker)) An *AR(1)* process is a time series with an autoregressive
    component of order 1\. You can check [ChapterÂ 10](ch10.html#ch10_linreg) for more
    information.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch11.html#id643-marker)) *AR(1)*è¿‡ç¨‹æ˜¯ä¸€ä¸ªå…·æœ‰ä¸€é˜¶è‡ªå›å½’åˆ†é‡çš„æ—¶é—´åºåˆ—ã€‚æ‚¨å¯ä»¥æŸ¥çœ‹[ç¬¬10ç« ](ch10.html#ch10_linreg)è·å–æ›´å¤šä¿¡æ¯ã€‚
