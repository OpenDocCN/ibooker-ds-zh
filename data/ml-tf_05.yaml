- en: 3 Linear regression and beyond
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 线性回归及其扩展
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Fitting a line to data points
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将线拟合到数据点上
- en: Fitting arbitrary curves to data points
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将任意曲线拟合到数据点上
- en: Testing performance of regression algorithms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试回归算法的性能
- en: Applying regression to real-world data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将回归应用于实际数据
- en: Remember science courses back in high school? It might have been a while ago,
    or who knows—maybe you’re in high school now, starting your journey in machine
    learning early. Either way, whether you took biology, chemistry, or physics, a
    common technique to analyze data is plotting how changing one variable affects
    another.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得高中时的科学课程吗？可能已经过去很久了，或者谁知道呢——也许你现在还在高中，早早地开始了机器学习的旅程。无论如何，无论你学习的是生物学、化学还是物理学，分析数据的一个常见技术是绘制一个变量如何影响另一个变量的变化。
- en: 'Imagine plotting the correlation between rainfall frequency and agriculture
    production. You may observe that an increase in rainfall produces an increase
    in agriculture production rate. Fitting a line to these data points enables you
    to make predictions about the production rate under different rain conditions:
    a little less rain, a little more rain, and so on. If you discover the underlying
    function from a few data points, that learned function empowers you to make predictions
    about the values of unseen data.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下绘制降雨频率与农业生产之间的相关性。你可能观察到降雨量的增加会导致农业生产率的增加。将这些数据点拟合到一条线上，使你能够预测在不同降雨条件下的生产率：稍微少一点雨，稍微多一点雨，等等。如果你从几个数据点中发现了潜在的函数，那么这个学到的函数将使你能够预测未见数据的价值。
- en: '*Regression* is the study of how to best fit a curve to summarize your data
    and is one of the most powerful, best-studied types of supervised-learning algorithms.
    In regression, we try to understand the data points by discovering the curve that
    might have generated them. In doing so, we seek an explanation for why the given
    data is scattered the way it is. The best-fit curve gives us a model for explaining
    how the dataset might have been produced.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*回归* 是研究如何最佳拟合曲线以总结数据的一种方法，是监督学习算法中最强大、研究最深入的类型之一。在回归中，我们试图通过发现可能生成这些数据点的曲线来理解数据点。在这个过程中，我们寻求解释为什么给定的数据会以这种方式分散。最佳拟合曲线为我们提供了一个模型，解释了数据集可能是如何产生的。'
- en: This chapter shows you how to formulate a real-world problem to use regression.
    As you’ll see, TensorFlow is the right tool, delivering some of the most powerful
    predictors.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将向你展示如何将实际问题用回归来解决。正如你将看到的，TensorFlow 是正确的工具，提供了最强大的预测器之一。
- en: 3.1 Formal notation
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 正式符号
- en: If you have a hammer, every problem looks like a nail. This chapter demonstrates
    the first major machine-learning tool, regression, and formally defines it by
    using precise mathematical symbols. Learning regression first is a great idea,
    because many of the skills you’ll develop will carry over to other types of problems
    in future chapters. By the end of this chapter, regression will become the “hammer”
    in your box of machine-learning tools.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一把锤子，每个问题看起来都像钉子。本章展示了第一个主要的机器学习工具，回归，并使用精确的数学符号正式定义它。首先学习回归是一个很好的主意，因为你在未来章节中遇到的其他类型的问题中，许多技能都会得到应用。到本章结束时，回归将成为你机器学习工具箱中的“锤子”。
- en: Let’s say you have data about how much money people spent on bottles of beer.
    Alice spent $4 on 2 bottles, Bob spent $6 on 3 bottles, and Clair spent $8 on
    4 bottles. You want to find an equation that describes how the number of bottles
    affects the total cost. If the linear equation *y* = 2*x* describes the cost of
    buying a particular number of bottles, for example, you can find out how much
    each bottle of beer costs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一些关于人们购买啤酒瓶花费的数据。爱丽丝购买了2瓶啤酒，花费了4美元，鲍勃购买了3瓶啤酒，花费了6美元，克莱尔购买了4瓶啤酒，花费了8美元。你想要找到一个方程来描述瓶数对总成本的影响。如果线性方程
    *y* = 2*x* 描述了购买特定数量瓶子的成本，例如，你可以计算出每瓶啤酒的成本。
- en: When a line appears to fit some data points well, you might claim that your
    linear model performs well. But you could have tried many possible slopes instead
    of choosing the value 2\. The choice of slope is the *parameter*, and the equation
    containing the parameter is the *model.* Speaking in machine-learning terms, the
    equation of the best-fit curve comes from learning the parameters of a model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当一条线看起来很好地拟合了一些数据点时，你可能会声称你的线性模型表现良好。但你可以尝试许多可能的斜率，而不仅仅是选择2这个值。斜率的选择是 *参数*，包含参数的方程是
    *模型*。用机器学习的术语来说，最佳拟合曲线的方程来自于学习模型的参数。
- en: 'As another example, the equation *y* = 3*x* is also a line, except with a steeper
    slope. You can replace that coefficient with any real number (let’s call it *w*),
    and the equation will still produce a line: *y* = *wx*. Figure 3.1 shows how changing
    the parameter *w* affects the model. The set of all equations you can generate
    this way is denoted as *M* `=` `{`*y* `=` *wx* `|` *w* ∈ ℝ}, which is read “all
    equations *y* `=` *wx* such that *w* is a real number.”'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个例子，方程 *y* = 3*x* 也是一个直线，只是斜率更陡。你可以用任何实数（让我们称它为 *w*）替换那个系数，方程仍然会产生一条直线：*y*
    = *wx*。图 3.1 展示了改变参数 *w* 如何影响模型。以这种方式生成的所有方程的集合表示为 *M* `=` `{`*y* `=` *wx* `|`
    *w* ∈ ℝ}，这读作“所有 *y* `=` *wx* 的方程，其中 *w* 是一个实数。”
- en: '*M* is a set of all possible models. Choosing a value for *w* generates a candidate
    model *M*`(`*w*`):` *y* `=` *wx*. The regression algorithms that you’ll write
    in TensorFlow will iteratively converge to progressively better values for the
    model’s parameter *w*. An optimal parameter, which we’ll call *w** (pronounced
    *w star*), is the best-fit equation *M*(*w**) : *y* = *w***x*. *Best-fit* implies
    that the model produces the least error or difference from its prediction and
    the actual value, often called the *ground truth*. We’ll talk more about this
    throughout the chapter.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*M* 是所有可能模型的集合。选择一个 *w* 的值生成候选模型 *M*`(`*w*`):` *y* `=` *wx*。你将在 TensorFlow
    中编写的回归算法将迭代收敛到模型参数 *w* 的更好值。最优参数，我们称之为 *w** (发音为 *w star*)，是最拟合方程 *M*(*w**) :
    *y* = *w***x*。*Best-fit* 意味着模型产生的误差或预测值与实际值之间的差异最小，通常称为 *ground truth*。我们将在本章中更多地讨论这一点。'
- en: In the most general sense, a regression algorithm tries to design a function,
    which we’ll call *f*, that maps an input to an output. The function’s domain is
    a real-valued vector ℝ^d, and its range is the set of real numbers ℝ.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在最一般的意义上，回归算法试图设计一个函数，我们可以称之为 *f*，它将输入映射到输出。该函数的定义域是实值向量 ℝ^d，其值域是实数集 ℝ。
- en: '![CH03_F01_Mattmann2](../Images/CH03_F01_Mattmann2.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F01_Mattmann2](../Images/CH03_F01_Mattmann2.png)'
- en: Figure 3.1 Different values of the parameter w result in different linear equations.
    The set of all these linear equations is what constitutes the linear model M.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 参数 *w* 的不同值导致不同的线性方程。这些所有线性方程的集合构成了线性模型 M。
- en: note Regression can also be posed with multiple outputs, as opposed to one real
    number. In that case, we call it *multivariate regression**.*
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: note 回归也可以有多个输出，而不是一个实数。在这种情况下，我们称之为 *多元回归**.*
- en: The input of the function could be continuous or discrete. But the output must
    be continuous, as demonstrated in figure 3.2.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的输入可以是连续的或离散的。但输出必须是连续的，如图 3.2 所示。
- en: '![CH03_F02_Mattmann2](../Images/CH03_F02_Mattmann2.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F02_Mattmann2](../Images/CH03_F02_Mattmann2.png)'
- en: Figure 3.2 A regression algorithm is meant to produce continuous output. The
    input is allowed to be discrete or continuous. This distinction is important because
    discrete-valued outputs are handled better by classification, which is discussed
    in chapters 5 and 6.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 回归算法旨在产生连续输出。输入可以是离散的或连续的。这种区别很重要，因为离散值输出更适合由分类处理，这在第 5 章和第 6 章中讨论。
- en: note Regression predicts continuous outputs, but sometimes, that’s overkill.
    Sometimes, we want to predict a discrete output, such as 0 or 1, and nothing in
    between. Classification is a technique better suited for such tasks, and it’s
    discussed in chapter 5.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: note 回归预测连续输出，但有时这过于冗余。有时，我们想要预测一个离散输出，例如 0 或 1，中间没有其他值。分类是一种更适合此类任务的技巧，将在第
    5 章中讨论。
- en: We’d like to discover a function *f* that agrees well with the given data points,
    which are essentially input/output pairs. Unfortunately, the number of possible
    functions is infinite, so we’ll have no luck trying them one by one. Having too
    many options available to choose among is usually a bad idea. It behooves us to
    tighten the scope of all the functions we want to deal with. If we look at only
    straight lines to fit a set of data points, for example, the search becomes much
    easier.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望找到一个函数 *f*，它与给定的数据点（本质上是一组输入/输出对）很好地吻合。不幸的是，可能函数的数量是无限的，所以逐个尝试它们是没有希望的。有太多选择通常是一个坏主意。我们有必要缩小我们想要处理的函数的范围。例如，如果我们只考虑直线来拟合一组数据点，搜索就会变得容易得多。
- en: Exercise 3.1
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 3.1
- en: How many possible functions exist that map 10 integers to 10 integers? Let f(x)
    be a function that can take numbers 0 through 9 and produce numbers 0 through
    9\. One example is the identity function that mimics its input—for example, f(0)
    = 0, f(1) = 1, and so on. How many other functions exist?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 有多少种可能的函数可以将10个整数映射到10个整数？设f(x)是一个可以接受从0到9的数字并产生从0到9的数字的函数。一个例子是恒等函数，它模仿其输入——例如，f(0)
    = 0，f(1) = 1，以此类推。还有多少其他函数存在？
- en: '**Answer**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: 1010 = 10,000,000,000
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 10^10 = 10,000,000,000
- en: 3.1.1 How do you know the regression algorithm is working?
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 你如何知道回归算法是有效的？
- en: Let’s say you’re trying to sell a housing-market-predictor algorithm to a real
    estate firm. The algorithm predicts housing prices given properties such as the
    number of bedrooms and lot size. Real estate companies can easily make millions
    with such information, but they need some proof that the algorithm works before
    buying it from you.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在尝试向一家房地产公司销售一个房价预测算法。该算法根据诸如卧室数量和地块大小等属性预测房价。房地产公司可以轻易地利用此类信息赚取数百万，但在从你那里购买之前，他们需要一些证明该算法有效的证据。
- en: 'To measure the success of the learning algorithm, you’ll need to understand
    two important concepts:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要衡量学习算法的成功，你需要了解两个重要的概念：
- en: '*Variance* indicates how sensitive a prediction is to the training set that
    was used. Ideally, how you choose the training set shouldn’t matter, meaning that
    a lower variance is desired.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*方差*表示预测对所使用的训练集的敏感度。理想情况下，你选择训练集的方式不应该很重要，这意味着希望方差更低。'
- en: '*Bias* indicates the strength of assumptions made about the training dataset.
    Making too many assumptions might make the model unable to generalize, so you
    should prefer low bias as well.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*偏差*表示对训练数据集所做假设的强度。做出太多的假设可能会使模型无法泛化，因此你应该偏好低偏差。'
- en: If a model is too flexible, it may accidentally memorize the training data instead
    of resolving useful patterns. You can imagine a curvy function passing through
    every point of a dataset, appearing to produce no error. If that happens, we say
    that the learning algorithm *overfits* the data. In this case, the best-fit curve
    will agree with the training data well, but it may perform abysmally when evaluated
    on the testing data (see figure 3.3).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个模型过于灵活，它可能会意外地记住训练数据而不是解决有用的模式。你可以想象一个曲线函数穿过数据集的每一个点，看起来没有产生错误。如果发生这种情况，我们说学习算法*过拟合*了数据。在这种情况下，最佳拟合曲线将与训练数据很好地吻合，但在测试数据上的表现可能非常糟糕（见图3.3）。
- en: '![CH03_F03_Mattmann2](../Images/CH03_F03_Mattmann2.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F03_Mattmann2](../Images/CH03_F03_Mattmann2.png)'
- en: Figure 3.3 Ideally, the best-fit curve fits well on both the training data and
    the test data. If we witness it fitting poorly with the test data and the training
    data, there’s a chance that our model is underfitting. On the other hand, if it
    performs poorly on the test data but well on the training data, we know that the
    model is overfitting.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 理想情况下，最佳拟合曲线在训练数据和测试数据上都拟合得很好。如果我们看到它与测试数据拟合得不好，而与训练数据拟合得很好，那么我们的模型可能欠拟合。另一方面，如果它在测试数据上表现不佳，但在训练数据上表现良好，我们知道该模型是过拟合的。
- en: Transfer learning and overfitting
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习和过拟合
- en: 'One of the big overfitting challenges today arises from the process of transfer
    learning: taking knowledge that a model learns in one domain and applying that
    knowledge to another. Amazingly, this process works extremely well for computer
    vision, speech recognition, and other domains. But many transfer learning models
    suffer from overfitting issues. As an example, consider the famous MNIST *(*Modified
    National Institute of Standards and Technology) dataset and problem for recognition
    of black-and-white digits for the numbers 1 through 10\. The learned model from
    MNIST can be applied to other non-black-and-white digits (such as street signs),
    but not without fine-tuning, because even the best MNIST model usually exhibits
    some overfitting.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，一个大的过拟合挑战来自于迁移学习的过程：将模型在一个领域学到的知识应用到另一个领域。令人惊讶的是，这个过程在计算机视觉、语音识别和其他领域工作得非常好。但许多迁移学习模型都存在过拟合问题。例如，考虑著名的MNIST（*修改后的国家标准与技术研究院）数据集和问题，用于识别1到10的黑白数字。从MNIST学到的模型可以应用于其他非黑白数字（如街牌），但需要微调，因为即使是最好的MNIST模型通常也会表现出一些过拟合。
- en: At the other end of the spectrum, a not-so-flexible model may generalize better
    to unseen testing data but score relatively low on the training data. That situation
    is called *underfitting*. A too-flexible model has high variance and low bias,
    whereas a too-strict model has low variance and high bias. Ideally, you want a
    model with both low-variance error and low-bias error. That way, the model both
    generalizes to unseen data and captures the regularities of the data. See figure
    3.4 for examples of a model underfitting and overfitting data points in 2D.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在光谱的另一端，一个不太灵活的模型可能对未见过的测试数据泛化得更好，但在训练数据上的得分相对较低。这种情况被称为*欠拟合*。过于灵活的模型具有高方差和低偏差，而过于严格的模型具有低方差和高偏差。理想情况下，你希望模型同时具有低方差误差和低偏差误差。这样，模型既能泛化到未见过的数据，又能捕捉数据的规律。参见图3.4，了解模型在二维空间中对数据欠拟合和过拟合的示例。
- en: '![CH03_F04_Mattmann2](../Images/CH03_F04_Mattmann2.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F04_Mattmann2](../Images/CH03_F04_Mattmann2.png)'
- en: Figure 3.4 Examples of underfitting and overfitting the data
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4欠拟合和过拟合数据的示例
- en: Concretely, the *variance* of a model is a measure of how badly the responses
    fluctuate, and the *bias* is a measure of how badly the response is offset from
    the ground truth, as discussed earlier in this chapter. You want your model to
    achieve accurate (low-bias) as well as reproducible (low-variance) results.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，模型的*方差*是衡量响应波动程度的一个指标，而*偏差*是衡量响应偏离真实值程度的一个指标，正如本章前面所讨论的。你希望你的模型实现准确（低偏差）以及可重复（低方差）的结果。
- en: Exercise 3.2
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 练习3.2
- en: 'Let’s say that your model is M(w) : y = wx. How many possible functions can
    you generate if the values of the weight parameter w must be integers between
    0 and 9 (inclusive)?'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的模型是M(w)：y = wx。如果权重参数w的值必须是介于0和9（包含）之间的整数，你能生成多少个可能的功能？
- en: '**Answer**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: 'Only 10: `{`y = 0, y = x`,` y = 2x, ..., y = 9x`}`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 只有10个：`{y = 0, y = x, y = 2x, ..., y = 9x}`
- en: In summary, measuring how well your model does on the training data isn’t a
    great indicator of its generalizability. Instead, you should evaluate your model
    on a separate batch of testing data. You might find out that your model performs
    well on the data you trained it with but terribly on the test data, in which case
    your model is likely overfitting the training data. If the testing error is around
    the same as the training error, and both errors are similar, your model may be
    fitting well or (if that error is high) underfitting.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，衡量你的模型在训练数据上的表现并不是衡量其泛化能力的一个很好的指标。相反，你应该在单独的一批测试数据上评估你的模型。你可能会发现，你的模型在你训练的数据上表现良好，但在测试数据上表现糟糕，在这种情况下，你的模型很可能是对训练数据过拟合。如果测试错误与训练错误大致相同，并且两个错误相似，那么你的模型可能拟合得很好，或者（如果那个错误很高）欠拟合。
- en: 'This is why, to measure success in machine learning, you partition the dataset
    into two groups: a training dataset and a testing dataset. The model is learned
    using the training dataset, and performance is evaluated on the testing dataset.
    (Section 3.2 describes how to evaluate performance.) Of the many possible weight
    parameters you can generate, the goal is to find the one that best fits the data.
    You measure best fit by defining a cost function, which is discussed in greater
    detail in section 3.2\. The cost function can also drive you to split your test
    data into even another parameter that tunes the cost and an evaluation dataset
    (which is the true unseen data). We’ll explain more in the ensuing sections.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么，为了衡量机器学习中的成功，你需要将数据集分成两组：训练数据集和测试数据集。模型使用训练数据集学习，性能在测试数据集上评估。（第3.2节描述了如何评估性能。）在你可以生成的许多可能的权重参数中，目标是找到最适合数据的那个。你通过定义一个成本函数来衡量最佳拟合，该函数在第3.2节中进行了更详细的讨论。成本函数还可以驱动你将测试数据分成另一个调整成本的参数和一个评估数据集（这是真正的未见数据）。我们将在接下来的章节中进一步解释。
- en: 3.2 Linear regression
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 线性回归
- en: Let’s start by creating fake data for a leap into the heart of linear regression.
    Create a Python source file called regression.py, and follow along with listing
    3.1 to initialize data. The code will produce output similar to figure 3.5.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先创建一些假数据，以便深入理解线性回归。创建一个名为regression.py的Python源文件，并按照列表3.1初始化数据。该代码将生成类似于图3.5的输出。
- en: Listing 3.1 Visualizing raw input
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.1可视化原始输入
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Imports NumPy to help generate initial raw data
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入NumPy以帮助生成初始原始数据
- en: ❷ Uses Matplotlib to visualize the data
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用Matplotlib可视化数据
- en: ❸ The input values are 101 evenly spaced numbers between -1 and 1.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 输入值是-1和1之间101个等间距的数字。
- en: ❹ The output values are proportional to the input, but with added noise.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 输出值与输入成正比，但增加了噪声。
- en: ❺ Uses Matplotlib’s function to generate a scatter plot of the data
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用Matplotlib的函数生成数据的散点图
- en: '![CH03_F05_Mattmann2](../Images/CH03_F05_Mattmann2.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F05_Mattmann2](../Images/CH03_F05_Mattmann2.png)'
- en: Figure 3.5 Scatter plot of y = x + (noise)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 y = x + (噪声)的散点图
- en: Now that you have some data points available, you can try fitting a line. At
    the very least, you need to provide TensorFlow with a score for each candidate
    parameter it tries. This score assignment is commonly called a *cost function*.
    The higher the cost, the worse the model parameter will be. If the best-fit line
    is *y* = 2*x*, a parameter choice of 2.01 should have low cost, but the choice
    of -1 should have higher cost.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有一些数据点可用，你可以尝试拟合一条线。至少，你需要为TensorFlow提供的每个候选参数提供一个分数。这种评分分配通常称为*成本函数*。成本越高，模型参数就越差。如果最佳拟合线是*y*
    = 2*x*，参数选择2.01应该有低成本，但选择-1应该有更高的成本。
- en: After you define the situation as a cost-minimization problem, as denoted in
    figure 3.6, TensorFlow takes care of the inner workings, trying to update the
    parameters in an efficient way to eventually reach the best possible value. Each
    step of looping through all your data to update the parameters is called an *epoch*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在将情况定义为成本最小化问题后，如图3.6所示，TensorFlow负责内部工作，尝试以高效的方式更新参数，最终达到最佳可能值。遍历所有数据以更新参数的每个步骤称为一个*epoch*。
- en: '![CH03_F06_Mattmann2](../Images/CH03_F06_Mattmann2.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F06_Mattmann2](../Images/CH03_F06_Mattmann2.png)'
- en: Figure 3.6 Whichever parameter w minimizes, the cost is optimal. Cost is defined
    as the norm of the error between the ideal value with the model response. Finally,
    the response value is calculated from the function in the model set.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 任何参数w最小化，成本就是最优的。成本定义为理想值与模型响应之间的误差范数。最后，响应值是从模型集中的函数计算得出的。
- en: In this example, you define *cost* by the sum of errors. The error in predicting
    *x* is often calculated by the squared difference between the actual value *f*(*x*)
    and the predicted value *M*(*w*, *x*). Therefore, the cost is the sum of the squared
    differences between the actual and predicted values, as shown in figure 3.7.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，你通过误差的总和定义*成本*。预测*x*的误差通常通过实际值*f*(*x*)与预测值*M*(*w*, *x*)之间的平方差来计算。因此，成本是实际值和预测值之间平方差的和，如图3.7所示。
- en: '![CH03_F07_Mattmann2](../Images/CH03_F07_Mattmann2.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F07_Mattmann2](../Images/CH03_F07_Mattmann2.png)'
- en: Figure 3.7 The cost is the norm of the pointwise difference between the model
    response and the true value.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 成本是模型响应与真实值之间逐点差异的范数。
- en: Update your previous code to look like listing 3.2\. This code defines the cost
    function and asks TensorFlow to run an optimizer to find the optimal solution
    for the model parameters.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 更新你的代码，使其看起来像列表3.2。此代码定义了成本函数并请求TensorFlow运行一个优化器来找到模型参数的最佳解决方案。
- en: Listing 3.2 Solving linear regression
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.2 解决线性回归
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Imports TensorFlow for the learning algorithm. You’ll need NumPy to set up
    the initial data, and you’ll use Matplotlib to visualize your data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入TensorFlow用于学习算法。你需要NumPy来设置初始数据，并使用Matplotlib来可视化你的数据。
- en: ❷ Defines constants used by the learning algorithm. These constants are called
    hyperparameters.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义学习算法使用的常数。这些常数称为超参数。
- en: ❸ Sets up fake data that you’ll use to find a best-fit line
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 设置用于找到最佳拟合线的假数据
- en: ❹ Sets up the input and output nodes as placeholders because the value will
    be injected by x_train and y_train
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将输入和输出节点设置为占位符，因为值将由x_train和y_train注入
- en: ❺ Defines the model as y = w*X
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 定义模型为y = w*X
- en: ❻ Sets up the weights variable
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 设置权重变量
- en: ❼ Defines the cost function
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 定义成本函数
- en: ❽ Defines the operation that will be called on each iteration of the learning
    algorithm
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 定义学习算法每次迭代将调用的操作
- en: ❾ Sets up a session and initializes all variables
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 设置会话并初始化所有变量
- en: ❿ Loops through the dataset multiple times per specified number of epochs
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 每个epoch多次遍历数据集
- en: ⓫ Loops through each item in the dataset
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 遍历数据集中的每个项目
- en: ⓬ Updates the model parameter(s) to try to minimize the cost function
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 更新模型参数以尝试最小化成本函数
- en: ⓭ Obtains the final parameter value
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ⓭ 获得最终参数值
- en: ⓮ Closes the session
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ⓮ 关闭会话
- en: ⓯ Plots the original data
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ⓯ 绘制原始数据
- en: ⓰ Plots the best-fit line
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ⓰ 绘制最佳拟合线
- en: As figure 3.8 shows, you’ve just solved linear regression by using TensorFlow!
    Conveniently, the rest of the topics in regression are minor modifications of
    listing 3.2\. The entire pipeline involves updating model parameters using TensorFlow,
    as summarized in figure 3.9.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 3.8 所示，你刚刚使用 TensorFlow 解决了线性回归问题！方便的是，回归中的其余主题都是列表 3.2 的微小修改。整个流程涉及使用 TensorFlow
    更新模型参数，如图 3.9 总结所示。
- en: '![CH03_F08_Mattmann2](../Images/CH03_F08_Mattmann2.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F08_Mattmann2](../Images/CH03_F08_Mattmann2.png)'
- en: Figure 3.8 Linear regression estimate shown by running listing 3.2
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 通过运行列表 3.2 显示的线性回归估计
- en: '![CH03_F09_Mattmann2](../Images/CH03_F09_Mattmann2.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F09_Mattmann2](../Images/CH03_F09_Mattmann2.png)'
- en: Figure 3.9 The learning algorithm updates the model’s parameters to minimize
    the given cost function.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9 学习算法通过更新模型参数来最小化给定的成本函数。
- en: You’ve learned how to implement a simple regression model in TensorFlow. Making
    further improvements is a simple matter of enhancing the model with the right
    medley of variance and bias, as discussed earlier. The linear regression model
    you’ve designed so far is burdened with a strong bias; it expresses only a limited
    set of functions, such as linear functions. In section 3.3, you’ll try a more
    flexible model. You’ll notice that only the TensorFlow graph needs to be rewired;
    everything else (such as preprocessing, training, and evaluation) stays the same.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学会了如何在 TensorFlow 中实现一个简单的回归模型。进一步改进是一个简单的问题，只需通过前面讨论的正确混合方差和偏差来增强模型即可。你迄今为止设计的线性回归模型负担着强烈的偏差；它只表达了一组有限的函数，例如线性函数。在第
    3.3 节中，你将尝试一个更灵活的模型。你会注意到，只需要重新布线 TensorFlow 图；其他所有事情（如预处理、训练和评估）都保持不变。
- en: 3.3 Polynomial model
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 多项式模型
- en: Linear models may be an intuitive first guess, but real-world correlations are
    rarely so simple. The trajectory of a missile through space, for example, is curved
    relative to the observer on Earth. Wi-Fi signal strength degrades with an inverse
    square law. The change in height of a flower over its lifetime certainly isn’t
    linear.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型可能是一个直观的第一选择，但现实世界的相关性很少如此简单。例如，导弹在空间中的轨迹相对于地球上的观察者来说是弯曲的。Wi-Fi 信号强度随着平方反比定律而减弱。一朵花在其一生中的高度变化当然不是线性的。
- en: 'When data points appear to form smooth curves rather than straight lines, you
    need to change your regression model from a straight line to something else. One
    such approach is to use a polynomial model. A *polynomial* is a generalization
    of a linear function. The *nth* degree polynomial looks like the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据点似乎形成平滑曲线而不是直线时，你需要将你的回归模型从直线改为其他东西。一种方法就是使用多项式模型。*多项式*是线性函数的推广。*n* 次多项式看起来如下所示：
- en: '*f* (*x*) = *w*[n] *x*^n *+ ... + w*[1] *x* + w[0]'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*f* (*x*) = *w*[n] *x*^n *+ ... + w*[1] *x* + w[0]'
- en: note When *n* = 1, a polynomial is simply a linear equation *f* (*x*) = *w*[1]
    *x* + w[0].
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：当 *n* = 1 时，多项式只是一个线性方程 *f* (*x*) = *w*[1] *x* + w[0]。
- en: Consider the scatter plot in figure 3.10, showing the input on the x-axis and
    the output on the y-axis. As you can tell, a straight line is insufficient to
    describe all the data. A polynomial function is a more flexible generalization
    of a linear function.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑图 3.10 中的散点图，其中 x 轴表示输入，y 轴表示输出。正如你所看到的，一条直线不足以描述所有数据。多项式函数是线性函数的更灵活的推广。
- en: '![CH03_F10_Mattmann2](../Images/CH03_F10_Mattmann2.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F10_Mattmann2](../Images/CH03_F10_Mattmann2.png)'
- en: Figure 3.10 Data points like these aren’t suitable for a linear model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 这样的数据点不适合线性模型。
- en: Let’s try to fit a polynomial to this kind of data. Create a new file called
    polynomial.py, and follow along with listing 3.3.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试将多项式拟合到这类数据。创建一个名为 polynomial.py 的新文件，并按照列表 3.3 进行操作。
- en: Listing 3.3 Using a polynomial model
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.3 使用多项式模型
- en: '[PRE2]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Imports the relevant libraries and initializes the hyperparameters
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入相关库并初始化超参数
- en: ❷ Sets up fake raw input data
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置伪造的原始输入数据
- en: ❸ Sets up raw output data based on a fifth-degree polynomial
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 基于五次多项式设置原始输出数据
- en: ❹ Adds noise
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 添加噪声
- en: ❺ Shows a scatter plot of the raw data
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 显示原始数据的散点图
- en: ❻ Defines the nodes to hold values for input/output pairs
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义节点以保存输入/输出对的值
- en: ❼ Defines your polynomial model
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 定义了你的多项式模型
- en: ❽ Sets up the parameter vector to all zeros
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 将参数向量设置为全零
- en: ❾ Defines the cost function as before
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 定义了与之前相同的成本函数
- en: ❿ Sets up the session and runs the learning algorithm as before
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 按照之前的方式设置会话并运行学习算法
- en: ⓫ Closes the session when done
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 完成后关闭会话
- en: ⓬ Plots the result
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 绘制结果图
- en: The final output of this code is a fifth-degree polynomial that fits the data,
    as shown in figure 3.11.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的最终输出是一个五次多项式，如图3.11所示，它拟合了数据。
- en: '![CH03_F11_Mattmann2](../Images/CH03_F11_Mattmann2.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F11_Mattmann2](../Images/CH03_F11_Mattmann2.png)'
- en: Figure 3.11 The best-fit curve aligns smoothly with the nonlinear data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 最佳拟合曲线与非线性数据平滑对齐。
- en: 3.4 Regularization
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 正则化
- en: Don’t be fooled by the wonderful flexibility of polynomials, as shown in section
    3.3\. Just because higher-order polynomials are extensions of lower ones doesn’t
    mean that you should always prefer the more flexible model.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 不要被多项式的美妙灵活性所迷惑，如第3.3节所示。仅仅因为高阶多项式是低阶多项式的扩展，并不意味着你应该总是偏好更灵活的模型。
- en: In the real world, raw data rarely forms a smooth curve mimicking a polynomial.
    Suppose that you’re plotting house prices over time. The data likely will contain
    fluctuations. The goal of regression is to represent the complexity in a simple
    mathematical equation. If your model is too flexible, the model may be overcomplicating
    its interpretation of the input.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，原始数据很少形成类似于多项式的平滑曲线。假设你正在绘制随着时间的推移的房价。数据可能包含波动。回归的目标是用一个简单的数学方程表示复杂性。如果你的模型过于灵活，模型可能会过度复杂化对输入的解释。
- en: Take, for example, the data presented in figure 3.12\. You try to fit an eighth-degree
    polynomial into points that appear to follow the equation *y* = *x*². This process
    fails miserably, as the algorithm tries its best to update the nine coefficients
    of the polynomial.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以图3.12中展示的数据为例。你试图将八次多项式拟合到似乎遵循方程 *y* = *x*²的点。这个过程失败得很惨，因为算法尽力更新多项式的九个系数。
- en: '![CH03_F12_Mattmann2](../Images/CH03_F12_Mattmann2.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F12_Mattmann2](../Images/CH03_F12_Mattmann2.png)'
- en: Figure 3.12 When the model is too flexible, a best-fit curve can look awkwardly
    complicated or unintuitive. We need to use regularization to improve the fit so
    that the learned model performs well against test data.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 当模型过于灵活时，最佳拟合曲线可能看起来复杂或难以直观理解。我们需要使用正则化来改善拟合，以便学习到的模型在测试数据上表现良好。
- en: '*Regularization* is a technique to structure the parameters in a form you prefer,
    often to solve the problem of overfitting (see figure 3.13). In this case, you
    anticipate the learned coefficients to be 0 everywhere except for the second term,
    thus producing the curve *y* = *x*². The regression algorithm may produce curves
    that score well but look strangely overcomplicated.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*正则化*是一种技术，可以将参数结构化为你偏好的形式，通常用于解决过拟合问题（见图3.13）。在这种情况下，你预计学习到的系数除了第二项外，其他地方都为0，从而产生曲线
    *y* = *x*²。回归算法可能会产生得分较高的曲线，但看起来过于复杂。'
- en: '![CH03_F13_Mattmann2](../Images/CH03_F13_Mattmann2.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F13_Mattmann2](../Images/CH03_F13_Mattmann2.png)'
- en: Figure 3.13 An overview of regularization. The modeling process takes data (X)
    as input and tries to learn the model parameters (W) that minimize the cost function
    or distance between the model predictions and the ground truth. The top yellow
    quadrant shows a 2D model parameter space for picking weights for simplicity.
    Regularization ensures that the training algorithm does not select weights in
    the less-than-ideal (yellow) areas, but stays inside the white circle of ideal
    weight values.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13 正则化的概述。建模过程以数据（X）作为输入，并试图学习模型参数（W），这些参数最小化成本函数或模型预测与真实值之间的距离。顶部黄色象限显示一个二维模型参数空间，用于选择权重以简化问题。正则化确保训练算法不会选择低于理想（黄色）区域的权重，而是保持在理想权重值的白色圆圈内部。
- en: 'To influence the learning algorithm to produce a smaller coefficient vector
    (let’s call it *w*), you add that penalty to the loss term. To control how significantly
    you want to weigh the penalty term, you multiply the penalty by a constant non-negative
    number, λ, as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了影响学习算法产生一个更小的系数向量（让我们称它为 *w*），你将这个惩罚项加到损失项中。为了控制你想要多显著地权衡惩罚项，你将惩罚乘以一个常数非负数，λ，如下所示：
- en: '*Cost*(*X*, *Y* ) = *Loss*(*X*, *Y* ) + λ'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*成本*(*X*, *Y* ) = *损失*(*X*, *Y* ) + λ'
- en: If λ is set to 0, regularization isn’t in play. As you set λ to larger and larger
    values, parameters with larger norms will be heavily penalized. The choice of
    norm varies case by case, but parameters are typically measured by their L1 or
    L2 norm. Simply put, regularization reduces some of the flexibility of the otherwise
    easily tangled model.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果λ设置为0，则正则化不起作用。随着你将λ设置为更大的值，具有较大范数的参数将受到严重惩罚。范数的选择因情况而异，但参数通常通过它们的L1或L2范数来衡量。简单来说，正则化减少了模型本应容易纠缠的灵活性。
- en: To figure out which value of the regularization parameter λ performs best, you
    must split your dataset into two disjointed sets. About 70% of the randomly chosen
    input/output pairs will consist of the training dataset; the remaining 30% will
    be used for testing. You’ll use the function provided in listing 3.4 for splitting
    the dataset.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找出正则化参数λ的最佳值，你必须将数据集分割成两个不相交的集合。大约70%的随机选择的输入/输出对将组成训练数据集；剩余的30%将用于测试。你将使用列表3.4中提供的函数来分割数据集。
- en: Listing 3.4 Splitting the dataset into testing and training sets
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.4 将数据集分割为测试集和训练集
- en: '[PRE3]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Takes the input and output dataset as well as the desired split ratio
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 以输入输出数据集以及期望的分割比率为输入
- en: ❷ Shuffles a list of numbers
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打乱数字列表
- en: ❸ Calculates the number of training examples
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算训练示例的数量
- en: ❹ Uses the shuffled list to split the x_dataset
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用打乱后的列表来分割x_dataset
- en: ❺ Likewise, splits the y_dataset
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 同样，分割y_dataset
- en: ❻ Returns the split x and y datasets
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 返回分割后的x和y数据集
- en: Exercise 3.3
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 练习3.3
- en: 'A Python library called SK-learn supports many useful data-preprocessing algorithms.
    You can call a function in SK-learn to do exactly what listing 3.4 achieves. Can
    you find this function in the library’s documentation? (Hint: See [http://mng.bz/7Grm](http://mng.bz/7Grm).)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一个名为SK-learn的Python库支持许多有用的数据处理算法。你可以调用SK-learn中的一个函数来完成列表3.4所实现的功能。你能在库的文档中找到这个函数吗？（提示：见[http://mng.bz/7Grm](http://mng.bz/7Grm)。）
- en: '**Answer**'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: It’s called `sklearn.model_selection.train_test_split`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 它被称为`sklearn.model_selection.train_test_split`。
- en: With this handy tool, you can begin testing which value of performs best on
    your data. Open a new Python file, and follow along with listing 3.5.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个便捷的工具，你可以开始测试哪个值在你的数据上表现最佳。打开一个新的Python文件，并按照列表3.5进行操作。
- en: Listing 3.5 Evaluating regularization parameters
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.5 评估正则化参数
- en: '[PRE4]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Imports the relevant libraries and initializes the hyperparameters
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入相关库并初始化超参数
- en: ❷ Creates a fake dataset, y = x²
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个假数据集，y = x²
- en: ❸ Splits the dataset into 70% training and 30% testing, using listing 3.4
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用列表3.4将数据集分割为70%训练和30%测试
- en: ❹ Sets up the input/output placeholders
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 设置输入/输出占位符
- en: ❺ Defines your model
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 定义你的模型
- en: ❻ Defines the regularized cost function
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义正则化成本函数
- en: ❼ Sets up the session
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 设置会话
- en: ❽ Tries various regularization parameters
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 尝试各种正则化参数
- en: ❾ Closes the session
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 关闭会话
- en: If you plot the corresponding output per each regularization parameter from
    listing 3.5, you can see how the curve changes as λ increases. When λ is 0, the
    algorithm favors using the higher-order terms to fit the data. As you start penalizing
    parameters with a high L2 norm, the cost decreases, indicating that you’re recovering
    from overfitting, as shown in figure 3.14.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你绘制列表3.5中每个正则化参数对应的输出，你可以看到曲线如何随着λ的增加而变化。当λ为0时，算法倾向于使用高阶项来拟合数据。当你开始惩罚具有高L2范数的参数时，成本降低，表明你正在从过拟合中恢复，如图3.14所示。
- en: '![CH03_F14_Mattmann2](../Images/CH03_F14_Mattmann2.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F14_Mattmann2](../Images/CH03_F14_Mattmann2.png)'
- en: Figure 3.14 As you increase the regularization parameter to some extent, the
    cost decreases. This result implies that the model was originally overfitting
    the data and that regularization helped add structure
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14 随着正则化参数的增加，成本降低。这一结果意味着模型最初过度拟合了数据，而正则化有助于增加结构
- en: TensorFlow library support for regularization
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow库对正则化的支持
- en: TensorFlow is a fully capable library for supporting machine learning, and even
    though the focus in this section is on how to implement regularization yourself,
    the library provides its own functions for computing L2 regularization. You can
    use the function `tf.nn.l2_loss(weights``)` to produce equivalent results by adding
    the regularization loss to your cost function for each of your weights.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是一个完全能够支持机器学习的库，尽管本节的重点是如何自己实现正则化，但该库提供了自己的函数来计算L2正则化。你可以使用函数`tf.nn.l2_loss(weights)`通过将正则化损失添加到每个权重的成本函数中，来产生等效的结果。
- en: 3.5 Application of linear regression
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 线性回归的应用
- en: 'Running linear regression on fake data is like buying a new car and never driving
    it. This awesome machinery begs to manifest itself in the real world! Fortunately,
    many datasets are available online to test your newfound knowledge of regression:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在假数据上运行线性回归就像买了一辆新车却从未驾驶过。这神奇的机械渴望在现实世界中展现自己！幸运的是，许多数据集都可在网上找到，以测试你对回归的新发现知识：
- en: The University of Massachusetts Amherst supplies small datasets of various types
    at [https://scholarworks.umass.edu/data](https://scholarworks.umass.edu/data/).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马萨诸塞大学阿默斯特分校在 [https://scholarworks.umass.edu/data](https://scholarworks.umass.edu/data/)
    提供了各种类型的小型数据集。
- en: Kaggle provides all types of large-scale data for machine-learning competitions
    at [https://www.kaggle.com/datasets](https://www.kaggle.com/datasets).
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaggle 在 [https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)
    提供了所有类型的大规模数据，用于机器学习竞赛。
- en: Data.gov ([https://catalog.data.gov](https://catalog.data.gov)) is an open data
    initiative by the US government that contains many interesting and practical datasets.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Data.gov ([https://catalog.data.gov](https://catalog.data.gov)) 是美国政府的一项开放数据倡议，其中包含许多有趣且实用的数据集。
- en: A good number of datasets contain dates. You can find a dataset of all phone
    calls to the 311 nonemergency line in Los Angeles, California, for example, at
    [https://www.dropbox.com/s/naw774olqkve7sc/311.csv?dl=0](https://www.dropbox.com/s/naw774olqkve7sc/311.csv?dl=0).
    A good feature to track could be the frequency of calls per day, week, or month.
    For convenience, listing 3.6 allows you to obtain a weekly frequency count of
    data items.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 许多数据集包含日期。例如，你可以在 [https://www.dropbox.com/s/naw774olqkve7sc/311.csv?dl=0](https://www.dropbox.com/s/naw774olqkve7sc/311.csv?dl=0)
    找到加利福尼亚州洛杉矶所有 311 非紧急电话的数据集。一个很好的跟踪特征可能是每天、每周或每月的通话频率。为了方便起见，列表 3.6 允许你获得数据项的每周频率计数。
- en: Listing 3.6 Parsing raw CSV datasets
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.6 解析原始 CSV 数据集
- en: '[PRE5]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ For reading CSV files easily
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为了轻松读取 CSV 文件
- en: ❷ For using useful date functions
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为了使用有用的日期函数
- en: ❸ Sets up initial frequency map
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 设置初始频率映射
- en: ❹ Reads data and aggregates count per period
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 读取数据并按周期汇总计数
- en: ❺ Obtains a weekly frequency count of 311 phone calls in 2014
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 获取 2014 年 311 电话通话的每周频率计数
- en: This code gives you the training data for linear regression. The `freq` variable
    is a dictionary that maps a period (such as a week) to a frequency count. A year
    has 52 weeks, so you’ll have 52 data points if you leave `bucket=7` as is.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码为你提供了线性回归的训练数据。`freq` 变量是一个字典，将周期（如一周）映射到频率计数。一年有 52 周，所以如果你将 `bucket=7`
    保持不变，你将会有 52 个数据点。
- en: Now that you have data points, you have exactly the input and output necessary
    to fit a regression model by using the techniques covered in this chapter. More
    practically, you can use the learned model to interpolate or extrapolate frequency
    counts.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了数据点，你正好有输入和输出，可以使用本章介绍的技术来拟合回归模型。更实际地说，你可以使用学习到的模型来插值或外推频率计数。
- en: Summary
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Regression is a type of supervised machine learning for predicting continuous-valued
    output.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归是一种监督机器学习方法，用于预测连续值输出。
- en: By defining a set of models, you greatly reduce the search space of possible
    functions. Moreover, TensorFlow takes advantage of the differentiable property
    of the functions by running its efficient gradient-descent optimizers to learn
    the parameters.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过定义一组模型，你大大减少了可能函数的搜索空间。此外，TensorFlow 通过运行其高效的梯度下降优化器来利用函数的可微性，从而学习参数。
- en: You can easily modify linear regression to learn polynomials and other, more
    complicated curves.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以轻松地将线性回归修改为学习多项式和其他更复杂的曲线。
- en: To avoid overfitting your data, regularize the cost function by penalizing larger-valued
    parameters.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了避免数据过拟合，通过惩罚较大值的参数来正则化成本函数。
- en: If the output of the function isn’t continuous, use a classification algorithm
    instead (see chapter 4).
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果函数的输出不连续，则使用分类算法代替（参见第 4 章）。
- en: TensorFlow enables you to solve linear-regression machine-learning problems
    effectively and efficiently, and hence to make useful predictions about important
    matters such as agricultural production, heart conditions, and housing prices.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 使你能够有效地解决线性回归机器学习问题，并因此对诸如农业生产、心脏病和房价等重要问题做出有用的预测。
