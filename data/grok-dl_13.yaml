- en: 'Chapter 14\. Learning to write like Shakespeare: long short-term memory'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第14章。学习像莎士比亚一样写作：长短期记忆
- en: '**In this chapter**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章**'
- en: Character language modeling
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符语言模型
- en: Truncated backpropagation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 截断反向传播
- en: Vanishing and exploding gradients
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度消失和梯度爆炸
- en: A toy example of RNN backpropagation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN反向传播的一个玩具示例
- en: Long short-term memory (LSTM) cells
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）单元
- en: “Lord, what fools these mortals be!”
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “主啊，这些凡人多么愚蠢！”
- en: ''
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*William Shakespeare *A Midsummer Night’s Dream**'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*莎士比亚 *仲夏夜之梦**'
- en: Character language modeling
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 字符语言模型
- en: Let’s tackle a more challenging task with the RNN
  id: totrans-11
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们用RNN解决一个更具挑战性的任务
- en: At the end of [chapters 12](kindle_split_020.xhtml#ch12) and [13](kindle_split_021.xhtml#ch13),
    you trained vanilla recurrent neural networks (RNNs) that learned a simple series
    prediction problem. But you were training over a toy dataset of phrases that were
    synthetically generated using rules.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在第12章和第13章的结尾，你训练了简单的循环神经网络（RNN），这些网络学习了一个简单的序列预测问题。但你是在一个玩具数据集上训练的，这个数据集是通过规则合成的短语。
- en: 'In this chapter, you’ll attempt language modeling over a much more challenging
    dataset: the works of Shakespeare. And instead of learning to predict the next
    word given the previous words (as in the preceding chapter), the model will train
    on characters. It needs to learn to predict the next character given the previous
    characters observed. Here’s what I mean:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将尝试在一个更具挑战性的数据集上进行语言建模：莎士比亚的作品。而且，与上一章中学习根据前面的单词预测下一个单词不同，该模型将训练在字符上。它需要学习根据观察到的先前字符预测下一个字符。这就是我的意思：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***1* From [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)**'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 来自 [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)**'
- en: 'Whereas in [chapters 12](kindle_split_020.xhtml#ch12) and [13](kindle_split_021.xhtml#ch13)
    the vocabulary was made up of the words from the dataset, now the vocabulary is
    made up the characters in the dataset. As such, the dataset is also transformed
    into a list of indices corresponding to characters instead of words. Above this
    is the `indices` NumPy array:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与第12章和第13章的词汇表由数据集中的单词组成不同，现在词汇表由数据集中的字符组成。因此，数据集也被转换为一个索引列表，这些索引对应于字符而不是单词。之上是`indices`
    NumPy数组：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This code should all look familiar. It initializes the embeddings to be of dimensionality
    8 and the RNN hidden state to be of size 512\. The output weights are initialized
    as 0s (not a rule, but I found it worked a bit better). Finally, you initialize
    the cross-entropy loss and stochastic gradient descent optimizer.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码看起来都很熟悉。它初始化嵌入维度为8，RNN隐藏状态的大小为512。输出权重初始化为0（这不是规则，但我觉得这样效果更好）。最后，你初始化交叉熵损失和随机梯度下降优化器。
- en: The need for truncated backpropagation
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 截断反向传播的需要
- en: Backpropagating through 100,000 characters is intractable
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过100,000个字符进行反向传播是不可行的
- en: 'One of the more challenging aspects of reading code for RNNs is the mini-batching
    logic for feeding in data. The previous (simpler) neural network had an inner
    `for` loop like this (the **bold** part):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读RNN代码的更具挑战性的方面之一是为输入数据而进行的批处理逻辑。之前的（更简单的）神经网络有一个这样的内部`for`循环（粗体部分）：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You might ask, “Why iterate to 5?” As it turns out, the previous dataset didn’t
    have any example longer than six words. It read in five words and then attempted
    to predict the sixth.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，“为什么迭代到5？”实际上，之前的语料库没有超过六个单词的例子。它读取了五个单词，然后尝试预测第六个。
- en: 'Even more important is the backpropagation step. Consider when you did a simple
    feedforward network classifying MNIST digits: the gradients always backpropagated
    all the way through the network, right? They kept backpropagating until they reached
    the input data. This allowed the network to adjust every weight to try to learn
    how to correctly predict given the entire input example.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是反向传播步骤。考虑当你对一个简单的前馈网络进行MNIST数字分类时：梯度总是反向传播到整个网络，对吧？它们一直反向传播，直到达到输入数据。这允许网络调整每个权重，试图学习如何根据整个输入示例正确预测。
- en: The recurrent example here is no different. You forward propagate through five
    input examples and then, when you later call `loss.backward()`, it backpropagates
    gradients all the way back through the network to the input datapoints. You can
    do this because you aren’t feeding in that many input datapoints at a time. But
    the Shakespeare dataset has 100,000 characters! This is way too many to backpropagate
    through for every prediction. What do you do?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的循环例子也没有不同。你通过五个输入示例进行前向传播，然后，当你稍后调用`loss.backward()`时，它将梯度反向传播回网络到输入数据点。你可以这样做，因为你一次没有输入那么多数据点。但是莎士比亚数据集有10万个字符！这对于每个预测进行反向传播来说太多了。你怎么办？
- en: You don’t! You backpropagate for a fixed number of steps into the past and then
    stop. This is called *truncated backpropagation*, and it’s the industry standard.
    The length you backprop becomes another tunable parameter (like batch size or
    alpha).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要！你向后传播一个固定数量的步骤到过去，然后停止。这被称为*截断反向传播*，并且是行业标准。你向后传播的长度成为另一个可调参数（就像批量大小或alpha）。
- en: Truncated backpropagation
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 截断反向传播
- en: Technically, it weakens the theoretical maximum of the neural network
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从技术上讲，它削弱了神经网络的最高理论极限
- en: The downside of using truncated backpropagation is that it shortens the distance
    a neural network can learn to remember things. Basically, cutting off gradients
    after, say, five timesteps, means the neural network can’t learn to remember events
    that are longer than five timesteps in the past.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用截断反向传播的缺点是它缩短了神经网络可以学习记住事物的距离。基本上，在比如说五个时间步长之后切断梯度，意味着神经网络无法学习记住过去超过五个时间步长的事件。
- en: 'Strictly speaking, it’s more nuanced than this. There can accidentally be residual
    information in an RNN’s hidden layer from more than five timesteps in the past,
    but the neural network can’t use gradients to specifically request that the model
    keep information around from six timesteps in the past to help with the current
    prediction. Thus, in practice, neural networks won’t learn to make predictions
    based on input signal from more than five timesteps in the past (if truncation
    is set at five timesteps). In practice, for language modeling, the truncation
    variable is called `bptt`, and it’s usually set somewhere between 16 and 64:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，情况比这更复杂。在RNN的隐藏层中，从过去超过五个时间步长可能会意外地保留一些残留信息，但神经网络不能使用梯度来特别请求模型从六个时间步长之前保留信息以帮助当前预测。因此，在实践中，神经网络不会学习基于过去超过五个时间步长的输入信号进行预测（如果截断设置为五个时间步长）。在实践中，对于语言建模，截断变量被称为`bptt`，它通常设置在16到64之间：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The other downside of truncated backpropagation is that it makes the mini-batching
    logic a bit more complex. To use truncated backpropagation, you pretend that instead
    of having one big dataset, you have a bunch of small datasets of size `bptt`.
    You need to group the datasets accordingly:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 截断反向传播的另一个缺点是它使得小批量逻辑变得稍微复杂一些。要使用截断反向传播，你假装你有一个大数据集，而不是大小为`bptt`的一堆小数据集。你需要相应地分组数据集：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'There’s a lot going on here. The top line makes the dataset an even multiple
    between the `batch_size` and `n_batches`. This is so that when you group it into
    tensors, it’s square (alternatively, you could pad the dataset with 0s to make
    it square). The second and third lines reshape the dataset so each column is a
    section of the initial `indices` array. I’ll show you that part, as if `batch_size`
    was set to 8 (for readability):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多事情在进行中。最上面一行使得数据集成为`batch_size`和`n_batches`之间的一个偶数倍。这样做是为了当你将其分组为张量时，它是平方的（或者你也可以用0填充数据集以使其成为平方）。第二行和第三行重新塑形数据集，使得每一列是初始`indices`数组的一个部分。我将展示这部分，就像`batch_size`被设置为8（为了可读性）：
- en: '|'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '|'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '|'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Those are the first five characters in the Shakespeare dataset. They spell
    out the string “That,”. Following are the first five rows of the output of the
    transformation contained within `batched_indices`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是莎士比亚数据集中的前五个字符。它们拼写出字符串“That，”。接下来是`batched_indices`中变换后的前五行的输出：
- en: '|'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '|'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '|'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: I’ve highlighted the first column in bold. See how the indices for the phrase
    “That,” are in the first column on the left? This is a standard construction.
    The reason there are eight columns is that the `batch_size` is 8\. This tensor
    is then used to construct a list of smaller datasets, each of length `bptt`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经用粗体突出显示了第一列。看看短语“That，”的索引是否在左边的第一列？这是一个标准构造。有八个列的原因是`batch_size`是8。这个张量随后被用来构建一个更小的数据集列表，每个数据集的长度为`bptt`。
- en: You can see here how the input and target are constructed. Notice that the target
    indices are the input indices offset by one row (so the network predicts the next
    character). Note again that `batch_size` is 8 in this printout so it’s easier
    to read, but you’re really setting it to 32.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里看到输入和目标是如何构建的。注意，目标索引是输入索引偏移一行（因此网络预测下一个字符）。再次注意，在这个打印输出中`batch_size`设置为8，这使得阅读更容易，但实际上你将其设置为32。
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Don’t worry if this doesn’t make sense to you yet. It doesn’t have much to do
    with deep learning theory; it’s just a particularly complex part of setting up
    RNNs that you’ll run into from time to time. I thought I’d spend a couple of pages
    explaining it.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这对你来说现在还不明白，也不要担心。这和深度学习理论关系不大；这只是设置 RNN 时一个特别复杂的部分，你有时会遇到。我想花几页纸来解释它。
- en: Let’s see how to iterate using truncated backpropagation
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们看看如何使用截断反向传播进行迭代
- en: 'The following code shows truncated backpropagation in practice. Notice that
    it looks very similar to the iteration logic from [chapter 13](kindle_split_021.xhtml#ch13).
    The only real difference is that you generate a `batch_loss` at each step; and
    after every `bptt` steps, you backpropagate and perform a weight update. Then
    you keep reading through the dataset like nothing happened (even using the same
    hidden state from before, which only gets reset with each epoch):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了截断反向传播的实际应用。注意，它看起来与第13章中的迭代逻辑非常相似。唯一的真正区别是，你会在每个步骤生成一个`batch_loss`；并且每进行一次`bptt`步骤后，你都会进行反向传播并更新权重。然后你继续像什么都没发生一样读取数据集（甚至使用之前的相同隐藏状态，它仅在每轮中重置）：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: A sample of the output
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出样本的一个示例
- en: By sampling from the predictions of the model, you can write Shakespeare!
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过从模型的预测中采样，你可以写出莎士比亚的作品！
- en: 'The following code uses a subset of the training logic to make predictions
    using the model. You store the predictions in a string and return the string version
    as output to the function. The sample that’s generated looks quite Shakespearian
    and even includes characters talking:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码使用训练逻辑的子集来使用模型进行预测。你将预测存储在一个字符串中，并将字符串版本作为输出返回给函数。生成的样本看起来非常像莎士比亚的作品，甚至包括对话角色：
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '***1* Temperature for sampling; higher = greedier**'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 样本采样的温度；越高 = 越贪婪**'
- en: '***2* Samples from pred**'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 预测样本**'
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Vanishing and exploding gradients
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 消失和爆炸梯度
- en: Vanilla RNNs suffer from vanishing and exploding gradients
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 简单的 RNN 会受到消失和爆炸梯度的影响
- en: 'You may recall this image from when you first put together a RNN. The idea
    was to be able to combine the word embeddings in a way that order mattered. You
    did this by learning a matrix that transformed each embedding to the next timestep.
    Forward propagation then became a two-step process: start with the first word
    embedding (the embedding for “Red” in the following example), multiply by the
    weight matrix, and add the next embedding (“Sox”). You then take the resulting
    vector, multiply it by the same weight matrix, and then add in the next word,
    repeating until you’ve read in the entire series of words.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得当你第一次组合 RNN 时的这个图像。想法是能够以某种方式组合单词嵌入，使得顺序很重要。你是通过学习一个矩阵来做到这一点的，该矩阵将每个嵌入转换到下一个时间步。然后，前向传播变成了两步过程：从第一个单词嵌入（以下示例中的“Red”嵌入）开始，乘以权重矩阵，并加上下一个嵌入（“Sox”）。然后你将得到的向量乘以相同的权重矩阵，并加入下一个单词，重复这个过程，直到读取整个单词序列。
- en: '![](Images/f0272-01.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0272-01.jpg)'
- en: 'But as you know, an additional nonlinearity was added to the hidden state-generation
    process. Thus, forward propagation becomes a three-step process: matrix multiply
    the previous hidden state by a weight matrix, add in the next word’s embedding,
    and apply a nonlinearity.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 但正如你所知，隐藏状态生成过程中添加了一个额外的非线性项。因此，前向传播变成了一个三步过程：将前一个隐藏状态通过权重矩阵进行矩阵乘法，加入下一个单词的嵌入，并应用非线性函数。
- en: Note that this nonlinearity plays an important role in the stability of the
    network. No matter how long the sequence of words is, the hidden states (which
    could in theory grow larger and larger over time) are forced to stay between the
    values of the nonlinearity (between 0 and 1, in the case of a sigmoid). But backpropagation
    happens in a slightly different way than forward propagation, which doesn’t have
    this nice property. Backpropagation tends to lead to either extremely large or
    extremely small values. Large values can cause divergence (lots of not-a-numbers
    [NaNs]), whereas extremely small values keep the network from learning. Let’s
    take a closer look at RNN backpropagation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这种非线性在网络稳定性中起着重要作用。无论单词序列有多长，隐藏状态（理论上可能随时间增长而增长）都被迫保持在非线性函数的值之间（在 sigmoid
    的情况下是 0 到 1）。但是，反向传播发生的方式与正向传播略有不同，不具有这种良好的性质。反向传播往往会引起极端大或极端小的值。大值可能导致发散（许多非数字
    [NaN]），而极端小的值则使网络无法学习。让我们更仔细地看看 RNN 反向传播。
- en: '![](Images/f0272-02.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0272-02.jpg)'
- en: A toy example of RNN backpropagation
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RNN 反向传播的玩具示例
- en: To see vanishing/exploding gradients firsthand, let’s synthesize an example
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为了亲身体验梯度消失/爆炸，让我们合成一个示例
- en: The following code shows a recurrent backpropagation loop for `sigmoid` and
    `relu` activations. Notice how the gradients become very small/large for `sigmoid`/`relu`,
    respectively. During backprop, they become large as the result of the matrix multiplication,
    and small as a result of the `sigmoid` activation having a very flat derivative
    at its tails (common for many nonlinearities).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了 `sigmoid` 和 `relu` 激活的反向传播循环。注意，对于 `sigmoid`/`relu`，梯度分别变得非常小/大。在反向传播过程中，由于矩阵乘法的结果，它们变得很大，而由于
    `sigmoid` 激活在尾部具有非常平坦的导数（许多非线性函数的常见情况），它们又变得很小。
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '***1* The derivative of sigmoid causes very small gradients when activation
    is very near 0 or 1 (the tails).**'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* sigmoid 的导数在激活值非常接近 0 或 1（尾部）时会导致非常小的梯度。**'
- en: '***2* The matrix multiplication causes exploding gradients that don’t get squished
    by a nonlinearity (as in sigmoid).**'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 矩阵乘法会导致梯度爆炸，而非线性函数（如 sigmoid）无法将其压缩。**'
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Long short-term memory (LSTM) cells
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）单元
- en: LSTMs are the industry standard model to counter vanishing/exploding gradients
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LSTM 是对抗梯度消失/爆炸的行业标准模型
- en: The previous section explained how vanishing/exploding gradients result from
    the way hidden states are updated in a RNN. The problem is the combination of
    matrix multiplication and nonlinearity being used to form the next hidden state.
    The solution that LSTMs provide is surprisingly simple.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节解释了梯度消失/爆炸是如何由 RNN 中隐藏状态的更新方式引起的。问题是矩阵乘法和非线性函数的组合用于形成下一个隐藏状态。LSTM 提供的解决方案出人意料地简单。
- en: '|  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**The gated copy trick**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**门控复制技巧**'
- en: LSTMs create the next hidden state by copying the previous hidden state and
    then adding or removing information as necessary. The mechanisms the LSTM uses
    for adding and removing information are called *gates*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 通过复制前一个隐藏状态并在必要时添加或删除信息来创建下一个隐藏状态。LSTM 用于添加和删除信息的机制被称为 *门*。
- en: '|  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The previous code is the forward propagation logic for the RNN cell. Following
    is the new forward propagation logic for the LSTM cell. The LSTM has two hidden
    state vectors: `h` (for hidden) and `cell`.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码是 RNN 单元的正向传播逻辑。以下是 LSTM 单元的新正向传播逻辑。LSTM 有两个隐藏状态向量：`h`（隐藏）和 `cell`。
- en: The one you care about is `cell`. Notice how it’s updated. Each new cell is
    the previous cell plus `u`, weighted by `i` and `f`. `f` is the “forget” gate.
    If it takes a value of 0, the new cell will erase what it saw previously. If `i`
    is 1, it will fully add in the value of `u` to create the new cell. `o` is an
    output gate that controls how much of the cell’s state the output prediction is
    allowed to see. For example, if `o` is all zeros, then the `self.w_ho.forward(h)`
    line will make a prediction ignoring the cell state entirely.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要关注的是 `cell`。注意它是如何更新的。每个新的单元是前一个单元加上 `u`，通过 `i` 和 `f` 加权。`f` 是“忘记”门。如果它取值为
    0，则新单元将擦除之前看到的内容。如果 `i` 为 1，它将完全添加 `u` 的值以创建新单元。`o` 是一个输出门，它控制输出预测可以查看多少单元状态。例如，如果
    `o` 全为零，则 `self.w_ho.forward(h)` 行将忽略单元状态进行预测。
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Some intuition about LSTM gates
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于 LSTM 门的直觉
- en: LSTM gates are semantically similar to reading/writing from memory
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LSTM 门与从内存中读取/写入的语义相似
- en: So there you have it! There are three gates—`f`, `i`, `o`—and a cell-update
    vector `u`; think of these as forget, input, output, and update, respectively.
    They work together to ensure that any information to be stored or manipulated
    in `c` can be so without requiring each update of `c` to have any matrix multiplications
    or nonlinearities applied to it. In other words, you’re avoiding ever calling
    `nonlinearity(c)` or `c.dot(weights)`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这就是全部了！有三个门控器——`f`、`i`、`o`——和一个细胞更新向量`u`；分别想象这些是忘记、输入、输出和更新。它们共同工作以确保要存储或操作在`c`中的任何信息都可以这样做，而无需要求每次更新`c`时都应用任何矩阵乘法或非线性。换句话说，您正在避免永远调用`nonlinearity(c)`或`c.dot(weights)`。
- en: This is what allows the LSTM to store information across a time series without
    worrying about vanishing or exploding gradients. Each step is a copy (assuming
    `f` is nonzero) plus an update (assuming `i` is nonzero). The hidden value `h`
    is then a masked version of the cell that’s used for prediction.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得LSTM能够在时间序列中存储信息而不用担心梯度消失或梯度爆炸。每一步都是一个复制（假设`f`不为零）加上一个更新（假设`i`不为零）。隐藏值`h`随后是用于预测的细胞的一个掩码版本。
- en: 'Notice further that each of the three gates is formed the same way. They have
    their own weight matrices, but each of them conditions on the input and the previous
    hidden state, passed through a `sigmoid`. It’s this `sigmoid` nonlinearity that
    makes them so useful as gates, because it saturates at 0 and 1:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，三个门控器都是用相同的方式形成的。它们有自己的权重矩阵，但每个门控器都基于输入和前一个隐藏状态，通过一个`sigmoid`函数进行条件化。正是这个`sigmoid`非线性使得它们作为门控器非常有用，因为它在0和1之间饱和：
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'One last possible critique is about `h`. Clearly it’s still prone to vanishing
    and exploding gradients, because it’s basically being used the same as the vanilla
    RNN. First, because the `h` vector is always created using a combination of vectors
    that are squished with `tanh` and `sigmoid`, exploding gradients aren’t really
    a problem—only vanishing gradients. But this ends up being OK because `h` is conditioned
    on `c`, which can carry long-range information: the kind of information vanishing
    gradients can’t learn to carry. Thus, all long-range information is transported
    using `c`, and `h` is only a localized interpretation of `c`, useful for making
    an output prediction and constructing gate activations at the following timestep.
    In short, `c` can learn to transport information over long distances, so it doesn’t
    matter if `h` can’t.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个可能的批评是关于`h`的。显然，它仍然容易受到梯度消失和梯度爆炸的影响，因为它基本上被用来和原始RNN一样。首先，因为`h`向量总是使用一个组合的向量创建，这些向量被`tanh`和`sigmoid`压缩，所以梯度爆炸实际上并不是一个问题——只有梯度消失。但最终这没问题，因为`h`依赖于`c`，它可以携带长距离信息：梯度消失无法学习携带的那种信息。因此，所有长距离信息都是通过`c`传输的，而`h`只是`c`的一个局部解释，对于在下一个时间步进行输出预测和构建门控激活很有用。简而言之，`c`可以学习在长距离上传输信息，所以即使`h`不能，这也没有关系。
- en: The long short-term memory layer
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 长短期记忆层
- en: You can use the autograd system to implement an LSTM
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 您可以使用自动微分系统来实现一个LSTM
- en: '[PRE18]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Upgrading the character language model
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 升级字符语言模型
- en: Let’s swap out the vanilla RNN with the new LSTM cell
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们用新的LSTM单元替换原始RNN
- en: 'Earlier in this chapter, you trained a character language model to predict
    Shakespeare. Now let’s train an LSTM-based model to do the same. Fortunately,
    the framework from the preceding chapter makes this easy to do (the complete code
    from the book’s website, [www.manning.com/books/grokking-deep-learning](http://www.manning.com/books/grokking-deep-learning);
    or on GitHub at [https://github.com/iamtrask/grokking-deep-learning](https://github.com/iamtrask/grokking-deep-learning)).
    Here’s the new setup code. All edits from the vanilla RNN code are in bold. Notice
    that hardly anything has changed about how you set up the neural network:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期，您训练了一个字符语言模型来预测莎士比亚。现在让我们训练一个基于LSTM的模型来做同样的事情。幸运的是，上一章的框架使得这变得很容易实现（书中的完整代码可在[www.manning.com/books/grokking-deep-learning](http://www.manning.com/books/grokking-deep-learning)；或在GitHub上[https://github.com/iamtrask/grokking-deep-learning](https://github.com/iamtrask/grokking-deep-learning)找到）。以下是新的设置代码。所有从原始RNN代码的编辑都在粗体中。注意，您设置神经网络的方式几乎没有变化：
- en: '[PRE19]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '***1* This seemed to help training.**'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 这似乎有助于训练。**'
- en: Training the LSTM character language model
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练LSTM字符语言模型
- en: The training logic also hasn’t changed much
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练逻辑也没有发生太多变化
- en: 'The only real change you have to make from the vanilla RNN logic is the truncated
    backpropagation logic, because there are two hidden vectors per timestep instead
    of one. But this is a relatively minor fix (in bold). I’ve also added a few bells
    and whistles that make training easier (`alpha` slowly decreases over time, and
    there’s more logging):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从标准的 RNN 逻辑中，你唯一需要做的真正改变是截断反向传播逻辑，因为每个时间步长有两个隐藏向量而不是一个。但这只是一个相对较小的修复（粗体）。我还增加了一些使训练更简单的功能（`alpha`
    随时间缓慢减少，并且有更多的日志记录）：
- en: '[PRE20]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Tuning the LSTM character language model
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调整 LSTM 字符语言模型
- en: I spent about two days tuning this model, and it trained overnight
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我花费了大约两天的时间调整这个模型，并且它是在夜间训练完成的
- en: Here’s some of the training output for this model. Note that it took a very
    long time to train (there are a *lot* of parameters). I also had to train it many
    times in order to find a good tuning (learning rate, batch size, and so on) for
    this task, and the final model trained overnight (8 hours). In general, the longer
    you train, the better your results will be.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是此模型的某些训练输出。请注意，训练这个模型花费了非常长的时间（有很多参数）。我还不得不多次训练它，以找到这个任务的良好调整（学习率、批量大小等），并且最终的模型是在夜间（8小时）训练完成的。一般来说，你训练的时间越长，你的结果就会越好。
- en: '[PRE21]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '***1* Takes the max prediction**'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 取最大预测**'
- en: '[PRE23]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Summary
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: LSTMs are incredibly powerful models
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LSTM 是极其强大的模型
- en: The distribution of Shakespearian language that the LSTM learned to generate
    isn’t to be taken lightly. Language is an incredibly complex statistical distribution
    to learn, and the fact that LSTMs can do so well (at the time of writing, they’re
    the state-of-the-art approach by a wide margin) still baffles me (and others as
    well). Small variants on this model either are or have recently been the state
    of the art in a wide variety of tasks and, alongside word embeddings and convolutional
    layers, will undoubtedly be one of our go-to tools for a long time to come.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 学习生成莎士比亚语言的分布不容小觑。语言是一个极其复杂的统计分布，学习起来非常困难，而 LSTM 能够做得如此出色（在撰写本文时，它们是广泛领先的最佳方法）仍然让我（以及其他一些人）感到困惑。这个模型的小型变体要么是，要么最近一直是各种任务中的最佳状态，并且与词嵌入和卷积层一起，无疑将是我们长期以来的首选工具。
