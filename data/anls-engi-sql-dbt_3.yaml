- en: Chapter 3\. SQL for Analytics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章：分析用 SQL
- en: In the vast landscape of data and analytics, it is critical to choose the right
    tools and technologies to efficiently process and manipulate data. One such tool
    that has stood the test of time and remains at the forefront is *Structured Query
    Language* (SQL). It offers a powerful and versatile approach to working with data,
    making it an excellent first choice for any analytical development task. SQL is
    a standardized programming language for managing and manipulating relational databases
    that enables data professionals to efficiently retrieve, store, modify, and analyze
    data stored in databases. Thanks to its intuitive syntax and wide acceptance in
    the community, SQL has become the standard language of data specialists, who use
    it to interact with databases and gain valuable insights from complex datasets.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据和分析的广阔领域中，选择正确的工具和技术以有效处理和操作数据至关重要。一个经受住时间考验并始终处于前沿的工具是*结构化查询语言*（SQL）。它提供了一种强大而多功能的处理数据的方法，使其成为处理任何分析开发任务的优秀首选。SQL
    是一种用于管理和操作关系数据库的标准化编程语言，使数据专业人员能够高效地检索、存储、修改和分析存储在数据库中的数据。由于其直观的语法和社区的广泛接受，SQL
    已经成为数据专家的标准语言，他们用它与数据库进行交互，并从复杂数据集中获得宝贵的见解。
- en: SQL serves as the spine for data consumption and analysis in today’s data-driven
    world. Businesses rely heavily on it in performing their data analytics operations
    to gain a competitive advantage. SQL’s versatility and rich functionality make
    it an essential tool for analytics professionals, empowering them to retrieve
    specific subsets of data, perform complex aggregations, and join multiple tables
    to find hidden patterns and relationships within the data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 在当今数据驱动的世界中作为数据消费和分析的支柱。企业在执行数据分析操作以获得竞争优势时高度依赖它。SQL 的多功能性和丰富功能使其成为分析专业人员的重要工具，使他们能够检索特定子集的数据，执行复杂的聚合操作，并连接多个表以发现数据中隐藏的模式和关系。
- en: One of SQL’s key strengths is its ability to retrieve and manipulate data quickly,
    which provides a wide range of query capabilities. This allows data specialists
    to filter, sort, and group data based on specific criteria, retrieving only the
    necessary data, and thus minimizing resource usage and improving performance.
    Furthermore, SQL enables data manipulation operations, such as inserting, updating,
    and deleting records, facilitating data cleaning and preparation tasks before
    analysis.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 的关键优势之一是其快速检索和操作数据的能力，提供了广泛的查询功能。这使得数据专家能够基于特定的标准对数据进行筛选、排序和分组，仅检索必要的数据，从而最大限度地减少资源使用并提高性能。此外，SQL
    还支持数据操作，如插入、更新和删除记录，有助于在分析之前进行数据清洗和准备任务。
- en: Another relevant benefit of using SQL is its seamless integration with various
    analytics tools and ecosystems, such as Python or BI platforms, making it a preferred
    language for data professionals and allowing them to combine the power of SQL
    with advanced statistical analysis, machine learning algorithms, and interactive
    visualizations. Additionally, the rise of cloud-based databases and data warehouses
    has further enhanced the relevance of SQL in analytics consumption, with platforms
    like Google BigQuery, Amazon Redshift, and Snowflake supporting SQL as their primary
    querying language.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SQL 的另一个相关好处是它与各种分析工具和生态系统（如 Python 或 BI 平台）的无缝集成，使其成为数据专业人士的首选语言，并允许他们将
    SQL 的强大功能与高级统计分析、机器学习算法和交互式可视化结合起来。此外，基于云的数据库和数据仓库的崛起进一步增强了 SQL 在分析消费中的相关性，像谷歌的
    BigQuery、亚马逊的 Redshift 和 Snowflake 等平台支持 SQL 作为其主要查询语言。
- en: In this chapter, we’ll discuss the resiliency of the SQL language as one of
    the most commonly used analytics languages. Then we will explore the fundamentals
    of databases, introducing SQL as the standard language for interacting with them.
    We also examine the creation and usage of views, which provide a powerful mechanism
    for simplifying complex queries and abstracting data structures.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论 SQL 语言作为最常用的分析语言之一的弹性。然后，我们将探讨数据库的基础知识，并介绍 SQL 作为与其交互的标准语言。我们还将研究视图的创建和使用，这为简化复杂查询和抽象数据结构提供了强大的机制。
- en: As we dig deeper into SQL, we will review the window functions that empower
    you to perform advanced calculations and aggregations. Furthermore, we’ll dive
    into CTEs, which provide a means to create temporary result sets and simplify
    complex queries. Finally, we will also provide a glimpse into SQL for distributed
    data processing, ending with a bonus section presenting SQL in action for training
    machine learning models.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们深入研究SQL时，我们将回顾窗口函数，这些函数使您能够进行高级计算和聚合。此外，我们还将深入研究CTE（公共表达式），它们提供了创建临时结果集和简化复杂查询的方法。最后，我们还将提供SQL用于分布式数据处理的一瞥，最后以一个额外的部分展示SQL在训练机器学习模型中的应用。
- en: The Resiliency of SQL
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SQL的韧性
- en: Over time, we have seen that data engineering pipelines developed in SQL often
    endure for many years and that queries and stored procedures are still the core
    of several critical systems supporting financial institutions, retail companies,
    and even scientific activities. What is amazing, however, is that SQL has been
    widely used and continuously evolved to meet the demands of modern data processing
    with new features. Also, it is fascinating that technologies such as dbt, DuckDB,
    and even the new data manipulation library Polars provide their functionalities
    through a SQL interface. But what is the main reason for this popularity? We believe
    that a few factors can be highlighted.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，我们发现使用SQL开发的数据工程管道通常能够持续多年，而查询和存储过程仍然是支持金融机构、零售公司甚至科学活动的几个关键系统的核心。然而，令人惊讶的是，SQL已被广泛应用并不断演进，以满足现代数据处理的需求，引入了新的功能。此外，令人着迷的是，诸如dbt、DuckDB甚至新的数据操作库Polars通过SQL接口提供其功能。但是，这种流行背后的主要原因是什么呢？我们认为可以强调几个因素。
- en: First and foremost is the readability of the code. This is a crucial aspect
    of data engineering. SQL’s syntax, while versatile, allows for both imperative
    and declarative usage, depending on the context and specific requirements. Many
    queries involve imperative tasks, such as retrieving specific data for a user
    or calculating results for a given date range. However, SQL’s declarative nature
    shines when specifying what data you want rather than dictating how to retrieve
    it. This flexibility enables a wide range of users, including BI developers, business
    analysts, data engineers, and data scientists, to understand and interpret the
    code. Unlike some other strictly imperative data processing languages, SQL allows
    authors to focus on describing desired results. This self-documenting feature
    makes SQL code more readable and understandable, promoting effective collaboration
    in cross-functional teams.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 首要的是代码的可读性。这是数据工程的一个关键方面。SQL的语法虽然多才多艺，允许根据上下文和具体需求同时使用命令式和声明式。许多查询涉及命令式任务，比如为用户检索特定数据或计算给定日期范围内的结果。然而，SQL的声明性质在指定需要的数据而非如何检索时表现出色。这种灵活性使得包括BI开发人员、业务分析师、数据工程师和数据科学家在内的广泛用户能够理解和解释代码。与一些其他严格的命令式数据处理语言不同，SQL允许作者专注于描述所需的结果。这种自我记录特性使得SQL代码更易读、更易理解，促进跨功能团队的有效协作。
- en: Another exciting factor is that although SQL as an interface has survived over
    time, the reality is that the engines behind it have evolved dramatically over
    the last few years. Traditional SQL engines have improved, while distributed tools
    like Spark and Presto have enabled SQL to process massive datasets. In recent
    times, DuckDB has emerged as a game-changer, empowering SQL with extremely fast
    parallelized analytics queries on a single machine. With its functionality rivaling
    other high-performance alternatives, DuckDB opens new possibilities for data engineering
    tasks of all sizes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个令人兴奋的因素是，尽管SQL作为一个接口经过了多年的考验，但事实上它背后的引擎在过去几年里发生了巨大的进步。传统的SQL引擎得到了改进，而像Spark和Presto这样的分布式工具使得SQL能够处理海量数据集。近年来，DuckDB已经成为一个变革者，通过在单台机器上进行极快的并行分析查询，为SQL赋予了新的功能。其功能与其他高性能替代方案不相上下，为各种规模的数据工程任务开辟了新的可能性。
- en: However, it’s important to note that not all SQL-powered systems are the same.
    While SQL Server, for example, was commonly used for warehousing, it is designed
    for OLTP. On the other hand, platforms like Snowflake and Redshift are specialized
    OLAP data warehouses. They excel in handling large-scale analytical workloads
    and are optimized for complex queries and reporting. These distinctions highlight
    the versatility of SQL, which can be adapted to various database architectures
    and purposes. SQL remains a unifying language that bridges the gap between OLAP
    and OLTP systems, facilitating data access and analytics across database types
    and technologies.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要注意的是，并非所有基于 SQL 的系统都相同。例如，SQL Server 通常用于数据仓库，但设计用于 OLTP。另一方面，像 Snowflake
    和 Redshift 这样的平台则是专门的 OLAP 数据仓库。它们擅长处理大规模的分析工作负载，并针对复杂查询和报告进行了优化。这些区别突显了 SQL 的多用途性，可以适应各种数据库架构和目的。SQL
    仍然是一种统一的语言，弥合了 OLAP 和 OLTP 系统之间的差距，促进了跨数据库类型和技术的数据访问和分析。
- en: Data typing is another notable strength of SQL, particularly in data engineering.
    Seasoned data engineers understand the challenges of managing data types across
    various programming languages and SQL engines, a process that can be laborious
    and error-prone. However, SQL engines excel in enforcing strong data typing, guaranteeing
    consistent handling of data types throughout the data pipeline. Moreover, the
    SQL ecosystem offers valuable tools like Apache Arrow that tackle the compatibility
    issues arising from diverse tools and databases. Arrow facilitates robust and
    consistent data type handling across environments, including R, Python, and various
    databases. Selecting SQL engines compatible with Arrow can effectively mitigate
    many data typing challenges, simplifying maintenance efforts and reducing the
    burdens of dependency management, thus allowing data engineers to focus more on
    the core aspects of their data engineering work.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 数据类型是 SQL 的另一个显著优势，特别是在数据工程领域。经验丰富的数据工程师了解跨多种编程语言和 SQL 引擎管理数据类型的挑战，这是一个可能繁琐且容易出错的过程。然而，SQL
    引擎在强制数据类型方面表现出色，确保数据管道中始终一致地处理数据类型。此外，SQL 生态系统提供像 Apache Arrow 这样的宝贵工具，解决不同工具和数据库带来的兼容性问题。Arrow
    在各种环境中，包括 R、Python 和各种数据库中，促进了强大和一致的数据类型处理。选择与 Arrow 兼容的 SQL 引擎可以有效减轻许多数据类型挑战，简化维护工作，并减少依赖管理的负担，从而使数据工程师能够更专注于他们工作的核心方面。
- en: SQL’s compatibility with software engineering best practices is a significant
    advantage in the field of data engineering. Data engineers often deal with complex
    SQL scripts that are important components of their organization’s data pipelines.
    In the past, maintaining and modifying such scripts was a major challenge and
    often resulted in code that was difficult to understand and modify. However, the
    development of SQL tools has addressed these challenges and made it easier to
    adapt SQL code to good technical practices. One notable advance is the advent
    of DuckDB, a specialized SQL engine designed for analytical queries. DuckDB’s
    unique features, such as the absence of dependencies and optimization for analytical
    workloads, enable data engineers to perform unit tests and facilitate rapid iteration
    of SQL code. This ensures that SQL code conforms to established technical principles,
    increasing its reliability and maintainability.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 与软件工程最佳实践的兼容性是数据工程领域的一个重要优势。数据工程师经常处理其组织数据管道中重要组成部分的复杂 SQL 脚本。过去，维护和修改此类脚本是一项重大挑战，并且通常导致难以理解和修改的代码。然而，SQL
    工具的发展已经解决了这些挑战，并使得将 SQL 代码适应良好的技术实践变得更加容易。一个显著的进步是 DuckDB 的出现，这是一款专门用于分析查询的 SQL
    引擎。DuckDB 的独特功能，如无依赖性和针对分析工作负载的优化，使数据工程师能够执行单元测试，并促进 SQL 代码的快速迭代。这确保 SQL 代码符合已建立的技术原则，增强了其可靠性和可维护性。
- en: Another helpful tool in the SQL ecosystem is CTEs, which can be used to break
    large queries into smaller, more manageable, and testable pieces. By breaking
    complex queries into semantically meaningful components, data engineers can easily
    validate and verify each part independently, promoting a more modular and robust
    development process.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 生态系统中另一个有用的工具是 CTEs，可用于将大型查询分解为更小、更可管理和可测试的部分。通过将复杂查询分解为语义上有意义的组件，数据工程师可以轻松验证和验证每个部分，促进更模块化和强大的开发过程。
- en: Other improvements are also helping push SQL to the forefront of analytics engineering.
    Lambda functions allow data engineers to write arbitrary functions directly into
    SQL statements. This capability improves the flexibility and agility of SQL code
    and enables dynamic calculations and transformations during data processing.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 其他改进也帮助推动 SQL 成为分析工程的前沿。Lambda 函数允许数据工程师将任意函数直接编写到 SQL 语句中。这种能力提高了 SQL 代码的灵活性和敏捷性，并在数据处理过程中实现动态计算和转换。
- en: Window functions have also long been recognized as a valuable tool in SQL because
    they provide enhanced analytical capabilities by dividing data into manageable
    segments. With window functions, data engineers can perform complex aggregations,
    rankings, and statistical calculations over defined subsets of data, opening up
    new possibilities for analysis and reporting.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 长期以来，窗口函数也被认为是 SQL 中的一种有价值的工具，因为它们通过将数据分割成可管理的段，提供了增强的分析能力。通过窗口函数，数据工程师可以在定义的数据子集上执行复杂的聚合、排名和统计计算，为分析和报告开辟了新的可能性。
- en: Last but not least, modern SQL engines have incorporated features such as full-text
    search, geodata functions, and user-defined functions, further expanding SQL’s
    capabilities. These additions target specific use cases and domain-specific requirements
    and enable data engineers to perform specialized operations within the SQL environment.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现代 SQL 引擎已经整合了全文搜索、地理数据功能和用户定义函数等功能，进一步扩展了 SQL 的能力。这些新增功能针对特定用例和领域需求，并允许数据工程师在
    SQL 环境中执行专业操作。
- en: All these and many more have contributed to the resiliency of SQL over time
    and encouraged many people to invest in learning and applying it in their day-to-day
    analytics activities. Now let’s step back and revisit the core concepts of SQL.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些因素及更多其他因素随着时间的推移都有助于 SQL 的韧性，并鼓励许多人投资于学习并将其应用于日常分析活动。现在让我们回顾 SQL 的核心概念。
- en: Database Fundamentals
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据库基础知识
- en: A strong understanding of database fundamentals is crucial for analytics and
    data engineers. Databases serve as the backbone for storing, organizing, and retrieving
    vast amounts of data. Over time, the evolution of databases has paved the way
    for the emergence and refinement of SQL as a powerful and widely adopted language
    for working with relational databases. However, before we explore the specificities
    of databases, it’s essential to understand the broader context of data, information,
    and knowledge, as they all either live in or are derived from databases.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库基础知识对于分析师和数据工程师至关重要。数据库作为存储、组织和检索大量数据的支柱。随着时间的推移，数据库的发展为 SQL 的出现和完善铺平了道路，SQL
    成为处理关系型数据库的强大且广泛采用的语言。然而，在探讨数据库的具体特性之前，了解数据、信息和知识的更广泛背景至关重要，因为它们或者存在于数据库中，或者从数据库中获取。
- en: At the foundation of this context, we have the *DIKW pyramid*, shown in [Figure 3-1](#dikw_pyramid).
    This conceptual model describes the hierarchical relationships among data, information,
    knowledge, and wisdom. Through a series of iterative processes, the DIKW pyramid
    provides a framework for understanding how to transform raw data into actionable
    wisdom.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景的基础上，我们有 *DIKW 金字塔*，如 [图 3-1](#dikw_pyramid) 所示。这个概念模型描述了数据、信息、知识和智慧之间的层级关系。通过一系列迭代过程，DIKW
    金字塔提供了一个框架，用于理解如何将原始数据转化为可操作的智慧。
- en: '![images/new_ch02_dikw.png](assets/aesd_0301.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图像](assets/aesd_0301.png)'
- en: Figure 3-1\. DIKW pyramid
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-1\. DIKW 金字塔
- en: 'To better understand the DIKW pyramid, let’s decompose each layer:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解 DIKW 金字塔，让我们分解每一层：
- en: Data
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据
- en: 'Raw facts and figures that lack context and meaning. Data can be considered
    as the building blocks of information. Examples of data: 1989, teacher, green.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏上下文和含义的原始事实和数字。数据可以被视为信息的构建块。数据的例子：1989、教师、绿色。
- en: Information
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: 'Organized and structured representation of data that provides context and answers
    specific questions. Examples of information:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 提供上下文并回答特定问题的数据的有序和结构化表示。信息的例子：
- en: My math teacher was born in 1989.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我的数学老师出生于 1989 年。
- en: The traffic light at the intersection of Albany Ave and Avenue J is green.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Albany Ave 和 Avenue J 交汇处的交通灯](https://example.org/traffic_light)显示为绿色。'
- en: Knowledge
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 知识
- en: 'Emerges when we combine information with experience, expertise, and understanding.
    It represents the insights gained from analyzing and interpreting information,
    which enable individuals and organizations to make informed decisions and take
    appropriate actions. Examples of knowledge:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将信息与经验、专业知识和理解结合时，知识就产生了。它代表通过分析和解释信息获得的见解，使个人和组织能够做出明智的决策并采取适当的行动。知识的例子：
- en: Since my math teacher was born in 1989, he is an adult.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我的数学老师是1989年出生的，他已经成年了。
- en: The traffic light that I am driving toward is turning green.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我正在驾驶的交通灯正在变绿。
- en: Wisdom
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 智慧
- en: 'A level of deep understanding that exceeds knowledge. Wisdom occurs when individuals
    and organizations can apply their knowledge and make sound judgments, leading
    to positive effects and transformative insights. Examples of wisdom:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 超越知识的深度理解层次。智慧发生在个人和组织能够应用他们的知识并做出明智判断的时候，导致积极影响和转变性的见解。智慧的例子：
- en: It might be time for my math teacher to start thinking about a retirement savings
    plan.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 也许是时候让我的数学老师开始考虑退休储蓄计划了。
- en: With the traffic light turning green, I can move ahead.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当交通灯变绿时，我可以前进了。
- en: Databases play a vital role in the DIKW pyramid, serving as the basis for storing,
    managing, and organizing data. This enables the conversion of data into meaningful
    insights, which ultimately allows businesses to gain the necessary knowledge to
    make educated decisions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库在DIKW金字塔中发挥着至关重要的作用，作为存储、管理和组织数据的基础。这使得数据能够转化为有意义的见解，最终使企业获得必要的知识，以做出明智的决策。
- en: Types of Databases
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据库类型
- en: 'Databases are a core component of modern data management systems, delivering
    structured approaches to storing, organizing, and retrieving data. To better understand
    how a database achieves this, let’s first explore the two main categories of databases:
    relational and non-relational. By understanding the features and differences between
    these two types, you will be more capable of selecting a database solution for
    your specific data needs.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库是现代数据管理系统的核心组成部分，提供了存储、组织和检索数据的结构化方法。为了更好地理解数据库如何实现这一点，让我们首先探讨数据库的两个主要类别：关系型和非关系型。通过理解这两种类型之间的特点和区别，您将更能够选择适合您特定数据需求的数据库解决方案。
- en: '[Figure 3-2](#types_of_db) shows the two primary categories of databases, mapping
    in each the most common types of databases.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-2](#types_of_db)显示了数据库的两个主要类别，在每个类别中映射了最常见的数据库类型。'
- en: '![images/new_ch02_types_db.png](assets/aesd_0302.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片：数据库类型](assets/aesd_0302.png)'
- en: Figure 3-2\. Database categories and their types
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-2\. 数据库类别及其类型
- en: Relational databases
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 关系型数据库
- en: In this most common and widely adopted database category, the data is organized
    into tables of rows and columns. Key are used to enforce relationships between
    tables, and SQL is used for querying and manipulating data. Relational databases
    provide strong data integrity, transactional reliability, and support for ACID
    properties, ensuring that database transactions are reliable, maintain data integrity,
    and can recover from failures.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个最常见且广泛采用的数据库类别中，数据被组织成行和列的表格。键用于强制表之间的关系，SQL用于查询和操作数据。关系型数据库提供强大的数据完整性、事务可靠性和支持ACID属性，确保数据库事务可靠、保持数据完整性，并能从故障中恢复。
- en: Non-relational databases
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 非关系型数据库
- en: Also known as *NoSQL* (not only SQL) databases, non-relational databases have
    emerged as an alternative for managing large volumes of unstructured and semi-structured
    data with scalability and flexibility. Compared to relational databases, non-relational
    databases do not rely on fixed schemas. They can store data in various formats,
    such as key-value pairs, documents, wide-column stores, or graphs. Non-relational
    databases prioritize high performance, horizontal scalability, and schema flexibility.
    They are well suited for scenarios like real-time analytics, applications dealing
    with unstructured data, and IoT data, among others.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 也被称为*NoSQL*（不仅仅是SQL）数据库，非关系型数据库作为管理大量非结构化和半结构化数据的替代方案而出现，具有可扩展性和灵活性。与关系型数据库相比，非关系型数据库不依赖于固定的模式。它们可以以各种格式存储数据，例如键-值对、文档、宽列存储或图形。非关系型数据库优先考虑高性能、水平扩展性和模式灵活性。它们非常适合于实时分析、处理非结构化数据的应用以及物联网数据等场景。
- en: Note
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In the following sections, we will primarily focus on relational databases as
    a consequence of the overall goal of this chapter, namely, to present the fundamentals
    of SQL.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将主要关注关系数据库，这是本章的整体目标的结果，即介绍SQL的基础知识。
- en: 'We can imagine a database as a subset of a universe of data—built, designed,
    and fed with data that has a purpose specific to your organization. Databases
    are an essential component of society. Some activities, such as the ones listed,
    are widely distributed across society in general, with a database in the center
    to store the data:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以把数据库想象成数据宇宙的一个子集 —— 它们被建立、设计和输入数据，这些数据具有特定于您的组织的特定目的。数据库是社会的重要组成部分。一些活动，例如列出的活动，广泛分布在整个社会中，并且一个数据库位于中心用于存储数据：
- en: Book a hotel
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预订酒店
- en: Book an airplane ticket
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预订飞机票
- en: Buy a phone in a well-known marketplace
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个知名市场购买一部手机
- en: Enter your favorite social network
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入您喜欢的社交网络
- en: Go to the doctor
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去看医生
- en: But what does this look like in practice? Funneling into the relational databases,
    we organize the data into tables with rows and columns. Tables represent an entity
    of our universe, like a student in a university or a book in a library. A column
    describes an attribute of the entity. For example, a student has a name or address.
    A book has a title or an ISBN (International Standard Book Number). Finally, a
    row is the data itself. A student’s name can be Peter Sousa or Emma Rock. For
    the book title, a row can be “Analytics Engineering with SQL and dbt.” [Figure 3-3](#table_rows_columns)
    presents an example of a table with its respective columns and rows.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 但在实践中是什么样子呢？进入关系数据库，我们将数据组织成具有行和列的表。表代表我们宇宙中的一个实体，例如大学的学生或图书馆的书籍。列描述实体的属性。例如，学生有姓名或地址。一本书有标题或国际标准书号（ISBN）。最后，行是数据本身。学生的姓名可以是Peter
    Sousa或Emma Rock。关于书名，一行可以是“使用SQL和dbt进行分析工程”。[图 3-3](#table_rows_columns) 展示了一个带有其相应列和行的表的示例。
- en: '![images/new_ch02_table_example.png](assets/aesd_0303.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aesd_0303.png)'
- en: Figure 3-3\. Sample of a table with its rows and columns
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-3\. 表格及其行列示例
- en: Another topic to consider is how we establish relationships with the data and
    grant consistency. This is an essential factor to highlight in relational databases,
    where we can enforce connections between tables by using keys. Enforcing these
    relationships and connections in a relational database involves implementing mechanisms
    to maintain the integrity and consistency of the data across related tables. These
    mechanisms maintain the relationships among tables, preventing inconsistencies
    or data anomalies.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的主题是我们如何与数据建立关系并确保一致性。这是关系数据库中需要重点强调的一个重要因素，通过使用键可以在表之间强制建立连接。在关系数据库中执行这些关系和连接的操作涉及实施机制，以维护跨相关表的数据的完整性和一致性。这些机制维护表之间的关系，防止不一致或数据异常。
- en: One way to enforce relationships is by using primary and foreign keys. We will
    get there, but for now, [Figure 3-4](#tables_relationship) presents an interrelationship
    between tables. The use case is a university in which one or more students can
    enroll in one or more classes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用主键和外键来强制关系的一种方式。我们将介绍这一点，但现在，[图 3-4](#tables_relationship) 展示了表之间的相互关系。使用案例是一个大学，其中一个或多个学生可以注册一个或多个课程。
- en: 'Understanding these types of databases sets the stage for our next topic: database
    management systems (DBMSs). In the next section, we will dive deeper into the
    functionalities and importance of DBMSs, which serve as the software tools that
    enable efficient data storage, retrieval, and management in various types of databases.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这些类型的数据库为我们的下一个主题铺平了道路：数据库管理系统（DBMS）。在下一节中，我们将更深入地探讨DBMS的功能和重要性，它们作为软件工具，能够在各种类型的数据库中实现高效的数据存储、检索和管理。
- en: '![images/new_ch02_table_relationship.png](assets/aesd_0304.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aesd_0304.png)'
- en: Figure 3-4\. Tables interrelationship
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-4\. 表之间的相互关系
- en: Database Management System
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据库管理系统
- en: A *DBMS* is a software system that enables database creation, organization,
    management, and manipulation. It provides an interface and set of tools for users
    and applications to interact with databases, allowing for efficient data storage,
    retrieval, modification, and deletion.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*DBMS* 是一个软件系统，它使数据库的创建、组织、管理和操作成为可能。它为用户和应用程序提供了一个接口和一组工具，用于与数据库交互，从而实现高效的数据存储、检索、修改和删除。'
- en: A DBMS acts as an intermediary between users or applications and the underlying
    database. It abstracts the complexities of interacting with the database, providing
    a convenient and standardized way to work with data. It acts as a software layer
    that handles the storage, retrieval, and management of data while also ensuring
    data integrity, security, and concurrency control.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: DBMS 充当用户或应用程序与底层数据库之间的中介。它抽象了与数据库交互的复杂性，提供了一种便捷和标准化的工作数据的方式。它作为软件层处理数据的存储、检索和管理，同时确保数据的完整性、安全性和并发控制。
- en: 'The primary functions of a DBMS include the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: DBMS 的主要功能包括以下几点：
- en: Data definition
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 数据定义
- en: A DBMS allows users to define the structure and organization of the data by
    creating and modifying database schemas. It enables the definition of tables,
    columns, relationships, and constraints that govern the data stored in the database.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库管理系统 (DBMS) 允许用户通过创建和修改数据库模式来定义数据的结构和组织方式。它支持定义表、列、关系和约束，这些规定了数据库中存储的数据。
- en: Data manipulation
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 数据操作
- en: Users can perform operations on the data stored in the database by using a query
    language, typically SQL. A DBMS provides mechanisms to insert, retrieve, update,
    and delete data, allowing for efficient and controlled manipulation of the database
    content.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以通过查询语言（通常是 SQL）对存储在数据库中的数据执行操作。DBMS 提供了插入、检索、更新和删除数据的机制，允许高效和受控地操作数据库内容。
- en: Data security and integrity
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 数据安全性和完整性
- en: A DBMS provides mechanisms to ensure data security by enforcing access control
    policies. It enables the definition of user roles and permissions, restricting
    access to sensitive data. Additionally, a DBMS enforces data integrity by implementing
    constraints and validations to maintain the consistency and accuracy of the data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: DBMS 提供机制来确保数据安全性，通过执行访问控制策略来实施。它支持定义用户角色和权限，限制对敏感数据的访问。此外，DBMS 通过实施约束和验证来保持数据的一致性和准确性。
- en: Data concurrency and transaction management
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并发性和事务管理
- en: A DBMS handles the concurrent access to the database by multiple users or applications,
    ensuring that data remains consistent and protected from conflicts. It provides
    transaction management capabilities to ensure that groups of operations are executed
    reliably and consistently, following the ACID properties.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: DBMS 处理多用户或应用程序对数据库的并发访问，确保数据保持一致性并受到保护，不会发生冲突。它提供事务管理功能，确保一组操作能够可靠和一致地执行，符合
    ACID 特性。
- en: Data recovery and backup
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 数据恢复和备份
- en: A DBMS incorporates features to ensure data durability and recoverability. It
    provides data backup and restore mechanisms, allowing for data recovery in case
    of system failures or disasters.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: DBMS 包含功能以确保数据的持久性和可恢复性。它提供数据备份和恢复机制，允许在系统故障或灾难发生时进行数据恢复。
- en: Some of the most common DBMSs for both relational and non-relational databases
    can be found in [Table 3-1](#dbms_example).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一些适用于关系型和非关系型数据库的常见 DBMS 可见于[表 3-1](#dbms_example)。
- en: Table 3-1\. Common DBMSs
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3-1\. 常见的 DBMS
- en: '| Relational databases | Non-relational databases |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 关系型数据库 | 非关系型数据库 |'
- en: '| --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Microsoft Access | MongoDB |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Microsoft Access | MongoDB |'
- en: '| Microsoft SQL Server | Apache Cassandra |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Microsoft SQL Server | Apache Cassandra |'
- en: '| Postgres | Apache CouchDB |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Postgres | Apache CouchDB |'
- en: '| MySQL | Redis |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| MySQL | Redis |'
- en: '| SQLite | Elasticsearch |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| SQLite | Elasticsearch |'
- en: “Speaking” with a Database
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '"与数据库交流"'
- en: 'From an external point of view, interacting with a database through a DBMS
    provides four types of language:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从外部角度看，通过 DBMS 与数据库交互提供了四种类型的语言：
- en: Data definition language (DDL)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 数据定义语言 (DDL)
- en: To handle schemas, like table creation
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 处理模式，如表的创建
- en: Data manipulation language (DML)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 数据操作语言 (DML)
- en: To work with the data
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数据
- en: Data control language (DCL)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 数据控制语言 (DCL)
- en: To manage permissions to the database
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 管理对数据库的权限
- en: Transaction control language (TCL)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 事务控制语言 (TCL)
- en: To address the transactions that occur in the database
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 处理发生在数据库中的事务
- en: '[Figure 3-5](#sql_main_commands) shows the main languages used while interacting
    with a database and their primary commands.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-5](#sql_main_commands) 展示了与数据库交互时使用的主要语言及其主要命令。'
- en: '![images/new_ch03_sql_commands.png](assets/aesd_0305.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![images/new_ch03_sql_commands.png](assets/aesd_0305.png)'
- en: Figure 3-5\. Main SQL commands
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-5\. 主要 SQL 命令
- en: For this book, our primary focus will be providing solid foundations on SQL
    by learning how to query, manipulate, and define database structures, and so we
    will discuss DDL and DML. The activities related to administration tasks, like
    the ones performed with DCL and TCL, will not be covered.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本书，我们的主要重点将是通过学习如何查询、操作和定义数据库结构来提供SQL的坚实基础，因此我们将讨论DDL和DML。不涉及像DCL和TCL这样的管理任务相关的活动。
- en: Creating and Managing Your Data Structures with DDL
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DDL创建和管理数据结构
- en: DDL, a subset of SQL, is a standardized language used to create and modify the
    structure of objects in a database. It includes commands and syntax for defining
    tables, indexes, sequences, aliases, etc.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: DDL是SQL的子集，是一种标准化的语言，用于创建和修改数据库中对象的结构。它包括定义表、索引、序列、别名等的命令和语法。
- en: 'The most common DDL commands are the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的DDL命令如下：
- en: '`CREATE`'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`CREATE`'
- en: Creates new database objects such as tables, views, indexes, or constraints.
    It specifies the name of the object and its structure, including columns, data
    types, and any additional properties.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 创建新的数据库对象，如表、视图、索引或约束。它指定对象的名称及其结构，包括列、数据类型和任何附加属性。
- en: '`DROP`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`DROP`'
- en: Removes or deletes existing database objects. It permanently deletes the specified
    object and all associated data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 删除或删除现有数据库对象。它永久删除指定的对象及其所有相关数据。
- en: '`ALTER`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`ALTER`'
- en: Modifies the structure of an existing database object. You can use it to add,
    modify, or delete a table’s columns, constraints, or other properties. It provides
    flexibility to adapt the database schema to changing requirements.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 修改现有数据库对象的结构。您可以使用它来添加、修改或删除表的列、约束或其他属性。它提供了适应不断变化的需求调整数据库架构的灵活性。
- en: '`RENAME`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`RENAME`'
- en: Renames an existing database object, such as a table or a column. It provides
    a way to change the name of an object without altering its structure or data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 重命名现有的数据库对象，如表或列。它提供了一种在不改变对象结构或数据的情况下更改对象名称的方式。
- en: '`TRUNCATE`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`TRUNCATE`'
- en: Quickly removes all data from a table, while keeping the table structure. It
    is faster than using the `DELETE` command to remove all rows since it deallocates
    the data pages without logging individual row deletions.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 快速从表中删除所有数据，同时保留表结构。与使用`DELETE`命令逐行删除数据相比，它更快速，因为它会释放数据页而不记录单个行的删除操作。
- en: '`CONSTRAINT`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`CONSTRAINT`'
- en: Defines constraints on table columns, ensuring the integrity and validity of
    the data by specifying rules or conditions that the data must meet.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 定义表列的约束，通过指定数据必须满足的规则或条件，确保数据的完整性和有效性。
- en: '`INDEX`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`INDEX`'
- en: Creates an index on one or multiple columns of a table. Usually, an index improves
    the performance of data retrieval operations by creating a sorted structure that
    allows faster data searching and sorting.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在表的一个或多个列上创建索引。通常，索引通过创建排序结构来提高数据检索操作的性能，从而实现更快的数据搜索和排序。
- en: Before jumping into a hands-on use case, there are some topics we need to discuss
    in detail and one additional topic we need to introduce. The truth is that the
    majority of the DDL commands are, in a sense, self-explanatory, and as long as
    we see them in the code later, they will be easy to understand. Nonetheless, the
    discussion of the `CONSTRAINT` command should be slightly more detailed to introduce
    its particularities.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行实际应用之前，我们需要详细讨论一些主题和一个额外的主题引入。事实上，大多数DDL命令在某种意义上是不言自明的，只要稍后在代码中看到它们，它们将很容易理解。尽管如此，我们需要稍微详细地讨论`CONSTRAINT`命令，以介绍其特殊性。
- en: 'As mentioned earlier, constraints are rules or conditions the data must meet
    to grant their integrity. Typically, these constraints are applied to a column
    or a table. The most common constraints are as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，约束是数据必须满足的规则或条件，以确保其完整性。通常，这些约束适用于列或表。最常见的约束如下：
- en: Primary key
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 主键
- en: A primary-key constraint ensures that a column or a combination of columns uniquely
    identifies each row in a table, preventing duplicate and null values. It is essential
    for data integrity and often used as a reference for foreign-key constraints in
    related tables.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 主键约束确保表中的列或列组合唯一标识每一行，防止重复和空值。它对数据完整性至关重要，并且通常用作相关表中外键约束的参考。
- en: Foreign key
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 外键
- en: A foreign-key constraint specifies a relationship between two tables. It ensures
    that values in a column or a combination of columns in one table match the primary-key
    values in another table, helping maintain referential integrity and enforcing
    data consistency across related tables.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 外键约束指定两个表之间的关系。它确保一个表中列或列组合中的值与另一个表中主键值匹配，有助于维护引用完整性并强制执行跨相关表的数据一致性。
- en: Unique
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一
- en: A unique constraint ensures that the values in a column or a combination of
    columns are unique, and does not allow duplicates. Unlike a primary key, a unique
    constraint can allow null values, but only one null value is allowed if a column
    has a unique constraint.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一约束确保列或列组合中的值是唯一的，并且不允许重复。与主键不同，唯一约束可以允许空值，但如果列有唯一约束，则只允许一个空值。
- en: Check
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 检查
- en: A check constraint imposes a condition on the values allowed in a column. These
    constraints are typically used to enforce business rules, domain-specific requirements,
    or any other custom conditions on the data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 检查约束对列中允许的值施加条件。通常用于强制执行业务规则、特定领域的要求或数据上的任何其他自定义条件。
- en: Not null
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 非空
- en: A not-null constraint guarantees that a column does not contain null values,
    and so a specific column with this constraint must have a value for every row
    inserted or updated. This helps enforce data completeness and avoids unexpected
    null values.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 非空约束保证列不包含空值，因此具有此约束的特定列必须为每个插入或更新的行提供一个值。这有助于强制数据完整性并避免意外的空值。
- en: 'Finally, there is one last point to discuss: the data types that categorize
    the data that can be stored in a column or variable. These fields can vary from
    database engine to engine. In our case, we will keep it simple and use the MySQL
    data types as reference:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有一个要讨论的点：用于分类可以存储在列或变量中的数据类型。这些字段在不同的数据库引擎中可能有所不同。在我们的情况下，我们将简单地使用MySQL数据类型作为参考：
- en: Integer
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 整数
- en: 'A whole number without a fractional part. The most common are `INT`, `SMALLINT`,
    `BIGINT`, `TINYINT`. Examples of possible values: 1, 156, 2012412, 2.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 没有分数部分的整数。最常见的是`INT`、`SMALLINT`、`BIGINT`、`TINYINT`。可能的值示例：1、156、2012412、2。
- en: Decimal
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 十进制
- en: 'A number with a fractional part. Some of the most common are `DECIMAL`, `NUMERIC`,
    `FLOAT`, `DOUBLE`. Examples of possible values: 3.14, 94.5482, 5161.17620.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 带有小数部分的数字。一些最常见的是`DECIMAL`、`NUMERIC`、`FLOAT`、`DOUBLE`。可能的值示例：3.14、94.5482、5161.17620。
- en: Boolean
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 布尔
- en: A binary value. Traditionally written as `BOOLEAN`, `BOOL`, `BIT`, `TINYINT`.
    Used for storing true/false or 0/1 values.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制值。传统上写作`BOOLEAN`、`BOOL`、`BIT`、`TINYINT`。用于存储真/假或0/1值。
- en: Date
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 日期
- en: Mostly self-explanatory, but it can vary in format. Declared as `DATE`, and
    a standard format used is 2023-07-06.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数是不言而喻的，但格式可能有所不同。声明为`DATE`，常用的标准格式是2023-07-06。
- en: Time
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 时间
- en: You can decide the format of the time data type as well. Written as `TIME` in
    the database, one common format is 18:34:59.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以决定时间数据类型的格式。在数据库中写作`TIME`，一个常见的格式是18:34:59。
- en: Timestamp
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 时间戳
- en: 'The date and the time together. Usually, we use `TIMESTAMP` or `DATETIME`.
    Example: 2017-12-31 18:34:59.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 日期和时间结合在一起。通常我们使用`TIMESTAMP`或`DATETIME`。示例：2017-12-31 18:34:59。
- en: Text
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 文本
- en: 'The most general data type. But it can only be alphabetical letters or a mix
    of letters, numbers, or any other characters. Normally declared as `CHAR`, `VARCHAR`,
    `NVARCHAR`, `TEXT`. Note that choosing the correct text data type is relevant
    since each one has a maximum specified length. Examples of text: “hello world,”
    “porto1987,” “Hélder,” “13,487*5487+588”.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最通用的数据类型。但它只能是字母或字母、数字或任何其他字符的混合。通常声明为`CHAR`、`VARCHAR`、`NVARCHAR`、`TEXT`。请注意，选择正确的文本数据类型很重要，因为每种类型都有指定的最大长度。文本示例："hello
    world"、"porto1987"、"Hélder"、"13,487*5487+588"。
- en: Note
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注释
- en: We will use MySQL because of its broad adoption. You can download the MySQL
    Workbench through the [MySQL website](https://oreil.ly/Mzrdt).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用MySQL因其广泛的采用率。您可以通过[MySQL网站](https://oreil.ly/Mzrdt)下载MySQL Workbench。
- en: Now that you have a better idea of the DDL commands and the most common database
    data types, let’s create a database for managing O’Reilly books. This aligns with
    the example in [Chapter 2](ch02.html#chapter_id_02), when we introduced a database
    for O’Reilly to track books, but now let’s start with the physical model creation.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您对DDL命令和最常见的数据库数据类型有了更好的理解，让我们为管理O'Reilly图书创建一个数据库。这与[第2章](ch02.html#chapter_id_02)中的示例相符，当时我们介绍了一个用于追踪图书的O'Reilly数据库，但现在让我们开始创建物理模型。
- en: As a note, for data engineers, proficiency in all types of SQL commands is crucial
    as they are responsible for both database design (DDL) and data manipulation (DML).
    Analysts primarily focus on DML SQL commands, often limited to `SELECT` queries
    for data analysis. On the other hand, analytics engineers typically work with
    a combination of DML and some DDL SQL commands, although they often abstract DDL
    operations through tools like dbt.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种说明，对于数据工程师来说，精通所有类型的 SQL 命令至关重要，因为他们负责数据库设计（DDL）和数据操作（DML）。分析师主要专注于 DML
    SQL 命令，通常仅限于用于数据分析的 `SELECT` 查询。另一方面，分析工程师通常使用 DML 和一些 DDL SQL 命令的组合，尽管他们经常通过工具如
    dbt 来抽象 DDL 操作。
- en: First, let’s create the database itself. In your MySQL client, execute the command
    in [Example 3-1](#code_sql_create_db).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建数据库本身。在您的 MySQL 客户端中，执行 [示例 3-1](#code_sql_create_db) 中的命令。
- en: Example 3-1\. Create the database
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-1\. 创建数据库
- en: '[PRE0]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now, with the database created, execute the code in [Example 3-2](#code_sql_create_tables).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，已经创建了数据库，执行 [示例 3-2](#code_sql_create_tables) 中的代码。
- en: Example 3-2\. Create the database, part 2
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-2\. 创建数据库，第二部分
- en: '[PRE1]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In summary, these two examples have created a database called `OReillyBooks`
    and defined four tables: `authors`, `books`, `category`, and `book_category` (which
    represents the many-to-many relationship between books and categories). Each table
    has its own set of columns and constraints, such as primary keys and foreign keys.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，这两个示例创建了一个名为 `OReillyBooks` 的数据库，并定义了四个表：`authors`、`books`、`category` 和 `book_category`（表示书籍与类别之间的多对多关系）。每个表都有自己的列集和约束条件，例如主键和外键。
- en: Finally, and to also test other DDL commands, imagine that a new requirement
    arises, and now we also need to store the `publication_year`, which refers to
    the year a particular book was published. The syntax to do so is shown in [Example 3-3](#code_dml_sql00_alter_table).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了测试其他 DDL 命令，假设现在有一个新的需求，我们还需要存储 `publication_year`，即特定书籍出版的年份。执行 [示例 3-3](#code_dml_sql00_alter_table)
    中显示的语法即可实现此目的。
- en: Example 3-3\. `ALTER TABLE` syntax
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-3\. `ALTER TABLE` 语法
- en: '[PRE2]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As per the syntax shown in [Example 3-3](#code_dml_sql00_alter_table), the modification
    that fits our needs is adding a new column. Let’s now add the `publication_year`
    by executing the code snippet in [Example 3-4](#code_dml_sql00_alter_table_1).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 [示例 3-3](#code_dml_sql00_alter_table) 中显示的语法，适合我们需求的修改是添加一个新列。现在通过执行 [示例 3-4](#code_dml_sql00_alter_table_1)
    中的代码片段来添加 `publication_year`。
- en: Example 3-4\. Add the publication year
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-4\. 添加出版年份
- en: '[PRE3]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Manipulating Data with DML
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DML 操作数据
- en: DML serves as an essential component in database management. This language enables
    data selection, insertion, deletion, and updating within a database system. Its
    primary purpose is to retrieve and manipulate data residing in a relational database
    while encompassing several key commands.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: DML 在数据库管理中是一个重要组成部分。该语言使得在数据库系统中进行数据选择、插入、删除和更新成为可能。其主要目的是检索和操作驻留在关系数据库中的数据，同时涵盖了几个关键命令。
- en: Inserting Data with INSERT
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 INSERT 插入数据
- en: The `INSERT` command facilitates the addition of new data into a table. With
    this command, users can seamlessly insert one or multiple records into a specific
    table within the database. By utilizing `INSERT`, it becomes possible to expand
    the content of a table by including additional entries. This command is instrumental
    in adding records to an initially empty table but also allows for continuously
    augmenting existing data within the database. [Example 3-5](#code_dml_sql02) shows
    the standard syntax of this command.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`INSERT` 命令便于向表中添加新数据。通过此命令，用户可以轻松地将一个或多个记录插入到数据库中的特定表中。利用 `INSERT`，可以通过包含额外条目来扩展表的内容。该命令在向最初为空的表中添加记录时非常有用，同时也允许不断扩展数据库中现有的数据。[示例 3-5](#code_dml_sql02)
    展示了该命令的标准语法。'
- en: Example 3-5\. Syntax of an `INSERT` statement
  id: totrans-167
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-5\. `INSERT` 语句的语法
- en: '[PRE4]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `INSERT INTO` statement specifies the table where the data will be inserted,
    where *`table_name`* represents the name of the table itself. The component `(__column1__,
    __column2__, ...)` is optional and allows for the specification of the columns
    into which the data will be inserted. If the columns are omitted, it is assumed
    that values will be provided for all columns in the table. The `VALUES` keyword
    indicates the start of the list of values to be inserted into the specified columns.
    Within the `VALUES` clause, `(__value1__, __value2__, ...)`, we have the actual
    values to be inserted into the respective columns. It is crucial to ensure that
    the number of values provided matches the number of columns specified. This is
    the only way to ensure the values are correctly mapped to the corresponding columns
    during the insertion process. Most database engines raise an error if this is
    not respected.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`INSERT INTO` 语句指定将插入数据的表，其中*`table_name`*代表表本身的名称。组件`(__column1__, __column2__,
    ...)`是可选的，允许指定要插入数据的列。如果省略列，则假定将为表中的所有列提供值。`VALUES`关键字表示要插入到指定列中的值列表的开始。在`VALUES`子句中，`(__value1__,
    __value2__, ...)`包含要插入到相应列中的实际值。确保提供的值数量与指定的列数相匹配至关重要，这是确保在插入过程中正确映射值到相应列的唯一方式。大多数数据库引擎如果不遵守此规则将引发错误。'
- en: Let’s now extend our use case, which we started with [“Manipulating Data with
    DML”](#ddl_chapter), and insert data into the previously created tables. For that,
    execute the command in [Example 3-6](#code_sql_insert_data).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们扩展我们的用例，我们从[“使用 DML 操作数据”](#ddl_chapter)开始，并将数据插入到先前创建的表中。为此，请执行 [示例 3-6](#code_sql_insert_data)
    中的命令。
- en: Example 3-6\. Create dummy data
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-6\. 创建虚拟数据
- en: '[PRE5]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This code creates several `INSERT` statements, each targeting a specific table.
    We start by inserting data into the `authors` table. Each row represents an author,
    with `author_id` and `author_name` columns indicating the author’s unique identifier
    and name, respectively.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码创建了几个 `INSERT` 语句，每个都针对特定的表。我们首先向 `authors` 表插入数据。每一行代表一个作者，其中 `author_id`
    和 `author_name` 列分别表示作者的唯一标识符和名称。
- en: Then, we inserted data into the `books` table. Each row represents a book, with
    `book_id`, `book_title`, and `author_id` columns indicating the unique identifier,
    title, and author identifier of the book, respectively. The `author_id` column
    is linked to the `author_id` column in the `authors` table to establish the relationship
    between books and authors. Note that we cannot insert a book referencing a nonexistent
    author because of referential integrity.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们向 `books` 表插入数据。每一行代表一本书，其中 `book_id`、`book_title` 和 `author_id` 列分别表示书籍的唯一标识符、标题和作者标识符。`author_id`
    列与 `authors` 表中的 `author_id` 列关联，以建立书籍与作者之间的关系。请注意，由于引用完整性，我们无法插入引用不存在作者的书籍。
- en: We also created a `category` table to correctly classify the book based on its
    content type. Each row represents a category, with `category_id` and `category_name`
    columns indicating the unique identifier and name of the category, respectively.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还创建了一个 `category` 表，根据其内容类型正确对书籍进行分类。每一行代表一个类别，其中 `category_id` 和 `category_name`
    列分别表示类别的唯一标识符和名称。
- en: Finally, our intermediate table, `book_category`, stores the relationship between
    books and their corresponding categories. Each row represents one occurrence of
    this relationship, with `book_id` and `category_id` columns indicating the book
    and category identifiers, respectively. These columns establish the many-to-many
    relationship between books and categories.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的中间表 `book_category` 存储了书籍和它们对应类别之间的关系。每一行表示此关系的一个发生，其中 `book_id` 和 `category_id`
    列分别表示书籍和类别的标识符。这些列建立了书籍和类别之间的多对多关系。
- en: Let’s have a look at the data we’ve inserted. Execute the code in [Example 3-7](#code_dml_sql00_select),
    line by line. We will cover the `SELECT` statement in detail in the next section,
    but for now, it is enough to check the data in each table.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看我们插入的数据。逐行执行 [示例 3-7](#code_dml_sql00_select) 中的代码。我们将在下一节详细介绍 `SELECT`
    语句，但目前仅检查每个表中的数据就足够了。
- en: Example 3-7\. A `SELECT` statement querying the `authors`, `book_category`,
    `books`, and `category` tables
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-7\. 使用 `SELECT` 语句查询 `authors`、`book_category`、`books` 和 `category` 表
- en: '[PRE6]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Selecting Data with SELECT
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用 SELECT 选择数据
- en: '`SELECT` is one of the most fundamental DML commands in SQL. This command allows
    you to extract specific data from a database. When this statement is executed,
    it retrieves the desired information and organizes it into a structured result
    table, commonly referred to as the *result set*. This result set contains data
    that meets the specified criteria, allowing users to access and analyze the selected
    information easily. In [Example 3-8](#code_dml_sql01), we can analyze the (simplest)
    syntax of this command.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`SELECT`是SQL中最基本的DML命令之一。此命令允许您从数据库中提取特定的数据。执行此语句时，它检索所需的信息并将其组织成结构化的结果表，通常称为*结果集*。这个结果集包含满足指定条件的数据，使用户能够轻松访问和分析所选信息。在[示例 3-8](#code_dml_sql01)中，我们可以分析此命令的（最简单的）语法。'
- en: Note
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you are already proficient with SQL and `SELECT` commands and seeking more
    advanced SQL statements, we recommend referring to [“Window Functions”](#sch_window_functions).
    If you already want to jump into the dbt world, you can find it in [Chapter 4](ch04.html#chapter_id_04).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经熟练掌握SQL和`SELECT`命令，并希望了解更高级的SQL语句，请参考[“窗口函数”](#sch_window_functions)。如果您已经想要进入dbt世界，您可以在[第四章](ch04.html#chapter_id_04)找到相关内容。
- en: Example 3-8\. Syntax of a `SELECT` statement
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-8\. `SELECT`语句的语法
- en: '[PRE7]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `SELECT` part of this structure indicates the specific columns or expressions
    that are retrieved from the table. The `FROM` component specifies the table from
    which the data will be retrieved. We have much more to elaborate on about this
    command, from data filtering and respective operators to data grouping or joins.
    In the next sections, we will discuss each property.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 此结构的`SELECT`部分指示从表中检索的具体列或表达式。`FROM`部分指定从中检索数据的表。我们还有更多内容可以详细说明这个命令，从数据过滤和相应的运算符到数据分组或连接。在接下来的部分，我们将讨论每个属性。
- en: Filtering data with WHERE
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用`WHERE`过滤数据
- en: The optional `WHERE` clause allows users to define conditions the retrieved
    data must meet, effectively filtering rows based on specified criteria. It is
    a fundamental part of SQL queries, allowing you to filter and retrieve specific
    subsets of data from tables. [Example 3-9](#code_dml_sql002) shows the syntax
    of the `WHERE` statement.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 可选的`WHERE`子句允许用户定义检索数据必须满足的条件，有效地根据指定的条件过滤行。这是SQL查询的基本部分，允许您根据指定的条件过滤和检索表中的特定数据子集。[示例 3-9](#code_dml_sql002)展示了`WHERE`语句的语法。
- en: Example 3-9\. Syntax of a `SELECT` statement with a `WHERE` clause
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-9\. 带有`WHERE`子句的`SELECT`语句语法
- en: '[PRE8]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To correctly understand how to write *conditions* in SQL and adequately filter
    the data, we must first become familiarized with the SQL operators with single
    or multiple conditions.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 要正确理解如何在SQL中编写*条件*并充分过滤数据，我们必须首先熟悉SQL运算符，这些运算符支持单个或多个条件。
- en: SQL operators
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SQL运算符
- en: SQL operators are frequently used in the `WHERE` clause to establish conditions
    for filtering data. These operators allow you to compare values and expressions
    in SQL that meet the defined conditions. [Table 3-2](#sql_operators) summarizes
    the most common operators.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: SQL运算符经常在`WHERE`子句中使用，用于建立过滤数据的条件。这些运算符允许您在SQL中比较符合定义条件的值和表达式。[表 3-2](#sql_operators)总结了最常见的运算符。
- en: Table 3-2\. SQL operators
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3-2\. SQL运算符
- en: '| Operator | Operator type | Meaning |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 运算符 | 运算符类型 | 意义 |'
- en: '| --- | --- | --- |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `=` | Comparison | Equal to |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| `=` | 比较 | 等于 |'
- en: '| `<>` or `!=` | Comparison | Not equal to |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| `<>` 或 `!=` | 比较 | 不等于 |'
- en: '| `<` | Comparison | Less than |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| `<` | 比较 | 小于 |'
- en: '| `>` | Comparison | Greater than |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| `>` | 比较 | 大于 |'
- en: '| `< =` | Comparison | Less than or equal to |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| `<=` | 比较 | 小于或等于 |'
- en: '| `> =` | Comparison | Greater than or equal to |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| `>=` | 比较 | 大于或等于 |'
- en: '| `LIKE ''%expression%`'' | Comparison | Contains `"expression"` |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| `LIKE ''%expression%''` | 比较 | 包含`"expression"` |'
- en: '| `IN ("exp1", "exp2")` | Comparison | Contains any of `"exp1"` or `"exp2"`
    |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| `IN ("exp1", "exp2")` | 比较 | 包含任何一个`"exp1"`或`"exp2"` |'
- en: '| `BETWEEN` | Logical | Selects values within a given range |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| `BETWEEN` | 逻辑 | 选择在给定范围内的值 |'
- en: '| `AND` | Logical | Combines two or more conditions and returns `true` only
    if all the conditions are true |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| `AND` | 逻辑 | 将两个或多个条件组合在一起，只有当所有条件都为真时才返回`true` |'
- en: '| `OR` | Logical | Combines two or more conditions and returns `true` if at
    least one of the conditions is true |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| `OR` | 逻辑 | 将两个或多个条件组合在一起，如果至少一个条件为真，则返回`true` |'
- en: '| `NOT` | Logical | Negates a condition, returning `true` if the condition
    is false, and vice versa |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| `NOT` | 逻辑 | 对条件进行否定，如果条件为假则返回`true`，反之亦然 |'
- en: '| `UNION` | Set | Combines the result of two `SELECT` statements, removing
    the duplicated rows |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| `UNION` | 集合 | 将两个 `SELECT` 语句的结果合并，去除重复的行。 |'
- en: '| `UNION ALL` | Set | Combines all records from two `SELECT` statements, yet
    with `UNION ALL`, duplicated rows aren’t eliminated |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| `UNION ALL` | 集合 | 将两个 `SELECT` 语句的所有记录合并，但使用 `UNION ALL` 时，重复的行不会被消除。 |'
- en: To better understand their applications, let’s explore examples for comparison
    operators and logical operators. To simplify things, since we didn’t dig into
    other elements of SQL, such as joins, let’s use the `books` table as the source
    of our use cases.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解它们的应用，让我们探讨比较运算符和逻辑运算符的示例。为了简化事情，由于我们没有深入研究 SQL 的其他元素，比如连接，让我们使用 `books`
    表作为我们用例的数据源。
- en: As an initial example of conditional and logical operators, let’s try to find
    books published earlier than 2015\. Then, let’s find only the books published
    in 2017 and, last, the books with “Python” in the title. [Example 3-10](#code_dml_sql01_select_with_condition)
    has the three code snippets to help you solve this challenge.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 作为条件和逻辑运算符的初始示例，让我们尝试找到早于 2015 年出版的书籍。然后，让我们仅找到 2017 年出版的书籍，最后是标题中包含“Python”的书籍。[示例
    3-10](#code_dml_sql01_select_with_condition) 包含了三个代码片段，帮助你解决这个挑战。
- en: Example 3-10\. Select data with conditional operators
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例 3-10\. 使用条件运算符选择数据
- en: '[PRE9]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[Example 3-10](#code_dml_sql01_select_with_condition) shows three examples
    of working with conditional operators. Feel free to play with the code and test
    the others introduced previously.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 3-10](#code_dml_sql01_select_with_condition) 展示了使用条件运算符处理的三个示例。随意尝试代码并测试之前介绍的其他内容。'
- en: Finally, to be familiarized with a logical operator, let’s search for books
    published in 2012 or after 2015\. [Example 3-11](#code_dml_sql01_select_with_condition_1)
    will help with this.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了熟悉逻辑运算符，让我们搜索 2012 年或之后出版的书籍。[示例 3-11](#code_dml_sql01_select_with_condition_1)
    将帮助你完成这个任务。
- en: Example 3-11\. Select data with a logical operator
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例 3-11\. 使用逻辑运算符选择数据
- en: '[PRE10]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: It is also essential to note that these operators are not exclusive to the `WHERE`
    clause. They can also be used along with other filtering techniques, such as the
    `HAVING` clause, that we will introduce in the next section.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 还需注意这些运算符并不仅限于 `WHERE` 子句。它们也可以与其他过滤技术一起使用，例如我们将在下一节介绍的 `HAVING` 子句。
- en: Aggregating data with GROUP BY
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 `GROUP BY` 聚合数据
- en: The `GROUP BY` clause is an optional feature in SQL utilized to group the result
    set by one or more columns. Often used with aggregate functions, `GROUP BY` calculates
    subsets of rows that share a common value in the specified column or columns.
    In other words, when using the `GROUP BY` clause, the result set is divided into
    groups, where each group represents a unique combination of values from the given
    aggregated column or columns. As stated, `GROUP BY` is typically used with aggregate
    functions for these groups, providing valuable insights into the data. Some of
    the most common aggregate functions are shown in [Table 3-3](#agg_functions).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`GROUP BY` 子句是 SQL 中的一个可选特性，用于将结果集按一个或多个列分组。通常与聚合函数一起使用，`GROUP BY` 计算具有相同值的指定列或列的子集的行集合。换句话说，使用
    `GROUP BY` 子句时，结果集被分成组，每个组代表给定聚合列或列的唯一组合的值。正如所述，`GROUP BY` 通常与聚合函数一起用于这些组，为数据提供有价值的见解。一些最常见的聚合函数显示在
    [表 3-3](#agg_functions) 中。'
- en: Table 3-3\. Aggregate functions
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3-3\. 聚合函数
- en: '| Aggregate function | Meaning |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 聚合函数 | 意义 |'
- en: '| --- | --- |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `COUNT()` | Calculates the number of rows or non-null values in a column.
    |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| `COUNT()` | 计算列中行数或非空值的数量。 '
- en: '| `SUM()` | Calculates the sum of numeric values in a column. |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| `SUM()` | 计算列中数值的总和。 '
- en: '| `AVG()` | Calculates the average (mean) value of a column. |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| `AVG()` | 计算列的平均值。 '
- en: '| `MAX()` | Retrieves the maximum value from a column. |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| `MAX()` | 从列中检索最大值。 '
- en: '| `MIN()` | Retrieves the minimum value from a column. |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| `MIN()` | 从列中检索最小值。 '
- en: '| `DISTINCT` | Although not an aggregate function in the rigorous sense, the
    `DISTINCT` keyword is often used with an aggregate function, inside the `SELECT`
    statement, to calculate distinct values. |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| `DISTINCT` | 虽然严格意义上不是聚合函数，`DISTINCT` 关键字通常与聚合函数一起在 `SELECT` 语句中使用，用于计算唯一的值。
    |'
- en: '`GROUP BY` is typically used in trend analysis and summary reports, like monthly
    sales reports and quarterly user accesses, among others. The generic syntax of
    the `GROUP BY` clause is presented in [Example 3-12](#code_dml_sql01_group_by).'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`GROUP BY`通常用于趋势分析和汇总报告，如月销售报告和季度用户访问报告等。`GROUP BY`子句的通用语法在[示例 3-12](#code_dml_sql01_group_by)中呈现。'
- en: Example 3-12\. Syntax of a `SELECT` statement with a `GROUP BY` clause
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-12\. 带有`GROUP BY`子句的`SELECT`语句的语法。
- en: '[PRE11]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Let’s now apply those functions in a straightforward use case. Using the table
    `book_category`, let’s analyze the average number of books per category. To help
    you with this challenge, let’s look at [Example 3-13](#code_dml_sql01_group_by_test).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在一个简单的用例中应用这些函数。使用表`book_category`，让我们分析每个类别平均书籍数量。为了帮助你解决这个问题，让我们看一下[示例 3-13](#code_dml_sql01_group_by_test)。
- en: Example 3-13\. Select and aggregate data
  id: totrans-235
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-13\. 选择并聚合数据。
- en: '[PRE12]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We’re using the `COUNT()` aggregate function, but others could be used, depending
    on the desired use case. Finally, this is a simple example since we see only `category_id`,
    but it could be better if we had the category name instead; however, this field
    is visible only in the `category` table. To include it, we need to know how to
    use joins. We will discuss this further in [“Joining data with INNER, LEFT, RIGHT,
    FULL, and CROSS JOIN”](#joins_chapter).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用`COUNT()`聚合函数，但也可以根据所需的用例使用其他函数。最后，这是一个简单的示例，因为我们只看到`category_id`，但如果我们有类别名称会更好；然而，这个字段仅在`category`表中可见。要包含它，我们需要知道如何使用连接。我们将在[“使用INNER、LEFT、RIGHT、FULL和CROSS
    JOIN连接数据”](#joins_chapter)中进一步讨论这个问题。
- en: At last, we come to the `HAVING` filter. An optional clause closely related
    to `GROUP BY`, the `HAVING` filter applies conditions to the grouped data. Compared
    with the `WHERE` clause, `HAVING` filters the rows after the aggregation, while
    in the `WHERE` clause, the filtering occurs before the grouping operation. Yet
    the same operators, such as “equal” and “greater than,” among others, are applied
    as in the `WHERE` statement.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来看看`HAVING`过滤器。作为与`GROUP BY`紧密相关的可选子句，`HAVING`过滤器应用于分组数据。与`WHERE`子句相比，`HAVING`在聚合之后过滤行，而`WHERE`在分组操作之前过滤行。然而，相同的运算符，如“等于”和“大于”，等等，与`WHERE`语句中的应用相同。
- en: The SQL syntax for a `HAVING` filter is displayed in [Example 3-14](#code_dml_sql01_group_by_having).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`HAVING`过滤器的SQL语法显示在[示例 3-14](#code_dml_sql01_group_by_having)中。'
- en: Example 3-14\. Syntax of a `SELECT` statement with a `GROUP BY` clause and `HAVING`
    filter
  id: totrans-240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-14\. 带有`GROUP BY`子句和`HAVING`过滤器的`SELECT`语句的语法。
- en: '[PRE13]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let’s see the `HAVING` filter in action. Referring to [Example 3-13](#code_dml_sql01_group_by_test),
    we now want only the categories with at least two books published. [Example 3-15](#code_dml_sql01_group_by_having_test)
    helps you with that.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看`HAVING`过滤器的实际应用。参考[示例 3-13](#code_dml_sql01_group_by_test)，我们现在只想要至少出版两本书的类别。[示例 3-15](#code_dml_sql01_group_by_having_test)将帮助你完成这一点。
- en: Example 3-15\. Select and aggregate data, applying the `HAVING` filter
  id: totrans-243
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-15\. 选择并聚合数据，应用`HAVING`过滤器。
- en: '[PRE14]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: By leveraging the `GROUP BY` clause and the `HAVING` filter, you can effectively
    organize and summarize data to perform calculations on aggregated datasets, enabling
    you to uncover patterns, trends, and relationships within the data; facilitate
    data analysis; and support the decision-making processes.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用`GROUP BY`子句和`HAVING`过滤器，您可以有效地组织和总结数据，对聚合数据集进行计算，从而发现数据中的模式、趋势和关系；促进数据分析；支持决策过程。
- en: Sorting data with ORDER BY
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用`ORDER BY`排序数据
- en: The `ORDER BY` clause is a sorting statement within SQL generally used to organize
    the query results in a specific sequence, making it simpler to analyze and interpret
    the data. It sorts the result set of a query based on one or more columns, allowing
    you to specify the sorting order for each column, namely, by ascending (the default)
    and descending order.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`ORDER BY`子句是SQL中的排序语句，通常用于按特定顺序组织查询结果，使数据分析和解释更简单。它根据一个或多个列对查询结果集进行排序，允许您为每列指定排序顺序，即按升序（默认）和降序排列。'
- en: The basic syntax of the `ORDER BY` clause is shown in [Example 3-16](#code_dml_sql01_order_by).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`ORDER BY`子句的基本语法显示在[示例 3-16](#code_dml_sql01_order_by)中。'
- en: Example 3-16\. Syntax of a `SELECT` statement with an `ORDER BY` clause
  id: totrans-249
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-16\. 带有`ORDER BY`子句的`SELECT`语句的语法。
- en: '[PRE15]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the previous use cases, one that stands out is the books published yearly.
    Looking at the table, it is difficult to determine which are the newest and the
    oldest books. The `ORDER BY` clause substantially simplifies this analysis. To
    test this clause, execute the code snippet in [Example 3-17](#code_dml_sql01_order_by_test)
    and check the results with and without `ORDER BY`. As a note, if the order `ASC/DESC`
    is not explicitly declared, SQL will use `ASC` by default.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的用例中，一个显著的例子是按年度出版的书籍。查看表后，很难确定哪些是最新的和最旧的书籍。`ORDER BY`子句大大简化了这种分析。为了测试这个子句，请执行[示例
    3-17](#code_dml_sql01_order_by_test)中的代码片段，并检查有无`ORDER BY`的结果。需要注意的是，如果未明确声明`ASC/DESC`顺序，SQL将默认使用`ASC`。
- en: Example 3-17\. `SELECT` statement with an `ORDER BY` clause
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-17\. 带有`ORDER BY`子句的`SELECT`语句
- en: '[PRE16]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In conclusion, the `ORDER BY` clause allows you to arrange your result set in
    a desired sequence that best suits your data exploration and analysis, simplifying
    the capture of meaningful data.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，`ORDER BY`子句允许您根据最适合数据探索和分析的顺序排列结果集，简化获取有意义数据的过程。
- en: Joining data with INNER, LEFT, RIGHT, FULL, and CROSS JOIN
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用INNER、LEFT、RIGHT、FULL和CROSS JOIN连接数据
- en: '*Joins* are a mechanism in SQL that combine data from multiple tables. Understanding
    and working with joins can largely improve your capacity to pull valuable insights
    and make more informed decisions from complex datasets. This section will guide
    you through the types of joins available in SQL, their syntax, and their usage.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '*连接*是SQL中将多个表的数据合并的机制。理解和使用连接可以极大地提高您从复杂数据集中提取有价值见解并做出更加明智决策的能力。本节将指导您了解SQL中可用的连接类型、它们的语法和用法。'
- en: SQL has several types of joins. Each allows you to combine data from multiple
    tables based on specified conditions. Before seeing the joins in action, let’s
    increment our dataset with a new author. This author will not have any books.
    To do this, execute the statement in [Example 3-18](#code_dml_sql01_insert_john).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: SQL有几种连接类型。每种类型允许您基于指定的条件从多个表中合并数据。在看到连接实际操作之前，让我们通过增加一个新的作者来增强我们的数据集。这位作者没有任何书籍。要执行此操作，请执行[示例
    3-18](#code_dml_sql01_insert_john)中的语句。
- en: Example 3-18\. Insert an author without books
  id: totrans-258
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-18\. 插入一个没有书籍的作者
- en: '[PRE17]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The reason we created an author without any books is to explore the several
    joins we will introduce. Following are the most common types of SQL joins.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建没有任何书籍的作者的原因是为了探索我们将介绍的几种连接。以下是SQL连接中最常见的几种类型。
- en: INNER JOIN
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内连接
- en: The `INNER JOIN` returns only the matching rows from both tables based on the
    specified join condition. If we think of a Venn diagram with circle A and circle
    B representing each dataset, in an `INNER JOIN` we would see only the overlapping
    area that contains the matching values of both tables. Let’s look at [Figure 3-6](#inner_join)
    to better visualize the Venn diagram.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`INNER JOIN`根据指定的连接条件仅返回两个表中匹配的行。如果我们想象一个Venn图，其中圆A和圆B分别代表每个数据集，在`INNER JOIN`中，我们只会看到包含两个表的匹配值的重叠区域。让我们查看[图
    3-6](#inner_join)以更好地可视化这个Venn图。'
- en: '![images/new_ch02_inner_join.png](assets/aesd_0306.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![images/new_ch02_inner_join.png](assets/aesd_0306.png)'
- en: Figure 3-6\. Venn diagram illustrating an `INNER JOIN`
  id: totrans-264
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-6\. 展示`INNER JOIN`的Venn图
- en: The code syntax for the `INNER JOIN` is shown in [Example 3-19](#code_dml_sql01_inner_join).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`INNER JOIN`的代码语法显示在[示例 3-19](#code_dml_sql01_inner_join)中。'
- en: Example 3-19\. Syntax of an `INNER JOIN`
  id: totrans-266
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-19\. `INNER JOIN`的语法
- en: '[PRE18]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: To see the `INNER JOIN` in action, let’s gather only authors with books. [Example 3-20](#code_dml_sql01_inner_join_test)
    shows the required code.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看`INNER JOIN`的实际操作，让我们仅获取有书籍的作者。[示例 3-20](#code_dml_sql01_inner_join_test)展示了所需的代码。
- en: Example 3-20\. Gather only authors with books
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-20\. 仅获取有书籍的作者
- en: '[PRE19]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[Figure 3-7](#inner_join_results) shows the query output.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-7](#inner_join_results)展示了查询结果。'
- en: '![new_ch03_inner_join.png](assets/aesd_0307.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![new_ch03_inner_join.png](assets/aesd_0307.png)'
- en: Figure 3-7\. `INNER JOIN` query output showing only the authors who have books
  id: totrans-273
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-7\. 展示仅包含有书籍的作者的`INNER JOIN`查询结果
- en: By analyzing the results, we can quickly identify the missing author John Doe.
    As you may remember, we’ve created him without any books, so when using an `INNER
    JOIN`, he was expected to be omitted.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析结果，我们可以快速识别缺失的作者John Doe。正如您可能记得的那样，我们创建了他但没有任何书籍，因此在使用`INNER JOIN`时，预期他会被排除。
- en: LEFT JOIN (or LEFT OUTER JOIN)
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LEFT JOIN（或LEFT OUTER JOIN）
- en: Returns all rows from the left table and the matching rows from the right table.
    If there is no match, null values are included for the columns from the right
    table. Similar to the previous exercise, a Venn diagram with a circle A on the
    left and a circle B on the right represents each dataset. In a `LEFT JOIN`, the
    left circle includes all rows from the left table, and the overlapping region
    represents the matching rows based on the join condition. The right circle includes
    the nonmatching rows from the right table, represented by null values in the result
    set. Have a look at [Figure 3-8](#left_join) to better visualize the Venn diagram.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 返回左表的所有行和右表的匹配行。如果没有匹配，则包括来自右表的列的空值。与前一个练习类似，圆圈 A 表示左表，圆圈 B 表示右表的 Venn 图。在 `LEFT
    JOIN` 中，左圆圈包含左表的所有行，重叠区域表示基于连接条件的匹配行。右圆圈包含来自右表的非匹配行，结果集中用空值表示。查看 [图 3-8](#left_join)
    更好地可视化 Venn 图。
- en: '![images/new_ch02_left_join.png](assets/aesd_0308.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![images/new_ch02_left_join.png](assets/aesd_0308.png)'
- en: Figure 3-8\. Venn diagram illustrating a `LEFT JOIN`
  id: totrans-278
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-8\. 展示了 `LEFT JOIN` 的维恩图。
- en: The code syntax for the `LEFT JOIN` is in [Example 3-21](#code_dml_sql01_left_join).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`LEFT JOIN` 的代码语法在 [示例 3-21](#code_dml_sql01_left_join) 中。'
- en: Example 3-21\. Syntax of a `LEFT JOIN`
  id: totrans-280
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-21\. `LEFT JOIN` 的语法。
- en: '[PRE20]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: To test the `LEFT JOIN`, let’s keep the same use case of relating the authors
    with their books, but now we want to list all authors and their respective books,
    and we must also include the authors with any book. Execute the code snippet in
    [Example 3-22](#code_dml_sql01_left_join_test).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试 `LEFT JOIN`，我们保持与之前相同的用例，将作者与其书籍关联，但现在我们要列出所有作者及其各自的书籍，并且必须包括没有书籍的作者。在
    [示例 3-22](#code_dml_sql01_left_join_test) 中执行代码片段。
- en: Example 3-22\. Gather authors and their books
  id: totrans-283
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-22\. 收集作者及其书籍。
- en: '[PRE21]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The query output is shown in [Figure 3-9](#left_join_results).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 查询结果显示在 [图 3-9](#left_join_results) 中。
- en: '![images/new_ch03_left_join.png](assets/aesd_0309.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![images/new_ch03_left_join.png](assets/aesd_0309.png)'
- en: Figure 3-9\. `LEFT JOIN` query output displaying the authors and their respective
    books
  id: totrans-287
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-9\. 展示了 `LEFT JOIN` 查询的作者及其相应书籍的输出。
- en: When compared with the `INNER JOIN`, the `LEFT JOIN` enables us to see the author
    John Doe. This is because on a `LEFT JOIN`, the left table, `authors`, is fully
    shown, while the right table, `books`, shows only the intersected result with
    `authors`.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `INNER JOIN` 相比，`LEFT JOIN` 允许我们看到作者 John Doe。这是因为在 `LEFT JOIN` 中，左表 `authors`
    被完全显示，而右表 `books` 仅显示与 `authors` 交集的结果。
- en: RIGHT JOIN (or RIGHT OUTER JOIN)
  id: totrans-289
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: RIGHT JOIN（或 RIGHT OUTER JOIN）
- en: A right join returns all rows from the right table and the matching rows from
    the left table. If there is no match, null values are included for the columns
    from the left table. Continue thinking of a Venn diagram with circle A (left)
    and circle B (right) representing each dataset. In a `RIGHT JOIN`, the right circle
    includes all rows from the right table, and the overlapping region represents
    the matching rows based on the join condition. The left circle includes the nonmatching
    rows from the left table, represented by null values in the result set. Finally,
    have a look at [Figure 3-10](#right_join) to better visualize the Venn diagram.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 右连接返回右表的所有行和左表的匹配行。如果没有匹配，则包括来自左表的列的空值。继续考虑每个数据集用圆圈 A（左）和圆圈 B（右）表示的 Venn 图。在
    `RIGHT JOIN` 中，右圆圈包含右表的所有行，重叠区域表示基于连接条件的匹配行。左圆圈包含来自左表的非匹配行，结果集中用空值表示。最后，查看 [图
    3-10](#right_join) 更好地可视化 Venn 图。
- en: '![images/new_ch02_right_join.png](assets/aesd_0310.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![images/new_ch02_right_join.png](assets/aesd_0310.png)'
- en: Figure 3-10\. Venn diagram illustrating a `RIGHT JOIN`
  id: totrans-292
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-10\. 展示了 `RIGHT JOIN` 的维恩图。
- en: The code syntax for the `RIGHT JOIN` is shown in [Example 3-23](#code_dml_sql01_right_join).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`RIGHT JOIN` 的代码语法如 [示例 3-23](#code_dml_sql01_right_join) 所示。'
- en: Example 3-23\. Syntax of a `RIGHT JOIN`
  id: totrans-294
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-23\. `RIGHT JOIN` 的语法。
- en: '[PRE22]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Let’s first contextualize our training to see the `RIGHT JOIN` in action. In
    this case, we want to see all the books and their authors, so execute the code
    in [Example 3-24](#code_dml_sql01_right_join_test).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先将我们的训练情境化，看看 `RIGHT JOIN` 的实际应用。在这种情况下，我们希望看到所有书籍及其作者，因此在 [示例 3-24](#code_dml_sql01_right_join_test)
    中执行代码。
- en: Example 3-24\. Gather books and their authors
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-24\. 收集书籍及其作者。
- en: '[PRE23]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The query output is shown in [Figure 3-11](#right_join_results).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 查询结果显示在 [图 3-11](#right_join_results) 中。
- en: '![new_ch03_inner_join.png](assets/aesd_0311.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![new_ch03_inner_join.png](assets/aesd_0311.png)'
- en: Figure 3-11\. `RIGHT JOIN` query output displaying the books and their respective
    authors
  id: totrans-301
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-11\. `RIGHT JOIN` 查询输出，显示了书籍及其对应作者
- en: By analyzing the query output, we see all books and their respective authors.
    Since we don’t have any books without an author, we cannot see any intersection
    between `books` and `authors` where a book exists without an author.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析查询输出，我们可以看到所有书籍及其相应的作者。由于没有任何没有作者的书籍，我们无法看到`books`和`authors`之间的任何交集，即存在书籍但没有作者的情况。
- en: FULL JOIN (or FULL OUTER JOIN)
  id: totrans-303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`FULL JOIN`（或`FULL OUTER JOIN`）'
- en: In this join, all rows are returned from both tables. It combines the result
    of the `LEFT JOIN` and the `RIGHT JOIN`. If there is no match, null values are
    included for the columns from the nonmatching table. In a Venn diagram with circle
    A (left) and circle B (right) representing each dataset, the diagram for a `FULL
    JOIN` will show the overlapping region representing the matching rows based on
    the join condition, while the nonoverlapping portions of each circle include the
    non-matching rows from their respective tables. In the end, the result set generated
    includes all rows from both tables, with null values for nonmatching rows. Let’s
    see [Figure 3-12](#new_ch02_full_join) to visualize it better.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在此连接中，从两个表中返回所有行。它结合了`LEFT JOIN`和`RIGHT JOIN`的结果。如果没有匹配项，将包括非匹配表中列的空值。在一个维恩图中，圆圈
    A（左）和圆圈 B（右）分别表示每个数据集，`FULL JOIN`的图表将显示重叠区域，表示基于连接条件的匹配行，而每个圆圈的非重叠部分包括各自表中的非匹配行。最终生成的结果集包括来自两个表的所有行，对于非匹配行，将包括空值。让我们查看[图 3-12](#new_ch02_full_join)以更好地可视化。
- en: '![images/new_ch02_full_join.png](assets/aesd_0312.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![images/new_ch02_full_join.png](assets/aesd_0312.png)'
- en: Figure 3-12\. Venn diagram illustrating a `FULL JOIN`
  id: totrans-306
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-12\. 用于说明`FULL JOIN`的维恩图
- en: The code syntax for the `FULL JOIN` is shown in [Example 3-25](#code_dml_sql01_full_join).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '`FULL JOIN`的代码语法见[示例 3-25](#code_dml_sql01_full_join)。'
- en: Example 3-25\. Syntax of a `FULL JOIN`
  id: totrans-308
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-25\. `FULL JOIN`的语法
- en: '[PRE24]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note
  id: totrans-310
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: MySQL doesn’t support the `FULL JOIN` natively. We must do a `UNION` between
    a `LEFT JOIN` and a `RIGHT JOIN` statement to achieve it. This effectively combines
    the data from both directions, replicating the behavior of a `FULL JOIN`.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: MySQL 不原生支持`FULL JOIN`。我们必须在`LEFT JOIN`和`RIGHT JOIN`语句之间执行`UNION`来实现它。这有效地结合了两个方向的数据，复制了`FULL
    JOIN`的行为。
- en: CROSS JOIN
  id: totrans-312
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '`CROSS JOIN`'
- en: The `CROSS JOIN` (or Cartesian join) returns the Cartesian product of both tables,
    combining every row from the first table with every row from the second table.
    It does not require a join condition. In a Venn diagram of a `CROSS JOIN`, we
    don’t have overlapping circles since it combines each row from circle A, left,
    and circle B, right. The result set includes all possible combinations of rows
    from both tables, as shown in [Figure 3-13](#new_ch02_cross_join).
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '`CROSS JOIN`（或笛卡尔积连接）返回两个表的笛卡尔积，即将第一个表的每一行与第二个表的每一行组合。它不需要连接条件。在`CROSS JOIN`的维恩图中，我们没有重叠的圆圈，因为它组合了圆圈
    A（左）和圆圈 B（右）的每一行。结果集包括来自两个表的所有可能的行组合，如[图 3-13](#new_ch02_cross_join)所示。'
- en: '![images/new_ch02_cross_join.png](assets/aesd_0313.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![images/new_ch02_cross_join.png](assets/aesd_0313.png)'
- en: Figure 3-13\. Venn diagram illustrating a `CROSS JOIN`
  id: totrans-315
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-13\. 用于说明`CROSS JOIN`的维恩图
- en: The code syntax for the `CROSS JOIN` is in [Example 3-26](#code_dml_sql01_cross_join).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`CROSS JOIN`的代码语法见[示例 3-26](#code_dml_sql01_cross_join)。'
- en: Example 3-26\. Syntax of a `CROSS JOIN`
  id: totrans-317
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-26\. `CROSS JOIN`的语法
- en: '[PRE25]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: A `CROSS JOIN` of the `authors` table and `books` table is shown in [Example 3-27](#code_dml_sql01_cross_join_test).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 展示了`authors`表和`books`表的`CROSS JOIN`，详见[示例 3-27](#code_dml_sql01_cross_join_test)。
- en: Example 3-27\. `CROSS JOIN` of `authors` and `books` tables
  id: totrans-320
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-27\. `authors`表和`books`表的`CROSS JOIN`
- en: '[PRE26]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In summary, SQL joins provide flexibility in combining data from multiple tables
    based on conditions. Understanding their usage and syntax allows you to extract
    the desired information and establish relationships for related data across tables.
    Visualizing the joins through a Venn diagram helps explain how the tables’ data
    overlaps and combines based on the join conditions by highlighting the matched
    and unmatched rows in the result set, and providing a clear representation of
    the relationship between tables during the join operation.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，SQL 连接提供了根据条件从多个表中组合数据的灵活性。了解它们的用法和语法允许您提取所需的信息，并为跨表相关数据建立关系。通过维恩图可视化连接有助于解释表数据如何基于连接条件重叠和组合，突出显示结果集中匹配和不匹配的行，并清晰地展示连接操作期间表之间关系的表示。
- en: Updating Data with UPDATE
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 UPDATE 更新数据
- en: The `UPDATE` command allows us to modify records within an existing table in
    a database. By executing this command, users can effectively update and alter
    the data stored in specific records. `UPDATE` enables changes to be made to one
    or more records within a table, ensuring that the data accurately reflects the
    latest information. By utilizing this command, users can seamlessly modify the
    content of a table, allowing for data refinement, corrections, or updates as needed.
    [Example 3-28](#code_dml_sql03) shows the syntax of this command.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '`UPDATE` 命令允许我们在数据库中现有的表内修改记录。通过执行此命令，用户可以有效地更新和修改特定记录中存储的数据。`UPDATE` 允许对表内的一个或多个记录进行更改，确保数据准确反映最新信息。通过利用此命令，用户可以无缝地修改表的内容，以便根据需要进行数据的细化、更正或更新。[示例
    3-28](#code_dml_sql03) 展示了该命令的语法。'
- en: Example 3-28\. Syntax of an `UPDATE` statement
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-28\. `UPDATE` 语句的语法
- en: '[PRE27]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The `UPDATE` keyword is used to specify the table that will be updated, and
    *`table_name`* represents the name of the table to be modified. The `SET` keyword
    indicates that columns will be updated and assigns new values to them. Within
    the `SET` clause, *`column1`* = *`value1`*, *`column2`* = *`value2`*…​ specifies
    the columns to be updated and their corresponding new values. Finally, the `WHERE`
    clause, which is optional, allows for the specification of conditions that the
    rows must satisfy to be updated. It filters the rows based on the specified conditions.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `UPDATE` 关键字指定将被更新的表，*`table_name`* 表示将被修改的表的名称。`SET` 关键字指示将更新列并为它们分配新值。在
    `SET` 子句内，*`column1`* = *`value1`*，*`column2`* = *`value2`*…​ 指定将要更新的列及其对应的新值。最后，可选的
    `WHERE` 子句允许指定行必须满足的条件以便更新。它根据指定条件过滤行。
- en: 'To test the `UPDATE` statement in reality, let’s assume that we have a typo
    in a book title: instead of “Learning React,” we want “Learning React Fundamentals.”
    Looking at the `books` table, we can see that `Learning React` has `book_id =
    2`. You can refer to the code in [Example 3-29](#code_dml_sql03_update) for guidance
    on how to achieve this update.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 要在实际中测试 `UPDATE` 语句，假设书名中有个拼写错误：“Learning React” 应该是 “Learning React Fundamentals”。查看
    `books` 表，可以看到 `Learning React` 的 `book_id = 2`。你可以参考 [示例 3-29](#code_dml_sql03_update)
    中的代码，了解如何进行此更新。
- en: Example 3-29\. Update the `books` table
  id: totrans-329
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-29\. 更新 `books` 表
- en: '[PRE28]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: And that’s it. If you look at the `books` table data again, you can see the
    new name ([Figure 3-14](#dml_update)).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。如果你再次查看 `books` 表的数据，你会看到新的名称（[图 3-14](#dml_update)）。
- en: '![images/new_ch02_update.png](assets/aesd_0314.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![images/new_ch02_update.png](assets/aesd_0314.png)'
- en: Figure 3-14\. Updating the `books` table
  id: totrans-333
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-14\. 更新 `books` 表
- en: Deleting Data with DELETE
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 DELETE 删除数据
- en: The `DELETE` command provides the ability to selectively delete certain records
    based on specified criteria or delete all records within a table. `DELETE` plays
    a vital role in data maintenance, allowing users to effectively manage and clean
    up the contents of a table by removing unnecessary or outdated records. This command
    ensures data integrity and helps optimize the database by eliminating redundant
    or irrelevant information. [Example 3-30](#code_dml_sql04) shows the syntax of
    this command.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '`DELETE` 命令提供了根据指定条件选择性删除某些记录或删除表内所有记录的能力。`DELETE` 在数据维护中发挥着重要作用，允许用户通过删除不必要或过时的记录有效地管理和清理表的内容。此命令确保数据完整性，并通过消除冗余或无关信息来优化数据库。[示例
    3-30](#code_dml_sql04) 展示了该命令的语法。'
- en: Example 3-30\. Syntax of a `DELETE` statement
  id: totrans-336
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-30\. `DELETE` 语句的语法
- en: '[PRE29]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `DELETE FROM` portion indicates the specific table from which data will
    be deleted, and *`table_name`* indicates the table’s name. The optional `WHERE`
    clause plays an essential function by allowing users to define conditions that
    must be met for rows to be deleted. By utilizing this clause, rows can be filtered
    based on specific criteria. If we don’t use the `WHERE` clause, all rows within
    the table will be deleted. Finally, *`condition`* refers to the specific conditions
    that rows must satisfy to be eligible for deletion.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '`DELETE FROM` 部分指示了将要删除数据的具体表，而 *`table_name`* 则表示表的名称。可选的 `WHERE` 子句通过允许用户定义必须满足的行删除条件，发挥着重要作用。利用此子句，可以根据特定条件筛选行。如果不使用
    `WHERE` 子句，则将删除表内的所有行。最后，*`condition`* 指的是行必须满足的具体条件，才能符合删除的资格。'
- en: To practically apply this command, let’s imagine that we will not publish any
    book from the Computer Science category. By looking into the `category_id`, we
    can see it’s the number 6\. Let’s now execute [Example 3-31](#code_dml_sql04_delete)
    and see what happens.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实际地应用此命令，我们假设不会从计算机科学类别中发布任何书籍。通过查看`category_id`，我们可以看到它是 number 6. 让我们现在执行
    [示例 3-31](#code_dml_sql04_delete) 并查看发生了什么。
- en: Example 3-31\. Delete a category from the `category` table
  id: totrans-340
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-31\. 从`category`表中删除类别
- en: '[PRE30]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If everything went well, you should be able to select the `category` table and
    see we no longer have the Computer Science category, as shown in [Figure 3-15](#dml_delete).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，您应该能够选择`category`表，并看到我们不再拥有计算机科学类别，如 [图 3-15](#dml_delete) 所示。
- en: '![images/new_ch02_delete.png](assets/aesd_0315.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![images/new_ch02_delete.png](assets/aesd_0315.png)'
- en: Figure 3-15\. Deleted category from the `category` table
  id: totrans-344
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-15\. 从`category`表中删除的类别
- en: Finally, you can also use another data management technique, named *soft delete*,
    to “delete” the data. This technique, instead of permanently erasing a record,
    sets a flag or attribute in the database to indicate that the record should be
    considered deleted. This preserves historical data, ensures easy recovery when
    needed, and supports compliance by maintaining an audit trail of changes.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您还可以使用另一种数据管理技术，称为*软删除*，来“删除”数据。这种技术不会永久擦除记录，而是在数据库中设置一个标志或属性来指示应该视为已删除的记录。这样可以保留历史数据，在需要时确保轻松恢复，并通过保持变更审计跟踪来支持合规性。
- en: Storing Queries as Views
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将查询存储为视图
- en: A *view* is a virtual table in a database defined by a query. It is similar
    to a regular table, consisting of named columns and rows of data. However, unlike
    a table, a view does not physically store data values in the database. Instead,
    it retrieves data dynamically from the tables referenced in its query when the
    view is accessed.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '*view* is a virtual table in a database defined by a query. It is similar to
    a regular table, consisting of named columns and rows of data. However, unlike
    a table, a view does not physically store data values in the database. Instead,
    it retrieves data dynamically from the tables referenced in its query when the
    view is accessed.'
- en: In [Example 3-32](#code_view_01), we see the generic syntax for creating a view.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 3-32](#code_view_01) 中，我们看到了创建视图的通用语法。
- en: Example 3-32\. `VIEW` syntax
  id: totrans-349
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-32\. `VIEW` 语法
- en: '[PRE31]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Using our `OReillyBooks` database, [Example 3-33](#code_view_02) creates a view
    for analyzing the number of books each author has created.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的 `OReillyBooks` 数据库，[示例 3-33](#code_view_02) 创建了一个视图，用于分析每位作者创建的书籍数量。
- en: Example 3-33\. A view for the `books` database
  id: totrans-352
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-33\. 用于`books`数据库的视图
- en: '[PRE32]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We can then query the `author_book_count` view to analyze the number of books
    each author has created; see [Example 3-34](#code_view_03).
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以查询 `author_book_count` 视图来分析每位作者创建的书籍数量; 参见 [示例 3-34](#code_view_03)。
- en: Example 3-34\. Query a view in the `books` database
  id: totrans-355
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-34\. 查询`books`数据库中的视图
- en: '[PRE33]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: One of the primary purposes of a view is to act as a filter on the underlying
    tables. The query to define a view can involve one or more tables or other views
    from the same or different databases. In fact, views can be created to consolidate
    data from multiple heterogeneous sources, allowing you to combine similar data
    from different servers across your organization, each storing data for a specific
    region.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 视图的主要目的之一是作为底层表的过滤器。定义视图的查询可以涉及同一数据库或不同数据库中的一个或多个表或其他视图。事实上，可以创建视图来 cons 折各式数据，允许您综合组织中的不同服务器存储在数据
    for specific region.
- en: Views are commonly used to simplify and customize each user’s perception of
    the database. By defining views, you can present a focused and tailored view of
    the data to different users, hiding unnecessary details and providing a more intuitive
    interface. Additionally, a view can serve as a security mechanism by allowing
    users to access data through the view without granting them direct access to the
    underlying base tables. This provides an additional layer of control and ensures
    that users see only the data they are authorized to view.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 视图通常用于简化和定制每个用户对数据库的感知。通过定义视图，您可以向不同的用户呈现数据的专注和定制视图，隐藏不必要的细节，并提供更直观的界面。此外，视图还可以作为安全机制，允许用户通过视图访问数据，而无需直接访问底层基表。这提供了额外的控制层，并确保用户只看到他们被授权查看的数据。
- en: In [Example 3-35](#code_view_04), we create the `renamed_books` view based on
    the `books` table. We use column aliases within the `SELECT` statement to rename
    the columns to something more familiar to a particular user without changing the
    table structure. We can even have different views on top of the same data with
    different naming conventions depending on the audience.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 3-35](#code_view_04) 中，我们基于 `books` 表创建了 `renamed_books` 视图。我们在 `SELECT`
    语句中使用列别名将列重命名为某个特定用户更熟悉的名称，而不改变表结构。甚至可以在相同数据上创建不同的视图，使用不同的命名约定，以适应不同的用户群体。
- en: Example 3-35\. A view for renaming columns
  id: totrans-360
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-35\. 重命名列的视图
- en: '[PRE34]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Furthermore, views are helpful when the schema of a table has changed. Instead
    of modifying existing queries and applications, you can create a view that emulates
    the old table structure, providing a backward-compatible interface for accessing
    the data. This way, you can maintain compatibility while changing the underlying
    data model.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当表的架构发生变化时，视图对于帮助很大。您可以创建一个视图来模拟旧的表结构，而不是修改现有的查询和应用程序，为访问数据提供向后兼容的接口。通过这种方式，您可以在改变底层数据模型的同时保持兼容性。
- en: Although views offer numerous advantages, they also have certain limitations
    and potential dangers. One limitation is the dependence on the underlying table
    structure, which we previously highlighted as a benefit; however, it’s also a
    curse. If the base table structure changes, the view definition must be updated
    accordingly, which can lead to increased maintenance overhead. In addition, views
    can affect query performance, especially for complex views that involve multiple
    tables or extensive calculations.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然视图提供了许多优点，但它们也有一些限制和潜在危险。一个限制是依赖于底层表结构，这是我们之前强调的好处；然而，它也是一个诅咒。如果基础表结构发生变化，视图定义必须相应更新，这可能会增加维护的工作量。此外，视图可能会影响查询性能，特别是对于涉及多个表或复杂计算的复杂视图。
- en: To avoid unnecessary overhead, it is essential to continuously optimize view
    queries and learn to use execution plans effectively to stop inefficiencies. Another
    danger is the possibility of creating overly complex or inefficient views, leading
    to poor performance and difficulty maintaining or modifying views over time. In
    addition, views can provide an illusion of data security by restricting access
    to specific columns or rows. However, they do not provide foolproof security,
    and unauthorized users can still access the underlying data if they gain access
    to the view. To ensure data protection, we must implement appropriate database-level
    security measures. Finally, views can lead to potential data integrity issues
    if not properly maintained, as they may not enforce constraints or referential
    integrity like physical tables. Overall, while views provide valuable functionality,
    we should understand and minimize their limitations and potential risks to ensure
    their effective and secure use.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免不必要的开销，持续优化视图查询并学会有效使用执行计划以消除低效率至关重要。另一个危险是可能创建过于复杂或低效的视图，导致性能下降，并且难以随时间进行维护或修改。此外，视图可以通过限制对特定列或行的访问来提供数据安全的错觉。然而，它们并不能提供完全安全性，未经授权的用户如果能够访问视图，仍然可以访问底层数据。为了确保数据保护，我们必须实施适当的数据库级安全措施。最后，如果不正确地维护视图，它们可能会导致潜在的数据完整性问题，因为它们可能不会像物理表那样强制执行约束或引用完整性。总体而言，虽然视图提供了有价值的功能，但我们应了解并尽量减少它们的限制和潜在风险，以确保它们的有效和安全使用。
- en: In [Example 3-36](#code_view_05), we demonstrate that because of the extensive
    number of joins and the inclusion of various columns from different tables, the
    complexity of the view increases, making it challenging to read and comprehend
    at a glance. An interesting way to fix this would be through the use of CTEs,
    which we describe in the next section.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 3-36](#code_view_05) 中，我们演示了由于连接数量繁多并且包含来自不同表的各种列，视图的复杂性增加，使得一目了然地阅读和理解变得具有挑战性。解决这个问题的一个有趣方法是通过使用公共表达式（CTE），我们将在下一节中详细描述。
- en: Example 3-36\. Complex views
  id: totrans-366
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-36\. 复杂视图
- en: '[PRE35]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Common Table Expressions
  id: totrans-368
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 公共表达式
- en: Many data analysts and developers have faced the challenge of understanding
    complex SQL queries. It’s not uncommon to struggle with knowing the purpose and
    dependencies of specific query components, especially when dealing with complicated
    business logic and multiple upstream dependencies. Add to this the frustration
    of unexpected query results that leave the analyst uncertain about which section
    of the query is causing the discrepancy. Common table expressions (CTEs) provide
    a valuable solution in such scenarios.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 许多数据分析师和开发人员面临理解复杂SQL查询的挑战。当处理复杂的业务逻辑和多个上游依赖时，特别是对特定查询组件的目的和依赖性不清楚时，很容易遇到困难。再加上意外的查询结果可能会让分析师不确定哪个查询部分导致了差异，这样的情况并不少见。公共表达式（CTEs）在这种情况下提供了宝贵的解决方案。
- en: CTEs offer a powerful tool for simplifying complex queries and improving query
    maintainability. Acting as a temporary result set, CTEs enhance the readability
    of SQL code by breaking complex queries into manageable blocks.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: CTE为简化复杂查询和提高查询可维护性提供了强大的工具。作为临时结果集，CTE通过将复杂查询分解为可管理的块来增强SQL代码的可读性。
- en: '[Example 3-37](#code_cte_01) shows the generic syntax to create a CTE in SQL.
    Even though it looks complex, it follows simple patterns.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 3-37](#code_cte_01)展示了在SQL中创建CTE的通用语法。尽管看起来复杂，但它遵循简单的模式。'
- en: Example 3-37\. CTE syntax
  id: totrans-372
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-37\. CTE语法
- en: '[PRE36]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[![1](assets/1.png)](#co_sql_for_analytics_CO1-1)'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_sql_for_analytics_CO1-1)'
- en: Declare the CTE by using the `WITH` keyword and give the expression a name.
    You can also specify the columns if needed, or use the * character.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`WITH`关键字声明公共表达式（CTE），并为表达式命名。如果需要，也可以指定列，或使用*字符。
- en: '[![2](assets/2.png)](#co_sql_for_analytics_CO1-2)'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_sql_for_analytics_CO1-2)'
- en: Define the CTE query after the `AS` keyword by writing the query that defines
    the CTE. This query can be as simple or complex as needed, including filtering,
    joining, aggregating, or any other SQL operations.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在`AS`关键字之后定义CTE查询，编写定义CTE的查询。该查询可以是简单或复杂的，包括过滤、连接、聚合或任何其他SQL操作。
- en: '[![3](assets/3.png)](#co_sql_for_analytics_CO1-3)'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_sql_for_analytics_CO1-3)'
- en: Use the CTE in a subsequent query, referencing the CTE by its name as if it
    were an actual table. You can select columns from the CTE or perform additional
    operations on the CTE’s data.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续查询中使用CTE，通过其名称引用CTE，就像引用实际表一样。您可以从CTE选择列，或对CTE的数据执行其他操作。
- en: '[![4](assets/4.png)](#co_sql_for_analytics_CO1-5)'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_sql_for_analytics_CO1-5)'
- en: Add more query operations—pipelining CTEs along your query. This step is optional.
    We can include additional query operations like filtering, sorting, grouping,
    or joining to further manipulate the data retrieved from the CTE.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在查询中添加更多的操作步骤——将CTE与您的查询管道化。这一步骤是可选的。我们可以包括额外的查询操作，如过滤、排序、分组或连接，以进一步操作从CTE检索的数据。
- en: '[Example 3-38](#code_cte_02) creates a CTE using the `books` table as a reference.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 3-38](#code_cte_02)使用`books`表作为参考创建了一个CTE。'
- en: Example 3-38\. A simple CTE
  id: totrans-383
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-38\. 一个简单的CTE
- en: '[PRE37]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Similar to derived tables and database views, CTEs provide several advantages
    that make query writing and maintenance easier. By breaking complex queries into
    smaller reusable blocks, CTEs enhance code readability and simplify the overall
    query structure. Let’s examine the difference between using a CTE and using only
    a subquery. For this exercise, we used a fictitious `sales` table with all book
    sales, as shown in [Example 3-39](#code_cte_03). This table is connected with
    the `books` table by the `book_id` primary key.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于派生表和数据库视图，CTE提供了几个优点，使查询编写和维护更加容易。通过将复杂查询分解为较小的可重用块，CTE增强了代码的可读性，并简化了整体查询结构。让我们来比较使用CTE和仅使用子查询的差异。在这个练习中，我们使用了一个虚构的`sales`表，展示了所有书籍销售情况，如[示例 3-39](#code_cte_03)所示。该表通过`book_id`主键与`books`表连接。
- en: Example 3-39\. A query without a CTE
  id: totrans-386
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-39\. 没有CTE的查询
- en: '[PRE38]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This code uses subqueries instead of CTEs to get the top five best-selling books
    in 2022\. Now, let’s use CTEs and see how readability improves in [Example 3-40](#code_cte_04).
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使用子查询而不是CTE来获取2022年销量前五的畅销书。现在，让我们使用CTE来看看在[示例 3-40](#code_cte_04)中如何提高可读性。
- en: Example 3-40\. A query with a CTE
  id: totrans-389
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-40\. 带有CTE的查询
- en: '[PRE39]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We have created two levels of CTEs. `popular_books` is the first CTE, and it
    selects the `book_id`, `title`, and `author` columns from the `books` table, filtering
    for books with a rating of 4.6 or higher. Note that this CTE focuses on a clear
    responsibility: get only the top-reviewed books.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了两个公用表达式（CTE）。`popular_books` 是第一个 CTE，它从 `books` 表中选择 `book_id`、`title`
    和 `author` 列，并筛选评分高于4.6的图书。需要注意的是，该 CTE 专注于一个明确的责任：仅获取评分最高的图书。
- en: 'Then we have `best_sellers`, the second CTE that builds upon the first CTE.
    It selects the `book_id`, `title`, `author`, and `total_sales` columns from `popular_books`
    and joins them with the `sales` table based on the `book_id` column. Additionally,
    it filters for sales that occurred in the year 2022, orders the results by total
    sales in descending order, and limits the output to the top five best-selling
    books. Again, this CTE focuses on another clear responsibility: getting the top
    five best sellers based on sales, but only for the books preselected with a `rating
    = 4.6`.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是`best_sellers`，第二个建立在第一个CTE基础上的CTE。它从`popular_books`选择`book_id`、`title`、`author`和`total_sales`列，并根据`book_id`列与`sales`表进行连接。此外，它筛选了2022年发生的销售，并按总销售额降序排列结果，并将输出限制为前五名畅销书。同样，该CTE专注于另一个明确的责任：基于销售获取评分为4.6的前五名畅销书。
- en: Finally, the main query selects all columns from `best_sellers` and retrieves
    the combined results. We could apply additional aggregations or filters on this
    main query. Still, it’s a best practice to keep the code simple and focused only
    on selecting the attributes needed for the final analysis.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，主查询从`best_sellers`中选择所有列，并检索合并的结果。我们可以在主查询上应用额外的聚合或过滤器，但将代码保持简单并仅专注于选择最终分析所需的属性是一种最佳实践。
- en: One common use case for CTEs is referencing a derived table multiple times within
    a single query. CTEs eliminate the need for redundant code by allowing the derived
    table to be defined once and referenced multiple times. This improves query clarity
    and reduces the chance of errors due to code duplication. To see it in action,
    let’s have a look at [Example 3-41](#code_cte_041), where we will keep using our
    fictional `sales` table.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: CTE的一个常见用例是在单个查询中多次引用派生表。CTE通过允许一次定义派生表并多次引用来消除冗余代码的需求。这提高了查询的清晰度，并减少了由于代码重复而导致错误的机会。为了看到其效果，让我们看一下[示例 3-41](#code_cte_041)，在这里我们将继续使用我们虚构的`sales`表。
- en: Example 3-41\. Query with CTE, derived tables
  id: totrans-395
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-41\. 使用CTE，派生表查询
- en: '[PRE40]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: As we can see, by using CTEs in this scenario, we eliminate the need for redundant
    code by defining the derived tables (`high_ratings` and `high_sales`) once. With
    this strategy, we could reference these tables multiple times within the main
    query or any subsequent CTE.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，通过在这种场景中使用CTE，我们通过一次定义派生表（`high_ratings` 和 `high_sales`）来消除冗余代码的需要。通过这种策略，我们可以在主查询或任何后续CTE中多次引用这些表。
- en: Another scenario in which CTEs shine is as an alternative to creating a permanent
    database view. Sometimes creating a view might not be necessary or feasible. A
    CTE can be used as a temporary and dynamic substitute in such cases, providing
    flexibility and simplicity by allowing you to define and reference the result
    set within the scope of a single query. We can see in [Example 3-42](#code_cte_05)
    that by using a CTE in this scenario, we avoid the need to create a permanent
    database view.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个CTE发光的场景是作为创建永久数据库视图的替代方案。有时创建视图可能并非必要或可行。在这种情况下，CTE可以作为临时和动态的替代品，提供灵活性和简单性，允许您在单个查询的范围内定义和引用结果集。我们可以在[示例 3-42](#code_cte_05)中看到，在这种情况下通过使用CTE，我们避免了创建永久数据库视图的需求。
- en: Example 3-42\. Query with a CTE to avoid permanent creation of a view
  id: totrans-399
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-42\. 使用CTE来避免永久创建视图
- en: '[PRE41]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: CTEs are also helpful when the same calculation must be performed across query
    components. Instead of duplicating the calculation in multiple places, a CTE allows
    the calculation to be defined once and reused as needed. This promotes code reusability,
    reduces maintenance efforts, and enhances query performance. Let’s start with
    [Example 3-43](#code_cte_06).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要在查询组件中执行相同的计算时，CTE也非常有帮助。与在多个地方复制计算不同，CTE允许计算只定义一次并根据需要重用。这促进了代码的可重用性，减少了维护工作，并增强了查询性能。让我们从[示例 3-43](#code_cte_06)开始。
- en: Example 3-43\. Query with a CTE to promote code reusability
  id: totrans-402
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-43\. 使用CTE促进代码的可重用性
- en: '[PRE42]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: We can see that by using the `total_sales` CTE, the calculation for total sales
    is defined once in the CTE and reused in the main query for calculating an average,
    showing the reusability of the first aggregation for another aggregation.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，通过使用`total_sales` CTE，在CTE中一次定义总销售额的计算，并在主查询中重复使用以计算平均值，展示了第一个聚合的可重用性用于另一个聚合。
- en: In conclusion, CTEs allow us to tackle complex problems by breaking them into
    smaller, more manageable pieces. By utilizing CTEs, we can organize and structure
    our queries more modularly and readably. They offer a solution for unpacking complex
    problems by allowing us to define intermediate result sets and reference them
    multiple times within a single query. This eliminates the need for redundant code
    and promotes code reusability, reducing maintenance efforts and the chance of
    errors due to code duplication.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，CTEs允许我们通过将复杂问题分解为更小、更可管理的部分来解决复杂问题。通过利用CTEs，我们可以更模块化和可读地组织和结构化我们的查询。它们为解包复杂问题提供了一个解决方案，允许我们定义中间结果集并在单个查询中多次引用它们。这消除了冗余代码的需求，促进了代码的可重用性，减少了维护工作量和由于代码重复而产生错误的机会。
- en: Window Functions
  id: totrans-406
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 窗口函数
- en: '*Window functions* are a helpful tool that improves efficiency and reduces
    query complexity when analyzing partitions or windows of a dataset. They provide
    an alternative approach to more complicated SQL concepts, such as derived queries,
    making it easier to perform advanced analysis operations.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '*窗口函数*是分析数据集的分区或窗口时，提高效率并减少查询复杂性的有用工具。它们提供了一种替代方法来处理更复杂的SQL概念，如派生查询，使得执行高级分析操作更加容易。'
- en: A common use case for window functions is ranking results within a given window,
    which allows ranking per group or creating relative rankings based on specific
    criteria. In addition, window functions allow access to data from another row
    within the same window, which is useful for tasks such as generating reports over
    a period of time or comparing data between adjacent rows.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口函数的常见用例是在给定窗口内对结果进行排名，这允许按组排名或根据特定标准创建相对排名。此外，窗口函数允许访问同一窗口内另一行的数据，这对于生成跨时间段的报告或比较相邻行的数据非常有用。
- en: At the same time, window functions facilitate aggregation within a given window,
    simplifying calculations such as running or cumulative totals. Using window functions
    makes queries more efficient, streamlined, and meaningful, allowing analysts and
    data scientists to perform sophisticated analyses of data partitions without having
    to use complicated subqueries or procedural logic. Ultimately, window functions
    enhance SQL’s analytical capabilities and provide a versatile tool for data analysis.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，窗口函数便于在给定窗口内进行聚合，简化了运行或累计总计等计算。使用窗口函数使查询更高效、流畅和有意义，允许分析师和数据科学家进行对数据分区的复杂分析，而无需使用复杂的子查询或过程逻辑。最终，窗口函数增强了SQL的分析能力，并为数据分析提供了多功能工具。
- en: A more practical way of seeing a window function is as a calculation performed
    on a set of table rows related to the current row. It is similar to an aggregate
    function but doesn’t group rows into a single output row. Instead, each row retains
    its separate identity. Window functions can access more than just the current
    row in the query result.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 更实际地看待窗口函数的一种方式是，它是在与当前行相关的一组表行上执行的计算。它类似于聚合函数，但不会将行分组为单个输出行。相反，每行保留其单独的身份。窗口函数可以访问查询结果中不止当前行的数据。
- en: The syntax for window functions, as seen in [Example 3-44](#code_wf_01), includes
    several components. First, we use the `SELECT` statement to specify the columns
    we want to include in the result set. These columns can be any combination of
    the available columns in the table. Next, we choose the window function we want
    to use. Standard window functions include `SUM()`, `COUNT()`, `ROW_NUMBER()`,
    `RANK()`, `LEAD()`, `LAG()`, and many more. We can use these functions to perform
    calculations or apply aggregate operations to a specific column or set of columns.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口函数的语法，如[示例 3-44](#code_wf_01)所示，包括几个组成部分。首先，我们使用`SELECT`语句指定要包含在结果集中的列。这些列可以是表中可用列的任意组合。接下来，我们选择要使用的窗口函数。标准窗口函数包括`SUM()`、`COUNT()`、`ROW_NUMBER()`、`RANK()`、`LEAD()`、`LAG()`等等。我们可以使用这些函数对特定列或列集执行计算或应用聚合操作。
- en: Example 3-44\. Window function syntax
  id: totrans-412
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-44\. 窗口函数语法
- en: '[PRE43]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'To define the window frame over which the window function is calculated, use
    the `OVER` clause. Inside the `OVER` clause, we have two main components: `PARTITION
    BY` and `ORDER BY`.'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义窗口函数计算的窗口帧，请使用 `OVER` 子句。在 `OVER` 子句内部，我们有两个主要组件：`PARTITION BY` 和 `ORDER
    BY`。
- en: The `PARTITION BY` clause divides the rows into partitions based on one or more
    columns. The window function is then applied separately to each partition. This
    is useful when we want to perform calculations on different data groups within
    the table.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '`PARTITION BY` 子句根据一个或多个列将行分成分区。然后，窗口函数分别应用于每个分区。当我们想要在表中不同数据组中执行计算时，这是非常有用的。'
- en: The `ORDER BY` clause allows you to specify one or more columns to determine
    the order within each partition. The window function is applied based on this
    order. It helps define the logical sequence or order of the data that the window
    function will work with. Combining the `PARTITION BY` and `ORDER BY` clauses within
    the `OVER` clause lets us control precisely how the window function acts on the
    data, allowing us to perform calculations or apply aggregate functions to a specific
    window or subset of rows in the table without changing the entire result set.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '`ORDER BY` 子句允许您指定一个或多个列来确定每个分区内的顺序。基于此顺序应用窗口函数。它有助于定义窗口函数处理的数据的逻辑顺序或顺序。在 `OVER`
    子句内结合 `PARTITION BY` 和 `ORDER BY` 子句，使我们能够精确控制窗口函数对数据的操作方式，允许我们对表中特定窗口或行子集执行计算或应用聚合函数，而不改变整个结果集。'
- en: One practical example of using window functions is the calculation of a running
    total. In the given query, the `running_count` column displays the sequential
    count of books based on their publication year. The window function `ROW_NUMBER()
    OVER (ORDER BY publication_year)` assigns a row number to each book, ordered by
    the publication year. This code can be seen in [Example 3-45](#code_wf_01_2),
    and the query output is shown in [Figure 3-16](#running_count_example).
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 使用窗口函数的一个实际示例是计算运行总数。在给定查询中，`running_count` 列显示根据它们的出版年份的书籍的顺序计数。窗口函数 `ROW_NUMBER()
    OVER (ORDER BY publication_year)` 为每本书分配一个行号，按出版年份排序。此代码可在 [示例 3-45](#code_wf_01_2)
    中看到，并且查询输出显示在 [图 3-16](#running_count_example) 中。
- en: Example 3-45\. Window function example
  id: totrans-418
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-45\. 窗口函数示例
- en: '[PRE44]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![images/new_ch03_running_count.png](assets/aesd_0316.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![图像/images/new_ch03_running_count.png](assets/aesd_0316.png)'
- en: Figure 3-16\. Running count
  id: totrans-421
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-16\. 运行计数
- en: With window functions, you can also use aggregate functions like `COUNT()` and
    `AVG()`, which are introduced in [“Aggregating data with GROUP BY”](#group_by_chapter).
    These functions can be used similarly to regular aggregations, but they operate
    on the specified window.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 使用窗口函数，您还可以使用像 `COUNT()` 和 `AVG()` 这样的聚合函数，这些函数在 [“使用 GROUP BY 聚合数据”](#group_by_chapter)
    中介绍。这些函数可以类似于常规聚合使用，但它们作用于指定的窗口。
- en: Window functions provide additional functionalities such as `ROW_NUMBER()`,
    `RANK()`, and `DENSE_RANK()` for numbering and ranking rows, `NTILE()` for determining
    percentiles or quartiles, and `LAG()` and `LEAD()` for accessing values from previous
    or subsequent rows.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口函数提供额外的功能，例如用于编号和排名行的 `ROW_NUMBER()`、`RANK()` 和 `DENSE_RANK()`，用于确定百分位数或四分位数的
    `NTILE()`，以及用于访问前后行的值的 `LAG()` 和 `LEAD()`。
- en: '[Table 3-4](#window_functions) summarizes the multiple types of window functions.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 3-4](#window_functions) 总结了多种类型的窗口函数。'
- en: Table 3-4\. Window functions
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3-4\. 窗口函数
- en: '| Type | Function | Example |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 函数 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Aggregate functions | Aggregate within each window and return a single value
    for each row | `MAX()`, `MIN()`, `AVG()`, `SUM()`, `COUNT()` |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| 聚合函数 | 在每个窗口内进行聚合并为每一行返回单个值 | `MAX()`, `MIN()`, `AVG()`, `SUM()`, `COUNT()`
    |'
- en: '| Ranking functions | Assign a rank or position to each row within the window
    based on a specified criterion | `ROW_NUMBER()`, `RANK()`, `DENSE_RANK()`, `NTILE()`,
    `PERCENT_RANK()`, `CUME_DIST()` |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| 排名函数 | 根据指定的标准为窗口内的每行分配排名或位置 | `ROW_NUMBER()`, `RANK()`, `DENSE_RANK()`,
    `NTILE()`, `PERCENT_RANK()`, `CUME_DIST()` |'
- en: '| Analytics functions | Compute values based on the data in the window without
    modifying the number of rows | `LEAD()`, `LAG()`, `FIRST_VALUE()`, `LAST_VALUE()`
    |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| 分析函数 | 根据窗口中的数据计算值，而不修改行数 | `LEAD()`, `LAG()`, `FIRST_VALUE()`, `LAST_VALUE()`
    |'
- en: To gain insight into each type of function, we’ll utilize the `publication_year`
    column as a foundation and experiment with a range of functions.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 为了深入了解每种类型的函数，我们将使用 `publication_year` 列作为基础，并尝试一系列函数。
- en: In the first example, we want to rank the newest to the oldest book by ascending
    order. Let’s have a look at the snippet in [Example 3-46](#code_wf_rank).
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个示例中，我们希望按照升序将最新到最旧的书籍进行排名。让我们来看一下[示例 3-46](#code_wf_rank)中的片段。
- en: Example 3-46\. Window function—`RANK()`
  id: totrans-433
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-46\. 窗口函数—`RANK()`
- en: '[PRE45]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: While using the `RANK()` function, one important consideration is that it assigns
    a unique rank to each row within the window based on the specified criteria, yet,
    if multiple rows share the same value and are assigned the same rank, the subsequent
    ranks are skipped. For example, if two books have the same `publication_year`,
    the next rank will be incremented by the number of rows with the same rank. If
    you don’t want repeated ranks, where distinct rows share the same rank, you might
    want to use the `ROW_NUMBER()` instead.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`RANK()`函数时，一个重要的考虑因素是它根据指定的条件为窗口内的每一行分配一个唯一的排名。然而，如果多行共享相同的值并且被分配相同的排名，则会跳过后续的排名。例如，如果两本书的`publication_year`相同，那么下一个排名将按照具有相同排名的行数递增。如果不想要重复的排名，可以考虑使用`ROW_NUMBER()`替代。
- en: In [Example 3-47](#code_wf_ntile), we want to bucket our data by the `publication_year`.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 3-47](#code_wf_ntile)中，我们希望按照`publication_year`来分桶我们的数据。
- en: Example 3-47\. Window function—`NTILE()`
  id: totrans-437
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-47\. 窗口函数—`NTILE()`
- en: '[PRE46]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '`NTILE()` is commonly used when you want to distribute rows into a specified
    number of groups evenly or when you need to divide data for further analysis or
    processing. This helps with tasks such as data segmentation, percentile calculations,
    or creating equal-sized samples.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '`NTILE()`通常用于在你想要将行均匀地分布到指定数量的组中，或者当你需要将数据划分为进一步分析或处理的时候。这有助于诸如数据分段、百分位数计算或创建大小相等的样本等任务。'
- en: Finally, we want to know the `publication_year` of the previously published
    book. For that, we use the `LAG()` function, as presented in [Example 3-48](#code_wf_lag).
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们想要知道先前发布的书籍的`publication_year`。为此，我们使用`LAG()`函数，如[示例 3-48](#code_wf_lag)中所示。
- en: Example 3-48\. Window function—`LAG()`
  id: totrans-441
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-48\. 窗口函数—`LAG()`
- en: '[PRE47]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The `LAG()` function in SQL allows you to access data from a previous row within
    the window frame. It retrieves the value of a specified column from a preceding
    row based on the ordering specified in the `OVER` clause.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: SQL中的`LAG()`函数允许你在窗口框架内访问前一行的数据。它根据`OVER`子句中指定的顺序检索指定列的值。
- en: SQL for Distributed Data Processing
  id: totrans-444
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式数据处理的SQL
- en: As enterprises make their way to the cloud, they encounter a common challenge.
    Their existing relational databases, which are the foundation for critical applications,
    cannot fully realize the potential of the cloud and are difficult to scale effectively.
    It is becoming clear that the database itself is emerging as a bottleneck, hindering
    the speed and efficiency of the transition. As a result, organizations are looking
    for a solution that combines the reliability of proven relational data stores
    such as Oracle, SQL Server, Postgres, and MySQL with the scalability and global
    reach of the cloud.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 随着企业进入云端，他们面临一个普遍的挑战。他们现有的关系数据库，作为关键应用程序的基础，无法充分发挥云端的潜力，并且难以有效扩展。显而易见，数据库本身正在成为限制速度和效率的瓶颈。因此，组织正在寻找一种解决方案，将诸如Oracle、SQL
    Server、Postgres和MySQL等经过验证的关系数据存储的可靠性与云端的可扩展性和全球覆盖性结合起来。
- en: In an attempt to meet these needs, some companies have turned to NoSQL databases.
    While these alternatives often meet scalability requirements, they tend to be
    unsuitable as transactional databases. The reason for this limitation lies in
    their design, as they were not originally designed to provide true consistency
    from the ground up. Although specific NoSQL solutions have recently introduced
    advances to handle certain types of challenges, they are subject to a variety
    of caveats and ultimately do not provide the necessary isolation levels for critical
    workloads such as banks or hospitals.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足这些需求，一些公司已经转向了NoSQL数据库。虽然这些替代方案通常可以满足可扩展性的要求，但它们往往不适合作为事务性数据库。这种限制的原因在于它们的设计，因为它们最初并不是为了从根本上提供真正的一致性而设计的。尽管特定的NoSQL解决方案最近已经引入了一些用于处理某些类型挑战的进展，但它们受到各种限制，最终无法提供银行或医院等关键工作负载所需的必要隔离级别。
- en: Recognizing the shortcomings of both legacy relational databases and NoSQL storage,
    companies have turned to a promising solution known as *distributed SQL*. This
    innovative approach deploys a single logical database across multiple physical
    nodes, either in a single data center or distributed across multiple data centers
    as needed. By leveraging the power of a distributed architecture, distributed
    SQL combines elastic scalability with unwavering resilience.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 企业意识到传统关系数据库和 NoSQL 存储的缺点后，转向了一个被称为*分布式 SQL*的有前景的解决方案。这种创新方法在单个数据中心或根据需要分布在多个数据中心，跨多个物理节点部署单一逻辑数据库。通过利用分布式架构的力量，分布式
    SQL 结合了弹性可扩展性和坚定的弹性。
- en: One of the key benefits of distributed SQL is its ability to scale seamlessly
    to meet the evolving needs of modern cloud environments. As data volumes grow
    and user demands increase, organizations can effortlessly add additional nodes
    to the distributed deployment, allowing the database to expand horizontally. This
    elastic scaling ensures that performance remains optimal even under heavy workloads
    and eliminates the limitations often faced by traditional relational databases.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式 SQL 的关键优势之一是其无缝扩展的能力，以满足现代云环境不断变化的需求。随着数据量的增长和用户需求的增加，组织可以轻松地向分布式部署中添加额外的节点，从而使数据库在水平方向上扩展。这种弹性扩展确保性能即使在高负载下也保持最佳，并消除了传统关系数据库经常面临的限制。
- en: At the same time, distributed SQL provides unparalleled resilience. Because
    data is distributed across multiple nodes, it is inherently fault-tolerant. If
    one node fails or becomes unavailable, the system can automatically forward queries
    to the remaining healthy nodes, ensuring uninterrupted access to critical data.
    This robust resilience significantly reduces the risk of downtime and data loss
    and increases the overall reliability of the database. Its distributed nature
    also enables global coverage and data availability. Organizations can deploy nodes
    in different geographic regions to strategically place them closer to end users
    and reduce latency. This geographically distributed approach ensures that data
    can be accessed quickly from anywhere worldwide, facilitating efficient data delivery
    and enabling organizations to serve a global user base.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，分布式 SQL 提供了无与伦比的弹性。由于数据分布在多个节点上，它天生具有容错能力。如果一个节点失败或变得不可用，系统可以自动将查询转发到其余健康节点，确保对关键数据的不间断访问。这种强大的弹性显著降低了停机和数据丢失的风险，并增加了数据库的整体可靠性。其分布式特性还能实现全球覆盖和数据可用性。组织可以在不同的地理区域部署节点，以使它们更靠近最终用户并减少延迟。这种地理分布的方法确保数据可以从全球任何地方快速访问，促进高效的数据交付，并使组织能够为全球用户群提供服务。
- en: The focus of this book is not on the actual distributed processing engines nor
    on how they work; rather, we touch upon only the interfaces they expose for us,
    to interact with. Most of them end up exposing either an API or SDK. However,
    a few that are more focused on data analytics use SQL as an interface language.
    In reality, distributed processing and SQL have emerged as a powerful combination,
    with SQL serving as a convenient and familiar interface for leveraging distributed
    computing capabilities.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的重点不在于实际的分布式处理引擎或它们的工作方式；相反，我们只触及它们为我们提供的接口，以便与之交互。大多数情况下，它们最终会公开 API 或 SDK。然而，一些更专注于数据分析的产品使用
    SQL 作为接口语言。实际上，分布式处理和 SQL 结合起来已经成为一种强大的组合，SQL 作为一种便捷和熟悉的接口，可以充分利用分布式计算能力。
- en: Distributed processing frameworks like Spark, Hadoop, and Dask provide the infrastructure
    for processing large-scale data across multiple machines or clusters. These frameworks
    distribute the workload and parallelize computations, enabling faster and more
    efficient data processing. On the other hand, SQL offers a declarative and intuitive
    way to express data operations. Users can leverage their SQL skills to harness
    the power of distributed computing frameworks by integrating SQL as an interface
    for distributed processing. This approach allows for seamless scalability, efficient
    data processing, and the ability to handle complex analytics tasks on vast datasets,
    all while using the familiar SQL syntax.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Spark、Hadoop 和 Dask 这样的分布式处理框架为跨多台机器或集群处理大规模数据提供了基础设施。这些框架分发工作负载并并行化计算，实现更快、更高效的数据处理。另一方面，SQL
    提供了一种声明式且直观的方式来表达数据操作。用户可以利用他们的 SQL 技能，通过将 SQL 作为分布式处理的接口来发挥分布式计算框架的强大功能。这种方法能够实现无缝扩展、高效数据处理，并能处理大规模数据集上的复杂分析任务，同时使用熟悉的
    SQL 语法。
- en: This combination empowers users to perform advanced data analytics and processing
    tasks in a straightforward and efficient manner. Examples of this powerful combination
    are DuckDB, dbt itself, and even FugueSQL. These interfaces act as a layer on
    top of distributed computing engines, allowing users to write SQL queries and
    leverage their familiarity with SQL syntax and semantics. DuckDB specifically
    aims to enable efficient and scalable execution of SQL queries while leveraging
    the power of distributed computing. It allows users to formulate their analysis
    and data processing workflows using SQL, while the underlying distributed processing
    engine handles parallel execution on multiple clusters of machines.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 这种组合使用户能够以简单高效的方式执行高级数据分析和处理任务。这种强大的组合的例子包括 DuckDB、dbt 本身，甚至是 FugueSQL。这些接口作为分布式计算引擎的一层，允许用户编写
    SQL 查询并利用他们对 SQL 语法和语义的熟悉程度。DuckDB 特别旨在实现 SQL 查询的高效和可扩展执行，同时利用分布式计算的力量。它允许用户使用
    SQL 制定其分析和数据处理工作流程，而底层的分布式处理引擎则处理多台机器上的并行执行。
- en: However, despite the existence of these SQL interfaces, they are frequently
    utilized in conjunction with Python code. Even in the Spark documentation, Python
    code is still required for various tasks, such as data transformations, DataFrame
    loading, and post processing after executing the SQL query. This reliance on Python
    code stems from standard SQL lacking the grammatical constructs to express many
    of the operations typically performed by users in distributed computing environments.
    Consequently, SQL alone is often insufficient for expressing comprehensive end-to-end
    workflows.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管存在这些 SQL 接口，它们经常与 Python 代码一起使用。即使在 Spark 文档中，Python 代码仍然需要用于各种任务，如数据转换、DataFrame
    加载以及执行 SQL 查询后的后处理。这种对 Python 代码的依赖源于标准 SQL 缺乏用于表达在分布式计算环境中用户通常执行的许多操作的语法结构。因此，仅仅依靠
    SQL 通常无法表达完整的端到端工作流程。
- en: Let’s dive deeper with an example. Say we need to create a SQL query to understand
    all the units sold by O’Reilly authors since its inception. This would be a straightforward
    consultation, as shown in [Example 3-49](#code_sql_simple_01).
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个示例深入探讨一下。假设我们需要创建一个 SQL 查询来了解自 O’Reilly 作者成立以来销售的所有单位。这将是一个直接的查询，如 [示例 3-49](#code_sql_simple_01)
    所示。
- en: Example 3-49\. A basic SQL query
  id: totrans-455
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-49\. 一个基本的 SQL 查询
- en: '[PRE48]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: At this point, the SQL query provides us with the desired aggregated results.
    However, if we want to perform additional data manipulations or integrate the
    results with external systems, we usually need to resort to Python or other programming
    languages.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，SQL 查询为我们提供了所需的聚合结果。然而，如果我们想要执行其他数据操作或将结果集成到外部系统中，通常需要借助 Python 或其他编程语言。
- en: For instance, we could join the aggregated results with customer demographic
    data stored in a separate dataset to gain deeper insights. This operation typically
    requires writing Python code to perform the data merge and post-processing steps.
    Additionally, if we intend to visualize the results or export them to another
    format, Python code is again necessary to accomplish these tasks.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以将聚合结果与存储在单独数据集中的客户人口统计数据进行连接，以获取更深入的见解。这种操作通常需要编写 Python 代码来执行数据合并和后处理步骤。此外，如果我们打算将结果可视化或导出到其他格式，同样需要使用
    Python 代码来完成这些任务。
- en: A common use case is actually exposing data as an API, which SQL doesn’t provide
    capabilities for. [Example 3-50](#code_sql_api_simple_01) shows how combining
    SQL with Python can enable an end-to-end flow.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的用例实际上是将数据公开为 API，而 SQL 无法提供这样的能力。[示例 3-50](#code_sql_api_simple_01) 展示了如何将
    SQL 与 Python 结合使用以实现端到端流程。
- en: Example 3-50\. A basic FastAPI
  id: totrans-460
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-50\. 一个基本的 FastAPI
- en: '[PRE49]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We have developed a FastAPI application and set up a single endpoint `GET` that
    is accessible via the `/top_books` route. In simpler terms, an *endpoint* is a
    specific web address (URL) that we can use to retrieve information from our application.
    When someone accesses this URL in their web browser or through an application,
    it triggers the execution of a specific function that we have defined, `get_top_books`.
    This function contains the instructions on what to do when someone retrieves information
    from the `/top_books` endpoint. Essentially, it’s as if we had a specific button
    that, when pressed, causes our application to perform a specific action, such
    as providing a list of the top-selling books.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开发了一个 FastAPI 应用程序，并设置了一个单一的 `GET` 端点，可通过 `/top_books` 路径访问。简单来说，*端点* 是一个特定的网络地址（URL），我们可以用它来从我们的应用程序中检索信息。当某人在其网络浏览器或应用程序中访问此
    URL 时，将触发我们定义的特定函数 `get_top_books` 的执行。这个函数包含了当某人从 `/top_books` 端点检索信息时该执行的指令。本质上，这就像我们有一个特定的按钮，当按下时，会导致我们的应用程序执行特定的操作，比如提供畅销书列表。
- en: Inside the function, we establish a connection to the DuckDB database by using
    `duckdb.connect()`. Then the SQL query is executed using the `execute()` method
    on the connection object. The query selects the titles and units sold from the
    `sales` table, filtered by the publisher `O'Reilly`. The result is ordered by
    units sold in descending order and limited to the top five books.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数内部，我们通过使用 `duckdb.connect()` 建立到 DuckDB 数据库的连接。然后使用连接对象的 `execute()` 方法执行
    SQL 查询。该查询从 `sales` 表中选择由出版商 `O'Reilly` 发布的书籍的标题和销量。结果按销量降序排序，并限制为前五本书。
- en: The query result is then transformed into a list of dictionaries; each dictionary
    represents a book with its title and units sold. Finally, the result is returned
    as JSON by wrapping it in a dictionary with the key `top_books`.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 查询结果然后被转换成一个字典列表；每个字典代表一本书及其标题和销量。最后，结果被包装在一个带有键`top_books`的字典中，以 JSON 格式返回。
- en: 'By leveraging both languages, we can create and manipulate data via our friendly
    interface SQL and expose it as an API via the excellent FastAPI framework. In
    the following section, we will explore three well-known Python frameworks that
    abstract access to distributed data processing with a SQL-like interface: DuckDB,
    FugueSQL, and Polars.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 通过同时利用这两种语言，我们可以通过友好的 SQL 接口创建和操作数据，并通过优秀的 FastAPI 框架将其作为 API 公开。在接下来的部分中，我们将探讨三个著名的
    Python 框架，它们通过类似 SQL 的接口抽象了对分布式数据处理的访问：DuckDB、FugueSQL 和 Polars。
- en: Data Manipulation with DuckDB
  id: totrans-466
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据操作与 DuckDB
- en: When it comes to data processing libraries, most data scientists are very familiar
    with pandas, the predominant data processing library in Python. Pandas is known
    for its simplicity, versatility, and ability to manage multiple data formats and
    sizes. It provides an intuitive user interface for data manipulation. Those who
    are familiar with SQL appreciate the powerful features that allow users to perform
    complicated data transformations using a concise syntax. However, in some cases
    a trade-off must be made between the speed of execution and the ease of use or
    expressiveness of the tools. This dilemma becomes especially difficult when dealing
    with large datasets that exceed memory limits or require complex data processing
    operations.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到数据处理库时，大多数数据科学家非常熟悉 pandas，这是 Python 中主要的数据处理库。Pandas 以其简单性、多功能性和管理多种数据格式和大小的能力而闻名。它为数据操作提供了直观的用户界面。熟悉
    SQL 的人士欣赏其强大的特性，使用户可以使用简洁的语法执行复杂的数据转换。然而，在某些情况下，必须在执行速度和工具的易用性或表达能力之间做出权衡。当处理超出内存限制或需要复杂数据处理操作的大型数据集时，这种困境尤为棘手。
- en: In such cases, using SQL instead of pandas may be a better solution. This is
    where DuckDB comes into play. DuckDB combines the strengths of pandas and SQL
    by providing a fast and efficient SQL query execution engine capable of processing
    complex queries on large datasets. It integrates seamlessly with pandas DataFrames
    and allows queries to be executed directly on the DataFrames without the need
    for frequent data transfers. With DuckDB, data scientists can harness the power
    of SQL while working with pandas, balancing performance with ease of use.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，使用 SQL 而不是 pandas 可能是更好的解决方案。这就是 DuckDB 的用武之地。DuckDB 结合了 pandas 和 SQL
    的优势，提供了一个快速高效的 SQL 查询执行引擎，能够处理大数据集上的复杂查询。它与 pandas DataFrames 无缝集成，允许直接在 DataFrames
    上执行查询，无需频繁数据传输。通过 DuckDB，数据科学家可以在使用 pandas 时利用 SQL 的强大功能，平衡性能与易用性。
- en: In addition, we are seeing the trend of some companies deciding to replace Spark
    as a data processing engine with dbt in combination with DuckDB. Of course, this
    has to be judged on a case-by-case basis, but it definitely opens the door for
    analysts to support more complex data transformations that can run ad hoc or automated
    in a data pipeline.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们看到一些公司决定将 Spark 作为数据处理引擎与 dbt 结合使用 DuckDB 来替代。当然，这必须根据具体情况进行评估，但它确实为分析师支持可以在数据管道中运行的更复杂的数据转换打开了大门。
- en: Installing DuckDB
  id: totrans-470
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 DuckDB
- en: DuckDB is a remarkably lightweight database engine that works within the host
    process without external dependencies. Installation is straightforward and requires
    only a few simple steps.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: DuckDB 是一个非常轻量级的数据库引擎，可以在主机进程内工作，无需外部依赖。安装简单，只需几个简单的步骤。
- en: To install DuckDB, we have several options, depending on the operating system
    and the type of installation we want to do. For now, let’s look at how to install
    DuckDB by using the pip package manager, as shown in [Example 3-51](#code_duckdb_00).
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 DuckDB，我们有几个选项，这取决于操作系统和我们想要进行的安装类型。现在，让我们看看如何使用 pip 包管理器安装 DuckDB，如示例 [3-51](#code_duckdb_00)
    中所示。
- en: Example 3-51\. Installing DuckDB
  id: totrans-473
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-51\. 安装 DuckDB
- en: '[PRE50]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: And that’s it. We can now use DuckDB in Python just like any other library.
    [Example 3-52](#code_duckdb_01) shows how easy it is to load a pandas DataFrame
    into DuckDB, manipulate the data, and store the result back as a DataFrame.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。我们现在可以像使用任何其他库一样在 Python 中使用 DuckDB。示例 [3-52](#code_duckdb_01) 展示了如何将 pandas
    DataFrame 载入 DuckDB，操作数据，并将结果存储回 DataFrame 中。
- en: Example 3-52\. Using DuckDB
  id: totrans-476
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-52\. 使用 DuckDB
- en: '[PRE51]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: As we can observe, the code imports both the pandas library as `pd` and the
    DuckDB library. This allows the code to access the functionalities provided by
    these libraries. Next, a pandas DataFrame named `mydf` is created, which consists
    of a single column `a` with three rows containing the values `[1, 2, 3]`. The
    subsequent line of code executes a SQL query using the DuckDB interface. The query
    is `SELECT SUM(a) FROM mydf`, which calculates the sum of values in the `a` column
    of the `mydf` DataFrame. The result of the SQL query is stored in the `result`
    variable. By using the `to_df()` method on the DuckDB query result, the data is
    converted into a pandas DataFrame. This allows further data manipulation or analysis
    using the rich set of functions and methods in pandas.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所观察到的，代码导入了 pandas 库作为`pd`和 DuckDB 库。这使得代码能够访问这些库提供的功能。接下来，创建了一个名为`mydf`的
    pandas DataFrame，其中包含一个名为`a`的单列，包含值为`[1, 2, 3]`的三行数据。代码的下一行使用 DuckDB 接口执行了一个 SQL
    查询。查询是`SELECT SUM(a) FROM mydf`，计算了`mydf` DataFrame 中`a`列的值的总和。SQL 查询的结果存储在`result`变量中。通过在
    DuckDB 查询结果上使用`to_df()`方法，数据被转换为 pandas DataFrame。这允许使用 pandas 中丰富的函数和方法进行进一步的数据操作或分析。
- en: Running SQL queries with DuckDB
  id: totrans-479
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 DuckDB 运行 SQL 查询
- en: Now that we have seen a simple example, let’s take a closer look at some of
    the core features of DuckDB. Unlike traditional systems, DuckDB works directly
    within the application, eliminating the need for external processes or client/server
    architectures. This paradigm is closely aligned with SQLite’s in-process model
    and ensures seamless integration and efficient execution of SQL queries.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过一个简单的例子，让我们更详细地了解一些 DuckDB 的核心特性。与传统系统不同，DuckDB 直接在应用程序内部运行，无需外部进程或客户端/服务器架构。这种范式与
    SQLite 的进程内模型密切相关，确保 SQL 查询的无缝集成和高效执行。
- en: The importance of this approach also extends to the area of OLAP, a technology
    that enables sophisticated analysis of large enterprise databases while minimizing
    the impact on transactional systems. Just like other OLAP-oriented database management
    systems, DuckDB handles complex analytical workloads by leveraging its innovative
    vectorized query execution engine. Its column-oriented approach improves performance
    and scalability, making it an optimal choice for processing analytical queries.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的重要性还延伸到OLAP领域，这是一种技术，它可以在尽量减少对事务系统影响的同时，对大型企业数据库进行复杂分析。与其他面向OLAP的数据库管理系统一样，DuckDB通过利用其创新的向量化查询执行引擎来处理复杂的分析工作负载。其列式存储方法提高了性能和可扩展性，使其成为处理分析查询的最佳选择。
- en: A notable advantage of DuckDB is its self-contained design. Unlike traditional
    databases, DuckDB doesn’t require you to install, update, or maintain any external
    dependencies or server software. This streamlined, self-contained architecture
    simplifies deployment and enables rapid data transfer between the application
    and the database. The result is an exceptionally responsive and efficient system.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: DuckDB的一个显著优势是其自包含设计。与传统数据库不同，DuckDB不需要您安装、更新或维护任何外部依赖项或服务器软件。这种简化的自包含架构简化了部署，并允许应用程序与数据库之间快速传输数据。结果是一个反应异常快速和高效的系统。
- en: Another interesting feature of DuckDB is that it owes its technical capabilities
    to the hardworking and capable developers who have ensured its stability and maturity.
    Rigorous testing with millions of queries from leading systems confirms DuckDB’s
    performance and reliability. It adheres to the ACID property principles, supports
    secondary indexes, and provides robust SQL capabilities, proving its versatility
    and suitability for demanding analytical workloads.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: DuckDB的另一个有趣特性是，它的技术能力归功于那些辛勤工作、能力出众的开发人员，他们确保了其稳定性和成熟性。从领先系统中的数百万个查询中进行的严格测试验证了DuckDB的性能和可靠性。它遵循ACID属性原则，支持次要索引，并提供强大的SQL能力，证明了其多才多艺和适合处理苛刻分析工作负载的特性。
- en: DuckDB integrates with popular data analysis frameworks such as Python and R,
    enabling seamless and efficient interactive data analysis. Moreover, it not only
    supports Python and R, but also provides APIs for C, C++, and Java, which allows
    it to be used in a variety of programming languages and environments. It is known
    for its exceptional performance and flexibility, making it well suited for efficiently
    processing and querying large amounts of data. Running SQL queries with DuckDB
    is a valuable skill for analysts. Analysts can leverage the power of DuckDB to
    effortlessly execute complex SQL queries and gain valuable insights from the data.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: DuckDB集成了流行的数据分析框架，如Python和R，实现了无缝和高效的交互式数据分析。此外，它不仅支持Python和R，还提供了C、C++和Java的API，使其能够在各种编程语言和环境中使用。它以其卓越的性能和灵活性而闻名，非常适合高效处理和查询大量数据。在分析师中运行SQL查询是一项有价值的技能。分析师可以利用DuckDB的强大功能轻松执行复杂的SQL查询，并从数据中获得有价值的见解。
- en: Now that we’ve learned more about DuckDB, let’s do a step-by-step exercise to
    illustrate some of the benefits. We’ll use the same book analysis query we used
    earlier. First, import the libraries we need, pandas and DuckDB, as shown in [Example 3-53](#code_duckdb_comp_00).
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经更多了解了DuckDB，让我们逐步进行一些练习，以说明一些好处。我们将使用之前使用的相同的书籍分析查询。首先，导入我们需要的库，如示例[3-53](#code_duckdb_comp_00)所示，包括pandas和DuckDB。
- en: Example 3-53\. Importing libs in DuckDB
  id: totrans-486
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-53\. 在DuckDB中导入库
- en: '[PRE52]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The next step is connecting to DuckDB’s in-memory database ([Example 3-54](#code_duckdb_comp_01)).
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是连接到DuckDB的内存数据库（参见[示例 3-54](#code_duckdb_comp_01)）。
- en: Example 3-54\. Connecting to DuckDB
  id: totrans-489
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-54\. 连接到DuckDB
- en: '[PRE53]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Let’s start by creating a fictitious pandas DataFrame to play with using DuckDB.
    Execute the code in [Example 3-55](#code_duckdb_comp_02).
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从创建一个虚构的pandas DataFrame开始，使用DuckDB进行操作。执行[示例 3-55](#code_duckdb_comp_02)中的代码。
- en: Example 3-55\. Loading the data file
  id: totrans-492
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-55\. 加载数据文件
- en: '[PRE54]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Now this is where we introduce DuckDB to our code. Specifically, we create a
    DuckDB table from the DataFrame. This is done by registering the DataFrame by
    using the connection and giving it a name (in this case, `sales`), as shown in
    [Example 3-56](#code_duckdb_comp_03). This allows us to use SQL to query and manipulate
    the data.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将DuckDB引入我们的代码。具体来说，我们通过使用连接将DataFrame注册为一个DuckDB表，并为其命名（在本例中为`sales`），如示例[3-56](#code_duckdb_comp_03)所示。这使我们能够使用SQL来查询和操作数据。
- en: Example 3-56\. Creating a DuckDB table
  id: totrans-495
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-56\. 创建 DuckDB 表
- en: '[PRE55]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: With our table available to be queried, we can now perform whatever analytics
    tasks are needed. For example, we could calculate total revenue for the O’Reilly
    books, as seen in [Example 3-57](#code_duckdb_comp_04).
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 有了可查询的表之后，我们现在可以执行所需的任何分析任务。例如，我们可以计算 O'Reilly 图书的总收入，如 [示例 3-57](#code_duckdb_comp_04)
    所示。
- en: Example 3-57\. Applying an analytics query
  id: totrans-498
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-57\. 应用分析查询
- en: '[PRE56]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: In case we are not interested in fetching the results but instead storing the
    result of the execution as a DataFrame, we can easily call the `duckdb df()` function
    right after the execution. [Example 3-58](#code_duckdb_comp_05) creates the DataFrame
    `df_total_revenue` that we can continue manipulating in pandas. This shows how
    smooth it is to transition between DuckDB’s SQL interface and pandas.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对获取结果不感兴趣，而是希望将执行结果存储为 DataFrame，我们可以在执行后立即调用 `duckdb df()` 函数。[示例 3-58](#code_duckdb_comp_05)
    创建了 DataFrame `df_total_revenue`，我们可以继续在 pandas 中进行操作。这展示了在 DuckDB 的 SQL 接口和 pandas
    之间如何平滑过渡。
- en: Example 3-58\. Calling the `df()` function
  id: totrans-501
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-58\. 调用 `df()` 函数
- en: '[PRE57]'
  id: totrans-502
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Last but not least, we plot the results by using any available data visualization
    library in Python, as shown in [Example 3-59](#code_duckdb_comp_06).
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，我们可以使用 Python 中任何可用的数据可视化库绘制结果，如 [示例 3-59](#code_duckdb_comp_06) 所示。
- en: Example 3-59\. Data visualization
  id: totrans-504
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-59\. 数据可视化
- en: '[PRE58]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Going back to pandas, it does provide a `pandas.read_sql` command, which allows
    SQL queries to be executed over an existing database connection and then loaded
    into pandas DataFrames. While this approach is suitable for lightweight operations,
    it is not optimized for intensive data processing tasks. Traditional relational
    database management systems (such as Postgres and MySQL), process rows sequentially,
    which results in long execution times and significant CPU overhead. DuckDB, on
    the other hand, was designed specifically for online analytical processing and
    uses a column vectorized approach. This decision allows DuckDB to effectively
    parallelize both disk I/O and query execution, resulting in significant performance
    gains.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 回到 pandas，它提供了 `pandas.read_sql` 命令，允许在现有数据库连接上执行 SQL 查询，然后加载到 pandas 的 DataFrame
    中。虽然这种方法适用于轻量级操作，但并不适用于密集的数据处理任务。传统的关系数据库管理系统（如 Postgres 和 MySQL）按顺序处理行，导致长时间的执行时间和显著的
    CPU 开销。另一方面，DuckDB 是专为在线分析处理而设计的，采用了列矢量化的方法。这个决策使得 DuckDB 能够有效地并行处理磁盘 I/O 和查询执行，从而获得显著的性能提升。
- en: Internally, DuckDB uses the Postgres SQL parser and provides full compatibility
    with SQL functions with Postgres. This uses the SQL functions you are familiar
    with while taking advantage of DuckDB’s efficient column processing. With its
    focus on performance and efficiency, DuckDB is a compelling solution for running
    SQL queries and resource-intensive data processing tasks, especially when compared
    to traditional RDBMSs.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，DuckDB 使用了 Postgres SQL 解析器，并与 Postgres 完全兼容 SQL 函数。这使用了你熟悉的 SQL 函数，同时利用
    DuckDB 的高效列处理。由于其性能和效率的重视，DuckDB 在运行 SQL 查询和处理资源密集型数据处理任务方面是一个引人注目的解决方案，特别是与传统的关系数据库管理系统相比。
- en: Data Manipulation with Polars
  id: totrans-508
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据处理与 Polars
- en: Like DuckDB, Polars also focuses on overcoming the low performance and inefficiency
    of pandas when dealing with large datasets. Polars is a high-performance DataFrame
    library written entirely in Rust, and one of the key advantages is that it does
    not use an index for the DataFrame. Unlike pandas, which relies on an index that
    can often be redundant, Polars eliminates the need for an index, simplifying DataFrame
    operations and making them more intuitive and efficient.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 与 DuckDB 类似，Polars 也专注于克服 pandas 在处理大数据集时的低性能和低效率。Polars 是一个完全用 Rust 编写的高性能
    DataFrame 库，其一个关键优势是不使用 DataFrame 的索引。与经常冗余的索引依赖的 pandas 不同，Polars 消除了索引的需求，简化了
    DataFrame 操作，使其更直观和高效。
- en: In addition, Polars utilizes Apache Arrow arrays for internal data representation.
    This is in contrast to pandas, which uses NumPy arrays (pandas 2.0 might fix that).
    The use of Arrow arrays provides significant benefits in terms of load time, memory
    usage, and computation. Polars leverages this efficient data representation to
    handle large datasets effortlessly and perform computations more efficiently.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Polars 利用 Apache Arrow 数组进行内部数据表示。与使用 NumPy 数组的 pandas 相比（pandas 2.0 可能会修复此问题），使用
    Arrow 数组在加载时间、内存使用和计算方面提供了显著的优势。Polars 利用这种高效的数据表示轻松处理大数据集，并更有效地执行计算。
- en: Another advantage of Polars is its support for parallel operations. Written
    in Rust, a language known for its focus on performance and concurrency, Polars
    can leverage multithreading and run multiple operations in parallel. This enhanced
    parallelization capability allows for faster and more scalable data processing
    tasks. Finally, it also introduced a powerful optimization technique called *lazy
    evaluation*. When executing a query in Polars, the library examines and optimizes
    the query and looks for opportunities to accelerate execution or reduce memory
    usage. This optimization process improves the overall performance of queries and
    enhances the efficiency of data processing. In contrast, pandas supports only
    eager evaluation, where expressions are immediately evaluated as soon as they
    are encountered.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: Polars 的另一个优势是其支持并行操作。使用 Rust 编写，这是一种以性能和并发性为重点的语言，Polars 可以利用多线程并行运行多个操作。这种增强的并行能力允许更快速和可扩展的数据处理任务。最后，它还引入了一种强大的优化技术称为*惰性评估*。在
    Polars 中执行查询时，库会检查和优化查询，并寻找加速执行或减少内存使用的机会。这种优化过程改善了查询的整体性能，并增强了数据处理的效率。相比之下，pandas
    只支持急切评估，即遇到表达式时立即评估。
- en: Data manipulation with Polars is of great value to analytics engineers because
    of its unique capabilities. Polars was designed with a strong focus on performance
    and scalability, making it well suited for efficiently processing large amounts
    of data. Analytics engineers working with large datasets can benefit from its
    memory-efficient operations and parallel processing support, resulting in faster
    data transformations. The integration of Polars with the Rust ecosystem also makes
    it a valuable tool for analysts working with Rust-based data pipelines, providing
    compatibility and ease of use. The query optimization capabilities, advanced data
    manipulation features, and support for multiple data sources make Polars a valuable
    addition to our toolkits, allowing them to tackle complex data tasks with efficiency
    and flexibility.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Polars 进行数据操作对分析工程师非常有价值，因为它具备独特的能力。Polars 的设计强调性能和可伸缩性，非常适合高效处理大量数据。处理大数据集的分析工程师可以从其高效的内存操作和并行处理支持中受益，从而实现更快速的数据转换。Polars
    与 Rust 生态系统的集成还使其成为使用 Rust 构建的数据管道的分析师的宝贵工具，提供了兼容性和易用性。查询优化能力、先进的数据操作功能以及对多数据源的支持使
    Polars 成为我们工具箱中的一个宝贵补充，使其能够高效灵活地处理复杂的数据任务。
- en: Installing Polars
  id: totrans-513
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 Polars
- en: To install Polars, we have several options depending on our operating system
    and the type of installation we want to do, but let’s look at [Example 3-60](#code_polars_00),
    which presents a simple example of how to install Polars by using the pip package
    manager.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 Polars，我们有几个选项，这取决于我们的操作系统和我们想要进行的安装类型，但让我们看看 [示例 3-60](#code_polars_00)，它展示了如何使用
    pip 软件包管理器安装 Polars 的简单示例。
- en: Example 3-60\. Installing Polars
  id: totrans-515
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-60\. 安装 Polars
- en: '[PRE59]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: This will immediately make the Polar library available for us to use within
    our Python context. Let’s test it by executing the code snippet in [Example 3-61](#code_polars_comp_00).
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 这将立即使 Polar 库在我们的 Python 环境中可用。让我们通过执行 [示例 3-61](#code_polars_comp_00) 中的代码片段来测试它。
- en: Example 3-61\. Polars DataFrame
  id: totrans-518
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-61\. Polars DataFrame
- en: '[PRE60]'
  id: totrans-519
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We have a DataFrame with three columns: `Title`, `UnitsSold`, and `Publisher`.
    The `Title` column represents the titles of various O’Reilly books. The `UnitsSold`
    column indicates the number of units sold for each book, and the `Publisher` column
    specifies that O’Reilly publishes all the books.'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个包含三列的 DataFrame：`Title`、`UnitsSold` 和 `Publisher`。`Title` 列表示各种 O'Reilly
    图书的标题。`UnitsSold` 列指示每本书的销售单位数，而 `Publisher` 列指定所有书籍都由 O'Reilly 出版。
- en: Running SQL queries with Polars
  id: totrans-521
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Polars 运行 SQL 查询
- en: Using Polars, we can perform various operations on this DataFrame to gain insights
    into O’Reilly’s book sales. Whether it’s calculating total revenue, analyzing
    sales by book title or author, or identifying the top-selling books, as shown
    in [Example 3-62](#code_polars_comp_01), Polars provides a versatile and efficient
    platform for data analysis.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Polars，我们可以对这个 DataFrame 执行各种操作，以深入了解 O'Reilly 的图书销售。无论是计算总收入，分析按书名或作者销售，还是识别畅销书籍，如
    [示例 3-62](#code_polars_comp_01) 所示，Polars 提供了一个多功能和高效的数据分析平台。
- en: Example 3-62\. Polars DataFrame—top-selling books
  id: totrans-523
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-62\. Polars DataFrame—畅销书籍
- en: '[PRE61]'
  id: totrans-524
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: As you can see, we sort the DataFrame `df` based on the `UnitsSold` column in
    descending order by using the `sort` method. Then, we select the top five books
    using the `limit` method. Finally, we convert the resulting DataFrame to a pandas
    DataFrame by using `to_pandas()` for easier printing and display.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们使用 `sort` 方法根据 `UnitsSold` 列按降序对 DataFrame `df` 进行排序。然后，我们使用 `limit`
    方法选择前五本书。最后，我们使用 `to_pandas()` 将结果 DataFrame 转换为 pandas DataFrame，以便更轻松地打印和显示。
- en: Although this is interesting and shows the similarity to pandas in terms of
    syntax, we did mention the capability of Polars to expose its functionalities
    as SQL. In reality, Polars offers multiple approaches for utilizing SQL capabilities
    within its framework.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这很有趣，并展示了与 pandas 语法的相似性，但我们提到了 Polars 具有将其功能作为 SQL 公开的能力。事实上，Polars 提供了多种在其框架内利用
    SQL 功能的方法。
- en: Just like pandas, Polars seamlessly integrates with external libraries such
    as DuckDB, allowing you to leverage their SQL functionalities. You can import
    data into Polars from DuckDB or pandas, perform SQL queries on the imported data,
    and seamlessly combine SQL operations with Polars DataFrame operations. This integration
    provides a comprehensive data analysis and manipulation ecosystem, offering the
    best of both SQL and Polars.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 pandas 一样，Polars 无缝集成了外部库（如 DuckDB），允许您利用它们的 SQL 功能。您可以从 DuckDB 或 pandas
    导入数据到 Polars，对导入的数据执行 SQL 查询，并无缝结合 SQL 操作与 Polars DataFrame 操作。这种集成提供了全面的数据分析和操作生态系统，融合了
    SQL 和 Polars 的优势。
- en: In [Example 3-63](#code_polars_comp_002), we create a DuckDB connection by using
    `duckdb.connect()`. Then, we create a Polars DataFrame `df` with columns for `Title`,
    `Author`, `Publisher`, `Price`, and `UnitsSold`, representing the O’Reilly books
    data. We register this DataFrame as a table named `books` in DuckDB by using `con.register()`.
    Next, we execute a SQL query on the `books` table by using `con.execute()`, selecting
    the `Title` and `UnitsSold` columns and filtering by `Publisher = "O'Reilly"`.
    The result is returned as a list of tuples. We convert the result to a Polars
    DataFrame `result_df` with specified column names.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [Example 3-63](#code_polars_comp_002) 中，我们通过 `duckdb.connect()` 创建了一个 DuckDB
    连接。然后，我们创建了一个 Polars DataFrame `df`，包含 `Title`、`Author`、`Publisher`、`Price` 和
    `UnitsSold` 列，表示 O’Reilly 图书的数据。我们使用 `con.register()` 将这个 DataFrame 注册为名为 `books`
    的表。接下来，我们使用 `con.execute()` 在 `books` 表上执行 SQL 查询，选择 `Title` 和 `UnitsSold` 列，并按
    `Publisher = "O'Reilly"` 进行筛选。结果以元组列表形式返回。我们将结果转换为一个指定列名的 Polars DataFrame `result_df`。
- en: Example 3-63\. Polars DataFrame with DuckDB
  id: totrans-529
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 3-63\. 使用 DuckDB 的 Polars DataFrame
- en: '[PRE62]'
  id: totrans-530
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Polars also provides native support for executing SQL queries without relying
    on external libraries. With Polars, you can write SQL queries directly within
    your code, leveraging the SQL syntax to perform data transformations, aggregations,
    and filtering operations. This allows you to harness the power of SQL within the
    Polars framework, thus providing a convenient and efficient approach to working
    with structured data.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: Polars 还提供了原生支持，可以在不依赖外部库的情况下执行 SQL 查询。使用 Polars，您可以直接在代码中编写 SQL 查询，利用 SQL 语法进行数据转换、聚合和过滤操作。这使您可以在
    Polars 框架内充分利用 SQL 的强大功能，从而提供了处理结构化数据的便捷高效方法。
- en: Using SQL in Polars is a simple and straightforward process. You can follow
    these steps to perform SQL operations on a Polars DataFrame. First, create a SQL
    context that sets up the environment for executing SQL queries. This context enables
    you to work with SQL seamlessly within the Polars framework, as shown in [Example 3-64](#code_polars_comp_02).
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Polars 中使用 SQL 是一个简单而直接的过程。您可以按照以下步骤在 Polars DataFrame 上执行 SQL 操作。首先，创建一个
    SQL 上下文，用于设置执行 SQL 查询的环境。这个上下文使您能够在 Polars 框架内无缝使用 SQL，如在 [Example 3-64](#code_polars_comp_02)
    中所示。
- en: Example 3-64\. Create a SQL context
  id: totrans-533
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 3-64\. 创建 SQL 上下文
- en: '[PRE63]'
  id: totrans-534
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[Example 3-65](#code_polars_comp_03) demonstrates the next step: registering
    the DataFrame you want to query.'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '[Example 3-65](#code_polars_comp_03) 展示了下一步操作：注册您要查询的 DataFrame。'
- en: Example 3-65\. Register the DataFrame
  id: totrans-536
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 3-65\. 注册 DataFrame
- en: '[PRE64]'
  id: totrans-537
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: By providing a name for the DataFrame, you establish a reference point for your
    SQL queries. This registration step ensures that the DataFrame is associated with
    a recognizable identifier.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 为 DataFrame 提供一个名称，可以为您的 SQL 查询建立一个参考点。此注册步骤确保 DataFrame 与一个可识别的标识符关联起来。
- en: Once the DataFrame is registered, you can execute SQL queries on it by using
    the `query()` function provided by Polars. This function takes the SQL query as
    input and returns a Polars DataFrame as the result. This DataFrame contains the
    data that matches the criteria specified in the SQL query. Let’s take a look at
    [Example 3-66](#code_polars_comp_04).
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦DataFrame被注册，您可以使用Polars提供的`query()`函数在其上执行SQL查询。该函数将SQL查询作为输入，并返回一个Polars
    DataFrame作为结果。这个DataFrame包含符合SQL查询指定条件的数据。让我们看一下[示例 3-66](#code_polars_comp_04)。
- en: Example 3-66\. Run analytics queries
  id: totrans-540
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-66\. 运行分析查询
- en: '[PRE65]'
  id: totrans-541
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: By integrating SQL with Polars, data professionals with deep SQL knowledge can
    easily leverage the power and efficiency of Polars. They can leverage their existing
    SQL skills and apply them directly to their data analysis and manipulation tasks
    within the Polars framework. This seamless integration allows users to take advantage
    of the library’s optimized query execution engine while using the familiar SQL
    syntax they are used to.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将SQL与Polars集成，具有深厚SQL知识的数据专业人士可以轻松利用Polars的强大和高效性。他们可以利用现有的SQL技能直接应用于Polars框架中的数据分析和操作任务。这种无缝集成允许用户在使用他们熟悉的SQL语法的同时，利用该库优化的查询执行引擎。
- en: Data Manipulation with FugueSQL
  id: totrans-543
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用FugueSQL进行数据操作
- en: Fugue is a powerful unified interface for distributed computing that allows
    users to seamlessly run Python, pandas, and SQL code on popular distributed frameworks
    like Spark, Dask, and Ray. With Fugue, users can realize the full potential of
    these distributed systems with minimal code changes.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: Fugue是一个强大的统一接口，用于分布式计算，允许用户在像Spark、Dask和Ray这样的流行分布式框架上无缝运行Python、pandas和SQL代码。借助Fugue，用户可以以最小的代码更改实现这些分布系统的全部潜力。
- en: The main use cases for Fugue revolve around parallelizing and scaling existing
    Python and pandas code to run effortlessly across distributed frameworks. By seamlessly
    transitioning code to Spark, Dask, or Ray, users can take advantage of these systems’
    scalability and performance benefits without having to rewrite extensive code.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: Fugue的主要用例围绕将现有的Python和pandas代码并行化和扩展到跨分布式框架轻松运行展开。通过无缝过渡到Spark、Dask或Ray，用户可以享受这些系统的可扩展性和性能优势，而无需重写大量代码。
- en: Relevant to our discussion is the fact that Fugue provides a unique feature
    called FugueSQL that allows users to define end-to-end workflows on pandas, Spark,
    and Dask DataFrames through an advanced SQL interface. It combines familiar SQL
    syntax with the ability to call Python code. This gives users a powerful tool
    to streamline and automate their data processing workflows.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 关于我们讨论的相关内容是，Fugue提供了一个称为FugueSQL的独特功能，允许用户通过高级SQL接口在pandas、Spark和Dask DataFrames上定义端到端工作流程。它结合了熟悉的SQL语法和调用Python代码的能力。这为用户提供了一个强大的工具，用于简化和自动化他们的数据处理工作流程。
- en: FugueSQL offers a variety of benefits that can be leveraged in multiple scenarios,
    including parallel code execution as part of the overall goals of the Fugue project
    or standalone querying on a single machine. Whether we are working with distributed
    systems or performing data analysis on a local machine, it allows us to efficiently
    query our DataFrames.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: FugueSQL提供了多种优势，在多种场景中可以利用，包括作为Fugue项目整体目标的并行代码执行或在单机上进行独立查询。无论是在分布式系统上工作还是在本地机器上进行数据分析，它都允许我们高效地查询我们的DataFrames。
- en: Installing Fugue and FugueSQL
  id: totrans-548
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装Fugue和FugueSQL
- en: We have several options to install Fugue, depending on our operating system
    and type of installation. [Example 3-67](#code_fugue_00) uses `pip install`.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有几种安装Fugue的选项，取决于我们的操作系统和安装类型。[示例 3-67](#code_fugue_00)使用`pip install`。
- en: Example 3-67\. Install Fugue
  id: totrans-550
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-67\. 安装Fugue
- en: '[PRE66]'
  id: totrans-551
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Fugue offers various installation extras that enhance its functionality and
    support different execution engines and data processing libraries. These installation
    extras include the following:'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: Fugue提供了各种安装额外功能，增强其功能并支持不同的执行引擎和数据处理库。这些安装额外功能包括以下内容：
- en: '`sql`'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: '`sql`'
- en: This extra enables FugueSQL support. While the non-SQL functionalities of Fugue
    still work without this extra, installing it is necessary if you intend to use
    FugueSQL. To achieve that, execute the code snippet in [Example 3-68](#code_fugue_01).
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 此附加功能支持FugueSQL。尽管Fugue的非SQL功能仍可在没有此附加功能的情况下使用，但如果您打算使用FugueSQL，则安装它是必要的。要实现这一点，请执行[示例 3-68](#code_fugue_01)中的代码片段。
- en: Example 3-68\. Install FugueSQL
  id: totrans-555
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-68\. 安装FugueSQL
- en: '[PRE67]'
  id: totrans-556
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '`spark`'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark`'
- en: Installing this extra adds support for Spark as the ExecutionEngine in Fugue.
    With this extra, users can leverage the capabilities of Spark to execute their
    Fugue workflows. To add this extra, run the code in [Example 3-69](#code_fugue_02).
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 安装此额外功能将 Spark 作为 Fugue 中的 ExecutionEngine。使用此额外功能，用户可以利用 Spark 的能力来执行其 Fugue
    工作流。要添加此额外功能，请运行 [Example 3-69](#code_fugue_02) 中的代码。
- en: Example 3-69\. Install FugueSpark
  id: totrans-559
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 3-69\. 安装 FugueSpark
- en: '[PRE68]'
  id: totrans-560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '`dask`'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '`dask`'
- en: This extra enables support for Dask as the ExecutionEngine in Fugue. By installing
    this extra, users can take advantage of Dask’s distributed computing capabilities
    within the Fugue framework.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 此额外功能启用了对 Dask 作为 Fugue 中的 ExecutionEngine 的支持。通过安装此额外功能，用户可以在 Fugue 框架内利用 Dask
    的分布式计算能力。
- en: '`ray`'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '`ray`'
- en: Installing this extra adds support for Ray as the ExecutionEngine in Fugue.
    With this extra, users can leverage Ray’s efficient task scheduling and parallel
    execution capabilities in their Fugue workflows.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 安装此额外功能将 Ray 作为 Fugue 中的 ExecutionEngine。使用此额外功能，用户可以利用 Ray 的高效任务调度和并行执行能力来执行其
    Fugue 工作流。
- en: '`duckdb`'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '`duckdb`'
- en: This extra enables support for DuckDB as the ExecutionEngine in Fugue. By installing
    this extra, users can utilize DuckDB’s blazing fast in-memory database for efficient
    query execution within the Fugue framework.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 此额外功能启用了对 DuckDB 作为 Fugue 中的 ExecutionEngine 的支持。通过安装此额外功能，用户可以在 Fugue 框架内使用
    DuckDB 的高速内存数据库进行高效的查询执行。
- en: '`polars`'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: '`polars`'
- en: Installing this extra provides support for Polars DataFrames and extensions
    using the Polars library. With this extra, users can leverage the features and
    functionalities of Polars for data processing within Fugue.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 安装此额外功能提供了对 Polars DataFrames 和使用 Polars 库的扩展的支持。使用此额外功能，用户可以在 Fugue 中进行数据处理时利用
    Polars 的功能和功能。
- en: '`ibis`'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '`ibis`'
- en: Enabling this extra allows users to integrate Ibis into Fugue workflows. Ibis
    provides an expressive and powerful interface for working with SQL-like queries,
    and by installing this extra, users can incorporate Ibis functionality into their
    Fugue workflows.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 启用此额外功能允许用户将 Ibis 集成到 Fugue 工作流中。Ibis 提供了一个表达丰富且功能强大的界面，用于处理类似 SQL 的查询，通过安装此额外功能，用户可以将
    Ibis 功能整合到其 Fugue 工作流中。
- en: '`cpp_sql_parser`'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '`cpp_sql_parser`'
- en: Enabling this extra utilizes the CPP (C++) antlr parser for Fugue SQL, which
    offers significantly faster parsing compared to the pure Python parser. While
    prebuilt binaries are available for the main Python versions and platforms, this
    extra may require a C++ compiler to build on-the-fly for other platforms.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 启用此额外功能使用 CPP（C++）antlr 解析器用于 Fugue SQL，与纯 Python 解析器相比，解析速度显著提高。虽然主要 Python
    版本和平台提供了预编译的二进制文件，但此额外功能可能需要在其他平台上即时构建 C++ 编译器。
- en: We can actually install several of the previous extras in a single `pip install`
    command. In [Example 3-70](#code_fugue_03), we install `duckdb`, `polars`, and
    `spark` extras with Fugue in a single command.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以在单个 `pip install` 命令中安装几个之前的额外功能。在 [Example 3-70](#code_fugue_03) 中，我们使用
    Fugue 一次性安装了 `duckdb`、`polars` 和 `spark` 的额外功能。
- en: Example 3-70\. Install multiple Fugue extras
  id: totrans-574
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 3-70\. 安装多个 Fugue 额外功能
- en: '[PRE69]'
  id: totrans-575
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Another interesting extra relates to notebooks. FugueSQL has a notebook extension
    for both Jupyter Notebooks and JupyterLab. This extension provides syntax highlighting.
    We can run another `pip install` to install the extension ([Example 3-71](#code_fugue_04)).
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的额外功能与笔记本有关。FugueSQL 在 Jupyter Notebooks 和 JupyterLab 中都有一个笔记本扩展。此扩展提供语法高亮。我们可以运行另一个
    `pip install` 命令来安装扩展 ([Example 3-71](#code_fugue_04)).
- en: Example 3-71\. Install the notebook extension
  id: totrans-577
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 3-71\. 安装笔记本扩展
- en: '[PRE70]'
  id: totrans-578
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: The second command, `fugue-jupyter install startup`, registers Fugue in the
    startup script of Jupyter so that it is available for you whenever you open Jupyter
    Notebooks or JupyterLab.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个命令 `fugue-jupyter install startup` 将 Fugue 注册到 Jupyter 的启动脚本中，以便您在每次打开 Jupyter
    Notebooks 或 JupyterLab 时都可以使用它。
- en: If you have installed Fugue and use JupyterLab, the `%%fsql` cell magic is automatically
    registered by default. This means you can use cell magic directly in your JupyterLab
    environment without any additional steps. However, if you are using Classic Jupyter
    Notebooks or the `%%fsql` cell magic is not registered, you can enable it by using
    the command in [Example 3-72](#code_fugue_005) in your notebook.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已安装 Fugue 并使用 JupyterLab，默认情况下会自动注册 `%%fsql` 单元格魔术。这意味着您可以在 JupyterLab 环境中直接使用单元格魔术，无需任何额外步骤。但是，如果您使用的是
    Classic Jupyter Notebooks 或者 `%%fsql` 单元格魔术未注册，则可以通过在笔记本中使用 [Example 3-72](#code_fugue_005)
    中的命令来启用它。
- en: Example 3-72\. Enable notebooks extensions
  id: totrans-581
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 3-72\. 启用笔记本扩展
- en: '[PRE71]'
  id: totrans-582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Running SQL queries with FugueSQL
  id: totrans-583
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 FugueSQL 运行 SQL 查询
- en: FugueSQL is designed specifically for SQL users who want to work with Python
    DataFrames such as pandas, Spark, and Dask. FugueSQL provides a SQL interface
    that parses and runs on the underlying engine of your choice. This is especially
    beneficial for data scientists and analysts who prefer to focus on defining logic
    and data transformations rather than dealing with execution complexity.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: FugueSQL专为希望使用Python数据框架（如pandas、Spark和Dask）的SQL用户设计。FugueSQL提供一个SQL接口，可以解析和运行在您选择的底层引擎上。这对于那些更喜欢专注于定义逻辑和数据转换而不是处理执行复杂性的数据科学家和分析师尤为有益。
- en: But it is also tailored to the needs of SQL enthusiasts, giving them the ability
    to define end-to-end workflows with SQL across popular data processing engines
    such as pandas, Spark, and Dask. This way, SQL enthusiasts can leverage their
    SQL skills and easily orchestrate complex data pipelines without switching between
    different tools or languages.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 但它也为SQL爱好者量身定制，使他们能够在流行的数据处理引擎（如pandas、Spark和Dask）上定义端到端工作流程。这样，SQL爱好者可以利用他们的SQL技能，轻松编排复杂的数据流水线，而无需在不同工具或语言之间切换。
- en: Fugue offers a practical solution for data scientists who work primarily with
    pandas and want to leverage the capabilities of Spark or Dask to process large
    datasets. Using Fugue, they can effortlessly scale their pandas code and seamlessly
    transition to Spark or Dask, realizing the potential of distributed computing
    with minimal effort. For example, if someone uses FugueSQL with Spark, the framework
    will use SparkSQL and PySpark to execute the queries. Even though FugueSQL supports
    nonstandard SQL commands, it is important to emphasize that Fugue remains fully
    compatible with standard SQL syntax. This compatibility ensures that SQL users
    can seamlessly switch to Fugue and leverage their existing SQL knowledge and skills
    without major customizations or complications.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: Fugue为主要使用pandas并希望利用Spark或Dask处理大数据集的数据科学家提供了实用的解决方案。使用Fugue，他们可以轻松地扩展其pandas代码，并顺利过渡到Spark或Dask，实现分布式计算的潜力，几乎不费力气。例如，如果有人在Spark中使用FugueSQL，框架将使用SparkSQL和PySpark来执行查询。尽管FugueSQL支持非标准SQL命令，但重要的是强调Fugue仍然完全兼容标准SQL语法。这种兼容性确保了SQL用户可以无缝地切换到Fugue并利用他们现有的SQL知识和技能，而无需进行重大的定制或复杂化。
- en: Finally, Fugue is proving to be a valuable asset for data teams working on Big
    Data projects that often face code maintenance issues. By adopting Fugue, these
    teams can benefit from a unified interface that simplifies the execution of code
    across distributed computing platforms, ensuring consistency, efficiency, and
    maintainability throughout the development process.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Fugue正在证明是处理大数据项目时非常有价值的资产，这些项目通常面临代码维护问题。通过采用Fugue，这些团队可以从一个统一的界面中受益，简化跨分布式计算平台执行代码的过程，确保开发过程中的一致性、效率和可维护性。
- en: '[Example 3-73](#code_fugue_05) shows an end-to-end example using FugueSQL.'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 3-73](#code_fugue_05)展示了一个使用FugueSQL的端到端示例。'
- en: Example 3-73\. FugueSQL full example
  id: totrans-589
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-73. FugueSQL完整示例
- en: '[PRE72]'
  id: totrans-590
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: This example creates a `FugueSQLWorkflow` instance. We register the pandas DataFrame
    `df` as a table by using the `workflow.df()` method. Then, we write SQL queries
    within the `workflow.run()` method to perform various operations on the data.
    This `FugueSQLWorkflow` is a class provided by the Fugue library that serves as
    the entry point for executing FugueSQL code. It allows us to define and execute
    SQL queries on various data sources, as mentioned before, without the need for
    explicit data transformations or handling the underlying execution engines.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例创建了一个`FugueSQLWorkflow`实例。我们使用`workflow.df()`方法将pandas DataFrame `df`注册为表格。然后，我们在`workflow.run()`方法中编写SQL查询来执行数据的各种操作。这个`FugueSQLWorkflow`是Fugue库提供的一个类，作为执行FugueSQL代码的入口点。它允许我们在各种数据源上定义和执行SQL查询，如前所述，无需显式数据转换或处理底层执行引擎。
- en: 'The example demonstrates three queries:'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 该示例演示了三个查询：
- en: Calculating the total revenue for O’Reilly books
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算O'Reilly书籍的总收入
- en: Calculating the average price of O’Reilly books
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算O’Reilly书籍的平均价格
- en: Retrieving the top-selling O’Reilly books
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索销量最高的O'Reilly书籍
- en: The results are stored in the `result` object, and we can access the data by
    using the `first()` and `collect()` methods.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 结果存储在`result`对象中，我们可以使用`first()`和`collect()`方法访问数据。
- en: Finally, we print the results to the console. Note that we use two single quotes
    ('') to escape the single quote within the SQL queries for the Publisher name
    `"O'Reilly"` to ensure proper syntax.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将结果打印到控制台。请注意，在SQL查询中，为了正确的语法，我们使用两个单引号（''）来转义出版者名称为`"O'Reilly"`的单引号。
- en: One might wonder if FugueSQL is an alternative to or an evolution of pandas,
    which has pandasql. We would argue that while pandasql supports only SQLite as
    a backend, FugueSQL supports multiple local backends, such as pandas, DuckDB,
    Spark, and SQLite. When using FugueSQL with the pandas backend, SQL queries are
    directly translated into pandas operations, eliminating the need for data transfer.
    Similarly, DuckDB has excellent pandas support, resulting in minimal overhead
    for data transfer. Therefore, both pandas and DuckDB are recommended backends
    for local data processing in FugueSQL. All in all, FugueSQL is a great framework
    to take advantage of SQL syntax, with added capabilities for distributed processing
    and data manipulation at scale.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 人们可能会想知道FugueSQL是否是pandas的替代品或进化版本，而pandasql有pandas支持。我们认为，虽然pandasql仅支持SQLite作为后端，但FugueSQL支持多个本地后端，如pandas、DuckDB、Spark和SQLite。在使用FugueSQL与pandas后端时，SQL查询直接转换为pandas操作，消除了数据传输的需求。同样，DuckDB对pandas有很好的支持，减少了数据传输的开销。因此，pandas和DuckDB都是FugueSQL本地数据处理的推荐后端。总而言之，FugueSQL是一个利用SQL语法的优秀框架，具有分布式处理和大规模数据操作的增强能力。
- en: In general, Fugue, DuckDB, and pandas are powerful tools that offer efficient
    data processing capabilities. However, regardless of the technology used, it is
    crucial to recognize that proper data modeling is fundamental for successful scalability.
    Without a well-designed data model, any system will struggle to handle large-scale
    data processing efficiently.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，Fugue、DuckDB和pandas是提供高效数据处理能力的强大工具。然而，无论使用何种技术，识别出正确的数据建模对于成功的可扩展性至关重要。没有设计良好的数据模型，任何系统都将难以有效处理大规模数据处理。
- en: The foundation of a robust data model ensures that data is structured, organized,
    and optimized for analysis and manipulation. By understanding the relationships
    between data entities, defining appropriate data types, and establishing efficient
    indexing strategies, we can create a scalable architecture that maximizes performance
    and enables seamless data operations across platforms and tools. Therefore, while
    Fugue, DuckDB, and pandas contribute to efficient data processing, the importance
    of proper data modeling cannot be overstated for achieving scalability. That is
    also one of the main reasons we covered data modeling in [Chapter 2](ch02.html#chapter_id_02).
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 健壮数据模型的基础确保数据被结构化、组织化并优化以供分析和操作。通过理解数据实体之间的关系、定义适当的数据类型和建立高效的索引策略，我们可以创建一个可扩展的架构，最大化性能并实现跨平台和工具的无缝数据操作。因此，尽管Fugue、DuckDB和pandas为高效的数据处理做出了贡献，但对于实现可扩展性来说，正确的数据建模的重要性不可言喻。这也是我们在[第2章](ch02.html#chapter_id_02)中涵盖数据建模的主要原因之一。
- en: 'Bonus: Training Machine Learning Models with SQL'
  id: totrans-601
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励：使用SQL训练机器学习模型
- en: This header might make you feel that we are pushing the limits with SQL-like
    capabilities, but the reality is that thanks to a very specific library, *dask-sql*,
    it is possible to use the Python machine learning ecosystem in SQL.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 这个标题可能会让你觉得我们在推动类似SQL的能力的极限，但事实是，多亏了一个非常特定的库*dask-sql*，我们可以在SQL中使用Python机器学习生态系统。
- en: Dask-sql is a recently developed SQL query engine, in the experimental phase,
    that builds upon the Python-based Dask distributed library. It offers the unique
    capability to seamlessly integrate Python and SQL, which gives users the ability
    to perform distributed and scalable computations. This innovative library opens
    up opportunities to leverage the strengths of both Python and SQL for data analysis
    and processing.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: Dask-sql是一个最近开发的SQL查询引擎，在实验阶段，它建立在基于Python的分布式库Dask之上。它提供了将Python和SQL无缝集成的独特能力，使用户能够执行分布式和可扩展的计算。这一创新库打开了利用Python和SQL的优势进行数据分析和处理的机会。
- en: We can run a `pip install` to install the extension, as shown in [Example 3-74](#code_dask-sql_01).
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以运行`pip install`来安装扩展，如[示例 3-74](#code_dask-sql_01)所示。
- en: Example 3-74\. Install dask-sql
  id: totrans-605
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-74\. 安装dask-sql
- en: '[PRE73]'
  id: totrans-606
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: In [Example 3-75](#code_dask-sql_02), we are creating an instance of a `Context`
    class with the line ​`c = Context()`. With it, we are initializing a new execution
    context for SQL queries. This context can be used to execute SQL queries against
    our data and perform operations like filtering, aggregating, and joining, but
    it can also apply a special type of command provided by Dask to train and test
    machine learning models.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 3-75](#code_dask-sql_02) 中，我们使用 `c = Context()` 这行代码创建了一个 `Context` 类的实例。通过这个实例，我们初始化了一个新的
    SQL 查询执行上下文。这个上下文可以用来执行 SQL 查询，对数据进行过滤、聚合和连接等操作，还可以应用 Dask 提供的特殊命令来训练和测试机器学习模型。
- en: Example 3-75\. Import the context from dask_sql
  id: totrans-608
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-75\. 从 dask_sql 导入上下文
- en: '[PRE74]'
  id: totrans-609
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: We now have all the tools to load a dataset to work with. In [Example 3-76](#code_dask-sql_03),
    we use the `read_csv()` function from Dask and employ it to read [the Iris dataset](https://oreil.ly/vt4-s).
    Once the data is loaded, we can access and manipulate the data as a Dask DataFrame.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了加载数据集并进行操作的所有工具。在 [示例 3-76](#code_dask-sql_03) 中，我们使用 Dask 的 `read_csv()`
    函数来读取 [Iris 数据集](https://oreil.ly/vt4-s)。一旦数据加载完成，我们可以将其视为 Dask DataFrame 并进行访问和操作。
- en: The next step is registering the loaded Dask DataFrame (`df`) as a table named
    `iris` in the *dask-sql* `Context`. The `create_table` method of the `Context`
    class is used to register the table. Once this step is completed, we are able
    to query the data by using SQL syntax.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤是将加载的 Dask DataFrame (`df`) 注册为 *dask-sql* `Context` 中名为 `iris` 的表。使用
    `Context` 类的 `create_table` 方法来注册表。完成此步骤后，我们可以使用 SQL 语法查询数据。
- en: Example 3-76\. Load the data as a Dask DataFrame and register it as a table
  id: totrans-612
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-76\. 将数据加载为 Dask DataFrame 并注册为表
- en: '[PRE75]'
  id: totrans-613
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Let’s run a simple select using the `sql()` function of our *dask-sql* `Context`
    object, in [Example 3-77](#code_dask-sql_003), and write our SQL query as a parameter.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行一个简单的选择，使用我们的 *dask-sql* `Context` 对象的 `sql()` 函数，在 [示例 3-77](#code_dask-sql-003)
    中，并将我们的 SQL 查询写为一个参数。
- en: Example 3-77\. Access the dask-sql table
  id: totrans-615
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-77\. 访问 dask-sql 表
- en: '[PRE76]'
  id: totrans-616
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: With the data ready, we can now use the training components to train a machine
    learning model. For that, we start by using the `CREATE OR REPLACE MODEL` statement,
    which is a *dask-sql* extension that allows you to define and train machine learning
    models within a SQL context.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备好后，我们可以使用训练组件来训练机器学习模型。为此，我们首先使用 `CREATE OR REPLACE MODEL` 语句，这是 *dask-sql*
    的扩展功能，允许在 SQL 上下文中定义和训练机器学习模型。
- en: In this case, the clustering model is named `clustering`, and the model is created
    using the KMeans algorithm from the *scikit-learn* library, which is a popular
    unsupervised learning algorithm for clustering data points. Interestingly enough,
    *dask-sql* allows us to use model classes from third-party libraries such as *scikit-learn*.
    The `n_clusters` parameter is set to 3, indicating that the algorithm should identify
    three clusters in the data.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，聚类模型被命名为 `clustering`，并且使用了 *scikit-learn* 库中的 KMeans 算法创建了这个模型，这是一种流行的无监督学习算法，用于对数据点进行聚类。有趣的是，*dask-sql*
    允许我们使用来自第三方库（如 *scikit-learn*）的模型类。`n_clusters` 参数设置为 3，表示算法应该在数据中识别三个聚类。
- en: In [Example 3-78](#code_dask-sql_04), we show that the training data for the
    model is obtained from the `iris` table registered in the `c` context. The `SELECT`
    statement specifies the features used for training, which include the `sepallength`,
    `sepalwidth`, `petallength`, and `petalwidth` columns from the `iris` table.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 3-78](#code_dask-sql_04) 中，我们展示了用于模型训练的训练数据来自于在 `c` 上下文中注册的 `iris` 表。`SELECT`
    语句指定了用于训练的特征，包括 `iris` 表中的 `sepallength`、`sepalwidth`、`petallength` 和 `petalwidth`
    列。
- en: Example 3-78\. Create our clustering model
  id: totrans-620
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-78\. 创建我们的聚类模型
- en: '[PRE77]'
  id: totrans-621
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: We can now validate that our model was actually created by running a `SHOW MODELS`
    command ([Example 3-79](#code_dask-sql_05)), which resembles the often-used `SHOW
    TABLES` from traditional SQL engines. While the latter shows all tables in a certain
    schema of a database, the former lists all models created and available to be
    used in a *dask-sql* context.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过运行 `SHOW MODELS` 命令（[示例 3-79](#code_dask-sql_05)）来验证我们的模型是否真的创建成功，这类似于传统
    SQL 引擎中常用的 `SHOW TABLES`。后者显示特定数据库架构中所有表格，而前者则列出了在 *dask-sql* 上下文中创建的所有可用模型。
- en: Example 3-79\. Show the list of models
  id: totrans-623
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-79\. 显示模型列表
- en: '[PRE78]'
  id: totrans-624
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Another interesting command is `DESCRIBE MODEL *MODEL_NAME*` ([Example 3-80](#code_dask-sql_06)),
    which shows all the hyperparameters used to train this model.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的命令是 `DESCRIBE MODEL *MODEL_NAME*`（[示例 3-80](#code_dask-sql_06)），它展示了用于训练该模型的所有超参数。
- en: Example 3-80\. Get all hyperparameters of a certain model
  id: totrans-626
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-80\. 获取某个模型的所有超参数
- en: '[PRE79]'
  id: totrans-627
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: In [Example 3-81](#code_dask-sql_07), we demonstrate one of the most captivating
    commands within *dask-sql*—the `PREDICT` command. It uses the recently created
    clustering model to predict the cluster classes for the rows of the `df` DataFrame.
    The `SELECT` statement with `PREDICT` applies trained machine learning models
    to new data points from a certain table, within a SQL context.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 3-81](#code_dask-sql_07) 中，我们展示了 *dask-sql* 中最引人注目的命令之一——`PREDICT` 命令。它使用最近创建的聚类模型来预测
    `df` DataFrame 的行的集群类。`PREDICT` 中的 `SELECT` 语句在 SQL 上下文中将训练好的机器学习模型应用于来自某个表的新数据点。
- en: In this case, the `PREDICT` command is used to apply the `clustering` model
    to the first 100 rows of the `iris` table. The `MODEL` clause specifies the name
    of the model to be used, which is `clustering`. The `SELECT` statement within
    the `PREDICT` command specifies the features to be used for prediction, which
    are the same features used during the model training step, as demonstrated in
    [Example 3-81](#code_dask-sql_07).
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`PREDICT` 命令用于将 `clustering` 模型应用于 `iris` 表的前 100 行数据。`MODEL` 子句指定要使用的模型名称为
    `clustering`。`PREDICT` 命令中的 `SELECT` 语句指定用于预测的特征，这些特征与模型训练步骤中使用的相同特征相同，正如 [示例 3-81](#code_dask-sql_07)
    中演示的那样。
- en: Example 3-81\. Make predictions
  id: totrans-630
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-81\. 进行预测
- en: '[PRE80]'
  id: totrans-631
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Another interesting capability of *dask-sql* is its experiments component. It
    runs an experiment to attempt different hyperparameter values for the clustering
    model by using the `CREATE EXPERIMENT` statement.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '*dask-sql* 的另一个有趣的能力是其实验组件。它通过使用 `CREATE EXPERIMENT` 语句对聚类模型尝试不同的超参数值来运行实验。'
- en: In [Example 3-82](#code_dask-sql_08), the experiment is named `first_experiment`.
    It uses the `GridSearchCV` class from *scikit-learn*, which is a popular technique
    for hyperparameter tuning. The hyperparameter being tuned in this case is the
    number of clusters (`n_clusters`), and this is only for showing the capability.
    The `tune_parameters` parameter specifies the range of values to try for the `n_clusters`
    hyperparameter. In this example, the experiment will try three values (2, 3, and
    4), meaning the number of clusters we expect to obtain.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 3-82](#code_dask-sql_08) 中，实验被命名为 `first_experiment`。它使用来自 *scikit-learn*
    的 `GridSearchCV` 类，这是一种流行的超参数调优技术。在这种情况下，正在调整的超参数是聚类数 (`n_clusters`)，这仅仅是展示其能力。`tune_parameters`
    参数指定尝试 `n_clusters` 超参数的值范围。在此示例中，实验将尝试三个值 (2, 3 和 4)，这意味着我们期望获得的聚类数。
- en: In a real-world scenario of a machine learning project, we should focus on selecting
    the most relevant hyperparameters of our model. This depends on the problem being
    a classification or regression task and the types of algorithms used.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习项目的实际场景中，我们应该专注于选择模型的最相关超参数。这取决于问题是否是分类或回归任务，以及所使用的算法类型。
- en: Example 3-82\. Hyperparameter tuning
  id: totrans-635
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-82\. 超参数调优
- en: '[PRE81]'
  id: totrans-636
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Last but not least, we have an `EXPORT MODEL` statement, as seen in [Example 3-83](#code_dask-sql_09).
    In this case, the model is exported in the pickle format by using the format parameter
    set to `pickle`. Pickle is a Python-specific binary serialization format that
    allows you to save and load Python objects.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有一个 `EXPORT MODEL` 语句，如在 [示例 3-83](#code_dask-sql_09) 中所见。在这种情况下，模型以 pickle
    格式导出，使用的格式参数设置为 `pickle`。Pickle 是一种 Python 特定的二进制序列化格式，允许您保存和加载 Python 对象。
- en: The `location` parameter specifies the path and filename where the exported
    model file should be saved. In this example, the model is saved in the current
    directory with the filename *clustering.pkl*.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: '`location` 参数指定应保存导出模型文件的路径和文件名。在此示例中，模型保存在当前目录，并命名为 *clustering.pkl*。'
- en: Example 3-83\. Export the model as a pickle file
  id: totrans-639
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-83\. 将模型导出为 pickle 文件
- en: '[PRE82]'
  id: totrans-640
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Overall, *dask-sql* is a powerful and promising tool for machine learning purposes,
    offering a SQL interface for data manipulation and machine learning operations
    on large datasets. With *dask-sql*, we can leverage the familiar SQL syntax to
    query and transform data, as well as train and evaluate machine learning models
    by using popular libraries like *scikit-learn*. It allows us to register data
    tables, apply SQL queries for data preprocessing, and create and train machine
    learning models within a SQL context.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，*dask-sql* 是一个强大且有前景的机器学习工具，为大数据集提供了一个 SQL 接口，用于数据操作和机器学习操作。使用 *dask-sql*，我们可以利用熟悉的
    SQL 语法来查询和转换数据，同时通过使用像 *scikit-learn* 这样的流行库来训练和评估机器学习模型。它允许我们注册数据表，在 SQL 上下文中应用
    SQL 查询进行数据预处理，并创建和训练机器学习模型。
- en: However, we must highlight that *dask-sql* is still in an experimental phase,
    and although it’s a fascinating tool for SQL lovers who want to explore the machine
    learning space, it must be used with caution as it grows and matures.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们必须强调，*dask-sql* 目前仍处于实验阶段，虽然它是一个对想要探索机器学习空间的 SQL 爱好者来说很吸引人的工具，但在其成长和成熟过程中必须谨慎使用。
- en: Summary
  id: totrans-643
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: As we conclude this chapter, let’s consider the significant journey of databases
    and SQL and their undeniable influence on our past and future. SQL remains a reliable
    and steadfast component in the constantly advancing data landscape, combining
    proven techniques with modern analytical insights, and thus ensuring an optimistic
    future.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们结束本章，让我们考虑数据库和 SQL 的重要旅程以及它们对我们过去和未来的不可否认的影响。SQL 仍然是在不断进步的数据景观中的可靠和坚定的组成部分，将成熟的技术与现代分析洞察结合起来，从而确保一个乐观的未来。
- en: Our exploration has shown that from clear table structures to sophisticated
    models that cater to pressing business requirements, SQL’s significance continues
    to endure, with databases experiencing ongoing innovation.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的探索表明，从清晰的表结构到满足迫切业务需求的复杂模型，SQL 的重要性仍然持续存在，数据库在不断创新中经历着持续的发展。
- en: However, it is worth acknowledging that the effectiveness of these tools depends
    on the skill of those using them. Ongoing education and flexibility are crucial
    for analytics engineers. SQL, database management, and data analysis fields are
    constantly evolving. To succeed, we must stay updated, maintain inquisitiveness,
    and face challenges confidently.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，值得注意的是，这些工具的有效性取决于使用者的技能。持续的教育和灵活性对分析工程师至关重要。SQL、数据库管理和数据分析领域不断发展。要取得成功，我们必须保持更新，保持好奇心，并自信面对挑战。
- en: As the data landscape continues to expand rapidly, the distinctions among roles
    in data engineering, analysis, and data science are becoming more pronounced.
    While there are certainly areas where these roles overlap and blend, data’s sheer
    volume and complexity drive the need for specialized skills and expertise. This
    chapter’s conclusion is a reminder that the field of analytics engineering is
    both extensive and captivating. In every query and database, there lies a fresh
    opportunity for exploration and innovation, driven by the growing demand for specialized
    roles to navigate the complexities of today’s data landscape.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据景观的迅速扩展，数据工程、分析和数据科学角色之间的区别变得更加显著。虽然这些角色确实存在重叠和融合的领域，但数据的庞大量和复杂性推动了对专业技能和专业知识的需求。本章的结论提醒我们，分析工程领域既广泛又迷人。在每一个查询和数据库中，都存在着探索和创新的新机会，这些机会是由对今天数据景观复杂性的导航需要的专业角色驱动的。
