- en: Chapter 2\. Preparing Data for Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章 准备数据进行分析
- en: Estimates of how long data scientists spend preparing their data vary, but it’s
    safe to say that this step takes up a significant part of the time spent working
    with data. In 2014, [the *New York Times* reported](https://oreil.ly/HX1cO) that
    data scientists spend from 50% to 80% of their time cleaning and wrangling their
    data. A [2016 survey by CrowdFlower](https://oreil.ly/5h28Y) found that data scientists
    spend 60% of their time cleaning and organizing data in order to prepare it for
    analysis or modeling work. Preparing data is such a common task that terms have
    sprung up to describe it, such as data munging, data wrangling, and data prep.
    (“Mung” is an acronym for Mash Until No Good, which I have certainly done on occasion.)
    Is all this data preparation work just mindless toil, or is it an important part
    of the process?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 估计数据科学家花在数据准备上的时间有所不同，但可以肯定的是，这一步骤占据了与数据工作的大部分时间。2014年，《纽约时报》报道称，数据科学家将50%至80%的时间用于清理和整理数据。CrowdFlower在2016年的调查发现，数据科学家将60%的时间用于清理和组织数据，以便为分析或建模工作做准备。准备数据是如此普遍的任务，以至于出现了用于描述它的术语，如数据munging、数据wrangling和数据准备。（“Mung”是Mash
    Until No Good的首字母缩写，我偶尔也会这样做。）所有这些数据准备工作只是毫无意义的苦工，还是过程中的一个重要部分呢？
- en: Data preparation is easier when a data set has a *data dictionary*, a document
    or repository that has clear descriptions of the fields, possible values, how
    the data was collected, and how it relates to other data. Unfortunately, this
    is frequently not the case. Documentation often isn’t prioritized, even by people
    who see its value, or it becomes out-of-date as new fields and tables are added
    or the way data is populated changes. Data profiling creates many of the elements
    of a data dictionary, so if your organization already has a data dictionary, this
    is a good time to use it and contribute to it. If no data dictionary exists currently,
    consider starting one! This is one of the most valuable gifts you can give to
    your team and to your future self. An up-to-date data dictionary allows you to
    speed up the data-profiling process by building on profiling that’s already been
    done rather than replicating it. It will also improve the quality of your analysis
    results, since you can verify that you have used fields correctly and applied
    appropriate filters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集有一个*数据字典*时，数据准备就更容易了，这是一个有关字段、可能的值、数据收集方式以及与其他数据关系的清晰描述的文档或存储库。不幸的是，这种情况经常并非如此。即使是认识到其价值的人，通常也不会将文档编制排在首位，或者随着添加新字段和表格或数据填充方式的变化，文档会变得过时。数据剖析生成了数据字典的许多元素，因此，如果你的组织已经有了数据字典，现在是使用它并为其做贡献的好时机。如果当前尚无数据字典存在，请考虑开始创建一个！这是你可以为团队和未来的自己提供的最有价值的礼物之一。一个最新的数据字典能够通过基于已完成的剖析工作而不是重复进行来加速数据剖析过程。它还将提高你分析结果的质量，因为你可以验证是否正确使用了字段并应用了适当的过滤器。
- en: Even when a data dictionary exists, you will still likely need to do data prep
    work as part of the analysis. In this chapter, I’ll start with a review of data
    types you are likely to encounter. This is followed by a review of SQL query structure.
    Next, I will talk about profiling the data as a way to get to know its contents
    and check for data quality. Then I’ll talk about some data-shaping techniques
    that will return the columns and rows needed for further analysis. Finally, I’ll
    walk through some useful tools for cleaning data to deal with any quality issues.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有数据字典存在，作为分析的一部分，你可能仍然需要进行数据准备工作。在本章中，我将从你可能遇到的数据类型的回顾开始。接下来是SQL查询结构的回顾。然后，我将讨论数据剖析作为了解其内容并检查数据质量的一种方式。接下来我会谈一些数据塑形技术，以返回需要进一步分析的列和行。最后，我将介绍一些清理数据的有用工具，以解决任何质量问题。
- en: Types of Data
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据类型
- en: Data is the foundation of analysis, and all data has a database data type and
    also belongs to one or more categories of data. Having a firm grasp of the many
    forms data can take will help you be a more effective data analyst. I’ll start
    with the database data types most frequently encountered in analysis. Then I’ll
    move on to some conceptual groupings that can help us understand the source, quality,
    and possible applications of the data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是分析的基础，所有数据都有数据库数据类型，并且还属于一个或多个数据类别。对数据可能采取的多种形式有坚实的理解将帮助你成为一名更有效的数据分析师。我将从分析中最常见的数据库数据类型开始。然后我将转向一些概念性分组，这些分组有助于我们理解数据的来源、质量和可能的应用。
- en: Database Data Types
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据库数据类型
- en: Fields in database tables all have defined data types. Most databases have good
    documentation on the types they support, and this is a good resource for any needed
    detail beyond what is presented here. You don’t necessarily need to be an expert
    on the nuances of data types to be good at analysis, but later in the book we’ll
    encounter situations in which considering the data type is important, so this
    section will cover the basics. The main types of data are strings, numeric, logical,
    and datetime, as summarized in [Table 2-1](#a_summary_of_common_database_data_types).
    These are based on Postgres but are similar across most major database types.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库表中的字段都有定义的数据类型。大多数数据库都有关于它们支持的类型的良好文档，这是获取本文档之外所需细节的良好资源。要成为一名优秀的分析师，您并不一定需要成为数据类型细微差别的专家，但在本书的后面，我们会遇到需要考虑数据类型的情况，因此本节将介绍基础知识。主要的数据类型包括字符串、数值、逻辑和日期时间，如[表
    2-1](#a_summary_of_common_database_data_types)所总结的。这些基于 Postgres，但在大多数主要数据库类型中是类似的。
- en: Table 2-1\. A summary of common database data types
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-1\. 常见数据库数据类型总结
- en: '| Type | Name | Description |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 名称 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **String** | CHAR / VARCHAR | Holds strings. A CHAR is always of fixed length,
    whereas a VARCHAR is of variable length, up to some maximum size (256 characters,
    for example). |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| **String** | CHAR / VARCHAR | 存储字符串。CHAR 是固定长度的，而 VARCHAR 是可变长度的，最大长度为某个值（例如
    256 个字符）。 |'
- en: '|   | TEXT / BLOB | Holds longer strings that don’t fit in a VARCHAR. Descriptions
    or free text entered by survey respondents might be held in these fields. |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '|   | TEXT / BLOB | 存储不适合在 VARCHAR 中的较长字符串。调查对象输入的描述或自由文本可能存储在这些字段中。 |'
- en: '| **Numeric** | INT / SMALLINT / BIGINT | Holds integers (whole numbers). Some
    databases have SMALLINT and/or BIGINT. SMALLINT can be used when the field will
    only hold values with a small number of digits. SMALLINT takes less memory than
    a regular INT. BIGINT is capable of holding numbers with more digits than an INT,
    but it takes up more space than an INT. |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| **Numeric** | INT / SMALLINT / BIGINT | 存储整数（整数）。一些数据库支持 SMALLINT 和/或 BIGINT。当字段只需存储位数较少的值时可以使用
    SMALLINT，它比普通的 INT 占用更少的内存。BIGINT 可以存储比 INT 更多位数的数字，但占用的空间比 INT 大。 |'
- en: '|   | FLOAT / DOUBLE / DECIMAL | Holds decimal numbers, sometimes with the
    number of decimal places specified. |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '|   | FLOAT / DOUBLE / DECIMAL | 存储十进制数，有时可以指定小数位数。 |'
- en: '| **Logical** | BOOLEAN | Holds values of TRUE or FALSE. |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| **Logical** | BOOLEAN | 存储 TRUE 或 FALSE 值。 |'
- en: '|   | DATETIME / TIMESTAMP | Holds dates with times. Typically in a YYYY-MM-DD
    hh:mi:ss format, where YYYY is the four-digit year, MM is the two-digit month
    number, DD is the two-digit day, hh is the two-digit hour (usually 24-hour time,
    or values of 0 to 23), mi is the two-digit minutes, and ss is the two-digit seconds.
    Some databases store only timestamps without time zone, while others have specific
    types for timestamps with and without time zones. |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '|   | DATETIME / TIMESTAMP | 存储带有时间的日期。通常以 YYYY-MM-DD hh:mi:ss 格式表示，其中 YYYY
    是四位数年份，MM 是两位数月份，DD 是两位数日期，hh 是两位数小时（通常为24小时制，取值范围为0到23），mi 是两位数分钟，ss 是两位数秒。一些数据库只存储没有时区的时间戳，而其他一些数据库有专门的类型用于带和不带时区的时间戳。
    |'
- en: '|   | TIME | Holds times. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '|   | TIME | 存储时间。 |'
- en: String data types are the most versatile. These can hold letters, numbers, and
    special characters, including unprintable characters like tabs and newlines. String
    fields can be defined to hold a fixed or variable number of characters. A CHAR
    field could be defined to allow only two characters to hold US state abbreviations,
    for example, whereas a field storing the full names of states would need to be
    a VARCHAR to allow a variable number of characters. Fields can be defined as TEXT,
    CLOB (Character Large Object), or BLOB (Binary Large Object, which can include
    additional data types such as images), depending on the database to hold very
    long strings, though since they often take up a lot of space, these data types
    tend to be used sparingly. When data is loaded, if strings arrive that are too
    big for the defined data type, they may be truncated or rejected entirely. SQL
    has a number of string functions that we will make use of for various analysis
    purposes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串数据类型最为灵活。这些可以包含字母、数字和特殊字符，包括制表符和换行符等不可打印字符。字符串字段可以定义为固定或可变长度的字符数。例如，CHAR字段可以定义为仅允许两个字符以保存美国州名缩写，而存储州全名的字段需要是VARCHAR以允许可变长度的字符。字段可以定义为TEXT、CLOB（字符大对象）或BLOB（二进制大对象，可以包括附加数据类型如图像），具体取决于数据库以存储非常长的字符串，尽管由于它们通常占用大量空间，这些数据类型往往被节俭使用。当数据加载时，如果到达的字符串超过了定义的数据类型，它们可能会被截断或完全拒绝。SQL有许多字符串函数，我们将用于各种分析目的。
- en: Numeric data types are all the ones that store numbers, both positive and negative.
    Mathematical functions and operators can be applied to numeric fields. Numeric
    data types include the INT types as well as FLOAT, DOUBLE, and DECIMAL types that
    allow decimal places. Integer data types are often implemented because they use
    less memory than their decimal counterparts. In some databases, such as Postgres,
    dividing integers results in an integer, rather than a value with decimal places
    as you might expect. We’ll discuss converting numeric data types to obtain correct
    results later in this chapter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数字数据类型是存储数字的所有类型，包括正数和负数。数学函数和运算符可以应用于数字字段。数字数据类型包括INT类型以及允许小数点的FLOAT、DOUBLE和DECIMAL类型。整数数据类型通常被实现，因为它们比它们的小数对应类型使用更少的内存。在某些数据库中，如Postgres，在整数除法时，结果是一个整数，而不是带有小数位的值，这可能与您的期望不同。我们将在本章后面讨论转换数字数据类型以获得正确结果。
- en: The logical data type is called BOOLEAN. It has values of TRUE and FALSE and
    is an efficient way to store information where these options are appropriate.
    Operations that compare two fields return a BOOLEAN value as a result. This data
    type is often used to create *flags*, fields that summarize the presence or absence
    of a property in the data. For example, a table storing email data might have
    a BOOLEAN `has_opened` field.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑数据类型称为BOOLEAN。它具有TRUE和FALSE的值，并且是存储适当情况下的信息的有效方式。比较两个字段的操作将返回一个BOOLEAN值作为结果。这种数据类型通常用于创建*标志*，即总结数据中属性存在或不存在的字段。例如，存储电子邮件数据的表可能有一个BOOLEAN
    `has_opened`字段。
- en: The datetime types include DATE, TIMESTAMP, and TIME. Date and time data should
    be stored in a field of one of these database types whenever possible, since SQL
    has a number of useful functions that operate on them. Timestamps and dates are
    very common in databases and are critical to many types of analysis, particularly
    time series analysis (covered in [Chapter 3](ch03.xhtml#time_series_analysis))
    and cohort analysis (covered in [Chapter 4](ch04.xhtml#cohort_analysis)). [Chapter 3](ch03.xhtml#time_series_analysis)
    will discuss date and time formatting, transformations, and calculations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 日期时间类型包括DATE、TIMESTAMP和TIME。尽可能将日期和时间数据存储在这些数据库类型的字段中，因为SQL有许多有用的函数可以对它们进行操作。时间戳和日期在数据库中非常常见，并且对许多类型的分析至关重要，特别是时间序列分析（见[第3章](ch03.xhtml#time_series_analysis)）和队列分析（见[第4章](ch04.xhtml#cohort_analysis)）。[第3章](ch03.xhtml#time_series_analysis)将讨论日期和时间格式化、转换和计算。
- en: Other data types, such as JSON and geographical types, are supported by some
    but not all databases. I won’t go into detail on all of them here since they are
    generally beyond the scope of this book. However, they are a sign that SQL continues
    to evolve to tackle emerging analysis tasks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其他数据类型，如JSON和地理类型，一些数据库支持，而一些不支持。我在这里不会详细介绍它们，因为它们通常超出本书的范围。但它们显示了SQL继续发展以应对新兴分析任务的迹象。
- en: Beyond database data types, there are a number of conceptual ways that data
    is categorized. These can have an impact both on how data is stored and on how
    we think about analyzing it. I will discuss these categorical data types next.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据库数据类型之外，还有许多概念上对数据进行分类的方式。这些分类方式可以影响数据的存储方式以及我们对其进行分析的方式。接下来我将讨论这些分类数据类型。
- en: Structured Versus Unstructured
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化与非结构化
- en: Data is often described as structured or unstructured, or sometimes as semistructured.
    Most databases were designed to handle *structured data*, where each attribute
    is stored in a column, and instances of each entity are represented as rows. A
    data model is first created, and then data is inserted according to that data
    model. For example, an address table might have fields for street address, city,
    state, and postal code. Each row would hold a particular customer’s address. Each
    field has a data type and allows only data of that type to be entered. When structured
    data is inserted into a table, each field is verified to ensure it conforms to
    the correct data type. Structured data is easy to query with SQL.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据通常被描述为结构化数据、非结构化数据或有时为半结构化数据。大多数数据库设计用来处理*结构化数据*，其中每个属性存储在一个列中，并且每个实体的实例表示为行。首先创建数据模型，然后根据该数据模型插入数据。例如，地址表可以有街道地址、城市、州和邮政编码字段。每一行将保存特定客户的地址。每个字段都有数据类型，并且只允许输入该类型的数据。当结构化数据插入表中时，将验证每个字段以确保其符合正确的数据类型。结构化数据易于使用SQL进行查询。
- en: '*Unstructured data* is the opposite of structured data. There is no predetermined
    structure, data model, or data types. Unstructured data is often the “everything
    else” that isn’t database data. Documents, emails, and web pages are unstructured.
    Photos, images, videos, and audio files are also examples of unstructured data.
    They don’t fit into the traditional data types, and thus they are more difficult
    for relational databases to store efficiently and for SQL to query. Unstructured
    data is often stored outside of relational databases as a result. This allows
    data to be loaded quickly, but lack of data validation can result in low data
    quality. As we saw in [Chapter 1](ch01.xhtml#analysis_with_sql), the technology
    continues to evolve, and new tools are being developed to allow SQL querying of
    many types of unstructured data.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*非结构化数据*与结构化数据相反。它没有预先确定的结构、数据模型或数据类型。非结构化数据通常是那些不属于数据库数据的“其他所有内容”。文件、电子邮件和网页都属于非结构化数据。照片、图像、视频和音频文件也是非结构化数据的例子。它们不符合传统的数据类型，因此对于关系型数据库来说更难以高效存储和使用SQL进行查询。因此，非结构化数据通常存储在关系型数据库之外。这样可以快速加载数据，但由于缺乏数据验证，可能导致数据质量不佳。正如我们在[第1章](ch01.xhtml#analysis_with_sql)中看到的，技术不断发展，正在开发新工具以允许SQL查询多种类型的非结构化数据。'
- en: '*Semistructured data* falls in between these two categories. Much “unstructured”
    data has some structure that we can make use of. For example, emails have from
    and to email addresses, subject lines, body text, and sent timestamps that can
    be stored separately in a data model with those fields. Metadata, or data about
    data, can be extracted from other file types and stored for analysis. For example,
    music audio files might be tagged with artist, song name, genre, and duration.
    Generally, the structured parts of semistructured data can be queried with SQL,
    and SQL can often be used to parse or otherwise extract structured data for further
    querying. We’ll see some applications of this in the discussion of text analysis
    in [Chapter 5](ch05.xhtml#text_analysis).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*半结构化数据*介于这两种类别之间。许多“非结构化”数据具有一定的结构，我们可以利用它。例如，电子邮件具有发件人和收件人电子邮件地址、主题行、正文文本和发送时间戳，这些可以与这些字段分开存储在数据模型中。元数据，或者关于数据的数据，可以从其他文件类型中提取并存储以进行分析。例如，音乐音频文件可能带有艺术家、歌曲名称、流派和时长的标签。通常情况下，半结构化数据的结构化部分可以使用SQL进行查询，SQL通常也可用于解析或提取结构化数据以进一步查询。我们将在[第5章](ch05.xhtml#text_analysis)中讨论文本分析时看到这些应用。'
- en: Quantitative Versus Qualitative Data
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定量数据与定性数据
- en: '*Quantitative data* is numeric. It measures people, things, and events. Quantitative
    data can include descriptors, such as customer information, product type, or device
    configurations, but it also comes with numeric information such as price, quantity,
    or visit duration. Counts, sums, average, or other numeric functions are applied
    to the data. Quantitative data is often machine generated these days, but it doesn’t
    need to be. Height, weight, and blood pressure recorded on a paper patient intake
    form are quantitative, as are student quiz scores typed into a spreadsheet by
    a teacher.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*定量数据*是数值型的。它测量人、事物和事件。定量数据可以包括描述符，例如客户信息、产品类型或设备配置，同时也包括数值信息，例如价格、数量或访问持续时间。数据可以应用计数、求和、平均数或其他数值函数。定量数据今天通常是由机器生成的，但并非必须如此。在纸质患者接待表上记录的身高、体重和血压是定量数据，教师在电子表格中输入的学生测验分数也是定量数据。'
- en: '*Qualitative data* is usually text based and includes opinions, feelings, and
    descriptions that aren’t strictly quantitative. Temperature and humidity levels
    are quantitative, while descriptors like “hot and humid” are qualitative. The
    price a customer paid for a product is quantitative; whether they like or dislike
    it is qualitative. Survey feedback, customer support inquiries, and social media
    posts are qualitative. There are whole professions that deal with qualitative
    data. In a data analysis context, we usually try to quantify the qualitative.
    One technique for this is to extract keywords or phrases and count their occurrences.
    We’ll look at this in more detail when we delve into text analysis in [Chapter 5](ch05.xhtml#text_analysis).
    Another technique is sentiment analysis, in which the structure of language is
    used to interpret the meaning of the words used, in addition to their frequency.
    Sentences or other bodies of text can be scored for their level of positivity
    or negativity, and then counts or averages are used to derive insights that would
    be hard to summarize otherwise. There have been exciting advances in the field
    of natural language processing, or NLP, though much of this work is done with
    tools such as Python.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*定性数据*通常是基于文本的，包括意见、感觉和非严格定量的描述。温度和湿度水平是定量数据，而像“炎热潮湿”这样的描述则是定性的。客户为产品支付的价格是定量的，他们喜欢还是不喜欢它则是定性的。调查反馈、客户支持询问和社交媒体帖子都属于定性数据。有整个专业处理定性数据。在数据分析的背景下，我们通常尝试量化定性数据。其中一种技术是提取关键词或短语并计算它们的出现次数。我们将在[第五章](ch05.xhtml#text_analysis)更详细地探讨这一点时，会看到更多的技术。另一种技术是情感分析，通过语言结构解释使用的词语含义，除了它们的频率。句子或其他文本主体可以评分其积极或消极的程度，然后使用计数或平均数来得出可能难以总结的见解。自然语言处理领域已经取得了令人兴奋的进展，尽管大部分工作是通过Python等工具完成的。'
- en: First-, Second-, and Third-Party Data
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一方、第二方和第三方数据
- en: '*First-party data* is collected by the organization itself. This can be done
    through server logs, databases that keep track of transactions and customer information,
    or other systems that are built and controlled by the organization and generate
    data of interest for analysis. Since the systems were created in-house, finding
    the people who built them and learning about how the data is generated is usually
    possible. Data analysts may also be able to influence or have control over how
    certain pieces of data are created and stored, particularly when bugs are responsible
    for poor data quality.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*第一方数据*由组织自行收集。这可以通过服务器日志、记录交易和客户信息的数据库或其他由组织建立和控制的系统完成，并生成有助于分析的数据。由于这些系统是内部创建的，通常可以找到建造它们的人，并了解数据生成的方式。数据分析师还可能能够影响或控制某些数据的创建和存储方式，尤其是在数据质量差的时候由于错误导致的情况下。'
- en: '*Second-party data* comes from vendors that provide a service or perform a
    business function on the organization’s behalf. These are often software as a
    service (SaaS) products; common examples are CRM, email and marketing automation
    tools, ecommerce-enabling software, and web and mobile interaction trackers. The
    data is similar to first-party data since it is about the organization itself,
    created by its employees and customers. However, both the code that generates
    and stores the data and the data model are controlled externally, and the data
    analyst typically has little influence over these aspects. Second-party data is
    increasingly imported into an organization’s data warehouse for analysis. This
    can be accomplished with custom code or ETL connectors, or with SaaS vendors that
    offer data integration.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*第二方数据*来自为组织提供服务或代表其进行业务功能的供应商。这些通常是软件即服务（SaaS）产品；常见示例包括CRM、电子邮件和营销自动化工具、电子商务支持软件以及网页和移动互动追踪器。这些数据与第一方数据类似，因为它们涉及组织本身，由其员工和客户创建。然而，生成和存储数据的代码以及数据模型都由外部控制，数据分析师通常对这些方面影响有限。第二方数据越来越多地被导入组织的数据仓库进行分析。可以通过自定义代码或ETL连接器完成此操作，或者使用提供数据集成的SaaS供应商。'
- en: Tip
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Many SaaS vendors provide some reporting capabilities, so the question may
    arise of whether to bother copying the data to a data warehouse. The department
    that interacts with a tool may find that reporting sufficient, such as a customer
    service department that reports on time to resolve issues and agent productivity
    from within its helpdesk software. On the other hand, customer service interactions
    might be an important input to a customer retention model, which would require
    integrating that data into a data store with sales and cancellation data. Here’s
    a good rule of thumb when deciding whether to import data from a particular data
    source: if the data will create value when combined with data from other systems,
    import it; if not, wait until there is a stronger case before doing the work.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 许多SaaS供应商提供一些报告功能，因此可能会出现是否值得将数据复制到数据仓库的问题。与工具交互的部门可能会发现报告足够，例如客户服务部门从其帮助台软件中报告解决问题的时间和代理人的生产力。另一方面，客户服务互动可能是客户保留模型的重要输入，这需要将这些数据与销售和取消数据整合到数据存储中。在决定是否从特定数据源导入数据时，有一个很好的经验法则：如果将数据与其他系统的数据组合能够创造价值，那就导入它；如果不能，那么在有更强的案例之前再等待进行这项工作。
- en: '*Third-party data* may be purchased or obtained from free sources such as those
    published by governments. Unless the data has been collected specifically on behalf
    of the organization, data teams usually have little control over the format, frequency,
    and data quality. This data often lacks the granularity of first- and second-party
    data. For example, most third-party sources do not have user-level data, and instead
    data might be joined with first-party data at the postal code or city level, or
    at a higher level. Third-party data can have unique and useful information, however,
    such as aggregate spending patterns, demographics, and market trends that would
    be very expensive or impossible to collect otherwise.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*第三方数据*可以从政府等免费来源购买或获取。除非数据是专门为组织收集的，数据团队通常无法控制格式、频率和数据质量。这些数据通常缺乏第一方和第二方数据的细粒度。例如，大多数第三方来源没有用户级数据，而是可能在邮政编码或城市级别或更高级别与第一方数据合并。然而，第三方数据可能包含独特和有用的信息，例如聚合消费模式、人口统计数据和市场趋势，否则这些信息收集可能非常昂贵或不可能实现。'
- en: Sparse Data
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稀疏数据
- en: '*Sparse data* occurs when there is a small amount of information within a larger
    set of empty or unimportant information. Sparse data might show up as many nulls
    and only a few values in a particular column. Null, different from a value of
    0, is the *absence* of data; that will be covered later in the section on data
    cleaning. Sparse data can occur when events are rare, such as software errors
    or purchases of products in the long tail of a product catalog. It can also occur
    in the early days of a feature or product launch, when only testers or beta customers
    have access. JSON is one approach that has been developed to deal with sparse
    data from a writing and storage perspective, as it stores only the data that is
    present and omits the rest. This is in contrast to a row-store database, which
    has to hold memory for a field even if there is no value in it.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*稀疏数据* 是指在较大集合中存在少量信息的情况。稀疏数据可能会表现为某一列中许多空值和少量实际值。空值不同于值为 0，是数据的 *缺失*，这将在数据清洗部分后面详细讨论。稀疏数据可能发生在事件稀缺的情况下，例如软件错误或产品目录长尾中的产品购买。它也可能发生在功能或产品推出的早期阶段，只有测试人员或测试客户可以访问时。JSON
    是一种处理稀疏数据的方法，从编写和存储的角度来看，它只存储存在的数据并省略其余部分。这与行存储数据库形成对比，后者即使没有值也必须为字段保留内存空间。'
- en: Sparse data can be problematic for analysis. When events are rare, trends aren’t
    necessarily meaningful, and correlations are hard to distinguish from chance fluctuations.
    It’s worth profiling your data, as discussed later in this chapter, to understand
    if and where your data is sparse. Some options are to group infrequent events
    or items into categories that are more common, exclude the sparse data or time
    period from the analysis entirely, or show descriptive statistics along with cautionary
    explanations that the trends are not necessarily meaningful.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分析而言，稀疏数据可能会带来问题。当事件稀疏时，趋势未必有意义，相关性也难以与偶然波动区分开来。值得分析数据，正如本章后面讨论的那样，以了解数据是否稀疏以及稀疏数据的位置。一些选项包括将不经常发生的事件或项目分组到更常见的类别中，完全排除稀疏数据或时间段的分析，或显示描述性统计信息以及警示说明，说明这些趋势未必有意义。
- en: There are a number of different types of data and a variety of ways that data
    is described, many of which are overlapping or not mutually exclusive. Familiarity
    with these types is useful not only in writing good SQL but also for deciding
    how to analyze the data in appropriate ways. You may not always know the data
    types in advance, which is why data profiling is so critical. Before we get to
    that, and to our first code examples, I’ll give a brief review of SQL query structure.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多种不同类型的数据及其描述方式，其中许多方式是重叠的或不是互斥的。熟悉这些类型不仅有助于编写良好的 SQL，还有助于决定如何以适当的方式分析数据。您可能并不总是能预先知道数据类型，这就是为什么数据分析如此关键的原因。在我们进入具体内容和我们的第一个代码示例之前，我将简要回顾
    SQL 查询结构。
- en: SQL Query Structure
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SQL 查询结构
- en: SQL queries have common clauses and syntax, although these can be combined in
    a nearly infinite number of ways to achieve analysis goals. This book assumes
    you have some prior knowledge of SQL, but I’ll review the basics here so that
    we have a common foundation for the code examples to come.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 查询具有常见的子句和语法，尽管可以以几乎无限的方式组合这些子句以实现分析目标。本书假定您具有一定的 SQL 知识，但我将在此回顾基础知识，以便我们在接下来的代码示例中有一个共同的基础。
- en: The *SELECT* clause determines the columns that will be returned by the query.
    One column will be returned for each expression within the *SELECT* clause, and
    expressions are separated by commas. An expression can be a field from the table,
    an aggregation such as a `sum`, or any number of calculations, such as CASE statements,
    type conversions, and various functions that will be discussed later in this chapter
    and throughout the book.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*SELECT* 子句确定查询将返回的列。每个 *SELECT* 子句中的表达式将返回一列，表达式之间用逗号分隔。表达式可以是来自表的字段，如 `sum`
    这样的聚合，或者任何数量的计算，如 CASE 语句、类型转换和本章后面以及整本书中将讨论的各种函数。'
- en: The *FROM* clause determines the tables from which the expressions in the *SELECT*
    clause are derived. A “table” can be a database table, a view (a type of saved
    query that otherwise functions like a table), or a subquery. A subquery is itself
    a query, wrapped in parentheses, and the result is treated like any other table
    by the query that references it. A query can reference multiple tables in the
    *FROM* clause, though they must use one of the *JOIN* types along with a condition
    that specifies how the tables relate. The *JOIN* condition usually specifies an
    equality between fields in each table, such as `orders.customer_id = customers.customer_id`.
    *JOIN* conditions can include multiple fields and can also specify inequalities
    or ranges of values, such as ranges of dates. We’ll see a variety of *JOIN* conditions
    that achieve specific analysis goals throughout the book. An *INNER JOIN* returns
    all records that match in both tables. A *LEFT JOIN* returns all records from
    the first table, but only those records from the second table that match. A *RIGHT
    JOIN* returns all records from the second table, but only those records from the
    first table that match. A *FULL OUTER JOIN* returns all records from both tables.
    A Cartesian *JOIN* can result when each record in the first table matches more
    than one record in the second table. Cartesian *JOIN*s should generally be avoided,
    though there are some specific use cases, such as generating data to fill in a
    time series, in which we will use them intentionally. Finally, tables in the *FROM*
    clause can be *aliased*, or given a shorter name of one or more letters that can
    be referenced in other clauses in the query. Aliases save query writers from having
    to type out long table names repeatedly, and they make queries easier to read.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*FROM* 子句确定 *SELECT* 子句中的表达式是从哪些表派生的。一个“表”可以是数据库表，视图（一种保存的查询，否则像表一样工作），或者子查询。子查询本身是一个用括号括起来的查询，其结果被引用它的查询视为任何其他表。一个查询可以在
    *FROM* 子句中引用多个表，尽管它们必须使用 *JOIN* 类型之一以及指定表之间关系的条件。*JOIN* 条件通常指定每个表中字段之间的相等性，例如
    `orders.customer_id = customers.customer_id`。*JOIN* 条件可以包括多个字段，并且还可以指定不等式或值的范围，例如日期范围。我们将在本书中看到一系列实现特定分析目标的
    *JOIN* 条件。*INNER JOIN* 返回两个表中都匹配的所有记录。*LEFT JOIN* 返回第一个表的所有记录，但只返回第二个表中匹配的记录。*RIGHT
    JOIN* 返回第二个表的所有记录，但只返回第一个表中匹配的记录。*FULL OUTER JOIN* 返回两个表的所有记录。当第一个表中的每条记录与第二个表中的多条记录匹配时，可以产生笛卡尔
    *JOIN*。通常应避免笛卡尔 *JOIN*，尽管有一些特定的用例，比如生成填充时间序列的数据时会故意使用它们。最后，*FROM* 子句中的表可以被别名化，或者给定一个或多个字母的较短名称，可以在查询的其他子句中引用。别名可以使查询编写人员免于重复输入长表名，并且使查询更易于阅读。'
- en: Tip
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: While both *LEFT JOIN* and *RIGHT JOIN* can be used in the same query, it’s
    much easier to keep track of your logic when you stick with only one or the other.
    In practice, *LEFT JOIN* is much more commonly used than *RIGHT JOIN*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可以在同一查询中使用 *LEFT JOIN* 和 *RIGHT JOIN*，但当您坚持使用其中一种时，跟踪逻辑要容易得多。实际上，*LEFT JOIN*
    比 *RIGHT JOIN* 更常用。
- en: The *WHERE* clause specifies restrictions or filters that are needed to exclude
    or remove rows from the result set. *WHERE* is optional.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*WHERE* 子句指定需要从结果集中排除或删除行的限制或过滤器。*WHERE* 是可选的。'
- en: 'The *GROUP BY* clause is required when the *SELECT* clause contains aggregations
    and at least one nonaggregated field. An easy way to remember what should go in
    the *GROUP BY* clause is that it should have every field that is not part of an
    aggregation. In most databases, there are two ways to list the *GROUP BY* fields:
    either by field name or by position, such as 1, 2, 3, and so on. Some people prefer
    to use the field name notation, and SQL Server requires this. I prefer the position
    notation, particularly when the *GROUP BY* fields contain complex expressions
    or when I’m doing a lot of iteration. This book will typically use the position
    notation.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *SELECT* 子句包含聚合函数并且至少有一个非聚合字段时，*GROUP BY* 子句是必需的。记住应该放在 *GROUP BY* 子句中的内容的一个简单方法是，它应该包括所有不是聚合的字段。在大多数数据库中，有两种列出
    *GROUP BY* 字段的方式：按字段名称或按位置，如 1、2、3 等。一些人更喜欢使用字段名称表示法，SQL Server 要求使用这种表示法。我更喜欢位置表示法，特别是当
    *GROUP BY* 字段包含复杂表达式或者当我进行大量迭代时。本书通常会使用位置表示法。
- en: 'That covers the basics of SQL query structure. [Chapter 8](ch08.xhtml#creating_complex_data_sets_for_analysis)
    will go into additional detail on each of these clauses, a few additional ones
    that are less commonly encountered but appear in this book, and the order in which
    each clause is evaluated. Now that we have this foundation, we can turn to one
    of the most important parts of the analysis process: data profiling.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这涵盖了SQL查询结构的基础。[第8章](ch08.xhtml#creating_complex_data_sets_for_analysis)将详细讨论这些子句的每一个，以及在本书中较少见但出现的一些额外子句的顺序评估。既然我们有了这个基础，我们可以转向分析过程中最重要的部分之一：数据剖析。
- en: 'Profiling: Distributions'
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 剖析：分布
- en: Profiling is the first thing I do when I start working with any new data set.
    I look at how the data is arranged into schemas and tables. I look at the table
    names to get familiar with the topics covered, such as customers, orders, or visits.
    I check out the column names in a few tables and start to construct a mental model
    of how the tables relate to one another. For example, the tables might include
    an `order_detail` table with line-item breakouts that relate to the `order` table
    via an `order_id`, while the `order` table relates to the `customer` table via
    a `customer_id`. If there is a data dictionary, I review that and compare it to
    the data I see in a sample of rows.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 数据剖析是我开始处理任何新数据集时首先做的事情。我查看数据是如何安排成模式和表的。我查看表名以熟悉所涵盖的主题，例如客户、订单或访问。我检查几个表的列名，并开始构建表之间关系的心理模型。例如，表可能包括一个通过`order_id`与`order`表相关的行项目分解的`order_detail`表，而`order`表则通过`customer_id`与`customer`表相关。如果有数据字典，我会审查并与样本行中看到的数据进行比较。
- en: The tables generally represent the operations of an organization, or some subset
    of the operations, so I think about what domain or domains are covered, such as
    ecommerce, marketing, or product interactions. Working with data is easier when
    we have knowledge of how the data was generated. Profiling can provide clues about
    this, or about what questions to ask of the source, or of people inside or outside
    the organization responsible for the collection or generation of the data. Even
    when you collect the data yourself, profiling is useful.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表通常代表组织的操作或某些操作的子集，因此我考虑涵盖的领域或领域，例如电子商务、营销或产品互动。当我们了解数据生成方式时，处理数据会更容易。剖析可以提供关于这一点的线索，或者关于向来源、组织内外负责收集或生成数据的人提出的问题。即使你自己收集数据，剖析也是有用的。
- en: Another detail I check for is how history is represented, if at all. Data sets
    that are replicas of production databases may not contain previous values for
    customer addresses or order statuses, for example, whereas a well-constructed
    data warehouse may have daily snapshots of changing data fields.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我检查的细节是如何表示历史数据，如果有的话。例如，复制生产数据库的数据集可能不包含客户地址或订单状态的先前值，而一个构建良好的数据仓库可能会有每日变化数据字段的快照。
- en: Profiling data is related to the concept of *exploratory data analysis*, or
    EDA, named by John Tukey. In his book of that name,^([1](ch02.xhtml#ch01fn3))
    Tukey describes how to analyze data sets by computing various summaries and visualizing
    the results. He includes techniques for looking at distributions of data, including
    stem-and-leaf plots, box plots, and histograms.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 数据剖析与*探索性数据分析*或EDA的概念相关，由John Tukey命名。在他的同名书籍中，^[1](ch02.xhtml#ch01fn3) Tukey描述了如何通过计算各种摘要和可视化结果来分析数据集。他包括了查看数据分布的技术，包括茎叶图、箱线图和直方图。
- en: After checking a few samples of data, I start looking at distributions. Distributions
    allow me to understand the range of values that exist in the data and how often
    they occur, whether there are nulls, and whether negative values exist alongside
    positive ones. Distributions can be created with continuous or categorical data
    and are also called frequencies. In this section, we’ll look at how to create
    histograms, how binning can help us understand the distribution of continuous
    values, and how to use n-tiles to get more precise about distributions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查几个数据样本后，我开始查看分布。分布可以帮助我了解数据中存在的值的范围及其出现频率，是否存在空值，以及负值是否与正值并存。分布可以针对连续或分类数据创建，并称为频率。在本节中，我们将看看如何创建直方图，如何使用分箱来理解连续值的分布，以及如何使用n-分位数来更精确地了解分布情况。
- en: Histograms and Frequencies
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 柱状图和频率
- en: One of the best ways to get to know a data set, and to know particular fields
    within the data set, is to check the frequency of values in each field. Frequency
    checks are also useful whenever you have a question about whether certain values
    are possible or if you spot an unexpected value and want to know how commonly
    it occurs. Frequency checks can be done on any data type, including strings, numerics,
    dates, and booleans. Frequency queries are a great way to detect sparse data as
    well.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 了解数据集以及其中特定字段的最佳方法之一是检查每个字段中值的频率。无论您是否怀疑某些值是否可能，或者是否发现意外值并想知道它们出现的频率，频率检查都非常有用。频率查询可以对任何数据类型执行，包括字符串、数字、日期和布尔值。频率查询还是检测稀疏数据的一种有效方式。
- en: 'The query is straightforward. The number of rows can be found with `count(*)`,
    and the profiled field is in the *GROUP BY*. For example, we can check the frequency
    of each type of `fruit` in a fictional `fruit_inventory` table:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 查询很简单。可以用`count(*)`找到行数，并在*GROUP BY*中列出要分析的字段。例如，我们可以检查虚构的`fruit_inventory`表中每种类型的水果的频率：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Tip
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: When using `count`, it’s worth taking a minute to consider whether there might
    be any duplicate records in the data set. You can use `count(*)` when you want
    the number of records, but use `count distinct` to find out how many unique items
    there are.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`count`时，考虑一下数据集中是否可能存在重复记录是值得的。当您想要记录数时可以使用`count(*)`，但是使用`count distinct`可以找出有多少个唯一项。
- en: A *frequency plot* is a way to visualize the number of times something occurs
    in the data set. The field being profiled is usually plotted on the x-axis, with
    the count of observations on the y-axis. [Figure 2-1](#frequency_plot_of_fruit_inventory)
    shows an example of plotting the frequency of fruit from our query. Frequency
    graphs can also be drawn horizontally, which accommodates long value names well.
    Notice that this is categorical data without any inherent order.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*频率图* 是一种可视化数据集中某些事物出现次数的方法。通常在x轴上绘制被分析的字段，y轴上显示观察次数。[图 2-1](#frequency_plot_of_fruit_inventory)展示了我们查询的水果频率图的示例。频率图也可以横向绘制，这样可以很好地容纳较长的值名称。请注意，这是无序的分类数据。'
- en: '![](Images/sfda_0201.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0201.png)'
- en: Figure 2-1\. Frequency plot of fruit inventory
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1\. 水果库存的频率图
- en: 'A *histogram* is a way to visualize the distribution of numerical values in
    a data set and will be familiar to those with a statistics background. A basic
    histogram might show the distribution of ages across a group of customers. Imagine
    that we have a `customers` table that contains names, registration date, age,
    and other attributes. To create a histogram by age, *GROUP BY* the numerical `age`
    field and `count customer_id`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*直方图* 是一种可视化数据集中数值分布的方法，对于具有统计学背景的人来说应该很熟悉。一个基本的直方图可以展示一组客户的年龄分布。假设我们有一个包含姓名、注册日期、年龄和其他属性的`customers`表。要按年龄创建直方图，通过数值字段`age`进行*GROUP
    BY*，并计算`count customer_id`：'
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The results of our hypothetical age distribution are graphed in [Figure 2-2](#customers_by_age).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设的年龄分布结果在[图 2-2](#customers_by_age)中显示。
- en: '![](Images/sfda_0202.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0202.png)'
- en: Figure 2-2\. Customers by age
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2\. 按年龄分布的客户
- en: 'Another technique I’ve used repeatedly and that has become the basis for one
    of my favorite interview questions involves an aggregation followed by a frequency
    count. I give candidates a hypothetical table called `orders`, which has a date,
    customer identifier, order identifier, and an amount, and then ask them to write
    a SQL query that returns the distribution of orders per customer. This can’t be
    solved with a simple query; it requires an intermediate aggregation step, which
    can be accomplished with a subquery. First, `count` the number of orders placed
    by each `customer_id` in the subquery. The outer query uses the number of `orders`
    as a category and `count`s the number of customers:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我反复使用的另一种技巧，也成为我最喜欢的面试问题之一的基础，涉及到聚合后跟随的频率计数。我给候选人一个名为`orders`的假设表，其中包含日期、客户标识符、订单标识符和金额，然后要求他们编写一个SQL查询，返回每个客户的订单分布。这不能用简单的查询解决；它需要一个中间聚合步骤，可以通过子查询完成。首先，在子查询中对每个`customer_id`的订单数进行`count`。外部查询使用`orders`数作为类别，并计算客户的数量：
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This type of profiling can be applied whenever you need to see how frequently
    certain entities or attributes appear in the data. In these examples, `count`
    has been used, but the other basic aggregations (`sum`, `avg`, `min`, and `max`)
    can be used to create histograms as well. For instance, we might want to profile
    customers by the `sum` of all their orders, their `avg` order size, their `min`
    order date, or their `max` (most recent) order date.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的分析可以应用于需要查看数据中特定实体或属性出现频率的任何情况。在这些示例中，已使用`count`，但也可以使用其他基本聚合函数（`sum`、`avg`、`min`和`max`）来创建直方图。例如，我们可能想通过他们所有订单的`sum`，他们的`avg`订单大小，他们的`min`订单日期或他们的`max`（最近）订单日期来对客户进行分析。
- en: Binning
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分箱
- en: Binning is useful when working with continuous values. Rather than the number
    of observations or records for each value being counted, ranges of values are
    grouped together, and these groups are called *bins* or *buckets*. The number
    of records that fall into each interval is then counted. Bins can be variable
    in size or have a fixed size, depending on whether your goal is to group the data
    into bins that have particular meaning for the organization, are roughly equal
    width, or contain roughly equal numbers of records. Bins can be created with CASE
    statements, rounding, and logarithms.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理连续值时，分箱非常有用。不是计算每个值的观察次数或记录数，而是将数值范围分组在一起，这些分组称为*箱*或*桶*。然后计算落入每个区间的记录数。箱可以具有可变大小或固定大小，这取决于您的目标是将数据分组为对组织具有特定含义的箱，还是大致相等宽度的箱，或包含大致相等记录数的箱。可以使用CASE语句、四舍五入和对数函数创建箱。
- en: 'A CASE statement allows for conditional logic to be evaluated. These statements
    are very flexible, and we will come back to them throughout the book, applying
    them to data profiling, cleaning, text analysis, and more. The basic structure
    of a CASE statement is:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: CASE语句允许评估条件逻辑。这些语句非常灵活，我们将在整本书中不断使用它们，应用于数据分析、清理、文本分析等领域。CASE语句的基本结构是：
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The WHEN condition can be an equality, inequality, or other logical condition.
    The THEN return value can be a constant, an expression, or a field in the table.
    Any number of conditions can be included, but the statement will stop executing
    and return the result the first time a condition evaluates to TRUE. ELSE tells
    the database what to use as a default value if no matches are found and can also
    be a constant or field. ELSE is optional, and if it is not included, any nonmatches
    will return null. CASE statements can also be nested so that the return value
    is another CASE statement.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: WHEN条件可以是相等性、不等式或其他逻辑条件。THEN返回值可以是常量、表达式或表中的字段。可以包括任意数量的条件，但语句将在条件第一次评估为TRUE时停止执行并返回结果。ELSE告诉数据库如果找不到匹配项应使用什么作为默认值，也可以是常量或字段。ELSE是可选的，如果不包括，则任何非匹配项将返回null。CASE语句还可以嵌套，以便返回值是另一个CASE语句。
- en: Tip
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The return values following THEN must all be the same data type (strings, numeric,
    BOOLEAN, etc.), or else you’ll get an error. Consider casting to a common data
    type such as string if you encounter this.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: THEN后的返回值必须是相同的数据类型（字符串、数值、布尔值等），否则会报错。如果遇到此类情况，请考虑转换为通用数据类型，例如字符串。
- en: 'A CASE statement is a flexible way to control the number of bins, the range
    of values that fall into each bin, and how the bins are named. I find them particularly
    useful when there is a long tail of very small or very large values that I want
    to group together rather than have empty bins in part of the distribution. Certain
    ranges of values have a business meaning that needs to be re-created in the data.
    Many B2B companies separate their customers into “enterprise” and “SMB” (small-
    and medium-sized businesses) categories based on number of employees or revenue,
    because their buying patterns are different. As an example, imagine we are considering
    discounted shipping offers and we want to know how many customers will be affected.
    We can group `order_amount` into three buckets using a CASE statement:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: CASE语句是一种灵活的方法，用于控制箱数、每个箱中落入的值的范围以及如何命名这些箱。当存在非常小或非常大值的长尾需要分组而不希望在分布的某些部分中出现空箱时，它们尤其有用。某些数值范围具有需要在数据中重新创建的业务含义。许多B2B公司根据员工数量或收入将客户分为“企业”和“SMB”（中小型企业）类别，因为它们的购买模式不同。例如，假设我们正在考虑打折运费优惠，并且想知道有多少客户会受到影响。我们可以使用CASE语句将`order_amount`分成三个桶：
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Arbitrary-sized bins can be useful, but at other times bins of fixed size are
    more appropriate for the analysis. Fixed-size bins can be accomplished in a few
    ways, including with rounding, logarithms, and n-tiles. To create equal-width
    bins, rounding is useful. Rounding reduces the precision of the values, and we
    usually think about rounding as reducing the number of decimal places or removing
    them altogether by rounding to the nearest integer. The `round` function takes
    the form:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 任意大小的箱子可能很有用，但有时固定大小的箱子更适合分析。固定大小的箱子可以通过几种方式实现，包括四舍五入、对数和n-tiles。创建等宽箱子时，四舍五入很有用。四舍五入会减少值的精度，我们通常认为四舍五入是通过减少小数位数或完全删除它们来减少的。`round`函数的形式为：
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The number of decimal places can also be a negative number, allowing this function
    to round to the nearest tens, hundreds, thousands, and so on. [Table 2-2](#the_number_onetwothreecommafourfivesixd)
    demonstrates the results of rounding with arguments ranging from –3 to 2.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 小数位数也可以是负数，允许该函数四舍五入到最接近的十位、百位、千位等。[表 2-2](#the_number_onetwothreecommafourfivesixd)展示了带有从–3到2的参数的四舍五入结果。
- en: Table 2-2\. The number 123,456.789 rounded with various decimal places
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Table 2-2\. 数字123,456.789的各种小数位数四舍五入
- en: '| Decimal places | Formula | Result |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 小数位数 | 公式 | 结果 |'
- en: '| --- | --- | --- |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 2 | round(123456.789,2) | 123456.79 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 2 | round(123456.789,2) | 123456.79 |'
- en: '| 1 | round(123456.789,1) | 123456.8 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 1 | round(123456.789,1) | 123456.8 |'
- en: '| 0 | round(123456.789,0) | 123457 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 0 | round(123456.789,0) | 123457 |'
- en: '| -1 | round(123456.789,-1) | 123460 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| -1 | round(123456.789,-1) | 123460 |'
- en: '| -2 | round(123456.789,-2) | 123500 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| -2 | round(123456.789,-2) | 123500 |'
- en: '| -3 | round(123456.789,-3) | 123000 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| -3 | round(123456.789,-3) | 123000 |'
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Logarithms are another way to create bins, particularly in data sets in which
    the largest values are orders of magnitude greater than the smallest values. The
    distribution of household wealth, the number of website visitors across different
    properties on the internet, and the shaking force of earthquakes are all examples
    of phenomena that have this property. While they don’t create bins of equal width,
    logarithms create bins that increase in size with a useful pattern. To refresh
    your memory, a logarithm is the exponent to which 10 must be raised to produce
    that number:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对数是创建箱子的另一种方式，特别是在数据集中，最大值的数量级远大于最小值的情况下。家庭财富的分布、互联网上不同属性的网站访客数量以及地震的震动力量等都是具有此属性的现象的示例。虽然它们不会创建等宽箱子，但对数会创建具有有用模式的箱子。刷新一下记忆，对数是必须将10提高到产生该数字的指数：
- en: log(*number*) = *exponent*
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: log(*number*) = *exponent*
- en: In this case, 10 is called the base, and this is usually the default implementation
    in databases, but technically the base can be any number. [Table 2-3](#results_of_log_function_on_powers_of_on)
    shows the logarithms for several powers of 10.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，10称为底数，这通常是数据库中的默认实现，但从技术上讲，底数可以是任何数字。[表 2-3](#results_of_log_function_on_powers_of_on)显示了几个10的幂的对数。
- en: Table 2-3\. Results of `log` function on powers of 10
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Table 2-3\. 对`log`函数在10的幂上的结果
- en: '| Formula | Result |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 公式 | 结果 |'
- en: '| --- | --- |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| log(1) | 0 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| log(1) | 0 |'
- en: '| log(10) | 1 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| log(10) | 1 |'
- en: '| log(100) | 2 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| log(100) | 2 |'
- en: '| log(1000) | 3 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| log(1000) | 3 |'
- en: '| log(10000) | 4 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| log(10000) | 4 |'
- en: 'In SQL, the `log` function returns the logarithm of its argument, which can
    be a constant or a field:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在SQL中，`log`函数返回其参数的对数，该参数可以是常数或字段：
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `log` function can be used on any positive value, not just multiples of
    10\. However, the logarithm function does not work when values can be less than
    or equal to 0; it will return null or an error, depending on the database.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`log`函数可以用于任何正值，而不仅仅是10的倍数。但是，当值小于或等于0时，对数函数无法工作；它将返回null或错误，这取决于数据库。'
- en: n-Tiles
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: n-Tiles
- en: 'You’re probably familiar with the *median*, or middle value, of a data set.
    This is the 50th percentile value. Half of the values are larger than the median,
    and the other half are smaller. With quartiles, we fill in the 25th and 75th percentile
    values. A quarter of the values are smaller and three quarters are larger for
    the 25th percentile; three quarters are smaller and one quarter are larger at
    the 75th percentile. Deciles break the data set into 10 equal parts. Making this
    concept generic, *n-tiles* allow us to calculate any percentile of the data set:
    27th percentile, 50.5th percentile, and so on.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能熟悉数据集的*中位数*，或者中间值。这是第50百分位值。一半的值大于中位数，另一半小于中位数。对于四分位数，我们填充25百分位和75百分位值。在第25百分位，四分之一的值较小，三分之三较大；在第75百分位，三分之三的值较小，四分之一较大。分位数将数据集分成10等份。通过使这个概念通用化，*n-tiles*允许我们计算数据集的任何百分位数：第27百分位数、第50.5百分位数等。
- en: 'Many databases have a `median` function built in but rely on more generic n-tile
    functions for the rest. These functions are window functions, computing across
    a range of rows to return a value for a single row. They take an argument that
    specifies the number of bins to split the data into and, optionally, a *PARTITION
    BY* and/or an *ORDER BY* clause:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 许多数据库内置了`median`函数，但对其余功能依赖于更通用的n-tile函数。这些函数是窗口函数，跨越多行计算以返回单行的值。它们接受一个参数，指定要将数据分割为的区间数，并且可以选择使用*PARTITION
    BY*和/或*ORDER BY*子句：
- en: '[PRE8]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As an example, imagine we had 12 transactions with `order_amount`s of $19.99,
    $9.99, $59.99, $11.99, $23.49, $55.98, $12.99, $99.99, $14.99, $34.99, $4.99,
    and $89.99\. Performing an `ntile` calculation with 10 bins sorts each `order_amount`
    and assigns a bin from 1 to 10:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设我们有12笔交易，订单金额分别为$19.99、$9.99、$59.99、$11.99、$23.49、$55.98、$12.99、$99.99、$14.99、$34.99、$4.99和$89.99。使用10个区间对`ntile`进行计算，对每个订单金额排序并分配一个从1到10的区间：
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This can be used to bin records in practice by first calculating the `ntile`
    of each row in a subquery and then wrapping it in an outer query that uses `min`
    and `max` to find the upper and lower boundaries of the value range:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过首先计算子查询中每行的`ntile`，然后在使用`min`和`max`查找值范围的上下界的外部查询中使用来实践中对记录进行分箱：
- en: '[PRE10]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'A related function is `percent_rank`. Instead of returning the bins that the
    data falls into, `percent_rank` returns the percentile. It takes no argument but
    requires parentheses and optionally takes a *PARTITION BY* and/or an *ORDER BY*
    clause:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相关的函数是`percent_rank`。与返回数据所在区间不同，`percent_rank`返回百分位数。它不带参数，但需要括号，并且可以带有*PARTITION
    BY*和/或*ORDER BY*子句：
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: While not as useful as `ntile` for binning, `percent_rank` can be used to create
    a continuous distribution, or it can be used as an output itself for reporting
    or further analysis. Both `ntile` and `percent_rank` can be expensive to compute
    over large data sets, since they require sorting all the rows. Filtering the table
    to only the data set you need helps. Some databases have implemented approximate
    versions of the functions that are faster to compute and generally return high-quality
    results if absolute precision is not required. We will look at additional uses
    for n-tiles in the discussion of anomaly detection in [Chapter 6](ch06.xhtml#anomaly_detection).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在分箱方面不如`ntile`有用，`percent_rank`可用于创建连续分布，或者作为报告或进一步分析的输出本身使用。由于需要对所有行进行排序，对大数据集进行计算可能会很昂贵。通过过滤表格仅包含所需数据集有助于减少计算成本。某些数据库实现了这些函数的近似版本，这些版本计算速度更快，并且如果不需要绝对精确性，则通常返回高质量的结果。在[第6章](ch06.xhtml#anomaly_detection)中讨论异常检测时，我们将进一步探讨n-tile的其他用途。
- en: In many contexts, there is no single correct or objectively best way to look
    at distributions of data. There is significant leeway for analysts to use the
    preceding techniques to understand data and present it to others. However, data
    scientists need to use judgment and must bring their ethical radar along whenever
    sharing distributions of sensitive data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情境中，查看数据分布没有单一正确或客观最佳的方式。分析人员可以在理解数据和向他人展示数据方面使用上述技巧。然而，数据科学家需要运用判断力，并且在分享敏感数据的时候必须带着他们的道德雷达。
- en: 'Profiling: Data Quality'
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析：数据质量
- en: Data quality is absolutely critical when it comes to creating good analysis.
    Although this may seem obvious, it has been one of the hardest lessons I’ve learned
    in my years of working with data. It’s easy to get overly focused on the mechanics
    of processing the data, finding clever query techniques and just the right visualization,
    only to have stakeholders ignore all of that and point out the one data inconsistency.
    Ensuring data quality can be one of the hardest and most frustrating parts of
    analysis. The saying “garbage in, garbage out” captures only part of the problem.
    Good ingredients in plus incorrect assumptions can also lead to garbage out.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及创建良好的分析时，数据质量是绝对关键的。尽管这似乎是显而易见的，但这是我多年从事数据工作中学到的最艰难的教训之一。过于关注数据处理的机制、找到巧妙的查询技术和恰到好处的可视化可能会导致利益相关者忽略所有这些，并指出一个数据不一致性。确保数据质量可能是分析中最难和最令人沮丧的部分之一。谚语“垃圾进，垃圾出”只捕捉了问题的一部分。好的输入再加上不正确的假设也可能导致垃圾输出。
- en: Comparing data against ground truth, or what is otherwise known to be true,
    is ideal though not always possible. For example, if you are working with a replica
    of a production database, you could compare the row counts in each system to verify
    that all rows arrived in the replica database. In other cases, you might know
    the dollar value and count of sales in a particular month and thus can query for
    this information in the database to make sure the `sum` of sales and `count` of
    records match. Often the difference between your query results and the expected
    value comes down to whether you applied the correct filters, such as excluding
    cancelled orders or test accounts; how you handled nulls and spelling anomalies;
    and whether you set up correct *JOIN* conditions between tables.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 与基本事实进行数据比较，或者已知为真的内容，是理想的，尽管并非总是可能的。例如，如果您正在使用生产数据库的副本工作，可以比较每个系统中的行数，以验证所有行是否已到达副本数据库。在其他情况下，您可能了解某个月销售的金额和数量，因此可以查询数据库以确保销售的`sum`和记录的`count`匹配。通常，您的查询结果与预期值之间的差异取决于您是否应用了正确的过滤器，例如排除取消订单或测试帐户；您如何处理空值和拼写异常；以及您是否在表之间设置了正确的*JOIN*条件。
- en: Profiling is a way to uncover data quality issues early on, before they negatively
    impact results and conclusions drawn from the data. Profiling reveals nulls, categorical
    codings that need to be deciphered, fields with multiple values that need to be
    parsed, and unusual datetime formats. Profiling can also uncover gaps and step
    changes in the data that have resulted from tracking changes or outages. Data
    is rarely perfect, and it’s often only through its use in analysis that data quality
    issues are uncovered.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 档案记录是在数据质量问题对结果和从数据中得出的结论产生负面影响之前早期发现数据质量问题的一种方法。档案记录揭示了空值，需要解密的分类编码，需要解析的具有多个值的字段以及不寻常的日期时间格式。档案记录还可以揭示由于跟踪更改或停机而导致的数据中的间隙和步变。数据很少是完美的，通常只有通过对数据的分析使用才能发现数据质量问题。
- en: Detecting Duplicates
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检测重复项
- en: A *duplicate* is when you have two (or more) rows with the same information.
    Duplicates can exist for any number of reasons. A mistake might have been made
    during data entry, if there is some manual step. A tracking call might have fired
    twice. A processing step might have run multiple times. You might have created
    it accidentally with a hidden many-to-many *JOIN*. However they come to be, duplicates
    can really throw a wrench in your analysis. I can recall times early in my career
    when I thought I had a great finding, only to have a product manager point out
    that my sales figure was twice the actual sales. It’s embarrassing, it erodes
    trust, and it requires rework and sometimes painstaking reviews of the code to
    find the problem. I’ve learned to check for duplicates as I go.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*重复* 是指有两行（或更多）信息相同的情况。重复项可能由于多种原因存在。在数据输入时可能出现错误，如果存在某些手动步骤。跟踪调用可能会触发两次。处理步骤可能会运行多次。您可能会意外创建隐藏的多对多*连接*。无论它们如何出现，重复项都可能严重影响您的分析。我记得在我职业生涯早期，我曾经认为发现了一个很好的发现，只是产品经理指出我的销售数字是实际销售的两倍。这令人尴尬，破坏了信任，并且需要重新工作，有时候还需要对代码进行痛苦的审查以找出问题。我已经学会了在进行时检查重复项。'
- en: 'Fortunately, it’s relatively easy to find duplicates in our data. One way is
    to inspect a sample, with all columns ordered:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，在我们的数据中找到重复项相对比较容易。一种方法是检查一个样本，所有列都有序：
- en: '[PRE12]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will reveal whether the data is full of duplicates, for example, when
    looking at a brand-new data set, when you suspect that a process is generating
    duplicates, or after a possible Cartesian *JOIN*. If there are only a few duplicates,
    they might not show up in the sample. And scrolling through data to try to spot
    duplicates is taxing on your eyes and brain. A more systematic way to find duplicates
    is to *SELECT* the columns and then `count` the rows (this might look familiar
    from the discussion of histograms!):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示数据是否充满重复记录，例如，在查看全新数据集时，当您怀疑某个过程正在生成重复记录时，或者在可能的笛卡尔*JOIN*之后。如果只有少量重复记录，则可能不会显示在样本中。滚动数据以尝试发现重复项对眼睛和大脑来说是一种负担。发现重复项的更系统的方法是*SELECT*列，然后`count`行数（这可能与直方图的讨论看起来很熟悉！）：
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This will tell you whether there are any cases of duplicates. If the query
    returns 0, you’re good to go. For more detail, you can list out the number of
    records (2, 3, 4, etc.):'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这将告诉您是否存在任何重复情况。如果查询返回 0，则一切正常。要获取更多详细信息，您可以列出记录的数量（2、3、4 等）：
- en: '[PRE14]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'As an alternative to a subquery, you can use a *HAVING* clause and keep everything
    in a single main query. Since it is evaluated after the aggregation and *GROUP
    BY*, *HAVING* can be used to filter on the aggregation value:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 作为子查询的替代方案，您可以使用*HAVING*子句并将所有内容保留在单个主查询中。由于它在聚合和*GROUP BY*之后进行评估，*HAVING*可用于在聚合值上进行过滤：
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: I prefer to use subqueries, because I find that they’re a useful way to organize
    my logic. [Chapter 8](ch08.xhtml#creating_complex_data_sets_for_analysis) will
    discuss order of evaluation and strategies for keeping your SQL queries organized.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我更喜欢使用子查询，因为我发现它们是组织逻辑的一种有用方式。[第 8 章](ch08.xhtml#creating_complex_data_sets_for_analysis)将讨论评估顺序和保持
    SQL 查询组织的策略。
- en: 'For full detail on which records have duplicates, you can list out all the
    fields and then use this information to chase down which records are problematic:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看哪些记录存在重复，您可以列出所有字段，然后使用这些信息来查找存在问题的记录：
- en: '[PRE16]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Detecting duplicates is one thing; figuring out what to do about them is another.
    It’s almost always useful to understand why duplicates are occurring and, if possible,
    fix the problem upstream. Can a data process be improved to reduce or remove duplication?
    Is there an error in an ETL process? Have you failed to account for a one-to-many
    relationship in a *JOIN*? Next, we’ll turn to some options for handling and removing
    duplicates with SQL.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 检测重复记录是一回事；解决它们又是另一回事。了解重复记录出现的原因几乎总是有用的，如果可能的话，修复上游问题。可以改进数据处理以减少或消除重复吗？是否存在
    ETL 过程中的错误？您是否未考虑*JOIN*中的一对多关系？接下来，我们将介绍一些处理和移除 SQL 中重复记录的选项。
- en: Deduplication with GROUP BY and DISTINCT
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 GROUP BY 和 DISTINCT 进行去重
- en: 'Duplicates happen, and they’re not always a result of bad data. For example,
    imagine we want to find a list of all the customers who have successfully completed
    a transaction so we can send them a coupon for their next order. We might *JOIN*
    the `customers` table to the `transactions` table, which would restrict the records
    returned to only those customers that appear in the `transactions` table:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 重复事件时常发生，而且它们并不总是由于数据质量问题引起的。例如，假设我们想要查找已成功完成交易的所有客户的列表，以便为他们的下一个订单发送优惠券。我们可能会*JOIN*
    `customers` 表和 `transactions` 表，这将限制返回的记录仅限于出现在 `transactions` 表中的客户：
- en: '[PRE17]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will return a row for each customer for each transaction, however, and
    there are hopefully at least a few customers who have transacted more than once.
    We have accidentally created duplicates, not because there is any underlying data
    quality problem but because we haven’t taken care to avoid duplication in the
    results. Fortunately, there are several ways to avoid this with SQL. One way to
    remove duplicates is to use the keyword *DISTINCT*:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为每个客户每个交易返回一行，但希望至少有几个客户进行了多次交易。我们意外地创建了重复记录，并非因为存在任何潜在的数据质量问题，而是因为我们没有注意避免结果中的重复。幸运的是，有几种方法可以在
    SQL 中避免这种情况。去除重复记录的一种方法是使用关键词*DISTINCT*：
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Another option is to use a *GROUP BY*, which, although typically seen in connection
    with an aggregation, will also deduplicate in the same way as *DISTINCT*. I remember
    the first time I saw a colleague use *GROUP BY* without an aggregation dedupe—I
    didn’t even realize it was possible. I find it somewhat less intuitive than *DISTINCT*,
    but the result is the same:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是使用 *GROUP BY*，虽然通常与聚合有关，但也会像 *DISTINCT* 一样进行去重。我记得第一次看到同事使用 *GROUP BY*
    进行去重时，我甚至没有意识到这是可能的。我觉得它比 *DISTINCT* 稍显不直观，但结果是一样的：
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Another useful technique is to perform an aggregation that returns one row
    per entity. Although technically not deduping, it has a similar effect. For example,
    if we have a number of transactions by the same customer and need to return one
    record per customer, we could find the `min` (first) and/or the `max` (most recent)
    `transaction_date`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种有用的技术是执行返回每个实体一行的聚合。虽然从技术上讲不是去重，但效果类似。例如，如果我们有同一客户的多笔交易并且需要返回每个客户的一条记录，我们可以找到`min`（第一个）和/或`max`（最近）的`transaction_date`：
- en: '[PRE20]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Duplicate data, or data that contains multiple records per entity even if they
    technically are not duplicates, is one of the most common reasons for incorrect
    query results. You can suspect duplicates as the cause if all of a sudden the
    number of customers or total sales returned by a query is many times greater than
    what you were expecting. Fortunately, there are several techniques that can be
    applied to prevent this from occurring.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 重复数据或包含多条记录每个实体的数据，即使它们在技术上不是重复的，是查询结果不正确的最常见原因之一。如果突然查询返回的客户数或总销售额比预期多许多倍，您可能会怀疑重复数据是原因。幸运的是，有几种技术可以应用来防止这种情况发生。
- en: Another common problem is missing data, which we’ll turn to next.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见问题是缺失数据，接下来我们会讨论。
- en: 'Preparing: Data Cleaning'
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作：数据清理
- en: Profiling often reveals where changes can make the data more useful for analysis. 
    Some of the steps are CASE transformations, adjusting for null, and changing data
    types.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 分析通常会揭示哪些变更可以使数据对分析更有用。一些步骤包括 CASE 转换、空值调整和数据类型更改。
- en: Cleaning Data with CASE Transformations
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 CASE 转换清洗数据
- en: CASE statements can be used to perform a variety of cleaning, enrichment, and
    summarization tasks. Sometimes the data exists and is accurate, but it would be
    more useful for analysis if values were standardized or grouped into categories.
    The structure of CASE statements was presented earlier in this chapter, in the
    section on binning.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: CASE 语句可以用于执行各种清洗、增强和汇总任务。有时数据存在且准确，但如果值被标准化或分组到类别中，则对分析更有用。CASE 语句的结构在本章早些时候已经介绍过，在分箱部分。
- en: Nonstandard values occur for a variety of reasons. Values might come from different
    systems with slightly different lists of choices, system code might have changed,
    options might have been presented to the customer in different languages, or the
    customer might have been able to fill out the value rather than pick from a list.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 非标准值由于各种原因而存在。值可能来自具有略有不同选项列表的不同系统，系统代码可能已更改，选项可能以不同语言呈现给客户，或者客户可能能够自行填写值而不是从列表中选择。
- en: 'Imagine a field containing information about the gender of a person. Values
    indicating a female person exist as “F,” “female,” and “femme.” We can standardize
    the values like this:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个字段包含有关个人性别的信息。表示女性的值包括“F”、“female”和“femme”。我们可以将值标准化如下：
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'CASE statements can also be used to add categorization or enrichment that does
    not exist in the original data. As an example, many organizations use a Net Promoter
    Score, or NPS, to monitor customer sentiment. NPS surveys ask respondents to rate,
    on a scale of 0 to 10, how likely they are to recommend a company or product to
    a friend or colleague. Scores of 0 to 6 are considered detractors, 7 and 8 are
    passive, and 9 and 10 are promoters. The final score is calculated by subtracting
    the percentage of detractors from the percentage of promoters. Survey result data
    sets usually include optional free text comments and are sometimes enriched with
    information the organization knows about the person surveyed. Given a data set
    of NPS survey responses, the first step is to group the responses into the categories
    of detractor, passive, and promoter:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: CASE 语句还可以用于添加原始数据中不存在的分类或增强功能。例如，许多组织使用净推荐值（Net Promoter Score，NPS）来监测客户情绪。NPS
    调查要求受访者评价他们有多大可能向朋友或同事推荐公司或产品，评分从 0 到 10。评分为 0 到 6 的人被视为贬损者，7 和 8 是中立者，9 和 10
    是推荐者。最终得分通过从推荐者百分比中减去贬损者百分比计算得出。调查结果数据集通常包括可选的自由文本评论，并有时会使用组织已知的被调查者信息进行增强。给定一个
    NPS 调查响应数据集，第一步是将响应分组为贬损者、中立者和推荐者类别：
- en: '[PRE22]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Note that the data type can differ between the field being evaluated and the
    return data type. In this case, we are checking an integer and returning a string.
    Listing out all the values with an IN list is also an option. The IN operator
    allows you to specify a list of items rather than having to write an equality
    for each one separately. It is useful when the input isn’t continuous or when
    values in order shouldn’t be grouped together:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在评估字段和返回数据类型之间可能存在数据类型差异。在这种情况下，我们正在检查一个整数并返回一个字符串。列出所有值并使用 IN 列表也是一种选择。IN
    运算符允许您指定一个项目列表，而不必为每个项目单独编写相等性。当输入不连续或值不应按顺序分组时，这是非常有用的：
- en: '[PRE23]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'CASE statements can consider multiple columns and can contain AND/OR logic.
    They can also be nested, though often this can be avoided with AND/OR logic:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: CASE 语句可以考虑多个列，并且可以包含 AND/OR 逻辑。它们也可以是嵌套的，虽然通常可以通过 AND/OR 逻辑避免这种情况：
- en: '[PRE24]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Another useful thing you can do with CASE statements is to create flags indicating
    whether a certain value is present, without returning the actual value. This can
    be useful during profiling for understanding how common the existence of a particular
    attribute is. Another use for flagging is during preparation of a data set for
    statistical analysis. In this case, a flag is also known as a dummy variable,
    taking a value of 0 or 1 and indicating the presence or absence of some qualitative
    variable. For example, we can create `is_female` and `is_promoter` flags with
    CASE statements on `gender` and `likelihood` (to recommend) fields:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可以使用 CASE 语句做的有用事情是创建指示某个值是否存在的标志，而不返回实际值。这在分析配置文件时非常有用，可以理解某个属性的存在有多普遍。标记的另一个用途是在准备用于统计分析的数据集时。在这种情况下，标志也称为虚拟变量，取值为
    0 或 1，指示某些定性变量的存在或不存在。例如，我们可以使用 CASE 语句在 `gender` 和 `likelihood`（推荐概率）字段上创建 `is_female`
    和 `is_promoter` 标志：
- en: '[PRE25]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'If you are working with a data set that has multiple rows per entity, such
    as with line items in an order, you can flatten the data with a CASE statement
    wrapped in an aggregate and turn it into a flag at the same time by using 1 and
    0 as the return value. We saw previously that a BOOLEAN data type is often used
    to create flags (fields that represent the presence or absence of some attribute).
    Here, 1 is substituted for TRUE and 0 is substituted for FALSE so that a `max`
    aggregation can be applied. The way this works is that for each customer, the
    CASE statement returns 1 for any row with a fruit type of “apple.” Then `max`
    is evaluated and will return the largest value from any of the rows. As long as
    a customer bought an apple at least once, the flag will be 1; if not, it will
    be 0:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在处理一个每个实体有多行的数据集，比如订单中的行项目，您可以使用包含在聚合函数中的 CASE 语句来展开数据，并同时将其转换为标志，返回值为 1
    或 0。我们之前看到 BOOLEAN 数据类型通常用于创建标志（表示某些属性的存在或不存在）。在这里，1 代表 TRUE，0 代表 FALSE，以便可以应用
    `max` 聚合函数。其工作原理是对于每个客户，CASE 语句对于水果类型为“苹果”的任何行都返回 1。然后计算 `max`，将从任何行中返回最大值。只要客户至少购买过一个苹果，标志就会是
    1；否则为 0：
- en: '[PRE26]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You can also construct more complex conditions for flags, such as requiring
    a threshold or amount of something before labeling with a value of 1:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以构建更复杂的条件以生成标志，例如在给定阈值或某物品数量后才进行标记为1的数值：
- en: '[PRE27]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: CASE statements are powerful, and as we saw, they can be used to clean, enrich,
    and flag or add dummy variables to data sets. In the next section, we’ll look
    at some special functions related to CASE statements that handle null values specifically.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: CASE语句非常强大，正如我们所见，它们可以用于清理、丰富数据集，并为数据集添加标志或添加虚拟变量。在下一节中，我们将看一些专门处理空值的CASE语句相关特殊函数。
- en: Type Conversions and Casting
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类型转换和转换
- en: Every field in a database is defined with a data type, which we reviewed at
    the beginning of this chapter. When data is inserted into a table, values that
    aren’t of the field’s type are rejected by the database. Strings can’t be inserted
    into integer fields, and booleans are not allowed in date fields. Most of the
    time, we can take the data types for granted and apply string functions to strings,
    date functions to dates, and so on. Occasionally, however, we need to override
    the data type of the field and force it to be something else. This is where type
    conversions and casting come in.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库中的每个字段都定义了一个数据类型，我们在本章开头进行了回顾。当数据插入表中时，不符合字段类型的值将被数据库拒绝。字符串不能插入到整数字段中，布尔值不能出现在日期字段中。大多数情况下，我们可以默认数据类型，并将字符串函数应用于字符串，日期函数应用于日期等等。然而偶尔我们需要覆盖字段的数据类型并强制其成为其他类型。这就是类型转换和转换的作用所在。
- en: '*Type conversion functions* allow pieces of data with the appropriate format
    to be changed from one data type to another. The syntax comes in a few forms that
    are basically equivalent. One way to change the data type is with the `cast` function,
    `cast (*input* as *data_type*)`, or two colons, `*input* :: *data_type*`. Both
    of these are equivalent and convert the integer 1,234 to a string:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*类型转换函数* 允许具有适当格式的数据片段从一种数据类型转换为另一种。语法有几种基本等效的形式。一种改变数据类型的方式是使用`cast`函数，`cast
    (*input* as *data_type*)`，或者使用两个冒号，`*input* :: *data_type*`。这两者是等效的，并将整数1,234转换为字符串：'
- en: '[PRE28]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Converting an integer to a string can be useful in CASE statements when categorizing
    numeric values with some unbounded upper or lower value. For example, in the following
    code, leaving the values that are less than or equal to 3 as integers while returning
    the string “4+” for higher values would result in an error:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 将整数转换为字符串在CASE语句中非常有用，用于对一些无上限或下限值的数值进行分类。例如，在以下代码中，保留小于或等于3的值为整数，同时对更高值返回字符串“4+”会导致错误：
- en: '[PRE29]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Casting the integers to the VARCHAR type solves the problem:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 将整数转换为VARCHAR类型可以解决这个问题：
- en: '[PRE30]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Type conversions also come in handy when values that should be integers are
    parsed out of a string, and then we want to aggregate the values or use mathematical
    functions on them. Imagine we have a data set of prices, but the values include
    the dollar sign ($), and so the data type of the field is VARCHAR.  We can remove
    the $ character with a function called `replace`, which will be discussed more
    during our look at text analysis in [Chapter 5](ch05.xhtml#text_analysis):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 类型转换在将本应为整数的值从字符串中解析出来后变得非常方便，然后我们希望对这些值进行聚合或者使用数学函数。假设我们有一个价格数据集，但这些值包含美元符号（$），因此字段的数据类型是VARCHAR。我们可以使用一个名为`replace`的函数来去除$字符，这将在我们研究文本分析中的[第5章](ch05.xhtml#text_analysis)中更详细地讨论：
- en: '[PRE31]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The result is still a VARCHAR, however, so trying to apply an aggregation will
    return an error. To fix this, we can `cast` the result as a FLOAT:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，结果仍然是VARCHAR类型，因此尝试应用聚合将返回错误。为了解决这个问题，我们可以将结果`cast`为FLOAT类型：
- en: '[PRE32]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Dates and datetimes can come in a bewildering array of formats, and understanding
    how to *cast* them to the desired format is useful. I’ll show a few examples on
    type conversion here, and [Chapter 3](ch03.xhtml#time_series_analysis) will go
    into more detail on date and datetime calculations. As a simple example, imagine
    that transaction or event data often arrives in the database as a TIMESTAMP, but
    we want to summarize some value such as transactions by day. Simply grouping by
    the timestamp will result in more rows than necessary. Casting the TIMESTAMP to
    a DATE reduces the size of the results and achieves our summarization goal:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 日期和日期时间可以以令人眼花缭乱的格式出现，了解如何将它们*转换*为所需格式是有用的。这里我会展示一些类型转换的示例，[第 3 章](ch03.xhtml#time_series_analysis)
    将更详细地介绍日期和日期时间的计算。举个简单的例子，想象一下，交易或事件数据通常以 TIMESTAMP 的形式到达数据库，但我们希望按天汇总某个值，例如交易。简单地按时间戳分组会导致不必要的行数。将
    TIMESTAMP 转换为 DATE 可减少结果的大小并实现我们的汇总目标：
- en: '[PRE33]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Likewise, a DATE can be cast to a TIMESTAMP when a SQL function requires a
    TIMESTAMP argument. Sometimes the year, month, and day are stored in separate
    columns, or they end up as separate elements because they’ve been parsed out of
    a longer string. These then need to be assembled back into a date. To do this,
    we use the concatenation operator || (double pipe) or `concat` function and then
    cast the result to a DATE. Any of these syntaxes works and returns the same value:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，当 SQL 函数需要 TIMESTAMP 参数时，DATE 可以转换为 TIMESTAMP。有时年、月和日存储在单独的列中，或者它们因从较长字符串中分析出来而成为单独的元素。然后需要将它们组装回日期。为此，我们使用连接运算符
    ||（双管道）或 `concat` 函数，然后将结果转换为 DATE。任何这些语法都有效且返回相同的值：
- en: '[PRE34]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Or equivalently:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 或者等价地：
- en: '[PRE35]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Yet another way to convert between string values and dates is by using the
    `date` function. For example, we can construct a string value as above and convert
    it into a date:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种在字符串值和日期之间进行转换的方法是使用 `date` 函数。例如，我们可以像上面那样构造一个字符串值，并将其转换为日期：
- en: '[PRE36]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The *to_datatype* functions can take both a value and a format string and thus
    give you more control over how the data is converted. [Table 2-4](#the_to_datatype_functions)
    summarizes the functions and their purposes. They are particularly useful when
    converting in and out of DATE or DATETIME formats, as they allow you to specify
    the order of the date and time elements.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*to_datatype* 函数可以同时接受值和格式字符串，从而更多地控制数据转换方式。[表 2-4](#the_to_datatype_functions)
    总结了这些函数及其用途。在转换进出 DATE 或 DATETIME 格式时特别有用，因为它们允许您指定日期和时间元素的顺序。'
- en: Table 2-4\. The to_datatype functions
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-4\. to_datatype 函数
- en: '| Function | Purpose |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 函数 | 用途 |'
- en: '| --- | --- |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `to_char` | Converts other types to string |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| `to_char` | 将其他类型转换为字符串。 |'
- en: '| `to_number` | Converts other types to numeric |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| `to_number` | 将其他类型转换为数值。 |'
- en: '| `to_date` | Converts other types to date, with specified date parts |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| `to_date` | 将其他类型转换为日期，并指定日期部分。 |'
- en: '| `to_timestamp` | Converts other types to date, with specified date and time
    parts |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| `to_timestamp` | 将其他类型转换为日期，并指定日期和时间部分。 |'
- en: Sometimes the database automatically converts a data type. This is called *type
    coercion*. For example, INT and FLOAT numerics can usually be used together in
    mathematical functions or aggregations without explicitly changing the type. CHAR
    and VARCHAR values can usually be mixed. Some databases will coerce BOOLEAN fields
    to 0 and 1 values, where 0 is FALSE and 1 is TRUE, but some databases require
    you to convert the values explicitly. Some databases are pickier than others about
    mixing dates and datetimes in result sets and functions. You can read through
    the documentation, or you can do some simple query experiments to learn how the
    database you’re working with handles data types implicitly and explicitly. There
    is usually a way to accomplish what you want, though sometimes you need to get
    creative in using functions in your queries.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库有时会自动转换数据类型。这称为*类型强制转换*。例如，INT 和 FLOAT 数值通常可以在数学函数或聚合中一起使用，而无需显式更改类型。CHAR
    和 VARCHAR 值通常可以混合使用。某些数据库会将 BOOLEAN 字段强制转换为 0 和 1 的值，其中 0 表示 FALSE，1 表示 TRUE，但某些数据库要求您显式转换值。某些数据库在混合日期和日期时间在结果集和函数中时要求更加严格。您可以阅读文档，或者进行一些简单的查询实验，了解您正在使用的数据库如何隐式和显式处理数据类型。通常有办法实现您想要的效果，尽管有时需要在查询中创造性地使用函数。
- en: 'Dealing with Nulls: coalesce, nullif, nvl Functions'
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理空值：coalesce、nullif、nvl 函数
- en: Null was one of the stranger concepts I had to get used to when I started working
    with data. Null just isn’t something we think about in daily life, where we’re
    used to dealing in concrete quantities of things. *Null* has a special meaning
    in databases and was introduced by Edgar Codd, the inventor of the relational
    database, to ensure that databases have a way to represent missing information.
    If someone asks me how many parachutes I have, I can answer “zero.” But if the
    question is never asked, I have null parachutes.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 当我开始处理数据时，Null 是我不得不适应的比较奇怪的概念之一。在日常生活中，我们习惯于处理具体的事物数量，而不是空值。*Null* 在数据库中有着特殊的含义，是关系数据库发明者埃德加·科德引入的，用于确保数据库有一种方式来表示丢失的信息。如果有人问我有多少个降落伞，我可以回答“零”。但如果从未问过这个问题，我就会有空的降落伞。
- en: Nulls can represent fields for which no data was collected or that aren’t applicable
    for that row. When new columns are added to a table, the values for previously
    created rows will be null unless explicitly filled with some other value. When
    two tables are joined via an *OUTER JOIN*, nulls will appear in any fields for
    which there is no matching record in the second table.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Null 值可以表示未收集到数据或对该行不适用的字段。当向表中添加新列时，以前创建的行的值将为空，除非显式填充为其他值。当两个表通过*外连接*进行连接时，任何在第二个表中找不到匹配记录的字段将显示为
    Null。
- en: Nulls are problematic for certain aggregations and groupings, and different
    types of databases handle them in different ways. For example, imagine I have
    five records, with 5, 10, 15, 20, and null. The sum of these is 50, but the average
    is either 10 or 12.5 depending on whether the null value is counted in the denominator.
    The whole question may also be considered invalid since one of the values is null.
    For most database functions, a null input will return a null output. Equalities
    and inequalities involving null also return null. A variety of unexpected and
    frustrating results can be output from your queries if you are not on the lookout
    for nulls.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Null 值对于某些聚合和分组是有问题的，不同类型的数据库以不同的方式处理它们。例如，假设我有五条记录，分别是 5、10、15、20 和 Null。这些数的总和是
    50，但平均值则是 10 或 12.5，这取决于是否将空值计入分母。整个问题也可能被认为无效，因为其中一个值是空的。对于大多数数据库函数，空输入将返回空输出。包含空值的等式和不等式也会返回空值。如果你对空值不加注意，你的查询可能会输出各种意料之外和令人沮丧的结果。
- en: When tables are defined, they can either allow nulls, reject nulls, or populate
    a default value if the field would otherwise be left null. In practice, this means
    that you can’t always rely on a field to show up as null if the data is missing,
    because it may have been filled with a default value such as 0\. I once had a
    long debate with a data engineer when it turned out that null dates in the source
    system were defaulting to “1970-01-01” in our data warehouse. I insisted that
    the dates should be null instead, to reflect the fact that they were unknown or
    not applicable. The engineer pointed out that I could remember to filter those
    dates or change them back to null with a CASE statement. I finally prevailed by
    pointing out that one day another user who wasn’t as aware of the nuances of default
    dates would come along, run a query, and get the puzzling cluster of customers
    about a year before the company was even founded.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 当定义表时，它们可以允许空值，拒绝空值，或在字段原本为空时填充默认值。实际操作中，这意味着你不能总是依赖字段显示为 Null，因为它可能已经填充了默认值，比如
    0。有一次我和一个数据工程师就源系统中的空日期默认为我们数据仓库中的“1970-01-01”进行了长时间的辩论。我坚持认为日期应该是空值，以反映其未知或不适用的事实。工程师指出，我可以记得筛选这些日期或使用
    CASE 语句将它们改回空值。最终，我成功地指出，总有一天会有另一个不那么了解默认日期细微差别的用户来运行查询，得到关于公司成立前约一年的令人困惑的客户群集。
- en: Nulls are often inconvenient or inappropriate for the analysis you want to do.
    They can also make output confusing to the intended audience for your analysis.
    Businesspeople don’t necessarily understand how to interpret a null value or may
    assume that null values represent a problem with data quality.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Null 值通常对你希望进行的分析是不方便的或不合适的。它们还可能使得分析结果对预期的受众变得混淆。商业人士未必了解如何解释空值，或者可能假设空值代表数据质量问题。
- en: 'There are a few ways to replace nulls with alternate values: CASE statements,
    and the specialized `coalesce` and `nullif` functions. We saw previously that
    CASE statements can check a condition and return a value. They can also be used
    to check for a null and, if one is found, replace it with another value:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以使用 CASE 语句和专用的`coalesce`和`nullif`函数替换空值。我们之前看到 CASE 语句可以检查条件并返回值。它们还可以用于检查空值，并且如果找到空值，则用另一个值替换它：
- en: '[PRE37]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The `coalesce` function is a more compact way to achieve this. It takes two
    or more arguments and returns the first one that is not null:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`coalesce` 函数是实现这一目标的更紧凑的方式。它接受两个或多个参数，并返回第一个非空参数：'
- en: '[PRE38]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Note
  id: totrans-210
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The function `nvl` exists in some databases and is similar to `coalesce`, but
    it allows only two arguments.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`nvl`存在于某些数据库中，类似于`coalesce`，但它只允许两个参数。
- en: 'The `nullif` function compares two numbers, and if they are not equal, it returns
    the first number; if they *are* equal, the function returns null. Running this
    code:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`nullif` 函数比较两个数字，如果它们不相等，则返回第一个数字；如果它们相等，则返回空值。运行此代码：'
- en: '[PRE39]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'returns 6, whereas null is returned by:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 6，而以下代码返回空值：
- en: '[PRE40]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '`nullif` is equivalent to the following, more wordy case statement:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`nullif` 等同于以下更冗长的 CASE 语句：'
- en: '[PRE41]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This function can be useful for turning values back into nulls when you know
    a certain default value has been inserted into the database. For example, with
    my default time example, we could change it back to null by using:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 当你知道某个默认值已经插入到数据库中时，此函数可以将值转换回空值。例如，对于我的默认时间示例，我们可以通过以下方式将其更改回空值：
- en: '[PRE42]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Warning
  id: totrans-220
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Nulls can be problematic when filtering data in the *WHERE* clause. Returning
    values that are null is fairly straightforward:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在*WHERE*子句中过滤数据时，空值可能会带来问题。返回空值的值相对直接：
- en: '[PRE43]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'However, imagine that `my_field` contains some nulls and also some names of
    fruits. I would like to return all rows that are not apples. It seems like this
    should work:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，假设`my_field`包含一些空值和一些水果名称。我希望返回所有不是苹果的行。看起来这应该可以工作：
- en: '[PRE44]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'However, some databases will exclude both the “apple” rows and all rows with
    null values in `my_field`. To correct this, the SQL should both filter out “apple”
    and explicitly include nulls by connecting the conditions with OR:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，某些数据库将排除所有“apple”行和`my_field`中的所有空值行。为了纠正这一点，SQL应该同时过滤“apple”并明确通过OR连接条件包含空值：
- en: '[PRE45]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Nulls are a fact of life when working with data. Regardless of why they occur,
    we often need to consider them in profiling and as targets for data cleaning.
    Fortunately, there are a number of ways to detect them with SQL, as well as several
    useful functions that allow us to replace nulls with alternate values. Next we’ll
    look at missing data, a problem that can cause nulls but has even wider implications
    and thus deserves a section of its own.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理数据时，空值是一个不可避免的事实。无论其产生的原因是什么，我们通常需要在分析中考虑它们，并将其作为数据清洗的目标。幸运的是，SQL有许多方法可以检测它们，并且有几个有用的函数可以用来将空值替换为其他值。接下来，我们将看一下缺失数据，这是一个可能导致空值的问题，但其影响更广泛，因此值得专门一节来讨论。
- en: Missing Data
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺失数据
- en: Data can be missing for a variety of reasons, each with its own implications
    for how you decide to handle the data’s absence. A field might not have been required
    by the system or process that collected it, as with an optional “how did you hear
    about us?” field in an ecommerce checkout flow. Requiring this field might create
    friction for the customer and decrease successful checkouts. Alternatively, data
    might normally be required but wasn’t collected due to a code bug or human error,
    such as in a medical questionnaire where the interviewer missed the second page
    of questions. A change in the way the data was collected can result in records
    before or after the change having missing values. A tool tracking mobile app interactions
    might add an additional field recording whether the interaction was a tap or a
    scroll, for example, or remove another field due to functionality change. Data
    can be orphaned when a table references a value in another table, and that row
    or the entire table has been deleted or is not yet loaded into the data warehouse.
    Finally, data may be available but not at the level of detail, or granularity,
    needed for the analysis. An example of this comes from subscription businesses,
    where customers pay on an annual basis for a monthly product and we want to analyze
    monthly revenue.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可能因各种原因而丢失，每种原因都会对您决定如何处理数据的缺失产生影响。例如，某个字段可能不是由收集它的系统或流程所要求的，就像在电子商务结账流程中可选的“您是如何知道我们的？”字段。要求填写此字段可能会给客户带来不便，并减少成功的结账次数。或者，由于代码错误或人为失误，例如在医疗问卷中面试者错过了第二页问题，通常需要数据但未收集。数据收集方式的变化可能导致在变更前或变更后的记录中存在缺失值。例如，跟踪移动应用交互的工具可能添加一个额外字段记录交互是点击还是滚动，或者由于功能变更而移除另一个字段。数据可能成为孤立数据，当一个表引用另一个表中的值，而该行或整个表已被删除或尚未加载到数据仓库中。最后，数据可能可用，但不足以进行所需的分析粒度。例如，订阅业务中，客户按年支付月度产品，我们希望分析每月收入。
- en: 'In addition to profiling the data with histograms and frequency analysis, we
    can often detect missing data by comparing values in two tables. For example,
    we might expect that each customer in the `transactions` table also has a record
    in the `customer` table. To check this, query the tables using a *LEFT JOIN* and
    add a *WHERE* condition to find the customers that do not exist in the second
    table:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用直方图和频率分析对数据进行分析外，我们还经常可以通过比较两个表中的值来检测缺失数据。例如，我们可能期望`transactions`表中的每个客户也在`customer`表中有记录。要检查这一点，可以使用*LEFT
    JOIN*查询这些表，并添加*WHERE*条件来查找不存在于第二个表中的客户：
- en: '[PRE46]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Missing data can be an important signal in and of itself, so don’t assume that
    it always needs to be fixed or filled. Missing data can reveal the underlying
    system design or biases in the data collection process.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失数据本身可能是一个重要的信号，因此不要假设它总是需要修复或填充。缺失数据可以揭示底层系统设计或数据收集过程中的偏见。
- en: Records with missing fields can be filtered out entirely, but often we want
    to keep them and instead make some adjustments based on what we know about expected
    or typical values. We have some options, called *imputation* techniques, for filling
    in missing data. These include filling with an average or median of the data set,
    or with the previous value. Documenting the missing data and how it was replaced
    is important, as this may impact the downstream interpretation and use of the
    data. Imputed values can be particularly problematic when the data is used in
    machine learning, for example.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 具有缺失字段的记录可以完全过滤掉，但通常我们希望保留它们，并根据我们对预期或典型值的了解进行一些调整。我们有一些选择，称为*填充*技术，用于填补缺失数据。这些包括用数据集的平均值或中位数填充，或者用前一个值填充。记录缺失数据及其替换方式的文档化非常重要，因为这可能会影响数据的下游解释和使用。例如，在机器学习中使用数据时，填充值可能特别有问题。
- en: 'A common option is to fill missing data with a constant value. Filling with
    a constant value can be useful when the value is known for some records even though
    they were not populated in the database. For example, imagine there was a software
    bug that prevented the population of the `price` for an item called “xyz,” but
    we know the price is always $20\. A CASE statement can be added to the query to
    handle this:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的选项是用常量值填充缺失数据。当某些记录的值已知但未在数据库中填充时，用常量值填充可能很有用。例如，假设由于软件错误导致未为名为“xyz”的项目填充`price`，但我们知道价格始终为$20。可以在查询中添加CASE语句来处理此问题：
- en: '[PRE47]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Another option is to fill with a derived value, either a mathematical function
    on other columns or a CASE statement. For example, imagine we have a field for
    the `net_sales` amount for each transaction. Due to a bug, some rows don’t have
    this field populated, but they do have the `gross_sales` and `discount` fields
    populated. We can calculate `net_sales` by subtracting `discount` from `gross_sales`:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是填充衍生值，可以是其他列的数学函数或CASE语句。例如，想象一下，我们为每笔交易的`net_sales`金额有一个字段。由于漏洞，有些行没有填充这个字段，但它们填充了`gross_sales`和`discount`字段。我们可以通过从`gross_sales`中减去`discount`来计算`net_sales`：
- en: '[PRE48]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Missing values can also be filled with values from other rows in the data set.
    Carrying over a value from the previous row is called *fill forward*, while using
    a value from the next row is called *fill backward*. These can be accomplished
    with the `lag` and `lead` window functions, respectively. For example, imagine
    that our transaction table has a `product_price` field that stores the undiscounted
    price a customer pays for a `product`. Occasionally this field is not populated,
    but we can make an assumption that the price is the same as the price paid by
    the last customer to buy that `product`. We can fill with the previous value using
    the `lag` function, *PARTITION BY* the `product` to ensure the price is pulled
    only from the same `product`, and *ORDER BY* the appropriate date to ensure the
    price is pulled from the most recent prior transaction:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用数据集中其他行的值来填充缺失的数值。从上一行传递数值称为*向前填充*，而从下一行传递数值称为*向后填充*。这可以通过`lag`和`lead`窗口函数实现，分别。例如，想象一下，我们的交易表有一个`product_price`字段，存储客户为`product`支付的未折扣价格。偶尔该字段未填充，但我们可以假设价格与上一个购买该`product`的客户支付的价格相同。我们可以使用`lag`函数在*PARTITION
    BY*`product`的条件下进行填充，以确保价格仅从相同的`product`中提取，并在适当的日期*ORDER BY*以确保价格是从最近的先前交易中提取的：
- en: '[PRE49]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The `lead` function could be used to fill with `product_price` for the following
    transaction. Alternatively, we could take the `avg` of prices for the `product`
    and use that to fill in the missing value. Filling with previous, next, or average
    values involves making some assumptions about typical values and what’s reasonable
    to include in an analysis. It’s always a good idea to check the results to make
    sure they are plausible and to note that you have interpolated the data when not
    available.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`lead`函数可用于在以下交易中使用`product_price`进行填充。或者，我们可以计算`product`的价格的平均值，并用其填充缺失的数值。使用上一个、下一个或平均值进行填充涉及对典型数值和合理包含在分析中的数值进行一些假设。检查结果确保其合理，并注明在数据不可用时进行了插值永远是一个好主意。'
- en: 'For data that is available but not at the granularity needed, we often have
    to create additional rows in the data set. For example, imagine we have a `customer_subscriptions`
    table with the fields `subscription_date` and `annual_amount`. We can spread this
    annual subscription amount into 12 equal monthly revenue amounts by dividing by
    12, effectively converting ARR (annual recurring revenue) into MRR (monthly recurring
    revenue):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 对于可用但粒度不够的数据，我们通常需要在数据集中创建额外的行。例如，假设我们有一个`customer_subscriptions`表，具有`subscription_date`和`annual_amount`字段。通过除以12，我们可以将这个年度订阅金额分成12个相等的月度收入金额，有效地将ARR（年度循环收入）转换为MRR（月度循环收入）。
- en: '[PRE50]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This gets a bit tedious, particularly if subscription periods can be two, three,
    or five years as well as one year. It’s also not helpful if what we want is the
    actual dates of the months. In theory we could write a query like this:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如果订阅期限可以为两年、三年或五年以及一年，这将变得有点繁琐。如果我们想要的是月份的实际日期，这也没有帮助。理论上，我们可以编写类似这样的查询：
- en: '[PRE51]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: However, if the data includes orders from customers across time, hardcoding
    the month names won’t be accurate. We could use CASE statements in combination
    with hardcoded month names, but again this is tedious and is likely to be error-prone
    as you add more convoluted logic. Instead, creating new rows through a *JOIN*
    to a table such as a date dimension provides an elegant solution.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果数据包括来自不同时间的客户的订单，硬编码月份名称将不准确。我们可以使用CASE语句与硬编码的月份名称结合使用，但这样做又很繁琐，而且随着添加更复杂的逻辑，很可能出错。相反，通过*JOIN*到日期维度表等表来创建新行提供了一个优雅的解决方案。
- en: A *date dimension* is a static table that has one row per day, with optional
    extended date attributes, such as day of the week, month name, end of month, and
    fiscal year. The dates extend far enough into the past and far enough into the
    future to cover all anticipated uses. Because there are only 365 or 366 days per
    year, tables covering even 100 years don’t take up a lot of space. [Figure 2-3](#a_date_dimension_table_with_date_attrib)
    shows a sample of the data in a date dimension table. Sample code to create a
    date dimension using SQL functions is on the book’s [GitHub site](https://oreil.ly/kv3dZ).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '*日期维度*是一个静态表，每天一行，可以包括扩展的日期属性，比如星期几、月份名、月末日期和财政年度。日期要足够远以覆盖所有预期的使用情况，从过去延伸到未来。由于每年只有365或366天，即使覆盖100年，表也不会占用很多空间。[图 2-3](#a_date_dimension_table_with_date_attrib)展示了日期维度表中数据的示例。创建日期维度的示例代码可以在本书的[GitHub网站](https://oreil.ly/kv3dZ)找到。'
- en: '![](Images/sfda_0203.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0203.png)'
- en: Figure 2-3\. A date dimension table with date attributes
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. 带有日期属性的日期维度表
- en: 'If you’re using a Postgres database, the `generate_series` function can be
    used to create a date dimension either to populate the table initially or if creating
    a table is not an option. It takes the following form:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是Postgres数据库，`generate_series`函数可以用来创建一个日期维度，无论是用来初始化表还是在创建表不可行的情况下。它的形式如下：
- en: '[PRE52]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'In this function, `*start*` is the first date you want in the series, `*stop*`
    is the last date, and `*step interval*` is the time period between values. The
    `*step interval*` can take any value, but one day is appropriate for a date dimension:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，`*start*`是系列中希望的第一个日期，`*stop*`是最后一个日期，`*step interval*`是值之间的时间间隔。`*step
    interval*`可以取任何值，但对于日期维度来说，一天是合适的：
- en: '[PRE53]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The `generate_series` function requires at least one of the arguments to be
    a TIMESTAMP, so “2000-01-01” is cast as a TIMESTAMP. We can then create a query
    that results in a row for every day, regardless of whether a customer ordered
    on a particular day. This is useful when we want to ensure that a customer is
    counted for each day, or when we specifically want to count or otherwise analyze
    days on which a customer did not make a purchase:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate_series`函数要求至少有一个参数是TIMESTAMP，因此将“2000-01-01”转换为TIMESTAMP。然后，我们可以创建一个查询，使每天都有一行，无论客户是否在特定日期下订单。当我们希望确保每天都计算一个客户时，或者当我们专门希望计算或分析客户没有购买的日期时，这非常有用：'
- en: '[PRE54]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Returning to our subscription example, we can use the date dimension to create
    a record for each month by *JOIN*ing the date dimension on dates that are between
    the `subscription_date` and 11 months later (for 12 total months):'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的订阅示例，我们可以使用日期维度来创建每个月的记录，通过将日期维度与订阅日期和11个月后（共12个月）之间的日期进行*JOIN*：
- en: '[PRE55]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Data can be missing for various reasons, and understanding the root cause is
    important in deciding how to deal with it. There are a number of options for finding
    and replacing missing data. These include using CASE statements to set default
    values, deriving values by performing calculations on other fields in the same
    row, and interpolating from other values in the same column.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可能因各种原因缺失，了解根本原因对于决定如何处理数据非常重要。有许多选项可以找到和替换缺失数据。这些选项包括使用CASE语句设置默认值，通过对同一行中的其他字段执行计算来导出值，以及从同一列中的其他值进行插值。
- en: Data cleaning is an important part of the data preparation process. Data may
    need to be cleaned for many different reasons. Some data cleaning needs to be
    done to fix poor data quality, such as when there are inconsistent or missing
    values in the raw data, while other data cleaning is done to make further analysis
    easier or more meaningful. The flexibility of SQL allows us to perform cleaning
    tasks in a variety of ways.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗是数据准备过程中的重要组成部分。数据可能因多种原因需要进行清洗。一些数据清洗是为了修复数据质量问题，比如原始数据中存在不一致或缺失值，而其他数据清洗则是为了使进一步的分析更容易或更有意义。SQL的灵活性允许我们以多种方式执行清洗任务。
- en: After data is cleaned, a common next step in the preparation process is shaping
    the data set.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗完成后，准备过程中的一个常见下一步是塑形数据集。
- en: 'Preparing: Shaping Data'
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作：数据塑形
- en: '*Shaping data* refers to manipulating the way the data is represented in columns
    and rows. Each table in the database has a shape. The result set of each query
    has a shape. Shaping data may seem like a rather abstract concept, but if you
    work with enough data, you will come to see its value. It is a skill that can
    be learned, practiced, and mastered.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '*塑造数据* 指的是操作数据在列和行中表示方式的过程。数据库中的每张表都有一个形状。每个查询的结果集也有一个形状。塑造数据可能看起来像是一个相当抽象的概念，但如果你处理足够多的数据，你会看到它的价值。这是一个可以学习、练习和掌握的技能。'
- en: One of the most important concepts in shaping data is figuring out the *granularity*
    of data that you need. Just as rocks can range in size from giant boulders down
    to grains of sand, and even further down to microscopic dust, so too can data
    have varying levels of detail. For example, if the population of a country is
    a boulder, then the population of a city is a small rock, and that of a household
    is a grain of sand. Data at a smaller level of detail might include individual
    births and deaths, or moves from one city or country to another.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在塑造数据中，一个最重要的概念是确定你需要的数据的*粒度*。就像岩石可以从大岩石到沙粒，甚至进一步到微观尘埃一样，数据的详细程度也可以有不同的层次。例如，如果一个国家的人口是一块大岩石，那么一个城市的人口是一块小石头，一个家庭的人口就是一粒沙子。在较小层次的详细数据可能包括个体的出生和死亡，或者从一个城市或国家搬到另一个城市或国家的迁移。
- en: '*Flattening data* is another important concept in shaping. This refers to reducing
    the number of rows that represent an entity, including down to a single row. Joining
    multiple tables together to create a single output data set is one way to flatten
    data. Another way is through aggregation.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '*展平数据* 是塑造中的另一个重要概念。这指的是减少表示实体的行数，甚至减少到单行。将多个表连接在一起创建单个输出数据集是展平数据的一种方法。另一种方法是通过聚合。'
- en: 'In this section, we’ll first cover some considerations for choosing data shapes.
    Then we’ll look at some common use cases: pivoting and unpivoting. We’ll see examples
    of shaping data for specific analyses throughout the remaining chapters. [Chapter 8](ch08.xhtml#creating_complex_data_sets_for_analysis)
    will go into more detail on keeping complex SQL organized when creating data sets
    for further analysis.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先会涵盖一些选择数据形状的考虑因素。然后我们将看一些常见的用例：透视和反透视。我们将在剩余章节中看到为特定分析塑造数据的示例。[第 8
    章](ch08.xhtml#creating_complex_data_sets_for_analysis) 将更详细地讨论在创建用于进一步分析的数据集时，如何保持复杂
    SQL 的组织结构。
- en: 'For Which Output: BI, Visualization, Statistics, ML'
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对于哪种输出：BI、可视化、统计、ML
- en: Deciding how to shape your data with SQL depends a lot on what you are planning
    to do with the data afterward. It’s generally a good idea to output a data set
    that has as few rows as possible while still meeting your need for granularity.
    This will leverage the computing power of the database, reduce the time it takes
    to move data from the database to somewhere else, and reduce the amount of processing
    you or someone else needs to do in other tools. Some of the other tools that your
    output might go to are a BI tool for reporting and dashboarding, a spreadsheet
    for business users to examine, a statistics tool such as R, or a machine learning
    model in Python—or you might output the data straight to a visualization created
    with a range of tools.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 决定如何使用 SQL 来塑造你的数据，很大程度上取决于之后你打算如何处理这些数据。通常情况下，输出尽可能少的行数，同时仍满足你对数据粒度的需求是个不错的主意。这样可以充分利用数据库的计算能力，减少从数据库到其他地方移动数据的时间，以及减少你或其他人需要在其他工具中进行的处理量。你的输出可能会进入一些其他工具，比如用于报告和仪表板的
    BI 工具，供业务用户检查的电子表格，诸如 R 这样的统计工具，或者 Python 中的机器学习模型，或者你可能直接将数据输出到用各种工具创建的可视化中。
- en: When outputting data to a business intelligence tool for reports and dashboards,
    it’s important to understand the use case. Data sets may need to be very detailed
    to enable exploration and slicing by end users. They may need to be small and
    aggregated and include specific calculations to enable fast loading and response
    times in executive dashboards. Understanding how the tool works, and whether it
    performs better with smaller data sets or is architected to perform its own aggregations
    across larger data sets, is important. There is no “one size fits all” answer.
    The more you know about how the data will be used, the better prepared you will
    be to shape the data appropriately.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 当将数据输出到商业智能工具以生成报告和仪表板时，理解使用场景非常重要。数据集可能需要非常详细，以便用户进行探索和切片。它们可能需要小而聚合，并包括特定的计算，以便在执行仪表板中实现快速加载和响应时间。了解工具的工作方式，以及它是否更适合处理较小数据集，还是设计成在较大数据集上执行自己的聚合，都非常重要。没有“一刀切”的答案。你对数据如何使用的了解越多，就越能准备好适当地塑造数据。
- en: Smaller, aggregated, and highly specific data sets often work best for visualizations,
    whether they are created in commercial software or using a programming language
    like R, Python, or JavaScript. Think about the level of aggregation and slices,
    or various elements, the end users will need to filter on. Sometimes the data
    sets require a row for each slice, as well as an “everything” slice. You may need
    to *UNION* together two queries—one at the detail level and one at the “everything”
    level.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 对于可视化，通常最适合使用更小、聚合和高度特定的数据集，无论是在商业软件中创建还是使用像 R、Python 或 JavaScript 这样的编程语言。考虑最终用户将需要对其进行过滤的聚合和切片级别，或各种元素。有时，数据集需要每个切片一个行，以及一个“全部”切片。你可能需要将两个查询
    *UNION* 在一起——一个在详细级别，一个在“全部”级别。
- en: 'When creating output for statistics packages or machine learning models, it’s
    important to understand the core entity being studied, the level of aggregation
    desired, and the attributes or features needed. For example, a model might need
    one record per customer with several attributes, or a record per transaction with
    its associated attributes as well as customer attributes. Generally, the output
    for modeling will follow the notion of “tidy data” proposed by Hadley Wickham.^([2](ch02.xhtml#ch01fn4))
    Tidy data has these properties:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在为统计软件包或机器学习模型创建输出时，理解所研究的核心实体、所需的聚合级别和属性或特征非常重要。例如，模型可能需要每个客户一个记录，带有多个属性，或者每个交易一个记录，带有其关联的属性以及客户属性。通常，建模输出将遵循
    Hadley Wickham 提出的“整洁数据”概念。^([2](ch02.xhtml#ch01fn4)) 整洁数据具有以下特性：
- en: Each variable forms a column.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个变量形成一列。
- en: Each observation forms a row.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个观察结果形成一行。
- en: Each value is a cell.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个数值都是一个单元格。
- en: We will next look at how to use SQL to transform data from the structure in
    which it exists in your database into any other pivoted or unpivoted structure
    that is needed for analysis.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将看看如何使用 SQL 将数据从数据库中的现有结构转换为分析所需的任何其他透视或非透视结构。
- en: Pivoting with CASE Statements
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 CASE 语句进行数据透视
- en: A *pivot table* is a way to summarize data sets by arranging the data into rows,
    according to the values of an attribute, and columns, according to the values
    of another attribute. At the intersection of each row and column, a summary statistic
    such as `sum`, `count`, or `avg` is calculated. Pivot tables are often a good
    way to summarize data for business audiences, since they reshape the data into
    a more compact and easily understandable form. Pivot tables are widely known from
    their implementation in Microsoft Excel, which has a drag-and-drop interface to
    create the summaries of data.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据透视表* 是一种通过根据一个属性的值排列数据行，并根据另一个属性的值排列数据列的方式来汇总数据集的方法。在每行和列的交汇处，计算汇总统计数据，如
    `sum`、`count` 或 `avg`。数据透视表通常是总结业务数据的好方法，因为它们将数据重塑为更紧凑和易于理解的形式。数据透视表因其在 Microsoft
    Excel 中的实现而广为人知，该软件具有拖放界面用于创建数据摘要。'
- en: 'Pivot tables, or pivoted output, can be created in SQL using a CASE statement
    along with one or more aggregation functions. We’ve seen CASE statements several
    times so far, and reshaping data is another major use case for them. For example,
    imagine we have an `orders` table with a row for each purchase made by customers.
    To flatten the data, *GROUP BY* the `customer_id` and `sum` the `order_amount`:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 CASE 语句和一个或多个聚合函数在 SQL 中创建透视表或透视输出。到目前为止，我们已经多次看到 CASE 语句，重塑数据是它们的另一个主要用途。例如，假设我们有一个
    `orders` 表，每个客户的每次购买都有一行。要扁平化数据，*GROUP BY* `customer_id` 并 `sum` `order_amount`：
- en: '[PRE56]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'To create a pivot, we will additionally create columns for each of the values
    of an attribute. Imagine the `orders` table also has a `product` field that contains
    the type of item purchased and the `order_date`. To create pivoted output, *GROUP
    BY* the `order_date`, and `sum` the result of a CASE statement that returns the
    `order_amount` whenever the row meets the product name criteria:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个透视，我们还将为属性的每个值创建列。假设 `orders` 表还有一个包含购买的产品类型和 `order_date` 的 `product`
    字段。要创建透视输出，*GROUP BY* `order_date`，并 `sum` 一个 CASE 语句的结果，该语句在满足产品名称条件的行时返回 `order_amount`：
- en: '[PRE57]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Note that with the `sum` aggregation, you can optionally use “else 0” to avoid
    nulls in the result set. With `count` or `count distinct`, however, you should
    not include an ELSE statement, as doing so would inflate the result set. This
    is because the database won’t count a null, but it will count a substitute value
    such as zero.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在 `sum` 聚合中，您可以选择使用 "else 0" 来避免结果集中的空值。然而，在 `count` 或 `count distinct`
    中，不应包含 ELSE 语句，因为这样做会增加结果集。这是因为数据库不会计算空值，但会计算零等替代值。
- en: Pivoting with CASE statements is quite handy, and having this ability opens
    up data warehouse table designs that are long and narrow rather than wide, which
    can be better for storing sparse data, because adding columns to a table can be
    an expensive operation. For example, rather than storing various customer attributes
    in many different columns, a table could contain multiple records per customer,
    with each attribute in a separate row, and with `attribute_name` and `attribute_value`
    fields specifying what the attribute is and its value. The data can then be pivoted
    as needed to assemble a customer record with the desired attributes. This design
    is efficient when there are many sparse attributes (only a subset of customers
    have values for many of the attributes).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CASE 语句进行透视非常方便，有了这种能力，可以打开数据仓库表设计的新局面，它们是长而窄的，而不是宽的，这对存储稀疏数据可能更好，因为向表添加列可能是一个昂贵的操作。例如，与其在许多不同的列中存储各种客户属性，不如在一张表中为每个客户包含多条记录，每个属性在一个单独的行中，具有
    `attribute_name` 和 `attribute_value` 字段指定属性及其值。然后可以根据需要对数据进行透视，以组装具有所需属性的客户记录。当存在许多稀疏属性（只有少数客户对许多属性具有值）时，这种设计非常有效。
- en: Pivoting data with a combination of aggregation and CASE statements works well
    when there are a finite number of items to pivot. For people who have worked with
    other programming languages, it’s essentially looping, but written out explicitly
    line by line. This gives you a lot of control, such as if you want to calculate
    different metrics in each column, but it can also be tedious. Pivoting with case
    statements doesn’t work well when new values arrive constantly or are rapidly
    changing, since the SQL code would need to be constantly updated. In those cases,
    pushing the computing to another layer of your analysis stack, such as a BI tool
    or statistical language, may be more appropriate.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要旋转的项目数量有限时，结合聚合和 CASE 语句进行数据透视非常有效。对于那些已经使用过其他编程语言的人来说，这本质上是循环，但是一行一行地明确写出来。这样做可以给你很多控制权，比如如果你想在每一列计算不同的指标，但这也可能会很乏味。使用
    CASE 语句进行透视在新值不断到来或者变化迅速时效果不佳，因为 SQL 代码需要经常更新。在这些情况下，将计算推向分析堆栈的另一层，比如 BI 工具或统计语言，可能更合适。
- en: Unpivoting with UNION Statements
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 UNION 语句进行反向旋转
- en: Sometimes we have the opposite problem and need to move data stored in columns
    into rows instead to create tidy data. This operation is called *unpivoting*.
    Data sets that may need unpivoting are those that are in a pivot table format.
    As an example, the populations of North American countries at 10-year intervals
    starting in 1980 are shown in [Figure 2-4](#country_population_by_year_left_parenth).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们遇到相反的问题，需要将存储在列中的数据转换为行，以创建整洁的数据。这个操作称为*unpivoting*。可能需要进行unpivoting的数据集是那些处于数据透视表格式的数据。例如，展示了从1980年开始每10年的北美国家的人口数据在[图2-4](#country_population_by_year_left_parenth)中。
- en: '![](Images/sfda_0204.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0204.png)'
- en: Figure 2-4\. Country population by year (in thousands)**^([3](ch02.xhtml#ch01fn5))**
  id: totrans-286
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4\. 按年份的国家人口（单位：千）
- en: 'To turn this into a result set with a row per country per year, we can use
    a *UNION* operator. *UNION* is a way to combine data sets from multiple queries
    into a single result set. There are two forms, *UNION* and *UNION ALL*. When using
    *UNION* or *UNION ALL*, the numbers of columns in each component query must match.
    The data types must match or be compatible (integers and floats can be mixed,
    but integers and strings cannot). The column names in the result set come from
    the first query. Aliasing the fields in the remaining queries is therefore optional
    but can make a query easier to read:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 要将其转换为每个国家每年一行的结果集，我们可以使用*UNION*运算符。*UNION*是将多个查询的数据集合并为单个结果集的一种方式。有两种形式，*UNION*和*UNION
    ALL*。在使用*UNION*或*UNION ALL*时，每个组成查询中的列数必须匹配。数据类型必须匹配或兼容（整数和浮点数可以混合使用，但整数和字符串不能）。结果集中的列名来自第一个查询。因此，对剩余查询中的字段进行别名是可选的，但可以使查询更易读：
- en: '[PRE58]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'In this example, we use a constant to hardcode the year, in order to keep track
    of the year that the population value corresponds to. The hardcoded values can
    be of any type, depending on your use case. You may need to explicitly cast certain
    hardcoded values, such as when entering a date:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用一个常数来硬编码年份，以便跟踪人口值对应的年份。硬编码的值可以是任何类型，具体取决于您的用例。您可能需要显式转换某些硬编码的值，例如在输入日期时：
- en: '[PRE59]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: What is the difference between *UNION* and *UNION ALL*? Both can be used to
    append or stack data together in this fashion, but they are slightly different.
    *UNION* removes duplicates from the result set, whereas *UNION ALL* retains all
    records, whether duplicates or not. *UNION ALL* is faster, since the database
    doesn’t have to do a pass over the data to find duplicates. It also ensures that
    every record ends up in the result set. I tend to use *UNION ALL*, using *UNION*
    only when I have a reason to suspect duplicate data.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '*UNION*和*UNION ALL*之间有什么区别？两者都可以用于以这种方式追加或堆叠数据，但它们略有不同。*UNION*从结果集中删除重复项，而*UNION
    ALL*保留所有记录，无论是否重复。*UNION ALL*速度更快，因为数据库不需要对数据进行一次遍历以查找重复项。它还确保每条记录最终都出现在结果集中。我倾向于使用*UNION
    ALL*，只有在怀疑有重复数据时才使用*UNION*。'
- en: '*UNION*ing data can also be useful for bringing together data from different
    sources. For example, imagine we have a `populations` table with yearly data per
    country, and another `gdp` table with yearly gross domestic product, or GDP. One
    option is to *JOIN* the tables and obtain a result set with one column for population
    and another for GDP:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '*UNION*数据也可以有助于将来自不同来源的数据整合在一起。例如，假设我们有一个每年数据表`populations`按国家统计的数据，还有一个年度国内生产总值或GDP的表`gdp`。一种选择是将这些表*JOIN*起来，得到一个包含人口和GDP两列的结果集：'
- en: '[PRE60]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Another option is to *UNION ALL* the data sets so that we end up with a stacked
    data set:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是将所有数据集*UNION ALL*起来，从而得到一个堆叠的数据集：
- en: '[PRE61]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Which approach you use largely depends on the output that you need for your
    analysis. The latter option can be useful when you have a number of different
    metrics in different tables and no single table has a full set of entities (in
    this case, countries). This is an alternative approach to a *FULL OUTER JOIN*.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 您选择的方法主要取决于您分析所需的输出。当您在不同的表中具有多个不同的指标且没有任何单个表具有完整的实体集（在本例中是国家）时，后一种选择可能很有用。这是*FULL
    OUTER JOIN*的一种替代方法。
- en: pivot and unpivot Functions
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: pivot和unpivot函数
- en: 'Recognizing that the pivot and unpivot use cases are common, some database
    vendors have implemented functions to do this with fewer lines of code. Microsoft
    SQL Server and Snowflake have `pivot` functions that take the form of extra expressions
    in the *WHERE* clause. Here, aggregation is any aggregation function, such as
    `sum` or `avg`, the `value_column` is the field to be aggregated, and a column
    will be created for each value of the `label_column` listed as a label:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到 pivot 和 unpivot 的使用案例很常见，一些数据库供应商已经实现了可以用更少代码完成此操作的函数。Microsoft SQL Server
    和 Snowflake 有 `pivot` 函数，它们在 *WHERE* 子句中以额外的表达式形式出现。在这里，聚合可以是任何聚合函数，如 `sum` 或
    `avg`，`value_column` 是要聚合的字段，而每个 `label_column` 列出的标签将创建一个列：
- en: '[PRE62]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We could rewrite the earlier pivoting example that used CASE statements as
    follows:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重写之前使用 CASE 语句的旋转示例，如下所示：
- en: '[PRE63]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Although this syntax is more compact than the CASE construction we saw earlier,
    the desired columns still need to be specified. As a result, `pivot` doesn’t solve
    the problem of newly arriving or rapidly changing sets of fields that need to
    be turned into columns. Postgres has a similar `crosstab` function, available
    in the `tablefunc` module.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种语法比我们之前看到的 CASE 结构更紧凑，但仍然需要指定所需的列。因此，`pivot` 并未解决需要转换为列的新到达或快速变化的字段集的问题。Postgres
    具有类似的 `crosstab` 函数，可在 `tablefunc` 模块中使用。
- en: 'Microsoft SQL Server and Snowflake also have `unpivot` functions that work
    in a similar fashion to expressions in the *WHERE* clause and transform rows into
    columns:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft SQL Server 和 Snowflake 也有 `unpivot` 函数，其工作方式与 *WHERE* 子句中的表达式类似，并将行转换为列：
- en: '[PRE64]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'For example, the `country_populations` data from the previous example could
    be reshaped in the following manner:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，前面示例中的 `country_populations` 数据可以按以下方式重塑：
- en: '[PRE65]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Here again the syntax is more compact than the *UNION* or *UNION ALL* approach
    we looked at earlier, but the list of columns must be specified in the query.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的语法比我们之前看到的 *UNION* 或 *UNION ALL* 方法更紧凑，但必须在查询中指定列的列表。
- en: 'Postgres has an `unnest` array function that can be used to unpivot data, thanks
    to its array data type. An array is a collection of elements, and in Postgres
    you can list the elements of an array in square brackets. The function can be
    used in the *SELECT* clause and takes this form:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: Postgres 具有 `unnest` 数组函数，可以通过其数组数据类型对数据进行 unpivot 处理。数组是元素的集合，在 Postgres 中，可以在方括号中列出数组的元素。该函数可以在
    *SELECT* 子句中使用，并采用以下形式：
- en: '[PRE66]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Returning to our earlier example with countries and populations, this query
    returns the same result as the query with the repeated *UNION ALL* clauses:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 回到早期的国家和人口示例，这个查询返回与使用重复的 *UNION ALL* 子句的查询相同的结果：
- en: '[PRE67]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Data sets arrive in many different formats and shapes, and they aren’t always
    in the format needed in our output. There are several options for reshaping data
    through pivoting or unpivoting it, either with CASE statements or *UNION*s, or
    with database-specific functions. Understanding how to manipulate your data in
    order to shape it in the way you want will give you greater flexibility in your
    analysis and in the way you present your results.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集以许多不同的格式和形状到达，并且它们并不总是以我们输出所需的格式。通过旋转或反转数据来重新塑造它们有几种选项，可以使用 CASE 语句、*UNION*
    或特定于数据库的函数。了解如何操作数据以按照您想要的方式进行塑形，将为您的分析和结果呈现方式提供更大的灵活性。
- en: Conclusion
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Preparing data for analysis can feel like the work you do before you get to
    the real work of analysis, but it is so fundamental to understanding the data
    that I always find it is time well spent. Understanding the different types of
    data you’re likely to encounter is critical, and you should take the time to understand
    the data types in each table you work with. Profiling data helps us learn more
    about what is in the data set and examine it for quality. I often return to profiling
    throughout my analysis projects, as I learn more about the data and need to check
    my query results along the way as I build in complexity. Data quality will likely
    never stop being a problem, so we’ve looked at some ways to handle the cleaning
    and enhancement of data sets. Finally, knowing how to shape the data to create
    the right output format is essential. We’ll see these topics recur in the context
    of various analyses throughout the book. The next chapter, on time series analysis,
    starts our journey into specific analysis techniques.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 准备数据进行分析可能感觉像是你在真正进行分析之前的工作，但它对于理解数据非常基础，我总是觉得这是值得花时间的事情。理解可能遇到的不同数据类型至关重要，你应该花时间了解每张表中的数据类型。数据概要分析帮助我们更多地了解数据集的内容，并检查其质量。在我的分析项目中，我经常回头进行概要分析，因为我在构建复杂查询时了解到更多数据并需要检查我的查询结果。数据质量可能永远是个问题，因此我们已经看过一些处理和增强数据集的方法。最后，了解如何塑造数据以创建正确的输出格式是至关重要的。我们将在本书中的各种分析上下文中反复看到这些主题。下一章，关于时间序列分析，将开始我们对特定分析技术的探索。
- en: '^([1](ch02.xhtml#ch01fn3-marker)) John W. Tukey, *Exploratory Data Analysis*
    (Reading, MA: Addison-Wesley, 1977).'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.xhtml#ch01fn3-marker)) 约翰·W·图基，《探索性数据分析》（雷丁，马萨诸塞州：Addison-Wesley，1977年）。
- en: '^([2](ch02.xhtml#ch01fn4-marker)) Hadley Wickham, “Tidy Data,” *Journal of
    Statistical Software* 59, no. 10 (2014): 1–23, *[*https://doi.org/10.18637/jss.v059.i10*](https://doi.org/10.18637/jss.v059.i10)*.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch02.xhtml#ch01fn4-marker)) 哈德利·维克姆，《整洁数据》，《统计软件杂志》第59卷，第10期（2014年）：1–23，[*https://doi.org/10.18637/jss.v059.i10*](https://doi.org/10.18637/jss.v059.i10)。
- en: ^([3](ch02.xhtml#ch01fn5-marker)) US Census Bureau, “International Data Base
    (IDB),” last updated December 2020, [*https://www.census.gov/data-tools/demo/idb*](https://www.census.gov/data-tools/demo/idb).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch02.xhtml#ch01fn5-marker)) 美国人口调查局，《国际数据数据库（IDB）》，最近更新于2020年12月，[*https://www.census.gov/data-tools/demo/idb*](https://www.census.gov/data-tools/demo/idb)。
