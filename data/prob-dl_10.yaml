- en: 7 Bayesian learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 贝叶斯学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Identifying extrapolation as the Achilles heel of DL
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将外推识别为深度学习的阿基里斯之踵
- en: A gentle introduction to Bayesian modeling
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯建模的温和介绍
- en: The concept of model uncertainty, which is called epistemic uncertainty
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型不确定性的概念，称为认知不确定性
- en: The Bayesian approach as a state-of-the-art method to dealing with parameter
    uncertainty
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯方法作为处理参数不确定性的最先进方法
- en: '![](../Images/7-unnumb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![7-unnumb](../Images/7-unnumb.png)'
- en: This chapter introduces Bayesian models. Besides the likelihood approach, the
    Bayesian approach is the most important method to fit the parameters of a probabilistic
    model and to estimate the associated parameter uncertainty. The Bayesian modeling
    approach incorporates an additional kind of uncertainty, called epistemic uncertainty.
    You will see that incorporating epistemic uncertainty results in better prediction
    performance and, more appropriately, quantification of the uncertainty of the
    predicted outcome distribution. The epistemic uncertainty becomes especially important
    when applying prediction models to situations not seen during the training. In
    regression, this is known as extrapolation.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了贝叶斯模型。除了似然方法之外，贝叶斯方法是最重要的方法来拟合概率模型的参数并估计相关的参数不确定性。贝叶斯建模方法还包含一种额外的不确定性，称为认知不确定性。您将看到，引入认知不确定性可以导致更好的预测性能，并且更恰当地量化预测结果分布的不确定性。当将预测模型应用于训练期间未见过的情境时，认知不确定性变得尤为重要。在回归中，这被称为外推。
- en: You’ll see in this chapter that traditional non-Bayesian models don’t express
    uncertainties if these are trained with few data or used for extrapolation. But
    Bayesian models do. Therefore, it’s better to use Bayesian approaches when you
    have little training data or when you can’t rule out that you’ll encounter situations
    not seen in training--like an elephant in a room (see the figure at the beginning
    of this chapter). You’ll also see that even state-of-the-art DL models like those
    that win the ImageNet challenge are usually great in classifying elephants, but
    these are not able to classify correctly an elephant in the room. Instead, a wrong
    class is predicted, often with a high probability. This inability to communicate
    uncertainty when dealing with new situations and so produce unreliable predictions
    are a serious deficit of non-Bayesian DL models. Bayesian DL models, on the other
    hand, have the ability to express uncertainty.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 您在本章中会看到，如果这些传统非贝叶斯模型用少量数据进行训练或用于外推，它们不会表达不确定性。但贝叶斯模型会。因此，当您有少量训练数据或无法排除遇到训练中未见过的情境时（如房间里的象，参见本章开头的图），使用贝叶斯方法会更好。您还会看到，即使是像赢得ImageNet挑战的顶级深度学习模型，在分类象方面通常表现很好，但这些模型无法正确分类房间里的象。相反，它们会预测一个错误的类别，通常概率很高。在处理新情境时无法传达不确定性并因此产生不可靠的预测，这是非贝叶斯深度学习模型的一个严重缺陷。另一方面，贝叶斯深度学习模型具有表达不确定性的能力。
- en: In this chapter, you’ll learn the Bayesian modeling approach and apply it to
    simple models; for example, treating coin tosses in a Bayesian way as a Hello
    World-like example. Further, you’ll also apply Bayesian modeling to linear regression.
    It turns out that more complex models like NNs need approximations to the Bayesian
    modeling approach. We’ll cover these in chapter 8\. This chapter is about understanding
    the principles of Bayesian modeling. Before diving into the Bayesian modeling
    approach, let’s see what’s wrong with the traditional, non-Bayesian NN models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习贝叶斯建模方法并将其应用于简单模型；例如，将抛硬币以贝叶斯方式作为类似Hello World的示例。此外，您还将应用贝叶斯建模到线性回归。结果证明，像神经网络这样的更复杂模型需要对贝叶斯建模方法进行近似。我们将在第8章中介绍这些内容。本章是关于理解贝叶斯建模原理的。在深入贝叶斯建模方法之前，让我们看看传统的非贝叶斯神经网络模型有什么问题。
- en: '7.1 What’s wrong with non-Bayesian DL: The elephant in the room'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 非贝叶斯深度学习的问题：房间里的象
- en: You’ll see in this section that DL models can sometimes tell (with innocent
    and proud confidence) a completely wrong story. We show two examples, one from
    regression and one from classification, in which non-Bayesian DL fails. For regression,
    we provide you with a simple, one-dimensional toy example, and you’ll immediately
    understand the reason why it fails. For classification, the example is the usual
    image classification, and thus, more complex, but the principle remains the same.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你会发现深度学习模型有时会（带着天真和自信）讲述一个完全错误的故事。我们展示了两个例子，一个是回归，另一个是分类，其中非贝叶斯深度学习失败了。对于回归，我们提供了一个简单的一维玩具示例，你将立即理解它失败的原因。对于分类，例子是通常的图像分类，因此更复杂，但原则是一样的。
- en: Usually, the predictions of traditional DL models are highly reliable when applied
    to the same data used in training, which somehow can lull you into a false sense
    of security. Bayesian modeling helps to alert you of potentially wrong predictions.
    Before addressing the weaknesses of traditional NN models, however, let’s first
    recall the successes of DL.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，传统的深度学习模型的预测在应用于训练中使用的相同数据时非常可靠，这可能会让你产生一种虚假的安全感。贝叶斯建模有助于提醒你潜在的错误预测。然而，在解决传统神经网络模型的弱点之前，我们首先回顾一下深度学习的成功。
- en: 'Let’s move back to a time when DL has not had its breakthrough. We are in the
    year 2012; the iPhone is just five years old. Here’s what we’re looking at:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到深度学习还没有取得突破的时代。我们是在2012年；iPhone才五岁。我们正在看的是：
- en: There’s no possibility of text-to-speech with reasonable performance.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有可能实现具有合理性能的文本到语音转换。
- en: Computers aren’t good at recognizing your handwriting.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机不擅长识别你的手写体。
- en: A team of linguists is required to develop a translation program.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要一个语言学团队来开发一个翻译程序。
- en: Machines can’t make sense of photos.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器无法理解照片。
- en: In figure 7.1, you see two examples that can be correctly classified by the
    VGG16 network. The VGG16 network, introduced in 2014, is an early network that
    started the DL revolution for image classification. You can use the following
    notebook to do the classifications yourself.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在图7.1中，你可以看到两个可以被VGG16网络正确分类的例子。VGG16网络是在2014年引入的，是早期开始图像分类深度学习革命的神经网络。你可以使用以下笔记本来自行进行分类。
- en: '![](../Images/7-1.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片7-1](../Images/7-1.png)'
- en: Figure 7.1 Good cases of DL. The left image shows a dog that’s correctly classified
    as an Affenpinscher. The right image shows an elephant that’s classified as a
    Tusker (an elephant species). The dog image is taken from [http://mng.bz/PABn](http://mng.bz/PABn)
    and the elephant image from [http://mng.bz/JyWV](http://mng.bz/JyWV) .
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：深度学习的良好案例。左侧图像展示了一只被正确分类为猴面犬的狗。右侧图像展示了一头被分类为象种（长鼻象）的大象。狗的图像来自[http://mng.bz/PABn](http://mng.bz/PABn)，大象的图像来自[http://mng.bz/JyWV](http://mng.bz/JyWV)。
- en: '| ![](../Images/computer-icon.png) | Hands-on time Open [http://mng.bz/1zZj](http://mng.bz/1zZj)
    and follow the notebook. It produces the figures in this chapter for this section.
    Try to understand what happened in the notebook. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| ![计算机图标](../Images/computer-icon.png) | 实践时间 打开[http://mng.bz/1zZj](http://mng.bz/1zZj)并遵循笔记本。它为这一节生成了本章的图像。尝试理解笔记本中发生的事情。
    |'
- en: All is done. There’s no problem to fix in DL, and we’re finished. Not quite.
    To get a feel for one important unmet challenge of DL, look at the left image
    in figure 7.2\. The image clearly shows some kind of elephant. However, the same
    network that nicely classified the elephant in figure 7.1 as some elephant species
    completely fails for the image in figure 7.2--the DL model can’t see the elephant
    in the room!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所有工作都已完成。在深度学习中没有问题需要修复，我们完成了。但并非如此。为了了解深度学习中的一个重要未满足的挑战，请看图7.2左边的图像。图像清楚地显示了一些大象。然而，在图7.1中将大象正确分类为某种大象种类的同一网络，在图7.2的图像上却完全失败了——深度学习模型没有看到房间里的大象！
- en: '![](../Images/7-2.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片7-2](../Images/7-2.png)'
- en: Figure 7.2 A bad case of DL. The high performant VGG16-CNN trained on ImageNet
    data fails to see the elephant in the room! The five highest-ranked class predictions
    of the objects in the image are horse_cart, shopping_cart, palace, streetcar,
    and gondola; the elephant is not found! This image is an extrapolation of the
    training set. In the regression problem on the right side of the dashed vertical
    line (extrapolation), there’s zero uncertainty in the regions where there’s no
    data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 深度学习的一个糟糕案例。在ImageNet数据上训练的高性能VGG16-CNN未能看到房间里的大象！图像中对象的五个最高排名类别预测是马车、购物车、宫殿、电车和贡多拉；大象没有被找到！这张图像是训练集的外推。在虚线右侧的回归问题（外推）中，在没有数据的区域没有不确定性。
- en: 'Why does the DL model fail to see the elephant? This is because in the training
    set used to fit the DL model, there were no pictures of elephants in rooms. Not
    surprisingly, the elephant images in the training set show these animals in their
    natural environment. This is a typical situation where a trained DL model fails:
    when presented with an instance of a novel class or situation not seen during
    the training phase. Not only DL models, but also traditional machine learning
    (ML) models get in trouble when the test data doesn’t come from the same distribution
    as the training data. Exaggerating a bit, but DL crucially depends on the big
    lie:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么深度学习模型看不到大象？这是因为用于拟合深度学习模型的训练集中没有房间内的大象图片。不出所料，训练集中的大象图片显示这些动物在自然环境中。这是一个典型的深度学习模型失败的情况：当面对在训练阶段没有见过的新的类别或情况时。不仅深度学习模型，当测试数据与训练数据不来自同一分布时，传统的机器学习（ML）模型也会遇到麻烦。夸张一点说，深度学习关键依赖于这个“大谎言”：
- en: P(train) = *P*(test) = The “big lie”
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: P(train) = *P*(test) = “大谎言”
- en: The condition *P*(train) = *P*(test) is always assumed; but in reality, the
    training and the test data don’t often come from the same distribution. Imagine,
    for example, that you train an image classification model with images taken with
    your old camera, and you now want to classify images that you’ve taken with your
    new camera.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 *P*(train) = *P*(test) 的条件始终成立；但在现实中，训练数据和测试数据并不经常来自相同的分布。想象一下，例如，你用一个旧相机拍摄的照片来训练一个图像分类模型，而现在你想要分类你用新相机拍摄的照片。
- en: The dependence on this questionable assumption that there’s no systematic difference
    between training and test data is a principal weakness of DL and ML in general.
    We, as humans, obviously learn differently. No child in the world would not see
    the elephant in figure 7.2 once he or she learned what an elephant looks like.
    There’s speculation why this is the case. Judea Pearl, for example, suggested
    that the key to a more intelligent and robust DL would be to include causal structures.
    So far, DL can only exploit statistical correlations in the data. Currently, there
    is no clear answer to solve this problem.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖于这种有问题的假设，即训练数据和测试数据之间没有系统性差异，是深度学习和机器学习的一般性主要弱点。显然，作为人类，我们显然以不同的方式学习。世界上没有哪个孩子在学习了大象的样子后，不会在图7.2中看到大象。有人推测为什么会这样。例如，朱迪亚·珀尔（Judea
    Pearl）建议，更智能和鲁棒的深度学习的关键是包括因果关系结构。到目前为止，深度学习只能利用数据中的统计相关性。目前，还没有明确的答案来解决这个问题。
- en: 'Even if we only hope to exploit the statistical correlations in DL, we have
    a severe problem: the network doesn’t tell us that it has a problem with the elephant
    in the room. It doesn’t know when it doesn’t know. It simply assigns the image
    with the elephant in the row to the wrong class, sometimes even with a high probability.
    Now, imagine yourself sitting in a self-driving car. Do you feel comfortable?'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们的目标只是利用深度学习中的统计相关性，我们也面临一个严重的问题：网络不会告诉我们它在房间里的大象有问题。它不知道自己不知道。它只是将大象在行中的图片错误地分配到错误的类别，有时甚至以高概率分配。现在，想象一下你自己坐在一辆自动驾驶汽车里。你感到舒服吗？
- en: 'Looking at the right side of figure 7.2, we can see the reason for this lack
    of recognition. The network does a perfect job of assigning the uncertainty in
    the regions where there’s ample data. The uncertainty is measured by the variance,
    which is modeled in addition to the mean. In chapter 5, we called this type of
    uncertainty aleatoric uncertainty. Now focus on the values *x* in the region indicated
    by extrapolation. While, for example, at *x* = 20, there’s no spread in the data
    and the near-zero uncertainty is correct, the problem begins when we move to larger
    values and enter the region where there’s no data (extrapolation). The network
    simply takes the last value of the uncertainty and extrapolates it to regions
    it hasn’t seen, thus quoting the Star Trek motto: “To boldly go where no one has
    gone before.” Well, maybe too boldly.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 看图7.2的右侧，我们可以看到这种缺乏识别的原因。网络在数据充足的区域对不确定性进行了完美的分配。不确定性是通过方差来衡量的，除了均值外还进行了建模。在第5章中，我们称这种类型的不确定性为随机不确定性。现在关注由外推指示的区域中的值*x*。例如，在*x*
    = 20时，数据没有分散，接近零的不确定性是正确的，但当移动到更大的值并进入没有数据（外推）的区域时，问题就开始了。网络简单地取最后的不确定性值并将其外推到它尚未见过的区域，从而引用了《星际迷航》的座右铭：“勇敢地走向无人去过的地方。”好吧，也许太勇敢了。
- en: Similarly, the problem with the elephant can be seen as a problem of extrapolation.
    When we have only one variable *x*, we can draw it on a line and it’s easy to
    see when we leave the regions where we have data. For the case of the elephant,
    it’s less easy. Instead of a single real-valued quantity *x*, we now have the
    number of pixels squared (images of width 256 have 65,535 values). It’s quite
    hard to see when you leave the space where you have data. But, there’s a real
    need for that (again, imagine sitting in the self-driving car). We talk about
    this in section 7.2.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，大象问题可以看作是一个外推问题。当我们只有一个变量*x*时，我们可以在一条线上绘制它，并且很容易看出我们离开数据区域的情况。对于大象的情况，这就不那么容易了。我们不再有一个单一的实值量*x*，现在我们有像素数的平方（宽度为256的图像有65,535个值）。当你离开数据空间时，很难看出这一点。但是，这确实是一个需要解决的问题（再次想象坐在自动驾驶汽车里）。我们在第7.2节中讨论了这一点。
- en: As another example, imagine you want to place a huge amount of your money in
    a certain investment and your DL model predicts a great reward. Wouldn’t you want
    to know if your DL model is certain about its predicted outcome distribution?
    In another example, imagine a DL system that classifies medical histological samples.
    Here you’d also like to know how certain are the predicted class probabilities.
    (In case of uncertain predictions, you could get a medical doctor to look more
    carefully at the sample.)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个例子，想象你想要将大量资金投资于某种投资，你的深度学习模型预测了巨大的回报。你难道不想知道你的深度学习模型对其预测结果分布是否确定吗？在另一个例子中，想象一个用于对医学组织学样本进行分类的深度学习系统。在这里，你也想知道预测类概率的确定性如何。（在不确定预测的情况下，你可以让医生更仔细地检查样本。）
- en: How can the network tell us that it feels unsure? The solution is to introduce
    a new kind of uncertainty--epistemic uncertainty. Epistemic comes from the ancient
    Greek word “episte-me-,” meaning knowledge. It reflects the uncertainty when you
    leave the regions with data. In practice, this uncertainty is modeled by the uncertainty
    in the parameters of the model, and is thus, sometimes also called parameter or
    model uncertainty. In section 7.2, we talk about a statistical way of thinking,
    called Bayesian reasoning. Bayesian reasoning makes it possible for us to model
    this kind of uncertainty.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 网络如何告诉我们它感到不确定呢？解决方案是引入一种新的不确定性——认知不确定性。认知不确定性来源于古希腊单词“episte-me-”，意为知识。它反映了当你离开数据区域时的不确定性。在实践中，这种不确定性通过模型参数的不确定性来建模，因此有时也称为参数或模型不确定性。在第7.2节中，我们讨论了一种统计思维方式，称为贝叶斯推理。贝叶斯推理使我们能够对这种不确定性进行建模。
- en: 7.2 The first encounter with a Bayesian approach
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 第一次接触贝叶斯方法
- en: In this section, we try to understand the principle of Bayesian statistics by
    an intuitive example and then see how the Bayesian approach to the problem models
    the epistemic uncertainty.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们试图通过一个直观的例子来理解贝叶斯统计学的原理，然后看看贝叶斯方法是如何对问题的认知不确定性进行建模的。
- en: In section 7.2.1, we extend the standard linear regression model by allowing
    not just one solution, but a whole ensemble of solutions.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第7.2.1节中，我们通过允许不仅仅有一个解，而是一系列解来扩展标准线性回归模型。
- en: In section 7.2.2, we take another look at the intuitive solution from section
    7.2.1 and describe it in Bayesian terms to introduce Bayesian terminology.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 7.2.2 节中，我们再次审视 7.2.1 节中的直观解决方案，并用贝叶斯术语描述它，以介绍贝叶斯术语。
- en: '7.2.1 Bayesian model: The hacker’s way'
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 贝叶斯模型：黑客的方法
- en: To get an idea of what’s meant by the epistemic uncertainty of a probabilistic
    prediction model, let’s start with a simple example demonstrating the hacker’s
    way to fit a Bayesian model. We fit a probabilistic linear regression model with
    the four points (see figure 7.3). In this model, we assume that the data has a
    constant spread, *σ* = 3, and so the model is
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解概率预测模型的认知不确定性意味着什么，让我们从一个简单的例子开始，该例子展示了黑客拟合贝叶斯模型的方法。我们使用四个点（见图 7.3）拟合一个概率线性回归模型。在这个模型中，我们假设数据有一个恒定的分散度，*σ*
    = 3，因此模型是
- en: '*P*(*y*|*x*, (*a*, *b*)) = *N*(*y* ;*μ* = *a* ⋅ *x* + *b* ,*σ* = 3)    *Equation
    7.1*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*y*|*x*, (*a*, *b*)) = *N*(*y* ;*μ* = *a* ⋅ *x* + *b* ,*σ* = 3)   *公式 7.1*'
- en: In figure 7.3, you see two examples of this linear regression model corresponding
    to two different parameter sets (*a, b*).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 7.3 中，你看到了两个与两个不同参数集 (*a, b*) 对应的线性回归模型示例。
- en: '![](../Images/7-3.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-3.png)'
- en: Figure 7.3 Linear regression and data. The left column shows the linear model
    for the indicated parameter values *a* and *b*, where the mean value is the solid
    line with the 2.5% and 97.5% percentiles. The right column shows the fitted conditional
    predictive distribution for the outcome *P*(*y*|*x*, (*a, b*)) = *N*(*y*; *μ*
    = *a* · *x* + *b*, *σ* = 3), indicated with color coding. In the upper row, the
    parameter values correspond to the maximum likelihood (MaxLike) values aml and
    bml.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 线性回归和数据。左侧列显示了指示参数值 *a* 和 *b* 的线性模型，其中均值线是实线，2.5% 和 97.5% 分位数。右侧列显示了结果
    *P*(*y*|*x*, (*a, b*)) = *N*(*y*; *μ* = *a* · *x* + *b*, *σ* = 3) 的拟合条件预测分布，用颜色编码表示。在上排中，参数值对应于最大似然（MaxLike）值
    aml 和 bml。
- en: Looking at figure 7.3, let’s first focus on the left part. In the upper left,
    we see the line fitted via the maximum likelihood (MaxLike) principle. As you
    may recall from the notebook [http://mng.bz/wBrP](http://mng.bz/wBrP) , the MaxLike
    is 0.064, and the parameter values for which the MaxLike is maximized are *a*
    = 2.92 and *b* = –1.73\. There’s a second line in the lower part of the figure
    that’s drawn with the parameter values *a* = 1.62 and *b* = –1.73 and has the
    likelihood of 0.056\. When you’re forced to come up with only one value for *a*
    and *b*, it makes absolute sense to use the value that maximized the likelihood
    (the upper part in the figure). But perhaps it’s not a good idea to totally ignore
    the other parameters. Take those into account but don’t trust those as much as
    you’d trust the best (MaxLike) solution.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 观察图 7.3，我们首先关注左部分。在上左部分，我们看到通过最大似然（MaxLike）原则拟合的线。如您从笔记本 [http://mng.bz/wBrP](http://mng.bz/wBrP)
    中回忆起，MaxLike 是 0.064，使 MaxLike 最大的参数值是 *a* = 2.92 和 *b* = –1.73。图的下部分还有另一条线，用参数值
    *a* = 1.62 和 *b* = –1.73 绘制，其似然为 0.056。当你被迫只给出一个 *a* 和 *b* 的值时，使用最大化似然（图中的上部分）的值是绝对有意义的。但也许完全忽略其他参数并不是一个好主意。考虑这些参数，但不要像信任最佳（MaxLike）解决方案那样信任它们。
- en: 'What’s a good measure of how much you can trust a set of parameters (*a, b*)?
    Why not take the likelihood *P*(*D*|(*a*, *b*))? The likelihood is proportional
    to the probability of observing the data D when assuming that our model parameters
    are given by the values *a* and *b*. Therefore, we want to weight each model (Defined
    by certain values *a* and *b*) proportional to its likelihood. However, you should
    normalize the correct weights so that these sum up to 1\. To achieve this, we
    use the normalized likelihoods *p**[n]*(*D*|(*a*, *b*)) as weights:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如何衡量你能够信任一组参数 (*a, b*) 的程度？为什么不采用似然 *P*(*D*|(*a*, *b*)) 呢？似然与在假设我们的模型参数由值 *a*
    和 *b* 给定时观察到的数据 D 的概率成正比。因此，我们希望按其似然比例对每个模型（由某些值 *a* 和 *b* 定义）进行加权。然而，你应该将这些正确的权重归一化，以便它们的总和为
    1。为了实现这一点，我们使用归一化似然 *p**[n]*(*D*|(*a*, *b*)) 作为权重：
- en: '![](../Images/7-3_E01.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-3_E01.png)'
- en: Practically, we calculate the sum of the likelihood  ![](../Images/7-3_E02.png) and
    divide the likelihood *P*(*D*|(*a*, *b*)) by that number to get a normalized version
    *p**[n]*(*D*|(*a*, *b*)). Figure 7.4 shows this normalized likelihood for different
    values of *a* and *b*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们计算似然之和 ![](../Images/7-3_E02.png) 并将似然 *P*(*D*|(*a*, *b*)) 除以该数以获得归一化版本
    *p**[n]*(*D*|(*a*, *b*)*)。图 7.4 显示了不同 *a* 和 *b* 值的归一化似然。
- en: '| ![](../Images/computer-icon.png) | Hands-on time Open [http://mng.bz/wBrP](http://mng.bz/wBrP)
    . This notebook, containing code for the method which we call Bayes the hacker’s
    way, produces the figures in this chapter for this section. Follow the notebook
    while you’re reading the main text until you reach the Go Back to the Main Text
    symbol before the analytical solution. |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| ![电脑图标](../Images/computer-icon.png) | 实践时间 打开 [http://mng.bz/wBrP](http://mng.bz/wBrP)
    。这个笔记本包含了我们称之为贝叶斯黑客方法的代码，它生成了本章本节中的图。在阅读正文内容时，跟随笔记本，直到你看到分析解之前的“返回正文”符号。'
- en: '![](../Images/7-4.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-4.png)'
- en: Figure 7.4 The normalized likelihood of the observed data for the true value
    *s* = 3 and different parameter values for the slope (*a*) and the intercept (*b*).
    The likelihood *pn*(*D*|(*a, b*)) is normalized, meaning that the sum over all
    pixels is 1.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 对于真实值 *s* = 3 和不同斜率 (*a*) 和截距 (*b*) 参数值的观测数据的标准化似然。似然 *pn*(*D*|(*a, b*))
    是标准化的，意味着所有像素的总和为1。
- en: Now we come to the right side of figure 7.3\. What’s the quantity we’re interested
    in? It’s the probability to observe a *y* value for a given value of *x* with
    a model that’s trained on some training data D *P*(*y*|*x*,*D*). For this quantity,
    we take all possible values of *P*(*y*|*x*, (*a*, *b*)) into account. The upper
    right side of figure 7.3 shows an example with the MaxLike estimates for *a* and
    *b*. Another example with different values for a and *b* is shown in the lower
    right side. We now add thousands of those *P*(*y*|*x*, (*a*, *b*)) with different
    values for a and *b* together and weight them with the normalized likelihood *p**[n]*(*D*|(*a*,*b*))
    to get the predicted distribution for *y* given *x*. Figure 7.5 shows this principle.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看图7.3的右侧。我们感兴趣的数量是什么？它是观察给定 *x* 值的 *y* 值的概率，该模型是在某些训练数据 D 上训练的 *P*(*y*|*x*,*D*)。对于这个数量，我们考虑了所有可能的
    *P*(*y*|*x*, (*a*, *b*)) 值。图7.3右上角显示了一个使用 *a* 和 *b* 的MaxLike估计的例子。另一个使用不同 *a*
    和 *b* 值的例子显示在图7.3右下角。我们现在将成千上万的 *P*(*y*|*x*, (*a*, *b*)) 与不同的 *a* 和 *b* 值相加，并用标准化似然
    *p**[n]*(*D*|(*a*,*b*)) 加权，以得到给定 *x* 的 *y* 的预测分布。图7.5展示了这个原理。
- en: '![](../Images/7-5.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-5.png)'
- en: Figure 7.5 The images on the left of the equality sign show probabilistic regression
    models *P*(*y*|*x*, (a, b)), each corresponding to a different set of parameters
    (*a, b*). The factors to the left of the images indicate the normalized likelihood
    *p**[n]*(*D*|(*a, b*)) of the observed four data points D under the corresponding
    model. Adding up the different models *P*(*y*|*x*, (*a, b*)) and weighing them
    with the normalized likelihood *p**[n]*(*D*|(*a, b*)) results in the Bayesian
    prediction model shown on the right of the equality sign.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 等号左边的图像显示了概率回归模型 *P*(*y*|*x*, (a, b))，每个模型对应一组不同的参数 (*a, b*)。图像左边的因素表示在相应模型下观测到的四个数据点
    D 的标准化似然 *p**[n]*(*D*|(*a, b*))。将不同的模型 *P*(*y*|*x*, (*a, b*)) 相加，并用标准化似然 *p**[n]*(*D*|(*a,
    b*)) 加权，结果就是等号右边显示的贝叶斯预测模型。
- en: In more mathematical terms
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 用更数学的语言来说
- en: '![](../Images/7-5_E01.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-5_E01.png)'
- en: The sum goes over all possible values of the parameters *a* and *b*. Let’s again
    look at the equation, maybe from a slightly different angle. The model *P*(*D*|(*a*,*b*))
    is determined by the parameters and *a* and *b*. The data enters only to determine
    the normalized probability of having the specific parameter values a and *b* (*σ*
    = 3 is given). Because a and *b* are continuous quantities, we actually integrate
    over *a* and *b*. But we’re even more sloppy; we just evaluate equation 7.2 at
    about 30 different values for *a* and *b*. Listing 7.1 provides the code corresponding
    to the equation, and figure 7.6 shows the resulting predictive distribution *P*(*y*|*x*,*D*).[1](#pgfId-1109159)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 求和遍历所有可能的参数 *a* 和 *b* 的值。让我们再次看看方程，也许从稍微不同的角度。模型 *P*(*D*|(*a*,*b*)) 由参数 *a*
    和 *b* 决定。数据只用来确定具有特定参数值 a 和 *b* 的标准化概率（σ = 3 已给出）。因为 a 和 *b* 是连续量，我们实际上是在 *a*
    和 *b* 上积分。但我们甚至更粗心；我们只是在约30个不同的 *a* 和 *b* 值上评估方程7.2。列表7.1提供了对应于方程的代码，图7.6显示了由此得到的预测分布
    *P*(*y*|*x*,*D*)。[1](#pgfId-1109159)
- en: '![](../Images/7-6.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-6.png)'
- en: Figure 7.6 The predictive distribution for the Bayesian linear regression model
    trained with the four data points shown on the left side by the color coding and
    on the right by the conditional distribution at two different *x* positions (indicated
    with the lines on the right side). You can clearly see that the uncertainty gets
    larger when leaving the *x* regions where there’s data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 使用左侧颜色编码显示的四个数据点训练的贝叶斯线性回归模型的预测分布，右侧通过条件分布显示在两个不同的 *x* 位置（右侧的线条所示）。你可以清楚地看到，当离开有数据的
    *x* 区域时，不确定性会增大。
- en: The code corresponding to equation 7.2
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对应于方程 7.2 的代码
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Starts with a blank canvas
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从一张空白画布开始
- en: ❷ Loops over all parameters a
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对所有参数 a 进行循环
- en: ❸ Gets the probability for the parameters a and *b* given the data
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 根据数据获取参数 a 和 *b* 的概率
- en: 7.2.2 What did we just do?
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 我们刚才做了什么？
- en: You’ll see in section 7.3 that the experiment we did in section 7.2.1 is actually
    well backed by a sound theory that’s called Bayesian statistics. In section 7.2.1,
    you saw a hacker’s way to fit a Bayesian model. It not only used a single MaxLike
    estimate for the weights, but it also used a whole distribution of possible weights.
    This distribution over w is given by the normalized likelihood *p**[n]*(*D*|*w*)
    = *C* ⋅ *P*(*D*|*w*), with C being a normalization constant.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在第 7.3 节中看到，我们在第 7.2.1 节中进行的实验实际上得到了一个称为贝叶斯统计学的坚实理论的良好支持。在第 7.2.1 节中，你看到了拟合贝叶斯模型的一种黑客方法。它不仅使用了对权重的一个
    MaxLike 估计，而且还使用了一个可能的权重分布。这个关于 w 的分布由归一化似然 *p**[n]*(*D*|*w*) = *C* ⋅ *P*(*D*|*w*)
    给出，其中 C 是归一化常数。
- en: 'When fitting a prediction model, you’re mainly interested in the predictive
    distribution of the outcome *y* for the given input *x*. This is what we introduced
    in chapter 4 as CPD (conditional probability distribution): *P*(*y*|*x*, *D*).
    Equation 7.2 tells you how to get this predictive distribution in a hacker’s approach.
    You take 900 possible weight vectors *w**[i]* = (*a*, *b*) corresponding to the
    possible combinations of 30 different values of *a* and *b*, respectively, and
    then take the following sum:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当拟合预测模型时，你主要对给定输入 *x* 的结果 *y* 的预测分布感兴趣。这是我们第 4 章中引入的 CPD（条件概率分布）：*P*(*y*|*x*,
    *D*)。方程 7.2 告诉你如何以黑客的方式获得这个预测分布。你取 900 个可能的权重向量 *w**[i]* = (*a*, *b*)，对应于 30 个不同值
    *a* 和 *b* 的可能组合，然后进行以下求和：
- en: '![](../Images/7-6_E01.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-6_E01.png)'
- en: 'For weights, we use the normalized likelihood *p**[n]*(*D*|*w*) = *C* ⋅ *P*(*D*|*w*)
    . The parameters *w**[i]* = (*a*, *b*) are continuous values. If you want to be
    mathematically more correct, you should use integrals instead of sums. To get
    the right predicted CPD *P*(*y*|*x*,*D*) for continuous weights for the outcome
    *y*, you should integrate over all possible weights like this:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于权重，我们使用归一化似然 *p**[n]*(*D*|*w*) = *C* ⋅ *P*(*D*|*w*)。参数 *w**[i]* = (*a*, *b*)
    是连续值。如果你想要在数学上更准确，你应该使用积分而不是求和。为了得到对结果 *y* 的连续权重预测 CPD *P*(*y*|*x*,*D*)，你应该对所有可能的权重进行如下积分：
- en: '![](../Images/7-6_E02.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-6_E02.png)'
- en: Let’s try to nail down what changed compared to the previous probabilistic models
    in chapters 4 and 5, which resulted from the MaxLike approach.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试确定与第 4 章和第 5 章中之前概率模型相比发生了什么变化，这些变化是由 MaxLike 方法引起的。
- en: First, a new thing is that you don’t work with fixed weights in the trained
    NN but with a weight distribution *p**[n]*(*D*|*w*). The weight distribution *p**[n]*(*D*|*w*)
    is called posterior distribution in the Bayesian treatment because it’s derived
    after (post) you’ve seen some data D. To emphasize that this is after seeing the
    data, we write *P*(*w*|*D*) for the posterior. It’s the probability of the weights
    w conditioned on the data D. We mathematically derive this equation in the next
    section. The name posterior somehow implies that there’s also some prior distribution
    before seeing some data. You’ll learn about prior distribution in the next section.
    But for now, note that we use a uniform prior in the preceding little experiment.
    In the next section, you’ll also learn how to get from the prior parameter distribution
    to the posterior distribution.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，一个新事物是，在训练好的神经网络中，你不是与固定权重工作，而是与权重分布 *p**[n]*(*D*|*w*) 工作。权重分布 *p**[n]*(*D*|*w*)
    在贝叶斯处理中被称为后验分布，因为它是在（后）你看到一些数据 D 之后推导出来的。为了强调这是在看到数据之后，我们用 *P*(*w*|*D*) 表示后验。这是在数据
    D 的条件下权重 w 的概率。我们将在下一节数学上推导这个方程。这个名字后验某种程度上暗示了在看到一些数据之前也存在某种先验分布。你将在下一节学习关于先验分布的内容。但就目前而言，请注意，在前面的小实验中，我们使用的是均匀先验。在下一节，你还将学习如何从先验参数分布得到后验分布。
- en: 'Previous probabilistic models predict a CPD that only captures the aleatoric
    uncertainty of the data inherent variability. Bayesian models predict a CPD incorporating
    two kinds of uncertainties (see equation 7.3): the aleatoric uncertainty of the
    data inherent variability and the epistemic uncertainty of the parameter values
    that are captured by the probability distribution of the parameter *P*(*w*|*D*).
    Later, you’ll see that the epistemic uncertainty can, in principle, be reduced
    to zero (if you have an infinite number of training data that cover all possible
    future situations). You cannot decrease the aleatoric uncertainty with more data.
    You’ll also see later that in the (rare) situation of a zero epistemic uncertainty,
    the outcome CPD, *P*(*y*|*x*, *D*), is the same as the MaxLike CPD from chapters
    4 and 5\. But in our little example in section 7.2.1, we only have four data points
    and, therefore, the epistemic uncertainty isn’t zero, and the MaxLike model (see
    figure 7.3) does look quite different from the Bayesian model (see figure 7.6).
    In the MaxLike method, the model predicts for each input *x* a CPD that has a
    constant width (also in *x* ranges where no data was available during training).
    In the Bayesian model, the predicted CPD gets broader when extrapolating to new
    *x* ranges. This is quite nice behavior. Your uncertainty should increase when
    leaving known grounds! We will come back to this point later in section 7.3.3
    and compare for our simple example of linear regression the ML approach with the
    Bayesian approach. But first, let’s have a closer look at the Bayesian approach.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的概率模型只预测了 CPD，它只捕捉了数据内在变异性中的随机不确定性。贝叶斯模型预测了一个包含两种不确定性的 CPD（见方程 7.3）：数据内在变异性中的随机不确定性和参数值的不确定性，这种不确定性被参数
    *P*(*w*|*D*) 的概率分布所捕捉。稍后，你会看到，在原则上，认知不确定性可以减少到零（如果你有无限数量的训练数据，这些数据涵盖了所有可能的情况）。你不能通过更多的数据来减少随机不确定性。你还会看到，在（罕见的）认知不确定性为零的情况下，结果
    CPD，*P*(*y*|*x*, *D*)，与第 4 章和第 5 章中的最大似然 CPD 相同。但在我们 7.2.1 节的小例子中，我们只有四个数据点，因此认知不确定性不是零，最大似然模型（见图
    7.3）与贝叶斯模型（见图 7.6）看起来相当不同。在最大似然方法中，模型为每个输入 *x* 预测一个 CPD，该 CPD 具有恒定的宽度（也在训练期间没有数据可用的
    *x* 范围内）。在贝叶斯模型中，预测的 CPD 在外推到新的 *x* 范围时会变得更宽。这是一种相当好的行为。你的不确定性应该在离开已知领域时增加！我们将在
    7.3.3 节稍后回到这个点，并比较我们简单的线性回归例子中的 ML 方法与贝叶斯方法。但首先，让我们更仔细地看看贝叶斯方法。
- en: 7.3 The Bayesian approach for probabilistic models
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 贝叶斯概率模型的方法
- en: The idea of setting up models that incorporate the uncertainty about its parameter
    values via a probability distribution is quite old. The Reverend Thomas Bayes
    (see figure 7.7) developed this approach in the 18th century. Nowadays, a whole
    branch in statistics is called Bayesian statistics.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过概率分布来设置包含其参数值不确定性的模型的想法相当古老。托马斯·贝叶斯牧师（见图 7.7）在 18 世纪开发了这种方法。如今，统计学中有一个分支被称为贝叶斯统计学。
- en: '![](../Images/7-7.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7](../Images/7-7.png)'
- en: Figure 7.7 Thomas Bayes (1701-1761) was an English statistician, philosopher,
    and Presbyterian minister. The image is taken from Wikipedia ([https://en.wikipedia.org/wiki/
    Thomas_Bayes](https://en.wikipedia.org/wiki/Thomas_Bayes)) and is likely not showing
    Bayes. But because no other portrait is available, it’s always used if an image
    of Bayes is needed.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 托马斯·贝叶斯（1701-1761）是一位英国统计学家、哲学家和长老会牧师。这张图片来自维基百科([https://en.wikipedia.org/wiki/Thomas_Bayes](https://en.wikipedia.org/wiki/Thomas_Bayes))，可能并没有展示贝叶斯本人。但由于没有其他肖像可用，如果需要贝叶斯的图片，它总是被使用。
- en: The Bayesian approach is a well-established, clear, and thorough approach to
    fit probabilistic models that capture different kinds of uncertainties. It’s an
    alternative way of doing statistics and interpreting probability. In mainstream
    statistics (the so-called frequentist statistics), probability is defined by analyzing
    repeated (frequent) measurements. More exactly, probability is defined as the
    theoretical limit of the relative frequency when doing an infinite number of repetitions.
    In Bayesian statistics, in contrast, probability is defined in terms of degree
    of belief. The more likely an outcome or a certain value of a parameter is, the
    higher the degree of belief in it. This seemingly loose idea also leads to a valid
    definition of a probability.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯方法是一种确立的、清晰且详尽的拟合概率模型的方法，这些模型可以捕捉不同类型的不确定性。它是一种进行统计学和解释概率的替代方法。在主流统计学（所谓的频率统计学）中，概率是通过分析重复（频繁）的测量来定义的。更确切地说，概率是在进行无限次重复时相对频率的理论极限。相比之下，在贝叶斯统计学中，概率是以信念程度来定义的。一个结果或参数的某个特定值越有可能发生，对其的信念程度就越高。这个看似松散的想法也导致了概率的有效定义。
- en: In the Bayesian approach, only the simplest problems can be tackled without
    the help of a computer. Therefore, this approach has languished away during the
    greater part of the 20th century. But now, with enough compute power, this approach
    is frequently used. It’s a powerful approach, especially when you have some prior
    knowledge that you like to model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯方法中，没有计算机的帮助，只能解决最简单的问题。因此，在20世纪的大部分时间里，这种方法都处于停滞状态。但现在，随着计算能力的增强，这种方法被频繁使用。它是一种强大的方法，特别是当你有一些你想要建模的先验知识时。
- en: In section 7.3.1, you learn how the fitting process works with the Bayesian
    approach. We give an overview of the most important terms and mathematical laws
    used in Bayesian statistics. In section 7.3.2, you use the learned skills to fit
    a Hello World Bayesian model.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在7.3.1节中，你将学习如何使用贝叶斯方法进行拟合过程。我们概述了在贝叶斯统计中使用的重要术语和数学定律。在7.3.2节中，你将使用所学技能来拟合一个Hello
    World贝叶斯模型。
- en: 7.3.1 Training and prediction with a Bayesian model
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 使用贝叶斯模型进行训练和预测
- en: One of the most famous formulae of Bayesian statistics is called the Bayes’
    theorem. It even made its appearance on a neon sign that hangs in the Cambridge
    offices of the software company, HP Autonomy (see figure 7.8). Besides Einstein’s
    *E* = *m* ⋅ *c*² , not many other mathematical formulas have achieved such popularity.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯统计中最著名的公式之一被称为贝叶斯定理。它甚至出现在软件公司HP Autonomy在剑桥办公室悬挂的霓虹灯牌上（见图7.8）。除了爱因斯坦的E =
    m ⋅ c²，没有多少其他数学公式达到了这样的知名度。
- en: '![](../Images/7-8.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图7.7 托马斯·贝叶斯 (1701-1761)](../Images/7-8.png)'
- en: Figure 7.8 The Bayesian theorem defines how to derive *P*(A|B) from the inverse
    conditional probability *P*(B|A), *P*(A), and *P*(B). (This image is taken from
    [http://mng.bz/7Xnv](http://mng.bz/7Xnv) .)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 贝叶斯定理定义了如何从逆条件概率P(B|A)、P(A)和P(B)推导出P(A|B)。（此图来自[http://mng.bz/7Xnv](http://mng.bz/7Xnv)
    。）
- en: 'The Bayes’ theorem links together four probabilities: the conditional probability
    of A given B, *P*(*A*|*B*), the inverse of that conditional probability, the probability
    of *b* given A *P*(*A*|*B*), and the unconditional probabilities of A, *P*(*A*),
    and B, *P*(*B*). In the sidebar at the end of this section, you’ll see that the
    derivation of the Bayesian theorem is easy. But let’s first use it to fit a Bayesian
    probabilistic model. For that, you write the parameter *θ* of the model instead
    of A, and you write D (which stands for the data) instead of B. This yields a
    more useful form of the Bayesian theorem:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理将四个概率联系起来：给定B的A的条件概率P(A|B)，该条件概率的倒数，给定A的b的概率P(b|A)，以及A和b的无条件概率P(A)和P(B)。在本节末尾的侧边栏中，你会看到贝叶斯定理的推导是简单的。但让我们首先用它来拟合一个贝叶斯概率模型。为此，你将用模型的参数θ代替A，用D（代表数据）代替B。这产生了一个更有用的贝叶斯定理形式：
- en: '![](../Images/7-8_E01.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图7.8 贝叶斯方法](../Images/7-8_E01.png)'
- en: 'The quantities in equation 7.5 are so prominent that all of them have names:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 方程7.5中的量非常突出，以至于它们都有名字：
- en: '*P*(*θ*|*D*) --The posterior (the probability of a certain value of a parameter
    *θ* given the data *D*)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*θ*|*D*) --后验（给定数据*D*的参数*θ*的某个值的概率）'
- en: '*P*(*D*|*θ*) --The inverse (called the likelihoo*D*)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*D*|*θ*) --逆（称为似然*D*）'
- en: '*P*(*θ*) --The prior'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*θ*) --先验'
- en: '*P*(*D*) --The quantity (also called the marginal likelihood or evidence)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*D*) --该量（也称为边缘似然或证据）'
- en: Note that in the Bayesian interpretation, a certain value of a parameter *θ*
    isn’t a sure thing but has an uncertainty described by the probability distribution
    *P*(*θ*). This distribution *P*(*θ*) , defines the probability of each parameter
    value *θ* . The probability of a certain parameter value *P*(*θ*) can be interpreted
    as a degree of belief for that certain value of the parameter.[2](#pgfId-1109251)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在贝叶斯解释中，参数*θ*的某个值并不是确定无疑的，而是由概率分布*P*(*θ*)描述的不确定性。这个分布*P*(*θ*)定义了每个参数值*θ*的概率。某个参数值*θ*的概率可以解释为对该参数值的信念程度。[2](#pgfId-1109251)
- en: 'The Bayes’ theorem is at the heart of fitting a Bayesian (or Bayes) model because
    it provides instruction on how to learn from the data the posterior distribution
    of the model parameter. This corresponds to finding the MaxLike parameter value
    *θ**[maxLik]* , given the training data D in non-Bayesian model fitting, but now
    we get a whole distribution *P*(*θ*|*D*). Knowing the posterior distribution of
    the parameter *θ* is all you need for a probabilistic Bayesian model. Now you
    can determine the predictive distribution:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理是拟合贝叶斯（或贝斯）模型的核心，因为它提供了从数据中学习模型参数后验分布的指导。这对应于在非贝叶斯模型拟合中，给定训练数据D找到最大似然参数值*θ**[maxLik]*，但现在我们得到了整个分布*P*(*θ*|*D*）。了解参数*θ*的后验分布是概率贝叶斯模型所需的一切。现在你可以确定预测分布：
- en: '![](../Images/7-8_E02.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-8_E02.png)'
- en: We’ve approximated this predictive distribution in the last section in a brute-force
    approach, where we considered only some discrete values *θ* i of the continuous
    parameter *θ* . In the previous regression example, the parameters are the slope
    and the intercept, *θ**[i]* = (*a**[i]* , *b**[i]* ).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用暴力方法近似了这个预测分布，其中我们只考虑了连续参数*θ*的一些离散值*θ* i。在先前的回归示例中，参数是斜率和截距，*θ**[i]*
    = (*a**[i]* , *b**[i]*)。
- en: '![](../Images/7-8_E03.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-8_E03.png)'
- en: 'To interpret the formula of the predictive distribution, it helps to remember
    the predictive distribution in the MaxLike model:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释预测分布的公式，记住最大似然模型中的预测分布是有帮助的：
- en: '![](../Images/7-8_E04.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-8_E04.png)'
- en: We picked a distribution for the CPD with parameter *θ* and then used the data
    to determine *θ**[maxLik]* . Given *θ**[maxLik]* , all possible outcomes *y* follow
    the CPD *P*(*y*|*x**[test]*, *θ**[maxLik]*), so you can forget about the data
    after you compute *θ**[maxLik]* . With the Bayes approach, you don’t work with
    a CPD that corresponds to a single (optimize*D*) parameter value, but you do a
    weighted average (with weight *P*(*θ**[i]*|*D*)) over many CPDs, (*P*(*y*|*x**[test]*,
    *θ**[i]*)), with different parameter values *θ**[i]* to get the predictive distribution
    (see, for example, figure 7.5).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为CPD选择了参数*θ*的分布，然后使用数据来确定*θ**[maxLik]*。给定*θ**[maxLik]*，所有可能的输出*y*都遵循CPD *P*(*y*|*x**[test]*,
    *θ**[maxLik]*)，因此你可以在计算*θ**[maxLik]*后忘记数据。使用贝叶斯方法，你不会处理与单个（优化*D*）参数值对应的CPD，而是对许多CPD（*P*(*y*|*x**[test]*,
    *θ**[i]*)）进行加权平均（权重为*P*(*θ**[i]*|*D*)），具有不同的参数值*θ**[i]*，以获得预测分布（例如，参见图7.5）。
- en: 'To gain better insight, imagine the following example. You run an actively
    managed fund, and you need a probabilistic forecast for the future value of a
    certain stock, so you want *P*(*y*|*x**[stock]*, *θ**[i]* ). You have several
    experts on your team. Each expert i provides a slightly different probabilistic
    prediction *P*(*y*|*x**[stock]* , *θ**[i]* ). The best you can do to get one forecast
    out of this is to average the predicted CPDs and give each *P*(*y*|*x**[stock]*
    , *θ**[i]* ) an appropriate weight. This weight should be proportional to the
    performance of the expert’s model on past data D of the given stock (the likelihood
    *P*(*D*|*θ**[i]* )). Further, you add your subjective judgment of the models coming
    from these experts in general (the prior *P*(*θ**[i]* )). Alternatively, not willing
    to judge the experts, you might as well give each expert the same subjective a
    priori judgment (the prior is constant). This gives you the unnormalized posterior
    distribution (see equation 7.5). After normalizing, this tells you to use the
    posterior *P*(*θ**[i]*|*D*) as weights, yielding:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更深入的理解，想象以下例子。你管理着一个主动管理的基金，你需要对未来某只股票价值的概率预测，因此你想要 *P*(*y*|*x**[stock]*,
    *θ**[i]*)。你的团队中有几位专家。每位专家 i 提供一个略微不同的概率预测 *P*(*y*|*x**[stock]* , *θ**[i]*)。从这些预测中，你能做的最好的事情就是平均预测的
    CPDs，并给每个 *P*(*y*|*x**[stock]* , *θ**[i]*) 一个适当的权重。这个权重应该与专家模型在过去给定股票数据 D 上的表现（似然
    *P*(*D*|*θ**[i]* )）成比例。进一步，你还需要加入你对这些专家模型的一般主观判断（先验 *P*(*θ**[i]* )）。或者，如果你不愿意评判专家，你也可以给每个专家相同的先验主观判断（先验是常数）。这给了你未归一化的后验分布（见方程
    7.5）。归一化后，这告诉你使用后验 *P*(*θ**[i]*|*D*) 作为权重，得到：
- en: '![](../Images/7-8_E05.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-8_E05.png)'
- en: In this view, a Bayesian model is a weighted ensemble model--we use the wisdom
    of the crowd but weight the contributions of the individual experts. To get the
    terms for the different distributions straight, we’ve collected those in table
    7.1 along with the corresponding formulas and some explanations.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个观点中，贝叶斯模型是一个加权集成模型——我们使用群众的智慧，但权衡个别专家的贡献。为了明确不同分布的术语，我们在表 7.1 中收集了这些分布，以及相应的公式和一些解释。
- en: Table 7.1 Probability distributions used in this chapter
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.1 本章使用的概率分布
- en: '| Name | Formula | Notes/Examples |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 公式 | 备注/示例 |'
- en: '| Likelihood | *P*(*D*&#124;*θ*) | D is for the data that might be single quantities
    (as in a coin toss) or that consists of pairs (as in linear regression or in typical
    DL settings).*y**[i]* is the specific value of the outcome taken in example i.Coin
    toss: data D is the outcome heads (*y* = 1) or tails (*y* = 0) of different throws.
    The likelihood for a single toss*P*(*y**[i]* , *θ*) is determined from a Bernoulli
    distribution: *θ* is the likelihood for heads and 1 - *θ* is the likelihood for
    tails.Linear regression: data D are now pairs (*x**[i]* , *y**[i]* ). The parameter
    *θ* consists of the slope a and the intercept b. The likelihood for a single example
    is given by *P*(*y**[i]*&#124;*x**[i]* , (*a*, *b*)), which is determined from
    a Normal distribution (*y**[i]*&#124;*x**[i]* ) ∼ *N*(*a* ⋅ *x*[1] + *b* , *σ*).
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 似然 | *P*(*D*&#124;*θ*) | D 是可能的数据，可以是单个量（如抛硬币）或成对的数据（如线性回归或典型的深度学习设置）。*y**[i]*
    是在例子 i 中取的具体结果值。抛硬币：数据 D 是不同投掷的结果（正面 *y* = 1 或反面 *y* = 0）。单次投掷的似然 *P*(*y**[i]*
    , *θ*) 由伯努利分布确定：*θ* 是正面的似然，1 - *θ* 是反面的似然。线性回归：数据 D 现在是成对 (*x**[i]* , *y**[i]*
    )。参数 *θ* 由斜率 a 和截距 b 组成。单个例子的似然由 *P*(*y**[i]*&#124;*x**[i]* , (*a*, *b*)) 给出，由正态分布确定
    (*y**[i]*&#124;*x**[i]* ) ∼ *N*(*a* ⋅ *x*[1] + *b* , *σ*)。 |'
- en: '| Prior | *P*(*θ*) | In the Bayes setting, parameters are distributed according
    to the prior distribution. This distribution needs to be defined before you see
    the data. There is some degree of subjectivity in stating your prior beliefs.
    Some examples include:Coin toss: *P*(*θ*) = *U*(*θ* ; 0, 1). (you don’t trust
    anybody. The probability *θ* for heads is uniformly distributed.)Regression: *P*(*a*)
    = *N*(*a* ; 0, 1).Bayesian networks: *P*(*w*) = *N*(*w* ; 0, 1) (the weights are
    a priori standard normal distribute*D*). |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 先验 | *P*(*θ*) | 在贝叶斯设置中，参数根据先验分布分布。在看到数据之前，你需要定义这个分布。在陈述你的先验信念时存在一定程度的主观性。一些例子包括：抛硬币：*P*(*θ*)
    = *U*(*θ* ; 0, 1)。 (你不相信任何人。正面出现的概率 *θ* 是均匀分布的。)回归：*P*(*a*) = *N*(*a* ; 0, 1)。贝叶斯网络：*P*(*w*)
    = *N*(*w* ; 0, 1)（权重是先验标准正态分布*D*）。 |'
- en: '| Posterior | *P*(*θ*&#124;*D*) | The parameter distribution is learned from
    the data using the Bayes formula.Mantra: The posterior is proportional to the
    likelihood times the prior *P*(*θ*&#124;*D*) ∝ *P*(*D*&#124;*θ*) ⋅ *P*(*θ*).The
    normalization constant can be determined from the condition that *∫* *P*(*θ*&#124;*D*)*dθ*
    = 1(for continuous parameters) or from ∑*[i]* *P*(*θ**[i]*&#124;*D*) = 1(for discrete
    parameters). |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 后验 | *P*(*θ*|*D*) | 参数分布是通过贝叶斯公式从数据中学习得到的。箴言：后验与似然乘以先验成比例 *P*(*θ*|*D*) ∝
    *P*(*D*|*θ*) ⋅ *P*(*θ*)。归一化常数可以通过条件 *∫* *P*(*θ*|*D*)*dθ* = 1（对于连续参数）或从 ∑*[i]*
    *P*(*θ**[i]*|*D*) = 1（对于离散参数）来确定。 |'
- en: '| Predictive distribution in the MaxLike settingA.k.a CPD (conditional probability
    distribution) for the outcome yOr unconditional outcome probability distribution
    (if it’s not predicted from *x*) | *P*(*y*&#124;*θ**[MaxLike]* , *x*)*P*(*y*&#124;*θ**[MaxLike]*)
    | y is now a variable and not a fixed value.Linear regression:*P*(*y*&#124;*x*)
    = 1 /√(2 *πσ*² e*^(−(a ⋅ *x* + *b* − *y*)² /2σ²)* = *N*(*y* ;*a* ⋅ *x* + *b* ,
    *σ*)with a and *b* estimated from the training data D. Here and in typical DL
    settings, this distribution is conditioned on *x*(a.k.a CPD).Coin toss: Bernoulli*P*(*y*
    = 1) = *θ* , *P*(*y* = 0) = 1 − *θ* , with *θ* estimated from the data via MaxLike.
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| MaxLike设置中的预测分布A.k.a CPD（条件概率分布）对于结果 y 或无条件结果概率分布（如果它不是从 *x* 预测的） | *P*(*y*|*θ**[MaxLike]*
    , *x*)*P*(*y*|*θ**[MaxLike]*) | y 现在是一个变量，而不是一个固定值。线性回归：*P*(*y*|*x*) = 1 /√(2
    *πσ*² e*^(−(a ⋅ *x* + *b* − *y*)² /2σ²)* = *N*(*y* ;*a* ⋅ *x* + *b* , *σ*)，其中
    a 和 *b* 从训练数据 D 中估计得出。在这里以及在典型的深度学习设置中，这个分布取决于 *x*（即 CPD）。抛硬币：伯努利 *P*(*y* = 1)
    = *θ* ，*P*(*y* = 0) = 1 − *θ* ，其中 *θ* 通过 MaxLike 从数据中估计得出。 |'
- en: '| Predictive distribution in a Bayesian setting (also posterior predictive
    distribution)A.k.a CPD(conditional probability distribution)Or unconditional outcome
    probability distribution (if it’s not predicted from *x*) | *P*(*y*&#124;*x*,
    *D*)*P*(*y*) | y is a variable and not a fixed value. Its distribution is conditioned
    on the input *x* and D.Typically, this is calculated via the posterior *P*(*θ*&#124;*D*)
    via *P*(*y*&#124;*x*, *D*) = *∫ p*(*y*&#124;*x*,*θ*) ⋅ *P*(*θ*&#124;*D*) *dθ*Note
    that *P*(*y*&#124;*x*,*θ*) is the predictive distribution for the outcome comprising
    contributions from a distribution of parameter values *θ*(not only a single value
    *θ**[MaxLik]* as in the MaxLike approach). We integrate over all values of *θ*
    and weight them given their posterior probability *P*(*θ*&#124;*D*). |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 贝叶斯设置中的预测分布（也称为后验预测分布）又称CPD（条件概率分布）或无条件结果概率分布（如果它不是从 *x* 预测的） | *P*(*y*|*x*,
    *D*) | y 是一个变量，而不是一个固定值。它的分布取决于输入 *x* 和 D。通常，这是通过后验 *P*(*θ*|*D*) 通过 *P*(*y*|*x*,
    *D*) = *∫ p*(*y*|*x*,*θ*) ⋅ *P*(*θ*|*D*) *dθ* 来计算的。注意 *P*(*y*|*x*,*θ*) 是结果预测分布，它由参数值
    *θ* 的分布（而不仅仅是 MaxLike 方法中的单个值 *θ**[MaxLik]*）的贡献组成。我们积分所有 *θ* 的值，并按照它们的后验概率 *P*(*θ*|*D*)
    给予它们权重。 |'
- en: NoteA small hint on how to read the formulas in table 7.1\. What’s on the right
    side of the pipe symbol (|) is the “from” part. What’s on the left of the pipe
    symbol is the “to” part. You read those terms from right to left. Sometimes it
    helps to draw an arrow below the formulae in our minds to make it clear. So, *P*(*D*|*θ*)
    is the probability that from the parameter *θ* , you get the data D. If there
    are mathematicians in the room, say, “ *P*(*D*|*θ*) is the probability of the
    data D given the parameters *θ* .” But thoughts are free and think what you prefer.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：关于如何阅读表7.1中公式的简单提示。管道符号（|）右侧的内容是“从”部分。管道符号左侧的内容是“到”部分。您应从右到左阅读这些术语。有时在心中在公式下方画一条箭头有助于使其更清晰。因此，*P*(*D*|*θ*)
    是从参数 *θ* 得到数据 D 的概率。如果房间里有人在场，可以说，“*P*(*D*|*θ*) 是给定参数 *θ* 的数据 D 的概率。”但思想是自由的，思考您喜欢的内容。
- en: 'The Bayesian theorem (equation 7.5) allows you to learn about the parameter
    distribution *P*(*θ*|*D*) of *θ* when you have some data D. The quantity *P*(*θ*|*D*)
    is therefore called posterior because you determine it after you’ve seen the data
    (the name posterior comes from the Latin post meaning after). But how can you
    derive *P*(*θ*|*D*)? You need to determine the likelihood of the observed data
    *P*(*D*|*θ*) under the model with parameter *θ* . In addition, you need to know
    the prior *P*(*θ*) and the evidence *P*(*D*). Because your training data D is
    fixed, *P*(*D*) is a constant. Realizing that *P*(*D*) is a constant leads you
    to the fact that the posterior distribution is proportional to the likelihood
    times the prior: *P*(*θ*|*D*) ∝ *P*(*D*|*θ*) ⋅ *P*(*θ*), which is also called
    the Bayesian mantra.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理（方程7.5）允许你在拥有一些数据D的情况下，了解参数分布 *P*(*θ*|*D*) 的 *θ*。因此，*P*(*θ*|*D*) 被称为后验，因为你在看到数据之后确定它（名字后验来自拉丁语中的“post”，意为“之后”）。但如何推导
    *P*(*θ*|*D*) 呢？你需要确定在参数 *θ* 的模型下观察到的数据 *P*(*D*|*θ*) 的似然性。此外，你需要知道先验 *P*(*θ*) 和证据
    *P*(*D*)。因为你的训练数据D是固定的，*P*(*D*) 是一个常数。意识到 *P*(*D*) 是一个常数，会让你得出后验分布与似然性和先验的乘积成比例的结论：*P*(*θ*|*D*)
    ∝ *P*(*D*|*θ*) ⋅ *P*(*θ*)，这也就是所谓的贝叶斯咒语。
- en: The Bayesian mantra The posterior is proportional to the likelihood times the
    prior.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯咒语 后验与似然性和先验的乘积成比例。
- en: 'This shows that the evidence *P*(*D*) is just there so that posterior probability
    sums up (or integrates) to 1\. It’s mathematically often more convenient to use
    the Bayesian mantra and calculate the proportionality afterward by the requirement
    that:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明证据 *P*(*D*) 只是为了确保后验概率的总和（或积分）为1。在数学上，通常更方便使用贝叶斯咒语，并通过以下要求在计算比例之后进行计算：
- en: ∫ *P*(*θ*|*D*)*dθ* = 1
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ∫ *P*(*θ*|*D*)*dθ* = 1
- en: 'Fixing *P*(*D*) was easy, but how to choose the prior *P*(*θ*)? If you have
    no prior knowledge about the parameter values, you can, for example, use a uniform
    distribution giving each parameter value the same probability. This means you
    pick the following prior: *P*(*θ*) = *const* . In this special case, the posterior
    distribution *P*(*θ*|*D*) is proportional to the likelihood. Why? Because *P*(*θ*|*D*)
    is a probability distribution; hence, it must integrate to 1, and therefore, it
    holds that the posterior *P*(*θ*|*D*) is given by the normalized likelihood if
    the prior is constant. This is exactly what was used in the hacker’s example where
    we used the different normalized likelihoods for different parameter values to
    weight the contributing CPDs in equation 7.2\. Here it is again:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 固定 *P*(*D*) 是容易的，但如何选择先验 *P*(*θ*) 呢？如果你对参数值没有先验知识，例如，你可以使用均匀分布，给每个参数值相同的概率。这意味着你选择的先验是：*P*(*θ*)
    = *const*。在这种情况下，后验分布 *P*(*θ*|*D*) 与似然性成比例。为什么？因为 *P*(*θ*|*D*) 是一个概率分布；因此，它必须积分到1，因此，如果先验是常数，后验
    *P*(*θ*|*D*) 就由归一化的似然性给出。这正是我们在黑客示例中使用的，我们在方程7.2中使用了不同参数值的归一化似然性来加权CPDs的贡献。这里再次展示：
- en: '![](../Images/7-8_E06.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-8_E06.png)'
- en: Derivation of the Bayes theorem
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理的推导
- en: 'The Bayesian theorem can be derived from the product rule as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理可以从乘法法则推导如下：
- en: '*P*(*a*, *B*) = *P*(*A*|*B*) ⋅ *P*(*B*)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*a*, *B*) = *P*(*A*|*B*) ⋅ *P*(*B*)'
- en: 'In a few words, the product rule says the joint probability that both events
    A and *b* occur, given by *P*(*a, b*), is the same as the probability *P*(A),
    where the (first) event A occurs, times the probability that when A occurs, then
    *b* or *P*(B|A) occurs. Read those equations from left to right: *P*(B|A) is from
    A to *b* or, in general, *P*(*to*|*from*). Makes sense! Think for example about
    the probability that during a beach stroll you find an oyster, *P*(*oyster* =
    0.2 . The probability that an oyster contains a pearl is *P*(*pearl*|*oyster*)
    = 0.01 .'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，乘法法则表明，给定事件A和*b*同时发生的联合概率 *P*(*a, b*)，等于事件A发生的概率 *P*(A)，乘以当A发生时，*b*发生的概率
    *P*(B|A)。从左到右阅读这些方程：*P*(B|A)是从A到*b*，或者更一般地，*P*(*to*|*from*)。这说得通！例如，考虑在海滩散步时找到牡蛎的概率
    *P*(*oyster* = 0.2。一个牡蛎含有珍珠的概率是 *P*(*pearl*|*oyster*) = 0.01。
- en: From this you can compute the probability that you find an oyster that contains
    a pearl by
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里你可以计算出找到含有珍珠的牡蛎的概率，通过
- en: '*P*(*oyster.with.pearl* = *P*(*oyster*) ⋅ *P*(*pearl*|*oyster*) = 0.2 ⋅ 0.001
    = 0.0002 . Let’s derive the Bayes formula.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*oyster.with.pearl* = *P*(*oyster*) ⋅ *P*(*pearl*|*oyster*) = 0.2 ⋅ 0.001
    = 0.0002。让我们推导贝叶斯公式。'
- en: '*P*(*θ*|*D*) = *P*(*D*|*θ*) ⋅ *P*(*θ*) / *P*(*D*)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*θ*|*D*) = *P*(*D*|*θ*) ⋅ *P*(*θ*) / *P*(*D*)'
- en: 'You need to use the product rule in the previous equation and realize that
    you can sweep *a* and *b*. Doing so yields this equation:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要使用前一个方程中的乘法法则，并意识到你可以消去 *a* 和 *b*。这样做得到这个方程：
- en: '*P*(*B*) ⋅ *P*(*A*|*B*) = *P*(*B*, *A*) = *P*(*a*, *B*) = *P*(*A*) ⋅ *P*(*B*|*A*)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*B*) ⋅ *P*(*A*|*B*) = *P*(*B*, *A*) = *P*(*a*, *B*) = *P*(*A*) ⋅ *P*(*B*|*A*)'
- en: 'Dividing both sides by *P*(B) gives us the Bayesian theorem:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 将两边除以 *P*(B) 得到贝叶斯定理：
- en: '*P*(*A*|*B*) = *P*(*A*) ⋅ *P*(*B*|*A*) / *P*(*B*)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*A*|*B*) = *P*(*A*) ⋅ *P*(*B*|*A*) / *P*(*B*)'
- en: That was easy! Given the power of the Bayes theorem, the derivation is a piece
    of cake. The derivation of the other powerful formula, *E* =*mc*² , however, is
    a bit harder.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 那很简单！鉴于贝叶斯定理的力量，推导过程就像吃蛋糕一样简单。然而，其他强大公式的推导，如 *E* =*mc*²，却要复杂一些。
- en: In Bayesian regression problems (and later deep Bayesian NNs), we want to predict
    for each input its CPD *p*(*y*|*x*) . But before we come to that, let’s first
    try to fit a single (unconditional) distribution *P*(*y*).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯回归问题（以及后来的深度贝叶斯神经网络）中，我们想要预测每个输入的联合概率分布 *p*(*y*|*x*)。但在我们到达那里之前，让我们首先尝试拟合一个单一（无条件）分布
    *P*(*y*)。
- en: 7.3.2 A coin toss as a Hello World example for Bayesian models
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 将抛硬币作为贝叶斯模型的“Hello World”示例
- en: Let’s use the learned concepts of Bayesian statistics from section 7.3.1 to
    fit your first Bayesian model. To keep it simple, let’s assume that you want to
    predict the outcome of a coin toss experiment. The two possible outcomes are heads
    (*y* = 1) and tails (*y* = 0). You want to determine the predictive distribution
    *P*(*y*) for the two possible outcomes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用第7.3.1节中学习的贝叶斯统计概念来拟合你的第一个贝叶斯模型。为了保持简单，让我们假设你想要预测抛硬币实验的结果。两种可能的结果是正面（*y*
    = 1）和反面（*y* = 0）。你想要确定两种可能结果的前瞻分布 *P*(*y*)。
- en: Note that in most other examples in this book, you have some input, and you’ve
    estimated the outcome distribution conditioned on the input values. In these kinds
    of examples, you estimate a CPD for the outcome. In this coin toss example, you
    have no input variables. The probability for getting heads doesn’t depend on any
    external variables; you always toss the same coin. Therefore, you only need to
    estimate an unconditional probability distribution for the outcome.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这本书的多数其他例子中，你有一些输入，并且你已经根据输入值估计了结果分布。在这些类型的例子中，你估计的是结果的条件概率分布。在这个抛硬币的例子中，你没有输入变量。得到正面的概率不依赖于任何外部变量；你总是抛同一个硬币。因此，你只需要估计结果的无条件概率分布。
- en: Predicting the (unconditional) outcome distribution for the coin toss example
    is easy if you know that it’s a fair coin. For a fair coin, the predictive distribution
    assigns a probability of 0.5 to heads and a probability of 0.5 to tails. This
    probabilistic outcome captures the aleatoric uncertainty inherent in a coin toss
    experiment--you just don’t know whether you’ll get heads or tails. The epistemic
    uncertainty on the other hand is zero. You know for sure that the probability
    for heads is 0.5; after all, it’s a fair coin.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你知道这是一个公平的硬币，预测抛硬币例子的（无条件）结果分布就很容易。对于一个公平的硬币，前瞻分布将正面分配0.5的概率，反面分配0.5的概率。这种概率结果捕捉了硬币实验中固有的随机不确定性——你不知道你会得到正面还是反面。另一方面，认知不确定性为零。你肯定知道正面的概率是0.5；毕竟，它是一个公平的硬币。
- en: 'In terms of probabilistic models, you describe the predictive distribution
    as a Bernoulli distribution with a binary outcome *y *(heads: *y* = 1, tails:
    *y* = 0). It has only one parameter, *θ* , which corresponds to the probability
    of getting heads, so *Θ* = *P*(*y* = 1). If you have a fair coin, the parameter
    *θ* is *θ* = 0.5\. But *θ* can take other values. The left side of figure 7.9
    shows the predictive distribution for a fixed *θ* .'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率模型方面，你将预测分布描述为具有二元结果 *y*（正面：*y* = 1，反面：*y* = 0）的伯努利分布。它只有一个参数 *θ*，对应于得到正面的概率，所以
    *Θ* = *P*(*y* = 1)。如果你有一个公平的硬币，参数 *θ* 是 *θ* = 0.5。但 *θ* 可以取其他值。图7.9的左侧显示了固定 *θ*
    的预测分布。
- en: 'Let’s assume that the coin comes from a suspicious gambler, and you can’t assume
    that it’s a fair coin. You also can’t tell the exact value of *θ* . This means
    you need to estimate the probability for heads: *Θ* = *P*(*y* = 1).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设这个硬币来自一个可疑的赌徒，你不能假设它是一个公平的硬币。你也不能说出 *θ* 的确切值。这意味着你需要估计正面的概率：*Θ* = *P*(*y*
    = 1)。
- en: 'To generate some training data, you throw the coin three times and observe
    heads all three times: D = (1, 1, 1). Uh-oh, the first impression is that you’ve
    got an unfair coin. But how sure can you be after three throws? We’ll come to
    the Bayes treatment in a second, but first, let’s find out what you get with a
    non-Bayesian approach.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成一些训练数据，你抛硬币三次，并且每次都观察到正面：D = (1, 1, 1)。哎呀，第一印象是你得到了一个不公平的硬币。但在三次抛掷之后你能有多确定？我们稍后会谈到贝叶斯处理，但首先，让我们用非贝叶斯方法来看看你能得到什么。
- en: The MaxLike approach for the coin toss example
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 抛硬币例子的最大似然方法
- en: 'Let’s use the traditional non-Bayesian MaxLike approach to fit the Bernoulli
    model to the results of the coin toss experiment. To fit the model, you use your
    training data: *D* = (*y*[1] = 1, *y*[2] = 1, *y*[3] = 1). Based on this data,
    what’s the best value of the parameter *θ* in the Bernoulli model (see figure
    7.9)?'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用传统的非贝叶斯最大似然方法来拟合伯努利模型到抛硬币实验的结果。为了拟合模型，你使用你的训练数据：*D* = (*y*[1] = 1, *y*[2]
    = 1, *y*[3] = 1)。基于这些数据，伯努利模型中参数 *θ* 的最佳值是多少（见图7.9）？
- en: '![](../Images/7-9.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-9.png)'
- en: Figure 7.9 The Bernoulli distribution of a binary variable *y* with a parameter
    *θ*(left). On the right, you see the predictive distribution for the outcome derived
    by a MaxLike approach after observing heads three times in a row.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 二元变量 *y* 的伯努利分布，参数为 *θ*（左）。在右侧，你看到通过观察连续三次正面得出的最大似然方法的预测分布。
- en: 'The MaxLike estimate for the probability for heads (*y* = 1) is computed by
    the number of observed heads (n1) divided by the number of throws (n): *θ**[MaxLik]*
    = *n*[1] / *n* = 3 / 3 = 1 . The standard deviation is given by *sd*(*θ**[MaxLik]*)
    =*θ**[MaxLik]* ⋅ (1 − *θ**[MaxLik]*) = 1 ⋅ 0 = 0\. (To check out the derivation
    of the formula for the standard deviation, see [http:// mng.bz/mB9a](http://mng.bz/mB9a)
    .) This means the MaxLike estimate assigns the probability of one and an aleatoric
    uncertainty of zero (*sd*(*θ**[MaxLik]*) = 0) to the outcome, heads. In this case,
    the predictive distribution (see figure 7.9 on the right side) doesn’t contain
    any uncertainty. The resulting model says that the coin toss always shows heads.
    That’s quite a risky statement after seeing only three throws!'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最大似然估计的正面概率（*y* = 1）是通过观察到的正面次数（n1）除以抛掷次数（n）来计算的：*θ**[MaxLik]* = *n*[1] / *n*
    = 3 / 3 = 1。标准差由 *sd*(*θ**[MaxLik]*) =*θ**[MaxLik]* ⋅ (1 − *θ**[MaxLik]*) = 1
    ⋅ 0 = 0给出。（要查看标准差公式的推导，请参阅[http:// mng.bz/mB9a](http://mng.bz/mB9a) 。）这意味着最大似然估计将概率设为1，并且随机不确定性为零（*sd*(*θ**[MaxLik]*)
    = 0），分配给结果，正面。在这种情况下，预测分布（见图7.9右侧）不包含任何不确定性。得到的模型表明，抛硬币总是显示正面。在只看到三次抛掷之后，这是一个相当冒险的声明！
- en: Bayesian approach for the coin toss example
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 抛硬币例子的贝叶斯方法
- en: 'Let’s take the Bayesian approach to fit the Bernoulli model to the outcome
    of the results of the coin toss experiment. You assume that you need to allow
    for some uncertainty about the parameter *θ* . After all, you only have training
    data of three points! Instead of estimating a single (optimal) value for the parameter
    *θ* , your aim is to determine the posterior distribution for the parameter. Look
    at the Bayes formula (equation 7.5) again:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们采用贝叶斯方法来拟合伯努利模型到抛硬币实验的结果。你假设你需要对参数 *θ* 的某些不确定性进行考虑。毕竟，你只有三个点的训练数据！而不是估计参数
    *θ* 的单个（最优）值，你的目标是确定参数的后验分布。再次看看贝叶斯公式（方程7.5）：
- en: '*P*(*θ*|*D*) = *P*(*D*|*θ*) *P*(*θ*) / *P*(*D*)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*θ*|*D*) = *P*(*D*|*θ*) *P*(*θ*) / *P*(*D*)'
- en: 'where *P*(*θ*|*D*) is the posterior, *P*(*D*|*θ*) is the likelihood, *P*(*θ*)
    is the prior, and *P*(*D*) is the marginal likelihood (for normalization). This
    tells you that you need to determine the joint likelihood *P*(*D*|*θ*) and that
    the prior *P*(*θ*) and *P*(*D*) serve as normalization constants. What’s the joint
    likelihood? You multiply the likelihoods of all three observations, which yields
    the joint likelihood:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *P*(*θ*|*D*) 是后验概率，*P*(*D*|*θ*) 是似然函数，*P*(*θ*) 是先验概率，*P*(*D*) 是边缘似然（用于归一化）。这告诉你需要确定联合似然
    *P*(*D*|*θ*)，并且先验 *P*(*θ*) 和 *P*(*D*) 作为归一化常数。联合似然是什么？你将所有三个观察到的似然相乘，从而得到联合似然：
- en: '*P*(*D*|*θ*) = *P*(*y* = 1) ⋅ *P*(*y* = 1) ⋅ *P*(*y* = 1) = *θ* ⋅ *θ* ⋅ *θ*
    = *θ*³'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*D*|*θ*) = *P*(*y* = 1) ⋅ *P*(*y* = 1) ⋅ *P*(*y* = 1) = *θ* ⋅ *θ* ⋅ *θ*
    = *θ*³'
- en: Now, what to choose for the prior? You know that the parameter *θ* must be a
    number between zero and one because it’s the probability to get heads for each
    toss. Let’s assume that all values of *θ* between zero and one are equally likely
    and take a uniform distribution. Because *θ* can take any value between zero and
    one, *P*(*θ*) is a continuous probability distribution, which needs to integrate
    to one (see the upper panel in figure 7.11).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们应该选择什么样的先验？你知道参数 *θ* 必须在零和一之间，因为它是每次抛掷得到正面的概率。让我们假设零和一之间的所有 *θ* 值都是等可能的，并采用均匀分布。因为
    *θ* 可以取零和一之间的任何值，所以 *P*(*θ*) 是一个连续概率分布，需要积分到一（参见图7.11的上部分）。
- en: But before dealing with continuous distributions and integrals to derive the
    analytical solution, let’s again use the brute-force approach. For this, we recommend
    that you follow the code in the notebook and do the exercises.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 但在处理连续分布和积分以推导解析解之前，我们再次使用暴力方法。为此，我们建议你遵循笔记本中的代码并做练习。
- en: Solving the Bayes solution of the coin toss example via brute-force approximation
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通过暴力近似求解抛硬币示例的贝叶斯解
- en: '| ![](../Images/computer-icon.png) | Hands-on time Open [http://mng.bz/5a6O](http://mng.bz/5a6O)
    . This notebook shows how to fit a Bernoulli distribution in the Bayesian way
    by taking the brute-force approach.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '| ![电脑图标](../Images/computer-icon.png) | 实践时间 打开[http://mng.bz/5a6O](http://mng.bz/5a6O)
    。这个笔记本展示了如何通过暴力方法以贝叶斯方式拟合伯努利分布。'
- en: Do the coin toss experiment with the brute-force approach assuming a uniform
    prior.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设均匀先验，使用暴力方法进行抛硬币实验。
- en: Investigate the development of the parameter’s posterior shape in case of a
    large training data set.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调查在大训练数据集情况下参数后验形状的发展。
- en: '|'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'To use brute force, we sample the prior *P*(*θ*) at 19 grid points (*θ*[1]
    = 0.05 , *θ*[2] = 1, *θ*[3] = 0.15 ,..., *θ*[19] = 0.95). In this brute-force
    approach, we only have a bunch of discrete values for *θ* 1, and we can therefore
    work with sums instead of integrals and probabilities instead of probability densities.
    That’s because we assume that all 19 values of prior *P*(*θ*) have the same probability:
    *P*(*θ*) = 1/19 ≈ 0.052632 (see the upper left panel in figure 7.10).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用暴力方法，我们在19个网格点（*θ*[1] = 0.05 , *θ*[2] = 1, *θ*[3] = 0.15 ,..., *θ*[19] =
    0.95）上采样先验 *P*(*θ*)。在这个暴力方法中，我们只有 *θ* 1的一组离散值，因此我们可以用总和代替积分，用概率代替概率密度。这是因为我们假设先验
    *P*(*θ*) 的所有19个值具有相同的概率：*P*(*θ*) = 1/19 ≈ 0.052632（参见图7.10的左上部分）。
- en: '![](../Images/7-10.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-10.png)'
- en: Figure 7.10 Prior distribution (upper left panel) and posterior distribution
    (upper right panel). The lower panels show the CPDs for the outcome (1 for heads
    and 0 for tails). The plots were created using the brute-force approach to the
    coin toss experiment.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 先验分布（左上部分）和后验分布（右上部分）。下部分显示了结果的CPD（条件概率分布）（正面为1，反面为0）。这些图是通过暴力方法对抛硬币实验进行创建的。
- en: 'Before computing the posterior parameter distribution, let’s check out how
    the predictive distribution for the outcome looks before we see any data. Because
    the prior is giving the same probability for all *θ* i values, you expect to get
    heads and tails with equal probabilities. Let’s check our intuition and derive
    the predictive distribution using this equation:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算后验参数分布之前，让我们看看在看到任何数据之前，结果的预测分布看起来如何。因为先验给出了所有 *θ* i 值相同的概率，所以你期望正面和反面的概率是相等的。让我们检验我们的直觉，并使用这个方程推导出预测分布：
- en: '![](../Images/7-10_E01.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-10_E01.png)'
- en: 'In the coin toss example, we called the model parameter *θ* instead of w, and
    we have no input *x*, which yields:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在抛硬币示例中，我们称模型参数为 *θ* 而不是 w，我们没有输入 *x*，这导致：
- en: '![](../Images/7-10_E02.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-10_E02.png)'
- en: 'Plugging in *P*(*y* = 1|*θ**[i]* ) = *θ**[i]*(the likelihood for heads) and
    *P*(*θ*) = 1/19 ≈ 0.052632(the prior) yields *P*(*y* = 1) = 0.5 and, accordingly,
    *P*(*y* = 0) = 1 − *P*(*y* = 1) = 0.5 . This is exactly what we expected: a 50:50
    chance for heads or tails (see the lower left plot in figure 7.10). You can determine
    the unnormalized likelihood by using the Bayesian mantra given here:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 将 *P*(*y* = 1|*θ**[i]* ) = *θ**[i]*（正面的似然）和 *P*(*θ*) = 1/19 ≈ 0.052632（先验）代入，得到
    *P*(*y* = 1) = 0.5，相应地，*P*(*y* = 0) = 1 − *P*(*y* = 1) = 0.5。这正是我们预期的：正面和反面的概率各为50%（参见图7.10的左下部分）。你可以使用这里给出的贝叶斯咒语确定未归一化的似然：
- en: The Bayesian mantra The posterior is proportional to the likelihood times the
    prior.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯咒语 后验是似然乘以先验的比例。
- en: 'Let’s compute the unnormalized posterior at each of the 19 grid points *θ*
    i (see table 7.2):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算每个 19 个网格点 *θ* i 的非规范化后验（见表 7.2）：
- en: '![](../Images/7-10_E03.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-10_E03.png)'
- en: 'To get the normalized posterior values, you divide each unnormalized posterior
    by the sum over all values:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 要得到规范化后的后验值，你需要将每个非规范化后验值除以所有值的总和：
- en: '![](../Images/7-10_E04.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-10_E04.png)'
- en: 'For the resulting values, look at the upper right plot in figure 7.10\. As
    expected, after seeing heads three times, the posterior favors a *θ* i that’s
    close to one. But the posterior still gives some probability to *θ* values smaller
    than one, allowing for a coin that doesn’t deterministically yield heads for each
    toss. Now you have all the ingredients to determine the predictive distribution
    based on the posterior:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于得到的结果，查看图 7.10 中的右上角图。正如预期的那样，在看到三次正面后，后验分布倾向于接近一的 *θ* i。但后验分布仍然给小于一的 *θ*
    值分配一些概率，允许硬币在每次投掷时不一定总是出现正面。现在你有了所有基于后验确定预测分布的要素：
- en: '![](../Images/7-10_E05.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-10_E05.png)'
- en: Table 7.2  Brute-force results collected in a table. Each row corresponds to
    one grid point. The columns hold the parameter values (`theta`), joint likelihood
    (`jointlik`), `prior`, unnormalized posterior (`unnorm_post`), and `post` .
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.2  收集在表中的暴力结果。每一行对应一个网格点。列包含参数值（`theta`）、联合似然（`jointlik`）、`prior`、非规范化后验（`unnorm_post`）和
    `post`。
- en: '![](../Images/table_7-2.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/table_7-2.png)'
- en: Plugging in *P*(*y* = 1 | *θ**[i]* ) = *θ**[i]*(the likelihood for heads) and
    *P*(*θ**[i]*|*D*) (posterior from the last column in table 7.2) yields
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 将 *P*(*y* = 1 | *θ**[i]* ) = *θ**[i]*（正面的似然）和 *P*(*θ**[i]*|*D*)（来自表 7.2 的最后一列的后验）代入
- en: '*P*(*y* = 1 | *D*) = 0.78'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*y* = 1 | *D*) = 0.78'
- en: and
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '*P*(*y* = 0 | *D*) = 1 −*P*(*y* = 1 | *D*) = 0.22'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*y* = 0 | *D*) = 1 −*P*(*y* = 1 | *D*) = 0.22'
- en: According to this predictive distribution based on the posterior, you can expect
    heads with a 78% chance and tails with a 22% probability (see the lower right
    plot in figure 7.10).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 根据基于后验的这种预测分布，你可以预期有 78% 的概率出现正面，22% 的概率出现反面（见图 7.10 的右下角图）。
- en: Solving the Bayes solution of the coin toss example analytically
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 解析地解决抛硬币例子的贝叶斯解决方案
- en: Fitting a Bayesian Bernoulli model for the coin toss example is such an easy
    problem that you can solve it exactly. Let’s see how it works though.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 对于抛硬币的例子，拟合贝叶斯伯努利模型是一个如此简单的问题，以至于你可以精确地解决它。不过，让我们看看它是如何工作的。
- en: 'For the prior, you again use a uniform distribution, giving each possible *θ*
    the same a priori probability. Because we want *P*(*θ*) to be a valid probability
    distribution, *P*(*θ*) needs to integrate to one. The prior parameter distribution
    is therefore *P*(*θ*) = 1 for *θ* between zero and one. To derive the prior predictive
    distribution under the continuous prior, you can start with equation 7.6:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于先验分布，你再次使用均匀分布，给每个可能的 *θ* 分配相同的先验概率。因为我们希望 *P*(*θ*) 是一个有效的概率分布，所以 *P*(*θ*)
    需要积分等于一。因此，先验参数分布是 *P*(*θ*) = 1，对于 *θ* 在零和一之间。要在连续先验下推导先验预测分布，你可以从方程 7.6 开始：
- en: '![](../Images/7-10_E06.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-10_E06.png)'
- en: 'But here you have no input feature *x* and you are in a situation before seeing
    data, meaning you also have no data D and you need to use the prior *P*(*θ*),
    instead of the posterior *P*(*D*|*θ*), yielding:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 但在这里你没有输入特征 *x*，你处于看到数据之前的情况，这意味着你也没有数据 D，你需要使用先验 *P*(*θ*)，而不是后验 *P*(*D*|*θ*)，得到：
- en: '*P*(*y*) = *∫**[θ]* *P*(*y*|*θ*) · *pn*(*θ*)*dθ*'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*y*) = *∫* [θ] *P*(*y*|*θ*) · *pn*(*θ*) *dθ*'
- en: Variant of equation 7.6 for the unconditional case before seeing data
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 7.6 的无条件情况下的变体
- en: 'Accordingly, the predicted probability for the outcome *y* = 1 before seeing
    data can be determined as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在看到数据之前预测结果 *y* = 1 的预测概率可以确定如下：
- en: '| ![](../Images/7-10_E07_T1.png)  | (We use equation 7.4.) |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/7-10_E07_T1.png)  | （我们使用方程 7.4。）|'
- en: '| ![](../Images/7-10_E07_T2.png)  | (We do the integration and get the antiderivative
    ½ *θ*² of *θ* .) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/7-10_E07_T2.png)  | （我们进行积分并得到 *θ* 的反导数 ½ *θ*²。）|'
- en: '| ![](../Images/7-10_E07_T3.png)  | (We put in the numbers.) |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/7-10_E07_T3.png)  | （我们输入数字。）|'
- en: '| ![](../Images/7-10_E07_T4.png)  | (We use the counter probability to calculate
    *P*(*y* = 0).) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/7-10_E07_T4.png)  | （我们使用逆概率来计算 *P*(*y* = 0)。）|'
- en: 'Uff! That worked out, and you got the same prior predictive distribution as
    in the brute-force approach (see the lower left panel in figure 7.11). To determine
    the posterior, you’ll again use the Bayes formula:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀！这成功了，你得到了与暴力方法相同的先验预测分布（见图 7.11 的左下角面板）。为了确定后验，你将再次使用贝叶斯公式：
- en: '*P*(*θ*|*D*) = *P*(*D*|*θ*) ⋅ *P*(*θ*) / *P*(*D*)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*θ*|*D*) = *P*(*D*|*θ*) ⋅ *P*(*θ*) / *P*(*D*)'
- en: '![](../Images/7-11.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-11.png)'
- en: Figure 7.11 Prior and posterior distributions (upper panel) and corresponding
    and predictive distributions (lower panel) for the analytical approach to the
    coin toss experiment.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 分析方法下抛硬币实验的先验和后验分布（上部分）以及相应的预测分布（下部分）。
- en: 'The terms in this formula have the following names:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式中的项有以下名称：
- en: '*P*(*θ*|*D*) = *P*(*D*|*θ*)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*θ*|*D*) = *P*(*D*|*θ*)'
- en: '*P*(*θ*|*D*) is the posterior.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*θ*|*D*) 是后验。'
- en: '*P*(*D*|*θ*) is the likelihood.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*D*|*θ*) 是似然。'
- en: '*P*(*θ*) is the prior.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*θ*) 是先验。'
- en: '*P*(*D*) is the marginal likelihood (for normalization).'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*D*) 是边缘似然（用于归一化）。'
- en: 'This formula tells you that you need to determine the joint likelihood *P*(*D*|*θ*)
    and the prior *P*(*θ*) , if you want to determine the unnormalized posterior.
    You already have the prior *P*(*θ*) = 1\. What is the joint likelihood? The same
    as in the brute-force approach. Let’s recap. You’ve got three observations (three
    times heads) yielding the joint likelihood:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式告诉你，如果你想确定未归一化的后验，你需要确定联合似然 *P*(*D*|*θ*) 和先验 *P*(*θ*)。你已经有了先验 *P*(*θ*) =
    1。那么联合似然是什么？与暴力方法相同。让我们回顾一下。你有三个观察结果（三次正面），得到联合似然：
- en: '*P*(*D*|*θ*) = *P*(*y* = 1) ⋅ *P*(*y* = 1) ⋅ *P*(*y* = 1) = *θ* ⋅ *θ* ⋅ *θ*
    = *θ*³'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*D*|*θ*) = *P*(*y* = 1) ⋅ *P*(*y* = 1) ⋅ *P*(*y* = 1) = *θ* ⋅ *θ* ⋅ *θ*
    = *θ*³'
- en: 'Now you have everything you need to calculate the posterior of the parameter
    distribution. To help with that, you can use the Bayesian mantra: the posterior
    is proportional to the likelihood times the prior. We’ll fix the normalization
    constant C next.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经拥有了计算参数分布后验所需的一切。为了帮助理解，你可以使用贝叶斯咒语：后验与似然乘以先验成正比。我们将在下一步确定归一化常数 C。
- en: '*P*(*θ*|*D*) = *posterior* = *C* ⋅ *likelihood* ⋅ *pior* = *C* ⋅ *θ*³ ⋅ 1'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*θ*|*D*) = *后验* = *C* ⋅ *似然* ⋅ *先验* = *C* ⋅ *θ*³ ⋅ 1'
- en: 'Let’s derive the normalization constant C that ensures that the posterior integrates
    to one:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们推导确保后验积分为一的归一化常数 C：
- en: '![](../Images/7-11_E01.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-11_E01.png)'
- en: Plugging C = 4 into the formula for the posterior, which you derived above,
    yields the posterior *P*(*θ*|*D*) = 4 ⋅ *θ*³ . Let’s enjoy your derived posterior
    (see figure 7.11, upper right panel). The shape looks similar to what you got
    with the brute-force approach, favoring *θ* values close to one. It’s worth noting
    that the posterior still captures some uncertainty; it doesn’t claim that the
    coin always falls on heads. In this case, the posterior would look like a sharp
    peak at 1 in figure 7.11.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 将 C = 4 代入你上面推导的后验公式，得到后验 *P*(*θ*|*D*) = 4 ⋅ *θ*³。让我们享受你推导出的后验（见图 7.11，右上部分）。形状看起来与暴力方法得到的结果相似，倾向于接近
    1 的 *θ* 值。值得注意的是，后验仍然捕捉到一些不确定性；它并不声称硬币总是落在正面。在这种情况下，后验在图 7.11 中看起来像 1 处的尖锐峰值。
- en: 'Let’s derive the predictive distribution under the posterior. Considering the
    posterior parameter distribution, you’d expect that it assigns a much higher probability
    to heads (*y* = 1) than to tails (*y* = 0). You derive the posterior predictive
    distribution by using a version of equation 7.6, but here we have no *x* and the
    equation looks like this:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们推导后验下的预测分布。考虑到后验参数分布，你可能会预期它为正面（*y* = 1）分配比反面（*y* = 0）更高的概率。你通过使用方程 7.6 的一个版本来推导后验预测分布，但在这里我们没有
    *x*，方程看起来是这样的：
- en: '![](../Images/7-11_E02.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-11_E02.png)'
- en: Equation 7.6 for an unconditional setting (where we have no *x*)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 7.6 对于无条件设置（我们没有任何 *x*）
- en: In the coin toss example, that yields to
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在抛硬币的例子中，这导致
- en: '![](../Images/7-11_E03.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-11_E03.png)'
- en: When plugging in *∫*[0]¹ *P*(*Y* = 1|*θ*) = *∫*[0]¹ θ ⋅ 4 ⋅ *θ*³ *dθ* and *P*(*θ*|*D*)
    = 4 ⋅ *θ*³ , you get
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 当将 *∫*[0]¹ *P*(*Y* = 1|*θ*) = *∫*[0]¹ θ ⋅ 4 ⋅ *θ*³ *dθ* 和 *P*(*θ*|*D*) = 4 ⋅
    *θ*³ 代入时，你得到
- en: '| *P*(*Y* = 1 &#124;*D*) = *∫*[0]¹ *P*(*Y* = 1&#124;*θ*) ⋅ *P*(*θ*&#124;*D*)
    *dθ* | (We use equation 7.4.) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| *P*(*Y* = 1 &#124;*D*) = *∫*[0]¹ *P*(*Y* = 1&#124;*θ*) ⋅ *P*(*θ*&#124;*D*)
    *dθ* | (我们使用方程 7.4.) |'
- en: '| *P*(*Y* = 1 &#124;*D*) = *∫*[0]¹ *θ* ⋅ 4 ⋅ *θ*³ *dθ* = 4/5 ⋅ *θ*⁵ &#124;[0]¹
    | (We do the integration, getting the antiderivative /5 *θ*⁵ of 4θ 4 .) |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| *P*(*Y* = 1 &#124;*D*) = *∫*[0]¹ *θ* ⋅ 4 ⋅ *θ*³ *dθ* = 4/5 ⋅ *θ*⁵ &#124;[0]¹
    | (我们进行积分，得到 4θ⁴ 的反导数 /5 *θ*⁵。) |'
- en: '| *P*(*Y* = 1 &#124;*D*) = 4/5 ⋅ 1⁵ − 4/5 ⋅ *θ* ⁵ = 0.8 | (We put in the numbers.)
    |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| *P*(*Y* = 1 &#124;*D*) = 4/5 ⋅ 1⁵ − 4/5 ⋅ *θ* ⁵ = 0.8 | (我们代入数字。) |'
- en: '| *P*(*Y* = 0) = 1 − *P*(*Y* =1) = 0.2 | (We use the counter probability to
    calculate *P*(*Y* = 0).) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| *P*(*Y* = 0) = 1 − *P*(*Y* =1) = 0.2 | (我们使用逆概率来计算 *P*(*Y* = 0).) |'
- en: Again, you get a similar (but more exact) result as with the brute-force approximation.
    The Bayesian predictive distribution leaves some uncertainty about the outcome
    and gives us a 20% probability to observe tails (see figure 7.11, lower right
    panel). This seems reasonable after observing only three throws. In this case,
    the Bayesian model seems to be preferable to the MaxLike model, which yields a
    predictive distribution that predicts heads with a probability of one (see figure
    7.9, the right panel).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，你得到的结果与暴力近似相似（但更精确）。贝叶斯预测分布对结果留有一定的不确定性，并给出了观察尾巴的20%概率（见图7.11，右下角面板）。在只观察了三次投掷后，这似乎是合理的。在这种情况下，贝叶斯模型似乎比MaxLike模型更可取，后者预测出头的概率为100%（见图7.9，右面板）。
- en: The take-home messages from the coin toss example and the exercises in the notebook
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 硬币投掷示例和笔记本中的练习得出的要点
- en: An analytical derivation of the posterior and the predictive distribution is
    possible in simple models like the Bernoulli model. With the brute-force method,
    you can approximate the analytical solutions. Brute force has the advantage that
    it still works when integration gets difficult. Note that the brute-force method
    isn’t the silver bullet either. It’s not possible to use it for complex problems
    like NNs.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在简单的模型，如伯努利模型中，可以对后验和预测分布进行解析推导。使用暴力方法，你可以近似解析解。暴力方法的优势在于，当积分变得困难时，它仍然有效。请注意，暴力方法也不是万能的。它不能用于像神经网络这样的复杂问题。
- en: Compared to the prior, the posterior gives more mass (probability) to parameter
    values that lead to higher likelihoods for the observed data.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与先验相比，后验给导致观察数据更高似然率的参数值分配了更多的质量（概率）。
- en: Unlike the MaxLike approach, in the Bayes approach you don’t pick only one parameter
    value with the highest probability to derive the predicted distribution. Instead,
    you average over all possible predicted distributions weighted with the posterior
    probabilities of the corresponding parameter values. The larger the training data
    set, the smaller the spread of the posterior (shown in the notebook [http://mng.bz/5a6O](http://mng.bz/5a6O)).
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与MaxLike方法不同，在贝叶斯方法中，你不会只选择具有最高概率的一个参数值来推导预测分布。相反，你会在所有可能的预测分布上平均，这些分布由相应参数值的后验概率加权。训练数据集越大，后验分布的分散就越小（如笔记本[http://mng.bz/5a6O](http://mng.bz/5a6O)中所示）。
- en: The larger the training set, the smaller the influence of the prior (shown in
    the notebook).
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集越大，先验的影响越小（如笔记本中所示）。
- en: For large training data sets, the posterior is a narrow (Gaussian) distribution
    around the MaxLike parameter estimate.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大型训练数据集，后验分布是围绕MaxLike参数估计的一个狭窄（高斯）分布。
- en: Some (fun) facts about the choice of priors
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 关于先验选择的（有趣）事实
- en: You saw in the regression and in the coin toss examples how to train a simple
    Bayesian model. In chapter 8, you’ll see how to train Bayesian DL models. But
    before doing so, let’s try to gain some confidence in the Bayesian approach and
    dissipate concerns on the usage of a prior. In Bayesian modeling, not only do
    you need to pick a distribution for the outcome’s CPD, but you also have to pick
    a distribution for the prior *P*(*θ*) before you see any data.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 你在回归和硬币投掷示例中看到了如何训练一个简单的贝叶斯模型。在第8章中，你将看到如何训练贝叶斯深度学习模型。但在这样做之前，让我们先对贝叶斯方法建立一些信心，并消除对先验使用的担忧。在贝叶斯建模中，你不仅需要为结果的CPD选择一个分布，而且在看到任何数据之前，你还需要为先验*P*(*θ*)选择一个分布。
- en: After getting some training data, you can determine the posterior *P*(*θ*|*D*)
    by using the Bayes theorem (equation 7.5). Still the prior distribution has some
    influence because you use it to determine the posterior. This Bayesian training
    procedure sparked a big discussion among the usually introverted statistician
    folks. The main arguments of the anti-Bayes camp are
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得一些训练数据后，你可以通过使用贝叶斯定理（方程7.5）来确定后验*P*(*θ*|*D*）。然而，先验分布仍然有一定的影响，因为你是用它来确定后验的。这种贝叶斯训练程序在通常内向的统计学家中引发了一场大讨论。反贝叶斯阵营的主要论点是
- en: Working with a prior introduces subjectivity. With the prior you can give a
    high probability to certain parameter ranges. As a consequence, you pull the posterior
    towards this range.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用先验引入了主观性。通过先验，你可以给某些参数范围赋予高概率。结果，你会将后验拉向这个范围。
- en: It’s more scientific to “let the data speak.”
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “让数据说话”更科学。
- en: On the other hand, the Bayes camp argues that
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，贝叶斯阵营认为
- en: All reasonable priors lead to similar models, which all converge for large data
    sets toward the model received with the MaxLike approach.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有合理的先验都会导致类似的模型，这些模型在大型数据集上都会收敛到使用最大似然方法得到的模型。
- en: The prior’s effect of “shrinkage towards the prior” helps to avoid false positive
    results.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 先验的“向先验收缩”效应有助于避免假阳性结果。
- en: As a catchy example to support their second argument, Bayesians picked an empirical
    study (published in a scientific biology journal) that was analyzed in a non-Bayesian
    way and came to the conclusion that good-looking parents get more daughters than
    sons (the p-value was 0.015).a This result was interpreted as an evolutionary
    effect because, for women, it’d be more beneficial to be good looking than for
    men. The results made it to the public media, such as “Daily Mail,” where you
    can find nice pictures of famous and beautiful parents with their firstborn baby,
    a girl (see [http://mng.bz/6Qoe](http://mng.bz/6Qoe) . The link might not work
    if you have ad blockers activate*D*).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持他们的第二个论点，贝叶斯统计学家选择了一项经验研究（发表在科学生物学期刊上），该研究以非贝叶斯方式进行分析，并得出结论：漂亮的父母比不漂亮的父母有更多的女儿（p值为0.015）。a
    这个结果被解释为进化效应，因为对于女性来说，比男性更有利于长得漂亮。这些结果被报道在公共媒体上，例如“每日邮报”，在那里你可以找到著名和美丽的父母和他们第一个女儿的照片（见[http://mng.bz/6Qoe](http://mng.bz/6Qoe)
    。如果你启用了广告拦截器，链接可能无法工作*D*）。
- en: The data came from a British study where teachers were asked to rate the attractiveness
    of their pupils. Forty years later, the grown-up pupils were asked about the gender
    of their children. The non-Bayesian analysis found a significantly higher percentage
    of daughters among the good-looking parents compared to the not-so-good-looking
    parents. Andrew Gelman, a famous Bayesian, re-analyzed the data by using a prior,
    which gave high probabilities on small effects of the parent’s attractiveness
    to their offspring’s gender. He justified his prior choice by the fact that all
    other known influential factors on the offspring’s gender (such as, for example,
    the stress of the parents during pregnancy) were also small. His analysis led
    to the conclusion that the attractiveness of the parents doesn’t impact the likelihood
    of getting a baby girl.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 数据来自一项英国研究，研究人员要求教师对学生的吸引力进行评分。四十年后，这些长大成人的学生被问及自己孩子的性别。非贝叶斯分析发现，与不那么吸引人的父母相比，漂亮的父母中女儿的比例显著更高。著名贝叶斯统计学家安德鲁·杰尔曼通过使用先验，对父母吸引力对其子女性别的影响给予了较高的概率，重新分析了这些数据。他通过以下事实来证明其先验选择的合理性：所有其他已知的影响子女性别的因素（例如，例如，父母在怀孕期间的压力）也都是小的。他的分析得出结论，父母的吸引力不会影响生女孩的可能性。
- en: To support the usage of priors, Gelman did a simulation study with small, true
    effect sizes and quite small sample sizes. Analyzing these data in a non-Bayesian
    manner led in most simulation runs to non-significant findings. But due to its
    random nature, some runs led to significant results. Among those significant results,
    40% of the reported effects pointed in the wrong direction! And if the result
    was catchy, it got published. Therefore, Gelman reasoned, it’s more reasonable
    to perform a Bayesian analysis with a conservative prior serving as a regularization
    method.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持先验的使用，杰尔曼进行了一项模拟研究，研究中有小到中等的效果大小和相当小的样本量。在大多数模拟运行中，以非贝叶斯方式分析这些数据导致了非显著的结果。但由于其随机性，一些运行导致了显著的结果。在这些显著结果中，40%的报告效应指向了错误的方向！而且如果结果引人注目，它就会被发表。因此，杰尔曼认为，使用保守的先验作为正则化方法进行贝叶斯分析更为合理。
- en: 'Is it reasonable for DL models to regularize the weights by using a prior that
    prefers small weight values? We think yes, and we give you some reasons:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型使用偏好小权重值的先验来正则化权重是否合理？我们认为是的，并给出一些理由：
- en: Experience shows that trained NNs often have small weights.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经验表明，经过训练的神经网络通常具有较小的权重。
- en: Smaller weights lead to less extreme outputs (in classification, less extreme
    probabilities), which is desirable for an untrained model.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较小的权重导致输出（在分类中，概率）不那么极端，这对于未经训练的模型来说是理想的。
- en: It’s a known property of prediction models that adding a component to the loss
    function, which prefers small weights, often helps to get a higher prediction
    performance. This approach is also known as regularization or weights decay in
    non-Bayesian NNs.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测模型的一个已知属性是，向损失函数中添加一个偏好较小权重的组件，通常有助于提高预测性能。这种方法也被称为正则化或非贝叶斯神经网络中的权重衰减。
- en: a The p value estimates the probability that an observed effect (or a stronger
    effect) is found due to pure chance. Usually a finding with a p value below 0.05
    is called statistically significant.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: a p值估计的是由于纯偶然性发现观察到的效应（或更强的效应）的概率。通常，p值低于0.05的发现被称为具有统计学意义。
- en: 7.3.3 Revisiting the Bayesian linear regression model
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.3 重新审视贝叶斯线性回归模型
- en: In the beginning of this chapter, you saw how to do Bayesian linear regression
    the hacker’s way. In the hacker’s way, we used a Bayes model with an infinitely
    large prior for the two parameters *a* and *b*. The resulting model had a nice
    property to get more uncertainty in the extrapolation range where no training
    data is available (see the right side of figure 7.12). This isn’t the case for
    a traditional model fitted via the MaxLike approach (see the left side of figure
    7.12).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，您看到了如何以黑客的方式做贝叶斯线性回归。在黑客的方式中，我们使用了具有无限大先验的贝叶斯模型来处理两个参数*a*和*b*。这个模型有一个很好的特性，即在没有训练数据的外推范围内获得更多的不确定性（参见图7.12的右侧）。而传统的通过MaxLike方法拟合的模型则不是这样（参见图7.12的左侧）。
- en: '![](../Images/7-12.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图7-12](../Images/7-12.png)'
- en: Figure 7.12 Four data points fitted with a probabilistic linear regression model
    when assuming a known data spread, *σ* = 3\. On the left, you see the MaxLike
    model; on the right, the Bayesian model with an infinitely large prior. The solid
    line represents the mean of the predicted outcome distribution, and the dashed
    lines show the 2.5% and 97.5% percentiles of the CPD.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 在假设已知数据分布，*σ* = 3的情况下，用概率线性回归模型拟合了四个数据点。在左侧，您可以看到MaxLike模型；在右侧，是具有无限大先验的贝叶斯模型。实线代表预测结果分布的均值，虚线表示CPD的2.5%和97.5%分位数。
- en: 'Because only a Bayesian model can express an enhanced uncertainty when leaving
    known grounds, it seems that fitting a Bayesian model is the better choice than
    fitting a traditional MaxLike-based model. This is visible in the two fits in
    figure 7.12\. When looking at the width of the predicted CPD, the Bayes model
    predicts a much broader CPD when leaving the range in which there’s data. But,
    also, in the interpolation range of the training data, the Bayes model has a higher
    uncertainty than the MaxLike model. Because the model fit relies on only four
    data points, the higher uncertainty of the Bayes model might be more realistic,
    which would then also support our vote for the Bayesian model. On the other hand,
    the large spread could also only be due to the broad prior (a uniform distribution),
    which was used for the model parameters (slope and intercept). To investigate
    if the Bayesian model yields more realistic predictions for the conditional outcome
    distribution, let’s perform some experiments to answer the following questions:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 由于只有贝叶斯模型能够在离开已知领域时表达增强的不确定性，因此似乎拟合贝叶斯模型比拟合传统的基于MaxLike的模型更好。这在图7.12中的两个拟合中是显而易见的。当观察预测CPD的宽度时，贝叶斯模型在离开有数据范围时预测了一个更宽的CPD。但是，在训练数据的插值范围内，贝叶斯模型的不确定性也比MaxLike模型高。因为模型拟合仅依赖于四个数据点，贝叶斯模型较高的不确定性可能更符合实际情况，这也支持我们选择贝叶斯模型。另一方面，大的分散也可能只是由于用于模型参数（斜率和截距）的宽泛先验（均匀分布）。为了调查贝叶斯模型是否为条件结果分布提供更现实的预测，让我们进行一些实验来回答以下问题：
- en: How does the predictive distribution depend on the prior and the amount of training
    data?
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测分布如何依赖于先验和训练数据量？
- en: Has a Bayesian model a better prediction performance than a traditional MaxLike-based
    model?
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯模型是否比传统的基于MaxLike的模型有更好的预测性能？
- en: 'To answer these two questions, you can perform the following experiments:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这两个问题，你可以进行以下实验：
- en: 'Simulate from a linear data generating process (with *σ* = 3) several training
    data sets with different sizes (for example, with 2, 4, 20, and 100 data points).
    Then fit for all training sets the MaxLike model and three Bayesian models: one
    with a uniform prior, one with a standard Normal prior, and one with a mean-centered
    Normal with a scale of 0.1\. Check how the resulting CPD changes with the width
    of the prior. Check also that the width of the Bayesian CPD decreases with growing
    training data and gets more similar to the CPD of the MaxLike model.'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从具有*σ* = 3的线性数据生成过程中模拟出几个不同大小的训练数据集（例如，2、4、20和100个数据点）。然后对所有训练集拟合MaxLike模型和三个贝叶斯模型：一个具有均匀先验，一个具有标准正态先验，一个具有以0.1为尺度的均值中心正态分布。检查结果CPD如何随着先验宽度的变化而变化。同时检查贝叶斯CPD的宽度是否随着训练数据的增加而减小，并逐渐接近MaxLike模型的CPD。
- en: Investigate for different training set sizes if the prediction performance is
    better for the Bayesian or the MaxLike-based model. Sample a training set and
    a test set that has *x* values in the range of the training set. Then fit a Bayesian
    and the MaxLike model and determine the test negative log likelihood (NLL). (The
    model with the better prediction performance yields a lower test NLL.) To get
    reliable results, you should use large test data sets and repeat the whole procedure
    several times, so that you can compare the resulting distributions and means of
    the obtained test NLLs.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调查不同训练集大小的情况下，贝叶斯模型或基于最大似然（MaxLike）的模型的预测性能是否更好。从训练集的 *x* 值范围内采样一个训练集和一个测试集。然后拟合贝叶斯模型和最大似然模型，并确定测试集的负对数似然（NLL）。（预测性能更好的模型会产生更低的测试NLL。）为了得到可靠的结果，你应该使用大的测试数据集，并重复整个过程几次，以便比较获得的测试NLLs的结果分布和均值。
- en: Before getting started with these experiments, you need to make sure that you
    can conduct them in a reasonable amount of time. Unfortunately, the brute-force
    method for Bayesian fitting is much too slow. We could speed up the fitting procedure
    a lot if we knew the analytical solution. You saw in the coin toss example in
    section 7.3.2 that determining the analytical Bayes solution requires solving
    some integrals. You were able to solve these integrals in the coin toss example
    because you only had one model parameter and an easy likelihood and prior distribution.
    But in a more complex model with many parameters, these integrals get quickly
    complicated. It turns out that Bayes usually can’t be solved analytically, and
    one has to resort to simulations or approximations. This is also the reason that
    Bayes models weren’t popular before massive computation power became available.
    You’ll learn more about those approximations in chapter 8.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始这些实验之前，你需要确保可以在合理的时间内完成它们。不幸的是，贝叶斯拟合的暴力方法太慢了。如果我们知道解析解，我们可以大大加快拟合过程。你在7.3.2节中的抛硬币例子中看到，确定解析贝叶斯解需要解决一些积分。你能够在抛硬币例子中解决这些积分，因为你只有一个模型参数，并且似然和先验分布都很简单。但在具有许多参数的更复杂模型中，这些积分会迅速变得复杂。结果发现，贝叶斯通常不能解析求解，而必须求助于模拟或近似。这也是贝叶斯模型在大量计算能力出现之前并不受欢迎的原因。你将在第8章中了解更多关于这些近似的内容。
- en: But for a simple linear regression model, such as the hacker’s example, it’s
    still possible to derive the Bayes solution. For that you need to assume that
    the data variance *σ* 2 is known (like we did in the hacker’s example). Further,
    you need to use a Gaussian prior for the model parameter’s slope and intercept.
    But already then, the derivations are quite lengthy. Therefore, you can skip the
    math and directly implement the resulting analytical formulas to compute the posterior
    and predictive distribution (see the last notebook in this chapter). A complete
    treatment and derivation of the formulas used is given in Christopher M. Bishop’s,
    Pattern Recognition and Machine Learning, which you can access via [http://mng.bz/oPWZ](http://mng.bz/oPWZ)
    .
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个简单的线性回归模型，例如黑客的例子，仍然可以推导出贝叶斯解。为此，你需要假设数据方差 *σ* 2 是已知的（就像我们在黑客的例子中所做的那样）。进一步，你需要为模型参数的斜率和截距使用高斯先验。但即便如此，推导过程已经相当冗长。因此，你可以跳过数学推导，直接实现计算后验和预测分布的解析公式（参见本章最后一份笔记本）。Christopher
    M. Bishop 的《模式识别与机器学习》中给出了公式的完整处理和推导，你可以通过[http://mng.bz/oPWZ](http://mng.bz/oPWZ)
    访问。
- en: Using an analytical expression, you can check that you can reproduce the results
    of the Bayes hacker’s example in section 7.2\. In the hacker’s example, we worked
    with a uniform prior. This can be achieved by setting in the analytical solution
    the scale parameter of the Gaussian prior to infinity (or something very large).
    If you want to convince yourself, we recommend you go to the last part of the
    Bayes the Hacker’s Way notebook from the beginning of this chapter (see [http://mng.bz/qMEr](http://mng.bz/qMEr)).
    Now, that you have the fast, analytical Bayes solution in hand, you can do the
    two suggested experiments in the following notebook.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 使用解析表达式，你可以检查是否可以重现7.2节中贝叶斯黑客例子的结果。在黑客的例子中，我们使用了均匀先验。这可以通过在解析解中将高斯先验的尺度参数设置为无穷大（或非常大）来实现。如果你想自己验证，我们建议你从本章开头的贝叶斯黑客之道笔记本的最后部分开始（参见[http://mng.bz/qMEr](http://mng.bz/qMEr)）。现在，你已经掌握了快速、解析的贝叶斯解，你可以在下面的笔记本中执行两个建议的实验。
- en: '| ![](../Images/computer-icon.png) | Hands-on time Open [http://mng.bz/nPj5](http://mng.bz/nPj5)
    . This notebook produces the plots in figure 7.13 and answers the following questions:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '| ![计算机图标](../Images/computer-icon.png) | 实践时间 打开[http://mng.bz/nPj5](http://mng.bz/nPj5)。这个笔记本生成了图7.13中的图表，并回答以下问题：'
- en: How does the predictive distribution depend on the prior and the amount of training
    data?
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测分布如何依赖于先验和训练数据量？
- en: Has a Bayesian model a better prediction performance than a traditional MaxLike-based
    model?
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯模型是否比传统的基于MaxLike的模型有更好的预测性能？
- en: '|'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Using this notebook, you can investigate these two questions. Let’s look at
    the first question: How does the predictive distribution depend on the prior and
    the amount of training data? The short answer is that the choice of the prior
    distribution isn’t critical if it’s not extremely narrow. The larger the training
    data, the closer the Bayes model gets to the MaxLike model. In the notebook, you
    can work this out step by step.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个笔记本，你可以调查这两个问题。让我们看看第一个问题：预测分布如何依赖于先验和训练数据量？简短的回答是，如果先验分布不是极其狭窄，那么先验分布的选择并不关键。训练数据量越大，贝叶斯模型就越接近MaxLike模型。在笔记本中，你可以一步步地解决这个问题。
- en: In the Gaussian prior of the analytical solution, you can set the mean and the
    standard deviation. As the mean of the prior, you pick zero because you don’t
    know if the intercept or slope are positive or negative numbers. The main tuning
    parameter is the scale, *σ* 0, of the prior. Setting it to a large number like
    *σ*[0] = 10000 is equivalent to a flat prior, and setting it to a small number
    like *σ*[0] = 0.1 results in a peaky prior around the mean at zero. In figure
    7.13, you can see that a prior with *σ*[0] = 0.1 yields a predictive distribution
    with a slope close to 1 and a relatively small width that doesn’t change over
    the displayed *x* range. A prior with *σ* 0 = 1, on the other hand, yields a CPD
    that’s quite similar to the CPD corresponding to a flat prior with *σ*[0] = 10000
    . This indicates that a Gaussian prior with a 0 mean and a standard deviation
    of 1 doesn’t impose a large bias on the resulting fit. These statements are valid
    for all sizes of the training data set (see the upper and lower rows in figure
    7.13). The main effect of the training data size is the reduced epistemic uncertainty
    of the Bayes model when working with a larger training data set. In the limits
    of a large training data set, the uncertainty is purely aleatoric and a Bayes
    model yields the same CPD as a MaxLike-based model (see the rightmost column in
    figure 7.13).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在解析解的高斯先验中，你可以设置均值和标准差。作为先验的均值，你选择零，因为你不知道截距或斜率是正数还是负数。主要的调整参数是先验的尺度*σ*0。将其设置为较大的数值，如*σ*[0]
    = 10000，相当于平坦的先验，而将其设置为较小的数值，如*σ*[0] = 0.1，则会在零均值周围产生尖锐的先验。在图7.13中，你可以看到*σ*[0]
    = 0.1的先验会产生一个斜率接近1的预测分布，其宽度相对较小，并且在显示的*x*范围内不发生变化。另一方面，*σ*0 = 1的先验会产生一个与*σ*[0]
    = 10000的平坦先验相对应的CPD。这表明均值为0、标准差为1的高斯先验对结果的拟合没有大的偏差。这些陈述对所有大小的训练数据集都有效（见图7.13的上行和下行）。训练数据量大小的主要影响是，当使用较大的训练数据集时，贝叶斯模型的认知不确定性降低。在大训练数据集的极限情况下，不确定性纯粹是随机的，贝叶斯模型产生的CPD与基于MaxLike的模型相同（见图7.13的最右侧列）。
- en: '![](../Images/7-13.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![7-13图](../Images/7-13.png)'
- en: Figure 7.13 Influence of the prior’s scale, *σ* 0, on the resulting predicted
    CPD in a Bayesian linear model (first three columns) and the corresponding CPD
    of a MaxLike-based linear model (rightmost column). The Bayesian models work with
    a given data standard deviation of 3 and a Gaussian prior for the slope and intercept
    with a mean 0 and a standard deviation of 0.1, 1, and 10,000, respectively (see
    the titles of the plots). In the upper row, the training data contained 4 data
    points; in the lower row, 20 data points.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13展示了先验尺度*σ*0对贝叶斯线性模型（前三个列）中得到的预测CPD的影响，以及基于MaxLike的线性模型对应的CPD（最右侧列）。贝叶斯模型使用给定的数据标准差为3，斜率和截距的先验为高斯分布，均值为0，标准差分别为0.1、1和10,000（见图表标题）。在上行中，训练数据包含4个数据点；在下行中，包含20个数据点。
- en: 'Let’s turn to the second question: Has a Bayesian model a better prediction
    performance than a traditional MaxLike-based model? The short answer is yes! As
    discussed in chapter 5, the test NLL is the proper measure to quantify and compare
    the prediction performance of different models: the lower, the better. To investigate
    the test NLL difference of the Bayes and MaxLike models beyond random variations,
    you average the test NLL over 100 models, each working with newly generated data.
    Figure 7.14 shows the results. You can see that the Bayes model outperforms the
    MaxLike-based model. The smaller the training data set, the more you gain in prediction
    performance when using a Bayesian model instead of a traditional MaxLike-based
    model.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们转向第二个问题：贝叶斯模型是否比传统的基于最大似然模型有更好的预测性能？简短的答案是肯定的！如第5章所述，测试负对数似然（NLL）是量化比较不同模型预测性能的适当度量：越低越好。为了调查贝叶斯和最大似然模型在测试NLL上的差异，并超出随机变化，你需要在100个模型上平均测试NLL，每个模型使用新生成数据。图7.14显示了结果。你可以看到贝叶斯模型优于基于最大似然模型。当使用贝叶斯模型而不是传统的基于最大似然模型时，训练数据集越小，预测性能的提升就越大。
- en: '![](../Images/7-14.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图7.14](../Images/7-14.png)'
- en: Figure 7.14 Prediction performance comparison of a Bayes model and MaxLike-based
    linear model via a test NLL (the lower, the better). Both models work with a given
    data variance. The Bayes model works for slope and intercept with a Gaussian with
    a mean of 0 and a scale of 1.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14通过测试NLL（越低越好）比较贝叶斯模型和基于最大似然线性模型的预测性能。两个模型都使用给定的数据方差。贝叶斯模型使用均值为0和尺度为1的高斯分布来处理斜率和截距。
- en: Is there an intuitive explanation why the Bayesian method outperforms the MaxLike
    method? Remember the example of financial experts in section 7.3.1? The Bayes
    approach uses the wisdom of many, while the MaxLike method relies on the expertise
    of the single best expert. If there is little data, it makes sense to listen to
    many experts.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 有没有直观的解释说明为什么贝叶斯方法优于最大似然方法？还记得第7.3.1节中的金融专家的例子吗？贝叶斯方法使用了许多人的智慧，而最大似然方法依赖于单个最佳专家的专长。如果数据很少，听取许多专家的意见是有意义的。
- en: Before turning to Bayesian NN in the next chapter, let’s summarize what you
    should take away from this introduction to Bayesian modeling. Bayesian models
    can capture epistemic uncertainty about its parameter values via probability distributions.
    To take a Bayesian approach, you need to pick a prior for the parameter distribution.
    The prior can be a constant (uniformly distribute*D*) or bell-shaped (normal distributed,
    often with a mean of zero), designed to incorporate a priori knowledge or to regularize
    the model. When training a Bayesian model, the posterior is determined. The more
    data used for training, the smaller is the spread (variance) of the posterior,
    indicating a decreased parameter (epistemic) uncertainty. If you have a large
    training data set, your Bayesian model yields similar results as a MaxLike model.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章转向贝叶斯神经网络之前，让我们总结一下你应该从贝叶斯建模的介绍中吸取的内容。贝叶斯模型可以通过概率分布捕捉关于其参数值的认知不确定性。要采取贝叶斯方法，你需要为参数分布选择一个先验。先验可以是常数（均匀分布*D*）或钟形（正态分布，通常均值为零），旨在包含先验知识或正则化模型。当训练贝叶斯模型时，后验被确定。用于训练的数据越多，后验的分布（方差）越小，这表明参数（认知）不确定性降低。如果你有一个大的训练数据集，你的贝叶斯模型会产生与最大似然模型相似的结果。
- en: Summary
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The inherent uncertainty in the data, called aleatoric uncertainty, can be modeled
    with the probabilistic approach introduced in chapters 4 to 6.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中的固有不确定性，称为随机不确定性，可以用第4章到第6章中介绍的概率方法进行建模。
- en: In addition, Bayesian probabilistic models also capture the epistemic uncertainty.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，贝叶斯概率模型也捕捉了认知不确定性。
- en: The epistemic uncertainty is caused by the uncertainty to the model parameter.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 认知不确定性是由对模型参数的不确定性引起的。
- en: Non-Bayesian models fail to express uncertainty when leaving known grounds.
    (These can’t talk about the elephant in the room.)
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非贝叶斯模型在离开已知基础时无法表达不确定性。（这些模型无法谈论房间里的大象。）
- en: Bayesian models can express uncertainty when doing predictions in the extrapolation
    regime or in the case of insufficient training data.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯模型在预测外推区域或训练数据不足的情况下可以表达不确定性。
- en: In Bayesian models, each parameter is replaced by a distribution.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在贝叶斯模型中，每个参数都被一个分布所取代。
- en: Before fitting a Bayesian model, you need to pick a prior distribution.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在拟合贝叶斯模型之前，你需要选择一个先验分布。
- en: The Bayes mantra is, “The posterior is the prior times the likelihood.” It is
    a consequence of the Bayes theorem.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯咒语是：“后验是先验乘以似然。”这是贝叶斯定理的一个结果。
- en: As opposed to the aleatoric data inherent uncertainty, you can reduce the epistemic
    model parameter uncertainty by extending the training data, resulting in a posterior
    with lower variance.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与固有的随机数据不确定性相反，通过扩展训练数据，你可以减少认知模型参数的不确定性，从而得到一个方差更低的后验分布。
- en: A Bayesian model shows a better prediction performance than non-Bayesian variants
    if the training data is limited.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果训练数据有限，贝叶斯模型比非贝叶斯变体显示出更好的预测性能。
- en: 1.This might ring a bell for those of you who have some experience with statistical
    regression models looking at the waisted prediction band in figure 7.6\. Indeed,
    you get a similar-looking result when computing a prediction interval that incorporates
    the confidence interval for the mean parameter of the conditional Normal distribution.
    Such a confidence interval for model parameters can be computed without Bayesian
    statistics, at least for models that are not too big and too complex. But for
    a DL model that is a complex non-linear model with millions of parameters, these
    non-Bayesian uncertainty measures require a resampling approach and many rounds
    of refitting the NN, which is too time-consuming to be feasible.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 对于那些对统计回归模型有一定经验的人来说，这可能会引起共鸣，他们可能会查看图7.6中的预测带。确实，当计算包含条件正态分布均值参数置信区间的预测区间时，你会得到一个类似的结果。对于不是太大太复杂的模型，这种模型参数的置信区间可以通过贝叶斯统计方法计算出来。但对于具有数百万参数的复杂非线性深度学习模型，这些非贝叶斯的不确定性度量需要重新采样方法以及多次重新拟合神经网络，这太耗时了，以至于不可行。
- en: 2.In this book, we’re a bit relaxed when it comes to mathematical notation.
    If the parameter *θ* is a continuous quantity, we should call it probability density
    instead of a probability, but we don’t do this here and in the rest of the book.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 在这本书中，我们对数学符号的使用比较宽松。如果参数 *θ* 是一个连续量，我们应该称之为概率密度而不是概率，但在这里以及本书的其余部分，我们并没有这样做。
