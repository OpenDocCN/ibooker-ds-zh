- en: 8 Learning with categorical features
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 使用分类特征进行学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Introducing categorical features in machine learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器学习中引入分类特征
- en: Preprocessing categorical features using supervised and unsupervised encoding
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用监督和未监督编码对分类特征进行预处理
- en: Understanding ordered boosting
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解有序提升
- en: Using CatBoost for categorical variables
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CatBoost对分类变量进行处理
- en: Handling high-cardinality categorical features
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理高基数分类特征
- en: 'Data sets for supervised machine learning consist of features that describe
    objects and labels that describe the targets we’re interested in modeling. At
    a high level, features, also known as attributes or variables, are usually classified
    into two types: continuous and categorical.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机器学习的数据集由描述对象的特征和描述我们感兴趣建模的目标的标签组成。在较高层次上，特征，也称为属性或变量，通常分为两种类型：连续型和分类型。
- en: 'A *categorical* feature is one that takes a discrete value from a set of finite,
    nonnumeric values, called categories. Categorical features are ubiquitous and
    appear in nearly every data set and in every domain. For example:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一个**分类**特征是指从一个有限且非数值的值集中取值的特征，这些值被称为类别。分类特征无处不在，几乎出现在每个数据集和每个领域中。例如：
- en: '*Demographic features*—These features, such as gender or race, are common attributes
    in many modeling problems in medicine, insurance, finance, advertising, recommendation
    systems, and many more. For instance, the US Census Bureau’s race attribute is
    a categorical feature that admits five choices or categories: (1) American Indian
    or Alaska Native, (2) Asian, (3) Black or African American, (4) Native Hawaiian
    or Other Pacific Islander, (5) White.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*人口统计特征*—这些特征，如性别或种族，是医学、保险、金融、广告、推荐系统等许多建模问题中的常见属性。例如，美国人口普查局的种族属性是一个允许五种选择或类别的分类特征：（1）美国印第安人或阿拉斯加原住民，（2）亚洲，（3）黑人或非裔美国人，（4）夏威夷原住民或其他太平洋岛民，（5）白人。'
- en: '*Geographical features*—These features, such as US State or ZIP code, are also
    categorical features. The feature US State is a categorical variable with 50 categories.
    The feature ZIP code is also a categorical variable, with 41,692 unique categories
    (!) in the United States, from 00501, belonging to the Internal Revenue Service
    in Holtsville, NY, to 99950 in Ketchikan, AK.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*地理特征*—这些特征，如美国州或ZIP代码，也是分类特征。特征“美国州”是一个有50个类别的分类变量。特征“ZIP代码”也是一个分类变量，在美国有41,692个独特的类别（！）从纽约霍尔特斯维尔的美国国内税务局的00501到阿拉斯加凯奇坎的99950。'
- en: Categorical features are usually represented as strings or in specific formats
    (e.g., ZIP codes, which have to be exactly five digits long and can start with
    zeros).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 分类特征通常表示为字符串或特定格式（例如，ZIP代码，必须恰好五位数字长，可以以零开头）。
- en: Since most machine-learning algorithms require numeric inputs, categorical features
    must be *encoded* or converted to numeric form before training. The nature of
    this encoding must be carefully chosen to capture the true underlying nature of
    the categorical features.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大多数机器学习算法需要数值输入，因此在训练之前必须将分类特征**编码**或转换为数值形式。这种编码的性质必须仔细选择，以捕捉分类特征的真正潜在性质。
- en: 'The ensemble setting has two approaches for handling categorical features:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 集成设置有两种处理分类特征的方法：
- en: '*Approach 1*—Preprocess categorical features using one of several standard
    or general-purpose encoding techniques available in libraries such as scikit-learn,
    and then train ensemble models with packages such as LightGBM or XGBoost with
    the preprocessed features.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*方法 1*—使用scikit-learn等库中提供的几种标准或通用编码技术对分类特征进行预处理，然后使用LightGBM或XGBoost等包用预处理后的特征训练集成模型。'
- en: '*Approach 2*—Use an ensemble method, such as CatBoost, that is designed to
    handle categorical features to train ensembles directly and carefully.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*方法 2*—使用如CatBoost这样的集成方法，该方法专为处理分类特征而设计，可以直接且仔细地训练集成模型。'
- en: 'Section 8.1 covers approach 1\. It introduces commonly used preprocessing methods
    for categorical features and how we can use them in practice (using the category
    _encoders package) with any machine-learning algorithm, including ensemble methods.
    Section 8.1 also discusses two common problems: training-to-test-set leakage and
    training-to-test-set distribution shift, or prediction shift, which affect our
    ability to accurately evaluate the generalization ability of our models to future,
    unseen data.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 第8.1节涵盖了方法1。它介绍了用于分类特征的常用预处理方法以及如何在实践中使用它们（使用category_encoders包）与任何机器学习算法一起使用，包括集成方法。第8.1节还讨论了两个常见问题：训练集到测试集泄露和训练集到测试集分布偏移，或预测偏移，这些问题影响我们评估模型泛化能力到未来未见数据的能力。
- en: Section 8.2 covers approach 2 and introduces a new ensemble approach called
    ordered boosting, which is an extension of boosting approaches we’ve already seen
    but is specially modified to address leakage and shift for categorical features.
    This section also introduces the CatBoost package and shows how we can use it
    to train ensemble methods on data sets with categorical features. We explore both
    approaches in a real-world case study in section 8.3, where we compare random
    forest, LightGBM, XGBoost, and CatBoost on an income-prediction task.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第8.2节涵盖了方法2，并介绍了一种新的集成方法，称为有序提升，它是我们之前看到的提升方法的扩展，但特别修改以解决分类特征的泄露和偏移问题。本节还介绍了CatBoost包，并展示了我们如何使用它来在具有分类特征的数据集上训练集成方法。我们在第8.3节中的实际案例研究中探讨了这两种方法，其中我们比较了随机森林、LightGBM、XGBoost和CatBoost在收入预测任务上的表现。
- en: Finally, many general-purpose approaches don’t scale well to high-cardinality
    categorical features (where the number of categories is very high, such as ZIP
    code) or in the presence of noise, or so-called “dirty” categorical variables.
    Section 8.4 shows how we can effectively handle such high-cardinality categories
    with the dirty_cat package.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，许多通用方法在处理高基数分类特征（如邮编，类别数量非常高）或存在噪声，或所谓的“脏”分类变量时，扩展性不好。第8.4节展示了我们如何使用dirty_cat包有效地处理这类高基数类别。
- en: 8.1 Encoding categorical features
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 编码分类特征
- en: 'This section reviews the different types of categorical features and introduces
    two classes of standard approaches to handling them: unsupervised encoding (specifically,
    ordinal and one-hot encoding) and supervised encoding (specifically, with target
    statistics).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了不同类型的分类特征，并介绍了处理它们的两种标准方法类别：无监督编码（特别是有序和独热编码）和监督编码（特别是使用目标统计）。
- en: Encoding techniques, like machine-learning methods, are either *unsupervised*
    or *supervised*. Unsupervised encoding methods use only the features to encode
    categories, while supervised encoding methods use both features and targets.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 编码技术，就像机器学习方法一样，可以是*无监督*或*监督*的。无监督编码方法仅使用特征来编码类别，而监督编码方法使用特征和目标。
- en: We’ll also see how supervised encoding techniques can lead to degraded performance
    in practice owing to a phenomenon called *target leakage*. This will help us understand
    the motivations behind the development of the ordinal boosting approach, which
    we’ll explore in section 8.3.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将看到监督编码技术如何因称为*目标泄露*的现象而导致实际性能下降。这将帮助我们理解开发有序提升方法背后的动机，我们将在第8.3节中探讨该方法。
- en: 8.1.1 Types of categorical features
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 分类特征的类型
- en: A categorical feature contains information about a category or group that a
    training example belongs to. The values, or categories, that make up such variables
    are often represented using strings or other nonnumeric tags.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 分类特征包含有关训练示例所属类别或组的信息。构成此类变量的值或类别通常使用字符串或其他非数字标签表示。
- en: 'Broadly, categorical features are of two types: *ordinal*, where an ordering
    exists between the categories, and *nominal*, where no ordering exists between
    the categories. Let’s look closely at nominal and ordinal categorical features
    in the context of a hypothetical fashion task, where the goal is to train a machine-learning
    algorithm to predict the cost of a T-shirt. Each T-shirt is described by two attributes:
    color and size (figure 8.1).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 广义上，分类特征分为两种类型：*有序*，其中类别之间存在顺序，和*无序*，其中类别之间不存在顺序。让我们在假设的时尚任务背景下仔细看看无序和有序的分类特征，该任务的目的是训练一个机器学习算法来预测T恤的成本。每件T恤由两个属性描述：颜色和尺寸（图8.1）。
- en: '![CH08_F01_Kunapuli](../Images/CH08_F01_Kunapuli.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F01_Kunapuli](../Images/CH08_F01_Kunapuli.png)'
- en: 'Figure 8.1 T-shirts in this example data set are described using two categorical
    features: color and size. Categorical features can be either (1) nominal, where
    there is no ordering between the various categories, or (2) ordinal, where there
    is an ordering between the categories. The third feature in this data set, cost,
    is a continuous, numeric variable.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 在这个示例数据集中，T恤是通过两个类别特征（颜色和尺寸）描述的。类别特征可以是（1）名义的，其中各种类别之间没有顺序，或（2）有序的，其中类别之间存在顺序。本数据集中的第三个特征，成本，是一个连续的数值变量。
- en: 'The feature color takes three discrete values: red, blue, and green. No ordering
    exists between these categories, which makes color a nominal feature. Since it
    doesn’t matter how we order color’s values, the ordering red-blue-green is equivalent
    to other ordering permutations such as blue-red-green or green-red-blue.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 特征颜色有三个离散值：红色、蓝色和绿色。这些类别之间不存在顺序，这使得颜色成为一个名义特征。由于颜色的值排序无关紧要，红色-蓝色-绿色的排序与其他排序排列，如蓝色-红色-绿色或绿色-红色-蓝色，是等价的。
- en: 'The feature size takes four discrete values: S, M, L, and XL. Unlike, color,
    however, there is an implicit ordering between the sizes: S < M < L < XL. This
    makes size an ordinal feature. While we can order sizes any way we want, ordering
    them in increasing order of size, S-M-L-XL, or in decreasing order of size, XL-L-M-S,
    is most sensible. Understanding the domain and the nature of each categorical
    feature is an important component of deciding how to encode them.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 特征尺寸有四个离散值：S、M、L和XL。然而，与颜色不同，尺寸之间存在隐含的顺序：S < M < L < XL。这使得尺寸成为一个有序特征。虽然我们可以以任何方式对尺寸进行排序，但按尺寸递增顺序排序，S-M-L-XL，或按尺寸递减顺序排序，XL-L-M-S，是最合理的。理解每个类别特征的领域和性质是决定如何编码它们的重要部分。
- en: 8.1.2 Ordinal and one-hot encoding
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 有序和独热编码
- en: Categorical variables such as color and size have to be encoded, that is, converted
    to some sort of numeric representation prior to training a machine-learning model.
    Encoding is a type of feature engineering and must be done with care because an
    inappropriate choice of encoding can affect model performance and interpretability.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 类别变量，如颜色和尺寸，必须在训练机器学习模型之前进行编码，即转换为某种数值表示。编码是一种特征工程，必须谨慎进行，因为编码选择不当可能会影响模型性能和可解释性。
- en: 'In this section, we’ll look at two commonly used unsupervised methods of encoding
    categorical variables: *ordinal encoding* and *one-hot encoding*. They are unsupervised
    because they don’t use the targets (labels) for encoding.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨两种常用的无监督类别变量编码方法：*有序编码*和*独热编码*。它们是无监督的，因为它们在编码时不使用目标（标签）。
- en: Ordinal encoding
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有序编码
- en: 'Ordinal encoding simply assigns each category a number. For example, the nominal
    feature color can be encoded by assigning {''red'': 0, ''blue'': 1, ''green'':
    2}. Since the categories don''t have any implicit ordering, we could have also
    encoded by assigning other permutations such as {''red'': 2, ''blue'': 0, ''green'':
    1}.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '有序编码简单地给每个类别分配一个数字。例如，名义特征颜色可以通过分配{''red'': 0, ''blue'': 1, ''green'': 2}进行编码。由于类别没有隐含的顺序，我们也可以通过分配其他排列，如{''red'':
    2, ''blue'': 0, ''green'': 1}进行编码。'
- en: 'On the other hand, since size is already an ordinal variable, it makes sense
    to assign numeric values to preserve this ordering. For size, either encoding
    with {''S'': 0, ''M'': 1, ''L'': 2, ''XL'': 3} (increasing) or {''S'': 3, ''M'':
    2, ''L'': 1, ''XL'': 0} (decreasing) preserves the inherent relationship between
    the size categories.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '另一方面，由于尺寸已经是有序变量，因此为保持这种顺序而分配数值是有意义的。对于尺寸，可以使用{''S'': 0, ''M'': 1, ''L'': 2,
    ''XL'': 3}（递增）或{''S'': 3, ''M'': 2, ''L'': 1, ''XL'': 0}（递减）进行编码，以保持尺寸类别之间的固有关系。'
- en: 'scikit-learn’s OrdinalEncoder can be used to create ordinal encodings. Let’s
    encode the two categorical features (color and size) in the data set from figure
    8.1 (denoted by X):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 'scikit-learn的OrdinalEncoder可以用来创建有序编码。让我们对图8.1中的数据集（用X表示）中的两个类别特征（颜色和尺寸）进行编码：  '
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We’ll specify our encoding for color assuming it can take four values: red,
    yellow, green, blue (even though we only see red, green, and blue in our data).
    We’ll also specify the ordering for size as XL, L, M, S:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将指定颜色的编码，假设它可以取四个值：红色、黄色、绿色、蓝色（尽管我们只看到红色、绿色和蓝色在我们的数据中）。我们还将指定尺寸的排序为XL、L、M、S：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Specifies that there are four possible colors
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指定有四种可能的颜色
- en: ❷ Specifies that size should be organized in decreasing order
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 指定尺寸应按递减顺序组织
- en: ❸ Encodes categorical features only using this specification
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 仅使用此规范对类别特征进行编码
- en: 'Now, we can look at the encodings for these features:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以查看这些特征的编码：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This encoding assigns numeric values to color as {''red'': 0, ''yellow'': 1,
    ''blue'': 2, ''green'': 3} and to size as {''XL'': 0, ''L'': 1, ''M'': 2, ''S'':
    3}. This encoding transforms these categorical features to numeric values:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '这种编码将数字值分配给颜色，如 {''red'': 0, ''yellow'': 1, ''blue'': 2, ''green'': 3}，并将尺寸分配为
    {''XL'': 0, ''L'': 1, ''M'': 2, ''S'': 3}。这种编码将这些分类特征转换为数值：'
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Compare the encoded color (the first column of Xenc) with the raw data (the
    first column of X). All the red entries are encoded as 0, green as 2, and blue
    as 3. As there are no yellow entries, we have no encodings of value 1 in this
    column.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 将编码的颜色（Xenc的第一列）与原始数据（X的第一列）进行比较。所有红色条目都被编码为0，绿色为2，蓝色为3。由于没有黄色条目，所以在这个列中没有值1的编码。
- en: Note that ordinal encoding imposes an inherent ordering between variables. While
    this is ideal for ordinal categorical features, it may not always make sense for
    nominal categorical features.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，顺序编码在变量之间施加了固有的顺序。虽然这对于顺序分类特征是理想的，但对于名义分类特征可能并不总是有道理。
- en: One-hot encoding
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码
- en: One-hot encoding is a way to encode a categorical feature without imposing any
    ordering among its values and is more suited for nominal features. Why use one-hot
    encoding? If we use ordinal encoding for nominal features, it would introduce
    an ordering that doesn’t exist between the categories in the real world, thus
    misleading the learning algorithm into thinking there was one. Unlike ordinal
    encoding, which encodes each category using a single number, one-hot encoding
    encodes each category using a vector of 0s and 1s. The size of the vector depends
    on the number of categories.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码是一种编码分类特征的方法，它不对其值之间的顺序施加任何限制，更适合用于名义特征。为什么使用独热编码？如果我们对名义特征使用顺序编码，它将引入一个在现实世界中类别之间不存在的顺序，从而误导学习算法认为存在一个。与顺序编码不同，顺序编码使用单个数字来编码每个类别，而独热编码使用0和1的向量来编码每个类别。向量的长度取决于类别的数量。
- en: 'For example, if we assume that color is a three-valued category (red, blue,
    green), it will be encoded as a length-3 vector. One such one-hot encoding can
    be {''red'': [1, 0, 0], ''blue'': [0, 1, 0], ''green'': [0, 0, 1]}. Observe the
    position of the 1s: red corresponds to the first encoding entry, blue corresponds
    to the second, and green to the third.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '例如，如果我们假设颜色是一个三值类别（红色、蓝色、绿色），它将被编码为长度为3的向量。一个这样的独热编码可以是 {''red'': [1, 0, 0],
    ''blue'': [0, 1, 0], ''green'': [0, 0, 1]}。观察1的位置：红色对应于第一个编码条目，蓝色对应于第二个，绿色对应于第三个。'
- en: If we assume that color is a four-valued category (red, yellow, blue, green),
    one-hot encoding will produce length-4 vectors for each category. For the rest
    of this chapter, we'll assume that color is a three-valued category.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们假设颜色是一个四值类别（红色、黄色、蓝色、绿色），独热编码将为每个类别生成长度为4的向量。在本章的其余部分，我们将假设颜色是一个三值类别。
- en: 'Because size takes four unique values, one-hot encoding produces length-4 vectors
    for each size category as well. One such one-hot encoding can be {''S'': [1, 0,
    0, 0], ''M'': [0, 1, 0, 0], ''L'': [0, 0, 1, 0], ''XL'': [0, 0, 0, 1]}.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '因为尺寸有四个独特的值，独热编码也为每个尺寸类别生成长度为4的向量。一个这样的独热编码可以是 {''S'': [1, 0, 0, 0], ''M'':
    [0, 1, 0, 0], ''L'': [0, 0, 1, 0], ''XL'': [0, 0, 0, 1]}。'
- en: 'scikit-learn’s OneHotEncoder can be used to create one-hot encodings. As before,
    let’s encode the two categorical features (color and size) in the data set from
    figure 8.1:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的OneHotEncoder可以用来创建独热编码。和之前一样，让我们将数据集（图8.1）中的两个分类特征（颜色和尺寸）进行编码：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Specifies that there are three possible colors
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指定有三种可能的颜色
- en: ❷ Specifies that there are four possible sizes
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 指定有四种可能的尺寸
- en: ❸ Encodes categorical features only using this specification
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 仅使用此规范对分类特征进行编码
- en: 'Now, we can look at the encodings for these features:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以查看这些特征的编码：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This encoding will introduce three one-hot features (first three columns in
    Xenc) to replace the color feature (first column in X) and four one-hot features
    (last four columns in Xenc) to replace the size feature (last column in X):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这种编码将引入三个独热特征（Xenc中的前三列）来替换颜色特征（X中的第一列）和四个独热特征（Xenc中的最后四列）来替换尺寸特征（X中的最后一列）：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Each individual category has its own column now (three for each color category
    and four for each size category), and any ordering between them has been lost.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单独的类别现在都有自己的列（每个颜色类别三个，每个尺寸类别四个），并且它们之间的任何顺序都已丢失。
- en: 'NOTE Since one-hot encoding removes any inherent ordering between categories,
    it’s an ideal choice to encode nominal features. This choice, however, comes with
    a cost: we often tend to blow up the size of our data set as we have to replace
    one category column with a large number of binary feature columns, one for each
    category.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：由于独热编码消除了类别之间的任何固有顺序，因此它是编码名义特征的理想选择。然而，这种选择也伴随着成本：我们往往倾向于扩大数据集的大小，因为我们不得不用一个包含大量二进制特征列的列来替换一个类别列，每个类别一个。
- en: Our original fashion data set was 8 examples × 2 features. With ordinal encoding,
    it remained 8 × 2, though a forced ordering was imposed on the nominal feature,
    that is, color. With one-hot encoding, the size became 8 × 7, and the inherent
    ordering in the ordinal feature, size, was removed.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们原始的时尚数据集是8个示例×2个特征。使用有序编码，它仍然是8×2，尽管对名义特征，即颜色，施加了一个强制排序。使用独热编码，尺寸变为8×7，有序特征尺寸的固有顺序被移除。
- en: 8.1.3 Encoding with target statistics
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.3 使用目标统计量进行编码
- en: We now shift our focus to *encoding with target statistics*, or *target encoding*,
    which is an example of a supervised encoding technique. In contrast to unsupervised
    encoding methods, supervised encoding methods use labels to encode categorical
    features.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将重点转向*使用目标统计量进行编码*，或称为*目标编码*，这是一种监督编码技术的例子。与无监督编码方法相比，监督编码方法使用标签来编码分类特征。
- en: 'The idea behind encoding with target statistics is fairly straightforward:
    for each category, we compute a statistic such as the mean over the targets (i.e.,
    labels) and replace the category with this newly computed numerical statistic.
    Encoding with label information often helps overcome the drawbacks of unsupervised
    encoding methods.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用目标统计量进行编码背后的思想相当简单：对于每个类别，我们计算一个统计量，如目标（即标签）的平均值，并用这个新计算的数值统计量替换类别。使用标签信息进行编码通常有助于克服无监督编码方法的缺点。
- en: Unlike one-hot encoding, target encoding doesn’t create any additional columns,
    meaning the dimensionality of the overall data set remains the same after encoding.
    Unlike ordinal encoding, target encoding doesn’t introduce spurious relationships
    between the categories.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与独热编码不同，目标编码不会创建任何额外的列，这意味着编码后的整体数据集的维度保持不变。与有序编码不同，目标编码不会在类别之间引入虚假的关系。
- en: Greedy target encoding
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 贪婪目标编码
- en: In the original fashion data set from the previous section, recall that each
    training example is a T-shirt with two attributes—color and size—and the target
    to predict is cost. Let’s say that we want to encode the color feature with target
    statistics. This feature has three categories—red, blue, and green—that need to
    be encoded.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节原始的时尚数据集中，回想一下，每个训练示例是一件T恤，有两个属性——颜色和尺寸——以及要预测的目标是成本。假设我们想要用目标统计量来编码颜色特征。这个特征有三个类别——红色、蓝色和绿色——需要编码。
- en: Figure 8.2 illustrates how encoding with target statistics works for the category
    red.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2说明了使用目标统计量进行编码对红色类别的工作方式。
- en: '![CH08_F02_Kunapuli](../Images/CH08_F02_Kunapuli.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F02_Kunapuli](../Images/CH08_F02_Kunapuli.png)'
- en: Figure 8.2 The category red of the feature color is replaced by its target statistic,
    the average (mean) of all the target values (cost) corresponding to the examples
    whose color is red. This is called greedy target encoding as all the training
    labels have been used for encoding.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 特征颜色的类别红色被其目标统计量替换，即所有目标值（成本）的平均值（均值），对应于颜色为红色的示例。这被称为贪婪目标编码，因为所有训练标签都已用于编码。
- en: 'There are three T-shirts, *x*[1], *x*[5], and *x*[8], whose color is red. Their
    corresponding target values (cost) are 8.99, 9.99, and 25.00\. The target statistic
    is computed as the mean of these values: (8.99 + 9.99 + 25.00) / 3 = 14.66\. Thus,
    each instance of red is replaced by its corresponding target statistic: 14.66\.
    The other two categories, blue and green, can similarly be encoded with their
    corresponding target statistics, 16.82 and 13.99.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 有三件T恤，*x*[1]，*x*[5]，和*x*[8]，它们的颜色是红色。它们对应的目标值（成本）分别是8.99，9.99和25.00。目标统计量是这些值的平均值：(8.99
    + 9.99 + 25.00) / 3 = 14.66。因此，每个红色的实例都被其对应的目标统计量14.66所替换。其他两个类别，蓝色和绿色，可以类似地用它们对应的目标统计量16.82和13.99进行编码。
- en: 'More formally, the target statistic for the *k*th category of the *j*th feature
    can be computed using the following formula:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，第*j*个特征的第*k*个类别的目标统计量可以使用以下公式计算：
- en: '![CH08_F02_Kunapuli-eqs-0x](../Images/CH08_F02_Kunapuli-eqs-0x.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F02_Kunapuli-eqs-0x](../Images/CH08_F02_Kunapuli-eqs-0x.png)'
- en: Here, the notation I(*x*^(*j*)[*i*] = *k*) denotes an indicator function, which
    returns 1 if the condition within the parentheses is true and 0 if false. For
    example, in our fashion data set, I(*x*[1]^(color) = red) because the first example
    corresponds to a medium red T-shirt, whereas I(*x*[4]^(color) = red) because the
    fourth example corresponds to an XL blue T-shirt.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，符号I(x^(j[i]) = k)表示一个指示函数，当括号内的条件为真时返回1，为假时返回0。例如，在我们的时尚数据集中，I(x^[1]^(color)
    = red)因为第一个示例对应于中红色T恤，而I(x^[4]^(color) = red)因为第四个示例对应于XL蓝色T恤。
- en: This formula for computing target statistics actually computes a *smoothed average*
    rather than just the average. Smoothing is performed by adding a parameter *a*
    > 0 to the denominator. This is to ensure that categories with a small number
    of values (and hence small denominators) don’t end up with target statistics that
    are scaled differently to other categories. The constant *p* in the numerator
    is typically the average target value of the entire data set, and it serves as
    a *prior*, or as a means of regularizing the target statistic.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算目标统计的公式实际上计算的是一个*平滑平均值*而不是简单的平均值。通过向分母添加一个参数*a* > 0来执行平滑。这是为了确保具有少量值（因此分母较小）的类别不会得到与其它类别不同缩放的目标统计。分子中的常数*p*通常是整个数据集的平均目标值，它作为*先验*，或者作为正则化目标统计的手段。
- en: 'Generally, a prior is any additional knowledge we have that we can pass on
    to a learning algorithm to improve its training. For example, in Bayesian learning,
    a prior probability distribution is often specified to express our belief in how
    the data set is distributed. In this case, the prior specifies how the encoding
    should be applied on classes that occur very infrequently: simply replace with
    a value close to *p*.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，先验是我们拥有的任何额外知识，我们可以将其传递给学习算法以改进其训练。例如，在贝叶斯学习中，通常指定一个先验概率分布来表达我们对数据集分布的信念。在这种情况下，先验指定了如何对非常罕见出现的类别应用编码：简单地用一个接近*p*的值替换。
- en: This target encoding approach is called *greedy target encoding*, as it uses
    all the available training data to compute the encodings. As we’ll see, a greedy
    encoding approach leaks information from the training to the test set. This “leakage”
    is problematic because a model identified as high performing during training and
    testing will often actually perform poorly in deployment and production.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这种目标编码方法被称为*贪婪目标编码*，因为它使用所有可用的训练数据来计算编码。正如我们将看到的，贪婪编码方法会从训练数据泄露信息到测试集。这种“泄露”是有问题的，因为在一个训练和测试过程中被识别为高性能的模型，在实际部署和生产中往往表现不佳。
- en: Information leakage and distribution shift
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 信息泄露和分布偏移
- en: 'Many preprocessing approaches are affected by one or both of two common practical
    problems: *training-to-test-set information leakage* and training-to-test-set
    *distribution shift*. Both problems affect our ability to evaluate our trained
    model and accurately estimate how it will behave on future, unseen data, that
    is, how it will generalize.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 许多预处理方法都受到两个常见实际问题的其中一个或两个的影响：*从训练到测试集的信息泄露*和*从训练到测试集的分布偏移*。这两个问题都会影响我们评估训练模型的能力，以及准确估计它在未来未见数据上的行为，即它如何泛化的能力。
- en: A key step in machine-learning model development is the creation of a hold-out
    test set, which is used to evaluate trained models. The test set must be completely
    held out from every stage of modeling (including preprocessing, training, and
    validation) and used purely for evaluating model performance to simulate model
    performance on unseen data. To do this effectively, we have to ensure that no
    part of the training data makes its way into test data. When this happens during
    modeling, it’s called *information leakage from the training-to-test set*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型开发的关键步骤之一是创建一个保留测试集，该集用于评估训练好的模型。测试集必须完全从建模的每个阶段（包括预处理、训练和验证）中分离出来，并且仅用于评估模型性能，以模拟模型在未见数据上的性能。为了有效地做到这一点，我们必须确保训练数据中的任何部分都不会进入测试数据。当这种情况在建模过程中发生时，被称为“从训练到测试集的信息泄露”。
- en: Data leakage occurs when information about features leaks into the test set,
    while target leakage occurs when information about targets (labels) leaks into
    the test set. Greedy target encoding leads to target leakage, as illustrated in
    figure 8.3\. In this example, a data set of 12 data points is partitioned into
    training and test sets. The training set is used to perform greedy target encoding
    of the category red of the feature color. More specifically, the target encoding
    from the training set is used to transform *both* the training *and* the test
    set. This leads to information leakage about the targets from the training set
    to the test set, making this an instance of target leakage.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当特征信息泄露到测试集时，发生数据泄露；而当目标（标签）信息泄露到测试集时，发生目标泄露。贪婪目标编码导致目标泄露，如图8.3所示。在这个例子中，一个包含12个数据点的数据集被划分为训练集和测试集。训练集被用来对特征颜色中的类别红色进行贪婪目标编码。更具体地说，从训练集的目标编码被用来转换*训练集和测试集*。这导致从训练集到测试集的目标信息泄露，使得这是一个目标泄露的实例。
- en: '![CH08_F03_Kunapuli](../Images/CH08_F03_Kunapuli.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F03_Kunapuli](../Images/CH08_F03_Kunapuli.png)'
- en: Figure 8.3 Target leakage from the training to test set illustrated. All the
    targets (labels) in the training set are greedily used to create an encoding for
    red, which is used to encode this category in both the training and test sets,
    leading to target leakage.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3展示了从训练集到测试集的目标泄露。训练集中的所有目标（标签）都被贪婪地用来为红色创建编码，这个编码被用来在训练集和测试集中编码这个类别，导致目标泄露。
- en: Another consideration to keep in mind for the train-test split is ensuring that
    the training and hold-out test sets have similar distributions; that is, they
    have similar statistical properties. This is often achieved by randomly sampling
    the held-out test set from the overall set.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练-测试集划分的另一个考虑因素是确保训练集和保留的测试集具有相似的分布；也就是说，它们具有相似的统计特性。这通常是通过从整体集中随机采样保留的测试集来实现的。
- en: However, preprocessing techniques such as greedy target encoding can introduce
    disparities between the training and test sets, leading to a *prediction shift*
    between the training and test sets, as illustrated in figure 8.4\. As before,
    the category red for the feature color is encoded using greedy target statistics.
    This encoding is computed as the mean of the targets corresponding to examples
    with color = red in the training data and is 14.66.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，预处理技术如贪婪目标编码可能会在训练集和测试集之间引入差异，导致训练集和测试集之间的*预测偏移*，如图8.4所示。与之前一样，特征颜色中的类别红色使用贪婪目标统计进行编码。这种编码是通过计算训练数据中颜色等于红色的示例对应的目标的平均值来计算的，其值为14.66。
- en: However, if we compute the mean of the targets corresponding to color = red
    in the test data only, the mean is 10.47\. This discrepancy between the training
    and test sets is a by-product of greedy target encoding, which causes the test
    set distributions to become shifted from the training set distribution. Put another
    way, the statistical properties of the test set are now no longer similar to that
    of the training set, which has an inevitable and cascading influence on our model
    evaluation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们只计算测试数据中颜色等于红色对应的目标的平均值，平均值为10.47。这种训练集和测试集之间的差异是贪婪目标编码的副产品，它导致测试集的分布相对于训练集分布发生偏移。换句话说，测试集的统计特性现在不再与训练集相似，这对我们的模型评估产生了不可避免的连锁影响。
- en: Both target leakage and prediction shift introduce a statistical bias into the
    performance metrics we use to evaluate the generalization performance of our trained
    models. Often, they overestimate generalization performance and make the trained
    model look better than it actually is, which causes a problem when this model
    is deployed and fails to perform according to expectations.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 目标泄露和预测偏移都会将统计偏差引入我们用来评估训练模型泛化性能的性能指标中。通常，它们高估了泛化性能，使得训练模型看起来比实际情况更好，当这种模型部署并无法按预期执行时，这会引发问题。
- en: '![CH08_F04_Kunapuli](../Images/CH08_F04_Kunapuli.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F04_Kunapuli](../Images/CH08_F04_Kunapuli.png)'
- en: Figure 8.4 Distribution shift between the training and test sets illustrated.
    Since the target encoding for the test set is computed using the training set,
    it can lead to a shift in the distribution and statistical properties of the test
    set (yellow) compared to the training set (red).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4展示了训练集和测试集之间的分布偏移。由于测试集的目标编码是使用训练集计算的，这可能导致测试集（黄色）相对于训练集（红色）的分布和统计特性发生偏移。
- en: Hold-out and leave-one-out target encoding
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 保留和留一法目标编码
- en: The best (and simplest) way to eliminate both target leakage and prediction
    shift is to hold out a part of the training data for encoding. Thus, in addition
    to the training and hold-out test sets, we also need to create a hold-out encoding
    set!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 消除目标泄露和预测偏差的最好（也是最简单）的方法是保留一部分训练数据用于编码。因此，除了训练集和保留测试集之外，我们还需要创建一个保留编码集！
- en: This approach, called *hold-out target encoding*, is illustrated in figure 8.5\.
    Here, our data set from figure 8.3 and figure 8.4 is split into three sets—a training
    set, a hold-out encoding set, and a hold-out test set—each with four data points.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法称为*保留目标编码*，如图8.5所示。在这里，我们的数据集来自图8.3和图8.4，被分为三个集合——一个训练集、一个保留编码集和一个保留测试集——每个集合包含四个数据点。
- en: The hold-out encoding set is used to compute the target encoding for both the
    training and test sets. This ensures the independence of training and test sets
    and eliminates target leakage. Further, because the same target statistic is used
    for both training and test sets, it also avoids prediction shift.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 保留编码集用于计算训练集和测试集的目标编码。这确保了训练集和测试集的独立性，消除了目标泄露。此外，因为训练集和测试集使用相同的目标统计，它也避免了预测偏差。
- en: A key drawback of hold-out target encoding is its data inefficiency. To avoid
    leakage, once the hold-out encoding set is used to compute the encoding, it needs
    to be discarded, which means that a good chunk of the total data available for
    modeling can potentially be wasted.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 保留目标编码的一个主要缺点是其数据效率低下。为了避免泄露，一旦使用保留编码集来计算编码，就需要将其丢弃，这意味着可用于建模的大量数据可能会被浪费。
- en: '![CH08_F05_Kunapuli](../Images/CH08_F05_Kunapuli.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F05_Kunapuli](../Images/CH08_F05_Kunapuli.png)'
- en: 'Figure 8.5 Hold-out encoding partitions the available data into three sets:
    training and test, as usual, and a third hold-out test set to be used exclusively
    for encoding with target statistics. This avoids both target leakage and distribution
    shift.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5中，保留编码将可用数据分为三个集合：训练集和测试集，如常规操作，以及第三个仅用于使用目标统计进行编码的保留测试集。这避免了目标泄露和分布偏移。
- en: One (imperfect) alternative to avoid data inefficiency is to use *leave-one-out*
    (LOO) target encoding, which is illustrated in figure 8.6\. LOO encoding works
    similarly to LOO cross validation (LOO CV), except that the left-out example is
    being encoded rather than being validated.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 避免数据效率低下的一个（不完美）的替代方案是使用*留一法*（LOO）目标编码，如图8.6所示。LOO编码的工作原理与LOO交叉验证（LOO CV）类似，不同之处在于被排除的示例是用于编码而不是验证。
- en: In figure 8.6, we see that to perform LOO target encoding for the red example
    x[5], we compute the target statistic using the other two red training examples
    x[1] and x[8], while leaving out x[5]. This procedure is repeated for the other
    two red training examples, x[1] and x8, in turn. Unfortunately, LOO encoding cannot
    include examples in the test set as we want to avoid leakage. Thus, we can apply
    greedy target encoding as before for the test set.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在图8.6中，我们看到为了对红色示例x[5]执行LOO目标编码，我们使用其他两个红色训练示例x[1]和x[8]来计算目标统计，同时排除x[5]。然后，依次对其他两个红色训练示例x[1]和x[8]重复此过程。不幸的是，LOO编码不能包括测试集中的示例，因为我们想避免泄露。因此，我们可以像之前一样，对测试集应用贪婪目标编码。
- en: '![CH08_F06_Kunapuli](../Images/CH08_F06_Kunapuli.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F06_Kunapuli](../Images/CH08_F06_Kunapuli.png)'
- en: Figure 8.6 LOO target encoding is applied to the training data to avoid creating
    a wasteful hold-out encoding set. Instead of holding out a subset of the data,
    only the example being encoded is held out. Test data is encoded using greedy
    target encoding as before.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6中，LOO目标编码应用于训练数据，以避免创建一个浪费的保留编码集。而不是保留数据的一个子集，只有正在编码的示例被保留。测试数据使用之前的方法，即贪婪目标编码进行编码。
- en: As we can see, the LOO target encoding procedure aims to emulate hold-out target
    encoding, while being significantly more data efficient. However, it should be
    noted that this overall procedure doesn’t fully eliminate target leakage and prediction
    shift problems.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，LOO目标编码过程旨在模拟保留目标编码，同时显著提高数据效率。然而，应该注意的是，这个整体过程并没有完全消除目标泄露和预测偏差问题。
- en: As we’ll see in section 8.2, another encoding strategy called *ordered target
    statistics* aims to further mitigate the problems of target leakage and prediction
    shift while ensuring both data and computational efficiency.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第8.2节中将要看到的，另一种编码策略称为*有序目标统计*，旨在进一步缓解目标泄露和预测偏差的问题，同时确保数据和计算效率。
- en: 8.1.4 The category_encoders package
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.4 category_encoders包
- en: This section provides examples of how to put together end-to-end encoding and
    training pipelines for data sets with categorical features. The subpackage sklearn
    .preprocessing provides some common encoders such as OneHotEncoder and OrdinalEncoder.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了如何为具有分类特征的数据集组合端到端编码和训练管道的示例。子包sklearn .preprocessing提供了一些常见的编码器，如OneHotEncoder和OrdinalEncoder。
- en: However, we’ll use the category_encoders ([http://mng.bz/41aQ](http://mng.bz/41aQ))
    package, which provides many more encoding strategies, including for greedy and
    LOO target encoding. category_encoders is scikit-learn compatible, which means
    that it can be used with other ensemble method implementations that provide sklearn-compatible
    interfaces (e.g., LightGBM and XGBoost) discussed in this book.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们将使用category_encoders ([http://mng.bz/41aQ](http://mng.bz/41aQ))包，它提供了许多更多的编码策略，包括贪婪和LOO目标编码。category_encoders与scikit-learn兼容，这意味着它可以与其他提供sklearn兼容接口的集成方法实现一起使用（例如，本书中讨论的LightGBM和XGBoost）。
- en: We’ll use the Australian Credit Approval data set from the UCI Machine Learning
    Repository ([http://mng.bz/Q8D4](http://mng.bz/Q8D4)). A clean version of this
    data set is available along with the source code for this book, and we’ll use
    this version to demonstrate category encoding in practice. The data set contains
    six continuous features, four binary features, and four categorical features,
    and the task is to determine whether to approve or deny a credit card application,
    that is, binary classification.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自UCI机器学习仓库的澳大利亚信用批准数据集 ([http://mng.bz/Q8D4](http://mng.bz/Q8D4))。此数据集的干净版本以及本书的源代码都可用，我们将使用此版本来演示实际中的分类编码。该数据集包含六个连续特征、四个二进制特征和四个分类特征，任务是确定是否批准或拒绝信用卡申请，即二元分类。
- en: 'First, let’s load the data set and look at the feature names and the first
    few rows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们加载数据集并查看特征名称和前几行：
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This code snippet prints the first few rows of the data set in tabular form,
    shown in figure 8.7.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码片段以表格形式打印数据集的前几行，如图8.7所示。
- en: '![CH08_F07_Kunapuli](../Images/CH08_F07_Kunapuli.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F07_Kunapuli](../Images/CH08_F07_Kunapuli.png)'
- en: Figure 8.7 The Australian Credit Approval data set from the UCI Machine Learning
    repository. Attribute names have been changed to protect confidentiality of the
    individuals represented in the data set.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 来自UCI机器学习仓库的澳大利亚信用批准数据集。属性名称已被更改，以保护数据集中代表个人的隐私。
- en: The feature names are of the form f1-bin, f2-cont, or f5-cat, indicating the
    column index and whether the feature is binary, continuous, or categorical. To
    protect applicant confidentiality, the category strings and names have been replaced
    with integer values; that is, the categorical features have already been processed
    with ordinal encoding!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 特征名称的形式为f1-bin、f2-cont或f5-cat，表示列索引以及特征是否为二进制、连续或分类。为了保护申请人的隐私，类别字符串和名称已被替换为整数值；也就是说，分类特征已经使用序数编码处理过！
- en: 'Let’s separate the columns into features and labels, and then further split
    into the training and test sets as usual:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将列分为特征和标签，然后像往常一样进一步分割为训练集和测试集：
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Furthermore, let’s explicitly identify the categorical and continuous features
    we’re interested in for preprocessing:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，让我们明确识别我们感兴趣的预处理中的分类和连续特征：
- en: '[PRE9]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We’ll preprocess the continuous and categorical features in different ways.
    The continuous features will be standardized; that is, each column of continuous
    features is rescaled to have zero mean and unit standard deviation. This rescaling
    ensures that different columns don’t have drastically different scales, which
    can mess up downstream learning algorithms.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以不同的方式预处理连续和分类特征。连续特征将被标准化；也就是说，连续特征的每一列将被重新缩放，以具有零均值和单位标准差。这种缩放确保不同的列不会具有截然不同的尺度，这可能会破坏下游学习算法。
- en: 'The categorical features will be preprocessed using one-hot encoding. For this,
    we’ll use the OneHotEncoder from the category_encoders package. We’ll create two
    separate preprocessing pipelines, one for continuous features and one for categorical
    features:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 分类特征将使用独热编码进行预处理。为此，我们将使用来自category_encoders包的OneHotEncoder。我们将创建两个独立的预处理管道，一个用于连续特征，另一个用于分类特征：
- en: '[PRE10]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that ce.OneHotEncoder requires us to explicitly specify the columns corresponding
    to the categorical features, without which it will apply encoding to *all* the
    columns.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，ce.OneHotEncoder需要我们明确指定对应于分类特征的列，否则它将对*所有*列应用编码。
- en: 'Now that we have two separate pipelines, we need to put these together to ensure
    that the correct preprocessing is applied to the correct feature type. We can
    do this with scikit-learn’s ColumnTransformer, which allows us to apply different
    steps to different columns:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了两个独立的管道，我们需要将这些管道组合起来，以确保正确的预处理应用于正确的特征类型。我们可以使用 scikit-learn 的 ColumnTransformer
    来实现这一点，它允许我们将不同的步骤应用于不同的列：
- en: '[PRE11]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Preprocesses continuous features here
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在这里预处理连续特征
- en: ❷ Preprocesses categorical features here
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在这里预处理分类特征
- en: ❸ Keeps the remaining features as is
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 保持剩余特征不变
- en: 'Now, we can fit a preprocessor on the training set and apply the transformation
    to both the training and test sets:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在训练集上拟合一个预处理程序，并将转换应用于训练集和测试集：
- en: '[PRE12]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Observe how the test set isn’t used to fit the preprocessor pipeline. This
    is a subtle but important practical step to ensure that the test set is held out
    and that there is no inadvertent data or target leakage due to preprocessing.
    Now, let’s see what one-hot encoding has done to our feature set size:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到测试集没有被用于拟合预处理管道。这是一个微妙但重要的实际步骤，以确保测试集被保留，并且由于预处理而没有意外数据或目标泄漏。现在，让我们看看独热编码对我们特征集大小做了什么：
- en: '[PRE13]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Since one-hot encoding introduces one new column for each category of a categorical
    feature, the overall number of columns has increased from 14 to 38! Now let’s
    train and evaluate a RandomForestClassifier on this preprocessed data set:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 由于独热编码为每个分类特征的每个类别引入了一个新列，因此列的总数从 14 增加到 38！现在让我们在预处理后的数据集上训练和评估 RandomForestClassifier：
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Our one-hot encoding strategy learned a model whose hold-out test accuracy
    is 89.9%. In addition to OneHotEncoder and OrdinalEncoder, the category_encoders
    package also provides many other encoders. Two of encoders of interest to us are
    the greedy TargetEncoder and the LeaveOneOutEncoder, which can be used in exactly
    the same way as OneHotEncoder. Specifically, we simply replace OneHotEncoder with
    TargetEncoder in the following:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的独热编码策略学习了一个模型，其保留测试准确率为 89.9%。除了 OneHotEncoder 和 OrdinalEncoder 之外，category_encoders
    包还提供了许多其他编码器。对我们感兴趣的编码器有两个：贪婪目标编码器（TargetEncoder）和 LeaveOneOutEncoder，它们可以像 OneHotEncoder
    一样使用。具体来说，我们只需在以下代码中将 OneHotEncoder 替换为 TargetEncoder：
- en: '[PRE15]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'TargetEncoder takes one additional parameter, smoothing, a positive value that
    combines the effect of smoothing and the effect of applying the prior (see section
    8.1.2). Higher values force higher smoothing and can counter overfitting. After
    preprocessing and training, we have the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: TargetEncoder 取一个额外的参数，即平滑度，这是一个正值，它结合了平滑和应用先验的效果（参见第 8.1.2 节）。更高的值会强制进行更高的平滑，并可以对抗过拟合。预处理和训练后，我们有以下结果：
- en: '[PRE16]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Unlike one-hot encoding, greedy target encoding doesn’t add any new columns,
    which means that the overall dimensions of the data set remain unchanged. We can
    use LeaveOneOutEncoder in a similar way:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 与独热编码不同，贪婪目标编码不会添加任何新列，这意味着数据集的整体维度保持不变。我们可以以类似的方式使用 LeaveOneOutEncoder：
- en: '[PRE17]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The sigma parameter is a noise parameter that aims to decrease overfitting.
    The user manual recommends using values between 0.05 to 0.6\. After preprocessing
    and training, we again have the following:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: sigma 参数是一个噪声参数，旨在减少过拟合。用户手册建议使用 0.05 到 0.6 之间的值。预处理和训练后，我们再次得到以下结果：
- en: '[PRE18]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As with TargetEncoder, the number of features remains unchanged due to preprocessing.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 与 TargetEncoder 一样，由于预处理，特征数量保持不变。
- en: '8.2 CatBoost: A framework for ordered boosting'
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 CatBoost：有序提升框架
- en: '*CatBoost* is another open source gradient-boosting framework developed by
    Yandex. CatBoost introduces three major modifications to the classical Newton-boosting
    approach:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*CatBoost* 是由 Yandex 开发的另一个开源梯度提升框架。CatBoost 对经典的牛顿提升方法进行了三项主要改进：'
- en: It’s specialized to categorical features, unlike other boosting approaches that
    are more general.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它专门针对分类特征，与其他更通用的提升方法不同。
- en: It uses ordered boosting as its underlying ensemble learning approach, which
    allows it to address target leakage and prediction shift implicitly during training.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用有序提升作为其底层集成学习方法，这使得它在训练过程中可以隐式地解决目标泄漏和预测偏移问题。
- en: It uses oblivious decision trees as base estimators, which often leads to faster
    training times.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用无知的决策树作为基估计器，这通常会导致更快的训练时间。
- en: NOTE CatBoost is available for Python on many platforms. See the CatBoost installation
    guide for detailed instructions on installation at [http://mng.bz/X5xE](http://mng.bz/X5xE).
    At the time of this writing, CatBoost is only supported by the 64-bit version
    of Python.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：CatBoost在许多平台上都可用，适用于Python。有关安装的详细说明，请参阅CatBoost安装指南，网址为[http://mng.bz/X5xE](http://mng.bz/X5xE)。在撰写本文时，CatBoost仅支持64位版本的Python。
- en: 8.2.1 Ordered target statistics and ordered boosting
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 有序目标统计量和有序提升
- en: 'CatBoost handles categorical features in two ways: (1) by encoding categorical
    features as described previously with target statistics, and (2) by cleverly creating
    categorical combinations of features (and encoding them with target statistics
    as well). While these modifications enable CatBoost to seamlessly handle categorical
    features, they do introduce some downsides that must be addressed.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: CatBoost以两种方式处理分类特征：(1) 通过使用目标统计量将分类特征编码，如之前所述，以及(2) 通过巧妙地创建特征的分类组合（并将它们也用目标统计量进行编码）。虽然这些修改使CatBoost能够无缝处理分类特征，但它们确实引入了一些必须解决的问题。
- en: As we’ve seen before, encoding with target statistics introduces target leakage
    and, more importantly, a prediction shift in the test set. The most ideal way
    to handle this is by creating a hold-out encoding set.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，使用目标统计量进行编码会引入目标泄漏，更重要的是，在测试集中会产生预测偏移。处理这个问题最理想的方法是创建一个保留编码集。
- en: Holding out training examples for just encoding and nothing else is rather wasteful
    of data, meaning that this approach is rarely used in practice. The alternative,
    LOO encoding, is more data efficient, but doesn’t completely mitigate prediction
    shift.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 仅为了编码而保留训练示例，而不做其他任何事情，这相当浪费数据，这意味着这种方法在实践中很少使用。另一种方法，LOO编码，更节省数据，但并不能完全缓解预测偏移。
- en: In addition to problems with encoding features, gradient boosting and Newton
    boosting both reuse data between iterations, leading to a gradient distribution
    shift, which ultimately causes a further prediction shift. In other words, even
    if we didn’t have categorical features, we would still have a prediction shift
    problem, which would bias our estimates of model generalization!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 除了编码特征的问题外，梯度提升和牛顿提升都在迭代之间重用数据，导致梯度分布偏移，这最终会导致进一步的预测偏移。换句话说，即使我们没有分类特征，我们仍然会有预测偏移问题，这会偏误我们对模型泛化的估计！
- en: CatBoost addresses this central problem of prediction shift by using permutation
    for ordering training examples to (1) compute target statistics for encoding categorical
    variables (called ordered target statistics), and (2) train its weak estimators
    (called ordered boosting).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: CatBoost通过使用排列对训练示例进行排序来解决预测偏移这一核心问题，以(1)计算编码分类变量（称为有序目标统计量）的目标统计量，以及(2)训练其弱估计器（称为有序提升）。
- en: Ordered target statistics
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 有序目标统计量
- en: 'At its heart, the ordering principle is simple and elegant and consists of
    two steps:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本质上，排序原则简单而优雅，包括两个步骤：
- en: Reorder the training examples according to a random permutation.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据随机排列重新排序训练示例。
- en: To compute target statistics for the *i*th training example, use the previous
    *i* - 1 training examples according to this random permutation.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了计算第*i*个训练示例的目标统计量，根据这个随机排列使用前*i* - 1个训练示例。
- en: 'This is illustrated in figure 8.8 for eight training examples. First, the examples
    are permuted into a random ordering: 4, 7, 1, 8, 2, 6, 5, 3\. Now, to compute
    target statistics for each training example, we assume that these examples arrive
    sequentially.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这在图8.8中用八个训练示例进行了说明。首先，示例被随机排列成随机顺序：4、7、1、8、2、6、5、3。现在，为了计算每个训练示例的目标统计量，我们假设这些示例是按顺序到达的。
- en: 'For example, to compute the target statistics for example 2, we can only use
    examples in the sequence that we’ve “previously seen”: 4, 7, 1, and 8\. Then,
    to compute the target statistics for example 6, we can only use examples in the
    sequence that we’ve previously seen, now: 4, 7, 1, 8, *and* 2, and so on.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了计算示例2的目标统计量，我们只能使用我们之前“看到”的序列中的示例：4、7、1和8。然后，为了计算示例6的目标统计量，我们只能使用我们之前已经看到的序列中的示例，现在：4、7、1、8、*以及*
    2，依此类推。
- en: '![CH08_F08_Kunapuli](../Images/CH08_F08_Kunapuli.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F08_Kunapuli](../Images/CH08_F08_Kunapuli.png)'
- en: Figure 8.8 Ordered target statistics first permutes the examples into a random
    sequence, using only the previous examples in the ordered sequence to compute
    the target statistics.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8展示了有序目标统计量首先将示例随机排列成随机序列，仅使用有序序列中的先前示例来计算目标统计量。
- en: Thus, to compute the encoding for the *i*th training example, ordered target
    statistics never uses its own target value; this behavior is similar to LOO target
    encoding. The key difference between the two is that ordered target statistics
    uses the notion of a “history” of examples it has already seen.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了计算第*i*个训练示例的编码，有序目标统计从不使用其自己的目标值；这种行为类似于LOO目标编码。两者之间的关键区别是有序目标统计使用它已经看到的示例的“历史”概念。
- en: One downside to this approach is that training examples that occur early in
    a randomized sequence are encoded with far fewer examples. To compensate for this
    in practice and increase robustness, CatBoost maintains several sequences (i.e.,
    histories), which are, in turn, randomly chosen. This means that CatBoost recomputes
    target statistics for categorical variables at each iteration.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点之一是，在随机序列中较早出现的训练示例编码的示例数量要少得多。为了在实践中补偿这一点并增加鲁棒性，CatBoost维护几个序列（即历史记录），这些序列反过来又是随机选择的。这意味着CatBoost在每个迭代中重新计算分类变量的目标统计。
- en: Ordered boosting
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 有序提升
- en: CatBoost is fundamentally a Newton-boosting algorithm (see chapter 6); that
    is, it uses both the first and second derivative of the loss function to train
    its constituent weak estimators.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: CatBoost本质上是一种牛顿提升算法（参见第6章）；也就是说，它使用损失函数的一阶和二阶导数来训练其组成部分的弱估计器。
- en: 'As mentioned previously, there are two sources of prediction shift: variable
    encoding and gradient computations themselves. To avoid prediction shift due to
    gradients, CatBoost extends the idea of ordering to training its weak learners.
    Another way to think about this is as Newton boosting + ordering = CatBoost.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，预测偏移有两个来源：变量编码和梯度计算本身。为了避免由于梯度引起的预测偏移，CatBoost将排序的想法扩展到训练其弱学习器。另一种思考方式是牛顿提升+排序=CatBoost。
- en: 'Figure 8.9 illustrates ordered boosting, analogous to ordered target statistics.
    For example, to compute the residuals and gradients for example 2, ordered boosting
    uses a model only trained on the examples in the sequence that it has previously
    seen: 4, 7, 1, and 8\. As with ordered target statistics, CatBoost uses multiple
    permutations to increase robustness. These residuals are now used to train its
    weak estimators.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9说明了有序提升，类似于有序目标统计。例如，为了计算示例2的残差和梯度，有序提升仅使用它之前看到的序列中的示例来训练模型：4, 7, 1, 和 8。与有序目标统计一样，CatBoost使用多个排列来增加鲁棒性。这些残差现在用于训练其弱估计器。
- en: '![CH08_F09_Kunapuli](../Images/CH08_F09_Kunapuli.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F09_Kunapuli](../Images/CH08_F09_Kunapuli.png)'
- en: Figure 8.9 Ordered boosting also permutes the examples into a random sequence
    and uses only the previous examples in the ordered sequence to compute the gradients
    (residuals). Shown here is how the residuals are computed at iteration 4 (using
    estimator M4 for example *x*[2]), at iteration 5 (using estimator M5 for example
    *x*[6]), and so on.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 有序提升也随机排列示例，并仅使用有序序列中的先前示例来计算梯度（残差）。这里展示了在第4次迭代（使用估计器M4示例*x*[2]）时如何计算残差，在第5次迭代（使用估计器M5示例*x*[6]）时如何计算，依此类推。
- en: 8.2.2 Oblivious decision trees
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 无意识决策树
- en: Another key difference between Newton-boosting implementations such as XGBoost
    and CatBoost are the base estimators. XGBoost uses standard decision trees as
    weak estimators, while CatBoost uses *oblivious decision trees*.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 与XGBoost和CatBoost等牛顿提升实现之间的另一个关键区别是基估计器。XGBoost使用标准决策树作为弱估计器，而CatBoost使用*无意识决策树*。
- en: Oblivious decision trees use the same splitting criterion in all the nodes across
    an entire level (depth) of the tree. This is illustrated in figure 8.10, which
    compares a standard decision tree with four leaf nodes with an oblivious decision
    tree with four leaf nodes.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 无意识决策树在整个树的某个级别（深度）的所有节点中使用相同的分割标准。这如图8.10所示，比较了具有四个叶子节点的标准决策树和无意识决策树。
- en: '![CH08_F10_Kunapuli](../Images/CH08_F10_Kunapuli.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F10_Kunapuli](../Images/CH08_F10_Kunapuli.png)'
- en: 'Figure 8.10 Comparing standard and oblivious decision trees, each with four
    leaf nodes. Observe that the decision nodes at depth 2 of the oblivious decision
    tree are both the same (size < 15). This is a key feature of oblivious decision
    trees: only one split criterion is learned for each depth.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 比较标准决策树和无意识决策树，每个都有四个叶子节点。观察发现，无意识决策树深度2的决策节点都是相同的（大小<15）。这是无意识决策树的关键特性：每个深度只学习一个分割标准。
- en: In this example, observe that the second level of the oblivious tree (right)
    uses the same decision criterion, size < 15, at each node in the second level.
    While this is a simple example, note already that we only need to learn two split
    criteria for the oblivious tree, as opposed to the standard decision tree. This
    makes oblivious trees easier and more efficient to train, which has the effect
    of speeding up overall training. In addition, oblivious trees are balanced and
    symmetric, making them less complex and less prone to overfitting.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，观察第二层无知的树（右侧）在每个节点都使用相同的决策标准，即大小 < 15。虽然这是一个简单的例子，但请注意，我们只需要为无知的树学习两个分割标准，而不是标准的决策树。这使得无知的树更容易且更高效地训练，从而加快了整体训练速度。此外，无知的树是平衡且对称的，这使得它们更简单，更不容易过度拟合。
- en: 8.2.3 CatBoost in practice
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.3 CatBoost 的实际应用
- en: 'This section shows how to create a training pipeline with CatBoost. We’ll also
    look at an example of how to set the learning rate and employ early stopping as
    a means to control overfitting, as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了如何使用 CatBoost 创建训练管道。我们还将查看如何设置学习率并采用早期停止作为控制过度拟合的手段的示例：
- en: By selecting an effective learning rate, we try to control the rate at which
    the model learns so that it doesn’t rapidly fit and then overfit the training
    data. We can think of this as a proactive modeling approach, where we try to identify
    a good training strategy so that it leads to a good model.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过选择一个有效的学习率，我们试图控制模型学习的速率，使其不会快速拟合并过度拟合训练数据。我们可以将其视为一种主动建模方法，其中我们试图确定一个好的训练策略，以便它能够导致一个好的模型。
- en: By enforcing early stopping, we try to stop training as soon as we observe that
    the model is starting to overfit. We can think of this as a reactive modeling
    approach, where we contemplate terminating training as soon as we think we have
    a good model.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过实施早期停止，我们试图在观察到模型开始过度拟合时立即停止训练。我们可以将其视为一种反应式建模方法，其中我们考虑在认为我们有一个好模型时立即终止训练。
- en: We’ll use the Australian Credit Approval data set that we used in section 8.1.4\.
    The following listing provides a simple illustration of how to use CatBoost.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用在 8.1.4 节中使用的澳大利亚信用批准数据集。以下列表提供了一个如何使用 CatBoost 的简单示例。
- en: Listing 8.1 Using CatBoost
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.1 使用 CatBoost
- en: '[PRE19]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Loads the data set as a pandas DataFrame
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将数据集作为 pandas DataFrame 加载
- en: ❷ Explicitly identifies the categorical features
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 明确识别分类特征
- en: ❸ Prepares data for training and evaluation
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 准备训练和评估数据
- en: ❹ Trains an ensemble of five oblivious trees, each of depth 3
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 训练一个由五个深度为 3 的无知的树组成的集成
- en: ❺ Makes sure CatBoost knows which features are categorical
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 确保CatBoost知道哪些特征是分类的
- en: 'This listing trains and evaluates a CatBoost model as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表如下训练和评估 CatBoost 模型：
- en: '[PRE20]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Cross validation with CatBoost
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CatBoost 进行交叉验证
- en: CatBoost provides support for many loss functions for regression and classification
    tasks, and many features to control various aspects of training. This includes
    hyperparameters to control overfitting by controlling the complexity of the ensemble
    (iterations, with one tree trained per iteration) and the complexity of the base
    estimators (depth of the oblivious decision trees).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: CatBoost 为回归和分类任务提供了许多损失函数，以及许多用于控制训练各个方面（包括通过控制集成的复杂性（每次迭代训练一棵树）和基估计器的复杂性（无知的决策树深度））以控制过度拟合的超参数）的功能。
- en: In addition to these, another key hyperparameter is the learning_rate. Recall
    that the learning rate allows greater control over how quickly the complexity
    of the ensemble grows. Therefore, identifying an optimal learning rate for our
    data set in practice can help avoid overfitting and generalize well after training.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些之外，另一个关键的超参数是学习率。回想一下，学习率允许我们更好地控制集成复杂性的增长速度。因此，在实践中为我们的数据集确定一个最佳的学习率可以帮助避免过度拟合，并在训练后很好地泛化。
- en: As with previous ensemble approaches, we’ll use 5-fold CV to search over several
    different hyperparameter combinations to identify the best model. The following
    listing illustrates how to perform CV with CatBoost.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的集成方法一样，我们将使用 5 折交叉验证来搜索几个不同的超参数组合，以确定最佳模型。以下列表说明了如何使用 CatBoost 进行交叉验证。
- en: Listing 8.2 Cross validation with CatBoost
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.2 使用 CatBoost 的交叉验证
- en: '[PRE21]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Creates a grid of possible parameter combinations
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建可能的参数组合的网格
- en: ❷ Explicitly identifies the categorical features
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 明确识别分类特征
- en: ❸ Uses CatBoost’s built-in grid search functionality
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用 CatBoost 内置的网格搜索功能
- en: ❹ Performs 5-fold CV and then refits a model using the best parameters identified
    after grid search
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 执行5折交叉验证，然后使用网格搜索后确定的最佳参数重新拟合模型
- en: 'This listing evaluates the (2 x 3 x 2 = 12) hyperparameter combinations specified
    in parameters using 5-fold CV to identify the best parameter combination and refits
    (i.e., retrains) a final model with it:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表使用5折交叉验证评估了在参数中指定的（2 x 3 x 2 = 12）超参数组合，以确定最佳参数组合，并使用它重新拟合（即重新训练）最终模型：
- en: '[PRE22]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Early stopping with CatBoost
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CatBoost进行早期停止
- en: As with other ensemble methods, with each successive iteration, CatBoost adds
    a new base estimator to the ensemble. This causes the complexity of the overall
    ensemble to steadily increase during training until the model begins to overfit
    the training data. As with other ensemble methods, it’s possible to employ early
    stopping with CatBoost, where we monitor the performance of CatBoost with the
    help of an evaluation set to stop training as soon as there is no significant
    improvement in performance.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他集成方法一样，CatBoost在每次迭代中都会向集成中添加一个新的基估计器。这导致整个集成在训练过程中的复杂性稳步增加，直到模型开始过拟合训练数据。与其他集成方法一样，可以使用CatBoost的早期停止，通过评估集监控CatBoost的性能，一旦性能没有显著改进，就立即停止训练。
- en: In listing 8.3, we initialize CatBoost to train 100 trees. With early stopping
    of CatBoost, it’s possible to terminate training early, thus ensuring a good model
    as well as training efficiency, similar to LightGBM and XGBoost.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表8.3中，我们初始化CatBoost以训练100棵树。通过CatBoost的早期停止，可以提前终止训练，从而确保模型质量以及训练效率，类似于LightGBM和XGBoost。
- en: Listing 8.3 Early stopping with CatBoost
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.3 使用CatBoost进行早期停止
- en: '[PRE23]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Initializes a CatBoostClassifier with ensemble size 100
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化一个具有100个集成大小的CatBoostClassifier
- en: ❷ Creates an evaluation set by pooling “Xtst” and “ytst”
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过合并“Xtst”和“ytst”创建一个评估集
- en: ❸ Stops training if no improvement detected after five rounds
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 如果在五轮之后没有检测到改进，则停止训练
- en: ❹ Sets plotting to “true” for CatBoost to plot training and evaluation curves
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将CatBoost的绘图设置为“true”以绘制训练和评估曲线
- en: This code generates training and curves as shown in figure 8.11, where the effect
    of overfitting is observable. Around the 80th iteration, the training curve (dashed)
    is continuing to decrease, while the evaluation curve has begun to flatten.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码生成了如图8.11所示的训练和曲线，其中可以观察到过拟合的影响。大约在第80次迭代时，训练曲线（虚线）仍在持续下降，而评估曲线已经开始变平。
- en: '![CH08_F11_Kunapuli](../Images/CH08_F11_Kunapuli.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F11_Kunapuli](../Images/CH08_F11_Kunapuli.png)'
- en: Figure 8.11 Training (dashed) and evaluation (solid) curves generated by CatBoost.
    Thedot at the 88th iteration indicates the early stopping point.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 CatBoost生成的训练（虚线）和评估（实线）曲线。第88次迭代的点表示早期停止点。
- en: This means that the training error is continuing to decrease without an equivalent
    decrease in our validation set, indicating overfitting. CatBoost observes this
    behavior for five more iterations (as early_stopping_rounds=5) and then terminates
    training.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着训练错误在继续下降，而我们的验证集没有相应的下降，表明过拟合。CatBoost观察到这种行为持续了五次迭代（因为early_stopping_rounds=5），然后终止了训练。
- en: 'The final model reports a test set performance of 82.61%, achieved after 88
    rounds, with early stopping avoiding training all the way to 100 iterations as
    originally specified:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最终模型报告了测试集性能为82.61%，经过88轮迭代后达到，通过早期停止避免了按照最初指定的100次迭代进行训练：
- en: '[PRE24]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '8.3 Case study: Income prediction'
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 案例研究：收入预测
- en: 'In this section, we study the problem of *income prediction* from demographic
    data. Demographic data typically contains many different types of features, including
    categorical and continuous features. We’ll explore two approaches to training
    ensemble methods:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们研究从人口统计数据中预测*收入*的问题。人口统计数据通常包含许多不同类型的特征，包括分类和连续特征。我们将探讨两种训练集成方法的途径：
- en: '*Approach 1 (sections 8.3.2 and 8.3.3)*—Preprocess categorical features using
    the category_encoders package and then train ensembles using scikit-learn’s random
    forest, LightGBM, and XGBoost with preprocessed features.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*方法1（第8.3.2节和第8.3.3节）*—使用category_encoders包预处理分类特征，然后使用scikit-learn的随机森林、LightGBM和XGBoost对预处理后的特征进行集成训练。'
- en: '*Approach 2 (section 8.3.4)*—Use CatBoost to directly handle categorical features
    during training through ordered target statistics and ordered boosting.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*方法2（第8.3.4节）*—使用CatBoost在训练过程中直接处理分类特征，通过有序目标统计和有序提升。'
- en: 8.3.1 Adult Data Set
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 成人数据集
- en: This case study uses the Adult Data Set from the UCI Machine Learning Repository.
    The task is to predict whether an individual will earn more or less than $50,000
    per year based on several demographic indicators such as education, marital status,
    race, and gender.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 本案例研究使用了UCI机器学习仓库中的成人数据集。任务是预测个人每年收入是否超过或低于50,000美元，基于教育、婚姻状况、种族和性别等几个人口统计指标。
- en: 'This data set contains a nice mix of categorical and continuous features, which
    makes it an ideal choice for this case study. The data set is available along
    with the source code. Let’s load the data set and visualize it (see figure 8.12):'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含了一系列分类和连续特征，这使得它成为本案例研究的理想选择。数据集和源代码都可用。让我们加载数据集并可视化它（见图8.12）：
- en: '[PRE25]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![CH08_F12_Kunapuli](../Images/CH08_F12_Kunapuli.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F12_Kunapuli](../Images/CH08_F12_Kunapuli.png)'
- en: Figure 8.12 The Adult Data Set contains categorical and continuous features.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 成人数据集包含分类和连续特征。
- en: 'This data set contains several categorical features:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含几个分类特征：
- en: 'workclass—Describes the classification of the type of employment and contains
    eight categories: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov,
    State-gov, Without-pay, Never-worked.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: workclass—描述了就业类型的分类，包含八个类别：私营、自营非营利、自营营利、联邦政府、地方政府、州政府、无报酬、从未工作过。
- en: 'education—Describes the highest education level attained and contains 16 categories:
    Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th,
    7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: education—描述了达到的最高教育水平，包含16个类别：学士学位、一些大学、11年级、高中毕业、专业学校、大专、专科、9年级、7-8年级、12年级、硕士学位、1-4年级、10年级、博士学位、5-6年级、学前教育。
- en: 'marital-status—Describes the marital situation and has seven categories: Married-civ-spouse,
    Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: marital-status—描述了婚姻状况，有七个类别：已婚平民配偶、离婚、未婚、分居、丧偶、已婚配偶缺席、已婚AF配偶。
- en: 'occupation—Describes the classification of the occupation area and contains
    14 categories: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial,
    Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing,
    Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: occupation—描述了职业领域的分类，包含14个类别：技术支持、工艺维修、其他服务、销售、执行管理、专业特长、搬运清洁工、机器操作检查员、行政文员、农业渔业、运输搬运、私人家庭服务、保护服务、武装部队。
- en: 'relationship—Describes relationship status and has six categories: Wife, Own-child,
    Husband, Not-in-family, Other-relative, Unmarried.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: relationship—描述了关系状态，有六个类别：妻子、亲生子女、丈夫、非家庭、其他亲属、未婚。
- en: 'sex—Describes gender and has two categories: male, female.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: sex—描述性别，有两个类别：男性、女性。
- en: native-country—This high(ish)-cardinality categorical variable describes the
    native country and contains 30 unique countries.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: native-country—这是一个高（ish）基数分类变量，描述了原籍国，包含30个独特的国家。
- en: In addition, the data set also contains several continuous features, such as
    age, number of years of education, number of hours worked per week, capital gains
    and losses, and so on.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，该数据集还包含几个连续特征，如年龄、教育年限、每周工作时间、资本收益和损失等。
- en: Fairness, bias, and the Adult Data Set
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性、偏见和成人数据集
- en: This data set was originally created from the 1994 Current Population Survey
    conducted by the US Census Bureau and has since been used in hundreds of research
    papers, machine-learning tutorials, and class projects, both as a benchmark data
    set and as a pedagogical tool.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集最初由美国人口普查局于1994年进行的1994年当前人口调查创建，此后已被用于数百篇研究论文、机器学习教程和课堂项目，既作为基准数据集，也作为教学工具。
- en: In recent years, it has also become an important data set for research in the
    area of Fairness in AI, also known as algorithmic fairness, which explores approaches
    to ensure that machine-learning algorithms don’t reinforce real-world biases and
    do strive for fair outcomes.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，它也已成为人工智能领域公平性研究的重要数据集，也称为算法公平性，该研究探索确保机器学习算法不会加强现实世界的偏见并努力追求公平结果的方法。
- en: For example, let’s say we were training an ensemble model to screen and then
    accept or reject job resumes for software engineering positions based on historical
    data. Historical hiring data would indicate that men are more likely to be hired
    for these positions than women. If we use such biased data for training, machine-learning
    models (including ensemble methods) will pick up this bias during learning and
    make biased hiring decisions when deployed, resulting in real world discriminatory
    outcomes!
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们正在训练一个集成模型，用于根据历史数据筛选并接受或拒绝软件工程职位的简历。历史招聘数据显示，男性比女性更有可能获得这些职位。如果我们使用这样的有偏数据来训练，机器学习模型（包括集成方法）将在学习过程中捕捉到这种偏见，并在部署时做出有偏的招聘决策，从而导致现实世界的歧视性结果！
- en: The Adult Data Set is also similarly biased, and subtly so as both the prediction
    target (“Will an individual earn more or less than $50,000 per year?”) and data
    features are disproportionately discriminative toward women and minorities. This
    means that models trained using this data set will also be discriminative and
    should not be used in practice for data-driven decision making. See the article
    by Ding et al.^a for more details on this fascinating and extremely important
    area of machine learning.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 成人数据集也存在类似的偏见，这种偏见是微妙的，因为预测目标（“个人一年内是否会赚得超过或低于50,000美元？”）和数据特征都不成比例地歧视女性和少数族裔。这意味着使用此数据集训练的模型也将具有歧视性，不应在实际的数据驱动决策中使用。有关这个令人着迷且极其重要的机器学习领域的更多详细信息，请参阅Ding等人^a的文章。
- en: Finally, it should be noted that this data set is used here solely as a teaching
    tool to illustrate different approaches to handling data sets with categorical
    variables.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，应该注意的是，这个数据集在这里仅作为教学工具，用于说明处理具有分类变量的数据集的不同方法。
- en: '^a *Retiring Adult: New Datasets for Fair Machine Learning*, by Frances Ding,
    Moritz Hardt, John Miller, and Ludwig Schmidt. Proceedings of the 32nd International
    Conference on Neural Information Processing Systems (2021) ([http://mng.bz/ydWe](http://mng.bz/ydWe)).'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ^a *《退休成年人：公平机器学习的新数据集》*，作者：Frances Ding、Moritz Hardt、John Miller 和 Ludwig Schmidt。第32届国际神经网络信息处理系统会议论文集（2021）([http://mng.bz/ydWe](http://mng.bz/ydWe))。
- en: In the following listing, we explore some of the categorical features using
    the seaborn package, which provides some neat functions for quickly exploring
    and visualizing data sets.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，我们使用seaborn包探索了一些分类特征，该包提供了一些方便的功能，用于快速探索和可视化数据集。
- en: Listing 8.4 Categorical features in the Adult Data Set
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.4 成人数据集中的分类特征
- en: '[PRE26]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This listing produces figure 8.13.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表生成图8.13。
- en: '![CH08_F13_Kunapuli](../Images/CH08_F13_Kunapuli.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F13_Kunapuli](../Images/CH08_F13_Kunapuli.png)'
- en: 'Figure 8.13 Visualizing the category counts of three categorical features in
    the Adult Data Set: workclass, marital-status, and race. Note that all the y-axes
    are in logarithmic (base 10) scale.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13 在成人数据集中可视化三个分类特征的类别计数：workclass、marital-status和race。注意，所有的y轴都是对数尺度（基数为10）。
- en: 8.3.2 Creating preprocessing and modeling pipelines
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 创建预处理和建模管道
- en: Listing 8.5 describes how to prepare the data. In particular, we use sklearn
    .preprocessing.LabelEncoder to convert the target labels from string (<=50k, >50k)
    to numeric (0/1). LabelEncoder is identical to OrdinalEncoder, except that it’s
    specifically designed to work with 1D data (targets).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.5描述了如何准备数据。特别是，我们使用sklearn .preprocessing.LabelEncoder将目标标签从字符串（<=50k, >50k）转换为数值（0/1）。LabelEncoder与OrdinalEncoder相同，但它专门设计用于处理1D数据（目标）。
- en: Listing 8.5 Preparing the Adult Data Set
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.5 准备成人数据集
- en: '[PRE27]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ Splits the data into features and targets
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将数据分为特征和目标
- en: ❷ Encodes the labels
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 编码标签
- en: ❸ Splits the data into train and test sets
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将数据分为训练集和测试集
- en: ❹ Explicitly identifies categorical and continuous features
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 明确识别分类和连续特征
- en: 'Recall that the task is to predict if the income is greater than $50,000 (with
    labels y=1) or less than $50,000 (with labels y=0). One thing to note about this
    data set is that it’s imbalanced; that is, it contains different proportions of
    the two classes:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，任务是预测收入是否超过50,000美元（标签y=1）或低于50,000美元（标签y=0）。关于这个数据集，有一点需要注意，那就是它是不平衡的；也就是说，它包含两个类别的不同比例：
- en: '[PRE28]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Here, we see that the positive-negative distribution is 24.1% to 75.9% (unbalanced),
    rather than 50% to 50% (balanced). This means that evaluation metrics such as
    accuracy can unintentionally skew our view of model performance as they assume
    a balanced data set.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到正负分布为24.1%到75.9%（不平衡），而不是50%到50%（平衡）。这意味着评估指标如准确率可能会无意中歪曲我们对模型性能的看法，因为它们假设数据集是平衡的。
- en: Next, we define a preprocessing function that can be reused with different types
    of category encoders. This function has two preprocessing pipelines, one to be
    applied to continuous features only, and the other for categorical features. The
    continuous features are preprocessed using StandardScaler, which normalizes each
    feature column to have zero mean and unit standard deviation.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个预处理函数，它可以与不同类型的类别编码器一起重复使用。这个函数有两个预处理管道，一个用于仅应用于连续特征，另一个用于类别特征。连续特征使用StandardScaler进行预处理，将每个特征列标准化为零均值和单位标准差。
- en: In addition, both pipelines have a SimpleImputer to impute missing values. Missing
    continuous values are imputed with their corresponding median feature value, while
    missing categorical features are imputed as a new category called 'missing' prior
    to encoding.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，两个管道都有一个SimpleImputer来填充缺失值。缺失的连续值用相应的中值特征值填充，而缺失的类别特征在编码之前填充为一个新的类别，称为'missing'。
- en: For example, the feature workclass has missing values (indicated by '?'), which
    are treated as a separate category for modeling purposes. The following listing
    implements separate preprocessing pipelines for continuous and categorical features
    and returns a ColumnTransformer, which can be applied directly to any subset of
    training data from this domain.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，特征workclass有缺失值（用'?'表示），在建模目的上被视为一个单独的类别。下面的列表实现了针对连续和类别特征的单独预处理管道，并返回一个ColumnTransformer，可以直接应用于该领域的任何训练数据子集。
- en: Listing 8.6 Preprocessing pipelines
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.6 预处理管道
- en: '[PRE29]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ Preprocessing pipeline for continuous features
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 连续特征的预处理管道
- en: ❷ Preprocessing pipeline for categorical features
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 类别特征的预处理管道
- en: ❸ “ColumnTransformer” object used to combine the pipelines
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 用于组合管道的“ColumnTransformer”对象
- en: This listing will create and return a scikit-learn ColumnTransformer object,
    which can apply a similar preprocessing strategy to training and test sets, ensuring
    consistency and minimizing data leakage.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表将创建并返回一个scikit-learn ColumnTransformer对象，它可以应用于训练集和测试集的类似预处理策略，确保一致性并最小化数据泄露。
- en: Finally, we define a function to train and evaluate different types of ensembles,
    combining them with various types of category encoding. This will enable us to
    create different ensemble models by combining ensemble learning packages with
    various types of category encoders.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义一个函数来训练和评估不同类型的集成，将它们与各种类型的类别编码相结合。这将使我们能够通过将集成学习包与各种类型的类别编码器相结合来创建不同的集成模型。
- en: The function in listing 8.7 allows us to pass an ensemble as well as a grid
    of ensemble parameters for ensemble parameter selection. It uses k-fold CV combined
    with randomized search to identify the best ensemble parameters before training
    a final model with these best parameters.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.7中的函数允许我们传递一个集成以及集成参数的网格，用于集成参数选择。它使用k折交叉验证结合随机搜索来识别最佳集成参数，然后在训练最终模型之前使用这些最佳参数。
- en: 'Once trained, the function evaluates final model performance on the test set
    using three metrics: accuracy, balanced accuracy, and F1 score. Balanced accuracy
    and F1 score are especially useful metrics when the data set is imbalanced, as
    they take label imbalance into account by weighting model performance on each
    class based on how often they appear in the labels.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，该函数将使用三个指标在测试集上评估最终模型性能：准确率、平衡准确率和F1分数。当数据集不平衡时，平衡准确率和F1分数是特别有用的指标，因为它们通过根据每个类别在标签中出现的频率来加权模型在每个类上的性能来考虑标签不平衡。
- en: Listing 8.7 Training and evaluating combinations of encoders and ensembles
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.7 训练和评估编码器和集成组合
- en: '[PRE30]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ Specifies ensemble and parameter grid
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指定集成和参数网格
- en: ❷ Maximum number of parameter combinations for randomized grid search
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 随机网格搜索的最大参数组合数
- en: ❸ Number of CV folds for parameter selection
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 参数选择的CV折叠数
- en: ❹ Different categorical encoding strategies to try
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 要尝试的不同类别编码策略
- en: ❺ Initializes preprocessor pipeline (refer to listing 8.6)
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 初始化预处理管道（参见表8.6）
- en: ❻ Parameter selection using randomized grid search
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用随机网格搜索进行参数选择
- en: ❼ Refits a final ensemble using the best parameters
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用最佳参数重新拟合最终集成
- en: ❽ Evaluates final ensemble performance and saves the results
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 评估最终集成性能并保存结果
- en: 8.3.3 Category encoding and ensembling
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.3 类别编码和集成
- en: 'In this section, we’ll train various combinations of encoders and ensemble
    methods. In particular, we consider the following:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将训练各种编码器和集成方法的组合。特别是，我们考虑以下内容：
- en: '*Encoders*—One-hot, ordinal, and greedy target encoding (from the category_encoders
    package)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*编码器*——One-hot、序数和贪婪目标编码（来自category_encoders包）'
- en: '*Ensembles*—scikit-learn’s random forest, gradient boosting with LightGBM,
    and Newton boosting with XGBoost'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*集成*——scikit-learn的随机森林、LightGBM的梯度提升和XGBoost的牛顿提升'
- en: 'For each combination of encoder and ensemble, we follow the same steps implemented
    in listings 8.6 and 8.7: preprocess the features, perform ensemble parameter selection
    to get the best ensemble parameters, refit a final ensemble model with the best
    parameter combination, and evaluate the final model.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 对于编码器和集成组合的每一种组合，我们遵循与列表8.6和8.7中实现相同的步骤：预处理特征，执行集成参数选择以获得最佳集成参数，使用最佳参数组合重新拟合最终集成模型，并评估最终模型。
- en: Random forest
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林
- en: The following listing trains and evaluates the best combination of categorical
    encoding (one-hot, ordinal, and greedy target) and random forest.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表训练并评估了类别编码（one-hot、序数和贪婪目标）和随机森林的最佳组合。
- en: Listing 8.8 Category encoding followed by ensembling with random forest
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.8：类别编码后使用随机森林进行集成
- en: '[PRE31]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ Number of trees in the random forest ensemble
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 随机森林集成中的树的数量
- en: ❷ Maximum depth of individual trees in the ensemble
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 集成中单个树的最大深度
- en: ❸ Fraction of features/columns during tree learning
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 树学习期间使用的特征/列的分数
- en: ❹ Randomized grid search with 25 parameter combinations and 5-fold CV
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用25个参数组合和5折交叉验证的随机网格搜索
- en: 'This listing returns the following results (edited to fit the page):'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表返回以下结果（编辑以适应页面）：
- en: '[PRE32]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Observe the difference between plain accuracy (Acc) and balanced accuracy (B
    Acc) or F1 score (F1) for both the training and test sets. Since balanced accuracy
    explicitly accounts for class imbalance, it provides a better estimate of model
    performance than accuracy. This illustrates the importance of using the right
    metric to evaluate our models.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 观察训练集和测试集的纯准确率（Acc）和平衡准确率（B Acc）或F1分数（F1）之间的差异。由于平衡准确率明确考虑了类别不平衡，它比准确率提供了更好的模型性能估计。这说明了使用正确的指标来评估我们的模型的重要性。
- en: While all encoding methods appear equally effective using plain accuracy as
    the evaluation metric, encoding with target statistics seems to be most effective
    in classifying between the positive and negative examples.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然所有编码方法在以纯准确率作为评估指标时看起来同样有效，但使用目标统计信息进行编码似乎在区分正例和负例方面最为有效。
- en: LightGBM
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM
- en: 'Next, we repeat this training and evaluation procedure with LightGBM, where
    we train an ensemble with 200 trees, as shown in following listing. Several other
    ensemble hyperparameters will be selected using 5-fold CV: maximum tree depth,
    learning rate, bagging fraction, and regularization parameters.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用LightGBM重复这一训练和评估过程，其中我们训练了一个包含200棵树的集成，如下所示。其他几个集成超参数将使用5折交叉验证进行选择：最大树深度、学习率、袋装分数和正则化参数。
- en: Listing 8.9 Category encoding followed by ensembling with LightGBM
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.9：类别编码后使用LightGBM进行集成
- en: '[PRE33]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ❶ Maximum depth of individual trees in the ensemble
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 集成中单个树的最大深度
- en: ❷ Learning rate for gradient boosting
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 梯度提升的学习率
- en: ❸ Fraction of examples used during tree learning
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 树学习期间使用的示例分数
- en: ❹ Parameters for weight regularization
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 权重正则化参数
- en: 'This listing returns the following results (edited to fit the page):'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表返回以下结果（编辑以适应页面）：
- en: '[PRE34]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: With LightGBM, all three encoding methods lead to ensembles with roughly similar
    generalization performance as evidenced by the test set balanced accuracy and
    F1 scores. The overall performance is also better than random forest.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LightGBM，所有三种编码方法都导致具有大致相似泛化性能的集成，这由测试集的平衡准确率和F1分数所证明。整体性能也优于随机森林。
- en: XGBoost
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost
- en: Finally, we repeat this training and evaluation procedure with XGBoost as well,
    where we again train an ensemble of 200 trees, as shown in the following listing.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们同样使用XGBoost重复了这一训练和评估过程，其中我们再次训练了一个包含200棵树的集成，如下所示。
- en: Listing 8.10 Category encoding followed by ensembling with XGBoost
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.10：类别编码后使用XGBoost进行集成
- en: '[PRE35]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ❶ Maximum depth of individual trees in the ensemble
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 集成中单个树的最大深度
- en: ❷ Learning rate for Newton boosting
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 牛顿提升的学习率
- en: ❸ Fraction of features/columns during tree learning
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 树学习期间的特征/列的分数
- en: ❹ Parameters for weight regularization
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 权重正则化的参数
- en: 'This listing returns the following results (edited to fit the page):'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 该列表返回以下结果（编辑以适应页面）：
- en: '[PRE36]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: As with LightGBM, all three encoding methods lead to XGBoost ensembles with
    roughly similar generalization performance. The overall performance of XGBoost
    is similar to that of LightGBM, but better than random forest.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 与LightGBM一样，所有三种编码方法都导致XGBoost集成具有大致相似的一般化性能。XGBoost的整体性能与LightGBM相似，但优于随机森林。
- en: 8.3.4 Ordered encoding and boosting with CatBoost
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.4 使用CatBoost进行有序编码和提升
- en: Finally, we explore the performance of CatBoost on this data set. Unlike the
    previous approaches, we won’t use the category_encoders package. This is because
    CatBoost uses ordered target statistics along with ordered boosting. Therefore,
    as long as we clearly identify the categorical features that need encoding with
    ordered target statistics, CatBoost will take care of the rest without any additional
    preprocessing! The following listing performs ordered boosting with CV-based randomized
    parameter search.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们探索了CatBoost在此数据集上的性能。与之前的方法不同，我们不会使用category_encoders包。这是因为CatBoost使用有序目标统计信息以及有序提升。因此，只要我们清楚地识别出需要使用有序目标统计信息进行编码的类别特征，CatBoost就会处理其余部分，无需任何额外的预处理！以下列表执行了基于CV的随机参数搜索的有序提升。
- en: Listing 8.11 Ordered target encoding and ordered boosting with CatBoost
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.11使用CatBoost进行有序目标编码和有序提升
- en: '[PRE37]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ❶ Number of trees in the random forest ensemble
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 随机森林集成中的树的数量
- en: ❷ Maximum depth of individual trees in the ensemble
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 集成中单个树的最大深度
- en: ❸ Learning rate for Newton boosting
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 牛顿提升的学习率
- en: ❹ Parameters for weight regularization
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 权重正则化的参数
- en: ❺ Uses CatBoost’s randomized search functionality
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用CatBoost的随机搜索功能
- en: 'CatBoost provides its own randomized_search feature, which can be initialized
    and invoked similarly to scikit-learn’s RandomizedGridCV, which we used in the
    previous section:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: CatBoost提供自己的randomized_search功能，其初始化和调用方式类似于我们在上一节中使用的scikit-learn的RandomizedGridCV：
- en: '[PRE38]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: CatBoost’s performance on this data set is comparable to that of LightGBM and
    XGBoost, and better than random forests.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: CatBoost在此数据集上的性能与LightGBM和XGBoost相当，且优于随机森林。
- en: Now, let’s put the results of all the approaches side by side; in figure 8.14,
    we look at how each approach performed with respect to balanced accuracy evaluated
    on the test set.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将所有方法的结果并排放置；在图8.14中，我们查看每种方法在测试集上根据平衡准确率评估的性能。
- en: '![CH08_F14_Kunapuli](../Images/CH08_F14_Kunapuli.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F14_Kunapuli](../Images/CH08_F14_Kunapuli.png)'
- en: Figure 8.14 The test set performance (with the balanced accuracy metric) of
    various encoding and ensemble method combinations
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14各种编码和集成方法组合的测试集性能（使用平衡准确率指标）
- en: 'In analyzing these results, always keep in mind that there is no free lunch,
    and no one method is best performing all the time. However, CatBoost does enjoy
    two key benefits:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析这些结果时，始终牢记没有免费的午餐，没有一种方法总是表现最佳。然而，CatBoost确实享有两个关键优势：
- en: CatBoost allows for a consolidated approach to encoding and handling categorical
    features, unlike other ensemble approaches that necessarily use a two-step encode
    + ensemble approach.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他必须使用两步编码+集成方法的集成方法不同，CatBoost允许对编码和类别特征的处理采取统一的方法。
- en: By design, CatBoost mitigates data and target leakage as well as distribution
    shift problems, which often need more care with other ensembling approaches.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按设计，CatBoost减轻了数据泄漏和目标泄漏以及分布偏移问题，这些问题在其他集成方法中通常需要更多的关注。
- en: 8.4 Encoding high-cardinality string features
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 编码高基数字符串特征
- en: We wrap up this chapter by exploring encoding techniques for *high-cardinality
    categorical features*. The cardinality of a categorical feature is simply the
    number of unique categories in the feature. The number of categories is an important
    consideration in categorical encoding.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过探索*高基数类别特征*的编码技术来结束本章。类别特征的基数简单地是该特征中唯一类别的数量。类别的数量在类别编码中是一个重要的考虑因素。
- en: Real-world data sets often contain categorical string features, where feature
    values are strings. For example, consider a categorical feature of job titles
    at an organization. This feature can contain dozens to hundreds of job titles
    from “Intern” to “President and CEO,” each with their own unique roles and responsibilities.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的数据集通常包含分类字符串特征，其中特征值是字符串。例如，考虑一个组织中的职位头衔的分类特征。这个特征可以包含从“Intern”到“President
    and CEO”的几十到几百个职位头衔，每个头衔都有其独特的角色和责任。
- en: Such features contain a large number of categories and are inherently high-cardinality
    features. This disqualifies encoding approaches such as one-hot encoding (because
    it increases feature dimension significantly) or ordinal encoding (because a natural
    ordering might not always exist).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的特征包含大量类别，并且本质上就是高基数特征。这使诸如独热编码（因为它显著增加了特征维度）或顺序编码（因为自然顺序可能并不总是存在）这样的编码方法不适用。
- en: 'What’s more, in real-world data sets, such high-cardinality features are also
    “dirty,” in that there are several variations of the same category:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，在现实世界的数据集中，这样的高基数特征也是“脏”的，因为同一个类别有几种不同的变体：
- en: 'Natural variations can arise because data is compiled from different sources.
    For example, two departments in the same organization may have different titles
    for the exact same role: “Lead Data Scientist” and “Senior Data Scientist.”'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然变异可能是因为数据是从不同的来源编译的。例如，同一组织中的两个部门可能对完全相同的角色有不同的头衔：“Lead Data Scientist” 和
    “Senior Data Scientist。”
- en: Many such data sets are manually entered into databases, which introduces noise
    due to typos and other errors. For example, “Data Scientsit” [sic] versus “Data
    Scientist.”
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多此类数据集都是手动输入到数据库中的，这会由于拼写错误和其他错误而引入噪声。例如，“Data Scientsit” [原样] 与 “Data Scientist。”
- en: Because two (or more!) such variants don’t match exactly, they are treated as
    their own unique categories, even though common sense suggests that they should
    be cleaned and/or merged. This causes additional problems with high-cardinality
    string features by adding new categories to an already large set of categories.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 因为两个（或更多！）这样的变体不完全匹配，它们被视为它们自己的独特类别，尽管常识表明它们应该被清理和/或合并。这通过向已经很大的类别集中添加新类别，给高基数字符串特征带来了额外的问题。
- en: To address this problem, we’ll need to determine categories (and how to encode
    them) by *string similarity* rather than by exact matching. The intuition behind
    this approach is to encode similar categories together in a way that a human might
    in order to ensure that the downstream learning algorithm treats them similarly
    (as it should).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们需要通过 *字符串相似度* 而不是通过精确匹配来确定类别（以及如何编码它们）。这种方法的直觉是将相似的类别一起编码，就像人类可能做的那样，以确保下游学习算法将它们视为相似（正如它应该做的那样）。
- en: For example, similarity-based encoding would encode “Data Scientsit” [sic] and
    “Data Scientist” with similar features so that they appear nearly identical to
    a learning algorithm. Similarity-based encoding methods use the notion of *string
    similarity* to identify similar categories.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，基于相似度的编码会将“Data Scientsit” [原样] 和 “Data Scientist” 编码为具有相似特征的，以便它们在学习算法中看起来几乎相同。基于相似度的编码方法使用
    *字符串相似度* 的概念来识别相似的类别。
- en: Such string similarity metrics, or measures, are widely used in natural language
    and text applications, for example, in autocorrect applications, database retrieval,
    or in language translation.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的字符串相似度度量或指标在自然语言和文本应用中得到了广泛的应用，例如在自动纠错应用、数据库检索或在语言翻译中。
- en: String similarity metrics
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串相似度度量
- en: A *similarity metric* is a function that takes two objects and returns a numeric
    similarity measure between them. Higher values mean that the two objects are more
    similar to each other. A string similarity metric operates on strings.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '*相似度度量* 是一个函数，它接受两个对象并返回它们之间的数值相似度。值越高意味着两个对象彼此越相似。字符串相似度度量在字符串上操作。'
- en: Measuring the similarity between strings is challenging as strings can be of
    different lengths and can have similar substrings in different locations. To identify
    if two strings are similar potentially requires matching characters and subsequences
    of all possible lengths and locations. This combinatorial complexity means that
    computing string similarity can be computationally expensive.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 测量字符串之间的相似度具有挑战性，因为字符串的长度可能不同，并且可能在不同的位置有相似的子字符串。要确定两个字符串是否相似，可能需要匹配所有可能的长度和位置的字符和子序列。这种组合复杂性意味着计算字符串相似度可能非常耗时。
- en: Several efficient approaches to computing string similarity between strings
    of different lengths exist. Two common types are *character-based* string similarity
    and *token-based* string similarity, depending on the granularity of the string
    components being compared.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 存在几种计算不同长度字符串之间相似度的有效方法。两种常见类型是 *基于字符* 的字符串相似度和 *基于标记* 的字符串相似度，这取决于比较的字符串组件的粒度。
- en: Character-based approaches measure string similarity by the number of operations
    at the character level (insertion, deletion, or substitution) needed to transform
    one string to another. These approaches are well suited for short strings.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 基于字符的方法通过在字符级别（插入、删除或替换）需要的操作数量来衡量字符串相似度，以将一个字符串转换为另一个字符串。这些方法非常适合短字符串。
- en: Longer strings are often decomposed into tokens, typically substrings or words,
    called n-grams. Token-based approaches measure string similarity at the token
    level.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 较长的字符串通常被分解成标记，通常是子字符串或单词，称为 n-gram。基于标记的方法在标记级别衡量字符串相似度。
- en: Irrespective of which string similarity metric you use, the similarity score
    can be used to encode both high-cardinality features (by grouping similar string
    categories together) and dirty features (by “cleaning” typos).
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 不论你使用哪种字符串相似度指标，相似度分数都可以用来编码高基数特征（通过将相似的字符串类别分组在一起）和脏特征（通过“清理”错误）。
- en: The dirty_cat package
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: The dirty_cat package
- en: 'The dirty_cat package ([https://dirty-cat.github.io/stable/index.xhtml](https://dirty-cat.github.io/stable/index.xhtml))
    provides category similarity metrics off the shelf and can be used seamlessly
    in modeling pipelines. The package provides three specialized encoders to handle
    so-called “dirty categories,” which are essentially noisy and/or high-cardinality
    string categories:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: The dirty_cat package ([https://dirty-cat.github.io/stable/index.xhtml](https://dirty-cat.github.io/stable/index.xhtml))
    提供了现成的类别相似度度量，并且可以无缝地用于建模管道。该包提供了三个专门的编码器来处理所谓的“脏类别”，这些类别本质上是有噪声的/或高基数的字符串类别：
- en: SimilarityEncoder—A version of one-hot encoding constructed using string similarities
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SimilarityEncoder—使用字符串相似度构建的一元编码版本
- en: GapEncoder—Encodes categories by considering frequently co-occurring substring
    combinations
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GapEncoder—通过考虑频繁共现的子字符串组合来编码类别
- en: MinHashEncoder—Encodes categories by applying hashing techniques to substrings
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MinHashEncoder—通过应用哈希技术到子字符串来编码类别
- en: We use another salary data set to see how we can use the dirty_cat package in
    practice. This data set is a modified version of a publicly available Employee
    Salaries data set from Data.gov, with the goal being to predict an individual’s
    salary given their job title and department.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用另一个薪资数据集来了解如何在实践中使用 dirty_cat 包。这个数据集是 Data.gov 公开可用的 Employee Salaries
    数据集的一个修改版本，目标是预测个人的薪资，给定他们的职位和部门。
- en: 'First, we load the data set (available along with the source code) and visualize
    the first few rows:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载数据集（与源代码一起提供）并可视化前几行：
- en: '[PRE39]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Figure 8.15 shows the first few rows of this data set.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.15 展示了该数据集的前几行。
- en: '![CH08_F15_Kunapuli](../Images/CH08_F15_Kunapuli.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![CH08_F15_Kunapuli](../Images/CH08_F15_Kunapuli.png)'
- en: Figure 8.15 The Employee Salaries data set mostly contains string categories.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.15 员工薪资数据集主要包含字符串类别。
- en: 'The “salary” column is the target variable, making this a regression problem.
    We split this data frame into features and labels:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: “salary”列是目标变量，这使得这是一个回归问题。我们将这个数据框拆分为特征和标签：
- en: '[PRE40]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can get a sense of which are high-cardinality features by counting the number
    of unique categories or values per column:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过计算每列的独特类别或值的数量来感知哪些是高基数特征：
- en: '[PRE41]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We see that the feature employee_position_title has 385 unique string categories,
    making this a high-cardinality feature. Directly encoding this using one-hot encoding,
    say, would introduce 385 new columns into our data set, thus increasing the number
    of columns greatly!
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到特征 employee_position_title 有 385 个独特的字符串类别，这使得这是一个高基数特征。直接使用一元编码，比如，会向我们的数据集中引入
    385 个新列，从而大大增加列的数量！
- en: 'Instead, let’s see how we can use the dirty_cat package to train an XGBoost
    ensemble on this data set. First, let’s identify the different types of features
    in our data set explicitly:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，让我们看看如何使用 dirty_cat 包来训练 XGBoost 集成模型。首先，让我们明确地识别数据集中不同类型的特征：
- en: '[PRE42]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, let’s initialize the different dirty_cat encoders we want to use:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们初始化我们想要使用的不同 dirty_cat 编码器：
- en: '[PRE43]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: ❶ Specifies the string similarity measure to use
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指定要使用的字符串相似度度量
- en: ❷ Encoding dimension
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 编码维度
- en: The most important encoding parameter for all encoding methods is the is n_components,
    which is also known as the *encoding dimension*.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有编码方法来说，最重要的编码参数是 is n_components，也称为 *编码维度*。
- en: 'SimilarityEncoder measures n-gram similarity between two strings. An n-gram
    is simply a sequence of n successive words. For example, the string “I love ensemble
    methods.” contains three 2-grams: “I love,” “love ensemble,” and “ensemble methods.”
    n-gram similarity between two strings first computes all possible n-grams in each
    string and then computes similarity over the n-grams. By default, SimilarityEncoder
    constructs all 2-, 3-, and 4-grams, and then encodes all similar strings using
    one-hot encoding. This means that it will determine its own encoding dimension.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: SimilarityEncoder 测量两个字符串之间的 n-gram 相似性。n-gram 简单地是一个连续的 n 个单词序列。例如，字符串 “I love
    ensemble methods.” 包含三个 2-gram：“I love”，“love ensemble” 和 “ensemble methods”。两个字符串之间的
    n-gram 相似性首先计算每个字符串中所有可能的 n-gram，然后对 n-gram 计算相似性。默认情况下，SimilarityEncoder 构建所有
    2-gram、3-gram 和 4-gram，然后使用独热编码对所有相似字符串进行编码。这意味着它将确定自己的编码维度。
- en: To understand the encoding dimension, consider that we’re one-hot encoding the
    feature employee_position_title, which contains 385 unique categories, which can
    be grouped into 225 “similar” categories with a similarity metric. One-hot encoding
    will convert each categorical value to a 225-dimensional vector, making the encoding
    dimension 225.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解编码维度，考虑我们正在对包含 385 个唯一类别的特征 employee_position_title 进行独热编码，这些类别可以通过相似性度量分组为
    225 个“相似”类别。独热编码将每个类别值转换为 225 维向量，使得编码维度为 225。
- en: MinHashEncoder and GapEncoder, on the other hand, can take in a user-specified
    encoding dimension and create an encoding of the specified size. In this case,
    the encoding dimension is specified to be 100 for both, which is much smaller
    than one-hot encoding would be forced to use.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: MinHashEncoder 和 GapEncoder 另一方面，可以接受用户指定的编码维度并创建指定大小的编码。在这种情况下，编码维度被指定为 100，对于两者来说，这比独热编码强制使用的要小得多。
- en: Practically, the encoding dimension (n_components) is a modeling choice, and
    the best value should be determined through k-fold CV, depending on the tradeoff
    between model training time versus model performance.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，编码维度（n_components）是一个建模选择，最佳值应通过 k 折交叉验证来确定，这取决于模型训练时间与模型性能之间的权衡。
- en: We put all this together into the following listing, which trains three different
    XGBoost models, one for each type of dirty_cat encoding.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些内容组合到以下列表中，该列表训练了三个不同的 XGBoost 模型，每个模型对应一种 dirty_cat 编码类型。
- en: Listing 8.12 Encoding and ensembling with high-cardinality features
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.12 使用高基数特征的编码和集成
- en: '[PRE44]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: ❶ Identifies low-cardinality features
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 识别低基数特征
- en: ❷ Identifies high-cardinality features
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 识别高基数特征
- en: ❸ Identifies continuous features
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 识别连续特征
- en: ❹ Splits into training and test sets
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将数据集分为训练集和测试集
- en: ❺ Uses XGBoost as the ensemble method
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用 XGBoost 作为集成方法
- en: ❻ Rescales continuous features to the [0, 1] range
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将连续特征缩放到 [0, 1] 范围内
- en: ❼ One-hot encodes the low-cardinality features
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 对低基数特征进行独热编码
- en: ❽ Encodes high-cardinality features using dirty_cat encoding
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 使用 dirty_cat 编码对高基数特征进行编码
- en: ❾ Creates a preprocessing and training pipeline
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 创建预处理和训练管道
- en: ❿ Uses the R² score to evaluate overall performance
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 使用 R² 分数来评估整体性能
- en: 'In this example, we identify three different types of features, each of which
    we preprocess differently:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们识别了三种不同类型的特征，每种特征我们都会进行不同的预处理：
- en: Low-cardinality features, such as gender (2 categories) and department_name
    (37 categories), are one-hot encoded.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低基数特征，例如性别（2 个类别）和 department_name（37 个类别），进行独热编码。
- en: High-cardinality features, such as employee_position_title, are encoded using
    dirty_cat encoders.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高基数特征，例如 employee_position_title，使用 dirty_cat 编码器进行编码。
- en: Continuous features, such as year_first_hired, are rescaled using MinMaxScaler
    to be in the range 0 to 1.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续特征，例如 year_first_hired，使用 MinMaxScaler 缩放到 0 到 1 的范围内。
- en: 'After encoding, we train an XGBoost regressor with 100 trees each of maximum
    depth 3 using the fairly standard mean squared error (MSE) loss function. The
    trained models are evaluated using the regression metric *R*² score (see chapter
    1, section 1.3.1, for details), which ranges from -∞ to 1, with values closer
    to 1 indicating better-performing regressors:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码后，我们使用标准均方误差（MSE）损失函数训练了一个 XGBoost 回归器，包含 100 棵树，每棵树的最大深度为 3。训练好的模型使用回归指标
    *R*² 分数进行评估（详见第 1 章，第 1.3.1 节，以获取详细信息），该分数范围从 -∞ 到 1，数值越接近 1 表示回归器的性能越好：
- en: '[PRE45]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: As with the other supervised methods, it’s often necessary to use CV to determine
    which encoding parameters produce the best results for the data set at hand.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他监督方法一样，通常需要使用交叉验证（CV）来确定哪些编码参数能产生针对当前数据集的最佳结果。
- en: Summary
  id: totrans-407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A categorical feature is a type of data attribute that takes discrete values
    called classes or categories. For this reason, categorical features are also called
    discrete features.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类别特征是一种数据属性类型，它取离散值，称为类别或类别。因此，类别特征也被称为离散特征。
- en: A nominal feature is a categorical variable whose values have no relationship
    between them (e.g., cat, dog, pig, cow).
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 名义特征是一个没有彼此之间关系的值的类别变量（例如，猫，狗，猪，牛）。
- en: An ordinal feature is a categorical variable whose values are ordered, either
    increasing or decreasing (e.g., freshman, sophomore, junior, senior).
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有序特征是一个有序的类别变量，其值是按顺序排列的，要么是递增的，要么是递减的（例如，大一新生，大二，大三，大四）。
- en: One-hot vectorization/encoding and ordinal encoding are commonly used unsupervised
    encoding methods.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独热向量化/编码和有序编码是常用的无监督编码方法。
- en: One-hot encoding introduces binary (0-1) columns for each category into the
    data set and can be inefficient when a feature has a large number of categories.
    Ordinal encoding introduces integer values sequentially for each category.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独热编码为数据集中的每个类别引入了二进制（0-1）列，当特征具有大量类别时可能会效率低下。有序编码为每个类别按顺序引入整数值。
- en: Using target statistics is a supervised encoding approach for categorical features;
    rather than a predetermined or learned encoding step, categorical features are
    replaced with a statistic that describes the category (e.g., mean).
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用目标统计是针对类别特征的有监督编码方法；而不是一个预定的或学习到的编码步骤，类别特征被替换为一个描述类别的统计量（例如，平均值）。
- en: Greedy target statistics use all the training data for encoding, leading to
    train-to-test target leakage and distribution shift problems, which affect how
    we evaluate model generalization performance.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贪婪目标统计使用所有训练数据用于编码，导致训练到测试目标泄漏和分布偏移问题，这些问题会影响我们评估模型泛化性能的方式。
- en: Hold-out target statistics use a special hold-out encoding set in addition to
    a hold-out test set. This eliminates leakage and shift but is wasteful of data.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留法目标统计除了保留测试集外，还使用一个特殊的保留编码集。这消除了泄漏和偏移，但会浪费数据。
- en: Leave-one-out (LOO) target statistics and ordered target statistics are data-efficient
    ways to mitigate leakage and shift.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 留一法（LOO）目标统计和有序目标统计是数据高效地减轻泄漏和偏移的方法。
- en: Gradient-boosting techniques use training data for both residual computation
    and model training, which causes a prediction shift and overfitting.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升技术使用训练数据用于残差计算和模型训练，这会导致预测偏移和过拟合。
- en: Ordered boosting is a modification of Newton boosting that uses a permutation-based
    approach to ensembling to further reduce prediction shift. Ordered boosting tackles
    prediction shift by training a sequence of models on different permutations and
    subsets of the data.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有序提升是对牛顿提升的一种修改，它使用基于排列的集成方法来进一步减少预测偏移。有序提升通过在不同的排列和数据子集上训练一系列模型来解决预测偏移。
- en: CatBoost is a publicly available boosting library that implements ordered target
    statistics and ordered boosting.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CatBoost 是一个公开可用的提升库，实现了有序目标统计和有序提升。
- en: While CatBoost is well suited for categorical features, it can also be applied
    to regular features.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然 CatBoost 非常适合类别特征，但它也可以应用于常规特征。
- en: CatBoost uses oblivious decision trees as weak learners. Oblivious decision
    trees use the same splitting criterion in all the nodes across an entire level/depth
    of the tree. Oblivious trees are balanced, less prone to overfitting, and allow
    speeding up execution significantly at testing time.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CatBoost使用无知的决策树作为弱学习器。无知的决策树在整个树的整个层级/深度上的所有节点都使用相同的分割标准。无知的树是平衡的，不太容易过拟合，并且在测试时可以显著加快执行速度。
- en: High-cardinality features contain many unique categories; one-hot encoding high-cardinality
    features can introduce a large number of new data columns, most of them sparse
    (with many zeros), which leads to inefficient learning.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高基数特征包含许多独特的类别；对高基数特征进行独热编码会引入大量新的数据列，其中大部分是稀疏的（包含许多零），这会导致学习效率低下。
- en: dirty_cat is a package that produces more compact encodings for discrete-valued
    features and uses string and substring similarity and hashing to create effective
    encodings.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dirty_cat 是一个包，它为离散值特征生成更紧凑的编码，并使用字符串和子字符串相似性以及哈希来创建有效的编码。
