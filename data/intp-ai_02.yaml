- en: 1 Introduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 引言
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Different types of machine learning systems
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的机器学习系统类型
- en: How machine learning systems are built
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习系统的构建方式
- en: What interpretability is and its importance
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释性的定义及其重要性
- en: How interpretable machine learning systems are built
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何构建可解释的机器学习系统
- en: A summary of interpretability techniques covered in this book
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本书涵盖的可解释性技术总结
- en: Welcome to this book! I’m really happy that you are embarking on this journey
    through the world of *Interpretable AI*, and I look forward to being your guide.
    In the last five years alone, we have seen major breakthroughs in the field of
    artificial intelligence (AI), especially in areas such as image recognition, natural
    language understanding, and board games like Go. As AI augments critical human
    decisions in industries like healthcare and finance, it is becoming increasingly
    important that we build robust and unbiased machine learning models that drive
    these AI systems. In this book, I wish to give you a practical guide to interpretable
    AI systems and how to build them. Through a concrete example, this chapter will
    explain why interpretability is important and will lay the foundations for the
    rest of the book.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到这本书！我非常高兴您开始这段探索可解释人工智能世界的旅程，并期待成为您的向导。仅在过去的五年里，我们就见证了人工智能（AI）领域的重大突破，尤其是在图像识别、自然语言理解和围棋等棋类游戏等领域。随着人工智能在医疗保健和金融等行业增强关键的人类决策，构建稳健且无偏见的机器学习模型来驱动这些人工智能系统变得越来越重要。在这本书中，我希望为您提供关于可解释人工智能系统和如何构建它们的实用指南。通过一个具体的例子，本章将解释为什么可解释性很重要，并为本书的其余部分奠定基础。
- en: 1.1 Diagnostics+ AI—an example AI system
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 Diagnostics+ AI——一个示例人工智能系统
- en: Let’s now look at a concrete example of a healthcare center called Diagnostics+
    that provides a service to help diagnose different types of diseases. Doctors
    who work for Diagnostics+ analyze blood smear samples and provide their diagnoses,
    which can be either positive or negative. This current state of Diagnostics+ is
    shown in figure 1.1.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一个名为 Diagnostics+ 的医疗中心的具体例子，该中心提供诊断不同类型疾病的服务。为 Diagnostics+ 工作的医生分析血涂片样本并提供他们的诊断，这些诊断可以是阳性或阴性。Diagnostics+
    的当前状态如图 1.1 所示。
- en: '![](../Images/CH01_F01_Thampi.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH01_F01_Thampi.png)'
- en: Figure 1.1 Current state of Diagnostics+
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 Diagnostics+ 当前状态
- en: The problem with the current state is that the doctors are manually analyzing
    the blood smear samples. With a finite set of resources, diagnosis, therefore,
    takes a considerable amount of time. Diagnostics+ would like to automate this
    process using AI and diagnose more blood samples so that patients get the right
    treatment sooner. This future state is shown in figure 1.2.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当前状态的问题在于医生正在手动分析血涂片样本。由于资源有限，因此诊断需要相当长的时间。Diagnostics+ 希望使用人工智能自动化此过程，并诊断更多的血样，以便患者能够更快地获得正确的治疗。这种未来状态在图
    1.2 中展示。
- en: '![](../Images/CH01_F02_Thampi.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH01_F02_Thampi.png)'
- en: Figure 1.2 Future state of Diagnostics+
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 Diagnostics+ 的未来状态
- en: The goal for Diagnostics+ AI is to use images of blood smear samples with other
    patient metadata to provide diagnoses—positive, negative, or neutral—with a confidence
    measure. Diagnostics+ would also like to have doctors in the loop to review the
    diagnoses, especially the harder cases, thereby allowing the AI system to learn
    from mistakes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Diagnostics+ AI 的目标是使用血涂片样本的图像以及其他患者元数据来提供诊断——阳性、阴性或中性，并带有置信度度量。Diagnostics+
    还希望医生参与审查诊断，特别是更困难的案例，从而使人工智能系统能够从错误中学习。
- en: 1.2 Types of machine learning systems
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 机器学习系统类型
- en: 'We can use three broad classes of machine learning systems to drive Diagnostics+
    AI: supervised learning, unsupervised learning, and reinforcement learning.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用三类机器学习系统来驱动 Diagnostics+ AI：监督学习、无监督学习和强化学习。
- en: 1.2.1 Representation of data
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.1 数据表示
- en: Let’s first see how to represent the data that a machine learning system can
    understand. For Diagnostics+, we know that there’s historical data of blood smear
    samples in the form of images and patient metadata.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看如何表示机器学习系统可以理解的数据。对于 Diagnostics+，我们知道有血涂片样本的历史数据，以图像和患者元数据的形式存在。
- en: 'How do we best represent the image data? This is shown in figure 1.3\. Suppose
    the image of a blood smear sample is a colored image of size 256 × 256 pixels
    consisting of three primary channels: red (R), green (G), and blue (B). We can
    represent this RGB image in mathematical form as three matrices of pixel values,
    one for each channel and each of size 256 × 256. The three two-dimensional matrices
    can be combined into a multidimensional matrix of size 256 × 256 × 3 to represent
    the RGB image. In general, the dimension of the matrix representing an image is
    of the following form: *{number of pixels vertically}* × *{number of pixels horizontally}*
    × *{number of channels}*.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何最好地表示图像数据？这如图 1.3 所示。假设血涂片样本的图像是一个 256 × 256 像素的彩色图像，由三个主通道组成：红色（R）、绿色（G）和蓝色（B）。我们可以将这个
    RGB 图像以数学形式表示为三个像素值矩阵，每个通道一个，每个大小为 256 × 256。这三个二维矩阵可以组合成一个 256 × 256 × 3 的大小多维矩阵来表示
    RGB 图像。一般来说，表示图像的矩阵维度具有以下形式：*{垂直像素数}* × *{水平像素数}* × *{通道数}*。
- en: '![](../Images/CH01_F03_Thampi.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH01_F03_Thampi.png)'
- en: Figure 1.3 Representation of a blood smear sample image
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 血涂片样本图像表示
- en: Now, how do we best represent the patient metadata? Suppose that the metadata
    consists of information such as the patient identifier (ID), age, sex, and the
    final diagnosis. The metadata can be represented as a structured table, as shown
    in figure 1.4, with *N* columns and *M* rows. We can easily convert this tabular
    representation of the metadata into a matrix of dimension *M × N*. In figure 1.4,
    you can see that the Patient Id, Sex, and Diagnosis columns are categorical and
    have to be encoded as integers. For instance, the patient ID “AAABBCC” is encoded
    as integer 0, sex “M” (for male) is encoded as integer 0, and diagnosis “Positive”
    is encoded as integer 1.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何最好地表示患者元数据？假设元数据包括患者标识符（ID）、年龄、性别和最终诊断等信息。元数据可以表示为结构化表格，如图 1.4 所示，有 *N*
    列和 *M* 行。我们可以轻松地将元数据的表格表示转换为 *M × N* 维度的矩阵。在图 1.4 中，你可以看到，患者 ID、性别和诊断列是分类的，必须编码为整数。例如，患者
    ID “AAABBCC” 编码为整数 0，性别 “M”（男性）编码为整数 0，诊断 “Positive” 编码为整数 1。
- en: '![](../Images/CH01_F04_Thampi.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH01_F04_Thampi.png)'
- en: Figure 1.4 Representation of tabular patient metadata
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 表格形式的患者元数据表示
- en: 1.2.2 Supervised learning
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.2 监督学习
- en: The objective of supervised learning is to learn a mapping from an input to
    an output based on example input-output pairs. It requires labeled training data
    where inputs (also known as *features*) have a corresponding label (also known
    as a *target*). How is this data represented? The input features are typically
    represented using a multidimensional array data structure or mathematically as
    a matrix *X*. The output or target is represented as a single-dimensional array
    data structure or mathematically as a vector *y*. The dimension of matrix *X*
    is typically *m* × *n*, where *m* represents the number of examples or labeled
    data and *n* represents the number of features. The dimension of vector *y* is
    typically *m* × *1* where *m* again represents the number of examples or labels.
    The objective is to learn a function *f* that maps from input features *X* to
    the target *y*. This is shown in figure 1.5.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习的目标是根据示例输入输出对学习从输入到输出的映射。它需要标记的训练数据，其中输入（也称为*特征*）有一个相应的标签（也称为*目标*）。这些数据是如何表示的？输入特征通常使用多维数组数据结构表示，或者从数学上表示为矩阵
    *X*。输出或目标表示为单维数组数据结构，或者从数学上表示为向量 *y*。矩阵 *X* 的维度通常是 *m* × *n*，其中 *m* 代表示例或标记数据的数量，*n*
    代表特征的数量。向量 *y* 的维度通常是 *m* × *1*，其中 *m* 再次代表示例或标签的数量。目标是学习一个函数 *f*，它将输入特征 *X* 映射到目标
    *y*。这如图 1.5 所示。
- en: '![](../Images/CH01_F05_Thampi.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH01_F05_Thampi.png)'
- en: Figure 1.5 Illustration of supervised learning
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5 监督学习说明
- en: In figure 1.5, you can see that with supervised learning, you are learning a
    function *f* that takes in multiple input features represented as *X* and provides
    an output that matches known labels or values, represented as the target variable
    *y*. The bottom half of the figure shows an example where a labeled dataset is
    given, and through supervised learning, you are learning how to map the input
    features to the output.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 1.5 中，你可以看到，通过监督学习，你正在学习一个函数 *f*，它接受多个以 *X* 表示的输入特征，并提供一个与已知标签或值匹配的输出，表示为目标变量
    *y*。图的下半部分显示了一个示例，其中提供了一个标记数据集，通过监督学习，你正在学习如何将输入特征映射到输出。
- en: 'The function *f* is a multivariate function—it maps from multiple input variables
    or features to a target. Two broad classes of supervised learning problems follow:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 *f* 是一个多元函数——它将多个输入变量或特征映射到一个目标。以下是无监督学习问题的两个广泛类别：
- en: '*Regression*—The target vector *y* is continuous. For example, predicting the
    price of a house at a location in U.S. dollars is a regression type of learning
    problem.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*回归*—目标向量 *y* 是连续的。例如，预测美国某地点的房价（以美元计）是一个回归类型的学习问题。'
- en: '*Classification*—The target variable *y* is discrete and bounded. For example,
    predicting whether or not an email is spam is a classification type of learning
    problem.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类*—目标变量 *y* 是离散且有界的。例如，预测一封电子邮件是否为垃圾邮件是一个分类学习问题。'
- en: 1.2.3 Unsupervised learning
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.3 无监督学习
- en: In unsupervised learning, the objective is to learn a representation of the
    data that best describes it. There is no labeled data, and the goal is to learn
    some unknown pattern from the raw data. The input features are represented as
    a matrix *X*, and the system learns a function *f* that maps from *X* to a pattern
    or representation of the input data. This is depicted in figure 1.6\. An example
    of unsupervised learning is clustering, where the goal is to form groups or clusters
    of data points with similar properties or characteristics. This is shown in the
    bottom half of the figure. The unlabeled data consists of two features and the
    datapoints are shown in 2-D space. There are no known labels, and the objective
    of an unsupervised learning system is to learn latent patterns present in the
    data. In this illustration, the system learns how to map the raw data points into
    clusters based on their proximity or similarity to each other. These clusters
    are not known beforehand because the dataset is unlabeled and, hence the learning
    is entirely unsupervised.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，目标是学习数据的最佳表示，以最好地描述它。没有标记的数据，目标是学习从原始数据中的一些未知模式。输入特征表示为矩阵 *X*，系统学习一个函数
    *f*，它将 *X* 映射到输入数据的模式或表示。这如图1.6所示。无监督学习的一个例子是聚类，其目标是形成具有相似属性或特征的数据点组或聚类。这显示在图的下半部分。未标记的数据包含两个特征，数据点在二维空间中显示。没有已知的标签，无监督学习系统的目标是学习数据中存在的潜在模式。在这个示例中，系统学习如何根据数据点之间的接近度或相似性将原始数据点映射到聚类中。这些聚类事先是未知的，因为数据集未标记，因此学习是完全无监督的。
- en: '![](../Images/CH01_F06_Thampi.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH01_F06_Thampi.png)'
- en: Figure 1.6 Illustration of unsupervised learning
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 无监督学习示意图
- en: 1.2.4 Reinforcement learning
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.4 强化学习
- en: Reinforcement learning consists of an agent that learns by interacting with
    an environment, as shown in figure 1.7\. The learning agent takes an action within
    the environment and receives a reward or penalty, depending on the quality of
    the action. Based on the action taken, the agent moves from one state to another.
    The overall objective of the agent is to maximize the cumulative reward by learning
    a policy function *f* that maps from an input state to an action. Some examples
    of reinforcement learning are a robot vacuum cleaner learning the best path to
    take to clean a home and an artificial agent learning how to play board games
    like chess and Go.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习由一个通过与环境交互来学习的代理组成，如图1.7所示。学习代理在环境中采取一个动作，并根据动作的质量获得奖励或惩罚。根据采取的动作，代理从一个状态移动到另一个状态。代理的整体目标是通过对从输入状态到动作映射的策略函数
    *f* 进行学习，以最大化累积奖励。强化学习的例子包括机器人吸尘器学习最佳清洁路径以清洁家庭，以及人工代理学习如何玩象棋和国际象棋等棋类游戏。
- en: '![](../Images/CH01_F07_Thampi.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH01_F07_Thampi.png)'
- en: Figure 1.7 An illustration of reinforcement learning
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7 强化学习示意图
- en: 'The bottom half of figure 1.7 illustrates a reinforcement learning system.
    The system consists of a robot (agent) in a maze (environment). The objective
    of the learning agent is to determine the optimum set of actions to take so that
    it can move from its current location to the finishing line (end state), indicated
    by the green star. The agent can take one of four actions: move left, right, up,
    or down.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7的下半部分展示了强化学习系统。该系统由一个机器人（代理）在一个迷宫（环境）中组成。学习代理的目标是确定最佳的动作集，以便它能从当前位置移动到终点线（最终状态），终点线由绿色星号指示。代理可以采取四种动作之一：向左、向右、向上或向下。
- en: 1.2.5 Machine learning system for Diagnostics+ AI
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.5 用于诊断+ AI的机器学习系统
- en: Now that you know the three broad types of machine learning systems, which system
    is most applicable for Diagnostics+ AI? Given that the dataset is labeled, and
    you know from historical data what diagnosis was made for a patient and blood
    sample, the machine learning system that can be used to drive Diagnostics+ AI
    is *supervised learning*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经知道了三种主要的机器学习系统类型，那么对于诊断+ AI，哪种系统最适用？鉴于数据集已标记，并且你可以从历史数据中知道对患者的诊断以及血液样本的诊断，可以用来驱动诊断+
    AI 的机器学习系统是 *监督学习*。
- en: What class of supervised learning problem is it? The target for the supervised
    learning problem is the diagnosis, which can be either positive or negative. Because
    the target is discrete and bounded, it is a *classification* type of learning
    problem.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是哪种监督学习问题？监督学习问题的目标是诊断，可以是阳性也可以是阴性。因为目标是离散且有界的，所以这是一个 *分类* 类型的学习问题。
- en: Primary focus of the book
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的主要关注点
- en: This book primarily focuses on *supervised learning* systems where labeled data
    is present. I will teach you how to implement interpretability techniques for
    both regression and classification types of problems. Although this book does
    not explicitly cover unsupervised learning or reinforcement learning systems,
    the techniques learned in this book can be extended to them.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本书主要关注存在标记数据的 *监督学习* 系统。我将教你如何实现回归和分类类型问题的可解释性技术。尽管本书没有明确涵盖无监督学习或强化学习系统，但本书中学到的技术可以扩展到它们。
- en: 1.3 Building Diagnostics+ AI
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 构建诊断+ AI
- en: 'Now that we’ve identified that Diagnostics+ AI is going to be a supervised
    learning system, how do we go about building it? The typical process goes through
    three main phases:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经确定诊断+ AI 将是一个监督学习系统，我们如何着手构建它？典型的过程包括三个主要阶段：
- en: Learning
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习
- en: Testing
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试
- en: Deploying
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署
- en: In the learning phase, illustrated in figure 1.8, we are in the development
    environment, where we use two subsets of the data called the training set and
    the dev set. As the name suggests, the training set is used to train a machine
    learning model to learn the mapping function *f* from the input features *X* (in
    this case, the image of the blood sample and metadata) to the target *y* (in this
    case, the diagnosis). Once we’ve trained the model, we use the dev set for validation
    purposes and tune the model based on the performance on that dev set. Tuning the
    model entails determining the optimum parameters for the model, called *hyperparameters*,
    that give the best performance. This is quite an iterative process, and we continue
    doing this until the model reaches an acceptable level of performance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习阶段，如图 1.8 所示，我们处于开发环境，在那里我们使用数据的两个子集，称为训练集和开发集。正如其名所示，训练集用于训练机器学习模型，学习从输入特征
    *X*（在这种情况下，血液样本的图像和元数据）到目标 *y*（在这种情况下，诊断）的映射函数 *f*。一旦我们训练了模型，我们就使用开发集进行验证，并根据该开发集上的性能调整模型。调整模型包括确定模型的最佳参数，称为
    *超参数*，以获得最佳性能。这是一个相当迭代的过程，我们继续这样做，直到模型达到可接受的性能水平。
- en: '![](../Images/CH01_F08_Thampi.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH01_F08_Thampi.png)'
- en: Figure 1.8 Process of building an AI system—learning phase
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8 构建AI系统流程——学习阶段
- en: In the testing phase, illustrated in figure 1.9, we now switch over to the test
    environment where we use a subset of the data called the *test set*, which is
    different from the training set. The objective is to obtain an unbiased assessment
    of the accuracy of the model. Stakeholders and experts (in this case, doctors)
    would at this point evaluate the functionality of the system and performance of
    the model on the test set. This additional testing, called user acceptance testing
    (UAT), is the final stage in the development of any software system. If the performance
    is not acceptable, then we go back to phase 1 to train a better model. If the
    performance is acceptable, then we move on to phase 3, which is deploying.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试阶段，如图 1.9 所示，我们现在切换到测试环境，在那里我们使用数据的一个子集，称为 *测试集*，它与训练集不同。目标是获得模型准确性的无偏评估。利益相关者和专家（在这种情况下，医生）将在此阶段评估系统的功能以及模型在测试集上的性能。这种额外的测试，称为用户验收测试（UAT），是任何软件系统开发的最终阶段。如果性能不可接受，那么我们就回到第
    1 阶段去训练一个更好的模型。如果性能可接受，那么我们就进入第 3 阶段，即部署。
- en: '![](../Images/CH01_F09_Thampi.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH01_F09_Thampi.png)'
- en: Figure 1.9 Process of building an AI system—testing phase
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9 构建AI系统流程——测试阶段
- en: Finally, in the deploying phase, we now deploy the learned model into the production
    system where the model is now exposed to new data that it hasn’t seen before.
    The complete process is illustrated in figure 1.10\. In the case of Diagnostics+
    AI, this data would be new blood samples and patient information that the model
    will use to predict whether the diagnosis is positive or negative with a confidence
    measure. This information is then consumed by the expert (the doctor) and in turn
    the end user (the patient).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在部署阶段，我们现在将学习到的模型部署到生产系统中，模型现在将暴露于它之前未见过的新的数据。整个过程如图1.10所示。在Diagnostics+
    AI的情况下，这些数据将是模型将用于预测诊断结果为阳性或阴性的新血液样本和患者信息，并附带置信度度量。然后，这些信息被专家（医生）消费，进而被最终用户（患者）消费。
- en: '![](../Images/CH01_F10_Thampi.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH01_F10_Thampi.png)'
- en: Figure 1.10 Process of building an AI system—complete
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10 构建AI系统的过程——完整
- en: 1.4 Gaps in Diagnostics+ AI
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 Diagnostics+ AI的差距
- en: 'Figure 1.10 shows some major gaps in the Diagnostics+ AI system. This AI system
    does not safeguard against some common issues for which the deployed model does
    not behave as expected in the production environment. These issues could have
    a detrimental effect on the business of the diagnostics center. The common issues
    follow:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10显示了Diagnostics+ AI系统的一些主要差距。这个AI系统无法防止一些常见问题，这些问题会导致部署的模型在生产环境中表现不符合预期。这些问题可能会对诊断中心的业务产生有害影响。常见问题如下：
- en: Data leakage
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据泄露
- en: Bias
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差
- en: Regulatory noncompliance
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监管不合规
- en: Concept drift
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概念漂移
- en: 1.4.1 Data leakage
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.1 数据泄露
- en: Data leakage happens when features in the training, dev, and test sets unintentionally
    leak information that would otherwise not appear in the production environment
    when the model is scored on new data. For Diagnostics+, suppose we use notes made
    by the doctor about the diagnosis as a feature or input for our model. While evaluating
    the model using the test set, we could get inflated performance results, thereby
    tricking ourselves into thinking we’ve built a great model. The notes made by
    the doctor could contain information about the final diagnosis, which would leak
    information about the target variable. This problem, if not detected earlier,
    could be catastrophic once the model is deployed into production—the model is
    scored before the doctor has had a chance to review the diagnosis and add their
    notes. Therefore, the model would either crash in production because the feature
    is missing or would start to make poor diagnoses.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练、开发和测试集中的特征无意中泄露了在生产环境中模型评分新数据时不会出现的信息时，就会发生数据泄露。对于Diagnostics+，假设我们使用医生关于诊断的笔记作为模型的特征或输入。在用测试集评估模型时，我们可能会得到夸大的性能结果，从而欺骗自己认为我们已经构建了一个优秀的模型。医生的笔记可能包含关于最终诊断的信息，这将泄露关于目标变量的信息。如果这个问题没有及早被发现，一旦模型部署到生产环境中，可能会造成灾难性的后果——模型在医生有机会审查诊断并添加笔记之前就已经进行了评分。因此，模型要么在生产中崩溃（因为缺少特征），要么开始做出糟糕的诊断。
- en: A classic case study of data leakage is the KDD Cup Challenge ([https://www.kdd.org/kdd-cup/view/kdd-cup-2008](https://www.kdd.org/kdd-cup/view/kdd-cup-2008))
    of 2008\. The objective of this machine learning competition based on real data
    was to detect whether a breast cancer cell was benign or malignant based on X-ray
    images. A study ([http://kdd.org/exploration_files/KDDCup08-P1.pdf](http://kdd.org/exploration_files/KDDCup08-P1.pdf))
    showed that teams that scored the most on the test set for this competition used
    a feature called Patient ID, which was an identifier generated by the hospital
    for the patient. It turned out that some hospitals used the patient ID to indicate
    the severity of the condition of the patient when they were admitted to the hospital,
    which, therefore, leaked information about the target variable.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 数据泄露的一个经典案例研究是2008年的KDD Cup挑战赛([https://www.kdd.org/kdd-cup/view/kdd-cup-2008](https://www.kdd.org/kdd-cup/view/kdd-cup-2008))。这个基于真实数据的机器学习竞赛的目标是根据X光图像检测乳腺癌细胞是良性还是恶性。一项研究([http://kdd.org/exploration_files/KDDCup08-P1.pdf](http://kdd.org/exploration_files/KDDCup08-P1.pdf))表明，在这次竞赛的测试集中得分最高的团队使用了一个名为“患者ID”的特征，这是医院为患者生成的标识符。结果发现，一些医院使用患者ID来表示患者在入院时的病情严重程度，因此泄露了关于目标变量的信息。
- en: 1.4.2 Bias
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.2 偏差
- en: Bias is when the machine learning model makes an unfair prediction that favors
    one person or group over another. This unfair prediction could be caused by the
    data or the model itself. There may be sampling biases in which systematic differences
    exist between the data sample used for training and the population. Systemic social
    biases, which the model picks up on, may also be inherent in the data. The trained
    model could also be flawed—it may have some strong preconceptions despite evidence
    to the contrary. For the case of Diagnostics+ AI, if there is sampling bias, for
    instance, the model could make more accurate predictions for one group and not
    generalize well to the whole population. This is far from ideal because the diagnostics
    center wants the new AI system to be used for every patient, regardless of which
    group they belong to.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差是指机器学习模型做出不公平的预测，这种预测偏向于某个人或群体，而损害了另一个人或群体。这种不公平的预测可能是由于数据或模型本身造成的。可能存在采样偏差，即用于训练的数据样本与总体之间存在系统性差异。模型所捕捉到的系统性社会偏差也可能存在于数据中。训练好的模型也可能存在缺陷——即使有相反的证据，它也可能有一些强烈的先入之见。对于Diagnostics+
    AI的情况，如果存在采样偏差，例如，模型可能对某一群体做出更准确的预测，但不能很好地推广到整个总体。这远远不是理想的，因为诊断中心希望新的AI系统能够为所有患者使用，无论他们属于哪个群体。
- en: A classic case study of machine bias is the COMPAS AI system used by U.S. courts
    to predict future criminals. The study was conducted by ProPublica ([http://mng.bz/7Ww4](http://mng.bz/7Ww4)).
    (The webpage contains links to the analysis and dataset.) ProPublica obtained
    the COMPAS scores for 7,000 people who had been arrested in a county in Florida
    in 2013 and 2014\. Using the scores, they found out that they could not accurately
    predict the recidivism rate (i.e., the rate at which a convicted person reoffends)—only
    20% of the people who were predicted to commit violent crimes actually did so.
    More importantly, they uncovered serious racial biases in the model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 机器偏差的一个经典案例研究是美国法院使用的COMPAS AI系统，用于预测未来的罪犯。这项研究由ProPublica进行。（网页包含分析数据和数据集的链接。）ProPublica获得了2013年和2014年在佛罗里达州一个县被捕的7,000人的COMPAS评分。使用这些评分，他们发现无法准确预测再犯率（即被定罪的人再次犯罪的比例）——预测会犯暴力犯罪的人中只有20%实际上犯了罪。更重要的是，他们发现了模型中严重的种族偏见。
- en: 1.4.3 Regulatory noncompliance
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.3 规章制度不遵守
- en: The General Data Protection Regulation (GDPR; [https://gdpr.eu/](https://gdpr.eu/))
    is a comprehensive set of regulations adopted by the European Parliament in 2016
    that deals with how data is collected, stored, and processed by foreign companies.
    The regulation contains article 17 ([https://gdpr-info.eu/art-17-gdpr/](https://gdpr-info.eu/art-17-gdpr/))—the
    “right to be forgotten”—where individuals can request a company collecting their
    data to erase all their personal data. The regulation also contains article 22
    ([https://gdpr-info.eu/art-22-gdpr/](https://gdpr-info.eu/art-22-gdpr/)), under
    which individuals can challenge decisions made by an algorithm or AI system using
    their personal data. This regulation presses the need for providing an interpretation
    or explanation for why the algorithm made a particular decision. The current Diagnostics+
    AI system does not comply with both sets of regulations. In this book, we are
    more concerned with article 22 because there are a lot of online resources on
    how to be compliant with article 17.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 《通用数据保护条例》（GDPR；[https://gdpr.eu/](https://gdpr.eu/））是欧洲议会于2016年通过的一套全面法规，涉及外国公司如何收集、存储和处理数据。该法规包含第17条（[https://gdpr-info.eu/art-17-gdpr/](https://gdpr-info.eu/art-17-gdpr/））——“被遗忘权”，个人可以要求收集其数据的公司删除其所有个人数据。该法规还包含第22条（[https://gdpr-info.eu/art-22-gdpr/](https://gdpr-info.eu/art-22-gdpr/）），个人可以挑战基于其个人数据做出的算法或AI系统的决策。该法规强调了提供解释或说明为什么算法做出了特定决策的需要。当前的Diagnostics+
    AI系统不符合这两套法规。在这本书中，我们更关注第22条，因为关于如何遵守第17条有很多在线资源。
- en: 1.4.4 Concept drift
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.4 概念漂移
- en: Concept drift happens when the properties or the distribution of the data in
    a production environment has changed when compared to the historical data used
    to train and evaluate the model. For Diagnostics+ AI, this could happen if new
    profiles of patients or diseases emerge that aren’t captured in the historical
    data. When concept drift happens, we observe a dip in the performance of the machine
    learning model in production over time. The current Diagnostics+ AI system does
    not properly deal with concept drift.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当生产环境中的数据属性或分布与用于训练和评估模型的原始数据相比发生变化时，就会发生概念漂移。对于 Diagnostics+ AI 来说，如果出现新的患者或疾病档案，而这些档案没有包含在原始数据中，就可能会发生这种情况。当发生概念漂移时，我们会观察到机器学习模型在生产环境中的性能随时间下降。当前的
    Diagnostics+ AI 系统并没有妥善处理概念漂移问题。
- en: 1.5 Building a robust Diagnostics+ AI system
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 构建鲁棒 Diagnostics+ AI 系统
- en: How do we address all the gaps highlighted in section 1.4 and build a robust
    Diagnostics+ AI system? We need to tweak the process. First, as shown in figure
    1.11, we add a model understanding phase after the testing phase and before deploying.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解决 1.4 节中提到的所有差距，并构建一个鲁棒的 Diagnostics+ AI 系统？我们需要调整这个过程。首先，如图 1.11 所示，我们在测试阶段之后、部署之前添加一个模型理解阶段。
- en: '![](../Images/CH01_F11_Thampi.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH01_F11_Thampi.png)'
- en: Figure 1.11 Process of building a robust AI system—understanding phase
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.11 构建鲁棒 AI 系统的过程——理解阶段
- en: The purpose of this new *understanding* phase is to answer the important how
    question—how did the model come up with a positive diagnosis for a given blood
    sample? This involves interpreting the important features for the model and how
    they interact with each other, interpreting what patterns the model learned, understanding
    the blind spots, checking for bias in the data, and ensuring those biases are
    not propagated by the model. This understanding phase should ensure that the AI
    system is safeguarded against the data leakage and bias issues highlighted in
    sections 1.4.1 and 1.4.2, respectively.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新的 *理解* 阶段的目的在于回答重要的“如何”问题——模型是如何对一个特定的血液样本给出阳性诊断的？这涉及到解释模型的重要特征以及它们是如何相互作用的，解释模型学习到的模式，理解盲点，检查数据中的偏差，并确保这些偏差不会被模型传播。这个理解阶段应确保
    AI 系统免受 1.4.1 节和 1.4.2 节中提到的数据泄露和偏差问题的影响。
- en: The second change is to add an *explaining* phase after deploying, as shown
    in figure 1.12\. The purpose of the explaining phase is to interpret how the model
    came up with the prediction on new data in the production environment. Interpreting
    the prediction on new data allows us to expose that information, if needed, to
    expert users of the system who challenge the decision made by the deployed model.
    Another purpose is to come up with a human-readable explanation so that it can
    be exposed to wider end users of the AI system. By including the interpretation
    step, we will be able to address the regulatory noncompliance issue highlighted
    in section 1.4.3.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个变化是在部署之后添加一个 *解释* 阶段，如图 1.12 所示。解释阶段的目的在于解释模型是如何在生产环境中对新的数据进行预测的。对新数据的预测解释使我们能够（如果需要的话）将信息暴露给挑战部署模型决策的系统专家用户。另一个目的是提供一个可读的解释，以便它可以被暴露给更广泛的
    AI 系统最终用户。通过包括解释步骤，我们将能够解决 1.4.3 节中提到的监管不合规问题。
- en: '![](../Images/CH01_F12_Thampi.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH01_F12_Thampi.png)'
- en: Figure 1.12 Process of building a robust AI—explaining phase
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12 构建鲁棒 AI 的过程——解释阶段
- en: Finally, to address the concept drift issue highlighted in section 1.4.4, we
    need to add a *monitoring* phase in the production environment. This complete
    process is shown in figure 1.13\. The purpose of the monitoring phase is to track
    the distribution of the data in the production environment as well as the performance
    of the deployed model. If any change occurs in data distribution or model performance
    dips, we will need to go back to the learning phase and incorporate the new data
    from the production environment to retrain the models.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了解决 1.4.4 节中提到的概念漂移问题，我们需要在生产环境中添加一个 *监控* 阶段。这个完整的过程如图 1.13 所示。监控阶段的目的在于跟踪生产环境中数据的分布以及部署模型的性能。如果数据分布或模型性能出现任何变化，我们需要回到学习阶段，并将生产环境中的新数据纳入以重新训练模型。
- en: Primary focus of the book
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的主要关注点
- en: This book primarily focuses on the *interpretation* step in the understanding
    and explaining phases. I intend to teach you various interpretability techniques
    that you can apply to answer the important how question and address the data leakage,
    bias, and regulatory noncompliance issues. Although explainability and monitoring
    are important steps in the process, they are not the primary focus of this book.
    It is also important to distinguish between interpretability and explainability.
    This is addressed in the following section.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 本书主要关注理解和解释阶段中的 *解释* 步骤。我打算教你各种可解释性技术，你可以应用这些技术来回答重要的“如何”问题，并解决数据泄露、偏差和监管不合规问题。尽管可解释性和监控是过程中的重要步骤，但它们不是本书的主要焦点。区分可解释性和可解释性也很重要。这将在以下部分中讨论。
- en: '![](../Images/CH01_F13_Thampi.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH01_F13_Thampi.png)'
- en: Figure 1.13 Process of building a robust AI system—complete
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.13 构建稳健 AI 系统的过程——完整
- en: 1.6 Interpretability vs. explainability
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6 可解释性 vs. 可解释性
- en: '*Interpretability* and *explainability* are sometimes used interchangeably,
    but it is important to make a distinction between the two terms.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*可解释性* 和 *可解释性* 有时被互换使用，但区分这两个术语很重要。'
- en: '*Interpretability* is all about understanding the cause and effect within an
    AI system. It is the degree to which we can consistently estimate what a model
    will predict given an input, understand how the model came up with the prediction,
    understand how the prediction changes with changes in the input or algorithmic
    parameters, and finally, understand when the model has made a mistake. Interpretability
    is mostly discernible by experts who are building, deploying, or using the AI
    system, and these techniques are building blocks that will help us get to explainability.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*可解释性* 主要是关于理解 AI 系统内部的因果关系。这是我们在给定输入的情况下，可以持续估计模型将预测什么，理解模型是如何得出预测的，理解预测如何随着输入或算法参数的变化而变化，以及最后理解模型何时犯错的程度。可解释性主要是由构建、部署或使用
    AI 系统的专家可辨别的，这些技术是帮助我们达到可解释性的基石。'
- en: '*Explainability*, on the other hand, goes beyond interpretability in that it
    helps us understand in a human-readable form how and why a model came up with
    a prediction. It explains the internal mechanics of the system in human terms,
    with the intent to reach a much wider audience. Explainability requires interpretability
    as a building block and also looks to other fields and areas, such as human-computer
    interaction (HCI), law, and ethics. In this book, I will focus more on interpretability
    and less on explainability. We have a lot to cover within interpretability itself,
    but it should give you a solid foundation to be able to later build an explainable
    AI system.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，*可解释性* 超出了可解释性的范畴，因为它帮助我们以人类可读的形式理解模型是如何以及为什么做出预测的。它用人类术语解释系统的内部机制，目的是达到更广泛的受众。可解释性需要可解释性作为基石，并参考其他领域和领域，如人机交互（HCI）、法律和伦理。在本书中，我将更多地关注可解释性，而不是可解释性。在可解释性本身就有很多要涵盖的内容，但它应该为你提供一个坚实的基础，以便你能够后来构建一个可解释的
    AI 系统。
- en: 'You should be aware of four different personas when you consider interpretability.
    They are the *data scientist* or *engineer* who is building the AI system, the
    *business stakeholder* who wants to deploy the AI system for their business, the
    *end user* of the AI system, and finally the *expert* or *regulator* who monitors
    or audits the health of the AI system. Note that interpretability means different
    things to these four personas, as described next:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当你考虑可解释性时，你应该意识到四种不同的角色。他们是构建 AI 系统的 *数据科学家* 或 *工程师*，希望为他们的业务部署 AI 系统的 *商业利益相关者*，AI
    系统的 *最终用户*，以及最后是监控或审计 AI 系统健康状况的 *专家* 或 *监管者*。请注意，可解释性对这四个角色意味着不同的事情，如下所述：
- en: For a *data scientist* or *engineer*, it means gaining a deeper understanding
    of how the model made a particular prediction, which features are important, and
    how to debug issues by analyzing cases where the model did badly. This understanding
    helps the data scientist build more robust models.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于一个 *数据科学家* 或 *工程师* 来说，这意味着更深入地理解模型如何做出特定的预测，哪些特征是重要的，以及如何通过分析模型表现不佳的案例来调试问题。这种理解有助于数据科学家构建更稳健的模型。
- en: For a *business stakeholder*, it means understanding how the model made a decision
    so as to ensure fairness and protect the business’s users and brand.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 *商业利益相关者* 来说，这意味着理解模型如何做出决策，以确保公平性并保护企业的用户和品牌。
- en: For an *end user*, it means understanding how the model made a decision and
    allowing for meaningful challenge if the model made a mistake.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于*最终用户*来说，这意味着理解模型是如何做出决策的，并在模型出错时允许进行有意义的挑战。
- en: For an *expert* or *regulator*, it means auditing the model and the AI system
    and following the decision trail, especially when things went wrong.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于*专家*或*监管者*来说，这意味着审计模型和AI系统，并追踪决策路径，尤其是在事情出错的时候。
- en: 1.6.1 Types of interpretability techniques
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.6.1 可解释性技术类型
- en: Figure 1.14 summarizes various types of interpretability techniques. *Intrinsic*
    interpretability techniques are related to machine learning models with a simple
    structure, also called *white-box models*. White-box models are inherently transparent,
    and interpreting the internals of the model is straightforward. Interpretability
    comes right out of the box for such models. *Post hoc* interpretability techniques
    are usually applied after model training and are used to interpret and understand
    the importance of certain inputs for the model prediction. Post hoc interpretability
    techniques are suited for white-box and black-box models, that is, models that
    are not inherently transparent.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.14总结了各种可解释性技术的类型。*内禀*可解释性技术是与结构简单的机器学习模型相关的，也称为*白盒模型*。白盒模型本质上是透明的，解释模型的内部结构是直接的。对于这类模型，可解释性是直接呈现的。“后验”可解释性技术通常在模型训练之后应用，用于解释和理解模型预测中某些输入的重要性。后验可解释性技术适用于白盒和黑盒模型，即那些本质上不透明的模型。
- en: '![](../Images/CH01_F14_Thampi.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH01_F14_Thampi.png)'
- en: Figure 1.14 Types of interpretability techniques
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.14 可解释性技术类型
- en: Interpretability techniques can also be model-specific or model-agnostic. *Model-specific*
    interpretability techniques, as the name suggests, can be applied only to certain
    types of models. Intrinsic interpretability techniques are model-specific by nature
    because the technique is tied to the specific structure of the model being used.
    *Model-agnostic* interpretability techniques, however, are not dependent on the
    specific type of model being used. They can be applied to any model because they
    are independent of the internal structure of the model. Post-hoc interpretability
    techniques are mostly model-agnostic by nature.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性技术也可以是模型特定的或模型无关的。*模型特定*的可解释性技术，正如其名所示，只能应用于某些类型的模型。内禀可解释性技术本质上就是模型特定的，因为这种技术是与所使用的模型的特定结构紧密相连的。然而，*模型无关*的可解释性技术并不依赖于所使用的特定模型类型。由于它们独立于模型的内部结构，因此可以应用于任何模型。后验可解释性技术本质上大多是模型无关的。
- en: Interpretability techniques can also be local or global in scope. *Local* interpretability
    techniques aim to give a better understanding of the model prediction for a specific
    instance or example. *Global* interpretability techniques, on the other hand,
    aim to give a better understanding of the model as a whole—the global effects
    of the input features on the model prediction. We cover all of these types of
    techniques in this book. Now let’s take a look at what specifically you will learn.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性技术也可以在范围上是局部的或全局的。*局部*可解释性技术旨在对特定实例或示例的模型预测有更好的理解。另一方面，*全局*可解释性技术旨在对模型整体有更好的理解——输入特征对模型预测的总体影响。我们在本书中涵盖了所有这些类型的技巧。现在让我们看看你将具体学到什么。
- en: 1.7 What will I learn in this book?
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.7 我在这本书中将学到什么？
- en: Figure 1.15 depicts a map of all the interpretability techniques you will learn
    in this book. When interpreting supervised learning models, it is important to
    distinguish between white-box and black-box models. Examples of white-box models
    are linear regression, logistic regression, decision trees, and generalized additive
    models (GAMs). Examples of black-box models include tree ensembles, like random
    forest and boosted trees, and neural networks. White-box models are much easier
    to interpret than black-box models. On the other hand, black-box models have much
    higher predictive power than white-box models. So, we need to make a trade-off
    between predictive power and interpretability. It is important to understand the
    scenarios in which we can apply white-box and black-box models.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.15展示了本书中将学习到的所有可解释性技术地图。在解释监督学习模型时，区分白盒模型和黑盒模型非常重要。白盒模型的例子包括线性回归、逻辑回归、决策树和广义加性模型（GAMs）。黑盒模型的例子包括树集成，如随机森林和提升树，以及神经网络。白盒模型比黑盒模型更容易解释。另一方面，黑盒模型的预测能力比白盒模型高得多。因此，我们需要在预测能力和可解释性之间做出权衡。了解我们可以应用白盒模型和黑盒模型的场景非常重要。
- en: In chapter 2, you’ll learn about characteristics that make white-box models
    inherently transparent and black-box models inherently opaque. You’ll learn how
    to interpret simple white-box models, such as linear regression and decision trees,
    and then we’ll switch gears to focus on GAMs. GAMs have high predictive power
    and are highly interpretable, too, so they offer more bang for the buck than GAMs.
    You’ll learn about the properties that give GAMs power and how to interpret them.
    At the time of writing, there are not a lot of practical resources on GAMs to
    give a good understanding of the internals of the model and how to interpret them.
    To address this gap, we pay a lot of attention to GAMs in chapter 2\. The rest
    of the chapters focus on black-box models.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2章中，你将了解使白盒模型本质上是透明的和黑盒模型本质上是模糊的特性。你将学习如何解释简单的白盒模型，例如线性回归和决策树，然后我们将转换方向，专注于广义加性模型（GAMs）。GAMs具有高度的预测能力，并且也是高度可解释的，因此它们比GAMs提供了更多的价值。你将了解赋予GAMs力量的属性以及如何解释它们。在撰写本文时，关于GAMs的实用资源不多，难以对模型内部结构和如何解释它们有一个好的理解。为了填补这一空白，我们在第2章中投入了大量关注GAMs。其余章节则专注于黑盒模型。
- en: We can interpret black-box models in two ways. One way is to interpret model
    processing, that is, to understand how the model processes the inputs and arrives
    at the final prediction. Chapters 3 to 5 focus on interpreting model processing.
    The other way is to interpret model representations, which is applicable only
    to deep neural networks. Chapters 6 and 7 focus on interpreting model representations
    with the goal of understanding what features or patterns have been learned by
    the neural network.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过两种方式来解释黑盒模型。一种方式是解释模型处理过程，即理解模型如何处理输入并得出最终预测。第3章到第5章专注于解释模型处理过程。另一种方式是解释模型表示，这仅适用于深度神经网络。第6章和第7章专注于通过解释模型表示来理解神经网络学习到的特征或模式。目标是理解神经网络学习到的特征或模式。
- en: '![](../Images/CH01_F15_Thampi.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图1.15](../Images/CH01_F15_Thampi.png)'
- en: Figure 1.15 Map of interpretability techniques covered in this book
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.15本书涵盖的可解释性技术地图
- en: In chapter 3, we focus on a class of black-box models called tree ensembles.
    You’ll learn about their characteristics and what makes them “black box.” You
    will also learn how to interpret them using post hoc model-agnostic methods that
    are global in scope. We will focus specifically on partial dependence plots (PDPs),
    individual conditional expectation (ICE) plots, and feature interaction plots.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我们关注一类称为树集成（tree ensembles）的黑盒模型。你将了解它们的特征以及它们为什么被称为“黑盒”。你还将学习如何使用全局范围内的模型无关的后验方法来解释它们。我们将特别关注局部依赖性图（PDPs）、个体条件期望（ICE）图和特征交互图。
- en: In chapter 4, we focus on deep neural networks, specifically the vanilla fully
    connected neural networks. You’ll learn about characteristics that make these
    models black box and also how to interpret them using post hoc model-agnostic
    methods that are local in scope. You’ll specifically learn about techniques such
    as local interpretable model- agnostic explanations (LIME), SHapley Additive exPlanations
    (SHAP), and anchors.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章中，我们关注深度神经网络，特别是纯全连接神经网络。你将了解使这些模型成为黑盒的特性，以及如何使用局部范围内的模型无关的后验方法来解释它们。你将特别学习到局部可解释模型无关解释（LIME）、SHapley
    Additive exPlanations（SHAP）和锚点等技术。
- en: In chapter 5, we focus on convolutional neural networks, which is a more advanced
    form of architecture used mainly for visual tasks such as image classification
    and object detection. You’ll learn how to visualize what the model is focusing
    on using saliency maps. You’ll also learn techniques such as gradients, guided
    backpropagation (backprop for short), gradient-weighted class activation mapping
    (grad-CAM), guided grad-CAM, and smooth gradients (SmoothGrad).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章中，我们专注于卷积神经网络，这是一种主要用于视觉任务（如图像分类和目标检测）的更高级的架构形式。你将学习如何使用显著性图来可视化模型关注的对象。你还将学习梯度、指导反向传播（简称backprop）、梯度加权类激活映射（grad-CAM）、指导grad-CAM和光滑梯度（SmoothGrad）等技术。
- en: In chapters 6 and 7, we focus on convolutional neural networks and neural networks
    used for language understanding. You’ll learn how to dissect the neural networks
    and understand what representations of the data are learned by the intermediate
    or hidden layers in the neural network. You’ll also learn how to visualize high-dimensional
    representations learned by the model using techniques like principal component
    analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6章和第7章中，我们专注于卷积神经网络和用于语言理解的神经网络。你将学习如何剖析神经网络，并理解神经网络中中间或隐藏层学习到的数据表示。你还将学习如何使用主成分分析（PCA）和t分布随机邻域嵌入（t-SNE）等技术来可视化模型学习到的高维表示。
- en: The book ends on the topic of building fair and unbiased models and learning
    what it takes to build explainable AI systems. In chapter 8, you’ll learn about
    various definitions of fairness and how to check whether your model is biased.
    You’ll also learn techniques to mitigate bias using a neutralizing technique.
    We discuss a standardizing approach to documenting datasets using datasheets that
    help improve transparency and accountability with the stakeholders and users of
    the system. In chapter 9, we pave the way for explainable AI by teaching how to
    build such systems, and you’ll also learn about contrastive explanations using
    counterfactual examples. By the end of this book, you will have various interpretability
    techniques in your toolkit. When it comes to model understanding, there is, unfortunately,
    no silver bullet. No one interpretability technique is applicable for all scenarios.
    You, therefore, need to look at the model using a few different lenses by applying
    multiple interpretability techniques. In this book, I help you identify the right
    tools for the right scenarios.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 本书以构建公平且无偏见的模型以及学习构建可解释人工智能系统所需的知识点结束。在第8章中，你将了解公平性的各种定义以及如何检查你的模型是否存在偏见。你还将学习使用中和技术来减轻偏见的技术。我们讨论了使用数据表来标准化记录数据集的方法，这有助于提高与系统利益相关者和用户的透明度和问责制。在第9章中，我们通过教授如何构建这样的系统为可解释人工智能铺平道路，你还将了解使用反事实示例进行对比解释。到本书结束时，你将拥有各种可解释性技术在你的工具箱中。当涉及到模型理解时，遗憾的是没有一劳永逸的解决方案。没有一种可解释性技术适用于所有场景。因此，你需要通过应用多种可解释性技术，从几个不同的角度来审视模型。在这本书中，我将帮助你识别适合特定场景的正确工具。
- en: 1.7.1 What tools will I be using in this book?
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7.1 我在这本书中将使用哪些工具？
- en: In this book, we will implement the models and the interpretability techniques
    in the Python programming language. The main reason I chose Python is because
    most of the state-of-the-art interpretability techniques are created and actively
    developed in this language. Figure 1.16 gives an overview of the tools used in
    this book. For representing data, we will be using Python data structures and
    common data science libraries such as *Pandas* and *NumPy*. To implement white-box
    models, we will use the *Scikit-Learn* library for simpler linear regression and
    decision trees, and *pyGAM* for GAMs. For black-box models, we will use *Scikit-Learn*
    for tree ensembles and *PyTorch* or *TensorFlow* for neural networks. For interpretability
    techniques used to understand model processing, we will use the *Matplotlib* library
    for visualization and open source libraries that implement techniques such as
    *PDP*, *LIME*, *SHAP*, *anchors*, *gradients, guided backprop, grad-CAM* and *SmoothGrad*.
    To interpret model representations, we will employ tools that implement *NetDissect*
    and *tSNE* and visualize them using the *Matplotlib* library. Finally, for mitigating
    bias, we will use *PyTorch* and *TensorFlow* to implement the bias-neutralizing
    technique and GANs for adversarial debiasing.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将使用 Python 编程语言实现模型和可解释性技术。我选择 Python 的主要原因是因为大多数最先进的可解释性技术都是在这个语言中创建和积极开发的。图
    1.16 展示了本书中使用的工具概览。对于数据表示，我们将使用 Python 数据结构和常见的数据科学库，如 *Pandas* 和 *NumPy*。为了实现白盒模型，我们将使用
    *Scikit-Learn* 库进行简单的线性回归和决策树，以及 *pyGAM* 用于 GAM。对于黑盒模型，我们将使用 *Scikit-Learn* 进行树集成，以及
    *PyTorch* 或 *TensorFlow* 进行神经网络。对于用于理解模型处理的可解释性技术，我们将使用 *Matplotlib* 库进行可视化，以及实现
    *PDP*、*LIME*、*SHAP*、*anchors*、*gradients*、*guided backprop*、*grad-CAM* 和 *SmoothGrad*
    等技术的开源库。为了解释模型表示，我们将使用实现 *NetDissect* 和 *tSNE* 的工具，并使用 *Matplotlib* 库进行可视化。最后，为了减轻偏差，我们将使用
    *PyTorch* 和 *TensorFlow* 实现偏差中和技术，以及 GANs 进行对抗性去偏差。
- en: '![](../Images/CH01_F16_Thampi.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH01_F16_Thampi.png)'
- en: Figure 1.16 An overview of the tools used in this book
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.16 本书使用的工具概览
- en: 1.7.2 What do I need to know before reading this book?
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7.2 在阅读本书之前我需要了解什么？
- en: This book is primarily focused on data scientists and engineers with experience
    programming in Python. A basic knowledge of common Python data science libraries
    such as NumPy, Pandas, Matplotlib, and Scikit-Learn will help, although this is
    not required. The book will show you how to use these libraries to load and represent
    data but will not give you an in-depth understanding of them, because it is beyond
    the scope of this book.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 本书主要面向有 Python 编程经验的科学家和工程师。对常见 Python 数据科学库（如 NumPy、Pandas、Matplotlib 和 Scikit-Learn）的基本了解会有所帮助，尽管这不是必需的。本书将向您展示如何使用这些库加载数据和表示数据，但不会深入探讨它们，因为这超出了本书的范围。
- en: The reader must be familiar with linear algebra, specifically vectors and matrices,
    and operations on them, such as dot product, matrix multiplication, transpose,
    and inversion. The reader must also have a good foundation in probability theory
    and statistics, specifically on the topics of random variables, basic discrete
    and continuous probability distributions, conditional probability, and Bayes’
    theorem. Basic knowledge of calculus is also expected, specifically single-variable
    and multivariate functions and specific operations on them such as derivatives
    (gradients) and partial derivatives. Although this book does not focus too much
    on the mathematics behind model interpretability, having this basic mathematical
    foundation is expected of data scientists and engineers interested in building
    machine learning models.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 读者必须熟悉线性代数，特别是向量和矩阵，以及它们的运算，如点积、矩阵乘法、转置和求逆。读者还必须具备概率论和统计学的基础知识，特别是关于随机变量、基本的离散和连续概率分布、条件概率和贝叶斯定理的主题。还期望读者具备微积分的基本知识，特别是单变量和多变量函数及其导数（梯度）和偏导数。尽管本书不会过多关注模型可解释性背后的数学，但期望对构建机器学习模型感兴趣的数据科学家和工程师具备这些基本数学基础。
- en: Basic knowledge of machine learning or practical experience training machine
    learning models is a plus, although this is not a hard requirement. This book
    does not cover machine learning in great depth because a lot of resources and
    books do justice to this topic. The book will, however, give you a basic understanding
    of the specific machine learning models being used and also show you how to train
    and evaluate them. The main focus is on the theory related to interpretability
    and how you can implement techniques to interpret the model after you have trained
    it.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对机器学习的基本知识或实际训练机器学习模型的实践经验是一个加分项，尽管这不是硬性要求。本书不会深入探讨机器学习，因为许多资源和书籍已经很好地涵盖了这一主题。然而，本书将为你提供特定机器学习模型的基本理解，并展示如何训练和评估它们。主要关注与可解释性相关的理论，以及你如何在训练后实施解释模型的技术。
- en: Summary
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Three broad types of machine learning systems exist: supervised learning, unsupervised
    learning, and reinforcement learning. This book focuses on interpretability techniques
    for supervised learning systems that include both regression and classification
    types of problems.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在三种广泛的机器学习系统类型：监督学习、无监督学习和强化学习。本书专注于监督学习系统的可解释性技术，这些系统包括回归和分类类型的问题。
- en: When building AI systems, it is important to add interpretability, model understanding,
    and monitoring to the process. If you don’t, you could experience disastrous consequences
    such as data leakage, bias, concept drift, and a general lack of trust. Moreover,
    with the GDPR, we have legal reasons for including interpretability in our AI
    processes.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在构建AI系统时，将可解释性、模型理解和监控添加到流程中非常重要。如果不这样做，你可能会遇到灾难性的后果，如数据泄露、偏差、概念漂移和普遍缺乏信任。此外，随着GDPR的实施，我们在AI流程中包含可解释性有法律上的原因。
- en: It is important to understand the difference between interpretability and explainability.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解可解释性和可解释性之间的区别很重要。
- en: Interpretability is the degree to which we can consistently estimate what a
    model will predict, understand how the model came up with the prediction, and
    understand when the model has made a mistake. Interpretability techniques are
    building blocks that will help you get to explainability.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释性是我们能够一致地估计模型将预测什么、理解模型如何得出预测以及理解模型何时犯错的程度。可解释性技术是帮助你达到可解释性的基石。
- en: Explainability goes beyond interpretability in that it helps us understand how
    and why a model came up with a prediction in a human-readable form. It makes use
    of interpretability techniques and also looks to other fields and areas, such
    as human-computer interaction (HCI), law, and ethics.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释性超越了可解释性，因为它帮助我们以人类可读的形式理解模型是如何以及为什么得出预测的。它利用可解释性技术，并参考其他领域和领域，如人机交互（HCI）、法律和伦理。
- en: You need to be mindful of different personas using or building the AI system,
    because interpretability means different things to different people.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要意识到不同的人在使用或构建AI系统时的不同角色，因为可解释性对不同的人意味着不同的事情。
- en: Interpretability techniques can be intrinsic or post hoc, model-specific or
    model-agnostic, local or global.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释性技术可以是内在的或事后确定的，模型特定的或模型无关的，局部的或全局的。
- en: Models that are inherently transparent are called white-box models, and models
    that are inherently opaque are called black-box models. White-box models are much
    easier to interpret but generally have lower predictive power than black-box models.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本质上透明的模型被称为白盒模型，而本质上不透明的模型被称为黑盒模型。白盒模型更容易解释，但通常比黑盒模型的预测能力低。
- en: 'Black-box models offer two broad classes of interpretability techniques: one
    that’s focused on interpreting the model processing and another that’s focused
    on interpreting the representation learned by the model.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黑盒模型提供了两种广泛的可解释性技术：一种专注于解释模型处理，另一种专注于解释模型学习到的表示。
