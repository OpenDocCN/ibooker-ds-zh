- en: Chapter 40\. Feature Engineering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 40 章 特征工程
- en: 'The previous chapters outlined the fundamental ideas of machine learning, but
    all of the examples so far have assumed that you have numerical data in a tidy,
    `[n_samples, n_features]` format. In the real world, data rarely comes in such
    a form. With this in mind, one of the more important steps in using machine learning
    in practice is *feature engineering*: that is, taking whatever information you
    have about your problem and turning it into numbers that you can use to build
    your feature matrix.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 前几章概述了机器学习的基本思想，但到目前为止的所有示例都假定您有数字数据以整洁的`[n_samples, n_features]`格式。在现实世界中，数据很少以这种形式出现。考虑到这一点，实际应用机器学习的一个更重要的步骤之一是*特征工程*：即，利用您对问题的任何信息，并将其转换为您可以用来构建特征矩阵的数字。
- en: 'In this chapter, we will cover a few common examples of feature engineering
    tasks: we’ll look at features for representing categorical data, text, and images.
    Additionally, we will discuss derived features for increasing model complexity
    and imputation of missing data. This process is commonly referred to as vectorization,
    as it involves converting arbitrary data into well-behaved vectors.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖几个常见的特征工程任务示例：我们将查看用于表示分类数据、文本和图像的特征。此外，我们还将讨论增加模型复杂性和填补缺失数据的派生特征。这个过程通常被称为向量化，因为它涉及将任意数据转换为行为良好的向量。
- en: Categorical Features
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类特征
- en: 'One common type of nonnumerical data is *categorical* data. For example, imagine
    you are exploring some data on housing prices, and along with numerical features
    like “price” and “rooms,” you also have “neighborhood” information. For example,
    your data might look something like this:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的非数值数据类型是*分类*数据。例如，想象一下您正在探索一些关于房价的数据，除了像“价格”和“房间”这样的数值特征之外，还有“街区”信息。例如，您的数据可能如下所示：
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You might be tempted to encode this data with a straightforward numerical mapping:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会被诱惑使用直接的数值映射来对这些数据进行编码：
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: But it turns out that this is not generally a useful approach in Scikit-Learn.
    The package’s models make the fundamental assumption that numerical features reflect
    algebraic quantities, so such a mapping would imply, for example, that *Queen
    Anne < Fremont < Wallingford*, or even that *Wallingford–Queen Anne = Fremont*,
    which (niche demographic jokes aside) does not make much sense.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 但事实证明，在 Scikit-Learn 中，这一般不是一个有用的方法。该软件包的模型假设数值特征反映了代数量，因此这样的映射会暗示，例如，*Queen
    Anne < Fremont < Wallingford*，甚至是*Wallingford–Queen Anne = Fremont*，这（除了小众的人口统计笑话）并没有多少意义。
- en: 'In this case, one proven technique is to use *one-hot encoding*, which effectively
    creates extra columns indicating the presence or absence of a category with a
    value of 1 or 0, respectively. When your data takes the form of a list of dictionaries,
    Scikit-Learn’s `DictVectorizer` will do this for you:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，一个经过验证的技术是使用*独热编码*，它有效地创建额外的列，指示类别的存在或不存在，分别为 1 或 0。当您的数据采取字典列表的形式时，Scikit-Learn
    的 `DictVectorizer` 将为您执行此操作：
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Notice that the `neighborhood` column has been expanded into three separate
    columns representing the three neighborhood labels, and that each row has a 1
    in the column associated with its neighborhood. With these categorical features
    thus encoded, you can proceed as normal with fitting a Scikit-Learn model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`neighborhood` 列已扩展为三个单独的列，表示三个街区标签，每一行都在与其街区相关联的列中具有 1。有了这些分类特征编码，您可以像正常情况下一样拟合一个
    Scikit-Learn 模型。
- en: 'To see the meaning of each column, you can inspect the feature names:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看每一列的含义，您可以检查特征名称：
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'There is one clear disadvantage of this approach: if your category has many
    possible values, this can *greatly* increase the size of your dataset. However,
    because the encoded data contains mostly zeros, a sparse output can be a very
    efficient solution:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有一个明显的缺点：如果您的类别有许多可能的值，这可能会*大大*增加数据集的大小。然而，因为编码数据主要包含零，所以稀疏输出可以是一个非常有效的解决方案：
- en: '[PRE4]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Nearly all of the Scikit-Learn estimators accept such sparse inputs when fitting
    and evaluating models. Two additional tools that Scikit-Learn includes to support
    this type of encoding are `sklearn.preprocessing.OneHotEncoder` and `sklearn​.fea⁠ture_​extraction.FeatureHasher`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的 Scikit-Learn 评估器都接受这种稀疏输入来拟合和评估模型。Scikit-Learn 包括的另外两个支持这种编码类型的工具是 `sklearn.preprocessing.OneHotEncoder`
    和 `sklearn​.fea⁠ture_​extraction.FeatureHasher`。
- en: Text Features
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本特征
- en: 'Another common need in feature engineering is to convert text to a set of representative
    numerical values. For example, most automatic mining of social media data relies
    on some form of encoding the text as numbers. One of the simplest methods of encoding
    this type of data is by *word counts*: you take each snippet of text, count the
    occurrences of each word within it, and put the results in a table.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程中另一个常见需求是将文本转换为一组代表性的数值。例如，大多数自动挖掘社交媒体数据都依赖于某种形式的文本编码为数字。编码这种类型数据的最简单方法之一是*词频*：你拿到每段文本，计算其中每个词的出现次数，并将结果放入表格中。
- en: 'For example, consider the following set of three phrases:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下三个短语的集合：
- en: '[PRE5]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'For a vectorization of this data based on word count, we could construct individual
    columns representing the words “problem,” “of,” “evil,” and so on. While doing
    this by hand would be possible for this simple example, the tedium can be avoided
    by using Scikit-Learn’s `CountVectorizer`:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于词频的数据向量化，我们可以构建代表词语“问题”、“of”、“evil”等的单独列。虽然在这个简单的例子中手动操作是可能的，但可以通过使用Scikit-Learn的`CountVectorizer`来避免这种单调的工作：
- en: '[PRE6]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The result is a sparse matrix recording the number of times each word appears;
    it is easier to inspect if we convert this to a `DataFrame` with labeled columns:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个记录每个词出现次数的稀疏矩阵；如果我们将其转换为带有标记列的`DataFrame`，检查起来就更容易了。
- en: '[PRE7]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'There are some issues with using a simple raw word count, however: it can lead
    to features that put too much weight on words that appear very frequently, and
    this can be suboptimal in some classification algorithms. One approach to fix
    this is known as *term frequency–inverse document frequency* (*TF–IDF*), which
    weights the word counts by a measure of how often they appear in the documents.
    The syntax for computing these features is similar to the previous example:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用简单的原始词频存在一些问题：它可能会导致对出现非常频繁的词给予过多权重，这在某些分类算法中可能不是最优的。修正这一问题的一种方法称为*词频-逆文档频率*（*TF-IDF*），它通过衡量词语在文档中出现的频率来加权词频。计算这些特征的语法与前面的例子类似：
- en: 'The solid lines show the new results, while the fainter dashed lines show the
    results on the previous smaller dataset. It is clear from the validation curve
    that the larger dataset can support a much more complicated model: the peak here
    is probably around a degree of 6, but even a degree-20 model isn’t seriously overfitting
    the data—the validation and training scores remain very close.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 实线显示了新结果，而较淡的虚线显示了之前较小数据集的结果。从验证曲线可以明显看出，较大的数据集可以支持更复杂的模型：这里的峰值可能在6次方附近，但即使是20次方的模型也没有严重过拟合数据——验证分数和训练分数保持非常接近。
- en: '[PRE8]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For an example of using TF-IDF in a classification problem, see [Chapter 41](ch41.xhtml#section-0505-naive-bayes).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 关于在分类问题中使用TF-IDF的示例，请参阅[第41章](ch41.xhtml#section-0505-naive-bayes)。
- en: Image Features
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像特征
- en: 'Another common need is to suitably encode images for machine learning analysis.
    The simplest approach is what we used for the digits data in [Chapter 38](ch38.xhtml#section-0502-introducing-scikit-learn):
    simply using the pixel values themselves. But depending on the application, such
    an approach may not be optimal.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见需求是为机器学习分析适当编码图像。最简单的方法是我们在[第38章](ch38.xhtml#section-0502-introducing-scikit-learn)中用于数字数据的方法：仅使用像素值本身。但根据应用程序的不同，这种方法可能不是最优的。
- en: A comprehensive summary of feature extraction techniques for images is well
    beyond the scope of this chapter, but you can find excellent implementations of
    many of the standard approaches in the [Scikit-Image project](http://scikit-image.org).
    For one example of using Scikit-Learn and Scikit-Image together, see [Chapter 50](ch50.xhtml#section-0514-image-features).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 关于图像的特征提取技术的全面总结远超出本章的范围，但你可以在[Scikit-Image项目](http://scikit-image.org)中找到许多标准方法的出色实现。关于如何同时使用Scikit-Learn和Scikit-Image的示例，请参阅[第50章](ch50.xhtml#section-0514-image-features)。
- en: Derived Features
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 派生特征
- en: Another useful type of feature is one that is mathematically derived from some
    input features. We saw an example of this in [Chapter 39](ch39.xhtml#section-0503-hyperparameters-and-model-validation)
    when we constructed *polynomial features* from our input data. We saw that we
    could convert a linear regression into a polynomial regression not by changing
    the model, but by transforming the input!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种有用的特征类型是从某些输入特征数学推导出来的特征。我们在[第39章](ch39.xhtml#section-0503-hyperparameters-and-model-validation)中看到了一个例子，当我们从输入数据构建*多项式特征*时。我们看到，我们可以将线性回归转换为多项式回归，而不是改变模型，而是通过转换输入！
- en: 'For example, this data clearly cannot be well described by a straight line
    (see [Figure 40-1](#fig_0504-feature-engineering_files_in_output_24_0)):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这些数据显然不能用一条直线很好地描述（见[图 40-1](#fig_0504-feature-engineering_files_in_output_24_0)）：
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![output 24 0](assets/output_24_0.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![output 24 0](assets/output_24_0.png)'
- en: Figure 40-1\. Data that is not well described by a straight line
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 40-1\. 不能很好地用直线描述的数据
- en: 'We can still fit a line to the data using `LinearRegression` and get the optimal
    result, as shown in [Figure 40-2](#fig_0504-feature-engineering_files_in_output_26_0):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然可以使用`LinearRegression`对数据进行线性拟合，并获得最优结果，如[图 40-2](#fig_0504-feature-engineering_files_in_output_26_0)所示：
- en: '[PRE10]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![output 26 0](assets/output_26_0.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![output 26 0](assets/output_26_0.png)'
- en: Figure 40-2\. A poor straight-line fit
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 40-2\. 一条较差的直线拟合
- en: But it’s clear that we need a more sophisticated model to describe the relationship
    between <math alttext="x"><mi>x</mi></math> and <math alttext="y"><mi>y</mi></math>
    .
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 但显然我们需要一个更复杂的模型来描述<math alttext="x"><mi>x</mi></math>和<math alttext="y"><mi>y</mi></math>之间的关系。
- en: 'One approach to this is to transform the data, adding extra columns of features
    to drive more flexibility in the model. For example, we can add polynomial features
    to the data this way:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对此的一种方法是转换数据，添加额外的特征列以增强模型的灵活性。例如，我们可以这样向数据中添加多项式特征：
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The derived feature matrix has one column representing <math alttext="x"><mi>x</mi></math>
    , a second column representing <math alttext="x squared"><msup><mi>x</mi> <mn>2</mn></msup></math>
    , and a third column representing <math alttext="x cubed"><msup><mi>x</mi> <mn>3</mn></msup></math>
    . Computing a linear regression on this expanded input gives a much closer fit
    to our data, as you can see in [Figure 40-3](#fig_0504-feature-engineering_files_in_output_30_0):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 派生的特征矩阵有一列表示<math alttext="x"><mi>x</mi></math>，第二列表示<math alttext="x squared"><msup><mi>x</mi>
    <mn>2</mn></msup></math>，第三列表示<math alttext="x cubed"><msup><mi>x</mi> <mn>3</mn></msup></math>。在这扩展输入上计算线性回归可以更接近我们的数据，如你在[图 40-3](#fig_0504-feature-engineering_files_in_output_30_0)中所见：
- en: '[PRE12]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![output 30 0](assets/output_30_0.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![output 30 0](assets/output_30_0.png)'
- en: Figure 40-3\. A linear fit to polynomial features derived from the data
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 40-3\. 对数据导出的多项式特征进行线性拟合
- en: This idea of improving a model not by changing the model, but by transforming
    the inputs, is fundamental to many of the more powerful machine learning methods.
    We’ll explore this idea further in [Chapter 42](ch42.xhtml#section-0506-linear-regression)
    in the context of *basis function regression*. More generally, this is one motivational
    path to the powerful set of techniques known as *kernel methods*, which we will
    explore in [Chapter 43](ch43.xhtml#section-0507-support-vector-machines).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 改进模型的一个思路不是改变模型本身，而是转换输入数据，这对许多更强大的机器学习方法至关重要。我们将在[第42章](ch42.xhtml#section-0506-linear-regression)进一步探讨这个想法，这是基函数回归的一个例子。更一般地说，这是强大技术集合——*核方法*的动机之一，我们将在[第43章](ch43.xhtml#section-0507-support-vector-machines)中探讨。
- en: Imputation of Missing Data
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺失数据的插补
- en: 'Another common need in feature engineering is handling of missing data. We
    discussed the handling of missing data in `DataFrame` objects in [Chapter 16](ch16.xhtml#section-0304-missing-values),
    and saw that `NaN` is often is used to mark missing values. For example, we might
    have a dataset that looks like this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程中另一个常见需求是处理缺失数据。我们在[第16章](ch16.xhtml#section-0304-missing-values)中讨论了在`DataFrame`对象中处理缺失数据的方法，并看到`NaN`通常用于标记缺失值。例如，我们可能有一个数据集看起来像这样：
- en: '[PRE13]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: When applying a typical machine learning model to such data, we will need to
    first replace the missing values with some appropriate fill value. This is known
    as *imputation* of missing values, and strategies range from simple (e.g., replacing
    missing values with the mean of the column) to sophisticated (e.g., using matrix
    completion or a robust model to handle such data).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当将典型的机器学习模型应用于这类数据时，我们需要首先用适当的填充值替换缺失值。这被称为*缺失值的插补*，策略从简单（例如，用列的平均值替换缺失值）到复杂（例如，使用矩阵完成或强健模型处理此类数据）。
- en: 'The sophisticated approaches tend to be very application-specific, and we won’t
    dive into them here. For a baseline imputation approach using the mean, median,
    or most frequent value, Scikit-Learn provides the `SimpleImputer` class:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 高级方法往往非常依赖于特定应用场景，我们在这里不会深入讨论。对于使用均值、中位数或最频繁值的基本插补方法，Scikit-Learn提供了`SimpleImputer`类：
- en: '[PRE14]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We see that in the resulting data, the two missing values have been replaced
    with the mean of the remaining values in the column. This imputed data can then
    be fed directly into, for example, a `LinearRegression` estimator:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到在结果数据中，两个缺失值已经被替换为该列其余值的平均值。这些填充的数据可以直接输入到例如`LinearRegression`估算器中：
- en: '[PRE15]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Feature Pipelines
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征管道
- en: 'With any of the preceding examples, it can quickly become tedious to do the
    transformations by hand, especially if you wish to string together multiple steps.
    For example, we might want a processing pipeline that looks something like this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任何上述示例，如果希望手动执行转换，尤其是希望串联多个步骤时，可能很快变得乏味。例如，我们可能希望一个处理管道看起来像这样：
- en: Impute missing values using the mean.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用均值填补缺失值。
- en: Transform features to quadratic.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将特征转换为二次项。
- en: Fit a linear regression model.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合线性回归模型。
- en: 'To streamline this type of processing pipeline, Scikit-Learn provides a `Pipeline`
    object, which can be used as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化这种类型的处理管道，Scikit-Learn提供了一个`Pipeline`对象，可以如下使用：
- en: '[PRE16]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This pipeline looks and acts like a standard Scikit-Learn object, and will
    apply all the specified steps to any input data:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个管道看起来和操作起来像一个标准的Scikit-Learn对象，将所有指定的步骤应用于任何输入数据：
- en: '[PRE17]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: All the steps of the model are applied automatically. Notice that for simplicity,
    in this demonstration we’ve applied the model to the data it was trained on; this
    is why it was able to perfectly predict the result (refer back to [Chapter 39](ch39.xhtml#section-0503-hyperparameters-and-model-validation)
    for further discussion).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的所有步骤都是自动应用的。请注意，为了简单起见，在这个演示中，我们已经将模型应用于它训练过的数据；这就是为什么它能够完美地预测结果（详见[第39章](ch39.xhtml#section-0503-hyperparameters-and-model-validation)进一步讨论）。
- en: For some examples of Scikit-Learn pipelines in action, see the following chapter
    on naive Bayes classification, as well as Chapters [42](ch42.xhtml#section-0506-linear-regression)
    and [43](ch43.xhtml#section-0507-support-vector-machines).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Scikit-Learn管道实际操作的一些示例，请参阅有关朴素贝叶斯分类的以下章节，以及第[42章](ch42.xhtml#section-0506-linear-regression)和第[43章](ch43.xhtml#section-0507-support-vector-machines)。
