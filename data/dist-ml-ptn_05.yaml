- en: 5 Workflow patterns
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 工作流程模式
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Using workflows to connect machine learning system components
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用工作流程连接机器学习系统组件
- en: Composing complex but maintainable structures within machine learning workflows
    with the fan-in and fan-out patterns
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用扇入和扇出模式在机器学习工作流程中构建复杂但可维护的结构
- en: Accelerating machine learning workloads with concurrent steps using synchronous
    and asynchronous patterns
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用同步和异步模式通过并发步骤加速机器学习工作负载
- en: Improving performance with the step memoization pattern
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用步骤记忆化模式提高性能
- en: Model serving is a critical step after successfully training a machine learning
    model. It is the final artifact produced by the entire machine learning workflow,
    and the results from model serving are presented to users directly. Previously,
    we explored some of the challenges involved in distributed model serving systems—for
    example, how to handle the growing number of model serving requests and the increased
    size of those requests—and investigated a few established patterns heavily adopted
    in industry. We learned how to achieve horizontal scaling with the help of replicated
    services to address these challenges and how the sharded services pattern can
    help the system process large model serving requests. Finally, we learned how
    to assess model serving systems and determine whether an event-driven design would
    be beneficial in real-world scenarios.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务是在成功训练机器学习模型之后的关键步骤。它是整个机器学习工作流程产生的最终成果，模型服务的成果直接呈现给用户。之前，我们探讨了分布式模型服务系统中的一些挑战——例如，如何处理不断增长的模型服务请求数量和这些请求的增大——并调查了在行业中广泛采用的几个已建立的模式。我们学习了如何通过复制服务来实现水平扩展以解决这些挑战，以及如何使用分片服务模式帮助系统处理大量的模型服务请求。最后，我们学习了如何评估模型服务系统，并确定在现实世界场景中事件驱动设计是否会带来益处。
- en: Workflow is an essential component in machine learning systems as it connects
    all other components in the system. A machine learning workflow can be as easy
    as chaining data ingestion, model training, and model serving. However, it can
    be very complex to handle real-world scenarios requiring additional steps and
    performance optimizations as part of the entire workflow. It’s essential to know
    what tradeoffs we may see when making design decisions to meet different business
    and performance requirements.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程是机器学习系统中的一个基本组件，因为它连接了系统中的所有其他组件。机器学习工作流程可能非常简单，就像链式数据摄取、模型训练和模型服务。然而，处理需要额外步骤和性能优化的现实世界场景可能非常复杂，这些步骤和优化是整个工作流程的一部分。了解在设计决策中可能遇到哪些权衡，以满足不同的业务和性能要求，这是非常重要的。
- en: In this chapter, we’ll explore some of the challenges involved when building
    machine learning workflows in practice. Each of these established patterns can
    be reused to build simple to complex machine learning workflows that are efficient
    and scalable. For example, we’ll see how to build a system to execute complex
    machine learning workflows to train multiple machine learning models. We will
    use the fan-in and fan-out patterns to select the most performant models that
    provide good entity-tagging results in the model serving system. We’ll also incorporate
    synchronous and asynchronous patterns to make machine learning workflows more
    efficient and avoid delays due to the long-running model training steps that block
    other consecutive steps.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨在实践构建机器学习工作流程时涉及的一些挑战。这些已建立的模式可以被重用来构建从简单到复杂的机器学习工作流程，这些工作流程既高效又可扩展。例如，我们将看到如何构建一个系统来执行复杂的机器学习工作流程，以训练多个机器学习模型。我们将使用扇入和扇出模式来选择在模型服务系统中提供良好实体标记结果的性能最优模型。我们还将结合同步和异步模式，使机器学习工作流程更加高效，并避免由于长时间运行的模型训练步骤而导致的延迟，这些步骤会阻塞其他连续步骤。
- en: 5.1 What is workflow?
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 什么是工作流程？
- en: '*Workflow* is the process of connecting multiple components or steps in an
    end-to-end machine learning system. A workflow consists of arbitrary combinations
    of the components commonly seen in real-world machine learning applications, such
    as data ingestion, distributed model training, and model serving, as discussed
    in the previous chapters.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*工作流程*是连接端到端机器学习系统中多个组件或步骤的过程。工作流程由现实世界中机器学习应用中常见的组件的任意组合组成，例如数据摄取、分布式模型训练和模型服务，如前几章所述。'
- en: 'Figure 5.1 shows a simple machine learning workflow. This workflow connects
    multiple components or steps in an end-to-end machine learning system that includes
    the following steps:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1展示了简单的机器学习工作流。该工作流连接了端到端机器学习系统中的多个组件或步骤，包括以下步骤：
- en: Data ingestion—Consumes the Youtube-8M videos dataset
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据摄入——消费YouTube-8M视频数据集
- en: Model training—Trains an entity-tagging model
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练——训练实体标记模型
- en: Model serving—Tags entities in unseen videos
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型服务——对未见过的视频中的实体进行标记
- en: Note A machine learning workflow is often referred to as a *machine learning
    pipeline*. I use these two terms interchangeably. Although I use different terms
    to refer to different technologies, there is no difference between the two terms
    in this book.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：机器学习工作流通常被称为**机器学习管道**。我在这里交替使用这两个术语。尽管我使用不同的术语来指代不同的技术，但在这本书中这两个术语之间没有区别。
- en: Since a machine learning workflow may consist of any combination of the components,
    we often see machine learning workflows in different forms in different situations.
    Unlike the straightforward workflow shown in figure 5.1, figure 5.2 illustrates
    a more complicated workflow where two separate model training steps are launched
    after a single data ingestion step, and then two separate model serving steps
    are used to serve different models trained via different model training steps.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习工作流可能由任何组合的组件组成，因此我们在不同情况下经常看到不同形式的机器学习工作流。与图5.1中展示的直接工作流不同，图5.2说明了更复杂的流程，其中在单个数据摄入步骤之后启动了两个独立的模型训练步骤，然后使用两个独立的模型服务步骤来服务通过不同模型训练步骤训练的不同模型。
- en: '![05-01](../../OEBPS/Images/05-01.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![05-01](../../OEBPS/Images/05-01.png)'
- en: Figure 5.1 A diagram showing a simple machine learning workflow, including data
    ingestion, model training, and model serving. The arrows indicate directions.
    For example, the arrow on the right-hand side denotes the order of the step execution
    (e.g., the workflow executes the model serving step after the model training step
    is completed).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1展示了简单的机器学习流程图，包括数据摄入、模型训练和模型服务。箭头表示方向。例如，右侧的箭头表示步骤执行的顺序（例如，工作流在模型训练步骤完成后执行模型服务步骤）。
- en: '![05-02](../../OEBPS/Images/05-02.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![05-02](../../OEBPS/Images/05-02.png)'
- en: Figure 5.2 A more complicated workflow, where two separate model training steps
    are launched after a single data ingestion step, and then two separate model serving
    steps are used to serve different models trained via different model training
    steps
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2展示了更复杂的流程，其中在单个数据摄入步骤之后启动了两个独立的模型训练步骤，然后使用两个独立的模型服务步骤来服务通过不同模型训练步骤训练的不同模型。
- en: Figures 5.1 and 5.2 are just some common examples. In practice, the complexity
    of machine learning workflows varies, which increases the difficulty of building
    and maintaining scalable machine learning systems.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1和图5.2只是常见的一些例子。在实践中，机器学习工作流的复杂性各不相同，这增加了构建和维护可扩展机器学习系统的难度。
- en: 'We will discuss some of the more complex machine learning workflows in this
    chapter, but to start, I’ll introduce and distinguish the differences between
    the following two concepts: *sequential workflow* and *directed acyclic graph
    (DAG)*.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中讨论一些更复杂的机器学习工作流，但首先，我将介绍并区分以下两个概念之间的差异：**顺序工作流**和**有向无环图（DAG）**。
- en: A *sequential workflow* represents a series of steps performed one after another
    until the last step in the series is complete. The exact order of execution varies,
    but steps will always be sequential. Figure 5.3 is an example sequential workflow
    with three steps executed sequentially.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个**顺序工作流**表示一系列依次执行直到系列中的最后一个步骤完成的步骤。执行的确切顺序可能不同，但步骤始终是顺序的。图5.3是一个包含三个顺序执行的步骤的示例顺序工作流。
- en: '![05-03](../../OEBPS/Images/05-03.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![05-03](../../OEBPS/Images/05-03.png)'
- en: 'Figure 5.3 An example sequential workflow with three steps that execute in
    the following order: A, B, and C.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3展示了三个步骤按以下顺序执行的示例顺序工作流：A、B和C。
- en: A workflow can be seen as a DAG if it only consists of steps directed from one
    step to another but never form a closed loop.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个工作流仅由从一步到另一步的定向步骤组成，但永远不会形成一个闭环，那么它可以被看作是一个有向无环图（DAG）。
- en: For example, the workflow in figure 5.3 is a valid DAG since the three steps
    are directed from step A to step B and then from step B to step C—the loop is
    not closed. Another example workflow, shown in figure 5.4, however, is not a valid
    DAG since there’s an additional step D that connects from step C and points to
    step A, which forms a closed loop.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，图 5.3 中的工作流程是一个有效有向无环图（DAG），因为三个步骤是从步骤 A 到步骤 B，然后从步骤 B 到步骤 C 的有向的——循环没有闭合。然而，另一个如图
    5.4 所示的工作流程不是一个有效有向无环图（DAG），因为有一个额外的步骤 D 从步骤 C 连接到步骤 A，形成一个闭合循环。
- en: '![05-04](../../OEBPS/Images/05-04.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![05-04](../../OEBPS/Images/05-04.png)'
- en: Figure 5.4 An example workflow where step D connects from step C and points
    to step A. These connections form a closed loop and thus the entire workflow is
    not a valid DAG.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 一个示例工作流程，其中步骤 D 从步骤 C 连接到步骤 A。这些连接形成一个闭合循环，因此整个工作流程不是一个有效有向无环图（DAG）。
- en: If step D does not point back to step A, as shown in figure 5.5, where the arrow
    is crossed out, this workflow becomes a valid DAG. The loop is no longer closed,
    and thus it becomes a simple sequential workflow, similar to figure 5.3.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果步骤 D 不指向步骤 A，如图 5.5 所示，箭头被划掉，则此工作流程成为一个有效有向无环图（DAG）。循环不再闭合，因此它变成了一个简单的顺序工作流程，类似于图
    5.3。
- en: '![05-05](../../OEBPS/Images/05-05.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![05-05](../../OEBPS/Images/05-05.png)'
- en: Figure 5.5 An example workflow where the last step D does not point back to
    step A. This workflow is not a valid DAG since the closed loop no longer exists.
    Instead, it is a simple sequential workflow similar to figure 5.3.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 一个示例工作流程，其中最后一步 D 不指向步骤 A。由于闭合循环不再存在，此工作流程不是一个有效有向无环图（DAG）。相反，它是一个类似于图
    5.3 的简单顺序工作流程。
- en: In real-world machine learning applications, workflows necessary to meet the
    requirements of different use cases (e.g., batch retraining of the models, hyperparameter
    tuning experiments, etc.) can get really complicated. We will go through some
    more complex workflows and abstract the structural patterns that can be reused
    to compose workflows for various scenarios.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的机器学习应用中，满足不同用例（例如，模型的批量重新训练、超参数调整实验等）所需的工作流程可能会变得非常复杂。我们将探讨一些更复杂的工作流程，并抽象出可以重用于组合各种场景的结构模式。
- en: '5.2 Fan-in and fan-out patterns: Composing complex machine learning workflows'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 输入和输出模式：组合复杂的机器学习工作流程
- en: In chapter 3, we built a machine learning model to tag the main themes of new
    videos that the model hadn’t seen before using the YouTube-8M dataset. The YouTube-8M
    dataset consists of millions of YouTube video IDs, with high-quality machine-generated
    annotations from a diverse vocabulary of 3,800+ visual entities such as Food,
    Car, Music, etc. In chapter 4, we also discussed patterns that are helpful to
    build scalable model serving systems where users can upload new videos, and then
    the system loads the previously trained machine learning model to tag entities/themes
    that appear in the uploaded videos. In real-world applications, we often want
    to chain these steps together and package them in a way that can be easily reused
    and distributed.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3 章中，我们使用 YouTube-8M 数据集构建了一个机器学习模型，用于标记模型之前未见过的新闻视频的主要主题。YouTube-8M 数据集包含数百万个
    YouTube 视频ID，以及来自 3,800 多个视觉实体（如食品、汽车、音乐等）的高质量的机器生成注释。在第 4 章中，我们还讨论了有助于构建可扩展的模型服务系统的模式，用户可以上传新视频，然后系统加载之前训练好的机器学习模型来标记上传视频中的实体/主题。在实际应用中，我们通常希望将这些步骤链接起来，并以易于重用和分发的方式打包。
- en: For example, what if the original YouTube-8M dataset has been updated, and we’d
    like to train a new model from scratch using the same model architecture? In this
    case, it’s pretty easy to containerize each of these components and chain them
    together in a machine learning workflow that can be reused by re-executing the
    end-to-end workflow when the data gets updated. As shown in figure 5.6, new videos
    are regularly being added to the original YouTube-8M dataset, and the workflow
    is executed every time the dataset is updated. The next model training step trains
    the entity-tagging model using the most recent dataset. Then, the last model serving
    step uses the trained model to tag entities in unseen videos.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果原始 YouTube-8M 数据集已经更新，而我们想使用相同的模型架构从头开始训练一个新模型？在这种情况下，将每个组件容器化并将它们在机器学习工作流程中链接起来以供重用是非常容易的。当数据更新时，通过重新执行端到端工作流程来重用该工作流程。如图
    5.6 所示，新视频定期添加到原始 YouTube-8M 数据集中，并且每次数据集更新时都会执行工作流程。下一个模型训练步骤使用最新的数据集训练实体标记模型。然后，最后一个模型服务步骤使用训练好的模型对未见过的视频中的实体进行标记。
- en: '![05-06](../../OEBPS/Images/05-06.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![05-06](../../OEBPS/Images/05-06.png)'
- en: Figure 5.6 New videos are regularly added to the original YouTube-8M dataset,
    and the workflow is executed every time the dataset is updated.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 新视频定期添加到原始 YouTube-8M 数据集中，并且每次数据集更新时都会执行工作流程。
- en: Now, let’s take a look at a more complex real-world scenario. Let’s assume we
    know the implementation details for model training of any machine learning model
    architecture. We want to build a machine learning system to train different models.
    We then want to use the top two models to generate predictions so that the entire
    system is less likely to miss any entities in the videos since the two models
    may capture information from different perspectives.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一个更复杂的真实世界场景。假设我们知道任何机器学习模型架构的模型训练实现细节。我们希望构建一个机器学习系统来训练不同的模型。然后，我们希望使用前两个模型来生成预测，这样整个系统就不太可能错过视频中的任何实体，因为两个模型可能从不同的角度捕获信息。
- en: 5.2.1 The problem
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 问题
- en: We want to build a machine learning workflow that would train different models
    after the system has ingested data from the data source. Then, we want to select
    the top two models and use the knowledge from both to provide model serving that
    generates predictions for users.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望构建一个机器学习工作流程，在系统从数据源摄取数据后，能够训练不同的模型。然后，我们希望选择前两个模型，并利用这两个模型的知识来提供模型服务，为用户提供预测。
- en: Building a workflow that includes the end-to-end normal process of a machine
    learning system with only data ingestion, model training, and model serving, where
    each component only appears once as an individual step in the workflow, is pretty
    straightforward. However, in our particular scenario, the workflow is much more
    complex as we need to include multiple model training steps as well as multiple
    model serving steps. How do we formalize and generalize the structure of this
    complex workflow so that it can be easily packaged, reused, and distributed?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个包含机器学习系统端到端正常流程的工作流程，仅包括数据摄取、模型训练和模型服务，其中每个组件在每个工作流程步骤中仅作为单独的步骤出现一次，这相当直接。然而，在我们的特定场景中，工作流程要复杂得多，因为我们需要包括多个模型训练步骤以及多个模型服务步骤。我们如何形式化和一般化这个复杂工作流程的结构，以便它可以轻松打包、重用和分发？
- en: 5.2.2 The solution
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 解决方案
- en: Let’s start with the most basic machine learning workflow that includes only
    data ingestion, model training, and model serving, where each of these components
    only appears once as an individual step in the workflow. We will build our system
    based on this workflow to serve as our baseline, as shown in figure 5.7.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最基础的机器学习工作流程开始，该工作流程仅包括数据摄取、模型训练和模型服务，其中这些组件在每个工作流程步骤中仅作为单独的步骤出现一次。我们将基于此工作流程构建我们的系统，作为我们的基线，如图
    5.7 所示。
- en: '![05-07](../../OEBPS/Images/05-07.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![05-07](../../OEBPS/Images/05-07.png)'
- en: Figure 5.7 A baseline workflow including only data ingestion, model training,
    and model serving, where each of these components only appears once as an individual
    step in the workflow
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 包含仅数据摄取、模型训练和模型服务的基线工作流程，其中这些组件在每个工作流程步骤中仅出现一次作为单独的步骤
- en: 'Our goal is to represent the machine learning workflow that builds and selects
    the top two best-performing models that will be used for model serving to give
    better inference results. Let’s take a moment to understand why this approach
    might be used in practice. For example, figure 5.8 shows two models: the first
    model has knowledge of four entities, and the second model has knowledge of three
    entities. Thus, each can tag the entities it knows from the videos. We can use
    both models to tag entities at the same time and then aggregate their results.
    The aggregated result is obviously more knowledgeable and is able to cover more
    entities. In other words, two models can be more effective and produce more comprehensive
    entity-tagging results.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是表示构建和选择用于模型服务的表现最佳的两个模型的机器学习工作流程。让我们花点时间来理解为什么这种方法在实践中可能会被使用。例如，图 5.8
    显示了两个模型：第一个模型了解四个实体，第二个模型了解三个实体。因此，每个模型都可以从视频中标记它所知道的实体。我们可以同时使用这两个模型来标记实体，然后汇总它们的结果。汇总的结果显然更知识渊博，能够覆盖更多实体。换句话说，两个模型可以更有效，产生更全面的实体标记结果。
- en: '![05-08](../../OEBPS/Images/05-08.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![05-08](../../OEBPS/Images/05-08.png)'
- en: Figure 5.8 A diagram of models where the first model has knowledge of four entities
    and the second model has knowledge of three entities. Thus, each can tag the entities
    it knows from the videos. We can use both models to tag entities at the same time
    and then aggregate their results. The aggregated result covers more entities than
    each individual model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 模型图，第一个模型了解四个实体，第二个模型了解三个实体。因此，每个模型都可以从视频中标记它所知道的实体。我们可以同时使用这两个模型来标记实体，然后汇总它们的结果。汇总结果覆盖了比每个单独模型更多的实体。
- en: 'Now that we understand the motivation behind building this complex workflow,
    let’s look at an overview of the entire end-to-end workflow process. We want to
    build a machine learning workflow that performs the following functions sequentially:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们理解了构建这个复杂工作流程背后的动机，那么让我们看一下整个端到端工作流程过程的概述。我们希望构建一个机器学习工作流程，该工作流程按顺序执行以下功能：
- en: Ingests data from the same data source
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从相同的数据源摄取数据
- en: Trains multiple different models, either different sets of hyperparameters of
    the same model architecture or various model architectures
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练多个不同的模型，这些模型可以是同一模型架构的不同超参数集合，或者是各种模型架构。
- en: Picks the two top-performing models to be used for model serving for each of
    the trained models
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择两个表现最好的模型用于每个训练模型的模型服务
- en: Aggregates the models’ results of the two model serving systems to present to
    users
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将两个模型服务系统的模型结果汇总以呈现给用户
- en: Let’s first add some placeholders to the baseline workflow for multiple model
    training steps after data ingestion. We can then add multiple model serving steps
    once the multiple model training steps finish. A diagram of the enhanced baseline
    workflow is shown in figure 5.9.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先在数据摄取后的基线工作流程中添加一些占位符以用于多个模型训练步骤。一旦多个模型训练步骤完成，我们就可以添加多个模型服务步骤。增强型基线工作流程的图示如图5.9所示。
- en: '![05-09](../../OEBPS/Images/05-09.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![05-09](../../OEBPS/Images/05-09.png)'
- en: Figure 5.9 A diagram of the enhanced baseline workflow where multiple model
    training steps occur after data ingestion, followed by multiple model serving
    steps
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 增强型基线工作流程图，在数据摄取之后发生多个模型训练步骤，随后是多个模型服务步骤
- en: The key difference from what we’ve dealt with before in the baseline is the
    presence of multiple model training and model serving components. The steps do
    not have direct, one-to-one relationships. For example, each model training step
    may be connected to a single model serving step or not connected to any steps
    at all. Figure 5.10 shows that the models trained from the first two model training
    steps outperform the model trained from the third model training step. Thus, only
    the first two model training steps are connected to the model serving steps.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在基线中之前处理的不同之处在于存在多个模型训练和模型服务组件。步骤之间没有直接的、一对一的关系。例如，每个模型训练步骤可能连接到单个模型服务步骤，也可能不连接到任何步骤。图5.10显示，从前两个模型训练步骤训练的模型优于从第三个模型训练步骤训练的模型。因此，只有前两个模型训练步骤连接到模型服务步骤。
- en: '![05-10](../../OEBPS/Images/05-10.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![05-10](../../OEBPS/Images/05-10.png)'
- en: Figure 5.10 The models trained from the first two model training steps outperform
    the model trained from the third model training step. Thus, only the first two
    model training steps are connected to the model serving steps.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 从前两个模型训练步骤训练的模型优于从第三个模型训练步骤训练的模型。因此，只有前两个模型训练步骤连接到模型服务步骤。
- en: We can compose this workflow as follows. On successful data ingestion, multiple
    model training steps are connected to the data ingestion step so that they can
    use the shared data that’s ingested and cleaned from the original data source.
    Next, a single step is connected to the model training steps to select the top
    two performing models. It produces two model serving steps that use the selected
    models to handle model serving requests from users. A final step at the end of
    this machine learning workflow is connected to the two model serving steps to
    aggregate the model inference results that will be presented to the users.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样构建这个工作流程。在成功的数据摄取后，多个模型训练步骤连接到数据摄取步骤，以便它们可以使用从原始数据源摄取并清洗的共享数据。接下来，一个步骤连接到模型训练步骤以选择表现最好的两个模型。它产生两个模型服务步骤，使用所选模型处理来自用户的模型服务请求。在这个机器学习工作流程的末尾，一个最终步骤连接到两个模型服务步骤，以汇总将呈现给用户的模型推理结果。
- en: A diagram of the complete workflow is shown in figure 5.11\. This workflow trains
    different models via three model training steps resulting in varying accuracy
    when tagging entities. A model selection step picks the top two models with at
    least 90% accuracy trained from the first two model training steps that will be
    used in the following two separate model serving steps. The results from the two
    model serving steps are then aggregated to present to users via a result aggregation
    step.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 完整工作流程的图示显示在图5.11中。该工作流程通过三个模型训练步骤训练不同的模型，在实体标注时产生不同的准确率。一个模型选择步骤从前两个模型训练步骤中选取至少90%准确率的两个顶级模型，这些模型将在接下来的两个单独的模型服务步骤中使用。然后，通过结果聚合步骤将两个模型服务步骤的结果汇总，以供用户查看。
- en: '![05-11](../../OEBPS/Images/05-11.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![05-11](../../OEBPS/Images/05-11.png)'
- en: Figure 5.11 A machine learning workflow that trains different models that result
    in varying accuracy when tagging entities and then selects the top two models
    with at least 90% accuracy to be used for model serving. The results from the
    two model serving steps are then aggregated to present to users.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 一个机器学习工作流程，该工作流程训练不同的模型，在实体标注时产生不同的准确率，然后选择至少90%准确率的两个顶级模型用于模型服务。然后，通过结果聚合步骤将两个模型服务步骤的结果汇总，以供用户查看。
- en: We can abstract out two patterns from this complex workflow. The first one we
    observe is the *fan-out* pattern. Fan-out describes the process of starting multiple
    separate steps to handle input from the workflow. In our workflow, the fan-out
    pattern appears when multiple separate model training steps connect to the data
    ingestion step, as shown in figure 5.12.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从这个复杂的工作流程中抽象出两种模式。首先观察到的是*扇出*模式。扇出描述了启动多个独立的步骤来处理工作流程输入的过程。在我们的工作流程中，如图5.12所示，当多个独立的模型训练步骤连接到数据摄取步骤时，就会出现扇出模式。
- en: '![05-12](../../OEBPS/Images/05-12.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![05-12](../../OEBPS/Images/05-12.png)'
- en: Figure 5.12 A diagram of the fan-out pattern that appears when multiple separate
    model training steps are connected to the data ingestion step
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 当多个独立的模型训练步骤连接到数据摄取步骤时出现的扇出模式图示
- en: There’s also the *fan-in* pattern in our workflow, where we have one single
    aggregation step that combines the results from the two model serving steps, as
    shown in figure 5.13\. Fan-in describes the process of combining results from
    multiple steps into one step.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的工作流程中，也存在*扇入*模式，其中我们有一个单一的聚合步骤，将两个模型服务步骤的结果合并，如图5.13所示。扇入描述了将多个步骤的结果合并到一个步骤的过程。
- en: '![05-13](../../OEBPS/Images/05-13.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![05-13](../../OEBPS/Images/05-13.png)'
- en: Figure 5.13 A diagram of the fan-in pattern, where we have one single aggregation
    step that combines the results from the two model serving steps
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 扇入模式的图示，其中我们有一个单一的聚合步骤，将两个模型服务步骤的结果合并
- en: Formalizing these patterns would help us build and organize more complex workflows
    by using different patterns for workflows based on real-world requirements.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些模式形式化将有助于我们通过使用不同模式来构建和组织更复杂的工作流程，这些模式基于现实世界的需求。
- en: We have successfully built the system as a complex workflow that trains different
    models and then uses the top two models to generate predictions so that the entire
    system is less likely to miss any entities in the videos. These patterns are powerful
    when constructing complex workflows to meet real-world requirements. We can construct
    various workflows, from a single data processing step to multiple model training
    steps to train different models with the same dataset. We can also start more
    than one model serving step from each of these model training steps if the predictions
    from different models are useful in real-world applications. We’ll apply this
    pattern in section 9.4.1.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功构建了一个复杂的工作流程，该工作流程训练不同的模型，然后使用顶级的前两个模型生成预测，从而使整个系统在视频中不太可能遗漏任何实体。当构建满足现实世界需求复杂工作流程时，这些模式非常强大。我们可以构建各种工作流程，从单一的数据处理步骤到多个模型训练步骤，以使用相同的数据集训练不同的模型。如果不同模型的预测在现实世界应用中有用，我们还可以从每个模型训练步骤启动多个模型服务步骤。我们将在第9.4.1节中应用此模式。
- en: 5.2.3 Discussion
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 讨论
- en: By using the fan-in and fan-out patterns in the system, the system is now able
    to execute complex workflows that train multiple machine learning models and pick
    the most performant ones to provide good entity-tagging results in the model serving
    system.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在系统中使用扇入和扇出模式，系统现在能够执行复杂的流程，这些流程训练多个机器学习模型，并选择性能最佳的模型以在模型服务系统中提供良好的实体标注结果。
- en: 'These patterns are great abstractions that can be incorporated into very complex
    workflows to meet the increasing demand for complex distributed machine learning
    workflows in the real world. But what kind of workflows are suitable for the fan-in
    and fan-out patterns? In general, if both of the following applies, we can consider
    incorporating these patterns:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模式是很好的抽象，可以融入非常复杂的流程中，以满足现实世界中日益增长的复杂分布式机器学习流程的需求。但哪些工作流程适合扇入和扇出模式？一般来说，如果以下两个条件都适用，我们可以考虑融入这些模式：
- en: The multiple steps that we are fanning-in or fanning-out are independent of
    each other.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在扇入或扇出的多个步骤是相互独立的。
- en: It takes a long time for these steps to run sequentially.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些步骤按顺序运行需要很长时间。
- en: The multiple steps need to be order-independent because we don’t know the order
    in which concurrent copies of those steps will run or the order in which they
    will return. For example, if the workflow also contains a step that trains an
    ensemble of other models (also known as *ensemble learning*; [http://mng.bz/N2vn](http://mng.bz/N2vn))
    to provide a better-aggregated model, this ensemble model depends on the completion
    of other model training steps. Consequently, we cannot use the fan-in pattern
    because the ensemble model training step will need to wait for other model training
    to complete before it can start running, which would require some extra waiting
    and delay the entire workflow.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 多个步骤需要是无序的，因为我们不知道这些步骤的并发副本将按什么顺序运行，也不知道它们将按什么顺序返回。例如，如果工作流程还包含一个训练其他模型集合（也称为*集成学习*；[http://mng.bz/N2vn](http://mng.bz/N2vn)）的步骤，以提供更好的聚合模型，这个集成模型依赖于其他模型训练步骤的完成。因此，我们不能使用扇入模式，因为集成模型训练步骤需要在其他模型训练完成后才能开始运行，这会需要额外的等待并延迟整个工作流程。
- en: Ensemble models
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型
- en: An ensemble model uses multiple machine learning models to obtain better predictive
    performance than could be obtained from any of the constituent models alone. It
    often consists of a number of alternative models that can learn the relationships
    in the dataset from different perspectives.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型使用多个机器学习模型来获得比任何单个组成模型单独获得的更好的预测性能。它通常由多个替代模型组成，可以从不同的角度学习数据集中的关系。
- en: Ensemble models tend to yield better results when diversity among the constituent
    models is significant. Therefore, many ensemble approaches try to increase the
    diversity of the models they combine.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当组成模型之间的多样性显著时，集成模型往往会产生更好的结果。因此，许多集成方法试图增加它们组合的模型的多样性。
- en: The fan-in and fan-out patterns can create very complex workflows that meet
    most of the requirements of machine learning systems. However, to achieve good
    performance on those complex workflows, we need to determine which parts of the
    workflows to run first and which parts of the workflows can be executed in parallel.
    As a result of the optimization, data science teams would spend less time waiting
    for workflows to complete, thus reducing infrastructure costs. I will introduce
    some patterns to help us organize the steps in the workflow from a computational
    perspective in the next section.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 扇入和扇出模式可以创建非常复杂的流程，满足机器学习系统的大部分需求。然而，为了在这些复杂的流程上实现良好的性能，我们需要确定哪些流程部分应该首先运行，哪些流程部分可以并行执行。由于优化，数据科学团队将花费更少的时间等待流程完成，从而降低基础设施成本。在下一节中，我将介绍一些模式，帮助我们从计算角度组织工作流程中的步骤。
- en: 5.2.4 Exercises
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.4 练习
- en: If the steps are not independent of each other, can we use the fan-in or fan-out
    patterns?
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果步骤之间不是相互独立的，我们能否使用扇入或扇出模式？
- en: What’s the main problem when trying to build ensemble models with the fan-in
    pattern?
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用扇入模式构建集成模型时，主要问题是什么？
- en: '5.3 Synchronous and asynchronous patterns: Accelerating workflows with concurrency'
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 同步和异步模式：通过并发加速工作流程
- en: Each model training step in the system takes a long time to complete; however,
    their durations may vary across different model architectures or model parameters.
    Imagine an extreme case where one of the model training steps takes two weeks
    to complete since it is training a complex machine learning model that requires
    a huge amount of computational resources. All other model training steps only
    take one week to complete. Many of the steps, such as model selection and model
    serving, in the machine learning workflow we built earlier that uses the fan-in
    and fan-out patterns will have to wait an additional week until this long-running
    model training step is completed. A diagram that illustrates the duration differences
    among the three model training steps is shown in figure 5.14.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 系统中的每个模型训练步骤都需要很长时间才能完成；然而，它们的持续时间可能因不同的模型架构或模型参数而异。想象一个极端情况，其中一个模型训练步骤需要两周时间才能完成，因为它正在训练一个需要大量计算资源的复杂机器学习模型。我们之前构建的机器学习工作流程中，许多步骤，如模型选择和模型服务，使用扇入和扇出模式，将不得不额外等待一周，直到这个长时间运行的模型训练步骤完成。图
    5.14 展示了三个模型训练步骤之间持续时间差异的示意图。
- en: '![05-14](../../OEBPS/Images/05-14.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![05-14](../../OEBPS/Images/05-14.png)'
- en: Figure 5.14 A workflow that illustrates the duration differences for the three
    model training steps
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14 一个说明三个模型训练步骤持续时间差异的工作流程
- en: In this case, since the model selection step and the steps following it require
    all model training steps to finish, the model training step that takes two weeks
    to complete will slow down the workflow by an entire week. We would rather use
    that additional week to re-execute all the model training steps that take one
    week to complete instead of wasting time waiting for one step!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，由于模型选择步骤及其后续步骤需要所有模型训练步骤完成，因此需要两周时间完成的模型训练步骤将使工作流程整体慢一周。我们宁愿用那额外的一周重新执行所有只需一周完成的模型训练步骤，而不是浪费时间等待一个步骤！
- en: 5.3.1 The problem
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 问题
- en: We want to build a machine learning workflow that trains different models and
    then selects the top two models to use for model serving, which generates predictions
    based on the knowledge of both models. Due to varying completion times for each
    model training step in the existing machine learning workflow, the start of the
    following steps, such as the model selection step and the model serving, depends
    on the completion of the previous steps.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望构建一个机器学习工作流程，该工作流程训练不同的模型，然后选择前两个模型用于模型服务，该服务基于两个模型的知识生成预测。由于现有机器学习工作流程中每个模型训练步骤的完成时间不同，因此后续步骤，如模型选择步骤和模型服务，的开始依赖于前一步骤的完成。
- en: However, a problem occurs when at least one of the model training steps takes
    much longer to complete than the remaining steps because the model selection step
    that follows can only start after this long model training step has completed.
    As a result, the entire workflow is delayed by this particularly long-running
    step. Is there a way to accelerate this workflow so it will not be affected by
    the duration of individual steps?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当至少有一个模型训练步骤的完成时间比其他步骤长得多时，会出现问题，因为后续的模型选择步骤只能在长时间运行的模型训练步骤完成后才能开始。结果，整个工作流程因此特别长时间运行的步骤而延迟。有没有办法加速这个工作流程，使其不会受到单个步骤持续时间的影响？
- en: 5.3.2 The solution
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 解决方案
- en: We want to build the same machine learning workflow as we did previously, which
    would train different models after the system has ingested data from the data
    source, select the top two models, and then use these two models to provide model
    serving to generate predictions using knowledge from both models.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望构建与之前相同的工作流程，该工作流程在系统从数据源摄取数据后，训练不同的模型，选择前两个模型，然后使用这两个模型提供模型服务，以生成使用两个模型知识的预测。
- en: However, this time we noticed a performance bottleneck because the start of
    each following step, such as model selection and model serving, depends on the
    completion of its previous steps. In our case, we have one long-running model
    training step that must complete before we can proceed to the next step.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这次我们注意到一个性能瓶颈，因为每个后续步骤的开始，例如模型选择和模型服务，都依赖于其前一步骤的完成。在我们的案例中，我们有一个必须完成才能进行下一步的长时间运行的模型训练步骤。
- en: What if we can exclude the long-running model training step completely? Once
    we do that, the rest of the model training steps will have consistent completion
    times. Thus, the remaining steps in the workflow can be executed without waiting
    for a particular step that’s still running. A diagram of the updated workflow
    is shown in figure 5.15.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以完全排除长时间运行的模型训练步骤，会怎样？一旦我们这样做，其余的模型训练步骤将具有一致的完成时间。因此，工作流程中剩余的步骤可以在等待某个仍在运行的特定步骤之前执行。更新后的工作流程图如图5.15所示。
- en: '![05-15](../../OEBPS/Images/05-15.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![05-15](../../OEBPS/Images/05-15.png)'
- en: Figure 5.15 The new workflow after the long-running model training step has
    been removed
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15 移除长时间运行的模型训练步骤后的新工作流程
- en: This naive approach may resolve our problem of extra waiting time for long-running
    steps. However, our original goal was to use this type of complex workflow to
    experiment with different machine learning model architectures and different sets
    of hyperparameters of those models to select the best-performing models to use
    for model serving. If we simply exclude the long-running model training step,
    we are essentially throwing away the opportunity to experiment with advanced models
    that may better capture the entities in the videos.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简单的方法可能解决了我们长时间等待长运行步骤的问题。然而，我们的原始目标是使用这种复杂的流程来实验不同的机器学习模型架构和这些模型的超参数集，以选择最佳性能的模型用于模型服务。如果我们简单地排除长运行模型训练步骤，我们实际上是在放弃实验可能更好地捕捉视频实体的高级模型的机会。
- en: Is there a better way to speed up the workflow so that it will not be affected
    by the duration of this individual step? Let’s focus on the model training steps
    that only take one week to complete. What can we do when those short-running model
    training steps are complete?
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 是否有更好的方法来加速工作流程，使其不会受到单个步骤持续时间的影响？让我们专注于那些只需一周就能完成的模型训练步骤。当这些短时间运行的模型训练步骤完成后，我们能做什么？
- en: When a model training step finishes, we have successfully obtained a trained
    machine learning model. In fact, we can use this trained model in our model serving
    system without waiting for the rest of the model training steps to complete. As
    a result, the users can see the results of tagged entities from their model serving
    requests that contain videos as soon as we have trained one model from one of
    the steps in the workflow. A diagram of this workflow is shown in figure 5.16.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型训练步骤完成时，我们已经成功获得了一个训练好的机器学习模型。实际上，我们可以在模型服务系统中使用这个训练好的模型，而无需等待模型训练步骤的其余部分完成。因此，当我们在工作流程中的某个步骤中训练了一个模型后，用户就可以看到他们模型服务请求中包含的视频的标记实体结果。这个工作流程的图如图5.16所示。
- en: '![05-16](../../OEBPS/Images/05-16.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![05-16](../../OEBPS/Images/05-16.png)'
- en: Figure 5.16 A workflow where the trained model from a short-running model training
    step is applied directly to our model serving system without waiting for the remaining
    model training steps to complete
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.16 在短时间运行的模型训练步骤中训练的模型直接应用于我们的模型服务系统，无需等待剩余的模型训练步骤完成的工作流程
- en: After a second model training step finishes, we can then pass the two trained
    models directly to model serving. The aggregated inference results are presented
    to users instead of the results from only the model we obtained initially, as
    shown in figure 5.17.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个模型训练步骤完成后，我们可以直接将两个训练好的模型传递给模型服务。向用户展示的是综合推理结果，而不是仅从最初获得的模型中得到的推理结果，如图5.17所示。
- en: '![05-17](../../OEBPS/Images/05-17.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![05-17](../../OEBPS/Images/05-17.png)'
- en: Figure 5.17 After a second model training step finishes, we pass the two trained
    models directly to model serving. The aggregated inference results are presented
    to users instead of only the results from the model that we obtained initially.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.17 在第二个模型训练步骤完成后，我们直接将两个训练好的模型传递给模型服务。向用户展示的是综合推理结果，而不仅仅是最初获得的模型的推理结果。
- en: Note that while we can continue to use the trained models for model selection
    and model serving, the long-running model training step is still running. In other
    words, the steps are executed *asynchronously*—they don’t depend on each other’s
    completion. The workflow starts executing the next step before the previous step
    finishes.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，虽然我们可以继续使用训练好的模型进行模型选择和模型服务，但长时间运行的模型训练步骤仍在进行中。换句话说，这些步骤是**异步执行**的——它们之间不依赖于彼此的完成。工作流程在上一步骤完成之前就开始执行下一步骤。
- en: Sequential steps are performed one at a time, and only when one has completed
    does the following step become unblocked. In other words, you must wait for a
    step to finish to move to the next one. For example, the data ingestion step must
    be completed before we start any of the model training steps.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序步骤一次执行一个，并且只有当上一个步骤完成时，下一个步骤才会解除阻塞。换句话说，您必须等待一个步骤完成才能进行下一个步骤。例如，数据摄取步骤必须完成，我们才能开始任何模型训练步骤。
- en: Contrary to asynchronous steps, synchronous steps can start running at the same
    time once dependencies are met. For example, the model training steps can run
    concurrently, as soon as the previous data ingestion step has finished. A different
    model training step does not have to wait for another to start. The synchronous
    pattern is typically useful when you have multiple similar workloads that can
    run concurrently and finish near the same time.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 与异步步骤相反，一旦满足依赖关系，同步步骤可以同时开始运行。例如，模型训练步骤可以并发运行，一旦之前的数据摄取步骤完成即可。不同的模型训练步骤不需要等待另一个开始。同步模式通常在您有多个可以并发运行且几乎同时完成的相似工作负载时非常有用。
- en: By incorporating these patterns, the entire workflow will no longer be blocked
    by the long-running model training step. Instead, it can continue using the already-trained
    models from the short-running model training steps in the model serving system,
    which can start handling users’ model serving requests.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合这些模式，整个工作流程将不再被长时间运行的模型训练步骤阻塞。相反，它可以使用模型服务系统中已经训练好的模型，这些模型可以开始处理用户的模型服务请求。
- en: The synchronous and asynchronous patterns are also extremely useful in other
    distributed systems to optimize system performance and maximize the use of existing
    computational resources—especially when the amount of computational resources
    for heavy workloads is limited. We’ll apply this pattern in section 9.4.1.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 同步和异步模式在其他分布式系统中也非常有用，可以优化系统性能并最大化现有计算资源的使用——尤其是在重负载的计算资源有限时。我们将在第 9.4.1 节中应用此模式。
- en: 5.3.3 Discussion
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.3 讨论
- en: By mixing synchronous and asynchronous patterns, we can create more efficient
    machine learning workflows and avoid any delays due to steps that prevent others
    from executing, such as a long-running model training step. However, the models
    trained from the short-running model training steps may not be very accurate.
    That is, the models with simpler architectures may not discover as many entities
    in the videos as the more complex model of the long-running model training step
    (figure 5.18).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过混合同步和异步模式，我们可以创建更高效的机器学习工作流程，并避免由于阻止其他步骤执行而导致的任何延迟，例如长时间运行的模型训练步骤。然而，来自时间较短的模型训练步骤训练的模型可能不太准确。也就是说，具有更简单架构的模型可能不会在视频中识别出像长时间运行的模型训练步骤（图
    5.18）中更复杂的模型那样多的实体。
- en: '![05-18](../../OEBPS/Images/05-18.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![05-18](../../OEBPS/Images/05-18.png)'
- en: Figure 5.18 A model trained from two finished short-running model training steps
    with very simple models that serve as a baseline. They can only identify a small
    number of entities, whereas the model trained from the most time-consuming step
    can identify many more entities.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.18 由两个完成的时间较短的模型训练步骤训练的模型，这些步骤使用了非常简单的模型作为基线。它们只能识别少量实体，而来自耗时最长的步骤训练的模型可以识别更多的实体。
- en: As a result, we should keep in mind that the models we get early on may not
    be the best and may only be able to tag a small number of entities, which may
    not be satisfactory to our users.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们应该记住，我们早期得到的模型可能不是最好的，可能只能标记少量实体，这可能不会满足我们的用户需求。
- en: When we deploy this end-to-end workflow to real-world applications, we need
    to consider whether users seeing inference results faster or seeing better results
    is more important. If the goal is to allow users to see the inference results
    as soon as a new model is available, they may not see the results they were expecting.
    However, if users can tolerate a certain period of delay, it’s better to wait
    for more model training steps to finish. Then, we can be selective about the models
    we’ve trained and pick the best-performing models that provide very good entity-tagging
    results. Whether a delay is acceptable is subject to the requirements of real-world
    applications.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将这个端到端工作流程部署到现实世界应用中时，我们需要考虑用户看到推理结果更快还是看到更好的结果更重要。如果目标是允许用户在新模型可用时立即看到推理结果，他们可能看不到他们预期的结果。然而，如果用户可以容忍一定程度的延迟，等待更多的模型训练步骤完成会更好。然后，我们可以选择性地挑选我们训练的模型，并选择那些提供非常好的实体标记结果的性能最好的模型。是否可以接受延迟取决于现实世界应用的要求。
- en: By using synchronous and asynchronous patterns, we can organize the steps in
    machine learning workflows from structural and computational perspectives. As
    a result, data science teams can spend less time waiting for workflows to complete
    to maximize performance, thus reducing infrastructure costs and idling computational
    resources. In the next section, we’ll introduce another pattern used very often
    in real-world systems that can save more computational resources and make workflows
    run even faster.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用同步和异步模式，我们可以从结构和计算的角度组织机器学习工作流程中的步骤。因此，数据科学团队可以花费更少的时间等待工作流程完成，以最大化性能，从而降低基础设施成本和闲置的计算资源。在下一节中，我们将介绍在现实世界系统中经常使用的一种模式，它可以节省更多的计算资源，并使工作流程运行得更快。
- en: 5.3.4 Exercises
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.4 练习
- en: What causes each step of the model training steps to start?
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练步骤的每个步骤是由什么引起的？
- en: Are the steps blocking each other if they are running asynchronously?
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果步骤异步运行，它们会相互阻塞吗？
- en: What do we need to consider when deciding whether we want to use any available
    trained model as early as possible?
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在决定是否尽可能早地使用任何可用的训练模型时，我们需要考虑什么？
- en: '5.4 Step memoization pattern: Skipping redundant workloads via memoized steps'
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 步骤记忆化模式：通过记忆化步骤跳过冗余工作负载
- en: With the fan-in and fan-out patterns in the workflow, the system can execute
    complex workflows that train multiple machine learning models and pick the most
    performant models to provide good entity-tagging results in the model serving
    system. The workflows we’ve seen in this chapter contain only a single data ingestion
    step. In other words, the data ingestion step in the workflows always executes
    first before the remaining steps, such as model training and model serving, can
    begin to process.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在工作流程中采用扇入和扇出模式后，系统可以执行复杂的流程，这些流程训练多个机器学习模型，并选择性能最好的模型在模型服务系统中提供良好的实体标记结果。我们在这章中看到的工作流程只包含一个数据摄入步骤。换句话说，在工作流程中，数据摄入步骤总是首先执行，然后剩余的步骤，如模型训练和模型服务，才能开始处理。
- en: Unfortunately, in real-world machine learning applications, the dataset does
    not always remain unchanged. Now, imagine that new YouTube videos are becoming
    available and are being added to the YouTube-8M dataset every week. Following
    our existing workflow architecture, if we would like to retrain the model so that
    it accounts for the additional videos that arrive on a regular basis, we need
    to run the entire workflow regularly from scratch—from the data ingestion step
    to the model serving step—as shown in figure 5.19.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在现实世界的机器学习应用中，数据集并不总是保持不变。现在，想象一下新的YouTube视频每周都在变得可用，并被添加到YouTube-8M数据集中。按照我们现有的工作流程架构，如果我们想重新训练模型以考虑定期到达的额外视频，我们需要定期从头开始运行整个工作流程——从数据摄入步骤到模型服务步骤，如图5.19所示。
- en: '![05-19](../../OEBPS/Images/05-19.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![05-19](../../OEBPS/Images/05-19.png)'
- en: Figure 5.19 A diagram of the entire workflow that is re-executed every time
    the dataset is updated
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.19 每次数据集更新时重新执行的整体工作流程图
- en: Say the dataset does not change, but we want to experiment with new model architectures
    or new sets of hyperparameters, which is very common for machine learning practitioners
    (figure 5.20). For example, we may change the model architecture from simple linear
    models to more complex models such as tree-based models or convolutional neural
    networks. We can also stick with the particular model architecture we’ve used
    and only change the set of model hyperparameters, such as the number of layers
    and hidden units in each of those layers for neural network models or the maximum
    depth of each tree for tree-based models. For cases like these, we still need
    to run the end-to-end workflow, which includes the data ingestion step to re-ingest
    the data from the original data source from scratch. Performing data ingestion
    again is very time-consuming.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数据集没有变化，但我们要尝试新的模型架构或新的超参数集，这在机器学习从业者中非常常见（见图5.20）。例如，我们可能将模型架构从简单的线性模型更改为更复杂的模型，如基于树的模型或卷积神经网络。我们也可以坚持使用我们使用的特定模型架构，并仅更改模型超参数集，例如神经网络模型中每层的层数和隐藏单元的数量，或基于树的模型中每棵树的深度。对于这些情况，我们仍然需要运行端到端的流程，包括从原始数据源从头开始重新摄入数据的步骤。再次进行数据摄入是非常耗时的。
- en: '![05-20](../../OEBPS/Images/05-20.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![05-20](../../OEBPS/Images/05-20.png)'
- en: Figure 5.20 A diagram where the entire workflow is re-executed every time we
    experiment with a new model type or hyperparameter even though the dataset has
    not changed
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.20 每次我们尝试新的模型类型或超参数时，即使数据集没有变化，整个流程都需要重新执行。
- en: 5.4.1 The problem
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 问题
- en: Machine learning workflows usually start with a data ingestion step. If the
    dataset is being updated regularly, we may want to rerun the entire workflow to
    train a fresh machine learning model that takes the new data into account. To
    do so, we need to execute the data ingestion step every time. Alternatively, if
    the dataset is not updated, but we want to experiment with new models, we still
    need to execute the entire workflow, including the data ingestion step. However,
    the data ingestion step can take a long time to complete depending on the size
    of the dataset. Is there a way to make this workflow more efficient?
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习流程通常从数据摄入步骤开始。如果数据集正在定期更新，我们可能希望重新运行整个流程以训练一个考虑新数据的全新机器学习模型。为此，我们需要每次都执行数据摄入步骤。或者，如果数据集没有更新，但我们要尝试新的模型，我们仍然需要执行整个流程，包括数据摄入步骤。然而，数据摄入步骤可能需要很长时间才能完成，这取决于数据集的大小。有没有办法使这个流程更高效？
- en: 5.4.2 The solution
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 解决方案
- en: Given how time-consuming data ingestion steps usually are, we probably don’t
    want to re-execute it to retrain or update our entity tagging models every time
    the workflow runs. Let’s first think about the root cause of this problem. The
    dataset of YouTube videos is being updated regularly, and the new data is persisted
    to the data source on a regular basis (e.g., once a month).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据摄入步骤通常非常耗时，我们可能不想每次流程运行时都重新执行它来重新训练或更新我们的实体标记模型。让我们首先考虑这个问题的根本原因。YouTube视频的数据集正在定期更新，并且新数据定期持久化到数据源上（例如，每月一次）。
- en: 'We have two use cases in which we need to re-execute the entire machine learning
    workflow:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个用例需要重新执行整个机器学习流程：
- en: After the dataset has been updated, rerun the workflow to train a new model
    that uses the updated dataset.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集更新后，重新运行流程以训练一个使用更新数据集的新模型。
- en: We want to experiment with a new model architecture using that dataset that’s
    already ingested, which may not have been updated yet.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想使用已经摄入的数据集尝试新的模型架构，这些数据可能尚未更新。
- en: The fundamental problem is the time-consuming data ingestion step. With the
    current workflow architecture, the data ingestion step will need to be executed
    regardless of whether the dataset has been updated.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 基本问题是耗时的数据摄入步骤。在当前的流程架构中，无论数据集是否已更新，数据摄入步骤都需要执行。
- en: Ideally, if the new data has not been updated, we don’t want to re-ingest the
    data that’s already collected. In other words, we would like to execute the data
    ingestion step only when we know that the dataset has been updated, as shown in
    figure 5.21.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，如果新数据尚未更新，我们不想重新摄入已经收集的数据。换句话说，我们希望在知道数据集已更新时才执行数据摄入步骤，如图5.21所示。
- en: '![05-21](../../OEBPS/Images/05-21.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![05-21](../../OEBPS/Images/05-21.png)'
- en: Figure 5.21 A diagram where the data ingestion step is skipped when the dataset
    has not been updated
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.21 当数据集未更新时跳过数据摄入步骤的示意图
- en: Now the challenge comes down to determining whether the dataset has been updated.
    Once we have a way to identify that, we can conditionally reconstruct the machine
    learning workflow and control whether we want to include a data ingestion step
    to be re-executed (figure 5.21).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的挑战在于确定数据集是否已被更新。一旦我们有了识别这一点的办法，我们就可以有条件地重建机器学习工作流程，并控制是否想要重新执行数据摄入步骤（图5.21）。
- en: One way to identify whether the dataset has been updated is through the use
    of cache. Since our dataset is being updated regularly on a fixed schedule (e.g.,
    once a month), we can create a *time-based cache* that stores the location of
    the ingested and cleaned dataset (assuming the dataset is located in a remote
    database) and the timestamp of its last updated time. The data ingestion step
    in the workflow will then be constructed and executed dynamically based on whether
    the last updated timestamp is within a particular window. For example, if the
    time window is set to two weeks, we consider the ingested data as fresh if it
    has been updated within the past two weeks. The data ingestion step will be skipped,
    and the following model training steps will use the already-ingested dataset from
    the location that’s stored in the cache.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 识别数据集是否已被更新的方法之一是通过使用缓存。由于我们的数据集正在按照固定的时间表定期更新（例如，每月一次），我们可以创建一个基于时间的缓存，存储摄入和清洗后的数据集的位置以及其最后更新时间戳。工作流程中的数据摄入步骤将根据最后更新时间戳是否在特定窗口内动态构建和执行。例如，如果时间窗口设置为两周，那么如果数据在过去两周内更新，我们就认为摄入的数据是新鲜的。数据摄入步骤将被跳过，接下来的模型训练步骤将直接使用缓存中存储的已摄入数据集。
- en: Figure 5.22 illustrates the case where a workflow has been triggered, and we
    check whether the data has been updated within the last two weeks by accessing
    the cache. If the data is fresh, we skip the execution of the unnecessary data
    ingestion step and execute the model training step directly.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.22说明了工作流程已被触发，并且通过访问缓存检查数据是否在过去两周内更新。如果数据是新鲜的，我们将跳过执行不必要的摄入步骤，并直接执行模型训练步骤。
- en: '![05-22](../../OEBPS/Images/05-22.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![05-22](../../OEBPS/Images/05-22.png)'
- en: Figure 5.22 The workflow has been triggered, and we check whether the data has
    been updated within the last two weeks by accessing the cache. If the data is
    fresh, we skip the execution of the unnecessary data ingestion step and execute
    the model training step directly.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.22 工作流程已被触发，通过访问缓存检查数据是否在过去两周内更新。如果数据是新鲜的，我们将跳过执行不必要的摄入步骤，并直接执行模型训练步骤。
- en: The time window can be used to control how old a cache can be before we consider
    the dataset fresh enough to be used directly for model training instead of re-ingesting
    the data again from scratch.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 时间窗口可以用来控制缓存可以有多旧，在我们认为数据集足够新鲜可以直接用于模型训练而不是重新从头开始摄入数据之前。
- en: Alternatively, we can store some of the important metadata about the data source
    in the cache, such as the number of records in the original data source currently
    available. This type of cache is called *content-based cache* since it stores
    information extracted from a particular step, such as the input and output information.
    With this type of cache, we can identify whether the data source has significant
    changes (e.g., the number of original records has doubled in the data source).
    If there’s a significant change, it’s usually a signal to re-execute the data
    ingestion step since the current dataset is very old and outdated. A workflow
    that illustrates this approach is shown in figure 5.23.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以在缓存中存储一些关于数据源的重要元数据，例如当前可用的原始数据源中的记录数。这种类型的缓存被称为基于内容的缓存，因为它存储从特定步骤提取的信息，例如输入和输出信息。有了这种类型的缓存，我们可以识别数据源是否有重大变化（例如，数据源中的原始记录数翻倍）。如果有重大变化，通常是一个重新执行数据摄入步骤的信号，因为当前的数据集非常旧且过时。图5.23展示了说明这种方法的流程图。
- en: '![05-23](../../OEBPS/Images/05-23.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![05-23](../../OEBPS/Images/05-23.png)'
- en: Figure 5.23 The workflow has been triggered, and we check whether the metadata
    collected from the dataset, such as the number of records in the dataset, has
    changed significantly. If it’s not significant, we then skip the execution of
    the unnecessary data ingestion step and execute the model training step directly.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.23 工作流程已被触发，我们检查从数据集中收集的元数据，例如数据集中的记录数是否发生了显著变化。如果没有显著变化，我们就跳过执行不必要的数据处理步骤，直接执行模型训练步骤。
- en: This pattern, which uses the cache to determine whether a step should be executed
    or skipped, is called *step memoization*. With the help of step memoization, a
    workflow can identify the steps with redundant workloads that can be skipped without
    being re-executed and thus greatly accelerate the execution of the end-to-end
    workflow. We’ll apply this pattern in section 9.4.2.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这种使用缓存来确定是否执行步骤或跳过的模式被称为*步骤记忆化*。借助步骤记忆化，工作流程可以识别出那些可以跳过而不需要重新执行的重负载步骤，从而大大加速端到端工作流程的执行。我们将在第9.4.2节中应用这个模式。
- en: 5.4.3 Discussion
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.3 讨论
- en: In real-world machine learning applications, many workloads besides data ingestion
    are computationally heavy and time-consuming. For example, the model training
    step uses a lot of computational resources to achieve high-performance model training
    and can sometimes take weeks to complete. If we are only experimenting with other
    components that do not require updating the trained model, it might make sense
    to avoid re-executing the expensive model training step. The step memoization
    pattern comes in handy when deciding whether you can skip heavy and redundant
    steps.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的机器学习应用中，除了数据处理之外，许多工作负载都是计算密集型和耗时的。例如，模型训练步骤需要大量的计算资源来实现高性能的模型训练，有时可能需要几周时间才能完成。如果我们只是在实验其他不需要更新训练模型的组件，那么避免重新执行昂贵的模型训练步骤可能是有意义的。在决定是否可以跳过重负载和冗余步骤时，步骤记忆模式非常有用。
- en: If we are creating content-based caches, the decision about the type of information
    to extract and store in the cache may not be trivial. For example, if we are trying
    to cache the results from a model training step, we may want to consider using
    the trained model artifact that includes information such as the type of machine
    learning model and the set of hyperparameters of the model. When the workflow
    is executed again, it will decide whether to re-execute the model training step
    based on whether we are trying the same model. Alternatively, we may store information
    like the performance statistics (e.g., accuracy, mean-squared error, etc.) to
    identify whether it’s beyond a threshold and worth training a more performant
    model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正在创建基于内容的数据缓存，关于要提取和存储在缓存中的信息类型的决策可能并不简单。例如，如果我们试图缓存模型训练步骤的结果，我们可能希望考虑使用包含诸如机器学习模型类型和模型超参数集等信息的训练模型工件。当工作流程再次执行时，它将根据我们是否尝试相同的模型来决定是否重新执行模型训练步骤。或者，我们可能存储诸如性能统计信息（例如，准确率、均方误差等）来识别是否超过了阈值，并且值得训练一个性能更好的模型。
- en: Furthermore, when applying the step memoization pattern in practice, be aware
    that it requires a certain level of maintenance efforts to manage the life cycle
    of the created cache. For example, if 1,000 machine learning workflows run every
    day with an average of 100 steps for each workflow being memoized, 100,000 caches
    will be created every day. Depending on the type of information they store, these
    caches require a certain amount of space that can accumulate rather quickly.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在实践应用步骤记忆模式时，请注意，它需要一定程度的维护工作来管理创建的缓存的生命周期。例如，如果每天有1,000个机器学习工作流程运行，每个工作流程平均有100个步骤被记忆，那么每天将创建100,000个缓存。根据它们存储的信息类型，这些缓存需要一定量的空间，这些空间可能会迅速积累。
- en: To apply this pattern at scale, a garbage collection mechanism must be in place
    to delete unnecessary caches automatically to prevent the accumulation of caches
    from taking up a huge amount of disk space. For example, one simple strategy is
    to record the timestamp when the cache is last hit and used by a step in a workflow
    and then scan the existing caches periodically to clean up those that are not
    used or hit after a long time.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在规模上应用此模式，必须有一个垃圾收集机制来自动删除不必要的缓存，以防止缓存积累占用大量磁盘空间。例如，一个简单的策略是记录缓存最后一次被工作流程中的步骤击中和使用的时间戳，然后定期扫描现有的缓存，清理那些未被使用或长时间未被击中的缓存。
- en: 5.4.4 Exercises
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.4 练习
- en: What type of steps can most benefit from step memoization?
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪种类型的步骤最能从步骤记忆化中受益？
- en: How do we tell whether a step’s execution can be skipped if its workflow has
    been triggered to run again?
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一个步骤的工作流程被触发再次运行，我们如何判断该步骤的执行是否可以跳过？
- en: What do we need to manage and maintain once we’ve used the pattern to apply
    the pattern at scale?
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们使用该模式进行大规模应用，我们需要管理和维护什么？
- en: 5.5 Answers to exercises
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 练习答案
- en: Section 5.2
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第5.2节
- en: No, because we have no guarantee in what order concurrent copies of those steps
    will run
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不，因为我们没有保证这些步骤的并发副本将按什么顺序运行
- en: Training an ensemble model depends on completing other model training steps
    for the sub-models. We cannot use the fan-in pattern because the ensemble model
    training step will need to wait for other model training to complete before it
    can start running, which would require some extra waiting and delay the entire
    workflow.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练集成模型取决于完成子模型的模型训练步骤。我们不能使用扇入模式，因为集成模型训练步骤将需要等待其他模型训练完成才能开始运行，这将需要额外的等待并延迟整个工作流程。
- en: Section 5.3
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第5.3节
- en: Due to the variation in completion times for each model training step in the
    existing machine learning workflow, the start of each following step, such as
    model selection and model serving, depends on the completion of the previous step.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于现有机器学习工作流程中每个模型训练步骤完成时间的差异，每个后续步骤（如模型选择和模型服务）的开始都取决于前一个步骤的完成。
- en: No, asynchronous steps won’t block each other.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不，异步步骤不会互相阻塞。
- en: We need to consider whether we want to use any available trained model as early
    as possible from the user’s perspective. We should think about whether it’s more
    important for users to see inference results faster or see better results. If
    the goal is to allow users to see the inference results as soon as a new model
    is available, those results may not be good enough or what users are expecting.
    Alternatively, if certain delays are acceptable to users, waiting for more model
    training steps to finish is preferable. You can then be selective about the trained
    models and pick the best-performing models that will provide very good entity-tagging
    results.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从用户的角度来看，我们需要考虑是否希望尽可能早地使用任何可用的训练模型。我们应该考虑对用户来说，是更快地看到推理结果更重要，还是看到更好的结果更重要。如果目标是允许用户在新的模型可用时立即看到推理结果，那么这些结果可能不够好或不是用户所期望的。或者，如果用户可以接受一定的延迟，等待更多的模型训练步骤完成可能更可取。然后，您可以有选择性地选择训练模型，并挑选出性能最好的模型，这将提供非常好的实体标记结果。
- en: Section 5.4
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第5.4节
- en: Steps that are time-consuming or require a huge amount of computational resources
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 耗时或需要大量计算资源的步骤
- en: We can use the information stored in the cache, such as when the cache is initially
    created or metadata collected from the step, to decide whether we should skip
    the execution of a particular step.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用存储在缓存中的信息，例如缓存最初创建时或从步骤收集的元数据，来决定是否应该跳过执行特定的步骤。
- en: We need to set up a garbage collection mechanism to recycle and delete the created
    caches automatically.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要设置一个垃圾回收机制来自动回收和删除创建的缓存。
- en: Summary
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Workflow is an essential component in machine learning systems as it connects
    all other components in a machine learning system. A machine learning workflow
    can be as easy as chaining data ingestion, model training, and model serving.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作流程是机器学习系统中的一个基本组件，因为它将机器学习系统中的所有其他组件连接起来。机器学习工作流程可以像链式数据摄取、模型训练和模型服务一样简单。
- en: The fan-in and fan-out patterns can be incorporated into complex workflows to
    make them maintainable and composable.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将扇入和扇出模式纳入复杂的工作流程中，可以使它们易于维护和组合。
- en: The synchronous and asynchronous patterns accelerate the machine learning workloads
    with the help of concurrency.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同步和异步模式通过并发加速机器学习工作负载。
- en: The step memoization pattern improves the performance of workflows by skipping
    duplicate workloads.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤记忆化模式通过跳过重复的工作负载来提高工作流程的性能。
