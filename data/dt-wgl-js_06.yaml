- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Dealing with huge data files
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 处理大型数据文件
- en: '**This chapter covers**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章涵盖**'
- en: Using Node.js streams
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Node.js流
- en: Processing files incrementally to handle large data files
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增量处理文件以处理大型数据文件
- en: Working with massive CSV and JSON files
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理大量CSV和JSON文件
- en: In this chapter, we’ll learn how to tackle large data files. How large? For
    this chapter, I downloaded a huge data set from the National Oceanic and Atmospheric
    Administration (NOAA). This data set contains measurements from weather stations
    around the world. The zipped download for this data is around 2.7 GB. This file
    uncompresses to a whopping 28 GB of data. The original data set contains more
    than 1 billion records. In this chapter, though, we’ll work with only a portion
    of that data, but even the cut-down example data for this chapter doesn’t fit
    into the available memory for Node.js, so to handle data of this magnitude, we’ll
    need new techniques.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何处理大型数据文件。有多大？对于本章，我从国家海洋和大气管理局（NOAA）下载了一个巨大的数据集。这个数据集包含来自世界各地气象站的测量数据。该数据集的压缩下载约为2.7
    GB。解压后，文件大小达到惊人的28 GB。原始数据集包含超过10亿条记录。然而，在本章中，我们只处理其中的一部分数据，但即使是本章的简化示例数据也无法适应Node.js可用的内存，因此为了处理如此大量的数据，我们需要新的技术。
- en: In the future, we’d like to analyze this data, and we’ll come back to that in
    that chapter 9\. But as it stands we can’t deal with this data using conventional
    techniques! To scale up our data-wrangling process and handle huge files, we need
    something more advanced. In this chapter, we’ll expand our toolkit to include
    incremental processing of CSV and JSON files using Node.js streams.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 未来，我们希望分析这些数据，我们将在第9章中回到这一点。但就目前而言，我们无法使用传统技术处理这些数据！为了扩大我们的数据处理过程并处理大型文件，我们需要更高级的技术。在本章中，我们将扩展我们的工具集，包括使用Node.js流进行CSV和JSON文件的增量处理。
- en: 7.1 Expanding our toolkit
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 扩展我们的工具集
- en: In this chapter, we’ll use a variety of new tools so that we can use Node.js
    streams for incremental processing of our large data files. We’ll revisit the
    familiar Papa Parse library for our CSV data, but this time we’ll use it in streaming
    mode. To work with streaming JSON data, I’ll introduce you to a new library called
    bfj (Big-Friendly JSON).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用各种新工具，以便我们可以使用Node.js流来增量处理我们的大型数据文件。我们将重新访问熟悉的Papa Parse库来处理我们的CSV数据，但这次我们将以流模式使用它。为了处理流式JSON数据，我将向您介绍一个名为bfj（Big-Friendly
    JSON）的新库。
- en: '[Table 7.1](#table7.1) lists the various tools that we cover in this chapter.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[表7.1](#table7.1) 列出了本章中我们涵盖的各种工具。'
- en: Table 7.1 Tools used in chapter 7
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1第7章中使用的工具
- en: '| **API / Library** | **Function / Class** | **Notes** |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| **API / Library** | **Function / Class** | **Notes** |'
- en: '| --- | --- | --- |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `Node.js fs` | `createReadStream` | Opens a streaming file for incremental
    reading |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| `Node.js fs` | `createReadStream` | 以增量方式打开文件进行读取|'
- en: '|  | `createWriteStream` | Opens a streaming fie for incremental writing |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '|  | `createWriteStream` | 以增量方式打开文件进行写入|'
- en: '|  | `stream.Readable` | We instantiate this to create custom readable data
    streams. |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '|  | `stream.Readable` | 我们实例化这个类以创建自定义的可读数据流。|'
- en: '|  | `stream.Writable` | We instantiate this to create custom writable data
    streams. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '|  | `stream.Writable` | 我们实例化这个类以创建自定义的可写数据流。|'
- en: '|  | `stream.Transform` | We instantiate this to create bidirectional transform
    streams that can modify our data as it passes through the stream. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|  | `stream.Transform` | 我们实例化这个类以创建双向转换流，可以在数据通过流时修改我们的数据。|'
- en: '| Papa Parse | `parse / unparse` | We’re using Papa Parse again, this time
    in streaming mode for CSV data serialization and deserialization. |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| Papa Parse | `parse / unparse` | 我们再次使用Papa Parse，这次是在流模式下用于CSV数据的序列化和反序列化。|'
- en: '| Bfj (Big-friendly JSON) | `walk` | We’re using third-party library bfj for
    streaming JSON deserialization. |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| Bfj (Big-friendly JSON) | `walk` | 我们使用第三方库bfj进行流式JSON反序列化。|'
- en: '| Data-Forge | `readFileStream` | Reads a file in streaming mode, allowing
    it to be incrementally transformed |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| Data-Forge | `readFileStream` | 以流模式读取文件，允许增量转换|'
- en: '|  | `writeFileStream` | Writes a file in streaming mode |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|  | `writeFileStream` | 以流模式写入文件|'
- en: 7.2 Fixing temperature data
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 修复温度数据
- en: For this chapter, we’re using the large data set that I downloaded from NOAA.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们使用的是我从NOAA下载的大型数据集。
- en: You could download the raw data set from here, although I wouldn’t recommend
    it; the download is 2.7 GB and it uncompresses to 28 GB. These files are available
    at [ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/](http://ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/)[.](http://.)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从这里下载原始数据集，尽管我不建议这样做；下载文件大小为2.7 GB，解压后为28 GB。这些文件可在[ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/](http://ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/)找到[.](http://.)
- en: I did preparatory work to convert this custom data set into 28 GB weather-stations.csv
    and an 80 GB weather-stations.json file that could be used to test this chapter’s
    code listings.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经做了前期工作，将这个自定义数据集转换为28 GB的weather-stations.csv文件和80 GB的weather-stations.json文件，这些文件可以用来测试本章的代码列表。
- en: Obviously, I can’t make files of that size available because they’re way too
    big for that; however, I made cut-down versions of these files available in the
    GitHub repository for chapter 7 (see the next section for details).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我无法提供这么大的文件，因为它们太大，不适合这样做；然而，我在GitHub存储库的第7章中提供了这些文件的缩减版本（下一节将详细介绍）。
- en: I’d like to analyze this data set, but I’ve got a problem. After an initial
    visual inspection of a sample of the data, I realized that the temperature fields
    aren’t in degrees Celsius. At first, I thought these values must have been in
    degrees Fahrenheit. But after experimentation and digging into the data set’s
    documentation, I discovered that the temperature values are expressed in tenths
    of degrees Celsius. This is an unusual unit of measurement, but apparently it
    was popular back when these records began and has been retained for ongoing consistency
    of the data set.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我想分析这个数据集，但我遇到了一个问题。在初步检查数据样本后，我发现温度字段不是以摄氏度表示的。起初，我以为这些值必须是华氏度。但经过实验和查阅数据集的文档后，我发现温度值是以十分之一摄氏度表示的。这是一个不寻常的计量单位，但显然在记录开始时很流行，并且为了保持数据集的一致性而被保留下来。
- en: Anyhow, I feel it’s more natural to work with degrees Celsius, which is our
    standard unit of measurement for temperature in Australia. I need to convert all
    the temperature fields in these humongous data files! This is almost a continuation
    of chapter 6, except now we need new techniques to deal with files this large.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我觉得使用摄氏度更自然，这是我们在澳大利亚的标准温度计量单位。我需要将这些巨大的数据文件中的所有温度字段转换为摄氏度！这几乎是第6章的延续，但现在我们需要新的技术来处理这些大型文件。
- en: 7.3 Getting the code and data
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 获取代码和数据
- en: The code and data for this chapter are available in the Data Wrangling with
    JavaScript Chapter-7 repository in GitHub. Don’t worry! The example data in GitHub
    has been cut down drastically and is much, much smaller than the original raw
    data set. You can find the data at [https://github.com/data-wrangling-with-javascript/chapter-7](https://github.com/data-wrangling-with-javascript/chapter-7)[.](http://.)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码和数据可在GitHub上的Data Wrangling with JavaScript Chapter-7存储库中找到。别担心！GitHub中的示例数据已被大幅缩减，比原始原始数据集小得多。您可以在[https://github.com/data-wrangling-with-javascript/chapter-7](https://github.com/data-wrangling-with-javascript/chapter-7)找到数据[.](http://.)
- en: The example data is located under the data subdirectory in the repository. Output
    generated by code is located under the output directory but isn’t included in
    the repo, so please run the code listings to generate the output.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 示例数据位于存储库中的数据子目录下。由代码生成的输出位于输出目录下，但未包含在repo中，因此请运行代码列表以生成输出。
- en: Refer to “Getting the code and data”*in chapter 2 if you need help getting the
    code and data.*
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要帮助获取代码和数据，请参考第2章中的“获取代码和数据”。
- en: '*## 7.4 When conventional data processing breaks down'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*## 7.4 当传统数据处理失败时'
- en: 'The methods presented so far in this book work to a large extent: they’re relatively
    simple and straightforward, and therefore you can be productive with them. You’ll
    go a long way with these techniques. However, there may come a time when you are
    presented with a huge data file and are expected to deal with it. At this point,
    the simple conventional techniques will break down—that’s because the simple techniques
    aren’t scalable to super-large data files.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本书迄今为止介绍的方法在很大程度上是有效的：它们相对简单直接，因此你可以使用它们提高生产力。这些技术能让你走得很远。然而，可能会有这样的时候，你面对一个巨大的数据文件，并被期望处理它。在这种情况下，简单的传统技术将失效——这是因为简单的技术无法扩展到超级大型数据文件。
- en: Let’s understand why that’s the case. [Figure 7.1](#figure7.1) shows how conventional
    data processing works.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解为什么是这样的情况。[图7.1](#figure7.1)展示了传统数据处理是如何工作的。
- en: We load the entire data file input.json into memory.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将整个数据文件 input.json 加载到内存中。
- en: We process the entire file in memory.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在内存中处理整个文件。
- en: We output the entire data file output.json.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们输出整个数据文件 output.json。
- en: '![c07_01.eps](Images/c07_01.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![c07_01.eps](Images/c07_01.png)'
- en: '[Figure 7.1](#figureanchor7.1) Conventional data processing: loading the entire
    file into memory'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7.1](#figureanchor7.1) 传统数据处理：将整个文件加载到内存中'
- en: Loading an entire data file into memory is simple, and it makes our data-wrangling
    process straightforward. Unfortunately, it doesn’t work for huge files. In [figure
    7.2](#figure7.2) you can see that large-file.json doesn’t fit in our available
    memory. The process fails at step 1, and we can’t read the entire file into memory
    at once. Afterward, we can’t process or output the file. Our process has broken
    down.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 将整个数据文件加载到内存中很简单，这使得我们的数据处理过程变得直接。不幸的是，它不适用于巨大的文件。在[图 7.2](#figure7.2)中，你可以看到
    large-file.json 不适合我们可用的内存。在第一步时，进程失败，我们无法一次性将整个文件读入内存。之后，我们无法处理或输出文件。我们的进程已经崩溃。
- en: Working on an entire file in memory is convenient, and we should do that where
    possible. However, if you know you need to deal with a large data set, then you
    should start preparing for it as early as possible. Soon we’ll look at how to
    deal with large files, but first let’s explore the limitations of Node.js.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在内存中处理整个文件很方便，我们应该尽可能这样做。然而，如果你知道你需要处理大量数据集，那么你应该尽早开始准备。不久我们将探讨如何处理大文件，但首先让我们来探索
    Node.js 的限制。
- en: '![c07_02.eps](Images/c07_02.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![c07_02.eps](Images/c07_02.png)'
- en: '[Figure 7.2](#figureanchor7.2) Conventional techniques break down for large
    files that are too big to fit into memory.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7.2](#figureanchor7.2) 对于太大而无法装入内存的大文件，传统技术会失效。'
- en: 7.5 The limits of Node.js
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 Node.js 的限制
- en: At what point exactly does our process break down? How big a file can we load
    into Node.js?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的进程在什么确切点会崩溃？我们可以在 Node.js 中加载多大文件？
- en: I was uncertain what the limits were. Search the web and you get a variety of
    answers; that’s because the answer probably depends on your version of Node.js
    and your operating system. I tested the limits of Node.js for myself. I used 64-bit
    Node.js v8.9.4 running on my Windows 10 laptop, which has 8 GB of memory.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我不确定这些限制是什么。在网上搜索，你会得到各种各样的答案；这是因为答案可能取决于你的 Node.js 版本和操作系统。我亲自测试了 Node.js 的限制。我使用了在我的
    Windows 10 笔记本电脑上运行的 64 位 Node.js v8.9.4，该电脑有 8 GB 的内存。
- en: I found that the largest CSV or JSON data file I could load in its entirety
    was limited by the size of the largest string that can be allocated in Node.js.
    In my tests, I found that the largest string size was around 512 MB (give or take
    a couple of MB) or around 268 million characters. This appears to be a limitation
    of the v8 JavaScript engine that powers Node.js, and it puts a constraint on the
    size of the data files that can pass through our conventional data processing
    pipeline.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现，我可以完全加载的最大 CSV 或 JSON 数据文件的大小受 Node.js 中可以分配的最大字符串大小的限制。在我的测试中，我发现最大的字符串大小约为
    512 MB（上下浮动几 MB）或约 2.68 亿个字符。这似乎是 Node.js 所使用的 v8 JavaScript 引擎的限制，这限制了可以通过我们传统数据处理管道的数据文件的大小。
- en: 'If you want to know more about how I conducted this test or run the test yourself,
    please see my code in the following GitHub repositories: [https://github.com/javascript-data-wrangling/nodejs-json-test](https://github.com/javascript-data-wrangling/nodejs-json-test)
    and [https://github.com/javascript-data-wrangling/nodejs-memory-test](https://github.com/javascript-data-wrangling/nodejs-memory-test)[.](http://.)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于我如何进行这项测试或自己运行测试的信息，请查看以下 GitHub 仓库中的我的代码：[https://github.com/javascript-data-wrangling/nodejs-json-test](https://github.com/javascript-data-wrangling/nodejs-json-test)
    和 [https://github.com/javascript-data-wrangling/nodejs-memory-test](https://github.com/javascript-data-wrangling/nodejs-memory-test)[.](http://.)
- en: The second repo more generally probes the limitations of Node.js and will help
    you understand how much heap memory, in total, that you can allocate.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个仓库更广泛地探讨了 Node.js 的限制，并将帮助你了解你可以分配的总堆内存量。
- en: 7.5.1 Incremental data processing
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.1 增量数据处理
- en: 'We have a large data file: weather_stations.csv. We need to do a transformation
    on this file to convert the MinTemp and MaxTemp temperature columns to degrees
    Celsius. After the conversion, we’ll output the file weather_stations.json. The
    fields we’re converting are currently expressed in tenths of degrees Celsius,
    apparently for backward compatibility with the older records. The formula to do
    the conversion is simple: we must divide each field by 10\. Our difficulty is
    in working with the huge file. The conventional workflow has failed us, and we
    can’t load the file into memory, so how can we deal with such a large file?'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个大的数据文件：weather_stations.csv。我们需要对这个文件进行转换，将MinTemp和MaxTemp温度列转换为摄氏度。转换后，我们将输出文件weather_stations.json。我们正在转换的字段目前以十分之一度摄氏度表示，显然是为了与较旧的记录保持向后兼容。转换的公式很简单：我们必须将每个字段除以10。我们的困难在于处理这个大文件。传统的流程已经失败了，我们无法将文件加载到内存中，那么我们如何处理这样一个大文件呢？
- en: Node.js streams are the solution. We can use a stream to process the data file
    incrementally, loading and processing the data chunk by chunk instead of trying
    to process it all at once.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Node.js流是解决方案。我们可以使用流来按增量处理数据文件，一次加载和处理一块数据，而不是一次性处理所有数据。
- en: '[Figure 7.3](#figure7.3) shows how this works. The file is divided into chunks.
    Each chunk of data easily fits into the available memory so that we can process
    it. We never have a time when we come close to exhausting our available memory.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.3](#figure7.3) 展示了这是如何工作的。文件被分成块。每个数据块都很容易适应可用的内存，这样我们就可以处理它。我们永远不会接近耗尽我们的可用内存。'
- en: '![c07_03.eps](Images/c07_03.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![c07_03.eps](Images/c07_03.png)'
- en: '[Figure 7.3](#figureanchor7.3) Processing data incrementally: loading only
    a chunk at a time into memory'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.3](#figureanchor7.3) 按增量处理数据：一次只将一块数据加载到内存中'
- en: The conventional data processing pipeline is ultra-convenient, and it works
    up to a point. When it starts to break down, we can introduce incremental processing,
    and this makes our data processing pipeline scalable to huge files.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的数据处理管道非常方便，并且在一定范围内有效。当它开始崩溃时，我们可以引入增量处理，这使得我们的数据处理管道能够扩展到处理大型文件。
- en: How big? We’re limited only by the available space in our file system because
    this places a limit on the size of our input and output files. We’re also limited
    by the time required to work through the entire file. For example, you might be
    able to fit a 100 GB CSV file in your file system, but if it takes a week to process,
    do you still care?
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有多大？我们受限于文件系统中可用的空间，因为这限制了我们的输入和输出文件的大小。我们还受限于处理整个文件所需的时间。例如，您可能能够在文件系统中容纳一个100GB的CSV文件，但如果处理需要一周时间，您还关心吗？
- en: We can essentially handle any size file, provided the files can fit on our hard
    drive and also provided we have the patience to wait for the processing to complete.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 只要文件可以放在我们的硬盘上，并且我们有耐心等待处理完成，我们基本上可以处理任何大小的文件。
- en: 7.5.2 Incremental core data representation
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.2 增量核心数据表示
- en: As you’ll recall, we’ve been working with a design pattern called the core data
    representation (CDR). The CDR defines a shared data format that connects the stages
    of our data processing pipeline. When I first introduced the CDR in chapter 3,
    we were working with entire files in memory and the CDR itself was a representation
    of our entire data set.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所忆，我们一直在使用一种名为核心数据表示（CDR）的设计模式。CDR定义了一个共享的数据格式，它连接了我们数据处理管道的各个阶段。当我首次在第3章介绍CDR时，我们是在内存中处理整个文件，而CDR本身就是我们整个数据集的表示。
- en: We must now adapt the CDR design pattern to work with incremental data processing.
    We don’t need to do anything, except maybe evolve our understanding of the CDR.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在必须调整CDR设计模式以适应增量数据处理。我们不需要做任何事情，也许只是深化我们对CDR的理解。
- en: The CDR is an array of JavaScript objects, where each object is a record from
    our data set. As it stands, each stage in the transformation pipeline operates
    on the entire data set. You can see an example of this in [figure 7.4](#figure7.4)
    where we take weather-stations.csv and pass it through several transformations
    before outputting another file named weather-stations-transformed.csv.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: CDR是一个JavaScript对象的数组，其中每个对象都是我们数据集中的一个记录。目前，转换管道中的每个阶段都在处理整个数据集。您可以在[图7.4](#figure7.4)中看到一个例子，我们取weather-stations.csv并通过几个转换，最后输出另一个名为weather-stations-transformed.csv的文件。
- en: '![c07_04.eps](Images/c07_04.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![c07_04.eps](Images/c07_04.png)'
- en: '[Figure 7.4](#figureanchor7.4) Conventional core data representation applies
    a transformation to an entire file in memory.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.4](#figureanchor7.4) 传统核心数据表示在内存中对整个文件应用转换。'
- en: Let’s change our thinking and redefine the CDR so that instead of representing
    an entire data set, it will now represent a chunk of our entire data set. [Figure
    7.5](#figure7.5) shows how the refashioned CDR can handle chunk-by-chunk processing
    of our data set in an incremental fashion.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们改变我们的思维方式，重新定义CDR，使其不再表示整个数据集，而是现在将表示我们整个数据集的一部分。[图7.5](#figure7.5)展示了经过改造的CDR如何以增量方式逐块处理我们的数据集。
- en: What this means is that any code modules already in your toolkit written to
    work with the CDR will work equally well using either conventional or incremental
    data processing. Our reusable code modules that work with the CDR take arrays
    of records, and now that we’re switching to the incremental version of the CDR,
    we’re still passing arrays of records to our transformation stages. But now those
    arrays each represent a chunk of records and not the entire data set.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，任何已经编写为与CDR一起工作的工具箱中的代码模块，无论是使用传统方法还是增量数据处理，都能同样良好地工作。我们与CDR一起工作的可重用代码模块处理记录数组，现在我们切换到CDR的增量版本，我们仍然将记录数组传递到我们的转换阶段。但现在这些数组每个都代表记录的一部分，而不是整个数据集。
- en: 7.5.3 Node.js file streams basics primer
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.3 Node.js文件流基础知识入门
- en: We’ll use Node.js streams to incrementally process our large CSV and JSON files,
    but before we can do that, we first need a basic understanding of Node.js streams.
    If you already understand how they work, please skip this section.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Node.js流来增量处理我们的大CSV和JSON文件，但在我们能够这样做之前，我们首先需要了解Node.js流的基本知识。如果您已经了解它们是如何工作的，请跳过本节。
- en: We need to learn about readable streams, writable streams, and the concept of
    piping. We’ll start with the most trivial example possible. [Figure 7.6](#figure7.6)
    demonstrates piping a readable input stream to a writable output stream. This
    is basically a file copy, but due to the use of Node.js streams, the data is copied
    chunk by chunk, never loading the entire file into memory at once. Node.js automatically
    chunkifies the file for us, and we don’t have to be concerned with chunk creation
    or management.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要了解可读流、可写流以及管道的概念。我们将从最简单的例子开始。![图7.6](#figure7.6)展示了将可读输入流管道连接到可写输出流。这基本上是一个文件复制，但由于使用了Node.js流，数据是分块复制的，一次不会将整个文件加载到内存中。Node.js会自动为我们分块化文件，我们不需要担心分块创建或管理。
- en: '![c07_05.eps](Images/c07_05.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![c07_05.eps](Images/c07_05.png)'
- en: '[Figure 7.5](#figureanchor7.5) Incremental core data representation: the design
    pattern is adapted to work incrementally.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.5](#figureanchor7.5) 增量核心数据表示：设计模式被调整为增量工作。'
- en: '![c07_06.eps](Images/c07_06.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![c07_06.eps](Images/c07_06.png)'
- en: '[Figure 7.6](#figureanchor7.6) Piping an input file stream to an output file
    stream'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.6](#figureanchor7.6) 将输入文件流管道连接到输出文件流'
- en: '[Listing 7.1](#listing7.1) shows the code that implements the process shown
    in [figure 7.6](#figure7.6). We open a readable file stream from weather-stations.csv
    and a writable file stream for weather-stations-transformed.csv. The pipe function
    is called to connect the streams and make the data flow from input file to output
    file. Try running the code and look at the transformed file that’s generated into
    the output subdirectory,'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表7.1](#listing7.1)展示了实现[图7.6](#figure7.6)中所示过程的代码。我们从一个可读文件流打开weather-stations.csv，并为weather-stations-transformed.csv创建一个可写文件流。调用pipe函数来连接流并使数据从输入文件流向输出文件。尝试运行代码，并查看生成的转换文件，该文件位于输出子目录中，'
- en: Listing 7.1 Simple Node.js file streaming (listing-7.1.js)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.1 简单Node.js文件流（listing-7.1.js）
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Pretty simple, right? Admittedly, [listing 7.1](#listing7.1) isn’t a particularly
    useful example. We’re using Node.js streams that don’t understand the structure
    of our data, but the point of this is to learn Node.js streams starting with a
    basic example. The interesting thing about piping is that we can now add any number
    of intermediate transformation stages by piping our stream through one or more
    transformation streams. For example, a data stream with three transformations
    (X, Y, and Z) might look like this:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 很简单，对吧？诚然，[列表7.1](#listing7.1)不是一个特别有用的例子。我们使用Node.js流，这些流不理解我们数据的结构，但这个例子是为了从基本示例开始学习Node.js流。管道的有趣之处在于，我们可以通过将流通过一个或多个转换流来添加任意数量的中间转换阶段。例如，具有三个转换（X、Y和Z）的数据流可能看起来像这样：
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Each intermediate transformation stage can be a separate reusable code module
    that you may have created earlier and have now pulled from your toolkit. Or they
    might be custom transformations that are specific for your current project.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 每个中间转换阶段都可以是一个独立的可重用代码模块，你可能之前已经创建过，现在从你的工具包中提取出来。或者它们可能是针对你当前项目的特定转换。
- en: It’s important to learn Node.js streams because they allow us to construct scalable
    data transformation pipelines from reusable code modules. Not only can our data
    pipelines have any number of intermediate processing stages, but they can now
    also handle arbitrarily large files (and that’s what we needed!).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 学习 Node.js 流很重要，因为它们允许我们从可重用代码模块中构建可伸缩的数据转换管道。不仅我们的数据管道可以有任意数量的中间处理阶段，而且它们现在也可以处理任意大小的文件（这正是我们所需要的！）。
- en: You should visualize a streaming data pipeline in the same way that you visualized
    any of the data pipelines in this book—as a series of boxes connected by arrows.
    See [figure 7.7](#figure7.7) for an example. The arrows show the direction of
    the data flow.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该像可视化本书中的任何数据管道一样，以相同的方式可视化流数据管道——一系列由箭头连接的框。参见 [图 7.7](#figure7.7) 中的示例。箭头显示了数据流的流向。
- en: '![c07_07.eps](Images/c07_07.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![c07_07.eps](Images/c07_07.png)'
- en: '[Figure 7.7](#figureanchor7.7) Piping a Node.js stream through multiple transformation
    stages'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7.7](#figureanchor7.7) 通过多个转换阶段管道传输 Node.js 流'
- en: To create a transformation like this for a Node.js stream, we need to instantiate
    the `Transform` class. This creates a bidirectional stream that is both readable
    and writable at the same time. It needs to be writable so that we can pipe our
    input data to it. It needs to be readable so that it can pipe the transformed
    data to the next stage in the pipeline.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要为 Node.js 流创建这样的转换，我们需要实例化 `Transform` 类。这创建了一个双向流，它可以同时读取和写入。它需要可写，以便我们可以将输入数据管道传输到它。它需要可读，以便它可以管道传输转换后的数据到管道的下一阶段。
- en: As an example, let’s look at a working example of a simple transformation. [Listing
    7.2](#listing7.2) is an expansion of [listing 7.1](#listing7.1) that pipes our
    data through a transformation stream that *lowercases* the text data as it passes
    through. The Node.js streaming API automatically divided our text file into chunks,
    and our transformation stream gets to work on only a small chunk of text at a
    time.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看看一个简单转换的工作示例。[列表 7.2](#listing7.2) 是 [列表 7.1](#listing7.1) 的扩展，它通过一个转换流将数据传递时将文本数据转换为小写。Node.js
    流 API 自动将我们的文本文件分割成块，我们的转换流一次只处理一小块文本。
- en: I told you this was going to be simple. We’re working with text files, and [listing
    7.2](#listing7.2) copies the input file to the output file. But in the process,
    it also converts all the text to lowercase. Run this code and then compare the
    input file weather-stations.csv to the output file weather-stations-transformed.csv
    to see the changes that were made.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我告诉过你这将会很简单。我们正在处理文本文件，[列表 7.2](#listing7.2) 将输入文件复制到输出文件。但在过程中，它也将所有文本转换为小写。运行此代码，然后比较输入文件
    weather-stations.csv 和输出文件 weather-stations-transformed.csv，以查看所做的更改。
- en: Listing 7.2 Transforming a Node.js stream (listing-7.2.js)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.2 转换 Node.js 流（listing-7.2.js）
- en: '[PRE2]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note the error handling at the end of [listing 7.2](#listing7.2). Stream error
    handling works in a similar way to promises: when an error or exception occurs
    at one stage in the pipeline, it terminates the entire pipeline.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 [列表 7.2](#listing7.2) 末尾的错误处理。流错误处理与承诺类似：当管道中的某个阶段发生错误或异常时，它将终止整个管道。
- en: 'This has been a brief primer on Node.js streams. We’ve barely scratched the
    surface, but already we can do something practical: we can stream our data through
    a transformation, and we’ve done it a way that scales to extremely large files.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个关于 Node.js 流的简要入门。我们只是触及了表面，但我们已经可以做一些实际的事情：我们可以通过转换流式传输我们的数据，而且我们已经以一种可以扩展到极大型文件的方式做到了这一点。
- en: 7.5.4 Transforming huge CSV files
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.4 转换巨大的 CSV 文件
- en: We aren’t interested only in plain text files; we need to transform structured
    data. Specifically, we have the data file weather-stations.csv, and we must enumerate
    its records and convert the temperature fields to degrees Celsius.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅对纯文本文件感兴趣；我们需要转换结构化数据。具体来说，我们有数据文件 weather-stations.csv，我们必须枚举其记录并将温度字段转换为摄氏度。
- en: How can we use Node.js streams to transform a huge CSV file? This could be difficult,
    but fortunately Papa Parse, the library we started using in chapter 3, already
    has support for reading a Node.js stream.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何使用Node.js流来转换一个大型CSV文件？这可能很困难，但幸运的是，我们在第3章开始使用的Papa Parse库已经支持读取Node.js流。
- en: Unfortunately, Papa Parse doesn’t provide us with a readable stream that we
    can easily pipe to another stream. Instead, it has a custom API and triggers its
    own event whenever a chunk of data has been parsed from the CSV format. What we’ll
    do, though, is create our own adapter for Papa Parse so that we can pipe its output
    into a Node.js stream. This is a useful technique in itself—taking a nonstreaming
    API and adapting it so that it fits into the Node.js streaming framework.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Papa Parse没有提供我们可以轻松管道连接到另一个流的可读流。相反，它有一个自定义API，每当从CSV格式解析出数据块时，它会触发自己的事件。不过，我们将创建自己的Papa
    Parse适配器，以便我们可以将它的输出管道连接到Node.js流。这本身就是一个有用的技术——将非流式API适配，使其适合Node.js流式框架。
- en: In [figure 7.8](#figure7.8) you can see how we’ll pipe our parsed CSV data through
    a convert temperature stream before piping out to another CSV file.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图7.8](#figure7.8)中，你可以看到我们将如何将解析后的CSV数据通过转换温度流，然后再将其输出到另一个CSV文件。
- en: '![c07_08.eps](Images/c07_08.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![c07_08.eps](Images/c07_08.png)'
- en: '[Figure 7.8](#figureanchor7.8) Streaming transformation of a huge CSV file'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.8](#figureanchor7.8) 大型CSV文件的流式转换'
- en: 'To give you an idea of what we are trying to accomplish here, consider the
    following snippet of code:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让你了解我们在这里试图实现的目标，考虑以下代码片段：
- en: '[PRE3]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So, what’s happening here?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这里发生了什么？
- en: We’re opening a readable stream for CSV data. The chunks of data we’re streaming
    here are expressed in the core data representation.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们正在打开一个可读的CSV数据流。这里流式传输的数据块以核心数据表示形式表达。
- en: We then pipe the CSV data through a transformation stream. This is where we
    convert the temperature fields to degrees Celsius.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将CSV数据通过一个转换流。这是我们将温度字段转换为摄氏度的位置。
- en: Finally, we pipe the transformed data to a writable stream for CSV data.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将转换后的数据管道连接到一个可写的CSV数据流。
- en: The function `convertTemperatureStream` could be a reusable code module, although
    it seems rather specific to this project, but if it was generally useful, we could
    give it a home in our toolkit.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`convertTemperatureStream`可能是一个可重用的代码模块，尽管它似乎非常特定于这个项目，但如果它具有通用性，我们可以在我们的工具包中为其提供一个位置。
- en: Installing Papa Parse
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安装Papa Parse
- en: 'If you’ve installed dependencies for the code repository, you already have
    Papa Parse installed; otherwise, you can install it in a fresh Node.js project
    as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经为代码仓库安装了依赖项，那么你已经有Papa Parse了；否则，你可以在一个新的Node.js项目中按照以下方式安装它：
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Opening a readable CSV stream
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 打开一个可读的CSV流
- en: The first part of our CSV streaming puzzle is to create a readable stream that
    can stream in a CSV file and incrementally parse it to JavaScript objects. It’s
    the deserialized JavaScript objects that we ultimately want. [Figure 7.9](#figure7.9)
    shows how we’ll encapsulate Papa Parse within a readable CSV data stream. This
    gives us an input stream that we can pipe to our data transformation stream.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们CSV流式传输难题的第一部分是创建一个可读流，它可以流式传输CSV文件并增量解析它到JavaScript对象。我们最终想要的是反序列化的JavaScript对象。[图7.9](#figure7.9)展示了我们将如何将Papa
    Parse封装在可读CSV数据流中。这给了我们一个输入流，我们可以将其管道连接到我们的数据转换流。
- en: '![c07_09.eps](Images/c07_09.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![c07_09.eps](Images/c07_09.png)'
- en: '[Figure 7.9](#figureanchor7.9) Encapsulating Papa Parse CSV deserialization
    in a readable CSV data stream'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.9](#figureanchor7.9) 将Papa Parse CSV反序列化封装在可读CSV数据流中'
- en: Let’s create a new toolkit function `openCsvInputStream` to create and return
    our readable CSV data stream. The code for this is presented in the following
    listing. It uses Papa Parse’s custom streaming API. As Papa Parse deserializes
    each JavaScript object from the file stream, the deserialized object is passed
    through to our CSV data stream.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个新的工具函数`openCsvInputStream`来创建并返回我们的可读CSV数据流。该代码在下面的列表中展示。它使用了Papa Parse的定制流式API。当Papa
    Parse从文件流中反序列化每个JavaScript对象时，反序列化的对象会被传递到我们的CSV数据流中。
- en: Listing 7.3 Toolkit function to open a CSV file input stream (toolkit/open-csv-input-stream.js)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.3 打开CSV文件输入流的工具函数（toolkit/open-csv-input-stream.js）
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Notice several key points in [listing 7.3](#listing7.3). First is that we create
    the readable stream with *object mode* enabled. Normally, a Node.js stream is
    low level, and it enumerates the raw contents of the file using Node.js `Buffer`
    objects. We want to work at a higher level of abstraction. We want to retrieve
    JavaScript objects and not raw file data, and that’s why we created the readable
    stream in object mode. This allows us to work with streaming data that’s expressed
    in the core data representation.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 [列表 7.3](#listing7.3) 中的几个关键点。首先，我们创建了一个启用 *对象模式* 的可读流。通常，Node.js 流是低级的，它使用
    Node.js `Buffer` 对象枚举文件的原始内容。我们希望在一个更高的抽象级别上工作。我们希望检索 JavaScript 对象而不是原始文件数据，这就是为什么我们以对象模式创建了可读流。这允许我们处理以核心数据表示形式表达的数据流。
- en: The next thing to note is how we pass CSV data forward to the readable stream.
    The `step` callback is invoked whenever Papa Parse has a chunk of CSV rows ready
    for us. We pass this data onto the readable stream through its `push` function.
    You can say that we’re *pushing* data into the stream.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个需要注意的是我们如何将 CSV 数据传递到可读流。每当 Papa Parse 准备好一批 CSV 行供我们使用时，`step` 回调函数就会被调用。我们通过其
    `push` 函数将此数据传递到可读流。可以说我们是在 *推送* 数据到流中。
- en: The `complete` callback is invoked when the entire CSV file has been parsed.
    At this point, no more CSV rows will come through, and we call the `push` function
    with a `null` parameter to indicate to the stream that we’ve finished.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当整个 CSV 文件被解析时，`complete` 回调函数会被调用。此时，不会再有 CSV 行通过，我们通过传递一个 `null` 参数给 `push`
    函数来向流指示我们已经完成。
- en: 'Last, don’t forget the `error` callback: this is how we forward Papa Parse
    errors to the readable stream.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，别忘了 `error` 回调：这是我们向可读流转发 Papa Parse 错误的方式。
- en: Opening a writable CSV stream
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 打开可写 CSV 流
- en: On the other side of our CSV streaming puzzle, we must create a writable stream
    that we can pass JavaScript objects to and have them written to a file in CSV
    format. [Figure 7.10](#figure7.10) shows how we’ll encapsulate Papa Parse in the
    writable CSV data stream. This gives us a stream that we can use to output our
    transformed data.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们 CSV 流式传输谜题的另一边，我们必须创建一个可写流，我们可以将 JavaScript 对象传递给它，并将它们以 CSV 格式写入文件。[图 7.10](#figure7.10)
    展示了我们将如何将 Papa Parse 封装在可写 CSV 数据流中。这为我们提供了一个可以用来输出转换后数据的流。
- en: '![c07_10.eps](Images/c07_10.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![c07_10.eps](Images/c07_10.png)'
- en: '[Figure 7.10](#figureanchor7.10) Encapsulating Papa Parse CSV serialization
    in a writable CSV data stream'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7.10](#figureanchor7.10) 在可写 CSV 数据流中封装 Papa Parse CSV 序列化'
- en: The following listing is a new toolkit function `openCsvOutputStream` that opens
    our writable CSV data stream. For each JavaScript object that’s passed into the
    CSV output stream, it’s serialized to CSV data by Papa Parse before being passed
    to the file output stream.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表是一个新的工具函数 `openCsvOutputStream`，它打开我们的可写 CSV 数据流。对于传递到 CSV 输出流的每个 JavaScript
    对象，在传递到文件输出流之前，它都会被 Papa Parse 序列化为 CSV 数据。
- en: Listing 7.4 Toolkit function to open a CSV file output stream (toolkit/open-csv-output-stream.js)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.4 打开 CSV 文件输出流的工具函数（toolkit/open-csv-output-stream.js）
- en: '[PRE6]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here again we’re opening our stream with *object mode* enabled so that we can
    work with a stream of JavaScript objects and not Node.js `Buffer` objects.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们再次以启用 *对象模式* 的方式打开我们的流，这样我们就可以处理 JavaScript 对象流，而不是 Node.js `Buffer` 对象。
- en: '[Listing 7.4](#listing7.4) is somewhat less complicated than [listing 7.3](#listing7.3).
    We implement the _`write` function to handle chunks of data that are *written*
    to the writable CSV data stream. Here we use Papa Parse to serialize the records
    and then forward them to the writable file stream for output.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 7.4](#listing7.4) 比列表 7.3 简单一些。我们实现了 _`write`_ 函数来处理写入到可写 CSV 数据流的块数据。在这里，我们使用
    Papa Parse 序列化记录，然后将它们转发到可写文件流以进行输出。'
- en: Note the use of the `firstOutput` variable to switch off CSV headers for all
    but the first record. We allow Papa Parse to output the CSV column names only
    at the start of the CSV file.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 注意使用 `firstOutput` 变量来关闭除了第一条记录之外的所有 CSV 头部。我们允许 Papa Parse 仅在 CSV 文件的开始处输出
    CSV 列表名。
- en: Toward the end of the listing, we handle the writable stream’s `finish` event,
    and this is where we close the writable file stream.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表的末尾，我们处理可写流的 `finish` 事件，这是关闭可写文件流的地方。
- en: Transforming the huge CSV file
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 转换巨大的 CSV 文件
- en: Now that we have our two toolkit functions in place, we can piece together the
    entire data pipeline. We can open a stream to read and parse weather-stations.csv.
    We can also open a stream to serialize our transformed data and output weather-stations-transformed.csv.
    The completed data transformation is presented in the following listing. After
    running this code, you should visually compare the temperature fields in the input
    and output files to make sure they’ve been correctly transformed.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了两个工具函数，我们可以拼凑整个数据管道。我们可以打开一个流来读取和解析 weather-stations.csv。我们还可以打开一个流来序列化我们的转换数据并输出
    weather-stations-transformed.csv。完成的数据转换在以下列表中展示。运行此代码后，你应该在输入和输出文件中视觉比较温度字段，以确保它们已被正确转换。
- en: Listing 7.5 Transforming a huge CSV file (listing-7.5.js)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.5 转换大型 CSV 文件（listing-7.5.js）
- en: '[PRE7]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that `transformRow` is the function that transforms a single record of
    data. It’s invoked numerous times, record by record, as the entire file is processed
    in a piecemeal fashion.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`transformRow` 是转换单个数据记录的函数。它会在整个文件以分块方式处理时，逐条记录多次调用。
- en: 7.5.5 Transforming huge JSON files
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.5 转换巨大的 JSON 文件
- en: Now let’s look at transforming huge JSON files. Arguably this is more difficult
    than working with huge CSV files, which is why I’ve saved it until last.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看转换巨大的 JSON 文件。这可能是比处理大型 CSV 文件更困难的事情，这也是为什么我把它留到最后。
- en: 'We’ll do a similar transformation to weather-stations.json: converting the
    temperature fields to degrees Celsius and then outputting weather-stations-transformed.json.
    We’ll use similar principles to when we transformed the huge CSV file.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对 weather-stations.json 执行类似的转换：将温度字段转换为摄氏度，然后输出 weather-stations-transformed.json。我们将使用与转换大型
    CSV 文件时类似的原则。
- en: But why is it more difficult to incrementally process a JSON? Normally, JSON
    files are easier to parse than CSV because the functions we need to do this are
    built into JavaScript and also because JSON fits so well with JavaScript. It’s
    more difficult in this case due to the nature of the JSON data format.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么增量处理 JSON 更困难呢？通常，JSON 文件比 CSV 文件更容易解析，因为我们需要进行此操作的功能已经内置在 JavaScript 中，而且
    JSON 与 JavaScript 的匹配度非常高。在这种情况下，由于 JSON 数据格式的特性，这变得更加困难。
- en: JSON is naturally a hierarchical data format. We can and do express simple and
    flat tabular data in JSON—as you’ve already seen in this book—but JSON files can
    be deeply nested and much more complicated than simple tabular data. I structured
    the code, you’ll see here, such that it assumes the JSON file contains only a
    flat array of objects with no nested data. Please be warned that the code presented
    here doesn’t necessarily work with general-purpose JSON data files, and you may
    have to adapt it to work in other situations depending on your needs.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 自然是一种分层的数据格式。我们可以在 JSON 中表达简单和扁平的表格数据——就像你在本书中已经看到的那样——但 JSON 文件可以深度嵌套，比简单的表格数据复杂得多。我编写的代码，你在这里会看到，它假设
    JSON 文件只包含一个扁平的对象数组，没有嵌套数据。请务必注意，这里展示的代码不一定适用于通用 JSON 数据文件，你可能需要根据你的需求对其进行调整。
- en: In this section, we’ll use a library called bfj or Big-Friendly JSON. This is
    a nifty library for parsing a streaming JSON file. It’s like what we did with
    Papa Parse; we’ll encapsulate bfj in a readable JSON stream, pipe it through the
    convert temperature stream, and then pipe it out to weather-stations-transformed.json
    using a writable JSON stream, as described in [figure 7.11](#figure7.11). We’ll
    reuse the same transformation stream we created earlier, but this time we’ll embed
    it in our pipeline between the input and output JSON files.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用一个名为 bfj 或 Big-Friendly JSON 的库。这是一个用于解析流式 JSON 文件的巧妙库。它就像我们使用 Papa
    Parse 所做的那样；我们将 bfj 封装在一个可读的 JSON 流中，通过转换温度流将其传递，然后使用可写 JSON 流将其输出到 weather-stations-transformed.json，如[图
    7.11](#figure7.11) 所述。我们将重用之前创建的相同转换流，但这次我们将它嵌入到输入和输出 JSON 文件之间的管道中。
- en: '![c07_11.eps](Images/c07_11.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![c07_11.eps](Images/c07_11.png)'
- en: '[Figure 7.11](#figureanchor7.11) Streaming transformation of a huge JSON file'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7.11](#figureanchor7.11) 大型 JSON 文件的流式转换'
- en: Installing bfj
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安装 bfj
- en: 'If you installed dependencies for the Chapter-7 code repository, then you’ve
    already installed bfj; otherwise, you can install it in a fresh Node.js project
    as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你为第 7 章代码仓库安装了依赖项，那么你已安装了 bfj；否则，你可以在新的 Node.js 项目中按以下方式安装它：
- en: '[PRE8]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Opening a readable JSON stream
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 打开可读的 JSON 流
- en: We must first create a readable stream that can incrementally read in a JSON
    file and parse it to JavaScript objects. [Figure 7.12](#figure7.12) shows how
    we’ll encapsulate bfj within our readable JSON data stream. This gives us an input
    stream that we can use to read a JSON file and pipe the deserialized data to another
    stream.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须首先创建一个可读流，它可以增量地读取JSON文件并将其解析为JavaScript对象。[图7.12](#figure7.12)展示了我们将如何封装bfj在我们的可读JSON数据流中。这给了我们一个可以用来读取JSON文件并将反序列化数据通过管道传输到另一个流的输入流。
- en: Let’s create a new toolkit function `openJsonInputStream` to create our readable
    JSON data stream. Bfj is a custom API with events that are emitted because it
    recognizes structures in the JSON file. It emits events when it recognizes JSON
    arrays, JSON objects, properties, and so forth. In [listing 7.6](#listing7.6)
    we handle these events to incrementally build our JavaScript objects and feed
    them to the readable JSON stream. As soon as we recognize each complete JSON object,
    we immediately pass the equivalent deserialized JavaScript object forward to the
    JSON data stream.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个新的工具函数`openJsonInputStream`来创建我们的可读JSON数据流。Bfj是一个自定义API，它通过识别JSON文件中的结构来触发事件。当它识别到JSON数组、JSON对象、属性等时，它会触发事件。在[列表7.6](#listing7.6)中，我们处理这些事件，以增量地构建我们的JavaScript对象并将它们喂送到可读JSON流中。一旦我们识别到每个完整的JSON对象，我们就立即将等效的反序列化JavaScript对象传递到JSON数据流中。
- en: '![c07_12.eps](Images/c07_12.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![c07_12.eps](Images/c07_12.png)'
- en: '[Figure 7.12](#figureanchor7.12) Encapsulating bfj JSON deserialization in
    a readable JSON data stream'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.12](#figureanchor7.12) 将bfj JSON反序列化封装在可读JSON数据流中'
- en: Listing 7.6 Toolkit function to open a JSON file input stream (toolkit/open-json-file-input-stream.js)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.6 工具函数用于打开JSON文件输入流（toolkit/open-json-file-input-stream.js）
- en: '[PRE9]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: A point to note in [listing 7.6](#listing7.6) is how we use bfj’s `walk` function
    to *walk* the structure of the JSON file. The *walk* terminology is used here
    because the JSON file is potentially a hierarchical document. It’s potentially
    structured as a tree, and we must walk (or traverse) said tree to process it,
    even though in this case we aren’t dealing with a hierarchical document. Instead,
    we’re assuming that weather-stations.json contains a flat array of data records.
    As bfj raises its events for the array, and for each object and property, we collect
    these together and build data records to feed to the JSON data stream through
    its `push` function.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表7.6](#listing7.6)中需要注意的一点是我们如何使用bfj的`walk`函数来*遍历*JSON文件的结构。在这里使用*遍历*这个术语是因为JSON文件可能是一个分层文档。它可能被组织成树状结构，我们必须遍历（或遍历）这个树来处理它，尽管在这种情况下我们并没有处理分层文档。相反，我们假设weather-stations.json包含一个扁平的数据记录数组。当bfj为数组、每个对象和属性触发事件时，我们收集这些事件并构建数据记录，通过其`push`函数将它们喂送到JSON数据流中。
- en: As we expect the input JSON file to be a flat array of records, when the bfj
    `endArray` event is raised, at that point, we signify the end of stream by passing
    `null` to the `push` function.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们期望输入的JSON文件是一个扁平的记录数组，当bfj的`endArray`事件被触发时，在那个点上，我们通过将`null`传递给`push`函数来表示流的结束。
- en: Opening a writable JSON stream
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 打开可写JSON流
- en: To complete our JSON file transformation stream, we must also have a writable
    JSON stream that we can pass JavaScript objects to and have them written out to
    an output file in the JSON format. [Figure 7.13](#figure7.13) shows how we’ll
    encapsulate JSON.stringify in a writable JSON data stream. This gives a writable
    stream that we can incrementally write objects to and have them serialized in
    sequence to our output file weather-stations-transformed.json.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成我们的JSON文件转换流，我们还必须有一个可写的JSON流，我们可以将JavaScript对象传递给它，并将它们以JSON格式写入输出文件。[图7.13](#figure7.13)展示了我们将如何封装JSON.stringify在可写JSON数据流中。这给了我们一个可写流，我们可以增量地将对象写入它，并将它们按顺序序列化到输出文件weather-stations-transformed.json中。
- en: '[Listing 7.7](#listing7.7) shows the toolkit function `openJsonOutputStream`
    that opens our writable JSON data stream, so we can start outputting JavaScript
    objects. For each JavaScript object that’s passed to the JSON data stream, we
    serialize it to JSON and pass the serialized JSON data forward to the file output
    stream.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表7.7](#listing7.7)展示了工具函数`openJsonOutputStream`，它打开我们的可写JSON数据流，因此我们可以开始输出JavaScript对象。对于传递给JSON数据流的每个JavaScript对象，我们将其序列化为JSON，并将序列化的JSON数据传递到文件输出流中。'
- en: '![c07_13.eps](Images/c07_13.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![c07_13.eps](Images/c07_13.png)'
- en: '[Figure 7.13](#figureanchor7.13) Encapsulating bfj JSON serialization in writable
    JSON data'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.13](#figureanchor7.13) 将bfj JSON序列化封装在可写JSON数据中'
- en: Listing 7.7 Toolkit function to open a JSON file output stream (toolkit/open-json-file-output-stream.js)
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.7 打开 JSON 文件输出流的工具函数（toolkit/open-json-file-output-stream.js）
- en: '[PRE10]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As we found with the CSV output stream, the code for opening a writable JSON
    stream is much simpler than the code for opening a readable JSON stream. Again,
    we implement the `_write` function to serialize records and write them to the
    file. Here we’re using `JSON.stringify` to serialize each data record.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 CSV 输出流中发现的那样，打开可写 JSON 流的代码比打开可读 JSON 流的代码简单得多。再次，我们实现 `_write` 函数来序列化记录并将它们写入文件。在这里，我们使用
    `JSON.stringify` 来序列化每个数据记录。
- en: Finally, we handle the `finish` event and use it to finalize the stream.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们处理 `finish` 事件并使用它来最终化流。
- en: Transforming the huge JSON file
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 转换巨大的 JSON 文件
- en: With our two new toolkit functions for opening input and output JSON data streams,
    we can now transform our massive JSON file, as shown in [listing 7.8](#listing7.8).
    To keep the listing small, I’ve omitted several functions that haven’t changed
    since [listing 7.5](#listing7.5). This is another complete code listing that you
    can run on its own; make sure you check the output data file to ensure that the
    data transformation was successful.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们为打开输入和输出 JSON 数据流而新增的两个工具函数，我们现在可以转换我们的巨大 JSON 文件，如[列表 7.8](#listing7.8)所示。为了使列表保持简洁，我已省略了自[列表
    7.5](#listing7.5)以来未发生变化的几个函数。这是一个可以独立运行的完整代码列表；请确保检查输出数据文件，以确保数据转换成功。
- en: Listing 7.8 Transforming a huge JSON file (listing-7.8.js)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.8 转换巨大的 JSON 文件（listing-7.8.js）
- en: '[PRE11]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can now use Node.js streams to process massive CSV and JSON files. What more
    do you want? As a side effect, we can now mix and match our streams, and this
    gives us the ability to quickly construct a variety of *streaming* data pipelines.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 Node.js 流来处理大量的 CSV 和 JSON 文件。你还需要什么？作为副作用，我们现在可以混合匹配我们的流，这使我们能够快速构建各种
    *流式* 数据管道。
- en: 7.5.6 Mix and match
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.6 混合匹配
- en: With the core data representation acting as the abstraction between the stages
    in our data pipeline, we can easily build conversion pipelines between different
    formats for huge data files.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据管道阶段之间，核心数据表示作为抽象，我们可以轻松构建不同格式之间的大型数据文件的转换管道。
- en: 'For example, consider how we can transform a CSV file to a JSON file:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑我们如何将 CSV 文件转换为 JSON 文件：
- en: '[PRE12]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this same way, we can transform JSON to CSV, or indeed from any format to
    any other format provided we create a stream appropriate for that data format.
    For example, you might want to work with XML, so you’d create a function to open
    a streaming XML file and then use that to transform XML files or convert them
    to CSV or JSON.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以同样的方式，我们可以将 JSON 转换为 CSV，或者实际上从任何格式转换为任何其他格式，前提是我们为该数据格式创建一个合适的流。例如，你可能想处理
    XML，那么你会创建一个函数来打开一个流式 XML 文件，然后使用它来转换 XML 文件或将它们转换为 CSV 或 JSON。
- en: In this chapter, we looked at how conventional data processing techniques can
    break down in the face of massive data files. Sometimes, hopefully infrequently,
    we must take more extreme measures and use Node.js streams to incrementally process
    such huge data files.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了传统数据处理技术在面对大型数据文件时可能会崩溃的情况。有时，希望这种情况不常发生，我们必须采取更极端的措施，并使用 Node.js
    流来增量处理这些巨大的数据文件。
- en: When you do find yourself getting bogged down in huge data files, you may wonder
    if there is a better way to tackle large data sets. Well, I’m sure you already
    guessed it, but we should work with a database. In the next chapter, we’ll build
    a Node.js stream that outputs our records to a database. This will allow us to
    move our large data files into a database for more efficient and convenient access
    to the data.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当你发现自己陷入处理大型数据文件的困境时，你可能想知道是否有更好的方法来处理大型数据集。好吧，我相信你已经猜到了，但我们应该与数据库一起工作。在下一章中，我们将构建一个
    Node.js 流，该流将我们的记录输出到数据库。这将使我们能够将大型数据文件移动到数据库中，以便更高效、更方便地访问数据。
- en: Summary
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: We discussed the memory limitations of Node.js.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们讨论了 Node.js 的内存限制。
- en: You learned that incremental processing can be used to tackle huge data files.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你了解到增量处理可以用来处理大型数据文件。
- en: We figured out how to adapt the core data representation design pattern to incremental
    processing.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们找到了如何将核心数据表示设计模式适应增量处理的方法。
- en: We used Node.js streams to build data pipelines from reusable code modules that
    are scalable to large data files.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 Node.js 流来构建由可重用代码模块组成的数据管道，这些模块可扩展到大型数据文件。
- en: You learned that you can mix and match Node.js streams to build a variety of
    data pipelines.*
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你了解到你可以混合匹配 Node.js 流来构建各种数据管道。
