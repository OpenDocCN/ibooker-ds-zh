- en: 11 The core of the control plane
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 控制平面的核心
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Investigating core components of the control plane
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索控制平面的核心组件
- en: Reviewing API server details
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审查API服务器细节
- en: Exploring scheduler interfaces and its inner workings
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索调度器接口及其内部工作原理
- en: Walking through the controller manager and cloud manager
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遍历控制器管理器和云管理器
- en: Previously, we provided a high-level overview of Pods, a web application outlining
    why we need Pods, and how Kubernetes is built with Pods. Now that we’ve covered
    all of our requirements for the use case, let’s dive into the details of the control
    plane. Typically, all of the control plane components are installed in the `kube-system`
    namespace, a namespace where you, as an operator, should install very few components.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们提供了一个Pod的高级概述，一个概述Web应用程序为什么需要Pod，以及Kubernetes如何使用Pod构建。现在我们已经涵盖了所有用例的要求，让我们深入了解控制平面的细节。通常，所有控制平面组件都安装在`kube-system`命名空间中，这是一个作为操作员，你应该安装非常少组件的命名空间。
- en: Note You should just not use `kube-system`! One of the main reasons is that
    noncontroller applications running inside `kube-system` increase the security
    blast radius, which refers to the breadth and depth of a security intrusion. Further,
    if you are on a hosted system like GKE or EKS, you cannot see all of the control
    plane components. We talk more about blast radius and security best practices
    in chapter 13.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你不应该只是使用`kube-system`！其中一个主要原因是运行在`kube-system`中的非控制器应用程序增加了安全爆炸半径，这指的是安全入侵的广度和深度。此外，如果你在一个托管系统上，如GKE或EKS，你无法看到所有控制平面组件。我们将在第13章中更多地讨论爆炸半径和安全最佳实践。
- en: 11.1 Investigating the control plane
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 探索控制平面
- en: 'One of the easiest ways to start and poke at the control plane is to use `kind`,
    which is Kubernetes in a container (see the following link for install instructions:
    [http://mng.bz/lalM](http://mng.bz/lalM)). To use `kind` to view the control plane,
    run the following commands:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 开始和探索控制平面的最简单方法之一是使用`kind`，这是容器中的Kubernetes（有关安装说明，请参阅以下链接：[http://mng.bz/lalM](http://mng.bz/lalM)）。要使用`kind`查看控制平面，请运行以下命令：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Creates a Kubernetes cluster running in a container
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在容器中创建运行的Kubernetes集群
- en: ❷ Sets your kubectl content to point to your local kind cluster
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将你的kubectl内容设置为指向你的本地kind集群
- en: ❸ Prints the control plane’s component Pods (we only print the Pod name here)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打印控制平面的组件Pod（我们这里只打印Pod名称）
- en: ❹ Runs two replicas as a Deployment
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 以Deployment运行两个副本
- en: ❺ The etcd database
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ etcd数据库
- en: ❻ The CNI provider
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ CNI提供者
- en: ❼ Kubernetes API server
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ Kubernetes API服务器
- en: ❽ Kubernetes controller manager
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ Kubernetes控制器管理器
- en: ❾ Node component kube proxy
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 节点组件kube proxy
- en: ❿ Kubernetes scheduler
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ Kubernetes调度器
- en: 'You’ll notice that the kubelet is not running as a Pod. Some systems run the
    kubelet inside a container, but in systems such as `kind`, the kubelet is just
    run as a binary. To see the kubelet running in a `kind` cluster, issue the following
    commands:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到kubelet不是作为一个Pod运行的。一些系统在容器内运行kubelet，但在`kind`等系统中，kubelet只是作为一个二进制文件运行。要查看在`kind`集群中运行的kubelet，请执行以下命令：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Gets an interactive terminal running inside the kind container
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在kind容器内运行交互式终端
- en: ❷ ps (process status) for the kubelet
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ kubelet的ps（进程状态）
- en: ❸ The running kubelet process
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 运行中的kubelet进程
- en: 'Type `exit` to get out of the interactive terminal inside the container. To
    truly get a feel of what comprises a control plane, take a look at the various
    Pods. For instance, you can print the API server Pod with this command:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 输入`exit`以退出容器内的交互式终端。要真正了解控制平面由什么组成，请查看各种Pod。例如，你可以使用以下命令打印API服务器Pod：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 11.2 API server details
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 API服务器细节
- en: Now, it’s time to dive into the details regarding the API server. That’s because
    it’s not only a RESTful web server, but it is also a critical component of the
    control plane. What is important to note is not only the API objects, but also
    the custom API objects. Later in the book, we will cover the authentication, authorization,
    and admission controllers, but first, let’s look in a bit more detail at Kubernetes
    API objects and custom resources.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候深入了解API服务器的细节了。因为它不仅是一个RESTful Web服务器，而且是控制平面的关键组件。需要注意的是，不仅包括API对象，还包括自定义API对象。本书后面将介绍身份验证、授权和准入控制器，但首先，让我们更详细地看看Kubernetes
    API对象和自定义资源。
- en: 11.2.1 API objects and custom API objects
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.1 API对象和自定义API对象
- en: 'The essence of Kubernetes is an open platform, meaning open APIs. Opening up
    a platform is what provides additional innovation and creation. The following
    lists a number of API resources associated with a Kubernetes cluster. You will
    recognize some of these API objects (like Deployments and Pods):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的本质是一个开放平台，这意味着开放的API。开放一个平台是提供额外创新和创造的方式。以下列出了一些与Kubernetes集群相关的API资源。您将认识到其中一些API对象（如Deployments和Pods）：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Shows the available APIs, just looking at the first 20 using head
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 显示可用的API，只需查看前20个使用head
- en: 'When we define a YAML manifest with a ClusterRoleBinding, part of that definition
    is the API version. For instance:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用ClusterRoleBinding定义YAML清单时，定义的一部分是API版本。例如：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ The apiVersion matches the API group in the previous code snippet.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ apiVersion与之前代码片段中的API组相匹配。
- en: The `apiVersion` stanza in the previous YAML defines the API’s version. Versioning
    APIs is a complex problem. In order to allow for APIs to move through different
    versions, Kubernetes has the capability to have a version and levels. For instance,
    in the previous YAML definition, you’ll notice that we have `v1beta1` in our `apiVersion`
    definition. This designates that the ClusterRoleBinding is a *beta* API object.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 之前YAML中的`apiVersion`部分定义了API的版本。API版本化是一个复杂的问题。为了允许API通过不同的版本进行迁移，Kubernetes具有版本和级别的功能。例如，在之前的YAML定义中，您会注意到我们在`apiVersion`定义中使用了`v1beta1`。这表示ClusterRoleBinding是一个*beta*
    API对象。
- en: 'API objects have the following levels: alpha, beta, and GA (general availability).
    Objects marked as alpha are ones that are never to be used in production because
    they cause serious upgrade path problems. Alpha API objects are going to change
    and are only for development and experimentation. Beta is really not beta! Beta
    software is often thought of as unstable and not for production, but beta API
    objects in Kubernetes *are* ready for production, and support for these objects
    is guaranteed, unlike alpha objects. For example, DaemonSets were beta for years,
    and virtually everyone ran them in production.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: API对象具有以下级别：alpha、beta和GA（通用可用性）。标记为alpha的对象永远不应该在生产中使用，因为它们会导致严重的升级路径问题。alpha
    API对象将会改变，并且仅用于开发和实验。beta实际上并不是beta！通常认为beta软件是不稳定的，不适合生产，但Kubernetes中的beta API对象**确实**适合生产，并且对这些对象的支持是保证的，与alpha对象不同。例如，DaemonSets已经beta了多年，几乎每个人都把它们用于生产。
- en: The *v1* prefix allows the Kubernetes developers to number the version of the
    API object. For instance, in Kubernetes v1.17.0, the autoscaling APIs are
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*v1*前缀允许Kubernetes开发者对API对象的版本进行编号。例如，在Kubernetes v1.17.0中，自动缩放API是'
- en: /apis/autoscaling/v1
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: /apis/autoscaling/v1
- en: /apis/autoscaling/v2beta1
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: /apis/autoscaling/v2beta1
- en: /apis/autoscaling/v2beta2
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: /apis/autoscaling/v2beta2
- en: 'Notice that this list is in a URI layout. You can view the URI layout of the
    API objects by first starting a `kind` Kubernetes cluster locally:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个列表是URI布局。您可以通过首先在本地启动一个`kind` Kubernetes集群来查看API对象的URI布局：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, run a `kubectl` command on a system that also has a web browser. For
    example:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在同时拥有网络浏览器的系统上运行一个`kubectl`命令。例如：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now go to the URL http://127.0.0.1:8181/. For brevity sake we’re not going to
    show the 120-line response from the API server, but if you do this locally, it
    will give you a graphical view of the API endpoints.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在转到URL http://127.0.0.1:8181/。为了简洁起见，我们不会显示API服务器返回的120行响应，但如果您在本地这样做，它将为您提供API端点的图形视图。
- en: 11.2.2 Custom resource definitions (CRDs)
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.2 自定义资源定义（CRDs）
- en: 'In the ClusterRoleBinding code snippet, we defined CRDs to communicate with
    the `cockroach` database operator. It’s time to discuss why CRDs exist. In Kubernetes
    v1.17.0, we have 54 API objects. The following command will provide some insight:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在ClusterRoleBinding代码片段中，我们定义了CRDs来与`cockroach`数据库操作员通信。现在是时候讨论CRDs存在的原因了。在Kubernetes
    v1.17.0中，我们有54个API对象。以下命令将提供一些见解：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Pipes the result of the kubectl command into wc to count the number of lines,
    words, and characters
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将kubectl命令的结果管道传输到wc以计算行数、单词数和字符数
- en: 'You can understand how much development time it requires to maintain a system
    that contains 54 different objects (frankly, we need more). In order to decouple
    non-core API objects from the API server, CRDs were created. These allow developers
    to create their own API object definition and then, with `kubectl`, apply that
    definition to the API server. The following command creates a CRD object within
    the API server:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以理解维护一个包含 54 个不同对象（坦白说，我们需要更多）的系统需要多少开发时间。为了将非核心 API 对象与 API 服务器解耦，创建了 CRD。这允许开发者创建自己的
    API 对象定义，然后使用 `kubectl` 将该定义应用到 API 服务器。以下命令在 API 服务器中创建一个 CRD 对象：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As with a Pod or other stock API objects within the API server, CRD objects
    extend the Kubernetes API platform programmatically with zero programmer interaction.
    Operators, custom admission controllers, Istio, Envoy, and other technologies
    now use the API server by defining their own CRDs. But these custom objects are
    not tightly coupled to the Kubernete’s API object implementation. Moreover, many
    new core components of Kubernetes are not being added to the stock definitions
    of the API server, but rather are added as CRDs. This, then, is the API server.
    Next, we’ll discuss the first controller that we will cover: the Kubernetes scheduler.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 与 API 服务器中的 Pod 或其他股票 API 对象一样，CRD 对象通过程序化扩展 Kubernetes API 平台，无需程序员交互。操作员、自定义准入控制器、Istio、Envoy
    以及其他技术现在通过定义自己的 CRD 来使用 API 服务器。但是，这些自定义对象并没有紧密耦合到 Kubernetes API 对象的实现中。此外，许多新的
    Kubernetes 核心组件并没有被添加到 API 服务器的基本定义中，而是作为 CRD 添加。这就是 API 服务器。接下来，我们将讨论我们将要覆盖的第一个控制器：Kubernetes
    调度器。
- en: 11.2.3 Scheduler details
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.3 调度器细节
- en: The *scheduler*, like other controllers, is comprised of various control loops
    that handle different events. As of Kubernetes v1.15.0, the scheduler was refactored
    to use a scheduling framework and, additionally, custom plugins. Kubernetes supports
    using custom schedulers that do not run in the actual scheduler but in another
    Pod. However, the problem with custom schedulers is often poor performance.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器，就像其他控制器一样，由各种控制循环组成，处理不同的事件。截至 Kubernetes v1.15.0，调度器被重构为使用调度框架，并且还增加了自定义插件。Kubernetes
    支持使用自定义调度器，这些调度器不在实际的调度器中运行，而是在另一个 Pod 中运行。然而，自定义调度器的问题通常是性能不佳。
- en: 'The first component of the scheduler framework is QueueSort. It sorts the Pods
    that require scheduling into a queue. The framework is then broken into two cycles:
    the scheduling cycle and the binding cycle. First, the scheduling cycle chooses
    which node a Pod runs on. Once the scheduling cycle is complete, the binding cycle
    takes over.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器框架的第一个组件是 QueueSort。它将需要调度的 Pod 排序到一个队列中。然后框架分为两个周期：调度周期和绑定周期。首先，调度周期选择 Pod
    运行的节点。一旦调度周期完成，绑定周期接管。
- en: The scheduler chooses which node the Pod can live on, and actually determining
    if the Pod can live there can take some time. For instance, a Pod needs a volume,
    so that volume needs to be created. What happens if the creation of the required
    volume fails? Then the Pod cannot run on the node and the scheduling for that
    Pod is requeued.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器选择 Pod 可以驻留的节点，实际上确定 Pod 是否可以驻留可能需要一些时间。例如，Pod 需要一个卷，因此需要创建该卷。如果所需卷的创建失败会发生什么？那么
    Pod 就不能在该节点上运行，该 Pod 的调度将被重新排队。
- en: 'We’ll walk through this to get an understanding of when the scheduler, for
    instance, handles Pod NodeAffinity during its scheduling process. Each of the
    cycles has individual components that are laid out in the following structure
    in the Scheduler API. The following code is from the Kubernetes v1.22 release,
    and as of v1.23, it has been refactored, moving to allow plugins to be enabled
    via multipoint. The scheduler itself and plugin fundamentals have not changed
    as of the writing of this book. This code snippet (located at [http://mng.bz/d2oX](http://mng.bz/d2oX))
    defines the various sets of plugins that are registered inside a running scheduling
    instance. Here is the base API definition:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过这个过程来了解调度器在调度过程中处理 Pod NodeAffinity 的时间点。每个周期都有其独立的组件，这些组件在调度器 API 中的结构如下。以下代码来自
    Kubernetes v1.22 版本，截至 v1.23，它已经被重构，允许通过多点启用插件。截至本书编写时，调度器本身和插件基础并未改变。此代码片段（位于
    [http://mng.bz/d2oX](http://mng.bz/d2oX)）定义了在运行中的调度实例内部注册的各种插件集合。以下是基本 API 定义：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Sorts the Pods in a Queue
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在队列中对 Pod 进行排序
- en: ❷ The scheduling cycle plugins start here and end at the Permit plugin.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 调度周期插件从这里开始，结束于 Permit 插件。
- en: ❸ These last three plugins are the binding cycle.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 这最后三个插件是绑定周期。
- en: 'The struct in the previous code snippet is instantiated in [http://mng.bz/rJaZ](http://mng.bz/rJaZ)
    (after 1.21 this code was refactored and moved). In the following code, you will
    recognize scheduling plugins that handle configurations like Pod NodeAffinity,
    which impacts a Pod’s scheduling. The first phase in this process is the QueueSort,
    but note that QueueSort is extendable and thus replaceable:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 之前代码片段中的结构体在 [http://mng.bz/rJaZ](http://mng.bz/rJaZ)（在 1.21 之后此代码被重构并移动）中被实例化。在下面的代码中，你会认出处理
    Pod NodeAffinity 等配置的调度插件，这会影响 Pod 的调度。此过程的第一阶段是队列排序，但请注意，队列排序是可扩展的，因此可以替换：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Calls getDefaultConfig()
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 再次调用 `getDefaultConfig()`
- en: ❷ Calls getDefaultConfig()
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 调用 `getDefaultConfig()`
- en: 'The private function, `getDefaultConfig()`, is called by `NewRegistry` in the
    same Go file. This returns an algorithm provider registry instance. The next members
    that are returned define the scheduling cycle. First, the Prefilter, which is
    a list of plugins run in serial:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 私有函数 `getDefaultConfig()` 在同一 Go 文件中的 `NewRegistry` 被调用。这返回一个算法提供程序注册实例。接下来返回的成员定义了调度周期。首先，是预过滤器，它是一系列按顺序运行的插件：
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Checks if a node has sufficient resources
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 检查节点是否有足够的资源
- en: ❷ Determines if a node has free ports to host the Pod
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 确定节点是否有空闲端口来托管 Pod
- en: ❸ Checks that the PodTopologySpread is met, which allows for the spreading of
    Pods evenly across zones
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 检查 PodTopologySpread 是否满足，这允许 Pod 在区域之间均匀分布
- en: ❹ Handles interpod affinity that, like Pod anti-affinity, repels Pods from nodes
    based on user-defined rules
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 处理节点间亲和性，类似于 Pod 反亲和性，根据用户定义的规则将 Pod 从节点排斥
- en: ❺ This is not actually a filter but creates a cache that is used later during
    the reserve and prebind phases.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 这实际上不是一个过滤器，但创建了一个在保留和预绑定阶段稍后使用的缓存。
- en: 'Next, the filter phase. Note that *Filter* is a list of plugins that determines
    whether a Pod can run on a specific node:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是过滤阶段。请注意，*Filter* 是一个插件列表，用于确定 Pod 是否可以在特定节点上运行：
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Ensures that Pods are not scheduled on a node marked as unschedulable in the
    pastschedulable (for instance, the nodes in the control plane)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 确保Pod不会被调度到过去标记为不可调度的节点（例如，控制平面中的节点）
- en: ❷ Executes the plugin again
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 再次执行插件
- en: ❸ The PodSpec API lets you set a nodeName, which identifies the node where you
    want the Pod.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ PodSpec API 允许您设置一个节点名，该节点名标识了您希望 Pod 所在的节点。
- en: ❹ Plugin executed a second time
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 插件执行了第二次
- en: ❺ Checks if the Pod node selector matches the node label
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 检查 Pod 节点选择器是否匹配节点标签
- en: ❻ Checks if various volume restric-tions are met
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 检查是否满足各种卷限制
- en: ❼ Checks if a Pod tolerates node taints
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 检查 Pod 是否容忍节点污点
- en: ❽ Checks that the node has the capacity to add more volumes (for instance, a
    node can mount 16 volumes in GCP).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 检查节点是否有能力添加更多卷（例如，节点可以在 GCP 中挂载 16 个卷）。
- en: ❾ Repeats a filter in PreFilter
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 在预过滤器中重复一个过滤器
- en: ❿ Checks that a volume exists in the zone that the node resides in
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 检查节点所在的区域中是否存在卷
- en: ⓫ These two filters are repeated.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 这两个过滤器会重复。
- en: 'In the filtering phase, the scheduler checks various constraints around mounting
    different volumes in GCP, AWS, Azure, ISCI, and RBD. For instance, Pod anti-affinity
    ensures that StatefulSet Pods reside on different nodes. You are probably starting
    to notice that the filters are scheduling Pods from settings that you may have
    already created on a Pod in the past. Now, let’s move on to the PostFilter. This
    plugin runs even if filtering is unsuccessful:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在过滤阶段，调度器检查 GCP、AWS、Azure、ISCI 和 RBD 中挂载不同卷的各种约束。例如，Pod 反亲和性确保有状态集的 Pod 驻留在不同的节点上。你可能已经开始注意到，过滤器是从你可能在过去已经创建在
    Pod 上的设置中调度 Pod 的。现在，让我们继续到后过滤器。即使过滤失败，此插件也会运行：
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Handles Pod preemption
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 处理 Pod 抢占
- en: A user can set a priority class for a Pod. If so, the default preemption plugin
    allows the scheduler to determine if it can set another Pod for eviction to make
    room for the scheduled Pod in the priority class. Note that these plugins do all
    of the filtering to determine if a Pod can run on a specific node.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以为 Pod 设置一个优先级类别。如果是这样，默认的抢占插件允许调度器确定是否可以设置另一个 Pod 进行驱逐，以便在优先级类别中为计划中的 Pod腾出空间。请注意，这些插件会执行所有过滤操作以确定
    Pod 是否可以在特定节点上运行。
- en: 'Next comes the scoring. The scheduler builds a list of which nodes the Pod
    can run on, so now it’s time to rank the list of nodes that can host the Pod by
    scoring the nodes. Because the scoring component is also part of the plugins that
    filter the nodes, you will notice a lot of repeated plugin names. The scheduler
    first prescores in order to create a shareable list for the score plugins:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是评分。调度器构建一个Pod可以运行的节点列表，现在是时候通过评分节点来对可以托管Pod的节点列表进行排名。因为评分组件也是过滤节点的插件之一，所以你会注意到很多重复的插件名称。调度器首先进行预评分，以便为评分插件创建一个可共享的列表：
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ All the defined plugins have already run during the filtering process.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在过滤过程中已经运行了所有定义的插件。
- en: 'The next code snippet defines the reuse of various repeated plugins, but also
    some new ones. The scheduler defines a weight value, which impacts the scheduling.
    All nodes that are scored have passed the different filtering phases:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段定义了各种重复插件的重复使用，但也定义了一些新的插件。调度器定义了一个权重值，它会影响调度。所有评分的节点都已通过不同的过滤阶段：
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Prioritizes nodes with balanced resource usage
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 优先考虑资源使用平衡的节点
- en: ❷ Nodes already downloading the Pod’s image(s) score higher.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 已经下载Pod的镜像的节点得分更高。
- en: ❸ Repeated plugin to score the cache that was built
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 重复插件以评分构建的缓存
- en: ❹ Nodes with fewer requests are favored.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 优先考虑请求较少的节点。
- en: ❺ Repeated plugin, again scoring the cache that was built
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 重复插件，再次评分构建的缓存
- en: ❻ Lowers a node score if preferAvoidPods is set
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 如果设置了preferAvoidPods，则降低节点分数
- en: ❼ Repeats these two plugins
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 重复这两个插件
- en: 'When prioritizing nodes with balanced resource usage, the scheduler calculates
    CPU, memory, and volume fractions. The algorithm is as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当优先考虑资源使用平衡的节点时，调度器计算CPU、内存和卷分数。算法如下：
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This algorithm scores the nodes with fewer requests. The node label, `preferAvoidPods`,
    denotes that a node should be avoided for scheduling.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法对请求较少的节点进行评分。节点标签`preferAvoidPods`表示应避免该节点进行调度。
- en: 'The last step in the filtering process is the reserve phase. In the reserve
    phase, we reserve a volume for a Pod to use during the binding cycle. In the following
    code, note that volumebinding is a repeated plugin:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤过程的最后一步是保留阶段。在保留阶段，我们为Pod在绑定周期内使用保留一个卷。在下面的代码中，请注意volumebinding是一个重复的插件：
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ The cache reserves a volume for the Pod.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 缓存为Pod保留一个卷。
- en: 'The scheduling cycle, which mostly filters nodes, determines which node the
    Pod should run on. But ensuring that the Pod actually runs on that node is a much
    longer process, and you may find the Pod requeued for scheduling. Let’s now look
    at the binding cycle in the scheduler framework, starting with a prebind phase.
    The following snippet shows the code for the PreBind plugin:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 调度周期，主要过滤节点，确定Pod应该运行在哪个节点上。但是确保Pod实际上运行在那个节点上是一个更长的过程，你可能会发现Pod被重新排队进行调度。现在让我们看看调度框架中的绑定周期，从预绑定阶段开始。下面的代码片段显示了PreBind插件的代码：
- en: '[PRE18]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Binds the volume to the Pod
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将卷绑定到Pod上
- en: ❷ Saves a Bind object via the API server, updating the node a Pod will start
    on
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过API服务器保存绑定对象，更新Pod将启动的节点
- en: 'Through all of this, the scheduler has multiple queues: an active queue, which
    is the scheduling Pod, and a backoff queue, which includes Pods that are not schedulable.
    The registry in the scheduler does not instantiate plugins for two different phases:
    Permit and PostBind. These entry points are used by other plugins, such as the
    batch scheduler, which soon will be an external plugin for the scheduler. Because
    we now have a scheduling framework, we can use and register other custom scheduler
    plugins. An example of these custom plugins can be found in the GitHub repository
    at [http://mng.bz/oaBN](http://mng.bz/oaBN).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些过程中，调度器有多个队列：一个活动队列，即要调度的Pod，和一个回退队列，其中包含不可调度的Pod。调度器中的注册表不会为两个不同的阶段：许可和PostBind实例化插件。这些入口点被其他插件使用，例如批处理调度器，它很快将成为调度器的外部插件。因为我们现在有一个调度框架，我们可以使用和注册其他自定义调度插件。这些自定义插件的示例可以在GitHub存储库中找到，网址为[http://mng.bz/oaBN](http://mng.bz/oaBN)。
- en: 11.2.4 Recap of scheduling
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.4 调度总结
- en: Figure 11.1 shows the three components that make up the scheduler framework.
    These include
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1显示了组成调度框架的三个组件。这些包括
- en: '*Queue builder*—Maintains a queue of Pods'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*队列构建器*—维护Pod队列'
- en: '*Scheduling cycle*—Filters nodes to find one to run the Pod'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*调度周期*—过滤节点以找到运行Pod的节点'
- en: '*Binding cycle*—Saves data to the API server, along with binding information'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*绑定周期*—将数据保存到API服务器，包括绑定信息'
- en: '![](../Images/CH11_F01_love.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH11_F01_love.png)'
- en: Figure 11.1 The Kubernetes scheduler
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 Kubernetes调度器
- en: 11.3 The controller manager
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 控制器管理器
- en: A lot of the functionality that was contained within KCM (the Kubernetes controller
    manager) was moved into CCM (the cloud controller manager). This binary is a combination
    of four components that are controllers themselves or just simply control loops.
    We’ll look at these in the following sections.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 许多原本包含在KCM（Kubernetes控制器管理器）中的功能已被移动到CCM（云控制器管理器）。这个二进制文件是四个组件的组合，这些组件本身是控制器或者仅仅是控制循环。我们将在接下来的章节中探讨这些内容。
- en: 11.3.1 Storage
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.1 存储
- en: Storage in Kubernetes is a bit of a moving target. As functionality moves out
    of KCM and into the CCM, there are also major changes with how storage functions
    inside of the Kubernetes control plane. Before the move, KCM storage adapters
    existed in the main repository, kubernetes/kubernetes. A user created a PVC (PersistentVolumeClaim)
    in the cloud, and the KCM called code that was inside the Kubernetes project.
    Then, there were flex volume controllers, which are still in existence today.
    But, looping back to KCM, it drives the creation of storage objects as of Kubernetes
    v1.18.x.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的存储有点像移动的目标。随着功能从KCM移出并进入CCM，Kubernetes控制平面中的存储功能也发生了重大变化。在迁移之前，KCM存储适配器存在于主仓库kubernetes/kubernetes中。用户在云中创建了一个PVC（持久卷声明），然后KCM调用了Kubernetes项目内部的代码。然后，还有灵活的卷控制器，这些控制器至今仍然存在。但是，回到KCM，它驱动了Kubernetes
    v1.18.x版本中存储对象的创建。
- en: When a user creates a PV or a PVC, or a PVC/PV combination that is required
    by the creation of a StatefulSet, a component on the control plane must initiate
    and control the creation of a storage volume. That storage volume can be hosted
    by a cloud provider or created in another virtual environment. The point to hold
    on to is that the KCM controls the creation and deletion of storage. Let’s go
    through the controllers that make up KCM.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户创建一个PV或PVC，或者一个需要创建StatefulSet的PVC/PV组合时，控制平面上的一个组件必须启动并控制存储卷的创建。这个存储卷可以由云提供商托管或在另一个虚拟环境中创建。需要注意的是，KCM控制存储的创建和删除。让我们来看看构成KCM的控制器。
- en: The Node controller watches for when a node goes down. It then updates the node
    status in the Nodes API object.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 节点控制器会监视节点何时宕机。然后，它会更新节点状态在Nodes API对象中。
- en: The Replication controller maintains the correct number of Pods for every replication
    controller object in the system. Replication controller objects, for the most
    part, have been replaced by deployments that use ReplicaSets.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 复制控制器维护系统中每个复制控制器对象的正确Pod数量。复制控制器对象大部分已被使用ReplicaSets的部署所取代。
- en: 'The Endpoint controller is the last controller, which manages Endpoint objects.
    The Endpoint object is defined within the Kubernetes API. These objects are typically
    not maintained manually, but are created to provide `kube-proxy` the information
    to join a Pod to a service. A Service may have one or more Pods that handle traffic
    from said Service. Here is an example of the endpoints created for `kube-dns`
    on a `kind` cluster:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 端点控制器是最后一个控制器，它管理端点对象。端点对象定义在Kubernetes API中。这些对象通常不是手动维护的，但它们被创建来为`kube-proxy`提供将Pod连接到服务的信息。一个服务可以有一个或多个Pod处理来自该服务的流量。以下是在`kind`集群上为`kube-dns`创建的端点示例：
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ The IP addresses of the Pods that are a member of the kube-dns service
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 属于kube-dns服务的Pod的IP地址
- en: 11.3.2 Service accounts and tokens
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.2 服务账户和令牌
- en: When a new Namespace is generated, the Kubernetes controller manager creates
    a default ServiceAccount and API access tokens for the new Namespace. If you do
    not name a specific ServiceAccount when you define a Pod, it joins the default
    ServiceAccount created in the Namespace. The ServiceAccount, not surprisingly,
    is used when a Pod accesses the cluster’s API server. When the Pod is started,
    the API access token is mounted to the Pod, unless a user disables the mounting
    of the token.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当生成新的命名空间时，Kubernetes控制器管理器为新的命名空间创建默认的服务账户和API访问令牌。如果您在定义Pod时没有指定特定的服务账户，它将加入在命名空间中创建的默认服务账户。不出所料，当Pod访问集群的API服务器时，会使用服务账户。当Pod启动时，API访问令牌会被挂载到Pod上，除非用户禁用了令牌的挂载。
- en: tip If a Pod does not need the ServiceAccount token, disable the mounting of
    the token by setting `automountServiceAccountToken` to `false`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果一个Pod不需要ServiceAccount令牌，可以通过将`automountServiceAccountToken`设置为`false`来禁用令牌的挂载。
- en: 11.4 Kubernetes cloud controller managers (CCMs)
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.4 Kubernetes云控制器管理器（CCM）
- en: 'Say we have a Kubernetes cluster running on a cloud service, or we are running
    Kubernetes on top of a virtualization provider. Either way, these different hosting
    platforms maintain different cloud controllers that interact with the API layer
    where Kubernetes is hosted. If you wanted to write a new cloud controller, you
    would need to include functionality for the following components:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个在云服务上运行的Kubernetes集群，或者我们在虚拟化提供商上运行Kubernetes。无论哪种方式，这些不同的托管平台都维护着不同的云控制器，它们与Kubernetes托管层的API层进行交互。如果您想编写一个新的云控制器，您需要包括以下组件的功能：
- en: '*Nodes*—For virtual instances'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*节点*—用于虚拟实例'
- en: '*Routing*—For traffic between nodes'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*路由*—用于节点间的流量'
- en: '*External LoadBalancers*—To create a load balancer that is external to the
    nodes in the cluster'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*外部负载均衡器*—用于创建集群节点外部的负载均衡器'
- en: 'The code behind interacting with these components inside of a cloud provider
    is specific to the provider via their APIs. The cloud controller interface now
    defines a common interface for different cloud providers. In order for Omega,
    for example, to build a cloud provider for Kubernetes, we need to build a controller
    that utilizes the following interface:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 与云服务提供商内部组件交互的代码是通过其API针对特定提供商的。云控制器接口现在为不同的云提供商定义了一个通用接口。例如，为了让Omega为Kubernetes构建一个云提供商，我们需要构建一个利用以下接口的控制器：
- en: '[PRE20]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The recommended design pattern for a CCM is to implement three control loops
    (controllers). These are then typically deployed as a single binary.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CCM，推荐的设计模式是实现三个控制循环（控制器）。这些通常作为一个单一的二进制文件部署。
- en: In order to mount a cloud volume onto a node, we have to find the node in the
    cloud, and the Node controller provides this functionality. The controller has
    to know which nodes are in a cluster, and this is beyond the information that
    the kubelet provides when a node starts. When running in a cloud environment,
    Kubernetes needs specific information (for example, zone information) about the
    node and how it is deployed in the cloud environment. Also, there is a layer that
    determines if a node has been completely deleted from a cloud environment. The
    node controller provides a bridge between the cloud API layer and stores that
    information in the API server.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将云卷挂载到节点上，我们必须在云中找到该节点，而Node控制器提供了这一功能。控制器必须知道集群中哪些节点，这超出了节点启动时kubelet提供的信息。在云环境中运行时，Kubernetes需要关于节点及其在云环境中的部署的具体信息（例如，区域信息）。此外，还有一个层来确定节点是否已完全从云环境中删除。节点控制器在云API层和存储之间提供了一个桥梁，并将这些信息存储在API服务器中。
- en: Kubernetes has to route traffic between nodes and this is handled by the Route
    controller. If a cloud requires configuration to route data between nodes, the
    CCM makes the API calls to all network traffic between nodes.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes需要在节点间路由流量，这由Route控制器处理。如果云需要配置来在节点间路由数据，CCM会对节点间的所有网络流量进行API调用。
- en: The name *service controller* is a bit of a misnomer. The service controller
    is only a controller that facilitates the creation of LoadBalancer type services
    within a cluster. It does not facilitate anything with ClusterIP services within
    a Kubernetes cluster.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: “服务控制器”这个名字有点误导。服务控制器只是一个控制器，它仅用于在集群内创建LoadBalancer类型的服务。它不促进Kubernetes集群内ClusterIP服务的任何操作。
- en: 11.5 Further reading
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.5 进一步阅读
- en: 'Acetozi. “Kubernetes Master Components: Etcd, API Server, Controller Manager,
    and Scheduler.” [http://mng.bz/doKX](http://mng.bz/doKX) (accessed 12/29/2021).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Acetozi. “Kubernetes主组件：Etcd、API服务器、控制器管理器和调度器。” [http://mng.bz/doKX](http://mng.bz/doKX)（访问日期：2021年12月29日）。
- en: Summary
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The Kubernetes control plane provides the functionality to orchestrate and host
    Pods in a Kubernetes cluster.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes控制平面提供了在Kubernetes集群中编排和托管Pod的功能。
- en: The scheduler is comprised of various control loops that handle different events.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度器由处理不同事件的多个控制循环组成。
- en: The scheduling cycle, which mostly filters nodes, determines which node the
    Pod should run on.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度周期主要过滤节点，确定Pod应该运行在哪个节点上。
- en: Beta API objects in Kubernetes are ready for production. Support for these objects
    is guaranteed, unlike alpha objects.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes中的Beta API对象已准备好投入生产。对这些对象的支持是保证的，与alpha对象不同。
- en: KCM and CCM work together to provision storage, Services, LoadBalancers, and
    other components via the different controllers that comprise the KCM and the CCM.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KCM和CCM协同工作，通过KCM和CCM包含的不同控制器提供存储、服务、负载均衡器和其他组件。
