- en: front matter
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前置材料
- en: foreword
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前言
- en: So, here’s the thing about reinforcement learning. It is difficult to learn
    and difficult to teach, for a number of reasons. First, it’s quite a technical
    topic. There is a great deal of math and theory behind it. Conveying the right
    amount of background without drowning in it is a challenge in and of itself.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，关于强化学习，这里有一些事情。由于多种原因，它既难以学习，也难以教授。首先，它是一个非常技术性的主题。它背后有大量的数学和理论。在不陷入其中时传达正确数量的背景知识本身就是一项挑战。
- en: 'Second, reinforcement learning encourages a conceptual error. RL is both a
    way of thinking about decision-making problems and a set of tools for solving
    those problem. By “a way of thinking,” I mean that RL provides a framework for
    making decisions: it discusses states and reinforcement signals, among other details.
    When I say “a set of tools,” I mean that when we discuss RL, we find ourselves
    using terms like *Markov decision processes* and *Bellman updates*. It is remarkably
    easy to confuse the way of thinking with the mathematical tools we use in response
    to that way of thinking.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，强化学习鼓励了一种概念错误。强化学习既是思考决策问题的一种方式，也是解决这些问题的工具集。我所说的“一种方式”是指强化学习提供了一个决策框架：它讨论了状态和强化信号等细节。我所说的“一套工具”是指当我们讨论强化学习时，我们会发现自己使用诸如*马尔可夫决策过程*和*贝尔曼更新*等术语。将思维方式与我们对这种思维方式所使用的数学工具混淆是极其容易的。
- en: Finally, RL is implementable in a wide variety of ways. Because RL is a way
    of thinking, we can discuss it by trying to realize the framework in a very abstract
    way, or ground it in code, or, for that matter, in neurons. The substrate one
    decides to use makes these two difficulties even more challenging—which bring
    us to deep reinforcement learning.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，强化学习可以用多种方式实现。因为强化学习是一种思维方式，我们可以通过尝试以非常抽象的方式实现框架来讨论它，或者将其基于代码，或者，就事论事，基于神经元。一个人决定使用的底层使得这两个困难变得更加具有挑战性——这把我们带到了深度强化学习。
- en: Focusing on deep reinforcement learning nicely compounds all these problems
    at once. There is background on RL, and background on deep neural networks. Both
    are separately worthy of study and have developed in completely different ways.
    Working out how to explain both in the context of developing tools is no easy
    task. Also, do not forget that understanding RL requires understanding not only
    the tools and their realization in deep networks, but also understanding the way
    of thinking about RL; otherwise, you cannot generalize beyond the examples you
    study directly. Again, teaching RL is hard, and there are so many ways for teaching
    deep RL to go wrong—which brings us to Miguel Morales and this book.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 专注于深度强化学习很好地将这些问题一次性放大。关于强化学习（RL）和深度神经网络都有背景知识。两者都值得单独研究，并且以完全不同的方式发展。在开发工具的背景下解释这两者的方法并不容易。此外，不要忘记，理解强化学习不仅需要理解工具及其在深度网络中的实现，还需要理解关于强化学习的思维方式；否则，你无法将所学内容推广到直接研究的例子之外。再次强调，教授强化学习是困难的，而且有那么多错误的方法可以用来教授深度强化学习——这把我们带到了米格尔·莫雷莱斯和这本书。
- en: This book is very well put together. It explains in technical but clear language
    what machine learning is, what deep learning is, and what reinforcement learning
    is. It allows the reader to understand the larger context of where the field is
    and what you can do with the techniques of deep RL, but also the way of thinking
    that ML, RL, and deep RL present. It is clear and concise. Thus, it works as both
    a learning guide and as a reference, and, at least for me, as a source of some
    inspiration.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书组织得非常好。它用技术但清晰的语言解释了机器学习是什么，深度学习是什么，以及强化学习是什么。它使读者能够理解该领域的更大背景，以及你可以使用深度强化学习的技巧做什么，同时也理解了机器学习（ML）、强化学习（RL）和深度强化学习所呈现的思维方式。它清晰简洁。因此，它既是一本学习指南，也是一本参考书，至少对我来说，它还是一些灵感的来源。
- en: I am not surprised by any of this. I’ve known Miguel for quite a few years now.
    He went from taking machine learning courses to teaching them. He has been the
    lead teaching assistant on my Reinforcement Learning and Decision Making course
    for the Online Masters of Science at Georgia Tech for more semesters than I can
    count. He’s reached thousands of students during that time. I’ve watched him grow
    as a practitioner, a researcher, and an educator. He has helped to make the RL
    course at GT better than it started out, and continues even as I write this to
    make the experience of grokking reinforcement learning a deeper one for the students.
    He is a natural teacher.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我对这一切并不感到惊讶。我已经认识米格尔好几年了。他从学习机器学习课程到教授它们。他作为佐治亚理工学院在线硕士学位课程《强化学习与决策制定》的主要助教，比我能够数得上的学期还要多。在那段时间里，他影响了成千上万的学生。我看着他作为一个从业者、一个研究人员和一个教育者的成长。他帮助使佐治亚理工学院的RL课程比最初更好，而且在我写这篇文章的时候，还在继续使强化学习的理解对学生来说更加深入。他是一个天生的教师。
- en: This text reflects his talent. I am happy to be able to work with him, and I’m
    happy he’s been moved to write this book. Enjoy. I think you’ll learn a lot. I
    learned a few things myself.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章反映了他的才华。我很高兴能够与他合作，并且我很高兴他决定写这本书。享受吧。我想你会学到很多东西。我自己也学到了一些东西。
- en: Charles Isbell, Jr.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 查尔斯·伊斯贝尔，小
- en: Professor and John P. Imlay Jr. Dean
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 教授和约翰·P·伊莱，小
- en: College of Computing
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机学院
- en: Georgia Institute of Technology
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 乔治亚理工学院
- en: preface
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前言
- en: Reinforcement learning is an exciting field with the potential to make a profound
    impact on the history of humankind. Several technologies have influenced the history
    of our world and changed the course of humankind, from fire, to the wheel, to
    electricity, to the internet. Each technological discovery propels the next discovery
    in a compounding way. Without electricity, the personal computer wouldn’t exist;
    without it, the internet wouldn’t exist; without it, search engines wouldn’t exist.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一个充满活力、有可能对人类历史产生深远影响的领域。几种技术已经影响了我们世界的历史，并改变了人类历史的进程，从火、轮子、电到互联网。每一次技术发现都以累积的方式推动下一次发现。没有电，个人电脑就不会存在；没有它，互联网就不会存在；没有它，搜索引擎就不会存在。
- en: To me, the most exciting aspect of RL and artificial intelligence, in general,
    is not so much to merely have other intelligent entities next to us, which is
    pretty exciting, but instead, what comes after that. I believe reinforcement learning,
    being a robust framework for optimizing specific tasks autonomously, has the potential
    to change the world. In addition to task automation, the creation of intelligent
    machines may drive the understanding of human intelligence to places we have never
    been before. Arguably, if you can know with certainty how to find optimal decisions
    for every problem, you likely understand the algorithm that finds those optimal
    decisions. I have a feeling that by creating intelligent entities, humans can
    become more intelligent beings.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，强化学习和人工智能最令人兴奋的方面，与其说是仅仅在我们身边有其他智能实体，这确实很令人兴奋，不如说是那之后的事情。我相信强化学习作为一个用于自主优化特定任务的稳健框架，有可能改变世界。除了任务自动化之外，智能机器的创造可能会推动我们对人类智能的理解达到前所未有的地方。可以说，如果你能够确定地找到每个问题的最优决策，你很可能就理解了找到这些最优决策的算法。我有一种感觉，通过创造智能实体，人类可以成为更加智能的存在。
- en: But we are far away from this point, and to fulfill these wild dreams, we need
    more minds at work. Reinforcement learning is not only in its infancy, but it’s
    been in that state for a while, so there is much work ahead. The reason I wrote
    this book is to get more people grokking deep RL, and RL in general, and to help
    you contribute.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们离这个目标还远着呢，要实现这些宏伟的梦想，我们需要更多的智慧。强化学习不仅处于起步阶段，而且已经处于这种状态有一段时间了，所以前方还有许多工作要做。我写这本书的原因是让更多的人理解深度强化学习，以及一般的强化学习，并帮助你做出贡献。
- en: Even though the RL framework is intuitive, most of the resources out there are
    difficult to understand for newcomers. My goal was not to write a book that provides
    code examples only, and most definitely not to create a resource that teaches
    the theory of reinforcement learning. Instead, my goal was to create a resource
    that can bridge the gap between theory and practice. As you’ll soon see, I don’t
    shy away from equations; they are essential if you want to grok a research field.
    And, even if your goal is practical, to build quality RL solutions, you still
    need that theoretical foundation. However, I also don’t solely rely on equations
    because not everybody interested in RL is fond of math. Some people are more comfortable
    with code and concrete examples, so this book provides the practical side of this
    fantastic field.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管强化学习框架直观易懂，但大多数资源对于新手来说都难以理解。我的目标不是写一本只提供代码示例的书，更不是创建一本教授强化学习理论的资源。相反，我的目标是创建一个能够弥合理论与实践之间差距的资源。正如你很快就会看到的，我不回避方程式；如果你想要深入理解一个研究领域，它们是必不可少的。即使你的目标是实际应用，构建高质量的强化学习解决方案，你仍然需要这个理论基础。然而，我也不会仅仅依赖于方程式，因为并不是所有对强化学习感兴趣的人都喜欢数学。有些人更习惯于代码和具体的例子，所以这本书提供了这个精彩领域的实际应用方面。
- en: 'Most of my effort during this three-year project went into bridging this gap;
    I don’t shy away from intuitively explaining the theory, and I don’t just plop
    down code examples. I do both, and in a very detail-oriented fashion. Those who
    have a hard time understanding the textbooks and lectures can more easily grasp
    the words top researchers use: why those specific words, why not other words.
    And those who know the words and love reading the equations but have trouble seeing
    those equations in code and how they connect can more easily understand the practical
    side of reinforcement learning.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个为期三年的项目中，我大部分的努力都投入到了弥合这个差距上；我不回避直观地解释理论，也不只是简单地列出代码示例。我两者都做，而且非常注重细节。对于那些理解教科书和讲座有困难的人来说，更容易掌握顶尖研究者使用的词汇：为什么用这些特定的词汇，而不是其他词汇。而对于那些知道这些词汇并喜欢阅读方程式，但难以看到这些方程式在代码中的表现形式以及它们如何连接的人来说，更容易理解强化学习的实际应用方面。
- en: Finally, I hope you enjoy this work, and more importantly that it does fulfill
    its goal for you. I hope that you emerge grokking deep reinforcement learning
    and can give back and contribute to this fantastic community that I’ve grown to
    love. As I mentioned before, you wouldn’t be reading this book if it wasn’t for
    a myriad of relatively recent technological innovations, but what happens after
    this book is up to you, so go forth and make an impact in the world.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我希望你喜欢这份工作，更重要的是，它确实实现了你的目标。我希望你能深入理解深度强化学习，并能够回馈并贡献于这个我逐渐热爱的出色社区。正如我之前提到的，如果不是因为一系列相对较新的技术创新，你不会读到这本书。但书后的世界取决于你，所以勇敢地去影响世界吧。
- en: acknowledgments
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: I want to thank the people at Georgia Tech for taking the risk and making available
    the first Online Master of Science in Computer Science for anyone in the world
    to get a high-quality graduate education. If it weren’t for those folks who made
    it possible, I probably would not have written this book.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我想感谢乔治亚理工学院的那些人，他们承担了风险，为世界上任何人都提供了第一个在线计算机科学硕士学位，让他们能够获得高质量的毕业教育。如果不是那些使这一切成为可能的人，我可能就不会写这本书。
- en: I want to thank Professor and Dean Charles Isbell and Professor Michael Littman
    for putting together an excellent reinforcement-learning course. I have a special
    appreciation for Dean Isbell, who has given me much room to grow and learn RL.
    Also, the way I teach reinforcement learning—by splitting the problem into three
    types of feedback—I learned from Professor Littman. I’m grateful to have received
    instruction from them.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我想感谢查尔斯·伊斯贝尔教授和迈克尔·利特曼教授，他们共同开设了一门优秀的强化学习课程。我对伊斯贝尔院长特别感激，他给了我很多成长和学习强化学习的机会。此外，我教授强化学习的方式——通过将问题分为三种类型的反馈——是从利特曼教授那里学到的。我非常感激能够从他们那里接受指导。
- en: 'I want to thank the vibrant teaching staff at Georgia Tech’s CS 7642 for working
    together on how to help students learn more and enjoy their time with us. Special
    thanks go to Tim Bail, Pushkar Kolhe, Chris Serrano, Farrukh Rahman, Vahe Hagopian,
    Quinn Lee, Taka Hasegawa, Tianhang Zhu, and Don Jacob. You guys are such great
    teammates. I also want to thank the folks who previously contributed significantly
    to that course. I’ve gotten a lot from our interactions: Alec Feuerstein, Valkyrie
    Felso, Adrien Ecoffet, Kaushik Subramanian, and Ashley Edwards. I want to also
    thank our students for asking the questions that helped me identify the gaps in
    knowledge for those trying to learn RL. I wrote this book with you in mind. A
    very special thank you goes out to that anonymous student who recommended me to
    Manning for writing this book; I still don’t know who you are, but you know who
    you are. Thank you.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我要感谢乔治亚理工学院CS 7642课程的充满活力的教学团队，他们一起努力探讨如何帮助学生学得更多并享受与我们在一起的时光。特别感谢Tim Bail、Pushkar
    Kolhe、Chris Serrano、Farrukh Rahman、Vahe Hagopian、Quinn Lee、Taka Hasegawa、Tianhang
    Zhu和Don Jacob。你们都是如此优秀的队友。我还要感谢那些之前对该课程做出重大贡献的人。我从我们的互动中学到了很多：Alec Feuerstein、Valkyrie
    Felso、Adrien Ecoffet、Kaushik Subramanian和Ashley Edwards。我还要感谢我们的学生，是他们的提问帮助我识别了那些试图学习强化学习的人的知识盲点。这本书是为你们写的。特别感谢那位推荐我给Manning写这本书的匿名学生；我仍然不知道你是谁，但你很清楚你是谁。谢谢。
- en: I want to thank the folks at Lockheed Martin for all their feedback and interactions
    during my time writing this book. Special thanks go to Chris Aasted, Julia Kwok,
    Taylor Lopez, and John Haddon. John was the first person to review my earliest
    draft, and his feedback helped me move the writing to the next level.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我要感谢Lockheed Martin团队在我写这本书期间的所有反馈和互动。特别感谢Chris Aasted、Julia Kwok、Taylor Lopez和John
    Haddon。John是第一个审阅我早期草稿的人，他的反馈帮助我将写作提升到下一个层次。
- en: 'I want to thank the folks at Manning for providing the framework that made
    this book a reality. I thank Brian Sawyer for reaching out and opening the door;
    Bert Bates for setting the compass early on and helping me focus on teaching;
    Candace West for helping me go from zero to something; Susanna Kline for helping
    me pick up the pace when life got busy; Jennifer Stout for cheering me on through
    the finish line; Rebecca Rinehart for putting out fires; Al Krinker for providing
    me with actionable feedback and helping me separate the signal from the noise;
    Matko Hrvatin for keeping up with MEAP releases and putting that extra pressure
    on me to keep writing; Candace Gillhoolley for getting the book out there, Stjepan
    Jurekovic´ for getting me out there; Ivan Martinovic for getting the much-needed
    feedback to improve the text; Lori Weidert for aligning the book to be production-ready
    twice; Jennifer Houle for being gentle with the design changes; Katie Petito for
    patiently working through the details; Katie Tennant for the meticulous and final
    polishing touches; and to anyone I missed, or who worked behind the scenes to
    make this book a reality. There are more, I know: thank you all for your hard
    work.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我要感谢Manning团队为使这本书成为现实提供的框架。感谢Brian Sawyer主动联系并打开大门；感谢Bert Bates在早期设定方向并帮助我专注于教学；感谢Candace
    West帮助我从零开始有所成就；感谢Susanna Kline在我忙碌时帮助我加快进度；感谢Jennifer Stout在我冲过终点线时为我加油；感谢Rebecca
    Rinehart处理紧急事务；感谢Al Krinker为我提供可操作的反馈并帮助我区分信号和噪音；感谢Matko Hrvatin跟进MEAP发布并给我施加额外的写作压力；感谢Candace
    Gillhoolley将这本书推广出去；感谢Stjepan Jurekovic´帮助我推广；感谢Ivan Martinovic为改进文本提供必要的反馈；感谢Lori
    Weidert两次将书籍调整为生产就绪；感谢Jennifer Houle在设计变更上保持温和；感谢Katie Petito耐心地处理细节；感谢Katie Tennant进行细致和最终的润色；以及任何我遗漏的人，或者那些在幕后使这本书成为现实的人。我知道还有更多：感谢你们的辛勤工作。
- en: To all the reviewers—Al Rahimi, Alain Couniot, Alberto Ciarlanti, David Finton,
    Doniyor Ulmasov, Edisson Reinozo, Ezra Joel Schroeder, Hank Meisse, Hao Liu, Ike
    Okonkwo, Jie Mei, Julien Pohie, Kim Falk Jørgensen, Marc-Philippe Huget, Michael
    Haller, Michel Klomp, Nacho Ormeño, Rob Pacheco, Sebastian Maier, Sebastian Zaba,
    Swaminathan Subramanian, Tyler Kowallis, Ursin Stauss, and Xiaohu Zhu—thank you,
    your suggestions helped make this a better book.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 向所有审稿人——Al Rahimi、Alain Couniot、Alberto Ciarlanti、David Finton、Doniyor Ulmasov、Edisson
    Reinozo、Ezra Joel Schroeder、Hank Meisse、Hao Liu、Ike Okonkwo、Jie Mei、Julien Pohie、Kim
    Falk Jørgensen、Marc-Philippe Huget、Michael Haller、Michel Klomp、Nacho Ormeño、Rob
    Pacheco、Sebastian Maier、Sebastian Zaba、Swaminathan Subramanian、Tyler Kowallis、Ursin
    Stauss和Xiaohu Zhu——表示感谢，你们的建议帮助使这本书变得更好。
- en: I want to thank the folks at Udacity for letting me share my passion for this
    field with their students and record the actor-critic lectures for their Deep
    Reinforcement Learning Nanodegree. Special thanks go to Alexis Cook, Mat Leonard,
    and Luis Serrano.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我要感谢Udacity的团队，他们让我有机会与他们学生分享我对这个领域的热情，并为他们深度强化学习纳米学位录制演员-评论家讲座。特别感谢Alexis Cook、Mat
    Leonard和Luis Serrano。
- en: I want to thank the RL community for helping me clarify the text and improve
    my understanding. Special thanks go to David Silver, Sergey Levine, Hado van Hasselt,
    Pascal Poupart, John Schulman, Pieter Abbeel, Chelsea Finn, Vlad Mnih, for their
    lectures; Rich Sutton for providing the gold copy of the field in a single place
    (his textbook); and James MacGlashan, and Joshua Achiam for their codebases, online
    resources, and guidance when I didn’t know where to go to get an answer to a question.
    I want to thank David Ha for giving me insights as to where to go next.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我要感谢强化学习社区帮助我澄清文本并提高我的理解。特别感谢David Silver、Sergey Levine、Hado van Hasselt、Pascal
    Poupart、John Schulman、Pieter Abbeel、Chelsea Finn、Vlad Mnih，感谢他们的讲座；Rich Sutton将这个领域的黄金副本集中在一个地方（他的教科书）；以及James
    MacGlashan和Joshua Achiam，感谢他们的代码库、在线资源和在我不知道去哪里寻找问题的答案时的指导。我要感谢David Ha，他让我了解到下一步该去哪里。
- en: Special thanks go to Silvia Mora for helping make all the figures in this book
    presentable and helping me in almost every side project that I undertake.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 特别感谢Silvia Mora，她帮助使本书中的所有图表都显得可接受，并在我几乎承担的每一个副项目中给予我帮助。
- en: Finally, I want to thank my family, who were my foundation throughout this project.
    I “knew” writing a book was a challenge, and then I learned. But my wife and kids
    were there regardless, waiting for my 15-minute breaks every 2 hours or so during
    the weekends. Thank you, Solo, for brightening up my life midway through this
    book. Thank you, Rosie, for sharing your love and beauty, and thank you Danelle,
    my wonderful wife, for everything you are and do. You are my perfect teammate
    in this interesting game called life. I’m so glad I found you.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我要感谢我的家人，他们是我整个项目的基础。我知道写一本书是一个挑战，然后我学会了。但我的妻子和孩子始终在那里，在周末每隔两小时左右等待我的15分钟休息时间。感谢Solo，在本书中途照亮了我的生活。感谢Rosie，分享你的爱和美丽，感谢Danelle，我亲爱的妻子，因为你的一切和你所做的一切。你是这个有趣的游戏（生活）中的完美队友。我很高兴我找到了你。
- en: about this book
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于这本书
- en: '*grokking Deep Reinforcement Learning* bridges the gap between the theory and
    practice of deep reinforcement learning. The book’s target audience is folks familiar
    with machine learning techniques, who want to learn reinforcement learning. The
    book begins with the foundations of deep reinforcement learning. It then provides
    an in-depth exploration of algorithms and techniques for deep reinforcement learning.
    Lastly, it provides a survey of advanced techniques with the potential for making
    an impact.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*掌握深度强化学习*架起了深度强化学习理论与实践之间的桥梁。本书的目标读者是熟悉机器学习技术、希望学习强化学习的人。本书从深度强化学习的基础开始。然后深入探讨了深度强化学习的算法和技术。最后，它提供了一项具有潜在影响力的高级技术的概述。'
- en: Who should read this book
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 谁应该阅读这本书
- en: Folks who are comfortable with a research field, Python code, a bit of math
    here and there, lots of intuitive explanations, and fun and concrete examples
    to drive the learning will enjoy this book. However, any person only familiar
    with Python can get a lot, given enough interest in learning. Even though basic
    DL knowledge is assumed, this book provides a brief refresher on neural networks,
    backpropagation, and related techniques. The bottom line is that this book is
    self contained, and anyone wanting to play around with AI agents and emerge grokking
    deep reinforcement learning can use this book to get them there.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个研究领域感到舒适、熟悉Python代码、偶尔有一些数学知识、大量的直观解释和有趣且具体的例子来推动学习的人来说，这本书会很有趣。然而，任何只熟悉Python的人，只要对学习有足够的兴趣，都能从中获得很多。尽管假设了基本的深度学习知识，但本书提供了关于神经网络、反向传播和相关技术的简要复习。总之，这本书是自包含的，任何想要尝试与AI代理互动并掌握深度强化学习的人都可以使用这本书来实现目标。
- en: 'How this book is organized: a roadmap'
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本书是如何组织的：一个路线图
- en: This book has 13 chapters divided into two parts.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本书分为两部分，共13章。
- en: In part 1, chapter 1 introduces the field of deep reinforcement learning and
    sets expectations for the journey ahead. Chapter 2 introduces a framework for
    designing problems that RL agents can understand. Chapter 3 contains details of
    algorithms for solving RL problems when the agent knows the dynamics of the world.
    Chapter 4 contains details of algorithms for solving simple RL problems when the
    agent does not know the dynamics of the world. Chapter 5 introduces methods for
    solving the prediction problem, which is a foundation for advanced RL methods.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分，第一章介绍了深度强化学习领域，并为未来的旅程设定了期望。第二章介绍了一个框架，用于设计RL代理能够理解的问题。第三章包含了当代理知道世界动态时解决RL问题的算法细节。第四章包含了当代理不知道世界动态时解决简单RL问题的算法细节。第五章介绍了解决预测问题的方法，这是高级RL方法的基础。
- en: In part 2, chapter 6 introduces methods for solving the control problem, methods
    that optimize policies purely from trial-and-error learning. Chapter 7 teaches
    more advanced methods for RL, including methods that use planning for more sample
    efficiency. Chapter 8 introduces the use of function approximation in RL by implementing
    a simple RL algorithm that uses neural networks for function approximation. Chapter
    9 dives into more advanced techniques for using function approximation for solving
    reinforcement learning problems. Chapter 10 teaches some of the best techniques
    for further improving the methods introduced so far. Chapter 11 introduces a slightly
    different technique for using DL models with RL that has proven to reach state-of-the-art
    performance in multiple deep RL benchmarks. Chapter 12 dives into more advanced
    methods for deep RL, state-of-the-art algorithms, and techniques commonly used
    for solving real-world problems. Chapter 13 surveys advanced research areas in
    RL that suggest the best path for progress toward artificial general intelligence.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分，第六章介绍了解决控制问题的方法，这些方法是纯粹通过试错学习来优化策略的。第七章教授了更高级的RL方法，包括使用规划来提高样本效率的方法。第八章通过实现一个简单的使用神经网络进行函数近似的RL算法，介绍了在RL中使用函数近似的方法。第九章深入探讨了使用函数近似解决强化学习问题的更高级技术。第十章教授了迄今为止介绍的方法的一些最佳改进技术。第十一章介绍了一种使用深度学习模型与RL结合的不同技术，该技术在多个深度RL基准测试中证明了达到最先进性能的能力。第十二章深入探讨了深度RL的更高级方法、最先进的算法以及常用于解决现实世界问题的技术。第十三章概述了RL的高级研究领域，这些领域表明了向通用人工智能进步的最佳路径。
- en: About the code
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于代码
- en: This book contains many examples of source code both in boxes titled “I speak
    Python” and in the text. Source code is formatted in a fixed-width font like this
    to separate it from ordinary text and has syntax highlighting to make it easier
    to read.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本书包含了许多源代码示例，既有标题为“我说Python”的框中的示例，也有文本中的示例。源代码以固定宽度字体格式化，如这样，以将其与普通文本区分开来，并具有语法高亮，以便更容易阅读。
- en: In many cases, the original source code has been reformatted; we’ve added line
    breaks, renamed variables, and reworked indentation to accommodate the available
    page space in the book. In rare cases, even this was not enough, and code includes
    line-continuation operator in Python, the backslash (\), to indicate that a statement
    is continued on the next line.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，原始源代码已经被重新格式化；我们添加了换行符，重命名了变量，并重新调整了缩进以适应书中的可用页面空间。在极少数情况下，即使这样也不够，代码中包含Python中的行续符（\），以指示语句将在下一行继续。
- en: Additionally, comments in the source code have often been removed from the boxes,
    and the code is described in the text. Code annotations point out important concepts.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，源代码中的注释通常已从框中移除，并在文本中描述了代码。代码注释指出了重要概念。
- en: The code for the examples in this book is available for download from the Manning
    website at [https://www.manning.com/books/grokking-deep-reinforcement-learning](https://www.manning.com/books/grokking-deep-reinforcement-learning)
    and from GitHub at [https://github.com/mimoralea/gdrl](https://github.com/mimoralea/gdrl).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的示例代码可以从Manning网站[https://www.manning.com/books/grokking-deep-reinforcement-learning](https://www.manning.com/books/grokking-deep-reinforcement-learning)和GitHub[https://github.com/mimoralea/gdrl](https://github.com/mimoralea/gdrl)下载。
- en: liveBook discussion forum
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: liveBook 讨论论坛
- en: Purchase of *grokking Deep Reinforcement Learning* includes free access to a
    private web forum run by Manning Publications where you can make comments about
    the book, ask technical questions, and receive help from the author and from other
    users. To access the forum, go to [https://livebook.manning.com/#!/book/grokking-deep-reinforcement-learning/discussion](https://livebook.manning.com/#!/book/grokking-deep-reinforcement-learning/discussion).
    You can also learn more about Manning’s forums and the rules of conduct at [https://livebook.manning.com/#!/discussion](https://livebook.manning.com/discussion).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 购买 *掌握深度强化学习* 包括免费访问由Manning Publications运营的私人网络论坛，您可以在论坛上对书籍发表评论、提出技术问题，并从作者和其他用户那里获得帮助。要访问论坛，请访问
    [https://livebook.manning.com/#!/book/grokking-deep-reinforcement-learning/discussion](https://livebook.manning.com/#!/book/grokking-deep-reinforcement-learning/discussion)。您还可以在
    [https://livebook.manning.com/#!/discussion](https://livebook.manning.com/discussion)
    了解更多关于Manning的论坛和行为准则。
- en: Manning’s commitment to our readers is to provide a venue where a meaningful
    dialogue between individual readers and between readers and the author can take
    place. It is not a commitment to any specific amount of participation on the part
    of the author, whose contribution to the forum remains voluntary (and unpaid).
    We suggest you try asking him some challenging questions lest his interest stray!
    The forum and the archives of previous discussions will be accessible from the
    publisher’s website as long as the book is in print.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Manning对我们读者的承诺是提供一个场所，让读者之间以及读者与作者之间可以进行有意义的对话。这不是对作者参与特定数量活动的承诺，作者对论坛的贡献仍然是自愿的（且未付费）。我们建议您尝试向他提出一些挑战性的问题，以免他的兴趣转移！只要书籍在印刷中，论坛和先前讨论的存档将从出版社的网站提供访问。
- en: about the author
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于作者
- en: '*Miguel Morales* works on reinforcement learning at Lockheed Martin, Missiles
    and Fire Control, Autonomous Systems, in Denver, Colorado. He is a part-time Instructional
    Associate at Georgia Institute of Technology for the course in Reinforcement Learning
    and Decision Making. Miguel has worked for Udacity as a machine learning project
    reviewer, a Self-driving Car Nanodegree mentor, and a Deep Reinforcement Learning
    Nanodegree content developer. He graduated from Georgia Tech with a Master’s in
    Computer Science, specializing in interactive intelligence.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*米格尔·莫雷莱斯* 在科罗拉多州丹佛的洛克希德·马丁公司，导弹和火控，自主系统部门从事强化学习工作。他是乔治亚理工学院强化学习和决策课程的部分时间制教学助理。米格尔曾作为机器学习项目审稿人、自动驾驶汽车纳米学位导师和深度强化学习纳米学位内容开发者为Udacity工作。他从乔治亚理工学院计算机科学硕士毕业，专攻交互智能。'
