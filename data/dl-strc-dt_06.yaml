- en: 6 Training the model and running experiments
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 训练模型和运行实验
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Viewing the end-to-end training process
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看端到端训练过程
- en: Selecting subsets of the dataset for training, validation, and testing
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择数据集的子集用于训练、验证和测试
- en: Doing an initial training run
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行初始训练运行
- en: Measuring the performance of your model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量你模型的性能
- en: Optimizing your training time by exploiting Keras’ early stopping feature
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过利用 Keras 的早期停止功能优化训练时间
- en: Shortcuts to scoring
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 得分快捷方式
- en: Saving trained models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存训练好的模型
- en: Running a series of training experiments to improve model performance
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行一系列训练实验以改进模型性能
- en: So far in this book, we have prepared the data and examined the code that makes
    up the model itself. Now we are finally ready to train the model. We’ll review
    some of the basics, including selecting the training, test, and validation dataset.
    Then we’ll go through an initial training run to validate that the code functions
    without errors and will cover the critical topic of monitoring your model’s performance.
    Next, we’ll show how to take advantage of the early stopping facility in Keras
    to get maximum benefit from your training runs. After that, we will go over how
    you can use your trained model to score a new record before you have the model
    fully deployed. Finally, we will run a series of experiments to improve the performance
    of the deep learning model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这本书中，我们已经准备好了数据并检查了构成模型本身的代码。现在我们终于准备好训练模型了。我们将回顾一些基础知识，包括选择训练、测试和验证数据集。然后我们将进行一次初始训练运行以验证代码没有错误，并涵盖监控模型性能的关键主题。接下来，我们将展示如何利用
    Keras 中的早期停止功能从训练运行中获得最大收益。之后，我们将介绍如何在模型完全部署之前使用你的训练模型对新记录进行评分。最后，我们将运行一系列实验以改进深度学习模型的表现。
- en: 6.1 Code for training the deep learning model
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 训练深度学习模型的代码
- en: When you have cloned the GitHub repo ([http://mng.bz/v95x](http://mng.bz/v95x))
    associated with this book, you’ll find the code related to training the model
    in the notebooks sub-directory. The following listing shows the files that contain
    the code described in this chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当你克隆了与本书相关的 GitHub 仓库 ([http://mng.bz/v95x](http://mng.bz/v95x))，你会在 notebooks
    子目录中找到与训练模型相关的代码。以下列表显示了包含本章描述的代码的文件。
- en: Listing 6.1 Code in the repo related to training the model
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.1 仓库中与训练模型相关的代码
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Directory for pickled intermediate dataset dataframes
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 保存中间数据集数据框的目录
- en: ❷ Directory for saving trained models
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 保存训练好的模型的目录
- en: ❸ Contains definitions of pipeline classes
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 包含管道类的定义
- en: ❹ Notebook containing dataset refactoring and model training code
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 包含数据集重构和模型训练代码的笔记本
- en: '❺ Config file for model training: definitions of hyperparameter values, train/validate/test
    proportions, and other configuration parameters'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 模型训练配置文件：超参数值的定义、训练/验证/测试比例以及其他配置参数
- en: ❻ Directory for saved pipelines
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 保存管道的目录
- en: 6.2 Reviewing the process of training a deep learning model
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 回顾训练深度学习模型的过程
- en: If you have already trained a classic machine learning model, the process for
    training a deep learning model will be familiar to you. A detailed review of training
    classic machine learning models is beyond the scope of this book, but Alexey Grigorev’s
    *Machine Learning Bookcamp (Manning Publications, 2021)* has an excellent section
    on the model training process ([http://mng.bz/Qxxm](http://mng.bz/Qxxm)) that
    covers the topic in more detail.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经训练了一个经典的机器学习模型，训练深度学习模型的过程对你来说应该是熟悉的。本书不涉及经典机器学习模型的详细审查，但 Alexey Grigorev
    的 *Machine Learning Bookcamp (Manning Publications, 2021)* 有一个关于模型训练过程的优秀部分 ([http://mng.bz/Qxxm](http://mng.bz/Qxxm))，更详细地介绍了这个主题。
- en: When a deep learning model is trained, the weights associated with each node
    are updated iteratively until the loss function is minimized and the model’s predictive
    performance is optimized. Note that by default, the weights are initialized randomly.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当深度学习模型被训练时，与每个节点关联的权重会迭代更新，直到损失函数最小化并且模型的预测性能优化。请注意，默认情况下，权重是随机初始化的。
- en: 'Let’s consider the high-level steps you would take to train a machine learning
    model:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑你训练机器学习模型时会采取的高级步骤：
- en: '*Define subsets* *of the dataset for training, validation, and testing* *.*
    In addition to the subset of the data used to train the model, reserve a distinct
    subset for validation (checking the performance of the training process in flight)
    and testing (checking the performance of the training process when it is complete).
    It is critical to maintain a subset of the data that was not used in training
    to check the performance of the model. Suppose that you used all your available
    data to train your model with no data reserved for testing. After you trained
    your model, how would you get an idea of its accuracy in making predictions on
    data it had never seen before? Without a validation dataset to track performance
    during training and a test dataset to assess performance when training is complete,
    you would not be able to get an idea of the performance of your model before deployment.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*定义数据集的子集用于训练、验证和测试*。除了用于训练模型的子集之外，还需要为验证（检查训练过程中的性能）和测试（检查训练完成时的性能）保留一个独特的子集。维护一个未用于训练的数据子集来检查模型性能是至关重要的。假设你使用了所有可用的数据来训练你的模型，没有为测试保留数据。在你训练了模型之后，你将如何了解它在预测之前从未见过的数据上的准确性？如果没有验证数据集来跟踪训练过程中的性能，以及测试数据集来评估训练完成时的性能，你将无法在部署之前了解你模型的表现。'
- en: '*Do an initial run to validate the function.* This initial run will ensure
    that your model code is functionally correct and doesn’t cause any immediate errors.
    The Keras deep learning model described in this book is simple. Even so, it took
    several iterations to get the model to complete its first run without errors and
    to clean up issues with how the final stage of the input data was defined and
    how the layers of the model were assembled. Getting your model to complete a small
    initial run, perhaps with only one epoch (one iteration through the training data),
    is a necessary step on the road to training the model.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*进行初始运行以验证函数*。这个初始运行将确保你的模型代码在功能上是正确的，不会导致任何立即的错误。本书中描述的Keras深度学习模型很简单。即便如此，它也经过了多次迭代，才能使模型无错误地完成第一次运行，并清理输入数据最终阶段定义和模型层组装的问题。使你的模型完成一个小型的初始运行，可能只包含一个epoch（一次训练数据的迭代），是训练模型道路上的一个必要步骤。'
- en: '*Iterate through training runs*. Check performance and make adjustments (to
    hyperparameters, including the number of *epochs* , or iterations through the
    training data) to get the optimal performance for your model. When you have completed
    an initial run to validate that your model code can run without errors, you need
    to make repeated experiments to observe your model’s behavior and see what you
    can adjust to improve performance. These iterative experiments (such as the experiments
    described in section 6.11) involve a certain amount of trial and error.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*迭代训练运行*。检查性能并进行调整（包括调整超参数，如*epoch*数量，即训练数据的迭代次数）以获得模型的最佳性能。当你完成了一个初始运行以验证你的模型代码可以无错误运行后，你需要进行重复实验来观察你的模型行为，并看看你可以调整什么来提高性能。这些迭代实验（例如第6.11节中描述的实验）涉及一定程度的试错。'
- en: When you start training a deep learning model, you may be tempted to start with
    long runs with many epochs (in the hope that the model will find its way to an
    optimal state if it’s allowed to run long enough) or to change multiple hyperparameters
    (such as learning rate or batch size) at the same time in the hope that you will
    find the golden combination of hyperparameter settings that will boost the performance
    of your model. I strongly recommend starting slowly. Start with a small number
    of epochs (iterations through the entire training set) and with a subset of the
    training set so that you can get a decent number of training runs done quickly.
    Note that in the case of the streetcar delay prediction model, the training set
    is not huge, so we can train with the whole training set right from the start
    and still complete a modest run of 20 epochs in less than five minutes in a standard
    Paperspace Gradient environment.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你开始训练一个深度学习模型时，你可能会倾向于从具有许多epochs的长运行开始（希望如果允许运行足够长的时间，模型将找到其最佳状态）或者同时更改多个超参数（如学习率或批量大小），希望你会找到将提高模型性能的黄金组合的超参数设置。我强烈建议你开始时要慢。从一个小的epochs数量（整个训练集的迭代）和一个训练集的子集开始，这样你可以快速完成相当数量的训练运行。请注意，在街车延误预测模型的情况下，训练集不是很大，因此我们可以从一开始就使用整个训练集进行训练，并且在标准Paperspace
    Gradient环境中在不到五分钟内完成20个epochs的适度运行。
- en: As the performance of your model improves, you can do longer runs with more
    epochs to observe whether additional iterations through the training data improve
    the performance of your model. I also strongly recommend that you adjust only
    one hyperparameter at a time.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着你的模型性能的提高，你可以进行更长时间的运行，并且有更多的epochs来观察额外的迭代是否提高了模型的性能。我也强烈建议你一次只调整一个超参数。
- en: '*Save the model from your optimal training run*. Section 6.9 describes how
    you can explicitly save trained models when you complete a training run. As you
    do longer training runs with more epochs, however, you will probably find that
    the model reaches its peak performance on an epoch that is not the last epoch.
    How can you ensure that you save the model on the epoch on which the model has
    optimal performance? Section 6.7 describes how to use the callback facility in
    Keras to save the model regularly during the training run and to stop training
    when the model’s performance is no longer improving. With this facility, you can
    do a long training run and be confident that at the end of the run, you will have
    saved the model with the best performance, even if that optimal performance was
    achieved in the middle of the run.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*保存最佳训练运行的模型*。第6.9节描述了如何在完成训练运行后显式保存训练好的模型。然而，当你进行更长时间的训练运行并且有更多的epoch时，你可能会发现模型在其不是最后一个epoch的epoch上达到了最佳性能。你如何确保在模型性能最佳的epoch上保存模型？第6.7节描述了如何使用Keras的回调功能在训练运行期间定期保存模型，并在模型性能不再提高时停止训练。有了这个功能，你可以进行长时间的训练运行，并且有信心在运行结束时，你会保存具有最佳性能的模型，即使这种最佳性能是在运行中途实现的。'
- en: '*Validate your trained model with the test set* *and score at least one new,
    single data point* *.* You want to get an early validation of the deployment process
    that you will create in chapter 8 by applying your trained model to a data point
    and examining the result. You can think of this early validation as being a dress
    rehearsal for your trained model. As a dress rehearsal is a way for the cast and
    crew of a performance to go through their paces before they need to face a live
    audience, applying a single data point to your trained model is a way to get a
    taste of how it will behave before you go through the effort of deployment.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用测试集验证你的训练模型*，并至少评分一个新数据点。你希望通过对一个数据点应用你的训练模型并检查结果来获得部署过程中你将在第8章创建的早期验证。你可以将这种早期验证视为你训练模型的彩排。正如彩排是表演的演员和工作人员在需要面对现场观众之前练习他们的动作的方式一样，将一个数据点应用于你的训练模型是在你进行部署之前了解它将如何表现的一种方式。'
- en: Section 6.8 describes how you can apply your model to the entire test set and
    also how you can get an early view of how your model will behave when it is deployed
    by exercising it on a new data point that is completely outside your original
    dataset. These two activities are related but have different goals. Applying your
    trained model to the test set gives you the best possible sense of its performance
    based on the dataset you have available because you are exercising the model with
    data that was not involved in the training process.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第6.8节描述了如何将你的模型应用于整个测试集，以及如何通过在新数据点上使用它来提前了解模型部署时的行为。这两个活动相关但目标不同。将训练好的模型应用于测试集，基于你拥有的数据集，给你提供了对其性能的最佳感觉，因为你在使用的数据并未参与训练过程。
- en: By contrast, *one-off scoring* *,* or scoring on a net new data point not taken
    from your original dataset, forces you to exercise the trained model as it will
    be exercised when it is deployed, but without doing all the work required to complete
    model deployment. One-off scoring gives you a quick glimpse of your model’s behavior
    when it is deployed and helps you anticipate and correct problems (such as a need
    for data that won’t be available when scoring happens) before you deploy the model.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相比之下，*一次性评分*，或者对来自原始数据集之外的新数据点的评分，迫使你像模型部署时那样使用训练好的模型，但无需完成模型部署所需的所有工作。一次性评分让你快速了解模型部署时的行为，帮助你预测并纠正问题（例如，评分时可能无法获得的数据），在你部署模型之前。
- en: These steps are largely common between deep learning and classic machine learning.
    The key difference is the number of hyperparameters that need to be tracked and
    maintained for a deep learning model. Looking at the list of hyperparameters from
    chapter 5 (figure 6.1), which ones are unique to training deep learning models?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤在深度学习和经典机器学习之间大体上是相同的。关键的区别是需要跟踪和维护的深度学习模型的超参数数量。查看第5章（图6.1）中的超参数列表，哪些是专门用于训练深度学习模型的？
- en: '![CH06_F01_Ryan](../Images/CH06_F01_Ryan.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F01_Ryan](../Images/CH06_F01_Ryan.png)'
- en: Figure 6.1 List of hyperparameters
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 超参数列表
- en: 'Of these hyperparameters, the following are specific to deep learning:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些超参数中，以下是与深度学习特定的：
- en: '`dropout_rate` —This parameter is related primarily to deep learning because
    it controls overfitting by turning off random subsets of the network. Other types
    of models employ dropout to control overfitting. XGBoost (Extreme Gradient Boosting;
    [http://mng.bz/8GnZ](http://mng.bz/8GnZ)) models can incorporate dropout (see
    parameters for Dart booster at [https://xgboost.readthedocs.io/en/latest/parameter
    .html](https://xgboost.readthedocs.io/en/latest/parameter.html)). But the dropout
    approach of controlling overfitting is more commonly applied to deep learning
    models.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout_rate` —这个参数主要与深度学习相关，因为它通过关闭网络中的随机子集来控制过拟合。其他类型的模型也使用dropout来控制过拟合。XGBoost（极端梯度提升；[http://mng.bz/8GnZ](http://mng.bz/8GnZ)）模型可以包含dropout（参见Dart增强器的参数[https://xgboost.readthedocs.io/en/latest/parameter.html](https://xgboost.readthedocs.io/en/latest/parameter.html)）。但控制过拟合的dropout方法更常应用于深度学习模型。'
- en: '`Output_activation` —This parameter is specific to deep learning because it
    controls the function that is applied in the final layer of a deep learning model.
    This parameter is less a tuning parameter (controlling the performance of the
    model) than a functional parameter because it controls the functional behavior
    of the model. You can set the output activation function to have your model produce
    a binary outcome (such as the streetcar delay prediction model, which predicts
    yes/no on whether a particular streetcar trip will be delayed), a prediction of
    one of a set of outcomes, or a continuous value.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Output_activation` —这个参数是专门针对深度学习的，因为它控制着应用于深度学习模型最终层的函数。这个参数与其说是调整参数（控制模型的性能）不如说是功能参数，因为它控制着模型的功能行为。你可以设置输出激活函数，使模型产生二元结果（例如，电车延迟预测模型，预测是否会有特定的电车行程延误），预测一组结果中的一个，或者是一个连续值。'
- en: 'The other parameters are common to deep learning and at least some classic
    machine learning approaches. Note that in the training process for this model
    I adjusted the hyperparameters manually. That is, I ran repeated experiments adjusting
    one hyperparameter at a time until I got adequate results. This learning process
    was great because I was able to observe closely the effects of changes such as
    adjusting the learning rate. But tuning hyperparameters manually is not practical
    for business-critical models. Here are some great resources that show you how
    to take a more methodical approach to hyperparameter tuning:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其他参数对深度学习和至少一些经典机器学习方法是通用的。请注意，在这个模型的训练过程中，我手动调整了超参数。也就是说，我一次调整一个超参数，重复进行实验，直到得到满意的结果。这个过程非常棒，因为我能够密切观察调整学习率等变化的影响。但是，手动调整超参数对于业务关键模型来说并不实用。以下是一些展示如何采取更系统方法进行超参数调整的优质资源：
- en: '*Real World Machine Learning by Henrik Brink, et al. (Manning Publications,
    2016)* includes a section that describes the basics of grid search ([http://mng.bz/X00Y](http://mng.bz/X00Y)),
    an approach that searches the possible combinations of values for each hyperparameter
    and assesses the model performance for each combination to find an optimal combination.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《现实世界机器学习》由 Henrik Brink 等人著（Manning Publications，2016）* 包含一个描述网格搜索（[http://mng.bz/X00Y](http://mng.bz/X00Y)）基础的章节，这是一种搜索每个超参数可能值组合的方法，并评估每个组合的模型性能以找到最佳组合。'
- en: The article at [http://mng.bz/yrrJ](http://mng.bz/yrrJ) recommends an end-to-end
    approach to hyperparameter tuning for Keras models.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://mng.bz/yrrJ](http://mng.bz/yrrJ) 上的文章推荐了一种针对 Keras 模型的端到端超参数调整方法。'
- en: 6.3 Reviewing the overall goal of the streetcar delay prediction model
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 审查电车延误预测模型的整体目标
- en: 'Before we jump into the steps involved in training a model, let’s review the
    overall purpose of the streetcar delay prediction model. We want to predict whether
    a given streetcar trip is going to encounter a delay. Note that for this model,
    we are not predicting the length of the delay, only whether there is going to
    be a delay. Here are the reasons for having the model predict a delay only instead
    of the length of the delay:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入到训练模型涉及的步骤之前，让我们回顾一下电车延误预测模型的整体目的。我们希望预测一次特定的电车旅行是否会遇到延误。请注意，对于这个模型，我们不是预测延误的长度，而是预测是否会延误。以下是只预测延误而不是延误长度的原因：
- en: Earlier experiments in getting a prediction of the length of the streetcar delay
    (that is, a *linear prediction* ) from the model were not successful. I suspect
    that the dataset is too small to give the model a decent chance of picking up
    a signal this specific.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 早期从模型中预测电车延误长度（即，*线性预测*）的实验并不成功。我怀疑数据集太小，无法给模型提供足够的机会来捕捉这种特定的信号。
- en: From a user’s perspective, the chance of a delay can be more significant than
    the duration of a delay. Any delay longer than five minutes makes it worthwhile
    to consider an alternative to a streetcar trip, such as walking, taking a taxi,
    or taking an alternative transit route. Thus, for simplicity, the binary prediction
    delay/no delay is more useful for the audience for this model than a linear prediction
    of the length of a delay.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从用户的角度来看，延误的可能性可能比延误的持续时间更为重要。任何超过五分钟的延误都值得考虑电车旅行的替代方案，例如步行、打车或选择其他交通路线。因此，为了简化，对于这个模型的受众来说，预测延误与否的二进制预测比预测延误长度更有用。
- en: 'Now that we have reviewed what the model will be predicting, let’s look at
    a specific example of the user’s experience. Assume that the user gets the model
    to predict whether the trip they want to take now, starting westbound on the Queen
    streetcar route, is going to be delayed. Let’s examine the possible outcomes:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了模型将要预测的内容，让我们看看一个具体的用户体验示例。假设用户让模型预测他们现在想要乘坐的旅行，从皇后街电车路线的西行开始，是否会延误。让我们分析可能的结果：
- en: '*The model predicts no delay, and no delay happens*. With this outcome, the
    model’s prediction matches what happens in real life. We call this result a *true
    negative* because the model predicts that the event (a delay on the westbound
    Queen streetcar) is not going to happen, and the event does not happen: there
    is no delay.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型预测无延误，并且确实没有延误发生*。在这种情况下，模型的预测与现实生活中发生的情况相符。我们称这种结果为 *真正负例*，因为模型预测该事件（西行皇后街电车的延误）不会发生，而事件确实没有发生：没有延误。'
- en: '*The model predicts no delay, but a delay happens.* With this outcome, the
    model’s prediction does not match what happens in real life. This result is called
    a *false negative* because the model predicts that the event (a delay on the westbound
    Queen streetcar) is not going to happen, but the event does happen: the trip is
    delayed.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*该模型预测没有延迟，但实际上发生了延迟。* 在这种结果下，模型的预测与现实生活中发生的情况不符。这种结果被称为*假阴性*，因为模型预测该事件（西行皇后街的电车延迟）不会发生，但事件实际上发生了：行程被延误。'
- en: '*The model predicts a delay, but no delay happens.* With this outcome, the
    model’s prediction does not match what happens in real life. This result is called
    a *false positive* because the model predicts that the event (a delay on the westbound
    Queen streetcar) is going to happen, but the event does not happen: there is no
    delay.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*该模型预测会有延迟，但实际上没有发生延迟。* 在这种结果下，模型的预测与现实生活中发生的情况不符。这种结果被称为*假阳性*，因为模型预测该事件（西行皇后街的电车延迟）将要发生，但事件实际上并没有发生：没有发生延迟。'
- en: '*The model predicts a delay, and a delay happens.* With this outcome, the model’s
    prediction matches what happens in real life. This result is called a *true positive*
    because the model predicts that the event (a delay on the westbound Queen streetcar)
    is going to happen, and the event does happen: the trip is delayed.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*该模型预测会有延迟，并且实际上发生了延迟。* 在这种结果下，模型的预测与现实生活中发生的情况相符。这种结果被称为*真正阳性*，因为模型预测该事件（西行皇后街的电车延迟）将要发生，并且事件确实发生了：行程被延误。'
- en: Figure 6.2 summarizes these four outcomes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2总结了这四种结果。
- en: '![CH06_F02_Ryan](../Images/CH06_F02_Ryan.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F02_Ryan](../Images/CH06_F02_Ryan.png)'
- en: Figure 6.2 Four possible outcomes for streetcar delay predictions
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 街车延迟预测的四种可能结果
- en: 'Of the four possible outcomes, we want to get the highest possible proportion
    of true negatives and true positives. But as we go through the iterative process
    of training the model, we see a trade-off between the number of false negatives
    and the number of false positives. Consider experiment 1, described in section
    6.10\. In this experiment, we do not make any adjustments in the model to account
    for the training dataset being imbalanced. Delays are rare. Only around 2% of
    all route/direction/time-slot combinations in the training dataset have delays.
    If we don’t account for this imbalance in the training dataset, the training process
    (which is optimized for accuracy in experiment 1) will generate weights for the
    model that result in the trained model’s predicting “no delay” all the time. Such
    a model will have what looks like great accuracy: more than 97%. But this model
    would be useless for our user because it never predicts a delay. Although delays
    may be rare, users of the model need to know when delays are likely to happen
    so that they can make alternative travel arrangements.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在四种可能的结果中，我们希望得到尽可能高的真正阴性和真正阳性的比例。但随着我们通过训练模型的迭代过程，我们看到假阴性和假阳性数量之间存在权衡。考虑第6.10节中描述的实验1。在这个实验中，我们没有对模型进行调整以考虑训练数据集的不平衡。延误很少见。训练数据集中所有路线/方向/时间段组合中只有大约2%有延误。如果我们不考虑到训练数据集的不平衡，训练过程（在实验1中是为了优化准确性）将为模型生成权重，导致训练好的模型总是预测“没有延误”。这样的模型看起来好像有很高的准确性：超过97%。但这个模型对我们用户来说将毫无用处，因为它永远不会预测有延误。尽管延误可能很少见，但模型的使用者需要知道何时可能发生延误，以便他们可以做出替代的旅行安排。
- en: 'Figure 6.3 shows the results of applying the model from experiment 1 to the
    test dataset: zero true positives and the maximum possible number of false negatives.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3显示了将实验1中的模型应用于测试数据集的结果：零个真正阳性和最大可能的假阴性数量。
- en: '![CH06_F03_Ryan](../Images/CH06_F03_Ryan.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F03_Ryan](../Images/CH06_F03_Ryan.png)'
- en: Figure 6.3 Outcomes of applying to test set to the trained model for experiment
    1
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 实验1中训练模型应用于测试集的结果
- en: As we go through the experiments in section 6.10, you will see that there can
    be a trade-off between false negatives and false positives. As we reduce the false
    negatives (the number of times the model predicts no delay when there is a delay),
    the false positives can increase (the number of times the model predicts a delay
    when there is no delay). Obviously, we want the model to have as low a proportion
    as possible of both false negatives and false positives, but if we have to make
    a trade-off between false negatives and false positives, what is the best result
    for our users? The worst outcome is that the model predicts no delay and then
    a delay occurs. In other words, the worst outcome for our users is a false negative.
    With a false positive, if a user takes the model’s advice and walks or takes a
    taxi to avoid a streetcar delay that never happens, they still have a decent chance
    of arriving at their destination on time. With a false negative, however, if the
    user follows the model’s advice and takes the streetcar, they miss the opportunity
    to take an alternative form of transit and risk arriving at their destination
    late.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进行第6.10节中的实验时，你会看到在误报和漏报之间可能存在权衡。当我们减少漏报（模型预测没有延误而实际上有延误的次数）时，误报可能会增加（模型预测有延误而实际上没有延误的次数）。显然，我们希望模型误报和漏报的比例尽可能低，但如果我们必须在误报和漏报之间做出权衡，对我们用户来说最好的结果是什么？最坏的结果是模型预测没有延误，然后实际上发生了延误。换句话说，对我们用户来说最坏的结果是漏报。如果有误报，如果用户遵循模型的建议步行或乘坐出租车避免从未发生的街车延误，他们仍然有相当的机会准时到达目的地。然而，如果有漏报，如果用户遵循模型的建议乘坐街车，他们就会错过乘坐替代交通方式的机会，并冒着迟到到达目的地的风险。
- en: 'As you will see in section 6.6, we direct the model training toward our overall
    goal by monitoring two measurements:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在第6.6节中将会看到的，我们通过监控两个指标来引导模型训练向我们的总体目标迈进：
- en: '*Recall* —true positive/(true positive + false negative)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*召回率* —真阳性/(真阳性 + 假阴性)'
- en: '*Validation accuracy* —The proportion of predictions that the model gets right
    on the validation dataset'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*验证准确率* —模型在验证数据集上预测正确的比例'
- en: Now that we have reviewed the overall goal of the training process for the streetcar
    delay prediction model and established which outcomes are most important for the
    user, let’s return to the steps for model training, starting with selecting the
    subsets of the dataset for training, validation, and testing.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经回顾了街车延误预测模型训练过程的总体目标，并确定了哪些结果对用户来说最为重要，那么让我们回到模型训练的步骤，从选择用于训练、验证和测试的数据库子集开始。
- en: 6.4 Selecting the train, validation, and test datasets
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 选择训练、验证和测试数据集
- en: 'The original dataset that we were working with had fewer than 100,000 records.
    With the refactoring we described in chapter 5, we now have a dataset with more
    than 2 million records. As described in section 6.2, we need to divide the dataset
    into the following subsets so we have records from the dataset that we can use
    to assess the performance of the model:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最初工作的原始数据库记录少于10万条。根据第5章中描述的重构，我们现在有一个包含超过200万条记录的数据库。如第6.2节所述，我们需要将数据库划分为以下子集，以便我们有可用于评估模型性能的数据库记录：
- en: '*Train* —Subset of the dataset that is used to train the model'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练* —用于训练模型的数据库子集'
- en: '*Validate* —Subset of the dataset that is used to track the performance of
    the model while it is being trained'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*验证* —用于在模型训练过程中跟踪模型性能的数据库子集'
- en: '*Test* —Subset of the dataset that is not used during the training itself.
    The trained model is applied to the test set as a final validation with data the
    model has never seen.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*测试* —在训练过程中未使用的数据库子集。训练好的模型应用于测试集，作为模型从未见过的数据的最终验证。'
- en: What proportion of the dataset should we allocate to each of these subsets?
    I chose a ratio of 60% of the dataset for training and 20% each for validation
    and testing. This proportion strikes a balance between having a big-enough training
    set to give the model a decent chance to extract a signal and get decent performance
    during training with the need to have big-enough validation and test sets to exercise
    the model on data it did not see during the training process. A ratio of 70/15/15
    would have been a reasonable choice as well. For a dataset that is less than millions
    of records, the proportion of validate and test should not drop below 10% each
    to ensure that there is a sufficient portion of the dataset to track performance
    during training iterations (the validation set), as well as a sufficient holdout
    (the test set) to apply to the trained model to ensure adequate performance on
    data it has never seen.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该将数据集的多少比例分配给这些子集？我选择了60%的数据集用于训练，每个验证和测试集各占20%。这个比例在拥有足够大的训练集以给模型一个很好的机会提取信号并在训练期间获得良好的性能，以及需要拥有足够大的验证和测试集以在训练过程中未看到的数据上测试模型之间取得了平衡。70/15/15的比例也是一个合理的选择。对于记录数少于百万的数据集，验证和测试的比例不应低于10%，以确保在训练迭代（验证集）期间有足够的数据集部分来跟踪性能，以及有足够的保留集（测试集）来应用于训练模型，以确保在它从未见过的数据上具有足够的性能。
- en: 6.5 Initial training run
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5初始训练运行
- en: Before making changes to optimize the training runs, we want to do an initial
    run to ensure that everything works functionally. In this initial run, we are
    not trying to get great accuracy or minimize false negatives. We will focus on
    the performance of the model in later runs. For this initial training run, we
    simply want to confirm that the code works functionally—executes from end to end
    without generating errors.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在对训练运行进行优化更改之前，我们想要先进行一次初始运行，以确保一切功能正常。在这个初始运行中，我们不是试图获得很高的准确率或最小化假阴性。我们将在后续运行中关注模型的性能。对于这次初始训练运行，我们只想确认代码在功能上是可行的——从头到尾执行而不产生错误。
- en: For your initial run of the model, you can follow along by using the streetcar_model
    _training notebook with the default settings in the streetcar model training config
    file to specify the hyperparameter values and other config settings for the run.
    The next listing shows the key values in the config file.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你模型的初始运行，你可以通过使用streetcar_model_training笔记本，并在streetcar模型训练配置文件中指定默认设置来指定运行的超参数值和其他配置设置来跟进。接下来的列表显示了配置文件中的关键值。
- en: Listing 6.2 Key parameters defined in the config file
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.2在配置文件中定义的关键参数
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ The proportion of the dataset reserved for the test dataset
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为测试数据集保留的数据集比例
- en: ❷ The experiment number determines a set of other parameters, including the
    number of epochs in the training run and whether early stopping is used.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 实验编号确定了一组其他参数，包括训练运行中的epoch数量以及是否使用提前停止。
- en: The following listing shows the settings for experiment 0, the initial training
    run.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表显示了实验0的设置，即初始训练运行。
- en: Listing 6.3 Settings for experiment 0
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.3实验0的设置
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Experiment 0 has no early stopping.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 实验0没有提前停止。
- en: ❷ Experiment 0 makes no allowance for the dataset’s being imbalanced.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 实验0没有考虑到数据集的不平衡性。
- en: '❸ Experiment 0 includes a single iteration through the training dataset: 1
    epoch.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 实验0包含对训练数据集的单次迭代：1个epoch。
- en: The next listing is the code block that triggers the training of the model.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表是触发模型训练的代码块。
- en: Listing 6.4 Code that triggers the training of the model
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.4触发模型训练的代码
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Call the function that builds the model, as described in chapter 5.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 调用第5章中描述的构建模型的功能。
- en: ❷ Because early_stop is set to False in experiment 0, this fit statement gets
    invoked.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 因为在实验0中early_stop被设置为False，所以这个拟合语句被调用。
- en: Let’s take a closer look at the statement that fits the model, shown in figure
    6.4.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看图6.4中显示的拟合模型语句。
- en: '![CH06_F04_Ryan](../Images/CH06_F04_Ryan.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F04_Ryan](../Images/CH06_F04_Ryan.png)'
- en: Figure 6.4 Key elements of the fit statement
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4拟合语句的关键元素
- en: The training set and labels (`target`) for training are identified.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集和标签（`target`）也被确定了。
- en: The validation set and label (`target`) for validation are also identified.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证集和验证标签（`target`）也被确定了。
- en: Because the dataset is skewed (there are many more route/direction/time slots
    with no delays than there are slots with delays), we use the Keras facility to
    apply weights to the output classes to account for this imbalance. Note that the
    use of `weight` here is purely to compensate for the imbalance in the input dataset.
    It is not related to the weights that are set in the training process as described
    in chapter 1.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于数据集存在偏差（没有延迟的路线/方向/时间槽位比有延迟的槽位多得多），我们使用Keras功能对输出类别应用权重，以解决这种不平衡。请注意，这里的`weight`的使用纯粹是为了补偿输入数据集的不平衡。它与第1章中描述的训练过程中设置的权重无关。
- en: As the `fit` command executes, you will see output like that shown in figure
    6.5.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 随着`fit`命令的执行，你将看到如图6.5所示的输出。
- en: '![CH06_F05_Ryan](../Images/CH06_F05_Ryan.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F05_Ryan](../Images/CH06_F05_Ryan.png)'
- en: Figure 6.5 Output of the fit command
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 fit命令的输出
- en: 'The output begins with a recap of the number of samples available to train
    and validate the model. Following is a line per epoch (iteration through the training
    dataset) that shows a variety of measurements. We will focus on these measurements:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 输出首先回顾了可用于训练和验证模型的样本数量。接下来是每个epoch（遍历训练数据集）的行，显示了各种测量值。我们将关注这些测量值：
- en: '*Loss* —Aggregate delta between the predictions and the actual target values
    for the training set'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损失* —预测值与训练集实际目标值之间的总delta'
- en: '*Accuracy* (*acc*)—Proportion of predictions in this epoch that match the actual
    target values for the training set'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*准确率* (*acc*) —在本轮预测中匹配训练集实际目标值的预测比例'
- en: '*Validation loss* (*val_loss* )—Aggregate delta between the predictions and
    the actual target values for the validation set'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*验证损失* (*val_loss*) —预测值与验证集实际目标值之间的总delta'
- en: '*Validation accuracy* (*val_accuracy*)—Proportion of predictions in this epoch
    that match the actual target values for the validation set'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*验证准确率* (*val_accuracy*) —在本轮预测中匹配验证集实际目标值的预测比例'
- en: 'When the `fit` command has completed, you have a trained model. Values have
    been assigned to the trainable parameters, and you have a model you can use to
    score new values:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当`fit`命令完成后，你将拥有一个训练好的模型。可训练参数已分配值，并且你可以使用该模型对新值进行评分：
- en: '[PRE4]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 6.6 Measuring the performance of your model
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 测量你的模型性能
- en: The output produced as the training run executes gives you an initial picture
    of the performance of your model. When the training run is complete, you have
    two easy ways to examine the performance of the model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 训练运行执行时产生的输出为你提供了模型性能的初步了解。当训练运行完成后，你有两种简单的方法来检查模型的性能。
- en: One way to examine the performance of the model is to plot training and validation
    loss and accuracy. The chart in figure 6.6 shows training and validation accuracy
    for a 30-epoch training run.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 检查模型性能的一种方法是将训练和验证损失以及准确率进行绘图。图6.6显示了30个epoch训练运行的训练和验证准确率。
- en: '![CH06_F06_Ryan](../Images/CH06_F06_Ryan.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F06_Ryan](../Images/CH06_F06_Ryan.png)'
- en: Figure 6.6 Training and validation accuracy graph
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 训练和验证准确率图
- en: The next listing shows the code that generates accuracy and loss graphs.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表显示了生成准确率和损失图的代码。
- en: Listing 6.5 Code to generate the accuracy and loss graphs
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.5 生成准确率和损失图的代码
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Specify the title of the chart.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指定图表的标题。
- en: ❷ Specify that the chart is tracking accuracy and val_accuracy
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 指定图表正在跟踪准确率和val_accuracy
- en: ❸ Specify the y-axis label.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 指定y轴标签。
- en: ❹ Specify the x-axis label.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 指定x轴标签。
- en: ❺ Specify the labels in the legend.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 指定图例中的标签。
- en: ❻ Display the chart.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 显示图表。
- en: Recall the table from section 6.3 that showed the four possible outcomes of
    a prediction, as shown in figure 6.7.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下6.3节中的表格，它显示了预测的四种可能结果，如图6.7所示。
- en: '![CH06_F07_Ryan](../Images/CH06_F07_Ryan.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F07_Ryan](../Images/CH06_F07_Ryan.png)'
- en: Figure 6.7 The four possible outcomes of the streetcar delay prediction model
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 街车延迟预测模型的四种可能结果
- en: The other way to examine the performance of the model is the confusion matrix,
    shown in figure 6.8\. The `sklearn.metrics` library includes a feature that lets
    you generate a table like this one, showing the count of true negative, false
    positive, false negative, and true positive outcomes for a run of your trained
    model.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 检查模型性能的另一种方法是混淆矩阵，如图6.8所示。`sklearn.metrics`库包括一个功能，允许你生成这样的表格，显示训练模型运行的真实负例、假正例、假负例和真正例的计数。
- en: '![CH06_F08_Ryan](../Images/CH06_F08_Ryan.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F08_Ryan](../Images/CH06_F08_Ryan.png)'
- en: Figure 6.8 Confusion matrix
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 混淆矩阵
- en: 'The confusion matrix has four quadrants:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵有四个象限：
- en: '*Top left* —There was no delay for this route/direction/time slot, and the
    model predicted no delay.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*左上角* —对于这条路线/方向/时间段，没有延迟，模型也预测没有延迟。'
- en: '*Bottom left* —There was a delay for this route/direction/time slot, but the
    model predicted no delay.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*左下角* —对于这条路线/方向/时间段，存在延迟，但模型预测没有延迟。'
- en: '*Top right* —There was no delay for this route/direction/time slot, but the
    model predicted a delay.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*右上角* —对于这条路线/方向/时间段，没有延迟，但模型预测了延迟。'
- en: '*Bottom right* —There was a delay for this route/direction/time slot, and the
    model predicted a delay.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*右下角* —对于这条路线/方向/时间段，存在延迟，并且模型预测了延迟。'
- en: 'The confusion matrix takes a bit of interpreting:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵需要一些解释：
- en: The outcome counts in each quadrant are in scientific notation, so there are
    370,000 (= 3.7E+05) true negatives.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个象限中的结果计数以科学记数法表示，因此有370,000（= 3.7E+05）个真实负例。
- en: The shades indicate the absolute number of outcomes in each quadrant. The lighter
    the shade, the bigger the number; the darker the shade, the smaller the number.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阴影表示每个象限中的结果绝对数量。阴影越浅，数量越大；阴影越深，数量越小。
- en: The quadrants are labeled as predictions (`0` /`1`) along the x axis to indicate
    that the model did not predict a delay (`0`) and the model predicted a delay (`1`).
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 四个象限沿x轴标记为预测（`0` /`1`），以表明模型没有预测延迟（`0`）和模型预测了延迟（`1`）。
- en: 'The y axis indicates what actually happened: no delay (`0`) and delay (`1`).'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: y轴表示实际发生的情况：没有延迟（`0`）和延迟（`1`）。
- en: Despite its drawbacks in terms of visual appeal, the confusion matrix is a useful
    tool for comparing the results of training runs because it packs a lot of information
    into one easy-to-generate package.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在视觉吸引力方面存在不足，混淆矩阵仍然是一个有用的工具，因为它可以将大量信息压缩到一个易于生成的包中。
- en: The following listing shows the code that generates the confusion matrix.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表显示了生成混淆矩阵的代码。
- en: Listing 6.6 Code to generate the confusion matrix
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.6 生成混淆矩阵的代码
- en: '[PRE6]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Specify the actual (y_true) and prediction (y_pred) outcomes.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指定实际（y_true）和预测（y_pred）结果。
- en: ❷ Show the graph.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 显示图表。
- en: 'We will use two measurements in particular to study the performance of the
    streetcar delay prediction model in several training iterations: validation accuracy
    and recall. *Validation accuracy* (the proportion of correct predictions on the
    validation dataset) will give us an indication of the overall accuracy of the
    model on new data, but it won’t tell the whole story. As mentioned in section
    6.3, we want to minimize the number of false negatives (the model predicts no
    delay when there is a delay). That is, we don’t want the model to miss predicting
    delays when a delay is going to happen. To monitor for this outcome, we can track
    recall:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用两个特定的度量来研究在多个训练迭代中街道车延迟预测模型的表现：验证准确率和回收率。*验证准确率*（在验证数据集上正确预测的比例）将给我们一个关于模型在新数据上整体准确性的指示，但它不会讲述整个故事。如6.3节所述，我们希望最小化错误负例的数量（模型在存在延迟时预测没有延迟）。也就是说，我们不希望模型在将发生延迟时错过预测延迟。为了监控这种结果，我们可以跟踪回收率：
- en: '[PRE7]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let’s consider what recall means in terms of the table of outcomes we introduced
    in section 6.3, using the labels shown in figure 6.9.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一下，在6.3节中介绍的输出表的情况下，如何用图6.9中显示的标签来解释回收率。
- en: '![CH06_F09_Ryan](../Images/CH06_F09_Ryan.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F09_Ryan](../Images/CH06_F09_Ryan.png)'
- en: Figure 6.9 Recall = A / (A + B)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 回收率 = A / (A + B)
- en: 'Recall is important because it allows us to track the key outcome that we want
    to avoid: predicting no delay when a delay occurs. By monitoring validation accuracy
    and recall together, we get a balanced picture of the performance of the model.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 回收率很重要，因为它允许我们跟踪我们想要避免的关键结果：当发生延迟时预测没有延迟。通过同时监控验证准确率和回收率，我们可以得到模型性能的平衡图景。
- en: '6.7 Keras callbacks: Getting the best out of your training runs'
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.7 Keras回调：充分利用训练运行
- en: By default, when you call the `fit` statement to kick off a training run for
    a Keras model, the training run keeps going for the number of epochs you specify
    in the `fit` statement, and you will be able to save the weights (a trained model)
    only after the last epoch has run. With its default behavior, you can think of
    a Keras training run as being a factory that produces pies on a conveyor belt.
    With each epoch, a pie (trained model) is baked. Our goal is to get the biggest
    pie (the optimally trained model), and if the factory has stopped baking bigger
    pies, we want to shut down the factory rather than waste ingredients making small
    pies. That is, we don’t want to run a bunch of epochs if the model is not improving.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，当你调用`fit`语句启动Keras模型的训练运行时，训练运行会持续进行，直到`fit`语句中指定的epoch数。你只能在最后一个epoch运行完毕后保存权重（一个训练好的模型）。以其默认行为，你可以将Keras训练运行想象成一个工厂，在传送带上生产派。随着每个epoch的进行，一个派（训练好的模型）被烘焙。我们的目标是得到最大的派（最优训练模型），如果工厂停止烘焙更大的派，我们希望关闭工厂，而不是浪费原料制作小派。也就是说，如果模型没有改进，我们不想运行很多epoch。
- en: The pie factory bakes a series of pies of different sizes (representing models
    with different performance characteristics), as shown in figure 6.10.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 派工厂烘焙了一系列不同大小的派（代表具有不同性能特性的模型），如图6.10所示。
- en: '![CH06_F10_Ryan](../Images/CH06_F10_Ryan.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F10_Ryan](../Images/CH06_F10_Ryan.png)'
- en: Figure 6.10 A default Keras training run
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 默认的Keras训练运行
- en: The problem is that with the default Keras training run behavior, the pie factory
    keeps baking pies even if it starts to bake smaller pies (the model performance
    is no longer improving), and the pie that gets saved (the trained model you have
    available to save at the end of the training run) is the last one, even if it
    isn’t the biggest pie. The result is that the pie factory can end up being wasteful
    by baking a lot of pies that aren’t getting bigger, and the pie that you get at
    the end can be a small one, as shown in figure 6.11.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，默认的Keras训练运行行为，派工厂即使在开始烘焙更小的派（模型性能不再改进）时也会继续烘焙派，而保存的派（在训练运行结束时可以保存的训练模型）是最后一个，即使它不是最大的派。结果是，派工厂可能会浪费很多没有变大的派，而最后得到的派可能是一个小的派，如图6.11所示。
- en: '![CH06_F11_Ryan](../Images/CH06_F11_Ryan.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F11_Ryan](../Images/CH06_F11_Ryan.png)'
- en: Figure 6.11 After a default training run, you can save only the final model,
    even if it’s not the best model.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 在默认训练运行之后，你只能保存最终模型，即使它不是最好的模型。
- en: 'Fortunately, Keras provides callback facilities that give you the opportunity
    to do much more effective training. In the terms of the pie factory, callbacks
    let you do two things:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Keras提供了回调功能，让你有机会进行更有效的训练。在派工厂的术语中，回调让你做两件事：
- en: Save the biggest pie (the optimal trained model), even if it’s not the last
    pie, as shown in figure 6.12.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使不是最后一个派，也要保存最大的派（最优训练模型），如图6.12所示。
- en: '![CH06_F12_Ryan](../Images/CH06_F12_Ryan.png)'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH06_F12_Ryan](../Images/CH06_F12_Ryan.png)'
- en: Figure 6.12 Save the biggest pie.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.12 保存最大的派。
- en: Stop the pie factory automatically if it is no longer baking bigger pies (generating
    models with better performance) so you are not wasting resources baking small
    pies, as shown in figure 6.13.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果派工厂不再烘焙更大的派（生成性能更好的模型），则自动停止派工厂，这样你就不会浪费资源烘焙小派，如图6.13所示。
- en: '![CH06_F13_Ryan](../Images/CH06_F13_Ryan.png)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![CH06_F13_Ryan](../Images/CH06_F13_Ryan.png)'
- en: Figure 6.13 Stop the pie factory early if it stops baking bigger pies.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.13 如果派工厂停止烘焙更大的派，则提前关闭派工厂。
- en: Let’s examine how Keras callbacks work for real. Keras callbacks give you the
    power to control how long your training run goes and stop the training run when
    it is no longer improving. Keras callbacks also allow you to save models to a
    file after every epoch in which a given criterion (such as validation accuracy)
    hits a local maximum. By combining these facilities (early stopping and saving
    the model that gets the optimal measurement for your key criteria), you can meet
    the two goals of controlling how long your training runs goes and stopping the
    training run when it stops improving.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实际考察一下Keras回调是如何工作的。Keras回调让你能够控制训练运行的时间长度，并在不再改进时停止训练运行。Keras回调还允许你在满足给定标准（如验证准确度）达到局部最大值后的每个epoch中保存模型。通过结合这些功能（提前停止和保存获得最优关键指标测量的模型），你可以实现两个目标：控制训练运行的时间长度，并在停止改进时停止训练运行。
- en: To use early stopping, we first have to define a callback ([https://keras.io/callbacks](https://keras.io/callbacks)).
    A *callback* consists of a set of functions that can be applied during the training
    run to give you insight into the training process. Callbacks allow us to interact
    with the training process while it is in motion. We can use callbacks to monitor
    a performance measure epoch by epoch during a training run and take actions based
    on what happens with this performance measure. We can track validation accuracy,
    for example, and allow the run to continue as long as validation accuracy continues
    to increase. When validation accuracy drops, we can get the callback to stop the
    training run. We can use the `patience` option to delay stopping the training
    run so that it continues for a given number of epochs when validation accuracy
    is no longer increasing. This option allows us to avoid missing increases in validation
    accuracy that occur after a temporary drop or plateau.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用早期停止，我们首先必须定义一个回调（[https://keras.io/callbacks](https://keras.io/callbacks)）。一个*回调*由一组可以在训练运行期间应用的功能组成，以提供对训练过程的洞察。回调允许我们在训练过程中与之交互。我们可以使用回调在训练运行中逐个epoch监控性能指标，并根据该性能指标发生的情况采取行动。例如，我们可以跟踪验证准确率，并允许运行在验证准确率继续增加的情况下继续进行。当验证准确率下降时，我们可以让回调停止训练运行。我们可以使用`patience`选项来延迟停止训练运行，以便在验证准确率不再增加的情况下继续进行给定数量的epochs。此选项允许我们避免错过在暂时下降或平台期之后发生的验证准确率增加。
- en: The control we get from an early-stopping callback is a big improvement over
    letting the training run go for the full set of epochs. But what happens if the
    best result for the performance measure we care about occurs for an epoch other
    than the last epoch? If we simply save the final model, we could be missing better
    performance that occurred in an interim epoch. We can address this situation by
    using another callback that allows us to keep saving through the training run
    the model that has the best result for the performance measure we are tracking.
    Using this callback in concert with the early-stopping callback, we know that
    the last model saved during the training run will have the best performance measure
    result of all the epochs in the run.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 从早期停止回调获得的控制权比让训练运行完整地运行所有epochs有大幅改进。但是，如果我们关心的性能指标的最佳结果发生在最后一个epoch之外的其他epoch，会发生什么？如果我们简单地保存最终模型，我们可能会错过在中间epoch发生的更好的性能。我们可以通过使用另一个回调来解决这种情况，该回调允许我们在训练运行期间持续保存具有我们跟踪的性能指标最佳结果的模型。使用此回调与早期停止回调一起，我们知道在训练运行期间最后保存的模型将具有所有epochs中最佳的性能指标结果。
- en: Figure 6.14 shows the snippets of code in the streetcar_model_training notebook
    where the callbacks are defined.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14显示了在streetcar_model_training笔记本中定义回调的代码片段。
- en: '![CH06_F14_Ryan](../Images/CH06_F14_Ryan.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F14_Ryan](../Images/CH06_F14_Ryan.png)'
- en: Figure 6.14 Defining callbacks to track a performance measure and save the model
    with the best performance
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 定义回调以跟踪性能指标并保存具有最佳性能的模型
- en: Now let’s look at an example to see the effect of callbacks on a training run.
    We are going to look at a 20-epoch run, first with no callbacks in place and then
    when callbacks have been applied. The next listing shows the parameters we need
    to set before doing this run.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过一个例子来看看回调对训练运行的影响。我们将查看一个20-epoch的运行，首先是没有回调的情况，然后是应用了回调的情况。接下来的列表显示了在进行此运行之前需要设置的参数。
- en: Listing 6.7 Parameters to set to control early stopping and dataset balancing
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.7 设置的参数以控制早期停止和数据集平衡
- en: '[PRE8]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Specify no callbacks.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指定不使用回调。
- en: ❷ Take into account the imbalance in the dataset between records with delays
    and records with no delays.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 考虑数据集中有延迟和无延迟的记录之间的不平衡性。
- en: To start, figure 6.15 is the accuracy graph for this 20-epoch run without an
    early-stopping callback in place.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，图6.15是此20-epoch运行没有早期停止回调的准确率图。
- en: '![CH06_F15_Ryan](../Images/CH06_F15_Ryan.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F15_Ryan](../Images/CH06_F15_Ryan.png)'
- en: Figure 6.15 Accuracy graph for a run of 20 epochs with no callbacks
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 无回调的20-epoch运行的准确率图
- en: 'After this 20-epoch run, the final `val_accuracy` has a value of `0.73` :'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个20-epoch的运行之后，最终的`val_accuracy`值为`0.73`：
- en: '[PRE9]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Figure 6.16 shows that the `val_accuracy` produced by the final epoch is not
    the maximum for the training run.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16显示，最终epoch产生的`val_accuracy`并不是训练运行中的最大值。
- en: '![CH06_F16_Ryan](../Images/CH06_F16_Ryan.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F16_Ryan](../Images/CH06_F16_Ryan.png)'
- en: Figure 6.16 Difference between the terminal val_accuracy and the maximum val_accuracy
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 终端val_accuracy与最大val_accuracy之间的差异
- en: Let’s see what happens when we add early stopping to this run, using the code
    shown in the next listing to stop the run when validation accuracy stops increasing.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当我们添加早期停止到这个运行时会发生什么，使用下一列表中的代码在验证准确度停止增加时停止运行。
- en: Listing 6.8 Code to set callbacks
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.8 设置回调的代码
- en: '[PRE10]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Function to define Keras callbacks. Parameters are es_monitor (the measurement
    to track in the mc callback) and es_mode (the extreme min or max to track in the
    es early-stopping callback).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义Keras回调的函数。参数是es_monitor（在mc回调中跟踪的测量）和es_mode（在es早期停止回调中跟踪的极端最小值或最大值）。
- en: ❷ List to contain all the callbacks defined in this function
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 包含在此函数中定义的所有回调的列表
- en: ❸ Define es callback for early stopping based on es_monitor measurement no longer
    moving in the direction indicated by es_mode, and add it to the list of callbacks
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义基于es_monitor测量不再按es_mode指示的方向移动的es回调，并将其添加到回调列表中
- en: ❹ Define path to save the model during the training process when the measurement
    es_monitor reaches a new optimal value
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在训练过程中，当测量es_monitor达到新的最优值时，定义保存模型的路径
- en: ❺ Define mc callback for saving the best mode based on es_monitor measurement
    getting to the best value defined by es_mode
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 定义基于es_monitor测量达到由es_mode定义的最佳值的最佳模式保存的mc回调
- en: ❻ If required, define TensorBoard callback. See chapter 5 for details on defining
    the TensorBoard callback.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 如有必要，定义TensorBoard回调。有关定义TensorBoard回调的详细信息，请参阅第5章。
- en: ❼ If early_stop is set to true, the fit command is called with the callbacks
    parameter set to the list of callbacks returned by the set_early_stop() function.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 如果early_stop设置为true，fit命令将使用回调参数调用set_early_stop()函数返回的回调列表。
- en: To get an early-stopping callback, we set the parameters in the next listing.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取早期停止回调，我们设置下一列表中的参数。
- en: Listing 6.9 Parameters to set for an early-stopping callback
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.9 设置早期停止回调的参数
- en: '[PRE11]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Specify that callbacks get included in the invocation of the fit statement.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指定回调包含在fit语句的调用中。
- en: ❷ Specify that val_accuracy is the measurement for controlling the callbacks.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 指定val_accuracy是控制回调的测量
- en: ❸ Specify that the max of val_accuracy is tracked by the callbacks.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 指定val_accuracy的最大值被回调跟踪
- en: With these parameters set, we rerun the experiment, and this time, the callbacks
    are invoked during the training process. You can see the result in figure 6.17.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 设置了这些参数后，我们重新运行实验，这次回调在训练过程中被调用。您可以在图6.17中看到结果。
- en: '![CH06_F17_Ryan](../Images/CH06_F17_Ryan.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F17_Ryan](../Images/CH06_F17_Ryan.png)'
- en: Figure 6.17 Accuracy with an early-stopping callback in place
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17 带有早期停止回调的准确度
- en: 'Instead of running for the full 20 epochs, the training run stops after 2 epochs
    because the validation accuracy has dropped. This result isn’t exactly what we
    want; we would like to give the model a chance to get better, and we certainly
    don’t want to stop training as soon as accuracy has dropped. To get a better result,
    we can set the `patience_threshold` parameter to a value other than the default
    `0` in the model training config file:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 由于验证准确度下降，训练运行在2个epoch后停止，而不是完整的20个epoch。这个结果并不是我们想要的；我们希望模型有改进的机会，我们当然不希望准确度一下降就停止训练。为了得到更好的结果，我们可以在模型训练配置文件中将`patience_threshold`参数设置为除默认`0`以外的值：
- en: '[PRE12]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: What happens if we rerun the same training exercise with the `patience_threshold`
    parameter added to the early-stopping callback? The run stops after 12 epochs
    instead of 2 and takes three minutes to complete. The terminal validation accuracy
    is 0.73164, as shown in figure 6.18.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在早期停止回调中添加`patience_threshold`参数重新运行相同的训练练习会发生什么？运行在12个epoch后停止，而不是2个，并且需要三分钟来完成。终端验证准确度为0.73164，如图6.18所示。
- en: '![CH06_F18_Ryan](../Images/CH06_F18_Ryan.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F18_Ryan](../Images/CH06_F18_Ryan.png)'
- en: Figure 6.18 Accuracy with a patient early stop
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18 带有患者早期停止的准确度
- en: The results of these changes (adding callbacks and setting a nonzero value for
    the `patience_threshold` parameter) are better validation accuracy with fewer
    epochs and less time required to complete the training. Figure 6.19 summarizes
    the results for this set of experiments.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这些更改（添加回调和将`patience_threshold`参数设置为非零值）的结果是更好的验证准确度，需要更少的epoch和更少的时间来完成训练。图6.19总结了这组实验的结果。
- en: '![CH06_F19_Ryan](../Images/CH06_F19_Ryan.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F19_Ryan](../Images/CH06_F19_Ryan.png)'
- en: Figure 6.19 Summary of callback experiments
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19 回调实验总结
- en: These experiments show that the callback facility in Keras is a powerful way
    of making your training runs more efficient by avoiding repeated epochs that aren’t
    providing benefit for your key measurements. The callback facility also allows
    you to save the model iteration that gives you the optimal value for your key
    measurement, regardless of where this iteration occurs in your training run. By
    adjusting the patience parameter, you can balance the cost of running more epochs
    with the potential benefit of getting better performance after a temporary dip.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实验表明，Keras中的回调功能是一种通过避免对关键测量没有带来益处的重复epoch来提高训练运行效率的强大方式。回调功能还允许你保存模型迭代，该迭代为你提供了关键测量的最优值，无论这个迭代发生在训练运行的哪个位置。通过调整耐心参数，你可以平衡运行更多epoch的成本与在暂时下降后获得更好性能的潜在益处。
- en: 6.8 Getting identical results from multiple training runs
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.8 从多次训练运行中获得相同的结果
- en: You may be asking how it was possible for the experiments shown in section 6.7
    to get consistent results between training runs. By default, aspects of the training
    process, including the initial weights assigned to nodes in the network, are set
    randomly. You will see this effect if you run repeated experiments with the deep
    learning model with identical inputs and parameter settings; you will get different
    results (such as validation accuracy per epoch) each time, even if the inputs
    are identical. If there is an intentional element of randomness in training a
    deep learning model, how can we control this element so that we can do repeated,
    controlled experiments to assess the effect of a specific change, such as the
    callbacks introduced in the example in section 6.7? The key is setting a fixed
    seed for the random number generator. The random number generator provides the
    random inputs for the training process (such as initial weights for the model)
    that result in different results between training runs. When you want to have
    identical results from multiple training runs, you can explicitly set the seed
    for the random number generator.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道第6.7节中展示的实验是如何在训练运行之间获得一致结果的。默认情况下，训练过程的各个方面，包括分配给网络节点的初始权重，都是随机设置的。如果你使用具有相同输入和参数设置的深度学习模型运行重复实验，你会看到这种效果；即使输入相同，每次运行也会得到不同的结果（例如每个epoch的验证准确率）。如果在训练深度学习模型中存在有意引入的随机元素，我们如何控制这个元素，以便我们可以进行重复的、受控的实验来评估特定变化的影响，例如第6.7节示例中引入的回调？关键是设置随机数生成器的固定种子。随机数生成器为训练过程提供随机输入（例如模型的初始权重），导致训练运行之间结果不同。当你想要从多次训练运行中获得相同的结果时，你可以显式设置随机数生成器的种子。
- en: If you look at the config file for the model training notebook, you see a parameter
    called `repeatable_run` in the `test_parms` section (see the next listing).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看模型训练笔记本的配置文件，你会在`test_parms`部分看到一个名为`repeatable_run`的参数（参见下一列表）。
- en: Listing 6.10 Parameters to control test execution
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.10 控制测试执行的参数
- en: '[PRE13]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Parameter to control whether we want a repeatable experiment. Do we want to
    seed the random number generator with a fixed value to get consistent results
    for multiple runs?
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 控制我们是否想要重复实验的参数。我们是否希望用固定值对随机数生成器进行种子设置，以获得多次运行的一致结果？
- en: 'The model training notebook uses the `repeatable_run` parameter to determine
    whether to explicitly set the seed for the random number generator and, thus,
    generate identical results from multiple training runs:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练笔记本使用`repeatable_run`参数来确定是否显式设置随机数生成器的种子，从而从多次训练运行中生成相同的结果：
- en: '[PRE14]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this section, we’ve summarized how you can get the same results from multiple
    training runs. For more details on getting repeatable results with Keras, check
    out the great article at [http://mng.bz/Moo2](http://mng.bz/Moo2).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们总结了如何从多次训练运行中获得相同的结果。有关使用Keras获得可重复结果的更多详细信息，请参阅优秀的文章[http://mng.bz/Moo2](http://mng.bz/Moo2)。
- en: 6.9 Shortcuts to scoring
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.9 得分捷径
- en: 'When we have a trained model, we want to be able to exercise it. Let’s quickly
    review the high-level steps:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有一个训练好的模型时，我们希望能够对其进行练习。让我们快速回顾一下高级步骤：
- en: '*Train the model* *.* Use the process described in this chapter, in which the
    weights in the model are iteratively set by the model repeatedly going through
    the training data set with the goal of minimizing the loss function. In our case,
    the loss function measures the delta between the predictions that the model makes
    for delay/no delay and the actual delay/no delay outcome that occurred for each
    route/direction/time-slot combination in the training dataset.'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*训练模型*。使用本章中描述的过程，模型通过反复遍历训练数据集，迭代地设置模型中的权重，以最小化损失函数。在我们的案例中，损失函数衡量模型对延迟/不延迟的预测与训练数据集中每个路线/方向/时间槽组合的实际延迟/不延迟结果之间的差异。'
- en: '*Score with the model (one-off scoring* *).* Get the trained model’s predictions
    for new data points: delay/no delay for route/direction/time-slot combinations
    that the model never saw during the training process. These new datapoints can
    be from the test subset of the original dataset or net new datapoints.'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*使用模型评分（一次性评分）*。获取训练好的模型对新数据点的预测：对于模型在训练过程中从未见过的路线/方向/时间槽组合，预测是延迟/不延迟。这些新数据点可以来自原始数据集的测试子集或全新的数据点。'
- en: '*Deploy the model.* Make the trained model available to provide efficient predictions
    on net new datapoints. In addition to describing deployment, chapter 8 describes
    the difference between deployment and one-off scoring.'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*部署模型*。使训练好的模型可用于对新数据点提供高效的预测。除了描述部署之外，第8章还描述了部署和一次性评分之间的区别。'
- en: As you will see in chapter 8, it can take several steps to get the model deployed.
    Before going through the full deployment process, you want to be able to do some
    scoring with the model to validate its performance on data that the model did
    not see during the training process. This section describes shortcuts to scoring
    that you can take advantage of before full-blown model deployment.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在第8章中将会看到的，部署模型可能需要几个步骤。在完成完整的部署过程之前，你想要能够使用模型进行一些评分，以验证模型在训练过程中未见过的数据上的性能。本节描述了在全面模型部署之前可以采用的评分捷径。
- en: 'To exercise the model on the entire test set, you can call `predict` on the
    model with the test set as the input:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 要在完整的测试集上测试模型，你可以用测试集作为输入调用模型的 `predict` 方法：
- en: '[PRE15]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'What if you want to exercise a single new test example? This is the typical
    use case of the deployed model: a client using the model to determine whether
    a streetcar trip they want to take is predicted to be delayed.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想测试一个单一的新测试示例呢？这是部署模型的典型用例：一个客户端使用模型来确定他们想要乘坐的电车行程是否会被预测为延迟。
- en: To score a single datapoint, we first need to examine the structure of the input
    to the model. What is the structure of `X_test` ?
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 要对一个单一数据点进行评分，我们首先需要检查模型输入的结构。`X_test` 的结构是什么？
- en: '[PRE16]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`X_test` is a dictionary in which each value is a numpy array. If we want to
    score a single new data point, we can create a dictionary that contains a single
    entry numpy array for each key in the dictionary:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '`X_test` 是一个字典，其中每个值都是一个 numpy 数组。如果我们想对一个单一的新数据点进行评分，我们可以创建一个字典，其中包含一个字典中每个键的单个条目
    numpy 数组：'
- en: '[PRE17]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now that we have defined a single data point, we can use the trained model
    to get a prediction for this data point:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了一个单一的数据点，我们可以使用训练好的模型来获取这个数据点的预测：
- en: '[PRE18]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'For one of the models we’ve trained, we get the following output:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们训练的其中一个模型，我们得到以下输出：
- en: '[PRE19]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: So for this single data point, the model does not predict a delay. This ability
    to score a single data point is a good way to validate the model quickly, particularly
    if you have prepared data points that represent a trip that you consider unlikely
    to be delayed, as well as data points that represent a trip that you believe will
    be delayed. By getting the trained model to score these two datapoints, you get
    validation of whether the trained model makes the predictions that you expect.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于这个单一数据点，模型不预测延迟。能够对一个单一数据点进行评分是快速验证模型的好方法，尤其是如果你已经准备了代表你认为不太可能延迟的行程的数据点，以及代表你认为将会延迟的行程的数据点。通过让训练好的模型对这些两个数据点进行评分，你可以验证训练好的模型是否做出了你预期的预测。
- en: Figure 6.20 shows two example trips that could be scored to exercise the trained
    model. We would expect Trip A (on a less-busy route late on a weekend) to not
    be delayed, and we would expect Trip B (during rush hour on a busy route) to have
    a high likelihood of being delayed.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.20显示了两个可以评分以锻炼训练模型的示例行程。我们预计行程A（在周末晚些时候的较不繁忙的路线上）不会延迟，而行程B（在高峰时段的繁忙路线上）有很高的可能性会延迟。
- en: '![CH06_F20_Ryan](../Images/CH06_F20_Ryan.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F20_Ryan](../Images/CH06_F20_Ryan.png)'
- en: Figure 6.20 Exercising the model by scoring individual trips
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.20 通过评分单个行程来锻炼模型
- en: 6.10 Explicitly saving trained models
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.10 明确保存训练好的模型
- en: Like a Pandas dataframe, a trained model exists only for the life of your Python
    session unless you save it. You need to be able to serialize and save your trained
    model so that you can load it again later to experiment with it and, ultimately,
    deploy it so you can conveniently score new data by using your trained model.
    The model is saved as part of the callbacks if you set the `early_stopping` parameter.
    If not, the code block in the next listing saves the model.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 与Pandas数据框一样，训练好的模型仅在Python会话的生命周期内存在，除非你将其保存。你需要能够序列化和保存你的训练模型，以便你可以在以后加载它来进行实验，并最终部署它，这样你可以方便地使用训练好的模型对新数据进行评分。如果你设置了`early_stopping`参数，模型将作为回调的一部分保存。如果没有，下一列表中的代码块将保存模型。
- en: Listing 6.11 Code to save the model if early stopping is not used
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.11 如果不使用早期停止，保存模型的代码
- en: '[PRE20]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Check whether the model has already been saved via callbacks.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过回调检查模型是否已经被保存。
- en: ❷ Save the model to a JSON file.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将模型保存到JSON文件中。
- en: ❸ Save the weights of the trained model to an h5 file.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将训练好的模型权重保存到h5文件中。
- en: ❹ Save the model and weights to an h5 file.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将模型和权重保存到h5文件中。
- en: You can exercise loading a model that you have saved to an h5 file (see the
    next listing).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以练习加载你保存到h5文件中的模型（见下一列表）。
- en: Listing 6.12 Code to load a model from an h5 file
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.12 从h5文件加载模型的代码
- en: '[PRE21]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Load your saved model by using the same path you used to save the model.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用与保存模型相同的路径加载你保存的模型。
- en: 'Now that you have loaded your saved model, you can apply it to get predictions
    on the test set:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经加载了保存的模型，你可以将其应用于测试集以获取预测：
- en: '[PRE22]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 6.11 Running a series of training experiments
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.11 运行一系列训练实验
- en: Now let’s pull everything together by running a series of experiments that build
    on what we’ve learned so far in this chapter. You can run these experiments yourself
    by changing the `current_experiment` parameter in the model training config file
    (listing 6.13), as shown in the next listing.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们通过运行一系列实验来汇总所有内容，这些实验基于本章到目前为止所学的内容。你可以通过更改模型训练配置文件中的`current_experiment`参数（列表6.13）来运行这些实验，如下所示。
- en: Listing 6.13 Parameters to control test execution
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.13 控制测试执行的参数
- en: '[PRE23]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Set the experiment number.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置实验编号。
- en: 'The `current_experiment` parameter in turn is used to set the parameters for
    the experiment in the call to the `set_experiment_parameters` function:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`current_experiment`参数反过来用于在调用`set_experiment_parameters`函数时设置实验的参数：'
- en: '[PRE24]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Figure 6.21 summarizes the parameter settings for these experiments along with
    the key results: validation accuracy, number of false negatives, and recall on
    the test set.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.21总结了这些实验的参数设置以及关键结果：验证准确率、测试集上的假阴性数量和召回率。
- en: '![CH06_F21_Ryan](../Images/CH06_F21_Ryan.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F21_Ryan](../Images/CH06_F21_Ryan.png)'
- en: Figure 6.21 Summary of the results of a set of experiments training the model
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.21 一组训练模型实验结果的总结
- en: These experiments layer in various techniques, from adding additional epochs
    to weighting the less common outcome (delay) to early stopping. These experiments
    are defined by values set for a series of parameters, as shown in the following
    listing.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实验层层叠加了各种技术，从增加额外的训练轮次到对较少出现的延迟结果进行加权，再到早期停止。这些实验由一系列参数的设置值定义，如下所示。
- en: Listing 6.14 Parameters that control numbered experiments
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.14 控制编号实验的参数
- en: '[PRE25]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In these experiments, we will adjust the number of epochs, the weighting of
    delay outcomes, and early-stopping callbacks. For each experiment, we will track
    the following performance measures:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些实验中，我们将调整训练轮次的数量、延迟结果的加权以及早期停止回调。对于每个实验，我们将跟踪以下性能指标：
- en: '*Terminal validation accuracy* —The validation accuracy for the last epoch
    of the run'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*终端验证准确率* — 运行的最后一个训练轮次的验证准确率'
- en: '*Total number of false negatives* —The number of times the model predicts no
    delay when there is a delay'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*总误判负例数* —模型预测无延迟而实际上存在延迟的次数'
- en: '*Recall* —True positives / (true positives + false negatives)'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*召回率* —真阳性数 / (真阳性数 + 误判负例数)'
- en: As we make changes in the parameters for the experiments, the performance measures
    improve. This set of experiments is useful for a problem as simple as the streetcar
    delay problem, but it is not representative of how much experimentation may be
    needed in real-world deep learning problems. In an industrial-strength model training
    situation, you would expect to include a larger variety of experiments that vary
    the values of a larger number of parameters (such as the learning rate, dropout,
    and regularization parameters described in chapter 5). It’s also possible that
    you may adjust the number and kind of layers in the model if you don’t get the
    required performance measures from your original model.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们对实验的参数进行更改时，性能指标会提高。这一组实验对于像电车延迟问题这样简单的问题是有用的，但它并不能代表在现实世界的深度学习问题中可能需要的实验量。在一个工业强度的模型训练情况下，你可能会包括更多种类的实验，这些实验会改变更多参数的值（例如，第5章中描述的学习率、dropout和正则化参数）。如果你从原始模型中没有获得所需的表现指标，你还可以调整模型中层数的数量和类型。
- en: When you are beginning with deep learning, it can be tempting to make these
    kinds of changes in the architecture of the model if you are not seeing adequate
    performance with your original architecture. I advise you to begin by focusing
    on understanding the performance characteristics of your original model, starting
    with test runs with a small number of epochs, methodically adjusting parameters
    one at a time, and measuring a consistent performance measure (such as validation
    accuracy or validation loss) throughout the runs. Then, if you exhaust performance
    improvements for your original architecture and still haven’t met your performance
    goals, consider changing the architecture of the model.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 当你刚开始接触深度学习时，如果你在原始架构中没有看到足够的表现，可能会倾向于对这些模型架构进行这类更改。我建议你首先专注于理解原始模型的表现特性，从少量epoch的测试运行开始，逐个调整参数，并在整个运行过程中测量一致的性能指标（如验证准确率或验证损失）。然后，如果你已经用尽原始架构的性能改进，仍然没有达到你的性能目标，考虑更改模型架构。
- en: 'Let’s go through each of the five training experiments we defined earlier in
    this section. Starting with experiment 1, we run for a small number of epochs,
    don’t account for the imbalance in the training data between delays and nondelays,
    and don’t employ callbacks. The accuracy result looks good, but the confusion
    matrix shown in figure 6.22 reveals what’s happening: the model is always predicting
    no delay.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下本节中定义的五个训练实验中的每一个。从实验1开始，我们运行了少量epoch，没有考虑到训练数据中延迟和非延迟之间的不平衡，也没有使用回调。准确率的结果看起来不错，但图6.22所示的混淆矩阵揭示了发生的情况：模型总是预测无延迟。
- en: '![CH06_F22_Ryan](../Images/CH06_F22_Ryan.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F22_Ryan](../Images/CH06_F22_Ryan.png)'
- en: Figure 6.22 Confusion matrix for experiment 1
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.22 实验一的混淆矩阵
- en: This model will not be useful for the application to which we want to apply
    this model because it will never predict a delay. Figure 6.23 shows what happens
    in experiment 2 when we have five times as many epochs.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个模型永远不会预测延迟，所以它对我们想要应用这个模型的应用场景将没有用。图6.23显示了在epoch增加到五倍时实验2发生的情况。
- en: '![CH06_F23_Ryan](../Images/CH06_F23_Ryan.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F23_Ryan](../Images/CH06_F23_Ryan.png)'
- en: Figure 6.23 Confusion matrix for experiment 2
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.23 实验二的混淆矩阵
- en: With more epochs, the model is predicting some delays, but there are twice as
    many false negatives as true positives, so this model is not meeting the goal
    of minimizing false negatives.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 随着epoch的增加，模型开始预测一些延迟，但误判负例的数量是真阳性数量的两倍，所以这个模型没有达到最小化误判负例的目标。
- en: In experiment 3, we account for the imbalance in the dataset between delays
    and nondelays by weighting the delays (figure 6.24).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验3中，我们通过加权延迟（图6.24）来考虑数据集中延迟和非延迟之间的不平衡。
- en: '![CH06_F24_Ryan](../Images/CH06_F24_Ryan.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F24_Ryan](../Images/CH06_F24_Ryan.png)'
- en: Figure 6.24 Confusion matrix for experiment 3
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.24 实验三的混淆矩阵
- en: With this change, we see more true positives than false negatives, so it’s a
    move in the right direction for recall, but we can do better.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这次更改，我们看到了比误判负例更多的真阳性，所以这是提高召回率的正确方向，但我们还可以做得更好。
- en: 'In experiment 4, we add callbacks. The training process will be monitored for
    validation loss: the cumulative difference between the predictions and the actual
    values for the validation set. The training runs will stop if the validation loss
    does not decrease for a given number of epochs. Also, the model for the epoch
    that has the lowest validation loss will be the one saved when the training run
    ends. Again, with this change the ratio of true positives to false negatives (as
    indicated in recall) increases, as shown in figure 6.25, but we can still do better.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验4中，我们添加了回调。训练过程将监控验证损失：验证集预测值与实际值之间的累积差异。如果验证损失在给定数量的epoch后没有下降，则训练运行将停止。此外，具有最低验证损失的epoch的模型将在训练运行结束时被保存。再次强调，随着这个变化，正如图6.25所示，真正阳性和假阴性（如召回率所示）的比例增加，但我们仍然可以做得更好。
- en: '![CH06_F25_Ryan](../Images/CH06_F25_Ryan.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F25_Ryan](../Images/CH06_F25_Ryan.png)'
- en: Figure 6.25 Confusion matrix for experiment 4
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.25 实验4的混淆矩阵
- en: In experiment 5, we also have callbacks, but instead of monitoring validation
    loss, we monitor validation accuracy. We stop training if the accuracy does not
    improve for a given number of epochs. We save the model that has the maximum value
    for this measurement by the end of the training run. Figure 6.26 shows the confusion
    matrix for this run.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验5中，我们也有回调，但不是监控验证损失，而是监控验证准确率。如果准确率在给定数量的epoch后没有提高，我们将停止训练。我们将在训练运行结束时保存具有此测量值最大值的模型。图6.26显示了此运行的混淆矩阵。
- en: '![CH06_F26_Ryan](../Images/CH06_F26_Ryan.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F26_Ryan](../Images/CH06_F26_Ryan.png)'
- en: Figure 6.26 Confusion matrix for experiment 5
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.26 实验5的混淆矩阵
- en: The ratio of true positives to false negatives (as reflected in recall) is even
    better in experiment 5, and the validation accuracy is also slightly better.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验5中，真正阳性和假阴性（如召回率所反映）的比例甚至更好，验证准确率也有所提高。
- en: 'Note that you may get different results when you run these experiments yourself
    with the same input. But you should see the same general trend as you layer in
    the changes shown in these five experiments. Also note that you can take additional
    steps to get better results: higher accuracy and a better ratio of true positives
    to false negatives. We will examine some of these steps in chapter 7.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当您使用相同的输入运行这些实验时，您可能会得到不同的结果。但您应该看到与这些五个实验中所示的变化相同的总体趋势。此外，请注意，您可以采取额外的步骤来获得更好的结果：更高的准确率和更好的真正阳性和假阴性比例。我们将在第7章中检查其中的一些步骤。
- en: Summary
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Training a deep learning model is an iterative process. By tracking the right
    performance measurements during and at the conclusion of training runs, you will
    be able to methodically adjust parameters involved in the training process, see
    the effect of the changes, and progress toward a trained model that meets your
    goals for the training process.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练深度学习模型是一个迭代的过程。通过在训练运行期间和结束时跟踪正确的性能度量，您将能够系统地调整训练过程中涉及的参数，看到变化的效果，并朝着满足训练过程目标的有训练模型迈进。
- en: Before starting the training process, you need to define subsets of the dataset
    for training, validation (tracking the performance of the model during the training
    process), and testing (assessing the performance of the trained model).
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开始训练过程之前，您需要定义数据集的子集用于训练、验证（跟踪训练过程中的模型性能）和测试（评估训练模型的性能）。
- en: For your initial training run, pick a simple, single-epoch run to validate that
    everything is working correctly and you don’t have any functional problems. When
    you have successfully completed this initial run, you can progress to more complex
    training runs to improve the performance of the model.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于您的初始训练运行，选择一个简单、单epoch的运行来验证一切是否正常工作，并且没有功能性故障。当您成功完成这个初始运行后，您可以进行更复杂的训练运行以提高模型的性能。
- en: Keras provides a set of measurements that you can use to assess the performance
    of your model. Which one you pick depends on how your trained model is going to
    be used. For the streetcar delay prediction model, we assess performance by validation
    accuracy (how well the trained model’s predictions on the validation set match
    the actual delay/no delay values for the validation set) and recall (how well
    the model avoids predicting no delay when a delay occurred).
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras提供了一套测量方法，您可以使用这些方法来评估您模型的性能。您选择哪一种取决于您的训练模型将如何被使用。对于电车延误预测模型，我们通过验证准确率（训练模型在验证集上的预测与验证集的实际延误/无延误值匹配的程度）和召回率（模型避免在发生延误时预测无延误的程度）来评估性能。
- en: By default, a Keras training run goes through all the specified epochs, and
    the trained model you get is whatever was produced in the final epoch. If you
    want the training process to be efficient by avoiding extraneous epochs and ensuring
    that you save the best model, you need to take advantage of callbacks. With callbacks,
    you can stop the training process when the performance measurements you care about
    stop improving and can ensure that the best model from your training run gets
    saved.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，Keras的训练运行会遍历所有指定的周期，您得到的训练模型是最终周期产生的模型。如果您希望通过避免不必要的周期并确保保存最佳模型来提高训练过程的效率，您需要利用回调功能。使用回调，您可以在您关心的性能测量停止改进时停止训练过程，并确保从您的训练运行中保存最佳模型。
- en: When you have a trained model, it’s a good idea to experiment with scoring a
    few data points with it. In the case of the streetcar delay prediction model,
    you would score some time/route/direction combinations with your trained model.
    Doing this gives you a way to validate the overall behavior of the trained model
    before investing effort in the full-blown deployment of the model.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您有一个训练好的模型时，尝试用它评分几个数据点是个好主意。在电车延误预测模型的情况下，您会用您的训练模型评分一些时间/路线/方向组合。这样做为您提供了一种在投入大量努力进行模型全面部署之前验证训练模型整体行为的方法。
