- en: Chapter 15\. Automatic scaling of pods and cluster nodes
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第15章. Pod和集群节点自动扩展
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Configuring automatic horizontal scaling of pods based on CPU utilization
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据CPU利用率配置Pod的自动水平扩展
- en: Configuring automatic horizontal scaling of pods based on custom metrics
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据自定义指标配置Pod的自动水平扩展
- en: Understanding why vertical scaling of pods isn’t possible yet
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解为什么Pod的垂直扩展目前不可行
- en: Understanding automatic horizontal scaling of cluster nodes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解集群节点自动水平扩展
- en: Applications running in pods can be scaled out manually by increasing the `replicas`
    field in the ReplicationController, ReplicaSet, Deployment, or other scalable
    resource. Pods can also be scaled vertically by increasing their container’s resource
    requests and limits (though this can currently only be done at pod creation time,
    not while the pod is running). Although manual scaling is okay for times when
    you can anticipate load spikes in advance or when the load changes gradually over
    longer periods of time, requiring manual intervention to handle sudden, unpredictable
    traffic increases isn’t ideal.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 运行在Pod中的应用程序可以通过在ReplicationController、ReplicaSet、Deployment或其他可扩展资源中增加`replicas`字段来手动扩展。Pod也可以通过增加其容器的资源请求和限制（尽管目前只能在Pod创建时进行，不能在Pod运行时进行）进行垂直扩展。尽管手动扩展在可以提前预测负载峰值或在较长时间内负载逐渐变化时是可行的，但需要手动干预来处理突然、不可预测的流量增加并不是理想的做法。
- en: Luckily, Kubernetes can monitor your pods and scale them up automatically as
    soon as it detects an increase in the CPU usage or some other metric. If running
    on a cloud infrastructure, it can even spin up additional nodes if the existing
    ones can’t accept any more pods. This chapter will explain how to get Kubernetes
    to do both pod and node autoscaling.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Kubernetes可以监控您的Pod，并在检测到CPU使用量或其他指标增加时自动扩展它们。如果在云基础设施上运行，它甚至可以在现有节点无法接受更多Pod时启动额外的节点。本章将解释如何让Kubernetes同时进行Pod和节点自动扩展。
- en: The autoscaling feature in Kubernetes was completely rewritten between the 1.6
    and the 1.7 version, so be aware you may find outdated information on this subject
    online.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的自动扩展功能在1.6和1.7版本之间完全重写，因此请注意，您可能会在网上找到关于这个主题的过时信息。
- en: 15.1\. Horizontal pod autoscaling
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 15.1. 水平Pod自动扩展
- en: Horizontal pod autoscaling is the automatic scaling of the number of pod replicas
    managed by a controller. It’s performed by the Horizontal controller, which is
    enabled and configured by creating a HorizontalPodAutoscaler (HPA) resource. The
    controller periodically checks pod metrics, calculates the number of replicas
    required to meet the target metric value configured in the HorizontalPodAutoscaler
    resource, and adjusts the `replicas` field on the target resource (Deployment,
    ReplicaSet, Replication-Controller, or StatefulSet).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 水平Pod自动扩展是由控制器管理的Pod副本数量的自动扩展。它由水平控制器执行，该控制器通过创建HorizontalPodAutoscaler (HPA)
    资源来启用和配置。控制器定期检查Pod指标，计算满足HorizontalPodAutoscaler资源中配置的目标指标值所需的副本数量，并调整目标资源（Deployment、ReplicaSet、Replication-Controller或StatefulSet）上的`replicas`字段。
- en: 15.1.1\. Understanding the autoscaling process
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 15.1.1. 理解自动扩展过程
- en: 'The autoscaling process can be split into three steps:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自动扩展过程可以分为三个步骤：
- en: Obtain metrics of all the pods managed by the scaled resource object.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取由扩展资源对象管理的所有Pod的指标。
- en: Calculate the number of pods required to bring the metrics to (or close to)
    the specified target value.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算需要多少Pod才能将指标提升到（或接近）指定的目标值。
- en: Update the `replicas` field of the scaled resource.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新扩展资源的`replicas`字段。
- en: Let’s examine all three steps next.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们接下来检查所有三个步骤。
- en: Obtaining pod metrics
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 获取Pod指标
- en: The Autoscaler doesn’t perform the gathering of the pod metrics itself. It gets
    the metrics from a different source. As we saw in the previous chapter, pod and
    node metrics are collected by an agent called cAdvisor, which runs in the Kubelet
    on each node, and then aggregated by the cluster-wide component called Heapster.
    The horizontal pod autoscaler controller gets the metrics of all the pods by querying
    Heapster through REST calls. The flow of metrics data is shown in [figure 15.1](#filepos1427900)
    (although all the connections are initiated in the opposite direction).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 自动扩展器本身不执行 pod 指标的收集。它从不同的来源获取指标。正如我们在上一章中看到的，pod 和节点指标由一个称为 cAdvisor 的代理收集，该代理在每个节点上运行
    Kubelet，然后由集群范围内的组件 Heapster 进行聚合。水平 pod 自动扩展器控制器通过 REST 调用查询 Heapster 来获取所有 pod
    的指标。指标数据的流程如图 [15.1](#filepos1427900) 所示（尽管所有连接都是相反方向发起的）。
- en: Figure 15.1\. Flow of metrics from the pod(s) to the HorizontalPodAutoscaler(s)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.1\. 从 pod 到水平 pod 自动扩展器的指标流程
- en: '![](images/00068.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00068.jpg)'
- en: This implies that Heapster must be running in the cluster for autoscaling to
    work. If you’re using Minikube and were following along in the previous chapter,
    Heapster should already be enabled in your cluster. If not, make sure to enable
    the Heapster add-on before trying out any autoscaling examples.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 Heapster 必须在集群中运行才能使自动扩展工作。如果你使用 Minikube 并且在上一章中跟随操作，Heapster 应该已经在你的集群中启用。如果没有，确保在尝试任何自动扩展示例之前启用
    Heapster 扩展。
- en: Although you don’t need to query Heapster directly, if you’re interested in
    doing so, you’ll find both the Heapster Pod and the Service it’s exposed through
    in the `kube-system` namespace.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你不需要直接查询 Heapster，但如果你有兴趣这样做，你将在 `kube-system` 命名空间中找到 Heapster Pod 和它暴露的
    Service。
- en: '|  |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: A look at changes related to how the Autoscaler obtains metrics
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 查看与自动扩展器获取指标相关联的更改
- en: Prior to Kubernetes version 1.6, the HorizontalPodAutoscaler obtained the metrics
    from Heapster directly. In version 1.8, the Autoscaler can get the metrics through
    an aggregated version of the resource metrics API by starting the Controller Manager
    with the `--horizontal-pod-autoscaler-use-rest-clients=true flag`. From version
    1.9, this behavior will be enabled by default.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 版本 1.6 之前，水平 pod 自动扩展器直接从 Heapster 获取指标。在版本 1.8 中，自动扩展器可以通过启动 Controller
    Manager 时使用 `--horizontal-pod-autoscaler-use-rest-clients=true` 标志来通过资源指标 API
    的聚合版本获取指标。从版本 1.9 开始，此行为将默认启用。
- en: The core API server will not expose the metrics itself. From version 1.7, Kubernetes
    allows registering multiple API servers and making them appear as a single API
    server. This allows it to expose metrics through one of those underlying API servers.
    We’ll explain API server aggregation in the last chapter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 核心API服务器不会自己公开指标。从版本 1.7 开始，Kubernetes 允许注册多个 API 服务器并将它们表现为单个 API 服务器。这允许它通过这些底层
    API 服务器之一公开指标。我们将在最后一章中解释 API 服务器聚合。
- en: Selecting what metrics collector to use in their clusters will be up to cluster
    administrators. A simple translation layer is usually required to expose the metrics
    in the appropriate API paths and in the appropriate format.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 选择在他们的集群中使用哪个指标收集器将由集群管理员决定。通常需要一个简单的翻译层来在适当的 API 路径和适当的格式中公开指标。
- en: '|  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Calculating the required number of pods
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 计算所需的 pod 数量
- en: Once the Autoscaler has metrics for all the pods belonging to the resource the
    Autoscaler is scaling (the Deployment, ReplicaSet, ReplicationController, or StatefulSet
    resource), it can use those metrics to figure out the required number of replicas.
    It needs to find the number that will bring the average value of the metric across
    all those replicas as close to the configured target value as possible. The input
    to this calculation is a set of pod metrics (possibly multiple metrics per pod)
    and the output is a single integer (the number of pod replicas).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦自动扩展器获得了正在扩展的资源（部署、副本集、复制控制器或有状态集资源）所属的所有 pod 的指标，它就可以使用这些指标来确定所需的副本数量。它需要找到将所有这些副本的指标平均值尽可能接近配置的目标值的数字。此计算的输入是一组
    pod 指标（每个 pod 可能可能有多个指标），输出是一个整数（pod 副本的数量）。
- en: When the Autoscaler is configured to consider only a single metric, calculating
    the required replica count is simple. All it takes is summing up the metrics values
    of all the pods, dividing that by the target value set on the HorizontalPodAutoscaler
    resource, and then rounding it up to the next-larger integer. The actual calculation
    is a bit more involved than this, because it also makes sure the Autoscaler doesn’t
    thrash around when the metric value is unstable and changes rapidly.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当自动扩展器配置为仅考虑单个指标时，计算所需的副本数量是简单的。只需将所有Pod的指标值相加，然后除以HorizontalPodAutoscaler资源上设置的目标值，最后向上取整到下一个更大的整数。实际的计算比这要复杂一些，因为它还确保当指标值不稳定且快速变化时，自动扩展器不会频繁波动。
- en: When autoscaling is based on multiple pod metrics (for example, both CPU usage
    and Queries-Per-Second [QPS]), the calculation isn’t that much more complicated.
    The Autoscaler calculates the replica count for each metric individually and then
    takes the highest value (for example, if four pods are required to achieve the
    target CPU usage, and three pods are required to achieve the target QPS, the Autoscaler
    will scale to four pods). [Figure 15.2](#filepos1431760) shows this example.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当自动扩展基于多个Pod指标（例如，CPU使用率和每秒查询数[QPS]）时，计算并没有变得复杂多少。自动扩展器会为每个指标单独计算副本数量，然后取最高值（例如，如果需要四个Pod来实现目标CPU使用率，而需要三个Pod来实现目标QPS，自动扩展器将扩展到四个Pod）。[图15.2](#filepos1431760)展示了这个例子。
- en: Figure 15.2\. Calculating the number of replicas from two metrics
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2\. 从两个指标计算副本数量
- en: '![](images/00087.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00087.jpg)'
- en: Updating the desired replica count on the scaled resource
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 更新扩展资源上的期望副本数量
- en: The final step of an autoscaling operation is updating the desired replica count
    field on the scaled resource object (a ReplicaSet, for example) and then letting
    the Replica-Set controller take care of spinning up additional pods or deleting
    excess ones.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 自动扩展操作的最终步骤是在扩展资源对象（例如ReplicaSet）上更新期望副本数量字段，然后让ReplicaSet控制器负责启动额外的Pod或删除多余的Pod。
- en: The Autoscaler controller modifies the `replicas` field of the scaled resource
    through the Scale sub-resource. It enables the Autoscaler to do its work without
    knowing any details of the resource it’s scaling, except for what’s exposed through
    the Scale sub-resource (see [figure 15.3](#filepos1432756)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 自动扩展器控制器通过Scale子资源修改扩展资源的`replicas`字段。它使自动扩展器能够在不知道它所扩展的资源任何细节的情况下（除了通过Scale子资源暴露的内容之外），完成其工作（参见[图15.3](#filepos1432756)）。
- en: Figure 15.3\. The Horizontal Pod Autoscaler modifies only on the Scale sub-resource.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3\. 水平Pod自动扩展器仅修改Scale子资源。
- en: '![](images/00105.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00105.jpg)'
- en: This allows the Autoscaler to operate on any scalable resource, as long as the
    API server exposes the Scale sub-resource for it. Currently, it’s exposed for
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得自动扩展器可以在任何可扩展资源上操作，只要API服务器为其公开Scale子资源。目前，它已公开用于
- en: Deployments
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deployments
- en: ReplicaSets
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReplicaSets
- en: ReplicationControllers
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReplicationControllers
- en: StatefulSets
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StatefulSets
- en: These are currently the only objects you can attach an Autoscaler to.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是目前您可以附加自动扩展器的唯一对象。
- en: Understanding the whole autoscaling process
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 理解整个自动扩展过程
- en: You now understand the three steps involved in autoscaling, so let’s visualize
    all the components involved in the autoscaling process. They’re shown in [figure
    15.4](#filepos1433911).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经了解了自动扩展涉及的三步，那么让我们可视化自动扩展过程中涉及的所有组件。它们在[图15.4](#filepos1433911)中展示。
- en: Figure 15.4\. How the autoscaler obtains metrics and rescales the target deployment
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4\. 自动扩展器如何获取指标并调整目标部署
- en: '![](images/00125.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00125.jpg)'
- en: The arrows leading from the pods to the cAdvisors, which continue on to Heapster
    and finally to the Horizontal Pod Autoscaler, indicate the direction of the flow
    of metrics data. It’s important to be aware that each component gets the metrics
    from the other components periodically (that is, cAdvisor gets the metrics from
    the pods in a continuous loop; the same is also true for Heapster and for the
    HPA controller). The end effect is that it takes quite a while for the metrics
    data to be propagated and a rescaling action to be performed. It isn’t immediate.
    Keep this in mind when you observe the Autoscaler in action next.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从Pod指向cAdvisors的箭头，这些箭头继续指向Heapster，最后指向Horizontal Pod Autoscaler，表示指标数据的流向。重要的是要意识到，每个组件都会定期从其他组件获取指标（即cAdvisor以连续循环的方式从Pod获取指标；Heapster和HPA控制器也是如此）。最终结果是，指标数据传播和重新扩展操作需要相当长的时间。这不是立即发生的。当你观察自动扩展器的工作时，请记住这一点。
- en: 15.1.2\. Scaling based on CPU utilization
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 15.1.2\. 基于CPU利用率的扩展
- en: Perhaps the most important metric you’ll want to base autoscaling on is the
    amount of CPU consumed by the processes running inside your pods. Imagine having
    a few pods providing a service. When their CPU usage reaches 100% it’s obvious
    they can’t cope with the demand anymore and need to be scaled either up (vertical
    scaling—increasing the amount of CPU the pods can use) or out (horizontal scaling—increasing
    the number of pods). Because we’re talking about the horizontal pod autoscaler
    here, we’re only focusing on scaling out (increasing the number of pods). By doing
    that, the average CPU usage should come down.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 可能你最希望基于的最重要的指标是运行在你Pod内的进程消耗的CPU数量。想象一下有几个Pod提供一种服务。当它们的CPU使用率达到100%时，很明显它们已经无法再满足需求，需要扩展，要么向上（垂直扩展——增加Pod可以使用的CPU数量），要么向外（水平扩展——增加Pod的数量）。因为我们在这里讨论的是Horizontal
    Pod Autoscaler，所以我们只关注向外扩展（增加Pod的数量）。通过这样做，平均CPU使用率应该会下降。
- en: Because CPU usage is usually unstable, it makes sense to scale out even before
    the CPU is completely swamped—perhaps when the average CPU load across the pods
    reaches or exceeds 80%. But 80% of what, exactly?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于CPU使用率通常不稳定，因此在CPU完全饱和之前进行扩展是有意义的——例如，当Pod的平均CPU负载达到或超过80%时。但这里的80%是指什么？
- en: '|  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Always set the target CPU usage well below 100% (and definitely never above
    90%) to leave enough room for handling sudden load spikes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 总是设置目标CPU使用率低于100%（并且绝对不能超过90%），以留出足够的空间来处理突发的负载峰值。
- en: '|  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: As you may remember from the previous chapter, the process running inside a
    container is guaranteed the amount of CPU requested through the resource requests
    specified for the container. But at times when no other processes need CPU, the
    process may use all the available CPU on the node. When someone says a pod is
    consuming 80% of the CPU, it’s not clear if they mean 80% of the node’s CPU, 80%
    of the pod’s guaranteed CPU (the resource request), or 80% of the hard limit configured
    for the pod through resource limits.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如你从上一章所记得的，运行在容器内的进程通过为容器指定的资源请求保证获得CPU数量。但在没有其他进程需要CPU的时候，进程可能会使用节点上所有可用的CPU。当有人说一个Pod消耗了80%的CPU时，并不清楚他们是指节点的80%的CPU，Pod保证的80%的CPU（资源请求），还是通过资源限制为Pod配置的硬限制的80%。
- en: As far as the Autoscaler is concerned, only the pod’s guaranteed CPU amount
    (the CPU requests) is important when determining the CPU utilization of a pod.
    The Autoscaler compares the pod’s actual CPU consumption and its CPU requests,
    which means the pods you’re autoscaling need to have CPU requests set (either
    directly or indirectly through a LimitRange object) for the Autoscaler to determine
    the CPU utilization percentage.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自动扩展器而言，在确定Pod的CPU利用率时，只有Pod保证的CPU数量（即CPU请求）是重要的。自动扩展器比较Pod的实际CPU消耗和其CPU请求，这意味着你正在自动扩展的Pod需要设置CPU请求（无论是直接设置还是通过LimitRange对象间接设置），以便自动扩展器能够确定CPU利用率百分比。
- en: Creating a HorizontalPodAutoscaler based on CPU usage
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 基于CPU使用情况创建HorizontalPodAutoscaler
- en: Let’s see how to create a HorizontalPodAutoscaler now and configure it to scale
    pods based on their CPU utilization. You’ll create a Deployment similar to the
    one in [chapter 9](index_split_074.html#filepos865425), but as we’ve discussed,
    you’ll need to make sure the pods created by the Deployment all have the CPU resource
    requests specified in order to make autoscaling possible. You’ll have to add a
    CPU resource request to the Deployment’s pod template, as shown in the following
    listing.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看如何创建一个HorizontalPodAutoscaler，并配置它根据Pod的CPU利用率进行扩展。你将创建一个类似于第9章中提到的Deployment，但正如我们讨论的那样，你需要确保由Deployment创建的所有Pod都指定了CPU资源请求，以便实现自动扩展。你必须在Deployment的Pod模板中添加一个CPU资源请求，如下所示。
- en: 'Listing 15.1\. Deployment with CPU requests set: deployment.yaml'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.1\. 设置了CPU请求的Deployment：deployment.yaml
- en: '`apiVersion: extensions/v1beta1 kind: Deployment metadata:   name: kubia spec:
      replicas: 3` `1` `template:     metadata:       name: kubia       labels:        
    app: kubia     spec:       containers:       - image: luksa/kubia:v1` `2` `name:
    nodejs         resources:` `3` `requests:` `3` `cpu: 100m` `3`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: extensions/v1beta1 kind: Deployment metadata:   name: kubia spec:
      replicas: 3` `1` `template:     metadata:       name: kubia       labels:        
    app: kubia     spec:       containers:       - image: luksa/kubia:v1` `2` `name:
    nodejs         resources:` `3` `requests:` `3` `cpu: 100m` `3`'
- en: 1 Manually setting the (initial) desired number of replicas to three
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 手动设置（初始）所需的副本数为三个
- en: 2 Running the kubia:v1 image
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 运行kubia:v1镜像
- en: 3 Requesting 100 millicores of CPU per pod
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个Pod请求100毫核CPU
- en: This is a regular Deployment object—it doesn’t use autoscaling yet. It will
    run three instances of the `kubia` NodeJS app, with each instance requesting 100
    millicores of CPU.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个常规的Deployment对象——它还没有使用自动扩展。它将运行三个`kubia` NodeJS应用的实例，每个实例请求100毫核的CPU。
- en: 'After creating the Deployment, to enable horizontal autoscaling of its pods,
    you need to create a HorizontalPodAutoscaler (HPA) object and point it to the
    Deployment. You could prepare and post the YAML manifest for the HPA, but an easier
    way exists—using the `kubectl autoscale` command:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建Deployment之后，为了启用其Pod的水平自动扩展，你需要创建一个HorizontalPodAutoscaler (HPA)对象并将其指向Deployment。你可以准备和发布HPA的YAML清单，但有一个更简单的方法——使用`kubectl
    autoscale`命令：
- en: '`$ kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5` `deployment
    "kubia" autoscaled`'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5` `deployment
    "kubia" autoscaled`'
- en: This creates the HPA object for you and sets the Deployment called `kubia` as
    the scaling target. You’re setting the target CPU utilization of the pods to 30%
    and specifying the minimum and maximum number of replicas. The Autoscaler will
    constantly keep adjusting the number of replicas to keep their CPU utilization
    around 30%, but it will never scale down to less than one or scale up to more
    than five replicas.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为你创建HPA对象，并将名为`kubia`的Deployment设置为扩展目标。你正在设置Pod的目标CPU利用率为30%，并指定了最小和最大副本数。自动扩展器将不断调整副本数，以保持其CPU利用率在30%左右，但绝不会将副本数缩减到少于一个或增加到多于五个。
- en: '|  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Always make sure to autoscale Deployments instead of the underlying ReplicaSets.
    This way, you ensure the desired replica count is preserved across application
    updates (remember that a Deployment creates a new ReplicaSet for each version).
    The same rule applies to manual scaling, as well.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 总是要确保自动扩展Deployment而不是底层的ReplicaSet。这样，你确保了在应用程序更新期间保留所需的副本数（记住，Deployment为每个版本创建一个新的ReplicaSet）。同样的规则也适用于手动扩展。
- en: '|  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Let’s look at the definition of the HorizontalPodAutoscaler resource to gain
    a better understanding of it. It’s shown in the following listing.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看HorizontalPodAutoscaler资源的定义，以便更好地理解它。它如下所示。
- en: Listing 15.2\. A HorizontalPodAutoscaler YAML definition
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.2\. HorizontalPodAutoscaler的YAML定义
- en: '`$ kubectl get hpa.v2beta1.autoscaling kubia -o yaml` `apiVersion: autoscaling/v2beta1`
    `1` `kind: HorizontalPodAutoscaler` `1` `metadata:   name: kubia` `2` `... spec:
      maxReplicas: 5` `3` `metrics:` `4` `- resource:` `4` `name: cpu` `4` `targetAverageUtilization:
    30` `4` `type: Resource` `4` `minReplicas: 1` `3` `scaleTargetRef:` `5` `apiVersion:
    extensions/v1beta1` `5` `kind: Deployment` `5` `name: kubia` `5` `status:   currentMetrics:
    []` `6` `currentReplicas: 3` `6` `desiredReplicas: 0` `6`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get hpa.v2beta1.autoscaling kubia -o yaml` `apiVersion: autoscaling/v2beta1`
    `1` `kind: HorizontalPodAutoscaler` `1` `metadata:   name: kubia` `2` `... spec:
      maxReplicas: 5` `3` `metrics:` `4` `- resource:` `4` `name: cpu` `4` `targetAverageUtilization:
    30` `4` `type: Resource` `4` `minReplicas: 1` `3` `scaleTargetRef:` `5` `apiVersion:
    extensions/v1beta1` `5` `kind: Deployment` `5` `name: kubia` `5` `status:   currentMetrics:
    []` `6` `currentReplicas: 3` `6` `desiredReplicas: 0` `6`'
- en: 1 HPA resources are in the autoscaling API group.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 HPA资源位于autoscaling API组中。
- en: 2 Each HPA has a name (it doesn’t need to match the name of the Deployment as
    in this case).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 每个HPA都有一个名称（它不需要与部署的名称匹配，如本例所示）。
- en: 3 The minimum and maximum number of replicas you specified
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 你指定的副本的最小和最大数量
- en: 4 You’d like the Autoscaler to adjust the number of pods so they each utilize
    30% of requested CPU.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4 你希望自动扩展器调整Pod的数量，使每个Pod利用30%的请求CPU。
- en: 5 The target resource which this Autoscaler will act upon
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 5 这个自动扩展器将要作用的资源目标
- en: 6 The current status of the Autoscaler
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 6 自动扩展器的当前状态
- en: '|  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Multiple versions of HPA resources exist: the new `autoscaling/v2beta1` and
    the old `autoscaling/v1`. You’re requesting the new version here.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: HPA资源存在多个版本：新的`autoscaling/v2beta1`和旧的`autoscaling/v1`。你在这里请求的是新版本。
- en: '|  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Seeing the first automatic rescale event
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 看到第一次自动缩放事件
- en: 'It takes a while for cAdvisor to get the CPU metrics and for Heapster to collect
    them before the Autoscaler can take action. During that time, if you display the
    HPA resource with `kubectl get`, the `TARGETS` column will show `<unknown>`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动扩展器可以采取行动之前，cAdvisor需要获取CPU指标，Heapster需要收集它们，这需要一段时间。在这段时间内，如果你使用`kubectl
    get`显示HPA资源，`TARGETS`列将显示`<unknown>`：
- en: '`$ kubectl get hpa` `NAME      REFERENCE          TARGETS           MINPODS  
    MAXPODS   REPLICAS kubia     Deployment/kubia   <unknown> / 30%   1         5        
    0`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get hpa` `NAME      REFERENCE          TARGETS           MINPODS  
    MAXPODS   REPLICAS kubia     Deployment/kubia   <unknown> / 30%   1         5        
    0`'
- en: Because you’re running three pods that are currently receiving no requests,
    which means their CPU usage should be close to zero, you should expect the Autoscaler
    to scale them down to a single pod, because even with a single pod, the CPU utilization
    will still be below the 30% target.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你正在运行三个当前没有收到请求的Pod，这意味着它们的CPU使用率应该接近零，因此你应该预期自动扩展器将它们缩放到单个Pod，因为即使只有一个Pod，CPU利用率仍然会低于30%的目标。
- en: 'And sure enough, the autoscaler does exactly that. It soon scales the Deployment
    down to a single replica:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 确实如此，自动扩展器正是这样做的。它很快将Deployment缩放到单个副本：
- en: '`$ kubectl get deployment` `NAME` `DESIRED``CURRENT   UP-TO-DATE   AVAILABLE  
    AGE kubia` `1``         1         1            1           23m`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get deployment` `NAME` `DESIRED``CURRENT   UP-TO-DATE   AVAILABLE  
    AGE kubia` `1``         1         1            1           23m`'
- en: Remember, the autoscaler only adjusts the desired replica count on the Deployment.
    The Deployment controller then takes care of updating the desired replica count
    on the ReplicaSet object, which then causes the ReplicaSet controller to delete
    two excess pods, leaving one pod running.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，自动扩展器只调整Deployment上的期望副本数。然后，Deployment控制器负责更新ReplicaSet对象上的期望副本数，这会导致ReplicaSet控制器删除两个多余的Pod，留下一个Pod运行。
- en: You can use `kubectl describe` to see more information on the HorizontalPod-Autoscaler
    and the operation of the underlying controller, as the following listing shows.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`kubectl describe`来查看关于HorizontalPod-Autoscaler和底层控制器操作的更多信息，如下面的列表所示。
- en: Listing 15.3\. Inspecting a HorizontalPodAutoscaler with `kubectl describe`
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.3\. 使用`kubectl describe`检查HorizontalPodAutoscaler
- en: '`$ kubectl describe hpa` `Name:                             kubia Namespace:                       
    default Labels:                           <none> Annotations:                     
    <none> CreationTimestamp:                Sat, 03 Jun 2017 12:59:57 +0200 Reference:                       
    Deployment/kubia Metrics:                          ( current / target )   resource
    cpu on pods   (as a percentage of request):   0% (0) / 30% Min replicas:                    
    1 Max replicas:                     5 Events: From                        Reason             
    Message ----                        ------              --- horizontal-pod-autoscaler  
    SuccessfulRescale   New size: 1; reason: All                                                
    metrics below target`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl describe hpa` `Name:                             kubia Namespace:                       
    default Labels:                           <none> Annotations:                     
    <none> CreationTimestamp:                Sat, 03 Jun 2017 12:59:57 +0200 Reference:                       
    Deployment/kubia Metrics:                          ( current / target )   resource
    cpu on pods   (as a percentage of request):   0% (0) / 30% Min replicas:                    
    1 Max replicas:                     5 Events: From                        Reason             
    Message ----                        ------              --- horizontal-pod-autoscaler  
    SuccessfulRescale   New size: 1; reason: All                                                
    metrics below target`'
- en: '|  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The output has been modified to make it more readable.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 输出已被修改以使其更易于阅读。
- en: '|  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Turn your focus to the table of events at the bottom of the listing. You see
    the horizontal pod autoscaler has successfully rescaled to one replica, because
    all metrics were below target.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 将你的注意力转向列表底部的事件表。你看到水平pod自动扩展器已成功调整到1个副本，因为所有指标都低于目标。
- en: Triggering a scale-up
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 触发扩容
- en: You’ve already witnessed your first automatic rescale event (a scale-down).
    Now, you’ll start sending requests to your pod, thereby increasing its CPU usage,
    and you should see the autoscaler detect this and start up additional pods.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经见证了你的第一个自动调整大小事件（缩小）。现在，你将开始向你的pod发送请求，从而增加其CPU使用率，你应该看到自动扩展器检测到这一点并启动额外的pods。
- en: 'You’ll need to expose the pods through a Service, so you can hit all of them
    through a single URL. You may remember that the easiest way to do that is with
    `kubectl expose`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要通过一个Service来暴露pods，这样你就可以通过一个URL访问它们。你可能记得，这样做最简单的方式是使用`kubectl expose`：
- en: '`$ kubectl expose deployment kubia --port=80 --target-port=8080` `service "kubia"
    exposed`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl expose deployment kubia --port=80 --target-port=8080` `service "kubia"
    exposed`'
- en: Before you start hitting your pod(s) with requests, you may want to run the
    following command in a separate terminal to keep an eye on what’s happening with
    the HorizontalPodAutoscaler and the Deployment, as shown in the following listing.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在你开始对你的pod(s)发送请求之前，你可能想在另一个终端中运行以下命令，以便密切关注HorizontalPodAutoscaler和Deployment的情况，如下所示。
- en: Listing 15.4\. Watching multiple resources in parallel
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.4.并行监视多个资源
- en: '`$ watch -n 1 kubectl get hpa,deployment` `Every 1.0s: kubectl get hpa,deployment  NAME       
    REFERENCE          TARGETS    MINPODS   MAXPODS   REPLICAS  AGE hpa/kubia   Deployment/kubia  
    0% / 30%   1         5         1         45m  NAME           DESIRED   CURRENT  
    UP-TO-DATE   AVAILABLE   AGE deploy/kubia   1         1         1            1          
    56m`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ watch -n 1 kubectl get hpa,deployment` `每1.0秒：kubectl get hpa,deployment  NAME       
    REFERENCE          TARGETS    MINPODS   MAXPODS   REPLICAS  AGE hpa/kubia   Deployment/kubia  
    0% / 30%   1         5         1         45m  NAME           DESIRED   CURRENT  
    UP-TO-DATE   AVAILABLE   AGE deploy/kubia   1         1         1            1          
    56m`'
- en: '|  |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: List multiple resource types with `kubectl get` by delimiting them with a comma.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用逗号分隔符，通过`kubectl get`列出多个资源类型。
- en: '|  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: If you’re using OSX, you’ll have to replace the `watch` command with a loop,
    manually run `kubectl get` periodically, or use `kubectl`’s `--watch` option.
    But although a plain `kubectl get` can show multiple types of resources at once,
    that’s not the case when using the aforementioned `--watch` option, so you’ll
    need to use two terminals if you want to watch both the HPA and the Deployment
    objects.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是OSX，你必须将`watch`命令替换为循环，手动定期运行`kubectl get`，或者使用`kubectl`的`--watch`选项。但尽管普通的`kubectl
    get`可以同时显示多种类型的资源，但在使用上述`--watch`选项时并非如此，所以如果你想同时监视HPA和Deployment对象，你需要使用两个终端。
- en: 'Keep an eye on the state of those two objects while you run a load-generating
    pod. You’ll run the following command in another terminal:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行负载生成pod时，请密切关注这两个对象的状态。你需要在另一个终端中运行以下命令：
- en: '`$ kubectl run -it --rm --restart=Never loadgenerator --image=busybox`![](images/00006.jpg)`--
    sh -c "while true; do wget -O - -q http://kubia.default; done"`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl run -it --rm --restart=Never loadgenerator --image=busybox`![](images/00006.jpg)`--
    sh -c "while true; do wget -O - -q http://kubia.default; done"`'
- en: This will run a pod which repeatedly hits the `kubia` Service. You’ve seen the
    `-it` option a few times when running the `kubectl exec` command. As you can see,
    it can also be used with `kubectl run`. It allows you to attach the console to
    the process, which will not only show you the process’ output directly, but will
    also terminate the process as soon as you press CTRL+C. The `--rm` option causes
    the pod to be deleted afterward, and the `--restart=Never` option causes `kubectl
    run` to create an unmanaged pod directly instead of through a Deployment object,
    which you don’t need. This combination of options is useful for running commands
    inside the cluster without having to piggyback on an existing pod. It not only
    behaves the same as if you were running the command locally, it even cleans up
    everything when the command terminates.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这将运行一个Pod，该Pod会反复调用`kubia`服务。你已经多次在运行`kubectl exec`命令时看到`-it`选项。正如你所见，它也可以与`kubectl
    run`一起使用。这允许你将控制台附加到进程，这不仅会直接显示进程的输出，而且在你按下CTRL+C时也会终止进程。`--rm`选项会在之后删除Pod，而`--restart=Never`选项会导致`kubectl
    run`直接创建一个未管理的Pod，而不是通过Deployment对象，这你不需要。这种选项组合对于在集群内部运行命令而不需要依赖现有Pod非常有用。它不仅表现得与你在本地运行命令时一样，而且在命令终止时还会清理所有内容。
- en: Seeing the Autoscaler scale up the deployment
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 观察自动扩展器扩展部署
- en: As the load-generator pod runs, you’ll see it initially hitting the single pod.
    As before, it takes time for the metrics to be updated, but when they are, you’ll
    see the autoscaler increase the number of replicas. In my case, the pod’s CPU
    utilization initially jumped to 108%, which caused the autoscaler to increase
    the number of pods to four. The utilization on the individual pods then decreased
    to 74% and then stabilized at around 26%.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 随着负载生成器Pod的运行，你会看到它最初攻击单个Pod。和之前一样，更新指标需要时间，但一旦更新，你就会看到自动扩展器增加副本的数量。在我的情况下，Pod的CPU利用率最初跳升到108%，这导致自动扩展器将Pod的数量增加到四个。然后，单个Pod的利用率下降到74%，然后稳定在大约26%左右。
- en: '|  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If the CPU load in your case doesn’t exceed 30%, try running additional load-generators.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你情况下的CPU负载不超过30%，尝试运行额外的负载生成器。
- en: '|  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Again, you can inspect autoscaler events with `kubectl describe` to see what
    the autoscaler has done (only the most important information is shown in the following
    listing).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，你可以使用`kubectl describe`来检查自动扩展器的事件，以查看自动扩展器做了什么（以下列表中只显示了最重要的信息）。
- en: Listing 15.5\. Events of a HorizontalPodAutoscaler
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.5. 水平Pod自动扩展器的事件
- en: '`From    Reason              Message ----    ------              ------- h-p-a  
    SuccessfulRescale   New size: 1; reason: All metrics below target h-p-a   SuccessfulRescale  
    New size: 4; reason: cpu resource utilization                             (percentage
    of request) above target`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`From    Reason              Message ----    ------              ------- h-p-a  
    SuccessfulRescale   New size: 1; reason: All metrics below target h-p-a   SuccessfulRescale  
    New size: 4; reason: cpu resource utilization (percentage of request) above target`'
- en: Does it strike you as odd that the initial average CPU utilization in my case,
    when I only had one pod, was 108%, which is more than 100%? Remember, a container’s
    CPU utilization is the container’s actual CPU usage divided by its requested CPU.
    The requested CPU defines the minimum, not maximum amount of CPU available to
    the container, so a container may consume more than the requested CPU, bringing
    the percentage over 100\.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否觉得奇怪，在我只有一个Pod的情况下，初始的平均CPU利用率竟然达到了108%，这超过了100%？记住，容器的CPU利用率是容器的实际CPU使用量除以其请求的CPU。请求的CPU定义了容器可用的CPU的最小量，而不是最大量，因此容器可能消耗的CPU超过请求量，导致百分比超过100%。
- en: Before we go on, let’s do a little math and see how the autoscaler concluded
    that four replicas are needed. Initially, there was one replica handling requests
    and its CPU usage spiked to 108%. Dividing 108 by 30 (the target CPU utilization
    percentage) gives 3.6, which the autoscaler then rounded up to 4\. If you divide
    108 by 4, you get 27%. If the autoscaler scales up to four pods, their average
    CPU utilization is expected to be somewhere in the neighborhood of 27%, which
    is close to the target value of 30% and almost exactly what the observed CPU utilization
    was.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们做一点数学计算，看看自动扩展器是如何得出需要四个副本的结论的。最初，有一个副本处理请求，其CPU利用率激增到108%。将108除以30（目标CPU利用率百分比）得到3.6，然后自动扩展器将其四舍五入到4。如果你将108除以4，你得到27%。如果自动扩展器扩展到四个Pod，它们的平均CPU利用率预计将在27%左右，这接近于30%的目标值，并且几乎与观察到的CPU利用率完全一致。
- en: Understanding the maximum rate of scaling
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 理解最大扩展速率
- en: In my case, the CPU usage shot up to 108%, but in general, the initial CPU usage
    could spike even higher. Even if the initial average CPU utilization was higher
    (say 150%), requiring five replicas to achieve the 30% target, the autoscaler
    would still only scale up to four pods in the first step, because it has a limit
    on how many replicas can be added in a single scale-up operation. The autoscaler
    will at most double the number of replicas in a single operation, if more than
    two current replicas exist. If only one or two exist, it will scale up to a maximum
    of four replicas in a single step.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，CPU使用率飙升至108%，但一般来说，初始CPU使用率可能会更高。即使初始平均CPU利用率更高（比如说150%），需要五个副本才能达到30%的目标，自动扩展器在第一步中仍然只会扩展到四个Pod，因为它对单次扩展操作中可以添加的副本数量有限制。如果当前副本数量超过两个，自动扩展器在单次操作中最多只会将副本数量翻倍。如果只有一到两个副本，它将单步扩展到最多四个副本。
- en: Additionally, it has a limit on how soon a subsequent autoscale operation can
    occur after the previous one. Currently, a scale-up will occur only if no rescaling
    event occurred in the last three minutes. A scale-down event is performed even
    less frequently—every five minutes. Keep this in mind so you don’t wonder why
    the autoscaler refuses to perform a rescale operation even if the metrics clearly
    show that it should.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它还对后续自动扩展操作可以在前一个操作之后多快发生有限制。目前，只有在最后三分钟内没有发生重新缩放事件时，才会发生扩展。缩放事件发生的频率更低——每五分钟一次。请记住这一点，以免您不明白为什么即使指标清楚地显示应该进行缩放操作，自动扩展器也拒绝执行缩放操作。
- en: Modifying the target metric value on an existing HPA object
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在现有的HPA对象上修改目标度量值
- en: To wrap up this section, let’s do one last exercise. Maybe your initial CPU
    utilization target of 30% was a bit too low, so increase it to 60%. You do this
    by editing the HPA resource with the `kubectl edit` command. When the text editor
    opens, change the `targetAverageUtilization` field to `60`, as shown in the following
    listing.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结本节内容，让我们进行最后一个练习。也许您最初的30% CPU利用率目标有点低，所以将其提高到60%。您可以通过使用`kubectl edit`命令编辑HPA资源来完成此操作。当文本编辑器打开时，将`targetAverageUtilization`字段更改为`60`，如下所示。
- en: Listing 15.6\. Increasing the target CPU utilization by editing the HPA resource
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.6\. 通过编辑HPA资源提高目标CPU利用率
- en: '`... spec:   maxReplicas: 5   metrics:   - resource:       name: cpu      
    targetAverageUtilization: 60` `1` `type: Resource ...`'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`... spec:    maxReplicas: 5    metrics:    - resource:        name: cpu        targetAverageUtilization:
    60` `1` `type: Resource ...`'
- en: 1 Change this from 30 to 60\.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 将此从30更改为60。
- en: As with most other resources, after you modify the resource, your changes will
    be detected by the autoscaler controller and acted upon. You could also delete
    the resource and recreate it with different target values, because by deleting
    the HPA resource, you only disable autoscaling of the target resource (a Deployment
    in this case) and leave it at the scale it is at that time. The automatic scaling
    will resume after you create a new HPA resource for the Deployment.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数其他资源一样，修改资源后，自动扩展器控制器会检测到您的更改并采取行动。您也可以删除资源，并使用不同的目标值重新创建它，因为通过删除HPA资源，您只是禁用了目标资源（在这种情况下是Deployment）的自动扩展，并使其保持在当时的规模。创建新的HPA资源后，自动扩展将重新启动。
- en: 15.1.3\. Scaling based on memory consumption
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 15.1.3\. 基于内存消耗进行缩放
- en: You’ve seen how easily the horizontal Autoscaler can be configured to keep CPU
    utilization at the target level. But what about autoscaling based on the pods’
    memory usage?
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到了如何轻松地配置水平自动扩展器以保持CPU利用率在目标水平。但基于Pod内存使用量的自动扩展又如何呢？
- en: Memory-based autoscaling is much more problematic than CPU-based autoscaling.
    The main reason is because after scaling up, the old pods would somehow need to
    be forced to release memory. This needs to be done by the app itself—it can’t
    be done by the system. All the system could do is kill and restart the app, hoping
    it would use less memory than before. But if the app then uses the same amount
    as before, the Autoscaler would scale it up again. And again, and again, until
    it reaches the maximum number of pods configured on the HPA resource. Obviously,
    this isn’t what anyone wants. Memory-based autoscaling was introduced in Kubernetes
    version 1.8, and is configured exactly like CPU-based autoscaling. Exploring it
    is left up to the reader.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 基于内存的自动扩展比基于CPU的自动扩展问题更多。主要原因是在扩展之后，旧的Pod需要以某种方式被迫释放内存。这需要由应用程序本身完成——系统无法完成。系统所能做的就是杀死并重新启动应用程序，希望它使用的内存比之前少。但如果应用程序随后使用与之前相同的内存量，自动扩展器会再次将其扩展。一次又一次，直到达到HPA资源上配置的最大Pod数量。显然，这不是任何人想要的。基于内存的自动扩展是在Kubernetes版本1.8中引入的，其配置方式与基于CPU的自动扩展完全相同。探索它留给读者自行完成。
- en: 15.1.4\. Scaling based on other and custom metrics
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 15.1.4\. 基于其他和自定义指标的扩展
- en: You’ve seen how easy it is to scale pods based on their CPU usage. Initially,
    this was the only autoscaling option that was usable in practice. To have the
    autoscaler use custom, app-defined metrics to drive its autoscaling decisions
    was fairly complicated. The initial design of the autoscaler didn’t make it easy
    to move beyond simple CPU-based scaling. This prompted the Kubernetes Autoscaling
    Special Interest Group (SIG) to redesign the autoscaler completely.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经看到，根据CPU使用情况扩展Pod是多么容易。最初，这是唯一在实践中可用的自动扩展选项。要让自动扩展器使用自定义的应用定义指标来驱动其自动扩展决策相当复杂。自动扩展器的初始设计并没有使其容易超越简单的基于CPU的扩展。这促使Kubernetes自动扩展特别兴趣小组（SIG）完全重新设计自动扩展器。
- en: If you’re interested in learning how complicated it was to use the initial autoscaler
    with custom metrics, I invite you to read my blog post entitled “Kubernetes autoscaling
    based on custom metrics without using a host port,” which you’ll find online at
    [http://medium.com/@marko.luksa](http://medium.com/@marko.luksa). You’ll learn
    about all the other problems I encountered when trying to set up autoscaling based
    on custom metrics. Luckily, newer versions of Kubernetes don’t have those problems.
    I’ll cover the subject in a new blog post.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对使用初始自动扩展器与自定义指标有多么复杂感兴趣，我邀请您阅读我的一篇博客文章，标题为“不使用主机端口进行基于自定义指标的Kubernetes自动扩展”，您可以在网上找到[http://medium.com/@marko.luksa](http://medium.com/@marko.luksa)。您将了解我在尝试根据自定义指标设置自动扩展时遇到的所有其他问题。幸运的是，Kubernetes的新版本没有这些问题。我将在一篇新的博客文章中介绍这个主题。
- en: Instead of going through a complete example here, let’s quickly go over how
    to configure the autoscaler to use different metrics sources. We’ll start by examining
    how we defined what metric to use in our previous example. The following listing
    shows how your previous HPA object was configured to use the CPU usage metric.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里不通过一个完整的示例，让我们快速了解一下如何配置自动扩展器以使用不同的指标源。我们将首先检查我们在上一个示例中定义了什么指标。以下列表显示了您的上一个HPA对象是如何配置为使用CPU使用率指标的。
- en: Listing 15.7\. HorizontalPodAutoscaler definition for CPU-based autoscaling
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.7\. 基于CPU的自动扩展的HorizontalPodAutoscaler定义
- en: '`... spec:   maxReplicas: 5   metrics:   - type: Resource` `1` `resource:      
    name: cpu` `2` `targetAverageUtilization: 30` `3` `...`'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`... spec:    maxReplicas: 5    metrics:    - type: Resource` `1` `resource:    name:
    cpu` `2` `targetAverageUtilization: 30` `3` `...`'
- en: 1 Defines the type of metric
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 定义指标类型
- en: 2 The resource, whose utilization will be monitored
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 将要监控的资源利用率
- en: 3 The target utilization of this resource
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 该资源的目标利用率
- en: 'As you can see, the `metrics` field allows you to define more than one metric
    to use. In the listing, you’re using a single metric. Each entry defines the `type`
    of metric—in this case, a `Resource` metric. You have three types of metrics you
    can use in an HPA object:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`metrics`字段允许您定义多个要使用的指标。在列表中，您正在使用单个指标。每个条目定义了指标的`type`——在这种情况下，是一个`Resource`指标。在HPA对象中，您可以使用三种类型的指标：
- en: '`Resource`'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Resource`'
- en: '`Pods`'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pods`'
- en: '`Object`'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Object`'
- en: Understanding the Resource metric type
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 理解资源指标类型
- en: The `Resource` type makes the autoscaler base its autoscaling decisions on a
    resource metric, like the ones specified in a container’s resource requests. We’ve
    already seen how to do that, so let’s focus on the other two types.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`Resource`类型使自动扩展器基于资源指标（如容器资源请求中指定的指标）进行自动扩展决策。我们已经看到了如何做到这一点，所以让我们关注其他两种类型。'
- en: Understanding the Pods metric type
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 理解Pods指标类型
- en: The `Pods` type is used to refer to any other (including custom) metric related
    to the pod directly. An example of such a metric could be the already mentioned
    Queries-Per-Second (QPS) or the number of messages in a message broker’s queue
    (when the message broker is running as a pod). To configure the autoscaler to
    use the pod’s QPS metric, the HPA object would need to include the entry shown
    in the following listing under its `metrics` field.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pods`类型用于引用与Pod直接相关的任何其他（包括自定义）指标。此类指标的一个例子可以是已经提到的每秒查询数（QPS）或消息代理队列中的消息数量（当消息代理作为Pod运行时）。要配置自动扩展器使用Pod的QPS指标，HPA对象需要在它的`metrics`字段下包含以下列表中显示的条目。'
- en: Listing 15.8\. Referring to a custom pod metric in the HPA
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.8.在HPA中引用自定义Pod指标
- en: '`... spec:   metrics:   - type: Pods` `1` `resource:       metricName: qps`
    `2` `targetAverageValue: 100` `3` `...`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`... spec: metrics: - type: Pods` `1` `resource: metricName: qps` `2` `targetAverageValue:
    100` `3` `...`'
- en: 1 Defines a pod metric
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 定义Pod指标
- en: 2 The name of the metric
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 指标的名称
- en: 3 The target average value across all targeted pods
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 在所有目标Pod中的目标平均值
- en: The example in the listing configures the autoscaler to keep the average QPS
    of all the pods managed by the ReplicaSet (or other) controller targeted by this
    HPA resource at `100`.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中给出的示例配置了自动扩展器，使其保持由该HPA资源针对的ReplicaSet（或其他）控制器管理的所有Pod的平均QPS为`100`。
- en: Understanding the Object metric type
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 理解Object指标类型
- en: The `Object` metric type is used when you want to make the autoscaler scale
    pods based on a metric that doesn’t pertain directly to those pods. For example,
    you may want to scale pods according to a metric of another cluster object, such
    as an Ingress object. The metric could be QPS as in [listing 15.8](#filepos1463077),
    the average request latency, or something else completely.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想根据不直接与这些Pod相关的指标来使自动扩展器扩展Pod时，使用`Object`指标类型。例如，你可能想根据另一个集群对象（如Ingress对象）的指标来扩展Pod。该指标可以是[列表15.8](#filepos1463077)中的QPS，平均请求延迟或其他完全不同的指标。
- en: Unlike in the previous case, where the autoscaler needed to obtain the metric
    for all targeted pods and then use the average of those values, when you use an
    `Object` metric type, the autoscaler obtains a single metric from the single object.
    In the HPA definition, you need to specify the target object and the target value.
    The following listing shows an example.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一个案例不同，在前一个案例中，自动扩展器需要获取所有目标Pod的指标，然后使用这些值的平均值，当你使用`Object`指标类型时，自动扩展器从单个对象获取单个指标。在HPA定义中，你需要指定目标对象和目标值。以下列表显示了一个示例。
- en: Listing 15.9\. Referring to a metric of a different object in the HPA
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.9.在HPA中引用不同对象的指标
- en: '`... spec:   metrics:   - type: Object` `1` `resource:       metricName: latencyMillis`
    `2` `target:         apiVersion: extensions/v1beta1` `3` `kind: Ingress` `3` `name:
    frontend` `3` `targetValue: 20` `4` `scaleTargetRef:` `5` `apiVersion: extensions/v1beta1`
    `5` `kind: Deployment` `5` `name: kubia` `5` `...`'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`... spec: metrics: - type: Object` `1` `resource: metricName: latencyMillis`
    `2` `target: apiVersion: extensions/v1beta1` `3` `kind: Ingress` `3` `name: frontend`
    `3` `targetValue: 20` `4` `scaleTargetRef:` `5` `apiVersion: extensions/v1beta1`
    `5` `kind: Deployment` `5` `name: kubia` `5` `...`'
- en: 1 Use metric of a specific object
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 使用特定对象的指标
- en: 2 The name of the metric
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 指标的名称
- en: 3 The specific object whose metric the autoscaler should obtain
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 自动扩展器应获取其指标的特定对象
- en: 4 The Autoscaler should scale so the value of the metric stays close to this.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4 自动扩展器应该扩展，使指标的值保持接近这个值。
- en: 5 The scalable resource the autoscaler will scale
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 5 自动扩展器将扩展的可伸缩资源
- en: In this example, the HPA is configured to use the `latencyMillis` metric of
    the `frontend` Ingress object. The target value for the metric is `20`. The horizontal
    pod autoscaler will monitor the Ingress’ metric and if it rises too far above
    the target value, the autoscaler will scale the `kubia` Deployment resource.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，HPA被配置为使用`frontend` Ingress对象的`latencyMillis`指标。该指标的的目标值是`20`。水平Pod自动扩展器将监控Ingress的指标，如果它上升得太远高于目标值，自动扩展器将扩展`kubia`
    Deployment资源。
- en: 15.1.5\. Determining which metrics are appropriate for autoscaling
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 15.1.5.确定哪些指标适合自动扩展
- en: You need to understand that not all metrics are appropriate for use as the basis
    of autoscaling. As mentioned previously, the pods’ containers’ memory consumption
    isn’t a good metric for autoscaling. The autoscaler won’t function properly if
    increasing the number of replicas doesn’t result in a linear decrease of the average
    value of the observed metric (or at least close to linear).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要理解并非所有指标都适合用作自动扩展的基础。如前所述，Pod容器的内存消耗不是自动扩展的好指标。如果增加副本数不会导致观察到的指标的平均值线性减少（或者至少接近线性），自动扩展器将无法正常工作。
- en: For example, if you have only a single pod instance and the value of the metric
    is X and the autoscaler scales up to two replicas, the metric needs to fall to
    somewhere close to X/2\. An example of such a custom metric is Queries per Second
    (QPS), which in the case of web applications reports the number of requests the
    application is receiving per second. Increasing the number of replicas will always
    result in a proportionate decrease of QPS, because a greater number of pods will
    be handling the same total number of requests.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你只有一个Pod实例，并且指标的值为X，自动扩展器将副本数扩展到两个，那么指标值需要下降到大约X/2。这种自定义指标的例子是每秒查询数（QPS），在Web应用程序的情况下，它报告每秒应用程序接收到的请求数量。增加副本数将始终导致QPS成比例下降，因为更多的Pod将处理相同数量的总请求。
- en: Before you decide to base the autoscaler on your app’s own custom metric, be
    sure to think about how its value will behave when the number of pods increases
    or decreases.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在你决定基于你应用程序自己的自定义指标设置自动扩展器之前，请务必考虑当Pod数量增加或减少时其值的行为。
- en: 15.1.6\. Scaling down to zero replicas
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 15.1.6. 缩放到零副本
- en: The horizontal pod autoscaler currently doesn’t allow setting the `minReplicas`
    field to 0, so the autoscaler will never scale down to zero, even if the pods
    aren’t doing anything. Allowing the number of pods to be scaled down to zero can
    dramatically increase the utilization of your hardware. When you run services
    that get requests only once every few hours or even days, it doesn’t make sense
    to have them running all the time, eating up resources that could be used by other
    pods. But you still want to have those services available immediately when a client
    request comes in.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 水平Pod自动扩展器目前不允许将`minReplicas`字段设置为0，因此自动扩展器永远不会缩放到零，即使Pod没有做任何事情。允许Pod的数量缩放到零可以显著提高硬件的利用率。当你运行每几小时或甚至每天只接收一次请求的服务时，让它们一直运行，消耗其他Pod可能使用的资源是没有意义的。但当你需要这些服务在客户端请求到来时立即可用时，你仍然希望它们存在。
- en: This is known as idling and un-idling. It allows pods that provide a certain
    service to be scaled down to zero. When a new request comes in, the request is
    blocked until the pod is brought up and then the request is finally forwarded
    to the pod.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为空闲和取消空闲。它允许提供特定服务的Pod被缩放到零。当新的请求到来时，请求将被阻塞，直到Pod启动，然后请求最终被转发到Pod。
- en: Kubernetes currently doesn’t provide this feature yet, but it will eventually.
    Check the documentation to see if idling has been implemented yet.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes目前还没有提供这个功能，但最终会提供。请检查文档以查看是否已实现空闲状态。
- en: 15.2\. Vertical pod autoscaling
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 15.2. 垂直Pod自动扩展
- en: Horizontal scaling is great, but not every application can be scaled horizontally.
    For such applications, the only option is to scale them vertically—give them more
    CPU and/or memory. Because a node usually has more resources than a single pod
    requests, it should almost always be possible to scale a pod vertically, right?
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 水平扩展很棒，但并非每个应用程序都可以水平扩展。对于这类应用程序，唯一的选项是将它们垂直扩展——给它们更多的CPU和/或内存。因为节点通常拥有的资源比单个Pod请求的资源多，所以几乎总是可以垂直扩展Pod，对吧？
- en: Because a pod’s resource requests are configured through fields in the pod manifest,
    vertically scaling a pod would be performed by changing those fields. I say “would”
    because it’s currently not possible to change either resource requests or limits
    of existing pods. Before I started writing the book (well over a year ago), I
    was sure that by the time I wrote this chapter, Kubernetes would already support
    proper vertical pod autoscaling, so I included it in my proposal for the table
    of contents. Sadly, what seems like a lifetime later, vertical pod autoscaling
    is still not available yet.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一个Pod的资源请求是通过Pod清单中的字段配置的，因此垂直扩展Pod将通过更改这些字段来实现。我说“将”是因为目前无法更改现有Pod的资源请求或限制。在我开始写这本书（超过一年前）的时候，我确信在我写这一章的时候，Kubernetes已经支持了适当的垂直Pod自动扩展，所以我将其包含在我的目录提案中。遗憾的是，似乎是一生的时间之后，垂直Pod自动扩展仍然还没有实现。
- en: 15.2.1\. Automatically configuring resource requests
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 15.2.1. 自动配置资源请求
- en: An experimental feature sets the CPU and memory requests on newly created pods,
    if their containers don’t have them set explicitly. The feature is provided by
    an Admission Control plugin called InitialResources. When a new pod without resource
    requests is created, the plugin looks at historical resource usage data of the
    pod’s containers (per the underlying container image and tag) and sets the requests
    accordingly.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一个实验性功能为新创建的Pod设置了CPU和内存请求，如果它们的容器没有明确设置这些值。这个功能由一个名为InitialResources的准入控制插件提供。当创建一个没有资源请求的新Pod时，该插件会查看Pod容器的历史资源使用数据（根据底层容器镜像和标签），并相应地设置请求。
- en: You can deploy pods without specifying resource requests and rely on Kubernetes
    to eventually figure out what each container’s resource needs are. Effectively,
    Kubernetes is vertically scaling the pod. For example, if a container keeps running
    out of memory, the next time a pod with that container image is created, its resource
    request for memory will be set higher automatically.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以部署Pod而不指定资源请求，并依赖Kubernetes最终确定每个容器的资源需求。实际上，Kubernetes正在垂直扩展Pod。例如，如果一个容器不断耗尽内存，那么下次创建具有该容器镜像的Pod时，其内存资源请求将自动设置得更高。
- en: 15.2.2\. Modifying resource requests while a pod is running
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 15.2.2. 在Pod运行时修改资源请求
- en: Eventually, the same mechanism will be used to modify an existing pod’s resource
    requests, which means it will vertically scale the pod while it’s running. As
    I’m writing this, a new vertical pod autoscaling proposal is being finalized.
    Please refer to the Kubernetes documentation to find out whether vertical pod
    autoscaling is already implemented or not.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，将使用相同的机制来修改现有Pod的资源请求，这意味着它将在Pod运行时垂直扩展Pod。在我写这段话的时候，一个新的垂直Pod自动扩展提案正在最终确定。请参考Kubernetes文档以了解垂直Pod自动扩展是否已经实现。
- en: 15.3\. Horizontal scaling of cluster nodes
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 15.3. 集群节点的水平扩展
- en: The Horizontal Pod Autoscaler creates additional pod instances when the need
    for them arises. But what about when all your nodes are at capacity and can’t
    run any more pods? Obviously, this problem isn’t limited only to when new pod
    instances are created by the Autoscaler. Even when creating pods manually, you
    may encounter the problem where none of the nodes can accept the new pods, because
    the node’s resources are used up by existing pods.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 水平Pod自动扩展器在需要时创建额外的Pod实例。但是，当所有节点都达到容量并且无法运行更多Pod时怎么办？显然，这个问题不仅限于当自动扩展器创建新的Pod实例时。即使手动创建Pod，你也可能遇到没有任何节点可以接受新Pod的问题，因为节点的资源已经被现有的Pod用完。
- en: In that case, you’d need to delete several of those existing pods, scale them
    down vertically, or add additional nodes to your cluster. If your Kubernetes cluster
    is running on premises, you’d need to physically add a new machine and make it
    part of the Kubernetes cluster. But if your cluster is running on a cloud infrastructure,
    adding additional nodes is usually a matter of a few clicks or an API call to
    the cloud infrastructure. This can be done automatically, right?
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在那种情况下，你需要删除一些现有的Pod，垂直缩小它们的规模，或者向你的集群添加额外的节点。如果你的Kubernetes集群在本地运行，你需要物理添加一台新机器并将其作为Kubernetes集群的一部分。但如果你的集群在云基础设施上运行，添加额外的节点通常只需要几点击或对云基础设施的API调用。这可以自动完成，对吧？
- en: Kubernetes includes the feature to automatically request additional nodes from
    the cloud provider as soon as it detects additional nodes are needed. This is
    performed by the Cluster Autoscaler.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 包含了在检测到需要额外的节点时自动从云服务提供商请求额外节点的功能。这是由集群自动扩展器执行的。
- en: 15.3.1\. Introducing the Cluster Autoscaler
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 15.3.1\. 介绍集群自动扩展器
- en: The Cluster Autoscaler takes care of automatically provisioning additional nodes
    when it notices a pod that can’t be scheduled to existing nodes because of a lack
    of resources on those nodes. It also de-provisions nodes when they’re underutilized
    for longer periods of time.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 集群自动扩展器负责在检测到由于节点资源不足而无法将 pod 调度到现有节点时，自动提供额外的节点。当节点长时间未被充分利用时，它也会取消节点分配。
- en: Requesting additional nodes from the cloud infrastructure
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 从云基础设施请求额外的节点
- en: A new node will be provisioned if, after a new pod is created, the Scheduler
    can’t schedule it to any of the existing nodes. The Cluster Autoscaler looks out
    for such pods and asks the cloud provider to start up an additional node. But
    before doing that, it checks whether the new node can even accommodate the pod.
    After all, if that’s not the case, it makes no sense to start up such a node.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在创建新的 pod 之后，调度器无法将其调度到任何现有节点，则会分配一个新的节点。集群自动扩展器会关注这类 pod，并要求云服务提供商启动一个额外的节点。但在这样做之前，它会检查新节点是否能够容纳该
    pod。毕竟，如果不行，启动这样的节点就没有意义了。
- en: Cloud providers usually group nodes into groups (or pools) of same-sized nodes
    (or nodes having the same features). The Cluster Autoscaler thus can’t simply
    say “Give me an additional node.” It needs to also specify the node type.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商通常将节点分组为相同大小（或具有相同功能）的节点组（或池）。因此，集群自动扩展器不能简单地说“给我一个额外的节点。”它还需要指定节点类型。
- en: The Cluster Autoscaler does this by examining the available node groups to see
    if at least one node type would be able to fit the unscheduled pod. If exactly
    one such node group exists, the Autoscaler can increase the size of the node group
    to have the cloud provider add another node to the group. If more than one option
    is available, the Autoscaler must pick the best one. The exact meaning of “best”
    will obviously need to be configurable. In the worst case, it selects a random
    one. A simple overview of how the cluster Autoscaler reacts to an unschedulable
    pod is shown in [figure 15.5](#filepos1475735).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 集群自动扩展器通过检查可用的节点组，以确定是否至少有一种节点类型能够容纳未调度的 pod。如果恰好存在一个这样的节点组，自动扩展器可以增加节点组的大小，让云服务提供商向该组添加另一个节点。如果存在多个选项，自动扩展器必须选择最佳方案。显然，“最佳”的确切含义将需要可配置。在最坏的情况下，它会随机选择一个。集群自动扩展器对不可调度的
    pod 的简单反应概述如图 15.5 所示。[图 15.5](#filepos1475735)。
- en: Figure 15.5\. The Cluster Autoscaler scales up when it finds a pod that can’t
    be scheduled to existing nodes.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.5\. 当集群自动扩展器找到一个无法调度到现有节点的 pod 时，它会进行扩展。
- en: '![](images/00142.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00142.jpg)'
- en: When the new node starts up, the Kubelet on that node contacts the API server
    and registers the node by creating a Node resource. From then on, the node is
    part of the Kubernetes cluster and pods can be scheduled to it.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当新节点启动时，该节点上的 Kubelet 会联系 API 服务器，并通过创建节点资源来注册节点。从那时起，该节点就成为了 Kubernetes 集群的一部分，pod
    可以被调度到该节点。
- en: Simple, right? What about scaling down?
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 简单吗？那缩小规模怎么办？
- en: Relinquishing nodes
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 放弃节点
- en: The Cluster Autoscaler also needs to scale down the number of nodes when they
    aren’t being utilized enough. The Autoscaler does this by monitoring the requested
    CPU and memory on all the nodes. If the CPU and memory requests of all the pods
    running on a given node are below 50%, the node is considered unnecessary.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 当节点未被充分利用时，集群自动扩展器还需要缩小节点数量。自动扩展器通过监控所有节点上请求的 CPU 和内存来完成这项工作。如果给定节点上所有 pod 的
    CPU 和内存请求都低于 50%，则认为该节点是不必要的。
- en: That’s not the only determining factor in deciding whether to bring a node down.
    The Autoscaler also checks to see if any system pods are running (only) on that
    node (apart from those that are run on every node, because they’re deployed by
    a DaemonSet, for example). If a system pod is running on a node, the node won’t
    be relinquished. The same is also true if an unmanaged pod or a pod with local
    storage is running on the node, because that would cause disruption to the service
    the pod is providing. In other words, a node will only be returned to the cloud
    provider if the Cluster Autoscaler knows the pods running on the node will be
    rescheduled to other nodes.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是决定是否关闭节点的唯一决定因素。自动缩放器还会检查是否有任何系统 pod 仅在该节点上运行（除了那些在每个节点上运行的 pod，因为它们是由 DaemonSet
    等部署的）。如果一个节点上运行了系统 pod，则该节点不会被释放。同样，如果一个未管理的 pod 或具有本地存储的 pod 在该节点上运行，因为这会导致 pod
    提供的服务中断，也会出现这种情况。换句话说，只有当集群自动缩放器知道节点上运行的 pod 将被重新调度到其他节点时，节点才会返回到云提供商。
- en: When a node is selected to be shut down, the node is first marked as unschedulable
    and then all the pods running on the node are evicted. Because all those pods
    belong to ReplicaSets or other controllers, their replacements are created and
    scheduled to the remaining nodes (that’s why the node that’s being shut down is
    first marked as unschedulable).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个节点被选中关闭时，该节点首先被标记为不可调度，然后从节点驱逐所有运行的 pod。因为这些 pod 都属于 ReplicaSet 或其他控制器，它们的替代品将被创建并调度到剩余的节点（这就是为什么正在关闭的节点首先被标记为不可调度）。
- en: '|  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Manually cordoning and draining nodes
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 手动隔离和驱逐节点
- en: 'A node can also be marked as unschedulable and drained manually. Without going
    into specifics, this is done with the following `kubectl` commands:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 节点也可以通过以下 `kubectl` 命令手动标记为不可调度并驱逐：
- en: '`kubectl cordon <node>` marks the node as unschedulable (but doesn’t do anything
    with pods running on that node).'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubectl cordon <node>` 将节点标记为不可调度（但对该节点上运行的 pod 不做任何操作）。'
- en: '`kubectl drain <node>` marks the node as unschedulable and then evicts all
    the pods from the node.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubectl drain <node>` 将节点标记为不可调度，然后从节点驱逐所有 pod。'
- en: In both cases, no new pods are scheduled to the node until you uncordon it again
    with `kubectl uncordon <node>`.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，在您再次使用 `kubectl uncordon <node>` 命令解除节点隔离之前，不会将新的 pod 调度到该节点。
- en: '|  |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 15.3.2\. Enabling the Cluster Autoscaler
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 15.3.2\. 启用集群自动缩放
- en: Cluster autoscaling is currently available on
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 集群自动缩放目前可在
- en: Google Kubernetes Engine (GKE)
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌 Kubernetes 引擎 (GKE)
- en: Google Compute Engine (GCE)
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌计算引擎 (GCE)
- en: Amazon Web Services (AWS)
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊网络服务 (AWS)
- en: Microsoft Azure
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微软 Azure
- en: 'How you start the Autoscaler depends on where your Kubernetes cluster is running.
    For your `kubia` cluster running on GKE, you can enable the Cluster Autoscaler
    like this:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 启动自动缩放器的方式取决于您的 Kubernetes 集群运行的位置。对于在 GKE 上运行的 `kubia` 集群，您可以通过以下方式启用集群自动缩放：
- en: '`$ gcloud container clusters update kubia --enable-autoscaling \``--min-nodes=3
    --max-nodes=5`'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ gcloud container clusters update kubia --enable-autoscaling \``--min-nodes=3
    --max-nodes=5`'
- en: 'If your cluster is running on GCE, you need to set three environment variables
    before running `kube-up.sh`:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的集群运行在 GCE 上，在运行 `kube-up.sh` 之前，您需要设置三个环境变量：
- en: '`KUBE_ENABLE_CLUSTER_AUTOSCALER=true`'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KUBE_ENABLE_CLUSTER_AUTOSCALER=true`'
- en: '`KUBE_AUTOSCALER_MIN_NODES=3`'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KUBE_AUTOSCALER_MIN_NODES=3`'
- en: '`KUBE_AUTOSCALER_MAX_NODES=5`'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KUBE_AUTOSCALER_MAX_NODES=5`'
- en: Refer to the Cluster Autoscaler GitHub repo at [https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler)
    for information on how to enable it on other platforms.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 有关在其他平台上启用自动缩放器的信息，请参阅集群自动缩放器 GitHub 仓库：[https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler)。
- en: '|  |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The Cluster Autoscaler publishes its status to the `cluster-autoscaler-status`
    ConfigMap in the `kube-system` namespace.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 集群自动缩放器将其状态发布到 `kube-system` 命名空间中的 `cluster-autoscaler-status` ConfigMap。
- en: '|  |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 15.3.3\. Limiting service disruption during cluster scale-down
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 15.3.3\. 限制集群缩小时服务中断
- en: When a node fails unexpectedly, nothing you can do will prevent its pods from
    becoming unavailable. But when a node is shut down voluntarily, either by the
    Cluster Autoscaler or by a human operator, you can make sure the operation doesn’t
    disrupt the service provided by the pods running on that node through an additional
    feature.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个节点意外失败时，您无法阻止其 pod 变得不可用。但是当一个节点自愿关闭，无论是集群自动缩放器还是由人工操作员关闭，您可以通过一个附加功能确保操作不会中断在该节点上运行的
    pod 提供的服务。
- en: Certain services require that a minimum number of pods always keeps running;
    this is especially true for quorum-based clustered applications. For this reason,
    Kubernetes provides a way of specifying the minimum number of pods that need to
    keep running while performing these types of operations. This is done by creating
    a PodDisruptionBudget resource.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 某些服务要求始终运行一定数量的 Pod；这对于基于法定人数的集群应用程序尤其如此。因此，Kubernetes 提供了一种指定在执行这些类型操作时需要保持运行的最小
    Pod 数量的方法。这是通过创建一个 PodDisruptionBudget 资源来实现的。
- en: Even though the name of the resource sounds complex, it’s one of the simplest
    Kubernetes resources available. It contains only a pod label selector and a number
    specifying the minimum number of pods that must always be available or, starting
    from Kubernetes version 1.7, the maximum number of pods that can be unavailable.
    We’ll look at what a PodDisruptionBudget (PDB) resource manifest looks like, but
    instead of creating it from a YAML file, you’ll create it with `kubectl create
    pod-disruptionbudget` and then obtain and examine the YAML later.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管资源的名称听起来很复杂，但它是最简单的 Kubernetes 资源之一。它仅包含一个 Pod 标签选择器和指定必须始终可用的最小 Pod 数量的数字，或者从
    Kubernetes 版本 1.7 开始，可以不可用的最大 Pod 数量。我们将查看 PodDisruptionBudget (PDB) 资源清单的样子，但您将使用
    `kubectl create pod-disruptionbudget` 创建它，然后稍后获取并检查 YAML。
- en: 'If you want to ensure three instances of your `kubia` pod are always running
    (they have the label `app=kubia`), create the PodDisruptionBudget resource like
    this:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想确保您的 `kubia` Pod 的三个实例始终运行（它们具有标签 `app=kubia`），则可以创建一个类似这样的 PodDisruptionBudget
    资源：
- en: '`$ kubectl create pdb kubia-pdb --selector=app=kubia --min-available=3` `poddisruptionbudget
    "kubia-pdb" created`'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl create pdb kubia-pdb --selector=app=kubia --min-available=3` `poddisruptionbudget
    "kubia-pdb" created`'
- en: Simple, right? Now, retrieve the PDB’s YAML. It’s shown in the next listing.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 简单，对吧？现在，检索 PDB 的 YAML。它将在下一个列表中显示。
- en: Listing 15.10\. A PodDisruptionBudget definition
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.10\. PodDisruptionBudget 定义
- en: '`$ kubectl get pdb kubia-pdb -o yaml` `apiVersion: policy/v1beta1 kind: PodDisruptionBudget
    metadata:   name: kubia-pdb spec:   minAvailable: 3` `1` `selector:` `2` `matchLabels:`
    `2` `app: kubia` `2` `status:   ...`'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get pdb kubia-pdb -o yaml` `apiVersion: policy/v1beta1 kind: PodDisruptionBudget
    metadata:   name: kubia-pdb spec:   minAvailable: 3` `1` `selector:` `2` `matchLabels:`
    `2` `app: kubia` `2` `status:   ...`'
- en: 1 How many pods should always be available
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 应该始终有多少个 Pod 可用
- en: 2 The label selector that determines which pods this budget applies to
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 确定此预算应用于哪些 Pod 的标签选择器
- en: You can also use a percentage instead of an absolute number in the `minAvailable`
    field. For example, you could state that 60% of all pods with the `app=kubia`
    label need to be running at all times.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在 `minAvailable` 字段中使用百分比而不是绝对数字。例如，您可以声明所有带有 `app=kubia` 标签的 Pod 中有 60%
    需要始终运行。
- en: '|  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Starting with Kubernetes 1.7, the PodDisruptionBudget resource also supports
    the `maxUnavailable` field, which you can use instead of `min-Available` if you
    want to block evictions when more than that many pods are unavailable.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Kubernetes 1.7 版本开始，PodDisruptionBudget 资源也支持 `maxUnavailable` 字段，如果您想阻止在更多
    Pod 不可用的情况下驱逐，可以使用它而不是 `minAvailable`。
- en: '|  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: We don’t have much more to say about this resource. As long as it exists, both
    the Cluster Autoscaler and the `kubectl drain` command will adhere to it and will
    never evict a pod with the `app=kubia` label if that would bring the number of
    such pods below three.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对这个资源没有更多要说的。只要它存在，集群自动扩展器和 `kubectl drain` 命令都将遵守它，并且如果驱逐 Pod 会将此类 Pod 的数量降至三个以下，则永远不会驱逐带有
    `app=kubia` 标签的 Pod。
- en: For example, if there were four pods altogether and `minAvailable` was set to
    three as in the example, the pod eviction process would evict pods one by one,
    waiting for the evicted pod to be replaced with a new one by the ReplicaSet controller,
    before evicting another pod.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果有四个 Pod 总共，并且 `minAvailable` 设置为三个，就像示例中那样，Pod 驱逐过程将逐个驱逐 Pod，等待被驱逐的 Pod
    被副本集控制器替换，然后再驱逐另一个 Pod。
- en: 15.4\. Summary
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 15.4\. 摘要
- en: This chapter has shown you how Kubernetes can scale not only your pods, but
    also your nodes. You’ve learned that
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向您展示了 Kubernetes 如何扩展您的 Pod 和节点。您已经了解到
- en: Configuring the automatic horizontal scaling of pods is as easy as creating
    a Horizontal-PodAutoscaler object and pointing it to a Deployment, ReplicaSet,
    or ReplicationController and specifying the target CPU utilization for the pods.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 Pod 的自动水平扩展与创建一个 Horizontal-PodAutoscaler 对象并将它指向 Deployment、ReplicaSet 或
    ReplicationController，并指定 Pod 的目标 CPU 利用率一样简单。
- en: Besides having the Horizontal Pod Autoscaler perform scaling operations based
    on the pods’ CPU utilization, you can also configure it to scale based on your
    own application-provided custom metrics or metrics related to other objects deployed
    in the cluster.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了让水平Pod自动缩放根据Pod的CPU利用率执行缩放操作外，您还可以配置它根据您自己的应用程序提供的自定义指标或与集群中部署的其他对象相关的指标进行缩放。
- en: Vertical pod autoscaling isn’t possible yet.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 垂直Pod自动缩放目前尚不可行。
- en: Even cluster nodes can be scaled automatically if your Kubernetes cluster runs
    on a supported cloud provider.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您的Kubernetes集群运行在受支持的云服务提供商上，甚至集群节点也可以自动缩放。
- en: You can run one-off processes in a pod and have the pod stopped and deleted
    automatically as soon you press CTRL+C by using `kubectl run` with the `-it` and
    `--rm` options.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用带有`-it`和`--rm`选项的`kubectl run`命令在Pod中运行一次性进程，并在您按下CTRL+C后自动停止并删除Pod。
- en: In the next chapter, you’ll explore advanced scheduling features, such as how
    to keep certain pods away from certain nodes and how to schedule pods either close
    together or apart.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将探索高级调度功能，例如如何将某些Pod与某些节点隔离开来，以及如何将Pod调度得既紧密又分散。
