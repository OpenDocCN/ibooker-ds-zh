- en: 'Chapter 6\. Building your first deep neural network: introduction to backpropagation'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6章\. 构建您的第一个深度神经网络：反向传播简介
- en: '**In this chapter**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章内容**'
- en: The streetlight problem
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 街灯问题
- en: Matrices and the matrix relationship
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵和矩阵关系
- en: Full, batch, and stochastic gradient descent
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全局、批量和随机梯度下降
- en: Neural networks learn correlation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络学习相关性
- en: Overfitting
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合
- en: Creating your own correlation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建您自己的相关性
- en: 'Backpropagation: long-distance error attribution'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播：长距离错误归因
- en: Linear versus nonlinear
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性与非线性的比较
- en: The secret to sometimes correlation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时相关性的秘密
- en: Your first deep network
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的第一个深度网络
- en: 'Backpropagation in code: bringing it all together'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码中的反向传播：整合一切
- en: “O Deep Thought computer,” he said, “the task we have designed you to perform
    is this. We want you to tell us...” he paused, “The Answer.”
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “O深思考计算机，”他说，“我们为你设计的任务是这个。我们想让你告诉我们……”他停顿了一下，“答案。”
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Douglas Adams, *The Hitchhiker’s Guide to the Galaxy**'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*道格拉斯·亚当斯，《银河系漫游指南》*'
- en: The streetlight problem
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 街灯问题
- en: This toy problem considers how a network learns entire datasets
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这个玩具问题考虑了网络如何学习整个数据集
- en: Consider yourself approaching a street corner in a foreign country. As you approach,
    you look up and realize that the street light is unfamiliar. How can you know
    when it’s safe to cross the street?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你正在一个外国国家的街角。当你接近时，你抬头发现街灯很陌生。你怎么知道什么时候可以过马路？
- en: '![](Images/f0100-01_alt.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0100-01_alt.jpg)'
- en: 'You can know when it’s safe to cross the street by interpreting the streetlight.
    But in this case, you don’t know how to interpret it. Which light combinations
    indicate when it’s time to *walk*? Which indicate when it’s time to *stop*? To
    solve this problem, you might sit at the street corner for a few minutes observing
    the correlation between each light combination and whether people around you choose
    to walk or stop. You take a seat and record the following pattern:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过解读街灯来判断是否安全过马路。但在这个情况下，你不知道如何解读它。哪些灯光组合表示是时候*走*了？哪些表示是时候*停*了？为了解决这个问题，你可能会坐在街角几分钟，观察每个灯光组合与周围人选择走或停之间的相关性。你坐下并记录以下模式：
- en: '![](Images/f0100-02.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0100-02.jpg)'
- en: 'OK, nobody walked at the first light. At this point you’re thinking, “Wow,
    this pattern could be anything. The left light or the right light could be correlated
    with stopping, or the central light could be correlated with walking.” There’s
    no way to know. Let’s take another datapoint:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，第一个灯没有人走过。这时你可能在想，“哇，这个模式可以是任何东西。左边的灯或右边的灯可能与停止相关，或者中央的灯可能与行走相关。”没有办法知道。让我们再取一个数据点：
- en: '![](Images/f0100-03.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0100-03.jpg)'
- en: 'People walked, so something about this light changed the signal. The only thing
    you know for sure is that the far-right light doesn’t seem to indicate one way
    or another. Perhaps it’s irrelevant. Let’s collect another datapoint:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 人们走了，所以这个灯的某些东西改变了信号。你唯一确定的是最右边的灯似乎没有指示一个方向或另一个方向。也许它无关紧要。让我们收集另一个数据点：
- en: '![](Images/f0101-01.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0101-01.jpg)'
- en: Now you’re getting somewhere. Only the middle light changed this time, and you
    got the opposite pattern. The working hypothesis is that the *middle* light indicates
    when people feel safe to walk. Over the next few minutes, you record the following
    six light patterns, noting when people walk or stop. Do you notice a pattern overall?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你正在取得进展。这次只有中间的灯改变了，你得到了相反的模式。工作假设是中间的灯表示人们感到安全可以走。在接下来的几分钟里，你记录以下六个灯光模式，注意人们是走还是停。你注意到整体上有模式吗？
- en: '![](Images/f0101-02.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0101-02.jpg)'
- en: As hypothesized, there is a *perfect correlation* between the middle (crisscross)
    light and whether it’s safe to walk. You learned this pattern by observing all
    the individual datapoints and *searching for correlation*. This is what you’re
    going to train a neural network to do.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，中间（交叉）灯与是否安全过马路之间存在完美的相关性。你是通过观察所有单个数据点并*寻找相关性*来学习这个模式的。这就是你要训练神经网络去做的。
- en: Preparing the data
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备数据
- en: Neural networks don’t read streetlights
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络不读街灯
- en: In the previous chapters, you learned about supervised algorithms. You learned
    that they can take one dataset and turn it into another. More important, they
    can take a dataset of *what you know* and turn it into a dataset of *what you
    want to know*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你学习了监督算法。你了解到它们可以将一个数据集转换成另一个数据集。更重要的是，它们可以将*你知道的*数据集转换成*你想知道的*数据集。
- en: How do you train a supervised neural network? You present it with two datasets
    and ask it to learn how to transform one into the other. Think back to the streetlight
    problem. Can you identify two datasets? Which one do you always know? Which one
    do you want to know?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如何训练一个监督神经网络？你向它展示两个数据集，并要求它学习如何将一个转换成另一个。回想一下街灯问题。你能识别出两个数据集吗？哪一个你总是知道？哪一个你想要知道？
- en: You do indeed have two datasets. On the one hand, you have six streetlight states.
    On the other hand, you have six observations of whether people walked. These are
    the two datasets.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你确实有两个数据集。一方面，你有六个街灯状态。另一方面，你有六个关于人们是否行走的观察。这些就是这两个数据集。
- en: You can train the neural network to convert from the dataset you *know* to the
    dataset that you *want to know*. In this particular real-world example, you know
    the state of the streetlight at any given time, and you want to know whether it’s
    safe to cross the street.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以训练神经网络将你所知道的数据库转换为你想要知道的数据库。在这个特定的现实世界例子中，你知道任何给定时间的街灯状态，而你想要知道是否安全过马路。
- en: '![](Images/f0102-01.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0102-01.jpg)'
- en: To prepare this data for the neural network, you need to first split it into
    these two groups (what you know and what you want to know). Note that you could
    attempt to go backward if you swapped which dataset was in which group. For some
    problems, this works.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备这些数据供神经网络使用，你首先需要将其分成这两组（你所知道的和你要知道的）。注意，如果你交换了哪个数据集在哪个组中，你可以尝试反向操作。对于某些问题，这可行。
- en: Matrices and the matrix relationship
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩阵及其矩阵关系
- en: Translate the streetlight into math
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将街灯转换为数学
- en: Math doesn’t understand streetlights. As mentioned in the previous section,
    you want to teach a neural network to translate a streetlight pattern into the
    correct stop/walk pattern. The operative word here is *pattern*. What you really
    want to do is mimic the pattern of the streetlight in the form of numbers. Let
    me show you what I mean.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数学不理解街灯。如前所述，你想要教会神经网络将街灯模式转换为正确的停止/行走模式。这里的关键词是*模式*。你真正想要做的是以数字的形式模仿街灯的模式。让我给你展示一下我的意思。
- en: '![](Images/f0103-01.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0103-01.jpg)'
- en: Notice that the pattern of numbers shown here mimics the pattern from the streetlights
    in the form of 1s and 0s. Each light gets a column (three columns total, because
    there are three lights). Notice also that there are six rows representing the
    six different observed streetlights.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这里显示的数字模式模仿了街灯以1和0的形式的模式。每个灯光都有一个列（总共有三列，因为有三个灯光）。还要注意的是，有六行代表六个不同的观察到的街灯。
- en: This structure of 1s and 0s is called a *matrix*. This relationship between
    the rows and columns is common in matrices, especially matrices of data (like
    the streetlights).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这种由1和0组成的结构被称为*矩阵*。行和列之间的关系在矩阵中很常见，尤其是在数据矩阵（如街灯）中。
- en: In data matrices, it’s convention to give each *recorded example* a single *row*.
    It’s also convention to give each *thing being recorded* a single *column*. This
    makes the matrix easy to read.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据矩阵中，惯例是给每个*记录的例子*一个单独的*行*。同样，惯例是给每个*被记录的事物*一个单独的*列*。这使得矩阵易于阅读。
- en: So, a column contains every state in which a thing was recorded. In this case,
    a column contains every on/off state recorded for a particular light. Each row
    contains the simultaneous state of every light at a particular moment in time.
    Again, this is common.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一列包含了一个事物被记录的每一个状态。在这种情况下，一列包含了一个特定灯光的每一个开/关状态的记录。每一行包含了一个特定时间点上每个灯光的同时状态。这同样是常见的。
- en: Good data matrices perfectly mimic the outside world
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 良好的数据矩阵完美地模仿外部世界
- en: 'The data matrix doesn’t have to be all 1s and 0s. What if the streetlights
    were on dimmers and turned on and off at varying degrees of intensity? Perhaps
    the streetlight matrix would look more like this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数据矩阵不必全是1和0。如果街灯是调光开关，并且以不同的强度打开和关闭，会怎样？也许街灯矩阵看起来会更像这样：
- en: '![](Images/f0104-01.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0104-01.jpg)'
- en: Matrix A is perfectly valid. It’s mimicking the patterns that exist in the real
    world (streetlight), so you can ask the computer to interpret them. Would the
    following matrix still be valid?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵A是完全有效的。它模仿了现实世界中（街灯）存在的模式，因此你可以要求计算机解释它们。以下矩阵仍然有效吗？
- en: '![](Images/f0104-02.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0104-02.jpg)'
- en: Matrix (B) *is* valid. It adequately captures the relationships between various
    training examples (rows) and lights (columns). Note that `Matrix A * 10 == Matrix
    B` (`A * 10 == B`). This means these matrices are *scalar multiples* of each other.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 (B) *是* 有效的。它充分捕捉了各种训练示例（行）和灯光（列）之间的关系。注意 `Matrix A * 10 == Matrix B` (`A
    * 10 == B`)。这意味着这些矩阵是 *标量倍数* 的关系。
- en: Matrices A and B both contain the same underlying pattern
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 矩阵 A 和 B 都包含相同的基础模式
- en: The important takeaway is that an *infinite* number of matrices exist that perfectly
    reflect the streetlight patterns in the dataset. Even the one shown next is perfect.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的启示是，存在无数个矩阵可以完美地反映数据集中的路灯模式。甚至下一个展示的也是完美的。
- en: '![](Images/f0105-01.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0105-01.jpg)'
- en: It’s important to recognize that the underlying pattern isn’t the same as the
    matrix. It’s a *property of* the matrix. In fact, it’s a property of all three
    of these matrices (A, B, and C). The pattern is what each of these matrices is
    *expressing*. The pattern also existed in the streetlights.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要认识到，基础模式并不等同于矩阵。它是矩阵的 *属性*。实际上，它是这三个矩阵（A、B 和 C）的属性。模式是这些矩阵所 *表达* 的。模式也存在于路灯中。
- en: This *input data pattern* is what you want the neural network to learn to transform
    into the *output data pattern*. But in order to learn the output data pattern,
    you also need to capture the pattern in the form of a matrix, as shown here.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 *输入数据模式* 是你希望神经网络学习转换成 *输出数据模式* 的。但为了学习输出数据模式，你还需要以矩阵的形式捕捉这种模式，如下所示。
- en: Note that you could reverse the 1s and 0s, and the output matrix would still
    capture the underlying STOP/WALK pattern that’s present in the data. You know
    this because regardless of whether you assign a 1 to WALK or to STOP, you can
    still decode the 1s and 0s into the underlying STOP/WALK pattern.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你可以反转 1 和 0，输出矩阵仍然可以捕捉数据中存在的底层 STOP/WALK 模式。你知道这一点，因为无论你将 1 分配给 WALK 还是 STOP，你仍然可以将
    1 和 0 解码成底层的 STOP/WALK 模式。
- en: The resulting matrix is called a *lossless representation* because you can perfectly
    convert back and forth between your stop/walk notes and the matrix.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的矩阵被称为 *无损表示*，因为你可以完美地在你的停止/行走笔记和矩阵之间进行转换。
- en: '![](Images/f0105-02.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0105-02.jpg)'
- en: Creating a matrix or two in Python
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Python 中创建一个矩阵或两个
- en: Import the matrices into Python
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将矩阵导入 Python
- en: 'You’ve converted the streetlight pattern into a matrix (one with just 1s and
    0s). Now let’s create that matrix (and, more important, its underlying pattern)
    in Python so the neural network can read it. Python’s NumPy library (introduced
    in [chapter 3](kindle_split_011.xhtml#ch03)) was built just for handling matrices.
    Let’s see it in action:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经将路灯模式转换成了一个矩阵（只包含 1 和 0 的矩阵）。现在让我们在 Python 中创建这个矩阵（以及更重要的是，其基础模式），以便神经网络可以读取它。Python
    的 NumPy 库（在第 3 章中介绍）正是为了处理矩阵而构建的。让我们看看它的实际应用：
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you’re a regular Python user, something should be striking in this code.
    A matrix is just a list of lists. It’s an array of arrays. What is NumPy? NumPy
    is really just a fancy wrapper for an array of arrays that provides special, matrix-oriented
    functions. Let’s create a NumPy matrix for the output data, too:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一个常规的 Python 用户，这段代码中应该有一些东西会令你印象深刻。矩阵只是一个列表的列表。它是一个数组的数组。什么是 NumPy？NumPy
    实际上只是一个用于数组的数组的高级包装器，它提供了特殊的、以矩阵为导向的函数。让我们也创建一个 NumPy 矩阵来输出数据：
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: What do you want the neural network to do? Take the `streetlights` matrix and
    learn to transform it into the `walk_vs_stop` matrix. More important, you want
    the neural network to take *any matrix containing the same underlying pattern*
    as `streetlights` and transform it into a matrix that contains the underlying
    pattern of `walk_vs_stop`. More on that later. Let’s start by trying to transform
    `streetlights` into `walk_vs_stop` using a neural network.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你想让神经网络做什么？将 `streetlights` 矩阵学习转换成 `walk_vs_stop` 矩阵。更重要的是，你希望神经网络将 `streetlights`
    具有相同基础模式的任何矩阵转换成一个包含 `walk_vs_stop` 基础模式的矩阵。关于这一点稍后详细说明。让我们先尝试使用神经网络将 `streetlights`
    转换成 `walk_vs_stop`。
- en: '![](Images/f0106-01.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0106-01.jpg)'
- en: Building a neural network
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建神经网络
- en: 'You’ve been learning about neural networks for several chapters now. You have
    a new dataset, and you’re going to create a neural network to solve it. Following
    is some example code to learn the first streetlight pattern. This should look
    familiar:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经学习了几个章节关于神经网络的内容。你有一个新的数据集，你将创建一个神经网络来解决它。以下是一些示例代码，用于学习第一个路灯模式。这应该看起来很熟悉：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***1* [1,0,1]**'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* [1,0,1]**'
- en: '***2* Equals 0 (stop)**'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 等于0（停止）**'
- en: '|  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'This code example may bring back several nuances you learned in [chapter 3](kindle_split_011.xhtml#ch03).
    First, the use of the `dot` function was a way to perform a dot product (weighted
    sum) between two vectors. But not included in [chapter 3](kindle_split_011.xhtml#ch03)
    was the way NumPy matrices can perform elementwise addition and multiplication:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码示例可能会让你回忆起在[第3章](kindle_split_011.xhtml#ch03)中学到的几个细微差别。首先，`dot`函数的使用是一种在两个向量之间执行点积（加权求和）的方法。但[第3章](kindle_split_011.xhtml#ch03)中没有包括NumPy矩阵执行元素级加法和乘法的方法：
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '***1* Elementwise multiplication**'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 元素级乘法**'
- en: '***2* Elementwise addition**'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 元素级加法**'
- en: '***3* Vector-scalar multiplication**'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 向量-标量乘法**'
- en: '***4* Vector-scalar addition**'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 向量-标量加法**'
- en: 'NumPy makes these operations easy. When you put a `+` between two vectors,
    it does what you expect: it adds the two vectors together. Other than these nice
    NumPy operators and the new dataset, the neural network shown here is the same
    as the ones built previously.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy使这些操作变得简单。当你将两个向量之间的`+`号时，它做你期望的事情：将两个向量相加。除了这些不错的NumPy运算符和新的数据集之外，这里显示的神经网络与之前构建的相同。
- en: '|  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Learning the whole dataset
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习整个数据集
- en: The neural network has been learning only one streetlight. Don’t - we want it
    to learn them all?
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络只学习了一个路灯。难道我们不想让它学习所有的路灯吗？
- en: 'So far in this book, you’ve trained neural networks that learned how to model
    a single training example (`input` -> `goal_pred` pair). But now you’re trying
    to build a neural network that tells you whether it’s safe to cross the street.
    You need it to know more than one streetlight. How do you do this? You train it
    on all the streetlights at once:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这本书中，你已经训练了能够学习如何模拟单个训练示例（`输入` -> `目标预测`对）的神经网络。但现在你正在尝试构建一个神经网络，告诉你是否可以过马路。你需要它知道不止一个路灯。你该如何做到这一点？你可以一次性在所有路灯上训练它：
- en: '[PRE4]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '***1* [1,0,1]**'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* [1,0,1]**'
- en: '***2* Equals 0 (stop)**'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 等于0（停止）**'
- en: Full, batch, and stochastic gradient descent
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完整、批量和随机梯度下降
- en: Stochastic gradient descent updates weights one example at a time
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机梯度下降一次更新一个示例的权重
- en: As it turns out, this idea of learning one example at a time is a variant on
    gradient descent called *stochastic gradient descent*, and it’s one of the handful
    of methods that can be used to learn an entire dataset.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这种一次学习一个示例的想法是梯度下降的一个变体，称为*随机梯度下降*，并且它是可以用来学习整个数据集的一小部分方法之一。
- en: How does stochastic gradient descent work? As you saw in the previous example,
    it performs a prediction and weight update for each training example separately.
    In other words, it takes the first streetlight, tries to predict it, calculates
    the `weight_delta`, and updates the weights. Then it moves on to the second streetlight,
    and so on. It iterates through the entire dataset many times until it can find
    a weight configuration that works well for all the training examples.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降是如何工作的？正如你在上一个示例中看到的，它对每个训练示例分别进行预测和权重更新。换句话说，它首先处理第一个路灯，尝试预测它，计算`权重变化量`，并更新权重。然后它继续处理第二个路灯，依此类推。它多次迭代整个数据集，直到找到对所有训练示例都适用的工作权重配置。
- en: (Full) gradient descent updates weights one dataset at a time
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: (Full) 梯度下降一次更新一个数据集的权重
- en: As introduced in [chapter 4](kindle_split_012.xhtml#ch04), another method for
    learning an entire dataset is gradient descent (or *average/full gradient descent*).
    Instead of updating the weights once for each training example, the network calculates
    the average `weight_delta` over the entire dataset, changing the weights only
    each time it computes a full average.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第4章[介绍](kindle_split_012.xhtml#ch04)中所述，学习整个数据集的另一种方法是梯度下降（或*平均/完整梯度下降*）。不是为每个训练示例更新一次权重，网络计算整个数据集上的平均`权重变化量`，只在计算完整平均时改变权重。
- en: Batch gradient descent updates weights after n examples
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 批量梯度下降在n个示例后更新权重
- en: This will be covered in more detail later, but there’s also a third configuration
    that sort of splits the difference between stochastic gradient descent and full
    gradient descent. Instead of updating the weights after just one example or after
    the entire dataset of examples, you choose a *batch size* (typically between 8
    and 256) of examples, after which the weights are updated.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在稍后更详细地介绍，但还有一个第三种配置，它在随机梯度下降和完整梯度下降之间取了一个折中。不是在仅一个示例或整个示例数据集之后更新权重，而是选择一个*批量大小*（通常在8到256之间）的示例，之后更新权重。
- en: We’ll discuss this more later in the book, but for now, recognize that the previous
    example created a neural network that can learn the entire streetlights dataset
    by training on each example, one at a time.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书的后面部分进一步讨论这个问题，但到目前为止，认识到之前的例子创建了一个神经网络，通过逐个训练每个示例，可以学习整个街灯数据集。
- en: Neural networks learn correlation
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络学习相关性
- en: What did the last neural network learn?
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 最后一个神经网络学到了什么？
- en: You just got done training a single-layer neural network to take a streetlight
    pattern and identify whether it was safe to cross the street. Let’s take on the
    neural network’s perspective for a moment. The neural network doesn’t know that
    it was processing streetlight data. All it was trying to do was identify which
    input (of the three possible) correlated with the output. It correctly identified
    the middle light by analyzing the final weight positions of the network.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚完成了一个单层神经网络的训练，用于识别街灯模式并判断是否安全过马路。让我们暂时从神经网络的视角来看一下。神经网络并不知道它正在处理街灯数据。它所试图做的只是识别哪个输入（在三个可能的输入中）与输出相关。通过分析网络的最终权重位置，它正确地识别了中间的灯。
- en: '![](Images/f0110-01.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0110-01.jpg)'
- en: 'Notice that the middle weight is very near 1, whereas the far-left and far-right
    weights are very near 0\. At a high level, all the iterative, complex processes
    for learning accomplished something rather simple: the network *identified correlation*
    between the middle input and output. The correlation is located wherever the weights
    were set to high numbers. Inversely, *randomness* with respect to the output was
    found at the far-left and far-right weights (where the weight values are very
    near 0).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 注意中间的权重非常接近1，而左侧和右侧的权重非常接近0。从高层次来看，所有用于学习的迭代、复杂过程实际上完成了一件非常简单的事情：网络*识别了中间输入和输出之间的相关性*。相关性位于权重被设置为高数值的地方。相反，输出方面的*随机性*出现在左侧和右侧的权重（权重值非常接近0）。
- en: How did the network identify correlation? Well, in the process of gradient descent,
    each training example asserts either *up pressure* or *down pressure* on the weights.
    On average, there was more up pressure for the middle weight and more down pressure
    for the other weights. Where does the pressure come from? Why is it different
    for different weights?
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 网络是如何识别相关性的呢？好吧，在梯度下降的过程中，每个训练示例都对权重施加了*上压力*或*下压力*。平均而言，中间权重有更多的上压力，而其他权重有更多的下压力。压力从何而来？为什么不同权重的压力不同？
- en: Up and down pressure
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上下压力
- en: It comes from the data
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它来自数据
- en: Each node is individually trying to correctly predict the output given the input.
    For the most part, each node ignores all the other nodes when attempting to do
    so. The only *cross communication* occurs in that all three weights must share
    the same error measure. The *weight update* is nothing more than taking this shared
    error measure and multiplying it by each respective input.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点都在尝试根据输入正确预测输出。在大多数情况下，每个节点在尝试这样做时都会忽略所有其他节点。唯一的*交叉通信*发生在所有三个权重必须共享相同的误差度量。*权重更新*不过是将这个共享的误差度量乘以每个相应的输入。
- en: Why do you do this? A key part of why neural networks learn is *error attribution*,
    which means given a shared error, the network needs to figure out which weights
    contributed (so they can be adjusted) and which weights did *not* contribute (so
    they can be left alone).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你为什么要这样做？神经网络学习的关键部分是*错误归因*，这意味着给定一个共享的误差，网络需要找出哪些权重做出了贡献（以便进行调整），哪些权重*没有*做出贡献（因此可以保持不变）。
- en: '![](Images/f0111-01_alt.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0111-01_alt.jpg)'
- en: Consider the first training example. Because the middle input is 0, the middle
    weight is *completely irrelevant* for this prediction. No matter what the weight
    is, it’s going to be multiplied by 0 (the input). Thus, any error at that training
    example (regardless of whether it’s too high or too low), can be *attributed*
    to only the far-left and right weights.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑第一个训练示例。因为中间输入是0，所以对于这个预测来说，中间权重*完全无关紧要*。无论权重是多少，它都会乘以0（输入）。因此，无论错误是过高还是过低，都可以*归因于*左侧和右侧的权重。
- en: Consider the pressure of this first training example. If the network should
    predict 0, and two inputs are 1s, then this will cause error, which drives the
    weight values *toward 0*.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑第一个训练示例的压力。如果网络应该预测0，而两个输入是1，这将导致错误，这会驱动权重值*趋向于0*。
- en: The Weight Pressure table helps describe the effect of each training example
    on each respective weight. + indicates that it has pressure toward 1, and – indicates
    that it has pressure toward 0\. Zeros (0) indicate that there is no pressure because
    the input datapoint is 0, so that weight won’t be changed. Notice that the far-left
    weight has two negatives and one positive, so on average the weight will move
    toward 0\. The middle weight has three positives, so on average the weight will
    move toward 1.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 权重压力表有助于描述每个训练示例对每个相应权重的影响。+表示它有向1的压力，-表示它有向0的压力。零（0）表示没有压力，因为输入数据点是0，所以该权重不会改变。注意，最左侧的权重有两个负号和一个正号，所以平均而言，权重将趋向于0。中间的权重有三个正号，所以平均而言，权重将趋向于1。
- en: '![](Images/f0112-01_alt.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0112-01_alt.jpg)'
- en: Each individual weight is attempting to compensate for error. In the first training
    example, there’s *discorrelation* between the far-right and far-left inputs and
    the desired output. This causes those weights to experience down pressure.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单独的权重都试图补偿错误。在第一个训练示例中，最右侧和最左侧的输入与期望输出之间存在*不相关性*。这导致这些权重承受向下压力。
- en: This same phenomenon occurs throughout all six training examples, rewarding
    correlation with pressure toward 1 and penalizing decorrelation with pressure
    toward 0\. On average, this causes the network to find the correlation present
    between the middle weight and the output to be the dominant predictive force (heaviest
    weight in the weighted average of the input), making the network quite accurate.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这种现象在所有六个训练示例中都发生，通过向1的压力奖励相关性，通过向0的压力惩罚不相关性。平均而言，这导致网络找到中间权重和输出之间的相关性成为主要的预测力量（输入加权平均中的最重权重），使网络非常准确。
- en: '|  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Bottom line**'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**底线**'
- en: The prediction is a weighted sum of the inputs. The learning algorithm rewards
    inputs that correlate with the output with upward pressure (toward 1) on their
    weight while penalizing inputs with discorrelation with downward pressure. The
    weighted sum of the inputs find perfect correlation between the input and the
    output by weighting decorrelated inputs to 0.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 预测是输入的加权总和。学习算法通过向上压力（趋向于1）奖励与输出相关的输入，同时通过向下压力惩罚与输出不相关的输入。输入的加权总和通过将不相关的输入加权到0，找到了输入和输出之间的完美相关性。
- en: '|  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The mathematician in you may be cringing a little. Upward pressure and downward
    pressure are hardly precise mathematical expressions, and they have plenty of
    edge cases where this logic doesn’t hold (which we’ll address in a second). But
    you’ll later find that this is an *extremely* valuable approximation, allowing
    you to temporarily overlook all the complexity of gradient descent and just remember
    that *learning rewards correlation* with larger weights (or more generally, *learning
    finds correlation between the two datasets*).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你体内的数学家可能有点不舒服。向上压力和向下压力几乎不是精确的数学表达式，而且有很多边缘情况，这种逻辑不成立（我们将在下一部分讨论）。但你会发现，这是一个*极其*有价值的近似，允许你暂时忽略梯度下降的所有复杂性，只需记住*学习奖励相关性*与更大的权重（或者更普遍地说，*学习在两个数据集之间找到相关性*）。
- en: 'Edge case: Overfitting'
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 边缘情况：过拟合
- en: Sometimes correlation happens accidentally
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 有时候相关性是偶然发生的
- en: Consider again the first example in the training data. What if the far-left
    weight was 0.5 and the far-right weight was –0.5? Their prediction would equal
    0\. The network would predict perfectly. But it hasn’t remotely learned how to
    safely predict streetlights (those weights would fail in the real world). This
    phenomenon is known as *overfitting*.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑训练数据中的第一个示例。如果最左侧的权重是0.5，而最右侧的权重是-0.5怎么办？他们的预测将等于0。网络将完美预测。但它并没有学会如何安全地预测街灯（这些权重在现实世界中会失败）。这种现象被称为*过拟合*。
- en: '|  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Deep learning’s greatest weakness: Overfitting**'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习的最大弱点：过拟合**'
- en: Error is shared among all the weights. If a particular configuration of weights
    *accidentally* creates perfect correlation between the prediction and the output
    dataset (such that `error` == 0) without giving the heaviest weight to the best
    inputs, *the neural network will stop learning*.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 错误在所有权重之间共享。如果特定的权重配置*意外地*在预测和输出数据集之间创建了完美的相关性（即`error`等于0），而没有给予最佳输入最重的权重，*神经网络将停止学习*。
- en: '|  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: If it wasn’t for the other training examples, this fatal flaw would cripple
    the neural network. What do the other training examples do? Well, let’s look at
    the second training example. It bumps the far-right weight upward while not changing
    the far-left weight. This throws off the equilibrium that stopped the learning
    in the first example. As long as you don’t train exclusively on the first example,
    the rest of the training examples will help the network avoid getting stuck in
    these edge-case configurations that exist for any one training example.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有其他训练示例，这个致命的缺陷将使神经网络瘫痪。其他训练示例做了什么？好吧，让我们看看第二个训练示例。它将最右侧权重向上推，同时不改变最左侧权重。这打破了第一个示例中停止学习的平衡。只要你不只训练第一个示例，其余的训练示例将帮助网络避免陷入任何单个训练示例存在的这些边缘情况配置。
- en: This is *very important*. Neural networks are so flexible that they can find
    many, many different weight configurations that will correctly predict for a subset
    of training data. If you trained this neural network on the first two training
    examples, it would likely stop learning at a point where it did *not* work well
    for the other training examples. In essence, it memorized the two training examples
    instead of finding the *correlation* that will *generalize* to any possible streetlight
    configuration.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这*非常重要*。神经网络非常灵活，可以找到许多不同的权重配置，这些配置可以正确预测训练数据的一个子集。如果你在这个神经网络上训练前两个训练示例，它很可能会在它对其他训练示例不起作用的地方停止学习。本质上，它记住了这两个训练示例，而不是找到将*泛化*到任何可能的街灯配置的*相关性*。
- en: If you train on only two streetlights and the network finds just these edge-case
    configurations, it could *fail* to tell you whether it’s safe to cross the street
    when it sees a streetlight that wasn’t in the training data.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只训练在两个街灯上，并且网络只找到这些边缘情况配置，它可能*无法*告诉你当它看到训练数据中没有的街灯时是否安全过马路。
- en: '|  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Key takeaway**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要点**'
- en: The greatest challenge you’ll face with deep learning is convincing your neural
    network to *generalize* instead of just *memorize*. You’ll see this again.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你在深度学习中最面临的挑战是说服你的神经网络*泛化*而不是仅仅*记忆*。你还会再次看到这一点。
- en: '|  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Edge case: Conflicting pressure'
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 边缘情况：冲突的压力
- en: Sometimes correlation fights itself
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 有时相关性会自我斗争
- en: Consider the far-right column in the following Weight Pressure table. What do
    you see?
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下权重压力表的最右侧列。你看到了什么？
- en: This column seems to have an equal number of upward and downward pressure moments.
    But the network correctly pushes this (far-right) weight down to 0, which means
    the downward pressure moments must be larger than the upward ones. How does this
    work?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这一列似乎向上和向下的压力时刻数量相等。但网络正确地将这个（最右侧）权重推至0，这意味着向下的压力时刻必须大于向上的。这是如何工作的？
- en: '![](Images/f0114-01_alt.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0114-01_alt.jpg)'
- en: The left and middle weights have enough signal to converge on their own. The
    left weight falls to 0, and the middle weight moves toward 1\. As the middle weight
    moves higher and higher, the error for positive examples continues to decrease.
    But as they approach their optimal positions, the decorrelation on the far-right
    weight becomes more apparent.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧和中间的权重有足够的信号自行收敛。左侧权重降至0，中间权重向1移动。随着中间权重的不断升高，正例的错误持续减少。但当它们接近最佳位置时，最右侧权重的去相关性变得更加明显。
- en: Let’s consider the extreme example, where the left and middle weights are perfectly
    set to 0 and 1, respectively. What happens to the network? If the right weight
    is above 0, then the network predicts too high; and if the right weight is beneath
    0, the network predicts too low.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个极端的例子，其中左侧和中间权重分别完美地设置为0和1。网络会发生什么？如果右侧权重高于0，那么网络预测过高；如果右侧权重低于0，网络预测过低。
- en: As other nodes learn, they absorb some of the error; they absorb part of the
    correlation. They cause the network to predict with *moderate* correlative power,
    which reduces the error. The other weights then only try to adjust their weights
    to correctly predict what’s left.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 随着其他节点学习，它们吸收了一些错误；它们吸收了部分相关性。它们使网络以*适度*的相关性进行预测，这减少了错误。其他权重随后只尝试调整它们的权重，以正确预测剩余的内容。
- en: In this case, because the middle weight has consistent signal to absorb all
    the correlation (because of the 1:1 relationship between the middle input and
    the output), the error when you want to predict 1 becomes very small, but the
    error to predict 0 becomes large, pushing the middle weight downward.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，因为中间权重有持续的信号来吸收所有的关联性（由于中间输入和输出之间的1:1关系），当你想要预测1时的错误变得非常小，但预测0时的错误变得很大，推动中间权重向下。
- en: It doesn’t always work out like this
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 并非总是能如预期般成功。
- en: In some ways, you kind of got lucky. If the middle node hadn’t been so perfectly
    correlated, the network might have struggled to silence the far-right weight.
    Later you’ll learn about *regularization*, which forces weights with conflicting
    pressure to move toward 0.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些方面，你有点幸运。如果中间节点没有如此完美地相关，网络可能难以沉默最右侧的权重。稍后你将学习到*正则化*，它迫使具有冲突压力的权重移动向0。
- en: As a preview, regularization is advantageous because if a weight has equal pressure
    upward and downward, it isn’t good for anything. It’s not helping either direction.
    In essence, regularization aims to say that only weights with really strong correlation
    can stay on; everything else should be silenced because it’s contributing noise.
    It’s sort of like natural selection, and as a side effect it would cause the neural
    network to train faster (fewer iterations) because the far-right weight has this
    problem of both positive and negative pressure.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 作为预览，正则化是有优势的，因为如果一个权重向上和向下的压力相等，它对任何事情都没有帮助。它对任何方向都没有帮助。本质上，正则化的目的是说，只有具有真正强关联性的权重才能保留；其他所有东西都应该被沉默，因为它们在增加噪声。这有点像自然选择，作为副作用，它会导致神经网络训练更快（迭代次数更少），因为最右侧的权重存在正负压力的问题。
- en: In this case, because the far-right node isn’t definitively correlative, the
    network would immediately start driving it toward 0\. Without regularization (as
    you trained it before), you won’t end up learning that the far-right input is
    useless until after the left and middle start to figure out their patterns. More
    on this later.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，因为最右侧的节点没有明确的关联性，网络会立即开始将其驱动向0。如果没有正则化（就像你之前训练的那样），你不会在学习到最右侧的输入是无用的，直到左侧和中间部分开始找出它们的模式。关于这一点，稍后还会详细说明。
- en: If networks look for correlation between an input column of data and the output
    column, what would the neural network do with the following dataset?
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果网络在数据的一个输入列和输出列之间寻找关联性，那么神经网络会如何处理以下数据集？
- en: '![](Images/f0115-01_alt.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0115-01_alt.jpg)'
- en: There is no correlation between any input column and the output column. Every
    weight has an equal amount of upward pressure and downward pressure. *This dataset
    is a real problem for the neural network.*
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 任何输入列和输出列之间都没有关联性。每个权重都有相等向上的压力和向下的压力。*这个数据集对神经网络来说是一个真正的问题。*
- en: Previously, you could solve for input datapoints that had both upward and downward
    pressure because other nodes would start solving for either the positive or negative
    predictions, drawing the balanced node to favor up or down. But in this case,
    all the inputs are equally balanced between positive and negative pressure. What
    do you do?
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，你可以解决具有向上和向下压力的输入数据点，因为其他节点会开始解决正面的或负面的预测，将平衡节点吸引到向上或向下。但在这个情况下，所有输入在正负压力之间都是平衡的。你该怎么办？
- en: Learning indirect correlation
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习间接关联
- en: If your data doesn’t have correlation, create intermediate data that does!
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如果你的数据没有关联性，就创建具有关联性的中间数据！
- en: Previously, I described a neural network as an instrument that searches for
    correlation between input and output *datasets*. I want to refine this just a
    touch. In reality, neural networks search for correlation between their input
    and output *layers*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，我描述神经网络为一个在输入和输出*数据集*之间寻找关联性的工具。我想稍微细化一下这个描述。实际上，神经网络在它们的输入和输出*层*之间寻找关联性。
- en: You set the values of the input layer to be individual rows of the input data,
    and you try to train the network so that the output layer equals the output dataset.
    Oddly enough, the neural network doesn’t know about data. It just searches for
    correlation between the input and output layers.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你将输入层的值设置为输入数据的单独行，并尝试训练网络，使得输出层等于输出数据集。奇怪的是，神经网络并不了解数据。它只是在输入层和输出层之间寻找关联性。
- en: '![](Images/f0116-01.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0116-01.jpg)'
- en: 'Unfortunately, this is a new streetlights dataset that has *no correlation*
    between the input and output. The solution is simple: use two of these networks.
    The first one will create an intermediate dataset that has limited correlation
    with the output, and the second will use that limited correlation to correctly
    predict the output.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这是一个新的没有输入和输出之间相关性的街灯数据集。解决方案很简单：使用两个这样的网络。第一个将创建一个与输出有有限相关性的中间数据集，第二个将使用这种有限的相关性来正确预测输出。
- en: '|  |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Because the input dataset doesn’t correlate with the output dataset, you’ll
    use the input dataset to create an intermediate dataset that *does* have correlation
    with the output. It’s kind of like cheating.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 因为输入数据集与输出数据集不相关，所以你会使用输入数据集来创建一个中间数据集，这个中间数据集与输出数据集*确实*相关。这有点像作弊。
- en: '|  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Creating correlation
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建相关性
- en: Here’s a picture of the new neural network. You basically stack two neural networks
    on top of each other. The middle layer of nodes (`layer_1`) represents the *intermediate
    dataset*. The goal is to train this network so that even though there’s no correlation
    between the input dataset and output dataset (`layer_0` and `layer_2`), the `layer_1`
    dataset that you create *using `layer_0`* will have correlation with `layer_2`.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一张新神经网络的图片。你基本上是将两个神经网络堆叠在一起。中间层的节点（`layer_1`）代表*中间数据集*。目标是训练这个网络，即使输入数据集（`layer_0`）和输出数据集（`layer_2`）之间没有相关性，你创建的`layer_1`数据集*使用`layer_0`*将与`layer_2`相关。
- en: '![](Images/f0117-01.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0117-01.jpg)'
- en: 'Note: this network is still just a function. It has a bunch of weights that
    are collected together in a particular way. Furthermore, gradient descent still
    works because you can calculate how much each weight contributes to the error
    and adjust it to reduce the error to 0\. And that’s exactly what you’re going
    to do.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这个网络仍然只是一个函数。它有一系列以特定方式收集在一起的权重。此外，梯度下降仍然有效，因为你可以计算出每个权重对误差的贡献，并调整它以将误差减少到0。这正是你将要做的。
- en: 'Stacking neural networks: A review'
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 堆叠神经网络：综述
- en: '[Chapter 3](kindle_split_011.xhtml#ch03) briefly mentioned stacked neural networks.
    Let’s review'
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '[第3章](kindle_split_011.xhtml#ch03)简要提到了堆叠神经网络。让我们回顾一下'
- en: When you look at the following architecture, the prediction occurs exactly as
    you might expect when I say, “Stack neural networks.” The output of the first
    lower network (`layer_0` to `layer_1`) is the input to the second upper neural
    network (`layer_1` to `layer_2`). The prediction for each of these networks is
    identical to what you saw before.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当你看到下面的架构时，预测发生的方式正如我所说的“堆叠神经网络”时你可能预期的那样。第一个较低网络的输出（`layer_0`到`layer_1`）是第二个较高网络的输入（`layer_1`到`layer_2`）。这些网络的预测与之前看到的是相同的。
- en: '![](Images/f0118-01.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0118-01.jpg)'
- en: As you start to think about how this neural network learns, you already know
    a great deal. If you ignore the lower weights and consider their output to be
    the training set, the top half of the neural network (`layer_1` to `layer_2`)
    is just like the networks trained in the preceding chapter. You can use all the
    same learning logic to help them learn.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始思考这个神经网络是如何学习的时候，你已经知道了很多。如果你忽略较低的权重，并认为它们的输出是训练集，那么神经网络的顶部（`layer_1`到`layer_2`）就像之前章节中训练的网络一样。你可以使用所有相同的学习逻辑来帮助它们学习。
- en: The part that you don’t yet understand is how to update the weights between
    `layer_0` and `layer_1`. What do they use as their error measure? As you may remember
    from [chapter 5](kindle_split_013.xhtml#ch05), the cached/normalized error measure
    is called `delta`. In this case, you want to figure out how to know the `delta`
    values at `layer_1` so they can help `layer_2` make accurate predictions.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 你还不理解的部分是如何更新`layer_0`和`layer_1`之间的权重。它们使用什么作为它们的误差度量？你可能还记得[第5章](kindle_split_013.xhtml#ch05)，缓存的/归一化的误差度量被称为`delta`。在这种情况下，你需要弄清楚如何知道`layer_1`的`delta`值，这样它们可以帮助`layer_2`做出准确的预测。
- en: 'Backpropagation: Long-distance error attribution'
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播：长距离误差归因
- en: The weighted average error
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 加权平均误差
- en: What’s the prediction from `layer_1` to `layer_2`? It’s a weighted average of
    the values at `layer_1`. If `layer_2` is too high by *x* amount, how do you know
    which values at `layer_1` contributed to the error? The ones with *higher weights*
    (`weights_1_2`) contributed more. The ones with *lower weights* from `layer_1`
    to `layer_2` contributed less.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `layer_1` 到 `layer_2` 的预测是什么？它是 `layer_1` 上值的加权平均值。如果 `layer_2` 比预期高 *x* 量，你怎么知道哪些
    `layer_1` 上的值导致了错误？那些具有 *更高权重* (`weights_1_2`) 的值贡献更多。那些从 `layer_1` 到 `layer_2`
    的 *较低权重* 的值贡献较少。
- en: Consider the extreme. Let’s say the far-left weight from `layer_1` to `layer_2`
    was zero. How much did that node at `layer_1` cause the network’s error? *Zero*.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个极端情况。假设 `layer_1` 到 `layer_2` 的最左侧权重为零。这个 `layer_1` 节点导致了网络错误的程度是多少？*零*。
- en: It’s so simple it’s almost hilarious. The weights from `layer_1` to `layer_2`
    exactly describe how much each `layer_1` node contributes to the `layer_2` prediction.
    This means those weights also exactly describe how much each `layer_1` node contributes
    to the `layer_2` error.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这太简单了，几乎有点好笑。从 `layer_1` 到 `layer_2` 的权重正好描述了每个 `layer_1` 节点对 `layer_2` 预测的贡献程度。这意味着这些权重也正好描述了每个
    `layer_1` 节点对 `layer_2` 错误的贡献程度。
- en: How do you use the `delta` at `layer_2` to figure out the `delta` at `layer_1`?
    You multiply it by each of the respective weights for `layer_1`. It’s like the
    prediction logic in reverse. This process of moving `delta` signal around is called
    *backpropagation*.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何使用 `layer_2` 上的 `delta` 来确定 `layer_1` 上的 `delta`？你将其乘以 `layer_1` 的相应权重。这就像预测逻辑的反向。这种移动
    `delta` 信号的过程被称为 *反向传播*。
- en: '![](Images/f0119-01_alt.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0119-01_alt.jpg)'
- en: 'Backpropagation: Why does this work?'
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播：为什么这行得通？
- en: The weighted average delta
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 加权平均 delta
- en: In the neural network from [chapter 5](kindle_split_013.xhtml#ch05), the `delta`
    variable told you the direction and amount the value of this node should change
    next time. All backpropagation lets you do is say, “Hey, if you want this node
    to be *x* amount higher, then each of these previous four nodes needs to be `x*weights_1_2`
    amount higher/lower, because these weights were amplifying the prediction by `weights_1_2`
    times.”
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 5 章（[chapter 5](kindle_split_013.xhtml#ch05)）的神经网络中，`delta` 变量告诉你这个节点的值在下一次应该改变的方向和量。所有反向传播能让你做的只是说，“嘿，如果你想使这个节点比现在高
    *x* 量，那么这四个先前的节点每个都需要比现在高/低 `x*weights_1_2` 量，因为这些权重将预测放大了 `weights_1_2` 倍。”
- en: When used in reverse, the `weights_1_2` matrix amplifies the error by the appropriate
    amount. It amplifies the error so you know how much each `layer_1` node should
    move up or down.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当反向使用时，`weights_1_2` 矩阵会按适当的比例放大错误。它放大错误，这样你知道每个 `layer_1` 节点应该向上或向下移动多少。
- en: Once you know this, you can update each weight matrix as you did before. For
    each weight, multiply its output `delta` by its input `value`, and adjust the
    weight by that much (or you can scale it with `alpha`).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你知道这一点，你就可以像之前一样更新每个权重矩阵。对于每个权重，将其输出 `delta` 乘以其输入 `value`，并按这个量调整权重（或者你可以用
    `alpha` 进行缩放）。
- en: '![](Images/f0120-01_alt.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0120-01_alt.jpg)'
- en: Linear vs. nonlinear
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性与非线性
- en: This is probably the hardest concept in the book. Let’s take it slowly
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这可能是书中最难理解的概念。让我们慢慢来
- en: 'I’m going to show you a phenomenon. As it turns out, you need one more piece
    to make this neural network train. Let’s take it from two perspectives. The first
    will show why the neural network can’t train without it. In other words, first
    I’ll show you why the neural network is currently broken. Then, once you add this
    piece, I’ll show you what it does to fix this problem. For now, check out this
    simple algebra:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我将向你展示一个现象。结果证明，你需要再添加一个部件才能使这个神经网络进行训练。让我们从两个角度来分析。第一个角度将展示为什么神经网络没有这个部件就无法训练。换句话说，首先我会向你展示为什么神经网络目前是出问题的。然后，一旦你添加了这个部件，我会向你展示它是如何解决这个问题。现在，先看看这个简单的代数式：
- en: '[PRE5]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here’s the takeaway: for any two multiplications, I can accomplish the same
    thing using a single multiplication. As it turns out, this is bad. Check out the
    following:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是关键点：对于任何两次乘法，我都可以使用一次乘法来完成相同的事情。结果证明，这是不好的。看看下面的例子：
- en: '![](Images/f0121-01.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0121-01.jpg)'
- en: '![](Images/f0121-02.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0121-02.jpg)'
- en: 'These two graphs show two training examples each, one where the input is 1.0
    and another where the input is –1.0\. The bottom line: *for any three-layer network
    you create, there’s a two-layer network that has identical behavior*. Stacking
    two neural nets (as you know them at the moment) doesn’t give you any more power.
    Two consecutive weighted sums is just a more expensive version of one weighted
    sum.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个图显示了两个训练示例，一个输入是1.0，另一个输入是-1.0。底线是：**对于你创建的任何三层网络，都有一个具有相同行为的两层网络**。堆叠两个神经网络（正如你目前所知道的）不会给你带来任何额外的力量。两个连续的加权求和只是单个加权求和的一个更昂贵的版本。
- en: Why the neural network still doesn’t work
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么神经网络仍然不起作用
- en: If you trained the three-layer network as it is now, it wouldn’t converge
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如果你像现在这样训练三层网络，它不会收敛
- en: '|  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Problem:** For any two consecutive weighted sums of the input, there exists
    a single weighted sum with exactly identical behavior. Anything that the three-layer
    network can do, the two-layer network can also do.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：**对于任何两个连续的加权求和的输入，都存在一个具有完全相同行为的单个加权求和。三层网络能做的，两层网络也能做。'
- en: '|  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Let’s talk about the middle layer (`layer_1`) before it’s fixed. Right now,
    each node (out of the four) has a weight coming to it from each of the inputs.
    Let’s think about this from a correlation standpoint. Each node in the middle
    layer subscribes to a certain amount of correlation with each input node. If the
    weight from an input to the middle layer is 1.0, then it subscribes to exactly
    100% of that node’s movement. If that node goes up by 0.3, the middle node will
    follow. If the weight connecting two nodes is 0.5, each node in the middle layer
    subscribes to exactly 50% of that node’s movement.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在修复之前，让我们谈谈中间层（`layer_1`）。现在，每个节点（四个中的每一个）都从每个输入接收一个权重。让我们从相关性的角度来考虑这个问题。中间层中的每个节点都订阅了与每个输入节点一定量的相关性。如果一个输入到中间层的权重是1.0，那么它订阅了该节点运动的100%。如果该节点上升0.3，中间节点将跟随。如果连接两个节点的权重是0.5，中间层中的每个节点都订阅了该节点运动的50%。
- en: The only way the middle node can escape the correlation of one particular input
    node is if it subscribes to additional correlation from another input node. *Nothing
    new* is being contributed to this neural network. Each hidden node subscribes
    to a little correlation from the input nodes.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 中间节点唯一摆脱特定输入节点相关性的方法就是它从另一个输入节点订阅额外的相关性。对这个神经网络**没有任何新的贡献**。每个隐藏节点都从输入节点订阅一小部分相关性。
- en: The middle nodes don’t get to add anything to the conversation; they don’t get
    to have correlation of their own. They’re more or less correlated to various input
    nodes.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 中间节点无法为对话添加任何内容；它们无法拥有自己的相关性。它们与各种输入节点或多或少都有相关性。
- en: '![](Images/f0122-01.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0122-01.jpg)'
- en: But because you *know* that in the new dataset there is no correlation between
    any of the inputs and the output, how can the middle layer help? It mixes up a
    bunch of correlation that’s already useless. What you really need is for the middle
    layer to be able to selectively correlate with the input.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 但因为你**知道**在新数据集中，任何输入与输出之间都没有相关性，那么中间层如何帮助呢？它混合了一堆已经无用的相关性。你真正需要的是中间层能够选择性地与输入相关联。
- en: You want the middle layer to *sometimes* correlate with an input, and *sometimes
    not correlate*. That gives it correlation of its own. This gives the middle layer
    the opportunity to not just always be *x*% correlated to one input and *y*% correlated
    to another input. Instead, it can be *x*% correlated to one input only when it
    wants to be, but other times not be correlated at all. This is called *conditional
    correlation* or *sometimes correlation*.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望中间层**有时**与输入相关联，**有时**不相关。这给了它自己的相关性。这使得中间层有机会不仅仅总是与一个输入保持**x**%的相关性，与另一个输入保持**y**%的相关性。相反，它可以在想要的时候与一个输入保持**x**%的相关性，但在其他时候完全不相关。这被称为**条件相关性**或**有时相关性**。
- en: The secret to sometimes correlation
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有时相关性的秘诀
- en: Turn off the node when the value would be below 0
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 当值低于0时关闭节点
- en: 'This might seem too simple to work, but consider this: if a node’s value dropped
    below 0, normally the node would still have the same correlation to the input
    as always. It would just happen to be negative in value. But if you turn off the
    node (setting it to 0) when it would be negative, then it has *zero correlation
    to any inputs* whenever it’s negative.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来太简单而不起作用，但考虑一下：如果一个节点的值下降到0以下，通常节点仍然会与输入保持相同的相关性。它只是碰巧是负值。但如果你在节点为负时将其关闭（将其设置为0），那么它在与输入相关时总是具有*零相关性*。
- en: What does this mean? The node can now selectively pick and choose when it wants
    to be correlated to something. This allows it to say something like, “Make me
    perfectly correlated to the left input, but only when the right input is turned
    off.” How can it do this? Well, if the weight from the left input is 1.0 and the
    weight from the right input is a huge negative number, then turning on both the
    left and right inputs will cause the node to be 0 all the time. But if only the
    left input is on, the node will take on the value of the left input.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着什么？节点现在可以有选择地挑选和选择它想要与什么相关联的时刻。这允许它说，“当右输入关闭时，让我与左输入完美相关。”它是如何做到这一点的呢？好吧，如果左输入的权重是1.0，而右输入的权重是一个巨大的负数，那么同时打开左输入和右输入将导致节点始终为0。但如果只有左输入打开，节点将采用左输入的值。
- en: This wasn’t possible before. Earlier, the middle node was either always correlated
    to an input or always not correlated. Now it can be conditional. Now it can speak
    for itself.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这在以前是不可能的。以前，中间节点要么总是与输入相关联，要么总是不相关。现在它可以是有条件的。现在它可以为自己发声了。
- en: '|  |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Solution:** By turning off any middle node whenever it would be negative,
    you allow the network to sometimes subscribe to correlation from various inputs.
    This is impossible for two-layer neural networks, thus adding power to three-layer
    nets.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案**：通过在任何中间节点为负时将其关闭，你允许网络有时从各种输入中订阅相关性。这对于两层神经网络是不可能的，因此为三层网络增加了力量。'
- en: '|  |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The fancy term for this “if the node would be negative, set it to 0” logic is
    *nonlinearity*. Without this tweak, the neural network is *linear*. Without this
    technique, the output layer only gets to pick from the same correlation it had
    in the two-layer network. It’s subscribing to pieces of the input layer, which
    means it can’t solve the new streetlights dataset.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这种“如果节点为负，则将其设置为0”的逻辑的术语是*非线性*。没有这种调整，神经网络是*线性的*。没有这种技术，输出层只能从两层网络中已有的相关性中进行选择。它正在订阅输入层的一部分，这意味着它不能解决新的街灯数据集。
- en: There are many kinds of nonlinearities. But the one discussed here is, in many
    cases, the best one to use. It’s also the simplest. (It’s called `relu`.)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性有很多种。但这里讨论的，在许多情况下，是最好的选择。它也是最简单的。（它被称为`relu`。）
- en: For what it’s worth, most other books and courses say that consecutive matrix
    multiplication is a linear transformation. I find this unintuitive. It also makes
    it harder to understand what nonlinearities contribute and why you choose one
    over the other (which we’ll get to later). It says, “Without the nonlinearity,
    two matrix multiplications might as well be 1.” My explanation, although not the
    most concise answer, is an intuitive explanation of why you need nonlinearities.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，大多数其他书籍和课程都说连续的矩阵乘法是线性变换。我觉得这不太直观。这也使得理解非线性如何贡献以及为什么选择一个而不是另一个（我们稍后会讨论）变得更困难。它说，“没有非线性，两次矩阵乘法可能就是1。”我的解释，虽然不是最简洁的答案，但是对为什么需要非线性的一种直观解释。
- en: A quick break
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 快速休息一下
- en: That last part probably felt a little abstract, and that’s totally OK
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 那最后一部分可能感觉有点抽象，这是完全可以接受的
- en: 'Here’s the deal. Previous chapters worked with simple algebra, so everything
    was ultimately grounded in fundamentally simple tools. This chapter started building
    on the premises you learned earlier. Previously, you learned lessons like this:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 事情是这样的。前几章使用的是简单的代数，所以所有东西最终都基于根本简单的工具。这一章开始基于你之前学到的前提。以前，你学到了这样的课程：
- en: '|  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: You can compute the relationship between the error and any one of the weights
    so that you know how changing the weight changes the error. You can then use this
    to reduce the error to 0.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以计算出错误与任何一个权重之间的关系，这样你就知道改变权重是如何影响错误的。然后你可以利用这一点将错误降低到0。
- en: '|  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'That was a *massive lesson*. But now we’re moving past it. Because we already
    worked through why that works, you can take the statement at face value. The next
    big lesson came at the beginning of this chapter:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 那是一个*巨大的教训*。但现在我们正在超越它。因为我们已经解决了为什么它会起作用的原因，所以你可以直接接受这个陈述。下一个重要的教训出现在本章的开头：
- en: '|  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Adjusting the weights to reduce the error over a series of training examples
    ultimately searches for correlation between the input and the output layers. If
    no correlation exists, then the error will never reach 0.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一系列训练示例调整权重以减少误差，最终是在寻找输入层和输出层之间的相关性。如果不存在相关性，则误差永远不会达到0。
- en: '|  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: This is an *even bigger lesson*. It largely means you can put the previous lesson
    out of your mind for now. You don’t need it. Now you’re focused on correlation.
    The takeaway is that you can’t constantly think about everything all at once.
    Take each lesson and let yourself trust it. When it’s a more concise summarization
    (a higher abstraction) of more granular lessons, you can set aside the granular
    and focus on understanding the higher summarizations.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个*更大的教训*。这主要意味着你现在可以暂时忘记之前的教训。你不需要它。现在你专注于相关性。要点是，你不能一次想太多。接受每一个教训，让自己相信它。当它是对更细粒度教训的更简洁总结（更高的抽象）时，你可以放下细粒度，专注于理解更高的总结。
- en: This is akin to a professional swimmer, biker, or similar athlete who requires
    a combined fluid knowledge of a bunch of small lessons. A baseball player who
    swings a bat learned thousands of little lessons to ultimately culminate in a
    great bat swing. But the player doesn’t think of all of them when he goes to the
    plate. His actions are fluid—even subconscious. It’s the same for studying these
    math concepts.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于一个专业的游泳者、骑自行车者或类似的运动员，他们需要结合对许多小课程的综合流畅知识。一个击球手挥棒击球，学习了成千上万的小课程，最终达到一个伟大的挥棒。但当他走到击球区时，他不会想到所有这些。他的动作是流畅的——甚至是无意识的。学习这些数学概念也是如此。
- en: Neural networks look for correlation between input and output, and you no longer
    have to worry about how that happens. You just know it does. Now we’re building
    on that idea. Let yourself relax and trust the things you’ve already learned.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络寻找输入和输出之间的相关性，你不再需要担心它是如何发生的。你只需要知道它确实发生了。现在我们正在建立在这个想法的基础上。让自己放松，相信你已经学到的东西。
- en: Your first deep neural network
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 您的第一个深度神经网络
- en: Here’s how to make the prediction
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这是如何进行预测的方法
- en: The following code initializes the weights and makes a forward propagation.
    New code is **bold**.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码初始化权重并执行前向传播。新代码是**粗体**。
- en: '[PRE6]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '***1* This function sets all negative numbers to 0.**'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 此函数将所有负数设置为0。**'
- en: '***2* Two sets of weights now to connect the three layers (randomly initialized)**'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 现在有两套权重连接三个层（随机初始化）**'
- en: '***3* The output of layer_1 is sent through relu, where negative values become
    0\. This is the input for the next layer, layer_2.**'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 层_1的输出通过relu函数处理，其中负值变为0。这是下一层，层_2的输入。**'
- en: For each piece of the code, follow along with the figure. Input data comes into
    `layer_0`. Via the `dot` function, the signal travels up the weights from `layer_0`
    to `layer_1` (performing a weighted sum at each of the four `layer_1` nodes).
    These weighted sums at `layer_1` are then passed through the `relu` function,
    which converts all negative numbers to 0\. Then a final weighted sum is performed
    into the final node, `layer_2`.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 对于代码的每一部分，请跟随图示。输入数据进入`layer_0`。通过`dot`函数，信号从`layer_0`沿着权重向上传递到`layer_1`（在每个四个`layer_1`节点上执行加权求和）。然后，`layer_1`上的这些加权求和通过`relu`函数传递，该函数将所有负数转换为0。然后对最终节点，`layer_2`执行最终的加权求和。
- en: '![](Images/f0125-01.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0125-01.jpg)'
- en: Backpropagation in code
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码中的反向传播
- en: You can learn the amount that each weight contributes to the final error
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你可以学习每个权重对最终误差的贡献量
- en: At the end of the previous chapter, I made an assertion that it would be important
    to memorize the two-layer neural network code so you could quickly and easily
    recall it when I reference more-advanced concepts. This is when that memorization
    matters.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章的结尾，我提出一个断言，即记住两层神经网络代码将非常重要，这样你就可以在我引用更高级的概念时快速轻松地回忆它。这就是记忆的重要性所在。
- en: The following listing is the new learning code, and it’s essential that you
    recognize and understand the parts addressed in the previous chapters. If you
    get lost, go to [chapter 5](kindle_split_013.xhtml#ch05), memorize the code, and
    then come back. It will make a big difference someday.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表是新的学习代码，并且你一定要识别和理解前几章中提到的部分。如果你迷失了方向，就去 [第 5 章](kindle_split_013.xhtml#ch05)，记住代码，然后回来。这将在某一天产生重大影响。
- en: '[PRE7]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '***1* Returns x if x > 0; returns 0 otherwise**'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 当 x > 0 时返回 x；否则返回 0**'
- en: '***2* Returns 1 for input > 0; returns 0 otherwise**'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 当输入大于 0 时返回 1；否则返回 0**'
- en: '***3* This line computes the delta at layer_1 given the delta at layer_2 by
    taking the layer_2_delta and multiplying it by its connecting weights_1_2.**'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 这一行通过将 layer_2_delta 乘以其连接权重_1_2，根据 layer_2 的 delta 计算出 layer_1 的 delta。**'
- en: Believe it or not, the only truly new code is in bold. Everything else is fundamentally
    the same as in previous pages. The `relu2deriv` function returns 1 when `output`
    > 0; otherwise, it returns 0\. This is the *slope* (the *derivative*) of the `relu`
    function. It serves an important purpose, as you’ll see in a moment.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 信不信由你，唯一真正新的代码是加粗的部分。其他所有内容在本质上都与前面的页面相同。`relu2deriv` 函数在 `output` > 0 时返回 1；否则返回
    0。这是 `relu` 函数的 *斜率*（即 *导数*）。它起着重要的作用，你将在下一刻看到。
- en: Remember, the goal is *error attribution*. It’s about figuring out how much
    each weight contributed to the final error. In the first (two-layer) neural network,
    you calculated a `delta` variable, which told you how much higher or lower you
    wanted the output prediction to be. Look at the code here. You compute the `layer_2_delta`
    in the same way. Nothing new. (Again, go back to [chapter 5](kindle_split_013.xhtml#ch05)
    if you’ve forgotten how that part works.)
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '记住，目标是 *错误归因*。这是关于弄清楚每个权重对最终错误贡献了多少。在第一个（两层）神经网络中，你计算了一个 `delta` 变量，它告诉你希望输出预测变得更高或更低。看看这里的代码。你以相同的方式计算
    `layer_2_delta`。没有什么新的。（再次，如果你忘记了这部分是如何工作的，请回到 [第 5 章](kindle_split_013.xhtml#ch05)。） '
- en: Now that you know how much the final prediction should move up or down (`delta`),
    you need to figure out how much each middle (`layer_1`) node should move up or
    down. These are effectively *intermediate predictions*. Once you have the `delta`
    at `layer_1`, you can use the same processes as before for calculating a weight
    update (for each weight, multiply its input value by its output `delta` and increase
    the `weight` value by that much).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了最终预测应该向上或向下移动多少（`delta`），你需要弄清楚每个中间（`layer_1`）节点应该向上或向下移动多少。这些实际上是 *中间预测*。一旦你有了
    `layer_1` 的 `delta`，你就可以使用之前相同的过程来计算权重更新（对于每个权重，将其输入值乘以其输出 `delta` 并将 `weight`
    值增加这么多）。
- en: 'How do you calculate the `delta`s for `layer_1`? First, do the obvious: multiply
    the output `delta` by each weight attached to it. This gives a weighting of how
    much each weight contributed to that error. There’s one more thing to factor in.
    If `relu` set the output to a `layer_1` node to be 0, then it didn’t contribute
    to the error. When this is true, you should also set the `delta` of that node
    to 0\. Multiplying each `layer_1` node by the `relu2deriv` function accomplishes
    this. `relu2deriv` is either 1 or 0, depending on whether the `layer_1` value
    is greater than 0.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何计算 `layer_1` 的 `delta`？首先，做显而易见的事情：将输出 `delta` 乘以连接到它的每个权重。这给出了每个权重对那个错误的贡献的加权。还有一件事需要考虑。如果
    `relu` 将 `layer_1` 节点的输出设置为 0，那么它没有对错误做出贡献。当这种情况发生时，你应该也将该节点的 `delta` 设置为 0。将每个
    `layer_1` 节点乘以 `relu2deriv` 函数可以实现这一点。`relu2deriv` 要么是 1，要么是 0，这取决于 `layer_1`
    的值是否大于 0。
- en: '![](Images/f0127-01_alt.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0127-01_alt.jpg)'
- en: One iteration of backpropagation
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一次反向传播迭代
- en: '![](Images/f0128-01_alt.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0128-01_alt.jpg)'
- en: '![](Images/f0128-02_alt.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0128-02_alt.jpg)'
- en: '![](Images/f0129-01_alt.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0129-01_alt.jpg)'
- en: '![](Images/f0129-02_alt.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0129-02_alt.jpg)'
- en: As you can see, backpropagation is about calculating `delta`s for intermediate
    layers so you can perform gradient descent. To do so, you take the weighted average
    `delta` on `layer_2` for `layer_1` (weighted by the weights in between them).
    You then turn off (set to 0) nodes that weren’t participating in the forward prediction,
    because they couldn’t have contributed to the error.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，反向传播是关于计算中间层的 `delta`，以便你可以执行梯度下降。要做到这一点，你需要取 `layer_2` 上 `layer_1` 的加权平均
    `delta`（由它们之间的权重加权）。然后关闭（设置为 0）没有参与前向预测的节点，因为它们不可能对错误做出贡献。
- en: Putting it all together
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: Here’s the self-sufficient program you should be able to run (run- ntime output
    follows)
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 下面是你可以运行的自我依赖程序（运行时输出如下）
- en: '[PRE8]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '***1* Returns x if x > 0; returns 0 otherwise**'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 当x大于0时返回x；否则返回0**'
- en: '***2* Returns 1 for input > 0; returns 0 otherwise**'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 当输入大于0时返回1；否则返回0**'
- en: '[PRE9]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Why do deep networks matter?
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度网络为什么重要？
- en: What’s the point of creating “intermediate datasets” that have correlation?
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建具有相关性的“中间数据集”有什么意义？
- en: Consider the cat picture shown here. Consider further that I had a dataset of
    images with cats and without cats (and I labeled them as such). If I wanted to
    train a neural network to take the pixel values and predict whether there’s a
    cat in the picture, the two-layer network might have a problem.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这里显示的猫图片。进一步考虑，我有一个包含有猫和无猫图片的数据集（并且我已将它们标记为这样）。如果我想训练一个神经网络，使其从像素值预测图片中是否有猫，那么两层网络可能存在问题。
- en: Just as in the last streetlight dataset, no individual pixel correlates with
    whether there’s a cat in the picture. Only different configurations of pixels
    correlate with whether there’s a cat.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在最后一个街灯数据集中一样，单个像素与图片中是否有猫无关。只有像素的不同配置与是否有猫相关。
- en: '![](Images/f0131-01.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0131-01.jpg)'
- en: This is the essence of deep learning. Deep learning is all about creating intermediate
    layers (datasets) wherein each node in an intermediate layer represents the presence
    or absence of a different configuration of inputs.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是深度学习的本质。深度学习全部在于创建中间层（数据集），其中每个中间层的节点代表不同输入配置的存在或不存在。
- en: This way, for the cat images dataset, no individual pixel has to correlate with
    whether there’s a cat in the photo. Instead, the middle layer will attempt to
    identify different configurations of pixels that may or may not correlate with
    a cat (such as an ear, or cat eyes, or cat hair). The presence of many cat-like
    configurations will then give the final layer the information (correlation) it
    needs to correctly predict the presence or absence of a cat.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，对于猫图像数据集，不需要单个像素与照片中是否有猫相关联。相反，中间层将尝试识别可能与猫相关联（例如耳朵、猫眼或猫毛）或不相关联的像素配置。许多类似猫的配置的存在将给最终层提供所需的信息（相关性），以便正确预测猫的存在或不存在。
- en: Believe it or not, you can take the three-layer network and continue to stack
    more and more layers. Some neural networks have hundreds of layers, each node
    playing its part in detecting different configurations of input data. The rest
    of this book will be dedicated to studying different phenomena within these layers
    in an effort to explore the full power of deep neural networks.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 信不信由你，你可以使用三层网络并继续堆叠更多的层。一些神经网络有数百层，每个节点都在检测不同输入数据的配置中扮演其角色。本书的其余部分将致力于研究这些层中的不同现象，以探索深度神经网络的全部力量。
- en: 'Toward that end, I must issue the same challenge I did in [chapter 5](kindle_split_013.xhtml#ch05):
    memorize the previous code. You’ll need to be very familiar with each of the operations
    in the code in order for the following chapters to be readable. Don’t progress
    past this point until you can build a three-layer neural network from memory!'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到这个目的，我必须提出与第5章相同的挑战：记住之前的代码。为了使接下来的章节可读，你需要非常熟悉代码中的每个操作。除非你能从记忆中构建一个三层神经网络，否则不要越过这一点！
