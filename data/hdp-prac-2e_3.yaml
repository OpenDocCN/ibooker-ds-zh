- en: Part 4\. Beyond MapReduce
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四部分\. 超越 MapReduce
- en: This part of the book is dedicated to examining languages, tools, and processes
    that make it easier to do your work with Hadoop.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的这一部分致力于检查语言、工具和流程，使使用 Hadoop 的工作更加容易。
- en: '[Chapter 9](kindle_split_022.html#ch09) dives into Hive, a SQL-like domain-specific
    language that’s one of the most accessible interfaces for working with data in
    Hadoop. Impala and Spark SQL are also shown as alternative SQL-processing systems
    on Hadoop; they provide some compelling features, such as increased performance
    over Hive and the ability to intermix SQL with Spark.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[第 9 章](kindle_split_022.html#ch09)深入探讨了 Hive，这是一种类似于 SQL 的特定领域语言，它是与 Hadoop
    中的数据进行交互的最易访问的接口之一。Impala 和 Spark SQL 也被展示为 Hadoop 上的替代 SQL 处理系统；它们提供了一些引人注目的特性，如比
    Hive 更高的性能以及将 SQL 与 Spark 混合使用的能力。'
- en: '[Chapter 10](kindle_split_023.html#ch10), the final chapter, shows you how
    to write a basic YARN application, and goes on to look at key features that will
    be important for your YARN applications.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[第 10 章](kindle_split_023.html#ch10)，最后一章，展示了如何编写基本的 YARN 应用程序，并继续探讨对您的 YARN
    应用程序至关重要的关键特性。'
- en: Chapter 9\. SQL on Hadoop
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 9 章\. Hadoop 上的 SQL
- en: '*This chapter covers*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Learning the Hadoop specifics of Hive, including user-defined functions and
    performance-tuning tips
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习 Hive 的 Hadoop 特性，包括用户定义的函数和性能调优技巧
- en: Learning about Impala and how you can write user-defined functions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 Impala 以及如何编写用户定义的函数
- en: Embedding SQL in your Spark code to intertwine the two languages and play to
    their strengths
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的 Spark 代码中嵌入 SQL 以结合两种语言并发挥它们的优势
- en: Let’s say that it’s nine o’clock in the morning and you’ve been asked to generate
    a report on the top 10 countries that generated visitor traffic over the last
    month. And it needs to be done by noon. Your log data is sitting in HDFS ready
    to be used. Are you going to break out your IDE and start writing Java MapReduce
    code? Not likely. This is where high-level languages such as Hive, Impala, and
    Spark come into play. With their SQL syntax, Hive and Impala allow you to write
    and start executing queries in the same time that it would take you to write your
    `main` method in Java.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 假设现在是上午九点钟，有人要求你生成一份关于上个月产生访问流量的前 10 个国家的报告。并且需要在中午之前完成。你的日志数据已经存放在 HDFS 中，准备使用。你会打破你的
    IDE 并开始编写 Java MapReduce 代码吗？不太可能。这就是高级语言如 Hive、Impala 和 Spark 发挥作用的地方。凭借它们的 SQL
    语法，Hive 和 Impala 允许你在编写 Java `main` 方法所需的时间内编写并开始执行查询。
- en: The big advantage of Hive is that it no longer requires MapReduce to execute
    queries—as of Hive 0.13, Hive can use Tez, which is a general DAG-execution framework
    that doesn’t impose the HDFS and disk barriers between successive steps as MapReduce
    does. Impala and Spark were also built from the ground up to not use MapReduce
    behind the scenes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 的一个主要优势是它不再需要 MapReduce 来执行查询——从 Hive 0.13 版本开始，Hive 可以使用 Tez，这是一个通用的 DAG
    执行框架，它不像 MapReduce 那样在连续步骤之间强加 HDFS 和磁盘障碍。Impala 和 Spark 也从头开始构建，不使用 MapReduce
    作为后台。
- en: These tools are the easiest ways to quickly start working with data in Hadoop.
    Hive and Impala are essentially Hadoop data-warehousing tools that in some organizations
    (such as Facebook) have replaced traditional RDBMS-based data-warehouse tools.
    They owe much of their popularity to the fact that they expose a SQL interface,
    and as such are accessible to those who’ve had some exposure to SQL in the past.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具是快速开始使用 Hadoop 数据的最容易的方式。Hive 和 Impala 实质上是 Hadoop 数据仓库工具，在某些组织（如 Facebook）中已经取代了传统的基于
    RDBMS 的数据仓库工具。它们的大部分流行归功于它们提供了一个 SQL 接口，并且因此对那些过去接触过 SQL 的人来说是可访问的。
- en: We’ll spend most of this chapter focusing on Hive, as it’s currently the most
    adopted SQL-on-Hadoop tool out there. I’ll also introduce Impala as an MPP database
    on Hadoop and a few features unique to Impala. Finally we’ll cover Spark SQL,
    which allows you to use SQL inline with your Spark code, and it could create a
    whole new paradigm for programmers, analysts, and data scientists.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这章的大部分内容中关注 Hive，因为它目前是使用最广泛的 SQL-on-Hadoop 工具。我还会介绍 Impala 作为 Hadoop 上的
    MPP 数据库以及一些 Impala 独有的特性。最后，我们将涵盖 Spark SQL，它允许你在 Spark 代码中内联使用 SQL，这可能会为程序员、分析师和数据科学家创造一个全新的范式。
- en: We’ll start with Hive, which has been the mainstay of SQL-on-Hadoop.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 Hive 开始，它一直是 SQL-on-Hadoop 的主要支柱。
- en: 9.1\. Hive
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1\. Hive
- en: Hive was originally an internal Facebook project that eventually tenured into
    a full-blown Apache project. It was created to simplify access to MapReduce by
    exposing a SQL-based language for data manipulation. The Hive architecture can
    be seen in [figure 9.1](#ch09fig01).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 最初是 Facebook 的一个内部项目，最终发展成为完整的 Apache 项目。它是为了简化对 MapReduce 的访问而创建的，通过公开一个基于
    SQL 的数据操作语言。Hive 架构可以在 [图 9.1](#ch09fig01) 中看到。
- en: Figure 9.1\. The Hive high-level architecture
  id: totrans-16
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.1\. Hive 高级架构
- en: '![](09fig01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig01.jpg)'
- en: In this chapter we’ll look at practical examples of how you can use Hive to
    work with Apache web server logs. We’ll look at different ways you can load and
    arrange data in Hive to optimize how you access that data. We’ll also look at
    some advanced join mechanisms and other relational operations such as grouping
    and sorting. We’ll kick things off with a brief introduction to Hive.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何使用 Hive 处理 Apache 网络服务器日志的实用示例。我们将探讨您可以在 Hive 中加载和安排数据的不同方式，以优化您访问这些数据的方式。我们还将探讨一些高级连接机制和其他关系操作，如分组和排序。我们将从对
    Hive 的简要介绍开始。
- en: '|  |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Learning more about Hive basics
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 学习更多关于 Hive 基础知识
- en: To fully understand Hive fundamentals, refer to Chuck Lam’s *Hadoop in Action*
    (Manning, 2010). In this section we’ll just skim through some Hive basics.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要全面理解 Hive 基础知识，请参阅 Chuck Lam 的 *Hadoop in Action* (Manning, 2010)。在本节中，我们只需简要浏览一些
    Hive 基础知识。
- en: '|  |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 9.1.1\. Hive basics
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.1\. Hive 基础知识
- en: Let’s quickly look at some Hive basics, including recent developments in its
    execution framework.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速了解一下 Hive 的基础知识，包括其执行框架的最新发展。
- en: Installing Hive
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 安装 Hive
- en: The appendix contains installation instructions for Hive. All the examples in
    this book were executed on Hive 0.13, and it’s possible some older Hive versions
    don’t support some of the features we’ll use in this book.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 附录包含 Hive 的安装说明。本书中的所有示例都是在 Hive 0.13 上执行的，并且可能一些较旧的 Hive 版本不支持本书中我们将使用的一些功能。
- en: The Hive metastore
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Hive 元数据仓库
- en: Hive maintains metadata about Hive in a metastore, which is stored in a relational
    database. This metadata contains information about what tables exist, their columns,
    user privileges, and more.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 在一个元数据仓库中维护关于 Hive 的元数据，该仓库存储在关系型数据库中。这些元数据包含有关存在哪些表、它们的列、用户权限等信息。
- en: By default, Hive uses Derby, an embedded Java relational database, to store
    the metastore. Because it’s embedded, Derby can’t be shared between users, and
    as such it can’t be used in a multi-user environment where the metastore needs
    to be shared.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Hive 使用 Derby，一个嵌入式的 Java 关系型数据库，来存储元数据仓库。因为它是嵌入式的，所以 Derby 不能在用户之间共享，因此它不能用于需要共享元数据仓库的多用户环境。
- en: Databases, tables, partitions, and storage
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据库、表、分区和存储
- en: Hive can support multiple databases, which can be used to avoid table-name collisions
    (two teams or users that have the same table name) and to allow separate databases
    for different users or products.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 可以支持多个数据库，这可以用来避免表名冲突（两个具有相同表名的团队或用户）并允许为不同的用户或产品提供单独的数据库。
- en: A Hive table is a logical concept that’s physically composed of a number of
    files in HDFS. Tables can either be internal, where Hive organizes them inside
    a warehouse directory (controlled by the `hive.metastore.warehouse.dir` property
    with a default value of /user/hive/warehouse [in HDFS]), or they can be external,
    in which case Hive doesn’t manage them. Internal tables are useful if you want
    Hive to manage the complete lifecycle of your data, including the deletion, whereas
    external tables are useful when the files are being used outside of Hive.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 表是一个逻辑概念，在物理上由 HDFS 中的多个文件组成。表可以是内部的，其中 Hive 在仓库目录（由 `hive.metastore.warehouse.dir`
    属性控制，默认值为 /user/hive/warehouse [在 HDFS 中]）内组织它们，或者它们可以是外部的，在这种情况下 Hive 不管理它们。如果想要
    Hive 管理数据的完整生命周期，包括删除，内部表很有用，而外部表在文件在 Hive 之外使用时很有用。
- en: Tables can be partitioned, which is a physical arrangement of data, into distinct
    subdirectories for each unique partitioned key. Partitions can be static and dynamic,
    and we’ll look at both cases in technique 92.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 表可以是分区的，这是一种将数据物理安排到每个唯一分区键的独立子目录中的数据排列。分区可以是静态的或动态的，我们将在技术 92 中探讨这两种情况。
- en: Hive’s data model
  id: totrans-34
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Hive 的数据模型
- en: 'Hive supports the following data types:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 支持以下数据类型：
- en: '***Signed integers*** —`BIGINT` (8 bytes), `INT` (4 bytes), `SMALLINT` (2 bytes),
    and `TINYINT` (1 byte)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***有符号整数*** —`BIGINT` (8 字节), `INT` (4 字节), `SMALLINT` (2 字节), 和 `TINYINT`
    (1 字节)'
- en: '***Floating-point numbers*** —`FLOAT` (single precision) and `DOUBLE` (double
    precision)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***浮点数*** —`FLOAT` (单精度) 和 `DOUBLE` (双精度)'
- en: '***Booleans*** —`TRUE` or `FALSE`'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***布尔值*** —`TRUE`或`FALSE`'
- en: '***Strings*** —Sequences of characters in specified character sets'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***字符串*** —指定字符集中的字符序列'
- en: '***Maps*** —Associative arrays with collections of key/value pairs where keys
    are unique'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***映射*** —包含键/值对集合的关联数组，其中键是唯一的'
- en: '***Arrays*** —Indexable lists, where all elements must be of the same type'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***数组*** —可索引的列表，其中所有元素必须是同一类型'
- en: '***Structs*** —Complex types that contain elements'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***结构体*** —包含元素的复杂类型'
- en: Hive’s query language
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Hive的查询语言
- en: 'Hive’s query language supports much of the SQL specification, along with Hive-specific
    extensions, some of which are covered in this section. The full list of statements
    supported in Hive can be viewed in the Hive Language Manual: [https://cwiki.apache.org/confluence/display/Hive/LanguageManual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Hive的查询语言支持SQL规范的大部分内容，以及Hive特定的扩展，其中一些在本节中介绍。Hive支持的所有语句的完整列表可以在Hive语言手册中查看：[https://cwiki.apache.org/confluence/display/Hive/LanguageManual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual)。
- en: Tez
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Tez
- en: On Hadoop 1, Hive was limited to using MapReduce to execute most of the statements
    because MapReduce was the only processing engine supported on Hadoop. This wasn’t
    ideal, as users coming to Hive from other SQL systems were used to highly interactive
    environments where queries are frequently completed in seconds. MapReduce was
    designed for high-throughput batch processing, so its startup overhead coupled
    with its limited processing capabilities resulted in very high-latency query executions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop 1中，Hive被限制使用MapReduce来执行大多数语句，因为MapReduce是Hadoop上唯一支持的处理器。这并不理想，因为从其他SQL系统来到Hive的用户已经习惯了高度交互的环境，其中查询通常在几秒钟内完成。MapReduce是为高吞吐量批处理而设计的，因此其启动开销加上其有限的处理能力导致了非常高的查询延迟。
- en: 'With the Hive 0.13 release, Hive now uses Tez on YARN to execute its queries,
    and as a result, it’s able to get closer to the interactive ideal for working
    with your data.^([[1](#ch09fn01)]) Tez is basically a generalized Directed Acyclic
    Graph (DAG) execution engine that doesn’t impose any limits on how you compose
    your execution graph (as opposed to MapReduce) and that also allows you to keep
    data in-memory in between phases, reducing the disk and network I/O that MapReduce
    requires. You can read more about Tez at the following links:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hive 0.13版本发布后，Hive现在使用YARN上的Tez来执行其查询，因此它能够更接近交互式理想的工作方式。[^([1](#ch09fn01))]
    Tez基本上是一个通用的有向无环图（DAG）执行引擎，它不对你如何组合执行图施加任何限制（与MapReduce相反），并且还允许你在阶段之间保持数据在内存中，从而减少了MapReduce所需的磁盘和网络I/O。你可以在以下链接中了解更多关于Tez的信息：
- en: ¹ Carter Shanklin, “Benchmarking Apache Hive 13 for Enterprise Hadoop,” [http://hortonworks.com/blog/benchmarking-apache-hive-13-enterprise-hadoop/](http://hortonworks.com/blog/benchmarking-apache-hive-13-enterprise-hadoop/).
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹ Carter Shanklin，“Benchmarking Apache Hive 13 for Enterprise Hadoop”，[http://hortonworks.com/blog/benchmarking-apache-hive-13-enterprise-hadoop/](http://hortonworks.com/blog/benchmarking-apache-hive-13-enterprise-hadoop/)。
- en: 'Hive on Tez: [https://cwiki.apache.org/confluence/display/Hive/Hive+on+Tez](https://cwiki.apache.org/confluence/display/Hive/Hive+on+Tez)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hive on Tez：[https://cwiki.apache.org/confluence/display/Hive/Hive+on+Tez](https://cwiki.apache.org/confluence/display/Hive/Hive+on+Tez)
- en: 'Tez incubation Apache project page: [http://incubator.apache.org/projects/tez.html](http://incubator.apache.org/projects/tez.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tez孵化Apache项目页面：[http://incubator.apache.org/projects/tez.html](http://incubator.apache.org/projects/tez.html)
- en: 'In the Hive 0.13 release, Tez isn’t enabled by default, so you’ll need to follow
    these instructions to get it up and running:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hive 0.13版本中，Tez默认未启用，因此你需要遵循以下说明来启动它：
- en: 'Tez installation instructions: [https://github.com/apache/incubator-tez/blob/branch-0.2.0/INSTALL.txt](https://github.com/apache/incubator-tez/blob/branch-0.2.0/INSTALL.txt)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tez安装说明：[https://github.com/apache/incubator-tez/blob/branch-0.2.0/INSTALL.txt](https://github.com/apache/incubator-tez/blob/branch-0.2.0/INSTALL.txt)
- en: 'Configuring Hive to work on Tez: [https://issues.apache.org/jira/browse/HIVE-6098](https://issues.apache.org/jira/browse/HIVE-6098)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置Hive以在Tez上工作：[https://issues.apache.org/jira/browse/HIVE-6098](https://issues.apache.org/jira/browse/HIVE-6098)
- en: Interactive and non-interactive Hive
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 交互式和非交互式Hive
- en: 'The Hive shell provides an interactive interface:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Hive shell提供交互式界面：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Hive in non-interactive mode lets you execute scripts containing Hive commands.
    The following example uses the `-S` option so that only the output of the Hive
    command is written to the console:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Hive在非交互模式下允许你执行包含Hive命令的脚本。以下示例使用了`-S`选项，以便只将Hive命令的输出写入控制台：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Another non-interactive feature is the `-e` option, which lets you supply a
    Hive command as an argument:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非交互式特性是 `-e` 选项，它允许你将 Hive 命令作为参数提供：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If you’re debugging something in Hive and you want to see more detailed output
    on the console, you can use the following command to run Hive:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在调试 Hive 中的某个问题，并且希望在控制台上看到更详细的输出，你可以使用以下命令来运行 Hive：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: That concludes our brief introduction to Hive. Next we’ll look at how you can
    use Hive to mine interesting data from your log files.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对 Hive 的简要介绍。接下来，我们将探讨如何使用 Hive 从你的日志文件中挖掘有趣的数据。
- en: 9.1.2\. Reading and writing data
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.2\. 读取和写入数据
- en: This section covers some of the basic data input and output mechanics in Hive.
    We’ll ease into things with a brief look at working with text data before jumping
    into how you can work with Avro and Parquet data, which are becoming common ways
    to store data in Hadoop.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了 Hive 中一些基本的数据输入和输出机制。我们将从一个简要的文本数据处理工作开始，然后跳转到如何处理 Avro 和 Parquet 数据，这些正在成为
    Hadoop 中存储数据的一种常见方式。
- en: This section also covers some additional data input and output scenarios, such
    as writing and appending to tables and exporting data out to your local filesystem.
    Once we’ve covered these basic functions, subsequent sections will cover more
    advanced topics such as writing UDFs and performance tuning tips.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本节还涵盖了额外的数据输入和输出场景，例如向表中写入和追加数据以及将数据导出到本地文件系统。一旦我们覆盖了这些基本功能，后续章节将涵盖更高级的主题，例如编写
    UDF 和性能调优技巧。
- en: Technique 89 Working with text files
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 89 处理文本文件
- en: Imagine that you have a number of CSV or Apache log files that you want to load
    and analyze using Hive. After copying them into HDFS (if they’re not already there),
    you’ll need to create a Hive table before you can issue queries. If the result
    of your work is also large, you may want to write it into a new Hive table. This
    section covers these text I/O use cases in Hive.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一系列 CSV 或 Apache 日志文件，你希望使用 Hive 加载和分析这些文件。在将它们复制到 HDFS（如果它们尚未在那里）之后，你需要在发出查询之前创建一个
    Hive 表。如果你的工作结果也很大，你可能希望将其写入一个新的 Hive 表。本节涵盖了 Hive 中的这些文本 I/O 用例。
- en: Problem
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to use Hive to load and analyze text files, and then save the results.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望使用 Hive 加载和分析文本文件，然后保存结果。
- en: Solution
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the `RegexSerDe` class, bundled with the contrib library in Hive, and define
    a regular expression that can be used to parse the contents of Apache log files.
    This technique also looks at how serialization and deserialization works in Hive,
    and how to write your own SerDe to work with log files.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Hive 中 contrib 库捆绑的 `RegexSerDe` 类，并定义一个可以用来解析 Apache 日志文件内容的正则表达式。这项技术还探讨了
    Hive 中的序列化和反序列化工作方式，以及如何编写自己的 SerDe 来处理日志文件。
- en: Discussion
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: If you issue a `CREATE TABLE` command without any row/storage format options,
    Hive assumes the data is text-based using the default line and field delimiters
    shown in [table 9.1](#ch09table01).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发出一个没有任何行/存储格式选项的 `CREATE TABLE` 命令，Hive 假设数据是基于文本的，使用默认的行和字段分隔符，如 [表 9.1](#ch09table01)
    所示。
- en: Table 9.1\. Default text file delimiters
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 9.1\. 默认文本文件分隔符
- en: '| Default delimiter | Syntax example to change default delimiter | Description
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 默认分隔符 | 修改默认分隔符的语法示例 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| \n | LINES TERMINATED BY ''\n'' | Record separator. |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| \n | LINES TERMINATED BY ''\n'' | 记录分隔符。|'
- en: '| ^A | FIELDS TERMINATED BY ''\t'' | Field separator. If you wanted to replace
    ^A with another non-readable character, you’d represent it in octal, e.g., ''\001''.
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| ^A | FIELDS TERMINATED BY ''\t'' | 字段分隔符。如果你想用另一个不可读字符替换 ^A，你可以用八进制表示，例如
    ''\001''。|'
- en: '| ^B | COLLECTION ITEMS TERMINATED BY '';'' | An element separator for ARRAY,
    STRUCT, and MAP data types. |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| ^B | COLLECTION ITEMS TERMINATED BY '';'' | ARRAY、STRUCT 和 MAP 数据类型的元素分隔符。|'
- en: '| ^C | MAP KEYS TERMINATED BY '':'' | Used as a key/value separator in MAP
    data types. |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| ^C | MAP KEYS TERMINATED BY '':'' | 在 MAP 数据类型中用作键/值分隔符。|'
- en: Because most of the text data that you’ll work with will be structured in more
    standard ways, such as CSV, let’s look at how you can work with CSV.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因为大多数你将处理的数据文本将以更标准的方式结构化，例如 CSV，让我们看看如何处理 CSV。
- en: First you’ll need to copy the stocks CSV file included with the book’s code
    into HDFS. Create a directory in HDFS and then copy the stocks file into the directory:^([[2](#ch09fn02)])
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要将书中代码包含的股票 CSV 文件复制到 HDFS。在 HDFS 中创建一个目录，然后将股票文件复制到该目录中：^([[2](#ch09fn02)])
- en: ² Hive doesn’t allow you to create a table over a file; it must be a directory.
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ² Hive 不允许你在文件上创建表；它必须是一个目录。
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now you can create an external Hive table over your stocks directory:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以在股票目录上创建一个外部Hive表：
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '|  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Creating managed tables with the LOCATION keyword
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用LOCATION关键字创建托管表
- en: When you create an external (unmanaged) table, Hive keeps the data in the directory
    specified by the `LOCATION` keyword intact. But if you were to execute the same
    `CREATE` command and drop the `EXTERNAL` keyword, the table would be a managed
    table, and Hive would move the contents of the `LOCATION` directory into /user/hive/warehouse/stocks,
    which may not be the behavior you expect.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当您创建一个外部（非托管）表时，Hive会保留由`LOCATION`关键字指定的目录中的数据不变。但是，如果您执行相同的`CREATE`命令并删除`EXTERNAL`关键字，则该表将是一个托管表，Hive会将`LOCATION`目录的内容移动到/user/hive/warehouse/stocks，这可能不是您期望的行为。
- en: '|  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Run a quick query to verify that things look good:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 运行一个快速查询以验证一切看起来是否良好：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Sweet! What if you wanted to save the results into a new table and then show
    the schema of the new table?
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！如果您想将结果保存到新表中并显示新表的架构怎么办？
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '|  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Create-Table-As-Select (CTAS) and external tables
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 创建-表-选择（CTAS）和外部表
- en: CAS statements like the preceding example don’t allow you to specify that the
    table is `EXTERNAL`. But because the table that you’re selecting from is already
    an external table, Hive ensures that the new table is also an external table.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 前面示例中的CAS语句不允许您指定表是`EXTERNAL`。但是，因为您要从中选择的表已经是一个外部表，所以Hive确保新表也是一个外部表。
- en: '|  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'If the target table already exists, you have two options—you can either overwrite
    the entire contents of the table, or you can append to the table:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果目标表已经存在，您有两个选择——您可以选择覆盖表的全部内容，或者您可以追加到表中：
- en: '![](393fig01_alt.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](393fig01_alt.jpg)'
- en: 'You can view the raw table data using the Hadoop CLI:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Hadoop CLI查看原始表数据：
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The great thing about Hive external tables is that you can write into them using
    any method (it doesn’t have to be via a Hive command), and Hive will automatically
    pick up the additional data the next time you issue any Hive statements.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Hive外部表的好处是您可以使用任何方法写入它们（不一定要通过Hive命令），Hive将在您下次发出任何Hive语句时自动获取额外的数据。
- en: Tokenizing files with regular expressions
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用正则表达式标记文件
- en: Let’s make things more complicated and assume you want to work with log data.
    This data is in text form, but it can’t be parsed using Hive’s default deserialization.
    Instead, you need a way to specify a regular expression to parse your log data.
    Hive comes with a contrib `RegexSerDe` class that can tokenize your logs.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使事情更加复杂，并假设您想处理日志数据。这些数据以文本形式存在，但无法使用Hive的默认反序列化进行解析。相反，您需要一种方式来指定一个正则表达式以解析您的日志数据。Hive附带了一个contrib
    `RegexSerDe`类，可以标记您的日志。
- en: 'First, copy some log data into HDFS:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将一些日志数据复制到HDFS中：
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, specify that you want to use a custom deserializer. The `RegexSerDe`
    is bundled with the Hive contrib JAR, so you’ll need to add this JAR to Hive:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，指定您想使用自定义反序列化器。`RegexSerDe`包含在Hive contrib JAR中，因此您需要将此JAR添加到Hive中：
- en: '![](394fig01_alt.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](394fig01_alt.jpg)'
- en: 'A quick test will tell you if the data is being correctly handled by the SerDe:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一个快速测试将告诉您SerDe是否正确处理数据：
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If you’re seeing nothing but `NULL` values in the output, it’s probably because
    you have a missing space in your regular expression. Ensure that the regex in
    the `CREATE` statement looks like [figure 9.2](#ch09fig02).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在输出中只看到`NULL`值，那可能是因为您的正则表达式中缺少一个空格。请确保`CREATE`语句中的正则表达式看起来像[图9.2](#ch09fig02)。
- en: Figure 9.2\. CREATE table regex showing spaces
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.2\. CREATE table regex showing spaces
- en: '![](09fig02_alt.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig02_alt.jpg)'
- en: Hive’s SerDe is a flexible mechanism that can be used to extend Hive to work
    with any file format, as long as an `InputFormat` exists that can work with that
    file format. For more details on SerDes, take a look at the Hive documentation
    at [https://cwiki.apache.org/confluence/display/Hive/SerDe](https://cwiki.apache.org/confluence/display/Hive/SerDe).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Hive的SerDe是一个灵活的机制，可以用来扩展Hive以支持任何文件格式，只要存在一个可以处理该文件格式的`InputFormat`。有关SerDes的更多详细信息，请参阅Hive文档，网址为[https://cwiki.apache.org/confluence/display/Hive/SerDe](https://cwiki.apache.org/confluence/display/Hive/SerDe)。
- en: Working with Avro and Parquet
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 处理Avro和Parquet
- en: Avro is an object model that simplifies working with your data, and Parquet
    is a columnar storage format that can efficiently support advanced query optimizations
    such as predicate pushdowns. Combined, they’re a compelling pair and could well
    become the canonical way that data is stored in Hadoop. We covered both Avro and
    Parquet in depth in [chapter 3](kindle_split_013.html#ch03), which in technique
    23 shows you how to use Avro and Parquet in Hive.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Avro是一个简化数据处理的对象模型，Parquet是一种列式存储格式，可以高效地支持诸如谓词下推等高级查询优化。结合使用，它们是一对很有吸引力的组合，可能会成为数据在Hadoop中存储的规范方式。我们在[第3章](kindle_split_013.html#ch03)中深入探讨了Avro和Parquet，其中技术23展示了如何在Hive中使用Avro和Parquet。
- en: Technique 90 Exporting data to local disk
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术篇90：将数据导出到本地磁盘
- en: Getting data out of Hive and Hadoop is an important function you’ll need to
    be able to perform when you have data that you’re ready to pull into your spreadsheets
    or other analytics software. This technique examines a few methods you can use
    to pull out your Hive data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有准备拉入电子表格或其他分析软件的数据时，从Hive和Hadoop中提取数据是一个重要的功能，你需要能够执行。这个技术探讨了你可以使用的一些方法来拉取你的Hive数据。
- en: Problem
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You have data sitting in Hive that you want to pull out to your local filesystem.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你有数据存储在Hive中，你想要将其拉取到本地文件系统。
- en: Solution
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the standard Hadoop CLI tools or a Hive command to pull out your data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标准的Hadoop CLI工具或Hive命令来拉取你的数据。
- en: Discussion
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: If you want to pull out an entire Hive table to your local filesystem and the
    data format that Hive uses for your table is the same format that you want your
    data exported in, you can use the Hadoop CLI and run a `hadoop -get /user/hive/warehouse/...`
    command to pull down the table.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要将整个Hive表拉取到本地文件系统，并且Hive为你表使用的数据格式与你想导出数据使用的格式相同，你可以使用Hadoop CLI并运行一个`hadoop
    -get /user/hive/warehouse/...`命令来拉取该表。
- en: Hive comes with `EXPORT` (and corresponding `IMPORT`) commands that can be used
    to export Hive data and metadata into a directory in HDFS. This is useful for
    copying Hive tables between Hadoop clusters, but it doesn’t help you much in getting
    data out to the local filesystem.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Hive自带`EXPORT`（以及相应的`IMPORT`）命令，可以将Hive数据和元数据导出到HDFS中的一个目录。这对于在Hadoop集群之间复制Hive表很有用，但它并不能帮助你将数据导出到本地文件系统。
- en: 'If you want to filter, project, and perform some aggregations on your data
    and then pull it out of Hive, you can use the `INSERT` command and specify that
    the results should be written to a local directory:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要过滤、投影并对你的数据进行一些聚合，然后将数据从Hive中拉取出来，你可以使用`INSERT`命令并指定结果应该写入本地目录：
- en: '[PRE11]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This will create a directory on your local filesystem containing one or more
    files. If you view the files in an editor such as vi, you’ll notice that Hive
    used the default field separator (`^A`) when writing the files. And if any of
    the columns you exported were complex types (such as `STRUCT` or MAP), then Hive
    will use JSON to encode these columns.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在你的本地文件系统中创建一个包含一个或多个文件的目录。如果你使用vi等编辑器查看这些文件，你会注意到Hive在写入文件时使用了默认的字段分隔符（`^A`）。并且如果你导出的任何列是复杂类型（例如`STRUCT`或MAP），那么Hive将使用JSON来编码这些列。
- en: 'Luckily, newer versions of Hive (including 0.13) allow you to specify a custom
    delimiter when you export tables:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Hive的新版本（包括0.13）允许你在导出表时指定自定义分隔符：
- en: '[PRE12]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: With Hive’s reading and writing basics out of the way, let’s take a look at
    more complex topics, such as user-defined functions.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hive的读写基础问题解决之后，让我们看看更复杂的话题，例如用户定义函数。
- en: 9.1.3\. User-defined functions in Hive
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.3\. Hive中的用户定义函数
- en: We’ve looked at how Hive reads and writes tables, so it’s time to start doing
    something useful with your data. Since we want to cover more advanced techniques,
    we’ll look at how you can write a custom Hive user-defined function (UDF) to geolocate
    your logs. UDFs are useful if you want to mix custom code inline with your Hive
    queries.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了Hive如何读取和写入表，现在是时候开始对你的数据进行一些有用的操作了。由于我们想要覆盖更高级的技术，我们将看看如何编写一个自定义的Hive用户定义函数（UDF）来地理定位你的日志。如果你想在Hive查询中混合自定义代码，UDF非常有用。
- en: Technique 91 Writing UDFs
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术篇91：编写UDF
- en: This technique shows how you can write a Hive UDF and then use it in your Hive
    Query Language (HiveQL).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技术展示了你如何编写一个Hive UDF，然后将其用于你的Hive查询语言（HiveQL）。
- en: Problem
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: How do you write a custom function in Hive?
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何在Hive中编写自定义函数？
- en: Solution
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Extend the `UDF` class to implement your user-defined function and call it as
    a function in your HiveQL.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展`UDF`类以实现你的用户定义函数，并在你的HiveQL中调用它作为函数。
- en: Discussion
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: You can geolocate the IP addresses from the logs table using the free geolocation
    database from MaxMind.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 MaxMind 提供的免费地理位置数据库从日志表中定位 IP 地址。
- en: 'Download the free country geolocation database,^([[3](#ch09fn03)]) `gunzip`
    it, and copy the GeoIP.dat file to your /tmp/ directory. Next, use a UDF to geolocate
    the IP address from the log table that you created in technique 89:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 下载免费的国籍地理位置数据库，^([[3](#ch09fn03)]) 使用 `gunzip` 解压，并将 GeoIP.dat 文件复制到您的 /tmp/
    目录。接下来，使用 UDF 从您在技术 89 中创建的日志表中定位 IP 地址：
- en: ³ See MaxMind’s “GeoIP Country Database Installation Instructions,” [http://dev.maxmind.com/geoip/legacy/install/country/](http://dev.maxmind.com/geoip/legacy/install/country/).
  id: totrans-145
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³ 请参阅 MaxMind 的“GeoIP 国家数据库安装说明”，[http://dev.maxmind.com/geoip/legacy/install/country/](http://dev.maxmind.com/geoip/legacy/install/country/).
- en: '![](396fig01_alt.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](396fig01_alt.jpg)'
- en: 'When writing a UDF, there are two implementation options: either extend the
    `UDF` class or implement the `GenericUDF` class. The main differences between
    them are that the `GenericUDF` class can work with arguments that are complex
    types, so UDFs that extend `GenericUDF` are more efficient because the `UDF` class
    requires Hive to use reflection for discovery and invocation. [Figure 9.3](#ch09fig03)
    shows the two Hive UDF classes, one of which you need to extend to implement your
    UDF.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当编写一个用户定义函数（UDF）时，有两种实现选项：要么扩展 `UDF` 类，要么实现 `GenericUDF` 类。它们之间的主要区别在于 `GenericUDF`
    类可以处理复杂类型的参数，因此扩展 `GenericUDF` 的 UDF 更有效率，因为 `UDF` 类需要 Hive 使用反射来发现和调用。图 9.3（[#ch09fig03](#ch09fig03)）显示了两个
    Hive UDF 类，您需要扩展其中一个来实现您的 UDF。
- en: Figure 9.3\. Hive UDF `class` diagram
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.3\. Hive UDF `类` 图
- en: '![](09fig03_alt.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig03_alt.jpg)'
- en: The following listing shows the geolocation UDF, which you’ll implement using
    the `GenericUDF` class.^([[4](#ch09fn04)])
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了地理位置 UDF，您将使用 `GenericUDF` 类来实现它.^([[4](#ch09fn04)]).
- en: '⁴ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch9/hive/Geoloc.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch9/hive/Geoloc.java).'
  id: totrans-151
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴ GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch9/hive/Geoloc.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch9/hive/Geoloc.java).
- en: Listing 9.1\. The geolocation UDF
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.1\. 地理位置UDF
- en: '![](ch09ex01-0.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](ch09ex01-0.jpg)'
- en: '![](ch09ex01-1.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](ch09ex01-1.jpg)'
- en: 'The `Description` annotation can be viewed in the Hive shell with the `describe
    function` command:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `describe function` 命令可以在 Hive shell 中查看 `Description` 注解：
- en: '[PRE13]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Summary
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: Although the UDF we looked at operates on scalar data, Hive also has something
    called user-defined aggregate functions (UDAF), which allows more complex processing
    capabilities over aggregated data. You can see more about writing a UDAF on the
    Hive wiki at the page titled “Hive Operators and User-Defined Functions (UDFs)”
    ([https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF)).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们查看的 UDF 操作的是标量数据，但 Hive 还有一种称为用户定义的聚合函数（UDAF），它允许对聚合数据进行更复杂的处理。您可以在 Hive
    wiki 的“Hive 操作符和用户定义函数（UDFs）”页面（[https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF)）上了解更多关于编写
    UDAF 的信息。
- en: Hive also has user-defined table functions (UDTFs), which operate on scalar
    data but can emit more than one output for each input. See the `GenericUDTF` class
    for more details.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 还具有用户定义的表函数（UDTFs），它们操作标量数据，但可以为每个输入生成多个输出。有关更多详细信息，请参阅 `GenericUDTF` 类。
- en: Next we’ll take a look at what you can do to optimize your workflows in Hive.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨您可以在 Hive 中优化工作流程的方法。
- en: 9.1.4\. Hive performance
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.4\. Hive 性能
- en: In this section, we’ll examine some methods that you can use to optimize data
    management and processing in Hive. The tips presented here will help you ensure
    that as you scale out your data, the rest of Hive will keep up with your needs.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨一些您可以使用的方法来优化 Hive 中的数据管理和处理。这里提供的提示将帮助您确保在扩展数据时，Hive 的其他部分能够满足您的需求。
- en: Technique 92 Partitioning
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术编号 92：分区
- en: Partitioning is a common technique employed by SQL systems to horizontally or
    vertically split data to speed up data access. With reduced overall volume of
    data in a partition, partitioned read operations have a lot less data to sift
    through, and as a result can execute much more rapidly.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 分区是 SQL 系统常用的一种技术，用于水平或垂直分割数据以加快数据访问速度。由于分区中的数据总体量减少，分区读取操作需要筛选的数据更少，因此可以执行得更快。
- en: 'This same principle applies equally well to Hive, and it becomes increasingly
    important as your data sizes grow. In this section you’ll explore the two types
    of partitions in Hive: static partitions and dynamic partitions.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这个原则同样适用于 Hive，并且随着数据量的增长变得越来越重要。在本节中，你将探索 Hive 中的两种分区类型：静态分区和动态分区。
- en: Problem
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to arrange your Hive files so as to optimize queries against your data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望安排你的 Hive 文件，以优化对数据的查询。
- en: Solution
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use `PARTITIONED BY` to partition by columns that you typically use when querying
    your data.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `PARTITIONED BY` 来按你在查询数据时通常使用的列进行分区。
- en: Discussion
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Imagine you’re working with log data. A natural way to partition your logs would
    be by date, allowing you to perform queries on specific time periods without incurring
    the overhead of a full table scan (reading the entire contents of the table).
    Hive supports partitioned tables and gives you control of determining which columns
    are partitioned.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在处理日志数据。对日志进行分区的一种自然方式是按日期进行，这样你就可以在不进行全表扫描（读取表的所有内容）的情况下执行特定时间段的查询。Hive
    支持分区表，并允许你控制确定哪些列是分区的。
- en: 'Hive supports two types of partitions: static partitions and dynamic partitions.
    They differ in the way you construct `INSERT` statements, as you’ll discover in
    this technique.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 支持两种类型的分区：静态分区和动态分区。它们在构造 `INSERT` 语句的方式上有所不同，你将在本技术中了解到这一点。
- en: Static partitioning
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 静态分区
- en: 'For the purpose of this technique, you’ll work with a very simple log structure.
    The fields are IP address, year, month, day, and HTTP status code:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了本技术的目的，你将使用一个非常简单的日志结构。字段包括 IP 地址、年份、月份、日期和 HTTP 状态码：
- en: '[PRE14]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Load them into HDFS and into an external table:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 将它们加载到 HDFS 和外部表中：
- en: '[PRE15]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now you can create a partitioned table, where the year, month, and day are
    partitions:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以创建一个分区表，其中年份、月份和日期是分区：
- en: '[PRE16]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'By default, Hive inserts follow a static partition method that requires all
    inserts to explicitly enumerate not only the partitions, but the column values
    for each partition. Therefore, an individual `INSERT` statement can only insert
    into one day’s worth of partitions:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Hive 的插入遵循静态分区方法，要求所有插入操作不仅要明确枚举分区，还要枚举每个分区的列值。因此，单个 `INSERT` 语句只能插入到一天的分区中：
- en: '[PRE17]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Luckily Hive has a special data manipulation language (DML) statement that
    allows you to insert into multiple partitions in a single statement. The following
    code will insert all the sample data (spanning three days) into the three partitions:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Hive 有一种特殊的数据操作语言（DML）语句，允许你在一个语句中插入到多个分区。以下代码将所有样本数据（跨越三天）插入到三个分区中：
- en: '[PRE18]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This approach has an additional advantage in that it will only make one pass
    over the logs_ext table to perform the inserts—the previous approach would have
    required *N* queries on the source table for *N* partitions.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法还有一个额外的优点，那就是它只需对 logs_ext 表进行一次遍历即可执行插入操作——之前的方法需要针对源表进行 *N* 次查询，以处理 *N*
    个分区。
- en: '|  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Flexibility of single-pass static partitioned inserts
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 单次遍历静态分区插入的灵活性
- en: Hive doesn’t limit either the destination tables or whether the query conditions
    need to align with the partitions. Therefore, there’s nothing stopping you from
    inserting into different tables and having overlapping rows in multiple partitions
    or tables.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Hive 不限制目标表或查询条件是否需要与分区对齐。因此，你无法阻止你将数据插入到不同的表中，并在多个分区或表中出现重叠的行。
- en: '|  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: One disadvantage of static partitions is that when you’re inserting data, you
    must explicitly specify the partition that’s being inserted into. But you’re not
    stuck with static partitions as the only partitions supported in Hive. Hive has
    the notion of dynamic partitions, which make life a little easier by not requiring
    you to specify the partition when inserting data.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 静态分区的一个缺点是，当你插入数据时，你必须明确指定要插入的分区。但 Hive 支持的分区类型不仅仅是静态分区。Hive 有动态分区的概念，这使得在插入数据时不需要指定分区，从而让生活变得更容易。
- en: Dynamic partitioning
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 动态分区
- en: Dynamic partitions are smarter than static partitions, as they can automatically
    determine which partition a record needs to be written to when data is being inserted.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 与静态分区相比，动态分区更智能，因为它们可以在插入数据时自动确定记录需要写入哪个分区。
- en: 'Let’s create a whole new table to store some dynamic partitions. Notice how
    the syntax to create a table that uses dynamic partitions is exactly the same
    as that for static partitioned tables:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个全新的表来存储一些动态分区。注意创建使用动态分区的表的语法与静态分区表的语法完全相同：
- en: '[PRE19]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The differences only come into play at `INSERT` time:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 差异仅在`INSERT`时才会显现：
- en: '![](401fig01_alt.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](401fig01_alt.jpg)'
- en: That’s a lot better—you no longer need to explicitly tell Hive which partitions
    you’re inserting into. It’ll dynamically figure this out.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这好多了——你不再需要明确告诉Hive你要插入哪些分区。它将动态地解决这个问题。
- en: Mixing dynamic and static partitions in the same table
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在同一表中混合动态和静态分区
- en: Hive supports mixing both static and dynamic columns in a table. There’s also
    nothing stopping you from transitioning from a static partition insert method
    to dynamically partitioned inserts.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Hive支持在表中混合静态和动态列。也没有什么阻止你从静态分区插入方法过渡到动态分区插入。
- en: Partition directory layout
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分区目录布局
- en: Partitioned tables are laid out in HDFS differently from nonpartitioned tables.
    Each partition value occupies a separate directory in Hive containing the partition
    column name as well as its value.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 分区表在HDFS中的布局与非分区表不同。每个分区值在Hive中占用一个单独的目录，包含分区列名称及其值。
- en: 'These are the contents of HDFS after running the most recent `INSERT`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是运行最近的`INSERT`后HDFS的内容：
- en: '[PRE20]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The “000000_0” are the files that contain the rows. There’s only one per partitioned
    day due to the small dataset (running with a larger dataset with more than one
    task will result in multiple files).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: “000000_0”是包含行的文件。由于数据集较小（使用包含多个任务的大型数据集运行将导致多个文件），每个分区日只有一个。
- en: Customizing partition directory names
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 自定义分区目录名称
- en: As you just saw, left to its own devices, Hive will create partition directory
    names using the `column=value` format. What if you wanted to have more control
    over the directories? Instead of your partitioned directory looking like this,
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，Hive会根据`column=value`格式创建分区目录名称。如果你想对目录有更多的控制，而不是你的分区目录看起来像这样，
- en: '[PRE21]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'what if you wanted it to look like this:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你想让它看起来像这样：
- en: '[PRE22]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You can achieve this by giving Hive the complete path that should be used to
    store a partition:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过给Hive提供应用于存储分区的完整路径来实现这一点：
- en: '[PRE23]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You can query the location of individual partitions with the `DESCRIBE` command:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`DESCRIBE`命令查询单个分区的位置：
- en: '[PRE24]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This can be a powerful tool, as Hive doesn’t require that all the partitions
    for a table be on the same cluster or type of filesystem. Therefore, a Hive table
    could have a partition sitting in Hadoop cluster A, another sitting in cluster
    B, and a third in a cluster in Amazon S3\. This opens up some powerful strategies
    for aging out data to other filesystems.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以是一个强大的工具，因为Hive不需要表的所有分区都在同一个集群或文件系统类型。因此，一个Hive表可以有一个分区位于Hadoop集群A中，另一个位于集群B中，第三个位于Amazon
    S3的集群中。这为将数据老化到其他文件系统提供了一些强大的策略。
- en: Querying partitions from Hive
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从Hive查询分区
- en: 'Hive provides some commands to allow you to see the current partitions for
    a table:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Hive提供了一些命令，允许你查看表的当前分区：
- en: '[PRE25]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Bypassing Hive to load data into partitions
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 跳过Hive将数据加载到分区
- en: Let’s say you had some data for a new partition (2014/6/24) that you wanted
    to manually copy into your partitioned Hive table using HDFS commands (or some
    other mechanism such as MapReduce).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一些新分区（2014/6/24）的数据，你想要使用HDFS命令（或某些其他机制，如MapReduce）手动将其复制到你的分区Hive表中。
- en: 'Here’s some sample data (note that the date parts are removed because Hive
    only retains these column details in the directory names):'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些示例数据（请注意，日期部分已被删除，因为Hive仅在目录名称中保留这些列的详细信息）：
- en: '[PRE26]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Create a new partitioned directory and copy the file into it:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的分区目录并将文件复制到其中：
- en: '[PRE27]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now go to your Hive shell and try to select the new data:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在转到你的Hive shell，并尝试选择新数据：
- en: '[PRE28]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'No results! This is because Hive doesn’t yet know about the new partition.
    You can run a repair command so that Hive can examine HDFS to determine the current
    partitions:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 没有结果！这是因为Hive还不知道新的分区。你可以运行一个修复命令，让Hive检查HDFS以确定当前分区：
- en: '[PRE29]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now your `SELECT` will work:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你的`SELECT`将可以工作：
- en: '[PRE30]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Alternatively, you could explicitly inform Hive about the new partition:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以明确通知Hive关于新的分区：
- en: '[PRE31]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Summary
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: Given the flexibility of dynamic partitions, in what situations would static
    partitions offer an advantage? One example is in cases where the data that you’re
    inserting doesn’t have any knowledge of the partitioned columns, but some other
    process does.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到动态分区的灵活性，在什么情况下静态分区会提供优势？一个例子是，当您要插入的数据对分区列没有任何了解，而其他某个过程有了解时。
- en: 'For example, suppose you have some log data that you want to insert, but for
    whatever reason the log data doesn’t contain dates. In this case, you can craft
    a static partitioned insert as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您有一些日志数据要插入，但出于某种原因，日志数据不包含日期。在这种情况下，您可以按照以下方式创建一个静态分区插入：
- en: '[PRE32]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Let’s next take a look at columnar data, which is another form of data partitioning
    that can provide dramatic query execution time improvements.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看列式数据，这是另一种可以提供显著查询执行时间改进的数据分区形式。
- en: Columnar data
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列式数据
- en: Most data that we’re used to working with is stored on disk in row-oriented
    order, meaning that all the columns for a row are contiguously located when stored
    at rest on persistent storage. CSV, SequenceFiles, and Avro are typically stored
    in rows.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们习惯处理的大多数数据都是以行顺序存储在磁盘上的，这意味着当数据在持久存储上静止存储时，一行中的所有列都是连续存放的。CSV、SequenceFiles
    和 Avro 通常以行存储。
- en: Using a column-oriented storage format for saving your data can offer huge performance
    benefits, both from space and execution-time perspectives. Contiguously locating
    columnar data together allows storage formats to use sophisticated data-compression
    schemes such as run-length encoding, which can’t be applied to row-oriented data.
    Furthermore, columnar data allows execution engines such as Hive, Map-Reduce,
    and Tez to push predicates and projections to the storage formats, allowing these
    storage formats to skip over data that doesn’t match the pushdown criteria.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 使用列导向的存储格式来保存您的数据可以在空间和执行时间方面提供巨大的性能优势。将列式数据连续存放在一起允许存储格式使用复杂的数据压缩方案，如运行长度编码，这些方案不能应用于行导向数据。此外，列式数据允许执行引擎如
    Hive、Map-Reduce 和 Tez 将谓词和投影推送到存储格式，允许这些存储格式跳过不匹配推送标准的数据。
- en: 'There are currently two hot options for columnar storage on Hive (and Hadoop):
    Optimized Row Columnar (ORC) and Parquet. They come out of Hortonworks and Cloudera/Twitter,
    respectively, and both offer very similar space- and time-saving optimizations.
    The only edge really comes out of the goal of Parquet to maximize compatibility
    in the Hadoop community, so at the time of writing, Parquet has greater support
    for the Hadoop ecosystem.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 目前在 Hive（和 Hadoop）上对列式存储有两个热门选择：优化行列式（ORC）和 Parquet。它们分别来自 Hortonworks 和 Cloudera/Twitter，两者都提供了非常相似的空间和时间节省优化。唯一的真正优势来自于
    Parquet 的目标是在 Hadoop 社区中最大化兼容性，因此在撰写本文时，Parquet 对 Hadoop 生态系统的支持更为广泛。
- en: '[Chapter 3](kindle_split_013.html#ch03) has a section devoted to Parquet, and
    technique 23 includes instructions on how Parquet can be used with Hive.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[第 3 章](kindle_split_013.html#ch03) 有一个专门介绍 Parquet 的部分，技术 23 包括了如何使用 Hive
    与 Parquet 一起使用的说明。'
- en: Technique 93 Tuning Hive joins
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术 93 调优 Hive 联接
- en: It’s not uncommon to execute a join over some large datasets in Hive and wait
    hours for it to complete. In this technique we’ll look at how joins can be optimized,
    much like we did for MapReduce in [chapter 4](kindle_split_014.html#ch04).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Hive 中执行联接操作并在一些大型数据集上等待数小时以完成它们并不罕见。在这个技术中，我们将探讨如何优化联接，就像我们在第 4 章 [chapter
    4](kindle_split_014.html#ch04) 中为 MapReduce 所做的那样。
- en: Problem
  id: totrans-243
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: Your Hive joins are running slower than expected, and you want to learn what
    options you have to speed them up.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 您的 Hive 联接运行速度比预期慢，您想了解有哪些选项可以加快它们的速度。
- en: Solution
  id: totrans-245
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Look at how you can optimize Hive joins with *repartition joins*, *replication
    joins*, and *semi-joins*.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 看看如何通过 *repartition 联接*、*复制联接* 和 *半联接* 来优化 Hive 联接。
- en: Discussion
  id: totrans-247
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'We’ll cover three types of joins in Hive: the repartition join, which is the
    standard reduce-side join; the replication join, which is the map-side join; and
    the semi-join, which only cares about retaining data from one table.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 Hive 中介绍三种类型的联接：repartition 联接，这是标准的 reduce-side 联接；replication 联接，这是 map-side
    联接；以及半联接，它只关心保留一个表中的数据。
- en: 'Before we get started, let’s create two tables to work with:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，让我们创建两个用于工作的表：
- en: '[PRE33]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You’ve created two tables. The stocks table contains just three columns—the
    stock symbol, the date, and the price. The names table contains the stock symbols
    and the company names:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经创建了两个表。股票表包含三个列——股票符号、日期和价格。名称表包含股票符号和公司名称：
- en: '[PRE34]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Join table ordering
  id: totrans-253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 连接表排序
- en: As with any type of tuning, it’s important to understand the internal workings
    of a system. When Hive executes a join, it needs to select which table is streamed
    and which table is cached. Hive picks the last table in the `JOIN` statement for
    streaming, so you should take care to ensure that this is the largest table.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何类型的调整一样，了解系统的内部工作原理非常重要。当Hive执行连接时，它需要选择哪个表是流式传输的，哪个表是缓存的。Hive选择`JOIN`语句中的最后一个表进行流式传输，因此你应该注意确保这是最大的表。
- en: 'Let’s look at the example of our two tables. The stocks table, which includes
    daily quotes, will continue to grow over time, but the names table, which contains
    the stock symbol names, will be mostly static. Therefore, when these tables are
    joined, it’s important that the larger table, stocks, comes last in the query:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们两个表的例子。包含每日报价的股票表将随着时间的推移继续增长，但包含股票符号名称的名称表将基本上是静态的。因此，当这些表进行连接时，重要的是较大的表，即股票表，在查询中排在最后：
- en: '[PRE35]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'You can also explicitly tell Hive which table it should stream:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以明确告诉Hive它应该流式传输哪个表：
- en: '[PRE36]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Map-side joins
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 映射连接
- en: A replicated join is a map-side join where a small table is cached in memory
    and the large table is streamed. You can see how it works in MapReduce in [figure
    9.4](#ch09fig04).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 复制连接是一种映射端连接，其中小表在内存中缓存，大表流式传输。你可以在[图9.4](#ch09fig04)中看到它是如何在MapReduce中工作的。
- en: Figure 9.4\. A replicated join
  id: totrans-261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.4\. 复制连接
- en: '![](09fig04_alt.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig04_alt.jpg)'
- en: 'Map-side joins can be used to execute both inner and outer joins. The current
    recommendation is that you configure Hive to automatically attempt to convert
    joins into map-side joins:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 映射端连接可以用于执行内连接和外连接。当前的推荐是配置Hive自动尝试将连接转换为映射端连接：
- en: '[PRE37]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The first two settings must be set to `true` to enable autoconversion of joins
    to map-side joins (in Hive 0.13 they’re both enabled by default). The last setting
    is used by Hive to determine whether a join can be converted. Imagine you have
    *N* tables in your join. If the size of the smallest *N* – 1 tables on disk is
    less than `hive.auto.convert.join.noconditionaltask.size`, then the join is converted
    to a map-side join. Bear in mind that the check is rudimentary and only examines
    the size of the tables on disk, so factors such as compression and filters or
    projections don’t come into the equation.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个设置必须设置为`true`以启用将连接自动转换为映射端连接（在Hive 0.13中它们默认都是启用的）。最后一个设置由Hive用于确定是否可以将连接转换为映射端连接。想象一下你在连接中有*N*个表。如果最小的*N*
    - 1个表在磁盘上的大小小于`hive.auto.convert.join.noconditionaltask.size`，则连接将转换为映射端连接。请注意，这个检查是基本的，并且只检查磁盘上表的大小，因此压缩、过滤器或投影等因素不计入等式。
- en: '|  |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Map-join hint
  id: totrans-267
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 映射连接提示
- en: 'Older versions of Hive supported a hint that you could use to instruct Hive
    which table was the smallest and should be cached. Here’s an example:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 旧版本的Hive支持一个提示，你可以用它来指示Hive哪个表是最小的并且应该被缓存。以下是一个示例：
- en: '[PRE38]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Recent versions of Hive ignore this hint (`hive.ignore.mapjoin.hint` is set
    to `true` by default) because it put the onus on the query author to determine
    the smaller table, which can lead to slow queries due to user error.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 新版本的Hive忽略了此提示（`hive.ignore.mapjoin.hint`默认设置为`true`），因为它将责任放在查询作者身上，由他们确定较小的表，这可能导致由于用户错误而查询缓慢。
- en: '|  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Sort-merge-bucket joins
  id: totrans-272
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 排序合并桶连接
- en: Hive tables can be bucketed and sorted, which helps you to easily sample data,
    and it’s also a useful join optimization as it enables sort-merge-bucket (SMB)
    joins. SMB joins require that all tables be sorted and bucketed, in which case
    joins are very efficient because they require a simple merge of the presorted
    tables.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: Hive表可以进行分桶和排序，这有助于你轻松采样数据，并且它也是一种有用的连接优化，因为它使得排序合并桶（SMB）连接成为可能。SMB连接要求所有表都必须排序和分桶，在这种情况下，连接非常高效，因为它们只需要对预排序表进行简单的合并。
- en: 'The following example shows how you’d create a sorted and bucketed stocks table:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例显示了如何创建排序和分桶的股票表：
- en: '[PRE39]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '|  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Inserting into bucketed tables
  id: totrans-277
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 向分桶表插入数据
- en: You can use regular `INSERT` statements to insert into bucketed tables, but
    you need to set the `hive.enforce.bucketing` property to `true`. This instructs
    Hive that it should look at the number of buckets in the table to determine the
    number of reducers that will be used when inserting into the table (the number
    of reducers must be equal to the number of buckets).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用常规的`INSERT`语句向分桶表插入数据，但需要将`hive.enforce.bucketing`属性设置为`true`。这指示Hive在插入表时应查看表中的桶数以确定将使用的reducer数量（reducer的数量必须等于桶的数量）。
- en: '|  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'To enable SMB joins, you must set the following properties:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用 SMB 连接，您必须设置以下属性：
- en: '[PRE40]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'In addition, you’ll also need to ensure that the following conditions hold
    true:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还需要确保以下条件成立：
- en: All tables being joined are bucketed and sorted on the join column.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有要连接的表都按连接列进行分桶和排序。
- en: The number of buckets in each join table must be equal, or factors of one another.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个连接表中的桶数必须相等，或者彼此是因数。
- en: Skew
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 偏差
- en: 'Skew can lead to lengthy MapReduce execution times because a small number of
    reducers may receive a disproportionately large number of records for some join
    values. Hive, by default, doesn’t attempt to do anything about this, but it can
    be configured to detect skew and optimize joins on skewed keys:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差可能导致 MapReduce 执行时间过长，因为少数几个 reducer 可能会接收到一些连接值的不成比例的大量记录。默认情况下，Hive 不会尝试做任何事情来解决这个问题，但它可以被配置为检测偏差并优化偏差键的连接：
- en: '![](407fig01_alt.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![图片](407fig01_alt.jpg)'
- en: So what happens when Hive detects skew? You can see the additional step that
    Hive adds in [figure 9.5](#ch09fig05), where skewed keys are written to HDFS and
    processed in a separate MapReduce job.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Hive 检测到偏差时会发生什么？您可以在[图 9.5](#ch09fig05)中看到 Hive 添加的额外步骤，其中偏差键被写入 HDFS 并在单独的
    MapReduce 作业中处理。
- en: Figure 9.5\. Hive skew optimization
  id: totrans-289
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.5\. Hive 偏差优化
- en: '![](09fig05_alt.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig05_alt.jpg)'
- en: It should be noted that this skew optimization only works with reduce-side repartition
    joins, not map-side replication joins.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，这种偏差优化仅适用于 reduce-side repartition joins，不适用于 map-side replication joins。
- en: Skewed tables
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 偏差表
- en: If you know ahead of time that there are particular keys with high skews, you
    can tell Hive about them when creating your table. If you do this, Hive will write
    out skewed keys into separate files that allow it to further optimize queries,
    and even to skip over the files if possible.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在创建表之前就知道有一些特定的键具有高偏差，你可以在创建表时告诉 Hive 这些键。如果你这样做，Hive 将将偏差键写入单独的文件中，从而允许它进一步优化查询，甚至在可能的情况下跳过这些文件。
- en: 'Imagine that you have two stocks (Apple and Google) that have a much larger
    number of records compared to the others—in this case you’d modify your `CREATE
    TABLE` statement with the keywords `SKEWED BY`, as follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有两支股票（苹果和谷歌）的记录数量比其他股票多得多——在这种情况下，您需要修改您的 `CREATE TABLE` 语句，使用关键字 `SKEWED
    BY`，如下所示：
- en: '[PRE41]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 9.2\. Impala
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2\. Impala
- en: Impala is a low-latency, massively parallel query engine, modeled after Google’s
    Dremel paper describing a scalable and interactive query system.^([[5](#ch09fn05)])
    Impala was conceived and developed out of Cloudera, which realized that using
    MapReduce to execute SQL wasn’t viable for a low-latency SQL environment.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: Impala 是一个低延迟、大规模并行查询引擎，其设计灵感来自 Google 的 Dremel 论文，该论文描述了一个可扩展且交互式的查询系统。[5](#ch09fn05)
    Impala 是在 Cloudera 的背景下构思和开发的，它意识到在低延迟 SQL 环境中使用 MapReduce 来执行 SQL 是不可行的。
- en: '⁵ Sergey Melnik et al., “Dremel: Interactive Analysis of Web-Scale Datasets,”
    [http://research.google.com/pubs/pub36632.html](http://research.google.com/pubs/pub36632.html).'
  id: totrans-298
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '⁵ Sergey Melnik 等人，“Dremel: Interactive Analysis of Web-Scale Datasets”，[http://research.google.com/pubs/pub36632.html](http://research.google.com/pubs/pub36632.html).'
- en: Each daemon in Impala is designed to be self-sufficient, and a client can send
    a query to any Impala daemon. Impala does have some metadata services, but it
    can continue to function even when they’re not working, as the daemon nodes talk
    directly to one another to execute queries. An overview of the Impala architecture
    can be seen in [figure 9.6](#ch09fig06).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Impala 守护进程都设计为自给自足的，客户端可以向任何 Impala 守护进程发送查询。Impala 确实有一些元数据服务，但即使它们不工作，它也可以继续运行，因为守护节点可以直接相互交谈以执行查询。Impala
    架构的概述可以在[图 9.6](#ch09fig06)中看到。
- en: Figure 9.6\. The Impala architecture
  id: totrans-300
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.6\. Impala 架构
- en: '![](09fig06_alt.jpg)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig06_alt.jpg)'
- en: Impala allows you to query data in HDFS or HBase with a SQL syntax, so it supports
    access via ODBC. It uses the Hive metastore, so it can read existing Hive tables,
    and DDL statements executed via Impala are also reflected in Hive.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: Impala 允许您使用 SQL 语法查询 HDFS 或 HBase 中的数据，因此它支持通过 ODBC 访问。它使用 Hive 元数据存储，因此它可以读取现有的
    Hive 表，并且通过 Impala 执行的 DDL 语句也会反映在 Hive 中。
- en: In this section I’ll present some of the differences between Impala and Hive,
    and we’ll also look at some basic examples of Impala in action, including how
    Hive UDFs can be used.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将介绍 Impala 和 Hive 之间的一些差异，我们还将查看一些 Impala 的基本示例，包括如何使用 Hive UDFs。
- en: 9.2.1\. Impala vs. Hive
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.1\. Impala 与 Hive
- en: 'There are a handful of differences between Impala and Hive:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: Impala 和 Hive 之间存在一些差异：
- en: Impala is designed from the ground up as a massively parallel query engine and
    doesn’t need to translate SQL into another processing framework. Hive relies on
    MapReduce (or more recently Tez) to execute.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Impala从一开始就被设计为一个大规模并行查询引擎，不需要将SQL转换为另一个处理框架。Hive依赖于MapReduce（或更近期的Tez）来执行。
- en: Impala and Hive are both open source, but Impala is a curated project under
    Cloudera’s control.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Impala和Hive都是开源的，但Impala是Cloudera控制下的精选项目。
- en: Impala isn’t fault-tolerant.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Impala不具有容错性。
- en: Impala doesn’t support complex types such as maps, arrays, and structs (including
    nested Avro data). You can basically only work with flat data.^([[6](#ch09fn06)])
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Impala不支持复杂类型，如映射、数组和结构（包括嵌套的Avro数据）。你基本上只能处理扁平数据.^([[6](#ch09fn06)])
- en: '⁶ Impala and Avro nested type support is planned for Impala 2.0: [https://issues.cloudera.org/browse/IMPALA-345](https://issues.cloudera.org/browse/IMPALA-345).'
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶ Impala和Avro嵌套类型支持计划在Impala 2.0中实现：[https://issues.cloudera.org/browse/IMPALA-345](https://issues.cloudera.org/browse/IMPALA-345)。
- en: There are various file formats and compression codec combinations that require
    you to use Hive to create and load tables. For example, you can’t create or load
    data into an Avro table in Impala, and you can’t load an LZO-compressed text file
    in Impala. For Avro you need to create the table in Hive before you can use it
    in Impala, and in both Avro and LZO-compressed text, you’ll need to load your
    data into these tables using Hive before you can use them in Impala.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有各种文件格式和压缩编解码器组合，需要你使用Hive来创建和加载数据表。例如，你无法在Impala中创建或加载数据到Avro表，也无法在Impala中加载LZO压缩的文本文件。对于Avro，你需要在Impala中使用它之前在Hive中创建表，而在Avro和LZO压缩的文本中，你需要在Impala中使用它们之前使用Hive将这些数据加载到这些表中。
- en: Impala doesn’t support Hive user-defined table-generating functions (UDTSs),
    although it does support Hive UDFs and UDAFs and can work with existing JARs that
    contain these UDFs without any changes to the JAR.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Impala不支持Hive用户定义的表生成函数（UDTS），尽管它支持Hive UDF和UDAF，并且可以与包含这些UDF的现有JAR文件一起工作，而无需对JAR文件进行任何更改。
- en: There are certain aggregate functions and HiveQL statements that aren’t supported
    in Impala.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有些聚合函数和HiveQL语句在Impala中不受支持。
- en: '|  |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Impala and Hive versions
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Impala和Hive版本
- en: This list compares Hive 0.13 and Impala 1.3.1, both of which are current at
    the time of writing. It should be noted that the Impala 2 release will address
    some of these items.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表比较了Hive 0.13和Impala 1.3.1，两者在撰写时都是当前的。需要注意的是，Impala 2版本将解决这些问题中的某些问题。
- en: '|  |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Cloudera has a detailed list of the SQL differences between Impala and Hive:
    [http://mng.bz/0c2F](http://mng.bz/0c2F).'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: Cloudera有一个详细的Impala和Hive之间SQL差异列表：[http://mng.bz/0c2F](http://mng.bz/0c2F)。
- en: 9.2.2\. Impala basics
  id: totrans-319
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.2. Impala基础知识
- en: This section covers what are likely the two most popular data formats for Impala—text
    and Parquet.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了Impala可能最流行的两种数据格式——文本和Parquet。
- en: Technique 94 Working with text
  id: totrans-321
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧94：处理文本
- en: Text is typically the first file format that you’ll work with when exploring
    a new tool, and it also serves as a good learning tool for understanding the basics.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 文本通常是探索新工具时首先处理的文件格式，它也作为理解基础的良好学习工具。
- en: Problem
  id: totrans-323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You have data in text form that you want to work with in Impala.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 你有以文本形式存在的数据，你希望在Impala中处理。
- en: Solution
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Impala’s text support is identical to Hive’s.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: Impala的文本支持与Hive相同。
- en: Discussion
  id: totrans-327
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Impala’s basic query language is identical to Hive’s. Let’s kick things off
    by copying the stocks data into a directory in HDFS:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: Impala的基本查询语言与Hive相同。让我们从将股票数据复制到HDFS中的一个目录开始：
- en: '[PRE42]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next you’ll create an external table and run a simple aggregation over the
    data:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将创建一个外部表并对数据进行简单的聚合操作：
- en: '[PRE43]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '|  |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Using Hive tables in Impala
  id: totrans-333
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在Impala中使用Hive表
- en: The example in technique 94 shows how to create a table called stocks in Impala.
    If you’ve already created the stocks table in Hive (as shown in technique 89),
    then rather than create the table in Impala, you should refresh Impala’s metadata
    and then use that Hive table in Impala.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 技巧94中的示例展示了如何在Impala中创建一个名为stocks的表。如果你已经在Hive中创建了stocks表（如技巧89所示），那么你不需要在Impala中创建该表，而应该刷新Impala的元数据，然后使用该Hive表在Impala中。
- en: 'After creating the table in Hive, issue the following statement in the Impala
    shell:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hive中创建表后，在Impala shell中发出以下语句：
- en: '[PRE44]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: At this point, you can issue queries against the stocks table inside the Impala
    shell.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可以在Impala shell中对stocks表发出查询。
- en: Alternatively, if you really want to create the table in Impala and you’ve already
    created the table in Hive, you’ll need to issue a `DROP TABLE` command prior to
    issuing the `CREATE TABLE` command in Impala.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果您真的想在 Impala 中创建表并且您已经在 Hive 中创建了表，您需要在 Impala 中发出 `CREATE TABLE` 命令之前发出
    `DROP TABLE` 命令。
- en: '|  |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: That’s it! You’ll notice that the syntax is exactly the same as in Hive. The
    one difference is that you can’t use `symbol` and `date` as column names because
    they’re reserved symbols in Impala (Hive doesn’t have any such restrictions).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些！您会注意到语法与 Hive 完全相同。唯一的区别是您不能使用 `symbol` 和 `date` 作为列名，因为它们是 Impala 中的保留符号（Hive
    没有这样的限制）。
- en: 'Let’s take a look at working with a storage format that’s a bit more interesting:
    Parquet.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何处理一个更有趣的存储格式：Parquet。
- en: Technique 95 Working with Parquet
  id: totrans-342
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 95 使用 Parquet
- en: It’s highly recommended that you use Parquet as your storage format for various
    space and time efficiencies (see [chapter 3](kindle_split_013.html#ch03) for more
    details on Parquet’s benefits). This technique looks at how you can create Parquet
    tables in Impala.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 非常推荐您使用 Parquet 作为存储格式，以获得各种空间和时间效率（有关 Parquet 优势的更多详细信息，请参阅第 3 章[章节链接](kindle_split_013.html#ch03)）。本技巧探讨了如何在
    Impala 中创建 Parquet 表。
- en: Problem
  id: totrans-344
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You need to save your data in Parquet format to speed up your queries and improve
    the compression of your data.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要将数据保存为 Parquet 格式以加快查询速度并提高数据的压缩率。
- en: Solution
  id: totrans-346
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use `STORED AS PARQUET` when creating tables.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建表时使用 `STORED AS PARQUET`。
- en: Discussion
  id: totrans-348
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'One way to get up and started quickly with Parquet is to create a new Parquet
    table based on an existing table (the existing table doesn’t need to be a Parquet
    table). Here’s an example:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 快速开始使用 Parquet 的一种方法是基于现有表（现有表不一定是 Parquet 表）创建一个新的 Parquet 表。以下是一个示例：
- en: '[PRE45]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then you can use an `INSERT` statement to copy the contents from the old table
    into the new Parquet table:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用 `INSERT` 语句将旧表的内容复制到新的 Parquet 表中：
- en: '[PRE46]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Now you can ditch your old table and start using your shiny new Parquet table!
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以丢弃旧表并开始使用您的新 Parquet 表！
- en: '[PRE47]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Alternatively, you can create a new table from scratch:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以从头创建一个新表：
- en: '[PRE48]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: One of the great things about Impala is that it allows the `INSERT ... VALUES`
    syntax, so you can easily get data into the table:^([[7](#ch09fn07)])
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: Impala 的一个优点是它允许使用 `INSERT ... VALUES` 语法，因此您可以轻松地将数据放入表中:^([[7](#ch09fn07)])
- en: ⁷ The use of `INSERT ... VALUES` isn’t recommended for large data loads. Instead,
    it’s more efficient to move files into your table’s HDFS directory, use the `LOAD
    DATA` statement, or use `INSERT INTO ... SELECT` or `CREATE TABLE AS SELECT ...`
    statements. The first two options will move files into the table’s HDFS directory,
    and the last two statements will load the data in parallel.
  id: totrans-358
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷ 不推荐使用 `INSERT ... VALUES` 进行大量数据加载。相反，将文件移动到表的 HDFS 目录中、使用 `LOAD DATA` 语句或使用
    `INSERT INTO ... SELECT` 或 `CREATE TABLE AS SELECT ...` 语句会更有效率。前两种选项会将文件移动到表的
    HDFS 目录中，后两种语句将并行加载数据。
- en: '[PRE49]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Parquet is a columnar storage format, so the fewer columns you select in your
    query, the faster your queries will execute. Selecting all the columns, as in
    the following example, can be considered an anti-pattern and should be avoided
    if possible:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 是一种列式存储格式，因此您在查询中选择的列越少，查询执行速度越快。在以下示例中，选择所有列可以被认为是一种反模式，如果可能的话应避免：
- en: '[PRE50]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Next, let’s look at how you can handle situations where the data in your tables
    is modified outside of Impala.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何处理在 Impala 外部修改表中数据的情况。
- en: Technique 96 Refreshing metadata
  id: totrans-363
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 96 刷新元数据
- en: If you make table or data changes inside of Impala, that information is automatically
    propagated to all the other Impala daemons to ensure that any subsequent queries
    will pick up that new data. But Impala (as of the 1.3 release) doesn’t handle
    cases where data is inserted into tables outside of Impala.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在 Impala 内部对表或数据进行更改，该信息将自动传播到所有其他 Impala 守护进程，以确保后续查询能够获取到新数据。但截至 1.3 版本，Impala
    无法处理在 Impala 外部插入到表中的数据的情况。
- en: Impala is also sensitive to the block placement of files that are in a table—if
    the HDFS balancer runs and relocates a block to another node, you’ll need to issue
    a refresh command to force Impala to reset the block locations cache.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: Impala 还对表中文件的块放置敏感——如果 HDFS 调平器运行并将块重新定位到另一个节点，您需要发出刷新命令来强制 Impala 重置块位置缓存。
- en: In this technique you’ll learn how to refresh a table in Impala so that it picks
    up the new data.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技术中，你将学习如何刷新Impala中的表，以便它能够获取新数据。
- en: Problem
  id: totrans-367
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You’ve inserted data into a Hive table outside of Impala.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 你已在Impala之外向Hive表插入了数据。
- en: Solution
  id: totrans-369
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the `REFRESH` statement.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`REFRESH`语句。
- en: Discussion
  id: totrans-371
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Impala daemons cache Hive metadata, including information about tables and block
    locations. Therefore, if data has been loaded into a table outside of Impala,
    you’ll need to use the `REFRESH` statement so that Impala can pull the latest
    metadata.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: Impala守护进程缓存Hive元数据，包括关于表和块位置的信息。因此，如果数据已加载到Impala之外的表，你需要使用`REFRESH`语句，以便Impala可以拉取最新的元数据。
- en: 'Let’s look at an example of this in action; we’ll work with the stocks table
    you created in technique 94\. Let’s add a new file into the external table’s directory
    with a quote for a brand new stock symbol:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个实际应用的例子；我们将使用你在技术94中创建的股票表。让我们向外部表的目录中添加一个包含全新股票代码报价的新文件：
- en: '[PRE51]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Bring up the Hive shell and you’ll immediately be able to see the stock:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 启动Hive shell，你将立即能够看到股票：
- en: '[PRE52]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Run the same query in Impala and you won’t see any results:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在Impala中运行相同的查询，你将看不到任何结果：
- en: '[PRE53]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'A quick `REFRESH` will remedy the situation:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 快速的`REFRESH`可以解决问题：
- en: '[PRE54]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '|  |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: What’s the difference between `REFRESH` and `INVALIDATE METADATA`?
  id: totrans-382
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '`REFRESH`和`INVALIDATE METADATA`之间有什么区别？'
- en: In the “[Using Hive tables in Impala](#ch09note08)” sidebar (see technique 94),
    you used the `INVALIDATE METADATA` command in Impala so that you could see a table
    that had been created in Hive. What’s the difference between the two commands?
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在“[在Impala中使用Hive表](#ch09note08)”侧边栏中（见技术94），你使用了Impala中的`INVALIDATE METADATA`命令，以便你能看到在Hive中创建的表。这两个命令之间有什么区别？
- en: The `INVALIDATE METADATA` command is more resource-intensive to execute, and
    it’s required when you want to refresh Impala’s state after creating, dropping,
    or altering a table using Hive. Once the table is visible in Impala, you should
    use the `REFRESH` command to update Impala’s state if new data is loaded, inserted,
    or changed.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '`INVALIDATE METADATA`命令执行时资源消耗更大，当你使用Hive创建、删除或修改表后想要刷新Impala的状态时，它是必需的。一旦表在Impala中可见，如果新数据被加载、插入或更改，你应该使用`REFRESH`命令来更新Impala的状态。'
- en: '|  |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Summary
  id: totrans-386
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: You don’t need to use `REFRESH` when you use Impala to insert and load data
    because Impala has an internal mechanism by which it shares metadata changes.
    Therefore, `REFRESH` is really only needed when loading data via Hive or when
    you’re externally manipulating files in HDFS.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Impala插入和加载数据时，你不需要使用`REFRESH`，因为Impala有一个内部机制，通过该机制它共享元数据更改。因此，`REFRESH`仅在通过Hive加载数据或当你在外部操作HDFS中的文件时才是必需的。
- en: 9.2.3\. User-defined functions in Impala
  id: totrans-388
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.3\. Impala中的用户定义函数
- en: Impala supports native UDFs written in C++, which ostensibly provide improved
    performance over their Hive counterparts. Coverage of the native UDFs is out of
    scope for this book, but Cloudera has excellent online documentation that comprehensively
    covers native UDFs.^([[8](#ch09fn08)]) Impala also supports using Hive UDFs, which
    we’ll explore in the next technique.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: Impala支持用C++编写的本地UDF，它们在性能上优于Hive的对应版本。本书不涵盖本地UDF的内容，但Cloudera有优秀的在线文档，全面涵盖了本地UDF。^([[8](#ch09fn08)])
    Impala还支持使用Hive UDF，我们将在下一个技术中探讨。
- en: ⁸ For additional details on Impala UDFs, refer to the “User-Defined Functions”
    page on Cloudera’s website at [http://mng.bz/319i](http://mng.bz/319i).
  id: totrans-390
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸ 关于Impala UDF的更多详细信息，请参阅Cloudera网站上的“用户定义函数”页面，网址为[http://mng.bz/319i](http://mng.bz/319i)。
- en: Technique 97 Executing Hive UDFs in Impala
  id: totrans-391
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术编号97 在Impala中执行Hive UDF
- en: If you’ve been working with Hive for a while, it’s likely that you’ve developed
    some UDFs that you regularly use in your queries. Luckily, Impala provides support
    for these Hive UDFs and allows you to use them without any change to the code
    or JARs.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一直在使用Hive，你很可能已经开发了一些你经常在查询中使用的UDF。幸运的是，Impala提供了对这些Hive UDF的支持，并允许你无需更改代码或JAR文件即可使用它们。
- en: Problem
  id: totrans-393
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to use custom or built-in Hive UDFs in Impala.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在Impala中使用自定义或内置的Hive UDF。
- en: Solution
  id: totrans-395
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Create a function in Impala referencing the JAR containing the UDF.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在Impala中创建一个函数，引用包含UDF的JAR文件。
- en: Discussion
  id: totrans-397
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Impala requires that the JAR containing the UDF be in HDFS:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: Impala要求包含UDF的JAR文件位于HDFS中：
- en: '[PRE55]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Next, in the Impala shell you’ll need to define a new function and point to
    the JAR location on HDFS and to the fully qualified class implementing the UDF.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在 Impala 命令行中，你需要定义一个新的函数，并指向 HDFS 上的 JAR 文件位置以及实现 UDF 的完全限定类。
- en: 'For this technique, we’ll use a UDF that’s packaged with Hive and converts
    the input data into a hex form. The UDF class is `UDFHex` and the following example
    creates a function for that class and gives it a logical name of `my_hex` to make
    it easier to reference it in your SQL:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种技术，我们将使用一个与 Hive 打包在一起的 UDF，它将输入数据转换为十六进制形式。UDF 类是 `UDFHex`，以下示例创建了一个该类的函数，并给它一个逻辑名称
    `my_hex`，以便在 SQL 中更容易引用它：
- en: '[PRE56]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'At this point you can use the UDF—here’s a simple example:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可以使用 UDF——以下是一个简单的示例：
- en: '[PRE57]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Summary
  id: totrans-405
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: What are the differences between using a Hive UDF in Hive versus using it in
    Impala?
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Hive 中使用 Hive UDF 与在 Impala 中使用它的区别是什么？
- en: The query language syntax for defining the UDF is different.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义 UDF 的查询语言语法是不同的。
- en: Impala requires you to define the argument types and the return type of the
    function. This means that even if the UDF is designed to work with any Hive type,
    the onus is on you to perform type conversion if the defined parameter type differs
    from the data type that you’re operating on.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Impala 要求你定义函数的参数类型和返回类型。这意味着即使 UDF 设计为与任何 Hive 类型一起工作，如果定义的参数类型与你要操作的数据类型不同，那么进行类型转换的责任就落在你身上。
- en: Impala currently doesn’t support complex types, so you can only return scalar
    types.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Impala 目前不支持复杂类型，因此你只能返回标量类型。
- en: Impala doesn’t support user-defined table functions.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Impala 不支持用户定义的表函数。
- en: This brings us to the end of our coverage of Impala. For a more detailed look
    at Impala, see Richard L. Saltzer and Istvan Szegedi’s book, *Impala in Action*
    (Manning, scheduled publication 2015).
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们对 Impala 的覆盖结束。要更详细地了解 Impala，请参阅 Richard L. Saltzer 和 Istvan Szegedi
    的著作，《Impala in Action》（Manning，计划于 2015 年出版）。
- en: Next let’s take a look at how you can use SQL inline with Spark for what may
    turn out to be the ultimate extract, transform, and load (ETL) and analytical
    tool in your toolbox.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何将 SQL 内联使用在 Spark 中，这可能成为你工具箱中最终的提取、转换和加载（ETL）和分析工具。
- en: 9.3\. Spark SQL
  id: totrans-413
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3. Spark SQL
- en: New SQL-on-Hadoop projects seem to pop up every day, but few look as promising
    as Spark SQL. Many believe that Spark is the future for Hadoop processing due
    to its simple APIs and efficient and flexible execution models, and the introduction
    of Spark SQL in the Spark 1.0 release only furthers the Spark toolkit.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 新的 SQL-on-Hadoop 项目似乎每天都有出现，但很少有像 Spark SQL 那样有前途的。许多人认为，由于 Spark 简单的 API 和高效灵活的执行模型，Spark
    是 Hadoop 处理的未来，而 Spark 1.0 版本中 Spark SQL 的引入进一步扩展了 Spark 工具包。
- en: Apache Spark is a cluster-computing engine that’s compatible with Hadoop. Its
    main selling points are enabling fast data processing by pinning datasets into
    memory across a cluster, and supporting a variety of ways for processing data,
    including Map-Reduce styles, iterative processing, and graph processing.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是一个与 Hadoop 兼容的集群计算引擎。其主要卖点是通过在集群中将数据集固定到内存中来实现快速数据处理，并支持多种数据处理方式，包括
    Map-Reduce 风格、迭代处理和图处理。
- en: Spark came out of UC Berkeley and became an Apache project in 2014\. It’s generating
    a lot of momentum due to its expressive language and because it lets you get up
    and running quickly with its API, which is currently defined in Java, Scala, and
    Python. In fact, Apache Mahout, the machine-learning project that historically
    has implemented its parallelizable algorithms in MapReduce, has recently stated
    that all new distributed algorithms will be implemented using Spark.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 诞生于加州大学伯克利分校，并于 2014 年成为 Apache 项目。由于其表达性语言和允许你通过其 API（目前定义在 Java、Scala
    和 Python 中）快速启动和运行，它正在产生巨大的动力。实际上，Apache Mahout，这个历史上在 MapReduce 中实现其并行化算法的机器学习项目，最近表示所有新的分布式算法都将使用
    Spark 实现。
- en: Early in Spark’s evolution, it used a system called Shark to provide a SQL interface
    to the Spark engine. More recently, in the Spark 1.0 release we were introduced
    to Spark SQL, which allows you to intermingle SQL with your Spark code. This promises
    a new Hadoop processing paradigm of intermixing SQL with non-SQL code.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 早期发展阶段，它使用了一个名为 Shark 的系统来为 Spark 引擎提供 SQL 接口。最近，在 Spark 1.0 版本中，我们介绍了
    Spark SQL，它允许你在 Spark 代码中混合使用 SQL。这预示着一种新的 Hadoop 处理范式，即混合使用 SQL 和非 SQL 代码。
- en: '|  |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: What’s the difference between Spark SQL and Shark?
  id: totrans-419
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Spark SQL 和 Shark 之间的区别是什么？
- en: Shark was the first Spark system that provided SQL abilities in Spark. Shark
    uses Hive for query planning and Spark for query execution. Spark SQL, on the
    other hand, doesn’t use the Hive query planner and instead uses its own planner
    (and execution) engine. The goal is to keep Shark as the Hive-compatible part
    of Spark, but there are plans to move to Spark SQL for query planning once Spark
    SQL has stabilized.^([[9](#ch09fn09)])
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: Shark是第一个在Spark中提供SQL能力的Spark系统。Shark使用Hive进行查询规划，使用Spark进行查询执行。另一方面，Spark SQL不使用Hive查询规划器，而是使用自己的规划器（和执行）引擎。目标是保持Shark作为Spark的Hive兼容部分，但计划在Spark
    SQL稳定后将其迁移到Spark SQL进行查询规划.^([[9](#ch09fn09)])
- en: '⁹ The future of Shark is discussed by Michael Armbrust and Reynold Xin, “Spark
    SQL: Manipulating Structured Data Using Spark,” [http://mng.bz/9057](http://mng.bz/9057).'
  id: totrans-421
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹ Michael Armbrust和Reynold Xin讨论了Shark的未来，“Spark SQL：使用Spark操作结构化数据”，[http://mng.bz/9057](http://mng.bz/9057)。
- en: '|  |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: In this section we’ll look at how you can work with SQL in Spark and also look
    at its SQL-like APIs, which offer a fluent style to compose your queries in.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨如何在Spark中处理SQL，并查看其类似SQL的API，这些API提供了一种流畅的风格来编写查询。
- en: '|  |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Production readiness of Spark SQL
  id: totrans-425
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Spark SQL的生产就绪性
- en: At the time of writing, Spark 1.0 has been released, which introduced Spark
    SQL for the first time. It is currently labeled as alpha quality and is being
    actively developed.^([[10](#ch09fn10)]) As a result, the code in this section
    may differ from the production-ready Spark SQL API.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Spark 1.0已经发布，它首次引入了Spark SQL。它目前被标记为alpha质量，并且正在积极开发中.^([[10](#ch09fn10)])
    因此，本节中的代码可能与生产就绪的Spark SQL API不同。
- en: ^(10) Michael Armbrust and Zongheng Yang, “Exciting Performance Improvements
    on the Horizon for Spark SQL,” [http://mng.bz/efqV](http://mng.bz/efqV).
  id: totrans-427
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(10) Michael Armbrust和Zongheng Yang，“Spark SQL的前景令人兴奋的性能改进”，[http://mng.bz/efqV](http://mng.bz/efqV)。
- en: '|  |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Before we get started with Spark SQL, let’s become familiar with Spark by looking
    at some simple Spark examples.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始学习Spark SQL之前，让我们通过查看一些简单的Spark示例来熟悉Spark。
- en: 9.3.1\. Spark 101
  id: totrans-430
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.1\. Spark 101
- en: Spark consists of a core set of APIs and an execution engine, on top of which
    exist other Spark systems that provide APIs and processing capabilities for specialized
    activities, such as designing stream-processing pipelines. The core Spark systems
    are shown in [figure 9.7](#ch09fig07).
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: Spark由一组核心API和执行引擎组成，在其之上存在其他Spark系统，这些系统提供API和针对特定活动的处理能力，例如设计流处理管道。核心Spark系统如图9.7所示。
- en: Figure 9.7\. Spark systems
  id: totrans-432
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.7\. Spark系统
- en: '![](09fig07_alt.jpg)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig07_alt.jpg)'
- en: The Spark components can be seen in [figure 9.8](#ch09fig08). The Spark driver
    is responsible for communicating with a cluster manager to execute operations
    and the Spark executors handle the actual operation execution and data management.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: Spark组件如图9.8所示。Spark驱动程序负责与集群管理器通信以执行操作，而Spark执行器处理实际的操作执行和数据管理。
- en: Figure 9.8\. Spark architecture
  id: totrans-435
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.8\. Spark架构
- en: '![](09fig08_alt.jpg)'
  id: totrans-436
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig08_alt.jpg)'
- en: Data in Spark is represented using RDDs (resilient distributed datasets), which
    are an abstraction over a collection of items. RDDs are distributed over a cluster
    so that each cluster node will store and manage a certain range of the items in
    an RDD. RDDs can be created from a number of sources, such as regular Scala collections
    or data from HDFS (synthesized via Hadoop input format classes). RDDs can be in-memory,
    on disk, or a mix of the two.^([[11](#ch09fn11)])
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中的数据使用RDDs（弹性分布式数据集）表示，它是对一系列项目的抽象。RDDs在集群上分布，以便每个集群节点将存储和管理RDD中一定范围内的项目。RDD可以从多个来源创建，例如常规Scala集合或来自HDFS（通过Hadoop输入格式类合成）的数据。RDD可以是内存中的，也可以是磁盘上的，或者两者兼而有之.^([[11](#ch09fn11)])
- en: ^(11) More information on RDD caching and persistence can be found in the Spark
    Programming Guide at [https://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence](https://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence).
  id: totrans-438
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^(11) 关于RDD缓存和持久化的更多信息，可以在Spark编程指南中找到，网址为[https://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence](https://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence)。
- en: 'The following example shows how an RDD can be created from a text file:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何从文本文件创建RDD：
- en: '[PRE58]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The Spark `RDD` class has various operations that you can perform on the RDD.
    RDD operations in Spark fall into two categories—transformations and actions:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: Spark `RDD`类具有各种可以在RDD上执行的操作。Spark中的RDD操作分为两类——转换和动作：
- en: '*Transformations* operate on an RDD to create a new RDD. Examples of transformation
    functions include `map`, `flatMap`, `reduceByKey`, and `distinct`.^([[12](#ch09fn12)])'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*转换* 在 RDD 上操作以创建一个新的 RDD。转换函数的示例包括 `map`、`flatMap`、`reduceByKey` 和 `distinct`.^([12](#ch09fn12)）'
- en: ^(12) A more complete list of transformations is shown in the Spark Programming
    Guide at [https://spark.apache.org/docs/latest/programming-guide.html#transformations](https://spark.apache.org/docs/latest/programming-guide.html#transformations).
  id: totrans-443
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([12](#ch09fn12)) 更完整的转换列表可以在 Spark 编程指南中找到，[https://spark.apache.org/docs/latest/programming-guide.html#transformations](https://spark.apache.org/docs/latest/programming-guide.html#transformations)。
- en: '*Actions* perform some activity over an RDD, after which they return results
    to the driver. For example, the `collect` function returns the entire RDD contents
    to the driver process, and the `take` function allows you to select the first
    *N* items in a dataset.^([[13](#ch09fn13)])'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*操作* 在 RDD 上执行一些活动，之后将结果返回给驱动程序。例如，`collect` 函数将整个 RDD 内容返回给驱动进程，而 `take` 函数允许您选择数据集中的前
    *N* 个项目.^([13](#ch09fn13)）'
- en: ^(13) A more complete list of actions can be found in the Spark Programming
    Guide at [https://spark.apache.org/docs/latest/programming-guide.html#actions](https://spark.apache.org/docs/latest/programming-guide.html#actions).
  id: totrans-445
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([13](#ch09fn13)) 更完整的操作列表可以在 Spark 编程指南中找到，[https://spark.apache.org/docs/latest/programming-guide.html#actions](https://spark.apache.org/docs/latest/programming-guide.html#actions)。
- en: '|  |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Lazy transformations
  id: totrans-447
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 懒惰转换
- en: Spark will lazily evaluate transformations, so you actually need to execute
    an action for Spark to execute your operations.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 会延迟评估转换，因此您实际上需要执行一个操作，Spark 才会执行您的操作。
- en: '|  |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Let’s take a look at an example of a Spark application that calculates the
    average stock price for each symbol. To run the example, you’ll need to have Spark
    installed,^([[14](#ch09fn14)]) after which you can launch the shell:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个 Spark 应用程序的例子，该程序计算每个符号的平均股票价格。要运行此示例，您需要安装 Spark，^([14](#ch09fn14))
    安装完成后，您可以启动 shell：
- en: ^(14) To install and configure Spark on YARN, follow the instructions on “Running
    Spark on YARN” at [http://spark.apache.org/docs/latest/running-on-yarn.html](http://spark.apache.org/docs/latest/running-on-yarn.html).
  id: totrans-451
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([14](#ch09fn14)) 在 YARN 上安装和配置 Spark，请遵循“在 YARN 上运行 Spark”页面上的说明，[http://spark.apache.org/docs/latest/running-on-yarn.html](http://spark.apache.org/docs/latest/running-on-yarn.html)。
- en: '![](418fig01_alt.jpg)'
  id: totrans-452
  prefs: []
  type: TYPE_IMG
  zh: '![](418fig01_alt.jpg)'
- en: This was a very brief introduction to Spark—the Spark online documentation is
    excellent and is worth exploring to learn more about Spark.^([[15](#ch09fn15)])
    Let’s now turn to an introduction to how Spark works with Hadoop.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对 Spark 的一个非常简短的介绍——Spark 在线文档非常出色，值得探索以了解更多关于 Spark 的信息.^([[15](#ch09fn15)])
    现在我们来介绍 Spark 如何与 Hadoop 一起工作。
- en: ^(15) A great starting place for learning about Spark is the Spark Programming
    Guide, [http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html).
  id: totrans-454
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([15](#ch09fn15)) 学习 Spark 的一个好起点是 Spark 编程指南，[http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html)。
- en: 9.3.2\. Spark on Hadoop
  id: totrans-455
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.2\. Spark on Hadoop
- en: 'Spark supports several cluster managers, one of them being YARN. In this mode,
    the Spark executors are YARN containers, and the Spark ApplicationMaster is responsible
    for managing the Spark executors and sending them commands. The Spark driver is
    either contained within the client process or inside the ApplicationMaster, depending
    on whether you’re running in client mode or cluster mode:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 支持多个集群管理器，其中之一是 YARN。在此模式下，Spark 执行器是 YARN 容器，Spark ApplicationMaster
    负责管理 Spark 执行器并向它们发送命令。Spark 驱动程序位于客户端进程内部或 ApplicationMaster 内部，具体取决于您是在客户端模式还是集群模式下运行：
- en: In *client mode* the driver resides inside the client, which means that executing
    a series of Spark tasks in this mode will be interrupted if the client process
    is terminated. This mode works well for the Spark shell, but it isn’t suitable
    for use when Spark is being used in a non-interactive method.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 *客户端模式* 下，驱动程序位于客户端内部，这意味着在此模式下执行一系列 Spark 任务将会被中断，如果客户端进程被终止。这种模式适合 Spark
    shell，但不适合在 Spark 以非交互方式使用时使用。
- en: In *cluster mode* the driver executes in the ApplicationMaster and doesn’t rely
    on the client to exist in order to execute tasks. This mode works best for cases
    where you have some existing Spark code that you wish to execute and that doesn’t
    require any interaction from you.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 *集群模式* 下，驱动程序在 ApplicationMaster 中执行，并且不需要客户端存在即可执行任务。这种模式最适合您有一些现有的 Spark
    代码需要执行，且不需要您进行任何交互的情况。
- en: '[Figure 9.9](#ch09fig09) shows the architecture of Spark running on YARN.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9.9](#ch09fig09) 展示了 Spark 在 YARN 上运行的架构。'
- en: Figure 9.9\. Spark running on YARN
  id: totrans-460
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.9\. Spark 在 YARN 上运行
- en: '![](09fig09_alt.jpg)'
  id: totrans-461
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig09_alt.jpg)'
- en: The default installation of Spark is set up for standalone mode, so you’ll have
    to configure Spark to make it work with YARN.^([[16](#ch09fn16)]) The Spark scripts
    and tools don’t change when you’re running on YARN, so once you’ve configured
    Spark to use YARN, you can run the Spark shell just like you did in the previous
    example.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的默认安装设置为独立模式，因此您必须配置 Spark 以使其与 YARN 一起工作。[16](#ch09fn16) Spark 脚本和工具在运行在
    YARN 上时不会改变，因此一旦您配置了 Spark 以使用 YARN，您就可以像上一个示例中那样运行 Spark shell。
- en: ^(16) Follow the instructions at [https://spark.apache.org/docs/latest/running-on-yarn.html](https://spark.apache.org/docs/latest/running-on-yarn.html)
    to set up Spark to use YARN.
  id: totrans-463
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([16) 按照以下说明在 [https://spark.apache.org/docs/latest/running-on-yarn.html](https://spark.apache.org/docs/latest/running-on-yarn.html)
    中设置 Spark 以使用 YARN。
- en: Now that you understand some Spark basics and how it works on YARN, let’s look
    at how you can execute SQL using Spark.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了 Spark 的基础知识以及它在 YARN 上的工作方式，让我们看看您如何使用 Spark 执行 SQL。
- en: 9.3.3\. SQL with Spark
  id: totrans-465
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.3\. 使用 Spark 的 SQL
- en: 'This section covers Spark SQL, which is part of the core Spark system. Three
    areas of Spark SQL will be examined: executing SQL against your RDDs, using integrated
    query language features that provide a more expressive way to work with your data,
    and integrating HiveQL with Spark.'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了 Spark SQL，它是 Spark 核心系统的一部分。Spark SQL 将涵盖三个领域：对 RDD 执行 SQL、使用集成查询语言特性以提供更丰富的数据处理方式，以及将
    HiveQL 与 Spark 集成。
- en: '|  |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Stability of Spark SQL
  id: totrans-468
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Spark SQL 的稳定性
- en: Spark SQL is currently labeled as alpha quality, so it’s probably best not to
    use it in your production code until it’s marked as production-ready.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 目前被标记为 alpha 质量，因此最好在它被标记为生产就绪之前不要在生产代码中使用它。
- en: '|  |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Technique 98 Calculating stock averages with Spark SQL
  id: totrans-471
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 98 使用 Spark SQL 计算股票平均价
- en: In this technique you’ll learn how to use Spark SQL to calculate the average
    price for each stock symbol.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技巧中，您将学习如何使用 Spark SQL 来计算每个股票符号的平均价格。
- en: Problem
  id: totrans-473
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You have a Spark processing pipeline, and expressing your functions would be
    simpler using SQL as opposed to the Spark APIs.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一个 Spark 处理管道，使用 SQL 表达函数比使用 Spark API 更简单。
- en: Solution
  id: totrans-475
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Register an RDD as a table and use the Spark `sql` function to execute SQL against
    the RDD.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 将 RDD 注册为表，并使用 Spark 的 `sql` 函数对 RDD 执行 SQL。
- en: Discussion
  id: totrans-477
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'The first step in this technique is to define a class that will represent each
    record in your Spark table. In this example, you’ll calculate the stock price
    averages, so all you need is a class with two fields to store the stock symbol
    and price:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 本技巧的第一步是定义一个类，该类将代表 Spark 表中的每条记录。在这个例子中，您将计算股票价格的平均值，因此您只需要一个包含两个字段的类来存储股票符号和价格：
- en: '[PRE59]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '|  |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Why use Scala for Spark examples?
  id: totrans-481
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为什么在 Spark 示例中使用 Scala？
- en: In this section we’ll use Scala to show Spark examples. The Scala API, until
    recently, has been much more concise than Spark’s Java API, although with the
    release of Spark 1.0, the Java support in Spark now uses lambdas to expose a less
    verbose API.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 Scala 来展示 Spark 示例。直到最近，Scala API 比 Spark 的 Java API 更简洁，尽管随着 Spark
    1.0 的发布，Spark 现在的 Java 支持使用 lambdas 来提供一个更简洁的 API。
- en: '|  |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Next you need to register an RDD of these `Stock` objects as a table so that
    you can perform SQL operations on it. You can create a table from any Spark RDD.
    The following example shows how you can load the stocks data from HDFS and register
    it as a table:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要将这些 `Stock` 对象的 RDD 注册为表，以便您可以在其上执行 SQL 操作。您可以从任何 Spark RDD 创建一个表。以下示例展示了您如何从
    HDFS 加载股票数据并将其注册为表：
- en: '![](421fig01_alt.jpg)'
  id: totrans-485
  prefs: []
  type: TYPE_IMG
  zh: '![](421fig01_alt.jpg)'
- en: 'Now you’re ready to issue queries against the stocks table. The following shows
    how you’d calculate the average price for each symbol:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以对股票表发出查询。以下是如何计算每个符号的平均价格：
- en: '[PRE60]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The `sql` function returns a `SchemaRDD`, which supports standard RDD operations.
    This is where the novel aspect of Spark SQL comes into play—mixing SQL and regular
    data processing paradigms together. You use SQL to create an RDD and you can then
    immediately turn around and execute your usual Spark transformations over that
    data.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '`sql` 函数返回一个 `SchemaRDD`，它支持标准的 RDD 操作。这正是 Spark SQL 的独特之处——将 SQL 和常规数据处理范式结合起来。您使用
    SQL 创建一个 RDD，然后可以立即对那些数据执行常规的 Spark 转换。'
- en: In addition to supporting the standard Spark RDD operations, `SchemaRDD` also
    allows you to execute SQL-like functions such as `where` and `join` over the data,
    which is covered in the next technique.^([[17](#ch09fn17)])
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 除了支持标准的Spark RDD操作外，`SchemaRDD`还允许你在数据上执行类似SQL的函数，如`where`和`join`，这将在下一个技术中介绍.^([[17](#ch09fn17)])
- en: ^(17) Language-integrated queries that allow more natural language expression
    of queries can be seen at the Scala docs for the `SchemaRDD` class at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SchemaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SchemaRDD).
  id: totrans-490
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (17) 允许以更自然语言表达查询的语言集成查询可以在`SchemaRDD`类的Scala文档中看到，链接为[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SchemaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SchemaRDD).
- en: Technique 99 Language-integrated queries
  id: totrans-491
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术编号 99：语言集成查询
- en: The previous technique demonstrated how you can execute SQL over your Spark
    data. Spark 1.0 also introduced a feature called language-integrated queries,
    which expose SQL constructs as functions, allowing you to craft code that’s not
    only fluent but that expresses operations using natural language constructs. In
    this technique you’ll see how to use these functions on your RDDs.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的技术展示了如何在Spark数据上执行SQL。Spark 1.0还引入了一个名为语言集成查询的功能，它将SQL结构作为函数暴露出来，允许你编写不仅流畅而且使用自然语言结构表达操作的代码。在这个技术中，你将看到如何在你自己的RDD上使用这些函数。
- en: Problem
  id: totrans-493
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: Although the Spark RDD functions are expressive, they don’t yield code that
    is particularly human-readable.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Spark RDD函数具有表现力，但它们生成的代码并不特别适合人类阅读。
- en: Solution
  id: totrans-495
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use Spark’s language-integrated queries.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark的语言集成查询。
- en: Discussion
  id: totrans-497
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Once again, let’s try to calculate the average stock prices, this time using
    language-integrated queries. This example uses the `groupBy` function to calculate
    the average stock price:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 再次尝试计算平均股票价格，这次使用语言集成查询。此示例使用`groupBy`函数来计算平均股票价格：
- en: '![](422fig01_alt.jpg)'
  id: totrans-499
  prefs: []
  type: TYPE_IMG
  zh: '![422fig01_alt.jpg](422fig01_alt.jpg)'
- en: The preceding code leverages the `Average` and `First` aggregate functions—there
    are other aggregate functions such as `Count`, `Min`, and `Max`, among others.^([[18](#ch09fn18)])
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码利用了`Average`和`First`聚合函数——还有其他聚合函数，如`Count`、`Min`和`Max`等.^([[18](#ch09fn18)])
- en: '^(18) See the code at the following link for the complete list: [https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregates.scala](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregates.scala).'
  id: totrans-501
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: (18) 请参阅以下链接以获取完整列表的代码：[https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregates.scala](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregates.scala).
- en: 'The next is more straightforward; it simply selects all the quotes for days
    where the value was over $100:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例更为直接；它只是简单地选择价值超过100美元的日期的所有报价：
- en: '[PRE61]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The third option with Spark SQL is to use HiveQL, which is useful when you want
    to execute more complex SQL grammar.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL的第三种选择是使用HiveQL，这在需要执行更复杂的SQL语法时很有用。
- en: Technique 100 Hive and Spark SQL
  id: totrans-505
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术编号 100：Hive和Spark SQL
- en: You can also work with data in Hive tables in Spark. This technique examines
    how you can execute a query against a Hive table.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在Spark中使用Hive表中的数据。这个技术探讨了如何对Hive表执行查询。
- en: Problem
  id: totrans-507
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to work with Hive data in Spark.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在Spark中处理Hive数据。
- en: Solution
  id: totrans-509
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use Spark’s `HiveContext` to issue HiveQL statements and work with the results
    in Spark.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark的`HiveContext`发布HiveQL语句并在Spark中处理结果。
- en: Discussion
  id: totrans-511
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Earlier in this chapter you created a stocks table in Hive (in technique 89).
    Let’s query that stocks table using HiveQL from within Spark and then perform
    some additional manipulations within Spark:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期，你已经在Hive中创建了一个股票表（在技术编号89中）。现在让我们使用HiveQL在Spark中查询这个股票表，然后在Spark中进行一些额外的操作：
- en: '![](423fig01_alt.jpg)'
  id: totrans-513
  prefs: []
  type: TYPE_IMG
  zh: '![423fig01_alt.jpg](423fig01_alt.jpg)'
- en: You have access to the complete HiveQL grammar in Spark, as the commands that
    are wrapped inside the `hql` calls are sent directly to Hive. You can load tables,
    insert into tables, and perform any Hive command that’s needed, all directly from
    Spark. Spark’s Hive integration also includes support for using Hive UDFs, UDAFs,
    and UDTFs in your queries.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，你可以访问完整的HiveQL语法，因为包裹在`hql`调用中的命令直接发送到Hive。你可以加载数表，向表中插入数据，并执行任何所需的Hive命令，所有这些都可以直接从Spark完成。Spark的Hive集成还包括在查询中使用Hive
    UDFs、UDAFs和UDTFs的支持。
- en: This completes our brief look at Spark SQL.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们对Spark SQL的简要了解。
- en: 9.4\. Chapter summary
  id: totrans-516
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4. 章节总结
- en: SQL access to data in Hadoop is essential for organizations, as not all users
    who want to interact with data are programmers. SQL is often the lingua franca
    for not only data analysts but also for data scientists and nontechnical members
    of your organization.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 对于组织来说，通过Hadoop访问数据是至关重要的，因为并非所有希望与数据交互的用户都是程序员。SQL不仅是数据分析师的通用语言，也是您组织中的数据科学家和非技术成员的通用语言。
- en: In this chapter I introduced three tools that can be used to work with your
    data via SQL. Hive has been around the longest and is currently the most full-featured
    SQL engine you can use. Impala is worth a serious look if Hive is not providing
    a rapid enough level of interaction with your data. And finally, Spark SQL provides
    a glimpse into the future, where technical members of your organization such as
    programmers and data scientists can fuse together SQL and Scala to build complex
    and efficient processing pipelines.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我介绍了三个可以用来通过SQL处理数据的工具。Hive存在时间最长，目前是您可以使用功能最全面的SQL引擎。如果Hive无法提供足够快的与数据交互的速度，Impala则值得认真考虑。最后，Spark
    SQL为未来提供了一个窗口，在这个窗口中，您组织中的技术成员，如程序员和数据科学家，可以将SQL和Scala融合在一起，构建复杂且高效的处理管道。
- en: Chapter 10\. Writing a YARN application
  id: totrans-519
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第10章. 编写YARN应用
- en: '*This chapter covers*'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Understanding key capabilities of a YARN application
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解YARN应用的关键功能
- en: How to write a basic YARN application
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何编写基本的YARN应用
- en: An examination of YARN frameworks and applications
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对YARN框架和应用的考察
- en: Looking at the source code for any reasonably sized YARN application typically
    results in words like “complex” and “low-level” being thrown around. At its core,
    writing a YARN application isn’t that complex, as you’ll discover in this chapter.
    The complexity with YARN is typically introduced once you need to build more advanced
    features into your application, such as supporting secure Hadoop clusters or handling
    failure scenarios, which are complicated in distributed systems regardless of
    the framework. That being said, there are emerging frameworks that abstract away
    the YARN APIs and provide common features that you’ll require.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 查看任何合理规模的YARN应用的源代码通常会导致人们使用“复杂”和“低级”这样的词汇。从本质上讲，编写YARN应用并不复杂，正如您在本章中将发现的那样。YARN的复杂性通常在您需要将更高级的功能构建到应用中时出现，例如支持安全的Hadoop集群或处理故障场景，这在分布式系统中无论框架如何都是复杂的。尽管如此，有一些新兴的框架抽象了YARN
    API，并提供了您所需的一些常见功能。
- en: In this chapter, you’ll write a simple YARN application that will run a Linux
    command on a node in the cluster. Once you’ve run your application, you’ll be
    introduced to some of the more advanced features that you may need in your YARN
    application. Finally, this chapter looks at some of the open source YARN abstractions
    and examine their features.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将编写一个简单的YARN应用，该应用将在集群中的一个节点上运行Linux命令。一旦您运行了您的应用，您将了解您在YARN应用中可能需要的更高级的功能。最后，本章将探讨一些开源的YARN抽象，并检查它们的功能。
- en: Before we get started, let’s ease into YARN programming by looking at the building
    blocks of a YARN application.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，让我们通过查看YARN应用的构建块来逐步了解YARN编程。
- en: 10.1\. Fundamentals of building a YARN application
  id: totrans-527
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1. 构建YARN应用的基础
- en: This section provides a brief high-level overview of the YARN actors and the
    basic communication flows that you’ll need to support in your YARN application.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了对YARN演员和您在YARN应用中需要支持的基本通信流程的简要概述。
- en: 10.1.1\. Actors
  id: totrans-529
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.1.1. 演员
- en: There are five separate pieces of a YARN application that are either part of
    the YARN framework or components that you must create yourself (which I call the
    *user space*), all of which are shown in [figure 10.1](#ch10fig01).
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: YARN应用由五个独立的部分组成，这些部分要么是YARN框架的一部分，要么是您必须自己创建的组件（我称之为“用户空间”），所有这些都在[图10.1](#ch10fig01)中展示。
- en: Figure 10.1\. The main actors and communication paths in a YARN application
  id: totrans-531
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.1. YARN应用中的主要演员和通信路径
- en: '![](10fig01_alt.jpg)'
  id: totrans-532
  prefs: []
  type: TYPE_IMG
  zh: '![10fig01_alt.jpg](10fig01_alt.jpg)'
- en: The actors in a YARN application and the YARN framework include
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: YARN应用中的演员和YARN框架包括
- en: '***YARN client*** —The YARN client, in the user space, is responsible for launching
    the YARN application. It sends `createApplication` and `submitApplication` requests
    to the ResourceManager and can also kill the application.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***YARN客户端*** —在用户空间中，YARN客户端负责启动YARN应用。它向ResourceManager发送`createApplication`和`submitApplication`请求，也可以终止应用。'
- en: '***ResourceManager*** —In the framework, a single cluster-wide ResourceManager
    is responsible for receiving container allocation requests and asynchronously
    notifying clients when resources become available for their containers.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***ResourceManager*** —在框架中，单个集群范围的ResourceManager负责接收容器分配请求，并在资源对容器可用时异步通知客户端。'
- en: '***ApplicationMaster*** —The ApplicationMaster in the user space is the main
    coordinator for an application, and it works with the ResourceManager and NodeManagers
    to request and launch containers.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***ApplicationMaster*** —用户空间中的ApplicationMaster是应用程序的主要协调器，它与ResourceManager和NodeManagers协作请求和启动容器。'
- en: '***NodeManager*** —In the framework, each node runs a NodeManager that’s responsible
    for servicing client requests to launch and kill containers.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***NodeManager*** —在框架中，每个节点运行一个NodeManager，负责处理启动和终止容器的客户端请求。'
- en: '***Container*** —The container in the user space is an application-specific
    process that performs work on behalf of the application. A container could be
    a simple fork of an existing Linux process (such as the `find` command to find
    files), or an application-developed service such as a map or reduce task for MapReduce
    YARN applications.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Container*** —用户空间中的容器是代表应用程序执行工作的特定应用程序进程。容器可以是现有Linux进程的简单分支（例如，使用`find`命令查找文件），也可以是应用程序开发的映射或减少任务，例如MapReduce
    YARN应用程序。'
- en: The following sections discuss these actors and their role in your Yarn application.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 以下几节将讨论这些参与者及其在您的Yarn应用中的作用。
- en: 10.1.2\. The mechanics of a YARN application
  id: totrans-540
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.1.2\. YARN应用的力学原理
- en: When implementing a YARN application, there are a number of interactions that
    you need to support. Let’s examine each interaction and what information is relayed
    between the components.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现YARN应用时，您需要支持多种交互。让我们检查每个交互以及组件之间传递的信息。
- en: Resource Allocation
  id: totrans-542
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 资源分配
- en: When the YARN client or the ApplicationMaster asks the ResourceManager for a
    new container, they indicate the resources that the container needs in a `Resource`
    object. In addition, the ApplicationMaster sends some more attributes in a `ResourceRequest`,
    as shown in [figure 10.2](#ch10fig02).
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 当YARN客户端或ApplicationMaster向ResourceManager请求新的容器时，它们在一个`Resource`对象中指明容器需要的资源。此外，ApplicationMaster在`ResourceRequest`中发送一些额外的属性，如图10.2所示。
- en: Figure 10.2\. Resource properties that can be requested for a container
  id: totrans-544
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.2\. 可请求的容器资源属性
- en: '![](10fig02.jpg)'
  id: totrans-545
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig02.jpg)'
- en: The `resourceName` specifies the host and rack where the container should be
    executed, and it can be wildcarded with an asterisk to inform the ResourceManager
    that the container can be launched on any node in the cluster.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '`resourceName`指定容器应执行的主机和机架，并且可以用星号通配符表示，以通知ResourceManager容器可以在集群中的任何节点上启动。'
- en: The ResourceManager responds to a resource request with a `Container` object
    that represents a single unit of execution (a process). The container includes
    an ID, a `resourceName`, and other attributes. Once the YARN client or ApplicationMaster
    receives this message from the ResourceManager, it can communicate with the NodeManager
    to launch the container.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: ResourceManager对资源请求响应时，返回一个表示单个执行单元（进程）的`Container`对象。容器包括一个ID、`resourceName`和其他属性。一旦YARN客户端或ApplicationMaster从ResourceManager接收到此消息，它就可以与NodeManager通信以启动容器。
- en: Launching a container
  id: totrans-548
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 启动容器
- en: Once a client receives the `Container` from the ResourceManager, it’s ready
    to talk to the NodeManager associated with the container to launch the container.
    [Figure 10.3](#ch10fig03) shows the information that the client sends to the NodeManager
    as part of the request.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦客户端从ResourceManager接收了`Container`，它就准备好与关联的NodeManager通信以启动容器。[图10.3](#ch10fig03)显示了客户端作为请求一部分发送给NodeManager的信息。
- en: Figure 10.3\. Container request metadata
  id: totrans-550
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.3\. 容器请求元数据
- en: '![](10fig03.jpg)'
  id: totrans-551
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig03.jpg)'
- en: The NodeManager is responsible for downloading any local resources identified
    in the request (including items such as any libraries required by the application
    or files in the distributed cache) from HDFS. Once these files are downloaded,
    the NodeManager launches the container process.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: NodeManager负责从HDFS下载请求中标识的任何本地资源（包括应用程序所需的任何库或分布式缓存中的文件等）。一旦这些文件下载完成，NodeManager将启动容器进程。
- en: With these YARN preliminaries out of the way, let’s go ahead and start writing
    a YARN application.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些YARN预备知识完成后，让我们继续编写一个YARN应用。
- en: 10.2\. Building a YARN application to collect cluster statistics
  id: totrans-554
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2\. 构建用于收集集群统计信息的 YARN 应用程序
- en: In this section you’ll build a simple YARN application that will launch a single
    container to execute the `vmstat` Linux command. As you build this simple example,
    we’ll focus on the plumbing needed to get a YARN application up and running. The
    next section covers the more advanced capabilities that you’ll likely require
    in a full-blown YARN application.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将构建一个简单的 YARN 应用程序，该程序将启动一个容器来执行 `vmstat` Linux 命令。在构建这个简单示例的过程中，我们将关注使
    YARN 应用程序启动和运行所需的管道。下一节将介绍你可能在完整 YARN 应用程序中需要的高级功能。
- en: '[Figure 10.4](#ch10fig04) shows the various components that you’ll build in
    this section and their interactions with the YARN framework.'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.4](#ch10fig04) 展示了在本节中你将构建的各种组件以及它们与 YARN 框架的交互。'
- en: Figure 10.4\. An overview of the YARN application that you’ll build
  id: totrans-557
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '![图10.4\. 你将构建的 YARN 应用程序的概述](#ch10fig04)'
- en: '![](10fig04_alt.jpg)'
  id: totrans-558
  prefs: []
  type: TYPE_IMG
  zh: '![10fig04_alt.jpg](10fig04_alt.jpg)'
- en: Let’s get started by building the YARN client.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从构建 YARN 客户端开始。
- en: Technique 101 A bare-bones YARN client
  id: totrans-560
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 101 一个基本的 YARN 客户端
- en: The role of the YARN client is to negotiate with the ResourceManager for a YARN
    application instance to be created and launched. As part of this work, you’ll
    need to inform the ResourceManager about the system resource requirements of your
    ApplicationMaster. Once the ApplicationMaster is up and running, the client can
    choose to monitor the status of the application.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: YARN 客户端的作用是与 ResourceManager 协商以创建和启动 YARN 应用程序实例。作为这项工作的部分，你需要向 ResourceManager
    通知你的 ApplicationMaster 的系统资源需求。一旦 ApplicationMaster 启动并运行，客户端可以选择监控应用程序的状态。
- en: This technique will show you how to write a client that performs the three steps
    illustrated in [figure 10.5](#ch10fig05).
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术将向你展示如何编写一个执行 [图10.5](#ch10fig05) 中展示的三项活动的客户端。
- en: Figure 10.5\. The three activities that your YARN client will perform
  id: totrans-563
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '![图10.5\. 你的 YARN 客户端将执行的三项活动](#ch10fig05)'
- en: '![](10fig05.jpg)'
  id: totrans-564
  prefs: []
  type: TYPE_IMG
  zh: '![10fig05.jpg](10fig05.jpg)'
- en: Problem
  id: totrans-565
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You’re building a YARN application, so you need to write a client to launch
    your application.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在构建一个 YARN 应用程序，因此你需要编写一个客户端来启动你的应用程序。
- en: Solution
  id: totrans-567
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the `YarnClient` class to create and submit a YARN application.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `YarnClient` 类创建和提交 YARN 应用程序。
- en: Discussion
  id: totrans-569
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Let’s walk through the code for each of the steps highlighted in [figure 10.5](#ch10fig05),
    starting with creating a new YARN application.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个分析 [图10.5](#ch10fig05) 中突出显示的每个步骤的代码，从创建一个新的 YARN 应用程序开始。
- en: Creating a YARN application
  id: totrans-571
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 创建 YARN 应用程序
- en: The first thing your YARN client needs to do is communicate with the ResourceManager
    about its intent to start a new YARN application. The response from the ResourceManager
    is a unique application ID that’s used to create the application and that’s also
    supported by the YARN command line for queries such as retrieving logs.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 你的 YARN 客户端需要做的第一件事是与 ResourceManager 通信，告知其启动新 YARN 应用程序的意图。ResourceManager
    的响应是一个唯一的应用程序 ID，用于创建应用程序，并且 YARN 命令行也支持查询日志等操作。
- en: The following code shows how you can get a handle to a `YarnClient` instance
    and use that to create the application:^([[1](#ch10fn01)])
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何获取 `YarnClient` 实例的句柄，并使用它来创建应用程序^([[1](#ch10fn01)])。
- en: '¹ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/Client.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/Client.java).'
  id: totrans-574
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹ GitHub 源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/Client.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/Client.java)。
- en: '![](430fig01_alt.jpg)'
  id: totrans-575
  prefs: []
  type: TYPE_IMG
  zh: '![430fig01_alt.jpg](430fig01_alt.jpg)'
- en: The `createApplication` method will call the ResourceManager, which will return
    a new application ID. In addition, the `YarnClientApplication` object contains
    information about the cluster, such as the resource capabilities that can be used
    to predetermine container resource properties.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: '`createApplication` 方法将调用 ResourceManager，它将返回一个新的应用程序 ID。此外，`YarnClientApplication`
    对象包含有关集群的信息，例如可用于预定义容器资源属性的可用资源能力。'
- en: The `YarnClient` class used in the preceding code contains a number of APIs
    that result in an RPC call to the ResourceManager. Some of these methods are shown
    in the following extract from the code:^([[2](#ch10fn02)])
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面代码中使用的 `YarnClient` 类包含多个API，这些API会导致对 ResourceManager 的 RPC 调用。其中一些方法在以下代码摘录中有所展示^([[2](#ch10fn02)])。
- en: '² Some queue and security APIs were omitted from the `YarnClient` class—refer
    to the `YarnClient` Javadocs for the complete API: [http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/yarn/client/api/YarnClient.html](http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/yarn/client/api/YarnClient.html).'
  id: totrans-578
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ² 从`YarnClient`类中省略了一些队列和安全API——请参阅`YarnClient`的Javadocs以获取完整的API：[http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/yarn/client/api/YarnClient.html](http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/yarn/client/api/YarnClient.html).
- en: '![](431fig01_alt.jpg)'
  id: totrans-579
  prefs: []
  type: TYPE_IMG
  zh: '![](431fig01_alt.jpg)'
- en: Creating an application in YARN doesn’t actually do anything other than inform
    the ResourceManager of your intent to actually launch the application. The next
    step shows what you need to do to have the ResourceManager launch your ApplicationMaster.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 在YARN中创建一个应用程序实际上并没有做任何事情，只是通知ResourceManager你打算启动应用程序的意图。下一步将展示你需要做什么才能让ResourceManager启动你的ApplicationMaster。
- en: Submitting a YARN application
  id: totrans-581
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提交YARN应用程序
- en: 'Submitting the YARN application launches your ApplicationMaster in a new container
    in your YARN cluster. But there are several items you need to configure before
    you can submit the application, including the following:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 提交YARN应用程序将在你的YARN集群中的新容器中启动你的ApplicationMaster。但在提交应用程序之前，你需要配置几个项目，包括以下内容：
- en: An application name
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序名称
- en: The command to launch the ApplicationMaster, along with the classpath and environment
    settings
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动ApplicationMaster的命令，包括类路径和环境设置
- en: Any JARs, configuration files, and other files that your application needs to
    perform its work
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何应用程序执行其工作所需的JAR文件、配置文件和其他文件
- en: The resource requirements for the ApplicationMaster (memory and CPU)
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ApplicationMaster的资源需求（内存和CPU）
- en: Which scheduler queue to submit the application to and the application priority
    within the queue
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将应用程序提交到哪个调度器队列以及队列中的应用程序优先级
- en: Security tokens
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全令牌
- en: 'Let’s look at the code required to get a basic Java-based ApplicationMaster
    up and running. We’ll break this code up into two subsections: preparing the `Container-LaunchContext`
    object, and then specifying the resource requirements and submitting the application.'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看启动一个基本的基于Java的ApplicationMaster所需的代码。我们将把这个代码分成两个小节：准备`Container-LaunchContext`对象，然后指定资源需求和提交应用程序。
- en: First up is the `ContainerLaunchContext`, which is where you specify the command
    to launch your ApplicationMaster, along with any other environmental details required
    for your application to execute:^([[3](#ch10fn03)])
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是`ContainerLaunchContext`，这是你指定启动你的ApplicationMaster的命令以及任何其他应用程序执行所需的环境细节的地方：^([[3](#ch10fn03)])
- en: '³ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/Client.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/Client.java).'
  id: totrans-591
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³ GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/Client.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/Client.java).
- en: '![](432fig01_alt.jpg)'
  id: totrans-592
  prefs: []
  type: TYPE_IMG
  zh: '![](432fig01_alt.jpg)'
- en: The final steps are specifying the memory and CPU resources needed by the ApplicationMaster,
    followed by the application submission:^([[4](#ch10fn04)])
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的步骤是指定ApplicationMaster所需的内存和CPU资源，然后提交应用程序：^([[4](#ch10fn04)])
- en: '⁴ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/Client.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/Client.java).'
  id: totrans-594
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴ GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/Client.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/Client.java).
- en: '![](432fig02_alt.jpg)'
  id: totrans-595
  prefs: []
  type: TYPE_IMG
  zh: '![](432fig02_alt.jpg)'
- en: All container requests sent to the ResourceManager are processed asynchronously,
    so just because `submitApplication` returns doesn’t mean your ApplicationMaster
    is up and running. To figure out the state of your application, you’ll need to
    poll the ResourceManager for the application status, which will be covered next.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 发送到ResourceManager的所有容器请求都是异步处理的，所以`submitApplication`返回并不意味着你的ApplicationMaster已经启动并运行。为了了解应用程序的状态，你需要轮询ResourceManager以获取应用程序状态，这将在下一部分介绍。
- en: Waiting for the YARN application to complete
  id: totrans-597
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 等待YARN应用程序完成
- en: After submitting an application, you can poll the ResourceManager for information
    on the state of your ApplicationMaster. The result will contain details such as
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 提交应用程序后，你可以轮询ResourceManager以获取有关ApplicationMaster状态的信息。结果将包含如下详细信息
- en: The state of your application
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序的状态
- en: The host the ApplicationMaster is running on, and an RPC port (if any) where
    it’s listening for client requests (not applicable in our example)
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序主控运行的主机，以及（如果有）它监听客户端请求的RPC端口（在我们的示例中不适用）
- en: A tracking URL, if supported by the ApplicationMaster, which provides details
    on the progress of the application (again not supported in our example)
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果应用程序主控支持，则可以提供一个跟踪URL，该URL提供有关应用程序进度的详细信息（在我们的示例中不支持）
- en: General information such as the queue name and container start time
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般信息，如队列名称和容器启动时间
- en: Your ApplicationMaster can be in any one of the states shown in [figure 10.6](#ch10fig06)
    (the states are contained in the enum `YarnApplicationState`).
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 您的应用程序主控（ApplicationMaster）可以是[图10.6](#ch10fig06)中显示的任何一种状态（这些状态包含在枚举`YarnApplicationState`中）。
- en: Figure 10.6\. ApplicationMaster states
  id: totrans-604
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.6\. 应用程序主控状态
- en: '![](10fig06_alt.jpg)'
  id: totrans-605
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig06_alt.jpg)'
- en: The following code performs the final step of your client, which is to regularly
    poll the ResourceManager until the ApplicationMaster has completed:^([[5](#ch10fn05)])
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码执行客户端的最终步骤，即定期轮询资源管理器，直到应用程序主控完成：^([[5](#ch10fn05)])
- en: '⁵ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/Client.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/Client.java).'
  id: totrans-607
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁵ GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/Client.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/Client.java)。
- en: '![](433fig01_alt.jpg)'
  id: totrans-608
  prefs: []
  type: TYPE_IMG
  zh: '![](433fig01_alt.jpg)'
- en: Summary
  id: totrans-609
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: There are a number of more advanced client capabilities that weren’t explored
    in this section, such as security. [Section 10.3](#ch10lev1sec3) discusses this
    and other features that you’ll probably want to build into your client.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中未探讨的许多更高级的客户端功能，例如安全性。[第10.3节](#ch10lev1sec3)讨论了这一点以及您可能希望构建到客户端的其他功能。
- en: With your YARN client in place, it’s time to turn to the second half of your
    YARN application—the ApplicationMaster.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的YARN客户端就绪后，是时候转向您的YARN应用程序的第二部分——应用程序主控。
- en: Technique 102 A bare-bones ApplicationMaster
  id: totrans-612
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技术篇102：一个基础的应用程序主控
- en: The ApplicationMaster is the coordinator of the YARN application. It’s responsible
    for asking the ResourceManager for containers and then launching the containers
    via the NodeManager. [Figure 10.7](#ch10fig07) shows these interactions, which
    you’ll explore in this technique.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序主控是YARN应用程序的协调器。它负责向资源管理器请求容器，然后通过节点管理器启动容器。[图10.7](#ch10fig07)显示了这些交互，您将在本技术中探索这些交互。
- en: Figure 10.7\. The basic functions that your ApplicationMaster will perform
  id: totrans-614
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.7\. 您的应用程序主控将执行的基本功能
- en: '![](10fig07.jpg)'
  id: totrans-615
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig07.jpg)'
- en: Problem
  id: totrans-616
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You’re building a YARN application and need to implement an ApplicationMaster.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 您正在构建一个YARN应用程序，并需要实现一个应用程序主控。
- en: Solution
  id: totrans-618
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the YARN ApplicationMaster APIs to coordinate your work via the ResourceManager
    and NodeManager.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 使用YARN应用程序主控API通过资源管理器和节点管理器协调您的工作。
- en: Discussion
  id: totrans-620
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: As in the previous technique, we’ll break down the actions that the ApplicationMaster
    needs to perform.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一个技术一样，我们将分解应用程序主控需要执行的操作。
- en: Register with the ResourceManager
  id: totrans-622
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在资源管理器中注册
- en: The first step is to register the ApplicationMaster with the ResourceManager.
    To do so, you need to get a handle to an `AMRMClient` instance, which you’ll use
    for all your communication with the ResourceManager:^([[6](#ch10fn06)])
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将应用程序主控注册到资源管理器。为此，您需要获取一个`AMRMClient`实例的句柄，您将使用它与资源管理器进行所有通信：^([[6](#ch10fn06)])
- en: '⁶ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/ApplicationMaster.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/ApplicationMaster.java).'
  id: totrans-624
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁶ GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/ApplicationMaster.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/ApplicationMaster.java)。
- en: '![](435fig01_alt.jpg)'
  id: totrans-625
  prefs: []
  type: TYPE_IMG
  zh: '![](435fig01_alt.jpg)'
- en: Submit a container request and launch it when one is available
  id: totrans-626
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提交容器请求并在可用时启动它
- en: Next you’ll need to specify all the containers that you want to request. In
    this simple example, you’ll request a single container, and you won’t specify
    a specific host or rack on which it’ll run:^([[7](#ch10fn07)])
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要指定所有希望请求的容器。在这个简单的示例中，您将请求一个容器，并且不会指定它将在哪个特定的主机或机架上运行：^([[7](#ch10fn07)])
- en: '⁷ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/ApplicationMaster.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/ApplicationMaster.java).'
  id: totrans-628
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁷ GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/ApplicationMaster.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/ApplicationMaster.java)。
- en: '![](435fig02_alt.jpg)'
  id: totrans-629
  prefs: []
  type: TYPE_IMG
  zh: '![](435fig02_alt.jpg)'
- en: 'The `AMRMClient`’s `allocate` method performs a number of important functions:'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '`AMRMClient`的`allocate`方法执行了多个重要功能：'
- en: It acts as a heartbeat message to the ResourceManager. If the ResourceManager
    doesn’t receive a heartbeat message after 10 minutes, it will consider the ApplicationMaster
    to be in a bad state and will kill the process. The default expiry value can be
    changed by setting `yarn.am.liveness-monitor.expiry-interval-ms`.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它充当向ResourceManager的心跳消息。如果ResourceManager在10分钟后没有收到心跳消息，它将认为ApplicationMaster处于不良状态，并将终止进程。默认的超时值可以通过设置`yarn.am.liveness-monitor.expiry-interval-ms`来更改。
- en: It sends any container allocation requests that were added to the client.
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它发送任何添加到客户端的容器分配请求。
- en: It receives zero or more allocated containers that resulted from container allocation
    requests.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它接收零个或多个由容器分配请求产生的分配容器。
- en: The first time that `allocate` is called in this code, the container request
    will be sent to the ResourceManager. Because the ResourceManager handles container
    requests asynchronously, the response won’t contain the allocated container. Instead,
    a subsequent invocation of `allocate` will return the allocated container.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，第一次调用`allocate`时，容器请求将被发送到ResourceManager。因为ResourceManager异步处理容器请求，所以响应不会包含分配的容器。相反，后续的`allocate`调用将返回分配的容器。
- en: Wait for the container to complete
  id: totrans-635
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 等待容器完成
- en: At this point you’ve asked the ResourceManager for a container, received a container
    allocation from the ResourceManager, and communicated with a NodeManager to launch
    the container. Now you have to continue to call the `allocate` method and extract
    from the response any containers that completed:^([[8](#ch10fn08)])
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你已经向ResourceManager请求了一个容器，从ResourceManager那里收到了容器分配，并与NodeManager通信以启动容器。现在你必须继续调用`allocate`方法，并从响应中提取任何完成的容器:^([[8](#ch10fn08)])
- en: '⁸ GitHub source: [https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/ApplicationMaster.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/ApplicationMaster.java).'
  id: totrans-637
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁸ GitHub源代码：[https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/ApplicationMaster.java](https://github.com/alexholmes/hiped2/blob/master/src/main/java/hip/ch10/dstat/ApplicationMaster.java)。
- en: '![](436fig01_alt.jpg)'
  id: totrans-638
  prefs: []
  type: TYPE_IMG
  zh: '![](436fig01_alt.jpg)'
- en: Summary
  id: totrans-639
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 摘要
- en: In this technique you used the `AMRMClient` and `NMClient` classes to communicate
    with the ResourceManager and NodeManagers. These clients provide synchronous APIs
    to the YARN services. They have asynchronous counterparts (`AMRMClientAsync` and
    `NMClientAsync`) that encapsulate the heartbeat functionality and will call back
    into your code when messages are received from the ResourceManager. The async
    APIs may make it easier to reason about the interactions with the ResourceManager
    because the ResourceManager processes everything asynchronously.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技术中，你使用了`AMRMClient`和`NMClient`类与ResourceManager和NodeManagers进行通信。这些客户端为YARN服务提供了同步API。它们有异步对应物（`AMRMClientAsync`和`NMClientAsync`），这些对应物封装了心跳功能，并在接收到ResourceManager的消息时回调到你的代码中。异步API可能会使你更容易理解与ResourceManager的交互，因为ResourceManager异步处理所有内容。
- en: There are a few more features that the ResourceManager and NodeManager expose
    to ApplicationMasters:^([[9](#ch10fn09)])
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: ResourceManager和NodeManager还向ApplicationMasters公开了一些其他功能:^([[9](#ch10fn09)])
- en: ⁹ The complete Javadocs for `AMRMClient` can be viewed at [http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/yarn/client/api/AMRMClient.html](http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/yarn/client/api/AMRMClient.html).
  id: totrans-642
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁹ 可以在[http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/yarn/client/api/AMRMClient.html](http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/yarn/client/api/AMRMClient.html)查看`AMRMClient`的完整Javadocs。
- en: '![](437fig01_alt.jpg)'
  id: totrans-643
  prefs: []
  type: TYPE_IMG
  zh: '![](437fig01_alt.jpg)'
- en: Similarly, the `NMClient` API exposes a handful of mechanisms that you can use
    to control and get metadata about your containers:^([[10](#ch10fn10)])
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，`NMClient` API公开了一些机制，你可以使用这些机制来控制和获取你的容器的元数据:^([[10](#ch10fn10)])
- en: ^(10) The complete Javadocs for `NMClient` are available at [http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/yarn/client/api/NMClient.html](http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/yarn/client/api/NMClient.html).
  id: totrans-645
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^（10）`NMClient` 的完整 Javadoc 可在 [http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/yarn/client/api/NMClient.html](http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/yarn/client/api/NMClient.html)
    找到。
- en: '![](438fig01_alt.jpg)'
  id: totrans-646
  prefs: []
  type: TYPE_IMG
  zh: '![](438fig01_alt.jpg)'
- en: At this point you’ve written the code for a complete YARN application! Next
    you’ll execute your application on a cluster.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经编写了一个完整的 YARN 应用程序的代码！接下来，你将在集群上执行你的应用程序。
- en: Technique 103 Running the application and accessing logs
  id: totrans-648
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 103 运行应用程序和访问日志
- en: At this point you have a functional YARN application. In this section, you’ll
    look at how to run the application and access its output.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经有了一个功能性的 YARN 应用程序。在本节中，你将了解如何运行应用程序并访问其输出。
- en: Problem
  id: totrans-650
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to run your YARN application.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要运行你的 YARN 应用程序。
- en: Solution
  id: totrans-652
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the regular Hadoop command line to launch it and view the container outputs.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 使用常规 Hadoop 命令行来启动它并查看容器输出。
- en: Discussion
  id: totrans-654
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: The `hip` script that you’ve been using to launch all the examples in this book
    also works for running the YARN application. Behind the scenes, `hip` calls the
    `hadoop` script to run the examples.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 你一直在使用的 `hip` 脚本，用于启动本书中的所有示例，也适用于运行 YARN 应用程序。在幕后，`hip` 调用 `hadoop` 脚本来运行示例。
- en: 'The following example shows the output of running the YARN application that
    was written in the last two techniques. It runs a `vmstat` Linux command in a
    single container:'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例显示了运行在最后两个技巧中编写的 YARN 应用程序的输出。它在单个容器中运行 `vmstat` Linux 命令：
- en: '[PRE62]'
  id: totrans-657
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'If you have log aggregation enabled (see technique 3 for more details), you
    can issue the following command to view the log output of both the ApplicationMaster
    and the `vmstat` container:'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你启用了日志聚合（有关更多详细信息，请参阅技巧 3），你可以使用以下命令查看应用程序主和 `vmstat` 容器的日志输出：
- en: '![](439fig01_alt.jpg)'
  id: totrans-659
  prefs: []
  type: TYPE_IMG
  zh: '![](439fig01_alt.jpg)'
- en: The ApplicationMaster directed the container standard output to the stdout file,
    and you can see the output of the `vmstat` command in that file.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序主将容器标准输出指向 stdout 文件，你可以在该文件中看到 `vmstat` 命令的输出。
- en: Accessing logs when containers fail to start
  id: totrans-661
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 容器启动失败时的日志访问
- en: It’s likely that during the development of your YARN application, either the
    ApplicationMaster or one of your containers will fail to launch due to missing
    resources or errors in the startup command. Depending on where the failure occurs,
    your container logs will have the error related to startup or you’ll need to examine
    the NodeManager logs if the process failed to start outright.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发你的 YARN 应用程序期间，应用程序主或你的某个容器可能会因为资源缺失或启动命令错误而无法启动。根据失败发生的位置，你的容器日志将包含与启动相关的错误，或者如果进程完全无法启动，你需要检查
    NodeManager 日志。
- en: Retaining localized and log directories
  id: totrans-663
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 保留本地化和日志目录
- en: The `yarn.nodemanager.delete.debug-delay-sec` configuration property controls
    how long the localized and log directories for the application are kept around.
    The localized directory contains the command executed by the NodeManager to launch
    containers (both the ApplicationMaster and the application containers), as well
    as any JARs and other localized resources that were specified by the application
    for the container.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 配置属性 `yarn.nodemanager.delete.debug-delay-sec` 控制应用程序本地化和日志目录保留的时间。本地化目录包含 NodeManager
    执行以启动容器的命令（包括应用程序主和应用程序容器），以及应用程序为容器指定的任何 JAR 和其他本地化资源。
- en: It’s recommended that you set this property to a value that gives you enough
    time to diagnose failures. But don’t set this value too high (say, in the order
    of days) as this could create pressure on your storage.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 建议你将此属性设置为足够的时间来诊断失败。但不要设置得太高（比如，以天为单位），因为这可能会对你的存储造成压力。
- en: An alternative to hunting down ApplicationMaster startup problems is to run
    an unmanaged ApplicationMaster, which is covered in the next technique.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 作为寻找应用程序主启动问题的替代方案，可以运行一个未管理的应用程序主，这将在下一个技巧中介绍。
- en: Technique 104 Debugging using an unmanaged application master
  id: totrans-667
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技巧 104 使用未管理的应用程序主进行调试
- en: Debugging a YARN ApplicationMaster is a challenge, as it’s launched on a remote
    node and requires you to pull logs from that node to troubleshoot your code. ApplicationMasters
    that are launched by the ResourceManager in this way are called *managed* ApplicationMasters,
    as shown in [figure 10.8](#ch10fig08).
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 调试YARN ApplicationMaster是一个挑战，因为它是在远程节点上启动的，需要你从该节点拉取日志来排查你的代码。以这种方式由ResourceManager启动的ApplicationMasters被称为*管理*的ApplicationMaster，如[图10.8](#ch10fig08)所示。
- en: Figure 10.8\. A managed ApplicationMaster
  id: totrans-669
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.8\. 一个管理的ApplicationMaster
- en: '![](10fig08_alt.jpg)'
  id: totrans-670
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig08_alt.jpg)'
- en: YARN also supports the notion of an *unmanaged* ApplicationMaster, where the
    ApplicationMaster is launched on a local node, as seen in [figure 10.9](#ch10fig09).
    Issues with an ApplicationMaster are easier to diagnose when it’s running on the
    local host.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: YARN还支持一个名为*非管理*的ApplicationMaster的概念，其中ApplicationMaster在本地节点上启动，如[图10.9](#ch10fig09)所示。当ApplicationMaster在本地主机上运行时，诊断其问题更容易。
- en: Figure 10.9\. An unmanaged ApplicationMaster
  id: totrans-672
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.9\. 一个非管理ApplicationMaster
- en: '![](10fig09_alt.jpg)'
  id: totrans-673
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig09_alt.jpg)'
- en: In this section you’ll discover how to run an unmanaged ApplicationMaster and
    learn how they can be used by projects.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将了解如何运行非管理ApplicationMaster，并学习它们如何被项目使用。
- en: Problem
  id: totrans-675
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You want to run a local instance of an ApplicationMaster.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 你想运行一个ApplicationMaster的本地实例。
- en: Solution
  id: totrans-677
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: Run an unmanaged ApplicationMaster.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 运行一个非管理ApplicationMaster。
- en: Discussion
  id: totrans-679
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: YARN comes bundled with an application called the `UnmanagedAMLauncher`, which
    launches an unmanaged ApplicationMaster. An unmanaged ApplicationMaster is one
    that is not launched by the ResourceManager. Instead, the `UnmanagedAMLauncher`
    liaises with the ResourceManager to create a new application, but instead of issuing
    a `submit-Application` call to the ResourceManager (as is the case with managed
    ApplicationMasters), the `UnmanagedAMLauncher` starts the process.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: YARN附带了一个名为`UnmanagedAMLauncher`的应用程序，它启动一个非管理的ApplicationMaster。非管理ApplicationMaster是指不由ResourceManager启动的ApplicationMaster。相反，`UnmanagedAMLauncher`与ResourceManager协商以创建一个新的应用程序，但与管理的ApplicationMaster不同，`UnmanagedAMLauncher`启动该过程。
- en: 'When using the `UnmanagedAMLauncher`, you don’t have to define a YARN client,
    so all you need to provide are the details required to launch your ApplicationMaster.
    The following example shows how you can execute the ApplicationMaster that you
    wrote in the previous techniques:'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`UnmanagedAMLauncher`时，你不需要定义YARN客户端，因此你只需要提供启动你的ApplicationMaster所需的详细信息。以下示例显示了如何执行你在上一技术中编写的ApplicationMaster：
- en: '![](441fig01_alt.jpg)'
  id: totrans-682
  prefs: []
  type: TYPE_IMG
  zh: '![](441fig01_alt.jpg)'
- en: The `UnmanagedAMLauncher` captures the ApplicationMaster’s standard output and
    standard error and outputs them to its own standard output. This is useful in
    situations where your ApplicationMaster is failing to start, in which case the
    error will be seen in the output of the preceding command, as opposed to being
    tucked away in the logs of the NodeManager.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: '`UnmanagedAMLauncher`捕获ApplicationMaster的标准输出和标准错误，并将它们输出到其自己的标准输出。这在你的ApplicationMaster无法启动的情况下很有用，在这种情况下，错误将显示在先前的命令输出中，而不是被隐藏在NodeManager的日志中。'
- en: '[Figure 10.10](#ch10fig10) shows the interactions between the `UnmanagedAMLauncher`
    and the ResourceManager.'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.10](#ch10fig10)显示了`UnmanagedAMLauncher`与ResourceManager之间的交互。'
- en: Figure 10.10\. The unmanaged launcher working with the ResourceManager to launch
    an unmanaged ApplicationMaster
  id: totrans-685
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.10\. 非管理启动器与ResourceManager协作启动非管理ApplicationMaster
- en: '![](10fig10.jpg)'
  id: totrans-686
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig10.jpg)'
- en: 'There’s nothing stopping you from writing your own unmanaged ApplicationMaster
    launcher if the capabilities in `UnmanagedAMLauncher` are too limited. The following
    code shows the key step that the `UnmanagedAMLauncher` takes to tell the ResourceManager
    that the ApplicationMaster is unmanaged:'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为`UnmanagedAMLauncher`中的功能过于有限，你可以编写自己的非管理ApplicationMaster启动器。以下代码显示了`UnmanagedAMLauncher`告诉ResourceManagerApplicationMaster是非管理的关键步骤：
- en: '[PRE63]'
  id: totrans-688
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Unmanaged ApplicationMasters are useful as they provide local access to an ApplicationMaster,
    which can ease your debugging and profiling efforts.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 非管理ApplicationMasters很有用，因为它们提供了对ApplicationMaster的本地访问，这可以简化你的调试和性能分析工作。
- en: Next, let’s look at some more advanced capabilities that you may want to support
    in your YARN applications.
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看你可能希望在YARN应用程序中支持的一些更高级的功能。
- en: 10.3\. Additional YARN application capabilities
  id: totrans-691
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3\. YARN应用程序的附加功能
- en: So far in this chapter, we’ve looked at a bare-bones YARN application that launches
    a Linux command in a container. However, if you’re developing a YARN application,
    it’s likely that you’ll need to support more sophisticated capabilities. This
    section highlights some features that you may need to support in your application.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们查看了一个基本的YARN应用程序，该应用程序在容器中启动Linux命令。然而，如果你正在开发YARN应用程序，你很可能需要支持更复杂的功能。本节突出了你可能需要在你的应用程序中支持的一些功能。
- en: 10.3.1\. RPC between components
  id: totrans-693
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.1. 组件间的RPC
- en: If you have a long-running application, you may want to allow clients to communicate
    with the ApplicationMaster. Your ApplicationMaster may also need to be able to
    communicate with containers, and vice versa. An example could be a SQL-on-Hadoop
    application that allows clients to send queries to the ApplicationMaster, and
    whose ApplicationMaster then coordinates containers to perform the work.
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个长时间运行的应用程序，你可能希望允许客户端与ApplicationMaster通信。你的ApplicationMaster也可能需要能够与容器通信，反之亦然。一个例子是一个SQL-on-Hadoop应用程序，它允许客户端向ApplicationMaster发送查询，然后ApplicationMaster协调容器执行工作。
- en: 'YARN doesn’t provide you with any plumbing here, so you need to pick an RPC
    protocol and supporting library. You have a few options:'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: YARN在这里不提供任何管道服务，因此你需要选择一个RPC协议和相应的库。你有几个选择：
- en: '***Thrift or Avro*** —Both of these provide an interface definition language
    (IDL) where you can define endpoints and messages, which are compiled into concrete
    client and service code that can be easily incorporated into your code. The advantages
    of these libraries are code generation and schema evolution, allowing your services
    to evolve over time.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Thrift或Avro*** —这两个都提供了一个接口定义语言（IDL），你可以在这里定义端点和消息，这些消息被编译成具体的客户端和服务代码，可以轻松地集成到你的代码中。这些库的优点是代码生成和模式演变，允许你的服务随着时间的推移而发展。'
- en: '***Protocol Buffers*** —Google didn’t open source the RPC layer, so you’ll
    need to roll your own. You can use REST over HTTP for your transport and easily
    implement it all using Jersey’s annotations.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Protocol Buffers*** —Google没有开源RPC层，所以你需要自己实现。你可以使用HTTP上的REST作为传输，并使用Jersey的注解轻松实现所有这些。'
- en: '***Hadoop’s RPC*** —Behind the scenes, this uses Protocol Buffers.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Hadoop的RPC*** —在幕后，它使用Protocol Buffers。'
- en: Because YARN doesn’t support communication between your components, how can
    you know which hosts or ports your services are listening on?
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 由于YARN不支持组件间的通信，你怎么知道你的服务监听在哪些主机或端口上？
- en: 10.3.2\. Service discovery
  id: totrans-700
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.2. 服务发现
- en: 'YARN can schedule multiple containers on the same node, so hard-wiring the
    listening port for any service in your container or ApplicationMaster isn’t ideal.
    Instead, you can pick one of the following strategies:'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: YARN可以在同一节点上调度多个容器，因此将任何服务在容器或ApplicationMaster中的监听端口硬编码并不是理想的选择。相反，你可以选择以下策略之一：
- en: If your ApplicationMaster has a built-in service, pass the launched containers
    the ApplicationMaster host and port details, and have containers call back to
    the ApplicationMaster with their port number.
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的ApplicationMaster有一个内置的服务，将启动的容器的ApplicationMaster主机和端口详情传递给它，并让容器使用它们的端口号回调到ApplicationMaster。
- en: Use ZooKeeper as a service registry by having containers publish their host
    and port details to ZooKeeper, and have clients look up services in ZooKeeper.
    This is the strategy that Apache Twill, covered later in this chapter, employs.
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过让容器将它们的宿主机和端口详情发布到ZooKeeper，并让客户端在ZooKeeper中查找服务，使用ZooKeeper作为服务注册。这是Apache
    Twill采用的策略，该策略在本章的后面部分将进行介绍。
- en: Next up is a look at maintaining state in your application so that you can resume
    from a well-known state in the event of an application restart.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何在你的应用程序中维护状态，以便在应用程序重启的情况下从已知状态恢复。
- en: 10.3.3\. Checkpointing application progress
  id: totrans-705
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.3. 检查点应用程序进度
- en: If your application is long-running and maintains and builds state during its
    execution, you may need to periodically persist that state so that in the event
    of a container restart, a container or ApplicationMaster can pick up where it
    left off. Containers can be killed for a variety of reasons, including making
    resources available for other users and applications. ApplicationMasters going
    down are typically the result of an error in your application logic, the node
    going down, or a cluster restart.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的应用程序运行时间较长，并在执行过程中维护和构建状态，你可能需要定期持久化该状态，以便在容器重启的情况下，容器或ApplicationMaster可以从上次停止的地方继续执行。容器可能因各种原因被终止，包括为其他用户和应用程序释放资源。ApplicationMaster崩溃通常是由于应用程序逻辑错误、节点崩溃或集群重启造成的。
- en: Two services you can use for checkpointing are HDFS and ZooKeeper. Apache Twill,
    an abstracted framework for writing YARN applications, uses ZooKeeper to checkpoint
    container and ApplicationMaster state.
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 可以用于检查点的两个服务是HDFS和ZooKeeper。Apache Twill，一个用于编写YARN应用程序的抽象框架，使用ZooKeeper来检查点容器和ApplicationMaster的状态。
- en: One area to be aware of with checkpointing is handling split-brain situations.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查点中需要注意的一个领域是处理脑裂情况。
- en: 10.3.4\. Avoiding split-brain
  id: totrans-709
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.4\. 避免脑裂
- en: It’s possible that a networking problem will result in the ResourceManager believing
    that an ApplicationMaster is down and launching a new ApplicationMaster. This
    can lead to an undesired outcome if your application produces outputs or intermediary
    data in a way that’s not idempotent.
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会出现网络问题，导致ResourceManager认为ApplicationMaster已关闭并启动一个新的ApplicationMaster。如果你的应用程序以非幂等的方式产生输出或中间数据，这可能会导致不希望的结果。
- en: This was a problem in the early MapReduce YARN application, where task- and
    job-level commits could be executed more than once, which was not ideal for commit
    actions that couldn’t be repeatedly executed.^([[11](#ch10fn11)]) The solution
    was to introduce a delay in committing, and to use the ResourceManager heartbeat
    to verify that the ApplicationMaster was still valid. Refer to the JIRA ticket
    for more details.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在早期MapReduce YARN应用程序中的一个问题，其中任务和作业级别的提交可以执行多次，这对于不能重复执行的提交操作来说并不理想。解决方案是在提交时引入延迟，并使用ResourceManager心跳来验证ApplicationMaster是否仍然有效。有关更多详细信息，请参阅JIRA工单。^([11](#ch10fn11))
- en: ^(11) See the JIRA ticket titled “MR AM can get in a split brain situation”
    at [https://issues.apache.org/jira/browse/MAPREDUCE-4832](https://issues.apache.org/jira/browse/MAPREDUCE-4832).
  id: totrans-712
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([11](#ch10fn11)) 请参阅标题为“MR AM可能陷入脑裂情况”的JIRA工单：[https://issues.apache.org/jira/browse/MAPREDUCE-4832](https://issues.apache.org/jira/browse/MAPREDUCE-4832)。
- en: 10.3.5\. Long-running applications
  id: totrans-713
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.5\. 长运行应用程序
- en: 'Some YARN applications, such as Impala, are long-running, and as a result have
    requirements that differ from applications that are more transient in nature.
    If your application is also long-lived, you should be aware of the following points,
    some of which are currently being worked on in the community:'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 一些YARN应用程序，如Impala，是长运行的，因此它们的要求与更短暂的、性质上更短暂的应用程序不同。如果你的应用程序也是长运行的，你应该注意以下一些点，其中一些目前正在社区中工作：
- en: Gang scheduling, which allows a large number of containers to be scheduled in
    a short period of time (YARN-624).
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 群组调度，允许在短时间内调度大量容器（YARN-624）。
- en: Long-lived container support, allowing containers to indicate the fact that
    they’re long-lived so that the scheduler can make better allocation and management
    decisions (YARN-1039).
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长生命周期容器支持，允许容器表明它们是长生命周期的，以便调度器可以做出更好的分配和管理决策（YARN-1039）。
- en: Anti-affinity settings, so that applications can specify that multiple containers
    aren’t allocated on the same node (YARN-397).
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反亲和性设置，以便应用程序可以指定多个容器不会分配在同一个节点上（YARN-397）。
- en: Renewal of delegation tokens when running on a secure Hadoop cluster. Kerberos
    tokens expire, and if they’re not renewed, you won’t be able to access services
    such as HDFS (YARN-941).
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在安全Hadoop集群上运行时，委托令牌的续订。Kerberos令牌会过期，如果它们没有被续订，你将无法访问诸如HDFS等服务（YARN-941）。
- en: 'There’s an umbrella JIRA ticket that contains more details: [https://issues.apache.org/jira/browse/YARN-896](https://issues.apache.org/jira/browse/YARN-896).'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个包含更多详细信息的综合JIRA工单：[https://issues.apache.org/jira/browse/YARN-896](https://issues.apache.org/jira/browse/YARN-896)。
- en: Even though Impala is a YARN application, it uses unmanaged containers and its
    own gang-scheduling mechanism to work around some of the issues with long-running
    applications. As a result, Cloudera created a project called Llama ([http://cloudera.github.io/llama/](http://cloudera.github.io/llama/)),
    which mediates resource management between Impala and YARN to provide these features.
    Llama may be worth evaluating for your needs.
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Impala是一个YARN应用程序，但它使用未管理的容器和自己的gang-scheduling机制来解决长运行应用程序的一些问题。因此，Cloudera创建了一个名为Llama([http://cloudera.github.io/llama/](http://cloudera.github.io/llama/))的项目，该项目在Impala和YARN之间进行资源管理的中介，以提供这些功能。Llama可能值得评估以符合你的需求。
- en: 10.3.6\. Security
  id: totrans-721
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.6. 安全性
- en: YARN applications running on secure Hadoop clusters need to pass tokens to the
    ResourceManager that will be passed on to your application. These tokens are required
    to access services such as HDFS. Twill, detailed in the next section, provides
    support for secure Hadoop clusters.
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 在安全Hadoop集群上运行的YARN应用程序需要向ResourceManager传递令牌，这些令牌将被传递到你的应用程序。这些令牌是访问如HDFS等服务所必需的。下一节中详细介绍的Twill提供了对安全Hadoop集群的支持。
- en: This concludes our overview of additional capabilities that you may need in
    your YARN applications. Next up is a look at YARN programming abstractions, some
    of which implement the capabilities discussed in this section.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对YARN应用程序可能需要的附加功能的概述。接下来，我们将探讨YARN编程抽象，其中一些实现了本节中讨论的功能。
- en: 10.4\. YARN programming abstractions
  id: totrans-724
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4. YARN编程抽象
- en: YARN exposes a low-level API and has a steep learning curve, especially if you
    need to support many of the features that were outlined in the previous section.
    There are a number of abstractions on top of YARN that simplify the development
    of YARN applications and help you focus on implementing your application logic
    without worrying about the mechanics of YARN. Some of these frameworks, such as
    Twill, also support more advanced capabilities, such as shipping logs to the YARN
    client and service discovery via ZooKeeper.
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: YARN暴露了一个低级API，并且学习曲线陡峭，尤其是如果你需要支持上一节中概述的许多功能。在YARN之上有一系列抽象，这些抽象简化了YARN应用程序的开发，并帮助你专注于实现应用程序逻辑，而无需担心YARN的机制。其中一些框架，如Twill，还支持更高级的功能，例如将日志发送到YARN客户端和服务发现通过ZooKeeper。
- en: 'In this section I’ll provide a brief summary of three such abstractions: Apache
    Twill, Spring, and REEF.'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将简要介绍三个这样的抽象：Apache Twill、Spring和REEF。
- en: 10.4.1\. Twill
  id: totrans-727
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.4.1. Twill
- en: Apache Twill ([http://twill.incubator.apache.org/](http://twill.incubator.apache.org/)),
    formerly known as Weave, not only provides a rich and high-level programming abstraction,
    but also supports many features that you’ll likely require in your YARN application,
    such as service discovery, log shipping, and resiliency to failure.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Twill([http://twill.incubator.apache.org/](http://twill.incubator.apache.org/))，以前称为Weave，不仅提供了一个丰富且高级的编程抽象，还支持你可能在YARN应用程序中需要的许多功能，例如服务发现、日志传输和容错。
- en: 'The following code shows an example YARN client written in Twill. You’ll note
    that construction of the `YarnTwillRunnerService` requires a ZooKeeper connection
    URL, which is used to register the YARN application. Twill also supports shipping
    logs to the client (via Kafka), and here you’re adding a log handler to write
    the container and ApplicationMaster logs to standard output:'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了使用Twill编写的示例YARN客户端。你会注意到构建`YarnTwillRunnerService`需要ZooKeeper连接URL，该URL用于注册YARN应用程序。Twill还支持将日志发送到客户端（通过Kafka），在这里你正在添加一个日志处理程序，将容器和ApplicationMaster日志写入标准输出：
- en: '![](446fig01_alt.jpg)'
  id: totrans-730
  prefs: []
  type: TYPE_IMG
  zh: '![图片](446fig01_alt.jpg)'
- en: 'Twill’s programming model uses well-known Java types such as `Runnable` to
    model container execution. The following code shows a container that launches
    the `vmstat` utility:'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: Twill的编程模型使用诸如`Runnable`等众所周知的Java类型来模拟容器执行。以下代码展示了启动`vmstat`实用程序的容器：
- en: '![](446fig02_alt.jpg)'
  id: totrans-732
  prefs: []
  type: TYPE_IMG
  zh: '![图片](446fig02_alt.jpg)'
- en: '[Figure 10.11](#ch10fig11) shows how Twill uses ZooKeeper and Kafka to support
    features such as log shipping and service discovery.'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.11](#ch10fig11)展示了Twill如何使用ZooKeeper和Kafka来支持日志传输和服务发现等功能。'
- en: Figure 10.11\. Twill features
  id: totrans-734
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.11. Twill特性
- en: '![](10fig11_alt.jpg)'
  id: totrans-735
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig11_alt.jpg)'
- en: You can get a detailed overview of Twill from Terence Yim’s “Harnessing the
    Power of YARN with Apache Twill” ([http://www.slideshare.net/TerenceYim1/twill-apachecon-2014?ref=](http://www.slideshare.net/TerenceYim1/twill-apachecon-2014?ref=)).
    Yim also has a couple of blog entries on programming with Twill (formerly Weave).^([[12](#ch10fn12)])
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从Terence Yim的“使用Apache Twill释放YARN的力量”([http://www.slideshare.net/TerenceYim1/twill-apachecon-2014?ref=](http://www.slideshare.net/TerenceYim1/twill-apachecon-2014?ref=))中获得Twill的详细概述。Yim还有几篇关于使用Twill（以前称为Weave）的博客文章。[^([12](#ch10fn12))]
- en: ^(12) Terence Yim, “Programming with Weave, [Part I](kindle_split_009.html#part01),”
    [http://blog.continuuity.com/post/66694376303/programming-with-weave-part-i](http://blog.continuuity.com/post/66694376303/programming-with-weave-part-i);
    “Programming with Apache Twill, [Part II](kindle_split_012.html#part02),” [http://blog.continuuity.com/post/73969347586/programming-with-apache-twill-part-ii](http://blog.continuuity.com/post/73969347586/programming-with-apache-twill-part-ii).
  id: totrans-737
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([12]) Terence Yim, “使用Weave编程，[第一部分](kindle_split_009.html#part01),” [http://blog.continuuity.com/post/66694376303/programming-with-weave-part-i](http://blog.continuuity.com/post/66694376303/programming-with-weave-part-i);
    “使用Apache Twill，[第二部分](kindle_split_012.html#part02),” [http://blog.continuuity.com/post/73969347586/programming-with-apache-twill-part-ii](http://blog.continuuity.com/post/73969347586/programming-with-apache-twill-part-ii).
- en: 10.4.2\. Spring
  id: totrans-738
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.4.2. Spring
- en: The 2.x release of Spring for Hadoop ([http://projects.spring.io/spring-hadoop/](http://projects.spring.io/spring-hadoop/))
    brings support for simplifying YARN development. It differs from Twill in that
    it’s focused on abstracting the YARN API and not on providing application features;
    Twill, in contrast, offers log shipping and service discovers. But it’s very possible
    that you may not want the added complexity that these features bring to Twill
    and instead want more control over your YARN application. If so, this may make
    Spring for Hadoop a better candidate.
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: Spring for Hadoop的2.x版本([http://projects.spring.io/spring-hadoop/](http://projects.spring.io/spring-hadoop/))为简化YARN开发提供了支持。它与Twill不同，因为它专注于抽象YARN
    API，而不是提供应用程序功能；相比之下，Twill提供日志传输和服务发现。但是，你很可能不希望这些功能给Twill带来的额外复杂性，而是希望对YARN应用程序有更多的控制。如果是这样，Spring
    for Hadoop可能是一个更好的选择。
- en: 'Spring for Hadoop provides default implementations of a YARN client, ApplicationMaster,
    and container that can be overridden to provide application-specific functionality.
    You can actually write a YARN application without writing any code! The following
    example is from the Spring Hadoop samples, showing how you can configure a YARN
    application to run a remote command.^([[13](#ch10fn13)]) This first snippet shows
    the application context, and configures the HDFS, YARN, and application JARs:'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: Spring for Hadoop提供了YARN客户端、ApplicationMaster和容器的默认实现，这些实现可以被覆盖以提供特定于应用程序的功能。实际上，你可以不写任何代码就编写一个YARN应用程序！以下示例来自Spring
    Hadoop样本，展示了如何配置YARN应用程序以运行远程命令。[^([13](#ch10fn13))] 以下代码片段显示了应用程序上下文，并配置了HDFS、YARN和应用程序JAR文件：
- en: ^(13) “Spring Yarn Simple Command Example,” [https://github.com/spring-projects/spring-hadoop-samples/tree/master/yarn/yarn/simple-command](https://github.com/spring-projects/spring-hadoop-samples/tree/master/yarn/yarn/simple-command).
  id: totrans-741
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([13]) “Spring Yarn简单命令示例，” [https://github.com/spring-projects/spring-hadoop-samples/tree/master/yarn/yarn/simple-command](https://github.com/spring-projects/spring-hadoop-samples/tree/master/yarn/yarn/simple-command).
- en: '[PRE64]'
  id: totrans-742
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The following code defines the ApplicationMaster properties and tells it to
    run the `vmstat` command:'
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码定义了ApplicationMaster属性，并指示它运行`vmstat`命令：
- en: '[PRE65]'
  id: totrans-744
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: The samples also include a look at how you can extend the client, ApplicationMaster,
    and container.^([[14](#ch10fn14)])
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 样本还包括如何扩展客户端、ApplicationMaster和容器的内容查看。[^([14](#ch10fn14))]
- en: '^(14) Example of extending the Spring YARN classes: “Spring Yarn Custom Application
    Master Service Example,” [https://github.com/spring-projects/spring-hadoop-samples/tree/master/yarn/yarn/custom-amservice](https://github.com/spring-projects/spring-hadoop-samples/tree/master/yarn/yarn/custom-amservice).'
  id: totrans-746
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([14]) 扩展Spring YARN类的示例：“Spring Yarn自定义Application Master服务示例，” [https://github.com/spring-projects/spring-hadoop-samples/tree/master/yarn/yarn/custom-amservice](https://github.com/spring-projects/spring-hadoop-samples/tree/master/yarn/yarn/custom-amservice).
- en: 'You can find some sample Spring for Hadoop applications on GitHub ([https://github.com/spring-projects/spring-hadoop-samples](https://github.com/spring-projects/spring-hadoop-samples)).
    There’s also a wiki for the project: [https://github.com/spring-projects/spring-hadoop/wiki](https://github.com/spring-projects/spring-hadoop/wiki).'
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到一些Spring for Hadoop的示例应用程序([https://github.com/spring-projects/spring-hadoop-samples](https://github.com/spring-projects/spring-hadoop-samples))。该项目也有一个维基页面：[https://github.com/spring-projects/spring-hadoop/wiki](https://github.com/spring-projects/spring-hadoop/wiki).
- en: 10.4.3\. REEF
  id: totrans-748
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.4.3. REEF
- en: REEF is a framework from Microsoft that simplifies scalable, fault-tolerant
    runtime environments for a range of computational models, including YARN and Mesos
    ([www.reef-project.org/](http://www.reef-project.org/); [https://github.com/Microsoft-CISL/REEF](https://github.com/Microsoft-CISL/REEF)).
    REEF has some interesting capabilities, such as container reuse and data caching.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: REEF是微软的一个框架，它简化了适用于各种计算模型的可扩展、容错运行环境，包括YARN和Mesos([www.reef-project.org/](http://www.reef-project.org/);
    [https://github.com/Microsoft-CISL/REEF](https://github.com/Microsoft-CISL/REEF))。REEF有一些有趣的功能，如容器重用和数据缓存。
- en: 'You can find a REEF tutorial on GitHub: [https://github.com/Microsoft-CISL/REEF/wiki/How-to-download-and-compile-REEF](https://github.com/Microsoft-CISL/REEF/wiki/How-to-download-and-compile-REEF).'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到REEF教程：[https://github.com/Microsoft-CISL/REEF/wiki/How-to-download-and-compile-REEF](https://github.com/Microsoft-CISL/REEF/wiki/How-to-download-and-compile-REEF).
- en: 10.4.4\. Picking a YARN API abstraction
  id: totrans-751
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.4.4. 选择YARN API抽象
- en: YARN abstractions are still in their early stages because YARN is a young technology.
    This section provided a brief overview of three abstractions that you could use
    to hide away some of the complexities of the YARN API. But which one should you
    pick for your application?
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: YARN的抽象还处于早期阶段，因为YARN是一项新技术。本节简要概述了您可以使用来隐藏YARN API一些复杂性的三个抽象。但您应该为您的应用程序选择哪一个呢？
- en: '*Apache Twill* looks the most promising, as it already encapsulates many of
    the features that you’ll need in your application. It has picked best-of-breed
    technologies such as Kafka and ZooKeeper to support these features.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Apache Twill* 看起来最有前途，因为它已经封装了您在应用程序中需要的许多功能。它选择了最佳的技术，如Kafka和ZooKeeper来支持这些功能。'
- en: '*Spring for Hadoop* may be a better fit if you’re developing a lightweight
    application and you don’t want a dependency on Kafka or ZooKeeper.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您正在开发一个轻量级的应用程序，并且不想依赖Kafka或ZooKeeper，*Spring for Hadoop* 可能更适合您。
- en: '*REEF* may be useful if you have some complex application requirements, such
    as the need to run on multiple execution frameworks, or if you need to support
    more complex container choreographies and state sharing across containers.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您有一些复杂的应用程序需求，例如需要在多个执行框架上运行，或者需要支持更复杂的容器编排和容器之间的状态共享，*REEF* 可能很有用。
- en: 10.5\. Chapter summary
  id: totrans-756
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5. 章节总结
- en: This chapter showed you how to write a simple YARN application and then introduced
    you to some of the more advanced capabilities that you may need in your YARN applications.
    It also looked at some YARN abstractions that make it easier to write your applications.
    You’re now all set to go out and start writing the next big YARN application.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向您展示了如何编写一个简单的YARN应用程序，并介绍了您可能在YARN应用程序中需要的更高级的功能。它还探讨了使编写应用程序更简单的YARN抽象。现在，您已经准备好开始编写下一个重大的YARN应用程序了。
- en: This concludes not only this chapter but the book as a whole! I hope you’ve
    enjoyed the journey and along the way have picked up some tips and tricks that
    you can employ in your Hadoop applications and environments. If you have any questions
    about items covered in this book, please head on over to Manning’s forum dedicated
    to this book and post a question.^([[15](#ch10fn15)])
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅结束了本章，也结束了整本书！我希望您喜欢这次旅程，并且在旅途中学到了一些可以在您的Hadoop应用程序和环境中使用的小技巧。如果您对本书中涵盖的任何内容有任何疑问，请前往Manning为本书设立的论坛并提问.^([15](#ch10fn15))
- en: '^(15) Manning forum for Hadoop in Practice: [http://www.manning-sandbox.com/forum.jspa?forumID=901](http://www.manning-sandbox.com/forum.jspa?forumID=901).'
  id: totrans-759
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^([15](#ch10fn15)) Manning论坛上的Hadoop实践：[http://www.manning-sandbox.com/forum.jspa?forumID=901](http://www.manning-sandbox.com/forum.jspa?forumID=901).
