- en: Chapter 9\. Orthogonal Matrices and QR Decomposition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬9ç«  æ­£äº¤çŸ©é˜µå’ŒQRåˆ†è§£
- en: 'You will learn five major decompositions in this book: orthogonal vector decomposition,
    QR decomposition, LU decomposition, eigendecomposition, and singular value decomposition.
    Those are not the only decompositions in linear algebra, but they are the most
    important ones for data science and machine learning.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å°†åœ¨æœ¬ä¹¦ä¸­å­¦ä¹ äº”ç§ä¸»è¦çš„åˆ†è§£ï¼šæ­£äº¤å‘é‡åˆ†è§£ã€QRåˆ†è§£ã€LUåˆ†è§£ã€ç‰¹å¾åˆ†è§£å’Œå¥‡å¼‚å€¼åˆ†è§£ã€‚è¿™äº›ä¸æ˜¯çº¿æ€§ä»£æ•°ä¸­å”¯ä¸€çš„åˆ†è§£ï¼Œä½†å®ƒä»¬æ˜¯æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ ä¸­æœ€é‡è¦çš„åˆ†è§£æ–¹æ³•ä¹‹ä¸€ã€‚
- en: In this chapter, you will learn QR. And along the way, youâ€™ll learn a new special
    type of matrix (orthogonal). QR decomposition is a workhorse that powers applications
    including the matrix inverse, least squares model fitting, and eigendecomposition.
    Therefore, understanding and gaining familiarity with QR decomposition will help
    you level up your linear algebra skills.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæ‚¨å°†å­¦ä¹ QRã€‚å¹¶ä¸”åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæ‚¨å°†å­¦ä¹ ä¸€ç§æ–°çš„ç‰¹æ®Šç±»å‹çš„çŸ©é˜µï¼ˆæ­£äº¤çŸ©é˜µï¼‰ã€‚QRåˆ†è§£æ˜¯è®¸å¤šåº”ç”¨çš„åŸºç¡€ï¼ŒåŒ…æ‹¬çŸ©é˜µæ±‚é€†ã€æœ€å°äºŒä¹˜æ¨¡å‹æ‹Ÿåˆå’Œç‰¹å¾åˆ†è§£ã€‚å› æ­¤ï¼Œç†è§£å’Œç†Ÿæ‚‰QRåˆ†è§£å°†å¸®åŠ©æ‚¨æå‡çº¿æ€§ä»£æ•°æŠ€èƒ½ã€‚
- en: Orthogonal Matrices
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ­£äº¤çŸ©é˜µ
- en: 'I will begin by introducing you to orthogonal matrices. An *orthogonal matrix*
    is a special matrix that is important for several decompositions, including QR,
    eigendecomposition, and singular value decomposition. The letter <math alttext="bold
    upper Q"><mi>ğ</mi></math> is often used to indicate orthogonal matrices. Orthogonal
    matrices have two properties:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†é¦–å…ˆä»‹ç»æ­£äº¤çŸ©é˜µç»™æ‚¨ã€‚*æ­£äº¤çŸ©é˜µ*æ˜¯ä¸€ç§ç‰¹æ®Šçš„çŸ©é˜µï¼Œå¯¹äºå¤šç§åˆ†è§£ï¼ˆåŒ…æ‹¬QRåˆ†è§£ã€ç‰¹å¾åˆ†è§£å’Œå¥‡å¼‚å€¼åˆ†è§£ï¼‰è‡³å…³é‡è¦ã€‚å­—æ¯<math alttext="bold
    upper Q"><mi>ğ</mi></math>é€šå¸¸ç”¨æ¥è¡¨ç¤ºæ­£äº¤çŸ©é˜µã€‚æ­£äº¤çŸ©é˜µå…·æœ‰ä¸¤ä¸ªç‰¹æ€§ï¼š
- en: Orthogonal columns
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£äº¤åˆ—
- en: All columns are pair-wise orthogonal.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰åˆ—éƒ½æ˜¯ä¸¤ä¸¤æ­£äº¤çš„ã€‚
- en: Unit-norm columns
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å•ä½èŒƒæ•°åˆ—
- en: The norm (geometric length) of each column is exactly 1.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯åˆ—çš„èŒƒæ•°ï¼ˆå‡ ä½•é•¿åº¦ï¼‰æ°å¥½ä¸º1ã€‚
- en: 'We can translate those two properties into a mathematical expression (remember
    that <math alttext="mathematical left-angle bold a comma bold b mathematical right-angle"><mrow><mo>âŒ©</mo>
    <mi>ğš</mi> <mo>,</mo> <mi>ğ›</mi> <mo>âŒª</mo></mrow></math> is an alternative notation
    for the dot product):'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸¤ä¸ªç‰¹æ€§ç¿»è¯‘æˆæ•°å­¦è¡¨è¾¾å¼ï¼ˆè¯·è®°ä½<math alttext="mathematical left-angle bold a comma bold
    b mathematical right-angle"><mrow><mo>âŒ©</mo> <mi>ğš</mi> <mo>,</mo> <mi>ğ›</mi>
    <mo>âŒª</mo></mrow></math>æ˜¯ç‚¹ç§¯çš„å¦ä¸€ç§è¡¨ç¤ºæ–¹æ³•ï¼‰ï¼š
- en: <math alttext="mathematical left-angle bold q Subscript i Baseline comma bold
    q Subscript j Baseline mathematical right-angle equals StartLayout Enlarged left-brace
    1st Row  0 comma if i not-equals j 2nd Row  1 comma if i equals j EndLayout" display="block"><mrow><mrow><mo>âŒ©</mo>
    <msub><mi>ğª</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>ğª</mi> <mi>j</mi></msub>
    <mo>âŒª</mo></mrow> <mo>=</mo> <mfenced close="" open="{" separators=""><mtable><mtr><mtd
    columnalign="left"><mrow><mn>0</mn> <mo>,</mo> <mtext>if</mtext> <mi>i</mi> <mo>â‰ </mo>
    <mi>j</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mn>1</mn> <mo>,</mo>
    <mtext>if</mtext> <mi>i</mi> <mo>=</mo> <mi>j</mi></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="mathematical left-angle bold q Subscript i Baseline comma bold
    q Subscript j Baseline mathematical right-angle equals StartLayout Enlarged left-brace
    1st Row  0 comma if i not-equals j 2nd Row  1 comma if i equals j EndLayout" display="block"><mrow><mrow><mo>âŒ©</mo>
    <msub><mi>ğª</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>ğª</mi> <mi>j</mi></msub>
    <mo>âŒª</mo></mrow> <mo>=</mo> <mfenced close="" open="{" separators=""><mtable><mtr><mtd
    columnalign="left"><mrow><mn>0</mn> <mo>,</mo> <mtext>if</mtext> <mi>i</mi> <mo>â‰ </mo>
    <mi>j</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mn>1</mn> <mo>,</mo>
    <mtext>if</mtext> <mi>i</mi> <mo>=</mo> <mi>j</mi></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: What does that mean? It means that the dot product of a column with itself is
    1 while the dot product of a column with any other column is 0\. Thatâ€™s a lot
    of dot products with only two possible outcomes. We can organize all of the dot
    products amongst all pairs of columns by premultiplying the matrix by its transpose.
    Remember that matrix multiplication is defined as dot products between all rows
    of the left matrix with all columns of the right matrix; therefore, the rows of
    <math alttext="bold upper Q Superscript upper T"><msup><mi>ğ</mi> <mtext>T</mtext></msup></math>
    are the columns of <math alttext="bold upper Q"><mi>ğ</mi></math> .
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿè¿™æ„å‘³ç€åˆ—ä¸è‡ªèº«çš„ç‚¹ç§¯ä¸º1ï¼Œè€Œåˆ—ä¸ä»»ä½•å…¶ä»–åˆ—çš„ç‚¹ç§¯ä¸º0ã€‚è¿™æ˜¯è®¸å¤šç‚¹ç§¯ï¼Œåªæœ‰ä¸¤ç§å¯èƒ½çš„ç»“æœã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å°†çŸ©é˜µä¸å…¶è½¬ç½®çš„ä¹˜ç§¯å‰ç½®æ¥ç»„ç»‡æ‰€æœ‰åˆ—å¯¹ä¹‹é—´çš„ç‚¹ç§¯ã€‚è¯·è®°ä½ï¼ŒçŸ©é˜µä¹˜æ³•å®šä¹‰ä¸ºå·¦çŸ©é˜µçš„æ‰€æœ‰è¡Œä¸å³çŸ©é˜µçš„æ‰€æœ‰åˆ—çš„ç‚¹ç§¯ï¼›å› æ­¤ï¼Œ<math
    alttext="bold upper Q Superscript upper T"><msup><mi>ğ</mi> <mtext>T</mtext></msup></math>çš„è¡Œæ˜¯<math
    alttext="bold upper Q"><mi>ğ</mi></math>çš„åˆ—ã€‚
- en: 'The matrix equation expressing the two key properties of an orthogonal matrix
    is simply marvelous:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨è¾¾ä¸¤ä¸ªæ­£äº¤çŸ©é˜µå…³é”®å±æ€§çš„çŸ©é˜µæ–¹ç¨‹å¼ç®€ç›´æ˜¯å¥‡å¦™çš„ï¼š
- en: <math alttext="bold upper Q Superscript upper T Baseline bold upper Q equals
    bold upper I" display="block"><mrow><msup><mi>ğ</mi> <mtext>T</mtext></msup> <mi>ğ</mi>
    <mo>=</mo> <mi>ğˆ</mi></mrow></math>
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper Q Superscript upper T Baseline bold upper Q equals
    bold upper I" display="block"><mrow><msup><mi>ğ</mi> <mtext>T</mtext></msup> <mi>ğ</mi>
    <mo>=</mo> <mi>ğˆ</mi></mrow></math>
- en: The expression <math alttext="bold upper Q Superscript upper T Baseline bold
    upper Q equals bold upper I"><mrow><msup><mi>ğ</mi> <mtext>T</mtext></msup> <mi>ğ</mi>
    <mo>=</mo> <mi>ğˆ</mi></mrow></math> is amazing. Really, itâ€™s a big deal.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨è¾¾å¼<math alttext="bold upper Q Superscript upper T Baseline bold upper Q equals
    bold upper I"><mrow><msup><mi>ğ</mi> <mtext>T</mtext></msup> <mi>ğ</mi> <mo>=</mo>
    <mi>ğˆ</mi></mrow></math>ä»¤äººæƒŠå¹ã€‚çœŸçš„ï¼Œè¿™æ˜¯ä¸€ä»¶å¤§äº‹ã€‚
- en: Why is it a big deal? Because <math alttext="bold upper Q Superscript upper
    T"><msup><mi>ğ</mi> <mtext>T</mtext></msup></math> is a matrix that multiplies
    <math alttext="bold upper Q"><mi>ğ</mi></math> to produce the identity matrix.
    Thatâ€™s the exact same definition as the matrix inverse. Thus, the inverse of an
    orthogonal matrix is its transpose. Thatâ€™s crazy cool, because the matrix inverse
    is tedious and prone to numerical inaccuracies, whereas the matrix transpose is
    fast and accurate.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦ï¼Ÿå› ä¸º<math alttext="bold upper Q Superscript upper T"><msup><mi>ğ</mi>
    <mtext>T</mtext></msup></math>æ˜¯ä¸€ä¸ªå°†<math alttext="bold upper Q"><mi>ğ</mi></math>ä¹˜ä»¥äº§ç”Ÿå•ä½çŸ©é˜µçš„çŸ©é˜µã€‚è¿™ä¸çŸ©é˜µçš„é€†çŸ©é˜µçš„ç¡®åˆ‡å®šä¹‰ç›¸åŒã€‚å› æ­¤ï¼Œæ­£äº¤çŸ©é˜µçš„é€†çŸ©é˜µæ˜¯å…¶è½¬ç½®çŸ©é˜µã€‚è¿™éå¸¸é…·ï¼Œå› ä¸ºçŸ©é˜µçš„é€†çŸ©é˜µå¤æ‚ä¸”å®¹æ˜“å‡ºç°æ•°å€¼ä¸å‡†ç¡®ï¼Œè€ŒçŸ©é˜µçš„è½¬ç½®çŸ©é˜µåˆ™å¿«é€Ÿä¸”å‡†ç¡®ã€‚
- en: 'Do such matrices really exist in the wild, or are they mere figments of a data
    scientistâ€™s imagination? Yes, they really do exist. In fact, the identity matrix
    is an example of an orthogonal matrix. Here are another two:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·çš„çŸ©é˜µçœŸçš„å­˜åœ¨å—ï¼Œè¿˜æ˜¯åªæ˜¯æ•°æ®ç§‘å­¦å®¶æƒ³è±¡ä¸­çš„äº§ç‰©ï¼Ÿæ˜¯çš„ï¼Œå®ƒä»¬ç¡®å®å­˜åœ¨ã€‚äº‹å®ä¸Šï¼Œå•ä½çŸ©é˜µå°±æ˜¯æ­£äº¤çŸ©é˜µçš„ä¸€ä¸ªä¾‹å­ã€‚ä»¥ä¸‹æ˜¯å¦å¤–ä¸¤ä¸ªä¾‹å­ï¼š
- en: <math alttext="StartFraction 1 Over StartRoot 2 EndRoot EndFraction Start 2
    By 2 Matrix 1st Row 1st Column 1 2nd Column negative 1 2nd Row 1st Column 1 2nd
    Column 1 EndMatrix comma one-third Start 3 By 3 Matrix 1st Row 1st Column 1 2nd
    Column 2 3rd Column 2 2nd Row 1st Column 2 2nd Column 1 3rd Column negative 2
    3rd Row 1st Column negative 2 2nd Column 2 3rd Column negative 1 EndMatrix" display="block"><mrow><mfrac><mn>1</mn>
    <msqrt><mn>2</mn></msqrt></mfrac> <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mrow><mo>-</mo> <mn>1</mn></mrow></mtd></mtr> <mtr><mtd><mn>1</mn></mtd>
    <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>1</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>,</mo> <mfrac><mn>1</mn> <mn>3</mn></mfrac> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>1</mn></mrow></mtd> <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>2</mn></mrow></mtd>
    <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>2</mn></mrow></mtd> <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>1</mn></mrow></mtd>
    <mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mo>-</mo>
    <mn>2</mn></mrow></mtd> <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>2</mn></mrow></mtd>
    <mtd><mrow><mo>-</mo> <mn>1</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction 1 Over StartRoot 2 EndRoot EndFraction Start 2
    By 2 Matrix 1st Row 1st Column 1 2nd Column negative 1 2nd Row 1st Column 1 2nd
    Column 1 EndMatrix comma one-third Start 3 By 3 Matrix 1st Row 1st Column 1 2nd
    Column 2 3rd Column 2 2nd Row 1st Column 2 2nd Column 1 3rd Column negative 2
    3rd Row 1st Column negative 2 2nd Column 2 3rd Column negative 1 EndMatrix" display="block"><mrow><mfrac><mn>1</mn>
    <msqrt><mn>2</mn></msqrt></mfrac> <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mrow><mo>-</mo> <mn>1</mn></mrow></mtd></mtr> <mtr><mtd><mn>1</mn></mtd>
    <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>1</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>,</mo> <mfrac><mn>1</mn> <mn>3</mn></mfrac> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>1</mn></mrow></mtd> <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>2</mn></mrow></mtd>
    <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>2</mn></mrow></mtd> <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>1</mn></mrow></mtd>
    <mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mo>-</mo>
    <mn>2</mn></mrow></mtd> <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>2</mn></mrow></mtd>
    <mtd><mrow><mo>-</mo> <mn>1</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'Please take a moment to confirm that each column has unit length and is orthogonal
    to other columns. Then we can confirm in Python:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·èŠ±ä¸€ç‚¹æ—¶é—´ç¡®è®¤æ¯ä¸€åˆ—çš„é•¿åº¦ä¸ºå•ä½é•¿åº¦ï¼Œå¹¶ä¸”ä¸å…¶ä»–åˆ—æ­£äº¤ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥åœ¨Pythonä¸­ç¡®è®¤ï¼š
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Both outputs are the identity matrix (within rounding errors on the order of
    10^(âˆ’15)). What happens if you compute <math alttext="bold upper Q bold upper
    Q Superscript upper T"><mrow><mi>ğ</mi> <msup><mi>ğ</mi> <mtext>T</mtext></msup></mrow></math>
    ? Is that still the identity matrix? Try it to find out!^([1](ch09.xhtml#idm45733297769472))
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªè¾“å‡ºéƒ½æ˜¯å•ä½çŸ©é˜µï¼ˆåœ¨çº¦ä¸º10^(-15)çš„èˆå…¥è¯¯å·®å†…ï¼‰ã€‚å¦‚æœè®¡ç®—<math alttext="bold upper Q bold upper Q Superscript
    upper T"><mrow><mi>ğ</mi> <msup><mi>ğ</mi> <mtext>T</mtext></msup></mrow></math>ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿé‚£ä»ç„¶æ˜¯å•ä½çŸ©é˜µå—ï¼Ÿè¯•è¯•çœ‹ï¼^([1](ch09.xhtml#idm45733297769472))
- en: Another example of an orthogonal matrix is the pure rotation matrices that you
    learned about in [ChapterÂ 7](ch07.xhtml#Chapter_7). You can go back to that code
    and confirm that the transformation matrix times its transpose is the identity
    matrix, regardless of the rotation angle (as long as the same rotation angle is
    used in all matrix elements). Permutation matrices are also orthogonal. Permutation
    matrices are used to exchange rows of a matrix; youâ€™ll learn about them in the
    discussion of LU decomposition in the next chapter.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæ­£äº¤çŸ©é˜µçš„ä¾‹å­æ˜¯ä½ åœ¨[ç¬¬7ç« ](ch07.xhtml#Chapter_7)ä¸­å­¦åˆ°çš„çº¯æ—‹è½¬çŸ©é˜µã€‚ä½ å¯ä»¥å›é¡¾é‚£æ®µä»£ç ï¼Œå¹¶ç¡®è®¤å˜æ¢çŸ©é˜µä¹˜ä»¥å…¶è½¬ç½®çŸ©é˜µä¸ºå•ä½çŸ©é˜µï¼Œæ— è®ºæ—‹è½¬è§’åº¦å¦‚ä½•ï¼ˆåªè¦æ‰€æœ‰çŸ©é˜µå…ƒç´ ä½¿ç”¨ç›¸åŒçš„æ—‹è½¬è§’åº¦ï¼‰ã€‚æ’åˆ—çŸ©é˜µä¹Ÿæ˜¯æ­£äº¤çš„ã€‚æ’åˆ—çŸ©é˜µç”¨äºäº¤æ¢çŸ©é˜µçš„è¡Œï¼›åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œä½ å°†å­¦ä¹ å…³äºLUåˆ†è§£çš„è®¨è®ºã€‚
- en: How do you create such majestic marvels of mathematics? An orthogonal matrix
    can be computed from a nonorthogonal matrix via QR decomposition, which is basically
    a sophisticated version of Gram-Schmidt. And how does Gram-Schmidt work? Thatâ€™s
    basically the orthogonal vector decomposition that you learned about in [ChapterÂ 2](ch02.xhtml#Chapter_2).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½•åˆ›å»ºè¿™æ ·å®ä¼Ÿçš„æ•°å­¦å¥‡è¿¹ï¼Ÿæ­£äº¤çŸ©é˜µå¯ä»¥é€šè¿‡QRåˆ†è§£ä»éæ­£äº¤çŸ©é˜µä¸­è®¡ç®—å¾—åˆ°ï¼Œè¿™åŸºæœ¬ä¸Šæ˜¯Gram-Schmidtçš„å¤æ‚ç‰ˆæœ¬ã€‚Gram-Schmidtæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿè¿™åŸºæœ¬ä¸Šæ˜¯ä½ åœ¨[ç¬¬2ç« ](ch02.xhtml#Chapter_2)ä¸­å­¦åˆ°çš„æ­£äº¤å‘é‡åˆ†è§£ã€‚
- en: Gram-Schmidt
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gram-Schmidt
- en: The Gram-Schmidt procedure is a way to transform a nonorthogonal matrix into
    an orthogonal matrix. Gram-Schmidt has high educational value, but unfortunately
    very little application value. The reason is thatâ€”as youâ€™ve now read several times
    beforeâ€”there are numerical instabilities resulting from many divisions and multiplications
    by tiny numbers. Fortunately, there are more sophisticated and numerically stable
    methods for QR decomposition, such as Householder reflections. The details of
    that algorithm are outside the scope of this book, but they are handled by low-level
    numerical computation libraries that Python calls.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Gram-Schmidtè¿‡ç¨‹æ˜¯å°†éæ­£äº¤çŸ©é˜µè½¬æ¢ä¸ºæ­£äº¤çŸ©é˜µçš„æ–¹æ³•ã€‚Gram-Schmidtå…·æœ‰å¾ˆé«˜çš„æ•™è‚²ä»·å€¼ï¼Œä½†éå¸¸é—æ†¾çš„æ˜¯åº”ç”¨ä»·å€¼å¾ˆå°ã€‚åŸå› åœ¨äºï¼Œæ­£å¦‚ä½ ä¹‹å‰å·²ç»è¯»è¿‡çš„é‚£æ ·ï¼Œç”±äºè®¸å¤šé™¤æ³•å’Œä¹˜æ³•æ“ä½œä¸­æ¶‰åŠåˆ°çš„å°æ•°ï¼Œå¯¼è‡´æ•°å€¼ä¸ç¨³å®šã€‚å¹¸è¿çš„æ˜¯ï¼Œè¿˜æœ‰æ›´å¤æ‚å’Œæ•°å€¼ç¨³å®šçš„QRåˆ†è§£æ–¹æ³•ï¼Œä¾‹å¦‚Householderåå°„ã€‚è¿™äº›ç®—æ³•çš„ç»†èŠ‚è¶…å‡ºäº†æœ¬ä¹¦çš„èŒƒå›´ï¼Œä½†ç”±Pythonè°ƒç”¨çš„ä½çº§æ•°å€¼è®¡ç®—åº“å¤„ç†ã€‚
- en: Nevertheless, Iâ€™m going to describe the Gram-Schmidt procedure (sometimes abbreviated
    to GS or G-S) because it shows an application of orthogonal vector decomposition,
    because you are going to program the algorithm in Python based on the following
    math and description, and because GS is the right way to conceptualize how and
    why QR decomposition works even if the low-level implementation is slightly different.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘å°†æè¿° Gram-Schmidt è¿‡ç¨‹ï¼ˆæœ‰æ—¶ç¼©å†™ä¸º GS æˆ– G-Sï¼‰ï¼Œå› ä¸ºå®ƒå±•ç¤ºäº†æ­£äº¤å‘é‡åˆ†è§£çš„åº”ç”¨ï¼Œå› ä¸ºä½ å°†åŸºäºä»¥ä¸‹æ•°å­¦å’Œæè¿°åœ¨ Python
    ä¸­ç¼–ç¨‹è¯¥ç®—æ³•ï¼Œå¹¶ä¸”å› ä¸º GS æ˜¯ç†è§£ QR åˆ†è§£å¦‚ä½•å·¥ä½œçš„æ­£ç¡®æ–¹å¼ï¼Œå³ä½¿ä½çº§å®ç°ç•¥æœ‰ä¸åŒã€‚
- en: A matrix <math alttext="bold upper V"><mi>ğ•</mi></math> comprising columns <math
    alttext="bold v 1"><msub><mi>ğ¯</mi> <mn>1</mn></msub></math> through <math alttext="bold
    v Subscript n"><msub><mi>ğ¯</mi> <mi>n</mi></msub></math> is transformed into an
    orthogonal matrix <math alttext="bold upper Q"><mi>ğ</mi></math> with columns
    <math alttext="bold q Subscript k"><msub><mi>ğª</mi> <mi>k</mi></msub></math> according
    to the following algorithm.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±åˆ— <math alttext="bold v 1"><msub><mi>ğ¯</mi> <mn>1</mn></msub></math> åˆ° <math
    alttext="bold v Subscript n"><msub><mi>ğ¯</mi> <mi>n</mi></msub></math> ç»„æˆçš„çŸ©é˜µ <math
    alttext="bold upper V"><mi>ğ•</mi></math> è¢«è½¬æ¢ä¸ºä¸€ä¸ªå…·æœ‰åˆ— <math alttext="bold q Subscript
    k"><msub><mi>ğª</mi> <mi>k</mi></msub></math> çš„æ­£äº¤çŸ©é˜µ <math alttext="bold upper Q"><mi>ğ</mi></math>
    ï¼Œæ ¹æ®ä»¥ä¸‹ç®—æ³•ã€‚
- en: 'For all column vectors in <math alttext="bold upper V"><mi>ğ•</mi></math> starting
    from the first (leftmost) and moving systematically to the last (rightmost):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºçŸ©é˜µ <math alttext="bold upper V"><mi>ğ•</mi></math> ä¸­ä»ç¬¬ä¸€åˆ—ï¼ˆæœ€å·¦è¾¹ï¼‰å¼€å§‹ç³»ç»Ÿåœ°ç§»åŠ¨åˆ°æœ€åä¸€åˆ—ï¼ˆæœ€å³è¾¹ï¼‰çš„æ‰€æœ‰åˆ—å‘é‡ï¼š
- en: Orthogonalize <math alttext="bold v Subscript k"><msub><mi>ğ¯</mi> <mi>k</mi></msub></math>
    to all previous columns in matrix <math alttext="bold upper Q"><mi>ğ</mi></math>
    using orthogonal vector decomposition. That is, compute the component of <math
    alttext="bold v Subscript k"><msub><mi>ğ¯</mi> <mi>k</mi></msub></math> that is
    perpendicular to <math alttext="bold q Subscript k minus 1"><msub><mi>ğª</mi> <mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    , <math alttext="bold q Subscript k minus 2"><msub><mi>ğª</mi> <mrow><mi>k</mi><mo>-</mo><mn>2</mn></mrow></msub></math>
    , and so on down to <math alttext="bold q 1"><msub><mi>ğª</mi> <mn>1</mn></msub></math>
    . The orthogonalized vector is called <math alttext="bold v Subscript k Superscript
    asterisk"><msubsup><mi>ğ¯</mi> <mi>k</mi> <mo>*</mo></msubsup></math> .^([2](ch09.xhtml#idm45733297664848))
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ­£äº¤å‘é‡åˆ†è§£å°† <math alttext="bold v Subscript k"><msub><mi>ğ¯</mi> <mi>k</mi></msub></math>
    æ­£äº¤åŒ–åˆ°çŸ©é˜µ <math alttext="bold upper Q"><mi>ğ</mi></math> ä¸­æ‰€æœ‰ä¹‹å‰çš„åˆ—ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œè®¡ç®— <math alttext="bold
    v Subscript k"><msub><mi>ğ¯</mi> <mi>k</mi></msub></math> åœ¨ <math alttext="bold
    q Subscript k minus 1"><msub><mi>ğª</mi> <mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    ï¼Œ <math alttext="bold q Subscript k minus 2"><msub><mi>ğª</mi> <mrow><mi>k</mi><mo>-</mo><mn>2</mn></mrow></msub></math>
    ç›´è‡³ <math alttext="bold q 1"><msub><mi>ğª</mi> <mn>1</mn></msub></math> çš„å‚ç›´åˆ†é‡ã€‚æ­£äº¤åŒ–åçš„å‘é‡ç§°ä¸º
    <math alttext="bold v Subscript k Superscript asterisk"><msubsup><mi>ğ¯</mi> <mi>k</mi>
    <mo>*</mo></msubsup></math> ã€‚^([2](ch09.xhtml#idm45733297664848))
- en: Normalize <math alttext="bold v Subscript k Superscript asterisk"><msubsup><mi>ğ¯</mi>
    <mi>k</mi> <mo>*</mo></msubsup></math> to unit length. This is now <math alttext="bold
    q Subscript k"><msub><mi>ğª</mi> <mi>k</mi></msub></math> , the *k*th column in
    matrix <math alttext="bold upper Q"><mi>ğ</mi></math> .
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°† <math alttext="bold v Subscript k Superscript asterisk"><msubsup><mi>ğ¯</mi>
    <mi>k</mi> <mo>*</mo></msubsup></math> å½’ä¸€åŒ–ä¸ºå•ä½é•¿åº¦ã€‚ç°åœ¨è¿™æ˜¯ <math alttext="bold q Subscript
    k"><msub><mi>ğª</mi> <mi>k</mi></msub></math> ï¼ŒçŸ©é˜µ <math alttext="bold upper Q"><mi>ğ</mi></math>
    ä¸­çš„ç¬¬ *k* åˆ—ã€‚
- en: Sounds simple, doesnâ€™t it? Implementing this algorithm in code can be tricky
    because of the repeated orthogonalizations. But with some perseverence, you can
    figure it out ([Exercise 9-2](#exercise_9_2)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¬èµ·æ¥å¾ˆç®€å•ï¼Œä¸æ˜¯å—ï¼Ÿåœ¨ä»£ç ä¸­å®ç°è¿™ä¸ªç®—æ³•å¯èƒ½ä¼šæœ‰äº›æ£˜æ‰‹ï¼Œå› ä¸ºéœ€è¦é‡å¤è¿›è¡Œæ­£äº¤åŒ–ã€‚ä½†åªè¦æœ‰äº›åšæŒï¼Œä½ å°±èƒ½ææ˜ç™½ï¼ˆ[ä¹ é¢˜ 9-2](#exercise_9_2)ï¼‰ã€‚
- en: QR Decomposition
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: QR åˆ†è§£
- en: The GS procedure transforms a matrix into an orthogonal matrix <math alttext="bold
    upper Q"><mi>ğ</mi></math> . (As I wrote in the previous section, <math alttext="bold
    upper Q"><mi>ğ</mi></math> is actually obtained using a series of vector-plane
    reflections known as the Householder transformation, but thatâ€™s due to numerical
    issues; GS is a great way to conceptualize how <math alttext="bold upper Q"><mi>ğ</mi></math>
    matrices are formed.)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: GS è¿‡ç¨‹å°†ä¸€ä¸ªçŸ©é˜µè½¬æ¢ä¸ºæ­£äº¤çŸ©é˜µ <math alttext="bold upper Q"><mi>ğ</mi></math> ã€‚ï¼ˆæ­£å¦‚æˆ‘åœ¨å‰ä¸€èŠ‚ä¸­æ‰€è¿°ï¼Œå®é™…ä¸Šé€šè¿‡ä¸€ç³»åˆ—ç§°ä¸º
    Householder å˜æ¢çš„å‘é‡å¹³é¢åå°„æ¥è·å¾— <math alttext="bold upper Q"><mi>ğ</mi></math> ï¼Œä½†è¿™æ˜¯ç”±äºæ•°å€¼é—®é¢˜ï¼›GS
    æ˜¯ç†è§£ QR åˆ†è§£å½¢æˆçš„ä¸€ä¸ªå¾ˆå¥½çš„æ–¹å¼ã€‚ï¼‰
- en: Whatâ€™s in a Sound?
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å£°éŸ³ä¸­æœ‰ä»€ä¹ˆï¼Ÿ
- en: The â€œQRâ€ in QR decomposition is pronounced â€œqueue are.â€ In my opinion, thatâ€™s
    a real missed opportunity; linear algebra would be more fun to learn if we pronounced
    it â€œQweRty decomposition.â€ Or maybe we could have pronounced it â€œcore decompositionâ€
    to appeal to the fitness crowd. But, for better and for worse, modern conventions
    are shaped by historical precedent.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: â€œQRâ€åˆ†è§£ä¸­çš„â€œQRâ€å‘éŸ³ä¸ºâ€œqueue areâ€ã€‚åœ¨æˆ‘çœ‹æ¥ï¼Œè¿™çœŸæ˜¯ä¸€ä¸ªé”™å¤±çš„æœºä¼šï¼›å¦‚æœæˆ‘ä»¬æŠŠå®ƒå‘éŸ³ä¸ºâ€œQweRty decompositionâ€ï¼Œå­¦ä¹ çº¿æ€§ä»£æ•°å¯èƒ½ä¼šæ›´æœ‰è¶£ã€‚æˆ–è€…ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶å‘éŸ³ä¸ºâ€œcore
    decompositionâ€ï¼Œä»¥å¸å¼•å¥èº«äººç¾¤ã€‚ä½†æ˜¯ï¼Œæ— è®ºå¥½åï¼Œç°ä»£æƒ¯ä¾‹éƒ½å—å†å²å…ˆä¾‹çš„å½±å“ã€‚
- en: '<math alttext="bold upper Q"><mi>ğ</mi></math> is obviously different from
    the original matrix (assuming the original matrix was not orthogonal). So we have
    lost information about that matrix. Fortunately, that â€œlostâ€ information can be
    easily retrieved and stored in another matrix <math alttext="bold upper R"><mi>ğ‘</mi></math>
    that multiplies <math alttext="bold upper Q"><mi>ğ</mi></math> .^([3](ch09.xhtml#idm45733297640832))
    That leads to the question of how we create <math alttext="bold upper R"><mi>ğ‘</mi></math>
    . In fact, creating <math alttext="bold upper R"><mi>ğ‘</mi></math> is straightforward
    and comes right from the definition of QR:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç„¶ï¼Œ<math alttext="bold upper Q"><mi>ğ</mi></math>ä¸åŸå§‹çŸ©é˜µä¸åŒï¼ˆå‡è®¾åŸå§‹çŸ©é˜µä¸æ˜¯æ­£äº¤çš„ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¤±å»äº†å…³äºè¯¥çŸ©é˜µçš„ä¿¡æ¯ã€‚å¹¸è¿çš„æ˜¯ï¼Œè¿™äº›â€œä¸¢å¤±â€çš„ä¿¡æ¯å¯ä»¥è½»æ¾åœ°æ¢å¤å¹¶å­˜å‚¨åœ¨å¦ä¸€ä¸ªçŸ©é˜µ<math
    alttext="bold upper R"><mi>ğ‘</mi></math>ä¸­ï¼Œè¯¥çŸ©é˜µä¹˜ä»¥<math alttext="bold upper Q"><mi>ğ</mi></math>ã€‚^([3](ch09.xhtml#idm45733297640832))
    è¿™å°±å¼•å‡ºäº†æˆ‘ä»¬å¦‚ä½•åˆ›å»º<math alttext="bold upper R"><mi>ğ‘</mi></math>çš„é—®é¢˜ã€‚äº‹å®ä¸Šï¼Œåˆ›å»º<math alttext="bold
    upper R"><mi>ğ‘</mi></math>æ˜¯ç›´æˆªäº†å½“çš„ï¼Œå¹¶ç›´æ¥æ¥è‡ªQRçš„å®šä¹‰ï¼š
- en: <math alttext="StartLayout 1st Row 1st Column bold upper A 2nd Column equals
    bold upper Q bold upper R 2nd Row 1st Column bold upper Q Superscript upper T
    Baseline bold upper A 2nd Column equals bold upper Q Superscript upper T Baseline
    bold upper Q bold upper R 3rd Row 1st Column bold upper Q Superscript upper T
    Baseline bold upper A 2nd Column equals bold upper R EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mi>ğ€</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>ğ</mi> <mi>ğ‘</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msup><mi>ğ</mi>
    <mtext>T</mtext></msup> <mi>ğ€</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mi>ğ</mi> <mtext>T</mtext></msup> <mi>ğ</mi> <mi>ğ‘</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msup><mi>ğ</mi> <mtext>T</mtext></msup> <mi>ğ€</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>ğ‘</mi></mrow></mtd></mtr></mtable></math>
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper A 2nd Column equals
    bold upper Q bold upper R 2nd Row 1st Column bold upper Q Superscript upper T
    Baseline bold upper A 2nd Column equals bold upper Q Superscript upper T Baseline
    bold upper Q bold upper R 3rd Row 1st Column bold upper Q Superscript upper T
    Baseline bold upper A 2nd Column equals bold upper R EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mi>ğ€</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>ğ</mi> <mi>ğ‘</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msup><mi>ğ</mi>
    <mtext>T</mtext></msup> <mi>ğ€</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mi>ğ</mi> <mtext>T</mtext></msup> <mi>ğ</mi> <mi>ğ‘</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msup><mi>ğ</mi> <mtext>T</mtext></msup> <mi>ğ€</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>ğ‘</mi></mrow></mtd></mtr></mtable></math>
- en: 'Here you see the beauty of orthogonal matrices: we can solve matrix equations
    without having to worry about computing the inverse.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæ‚¨å¯ä»¥çœ‹åˆ°æ­£äº¤çŸ©é˜µçš„ç¾å¦™ä¹‹å¤„ï¼šæˆ‘ä»¬å¯ä»¥è§£å†³çŸ©é˜µæ–¹ç¨‹ï¼Œè€Œä¸å¿…æ‹…å¿ƒè®¡ç®—é€†çŸ©é˜µã€‚
- en: 'The following Python code shows how to compute QR decomposition of a square
    matrix, and [FigureÂ 9-1](#fig_9_1) illustrates the three matrices:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹Pythonä»£ç å±•ç¤ºäº†å¦‚ä½•è®¡ç®—æ–¹é˜µçš„QRåˆ†è§£ï¼Œ[å›¾Â 9-1](#fig_9_1)è¯´æ˜äº†è¿™ä¸‰ä¸ªçŸ©é˜µï¼š
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![QweRty](assets/plad_0901.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![QweRty](assets/plad_0901.png)'
- en: Figure 9-1\. QR decomposition of a random-numbers matrix
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾9-1ã€‚éšæœºæ•°çŸ©é˜µçš„QRåˆ†è§£
- en: Several important features of QR decomposition are visible in [FigureÂ 9-1](#fig_9_1),
    including that <math alttext="bold upper A equals bold upper Q bold upper R"><mrow><mi>ğ€</mi>
    <mo>=</mo> <mi>ğ</mi> <mi>ğ‘</mi></mrow></math> (their difference is the zeros
    matrix) and that <math alttext="bold upper Q"><mi>ğ</mi></math> times its transpose
    gives the identity matrix.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: QRåˆ†è§£çš„å‡ ä¸ªé‡è¦ç‰¹å¾åœ¨[å›¾Â 9-1](#fig_9_1)ä¸­å¯è§ï¼ŒåŒ…æ‹¬<math alttext="bold upper A equals bold upper
    Q bold upper R"><mrow><mi>ğ€</mi> <mo>=</mo> <mi>ğ</mi> <mi>ğ‘</mi></mrow></math>ï¼ˆå®ƒä»¬çš„å·®å¼‚æ˜¯é›¶çŸ©é˜µï¼‰ä»¥åŠ<math
    alttext="bold upper Q"><mi>ğ</mi></math>ä¹˜ä»¥å…¶è½¬ç½®çŸ©é˜µå¾—åˆ°å•ä½çŸ©é˜µã€‚
- en: 'Check out the <math alttext="bold upper R"><mi>ğ‘</mi></math> matrix: itâ€™s upper-triangular
    (all elements below the diagonal are zero). That seems unlikely to have occurred
    by chance, considering we started from a random matrix. In fact, the <math alttext="bold
    upper R"><mi>ğ‘</mi></math> matrix is *always* upper-triangular. To understand
    why, you need to think about the GS algorithm and the organization of the dot
    products in matrix multiplication. I will explain why <math alttext="bold upper
    R"><mi>ğ‘</mi></math> is upper-triangular in the next section; before then, I want
    you to come up with an answer.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥<math alttext="bold upper R"><mi>ğ‘</mi></math>çŸ©é˜µï¼šå®ƒæ˜¯ä¸Šä¸‰è§’çŸ©é˜µï¼ˆå¯¹è§’çº¿ä»¥ä¸‹çš„æ‰€æœ‰å…ƒç´ éƒ½ä¸ºé›¶ï¼‰ã€‚è€ƒè™‘åˆ°æˆ‘ä»¬ä»ä¸€ä¸ªéšæœºçŸ©é˜µå¼€å§‹ï¼Œè¿™ä¼¼ä¹ä¸å¯èƒ½æ˜¯å¶ç„¶å‘ç”Ÿçš„ã€‚äº‹å®ä¸Šï¼Œ<math
    alttext="bold upper R"><mi>ğ‘</mi></math>çŸ©é˜µæ€»æ˜¯ä¸Šä¸‰è§’çš„ã€‚è¦ç†è§£å…¶ä¸­çš„åŸå› ï¼Œæ‚¨éœ€è¦æ€è€ƒGSç®—æ³•å’ŒçŸ©é˜µä¹˜æ³•ä¸­ç‚¹ç§¯çš„ç»„ç»‡æ–¹å¼ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘å°†è§£é‡Šä¸ºä»€ä¹ˆ<math
    alttext="bold upper R"><mi>ğ‘</mi></math>æ˜¯ä¸Šä¸‰è§’çš„ï¼›åœ¨é‚£ä¹‹å‰ï¼Œæˆ‘å¸Œæœ›æ‚¨èƒ½æƒ³å‡ºä¸€ä¸ªç­”æ¡ˆã€‚
- en: Sizes of Q and R
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Qå’ŒRçš„å°ºå¯¸
- en: The sizes of <math alttext="bold upper Q"><mi>ğ</mi></math> and <math alttext="bold
    upper R"><mi>ğ‘</mi></math> depend on the size of the to-be-decomposed matrix <math
    alttext="bold upper A"><mi>ğ€</mi></math> and on whether the QR decomposition is
    â€œeconomyâ€ (also called â€œreducedâ€) or â€œfullâ€ (also called â€œcompleteâ€). [FigureÂ 9-2](#fig_9_2)
    shows an overview of all possible sizes.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper Q"><mi>ğ</mi></math>å’Œ<math alttext="bold upper R"><mi>ğ‘</mi></math>çš„å°ºå¯¸å–å†³äºå¾…åˆ†è§£çŸ©é˜µ<math
    alttext="bold upper A"><mi>ğ€</mi></math>çš„å¤§å°ï¼Œä»¥åŠQRåˆ†è§£æ˜¯â€œç»æµâ€ï¼ˆä¹Ÿç§°ä¸ºâ€œå‡å°‘â€ï¼‰è¿˜æ˜¯â€œå®Œå…¨â€ï¼ˆä¹Ÿç§°ä¸ºâ€œå®Œæ•´â€ï¼‰ã€‚[å›¾Â 9-2](#fig_9_2)æ˜¾ç¤ºäº†æ‰€æœ‰å¯èƒ½å°ºå¯¸çš„æ¦‚è¿°ã€‚
- en: 'Economy versus full QR decomposition applies only to tall matrices. The question
    is this: for a tall matrix (*M* > *N*), do we create a <math alttext="bold upper
    Q"><mi>ğ</mi></math> matrix with *N* columns or *M* columns? The former option
    is called *economy* or *reduced*, and gives a tall <math alttext="bold upper Q"><mi>ğ</mi></math>
    ; and the latter option is called *full* or *complete*, and gives a square <math
    alttext="bold upper Q"><mi>ğ</mi></math> .'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç»æµä¸å®Œæ•´çš„QRåˆ†è§£ä»…é€‚ç”¨äºé«˜çŸ©é˜µã€‚é—®é¢˜åœ¨äºå¯¹äºä¸€ä¸ªé«˜çŸ©é˜µï¼ˆ*M* > *N*ï¼‰ï¼Œæˆ‘ä»¬æ˜¯åˆ›å»ºä¸€ä¸ªå…·æœ‰*N*åˆ—è¿˜æ˜¯*M*åˆ—çš„<math alttext="bold
    upper Q"><mi>ğ</mi></math>çŸ©é˜µï¼Ÿå‰è€…é€‰é¡¹ç§°ä¸º*ç»æµ*æˆ–*å‡å°‘*ï¼Œç»™å‡ºä¸€ä¸ªé«˜<math alttext="bold upper Q"><mi>ğ</mi></math>ï¼›åè€…é€‰é¡¹ç§°ä¸º*å®Œæ•´*æˆ–*å®Œå…¨*ï¼Œç»™å‡ºä¸€ä¸ªæ–¹å½¢<math
    alttext="bold upper Q"><mi>ğ</mi></math>ã€‚
- en: '![QR sizes](assets/plad_0902.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![QR sizes](assets/plad_0902.png)'
- en: Figure 9-2\. Sizes of **Q** and **R** depending on the size of **A**. The â€œ?â€
    indicates that the matrix elements depend on the values in **A**, i.e., it is
    not the identity matrix.
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾9-2ã€‚**Q**å’Œ**R**çš„å¤§å°å–å†³äº**A**çš„å¤§å°ã€‚â€œ?â€è¡¨ç¤ºçŸ©é˜µå…ƒç´ å–å†³äº**A**ä¸­çš„å€¼ï¼Œå³å®ƒä¸æ˜¯å•ä½çŸ©é˜µã€‚
- en: 'It may seem surprising that <math alttext="bold upper Q"><mi>ğ</mi></math>
    can be square when <math alttext="bold upper A"><mi>ğ€</mi></math> is tall (in
    other words, that <math alttext="bold upper Q"><mi>ğ</mi></math> can have more
    columns than <math alttext="bold upper A"><mi>ğ€</mi></math> ): where do the extra
    columns come from? It is, in fact, possible to craft orthogonal vectors â€œout of
    thin air.â€ Consider the following example in Python:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å½“<math alttext="bold upper A"><mi>ğ€</mi></math>é«˜æ—¶ï¼ˆæ¢å¥è¯è¯´ï¼Œ<math alttext="bold upper
    Q"><mi>ğ</mi></math>å¯ä»¥æ¯”<math alttext="bold upper A"><mi>ğ€</mi></math>åˆ—æ›´å¤šï¼‰ï¼Œ<math
    alttext="bold upper Q"><mi>ğ</mi></math>å¯ä»¥æ˜¯æ–¹å½¢çš„äº‹å®ï¼Œé¢å¤–çš„åˆ—æ¥è‡ªå“ªé‡Œï¼Ÿäº‹å®ä¸Šï¼Œå¯ä»¥â€œå‡­ç©ºâ€åˆ¶ä½œæ­£äº¤å‘é‡ã€‚è€ƒè™‘ä»¥ä¸‹Pythonç¤ºä¾‹ï¼š
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Notice the optional second input `'complete'`, which produces a full QR decomposition.
    Setting that to `'reduced'`, which is the default, gives the economy-mode QR decomposition,
    in which <math alttext="bold upper Q"><mi>ğ</mi></math> is the same size as <math
    alttext="bold upper A"><mi>ğ€</mi></math> .
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„å¯é€‰çš„ç¬¬äºŒä¸ªè¾“å…¥`'complete'`ï¼Œå®ƒä¼šäº§ç”Ÿå®Œæ•´çš„QRåˆ†è§£ã€‚å°†å…¶è®¾ç½®ä¸º`'reduced'`ï¼ˆé»˜è®¤æƒ…å†µï¼‰ï¼Œåˆ™ä¼šå¾—åˆ°ç»æµæ¨¡å¼çš„QRåˆ†è§£ï¼Œå…¶ä¸­<math
    alttext="bold upper Q"><mi>ğ</mi></math>çš„å¤§å°ä¸<math alttext="bold upper A"><mi>ğ€</mi></math>ç›¸åŒã€‚
- en: Because it is possible to craft more than *M* > *N* orthogonal vectors from
    a matrix with *N* columns, the rank of <math alttext="bold upper Q"><mi>ğ</mi></math>
    is always the maximum possible rank, which is *M* for all square <math alttext="bold
    upper Q"><mi>ğ</mi></math> matrices and *N* for the economy <math alttext="bold
    upper Q"><mi>ğ</mi></math> . The rank of <math alttext="bold upper R"><mi>ğ‘</mi></math>
    is the same as the rank of <math alttext="bold upper A"><mi>ğ€</mi></math> .
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºå¯ä»¥ä»å…·æœ‰*N*åˆ—çš„çŸ©é˜µä¸­åˆ¶ä½œå¤šäº*M* > *N*ä¸ªæ­£äº¤å‘é‡ï¼Œæ‰€ä»¥<math alttext="bold upper Q"><mi>ğ</mi></math>çš„ç§©æ€»æ˜¯å¯èƒ½çš„æœ€å¤§ç§©ï¼Œå¯¹äºæ‰€æœ‰æ–¹å½¢<math
    alttext="bold upper Q"><mi>ğ</mi></math>çŸ©é˜µä¸º*M*ï¼Œå¯¹äºç»æµ<math alttext="bold upper Q"><mi>ğ</mi></math>ä¸º*N*ã€‚<math
    alttext="bold upper R"><mi>ğ‘</mi></math>çš„ç§©ä¸<math alttext="bold upper A"><mi>ğ€</mi></math>çš„ç§©ç›¸åŒã€‚
- en: The difference in rank between <math alttext="bold upper Q"><mi>ğ</mi></math>
    and <math alttext="bold upper A"><mi>ğ€</mi></math> resulting from orthogonalization
    means that <math alttext="bold upper Q"><mi>ğ</mi></math> spans all of <math alttext="double-struck
    upper R Superscript upper M"><msup><mi>â„</mi> <mi>M</mi></msup></math> even if
    the column space of <math alttext="bold upper A"><mi>ğ€</mi></math> is only a lower-dimensional
    subspace of <math alttext="double-struck upper R Superscript upper M"><msup><mi>â„</mi>
    <mi>M</mi></msup></math> . That fact is central to why the singular value decomposition
    is so useful for revealing properties of a matrix, including its rank and null
    space. Yet another reason to look forward to learning about the SVD in [ChapterÂ 14](ch14.xhtml#Chapter_14)!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±æ­£äº¤åŒ–å¼•èµ·çš„<math alttext="bold upper Q"><mi>ğ</mi></math>å’Œ<math alttext="bold upper
    A"><mi>ğ€</mi></math>ç§©çš„å·®å¼‚æ„å‘³ç€<math alttext="bold upper Q"><mi>ğ</mi></math>è·¨è¶Šäº†æ‰€æœ‰<math
    alttext="double-struck upper R Superscript upper M"><msup><mi>â„</mi> <mi>M</mi></msup></math>ï¼Œå³ä½¿<math
    alttext="bold upper A"><mi>ğ€</mi></math>çš„åˆ—ç©ºé—´ä»…æ˜¯<math alttext="double-struck upper
    R Superscript upper M"><msup><mi>â„</mi> <mi>M</mi></msup></math>çš„ä½ç»´å­ç©ºé—´ã€‚è¿™ä¸ªäº‹å®æ˜¯ä¸ºä»€ä¹ˆå¥‡å¼‚å€¼åˆ†è§£å¯¹äºæ­ç¤ºçŸ©é˜µçš„æ€§è´¨ï¼ŒåŒ…æ‹¬å…¶ç§©å’Œé›¶ç©ºé—´ï¼Œéå¸¸æœ‰ç”¨çš„ä¸­å¿ƒåŸå› ã€‚æœŸå¾…åœ¨[ç¬¬14ç« ](ch14.xhtml#Chapter_14)å­¦ä¹ SVDçš„å¦ä¸€ä¸ªåŸå› ï¼
- en: 'A note about uniqueness: QR decomposition is not unique for all matrix sizes
    and ranks. This means that it is possible to obtain <math alttext="bold upper
    A equals bold upper Q 1 bold upper R 1"><mrow><mi>ğ€</mi> <mo>=</mo> <msub><mi>ğ</mi>
    <mn>1</mn></msub> <msub><mi>ğ‘</mi> <mn>1</mn></msub></mrow></math> and <math alttext="bold
    upper A equals bold upper Q 2 bold upper R 2"><mrow><mi>ğ€</mi> <mo>=</mo> <msub><mi>ğ</mi>
    <mn>2</mn></msub> <msub><mi>ğ‘</mi> <mn>2</mn></msub></mrow></math> where <math
    alttext="bold upper Q 1 not-equals bold upper Q 2"><mrow><msub><mi>ğ</mi> <mn>1</mn></msub>
    <mo>â‰ </mo> <msub><mi>ğ</mi> <mn>2</mn></msub></mrow></math> . However, all QR
    decomposition results have the same properties described in this section. QR decomposition
    can be made unique given additional constraints (e.g., positive values on the
    diagonals of <math alttext="bold upper R"><mi>ğ‘</mi></math> ), although this is
    not necessary in most cases, and is not implemented in Python or MATLAB. Youâ€™ll
    see this nonuniqueness when comparing GS to QR in [Exercise 9-2](#exercise_9_2).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºå”¯ä¸€æ€§çš„æ³¨æ„äº‹é¡¹ï¼šQRåˆ†è§£å¯¹äºæ‰€æœ‰çŸ©é˜µå¤§å°å’Œç§©éƒ½ä¸æ˜¯å”¯ä¸€çš„ã€‚è¿™æ„å‘³ç€å¯èƒ½ä¼šå¾—åˆ° <math alttext="bold upper A equals
    bold upper Q 1 bold upper R 1"><mrow><mi>ğ€</mi> <mo>=</mo> <msub><mi>ğ</mi> <mn>1</mn></msub>
    <msub><mi>ğ‘</mi> <mn>1</mn></msub></mrow></math> å’Œ <math alttext="bold upper A
    equals bold upper Q 2 bold upper R 2"><mrow><mi>ğ€</mi> <mo>=</mo> <msub><mi>ğ</mi>
    <mn>2</mn></msub> <msub><mi>ğ‘</mi> <mn>2</mn></msub></mrow></math>ï¼Œå…¶ä¸­ <math alttext="bold
    upper Q 1 not-equals bold upper Q 2"><mrow><msub><mi>ğ</mi> <mn>1</mn></msub>
    <mo>â‰ </mo> <msub><mi>ğ</mi> <mn>2</mn></msub></mrow></math> ã€‚ç„¶è€Œï¼Œæ‰€æœ‰QRåˆ†è§£ç»“æœåœ¨æœ¬èŠ‚ä¸­æè¿°çš„å±æ€§ä¸Šéƒ½æ˜¯ç›¸åŒçš„ã€‚é€šè¿‡é™„åŠ çº¦æŸï¼ˆä¾‹å¦‚ï¼Œ<math
    alttext="bold upper R"><mi>ğ‘</mi></math> çš„å¯¹è§’çº¿ä¸Šçš„æ­£å€¼ï¼‰ï¼Œå¯ä»¥ä½¿QRåˆ†è§£å”¯ä¸€åŒ–ï¼Œå°½ç®¡åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹è¿™å¹¶ä¸æ˜¯å¿…éœ€çš„ï¼Œå¹¶ä¸”åœ¨Pythonæˆ–MATLABä¸­ä¹Ÿæ²¡æœ‰å®ç°ã€‚åœ¨[Exercise
    9-2](#exercise_9_2)ä¸­æ¯”è¾ƒGSå’ŒQRæ—¶ï¼Œä½ ä¼šçœ‹åˆ°è¿™ç§éå”¯ä¸€æ€§ã€‚
- en: Why <math alttext="bold upper R"><mi>ğ‘</mi></math> is upper triangular
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆ <math alttext="bold upper R"><mi>ğ‘</mi></math> æ˜¯ä¸Šä¸‰è§’å½¢çš„
- en: I hope you gave this question some serious thought. Itâ€™s a tricky point about
    QR decomposition, so if you couldnâ€™t figure it out on your own, then please read
    the following few paragraphs, and then look away from the book/screen and rederive
    the argument.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›æ‚¨è®¤çœŸè€ƒè™‘äº†è¿™ä¸ªé—®é¢˜ã€‚è¿™æ˜¯å…³äºQRåˆ†è§£çš„ä¸€ä¸ªæ£˜æ‰‹ç‚¹ï¼Œå¦‚æœæ‚¨è‡ªå·±æ— æ³•å¼„æ¸…æ¥šï¼Œè¯·é˜…è¯»æ¥ä¸‹æ¥çš„å‡ æ®µæ–‡å­—ï¼Œç„¶åè¿œç¦»ä¹¦æœ¬/å±å¹•ï¼Œé‡æ–°æ¨å¯¼å‡ºè¿™ä¸ªè®ºç‚¹ã€‚
- en: 'I will start by reminding you of three facts:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†é¦–å…ˆæé†’æ‚¨ä¸‰ä¸ªäº‹å®ï¼š
- en: <math alttext="bold upper R"><mi>ğ‘</mi></math> comes from the formula <math
    alttext="bold upper Q Superscript upper T Baseline bold upper A equals bold upper
    R"><mrow><msup><mi>ğ</mi> <mtext>T</mtext></msup> <mi>ğ€</mi> <mo>=</mo> <mi>ğ‘</mi></mrow></math>
    .
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="bold upper R"><mi>ğ‘</mi></math> æ¥è‡ªå…¬å¼ <math alttext="bold upper
    Q Superscript upper T Baseline bold upper A equals bold upper R"><mrow><msup><mi>ğ</mi>
    <mtext>T</mtext></msup> <mi>ğ€</mi> <mo>=</mo> <mi>ğ‘</mi></mrow></math> ã€‚
- en: The lower triangle of a product matrix comprises dot products between *later*
    rows of the left matrix and *earlier* columns of the right matrix.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªä¹˜ç§¯çŸ©é˜µçš„ä¸‹ä¸‰è§’ç”±å·¦çŸ©é˜µçš„*åç»­*è¡Œå’Œå³çŸ©é˜µçš„*å…ˆå‰*åˆ—çš„ç‚¹ç§¯ç»„æˆã€‚
- en: The rows of <math alttext="bold upper Q Superscript upper T"><msup><mi>ğ</mi>
    <mtext>T</mtext></msup></math> are the columns of <math alttext="bold upper Q"><mi>ğ</mi></math>
    .
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="bold upper Q Superscript upper T"><msup><mi>ğ</mi> <mtext>T</mtext></msup></math>
    çš„è¡Œæ˜¯ <math alttext="bold upper Q"><mi>ğ</mi></math> çš„åˆ—ã€‚
- en: 'Putting those together: because orthogonalization works column-wise from left
    to right, *later* columns in <math alttext="bold upper Q"><mi>ğ</mi></math> are
    orthogonalized to *earlier* columns of <math alttext="bold upper A"><mi>ğ€</mi></math>
    . Therefore, the lower triangle of <math alttext="bold upper R"><mi>ğ‘</mi></math>
    comes from pairs of vectors that have been orthogonalized. In contrast, *earlier*
    columns in <math alttext="bold upper Q"><mi>ğ</mi></math> are not orthogonalized
    to *later* columns of <math alttext="bold upper A"><mi>ğ€</mi></math> , so we would
    not expect their dot products to be zero.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å®ƒä»¬æ”¾åœ¨ä¸€èµ·ï¼šå› ä¸ºæ­£äº¤åŒ–æ˜¯ä»å·¦åˆ°å³é€åˆ—è¿›è¡Œçš„ï¼Œ<math alttext="bold upper Q"><mi>ğ</mi></math> ä¸­çš„*åç»­*åˆ—è¢«æ­£äº¤åŒ–ä¸º<math
    alttext="bold upper A"><mi>ğ€</mi></math> ä¸­çš„*å…ˆå‰*åˆ—ã€‚å› æ­¤ï¼Œ<math alttext="bold upper
    R"><mi>ğ‘</mi></math> çš„ä¸‹ä¸‰è§’æ¥è‡ªå·²ç»æ­£äº¤åŒ–çš„å‘é‡å¯¹ã€‚ç›¸åï¼Œ<math alttext="bold upper Q"><mi>ğ</mi></math>
    ä¸­çš„*å…ˆå‰*åˆ—æœªè¢«æ­£äº¤åŒ–ä¸º<math alttext="bold upper A"><mi>ğ€</mi></math> ä¸­çš„*åç»­*åˆ—ï¼Œå› æ­¤æˆ‘ä»¬ä¸ä¼šæœŸæœ›å®ƒä»¬çš„ç‚¹ç§¯ä¸ºé›¶ã€‚
- en: 'Final comment: if columns *i* and *j* of <math alttext="bold upper A"><mi>ğ€</mi></math>
    were already orthogonal, then the corresponding (*i,j*)th element in <math alttext="bold
    upper R"><mi>ğ‘</mi></math> would be zero. In fact, if you compute the QR decomposition
    of an orthogonal matrix, then <math alttext="bold upper R"><mi>ğ‘</mi></math> will
    be a diagonal matrix in which the diagonal elements are the norms of each column
    in <math alttext="bold upper A"><mi>ğ€</mi></math> . That means that if <math alttext="bold
    upper A equals bold upper Q"><mrow><mi>ğ€</mi> <mo>=</mo> <mi>ğ</mi></mrow></math>
    , then <math alttext="bold upper R equals bold upper I"><mrow><mi>ğ‘</mi> <mo>=</mo>
    <mi>ğˆ</mi></mrow></math> , which is obvious from the equation solved for <math
    alttext="bold upper R"><mi>ğ‘</mi></math> . Youâ€™ll get to explore this in [Exercise
    9-3](#exercise_9_3).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åè¯„è®ºï¼šå¦‚æœ <math alttext="bold upper A"><mi>ğ€</mi></math> çš„ç¬¬ *i* åˆ—å’Œç¬¬ *j* åˆ—å·²ç»æ­£äº¤ï¼Œåˆ™
    <math alttext="bold upper R"><mi>ğ‘</mi></math> çš„å¯¹åº” (*i,j*) å…ƒç´ å°†ä¸ºé›¶ã€‚äº‹å®ä¸Šï¼Œå¦‚æœä½ è®¡ç®—ä¸€ä¸ªæ­£äº¤çŸ©é˜µçš„
    QR åˆ†è§£ï¼Œé‚£ä¹ˆ <math alttext="bold upper R"><mi>ğ‘</mi></math> å°†æ˜¯ä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼Œå…¶å¯¹è§’çº¿å…ƒç´ æ˜¯ <math
    alttext="bold upper A"><mi>ğ€</mi></math> æ¯åˆ—çš„èŒƒæ•°ã€‚è¿™æ„å‘³ç€å¦‚æœ <math alttext="bold upper
    A equals bold upper Q"><mrow><mi>ğ€</mi> <mo>=</mo> <mi>ğ</mi></mrow></math> ï¼Œé‚£ä¹ˆ
    <math alttext="bold upper R equals bold upper I"><mrow><mi>ğ‘</mi> <mo>=</mo> <mi>ğˆ</mi></mrow></math>
    ï¼Œè¿™ä»æ±‚è§£ <math alttext="bold upper R"><mi>ğ‘</mi></math> çš„æ–¹ç¨‹å¼ä¸­æ˜¾è€Œæ˜“è§ã€‚ä½ å°†åœ¨ [ç»ƒä¹  9-3](#exercise_9_3)
    ä¸­æ·±å…¥æ¢è®¨è¿™ä¸€ç‚¹ã€‚
- en: QR and Inverses
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: QR å’Œé€†çŸ©é˜µ
- en: QR decomposition provides a more numerically stable way to compute the matrix
    inverse.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: QR åˆ†è§£æä¾›äº†ä¸€ç§è®¡ç®—çŸ©é˜µé€†çš„æ›´ä¸ºæ•°å€¼ç¨³å®šçš„æ–¹å¼ã€‚
- en: 'Letâ€™s start by writing out the QR decomposition formula and inverting both
    sides of the equation (note the application of the LIVE EVIL rule):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é¦–å…ˆå†™å‡º QR åˆ†è§£å…¬å¼ï¼Œå¹¶å€’è½¬æ–¹ç¨‹çš„ä¸¤ä¾§ï¼ˆæ³¨æ„åº”ç”¨ LIVE EVIL è§„åˆ™ï¼‰ï¼š
- en: <math alttext="StartLayout 1st Row 1st Column bold upper A 2nd Column equals
    bold upper Q bold upper R 2nd Row 1st Column bold upper A Superscript negative
    1 2nd Column equals left-parenthesis bold upper Q bold upper R right-parenthesis
    Superscript negative 1 Baseline 3rd Row 1st Column bold upper A Superscript negative
    1 2nd Column equals bold upper R Superscript negative 1 Baseline bold upper Q
    Superscript negative 1 Baseline 4th Row 1st Column bold upper A Superscript negative
    1 2nd Column equals bold upper R Superscript negative 1 Baseline bold upper Q
    Superscript upper T EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mi>ğ€</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>ğ</mi> <mi>ğ‘</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><msup><mi>ğ€</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mrow><mo>(</mo><mi>ğ</mi><mi>ğ‘</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msup><mi>ğ€</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi>ğ‘</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi>ğ</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><msup><mi>ğ€</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi>ğ‘</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi>ğ</mi> <mtext>T</mtext></msup></mrow></mtd></mtr></mtable></math>
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper A 2nd Column equals
    bold upper Q bold upper R 2nd Row 1st Column bold upper A Superscript negative
    1 2nd Column equals left-parenthesis bold upper Q bold upper R right-parenthesis
    Superscript negative 1 Baseline 3rd Row 1st Column bold upper A Superscript negative
    1 2nd Column equals bold upper R Superscript negative 1 Baseline bold upper Q
    Superscript negative 1 Baseline 4th Row 1st Column bold upper A Superscript negative
    1 2nd Column equals bold upper R Superscript negative 1 Baseline bold upper Q
    Superscript upper T EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mi>ğ€</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>ğ</mi> <mi>ğ‘</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><msup><mi>ğ€</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mrow><mo>(</mo><mi>ğ</mi><mi>ğ‘</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msup><mi>ğ€</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi>ğ‘</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi>ğ</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><msup><mi>ğ€</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi>ğ‘</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi>ğ</mi> <mtext>T</mtext></msup></mrow></mtd></mtr></mtable></math>
- en: Thus, we can obtain the inverse of <math alttext="bold upper A"><mi>ğ€</mi></math>
    as the inverse of <math alttext="bold upper R"><mi>ğ‘</mi></math> times the transpose
    of <math alttext="bold upper Q"><mi>ğ</mi></math> . <math alttext="bold upper
    Q"><mi>ğ</mi></math> is numerically stable due to the Householder reflection algorithm,
    and <math alttext="bold upper R"><mi>ğ‘</mi></math> is numerically stable because
    it simply results from matrix multiplication.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°† <math alttext="bold upper A"><mi>ğ€</mi></math> çš„é€†çŸ©é˜µè¡¨ç¤ºä¸º <math alttext="bold
    upper R"><mi>ğ‘</mi></math> çš„é€†ä¹˜ä»¥ <math alttext="bold upper Q"><mi>ğ</mi></math>
    çš„è½¬ç½®ã€‚ç”±äº Householder åå°„ç®—æ³•ï¼Œ<math alttext="bold upper Q"><mi>ğ</mi></math> æ˜¯æ•°å€¼ç¨³å®šçš„ï¼Œè€Œç”±çŸ©é˜µä¹˜æ³•å¾—åˆ°çš„
    <math alttext="bold upper R"><mi>ğ‘</mi></math> ä¹Ÿæ˜¯æ•°å€¼ç¨³å®šçš„ã€‚
- en: 'Now, we still need to invert <math alttext="bold upper R"><mi>ğ‘</mi></math>
    explicitly, but inverting triangular matrices is highly numerically stable when
    done through a procedure called back substitution. Youâ€™ll learn more about that
    in the next chapter, but the key point is this: an important application of QR
    decomposition is providing a more numerically stable way to invert matrices, compared
    to the algorithm presented in the previous chapter.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦æ˜ç¡®åœ°æ±‚è§£ <math alttext="bold upper R"><mi>ğ‘</mi></math> çš„é€†ï¼Œä½†é€šè¿‡ç§°ä¸ºå›ä»£çš„è¿‡ç¨‹æ¥æ±‚è§£ä¸Šä¸‰è§’çŸ©é˜µåœ¨æ•°å€¼ä¸Šæ˜¯é«˜åº¦ç¨³å®šçš„ã€‚ä½ å°†åœ¨ä¸‹ä¸€ç« èŠ‚ä¸­äº†è§£æ›´å¤šï¼Œä½†å…³é”®ç‚¹åœ¨äºï¼šQR
    åˆ†è§£çš„ä¸€ä¸ªé‡è¦åº”ç”¨æ˜¯æä¾›ä¸€ç§æ¯”å‰ä¸€ç« èŠ‚ä»‹ç»çš„ç®—æ³•æ›´ä¸ºæ•°å€¼ç¨³å®šçš„æ–¹å¼æ¥æ±‚é€†çŸ©é˜µã€‚
- en: On the other hand, keep in mind that matrices that are theoretically invertible
    but are close to singular are still very difficult to invert; QR decomposition
    may be *more* numerically stable than the traditional algorithm presented in the
    previous chapter, but that doesnâ€™t guarantee a high-quality inverse. A rotten
    apple dipped in honey is still rotten.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œè¯·è®°ä½ï¼Œç†è®ºä¸Šå¯é€†ä½†æ¥è¿‘å¥‡å¼‚çš„çŸ©é˜µä»ç„¶éå¸¸éš¾ä»¥æ±‚é€†ï¼›QR åˆ†è§£å¯èƒ½æ¯”å‰ä¸€ç« èŠ‚ä»‹ç»çš„ä¼ ç»Ÿç®—æ³•*æ›´*æ•°å€¼ç¨³å®šï¼Œä½†è¿™å¹¶ä¸ä¿è¯é«˜è´¨é‡çš„é€†çŸ©é˜µã€‚æµ¸åœ¨èœ‚èœœä¸­çš„çƒ‚è‹¹æœä»ç„¶æ˜¯çƒ‚çš„ã€‚
- en: Summary
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: 'QR decomposition is great. Itâ€™s definitely on my list of the top five awesomest
    matrix decompositions in linear algebra. Here are the key take-home messages of
    this chapter:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: QR åˆ†è§£éå¸¸æ£’ã€‚å®ƒç»å¯¹æ˜¯çº¿æ€§ä»£æ•°ä¸­å‰äº”ä¸ªæœ€æ£’çš„çŸ©é˜µåˆ†è§£ä¹‹ä¸€ã€‚ä»¥ä¸‹æ˜¯æœ¬ç« çš„ä¸»è¦è¦ç‚¹ï¼š
- en: An orthogonal matrix has columns that are pair-wise orthogonal and with norm
    = 1\. Orthogonal matrices are key to several matrix decompositions, including
    QR, eigendecomposition, and singular value decomposition. Orthogonal matrices
    are also important in geometry and computer graphics (e.g., pure rotation matrices).
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ­£äº¤çŸ©é˜µå…·æœ‰ä¸¤ä¸¤æ­£äº¤ä¸”èŒƒæ•°ä¸º 1 çš„åˆ—ã€‚æ­£äº¤çŸ©é˜µæ˜¯å‡ ç§çŸ©é˜µåˆ†è§£çš„å…³é”®ï¼ŒåŒ…æ‹¬ QR åˆ†è§£ã€ç‰¹å¾åˆ†è§£å’Œå¥‡å¼‚å€¼åˆ†è§£ã€‚åœ¨å‡ ä½•å­¦å’Œè®¡ç®—æœºå›¾å½¢å­¦ä¸­ï¼ˆå¦‚çº¯æ—‹è½¬çŸ©é˜µï¼‰ï¼Œæ­£äº¤çŸ©é˜µä¹Ÿéå¸¸é‡è¦ã€‚
- en: You can transform a nonorthogonal matrix into an orthogonal matrix via the Gram-Schmidt
    procedure, which involves applying orthogonal vector decomposition to isolate
    the component of each column that is orthogonal to all previous columns (â€œpreviousâ€
    meaning left to right).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡Gram-Schmidtè¿‡ç¨‹å°†éæ­£äº¤çŸ©é˜µè½¬æ¢ä¸ºæ­£äº¤çŸ©é˜µï¼Œè¯¥è¿‡ç¨‹æ¶‰åŠåº”ç”¨æ­£äº¤å‘é‡åˆ†è§£æ¥éš”ç¦»æ¯ä¸€åˆ—ä¸­ä¸æ‰€æœ‰å‰ä¸€åˆ—æ­£äº¤çš„åˆ†é‡ï¼ˆâ€œå‰ä¸€åˆ—â€æŒ‡ä»å·¦åˆ°å³ï¼‰ã€‚
- en: QR decomposition is the result of Gram-Schmidt (technically, it is implemented
    by a more stable algorithm, but GS is still the right way to understand it).
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QRåˆ†è§£æ˜¯Gram-Schmidtçš„ç»“æœï¼ˆä¸¥æ ¼æ¥è¯´ï¼Œå®ƒæ˜¯é€šè¿‡æ›´ç¨³å®šçš„ç®—æ³•å®ç°çš„ï¼Œä½†GSä»æ˜¯ç†è§£å®ƒçš„æ­£ç¡®æ–¹å¼ï¼‰ã€‚
- en: Code Exercises
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»£ç ç»ƒä¹ 
- en: Exercise 9-1\.
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  9-1\.
- en: 'A square <math alttext="bold upper Q"><mi>ğ</mi></math> has the following equalities:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ–¹é˜µ<math alttext="bold upper Q"><mi>ğ</mi></math>å…·æœ‰ä»¥ä¸‹ç­‰å¼ï¼š
- en: <math alttext="bold upper Q Superscript upper T Baseline bold upper Q equals
    bold upper Q bold upper Q Superscript upper T Baseline equals bold upper Q Superscript
    hyphen 1 Baseline bold upper Q equals bold upper Q bold upper Q Superscript hyphen
    1 Baseline equals bold upper I" display="block"><mrow><msup><mi>ğ</mi> <mtext>T</mtext></msup>
    <mi>ğ</mi> <mo>=</mo> <mi>ğ</mi> <msup><mi>ğ</mi> <mtext>T</mtext></msup> <mo>=</mo>
    <msup><mi>ğ</mi> <mtext>-1</mtext></msup> <mi>ğ</mi> <mo>=</mo> <mi>ğ</mi> <msup><mi>ğ</mi>
    <mtext>-1</mtext></msup> <mo>=</mo> <mi>ğˆ</mi></mrow></math>
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper Q Superscript upper T Baseline bold upper Q equals
    bold upper Q bold upper Q Superscript upper T Baseline equals bold upper Q Superscript
    hyphen 1 Baseline bold upper Q equals bold upper Q bold upper Q Superscript hyphen
    1 Baseline equals bold upper I" display="block"><mrow><msup><mi>ğ</mi> <mtext>T</mtext></msup>
    <mi>ğ</mi> <mo>=</mo> <mi>ğ</mi> <msup><mi>ğ</mi> <mtext>T</mtext></msup> <mo>=</mo>
    <msup><mi>ğ</mi> <mtext>-1</mtext></msup> <mi>ğ</mi> <mo>=</mo> <mi>ğ</mi> <msup><mi>ğ</mi>
    <mtext>-1</mtext></msup> <mo>=</mo> <mi>ğˆ</mi></mrow></math>
- en: Demonstrate this in code by computing <math alttext="bold upper Q"><mi>ğ</mi></math>
    from a random-numbers matrix, then compute <math alttext="bold upper Q Superscript
    upper T"><msup><mi>ğ</mi> <mtext>T</mtext></msup></math> and <math alttext="bold
    upper Q Superscript negative 1"><msup><mi>ğ</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    . Then show that all four expressions produce the identity matrix.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä»éšæœºæ•°çŸ©é˜µè®¡ç®—<math alttext="bold upper Q"><mi>ğ</mi></math>æ¥åœ¨ä»£ç ä¸­æ¼”ç¤ºè¿™ä¸€ç‚¹ï¼Œç„¶åè®¡ç®—<math
    alttext="bold upper Q Superscript upper T"><msup><mi>ğ</mi> <mtext>T</mtext></msup></math>å’Œ<math
    alttext="bold upper Q Superscript negative 1"><msup><mi>ğ</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>ã€‚ç„¶åå±•ç¤ºè¿™å››ä¸ªè¡¨è¾¾å¼éƒ½äº§ç”Ÿå•ä½çŸ©é˜µã€‚
- en: Exercise 9-2\.
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  9-2\.
- en: Implement the Gram-Schmidt procedure as described earlier.^([4](ch09.xhtml#idm45733297281040))
    Use a <math alttext="4 times 4"><mrow><mn>4</mn> <mo>Ã—</mo> <mn>4</mn></mrow></math>
    random-numbers matrix. Check your answer against <math alttext="bold upper Q"><mi>ğ</mi></math>
    from `np.linalg.qr`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‰ç…§å‰è¿°æ­¥éª¤å®ç°Gram-Schmidtè¿‡ç¨‹ã€‚ä½¿ç”¨ä¸€ä¸ª<math alttext="4 times 4"><mrow><mn>4</mn> <mo>Ã—</mo>
    <mn>4</mn></mrow></math>éšæœºæ•°çŸ©é˜µã€‚æ£€æŸ¥æ‚¨çš„ç­”æ¡ˆæ˜¯å¦ä¸`np.linalg.qr`ä¸­çš„<math alttext="bold upper
    Q"><mi>ğ</mi></math>ä¸€è‡´ã€‚
- en: '**Important**: there is a fundamental sign uncertainty in transformations like
    the Householder reflection. This means that vectors can â€œflipâ€ (be multiplied
    by âˆ’1) depending on minor differences in algorithm and implementation. This feature
    exists in many matrix decompositions including eigenvectors. I have a longer and
    more in-depth discussion of why this is and what it means in [ChapterÂ 13](ch13.xhtml#Chapter_13).
    For now, the upshot is this: *subtract* your <math alttext="bold upper Q"><mi>ğ</mi></math>
    from Pythonâ€™s <math alttext="bold upper Q"><mi>ğ</mi></math> and *add* your <math
    alttext="bold upper Q"><mi>ğ</mi></math> and Pythonâ€™s <math alttext="bold upper
    Q"><mi>ğ</mi></math> . Nonzero columns in one will be zeros in the other.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**é‡è¦æç¤º**ï¼šåœ¨åƒHouseholderåå°„è¿™æ ·çš„å˜æ¢ä¸­å­˜åœ¨æ ¹æœ¬æ€§çš„ç¬¦å·ä¸ç¡®å®šæ€§ã€‚è¿™æ„å‘³ç€å‘é‡å¯ä»¥æ ¹æ®ç®—æ³•å’Œå®ç°çš„ç»†å¾®å·®å¼‚â€œç¿»è½¬â€ï¼ˆä¹˜ä»¥-1ï¼‰ã€‚è¿™ç§ç‰¹æ€§å­˜åœ¨äºè®¸å¤šçŸ©é˜µåˆ†è§£ä¸­ï¼ŒåŒ…æ‹¬ç‰¹å¾å‘é‡ã€‚æˆ‘åœ¨[ç¬¬13ç« ](ch13.xhtml#Chapter_13)ä¸­å¯¹æ­¤è¿›è¡Œäº†æ›´é•¿æ›´æ·±å…¥çš„è®¨è®ºã€‚æš‚ä¸”è€Œè¨€ï¼Œè¦ç‚¹æ˜¯è¿™æ ·çš„ï¼šä»Pythonçš„<math
    alttext="bold upper Q"><mi>ğ</mi></math>ä¸­*å‡å»*æ‚¨çš„<math alttext="bold upper Q"><mi>ğ</mi></math>ï¼Œå¹¶*åŠ ä¸Š*æ‚¨çš„<math
    alttext="bold upper Q"><mi>ğ</mi></math>å’ŒPythonçš„<math alttext="bold upper Q"><mi>ğ</mi></math>ã€‚å…¶ä¸­ä¸€ä¸ªä¸­çš„éé›¶åˆ—å°†åœ¨å¦ä¸€ä¸ªä¸­ä¸ºé›¶åˆ—ã€‚'
- en: Exercise 9-3\.
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  9-3\.
- en: In this exercise, you will find out what happens when you apply QR decomposition
    to a matrix that is almost-but-not-quite orthogonal. First, create an orthogonal
    matrix, called <math alttext="bold upper U"><mi>ğ”</mi></math> , from the QR decomposition
    of a <math alttext="6 times 6"><mrow><mn>6</mn> <mo>Ã—</mo> <mn>6</mn></mrow></math>
    random-numbers matrix. Compute the QR decomposition of <math alttext="bold upper
    U"><mi>ğ”</mi></math> , and confirm that <math alttext="bold upper R equals bold
    upper I"><mrow><mi>ğ‘</mi> <mo>=</mo> <mi>ğˆ</mi></mrow></math> (and make sure you
    understand why!).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç»ƒä¹ ä¸­ï¼Œæ‚¨å°†äº†è§£åœ¨å°†å‡ ä¹æ­£äº¤çš„çŸ©é˜µåº”ç”¨QRåˆ†è§£æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆã€‚é¦–å…ˆï¼Œä»ä¸€ä¸ª<math alttext="6 times 6"><mrow><mn>6</mn>
    <mo>Ã—</mo> <mn>6</mn></mrow></math>éšæœºæ•°çŸ©é˜µçš„QRåˆ†è§£åˆ›å»ºä¸€ä¸ªç§°ä¸º<math alttext="bold upper U"><mi>ğ”</mi></math>çš„æ­£äº¤çŸ©é˜µã€‚è®¡ç®—<math
    alttext="bold upper U"><mi>ğ”</mi></math>çš„QRåˆ†è§£ï¼Œå¹¶ç¡®è®¤<math alttext="bold upper R equals
    bold upper I"><mrow><mi>ğ‘</mi> <mo>=</mo> <mi>ğˆ</mi></mrow></math>ï¼ˆå¹¶ç¡®ä¿æ‚¨ç†è§£ä¸ºä»€ä¹ˆï¼ï¼‰ã€‚
- en: Second, modify the norms of each column of <math alttext="bold upper U"><mi>ğ”</mi></math>
    . Set the norms of columns 1â€“6 to be 10â€“15 (that is, the first column of <math
    alttext="bold upper U"><mi>ğ”</mi></math> should have a norm of 10, the second
    column should have a norm of 11, and so on). Run that modulated <math alttext="bold
    upper U"><mi>ğ”</mi></math> matrix through QR decomposition and confirm that its
    <math alttext="bold upper R"><mi>ğ‘</mi></math> is a diagonal matrix with diagonal
    elements equaling 10â€“15\. What is <math alttext="bold upper Q Superscript upper
    T Baseline bold upper Q"><mrow><msup><mi>ğ</mi> <mtext>T</mtext></msup> <mi>ğ</mi></mrow></math>
    for this matrix?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶æ¬¡ï¼Œä¿®æ”¹æ¯åˆ— <math alttext="bold upper U"><mi>ğ”</mi></math> çš„èŒƒæ•°ã€‚å°†åˆ— 1â€“6 çš„èŒƒæ•°è®¾ç½®ä¸º 10â€“15ï¼ˆå³ï¼Œ<math
    alttext="bold upper U"><mi>ğ”</mi></math> çš„ç¬¬ä¸€åˆ—åº”è¯¥æœ‰ä¸€ä¸ªèŒƒæ•°ä¸º 10ï¼Œç¬¬äºŒåˆ—åº”è¯¥æœ‰ä¸€ä¸ªèŒƒæ•°ä¸º 11ï¼Œä¾æ­¤ç±»æ¨ï¼‰ã€‚å°†è°ƒæ•´åçš„
    <math alttext="bold upper U"><mi>ğ”</mi></math> çŸ©é˜µé€šè¿‡ QR åˆ†è§£ï¼Œå¹¶ç¡®è®¤å…¶ <math alttext="bold
    upper R"><mi>ğ‘</mi></math> æ˜¯å¯¹è§’çŸ©é˜µï¼Œå¯¹è§’çº¿å…ƒç´ ç­‰äº 10â€“15\. å¯¹äºè¿™ä¸ªçŸ©é˜µï¼Œ<math alttext="bold upper
    Q Superscript upper T Baseline bold upper Q"><mrow><msup><mi>ğ</mi> <mtext>T</mtext></msup>
    <mi>ğ</mi></mrow></math> æ˜¯ä»€ä¹ˆï¼Ÿ
- en: Third, break the orthogonality of <math alttext="bold upper U"><mi>ğ”</mi></math>
    by setting element <math alttext="u Subscript 1 comma 4 Baseline equals 0"><mrow><msub><mi>u</mi>
    <mrow><mn>1</mn><mo>,</mo><mn>4</mn></mrow></msub> <mo>=</mo> <mn>0</mn></mrow></math>
    . What happens to <math alttext="bold upper R"><mi>ğ‘</mi></math> and why?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰æ­¥ï¼Œé€šè¿‡è®¾ç½®å…ƒç´  <math alttext="u Subscript 1 comma 4 Baseline equals 0"><mrow><msub><mi>u</mi>
    <mrow><mn>1</mn><mo>,</mo><mn>4</mn></mrow></msub> <mo>=</mo> <mn>0</mn></mrow></math>
    æ¥æ‰“ç ´ <math alttext="bold upper U"><mi>ğ”</mi></math> çš„æ­£äº¤æ€§ã€‚ <math alttext="bold upper
    R"><mi>ğ‘</mi></math> ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
- en: Exercise 9-4\.
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  9-4\.
- en: The purpose of this exercise is to compare the numerical errors using the â€œold-schoolâ€
    inverse method you learned in the previous chapter to the QR-based inverse method.
    We will use random-numbers matrices, keeping in mind that they tend to be numerically
    stable and thus have accurate inverses.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç»ƒä¹ çš„ç›®çš„æ˜¯æ¯”è¾ƒä½¿ç”¨å‰ä¸€ç« å­¦åˆ°çš„â€œè€æ´¾â€é€†æ–¹æ³•ä¸åŸºäº QR çš„é€†æ–¹æ³•çš„æ•°å€¼è¯¯å·®ã€‚æˆ‘ä»¬å°†ä½¿ç”¨éšæœºæ•°çŸ©é˜µï¼Œæ³¨æ„å®ƒä»¬å€¾å‘äºæ˜¯æ•°å€¼ç¨³å®šçš„ï¼Œå› æ­¤å…·æœ‰å‡†ç¡®çš„é€†çŸ©é˜µã€‚
- en: 'Hereâ€™s what to do: copy the code from [Exercise 8-2](ch08.xhtml#exercise_8_2)
    into a Python function that takes a matrix as input and provides its inverse as
    output. (You can also include a check that the input matrix is square and full-rank.)
    I called this function `oldSchoolInv`. Next, create a <math alttext="5 times 5"><mrow><mn>5</mn>
    <mo>Ã—</mo> <mn>5</mn></mrow></math> random-numbers matrix. Compute its inverse
    using the old-school method and the QR decomposition method introduced in this
    chapter (you can use your â€œold-school methodâ€ to compute <math alttext="bold upper
    R Superscript negative 1"><msup><mi>ğ‘</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    ). Compute the inverse-estimation error as the Euclidean distance from the matrix
    times its inverse to the true identity matrix from `np.eye`. Make a barplot of
    the results, showing the two methods on the *x*-axis and the error (Euclidean
    distance to <math alttext="bold upper I"><mi>ğˆ</mi></math> ) on the *y*-axis,
    as in [FigureÂ 9-3](#fig_9_3).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯è¯¥åšçš„äº‹æƒ…ï¼šå°†ä»£ç ä» [Exercise 8-2](ch08.xhtml#exercise_8_2) å¤åˆ¶åˆ°ä¸€ä¸ª Python å‡½æ•°ä¸­ï¼Œè¯¥å‡½æ•°ä»¥çŸ©é˜µä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºå…¶é€†çŸ©é˜µã€‚ï¼ˆæ‚¨è¿˜å¯ä»¥åŒ…å«ä¸€ä¸ªæ£€æŸ¥è¾“å…¥çŸ©é˜µæ˜¯å¦ä¸ºæ–¹é˜µå’Œæ»¡ç§©çš„æ£€æŸ¥ã€‚ï¼‰æˆ‘å°†è¿™ä¸ªå‡½æ•°ç§°ä¸º
    `oldSchoolInv`ã€‚æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸€ä¸ª <math alttext="5 times 5"><mrow><mn>5</mn> <mo>Ã—</mo>
    <mn>5</mn></mrow></math> çš„éšæœºæ•°çŸ©é˜µã€‚ä½¿ç”¨ä¸Šä¸€ç« ä»‹ç»çš„â€œè€æ´¾æ–¹æ³•â€å’Œæœ¬ç« ä»‹ç»çš„ QR åˆ†è§£æ–¹æ³•è®¡ç®—å…¶é€†çŸ©é˜µï¼ˆæ‚¨å¯ä»¥ä½¿ç”¨æ‚¨çš„â€œè€æ´¾æ–¹æ³•â€è®¡ç®—
    <math alttext="bold upper R Superscript negative 1"><msup><mi>ğ‘</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    ï¼‰ã€‚å°†çŸ©é˜µä¹˜ä»¥å…¶é€†çŸ©é˜µï¼Œå¹¶è®¡ç®—åˆ°çœŸå®å•ä½çŸ©é˜µ `np.eye` çš„æ¬§æ°è·ç¦»ä½œä¸ºé€†ä¼°è®¡è¯¯å·®ã€‚åˆ¶ä½œä¸€ä¸ªæ¡å½¢å›¾æ˜¾ç¤ºç»“æœï¼Œå°†ä¸¤ç§æ–¹æ³•æ˜¾ç¤ºåœ¨ *x* è½´ä¸Šï¼Œè¯¯å·®ï¼ˆåˆ°
    <math alttext="bold upper I"><mi>ğˆ</mi></math> çš„æ¬§æ°è·ç¦»ï¼‰æ˜¾ç¤ºåœ¨ *y* è½´ä¸Šï¼Œå°±åƒ [FigureÂ 9-3](#fig_9_3)
    ä¸­æ‰€ç¤ºã€‚
- en: '![QR and oldschool](assets/plad_0903.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![QR å’Œ oldschool](assets/plad_0903.png)'
- en: Figure 9-3\. Results of Exercise 9-4
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 9-3\. ç»ƒä¹  9-4 çš„ç»“æœ
- en: Run the code multiple times and inspect the barplot. Youâ€™ll find that sometimes
    the old-school method is better while other times the QR method is better (smaller
    numbers are better; in theory, the bars should have zero height). Try it again
    using a <math alttext="30 times 30"><mrow><mn>30</mn> <mo>Ã—</mo> <mn>30</mn></mrow></math>
    matrix. Are the results more consistent? In fact, there is a lot of variance from
    run to run. This means we should run an experiment where we repeat the comparison
    many times. Thatâ€™s the next exercise.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šæ¬¡è¿è¡Œä»£ç å¹¶æ£€æŸ¥æ¡å½¢å›¾ã€‚æ‚¨ä¼šå‘ç°æœ‰æ—¶å€™â€œè€æ´¾â€æ–¹æ³•æ›´å¥½ï¼Œè€Œå…¶ä»–æ—¶å€™ QR æ–¹æ³•æ›´å¥½ï¼ˆè¾ƒå°çš„æ•°å­—æ›´å¥½ï¼›ç†è®ºä¸Šï¼Œæ¡åº”è¯¥é«˜åº¦ä¸ºé›¶ï¼‰ã€‚å°è¯•ä½¿ç”¨ <math alttext="30
    times 30"><mrow><mn>30</mn> <mo>Ã—</mo> <mn>30</mn></mrow></math> çŸ©é˜µå†æ¬¡å°è¯•ã€‚ç»“æœæ›´ä¸€è‡´äº†å—ï¼Ÿå®é™…ä¸Šï¼Œæ¯æ¬¡è¿è¡Œç»“æœéƒ½æœ‰å¾ˆå¤§å·®å¼‚ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬åº”è¯¥è¿›è¡Œä¸€é¡¹å®éªŒï¼Œå¤šæ¬¡é‡å¤æ¯”è¾ƒã€‚è¿™å°±æ˜¯ä¸‹ä¸€ä¸ªç»ƒä¹ ã€‚
- en: Exercise 9-5\.
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  9-5\.
- en: Put the code from the previous exercise into a `for` loop over a hundred iterations
    in which you repeat the experiment, each time using a different random-numbers
    matrix. Store the error (Euclidean distance) for each iteration, and make a plot
    like [FigureÂ 9-4](#fig_9_4), which shows the average over all experiment runs
    (gray bar) and all individual errors (black dots). Run the experiment for <math
    alttext="5 times 5"><mrow><mn>5</mn> <mo>Ã—</mo> <mn>5</mn></mrow></math> and <math
    alttext="30 times 30"><mrow><mn>30</mn> <mo>Ã—</mo> <mn>30</mn></mrow></math> matrices.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å‰ä¸€ä¸ªç»ƒä¹ ä¸­çš„ä»£ç æ”¾å…¥ä¸€ä¸ª`for`å¾ªç¯ä¸­ï¼Œé‡å¤100æ¬¡å®éªŒï¼Œæ¯æ¬¡ä½¿ç”¨ä¸åŒçš„éšæœºæ•°çŸ©é˜µã€‚å­˜å‚¨æ¯æ¬¡è¿­ä»£çš„è¯¯å·®ï¼ˆæ¬§å‡ é‡Œå¾—è·ç¦»ï¼‰ï¼Œå¹¶åˆ¶ä½œåƒ[å›¾Â 9-4](#fig_9_4)é‚£æ ·çš„å›¾è¡¨ï¼Œæ˜¾ç¤ºæ‰€æœ‰å®éªŒè¿è¡Œçš„å¹³å‡å€¼ï¼ˆç°è‰²æ¡ï¼‰å’Œæ‰€æœ‰å•ä¸ªè¯¯å·®ï¼ˆé»‘è‰²ç‚¹ï¼‰ã€‚è¿è¡Œ<math
    alttext="5 times 5"><mrow><mn>5</mn> <mo>Ã—</mo> <mn>5</mn></mrow></math>å’Œ<math
    alttext="30 times 30"><mrow><mn>30</mn> <mo>Ã—</mo> <mn>30</mn></mrow></math>çŸ©é˜µçš„å®éªŒã€‚
- en: You can also try using `np.linalg.inv` to invert <math alttext="bold upper R"><mi>ğ‘</mi></math>
    instead of the old-school method to see if that has an effect.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¹Ÿå¯ä»¥å°è¯•ä½¿ç”¨`np.linalg.inv`æ¥åè½¬<math alttext="bold upper R"><mi>ğ‘</mi></math>ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„æ–¹æ³•ï¼Œçœ‹çœ‹æ˜¯å¦æœ‰å½±å“ã€‚
- en: '![QR > oldschool](assets/plad_0904.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![QR > oldschool](assets/plad_0904.png)'
- en: Figure 9-4\. Results of Exercise 9-5\. Note the difference in y-axis scaling
    between the left and right panels.
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾Â 9-4\. ç»ƒä¹ Â 9-5Â çš„ç»“æœã€‚è¯·æ³¨æ„å·¦å³é¢æ¿åœ¨yè½´ç¼©æ”¾ä¸Šçš„å·®å¼‚ã€‚
- en: Exercise 9-6\.
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹ Â 9-6.
- en: An interesting property of square orthogonal matrices is that all of their singular
    values (and their eigenvalues) are 1\. That means that they have an induced 2-norm
    of 1 (the induced norm is the largest singular value), and they have a Frobenius
    norm of *M*. The latter result is because the Frobenius norm equals the square
    root of the sum of the squared singular values. In this exercise, you will confirm
    these properties.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹é˜µæ­£äº¤çŸ©é˜µçš„ä¸€ä¸ªæœ‰è¶£ç‰¹æ€§æ˜¯å®ƒä»¬çš„æ‰€æœ‰å¥‡å¼‚å€¼ï¼ˆåŠå…¶ç‰¹å¾å€¼ï¼‰éƒ½æ˜¯1ã€‚è¿™æ„å‘³ç€å®ƒä»¬æœ‰ä¸€ä¸ªè¯±å¯¼2-èŒƒæ•°ä¸º1ï¼ˆè¯±å¯¼èŒƒæ•°æ˜¯æœ€å¤§å¥‡å¼‚å€¼ï¼‰ï¼Œå¹¶ä¸”å®ƒä»¬æœ‰ä¸€ä¸ªFrobeniusèŒƒæ•°ä¸º*M*ã€‚åè€…çš„ç»“æœæ˜¯å› ä¸ºFrobeniusèŒƒæ•°ç­‰äºæ‰€æœ‰å¥‡å¼‚å€¼çš„å¹³æ–¹å’Œçš„å¹³æ–¹æ ¹ã€‚åœ¨è¿™ä¸ªç»ƒä¹ ä¸­ï¼Œä½ å°†ç¡®è®¤è¿™äº›ç‰¹æ€§ã€‚
- en: Create an <math alttext="upper M times upper M"><mrow><mi>M</mi> <mo>Ã—</mo>
    <mi>M</mi></mrow></math> orthogonal matrix as the QR decomposition of a random
    matrix. Compute its induced 2-norm using `np.linalg.norm` and compute its Frobenius
    norm using the equation you learned in [ChapterÂ 6](ch06.xhtml#Chapter_6), divided
    by the square root of *M*. Confirm that both quantities are 1 (to within a reasonable
    tolerance of rounding error). Check using several different values of *M*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ª<math alttext="upper M times upper M"><mrow><mi>M</mi> <mo>Ã—</mo> <mi>M</mi></mrow></math>æ­£äº¤çŸ©é˜µä½œä¸ºä¸€ä¸ªéšæœºçŸ©é˜µçš„QRåˆ†è§£ã€‚ä½¿ç”¨`np.linalg.norm`è®¡ç®—å…¶è¯±å¯¼2-èŒƒæ•°ï¼Œå¹¶ä½¿ç”¨ä½ åœ¨[ç¬¬Â 6Â ç« ](ch06.xhtml#Chapter_6)å­¦åˆ°çš„å…¬å¼è®¡ç®—å…¶FrobeniusèŒƒæ•°ï¼Œé™¤ä»¥*M*çš„å¹³æ–¹æ ¹ã€‚ç¡®è®¤è¿™ä¸¤ä¸ªé‡çº¦ä¸º1ï¼ˆè€ƒè™‘åˆ°å››èˆäº”å…¥è¯¯å·®çš„åˆç†å®¹å·®ï¼‰ã€‚ä½¿ç”¨å‡ ä¸ªä¸åŒçš„*M*å€¼è¿›è¡Œæ£€æŸ¥ã€‚
- en: Next, explore the meaning of the induced norm using matrix-vector multiplication.
    Create a random *M*-element column vector <math alttext="bold v"><mi>ğ¯</mi></math>
    . Then compute the norms of <math alttext="bold v"><mi>ğ¯</mi></math> and <math
    alttext="bold upper Q bold v"><mrow><mi>ğ</mi> <mi>ğ¯</mi></mrow></math> . Those
    norms should equal each other (although you wouldnâ€™t expect them to equal 1).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œé€šè¿‡çŸ©é˜µ-å‘é‡ä¹˜æ³•æ¢ç´¢è¯±å¯¼èŒƒæ•°çš„å«ä¹‰ã€‚åˆ›å»ºä¸€ä¸ªéšæœºçš„*M*å…ƒç´ åˆ—å‘é‡<math alttext="bold v"><mi>ğ¯</mi></math>ã€‚ç„¶åè®¡ç®—<math
    alttext="bold v"><mi>ğ¯</mi></math>å’Œ<math alttext="bold upper Q bold v"><mrow><mi>ğ</mi>
    <mi>ğ¯</mi></mrow></math>çš„èŒƒæ•°ã€‚è¿™äº›èŒƒæ•°åº”è¯¥ç›¸ç­‰ï¼ˆå°½ç®¡ä½ ä¸ä¼šæœŸæœ›å®ƒä»¬ç­‰äº1ï¼‰ã€‚
- en: Finally, get a piece of paper and develop a proof of that empirical demonstration.
    That proof is printed in the next paragraph, so donâ€™t look down! But you can check
    the footnote if you need a hint.^([5](ch09.xhtml#idm45733297196544))
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ‹¿ä¸€å¼ çº¸ï¼Œå‘å±•ä¸€ä¸ªè¯æ˜è¿™ä¸ªç»éªŒæ¼”ç¤ºçš„è¯æ˜ã€‚è¿™ä¸ªè¯æ˜åœ¨ä¸‹ä¸€æ®µæ‰“å°å‡ºæ¥ï¼Œæ‰€ä»¥ä¸è¦å¾€ä¸‹çœ‹ï¼ä½†å¦‚æœä½ éœ€è¦æç¤ºï¼Œå¯ä»¥æŸ¥çœ‹è„šæ³¨^([5](ch09.xhtml#idm45733297196544))ã€‚
- en: I sincerely hope you are reading this to check your reasoning, not because you
    are cheating! Anyway, the proof is that the vector norm <math alttext="parallel-to
    bold v parallel-to"><mrow><mo>âˆ¥</mo> <mi>ğ¯</mi> <mo>âˆ¥</mo></mrow></math> can be
    computed as <math alttext="bold v Superscript upper T Baseline bold v"><mrow><msup><mi>ğ¯</mi>
    <mtext>T</mtext></msup> <mi>ğ¯</mi></mrow></math> ; therefore, the vector norm
    <math alttext="parallel-to bold upper Q bold v parallel-to"><mrow><mo>âˆ¥</mo> <mi>ğ</mi>
    <mi>ğ¯</mi> <mo>âˆ¥</mo></mrow></math> is computed as <math alttext="left-parenthesis
    bold upper Q bold v right-parenthesis Superscript upper T Baseline bold upper
    Q bold v equals bold v Superscript upper T Baseline bold upper Q Superscript upper
    T Baseline bold upper Q bold v"><mrow><msup><mrow><mo>(</mo><mi>ğ</mi><mi>ğ¯</mi><mo>)</mo></mrow>
    <mtext>T</mtext></msup> <mi>ğ</mi> <mi>ğ¯</mi> <mo>=</mo> <msup><mi>ğ¯</mi> <mtext>T</mtext></msup>
    <msup><mi>ğ</mi> <mtext>T</mtext></msup> <mi>ğ</mi> <mi>ğ¯</mi></mrow></math> .
    The <math alttext="bold upper Q Superscript upper T Baseline bold upper Q"><mrow><msup><mi>ğ</mi>
    <mtext>T</mtext></msup> <mi>ğ</mi></mrow></math> cancels to give the identity
    matrix, leaving the dot product of the vector with itself. The conclusion is that
    orthogonal matrices can rotate but never scale a vector.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çœŸè¯šåœ°å¸Œæœ›ä½ è¯»è¿™ç¯‡æ–‡ç« æ˜¯ä¸ºäº†æ£€æŸ¥ä½ çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œä¸æ˜¯ä½œå¼Šï¼ä¸ç®¡æ€æ ·ï¼Œè¯æ˜æ˜¯å‘é‡èŒƒæ•° <math alttext="parallel-to bold v
    parallel-to"><mrow><mo>âˆ¥</mo> <mi>ğ¯</mi> <mo>âˆ¥</mo></mrow></math> å¯ä»¥è®¡ç®—ä¸º <math
    alttext="bold v Superscript upper T Baseline bold v"><mrow><msup><mi>ğ¯</mi> <mtext>T</mtext></msup>
    <mi>ğ¯</mi></mrow></math> ï¼›å› æ­¤ï¼Œå‘é‡èŒƒæ•° <math alttext="parallel-to bold upper Q bold
    v parallel-to"><mrow><mo>âˆ¥</mo> <mi>ğ</mi> <mi>ğ¯</mi> <mo>âˆ¥</mo></mrow></math>
    è®¡ç®—ä¸º <math alttext="left-parenthesis bold upper Q bold v right-parenthesis Superscript
    upper T Baseline bold upper Q bold v equals bold v Superscript upper T Baseline
    bold upper Q Superscript upper T Baseline bold upper Q bold v"><mrow><msup><mrow><mo>(</mo><mi>ğ</mi><mi>ğ¯</mi><mo>)</mo></mrow>
    <mtext>T</mtext></msup> <mi>ğ</mi> <mi>ğ¯</mi> <mo>=</mo> <msup><mi>ğ¯</mi> <mtext>T</mtext></msup>
    <msup><mi>ğ</mi> <mtext>T</mtext></msup> <mi>ğ</mi> <mi>ğ¯</mi></mrow></math> ã€‚<math
    alttext="bold upper Q Superscript upper T Baseline bold upper Q"><mrow><msup><mi>ğ</mi>
    <mtext>T</mtext></msup> <mi>ğ</mi></mrow></math> å–æ¶ˆä»¥å¾—åˆ°å•ä½çŸ©é˜µï¼Œç•™ä¸‹å‘é‡ä¸è‡ªèº«çš„ç‚¹ç§¯ã€‚ç»“è®ºæ˜¯æ­£äº¤çŸ©é˜µå¯ä»¥æ—‹è½¬ä½†ä¸ä¼šç¼©æ”¾å‘é‡ã€‚
- en: Exercise 9-7\.
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  9-7ã€‚
- en: 'This exercise will highlight one feature of the <math alttext="bold upper R"><mi>ğ‘</mi></math>
    matrix that is relevant for understanding how to use QR to implement least squares
    ([ChapterÂ 12](ch12.xhtml#Chapter_12)): when <math alttext="bold upper A"><mi>ğ€</mi></math>
    is tall and full column-rank, the first *N* rows of <math alttext="bold upper
    R"><mi>ğ‘</mi></math> are upper-triangular, whereas rows *N* + 1 through *M* are
    zeros. Confirm this in Python using a random <math alttext="10 times 4"><mrow><mn>10</mn>
    <mo>Ã—</mo> <mn>4</mn></mrow></math> matrix. Make sure to use the complete (full)
    QR decomposition, not the economy (compact) decomposition.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç»ƒä¹ å°†çªå‡º <math alttext="bold upper R"><mi>ğ‘</mi></math> çŸ©é˜µçš„ä¸€ä¸ªç‰¹ç‚¹ï¼Œè¿™ä¸ªç‰¹ç‚¹å¯¹äºç†è§£å¦‚ä½•ä½¿ç”¨
    QR åˆ†è§£æ¥å®ç°æœ€å°äºŒä¹˜æ³•ï¼ˆ[ç¬¬ 12 ç« ](ch12.xhtml#Chapter_12)ï¼‰éå¸¸é‡è¦ï¼šå½“ <math alttext="bold upper
    A"><mi>ğ€</mi></math> é«˜ä¸”æ»¡åˆ—ç§©æ—¶ï¼Œ<math alttext="bold upper R"><mi>ğ‘</mi></math> çš„å‰
    *N* è¡Œæ˜¯ä¸Šä¸‰è§’çš„ï¼Œè€Œç¬¬ *N* + 1 è¡Œåˆ° *M* è¡Œæ˜¯é›¶ã€‚åœ¨ Python ä¸­ä½¿ç”¨ä¸€ä¸ªéšæœºçš„ <math alttext="10 times 4"><mrow><mn>10</mn>
    <mo>Ã—</mo> <mn>4</mn></mrow></math> çŸ©é˜µè¿›è¡Œç¡®è®¤ã€‚ç¡®ä¿ä½¿ç”¨å®Œå…¨ï¼ˆå…¨ï¼‰QR åˆ†è§£ï¼Œè€Œä¸æ˜¯ç»æµï¼ˆç´§å‡‘ï¼‰åˆ†è§£ã€‚
- en: Of course, <math alttext="bold upper R"><mi>ğ‘</mi></math> is noninvertible because
    it is nonsquare. But (1) the submatrix comprising the first *N* rows is square
    and full-rank (when <math alttext="bold upper A"><mi>ğ€</mi></math> is full column-rank)
    and thus has a full inverse, and (2) the tall <math alttext="bold upper R"><mi>ğ‘</mi></math>
    has a pseudoinverse. Compute both inverses, and confirm that the full inverse
    of the first *N* rows of <math alttext="bold upper R"><mi>ğ‘</mi></math> equals
    the first *N* columns of the pseudoinverse of the tall <math alttext="bold upper
    R"><mi>ğ‘</mi></math> .
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œ<math alttext="bold upper R"><mi>ğ‘</mi></math> æ˜¯ä¸å¯é€†çš„ï¼Œå› ä¸ºå®ƒä¸æ˜¯æ–¹é˜µã€‚ä½†æ˜¯ï¼ˆ1ï¼‰å½“<math
    alttext="bold upper A"><mi>ğ€</mi></math>æ˜¯æ»¡åˆ—ç§©æ—¶ï¼Œç¬¬ä¸€ *N* è¡Œæ„æˆçš„å­çŸ©é˜µæ˜¯æ–¹é˜µä¸”æ»¡ç§©ï¼Œå› æ­¤å…·æœ‰å®Œå…¨çš„é€†ï¼›ï¼ˆ2ï¼‰é«˜ç˜¦çš„
    <math alttext="bold upper R"><mi>ğ‘</mi></math> å…·æœ‰ä¼ªé€†ã€‚è®¡ç®—è¿™ä¸¤ä¸ªé€†ï¼Œå¹¶ç¡®è®¤ <math alttext="bold
    upper R"><mi>ğ‘</mi></math> çš„ç¬¬ä¸€ *N* è¡Œçš„å®Œå…¨é€†ç­‰äºé«˜ç˜¦ <math alttext="bold upper R"><mi>ğ‘</mi></math>
    çš„ä¼ªé€†çš„ç¬¬ä¸€ *N* åˆ—ã€‚
- en: ^([1](ch09.xhtml#idm45733297769472-marker)) This is further explored in [Exercise
    9-1](#exercise_9_1).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch09.xhtml#idm45733297769472-marker)) è¿™åœ¨ [Exercise 9-1](#exercise_9_1)
    ä¸­è¿›ä¸€æ­¥æ¢è®¨ã€‚
- en: ^([2](ch09.xhtml#idm45733297664848-marker)) The first column vector is not orthogonalized
    because there are no preceeding vectors; therefore, you begin with the following
    normalization step.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch09.xhtml#idm45733297664848-marker)) ç¬¬ä¸€åˆ—å‘é‡ä¸æ˜¯æ­£äº¤çš„ï¼Œå› ä¸ºæ²¡æœ‰å‰é¢çš„å‘é‡ï¼›å› æ­¤ï¼Œä½ ä»ä»¥ä¸‹çš„æ ‡å‡†åŒ–æ­¥éª¤å¼€å§‹ã€‚
- en: ^([3](ch09.xhtml#idm45733297640832-marker)) Recovering **R** through matrix
    multiplication is possible because GS is a series of linear transformations.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch09.xhtml#idm45733297640832-marker)) é€šè¿‡çŸ©é˜µä¹˜æ³•æ¢å¤**R**æ˜¯å¯èƒ½çš„ï¼Œå› ä¸ºGSæ˜¯ä¸€ç³»åˆ—çº¿æ€§å˜æ¢ã€‚
- en: ^([4](ch09.xhtml#idm45733297281040-marker)) Take your time with this exercise;
    itâ€™s quite challenging.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch09.xhtml#idm45733297281040-marker)) åœ¨è¿™ä¸ªç»ƒä¹ ä¸­èŠ±äº›æ—¶é—´ï¼Œå®ƒç›¸å½“å…·æœ‰æŒ‘æˆ˜æ€§ã€‚
- en: '^([5](ch09.xhtml#idm45733297196544-marker)) Hint: write down the dot-product
    formula for the vector norm.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch09.xhtml#idm45733297196544-marker)) æç¤ºï¼šå†™å‡ºå‘é‡èŒƒæ•°çš„ç‚¹ç§¯å…¬å¼ã€‚
