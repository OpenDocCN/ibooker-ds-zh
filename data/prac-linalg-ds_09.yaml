- en: Chapter 9\. Orthogonal Matrices and QR Decomposition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 正交矩阵和QR分解
- en: 'You will learn five major decompositions in this book: orthogonal vector decomposition,
    QR decomposition, LU decomposition, eigendecomposition, and singular value decomposition.
    Those are not the only decompositions in linear algebra, but they are the most
    important ones for data science and machine learning.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在本书中学习五种主要的分解：正交向量分解、QR分解、LU分解、特征分解和奇异值分解。这些不是线性代数中唯一的分解，但它们是数据科学和机器学习中最重要的分解方法之一。
- en: In this chapter, you will learn QR. And along the way, you’ll learn a new special
    type of matrix (orthogonal). QR decomposition is a workhorse that powers applications
    including the matrix inverse, least squares model fitting, and eigendecomposition.
    Therefore, understanding and gaining familiarity with QR decomposition will help
    you level up your linear algebra skills.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习QR。并且在此过程中，您将学习一种新的特殊类型的矩阵（正交矩阵）。QR分解是许多应用的基础，包括矩阵求逆、最小二乘模型拟合和特征分解。因此，理解和熟悉QR分解将帮助您提升线性代数技能。
- en: Orthogonal Matrices
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正交矩阵
- en: 'I will begin by introducing you to orthogonal matrices. An *orthogonal matrix*
    is a special matrix that is important for several decompositions, including QR,
    eigendecomposition, and singular value decomposition. The letter <math alttext="bold
    upper Q"><mi>𝐐</mi></math> is often used to indicate orthogonal matrices. Orthogonal
    matrices have two properties:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我将首先介绍正交矩阵给您。*正交矩阵*是一种特殊的矩阵，对于多种分解（包括QR分解、特征分解和奇异值分解）至关重要。字母<math alttext="bold
    upper Q"><mi>𝐐</mi></math>通常用来表示正交矩阵。正交矩阵具有两个特性：
- en: Orthogonal columns
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 正交列
- en: All columns are pair-wise orthogonal.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 所有列都是两两正交的。
- en: Unit-norm columns
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 单位范数列
- en: The norm (geometric length) of each column is exactly 1.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 每列的范数（几何长度）恰好为1。
- en: 'We can translate those two properties into a mathematical expression (remember
    that <math alttext="mathematical left-angle bold a comma bold b mathematical right-angle"><mrow><mo>〈</mo>
    <mi>𝐚</mi> <mo>,</mo> <mi>𝐛</mi> <mo>〉</mo></mrow></math> is an alternative notation
    for the dot product):'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这两个特性翻译成数学表达式（请记住<math alttext="mathematical left-angle bold a comma bold
    b mathematical right-angle"><mrow><mo>〈</mo> <mi>𝐚</mi> <mo>,</mo> <mi>𝐛</mi>
    <mo>〉</mo></mrow></math>是点积的另一种表示方法）：
- en: <math alttext="mathematical left-angle bold q Subscript i Baseline comma bold
    q Subscript j Baseline mathematical right-angle equals StartLayout Enlarged left-brace
    1st Row  0 comma if i not-equals j 2nd Row  1 comma if i equals j EndLayout" display="block"><mrow><mrow><mo>〈</mo>
    <msub><mi>𝐪</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>𝐪</mi> <mi>j</mi></msub>
    <mo>〉</mo></mrow> <mo>=</mo> <mfenced close="" open="{" separators=""><mtable><mtr><mtd
    columnalign="left"><mrow><mn>0</mn> <mo>,</mo> <mtext>if</mtext> <mi>i</mi> <mo>≠</mo>
    <mi>j</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mn>1</mn> <mo>,</mo>
    <mtext>if</mtext> <mi>i</mi> <mo>=</mo> <mi>j</mi></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="mathematical left-angle bold q Subscript i Baseline comma bold
    q Subscript j Baseline mathematical right-angle equals StartLayout Enlarged left-brace
    1st Row  0 comma if i not-equals j 2nd Row  1 comma if i equals j EndLayout" display="block"><mrow><mrow><mo>〈</mo>
    <msub><mi>𝐪</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>𝐪</mi> <mi>j</mi></msub>
    <mo>〉</mo></mrow> <mo>=</mo> <mfenced close="" open="{" separators=""><mtable><mtr><mtd
    columnalign="left"><mrow><mn>0</mn> <mo>,</mo> <mtext>if</mtext> <mi>i</mi> <mo>≠</mo>
    <mi>j</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mn>1</mn> <mo>,</mo>
    <mtext>if</mtext> <mi>i</mi> <mo>=</mo> <mi>j</mi></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: What does that mean? It means that the dot product of a column with itself is
    1 while the dot product of a column with any other column is 0\. That’s a lot
    of dot products with only two possible outcomes. We can organize all of the dot
    products amongst all pairs of columns by premultiplying the matrix by its transpose.
    Remember that matrix multiplication is defined as dot products between all rows
    of the left matrix with all columns of the right matrix; therefore, the rows of
    <math alttext="bold upper Q Superscript upper T"><msup><mi>𝐐</mi> <mtext>T</mtext></msup></math>
    are the columns of <math alttext="bold upper Q"><mi>𝐐</mi></math> .
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这是什么意思？这意味着列与自身的点积为1，而列与任何其他列的点积为0。这是许多点积，只有两种可能的结果。我们可以通过将矩阵与其转置的乘积前置来组织所有列对之间的点积。请记住，矩阵乘法定义为左矩阵的所有行与右矩阵的所有列的点积；因此，<math
    alttext="bold upper Q Superscript upper T"><msup><mi>𝐐</mi> <mtext>T</mtext></msup></math>的行是<math
    alttext="bold upper Q"><mi>𝐐</mi></math>的列。
- en: 'The matrix equation expressing the two key properties of an orthogonal matrix
    is simply marvelous:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 表达两个正交矩阵关键属性的矩阵方程式简直是奇妙的：
- en: <math alttext="bold upper Q Superscript upper T Baseline bold upper Q equals
    bold upper I" display="block"><mrow><msup><mi>𝐐</mi> <mtext>T</mtext></msup> <mi>𝐐</mi>
    <mo>=</mo> <mi>𝐈</mi></mrow></math>
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper Q Superscript upper T Baseline bold upper Q equals
    bold upper I" display="block"><mrow><msup><mi>𝐐</mi> <mtext>T</mtext></msup> <mi>𝐐</mi>
    <mo>=</mo> <mi>𝐈</mi></mrow></math>
- en: The expression <math alttext="bold upper Q Superscript upper T Baseline bold
    upper Q equals bold upper I"><mrow><msup><mi>𝐐</mi> <mtext>T</mtext></msup> <mi>𝐐</mi>
    <mo>=</mo> <mi>𝐈</mi></mrow></math> is amazing. Really, it’s a big deal.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 表达式<math alttext="bold upper Q Superscript upper T Baseline bold upper Q equals
    bold upper I"><mrow><msup><mi>𝐐</mi> <mtext>T</mtext></msup> <mi>𝐐</mi> <mo>=</mo>
    <mi>𝐈</mi></mrow></math>令人惊叹。真的，这是一件大事。
- en: Why is it a big deal? Because <math alttext="bold upper Q Superscript upper
    T"><msup><mi>𝐐</mi> <mtext>T</mtext></msup></math> is a matrix that multiplies
    <math alttext="bold upper Q"><mi>𝐐</mi></math> to produce the identity matrix.
    That’s the exact same definition as the matrix inverse. Thus, the inverse of an
    orthogonal matrix is its transpose. That’s crazy cool, because the matrix inverse
    is tedious and prone to numerical inaccuracies, whereas the matrix transpose is
    fast and accurate.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这很重要？因为<math alttext="bold upper Q Superscript upper T"><msup><mi>𝐐</mi>
    <mtext>T</mtext></msup></math>是一个将<math alttext="bold upper Q"><mi>𝐐</mi></math>乘以产生单位矩阵的矩阵。这与矩阵的逆矩阵的确切定义相同。因此，正交矩阵的逆矩阵是其转置矩阵。这非常酷，因为矩阵的逆矩阵复杂且容易出现数值不准确，而矩阵的转置矩阵则快速且准确。
- en: 'Do such matrices really exist in the wild, or are they mere figments of a data
    scientist’s imagination? Yes, they really do exist. In fact, the identity matrix
    is an example of an orthogonal matrix. Here are another two:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的矩阵真的存在吗，还是只是数据科学家想象中的产物？是的，它们确实存在。事实上，单位矩阵就是正交矩阵的一个例子。以下是另外两个例子：
- en: <math alttext="StartFraction 1 Over StartRoot 2 EndRoot EndFraction Start 2
    By 2 Matrix 1st Row 1st Column 1 2nd Column negative 1 2nd Row 1st Column 1 2nd
    Column 1 EndMatrix comma one-third Start 3 By 3 Matrix 1st Row 1st Column 1 2nd
    Column 2 3rd Column 2 2nd Row 1st Column 2 2nd Column 1 3rd Column negative 2
    3rd Row 1st Column negative 2 2nd Column 2 3rd Column negative 1 EndMatrix" display="block"><mrow><mfrac><mn>1</mn>
    <msqrt><mn>2</mn></msqrt></mfrac> <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mrow><mo>-</mo> <mn>1</mn></mrow></mtd></mtr> <mtr><mtd><mn>1</mn></mtd>
    <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>1</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>,</mo> <mfrac><mn>1</mn> <mn>3</mn></mfrac> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>1</mn></mrow></mtd> <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>2</mn></mrow></mtd>
    <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>2</mn></mrow></mtd> <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>1</mn></mrow></mtd>
    <mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mo>-</mo>
    <mn>2</mn></mrow></mtd> <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>2</mn></mrow></mtd>
    <mtd><mrow><mo>-</mo> <mn>1</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction 1 Over StartRoot 2 EndRoot EndFraction Start 2
    By 2 Matrix 1st Row 1st Column 1 2nd Column negative 1 2nd Row 1st Column 1 2nd
    Column 1 EndMatrix comma one-third Start 3 By 3 Matrix 1st Row 1st Column 1 2nd
    Column 2 3rd Column 2 2nd Row 1st Column 2 2nd Column 1 3rd Column negative 2
    3rd Row 1st Column negative 2 2nd Column 2 3rd Column negative 1 EndMatrix" display="block"><mrow><mfrac><mn>1</mn>
    <msqrt><mn>2</mn></msqrt></mfrac> <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mrow><mo>-</mo> <mn>1</mn></mrow></mtd></mtr> <mtr><mtd><mn>1</mn></mtd>
    <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>1</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>,</mo> <mfrac><mn>1</mn> <mn>3</mn></mfrac> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>1</mn></mrow></mtd> <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>2</mn></mrow></mtd>
    <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>2</mn></mrow></mtd> <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>1</mn></mrow></mtd>
    <mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mo>-</mo>
    <mn>2</mn></mrow></mtd> <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>2</mn></mrow></mtd>
    <mtd><mrow><mo>-</mo> <mn>1</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'Please take a moment to confirm that each column has unit length and is orthogonal
    to other columns. Then we can confirm in Python:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 请花一点时间确认每一列的长度为单位长度，并且与其他列正交。然后我们可以在Python中确认：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Both outputs are the identity matrix (within rounding errors on the order of
    10^(−15)). What happens if you compute <math alttext="bold upper Q bold upper
    Q Superscript upper T"><mrow><mi>𝐐</mi> <msup><mi>𝐐</mi> <mtext>T</mtext></msup></mrow></math>
    ? Is that still the identity matrix? Try it to find out!^([1](ch09.xhtml#idm45733297769472))
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 两个输出都是单位矩阵（在约为10^(-15)的舍入误差内）。如果计算<math alttext="bold upper Q bold upper Q Superscript
    upper T"><mrow><mi>𝐐</mi> <msup><mi>𝐐</mi> <mtext>T</mtext></msup></mrow></math>会发生什么？那仍然是单位矩阵吗？试试看！^([1](ch09.xhtml#idm45733297769472))
- en: Another example of an orthogonal matrix is the pure rotation matrices that you
    learned about in [Chapter 7](ch07.xhtml#Chapter_7). You can go back to that code
    and confirm that the transformation matrix times its transpose is the identity
    matrix, regardless of the rotation angle (as long as the same rotation angle is
    used in all matrix elements). Permutation matrices are also orthogonal. Permutation
    matrices are used to exchange rows of a matrix; you’ll learn about them in the
    discussion of LU decomposition in the next chapter.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个正交矩阵的例子是你在[第7章](ch07.xhtml#Chapter_7)中学到的纯旋转矩阵。你可以回顾那段代码，并确认变换矩阵乘以其转置矩阵为单位矩阵，无论旋转角度如何（只要所有矩阵元素使用相同的旋转角度）。排列矩阵也是正交的。排列矩阵用于交换矩阵的行；在下一章中，你将学习关于LU分解的讨论。
- en: How do you create such majestic marvels of mathematics? An orthogonal matrix
    can be computed from a nonorthogonal matrix via QR decomposition, which is basically
    a sophisticated version of Gram-Schmidt. And how does Gram-Schmidt work? That’s
    basically the orthogonal vector decomposition that you learned about in [Chapter 2](ch02.xhtml#Chapter_2).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如何创建这样宏伟的数学奇迹？正交矩阵可以通过QR分解从非正交矩阵中计算得到，这基本上是Gram-Schmidt的复杂版本。Gram-Schmidt是如何工作的？这基本上是你在[第2章](ch02.xhtml#Chapter_2)中学到的正交向量分解。
- en: Gram-Schmidt
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gram-Schmidt
- en: The Gram-Schmidt procedure is a way to transform a nonorthogonal matrix into
    an orthogonal matrix. Gram-Schmidt has high educational value, but unfortunately
    very little application value. The reason is that—as you’ve now read several times
    before—there are numerical instabilities resulting from many divisions and multiplications
    by tiny numbers. Fortunately, there are more sophisticated and numerically stable
    methods for QR decomposition, such as Householder reflections. The details of
    that algorithm are outside the scope of this book, but they are handled by low-level
    numerical computation libraries that Python calls.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Gram-Schmidt过程是将非正交矩阵转换为正交矩阵的方法。Gram-Schmidt具有很高的教育价值，但非常遗憾的是应用价值很小。原因在于，正如你之前已经读过的那样，由于许多除法和乘法操作中涉及到的小数，导致数值不稳定。幸运的是，还有更复杂和数值稳定的QR分解方法，例如Householder反射。这些算法的细节超出了本书的范围，但由Python调用的低级数值计算库处理。
- en: Nevertheless, I’m going to describe the Gram-Schmidt procedure (sometimes abbreviated
    to GS or G-S) because it shows an application of orthogonal vector decomposition,
    because you are going to program the algorithm in Python based on the following
    math and description, and because GS is the right way to conceptualize how and
    why QR decomposition works even if the low-level implementation is slightly different.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我将描述 Gram-Schmidt 过程（有时缩写为 GS 或 G-S），因为它展示了正交向量分解的应用，因为你将基于以下数学和描述在 Python
    中编程该算法，并且因为 GS 是理解 QR 分解如何工作的正确方式，即使低级实现略有不同。
- en: A matrix <math alttext="bold upper V"><mi>𝐕</mi></math> comprising columns <math
    alttext="bold v 1"><msub><mi>𝐯</mi> <mn>1</mn></msub></math> through <math alttext="bold
    v Subscript n"><msub><mi>𝐯</mi> <mi>n</mi></msub></math> is transformed into an
    orthogonal matrix <math alttext="bold upper Q"><mi>𝐐</mi></math> with columns
    <math alttext="bold q Subscript k"><msub><mi>𝐪</mi> <mi>k</mi></msub></math> according
    to the following algorithm.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由列 <math alttext="bold v 1"><msub><mi>𝐯</mi> <mn>1</mn></msub></math> 到 <math
    alttext="bold v Subscript n"><msub><mi>𝐯</mi> <mi>n</mi></msub></math> 组成的矩阵 <math
    alttext="bold upper V"><mi>𝐕</mi></math> 被转换为一个具有列 <math alttext="bold q Subscript
    k"><msub><mi>𝐪</mi> <mi>k</mi></msub></math> 的正交矩阵 <math alttext="bold upper Q"><mi>𝐐</mi></math>
    ，根据以下算法。
- en: 'For all column vectors in <math alttext="bold upper V"><mi>𝐕</mi></math> starting
    from the first (leftmost) and moving systematically to the last (rightmost):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于矩阵 <math alttext="bold upper V"><mi>𝐕</mi></math> 中从第一列（最左边）开始系统地移动到最后一列（最右边）的所有列向量：
- en: Orthogonalize <math alttext="bold v Subscript k"><msub><mi>𝐯</mi> <mi>k</mi></msub></math>
    to all previous columns in matrix <math alttext="bold upper Q"><mi>𝐐</mi></math>
    using orthogonal vector decomposition. That is, compute the component of <math
    alttext="bold v Subscript k"><msub><mi>𝐯</mi> <mi>k</mi></msub></math> that is
    perpendicular to <math alttext="bold q Subscript k minus 1"><msub><mi>𝐪</mi> <mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    , <math alttext="bold q Subscript k minus 2"><msub><mi>𝐪</mi> <mrow><mi>k</mi><mo>-</mo><mn>2</mn></mrow></msub></math>
    , and so on down to <math alttext="bold q 1"><msub><mi>𝐪</mi> <mn>1</mn></msub></math>
    . The orthogonalized vector is called <math alttext="bold v Subscript k Superscript
    asterisk"><msubsup><mi>𝐯</mi> <mi>k</mi> <mo>*</mo></msubsup></math> .^([2](ch09.xhtml#idm45733297664848))
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用正交向量分解将 <math alttext="bold v Subscript k"><msub><mi>𝐯</mi> <mi>k</mi></msub></math>
    正交化到矩阵 <math alttext="bold upper Q"><mi>𝐐</mi></math> 中所有之前的列。也就是说，计算 <math alttext="bold
    v Subscript k"><msub><mi>𝐯</mi> <mi>k</mi></msub></math> 在 <math alttext="bold
    q Subscript k minus 1"><msub><mi>𝐪</mi> <mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    ， <math alttext="bold q Subscript k minus 2"><msub><mi>𝐪</mi> <mrow><mi>k</mi><mo>-</mo><mn>2</mn></mrow></msub></math>
    直至 <math alttext="bold q 1"><msub><mi>𝐪</mi> <mn>1</mn></msub></math> 的垂直分量。正交化后的向量称为
    <math alttext="bold v Subscript k Superscript asterisk"><msubsup><mi>𝐯</mi> <mi>k</mi>
    <mo>*</mo></msubsup></math> 。^([2](ch09.xhtml#idm45733297664848))
- en: Normalize <math alttext="bold v Subscript k Superscript asterisk"><msubsup><mi>𝐯</mi>
    <mi>k</mi> <mo>*</mo></msubsup></math> to unit length. This is now <math alttext="bold
    q Subscript k"><msub><mi>𝐪</mi> <mi>k</mi></msub></math> , the *k*th column in
    matrix <math alttext="bold upper Q"><mi>𝐐</mi></math> .
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 <math alttext="bold v Subscript k Superscript asterisk"><msubsup><mi>𝐯</mi>
    <mi>k</mi> <mo>*</mo></msubsup></math> 归一化为单位长度。现在这是 <math alttext="bold q Subscript
    k"><msub><mi>𝐪</mi> <mi>k</mi></msub></math> ，矩阵 <math alttext="bold upper Q"><mi>𝐐</mi></math>
    中的第 *k* 列。
- en: Sounds simple, doesn’t it? Implementing this algorithm in code can be tricky
    because of the repeated orthogonalizations. But with some perseverence, you can
    figure it out ([Exercise 9-2](#exercise_9_2)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来很简单，不是吗？在代码中实现这个算法可能会有些棘手，因为需要重复进行正交化。但只要有些坚持，你就能搞明白（[习题 9-2](#exercise_9_2)）。
- en: QR Decomposition
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: QR 分解
- en: The GS procedure transforms a matrix into an orthogonal matrix <math alttext="bold
    upper Q"><mi>𝐐</mi></math> . (As I wrote in the previous section, <math alttext="bold
    upper Q"><mi>𝐐</mi></math> is actually obtained using a series of vector-plane
    reflections known as the Householder transformation, but that’s due to numerical
    issues; GS is a great way to conceptualize how <math alttext="bold upper Q"><mi>𝐐</mi></math>
    matrices are formed.)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: GS 过程将一个矩阵转换为正交矩阵 <math alttext="bold upper Q"><mi>𝐐</mi></math> 。（正如我在前一节中所述，实际上通过一系列称为
    Householder 变换的向量平面反射来获得 <math alttext="bold upper Q"><mi>𝐐</mi></math> ，但这是由于数值问题；GS
    是理解 QR 分解形成的一个很好的方式。）
- en: What’s in a Sound?
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 声音中有什么？
- en: The “QR” in QR decomposition is pronounced “queue are.” In my opinion, that’s
    a real missed opportunity; linear algebra would be more fun to learn if we pronounced
    it “QweRty decomposition.” Or maybe we could have pronounced it “core decomposition”
    to appeal to the fitness crowd. But, for better and for worse, modern conventions
    are shaped by historical precedent.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: “QR”分解中的“QR”发音为“queue are”。在我看来，这真是一个错失的机会；如果我们把它发音为“QweRty decomposition”，学习线性代数可能会更有趣。或者，我们可以将其发音为“core
    decomposition”，以吸引健身人群。但是，无论好坏，现代惯例都受历史先例的影响。
- en: '<math alttext="bold upper Q"><mi>𝐐</mi></math> is obviously different from
    the original matrix (assuming the original matrix was not orthogonal). So we have
    lost information about that matrix. Fortunately, that “lost” information can be
    easily retrieved and stored in another matrix <math alttext="bold upper R"><mi>𝐑</mi></math>
    that multiplies <math alttext="bold upper Q"><mi>𝐐</mi></math> .^([3](ch09.xhtml#idm45733297640832))
    That leads to the question of how we create <math alttext="bold upper R"><mi>𝐑</mi></math>
    . In fact, creating <math alttext="bold upper R"><mi>𝐑</mi></math> is straightforward
    and comes right from the definition of QR:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，<math alttext="bold upper Q"><mi>𝐐</mi></math>与原始矩阵不同（假设原始矩阵不是正交的）。因此，我们失去了关于该矩阵的信息。幸运的是，这些“丢失”的信息可以轻松地恢复并存储在另一个矩阵<math
    alttext="bold upper R"><mi>𝐑</mi></math>中，该矩阵乘以<math alttext="bold upper Q"><mi>𝐐</mi></math>。^([3](ch09.xhtml#idm45733297640832))
    这就引出了我们如何创建<math alttext="bold upper R"><mi>𝐑</mi></math>的问题。事实上，创建<math alttext="bold
    upper R"><mi>𝐑</mi></math>是直截了当的，并直接来自QR的定义：
- en: <math alttext="StartLayout 1st Row 1st Column bold upper A 2nd Column equals
    bold upper Q bold upper R 2nd Row 1st Column bold upper Q Superscript upper T
    Baseline bold upper A 2nd Column equals bold upper Q Superscript upper T Baseline
    bold upper Q bold upper R 3rd Row 1st Column bold upper Q Superscript upper T
    Baseline bold upper A 2nd Column equals bold upper R EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mi>𝐀</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>𝐐</mi> <mi>𝐑</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msup><mi>𝐐</mi>
    <mtext>T</mtext></msup> <mi>𝐀</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mi>𝐐</mi> <mtext>T</mtext></msup> <mi>𝐐</mi> <mi>𝐑</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msup><mi>𝐐</mi> <mtext>T</mtext></msup> <mi>𝐀</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>𝐑</mi></mrow></mtd></mtr></mtable></math>
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper A 2nd Column equals
    bold upper Q bold upper R 2nd Row 1st Column bold upper Q Superscript upper T
    Baseline bold upper A 2nd Column equals bold upper Q Superscript upper T Baseline
    bold upper Q bold upper R 3rd Row 1st Column bold upper Q Superscript upper T
    Baseline bold upper A 2nd Column equals bold upper R EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mi>𝐀</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>𝐐</mi> <mi>𝐑</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msup><mi>𝐐</mi>
    <mtext>T</mtext></msup> <mi>𝐀</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mi>𝐐</mi> <mtext>T</mtext></msup> <mi>𝐐</mi> <mi>𝐑</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msup><mi>𝐐</mi> <mtext>T</mtext></msup> <mi>𝐀</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>𝐑</mi></mrow></mtd></mtr></mtable></math>
- en: 'Here you see the beauty of orthogonal matrices: we can solve matrix equations
    without having to worry about computing the inverse.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以看到正交矩阵的美妙之处：我们可以解决矩阵方程，而不必担心计算逆矩阵。
- en: 'The following Python code shows how to compute QR decomposition of a square
    matrix, and [Figure 9-1](#fig_9_1) illustrates the three matrices:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python代码展示了如何计算方阵的QR分解，[图 9-1](#fig_9_1)说明了这三个矩阵：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![QweRty](assets/plad_0901.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![QweRty](assets/plad_0901.png)'
- en: Figure 9-1\. QR decomposition of a random-numbers matrix
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1。随机数矩阵的QR分解
- en: Several important features of QR decomposition are visible in [Figure 9-1](#fig_9_1),
    including that <math alttext="bold upper A equals bold upper Q bold upper R"><mrow><mi>𝐀</mi>
    <mo>=</mo> <mi>𝐐</mi> <mi>𝐑</mi></mrow></math> (their difference is the zeros
    matrix) and that <math alttext="bold upper Q"><mi>𝐐</mi></math> times its transpose
    gives the identity matrix.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: QR分解的几个重要特征在[图 9-1](#fig_9_1)中可见，包括<math alttext="bold upper A equals bold upper
    Q bold upper R"><mrow><mi>𝐀</mi> <mo>=</mo> <mi>𝐐</mi> <mi>𝐑</mi></mrow></math>（它们的差异是零矩阵）以及<math
    alttext="bold upper Q"><mi>𝐐</mi></math>乘以其转置矩阵得到单位矩阵。
- en: 'Check out the <math alttext="bold upper R"><mi>𝐑</mi></math> matrix: it’s upper-triangular
    (all elements below the diagonal are zero). That seems unlikely to have occurred
    by chance, considering we started from a random matrix. In fact, the <math alttext="bold
    upper R"><mi>𝐑</mi></math> matrix is *always* upper-triangular. To understand
    why, you need to think about the GS algorithm and the organization of the dot
    products in matrix multiplication. I will explain why <math alttext="bold upper
    R"><mi>𝐑</mi></math> is upper-triangular in the next section; before then, I want
    you to come up with an answer.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 检查<math alttext="bold upper R"><mi>𝐑</mi></math>矩阵：它是上三角矩阵（对角线以下的所有元素都为零）。考虑到我们从一个随机矩阵开始，这似乎不可能是偶然发生的。事实上，<math
    alttext="bold upper R"><mi>𝐑</mi></math>矩阵总是上三角的。要理解其中的原因，您需要思考GS算法和矩阵乘法中点积的组织方式。在下一节中，我将解释为什么<math
    alttext="bold upper R"><mi>𝐑</mi></math>是上三角的；在那之前，我希望您能想出一个答案。
- en: Sizes of Q and R
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q和R的尺寸
- en: The sizes of <math alttext="bold upper Q"><mi>𝐐</mi></math> and <math alttext="bold
    upper R"><mi>𝐑</mi></math> depend on the size of the to-be-decomposed matrix <math
    alttext="bold upper A"><mi>𝐀</mi></math> and on whether the QR decomposition is
    “economy” (also called “reduced”) or “full” (also called “complete”). [Figure 9-2](#fig_9_2)
    shows an overview of all possible sizes.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper Q"><mi>𝐐</mi></math>和<math alttext="bold upper R"><mi>𝐑</mi></math>的尺寸取决于待分解矩阵<math
    alttext="bold upper A"><mi>𝐀</mi></math>的大小，以及QR分解是“经济”（也称为“减少”）还是“完全”（也称为“完整”）。[图 9-2](#fig_9_2)显示了所有可能尺寸的概述。
- en: 'Economy versus full QR decomposition applies only to tall matrices. The question
    is this: for a tall matrix (*M* > *N*), do we create a <math alttext="bold upper
    Q"><mi>𝐐</mi></math> matrix with *N* columns or *M* columns? The former option
    is called *economy* or *reduced*, and gives a tall <math alttext="bold upper Q"><mi>𝐐</mi></math>
    ; and the latter option is called *full* or *complete*, and gives a square <math
    alttext="bold upper Q"><mi>𝐐</mi></math> .'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 经济与完整的QR分解仅适用于高矩阵。问题在于对于一个高矩阵（*M* > *N*），我们是创建一个具有*N*列还是*M*列的<math alttext="bold
    upper Q"><mi>𝐐</mi></math>矩阵？前者选项称为*经济*或*减少*，给出一个高<math alttext="bold upper Q"><mi>𝐐</mi></math>；后者选项称为*完整*或*完全*，给出一个方形<math
    alttext="bold upper Q"><mi>𝐐</mi></math>。
- en: '![QR sizes](assets/plad_0902.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![QR sizes](assets/plad_0902.png)'
- en: Figure 9-2\. Sizes of **Q** and **R** depending on the size of **A**. The “?”
    indicates that the matrix elements depend on the values in **A**, i.e., it is
    not the identity matrix.
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2。**Q**和**R**的大小取决于**A**的大小。“?”表示矩阵元素取决于**A**中的值，即它不是单位矩阵。
- en: 'It may seem surprising that <math alttext="bold upper Q"><mi>𝐐</mi></math>
    can be square when <math alttext="bold upper A"><mi>𝐀</mi></math> is tall (in
    other words, that <math alttext="bold upper Q"><mi>𝐐</mi></math> can have more
    columns than <math alttext="bold upper A"><mi>𝐀</mi></math> ): where do the extra
    columns come from? It is, in fact, possible to craft orthogonal vectors “out of
    thin air.” Consider the following example in Python:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当<math alttext="bold upper A"><mi>𝐀</mi></math>高时（换句话说，<math alttext="bold upper
    Q"><mi>𝐐</mi></math>可以比<math alttext="bold upper A"><mi>𝐀</mi></math>列更多），<math
    alttext="bold upper Q"><mi>𝐐</mi></math>可以是方形的事实，额外的列来自哪里？事实上，可以“凭空”制作正交向量。考虑以下Python示例：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Notice the optional second input `'complete'`, which produces a full QR decomposition.
    Setting that to `'reduced'`, which is the default, gives the economy-mode QR decomposition,
    in which <math alttext="bold upper Q"><mi>𝐐</mi></math> is the same size as <math
    alttext="bold upper A"><mi>𝐀</mi></math> .
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意可选的第二个输入`'complete'`，它会产生完整的QR分解。将其设置为`'reduced'`（默认情况），则会得到经济模式的QR分解，其中<math
    alttext="bold upper Q"><mi>𝐐</mi></math>的大小与<math alttext="bold upper A"><mi>𝐀</mi></math>相同。
- en: Because it is possible to craft more than *M* > *N* orthogonal vectors from
    a matrix with *N* columns, the rank of <math alttext="bold upper Q"><mi>𝐐</mi></math>
    is always the maximum possible rank, which is *M* for all square <math alttext="bold
    upper Q"><mi>𝐐</mi></math> matrices and *N* for the economy <math alttext="bold
    upper Q"><mi>𝐐</mi></math> . The rank of <math alttext="bold upper R"><mi>𝐑</mi></math>
    is the same as the rank of <math alttext="bold upper A"><mi>𝐀</mi></math> .
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因为可以从具有*N*列的矩阵中制作多于*M* > *N*个正交向量，所以<math alttext="bold upper Q"><mi>𝐐</mi></math>的秩总是可能的最大秩，对于所有方形<math
    alttext="bold upper Q"><mi>𝐐</mi></math>矩阵为*M*，对于经济<math alttext="bold upper Q"><mi>𝐐</mi></math>为*N*。<math
    alttext="bold upper R"><mi>𝐑</mi></math>的秩与<math alttext="bold upper A"><mi>𝐀</mi></math>的秩相同。
- en: The difference in rank between <math alttext="bold upper Q"><mi>𝐐</mi></math>
    and <math alttext="bold upper A"><mi>𝐀</mi></math> resulting from orthogonalization
    means that <math alttext="bold upper Q"><mi>𝐐</mi></math> spans all of <math alttext="double-struck
    upper R Superscript upper M"><msup><mi>ℝ</mi> <mi>M</mi></msup></math> even if
    the column space of <math alttext="bold upper A"><mi>𝐀</mi></math> is only a lower-dimensional
    subspace of <math alttext="double-struck upper R Superscript upper M"><msup><mi>ℝ</mi>
    <mi>M</mi></msup></math> . That fact is central to why the singular value decomposition
    is so useful for revealing properties of a matrix, including its rank and null
    space. Yet another reason to look forward to learning about the SVD in [Chapter 14](ch14.xhtml#Chapter_14)!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由正交化引起的<math alttext="bold upper Q"><mi>𝐐</mi></math>和<math alttext="bold upper
    A"><mi>𝐀</mi></math>秩的差异意味着<math alttext="bold upper Q"><mi>𝐐</mi></math>跨越了所有<math
    alttext="double-struck upper R Superscript upper M"><msup><mi>ℝ</mi> <mi>M</mi></msup></math>，即使<math
    alttext="bold upper A"><mi>𝐀</mi></math>的列空间仅是<math alttext="double-struck upper
    R Superscript upper M"><msup><mi>ℝ</mi> <mi>M</mi></msup></math>的低维子空间。这个事实是为什么奇异值分解对于揭示矩阵的性质，包括其秩和零空间，非常有用的中心原因。期待在[第14章](ch14.xhtml#Chapter_14)学习SVD的另一个原因！
- en: 'A note about uniqueness: QR decomposition is not unique for all matrix sizes
    and ranks. This means that it is possible to obtain <math alttext="bold upper
    A equals bold upper Q 1 bold upper R 1"><mrow><mi>𝐀</mi> <mo>=</mo> <msub><mi>𝐐</mi>
    <mn>1</mn></msub> <msub><mi>𝐑</mi> <mn>1</mn></msub></mrow></math> and <math alttext="bold
    upper A equals bold upper Q 2 bold upper R 2"><mrow><mi>𝐀</mi> <mo>=</mo> <msub><mi>𝐐</mi>
    <mn>2</mn></msub> <msub><mi>𝐑</mi> <mn>2</mn></msub></mrow></math> where <math
    alttext="bold upper Q 1 not-equals bold upper Q 2"><mrow><msub><mi>𝐐</mi> <mn>1</mn></msub>
    <mo>≠</mo> <msub><mi>𝐐</mi> <mn>2</mn></msub></mrow></math> . However, all QR
    decomposition results have the same properties described in this section. QR decomposition
    can be made unique given additional constraints (e.g., positive values on the
    diagonals of <math alttext="bold upper R"><mi>𝐑</mi></math> ), although this is
    not necessary in most cases, and is not implemented in Python or MATLAB. You’ll
    see this nonuniqueness when comparing GS to QR in [Exercise 9-2](#exercise_9_2).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 关于唯一性的注意事项：QR分解对于所有矩阵大小和秩都不是唯一的。这意味着可能会得到 <math alttext="bold upper A equals
    bold upper Q 1 bold upper R 1"><mrow><mi>𝐀</mi> <mo>=</mo> <msub><mi>𝐐</mi> <mn>1</mn></msub>
    <msub><mi>𝐑</mi> <mn>1</mn></msub></mrow></math> 和 <math alttext="bold upper A
    equals bold upper Q 2 bold upper R 2"><mrow><mi>𝐀</mi> <mo>=</mo> <msub><mi>𝐐</mi>
    <mn>2</mn></msub> <msub><mi>𝐑</mi> <mn>2</mn></msub></mrow></math>，其中 <math alttext="bold
    upper Q 1 not-equals bold upper Q 2"><mrow><msub><mi>𝐐</mi> <mn>1</mn></msub>
    <mo>≠</mo> <msub><mi>𝐐</mi> <mn>2</mn></msub></mrow></math> 。然而，所有QR分解结果在本节中描述的属性上都是相同的。通过附加约束（例如，<math
    alttext="bold upper R"><mi>𝐑</mi></math> 的对角线上的正值），可以使QR分解唯一化，尽管在大多数情况下这并不是必需的，并且在Python或MATLAB中也没有实现。在[Exercise
    9-2](#exercise_9_2)中比较GS和QR时，你会看到这种非唯一性。
- en: Why <math alttext="bold upper R"><mi>𝐑</mi></math> is upper triangular
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么 <math alttext="bold upper R"><mi>𝐑</mi></math> 是上三角形的
- en: I hope you gave this question some serious thought. It’s a tricky point about
    QR decomposition, so if you couldn’t figure it out on your own, then please read
    the following few paragraphs, and then look away from the book/screen and rederive
    the argument.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 希望您认真考虑了这个问题。这是关于QR分解的一个棘手点，如果您自己无法弄清楚，请阅读接下来的几段文字，然后远离书本/屏幕，重新推导出这个论点。
- en: 'I will start by reminding you of three facts:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我将首先提醒您三个事实：
- en: <math alttext="bold upper R"><mi>𝐑</mi></math> comes from the formula <math
    alttext="bold upper Q Superscript upper T Baseline bold upper A equals bold upper
    R"><mrow><msup><mi>𝐐</mi> <mtext>T</mtext></msup> <mi>𝐀</mi> <mo>=</mo> <mi>𝐑</mi></mrow></math>
    .
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="bold upper R"><mi>𝐑</mi></math> 来自公式 <math alttext="bold upper
    Q Superscript upper T Baseline bold upper A equals bold upper R"><mrow><msup><mi>𝐐</mi>
    <mtext>T</mtext></msup> <mi>𝐀</mi> <mo>=</mo> <mi>𝐑</mi></mrow></math> 。
- en: The lower triangle of a product matrix comprises dot products between *later*
    rows of the left matrix and *earlier* columns of the right matrix.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个乘积矩阵的下三角由左矩阵的*后续*行和右矩阵的*先前*列的点积组成。
- en: The rows of <math alttext="bold upper Q Superscript upper T"><msup><mi>𝐐</mi>
    <mtext>T</mtext></msup></math> are the columns of <math alttext="bold upper Q"><mi>𝐐</mi></math>
    .
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="bold upper Q Superscript upper T"><msup><mi>𝐐</mi> <mtext>T</mtext></msup></math>
    的行是 <math alttext="bold upper Q"><mi>𝐐</mi></math> 的列。
- en: 'Putting those together: because orthogonalization works column-wise from left
    to right, *later* columns in <math alttext="bold upper Q"><mi>𝐐</mi></math> are
    orthogonalized to *earlier* columns of <math alttext="bold upper A"><mi>𝐀</mi></math>
    . Therefore, the lower triangle of <math alttext="bold upper R"><mi>𝐑</mi></math>
    comes from pairs of vectors that have been orthogonalized. In contrast, *earlier*
    columns in <math alttext="bold upper Q"><mi>𝐐</mi></math> are not orthogonalized
    to *later* columns of <math alttext="bold upper A"><mi>𝐀</mi></math> , so we would
    not expect their dot products to be zero.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 将它们放在一起：因为正交化是从左到右逐列进行的，<math alttext="bold upper Q"><mi>𝐐</mi></math> 中的*后续*列被正交化为<math
    alttext="bold upper A"><mi>𝐀</mi></math> 中的*先前*列。因此，<math alttext="bold upper
    R"><mi>𝐑</mi></math> 的下三角来自已经正交化的向量对。相反，<math alttext="bold upper Q"><mi>𝐐</mi></math>
    中的*先前*列未被正交化为<math alttext="bold upper A"><mi>𝐀</mi></math> 中的*后续*列，因此我们不会期望它们的点积为零。
- en: 'Final comment: if columns *i* and *j* of <math alttext="bold upper A"><mi>𝐀</mi></math>
    were already orthogonal, then the corresponding (*i,j*)th element in <math alttext="bold
    upper R"><mi>𝐑</mi></math> would be zero. In fact, if you compute the QR decomposition
    of an orthogonal matrix, then <math alttext="bold upper R"><mi>𝐑</mi></math> will
    be a diagonal matrix in which the diagonal elements are the norms of each column
    in <math alttext="bold upper A"><mi>𝐀</mi></math> . That means that if <math alttext="bold
    upper A equals bold upper Q"><mrow><mi>𝐀</mi> <mo>=</mo> <mi>𝐐</mi></mrow></math>
    , then <math alttext="bold upper R equals bold upper I"><mrow><mi>𝐑</mi> <mo>=</mo>
    <mi>𝐈</mi></mrow></math> , which is obvious from the equation solved for <math
    alttext="bold upper R"><mi>𝐑</mi></math> . You’ll get to explore this in [Exercise
    9-3](#exercise_9_3).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最后评论：如果 <math alttext="bold upper A"><mi>𝐀</mi></math> 的第 *i* 列和第 *j* 列已经正交，则
    <math alttext="bold upper R"><mi>𝐑</mi></math> 的对应 (*i,j*) 元素将为零。事实上，如果你计算一个正交矩阵的
    QR 分解，那么 <math alttext="bold upper R"><mi>𝐑</mi></math> 将是一个对角矩阵，其对角线元素是 <math
    alttext="bold upper A"><mi>𝐀</mi></math> 每列的范数。这意味着如果 <math alttext="bold upper
    A equals bold upper Q"><mrow><mi>𝐀</mi> <mo>=</mo> <mi>𝐐</mi></mrow></math> ，那么
    <math alttext="bold upper R equals bold upper I"><mrow><mi>𝐑</mi> <mo>=</mo> <mi>𝐈</mi></mrow></math>
    ，这从求解 <math alttext="bold upper R"><mi>𝐑</mi></math> 的方程式中显而易见。你将在 [练习 9-3](#exercise_9_3)
    中深入探讨这一点。
- en: QR and Inverses
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: QR 和逆矩阵
- en: QR decomposition provides a more numerically stable way to compute the matrix
    inverse.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: QR 分解提供了一种计算矩阵逆的更为数值稳定的方式。
- en: 'Let’s start by writing out the QR decomposition formula and inverting both
    sides of the equation (note the application of the LIVE EVIL rule):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先写出 QR 分解公式，并倒转方程的两侧（注意应用 LIVE EVIL 规则）：
- en: <math alttext="StartLayout 1st Row 1st Column bold upper A 2nd Column equals
    bold upper Q bold upper R 2nd Row 1st Column bold upper A Superscript negative
    1 2nd Column equals left-parenthesis bold upper Q bold upper R right-parenthesis
    Superscript negative 1 Baseline 3rd Row 1st Column bold upper A Superscript negative
    1 2nd Column equals bold upper R Superscript negative 1 Baseline bold upper Q
    Superscript negative 1 Baseline 4th Row 1st Column bold upper A Superscript negative
    1 2nd Column equals bold upper R Superscript negative 1 Baseline bold upper Q
    Superscript upper T EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mi>𝐀</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>𝐐</mi> <mi>𝐑</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><msup><mi>𝐀</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mrow><mo>(</mo><mi>𝐐</mi><mi>𝐑</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msup><mi>𝐀</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi>𝐑</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi>𝐐</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><msup><mi>𝐀</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi>𝐑</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi>𝐐</mi> <mtext>T</mtext></msup></mrow></mtd></mtr></mtable></math>
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper A 2nd Column equals
    bold upper Q bold upper R 2nd Row 1st Column bold upper A Superscript negative
    1 2nd Column equals left-parenthesis bold upper Q bold upper R right-parenthesis
    Superscript negative 1 Baseline 3rd Row 1st Column bold upper A Superscript negative
    1 2nd Column equals bold upper R Superscript negative 1 Baseline bold upper Q
    Superscript negative 1 Baseline 4th Row 1st Column bold upper A Superscript negative
    1 2nd Column equals bold upper R Superscript negative 1 Baseline bold upper Q
    Superscript upper T EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mi>𝐀</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>𝐐</mi> <mi>𝐑</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><msup><mi>𝐀</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mrow><mo>(</mo><mi>𝐐</mi><mi>𝐑</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msup><mi>𝐀</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi>𝐑</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi>𝐐</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><msup><mi>𝐀</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi>𝐑</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi>𝐐</mi> <mtext>T</mtext></msup></mrow></mtd></mtr></mtable></math>
- en: Thus, we can obtain the inverse of <math alttext="bold upper A"><mi>𝐀</mi></math>
    as the inverse of <math alttext="bold upper R"><mi>𝐑</mi></math> times the transpose
    of <math alttext="bold upper Q"><mi>𝐐</mi></math> . <math alttext="bold upper
    Q"><mi>𝐐</mi></math> is numerically stable due to the Householder reflection algorithm,
    and <math alttext="bold upper R"><mi>𝐑</mi></math> is numerically stable because
    it simply results from matrix multiplication.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将 <math alttext="bold upper A"><mi>𝐀</mi></math> 的逆矩阵表示为 <math alttext="bold
    upper R"><mi>𝐑</mi></math> 的逆乘以 <math alttext="bold upper Q"><mi>𝐐</mi></math>
    的转置。由于 Householder 反射算法，<math alttext="bold upper Q"><mi>𝐐</mi></math> 是数值稳定的，而由矩阵乘法得到的
    <math alttext="bold upper R"><mi>𝐑</mi></math> 也是数值稳定的。
- en: 'Now, we still need to invert <math alttext="bold upper R"><mi>𝐑</mi></math>
    explicitly, but inverting triangular matrices is highly numerically stable when
    done through a procedure called back substitution. You’ll learn more about that
    in the next chapter, but the key point is this: an important application of QR
    decomposition is providing a more numerically stable way to invert matrices, compared
    to the algorithm presented in the previous chapter.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们仍然需要明确地求解 <math alttext="bold upper R"><mi>𝐑</mi></math> 的逆，但通过称为回代的过程来求解上三角矩阵在数值上是高度稳定的。你将在下一章节中了解更多，但关键点在于：QR
    分解的一个重要应用是提供一种比前一章节介绍的算法更为数值稳定的方式来求逆矩阵。
- en: On the other hand, keep in mind that matrices that are theoretically invertible
    but are close to singular are still very difficult to invert; QR decomposition
    may be *more* numerically stable than the traditional algorithm presented in the
    previous chapter, but that doesn’t guarantee a high-quality inverse. A rotten
    apple dipped in honey is still rotten.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，请记住，理论上可逆但接近奇异的矩阵仍然非常难以求逆；QR 分解可能比前一章节介绍的传统算法*更*数值稳定，但这并不保证高质量的逆矩阵。浸在蜂蜜中的烂苹果仍然是烂的。
- en: Summary
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'QR decomposition is great. It’s definitely on my list of the top five awesomest
    matrix decompositions in linear algebra. Here are the key take-home messages of
    this chapter:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: QR 分解非常棒。它绝对是线性代数中前五个最棒的矩阵分解之一。以下是本章的主要要点：
- en: An orthogonal matrix has columns that are pair-wise orthogonal and with norm
    = 1\. Orthogonal matrices are key to several matrix decompositions, including
    QR, eigendecomposition, and singular value decomposition. Orthogonal matrices
    are also important in geometry and computer graphics (e.g., pure rotation matrices).
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正交矩阵具有两两正交且范数为 1 的列。正交矩阵是几种矩阵分解的关键，包括 QR 分解、特征分解和奇异值分解。在几何学和计算机图形学中（如纯旋转矩阵），正交矩阵也非常重要。
- en: You can transform a nonorthogonal matrix into an orthogonal matrix via the Gram-Schmidt
    procedure, which involves applying orthogonal vector decomposition to isolate
    the component of each column that is orthogonal to all previous columns (“previous”
    meaning left to right).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过Gram-Schmidt过程将非正交矩阵转换为正交矩阵，该过程涉及应用正交向量分解来隔离每一列中与所有前一列正交的分量（“前一列”指从左到右）。
- en: QR decomposition is the result of Gram-Schmidt (technically, it is implemented
    by a more stable algorithm, but GS is still the right way to understand it).
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QR分解是Gram-Schmidt的结果（严格来说，它是通过更稳定的算法实现的，但GS仍是理解它的正确方式）。
- en: Code Exercises
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码练习
- en: Exercise 9-1\.
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 9-1\.
- en: 'A square <math alttext="bold upper Q"><mi>𝐐</mi></math> has the following equalities:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一个方阵<math alttext="bold upper Q"><mi>𝐐</mi></math>具有以下等式：
- en: <math alttext="bold upper Q Superscript upper T Baseline bold upper Q equals
    bold upper Q bold upper Q Superscript upper T Baseline equals bold upper Q Superscript
    hyphen 1 Baseline bold upper Q equals bold upper Q bold upper Q Superscript hyphen
    1 Baseline equals bold upper I" display="block"><mrow><msup><mi>𝐐</mi> <mtext>T</mtext></msup>
    <mi>𝐐</mi> <mo>=</mo> <mi>𝐐</mi> <msup><mi>𝐐</mi> <mtext>T</mtext></msup> <mo>=</mo>
    <msup><mi>𝐐</mi> <mtext>-1</mtext></msup> <mi>𝐐</mi> <mo>=</mo> <mi>𝐐</mi> <msup><mi>𝐐</mi>
    <mtext>-1</mtext></msup> <mo>=</mo> <mi>𝐈</mi></mrow></math>
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper Q Superscript upper T Baseline bold upper Q equals
    bold upper Q bold upper Q Superscript upper T Baseline equals bold upper Q Superscript
    hyphen 1 Baseline bold upper Q equals bold upper Q bold upper Q Superscript hyphen
    1 Baseline equals bold upper I" display="block"><mrow><msup><mi>𝐐</mi> <mtext>T</mtext></msup>
    <mi>𝐐</mi> <mo>=</mo> <mi>𝐐</mi> <msup><mi>𝐐</mi> <mtext>T</mtext></msup> <mo>=</mo>
    <msup><mi>𝐐</mi> <mtext>-1</mtext></msup> <mi>𝐐</mi> <mo>=</mo> <mi>𝐐</mi> <msup><mi>𝐐</mi>
    <mtext>-1</mtext></msup> <mo>=</mo> <mi>𝐈</mi></mrow></math>
- en: Demonstrate this in code by computing <math alttext="bold upper Q"><mi>𝐐</mi></math>
    from a random-numbers matrix, then compute <math alttext="bold upper Q Superscript
    upper T"><msup><mi>𝐐</mi> <mtext>T</mtext></msup></math> and <math alttext="bold
    upper Q Superscript negative 1"><msup><mi>𝐐</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    . Then show that all four expressions produce the identity matrix.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从随机数矩阵计算<math alttext="bold upper Q"><mi>𝐐</mi></math>来在代码中演示这一点，然后计算<math
    alttext="bold upper Q Superscript upper T"><msup><mi>𝐐</mi> <mtext>T</mtext></msup></math>和<math
    alttext="bold upper Q Superscript negative 1"><msup><mi>𝐐</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>。然后展示这四个表达式都产生单位矩阵。
- en: Exercise 9-2\.
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 9-2\.
- en: Implement the Gram-Schmidt procedure as described earlier.^([4](ch09.xhtml#idm45733297281040))
    Use a <math alttext="4 times 4"><mrow><mn>4</mn> <mo>×</mo> <mn>4</mn></mrow></math>
    random-numbers matrix. Check your answer against <math alttext="bold upper Q"><mi>𝐐</mi></math>
    from `np.linalg.qr`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 按照前述步骤实现Gram-Schmidt过程。使用一个<math alttext="4 times 4"><mrow><mn>4</mn> <mo>×</mo>
    <mn>4</mn></mrow></math>随机数矩阵。检查您的答案是否与`np.linalg.qr`中的<math alttext="bold upper
    Q"><mi>𝐐</mi></math>一致。
- en: '**Important**: there is a fundamental sign uncertainty in transformations like
    the Householder reflection. This means that vectors can “flip” (be multiplied
    by −1) depending on minor differences in algorithm and implementation. This feature
    exists in many matrix decompositions including eigenvectors. I have a longer and
    more in-depth discussion of why this is and what it means in [Chapter 13](ch13.xhtml#Chapter_13).
    For now, the upshot is this: *subtract* your <math alttext="bold upper Q"><mi>𝐐</mi></math>
    from Python’s <math alttext="bold upper Q"><mi>𝐐</mi></math> and *add* your <math
    alttext="bold upper Q"><mi>𝐐</mi></math> and Python’s <math alttext="bold upper
    Q"><mi>𝐐</mi></math> . Nonzero columns in one will be zeros in the other.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**重要提示**：在像Householder反射这样的变换中存在根本性的符号不确定性。这意味着向量可以根据算法和实现的细微差异“翻转”（乘以-1）。这种特性存在于许多矩阵分解中，包括特征向量。我在[第13章](ch13.xhtml#Chapter_13)中对此进行了更长更深入的讨论。暂且而言，要点是这样的：从Python的<math
    alttext="bold upper Q"><mi>𝐐</mi></math>中*减去*您的<math alttext="bold upper Q"><mi>𝐐</mi></math>，并*加上*您的<math
    alttext="bold upper Q"><mi>𝐐</mi></math>和Python的<math alttext="bold upper Q"><mi>𝐐</mi></math>。其中一个中的非零列将在另一个中为零列。'
- en: Exercise 9-3\.
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 9-3\.
- en: In this exercise, you will find out what happens when you apply QR decomposition
    to a matrix that is almost-but-not-quite orthogonal. First, create an orthogonal
    matrix, called <math alttext="bold upper U"><mi>𝐔</mi></math> , from the QR decomposition
    of a <math alttext="6 times 6"><mrow><mn>6</mn> <mo>×</mo> <mn>6</mn></mrow></math>
    random-numbers matrix. Compute the QR decomposition of <math alttext="bold upper
    U"><mi>𝐔</mi></math> , and confirm that <math alttext="bold upper R equals bold
    upper I"><mrow><mi>𝐑</mi> <mo>=</mo> <mi>𝐈</mi></mrow></math> (and make sure you
    understand why!).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，您将了解在将几乎正交的矩阵应用QR分解时会发生什么。首先，从一个<math alttext="6 times 6"><mrow><mn>6</mn>
    <mo>×</mo> <mn>6</mn></mrow></math>随机数矩阵的QR分解创建一个称为<math alttext="bold upper U"><mi>𝐔</mi></math>的正交矩阵。计算<math
    alttext="bold upper U"><mi>𝐔</mi></math>的QR分解，并确认<math alttext="bold upper R equals
    bold upper I"><mrow><mi>𝐑</mi> <mo>=</mo> <mi>𝐈</mi></mrow></math>（并确保您理解为什么！）。
- en: Second, modify the norms of each column of <math alttext="bold upper U"><mi>𝐔</mi></math>
    . Set the norms of columns 1–6 to be 10–15 (that is, the first column of <math
    alttext="bold upper U"><mi>𝐔</mi></math> should have a norm of 10, the second
    column should have a norm of 11, and so on). Run that modulated <math alttext="bold
    upper U"><mi>𝐔</mi></math> matrix through QR decomposition and confirm that its
    <math alttext="bold upper R"><mi>𝐑</mi></math> is a diagonal matrix with diagonal
    elements equaling 10–15\. What is <math alttext="bold upper Q Superscript upper
    T Baseline bold upper Q"><mrow><msup><mi>𝐐</mi> <mtext>T</mtext></msup> <mi>𝐐</mi></mrow></math>
    for this matrix?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，修改每列 <math alttext="bold upper U"><mi>𝐔</mi></math> 的范数。将列 1–6 的范数设置为 10–15（即，<math
    alttext="bold upper U"><mi>𝐔</mi></math> 的第一列应该有一个范数为 10，第二列应该有一个范数为 11，依此类推）。将调整后的
    <math alttext="bold upper U"><mi>𝐔</mi></math> 矩阵通过 QR 分解，并确认其 <math alttext="bold
    upper R"><mi>𝐑</mi></math> 是对角矩阵，对角线元素等于 10–15\. 对于这个矩阵，<math alttext="bold upper
    Q Superscript upper T Baseline bold upper Q"><mrow><msup><mi>𝐐</mi> <mtext>T</mtext></msup>
    <mi>𝐐</mi></mrow></math> 是什么？
- en: Third, break the orthogonality of <math alttext="bold upper U"><mi>𝐔</mi></math>
    by setting element <math alttext="u Subscript 1 comma 4 Baseline equals 0"><mrow><msub><mi>u</mi>
    <mrow><mn>1</mn><mo>,</mo><mn>4</mn></mrow></msub> <mo>=</mo> <mn>0</mn></mrow></math>
    . What happens to <math alttext="bold upper R"><mi>𝐑</mi></math> and why?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步，通过设置元素 <math alttext="u Subscript 1 comma 4 Baseline equals 0"><mrow><msub><mi>u</mi>
    <mrow><mn>1</mn><mo>,</mo><mn>4</mn></mrow></msub> <mo>=</mo> <mn>0</mn></mrow></math>
    来打破 <math alttext="bold upper U"><mi>𝐔</mi></math> 的正交性。 <math alttext="bold upper
    R"><mi>𝐑</mi></math> 会发生什么？为什么？
- en: Exercise 9-4\.
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 9-4\.
- en: The purpose of this exercise is to compare the numerical errors using the “old-school”
    inverse method you learned in the previous chapter to the QR-based inverse method.
    We will use random-numbers matrices, keeping in mind that they tend to be numerically
    stable and thus have accurate inverses.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习的目的是比较使用前一章学到的“老派”逆方法与基于 QR 的逆方法的数值误差。我们将使用随机数矩阵，注意它们倾向于是数值稳定的，因此具有准确的逆矩阵。
- en: 'Here’s what to do: copy the code from [Exercise 8-2](ch08.xhtml#exercise_8_2)
    into a Python function that takes a matrix as input and provides its inverse as
    output. (You can also include a check that the input matrix is square and full-rank.)
    I called this function `oldSchoolInv`. Next, create a <math alttext="5 times 5"><mrow><mn>5</mn>
    <mo>×</mo> <mn>5</mn></mrow></math> random-numbers matrix. Compute its inverse
    using the old-school method and the QR decomposition method introduced in this
    chapter (you can use your “old-school method” to compute <math alttext="bold upper
    R Superscript negative 1"><msup><mi>𝐑</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    ). Compute the inverse-estimation error as the Euclidean distance from the matrix
    times its inverse to the true identity matrix from `np.eye`. Make a barplot of
    the results, showing the two methods on the *x*-axis and the error (Euclidean
    distance to <math alttext="bold upper I"><mi>𝐈</mi></math> ) on the *y*-axis,
    as in [Figure 9-3](#fig_9_3).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是该做的事情：将代码从 [Exercise 8-2](ch08.xhtml#exercise_8_2) 复制到一个 Python 函数中，该函数以矩阵作为输入，并输出其逆矩阵。（您还可以包含一个检查输入矩阵是否为方阵和满秩的检查。）我将这个函数称为
    `oldSchoolInv`。接下来，创建一个 <math alttext="5 times 5"><mrow><mn>5</mn> <mo>×</mo>
    <mn>5</mn></mrow></math> 的随机数矩阵。使用上一章介绍的“老派方法”和本章介绍的 QR 分解方法计算其逆矩阵（您可以使用您的“老派方法”计算
    <math alttext="bold upper R Superscript negative 1"><msup><mi>𝐑</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    ）。将矩阵乘以其逆矩阵，并计算到真实单位矩阵 `np.eye` 的欧氏距离作为逆估计误差。制作一个条形图显示结果，将两种方法显示在 *x* 轴上，误差（到
    <math alttext="bold upper I"><mi>𝐈</mi></math> 的欧氏距离）显示在 *y* 轴上，就像 [Figure 9-3](#fig_9_3)
    中所示。
- en: '![QR and oldschool](assets/plad_0903.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![QR 和 oldschool](assets/plad_0903.png)'
- en: Figure 9-3\. Results of Exercise 9-4
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-3\. 练习 9-4 的结果
- en: Run the code multiple times and inspect the barplot. You’ll find that sometimes
    the old-school method is better while other times the QR method is better (smaller
    numbers are better; in theory, the bars should have zero height). Try it again
    using a <math alttext="30 times 30"><mrow><mn>30</mn> <mo>×</mo> <mn>30</mn></mrow></math>
    matrix. Are the results more consistent? In fact, there is a lot of variance from
    run to run. This means we should run an experiment where we repeat the comparison
    many times. That’s the next exercise.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 多次运行代码并检查条形图。您会发现有时候“老派”方法更好，而其他时候 QR 方法更好（较小的数字更好；理论上，条应该高度为零）。尝试使用 <math alttext="30
    times 30"><mrow><mn>30</mn> <mo>×</mo> <mn>30</mn></mrow></math> 矩阵再次尝试。结果更一致了吗？实际上，每次运行结果都有很大差异。这意味着我们应该进行一项实验，多次重复比较。这就是下一个练习。
- en: Exercise 9-5\.
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 9-5\.
- en: Put the code from the previous exercise into a `for` loop over a hundred iterations
    in which you repeat the experiment, each time using a different random-numbers
    matrix. Store the error (Euclidean distance) for each iteration, and make a plot
    like [Figure 9-4](#fig_9_4), which shows the average over all experiment runs
    (gray bar) and all individual errors (black dots). Run the experiment for <math
    alttext="5 times 5"><mrow><mn>5</mn> <mo>×</mo> <mn>5</mn></mrow></math> and <math
    alttext="30 times 30"><mrow><mn>30</mn> <mo>×</mo> <mn>30</mn></mrow></math> matrices.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 将前一个练习中的代码放入一个`for`循环中，重复100次实验，每次使用不同的随机数矩阵。存储每次迭代的误差（欧几里得距离），并制作像[图 9-4](#fig_9_4)那样的图表，显示所有实验运行的平均值（灰色条）和所有单个误差（黑色点）。运行<math
    alttext="5 times 5"><mrow><mn>5</mn> <mo>×</mo> <mn>5</mn></mrow></math>和<math
    alttext="30 times 30"><mrow><mn>30</mn> <mo>×</mo> <mn>30</mn></mrow></math>矩阵的实验。
- en: You can also try using `np.linalg.inv` to invert <math alttext="bold upper R"><mi>𝐑</mi></math>
    instead of the old-school method to see if that has an effect.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以尝试使用`np.linalg.inv`来反转<math alttext="bold upper R"><mi>𝐑</mi></math>，而不是传统的方法，看看是否有影响。
- en: '![QR > oldschool](assets/plad_0904.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![QR > oldschool](assets/plad_0904.png)'
- en: Figure 9-4\. Results of Exercise 9-5\. Note the difference in y-axis scaling
    between the left and right panels.
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-4\. 练习 9-5 的结果。请注意左右面板在y轴缩放上的差异。
- en: Exercise 9-6\.
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 9-6.
- en: An interesting property of square orthogonal matrices is that all of their singular
    values (and their eigenvalues) are 1\. That means that they have an induced 2-norm
    of 1 (the induced norm is the largest singular value), and they have a Frobenius
    norm of *M*. The latter result is because the Frobenius norm equals the square
    root of the sum of the squared singular values. In this exercise, you will confirm
    these properties.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 方阵正交矩阵的一个有趣特性是它们的所有奇异值（及其特征值）都是1。这意味着它们有一个诱导2-范数为1（诱导范数是最大奇异值），并且它们有一个Frobenius范数为*M*。后者的结果是因为Frobenius范数等于所有奇异值的平方和的平方根。在这个练习中，你将确认这些特性。
- en: Create an <math alttext="upper M times upper M"><mrow><mi>M</mi> <mo>×</mo>
    <mi>M</mi></mrow></math> orthogonal matrix as the QR decomposition of a random
    matrix. Compute its induced 2-norm using `np.linalg.norm` and compute its Frobenius
    norm using the equation you learned in [Chapter 6](ch06.xhtml#Chapter_6), divided
    by the square root of *M*. Confirm that both quantities are 1 (to within a reasonable
    tolerance of rounding error). Check using several different values of *M*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个<math alttext="upper M times upper M"><mrow><mi>M</mi> <mo>×</mo> <mi>M</mi></mrow></math>正交矩阵作为一个随机矩阵的QR分解。使用`np.linalg.norm`计算其诱导2-范数，并使用你在[第 6 章](ch06.xhtml#Chapter_6)学到的公式计算其Frobenius范数，除以*M*的平方根。确认这两个量约为1（考虑到四舍五入误差的合理容差）。使用几个不同的*M*值进行检查。
- en: Next, explore the meaning of the induced norm using matrix-vector multiplication.
    Create a random *M*-element column vector <math alttext="bold v"><mi>𝐯</mi></math>
    . Then compute the norms of <math alttext="bold v"><mi>𝐯</mi></math> and <math
    alttext="bold upper Q bold v"><mrow><mi>𝐐</mi> <mi>𝐯</mi></mrow></math> . Those
    norms should equal each other (although you wouldn’t expect them to equal 1).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过矩阵-向量乘法探索诱导范数的含义。创建一个随机的*M*元素列向量<math alttext="bold v"><mi>𝐯</mi></math>。然后计算<math
    alttext="bold v"><mi>𝐯</mi></math>和<math alttext="bold upper Q bold v"><mrow><mi>𝐐</mi>
    <mi>𝐯</mi></mrow></math>的范数。这些范数应该相等（尽管你不会期望它们等于1）。
- en: Finally, get a piece of paper and develop a proof of that empirical demonstration.
    That proof is printed in the next paragraph, so don’t look down! But you can check
    the footnote if you need a hint.^([5](ch09.xhtml#idm45733297196544))
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，拿一张纸，发展一个证明这个经验演示的证明。这个证明在下一段打印出来，所以不要往下看！但如果你需要提示，可以查看脚注^([5](ch09.xhtml#idm45733297196544))。
- en: I sincerely hope you are reading this to check your reasoning, not because you
    are cheating! Anyway, the proof is that the vector norm <math alttext="parallel-to
    bold v parallel-to"><mrow><mo>∥</mo> <mi>𝐯</mi> <mo>∥</mo></mrow></math> can be
    computed as <math alttext="bold v Superscript upper T Baseline bold v"><mrow><msup><mi>𝐯</mi>
    <mtext>T</mtext></msup> <mi>𝐯</mi></mrow></math> ; therefore, the vector norm
    <math alttext="parallel-to bold upper Q bold v parallel-to"><mrow><mo>∥</mo> <mi>𝐐</mi>
    <mi>𝐯</mi> <mo>∥</mo></mrow></math> is computed as <math alttext="left-parenthesis
    bold upper Q bold v right-parenthesis Superscript upper T Baseline bold upper
    Q bold v equals bold v Superscript upper T Baseline bold upper Q Superscript upper
    T Baseline bold upper Q bold v"><mrow><msup><mrow><mo>(</mo><mi>𝐐</mi><mi>𝐯</mi><mo>)</mo></mrow>
    <mtext>T</mtext></msup> <mi>𝐐</mi> <mi>𝐯</mi> <mo>=</mo> <msup><mi>𝐯</mi> <mtext>T</mtext></msup>
    <msup><mi>𝐐</mi> <mtext>T</mtext></msup> <mi>𝐐</mi> <mi>𝐯</mi></mrow></math> .
    The <math alttext="bold upper Q Superscript upper T Baseline bold upper Q"><mrow><msup><mi>𝐐</mi>
    <mtext>T</mtext></msup> <mi>𝐐</mi></mrow></math> cancels to give the identity
    matrix, leaving the dot product of the vector with itself. The conclusion is that
    orthogonal matrices can rotate but never scale a vector.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我真诚地希望你读这篇文章是为了检查你的推理能力，而不是作弊！不管怎样，证明是向量范数 <math alttext="parallel-to bold v
    parallel-to"><mrow><mo>∥</mo> <mi>𝐯</mi> <mo>∥</mo></mrow></math> 可以计算为 <math
    alttext="bold v Superscript upper T Baseline bold v"><mrow><msup><mi>𝐯</mi> <mtext>T</mtext></msup>
    <mi>𝐯</mi></mrow></math> ；因此，向量范数 <math alttext="parallel-to bold upper Q bold
    v parallel-to"><mrow><mo>∥</mo> <mi>𝐐</mi> <mi>𝐯</mi> <mo>∥</mo></mrow></math>
    计算为 <math alttext="left-parenthesis bold upper Q bold v right-parenthesis Superscript
    upper T Baseline bold upper Q bold v equals bold v Superscript upper T Baseline
    bold upper Q Superscript upper T Baseline bold upper Q bold v"><mrow><msup><mrow><mo>(</mo><mi>𝐐</mi><mi>𝐯</mi><mo>)</mo></mrow>
    <mtext>T</mtext></msup> <mi>𝐐</mi> <mi>𝐯</mi> <mo>=</mo> <msup><mi>𝐯</mi> <mtext>T</mtext></msup>
    <msup><mi>𝐐</mi> <mtext>T</mtext></msup> <mi>𝐐</mi> <mi>𝐯</mi></mrow></math> 。<math
    alttext="bold upper Q Superscript upper T Baseline bold upper Q"><mrow><msup><mi>𝐐</mi>
    <mtext>T</mtext></msup> <mi>𝐐</mi></mrow></math> 取消以得到单位矩阵，留下向量与自身的点积。结论是正交矩阵可以旋转但不会缩放向量。
- en: Exercise 9-7\.
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 9-7。
- en: 'This exercise will highlight one feature of the <math alttext="bold upper R"><mi>𝐑</mi></math>
    matrix that is relevant for understanding how to use QR to implement least squares
    ([Chapter 12](ch12.xhtml#Chapter_12)): when <math alttext="bold upper A"><mi>𝐀</mi></math>
    is tall and full column-rank, the first *N* rows of <math alttext="bold upper
    R"><mi>𝐑</mi></math> are upper-triangular, whereas rows *N* + 1 through *M* are
    zeros. Confirm this in Python using a random <math alttext="10 times 4"><mrow><mn>10</mn>
    <mo>×</mo> <mn>4</mn></mrow></math> matrix. Make sure to use the complete (full)
    QR decomposition, not the economy (compact) decomposition.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习将突出 <math alttext="bold upper R"><mi>𝐑</mi></math> 矩阵的一个特点，这个特点对于理解如何使用
    QR 分解来实现最小二乘法（[第 12 章](ch12.xhtml#Chapter_12)）非常重要：当 <math alttext="bold upper
    A"><mi>𝐀</mi></math> 高且满列秩时，<math alttext="bold upper R"><mi>𝐑</mi></math> 的前
    *N* 行是上三角的，而第 *N* + 1 行到 *M* 行是零。在 Python 中使用一个随机的 <math alttext="10 times 4"><mrow><mn>10</mn>
    <mo>×</mo> <mn>4</mn></mrow></math> 矩阵进行确认。确保使用完全（全）QR 分解，而不是经济（紧凑）分解。
- en: Of course, <math alttext="bold upper R"><mi>𝐑</mi></math> is noninvertible because
    it is nonsquare. But (1) the submatrix comprising the first *N* rows is square
    and full-rank (when <math alttext="bold upper A"><mi>𝐀</mi></math> is full column-rank)
    and thus has a full inverse, and (2) the tall <math alttext="bold upper R"><mi>𝐑</mi></math>
    has a pseudoinverse. Compute both inverses, and confirm that the full inverse
    of the first *N* rows of <math alttext="bold upper R"><mi>𝐑</mi></math> equals
    the first *N* columns of the pseudoinverse of the tall <math alttext="bold upper
    R"><mi>𝐑</mi></math> .
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，<math alttext="bold upper R"><mi>𝐑</mi></math> 是不可逆的，因为它不是方阵。但是（1）当<math
    alttext="bold upper A"><mi>𝐀</mi></math>是满列秩时，第一 *N* 行构成的子矩阵是方阵且满秩，因此具有完全的逆；（2）高瘦的
    <math alttext="bold upper R"><mi>𝐑</mi></math> 具有伪逆。计算这两个逆，并确认 <math alttext="bold
    upper R"><mi>𝐑</mi></math> 的第一 *N* 行的完全逆等于高瘦 <math alttext="bold upper R"><mi>𝐑</mi></math>
    的伪逆的第一 *N* 列。
- en: ^([1](ch09.xhtml#idm45733297769472-marker)) This is further explored in [Exercise
    9-1](#exercise_9_1).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch09.xhtml#idm45733297769472-marker)) 这在 [Exercise 9-1](#exercise_9_1)
    中进一步探讨。
- en: ^([2](ch09.xhtml#idm45733297664848-marker)) The first column vector is not orthogonalized
    because there are no preceeding vectors; therefore, you begin with the following
    normalization step.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch09.xhtml#idm45733297664848-marker)) 第一列向量不是正交的，因为没有前面的向量；因此，你从以下的标准化步骤开始。
- en: ^([3](ch09.xhtml#idm45733297640832-marker)) Recovering **R** through matrix
    multiplication is possible because GS is a series of linear transformations.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch09.xhtml#idm45733297640832-marker)) 通过矩阵乘法恢复**R**是可能的，因为GS是一系列线性变换。
- en: ^([4](ch09.xhtml#idm45733297281040-marker)) Take your time with this exercise;
    it’s quite challenging.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch09.xhtml#idm45733297281040-marker)) 在这个练习中花些时间，它相当具有挑战性。
- en: '^([5](ch09.xhtml#idm45733297196544-marker)) Hint: write down the dot-product
    formula for the vector norm.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch09.xhtml#idm45733297196544-marker)) 提示：写出向量范数的点积公式。
