- en: Chapter 10\. Exploring Semantic Relationships with Word Embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 10 章。利用词嵌入探索语义关系
- en: The concept of similarity is fundamental to all machine learning tasks. In [Chapter 5](ch05.xhtml#ch-vectorization),
    we explained how to compute text similarity based on the bag-of-words model. Given
    two TF-IDF vectors for documents, their cosine similarity can be easily computed,
    and we can use this information to search, cluster, or classify similar documents.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 相似性的概念对所有机器学习任务都是基础性的。在[第 5 章](ch05.xhtml#ch-vectorization)中，我们解释了如何基于词袋模型计算文本相似性。给定两个文档的
    TF-IDF 向量，它们的余弦相似度可以轻松计算，我们可以使用这些信息来搜索、聚类或分类相似的文档。
- en: 'However, the concept of similarity in the bag-of-words model is completely
    based on the number of common words in two documents. If documents do not share
    any tokens, the dot product of the document vectors and hence the cosine similarity
    will be zero. Consider the following two comments about a new movie, which could
    be found on a social platform:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在词袋模型中，相似性的概念完全基于两个文档中共同单词的数量。如果文档没有共享任何标记，文档向量的点积以及因此的余弦相似度将为零。考虑以下关于一部新电影的两条社交平台评论：
- en: “What a wonderful movie.”
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “多么美妙的电影。”
- en: ''
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “The film is great.”
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “这部电影很棒。”
- en: Obviously, the comments have a similar meaning even though they use completely
    different words. In this chapter, we will introduce word embeddings as a means
    to capture the semantics of words and use them to explore semantic similarities
    within a corpus.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，尽管使用完全不同的词语，这些评论具有类似的含义。在本章中，我们将介绍词嵌入作为捕捉单词语义并用于探索语义相似性的一种手段。
- en: What You’ll Learn and What We’ll Build
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 您将学到什么以及我们将构建什么
- en: For our use case we assume that we are market researchers and want to use texts
    about cars to better understand some relationships in the car market. Specifically,
    we want to explore similarities among car brands and models. For example, which
    models of brand A are most similar to a given model of brand B?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的用例，我们假设我们是市场研究人员，希望使用关于汽车的文本来更好地理解汽车市场中的一些关系。具体而言，我们想探索汽车品牌和型号之间的相似性。例如，品牌
    A 的哪些型号与品牌 B 的特定型号最相似？
- en: Our corpus consists of the 20 subreddits in the autos category of the Reddit
    Self-Posts dataset, which was already used in [Chapter 4](ch04.xhtml#ch-preparation).
    Each of these subreddits contains 1,000 posts on cars and motorcycles with brands
    such as Mercedes, Toyota, Ford, and Harley-Davidson. Since those posts are questions,
    answers, and comments written by users, we will actually get an idea of what these
    users implicitly *consider* as being similar.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的语料库包括 Reddit 自我帖子数据集汽车类别中的 20 个子社区，这在[第 4 章](ch04.xhtml#ch-preparation)中已经使用过。每个子社区都包含关于
    Mercedes、Toyota、Ford 和 Harley-Davidson 等品牌的汽车和摩托车的 1,000 条帖子。由于这些帖子是用户编写的问题、答案和评论，我们实际上可以了解到这些用户隐含地认为什么是相似的。
- en: We will use the [Gensim library](https://oreil.ly/HaYkR) again, which was introduced
    in [Chapter 8](ch08.xhtml#ch-topicmodels). It provides a nice API to train different
    types of embeddings and to use those models for semantic reasoning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用[Gensim 库](https://oreil.ly/HaYkR)，这在[第 8 章](ch08.xhtml#ch-topicmodels)中已经介绍过。它提供了一个良好的
    API 来训练不同类型的嵌入，并使用这些模型进行语义推理。
- en: After studying this chapter, you will be able to use word embeddings for semantic
    analysis. You will know how to use pretrained embeddings, how to train your own
    embeddings, how to compare different models, and how to visualize them. You can
    find the source code for this chapter along with some of the images in our [GitHub
    repository](https://oreil.ly/W1ztU).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 学习本章后，您将能够使用词嵌入进行语义分析。您将知道如何使用预训练的嵌入，如何训练自己的嵌入，如何比较不同的模型以及如何可视化它们。您可以在我们的[GitHub
    代码库](https://oreil.ly/W1ztU)中找到本章的源代码以及部分图像。
- en: The Case for Semantic Embeddings
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语义嵌入的理由
- en: 'In the previous chapters, we used the TF-IDF vectorization for our models.
    It is easy to compute, but it has some severe disadvantages:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们使用 TF-IDF 向量化我们的模型。这种方法易于计算，但也有一些严重的缺点：
- en: The document vectors have a very high dimensionality that is defined by the
    size of the vocabulary. Thus, the vectors are extremely sparse; i.e., most entries
    are zero.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档向量具有由词汇量大小定义的非常高的维度。因此，向量非常稀疏；即大多数条目为零。
- en: It does not work well for short texts like Twitter messages, service comments,
    and similar content because the probability for common words is low for short
    texts.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在短文本（如 Twitter 消息、服务评论和类似内容）中表现不佳，因为短文本中共同词的概率较低。
- en: Advanced applications such as sentiment analysis, question answering, or machine
    translation require capturing the real meaning of the words to work correctly.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级应用例如情感分析、问答或机器翻译需要准确捕捉单词的实际含义以正确工作。
- en: Still, the bag-of-words model works surprisingly well for tasks such as classification
    or topic modeling, but only if the texts are sufficiently long and enough training
    data is available. Remember that similarity in the bag-of-words model is solely
    based on the existence of significant common words.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管词袋模型在分类或主题建模等任务中表现出色，但仅当文本足够长且有足够的训练数据时。请记住，词袋模型中的相似性仅基于显著共同单词的存在。
- en: An *embedding*, in contrast, is a dense numerical vector representation of an
    object that captures some kind of *semantic* similarity. When we talk of embeddings
    in the context of text analysis, we have to distinguish word and document embeddings.
    A *word embedding* is a vector representation for a single word, while a *document
    embedding* is a vector representing a document. By *document* we mean any sequence
    of words, be it a short phrase, a sentence, a paragraph, or even a long article.
    In this chapter, we will focus on dense vector representations for words.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 而*嵌入*则是一个密集的数值向量表示对象，捕捉某种*语义*相似性。当我们在文本分析的背景下讨论嵌入时，我们必须区分单词嵌入和文档嵌入。*单词嵌入*是单个单词的向量表示，而*文档嵌入*是代表文档的向量。在这一章节中，我们将重点关注单词的密集向量表示。
- en: Word Embeddings
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单词嵌入
- en: 'The target of an embedding algorithm can be defined as follows: given a dimensionality
    <math alttext="d"><mi>d</mi></math> , find vector representations for words such
    that words with similar meanings have similar vectors. The dimensionality <math
    alttext="d"><mi>d</mi></math> is a hyperparameter of any word embedding algorithm.
    It is typically set to a value between 50 and 300.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入算法的目标可以定义如下：给定一个维度<math alttext="d"><mi>d</mi></math>，找到单词的向量表示，使得具有相似含义的单词具有相似的向量。维度<math
    alttext="d"><mi>d</mi></math>是任何单词嵌入算法的超参数。通常设置在50到300之间。
- en: The dimensions themselves have no predefined or human-understandable meaning.
    Instead, the model learns latent relations among the words from the text. [Figure 10-1](#fig-embeddings)
    (left) illustrates the concept. We have five-dimensional vectors for each word.
    Each of these dimensions represents some relation among the words so that words
    similar in that aspect have similar values in this dimension. Dimension names
    shown are possible interpretations of those values.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 维度本身没有预定义或人类可理解的含义。相反，模型从文本中学习单词之间的潜在关系。[图 10-1](#fig-embeddings)（左）展示了这一概念。我们对每个单词有五维向量。这些维度中的每一个代表单词之间某种关系，使得在这一维度上相似的单词具有类似的值。所示的维度名称是对这些值的可能解释。
- en: '![](Images/btap_1001.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1001.jpg)'
- en: Figure 10-1\. Dense vector representations captioning semantic similarity of
    words (left) can be used to answer analogy questions (right). We gave the vector
    dimensions hypothetical names like “Royalty” to show possible interpretations.^([1](ch10.xhtml#idm45634182078904))
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1\. 密集向量表示语义相似性的标注（左）可用于回答类比问题（右）。我们对向量维度命名为“Royalty”等来展示可能的解释。^([1](ch10.xhtml#idm45634182078904))
- en: The basic idea for training is that words occurring in similar contexts have
    similar meanings. This is called the *distributional hypothesis*. Take, for example,
    the following sentences describing *tesgüino*:^([2](ch10.xhtml#idm45634182074280))
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的基本思想是在相似上下文中出现的单词具有相似的含义。这被称为*分布假设*。例如，以下描述*tesgüino*的句子：^([2](ch10.xhtml#idm45634182074280))
- en: A bottle of ___ is on the table.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 桌子上有一瓶___。
- en: Everybody likes ___ .
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个人都喜欢___。
- en: Don’t have ___ before you drive.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开车前不要___。
- en: We make ___ out of corn.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们用玉米制造___。
- en: Even without knowing the word *tesgüino*, you get a pretty good understanding
    of its meaning by analyzing typical contexts. You could also identify semantically
    similar words because you know it’s an alcoholic beverage.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 即使不了解*tesgüino*这个词，通过分析典型语境，你也能对其含义有相当好的理解。你还可以识别语义上相似的单词，因为你知道它是一种酒精饮料。
- en: Analogy Reasoning with Word Embeddings
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用单词嵌入进行类比推理
- en: 'What’s really amazing is that word vectors built this way allow us to detect
    analogies like “queen is to king like woman is to man” with vector algebra ([Figure 10-1](#fig-embeddings),
    right). Let <math alttext="v left-parenthesis w right-parenthesis"><mrow><mi>v</mi>
    <mo>(</mo> <mi>w</mi> <mo>)</mo></mrow></math> be the word embedding for a word
    <math alttext="w"><mi>w</mi></math> . Then the analogy can be expressed mathematically
    like this:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 真正令人惊讶的是，用这种方法构建的词向量使我们能够通过向量代数检测类似于“queen is to king like woman is to man”的类比（见[图 10-1](#fig-embeddings)右侧）。设
    <math alttext="v left-parenthesis w right-parenthesis"><mrow><mi>v</mi> <mo>(</mo>
    <mi>w</mi> <mo>)</mo></mrow></math> 为单词 <math alttext="w"><mi>w</mi></math> 的词嵌入。那么这个类比可以用数学方式表达如下：
- en: <math alttext="v left-parenthesis q u e e n right-parenthesis minus v left-parenthesis
    k i n g right-parenthesis almost-equals v left-parenthesis w o m a n right-parenthesis
    minus v left-parenthesis m a n right-parenthesis" display="block"><mrow><mi>v</mi>
    <mo>(</mo> <mi>q</mi> <mi>u</mi> <mi>e</mi> <mi>e</mi> <mi>n</mi> <mo>)</mo> <mo>-</mo>
    <mi>v</mi> <mo>(</mo> <mi>k</mi> <mi>i</mi> <mi>n</mi> <mi>g</mi> <mo>)</mo> <mo>≈</mo>
    <mi>v</mi> <mo>(</mo> <mi>w</mi> <mi>o</mi> <mi>m</mi> <mi>a</mi> <mi>n</mi> <mo>)</mo>
    <mo>-</mo> <mi>v</mi> <mo>(</mo> <mi>m</mi> <mi>a</mi> <mi>n</mi> <mo>)</mo></mrow></math>
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="v left-parenthesis q u e e n right-parenthesis minus v left-parenthesis
    k i n g right-parenthesis almost-equals v left-parenthesis w o m a n right-parenthesis
    minus v left-parenthesis m a n right-parenthesis" display="block"><mrow><mi>v</mi>
    <mo>(</mo> <mi>q</mi> <mi>u</mi> <mi>e</mi> <mi>e</mi> <mi>n</mi> <mo>)</mo> <mo>-</mo>
    <mi>v</mi> <mo>(</mo> <mi>k</mi> <mi>i</mi> <mi>n</mi> <mi>g</mi> <mo>)</mo> <mo>≈</mo>
    <mi>v</mi> <mo>(</mo> <mi>w</mi> <mi>o</mi> <mi>m</mi> <mi>a</mi> <mi>n</mi> <mo>)</mo>
    <mo>-</mo> <mi>v</mi> <mo>(</mo> <mi>m</mi> <mi>a</mi> <mi>n</mi> <mo>)</mo></mrow></math>
- en: 'If this approximate equation holds, we can reformulate the analogy as a question:
    What is to *king* like “woman” is to “man”? Or mathematically:^([3](ch10.xhtml#idm45634182038552))'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个近似等式成立，我们可以将这个类比重述为一个问题：像“king”对应于“man”，“woman”对应于什么？或者数学上表示为：^([3](ch10.xhtml#idm45634182038552))
- en: <math alttext="v left-parenthesis w o m a n right-parenthesis plus left-bracket
    v left-parenthesis k i n g right-parenthesis minus v left-parenthesis m a n right-parenthesis
    right-bracket almost-equals question-mark" display="block"><mrow><mi>v</mi> <mrow><mo>(</mo>
    <mi>w</mi> <mi>o</mi> <mi>m</mi> <mi>a</mi> <mi>n</mi> <mo>)</mo></mrow> <mo>+</mo>
    <mfenced separators="" open="[" close="]"><mi>v</mi> <mo>(</mo> <mi>k</mi> <mi>i</mi>
    <mi>n</mi> <mi>g</mi> <mo>)</mo> <mo>-</mo> <mi>v</mi> <mo>(</mo> <mi>m</mi> <mi>a</mi>
    <mi>n</mi> <mo>)</mo></mfenced> <mo>≈</mo> <mo>?</mo></mrow></math>
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="v left-parenthesis w o m a n right-parenthesis plus left-bracket
    v left-parenthesis k i n g right-parenthesis minus v left-parenthesis m a n right-parenthesis
    right-bracket almost-equals question-mark" display="block"><mrow><mi>v</mi> <mrow><mo>(</mo>
    <mi>w</mi> <mi>o</mi> <mi>m</mi> <mi>a</mi> <mi>n</mi> <mo>)</mo></mrow> <mo>+</mo>
    <mfenced separators="" open="[" close="]"><mi>v</mi> <mo>(</mo> <mi>k</mi> <mi>i</mi>
    <mi>n</mi> <mi>g</mi> <mo>)</mo> <mo>-</mo> <mi>v</mi> <mo>(</mo> <mi>m</mi> <mi>a</mi>
    <mi>n</mi> <mo>)</mo></mfenced> <mo>≈</mo> <mo>?</mo></mrow></math>
- en: 'This allows some kind of fuzzy reasoning to answer analogy questions like this
    one: “Given that Paris is the capital of France, what is the capital of Germany?”
    Or in a market research scenario as the one we will explore: “Given that F-150
    is a pickup truck from Ford, what is the similar model from Toyota?”'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方式允许一种模糊推理来回答类似于这样的类比问题：“巴黎是法国的首都，那么德国的首都是什么？”或者在市场研究场景中，正如我们将要探索的那样：“考虑到
    F-150 是福特的皮卡，那么丰田的类似车型是什么？”
- en: Types of Embeddings
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入类型
- en: Several algorithms have been developed to train word embeddings. Gensim allows
    you to train Word2Vec and FastText embeddings. GloVe embeddings can be used for
    similarity queries but not trained with Gensim. We introduce the basic ideas of
    these algorithms and briefly explain the more advanced but also more complex contextualized
    embedding methods. You will find the references to the original papers and further
    explanations at the end of this chapter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 已经开发了几种算法来训练词嵌入。Gensim 允许您训练 Word2Vec 和 FastText 词嵌入。GloVe 词嵌入可以用于相似性查询，但不能与
    Gensim 一起训练。我们介绍了这些算法的基本思想，并简要解释了更先进但也更复杂的上下文嵌入方法。您将在本章末找到原始论文的参考文献和进一步的解释。
- en: Word2Vec
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Word2Vec
- en: Even though there have been approaches for word embeddings before, the work
    of Tomáš Mikolov at Google (Mikolov et al., 2013) marks a milestone because it
    dramatically outperformed previous approaches, especially on analogy tasks such
    as the ones just explained. There exist two variants of Word2Vec, the *continuous
    bag-of-words model* (CBOW) and the *skip-gram model* (see [Figure 10-2](#fig-word2vec)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管之前已经有过词嵌入的方法，但谷歌的 Tomáš Mikolov（Mikolov 等人，2013年）的工作标志着一个里程碑，因为它在类比任务上显著优于以前的方法，特别是刚刚解释的那些任务。Word2Vec
    有两个变体，即*连续词袋模型*（CBOW）和*跳字模型*（见[图 10-2](#fig-word2vec)）。
- en: '![](Images/btap_1002.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1002.jpg)'
- en: Figure 10-2\. Continuous bag-of words (left) versus skip-gram model (right).
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-2\. 连续词袋模型（左）与跳字模型（右）。
- en: 'Both algorithms use a sliding window over the text, defined by a target word
    <math alttext="w Subscript t"><msub><mi>w</mi> <mi>t</mi></msub></math> and the
    size of the context window <math alttext="c"><mi>c</mi></math> . In the example,
    <math alttext="c equals 2"><mrow><mi>c</mi> <mo>=</mo> <mn>2</mn></mrow></math>
    , i.e., the training samples consist of the five words <math alttext="w Subscript
    t minus 2 Baseline comma ellipsis comma w Subscript t plus 2 Baseline"><mrow><msub><mi>w</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>2</mn></mrow></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <msub><mi>w</mi> <mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub></mrow></math>
    . One such training sample is printed in bold: **... is trying *things* to see
    ...**. In the CBOW architecture (left), the model is trained to predict the target
    words from their context words. Here, a training sample consists of the sum or
    average of the one-hot encoded vectors of the context words and the target word
    as the label. In contrast, the skip-gram model (right) is trained to predict the
    context words given the target word. In this case, each target word generates
    a separate training sample for each context word; there is no vector averaging.
    Thus, skip-gram trains slower (much slower for large window sizes!) but often
    gives better results for infrequent words.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种算法都在文本上使用一个滑动窗口，由目标词 <math alttext="w Subscript t"><msub><mi>w</mi> <mi>t</mi></msub></math>
    和上下文窗口大小 <math alttext="c"><mi>c</mi></math> 定义。在这个例子中，<math alttext="c equals
    2"><mrow><mi>c</mi> <mo>=</mo> <mn>2</mn></mrow></math> ，即训练样本由五个词组成 <math alttext="w
    Subscript t minus 2 Baseline comma ellipsis comma w Subscript t plus 2 Baseline"><mrow><msub><mi>w</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>2</mn></mrow></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <msub><mi>w</mi> <mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub></mrow></math>
    。其中一种训练样本以粗体显示：**... is trying *things* to see ...**。在CBOW架构（左侧），模型被训练来预测从上下文词到目标词。这里，一个训练样本由上下文词的独热编码向量的总和或平均值以及目标词作为标签。相比之下，skip-gram模型（右侧）被训练来预测给定目标词的上下文词。在这种情况下，每个目标词为每个上下文词生成一个单独的训练样本；没有向量平均。因此，skip-gram训练速度较慢（对于大窗口大小来说要慢得多！），但通常能够更好地处理不常见的词语。
- en: Both embedding algorithms use a simple single-layer neural network and some
    tricks for fast and scalable training. The learned embeddings are actually defined
    by the weight matrix of the hidden layer. Thus, if you want to learn 100-dimensional
    vector representations, the hidden layer has to consist of 100 neurons. The input
    and output words are represented by one-hot vectors. The dimensionality of the
    embeddings and size of the context window <math alttext="c"><mi>c</mi></math>
    are hyperparameters in all of the embedding methods presented here. We will explore
    their impact on the embeddings later in this chapter.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种嵌入算法都使用了一个简单的单层神经网络和一些技巧来进行快速和可扩展的训练。学习到的嵌入实际上是由隐藏层的权重矩阵定义的。因此，如果你想学习100维的向量表示，隐藏层必须由100个神经元组成。输入和输出的词语都由独热向量表示。嵌入的维度和上下文窗口的大小
    <math alttext="c"><mi>c</mi></math> 都是所有这里介绍的嵌入方法中的超参数。我们将在本章后面探讨它们对嵌入的影响。
- en: GloVe
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GloVe
- en: The [GloVe (global vectors) approach](https://oreil.ly/7hIGW), developed in
    2014 by Stanford’s NLP group, uses a global co-occurrence matrix to compute word
    vectors instead of a prediction task (Pennington et al., 2014). A co-occurrence
    matrix for a vocabulary of size <math alttext="upper V"><mi>V</mi></math> has
    the dimensionality <math alttext="upper V times upper V"><mrow><mi>V</mi> <mo>×</mo>
    <mi>V</mi></mrow></math> . Each cell <math alttext="left-parenthesis i comma j
    right-parenthesis"><mrow><mo>(</mo> <mi>i</mi> <mo>,</mo> <mi>j</mi> <mo>)</mo></mrow></math>
    in the matrix contains the number of co-occurrences of the words <math alttext="w
    Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math> and <math alttext="w Subscript
    j"><msub><mi>w</mi> <mi>j</mi></msub></math> based again on a fixed context window
    size. The embeddings are derived using a matrix factorization technique similar
    to those used for topic modeling or dimensionality reduction.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[全局向量（GloVe）方法](https://oreil.ly/7hIGW)，由斯坦福自然语言处理组在2014年开发，使用全局共现矩阵来计算词向量，而不是一个预测任务（Pennington等，2014年）。一个大小为
    <math alttext="upper V"><mi>V</mi></math> 的词汇的共现矩阵具有维度 <math alttext="upper V
    times upper V"><mrow><mi>V</mi> <mo>×</mo> <mi>V</mi></mrow></math> 。矩阵中的每个单元
    <math alttext="left-parenthesis i comma j right-parenthesis"><mrow><mo>(</mo>
    <mi>i</mi> <mo>,</mo> <mi>j</mi> <mo>)</mo></mrow></math> 包含基于固定上下文窗口大小的词 <math
    alttext="w Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math> 和 <math alttext="w
    Subscript j"><msub><mi>w</mi> <mi>j</mi></msub></math> 的共现次数。这些嵌入是通过类似于主题建模或降维技术中使用的矩阵分解技术来推导的。'
- en: The model is called *global* because the co-occurrence matrix captures global
    corpus statistics in contrast to Word2Vec, which uses only the local context window
    for its prediction task. GloVe does not generally perform better than Word2Vec
    but produces similarly good results with some differences depending on the training
    data and the task (see Levy et al., 2014, for a discussion).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型被称为*全局*，因为共现矩阵捕获全局语料库统计，与只使用局部上下文窗口进行预测任务的Word2Vec形成对比。 GloVe通常不比Word2Vec表现更好，但根据训练数据和任务的不同，它产生类似的好结果（参见Levy等人，2014年，进行讨论）。
- en: FastText
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FastText
- en: The third model we introduce was developed again by a team with Tomáš Mikolov,
    this time at Facebook (Joulin et al., 2017). The main motivation was to handle
    out-of-vocabulary words. Both Word2Vec and GloVe produce word embeddings only
    for words contained in the training corpus. [FastText](https://fasttext.cc), in
    contrast, uses subword information in the form of character n-grams to derive
    vector representations. The character trigrams for *fasttext* are, for example,
    *fas*, *ast*, *stt*, *tte*, *tex*, and *ext*. The lengths of n-grams used (minimum
    and maximum) are hyperparameters of the model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍的第三种模型再次由一支由**Tomáš Mikolov**领导的团队在Facebook开发（Joulin等人，2017年）。 主要动机是处理词汇外的词汇。
    无论是Word2Vec还是GloVe，都仅为训练语料库中包含的词汇生成词嵌入。 相比之下，[FastText](https://fasttext.cc)利用字符n-gram的子词信息来推导向量表示。
    例如，*fasttext*的字符三元组是*fas*，*ast*，*stt*，*tte*，*tex*和*ext*。 使用的n-gram长度（最小和最大）是模型的超参数。
- en: Any word vector is built from the embeddings of its character n-grams. And that
    does work even for words previously unseen by the model because most of the character
    n-grams have embeddings. For example, the vector for *fasttext* will be similar
    to *fast* and *text* because of the common n-grams. Thus, FastText is pretty good
    at finding embeddings for misspelled words that are usually out-of-vocabulary.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 任何单词向量都是从其字符n-grams的嵌入构建的。 并且即使是模型以前未见过的单词，大多数字符n-gram也有嵌入。 例如，*fasttext*的向量将类似于*fast*和*text*，因为它们有共同的n-grams。
    因此，FastText非常擅长为通常是词汇外的拼写错误的单词找到嵌入。
- en: Deep contextualized embeddings
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度上下文化的嵌入
- en: The semantic meaning of a word often depends on its context. Think of different
    meanings of the word *right* in “I am right” and “Please turn right.”^([4](ch10.xhtml#idm45634181941800))
    All three models (Word2Vec, GloVe, and FastText) have just one vector representation
    per word; they cannot distinguish between context-dependent semantics.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 单词的语义含义往往取决于其上下文。 想想“我是对的”和“请右转”中*right*一词的不同含义^（[4](ch10.xhtml#idm45634181941800)）。
    所有这三种模型（Word2Vec，GloVe和FastText）每个单词仅有一个向量表示；它们无法区分依赖上下文的语义。
- en: 'Contextualized embeddings like *Embedding from Language Models* (ELMo) take
    the context, i.e., the preceding and following words, into account (Peters et
    al., 2018). There is not one word vector stored for each word that can simply
    be looked up. Instead, ELMo passes the whole sentence through a multilayer bidirectional
    long short-term memory neural network (LSTM) and assembles the vectors for each
    word from weights of the internal layers. Recent models such as BERT and its successors
    improve the approach by using attention transformers instead of bidirectional
    LSTMs. The main benefit of all these models is transfer learning: the ability
    to use a pretrained language model and fine-tune it for specific downstream tasks
    such as classification or question answering. We will cover this concept in more
    detail in [Chapter 11](ch11.xhtml#ch-sentiment).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 类似*来自语言模型的嵌入*（ELMo）的上下文化嵌入考虑上下文，即前后的单词（Peters等人，2018年）。 没有为每个单词存储一个可以简单查找的单词向量。
    相反，ELMo通过多层双向长短期记忆神经网络（LSTM）传递整个句子，并从内部层的权重组合每个单词的向量。 最近的模型如BERT及其后继模型通过使用注意力变换器而不是双向LSTM改进了这种方法。
    所有这些模型的主要优点是迁移学习：能够使用预训练的语言模型并针对特定的下游任务（如分类或问题回答）进行微调。 我们将在[第11章](ch11.xhtml#ch-sentiment)中更详细地介绍这个概念。
- en: 'Blueprint: Using Similarity Queries on Pretrained Models'
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：在预训练模型上使用相似性查询
- en: After all this theory, let’s start some practice. For our first examples, we
    use pretrained embeddings. These have the advantage that somebody else spent the
    training effort, usually on a large corpus like Wikipedia or news articles. In
    our blueprint, we will check out available models, load one of them, and do some
    reasoning with word vectors.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些理论之后，让我们开始一些实践。在我们的第一个例子中，我们使用预训练的嵌入。这些具有优势，即其他人已经在大型语料库（如维基百科或新闻文章）上花费了训练工作。在我们的蓝图中，我们将检查可用的模型，加载其中一个，并对单词向量进行推理。
- en: Loading a Pretrained Model
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载预训练模型
- en: Several models are publicly available for download.^([5](ch10.xhtml#idm45634181919512))
    We will describe later how to load custom models, but here we will use Gensim’s
    convenient downloader API instead.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 几个模型可以公开下载。^([5](ch10.xhtml#idm45634181919512)) 我们稍后会描述如何加载自定义模型，但在这里，我们将使用
    Gensim 的方便下载 API。
- en: 'Per the default, Gensim stores models under `~/gensim-data`. If you want to
    change this to a custom path, you can set the environment variable `GENSIM_DATA_DIR`
    before importing the downloader API. We will store all models in the local directory
    `models`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 根据默认设置，Gensim 将模型存储在 `~/gensim-data` 下。如果您想将其更改为自定义路径，可以在导入下载器 API 之前设置环境变量
    `GENSIM_DATA_DIR`。我们将所有模型存储在本地目录 `models` 中：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now let’s take a look at the available models. The following lines transform
    the dictionary returned by `api.info()[''models'']` into a `DataFrame` to get
    a nicely formatted list and show the first five of a total of 13 entries:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看可用的模型。以下行将由 `api.info()['models']` 返回的字典转换为 `DataFrame`，以获得格式良好的列表，并显示总共
    13 个条目中的前五个：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '|  | file_size | base_dataset | parameters |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | file_size | base_dataset | parameters |'
- en: '| --- | --- | --- | --- |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| fasttext-wiki-news-subwords-300 | 1005007116 | Wikipedia 2017, UMBC webbase
    corpus and statmt.org news dataset (16B tokens) | {''dimension’: 300} |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| fasttext-wiki-news-subwords-300 | 1005007116 | Wikipedia 2017, UMBC webbase
    corpus and statmt.org news dataset (16B tokens) | {''dimension’: 300} |'
- en: '| conceptnet-numberbatch-17-06-300 | 1225497562 | ConceptNet, word2vec, GloVe,
    and OpenSubtitles 2016 | {''dimension’: 300} |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| conceptnet-numberbatch-17-06-300 | 1225497562 | ConceptNet, word2vec, GloVe,
    and OpenSubtitles 2016 | {''dimension’: 300} |'
- en: '| word2vec-ruscorpora-300 | 208427381 | Russian National Corpus (about 250M
    words) | {''dimension’: 300, ‘window_size’: 10} |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| word2vec-ruscorpora-300 | 208427381 | Russian National Corpus (about 250M
    words) | {''dimension’: 300, ‘window_size’: 10} |'
- en: '| word2vec-google-news-300 | 1743563840 | Google News (about 100 billion words)
    | {''dimension’: 300} |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| word2vec-google-news-300 | 1743563840 | Google News (about 100 billion words)
    | {''dimension’: 300} |'
- en: '| glove-wiki-gigaword-50 | 69182535 | Wikipedia 2014 + Gigaword 5 (6B tokens,
    uncased) | {''dimension’: 50} |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| glove-wiki-gigaword-50 | 69182535 | Wikipedia 2014 + Gigaword 5（6B tokens,
    uncased）| {''dimension’: 50} |'
- en: 'We will use the *glove-wiki-gigaword-50* model. This model with 50-dimensional
    word vectors is small in size but still quite comprehensive and fully sufficient
    for our purposes. It was trained on roughly 6 billion lowercased tokens. `api.load`
    downloads the model if required and then loads it into memory:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 *glove-wiki-gigaword-50* 模型。这个具有 50 维单词向量的模型体积较小，但对我们的目的来说完全足够。它在大约 60
    亿个小写标记上进行了训练。`api.load` 如果需要会下载模型，然后将其加载到内存中：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The file we downloaded actually does not contain a full GloVe model but just
    the plain word vectors. As the internal states of the model are not included,
    such reduced models cannot be trained further.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下载的文件实际上并不包含完整的 GloVe 模型，而只包含纯粹的词向量。由于未包含模型的内部状态，这种简化模型无法进一步训练。
- en: Similarity Queries
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相似性查询
- en: Given a model, the vector for a single word like *king* can be accessed simply
    via the property `model.wv['king']` or even more simply by the shortcut `model['king']`.
    Let’s take a look at the first 10 components of the 50-dimensional vectors for
    *king* and *queen*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个模型，可以通过属性 `model.wv['king']` 或甚至更简单地通过快捷方式 `model['king']` 访问单词 *king* 的向量。让我们看看
    *king* 和 *queen* 的 50 维向量的前 10 个分量。
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`Out:`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As expected, the values are similar in many dimensions, resulting in a high
    similarity score of over 0.78\. So *queen* is quite similar to *king*, but is
    it the most similar word? Well, let’s check the three words most similar to *king*
    with a call to the respective function:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，在许多维度上的值是相似的，导致高达 0.78 的高相似性分数。因此，*queen* 与 *king* 相当相似，但它是最相似的词吗？好的，让我们通过调用相应的函数来检查与
    *king* 最相似的三个词：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`Out:`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In fact, the male *prince* is more similar than *queen*, but *queen* is second
    in the list, followed by the roman numeral II, because many kings have been named
    “the second.”
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，男性的*prince*比*queen*更相似，但*queen*在列表中排名第二，其后是罗马数字II，因为许多国王被称为“第二”。
- en: 'Similarity scores on word vectors are generally computed by cosine similarity,
    which was introduced in [Chapter 5](ch05.xhtml#ch-vectorization). Gensim provides
    several variants of similarity functions. For example, the `cosine_similarities`
    method computes the similarity between a word vector and an array of other word
    vectors. Let’s compare *king* to some more words:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 单词向量的相似性分数通常通过余弦相似度计算，这在[第5章](ch05.xhtml#ch-vectorization)中介绍过。Gensim提供了几种变体的相似性函数。例如，`cosine_similarities`方法计算单词向量与其他单词向量数组之间的相似度。让我们比较*king*与更多单词：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`Out:`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Based on the training data for the model (Wikipedia and Gigaword), the model
    assumes the word *king* to be similar to *queen*, still a little similar to *lion*,
    but not at all similar to *nanotechnology*. Note, that in contrast to nonnegative
    TF-IDF vectors, word embeddings can also be negative in some dimensions. Thus,
    the similarity values range from <math alttext="plus 1"><mrow><mo>+</mo> <mn>1</mn></mrow></math>
    to <math alttext="negative 1"><mrow><mo>-</mo> <mn>1</mn></mrow></math> .
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的训练数据（维基百科和Gigaword），模型假设单词*king*与*queen*相似，与*lion*略有相似，但与*nanotechnology*完全不相似。需要注意的是，与非负TF-IDF向量不同，单词嵌入在某些维度上也可能是负的。因此，相似度值范围从<math
    alttext="plus 1"><mrow><mo>+</mo> <mn>1</mn></mrow></math>到<math alttext="negative
    1"><mrow><mo>-</mo> <mn>1</mn></mrow></math>不等。
- en: The function `most_similar()` used earlier allows also two parameters, `positive`
    and `negative`, each a list of vectors. If <math alttext="p o s i t i v e equals
    left-bracket p o s 1 comma ellipsis comma p o s Subscript n Baseline right-bracket"><mrow><mi>p</mi>
    <mi>o</mi> <mi>s</mi> <mi>i</mi> <mi>t</mi> <mi>i</mi> <mi>v</mi> <mi>e</mi> <mo>=</mo>
    <mo>[</mo> <mi>p</mi> <mi>o</mi> <msub><mi>s</mi> <mn>1</mn></msub> <mo>,</mo>
    <mo>⋯</mo> <mo>,</mo> <mi>p</mi> <mi>o</mi> <msub><mi>s</mi> <mi>n</mi></msub>
    <mo>]</mo></mrow></math> and <math alttext="n e g a t i v e equals left-bracket
    n e g 1 comma ellipsis comma n e g Subscript m Baseline right-bracket"><mrow><mi>n</mi>
    <mi>e</mi> <mi>g</mi> <mi>a</mi> <mi>t</mi> <mi>i</mi> <mi>v</mi> <mi>e</mi> <mo>=</mo>
    <mo>[</mo> <mi>n</mi> <mi>e</mi> <msub><mi>g</mi> <mn>1</mn></msub> <mo>,</mo>
    <mo>⋯</mo> <mo>,</mo> <mi>n</mi> <mi>e</mi> <msub><mi>g</mi> <mi>m</mi></msub>
    <mo>]</mo></mrow></math> , then this function finds the word vectors most similar
    to <math alttext="sigma-summation Underscript i equals 1 Overscript n Endscripts
    p o s Subscript i minus sigma-summation Underscript j equals 1 Overscript m Endscripts
    n e g Subscript j"><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <mi>p</mi> <mi>o</mi> <msub><mi>s</mi> <mi>i</mi></msub>
    <mo>-</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></msubsup>
    <mi>n</mi> <mi>e</mi> <msub><mi>g</mi> <mi>j</mi></msub></mrow></math> .
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 先前使用的`most_similar()`函数还允许两个参数，`positive`和`negative`，每个参数都是向量列表。如果<math alttext="p
    o s i t i v e equals left-bracket p o s 1 comma ellipsis comma p o s Subscript
    n Baseline right-bracket"><mrow><mi>p</mi> <mi>o</mi> <mi>s</mi> <mi>i</mi> <mi>t</mi>
    <mi>i</mi> <mi>v</mi> <mi>e</mi> <mo>=</mo> <mo>[</mo> <mi>p</mi> <mi>o</mi> <msub><mi>s</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <mi>p</mi> <mi>o</mi> <msub><mi>s</mi>
    <mi>n</mi></msub> <mo>]</mo></mrow></math>和<math alttext="n e g a t i v e equals
    left-bracket n e g 1 comma ellipsis comma n e g Subscript m Baseline right-bracket"><mrow><mi>n</mi>
    <mi>e</mi> <mi>g</mi> <mi>a</mi> <mi>t</mi> <mi>i</mi> <mi>v</mi> <mi>e</mi> <mo>=</mo>
    <mo>[</mo> <mi>n</mi> <mi>e</mi> <msub><mi>g</mi> <mn>1</mn></msub> <mo>,</mo>
    <mo>⋯</mo> <mo>,</mo> <mi>n</mi> <mi>e</mi> <msub><mi>g</mi> <mi>m</mi></msub>
    <mo>]</mo></mrow></math>，那么此函数将找到与<math alttext="sigma-summation Underscript i
    equals 1 Overscript n Endscripts p o s Subscript i minus sigma-summation Underscript
    j equals 1 Overscript m Endscripts n e g Subscript j"><mrow><msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <mi>p</mi> <mi>o</mi>
    <msub><mi>s</mi> <mi>i</mi></msub> <mo>-</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <mi>n</mi> <mi>e</mi> <msub><mi>g</mi> <mi>j</mi></msub></mrow></math>最相似的单词向量。
- en: 'Thus, we can formulate our analogy query about the royals in Gensim this way:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以用Gensim来制定关于皇室的类比查询：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`Out:`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And the question for the German capital:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以及关于德国首都的问题：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`Out:`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can also leave out the negative list to find the word closest to the sum
    of *france* and *capital*:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以省略负面列表，以找到与*france*和*capital*之和最接近的单词：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`Out:`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'It is indeed `paris`! That’s really amazing and shows the great power of word
    vectors. However, as always in machine learning, the models are not perfect. They
    can learn only what’s in the data. Thus, by far not all similarity queries yield
    such staggering results, as the following example demonstrates:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，它就是`paris`！这真是令人惊叹，显示了词向量的巨大威力。然而，正如在机器学习中一样，模型并不完美。它们只能学习到数据中存在的内容。因此，并非所有相似性查询都会产生如此惊人的结果，下面的例子就说明了这一点：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`Out:`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Obviously, there has not been enough training data for the model to derive the
    relation between Athens and Greece.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，模型没有足够的训练数据来推导雅典和希腊之间的关系。
- en: Note
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Gensim also offers a variant of cosine similarity, `most_similar_cosmul`. This
    is supposed to work better for analogy queries than the one shown earlier because
    it smooths the effects of one large similarity term dominating the equation (Levy
    et al., 2015). For the previous examples, however, the returned words would be
    the same, but the similarity scores would be higher.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim还提供了余弦相似度的一种变体，`most_similar_cosmul`。这对于类比查询比前面显示的方法更有效，因为它平滑了一个大相似性项主导方程的效果（Levy等，2015）。然而，对于前面的例子，返回的单词将是相同的，但相似性分数将更高。
- en: If you train embeddings with redacted texts from Wikipedia and news articles,
    your model will be able to capture factual relations like capital-country quite
    well. But what about the market research question comparing products of different
    brands? Usually you won’t find this information on Wikipedia but rather on up-to-date
    social platforms where people discuss products. If you train embeddings on user
    comments from a social platform, your model will learn word associations from
    user discussions. This way, it becomes a representation of what people *think*
    about a relationship, independent of whether this is objectively true. This is
    an interesting side effect you should be aware of. Often you want to capture exactly
    this application specific bias, and this is what we are going to do next. But
    be aware that every training corpus contains some bias, which may also lead to
    unwanted side effects (see [“Man Is to Computer Programmer as Woman Is to Homemaker”](#man_computer)).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用来自维基百科和新闻文章的编辑文本来训练嵌入，您的模型将能够很好地捕捉到类似首都-国家的事实关系。但是，对于市场研究问题，比较不同品牌产品的情况呢？通常这些信息在维基百科上找不到，而是在最新的社交平台上，人们在讨论产品。如果您在社交平台上使用用户评论来训练嵌入，您的模型将学习到来自用户讨论的词语关联。这样，它就成为了人们对关系的*认知*表示，独立于其是否客观真实。这是一个有趣的副作用，您应该意识到。通常，您希望捕捉到这种特定应用的偏见，这也是我们接下来要做的事情。但是请注意，每个训练语料库都包含一定的偏见，这可能还会导致一些不希望的副作用（参见[“男人对计算机程序员如同女人对家庭主妇”](#man_computer)）。
- en: Blueprints for Training and Evaluating Your Own Embeddings
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和评估自己嵌入的蓝图
- en: In this section, we will train and evaluate domain-specific embeddings on 20,000
    user posts on autos from the Reddit Selfposts dataset. Before we start training,
    we have to consider the options for data preparation as they always have a significant
    impact on the usefulness of a model for a specific task.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在Reddit Selfposts数据集中的2万个关于汽车的用户帖子上训练和评估特定领域的嵌入。在开始训练之前，我们必须考虑数据准备的选项，因为这总是对模型在特定任务中的实用性产生重要影响的因素。
- en: Data Preparation
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'Gensim requires sequences of tokens as input for the training. Besides tokenization
    there are some other aspects to consider for data preparation. Based on the distributional
    hypothesis, words frequently appearing together or in similar context will get
    similar vectors. Thus, we should make sure that co-occurrences are actually identified
    as such. If you do not have very many training sentences, as in our example here,
    you should include these steps in your preprocessing:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim要求输入训练的令牌序列。除了分词之外，还有一些其他方面需要考虑数据准备。根据分布假设，经常一起出现或在相似上下文中的单词将获得相似的向量。因此，我们应确保确实识别了这些共现关系。如果像我们这里的示例一样训练句子不多，您应在预处理中包括这些步骤：
- en: Clean text from unwanted tokens (symbols, tags, etc.).
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清理文本，去除不需要的标记（符号、标签等）。
- en: Put all words into lowercase.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有单词转换为小写。
- en: Use lemmas.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用引理。
- en: 'All this keeps the vocabulary small and training times short. Of course, inflected
    and uppercase words will be out-of-vocabulary if we prune our training data according
    to these rules. This is not a problem for semantic reasoning on nouns as we want
    to do, but it could be, if we wanted to analyze, for example, emotions. In addition,
    you should consider these token categories:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都使得词汇量保持较小，训练时间较短。当然，如果根据这些规则修剪我们的训练数据，屈折形式和大写字词将会是词汇外的情况。对于我们想要进行的名词语义推理来说，这不是问题，但如果我们想要分析例如情感，这可能会成为问题。此外，您应考虑以下标记类别：
- en: Stop words
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词
- en: Stop words can carry valuable information about the context of non-stop words.
    Thus, we prefer to keep the stop words.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词可以提供有关非停用词语境的宝贵信息。因此，我们更倾向于保留停用词。
- en: Numbers
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 数字
- en: Depending on the application, numbers can be valuable or just noise. In our
    example, we are looking at auto data and definitely want to keep tokens like `328`
    because it’s a BMW model name. You should keep numbers if they carry relevant
    information.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 根据应用程序的不同，数字可能是有价值的，也可能只是噪音。在我们的例子中，我们正在查看汽车数据，并且肯定希望保留像`328`这样的标记，因为它是宝马车型的名称。如果数字携带相关信息，则应保留这些数字。
- en: Another question is whether we should split on sentences or just keep the posts
    as they are. Consider the imaginary post “I like the BMW 328\. But the Mercedes
    C300 is also great.” Should these sentences be treated like two different posts
    for our similarity task? Probably not. Thus, we will treat the list of all lemmas
    in one user post as a single “sentence” for training.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是我们是否应该按句子拆分，还是仅保留帖子的原样。考虑虚构帖子“I like the BMW 328\. But the Mercedes C300
    is also great.”这两个句子在我们的相似性任务中应该被视为两个不同的帖子吗？可能不应该。因此，我们将所有用户帖子中的所有词形的列表视为一个单独的“句子”用于训练。
- en: 'We already prepared the lemmas for the 20,000 Reddit posts on autos in [Chapter 4](ch04.xhtml#ch-preparation).
    Therefore, we can skip that part of data preparation here and just load the lemmas
    into a Pandas `DataFrame`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为[第四章](ch04.xhtml#ch-preparation)中的2万条Reddit汽车帖子准备了词形。因此，在这里我们可以跳过数据准备的这一部分，直接将词形加载到Pandas的`DataFrame`中：
- en: '[PRE17]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Phrases
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 短语
- en: Especially in English, the meaning of a word may change if the word is part
    of a compound phrase. Take, for example, *timing belt*, *seat belt*, or *rust
    belt*. All of these compounds have different meanings, even though all of them
    can be found in our corpus. So, it may better to treat such compounds as single
    tokens.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是在英语中，如果一个词是复合短语的一部分，那么该词的含义可能会发生变化。例如，*timing belt*，*seat belt*或*rust belt*。所有这些复合词虽然都可以在我们的语料库中找到，但它们的含义各不相同。因此，将这些复合词视为单个标记可能更为合适。
- en: We can use any algorithm to detect such phrases, for example, spaCy’s detection
    of noun chunks (see [“Linguistic Processing with spaCy”](ch04.xhtml#ch4-spacy)).
    A number of statistical algorithms also exist to identify such collocations, such
    as extraordinary frequent n-grams. The original Word2Vec paper (Mikolov et al.,
    2013) uses a simple but effective algorithm based on *pointwise mutual information*
    (PMI), which basically measures the statistical dependence between the occurrences
    of two words.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用任何算法来检测这些短语，例如spaCy检测名词块（见[“使用spaCy进行语言处理”](ch04.xhtml#ch4-spacy)）。还有许多统计算法可用于识别这样的搭配，如异常频繁的n-gram。原始的Word2Vec论文（Mikolov等人，2013）使用了一种简单但有效的基于*点间互信息*（PMI）的算法，基本上衡量了两个词出现之间的统计依赖性。
- en: 'For the model that we are now training, we use an advanced version called *normalized
    pointwise mutual information* (NPMI) because it gives more robust results. And
    given its limited value range from <math alttext="negative 1"><mrow><mo>-</mo>
    <mn>1</mn></mrow></math> to <math alttext="plus 1"><mrow><mo>+</mo> <mn>1</mn></mrow></math>
    , it is also easier to tune. The NPMI threshold in our initial run is set to a
    rather low value of 0.3\. We choose a hyphen as a delimiter to connect the words
    in a phrase. This generates compound tokens like *harley-davidson*, which will
    be found in the text anyway. The default underscore delimiter would result in
    a different token:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们现在正在训练的模型，我们使用了一个高级版本，称为*归一化点间互信息*（NPMI），因为它能提供更稳健的结果。鉴于其值范围有限，从<math alttext="negative
    1"><mrow><mo>-</mo> <mn>1</mn></mrow></math>到<math alttext="plus 1"><mrow><mo>+</mo>
    <mn>1</mn></mrow></math>，它也更容易调整。我们在初始运行中将NPMI阈值设定为一个相当低的值，即0.3\. 我们选择使用连字符作为短语中单词的分隔符。这将生成类似*harley-davidson*的复合标记，无论如何这些标记都会在文本中找到。如果使用默认的下划线分隔符，则会产生不同的标记：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'With this phrase model we can identify some interesting compound words:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种短语模型，我们可以识别一些有趣的复合词：
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`Out:`'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE20]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '*timing-belt* is good, but we do not want to build compounds for combinations
    of brands and model names, like *mercedes c300*. Thus, we will analyze the phrase
    model to find a good threshold. Obviously, the chosen value was too low. The following
    code exports all phrases found in our corpus together with their scores and converts
    the result to a `DataFrame` for easy inspection:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*timing-belt*很好，但我们不希望为品牌和型号名称的组合构建复合词，比如*奔驰 c300*。因此，我们将分析短语模型，找到一个合适的阈值。显然，选择的值太低了。以下代码导出我们语料库中找到的所有短语及其分数，并将结果转换为`DataFrame`以便轻松检查：'
- en: '[PRE21]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now we can check what would be a good threshold for *mercedes*:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以检查哪个阈值适合*奔驰*：
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '|  | phrase | score |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | 短语 | 分数 |'
- en: '| --- | --- | --- |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 83 | mercedes benz | 0.80 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 83 | 奔驰 | 0.80 |'
- en: '| 1417 | mercedes c300 | 0.47 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 1417 | 奔驰 c300 | 0.47 |'
- en: As we can see, it should be larger than 0.5 and less than 0.8\. Checking with
    a few other brands like *bmw*, *ford*, or *harley davidson* lets us identify 0.7
    as a good threshold to identify compound vendor names but keep brands and models
    separate. In fact, with the rather stringent threshold of 0.7, the phrase model
    still keeps many relevant word combinations, for example, *street glide* (Harley-Davidson),
    *land cruiser* (Toyota), *forester xt* (Subaru), *water pump*, *spark plug*, or
    *timing belt*.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，阈值应该大于0.5且小于0.8。通过检查*宝马*、*福特*或*哈雷戴维森*等几个其他品牌，我们确定0.7是一个很好的阈值，可以识别复合供应商名称，但保持品牌和型号分开。实际上，即使是0.7这样严格的阈值，短语模型仍然保留了许多相关的词组，例如*street
    glide*（哈雷戴维森）、*land cruiser*（丰田）、*forester xt*（斯巴鲁）、*water pump*、*spark plug*或*timing
    belt*。
- en: 'We rebuild our phraser and create a new column in our `DataFrame` with single
    tokens for compound words:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重建了我们的短语分析器，并在我们的`DataFrame`中为复合词创建了一个新列，该列包含单词标记：
- en: '[PRE23]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The result of our data preparation steps are sentences consisting of lemmas
    and phrases. We will now train different embedding models and check which insights
    we can gain from them.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据准备步骤的结果是由词形和短语组成的句子。现在，我们将训练不同的嵌入模型，并检查我们能从中获得哪些见解。
- en: 'Blueprint: Training Models with Gensim'
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：使用Gensim训练模型
- en: 'Word2Vec and FastText embeddings can be conveniently trained by Gensim. The
    following call to `Word2Vec` trains 100-dimensional Word2Vec embeddings on the
    corpus with a window size of 2, i.e., target word ±2 context words. Some other
    relevant hyperparameters are passed as well for illustration. We use the skip-gram
    algorithm and train the network in four threads for five epochs:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Gensim可以方便地训练Word2Vec和FastText嵌入。以下调用`Word2Vec`在语料库上训练了100维的Word2Vec嵌入，窗口大小为2，即目标词的±2个上下文词。为了说明，还传递了一些其他相关超参数。我们使用skip-gram算法，并在四个线程中训练网络五次迭代：
- en: '[PRE24]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This takes about 30 seconds on an i7 laptop for the 20,000 sentences, so it
    is quite fast. More samples and more epochs, as well as longer vectors and larger
    context windows, will increase the training time. For example, training 100-dimensional
    vectors with a context window size of 30 requires about 5 minutes in this setting
    for skip-gram. The CBOW training time, in contrast, is rather independent of the
    context window size.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在i7笔记本电脑上，处理2万个句子大约需要30秒，速度相当快。增加样本数和迭代次数，以及更长的向量和更大的上下文窗口，会增加训练时间。例如，在这种设置下训练30大小的100维向量，跳跃图算法大约需要5分钟。相比之下，CBOW的训练时间与上下文窗口的大小无关。
- en: 'The following call saves the full model to disk. *Full model* means the complete
    neural network, including all internal states. This way, the model can be loaded
    again and trained further:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 以下调用将完整模型保存到磁盘。*完整模型*意味着包括所有内部状态的完整神经网络。这样，模型可以再次加载并进一步训练：
- en: '[PRE25]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The choice of the algorithm as well as those hyperparameters have quite an
    impact on the resulting models. Therefore, we provide a blueprint to train and
    inspect different models. A parameter grid defines which algorithm variant (CBOW
    or skip-gram) and window sizes will be trained for Word2Vec or FastText. We could
    also vary vector size here, but that parameter does not have such a big impact.
    In our experience, 50- or 100-dimensional vectors work well on smaller corpora.
    So, we fix the vector size to 100 in our experiments:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的选择以及这些超参数对生成的模型影响很大。因此，我们提供了一个训练和检查不同模型的蓝图。参数网格定义了将为Word2Vec或FastText训练哪些算法变体（CBOW或skip-gram）和窗口大小。我们也可以在这里变化向量大小，但这个参数的影响不是很大。根据我们的经验，在较小的语料库中，50或100维的向量效果很好。因此，我们在实验中将向量大小固定为100：
- en: '[PRE26]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As we just want to analyze the similarities within our corpus, we do not save
    the complete models here but just the plain word vectors. These are represented
    by the class `KeyedVectors` and can be accessed by the model property `model.wv`.
    This generates much smaller files and is fully sufficient for our purpose.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只想分析语料库内的相似性，我们不保存完整的模型，而是仅保存纯单词向量。这些由`KeyedVectors`类表示，并且可以通过模型属性`model.wv`访问。这样生成的文件更小，并且完全足够我们的目的。
- en: Warning
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Beware of information loss! When you reload models consisting only of the word
    vectors, they cannot be trained further. Moreover, FastText models lose the ability
    to derive embeddings for out-of-vocabulary words.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要注意信息丢失！当您重新加载仅由单词向量组成的模型时，它们无法进一步训练。此外，FastText 模型失去了为超出词汇表单词推导嵌入的能力。
- en: 'Blueprint: Evaluating Different Models'
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：评估不同的模型
- en: Actually, it is quite hard to algorithmically identify the best hyperparameters
    for a domain-specific task and corpus. Thus, it is not a bad idea to inspect the
    models manually and check how they perform to identify some already-known relationships.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，对于特定领域任务和语料库，算法化地确定最佳超参数是相当困难的。因此，检查模型的表现并手动验证它们如何执行以识别一些已知的关系并非坏主意。
- en: 'The saved files containing only the word vectors are small (about 5 MB each),
    so we can load many of them into memory and run some comparisons. We use a subset
    of five models to illustrate our findings. The models are stored in a dictionary
    indexed by the model name. You could add any models you’d like to compare, even
    the pretrained GloVe model from earlier:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 仅包含单词向量的保存文件很小（每个约 5 MB），因此我们可以将许多文件加载到内存中并运行一些比较。我们使用五个模型的子集来说明我们的发现。这些模型存储在一个由模型名称索引的字典中。您可以添加任何您想比较的模型，甚至是早期预训练的
    GloVe 模型：
- en: '[PRE27]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We provide a small blueprint function for the comparison. It takes a list of
    models and a word and produces a `DataFrame` with the most similar words according
    to each model:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了一个小的蓝图函数用于比较。它接受一个模型列表和一个单词，并生成一个`DataFrame`，其中包含根据每个模型最相似的单词：
- en: '[PRE28]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now let’s see what effect the parameters have on our computed models. As we
    are going to analyze the car market, we check out the words most similar to *bmw*:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看参数对我们计算的模型有什么影响。因为我们要分析汽车市场，我们查看与*宝马*最相似的单词：
- en: '[PRE29]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '|  | autos_w2v_cbow_2 | autos_w2v_sg_2 | autos_w2v_sg_5 | autos_w2v_sg_30 |
    autos_ft_sg_5 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | autos_w2v_cbow_2 | autos_w2v_sg_2 | autos_w2v_sg_5 | autos_w2v_sg_30 |
    autos_ft_sg_5 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 1 | mercedes 0.873 | mercedes 0.772 | mercedes 0.808 | xdrive 0.803 | bmws
    0.819 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 梅赛德斯 0.873 | 奔驰 0.772 | 奔驰 0.808 | xdrive 0.803 | 宝马 0.819 |'
- en: '| 2 | lexus 0.851 | benz 0.710 | 335i 0.740 | 328i 0.797 | bmwfs 0.789 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 莱克萨斯 0.851 | 奔驰 0.710 | 335i 0.740 | 328i 0.797 | bmwfs 0.789 |'
- en: '| 3 | vw 0.807 | porsche 0.705 | 328i 0.736 | f10 0.762 | m135i 0.774 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 大众 0.807 | 保时捷 0.705 | 328i 0.736 | f10 0.762 | m135i 0.774 |'
- en: '| 4 | benz 0.806 | lexus 0.704 | benz 0.723 | 335i 0.760 | 335i 0.773 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 奔驰 0.806 | 莱克萨斯 0.704 | 奔驰 0.723 | 335i 0.760 | 335i 0.773 |'
- en: '| 5 | volvo 0.792 | merc 0.695 | x-drive 0.708 | 535i 0.755 | mercedes_benz
    0.765 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 沃尔沃 0.792 | 奔驰 0.695 | x-drive 0.708 | 535i 0.755 | 梅赛德斯-奔驰 0.765 |'
- en: '| 6 | harley 0.783 | mercede 0.693 | 135i 0.703 | bmws 0.745 | mercedes 0.760
    |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 哈雷 0.783 | 梅赛德斯 0.693 | 135i 0.703 | 宝马 0.745 | 奔驰 0.760 |'
- en: '| 7 | porsche 0.781 | mercedes-benz 0.680 | mercede 0.690 | x-drive 0.740 |
    35i 0.747 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 保时捷 0.781 | 奔驰-奔驰 0.680 | 梅赛德斯 0.690 | x-drive 0.740 | 35i 0.747 |'
- en: '| 8 | subaru 0.777 | audi 0.675 | e92 0.685 | 5-series 0.736 | merc 0.747 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 斯巴鲁 0.777 | 奥迪 0.675 | e92 0.685 | 5系列 0.736 | 奔驰 0.747 |'
- en: '| 9 | mb 0.769 | 335i 0.670 | mercedes-benz 0.680 | 550i 0.728 | 135i 0.746
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 9 | MB 0.769 | 335i 0.670 | 奔驰-奔驰 0.680 | 550i 0.728 | 135i 0.746 |'
- en: '| 10 | volkswagen 0.768 | 135i 0.662 | merc 0.679 | 435i 0.726 | 435i 0.744
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 大众 0.768 | 135i 0.662 | 奔驰 0.679 | 435i 0.726 | 435i 0.744 |'
- en: Interestingly, the first models with the small window size of 2 produce mainly
    other car brands, while the model with window size 30 produces basically lists
    of different BMW models. In fact, shorter windows emphasize paradigmatic relations,
    i.e., words that can be substituted for each other in a sentence. In our case,
    this would be brands as we are searching for words similar to *bmw*. Larger windows
    capture more syntagmatic relations, where words are similar if they frequently
    show up in the same context. Window size 5, which is the default, produced a mix
    of both. For our data, paradigmatic relations are best represented by the CBOW
    model, while syntagmatic relations require a large window size and are therefore
    better captured by the skip-gram model. The outputs of the FastText model demonstrate
    its property that similarly spelled words get similar scores.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，窗口大小为2的第一批模型主要生成其他汽车品牌，而窗口大小为30的模型基本上生成了不同BMW型号的列表。事实上，较短的窗口强调范式关系，即可以在句子中互换的词语。在我们的案例中，这将是品牌，因为我们正在寻找类似*BMW*的词语。较大的窗口捕获更多的语法关系，其中词语之间的相似性在于它们经常在相同的上下文中出现。窗口大小为5，即默认值，产生了两者的混合。对于我们的数据，CBOW模型最好地表示了范式关系，而语法关系则需要较大的窗口大小，因此更适合由skip-gram模型捕获。FastText模型的输出显示了其性质，即拼写相似的词语得到相似的分数。
- en: Looking for similar concepts
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 寻找相似概念
- en: The CBOW vectors with window size 2 are pretty precise on paradigmatic relations.
    Starting from some known terms, we can use such a model to identify the central
    terms and concepts of a domain. [Table 10-1](#tab-most-sim) shows the output of
    some similarity queries on model `autos_w2v_cbow_2`. The column `concept` was
    added by us to highlight what kind of words we would expect as output.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口大小为2的CBOW向量在范式关系上非常精确。从一些已知术语开始，我们可以使用这样的模型来识别领域的核心术语和概念。[表 10-1](#tab-most-sim)
    展示了在模型`autos_w2v_cbow_2`上进行一些相似性查询的输出。列`concept`是我们添加的，以突出我们预期的输出词语类型。
- en: Table 10-1\. Most similar neighbors for selected words using the CBOW model
    with window size 2
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 表10-1\. 使用CBOW模型和窗口大小为2查找选定词语的最相似邻居
- en: '| Word | Concept | Most Similar |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| Word | Concept | Most Similar |'
- en: '| --- | --- | --- |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| toyota | car brand | ford mercedes nissan certify dodge mb bmw lexus chevy
    honda |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| toyota | 汽车品牌 | ford mercedes nissan certify dodge mb bmw lexus chevy honda
    |'
- en: '| camry | car model | corolla f150 f-150 c63 is300 ranger 335i 535i 328i rx
    |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| camry | 汽车型号 | corolla f150 f-150 c63 is300 ranger 335i 535i 328i rx |'
- en: '| spark-plug | car part | water-pump gasket thermostat timing-belt tensioner
    throttle-body serpentine-belt radiator intake-manifold fluid |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| spark-plug | 汽车部件 | water-pump gasket thermostat timing-belt tensioner throttle-body
    serpentine-belt radiator intake-manifold fluid |'
- en: '| washington | location | oregon southwest ga ottawa san_diego valley portland
    mall chamber county |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| washington | 地点 | oregon southwest ga ottawa san_diego valley portland mall
    chamber county |'
- en: Of course, the answers are not always correct with regard to our expectations;
    they are just similar words. For example, the list for *toyota* contains not only
    car brands but also several models. In real-life projects, however, domain experts
    from the business department can easily identify the wrong terms and still find
    interesting new associations. But manual curation is definitely necessary when
    you work with word embeddings this way.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，答案并不总是符合我们的期望；它们只是类似的词语。例如，*Toyota* 的列表中不仅包含汽车品牌，还包括多种型号。然而，在实际项目中，业务部门的领域专家可以轻松识别错误的术语，仍然找到有趣的新联想。但是，在以这种方式处理词嵌入时，手动筛选绝对是必要的。
- en: Analogy reasoning on our own models
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们自己模型上的类比推理
- en: 'Now let’s find out how our different models are capable of detecting analogous
    concepts. We want to find out if Toyota has a product comparable to Ford’s F-150
    pickup truck. So our question is: What is to “toyota” as “f150” is to “ford”?
    We use our function `compare_models` from earlier and transpose the result to
    compare the results of `wv.most_similar()` for different models:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看我们的不同模型如何能够检测类似的概念。我们想要知道 Toyota 是否有一款与 Ford 的 F-150 皮卡相媲美的产品。因此，我们的问题是：“Toyota”对应于“Ford”的“F-150”的什么？我们使用之前的函数`compare_models`并对结果进行转置，以比较不同模型的`wv.most_similar()`结果：
- en: '[PRE30]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`Out:`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '|  | 1 | 2 | 3 | 4 | 5 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | 1 | 2 | 3 | 4 | 5 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| autos_w2v_cbow_2 | f-150 0.850 | 328i 0.824 | s80 0.820 | 93 0.819 | 4matic
    0.817 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| autos_w2v_cbow_2 | f-150 0.850 | 328i 0.824 | s80 0.820 | 93 0.819 | 4matic
    0.817 |'
- en: '| autos_w2v_sg_2 | f-150 0.744 | f-250 0.727 | dodge-ram 0.716 | tacoma 0.713
    | ranger 0.708 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| autos_w2v_sg_2 | f-150 0.744 | f-250 0.727 | dodge-ram 0.716 | tacoma 0.713
    | ranger 0.708 |'
- en: '| autos_w2v_sg_5 | tacoma 0.724 | tundra 0.707 | f-150 0.664 | highlander 0.644
    | 4wd 0.631 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| autos_w2v_sg_5 | tacoma 0.724 | tundra 0.707 | f-150 0.664 | highlander 0.644
    | 4wd 0.631 |'
- en: '| autos_w2v_sg_30 | 4runner 0.742 | tacoma 0.739 | 4runners 0.707 | 4wd 0.678
    | tacomas 0.658 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| autos_w2v_sg_30 | 4runner 0.742 | tacoma 0.739 | 4runners 0.707 | 4wd 0.678
    | tacomas 0.658 |'
- en: '| autos_ft_sg_5 | toyotas 0.777 | toyo 0.762 | tacoma 0.748 | tacomas 0.745
    | f150s 0.744 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| autos_ft_sg_5 | toyotas 0.777 | toyo 0.762 | tacoma 0.748 | tacomas 0.745
    | f150s 0.744 |'
- en: In reality, the Toyota Tacoma is a direct competitor to the F-150 as well as
    the Toyota Tundra. With that in mind, the skip-gram model with the window size
    5 gives the best results.^([6](ch10.xhtml#idm45634180172056)) In fact, if you
    exchange *toyota* for *gmc*, you get the *sierra*, and if you ask for *chevy*,
    you get *silverado* as the most similar to this model. All of these are competing
    full-size pickup trucks. This also works quite well for other brands and models,
    but of course it works best for those models that are heavily discussed in the
    Reddit forum.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，Toyota Tacoma直接与F-150以及Toyota Tundra竞争。考虑到这一点，窗口大小为5的跳字模型给出了最佳结果。[^6]实际上，如果你用*gmc*替换*toyota*，你会得到*sierra*，如果你要*chevy*，你会得到*silverado*作为这个模型最相似的车型。所有这些都是竞争激烈的全尺寸皮卡。对于其他品牌和车型，这也效果很好，但当然最适合那些在Reddit论坛中广泛讨论的模型。
- en: Blueprints for Visualizing Embeddings
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化嵌入的蓝图
- en: If we explore our corpus on the basis of word embeddings, as we do in this chapter,
    we are not interested in the actual similarity scores because the whole concept
    is inherently fuzzy. What we want to understand are semantic relations based on
    the concepts of closeness and similarity. Therefore, visual representations can
    be extremely helpful for the exploration of word embeddings and their relations.
    In this section, we will first visualize embeddings by using different dimensionality
    reduction techniques. After that, we will show how to visually explore the semantic
    neighborhood of given keywords. As we will see, this type of data exploration
    can reveal quite interesting relationships between domain-specific terms.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们像本章一样基于词嵌入探索我们的语料库，我们对实际相似度分数不感兴趣，因为整个概念本质上是模糊的。我们想要理解的是基于接近性和相似性概念的语义关系。因此，视觉表现对于探索词嵌入及其关系非常有帮助。在本节中，我们将首先使用不同的降维技术来可视化嵌入。之后，我们将展示如何通过视觉探索给定关键词的语义邻域。正如我们将看到的那样，这种数据探索可以揭示领域特定术语之间非常有趣的关系。
- en: 'Blueprint: Applying Dimensionality Reduction'
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用降维蓝图
- en: High-dimensional vectors can be visualized by projecting the data into two or
    three dimensions. If the projection works well, it is possible to visually detect
    clusters of related terms and get a much deeper understanding of semantic concepts
    in the corpus. We will look for clusters of related words and explore the semantic
    neighborhood of certain keywords in the model with window size 30, which favors
    syntagmatic relations. Thus, we expect to see a “BMW” cluster with BMW terms,
    a “Toyota” cluster with Toyota terms, and so on.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 高维向量可以通过将数据投影到二维或三维来进行可视化。如果投影效果良好，可以直观地检测到相关术语的聚类，并更深入地理解语料库中的语义概念。我们将寻找相关词汇的聚类，并使用窗口大小为30的模型探索某些关键词的语义邻域，这有利于同位语关系。因此，我们期望看到一个“BMW”词汇组，包含BMW相关术语，一个“Toyota”词汇组，包含Toyota相关术语，等等。
- en: Dimensionality reduction also has many use cases in the area of machine learning.
    Some learning algorithms have problems with high-dimensional and often sparse
    data. Dimensionality reduction techniques such as PCA, t-SNE, or UMAP (see [“Dimensionality
    Reduction Techniques”](#drt)) try to preserve or even highlight important aspects
    of the data distribution by the projection. The general idea is to project the
    data in a way that objects close to each other in high-dimensional space are close
    in the projection and, similarly, distant objects remain distant. In our examples,
    we will use the UMAP algorithm because it provides the best results for visualization.
    But as the `umap` library implements the scikit-learn estimator interface, you
    can easily replace the UMAP reducer with scikit-learn’s `PCA` or `TSNE` classes.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习领域，降维也有许多用例。一些学习算法对高维且常稀疏的数据存在问题。诸如 PCA、t-SNE 或 UMAP（见[“降维技术”](#drt)）之类的降维技术试图通过投影来保留或甚至突出数据分布的重要方面。其一般思想是以一种方式投影数据，使得在高维空间中彼此接近的对象在投影中也接近，而远离的对象仍然保持距离。在我们的示例中，我们将使用
    UMAP 算法，因为它为可视化提供了最佳结果。但是由于 `umap` 库实现了 scikit-learn 的估算器接口，你可以轻松地用 scikit-learn
    的 `PCA` 或 `TSNE` 类替换 UMAP 缩减器。
- en: The following code block contains the basic operations to project the embeddings
    into two-dimensional space with UMAP, as shown in [Figure 10-3](#fig-umap-all).
    After the selection of the embedding models and the words to plot (in this case
    we take the whole vocabulary), we instantiate the UMAP dimensionality reducer
    with target dimensionality `n_components=2`. Instead of the standard Euclidean
    distance metric, we use the cosine as usual. The embeddings are then projected
    to 2D by calling `reducer.fit_transform(wv)`.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码块包含了使用 UMAP 将嵌入投影到二维空间的基本操作，如 [图 10-3](#fig-umap-all) 所示。在选择嵌入模型和要绘制的词（在本例中我们采用整个词汇表）之后，我们使用目标维数
    `n_components=2` 实例化 UMAP 降维器。我们像往常一样使用余弦而不是标准的欧氏距离度量。然后通过调用 `reducer.fit_transform(wv)`
    将嵌入投影到 2D。
- en: '[PRE31]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![](Images/btap_1003.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1003.jpg)'
- en: Figure 10-3\. Two-dimensional UMAP projections of all word embeddings of our
    model. A few words and their most similar neighbors are highlighted to explain
    some of the clusters in this scatter plot.
  id: totrans-205
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. 我们模型的所有词嵌入的二维 UMAP 投影。突出显示了一些词及其最相似的邻居，以解释此散点图中的一些聚类。
- en: 'We use Plotly Express here for visualization instead of Matplotlib because
    it has two nice features. First, it produces interactive plots. When you hover
    with the mouse over a point, the respective word will be displayed. Moreover,
    you can zoom in and out and select regions. The second nice feature of Plotly
    Express is its simplicity. All you need to prepare is a `DataFrame` with the coordinates
    and the metadata to be displayed. Then you just instantiate the chart, in this
    case the scatter plot (`px.scatter`):'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用 Plotly Express 进行可视化，而不是 Matplotlib，因为它有两个很好的特性。首先，它生成交互式图。当你用鼠标悬停在一个点上时，相应的词将被显示出来。此外，你可以放大和缩小并选择区域。Plotly
    Express 的第二个很好的特性是它的简单性。你只需要准备一个带有坐标和要显示的元数据的 `DataFrame`。然后你只需实例化图表，本例中为散点图 (`px.scatter`)：
- en: '[PRE32]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: You can find a more general blueprint function `plot_embeddings` in the `embeddings`
    package in our [GitHub repository](https://oreil.ly/gX6Ti). It allows you to choose
    the dimensionality reduction algorithm and highlight selected search words with
    their most similar neighbors in the low-dimensional projection. For the plot in
    [Figure 10-3](#fig-umap-all) we inspected some clusters manually beforehand and
    then explicitly named a few typical search words to colorize the clusters.^([7](ch10.xhtml#idm45634179916184))
    In the interactive view, you could see the words when you hover over the points.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我们的 [GitHub 仓库](https://oreil.ly/gX6Ti) 中的 `embeddings` 包中找到一个更通用的蓝图函数 `plot_embeddings`。它允许你选择降维算法，并突出显示低维投影中的选定搜索词及其最相似的邻居。对于
    [图 10-3](#fig-umap-all) 中的绘图，我们事先手动检查了一些聚类，然后明确命名了一些典型的搜索词来着色聚类。^([7](ch10.xhtml#idm45634179916184))
    在交互视图中，你可以在悬停在点上时看到这些词。
- en: 'Here is the code to produce this diagram:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是生成此图的代码：
- en: '[PRE33]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'For data exploration, it might be more interesting to visualize only the set
    of search words and their most similar neighbors, without all other points. [Figure 10-4](#fig-umap-selected-2d)
    shows an example generated by the following lines. Displayed are the search words
    and their top 10 most similar neighbors:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据探索，仅可视化搜索词集合及其最相似的邻居可能更有趣。[图 10-4](#fig-umap-selected-2d) 展示了以下几行代码生成的示例。展示的是搜索词及其前
    10 个最相似的邻居：
- en: '[PRE34]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](Images/btap_1004.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1004.jpg)'
- en: Figure 10-4\. Two-dimensional UMAP projection of selected keywords words and
    their most similar neighbors.
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. 选定关键词及其最相似邻居的二维 UMAP 投影。
- en: '[Figure 10-5](#fig-umap-selected-3d) shows the same keywords but with many
    more similar neighbors as a three-dimensional plot. It is nice that Plotly allows
    you to rotate and zoom into the point cloud. This way it is easy to investigate
    interesting areas. Here is the call to generate that diagram:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-5](#fig-umap-selected-3d) 显示了相同的关键词，但具有更多相似邻居的三维绘图。Plotly 允许您旋转和缩放点云，这样可以轻松调查感兴趣的区域。以下是生成该图的调用：'
- en: '[PRE35]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: To visualize analogies such as *tacoma is to toyota like f150 is to ford*, you
    should use the linear PCA transformation. Both UMAP and t-SNE distort the original
    space in a nonlinear manner. Therefore, the direction of difference vectors in
    the projected space can be totally unrelated to the original direction. Even PCA
    distorts because of shearing, but the effect is not as strong as in UMAP or t-SNE.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 要可视化如 *tacoma is to toyota like f150 is to ford* 的类比，应使用线性 PCA 转换。UMAP 和 t-SNE
    都以非线性方式扭曲原始空间。因此，投影空间中的差异向量方向可能与原始方向毫无关联。即使 PCA 也因剪切而扭曲，但效果不及 UMAP 或 t-SNE 明显。
- en: '![](Images/btap_1005.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1005.jpg)'
- en: Figure 10-5\. Three-dimensional UMAP projection of selected keywords and their
    most similar neighbors.
  id: totrans-219
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-5\. 选定关键词及其最相似邻居的三维 UMAP 投影。
- en: 'Blueprint: Using the TensorFlow Embedding Projector'
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：使用 TensorFlow Embedding Projector
- en: A nice alternative to a self-implemented visualization function is the TensorFlow
    Embedding Projector. It also supports PCA, t-SNE, and UMAP and offers some convenient
    options for data filtering and highlighting. You don’t even have to install TensorFlow
    to use it because there is an [online version available](https://oreil.ly/VKLxe).
    A few datasets are already loaded as a demo.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的替代自实现可视化函数的选择是 TensorFlow Embedding Projector。它还支持 PCA、t-SNE 和 UMAP，并为数据过滤和突出显示提供了一些便利选项。您甚至无需安装
    TensorFlow 就可以使用它，因为有一个[在线版本可用](https://oreil.ly/VKLxe)。一些数据集已加载为演示。
- en: 'To display our own word embeddings with the TensorFlow Embedding Projector,
    we need to create two files with tabulator-separated values: one file with the
    word vectors and an optional file with metadata for the embeddings, which in our
    case are simply the words. This can be achieved with a few lines of code:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 要显示我们自己的单词嵌入与 TensorFlow Embedding Projector，我们需要创建两个以制表符分隔值的文件：一个包含单词向量的文件和一个可选的包含嵌入元数据的文件，在我们的情况下，它们只是单词。这可以通过几行代码实现：
- en: '[PRE36]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now we can load our embeddings into the projector and navigate through the 3D
    visualization. For the detection of clusters, you should use UMAP or t-SNE. [Figure 10-6](#fig-tf-projector)
    shows a cutout of the UMAP projection for our embeddings. In the projector, you
    can click any data point or search for a word and get the first 100 neighbors
    highlighted. We chose *harley* as a starting point to explore the terms related
    to Harley-Davidson. As you can see, this kind of visualization can be extremely
    helpful when exploring important terms of a domain and their semantic relationship.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将我们的嵌入加载到投影仪中，并浏览 3D 可视化效果。要检测聚类，应使用 UMAP 或 t-SNE。[图 10-6](#fig-tf-projector)
    显示了我们嵌入的 UMAP 投影的截图。在投影仪中，您可以单击任何数据点或搜索单词，并突出显示其前 100 个邻居。我们选择 *harley* 作为起点来探索与哈雷
    - 戴维森相关的术语。正如您所见，这种可视化在探索领域重要术语及其语义关系时非常有帮助。
- en: '![](Images/btap_1006.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1006.jpg)'
- en: Figure 10-6\. Visualization of embeddings with TensorFlow Embedding Projector.
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-6\. 使用 TensorFlow Embedding Projector 可视化嵌入。
- en: 'Blueprint: Constructing a Similarity Tree'
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：构建相似性树
- en: 'The words with their similarity relations can be interpreted as a network graph
    in the following way: the words represent the nodes of the graph, and an edge
    is created whenever two nodes are “very” similar. The criterion for this could
    be either that the nodes are among their top-n most-similar neighbors or a threshold
    for the similarity score. However, most of the words in the vicinity of a word
    are similar not only to that word but also to each other. Thus, the complete network
    graph even for a small subset of words would have too many edges for comprehensible
    visualization. Therefore, we start with a slightly different approach and create
    a subgraph of this network, a similarity tree. [Figure 10-7](#fig-sim-tree-noise)
    shows such a similarity tree for the root word *noise*.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这些词及其相似关系可以被解释为网络图，如下所示：词表示图的节点，当两个节点“非常”相似时，就创建一条边。此标准可以是节点位于它们的前n个最相似邻居之间，或者是相似度分数的阈值。然而，一个词附近的大多数词不仅与该词相似，而且彼此也相似。因此，即使对于少量词的子集，完整的网络图也会有太多的边，以至于无法理解的可视化。因此，我们从略微不同的角度出发，创建这个网络的子图，即相似性树。[图 10-7](#fig-sim-tree-noise)
    展示了这样一个根词 *noise* 的相似性树。
- en: '![](Images/btap_1007.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1007.jpg)'
- en: Figure 10-7\. Similarity tree of words most similar to noise.
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-7\. *noise* 最相似的单词的相似性树。
- en: We provide two blueprint functions to create such visualizations. The first
    one, `sim_tree`, generates the similarity tree starting from a root word. The
    second one, `plot_tree`, creates the plot. We use Python’s graph library `networkx`
    in both functions.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供两个蓝图函数来创建这样的可视化效果。第一个函数 `sim_tree` 从根词开始生成相似性树。第二个函数 `plot_tree` 创建绘图。我们在两个函数中都使用
    Python 的图形库 `networkx`。
- en: 'Let’s first look at `sim_tree`. Starting from a root word, we look for the
    top-n most-similar neighbors. They are added to the graph with the according edges.
    Then we do the same for each of these newly discovered neighbors, and their neighbors,
    and so on, until a maximum distance to the root node is reached. Internally, we
    use a queue (`collections.deque`) to implement a breadth-first search. The edges
    are weighted by similarity, which is used later to style the line width:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看一下 `sim_tree`。从根词开始，我们寻找前n个最相似的邻居。它们被添加到图中，并且相应地创建边。然后，我们对每个新发现的邻居及其邻居执行相同的操作，依此类推，直到达到与根节点的最大距离。在内部，我们使用队列
    (`collections.deque`) 实现广度优先搜索。边的权重由相似度确定，稍后用于设置线宽：
- en: '[PRE37]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The function `plot_tree` consists of just a few calls to create the layout
    and to draw the nodes and edges with some styling. We used Graphviz’s `twopi`
    layout to create the snowflake-like positioning of nodes. A few details have been
    left out here for the sake of simplicity, but you can find the [full code on GitHub](https://oreil.ly/W-zbu):'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 `plot_tree` 只需几个调用来创建布局并绘制节点和边，并对其进行一些样式设置。我们使用 Graphviz 的 `twopi` 布局来创建节点的雪花状位置。为简化起见，这里略去了一些细节，但你可以在
    [GitHub 上找到完整代码](https://oreil.ly/W-zbu)。
- en: '[PRE38]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[Figure 10-7](#fig-sim-tree-noise) was generated with these functions using
    this parametrization:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-7](#fig-sim-tree-noise) 使用这些函数和参数生成。'
- en: '[PRE39]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: It shows the most similar words to *noise* and their most similar words up to
    an imagined distance of 3 to *noise*. The visualization suggests that we created
    a kind of a taxonomy, but actually we didn’t. We just chose to include only a
    subset of the possible edges in our graph to highlight the relationships between
    a “parent” word and its most similar “child” words. The approach ignores possible
    edges among siblings or to grandparents. The visual presentation nevertheless
    helps to explore the specific vocabulary of an application domain around the root
    word. However, Gensim also implements [Poincaré embeddings](https://oreil.ly/mff7p)
    for learning hierarchical relationships among words.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 它展示了与 *noise* 最相似的单词及其与 *noise* 的最相似单词，直到设想的距离为3。可视化表明，我们创建了一种分类法，但实际上并非如此。我们只选择在我们的图中包含可能的边的子集，以突出“父”词与其最相似的“子”词之间的关系。这种方法忽略了兄弟之间或祖父辈之间可能的边。然而，视觉呈现有助于探索围绕根词的特定应用领域的词汇。然而，Gensim
    还实现了用于学习单词之间分层关系的 [Poincaré embeddings](https://oreil.ly/mff7p)。
- en: 'The model with the small context window of 2 used for this figure brings out
    the different kinds and synonyms of noises. If we choose a large context window
    instead, we get more concepts related to the root word. [Figure 10-8](#fig-sim-tree-plug)
    was created with these parameters:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 本图使用了窗口大小为 2 的模型，突显了不同种类和同义词的噪声。如果我们选择较大的窗口大小，我们将得到与根词相关的更多概念。[图 10-8](#fig-sim-tree-plug)
    是使用以下参数创建的：
- en: '[PRE40]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![](Images/btap_1008.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1008.jpg)'
- en: Figure 10-8\. Similarity tree of words most similar to spark-plug’s most similar
    words.
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-8\. 与火花塞最相似的单词的相似性树。
- en: Here, we chose *spark-plug* as root word and selected the model with window
    size 30\. The generated diagram gives a nice overview about domain-specific terms
    related to *spark-plugs*. For example, the codes like *p0302*, etc., are the standardized
    OBD2 error codes for misfires in the different cylinders.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们选择了 *spark-plug* 作为根词，并选择了窗口大小为 30 的模型。生成的图表很好地概述了与 *spark-plugs* 相关的领域特定术语。例如，*p0302*
    等代码是不同汽缸中点火故障的标准化 OBD2 故障代码。
- en: Of course, these charts also bring up some the weaknesses of our data preparation.
    We see four nodes for *spark-plug*, *sparkplug*, *spark*, and *plugs*, all of
    which are representing the same concept. If we wanted to have a single embedding
    for all of these, we would have to merge the different forms of writing into a
    single token.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些图表也揭示了我们数据准备中的一些弱点。我们看到 *spark-plug*、*sparkplug*、*spark* 和 *plugs* 四个节点，它们都代表着相同的概念。如果我们希望为所有这些形式的写法创建单一的嵌入向量，就必须将它们合并成一个标记。
- en: Closing Remarks
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语
- en: Exploring the similar neighbors of certain key terms in domain-specific models
    can be a valuable technique to discover latent semantic relationships among words
    in a domain-specific corpus. Even though the whole concept of word similarity
    is inherently fuzzy, we produced really interesting and interpretable results
    by training a simple neural network on just 20,000 user posts about cars.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 探索特定关键术语在领域特定模型中相似邻居可以是一种有价值的技术，以发现领域特定语料库中单词之间的潜在语义关系。尽管单词相似性的整体概念本质上是模糊的，但我们通过仅在约
    20,000 用户关于汽车的帖子上训练一个简单的神经网络，产生了非常有趣和可解释的结果。
- en: As in most machine learning tasks, the quality of the results is strongly influenced
    by data preparation. Depending on the task you are going to achieve, you should
    decide consciously which kind of normalization and pruning you apply to the original
    texts. In many cases, using lemmas and lowercase words produces good embeddings
    for similarity reasoning. Phrase detection can be helpful, not only to improve
    the result but also to identify possibly important compound terms in your application
    domain.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数机器学习任务一样，结果的质量受到数据准备的强烈影响。根据您要完成的任务，您应该有意识地决定对原始文本应用哪种规范化和修剪。在许多情况下，使用词形和小写字母单词能产生良好的相似性推理嵌入。短语检测可能有助于改进结果，还可以识别应用领域中可能重要的复合术语。
- en: We used Gensim to train, store, and analyze our embeddings. Gensim is very popular,
    but you may also want to check possibly faster alternatives like [(Py)Magnitude](https://oreil.ly/UlRzX)
    or [finalfusion](https://oreil.ly/TwM4h). Of course, you can also use TensorFlow
    and PyTorch to train different kinds of embeddings.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 Gensim 来训练、存储和分析我们的嵌入向量。Gensim 非常流行，但您可能也想检查可能更快的替代方案，比如 [(Py)Magnitude](https://oreil.ly/UlRzX)
    或者 [finalfusion](https://oreil.ly/TwM4h)。当然，您也可以使用 TensorFlow 和 PyTorch 来训练不同类型的嵌入向量。
- en: Today, semantic embeddings are fundamental for all complex machine learning
    tasks. However, for tasks such as sentiment analysis or paraphrase detection,
    you don’t need embeddings for words but for sentences or complete documents. Many
    different approaches have been published to create document embeddings (Wolf,
    2018; Palachy, 2019). A common approach is to compute the average of the word
    vectors in a sentence. Some of spaCy’s models include [word vectors](https://oreil.ly/zI1wm)
    in their vocabulary and allow the computation of document similarities based on
    average word vectors out of the box. However, averaging word vectors only works
    reasonably well for single sentences or very short documents. In addition, the
    whole approach is limited to the bag-of-words idea where the word order is not
    considered.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，语义嵌入对所有复杂的机器学习任务至关重要。然而，对于诸如情感分析或释义检测等任务，您不需要单词的嵌入，而是需要句子或完整文档的嵌入。已经发表了许多不同的方法来创建文档嵌入（Wolf,
    2018; Palachy, 2019）。一个常见的方法是计算句子中单词向量的平均值。一些 spaCy 模型在其词汇表中包含了[单词向量](https://oreil.ly/zI1wm)，并且可以基于平均单词向量计算文档相似性。然而，对于单个句子或非常短的文档，仅平均单词向量的方法效果还不错。此外，整个方法受到袋装词袋思想的限制，其中不考虑单词顺序。
- en: State-of-the-art models utilize both the power of semantic embeddings and the
    word order. We will use such a model in the next chapter for sentiment classification.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 当前最先进的模型利用了语义嵌入的能力以及词序。在下一章节中，我们将使用这样的模型进行情感分类。
- en: Further Reading
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Bolukbasi, Tolga, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai.
    *Man Is to Computer Programmer as Woman Is to Homemaker? Debiasing Word Embeddings*.
    2016\. [*https://arxiv.org/abs/1607.06520*](https://arxiv.org/abs/1607.06520).
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bolukbasi, Tolga, Kai-Wei Chang, James Zou, Venkatesh Saligrama 和 Adam Kalai.
    *Man Is to Computer Programmer as Woman Is to Homemaker? Debiasing Word Embeddings*.
    2016\. [*https://arxiv.org/abs/1607.06520*](https://arxiv.org/abs/1607.06520).
- en: Joulin, Armand, Edouard Grave, Piotr Bojanowski, and Tomáš Mikolov. *Bag of
    Tricks for Efficient Text Classification*. 2017\. [*https://www.aclweb.org/anthology/E17-2068*](https://www.aclweb.org/anthology/E17-2068).
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joulin, Armand, Edouard Grave, Piotr Bojanowski 和 Tomáš Mikolov. *高效文本分类的技巧包*.
    2017\. [*https://www.aclweb.org/anthology/E17-2068*](https://www.aclweb.org/anthology/E17-2068).
- en: Levy, Omer, Yoav Goldberg, and Ido Dagan. *Improving Distributional Similarity
    with Lessons Learned from Word Embeddings*. [*https://www.aclweb.org/anthology/Q15-1016*](https://www.aclweb.org/anthology/Q15-1016).
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levy, Omer, Yoav Goldberg 和 Ido Dagan. *Improving Distributional Similarity
    with Lessons Learned from Word Embeddings*. [*https://www.aclweb.org/anthology/Q15-1016*](https://www.aclweb.org/anthology/Q15-1016).
- en: McCormick, Chris. *Word2Vec Tutorial*. [*http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model*](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model)
    and [*http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling*](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling).
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCormick, Chris. *Word2Vec 教程*. [*http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model*](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model)
    和 [*http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling*](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling).
- en: Mikolov, Tomáš, Kai Chen, Greg Corrado, and Jeffrey Dean. *Efficient Estimation
    of Word Representations in Vector Space*. 2013\. [*https://arxiv.org/abs/1301.3781*](https://arxiv.org/abs/1301.3781).
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov, Tomáš, Kai Chen, Greg Corrado 和 Jeffrey Dean. *Efficient Estimation
    of Word Representations in Vector Space*. 2013\. [*https://arxiv.org/abs/1301.3781*](https://arxiv.org/abs/1301.3781).
- en: Mikolov, Tomáš, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. *Distributed
    Representations of Words and Phrases and Their Compositionality*. 2013\. [*https://arxiv.org/abs/1310.4546*](https://arxiv.org/abs/1310.4546).
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov, Tomáš, Ilya Sutskever, Kai Chen, Greg Corrado 和 Jeffrey Dean. *Distributed
    Representations of Words and Phrases and Their Compositionality*. 2013\. [*https://arxiv.org/abs/1310.4546*](https://arxiv.org/abs/1310.4546).
- en: 'Palachy, Shay. *Beyond Word Embedding: Key Ideas in Document Embedding*. [*https://www.kdnuggets.com/2019/10/beyond-word-embedding-document-embedding.html*](https://www.kdnuggets.com/2019/10/beyond-word-embedding-document-embedding.html).'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Palachy, Shay. *超越词嵌入：文档嵌入中的关键思想*. [*https://www.kdnuggets.com/2019/10/beyond-word-embedding-document-embedding.html*](https://www.kdnuggets.com/2019/10/beyond-word-embedding-document-embedding.html).
- en: 'Pennington, Jeffrey, Richard Socher, and Christopher Manning. *Glove: Global
    Vectors for Word Representation*. 2014\. [*https://nlp.stanford.edu/pubs/glove.pdf*](https://nlp.stanford.edu/pubs/glove.pdf).'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pennington, Jeffrey, Richard Socher 和 Christopher Manning. *Glove: Global Vectors
    for Word Representation*. 2014\. [*https://nlp.stanford.edu/pubs/glove.pdf*](https://nlp.stanford.edu/pubs/glove.pdf).'
- en: Peters, Matthew E., Mark Neumann, Mohit Iyyer, et al. *Deep contextualized word
    representations*. 2018\. [*https://arxiv.org/abs/1802.05365*](https://arxiv.org/abs/1802.05365).
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters, Matthew E., Mark Neumann, Mohit Iyyer等人。*深度上下文化的词表示*. 2018\. [*https://arxiv.org/abs/1802.05365*](https://arxiv.org/abs/1802.05365)。
- en: Wolf, Thomas. *The Current Best of Universal Word Embeddings and Sentence Embeddings*.
    2018\. [*https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a*](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a).
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf, Thomas。*通用词向量和句子向量的最新进展*. 2018\. [*https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a*](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a)。
- en: ^([1](ch10.xhtml#idm45634182078904-marker)) Inspired by Adrian Colyer’s [“The
    Amazing Power of Word Vectors” blog post](https://oreil.ly/8iMPF).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch10.xhtml#idm45634182078904-marker)) 受到Adrian Colyer的[“词向量的惊人力量”博文](https://oreil.ly/8iMPF)的启发。
- en: ^([2](ch10.xhtml#idm45634182074280-marker)) This frequently cited example originally
    came from the linguist Eugene Nida in 1975.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch10.xhtml#idm45634182074280-marker)) 这个经常被引用的例子最初来自语言学家尤金·尼达，于1975年提出。
- en: ^([3](ch10.xhtml#idm45634182038552-marker)) Jay Alammar’s blog post entitled
    [“The Illustrated Word2Vec”](https://oreil.ly/TZNTT) gives a wonderful visual
    explanation of this equation.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch10.xhtml#idm45634182038552-marker)) Jay Alammar的博文[“图解Word2Vec”](https://oreil.ly/TZNTT)生动地解释了这个方程。
- en: ^([4](ch10.xhtml#idm45634181941800-marker)) Words having the same pronunciation
    but different meanings are called *homonyms*. If they are spelled identically,
    they are called *homographs*.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch10.xhtml#idm45634181941800-marker)) 拥有相同发音但不同意义的单词被称为*同音异义词*。如果它们拼写相同，则被称为*同形异义词*。
- en: ^([5](ch10.xhtml#idm45634181919512-marker)) For example, from [RaRe Technologies](https://oreil.ly/two0R)
    and [3Top](https://oreil.ly/4DwDy).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch10.xhtml#idm45634181919512-marker)) 例如，来自[RaRe Technologies](https://oreil.ly/two0R)和[3Top](https://oreil.ly/4DwDy)。
- en: ^([6](ch10.xhtml#idm45634180172056-marker)) If you run the code yourself, the
    results may be slightly different than the ones printed in the book because of
    random initialization.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch10.xhtml#idm45634180172056-marker)) 如果你自己运行这段代码，由于随机初始化的原因，结果可能会与书中打印的略有不同。
- en: ^([7](ch10.xhtml#idm45634179916184-marker)) You’ll find the colorized figures
    in the electronic versions and on [GitHub](https://oreil.ly/MWJLd).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch10.xhtml#idm45634179916184-marker)) 你可以在电子版和[GitHub](https://oreil.ly/MWJLd)上找到彩色的图表。
