- en: '3 Model-agnostic methods: Global interpretability'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 模型无关方法：全局可解释性
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Characteristics of model-agnostic methods and global interpretability
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型无关方法和全局可解释性的特点
- en: How to implement tree ensembles, specifically random forest—a black-box model
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何实现树集成，特别是随机森林——一个黑盒模型
- en: How to interpret random forest models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何解释随机森林模型
- en: How to interpret black-box models using a model-agnostic method called partial
    dependence plots (PDPs)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用称为部分依赖图（PDPs）的模型无关方法来解释黑盒模型
- en: How to uncover bias by looking at feature interactions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过观察特征交互来揭示偏差
- en: In the previous chapter, we saw two different types of machine learning models—white
    box and black box—and focused most of our attention on how to interpret white-box
    models. Black-box models have a high predictive power and, as the name suggests,
    are hard to interpret. In this chapter, we will focus on interpreting black-box
    models, and you’ll learn specifically about techniques that are *model agnostic*
    and (*global*) in scope. Recall from chapter 1 that model-agnostic interpretability
    techniques are not dependent on the specific type of model being used. They can
    be applied to any model because they are independent of the internal structure
    of the model. Also, interpretability techniques that are global in scope can help
    us understand the entire model as a whole. We will also focus on tree ensembles,
    specifically random forest. Although we focus on random forest, you can apply
    the model-agnostic techniques that you will learn in this chapter to any model.
    We will switch our attention to more complex black-box models, like neural networks,
    in the following chapter. In chapter 4, you will also learn about model-agnostic
    techniques that are local in scope, such as LIME, SHAP, and anchors.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到了两种不同的机器学习模型——白盒模型和黑盒模型，并将大部分注意力集中在如何解释白盒模型上。黑盒模型具有很高的预测能力，正如其名称所示，很难解释。在本章中，我们将专注于解释黑盒模型，并具体介绍那些*模型无关*和(*全局*)范围内的技术。回想一下第一章，模型无关的可解释性技术不依赖于所使用的特定模型类型。由于它们独立于模型的内部结构，因此可以应用于任何模型。此外，全局范围内的可解释性技术可以帮助我们理解整个模型。我们还将专注于树集成，特别是随机森林。尽管我们专注于随机森林，但你可以在本章中学到的模型无关技术应用于任何模型。在下一章中，我们将转向更复杂的黑盒模型，如神经网络。在第四章中，你还将了解局部范围内的模型无关技术，如LIME、SHAP和锚点。
- en: The structure of chapter 3 is similar to that of chapter 2\. We will start off
    by looking at a concrete example. In this chapter, we will take a break from Diagnostics+
    and focus on another problem related to education. I’ve chosen this problem because
    the dataset has some interesting characteristics and we can expose some issues
    in this dataset through the interpretability techniques that you will learn in
    this chapter. As in chapter 2, the main focus of this chapter is on implementing
    interpretability techniques to gain a better understanding of black-box models
    (specifically tree ensembles). We will apply these interpretability techniques
    during model development and testing. You will also learn about model training
    and testing, especially the implementation aspects. Because the model learning,
    testing, and understanding stages are iterative, it is important to cover all
    three stages together. Readers who are already familiar with training and testing
    tree ensembles are free to skip those sections and jump straight into interpretability.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 第三章的结构与第二章相似。我们将从一个具体的例子开始。在本章中，我们将暂时放下Diagnostics+，专注于与教育相关的问题。我选择这个问题是因为数据集有一些有趣的特点，我们可以通过本章中你将学习的可解释性技术来揭示这个数据集中的某些问题。与第二章一样，本章的主要重点是实现可解释性技术，以更好地理解黑盒模型（特别是树集成）。我们将在模型开发和测试过程中应用这些可解释性技术。你还将了解模型训练和测试，特别是实施方面。由于模型学习、测试和理解阶段是迭代的，因此涵盖这三个阶段是很重要的。对于已经熟悉树集成训练和测试的读者，可以自由跳过这些部分，直接进入可解释性部分。
- en: 3.1 High school student performance predictor
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 高中生成绩预测器
- en: Let’s begin by looking at a concrete example. We will switch from Diagnostic+
    and the healthcare sector to education. A superintendent of a school district
    in the United States has approached you to help her with a data science problem.
    The superintendent would like to understand how students are performing in three
    key subject areas—math, reading, and writing—to determine the level of funding
    required for various schools and also to ensure that every student succeeds as
    part of the Every Student Succeeds Act (ESSA).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个具体的例子开始。我们将从诊断+和医疗保健行业切换到教育行业。美国一个学校区的区主管向您寻求帮助，以解决她的数据科学问题。区主管希望了解学生在三个关键学科领域——数学、阅读和写作——的表现，以确定不同学校的资金需求，并确保每个学生都能在《每个学生成功法案》（ESSA）的框架下取得成功。
- en: 'The superintendent is specifically looking for help in predicting the grades
    of a high school student in her district in math, reading, and writing subjects.
    The grade can be A, B, C, or F. Given this information, how would you formulate
    this as a machine learning problem? Because the target of the model is to predict
    the grade, which can be one of four discrete values, the problem can be formulated
    as a *classification* problem. In terms of data, the superintendent has collected
    the data of 1,000 students in her district representing various schools and backgrounds.
    The following five data points are collected for each student:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 区主管特别希望得到帮助，预测她辖区的高中生在数学、阅读和写作科目中的成绩。成绩可以是A、B、C或F。根据这些信息，您会如何将这个问题表述为一个机器学习问题？因为模型的目的是预测成绩，这可以是四个离散值之一，所以这个问题可以表述为一个*分类*问题。在数据方面，区主管收集了她辖区代表不同学校和背景的1,000名学生的数据。以下五个数据点为每个学生收集：
- en: Gender
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性别
- en: Ethnicity
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 种族
- en: Parent Level of Education
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 父母的教育水平
- en: Type of Lunch purchased by the student
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学生购买的午餐类型
- en: Test Preparation level
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试准备水平
- en: Given this data, you will, therefore, need to train three separate classifiers,
    one for each subject area. This is illustrated in figure 3.1.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些数据，因此您需要训练三个独立的分类器，每个分类器对应一个学科领域。这如图3.1所示。
- en: '![](../Images/CH03_F01_Thampi.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F01_Thampi.png)'
- en: Figure 3.1 An illustration of student performance models required by the superintendent
    of the school district
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 学校区主管所需的学生表现模型示意图
- en: Protected attributes and fairness
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 受保护属性和公平性
- en: Protected attributes are attributes associated with an individual related to
    social bias, including gender, age, race, ethnicity, sexual orientation, and so
    on. Laws in certain regions like the United States and Europe prohibit discriminating
    against individuals on the basis of these protected attributes, especially in
    domains like housing, employment, and credit lending. It is important to be aware
    of these legal frameworks and nondiscrimination laws when building machine learning
    models that may use these protected attributes as features. We want to ensure
    that machine learning models do not embed bias and discriminate against certain
    individuals on the basis of protected attributes. In this chapter, our dataset
    contains a couple of protected attributes that we use as features for the model
    primarily to learn how to expose, through interpretability techniques, possible
    issues with the model pertaining to bias. We will cover the legal frameworks around
    protected attributes and various fairness criteria in more depth in chapter 8\.
    Moreover, the dataset used in this chapter is contrived and does not reflect actual
    student performance in a school district. The race/ethnicity feature is also anonymized.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 受保护属性是与个人相关的、与社会偏见有关的属性，包括性别、年龄、种族、民族、性取向等。美国和欧洲等某些地区的法律禁止基于这些受保护属性歧视个人，特别是在住房、就业和信贷贷款等领域。在构建可能使用这些受保护属性作为特征的机器学习模型时，了解这些法律框架和非歧视法律非常重要。我们希望确保机器学习模型不会嵌入偏见，并基于受保护属性歧视某些个人。在本章中，我们的数据集包含一些受保护属性，我们主要将其用作模型的特征，以学习如何通过可解释性技术揭示模型与偏见相关的问题。我们将在第8章更深入地介绍受保护属性的法律框架和各种公平性标准。此外，本章中使用的数据集是虚构的，并不反映学校区实际的学生表现。种族/民族特征也已匿名化。
- en: 3.1.1 Exploratory data analysis
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 探索性数据分析
- en: 'We are dealing with a new dataset here, so before we train the model, let’s
    first understand the different features and possible values for them. The dataset
    contains five features: the student’s Gender, their Ethnicity, Parent Level of
    Education, the Type of Lunch that they purchase, and their Test Preparation level.
    All of these features are *categorical* features where the possible values are
    discrete and finite. There are three target variables for each student: math grade,
    reading grade, and writing grade. The grades can be A, B, C, or F.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里处理的是一个新数据集，所以在训练模型之前，让我们首先了解不同的特征及其可能的值。数据集包含五个特征：学生的性别、他们的民族、父母的教育水平、他们购买的午餐类型以及他们的测试准备水平。所有这些特征都是*分类*特征，其中可能的值是离散且有限的。对于每个学生有三个目标变量：数学成绩、阅读成绩和写作成绩。成绩可以是A、B、C或F。
- en: There are two gender categories—male and female—and the female population (52%)
    of students is slightly higher than the male population (48%). Let’s now focus
    on two other features—the student’s Ethnicity and Parent Level of Education. Figure
    3.2 shows the different categories for each of those features and the proportion
    of students who fall under those categories. Five groups or ethnicities are in
    the population, and groups C and D are the most represented, accounting for about
    58% of the student population. Six different levels of parent education occur.
    In ascending order, they are some high school, (recognized) high school, some
    college, associate degree, bachelor’s degree, and master’s degree. It looks like
    there are a lot more students in the population whose parents have lower levels
    of education. Roughly 82% of the students have parents with a high school or college
    level of education or an associate degree. Only 18% of the students have parents
    with a bachelor’s or master’s degree.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个性别类别——男性和女性，学生的女性人口（52%）略高于男性人口（48%）。现在让我们关注另外两个特征——学生的民族和父母的教育水平。图3.2显示了这些特征的各个类别以及属于这些类别的学生的比例。人口中有五个群体或民族，C组和D组是最具代表性的，占学生人口的约58%。有六个不同的父母教育水平。按顺序排列，它们是某些高中、（认可）高中、某些大学、副学士学位、学士学位和硕士学位。看起来有更多学生的父母教育水平较低。大约82%的学生父母的教育水平是高中或大学水平或副学士学位。只有18%的学生父母拥有学士学位或硕士学位。
- en: '![](../Images/CH03_F02_Thampi.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F02_Thampi.png)'
- en: Figure 3.2 Values and proportions for the features Ethnicity and Parent Level
    of Education
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 特征“民族”和“父母教育水平”的值和比例
- en: Now for the remaining two features—Type of Lunch purchased and the Test Preparation
    level. The majority (roughly 65%) purchase a standard lunch and the rest purchase
    free/reduced lunches. In terms of test preparation, only 36% of the students completed
    their preparation for the tests, whereas for the remaining, it is either not completed
    or unknown.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是剩余的两个特征——购买的午餐类型和测试准备水平。大多数人（大约65%）购买标准午餐，其余的人购买免费/减价午餐。在测试准备方面，只有36%的学生完成了他们的测试准备，而对于剩余的学生，要么没有完成，要么未知。
- en: 'Let’s now look at the proportion of students who earn grades A, B, C, or F
    for the three subject areas. This is shown in figure 3.3\. We can see that the
    majority of the students (48–50%) get grade B and a very small proportion of the
    students (3–4%) get grade F. About 18–25% of the students get grade A, and 22–28%
    of the students get grade C across all the three subject areas. It is important
    to note that the data is quite *imbalanced* before we train our models. Why is
    that important, and how do we deal with imbalanced data? In a classification type
    of problem, we say that the data is imbalanced when a disproportionate number
    of examples or data points exist for a given class. It is important to note this
    because most machine learning algorithms work best when the proportion of samples
    for each class is roughly the same. Most algorithms are designed to minimize error
    or maximize accuracy, and these algorithms tend to naturally bias toward the majority
    class. We can deal with imbalanced classes in a few ways, including these common
    approaches:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下在三个学科领域中取得A、B、C或F成绩的学生比例。这显示在图3.3中。我们可以看到，大多数学生（48-50%）获得B级成绩，而极少数学生（3-4%）获得F级成绩。大约18-25%的学生获得A级成绩，在整个三个学科领域中，22-28%的学生获得C级成绩。值得注意的是，在我们训练模型之前，数据相当不平衡。这为什么很重要，我们如何处理不平衡数据？在分类问题中，当我们给定类别存在不成比例的例子或数据点时，我们说数据是不平衡的。这一点很重要，因为大多数机器学习算法在各个类别的样本比例大致相同的情况下表现最佳。大多数算法旨在最小化错误或最大化准确度，这些算法倾向于自然地偏向多数类。我们可以通过几种方式处理不平衡的类别，包括以下常见方法：
- en: Use the right performance metrics when we test and evaluate the models.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在测试和评估模型时，使用正确的性能指标。
- en: Resample the training data such that the majority class is either undersampled
    or the minority class is oversampled.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新采样训练数据，使得多数类要么被欠采样，要么少数类被过采样。
- en: You will learn more about these methods in section 3.2.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在第3.2节中了解更多关于这些方法的信息。
- en: '![](../Images/CH03_F03_Thampi.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F03_Thampi.png)'
- en: Figure 3.3 Values and proportions of the grade target variable for the three
    subject areas
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3三个学科领域中成绩目标变量的值和比例
- en: Let’s dissect the data a bit more. The insights that follow will be useful in
    section 3.4 when we have to interpret and validate what the model has learned.
    How do students generally perform when their parents have the lowest and highest
    levels of education? Let’s compare the grade distributions for students whose
    parents have the lowest level of education (i.e., high school) with students whose
    parents have the highest level of education (i.e., master’s degree). Figure 3.4
    shows this comparison across all three subject areas.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地剖析一下数据。以下见解将在第3.4节中对我们解释和验证模型学到了什么很有用。当学生的父母教育水平最低和最高时，学生通常表现如何？让我们比较父母教育水平最低（即高中）的学生与父母教育水平最高（即硕士学位）的学生的成绩分布。图3.4显示了所有三个学科领域的这种比较。
- en: '![](../Images/CH03_F04_Thampi.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F04_Thampi.png)'
- en: Figure 3.4 Grade distributions comparing the percentage of students whose parents
    have high school education vs. a master’s degree
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 比较父母拥有高中教育与硕士学位的学生成绩分布百分比
- en: Let’s focus on the students whose parents have high school education. Across
    all three subject areas, it looks like, in general, fewer students get grade A
    and more students get grade F than the overall population. For the math subject
    area, for instance, only 10% of the students whose parents have a high school
    education get grade A, whereas in the overall population (as we saw in figure
    3.3), roughly 20% of the students get grade A. Let’s now focus on students whose
    parents have a master’s degree. It looks like, in general, more students get grade
    A and zero students get grade F when compared to the overall population. For the
    math subject area, for instance, roughly 30% of the students whose parents have
    a master’s degree get grade A. If we now compare the two bars in figure 3.4, we
    can see a lot more students get a higher grade (A or B) when their parents have
    a higher level of education across all three subject areas.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们关注那些父母有高中教育背景的学生。在所有三个学科领域，从总体上看，获得A级的学生比总体人口中更多，而获得F级的学生比总体人口中更少。例如，在数学学科领域，只有10%的父母有高中教育背景的学生获得A级，而总体人口（如图3.3所示）中大约有20%的学生获得A级。现在让我们关注那些父母有硕士学位的学生。从总体上看，与总体人口相比，获得A级的学生更多，没有学生获得F级。例如，在数学学科领域，大约有30%的父母有硕士学位的学生获得A级。如果我们现在比较图3.4中的两个条形图，我们可以看到，当父母在所有三个学科领域拥有更高教育水平时，获得更高等级（A或B）的学生数量会显著增加。
- en: How about ethnicity? How does the performance of a student belonging to the
    most represented group compare with one from the least represented group? From
    figure 3.2, we know that the most represented group is C and the least represented
    group is A. Figure 3.5 compares the grade distributions of students belonging
    to group C with students belonging to group A.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，种族因素如何呢？属于最代表性群体（C组）的学生与属于最少代表性群体（A组）的学生在表现上有什么不同？从图3.2中，我们知道最代表性群体是C组，最少代表性群体是A组。图3.5比较了属于C组的学生与属于A组的学生成绩分布。
- en: It looks like, in general, students from group C perform better than those from
    group A—a larger proportion of students seem to get a higher grade (A or B) and
    a smaller proportion of students get a lower grade (C or F). As mentioned earlier,
    the insights in this section will come in handy in section 3.4 when we interpret
    and validate what the model has learned.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从总体上看，C组的学生似乎比A组的学生表现更好——似乎有更大比例的学生获得了更高的成绩（A或B），而获得较低成绩（C或F）的学生比例较小。如前所述，本节中的见解将在第3.4节中派上用场，当我们解释和验证模型学到了什么时。
- en: '![](../Images/CH03_F05_Thampi.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F05_Thampi.png)'
- en: Figure 3.5 Grade distributions comparing the percentage of students belonging
    to Ethnicity group A vs. group C
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 比较属于种族群体A和C的学生成绩分布百分比
- en: 3.2 Tree ensembles
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 树集成
- en: In chapter 2, you learned about decision trees, a powerful way of modeling nonlinear
    relationships. Decision trees are white-box models and are easy to interpret.
    We saw, however, that more complex decision trees suffer from the problem of overfitting
    where the model heavily fits the noise or variance in the data. To overcome the
    problem of overfitting, we can reduce the complexity of decision trees by pruning
    them in terms of depth and the minimum number of samples required for the leaf
    nodes. This results in low predictive power, however.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2章中，你学习了决策树，这是一种建模非线性关系的强大方法。决策树是白盒模型，易于解释。然而，我们注意到更复杂的决策树存在过拟合的问题，即模型过度拟合数据中的噪声或方差。为了克服过拟合的问题，我们可以通过剪枝来降低决策树的复杂性，包括深度和叶节点所需的最小样本数。然而，这会导致预测能力降低。
- en: 'By combining several decision trees, we can circumvent the overfitting problem
    without compromising on predictive power. This is the principle behind tree ensembles.
    We can combine, or ensemble, decision trees in the following two broad ways:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合多个决策树，我们可以在不牺牲预测能力的情况下避免过拟合问题。这是树集成背后的原理。我们可以以下两种广泛的方式组合或集成决策树：
- en: '*Bagging*—Multiple decision trees are trained in parallel on separate random
    subsets of the training data. We can use these individual decision trees to make
    predictions and combine them by taking an average to come up with the final prediction.
    Random forest is a tree ensemble using the bagging technique. In addition to training
    individual decision trees on random subsets of the data, the random forest algorithm
    also takes a random subset of the features to split the data on.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Bagging*——在训练数据的独立随机子集上并行训练多个决策树。我们可以使用这些独立的决策树进行预测，并通过取平均值将它们组合起来，得出最终的预测。随机森林是使用Bagging技术的一种树集成。除了在数据的随机子集上训练独立的决策树外，随机森林算法还随机抽取特征子集来分割数据。'
- en: '*Boosting*—Like in bagging, the boosting technique also trains multiple decision
    trees but in sequence. The first decision tree is typically a shallow tree and
    is trained on the training set. The objective of the second decision tree is to
    learn from the errors made by the first tree and to further improve the performance.
    Using this technique, we string together multiple decision trees, and they iteratively
    try to optimize and reduce the errors made by the previous one. Adaptive boosting
    and gradient boosting are two common boosting algorithms.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Boosting*——与Bagging类似，Boosting技术也训练多个决策树，但顺序不同。第一个决策树通常是一个浅层树，并在训练集上训练。第二个决策树的目标是从第一个决策树的错误中学习，并进一步提高性能。使用这种技术，我们将多个决策树串联起来，它们迭代地尝试优化和减少前一个决策树所犯的错误。自适应提升和梯度提升是两种常见的Boosting算法。'
- en: In this chapter, we will be focusing on the bagging technique, specifically
    the *random forest* algorithm, which is illustrated in figure 3.6\. First, we
    take random subsets of the training data and train separate decision trees on
    them. Each decision tree is then split on random subsets of the features. We obtain
    the final prediction by taking the majority vote across all decision trees. As
    you can see, a random forest model is much more complex than a decision tree.
    As the number of trees in the ensemble increases, the complexity increases. Moreover,
    it is much harder to visualize and interpret how features are split across all
    decision trees because random subsets of the data and features are taken for each
    of them. This makes random forest a black-box model and much harder to interpret.
    Being able to explain the algorithm does not guarantee interpretability in this
    case.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点关注集成学习技术，特别是*随机森林*算法，如图3.6所示。首先，我们从训练数据中随机抽取子集，并在这些子集上分别训练独立的决策树。然后，每个决策树在特征随机子集上进行分割。我们通过在所有决策树中进行多数投票来获得最终的预测。正如您所看到的，随机森林模型比决策树复杂得多。随着集成中树的数量增加，复杂性也随之增加。此外，由于每个决策树都从数据特征中抽取随机子集进行分割，因此很难可视化和解释所有决策树中特征的分割情况。这使得随机森林成为一个黑盒模型，难以解释。能够解释算法并不意味着在这种情况下具有可解释性。
- en: '![](../Images/CH03_F06_Thampi.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F06_Thampi.png)'
- en: Figure 3.6 An illustration of the random forest algorithm
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 随机森林算法的示意图
- en: For completeness, let’s also discuss how the adaptive-boosting and gradient-boosting
    algorithms work. *Adaptive boosting*, usually shortened as *AdaBoost*, is illustrated
    in figure 3.7\. The algorithm works as follows. First, we train a decision tree
    using all the training data. Each data point is given equal weight for the first
    decision tree. Once the first decision tree is trained, calculate the error rate
    of the tree by taking a weighted sum of the error for each data point. We then
    use this weighted error rate to determine the weight of the decision tree. If
    the error rate of the tree is high, then a lower weight is given for the tree
    because its predictive power is low. If the error rate is low, then a higher weight
    is given for the tree because it has a higher predictive power. We then use the
    weight of the first decision tree to determine the weights for each data point
    for the second decision tree. The wrongly classified data points will be given
    a higher weight so that the second decision tree can try to reduce the error rate.
    We repeat this process in sequence until the number of trees we set during training
    is reached. After all the trees have been trained, we come up with the final prediction
    by taking a weighted majority vote. Because a decision tree with a higher weight
    has higher predictive power, it is given more influence in the final prediction.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，让我们也讨论一下自适应提升和梯度提升算法的工作原理。*自适应提升*，通常简称为*AdaBoost*，如图3.7所示。该算法的工作原理如下。首先，我们使用所有训练数据训练一个决策树。在第一个决策树中，每个数据点都被赋予相同的权重。一旦第一个决策树训练完成，通过计算每个数据点的加权误差总和来计算树的误差率。然后，我们使用这个加权误差率来确定决策树的权重。如果树的误差率较高，则给树赋予较低的权重，因为其预测能力较低。如果误差率较低，则给树赋予较高的权重，因为它具有更高的预测能力。然后，我们使用第一个决策树的权重来确定第二个决策树的每个数据点的权重。被错误分类的数据点将被赋予更高的权重，以便第二个决策树尝试降低误差率。我们按顺序重复此过程，直到达到训练期间设置的树的数量。在所有树都训练完成后，我们通过加权多数投票来得出最终预测。由于权重较高的决策树具有更高的预测能力，它在最终预测中具有更大的影响力。
- en: '![](../Images/CH03_F07_Thampi.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F07_Thampi.png)'
- en: Figure 3.7 An illustration of the AdaBoost algorithm
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 AdaBoost算法的示意图
- en: The *gradient-boosting* algorithm works slightly differently and is illustrated
    in figure 3.8\. As with AdaBoost, the first decision tree is trained on all of
    the training data, but unlike AdaBoost, no weights are associated with the data
    points. After training the first decision tree, we calculate a residual error
    metric, which is the difference between the actual target and the predicted target.
    We then train the second decision tree to predict the residual error made by the
    first decision tree. So, rather than updating the weights for each data point
    like in AdaBoost, gradient boosting predicts the residual error directly. The
    objective for each tree is to fix the errors of the previous tree. This process
    is repeated in sequence until the number of trees we set during training is reached.
    After all the trees have been trained, we come up with the final prediction by
    summing the predictions of all the trees.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*梯度提升*算法的工作原理略有不同，如图3.8所示。与AdaBoost一样，第一个决策树是在所有训练数据上训练的，但与AdaBoost不同，没有与数据点关联的权重。在训练第一个决策树后，我们计算一个残差误差指标，即实际目标和预测目标之间的差异。然后，我们训练第二个决策树来预测第一个决策树造成的残差误差。因此，与AdaBoost中更新每个数据点的权重不同，梯度提升直接预测残差误差。每个树的目标是修正前一个树的错误。这个过程按顺序重复，直到达到训练期间设置的树的数量。在所有树都训练完成后，我们通过求和所有树的预测来得出最终预测。'
- en: As mentioned earlier, we will focus on the random forest algorithm, but the
    methods used to train, evaluate, and interpret the algorithm can be extended to
    the boosting techniques as well.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将重点关注随机森林算法，但用于训练、评估和解释算法的方法也可以扩展到提升技术。
- en: '![](../Images/CH03_F08_Thampi.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F08_Thampi.png)'
- en: Figure 3.8 An illustration of the gradient-boosting algorithm
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 梯度提升算法的示意图
- en: 3.2.1 Training a random forest
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 训练随机森林
- en: 'Let’s now train our random forest model to predict high school student performance.
    The following code snippet shows how to prepare the data before training the model.
    Note that when splitting the data into the training and test sets, 20% of the
    data is used for testing. The rest of the data is used for training and validation.
    Also, we take a stratified sample on the math target variable so that the distribution
    of the grades is the same for both the training and test sets. You can easily
    create similar splits using the reading and writing grades as well:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们训练我们的随机森林模型来预测高中生的表现。以下代码片段显示了在训练模型之前如何准备数据。请注意，当将数据分为训练集和测试集时，20%的数据用于测试。其余数据用于训练和验证。此外，我们对数学目标变量进行分层抽样，以确保训练集和测试集的分数分布相同。您也可以使用阅读和写作分数轻松创建类似的分割：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Loads the data into a Pandas DataFrame
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将数据加载到Pandas DataFrame中
- en: ② Because the input features are textual and categorical, we need to encode
    them into a numerical value.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ② 因为输入特征是文本和分类的，我们需要将它们编码成数值。
- en: ③ Fits and transforms the input features into numerical values
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将输入特征拟合并转换为数值
- en: ④ Initializes the LabelEncoders for the target variables as well because letter
    grades have to be converted to a numerical value
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 初始化目标变量的LabelEncoders，因为字母成绩需要转换为数值
- en: ⑤ Fits and transforms the target variables into numerical values
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将目标变量拟合并转换为数值
- en: ⑥ Splits the data into training/validation and test sets
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 将数据分割为训练/验证集和测试集
- en: ⑦ Extracts the feature matrix for the train/validation and test sets
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 提取训练/验证集和测试集的特征矩阵
- en: ⑧ Extracts the target vectors for math, reading, and writing for both the train/validation
    and test sets
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 提取训练/验证集和测试集中的数学、阅读和写作的目标向量
- en: 'Once you have prepared the data, you are now ready to train the three random
    forest models for math, reading, and writing, as shown in the next code sample.
    Note that we can determine the optimum parameters for the random forest classifier
    using cross-validation. Also note that the random forest algorithm first takes
    random subsets of the training data to train each decision tree, and for each
    decision tree, the model takes random subsets of the feature on which to split
    the data. For both of these random elements in the algorithm, it is important
    to set the seed for the random-number generator, using the `random_state` parameter.
    If this seed is not set, you will not be able to achieve reproducible and consistent
    results. First, let’s use a helper function to create a random forest model with
    predefined parameters:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您准备好了数据，现在就可以训练数学、阅读和写作的三个随机森林模型，如下代码示例所示。请注意，我们可以使用交叉验证来确定随机森林分类器的最佳参数。此外，请注意，随机森林算法首先随机选取训练数据的子集来训练每个决策树，对于每个决策树，模型随机选取特征子集来分割数据。对于算法中的这两个随机元素，设置随机数生成器的种子（使用`random_state`参数）非常重要。如果没有设置这个种子，您将无法获得可重复和一致的结果。首先，让我们使用一个辅助函数来创建一个具有预定义参数的随机森林模型：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Sets the number of decision trees in the random forest
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ① 设置随机森林中的决策树数量
- en: ② The maximum depth parameter for the decision tree
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ② 决策树的最大深度参数
- en: ③ Gini impurity is used as the cost function on which to optimize each decision
    tree.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用基尼不纯度作为优化每个决策树的代价函数
- en: ④ For reproducibility, sets the seed for the random-number generator
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 为了可重复性，设置随机数生成器的种子
- en: ⑤ Sets n_jobs to train the individual decision trees in parallel, using all
    available cores in your computer
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将n_jobs设置为并行训练单个决策树，使用您计算机上所有可用的核心
- en: 'Now let’s use this helper function to initialize and train the three random
    forest models for the math, reading, and writing subject areas, as shown here:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用这个辅助函数来初始化和训练数学、阅读和写作三个随机森林模型，如下所示：
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Initializes the math model random forest classifier with 50 decision trees
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用50个决策树初始化数学模型的随机森林分类器
- en: ② Fits the math student performance classifier on the training data using the
    math grade as the target
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用数学成绩作为目标，在训练数据上拟合数学学生表现分类器
- en: ③ Predicts the math grade for all the students in the test set using the pretrained
    model
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用预训练模型预测测试集中所有学生的数学成绩
- en: ④ Initializes and trains the random forest classifier with 25 decision trees
    to predict the reading grade
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用25个决策树初始化和训练随机森林分类器以预测阅读成绩
- en: ⑤ Initializes and trains the random forest classifier with 40 decision trees
    to predict the writing grade
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 使用40个决策树初始化和训练随机森林分类器以预测写作成绩
- en: 'Now that we’ve trained the three random forest models for math, reading, and
    writing, let’s evaluate them and compare them with a baseline model that always
    predicts the majority grade (in this case, B) for all the subjects. A metric that
    is typically used for classification problems is accuracy. This metric, however,
    is not suitable for situations where the classes are imbalanced. In our case,
    we’ve seen the student grades are pretty imbalanced, as depicted earlier in figure
    3.3\. If, for instance, 98% of the students obtain grade B in math, you can trick
    yourself into building a highly accurate model with 98% accuracy by always predicting
    grade B for all students. To gauge the performance of the model across all classes,
    we can use better metrics like precision, recall, and F1\. *Precision* is a metric
    that measures the proportion of predicted classes that are accurate. *Recall*
    is a metric that measures the proportion of actual classes that the model predicted
    accurately. The formulas for precision and recall are shown next:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了数学、阅读和写作的三个随机森林模型，让我们评估它们，并将它们与一个总是预测所有科目大多数成绩（在这种情况下，为B）的基线模型进行比较。通常用于分类问题的指标是准确率。然而，这个指标在类别不平衡的情况下并不适用。在我们的案例中，我们已经看到学生成绩相当不平衡，如图3.3所示。例如，如果98%的学生在数学中获得B级成绩，你可以通过总是预测所有学生的B级成绩来欺骗自己构建一个具有98%准确率的高度准确模型。为了衡量模型在所有类别上的性能，我们可以使用更好的指标，如精确度、召回率和F1。*精确度*是一个衡量预测类别准确比例的指标。*召回率*是一个衡量模型准确预测的实际类别比例的指标。精确度和召回率的公式如下：
- en: '![](../Images/CH03_F08_Thampi_eqution01.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F08_Thampi_eqution01.png)'
- en: '![](../Images/CH03_F08_Thampi_eqution02.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F08_Thampi_eqution02.png)'
- en: 'The perfect classifier has a precision score of 1 and a recall score of 1 because
    the number of false positives and false negatives will be 0\. But in practice,
    these two metrics are at odds with each other—there is always a trade-off that
    you need to make between false positives and false negatives. As you reduce the
    false positives and increase precision, the cost will be increased false negatives
    and lower recall. To find the right balance between precision and recall, we can
    combine the two metrics into a score called F1\. The *F1 score* is the harmonic
    mean of precision and recall, as shown here:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 完美分类器的精确度和召回率均为1，因为假阳性和假阴性的数量将为0。但在实践中，这两个指标是相互矛盾的——你总是在假阳性和假阴性之间做出权衡。随着你减少假阳性并提高精确度，成本将增加假阴性和降低召回率。为了在精确度和召回率之间找到正确的平衡，我们可以将这两个指标结合成一个称为F1的分数。*F1分数*是精确度和召回率的调和平均值，如下所示：
- en: '![](../Images/CH03_F08_Thampi_eqution03.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F08_Thampi_eqution03.png)'
- en: Table 3.1 shows the performance of all three models . They are compared against
    baselines for each subject to see how much of an improvement in performance the
    new models provide. A reasonable baseline used by the superintendent is to predict
    the majority grade (in this case, B) for each subject.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1显示了所有三个模型的性能。它们与每个科目的基线进行比较，以查看新模型提供了多少性能改进。校长使用的一个合理的基线是预测每个科目的大多数成绩（在这种情况下，为B）。
- en: Table 3.1 Performance of math, reading, and writing models
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1 数学、阅读和写作模型的性能
- en: '|  | Precision (%) | Recall (%) | F1 Score (%) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | 精确度（%） | 召回率（%） | F1分数（%） |'
- en: '| Math baseline | 23 | 49 | 32 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 数学基线 | 23 | 49 | 32 |'
- en: '| Math model | 39 | 41 | 39 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 数学模型 | 39 | 41 | 39 |'
- en: '| Reading baseline | 24 | 49 | 32 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 阅读基线 | 24 | 49 | 32 |'
- en: '| Reading model | 39 | 43 | 41 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 阅读模型 | 39 | 43 | 41 |'
- en: '| Writing baseline | 18 | 43 | 25 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 写作基线 | 18 | 43 | 25 |'
- en: '| Writing model | 44 | 45 | 41 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 写作模型 | 44 | 45 | 41 |'
- en: In terms of performance, we can see that the math and reading random forest
    models perform better than the baseline in terms of precision and F1\. The baseline
    math and reading models, however, perform better than the random forest models
    in terms of recall. Because the baseline models always predict the majority class,
    it gets all the majority class predictions right. But the precision metric and
    F1 give us a better measure of the accuracy of all the predictions. The random
    forest model for the writing subject area does better than the baseline for all
    three metrics. The superintendent is happy with this improvement in performance
    but would like to now understand how the model came up with the prediction. In
    sections 3.3 and 3.4, we will see how to interpret a random forest model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在性能方面，我们可以看到，在精确度和F1方面，数学和阅读随机森林模型优于基线模型。然而，在召回率方面，基线数学和阅读模型在随机森林模型中表现更好。因为基线模型总是预测多数类，所以它总是正确地预测所有多数类。但是，精确度指标和F1为我们提供了衡量所有预测准确性的更好指标。写作主题领域的随机森林模型在所有三个指标上都优于基线模型。校长对这种性能提升感到满意，但现在想了解模型是如何得出预测的。在第3.3节和第3.4节中，我们将看到如何解释随机森林模型。
- en: Training AdaBoost and gradient-boosting trees
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 训练AdaBoost和梯度提升树
- en: 'We can train the AdaBoost classifier by using the `AdaBoostClassifier` class
    provided by Scikit-Learn. We initialize an AdaBoost classifier in Python as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Scikit-Learn提供的`AdaBoostClassifier`类来训练AdaBoost分类器。在Python中初始化AdaBoost分类器如下：
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We train a gradient-boosting tree classifier by using the `GradientBoostingClassifier`
    class provided by Scikit-Learn as shown next:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Scikit-Learn提供的`GradientBoostingClassifier`类来训练梯度提升树分类器，如下所示：
- en: '[PRE4]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We train the models the same way as we do the random forest classifier. Variants
    of gradient-boosting trees are available that are faster and scalable, such as
    CatBoost and XGBoost. As an exercise, try training AdaBoost and gradient-boosting
    classifiers for all three subject areas and compare your results with the random
    forest models.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以与随机森林分类器相同的方式进行模型训练。梯度提升树的变体（如CatBoost和XGBoost）更快且可扩展。作为练习，尝试为所有三个主题领域训练AdaBoost和梯度提升分类器，并将你的结果与随机森林模型进行比较。
- en: 3.3 Interpreting a random forest
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 随机森林的解释
- en: 'Random forest is an ensemble of multiple decision trees, so we could look at
    the global relative importance of each feature by averaging the normalized feature
    importance across all decision trees. In chapter 2, we saw how to compute the
    importance of features for a decision tree. This is shown next for a given decision
    tree *t*:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是由多个决策树组成的集成，因此我们可以通过平均所有决策树中每个特征的归一化特征重要性来查看每个特征的全球相对重要性。在第2章中，我们看到了如何计算决策树的特征重要性。以下是一个给定决策树*t*的示例：
- en: '![](../Images/CH03_F08_Thampi_equation04.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F08_Thampi_equation04.png)'
- en: 'To compute the relative importance, we need to normalize the feature importance
    shown previously by dividing it by the sum of all feature importance values, as
    shown next:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算相对重要性，我们需要通过除以所有特征重要性值的总和来归一化之前显示的特征重要性，如下所示：
- en: '![](../Images/CH03_F08_Thampi_equation05.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F08_Thampi_equation05.png)'
- en: 'You can now easily compute the global relative importance of each feature for
    the random forest by averaging the normalized feature importance for that feature
    across all decision trees, as shown next. Note that feature importance is computed
    the same way for AdaBoost and gradient-boosting trees:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以轻松地通过平均所有决策树中该特征的归一化特征重要性来计算随机森林中每个特征的全球相对重要性，如下所示。请注意，特征重要性在AdaBoost和梯度提升树中是按相同方式计算的：
- en: '![](../Images/CH03_F08_Thampi_equation06.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F08_Thampi_equation06.png)'
- en: 'In Python, we can obtain feature importance from the Scikit-Learn random forest
    model and plot it as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，我们可以从Scikit-Learn的随机森林模型中获取特征重要性，并如下所示进行绘图：
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Gets the feature importance of the math random forest model
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ① 获取数学随机森林模型的特征重要性
- en: ② Gets the feature importance of the reading random forest model
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ② 获取阅读随机森林模型的特征重要性
- en: ③ Gets the feature importance of the writing random forest model
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 获取写作随机森林模型的特征重要性
- en: ④ Initializes the list of feature names
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 初始化特征名称列表
- en: The features and their importance values are shown in figure 3.9\. As can be
    seen in the figure, the two most important features for the three subjects are
    Parent Level of Education and the Ethnicity of the student. This is useful information,
    but it does not tell us anything about how the grade is influenced by different
    levels of education and how race and education interact with each other.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 特征及其重要性值显示在图3.9中。如图所示，对于三个主题来说，最重要的两个特征是父母的受教育程度和学生的种族。这是有用的信息，但它并没有告诉我们关于成绩如何受不同教育水平的影响，以及种族和教育如何相互作用的任何信息。
- en: '![](../Images/CH03_F09_Thampi.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图3.9 随机森林模型的特征重要性](../Images/CH03_F09_Thampi.png)'
- en: Figure 3.9 Feature importance of the random forest model
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 随机森林模型的特征重要性
- en: Moreover, we can easily compute and visualize feature importance for tree ensembles,
    but this becomes a lot harder when we look at neural networks and more complex
    black-box models, as will be more apparent in chapter 4\. We, therefore, need
    to look at interpretability techniques that are agnostic to the type of black-box
    model. These model-agnostic methods are introduced in the following section.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以轻松地计算和可视化树集成特征的重要性，但当我们将目光转向神经网络和更复杂的黑盒模型时，这会变得非常困难，这一点在第4章中会更加明显。因此，我们需要考虑对黑盒模型类型无差别的可解释性技术。以下章节将介绍这些模型无关的方法。
- en: '3.4 Model-agnostic methods: Global interpretability'
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 模型无关方法：全局可解释性
- en: So far, we have been looking at interpretability techniques that are model-specific
    or dependent. For white-box models, we saw how to interpret linear regression
    models using the weights learned by the method of least squares. We interpreted
    decision trees by visualizing them as binary trees where each node splits the
    data using a feature determined using the CART algorithm. We were also able to
    visualize the global importance of features, the computation of which was specific
    to the model. We interpreted GAMs by visualizing the average effect of the basis
    splines for an individual feature on the target and then marginalizing the rest
    of the features. These visualizations were called partial dependence, or partial
    effect plots.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在关注特定模型或依赖的可解释性技术。对于白盒模型，我们看到了如何使用最小二乘法学习到的权重来解释线性回归模型。我们通过将决策树可视化为一棵二叉树来解释它们，其中每个节点使用CART算法确定的特征来分割数据。我们还能够可视化特征的全局重要性，其计算对模型是特定的。我们通过可视化单个特征对目标变量的平均基函数效应，然后对其他特征进行边缘化来解释GAMs。这些可视化被称为部分依赖图或部分效应图。
- en: For black-box models like tree ensembles, we can compute the global relative
    importance of features, but we cannot extend this computation to other black-box
    models like neural networks. To better interpret black-box models, we will now
    explore model-agnostic methods that can be applied to any type of model. We will
    also focus our attention in this chapter on interpretability techniques that are
    global in scope. Global interpretability techniques aim to give a better understanding
    of the model as a whole, that is, the global effects of the features on the target
    variable. One globally interpretable model-agnostic method is partial dependence
    plots (PDPs). You will see in the following section how to extend the PDPs that
    you learned for GAMs in chapter 2 to black-box models like random forest. We will
    formalize the definition of PDPs and also see how to extend PDPs to visualize
    interactions between any two features to validate whether the model has picked
    up on any dependence between them.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像树集成这样的黑盒模型，我们可以计算特征的全球相对重要性，但不能将这种计算扩展到其他黑盒模型，如神经网络。为了更好地解释黑盒模型，我们现在将探索可以应用于任何类型模型的模型无关方法。在本章中，我们还将关注全局范围内的可解释性技术。全局可解释性技术旨在更好地理解模型的整体，即特征对目标变量的全局影响。一个全局可解释的模型无关方法是部分依赖图（PDPs）。你将在下一节中看到如何将你在第2章中学到的用于GAMs的部分依赖图扩展到像随机森林这样的黑盒模型。我们将正式定义PDPs，并了解如何将PDPs扩展以可视化任意两个特征之间的交互，以验证模型是否捕捉到了它们之间的任何依赖关系。
- en: Model-agnostic interpretability techniques can also be local in scope. We can
    use these techniques to interpret a model for a given local instance or prediction.
    Techniques such as LIME, SHAP, and anchors are model-agnostic and local in scope,
    and you will learn more about them in chapter 4.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 模型无关的可解释性技术也可以是局部的。我们可以使用这些技术来解释给定局部实例或预测的模型。例如，LIME、SHAP和锚定技术是模型无关且局部的，你将在第4章中了解更多关于它们的内容。
- en: 3.4.1 Partial dependence plots
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 偏依赖图
- en: As we saw with GAMs in chapter 2, the idea behind partial dependence plots (PDPs)
    is to show the marginal or average effects of different feature values on the
    model prediction. Let f be the function learned by the model. For the high school
    student prediction problem, let *f*[math], *f*[reading], and *f*[writing] be the
    functions learned by the random forest models trained for the math, reading, and
    writing subject areas, respectively. For each subject, the function returns the
    probability of receiving a certain grade given the input features. Let’s now focus
    on the math random forest model for ease of understanding. You can easily extend
    the theory that you will learn now to the other subject areas.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第2章中看到的 GAMs 一样，偏依赖图（PDPs）背后的思想是展示不同特征值对模型预测的边缘或平均效应。设 f 为模型学习到的函数。对于高中学生预测问题，设
    *f*[math]、*f*[reading] 和 *f*[writing] 分别为为数学、阅读和写作学科领域训练的随机森林模型学习到的函数。对于每个学科，该函数返回给定输入特征下获得一定等级的概率。现在让我们专注于数学随机森林模型以便于理解。你可以轻松地将你现在学到的理论扩展到其他学科领域。
- en: 'Suppose that for the math random forest model, we want to understand what effect
    different parent levels of education have on predicting a given grade. To achieve
    this, we will need to do the following:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 假设对于数学随机森林模型，我们想了解不同的父母教育水平对预测给定等级有什么影响。为了实现这一点，我们需要做以下几步：
- en: Use the same values for the rest of the features as were used in the dataset.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用与数据集中相同的值来设置其余特征。
- en: Create an artificial dataset by setting Parent Level of Education to be the
    value of interest for all data points—if you are interested in looking at the
    average effects of high school education on the student’s grade, then set Parent
    Level of Education to be high school for all data points.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将父母教育水平设置为所有数据点的感兴趣值来创建一个人工数据集——如果你对查看高中教育对学生成绩的平均效应感兴趣，那么将父母教育水平设置为所有数据点的高中教育。
- en: Run through the model, and obtain the predictions for all data points in this
    artificial set.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行模型，并获取这个人工集中所有数据点的预测。
- en: Take the average of the predictions to determine the overall average effect
    for that Parent Level of Education.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 取预测的平均值以确定该父母教育水平的整体平均效应。
- en: 'More formally, if we want to plot the partial dependence of feature *S*, we
    marginalize on the rest of the features represented as set *C*, set feature *S*
    to be the value of interest, and then look at the average effect of the math model
    for feature *S*, assuming values of all the features in set *C* are known. Let’s
    look at a concrete example. Suppose we are interested in understanding the marginal
    effects of Parent Level of Education on students’ math grades. In this case, feature
    *S* is Parent Level of Education, and the rest of the features are represented
    as *C*. To understand the effect of, say, a high school level of education, we
    set feature *S* to the value corresponding to high school education (the value
    of interest) and take the average of the math model’s output, assuming we know
    the values of the rest of the features. Mathematically, this is shown by the following
    equation:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，如果我们想绘制特征 *S* 的偏依赖图，我们就在其他特征（表示为集合 *C*）上求边缘，将特征 *S* 设置为感兴趣的值，然后观察数学模型对特征
    *S* 的平均影响，假设集合 *C* 中所有特征的值都是已知的。让我们来看一个具体的例子。假设我们想了解父母教育水平对学生数学成绩的边缘效应。在这种情况下，特征
    *S* 是父母教育水平，其余的特征表示为 *C*。为了了解，比如说，高中教育水平的影响，我们将特征 *S* 设置为对应高中教育的值（感兴趣的值），并取数学模型输出的平均值，假设我们知道其余特征的值。数学上，这可以通过以下方程表示：
- en: '![](../Images/CH03_F09_Thampi_eqution06.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F09_Thampi_eqution06.png)'
- en: In this equation, the partial function for feature *S* is obtained by computing
    the average of the learned function *f*[math], assuming the values for features
    in set *C* are known for all the examples in the training set, represented as
    *n*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，特征 *S* 的偏函数是通过计算学习函数 *f*[math] 的平均值得到的，假设训练集中所有示例的特征 *C* 的值都是已知的，表示为
    *n*。
- en: It is important to note that the PDP cannot be trusted if feature *S* is correlated
    with features in set *C*. Why is that? To determine the average effects of a given
    value for feature *S*, we are creating an artificial dataset where we use the
    actual feature values for all the other features in set *C* but change the value
    of feature *S* to be the one of interest. If feature *S* is highly correlated
    with any feature in set *C*, we could be creating an artificial dataset that is
    highly unlikely. Let’s look at a concrete example. Suppose we are interested in
    understanding the average effects of a high school level of education for a student’s
    parent on their grade. We will be setting Parent Level of Education as high school
    for all the instances in our training set. Now, if Parent Level of Education is
    highly correlated with Ethnicity, where we know Parent Level of Education given
    the Ethnicity, we could have an instance where it is highly unlikely for parents
    belonging to a certain ethnicity to have just a high school education. We are,
    therefore, creating an artificial dataset whose distribution does not match the
    original training data. Because the model has not been exposed to that distribution
    of the data, the predictions from that model may be way off, resulting in untrustworthy
    PDPs. We will come back to this limitation in section 3.4.2.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，如果特征 *S* 与集合 *C* 中的特征相关联，则 PDP（部分依赖图）不可信。为什么是这样呢？为了确定特征 *S* 的给定值的平均效应，我们正在创建一个人工数据集，其中我们使用集合
    *C* 中所有其他特征的原始特征值，但将特征 *S* 的值更改为感兴趣的值。如果特征 *S* 与集合 *C* 中的任何特征高度相关，我们可能会创建一个极不可能的人工数据集。让我们看一个具体的例子。假设我们感兴趣的是了解学生家长的高中教育水平对其成绩的平均影响。我们将设置训练集中所有实例的家长教育水平为高中。现在，如果家长教育水平与种族高度相关，我们知道给定种族的家长教育水平，那么我们可能会遇到一个不太可能的情况，即属于某个种族的家长只有高中教育。因此，我们正在创建一个与原始训练数据分布不匹配的人工数据集。由于模型尚未接触到该数据分布，该模型的预测可能会非常不准确，导致不可信的
    PDP。我们将在第 3.4.2 节中回到这个限制。
- en: 'Let’s now see how to implement PDPs. In Python, you can use the implementation
    provided by Scikit-Learn, but this limits you to gradient-boosted regressors or
    classifiers. A better implementation in Python that is truly model-agnostic is
    PDPBox, developed by Jiangchun Lee. You can install this library as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看如何实现 PDP。在 Python 中，您可以使用 Scikit-Learn 提供的实现，但这将限制您使用梯度提升回归器或分类器。Python
    中更好的实现是 PDPBox，由 Jiangchun Lee 开发。您可以通过以下方式安装此库：
- en: '[PRE6]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now let’s see PDPs in action. We will first focus on the most important feature,
    which you learned in section 3.3 is Parent Level of Education (see figure 3.9).
    We can look at the influence of different levels of education on predicting grades
    A, B, C, and F for math as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看 PDP 的实际应用。我们首先关注最重要的特征，即您在第 3.3 节中学到的家长教育水平（见图 3.9）。我们可以查看不同教育水平对预测数学成绩
    A、B、C 和 F 的影响如下：
- en: '[PRE7]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① Imports the PDP function from PDPBox
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从 PDPBox 导入 PDP 函数
- en: ② Extracts only the label-encoded feature columns
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ② 仅提取标签编码的特征列
- en: ③ Obtains the partial dependence function for each level of education by passing
    in the learned math random forest model
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 通过传递学习到的数学随机森林模型，为每个教育水平获取部分依赖函数
- en: ④ Uses the preloaded dataset
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用预加载数据集
- en: ⑤ Marginalizes on all the other features except Parent Level of Education
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 除了家长教育水平之外，对其他所有特征进行边缘化
- en: ⑥ Initializes the labels for the xticks starting from the lowest level of education
    until the highest
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 从最低教育水平开始初始化 xticks 的标签，直到最高水平
- en: The plot generated by this code snippet is shown in figure 3.10\. The partial
    dependence of Parent Level of Education is shown separately for each grade—A,
    B, C, and F. The range of values for the partial dependence function is between
    0 and 1 because the learned math model *f*unction for this classifier is a probability
    measure that ranges from 0 to 1\. Let’s now zoom in on a couple of grades to analyze
    the impact of Parent Level of Education on the student’s grade.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码片段生成的图示如图 3.10 所示。家长教育水平的部分依赖图分别显示在每个等级——A、B、C 和 F 上。部分依赖函数的值域在 0 到 1 之间，因为该分类器的学习数学模型
    *f* unction 是一个范围在 0 到 1 之间的概率度量。现在让我们放大几个等级来分析家长教育水平对学生成绩的影响。
- en: '![](../Images/CH03_F10_Thampi.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F10_Thampi.png)'
- en: Figure 3.10 PDP of various Parent Level of Education features for math grades
    A, B, C, and F
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 各种家长教育水平特征对数学成绩 A、B、C 和 F 的 PDP
- en: In figure 3.11, we have zoomed in on the PDP for math grade A. We saw in section
    3.1.1 that the proportion of students getting grade A in math is higher when the
    parent has a master’s degree than when the parent has a high school degree (see
    figure 3.4). Has the random forest model learned this pattern? We can see from
    figure 3.11 that the impact on getting grade A increases as the Parent Level of
    Education increases. For a parent with a high school education, the effect on
    predicting grade A in math is negligible—close to 0\. This means that having a
    high school education does not change anything for the model, and other features
    besides Parent Level of Education come into play when predicting grade A. We can,
    however, see a high positive impact of roughly +0.2 when the parent has a master’s
    degree. This means that on average, a master’s degree pushes the probability of
    a student getting grade A by roughly 0.2.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3.11中，我们放大了数学成绩A的PDP。我们在3.1.1节中看到，当父母拥有硕士学位时，数学成绩A的学生比例高于父母拥有高中文凭时（见图3.4）。随机森林模型是否学习了这种模式？从图3.11中我们可以看到，随着父母受教育程度的提高，获得成绩A的影响也在增加。对于受过高中教育的父母，预测数学成绩A的影响可以忽略不计——接近0。这意味着高中教育对模型没有任何影响，并且在预测成绩A时，除了父母的受教育程度之外的其他特征开始发挥作用。然而，当父母拥有硕士学位时，我们可以看到大约+0.2的高正影响。这意味着平均而言，硕士学位将学生获得成绩A的概率提高了大约0.2。
- en: '![](../Images/CH03_F11_Thampi.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F11_Thampi.png)'
- en: Figure 3.11 Interpreting the Parent Level of Education PDP for math grade A
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 解释数学成绩A的父母的受教育程度PDP
- en: In figure 3.12, we have zoomed in on the PDP of math grade F. We can notice
    a downward trend for grade F—the more educated the parent is, the more of a negative
    impact on predicting grade F. We can see that a student whose parent has a master’s
    degree has a negative impact of roughly –0.05 on average in predicting grade F.
    This means that the parent having a master’s degree decreases the likelihood of
    the student getting grade F and, therefore, increases the likelihood of the student
    getting grade A. This insight is great and would not have been possible by just
    looking at the feature importance. The end user of this system (i.e., the superintendent)
    will have more trust in the model that she is using.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3.12中，我们放大了数学成绩F的PDP。我们可以注意到成绩F呈下降趋势——父母受教育程度越高，对预测成绩F的负面影响越大。我们可以看到，父母拥有硕士学位的学生在预测成绩F时平均有大约-0.05的负面影响。这意味着拥有硕士学位的父母降低了学生获得成绩F的可能性，因此增加了学生获得成绩A的可能性。这个见解很棒，仅通过查看特征重要性是无法实现的。该系统的最终用户（即校长）将对她所使用的模型有更多的信任。
- en: '![](../Images/CH03_F12_Thampi.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F12_Thampi.png)'
- en: Figure 3.12 Interpreting the Parent Level of Education PDP for math grade F
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 解释数学成绩F的父母的受教育程度PDP
- en: As an exercise, I encourage you to extend the PDP code for math grade and Parent
    Level of Education to the other subject areas—reading and writing. You can check
    whether the patterns observed in section 3.1.1 (see figure 3.4) are learned by
    the random forest models. You can also extend the code to other features. As an
    exercise, pick the second most important feature, which is Race or Ethnicity of
    the student, and generate the PDP for that feature.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项练习，我鼓励你将数学成绩和父母受教育程度的PDP代码扩展到其他学科领域——阅读和写作。你可以检查在3.1.1节中观察到的模式是否被随机森林模型学习（见图3.4）。你还可以将代码扩展到其他特征。作为练习，选择第二重要的特征，即学生的种族或民族，并生成该特征的PDP。
- en: 3.4.2 Feature interactions
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 特征交互
- en: 'We can extend PDPs to understand feature interactions. Returning to the equation
    in section 3.4.1, we will now look at two features in set S and marginalize on
    the rest. Let’s examine the interactions between the two most important features—Parent
    Level of Education and student Ethnicity—in predicting grades A, B, C, and F in
    math. Using PDPBox, we can easily visualize pairwise feature interactions as shown
    in the following code snippet:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将PDPs扩展以理解特征交互。回到3.4.1节中的方程，我们现在将关注集合S中的两个特征，并对其他特征进行边缘化。让我们检查两个最重要的特征——父母的受教育程度和学生的种族——在预测数学成绩A、B、C和F之间的交互作用。使用PDPBox，我们可以轻松地可视化成对的特征交互，如下面的代码片段所示：
- en: '[PRE8]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① Obtains the feature interactions between two features for the learned math
    random forest model
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ① 获取学习数学随机森林模型中两个特征之间的特征交互
- en: ② Uses the preloaded dataset
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用预加载的数据集
- en: ③ Sets the feature column names
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 设置特征列名称
- en: ④ List of the features for which to obtain the feature interactions
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 获取特征交互的特定期表
- en: The plot generated by this code is shown in figure 3.13\. There are four plots
    generated, one for each grade. Feature interactions are visualized in a 2-D grid
    where the six Parent Level of Education features are on the *y*-axis and the five
    Ethnicity features are on the *x*-axis. I’ll zoom in on grade A to decompose and
    explain this plot further.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 由该代码生成的图示显示在图3.13中。共生成了四个图，每个图对应一个等级。特征交互在二维网格中可视化，其中六个家长教育水平特征位于*y*轴上，五个族裔特征位于*x*轴上。我将放大查看等级A，进一步分解和解释此图。
- en: '![](../Images/CH03_F13_Thampi.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F13_Thampi.png)'
- en: Figure 3.13 Interaction between Parent Level of Education and Ethnicity for
    all math grades A, B, C, and F
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13 家长教育水平和族裔对所有数学等级A、B、C和F的交互
- en: Figure 3.14 shows the feature interaction plot for math grade A. Parent Level
    of Education is on the *y*-axis, and the anonymized Ethnicity of the student is
    on the *x*-axis. As you go from the bottom to the top of the *y*-axis, Parent
    Level of Education increases from high school all the way to a master’s degree.
    High school education is represented by a value of 0, and a master’s degree is
    represented by a value of 5\. The *x*-axis shows the five distinct ethnicity groups—A,
    B, C, D, and E. Ethnicity group A is represented by a value of 0, group B is represented
    by a value of 1, group C by a value of 2, and so on. The number in each cell represents
    the impact of a given Parent Level of Education and student Ethnicity on getting
    grade A.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14显示了数学等级A的特征交互图。家长教育水平位于*y*轴上，学生的匿名族裔位于*x*轴上。从*y*轴的底部到顶部，家长教育水平从高中一直增加到硕士学位。高中教育用0表示，硕士学位用5表示。*x*轴显示了五个不同的族裔群体——A、B、C、D和E。族裔群体A用0表示，群体B用1表示，群体C用2表示，依此类推。每个单元格中的数字表示给定家长教育水平和学生族裔对获得等级A的影响。
- en: '![](../Images/CH03_F14_Thampi.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F14_Thampi.png)'
- en: Figure 3.14 Zooming in on math grade A and decomposing the feature interaction
    plot
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14 放大查看数学等级A并分解特征交互图
- en: For instance, the cell in the bottom-most row and the left-most column represents
    the average impact of a student belonging to ethnicity group A and having a parent
    who has a high school education on getting grade A. Please also note the numerical
    values in each cell of the grid—a lower number represents lower impact, and a
    higher number represents a higher impact in predicting grade A.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，最底部行和最左侧列的单元格表示属于族裔群体A且家长拥有高中教育背景的学生在获得等级A上的平均影响。请注意网格中每个单元格的数值——数字越低表示影响越低，数字越高表示在预测等级A上的影响越高。
- en: Now let’s focus on ethnicity group A, which is the left-most column in the grid,
    highlighted in figure 3.15\. You can see that as Parent Level of Education increases,
    the impact on predicting grade A also increases. This makes sense because it shows
    that Parent Level of Education has more influence on the grade than Ethnicity.
    This is validated by the feature importance plot shown in figure 3.9 as well.
    The model has, therefore, learned this pattern well.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们专注于族裔群体A，它是网格中最左侧的列，如图3.15所示。您可以看到，随着家长教育水平的提高，预测等级A的影响也增加。这很有道理，因为它表明家长教育水平对等级的影响大于族裔。这一点也由图3.9中显示的特征重要性图得到验证。因此，模型已经很好地学习了这种模式。
- en: '![](../Images/CH03_F15_Thampi.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F15_Thampi.png)'
- en: Figure 3.15 Impact of predicting grade A by conditioning on Ethnicity group
    A
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.15 通过对族裔群体A进行条件预测等级A的影响
- en: But what is going on with ethnicity group C, the third column, highlighted in
    figure 3.16? It looks like a student whose parent has a high school degree has
    a higher positive impact in predicting grade A than a student whose parent has
    a master’s degree (compare the bottom-most cell with the top-most cell of the
    highlighted column). It also looks like a student whose parent has an associate
    degree has the highest positive impact in predicting grade A than any other level
    of education (see the third cell from the top in the highlighted column).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 但族裔群体C，即图3.16中突出显示的第三列，发生了什么？看起来一个家长拥有高中学位的学生在预测等级A时比一个家长拥有硕士学位的学生有更高的积极影响（比较突出列的最底部单元格与最顶部单元格）。看起来一个家长拥有副学士学位的学生在预测等级A时比其他任何教育水平都有更高的积极影响（参见突出列中从顶部数起的第三个单元格）。
- en: '![](../Images/CH03_F16_Thampi.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH03_F16_Thampi.png)'
- en: Figure 3.16 Impact of predicting grade A by conditioning on Ethnicity group
    C
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16 在种族组C的条件下预测A等级的影响
- en: 'This is a bit concerning because it may expose one or more of the following
    problems:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点令人担忧，因为它可能暴露以下一个或多个问题：
- en: Parent Level of Education might be correlated with the Ethnicity feature and,
    therefore, results in untrustworthy feature interaction plots.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 父母教育水平可能与种族特征相关，因此导致不可信的特征交互图。
- en: The dataset does not properly represent the population, especially Ethnicity
    group C. This is called sampling bias.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集没有正确地代表人口，特别是种族组C。这被称为抽样偏差。
- en: The model is biased and has not learned the interaction between Parent Level
    of Education and Ethnicity well.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是有偏见的，并且没有很好地学习父母教育水平与种族之间的交互。
- en: The dataset exposes a bias that is systemic in society.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集揭示了社会中系统性的偏差。
- en: The first problem exposes the limitation of PDPs, and we will discuss this limitation
    in the following paragraph. The second problem can be solved by collecting more
    data that is representative of the population. You will learn about other forms
    of bias and how to mitigate them in chapter 8\. The third problem can be solved
    by adding or engineering more features or by training a better, more complex model.
    The last problem is much harder to solve, requiring better policies and laws,
    and this is beyond the scope of this book.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题揭示了PDPs的限制，我们将在下一段中讨论这个限制。第二个问题可以通过收集更多代表人口的代表性数据来解决。你将在第8章中了解其他形式的偏差以及如何减轻它们。第三个问题可以通过添加或工程更多特征，或者通过训练一个更好、更复杂的模型来解决。最后一个问题要难得多，需要更好的政策和法律，这超出了本书的范围。
- en: 'To check whether the first problem exists, let’s look at the correlation between
    Parent Level of Education and Ethnicity. We saw in chapter 2 how to compute and
    visualize the correlation matrix. We used the Pearson correlation coefficient
    to quantify the correlation between the features for that problem. This coefficient
    can be used only for numerical features and not for categorical features. Because
    we are dealing with categorical features in this example, we have to use a different
    metric. We can use the *Cramer’s V statistic* here because it measures the association
    between two categorical variables. This statistic can be between 0 and 1, where
    0 signifies no correlation/association and 1 signifies maximum correlation/association.
    The following helper function can be used to compute this statistic:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查第一个问题是否存在，让我们看看父母教育水平与种族之间的相关性。我们在第二章中介绍了如何计算和可视化相关性矩阵。我们使用皮尔逊相关系数来量化该问题中特征之间的相关性。这个系数只能用于数值特征，而不能用于分类特征。因为我们在这个例子中处理的是分类特征，所以我们必须使用不同的指标。我们可以使用*Cramer's
    V统计量*，因为它衡量的是两个分类变量之间的关联。这个统计量可以在0到1之间，其中0表示没有相关性/关联，1表示最大相关性/关联。以下辅助函数可以用来计算这个统计量：
- en: '[PRE9]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can compute the correlation between Parent Level of Education and Ethnicity
    as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下计算父母教育水平与种族之间的相关性：
- en: '[PRE10]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: By executing these lines of code, we can see that the correlation or association
    between Parent Level of Education and Ethnicity is *0.0486*. This is quite low,
    and we can, therefore, rule out the issue of the feature interaction plot or PDP
    being untrustworthy.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行这些代码行，我们可以看到父母教育水平与种族之间的相关性或关联为*0.0486*。这相当低，因此我们可以排除特征交互图或PDP不可信的问题。
- en: We saw in figure 3.5 that students belonging to group C perform better in general
    than students belonging to group A. It could be the case that the model has learned
    this pattern. We can validate it by looking at the top-most legend in figures
    3.14, 3.15, and 3.16\. If the student belongs to group C, it has a positive impact
    of +0.153 on predicting grade A, which is greater than the impact that student
    has when belonging to group A, which is +0.125\. Let us now look at the difference
    in the distributions of Parent Level of Education between Ethnicity groups A and
    C, shown in figure 3.17.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图3.5中看到，属于C组的学生的表现总体上优于属于A组的学生的表现。可能的情况是模型已经学会了这种模式。我们可以通过查看图3.14、3.15和3.16中最上面的图例来验证它。如果学生属于C组，对预测A等级有+0.153的积极影响，这比学生属于A组时的+0.125的影响要大。现在让我们来看看图3.17中显示的A族和C族之间父母教育水平的分布差异。
- en: '![](../Images/CH03_F17_Thampi.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F17_Thampi.png)'
- en: Figure 3.17 Comparison of Parent Level of Education distributions of the overall
    population with Ethnicity groups A and C
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17 总体人口与种族群体A和C的家长教育水平分布比较
- en: In figure 3.17, we can see that parents of students belonging to Ethnicity group
    A are much more likely to have high school or some high school education than
    the overall population and students belonging to group C. It also looks like group
    C has a higher proportion of students whose parents have an associate degree than
    the overall population and group A. The differences in distributions are quite
    striking. We are not sure if the dataset represents the overall population and
    each Ethnicity group accurately. As a data scientist, it is important to highlight
    this problem to the stakeholder (the superintendent, in this example) and ensure
    that the dataset is legitimate and that there is no sampling bias.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在图3.17中，我们可以看到，属于种族群体A的学生家长比总体人口和属于群体C的学生家长更有可能拥有高中或某些高中教育。看起来群体C的学生家长拥有副学士学位的比例也比总体人口和群体A高。分布的差异非常明显。我们不确定数据集是否准确地代表了总体人口和每个种族群体。作为数据科学家，向利益相关者（例如本例中的校长）强调这个问题，并确保数据集是合法的，且没有抽样偏差，这一点非常重要。
- en: The important point to take away from this section is that interpretability
    techniques, especially PDPs and feature interactions, are great tools for exposing
    potential problems with the model or the data before the model is deployed into
    production. None of the insights in this section would have been possible by just
    looking at the feature importance. As an exercise, I encourage you to use the
    PDPBox package on other black-box models, such as gradient-boosting trees.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 从本节中可以吸取的重要观点是，可解释性技术，尤其是PDPs和特征交互，是暴露模型或数据潜在问题的强大工具，在模型部署到生产之前。本节中的所有见解仅通过查看特征重要性是无法实现的。作为练习，我鼓励您使用PDPBox包在其他黑盒模型上，例如梯度提升树。
- en: Accumulated local effects (ALE)
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 累积局部效应（ALE）
- en: We have seen in this chapter that PDPs and feature interaction plots based on
    them are not trustworthy if the features are correlated with each other. An interpretability
    technique that is unbiased and overcomes the limitation of PDPs is accumulated
    local effects (ALE). This technique was proposed in 2016 by Daniel W. Apley and
    Jingyu Zhu. At the time of writing, ALE is implemented only in the R programming
    language. A Python implementation is still a work in progress and does not support
    categorical features yet. Because the implementation of ALE is not mature yet,
    we will cover this technique in greater depth in a later release of this book.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经看到，如果特征之间存在相关性，基于它们的PDPs（部分依赖图）和特征交互图就不可信。一种无偏且克服PDPs局限性的可解释性技术是累积局部效应（ALE）。这项技术由Daniel
    W. Apley和Jingyu Zhu于2016年提出。在撰写本文时，ALE仅在R编程语言中得到实现。Python实现仍在进行中，并且目前尚不支持分类特征。由于ALE的实现还不够成熟，我们将在本书的后续版本中更深入地介绍这项技术。
- en: In the next chapter, we will enter the world of black-box neural networks. This
    may seem like a pretty big jump because neural networks are inherently complex
    and, therefore, require more sophisticated interpretability techniques to understand
    them. We will specifically focus on model-agnostic techniques that are local in
    scope, such as LIME, SHAP, and anchors.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将进入黑盒神经网络的世界。这可能看起来是一个相当大的跳跃，因为神经网络本质上是复杂的，因此需要更复杂的可解释性技术来理解它们。我们将特别关注局部范围内的模型无关技术，例如LIME、SHAP和锚点。
- en: Summary
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Model-agnostic interpretability techniques are not dependent on the specific
    type of model being used. They can be applied to any model because they are independent
    of the internal structure of the model.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型无关的可解释性技术不依赖于所使用的特定模型类型。由于它们独立于模型的内部结构，因此可以应用于任何模型。
- en: Interpretability techniques that are global in scope will help us understand
    the entire model as a whole.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 范围全局的可解释性技术将帮助我们理解整个模型。
- en: 'To overcome the problem of overfitting, we can combine or ensemble decision
    trees in two broad ways: bagging and boosting.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了克服过拟合的问题，我们可以以两种广泛的方式结合或集成决策树：袋装和提升。
- en: Using the bagging technique, we train multiple decision trees in parallel on
    separate random subsets of the training data. We use these individual decision
    trees to make predictions and combine them by taking an average to come up with
    the final prediction. Random forest is a tree ensemble that uses the bagging technique.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用袋装技术，我们在训练数据的独立随机子集上并行训练多个决策树。我们使用这些单独的决策树进行预测，并通过取平均值来得出最终的预测。随机森林是一种使用袋装技术的树集成。
- en: Like in bagging, the boosting technique also trains multiple decision trees,
    but in sequence. The first decision tree is typically a shallow tree and is trained
    on the training set. The objective of the second decision tree is to learn from
    the errors made by the first tree and to further improve the performance. Using
    this technique, we string multiple decision trees together, and they iteratively
    try to optimize and reduce the errors made by the previous one. Adaptive boosting
    and gradient boosting are two common boosting algorithms.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与袋装技术类似，提升技术也训练多个决策树，但顺序不同。第一个决策树通常是浅层树，并在训练集上训练。第二个决策树的目的是从第一个决策树的错误中学习，并进一步提高性能。使用这种技术，我们将多个决策树串联起来，它们迭代地尝试优化和减少前一个决策树所犯的错误。自适应提升和梯度提升是两种常见的提升算法。
- en: We can train a random forest model for classification tasks in Python using
    the `RandomForestClassifier` class provided by the Scikit-Learn package. This
    implementation will also help you easily compute the global relative importance
    of features.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用Scikit-Learn包提供的`RandomForestClassifier`类在Python中为分类任务训练随机森林模型。这种实现还将帮助您轻松计算特征的全球相对重要性。
- en: We can train the Scikit-Learn adaptive-boosting and gradient-boosting classifiers
    by using the Scikit-Learn `AdaBoostClassifier` and `GradientBoostingClassifier`
    classes, respectively. Variants of gradient-boosting trees are available that
    are faster and scalable, such as `CatBoost` and `XGBoost`.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过使用Scikit-Learn的`AdaBoostClassifier`和`GradientBoostingClassifier`类分别训练Scikit-Learn的自适应提升和梯度提升分类器。梯度提升树的变体更快且可扩展，例如`CatBoost`和`XGBoost`。
- en: For tree ensembles, we can compute the global relative importance of features,
    but we cannot extend this computation to other black-box models like neural networks.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于树集成，我们可以计算特征的全球相对重要性，但不能将这种计算扩展到其他黑盒模型，如神经网络。
- en: Partial dependence plot (PDP) is a global, model-agnostic interpretability technique
    that we can use to understand the marginal or average effects of different feature
    values on the model prediction. PDPs cannot be trusted if features are correlated
    with each other. We can implement PDPs using the `PDPBox` Python package.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部分依赖图（PDP）是一种全局、模型无关的可解释技术，我们可以用它来理解不同特征值对模型预测的边缘或平均效应。如果特征之间存在相关性，则PDPs不可信。我们可以使用`PDPBox`
    Python包来实现PDPs。
- en: PDPs can be extended to understand feature interactions as well. PDPs and feature
    interaction plots can be used to expose possible issues such as sampling bias
    and model bias.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PDPs可以扩展以理解特征交互。PDPs和特征交互图可以用来揭示可能的问题，如采样偏差和模型偏差。
