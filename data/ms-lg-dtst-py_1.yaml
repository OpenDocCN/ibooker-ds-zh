- en: Part 2\.
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二部分
- en: '[Part 2](#part02) teaches how to use two popular open source distributed computing
    frameworks: Hadoop and Spark. Hadoop is the originator and foundation of contemporary
    distributed computing. We’ll explore how to use Hadoop streaming and how to write
    Hadoop jobs with the mrjob library. We’ll also learn Spark, a modern distributed
    computing framework that can take full advantage of the latest, high-memory compute
    resources. You can use the tools and techniques in this part for large data in
    categories 2 and 3: tasks that needs parallelization to finish in a reasonable
    amount of time.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[第二部分](#part02) 介绍了如何使用两个流行的开源分布式计算框架：Hadoop 和 Spark。Hadoop 是当代分布式计算的创始者和基础。我们将探讨如何使用
    Hadoop 流和如何使用 mrjob 库编写 Hadoop 作业。我们还将学习 Spark，这是一个现代分布式计算框架，可以充分利用最新的、高内存的计算资源。您可以使用本部分中的工具和技术来处理类别
    2 和 3 中的大数据：需要并行化以在合理时间内完成的任务。'
- en: Chapter 7\. Processing truly big datasets with Hadoop and Spark
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7章\. 使用 Hadoop 和 Spark 处理真正的大数据集
- en: '*This chapter covers*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Recognizing the `reduce` pattern for N-to-X data transformations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别 `reduce` 模式进行 N-to-X 数据转换
- en: Writing helper functions for reductions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写用于归约的辅助函数
- en: Writing lambda functions for simple reductions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写用于简单归约的 lambda 函数
- en: Using `reduce` to summarize data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `reduce` 汇总数据
- en: In the previous chapters of the book, we’ve focused on developing a foundational
    set of programming patterns—in the map and reduce style—that allow us to scale
    our programming. We can use the techniques we’ve covered so far to make the most
    of our laptop’s hardware. I’ve shown you how to work on large datasets using techniques
    like `map` ([chapter 2](kindle_split_011.html#ch02)), `reduce` ([chapter 5](kindle_split_014.html#ch05)),
    parallelism ([chapter 2](kindle_split_011.html#ch02)), and lazy programming ([chapter
    4](kindle_split_013.html#ch04)). In this chapter, we begin to look at working
    on big datasets beyond our laptop.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前几章中，我们专注于开发一组基础编程模式——以 map 和 reduce 风格——允许我们扩展我们的编程。我们可以使用我们迄今为止所介绍的技术来充分利用我们的笔记本电脑的硬件。我已经向您展示了如何使用
    `map` ([第2章](kindle_split_011.html#ch02))、`reduce` ([第5章](kindle_split_014.html#ch05))、并行性
    ([第2章](kindle_split_011.html#ch02)) 和惰性编程 ([第4章](kindle_split_013.html#ch04))
    等技术来处理大型数据集。在本章中，我们开始探讨在笔记本电脑之外处理大型数据集。
- en: 'In this chapter, we introduce distributed computing—that is, computing that
    occurs on more than one computer—and two technologies we’ll use to do distributed
    computing: Apache Hadoop and Apache Spark. Hadoop is a set of tools that support
    distributed map and reduce style programming through Hadoop MapReduce. Spark is
    an analytics toolkit designed to modernize Hadoop. We’ll focus on Hadoop for batch
    processing of big datasets and focus on applying Spark in analytics and machine
    learning use cases.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了分布式计算——即在多台计算机上发生的计算——以及我们将用于进行分布式计算的两个技术：Apache Hadoop 和 Apache Spark。Hadoop
    是一套工具，通过 Hadoop MapReduce 支持分布式 map 和 reduce 风格的编程。Spark 是一个分析工具包，旨在使 Hadoop 现代化。我们将专注于
    Hadoop 的批处理大型数据集，并专注于在分析和机器学习用例中应用 Spark。
- en: 7.1\. Distributed computing
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 分布式计算
- en: In this chapter, we’ll review the basics of distributed computing—a method of
    computing where we share not just a single workflow, but tasks and data long-term
    across a network of computers. Computing in this way has challenges, such as keeping
    track of all our data and coordinating our work, but offers large benefits in
    speed when we can parallelize our work.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将回顾分布式计算的基础——这是一种计算方法，我们不仅共享单个工作流程，而且长期在计算机网络上共享任务和数据。以这种方式进行计算具有挑战性，例如跟踪所有我们的数据和协调我们的工作，但当我们能够并行化我们的工作时，它提供了速度上的巨大好处。
- en: In [chapter 1](kindle_split_010.html#ch01), I laid out three sizes of datasets.
    Those that are
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第1章](kindle_split_010.html#ch01) 中，我概述了三种数据集的大小。那些
- en: small enough to work with in memory on a single computer
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 足够小，可以在单台计算机的内存中处理
- en: too big to work with in memory on a single computer but small enough that we
    can process them with a single computer
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 太大，无法在单台计算机的内存中处理，但足够小，我们可以用单台计算机处理它们
- en: both too big to fit into memory on a single computer and too big to process
    on a single computer
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 既有太大无法适应单台计算机的内存，又有太大无法在单台计算机上处理
- en: 'The first dataset size poses no inherent challenges: most developers can work
    with these datasets just fine. Somewhere between the second size—too big for memory,
    but we can still process it locally—and the third size, however, most people will
    start to say they’re working with *big datasets*. In other words, they’re starting
    to have problems doing what they want to do with the datasets, and sometimes rightfully
    so—if we have a dataset of the third size and only a single computer, we’re out
    of luck.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个数据集大小不会带来固有的挑战：大多数开发者可以很好地处理这些数据集。然而，在第二个大小——太大而无法存入内存，但我们仍然可以本地处理——和第三个大小之间，大多数人会开始说他们在处理*大数据集*。换句话说，他们开始遇到问题，无法用数据集做他们想做的事情，有时这是合理的——如果我们有一个第三个大小的数据集，而只有一台计算机，我们就无能为力了。
- en: Distributed computing solves that problem ([figure 7.1](#ch07fig01)). It’s the
    act of writing and running programs not for a single computer, but for a cluster
    of them. This cluster of computers works together to execute a task or solve a
    problem. We can use distributed computing to great effect when we pair it with
    parallel programming.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式计算解决了这个问题([图7.1](#ch07fig01))。这是编写和运行程序不是为单台计算机，而是为计算机集群的行为。这个计算机集群协同工作以执行任务或解决问题。当我们与并行编程结合使用时，我们可以有效地使用分布式计算。
- en: Figure 7.1\. Distributed computing involves several computers working together
    to execute a single task.
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.1\. 分布式计算涉及多台计算机协同工作以执行单个任务。
- en: '![](07fig01_alt.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![07fig01_alt.jpg](07fig01_alt.jpg)'
- en: If we think back to our discussions of parallel programming, the main advantage
    we talked about was that parallel programming allowed us to do lots of different
    bits of work all at once. We split the task at hand up into pieces and worked
    it several pieces at a time. For small problems, this had few, if any, benefits.
    As tasks got larger, however, we saw the value of parallelization rise. By using
    distributed computing, we can multiply this effect ([figure 7.2](#ch07fig02)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回顾一下我们关于并行编程的讨论，我们讨论的主要优势是并行编程允许我们同时完成许多不同的工作。我们将手头的任务分成几部分，一次处理几部分。对于小问题，这几乎没有好处。然而，随着任务的增大，我们看到了并行化的价值。通过使用分布式计算，我们可以放大这种效果([图7.2](#ch07fig02))。
- en: Figure 7.2\. Distributed computing allows us to reduce our compute time by parallelizing
    our work across multiple machines. We can use distributed computing to solve problems
    in days, hours, or minutes that would have taken weeks.
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2\. 分布式计算使我们能够通过在多台机器上并行化我们的工作来减少我们的计算时间。我们可以使用分布式计算在几天、几小时或几分钟内解决那些原本需要几周时间的问题。
- en: '![](07fig02_alt.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![07fig02_alt.jpg](07fig02_alt.jpg)'
- en: When we add computers to our workflow, we’re adding all the processing power
    of those computers. For example, if each computer we add has four cores, every
    time we add a new machine to our cluster, we’ll add four additional cores. If
    we started with a four-core machine, running in parallel might cut our processing
    time down to one-fourth, but with two machines, we could be down to one-eighth.
    Adding two more machines might bring us down to one-sixteenth of the time it originally
    took to process our data in linear time.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将计算机添加到我们的工作流程中时，我们正在添加这些计算机的所有处理能力。例如，如果我们添加的每台计算机都有四个核心，每次我们向我们的集群添加一台新机器时，我们就会增加四个额外的核心。如果我们从一台四核心的机器开始，并行运行可能会将我们的处理时间缩短到原来的四分之一，但如果有两台机器，我们可能会缩短到八分之一。再添加两台机器可能会将我们处理数据所需的时间缩短到原来的十六分之一。
- en: And although there is a physical limit to how many processors we can reasonably
    have on a single machine, there’s no limit to how many processors we can have
    in a distributed network. Dedicated *supercomputers* might have hundreds of thousands
    of processors across tens of thousands of machines, whereas *scientific computing
    networks* make hundreds of thousands of computers available to researchers engaged
    in serious number crunching. More commonly, companies, government entities, not-for-profits,
    and researchers are all turning to the cloud for on-demand cluster computing.
    We’ll talk more about that in [chapter 12](kindle_split_023.html#ch12).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在单台机器上我们可以合理拥有的处理器数量有物理限制，但在分布式网络中我们可以拥有的处理器数量没有限制。专门的*超级计算机*可能拥有分布在数万台机器上的数十万个处理器，而*科学计算网络*为从事严肃数值计算的科研人员提供了数十万台计算机。更常见的是，公司、政府机构、非营利组织和研究人员都在转向云服务以获取按需集群计算。我们将在第12章中更多地讨论这一点。
- en: Of course, distributed computing is not without its drawbacks. The curse of
    communication pops up again. If we distribute our work prematurely, we’ll end
    up losing performance spending too much time talking between computers and processors.
    A lot of performance improvements at the high-performance limits of distributed
    computing revolve around optimizing communication between machines.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，分布式计算并非没有缺点。通信的诅咒再次出现。如果我们过早地分配我们的工作，我们最终会因在计算机和处理器之间花费太多时间交谈而失去性能。在分布式计算的高性能极限上，许多性能改进都围绕着优化机器之间的通信。
- en: For most use cases, however, we can rest assured that by the time we’re considering
    a distributed workflow, our problem is so time-consuming that distributing work
    is sure to speed things up. One indicator is that distributed workflows tend to
    be measured in minutes or hours, rather than the seconds, milliseconds, or microseconds
    that we traditionally use to measure compute processes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于大多数用例，我们可以放心，当我们考虑分布式工作流程时，我们的问题已经如此耗时，以至于分配工作肯定能加快速度。一个指标是，分布式工作流程通常以分钟或小时来衡量，而不是我们传统上用来衡量计算过程的秒、毫秒或微秒。
- en: 7.2\. Hadoop for batch processing
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. Hadoop用于批量处理
- en: In this section, we’ll talk about the fundamentals of Apache Hadoop. Hadoop
    is a prominent distributed computing framework and one that you can use to tackle
    even the largest datasets. We’ll first review the different parts of the Hadoop
    framework, then we’ll write a Hadoop MapReduce job to see the framework in action.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论Apache Hadoop的基本原理。Hadoop是一个突出的分布式计算框架，你可以用它来解决甚至最大的数据集。我们首先将回顾Hadoop框架的不同部分，然后我们将编写一个Hadoop
    MapReduce作业来看到框架的实际应用。
- en: The Hadoop framework focuses specifically on the processing of big datasets
    on distributed clusters. Hadoop’s basic premise is that we can combine the map
    and reduce techniques we’ve seen so far, along with the idea of moving our code
    (not our data), to solve problems with small and large datasets alike.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop框架专注于在分布式集群上处理大数据集。Hadoop的基本前提是我们可以将我们迄今为止看到的映射和归约技术结合起来，再加上将我们的代码（而不是我们的数据）移动到我们的想法，以解决大小数据集的问题。
- en: We can find a lot of similarities between Hadoop and the way we’ve been thinking
    about computing so far in this book. I’ve been preaching that we should start
    small (and local) and then scale up as we need more resources. Hadoop promises
    the same thing. You can develop and test on a single local machine and then scale
    out to a thousand-machine cluster hosted in the cloud. Hadoop advocates for this
    in much the same way we do, through a map and reduce style of programming.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在Hadoop和我们在这本书中迄今为止思考计算方式之间找到很多相似之处。我一直宣扬我们应该从小（和本地）开始，然后随着我们需要更多资源而扩展。Hadoop承诺了同样的事情。你可以在单个本地机器上开发和测试，然后扩展到托管在云中的千机集群。Hadoop以我们相同的方式倡导这一点，通过映射和归约风格的编程。
- en: 7.2.1\. Getting to know the five Hadoop modules
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.1\. 了解五个Hadoop模块
- en: 'The Hadoop framework includes five modules for big dataset processing and cluster
    computing ([figure 7.3](#ch07fig03)):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop框架包括五个用于大数据集处理和集群计算的模块([图7.3](#ch07fig03))：
- en: '***MapReduce—*** A way of dividing work into parallelizable chunks'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***MapReduce—*** 将工作划分为可并行处理块的方法'
- en: '***YARN—*** A scheduler and resource manager'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***YARN—*** 调度器和资源管理器'
- en: '***HDFS—*** The file system for Hadoop'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***HDFS—*** Hadoop的文件系统'
- en: '***Ozone—*** A Hadoop extension for object storage and semantic computing'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***Ozone—*** 用于对象存储和语义计算的Hadoop扩展'
- en: '***Common—*** A set of utilities that are shared across the previous four modules'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***Common—*** 在前四个模块之间共享的一组实用工具'
- en: Figure 7.3\. The Hadoop framework is made up of five pieces of software, each
    of which tackles a different big dataset processing problem.
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.3\. Hadoop框架由五个软件组件组成，每个组件都针对不同的大数据集处理问题。
- en: '![](07fig03_alt.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![07fig03_alt.jpg](07fig03_alt.jpg)'
- en: MapReduce is an implementation of the map and reduce steps you’ve already seen
    in this book that is designed to work in parallel on distributed clusters. YARN
    is a job scheduling service with cluster management features. HDFS—or Hadoop Distributed
    File System—is the data storage system of Hadoop. Ozone is a new (version 0.3.0
    as I’m writing this) Hadoop project that provides for semantic object store capabilities.
    Common is a set of utilities common to all the Hadoop libraries.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 是对本书中已经看到的映射和减少步骤的实现，旨在在分布式集群上并行工作。YARN 是一个具有集群管理功能的作业调度服务。HDFS 或
    Hadoop 分布式文件系统是 Hadoop 的数据存储系统。Ozone 是一个新（版本 0.3.0，在我撰写本文时）的 Hadoop 项目，提供了语义对象存储功能。Common
    是一组适用于所有 Hadoop 库的通用工具。
- en: We’ll touch on the first three—MapReduce, YARN, and HDFS—now. These three libraries
    are the classic Hadoop stack. The Hadoop Distributed File System manages the data,
    YARN manages tasks, and MapReduce defines the data processing logic ([figure 7.4](#ch07fig04)).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将简要介绍前三个——MapReduce、YARN 和 HDFS。这三个库是经典的 Hadoop 栈。Hadoop 分布式文件系统管理数据，YARN
    管理任务，MapReduce 定义数据处理逻辑（[图 7.4](#ch07fig04)）。
- en: Figure 7.4\. The classic Hadoop stack is MapReduce, running on top of YARN,
    running on top of HDFS.
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.4\. 经典的 Hadoop 栈是 MapReduce，运行在 YARN 之上，运行在 HDFS 之上。
- en: '![](07fig04_alt.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig04_alt.jpg)'
- en: Hadoop’s twist on map and reduce
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Hadoop 对映射和减少的独特处理
- en: 'The main aspect of Hadoop with which we’ll concern ourselves in this book is
    the MapReduce library. Hadoop MapReduce is a massive data processing library that
    we can use to scale the map and reduce style of programming up to tens of terabytes
    or even petabytes by extending it across tens, hundreds, or thousands of worker
    machines. MapReduce divides programming tasks into two tasks: a `map` task and
    a `reduce` task—just like we saw at the end of [chapter 5](kindle_split_014.html#ch05).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本书我们将关注的 Hadoop 的主要方面是 MapReduce 库。Hadoop MapReduce 是一个庞大的数据处理库，我们可以通过扩展到数十、数百甚至数千个工作机来扩展其映射和减少编程风格，使其能够处理高达数十太字节甚至拍字节的数据。MapReduce
    将编程任务分为两个任务：一个 `map` 任务和一个 `reduce` 任务——就像我们在第 5 章末看到的那样。
- en: YARN for job scheduling
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: YARN 用于作业调度
- en: 'YARN is a job scheduler and resource manager that splits resource and job management
    into two components: scheduling and application management. The scheduler, or
    *resource manager*, oversees all of the work that is being done and acts as a
    final decision maker in terms of how resources should be allocated across the
    cluster. Application managers, or *node managers*, work at the node (single-machine)
    level to determine how resources should be allocated within that machine ([figure
    7.5](#ch07fig05)). Application managers also monitor what’s going on within their
    node and report that information back to the scheduler.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: YARN 是一个作业调度器和资源管理器，将资源和作业管理分为两个组件：调度和应用程序管理。调度器，或 *资源管理器*，监督所有正在进行的工作，并在资源如何在集群中分配方面作为最终决策者。应用程序管理器，或
    *节点管理器*，在节点（单机）级别工作，以确定在该机器内部如何分配资源（[图 7.5](#ch07fig05)）。应用程序管理器还监控其节点内发生的事情，并将该信息报告给调度器。
- en: Figure 7.5\. The YARN resource manager oversees the entire job, whereas a node
    manager oversees what happens within a single node.
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.5\. YARN 资源管理器监督整个作业，而节点管理器监督单个节点内发生的事情。
- en: '![](07fig05_alt.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig05_alt.jpg)'
- en: We can tie together resource managers in extremely high demand use cases where
    thousands of nodes are not sufficient. This process is called *federation*. When
    we federate YARN resource managers together, we can treat several YARN resource
    managers as a single resource manager and run them in parallel across multiple
    subclusters as if they were a single massive cluster.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在极端高需求的使用案例中，我们可以将资源管理器连接起来，其中数千个节点不足以满足需求。这个过程被称为 *联邦化*。当我们联邦化 YARN 资源管理器时，我们可以将多个
    YARN 资源管理器视为单个资源管理器，并在多个子集群中并行运行，就像它们是一个单一的庞大集群一样。
- en: 'The data storage backbone of Hadoop: HDFS'
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Hadoop 的数据存储骨干：HDFS
- en: The foundation of the Hadoop framework is its distributed file system abstraction,
    aptly named Hadoop Distributed File System. The Hadoop authors designed HDFS to
    work for cases where users want to
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 框架的基础是其分布式文件系统抽象，恰如其名，称为 Hadoop 分布式文件系统。Hadoop 作者设计 HDFS 以适应用户希望的情况
- en: process big datasets (several terabytes and up; too big for local processing)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理大型数据集（数太字节及以上；太大，无法进行本地处理）
- en: be flexible in their choice of hardware
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在硬件选择上要灵活
- en: be protected against hardware failure—a common cluster computing problem
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防止硬件故障——一个常见的集群计算问题
- en: 'Additionally, HDFS operates based on another key observation: that moving code
    is faster than moving data. When we introduced parallelization in [chapter 2](kindle_split_011.html#ch02),
    we talked about how Python’s base map moves both code and data. This is effective
    up to a point, but eventually the cost of moving data around—especially if the
    data files are large or numerous—becomes too much to justify parallelization.
    We run into the same problem we saw at the end of [chapter 5](kindle_split_014.html#ch05):
    the act of parallelization costs more than the benefits of doing the work in parallel.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，HDFS 还基于另一个关键观察：移动代码比移动数据更快。当我们介绍并行化[第 2 章](kindle_split_011.html#ch02)时，我们讨论了
    Python 的基本 map 如何同时移动代码和数据。这在某种程度上是有效的，但最终移动数据的成本——尤其是如果数据文件很大或很多——变得过高，以至于无法证明并行化的好处。我们在[第
    5 章](kindle_split_014.html#ch05)结尾遇到的问题同样存在：并行化的成本超过了并行工作的好处。
- en: By distributing the data across the cluster and moving the code to the data,
    we avoid this problem. Code—even in its lengthiest, most obtuse forms—will be
    small and cheaper to move than the data it needs to work on. In the typical case,
    our data is large and our code is small.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在集群中分布数据并将代码移动到数据处，我们避免了这个问题。代码——即使在其最长、最晦涩的形式中——也将比它需要处理的数据更小，且移动成本更低。在典型情况下，我们的数据量很大，而代码量很小。
- en: '|  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Distributed file systems**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**分布式文件系统**'
- en: HDFS is a reliable, performant foundation for high-performance distributed computing,
    but with that comes complexity. Because this book is not focused on data engineering,
    I’ve chosen to omit the details of HDFS. The book *Hadoop in Action* (Manning,
    2010) goes into HDFS in more depth and includes cookbook-style recipes for common
    HDFS operations. Chuck Lam, the book’s author, introduces Hadoop’s Distributed
    File System in [section 3.1](kindle_split_012.html#ch03lev1sec1) and does a deep
    dive into HDFS in [chapter 8](kindle_split_018.html#ch08).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS 是高性能分布式计算的一个可靠、高效的基石，但这也带来了复杂性。由于本书不专注于数据工程，我选择省略 HDFS 的细节。《Hadoop in Action》一书（Manning，2010）对
    HDFS 进行了更深入的探讨，并包含了常见 HDFS 操作的食谱式配方。本书的作者 Chuck Lam 在[第 3.1 节](kindle_split_012.html#ch03lev1sec1)介绍了
    Hadoop 的分布式文件系统，并在[第 8 章](kindle_split_018.html#ch08)中对 HDFS 进行了深入探讨。
- en: '|  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 7.3\. Using Hadoop to find high-scoring words
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3\. 使用 Hadoop 查找高分词
- en: 'Now that we’ve covered the fundamentals of Hadoop, let’s dive into some code
    to really see how it works. Consider the following scenario. (You can find the
    data for the scenario in the book’s code repository online: [https://github.com/jtwool/mastering-large-datasets](https://github.com/jtwool/mastering-large-datasets).)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 Hadoop 的基础知识，让我们深入一些代码，真正看看它是如何工作的。考虑以下场景。（您可以在本书的在线代码库中找到场景所需的数据：[https://github.com/jtwool/mastering-large-datasets](https://github.com/jtwool/mastering-large-datasets)。）
- en: '|  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Scenario
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 场景
- en: 'Two of your friends—one a nurse and the other a pop culture critic—have been
    arguing for days about a peculiar topic: the relative sophistication of the two
    seemingly unrelated figures Florence and the Machine (a contemporary English rock
    band) and Florence Nightingale (a legendary English nurse). To settle their dispute,
    you’ve been asked to count the frequencies of words longer than six letters occurring
    in songs by Florence and the Machine and the writings of Florence Nightingale.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你的两个朋友——一个是护士，另一个是流行文化评论家——已经就一个奇特的话题争论了几天：看似无关的两位人物佛罗伦萨和机器（一支当代英国摇滚乐队）以及弗洛伦斯·南丁格尔（一位传奇的英国护士）的相对复杂程度。为了解决他们的争论，你被要求计算佛罗伦萨和机器的歌曲以及弗洛伦斯·南丁格尔的著作中超过六个字母的单词出现的频率。
- en: '|  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'To do this we’ll need to do a few things:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们需要做几件事情：
- en: Install Hadoop
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 Hadoop
- en: Prepare a mapper—a Python script to do our `map` transformation
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备一个 mapper——一个用于我们 `map` 转换的 Python 脚本
- en: Prepare a reducer—a Python script to do our reduction
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备一个 reducer——一个用于我们归约的 Python 脚本
- en: Call the mapper and reducer from the command line
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从命令行调用 mapper 和 reducer
- en: 7.3.1\. MapReduce jobs using Python and Hadoop Streaming
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.1\. 使用 Python 和 Hadoop Streaming 进行 MapReduce 作业
- en: Before we get into the details of implementation, though, let’s take a look
    at what Hadoop’s MapReduce does. Hadoop’s MapReduce is a piece of software, written
    in Java, that we can use to execute MapReduce on distributed systems. When we
    talk about running Hadoop MapReduce with Python, we are (generally speaking) talking
    about running Hadoop Streaming, a Hadoop utility for using Hadoop MapReduce with
    programming languages besides Java.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解实现细节之前，让我们看看Hadoop的MapReduce做了什么。Hadoop的MapReduce是一个用Java编写的软件组件，我们可以用它来在分布式系统上执行MapReduce。当我们谈论使用Python运行Hadoop
    MapReduce时，我们（一般而言）是在谈论运行Hadoop Streaming，这是一个Hadoop实用程序，用于使用Java以外的编程语言来使用Hadoop
    MapReduce。
- en: To run that utility, we’ll call it from the command line along with options
    such as
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行该实用程序，我们将从命令行调用它，并附带选项，例如
- en: the mapper
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 映射器
- en: the reducer
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算子
- en: input data files
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入数据文件
- en: output data location
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出数据位置
- en: Hadoop provides an example code snippet demonstrating this command. An annotated
    version of this snippet appears in [figure 7.6](#ch07fig06).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop提供了一个示例代码片段来演示此命令。此代码片段的注释版本出现在[图7.6](#ch07fig06)中。
- en: Figure 7.6\. A word count example in Hadoop, using Hadoop Streaming and Unix
    tools.
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.6\. 使用Hadoop流和Unix工具的Hadoop单词计数示例。
- en: '![](07fig06_alt.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig06_alt.jpg)'
- en: The code snippet in [figure 7.6](#ch07fig06) calls on two Unix commands to serve
    as its mapper and reducer. `/bin/cat` refers to the Unix concatenate software,
    and `/bin/wc` refers to the Unix word count software. Used together like this,
    `cat` will print the text and `wc` will count the words. Hadoop will ensure that
    these actions are performed in parallel on the documents in the directory located
    at the input location and the results are written to the output directory.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.6](#ch07fig06)中的代码片段调用了两个Unix命令来充当其映射器和归约器。`/bin/cat`指的是Unix的连接软件，而`/bin/wc`指的是Unix的单词计数软件。这样一起使用，`cat`将打印文本，而`wc`将计数单词。Hadoop将确保这些操作在位于输入位置的目录中的文档上并行执行，并将结果写入输出目录。'
- en: Once run, the result will be that we can go into whatever directory we pointed
    output to and retrieve the count of the words. Before we move on to a full-scope
    example, let’s implement the word count mappers and reducers in Python. To emulate
    the `cat` capability in Python, let’s print each word to a new line. To emulate
    the `wc` capability, we’ll increment a counter for each word we come across. We’ll
    need to wrap both of these capabilities in solo, executable scripts.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦运行，结果就是我们可以在我们指向的任何目录中进入，并检索单词计数。在我们继续到完整示例之前，让我们用Python实现单词计数的映射器和归约器。为了模拟`cat`功能，让我们将每个单词打印到新的一行。为了模拟`wc`功能，我们将为遇到的每个单词增加一个计数器。我们需要将这两个功能包装在单独的可执行脚本中。
- en: The mapper might look like [listing 7.1](#ch07ex01), and the reducer might look
    like [listing 7.2](#ch07ex02).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 映射器可能看起来像[列表7.1](#ch07ex01)，而归约器可能看起来像[列表7.2](#ch07ex02)。
- en: Listing 7.1\. Word count mapper in Python
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.1\. Python中的单词计数映射器
- en: '[PRE0]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Listing 7.2\. Word count reducer in Python
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.2\. Python中的单词计数归约器
- en: '[PRE1]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In these two examples, some strange new things are going on. First, we’re reading
    from `stdin`. This is because Hadoop handles the opening of files for us, along
    with chopping up extra-large files into smaller bits. Hadoop is designed to be
    used with massive files, so having the ability to split a big file across several
    processors is important. We also can use Hadoop to work with compressed data—it
    natively supports compression formats such as .gz, .bz2, and .snappy (as shown
    in [table 7.1](#ch07table01)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个示例中，有一些奇怪的新情况正在发生。首先，我们正在从`stdin`读取。这是因为Hadoop为我们处理文件的打开，以及将大文件切割成更小的部分。Hadoop旨在用于处理大规模文件，因此能够将大文件分割到多个处理器上是很重要的。我们还可以使用Hadoop来处理压缩数据——它原生支持如.gz、.bz2和.snappy（如[表7.1](#ch07table01)所示）这样的压缩格式。
- en: Table 7.1\. A comparison of compression formats available for use out of the
    box with Hadoop
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表7.1\. Hadoop开箱即用的可用压缩格式的比较
- en: '| Format | Description | Use case | Hadoop Codec |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 格式 | 描述 | 用例 | Hadoop Codec |'
- en: '| --- | --- | --- | --- |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| .bz2 | Slow compression, but shrinks files more than older algorithms | Semi-long-term
    storage, file transfer between people | BZip2Codec |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| .bz2 | 慢速压缩，但比旧算法更能缩小文件大小 | 半长期存储，人与人之间的文件传输 | BZip2Codec |'
- en: '| .gz | Fast, well-supported compression algorithm | Transfer of files between
    processes (such as Hadoop steps) | GzipCodec |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| .gz | 快速，支持良好的压缩算法 | 进程间文件传输（如Hadoop步骤） | GzipCodec |'
- en: '| .snappy | New, fast compression algorithm; less support than .gz but better
    compression | Transfer of files between processes (such as Hadoop steps) | SnappyCodec
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| .snappy | 新的、快速的压缩算法；比 .gz 支持少，但压缩效果更好 | 进程之间（如 Hadoop 步骤）的文件传输 | SnappyCodec
    |'
- en: Second, both of our scripts print their output to the terminal. Again, this
    is because of how Hadoop is oriented. Hadoop will capture what’s printed to `stdout`
    and use that later on in the workflow. This creates an additional step on top
    of our standard workflow and can cause us to have to convert strings into Python
    objects.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们的两个脚本都将输出打印到终端。这同样是因为 Hadoop 的方向。Hadoop 将捕获打印到 `stdout` 的内容，并在工作流程的后续步骤中使用它。这在我们标准工作流程之上创建了一个额外的步骤，并可能导致我们必须将字符串转换为
    Python 对象。
- en: Lastly, both scripts start with the Python shebang. This line tells the computer
    to use these scripts as executables. Hadoop will try to call these scripts using
    the program at the designated shebang path, in this case, Python.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这两个脚本都以 Python 的 shebang 开头。这一行告诉计算机将这些脚本作为可执行文件使用。Hadoop 将尝试使用指定 shebang
    路径上的程序调用这些脚本，在这种情况下，是 Python。
- en: If you haven’t already tried, replacing the mapper and reducer from before with
    our two scripts will let us run our MapReduce job. This is shown in the following
    listing.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有尝试过，用我们的两个脚本替换之前的映射器和归约器将允许我们运行我们的 MapReduce 作业。这将在下面的列表中展示。
- en: Listing 7.3\. Running a Streaming MapReduce with Python
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.3\. 使用 Python 运行 Streaming MapReduce
- en: '[PRE2]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The output of this command will be in myOutputDir2 inside a file called results.
    The result should be the same as the second number that the command we called
    in [figure 7.6](#ch07fig06) returns.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令的输出将在我OutputDir2目录下的一个名为 results 的文件中。结果应该与我们在[图 7.6](#ch07fig06)中调用的命令返回的第二个数字相同。
- en: 7.3.2\. Scoring words using Hadoop Streaming
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.2\. 使用 Hadoop Streaming 计分单词
- en: Let’s turn back to our example of finding the counts of long words. For Hadoop,
    we’ll focus only on the words by Florence and the Machine. (We’ll save the texts
    of Florence Nightingale for Spark later in this chapter.) To get counts of specific
    words with Hadoop—instead of simply an overall count of words—we’ll have to modify
    our mapper and our reducer. Before we jump right into the code, let’s take a look
    at how this process will compare with our word counting example. I’ve diagrammed
    both processes, step by step, in [figure 7.7](#ch07fig07).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们寻找长单词计数的例子。对于 Hadoop，我们将只关注 Florence and the Machine 的单词。（我们将在本章后面将 Florence
    Nightingale 的文本保存到 Spark。）要使用 Hadoop 获取特定单词的计数——而不是简单地获取单词的总计数——我们必须修改我们的映射器和归约器。在我们直接跳入代码之前，让我们看看这个过程将如何与我们的词计数示例进行比较。我已经将这两个过程，一步一步地，图解在[图
    7.7](#ch07fig07)中。
- en: Figure 7.7\. Counting words and getting the frequencies of a subset of words
    have similar forms but require different mappers and reducers.
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.7\. 计算单词和获取单词子集的频率具有相似的形式，但需要不同的映射器和归约器。
- en: '![](07fig07_alt.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![07fig07_alt.jpg](07fig07_alt.jpg)'
- en: With our word count mapper, we had to extract the words from the document and
    print them to the terminal. We’ll do something very similar for our long word
    frequency example; however, we’ll want to add a check to ensure we’re only printing
    out long words. Note that this behavior—doing our filtering and breaking our documents
    into sequences of words—is very similar to how the workflow might execute in Python.
    As we iterated through the sequence, both the transformation and the filter would
    lazily be called on the lines of a document.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的词频映射器中，我们必须从文档中提取单词并将它们打印到终端。对于我们的长单词频率示例，我们将做类似的事情；然而，我们希望添加一个检查以确保我们只打印出长单词。请注意，这种行为——执行我们的过滤和将文档分解成单词序列——与工作流程可能在
    Python 中执行的方式非常相似。当我们遍历序列时，转换和过滤将懒加载地应用于文档的行。
- en: For our word count reducer, we had a counter that we incremented every time
    we saw a word. This time, we’ll need more complex behavior. Luckily, we already
    have this behavior on hand. We’ve implemented a frequency reduction several times
    and can reuse that reduction code here. Let’s modify our reducer from [listing
    7.2](#ch07ex02) so it uses the `make_counts` function we first wrote back in [chapter
    5](kindle_split_014.html#ch05). Our mapper will look like [listing 7.4](#ch07ex04),
    and our reducer will look like [listing 7.5](#ch07ex05).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的词计数归约器，我们有一个计数器，每次我们看到一个单词时都会增加。这次，我们需要更复杂的行为。幸运的是，我们已经有这种行为在手上了。我们已经多次实现了频率归约，并且可以在这里重用那段归约代码。让我们修改我们的归约器从[列表
    7.2](#ch07ex02)，使其使用我们在[第 5 章](kindle_split_014.html#ch05)中首次编写的`make_counts`函数。我们的映射器将类似于[列表
    7.4](#ch07ex04)，我们的归约器将类似于[列表 7.5](#ch07ex05)。
- en: Listing 7.4\. Hadoop mapper script to get and filter words
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.4\. Hadoop mapper 脚本用于获取和过滤单词
- en: '[PRE3]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Listing 7.5\. Hadoop reducer script to accumulate counts
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.5\. Hadoop reducer 脚本用于累计计数
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '***1* This is our make_counts function from [chapter 5](kindle_split_014.html#ch05).**'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 这是来自[第5章](kindle_split_014.html#ch05)的make_counts函数。**'
- en: '***2* We apply it to the sys.stdin stream, which is where our data will come
    in.**'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 我们将其应用于sys.stdin流，这是我们数据将进入的地方。**'
- en: The output of our MapReduce job will be a single file with a sequence of words
    and their counts in it. The results should look like [figure 7.8](#ch07fig08).
    We also should see some log text printed to the screen. We can quickly check to
    see that all the words are longer than six letters, just as we’d hoped. In [chapter
    8](kindle_split_018.html#ch08), we’ll explore Hadoop in more depth and tackle
    scenarios beyond word filtering and counting.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们MapReduce作业的输出将是一个包含一系列单词及其计数的单个文件。结果应该看起来像[图7.8](#ch07fig08)。我们还应该看到一些打印到屏幕上的日志文本。我们可以快速检查，看看所有单词的长度都超过六个字母，正如我们所希望的。在[第8章](kindle_split_018.html#ch08)中，我们将更深入地探讨Hadoop，并处理超出单词过滤和计数的场景。
- en: Figure 7.8\. The output of our MapReduce job is a sequence of words.
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.8\. 我们MapReduce作业的输出是一系列单词。
- en: '![](07fig08_alt.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig08_alt.jpg)'
- en: 7.4\. Spark for interactive workflows
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4\. Spark用于交互式工作流程
- en: 'So far in this chapter, we’ve been talking about the Hadoop framework for working
    with big datasets. In this section, we’ll turn our attention to another popular
    framework for big dataset processing: Apache Spark. Spark is an analytics-oriented
    data processing framework designed to take advantage of higher-RAM compute clusters
    that are now available.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们一直在谈论用于处理大数据集的Hadoop框架。在本节中，我们将把注意力转向另一个流行的用于大数据集处理的框架：Apache Spark。Spark是一个面向分析的数据处理框架，旨在利用现在可用的具有更高RAM的计算集群。
- en: 'Spark offers several other advantages, from the perspective of most Python
    programmers:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 从大多数Python程序员的视角来看，Spark提供了其他一些优势：
- en: Spark has a direct Python interface—PySpark.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark有一个直接的Python接口——PySpark。
- en: Spark can query SQL databases directly.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark可以直接查询SQL数据库。
- en: Spark has a `DataFrame` API—a rows-and-columns data structure that should feel
    familiar to Python programmers with experience in `pandas`.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark有一个`DataFrame` API——一个行列数据结构，对于有`pandas`经验的Python程序员来说应该很熟悉。
- en: 7.4.1\. Big datasets in memory with Spark
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.4.1\. Spark中的内存中大数据集
- en: As we touched on briefly in the introduction to [section 7.3](#ch07lev1sec3),
    Spark processes data in memory on the distributed network instead of storing intermediate
    data to a filesystem. This can lead to up to 100 times improvements in processing
    speed versus Hadoop on some workflows, to say nothing about the difference between
    a Spark task and a linear Python task. The caveat to this is that Spark requires
    machines with greater memory capacity.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[第7.3节](#ch07lev1sec3)的介绍中简要提到的，Spark在分布式网络上内存中处理数据，而不是将中间数据存储到文件系统中。这可以在某些工作流程中相对于Hadoop提高处理速度高达100倍，更不用说Spark任务与线性Python任务之间的差异了。这个缺点是Spark需要具有更大内存容量的机器。
- en: Choosing Spark versus Hadoop
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 选择Spark还是Hadoop
- en: Because Spark makes full use of a cluster’s RAM, we should favor Spark over
    Hadoop when we
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 因为Spark充分利用了集群的RAM，所以当我们
- en: are processing streaming data
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正在处理流数据
- en: need to get the task completed nearly instantaneously
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要几乎瞬间完成任务
- en: are willing to pay for high-RAM compute clusters
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 愿意为高RAM计算集群付费
- en: Spark’s use of in-memory processing means we don’t necessarily have to save
    the data anywhere. This makes Spark ideal for streaming data—one aspect of the
    conventional definition of big data. We should reserve Hadoop for batch processing.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Spark使用内存处理，这意味着我们不一定需要将数据保存到任何地方。这使得Spark非常适合流数据——这是大数据传统定义的一个方面。我们应该将Hadoop保留用于批量处理。
- en: Because Spark can be so much faster than Hadoop, we should use Spark when we
    need near instant processing of data. Of course, this is only really feasible
    up to a certain point. Eventually the data will be too big to process immediately,
    unless we throw an unjustifiable amount of resources at the problem.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 因为Spark可以比Hadoop快得多，所以当我们需要几乎即时处理数据时，我们应该使用Spark。当然，这只有在一定范围内才是真正可行的。最终，数据将太大而无法立即处理，除非我们向问题投入不合理的资源。
- en: 'That situation is directly tied to the last factor in our list: if money is
    of no concern, we can freely choose Spark. Because Spark runs faster when it has
    access to many high-RAM machines, if we can afford to assemble a cluster of high-RAM
    machines, then Spark is the obvious choice. Hadoop is designed to make the most
    out of low-cost computing clusters.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况直接关联到我们列表中的最后一个因素：如果资金不是问题，我们可以自由选择Spark。因为Spark在访问许多高RAM机器时运行得更快，如果我们能够负担得起组装一个高RAM机器的集群，那么Spark显然是最佳选择。Hadoop旨在充分利用低成本计算集群。
- en: As you can imagine, the answer to which distributed computing framework to use
    is not always clear cut; however, the map and reduce style we’ve developed throughout
    this book will serve you well working with big datasets in either one.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所想象的那样，关于使用哪个分布式计算框架的答案并不总是那么明确；然而，我们在整本书中开发的map和reduce风格将很好地服务于你在两个框架中处理大数据集。
- en: 7.4.2\. PySpark for mixing Python and Spark
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.4.2\. PySpark用于Python和Spark的混合
- en: Spark was designed for data analytics, and one way we can see that is in the
    Spark design team’s commitment to developing APIs for both Python and R. Like
    Hadoop, Spark is written to run on the Java Virtual Machine (JVM), which would
    normally make it hard for scientists, researchers, data scientists, or business
    analysts, who most often use languages like Python, R, and Matlab. We saw this
    problem in Hadoop. We were not able to interact directly with Hadoop through Python.
    Instead, we had to call our Python functions through Hadoop Streaming, and we
    had to use somewhat clumsy workarounds to work with Python data beyond strings.
    When we’re working with Spark, we can use its Python API, PySpark, to get around
    that issue.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是为数据分析而设计的，我们可以从Spark设计团队致力于开发Python和R的API中看到这一点。像Hadoop一样，Spark被编写为在Java虚拟机（JVM）上运行，这通常会使科学家、研究人员、数据科学家或业务分析师难以使用，他们通常使用Python、R和Matlab等语言。我们在Hadoop中遇到了这个问题。我们无法直接通过Python与Hadoop交互。相反，我们必须通过Hadoop
    Streaming调用我们的Python函数，并且我们必须使用一些笨拙的解决方案来处理Python数据，而不仅仅是字符串。当我们使用Spark时，我们可以使用其Python
    API PySpark来解决这个问题。
- en: With PySpark, we can call Spark’s Scala methods through Python just like we
    would a normal Python library, by importing the modules and functions we need.
    For example, we’ll often be using the `SparkConf` and `SparkContext` functions
    to set up our Spark jobs. We’ll talk more about these functions in [chapter 9](kindle_split_019.html#ch09)
    when we dive into Spark. For now, we can work with them in Python by importing
    them from PySpark, as shown in the following listing.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PySpark，我们可以像调用正常的Python库一样通过Python调用Spark的Scala方法，通过导入所需的模块和函数。例如，我们经常使用`SparkConf`和`SparkContext`函数来设置我们的Spark作业。我们将在深入探讨Spark的[第9章](kindle_split_019.html#ch09)中更多地讨论这些函数。现在，我们可以通过从PySpark导入它们来在Python中使用它们，如下面的列表所示。
- en: Listing 7.6\. Importing from Spark into Python
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.6\. 从Spark导入到Python
- en: '[PRE5]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We’ll see this in full force later in this chapter when we dive into PySpark
    in [section 7.5](#ch07lev1sec5).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们深入到[第7.5节](#ch07lev1sec5)中的PySpark时，我们将在本章的后面看到这一点。
- en: 7.4.3\. Enterprise data analytics using Spark SQL
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.4.3\. 使用Spark SQL进行企业数据分析
- en: A significant benefit of Spark is its support for SQL databases through Spark
    SQL. Built on top of the widespread Java Database Connectivity—which you’ll often
    see abbreviated as JDBC—Spark SQL makes it easy to work with structured data.
    This is especially important if we’re working with *enterprise data.* Enterprise
    data refers to common business data—HR or employee data, financial or payroll
    data, and sales order or operational data—and the most common means of storing
    that data—relational databases, especially Oracle DB or Microsoft SQL Server.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的一个重要优势是它通过Spark SQL支持SQL数据库。Spark SQL建立在广泛使用的Java数据库连接（通常缩写为JDBC）之上，这使得处理结构化数据变得容易。如果我们正在处理*企业数据*，这一点尤为重要。企业数据指的是常见的商业数据——如人力资源或员工数据、财务或工资数据，以及销售订单或运营数据——以及存储这些数据最常见的方式——关系数据库，尤其是Oracle
    DB或Microsoft SQL Server。
- en: Because Spark is designed first and foremost for Scala, the Spark SQL Python
    API is not compliant with the PEP 249 specification for Python database connections.
    Nonetheless, its core functionality makes intuitive sense, and we can use it with
    any database that has a JDBC connection, including popular free and open source
    databases such as MySQL, PostgreSQL, and MariaDB. In its simplest form, querying
    databases with Spark is as easy as passing our SQL query into the .`sql` method
    of a `SparkSession` object.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Spark 首先是为 Scala 设计的，因此 Spark SQL Python API 不符合 Python 数据库连接的 PEP 249 规范。尽管如此，其核心功能直观易懂，并且我们可以使用它与任何具有
    JDBC 连接的数据库一起使用，包括 MySQL、PostgreSQL 和 MariaDB 等流行的免费和开源数据库。在最简单的情况下，使用 Spark 查询数据库与将我们的
    SQL 查询传递给 `SparkSession` 对象的 `.sql` 方法一样简单。
- en: 7.4.4\. Columns of data with Spark DataFrame
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.4.4\. Spark DataFrame 的数据列
- en: When we’ve queried data using Spark, our data will end up in what’s known as
    a `DataFrame`, a Spark class that we can think of as being equivalent to a SQL
    table or a `pandas`.`DataFrame`. Unlike either a SQL table or a `DataFrame` from
    `pandas` though, the `DataFrame` in Spark is optimized for distributed computing
    workflows.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用 Spark 查询数据时，我们的数据最终会变成一个名为 `DataFrame` 的 Spark 类，我们可以将其视为与 SQL 表或 `pandas`.`DataFrame`
    等效。然而，与 SQL 表或 `pandas` 的 `DataFrame` 不同，Spark 中的 `DataFrame` 优化了分布式计算工作流程。
- en: Like SQL and `pandas`, Spark `DataFrame`s are organized around columns with
    names. This is helpful if we want to make conditional subsets of our data for
    machine learning or statistical summary. For example, if we wanted to get the
    average purchase size of customers with more than 20 orders, we could use the
    `DataFrame .filter` and `.agg` methods, combined with Spark’s knowledge of our
    column names, to get that information. We can see this example in [figure 7.9](#ch07fig09).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 与 SQL 和 `pandas` 一样，Spark `DataFrame`s 是围绕具有名称的列组织的。如果我们想为机器学习或统计摘要制作数据的条件子集，这很有帮助。例如，如果我们想获取拥有超过20个订单的客户的平均购买大小，我们可以使用
    `DataFrame .filter` 和 `.agg` 方法，结合 Spark 对我们列名的了解，来获取这些信息。我们可以在 [图7.9](#ch07fig09)
    中看到这个示例。
- en: Figure 7.9\. Spark `DataFrame`s have a `.filter` method that we can use to quickly
    take subsets of our big datasets.
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.9\. Spark `DataFrame`s 有一个 `.filter` 方法，我们可以用它来快速获取大数据集的子集。
- en: '![](07fig09_alt.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig09_alt.jpg)'
- en: '`DataFrame`’s version of `.filter` has a use similar to that of the `filter`
    function we saw in [chapters 4](kindle_split_013.html#ch04)–[6](kindle_split_015.html#ch06).
    In fact, a lot of the map and reduce-oriented data processing functions make their
    way into the `pyspark.sql.functions` library, including `zip` as `arrays_zip`.
    The `DataFrame` API is a more general API that provides a convenience layer on
    top of the core Spark data object: the `RDD` or Resilient Distributed Dataset.
    `RDD`s are the Hadoop-abstraction that powers Spark’s in-memory distributed processing,
    and the PySpark `RDD` API provides access to all the functions we’ve become familiar
    with, including `map`, `reduce`, `filter`, and `zip`. We’ll see an example of
    these functions in the next section.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrame` 的 `.filter` 版本与我们在 [第4](kindle_split_013.html#ch04)–[6](kindle_split_015.html#ch06)
    章节中看到的 `filter` 函数的作用类似。事实上，许多以 map 和 reduce 为导向的数据处理函数都进入了 `pyspark.sql.functions`
    库，包括 `zip` 作为 `arrays_zip`。`DataFrame` API 是一个更通用的 API，它提供了一个便利层在核心 Spark 数据对象之上：`RDD`
    或弹性分布式数据集。`RDD`s 是 Hadoop 的抽象，为 Spark 的内存分布式处理提供动力，PySpark `RDD` API 提供了对我们熟悉的所有函数的访问，包括
    `map`、`reduce`、`filter` 和 `zip`。我们将在下一节中看到这些函数的示例。'
- en: 7.5\. Document word scores in Spark
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5\. Spark 中的文档单词得分
- en: Now that we’ve covered the fundamentals of Spark, let’s dive into some code.
    In the previous example in this chapter, we found all the words with more than
    six letters from the songs of the band Florence and the Machine. This served as
    evidence of their lyrical sophistication and also helped introduce us to Hadoop.
    In this section, we’ll complete the comparison between Florence and the Machine
    and Florence Nightingale by running the same process on a document by Florence
    Nightingale in Spark.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 Spark 的基础知识，让我们深入一些代码。在本章前面的示例中，我们找到了来自 Florence and the Machine 乐队歌曲中超过六个字母的所有单词。这证明了他们的歌词复杂性和帮助我们了解
    Hadoop。在本节中，我们将通过在 Spark 中对 Florence Nightingale 的文档运行相同的过程来完成 Florence and the
    Machine 与 Florence Nightingale 之间的比较。
- en: 'As in [section 7.3](#ch07lev1sec3), we’ll break this process down into three
    areas:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [第7.3节](#ch07lev1sec3) 中所述，我们将这个过程分解为三个领域：
- en: A mapper
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个映射器
- en: A reducer
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个减少器
- en: Running the code in Spark
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Spark 中运行代码
- en: Our mapper will be responsible for taking the files and turning them into sequences
    of words with more than six characters, and the reducer will be responsible for
    counting up the words we find. Running the code in Spark parallelizes the workflow
    for us. We can see this process play out in [figure 7.10](#ch07fig10).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的映射器将负责将文件转换为包含六个以上字符的单词序列，而归约器将负责计算我们找到的单词。在Spark中运行代码将并行化我们的工作流程。我们可以在[图7.10](#ch07fig10)中看到这个过程是如何展开的。
- en: Figure 7.10\. Counting up the big words used by Florence Nightingale involves
    three steps in Spark.
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.10. 在Spark中，计算弗洛伦斯·南丁格尔使用过的关键词涉及三个步骤。
- en: '![](07fig10_alt.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig10_alt.jpg)'
- en: 7.5.1\. Setting up Spark
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.5.1. 设置Spark
- en: Before we can jump into our Spark job, let’s take a second to set up Spark.
    Unlike setting up Hadoop—which may have been a hairy process if you weren’t familiar
    with Java—installing Spark is pretty straightforward. Go to [https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)
    and follow the download instructions on the page, and that’s it! You’ve got everything
    you need to use Spark.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始Spark作业之前，让我们花点时间来设置Spark。与设置Hadoop不同——如果您不熟悉Java，这可能是一个棘手的过程——安装Spark相当直接。访问[https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)，按照页面上的下载说明操作，就这样！您已经拥有了使用Spark所需的一切。
- en: '|  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Spark clusters**'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark集群**'
- en: Just like we didn’t do a deep dive into setting up a Hadoop cluster in this
    book, we also won’t do a deep dive into setting up a Spark cluster—though we will
    show you how to provision cloud resources for these technologies in [chapter 12](kindle_split_023.html#ch12).
    If you’re interested in a full Spark book after the two and a half chapters we’ll
    spend on it here, Manning has several books dedicated to Spark, including *Spark
    in Action* (2016) and *Spark GraphX in Action* (2016).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中没有深入探讨如何设置Hadoop集群一样，我们也不会深入探讨如何设置Spark集群——尽管我们将在[第12章](kindle_split_023.html#ch12)中向您展示如何为这些技术配置云资源。如果您在接下来的两章半时间里对Spark感兴趣，Manning有几本书是专门介绍Spark的，包括*Spark
    in Action*（2016）和*Spark GraphX in Action*（2016）。
- en: '|  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Now that we have Spark installed, we can run Spark jobs and interact with Spark
    using PySpark. The easiest way to take either of these actions is through the
    utilities that Spark provides. Just like Hadoop provided us the Hadoop Streaming
    utility, Spark provides two utilities: one that sets up an interactive Python
    shell called `pyspark` and one that allows us to run Spark jobs—similar to Hadoop
    streaming—called `spark-submit`.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了Spark，我们可以运行Spark作业并使用PySpark与Spark交互。执行这些操作的最简单方法是通过Spark提供的工具。就像Hadoop为我们提供了Hadoop
    Streaming工具一样，Spark提供了两个工具：一个用于设置交互式Python shell的`pyspark`，另一个允许我们运行Spark作业——类似于Hadoop
    streaming的`spark-submit`。
- en: Exploring big data interactively
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 交互式探索大数据
- en: One of the reasons why Spark is so popular is that it allows for us to interactively
    explore big data through a PySpark shell REPL. This more playful style of development,
    where we iterate through our problem line by line, is more familiar to a lot of
    data scientists than writing out extended chunks of code all at once. It also
    allows us to see what our intermediate results are or consult the Python documentation
    as we develop.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Spark之所以如此受欢迎，其中一个原因是可以通过PySpark shell REPL交互式地探索大数据。这种更轻松的开发风格，即我们逐行迭代问题，比一次性编写大量代码更受许多数据科学家的欢迎。它还允许我们在开发过程中查看我们的中间结果或查阅Python文档。
- en: We kick this process off by running the utility `pyspark`. That process brings
    up a screen—like [figure 7.11](#ch07fig11)—where we can enter Python commands.
    Right off the bat, we have access to `SparkContext` and `SparkSession` instances
    as `sc` and `spark.` (The `pyspark` utility imports them for us; when we write
    our own Spark scripts, we’ll need to import them ourselves.) The `sc` variable
    has methods for building the Resilient Distributed Dataset instances we mentioned
    in [section 7.4.4](#ch07lev2sec7). We can use the `spark` variable to bring data
    into `DataFrame`s—the parallel optimized tabular data abstractions we also mentioned
    in [section 7.4.4](#ch07lev2sec7). If we run python’s `help` command on these
    variables in the interactive session, we’ll see a list of methods available for
    each one. We’ll go into some of them in this book, but a full list of methods
    for each variable is available in the online documentation.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过运行实用程序 `pyspark` 来启动这个过程。这个过程会弹出一个屏幕——就像[图7.11](#ch07fig11)——在这里我们可以输入Python命令。一开始，我们就有了对
    `SparkContext` 和 `SparkSession` 实例的访问权限，分别以 `sc` 和 `spark` 的形式。（`pyspark` 实用程序为我们导入了它们；当我们编写自己的Spark脚本时，我们将需要自己导入它们。）`sc`
    变量具有构建我们在[7.4.4节](#ch07lev2sec7)中提到的弹性分布式数据集实例的方法。我们可以使用 `spark` 变量将数据带入 `DataFrame`s——我们在[7.4.4节](#ch07lev2sec7)中也提到过的并行优化的表格数据抽象。如果我们在这交互式会话中对这些变量运行python的
    `help` 命令，我们将看到每个变量可用的方法列表。我们将在本书中介绍其中的一些，但每个变量的方法完整列表可在在线文档中找到。
- en: Figure 7.11\. Spark provides an interactive terminal where we can run Python
    commands with all the power of a Spark cluster behind them.
  id: totrans-169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.11\. Spark提供了一个交互式终端，我们可以在这里运行Python命令，并利用Spark集群的全部功能。
- en: '![](07fig11_alt.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![07fig11_alt.jpg]'
- en: Running jobs
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 运行作业
- en: When we’re not working with Spark interactively, we’ll work with it by running
    Spark jobs. This is a similar process to how we ran MapReduce jobs in Hadoop.
    We write some code, and then we pass it as an argument to a utility. In the case
    of Spark, we’ll use the `spark-submit` utility and we’ll pass it a single Python
    script.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们不与Spark交互式工作的时候，我们将通过运行Spark作业来与之工作。这个过程与我们在Hadoop中运行MapReduce作业的过程类似。我们编写一些代码，然后将它们作为参数传递给一个实用程序。在Spark的情况下，我们将使用
    `spark-submit` 实用程序，并将单个Python脚本传递给它。
- en: In that Python script, we can create instances of any of the Spark objects we
    need. We’ll have access to them once we import the `pyspark` module. Let’s take
    a look at this method of working with Spark in action.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在那个Python脚本中，我们可以创建我们需要的任何Spark对象的实例。一旦我们导入了 `pyspark` 模块，我们就可以访问它们。让我们看看这种与Spark一起工作的方法是如何实施的。
- en: 7.5.2\. MapReduce Spark jobs with spark-submit
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.5.2\. 使用spark-submit的MapReduce Spark作业
- en: 'Turning our attention back to the question at hand—the lexical excellence of
    Florence Nightingale—we’ll break our work into three steps:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的注意力转回到手头的问题——弗洛伦斯·南丁格尔的词汇优秀——我们将我们的工作分为三个步骤：
- en: Turning a document into a sequence of words
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文档转换成一系列单词
- en: Filtering those words down to those having more than six characters
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些单词过滤成超过六个字符的单词
- en: Gathering counts of the rest
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集剩余的计数
- en: When we worked through this process in Hadoop, we accomplished step 1, turning
    a document into a sequence of words, and step 2, filtering out the small words,
    together in the mapper. With Spark, the three steps will all stand apart.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在Hadoop中完成这个过程时，我们在映射器中一起完成了步骤1，将文档转换成一系列单词，以及步骤2，过滤掉小单词。在Spark中，这三个步骤都将独立存在。
- en: To accomplish this process in Spark, the first thing we’ll want to do is bring
    our data into an `RDD`—Spark’s powerful parallel data structure. This is a good
    starting point for most work in Spark. To do that, we’ll need a `SparkContext`,
    so we’ll have to instantiate a `SparkContext` instance. Then we can use the `SparkContext`
    method `.textFile` to read in text files from our filesystem. This method creates
    an `RDD` with the lines of those documents as elements.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中完成这个过程，我们首先想要做的是将我们的数据带入一个 `RDD`——Spark强大的并行数据结构。这对于Spark中的大多数工作来说是一个好的起点。为了做到这一点，我们需要一个
    `SparkContext`，因此我们必须实例化一个 `SparkContext` 实例。然后我们可以使用 `SparkContext` 的 `.textFile`
    方法从我们的文件系统中读取文本文件。此方法创建一个包含那些文档行作为元素的 `RDD`。
- en: We can turn this dataset into a sequence of words by calling the `.flatMap`
    method of the `RDD`. The `.flatMap` method is like `map` but results in a flat
    sequence, not a nested sequence. `.flatMap` also returns an `RDD`, so we can use
    the `.filter` method of the `RDD` to filter down to only the large words, and
    then the `.countByValue` method of that resulting `RDD` to gather the counts.
    We can see this whole process in just a few lines in the following listing.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调用`RDD`的`.flatMap`方法将这个数据集转换成一个单词序列。`.flatMap`方法类似于`map`，但结果是一个扁平序列，而不是嵌套序列。`.flatMap`也返回一个`RDD`，因此我们可以使用`RDD`的`.filter`方法来过滤出只有大单词，然后使用该结果`RDD`的`.countByValue`方法来收集计数。我们可以在以下列表中仅用几行代码看到整个流程。
- en: Listing 7.7\. Counting words of six letters or more in Spark
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.7\. 在Spark中统计六个字母或以上的单词
- en: '[PRE6]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '***1* Because this is a script, most of the code will run only when called
    as such.**'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 因为这是一个脚本，大部分代码只有在作为这样的调用时才会运行。**'
- en: '***2* Initializes the SparkContext, with appName an optional but useful parameter**'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 初始化SparkContext，其中appName是一个可选但有用的参数**'
- en: '***3* Uses a regular expression to make our splits better quality**'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 使用正则表达式来提高我们的分割质量**'
- en: '***4* .textFile will load all the files matched as an RDD.**'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* .textFile将加载所有匹配的文件作为一个RDD。**'
- en: '***5* Then, we can use the RDD’s .flatMap to turn each line into words.**'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 然后，我们可以使用RDD的.flatMap将每一行转换为单词。**'
- en: '***6* Those words can then be filtered down to only the large words.**'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 这些单词可以被过滤成只有大单词。**'
- en: '***7* Then, lastly, we can count them up using a built-in method.**'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 最后，我们可以使用内置方法进行计数。**'
- en: '***8* Prints the results for convenience**'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 为了方便起见打印结果**'
- en: When you’re done running the code, you should see a long list of large words
    output. If all’s right, the words should all be over six letters in length. There
    will also be a bunch of output related to the Spark job that was run to process
    this code. The final result will look something like the following listing.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行完代码后，你应该会看到一个包含大量单词的长列表输出。如果一切正常，这些单词的长度都应该超过六个字母。还会有一系列与运行此代码的Spark作业相关的输出。最终结果将类似于以下列表。
- en: Listing 7.8\. Code output from Spark, counting up large words
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.8\. Spark的代码输出，统计大单词
- en: '[PRE7]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Unlike Hadoop, where we’re free to print our results to get them to write to
    the output file, with Spark, we’ll typically want to write our results directly
    to a file. That way, we won’t have to dig them out of a mass of terminal messages.
    In the next three chapters, we’ll touch on some more best practices for using
    Hadoop and Spark by working through more in-depth examples.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 与Hadoop不同，在Hadoop中我们可以自由地将结果打印出来以便写入输出文件，而在Spark中，我们通常希望直接将结果写入文件。这样，我们就不必从大量的终端消息中挖掘它们。在接下来的三章中，我们将通过更深入的示例来探讨使用Hadoop和Spark的一些最佳实践。
- en: 7.6\. Exercises
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6\. 练习
- en: 7.6.1\. Hadoop streaming scripts
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.6.1\. Hadoop streaming scripts
- en: What are the scripts called that we write for a Hadoop Streaming job? (Choose
    one.)
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为Hadoop Streaming作业编写的脚本叫什么？（选择一个。）
- en: Mapper and Reducer
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mapper和Reducer
- en: Applier and Accumulator
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序和累加器
- en: Functor and Folder
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数和文件夹
- en: 7.6.2\. Spark interface
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.6.2\. Spark interface
- en: When we interact with Spark, we’ll do it through PySpark, which is a Python
    wrapper around the Spark code written in which programming language? (Choose one.)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们与Spark交互时，我们将通过PySpark进行，PySpark是围绕用哪种编程语言编写的Spark代码的Python包装器？（选择一个。）
- en: Clojure
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clojure
- en: Scala
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala
- en: Java
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java
- en: Kotlin
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kotlin
- en: Groovy
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Groovy
- en: 7.6.3\. RDDs
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.6.3\. RDDs
- en: Spark’s innovations center around a data structure called an `RDD`. What does
    `RDD` stand for? (Choose one.)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的创新集中在一种称为`RDD`的数据结构上。`RDD`代表什么？（选择一个。）
- en: Resilient Distributed Dataset
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弹性分布式数据集
- en: Reliable Defined Data
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可靠定义数据
- en: Reduceable Durable Definition
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可减少的持久定义
- en: 7.6.4\. Passing data between steps
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.6.4\. Passing data between steps
- en: With Hadoop Streaming, we need to manually ensure that the data can pass between
    the map and reduce steps. What do we need to call at the end of each step? (Choose
    one.)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Hadoop Streaming时，我们需要手动确保数据可以在map和reduce步骤之间传递。每个步骤结束时我们需要调用什么？（选择一个。）
- en: '`return`'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return`'
- en: '`yield`'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`yield`'
- en: '`print`'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`print`'
- en: '`pass`'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pass`'
- en: Summary
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Hadoop is a Java framework that we can use to run code on data across distributed
    clusters.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop是一个Java框架，我们可以用它来在分布式集群上运行代码。
- en: When writing Python for Hadoop MapReduce jobs, we write one script for the mapper
    and one for the reducer.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当为Hadoop MapReduce作业编写Python时，我们为mapper和reducer编写一个脚本。
- en: Both the Python mapper script and the Python reducer script need to `print`
    their results to the console.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python mapper 脚本和 Python reducer 脚本都需要将结果 `print` 到控制台。
- en: In Spark, we can write a single Python script that handles both the map and
    reduce portions of our problem.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Spark 中，我们可以编写一个单一的 Python 脚本，处理我们问题的 map 和 reduce 部分。
- en: We interact with Spark through Python using the `pyspark` API.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过 `pyspark` API 使用 Python 与 Spark 交互。
- en: We can work with Spark in interactive mode or by running jobs—this gives us
    flexibility in our development workflow.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以以交互模式或运行作业的方式与 Spark 一起工作——这为我们提供了灵活的开发工作流程。
- en: 'Spark has two high-performance data structures: `RDD`s, which are excellent
    for any type of data, and `DataFrame`s, which are optimized for tabular data.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 有两种高性能数据结构：`RDD`s，适用于任何类型的数据，以及 `DataFrame`s，针对表格数据进行了优化。
- en: Chapter 8\. Best practices for large data with Apache Streaming and mrjob
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 8 章\. 使用 Apache Streaming 和 mrjob 的大型数据集最佳实践
- en: '*This chapter covers*'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Using JSON to transfer complex data structures between Apache Streaming steps
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 JSON 在 Apache Streaming 步骤之间传输复杂的数据结构
- en: Writing mrjob scripts to interact with Hadoop without Apache Streaming
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写 mrjob 脚本以与 Hadoop 交互，而不使用 Apache Streaming
- en: Thinking about mappers and reducers as key-value consumers and producers
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 mapper 和 reducer 视为键值消费者和生产者
- en: Analyzing web traffic logs and tennis match logs with Apache Hadoop
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Apache Hadoop 分析网络流量日志和网球比赛日志
- en: 'In [chapter 7](kindle_split_017.html#ch07), we learned about two distributed
    frameworks for processing large datasets: Hadoop and Spark. In this chapter, we’ll
    dive deep into Hadoop—the Java-based large dataset processing framework. As we
    touched on last chapter, Hadoop has a lot of benefits. We can use Hadoop to process'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 7 章](kindle_split_017.html#ch07) 中，我们学习了两个用于处理大型数据集的分布式框架：Hadoop 和 Spark。在本章中，我们将深入探讨
    Hadoop——一个基于 Java 的大型数据集处理框架。正如我们在上一章中提到的，Hadoop 有很多好处。我们可以使用 Hadoop 来处理
- en: lots of data fast—distributed parallelization
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速处理大量数据——分布式并行化
- en: data that’s important—low data loss
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重要的数据——低数据丢失
- en: absolutely enormous amounts of data—petabyte scale
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全巨大的数据量——PB 级别
- en: 'Unfortunately, we also saw some drawbacks to working with Hadoop:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们也看到了与 Hadoop 一起工作的缺点：
- en: To use Hadoop with Python, we need to use the Hadoop Streaming utility.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使用 Python 与 Hadoop 一起工作，我们需要使用 Hadoop Streaming 工具。
- en: We need to repeatedly read in strings from `stdin`.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要反复从 `stdin` 读取字符串。
- en: The error messages for Java are not super helpful.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java 的错误信息并不特别有帮助。
- en: In this chapter, we’ll look at how we can deal with those issues by working
    through some scenarios. We’ll analyze the skill of tennis players over time and
    find the most talented players in the sport.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过分析一些场景来探讨如何处理这些问题。我们将分析网球运动员随时间的变化技能，并找出体育中最有才华的运动员。
- en: '8.1\. Unstructured data: Logs and documents'
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1\. 非结构化数据：日志和文档
- en: The Hadoop creators designed Hadoop to work on *unstructured data*—a term that
    refers to data in the form of documents. Though they will often contain useful,
    interpretable metadata—for example, author or date—the important content is typically
    unrestricted in form. A classic example of unstructured data is a web page.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 的创建者设计了 Hadoop 以处理 *非结构化数据*——这是一个指代文档形式数据的术语。尽管它们通常会包含有用的、可解释的元数据——例如，作者或日期——但重要内容通常不受形式限制。非结构化数据的经典例子是一个网页。
- en: 'The web page is written in HTML and has some general formatting requirements:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 网页是用 HTML 编写的，并有一些通用的格式要求：
- en: The page starts with a head tag.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 页面以 head 标签开始。
- en: Inside the head tag are CSS and JavaScript imports.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 head 标签内包含 CSS 和 JavaScript 导入。
- en: There should also be some metadata inside the head tag—maybe a description of
    the page or the page’s title.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: head 标签内也应包含一些元数据——可能是对页面或页面标题的描述。
- en: Then there’s the body tag, which is the main content of the page.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后是 body 标签，它是页面的主要内容。
- en: The web page has useful metadata—we could quickly write up some code to find
    the title of any web page and even its keywords and description, if they’re listed
    as metadata—however, none of these aspects of the page is why anyone goes to it.
    What users are interested in is entirely in the body section. And, of course,
    the body of the web page can contain anything its author pleases, such as text,
    images, videos, music, and so on.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 网页包含有用的元数据——我们可以快速编写一些代码来找到任何网页的标题，甚至其关键词和描述（如果它们作为元数据列出），然而，页面的这些方面并不是人们访问它的原因。用户感兴趣的全部都在
    body 部分。当然，网页的主体可以包含作者喜欢的任何内容，如文本、图像、视频、音乐等。
- en: Compare this to other common forms of unstructured data, such as social media
    content, text or office documents, spreadsheets, and logs. If we think about a
    social media post, we know that these posts have required or imputed fields (such
    as time of post), but, more importantly, they also have freeform fields, such
    as the text of a tweet or a Facebook status. If we think about an office document,
    we know that our office software will record information like time last saved
    and the names of the users who have edited the document, but the main body of
    the document can be anything from a love letter to a business report. If we think
    about log data, we typically have more structure—machines do more logging than
    people—however, logs are often saved in file formats that are considered unstructured,
    such as plain text, and fall into this category for that reason.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 将此与其他常见的非结构化数据形式进行比较，例如社交媒体内容、文本或办公文档、电子表格和日志。如果我们考虑一条社交媒体帖子，我们知道这些帖子有需要或推断的字段（如发布时间），但更重要的是，它们还有自由格式字段，如推文或Facebook状态更新中的文本。如果我们考虑办公文档，我们知道我们的办公软件会记录诸如最后保存时间和编辑文档的用户姓名等信息，但文档的主体可以是情书到商业报告的任何内容。如果我们考虑日志数据，我们通常有更多的结构——机器比人做更多的日志记录——然而，日志通常以被认为是非结构化的文件格式保存，如纯文本，因此归入这一类别。
- en: Unstructured data is notoriously unwieldly. It’s not amenable to the kind of
    tabular analysis that most data analysts cut their teeth on. This makes problems
    that involve unstructured data more frustrating for analysts, because their standard
    bag of tricks doesn’t typically work, and less satisfying for customers, because
    the analysis takes longer and may be less fruitful.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化数据因其难以处理而闻名。它不适合大多数数据分析师所熟悉的那种表格分析。这使得涉及非结构化数据的问题对分析师来说更加令人沮丧，因为他们的标准技巧通常不起作用，对客户来说也不那么令人满意，因为分析过程更长，可能收获更少。
- en: At the same time, unstructured data is one of the most common forms of data
    around. Companies that have made an effort to assess how much data they have in
    structured versus unstructured formats have consistently found unstructured data
    makes up more than 80%, sometimes even as much as 95%, of their data. This makes
    sense when we consider that technologies such as personal web pages, social media,
    email, blogs, and other self-publishing platforms all produce unstructured data.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，非结构化数据是周围最常见的几种数据形式之一。那些努力评估他们有多少结构化数据和非结构化数据的公司发现，非结构化数据通常占他们数据的80%以上，有时甚至高达95%。当我们考虑到个人网页、社交媒体、电子邮件、博客和其他自出版平台都会产生非结构化数据时，这一点是有道理的。
- en: Keeping data in an unstructured format does have some advantages, chiefly that
    it’s loosely coupled with the systems that rely on it. If the format of the data
    is not under your control (for example, it comes from a system owned by another
    group or another company), or you’re working with data in several different formats,
    keeping that data unstructured provides an advantage because you will never need
    to restructure a datastore to accommodate changes. These facts about unstructured
    data—especially its prevalence—make it important to have a tool like Hadoop, which
    is designed for unstructured data, in one’s belt.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据以非结构化格式存储确实有一些优点，主要是有利于它与依赖它的系统松散耦合。如果数据的格式不在你的控制之下（例如，它来自另一个组或另一个公司拥有的系统），或者你正在处理多种不同格式的数据，那么保持数据非结构化将提供优势，因为你永远不需要重新结构化数据存储以适应变化。这些关于非结构化数据的事实——尤其是其普遍性——使得拥有一个像Hadoop这样的工具变得重要，Hadoop是为非结构化数据设计的。
- en: 8.2\. Tennis analytics with Hadoop
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2. 使用Hadoop进行网球分析
- en: To demonstrate the power of Hadoop—and how we can use it to turn log-style data
    into usable information—we’ll tackle an example from the world of tennis.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示Hadoop的力量——以及我们如何使用它将日志风格的数据转换为可用的信息——我们将从网球界的一个例子入手。
- en: '|  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Scenario
  id: totrans-258
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 场景
- en: A new professional tennis league is forming, and they have hired you to come
    up with skill estimates for professional tennis players so that they can direct
    their efforts for recruiting players to the new league. They have provided you
    with data for several years of matches and would like you to return to them with
    a list of players and their corresponding skills.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 一个新的职业网球联赛正在形成，他们聘请你来为职业网球运动员制定技能评估，以便他们可以针对新联赛招募球员。他们已经为你提供了几年的比赛数据，并希望你能提供一份球员及其对应技能的名单。
- en: '|  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Solving this problem will involve three steps. We need to
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题将涉及三个步骤。我们需要
- en: read in the data for each match
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取每场比赛的数据
- en: update the rankings of the winner and loser of each match
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新每场比赛胜者和败者的排名
- en: sort the rankings when all of our work is done
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有工作完成后对排名进行排序
- en: We’ll break each of these steps up into a Hadoop MapReduce job in the streaming
    style we learned in [chapter 7](kindle_split_017.html#ch07). Thinking back to
    [chapter 7](kindle_split_017.html#ch07), we know we’ll need a mapper script and
    a reducer script. Our mapper script will handle step 1, and our reducer script
    will handle steps 2 and 3 ([figure 8.1](#ch08fig01)).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这些步骤分解成Hadoop MapReduce作业，采用我们在[第7章](kindle_split_017.html#ch07)中学到的流式处理风格。回顾[第7章](kindle_split_017.html#ch07)，我们知道我们需要一个映射器脚本和一个减少器脚本。我们的映射器脚本将处理第一步，我们的减少器脚本将处理第二步和第三步([图8.1](#ch08fig01))。
- en: Figure 8.1\. The tennis analytics problem requires three steps broken up between
    mapper and reducer scripts. In the mapper, we assemble the information we need,
    and in the reducer, we rank and sort the players.
  id: totrans-266
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.1\. 网球分析问题需要三个步骤，这些步骤在映射器和减少器脚本之间分割。在映射器中，我们组装所需的信息，在减少器中，我们排名和排序球员。
- en: '![](08fig01_alt.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig01_alt.jpg)'
- en: '[Figure 8.1](#ch08fig01) shows what data will look like as it flows through
    this process. We’ll start with our input datafiles, we’ll read the matches from
    those files, then we’ll reduce the matches into ratings for each tennis player.
    Finally, we’ll sort the matches to return the players in order. As the data moves,
    you’ll notice it changes from a comma-separated string into key-pairs into a sequence
    of key-value pairs.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8.1](#ch08fig01)显示了数据在通过此过程时的样子。我们将从我们的输入数据文件开始，从那些文件中读取比赛，然后我们将比赛减少为每个网球球员的评级。最后，我们将对比赛进行排序，以按顺序返回球员。随着数据移动，你会注意到它从逗号分隔的字符串变为键值对，再到一系列键值对。'
- en: 8.2.1\. A mapper for reading match data
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.1\. 用于读取比赛数据的映射器
- en: Because step 1 is neatly contained within our mapper script, let’s start our
    process there. We’ll start by inspecting how the matches are contained within
    a file, part of which is previewed in [figure 8.2](#ch08fig02).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 因为第一步被巧妙地包含在我们的映射器脚本中，所以让我们从那里开始我们的过程。我们将首先检查比赛是如何包含在文件中的，部分内容已在[图8.2](#ch08fig02)中预览。
- en: Figure 8.2\. The tennis match logs contain matches as comma-separated strings.
  id: totrans-271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.2\. 网球比赛日志包含以逗号分隔的字符串形式的比赛。
- en: '![](08fig02_alt.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig02_alt.jpg)'
- en: 'In the file, we can see that each match is a single line—just like we’ll need
    it for Hadoop—and that each line contains a number of attributes describing the
    match, such as the winner, the loser, the surface, and more. For our purposes,
    we can concern ourselves with these three elements: winner, loser, and surface.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件中，我们可以看到每场比赛都是一行——就像我们为Hadoop所需要的那样——并且每一行包含描述比赛的多个属性，例如胜者、败者、场地和更多。就我们的目的而言，我们可以关注这三个元素：胜者、败者和场地。
- en: 'To access these elements of each match, we’ll need to split each line on the
    commas and then call the elements we want by number: the surface is in the 2nd
    position, the winner is in the 10th position, and the loser is in the 20th position.
    This isn’t especially clear to someone else reading our script, so we’ll pass
    the data on to our reducer function using key-value pairs. Key-value pairs provide
    much greater interpretability than comma-separated value data, at the cost of
    being bulkier to store. Key-value pairs are more costly to store because the keys
    must be stored in addition to the values, whereas a comma-separated value string
    needs no keys.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 为了访问每场比赛的这些元素，我们需要在每个行上分割逗号，然后通过编号调用我们想要的元素：场地在第2位，胜者在第10位，败者在第20位。这对阅读我们脚本的人来说并不特别清晰，所以我们将使用键值对将数据传递给我们的减少器函数。键值对比逗号分隔值数据提供了更大的可解释性，但存储起来更庞大。键值对存储成本更高，因为除了值之外，还必须存储键，而逗号分隔值字符串不需要键。
- en: JSON for passing data between mapper and reducer
  id: totrans-275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 用于在映射器和减少器之间传递数据的JSON
- en: To pass the key-value pair between our mapper and reducer, we’ll use a data
    interchange format known as JSON. JSON—or JavaScript Object Notation—is a data
    format used for moving data in plain text between one place (typically a computer)
    and another (again, typically a computer). Modern web developers are fond of JSON
    because it
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在映射器和减少器之间传递键值对，我们将使用一种称为JSON的数据交换格式。JSON——或JavaScript对象表示法——是一种用于在文本中移动数据的数据格式，从一个地方（通常是计算机）移动到另一个地方（再次，通常是计算机）。现代网络开发者喜欢JSON，因为它
- en: is easy for humans and machines to read
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对人类和机器来说都易于阅读
- en: provides a number of useful basic data types (such as string, numeric, and array)
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供了多种有用的基本数据类型（例如字符串、数字和数组）
- en: has an emphasis on key-value pairs that aids the loose coupling of systems
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 侧重于键值对，有助于系统的松散耦合
- en: As a Python developer, you can use Python’s built-in `JSON` module for converting
    Python objects into JSON data and back. We’ll use the `json.dumps` (dump string)
    function to turn a Python `dict` into a JSON string that we can print to the `stdout`
    with our mapper. Then we’ll use the `json.loads` (load string) function for reading
    it in with our reducer.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名 Python 开发者，您可以使用 Python 的内置 `JSON` 模块将 Python 对象转换为 JSON 数据，反之亦然。我们将使用
    `json.dumps`（导出字符串）函数将 Python `dict` 转换为可以由我们的 mapper 打印到 `stdout` 的 JSON 字符串。然后我们将使用
    `json.loads`（加载字符串）函数在我们的 reducer 中读取它。
- en: Altogether, our mapper script looks like the following listing.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们的 mapper 脚本如下所示。
- en: Listing 8.1\. Mapper for analyzing tennis scores
  id: totrans-282
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.1\. 分析网球分数的 mapper
- en: '[PRE8]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We bring each line in and process it with a helper function called `clean_match`.
    This is the function that we would map across all our data. To process each match,
    we split it on the comma and select the 10th, 20th, and 2nd elements of the line.
    These are the positions of the winner, loser, and surface respectively. We then
    populate a `dict` with those three elements, labeling each with an appropriate
    key. Finally, our `clean_match` function returns the `dict`.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每一行输入并使用一个名为 `clean_match` 的辅助函数进行处理。这是我们将在所有数据上映射的函数。为了处理每一场比赛，我们将其按逗号分割，并选择行中的第
    10、20 和第 2 个元素。这些是获胜者、失败者和表面的位置。然后我们使用这三个元素填充一个 `dict`，并为每个元素分配一个适当的键。最后，我们的 `clean_match`
    函数返回该 `dict`。
- en: If we were working in Python alone, this would be enough; however, we have to
    move our data across the terminal as a string to use Hadoop streaming. For this
    reason, we pass our data into the `json.dumps` function, which converts our Python
    `dict` into a corresponding JSON format—in this case, an object.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只使用 Python，这已经足够了；然而，为了使用 Hadoop streaming，我们必须将数据作为字符串通过终端传输。因此，我们将数据传递给
    `json.dumps` 函数，该函数将我们的 Python `dict` 转换为相应的 JSON 格式——在这种情况下，是一个对象。
- en: The other elements of the script are identical to the Hadoop Streaming scripts
    you’ve already written. We use a Python3 shebang to declare how the script should
    be executed, and we read data in from `stdin`. The shebang, `#! /usr/bin/python3`,
    tells your machine to process the script using Python.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的其他元素与您已经编写的 Hadoop Streaming 脚本完全相同。我们使用 Python3 的 shebang 来声明脚本应该如何执行，并从
    `stdin` 读取数据。shebang `#! /usr/bin/python3` 告诉您的机器使用 Python 处理脚本。
- en: 8.2.2\. Reducer for calculating tennis player ratings
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.2\. 计算网球运动员评分的 reducer
- en: 'With our mapper finished, we’re ready to tackle the reducer. The reducer is
    responsible for turning matches into assessments of players’ skill. To do that,
    we’ll rely on a simplified version of a formula that was originally developed
    to rate chess players: the Elo rating system.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成 mapper 后，我们准备处理 reducer。reducer 负责将比赛转换为对球员技能的评估。为此，我们将依赖一个简化的公式，该公式最初是为了评估棋手而开发的：Elo
    评分系统。
- en: Rating players based on match performance
  id: totrans-289
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 根据比赛表现评估球员
- en: 'The Elo rating system has a simple goal: take match results and use them to
    update the ratings of the players who participated in that match. To do this,
    the system makes statements about how often players of one rating beat players
    of another rating when they compete head-to-head. Typically, a 200-point rating
    difference between two players corresponds to the higher-rated player having a
    75% chance to beat the lower-rated player.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: Elo 评分系统有一个简单的目标：通过比赛结果来更新参与该比赛的球员的评分。为此，该系统会根据球员在直接对抗中击败另一评分球员的频率做出声明。通常，两名球员之间
    200 分的评分差距意味着高分球员有 75% 的机会击败低分球员。
- en: Mathematically, we’ll update players’ ratings using a simplified Elo formula
    that calculates the expected chance of winning for each player and then grants
    the winner the number of points staked by their opponent. Each player must stake
    a number of points proportional to their likelihood of winning the match, so the
    higher rated player is risking more but is also expected to win more often. We’ll
    also use a common heuristic for calculating the Elo rating, such as starting off
    never-before-seen players at 1,400 points. We can see the concept of Elo rating
    illustrated in [figure 8.3](#ch08fig03).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，我们将使用简化的 Elo 公式来更新玩家的评分，该公式计算每个玩家获胜的预期概率，然后授予获胜者对手所押注的分数。每个玩家必须押注与他们在比赛中获胜的可能性成比例的分数，因此评分较高的玩家风险更大，但也预期会赢得更频繁。我们还将使用计算
    Elo 评分的常见启发式方法，例如，从未见过的玩家开始时设定为 1,400 分。我们可以在[图 8.3](#ch08fig03)中看到 Elo 评分的概念。
- en: Figure 8.3\. The Elo rating approach works by adjusting players’ rankings after
    each match they play, with their ratings going up in a win or down in a loss.
    Underdogs are set to gain more points in a win than they would lose in a loss.
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.3\. Elo 评分方法通过调整玩家在每场比赛后的排名来工作，他们的评分在胜利时上升或在失败时下降。劣势方在胜利时获得的分数要多于他们在失败时失去的分数。
- en: '![](08fig03_alt.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig03_alt.jpg)'
- en: '[Figure 8.3](#ch08fig03) shows how in a match between a player with a 1600
    rating and a player with a 1550 rating, the 1550 player has more to gain and less
    to lose. That’s because the rating system expects the 1550 player to lose to the
    1600 player more often than not. When the 1600 player wins, they’ll still gain
    points, but it will be a modest amount.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8.3](#ch08fig03)展示了在一个 1600 评分的玩家和一个 1550 评分的玩家之间的比赛中，1550 评分的玩家有更多的收益和更少的损失。这是因为评分系统预期
    1550 评分的玩家会经常输给 1600 评分的玩家。当 1600 评分的玩家获胜时，他们仍然会获得分数，但数量会很少。'
- en: Using the techniques we learned in [chapter 5](kindle_split_014.html#ch05),
    we’ll structure the score accumulation in a reduce pattern. We’ll bring in new
    matches and use their results to adjust the ratings for each player, which are
    being stored in a `dict` that we’re holding onto throughout the reduce step. We
    can see this process in [figure 8.4](#ch08fig04).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们在[第 5 章](kindle_split_014.html#ch05)中学到的技术，我们将以 reduce 模式结构化得分累积。我们将引入新的比赛并使用它们的结果来调整每个玩家的评分，这些评分被存储在一个
    `dict` 中，我们在 reduce 步骤中一直保留这个 `dict`。我们可以在[图 8.4](#ch08fig04)中看到这个过程。
- en: Figure 8.4\. To calculate player ratings, we can reduce over matches, awarding
    them points for wins and taking points away for losses.
  id: totrans-296
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.4\. 要计算玩家评分，我们可以对比赛进行 reduce 操作，为胜利者加分，为失败者减分。
- en: '![](08fig04_alt.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig04_alt.jpg)'
- en: In [figure 8.4](#ch08fig04), we can see how the accumulation of player scores
    occurs.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 8.4](#ch08fig04)中，我们可以看到玩家得分的累积过程。
- en: We bring in the data for the next match.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们引入下一场比赛的数据。
- en: We calculate the impact that match had on each player’s rating.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们计算比赛对每个玩家评分的影响。
- en: We give the match winner the points they won.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将赢得比赛的选手获得的分数给予他们。
- en: We deduct from the match loser the points they lost.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从比赛失败者那里扣除他们失去的分数。
- en: If either the winner or loser are new observations, we start them at 1,400 points.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果获胜者或失败者都是新的观察结果，我们将它们从 1,400 分开始。
- en: We return the `dict` to be used for the next match.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们返回 `dict` 以用于下一场比赛。
- en: 'This process occurs for each match until we’ve reduced all the matches to a
    single `dict`: our N (many matches) to X (a single `dict` of ratings) transformation.
    Lastly, when we’re done, we can print our `dict` to the screen as a JSON object
    so we can easily use it for further analysis down the road. The code for this
    process appears in [listing 8.2](#ch08ex02).'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程对每一场比赛都会发生，直到我们将所有比赛 reduce 到一个单一的 `dict`：我们的 N（许多比赛）到 X（单个评分 `dict`）转换。最后，当我们完成时，我们可以将我们的
    `dict` 打印到屏幕上作为 JSON 对象，这样我们就可以轻松地用于后续分析。这个过程的相关代码出现在[列表 8.2](#ch08ex02)中。
- en: Listing 8.2\. Reducing over matches to calculate player ratings
  id: totrans-306
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.2\. 通过减少匹配次数来计算玩家评分
- en: '[PRE9]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In [listing 8.2](#ch08ex02), we’re reducing over the matches with a function
    we’ve called `elo_acc`. The first thing to notice about the `elo_acc` function
    is that we’re reading the line in as a JSON string with `json.loads`. Because
    we output our `dict`s representing the matches as JSON strings, we can reconstitute
    them using a JSON string reader. This gives us `match_info`, a `dict` that contains
    the data we want about the match. Furthermore, because we’ve already done the
    work of creating keys for `winner` and `loser`, we can quickly retrieve the values
    by their corresponding keys.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 8.2](#ch08ex02)中，我们通过一个我们称之为 `elo_acc` 的函数来减少匹配。关于 `elo_acc` 函数的第一个要注意的事情是，我们使用
    `json.loads` 将行读取为 JSON 字符串。因为我们以 JSON 字符串的形式输出代表匹配的 `dict`，我们可以使用 JSON 字符串读取器来重新构建它们。这给了我们
    `match_info`，一个包含我们想要关于匹配的数据的 `dict`。此外，因为我们已经为 `winner` 和 `loser` 创建了键，我们可以通过相应的键快速检索值。
- en: From there, we can use this information to calculate the adjustments to the
    players’ ratings. In short, this process involves taking each player’s rating,
    dividing it by 400, and comparing those two values to come up with the amount
    of points that each player has at stake during the match. I round this number
    off to the nearest five-point interval out of personal preference. You can omit
    this step or round off to a larger number, like 10, 25, or even 100, if you’d
    like. Lastly, we’ll print the players and their ratings by unpacking the `tuple`s
    that `reduce` created.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里，我们可以使用这些信息来计算玩家评分的调整。简而言之，这个过程涉及取每个玩家的评分，除以 400，并将这两个值进行比较，以得出每个玩家在比赛中涉及的分数。我根据个人偏好将这个数字四舍五入到最近的五点间隔。如果你想省略这个步骤或四舍五入到更大的数字，比如
    10、25 或甚至 100，都可以。最后，我们将通过解包 `reduce` 创建的 `tuple` 来打印玩家及其评分。
- en: Finally, we can set these two scripts as executables and run them from the command
    line. The command will look like the following listing.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将这些脚本设置为可执行文件，并从命令行运行它们。命令将如下所示。
- en: Listing 8.3\. The Hadoop streaming command to run our rating calculator
  id: totrans-311
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.3\. 运行我们的评分计算器的 Hadoop streaming 命令
- en: '[PRE10]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After it finishes running, which should only be a few seconds, you should be
    able to open the results file included in the `tennis_ratings` directory and see
    output like this:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 运行完成后，应该只需要几秒钟，你应该能够打开 `tennis_ratings` 目录中包含的结果文件，并看到如下输出：
- en: '[PRE11]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As we had planned, our output is a map of players and their corresponding Elo
    ratings, reflecting how skillful those players are estimated to have been during
    the period we analyzed. Before we move on, there is a caveat to this analysis
    that we’ve seen a few times throughout this book (including [chapters 2](kindle_split_011.html#ch02)
    and [6](kindle_split_015.html#ch06)). You’ll note that if you run this analysis
    several times, you’ll receive different results each time. That’s because the
    order in which the matches are played affects the ratings each player (and their
    opponents) accumulates, altering the number of points they have at stake in each
    match. This was one of the problems we saw with parallel processing back when
    we first learned about it in [chapter 2](kindle_split_011.html#ch02). For a real
    Elo rating, we would want to process the matches in order.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们计划的那样，我们的输出是玩家及其对应 Elo 评分的映射，反映了这些玩家在我们分析期间估计的技能水平。在我们继续之前，这里有一个关于这种分析的警告，我们在本书的几个地方都看到了（包括[第
    2 章](kindle_split_011.html#ch02)和[第 6 章](kindle_split_015.html#ch06)）。你会注意到，如果你多次运行这个分析，每次都会得到不同的结果。这是因为比赛进行的顺序会影响每个玩家（及其对手）积累的评分，改变他们在每场比赛中涉及的分数。这是我们第一次在[第
    2 章](kindle_split_011.html#ch02)中学习并行处理时看到的问题之一。对于真实的 Elo 评分，我们希望按顺序处理比赛。
- en: 8.3\. mrjob for Pythonic Hadoop streaming
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3\. mrjob for Pythonic Hadoop streaming
- en: Assurances about order aside, perhaps the most striking thing about working
    with Hadoop streaming is that it doesn’t really feel like writing Python. Sure,
    we write two Python scripts, but we keep needing to print our data to `stdout`
    instead of passing it around inside the code. We have to resort to tricks like
    `json.loads` and `json.dumps` to work with complex file formats in any way. What
    we really want is a Pythonic way of working with Hadoop. For this, we can turn
    to mrjob—a Python library for Hadoop Streaming that focuses on cloud compatibility
    for truly scalable analysis.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 除了关于顺序的保证之外，与Hadoop流式处理一起工作的最引人注目的事情可能是它并不真正感觉像是在编写Python。当然，我们编写了两个Python脚本，但我们仍然需要将数据打印到`stdout`而不是在代码内部传递。我们必须求助于像`json.loads`和`json.dumps`这样的技巧来以任何方式处理复杂文件格式。我们真正想要的是一种Python风格的Hadoop工作方式。为此，我们可以转向mrjob——这是一个专注于云兼容性的Python库，用于Hadoop流式处理，以实现真正可扩展的分析。
- en: 'Yelp originally created the mrjob library for its own Hadoop MapReduce needs,
    including several high-importance recommendation systems that power the eatery
    review site:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: Yelp最初创建mrjob库是为了满足其自身的Hadoop MapReduce需求，包括几个高重要性的推荐系统，这些系统为餐饮评论网站提供动力：
- en: “People who viewed this also viewed” recommendations
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “查看此内容的人还看了”推荐
- en: Review highlights
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评论亮点
- en: Text autocomplete
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本自动完成
- en: Restaurant search
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 餐厅搜索
- en: Advertisements
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广告
- en: The company developed the mrjob framework because the framework allowed its
    engineers to use Python—a quick to write, easy to debug language—to work with
    massive, distributed data through Hadoop. And, indeed, massive is the operative
    word. Yelp’s data systems were processing more than 100 GB of data each day when
    it developed the framework. The scalability is important—that’s why we want to
    use Hadoop and distributed computing in the first place—but here we’ll focus on
    the Python.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 公司开发mrjob框架是因为该框架允许其工程师使用Python——一种易于编写、易于调试的语言——通过Hadoop处理大量分布式数据。实际上，“大量”是关键词。当公司开发该框架时，Yelp的数据系统每天处理超过100
    GB的数据。可扩展性很重要——这就是我们最初想要使用Hadoop和分布式计算的原因——但在这里我们将重点放在Python上。
- en: 8.3.1\. The Pythonic structure of a mrjob job
  id: totrans-325
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.1. mrjob作业的Python结构
- en: A chief benefit of mrjob is that we get to write more Python. Indeed, instead
    of writing two scripts and calling them from the command line (and getting weird
    Java-based errors back when we make a mistake), with mrjob we can write our entire
    Hadoop Streaming job in Python. The mrjob library removes the need to interact
    directly with Hadoop at all.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: mrjob的主要好处之一是我们能够编写更多的Python代码。确实，我们不必编写两个脚本并在命令行中调用它们（而且当我们出错时，会收到基于Java的错误），使用mrjob，我们可以用Python编写整个Hadoop
    Streaming作业。mrjob库消除了直接与Hadoop交互的需求。
- en: 'Yelp created mrjob to analyze web logs, and we’ll do the same to get used to
    the mrjob syntax. For example, let’s consider the problem of finding the pages
    on a website that throw a 404 error the most. The 404 error represents a page
    that can’t be found, so the presence of these errors in our logs is a direct reflection
    of inconvenience for our users. In a standard map and reduce workflow, we’d break
    this task up into two steps ([figure 8.5](#ch08fig05)):'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: Yelp创建mrjob是为了分析网络日志，我们将使用它来熟悉mrjob语法。例如，让我们考虑找到网站上抛出最多404错误的页面的问题。404错误表示找不到的页面，因此这些错误出现在我们的日志中直接反映了用户的不便。在一个标准的map和reduce工作流程中，我们会将这个任务分成两个步骤([图8.5](#ch08fig05))：
- en: A `map` step where we turn each line of a log into the error we’re interested
    in
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个`map`步骤，我们将日志的每一行转换为我们感兴趣的错误
- en: A `reduce` step where we count up the errors and find the offending pages
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个`reduce`步骤，我们统计错误并找到违规页面
- en: Figure 8.5\. To find 404 error offenders, we’d break the task up in a standard
    map and reduce style.
  id: totrans-330
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.5。为了找到404错误违规者，我们会按照标准的map和reduce风格来分解任务。
- en: '![](08fig05_alt.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![08fig05_alt](08fig05_alt.jpg)'
- en: As shown in the figure, we’d start by ingesting our log files with a `map`.
    Then we would transform each of those files into a sequence of errors and offending
    pages. And finally, we’d `reduce` over this sequence and count up the error messages.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 如图中所示，我们首先会使用`map`来摄取我们的日志文件。然后，我们将每个文件转换成一系列错误和违规页面。最后，我们会对这个序列进行`reduce`操作并统计错误消息。
- en: 'To do this with mrjob, we’ll have to use a slightly different approach. mrjob
    keeps the mapper and reducer steps but wraps them up in a single worker class
    named `mrjob`. The methods of `mrjob` correspond directly to the steps we’re used
    to: there’s a `.mapper` method for the `map` step and a `.reducer` method for
    the `reduce` step. The required parameters for these two methods, though, are
    a little different from the `map` and `reduce` functions we’ve come to know. In
    `mrjob`, all the methods take a key and value parameter as input and return `tuple`s
    of a key and a value as output ([figure 8.6](#ch08fig06)).'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`mrjob`实现这一点，我们需要采用稍微不同的方法。`mrjob`保留了mapper和reducer步骤，但将它们封装在一个名为`mrjob`的单个工作类中。`mrjob`的方法直接对应于我们熟悉的步骤：有一个`.mapper`方法用于`map`步骤，一个`.reducer`方法用于`reduce`步骤。然而，这两个方法的必需参数与我们所熟知的`map`和`reduce`函数略有不同。在`mrjob`中，所有方法都接受键和值参数作为输入，并以键和值的元组作为输出（[图8.6](#ch08fig06)）。
- en: Figure 8.6\. The mrjob versions of `map` and `reduce` share the same type signature,
    taking in keys and values and outputting keys and values.
  id: totrans-334
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.6. `mrjob`版本的`map`和`reduce`具有相同的类型签名，接收键和值并输出键和值。
- en: '![](08fig06_alt.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig06_alt.jpg)'
- en: At first, thinking about `map` and `reduce` as consumers and producers of key-value
    pairs might be a little confusing, especially because we’ve been talking about
    both of these processes working on sequences of any form, not just on those that
    take the shape of key-value pairs. Under the hood, however, this is how Hadoop
    treats `map` and `reduce`.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，将`map`和`reduce`视为键值对的消费者和生产者可能会有些困惑，尤其是因为我们一直在谈论这两种过程可以作用于任何形式的序列，而不仅仅是键值对形式的序列。然而，在底层，这正是Hadoop处理`map`和`reduce`的方式。
- en: '|  |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**The key-value method of map and reduce**'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '**`map`和`reduce`的键值方法**'
- en: 'In Hadoop, `map` and `reduce` are implemented as two methods: `.mapper` and
    `.reducer`. Each method takes a sequence of key-value pairs and produces key-value
    pairs in return. The `.mapper` method produces *intermediate key-value pairs.*
    In other words, it takes in data as keys and values and outputs them for the `.reducer`.
    Because the `.reducer` is expecting a key-value pair, this is perfect. In fact,
    a hidden step between the `map` and `reduce` steps in Hadoop sorts the keys and
    values Hadoop consumes by key. This makes the `.reducer` job even easier.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop中，`map`和`reduce`被实现为两个方法：`.mapper`和`.reducer`。每个方法接收一系列键值对，并返回键值对。`.mapper`方法产生*中间键值对*。换句话说，它接收数据作为键和值，并将它们输出给`.reducer`。因为`.reducer`期望一个键值对，所以这是完美的。实际上，在Hadoop的`map`和`reduce`步骤之间有一个隐藏的步骤，它会根据键对Hadoop消耗的键和值进行排序。这使得`.reducer`作业变得更加容易。
- en: Using keys allows Hadoop to make good use of our compute resources as it allocates
    work. Intermediate records output by `map` with like keys will tend to go to the
    same location for processing.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 使用键可以让Hadoop在分配工作时充分利用我们的计算资源。`map`输出具有相同键的中间记录往往会倾向于发送到同一位置进行处理。
- en: '|  |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: For our .`mapper` step in a standard MapReduce job, the key to our .`mapper`
    will be `None`, and the value will be the lines we consume. Because of this, our
    thinking about `map` and .`mapper` doesn’t have to change dramatically. We can
    ignore the key-value expectation of .`mapper` by simply ignoring the first parameter.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标准MapReduce作业中的`.mapper`步骤，我们的`.mapper`的键将是`None`，值是我们消耗的行。正因为如此，我们关于`map`和`.mapper`的思考不需要有太大的变化。我们可以通过简单地忽略第一个参数来忽略`.mapper`的键值期望。
- en: For the .`reducer` though, we will want to be aware of the key-value structure.
    Hadoop, through mrjob, does a lot of the organizing of keys and values for us.
    We can take advantage of that by considering the .`mapper` output not as a sequence
    but as a `dict` populated with keys and sequences. In our error analysis example,
    we’ll set these keys to the page URLs so we can quickly count the number of 404
    errors associated with those pages. We can see this play out in [listing 8.4](#ch08ex04).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`.reducer`来说，我们则需要关注键值结构。通过`mrjob`，Hadoop为我们做了大量的键和值组织工作。我们可以通过将`.mapper`输出视为一个填充了键和序列的`dict`而不是一个序列来利用这一点。在我们的错误分析示例中，我们将这些键设置为页面URL，这样我们就可以快速计算与这些页面相关的404错误数量。我们可以在[列表8.4](#ch08ex04)中看到这一点。
- en: 8.3.2\. Counting errors with mrjob
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.2. 使用mrjob计数错误
- en: '[Listing 8.4](#ch08ex04) is a small example, but because this is the first
    time we’ve seen mrjob code in the book, we’ll want to look at it pretty closely.
    On the first line, we’re importing a class `MRJob`, from the mrjob library’s `job`
    module This class contains all the core MapReduce capability that we’ll need to
    interact with Hadoop. The primary thing we’ll do when working with the mrjob library
    is create new classes that inherit from the `MRJob` class.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表8.4](#ch08ex04)是一个小例子，但由于这是我们第一次在书中看到mrjob代码，我们想要仔细地查看它。在第一行，我们正在从mrjob库的`job`模块导入一个名为`MRJob`的类。这个类包含我们与Hadoop交互所需的所有核心MapReduce功能。当我们使用mrjob库时，我们将主要做的事情是创建从`MRJob`类继承的新类。'
- en: Listing 8.4\. `MRJob` script for finding 404 error messages in a traffic log
  id: totrans-346
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.4。查找流量日志中404错误消息的`MRJob`脚本
- en: '[PRE12]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Not surprisingly, the next thing we do is create a new class `ErrorCounter`
    that inherits from the `MRJob` class. We’re also going to define `.mapper` and
    `.reducer` methods for this class. As discussed in [section 8.3.1](#ch08lev2sec3),
    both of these methods expect keys and values, and you can see that they both use
    three parameters: `self`, a key, and a value.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 不出所料，我们接下来要做的事情是创建一个新的名为`ErrorCounter`的类，它继承自`MRJob`类。我们还将为这个类定义`.mapper`和`.reducer`方法。如[第8.3.1节](#ch08lev2sec3)所述，这两个方法都期望键和值，你可以看到它们都使用了三个参数：`self`、一个键和一个值。
- en: For our `.mapper`, we’ll ignore the first of these parameters (the key), as
    suggested in the previous section. To do that, we’ll use the underscore variable,
    which is our way of saying we won’t do anything with this variable. We’ll name
    the second parameter `line` because the value of our input is going to be each
    line of our log file. Coming from working directly with Hadoop Streaming, this
    should feel pretty familiar. We’re receiving the data just as if it came from
    `stdin`.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的`.mapper`，我们将忽略这些参数中的第一个（键），正如前一部分所建议的。为此，我们将使用下划线变量，这是我们不使用这个变量的方式。我们将第二个参数命名为`line`，因为我们的输入值将是日志文件的每一行。从直接使用Hadoop
    Streaming的工作经验来看，这应该感觉非常熟悉。我们接收数据就像它来自`stdin`一样。
- en: Because the data we’re getting comes in as a comma-separated string, we’ll use
    Python’s `.split` method to split the input line into fields. We’ll check the
    HTTP response code field, which happens to be in 7th position, to see if it is
    a 404 error, and if it is, we’ll return the page name—which is in 6th position—and
    a 1\. We can visualize that process as shown in [figure 8.7](#ch08fig07).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们所得到的数据是以逗号分隔的字符串形式传入的，所以我们将使用Python的`.split`方法将输入行分割成字段。我们将检查HTTP响应代码字段，它恰好位于第7位，看看它是否是404错误，如果是，我们将返回页面名称——它位于第6位——和一个1。我们可以将这个过程可视化，如图[图8.7](#ch08fig07)所示。
- en: Figure 8.7\. Our `.mapper` consumes lines, splits them into fields, checks the
    value of the error message field, and then returns the page name and a 1.
  id: totrans-351
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.7。我们的`.mapper`消费行，将它们分割成字段，检查错误消息字段的值，然后返回页面名称和1。
- en: '![](08fig07_alt.jpg)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![图8.7](08fig07_alt.jpg)'
- en: '|  |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**The return values of mrjob methods**'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '**mrjob方法返回值**'
- en: Earlier in this chapter, we discussed how when working with complex data structures,
    it is often helpful to pass our data around as JSON so that we can quickly and
    easily reconstitute it from strings. The mrjob library authors thought this was
    such a good idea that they require every `.mapper` and `.reducer` output to be
    JSON serializable. This means that you’ll be best served by using simple Python
    data structures when you can, such as `float`s, `int`s, `string`s, `list`s, and
    `dict`s. These data structures are serializable in base Python. That said, you
    can turn any Python data structure into JSON by implementing your own method.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前面部分，我们讨论了当处理复杂的数据结构时，将数据作为JSON传递通常很有帮助，这样我们可以快速轻松地从字符串中重新构建它。mrjob库的作者认为这是一个非常好的主意，因此他们要求每个`.mapper`和`.reducer`输出都必须是JSON可序列化的。这意味着当你可以使用时，最好使用简单的Python数据结构，例如`float`、`int`、`string`、`list`和`dict`。这些数据结构在基本的Python中是可序列化的。尽管如此，你可以通过实现自己的方法将任何Python数据结构转换为JSON。
- en: '|  |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: With our `.mapper` sending out data as keys (page names) and values (indicator
    counts of those pages), we’re ready to move on to our `.reducer`. For our `.reducer`,
    we’ll sum up the number of 404s our `.mapper` reported and return that value along
    with the original key. I’m restricting this to only pages that have more than
    five 404 errors because I’m personally only interested in high-frequency offenders,
    but you can omit this step if you’d like.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`.mapper`将数据作为键（页面名称）和值（这些页面的指示计数）发送出去，我们现在可以继续到我们的`.reducer`。对于我们的`.reducer`，我们将汇总`.mapper`报告的404错误数量，并返回这个值以及原始键。我限制这个操作只针对有超过五个404错误的页面，因为我个人只对高频违规者感兴趣，但如果你愿意，可以省略这一步。
- en: This `.reducer` is the first place we really see how the key-value expectation
    changes how we think about our map and reduce steps. We can still think about
    `reduce` moving through a sequence, but this time we’re moving through a sequence
    of key-value pairs. And the value for each key is a sequence. In this specific
    situation, our key will be a page name, like `index.html`, and our value will
    be a list of indicators of 404 errors, such as `[1, 1, 1, 1]`. [Figure 8.8](#ch08fig08)
    shows what this looks like.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`.reducer`是我们真正看到关键值期望如何改变我们对map和reduce步骤思考的第一个地方。我们仍然可以思考`reduce`通过一个序列移动，但这次我们是通过一个键值对的序列移动。每个键的值是一个序列。在这个特定的情况下，我们的键将是一个页面名称，如`index.html`，我们的值将是一系列404错误指示器，例如`[1,
    1, 1, 1]`。[图8.8](#ch08fig08)展示了这看起来是什么样子。
- en: Figure 8.8\. Our `.mapper` produces key-value pairs that our `.reducer` then
    iterates through, operating on the key and its associated values.
  id: totrans-359
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.8. 我们的`.mapper`生成键值对，然后`.reducer`遍历这些键值对，对键及其相关值进行操作。
- en: '![](08fig08_alt.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig08_alt.jpg)'
- en: When we think about the line `num_404s = sum(vals)` from [listing 8.4](#ch08ex04),
    this line works because Hadoop has already sorted our data into a format where
    the key is the page and the `vals` variable contains a sequence of all the indicators
    (`1`). Summing up all those 1s then gives us a count of the number of 404 errors.
    Then we can return this value along with the key to get a count of the errors
    associated with each page.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们思考`num_404s = sum(vals)`这一行时，这一行之所以有效，是因为Hadoop已经将我们的数据排序成一种格式，其中键是页面，而`vals`变量包含所有指示器（`1`）的序列。然后，将这些1相加，就得到了404错误的计数。然后我们可以返回这个值以及键，以得到与每个页面相关的错误计数。
- en: 'Lastly, also in [listing 8.4](#ch08ex04), we see the Pythonic `main` call at
    the end of our script. To run our mrjob MapReduce job, we’ll call this script
    from the command line, adding our input data as an additional parameter. We’ll
    need to have Hadoop still installed from [chapter 7](kindle_split_017.html#ch07)
    to run the mrjob operation:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在[列表8.4](#ch08ex04)中，我们看到了脚本末尾的Pythonic `main`调用。要运行我们的mrjob MapReduce作业，我们将从命令行调用此脚本，并将我们的输入数据作为附加参数。我们需要从[第7章](kindle_split_017.html#ch07)安装Hadoop，以便运行mrjob操作：
- en: '[PRE13]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After that, we should see pages and their corresponding error counts printed
    to the screen:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们应该会在屏幕上看到页面及其对应的错误计数：
- en: '[PRE14]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: From this output, we can see that most of the errors on our site are coming
    from links pointing to a file called “.txt”.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个输出中，我们可以看到我们网站上大部分的错误都来自指向名为“ .txt”的文件的链接。
- en: 8.4\. Tennis match analysis with mrjob
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4. 使用mrjob进行网球比赛分析
- en: 'Having seen a small mrjob MapReduce workflow, let’s return to our tennis match
    data and dive into two more examples, each revolving around one of the greatest
    tennis players in history: Serena Williams.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在看到了一个小型的mrjob MapReduce工作流程之后，让我们回到我们的网球比赛数据，深入探讨两个更多例子，每个例子都围绕历史上最伟大的网球运动员之一：塞雷娜·威廉姆斯。
- en: 8.4.1\. Counting Serena’s dominance by court type
  id: totrans-369
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.4.1. 按球场类型计算塞雷娜的统治地位
- en: In this scenario, we’ll analyze Serena Williams’ historical dominance and learn
    to think in the key-value style that mrjob expects.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个场景中，我们将分析塞雷娜·威廉姆斯的历史统治地位，并学习以mrjob期望的关键值风格进行思考。
- en: '|  |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Scenario
  id: totrans-372
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 场景
- en: One of the most interesting things about tennis is that it’s one of the few
    sports where the playing surface changes. Successful professionals learn to play
    on courts made of grass, clay, and concrete. Regardless of court, Serena Williams
    has been one of the most impressive tennis players in history. A sports writer
    has asked us to analyze the match logs and count her wins and losses on each court
    type.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 网球最有趣的一点是，它是有少数几个比赛场地会改变的体育项目之一。成功的职业选手学会在由草地、红土和混凝土制成的球场上打球。无论球场类型如何，塞雷娜·威廉姆斯一直是历史上最令人印象深刻的网球运动员之一。一位体育记者要求我们分析比赛日志，并计算她在每种球场类型上的胜负。
- en: '|  |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: If we were to think about this process in our old map and reduce way of thinking,
    we could imagine mapping each file into `dict`s that contained the information
    we’d be interested in and then reducing over that information to get the counts
    ([figure 8.9](#ch08fig09)). To use mrjob, though, we want to be thinking about
    keys and values. What data do we want to end up as keys in our final output, and
    what data should be in the values?
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用我们旧的map和reduce思维方式来考虑这个过程，我们可以想象将每个文件映射到包含我们感兴趣信息的`dict`中，然后对那些信息进行reduce以获取计数（[图8.9](#ch08fig09)）。但是，要使用mrjob，我们想要考虑键和值。我们希望最终输出中的键是什么数据，值中应该有什么数据？
- en: Figure 8.9\. A traditional map and reduce solution would map information into
    `dict`s to allow us to count Serena’s wins.
  id: totrans-376
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.9. 一个传统的map和reduce解决方案会将信息映射到`dict`中，以便我们统计塞雷娜的胜利次数。
- en: '![](08fig09_alt.jpg)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![08fig09_alt.jpg]'
- en: 'Well, we know we want to have the data organized by surface, so that makes
    sense as a key. As a value, we want Serena’s record: a count of her wins and a
    count of her losses. We can accumulate those counts in our `.reducer` by using
    our `frequencies` function—the same one we wrote back in [chapter 5](kindle_split_014.html#ch05)
    when we introduced `reduce`—if we have a list of wins and losses. What we’ll want
    to output from our `.mapper` is the surface and either a `W` for a win or an `L`
    for a loss. Consider how this compares to our traditional map and reduce style
    approach in [figure 8.9](#ch08fig09).'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们知道我们希望数据按场地组织，所以这作为一个键是有意义的。作为值，我们希望得到塞雷娜的记录：她的胜利次数和失败次数。我们可以在`.reducer`中使用我们的`frequencies`函数来累积这些计数——这是我们之前在[第5章](kindle_split_014.html#ch05)介绍`reduce`时编写的同一个函数——如果我们有一个胜负列表。我们希望从`.mapper`中输出的将是场地和`W`（胜利）或`L`（失败）。考虑一下这与我们的传统map和reduce风格方法[图8.9](#ch08fig09)的比较。
- en: To implement this arrangement, we’ll make a new class that inherits from `MRJob`
    called `SerenaCounter` with a `.mapper` method that returns either the surface
    and a `W` or the surface and an `L`. That class also will need to have a `.reducer`
    method that gets the frequencies of her results for each surface. To do that,
    we’ll bring back our `frequencies` code from [chapter 5](kindle_split_014.html#ch05).
    We can see what this process looks like in the following listing.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这种安排，我们将创建一个新的类，它从`MRJob`继承，称为`SerenaCounter`，它有一个`.mapper`方法，该方法返回场地和`W`或场地和`L`。这个类还需要有一个`.reducer`方法，该方法获取每个场地的胜负频率。为此，我们将从[第5章](kindle_split_014.html#ch05)中带回我们的`frequencies`代码。我们可以在以下列表中看到这个过程的样子。
- en: Listing 8.5\. Counting Serena Williams’ wins and losses by surface with `MRJob`
  id: totrans-380
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.5\. 使用`MRJob`按场地统计塞雷娜·威廉姆斯的胜负次数
- en: '[PRE15]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Our `.mapper` method ingests the lines from our match logs and checks the winner
    and loser fields for Serena’s name. If we find her in the winner field, we’ll
    output the court type and a `W`. If we find her in the loser field, we’ll output
    the court type and an `L`. If we don’t find her in either field, we won’t output
    anything.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`.mapper`方法处理我们的比赛日志中的行，并检查胜者和败者字段中是否有塞雷娜的名字。如果我们发现她在胜者字段中，我们将输出场地类型和`W`。如果我们发现她在败者字段中，我们将输出场地类型和`L`。如果我们在这两个字段中都没有找到她，我们不会输出任何内容。
- en: The `.reducer` method receives that information by key, which we take in through
    the `surface` parameter, and value, which we take in through the `results` parameter.
    Because we passed the court type out in first position in our `.mapper`, that
    value will be read as the key. The results will be accessible for each court type
    as a sequence, like `['W', 'L', 'W', 'W', . . . ]`. We can use our `frequencies`
    function to get a `dict` of the counts of each unique element. We’ll output the
    surface and counts at the end of our `.reducer` to see the court type with which
    each grouping of wins and losses is associated.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '`.reducer`方法通过键接收该信息，我们通过`surface`参数接收，以及值，我们通过`results`参数接收。因为我们在`.mapper`中第一个位置传递了场地类型，所以这个值将被读取为键。结果将作为序列对每个场地类型都是可访问的，例如`[''W'',
    ''L'', ''W'', ''W'', ...]`。我们可以使用我们的`frequencies`函数来获取每个唯一元素的计数字典。我们将在`.reducer`的末尾输出场地和计数，以查看与每个胜负组合关联的场地类型。'
- en: 'When we’re ready to run the script, we can run it from the command line:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们准备好运行脚本时，我们可以从命令行运行它：
- en: '[PRE16]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Shortly after, we should see something like the following printed to the screen.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后，我们应该在屏幕上看到类似以下内容打印出来。
- en: '[PRE17]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`MRJob` combs through each record in all the match log files and sums up all
    of Serena’s wins and losses, providing us her record by court type. From this,
    we can see that she’s a dominant grass court player, winning more than eight matches
    for every loss. On clay comparatively, she’s the most human, winning just shy
    of 75% (67 wins in 90 matches) of her matches. On hard courts, where she plays
    most of her matches, she has racked up more than 240 wins, claiming victory more
    than 80% (243 wins in 290 matches) of the time.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '`MRJob` 会遍历所有比赛日志文件中的每条记录，总结出塞雷娜的所有胜利和失败，提供她按球场类型的记录。从这些记录中，我们可以看到她是一位主导草地球员，每输掉一场比赛就能赢得超过八场比赛。相比之下，在红土场上，她表现得最像人类，她的胜利次数接近75%（在90场比赛中赢得了67场）。在硬地上，她大多数比赛都在这里进行，她已经赢得了超过240场胜利，超过80%的时间（在290场比赛中赢得了243场）取得了胜利。'
- en: 8.4.2\. Sibling rivalry for the ages
  id: totrans-389
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.4.2\. 持久性的姐妹竞争
- en: Serena Williams is not alone in the Williams family when it comes to dominance
    in the sport of tennis.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 塞雷娜·威廉姆斯在网球运动中的统治地位并非威廉姆斯家族中唯一。
- en: '|  |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Scenario
  id: totrans-392
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 场景
- en: 'Serena Williams’ story is made all the more interesting by her rivalry with
    another tennis great, her sister: Venus Williams, an Olympic gold medalist and
    five-time Wimbledon winner. Coming to terms with the fact that a story about Serena’s
    dominance across court types is not going to be very interesting—she’s amazing,
    we get it—the same sports writer has asked us to assess which sister has the advantage
    over the other on each type of court.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 塞雷娜·威廉姆斯的故事因她与另一位网球巨星的竞争而更加有趣，这位巨星就是她的姐妹：维纳斯·威廉姆斯，奥运金牌得主和五次温布尔登冠军。考虑到关于塞雷娜在所有球场类型上的统治地位的故事不会很有趣——她太出色了，我们明白这一点——同一位体育记者还要求我们评估每位姐妹在每种球场类型上的优势。
- en: '|  |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'We’ll attack this scenario just like the last one: working our way backwards
    from the results we want to the transformations we need to make. We know we’ll
    need our data organized by court type—that’s what the reporter wants to see—and
    we’ll also need counts of each sister’s victories on those courts. Sounds like
    our court types should be the keys, and the winners and wins should be our values.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将像上次一样处理这个场景：从我们想要的结果反向工作到我们需要进行的转换。我们知道我们需要按球场类型组织我们的数据——这就是报告者想要看到的内容——我们还需要统计每位姐妹在这些球场上的胜利次数。听起来我们的球场类型应该是键，而胜者和胜利次数应该是我们的值。
- en: Because we know that both Williams sisters will play in all the matches they
    play against each other, it’s enough to count the winner—the loser of the match
    will be whoever doesn’t win. Our `.mapper`, then, will check to see if it’s a
    match between the sisters, and if it is, it’ll output the surface and the winner.
    Our `.reducer` will count up the wins for each sister by surface type. This process
    is illustrated in [figure 8.10](#ch08fig10).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们知道威廉姆斯姐妹会在她们之间进行的所有比赛中都参赛，所以我们只需要计算胜者——输掉比赛的人将是那些没有获胜的人。因此，我们的 `.mapper`
    将会检查是否是姐妹之间的比赛，如果是的话，它将输出比赛表面和胜者。我们的 `.reducer` 将会根据表面类型统计每位姐妹的胜利次数。这个过程在[图8.10](#ch08fig10)中得到了说明。
- en: Figure 8.10\. The `MRJob` workflow uses keys and values to count up wins for
    the Williams sisters by surface.
  id: totrans-397
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.10\. `MRJob` 工作流程使用键和值来统计威廉姆斯姐妹在不同表面上的胜利次数。
- en: '![](08fig10_alt.jpg)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig10_alt.jpg)'
- en: 'Programmatically, we’ll have to create another class inheriting from the `MRJob`
    class. This one we’ll call `WilliamsRivalry`. The `WilliamsRivalry` class will
    need two methods: a `.mapper` and a `.reducer`. The `.mapper` will split the lines
    up into fields, check that both Venus and Serena Williams are playing, and output
    the winning sister and the surface they played on. The `.reducer` will need to
    count up each sister’s victories on the different types of courts. The code will
    look like the following listing.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 从程序的角度来看，我们不得不创建另一个从 `MRJob` 类继承的类。我们将这个类称为 `WilliamsRivalry`。`WilliamsRivalry`
    类将需要两个方法：一个 `.mapper` 和一个 `.reducer`。`.mapper` 将将行分割成字段，检查维纳斯和塞雷娜·威廉姆斯是否参赛，并输出获胜的姐妹和她们比赛的表面。`.reducer`
    需要统计每位姐妹在不同类型球场上的胜利次数。代码看起来如下所示。
- en: Listing 8.6\. Evaluating the Williams sisters’ rivalry with `MRJob`
  id: totrans-400
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.6\. 使用 `MRJob` 评估威廉姆斯姐妹的竞争关系
- en: '[PRE18]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: A lot of the code in [listing 8.6](#ch08ex06) is similar to the code in [listing
    8.5](#ch08ex05). In fact, the only substantive change is in the `.mapper` method.
    The new `.mapper` method breaks each line up into fields like our old `.mapper`,
    then creates a `players` variable—a string that holds the names of the winning
    and losing players. We use this to check that each of the Williams sisters is
    playing. If we find both sisters’ names in the `players` variable, we can then
    output the surface type, which is stored in the 2nd position, and the winner,
    which is stored in the 10th position.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.6中的大部分代码与列表8.5中的代码相似。事实上，唯一的实质性变化是在`.mapper`方法中。新的`.mapper`方法将每一行分解成像我们旧的`.mapper`那样的字段，然后创建一个`players`变量——一个包含获胜者和失败者名字的字符串。我们使用这个变量来检查威廉姆斯姐妹是否在比赛。如果我们发现`players`变量中包含两位姐妹的名字，我们就可以输出表面类型，它存储在第2个位置，以及获胜者，它存储在第10个位置。
- en: Then, because our `my_frequencies` function counts up whatever is passed to
    it, we can achieve the desired results without changing our `.reducer` at all.
    Instead of counting up wins and losses by surface type, the counter will count
    up the winners by surface type. Ultimately, the `.reducer` will output the surface
    type and a `dict` containing each sister’s name and the number of times they bested
    the other on that surface.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，因为我们的`my_frequencies`函数计算传递给它的任何内容，我们可以不改变我们的`.reducer`就能达到预期的结果。而不是按表面类型计算胜负，计数器将按表面类型计算获胜者。最终，`.reducer`将输出表面类型和一个包含每个姐妹的名字以及她们在该表面类型上击败对方次数的`dict`。
- en: 'We can run this code from the command line, remembering to pass the path to
    the data as an argument, and we should see output like this:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从命令行运行此代码，记得传递数据的路径作为参数，我们应该看到如下输出：
- en: '[PRE19]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: From the output, we can see that the siblings are competitive across both grass
    and hard courts. Venus won two of the grass-court matches, and Serena won four
    (66%). In hard-court matches, Serena is besting her sister in a similar percentage
    of the matches they play (64%). For Serena, winning 64% of the matches is a poor
    showing—remember that she won nearly 80% of her matches against the professional
    circuit at large on hard courts and nearly 90% of her matches on grass.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中，我们可以看到兄弟姐妹在草地和硬地上的竞争都很激烈。维纳斯赢得了两场草地比赛，而塞蕾娜赢得了四场（66%）。在硬地比赛中，塞蕾娜在与她妹妹比赛的比赛中以相似的比例击败了她的妹妹（64%）。对于塞蕾娜来说，赢得64%的比赛表现不佳——记住她在硬地上的比赛中有近80%是对抗职业巡回赛，在草地上有近90%。
- en: 8.5\. Exercises
  id: totrans-407
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5\. 练习
- en: 8.5.1\. Hadoop data formats
  id: totrans-408
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.5.1\. Hadoop数据格式
- en: Which data format does `MRJob` use to share data between the `map` step and
    the `reduce` step? (Choose one.)
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '`MRJob`使用哪种数据格式在`map`步骤和`reduce`步骤之间共享数据？（选择一个。）'
- en: Binary
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二进制
- en: Raw text
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始文本
- en: JSON
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSON
- en: Pickle
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pickle
- en: 8.5.2\. More Hadoop data formats
  id: totrans-414
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.5.2\. 更多Hadoop数据格式
- en: '**True** or **False**: Parallel processes like Hadoop MapReduce jobs are deterministic—their
    outputs are always produced in the same order.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '**正确**或**错误**：像Hadoop MapReduce作业这样的并行进程是确定性的——它们的输出总是以相同的顺序产生。'
- en: 8.5.3\. Hadoop’s native tongue
  id: totrans-416
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.5.3\. Hadoop的本地语言
- en: Which of the following languages is Hadoop written in? (Choose one.)
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop是用以下哪种语言编写的？（选择一个。）
- en: Haskell
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haskell
- en: C++
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C++
- en: JavaScript
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JavaScript
- en: Java
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java
- en: 8.5.4\. Designing common patterns in MRJob
  id: totrans-422
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.5.4\. 在MRJob中设计常见模式
- en: When working with `MRJob`, we’ll achieve better performance if we attempt to
    code in an `MRJob` style—using keys and values along with `mapper`s and `reducer`s.
    Implement some of the common map and reduce style patterns we’ve seen so far.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`MRJob`时，如果我们尝试以`MRJob`风格进行编码——使用键和值以及`mapper`和`reducer`，我们将获得更好的性能。实现我们迄今为止看到的常见映射和归约风格模式。
- en: '|  |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-425
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The code snippets here illustrate the functionality of the desired functions.
    You’ll want to implement the `MRJob` class for each snippet.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提供的代码片段展示了所需函数的功能。您将需要为每个片段实现`MRJob`类。
- en: '|  |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '***Filter—*** Take a sequence and return a subset of that sequence.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过滤（Filter）**——取一个序列并返回该序列的子集。'
- en: '[PRE20]'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '***Frequencies—*** Take a sequence and count the things in that sequence.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**频率（Frequencies）**——取一个序列并计算该序列中的事物。'
- en: '[PRE21]'
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '***GroupBy—*** Group a sequence by values resulting from a function.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分组（GroupBy）**——根据函数的结果值对序列进行分组。'
- en: '[PRE22]'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '***CountBy—*** Get counts of keys resulting from a function.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计数（CountBy）**——获取函数结果的键的计数。'
- en: '[PRE23]'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Summary
  id: totrans-436
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: JSON is a data format that we can use to pass complex data structures between
    the `mapper` and `reducer` steps of an Apache Hadoop Streaming MapReduce job.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSON是一种数据格式，我们可以用它来在Apache Hadoop Streaming MapReduce作业的`mapper`步骤和`reducer`步骤之间传递复杂的数据结构。
- en: We use the `json.dumps()` and `json.loads()` functions from Python’s json library
    to achieve this transfer.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 Python 的 json 库中的 `json.dumps()` 和 `json.loads()` 函数来实现这种转换。
- en: We can use the mrjob library to write MapReduce jobs without having to interact
    directly with Hadoop.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用 mrjob 库来编写 MapReduce 作业，而无需直接与 Hadoop 交互。
- en: The mrjob library forces us to think about our `map` and `reduce` steps as taking
    in and spitting out key-value pairs.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mrjob 库迫使我们思考我们的 `map` 和 `reduce` 步骤是接受和输出键值对。
- en: Hadoop uses these keys under the hood to allocate data to the proper location.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 在底层使用这些键来分配数据到正确的位置。
- en: The mrjob library enforces JSON data exchange between the `mapper` and `reducer`
    phases, so we need to ensure that our output data is JSON serializable.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mrjob 库强制在 `mapper` 和 `reducer` 阶段之间进行 JSON 数据交换，因此我们需要确保我们的输出数据是可序列化的 JSON。
- en: The mrjob library was designed for big data processing in the cloud—it has excellent
    support for Amazon Web Services’ Elastic MapReduce, which we will cover in [chapter
    12](kindle_split_023.html#ch12).
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mrjob 库是为云中的大数据处理而设计的——它对亚马逊网络服务（Amazon Web Services）的弹性映射和减少（Elastic MapReduce）提供了出色的支持，我们将在
    [第 12 章](kindle_split_023.html#ch12) 中介绍。
- en: Chapter 9\. PageRank with map and reduce in PySpark
  id: totrans-444
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 9 章\. 在 PySpark 中使用 map 和 reduce 实现 PageRank
- en: '*This chapter covers*'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Options for parallel map and reduce routines in PySpark
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 中并行 map 和 reduce 程序的选项
- en: Convenience methods of PySpark’s `RDD` class for common operations
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 的 `RDD` 类的便捷方法，用于常见操作
- en: Implementing the historic PageRank algorithm in PySpark
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 PySpark 中实现历史悠久的 PageRank 算法
- en: In [chapter 7](kindle_split_017.html#ch07), we learned about Hadoop and Spark,
    two frameworks for distributed computing. In [chapter 8](kindle_split_018.html#ch08),
    we dove into the weeds of Hadoop, taking a close look at how we might use it to
    parallelize our Python work for large datasets. In this chapter, we’ll become
    familiar with PySpark—the Scala-based, in-memory, large dataset processing framework.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 7 章](kindle_split_017.html#ch07) 中，我们了解了 Hadoop 和 Spark，这两个是分布式计算框架。在 [第
    8 章](kindle_split_018.html#ch08) 中，我们深入研究了 Hadoop 的细节，仔细查看我们如何使用它来并行化我们的 Python
    工作以处理大型数据集。在本章中，我们将熟悉 PySpark——基于 Scala、内存中、用于处理大型数据集的框架。
- en: 'As mentioned in [chapter 7](kindle_split_017.html#ch07), Spark has some advantages:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [第 7 章](kindle_split_017.html#ch07) 中所述，Spark 有一些优势：
- en: Spark can be very, very fast.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 可以非常快。
- en: Spark programs use all the same map and reduce techniques we learned about in
    [chapters 2](kindle_split_011.html#ch02) through [6](kindle_split_015.html#ch06).
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 程序使用我们在 [第 2 章](kindle_split_011.html#ch02) 到 [第 6 章](kindle_split_015.html#ch06)
    中学到的所有相同的 map 和 reduce 技术。
- en: We can code our Spark programs entirely in Python, taking advantage of the thorough
    PySpark API.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以完全用 Python 编写我们的 Spark 程序，利用 PySpark API 的全面性。
- en: 'In this chapter, we’ll take a look at how we can make the most of PySpark by
    focusing on its foundational class: the `RDD`—Resilient Distributed Dataset. We’ll
    explore the map and reduce-like methods of the `RDD` that we can use to perform
    familiar map and reduce workflows in parallel. We’ll learn about some of the `RDD`
    class’s convenience methods that make our lives easier. And we’ll learn all this
    by implementing the PageRank algorithm—the simple but elegant ranking algorithm
    that once formed the backbone of Google’s search.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何通过关注其基础类 `RDD`（弹性分布式数据集）来充分利用 PySpark。我们将探索 `RDD` 的 map 和 reduce
    类似方法，我们可以使用这些方法并行执行熟悉的 map 和 reduce 工作流程。我们将了解一些使我们的生活更轻松的 `RDD` 类便捷方法。而且，我们将通过实现
    PageRank 算法来学习所有这些——这是一个简单但优雅的排名算法，曾经是谷歌搜索的骨干。
- en: 9.1\. A closer look at PySpark
  id: totrans-455
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1\. 深入了解 PySpark
- en: In [chapter 7](kindle_split_017.html#ch07), we introduced Spark and saw that
    we could use it to write Python code and have that code translated into fast parallel
    map and reduce programs. This process of translation—from Python into Scala—was
    reflected in the style of our Python code. In this chapter, we’ll take a look
    at the map and reduce style utilities available to us through PySpark’s `RDD`
    class.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 7 章](kindle_split_017.html#ch07) 中，我们介绍了 Spark 并看到我们可以用它来编写 Python 代码，并将这些代码转换为快速的并行
    map 和 reduce 程序。这种从 Python 到 Scala 的转换过程反映在我们的 Python 代码风格中。在本章中，我们将探讨通过 PySpark
    的 `RDD` 类可用的 map 和 reduce 风格实用工具。
- en: 'The `RDD` class has methods that we can group into three categories:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '`RDD` 类有方法，我们可以将其分为三个类别：'
- en: '**`map`*-like methods—*** Methods we can use to replicate the function of `map`'
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**`map`-类似的方法——** 可以用来复制 `map` 功能的方法'
- en: '**`reduce`*-like methods—*** Methods we can use to replicate the function of
    `reduce`'
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**`reduce`-类似的方法——** 可以用来复制 `reduce` 功能的方法'
- en: '***Convenience methods—*** Methods that solve common problems'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***便利方法——*** 解决常见问题的方法'
- en: We’ve seen functions throughout this book that fall into each of these categories;
    for example, the map variations (`imap`, `starmap`) all fall into `map`-like methods,
    and functions like `filter` and `frequencies` fall into the convenience methods.
    PySpark has its own tools that offer similar convenience as well as PySpark `RDD`-based
    parallelization.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们已经看到了属于这些类别中的函数；例如，map变体（`imap`、`starmap`）都属于类似`map`的方法，而像`filter`和`frequencies`这样的函数属于便利方法。PySpark有自己的工具，提供了类似的便利性以及基于PySpark
    `RDD`的并行化。
- en: 9.1.1\. Map-like methods in PySpark
  id: totrans-462
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.1\. PySpark中的类似map的方法
- en: 'We’ll start our closer look at PySpark by examining `map`-like methods: `.map`,
    `.flatMap`, `.mapValues`, `.flatMapValues`, `.mapPartitions`, and `.mapPartitionsWithIndex`.
    You’re already familiar with the first two—we’ve seen them in previous chapters.
    The second two are unique to Spark and require us to dive a little more into how
    Spark works. In this section, we’ll take a look at how we can use these methods
    to replicate the `map` behaviors we’ve seen in previous chapters.'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始更深入地研究PySpark，首先检查类似`map`的方法：`.map`、`.flatMap`、`.mapValues`、`.flatMapValues`、`.mapPartitions`和`.mapPartitionsWithIndex`。你已经熟悉前两个——我们在前面的章节中见过它们。后两个是Spark独有的，需要我们更深入地了解Spark的工作原理。在本节中，我们将探讨如何使用这些方法来复制我们在前面章节中看到的`map`行为。
- en: '|  |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Refresher
  id: totrans-465
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 复习
- en: Resilient Distributed Dataset objects are the foundation of Spark’s power. They
    are an abstraction that allows programmers to use high-level methods (like `.map`
    and `.reduce`) to execute parallel operations in-memory across a distributed system.
    Because `RDD`s hold as much data in memory as possible, Spark can be much, much
    faster than Hadoop. In PySpark, most of the parallel operations we’ll want to
    take advantage of are implemented as methods to an `RDD` class. This class represents
    the Resilient Distributed Dataset we’re operating on.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性分布式数据集（Resilient Distributed Dataset）对象是Spark强大功能的基础。它们是一种抽象，允许程序员使用高级方法（如`.map`和`.reduce`）在分布式系统内执行内存中的并行操作。因为`RDD`尽可能多地保留内存中的数据，所以Spark可以比Hadoop快得多。在PySpark中，我们想要利用的大多数并行操作都作为`RDD`类的方法实现。这个类代表我们正在操作的可恢复分布式数据集。
- en: '|  |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The `RDD`’s `.map` method, as we would expect, takes a function and applies
    it to each of the elements of our `RDD`. For example, if we open some text files
    with `SparkContext`’s `.textFile` method (which we introduced in [chapter 7](kindle_split_017.html#ch07)),
    we would map a function over the resulting strings. We can imagine a function
    `make_words` that splits a string into words and would turn our list of text strings
    into a list of word lists, with one word list for each string. We can see this
    process in [figure 9.1](#ch09fig01).
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所期待的，`RDD`的`.map`方法接受一个函数并将其应用于`RDD`中的每个元素。例如，如果我们使用`SparkContext`的`.textFile`方法（我们在第7章中介绍了这个方法）打开一些文本文件，我们将对生成的字符串映射一个函数。我们可以想象一个`make_words`函数，它将字符串分割成单词，并将我们的文本字符串列表转换成单词列表的列表，每个字符串对应一个单词列表。我们可以在[图9.1](#ch09fig01)中看到这个过程。
- en: Figure 9.1\. The `RDD.map` method maps across the `RDD`, in this case turning
    words in each string into a list of strings.
  id: totrans-469
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.1\. `RDD.map`方法在`RDD`上映射，在这种情况下将每个字符串中的单词转换成字符串列表。
- en: '![](09fig01_alt.jpg)'
  id: totrans-470
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig01_alt.jpg)'
- en: Like we saw in [listing 7.7](kindle_split_017.html#ch07ex07), though, sometimes
    we’ll want one big sequence instead of a sequences of sequences. For that, we
    can use the `RDD .flatMap` method. `.flatMap` is equivalent to `map`, but it returns
    a flattened sequence of the elements. Using the same example from [figure 9.1](#ch09fig01),
    `.flatMap` would return a single long sequence of words, disregarding the information
    about which string they came from. [Figure 9.2](#ch09fig02) shows an example of
    this.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在[列表7.7](kindle_split_017.html#ch07ex07)中看到的那样，然而，有时我们可能想要一个大的序列而不是序列的序列。为此，我们可以使用`RDD
    .flatMap`方法。`.flatMap`与`map`等效，但它返回一个扁平化的元素序列。使用与[图9.1](#ch09fig01)相同的示例，`.flatMap`将返回一个单词的单长序列，不考虑它们来自哪个字符串。[图9.2](#ch09fig02)展示了这样一个示例。
- en: Figure 9.2\. The `RDD.flatMap` method returns a flattened sequence and is useful
    when we’re interested in the elements of each partition all together.
  id: totrans-472
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.2\. `RDD.flatMap`方法返回一个扁平化的序列，当我们对每个分区的元素整体感兴趣时非常有用。
- en: '![](09fig02_alt.jpg)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig02_alt.jpg)'
- en: The `.mapValues` and `.flatMapValues` methods are like `.map` and `.flatMap`,
    except they operate only on the values of key-value pairs. For example, we may
    have data about web pages on our site and the IP addresses that visited them,
    and we’re interested in the number of unique visitors for each page. Assuming
    we had this data stored as key-value pairs, we could then use `.mapValues` to
    retain the information in the key (the web page) but alter each value, transforming
    a list of IP addresses into a count. We can see this example in [figure 9.3](#ch09fig03).
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '`.mapValues`和`.flatMapValues`方法类似于`.map`和`.flatMap`，但它们只对键值对中的值进行操作。例如，我们可能对我们网站上的网页和访问它们的IP地址有数据，我们感兴趣的是每个页面的独立访问者数量。假设我们把这些数据存储为键值对，然后我们可以使用`.mapValues`来保留键（网页）中的信息，但改变每个值，将IP地址列表转换为计数。我们可以在[图9.3](#ch09fig03)中看到这个例子。'
- en: Figure 9.3\. You can use the `RDD`’s `.mapValues` method to retain the keys
    while altering the values.
  id: totrans-475
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.3。您可以使用`RDD`的`.mapValues`方法在改变值的同时保留键。
- en: '![](09fig03_alt.jpg)'
  id: totrans-476
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig03_alt.jpg)'
- en: Lastly, the `.mapPartions` and the `.mapPartionsWithIndex` methods of the `RDD`
    class are variations of `map` that are *partition aware*—they know which partition
    of our `RDD` the data being processed resides on. Partitions, as we mentioned
    in [chapter 7](kindle_split_017.html#ch07), are the abstraction that `RDD`s use
    to implement parallelization. The data in an `RDD` is split up across different
    partitions, and each partition is handled in memory. It is common in (very) large
    data tasks to partition an `RDD` by a key. For example, if we have an enormously
    popular web page—going back to the example we just discussed in [figure 9.3](#ch09fig03)—we
    may partition the website by page using the `RDD`’s `.partitionBy` method. Using
    this partitioning strategy, we could perform operations on each page in memory
    in a single partition (in other words, we’d perform those operations quickly).
    `.mapPartitions` and `.mapPartitionsWithIndex` are the `.map` and `.mapValues`
    equivalents for partitions. An example of this is shown in [figure 9.4](#ch09fig04).
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`RDD`类的`.mapPartitions`和`.mapPartitionsWithIndex`方法是`map`的变体，它们是*分区感知*的——它们知道正在处理的数据位于我们的`RDD`的哪个分区上。正如我们在[第7章](kindle_split_017.html#ch07)中提到的，分区是`RDD`s用来实现并行化的抽象。`RDD`中的数据被分割到不同的分区中，每个分区都在内存中处理。在（非常）大的数据任务中，通常通过键对`RDD`进行分区。例如，如果我们有一个非常受欢迎的网页——回到我们刚才在[图9.3](#ch09fig03)中讨论的例子——我们可能会使用`RDD`的`.partitionBy`方法按页面来分区网站。使用这种分区策略，我们可以在单个分区（换句话说，我们会快速）内对每个页面进行内存操作。`.mapPartitions`和`.mapPartitionsWithIndex`是`.map`和`.mapValues`在分区上的等价方法。一个例子可以在[图9.4](#ch09fig04)中看到。
- en: Figure 9.4\. Partitioning a large dataset by logical keys optimizes our compute
    processes and makes future join operations easier.
  id: totrans-478
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.4。通过逻辑键对大型数据集进行分区优化了我们的计算过程，并使得未来的连接操作更容易。
- en: '![](09fig04_alt.jpg)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig04_alt.jpg)'
- en: 9.1.2\. Reduce-like methods in PySpark
  id: totrans-480
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.2. PySpark中的类似`reduce`的方法
- en: 'Of course, while we need `map` to do our data transformation, we also need
    `reduce` to summarize our data. The `RDD` class has three methods that I consider
    `reduce`-like:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，虽然我们需要`map`来进行数据转换，但我们还需要`reduce`来汇总数据。`RDD`类有三个我认为类似于`reduce`的方法：
- en: '`.reduce`'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.reduce`'
- en: '`.fold`'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.fold`'
- en: '`.aggregate`'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.aggregate`'
- en: 'Each of these methods has a `byKey` variation:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 每种方法都有一个`byKey`变体：
- en: '`.reduceByKey`'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.reduceByKey`'
- en: '`.foldByKey`'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.foldByKey`'
- en: '`.aggregateByKey`'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.aggregateByKey`'
- en: The methods `.reduce`, `.fold`, and `.aggregate` are all similar to Python’s
    `reduce` function you’ve gotten to know, except that they each have differing
    levels of assumptions—with `.reduce` being the most presumptive, `.aggregate`
    being the most flexible, and `.fold` falling in between.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 方法`.reduce`、`.fold`和`.aggregate`都与您所熟知的Python的`reduce`函数类似，但它们各自有不同的假设级别——其中`.reduce`假设最多，`.aggregate`最灵活，而`.fold`介于两者之间。
- en: The `RDD .reduce` method provides `reduce` functionality—taking a sequence and
    accumulating it into some other data structure—however, we can’t provide either
    an initializer value or a combination function to `RDD`’s `.reduce` method. This
    means that to use the `RDD .reduce` method, we’ll expect to have the same data
    type all the way through the operation—including our ultimate data structure.
    A good example of the type of operation this would be suited for is summation.
    In summation, all of our elements will be numeric data types, and our output value
    will be a numeric data type.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '`RDD .reduce` 方法提供了 `reduce` 功能——接受一个序列并将其累积到其他数据结构中——然而，我们无法为 `RDD` 的 `.reduce`
    方法提供初始化值或组合函数。这意味着要使用 `RDD .reduce` 方法，我们期望在整个操作过程中数据类型保持一致——包括我们的最终数据结构。这种操作的一个很好的例子是求和。在求和中，所有我们的元素都将是有数字数据类型，我们的输出值也将是数字数据类型。'
- en: Slightly more nuanced, the `.fold` method allows us to provide an initializer
    value in addition to an aggregation operation. This makes `.fold` suitable in
    situations where we may want to have a guaranteed value that doesn’t exist in
    our sequence. For example, if we wanted to find the minimum value of a sequence
    of numbers but we wanted to ensure that it would be at least as small as one,
    we could use `.fold` with the `min` function and 1 as an initializer.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 略微复杂一些，`.fold` 方法允许我们除了聚合操作外，还可以提供一个初始化值。这使得 `.fold` 在我们可能想要有一个保证值，而这个值在我们的序列中不存在的场景下非常适用。例如，如果我们想要找到一个数字序列的最小值，但同时又想确保它至少和1一样小，我们可以使用
    `.fold` 方法结合 `min` 函数和1作为初始化值。
- en: The `.aggregate` method provides all the functionality of a parallel `reduce`.
    We can provide an initializer value, an aggregation function, and a combination
    function. (We introduced combination functions in [chapter 6](kindle_split_015.html#ch06)
    on parallel `reduce`. They provide the instructions for how to join work accumulated
    in parallel by the accumulation functions and may be different from the accumulation
    functions in complex workflows.) We can use this method for anything `.reduce`
    and `.fold` can do, and anything else we may want to use a parallel `reduc`e for.
    [Table 9.1](#ch09table01) summarizes the differences between the methods.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '`.aggregate` 方法提供了并行 `reduce` 的所有功能。我们可以提供一个初始化值、一个聚合函数和一个组合函数。（我们在第6章介绍了组合函数，关于并行
    `reduce`。它们提供了如何将累积函数在并行中累积的工作合并的指令，并且可能不同于复杂工作流中的累积函数。）我们可以使用这个方法来完成`.reduce`和`.fold`能做的任何事情，以及我们可能想要使用并行
    `reduce` 的任何其他事情。[表9.1](#ch09table01)总结了这些方法之间的差异。'
- en: Table 9.1\. Differences between the `RDD`’s `.reduce`, `.fold`, and `.aggregate`
    methods
  id: totrans-493
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表9.1\. `RDD`的`.reduce`、`.fold`和`.aggregate`方法之间的差异
- en: '| Method | Aggregate | Initialize | Combine |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 聚合 | 初始化 | 组合 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| RDD.reduce() | Yes | No | No |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| RDD.reduce() | 是 | 否 | 否 |'
- en: '| RDD.fold() | Yes | Yes | No |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| RDD.fold() | 是 | 是 | 否 |'
- en: '| RDD.aggregate() | Yes | Yes | Yes |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| RDD.aggregate() | 是 | 是 | 是 |'
- en: 'As mentioned, each of the three methods we just looked at—`.reduce`, `.fold`,
    and `.aggregate`—also has a `byKey` variation: `.reduceByKey`, `.foldByKey`, and
    `.aggregateByKey`. Each of these methods works like the previous methods, but
    they operate on the values of a sequence of key-value pairs and only accumulate
    one value per key. For example, if we had a sequence of keys and values indicating
    pages and the number of seconds a user spent on a page during a single visit to
    it, we could get totals for each page using the `.reduceByKey` method. This is
    illustrated in [figure 9.5](#ch09fig05) and shown in [listing 9.1](#ch09ex01).'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们刚才查看的三个方法——`.reduce`、`.fold`和`.aggregate`——也都有一个 `byKey` 变体：`.reduceByKey`、`.foldByKey`和`.aggregateByKey`。这些方法的工作方式与之前的方法类似，但它们操作的是键值对序列的值，并且每个键只累积一个值。例如，如果我们有一个键值对序列，表示页面和用户在单个访问中在该页面上花费的秒数，我们可以使用
    `.reduceByKey` 方法获取每个页面的总计。这如图9.5所示，并在列表9.1中展示。
- en: Figure 9.5\. You can use the `.reduceByKey` method (as well as `.foldByKey`
    and `.aggregateByKey`) to accumulate values specifically for each key in a sequence
    of key-value pairs.
  id: totrans-500
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.5\. 你可以使用`.reduceByKey`方法（以及`.foldByKey`和`.aggregateByKey`）来累积序列中每个键的特定值。
- en: '![](09fig05_alt.jpg)'
  id: totrans-501
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig05_alt.jpg)'
- en: Listing 9.1\. Counting page visit time with `.reduceByKey`
  id: totrans-502
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.1\. 使用`.reduceByKey`计算页面访问时间
- en: '[PRE24]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 9.1.3\. Convenience methods in PySpark
  id: totrans-504
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.3\. PySpark中的便利方法
- en: Lastly, PySpark provides a number of convenience methods for manipulating `RDD`s.
    Many of these mirror the convenience functions in the functools, itertools, and
    toolz libraries we’ve seen already in [chapters 4](kindle_split_013.html#ch04)
    and [6](kindle_split_015.html#ch06). Others are Python mirrors of methods that
    exist in Scala—the language in which Spark is written. Methods you should be aware
    of include
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，PySpark 为操作 `RDD` 提供了许多便利方法。其中许多与我们在第 4 章（[kindle_split_013.html#ch04](https://example.org/kindle_split_013.html#ch04)）和第
    6 章（[kindle_split_015.html#ch06](https://example.org/kindle_split_015.html#ch06)）中看到的
    functools、itertools 和 toolz 库中的便利函数相对应。其他的是 Scala 语言中存在的方法的 Python 版本——Spark 就是用这种语言编写的。你应该注意的方法包括
- en: '`.countByKey()`'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.countByKey()`'
- en: '`.countByValue()`'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.countByValue()`'
- en: '`.distinct()`'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.distinct()`'
- en: '`.countApproxDistinct()`'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.countApproxDistinct()`'
- en: '`.filter()`'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.filter()`'
- en: '`.first()`'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.first()`'
- en: '`.groupBy()`'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.groupBy()`'
- en: '`.groupByKey()`'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.groupByKey()`'
- en: '`.saveAsTextFile()`'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.saveAsTextFile()`'
- en: '`.take()`'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.take()`'
- en: The .filter, .first, and .take methods of Spark’s RDD
  id: totrans-516
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Spark 的 RDD 的 `.filter`、`.first` 和 `.take` 方法
- en: 'Let’s start with the ones that have direct mirrors in previous chapters: `.filter`,
    `.first`, and `.take`. The `RDD` class’s `.filter` method behaves like Python’s
    `filter` function: it uses a function to return a new sequence with only elements
    that pass the filter by making the function return `True`. The `RDD` class’s `.first`
    method returns the first value in the sequence. And the `.take` method, like `take`
    from toolz, allows us to retrieve the first however many elements of a sequence.'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从那些在前面章节中有直接对应的方法开始：`.filter`、`.first` 和 `.take`。`RDD` 类的 `.filter` 方法的行为类似于
    Python 的 `filter` 函数：它使用一个函数来返回一个新序列，只包含通过过滤函数返回 `True` 的元素。`RDD` 类的 `.first`
    方法返回序列中的第一个值。而 `.take` 方法，就像 toolz 中的 `take` 方法一样，允许我们检索序列中的前若干个元素。
- en: Counting elements of an RDD with .countByKey and .countByValue
  id: totrans-518
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用 `.countByKey` 和 `.countByValue` 计数 RDD 的元素
- en: Next, we have `.countByKey` and `.countByValue`. These methods behave like the
    `frequencies` function—both the one we built ourselves and the one implemented
    in toolz. We can use these methods to get a key-value sequence of things and their
    counts. `.countByKey` returns counts of the keys in the `RDD`, whereas `.countByValue`
    returns counts of the values. We can see an example of how the two differ in the
    following listing.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们介绍 `.countByKey` 和 `.countByValue` 方法。这两个方法的行为类似于 `frequencies` 函数——无论是我们自己构建的还是
    toolz 中实现的。我们可以使用这些方法来获取事物的键值序列及其计数。`.countByKey` 返回 `RDD` 中键的计数，而 `.countByValue`
    返回值的计数。以下列表展示了这两个方法如何不同。
- en: Listing 9.2\. PySpark `RDD`’s `.countByKey` and `.countByValue` methods
  id: totrans-520
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.2\. PySpark `RDD` 的 `.countByKey` 和 `.countByValue` 方法
- en: '[PRE25]'
  id: totrans-521
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[Listing 9.2](#ch09ex02) shows that if we have an `RDD` of words and their
    lengths as `tuple`s, we can use the `.countByKey` method to get a count of all
    the unique words and the `.countByValue` method to get a count of the unique lengths.
    In this case, the words are acting as the keys because they’re in first position,
    and the lengths are acting as the values because they’re in second position.'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 9.2](#ch09ex02) 展示了，如果我们有一个包含单词及其长度作为 `tuple` 的 `RDD`，我们可以使用 `.countByKey`
    方法来获取所有唯一单词的计数，以及使用 `.countByValue` 方法来获取唯一长度的计数。在这种情况下，单词作为键，因为它们位于第一个位置，而长度作为值，因为它们位于第二个位置。'
- en: Counting unique things with RDD’s .countApproxDistinct
  id: totrans-523
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用 RDD 的 `.countApproxDistinct` 计数唯一元素
- en: Another counting method that’s useful—especially when working with large datasets—is
    the `.countApproxDistinct` method. Often, we want to know how many unique elements
    are in our dataset. How many unique words were used in a document collection?
    How many unique IP addresses are in our logs? How many unique sessions visited
    our site? Spark provides the `.distinct` method for when we have a small enough
    dataset that it’s fine to calculate an exact number. The problem when we have
    large datasets is that these counts are time expensive; they require a full pass
    of often very long sequences. `.countApproxDistinct` allows that process to be
    sped up and parallelized, if a small window of error is allowable. It uses an
    approximation algorithm that is parallelizable, allowing us to benefit from the
    time savings of parallelization
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的计数方法——尤其是在处理大型数据集时——是 `.countApproxDistinct` 方法。我们通常想知道数据集中有多少唯一的元素。在一个文档集合中使用了多少唯一的单词？在我们的日志中有多少唯一的
    IP 地址？有多少唯一的会话访问了我们的网站？当数据集足够小，可以计算确切数字时，Spark 提供了 `.distinct` 方法。当我们有大型数据集时，这些计数是时间消耗昂贵的；它们需要遍历通常非常长的序列。`.countApproxDistinct`
    允许在允许一定误差窗口的情况下加速和并行化这个过程，它使用一个可并行化的近似算法，使我们能够从并行化中获得时间节省的好处。
- en: Collecting elements of an RDD with .groupBy and .groupByKey
  id: totrans-525
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用.groupBy和.groupByKey收集RDD的元素
- en: Another category of convenience methods we’ll want to know about are two methods—`.groupBy`
    and `.groupByKey`—we can use for restructuring our `RDD`. Each of these methods
    collects all the instances of items in our `RDD` and returns an `RDD` of key-value
    `tuple`s. For `.groupByKey`, the items are organized using the keys of the existing
    key-value `tuple`s. For `.groupBy`, the items are organized under new keys resulting
    from a function (that we get to provide) applied to each element of the `RDD`.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还想要了解的另一类便利方法包括两个方法——`.groupBy`和`.groupByKey`——我们可以使用它们来重构我们的`RDD`。这两个方法都会收集`RDD`中所有项目的实例，并返回一个键值`tuple`的`RDD`。对于`.groupByKey`，项目使用现有的键值`tuple`的键进行组织。对于`.groupBy`，项目在应用于`RDD`每个元素的一个函数（我们将提供）产生的新键下进行组织。
- en: For example, if we had a sequence of words and wanted to collect them based
    on their first letter, we would pass a function to `.groupBy` that returned the
    first character of a string.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有一个单词序列，并希望根据它们的首字母收集它们，我们会传递一个函数到`.groupBy`，该函数返回字符串的第一个字符。
- en: '[PRE26]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: If we had a sequence where we already had key-value `tuple`s, we could use .`groupByKey`,
    similarly, to obtain groupings of the elements that shared a key.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个已经包含键值`tuple`的序列，我们可以使用`.groupByKey`，同样地，来获取具有相同键的元素分组。
- en: '[PRE27]'
  id: totrans-530
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Somewhat counterintuitively, `.groupBy` is a special implementation of the `.groupByKey`
    method, so whenever you’re given a choice between the two, it’s better to use
    `.groupByKey`.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 有些反直觉的是，`.groupBy`是`.groupByKey`方法的一种特殊实现，所以当你在两者之间做出选择时，最好使用`.groupByKey`。
- en: Saving RDDs to text files
  id: totrans-532
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将RDD保存到文本文件
- en: 'Lastly, there’s the `.saveAsTextFile` method, which does what its name implies
    it does: it saves an `RDD` to a text file. Each element of the `RDD` will be written
    in string form to a text file, separated from the next element by a newline. This
    is excellent for a few reasons:'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，有`.saveAsTextFile`方法，正如其名称所暗示的那样，它将`RDD`保存到文本文件。`RDD`的每个元素将以字符串形式写入文本文件，每个元素之间由换行符分隔。这有几个优点：
- en: The data is in a human-readable, persistent format.
  id: totrans-534
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据是以可读、持久的形式存在的。
- en: We can easily read this data back into Spark with the `.textFile` method of
    `SparkContext`.
  id: totrans-535
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以轻松地将这些数据使用`SparkContext`的`.textFile`方法读回到Spark中。
- en: The data is well structured for other parallel tools, such as Hadoop’s MapReduce.
  id: totrans-536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据结构非常适合其他并行工具，例如Hadoop的MapReduce。
- en: We can specify a compression format for efficient data storage or transfer.
  id: totrans-537
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以指定一个压缩格式以实现高效的数据存储或传输。
- en: First, having the data in a persistent, human-readable format puts us in a good
    position to have high-quality data for a long time. Because the data is human
    readable, it can be manually inspected—even by nonprogrammers—to ensure that it’s
    free from errors. Because the data is in plain text and not bytecode, we have
    some security that changes to our operating system or our runtime environment
    won’t render the data obsolete.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将数据以持久、可读的格式保存，使我们能够长时间拥有高质量的数据。因为数据是可读的，它可以被手动检查——甚至是非程序员也可以检查——以确保它没有错误。因为数据是纯文本而不是字节码，所以我们有某种保障，即操作系统或我们的运行时环境的变化不会使数据过时。
- en: Second, we can quickly read the data back in using the `.textFile` method of
    `SparkContext`. This is excellent if we have textual data where we want to be
    working with strings, or if we have simply structured data. If our data is complex,
    we may not want to store the data in this format; the process of reconstituting
    it could be painful. Most of the work we’ll do in Spark will use straightforward
    data structures.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们可以使用`SparkContext`的`.textFile`方法快速读取数据。如果我们有文本数据，我们希望处理字符串，或者我们只是有结构化的数据，这将是极好的。如果我们有复杂的数据，我们可能不想以这种格式存储数据；重新构成它的过程可能会很痛苦。我们在Spark中做的绝大部分工作将使用直接的数据结构。
- en: Third, this format is excellent for Hadoop’s MapReduce, which expects a file
    with lines to process. If you have MapReduce code that you like and you’re doing
    work in Spark as well, this can be a great way to share data between the two processes.
    This is a common use case, with lots of teams having legacy MapReduce jobs they
    like but starting to incorporate more and more Spark into their work.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，这种格式非常适合Hadoop的MapReduce，它期望一个包含要处理的行的文件。如果你有喜欢的MapReduce代码，并且你也在Spark中工作，这可以是在两个进程之间共享数据的一种很好的方式。这是一个常见的用例，许多团队都有他们喜欢的遗留MapReduce作业，但开始越来越多地融入Spark到他们的工作中。
- en: 'Fourth, and finally, we can specify a compression format for the text file
    so it’s saved in a space-efficient way. A wide range of codecs are available for
    this, including two common codecs: bz2 and gzip. Between bz2 and gzip, bz2 is
    the slower, more compressed format, and gzip is the faster, less compressed format.
    Specifying a compression format will make the data unreadable by a human until
    it is decompressed. However, we don’t need to decompress the data before using
    it again in Spark or Hadoop jobs.'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 第四点，也是最后一点，我们可以指定文本文件的压缩格式，以便以空间高效的方式保存。为此，有多种编解码器可供选择，包括两种常见的编解码器：bz2 和 gzip。在
    bz2 和 gzip 之间，bz2 是较慢、压缩程度更高的格式，而 gzip 是较快、压缩程度较低的格式。指定压缩格式将使数据在解压缩之前对人类不可读。然而，我们不需要在
    Spark 或 Hadoop 作业中使用数据之前对其进行解压缩。
- en: To specify a compression format, we have to call the format’s full Hadoop codec
    name. The full name for bz2 is `org.apache.hadoop.io.compress.BZip2Codec`, and
    the full name for gzip is `org.apache.hadoop.io.compress.GzipCodec`.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 要指定压缩格式，我们必须调用格式的完整 Hadoop 编解码器名称。bz2 的完整名称为 `org.apache.hadoop.io.compress.BZip2Codec`，gzip
    的完整名称为 `org.apache.hadoop.io.compress.GzipCodec`。
- en: '[PRE28]'
  id: totrans-543
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: It’s convention to save a bz2 compressed file with a .bz2 ending and a gzip
    compressed file with a .gz ending.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 保存以 .bz2 结尾的 bz2 压缩文件和以 .gz 结尾的 gzip 压缩文件是惯例。
- en: 9.2\. Tennis rankings with Elo and PageRank in PySpark
  id: totrans-545
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2. 使用 PySpark 中的 Elo 和 PageRank 进行网球排名
- en: 'Now that we have the basics of Spark under us, let’s use it to build one of
    the classic large dataset algorithms: PageRank. Consider the scenario from [chapter
    8](kindle_split_018.html#ch08).'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了 Spark 的基础知识，让我们用它来构建一个经典的大数据集算法：PageRank。考虑第 8 章的场景 [chapter 8](kindle_split_018.html#ch08)。
- en: '|  |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Scenario
  id: totrans-548
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 场景
- en: A new professional tennis league is forming, and they have hired you to come
    up with skill estimates for professional tennis players so that they can direct
    their efforts for recruiting players to the new league. They have provided you
    with data for several years of matches and would like you to return to them with
    a list of players and their corresponding skills.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 一个新的职业网球联赛正在形成，他们聘请你来为职业网球运动员制定技能评估，以便他们可以针对新联赛招募球员。他们为你提供了多年的比赛数据，并希望你能返回一个包含球员及其对应技能的列表。
- en: '|  |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 9.2.1\. Revisiting Elo ratings with PySpark
  id: totrans-551
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.1. 使用 PySpark 重新审视 Elo 评分
- en: We looked at how we could solve this problem using Elo ratings—a ranking system
    that iteratively adjusts players’ scores after each win and loss—with Hadoop MapReduce.
    We can implement this solution in Spark as well, using Spark’s `reduce` capabilities.
    To do that, we’ll need to
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了如何使用 Elo 评分——一种在每次胜负后迭代调整玩家分数的排名系统——结合 Hadoop MapReduce 解决这个问题。我们也可以在 Spark
    中实现这个解决方案，利用 Spark 的 `reduce` 功能。为此，我们需要
- en: write Spark code to bring in the data
  id: totrans-553
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写 Spark 代码以导入数据
- en: copy the `elo_acc` accumulator function from [listing 8.2](kindle_split_018.html#ch08ex02)
  id: totrans-554
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 [列表 8.2](kindle_split_018.html#ch08ex02) 复制 `elo_acc` 累加器函数
- en: call the `elo_acc` function with the right Spark `reduce`-like method
  id: totrans-555
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用正确的 Spark `reduce`-like 方法调用 `elo_acc` 函数
- en: We can see what this will look like in the following listing.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下面的列表中看到这将是什么样子。
- en: Listing 9.3\. Elo rating reduction in Spark
  id: totrans-557
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.3. Spark 中的 Elo 评分减少
- en: '[PRE29]'
  id: totrans-558
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The majority of the code in the listing comes from [chapter 8](kindle_split_018.html#ch08)
    and needs little further explanation. We reviewed the `clean_match` and `elo_acc`
    functions in [section 8.2.2](kindle_split_018.html#ch08lev2sec2). These are the
    two major differences between the code in [listing 9.3](#ch09ex03)—written for
    PySpark—and the code from [listing 8.2](kindle_split_018.html#ch08ex02):'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中的大部分代码来自 [第 8 章](kindle_split_018.html#ch08)，需要进一步解释的很少。我们在 [第 8.2.2 节](kindle_split_018.html#ch08lev2sec2)中回顾了
    `clean_match` 和 `elo_acc` 函数。这些是 [列表 9.3](#ch09ex03) 中的代码（为 PySpark 编写）与 [列表 8.2](kindle_split_018.html#ch08ex02)
    中的代码之间的两个主要区别：
- en: This code does without any of the `stdin`/`stdout` and JSON we had to concern
    ourselves with using MapReduce.
  id: totrans-560
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码无需使用 MapReduce 必须关注的 `stdin`/`stdout` 和 JSON。
- en: We use Spark methods to read in the data, clean the data, and aggregate over
    the data.
  id: totrans-561
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 Spark 方法读取数据，清理数据，并对数据进行聚合。
- en: The thing that we’re probably most happy with about the PySpark version of our
    Elo rating code is that the code is entirely in Python—without any need to interact
    with the terminal through `stdout` or `stdin` and without any need to translate
    our data types into JSON. By using PySpark, we use Python data types everywhere,
    and we don’t even need to import the `JSON` module. This is a pretty big advantage
    in terms of convenience, especially if we’re dealing with more sophisticated data
    structures that may not convert neatly into JSON.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对PySpark版本的Elo评分代码最满意的事情可能是代码完全是Python编写的——无需通过`stdout`或`stdin`与终端交互，也无需将数据类型转换为JSON。通过使用PySpark，我们可以在任何地方使用Python数据类型，甚至不需要导入`JSON`模块。这在便利性方面是一个很大的优势，尤其是如果我们处理的是可能无法整洁地转换为JSON的更复杂的数据结构时。
- en: 'It’s also important that we can use Spark methods to handle all our data processing:'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是，我们可以使用Spark方法来处理所有我们的数据处理：
- en: The first method we call, `.textFile`, brings in the text data.
  id: totrans-564
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先调用的方法是`.textFile`，它用于引入文本数据。
- en: The `.textFile` method returns an `RDD`, which has a `.map` method we can use
    to clean the data with our `clean_matches` function, so we do that next.
  id: totrans-565
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`.textFile`方法返回一个`RDD`，我们可以使用它来通过`clean_matches`函数清理数据，所以我们接下来就做这件事。'
- en: Then, lastly, we use the `.aggregate` method with our `elo_acc` function and
    a new `elo_comb` function to score the players.
  id: totrans-566
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，最后，我们使用`.aggregate`方法和我们的`elo_acc`函数以及一个新的`elo_comb`函数来评分运动员。
- en: We use the `.aggregate` method in this instance because it’s the simplest `reduce`-like
    method that meets our needs. We need an empty `dict` to start with, so we can’t
    use the `.reduce` method because that has no space for an initializer. And we
    also need a different combine function than the `aggregate` function, so we can’t
    use `.fold`—`.fold` has no room for a combine function. The only `reduce`-like
    method left is `.aggregate`, which gives us the opportunity to specify all three
    pieces of our parallel `reduce`.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用`.aggregate`方法，因为它是最简单的符合我们需求的`reduce`类似方法。我们需要一个空的`dict`来开始，所以不能使用`.reduce`方法，因为那个方法没有初始化器的空间。我们还需要一个不同于`aggregate`函数的合并函数，所以不能使用`.fold`——`.fold`没有合并函数的空间。唯一剩下的`reduce`类似方法就是`.aggregate`，它给我们提供了指定我们并行`reduce`所有三个部分的机会。
- en: That covers the substantive changes to the code; however, you may have noticed
    one cosmetic change in [listing 9.3](#ch09ex03). It’s subtle, but in the middle
    of our map and reduce workflow, we insert a backslash character and move to the
    next line. This is a PySpark convention and an aspect of the Scala programming
    language adapted for Python. In Scala, you can chain methods, with each method
    on a new line by default. If we do that in Python, Python will throw an error.
    That’s because Python is famously white-space aware. We can get around this error
    by adding a backslash after our method call, as shown in the following listing.
    The backslash character is Python’s manual line wrap character.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 这涵盖了代码的实质性更改；然而，你可能已经注意到了[列表9.3](#ch09ex03)中的一个细微的变化。这是微妙的，但在我们的map和reduce工作流程中，我们在中间插入了一个反斜杠字符并移到下一行。这是一个PySpark约定，也是Scala编程语言的一个方面，它被用于Python。在Scala中，你可以链式调用方法，默认情况下每个方法都在新的一行。如果我们这样做在Python中，Python会抛出一个错误。这是因为Python以其对空白敏感而闻名。我们可以通过在方法调用后添加一个反斜杠来绕过这个错误，如下面的列表所示。反斜杠字符是Python的手动换行符。
- en: Listing 9.4\. Method chaining in Scala, and two ways in Python
  id: totrans-569
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.4\. Scala中的方法链，以及Python中的两种方式
- en: '[PRE30]'
  id: totrans-570
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If you’re working with other PySpark developers, they’re more likely than not
    going to be aware of this convention. For traditional Python developers, however,
    this convention might seem strange or even incorrect.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你与其他PySpark开发者一起工作，他们很可能已经知道这个约定。然而，对于传统的Python开发者来说，这个约定可能看起来很奇怪，甚至是不正确的。
- en: Ultimately, though, we’ll run our script and receive Elo ratings for our tennis
    players that look something like [figure 9.6](#ch09fig06). Here, we can see a
    sorted collection of players and their rankings.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最终我们运行脚本并接收网球运动员的Elo评分，看起来像[图9.6](#ch09fig06)。在这里，我们可以看到按顺序排列的运动员及其排名。
- en: Figure 9.6\. When we calculate the Elo ratings of tennis players using PySpark,
    our output will be a sequence of players and their ratings—the higher the rating,
    the better the player.
  id: totrans-573
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.6\. 当我们使用PySpark计算网球运动员的Elo评分时，我们的输出将是一系列运动员及其评分——评分越高，运动员越好。
- en: '![](09fig06_alt.jpg)'
  id: totrans-574
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig06_alt.jpg)'
- en: 9.2.2\. Introducing the PageRank algorithm
  id: totrans-575
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.2\. 介绍PageRank算法
- en: Now we can rank players based on their Elo rating with both MapReduce and Spark,
    but what if we didn’t want to use Elo ratings? What if we wanted a system that
    didn’t punish players for losing matches but only rewarded players for beating
    opponents? What if we wanted to reward players extra for beating high-quality
    opponents, encouraging top-ranked players to play competitive matches against
    one another? We can design a system like that using a variation of the PageRank
    algorithm.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用MapReduce和Spark根据他们的Elo评级对运动员进行排名，但如果我们不想使用Elo评级怎么办？如果我们想要一个不惩罚输掉比赛的运动员，但只奖励击败对手的运动员的系统怎么办？如果我们想要奖励击败高质量对手的运动员，以鼓励排名靠前的运动员之间进行竞争性比赛怎么办？我们可以使用PageRank算法的变体来设计这样的系统。
- en: PageRank is famous as the former backbone to Google’s ranking system. Websites
    that had a higher PageRank score would show up higher in the Google search results,
    and websites with low PageRank scores may not show up at all. Over time, this
    process has changed, but the algorithm’s simple and powerful assumptions have
    led to its longevity, and it’s still used outside of Google searches (including
    in a variety of capacities related to sports analytics).
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: PageRank因其曾是谷歌排名系统的基础而闻名。得分更高的网站会在谷歌搜索结果中显示得更高，而得分较低的网站可能根本不会显示。随着时间的推移，这个过程已经发生了变化，但该算法简单而强大的假设导致了其持久性，并且它仍在谷歌搜索之外（包括与体育分析相关的各种能力）使用。
- en: '|  |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**PageRank and Google search**'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '**PageRank和谷歌搜索**'
- en: The PageRank algorithm was the result of a research project by then Stanford
    PhD students Larry Page (who named the algorithm after himself) and Sergey Brin.
    The two would go on to use the algorithm as the backbone of Google’s search engine.
    Historians credit Google’s success to the ease of distributing the algorithm,
    which allowed Google to scale, and the way the algorithm naturally aligns with
    human assessments of importance.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: PageRank算法是当时斯坦福大学博士生拉里·佩奇（他用自己的名字命名了这个算法）和谢尔盖·布林进行的研究项目的成果。两人将继续使用该算法作为谷歌搜索引擎的基础。历史学家认为，谷歌的成功归功于算法的易于分发，这使得谷歌能够扩展，以及算法与人类对重要性的评估自然对齐。
- en: Over time, Google’s search engine has become more complex. The search algorithm
    now uses hundreds of features. Google still uses a version of PageRank to assess
    the reliability and authority of websites. The Google Knowledge Graph and Google’s
    preference for mobile-friendly and social-friendly content are the most evident
    forces in contemporary Google search.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，谷歌的搜索引擎变得更加复杂。现在的搜索算法使用数百个特征。谷歌仍然使用PageRank版本来评估网站的可靠性和权威性。谷歌知识图谱和谷歌对移动友好型和社会友好型内容的偏好是当代谷歌搜索中最明显的力量。
- en: '|  |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'The basic premise of PageRank was to treat website rankings like an election,
    with somewhat unique rules. The general rules are as follows:'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: PageRank的基本前提是将网站排名视为一次选举，有一些独特的规则。一般规则如下：
- en: 'Every page has a number of points: its PageRank score.'
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每一页都有一个分数：它的PageRank得分。
- en: Every page votes for the pages that it links to, distributing to each page a
    number of points equal to the linking page’s PageRank score divided by the number
    of pages it links to.
  id: totrans-585
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个页面都会对其链接的页面进行投票，将等于链接页面PageRank得分的分数分配给每个页面。
- en: Each page then receives a new PageRank score, based on the sum of all the votes
    it received.
  id: totrans-586
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，每个页面都会根据它收到的所有投票的总和获得一个新的PageRank得分。
- en: The process is repeated until the scores are “good enough.”
  id: totrans-587
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该过程会重复进行，直到分数“足够好”为止。
- en: Of course, tennis players don’t link to other tennis players. We can, however,
    use players’ losses as a vote for the players who are better than them. For example,
    if Venus Williams defeats her sister Serena at tennis, then Serena will vote for
    Venus with some of her points. A small-scale example of the PageRank algorithm
    for tennis rankings is shown in [figure 9.7](#ch09fig07).
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，网球运动员不会链接到其他网球运动员。然而，我们可以使用运动员的输球作为对表现更好的运动员的投票。例如，如果维纳斯·威廉姆斯在网球比赛中击败了她的妹妹塞雷娜，那么塞雷娜会用她的一些分数为维纳斯投票。网球排名的PageRank算法的小规模示例显示在[图9.7](#ch09fig07)中。
- en: Figure 9.7\. We can apply the PageRank algorithm to tennis players, where each
    player contributes points to the players who are better than them.
  id: totrans-589
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.7。我们可以将PageRank算法应用于网球运动员，其中每位运动员都会给比他们表现更好的运动员贡献分数。
- en: '![](09fig07_alt.jpg)'
  id: totrans-590
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig07_alt.jpg)'
- en: In the figure, we see five tennis players, each with 100 points to distribute.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，我们看到五位网球运动员，每人有100分可以分配。
- en: Player 1 lost to players 2, 3, and 5 (3 total).
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩家1输给了玩家2、3和5（总共3人）。
- en: Player 2 lost to players 1 and 4 (2 total).
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩家2输给了玩家1和4（总共2场）。
- en: Player 3 lost to players 1 and 2 (2 total).
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩家3输给了玩家1和2（总共2场）。
- en: Player 4 lost to players 2 and 3 (2 total).
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩家4输给了玩家2和3（总共2场）。
- en: Player 5 lost to everyone (4 total).
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩家5输给了所有人（总共4场）。
- en: Because three players have two losses, it’s hard to tell immediately who’s the
    best, but PageRank will help us figure it out. Player 1 distributes 33 (1/3) of
    their 100 points to players 2, 3, and 5\. Players 2, 3, and 4 each vote with 50
    points to the players to whom they lost. And player 5 votes with 25 points to
    each other player.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有三个球员有两场败绩，很难立即判断谁是最佳球员，但PageRank将帮助我们解决这个问题。玩家1将他们100分的1/3（即33分）分配给玩家2、3和5。玩家2、3和4各自用50分投票给输掉的球员。而玩家5用25分投票给其他所有球员。
- en: 'Next, we would add up all the votes. Player 2 ends up with the most points
    at 158 (50+50+25+33), followed by player 1 with 125 (50+50+25), followed by player
    3 at 108 (50+33+25), player 4 at 75 (50+25), and player 5 at 33, from their lone
    victory over player 1\. In a more robust example, we would then repeat this process
    a few times so that victories over higher rated players would be worth more. For
    example, players 1 and 4 should get lots of points for their victory over player
    2—who is the best player—whereas beating player 5, who lost to everyone, should
    adjust the ratings much less. After three iterations, the players would be rated
    as follows:'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把所有投票加起来。玩家2最终获得158分（50+50+25+33），其次是玩家1，获得125分（50+50+25），然后是玩家3，获得108分（50+33+25），玩家4获得75分（50+25），玩家5获得33分，这是他们唯一一次战胜玩家1。在一个更稳健的例子中，我们将会重复这个过程几次，这样战胜高排名的玩家就会更有价值。例如，玩家1和4应该因为战胜最佳玩家玩家2而获得很多分数，而战胜输给所有人的玩家5，应该对排名的影响较小。经过三次迭代，球员的排名如下：
- en: Player 2—145 points
  id: totrans-599
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 玩家2——145分
- en: Player 1—125 points
  id: totrans-600
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 玩家1——125分
- en: Player 3—101 points
  id: totrans-601
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 玩家3——101分
- en: Player 4—81 points
  id: totrans-602
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 玩家4——81分
- en: Player 5—47 points
  id: totrans-603
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 玩家5——47分
- en: One of the largest advantages of the PageRank algorithm, which we’ll see in
    the next section, is that it’s naturally parallelizable. We can do all the point
    giving and point summing in parallel with our most strict assumptions about parallelization.
    This is one of the reasons why it worked so well for Google—they were able to
    parallelize their problem and scale it to the massive dataset they were working
    with. In the next section, we’ll implement a parallel PageRank algorithm with
    PySpark.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: PageRank算法最大的优点之一，我们将在下一节中看到，是它自然地可并行化。我们可以在最严格的并行化假设下并行地进行所有点数分配和点数求和。这也是它对谷歌工作效果如此之好的原因之一——他们能够并行化他们的问题并将其扩展到他们正在处理的巨大数据集。在下一节中，我们将使用PySpark实现一个并行PageRank算法。
- en: 9.2.3\. Ranking tennis players with PageRank
  id: totrans-605
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.3\. 使用PageRank对网球运动员进行排名
- en: 'Now that we have an idea of how PageRank works, how should we go about implementing
    it in PySpark? Well, we know our implementation will need to have five steps:'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了PageRank的工作原理，那么我们应该如何使用PySpark来实现它呢？嗯，我们知道我们的实现将需要五个步骤：
- en: Read in the data.
  id: totrans-607
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取数据。
- en: Structure the data in the right way.
  id: totrans-608
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以正确的方式结构化数据。
- en: Do an initial point allocation.
  id: totrans-609
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行初始点数分配。
- en: Do several rounds of point allocation until we’re satisfied with the results.
  id: totrans-610
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行几轮点数分配，直到我们对结果满意为止。
- en: Return some ratings.
  id: totrans-611
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回一些评分。
- en: '[Figure 9.8](#ch09fig08) illustrates the first four steps.'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9.8](#ch09fig08) 展示了前四个步骤。'
- en: Figure 9.8\. Using the PageRank algorithm for rating tennis players in PySpark
    requires both custom Python functions and parallel PySpark methods.
  id: totrans-613
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.8\. 在PySpark中使用PageRank算法对网球运动员进行评分需要自定义Python函数和并行PySpark方法。
- en: '![](09fig08_alt.jpg)'
  id: totrans-614
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig08_alt.jpg)'
- en: 'To read in the data, we’ll use the same method we’ve been using so far in this
    book: the `.textFiles` method from `SparkContext`. This method returns an `RDD`,
    which is the Spark class that has all the nice parallel map and reduce options
    we’ll want to use to build our programs.'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 为了读取数据，我们将使用本书中迄今为止一直在使用的方法：`SparkContext`的`.textFiles`方法。此方法返回一个`RDD`，这是Spark类，它具有我们构建程序时想要使用的所有优秀的并行映射和归约选项。
- en: 'Next, we’ll move on to structuring the data. For that, we’ll use the `RDD`’s
    `.map` method to retrieve the winners and losers of each match, and we’ll use
    the `RDD`’s `.groupByKey` method to get a list of defeats for each player. To
    ensure that `.groupByKey` does what we want, we’ll return winners and losers as
    a `tuple` in the form of: `(<loser>, <winner>)`. From there, we’ll use another
    `.map` statement to add some metadata that will be helpful when calculating the
    PageRank scores.'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将转向数据结构化。为此，我们将使用 `RDD` 的 `.map` 方法检索每场比赛的赢家和输家，并使用 `RDD` 的 `.groupByKey`
    方法获取每个玩家的失败列表。为了确保 `.groupByKey` 做我们想要的事情，我们将赢家和输家以 `tuple` 的形式返回：`(<输家>, <赢家>)`。从那里，我们将使用另一个
    `.map` 语句添加一些在计算 PageRank 分数时将很有帮助的元数据。
- en: With the data in the right format, we’ll reduce over our data several times.
    Each time, we’ll calculate the PageRank scores for each player, based on who defeated
    who, by looping through the losing players and giving a fraction of their score
    to each player who defeated them. Every new round, we’ll use the latest score
    of the player.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据格式正确后，我们将多次遍历我们的数据。每次，我们将根据谁打败了谁来计算每个玩家的 PageRank 分数，通过遍历输家并给打败他们的每个玩家分配他们分数的一部分。在每一轮中，我们将使用玩家的最新分数。
- en: 'Lastly, after a few rounds, we can sort our players, return their scores, and
    call it a day. All in all, the solution we’ll draw up for this problem will be
    a pretty large program. You can find the full script in the code repository for
    this book ([https://github.com/jtwool/mastering-large-datasets](https://github.com/jtwool/mastering-large-datasets)).
    Here, I think it’s worth focusing our attention on three major areas:'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，经过几轮之后，我们可以对玩家进行排序，返回他们的分数，然后结束。总的来说，我们为这个问题制定的解决方案将是一个非常庞大的程序。您可以在本书的代码仓库中找到完整的脚本（[https://github.com/jtwool/mastering-large-datasets](https://github.com/jtwool/mastering-large-datasets)）。在这里，我认为值得我们关注三个主要领域：
- en: The data preparation process with `.groupByKey` and `.mapValues`
  id: totrans-619
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.groupByKey` 和 `.mapValues` 的数据准备过程
- en: The allocate points aggregator function and the `combine_scores` combiner
  id: totrans-620
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分配分数聚合函数和 `combine_scores` 组合函数
- en: The iterative score calculations and the partial application of the allocate
    points function
  id: totrans-621
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代分数计算和分配分数函数的局部应用
- en: Preparing the tennis match data with .groupByKey and .mapValues
  id: totrans-622
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用 .groupByKey 和 .mapValues 准备网球比赛数据
- en: 'The first section, data preparation, revolves around this bit of code:'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分，数据准备，围绕以下代码展开：
- en: '[PRE31]'
  id: totrans-624
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In this section, we’ve already read the data into a variable called `match_data`,
    so we’re working with an `RDD` of strings. We know that what we want to have is
    an `RDD` of keys (player names) with `dict`s as their values. Each of those `dict`s
    must have the information we need to calculate PageRank scores later on. To that
    end, they’ll need the players the player lost to, the number of those players,
    and the player’s current page rank score.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已将数据读入一个名为 `match_data` 的变量中，因此我们正在处理一个字符串的 `RDD`。我们知道我们想要的是一个键（玩家名称）的
    `RDD`，其值是 `dict`。这些 `dict` 必须包含我们稍后计算 PageRank 分数所需的信息。为此，它们需要包含玩家输掉的对手、这些对手的数量以及玩家的当前页面排名分数。
- en: 'To get from a match string to this value will be a three-step process:'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 从比赛字符串到这个值的过程将是一个三步过程：
- en: We’ll map the match data into `tuple`s of losers and winners.
  id: totrans-627
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将比赛数据映射成输家和赢家的元组。
- en: We’ll group the matches by the losing player.
  id: totrans-628
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将比赛按输家分组。
- en: We’ll map a transformation across the keys and values to prepare our data for
    PageRank.
  id: totrans-629
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将映射一个转换到键和值上，以准备我们的数据用于 PageRank。
- en: Altogether, this process will look like [figure 9.9](#ch09fig09).
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程看起来就像 [图 9.9](#ch09fig09) 所示。
- en: Figure 9.9\. We prepare tennis match data for PageRank in PySpark with `.map`,
    `.groupByKey`, and `.mapValues`.
  id: totrans-631
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.9。我们使用 `.map`、`.groupByKey` 和 `.mapValues` 在 PySpark 中为 PageRank 准备网球比赛数据。
- en: '![](09fig09_alt.jpg)'
  id: totrans-632
  prefs: []
  type: TYPE_IMG
  zh: '![](09fig09_alt.jpg)'
- en: As we can see in [figure 9.9](#ch09fig09), our first map step involves taking
    a subset of the match data and arranging it into `tuple`s. This will return an
    `RDD` of `tuple`s, which we can use `.groupByKey` on to return an `RDD` of keys
    and values. The keys in these instances represent the losing players, whereas
    the values are a sequence of players to whom the losing player lost. Lastly, we
    can use the `initialize_for_voting` function to add metadata and convert the list
    into a `dict` for a clearer workflow down the road.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图9.9](#ch09fig09)所示，我们的第一个映射步骤涉及从匹配数据中提取一个子集并将其排列成`tuple`。这将返回一个`tuple`的`RDD`，我们可以使用`.groupByKey`来返回一个键值对的`RDD`。这些实例中的键代表输掉比赛的玩家，而值是输掉比赛的玩家输掉的一系列玩家。最后，我们可以使用`initialize_for_voting`函数添加元数据并将列表转换为`dict`，以便在未来的工作中有更清晰的流程。
- en: Allocating points and combining the scores
  id: totrans-634
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分配分数和组合分数
- en: The next two parts of the process we’ll want to pay extra attention to are the
    aggregation and combination functions. These are the functions we call during
    the `reduce` step that constitute the heavy lifting of our program. These functions
    are how we implement PageRank, and we can see them in [figure 9.10](#ch09fig10).
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在处理过程中需要特别注意的下一部分是聚合和组合函数。这些是我们调用`reduce`步骤时的函数，构成了我们程序的重头戏。这些函数是我们实现PageRank的方式，我们可以在[图9.10](#ch09fig10)中看到它们。
- en: Figure 9.10\. We can parallelize the ranking step of PageRank into a two-step
    parallel reduce workflow.
  id: totrans-636
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '[图9.10](#ch09fig10)。我们可以将PageRank的排名步骤并行化为两步并行减少工作流程。'
- en: '![](09fig10_alt.jpg)'
  id: totrans-637
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig10_alt.jpg)'
- en: Our aggregation function—`allocate_points—`is responsible for taking in a new
    player, their losses, and associated metadata, and assigning points to the players
    who defeated them. The points are then stored in a `dict`, with players’ names
    as keys and players’ PageRank scores as values. We can see this process in [figure
    9.11](#ch09fig11).
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的聚合函数`allocate_points`负责接收一个新玩家、他们的输掉的比赛和相关的元数据，并将分数分配给击败他们的玩家。然后，这些分数存储在一个`dict`中，以玩家的名字作为键，玩家的PageRank分数作为值。我们可以在[图9.11](#ch09fig11)中看到这个过程。
- en: Figure 9.11\. The `allocate_points` function takes in players’ information and
    updates the accumulation variable to reflect the players’ updated scores.
  id: totrans-639
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '[图9.11](#ch09fig11)。`allocate_points`函数接收玩家信息并更新累积变量以反映玩家的更新分数。'
- en: '![](09fig11_alt.jpg)'
  id: totrans-640
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig11_alt.jpg)'
- en: Taking a look at the code for the `allocate_points` function, we can see precisely
    how this works. We split the player into a key and value because we had the player
    data stored as a two-`tuple` coming out of our `.mapValues` step from the previous
    subsection.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 查看`allocate_points`函数的代码，我们可以精确地看到它是如何工作的。我们将玩家分为键和值，因为我们之前小节中的`.mapValues`步骤将玩家数据存储为两个`tuple`。
- en: '[PRE32]'
  id: totrans-642
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Next, we calculate the boost that each player who defeated the current player
    will receive. Each player allocates their entire rating uniformly to all those
    who defeated them. This means that the amount of the boost a player receives by
    beating our current player is equal to that player’s rating divided by their number
    of losses. To prevent a divide by zero error, I add a small value to the number
    of losses a player has, in the event they’re undefeated.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算击败当前玩家的每个玩家将获得的提升。每个玩家将他们的整个评分均匀地分配给所有击败他们的玩家。这意味着一个玩家通过击败我们的当前玩家获得的提升量等于该玩家的评分除以他们的输掉的比赛数。为了防止除以零的错误，我在玩家输掉的比赛数中添加了一个小值，以防他们没有输过比赛。
- en: Then, we allocate those points to each player who has defeated our current player—updating
    the accumulation variable. We do this by setting the opposing player’s rating
    equal to their current rating plus the boost factor. After this, we return the
    accumulator and move on to the next player.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将这些分数分配给击败当前玩家的每个玩家——更新累积变量。我们通过将对手玩家的评分设置为他们的当前评分加上提升因子来实现这一点。之后，我们返回累积变量并继续下一个玩家。
- en: 'This takes care of the accumulation step of our parallel `reduce`. As we know
    from [chapter 6](kindle_split_015.html#ch06), though, parallel `reduce` has two
    parts: the parallel accumulation and the combination. In our combination step,
    we have to join together all the values we accumulated in parallel. Typically,
    this is the challenging part of parallel `reduce` because we’ll concoct complex
    data structures—no such problems here.'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 这处理了我们的并行`reduce`的累积步骤。然而，正如我们在[第6章](kindle_split_015.html#ch06)中所知，并行`reduce`有两个部分：并行累积和组合。在我们的组合步骤中，我们必须将我们在并行中累积的所有值连接起来。通常，这是并行`reduce`的难点，因为我们将构建复杂的数据结构——这里没有这样的问题。
- en: Coming out of our `reduce` step, we’ll want to join `dict`s with keys as strings
    and values that are integers, such that the resulting `dict` has all the keys
    of both `dict`s and the values are the sums of the values. We can see this process
    in [figure 9.12](#ch09fig12).
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 `reduce` 步骤之后，我们希望将 `dict`s 通过键作为字符串和整数值连接起来，这样生成的 `dict` 将包含两个 `dict`
    的所有键，而值是值的总和。我们可以在[图 9.12](#ch09fig12)中看到这个过程。
- en: Figure 9.12\. Combining the players’ PageRank ratings together requires joining
    `dict`s into a single `dict`.
  id: totrans-647
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.12\. 将球员的 PageRank 评分合并需要将 `dict`s 合并成一个单一的 `dict`。
- en: '![](09fig12_alt.jpg)'
  id: totrans-648
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.12](09fig12_alt.jpg)'
- en: 'In Python, we’ll implement this process by looping through all the elements
    of one `dict` and attempting to add the values to the current value for that key
    in the other `dict`. As we do that, we’ll update the `dict` that we aren’t looping
    through. If we don’t find a key from the `dict` we are looping through in the
    other, we’ll update the other so that key is equal to the value from our looping
    `dict`. Finally, we’ll return the `dict` we didn’t loop through, since that’s
    the `dict` we’ve been updating. Here’s how that looks:'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，我们将通过遍历一个 `dict` 的所有元素，并尝试将值添加到另一个 `dict` 中对应键的当前值来实现这个过程。在这个过程中，我们将更新我们未遍历的
    `dict`。如果我们未遍历的 `dict` 中没有找到来自遍历 `dict` 的键，我们将更新另一个 `dict`，使其键等于遍历 `dict` 的值。最后，我们将返回未遍历的
    `dict`，因为这是我们一直在更新的 `dict`。以下是这个过程的样子：
- en: '[PRE33]'
  id: totrans-650
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Together, these two steps represent a single round of PageRank. One of the beauties
    of the PageRank algorithm is that we can do the entire process in parallel. We
    can take advantage of this fact if we need to rank large amounts of information
    quickly. By increasing our compute capacity, we can decrease the time we spend
    ranking.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个步骤共同代表了一轮 PageRank。PageRank 算法的一个优点是我们可以在并行中完成整个过程。如果我们需要快速对大量信息进行排名，我们可以利用这个事实。通过增加我们的计算能力，我们可以减少排名所需的时间。
- en: Iteratively calculating scores
  id: totrans-652
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 迭代计算分数
- en: The last step we’ll want to pay extra attention to is the way we iteratively
    calculate these scores. In the first round of a PageRank process, each of the
    *pages*—in our case, tennis players—are rated evenly. I decided to start everyone
    with 100 points, but any number of points will do. Having a uniform number of
    points, however, doesn’t reflect reality. Some web pages are more important than
    others, and some tennis players are better than others. A link from the *New York
    Times* web page will mean more traffic than a link from a high school newspaper’s
    web page, and a victory over Serena Williams is more notable than a victory over
    a career journeywoman.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要特别注意的最后一步是我们迭代计算这些分数的方式。在 PageRank 过程的第一轮中，每个页面——在我们的例子中，是网球运动员——都被平等地评分。我决定从每个人开始时给
    100 分，但任何数量的分数都可以。然而，拥有统一数量的分数并不能反映现实。一些网页比其他网页更重要，一些网球运动员比其他运动员更优秀。来自《纽约时报》网页的链接将比来自高中报纸网页的链接带来更多的流量，而击败塞雷娜·威廉姆斯比击败职业老将更有意义。
- en: To resolve this problem, we run the PageRank process several times. Each time
    we do the same thing, but we’ll use the scores from the previous round to inform
    our ratings. This way, wins over Serena or links from the *New York Times* become
    more important in each subsequent round.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们多次运行 PageRank 过程。每次我们都做同样的事情，但我们将使用上一轮的分数来告知我们的评分。这样，击败塞雷娜或来自《纽约时报》的链接在每一轮后续中都会变得更加重要。
- en: 'To do this, we’ll insert our `reduce` step inside a `for` loop and bookend
    it with some code to set up the next round of the reduction:'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们将 `reduce` 步骤插入到 `for` 循环中，并用一些代码来设置下一轮的减少：
- en: '[PRE34]'
  id: totrans-656
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Before we start our `reduce` step, we need to set up our accumulation variable:
    `acc`. This is the variable that holds all the players and their updated ratings.
    To get this variable, we’ll empty the ratings of all the keys from our `dict`
    of `dicts`.. This will give each player a fresh new rating of 0 at the beginning
    of each PageRank step. From there, we can `reduce`.'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始 `reduce` 步骤之前，我们需要设置我们的累积变量：`acc`。这是包含所有球员及其更新评分的变量。为了获取这个变量，我们将清空来自我们的
    `dict` 的 `dict` 中所有键的评分。这将给每个球员在每个 PageRank 步骤开始时一个全新的评分为 0。从那里，我们可以进行 `reduce`。
- en: Then, after each `reduce` step beyond the first, we’ll create a new sequence
    of players to `reduce` over. This sequence will have all the metadata from our
    initialization, plus the new ratings that we can use in the next PageRank iteration.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在每个 `reduce` 步骤之后（除了第一个之外），我们将创建一个新的球员序列来 `reduce`。这个序列将包含我们初始化的所有元数据，以及我们可以在下一轮
    PageRank 迭代中使用的新的评分。
- en: 'Importantly, though, our `reduce` process—which we call using the `RDD .aggregate`
    method—returns a `dict`. We need an `RDD` so that we can take advantage of Spark’s
    parallelization. To get an `RDD`, we’ll need to explicitly convert the items of
    that `dict` into an `RDD` using the `.parallelize` method from our SparkContext:
    `sc`.'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，我们的`reduce`过程——我们通过`RDD .aggregate`方法来调用它——返回一个`dict`。我们需要一个`RDD`，这样我们就可以利用Spark的并行化。为了得到一个`RDD`，我们需要使用SparkContext中的`.parallelize`方法显式地将该`dict`的项转换为`RDD`：`sc`。
- en: Once our iteration is complete, we’ll have a `dict` with the players as keys
    and their scores as values. When you run this script, remember to run it with
    the `spark-submit` utility to take advantage of Spark’s parallelization. You can
    run it with your local Python runtime as well, but it won’t take advantage of
    the full power of Spark. We can see the script’s output in [figure 9.13](#ch09fig13).
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的迭代完成，我们将有一个以玩家为键、以分数为值的`dict`。当你运行此脚本时，请记住使用`spark-submit`实用程序来利用Spark的并行化。你也可以使用你的本地Python运行时运行它，但它不会充分利用Spark的全部功能。我们可以在[图9.13](#ch09fig13)中看到脚本的输出。
- en: Figure 9.13\. The output of our PageRank process shows the top players and their
    PageRank scores.
  id: totrans-661
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.13\. 我们PageRank过程的输出显示了顶级玩家及其PageRank分数。
- en: '![](09fig13_alt.jpg)'
  id: totrans-662
  prefs: []
  type: TYPE_IMG
  zh: '![图片](09fig13_alt.jpg)'
- en: Note that in addition to the PageRank scores, we also include the log of the
    players’ PageRank scores. Taking the log of each player’s scores groups players
    whose scores are similar. When Google released the PageRank toolbar, they revealed
    a log-scaled version of their PageRank scores instead of the PageRank scores themselves.
    The log-scaled scores may be better representations of PageRank scores, as the
    difference between 4100 and 3990 is quite small.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，除了PageRank分数外，我们还包含了玩家PageRank分数的日志。对每个玩家的分数取对数可以将分数相似的玩家分组。当Google发布PageRank工具栏时，他们揭示了PageRank分数的对数刻度版本，而不是PageRank分数本身。对数刻度分数可能更好地代表了PageRank分数，因为4100和3990之间的差异相当小。
- en: 9.3\. Exercises
  id: totrans-664
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3\. 练习
- en: 9.3.1\. sumByKey
  id: totrans-665
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.1\. sumByKey
- en: A common situation in which you’ll find yourself in Spark will be having an
    `RDD` of keys and values in two-`tuple`s. A common operation on those keys and
    values will be summing all the values by key. This operation can be called `sumByKey`.
    Use the right `reduce`-like method of the `RDD` to sum the values in an `RDD`
    by key.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，你可能会遇到的一个常见情况是拥有包含两个元组的键和值的`RDD`。对这些键和值的一个常见操作是按键求所有值的和。这个操作可以称为`sumByKey`。使用`RDD`的正确的`reduce`-like方法按键对`RDD`中的值求和。
- en: '[PRE35]'
  id: totrans-667
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 9.3.2\. sumByKey with toolz
  id: totrans-668
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.2\. sumByKey with toolz
- en: The toolz library has a `reduceBy` function that takes a key function, an operation,
    and a sequence to achieve the same effect as the Spark `reduceByKey`. Implement
    `sumByKey` using the toolz `reduceBy` function for use in non-Spark workflows.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: toolz库有一个`reduceBy`函数，它接受一个键函数、一个操作和一个序列，以实现与Spark的`reduceByKey`相同的效果。使用toolz的`reduceBy`函数实现`sumByKey`，以便在非Spark工作流程中使用。
- en: 9.3.3\. Spark and toolz
  id: totrans-670
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.3\. Spark和toolz
- en: 'One of the great things about Spark is that it has many of the same convenience
    methods that we’ve already learned to love from the toolz library. In Scala, replicate
    the following transaction written using toolz. Bonus: Use Spark-style method chaining
    for added readability.'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的其中一个优点是它拥有许多我们从toolz库中已经学会并喜爱的便捷方法。在Scala中，复制以下使用toolz编写的交易。加分项：使用Spark风格的链式方法以增加可读性。
- en: '[PRE36]'
  id: totrans-672
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 9.3.4\. Wikipedia PageRank
  id: totrans-673
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.4\. Wikipedia PageRank
- en: PageRank works for ranking tennis players, but it was designed to rank web pages
    in a network. Modify the code we wrote in this chapter to perform a PageRank of
    the pages from the Wikipedia network we collected in [chapter 2](kindle_split_011.html#ch02).
    A dataset for this exercise is provided for your convenience in the code repository
    for this book ([https://github.com/jtwool/mastering-large-datasets](https://github.com/jtwool/mastering-large-datasets)).
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: PageRank可以用于排名网球运动员，但它最初是为了在网络上排名网页而设计的。修改本章中我们编写的代码，以对我们在[第2章](kindle_split_011.html#ch02)中收集的Wikipedia网络页面进行PageRank。为此练习提供了一个数据集，方便你在本书的代码库中找到（[https://github.com/jtwool/mastering-large-datasets](https://github.com/jtwool/mastering-large-datasets)）。
- en: Summary
  id: totrans-675
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概述
- en: 'The `RDD` class has three different `reduce`-like methods: .`reduce`, for operations
    where the data is the same all the way through, `.fold`, for when we want to specify
    an initializer value, and `.aggregate`, for when we want an initializer and a
    custom combiner function.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RDD`类有三个不同的`reduce`-like方法：`.reduce`，用于数据始终相同的情况，`.fold`，当我们想要指定一个初始化值时使用，`.aggregate`，当我们想要一个初始化值和自定义组合函数时使用。'
- en: The `RDD` class’s `.saveAsTextFile` method is an excellent way to persist an
    `RDD` on-disk for long-term storage or for sharing with others—we can even use
    it to save our data in a compressed format!
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RDD`类的`.saveAsTextFile`方法是将`RDD`持久化到磁盘以进行长期存储或与他人共享的绝佳方式——我们甚至可以使用它以压缩格式保存我们的数据！'
- en: To take advantage of Spark’s parallelization, we need to ensure that our data
    is in the `RDD` class. We can turn data into an `RDD` with the `SparkContext`
    class’s `.parallelize` method.
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了利用Spark的并行化，我们需要确保我们的数据在`RDD`类中。我们可以使用`SparkContext`类的`.parallelize`方法将数据转换为`RDD`。
- en: Spark programs often use \ characters in their method chaining to increase their
    readability.
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark程序通常在方法链中使用反斜杠字符来提高其可读性。
- en: Using the `byKey` variations of methods in PySpark often results in significant
    speed-ups because like data is worked on by the same distributed compute worker.
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PySpark中方法的`byKey`变体通常会导致显著的加速，因为相同的数据是由相同的分布式计算工作器处理的。
- en: Chapter 10\. Faster decision-making with machine learning and PySpark
  id: totrans-681
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第10章\. 使用机器学习和PySpark进行更快的决策
- en: '*This chapter covers*'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: An introduction to machine learning
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习简介
- en: Training and applying decision tree classifiers in parallel with PySpark
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PySpark并行训练和应用决策树分类器
- en: Matching problems and appropriate machine learning algorithms
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 匹配问题和适当的机器学习算法
- en: Training and applying random forest regressors with PySpark
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PySpark训练和应用随机森林回归器
- en: '[Chapter 9](kindle_split_019.html#ch09) showed how we can write Python and
    take advantage of Spark, one of the most popular distributed computing frameworks.
    We saw some of Spark’s raw data transformation options, and we used Spark in the
    map and reduce style we’ve been exploring throughout the book. However, one of
    the reasons why Spark is so popular is its built-in machine learning capabilities.'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: '[第9章](kindle_split_019.html#ch09)展示了我们如何编写Python代码并利用Spark，这是最受欢迎的分布式计算框架之一。我们看到了一些Spark的原始数据转换选项，并且我们使用了Spark在本书中一直在探索的map和reduce风格。然而，Spark之所以如此受欢迎，其中一个原因就是其内置的机器学习功能。'
- en: Machine learning refers to the design, training, application, and study of judgmental
    algorithms that adjust themselves based on input data. A familiar example of machine
    learning is the spam filter. Spam filter designers feed spam into their spam filter
    algorithms, which either are or contain machine learning algorithms. Then the
    spam filter algorithm learns to make judgments about whether or not an email is
    spam ([figure 10.1](#ch10fig01)).
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是指设计、训练、应用和研究那些根据输入数据自行调整的判断算法。机器学习的一个熟悉例子是垃圾邮件过滤器。垃圾邮件过滤器的设计者将垃圾邮件输入到他们的垃圾邮件过滤器算法中，这些算法要么本身就是机器学习算法，要么包含机器学习算法。然后垃圾邮件过滤器算法学会判断一封电子邮件是否为垃圾邮件（[图10.1](#ch10fig01)）。
- en: Figure 10.1\. Spam filters are machine learning algorithms that learn how to
    judge emails as spam or not by looking at lots of spam emails and nonspam emails.
  id: totrans-689
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.1\. 垃圾邮件过滤器是机器学习算法，通过查看大量的垃圾邮件和非垃圾邮件来学习如何判断电子邮件是否为垃圾邮件。
- en: '![](10fig01_alt.jpg)'
  id: totrans-690
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig01_alt.jpg)'
- en: 'In this chapter, we’ll look at how to use PySpark for machine learning. First,
    we’ll explore what machine learning is in greater depth. Then we’ll build two
    machine learners in PySpark:'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何使用PySpark进行机器学习。首先，我们将更深入地探讨什么是机器学习。然后，我们将在PySpark中构建两个机器学习器：
- en: One that uses PySpark’s decision tree classifier—a classifier that makes judgements
    by following learned yes/no rules
  id: totrans-692
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个使用PySpark的决策树分类器——一个通过遵循学习到的是/否规则进行判断的分类器
- en: One that uses the random forest classifier—a classifier that has multiple decision
    trees vote on an outcome
  id: totrans-693
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其中一个是使用随机森林分类器——一个通过多个决策树对结果进行投票的分类器
- en: 10.1\. What is machine learning?
  id: totrans-694
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1\. 什么是机器学习？
- en: 'Before we look at implementing machine learning algorithms in the later sections
    of this chapter, it makes sense to delve deeper into what machine learning is.
    I’ve offered a definition of machine learning:'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看本章后面部分实现机器学习算法之前，深入探讨什么是机器学习是有意义的。我提出了机器学习的定义：
- en: '|  |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Definition
  id: totrans-697
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 定义
- en: Machine learning refers to the design, training, application, and study of judgmental
    algorithms that adjust themselves based on input data.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是指设计、训练、应用和研究基于输入数据自我调整的判断算法。
- en: '|  |'
  id: totrans-699
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: In this section, we’ll examine that definition in greater depth and take a look
    at some machine learning applications with which you may already be familiar.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将更深入地探讨这个定义，并查看一些你可能已经熟悉的机器学习应用。
- en: 10.1.1\. Machine learning as self-adjusting judgmental algorithms
  id: totrans-701
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.1.1\. 将机器学习视为自我调整的判断算法
- en: 'Let’s examine a few examples to better understand our definition, which has
    four core components ([figure 10.2](#ch10fig02)):'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过几个例子来更好地理解我们的定义，它包含四个核心组件([图10.2](#ch10fig02))：
- en: There must be an algorithm involved.
  id: totrans-703
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 必须涉及算法。
- en: That algorithm must make judgments.
  id: totrans-704
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 那个算法必须做出判断。
- en: The algorithm must adjust itself.
  id: totrans-705
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法必须自我调整。
- en: That adjustment must take place based on data.
  id: totrans-706
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这种调整必须基于数据进行。
- en: 'Figure 10.2\. Machine learning has four components: algorithms, judging, self-adjusting,
    and data.'
  id: totrans-707
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.2\. 机器学习有四个组成部分：算法、判断、自我调整和数据。
- en: '![](10fig02_alt.jpg)'
  id: totrans-708
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig02_alt.jpg)'
- en: 'The first of these components insists that all machine learning must involve
    at least one algorithm: a sequence of computations that we can use to solve a
    problem. This is good because it means that any type of machine learning we’ll
    want to do can be solved using computers.'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组成部分中的第一个坚持认为所有机器学习都必须至少涉及一个算法：一系列我们可以用来解决问题的计算。这是好事，因为它意味着我们将想要进行的任何类型的机器学习都可以使用计算机来解决。
- en: Second, I consider only algorithms that make judgments to be machine learning
    algorithms. That means that algorithms that describe data, such as summation,
    or algorithms that simply transform data, such as a doubling algorithm, are not
    machine learning. However, that doesn’t mean the judgments have to be important,
    true, or difficult. Silly, wrong, and simple judgments count too.
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我只考虑做出判断的算法是机器学习算法。这意味着描述数据，如求和，或简单转换数据的算法，如加倍算法，不是机器学习。然而，这并不意味着判断必须是重要的、真实的或困难的。愚蠢、错误和简单的判断也计入其中。
- en: '|  |'
  id: totrans-711
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**More on machine learning from Manning Publications**'
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: '**更多关于机器学习的Manning Publications内容**'
- en: Machine learning is a complex and rapidly evolving topic. Though we don’t need
    to go into mathematical proofs to understand the big picture of machine learning,
    I suspect many readers of this book will be interested in the finer details. Manning
    has some excellent and accessible books and other resources geared toward the
    topic of machine learning. I’d recommend three in particular.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一个复杂且快速发展的主题。虽然我们不需要深入数学证明来理解机器学习的大图景，但我怀疑这本书的许多读者会对更详细的细节感兴趣。Manning有一些针对机器学习主题的优秀且易于获取的书籍和其他资源。我特别推荐三本。
- en: First, for someone looking to get an overview of machine learning, is *Grokking
    Machine Learning*, by Luis G. Serrano (2020). This book teaches machine learning
    with an emphasis on conceptual understanding instead of mathematical proofs. It’s
    a great entry point into the material. [Chapter 5](kindle_split_014.html#ch05)
    covers decision trees.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，对于想要了解机器学习概述的人来说，是路易斯·G·塞拉诺（2020）所著的《Grokking Machine Learning》。这本书通过强调概念理解而不是数学证明来教授机器学习。它是一个进入该材料的绝佳起点。[第5章](kindle_split_014.html#ch05)涵盖了决策树。
- en: '*Machine Learning in Action*, by Peter Harrington (2012), has an entire chapter—[chapter
    3](kindle_split_012.html#ch03)—dedicated to decision trees in Python. This chapter
    would be a good starting point for anyone interested in more detail on decision
    trees than I go into here. The rest of the book is solid as well.'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 《机器学习实战》，由彼得·哈灵顿（2012）所著，有一整章——[第3章](kindle_split_012.html#ch03)——专门介绍Python中的决策树。对于想要了解比这里更详细决策树的人来说，这一章是一个很好的起点。这本书的其他部分也很扎实。
- en: '*AWS Machine Learning in Motion*, a Manning LiveVideo by Kesha Williams, covers
    implementing machine learning on AWS. That course expands on the overlap between
    concepts introduced in this chapter, as well as the next two chapters on cloud
    computing.'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: '《AWS机器学习实战》，由凯莎·威廉姆斯（Kesha Williams）所著的Manning LiveVideo，涵盖了在AWS上实施机器学习。该课程扩展了本章以及接下来两章关于云计算中引入的概念的重叠。 '
- en: '|  |'
  id: totrans-717
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Third, machine learning algorithms must be self-adjusting. This is what makes
    them *machine learning* algorithms instead of just *machine judging* algorithms.
    The algorithms must define rules for them to get better at judging. Consider the
    Elo rating example from [chapters 8](kindle_split_018.html#ch08) and [9](kindle_split_019.html#ch09)
    ([figure 10.3](#ch10fig03)): we defined some rules, then the algorithm applied
    those rules to judge who the best players were and make judgments about how likely
    they were to beat one another. We didn’t tell the algorithm anything about the
    players, it learned all that itself.'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，机器学习算法必须是自我调整的。这就是它们被称为*机器学习*算法而不是仅仅*机器判断*算法的原因。算法必须定义规则，以便它们在判断方面变得更好。以第8章和第9章的Elo评级示例（[图10.3](#ch10fig03)）为例：我们定义了一些规则，然后算法应用这些规则来判断谁是最佳选手，并判断他们相互击败的可能性。我们没有告诉算法任何关于选手的信息，它自己学习了所有这些。
- en: 'Figure 10.3\. We can consider calculating Elo ratings to be machine learning:
    the rating rules define a learning process, and the algorithm can use the output
    ratings to judge future match win likelihoods.'
  id: totrans-719
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.3\. 我们可以将计算Elo评级视为机器学习：评级规则定义了一个学习过程，算法可以使用输出评级来判断未来比赛获胜的可能性。
- en: '![](10fig03_alt.jpg)'
  id: totrans-720
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig03_alt.jpg)'
- en: Fourth and finally, the algorithm must adjust itself based on data. Again, looking
    at our Elo rating example from [chapters 8](kindle_split_018.html#ch08) and [9](kindle_split_019.html#ch09),
    the match data was necessary to obtain ratings for the players. We didn’t go in
    and encode player ratings based on how we felt about the players personally. This
    last component gives machine learning algorithms their mystique. Business, science,
    and government are all interested in the hidden insights that algorithms can find
    that humans would typically overlook. If these algorithms learn differently than
    humans, the theory goes, perhaps they’re capable of learning better than humans.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 第四点，算法必须根据数据自行调整。再次以我们从第[8章](kindle_split_018.html#ch08)和第[9章](kindle_split_019.html#ch09)的Elo评级示例来看，比赛数据对于获得选手的评级是必要的。我们并没有根据个人对选手的看法来编码选手评级。这个最后的部分赋予了机器学习算法其神秘性。商业、科学和政府都对算法能找到人类通常忽视的隐藏洞察力感兴趣。如果这些算法的学习方式与人类不同，理论认为，也许它们能够比人类学习得更好。
- en: This interest becomes especially great when we take machine learning into the
    realm of large datasets. One of the hallmarks of large datasets—those you can
    process but not store on your laptop, and larger—is that manually they’re almost
    impenetrable. People have a variety of cognitive biases and shortcuts that make
    them ill-suited at assessing large datasets. Computers, which excel at repeating
    simple behaviors again and again, doing exactly what they are told and nothing
    else, excel at assessing large datasets.
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将机器学习引入大数据集领域时，这种兴趣变得尤为强烈。大数据集的一个显著特点——那些你可以在笔记本电脑上处理但无法存储的数据，以及更大的数据集——是它们在人工处理时几乎无法穿透。人们有多种认知偏差和捷径，使他们不适合评估大数据集。计算机擅长重复简单的行为，精确地执行它们被告知的事情，而不做其他任何事情，因此在评估大数据集方面表现出色。
- en: 10.1.2\. Common applications of machine learning
  id: totrans-723
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.1.2\. 机器学习的常见应用
- en: 'Because machine learning goes so neatly hand-in-hand with large datasets, many
    common machine learning applications are large dataset applications. Consider
    a few:'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习与大数据集紧密相连，许多常见的机器学习应用都是大数据集应用。考虑以下几个例子：
- en: '***Media content recommendations—*** Judging what new songs, videos, or clips
    you might like based on what you’ve listened to or watched in the past'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***媒体内容推荐——*** 根据你过去听过的或看过的内容判断你可能喜欢的新歌曲、视频或剪辑'
- en: '***Online review summarization—*** Judging what words best encapsulate the
    meaning of a restaurant, video, or other product review'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***在线评论摘要——*** 判断哪些词语最能概括餐厅、视频或其他产品评论的意义'
- en: '***Website feature testing—*** Judging what features of web pages best improve
    user experience'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***网站功能测试——*** 判断哪些网页功能最能改善用户体验'
- en: '***Image recognition—*** Adding metadata to images or identifying objects in
    images'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***图像识别——*** 为图像添加元数据或识别图像中的对象'
- en: '***Medical diagnoses—*** Judging which diseases are most likely to be causing
    the symptoms of a patient'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***医疗诊断——*** 判断哪些疾病最有可能导致患者的症状'
- en: '***Voice recognition—*** Judging which words a speaker intended'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***语音识别——*** 判断说话者意图的词语'
- en: Most of these areas are only a decade and a half old. Media content recommendation,
    for example, is perhaps most famously recognized on platforms like Netflix and
    YouTube, which both have recommendations prominently featured in their applications.
    These organizations didn’t come into their own until the mid-2000s, when Google
    purchased YouTube and Netflix launched its video streaming service. Let’s look
    at these five applications of machine learning and identify the four components
    of machine learning involved in each application.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 这些领域中的大多数只有十多年历史。例如，媒体内容推荐在Netflix和YouTube等平台上最为著名，这两个平台都在其应用中突出显示了推荐功能。这些组织直到2005年中期才真正崭露头角，当时谷歌收购了YouTube，Netflix推出了其视频流媒体服务。让我们来看看这五个机器学习的应用，并确定每个应用中涉及的四个机器学习组件。
- en: Media content recommendations
  id: totrans-732
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 媒体内容推荐
- en: Media organizations use machine learning to recommend new content to their audience
    based on information that the organizations accumulate about the tastes and interests
    of viewers. The primary goal of these machine learning algorithms is to recommend
    new content that the media consumer would like to continue consuming (typically
    to sell more advertising).
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 媒体机构使用机器学习根据他们积累的关于观众口味和兴趣的信息向观众推荐新内容。这些机器学习算法的主要目标是推荐媒体消费者愿意继续消费的新内容（通常是为了卖出更多广告）。
- en: These algorithms learn to judge what a user will like from logs that indicate
    which users have consumed which media ([figure 10.4](#ch10fig04)). For example,
    in the case of YouTube, its algorithm would compare the videos you’ve watched
    there against the site’s records of which videos all of its users have watched.
    The algorithm would judge videos that users similar to you liked, but you haven’t
    yet seen, as good videos for you to watch.
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法通过分析哪些用户消费了哪些媒体日志来学习判断用户会喜欢什么（[图10.4](#ch10fig04)）。例如，在YouTube的情况下，其算法会将其用户观看的视频与网站上所有用户观看的视频记录进行比较。算法会判断与您相似的用户喜欢的、您尚未观看的视频，作为您应该观看的好视频。
- en: Figure 10.4\. Media content recommendation algorithms are an example of machine
    learning, where an algorithm learns to judge which content a user would like based
    on what previous, similar users have liked.
  id: totrans-735
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.4。媒体内容推荐算法是机器学习的一个例子，其中算法学习根据之前类似用户喜欢的什么内容来判断用户会喜欢什么内容。
- en: '![](10fig04_alt.jpg)'
  id: totrans-736
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig04_alt.jpg)'
- en: Online review summarization
  id: totrans-737
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在线评论摘要
- en: Another area where machine learning overlaps with large datasets is when online
    retailers, like Amazon, summarize reviews of their products. The goal of these
    machine learning algorithms is to judge which reviews are related and how to best
    describe those review groupings. Shoppers can then use the groupings to look for
    specific product information.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个机器学习与大数据集重叠的领域是，在线零售商，如亚马逊，总结其产品的评论。这些机器学习算法的目标是判断哪些评论相关，以及如何最好地描述这些评论分组。购物者可以使用这些分组来查找特定的产品信息。
- en: Amazon developers write programs to learn which words best describe which reviews
    ([figure 10.5](#ch10fig05)). These programs—machine learning algorithms—take in
    a large dataset of product reviews and adjust themselves until they can accurately
    group and summarize reviews. Then, once these programs have learned enough, developers
    can incorporate them into the product page for customers to interact with.
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊的开发者编写程序来学习哪些词汇最能描述哪些评论（[图10.5](#ch10fig05)）。这些程序——机器学习算法——接受大量产品评论数据集，并自我调整，直到它们能够准确地对评论进行分组和总结。然后，一旦这些程序学习足够多，开发者就可以将它们集成到产品页面上，供客户互动。
- en: Figure 10.5\. Amazon uses machine learning to find short phrases that best encapsulate
    product reviews on its website. Those summaries help shoppers learn about the
    products from other shoppers.
  id: totrans-740
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.5。亚马逊使用机器学习在其网站上找到最能概括产品评论的短语。这些摘要帮助购物者了解其他购物者对产品的看法。
- en: '![](10fig05_alt.jpg)'
  id: totrans-741
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig05_alt.jpg)'
- en: Website feature testing
  id: totrans-742
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 网站功能测试
- en: In the pursuit of constantly improving user experience, many websites will show
    subsets of their visitors features that are under development. For example, a
    company may want to test whether making a purchase button yellow, red, or green
    results in the most purchases. Developers can write programs that learn users’
    favorite features from what users do on the site.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 为了不断改进用户体验，许多网站会向其访客展示正在开发中的功能子集。例如，一家公司可能想测试将购买按钮设置为黄色、红色或绿色是否会导致最多的购买。开发者可以编写程序，从用户在网站上的行为中学习用户的喜好功能。
- en: Like the media content recommender programs, these programs also learn from
    user log data. Instead of grouping users together, however, these programs learn
    to judge which features make users more likely to engage in behaviors that the
    website designers value, such as spending more time on the site, adding more items
    to their shopping cart, or purchasing more products from the website.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 与媒体内容推荐程序一样，这些程序也从用户日志数据中学习。然而，与将用户分组不同，这些程序学会了判断哪些特征使用户更有可能参与网站设计师重视的行为，例如在网站上花费更多时间，将更多项目添加到购物车中，或从网站上购买更多产品。
- en: Image recognition
  id: totrans-745
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图像识别
- en: 'An area in machine learning where advances are being rapidly made is in image
    recognition. The goal of this subfield is to identify objects in images, or otherwise
    generate metadata about the image (such as where it was taken), based on visual
    cues alone. Facebook is famous for using image recognition on all the photos uploaded
    to its site. For example, when I upload my author photo to Facebook, it provides
    these tags: photo of one person, smiling, beard, close-up ([figure 10.6](#ch10fig06)).'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习领域，图像识别是一个正在迅速取得进展的领域。这个子领域的目标是仅基于视觉线索来识别图像中的对象，或者生成关于图像的元数据（例如它在哪里被拍摄），例如，Facebook因其对所有上传到其网站的图片使用图像识别而闻名。例如，当我将我的作者照片上传到Facebook时，它提供了以下标签：单人照片，微笑，有胡须，特写（[图10.6](#ch10fig06)）。
- en: 'Figure 10.6\. These photos demonstrate two examples of image recognition: metadata
    tagging of images (top) and object detection (bottom).'
  id: totrans-747
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.6\. 这些照片展示了两种图像识别的例子：图像元数据标记（顶部）和对象检测（底部）。
- en: '![](10fig06_alt.jpg)'
  id: totrans-748
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig06_alt.jpg)'
- en: We can see another example of object detection in [figure 10.6](#ch10fig06).
    This form of image recognition attempts to put boxes around items that the algorithm
    identifies. In this case, our algorithm recognized three boats in the picture.
    Amazon is using this technology—along with others—in an attempt to create point-of-sale-free
    stores where machine learning technology identifies what you’ve placed in your
    bag.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在[图10.6](#ch10fig06)中看到另一个目标检测的例子。这种图像识别形式试图围绕算法识别的项目放置方框。在这种情况下，我们的算法识别了图片中的三艘船。亚马逊正在使用这项技术——以及其他技术——试图创建无需结账的商店，其中机器学习技术可以识别你放在包里的物品。
- en: Medical diagnoses
  id: totrans-750
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 医疗诊断
- en: Yet another place where machine learning is being used is in the arena of medical
    diagnoses. There, programmers, doctors, and scientists are collaborating to improve
    how we judge which illness someone has based on their symptoms, medical history,
    and test results. For example, machine learning allows radiologists to work on
    lower quality images than they previously could, because the machine learning
    algorithms can learn to judge unclear or blurry images better than humans can.
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个机器学习被使用的领域是在医疗诊断的领域。在那里，程序员、医生和科学家正在合作，以改善我们根据症状、病史和检测结果判断某人患有什么疾病的方法。例如，机器学习允许放射科医生处理比以前更低的图像质量，因为机器学习算法可以学会比人类更好地判断不清晰或模糊的图像。
- en: These algorithms learn from large datasets of electronic health information
    to judge health outcomes, much like doctors themselves learn diagnostics during
    medical school. However, unlike medical students, who are taught by experienced
    doctors, these algorithms teach themselves. And sometimes they’ll learn patterns
    that are entirely different from what trained experts expect.
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法从大量的电子健康信息数据集中学习，以判断健康结果，这与医生在医学院学习诊断学时的情况类似。然而，与由经验丰富的医生教授的医疗学生不同，这些算法是自我学习的。有时，它们会学习到与训练有素的专家预期的完全不同的模式。
- en: Voice recognition
  id: totrans-753
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 语音识别
- en: The last machine learning example is voice recognition. In voice recognition,
    programmers are attempting to write code that can take in sound from a person’s
    voice and judge which words the speaker intended ([figure 10.7](#ch10fig07)).
    You may be familiar with this technology from voice-to-text capabilities on your
    smartphone or an Amazon Alexa, Google Home, or Facebook Portal device.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个机器学习例子是语音识别。在语音识别中，程序员试图编写代码，可以从人的声音中接收声音并判断说话人意图的词语（[图10.7](#ch10fig07)）。你可能熟悉这项技术，比如在你的智能手机上的语音转文字功能，或者Amazon
    Alexa、Google Home或Facebook Portal设备上。
- en: Figure 10.7\. Voice recognition machine learning attempts to judge which words
    a speaker meant by analyzing sound waves produced by their speech.
  id: totrans-755
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.7\. 语音识别机器学习试图通过分析说话人产生的声波来判断说话人意图的词语。
- en: '![](10fig07_alt.jpg)'
  id: totrans-756
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig07_alt.jpg)'
- en: Programmers write these programs to learn which sounds suggest which words by
    processing large datasets of sound files with corresponding transcripts. The often
    lackluster performance of these programs compared to the relative ease with which
    people are able to understand one another’s voices highlights a difference in
    how algorithms learn versus how people learn. Even the best voice recognition
    algorithms have not taught themselves how to understand words as well as most
    elementary school children do.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 程序员编写这些程序是为了通过处理包含对应转录的大数据集的音频文件来学习哪些声音表示哪些单词。与人们理解彼此声音的相对容易程度相比，这些程序往往表现不佳，这突显了算法学习和人类学习之间的差异。即使最好的语音识别算法也没有像大多数小学生那样学会如何理解单词。
- en: Now that we’ve gone over five (plus two) examples of machine learning and how
    to think about them as self-learning judgment algorithms, we’re ready to try our
    hand at some machine learning. In the next section, we’ll take a look at using
    PySpark’s decision tree classifier, a type of machine learning algorithm that
    learns to judge alternative outcomes by learning yes/no rules from the data.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了五个（加上两个）机器学习示例以及如何将它们视为自我学习的判断算法，我们准备尝试一些机器学习。在下一节中，我们将探讨使用PySpark的决策树分类器，这是一种从数据中学习是/否规则以判断不同结果的机器学习算法。
- en: 10.2\. Machine learning basics with decision tree classifiers
  id: totrans-759
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2. 机器学习基础与决策树分类器
- en: For our introduction to machine learning, we’ll be looking at decision tree
    classifiers. Decision tree classifiers are an excellent choice of machine learning
    algorithm when we want interpretable results, because the yes/no rules are intuitively
    simple. Even the mathematically uninclined can usually trace their way down a
    decision tree to see how the algorithm arrived at its judgment. Because of this,
    they’re a great way to solve the scenario we’ll be approaching in this chapter.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 为了介绍机器学习，我们将研究决策树分类器。当我们想要可解释的结果时，决策树分类器是机器学习算法的一个很好的选择，因为是/否规则直观简单。即使数学上不感兴趣的人通常也能沿着决策树追踪，看看算法是如何得出判断的。正因为如此，它们是解决我们在本章中将要处理场景的一个很好的方法。
- en: '|  |'
  id: totrans-761
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Scenario
  id: totrans-762
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 场景
- en: A group of hikers is tired of having to bring snacks on the trail. They’ve collected
    a bunch of data about mushrooms—such as the mushrooms’ size, color, and cap shape—and
    they want you to use that data and come up with a way to judge whether or not
    a mushroom is safe to eat. Design a machine learning algorithm that can provide
    the hikers rules for choosing which mushrooms are edible and which are poisonous.
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 一群徒步旅行者厌倦了在途中携带零食。他们收集了大量关于蘑菇的数据——例如蘑菇的大小、颜色和帽子形状——他们希望您使用这些数据并想出一个判断蘑菇是否安全的方法。设计一个机器学习算法，可以为徒步旅行者提供选择哪些蘑菇可食用和哪些蘑菇有毒的规则。
- en: '|  |'
  id: totrans-764
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-765
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Warning
  id: totrans-766
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: The information on mushrooms in this section is for learning purposes only and
    is **not** to be used for identifying mushrooms. Eating wild mushrooms can have
    serious and possibly fatal consequences.
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中关于蘑菇的信息仅用于学习目的，**不应用于识别蘑菇**。食用野生蘑菇可能具有严重甚至致命的后果。
- en: '|  |'
  id: totrans-768
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: With this setup, we know we’re in a good situation to use machine learning.
    We have a judgment problem—judging which mushrooms are poisonous and which are
    safe to eat—and we have historical data from which we can learn. That takes care
    of two of the criteria. The two remaining can be met by writing some code to learn
    to make judgments from the data. That part is up to us.
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置下，我们知道我们处于使用机器学习的良好状态。我们有一个判断问题——判断哪些蘑菇是有毒的，哪些是安全的可以食用——并且我们有历史数据，我们可以从中学习。这样就解决了两个标准。剩下的两个可以通过编写一些代码来学习从数据中做出判断来满足。这部分取决于我们。
- en: 10.2.1\. Designing decision tree classifiers
  id: totrans-770
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.2.1. 设计决策树分类器
- en: Before we write our decision tree classifier code, let’s take a look at how
    decision tree classifiers work. [Table 10.1](#ch10table01) shows what a subset
    of the mushroom data might look like.
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们编写决策树分类器代码之前，让我们先看看决策树分类器是如何工作的。[表10.1](#ch10table01)展示了蘑菇数据的一个子集可能的样子。
- en: Table 10.1\. A subset of mushroom data for a small decision tree classifier
  id: totrans-772
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表10.1. 小型决策树分类器的蘑菇数据子集
- en: '| Is it edible? | Cap color | Odor | Habitat |'
  id: totrans-773
  prefs: []
  type: TYPE_TB
  zh: '| 是否可食用？ | 帽子颜色 | 气味 | 栖息地 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-774
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Poison | Brown | Almond | Meadow |'
  id: totrans-775
  prefs: []
  type: TYPE_TB
  zh: '| 有毒 | 棕色 | 杏仁 | 草地 |'
- en: '| Poison | Red | Spicy | Meadow |'
  id: totrans-776
  prefs: []
  type: TYPE_TB
  zh: '| 有毒 | 红色 | 辛辣 | 草地 |'
- en: '| Poison | Purple | Musty | Woods |'
  id: totrans-777
  prefs: []
  type: TYPE_TB
  zh: '| 有毒 | 紫色 | 发霉 | 森林 |'
- en: '| Edible | Brown | Musty | Meadow |'
  id: totrans-778
  prefs: []
  type: TYPE_TB
  zh: '| 可食用 | 棕色 | 发霉 | 草地 |'
- en: '| Edible | Grey | Musty | Woods |'
  id: totrans-779
  prefs: []
  type: TYPE_TB
  zh: '| 可食用 | 灰色 | 霉味 | 森林 |'
- en: 'The decision tree classifier we’ll write works by learning a series of rules
    against which to judge new mushrooms. For example, for the data in [table 10.1](#ch10table01),
    our decision tree classifier may learn to ask three questions ([figure 10.8](#ch10fig08)):'
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要编写的决策树分类器通过学习一系列规则来判断新蘑菇。例如，对于[表10.1](#ch10table01)中的数据，我们的决策树分类器可能学习提出三个问题([图10.8](#ch10fig08))：
- en: Does that mushroom smell musty?
  id: totrans-781
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 那个蘑菇有霉味吗？
- en: Was the mushroom found in the woods?
  id: totrans-782
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 蘑菇是在森林里找到的吗？
- en: Is the mushroom purple?
  id: totrans-783
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 蘑菇是紫色的吗？
- en: Figure 10.8\. Decision tree algorithms learn to construct binary rules against
    which they can judge new data.
  id: totrans-784
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.8\. 决策树算法学习构建二元规则，以便它们可以判断新数据。
- en: '![](10fig08_alt.jpg)'
  id: totrans-785
  prefs: []
  type: TYPE_IMG
  zh: '![图10.8](10fig08_alt.jpg)'
- en: By answering these three questions, we can judge all of the five mushrooms in
    our dataset. We can represent these rules as a series of `if`-`else` statements
    or a tree of yes/no questions. In fact, the decision tree algorithm gets its name
    from the fact that these rules can be represented in a flowchart-like tree diagram.
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: 通过回答这三个问题，我们可以判断数据集中的所有五种蘑菇。我们可以将这些规则表示为一系列的`if`-`else`语句或是一个是/否问题的树状图。实际上，决策树算法之所以得名，是因为这些规则可以用类似流程图的树状图来表示。
- en: For the miniature example in this section, the process of learning these rules
    would be fast. There are only three variables to test and only a few options for
    each variable. As noted, our self-adjusting algorithm would need to learn to judge
    based on only two rules. With more data, there are more calculations to make,
    and the process takes longer.
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中的微型示例中，学习这些规则的过程会很快。只有三个变量需要测试，每个变量的选项也有限。正如所提到的，我们的自适应算法需要学习根据仅两条规则进行判断。随着数据的增加，需要进行的计算更多，过程也会更长。
- en: At each step in the rule-making process, the decision tree algorithm learns
    to create rules that optimally separate the group into maximally similar categories.
    In our case, the algorithm will learn to maximally separate edible and poisonous
    mushrooms at each step. This is why smell would be the first question our algorithm
    would learn to ask. If the mushroom is musty smelling, then it will be safe to
    eat 2 times out of 3\. We can see that in [table 10.1](#ch10table01). Indeed,
    none of the mushrooms in our small dataset that don’t smell musty are edible.
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 在规则制定过程的每一步，决策树算法都会学习创建规则，以最优地分离组别到最大相似类别。在我们的例子中，算法将在每一步学习最大程度地分离可食用和有毒蘑菇。这就是为什么气味会成为我们的算法首先学习的第一个问题。如果蘑菇有霉味，那么它有三分之二的机会是安全的。我们可以在[表10.1](#ch10table01)中看到这一点。实际上，在我们的小数据集中，没有霉味的蘑菇都是可食用的。
- en: Compare this to if we had chosen to split on color. If we split on color first,
    we would have almost no new information. Each of the mushrooms is a different
    color! Sure, we could go through each of the colors one by one, but we prefer
    to ask the questions in an order such that the groups separate more quickly.
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们选择按颜色分割相比。如果我们首先按颜色分割，我们将几乎没有新的信息。每种蘑菇都是不同的颜色！当然，我们可以逐个检查每种颜色，但我们更倾向于按照使组别更快分离的顺序提问。
- en: We can refer to this process of sorting data into groups of similar classification
    items as maximizing homogeneity. As users of decision trees, we’ll often face
    the choice of which measurement to use for this process. The two common metrics
    you’ll hear of are called *Gini impurity* and *information gain*. I won’t go into
    detail on either of these terms—for the purposes of an introduction to PySpark’s
    machine learning capabilities, it’s enough to know they’re both measures of the
    differences in a grouping of data.
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将将数据排序到相似分类项的组别的过程称为最大化同质性。作为决策树的使用者，我们经常会面临选择哪种度量用于此过程的选择。你将听到的两个常见度量被称为*基尼不纯度*和*信息增益*。我不会详细解释这两个术语——对于PySpark机器学习能力的介绍来说，了解它们都是数据分组差异的度量就足够了。
- en: '[Figure 10.9](#ch10fig09) shows how our algorithm may judge a new observation.
    We can see that first, it checks the smell. The smell is musty, so we move on
    to the second rule. The mushroom was found in the woods, so we move on to the
    final question. Indeed, this mushroom was purple, so we put it aside: our decision
    tree expects this mushroom to be poisonous.'
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10.9](#ch10fig09)显示了我们的算法如何判断新的观察结果。我们可以看到，首先，它检查气味。气味是霉味，所以我们继续到第二个规则。蘑菇是在森林里找到的，所以我们继续到最后一个问题。确实，这个蘑菇是紫色的，所以我们把它放一边：我们的决策树预计这个蘑菇是有毒的。'
- en: Figure 10.9\. Decision tree algorithms work by learning to group the data into
    the most similar chunks. The algorithm will judge new data based on the grouping
    that data would end up in if it was applied against the tree.
  id: totrans-792
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.9\. 决策树算法通过学习将数据分组到最相似的块中工作。算法将根据数据如果应用于树将最终进入的分组来评估新数据。
- en: '![](10fig09_alt.jpg)'
  id: totrans-793
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig09_alt.jpg)'
- en: Now that we’ve taken a look at how our decision tree algorithms work, let’s
    take a look at using them in PySpark.
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了决策树算法的工作原理，让我们来看看如何在PySpark中使用它们。
- en: 10.2.2\. Implementing a decision tree in PySpark
  id: totrans-795
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.2.2\. 在PySpark中实现决策树
- en: PySpark’s machine learning capabilities live in a package called `ml`. This
    package itself contains a few different modules categorizing some of the core
    machine learning capabilities, including
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark的机器学习功能存在于一个名为`ml`的包中。该包本身包含几个不同的模块，分类了一些核心机器学习功能，包括
- en: '**`pyspark.ml.feature`—** For feature transformation and creation'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`pyspark.ml.feature`—** 用于特征转换和创建'
- en: '**`pyspark.ml.classification`—** Algorithms for judging the category in which
    a data point belongs'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`pyspark.ml.classification`—** 用于判断数据点所属类别的算法'
- en: '**`pyspark.ml.tuning`—** Algorithms for improving our machine learners'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`pyspark.ml.tuning`—** 用于改进我们的机器学习算法'
- en: '**`pyspark.ml.evaluation`—** Algorithms for evaluating machine leaners'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`pyspark.ml.evaluation`—** 用于评估机器学习算法'
- en: '**`pyspark.ml.util`—** Methods of saving and loading machine learners'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`pyspark.ml.util`—** 保存和加载机器学习方法的函数'
- en: All of these modules are similar in style to the PySpark methods we looked at
    in [chapters 7](kindle_split_017.html#ch07) and [9](kindle_split_019.html#ch09).
    However, all of PySpark’s machine learning features expect us to have our data
    in a PySpark `DataFrame` object—not an `RDD`, as we’ve been using. The `RDD` is
    an abstract parallelizable data structure at the core of Spark, whereas the `DataFrame`
    is a layer on top of the `RDD` that provides a notion of rows and columns. If
    you remember, back in [chapter 7](kindle_split_017.html#ch07) when we introduced
    PySpark, I mentioned that PySpark `DataFrame`s are Spark’s preferred data type
    for interacting with SQL databases. This is because the Spark `DataFrame` provides
    a tabular interface to data stored in an `RDD`, just like SQL databases provide
    tabular storage and retrieval.
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些模块在风格上都与我们在第7章和第9章中查看的PySpark方法相似。然而，PySpark的所有机器学习功能都期望我们的数据在一个PySpark
    `DataFrame`对象中——而不是我们一直在使用的`RDD`。`RDD`是Spark核心中的抽象并行化数据结构，而`DataFrame`是建立在`RDD`之上的一个层，它提供了行和列的概念。如果你还记得，在第7章中介绍PySpark时，我提到PySpark
    `DataFrame`s是Spark与SQL数据库交互的首选数据类型。这是因为Spark `DataFrame`为存储在`RDD`中的数据提供了一个表格接口，就像SQL数据库提供表格存储和检索一样。
- en: Bringing the data into a `DataFrame` will be the first step in our machine learning
    process. The other steps include running our decision tree learner and evaluating
    the decision tree we’ve built.
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据引入`DataFrame`将是我们的机器学习过程中的第一步。其他步骤包括运行我们的决策树学习器和评估我们构建的决策树。
- en: Brining data into a DataFrame
  id: totrans-804
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将数据引入DataFrame
- en: 'The first step in our machine learning process is getting the data ready for
    our analysis. This step includes any preprocessing we might want to do—such as
    changing formats of the variables and data cleaning. In this case, we’re lucky:
    our data is coming in clean.'
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 我们机器学习过程中的第一步是准备好数据以供分析。这一步骤包括我们可能想要进行的任何预处理——例如改变变量的格式和数据清洗。在这种情况下，我们很幸运：我们的数据是干净进入的。
- en: For `RDD`s, Spark provided a simple method—`.textFile`—that we could use to
    read in text data and process it. Similarly, for `DataFrame`s, we have several
    convenient options. If the data is already in an `RDD`, we can call `DataFrame`
    on the `RDD` and convert it. If the data is in a database, we can use `SparkSession`’s
    `.sql` method to return a `DataFrame` representation of the results of a SQL query.
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`RDD`s，Spark提供了一个简单的方法——`.textFile`——我们可以用它来读取文本数据并处理它。同样，对于`DataFrame`s，我们有几个方便的选项。如果数据已经在`RDD`中，我们可以在`RDD`上调用`DataFrame`并转换它。如果数据在数据库中，我们可以使用`SparkSession`的`.sql`方法来返回SQL查询结果的`DataFrame`表示。
- en: 'For our example, we have our data in a flat file (which you can find on this
    book’s repository online: [https://www.manning.com/downloads/1961](https://www.manning.com/downloads/1961)).
    To handle that format, PySpark has a method called `.csv` that returns a `DataFrameReader`.
    We can turn a CSV file into a PySpark `DataFrame` by calling `SparkSession.read.csv`
    and passing in the name of our file. The method has options for just about anything
    you would need to ensure your tabular flat-file data is coming in properly. One
    of my favorites is `inferSchema`, which is used in the following listing.'
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们的数据存储在一个平面文件中（你可以在本书的在线仓库中找到：[https://www.manning.com/downloads/1961](https://www.manning.com/downloads/1961)）。为了处理这种格式，PySpark有一个名为`.csv`的方法，它返回一个`DataFrameReader`。我们可以通过调用`SparkSession.read.csv`并将文件名传递给它，将CSV文件转换为PySpark的`DataFrame`。该方法提供了几乎所有你需要确保你的表格平面文件数据正确输入的选项。我最喜欢的一个是`inferSchema`，这在下面的列表中有所体现。
- en: Listing 10.1\. Reading in text data
  id: totrans-808
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.1\. 读取文本数据
- en: '[PRE37]'
  id: totrans-809
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The `inferSchema` option of the `.csv` method tells Spark to make a guess at
    the type of the variables in our data. If you remember, in the last two chapters,
    unless the data was coming in as JSON, we had to explicitly cast our data to the
    types we wanted it to be. For small datasets, this isn’t a challenge, but if we
    have hundreds of variables, this can be a tiresome process. In these cases, `inferSchema`
    can be a real time saver.
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: '`.csv`方法的`inferSchema`选项告诉Spark猜测我们数据中变量的类型。如果你还记得，在前两章中，除非数据以JSON格式传入，否则我们必须显式地将我们的数据转换为想要的类型。对于小型数据集，这并不是一个挑战，但如果我们有数百个变量，这可能是一个繁琐的过程。在这些情况下，`inferSchema`可以节省大量时间。'
- en: Organizing the data for learning
  id: totrans-811
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为学习组织数据
- en: Now that we have data in a `DataFrame`, we’re one step closer to feeding it
    into a Spark machine learner. Before we can do that, however, we have to get the
    data into the specific type of `DataFrame` format that Spark insists on.
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将数据放入`DataFrame`中，我们离将其输入Spark机器学习器又近了一步。然而，在我们能够这样做之前，我们必须将数据放入Spark坚持的特定类型的`DataFrame`格式中。
- en: 'Spark’s machine learning classifiers look for two columns in a `DataFrame`:'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的机器学习分类器在`DataFrame`中寻找两个列：
- en: A `label` column that indicates the correct classification of the data
  id: totrans-814
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个`label`列，表示数据的正确分类
- en: A `features` column that contains the features we’re going to use to predict
    the label
  id: totrans-815
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个包含我们将要用于预测标签的特征的`features`列
- en: Your `DataFrame` can contain as many columns as you would like, with whatever
    names you’d like, but these two columns are the ones that Spark will use for its
    machine learning. The `label` column is what Spark’s machine learning classifiers
    learn to judge—is the data the algorithm sees more like this label or that label?
    The `features` column is the data about each observation that the machine learning
    algorithm will learn to use to make that judgment.
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: 你的`DataFrame`可以包含你想要的任意多列，以及你想要的任意名称，但这两个列是Spark将用于其机器学习的列。`label`列是Spark机器学习分类器学习来判断的——数据算法看到的是这个标签还是那个标签？`features`列是机器学习算法将学习用于做出这种判断的每个观察的数据。
- en: Furthermore, Spark expects specific data types for these columns. For our numerical
    data—data that would be represented as floats and integers in Python—Spark knows
    what to do. For categorical data, we’ll have some choices to make. The simplest
    way to handle such data is to use PySpark’s `StringIndexer`. The `StringIndexer`
    transforms categorical data stored as category names (using strings) and indexes
    the names as numerical variables. `StringIndexer` indexes categories in order
    of frequency—from most common to least common—not in order observed. The most
    common category will be 0, the second most common category 1, and so on ([figure
    10.10](#ch10fig10)).
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Spark期望这些列具有特定的数据类型。对于我们的数值数据——在Python中表示为浮点数和整数的数值数据——Spark知道如何处理。对于分类数据，我们将有一些选择。处理此类数据的最简单方法是使用PySpark的`StringIndexer`。`StringIndexer`将存储为类别名称（使用字符串）的分类数据转换为数值变量，并将名称索引为数值变量。`StringIndexer`按频率顺序索引类别——从最常见到最不常见，而不是按观察顺序。最常见的类别将是0，第二常见的类别是1，依此类推（[图10.10](#ch10fig10)）。
- en: Figure 10.10\. Spark’s `StringIndexer` transforms categorical variables as strings
    into numerical categories. More common categories have lower indexes.
  id: totrans-818
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.10\. Spark的`StringIndexer`将字符串形式的分类变量转换为数值类别。更常见的类别具有较低的索引。
- en: '![](10fig10_alt.jpg)'
  id: totrans-819
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig10_alt.jpg)'
- en: When we use `StringIndexer`, Spark returns a new `DataFrame`, with our old columns
    and our new indexed column ([figure 10.11](#ch10fig11)). Spark has to return a
    new `DataFrame` because most data structures in Spark are immutable—they can’t
    be changed once they’re created. That’s a property of the Scala programming language
    in which Spark is written. For our purposes, this is great because it means we
    can write a small `reduce` statement and update our `DataFrame`, as we can see
    in the following listing.
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用`StringIndexer`时，Spark返回一个新的`DataFrame`，包含我们的旧列和我们的新索引列（[图10.11](#ch10fig11)）。Spark必须返回一个新的`DataFrame`，因为Spark中的大多数数据结构都是不可变的——一旦创建就无法更改。这是Spark所使用的Scala编程语言的一个特性。对我们来说，这是一个优点，因为它意味着我们可以编写一个小的`reduce`语句并更新我们的`DataFrame`，如下面的列表所示。
- en: Figure 10.11\. `Transformer`s in PySpark, such as `StringIndexer`, return a
    `DataFrame` that contains all the columns of the original, plus a new column,
    specified by the transformation.
  id: totrans-821
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.11\. PySpark中的`Transformer`，例如`StringIndexer`，返回一个包含原始所有列以及由转换指定的新列的`DataFrame`。
- en: '![](10fig11_alt.jpg)'
  id: totrans-822
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig11_alt.jpg)'
- en: Listing 10.2\. Transforming strings to indexed categorical variables with `StringIndexer`
  id: totrans-823
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.2\. 使用`StringIndexer`将字符串转换为索引分类变量
- en: '[PRE38]'
  id: totrans-824
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '***1* Defines a helper function for our reduce statement—instead of acc—and
    next we’ll use label and df**'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 定义了一个用于我们的reduce语句的辅助函数——而不是acc——接下来我们将使用标签和df**'
- en: '***2* Takes column labels—input and output—as parameters and appends to the
    DataFrame a transformed version of the input column with the new label**'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 以列标签（输入和输出）作为参数，并将输入列的转换版本附加到DataFrame中，带有新的标签**'
- en: '***3* The .fit and .transform methods apply the changes and return a new DataFrame.**'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* .fit和.transform方法应用更改并返回一个新的DataFrame。**'
- en: '***4* We’ll need a sequence of columns to transform—this is what we’ll reduce
    over.**'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 我们需要一个列的序列来转换——这是我们将在其上reduce的内容。**'
- en: '***5* Lastly, we’ll call reduce and transform our data frame.**'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 最后，我们将调用reduce并转换我们的数据框。**'
- en: '[Listing 10.2](#ch10ex02) shows this process in action. We first write a helper
    function that will apply the `StringIndexer` to a given column. The helper function
    calls `StringIndexer` and passes it an input label, which is specified by the
    parameter in first position, and an output label, in this case, that variable
    preceded by an `"i-"`. Our transformed columns will be added into the `DataFrame`,
    so they need to have unique names. All columns in a `DataFrame` must have unique
    names.'
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表10.2](#ch10ex02)展示了这一过程在实际中的应用。我们首先编写一个辅助函数，该函数将应用`StringIndexer`到指定的列。辅助函数调用`StringIndexer`，并传递一个输入标签，该标签由第一个位置的参数指定，以及一个输出标签，在这种情况下，该变量前面加上一个`"i-"`。我们的转换列将被添加到`DataFrame`中，因此它们需要具有唯一的名称。`DataFrame`中的所有列都必须具有唯一的名称。'
- en: Then, we select some categories we want to transform. In [listing 10.2](#ch10ex02),
    I’ve chosen to use cap-shape, cap-surface, and cap-color. I’m hoping that mushroom
    caps can tell me something about whether a mushroom is poisonous or not. We can
    then call `reduce`, passing it our helper function, our categories, and our `DataFrame`.
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们选择一些我们想要转换的类别。在[列表10.2](#ch10ex02)中，我选择使用cap-shape、cap-surface和cap-color。我希望蘑菇的盖子可以告诉我有关蘑菇是否有毒的信息。然后我们可以调用`reduce`，传递我们的辅助函数、我们的类别和我们的`DataFrame`。
- en: 'This process results in a `DataFrame` with three additional columns:'
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程产生了一个包含三个额外列的`DataFrame`：
- en: '**`i-cap-shape`—** An indexed transformation of cap-shape'
  id: totrans-833
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**`i-cap-shape`—** 对cap-shape的索引转换'
- en: '**`i-cap-surface`—** An indexed transformation of cap-surface'
  id: totrans-834
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**`i-cap-surface`—** 对cap-surface的索引转换'
- en: '**`i-cap-color`—** An indexed transformation of cap-color'
  id: totrans-835
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**`i-cap-color`—** 对cap-color的索引转换'
- en: 'Spark’s machine learning classifiers, though, only want one column named `features`.
    To use these three columns as features, we’ll have to gather them up in another
    column. Conveniently, PySpark has a class for this as well: `VectorAssembler`.
    `VectorAssembler` is a `Transformer` like `StringIndexer`—it takes some input
    column names and an output column name and has methods to return a new `DataFrame`
    that has all the columns of the original, plus the new column we want to add ([figure
    10.12](#ch10fig12)).'
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Spark的机器学习分类器只想要一个名为`features`的列。为了使用这三个列作为特征，我们必须将它们收集到另一个列中。幸运的是，PySpark也有一个用于此目的的类：`VectorAssembler`。`VectorAssembler`与`StringIndexer`类似，它接受一些输入列名称和一个输出列名称，并具有返回一个新`DataFrame`的方法，该`DataFrame`包含原始的所有列以及我们想要添加的新列（[图10.12](#ch10fig12)）。
- en: Figure 10.12\. `VectorAssembler` is a `Transformer` that can take several columns
    and gather them up as a vector in a single column. This class is especially useful
    for preparing features for machine learning.
  id: totrans-837
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.12\. `VectorAssembler`是一个可以将多个列收集到一个单独列中的`Transformer`。这个类特别适用于为机器学习准备特征。
- en: '![](10fig12_alt.jpg)'
  id: totrans-838
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig12_alt.jpg)'
- en: Unlike `StringIndexer`, which expects to work on one column at a time, `VectorAssembler`
    expects to round up a host of columns. For our transformation, we only need a
    single call to `VectorAssembler`, as shown in the following listing.
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 与期望一次处理一个列的`StringIndexer`不同，`VectorAssembler`期望汇总大量列。对于我们的转换，我们只需要调用一次`VectorAssembler`，如下面的列表所示。
- en: Listing 10.3\. Gathering features for machine learning with `VectorAssembler`
  id: totrans-840
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.3\. 使用`VectorAssembler`收集机器学习特征
- en: '[PRE39]'
  id: totrans-841
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '***1* We initialize VectorAssembler with the names of the columns we want to
    assemble and the desired output column name.**'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 我们使用要组装的列的名称和所需的输出列名称初始化`VectorAssembler`。**'
- en: '***2* Calling .transform on a DataFrame returns a new DataFrame with an additional
    column.**'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 在DataFrame上调用.transform会返回一个新的DataFrame，并增加一个额外的列。**'
- en: In [listing 10.3](#ch10ex03), we can see an example of how `VectorAssembler`
    works. We can see that we’re passing the three columns we want to use as features
    to the `inputCols` parameter as a list, and the `outputCol` parameter is set to
    `"features"`. This tells `VectorAssembler` to gather those three columns up and
    make a new column called `features`. At the end of this step, our `DataFrame`
    will contain all the columns of the original `DataFrame`, plus four new columns—one
    for each categorical variable we indexed and one containing all of them together.
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表10.3](#ch10ex03)中，我们可以看到一个`VectorAssembler`如何工作的例子。我们可以看到，我们将想要用作特征的三个列作为列表传递给`inputCols`参数，并将`outputCol`参数设置为`"features"`。这告诉`VectorAssembler`将这三个列收集起来，并创建一个名为`features`的新列。在这个步骤结束时，我们的`DataFrame`将包含原始`DataFrame`的所有列，以及四个新列——一个用于每个我们索引的类别变量，还有一个包含所有这些变量的列。
- en: 'At this point, the only thing we need before we can move on to machine learning
    is the labels. Our labels are contained in a column called `edible?`, which has
    two labels—`edible` or `poisonous`—both represented as strings. Again, we can
    use `StringTransformer`. Instead of looping through a sequence of column names
    though, we only need to worry about one column: `edible?`, as shown in the following
    code.'
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，在我们继续到机器学习之前，我们唯一需要的是标签。我们的标签包含在一个名为`edible?`的列中，有两个标签——`edible`或`poisonous`——都表示为字符串。同样，我们可以使用`StringTransformer`。不过，我们不需要遍历一系列列名，我们只需要关注一个列：`edible?`，如下面的代码所示。
- en: '[PRE40]'
  id: totrans-846
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: In [listing 10.4](#ch10ex04), you can see that we specify the `edible?` column
    as we initialize `StringIndexer`, along with the name `label`, which Spark’s machine
    learning classifier will be looking for. Just like when we transformed our feature
    columns, we call `.fit` and `.transform` and then assign this `DataFrame` back
    on top of our original variable.
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表10.4](#ch10ex04)中，你可以看到我们在初始化`StringIndexer`时指定了`edible?`列，以及名称`label`，这是Spark的机器学习分类器将要寻找的。就像我们转换特征列时一样，我们调用`.fit`和`.transform`，然后将这个`DataFrame`重新赋值到我们的原始变量上。
- en: '|  |'
  id: totrans-848
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Label names and data frames**'
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: '**标签名称和数据框**'
- en: Because Spark’s `DataFrame`s are immutable and we’ll usually want to transform
    our `label` column before using it with Spark’s machine learning, we can run into
    problems if the original column name is `"label"`. When this happens, we’ll need
    to rename the column when we transform it. Spark will not let you overwrite columns
    in a `DataFrame`. We can, however, pick an alternate column name by specifying
    the `labelCol` parameter of our machine learning function, such as `DecisionTreeClassifier(labelCol="my-column-name")`.
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Spark的`DataFrame`是不可变的，并且我们通常希望在将`label`列与Spark的机器学习一起使用之前对其进行转换，如果原始列名为`"label"`，我们可能会遇到问题。当这种情况发生时，我们需要在转换时重命名列。Spark不允许你在`DataFrame`中覆盖列。然而，我们可以通过指定机器学习函数的`labelCol`参数来选择一个备选列名，例如`DecisionTreeClassifier(labelCol="my-column-name")`。
- en: '|  |'
  id: totrans-851
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: With these transformations complete, we have our `DataFrame` prepared just like
    Spark needs. We have a `label` column, which contains the labels the algorithm
    will learn to judge, and we have a `features` column, which has the features the
    algorithm will use to do the judging. Finally, we’re ready to learn.
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 这些转换完成后，我们的`DataFrame`就准备好了，就像Spark需要的那样。我们有一个`label`列，其中包含算法将学习的标签，还有一个`features`列，其中包含算法将用于判断的特征。最后，我们准备好学习。
- en: Running our decision tree learner
  id: totrans-853
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 运行我们的决策树学习器
- en: 'Running the machine learning classifier in Spark will feel similar to transforming
    the data. We’ll use a class from Spark’s ml.classifier library called `DecisionTreeClassifier`,
    and we’ll call its `.fit` method on the `DataFrame` we have prepared. For the
    amount of math that’s going on behind the scenes, you would think that this process
    would be more difficult than two short lines:'
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中运行机器学习分类器会感觉与转换数据相似。我们将使用 Spark 的 ml.classifier 库中的一个名为 `DecisionTreeClassifier`
    的类，并在我们准备好的 `DataFrame` 上调用其 `.fit` 方法。对于幕后进行的数学运算量，你可能会认为这个过程比两行简短的代码要复杂得多：
- en: '[PRE41]'
  id: totrans-855
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: However, these two lines show all the code that’s necessary to run a decision
    tree classifier on our `DataFrame`. The first line initializes the classifier
    with the default parameters, and the second fits the classifier to the data. The
    classifier’s `.fit` method returns a model—this is the tree that has learned to
    judge our label based on our data. In our case, the model is a type of `DecisionTreeClassificationModel`
    object. Each classifier in PySpark has a `.fit` method that produces a corresponding
    model object. These models describe the model that’s been learned and have convenient
    functions for inspecting them.
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这两行代码展示了运行决策树分类器在 `DataFrame` 上的所有必要代码。第一行使用默认参数初始化分类器，第二行将分类器拟合到数据上。分类器的
    `.fit` 方法返回一个模型——这是学习根据我们的数据判断标签的树。在我们的例子中，模型是 `DecisionTreeClassificationModel`
    对象的一种类型。PySpark 中的每个分类器都有一个 `.fit` 方法，该方法生成相应的模型对象。这些模型描述了学习到的模型，并具有方便的检查功能。
- en: '|  |'
  id: totrans-857
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**`.fit` and `.transform` in Spark**'
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark 中的 `.fit` 和 `.transform`**'
- en: You may have noticed a lot of `.fit` and `.transform` floating around in this
    chapter. That’s because the classes upon which much of the Spark machine learning
    capability is built share these methods. `.fit` is inherited from Spark’s `Estimator`
    class. This class is used for learning information about data, such as when we
    learn how to index a dataset or how to make judgments about data with a decision
    tree. The `.fit` method returns a `Model`. A `Model` inherits from a `Transformer`,
    which provides a `.transform` method. This method executes the transformation
    that we learn with the `Estimator`.
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到在本章中有很多 `.fit` 和 `.transform`。这是因为构建 Spark 机器学习能力的大部分类都共享这些方法。`.fit`
    是从 Spark 的 `Estimator` 类继承而来的。此类用于学习有关数据的信息，例如当我们学习如何索引数据集或如何使用决策树对数据进行判断时。`.fit`
    方法返回一个 `Model`。`Model` 从 `Transformer` 继承而来，它提供了一个 `.transform` 方法。此方法执行我们使用 `Estimator`
    学习到的转换。
- en: '|  |'
  id: totrans-860
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: For example, `DecisionTreeClassificationModel` has a method called `.toDebugString`
    that shows us all the rules that the model uses to make judgments. We can print
    that string to the screen to see the rules by using `print(model.toDebugString)`.
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`DecisionTreeClassificationModel` 有一个名为 `.toDebugString` 的方法，它会显示模型用于做出判断的所有规则。我们可以使用
    `print(model.toDebugString)` 将该字符串打印到屏幕上，以查看这些规则。
- en: In the following code lines, we can see these rules written as `if`-`else` statements.
    You’ll notice that none of the feature names are included. This is because the
    `features` column we assembled with `VectorAssembler` doesn’t hold onto the names
    of the inputs. To use this decision tree manually, you would have to remember
    the order in which you placed the variables. If we’re writing a script and not
    working in the terminal interactively, we can usually find this in our script.
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码行中，我们可以看到这些规则被写成 `if`-`else` 语句。你会注意到没有任何特征名称。这是因为我们使用 `VectorAssembler`
    组装的 `features` 列不保留输入的名称。要手动使用此决策树，你必须记住变量放置的顺序。如果我们正在编写脚本而不是在终端交互式工作，我们通常可以在脚本中找到这个顺序。
- en: '[PRE42]'
  id: totrans-863
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: For example, we can see in the following listing the order of our variables.
    The first column label we specified, in this case, `i-cap-shape`, will be variable
    `0`; the second, `i-cap-surface`, will be variable `1`; and so on.
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以在以下列表中看到变量的顺序。我们指定的第一个列标签，在这种情况下，`i-cap-shape` 将是变量 `0`；第二个，`i-cap-surface`
    将是变量 `1`；依此类推。
- en: Listing 10.4\. Gathering features for machine learning with `VectorAssembler`
  id: totrans-865
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.4\. 使用 `VectorAssembler` 收集机器学习特征
- en: '[PRE43]'
  id: totrans-866
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '***1* i-cap-shape will be feature 0, i-cap-surface will be feature 1, and i-cap-color
    will be feature 2**'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* i-cap-shape 将是特征 0，i-cap-surface 将是特征 1，i-cap-color 将是特征 2**'
- en: Evaluating the judgments of a decision tree
  id: totrans-868
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评估决策树的判断
- en: 'After the machine learning algorithm is trained, a good question to ask is:
    How good is the algorithm at actually making judgments? This is the question that
    PySpark’s `ml.evaluation` module is designed to answer. The `evaluation` module
    contains classes that compute different evaluation metrics for different machine
    learners:'
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习算法训练完成后，一个值得问的问题是：算法在实际上做出判断方面有多好？这正是PySpark的`ml.evaluation`模块旨在回答的问题。`evaluation`模块包含用于计算不同机器学习器不同评估指标的类：
- en: '**`BinaryClassificationEvaluator`—** For evaluating cases learners with two
    possible outcomes'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`BinaryClassificationEvaluator`—** 用于评估具有两种可能结果的案例学习器'
- en: '**`RegressionEvaluator`—** For evaluating continuous value judgments'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`RegressionEvaluator`—** 用于评估连续值判断'
- en: '**`MulticlassClassificationEvaluator`—** For evaluating multiple label judgments'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`MulticlassClassificationEvaluator`—** 用于评估多个标签判断'
- en: 'Because in our case we only have two options—`poisonous` or `edible`—we want
    to use the `BinaryClassificationEvaluator`. Using this `Evaluator` should feel
    similar to using our machine learner or our `Transformers`. We’ll first initialize
    the `Evaluator`, then we’ll call its `.evaluate` method on a modeled version of
    our `DataFrame`:'
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的情况中只有两个选项——`有毒`或`可食用`——我们希望使用`BinaryClassificationEvaluator`。使用这个`Evaluator`应该感觉与使用我们的机器学习器或`Transformers`相似。我们首先初始化`Evaluator`，然后在其`DataFrame`的模型版本上调用其`.evaluate`方法：
- en: '[PRE44]'
  id: totrans-874
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'When we initialize the `BinaryClassificationEvaluator`, we have the opportunity
    to pick an evaluation metric. The area under the receiver operating characteristic
    (confusingly known by two acronyms: AUC and ROC) curve is the default choice and
    the one I recommend using for most problems ([figure 10.13](#ch10fig13)). This
    metric is one way of evaluating the trade-off between false-positive and false-negative
    assessments.'
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们初始化`BinaryClassificationEvaluator`时，我们有选择评估指标的机会。接收者操作特征（ROC）曲线下的面积（混淆地用两个缩写词：AUC和ROC表示）是默认选择，也是我推荐在大多数问题中使用的选择（[图10.13](#ch10fig13)）。这个指标是评估假阳性评估和假阴性评估之间权衡的一种方式。
- en: Figure 10.13\. The receiver operating characteristic (ROC) curve allows us to
    balance making cautious judgments about poisonous mushrooms, while judging a reasonable
    number of mushrooms as safe.
  id: totrans-876
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.13。接收者操作特征（ROC）曲线使我们能够在谨慎判断有毒蘑菇的同时，合理地判断一定数量的蘑菇为安全。
- en: '![](10fig13_alt.jpg)'
  id: totrans-877
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig13_alt.jpg)'
- en: The curve represents the balance between making true positive and false positive
    judgments. In our case, it represents how good we are at judging poisonous mushrooms
    to be poisonous, without misidentifying edible mushrooms as poisonous. The model
    is a curve because the more we favor identifying mushrooms as poisonous—to prevent
    people from dying—the more we will misjudge edible mushrooms as poisonous. The
    curve helps us find an acceptable point.
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: 曲线代表了做出真正阳性判断和假阳性判断之间的平衡。在我们的案例中，它代表了我们在判断有毒蘑菇为有毒，而不将可食用蘑菇误判为有毒方面的能力。模型是一条曲线，因为当我们更倾向于将蘑菇判断为有毒——以防止人们死亡——我们就更有可能将可食用蘑菇误判为有毒。曲线帮助我们找到一个可接受的点。
- en: With both metrics—area under the receiver operating characteristic curve and
    area under the precision-recall curve—we’re hoping to have as large a number as
    possible. If we have an area under the receiver operating characteristic curve
    value of 1, that means we can correctly judge all poisonous mushrooms as poisonous,
    without judging a single edible mushroom to be inedible. Anything less than 1,
    and there’s some room for improvement. A 0.63 area under the receiver operating
    characteristic curve is not great, but it’s acceptable for an early pass. Next,
    we’ll take a look at some ways we can improve our model.
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这两个指标——接收者操作特征曲线下的面积和精确率-召回率曲线下的面积——我们希望尽可能得到一个较大的数值。如果我们有接收者操作特征曲线下的面积为1，这意味着我们可以正确地将所有有毒蘑菇判断为有毒，而不会将任何可食用蘑菇误判为不可食用。任何小于1的数值，都意味着还有改进的空间。接收者操作特征曲线下的面积为0.63并不理想，但对于初步评估来说是可以接受的。接下来，我们将探讨一些改进模型的方法。
- en: 10.3\. Fast random forest classifications in PySpark
  id: totrans-880
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3. PySpark中的快速随机森林分类
- en: In the previous section, we built a decision tree to judge whether a mushroom
    was poisonous or not. However, the area under the receiver operating characteristic
    curve suggests that we can do better. One way we can try to do better is to use
    a random forest classifier—a machine learning algorithm that’s closely related
    to the decision tree. In this section, we’ll look at random forests and implement
    one in PySpark to achieve better results.
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们构建了一个决策树来判断蘑菇是否有毒。然而，接收者操作特征曲线下的面积表明我们可以做得更好。我们可以尝试做得更好的方法之一是使用随机森林分类器——这是一种与决策树密切相关的人工智能算法。在本节中，我们将探讨随机森林，并在PySpark中实现它以获得更好的结果。
- en: 10.3.1\. Understanding random forest classifiers
  id: totrans-882
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.1\. 理解随机森林分类器
- en: Random forest classifiers work by growing lots of different decision trees and
    then taking a poll of them. During the learning phase, they grow a diverse selection
    of trees by randomly selecting features to use. During the judgment phase, each
    tree classifies the observation based on its rules and votes for the classification
    that results from those rules—the random forest judges the observation to belong
    to the category with the most votes ([figure 10.14](#ch10fig14)).
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林分类器通过生长许多不同的决策树，然后对它们进行投票来工作。在学习阶段，它们通过随机选择要使用的特征来生长多样化的树。在判断阶段，每一棵树根据其规则对观测进行分类，并为那些规则产生的分类投票——随机森林根据得票最多的类别来判断观测属于哪个类别（[图10.14](#ch10fig14)）。
- en: Figure 10.14\. A random forest classifier relies on growing different decision
    trees, each seeded with different randomly selected features. Those trees then
    vote to classify new observations.
  id: totrans-884
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.14\. 随机森林分类器依赖于生长不同的决策树，每棵树都使用不同随机选择的特征作为种子。然后这些树对新的观测进行投票分类。
- en: '![](10fig14_alt.jpg)'
  id: totrans-885
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig14_alt.jpg)'
- en: 'As an example, consider a reduced version of the mushrooms dataset that only
    has seven features related to the mushroom’s caps and gills:'
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: 以蘑菇数据集的一个简化版本为例，它只包含与蘑菇帽和鳃相关的七个特征：
- en: Cap shape
  id: totrans-887
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 帽形状
- en: Cap surface
  id: totrans-888
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 帽表面
- en: Cap color
  id: totrans-889
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 帽颜色
- en: Gill attachment
  id: totrans-890
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鳃附着
- en: Gill spacing
  id: totrans-891
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鳃间距
- en: Gill size
  id: totrans-892
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鳃大小
- en: Gill color
  id: totrans-893
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鳃颜色
- en: A simple random forest might grow five classifiers from these. The first might
    contain cap shape, gill spacing, gill size, and gill color; the second might contain
    cap surface, gill attachment, gill color, and gill size; and so on ([table 10.2](#ch10table02)).
    Each tree has the features it can use randomly selected.
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的随机森林可能会从这些特征中生长出五个分类器。第一个可能包含帽形状、鳃间距、鳃大小和鳃颜色；第二个可能包含帽表面、鳃附着、鳃颜色和鳃大小；等等（[表10.2](#ch10table02)）。每一棵树都有它可以使用随机选择的特征。
- en: Table 10.2\. Five randomly seeded decision trees for an example random forest
  id: totrans-895
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表10.2\. 一个示例随机森林的五个随机种子决策树
- en: '| Tree 1 | Tree 2 | Tree 3 | Tree 4 | Tree 5 |'
  id: totrans-896
  prefs: []
  type: TYPE_TB
  zh: '| 树1 | 树2 | 树3 | 树4 | 树5 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-897
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Cap shape | Gill spacing | Gill size | Gill color | Cap surface |'
  id: totrans-898
  prefs: []
  type: TYPE_TB
  zh: '| 帽形状 | 鳃间距 | 鳃大小 | 鳃颜色 | 帽表面 |'
- en: '| Gill attachment | Gill color | Gill size | Cap surface | Cap color |'
  id: totrans-899
  prefs: []
  type: TYPE_TB
  zh: '| 鳃附着 | 鳃颜色 | 鳃大小 | 帽表面 | 帽颜色 |'
- en: '| Gill attachment | Gill color | Cap shape | Cap color | Gill attachment |'
  id: totrans-900
  prefs: []
  type: TYPE_TB
  zh: '| 鳃附着 | 鳃颜色 | 帽形状 | 帽颜色 | 鳃附着 |'
- en: '| Gill size | Cap color | Gill attachment | Gill spacing | Gill size |'
  id: totrans-901
  prefs: []
  type: TYPE_TB
  zh: '| 鳃大小 | 帽颜色 | 鳃附着 | 鳃间距 | 鳃大小 |'
- en: When we have a new observation we want to label, we can pass it to the random
    forest, and the random forest will poll each tree in it. For example, trees 1,
    2, and 4 might judge the observation to be edible, whereas trees 3 and 5 might
    say that it’s poisonous. Between the five of them, the vote is 3 to 2 in favor
    of edible. That would be the class that the random forest would ultimately judge
    the new observation to be.
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有一个新的观测需要标记时，我们可以将其传递给随机森林，随机森林将对它中的每一棵树进行投票。例如，树1、2和4可能判断观测是可食用的，而树3和5可能说它是有毒的。在这五棵树中，投票结果是3比2支持可食用。这将最终是随机森林判断新观测所属的类别。
- en: 'This process works because the randomization of features available to the decision
    trees makes random forests resilient to overfitting: a problem in machine learning
    where the algorithms disproportionately use one feature to make judgments. The
    improved resilience, high performance, and overall versatility of random forest
    models—which can be used for any type of judgment problem: binary classification,
    multiclass classification, and regression—makes random forest models a popular
    machine learning tool.'
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程之所以有效，是因为决策树可用的特征随机化使随机森林对过拟合具有弹性：这是机器学习中算法不成比例地使用一个特征进行判断的问题。随机森林模型改进的弹性、高性能和整体通用性——可用于任何类型的判断问题：二元分类、多类分类和回归——使随机森林模型成为流行的机器学习工具。
- en: 10.3.2\. Implementing a random forest classifier
  id: totrans-904
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.2\. 实现随机森林分类器
- en: 'To build our random forest classifier, we’ll start off the same way we started
    with our decision tree: by bringing in the data and arranging it into a `label`
    column and a `features` column. Unlike our previous attempt with decision trees,
    this time we won’t make any assumptions about which features will be useful and
    which won’t be. This time, we’ll select all the features and let the random forest
    sort it out.'
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建我们的随机森林分类器，我们将从与我们的决策树相同的方式开始：引入数据并将其排列成`标签`列和`特征`列。与之前我们使用决策树时的尝试不同，这次我们不会对哪些特征会有用以及哪些不会有用做出任何假设。这次，我们将选择所有特征，并让随机森林来排序。
- en: To use all the features, we’ll use the same reduce strategy as before. This
    time, though, instead of passing in a list where we name every feature we want,
    we’ll create the list from the `DataFrame`’s columns attribute and pop the label
    off, as shown in [listing 10.5](#ch10ex05) We’ll also need to construct a list
    that has the new labels. To do this, I like to use a list comprehension that prepends
    the feature indicator to the feature name.
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用所有特征，我们将使用之前相同的减少策略。不过，这次，我们不是传递一个包含我们想要命名的每个特征的列表，而是从`DataFrame`的列属性中创建列表，并将标签弹出，如[列表10.5](#ch10ex05)所示。我们还需要构建一个包含新标签的列表。为此，我喜欢使用一个列表推导，将特征指示符添加到特征名称前面。
- en: Listing 10.5\. Reading and preparing data for random forest classification
  id: totrans-907
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.5\. 读取和准备随机森林分类数据
- en: '[PRE45]'
  id: totrans-908
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '***1* Our categories will include all the columns in our DataFrame.**'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 我们的类别将包括DataFrame中的所有列。**'
- en: '***2* The only category we don’t want is the label, so we’ll pop that out.**'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 我们不想要的唯一类别是标签，所以我们会将其弹出。**'
- en: '***3* Transforms all these strings into indexes**'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将所有这些字符串转换为索引**'
- en: '***4* We can use a list comprehension to get a list of index names—we’ll need
    this to assemble the indexes.**'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 我们可以使用列表推导来获取索引名称列表——我们需要这些来组装索引。**'
- en: 'With the `DataFrame` in good shape, we’re ready to start building our random
    forest. We’ll build the random forest similarly to how we built the decision tree
    earlier in this chapter:'
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: 当`DataFrame`处于良好状态时，我们就准备好开始构建我们的随机森林了。我们将以与本章早期构建决策树相同的方式构建随机森林：
- en: First, we’ll import the `RandomForestClassifier` class.
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们将导入`RandomForestClassifier`类。
- en: Then, we’ll instantiate the class using the default settings.
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将使用默认设置实例化该类。
- en: 'But we’ll also do some things a little differently:'
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们也会做一些不同的事情：
- en: We’ll use a parameter grid to optimize hyperparameters.
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用参数网格来优化超参数。
- en: We’ll use a cross validator to ensure our results are more robust.
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用交叉验证器来确保我们的结果更加稳健。
- en: In the decision tree example, you may have noticed that we evaluated our decision
    tree on the same dataset that we learned it from. This is fine for getting used
    to writing PySpark machine learning code, but the results will not be reliable.
    To get a better assessment of how well our machine learners judge new observations,
    we should always cross-validate our models by testing them on data that we’ve
    held out from the learning process.
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树示例中，你可能已经注意到我们是在我们从中学到的同一个数据集上评估我们的决策树。这对于习惯于编写PySpark机器学习代码来说是不错的，但结果并不可靠。为了更好地评估我们的机器学习器对新观察结果的判断能力，我们应该始终通过在从学习过程中排除的数据上测试我们的模型来进行交叉验证。
- en: 'Two types of cross-validation are worth knowing about:'
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种交叉验证类型值得了解：
- en: K-fold cross-validation
  id: totrans-921
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K折交叉验证
- en: Train-test-evaluate validation
  id: totrans-922
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练-测试-评估验证
- en: As shown in [figure 10.15](#ch10fig15), in k-fold cross-validation we split
    the dataset up into K chunks, then we rotate through the chunks, considering one
    chunk the evaluation data and all the other chunks the test data. This process
    can be time-consuming if both K and your dataset are large, because you’ll end
    up training a machine learning model many times on a large dataset. Common values
    of K include 5, 10, 100, and the total number of observations in your dataset.
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图10.15](#ch10fig15)所示，在k折交叉验证中，我们将数据集分成K部分，然后我们旋转通过这些部分，考虑一个部分为评估数据，所有其他部分为测试数据。如果K和你的数据集都很大，这个过程可能会很耗时，因为你最终会在大型数据集上多次训练机器学习模型。K的常见值包括5、10、100以及你数据集中的观察总数。
- en: Figure 10.15\. K-fold cross-validation splits the data into K groups and then
    learns a model from all the other groups to judge the selected group.
  id: totrans-924
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.15。K折交叉验证将数据分成K组，然后从所有其他组中学习一个模型来评估选定的组。
- en: '![](10fig15_alt.jpg)'
  id: totrans-925
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig15_alt.jpg)'
- en: 'In train-test-evaluate validation, the dataset is split into three chunks:
    a large training chunk, a small testing chunk, and an even smaller evaluation
    chunk. The training chunk is used for training the model. The testing chunk is
    used during an iterative training cycle, as shown in [figure 10.16](#ch10fig16).
    Whenever we have an idea about how to improve the algorithm, we make the improvement,
    relearn from the training chunk, and test on the testing chunk. Then, when we’re
    happy with the model we have, we can judge the evaluation data and use that to
    assess our model. The trick here is to keep the evaluation set removed from the
    process as much as possible. If you can stick to rarely judging the evaluation
    chunk, train-test-evaluation may work for you. Otherwise, you may be better off
    using k-fold cross-validation. With the train-test-evaluate approach, it’s common
    to use 70% of the dataset as training data, 20% as testing data, and 10% as evaluation
    data.'
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练-测试-评估验证中，数据集被分成三个部分：一个大的训练部分，一个小型的测试部分，以及一个更小的评估部分。训练部分用于训练模型。测试部分在迭代训练周期中使用，如图10.16所示。[链接](https://wiki.example.org/feynmans_learning_method)。每当我们对如何改进算法有想法时，我们就进行改进，从训练部分重新学习，并在测试部分进行测试。然后，当我们对现有的模型满意时，我们可以判断评估数据，并使用这些数据来评估我们的模型。这里的技巧是尽可能将评估集从过程中分离出来。如果你能坚持很少判断评估部分，那么训练-测试-评估可能适合你。否则，你可能更适合使用k折交叉验证。在训练-测试-评估方法中，通常使用70%的数据集作为训练数据，20%作为测试数据，10%作为评估数据。
- en: Figure 10.16\. Train-test-evaluate validation splits the data into three chunks,
    two of which are used for iterative learning and testing. The remaining one is
    used rarely to evaluate the model.
  id: totrans-927
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.16。训练-测试-评估验证将数据分成三个部分，其中两个用于迭代学习和测试。剩余的一个很少用于评估模型。
- en: '![](10fig16_alt.jpg)'
  id: totrans-928
  prefs: []
  type: TYPE_IMG
  zh: '![图片](10fig16_alt.jpg)'
- en: To implement cross-validation in PySpark, we’ll use the `CrossValidator` class,
    which we can use to do k-fold cross-validation. The `CrossValidator` needs to
    be initialized with
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: 要在PySpark中实现交叉验证，我们将使用`CrossValidator`类，我们可以使用它来进行k折交叉验证。`CrossValidator`需要用以下内容初始化：
- en: '***An estimator—*** The classifier we want to use'
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***估计器——*** 我们想要使用的分类器'
- en: '***A parameter estimator—*** A `ParamGridBuilder` object'
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***参数估计器——*** 一个`ParamGridBuilder`对象'
- en: '***An evaluator—*** We’ll use the `BinaryClassificationEvaluator` we used in
    our decision tree example. I like to do 10-fold validation unless I have a compelling
    reason not to—we also pass this choice into the `CrossValidator` class as we initialize
    it.'
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***评估器——*** 我们将使用我们在决策树示例中使用的`BinaryClassificationEvaluator`。除非有充分的理由不这样做，否则我喜欢进行10折验证——我们也在初始化时将这个选择传递给`CrossValidator`类。'
- en: '[Listing 10.6](#ch10ex06) shows the training of the random forest classifier.
    You’ll notice that instead of using the classifier directly to fit the data, we
    pass the `RandomForestClassifier` to the `CrossValidator` object and use the `CrossValidator`’s
    `.fit` method. From there, though, the evaluation process is similar. We can find
    the area under the operating receiver characteristic curve using the `BinaryClassificationEvaluator`.
    Lastly, we can print the best model from our cross-validation attempts and see
    what rules we ended up with.'
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表10.6](#ch10ex06)展示了随机森林分类器的训练过程。你会注意到，我们不是直接使用分类器来拟合数据，而是将`RandomForestClassifier`传递给`CrossValidator`对象，并使用`CrossValidator`的`.fit`方法。然而，从那里开始，评估过程是相似的。我们可以使用`BinaryClassificationEvaluator`找到操作接收器特性曲线下的面积。最后，我们可以打印出交叉验证尝试中的最佳模型，并查看我们最终得到的规则。'
- en: Listing 10.6\. A robust random forest model using PySpark
  id: totrans-934
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.6。使用PySpark的鲁棒随机森林模型
- en: '[PRE46]'
  id: totrans-935
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '***1* Creates an instance of our desired classifier**'
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 创建我们所需的分类器实例**'
- en: '***2* Creates a parameter grid search over some parameters**'
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 在一些参数上创建参数网格搜索**'
- en: '***3* Initializes the cross-validator to train several models**'
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 初始化交叉验证器以训练多个模型**'
- en: '***4* Fits the models**'
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 训练模型**'
- en: '***5* Prints the best model**'
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 打印最佳模型**'
- en: In these rules, we can see the different trees that make up the decision forest.
    In your own output, you’ll notice that some trees have the same rules—this must
    be a good way to make judgments about mushrooms!
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些规则中，我们可以看到构成决策森林的不同树。在你的输出中，你会注意到一些树有相同的规则——这肯定是一种判断蘑菇的好方法！
- en: '[PRE47]'
  id: totrans-942
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Summary
  id: totrans-943
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: PySpark’s SQL module has a tabular `DataFrame` structure that provides table-like
    features, such as column names, on top of `RDD`-powered parallelization.
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 的 SQL 模块具有表格 `DataFrame` 结构，它提供了类似于表的功能，例如列名，同时基于 `RDD` 的并行化。
- en: PySpark has a machine learning library that includes tools for every step of
    the machine learning pipeline, including data ingestion, data preparing, machine
    learning, cross-validation, and model evaluation.
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 拥有一个机器学习库，其中包括机器学习管道每个步骤的工具，包括数据摄入、数据准备、机器学习、交叉验证和模型评估。
- en: Machine learners in PySpark are represented as classes that learn using the
    `.fit` method. They return a model object, which can judge data using the `.transform`
    method.
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 中的机器学习器表示为使用 `.fit` 方法进行学习的类。它们返回一个模型对象，可以使用 `.transform` 方法对数据进行判断。
- en: We can use PySpark’s feature creation classes—such as `StringIndexer` and `VectorAssembler`—to
    format `DataFrame`s for machine learning.
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用 PySpark 的特征创建类——例如 `StringIndexer` 和 `VectorAssembler`——来格式化 `DataFrame`s
    以进行机器学习。
- en: The feature creation classes are `Transformer`-class objects, and their methods
    return new `DataFrame`s, rather than transforming them in place.
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征创建类是 `Transformer` 类对象，它们的方法返回新的 `DataFrame`s，而不是就地转换它们。
