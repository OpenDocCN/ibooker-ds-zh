- en: Chapter 6\. Optimizing for Self-Service
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章。为自助服务进行优化
- en: The power of data can be realized only if decision makers can base their actions
    on the data. In the past, business users had to wait for specialists to prepare
    data and run analyses. This effectively prevented many worthwhile queries from
    being run, and routinely led to delays, mistakes, and misinterpretations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的力量只有在决策者能够基于数据采取行动时才能实现。过去，业务用户必须等待专家准备数据和运行分析。这有效地阻止了许多有价值的查询进行，并经常导致延迟、错误和误解。
- en: I once spoke to a doctor from a leading medical research hospital who had used
    a week’s vacation to take a SQL class. He explained that he was concerned about
    the efficacy of a specific medical treatment protocol, but he couldn’t change
    the protocol without proving that the changes were safe. He’d spent a year trying
    to explain what he wanted to IT—waiting weeks to receive the data sets, realizing
    that they weren’t what he was looking for, requesting more data, waiting for it,
    then investing more time only to discover that it wasn’t what he needed either.
    He eventually became so frustrated that he took the SQL class so he could explore
    the data himself. Within two weeks of applying his newly acquired knowledge, he
    was able to find the data he needed to improve the treatment protocol. This is
    just one of many stories that showcase the value of self-service and the amazing
    breakthroughs that analysts can make if they are able to explore the data directly.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾与一位来自领先医学研究医院的医生交谈过，他利用了一周的假期学习了SQL课程。他解释说，他对特定的医疗治疗方案的效力感到担忧，但在没有证明变更安全的情况下，他无法更改方案。他花了一年的时间试图向IT解释他想要的内容——等待几周以接收数据集，意识到它们不是他要找的，请求更多数据，再等待，然后投入更多时间，最终发现这些数据也不是他需要的。最终，他对此感到非常沮丧，于是决定学习SQL课程，以便自己探索数据。在应用了他新获得的知识的两周内，他能够找到所需的数据，以改善治疗方案。这只是展示自助服务价值以及分析师能够取得的惊人突破的众多故事之一。
- en: This chapter delves into how an organization has to reconsider its ways of collecting,
    labeling, and sharing data in order to achieve the self-service model required
    to empower business users. We’ll explore issues such as helping users find useful
    data in the data lake, establishing trust that the data is correct and valuable,
    and helping users do their own analyses. Without establishing this trust in the
    data, business analysts will be reluctant to use the available data to make decisions,
    or may end up making the wrong decisions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨了一个组织如何重新考虑其收集、标记和共享数据的方式，以实现赋予业务用户所需的自助服务模型。我们将探讨诸如帮助用户在数据湖中找到有用数据、建立数据正确性和价值的信任以及帮助用户进行自己的分析等问题。如果没有建立对数据的这种信任，业务分析师将不愿意使用现有数据进行决策，或者最终做出错误的决策。
- en: The Beginnings of Self-Service
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自助服务的开端
- en: In the past, the rule of thumb was that the first set of requirements for the
    data warehouse were always wrong—this was a well-known Data Warehouse 1.0 problem.
    To start the journey to a functioning data warehouse, IT had to build the first-generation
    schema, along with the accompanying reports. This gave users something tangible
    to figure out what they really needed so IT could produce “real” requirements.
    Except for a few power users, most analysts did not have the skills or the tools
    to work with the data directly.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 过去的经验法则是数据仓库的第一组需求总是错误的——这是众所周知的数据仓库1.0问题。为了开始通向一个正常运作的数据仓库的旅程，IT必须建立第一代架构，以及相应的报告。这为用户提供了一个具体的东西，以弄清他们真正需要什么，从而使IT能够生成“真正”的需求。除了少数几个高级用户外，大多数分析师没有直接处理数据的技能或工具。
- en: The lengthy response times from IT that so many users experienced, like the
    doctor in my previous story, arose from the growing interest in and number of
    requests for the data. We’ve witnessed an explosion in the amount of data generated
    by applications and acquired from external providers, and a concomitant increase
    in business users’ expectations to be able to leverage that data at near-real-time
    speeds. This simultaneous increase in data volume and user expectations made it
    impossible for IT to keep up.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: IT给用户带来的长时间响应，像我之前故事中的那位医生一样，源于对数据日益增长的兴趣和请求的数量。我们目睹了应用程序生成的数据量和从外部提供者获取的数据量的激增，以及业务用户希望能够在几乎实时的速度下利用这些数据的期望的同时增加。这两者同时增加的数据量和用户期望使得IT难以跟上步伐。
- en: However, the new generation of analysts, subject matter experts (SMEs), and
    decision makers are more technical and computer savvy than any previous generation,
    as they’ve grown up in the digital age and most have had some exposure to programming
    as part of their high school or college curriculums. This generation of users
    would rather have “self-service” access to the data; they want to find, understand,
    and use the data themselves. In addition, the cloud makes it possible for the
    business users to sidestep their IT departments and provision the infrastructure
    needed to run their analyses themselves.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，新一代的分析师、主题专家（SME）和决策者比以往任何一代都更懂技术和计算机，因为他们在数字时代成长，并且大多数人在高中或大学课程中接触过编程。这一代用户更希望“自助服务”地访问数据；他们想要自己找到、理解和使用数据。此外，云技术使得业务用户能够绕过IT部门，自行提供运行分析所需的基础设施。
- en: In this chapter we will contrast the old approach, where IT departments provided
    analytic services for business analysts who performed analyses for business users,
    with the new self-service approach, where business users expect to be able to
    do their own analyses. To avoid unnecessary complexity, I’ll refer to whoever
    is performing the analyses as analysts—this may include anyone from people with
    formal analyst titles, to business users doing their own analyses, to data scientists
    doing advanced analytics.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将对比旧的方法和新的自助服务方法。在旧方法中，IT部门为业务分析师提供分析服务，他们为业务用户执行分析。而现在，业务用户希望能够自行进行分析，避免不必要的复杂性，我将称执行分析的人为分析师——这可能包括任何人，从拥有正式分析师职称的人员，到自行进行分析的业务用户，再到进行高级分析的数据科学家。
- en: 'While the previous ETL versions of data modeling and business intelligence
    (BI) tools were created for programmers and architects, the new generation of
    tools are designed for power users to access directly. In the past, specialists
    did most of the work:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然以前的ETL版本的数据建模和商业智能（BI）工具是为程序员和架构师设计的，新一代工具则专为高级用户直接访问而设计。过去，专家大多数工作是：
- en: Data modelers designed the schema for the data warehouse.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据建模师为数据仓库设计了模式。
- en: ETL developers created ETL jobs to extract data from source applications, transform
    it, and load it into the data warehouse.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ETL开发人员创建了ETL作业，从源应用程序中提取数据，转换数据，并将其加载到数据仓库中。
- en: Data quality analysts created validation jobs to check the correctness of the
    data.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据质量分析师创建了验证作业来检查数据的正确性。
- en: BI developers created reports and online analytical processing (OLAP) cubes
    that the users could slice and dice.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BI开发人员创建报告和在线分析处理（OLAP）立方体，用户可以对其进行切片和切块。
- en: Metadata architects created business glossaries to try to capture the meaning
    of various data elements and metadata repositories to try to keep track of the
    data in the enterprise.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据架构师创建了业务术语词汇表，试图捕捉各种数据元素的含义，并创建了元数据存储库，试图跟踪企业中的数据。
- en: The only analysis that an analyst could perform was through semantic layers
    such as Business Objects Universes that allowed end users to combine data by using
    higher-level prebuilt constructs (the business objects, such as *customer* and
    *order*), while the complexity of the actual data manipulations was hidden. For
    example, a user could add business objects for both a customer and an order to
    a report and would be able to see the orders for each customer. While very convenient,
    this approach was restricted to whatever business objects the IT staff created.
    Any changes required multi-person reviews and approvals that sometimes took months.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 分析师唯一可以执行的分析是通过语义层，例如Business Objects Universes，允许最终用户使用高级预构建结构（如*客户*和*订单*）来组合数据，同时隐藏了实际数据操作的复杂性。例如，用户可以将客户和订单的业务对象添加到报告中，并能够查看每个客户的订单。虽然非常便利，但这种方法受制于IT工作人员创建的任何业务对象。任何变更都需要多人审核和批准，有时需要数月时间。
- en: The data self-service revolution has upended this brittle situation. Self-service
    data exploration and visualization tools such as Tableau, Power BI, and Qlik,
    which allow analysts to visually explore data and work with it directly to create
    charts, are rapidly replacing traditional BI offerings. Analysts are now using
    self-service data prep tools like Excel, Trifacta, and Paxata to transform data
    into whatever shape they need.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 自助服务数据探索和可视化工具（如Tableau、Power BI和Qlik）已颠覆了这种脆弱的情况。分析师现在使用自助服务数据准备工具（如Excel、Trifacta和Paxata）直接将数据转换为所需的形式。
- en: Furthermore, self-service catalog tools such as Waterline Data and IBM Watson
    Catalog are now allowing the analysts to annotate, find, and understand data sets
    themselves without having to request them from IT.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，自助服务目录工具（如Waterline Data和IBM Watson Catalog）现在允许分析师自行注释、查找和理解数据集，无需从IT请求。
- en: '[Figure 6-1](#enabling_analysts_and_reducing_the_load) illustrates how analysts’
    reliance on IT, and the consequent load on IT, is significantly reduced in the
    self-service analytics world. The self-service tools available today were almost
    all developed with the analyst in mind as the target user, and they often do not
    require any IT involvement to deploy and use (one exception is the catalog tools
    that straddle IT and business, which are usually administered by IT but used by
    analysts). The underlying data infrastructure remains squarely in the hands of
    IT, however, which keeps the data stable.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-1](#enabling_analysts_and_reducing_the_load)展示了在自助服务分析环境中，分析师依赖IT的程度以及对IT的负荷显著减少的情况。今天可用的自助服务工具几乎都是以分析师为目标用户开发的，并且通常不需要IT参与部署和使用（其中一个例外是跨越IT和业务的目录工具，通常由IT管理但由分析师使用）。然而，底层数据基础设施仍然完全掌握在IT手中，这保持了数据的稳定性。'
- en: '![Enabling analysts and reducing the load on IT with self-service analytics](Images/ebdl_0601.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![通过自助服务分析减轻IT负担并支持分析师](Images/ebdl_0601.png)'
- en: Figure 6-1\. Enabling analysts and reducing the load on IT with self-service
    analytics
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-1\. 通过自助服务分析减轻IT负担并支持分析师
- en: Business Analysts
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 业务分析师们
- en: Unfortunately, most enterprises today don’t really support the self-service
    model, because the data warehouses are not designed to handle a large volume of
    ad hoc queries and analytics. As we discussed earlier, they are carefully tuned
    to support mission-critical production reports and analytics. Allowing hundreds
    or even thousands of users to issue random and sometimes ill-formed queries would
    interfere with those functions. Furthermore, analytics often require combining
    data in the data warehouse with other data sets, but adding anything to a data
    warehouse is an expensive and lengthy process that requires a lot of design work,
    architectural and security approvals, and ETL development.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，今天大多数企业并不真正支持自助服务模型，因为数据仓库并未设计用于处理大量的临时查询和分析。正如我们之前讨论的那样，它们都精心调整以支持关键生产报告和分析。允许数百甚至数千用户发布随机且有时形式不完整的查询将干扰这些功能。此外，分析通常需要将数据仓库中的数据与其他数据集结合，但向数据仓库添加任何内容都是一个昂贵且漫长的过程，需要大量的设计工作、架构和安全批准以及ETL开发。
- en: Therefore, one of the main purposes of a data lake in many enterprises is to
    create an environment where such self-service is possible. To understand self-service,
    we need to examine the workflow of a typical business analyst ([Figure 6-2](#the_business_analystapostrophes_workflow)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在许多企业中，数据湖的主要目的之一是创建一个能够实现自助服务的环境。要理解自助服务，我们需要审视典型业务分析师的工作流程（[图 6-2](#the_business_analystapostrophes_workflow)）。
- en: '![The business analyst’s workflow](Images/ebdl_0602.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![业务分析师的工作流程](Images/ebdl_0602.png)'
- en: Figure 6-2\. The business analyst’s workflow
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-2\. 业务分析师的工作流程
- en: As we saw in [Chapter 1](ch01.xhtml#introduction_to_data_lakes), first the analyst
    has to *find and understand* the required data. The next step is to *provision*
    the data—that is, obtain it in a usable form and format. Next, the data needs
    to be *prepared* for analysis. This may involve combining, filtering, aggregating,
    fixing data quality problems, and so on. Once the data is in the correct shape,
    the analyst can *analyze* it using data discovery and visualization tools.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如同我们在[第一章](ch01.xhtml#introduction_to_data_lakes)中看到的那样，分析师首先必须*找到并理解*所需的数据。接下来的步骤是*准备*数据，即以可用的形式和格式获取它。然后，数据需要*准备*进行分析。这可能涉及合并、过滤、聚合、修复数据质量问题等等。一旦数据形状正确，分析师可以使用数据发现和可视化工具*分析*它。
- en: We’ll look at the first three steps in this workflow in more detail here, along
    with the important issue of establishing trust in the data that’s identified.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在此处详细查看此工作流程的前三个步骤，以及识别数据中建立信任的重要问题。
- en: Finding and Understanding Data—Documenting the Enterprise
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 找到并理解数据——企业文档化
- en: Analysts want to search for data using business terms that they are familiar
    with (for example, “I need customer demographics including annual spend, age,
    and location”), whereas data sets and fields are often exposed through cryptic
    technical names. This makes it very challenging for analysts to find and understand
    data. To bridge that gap, many enterprises are investing in data catalogs that
    associate business terms or tags with data sets and their fields, allowing analysts
    to quickly find data sets using such tags and to understand these data sets by
    looking at the tags associated with each field. Usually, multiple data sets contain
    the data that analysts need, so the next step becomes selecting which one to use.
    Analysts usually include judgments about how complete, accurate, and trustworthy
    the data is when making their choices (we’ll consider the issue of establishing
    trust in the next section).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 分析师希望使用他们熟悉的业务术语来搜索数据（例如，“我需要包括年度支出、年龄和位置在内的客户人口统计数据”），而数据集和字段通常通过晦涩的技术名称公开。这使得分析师在寻找和理解数据方面面临巨大挑战。为了弥合这一差距，许多企业正在投资于数据目录，将业务术语或标签与数据集及其字段关联起来，使分析师能够快速使用这些标签找到数据集，并通过查看与每个字段关联的标签来理解这些数据集。通常，多个数据集包含分析师需要的数据，因此下一步是选择使用哪一个。在做出选择时，分析师通常会考虑数据的完整性、准确性和可信度（我们将在下一节考虑建立对数据的信任问题）。
- en: While catalogs are critical to enabling self-service for business analysts,
    they are challenging to build and maintain. This is because in most enterprises
    the knowledge about where data is, which data sets to use for what, and what data
    means is locked in people’s heads—this is commonly referred to as “tribal knowledge.”
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管目录对于为业务分析师提供自助服务至关重要，但建立和维护它们也具有挑战性。这是因为在大多数企业中，关于数据在何处、哪些数据集用于什么目的以及数据意味着什么的知识都锁在人们的脑中——这通常被称为“部落知识”。
- en: Without a catalog, in order to find a data set to use for a specific problem,
    analysts have to ask around until they find someone—if they’re lucky, a subject
    matter expert—who can point them to the right data. SMEs can be difficult to find,
    though, so the analyst may instead run into someone who tells them about a data
    set that they used for a similar problem, and will then use that data set without
    really understanding what was done to it or where it came from.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 没有目录的情况下，为了找到用于特定问题的数据集，分析师必须四处打听，直到找到可以指导他们的人员——如果他们幸运的话，可能是一个主题专家——帮助他们找到正确的数据。尽管如此，找到SME（主题专家）可能很困难，所以分析师可能会遇到告诉他们使用了类似问题的数据集的人，然后在不真正了解它们是如何处理或它们来自何处的情况下使用该数据集。
- en: This is sort of like playing Russian Roulette with your project—it’s akin to
    asking around if anyone knows a doctor who might know anything about the pain
    in your right side, running into someone who says that they had a pain in their
    right side and took a particular medicine, and then taking some of their medicine.
    Not only may their medicine not be right for you, but you have no idea what it
    is, where it came from, or how old it is. Even if you find a doctor who claims
    to know something about your pain, you have no idea if this doctor is qualified
    to diagnose it. Needless to say, this is an incredibly painful (pun intended),
    time-consuming, and error-prone process, whether applied to physical or data doctoring.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点像用你的项目来玩俄罗斯轮盘赌——这就像询问周围是否有人知道可能了解你右侧疼痛的医生，然后遇到有人说他们右侧有疼痛并服用了特定的药物，然后你服用了他们的药物。不仅可能他们的药物不适合你，而且你不知道这是什么、它来自哪里，或者它有多老。即使找到了一个声称知道你疼痛情况的医生，你也不知道这个医生是否有资格来诊断它。不用说，无论是应用于身体还是数据处理，这都是一个非常痛苦（玩笑意味），耗时且容易出错的过程。
- en: One thing we have going for us in this age of Google, Yelp, and Wikipedia is
    that we are used to capturing knowledge through crowdsourcing. The same approach
    has been applied by enterprises to crowdsourcing tribal knowledge about data from
    analysts, so the information in their heads can be captured in glossaries and
    metadata repositories. These efforts are time-consuming, however, and run into
    two obstacles.  First, only the most important data gets documented—typically
    called *critical data elements* (CDEs). These usually include descriptive fields
    found in master data lists, such as customer and product attributes, and core
    transaction fields, such as order IDs, dates, and amounts. Second, even for CDEs,
    the repositories quickly become out of date as data sets, business processes,
    and rules change and technology evolves.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个Google、Yelp和Wikipedia时代，我们有一个优势，那就是习惯通过众包来获取知识。企业也已经开始将类似的方法应用于分析师从头脑中获取数据的众包部落知识，这样他们头脑中的信息可以被记录在词汇表和元数据存储库中。然而，这些努力是耗时的，并且遇到两个障碍。首先，只有最重要的数据得到了记录——通常称为*关键数据元素*（CDEs）。这些通常包括在主数据列表中找到的描述字段，例如客户和产品属性，以及核心交易字段，例如订单ID、日期和金额。其次，即使对于关键数据元素，存储库也很快变得过时，因为数据集、业务流程和规则的变化以及技术的进步。
- en: 'The best practices for overcoming these challenges are:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 克服这些挑战的最佳实践是：
- en: Crowdsourcing all tribal knowledge and making it available to everyone
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有部落知识进行众包，并向所有人提供
- en: Automating annotation of data sets
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动标注数据集
- en: Ten years ago, analysis was typically performed by dedicated staff who spent
    all their time working with data, so knowledge about the data was concentrated
    in analytics and data architecture teams. In the enterprise today, analysis is
    done by everyone who needs to make decisions. Combined with proliferation of data,
    this makes it more difficult to find subject matter experts who are knowledgeable
    about the data. SME is usually not a full-time job or an official role. Some enterprises
    have developed a formal data stewardship framework where people are assigned full-time
    or, more often, part-time responsibility for stewarding data—that is, making sure
    it is used appropriately, complies with governmental and internal regulations,
    and is kept at high levels of quality. However, most SMEs and even most official
    data stewards do not get compensated for helping other teams, yet are still expected
    to complete their primary work. The SMEs frequently resent this role and don’t
    really enjoy explaining the same material over and over to different groups. Analysts
    have been known to bribe SMEs with lunches or other incentives in exchange for
    their time and knowledge. But at the end of such a lunch, the analyst may have
    only part of the knowledge they need, and, if they’re lucky, have been given pointers
    to other SMEs they can consult for the remaining knowledge. Which leads to more
    expensive lunches!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 十年前，分析通常由专门的员工执行，他们将所有时间都花在与数据打交道，因此关于数据的知识集中在分析和数据架构团队中。在今天的企业中，决策需要的人员都会进行分析。再加上数据的激增，这使得更难以找到了解数据的专业人员。SME通常不是全职工作或正式角色。一些企业已经开发了正式的数据管理框架，在这些框架中，人们被分配全职或更常见的兼职责任来管理数据，即确保其被适当使用，符合政府和内部规定，并保持高质量水平。然而，大多数SME甚至大多数官方数据管理者并没有得到帮助其他团队的报酬，但仍然期望完成他们的主要工作。SME经常对这种角色感到不满，并不喜欢一遍又一遍地向不同的团队解释同样的材料。分析师已经知道通过午餐或其他激励手段贿赂SMEs以换取他们的时间和知识。但在这样的午餐结束时，分析师可能只得到了他们所需知识的一部分，并且如果幸运的话，可能已经得到了指向其他SME的线索。这导致了更昂贵的午餐！
- en: 'Since enterprises are beginning to recognize the value of SMEs and their knowledge,
    they are experimenting with various ways of incentivizing crowdsourcing. Some
    of the best practices include:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于企业开始认识到中小企业（SMEs）及其知识的价值，它们正在尝试各种激励众包的方法。一些最佳实践包括：
- en: Making it as easy and efficient as possible for SMEs to document their knowledge.
    Commonly, this is achieved by creating glossaries or taxonomies of terms and letting
    SMEs tag data sets with those terms instead to having to write elaborate descriptions
    for each field.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能简化和高效化SMEs记录知识的过程。通常，通过创建术语表或术语分类体系来实现这一目标，并让SMEs使用这些术语标记数据集，而不是为每个字段撰写详细的描述。
- en: Enhancing tags further through a practice called “folksonomy,” which allows
    SMEs to use the terms that they are familiar with as tags instead of forcing them
    to learn an imposed taxonomy (whether homegrown or industry-standard, like FIBO
    for financial services). For example, an American analyst may look for a “first
    name” and “last name,” while a European one looks for “given name” and “family
    name.” While these are simple synonyms, sometimes the terms have additional connotations
    and semantics. For example, one business may consider “due date” and “default
    date” the same, while another business may have a “grace period,” such that “default
    date” is “due date plus grace period.” The variety and complexity introduced by
    different geographies, businesses, functions, and acquisitions are quite astounding.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进一步通过“民间分类法”（folksonomy）提升标签的效果，允许SMEs使用他们熟悉的术语作为标签，而不是强迫他们学习强加的分类体系（无论是自行开发的还是行业标准，如金融服务的FIBO）。例如，美国分析师可能寻找“first
    name”和“last name”，而欧洲分析师则寻找“given name”和“family name”。虽然这些只是简单的同义词，有时术语还具有额外的内涵和语义。例如，一个企业可能认为“due
    date”和“default date”是相同的，而另一个企业可能有一个“宽限期”，因此“default date”就是“due date plus grace
    period”。不同地理位置、企业、功能和收购所引入的多样性和复杂性令人震惊。
- en: Encouraging SMEs to share their knowledge by giving public recognition for their
    work—from gamification and badges to simple recognition by projects that they’ve
    helped.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鼓励SMEs分享他们的知识，通过公开认可他们的工作，从游戏化和徽章到项目简单的认可，表明他们已经帮助过的项目。
- en: Making it easy to find out whom to ask about which data sets. This not only
    helps analysts find the right people to ask, but encourages SMEs to document their
    data sets, so they do not have to explain them to every new user. For example,
    when Google implemented a searchable catalog where users could find SMEs for each
    data set, they found that the frequently used data sets got documented quickly
    as SMEs got tired of responding to questions.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方便查找该询问哪些数据集的人。这不仅帮助分析师找到正确的联系人，还鼓励SMEs记录他们的数据集，从而避免每次都需要向新用户解释。例如，当Google实施了可搜索的目录，用户可以找到每个数据集的SMEs时，他们发现经常使用的数据集很快得到了记录，因为SMEs厌倦了回答问题。
- en: Making it easy for the analysts who talk to SMEs to document what they’ve learned
    as tags and annotations, so they can retain it for the future and avoid bothering
    SMEs again. This is probably the most effective technique—in this way, knowledge
    quickly spreads and becomes institutionalized.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过让与SMEs交谈的分析师轻松记录他们学到的东西作为标签和注释，以便将来保留并避免再次打扰SMEs，使知识迅速传播并得到制度化，这可能是最有效的技术。
- en: While crowdsourcing SMEs’ knowledge is an important step toward self-service,
    the sheer volume of data in the enterprise makes it prohibitive to manually document
    everything. As a consequence, often only a few well-known and frequently used
    data sets become well documented, while the majority of data remains dark. This
    also causes problems with new data sets, which may not get documented right away
    and thus may not be found by analysts.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管众包SMEs的知识是迈向自助服务的重要一步，但企业中的数据量庞大，手动记录所有内容是不现实的。因此，通常只有少数几个广为人知且经常使用的数据集得到充分记录，而大多数数据则保持在黑暗之中。这也导致了新数据集的问题，可能不会立即得到记录，因此分析师可能无法找到这些数据。
- en: The answer to this problem is automation. New tools effectively combine crowdsourcing
    and automation to do “automated data discovery”—the automated tagging and annotation
    of data sets based on the tags provided by SMEs and analysts. These tools leverage
    artificial intelligence (AI) and machine learning to identify and auto-tag elements
    in dark data sets, so analysts can find and use them. Waterline Data’s Smart Data
    Catalog and IBM’s Watson Data Catalog are good examples of this approach. Catalogs
    are explored in more detail in [Chapter 8](ch08.xhtml#cataloging_the_data_lake).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的答案是自动化。新工具有效地结合了众包和自动化，进行“自动化数据发现”——基于SMEs和分析师提供的标签对数据集进行自动标记和注释。这些工具利用人工智能（AI）和机器学习识别和自动标记黑暗数据集中的元素，以便分析师可以找到并使用它们。Waterline
    Data的智能数据目录和IBM的Watson数据目录就是这种方法的良好示例。目录在第8章中有更详细的探讨，见[ch08.xhtml#cataloging_the_data_lake](ch08.xhtml#cataloging_the_data_lake)。
- en: Establishing Trust
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建立信任
- en: 'Once an analyst finds the pertinent data set, the next question becomes whether
    the data can be trusted. While analysts sometimes have the luxury of access to
    clean, trusted, curated data sets, more often than not they have to independently
    ascertain whether they can trust the data. Trust is usually based on three pillars:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦分析师找到相关的数据集，下一个问题是数据是否可信。虽然分析师有时可以访问干净、可信、经过策划的数据集，但更多情况下他们必须独立判断是否可以信任这些数据。信任通常基于三个支柱：
- en: Data quality—how complete and clean the data set is
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据质量—数据集的完整性和清洁度如何
- en: Lineage (aka provenance)—where the data came from
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lineage（又称来源）—数据的来源
- en: Stewardship—who created the data set, and why
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理—谁创建了数据集，以及为什么创建
- en: Data quality
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据质量
- en: 'Data quality is a wide and complex topic. In practice, quality can be defined
    as compliance of data to policies, which can range from simple (e.g., the customer
    name field should never be empty) to complex (e.g., sales tax must be correctly
    calculated based on purchase location). The most common data quality rules are:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量是一个广泛而复杂的主题。在实践中，质量可以定义为数据符合政策的程度，这些政策可以从简单（例如，客户姓名字段不应为空）到复杂（例如，根据购买地点正确计算销售税）。最常见的数据质量规则包括：
- en: Completeness
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 完整性
- en: The field is not empty.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 该字段不为空。
- en: Data type
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 数据类型
- en: The field is of the correct type (for example, age is a number).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 该字段为正确类型（例如，年龄是一个数字）。
- en: Range
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 范围
- en: The field is in a specified range (for example, age is between 0 and 125).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 该字段位于指定范围内（例如，年龄在0至125之间）。
- en: Format
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 格式
- en: The field has a specific format (for example, a US postal code is composed of
    either five digits, nine digits, or five digits followed by a dash and four digits).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该字段具有特定格式（例如，美国邮政编码由五位数字、九位数字或五位数字后跟连字符和四位数字组成）。
- en: Cardinality
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 基数
- en: The field has a specific number of unique values. (For example, if a US states
    field has more than 50 unique values, we know there is a problem. We may still
    not know whether every value is a legal state name, but if we already have every
    legal state name represented, checking the cardinality is enough of a sanity check
    to catch any illegal names because they will push the number of values above 50.)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 该字段具有特定数量的唯一值。（例如，如果一个美国州名字段有超过50个唯一值，我们就知道有问题。我们可能还不知道每个值是否都是合法的州名，但如果我们已经有了每个合法的州名的表示，检查基数就足以进行健全检查，以捕捉任何非法名称，因为它们会将值的数量推高到50以上。）
- en: Selectivity
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 选择性
- en: The values for the field are unique (for example, customer IDs should be unique
    in a customer list).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 该字段的值是唯一的（例如，客户ID在客户列表中应是唯一的）。
- en: Referential integrity
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 参照完整性
- en: The values for the field are in the reference value set. (For example, all the
    customer status codes are legal, and each customer ID in the order list refers
    to one of the customers in the customer list. While for some values, like states,
    we may get away with a cardinality check, customer status codes may have significant
    implications for how we treat the customer, what we charge, and so on, so when
    making sure each customer has a legitimate status code it’s important to check
    every value.)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 该字段的值位于参考值集中。（例如，所有客户状态代码都是合法的，订单列表中的每个客户ID都指向客户列表中的一个客户。对于某些值，如州名，我们可能可以通过基数检查来进行检查，但客户状态代码可能会对我们如何对待客户、收取费用等方面产生重大影响，因此在确保每个客户具有合法状态代码时，检查每个值非常重要。）
- en: The most common way to check data quality is called *data profiling*. This approach
    involves reading the data in every field and calculating metrics such as the number
    of empty fields (completeness), number of unique values (cardinality), and percentage
    of unique values (selectivity), as well as checking the data type, range, and
    format and performing referential integrity checks.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 检查数据质量的最常见方法称为*数据概要分析*。这种方法涉及读取每个字段中的数据并计算指标，例如空字段数（完整性）、唯一值数（基数）、唯一值百分比（选择性），以及检查数据类型、范围和格式，并执行参照完整性检查。
- en: In addition to basic data profiling, custom rules can be defined to validate
    specific aspects of the data. The advantage of profiling is that it can be done
    automatically and universally for all fields, then reviewed by analysts who are
    considering whether to use a data set to ascertain quality levels. Custom rules,
    on the other hand, have to be manually designed, implemented, and applied to the
    data set in question.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基本数据概要分析外，还可以定义自定义规则来验证数据的特定方面。概要分析的优点在于可以自动和普遍地对所有字段进行，然后由考虑是否使用数据集来确定质量水平的分析员进行审查。另一方面，自定义规则必须手动设计、实施并应用于相关数据集。
- en: Lineage (provenance)
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 血统（来源）
- en: While data quality checks tell the analysts how good the data is, lineage tells
    them where the data came from. For example, customer data from a CRM system is
    more trustworthy than a customer list from a specialized data mart because the
    former is a system of record for customer data, where the latter could be a subset
    of customers from a certain geography or demographic and might contain modified
    or out-of-date customer data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据质量检查告诉分析员数据的好坏程度，但数据血统告诉他们数据的来源。例如，来自CRM系统的客户数据比来自专门数据市场的客户列表更可靠，因为前者是客户数据的记录系统，而后者可能是某个地理区域或人口统计的客户子集，并可能包含修改或过时的客户数据。
- en: In some industries, such as financial services, lineage is required as part
    of regulatory compliance. For example, the Basel Committee on Banking Supervision’s
    rule 239 requires that financial services companies demonstrate to auditors the
    lineage of the data used for financial reporting. So, if data in the gold zone
    is used for financial reporting, it is imperative to document its lineage and
    keep it up to date.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些行业中，如金融服务业，数据血统作为法规合规的一部分是必需的。例如，巴塞尔银行监督委员会的规则239要求金融服务公司向审计员展示用于财务报告的数据的血统。因此，如果黄金区的数据用于财务报告，有必要记录其血统并保持更新。
- en: There are many challenges with representing data lineage, especially around
    system identity and transformation logic. Because data passes through many systems
    and tools, it is often difficult to identify whether different tools are referring
    to the same system or to different systems. Furthermore, because different tools
    express their transformations differently—some visually, some using a programming
    language, some using a query language or script—it can be difficult to represent
    all the transformations that have been performed in a unified way.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表示数据血统存在许多挑战，特别是系统标识和转换逻辑方面。由于数据经过多个系统和工具，很难确定不同工具是在指同一系统还是不同系统。此外，由于不同工具以不同方式表达它们的转换——有些可视化，有些使用编程语言，有些使用查询语言或脚本——因此很难以统一的方式表示所有已执行的转换。
- en: Let’s look at identity first. Imagine that one Hadoop file is created with an
    open source Hadoop utility called Sqoop that uses Java Database Connectivity (JDBC)
    to execute relational database queries and load the results into a file. However,
    another Hadoop file is created by reading data from the same table in the same
    database with an ETL tool that uses an Open Database Connectivity (ODBC) interface.
    There may be no programmatic way of recognizing that both files were extracted
    from the same database. Furthermore, because Sqoop may execute a free-form query,
    it may not be possible to recognize that Sqoop is ultimately reading the same
    table as the ETL tool.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 首先看看标识。想象一下，一个Hadoop文件是使用名为Sqoop的开源Hadoop实用程序创建的，该实用程序使用Java数据库连接（JDBC）执行关系数据库查询，并将结果加载到文件中。然而，另一个Hadoop文件是通过读取相同数据库中同一表的数据创建的，使用的是一个使用开放数据库连接（ODBC）接口的ETL工具。也许没有程序化的方法能够识别这两个文件都是从同一个数据库中提取的。此外，因为Sqoop可能执行一个自由格式的查询，可能无法识别Sqoop最终读取的是与ETL工具相同的表。
- en: One of the selling points of an ETL tool is that if all the transformations
    are performed by a single tool, the identity problem is resolved by that tool
    and technical lineage is easy to represent using whatever representation that
    ETL tool uses natively. However, if your enterprise, like most, is using multiple
    ETL tools, you have to resolve the identity problem as well as the representation
    problem. There are many ways to represent lineage, depending on the target audience.
    Business analysts, for example, prefer business-level lineage that describes how
    the data was generated using business terms and simple explanations. Most technical
    users prefer technical lineage that shows the specific code that was used to generate
    the target data set, or a rigorous equivalent graphical representation of that
    code.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ETL工具的一个卖点是，如果所有转换都由单个工具执行，那么该工具解决了标识问题，并且可以使用该ETL工具本地使用的任何表示来轻松表示技术血统。然而，如果您的企业像大多数企业一样使用多个ETL工具，则必须解决标识问题以及表示问题。有许多表示血统的方法，具体取决于目标受众。例如，业务分析师更喜欢描述使用业务术语和简单解释生成数据的业务级血统。大多数技术用户更喜欢显示用于生成目标数据集的具体代码或该代码的严格等效图形表示的技术血统。
- en: 'Technical lineage is challenging because the data set may be generated through
    multiple steps using a variety of programs and programming languages, scripts,
    and tools. Two aspects of technical lineage need to be considered: granularity
    and transformation representation.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 技术血统具有挑战性，因为数据集可能通过多个步骤使用各种程序、编程语言、脚本和工具生成。需要考虑技术血统的两个方面：粒度和转换表示。
- en: 'There are two levels of granularity that you may encounter:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会遇到两种粒度级别：
- en: Data set–level granularity
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集级别的粒度
- en: The lineage relationship between various data sets is usually captured and represented
    as a *directed graph*. In other words, each step in obtaining or transforming
    the data is shown as a node or box, with arrows showing the one-way flow of data
    through these nodes. Frequently, the programs used to generate data sets are represented
    as well, but as nodes on a graph rather than detailed pieces of code.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 各种数据集之间的血统关系通常被捕获和表示为*有向图*。换句话说，获取或转换数据的每个步骤都显示为一个节点或框，箭头显示数据通过这些节点的单向流动。经常，用于生成数据集的程序也被表示为图上的节点，而不是详细的代码片段。
- en: Field-level granularity
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 字段级别的粒度
- en: The lineage of each field is captured and represented as a directed graph, often
    with different nodes representing different transformations. Sometimes, this level
    of lineage is combined with the data set level on a single graph, so the users
    can drill down from data set level to field level in the interface for a specific
    target data set.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 每个字段的血统都被捕获并表示为一个有向图，通常不同节点表示不同的变换。有时，这种血统级别与数据集级别合并为单一图，用户可以在界面上从数据集级别钻取到特定目标数据集的字段级别。
- en: 'There are also two transformation representations that you might see:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 还有两种可能看到的转换表示：
- en: Normalized representation
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 规范化表示
- en: All transformations are translated to a common representation. This is difficult
    because the data set may have been generated using a variety of procedural and
    declarative languages, so the translation may be complicated (or even impossible).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 所有变换都被转换为一个共同的表示。这很困难，因为数据集可能使用各种过程和声明性语言生成，因此翻译可能复杂（甚至不可能）。
- en: Original representation
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 原始表示
- en: All transformations are presented in the original language or script. This is
    also challenging because a data set may be generated using complex software, making
    it extremely difficult to programmatically extract just the logic that was used
    to generate that data set.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所有变换都以原始语言或脚本呈现。这也是有挑战性的，因为可能使用复杂软件生成数据集，仅从逻辑上提取生成数据集时使用的代码可能非常困难。
- en: 'Let’s look at an example. Imagine we have two Hadoop files: one downloaded
    from the data warehouse that contains our complete customer list with customer
    addresses and one from a public website called [Data.gov](http://Data.gov) that
    provides average income for each zip code.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个例子。假设我们有两个Hadoop文件：一个从数据仓库下载，包含完整的客户列表及其地址；另一个来自名为[Data.gov](http://Data.gov)的公共网站，提供每个邮政编码的平均收入。
- en: Once we’ve downloaded the files, we create two Hive tables based on the directories
    where the files reside—`Cust` and `IncomebyZip`—to provide a relational (SQL)
    interface. We then execute a SQL query to select Californian customers from the
    `Cust` table and join it with the `IncomebyZip` table to generate a final table,
    `CalCustRelIncome`, that rates each customer based on how their household income
    compares to the average in their zip code.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 下载文件后，我们基于文件所在目录创建了两个Hive表——`Cust`和`IncomebyZip`——以提供关系（SQL）接口。然后，我们执行SQL查询，从`Cust`表中选择加利福尼亚客户，并与`IncomebyZip`表进行连接，生成最终表`CalCustRelIncome`，该表根据他们家庭收入与其邮政编码平均收入的比较对每位客户进行评级。
- en: The business lineage for this process will look something like [Figure 6-3](#business-level_lineage).
    It will skip all the intermediate steps and details and provide English (or whatever
    language is used) descriptions as callouts documenting what each major step accomplishes.
    Of course, the issue with verbal business-level descriptions is that someone has
    to write them up and, even more challenging, maintain them as the code changes.
    On the other hand, these descriptions are probably the only practical way to inform
    a business analyst how a data set was generated.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个流程的业务血统将看起来像[图6-3](#business-level_lineage)。它会跳过所有中间步骤和细节，并提供英语（或其他语言）的描述作为标注，记录每个主要步骤的完成情况。当然，用口头方式描述业务级别的问题是，必须有人编写它们，而且更具挑战性的是，在代码发生变化时进行维护。另一方面，这些描述可能是向业务分析师解释数据集生成方式的唯一实用方式。
- en: '![Business-level lineage](Images/ebdl_0603.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![业务级血统](Images/ebdl_0603.png)'
- en: Figure 6-3\. Business-level lineage
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3\. 业务级血统
- en: A more technical data set representation is illustrated in [Figure 6-4](#data_seten_dashlevel_technical_lineage).
    It gives a reasonably high-level view of what the major steps are and where the
    data comes from. It does not, however, give the details of the operations. For
    example, if we did not call the target table `CalCustRelIncome`, there would be
    no way from the data set–level lineage to infer that only Californian customers
    are being selected from the `Cust` table.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 更技术性的数据集表示在[图6-4](#data_seten_dashlevel_technical_lineage)中有所体现。它给出了主要步骤的相对高层视图以及数据来源。但是，它并不提供操作的详细信息。例如，如果我们没有将目标表称为`CalCustRelIncome`，则无法从数据集级血统推断出只选择了`Cust`表中的加利福尼亚客户。
- en: '![Data set–level technical lineage](Images/ebdl_0604.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![数据集级技术血统](Images/ebdl_0604.png)'
- en: Figure 6-4\. Data set–level technical lineage
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-4\. 数据集级技术血统
- en: Sometimes it is possible to add more details to this lineage—for example, a
    callout to the Sqoop node that shows the Sqoop query or a Hive node to show the
    Hive query, as illustrated in [Figure 6-5](#adding_detail_to_data_seten_dashlevel_tr).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 有时可以向此谱系添加更多细节，例如指向显示Sqoop查询的Sqoop节点或显示Hive查询的Hive节点，如[图 6-5](#adding_detail_to_data_seten_dashlevel_tr)所示。
- en: '![Adding detail to data set–level transformation nodes](Images/ebdl_0605.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![添加数据集级转换节点的详细信息](Images/ebdl_0605.png)'
- en: Figure 6-5\. Adding detail to data set–level transformation nodes
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-5\. 添加数据集级转换节点的详细信息
- en: Let’s now take a look at how this single Hive query would be represented graphically
    as field-level lineage using a  normalized graphical representation, as illustrated
    in [Figure 6-6](#field-level_technical_lineage).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何将这个单一的Hive查询以图形方式表示为字段级别的谱系，使用规范化的图形表示，如[图 6-6](#field-level_technical_lineage)所示。
- en: '![Field-level technical lineage](Images/ebdl_0606.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![字段级技术谱系](Images/ebdl_0606.png)'
- en: Figure 6-6\. Field-level technical lineage
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-6\. 字段级技术谱系
- en: 'In [Figure 6-6](#field-level_technical_lineage), the field-level operations
    are depicted using dashed blue lines and the data set operations using solid dark
    red lines. If a field is simply copied from source to target, it is represented
    with a dashed blue line from the source field to the target field. If there is
    an operation involved, it is represented by a blue oval, and the inputs and outputs
    of the operation are depicted using dashed blue lines. The inputs and outputs
    of set operations (in the red ovals) are indicated by solid red lines between
    the operations and specific data sets. A single Hive SQL query is represented
    as three steps or nodes:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 6-6](#field-level_technical_lineage)中，使用虚线蓝线表示字段级别的操作，使用实线深红线表示数据集操作。如果一个字段只是从源复制到目标，则用虚线蓝线表示从源字段到目标字段。如果涉及操作，则用蓝色椭圆表示，并使用虚线蓝线表示操作的输入和输出。集合操作的输入和输出（在红色椭圆中）通过实线红线表示在操作和特定数据集之间。单个Hive
    SQL查询表示为三个步骤或节点：
- en: Filter node
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤节点
- en: Applies a filter to the `Cust` Hive table to select only Californian customers.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对`Cust` Hive表应用过滤器，仅选择加利福尼亚客户。
- en: Join node
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 连接节点
- en: Combines the results of the first operation and the `IncomebyZip` Hive table
    to add the `AverageIncome` for each customer based on their zip code.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 将第一个操作的结果与`IncomebyZip` Hive表合并，根据其邮政编码为每个客户添加`AverageIncome`。
- en: Function node
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 函数节点
- en: Extracts the fields that will go into the `CalCustRelIncome` Hive table and
    calculates each customer’s `IncomeRatio` by dividing the customer’s income by
    the average income. Because the calculation is done in place as part of the query,
    the function does not have a name and is represented by the code that performs
    the desired calculation.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 提取将进入`CalCustRelIncome` Hive表的字段，并通过将客户的收入除以平均收入来计算每个客户的`IncomeRatio`。由于计算是作为查询的一部分直接进行的，所以该函数没有名称，并由执行所需计算的代码表示。
- en: To generate this diagram, the lineage system has to be able to parse and understand
    Hive SQL and break it into separate operations. This is a nontrivial amount of
    work, especially since, instead of using a declarative language like SQL, the
    user may have chosen to write a Java MapReduce program or a Pig script, or used
    any of a number of other options that may be difficult or impossible to represent
    in a normalized way.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成此图表，谱系系统必须能够解析和理解Hive SQL，并将其分解为单独的操作。这是一项相当复杂的工作，特别是因为用户可能选择编写Java MapReduce程序或Pig脚本，或使用其他一些难以或无法规范化表示的选项，而不是使用SQL等声明性语言。
- en: Stewardship
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管理
- en: There is a strong social aspect to trust. Analysts rely on word of mouth to
    find trusted SMEs. Just as some bloggers, YouTubers, and industry experts stand
    out by developing credibility and a large following, some users’ annotations and
    curation in a modern data lake may be more credible than others. These trusted
    users may have organizational responsibility for the data, they may be official
    data stewards, or they may be widely recognized and respected experts. Even in
    organizations with a mature governance structure and officially designated data
    stewards, some data stewards may be more knowledgeable than others, and sometimes
    analysts can have better insight and knowledge—especially about data sets they’ve
    created or use all the time—than the official data stewards. To address the distributed
    and unofficial nature of expertise, some enterprises are turning to paradigms
    used by consumer websites such as TripAdvisor and Yelp to identify credible reviewers
    by allowing users to rate whether they found the information helpful and accurate.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 信任有很强的社会层面。分析师依赖口口相传来找到可信赖的SME。就像一些博客作者、YouTuber和行业专家通过建立可信度和庞大的追随者群体脱颖而出一样，现代数据湖中某些用户的注释和策划可能比其他人更可信。这些受信任的用户可能有数据的组织责任，他们可能是官方的数据管理者，或者是广受认可和尊重的专家。即使在拥有成熟治理结构和官方指定数据管理者的组织中，有些数据管理者可能比其他人更具知识，有时分析师可以比官方的数据管理者更了解和掌握洞察力，尤其是关于他们创建或经常使用的数据集。为了解决专业知识的分散和非官方特性，一些企业正在转向像TripAdvisor和Yelp这样的消费者网站使用的范例，通过允许用户评价信息是否有帮助和准确来识别可信的评论者。
- en: Provisioning
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置
- en: 'Once the right data set is identified, the analyst needs to make it available
    for use, or “provision” it. Provisioning has two aspects: getting permission to
    use the data and getting physical access to the data.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了正确的数据集，分析师需要将其提供供使用，或者“配置”它。配置有两个方面：获取使用数据的权限和获取访问数据的物理访问权限。
- en: One of the big challenges with a data lake is deciding which analysts to give
    access to what data. In some industries, giving everyone access to all the data
    is perfectly acceptable and solves the first problem. Most industries, however,
    handle a lot of sensitive data. Data sets may contain personally identifiable
    information (PII), financial information such as credit card and account numbers,
    and business-sensitive information such as order sizes and discounts.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖面临的一个重大挑战是决定将访问权限授予哪些分析师访问哪些数据。在某些行业中，让每个人都能访问所有数据是完全可以接受的，并解决了第一个问题。然而，大多数行业处理大量敏感数据。数据集可能包含个人可识别信息（PII）、财务信息如信用卡和账户号码，以及业务敏感信息如订单大小和折扣。
- en: 'Traditional access control approaches create an account for each user and potentially
    add users to one or more groups, and specify access permissions for specific users
    and groups to each data set or field. For example, all US marketing analysts may
    get access to US sales data, but not EU sales data. When a new marketing analyst
    is hired in the US, they’re added to the US marketing analysts group and get access
    to the US sales data. If they transfer to a different team or join a different
    project, their permissions and group memberships will have to be reviewed. When
    a new data set is received, the security team and data stewards have to figure
    out who should get access to it. There are many problems with this approach:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的访问控制方法为每个用户创建一个账户，并可能将用户添加到一个或多个组中，并为每个数据集或字段指定特定用户和组的访问权限。例如，所有美国市场分析师可能会访问美国销售数据，但不会访问欧盟销售数据。当在美国新雇佣了一个市场分析师时，他们将被添加到美国市场分析师组，并获得访问美国销售数据的权限。如果他们转到不同的团队或加入不同的项目，他们的权限和组成员资格将需要重新审查。当接收到新的数据集时，安全团队和数据管理者必须找出谁应该访问它。这种方法存在许多问题：
- en: It is very time-consuming. Large companies may be hiring people or moving them
    between projects all the time. Someone—usually a high-cost and highly responsible
    person—has to figure out who should have access to what.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这非常耗时。大公司可能一直在雇佣人员或在项目之间调动人员。通常是一位成本高且责任重大的人需要找出应该让谁访问什么内容。
- en: There is a lot of historical baggage, because someone who transfers to another
    project may still have responsibilities in the old team for a period of time.
    Analysts sometimes end up with access privileges for data that is no longer relevant
    to their work and to which they shouldn’t have access.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在大量历史包袱，因为转移到另一个项目的人可能在一段时间内仍然对旧团队负责。分析师有时会因为访问已经与其工作无关且不应访问的数据而拥有访问权限。
- en: There is a lot of data, and it is sometimes difficult to figure out correctly
    who should have access to what data.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有很多数据，有时很难正确判断谁应该访问哪些数据。
- en: Ideally, the analysts should be able to request access to the data they need.
    However, if they cannot find the data without having access to it, we have a catch-22.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，分析员应该能够请求访问他们需要的数据。但是，如果他们无法在没有访问权限的情况下找到数据，我们就陷入了一个进退两难的境地。
- en: The solution is a more agile approach to access control that some enterprises
    are beginning to adopt. They create metadata catalogs that allow the analysts
    to find any data set without having access to it. Once the right data sets have
    been identified, the analysts request access to them and the data steward or data
    owner decides whether to grant access, for how long, and for which portions of
    the data. Once the access period expires, the access can be automatically revoked
    or an extension requested.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这种解决方案是一种更灵活的访问控制方法，一些企业开始采用。它们创建元数据目录，使分析员能够找到任何数据集而无需访问它。一旦确定了正确的数据集，分析员请求访问它们，数据管理员或数据所有者决定是否授予访问权限，持续多久，以及数据的哪些部分。一旦访问期限到期，可以自动撤销访问权限或请求延期。
- en: 'There are many advantages to this approach:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有很多优势：
- en: No work has to be done detecting and protecting sensitive data inside a data
    set until someone requests this data set.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在有人请求此数据集之前，无需进行检测和保护数据集内的敏感数据。
- en: The analysts can find any data in the data lake, including the newly ingested
    data sets, without having access to it.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析员可以在数据湖中找到任何数据，包括新加入的数据集，而无需访问它。
- en: The data stewards and owners do not have to invest time in figuring out who
    should have access to what data unless there is an actual project requiring it.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据管理者和所有者不必投入时间来确定谁应该访问哪些数据，除非真正需要进行项目。
- en: Access requests can require justification, creating an audit trail of who is
    requesting what data sets and why.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问请求可能需要理由，创建请求者和数据集的审计轨迹。
- en: Access may be granted to a portion of a data set and for a specific period of
    time.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以授权访问数据集的部分内容，并限定特定的时间段。
- en: It is always clear which data sets are in use, so data quality and governance
    efforts can be focused on those data sets. For example, ETL jobs can update only
    the data sets that are currently in use, and sensitive data deidentification and
    data quality rules may be applied just to those data sets.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 始终清楚哪些数据集正在使用，因此数据质量和治理工作可以集中在这些数据集上。例如，ETL 作业只能更新当前正在使用的数据集，敏感数据去识别和数据质量规则可能仅应用于这些数据集。
- en: We’ll talk more about catalogs in [Chapter 8](ch08.xhtml#cataloging_the_data_lake)
    and look at access control and provisioning in much more detail in [Chapter 9](ch09.xhtml#governing_data_access).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第 8 章](ch08.xhtml#cataloging_the_data_lake)中更详细地讨论目录，并在[第 9 章](ch09.xhtml#governing_data_access)中深入探讨访问控制和供应。
- en: Preparing Data for Analysis
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为分析准备数据
- en: While some data is usable as is, more often than not it requires some preparation.
    Preparation may be as simple as selecting the proper subset of data, or it may
    involve a complex cleansing and transformation process to get the data into the
    right form. The most common data preparation tool is Microsoft Excel. Unfortunately,
    Excel has significant limitations that make it impractical to use for working
    with large data lake files. Fortunately, new tools that scale better have been
    brought to market by newer companies like Alteryx, Datameer, Paxata, and Trifacta,
    as well as more established data integration vendors such as Informatica and Talend.
    Even some of the data visualization vendors, like Tableau and Qlik, are incorporating
    common data prep capabilities into their tools. Excel is evolving too—Microsoft
    is working on a Hadoop interface for Excel running in Azure.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一些数据可以直接使用，但更多时候需要一些准备工作。准备工作可能只是选择适当的数据子集，或者可能涉及复杂的清洗和转换过程，将数据转换为正确的形式。最常见的数据准备工具是
    Microsoft Excel。不幸的是，Excel 有显著的限制，使其难以处理大数据湖文件。幸运的是，新公司如 Alteryx、Datameer、Paxata
    和 Trifacta，以及更成熟的数据集成供应商如 Informatica 和 Talend，已经推出了更好的可扩展工具。甚至一些数据可视化供应商，如 Tableau
    和 Qlik，也正在将常见的数据准备功能整合到其工具中。Excel 也在进化中，微软正在为在 Azure 中运行的 Excel 开发 Hadoop 接口。
- en: Because traditional data warehouses were designed to do fairly narrow and predefined
    types of analysis, they relied on well-tested and optimized ETL jobs developed
    by IT to transform data to a single common schema and load it. Any data quality
    issues were resolved in the same way for everyone, and all data was transformed
    to a common set of measurements and representations. All the analysts had to make
    do with this one-size-fits-all approach.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 由于传统的数据仓库设计用于执行相对狭窄和预定义类型的分析，它们依赖于由IT开发的经过充分测试和优化的ETL作业，用于将数据转换为单一的公共模式并加载它。任何数据质量问题都会以同样的方式为每个人解决，并且所有数据都会转换为一组公共的度量和表示。所有分析师都不得不使用这种一刀切的方法。
- en: Modern self-service analytics, and especially data science, are more agile and
    exploratory. Analysts can leverage more of the data that’s available in the data
    warehouse and often seek out the original or raw data to work with, so they can
    prepare it in the way that fits their specific needs and use cases.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现代的自助服务分析，尤其是数据科学，更加灵活和探索性。分析师可以利用数据仓库中更多可用的数据，通常会寻找原始或原始数据进行处理，以适应其特定的需求和用例。
- en: This need to create “fit for purpose” data is impossible for IT to accommodate.
    Fortunately, a set of tools called *data preparation* or *data wrangling* tools
    have become popular that make it easy for analysts to convert raw data into a
    format suitable for analytics without needing deep technical skills. These data
    preparation tools present a visual spreadsheet-like interface for the analysts
    to work with. The following essay by Bertrand Cariou describes different use cases
    for data wrangling and describes how one of the modern data preparation tools,
    Trifacta, provides sophisticated machine learning interfaces that try to guess
    and automatically suggest operations based on the user’s data selections.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 创建“合适用途”的数据对于IT部门来说几乎是不可能的任务。幸运的是，一组称为*数据准备*或*数据整理*工具变得流行起来，使分析师能够轻松将原始数据转换为适合分析的格式，而无需深入的技术技能。这些数据准备工具为分析师提供了类似电子表格的视觉界面。Bertrand
    Cariou的以下文章描述了数据整理的不同用例，并描述了现代数据准备工具之一Trifacta如何提供复杂的机器学习接口，尝试基于用户的数据选择自动推荐操作。
- en: Data Wrangling in the Data Lake
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据湖中的数据整理
- en: '![](Images/ebdl_06in01.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ebdl_06in01.png)'
- en: '***Bertrand Cariou** is Senior Director of Partner Marketing at Trifacta. Bertrand
    has focused on making data accessible and usable at Informatica and a number of
    other US and European companies.*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '***Bertrand Cariou** 是Trifacta的高级合作伙伴营销总监。在Informatica以及其他一些美国和欧洲公司，Bertrand专注于使数据可访问和可用。*'
- en: The term “data wrangling” is often used to describe the preliminary preparation
    that business professionals, such as business analysts, data analysts, and data
    scientists, do in order to get data ready for analytics. Data wrangling can thus
    be part of the “self-service” described elsewhere in this book. However, we also
    see an increasing amount of data wrangling done by data engineers to facilitate
    their work and improve collaboration with business users. Whoever carries out
    the task, data wrangling is the process of converting diverse data from its raw
    formats into a structured and consumable format for business intelligence, statistical
    modeling tools, and machine learning, or to supply data to business applications.
    New tools informed by machine learning, such as those offered by my company, [Trifacta](https://www.trifacta.com/),
    take much of the effort out of data preparation by making suggestions and interacting
    with users to accelerate and automate data wrangling for their particular data-driven
    needs.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: “数据整理”一词通常用于描述商业专业人士（如业务分析师、数据分析师和数据科学家）在准备数据以供分析之前所做的初步准备工作。数据整理因此可以成为本书其他地方描述的“自助服务”的一部分。然而，我们也看到越来越多的数据整理由数据工程师完成，以促进他们的工作并改善与业务用户的协作。无论谁执行这项任务，数据整理都是将多样化的数据从其原始格式转换为结构化和可消费的格式，供业务智能、统计建模工具和机器学习使用，或者向业务应用提供数据。通过我的公司提供的机器学习驱动的新工具，例如[Trifacta](https://www.trifacta.com/)，大大简化了数据准备的工作，通过提供建议和与用户互动来加速和自动化数据整理，以满足他们特定的数据驱动需求。
- en: Situating Data Preparation in Hadoop
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据准备置于Hadoop环境中
- en: Data preparation sits between the data storage and processing layer—tools such
    as Hadoop, Spark, and other data computation engines—and the visualization or
    statistical applications used downstream in the process.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备位于数据存储和处理层（如Hadoop、Spark和其他数据计算引擎）以及后续过程中使用的可视化或统计应用程序之间。
- en: 'As pictured in [Figure 6-7](#trifacta_ecosystem), data wrangling happens in
    various places within an analytical workflow:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [Figure 6-7](#trifacta_ecosystem)，数据整理发生在分析工作流的各个环节中：
- en: Exploratory prep
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 探索性准备
- en: Data wrangling in the data lake typically occurs within a zone or during a move
    between zones. Users may access raw and refined data to combine and structure
    it for their exploratory work or to define new transformation rules they want
    to automate and run on a regular basis.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖中的数据整理通常发生在区域内或在区域之间移动时。用户可以访问原始和精炼数据，将其组合和结构化，用于他们的探索工作，或者定义他们想要自动化并定期运行的新转换规则。
- en: Data preparation can also be used for lightweight ingestion, which brings in
    external data sources (e.g., spreadsheets and relational data) to augment data
    already in the data lake for the purpose of exploration and cleansing.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备也可用于轻量级摄取，将外部数据源（如电子表格和关系数据）引入数据湖中已有的数据，以进行探索和清洗。
- en: Consumption
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 消费
- en: Wrangling often occurs in the production zone to deliver data to the business
    insight layer. This can be done by SQL-based BI tools, or by exporting the data
    in a file format (e.g., CSV, JSON, or the Tableau Data Extract format) for further
    use with analytics tools including Tableau, SAS, or R.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产区域进行整理通常是将数据传递到商业洞察层。这可以通过基于SQL的BI工具完成，或者通过导出数据到文件格式（例如CSV、JSON或Tableau数据提取格式）以供包括Tableau、SAS或R在内的分析工具进一步使用。
- en: Operationalization
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 运营化
- en: In addition to the actual work of transforming data, data preparation tools
    are used within the operationalization layer, where teams can regularly schedule
    data preparation jobs and control their execution.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据转换的实际工作外，数据准备工具还在运营化层内使用，团队可以定期安排数据准备作业并控制其执行。
- en: '![Trifacta ecosystem](Images/ebdl_0607.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![Trifacta 生态系统](Images/ebdl_0607.png)'
- en: Figure 6-7\. Trifacta ecosystem
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-7\. Trifacta 生态系统
- en: All data access, transformation, and interaction within the solution is logged
    and made available to data governance tools so administrators can understand the
    lineage of data.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案内的所有数据访问、转换和交互都已记录，并提供给数据治理工具，因此管理员可以了解数据的来源。
- en: Common Use Cases for Data Preparation
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备常见用例
- en: The use of data preparation tools can be classified into three major scenarios
    (with variations) that may benefit from automation organized by a business team
    or an IT team. Here is a summary of three common use cases for data wrangling
    and corresponding Trifacta customer examples.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备工具的使用可分类为三种主要场景（具有变体），可能通过由业务团队或IT团队组织的自动化受益。以下是数据整理的三个常见用例及相应的 Trifacta
    客户示例摘要。
- en: 'Use case: Self-service automation for analytics or business applications'
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用场景：自助式分析或业务应用的自动化
- en: For these types of initiatives, the business teams manage the analytical process,
    from initial data ingestion to the eventual data consumption, including data preparation.
    Often, their end goal is to create a “master report” for compliance purposes or
    to aggregate disparate data. In these initiatives, the IT organization is responsible
    for setting up the data lake and data ingestion so that, from there, the business
    team can handle its own data requirements and schedule data preparation tasks
    without IT involvement.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这类倡议，业务团队管理分析流程，从初始数据摄取到最终数据消耗，包括数据准备。通常，他们的最终目标是为了合规目的或聚合不同数据而创建“主报告”。在这些倡议中，IT组织负责设置数据湖和数据摄取，以便业务团队可以处理自己的数据需求，并安排数据准备任务，无需IT参与。
- en: Customer example
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 客户示例
- en: PepsiCo needed to optimize its retail sales forecasts, which combine retailer
    point-of-sales (POS) data with internal transaction information. For PepsiCo,
    the major challenge sprang from the different formats provided by each retailer
    through automatically generated reports or email attachments. With Trifacta, the
    business analyst team assumes ownership of the ingestion of the retailer data
    into PepsiCo’s data lake, can explore and define how the data should be transformed,
    and can execute jobs on demand or through routine scheduling to deliver a consumable
    outcome to the downstream applications or processes. Trifacta learns from the
    user’s interactions, providing immediate feedback to better guide them through
    structuring, enriching, and validating the data at scale.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: PepsiCo 需要优化其零售销售预测，这涉及将零售商销售点（POS）数据与内部交易信息相结合。对于 PepsiCo 来说，主要挑战来自于每个零售商提供的不同格式，通过自动生成的报告或电子邮件附件提供。通过
    Trifacta，业务分析团队承担了将零售商数据引入 PepsiCo 数据湖的责任，可以探索和定义数据转换方式，并可以根据需求执行作业或通过常规调度以向下游应用程序或流程提供可消费的结果。Trifacta
    从用户的互动中学习，提供即时反馈，以更好地指导他们在规模上构建、丰富和验证数据。
- en: 'Use case: Preparation for IT operationalization'
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用例：IT 运营准备
- en: In this scenario, the data specialists—usually data analysts or data engineers—design
    the preparation work  themselves, then test, validate, and run the rules at scale
    to produce the desired outcome. After end users create operational workflows,
    the IT team usually integrates them into enterprise workloads.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，数据专家——通常是数据分析师或数据工程师——自行设计准备工作，然后测试、验证并按规模运行规则，以产生期望的结果。在最终用户创建操作工作流后，IT团队通常会将其集成到企业工作负载中。
- en: Customer example
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 客户案例
- en: A large European bank needed to extract chat logs from its website to improve
    customer service, as well as to analyze product and service needs. The bank used
    Trifacta to transform these complex formats into discrete attributes for a broader
    Customer 360 initiative that incorporated additional data channels. In this case,
    the teams provided their IT organization with the data wrangling rules they’d
    created so that IT could combine the various data flows consistently.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一家大型欧洲银行需要从其网站提取聊天日志以改进客户服务，以及分析产品和服务需求。该银行使用 Trifacta 将这些复杂的格式转换为更广泛的客户 360
    计划中的离散属性，该计划还包含其他数据渠道。在这种情况下，团队提供了他们创建的数据整理规则，以便IT能够一致地组合各种数据流。
- en: 'Use case: Exploratory analytics and machine learning'
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用例：探索性分析和机器学习
- en: As their name suggests, exploratory analytics use data to explore various aspects
    of the business and involve using a data preparation tool on an ad hoc basis to
    explore the data, investigate use cases, find relevant third-party data, validate
    hypotheses, discover patterns in the data, or generate data sets for data scientist
    modeling.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，探索性分析利用数据探索业务的各个方面，并使用临时数据准备工具探索数据，调查使用案例，找到相关的第三方数据，验证假设，在数据中发现模式，或生成数据集供数据科学家建模。
- en: Customer example
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 客户案例
- en: A well-established marketing analytics provider aggregates and examines client
    data to provide analytic outputs for its customers, thus helping those customers
    measure, predict, and optimize the impacts of marketing efforts. Each customer
    has different data sources and formats. Trifacta helps expedite the process of
    discovery and transformation of client data to create structured and clean data
    sets. For example, Trifacta uses machine learning to automatically discover the
    data, structure it in a familiar grid interface, identify potentially invalid
    data, and suggest the best ways to clean and transform the data.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一家成熟的营销分析提供商聚合并检验客户数据，为其客户提供分析输出，从而帮助客户衡量、预测和优化营销工作的影响。每个客户具有不同的数据来源和格式。Trifacta
    帮助加快客户数据的发现和转换过程，以创建结构化和干净的数据集。例如，Trifacta 使用机器学习自动发现数据，在熟悉的网格界面中结构化数据，识别可能无效的数据，并建议最佳的清洁和转换数据的方法。
- en: Analyzing and Visualizing
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析和可视化
- en: There are a plethora of great self-service data visualization and analytics
    tools. Tableau and Qlik have been around for years, and a number of smaller vendors,
    like Arcadia Data and AtScale, deliver high-quality, easy-to-use functionality
    specifically for big data environments. The following essay by Donald Farmer discusses
    the self-service trends in business intelligence.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有很多出色的自助数据可视化和分析工具。Tableau 和 Qlik 已经存在多年，还有许多较小的供应商，如Arcadia Data 和 AtScale，专门为大数据环境提供高质量、易于使用的功能。Donald
    Farmer 的以下文章讨论了商业智能中的自助服务趋势。
- en: The New World of Self-Service Business Intelligence
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自助式商业智能的新世界
- en: '![](Images/ebdl_06in02.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ebdl_06in02.png)'
- en: '***Donald Farmer** is VP of Innovation and Design at Qlik. He has been pushing
    the boundaries of data strategy for almost 30 years, developing, writing, and
    speaking internationally on advanced analytics and innovation strategy.*'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**Donald Farmer** 是 Qlik 公司创新与设计副总裁。他在数据战略方面的工作已有近 30 年，国际上发表了大量关于高级分析和创新战略的著作和演讲。'
- en: In the past few years, business users have dramatically transformed their relationship
    with IT. The change began, as so many have recently, with the iPhone and the iPad
    and the phenomenon known as Bring Your Own Device. BYOD, as it is often abbreviated,
    is a strategic response or a tactical adjustment to the realization that users
    now have easy access to better technology and faster upgrades than IT can provision.
    This new reality is also reflected in the world of data analysis. In a clear parallel
    with Bring Your Own provisioning of devices, business analysts have embraced self-service
    business intelligence. With self-service, users build their own solutions and
    may even choose their own tools, “with or without IT’s permission,” as the analyst
    firm Gartner has noted.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年里，业务用户与IT的关系发生了显著变化。这种变化的起因与iPhone、iPad以及被称为Bring Your Own Device（自带设备）的现象有关。BYOD，通常缩写为这样，是对用户现在比IT能提供的更好的技术和更快的升级有了战略性响应或战术性调整的认识。这一新现实在数据分析领域也有所体现。与设备自带的现象有明显的类似之处，业务分析师已经接受了自助式商业智能。在自助服务中，用户可以构建自己的解决方案，甚至可以选择自己的工具，“无论是否得到IT的许可”，正如分析公司Gartner所指出的。
- en: 'In the past, IT departments necessarily provisioned the reporting infrastructure,
    dashboards, and analytics for enterprises. Only the IT team could deploy the expensive
    storage and computing power needed. Only IT understood the technical issues involved
    in extracting and consolidating data or building the analytic models. And very
    significantly, only IT could secure the data and the resulting analysis to ensure
    the right people had the right insights. The workflow of BI followed the classic
    lifecycle model: IT gathered requirements, built solutions, deployed them into
    production, and started another round of requirements gathering.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，IT部门必须为企业提供报告基础设施、仪表板和分析。只有IT团队能够部署所需的昂贵存储和计算能力。只有IT了解从数据中提取和 consolidaing
    数据或构建分析模型涉及的技术问题。非常重要的是，只有IT能够保护数据和相关分析，确保正确的人拥有正确的洞察力。商业智能的工作流程遵循经典的生命周期模型：IT收集需求，构建解决方案，将其部署到生产环境，并开始下一轮需求收集。
- en: In truth, there was always a secondary “dark side” to this IT-led model. As
    developers struggled to run this lifecycle quickly enough for increasingly agile
    businesses, analysts in finance and marketing departments simply used Excel as
    a good-enough tool. Often they exported data from reports for further analysis.
    Sometimes they had access to source data. Excel was effective, but not powerful.
    Its lack of security, compounded by the habits of business users, caused poor
    analyses or even confidential data to proliferate in the shadows of an organization.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这种由IT主导的模型始终存在着一个“黑暗面”。随着开发人员为了越来越敏捷的业务而奋力推进生命周期，财务和营销部门的分析师简单地使用Excel作为一个足够好的工具。他们经常从报告中导出数据进行进一步分析，有时他们也能访问源数据。Excel
    是有效的，但不强大。它的安全性缺失，再加上业务用户的习惯，导致组织内部产生了较差的分析甚至是机密数据。
- en: Self-service BI tools brought this dark side not only into the light, but into
    the mainstream of enterprise analytics. From the mid-2000s, as 64-bit computing
    became established as the norm, applications such as QlikView, Tableau, and Microsoft’s
    PowerPivot brought powerful analysis capabilities to any business user. These
    tools rolled the lifecycle of ETL and building data models into a single, simple
    environment. They used elegant visualizations to enable users to find patterns
    and communicate insights easily and effectively. Using in-memory storage and compression,
    the same tools could bring data capacity and computing power to the desktop that
    was once available only in the carefully managed server room. With this power
    (as the cliche says) comes great responsibility, but well-designed self-service
    applications can help with that too.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 自助式BI工具不仅将这种黑暗面带入了光明中，而且将其带入了企业分析的主流。从2000年代中期开始，随着64位计算成为常态，诸如QlikView、Tableau和Microsoft的PowerPivot等应用程序为任何业务用户提供了强大的分析能力。这些工具将ETL生命周期和数据模型构建集成到一个简单的环境中。它们使用优雅的可视化方式使用户能够轻松有效地发现模式并传达见解。通过使用内存存储和压缩，相同的工具可以将数据容量和计算能力带到桌面，这些功能曾经只能在精心管理的服务器房间中获得。正如俗话所说，拥有这种力量也带来了巨大的责任，但设计良好的自助式应用程序也可以帮助解决这个问题。
- en: This transformation in the power of business users has brought about many changes,
    especially in the analytic workflow and, dramatically, the role of IT.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这种业务用户权力的转变带来了许多变化，特别是在分析工作流程和IT角色方面。
- en: The New Analytic Workflow
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 新的分析工作流程
- en: 'As I have already noted, the workflow for analytic applications and reporting
    used to be a variation on the classic application lifecycle: requirements, design,
    deployment, and new requirements. However, with self-service, the business analysts
    know their own requirements and develop their own solutions, so the process may
    seem a little haphazard. Requirements can change at any time. A tweak to the frontend
    design (adding a new element to a chart, for example) can alter the data extraction
    process. An analyst may deploy and share a half-finished solution just to get
    moving.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，用于分析应用程序和报告的工作流程曾经是经典应用生命周期的一种变体：需求、设计、部署和新需求。然而，自助服务使得业务分析师了解自己的需求并开发自己的解决方案，因此这个过程可能显得有些零散。需求随时可能发生变化。例如，前端设计的微调（例如向图表添加新元素）可能会改变数据提取过程。分析师可能会部署并分享一个半成品解决方案以便推动进展。
- en: Rather than thinking of this agile, ad hoc process as a lifecycle, I find it
    more useful to picture it as a supply chain, with data as the raw material flowing
    through a number of processes and value being added at each step. The great advantage
    of a supply chain is that sometimes steps may be consolidated for efficiency.
    When delivering food from farm to table, for instance, a wholesaler may not only
    resell, but also wash and partially prepare some vegetables. Similarly, each step
    of the supply chain may be simply operational (transport from farm to market)
    or may add value as it takes place, such as sorting by size or quality grade.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 与其将这种敏捷、临时的过程视为生命周期，我认为将其描绘为一个供应链更为有用，其中数据作为原材料流经多个流程，每个步骤都增加了价值。供应链的巨大优势在于，有时可以为了效率而合并步骤。例如，将食物从农场送到餐桌时，批发商不仅可以转售，还可以洗涤和部分准备一些蔬菜。同样，供应链的每个步骤可以简单地是操作性的（从农场到市场的运输），也可以在进行时增加价值，例如按大小或质量等级进行分类。
- en: In a business analytics supply chain, with data as the raw material, business
    users may take data from wherever it is available. Self-service tools typically
    offer a simple wizard, script, or visual environment to view, join, consolidate,
    or clean data. Often this phase is known as data blending—or, more poetically,
    data wrangling. However, unlike traditional ETL processes, which loaded a data
    warehouse model from the sources in advance of any analytic work, blending and
    wrangling may happen before, during, or (with an analytic app used as a source)
    even after an analytic process.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在业务分析供应链中，数据作为原材料，业务用户可以从任何可用的地方获取数据。自助服务工具通常提供简单的向导、脚本或视觉环境，用于查看、连接、 consoli
    data或清理数据。通常这个阶段被称为数据混合——或者更有诗意地称为数据梳理。然而，与传统的ETL过程不同，传统的ETL过程在任何分析工作之前从源中加载数据仓库模型，混合和梳理可以在分析过程之前、过程中或（使用分析应用程序作为源）甚至之后发生。
- en: For business analysts using self-service tools, blending data *is* an analytic
    process. As they see initial results in their visualizations, they form a better
    understanding of the data as they work. They then modify their scripts or data
    wizards in order to see the data differently, or to enable a better visualization.
    Note an important difference here. In the traditional data warehouse lifecycle,
    ETL populates a model, which then drives the analysis. The model may be a star
    schema or a sophisticated OLAP model requiring specialized design and engineering
    skills. In the self-service supply chain, the model is still there, but the user
    may not even be aware of it. The business analyst, typically, is not a conscious
    modeler.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用自助工具的业务分析师来说，混合数据*是*一种分析过程。当他们在可视化中看到初步结果时，他们会随着工作对数据有更好的理解。然后，他们修改他们的脚本或数据向导，以便以不同的方式查看数据，或者启用更好的可视化。这里有一个重要的区别。在传统的数据仓库生命周期中，ETL填充了一个模型，然后驱动分析。该模型可以是星型模式或需要专业设计和工程技能的复杂OLAP模型。在自助供应链中，模型仍然存在，但用户可能甚至没有意识到它。通常情况下，业务分析师不是一个有意识的建模者。
- en: The flexibility of this new approach is of particular value when business analysts
    work with a data lake. In a traditional model, with complex relational sources
    and expensive storage, data was often transformed through remarkably complex processes
    to get it into a shape that was efficient for both storage and querying. ETL and
    OLAP, for example, required substantial skills.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当业务分析师与数据湖一起工作时，这种新方法的灵活性尤为重要。在传统模型中，通过复杂的关系源和昂贵的存储，数据经常通过非常复杂的过程进行转换，以使其以对存储和查询都有效的形式呈现。例如，ETL和OLAP需要相当的技能。
- en: The data lake, on the other hand, can store vast quantities of data easily.
    With the flexibility of schema-on-read there is no need to try to model all scenarios
    in a single data warehouse. So long as the data can be presented to business analysts
    with effective semantics (they should never be required to write MapReduce!) and
    reasonable performance, they can work with a data lake and self-service tools.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，数据湖可以轻松存储大量数据。有了按需模式的灵活性，就没有必要在单个数据仓库中建模所有情况。只要能向业务分析师呈现有效的语义（他们永远不应被要求编写MapReduce！）和合理的性能，他们就可以使用数据湖和自助工具。
- en: Gatekeepers to Shopkeepers
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从“门卫”到“店主”
- en: At this point, we should consider the role of IT. It is important to say that
    IT will likely still deliver mission-critical intelligence. The enterprise data
    warehouse will still be with us for year-on-year consolidated financial reporting,
    tax analysis, and HR. IT will still need their OLAP and ETL skills for some years
    to come. They will need MapReduce skills too!
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们应该考虑IT的角色。重要的是要说，IT可能仍然会提供至关重要的情报。企业数据仓库仍将在我们身边，用于年度财务报告、税务分析和人力资源。IT在未来几年仍然需要他们的OLAP和ETL技能。他们也需要MapReduce技能！
- en: Naturally, IT has an important role to play in the analytic supply chain. At
    the very least, IT must “keep the lights on” with robust networking, storage,
    and data sources to make the process possible. However, its role is much broader
    than that.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，IT在分析供应链中扮演着重要角色。至少，IT必须通过强大的网络、存储和数据源来“保持灯火通明”，使这一过程成为可能。然而，它的角色远不止如此。
- en: In the past, as we have seen, IT teams provided the entire lifecycle of analytics,
    because they were the only ones who could. In addition, they secured systems,
    offering “data access” as needed and as permitted. IT departments took this role
    of “gatekeeper” very seriously, as well they might, to the extent that business
    analysts were often frustrated by their limited access to essential data.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，过去，IT团队提供了整个分析生命周期，因为他们是唯一能够提供的人。此外，他们保证系统安全，并根据需要和许可提供“数据访问”。IT部门非常认真地承担起这个“门卫”的角色，他们可能会这样做，以至于商业分析师经常对他们对重要数据的有限访问感到沮丧。
- en: This frustration with the IT gatekeepers led to professional tensions and frequently
    to exactly the “dark side” of unmanaged data sharing that the controls were meant
    to avoid. Spreadsheets got out of control, because they were the only tool left
    to the analysts that IT could not lock down.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对IT门卫的这种挫折导致了专业紧张，并频繁地导致正是控制措施本意避免的未管理数据共享的“黑暗面”。电子表格失控了，因为这是分析师留给他们的唯一工具，而IT无法封锁它们。
- en: With self-service, we need a new approach. IT teams must move from being “gatekeepers”
    to “shopkeepers.”
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 随着自助服务的到来，我们需要一种新的方法。IT团队必须从“门卫”转变为“店主”。
- en: 'A gatekeeper is concerned with keeping the wrong people out. A shopkeeper invites
    the right people in, preparing, presenting, and provisioning the shop’s goods
    to encourage their appropriate use. In IT terms, a data provisioning team can
    build feeds and models designed for business users to serve themselves. Rather
    than opening the gate to give users access to source systems, an effective team
    can instead provision data out for the users, cleaned, consolidated, and even
    anonymized as needed for effective analysis and good governance. What IT doesn’t
    need to do is prepare every source for a specific use case: the business analysts
    use their tools on the data supply chain to serve themselves those solutions.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 一个守门人关心的是如何阻止错误的人进入。而一个店主则邀请合适的人进入，准备、展示和供应商店的商品，鼓励其适当的使用。在IT术语中，数据供应团队可以构建供业务用户自服务的数据源和模型。与其打开大门让用户访问源系统，一个有效的团队可以为用户提供经过清理、合并甚至匿名化处理的数据，以便进行有效的分析和良好的治理。IT不需要为特定用例准备每个数据源：业务分析师使用他们的工具在数据供应链上为自己提供这些解决方案。
- en: Governing Self-Service
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自助服务的治理
- en: In this supply chain model, where IT teams act as shopkeepers, they still play
    a major role in securing and governing data. One of the most important moves they
    can make is to provision well-designed self-service tools for their users.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种供应链模型中，IT团队充当店主的角色，他们在安全和数据治理方面仍然发挥着重要作用。他们可以采取的最重要措施之一是为用户提供设计良好的自助服务工具。
- en: A well-designed, enterprise-ready application for self-service doesn’t only
    provide powerful, simple tools for the business user—it must also feature a powerful
    server architecture, in the cloud or on premises, which gives IT the necessary
    insight and oversight to govern the use of the system.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一个设计良好、企业级的自助应用不仅为业务用户提供强大而简单的工具，还必须具备强大的服务器架构，可以部署在云端或本地，为IT部门提供必要的洞察力和监督权，以管理系统的使用。
- en: IT’s oversight includes managing the deployment, user permissions, server performance,
    and scaling. The insights a well-designed application offers include understanding
    what data sources analysts are using, who they are sharing their apps and visualizations
    with, and how the data is prepared and refreshed.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: IT的监督工作包括管理部署、用户权限、服务器性能和扩展能力。一个设计良好的应用程序所提供的洞察力包括了解分析师使用的数据源、他们与谁分享他们的应用程序和可视化内容，以及数据的准备和刷新方式。
- en: Remember that IT still provides the mission-critical analytics, such as financial
    reporting. They are still the gatekeepers of the inner sanctum. But much of the
    work of business analytics can be handled with a lighter touch—still secure and
    governed—with a new provisioning approach that is not only more agile, but simply
    friendlier to the rest of the business too.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，IT仍然提供关键的分析服务，比如财务报告。他们仍然是内部核心的守门人。但是，商业分析的许多工作可以通过更轻松的方式处理——仍然安全、受控——采用新的供应方法，不仅更具敏捷性，而且更易于与业务其他部门合作。
- en: Conclusion
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: If leveraging data to make better decisions is key to the success of the modern
    enterprise, the old practices of IT-built rigid analytics and data warehouses
    are not going to keep up. The only practical way to leverage data to make better
    decisions is to enable the analysts to do their own analytics without having to
    involve IT in every project (and thereby making IT a bottleneck and slowing things
    down). As you saw in this chapter, a new generation of analytics and data infrastructure
    tools—from data visualization tools to data prep tools and data catalogs—has arisen
    to make it possible for the analysts to work with data without involving IT. Methods
    of providing search, provenance, and trust can be built into the data lake for
    all its users.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如果利用数据做出更好的决策是现代企业成功的关键，那么依赖于IT构建的僵化分析和数据仓库的旧实践是跟不上的。利用数据做出更好的决策的唯一实际方法是使分析师能够自行进行分析，而无需在每个项目中都涉及IT（从而使IT成为瓶颈并减慢事务进展）。正如您在本章中看到的那样，从数据可视化工具到数据准备工具和数据目录等新一代分析和数据基础设施工具已经出现，使分析师能够在不涉及IT的情况下处理数据。可以将提供搜索、溯源和信任的方法构建到数据湖中，供所有用户使用。
