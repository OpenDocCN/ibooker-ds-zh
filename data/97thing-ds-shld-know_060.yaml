- en: 'Chapter 55\. Responsible Design and Use of AI: Managing Safety, Risk, and Transparency'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第55章。AI的负责任设计和使用：管理安全性、风险和透明度
- en: Pamela Passman
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pamela Passman
- en: '![](Images/Pamela_Passman.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/Pamela_Passman.png)'
- en: Vice Chair, Ethisphere
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 副主席，Ethisphere
- en: CEO, Center for Responsible Enterprise And Trade (CREATe.org)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: CEO，负责任企业与贸易中心（CREATe.org）
- en: AI is having a growing impact on markets and business practices around the world.
    And its potential is even greater. The [IDC found in September 2019](https://oreil.ly/sMAiL)
    that “spending on AI systems will reach $97.9 billion in 2023, more than two and
    one half times the $37.5 billion that will be spent in 2019.” According to the
    McKinsey Global Institute, [AI could deliver additional global economic output](https://oreil.ly/hJ1d5)
    of $13 trillion per year by 2030.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: AI正对全球市场和商业实践产生日益增长的影响。而其潜力更是巨大。[IDC在2019年9月发现](https://oreil.ly/sMAiL)，“到2023年，AI系统的支出将达到979亿美元，是2019年375亿美元的两倍多”。根据麦肯锡全球研究所的数据，[到2030年，AI可能会为全球经济增加额外的产出](https://oreil.ly/hJ1d5)达到13万亿美元。
- en: Yet even as it unleashes business potential and broader societal benefits, the
    use of AI can also result in a host of [unwanted and sometimes serious consequences](https://oreil.ly/lz_zR).
    These considerations have given rise to no fewer than [32 different industry,
    NGO, and government AI ethics codes](https://oreil.ly/TetYv), which outline steps
    that organizations should take to develop, implement, and use AI in ways that
    support societal values and manage risks.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管AI释放出业务潜力和更广泛的社会效益，但其使用也可能导致一系列[不受欢迎且有时严重的后果](https://oreil.ly/lz_zR)。这些考虑因素已经催生了不少于[32个不同的行业、非政府组织和政府AI伦理准则](https://oreil.ly/TetYv)，这些准则详述了组织应采取的步骤，以开发、实施和使用符合社会价值并管理风险的AI技术。
- en: 'Many forward-thinking companies—some with firsthand experience in dealing with
    unintended consequences of AI—have also developed their own codes of ethical AI.
    While these codes can vary quite a bit, nine common responsibilities [have been
    identified](https://oreil.ly/PIYYz). These responsibilities can be divided into
    three groups: responsible design and use, lawful use, and ethical use. Here we
    take a focused look into the first group, *responsible design and use,* which
    encompasses AI security, safety, risk management, and transparency.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 许多有远见的公司——其中一些公司有处理AI意外后果的第一手经验——也制定了自己的AI伦理准则。虽然这些准则可能有很大不同，但已经确定了九项共同的责任[（已被确认）](https://oreil.ly/PIYYz)。这些责任可以分为三组：负责任的设计和使用、合法使用和伦理使用。在这里，我们重点关注第一组，*负责任的设计和使用*，它涵盖了AI安全、安全性、风险管理和透明度。
- en: Security and Safety
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全和安全性
- en: From time to time, the media will highlight a dramatic AI incident, such as
    an accident involving a self-driving car. This type of coverage reflects the widespread
    concern of consumers and businesses, thus solidifying the need for AI to be developed,
    implemented, and used in a safe and secure way.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 媒体偶尔会报道一些引人注目的AI事件，比如涉及自动驾驶汽车的事故。这种报道反映了消费者和企业的广泛关切，从而巩固了AI需以安全和可靠的方式开发、实施和使用的必要性。
- en: For companies, this means taking a comprehensive approach to managing the security
    and safety implications of AI, engaging all relevant parts of the organization
    beyond technology. This cross-functional institutional approach would allow companies
    to embrace the power and responsibility of AI in an efficient, effective way while
    also avoiding unintended risks of harm.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对公司而言，这意味着全面管理AI的安全性和安全性影响，使组织所有相关部门超越技术参与进来。这种跨功能的机构方法使公司能够高效、有效地拥抱AI的力量和责任，同时避免意外风险和伤害。
- en: For example, Microsoft, as one of the early adopters of AI, directly addresses
    safety and security considerations in its [principles for responsible AI](https://oreil.ly/iBUvd).
    By requiring that “AI systems should perform reliably and safely” under both normal
    and unexpected conditions, Microsoft commits to having AI systems that operate
    as they were originally designed to do, respond safely to unanticipated conditions,
    and resist harmful manipulation. Initial and ongoing testing, maintenance, and
    protection of AI systems are thus vital. And human judgment remains key to identifying
    potential blind spots and biases in AI systems, and to determining how, when,
    and for how long an AI system should be used.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，作为人工智能早期采用者之一，微软在其[负责任人工智能原则](https://oreil.ly/iBUvd)中直接涉及安全和安全考虑。通过要求“人工智能系统在正常和意外情况下都应可靠且安全运行”，微软承诺其人工智能系统将按照最初设计的方式运行，安全应对意外情况，并抵抗有害操纵。因此，对人工智能系统的初步和持续测试、维护和保护至关重要。而人类判断力仍然是识别人工智能系统潜在盲点和偏见的关键，并确定何时、如何以及多长时间使用人工智能系统的重要因素。
- en: Ongoing Risk Management
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持续风险管理
- en: Given the relatively recent appearance and use of AI technologies, it appears
    that not many companies have taken a close, institutional look at AI risks and
    risk management. Thus, it is critical for companies to recognize that while the
    potential risks posed by AI cannot be eliminated entirely, they can and should
    be anticipated, assessed, and managed to an extent commensurate with their expected
    impact.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于人工智能技术的相对新出现和使用，似乎还没有多少公司对人工智能风险和风险管理进行了深入的机构化审视。因此，对公司来说至关重要的是认识到，尽管人工智能可能带来的潜在风险无法完全消除，但可以并且应该预见、评估和管理这些风险，以符合其预期影响的程度。
- en: “Few leaders have had the opportunity to hone their intuition about the full
    scope of societal, organizational, and individual risks [of AI],” notes the McKinsey
    analysis [*Confronting the Risks of Artificial Intelligence*](https://oreil.ly/RCggZ)*.*
    “As a result, executives often overlook potential perils (‘We’re not using AI
    in anything that could “blow up,” like self-driving cars’) or overestimate an
    organization’s risk-mitigation capabilities (‘We’ve been doing analytics for a
    long time, so we already have the right controls in place, and our practices are
    in line with those of our industry peers’). It’s also common for leaders to lump
    in AI risks with others owned by specialists in the IT and analytics organizations
    (‘I trust my technical team; they’re doing everything possible to protect our
    customers and our company’).”
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 麦肯锡分析《*面对人工智能风险*》指出，“很少有领导人有机会磨练自己对社会、组织和个人人工智能风险的整体认识。”因此，领导们往往忽视潜在的危险（‘我们没有在像自动驾驶车辆等可能“爆炸”的领域使用人工智能’），或者过高估计组织的风险缓解能力（‘我们长期以来一直在进行分析，所以我们已经有了正确的控制措施，并且我们的做法与同行业相符’）。领导们还经常将人工智能风险与由IT和分析专家负责的其他风险混为一谈（‘我信任我的技术团队；他们正在尽一切可能保护我们的客户和公司’）。
- en: Many companies already use the enterprise risk management (ERM) approach of
    *Identify > Assess > Manage* to address other kinds of risks across their organizations.
    So it would be logical to assess and manage newly arising AI risks within the
    overall ERM framework.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司已经在其组织中使用企业风险管理（ERM）方法*识别 > 评估 > 管理*来处理其他类型的风险。因此，在整体ERM框架内评估和管理新出现的人工智能风险是合乎逻辑的。
- en: As our networks grow more and more interconnected, it would also be logical
    to extend AI risk management practices to a company’s third parties. Whether they
    are suppliers, customers, or other business partners, the management of key AI
    risks that may arise among such third parties should not be neglected. Telefónica
    illustrates this principle by [contractually reserving the right](https://oreil.ly/zzQ0D)
    to verify with its third parties on an ongoing basis that their disclosures on
    the logic and data use of the suppliers’ AI-based products are true.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们的网络日益互联，将人工智能风险管理实践扩展到公司的第三方也是合乎逻辑的。无论是供应商、客户还是其他业务伙伴，都不应忽视这些第三方可能出现的关键人工智能风险管理。Telefónica通过[合同约定保留权利](https://oreil.ly/zzQ0D)，持续验证其第三方供应商AI产品的逻辑和数据使用披露的真实性，从而阐明了这一原则。
- en: Transparency
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 透明度
- en: 'As AI systems expand and grow, and as more risks resulting from the use of
    AI appear, consumers and companies are starting to demand more transparency about
    the AI-based products and services that they use. [Research reveals](https://oreil.ly/rIpRv)
    that consumers are split on their feelings about AI: only 35% say they are comfortable
    with a business using AI to interact with them, while 28% say they are not comfortable
    with this, and the biggest group—37%—say they just do not know yet.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能系统的扩展和发展，以及由于使用人工智能带来的更多风险的出现，消费者和企业开始要求更多关于其使用的基于人工智能的产品和服务的透明度。[研究显示](https://oreil.ly/rIpRv)，消费者对人工智能的感受存在分歧：仅有35%的人表示他们对企业使用人工智能与他们进行交互感到舒适，28%的人表示他们对此感到不舒服，而最大的群体——37%的人表示他们还不知道。
- en: 'This helps to explain why most of the industry, NGO, government, and company
    AI ethics codes include a requirement of transparency for various aspects of the
    development, implementation, and use of AI. For example, IBM’s [Principles for
    Trust and Transparency of AI](https://oreil.ly/G9ykX) mandate that if AI is used
    to make important decisions, it must be explainable: “Technology companies must
    be clear about who trains their AI systems, what data was used in that training,
    and, most importantly, what went into their algorithm’s recommendations.”'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于解释为什么大多数行业、非政府组织、政府以及公司人工智能伦理准则都包括对人工智能开发、实施和使用各个方面透明性的要求。例如，IBM的[《人工智能信任和透明原则》](https://oreil.ly/G9ykX)要求，如果人工智能用于做出重要决策，必须能够解释其决策过程：“技术公司必须清楚地说明是谁训练了他们的人工智能系统，使用了哪些数据进行了训练，最重要的是，他们的算法推荐的依据是什么。”
- en: As part of its particular AI transparency principle, IBM has undertaken to make
    clear when and for what purposes AI is being applied, the data and training methods
    used in its AI systems, its commitment to ongoing testing and improvement, its
    protection of client data, and its support for ensuring that people can understand
    how an AI system came to a conclusion or recommendation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作为其特定的人工智能透明原则的一部分，IBM已经承诺在何时以及出于何种目的应用人工智能时进行明确说明，其人工智能系统使用的数据和训练方法，其持续测试和改进的承诺，以及其保护客户数据的措施，以及支持确保人们能够理解人工智能系统是如何得出结论或建议的。
- en: Conclusion
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: The further expansion of AI is inevitable. With many consumers still harboring
    fears about AI, the emerging requirements for responsible AI present a major opportunity
    for businesses to develop and explain their AI initiatives in a way that is consistent
    with customers’ expectations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的进一步扩展是不可避免的。尽管许多消费者仍然对人工智能心存恐惧，但对负责任人工智能的新需求为企业提供了一个重大机遇，可以以符合客户期望的方式开发和解释其人工智能计划。
- en: By focusing on safety and security, managing the risks, and maintaining transparency
    and responsible disclosure of AI, companies are in a position to not only earn
    the trust of their customers but also improve business and society in unimaginable
    ways.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过专注于安全性和保障、管理风险以及保持人工智能透明和负责任披露，企业不仅能赢得客户的信任，还能以难以想象的方式改善业务和社会。
