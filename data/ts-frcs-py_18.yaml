- en: 15 Remembering the past with LSTM
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 15 使用LSTM记住过去
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Examining the long short-term memory (LSTM) architecture
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查长短期记忆（LSTM）架构
- en: Implementing an LSTM with Keras
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Keras实现LSTM
- en: In the last chapter, we built our first models in deep learning, implementing
    both linear and deep neural network models. In the case of our dataset, we saw
    that both models outperformed the baselines we built in chapter 13, with the deep
    neural network being the best model for single-step, multi-step, and multi-output
    tasks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们构建了我们的第一个深度学习模型，实现了线性和深度神经网络模型。在我们的数据集情况下，我们看到了这两个模型都优于我们在第13章中构建的基线，其中深度神经网络是单步、多步和多输出任务的最好模型。
- en: Now we’ll explore a more advanced architecture called *long short-term memory*
    (LSTM), which is a particular case of a *recurrent neural network* (RNN). This
    type of neural network is used to process sequences of data, where the order matters.
    One common application of RNN and LSTM is in natural language processing. Words
    in a sentence have an order, and changing that order can completely change the
    meaning of a sentence. Thus, we often find this architecture behind text classification
    and text generation algorithms.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将探索一个更高级的架构，称为**长短期记忆**（LSTM），它是**循环神经网络**（RNN）的一个特例。这种神经网络用于处理数据序列，其中顺序很重要。RNN和LSTM的一个常见应用是在自然语言处理中。句子中的单词有顺序，改变这个顺序可以完全改变句子的意思。因此，我们经常在文本分类和文本生成算法中找到这种架构。
- en: Another situation where the order of data matters is time series. We know that
    time series are sequences of data equally spaced in time, and that their order
    cannot be changed. The data point observed at 9 a.m. must come before the data
    point at 10 a.m. and after the data point at 8 a.m. Thus, it makes sense to apply
    the LSTM architecture for forecasting time series.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个数据顺序很重要的情况是时间序列。我们知道时间序列是时间上等间隔的数据序列，它们的顺序不能改变。上午9点的观测数据点必须在上午10点的数据点之前，在上午8点的数据点之后。因此，应用LSTM架构进行时间序列预测是有意义的。
- en: In this chapter, we’ll first explore the general architecture of a recurrent
    neural network, and then we’ll dive deep into the LSTM architecture and examine
    its unique features and inner workings. Then we’ll implement an LSTM using Keras
    to produce single-step, multi-step, and multi-output models. We’ll finally compare
    the performance of LSTM against all the models we’ve built, from the baselines
    to the deep neural networks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先探索循环神经网络的一般架构，然后我们将深入探讨LSTM架构，并检查其独特的特性和内部工作原理。然后我们将使用Keras实现LSTM，以产生单步、多步和多输出模型。最后，我们将比较LSTM与我们所构建的所有模型的性能，从基线到深度神经网络。
- en: 15.1 Exploring the recurrent neural network (RNN)
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.1 探索循环神经网络（RNN）
- en: 'A recurrent neural network (RNN) is a deep learning architecture especially
    adapted to processing sequences of data. It denotes a set of networks that share
    a similar architecture: long short-term memory (LSTM) and gated recurrent unit
    (GRU) are subtypes of RNNs. In this chapter, we’ll solely focus on the LSTM architecture.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）是一种特别适合处理数据序列的深度学习架构。它表示一组具有相似架构的网络：长短期记忆（LSTM）和门控循环单元（GRU）是RNN的子类型。在本章中，我们将专注于LSTM架构。
- en: To understand the inner workings of an RNN, we’ll start with figure 15.1, which
    shows a compact illustration of an RNN. Just like in a deep neural network (DNN),
    we have an input, denoted as *x[t]*, and an output, denoted as *y[t]*. Here *x[t]*
    is an element of a sequence. When it is fed to the RNN, it computes a hidden state,
    denoted as *h[t]*. This hidden state acts as memory. It is computed for each element
    of the sequence and fed back to the RNN as an input. That way, the network effectively
    uses past information computed for previous elements of the sequence to inform
    the output for the next element of the sequence.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解RNN的内部工作原理，我们将从图15.1开始，它展示了RNN的一个紧凑的示意图。就像在深度神经网络（DNN）中一样，我们有一个输入，表示为*x[t]*，和一个输出，表示为*y[t]*。在这里，*x[t]*是序列中的一个元素。当它被输入到RNN中时，它计算一个隐藏状态，表示为*h[t]*。这个隐藏状态充当记忆。它为序列中的每个元素计算，并作为输入反馈到RNN中。这样，网络有效地使用为序列的前一个元素计算的历史信息来告知下一个元素序列的输出。
- en: '![](../../OEBPS/Images/15-01.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/15-01.png)'
- en: Figure 15.1 A compact illustration of an RNN. It computes a hidden state *h*[t],
    which is looped back in the network and combined with the next input of the sequence.
    This is how RNNs keep information from past elements of a sequence and use them
    to process the next element of a sequence.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1 RNN的紧凑示意图。它计算一个隐藏状态 *h*[t]，这个状态在网络中循环并与其他输入序列结合。这就是RNN如何从序列的过去元素中保持信息并使用它们来处理序列的下一个元素。
- en: Figure 15.2 shows an expanded illustration of an RNN. You can see how the hidden
    state is first computed at *t* = 0 and then is updated and passed on as each element
    of the sequence is processed. This is how the RNN effectively replicates the concept
    of memory and uses past information to produce a new output.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2展示了RNN的扩展示意图。你可以看到隐藏状态首先在 *t* = 0时计算，然后随着序列的每个元素被处理而更新和传递。这就是RNN有效地复制记忆概念并使用过去信息产生新输出的方式。
- en: '![](../../OEBPS/Images/15-02.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/15-02.png)'
- en: Figure 15.2 Expanded illustration of an RNN. Here you can see how the hidden
    state is updated and passed on to the next element of the sequence as an input.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2 RNN的扩展示意图。在这里，你可以看到隐藏状态是如何更新并作为输入传递到序列的下一个元素的。
- en: Recurrent neural network
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: A recurrent neural network (RNN) is especially adapted to processing sequences
    of data. It uses a hidden state that is fed back into the network so it can use
    past information as an input when processing the next element of a sequence. This
    is how it replicates the concept of memory.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）特别适合处理数据序列。它使用一个反馈到网络中的隐藏状态，这样在处理序列的下一个元素时，它可以利用过去的信息作为输入。这就是它复制记忆概念的方式。
- en: However, RNNs suffer from short-term memory, meaning that information from an
    early element in the sequence will stop having an impact further into the sequence.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，RNN受到短期记忆的限制，这意味着序列中早期元素的信息将停止对序列进一步的影响。
- en: 'However, the basic RNNs that we have examined come with a drawback: they suffer
    from short-term memory due to the vanishing gradient. The gradient is simply the
    function that tells the network how to change the weights. If the change in gradient
    is large, the weights change by a large magnitude. On the other hand, if the change
    in gradient is small, the weights do not change significantly. The vanishing gradient
    problem refers to what happens when the change in gradient becomes very small,
    sometimes close to 0\. This in turn means that the weights of the network do not
    get updated, and the network stops learning.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们检查的基本RNN有一个缺点：由于梯度消失，它们受到短期记忆的限制。梯度是告诉网络如何改变权重的函数。如果梯度的变化很大，权重会以很大的幅度改变。另一方面，如果梯度的变化很小，权重不会显著改变。梯度消失问题指的是当梯度变化变得非常小，有时接近0时发生的情况。这反过来意味着网络的权重不会更新，网络停止学习。
- en: In practice, this means the RNN forgets about past information that is far away
    in the sequence. It therefore suffers from a short-term memory. For example, if
    an RNN is processing 24 hours of hourly data, the points at hours 9, 10, and 11
    might still impact the output at hour 12, but any point prior to hour 9 might
    not contribute at all to the network’s learning, because the gradient gets very
    small for those early data points.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这意味着RNN会忘记序列中远离过去的信息。因此，它受到短期记忆的限制。例如，如果一个RNN正在处理24小时的每小时数据，第9、10和11小时的数据点可能仍然会影响第12小时的结果，但任何在第9小时之前的数据点可能根本不会对网络的训练有任何贡献，因为这些早期数据点的梯度变得非常小。
- en: Therefore, we must find a way to retain the importance of past information in
    our network. This brings us to the long short-term memory (LSTM) architecture,
    which uses the cell state as an additional way of keeping past information in
    memory for a long time.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们必须找到一种方法来保留网络中过去信息的重要性。这把我们带到了长短期记忆（LSTM）架构，它使用细胞状态作为在内存中长时间保持过去信息的额外方式。
- en: 15.2 Examining the LSTM architecture
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.2 检查LSTM架构
- en: The *long short-term memory* (LSTM) architecture adds a cell state to the RNN
    architecture to avoid the vanishing gradient problem, where past information ceases
    to impact the learning of the network. This allows the network to keep past information
    in memory for a longer time.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*长短期记忆*（LSTM）架构通过向RNN架构添加细胞状态来避免梯度消失问题，即过去信息不再影响网络的训练。这使得网络能够将过去信息保留在内存中更长时间。'
- en: The LSTM architecture is shown in figure 15.3, and you can see that it is more
    complex than the basic RNN architecture. You’ll notice the addition of the cell
    state, denoted as *C*. This cell state is what allows the network to keep past
    information in the network for a longer time, thus resolving the vanishing gradient
    problem. Note that this is unique to the LSTM architecture. We still have an element
    of a sequence being processed, shown as *x[t]*, and a hidden state is also computed,
    denoted as *h[t]*. In this case, both the cell state *C[t]* and the hidden *h[t]*
    are passed on to the next element of the sequence, making sure that past information
    is used as an input for the next element in the sequence being processed.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM架构如图15.3所示，你可以看到它比基本的RNN架构更复杂。你会注意到细胞状态的添加，表示为*C*。这个细胞状态使得网络能够将过去的信息在网络中保持更长时间，从而解决了梯度消失问题。请注意，这是LSTM架构的独特之处。我们仍然有一个正在处理的序列元素，表示为*x[t]*，并且还计算了一个隐藏状态，表示为*h[t]*。在这种情况下，细胞状态*C[t]*和隐藏状态*h[t]*都被传递到序列的下一个元素，确保过去的信息被用作下一个正在处理的序列元素的输入。
- en: '![](../../OEBPS/Images/15-03.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/15-03.png)'
- en: Figure 15.3 The architecture of an LSTM neuron. The cell state is denoted as
    *C*, while the input is *x* and the hidden state is *h*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3 LSTM神经元的架构。细胞状态表示为*C*，输入表示为*x*，隐藏状态表示为*h*。
- en: 'You’ll also notice the presence of three gates: the forget gate, the input
    gate, and the output gate. Each has its specific function in the LSTM, so let’s
    explore each one in detail.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你还会注意到存在三个门：遗忘门、输入门和输出门。每个门在LSTM中都有其特定的功能，所以让我们详细探讨每一个。
- en: Long short-term memory
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆
- en: Long short-term memory (LSTM) is a deep learning architecture that is a subtype
    of RNN. LSTM addresses the problem of short-term memory by adding the cell state.
    This allows for past information to flow through the network for a longer period
    of time, meaning that the network still carries information from early values
    in the sequence.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）是一种深度学习架构，它是循环神经网络（RNN）的一个子类型。LSTM通过添加细胞状态来解决短期记忆问题。这使得过去的信息可以在网络中流动更长的时间，这意味着网络仍然携带序列早期值的信息。
- en: 'The LSTM is made up of three gates:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM由三个门组成：
- en: The *forget gate* determines what information from past steps is still relevant.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*遗忘门*决定了哪些过去步骤的信息仍然相关。'
- en: The *input gate* determines what information from the current step is relevant.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入门*决定了哪些当前步骤的信息是相关的。'
- en: The *output gate* determines what information is passed on to the next element
    of the sequence or as a result to the output layer.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出门*决定了哪些信息被传递到序列的下一个元素或作为结果传递到输出层。'
- en: 15.2.1 The forget gate
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.1 遗忘门
- en: The *forget gate* is the first gate in an LSTM cell. Its role is to determine
    what information, from both the past values and the current value of the sequence,
    should be forgotten or kept in the network.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*遗忘门*是LSTM单元中的第一个门。它的作用是确定从过去值和当前值中哪些信息应该被遗忘或保留在网络中。'
- en: '![](../../OEBPS/Images/15-04.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/15-04.png)'
- en: Figure 15.4 The forget gate in an LSTM cell. The present element of a sequence,
    *x*[t], and past information, *h*[*t*–1], are first combined. They are duplicated,
    and one is sent to the input gate while the other goes through the sigmoid activation
    function. The sigmoid outputs a value between 0 and 1, and if the output is close
    to 0, this means that information must be forgotten. If it is close to 1, the
    information is kept. The output is then combined with the past cell state using
    pointwise multiplication, generating an updated cell state *C'*[*t*–1].
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4 LSTM单元中的遗忘门。序列的当前元素*x*[t]和过去信息*h*[*t*–1]首先被组合。它们被复制，一个被发送到输入门，另一个通过sigmoid激活函数。sigmoid输出一个介于0和1之间的值，如果输出接近0，这意味着信息必须被遗忘。如果它接近1，信息则被保留。然后，输出与过去的细胞状态通过点乘结合，生成一个更新的细胞状态*C'*[*t*–1]。
- en: Looking at figure 15.4, we can see how the different inputs flow through the
    forget gate. First, the past hidden state *h*[*t*–1] and the present value of
    a sequence *x[t]* are fed into the forget gate. Recall that the past hidden state
    carries information from past values. Then, *h*[*t*–1] and *x[t]* are combined
    and duplicated. One copy goes straight to the input gate, which we’ll study in
    the next section. The other copy is sent through a sigmoid activation function,
    which is expressed as equation 15.1 and is shown in figure 15.5.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 查看图15.4，我们可以看到不同的输入是如何通过遗忘门的。首先，过去的隐藏状态 *h*[*t*–1] 和序列的当前值 *x[t]* 被输入到遗忘门中。回想一下，过去的隐藏状态携带了来自过去值的信息。然后，*h*[*t*–1]
    和 *x[t]* 被组合并复制。一份直接进入输入门，我们将在下一节中研究它。另一份通过一个Sigmoid激活函数，这被表示为方程15.1，并在图15.5中展示。
- en: '![](../../OEBPS/Images/15-04-Equation-15-1.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/15-04-Equation-15-1.png)'
- en: Equation 15.1
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 方程15.1
- en: '![](../../OEBPS/Images/15-05.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/15-05.png)'
- en: Figure 15.5 The sigmoid function outputs values between 0 and 1\. In the context
    of the forget gate, if the output of the sigmoid function is close to 0, the output
    is information that is forgotten. If the output is close to 1, it is information
    that must be kept.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.5 Sigmoid函数输出介于0和1之间的值。在遗忘门的上下文中，如果Sigmoid函数的输出接近0，则输出是即将被遗忘的信息。如果输出接近1，则是必须保留的信息。
- en: The sigmoid function determines which information to keep or to forget. That
    output is then combined with the previous cell state *C*[*t*–1] using pointwise
    multiplication. This results in an updated cell state that we call *C'*[*t*–1].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数确定要保留或遗忘的信息。然后，这个输出与之前的细胞状态 *C*[*t*–1] 通过逐点乘法结合。这导致了一个更新的细胞状态，我们称之为
    *C'*[*t*–1]。
- en: 'Once this is done, two things are sent to the input gate: an updated cell state,
    and a copy of the combination of the past hidden state and the current element
    of the sequence.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，将两样东西发送到输入门：一个更新的细胞状态，以及过去隐藏状态和当前序列元素的组合的副本。
- en: 15.2.2 The input gate
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.2 输入门
- en: Once information has passed through the forget gate, it proceeds to the input
    gate. This is the step where the network determines which information is relevant
    from the current element of the sequence. The cell state is updated again here,
    resulting in the final cell state.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 信息通过遗忘门后，继续进入输入门。这是网络确定当前序列元素中哪些信息相关的步骤。在这里再次更新细胞状态，得到最终的细胞状态。
- en: '![](../../OEBPS/Images/15-06.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/15-06.png)'
- en: Figure 15.6 The input gate of an LSTM. The past hidden state and current element
    of the sequence are duplicated again and sent through a sigmoid activation function
    and a hyperbolic tangent (tanh) activation function. Again, the sigmoid determines
    what information is kept or discarded, while the tanh function regulates the network
    to keep it computationally efficient. The results of both operations are combined
    using pointwise multiplication, and the result is used to update the cell state
    using pointwise addition, resulting in the final cell state *C[t]*. This final
    cell state is then sent to the output gate. Meanwhile, the same combination, [*h*[*t*–1]+
    *x*[t]], is sent to the output gate too.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.6 LSTM的输入门。过去的隐藏状态和序列的当前元素再次被复制并通过Sigmoid激活函数和双曲正切（tanh）激活函数发送。同样，Sigmoid确定要保留或丢弃的信息，而tanh函数调节网络以保持计算效率。这两个操作的结果通过逐点乘法结合，然后通过逐点加法更新细胞状态，得到最终的细胞状态
    *C[t]*。这个最终的细胞状态随后被发送到输出门。同时，相同的组合 [*h*[*t*–1]+ *x*[t]] 也被发送到输出门。
- en: Again, let’s zoom in on the input gate using figure 15.6\. The combination of
    the past hidden state and the current element of a sequence [*h*[*t*–1] + *x[t]*]
    coming from the forget gate is fed into the input gate and it is again duplicated.
    One copy goes out the input gate toward the output gate, which we’ll explore in
    the next section. Another copy is sent through the sigmoid activation function
    to determine if the information will be kept or forgotten. Another copy is sent
    through the hyperbolic tangent (tanh) function, which is shown in figure 15.7.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，让我们通过图15.6来聚焦于输入门。来自遗忘门的过去隐藏状态和序列当前元素的组合 [*h*[*t*–1] + *x[t]*] 被输入到输入门，并且再次被复制。一份从输入门输出，朝向输出门，我们将在下一节中探讨。另一份通过Sigmoid激活函数来确定信息是否将被保留或遗忘。另一份通过双曲正切（tanh）函数，这在图15.7中展示。
- en: '![](../../OEBPS/Images/15-07.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/15-07.png)'
- en: Figure 15.7 The hyperbolic tangent (tanh) function outputs values between –1
    and 1\. In the context of the LSTM, this serves as a way to regulate the network,
    making sure that values do not get very large and ensuring that computation remains
    efficient.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.7 双曲正切（tanh）函数输出介于 –1 和 1 之间的值。在 LSTM 的上下文中，这作为调节网络的一种方式，确保值不会变得非常大，并确保计算效率。
- en: The outputs of the sigmoid and tanh functions are combined using pointwise multiplication,
    and the result is combined with the updated cell state coming from the forget
    gate *C'*[*t*–1] using pointwise addition. This operation generates the final
    cell state *C[t]*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用逐点乘法将 sigmoid 和 tanh 函数的输出结合起来，然后将结果与来自遗忘门的更新后的细胞状态 *C'*[*t*–1] 使用逐点加法结合起来。这个操作生成了最终的细胞状态
    *C[t]*。
- en: Therefore, it is in the input gate that we add information from the current
    element in the sequence to the long memory of the network. This newly updated
    cell state is then sent to the output gate.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在输入门中添加序列当前元素的信息到网络的长时记忆中。这个新更新的细胞状态随后被发送到输出门。
- en: 15.2.3 The output gate
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.3 输出门
- en: Information has now passed from the forget gate to the input gate, and now it
    arrives at the output gate. It is in this gate that past information contained
    in the network’s memory, represented by the cell state *C[t]*, is finally used
    to process the current element of the sequence. This is also where the network
    either outputs a result to the output layer or computes new information to be
    sent to the processing of the next element in the sequence.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 信息现在已经从遗忘门传递到输入门，现在它到达输出门。正是在这个门中，网络内存中包含的过去信息，由细胞状态 *C[t]* 表示，最终被用来处理序列的当前元素。这也是网络输出结果到输出层或计算新信息以发送到序列下一个元素处理的地方。
- en: '![](../../OEBPS/Images/15-08.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/15-08.png)'
- en: Figure 15.8 The output gate of an LSTM. The past hidden state and current element
    of a sequence [*h*[*t*–1] + *x*[t]] are passed through the sigmoid function to
    determine if information will be kept or discarded. Then the cell state is passed
    through the tanh function and combined with the output of the sigmoid using pointwise
    multiplication. This is the step where past information is used to process the
    current element of a sequence. We then output a new hidden state *h*[t], which
    is passed to the next LSTM neuron or to the output layer. The cell state is also
    output.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.8 LSTM 的输出门。序列的过去隐藏状态和当前元素 [*h*[*t*–1] + *x*[t]] 通过 sigmoid 函数传递，以确定信息是否将被保留或丢弃。然后细胞状态通过
    tanh 函数传递，并与 sigmoid 的输出使用逐点乘法结合。这是使用过去信息处理序列当前元素的一步。然后我们输出一个新的隐藏状态 *h*[t]，它被传递到下一个
    LSTM 神经元或输出层。细胞状态也被输出。
- en: In figure 15.8 the past hidden state and current element of a sequence are sent
    through the sigmoid function. In parallel, the cell state goes through the tanh
    function. The resulting values from the tanh and sigmoid functions are then combined
    using pointwise multiplication, generating an updated hidden state *h[t]*. This
    is the step where past information, represented by the cell state *C[t]*, is used
    to process the information of the present element of the sequence.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 15.8 中，过去隐藏状态和序列的当前元素通过 sigmoid 函数。同时，细胞状态通过 tanh 函数。然后使用逐点乘法将 tanh 和 sigmoid
    函数的结果结合起来，生成一个更新的隐藏状态 *h[t]*。这是使用表示过去信息的细胞状态 *C[t]* 来处理序列当前元素信息的一步。
- en: The current hidden state is then sent out of the output gate. This will either
    be sent to the output layer of the network or to the next LSTM neuron treating
    the next element of the sequence. The same applies for the cell state *C[t]*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将当前的隐藏状态从输出门发送出去。这将被发送到网络的输出层，或者发送到下一个 LSTM 神经元处理序列的下一个元素。对于细胞状态 *C[t]* 也适用。
- en: In summary, the forget gate determines which information from the past is kept
    or discarded. The input gate determines which information from the current step
    is kept to update the network’s memory or is discarded. Finally, the output gate
    uses the information from the past stored in the network’s memory to process the
    current element of a sequence.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，遗忘门决定了哪些过去信息被保留或丢弃。输入门决定了哪些当前步骤的信息被保留以更新网络的内存或被丢弃。最后，输出门使用存储在网络内存中的过去信息来处理序列的当前元素。
- en: Having examined the inner workings of the LSTM architecture, we can now implement
    it for our interstate traffic dataset.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 经过检查 LSTM 架构的内部工作原理，我们现在可以将其应用于我们的州际交通数据集。
- en: 15.3 Implementing the LSTM architecture
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.3 实现LSTM架构
- en: We’ll now implement the LSTM architecture for the interstate traffic dataset
    we have been working with since chapter 12\. Recall that the main target of our
    scenario is the traffic volume. For the multi-output model, the targets are traffic
    volume and temperature.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将实现自第12章以来一直在使用的州际交通数据集的LSTM架构。回想一下，我们场景的主要目标是交通量。对于多输出模型，目标是交通量和温度。
- en: We’ll implement LSTM as a single-step model, a multi-step model, and a multi-output
    model. The single-step model will predict the traffic volume for the next timestep
    only, the multi-step model will predict the traffic volume for the next 24 hours,
    and the multi-output model will predict the temperature and traffic volume for
    the next timestep.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现LSTM作为单步模型、多步模型和多输出模型。单步模型将只预测下一个时间步长的交通量，多步模型将预测下一个24小时的交通量，多输出模型将预测下一个时间步长的温度和交通量。
- en: Make sure you have the `DataWindow` class and the `compile_and_fit` function
    (from chapters 13 and 14) in your notebook or Python script, as we’ll use these
    pieces of code to create windows of data and train the LSTM model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您的笔记本或Python脚本中包含`DataWindow`类和`compile_and_fit`函数（来自第13章和第14章），因为我们将使用这些代码片段来创建数据窗口并训练LSTM模型。
- en: 'The other prerequisite is to read the training set, the validation set, and
    the test set, so let’s do that right now:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个先决条件是读取训练集、验证集和测试集，所以让我们现在就做：
- en: '[PRE0]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note At any point, feel free to consult the source code for this chapter on
    GitHub: [https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH15](https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH15).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在任何时候，您都可以自由地查阅GitHub上本章的源代码：[https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH15](https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH15)。
- en: 15.3.1 Implementing an LSTM as a single-step model
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.1 实现作为单步模型的LSTM
- en: We’ll start by implementing the LSTM architecture as a single-step model. In
    this case, we’ll use 24 hours of data as an input to predict the next timestep.
    That way, there is a sequence of time that can be processed by the LSTM, allowing
    us to leverage past information to make a future prediction.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先实现LSTM架构作为单步模型。在这种情况下，我们将使用24小时的数据作为输入来预测下一个时间步长。这样，就有一个时间序列可以被LSTM处理，使我们能够利用过去的信息来做出未来的预测。
- en: First we need to create a data window to train the model. This will be a wide
    window, with 24 hours of data as input. For plotting purposes, the `label_width`
    is also 24, so that we can compare the predictions to the actual values over 24
    timesteps. Note that this is still a single-step model, so over 24 hours the model
    will only predict one timestep at a time, just like a rolling forecast.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个数据窗口来训练模型。这将是一个宽窗口，包含24小时的数据作为输入。为了绘图目的，`label_width`也是24，这样我们就可以在24个时间步长内比较预测值和实际值。请注意，这仍然是一个单步模型，所以24小时内模型将一次只预测一个时间步长，就像滚动预测一样。
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then we need to define our LSTM model in Keras. Again we’ll use the `Sequential`
    model to allow us to stack different layers in our network. Keras conveniently
    comes with the `LSTM` layer, which implements an LSTM. We’ll set `return_sequences`
    to `True`, as this signals Keras to use past information from the sequence, in
    the form of the hidden state and cell state, which we covered earlier. Finally,
    we’ll define the output layer, which is simply a `Dense` layer with one unit because
    we are forecasting the traffic volume only.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要在Keras中定义我们的LSTM模型。我们再次使用`Sequential`模型，以便我们可以在网络中堆叠不同的层。Keras方便地提供了`LSTM`层，它实现了LSTM。我们将`return_sequences`设置为`True`，因为这会指示Keras使用序列中的过去信息，即我们之前提到的隐藏状态和细胞状态。最后，我们将定义输出层，它只是一个具有一个单位的`Dense`层，因为我们只预测交通量。
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Set return_sequences to True to make sure that past information is being used
    by the network.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将`return_sequences`设置为`True`以确保网络正在使用过去的信息。
- en: It is as simple as that. We can now train the model using the `compile_and_fit`
    function and store its performance on the validation and test sets.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 就这么简单。现在我们可以使用`compile_and_fit`函数来训练模型，并将其在验证集和测试集上的性能存储起来。
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Optionally, we can visualize the predictions of our model on three sampled sequences
    using the `plot` method of our data window. The result is shown in figure 15.9.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，我们可以使用数据窗口的`plot`方法可视化模型在三个采样序列上的预测结果。结果如图15.9所示。
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../../OEBPS/Images/15-09.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/15-09.png)'
- en: Figure 15.9 Predicting traffic volume using an LSTM as a single-step model.
    Many predictions (shown as crosses) overlap the labels (shown as squares), suggesting
    we have a performant model with accurate predictions.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.9展示了使用LSTM作为单步模型预测交通量。许多预测（以交叉表示）与标签（以正方形表示）重叠，这表明我们有一个性能良好且预测准确的模型。
- en: Figure 15.9 shows that we have a performant model generating accurate predictions.
    Of course, this visualization is only three sampled sequences of 24 hours, so
    let’s visualize the model’s performance on the entire validation and test sets
    and compare it to the previous models we have built so far.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.9显示，我们有一个性能良好的模型，能够生成准确的预测。当然，这个可视化只是三个24小时序列的样本，所以让我们可视化模型在整个验证集和测试集上的性能，并将其与我们迄今为止构建的先前模型进行比较。
- en: '![](../../OEBPS/Images/15-10.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/15-10.png)'
- en: Figure 15.10 Mean absolute error (MAE) of all single-step models built so far.
    For now, the LSTM is the winning model, since it has the lowest MAE on both the
    validation and test sets.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.10显示了迄今为止构建的所有单步模型的平均绝对误差（MAE）。目前，LSTM是获胜模型，因为它在验证集和测试集上都具有最低的MAE。
- en: Figure 15.10 shows that the LSTM is the winning model, since it has the lowest
    MAE on both the validation and test sets, meaning that it generated the most accurate
    predictions of all the models.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.10显示，LSTM是获胜模型，因为它在验证集和测试集上都具有最低的MAE，这意味着它生成了所有模型中最准确的预测。
- en: 15.3.2 Implementing an LSTM as a multi-step model
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.2 实现一个多步LSTM模型
- en: We’ll move on to implementing the LSTM architecture as a multi-step model. In
    this case, we wish to predict traffic volume for next 24 hours, using an input
    window of 24 hours.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续实现LSTM架构作为多步模型。在这种情况下，我们希望使用24小时的输入窗口来预测未来24小时的交通量。
- en: First, we’ll define the time window to feed our model. The `input_width` and
    `label_width` are both 24, since we want to input 24 hours of data and evaluate
    the predictions on 24 hours of data as well. This time the `shift` is also 24,
    specifying that the model must output predictions for the next 24 hours in a single
    shot.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义输入模型的时间窗口。`input_width`和`label_width`都是24，因为我们想输入24小时的数据，并评估24小时的数据预测。这次`shift`也是24，指定模型必须一次性输出对未来24小时的预测。
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Next, we’ll define our model in Keras. From chapter 14, you might recall that
    the process of defining the multi-step model and single-step model was exactly
    the same. The same is true here. We still use the `Sequential` model, along with
    the `LSTM` layer and a `Dense` output layer with one unit.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用Keras定义我们的模型。从第14章，您可能还记得定义多步模型和单步模型的过程是完全相同的。这里也是一样。我们仍然使用`Sequential`模型，以及`LSTM`层和一个单元的`Dense`输出层。
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once it’s defined, we’ll train the model and store its evaluation metrics for
    comparison. By now, you should be comfortable with this workflow.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了模型，我们将对其进行训练并存储其评估指标以供比较。到目前为止，您应该已经熟悉了这个工作流程。
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can visualize the predictions of the model using the `plot` method, as shown
    in figure 15.11.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`plot`方法可视化模型的预测，如图15.11所示。
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../../OEBPS/Images/15-11.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/15-11.png)'
- en: Figure 15.11 Predicting the traffic volume over the next 24 hours using a multi-step
    LSTM model. We can see some discrepancies between the predictions and the labels.
    Of course, this visual inspection is not enough to assess the performance of the
    model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.11展示了使用多步LSTM模型预测未来24小时的交通量。我们可以看到预测和标签之间存在一些差异。当然，这种视觉检查不足以评估模型的性能。
- en: In figure 15.11 you’ll see that the predictions for the top sequence are very
    good, as most predictions overlap the actual values. However, there are some discrepancies
    between the output and labels in the bottom two sequences. Let’s compare its MAE
    to that of the other multi-step models we have built.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在图15.11中，您会看到对顶部序列的预测非常好，因为大多数预测与实际值重叠。然而，在底部两个序列的输出和标签之间存在一些差异。让我们将其MAE与其他我们构建的多步模型进行比较。
- en: '![](../../OEBPS/Images/15-12.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/15-12.png)'
- en: Figure 15.12 The MAE of all the multi-step models built so far. Again, the LSTM
    is the winning model, since it achieves the lowest MAE on both the validation
    and test sets.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.12显示了迄今为止构建的所有多步模型的MAE。同样，LSTM是获胜模型，因为它在验证集和测试集上都实现了最低的MAE。
- en: As you can see in figure 15.12, the LSTM is our most accurate model so far,
    as it achieved the lowest MAE on both the validation and test sets.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在图15.12中可以看到，LSTM是我们迄今为止最精确的模型，因为它在验证集和测试集上都实现了最低的MAE。
- en: 15.3.3 Implementing an LSTM as a multi-output model
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.3 实现作为多输出模型的 LSTM
- en: Finally, we’ll implement an LSTM as a multi-output model. Again, we’ll use 24
    hours of input data, so that the network can process a sequence of data points
    and use past information to produce forecasts. The predictions will be for both
    the traffic volume and temperature at the next timestep.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将实现一个 LSTM 作为多输出模型。同样，我们将使用 24 小时的输入数据，这样网络就可以处理数据点序列并使用过去的信息来生成预测。预测将是下一个时间步的交通量和温度。
- en: In this situation, the data window consists of an input of 24 timesteps and
    24 timesteps of labels. The `shift` is 1, as we want to produce forecasts for
    the next timestep only. Thus, our model will be creating rolling forecasts to
    generate predictions one timestep at a time, over 24 timesteps. We’ll specify
    temp and traffic_volume as our target columns.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，数据窗口由 24 个时间步长的输入和 24 个时间步长的标签组成。`shift` 为 1，因为我们只想预测下一个时间步。因此，我们的模型将创建滚动预测，一次生成一个时间步的预测，共
    24 个时间步。我们将 temp 和 traffic_volume 作为我们的目标列。
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The next step is to define our LSTM model. Just as before, we’ll use the `Sequential`
    model to stack an `LSTM` layer and a `Dense` output layer with two units, since
    we have two targets.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是定义我们的 LSTM 模型。就像之前一样，我们将使用 `Sequential` 模型堆叠一个 `LSTM` 层和一个具有两个单位的 `Dense`
    输出层，因为我们有两个目标。
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '❶ We have two units because we have two targets: the temperature and the traffic
    volume.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们有两个单位，因为我们有两个目标：温度和交通量。
- en: Then we’ll train the model and store its performance metrics for comparison.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将训练模型并存储其性能指标以进行比较。
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can now visualize the prediction for the traffic volume (figure 15.13) and
    temperature (figure 15.14). Both figures show many predictions (shown as crosses)
    overlapping the labels (shown as squares), which means that we have a performant
    model generating accurate predictions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以可视化交通量（图 15.13）和温度（图 15.14）的预测。这两个图都显示了大量的预测（以交叉表示）与标签（以正方形表示）重叠，这意味着我们有一个性能良好的模型，能够生成准确的预测。
- en: '![](../../OEBPS/Images/15-13.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/15-13.png)'
- en: Figure 15.13 Predicting the traffic volume with an LSTM as a multi-output model.
    Many predictions (shown as crosses) overlap the labels (shown as squares), suggesting
    very accurate predictions for the traffic volume.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.13 使用 LSTM 作为多输出模型预测交通量。许多预测（以交叉表示）与标签（以正方形表示）重叠，表明对交通量的预测非常准确。
- en: '![](../../OEBPS/Images/15-14.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/15-14.png)'
- en: Figure 15.14 Predicting the temperature using an LSTM as a multi-output model.
    Again, we see a lot of overlap between the predictions (shown as crosses) and
    the labels (shown as squares), indicating accurate predictions.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.14 使用 LSTM 作为多输出模型预测温度。同样，我们看到预测（以交叉表示）和标签（以正方形表示）之间有很多重叠，这表明预测是准确的。
- en: Let’s compare our LSTM model’s performance to the other multi-output models
    built so far. Figure 15.15 again shows the LSTM as the winning model, since it
    achieves the lowest MAE on the validation and test sets. Thus, it generated the
    most accurate predictions so far for both our targets.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较我们的 LSTM 模型的性能与其他到目前为止构建的多输出模型。图 15.15 再次显示 LSTM 是获胜模型，因为它在验证集和测试集上实现了最低的
    MAE。因此，它为我们两个目标生成了迄今为止最准确的预测。
- en: '![](../../OEBPS/Images/15-15.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/15-15.png)'
- en: Figure 15.15 The MAE of all the multi-output models built so far. Again, the
    winning model is the LSTM, as it achieved the lowest MAE of all.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.15 到目前为止构建的所有多输出模型的平均绝对误差（MAE）。同样，获胜模型是 LSTM，因为它实现了所有模型中最低的 MAE。
- en: 15.4 Next steps
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.4 下一步
- en: In this chapter, we examined the long short-term memory (LSTM) architecture.
    You learned that it is a subtype of RNN, and you saw how it uses a cell state
    to overcome the problem of short-term memory that occurs in a basic RNN that uses
    only the hidden state.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了长短期记忆（LSTM）架构。你了解到它是一种 RNN 的子类型，并看到了它是如何使用细胞状态来克服基本 RNN 中出现的短期记忆问题的。
- en: We also studied the three gates of the LSTM. The forget gate determines which
    information from the past and present must be kept, the input gate determines
    the relevant information from the current element of a sequence, and the output
    gate uses the information stored in memory to generate a prediction.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还研究了 LSTM 的三个门。遗忘门确定哪些过去和现在的信息必须保留，输入门确定序列当前元素的相关信息，输出门使用存储在内存中的信息来生成预测。
- en: We then implemented the LSTM as a single-step model, multi-step model, and multi-output
    model. In all cases, the LSTM was the winning model, as it achieved the lowest
    MAE of all models built so far.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后实现了LSTM作为单步模型、多步模型和多输出模型。在所有情况下，LSTM都是获胜模型，因为它实现了迄今为止所有构建的模型中最低的MAE。
- en: The deep learning architecture that we will explore in the next chapter is the
    *convolutional neural network* (CNN). You might have come across a CNN, especially
    in computer vision, as it is a very popular architecture for analyzing pictures.
    We will apply it for time series forecasting, as CNNs are faster to train than
    LSTMs, they are robust to noise, and they are good feature extractors.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章中探讨的深度学习架构是卷积神经网络（CNN）。你可能在计算机视觉中遇到过CNN，因为它是一种非常流行的用于分析图片的架构。我们将将其应用于时间序列预测，因为CNN比LSTM训练更快，对噪声鲁棒，并且是好的特征提取器。
- en: 15.5 Exercises
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.5 练习
- en: 'In the last chapter, we built linear models and deep neural networks to forecast
    the air quality. Now we’ll try LSTM models and see if there is a gain in performance.
    The solution to these exercises can be found on GitHub: [https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH15](https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH15).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们构建了线性模型和深度神经网络来预测空气质量。现在我们将尝试LSTM模型，看看是否在性能上有提升。这些练习的解决方案可以在GitHub上找到：[https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH15](https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH15)。
- en: 'For the single-step model:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于单步模型：
- en: Build an LSTM model.
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个LSTM模型。
- en: Plot its predictions.
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制其预测。
- en: Evaluate it using the MAE and store the MAE.
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MAE评估它并存储MAE。
- en: Is it the most performant model?
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它是最有效的模型吗？
- en: 'For the multi-step model:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于多步模型：
- en: Build an LSTM model.
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个LSTM模型。
- en: Plot its predictions.
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制其预测。
- en: Evaluate it using the MAE and store the MAE.
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MAE评估它并存储MAE。
- en: Is it the most performant model?
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它是最有效的模型吗？
- en: 'For the multi-output model:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于多输出模型：
- en: Build an LSTM model.
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个LSTM模型。
- en: Plot its predictions.
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制其预测。
- en: Evaluate it using the MAE and store the MAE.
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MAE评估它并存储MAE。
- en: Is it the most performant model?
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它是最有效的模型吗？
- en: 'At any point, try to experiment with the following ideas:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何时候，尝试以下想法进行实验：
- en: Add more `LSTM` layers.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加更多的`LSTM`层。
- en: Change the number of units in the `LSTM` layer.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变`LSTM`层的单元数量。
- en: Set `return_sequences` to `False`.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`return_sequences`设置为`False`。
- en: Experiment with different initializers in the output `Dense` layer.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在输出`Dense`层尝试不同的初始化器。
- en: Run as many experiments as you want, and see how they impact the error metric.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行尽可能多的实验，并观察它们对误差指标的影响。
- en: Summary
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A recurrent neural network (RNN) is a deep learning architecture especially
    adapted to processing sequences of data like a time series.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）是一种特别适合处理时间序列等数据序列的深度学习架构。
- en: RNNs use a hidden state to store information in memory. However, this is only
    short-term memory due to the vanishing gradient problem.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN使用隐藏状态在内存中存储信息。然而，由于梯度消失问题，这仅是短期记忆。
- en: Long short-term memory (LSTM) is a type of RNN that addresses the short-term
    memory problem. It uses a cell state to store information for a longer time, giving
    the network a *long* memory.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）是一种解决短期记忆问题的RNN。它使用细胞状态来存储信息更长时间，给网络一个*长*的记忆。
- en: 'The LSTM is made of three gates:'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM由三个门组成：
- en: The forget gate determines what information from the past and present must be
    kept.
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遗忘门决定哪些过去和现在的信息必须被保留。
- en: The input gate determines what information from the present must be kept.
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入门决定哪些现在的信息必须被保留。
- en: The output gate uses information stored in memory to process the current element
    of a sequence.
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出门使用存储在内存中的信息来处理序列的当前元素。
