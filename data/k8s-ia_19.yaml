- en: Chapter 17\. Best practices for developing apps
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第 17 章. 开发应用程序的最佳实践
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding which Kubernetes resources appear in a typical application
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解在典型应用程序中哪些 Kubernetes 资源出现
- en: Adding post-start and pre-stop pod lifecycle hooks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加 post-start 和 pre-stop Pod 生命周期钩子
- en: Properly terminating an app without breaking client requests
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确终止应用程序而不会破坏客户端请求
- en: Making apps easy to manage in Kubernetes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使应用程序在 Kubernetes 中易于管理
- en: Using init containers in a pod
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Pod 中使用 init 容器
- en: Developing locally with Minikube
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Minikube 在本地开发
- en: We’ve now covered most of what you need to know to run your apps in Kubernetes.
    We’ve explored what each individual resource does and how it’s used. Now we’ll
    see how to combine them in a typical application running on Kubernetes. We’ll
    also look at how to make an application run smoothly. After all, that’s the whole
    point of using Kubernetes, isn’t it?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经涵盖了您在 Kubernetes 中运行应用程序所需了解的大部分内容。我们已经探讨了每个单独的资源做什么以及如何使用它。现在我们将看到如何在
    Kubernetes 上运行的典型应用程序中将它们结合起来。我们还将看看如何使应用程序运行顺畅。毕竟，这就是使用 Kubernetes 的全部意义，对吧？
- en: Hopefully, this chapter will help to clear up any misunderstandings and explain
    things that weren’t explained clearly yet. Along the way, we’ll also introduce
    a few additional concepts that haven’t been mentioned up to this point.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 希望本章能帮助澄清任何误解，并解释那些尚未明确说明的事情。在这个过程中，我们还将介绍一些到目前为止尚未提到的额外概念。
- en: 17.1\. Bringing everything together
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 17.1. 将一切整合
- en: Let’s start by looking at what an actual application consists of. This will
    also give you a chance to see if you remember everything you’ve learned so far
    and look at the big picture. [Figure 17.1](#filepos1550485) shows the Kubernetes
    components used in a typical application.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看一个实际应用程序由什么组成。这将也给你一个机会看看你是否记得你所学到的所有内容，并看看大局。![图 17.1](#filepos1550485)显示了在典型应用程序中使用的
    Kubernetes 组件。
- en: Figure 17.1\. Resources in a typical application
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.1. 典型应用程序中的资源
- en: '![](images/00095.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片 00095](images/00095.jpg)'
- en: A typical application manifest contains one or more Deployment and/or StatefulSet
    objects. Those include a pod template containing one or more containers, with
    a liveness probe for each of them and a readiness probe for the service(s) the
    container provides (if any). Pods that provide services to others are exposed
    through one or more Services. When they need to be reachable from outside the
    cluster, the Services are either configured to be `LoadBalancer` or `NodePort`-type
    Services, or exposed through an Ingress resource.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的应用程序清单包含一个或多个 Deployment 和/或 StatefulSet 对象。这些对象包括一个包含一个或多个容器的 Pod 模板，每个容器都有一个存活探针，以及为容器提供的（如果有的话）服务（s）的就绪探针。为其他
    Pod 提供服务的 Pod 通过一个或多个 Service 进行暴露。当它们需要从集群外部可达时，Service 可以配置为 `LoadBalancer`
    或 `NodePort` 类型的 Service，或者通过 Ingress 资源进行暴露。
- en: The pod templates (and the pods created from them) usually reference two types
    of Secrets—those for pulling container images from private image registries and
    those used directly by the process running inside the pods. The Secrets themselves
    are usually not part of the application manifest, because they aren’t configured
    by the application developers but by the operations team. Secrets are usually
    assigned to Service-Accounts, which are assigned to individual pods.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 模板（以及从中创建的 Pod）通常引用两种类型的 Secrets——用于从私有镜像仓库拉取容器镜像的 Secrets 和直接由 Pod 内运行的进程使用的
    Secrets。Secrets 本身通常不是应用程序清单的一部分，因为它们不是由应用程序开发者配置的，而是由运维团队配置的。Secrets 通常分配给 Service-Accounts，这些
    Service-Accounts 被分配给单个 Pod。
- en: The application also contains one or more ConfigMaps, which are either used
    to initialize environment variables or mounted as a `configMap` volume in the
    pod. Certain pods use additional volumes, such as an `emptyDir` or a `gitRepo`
    volume, whereas pods requiring persistent storage use `persistentVolumeClaim`
    volumes. The Persistent-VolumeClaims are also part of the application manifest,
    whereas StorageClasses referenced by them are created by system administrators
    upfront.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序还包含一个或多个 ConfigMaps，这些 ConfigMaps 既可以用来初始化环境变量，也可以作为 `configMap` 卷挂载到 Pod
    中。某些 Pod 使用额外的卷，例如 `emptyDir` 或 `gitRepo` 卷，而需要持久存储的 Pod 则使用 `persistentVolumeClaim`
    卷。Persistent-VolumeClaims 也是应用程序清单的一部分，而它们所引用的 StorageClasses 则是由系统管理员预先创建的。
- en: In certain cases, an application also requires the use of Jobs or CronJobs.
    DaemonSets aren’t normally part of application deployments, but are usually created
    by sysadmins to run system services on all or a subset of nodes. HorizontalPodAutoscalers
    are either included in the manifest by the developers or added to the system later
    by the ops team. The cluster administrator also creates LimitRange and ResourceQuota
    objects to keep compute resource usage of individual pods and all the pods (as
    a whole) under control.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，应用程序还需要使用Jobs或CronJobs。DaemonSets通常不是应用程序部署的一部分，但通常由系统管理员创建，以在所有或部分节点上运行系统服务。HorizontalPodAutoscalers要么由开发者包含在清单中，要么由运维团队在系统后期添加。集群管理员还会创建LimitRange和ResourceQuota对象，以保持单个Pod和所有Pod（作为一个整体）的计算资源使用量在可控范围内。
- en: After the application is deployed, additional objects are created automatically
    by the various Kubernetes controllers. These include service Endpoints objects
    created by the Endpoints controller, ReplicaSets created by the Deployment controller,
    and the actual pods created by the ReplicaSet (or Job, CronJob, StatefulSet, or
    Daemon-Set) controllers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序部署后，各种Kubernetes控制器会自动创建额外的对象。这包括由Endpoints控制器创建的服务端点对象，由Deployment控制器创建的ReplicaSet，以及由ReplicaSet（或Job、CronJob、StatefulSet或Daemon-Set）控制器实际创建的Pod。
- en: Resources are often labeled with one or more labels to keep them organized.
    This doesn’t apply only to pods but to all other resources as well. In addition
    to labels, most resources also contain annotations that describe each resource,
    list the contact information of the person or team responsible for it, or provide
    additional metadata for management and other tools.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 资源通常被标记为一个或多个标签以保持其组织有序。这不仅适用于Pod，也适用于所有其他资源。除了标签之外，大多数资源还包含描述每个资源的注释，列出负责该资源的人员或团队的联系方式，或为管理和其他工具提供额外的元数据。
- en: At the center of all this is the Pod, which arguably is the most important Kubernetes
    resource. After all, each of your applications runs inside it. To make sure you
    know how to develop apps that make the most out of their environment, let’s take
    one last close look at pods—this time from the application’s perspective.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些的中心是Pod，可以说是Kubernetes最重要的资源。毕竟，每个应用程序都在其中运行。为了确保你知道如何开发能够充分利用其环境的应用程序，让我们最后一次仔细看看Pod——这次是从应用程序的角度来看。
- en: 17.2\. Understanding the pod’s lifecycle
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 17.2. 理解Pod的生命周期
- en: We’ve said that pods can be compared to VMs dedicated to running only a single
    application. Although an application running inside a pod is not unlike an application
    running in a VM, significant differences do exist. One example is that apps running
    in a pod can be killed any time, because Kubernetes needs to relocate the pod
    to another node for a reason or because of a scale-down request. We’ll explore
    this aspect next.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经说过，Pod可以与仅运行单个应用的VM进行比较。尽管在Pod内运行的应用程序与在VM中运行的应用程序没有太大区别，但确实存在一些显著差异。一个例子是，在Pod中运行的应用程序可以随时被终止，因为Kubernetes需要将Pod重新定位到另一个节点，原因可能是出于某种原因，也可能是由于缩放请求。我们将在下一节探讨这个方面。
- en: 17.2.1\. Applications must expect to be killed and relocated
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 17.2.1. 应用程序必须预期会被终止和重新定位
- en: Outside Kubernetes, apps running in VMs are seldom moved from one machine to
    another. When an operator moves the app, they can also reconfigure the app and
    manually check that the app is running fine in the new location. With Kubernetes,
    apps are relocated much more frequently and automatically—no human operator reconfigures
    them and makes sure they still run properly after the move. This means application
    developers need to make sure their apps allow being moved relatively often.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes之外，运行在VM中的应用程序很少从一个机器移动到另一个机器。当操作员移动应用程序时，他们也可以重新配置应用程序，并手动检查应用程序在新位置是否运行良好。在Kubernetes中，应用程序的迁移更加频繁和自动——没有人类操作员重新配置它们并确保迁移后仍然正常运行。这意味着应用程序开发者需要确保他们的应用程序允许相对频繁地移动。
- en: Expecting the local IP and hostname to change
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 预期本地IP和主机名会发生变化
- en: When a pod is killed and run elsewhere (technically, it’s a new pod instance
    replacing the old one; the pod isn’t relocated), it not only has a new IP address
    but also a new name and hostname. Most stateless apps can usually handle this
    without any adverse effects, but stateful apps usually can’t. We’ve learned that
    stateful apps can be run through a StatefulSet, which ensures that when the app
    starts up on a new node after being rescheduled, it will still see the same host
    name and persistent state as before. The pod’s IP will change nevertheless. Apps
    need to be prepared for that to happen. The application developer therefore should
    never base membership in a clustered app on the member’s IP address, and if basing
    it on the hostname, should always use a StatefulSet.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个Pod被终止并在其他地方运行时（技术上，这是一个新的Pod实例替换了旧的Pod；Pod并没有被重新定位），它不仅有一个新的IP地址，还有一个新的名称和主机名。大多数无状态应用程序通常可以处理这种情况而不会产生任何不利影响，但状态化应用程序通常不能。我们已经了解到，可以通过StatefulSet运行状态化应用程序，这确保了当应用程序在重新调度后在新的节点上启动时，它仍然会看到之前相同的宿主机名和持久状态。尽管如此，Pod的IP地址仍然会改变。应用程序需要为此做好准备。因此，应用程序开发者永远不应该基于集群应用程序成员的IP地址来确定成员资格，如果基于主机名，则始终应使用StatefulSet。
- en: Expecting the data written to disk to disappear
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 期待写入磁盘的数据消失
- en: Another thing to keep in mind is that if the app writes data to disk, that data
    may not be available after the app is started inside a new pod, unless you mount
    persistent storage at the location the app is writing to. It should be clear this
    happens when the pod is rescheduled, but files written to disk will disappear
    even in scenarios that don’t involve any rescheduling. Even during the lifetime
    of a single pod, the files written to disk by the app running in the pod may disappear.
    Let me explain this with an example.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一点需要记住的是，如果应用程序将数据写入磁盘，那么在应用程序在新的Pod内部启动后，这些数据可能不可用，除非你在应用程序写入数据的位置挂载持久存储。应该清楚，当Pod重新调度时会发生这种情况，但即使在不涉及任何重新调度的场景中，写入磁盘的文件也会消失。即使在单个Pod的生命周期内，Pod中运行的应用程序写入磁盘的文件也可能消失。让我用一个例子来解释这一点。
- en: Imagine an app that has a long and computationally intensive initial startup
    procedure. To help the app come up faster on subsequent startups, the developers
    make the app cache the results of the initial startup on disk (an example of this
    would be the scanning of all Java classes for annotations at startup and then
    writing the results to an index file). Because apps in Kubernetes run in containers
    by default, these files are written to the container’s filesystem. If the container
    is then restarted, they’re all lost, because the new container starts off with
    a completely new writable layer (see [figure 17.2](#filepos1557529)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个应用程序有一个漫长且计算密集型的初始启动过程。为了帮助应用程序在后续启动时更快地启动，开发者将应用程序的初始启动结果缓存到磁盘上（例如，在启动时扫描所有Java类以查找注解，然后将结果写入索引文件）。由于Kubernetes中的应用程序默认在容器中运行，这些文件会被写入容器的文件系统。如果容器随后被重启，它们都会丢失，因为新的容器从完全新的可写层开始（参见[图17.2](#filepos1557529)）。
- en: Figure 17.2\. Files written to the container’s filesystem are lost when the
    container is restarted.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.2\. 当容器重启时，写入容器文件系统的文件会丢失。
- en: '![](images/00112.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00112.jpg)'
- en: Don’t forget that individual containers may be restarted for several reasons,
    such as because the process crashes, because the liveness probe returned a failure,
    or because the node started running out of memory and the process was killed by
    the OOMKiller. When this happens, the pod is still the same, but the container
    itself is completely new. The Kubelet doesn’t run the same container again; it
    always creates a new container.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记，单个容器可能由于多种原因被重启，例如进程崩溃、存活性检查返回失败，或者因为节点开始运行内存不足，进程被OOMKiller杀死。当这种情况发生时，Pod仍然是相同的，但容器本身是完全新的。Kubelet不会再次运行相同的容器；它总是创建一个新的容器。
- en: Using volumes to preserve data across container restarts
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用卷来在容器重启之间保留数据
- en: When its container is restarted, the app in the example will need to perform
    the intensive startup procedure again. This may or may not be desired. To make
    sure data like this isn’t lost, you need to use at least a pod-scoped volume.
    Because volumes live and die together with the pod, the new container will be
    able to reuse the data written to the volume by the previous container ([figure
    17.3](#filepos1558844)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当其容器重启时，示例中的应用程序需要再次执行密集的启动程序。这可能或可能不是期望的。为了确保像这样的数据不会丢失，你需要至少使用一个Pod作用域的卷。因为卷与Pod共存亡，新的容器将能够重用前一个容器写入卷的数据（[图17.3](#filepos1558844)）。
- en: Figure 17.3\. Using a volume to persist data across container restarts
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.3\. 使用卷在容器重启之间持久化数据
- en: '![](images/00132.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00132.jpg)'
- en: Using a volume to preserve files across container restarts is a great idea sometimes,
    but not always. What if the data gets corrupted and causes the newly created process
    to crash again? This will result in a continuous crash loop (the pod will show
    the `CrashLoopBackOff` status). If you hadn’t used a volume, the new container
    would start from scratch and most likely not crash. Using volumes to preserve
    files across container restarts like this is a double-edged sword. You need to
    think carefully about whether to use them or not.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用卷在容器重启之间保留文件有时是个好主意，但并不总是如此。如果数据被损坏并导致新创建的进程再次崩溃怎么办？这将导致持续崩溃循环（Pod将显示`CrashLoopBackOff`状态）。如果你没有使用卷，新容器将从头开始启动，并且很可能会崩溃。像这样使用卷在容器重启之间保留文件是一把双刃剑。你需要仔细考虑是否使用它们。
- en: 17.2.2\. Rescheduling of dead or partially dead pods
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 17.2.2\. 已死亡或部分死亡Pod的重新调度
- en: If a pod’s container keeps crashing, the Kubelet will keep restarting it indefinitely.
    The time between restarts will be increased exponentially until it reaches five
    minutes. During those five minute intervals, the pod is essentially dead, because
    its container’s process isn’t running. To be fair, if it’s a multi-container pod,
    certain containers may be running normally, so the pod is only partially dead.
    But if a pod contains only a single container, the pod is effectively dead and
    completely useless, because no process is running in it anymore.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个Pod的容器持续崩溃，Kubelet会无限期地重启它。重启之间的时间将以指数级增加，直到达到五分钟。在这五分钟的时间间隔内，Pod实际上已经死亡，因为其容器的进程没有运行。公平地说，如果这是一个多容器Pod，某些容器可能正常运行，所以Pod只是部分死亡。但如果Pod只包含一个容器，Pod实际上已经死亡，完全无用，因为其中不再有进程运行。
- en: You may find it surprising to learn that such pods aren’t automatically removed
    and rescheduled, even if they’re part of a ReplicaSet or similar controller. If
    you create a ReplicaSet with a desired replica count of three, and then one of
    the containers in one of those pods starts crashing, Kubernetes will not delete
    and replace the pod. The end result is a ReplicaSet with only two properly running
    replicas instead of the desired three ([figure 17.4](#filepos1560870)).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会惊讶地发现，即使这些Pod是ReplicaSet或类似控制器的一部分，它们也不会自动被移除并重新调度。如果你创建一个期望副本数为三的ReplicaSet，然后其中一个Pod中的一个容器开始崩溃，Kubernetes不会删除并替换Pod。最终结果是ReplicaSet只有两个正确运行的副本，而不是期望的三（[图17.4](#filepos1560870)）。
- en: Figure 17.4\. A ReplicaSet controller doesn’t reschedule dead pods.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.4\. ReplicaSet控制器不会重新调度已死亡的Pod。
- en: '![](images/00149.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00149.jpg)'
- en: You’d probably expect the pod to be deleted and replaced with another pod instance
    that might run successfully on another node. After all, the container may be crashing
    because of a node-related problem that doesn’t manifest itself on other nodes.
    Sadly, that isn’t the case. The ReplicaSet controller doesn’t care if the pods
    are dead—all it cares about is that the number of pods matches the desired replica
    count, which in this case, it does.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能预期Pod会被删除，并替换为另一个可能在其他节点上成功运行的Pod实例。毕竟，容器可能因为与节点相关的问题而崩溃，这个问题在其他节点上没有表现出来。遗憾的是，情况并非如此。ReplicaSet控制器不在乎Pod是否已死亡——它只关心Pod的数量是否与期望的副本数匹配，在这种情况下，它确实匹配。
- en: If you’d like to see for yourself, I’ve included a YAML manifest for a ReplicaSet
    whose pods will keep crashing (see file replicaset-crashingpods.yaml in the code
    archive). If you create the ReplicaSet and inspect the pods that are created,
    the following listing is what you’ll see.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想亲自查看，我已包含一个ReplicaSet的YAML清单，其Pod将不断崩溃（请参阅代码存档中的文件replicaset-crashingpods.yaml）。如果你创建了ReplicaSet并检查创建的Pod，你将看到以下列表。
- en: Listing 17.1\. ReplicaSet and pods that keep crashing
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.1\. ReplicaSet和持续崩溃的Pod
- en: '`$ kubectl get po` `NAME                  READY     STATUS             RESTARTS  
    AGE crashing-pods-f1tcd   0/1` `CrashLoopBackOff``5          6m` `1` `crashing-pods-k7l6k  
    0/1       CrashLoopBackOff   5          6m crashing-pods-z7l3v   0/1       CrashLoopBackOff  
    5          6m` `$ kubectl describe rs crashing-pods` `Name:           crashing-pods
    Replicas:` `3 current` `/` `3 desired``2` `Pods Status:` `3 Running` `/ 0 Waiting
    / 0 Succeeded / 0 Failed` `3``$ kubectl describe po crashing-pods-f1tcd` `Name:          
    crashing-pods-f1tcd Namespace:      default Node:           minikube/192.168.99.102
    Start Time:     Thu, 02 Mar 2017 14:02:23 +0100 Labels:         app=crashing-pods
    Status:         Running` `4`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po` `NAME                  READY     STATUS             RESTARTS  
    AGE crashing-pods-f1tcd   0/1` `CrashLoopBackOff``5          6m` `1` `crashing-pods-k7l6k  
    0/1       CrashLoopBackOff   5          6m crashing-pods-z7l3v   0/1       CrashLoopBackOff  
    5          6m` `$ kubectl describe rs crashing-pods` `Name:           crashing-pods
    Replicas:` `3 current` `/` `3 desired``2` `Pods Status:` `3 Running` `/ 0 Waiting
    / 0 Succeeded / 0 Failed` `3``$ kubectl describe po crashing-pods-f1tcd` `Name:          
    crashing-pods-f1tcd Namespace:      default Node:           minikube/192.168.99.102
    Start Time:     Thu, 02 Mar 2017 14:02:23 +0100 Labels:         app=crashing-pods
    Status:         Running` `4`'
- en: 1 The pod’s status shows the Kubelet is delaying the restart because the container
    keeps crashing.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 Pod的状态显示Kubelet正在延迟重启，因为容器持续崩溃。
- en: 2 No action taken by the controller, because current replicas match desired
    replicas
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 控制器没有采取任何行动，因为当前副本数与期望副本数匹配
- en: 3 Three replicas are shown as running.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 显示了3个副本正在运行。
- en: 4 kubectl describe also shows pod’s status as running
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4 kubectl describe同样显示Pod的状态为运行
- en: In a way, it’s understandable that Kubernetes behaves this way. The container
    will be restarted every five minutes in the hope that the underlying cause of
    the crash will be resolved. The rationale is that rescheduling the pod to another
    node most likely wouldn’t fix the problem anyway, because the app is running inside
    a container and all the nodes should be mostly equivalent. That’s not always the
    case, but it is most of the time.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种程度上，Kubernetes以这种方式运行是可以理解的。容器每五分钟会重启一次，希望解决崩溃的根本原因。其逻辑是，将Pod重新调度到另一个节点最可能也无法解决问题，因为应用程序运行在容器内，所有节点应该大致相同。这并不总是情况，但大多数情况下是这样的。
- en: 17.2.3\. Starting pods in a specific order
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 17.2.3\. 按特定顺序启动Pod
- en: One other difference between apps running in pods and those managed manually
    is that the ops person deploying those apps knows about the dependencies between
    them. This allows them to start the apps in order.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Pod中运行的应用程序与手动管理中的应用程序之间还有一个区别是，部署这些应用的运维人员了解它们之间的依赖关系。这使得他们可以按顺序启动应用程序。
- en: Understanding how pods are started
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 理解Pod的启动过程
- en: When you use Kubernetes to run your multi-pod applications, you don’t have a
    built-in way to tell Kubernetes to run certain pods first and the rest only when
    the first pods are already up and ready to serve. Sure, you could post the manifest
    for the first app and then wait for the pod(s) to be ready before you post the
    second manifest, but your whole system is usually defined in a single YAML or
    JSON containing multiple Pods, Services, and other objects.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用Kubernetes运行你的多Pod应用程序时，你没有内置的方式来告诉Kubernetes先运行某些Pod，其余的只有在第一个Pod已经启动并准备好服务时才运行。当然，你可以在发布第一个应用的清单之后等待Pod准备好，然后再发布第二个清单，但你的整个系统通常定义在单个YAML或JSON文件中，包含多个Pod、服务和其他对象。
- en: The Kubernetes API server does process the objects in the YAML/JSON in the order
    they’re listed, but this only means they’re written to etcd in that order. You
    have no guarantee that pods will also be started in that order.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes API服务器确实会按照列表中的顺序处理YAML/JSON中的对象，但这仅仅意味着它们会按照这个顺序写入etcd。你不能保证Pod也会按照这个顺序启动。
- en: But you can prevent a pod’s main container from starting until a precondition
    is met. This is done by including an init containers in the pod.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，你可以通过在Pod中包含一个init容器来防止Pod的主容器在满足预条件之前启动。
- en: Introducing Init Containers
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 引入Init容器
- en: In addition to regular containers, pods can also include init containers. As
    the name suggests, they can be used to initialize the pod—this often means writing
    data to the pod’s volumes, which are then mounted into the pod’s main container(s).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 除了常规容器之外，Pod还可以包含init容器。正如其名所示，它们可以用来初始化Pod——这通常意味着将数据写入Pod的卷，然后这些卷会被挂载到Pod的主容器中。
- en: A pod may have any number of init containers. They’re executed sequentially
    and only after the last one completes are the pod’s main containers started. This
    means init containers can also be used to delay the start of the pod’s main container(s)—for
    example, until a certain precondition is met. An init container could wait for
    a service required by the pod’s main container to be up and ready. When it is,
    the init container terminates and allows the main container(s) to be started.
    This way, the main container wouldn’t use the service before it’s ready.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 pod 可以有任意数量的初始化容器。它们按顺序执行，并且只有最后一个完成之后，pod 的主容器才会启动。这意味着初始化容器也可以用来延迟 pod
    主容器的启动——例如，直到满足某个先决条件。初始化容器可以等待 pod 主容器所需的服务启动并就绪。当它就绪时，初始化容器终止，并允许主容器启动。这样，主容器就不会在服务就绪之前使用该服务。
- en: Let’s look at an example of a pod using an init container to delay the start
    of the main container. Remember the `fortune` pod you created in [chapter 7](index_split_063.html#filepos687721)?
    It’s a web server that returns a fortune quote as a response to client requests.
    Now, let’s imagine you have a `fortune-client` pod that requires the `fortune`
    Service to be up and running before its main container starts. You can add an
    init container, which checks whether the Service is responding to requests. Until
    that’s the case, the init container keeps retrying. Once it gets a response, the
    init container terminates and lets the main container start.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个使用初始化容器来延迟主容器启动的 pod 示例。还记得你在[第 7 章](index_split_063.html#filepos687721)中创建的
    `fortune` pod 吗？它是一个返回幸运名言作为对客户端请求响应的 Web 服务器。现在，让我们假设你有一个 `fortune-client` pod，它需要在主容器启动之前，`fortune`
    服务必须处于运行状态。你可以添加一个初始化容器，该容器会检查服务是否对请求做出响应。在得到响应之前，初始化容器会不断重试。一旦得到响应，初始化容器就会终止，并允许主容器启动。
- en: Adding an Init Container to a pod
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 将初始化容器添加到 pod 中
- en: Init containers can be defined in the pod spec like main containers but through
    the `spec.initContainers` field. You’ll find the complete YAML for the fortune-client
    pod in the book’s code archive. The following listing shows the part where the
    init container is defined.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化容器可以在 pod 规范中定义，就像主容器一样，但通过 `spec.initContainers` 字段。你可以在本书的代码存档中找到 fortune-client
    pod 的完整 YAML。以下列表显示了定义初始化容器的部分。
- en: 'Listing 17.2\. An init container defined in a pod: fortune-client.yaml'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.2\. 在 pod 中定义的初始化容器：fortune-client.yaml
- en: '`spec:   initContainers:` `1` `- name: init     image: busybox     command:
        - sh     - -c     - ''while true; do echo "Waiting for fortune service to
    come up...";` `2`![](images/00006.jpg) `wget http://fortune -q -T 1 -O /dev/null
    >/dev/null 2>/dev/null` `2`![](images/00006.jpg) `&& break; sleep 1; done; echo
    "Service is up! Starting main` `2`![](images/00006.jpg) `container."''`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`spec:    initContainers:` `1` `- name: init    image: busybox    command:    -
    sh    - -c    - ''while true; do echo "等待 fortune 服务启动...";'' `2`![](images/00006.jpg)
    `wget http://fortune -q -T 1 -O /dev/null >/dev/null 2>/dev/null` `2`![](images/00006.jpg)
    `&& break; sleep 1; done; echo "服务已启动！启动主` `2`![](images/00006.jpg) `容器。"``'
- en: 1 You’re defining an init container, not a regular container.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 你正在定义一个初始化容器，而不是一个普通容器。
- en: 2 The init container runs a loop that runs until the fortune Service is up.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 初始化容器运行一个循环，直到 fortune 服务启动。
- en: 'When you deploy this pod, only its init container is started. This is shown
    in the pod’s status when you list pods with `kubectl get`:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当你部署此 pod 时，只有它的初始化容器会启动。这在你使用 `kubectl get` 列出 pod 时 pod 的状态中显示：
- en: '`$ kubectl get po` `NAME             READY` `STATUS``RESTARTS   AGE fortune-client  
    0/1` `Init:0/1``   0          1m`'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po` `NAME             READY` `STATUS` `RESTARTS` `AGE fortune-client  
    0/1` `Init:0/1` `0` `1m`'
- en: 'The `STATUS` column shows that zero of one init containers have finished. You
    can see the log of the init container with `kubectl logs`:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`STATUS` 列显示零个或一个初始化容器已完成。你可以使用 `kubectl logs` 查看初始化容器的日志：'
- en: '`$ kubectl logs fortune-client -c init` `Waiting for fortune service to come
    up...`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl logs fortune-client -c init` `等待 fortune 服务启动...`'
- en: When running the `kubectl logs` command, you need to specify the name of the
    init container with the `-c` switch (in the example, the name of the pod’s init
    container is `init`, as you can see in [listing 17.2](#filepos1568053)).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行 `kubectl logs` 命令时，你需要使用 `-c` 开关指定初始化容器的名称（在示例中，pod 的初始化容器名称为 `init`，如你在[列表
    17.2](#filepos1568053)中看到的）。
- en: The main container won’t run until you deploy the `fortune` Service and the
    `fortune-server` pod. You’ll find them in the fortune-server.yaml file.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 主容器将在你部署 `fortune` 服务和 `fortune-server` pod 之后才运行。你可以在 fortune-server.yaml 文件中找到它们。
- en: Best practices for handling inter-pod dependencies
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 处理 pod 间依赖关系的最佳实践
- en: You’ve seen how an init container can be used to delay starting the pod’s main
    container(s) until a precondition is met (making sure the Service the pod depends
    on is ready, for example), but it’s much better to write apps that don’t require
    every service they rely on to be ready before the app starts up. After all, the
    service may also go offline later, while the app is already running.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到了如何使用初始化容器来延迟启动 pod 的主容器（直到满足某个条件，例如确保 pod 所依赖的服务已就绪），但编写不需要在应用启动前依赖的所有服务都就绪的应用程序会更好。毕竟，服务也可能在应用已经运行后离线。
- en: The application needs to handle internally the possibility that its dependencies
    aren’t ready. And don’t forget readiness probes. If an app can’t do its job because
    one of its dependencies is missing, it should signal that through its readiness
    probe, so Kubernetes knows it, too, isn’t ready. You’ll want to do this not only
    because it prevents the app from being added as a service endpoint, but also because
    the app’s readiness is also used by the Deployment controller when performing
    a rolling update, thereby preventing a rollout of a bad version.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 应用需要内部处理其依赖项可能未就绪的可能性。别忘了就绪性探针。如果一个应用因为其依赖项之一缺失而无法执行其工作，它应该通过其就绪性探针发出信号，这样 Kubernetes
    就知道它也不就绪。你想要这样做不仅因为这样可以防止应用被添加为服务端点，而且因为应用的就绪性也被 Deployment 控制器在执行滚动更新时使用，从而防止推出一个坏版本。
- en: 17.2.4\. Adding lifecycle hooks
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 17.2.4. 添加生命周期钩子
- en: 'We’ve talked about how init containers can be used to hook into the startup
    of the pod, but pods also allow you to define two lifecycle hooks:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了如何使用初始化容器来挂钩 pod 的启动，但 pod 也允许你定义两个生命周期钩子：
- en: Post-start hooks
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动后钩子
- en: Pre-stop hooks
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预停止钩子
- en: These lifecycle hooks are specified per container, unlike init containers, which
    apply to the whole pod. As their names suggest, they’re executed when the container
    starts and before it stops.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这些生命周期钩子是针对每个容器指定的，与初始化容器不同，初始化容器适用于整个 pod。正如它们的名称所暗示的，它们在容器启动时执行，在容器停止前执行。
- en: Lifecycle hooks are similar to liveness and readiness probes in that they can
    either
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 生命周期钩子与存活性和就绪性探针类似，因为它们可以
- en: Execute a command inside the container
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在容器内执行命令
- en: Perform an HTTP GET request against a URL
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对一个 URL 执行 HTTP GET 请求
- en: Let’s look at the two hooks individually to see what effect they have on the
    container lifecycle.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分别看看这两个钩子，看看它们对容器生命周期有什么影响。
- en: Using a post-start container lifecycle hook
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用启动后容器生命周期钩子
- en: A post-start hook is executed immediately after the container’s main process
    is started. You use it to perform additional operations when the application starts.
    Sure, if you’re the author of the application running in the container, you can
    always perform those operations inside the application code itself. But when you’re
    running an application developed by someone else, you mostly don’t want to (or
    can’t) modify its source code. Post-start hooks allow you to run additional commands
    without having to touch the app. These may signal to an external listener that
    the app is starting, or they may initialize the application so it can start doing
    its job.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一个启动后钩子在容器的主进程启动后立即执行。你用它来在应用启动时执行额外的操作。当然，如果你是运行在容器中的应用的作者，你总是可以在应用代码内部执行这些操作。但当你运行由其他人开发的应用时，你通常不希望（或不能）修改其源代码。启动后钩子允许你在不接触应用的情况下运行额外的命令。这些可能向外部监听器发出应用正在启动的信号，或者初始化应用以便它可以开始执行其工作。
- en: The hook is run in parallel with the main process. The name might be somewhat
    misleading, because it doesn’t wait for the main process to start up fully (if
    the process has an initialization procedure, the Kubelet obviously can’t wait
    for the procedure to complete, because it has no way of knowing when that is).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 钩子与主进程并行运行。这个名字可能有些误导，因为它不会等待主进程完全启动（如果进程有一个初始化过程，Kubelet 显然不能等待该过程完成，因为它没有方法知道何时完成）。
- en: But even though the hook runs asynchronously, it does affect the container in
    two ways. Until the hook completes, the container will stay in the `Waiting` state
    with the reason `ContainerCreating`. Because of this, the pod’s status will be
    `Pending` instead of `Running`. If the hook fails to run or returns a non-zero
    exit code, the main container will be killed.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 但即使钩子是异步运行的，它也会以两种方式影响容器。直到钩子完成，容器将保持 `Waiting` 状态，原因标记为 `ContainerCreating`。因此，Pod
    的状态将是 `Pending` 而不是 `Running`。如果钩子运行失败或返回非零退出代码，主容器将被终止。
- en: A pod manifest containing a post-start hook looks like the following listing.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 包含启动后钩子的 Pod 清单看起来如下所示。
- en: 'Listing 17.3\. A pod with a post-start lifecycle hook: post-start-hook.yaml'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.3\. 包含启动后生命周期钩子的 Pod：post-start-hook.yaml
- en: '`apiVersion: v1 kind: Pod metadata:   name: pod-with-poststart-hook spec:  
    containers:   - image: luksa/kubia     name: kubia     lifecycle:` `1` `postStart:`
    `1` `exec:` `2` `command:` `2` `- sh` `2` `- -c` `2` `- "echo ''hook will fail
    with exit code 15''; sleep 5; exit 15"` `2`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Pod metadata:   name: pod-with-poststart-hook spec:  
    containers:   - image: luksa/kubia     name: kubia     lifecycle:` `1` `postStart:`
    `1` `exec:` `2` `command:` `2` `- sh` `2` `- -c` `2` `- "echo ''hook will fail
    with exit code 15''; sleep 5; exit 15"` `2`'
- en: 1 The hook is executed as the container starts.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 钩子在容器启动时执行。
- en: 2 It executes the postStart.sh script in the /bin directory inside the container.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 它在容器内 /bin 目录中执行 postStart.sh 脚本。
- en: In the example, the `echo`, `sleep`, and `exit` commands are executed along
    with the container’s main process as soon as the container is created. Rather
    than run a command like this, you’d typically run a shell script or a binary executable
    file stored in the container image.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例中，`echo`、`sleep` 和 `exit` 命令与容器的主进程一起在容器创建时执行。你通常不会运行这样的命令，而是会运行存储在容器镜像中的
    shell 脚本或二进制可执行文件。
- en: Sadly, if the process started by the hook logs to the standard output, you can’t
    see the output anywhere. This makes debugging lifecycle hooks painful. If the
    hook fails, you’ll only see a `FailedPostStartHook` warning among the pod’s events
    (you can see them using `kubectl describe pod`). A while later, you’ll see more
    information on why the hook failed, as shown in the following listing.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 很遗憾，如果由钩子启动的过程将日志记录到标准输出，你将无法在任何地方看到输出。这使得调试生命周期钩子变得痛苦。如果钩子失败，你只能在 Pod 的事件中看到
    `FailedPostStartHook` 警告（你可以使用 `kubectl describe pod` 来查看它们）。稍后，你将看到更多关于钩子失败原因的信息，如下所示。
- en: Listing 17.4\. Pod’s events showing the exit code of the failed command-based
    hook
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.4\. 显示失败命令基于钩子的退出代码的 Pod 事件
- en: '`FailedSync``Error syncing pod, skipping: failed to "StartContainer" for             
    "kubia" with PostStart handler: command ''sh -c echo ''hook              will
    fail with exit code 15''; sleep 5 ; exit 15'' exited              with` `15``:
    : "PostStart Hook Failed"`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`FailedSync``Error syncing pod, skipping: failed to "StartContainer" for             
    "kubia" with PostStart handler: command ''sh -c echo ''hook              will
    fail with exit code 15''; sleep 5 ; exit 15'' exited              with` `15``:
    : "PostStart Hook Failed"`'
- en: The number `15` in the last line is the exit code of the command. When using
    an HTTP GET hook handler, the reason may look like the following listing (you
    can try this by deploying the post-start-hook-httpget.yaml file from the book’s
    code archive).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最后行中的数字 `15` 是命令的退出代码。当使用 HTTP GET 钩子处理程序时，原因可能看起来如下所示（你可以通过从本书的代码存档中部署 post-start-hook-httpget.yaml
    文件来尝试此操作）。
- en: Listing 17.5\. Pod’s events showing the reason why an HTTP GET hook failed
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.5\. 显示 HTTP GET 钩子失败原因的 Pod 事件
- en: '`FailedSync``Error syncing pod, skipping: failed to "StartContainer" for             
    "kubia" with PostStart handler: Get              http://10.32.0.2:9090/postStart:
    dial tcp 10.32.0.2:9090:              getsockopt:` `connection refused``: "PostStart
    Hook Failed"`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`FailedSync``Error syncing pod, skipping: failed to "StartContainer" for             
    "kubia" with PostStart handler: Get              http://10.32.0.2:9090/postStart:
    dial tcp 10.32.0.2:9090:              getsockopt:` `connection refused``: "PostStart
    Hook Failed"`'
- en: '|  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The post-start hook is intentionally misconfigured to use port 9090 instead
    of the correct port 8080, to show what happens when the hook fails.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 启动后钩子故意配置错误，使用端口 9090 而不是正确的端口 8080，以展示钩子失败时会发生什么。
- en: '|  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'The standard and error outputs of command-based post-start hooks aren’t logged
    anywhere, so you may want to have the process the hook invokes log to a file in
    the container’s filesystem, which will allow you to examine the contents of the
    file with something like this:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 基于命令的启动后钩子的标准输出和错误输出不会记录在任何地方，因此你可能希望钩子调用的进程将日志记录到容器文件系统中的文件，这将允许你使用类似以下方式检查文件内容：
- en: '`$ kubectl exec my-pod cat logfile.txt`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl exec my-pod cat logfile.txt`'
- en: If the container gets restarted for whatever reason (including because the hook
    failed), the file may be gone before you can examine it. You can work around that
    by mounting an `emptyDir` volume into the container and having the hook write
    to it.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果由于任何原因（包括钩子失败）容器被重新启动，文件可能在你可以检查它之前就已经消失了。你可以通过将`emptyDir`卷挂载到容器中，并让钩子将其写入，来解决这个问题。
- en: Using a pre-stop container lifecycle hook
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预停止容器生命周期钩子
- en: A pre-stop hook is executed immediately before a container is terminated. When
    a container needs to be terminated, the Kubelet will run the pre-stop hook, if
    configured, and only then send a `SIGTERM` to the process (and later kill the
    process if it doesn’t terminate gracefully).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 预停止钩子在容器终止前立即执行。当容器需要被终止时，如果配置了预停止钩子，Kubelet将运行它，然后只向进程发送`SIGTERM`信号（如果进程没有优雅地终止，稍后将其杀死）。
- en: A pre-stop hook can be used to initiate a graceful shutdown of the container,
    if it doesn’t shut down gracefully upon receipt of a `SIGTERM` signal. They can
    also be used to perform arbitrary operations before shutdown without having to
    implement those operations in the application itself (this is useful when you’re
    running a third-party app, whose source code you don’t have access to and/or can’t
    modify).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果容器在接收到`SIGTERM`信号后没有优雅地关闭，可以使用预停止钩子来启动容器的优雅关闭。它们还可以在关闭前执行任意操作，而无需在应用程序本身中实现这些操作（当你运行一个第三方应用程序，且没有访问其源代码或无法修改它时，这很有用）。
- en: Configuring a pre-stop hook in a pod manifest isn’t very different from adding
    a post-start hook. The previous example showed a post-start hook that executes
    a command, so we’ll look at a pre-stop hook that performs an HTTP GET request
    now. The following listing shows how to define a pre-stop HTTP GET hook in a pod.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在pod清单中配置预停止钩子与添加启动后钩子没有太大区别。之前的例子展示了执行命令的启动后钩子，所以现在我们将看看执行HTTP GET请求的预停止钩子。以下列表显示了如何在pod中定义预停止HTTP
    GET钩子。
- en: 'Listing 17.6\. A pre-stop hook YAML snippet: pre-stop-hook-httpget.yaml'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.6\. 预停止钩子YAML片段：pre-stop-hook-httpget.yaml
- en: '`lifecycle:       preStop:` `1` `httpGet:` `1` `port: 8080` `2` `path: shutdown`
    `2`'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`lifecycle:       preStop:` `1` `httpGet:` `1` `port: 8080` `2` `path: shutdown`
    `2`'
- en: 1 This is a pre-stop hook that performs an HTTP GET request.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 这是一个执行HTTP GET请求的预停止钩子。
- en: 2 The request is sent to [http://POD_IP:8080/shutdown](http://POD_IP:8080/shutdown).
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 请求发送到[http://POD_IP:8080/shutdown](http://POD_IP:8080/shutdown)。
- en: The pre-stop hook defined in this listing performs an HTTP GET request to [http://POD_IP:8080/shutdown](http://POD_IP:8080/shutdown)
    as soon as the Kubelet starts terminating the container. Apart from the `port`
    and `path` shown in the listing, you can also set the fields `scheme` (HTTP or
    HTTPS) and `host`, as well as `httpHeaders` that should be sent in the request.
    The `host` field defaults to the pod IP. Be sure not to set it to localhost, because
    localhost would refer to the node, not the pod.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 本列表中定义的预停止钩子会在Kubelet开始终止容器时立即执行一个HTTP GET请求到[http://POD_IP:8080/shutdown](http://POD_IP:8080/shutdown)。除了列表中显示的`port`和`path`之外，你还可以设置`scheme`字段（HTTP或HTTPS）、`host`以及应发送到请求中的`httpHeaders`。`host`字段默认为pod
    IP。请确保不要将其设置为localhost，因为localhost将指向节点，而不是pod。
- en: In contrast to the post-start hook, the container will be terminated regardless
    of the result of the hook—an error HTTP response code or a non-zero exit code
    when using a command-based hook will not prevent the container from being terminated.
    If the pre-stop hook fails, you’ll see a `FailedPreStopHook` warning event among
    the pod’s events, but because the pod is deleted soon afterward (after all, the
    pod’s deletion is what triggered the pre-stop hook in the first place), you may
    not even notice that the pre-stop hook failed to run properly.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与启动后钩子相比，无论钩子的结果如何——使用基于命令的钩子时，错误HTTP响应代码或非零退出代码都不会阻止容器被终止。如果预停止钩子失败，你将在pod的事件中看到`FailedPreStopHook`警告事件，但由于pod随后很快被删除（毕竟，pod的删除最初触发了预停止钩子），你可能甚至都没有注意到预停止钩子未能正确运行。
- en: '|  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: If the successful completion of the pre-stop hook is critical to the proper
    operation of your system, verify whether it’s being executed at all. I’ve witnessed
    situations where the pre-stop hook didn’t run and the developer wasn’t even aware
    of that.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果预停止钩子的成功完成对于系统的正常操作至关重要，请验证它是否真的被执行了。我曾目睹过预停止钩子没有运行，而开发者甚至都没有意识到这一点。
- en: '|  |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Using a pre-stop hook because your app doesn’t receive the SIGTERM signal
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你的应用程序没有接收到SIGTERM信号而使用停止前钩子
- en: Many developers make the mistake of defining a pre-stop hook solely to send
    a `SIGTERM` signal to their apps in the pre-stop hook. They do this because they
    don’t see their application receive the `SIGTERM` signal sent by the Kubelet.
    The reason why the signal isn’t received by the application isn’t because Kubernetes
    isn’t sending it, but because the signal isn’t being passed to the app process
    inside the container itself. If your container image is configured to run a shell,
    which in turn runs the app process, the signal may be eaten up by the shell itself,
    instead of being passed down to the child process.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 许多开发者犯了一个错误，就是仅仅为了在停止前钩子中向他们的应用程序发送`SIGTERM`信号而定义一个停止前钩子。他们这样做是因为他们没有看到他们的应用程序接收到由Kubelet发送的`SIGTERM`信号。应用程序没有接收到信号的原因并不是Kubernetes没有发送它，而是信号没有传递到容器内部的app进程。如果你的容器镜像配置为运行一个shell，该shell又运行app进程，信号可能会被shell本身消耗掉，而不是传递给子进程。
- en: 'In such cases, instead of adding a pre-stop hook to send the signal directly
    to your app, the proper fix is to make sure the shell passes the signal to the
    app. This can be achieved by handling the signal in the shell script running as
    the main container process and then passing it on to the app. Or you could not
    configure the container image to run a shell at all and instead run the application
    binary directly. You do this by using the exec form of `ENTRYPOINT` or `CMD` in
    the Dockerfile: `ENTRYPOINT ["/mybinary"]` instead of `ENTRYPOINT /mybinary`.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，与其在停止前钩子中直接向你的应用程序发送信号，正确的修复方法是确保shell将信号传递给应用程序。这可以通过在作为主容器进程运行的shell脚本中处理信号并将其传递给应用程序来实现。或者你也可以不配置容器镜像以运行shell，而是直接运行应用程序的二进制文件。你可以通过在Dockerfile中使用`ENTRYPOINT`或`CMD`的exec形式来实现这一点：`ENTRYPOINT
    ["/mybinary"]`而不是`ENTRYPOINT /mybinary`。
- en: A container using the first form runs the `mybinary` executable as its main
    process, whereas the second form runs a shell as the main process with the `mybinary`
    process executed as a child of the shell process.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 使用第一种形式的容器以`mybinary`可执行文件作为其主进程运行，而第二种形式则以shell作为主进程，并将`mybinary`进程作为shell进程的子进程执行。
- en: Understanding that lifecycle hooks target containers, not pods
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 理解生命周期钩子针对的是容器，而不是Pod
- en: As a final thought on post-start and pre-stop hooks, let me emphasize that these
    lifecycle hooks relate to containers, not pods. You shouldn’t use a pre-stop hook
    for running actions that need to be performed when the pod is terminating. The
    reason is that the pre-stop hook gets called when the container is being terminated
    (most likely because of a failed liveness probe). This may happen multiple times
    in the pod’s lifetime, not only when the pod is in the process of being shut down.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 关于启动后和停止前的钩子，让我最后强调一点，这些生命周期钩子与容器相关，而不是与Pod相关。你不应该使用停止前钩子来执行在Pod终止时需要执行的操作。原因是停止前钩子在容器被终止时会被调用（很可能是由于存活性探测失败）。这种情况在Pod的生命周期中可能发生多次，而不仅仅是当Pod正在关闭过程中。
- en: 17.2.5\. Understanding pod shutdown
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 17.2.5. 理解Pod关闭
- en: We’ve touched on the subject of pod termination, so let’s explore this subject
    in more detail and go over exactly what happens during pod shutdown. This is important
    for understanding how to cleanly shut down an application running in a pod.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到了Pod终止的话题，那么让我们更详细地探讨这个话题，并了解在Pod关闭期间确切发生了什么。这对于理解如何干净地关闭在Pod中运行的应用程序非常重要。
- en: Let’s start at the beginning. A pod’s shut-down is triggered by the deletion
    of the Pod object through the API server. Upon receiving an HTTP DELETE request,
    the API server doesn’t delete the object yet, but only sets a `deletionTimestamp`
    field in it. Pods that have the `deletionTimestamp` field set are terminating.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从开始讲起。Pod的关闭是由API服务器通过删除Pod对象触发的。当收到HTTP DELETE请求时，API服务器不会立即删除对象，而是在其中设置一个`deletionTimestamp`字段。设置了`deletionTimestamp`字段的Pod正在终止。
- en: 'Once the Kubelet notices the pod needs to be terminated, it starts terminating
    each of the pod’s containers. It gives each container time to shut down gracefully,
    but the time is limited. That time is called the termination grace period and
    is configurable per pod. The timer starts as soon as the termination process starts.
    Then the following sequence of events is performed:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Kubelet注意到Pod需要被终止，它就开始终止Pod中的每个容器。它给每个容器时间来优雅地关闭，但时间是有限的。这个时间被称为终止宽限期，并且可以按Pod进行配置。计时器在终止过程开始时启动。然后执行以下事件序列：
- en: Run the pre-stop hook, if one is configured, and wait for it to finish.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果已配置，运行预停止钩子，并等待其完成。
- en: Send the `SIGTERM` signal to the main process of the container.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向容器的主进程发送 `SIGTERM` 信号。
- en: Wait until the container shuts down cleanly or until the termination grace period
    runs out.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待容器干净地关闭或直到终止宽限期结束。
- en: Forcibly kill the process with `SIGKILL`, if it hasn’t terminated gracefully
    yet.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果进程还没有优雅地终止，则使用 `SIGKILL` 强制终止进程。
- en: The sequence of events is illustrated in [figure 17.5](#filepos1587837).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 事件序列如图 17.5 所示。[figure 17.5](#filepos1587837)。
- en: Figure 17.5\. The container termination sequence
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.5\. 容器终止序列
- en: '![](images/00167.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00167.jpg)'
- en: Specifying the termination grace period
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 指定终止宽限期
- en: The termination grace period can be configured in the pod spec by setting the
    `spec.terminationGracePeriodSeconds` field. It defaults to 30, which means the
    pod’s containers will be given 30 seconds to terminate gracefully before they’re
    killed forcibly.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过设置 `spec.terminationGracePeriodSeconds` 字段在 pod 规范中配置终止宽限期。默认值为 30，这意味着
    pod 的容器在被强制杀死之前将获得 30 秒的时间来优雅地终止。
- en: '|  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: You should set the grace period to long enough so your process can finish cleaning
    up in that time.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该将宽限期设置得足够长，以便你的进程可以在那段时间内完成清理。
- en: '|  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'The grace period specified in the pod spec can also be overridden when deleting
    the pod like this:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在删除 pod 时，也可以覆盖 pod 规范中指定的宽限期：
- en: '`$ kubectl delete po mypod --grace-period=5`'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl delete po mypod --grace-period=5`'
- en: 'This will make the Kubelet wait five seconds for the pod to shut down cleanly.
    When all the pod’s containers stop, the Kubelet notifies the API server and the
    Pod resource is finally deleted. You can force the API server to delete the resource
    immediately, without waiting for confirmation, by setting the grace period to
    zero and adding the `--force` option like this:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使 Kubelet 等待五秒钟，直到 pod 优雅地关闭。当 pod 的所有容器都停止时，Kubelet 会通知 API 服务器，并且 Pod 资源最终被删除。你可以通过将宽限期设置为零并添加
    `--force` 选项来强制 API 服务器立即删除资源，而不必等待确认，如下所示：
- en: '`$ kubectl delete po mypod --grace-period=0 --force`'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl delete po mypod --grace-period=0 --force`'
- en: Be careful when using this option, especially with pods of a StatefulSet. The
    StatefulSet controller takes great care to never run two instances of the same
    pod at the same time (two pods with the same ordinal index and name and attached
    to the same Persistent-Volume). By force-deleting a pod, you’ll cause the controller
    to create a replacement pod without waiting for the containers of the deleted
    pod to shut down. In other words, two instances of the same pod might be running
    at the same time, which may cause your stateful cluster to malfunction. Only delete
    stateful pods forcibly when you’re absolutely sure the pod isn’t running anymore
    or can’t talk to the other members of the cluster (you can be sure of this when
    you confirm that the node that hosted the pod has failed or has been disconnected
    from the network and can’t reconnect).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此选项时要小心，特别是与有状态集的 pod 一起使用。有状态集控制器非常小心，从不同时运行相同 pod 的两个实例（具有相同序号索引和名称，并附加到同一持久卷的两个
    pod）。通过强制删除 pod，你将导致控制器在没有等待被删除 pod 的容器关闭的情况下创建替换 pod。换句话说，同一 pod 的两个实例可能会同时运行，这可能导致你的有状态集群出现故障。只有在你绝对确定
    pod 已经不再运行或无法与集群的其他成员通信时（当你确认托管 pod 的节点已失败或已从网络断开且无法重新连接时），才强制删除有状态 pod。
- en: Now that you understand how containers are shut down, let’s look at it from
    the application’s perspective and go over how applications should handle the shutdown
    procedure.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了容器是如何关闭的，让我们从应用程序的角度来看，并回顾一下应用程序应该如何处理关闭过程。
- en: Implementing the proper shutdown handler in your application
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的应用程序中实现适当的关闭处理程序
- en: Applications should react to a `SIGTERM` signal by starting their shut-down
    procedure and terminating when it finishes. Instead of handling the `SIGTERM`
    signal, the application can be notified to shut down through a pre-stop hook.
    In both cases, the app then only has a fixed amount of time to terminate cleanly.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序应该通过启动它们的关闭过程并在完成后终止来响应 `SIGTERM` 信号。除了处理 `SIGTERM` 信号外，应用程序还可以通过预停止钩子来通知关闭。在两种情况下，应用程序都只有固定的时间来干净地终止。
- en: But what if you can’t predict how long the app will take to shut down cleanly?
    For example, imagine your app is a distributed data store. On scale-down, one
    of the pod instances will be deleted and therefore shut down. In the shut-down
    procedure, the pod needs to migrate all its data to the remaining pods to make
    sure it’s not lost. Should the pod start migrating the data upon receiving a termination
    signal (through either the `SIGTERM` signal or through a pre-stop hook)?
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你无法预测应用程序干净关闭需要多长时间怎么办？例如，想象你的应用程序是一个分布式数据存储。在缩小时，一个 Pod 实例将被删除并因此关闭。在关闭过程中，Pod
    需要将所有数据迁移到剩余的 Pod 中，以确保数据不会丢失。Pod 是否应该在收到终止信号（通过 `SIGTERM` 信号或通过 pre-stop 钩子）时开始迁移数据？
- en: 'Absolutely not! This is not recommended for at least the following two reasons:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对不行！至少有以下两个原因不建议这样做：
- en: A container terminating doesn’t necessarily mean the whole pod is being terminated.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器终止并不一定意味着整个 Pod 正在被终止。
- en: You have no guarantee the shut-down procedure will finish before the process
    is killed.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你没有任何保证关闭程序会在进程被杀死之前完成。
- en: This second scenario doesn’t happen only when the grace period runs out before
    the application has finished shutting down gracefully, but also when the node
    running the pod fails in the middle of the container shut-down sequence. Even
    if the node then starts up again, the Kubelet will not restart the shut-down procedure
    (it won’t even start up the container again). There are absolutely no guarantees
    that the pod will be allowed to complete its whole shut-down procedure.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这种第二种情况不仅发生在应用程序在优雅关闭过程中 grace period 超出之前，还发生在运行 Pod 的节点在容器关闭序列中途失败时。即使节点随后重新启动，Kubelet
    也不会重新启动关闭程序（甚至不会再次启动容器）。绝对没有保证 Pod 能够完成整个关闭程序。
- en: Replacing critical shut-down procedures with dedicated shut-down procedure pods
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 用专用关闭程序 Pod 替换关键关闭程序
- en: How do you ensure that a critical shut-down procedure that absolutely must run
    to completion does run to completion (for example, to ensure that a pod’s data
    is migrated to other pods)?
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何确保绝对必须运行到完成的临界关闭程序确实运行到完成（例如，确保 Pod 的数据迁移到其他 Pod）？
- en: One solution is for the app (upon receipt of a termination signal) to create
    a new Job resource that would run a new pod, whose sole job is to migrate the
    deleted pod’s data to the remaining pods. But if you’ve been paying attention,
    you’ll know that you have no guarantee the app will indeed manage to create the
    Job object every single time. What if the node fails exactly when the app tries
    to do that?
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案是应用程序（在收到终止信号后）创建一个新的 Job 资源，该资源将运行一个新的 Pod，其唯一任务是迁移被删除 Pod 的数据到剩余的 Pod
    中。但如果你一直很注意，你就会知道你没有任何保证应用程序确实每次都能成功创建 Job 对象。如果节点在应用程序尝试这样做时失败，那会怎样？
- en: The proper way to handle this problem is by having a dedicated, constantly running
    pod that keeps checking for the existence of orphaned data. When this pod finds
    the orphaned data, it can migrate it to the remaining pods. Rather than a constantly
    running pod, you can also use a CronJob resource and run the pod periodically.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这个问题的正确方法是运行一个专用、持续运行的 Pod，不断检查孤儿数据的存在。当这个 Pod 发现孤儿数据时，它可以将其迁移到剩余的 Pod 中。除了持续运行的
    Pod 之外，您还可以使用 CronJob 资源定期运行 Pod。
- en: You may think StatefulSets could help here, but they don’t. As you’ll remember,
    scaling down a StatefulSet leaves PersistentVolumeClaims orphaned, leaving the
    data stored on the PersistentVolume stranded. Yes, upon a subsequent scale-up,
    the Persistent-Volume will be reattached to the new pod instance, but what if
    that scale-up never happens (or happens after a long time)? For this reason, you
    may want to run a data-migrating pod also when using StatefulSets (this scenario
    is shown in [figure 17.6](#filepos1594517)). To prevent the migration from occurring
    during an application upgrade, the data-migrating pod could be configured to wait
    a while to give the stateful pod time to come up again before performing the migration.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '你可能会认为 StatefulSets 可以在这里有所帮助，但它们并不能。正如你将记得的那样，缩小 StatefulSet 会导致 PersistentVolumeClaims
    成孤儿，使得存储在 PersistentVolume 上的数据变得孤立。是的，在随后的扩展中，Persistent-Volume 将重新连接到新的 Pod
    实例，但如果没有发生扩展（或者发生得很晚），那会怎样？因此，当使用 StatefulSets 时，你可能还想要运行一个数据迁移 Pod（此场景在[图 17.6](#filepos1594517)
    中显示）。为了防止在应用程序升级期间发生迁移，数据迁移 Pod 可以配置为等待一段时间，以便在执行迁移之前给有状态的 Pod 时间重新启动。 '
- en: Figure 17.6\. Using a dedicated pod to migrate data
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.6\. 使用专用 Pod 迁移数据
- en: '![](images/00185.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00185.jpg)'
- en: 17.3\. Ensuring all client requests are handled properly
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 17.3. 确保所有客户端请求得到妥善处理
- en: You now have a good sense of how to make pods shut down cleanly. Now, we’ll
    look at the pod’s lifecycle from the perspective of the pod’s clients (clients
    consuming the service the pod is providing). This is important to understand if
    you don’t want clients to run into problems when you scale pods up or down.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在对如何使Pod干净地关闭有了很好的理解。现在，我们将从Pod客户端的角度来看待Pod的生命周期（客户端正在消费Pod提供的服务）。如果您不想在扩展Pod时遇到问题，这一点很重要。
- en: It goes without saying that you want all client requests to be handled properly.
    You obviously don’t want to see broken connections when pods are starting up or
    shutting down. By itself, Kubernetes doesn’t prevent this from happening. Your
    app needs to follow a few rules to prevent broken connections. First, let’s focus
    on making sure all connections are handled properly when the pod starts up.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 毋庸置疑，您希望所有客户端请求都得到妥善处理。显然，您不希望在Pod启动或关闭时看到断开连接。仅凭Kubernetes本身并不能防止这种情况发生。您的应用程序需要遵循一些规则来防止断开连接。首先，让我们专注于确保在Pod启动时所有连接都得到妥善处理。
- en: 17.3.1\. Preventing broken client connections when a pod is starting up
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 17.3.1. 防止Pod启动时客户端连接断开
- en: Ensuring each connection is handled properly at pod startup is simple if you
    understand how Services and service Endpoints work. When a pod is started, it’s
    added as an endpoint to all the Services, whose label selector matches the pod’s
    labels. As you may remember from [chapter 5](index_split_046.html#filepos469093),
    the pod also needs to signal to Kubernetes that it’s ready. Until it is, it won’t
    become a service endpoint and therefore won’t receive any requests from clients.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您理解服务和服务端点的工作方式，确保在Pod启动时每个连接都得到妥善处理是简单的。当Pod启动时，它被添加为所有标签选择器与Pod标签匹配的服务的端点。如您可能记得的[第5章](index_split_046.html#filepos469093)，Pod还需要向Kubernetes发出就绪信号。直到它就绪，它不会成为服务端点，因此不会从客户端接收任何请求。
- en: If you don’t specify a readiness probe in your pod spec, the pod is always considered
    ready. It will start receiving requests almost immediately—as soon as the first
    kube-proxy updates the `iptables` rules on its node and the first client pod tries
    to connect to the service. If your app isn’t ready to accept connections by then,
    clients will see “connection refused” types of errors.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在Pod规范中没有指定就绪探针，则Pod始终被认为是就绪的。它将几乎立即开始接收请求——一旦第一个kube-proxy在其节点上更新了`iptables`规则，并且第一个客户端Pod尝试连接到服务。如果您的应用程序当时还没有准备好接受连接，客户端将看到“连接拒绝”类型的错误。
- en: All you need to do is make sure that your readiness probe returns success only
    when your app is ready to properly handle incoming requests. A good first step
    is to add an HTTP GET readiness probe and point it to the base URL of your app.
    In many cases that gets you far enough and saves you from having to implement
    a special readiness endpoint in your app.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要做的只是确保您的就绪探针仅在您的应用程序准备好妥善处理传入请求时返回成功。一个好的第一步是添加一个HTTP GET就绪探针，并将其指向您应用程序的基本URL。在许多情况下，这足以让您走得很远，并让您免于在应用程序中实现特殊的就绪端点。
- en: 17.3.2\. Preventing broken connections during pod shut-down
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 17.3.2. 防止Pod关闭期间的连接断开
- en: Now let’s see what happens at the other end of a pod’s life—when the pod is
    deleted and its containers are terminated. We’ve already talked about how the
    pod’s containers should start shutting down cleanly as soon they receive the `SIGTERM`
    signal (or when its pre-stop hook is executed). But does that ensure all client
    requests are handled properly?
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看一个Pod生命周期的另一端会发生什么——当Pod被删除并且其容器被终止时。我们已经讨论了Pod的容器应该在接收到`SIGTERM`信号（或者当其预停止钩子被执行）后立即干净地关闭。但是，这能确保所有客户端请求都得到妥善处理吗？
- en: How should the app behave when it receives a termination signal? Should it continue
    to accept requests? What about requests that have already been received but haven’t
    completed yet? What about persistent HTTP connections, which may be in between
    requests, but are open (when no active request exists on the connection)? Before
    we can answer those questions, we need to take a detailed look at the chain of
    events that unfolds across the cluster when a Pod is deleted.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序接收到终止信号时，它应该如何表现？它应该继续接受请求吗？对于已经接收但尚未完成的请求怎么办？对于可能处于请求之间但处于打开状态的持久HTTP连接怎么办（当连接上没有活跃请求时）？在我们可以回答这些问题之前，我们需要详细查看当Pod被删除时在集群中展开的事件链。
- en: Understanding the sequence of events occurring at pod deletion
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 pod 删除时发生的事件序列
- en: In [chapter 11](index_split_087.html#filepos1036287) we took an in-depth look
    at what components make up a Kubernetes cluster. You need to always keep in mind
    that those components run as separate processes on multiple machines. They aren’t
    all part of a single big monolithic process. It takes time for all the components
    to be on the same page regarding the state of the cluster. Let’s explore this
    fact by looking at what happens across the cluster when a Pod is deleted.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 11 章](index_split_087.html#filepos1036287) 中，我们深入探讨了构成 Kubernetes 集群的组件。你需要始终记住，这些组件在多台机器上作为单独的进程运行。它们并不都是单一大型单体进程的一部分。所有组件都达到关于集群状态的共识需要时间。让我们通过查看
    pod 被删除时集群中发生的情况来探索这一事实。
- en: When a request for a pod deletion is received by the API server, it first modifies
    the state in etcd and then notifies its watchers of the deletion. Among those
    watchers are the Kubelet and the Endpoints controller. The two sequences of events,
    which happen in parallel (marked with either A or B), are shown in [figure 17.7](#filepos1599149).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当 API 服务器接收到 pod 删除请求时，它首先在 etcd 中修改状态，然后通知其监视者关于删除的消息。在这些监视者中包括 Kubelet 和端点控制器。这两个并行发生的事件序列（用
    A 或 B 标记），在 [图 17.7](#filepos1599149) 中显示。
- en: Figure 17.7\. Sequence of events that occurs when a Pod is deleted
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.7\. 删除 pod 时发生的事件序列
- en: '![](images/00005.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.7](images/00005.jpg)'
- en: In the A sequence of events, you’ll see that as soon as the Kubelet receives
    the notification that the pod should be terminated, it initiates the shutdown
    sequence as explained in [section 17.2.5](index_split_125.html#filepos1586038)
    (run the pre-stop hook, send `SIGTERM`, wait for a period of time, and then forcibly
    kill the container if it hasn’t yet terminated on its own). If the app responds
    to the `SIGTERM` by immediately ceasing to receive client requests, any client
    trying to connect to it will receive a Connection Refused error. The time it takes
    for this to happen from the time the pod is deleted is relatively short because
    of the direct path from the API server to the Kubelet.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在事件序列 A 中，你会看到，一旦 Kubelet 收到 pod 应该终止的通知，它就会启动关闭序列，如 [第 17.2.5 节](index_split_125.html#filepos1586038)
    所解释的那样（运行预停止钩子，发送 `SIGTERM`，等待一段时间，如果容器尚未自行终止，则强制杀死容器）。如果应用程序通过立即停止接收客户端请求来响应
    `SIGTERM`，那么任何尝试连接到它的客户端都会收到连接拒绝错误。由于从 pod 删除到发生此事件的时间相对较短，这是由于 API 服务器到 Kubelet
    的直接路径。
- en: Now, let’s look at what happens in the other sequence of events—the one leading
    up to the pod being removed from the `iptables` rules (sequence B in the figure).
    When the Endpoints controller (which runs in the Controller Manager in the Kubernetes
    Control Plane) receives the notification of the Pod being deleted, it removes
    the pod as an endpoint in all services that the pod is a part of. It does this
    by modifying the Endpoints API object by sending a REST request to the API server.
    The API server then notifies all clients watching the Endpoints object. Among
    those watchers are all the kube-proxies running on the worker nodes. Each of these
    proxies then updates the `iptables` rules on its node, which is what prevents
    new connections from being forwarded to the terminating pod. An important detail
    here is that removing the `iptables` rules has no effect on existing connections—clients
    who are already connected to the pod will still send additional requests to the
    pod through those existing connections.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看在另一个事件序列中会发生什么——即 pod 从 `iptables` 规则中移除的事件序列（图中的序列 B）。当端点控制器（在 Kubernetes
    控制平面的控制器管理器中运行）接收到 pod 被删除的通知时，它会将 pod 从 pod 所在的所有服务中的端点移除。它是通过向 API 服务器发送 REST
    请求来修改端点 API 对象来做到这一点的。然后 API 服务器通知所有监视端点对象的客户端。在这些监视者中包括所有在工作节点上运行的 kube-proxies。然后，每个代理都会更新其节点上的
    `iptables` 规则，这是防止新连接被转发到正在终止的 pod 的原因。这里的一个重要细节是，移除 `iptables` 规则对现有连接没有影响——已经连接到
    pod 的客户端仍然会通过这些现有连接向 pod 发送额外的请求。
- en: Both of these sequences of events happen in parallel. Most likely, the time
    it takes to shut down the app’s process in the pod is slightly shorter than the
    time required for the `iptables` rules to be updated. The chain of events that
    leads to `iptables` rules being updated is considerably longer (see [figure 17.8](#filepos1601948)),
    because the event must first reach the Endpoints controller, which then sends
    a new request to the API server, and then the API server must notify the kube-proxy
    before the proxy finally modifies the `iptables` rules. A high probability exists
    that the `SIGTERM` signal will be sent well before the `iptables` rules are updated
    on all nodes.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个事件序列都是并行发生的。很可能关闭 Pod 中应用程序进程所需的时间略短于更新 `iptables` 规则所需的时间。导致更新 `iptables`
    规则的事件链相当长（见 [图 17.8](#filepos1601948)），因为事件必须首先到达 Endpoints 控制器，然后控制器向 API 服务器发送新的请求，然后
    API 服务器必须通知 kube-proxy，最后代理才会修改 `iptables` 规则。有很大可能性，`SIGTERM` 信号会在所有节点上的 `iptables`
    规则更新之前被发送。
- en: Figure 17.8\. Timeline of events when pod is deleted
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.8\. 删除 Pod 时的事件时间线
- en: '![](images/00023.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00023.jpg)'
- en: The end result is that the pod may still receive client requests after it was
    sent the termination signal. If the app closes the server socket and stops accepting
    connections immediately, this will cause clients to receive “Connection Refused”
    types of errors (similar to what happens at pod startup if your app isn’t capable
    of accepting connections immediately and you don’t define a readiness probe for
    it).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果是，Pod 在收到终止信号后可能仍然会收到客户端请求。如果应用程序立即关闭服务器套接字并停止接受连接，这将导致客户端收到“连接被拒绝”类型的错误（类似于
    Pod 启动时如果应用程序无法立即接受连接且没有为其定义就绪探针时发生的情况）。
- en: Solving the problem
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 解决问题
- en: Googling solutions to this problem makes it seem as though adding a readiness
    probe to your pod will solve the problem. Supposedly, all you need to do is make
    the readiness probe start failing as soon as the pod receives the `SIGTERM`. This
    is supposed to cause the pod to be removed as the endpoint of the service. But
    the removal would happen only after the readiness probe fails for a few consecutive
    times (this is configurable in the readiness probe spec). And, obviously, the
    removal then still needs to reach the kube-proxy before the pod is removed from
    `iptables` rules.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google 上搜索这个问题的解决方案，似乎添加一个就绪探针到你的 Pod 就能解决这个问题。据说，你所需要做的就是让就绪探针在 Pod 收到 `SIGTERM`
    信号后立即开始失败。这应该会导致 Pod 作为服务的端点被移除。但是，移除只会发生在就绪探针连续失败几次之后（这在就绪探针规范中是可配置的）。显然，移除后还需要到达
    kube-proxy，然后 Pod 才会从 `iptables` 规则中移除。
- en: In reality, the readiness probe has absolutely no bearing on the whole process
    at all. The Endpoints controller removes the pod from the service Endpoints as
    soon as it receives notice of the pod being deleted (when the `deletionTimestamp`
    field in the pod’s spec is no longer `null`). From that point on, the result of
    the readiness probe is irrelevant.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，就整个流程而言，就绪探针根本没有任何影响。当 Endpoints 控制器收到删除 Pod 的通知（当 Pod 的 spec 中的 `deletionTimestamp`
    字段不再是 `null` 时），它就会立即将 Pod 从服务端点中移除。从那时起，就绪探针的结果就无关紧要了。
- en: What’s the proper solution to the problem? How can you make sure all requests
    are handled fully?
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 问题的正确解决方案是什么？你如何确保所有请求都得到完全处理？
- en: It’s clear the pod needs to keep accepting connections even after it receives
    the termination signal up until all the kube-proxies have finished updating the
    `iptables` rules. Well, it’s not only the kube-proxies. There may also be Ingress
    controllers or load balancers forwarding connections to the pod directly, without
    going through the Service (`iptables`). This also includes clients using client-side
    load-balancing. To ensure none of the clients experience broken connections, you’d
    have to wait until all of them somehow notify you they’ll no longer forward connections
    to the pod.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，Pod 需要在接收到终止信号后继续接受连接，直到所有 kube-proxies 完成更新 `iptables` 规则。嗯，不仅仅是 kube-proxies。还可能有
    Ingress 控制器或负载均衡器直接将连接转发到 Pod，而不通过 Service (`iptables`)。这还包括使用客户端负载均衡的客户端。为了确保没有任何客户端经历断开连接的情况，你必须等待所有客户端以某种方式通知你，他们将不再将连接转发到
    Pod。
- en: That’s impossible, because all those components are distributed across many
    different computers. Even if you knew the location of every one of them and could
    wait until all of them say it’s okay to shut down the pod, what do you do if one
    of them doesn’t respond? How long do you wait for the response? Remember, during
    that time, you’re holding up the shut-down process.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这是不可能的，因为所有这些组件都分布在不同计算机上。即使你知道每一个组件的位置，并且可以等待它们都表示可以关闭 pod，如果其中一个没有响应怎么办？你等待响应需要多长时间？记住，在这段时间里，你正在阻碍关闭过程。
- en: The only reasonable thing you can do is wait for a long-enough time to ensure
    all the proxies have done their job. But how long is long enough? A few seconds
    should be enough in most situations, but there’s no guarantee it will suffice
    every time. When the API server or the Endpoints controller is overloaded, it
    may take longer for the notification to reach the kube-proxy. It’s important to
    understand that you can’t solve the problem perfectly, but even adding a 5- or
    10-second delay should improve the user experience considerably. You can use a
    longer delay, but don’t go overboard, because the delay will prevent the container
    from shutting down promptly and will cause the pod to be shown in lists long after
    it has been deleted, which is always frustrating to the user deleting the pod.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 你唯一合理能做的事情是等待足够长的时间以确保所有代理都完成了它们的工作。但多长时间才算足够长？在大多数情况下，几秒钟应该足够了，但无法保证每次都足够。当
    API 服务器或端点控制器过载时，通知到达 kube-proxy 可能需要更长的时间。重要的是要理解你无法完美解决这个问题，但即使添加 5 或 10 秒的延迟也应该显著改善用户体验。你可以使用更长的延迟，但不要过度，因为延迟将阻止容器及时关闭，并导致
    pod 在被删除很久之后仍然出现在列表中，这对删除 pod 的用户来说总是令人沮丧的。
- en: Wrapping up this section
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 总结本节内容
- en: 'To recap—properly shutting down an application includes these steps:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下——正确关闭应用包括以下步骤：
- en: Wait for a few seconds, then stop accepting new connections.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等待几秒钟，然后停止接受新的连接。
- en: Close all keep-alive connections not in the middle of a request.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关闭所有不在请求中间的保持连接。
- en: Wait for all active requests to finish.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等待所有活跃的请求完成。
- en: Then shut down completely.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后完全关闭。
- en: To understand what’s happening with the connections and requests during this
    process, examine [figure 17.9](#filepos1606536) carefully.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解在此过程中连接和请求的情况，仔细检查[图 17.9](#filepos1606536)。
- en: Figure 17.9\. Properly handling existing and new connections after receiving
    a termination signal
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.9. 接收到终止信号后正确处理现有和新连接
- en: '![](images/00044.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00044.jpg)'
- en: Not as simple as exiting the process immediately upon receiving the termination
    signal, right? Is it worth going through all this? That’s for you to decide. But
    the least you can do is add a pre-stop hook that waits a few seconds, like the
    one in the following listing, perhaps.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不像一收到终止信号就立即退出进程那么简单，对吧？这样做值得吗？这由你决定。但至少你可以添加一个预停止钩子，等待几秒钟，就像下面列表中的那样，也许。
- en: Listing 17.7\. A pre-stop hook for preventing broken connections
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.7. 用于防止连接损坏的预停止钩子
- en: '`    lifecycle:       preStop:         exec:           command:           -
    sh           - -c           - "sleep 5"`'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`    lifecycle:    preStop:      exec:            command:            - sh           
    - -c            - "sleep 5"`'
- en: This way, you don’t need to modify the code of your app at all. If your app
    already ensures all in-flight requests are processed completely, this pre-stop
    delay may be all you need.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，你根本不需要修改你应用的代码。如果你的应用已经确保所有进行中的请求都被完全处理，那么这个预停止延迟可能就是你所需要的全部。
- en: 17.4\. Making your apps easy to run and manage in Kubernetes
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 17.4. 制作易于在 Kubernetes 中运行和管理的应用
- en: I hope you now have a better sense of how to make your apps handle clients nicely.
    Now we’ll look at other aspects of how an app should be built to make it easier
    to manage in Kubernetes.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你现在对如何让你的应用优雅地处理客户端有了更好的理解。现在我们将探讨应用应该如何构建以便在 Kubernetes 中更容易管理。
- en: 17.4.1\. Making manageable container images
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 17.4.1. 制作可管理的容器镜像
- en: When you package your app into an image, you can choose to include the app’s
    binary executable and any additional libraries it needs, or you can package up
    a whole OS filesystem along with the app. Way too many people do this, even though
    it’s usually unnecessary.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将应用打包成镜像时，你可以选择包含应用的二进制可执行文件和它需要的任何附加库，或者你可以将整个操作系统文件系统与应用一起打包。太多的人这样做，尽管这通常是不必要的。
- en: Do you need every single file from an OS distribution in your image? Probably
    not. Most of the files will never be used and will make your image larger than
    it needs to be. Sure, the layering of images makes sure each individual layer
    is downloaded only once, but even having to wait longer than necessary the first
    time a pod is scheduled to a node is undesirable.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否需要在图像中包含操作系统分布的每个文件？可能不是。大多数文件永远不会被使用，会使你的图像比所需的更大。当然，图像分层确保每个单独的层只下载一次，但即使第一次将pod调度到节点时需要等待更长的时间也是不理想的。
- en: Deploying new pods and scaling them should be fast. This demands having small
    images without unnecessary cruft. If you’re building apps using the Go language,
    your images don’t need to include anything else apart from the app’s single binary
    executable file. This makes Go-based container images extremely small and perfect
    for Kubernetes.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 部署新的pod并对其进行扩展应该是快速的。这要求拥有没有不必要的冗余的小图像。如果你使用Go语言构建应用程序，你的图像不需要包含除了应用程序的单个可执行二进制文件之外的其他任何内容。这使得基于Go的容器图像非常小，非常适合Kubernetes。
- en: '|  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Use the `FROM scratch` directive in the Dockerfile for these images.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在Dockerfile中使用`FROM scratch`指令来创建这些图像。
- en: '|  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: But in practice, you’ll soon see these minimal images are extremely difficult
    to debug. The first time you need to run a tool such as `ping`, `dig`, `curl`,
    or something similar inside the container, you’ll realize how important it is
    for container images to also include at least a limited set of these tools. I
    can’t tell you what to include and what not to include in your images, because
    it depends on how you do things, so you’ll need to find the sweet spot yourself.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 但在实践中，你很快会发现这些最小图像极其难以调试。当你第一次需要在容器内部运行像`ping`、`dig`、`curl`或类似工具时，你会意识到容器图像也至少需要包括这些工具的有限集合是多么重要。我无法告诉你应该在图像中包含什么和排除什么，因为这取决于你如何做事，所以你需要自己找到最佳平衡点。
- en: 17.4.2\. Properly tagging your images and using imagePullPolicy wisely
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 17.4.2\. 正确标记您的图像并明智地使用imagePullPolicy
- en: You’ll also soon learn that referring to the `latest` image tag in your pod
    manifests will cause problems, because you can’t tell which version of the image
    each individual pod replica is running. Even if initially all your pod replicas
    run the same image version, if you push a new version of the image under the `latest`
    tag, and then pods are rescheduled (or you scale up your Deployment), the new
    pods will run the new version, whereas the old ones will still be running the
    old one. Also, using the `latest` tag makes it impossible to roll back to a previous
    version (unless you push the old version of the image again).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 你很快就会了解到，在您的pod配置文件中引用`latest`图像标签会导致问题，因为你无法确定每个单独的pod副本正在运行哪个版本的图像。即使最初所有pod副本都运行相同的图像版本，如果你在`latest`标签下推送了新版本的图像，然后pod被重新调度（或者你扩展了Deployment），新的pod将运行新版本，而旧的pod仍然会运行旧版本。此外，使用`latest`标签使得无法回滚到之前的版本（除非你再次推送图像的旧版本）。
- en: It’s almost mandatory to use tags containing a proper version designator instead
    of `latest`, except maybe in development. Keep in mind that if you use mutable
    tags (you push changes to the same tag), you’ll need to set the `imagePullPolicy`
    field in the pod spec to `Always`. But if you use that in production pods, be
    aware of the big caveat associated with it. If the image pull policy is set to
    `Always`, the container runtime will contact the image registry every time a new
    pod is deployed. This slows down pod startup a bit, because the node needs to
    check if the image has been modified. Worse yet, this policy prevents the pod
    from starting up when the registry cannot be contacted.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎强制使用包含适当版本指定符的标签，而不是`latest`，除了可能在开发环境中。记住，如果你使用可变标签（你向相同的标签推送更改），你需要将pod规范中的`imagePullPolicy`字段设置为`Always`。但如果你在生产pod中使用它，请注意与之相关的大问题。如果图像拉取策略设置为`Always`，容器运行时会每次部署新pod时都联系图像注册库。这会稍微减慢pod的启动速度，因为节点需要检查图像是否已被修改。更糟糕的是，此策略阻止pod在无法联系注册库时启动。
- en: 17.4.3\. Using multi-dimensional instead of single-dimensional labels
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 17.4.3\. 使用多维标签而不是单维标签
- en: Don’t forget to label all your resources, not only Pods. Make sure you add multiple
    labels to each resource, so they can be selected across each individual dimension.
    You (or the ops team) will be grateful you did it when the number of resources
    increases.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记标记所有资源，而不仅仅是Pod。确保为每个资源添加多个标签，这样它们就可以在每个单独的维度上被选择。当资源数量增加时，你（或运维团队）会感激你这样做。
- en: Labels may include things like
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 标签可能包括以下内容
- en: The name of the application (or perhaps microservice) the resource belongs to
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源所属的应用程序（或可能是微服务）的名称
- en: Application tier (front-end, back-end, and so on)
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序层（前端、后端等）
- en: Environment (development, QA, staging, production, and so on)
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境（开发、QA、预发布、生产等）
- en: Version
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本
- en: Type of release (stable, canary, green or blue for green/blue deployments, and
    so on)
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布类型（稳定、金丝雀、绿色或蓝色用于绿色/蓝色部署等）
- en: Tenant (if you’re running separate pods for each tenant instead of using namespaces)
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户（如果您为每个客户运行单独的 pod 而不是使用命名空间）
- en: Shard for sharded systems
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分片系统中的分片
- en: This will allow you to manage resources in groups instead of individually and
    make it easy to see where each resource belongs.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这将允许您以组为单位而不是单个资源来管理资源，并使查看每个资源属于何处变得容易。
- en: 17.4.4\. Describing each resource through annotations
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 17.4.4. 通过注释描述每个资源
- en: To add additional information to your resources use annotations. At the least,
    resources should contain an annotation describing the resource and an annotation
    with contact information of the person responsible for it.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 要向资源添加更多信息，请使用注释。至少，资源应包含一个描述资源的注释以及负责人的联系信息注释。
- en: In a microservices architecture, pods could contain an annotation that lists
    the names of the other services the pod is using. This makes it possible to show
    dependencies between pods. Other annotations could include build and version information
    and metadata used by tooling or graphical user interfaces (icon names, and so
    on).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在微服务架构中，pod 可能包含一个注释，列出 pod 正在使用的其他服务的名称。这使得显示 pod 之间的依赖关系成为可能。其他注释可能包括构建和版本信息以及由工具或图形用户界面（图标名称等）使用的元数据。
- en: Both labels and annotations make managing running applications much easier,
    but nothing is worse than when an application starts crashing and you don’t know
    why.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 标签和注释都使管理运行中的应用程序变得容易得多，但没有什么比应用程序开始崩溃而您不知道原因更糟糕的了。
- en: 17.4.5\. Providing information on why the process terminated
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 17.4.5. 提供进程终止原因的信息
- en: Nothing is more frustrating than having to figure out why a container terminated
    (or is even terminating continuously), especially if it happens at the worst possible
    moment. Be nice to the ops people and make their lives easier by including all
    the necessary debug information in your log files.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么比不得不弄清楚为什么容器终止（或者甚至持续终止）更令人沮丧的了，尤其是在最糟糕的时刻发生时。对运维人员友好，通过在日志文件中包含所有必要的调试信息来使他们的生活变得更轻松。
- en: But to make triage even easier, you can use one other Kubernetes feature that
    makes it possible to show the reason why a container terminated in the pod’s status.
    You do this by having the process write a termination message to a specific file
    in the container’s filesystem. The contents of this file are read by the Kubelet
    when the container terminates and are shown in the output of `kubectl describe
    pod`. If an application uses this mechanism, an operator can quickly see why the
    app terminated without even having to look at the container logs.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 但为了使分类更加容易，您还可以使用另一个 Kubernetes 功能，该功能可以显示 pod 中容器终止的原因。您通过让进程将终止消息写入容器文件系统中的特定文件来实现这一点。当容器终止时，Kubelet
    会读取该文件的内容，并在 `kubectl describe pod` 的输出中显示。如果应用程序使用此机制，操作员可以快速看到应用程序终止的原因，甚至无需查看容器日志。
- en: The default file the process needs to write the message to is /dev/termination-log,
    but it can be changed by setting the `terminationMessagePath` field in the container
    definition in the pod spec.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 进程需要写入消息的默认文件是 /dev/termination-log，但可以通过在 pod 规范中的容器定义中设置 `terminationMessagePath`
    字段来更改。
- en: You can see this in action by running a pod whose container dies immediately,
    as shown in the following listing.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过运行一个容器立即死亡的 pod 来看到这个功能在行动，如下面的列表所示。
- en: 'Listing 17.8\. Pod writing a termination message: termination-message.yaml'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.8. Pod 写入终止消息：termination-message.yaml
- en: '`apiVersion: v1 kind: Pod metadata:   name: pod-with-termination-message spec:
      containers:   - image: busybox     name: main     terminationMessagePath: /var/termination-reason`
    `1` `command:     - sh     - -c     - ''echo "I''''ve had enough" > /var/termination-reason
    ; exit 1''` `2`'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Pod metadata: name: pod-with-termination-message spec:
    containers: - image: busybox name: main terminationMessagePath: /var/termination-reason
    command: - sh - -c - ''echo "I\''ve had enough" > /var/termination-reason ; exit
    1''` `1` `2`'
- en: 1 You’re overriding the default path of the termination message file.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 您正在覆盖终止消息文件默认路径。
- en: 2 The container will write the message to the file just before exiting.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 容器将在退出前将消息写入文件。
- en: When running this pod, you’ll soon see the pod’s status shown as `CrashLoopBackOff`.
    If you then use `kubectl describe`, you can see why the container died, without
    having to dig down into its logs, as shown in the following listing.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行此Pod时，您很快就会看到Pod的状态显示为`CrashLoopBackOff`。如果您此时使用`kubectl describe`，您可以看到容器死亡的原因，而无需深入查看其日志，如下所示。
- en: Listing 17.9\. Seeing the container’s termination message with `kubectl describe`
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.9\. 使用 `kubectl describe` 查看容器的终止消息
- en: '`$ kubectl describe po` `Name:           pod-with-termination-message ... Containers:
    ...     State:      Waiting       Reason:   CrashLoopBackOff     Last State: Terminated
          Reason:   Error       Message:` `I''ve had enough``1` `Exit Code:       
    1       Started:          Tue, 21 Feb 2017 21:38:31 +0100       Finished:        
    Tue, 21 Feb 2017 21:38:31 +0100     Ready:              False     Restart Count:     
    6`'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl describe po` `Name:           pod-with-termination-message ... Containers:
    ...     State:      Waiting       Reason:   CrashLoopBackOff     Last State: Terminated
          Reason:   Error       Message:` `I''ve had enough``1` `Exit Code:       
    1       Started:          Tue, 21 Feb 2017 21:38:31 +0100       Finished:        
    Tue, 21 Feb 2017 21:38:31 +0100     Ready:              False     Restart Count:     
    6`'
- en: 1 You can see the reason why the container died without having to inspect its
    logs.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 您可以查看容器死亡的原因，而无需检查其日志。
- en: As you can see, the “`I've had enough"` message the process wrote to the file
    /var/termination-reason is shown in the container’s `Last State` section. Note
    that this mechanism isn’t limited only to containers that crash. It can also be
    used in pods that run a completable task and terminate successfully (you’ll find
    an example in the file termination-message-success.yaml).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，进程写入文件`/var/termination-reason`的“`I've had enough`”消息显示在容器的`Last State`（最后状态）部分。请注意，此机制不仅限于崩溃的容器。它也可以用于运行可完成任务并成功终止的Pod（您可以在文件`termination-message-success.yaml`中找到一个示例）。
- en: This mechanism is great for terminated containers, but you’ll probably agree
    that a similar mechanism would also be useful for showing app-specific status
    messages of running, not only terminated, containers. Kubernetes currently doesn’t
    provide any such functionality and I’m not aware of any plans to introduce it.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 此机制非常适合已终止的容器，但你可能会同意，类似的机制对于显示运行中（而不仅仅是已终止）容器的应用程序特定状态消息也同样有用。Kubernetes目前不提供此类功能，我也不了解有任何计划引入它。
- en: '|  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If the container doesn’t write the message to any file, you can set the `terminationMessagePolicy`
    field to `FallbackToLogsOnError`. In that case, the last few lines of the container’s
    log are used as its termination message (but only when the container terminates
    unsuccessfully).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如果容器没有将消息写入任何文件，您可以设置`terminationMessagePolicy`字段为`FallbackToLogsOnError`。在这种情况下，容器日志的最后几行用作其终止消息（但仅当容器未成功终止时）。
- en: '|  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 17.4.6\. Handling application logs
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 17.4.6\. 处理应用程序日志
- en: While we’re on the subject of application logging, let’s reiterate that apps
    should write to the standard output instead of files. This makes it easy to view
    logs with the `kubectl logs` command.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论应用程序日志的问题上，让我们重申，应用程序应该写入标准输出而不是文件。这使得使用`kubectl logs`命令查看日志变得容易。
- en: '|  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: If a container crashes and is replaced with a new one, you’ll see the new container’s
    log. To see the previous container’s logs, use the `--previous` option with `kubectl
    logs`.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果容器崩溃并被新的容器替换，您将看到新容器的日志。要查看前一个容器的日志，请使用`kubectl logs`的`--previous`选项。
- en: '|  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'If the application logs to a file instead of the standard output, you can display
    the log file using an alternative approach:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如果应用程序将日志记录到文件而不是标准输出，您可以使用另一种方法显示日志文件：
- en: '`$ kubectl exec <pod> cat <logfile>`'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl exec <pod> cat <logfile>`'
- en: This executes the `cat` command inside the container and streams the logs back
    to kubectl, which prints them out in your terminal.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在容器内部执行`cat`命令并将日志流回kubectl，kubectl在您的终端中打印它们。
- en: Copying log and other files to and from a container
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 将日志和其他文件复制到和从容器中
- en: 'You can also copy the log file to your local machine using the `kubectl cp`
    command, which we haven’t looked at yet. It allows you to copy files from and
    into a container. For example, if a pod called `foo-pod` and its single container
    contains a file at `/var/log/ foo.log`, you can transfer it to your local machine
    with the following command:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用我们尚未讨论的`kubectl cp`命令将日志文件复制到您的本地机器。它允许您从容器中复制文件到容器中。例如，如果Pod名为`foo-pod`且其单个容器在`/var/log/foo.log`位置有一个文件，您可以使用以下命令将其传输到您的本地机器：
- en: '`$ kubectl cp foo-pod:/var/log/foo.log foo.log`'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl cp foo-pod:/var/log/foo.log foo.log`'
- en: 'To copy a file from your local machine into the pod, specify the pod’s name
    in the second argument:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 要将文件从您的本地机器复制到 Pod 中，请在第二个参数中指定 Pod 的名称：
- en: '`$ kubectl cp localfile foo-pod:/etc/remotefile`'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl cp localfile foo-pod:/etc/remotefile`'
- en: This copies the file localfile to /etc/remotefile inside the pod’s container.
    If the pod has more than one container, you specify the container using the `-c
    containerName` option.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这会将本地文件复制到 Pod 容器内的 /etc/remotefile。如果 Pod 有多个容器，您可以使用 `-c containerName` 选项指定容器。
- en: Using centralized logging
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 使用集中式日志
- en: In a production system, you’ll want to use a centralized, cluster-wide logging
    solution, so all your logs are collected and (permanently) stored in a central
    location. This allows you to examine historical logs and analyze trends. Without
    such a system, a pod’s logs are only available while the pod exists. As soon as
    it’s deleted, its logs are deleted also.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产系统中，您会希望使用集中式、集群范围内的日志解决方案，以便将所有日志收集并（永久）存储在中央位置。这允许您检查历史日志并分析趋势。如果没有这样的系统，Pod
    的日志仅在 Pod 存在期间可用。一旦删除，其日志也会被删除。
- en: Kubernetes by itself doesn’t provide any kind of centralized logging. The components
    necessary for providing a centralized storage and analysis of all the container
    logs must be provided by additional components, which usually run as regular pods
    in the cluster.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 本身不提供任何类型的集中式日志。提供所有容器日志集中存储和分析所需组件必须由额外的组件提供，这些组件通常作为在集群中运行的常规 Pod
    运行。
- en: Deploying centralized logging solutions is easy. All you need to do is deploy
    a few YAML/JSON manifests and you’re good to go. On Google Kubernetes Engine,
    it’s even easier. Check the Enable Stackdriver Logging checkbox when setting up
    the cluster. Setting up centralized logging on an on-premises Kubernetes cluster
    is beyond the scope of this book, but I’ll give you a quick overview of how it’s
    usually done.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 部署集中式日志解决方案很简单。您只需部署几个 YAML/JSON 清单，然后就可以开始了。在 Google Kubernetes Engine 上，这甚至更简单。在设置集群时，请勾选启用
    Stackdriver 日志复选框。在本地 Kubernetes 集群上设置集中式日志超出了本书的范围，但我将简要概述通常是如何操作的。
- en: You may have already heard of the ELK stack composed of ElasticSearch, Logstash,
    and Kibana. A slightly modified variation is the EFK stack, where Logstash is
    replaced with FluentD.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经听说过由 ElasticSearch、Logstash 和 Kibana 组成的 ELK 堆栈。略有修改的变体是 EFK 堆栈，其中 Logstash
    被替换为 FluentD。
- en: When using the EFK stack for centralized logging, each Kubernetes cluster node
    runs a FluentD agent (usually as a pod deployed through a DaemonSet), which is
    responsible for gathering the logs from the containers, tagging them with pod-specific
    information, and delivering them to ElasticSearch, which stores them persistently.
    ElasticSearch is also deployed as a pod somewhere in the cluster. The logs can
    then be viewed and analyzed in a web browser through Kibana, which is a web tool
    for visualizing ElasticSearch data. It also usually runs as a pod and is exposed
    through a Service. The three components of the EFK stack are shown in the following
    figure.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 EFK 堆栈进行集中式日志时，每个 Kubernetes 集群节点运行一个 FluentD 代理（通常作为通过 DaemonSet 部署的 Pod），负责从容器收集日志，用
    Pod 特定的信息标记它们，并将它们交付给 ElasticSearch，它将它们持久化存储。ElasticSearch 也作为 Pod 部署在集群的某个位置。然后可以通过
    Kibana 在网络浏览器中查看和分析日志，Kibana 是一个用于可视化 ElasticSearch 数据的 Web 工具。它通常也作为 Pod 运行，并通过服务暴露。EFK
    堆栈的三个组件在以下图中显示。
- en: Figure 17.10\. Centralized logging with FluentD, ElasticSearch, and Kibana
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.10\. 使用 FluentD、ElasticSearch 和 Kibana 的集中式日志
- en: '![](images/00062.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00062.jpg)'
- en: '|  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In the next chapter, you’ll learn about Helm charts. You can use charts created
    by the Kubernetes community to deploy the EFK stack instead of creating your own
    YAML manifests.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将了解 Helm 图表。您可以使用 Kubernetes 社区创建的图表来部署 EFK 堆栈，而不是创建自己的 YAML 清单。
- en: '|  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Handling multi-line log statements
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 处理多行日志语句
- en: The FluentD agent stores each line of the log file as an entry in the ElasticSearch
    data store. There’s one problem with that. Log statements spanning multiple lines,
    such as exception stack traces in Java, appear as separate entries in the centralized
    logging system.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: FluentD 代理将日志文件的每一行存储为 ElasticSearch 数据存储中的一个条目。这里有一个问题。跨越多行的日志语句，如 Java 中的异常堆栈跟踪，在集中式日志系统中显示为单独的条目。
- en: To solve this problem, you can have the apps output JSON instead of plain text.
    This way, a multiline log statement can be stored and shown in Kibana as a single
    entry. But that makes viewing logs with `kubectl logs` much less human-friendly.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个问题，你可以让应用输出 JSON 而不是纯文本。这样，多行日志语句就可以作为一个单独的条目存储和显示在 Kibana 中。但这样做会让使用 `kubectl
    logs` 查看日志变得不太人性化。
- en: The solution may be to keep outputting human-readable logs to standard output,
    while writing JSON logs to a file and having them processed by FluentD. This requires
    configuring the node-level FluentD agent appropriately or adding a logging sidecar
    container to every pod.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案可能是继续将可读日志输出到标准输出，同时将 JSON 日志写入文件，并由 FluentD 处理。这需要适当地配置节点级别的 FluentD 代理或为每个
    Pod 添加一个日志边车容器。
- en: 17.5\. Best practices for development and testing
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 17.5. 开发和测试的最佳实践
- en: We’ve talked about what to be mindful of when developing apps, but we haven’t
    talked about the development and testing workflows that will help you streamline
    those processes. I don’t want to go into too much detail here, because everyone
    needs to find what works best for them, but here are a few starting points.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了在开发应用时需要注意的事项，但还没有讨论那些可以帮助你简化这些流程的开发和测试工作流程。我不想在这里过多地详细说明，因为每个人都需要找到最适合他们的方法，但这里有一些起点。
- en: 17.5.1\. Running apps outside of Kubernetes during development
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 17.5.1. 在开发期间在 Kubernetes 之外运行应用
- en: When you’re developing an app that will run in a production Kubernetes cluster,
    does that mean you also need to run it in Kubernetes during development? Not really.
    Having to build the app after each minor change, then build the container image,
    push it to a registry, and then re-deploy the pods would make development slow
    and painful. Luckily, you don’t need to go through all that trouble.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开发一个将在生产 Kubernetes 集群中运行的应用时，这意味着你也需要在开发期间在 Kubernetes 中运行它吗？实际上并不是。每次进行小修改后都必须构建应用，然后构建容器镜像，推送到注册表，然后重新部署
    Pod，这会让开发变得缓慢且痛苦。幸运的是，你不需要经历所有这些麻烦。
- en: You can always develop and run apps on your local machine, the way you’re used
    to. After all, an app running in Kubernetes is a regular (although isolated) process
    running on one of the cluster nodes. If the app depends on certain features the
    Kubernetes environment provides, you can easily replicate that environment on
    your development machine.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 你总是可以在你的本地机器上开发和运行应用，就像你习惯的那样。毕竟，在 Kubernetes 中运行的应用是一个在集群节点上运行的常规（尽管是隔离的）进程。如果应用依赖于
    Kubernetes 环境提供的某些功能，你可以在你的开发机器上轻松地复制该环境。
- en: I’m not even talking about running the app in a container. Most of the time,
    you don’t need that—you can usually run the app directly from your IDE.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我甚至不是在谈论在容器中运行应用。大多数时候，你不需要那样做——你通常可以直接从你的 IDE 中运行应用。
- en: Connecting to backend services
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到后端服务
- en: In production, if the app connects to a backend Service and uses the `BACKEND_SERVICE_HOST`
    and `BACKEND_SERVICE_PORT` environment variables to find the Service’s coordinates,
    you can obviously set those environment variables on your local machine manually
    and point them to the backend Service, regardless of if it’s running outside or
    inside a Kubernetes cluster. If it’s running inside Kubernetes, you can always
    (at least temporarily) make the Service accessible externally by changing it to
    a `NodePort` or a `LoadBalancer`-type Service.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，如果应用连接到后端服务并使用 `BACKEND_SERVICE_HOST` 和 `BACKEND_SERVICE_PORT` 环境变量来查找服务的坐标，你显然可以在本地机器上手动设置这些环境变量并将它们指向后端服务，无论它是在
    Kubernetes 集群外部还是内部运行。如果它在 Kubernetes 内部运行，你始终可以（至少暂时地）通过将其更改为 `NodePort` 或 `LoadBalancer`
    类型的服务来使服务对外部可访问。
- en: Connecting to the API server
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到 API 服务器
- en: Similarly, if your app requires access to the Kubernetes API server when running
    inside a Kubernetes cluster, it can easily talk to the API server from outside
    the cluster during development. If it uses the ServiceAccount’s token to authenticate
    itself, you can always copy the ServiceAccount’s Secret’s files to your local
    machine with `kubectl cp`. The API server doesn’t care if the client accessing
    it is inside or outside the cluster.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果你的应用在 Kubernetes 集群内部运行时需要访问 Kubernetes API 服务器，它可以在开发期间轻松地从集群外部与 API 服务器通信。如果它使用
    ServiceAccount 的令牌进行身份验证，你可以始终使用 `kubectl cp` 将 ServiceAccount 的 Secret 文件复制到你的本地机器。API
    服务器不会关心访问它的客户端是在集群内部还是外部。
- en: If the app uses an ambassador container like the one described in [chapter 8](index_split_070.html#filepos790863),
    you don’t even need those Secret files. Run `kubectl proxy` on your local machine,
    run your app locally, and it should be ready to talk to your local `kubectl proxy`
    (as long as it and the ambassador container bind the proxy to the same port).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 如果应用使用像第 8 章中描述的代理容器（[chapter 8](index_split_070.html#filepos790863)），你甚至不需要那些
    Secret 文件。在你的本地机器上运行 `kubectl proxy`，本地运行你的应用，它应该已经准备好与你的本地 `kubectl proxy` 进行通信（只要它和代理容器将代理绑定到相同的端口）。
- en: In this case, you’ll need to make sure the user account your local `kubectl`
    is using has the same privileges as the ServiceAccount the app will run under.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你需要确保你的本地 `kubectl` 所使用的用户账户具有与应用将运行的 ServiceAccount 相同的权限。
- en: Running inside a container even during development
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发过程中在容器内运行
- en: When during development you absolutely have to run the app in a container for
    whatever reason, there is a way of avoiding having to build the container image
    every time. Instead of baking the binaries into the image, you can always mount
    your local filesystem into the container through Docker volumes, for example.
    This way, after you build a new version of the app’s binaries, all you need to
    do is restart the container (or not even that, if hot-redeploy is supported).
    No need to rebuild the image.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在开发过程中出于任何原因绝对需要在容器中运行应用时，有一种方法可以避免每次都需要构建容器镜像。你不需要将二进制文件烘焙到镜像中，你总是可以通过 Docker
    卷将你的本地文件系统挂载到容器中，例如。这样，在你构建了应用二进制的新版本后，你所需要做的就是重启容器（或者如果支持热重载，甚至不需要这样做）。无需重新构建镜像。
- en: 17.5.2\. Using Minikube in development
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 17.5.2\. 在开发中使用 Minikube
- en: As you can see, nothing forces you to run your app inside Kubernetes during
    development. But you may do that anyway to see how the app behaves in a true Kubernetes
    environment.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，没有强制要求你在开发期间在 Kubernetes 内运行你的应用。但你仍然可以这样做，以查看应用在真实 Kubernetes 环境中的行为。
- en: You may have used Minikube to run examples in this book. Although a Minikube
    cluster runs only a single worker node, it’s nevertheless a valuable method of
    trying out your app in Kubernetes (and, of course, developing all the resource
    manifests that make up your complete application). Minikube doesn’t offer everything
    that a proper multi-node Kubernetes cluster usually provides, but in most cases,
    that doesn’t matter.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经使用 Minikube 运行过本书中的示例。尽管 Minikube 集群只运行一个工作节点，但无论如何，它都是一个在 Kubernetes 中尝试你的应用（当然，还包括开发构成完整应用的资源清单）的有价值的方法。Minikube
    并不提供像正常的多个节点 Kubernetes 集群那样的一切，但在大多数情况下，这并不重要。
- en: Mounting local files into the minikube VM and then into your containers
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 将本地文件挂载到 minikube VM 然后挂载到你的容器中
- en: When you’re developing with Minikube and you’d like to try out every change
    to your app in your Kubernetes cluster, you can mount your local filesystem into
    the Minikube VM using the `minikube mount` command and then mount it into your
    containers through a `hostPath` volume. You’ll find additional instructions on
    how to do that in the Minikube documentation at [https://github.com/kubernetes/minikube/tree/master/docs](https://github.com/kubernetes/minikube/tree/master/docs).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在使用 Minikube 进行开发，并希望尝试将你的应用的所有更改在 Kubernetes 集群中运行时，你可以使用 `minikube mount`
    命令将你的本地文件系统挂载到 Minikube VM 中，然后通过 `hostPath` 卷将其挂载到容器中。你可以在 Minikube 文档中找到如何操作的额外说明，文档地址为
    [https://github.com/kubernetes/minikube/tree/master/docs](https://github.com/kubernetes/minikube/tree/master/docs)。
- en: Using the Docker daemon inside the minikube VM to build your images
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 minikube VM 内部的 Docker 守护进程构建你的镜像
- en: 'If you’re developing your app with Minikube and planning to build the container
    image after every change, you can use the Docker daemon inside the Minikube VM
    to do the building, instead of having to build the image through your local Docker
    daemon, push it to a registry, and then have it pulled by the daemon in the VM.
    To use Minikube’s Docker daemon, all you need to do is point your `DOCKER_HOST`
    environment variable to it. Luckily, this is much easier than it sounds. All you
    need to do is run the following command on your local machine:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 Minikube 开发你的应用，并计划在每次更改后构建容器镜像，你可以使用 Minikube VM 内部的 Docker 守护进程来构建镜像，而不是通过你的本地
    Docker 守护进程构建镜像，推送到仓库，然后由 VM 内的守护进程拉取。要使用 Minikube 的 Docker 守护进程，你只需要将你的 `DOCKER_HOST`
    环境变量指向它。幸运的是，这比听起来要简单得多。你只需要在本地机器上运行以下命令：
- en: '`$ eval $(minikube docker-env)`'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ eval $(minikube docker-env)`'
- en: This will set all the required environment variables for you. You then build
    your images the same way as if the Docker daemon was running on your local machine.
    After you build the image, you don’t need to push it anywhere, because it’s already
    stored locally on the Minikube VM, which means new pods can use the image immediately.
    If your pods are already running, you either need to delete them or kill their
    containers so they’re restarted.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为您设置所有必需的环境变量。然后，您将以与Docker守护进程在本地机器上运行相同的方式构建您的镜像。构建镜像后，您不需要将其推送到任何地方，因为它已经存储在Minikube
    VM的本地，这意味着新的Pod可以立即使用该镜像。如果您的Pod已经在运行，您需要删除它们或终止它们的容器以便重新启动。
- en: Building images locally and copying them over to the minikube VM directly
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地构建镜像并将其直接复制到minikube VM
- en: 'If you can’t use the daemon inside the VM to build the images, you still have
    a way to avoid having to push the image to a registry and have the Kubelet running
    in the Minikube VM pull it. If you build the image on your local machine, you
    can copy it over to the Minikube VM with the following command:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您无法在VM内部使用守护进程来构建镜像，您仍然有方法避免将镜像推送到注册表，并让运行在Minikube VM中的Kubelet拉取它。如果您在本地机器上构建镜像，可以使用以下命令将其复制到Minikube
    VM：
- en: '`$ docker save <image> | (eval $(minikube docker-env) && docker load)`'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ docker save <image> | (eval $(minikube docker-env) && docker load)`'
- en: As before, the image is immediately ready to be used in a pod. But make sure
    the `imagePullPolicy` in your pod spec isn’t set to `Always`, because that would
    cause the image to be pulled from the external registry again and you’d lose the
    changes you’ve copied over.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，镜像立即准备好在Pod中使用。但请确保您的Pod规范中的`imagePullPolicy`没有设置为`Always`，因为这会导致镜像再次从外部注册表中拉取，您将丢失复制过的更改。
- en: Combining Minikube with a proper Kubernetes cluster
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 将Minikube与合适的Kubernetes集群结合使用
- en: You have virtually no limit when developing apps with Minikube. You can even
    combine a Minikube cluster with a proper Kubernetes cluster. I sometimes run my
    development workloads in my local Minikube cluster and have them talk to my other
    workloads that are deployed in a remote multi-node Kubernetes cluster thousands
    of miles away.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Minikube开发应用程序时，您几乎没有任何限制。您甚至可以将Minikube集群与一个合适的Kubernetes集群结合起来。我有时在我的本地Minikube集群中运行我的开发工作负载，并让它们与部署在数千英里外的远程多节点Kubernetes集群中的其他工作负载通信。
- en: Once I’m finished with development, I can move my local workloads to the remote
    cluster with no modifications and with absolutely no problems thanks to how Kubernetes
    abstracts away the underlying infrastructure from the app.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦开发完成，我可以将本地工作负载无缝迁移到远程集群，无需任何修改，也无需任何问题，这得益于Kubernetes如何将底层基础设施从应用程序中抽象出来。
- en: 17.5.3\. Versioning and auto-deploying resource manifests
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 17.5.3\. 版本控制和自动部署资源清单
- en: Because Kubernetes uses a declarative model, you never have to figure out the
    current state of your deployed resources and issue imperative commands to bring
    that state to what you desire. All you need to do is tell Kubernetes your desired
    state and it will take all the necessary actions to reconcile the cluster state
    with the desired state.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Kubernetes使用声明性模型，您永远不需要确定已部署资源的当前状态并发出 imperative 命令来将状态带到您所期望的状态。您需要做的只是告诉Kubernetes您所期望的状态，它将采取所有必要的行动来使集群状态与期望状态相协调。
- en: You can store your collection of resource manifests in a Version Control System,
    enabling you to perform code reviews, keep an audit trail, and roll back changes
    whenever necessary. After each commit, you can run the `kubectl apply` command
    to have your changes reflected in your deployed resources.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将资源清单的集合存储在版本控制系统（Version Control System）中，这样您就可以执行代码审查、保留审计跟踪，并在必要时回滚更改。在每次提交后，您都可以运行`kubectl
    apply`命令，以便您的更改反映在已部署的资源中。
- en: If you run an agent that periodically (or when it detects a new commit) checks
    out your manifests from the Version Control System (VCS), and then runs the `apply`
    command, you can manage your running apps simply by committing changes to the
    VCS without having to manually talk to the Kubernetes API server. Luckily, the
    people at Box (which coincidently was used to host this book’s manuscript and
    other materials) developed and released a tool called `kube-applier`, which does
    exactly what I described. You’ll find the tool’s source code at [https://github.com/box/kube-applier](https://github.com/box/kube-applier).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行一个代理，该代理定期（或当它检测到新的提交时）从版本控制系统（VCS）检出你的清单，然后运行 `apply` 命令，你可以通过将更改提交到 VCS
    来简单地管理你的运行中的应用程序，而无需手动与 Kubernetes API 服务器通信。幸运的是，Box 的人（巧合的是，他们使用了这个书的手稿和其他材料）开发和发布了一个名为
    `kube-applier` 的工具，它正好做了我描述的事情。你可以在 [https://github.com/box/kube-applier](https://github.com/box/kube-applier)
    找到这个工具的源代码。
- en: You can use multiple branches to deploy the manifests to a development, QA,
    staging, and production cluster (or in different namespaces in the same cluster).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用多个分支将清单部署到开发、QA、预发布和生产集群（或在同一集群的不同命名空间中）。
- en: 17.5.4\. Introducing Ksonnet as an alternative to writing YAML/JSON manifests
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 17.5.4\. 介绍 Ksonnet 作为编写 YAML/JSON 清单的替代方案
- en: We’ve seen a number of YAML manifests throughout the book. I don’t see writing
    YAML as too big of a problem, especially once you learn how to use `kubectl explain`
    to see the available options, but some people do.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在书中看到了许多 YAML 清单。我认为编写 YAML 并不是太大的问题，尤其是当你学会了如何使用 `kubectl explain` 来查看可用选项时，但有些人确实觉得有困难。
- en: Just as I was finalizing the manuscript for this book, a new tool called Ksonnet
    was announced. It’s a library built on top of Jsonnet, which is a data templating
    language for building JSON data structures. Instead of writing the complete JSON
    by hand, it lets you define parameterized JSON fragments, give them a name, and
    then build a full JSON manifest by referencing those fragments by name, instead
    of repeating the same JSON code in multiple locations—much like you use functions
    or methods in a programming language.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 正当我正在完成这本书的手稿时，一个名为 Ksonnet 的新工具被宣布推出。它是一个建立在 Jsonnet 之上的库，而 Jsonnet 是一种用于构建
    JSON 数据结构的数据模板语言。它允许你定义参数化的 JSON 片段，给它们命名，然后通过引用这些片段的名称来构建完整的 JSON 清单，而不是在多个位置重复相同的
    JSON 代码——这就像你在编程语言中使用函数或方法一样。
- en: Ksonnet defines the fragments you’d find in Kubernetes resource manifests, allowing
    you to quickly build a complete Kubernetes resource JSON manifest with much less
    code. The following listing shows an example.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: Ksonnet 定义了你在 Kubernetes 资源清单中找到的片段，允许你用更少的代码快速构建完整的 Kubernetes 资源 JSON 清单。以下列表显示了一个示例。
- en: 'Listing 17.10\. The `kubia` Deployment written with Ksonnet: kubia.ksonnet'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.10\. 使用 Ksonnet 编写的 `kubia` 部署：kubia.ksonnet
- en: '`local k = import "../ksonnet-lib/ksonnet.beta.1/k.libsonnet";  local container
    = k.core.v1.container; local deployment = k.apps.v1beta1.deployment;  local kubiaContainer
    =` `1` `container.default("kubia", "luksa/kubia:v1") +` `1` `container.helpers.namedPort("http",
    8080);` `1` `deployment.default("kubia", kubiaContainer) +` `2` `deployment.mixin.spec.replicas(3)`
    `2`'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '`local k = import "../ksonnet-lib/ksonnet.beta.1/k.libsonnet";  local container
    = k.core.v1.container; local deployment = k.apps.v1beta1.deployment;  local kubiaContainer
    =` `1` `container.default("kubia", "luksa/kubia:v1") +` `1` `container.helpers.namedPort("http",
    8080);` `1` `deployment.default("kubia", kubiaContainer) +` `2` `deployment.mixin.spec.replicas(3)`
    `2`'
- en: 1 This defines a container called kubia, which uses the luksa/kubia:v1 image
    and includes a port called http.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 这定义了一个名为 kubia 的容器，它使用 luksa/kubia:v1 镜像，并包含一个名为 http 的端口。
- en: 2 This will be expanded into a full Deployment resource. The kubiaContainer
    defined here will be included in the Deployment’s pod template.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 这将被扩展为一个完整的 Deployment 资源。这里定义的 kubiaContainer 将包含在 Deployment 的 pod 模板中。
- en: 'The kubia.ksonnet file shown in the listing is converted to a full JSON Deployment
    manifest when you run the following command:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行以下命令时，将列表中显示的 kubia.ksonnet 文件转换为完整的 JSON Deployment 清单：
- en: '`$ jsonnet kubia.ksonnet`'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ jsonnet kubia.ksonnet`'
- en: The power of Ksonnet and Jsonnet becomes apparent when you realize you can define
    your own higher-level fragments and make all your manifests consistent and duplication-free.
    You’ll find more information on using and installing Ksonnet and Jsonnet at [https://github.com/ksonnet/ksonnet-lib](https://github.com/ksonnet/ksonnet-lib).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 当你意识到你可以定义自己的高级片段，并使所有清单保持一致且无重复时，Ksonnet 和 Jsonnet 的强大功能就显现出来了。你可以在 [https://github.com/ksonnet/ksonnet-lib](https://github.com/ksonnet/ksonnet-lib)
    找到有关使用和安装 Ksonnet 和 Jsonnet 的更多信息。
- en: 17.5.5\. Employing Continuous Integration and Continuous Delivery (CI/CD)
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 17.5.5. 采用持续集成和持续交付 (CI/CD)
- en: We’ve touched on automating the deployment of Kubernetes resources two sections
    back, but you may want to set up a complete CI/CD pipeline for building your application
    binaries, container images, and resource manifests and then deploying them in
    one or more Kubernetes clusters.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前两节中提到了自动化部署 Kubernetes 资源，但你可能希望设置一个完整的 CI/CD 流水线，用于构建你的应用程序二进制文件、容器镜像和资源清单，然后在一个或多个
    Kubernetes 集群中部署它们。
- en: You’ll find many online resources talking about this subject. Here, I’d like
    to point you specifically to the Fabric8 project ([http://fabric8.io](http://fabric8.io)),
    which is an integrated development platform for Kubernetes. It includes Jenkins,
    the well-known, open-source automation system, and various other tools to deliver
    a full CI/CD pipeline for DevOps-style development, deployment, and management
    of microservices on Kubernetes.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现许多在线资源都在讨论这个主题。在这里，我想特别指出 Fabric8 项目 ([http://fabric8.io](http://fabric8.io))，这是一个针对
    Kubernetes 的集成开发平台。它包括知名的、开源的自动化系统 Jenkins，以及其他各种工具，以提供完整的 CI/CD 流水线，用于 DevOps
    风格的开发、部署和管理 Kubernetes 上的微服务。
- en: If you’d like to build your own solution, I also suggest looking at one of the
    Google Cloud Platform’s online labs that talks about this subject. It’s available
    at [https://github.com/GoogleCloudPlatform/continuous-deployment-on-kubernetes](https://github.com/GoogleCloudPlatform/continuous-deployment-on-kubernetes).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要构建自己的解决方案，我还建议查看 Google Cloud Platform 的在线实验室之一，该实验室讨论了这个主题。它可在 [https://github.com/GoogleCloudPlatform/continuous-deployment-on-kubernetes](https://github.com/GoogleCloudPlatform/continuous-deployment-on-kubernetes)
    找到。
- en: 17.6\. Summary
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 17.6. 摘要
- en: Hopefully, the information in this chapter has given you an even deeper insight
    into how Kubernetes works and will help you build apps that feel right at home
    when deployed to a Kubernetes cluster. The aim of this chapter was to
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这一章的信息能让你对 Kubernetes 的工作原理有更深入的了解，并帮助你构建在 Kubernetes 集群中部署时感觉如鱼得水的应用程序。本章的目标是
- en: Show you how all the resources covered in this book come together to represent
    a typical application running in Kubernetes.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示这本书中涵盖的所有资源是如何结合在一起，以表示在 Kubernetes 中运行的典型应用程序。
- en: Make you think about the difference between apps that are rarely moved between
    machines and apps running as pods, which are relocated much more frequently.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让你思考很少在机器之间移动的应用程序和作为 pods 运行的应用程序之间的区别，后者被重新定位得更加频繁。
- en: Help you understand that your multi-component apps (or microservices, if you
    will) shouldn’t rely on a specific start-up order.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帮助你理解你的多组件应用程序（或者如果你愿意，微服务）不应该依赖于特定的启动顺序。
- en: Introduce init containers, which can be used to initialize a pod or delay the
    start of the pod’s main containers until a precondition is met.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍初始化容器，它们可以用来初始化一个 pod 或在满足先决条件之前延迟 pod 的主要容器的启动。
- en: Teach you about container lifecycle hooks and when to use them.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教你关于容器生命周期钩子和何时使用它们。
- en: Gain a deeper insight into the consequences of the distributed nature of Kubernetes
    components and its eventual consistency model.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入了解 Kubernetes 组件的分布式性质及其最终一致性模型的后果。
- en: Learn how to make your apps shut down properly without breaking client connections.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何让你的应用程序正确关闭，而不会中断客户端连接。
- en: Give you a few small tips on how to make your apps easier to manage by keeping
    image sizes small, adding annotations and multi-dimensional labels to all your
    resources, and making it easier to see why an application terminated.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给你一些小贴士，如何通过保持镜像大小小、为所有资源添加注释和多维标签，以及使查看应用程序终止原因更容易，来使你的应用程序更容易管理。
- en: Teach you how to develop Kubernetes apps and run them locally or in Minikube
    before deploying them on a proper multi-node cluster.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教你如何开发 Kubernetes 应用程序，并在将它们部署到真正的多节点集群之前，在本地或 Minikube 中运行它们。
- en: In the next and final chapter, we’ll learn how you can extend Kubernetes with
    your own custom API objects and controllers and how others have done it to create
    complete Platform-as-a-Service solutions on top of Kubernetes.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章和最后一章中，我们将学习如何使用你自己的自定义 API 对象和控制器扩展 Kubernetes，以及其他人是如何做到的，以在 Kubernetes
    之上创建完整的 Platform-as-a-Service 解决方案。
